{
  "paper_id": "http://arxiv.org/abs/2506.04205v1",
  "extraction_timestamp": "2025-06-06 01:31:54.275924",
  "extraction_version": "1.0",
  "key_findings": [
    "EPiC achieves 1.5x faster training speed with competitive accuracy by condensing Chain-of-Thought (CoT) traces at 50% condensation ratio, specifically removing middle portions while preserving head (problem understanding) and tail (solution convergence) sections.",
    "CoT traces follow a predictable three-stage structure: problem understanding, exploration, and solution convergence, where the middle exploration stage contains the most noise and speculative content that can be safely removed without performance loss.",
    "Edge-preserving condensation technique maintains both answer accuracy and coherent reasoning generation capability by retaining reflective cues in early stages and solution-relevant information in final stages of reasoning traces.",
    "The method addresses the significant training cost problem in knowledge distillation from large reasoning models like DeepSeek-R1 to smaller base models, where verbose CoT traces create computational bottlenecks during supervised fine-tuning.",
    "Empirical validation on QWEN 2.5-MATH-7B-INSTRUCT demonstrates that structural preservation of reasoning traces is more important than maintaining complete verbosity, enabling resource-efficient reasoning model training without sacrificing reasoning quality."
  ],
  "limitations": [
    "Method may not generalize to all types of reasoning tasks beyond mathematical problem solving",
    "Optimal condensation ratio may vary across different model architectures and reasoning domains"
  ],
  "future_work": [
    "Extending EPiC to other reasoning domains beyond mathematics",
    "Investigating adaptive condensation ratios based on problem complexity"
  ],
  "study_type": "case_study",
  "techniques_used": [],
  "implementation_complexity": "medium",
  "resource_requirements": {
    "compute_requirements": "Training infrastructure capable of handling 7B parameter models",
    "data_requirements": "OpenR1Math dataset with CoT traces from large reasoning models",
    "budget_tier": null,
    "special_hardware": [],
    "cloud_services": []
  },
  "success_metrics": [],
  "problem_addressed": "Reducing training costs and time for reasoning model distillation while preserving reasoning quality and answer accuracy",
  "prerequisites": [
    "Access to large reasoning model outputs for CoT trace generation",
    "Understanding of chain-of-thought reasoning structure",
    "Supervised fine-tuning infrastructure"
  ],
  "comparable_approaches": [],
  "real_world_applications": [
    "Cost-efficient training of reasoning models for educational applications",
    "Scalable deployment of mathematical reasoning assistants",
    "Resource-constrained environments requiring reasoning capabilities"
  ],
  "evidence_strength": 0.8,
  "practical_applicability": 0.9,
  "extraction_confidence": 0.85,
  "has_code_available": false,
  "has_dataset_available": true,
  "reproducibility_score": null,
  "industry_validation": true
}