{
  "paper_id": "http://arxiv.org/abs/2506.01055v1",
  "extraction_timestamp": "2025-06-08 15:47:58.281472",
  "extraction_version": "1.0",
  "key_findings": [
    "Prompt injection attacks against LLM agents cause 15-50% utility degradation across 16 banking tasks, with average attack success rates around 20%, demonstrating significant vulnerability in tool-calling agents during task execution.",
    "Data flow-based attacks successfully exploit LLM agents to leak personal data observed during banking operations, though safety alignments prevent leakage of highly sensitive data like passwords in isolation.",
    "Password leakage vulnerability increases when attackers request passwords alongside 1-2 additional personal details, suggesting that bundled data requests can bypass safety mechanisms more effectively than single-item requests.",
    "Fund Transfer & Payment and Profile & Authentication Management tasks show lowest utility under attack, while Transactions & Insights demonstrate highest resilience, indicating task-specific vulnerability patterns that can guide defense prioritization.",
    "Extended evaluation across 48 tasks reveals 15% average attack success rate with no built-in AgentDojo defense fully preventing data leakage, highlighting the need for more robust security frameworks in agentic AI systems."
  ],
  "limitations": [
    "Study limited to fictitious banking agent scenarios, may not generalize to other domains or real-world banking systems",
    "Evaluation focused on specific prompt injection techniques, may not cover emerging attack vectors or sophisticated adversarial methods"
  ],
  "future_work": [],
  "study_type": "empirical",
  "techniques_used": [
    "synthetic_data_generation",
    "other"
  ],
  "implementation_complexity": "medium",
  "resource_requirements": {
    "compute_requirements": null,
    "data_requirements": null,
    "budget_tier": null,
    "special_hardware": [],
    "cloud_services": []
  },
  "success_metrics": [],
  "problem_addressed": "Personal data leakage vulnerability in LLM agents through prompt injection attacks during task execution",
  "prerequisites": [
    "LLM agent framework",
    "Tool-calling capabilities",
    "Security testing environment",
    "Synthetic conversation datasets"
  ],
  "comparable_approaches": [],
  "real_world_applications": [
    "Banking agent security",
    "Financial AI assistant protection",
    "Personal data privacy in conversational AI",
    "Agent-based customer service security"
  ],
  "total_author_hindex": 0,
  "has_conference_mention": false,
  "author_hindices": {},
  "extraction_confidence": 0.85,
  "has_code_available": false,
  "has_dataset_available": true,
  "reproducibility_score": null,
  "industry_validation": false
}