{
  "id": "http://arxiv.org/abs/2506.06280v1",
  "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias",
  "summary": "Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\nmatrices has been an active area of research in recent years. At a high level,\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\nempirical spectral densities (ESD) of weight matrices. It provides insight into\nhow well a model is trained and can guide decisions on assigning better\nlayer-wise training hyperparameters. In this paper, we address a challenge\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\nweight matrices on estimated heavytailness metrics. We demonstrate that\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\nof measuring the heavytailness of the original ESD, we measure the average ESD\nof these subsampled submatrices. We show that measuring the heavytailness of\nthese submatrices with the fixed aspect ratio can effectively mitigate the\naspect ratio bias. We validate our approach across various optimization\ntechniques and application domains that involve eigenspectrum analysis of\nweights, including image classification in computer vision (CV) models,\nscientific machine learning (SciML) model training, and large language model\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\nimproves the accuracy of eigenspectrum analysis while enabling more effective\nlayer-wise hyperparameter assignment in these application domains. In one of\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\nby 17.3% when compared with the state-of-the-art method.",
  "authors": [
    "Yuanzhe Hu",
    "Kinshuk Goel",
    "Vlad Killiakov",
    "Yaoqing Yang"
  ],
  "published": "2025-06-06T17:59:28Z",
  "updated": "2025-06-06T17:59:28Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.06280v1",
  "comments": "30 pages, 14 figures, published to ICML 2025",
  "full_text": "--- Page 1 ---\narXiv:2506.06280v1  [cs.LG]  6 Jun 2025Eigenspectrum Analysis of Neural Networks without Aspect Ratio\nBias\nYuanzhe Hu1, Kinshuk Goel2,3, Vlad Killiakov4, Yaoqing Yang2\n1Department of Computer Science and Engineering, University of California, San Diego\n2Department of Computer Science, Dartmouth College\n3Department of Computer Science & Engineering, SRM Institute of Science & Technology\n4Independent Researcher, University of California, Berkeley\nAbstract\nDiagnosing deep neural networks (DNNs) through the eigenspectrum of weight matrices has been an\nactive area of research in recent years. At a high level, eigenspectrum analysis of DNNs involves measuring\ntheheavytailness of the empirical spectral densities (ESD) of weight matrices. It provides insight into how\nwell a model is trained and can guide decisions on assigning better layer-wise training hyperparameters. In\nthis paper, we address a challenge associated with such eigenspectrum methods: the impact of the aspect\nratio of weight matrices on estimated heavytailness metrics. We demonstrate that matrices of varying\nsizes (and aspect ratios) introduce a non-negligible bias in estimating heavytailness metrics, leading to\ninaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we\npropose FARMS (Fixed-Aspect- RatioMatrixSubsampling), a method that normalizes the weight matrices\nby subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the\noriginal ESD, we measure the average ESD of these subsampled submatrices. We show that measuring\nthe heavytailness of these submatrices with the fixed aspect ratio can effectively mitigate the aspect\nratio bias. We validate our approach across various optimization techniques and application domains\nthat involve eigenspectrum analysis of weights, including image classification in computer vision (CV)\nmodels, scientific machine learning (SciML) model training, and large language model (LLM) pruning.\nOur results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum\nanalysis while enabling more effective layer-wise hyperparameter assignment in these application domains.\nIn one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3%\nwhen compared with the state-of-the-art method.\n1 Introduction\nRandom Matrix Theory (RMT) has long been used as a foundational tool in multiple research domains (Bai\nand Silverstein, 2010; Guhr et al., 1998; Nadakuditi and Newman, 2012; Tao, 2012; Tulino and Verd¬¥ u, 2004)\nand has now been applied to providing precise characterizations of neural networks (NNs) (Adlam and\nPennington, 2020; Ba et al., 2022; Couillet and Liao, 2022; Derezinski et al., 2020; Dobriban and Wager,\n2018; Hastie et al., 2022; Hu and Lu, 2022; Mei and Montanari, 2022; Pennington and Worah, 2017). Among\nseveral emerging topics in this field, Heavy-Tailed Self-Regularization (HT-SR) (Mahoney and Martin, 2019;\nMartin and Mahoney, 2019, 2021; Martin et al., 2021) Theory has been gaining significant attention. Unlike\nconventional RMT approaches, HT-SR Theory focuses on studying weight matrices of DNNs with strongly\ncorrelated elements, a typical characteristic of well-trained, practical DNNs. The essence of HT-SR theory is\nthat the weight matrices of well-trained DNNs are strongly correlated, and that leads to heavy-tailed (HTed)\nESDs (Martin and Mahoney, 2021). Analyzing these HT ESDs provides valuable insights into the quality\nof trained models, which provides methods and techniques for model diagnostics and model training. For\nexample, Martin and Mahoney (2021) show that one can locate undertrained layers by finding weight matrices\nwith less HTed ESDs, which provides useful metrics for model ranking, comparison and selection (Martin et al.,\n1\n--- Page 2 ---\nAccurate  \nPL_Alpha Values\nNWeight Matrices with Dif ferent\n Aspect Ratio Sampling for a \nWell-Trained Layer\n....\n....\nA Well-T rained Layer\nWithout Sampling\nPL Fitting\nPL Fitting\nLess Heavy TailedMore Heavy TailedOur Method\nPrevious MethodPrediction:\nBadly Trained.\nWrong!Prediction:\nWell Trained.\nCorrect!\nInaccurate  \nPL_Alpha ValuesExample Application:\nLeanring Rate Assignment\n....\n....\n.... ........ ....\nLearning RateGood Assignment\nImbalanced AssignmentLayers assigned with\n¬†inappropriate LR¬†....\n....Well Optimized \nModels\nBadly Optimized \nModelsüòä\nüò¢Figure 1: Comparing FARMS with common HT-SR methods in analyzing the weight matrices of a DNN. We\nuse the weight matrix Wiof a well-trained layer with a large aspect ratio as an example (i.e., Œ≥=m/n is\nmuch larger than 1). Due to the influence of the aspect ratio Œ≥=m/n‚â´1, the ESD is more concentrated\nand less HTed than other layers. As a result, previous methods for fitting a power-law distribution (shown as\n‚ÄúPL Fitting‚Äù) tend to overestimate the heavytailness metric PLAlpha Hill and incorrectly suggest that the\nlayer is poorly trained. Due to this aspect ratio bias, some optimization methods (such as TempBalance )\ncannot accurately assign per-layer hyperparameters, leading to suboptimal training. In contrast, our method,\nFARMS , which conducts ESD analysis with a fixed aspect ratio, accurately measures the training quality of\nthe layer. See a concrete numerical example in Figure 7 in Appendix A.\n2021; Yang et al., 2023). Furthermore, one can use a larger learning rate on the undertrained layers (Zhou\net al., 2024), which provides an empirically successful training method to balance these layers with other,\nmore well-trained ones. This technique has been developed into a general approach to improve test accuracy,\ntraining efficiency, and model interpretability in various applications, such as image classification (Zhou et al.,\n2024), LLM model pruning (Lu et al., 2024), and SciML model training and fine-tuning (Liu et al., 2024,\n2025b).\nThe core of the HT-SR theory lies in the measurement of the heavytailness of ESDs. In this context, we\nidentified an often-overlooked issue in prior studies: weight matrices of different sizes are analyzed using\nthe same HT measuring procedure, even though their spectral distributions can differ in theory . Specifically,\nalgorithms inspired by the HT-SR theory often begin by measuring the ESD of a weight matrix W, that is,\nthe empirical distribution of eigenvalues of the correlation matrix W‚ä§W. The ESD will then be analyzed\nand used for downstream applications such as model selection, training, and compression (Liu et al., 2024;\nLu et al., 2024; Mahoney and Martin, 2019; Martin and Mahoney, 2019, 2021; Zhou et al., 2024). For weight\nmatrices initialized using the i.i.d. random Gaussian elements, it is well-known that the ESD converges\nto the Marchenko‚ÄìPastur (MP) distribution as the dimensions of the matrix grow infinitely large in the\nproportional limit, i.e., when the number of rows mand the number of columns nincrease but the ratio m/n\nstays fixed. The MP distribution depends explicitly on the aspect ratio Œ≥=m/n, since this aspect ratio\ndetermines the limiting bulk range, [(1 ‚àí‚àöŒ≥)2,(1 +‚àöŒ≥)2], and it influences the overall shape of the ESD.\nAs a result, weight matrices of different aspect ratios exhibit distinct shapes in their ESDs, especially when\nthey transition from the i.i.d. initialization to being correlated. Comparing ESDs with different aspect ratios\nŒ≥directly, as is commonly done in prior work, overlooks this dependency on aspect ratio and can lead to\ninaccurate conclusions. Therefore, a careful consideration of the aspect ratio is critical when analyzing or\ncomparing weight matrices, especially in eigenspectrum analysis.\n2\n--- Page 3 ---\nConsidering these challenges, we introduce FARMS (Fixed-Aspect- RatioMatrix Subsampling), a new\nmethod for measuring HT characteristics in HT-SR weight analysis. FARMS analyzes the training quality\nof layers by partitioning each weight matrix into (overlapping) submatrices of fixed aspect ratios and then\ndo an eigenspectrum analysis on the average ESD of these submatrices. This approach enables accurate\ncomputation of HT-SR metrics regardless of variations in matrix aspect ratios, ensuring a robust evaluation\nof layer quality across diverse matrix sizes that one can have in a DNN.\nTo validate the effectiveness of FARMS , we conduct experiments across various application domains,\nincluding CV, SciML, and LLM pruning. Additionally, these experiments are conducted across different\nparameter settings and model architectures. We compare FARMS with several prior HT-SR approaches that\nuse weight eigenspectrum analysis for hyperparameter scheduling (Liu et al., 2024; Lu et al., 2024; Zhou\net al., 2024). We also do weight analysis to measure various post-training and pruned models, making FARMS\nuseful for efficient, compressed models. Our findings demonstrate that models optimized using FARMS exhibit\nlower mean and variation in HT-SR metrics across layers, a sign of good-quality training as reported in\nprior work (Liu et al., 2024; Martin et al., 2021). Our code is available here1. Our key contributions are\nsummarized below.\n‚Ä¢Mitigating Aspect Ratio Bias in Eigenspectrum Analysis .FARMS addresses the aspect ratio\nbias of existing HT-SR eigenspectrum analysis, and it enables better computation of HT metrics on\nvarious DNN models. This improvement is achieved through a subsampling-based HT estimation\nmethod independent of the aspect ratio of weight matrices. In particular, we use a numerical example\nin Figure 6 and multiple real-data experiments in Section 4.3 to demonstrate that FARMS mitigates the\naspect ratio bias in training.\n‚Ä¢Improved Layer-wise Hyperparameter Tuning . Since HT-SR Theory has been recently applied\nto various layer-wise hyperparameter tuning methods, FARMS thus improves these methods by offering a\nmore accurate evaluation of HT metrics. In particular, we conduct experiments on DNN training and\npruning across various model architectures. In CV models like ResNet and VGG, integrating FARMS\ninto learning rate assignments yields improved accuracy compared to TempBalance , a recently proposed\nlayer-wise learning rate scheduling method (Zhou et al., 2024). In LLMs pruning, FARMS improves\nAlphaPruning (Lu et al., 2024), the SOTA method in assigning layer-wise pruning ratios. Specifically,\nFARMS can reduce the perplexity of the LLaMA-13B model from 2029.20 to 413.76 using the magnitude\npruning method at a 0.7 sparsity ratio. As another example in LLaMA-7B, it reduces the perplexity\nfrom 96.02 to 79.42 using the SparseGPT pruning method (Frantar and Alistarh, 2023) at a 0.8 sparsity\nratio. In SciML, FARMS helps scientific models achieve up to a 5.66% error reduction during fine-tuning\ncompared to the HT-SR based method TBSigmoid (Zhou et al., 2024), even at relatively low L2 relative\nerror (L2RE) levels.\n2 Related Work\n2.1 Prior Work on HT-SR Theory\nIn this section, we provide an overview of prior work on HT-SR theory, a framework derived from RMT and\nstatistical physics that is relevant to understanding modern, practical DNNs. HT-SR theory (Martin and\nMahoney, 2019, 2021) originates from RMT but extends well beyond it. The theory was proposed based\non the observation that well-trained, state-of-the-art DNNs often exhibit HTed structures in the ESD of\neach layer. In the meantime, several rigorous theories in SGD relating HT phenomena to generalization\nperformance were established, providing further theoretical support for HT-SR theory (Gurbuzbalaban et al.,\n2021; Hodgkinson and Mahoney, 2021; Hodgkinson et al., 2022; Simsekli et al., 2019, 2020). Building on the\ntheoretical foundation of HT-SR, a model analysis tool called WeightWatcher (Martin and Mahoney, 2021)\nwas developed. Without accessing any training or test data, methods based on HT-SR Theory can be used to\n1https://github.com/HUST-AI-HYZ/FARMS\n3\n--- Page 4 ---\nassess the performance of models across various domains, such as CV and NLP (Liu et al., 2025b; Martin\nand Mahoney, 2021; Martin et al., 2021; Yang et al., 2023).\n2.2 Optimization Methods Based on Eigenspectrum Analysis\nOur paper is mainly motivated by HT-SR theory, focusing on the eigenspectrum analysis of weight matrices.\nOur paper connects to several DNN optimization methods that use eigenspectrum analysis to improve model\nperformance. For example, spectral norm regularization has been applied to improve generalizability of\ntrained models (Farnia et al., 2018; Miyato et al., 2018; Yoshida and Miyato, 2017). Stable rank normalization\n(SRN) (Sanyal et al., 2019), a normalization technique, scales matrices using their stable rank to enhance\ntraining stability in GANs and generalization in DNNs. TE-NAS (Chen et al., 2021) is a training-free neural\narchitecture search framework that identifies high-performing networks by analyzing the neural tangent kernel\nspectrum and input space linear regions.\nAlthough the aforementioned optimization methods based on eigenspectrum analysis have achieved a\ncertain degree of model improvement, they do not provide fine-grained layer-wise optimization within DNNs.\nHowever, eigenspectrum analysis based on HT-SR theory offers a novel perspective by analyzing the shape of\nthe HT ESDs, enabling a more precise estimate of the training quality for each layer. TempBalance (Zhou\net al., 2024) performs eigenspectrum analysis on ESDs of the weight matrices of DNNs to assess the training\nprogress of each layer. It enables a balanced adjustment of the learning rate across layers during training‚Äîan\nimportant parameter that functions as a ‚Äútemperature-like‚Äù parameter within the language of statistical\nmechanics of learning. AlphaPruning (Lu et al., 2024), on the other hand, uses a similar weight analysis\nmethod to evaluate the training quality of each layer in LLMs. It then uses this information to strategically\nallocate layer-wise sparsity ratios to maintain model performance after pruning.\n3 Method\nIn this section, we first discuss the motivation behind our method FARMS , which is to address the aspect\nratio bias of typical HT-SR methods. Then, we describe FARMS in detail and show how FARMS can reduce the\naspect ratio bias.\n3.1 Typical HT-SR Analysis\nBefore introducing FARMS , we first review the commonly adopted procedures in HT-SR weight analysis.\nHT-SR analysis relies on estimating the layer quality based on the HT characteristic of the layer ESDs, which\nis quantified by HT metric PLAlpha . The ESD of a weight matrix is the histogram of eigenvalues. The ESD\nof weight matrices evolves during training, transitioning from a bulk-dominated regime to an HTed regime\n(Martin et al., 2021). The HTed portion can be modeled by a power-law (PL) distribution within an interval\n(Œªmin,Œªmax):\np(Œª)‚àùŒª‚àíŒ±, Œªmin< Œª < Œª max, (1)\nwhere Œ±, the PL exponent, is a critical metric for analyzing training quality. To fit a PL distribution to the\nESD, methods in HT-SR often use the Hill Estimator (Hill, 1975; Liu et al., 2024; Lu et al., 2024; Zhou\net al., 2024)2. For the i-th layer, suppose the weight matrix is Wiand the correlation matrix W‚ä§\niWihas\nascending eigenvalues {Œªi}n\ni=1. The Hill estimator calculates PLAlpha Hill as:\nPLAlpha Hill = 1 +k\n(Pk\ni=1lnŒªn‚àíi+1\nŒªn‚àík), (2)\n2One important point to note is that estimating PLs is inherently a challenging task (Clauset et al., 2009). Previous research\nsuggests using the maximum likelihood estimate (Alstott et al., 2014; Martin and Mahoney, 2021). However, empirical evidence\nindicates that the Hill estimator performs more reliably in DNN optimization applications (Liu et al., 2024; Lu et al., 2024;\nZhou et al., 2024).\n4\n--- Page 5 ---\nwhere kis an adjustable parameter. Changing kessentially changes the lower eigenvalue threshold Œªminfor\n(truncated) PL estimation. Various metrics have been proposed to analyze the properties of ESDs, among\nwhich shape metrics‚Äîthose that characterize the distributional shape of ESDs‚Äîhave been shown to effectively\npredict the training quality of individual layers. The PLAlpha Hill metric is one such shape metrics.\nThere are several metrics used to quantify the structure of ESDs in HT-SR theory. In this work, we\nmainly consider the PLAlpha Hill , which is empirically shown to be effective for training tasks (Liu et al.,\n2024; Lu et al., 2024; Qing et al., 2024; Zhou et al., 2024). In general, undertrained layers in DNNs tend\nto exhibit larger PLAlpha Hill values, whereas well-trained or over-trained layers typically have smaller\nPLAlpha Hill values. This metric can also be used for a comprehensive analysis across a series of DNN\nmodels with similar architectures to find the best model. Well-trained models tend to exhibit a lower average\nPLAlpha Hill across all layers. Moreover, in fine-tuning tasks, a larger amount of data generally results in a\nlower standard deviation (STD) of PLAlpha Hill across the model, indicating a more balanced training\nprogression across different layers (Liu et al., 2024).\n3.2 Rationale of FARMS : Why Typical HT-SR Methods Are Insufficient\nHere, we explain the rationale behind our method FARMS . In Section 3.1, we mentioned that layers exhibiting\nmore HTed ESDs tend to be more well-trained. This observation is the key to using HT-SR methods to\nmeasure model quality. However, beyond the training quality of weights, the aspect ratio of a weight matrix\nalso influences the heavytailness of its ESD. Specifically, in RMT, the Marchenko-Pastur (MP) distribution\ndescribes the limiting behavior of singular values in large rectangular random matrices. According to the\nMP distribution, the ESD of the correlation matrix W‚ä§Wof an i.i.d. Gaussian random matrix Wm√ón\nbecomes more concentrated as the aspect ratio m/n deviates from 1. An example is presented in Figure 2.\nConsequently, weight matrices with different aspect ratios naturally exhibit varying ESD shapes, which can\ninterfere with the quality estimation of model layers. We refer to this issue as the aspect ratio bias in HT-SR\nmethods.\n0 2 4\nEigenvalue / n02Densitym=4000, n=4000\nEmpirical\nMar enko Pastur PDF\n1 2\nEigenvalue / n0.00.5Densitym=4000, n=16000\nEmpirical\nMar enko Pastur PDF\nFigure 2: ESD shapes can be biased by the aspect ratio m/n. The left ESD is more HTed, while the right\nESD is more concentrated. However, this change in the shape of the ESD is entirely due to the different\naspect ratios of these two matrices and is not caused by differences in training quality. See detailed settings\nand supplementary results in Appendix B Figure 8.\nAs another example, consider a well-trained layer with a large aspect ratio, such as the final layer of a\nResNet 18 model trained on the CIFAR 100 dataset. This weight matrix has size 512 √ó100 and a large\naspect ratio of 512 /100. Thus, the ESD may not exhibit an obvious enough HT structure (See a related\nexample in Figure 7 in Appendix A). If previous HT-SR methods are used, such layers may be quantified as\npoorly trained, but that conclusion results from the aspect ratio bias and is an inaccurate assessments of\ntheir training quality. Such misjudgment not only interferes with the evaluation of model performance but\nalso leads to inaccurate tuning of hyperparameters in layer-wise optimization methods (Lu et al., 2024; Zhou\net al., 2024), as these methods all assign different layer-wise hyperparameters based on the HT estimates.\nTherefore, eliminating such aspect ratio bias is crucial for improving HT-SR eigenspectrum analysis.\n5\n--- Page 6 ---\n3.3 Analyzing ESDs Using FARMS\nTo mitigate the aspect ratio bias in analyzing ESDs, we use a block-wise sampling method when processing\nweight matrices. We partition each weight matrix into overlapping sub-matrices with a fixed aspect ratio\n(across all layers) following a predefined scheme described below.\nConsider the weight matrix Wiof the i-th layer, which has a shape of m√ón. Without loss of generality,\nconsider the case when m‚â•n. Note that the shape m√ónchanges across layers. To process this weight\nmatrix, we apply a sliding window approach to partition it into several equally sized submatrices (potentially\nwith overlap), denoted as Wi1,Wi2, . . . ,Wil, where lrepresents the number of submatrices. Each submatrix\nhas a shape of m‚Ä≤√ón‚Ä≤and satisfies a fixed aspect ratio Q=m‚Ä≤/n‚Ä≤. The parameters m‚Ä≤,n‚Ä≤,l, and Qare\ntunable hyperparameters (on which we will provide ablation studies in Section 4.6), allowing flexibility in the\npartitioning strategy. The key requirement is that even if the aspect ratio of the whole weight matrix m/n\nchanges for different layer index i, that of the submatrices Q=m‚Ä≤/n‚Ä≤is fixed across all layers.\nSubsequently, for 2D Linear layers we compute the eigenvalues of the correlation matrices Xij=W‚ä§\nijWij\nfor each submatrix in the i-th layer. We then average the ESDs of Wi1,Wi2, . . . ,Wil(by concatenating their\ncorresponding eigenvalue series) and measure the HT metrics of the averaged ESD instead. We concatenate\nthe eigenvalue series because that‚Äôs equivalent to averaging the empirical densities. The average of the ESDs\nleads to a less variant estimate of HT characteristics.\nFor CNN layers, since they have 4D tensor weight matrices with four dimensions [ C1, C2, kH, kW] (which\nrepresent input channels, output channels, height, and width, respectively), we employ a slightly different\nmethod for subsampling and measuring. We first flatten the two dimensions representing the convolution\nkernel size, [ kH, kW], in the 4D tensor into a single dimension. This results in l‚Ä≤=kH√ókWtwo-dimensional\nmatrices of shape [ C1, C2]. We assume here that C1‚â•C2for convenience. For a 3D tensor of shape\nl‚Ä≤√óC1√óC2, we perform subsampling of size m‚Ä≤√ón‚Ä≤on each C1√óC2matrix corresponding to each l‚Ä≤. This\nresults in [C1\nm‚Ä≤]√ó[C2\nn‚Ä≤]√ól‚Ä≤submatrices, each of size m‚Ä≤√ón‚Ä≤. Next, we average the ESDs by concatenating the\nrooted singular values for each submatrix along the l‚Ä≤dimension, and then measure the HT metrics of the\nESDs. In this way, we obtain [C1\nm‚Ä≤]√ó[C2\nn‚Ä≤] HT metrics values. Finally, we take the average of these metrics\nvalues to obtain the HT metrics corresponding to the 2D CNN. We also considered concatenating all the\nrooted singular values into a single list and calculating the corresponding ESD to measure the degree of\nheavy-tailedness. However, our experimental results in Table 7 in Appendix C.2 show that averaging the\nresults separately leads to better model performance. The detailed procedures of FARMS are shown in Figure 1\nand Figure 3.\n4 Empirical Results\nIn this section, we apply FARMS to measure HT metrics. To demonstrate the effectiveness of the new approach,\nwe use these metrics across various optimization methods and tasks from different machine learning subfields.\nIn Section 4.1, we give full details of the experimental setup. In Section 4.2, we applied FARMS to LLM\nlayer-wise pruning. In Section 4.3, we employ FARMS on training VGGs and ResNets on image classification\ntasks. In Section 4.4, we validate that FARMS achieves better performance in SciML fine-tuning experiments.\nIn Section 4.5, we apply FARMS to analyze model quality, such as the balance of different layers. Finally, we\nperform ablation studies in Section 4.6.\n4.1 Experimental Setup\nDatasets. For image classification, we consider the CIFAR100 dataset (Krizhevsky et al., 2009). CIFAR100\nconsists of 50K pictures for the training set and 10K pictures for the testing set with 100 categories.\nFor evaluating LLM pruning methods, we calculate model perplexity on the held-out WikiText (Merity\net al., 2016) validation set and use seven tasks, including BoolQ (Clark et al., 2019), RTE (Wang, 2018),\nHellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC Easy and Challenge (Clark et al.,\n2018) and OpenbookQA (Mihaylov et al., 2018) for downstream zero-shot evaluation (Gao et al., 2021). For\n6\n--- Page 7 ---\n....\n....Performing SVD\n....\n....\nPL FittingFixed Matrix Sampling \nCalculate HT -SR Metrics Concatenating Eigenvalues Figure 3: Main Steps in FARMS .\nSciML, we fine-tune the models on simulated solutions of time-dependent PDE dataset 2D Compressible\nNavier-Stokes (CFD)3from PDEBench (Takamoto et al., 2022). All datasets considered in this paper are\nstandard and widely studied.\nModels. We consider different types of NNs in various research fields: VGG (Simonyan and Zisserman,\n2014), ResNet (He et al., 2016), OPT (Zhang et al., 2022), LLaMA (Grattafiori et al., 2024; Touvron\net al., 2023a,b) and DPOT (Hao et al., 2024). For VGG series, we consider VGG16 and VGG19. For\nResNet series, we consider ResNet18 and ResNet34. For OPT series, we consider four different model\nsize: OPT-125M/350M/1.3B/6.7B. For LLaMA series, we consider six different moodels: LLaMA-7B/13B,\nLLaMA-V2-7B/13B and LLaMA-V3-3B/8B. For DPOT series, we consider DPOT-Tiny and DPOT-Small.\nBaseline. We considered several model diagnostic and layer-wise hyperparameter scheduling methods as\nbaselines for comparison. TempBalance (Zhou et al., 2024) is a layer-wise learning rate allocation algorithm\nbased on HT-SR theory, designed to analyze the training progress of each layer and allocate learning rates\naccordingly. TempBalance measures the HT metrics of all layers, and it assigns a larger learning rate to weight\nmatrices with a more lighted-tailed ESD. AlphaPruning (Lu et al., 2024), on the other hand, is a layer-wise\npruning method for LLMs based on HT-SR. It assigns a larger pruning ratio to Transformer weight matrices\nwith a more lighted-tailed ESD. Additionally, we compared our method with the TBSigmoid (Liu et al.,\n2024) approach, which was applied to experiments on SciML models. All three optimization methods adopt\nESD analysis techniques, which are susceptible to aspect ratio bias because the HT metrics are measured on\nthe whole weight matrix. Therefore, one can replace the HT measurement procedures in these methods with\nFARMS . The goal is to evaluate the improvement of FARMS compared to the existing HT-SR analysis used in\nthese optimization methods.\n4.2 Improving LLM Pruning with FARMS\nTo explore the effectiveness of FARMS , we conduct experiments in the field of LLM pruning. As we have\nmentioned, we can replace the HT measurement procedures in AlphaPruning with FARMS . Therefore, we can\ncompare it with FARMS and without. We make the comparison with different sparsity ratios in the range\n{0.7, 0.75, 0.8, 0.85 }and different LLM pruning methods, including Magnitude-based (Han et al., 2015),\n3CFD means compressible fluid dynamics or, equivalently, the compressible Navier-Stokes equations.\n7\n--- Page 8 ---\nTable 1: WikiText validation perplexity for pruned LLaMA-7B and LLaMA-13B models at different sparsity\nsettings. Our method is compared to AlphaPruning , each paired with magnitude based pruning, Wanda,\nand SparseGPT. Lower perplexity indicates improved model performance. For calculating the standard\ndeviation(STD) and demonstrating the stability of our method, we sample different calibration sets with 128\nsamples using six different seeds [0, 1, 2, 3, 4, 5] in Wanda and SparseGPT.\nSparsity Ratio Layer-wise LLaMA-7B LLaMA-13B\nMethod Magnitude Wanda SparseGPT Magnitude Wanda SparseGPT\n0.7 AlphaPruning 231.76 24.30 ¬±0.25 18.66 ¬±0.49 2029.20 14.47 ¬±0.08 13.29 ¬±0.17\nOurs 173.49 22.61 ¬±0.18 18.53 ¬±0.40 413.76 14.20 ¬±0.09 13.06 ¬±0.14\n0.75 AlphaPruning 2046.22 104.53 ¬±4.49 36.52 ¬±1.13 2710.49 32.18 ¬±0.31 22.26 ¬±0.59\nOurs 1704.56 71.67 ¬±1.71 35.47 ¬±1.01 2634.82 29.56 ¬±0.33 20.80 ¬±0.43\n0.8 AlphaPruning 28865.67 772.20 ¬±78.70 96.02 ¬±1.59 5399.87 160.59 ¬±4.05 47.57 ¬±2.64\nOurs 12799.58 504.58 ¬±23.05 79.42 ¬±3.86 5026.86 127.49 ¬±2.12 41.44 ¬±1.58\n0.85 AlphaPruning 71710.96 4609.70 ¬±978.39 272.84 ¬±30.84 38140.95 3144.01 ¬±597.79 122.38 ¬±8.88\nOurs 66808.51 3595.54 ¬±810.54 234.46 ¬±16.42 37453.06 2847.85 ¬±368.10 101.06 ¬±4.48\nTable 2: Comparison of mean zero-shot accuracies (%) for pruned LLaMA-7B and LLaMA-13B models at\ndifferent sparsity settings. We evaluate our method against AlphaPruning , each integrated with magnitude-\nbased pruning, Wanda, and SparseGPT. Higher accuracy values indicate better zero-shot ability.\nSparsity Ratio Layer-wise LLaMA-7B LLaMA-13B\nMethod Magnitude Wanda SparseGPT Magnitude Wanda SparseGPT\n0.7 AlphaPruning 35.67 43.67 44.79 38.23 47.46 49.07\nOurs 35.96 44.26 44.84 38.87 47.75 49.61\n0.75 AlphaPruning 34.59 37.99 40.89 38.16 42.73 44.17\nOurs 35.88 39.66 40.93 37.68 42.66 44.47\n0.8 AlphaPruning 33.79 34.06 36.63 35.59 38.42 39.07\nOurs 34.33 35.76 37.50 36.94 39.23 40.18\n0.85 AlphaPruning 33.29 31.69 34.86 33.11 32.05 36.73\nOurs 34.27 33.09 35.25 33.29 32.41 37.04\nSparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023). We use different LLM pruning\nmethods because AlphaPruning assigns layer-wise ratios but not precise pruning locations. We followed\nthe experimental setup from Lu et al. (2024) and our hyperparameter ranges are reported in Table 14 of\nAppendix F.\nLanguage Modeling. The results in Table 1 illustrate that using FARMS helps AlphaPruning achieve\nbetter performance under different sparsity ratios and LLM pruning methods. Specifically, when using the\nMagnitude method, our approach reduces the perplexity of LLaMA-7B from 231.76 to 173.49 at a sparsity\nratio of 0.7. Similarly, when employing more advanced pruning methods such as Wanda and SparseGPT,\nour approach further reduces the perplexity of LLaMA-7B from 96.02 to 79.42 and LLaMA-13B from 47.57\nto 41.44 at a sparsity ratio of 0.8. One notable advantage of AlphaPruning is that it allows pruning LLMs\nto a sparsity ratio of 0.8 without significantly impacting perplexity. With FARMS , this performance can be\nfurther improved. We can also find that FARMS achieves lower STD in most settings. In Table 8 and Table 9\nin Appendix C, we provide additional experiment results on OPT series models and more recent LLaMA\nmodels.\nZero-Shot Tasks. We test the zero-shot ability of pruned LLMs on seven zero-shot downstream tasks\nmentioned in Section 4.1 with prompting. Results are shown in Table 2, where we report the mean zero-shot\naccuracy on seven zero-shot tasks of pruned LLaMA-7B and LLaMA-13B models. FARMS can outperform\n8\n--- Page 9 ---\nAlphaPruning in improving accuracy across most settings. For example, although AlphaPruning achieved\nsignificant improvements over the uniform method, FARMS further improved accuracy by values of 1.11% in\nSparseGPT pruning over AlphaPruning at a sparsity ratio of 0.8. We report the detailed performance for\neach individual task in Table 10 and Table 11 in Appendix C.\n4.3 Improving Image Classification with FARMS\n(a) ResNet 18\n (b) ResNet 34\n (c) VGG 16\n (d) VGG 19\nFigure 4: Comparing our method to TempBalance at different layer usage settings in training ResNet and\nVGG series models on CIFAR100. Higher test accuracy values indicate better model performance. See Table\n13 in Appendix F for the details in all hyperparameters.\nIn this subsection, we compare FARMS toTempBalance on image classification tasks. As we mentioned,\nTempBalance is a layer-wise learning rate assignment method using HT metrics. In Figure 4, we report\nthe evaluation results of training the ResNet and VGG series models under different optimizing settings.\nAgain, we compare TempBalance with or without FARMS . ‚ÄúOurs‚Äù means replacing the weight analysis method\npreviously employed in the TempBalance with FARMS .\nAn important configuration when using the TempBalance method is the exclusion of certain layers from\nthe learning rate assignment process. Instead of receiving layer-wise learning rate adjustments, these layers\nare assigned a global learning rate that decays according to cosine annealing. These excluded layers have ‚Äútall-\nand-skinny matrices,‚Äù which are matrices with a large aspect ratio and a relatively small number of eigenvalues.\nFor example, the final layer of the ResNet 18 model has dimensions of 512 √ó100, resulting in an aspect ratio\nof 512 /100. This kind of extreme size makes it difficult for previous methods to accurately estimate the\nheavytailness of the ESDs. This issue further exacerbates the inability of the previous TempBalance method\nto accurately allocate per-layer learning rates, resulting in certain layers remaining at either excessively high\nor extremely low learning rates. We illustrate this phenomenon in more detail in Figure 12 of Appendix C.\nConsequently, this imbalance leads to suboptimal model performance after training.\nTherefore, according to the experimental setup described in Zhou et al. (2024), the first and last layers\nof ResNet and VGG models, along with certain layers whose correlation matrix of weight matrix contains\na relatively small number of eigenvalues, will be excluded from the layer-wise learning rate scheduling. To\nensure a comprehensive study, we compare FARMS andTempBalance with and without this layer selection (LS)\nheuristic. See Figure 4, when the LS method is not applied (and all layers will be included in the adjustable\nlearning rate scheduling), the performance of the original TempBalance method (shown as ‚ÄúTB (no LS)‚Äù)\ndeteriorates significantly. For example, compared to the LS-enabled setup (shown as ‚ÄúLS+TB‚Äù), the test\naccuracy of the VGG 19 model trained on CIFAR100 without LS using the TempBalance method drops from\n74.19% to 73.22%, while the standard deviation increases from 0.159 to 0.277. In contrast, when training\nmodels using the TempBalance method with FARMS , performance remains stable or even improves when LS is\ndisabled. This observation demonstrates that our method is more robust to weight matrices of extreme sizes,\nand the reason is that our method provides a more accurate assessment of the training progress independent\nof the matrix size.\nInTempBalance algorithm, the layer-wise adjustable learning rate is scaled in the range of ( s1Œ∑t,s2Œ∑t),\nwhere Œ∑tis the global learning rate at time t, and s1ands2are two tunable lower and upper limits of learning\n9\n--- Page 10 ---\n1.0 1.5 2.0 2.5\nPL_Alpha_HillCAL\nTemp-\nBalance\nOurs(a) ResNet 18, Cifar100\n1.0 1.5 2.0 2.5\nPL_Alpha_HillCAL\nTemp-\nBalance\nOurs (b) ResNet 34, Cifar100\n1.0 1.5 2.0 2.5\nPL_Alpha_HillCAL\nTemp-\nBalance\nOurs (c) VGG 16, Cifar100\n1.0 1.5 2.0 2.5\nPL_Alpha_HillCAL\nTemp-\nBalance\nOurs (d) VGG 19, Cifar100\nFigure 5: Comparing the distribution of PLAlpha Hill of ResNet and VGG series models. The top blue-\nshaded section in each subplot of the experiments (shown as ‚ÄùCAL‚Äù) represents models trained without\nemploying layer-wise optimization methods. The middle brown-shaded section in each subplot indicates\nmodels trained using TempBalance ; Lastly, the bottom orange-shaded section in each subplot corresponds\nto the results obtained using FARMS . Figures 5a, 5b, 5c and 5d present the averaged results obtained from\nmultiple experiments within the optimal parameter range.\nrates. To demonstrate that FARMS can more accurately evaluate the training quality of each model layer, we\npresent the test performance of models trained using the different learning rate scaling ratios ( s1,s2). in\nwhich we consider five different settings for ( s1,s2): [(0.1, 1.9), (0.2, 1.8), (0.3, 1.7), (0.4, 1.6), (0.5, 1.5)].\nAdditionally, we consider that these five scaling ratios are generally close to the optimal scaling ratio. We\nrun tasks on CIFAR100 with four VGG and ResNet architectures. Our results in Table 3 show that FARMS\ncan improve the test accuracy of models among the five scaling ratios and help models achieve a lower overall\nstandard deviation compared to TempBalance . Detailed results can be found in Figure 9, Appendix C.\nTable 3: Comparing our method to TempBalance among the five scaling ratios. All results are reported as the\nmean test accuracy and standard deviation obtained across five different scaling ratios. In this experiment,\nwe allowed TempBalance baseline to use the LS heuristic to achieve slightly better test accuracy.\nMethod ResNet 18 ResNet 34 VGG 16 VGG 19\nTB 79.03 ¬±0.169 79.81 ¬±0.145 74.87 ¬±0.214 73.89 ¬±0.199\nOurs 79.35 ¬±0.126 80.07 ¬±0.097 75.16 ¬±0.212 74.03 ¬±0.163\n4.4 Improving SciML Models with FARMS\nTo explore the potential applications of FARMS in multiple domains of machine learning research, we also\nperformed SciML fine-tuning experiments using the 2DCFD dataset with DPOT-Tiny and DPOT-Small\nmodels and compared FARMS with TBSigmoid method (Liu et al., 2024). We followed the experimental setup\nfrom Liu et al. (2024), and our hyperparameter ranges are detailed in Table 14 of Appendix F. In Table 4, we\nshow the results of comparing FARMS with the TBSigmoid (shown as ‚ÄúTB Sig‚Äù) with layer selection and the\nbaseline fine-tuning (shown as ‚ÄúFT‚Äù) with a uniform learning rate for each layer in different data subsampling\nratios in the range {5%, 10%, 25%, 50%, 100% }. Our method consistently outperformed TBSigmoid across\nall subsampling ratios. For instance, with a full dataset (100% of the data), our approach reduced the model‚Äôs\nL2RE to 1.017e-2. Comparing to the TBSigmoid , the L2RE decreased by 5.66%.\n4.5 HT Metrics Analysis\nIn this subsection, we demonstrate that FARMS can effectively control the shape of ESDs, resulting in a\nmore balanced distribution of PLAlpha Hill values among the layers of NNs compared to the previous\nmethods. We assess the models presented in Section 4.3, which include 2D Convolution and 2D Linear\n10\n--- Page 11 ---\nTable 4: FARMS achieves lower L2RE( ‚Üì) on the test dataset than TBSigmoid and baseline fine-tuning method\non SciML tasks.\nSubsampling\nRatio Method DPOT-Tiny DPOT-Small\n5% FT 1.863e-2 1.546e-2\nTBSig 1.856e-2 1.539e-2\nOurs 1.842e-2 1.536e-2\n10% FT 1.747e-2 1.426e-2\nTBSig 1.730e-2 1.415e-2\nOurs 1.706e-2 1.407e-2\n25% FT 1.543e-2 1.226e-2\nTBSig 1.517e-2 1.203e-2\nOurs 1.499e-2 1.189e-2\n50% FT 1.309e-2 1.025e-2\nTBSig 1.283e-2 1.005e-2\nOurs 1.242e-2 9.822e-3\n100% FT 1.096e-2 8.400e-3\nTBSig 1.078e-2 8.193e-3\nOurs 1.017e-2 7.949e-3\nlayers of ResNets and VGG networks. Results in Figure 5 show the distribution of PLAlpha Hill of models\ntrained using baseline cosine annealing (CAL), TempBalance , and TempBalance using our method FARMS . It\ncan be seen that FARMS generally leads to a more concentrated distribution and, in most cases, reduces the\naverage PLAlpha Hill value. These experimental results suggest that our method FARMS enables a more\nbalanced training progression across different layers of the model. These observations align well with the\nfindings reported by Liu et al. (2024); Martin et al. (2021), which suggest that a decrease in both average\nPLAlpha Hill values and the variance across layers indicate better training quality.\n4.6 Ablation Study\nDifferent Aspect Ratios of Weight Matrix. Here, we study the effect of the aspect ratio of the\nsubsampled matrices in FARMS . We consider image classification tasks and use FARMS to replace the HT\nmetrics in TempBalance for assigning layer-wise learning rates. We consider five different aspect ratios in the\nrange {0.5, 0.75, 1.0, 1.5, 2.0 }and keep other hyperparameters optimal. We again use ResNet18 and VGG16\nas our architectures and show results on CIFAR100. Results in Table 5 show that FARMS achieves a higher\ntest accuracy when the aspect ratio is 1.0.\nTable 5: Comparing the different Aspect ratios settings for submatrices on ResNet 18 and VGG 16 models\ntrained on CIFAR100 Dataset.\nQ Ratio 0.5 0.75 1.0 1.5 2.0\nResNet 18 79.13 ¬±0.158 79.33 ¬±0.122 79.53 ¬±0.177 79.13 ¬±0.282 79.31 ¬±0.046\nVGG 16 75.05 ¬±0.285 75.19 ¬±0.517 75.36 ¬±0.118 75.21 ¬±0.345 75.10 ¬±0.360\nSubsampling Window Sizes and Sampling Steps. Here, we study different subsampling window\nsizes and sampling steps (i.e., the number of subsampled submatrices) in the task of LLaMA-7B pruning.\n11\n--- Page 12 ---\nThe aspect ratio Qis fixed to be 1.0. For all weight matrices in LLaMA-7B, the smallest dimension is 4096.\nTherefore, when using a window size of 4096, sliding window downsampling is applied only along the larger\ndimension. This process generates multiple 4096 √ó4096 submatrices. We report our results in Table 6. We\nfind that using an appropriately sized sampling window, such as 2000 √ó2000, the model achieves a lower\nperplexity. As a comparison, baseline perplexity achieved by AlphaPruning without using FARMS is 96.02.\nTable 6: Different Window Size and Sampling Steps for pruning the LLaMA-7B at 0.8 sparsity ratio. The\npruning method is SparseGPT. The model is evaluated on WikiText dataset with perplexity ( ‚Üì).\nSampling Steps\nWindow size 5 10 15 20\n500 96.34 ¬±9.15 95.93 ¬±2.80 99.23 ¬±3.53 88.08 ¬±4.23\n1000 84.71 ¬±4.97 81.96 ¬±4.22 89.20 ¬±3.91 84.04 ¬±2.48\n2000 82.33 ¬±3.18 79.42 ¬±3.86 80.14 ¬±2.32 86.54 ¬±3.28\n4096 82.63 ¬±2.45 89.61 ¬±5.25 84.19 ¬±6.05 84.07 ¬±5.64\n5 Conclusion\nWe propose a subsampling strategy to address the measurement bias introduced by varying aspect ratios of\nweight matrices in HT-SR eigenspectrum analysis. The main idea of our method is to extract submatrices\nof the original matrix with a fixed aspect ratio, followed by averaging the ESDs of these submatrices to\naccurately assess the training quality of each weight matrix. Our extensive experiments demonstrate that our\nmethod can precisely estimate layer-wise training quality, improve the performance of layer-wise optimization\nalgorithms, and contribute to more balanced training and efficient pruning. Furthermore, results on visual\nanalytics confirm the effectiveness of our approach, showing that it successfully eliminates the measurement\nbias caused by aspect ratio variations.\nImpact Statement\nThis paper presents research aimed at advancing the fields of Machine Learning and Deep Learning, particularly\nin training using information from eigenspectrum analysis. While our work has various potential societal\nimplications, we do not find it necessary to highlight any specific ones here.\nAcknowledgments\nThis work is supported by DOE under Award Number DE-SC0025584 and Dartmouth College.\nReferences\nBen Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a\nmulti-scale theory of generalization. In International Conference on Machine Learning , pages 74‚Äì84. PMLR,\n2020.\nJeff Alstott, Ed Bullmore, and Dietmar Plenz. powerlaw: a python package for analysis of heavy-tailed\ndistributions. PloS one , 9(1):e85777, 2014.\n12\n--- Page 13 ---\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional\nasymptotics of feature learning: How one gradient step improves the representation. Advances in Neural\nInformation Processing Systems , 35:37932‚Äì37946, 2022.\nZhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices , volume 20.\nSpringer, 2010.\nWuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours:\nA theoretically inspired perspective. arXiv preprint arXiv:2102.11535 , 2021.\nXiong-Hui Chen, Ziyan Wang, Yali Du, Shengyi Jiang, Meng Fang, Yang Yu, and Jun Wang. Policy learning\nfrom tutorial books via understanding, rehearsing and introspecting. In Advances in Neural Information\nProcessing Systems , volume 37, 2024.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044 ,\n2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457 , 2018.\nAaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical data.\nSIAM review , 51(4):661‚Äì703, 2009.\nRomain Couillet and Zhenyu Liao. Random Matrix Methods for Machine Learning . Cambridge University\nPress, 2022.\nMichal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions for double descent and\nimplicit regularization via surrogate random design. Advances in neural information processing systems ,\n33:5152‚Äì5164, 2020.\nEdgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and\nclassification. The Annals of Statistics , 46(1):247‚Äì279, 2018.\nFarzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral normalization.\narXiv preprint arXiv:1811.07457 , 2018.\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot.\nInInternational Conference on Machine Learning , pages 10323‚Äì10337. PMLR, 2023.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation.\nVersion v0. 0.1. Sept , 10:8‚Äì9, 2021.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 , 2024.\nThomas Guhr, Axel M¬® uller‚ÄìGroeling, and Hans A. Weidenm¬® uller. Random-matrix theories in quan-\ntum physics: common concepts. Physics Reports , 299(4):189‚Äì425, June 1998. ISSN 0370-1573.\ndoi: 10.1016/S0370-1573(97)00088-4. URL https://www.sciencedirect.com/science/article/pii/\nS0370157397000884 .\nMert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In International\nConference on Machine Learning , pages 3964‚Äì3975. PMLR, 2021.\n13\n--- Page 14 ---\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural\nnetwork. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems , volume 28. Curran Associates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf .\nZhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian\nSong, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training.\narXiv preprint arXiv:2403.03542 , 2024.\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional\nridgeless least squares interpolation. Annals of statistics , 50(2):949, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-\nlevel performance on imagenet classification. In Proceedings of the IEEE international conference on\ncomputer vision , pages 1026‚Äì1034, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pages 770‚Äì778, 2016.\nBruce M Hill. A simple general approach to inference about the tail of a distribution. The annals of statistics ,\npages 1163‚Äì1174, 1975.\nLiam Hodgkinson and Michael Mahoney. Multiplicative noise and heavy tails in stochastic optimization. In\nInternational Conference on Machine Learning , pages 4262‚Äì4274. PMLR, 2021.\nLiam Hodgkinson, Umut Simsekli, Rajiv Khanna, and Michael Mahoney. Generalization bounds using lower\ntail exponents in stochastic optimizers. In International Conference on Machine Learning , pages 8774‚Äì8795.\nPMLR, 2022.\nHong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features. IEEE\nTransactions on Information Theory , 69(3):1932‚Äì1964, 2022.\nVignesh Kothapalli, Tianyu Pang, Shenyang Deng, Zongmin Liu, and Yaoqing Yang. Crafting heavy-tails in\nweight matrix spectrum without gradient noise, 2024. URL https://arxiv.org/abs/2406.04657 .\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nLian Liu, Xiandong Zhao, Guanchen Li, Dong Li, Wang, Yinhe Han, Xiaowei Li, and Ying Wang. BaWA:\nAutomatic optimizing pruning metric for large language models with balanced weight and activation. In\nProceedings of the 42nd International Conference on Machine Learning , ICML ‚Äô25. PMLR, 2025a. URL\nhttps://icml.cc/virtual/2025/poster/44892 .\nZihang Liu, Yuanzhe Hu, Tianyu Pang, Yefan Zhou, Pu Ren, and Yaoqing Yang. Model balancing helps\nlow-data training and fine-tuning. arXiv preprint arXiv:2410.12178 , 2024.\nZihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, and\nShiwei Liu. Lift the veil for the truth: Principal weights emerge after rank reduction for reasoning-focused\nsupervised fine-tuning. In Proceedings of the 42nd International Conference on Machine Learning (ICML) ,\nVancouver, Canada, July 2025b. URL https://icml.cc/virtual/2025/poster/44967 .\nHaiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W Mahoney, and Yaoqing Yang. Alphaprun-\ning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large language models.\narXiv preprint arXiv:2410.10912 , 2024.\nMichael Mahoney and Charles Martin. Traditional and heavy tailed self regularization in neural network\nmodels. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\nConference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages 4284‚Äì4293.\nPMLR, 09‚Äì15 Jun 2019. URL https://proceedings.mlr.press/v97/mahoney19a.html .\n14\n--- Page 15 ---\nCharles H Martin and Michael W Mahoney. Traditional and heavy-tailed self regularization in neural network\nmodels. arXiv preprint arXiv:1901.08276 , 2019.\nCharles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence\nfrom random matrix theory and implications for learning. Journal of Machine Learning Research , 22(165):\n1‚Äì73, 2021.\nCharles H. Martin, Tongsu Peng, and Michael W. Mahoney. Predicting trends in the quality of state-of-the-art\nneural networks without access to training or testing data. Nature Communications , 12(1), July 2021. ISSN\n2041-1723. doi: 10.1038/s41467-021-24025-8. URL http://dx.doi.org/10.1038/s41467-021-24025-8 .\nSong Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics\nand the double descent curve. Communications on Pure and Applied Mathematics , 75(4):667‚Äì766, 2022.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843 , 2016.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?\na new dataset for open book question answering. arXiv preprint arXiv:1809.02789 , 2018.\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative\nadversarial networks, 2018. URL https://arxiv.org/abs/1802.05957 .\nRaj Rao Nadakuditi and Mark EJ Newman. Graph spectra and the detectability of community structure in\nnetworks. Physical review letters , 108(18):188701, 2012.\nJeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances\nin Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https:\n//proceedings.neurips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html .\nPeijun Qing, Chongyang Gao, Yefan Zhou, Xingjian Diao, Yaoqing Yang, and Soroush Vosoughi. Alphalora:\nAssigning lora experts based on layer training quality. arXiv preprint arXiv:2410.10054 , 2024.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM , 64(9):99‚Äì106, 2021.\nAmartya Sanyal, Philip HS Torr, and Puneet K Dokania. Stable rank normalization for improved generalization\nin neural networks and gans. arXiv preprint arXiv:1906.04659 , 2019.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 , 2014.\nUmut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in\ndeep neural networks. In International Conference on Machine Learning , pages 5827‚Äì5837. PMLR, 2019.\nUmut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension, heavy tails,\nand generalization in neural networks. Advances in Neural Information Processing Systems , 33:5138‚Äì5151,\n2020.\nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large\nlanguage models. arXiv preprint arXiv:2306.11695 , 2023.\nMakoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pfl¬® uger,\nand Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances in\nNeural Information Processing Systems , 35:1596‚Äì1611, 2022.\nTerence Tao. Topics in random matrix theory , volume 132. American Mathematical Soc., 2012.\n15\n--- Page 16 ---\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth¬¥ ee Lacroix,\nBaptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\nAntonia M. Tulino and Sergio Verd¬¥ u. Random Matrix Theory and Wireless Communications. Foundations\nand Trends ¬Æin Communications and Information Theory , 1(1):1‚Äì182, June 2004. ISSN 1567-2190, 1567-\n2328. doi: 10.1561/0100000001. URL https://www.nowpublishers.com/article/Details/CIT-001 .\nPublisher: Now Publishers, Inc.\nAlex Wang. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461 , 2018.\nZhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral Evolution and\nInvariance in Linear-width Neural Networks, November 2023. URL http://arxiv.org/abs/2211.06506 .\narXiv:2211.06506 [cs].\nZiyan Wang, Meng Fang, Tristan Tomilin, Fei Fang, and Yali Du. Safe multi-agent reinforcement learning\nwith natural language constraints. arXiv preprint arXiv:2405.20018 , 2024.\nZiyan Wang, Zhicheng Zhang, Fei Fang, and Yali Du. M3hf: Multi-agent reinforcement learning from\nmulti-phase human feedback of mixed quality. In The Twelfth International Conference on Learning\nRepresentations , 2025.\nGreg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In\nInternational Conference on Machine Learning , pages 11727‚Äì11737. PMLR, 2021.\nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub\nPachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot\nhyperparameter transfer. arXiv preprint arXiv:2203.03466 , 2022.\nJinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, and Fei Wu. Mitigating the backdoor effect for\nmulti-task model merging via safety-aware subspace. arXiv preprint arXiv:2410.13910 , 2024.\nShuo Yang, Siwen Luo, and Soyeon Caren Han. Multimodal commonsense knowledge distillation for visual\nquestion answering (student abstract). In Proceedings of the AAAI conference on artificial intelligence ,\npages 29545‚Äì29547, 2025a.\nShuo Yang, Siwen Luo, Soyeon Caren Han, and Eduard Hovy. Magic-vqa: Multimodal and grounded inference\nwith commonsense knowledge for visual question answering. arXiv preprint arXiv:2503.18491 , 2025b.\nYaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H.\nMartin, and Michael W. Mahoney. Test accuracy vs. generalization gap: Model selection in nlp without\naccessing training or testing data. KDD ‚Äô23, page 3011‚Äì3021, New York, NY, USA, 2023. Association for\nComputing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599518. URL https://doi.org/10.\n1145/3580305.3599518 .\nLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, Ajay Jaiswal, Mykola\nPechenizkiy, Yi Liang, et al. Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning\nllms to high sparsity. arXiv preprint arXiv:2310.05175 , 2023.\nYuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep\nlearning, 2017. URL https://arxiv.org/abs/1705.10941 .\n16\n--- Page 17 ---\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068 , 2022.\nYefan Zhou, Tianyu Pang, Keqin Liu, Michael W Mahoney, Yaoqing Yang, et al. Temperature balancing,\nlayer-wise weight analysis, and neural network training. Advances in Neural Information Processing Systems ,\n36, 2024.\nDidi Zhu, Yibing Song, Tao Shen, Ziyu Zhao, Jinluan Yang, Min Zhang, and Chao Wu. Remedy: Recipe\nmerging dynamics in large vision-language models. In The Thirteenth International Conference on Learning\nRepresentations , 2025.\n17\n--- Page 18 ---\nAppendix\nA Detailed Matrix Analysis\nA.1 Random Initialization\nIn this section, we do weight analysis for ResNet 34 and VGG 19 models that are randomly initialized using\nmethods proposed in He et al. (2015). The weights ware drawn from a normal distribution with mean 0 and\nvariance 2 /nin, where ninrepresents the number of input neurons. This is represented as:\nw‚àº N\u0012\n0,2\nnin\u0013\nWe present our results of comparing FARMS with the previous HT estimation methods in measuring\nrandomly initialized models in Figure 6. In Figure 6a and Figure 6c, all layers in these models should be\nsimilarly under-trained (because they are all just initialized). However, previous methods tend to report\nsignificantly higher values for PLAlpha Hill in certain layers, suggesting that these layers are much more\nunder-trained. In contrast, FARMS produces results where the training quality of all initialized layers is nearly\nidentical, aligning more closely with the expected result. Similarly, in Figure 6b and Figure 6d, for randomly\ninitialized ResNet-34 and VGG-19 models with varying widths, the previous method measures PLAlpha Hill\nvalues that increase as model width increases. In contrast, FARMS shows PLAlpha Hill values that remain\nunaffected by the aspect ratio bias introduced by model width variations.\n0 10 20 30\nLayer Index2468PL_Alpha_Hill\nOurs\nPrevious Hill Estimator\n(a) ResNet 34, All Layers\n0.5 1 1.5 2.0 3.0\nWiden Factor2345PL_Alpha_Hill\nOurs\nPrevious Hill Estimator (b) ResNet 34, Width\n0 5 10 15\nLayer Index246810PL_Alpha_Hill\nOurs\nPrevious Hill Estimator (c) VGG 19, All Layers\n0.5 1 1.5 2.0 3.0\nWiden Factor23456PL_Alpha_Hill\nOurs\nPrevious Hill Estimator (d) VGG 19, Width\nFigure 6: Comparing FARMS and previous HT-SR methods for measuring the randomly initialized ResNet\n34 and VGG 19 weights. Figure 6a and Figure 6c show the PLAlpha Hill values for each layer in models\n(widen factor is 1.0) by using different methods. Figure 6b and Figure 6d show the measured PLAlpha Hill\nvalues of the final linear layer across different model width factors. As the width increases, the aspect ratio of\nthe weight matrix also becomes larger, leading to bias in previous HT-SR methods.\nA.2 Mitigating Aspect Ratio Bias in Specific Layers\nIn this section, we do weight analysis for the last layer from the ResNet 34 and VGG 16 models trained\non CIFAR100. These models are trained with hyperparameters according to the Appendix F. The weight\nmatrices of the final layers in both models have dimensions of 512 √ó100 (aspect ratio = 5.12). This means\nthat the previous weight matrix analysis method will be significantly affected by aspect ratio bias, leading to\ninaccurate assessments of the layer‚Äôs training quality.\nIn Figure 7, the presented experimental results validate this observation. Figure 7a and Figure 7c show the\nESD fitting results obtained using the previous weight analysis method, where the measured PLAlpha Hill\nvalues are larger than those typically observed in well-trained layers. This leads to the erroneous classification\nof these layers as poorly trained, ultimately affecting both model performance evaluation and optimization.\nIn contrast, Figure 7b and Figure 7d present the ESD fitting results obtained using FARMS . Our method\n18\n--- Page 19 ---\nproduces measurements that align more closely with the expected ESD of well-trained layers, providing a\nmore accurate assessment of training quality.\n100\nEigenvalues101\n100ESDPL_Alpha_Hill: 3.091\n(a) Previous, ResNet 34\n102\n101\n100\nEigenvalues101\n100101ESDPL_Alpha_Hill: 1.899 (b)FARMS , ResNet 34\n100\nEigenvalues101\n100ESDPL_Alpha_Hill: 2.951 (c) Previous, VGG 16\n102\n100\nEigenvalues101\n100101ESDPL_Alpha_Hill: 1.905 (d)FARMS , VGG 16\nFigure 7: Comparing FARMS and the previous method for measuring the ESD of final layers in ResNet 34 and\nVGG 16 trained on CIFAR 100.\nB Marchenko‚ÄìPastur Distribution\n0 2 4\nEigenvalue / n02Densitym=4000, n=4000\nEmpirical\nMar enko Pastur PDF\n0 2\nEigenvalue / n01Densitym=4000, n=5000\nEmpirical\nMar enko Pastur PDF\n0 2\nEigenvalue / n0.00.51.0Densitym=4000, n=6000\nEmpirical\nMar enko Pastur PDF\n0 2\nEigenvalue / n0.00.5Densitym=4000, n=8000\nEmpirical\nMar enko Pastur PDF\n1 2\nEigenvalue / n0.00.5Densitym=4000, n=12000\nEmpirical\nMar enko Pastur PDF\n1 2\nEigenvalue / n0.00.5Densitym=4000, n=16000\nEmpirical\nMar enko Pastur PDF\nFigure 8: The Marchenko-Pastur (MP) Law for different values of n. The ESD (blue histogram) of the\neigenvalues of the sample covariance matrix is compared with the theoretical Marchenko-Pastur probability\ndensity function (red curve) for various aspect ratios Œ≥=m/n, where m= 4000 is fixed, and nvaries from\n4000 to 16000.\nIn RMT, the Marchenko-Pastur distribution, also known as the Marchenko-Pastur law, characterizes the\nasymptotic behavior of singular values of large rectangular random matrices.\nLetXij,1‚â§i‚â§m,1‚â§j‚â§n, be independent random variables with EXij= 0 and EX2\nij= 1, and\nXm= (Xij)1‚â§i‚â§m,1‚â§j‚â§n. Denote by Œª1‚â§ ¬∑¬∑¬∑ ‚â§ Œªmthe eigenvalues of the symmetric matrix\nW:=Wm:=1\nnXmX‚ä§\nm\n19\n--- Page 20 ---\nand defined its empirical distribution by\nFm(x) =1\nmmX\nk=1I{Œªk‚â§x},\nwhere I{B}denotes the indicator of an event B. One often investigates the rate of convergence of the expected\nspectral distribution EFm(x) as well as Fm(x) to the Marchenko‚ÄìPastur distribution function Fy(x) with\ndensity\nfy(x) =1\n2xyœÄp\n(b‚àíx)(x‚àía)I{[a,b]}(x) +I{[1,‚àû)}(y)(1‚àíy‚àí1)Œ¥(x),\nwhere y=m/n,y‚àà(0,‚àû) and a= (1‚àí‚àöy)2, b= (1 +‚àöy)2. Here we denote by Œ¥(x) the Dirac delta\nfunction and by I{[a,b]}(x) the indicator function of the interval [ a, b].\nWe visualize the MP distribution and the ESD of the weight matrices in Figure 8. In this figure, we\ncan observe that the empirical distribution converges to the theoretical MP distribution. Additionally, as n\nincreases, the ESD distribution exhibits an increasingly concentrated shape with a reduced degree of HT.\nTherefore, ignoring the impact of aspect ratio and directly measuring the HT degree of the ESD to estimate\na layer‚Äôs training quality may lead to inaccurate results.\nC Additional Experiment Results\nC.1 Detailed Performance on Different Scaling Ratios\nWe provide detailed results of a hyperparameter study on learning rate scaling ratio ( s1, s2). We set other\nhyperparameters (including layer selection, initial learning rate, and so on) as the optimal value for each\ntraining set. The results in Figure 9 show that across all tested architectures and most scaling ratios, FARMS\nconsistently outperforms TempBalance . This demonstrates that FARMS provides a more accurate measurement\nof each layer‚Äôs training quality. As a result, it helps the model achieve better training performance when\nusing different learning rate scaling ratios.\n(a) ResNet 18, CIFAR 100\n (b) ResNet 34, CIFAR 100\n (c) VGG 16, CIFAR 100\n (d) VGG 19, CIFAR 100\nFigure 9: Comparison of test accuracy across different architectures and learning rate scaling ranges. The\nfigure presents the test accuracy (%) of TempBalance (blue, dotted) and FARMS (orange, hatched). These two\nmethods are evaluated on the CIFAR 100 dataset using ResNet and VGG series architectures. The x-axis\nrepresents different learning rate scaling ranges ( s1, s2), while the y-axis indicates test accuracy. Error bars\ndenote standard deviations across multiple runs.\nC.2 Comparison of Different CNN Processing Methods\nWe compared two different methods for processing the ESD of CNNs. The first method concatenates the\nrooted singular values of all submatrices and then measures the HT metrics of the resulting ESD. The\n20",
  "text_length": 64602
}