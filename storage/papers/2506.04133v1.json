{
  "id": "http://arxiv.org/abs/2506.04133v1",
  "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
  "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
  "authors": [
    "Shaina Raza",
    "Ranjan Sapkota",
    "Manoj Karkee",
    "Christos Emmanouilidis"
  ],
  "published": "2025-06-04T16:26:11Z",
  "updated": "2025-06-04T16:26:11Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04133v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04133v1  [cs.AI]  4 Jun 2025TRISM FOR AGENTIC AI: A R EVIEW OF TRUST , RISK,AND\nSECURITY MANAGEMENT IN LLM- BASED AGENTIC\nMULTI -AGENT SYSTEMS\nShaina Raza1∗, Ranjan Sapkota2∗, Manoj Karkee2, Christos Emmanouilidis3\n1Vector Institute, Toronto, Canada\n2Cornell University, USA\n3University of Groningen, Netherlands\nABSTRACT\nAgentic AI systems, built on large language models (LLMs) and deployed in multi-agent configura-\ntions, are redefining intelligent autonomy, collaboration and decision-making across enterprise and\nsocietal domains. This review presents a structured analysis of Trust, Risk, and Security Management\n(TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining\nthe conceptual foundations of agentic AI, its architectural differences from traditional AI agents,\nand the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the\nagentic AI framework is then detailed through four pillars governance, explainability, ModelOps,\nand privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and\nintroduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies\nillustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability strategies in distributed\nLLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered\nperformance are reviewed alongside open benchmarking challenges. Security and privacy are ad-\ndressed through encryption, adversarial defense, and compliance with evolving AI regulations. The\npaper concludes with a roadmap for responsible agentic AI, proposing research directions to align\nemerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent\ndeployment.\nKeywords Agentic AI, LLM-based Multi-Agent Systems, TRISM, AI Governance, Explainability, ModelOps,\nApplication Security, Model Privacy, Autonomous Agents, Trustworthy AI, Risk Management, AI Safety,\nPrivacy-Preserving AI, Adversarial Robustness, Human-in-the-Loop\n1 Introduction\n1.1 Background and Motivation\nThe global market for AI agents is projected to grow from $5.4 billion in 2024 to $7.6 billion in 2025 [ 1]. By mid-2025,\nover 70% of enterprise AI deployments are expected to involve multi-agent or action-based systems, reflecting a\ndramatic shift from traditional single-agent or rule-based conversational models [ 2]. An AI agent , typically defined as a\ncomputational entity that perceives its environment and takes actions to achieve goals [ 3], has undergone a fundamental\ntransformation. Whereas early agents were task-specific and deterministic, modern systems, referred to as Agentic\nAI, now integrate large language models (LLMs), tools, and persistent memory to support complex planning and\ncoordination [4, 5].\n*Shaina Raza and Ranjan Sapkota contributed equally and are corresponding authors.\nEmails: shaina.raza@torontomu.ca ,rs2672@cornell.edu ,mk2684@cornell.edu ,c.emmanouilidis@rug.nl .\n--- Page 2 ---\nAPREPRINT\nFrom isolated agents to Agentic AI .The autonomous software agents date back to the 1990s [ 6], the advent of\nLLM -driven multi -agent ecosystems [ 7] since 2023 has made “Agentic AI” a distinctly new and rapidly expanding\nresearch focus [ 4]. Traditional agents automated narrow tasks, such as information retrieval [ 8,9], data summarization\n[10,11,12], and dialogue response [ 13]. Driven by single step logic [ 14] or scripted rules, they lacked deep reasoning,\nadaptability, and persistence [15, 16].\nIn contrast, Agentic AI consists of collaborative agents with specialized roles (e.g., planner, coder, analyst), enabled by\nLLMs and tool use [ 17]. These systems dynamically decompose tasks, share context, and pursue high-level goals across\nlong timelines [ 18,19,20]. This shift represents not just a technological upgrade but a qualitative leap in complexity\nand autonomy, marking the emergence of machine collectives capable of emergent, decentralized behavior.\nTrust, risk, and security challenges. However, this evolution introduces serious challenges. Unlike deterministic\nagents [ 21], Agentic AI systems can produce non-linear, opaque decisions, increasing the risks of failure, bias, and\nunintended consequences. For example, a multi-agent supply chain optimizer may autonomously coordinate between\nprocurement and logistics agents, yet inadvertently leak sensitive information or violate compliance protocols if\nsafeguards are absent [22].\nTraditional evaluation and safety frameworks, built for static or single-function AI , are no longer sufficient. This\nunderscores the urgent need for a new paradigm that integrates trust, risk, and security as core design principles.\nTo address this gap, recent frameworks like AI TRiSM (Trust, Risk, and Security Management) [ 23,24] propose\nlifecycle-level controls, including explainability, secure model orchestration, and privacy management. These are\nessential for deploying agentic systems in high-stakes domains such as finance, healthcare, and defense.\nProblem Statement Agentic AI systems now coordinate multiple LLM -powered agents, yet no unified framework\nexists for managing their trust, risk, and security (TRiSM). High -profile lapses, e.g., an autonomous research agent\nciting unverifiable medical sources or a forecasting swarm drifting off -target due to memory inconsistencies show that\ntraditional safeguards do not scale to dynamically evolving, multi -agent workflows. A structured TRiSM perspective is\ntherefore crucial to mitigate orchestration failures, collusion, data leakage, and opaque decision paths. This review\ndistills current findings and outlines practical guidelines to build secure, reliable, and trustworthy Agentic AI ecosystems.\nScope and Objectives This survey maps the TRiSM (Trust, Risk, and Security Management) landscape for\nLLM -powered, multi -agent “agentic” AI. We frame four pillars explainability, security, lifecycle governance, and pri-\nvacy and show why safeguards designed for bounded, rule -based agents cannot contain open -world, self -orchestrating\ncollectives [ 4,17]. Our goal is to give researchers and practitioners a concise, unified reference for building and\ngoverning trustworthy Agentic AI systems.\nNecessity of this Survey Agentic AI research often concentrates on agent modelling or task coordination and rarely\ntackles adversarial threats, lifecycle governance, or large -scale explainability. As deployments reach high -stakes arenas\nhealthcare, science, finance the absence of a unified TRiSM framework exposes stakeholders to opaque reasoning and\nunmanaged risks. This survey closes that gap by synthesising TRiSM principles for LLM -driven multi -agent systems\nand providing actionable guidance for researchers, engineers, and policy makers. A comparison with related surveys\nwill be presented in Table 1.\n1.2 Contributions\nIn this survey, we address the urgent need for a unified governance framework for LLM-based Agentic AI systems. Our\nframework is shown in Figure 1. Our key contributions are as follows:\n•A Conceptual TRISM Framework Tailored to Agentic AI We introduce a structured Trust, Risk, and Security\nManagement (TRISM) framework specifically designed to accommodate the unique characteristics of multi-agent,\nLLM-driven systems. This framework defines four pillars: Explainability, ModelOps, Application Security, and\nModel Privacy, and explains how each pillar addresses the challenges of autonomy, coordination, and persistent\nmemory in Agentic AI.\n•A Threat Taxonomy for LLM-Powered Agents We develop a detailed taxonomy of risks and threat vectors\nthat arise when multiple LLM agents collaborate. By categorizing vulnerabilities, such as prompt injection,\nmemory poisoning, collusive failures, and emergent misbehavior, we clarify how attack surfaces expand in agentic\nenvironments and why traditional LLM security measures are insufficient.\n•A Comparative Evaluation of Existing Methods We survey and compare state-of-the-art techniques for each\nTRISM pillar as applied to Agentic AI. For example, we examine how explainable-AI methods (e.g., LIME, SHAP,\ndecision provenance) have been adapted for multi-agent workflows, how ModelOps best practices must be extended\nto manage multiple LLM instances, and which security/hardening measures (e.g., prompt hygiene, sandboxing,\naccess controls) are most effective.\n2\n--- Page 3 ---\nAPREPRINT\nTable 1: Comparison of Related Surveys on LLM-based Multi-Agent Systems and TRiSM Aspects\nSurvey Adversarial\nThreatsLifecycle\nGovernanceLarge-Scale\nExplainabilityTRiSM\nIntegrationLLM-\nSpecificApplication\nDomainsActionable\nGuidance\nGuo et al. (2024) [25] ✗ ✗ ✗ ✗ ✓✓\n(simulated env’ts)~\n(research\nchallenges)\nChen et al. (2025) [26] ✗ ✗ ✗ ✗ ✓~\n(task/simulation\nfocus)~\n(future research\nareas)\nYan et al. (2025) [27] ✓ ✗ ✗ ✗ ✓✓\n(mentions\ndiverse)~\n(future work\ndirections)\nTran et al. (2025) [28] ✗ ✗ ✗ ✗ ✓✓\n(networks, QA,\netc.)~\n(open challenges)\nLin et al. (2025) [29] ✗ ✗ ✗ ✗ ✓~\n(creative tasks)~\n(roadmap for\nresearch)\nFang et al. (2025) [30] ✓ ✗ ✗ ~ ~✓\n(health, finance\ncited)~\n(technical “next\nsteps”)\nThis Survey (2025) ✓ ✓ ✓ ✓ ✓✓\n(high-stakes\ndomains)✓\n(practical\nguidance)\nLegend: ✓= explicitly addressed; ~ = partially/indirectly addressed; ✗= not addressed.\n•Future Research Directions for Trustworthy, Secure Agentic AI Building on our TRISM analysis, we outline\nconcrete directions for next-generation research and setup roadmap items aim to guide researchers and practitioners\ntoward robust, scalable, and compliant deployments of Agentic AI.\nThis study provide a unified reference for researchers, engineers, and policymakers who seek to design, evaluate, and\ngovern LLM-based Agentic AI systems in high-stakes domains.\n1.3 Paper Organization\nThe remainder of this paper is structured as follows. In Section 2, we present our literature-search methodology,\nincluding databases queried, inclusion criteria, and classification strategies. Section 3 reviews the fundamental\ncharacteristics and typical architectures of Agentic AI systems, establishing the groundwork for subsequent TRISM\nanalysis. Section 4 introduces the TRISM framework and its four core pillars : Explainability, ModelOps, Application\nSecurity, and Model Privacy. Building on this foundation, Section 5 identifies the primary risks, vulnerabilities,\nand attack surfaces inherent in multi-agent AI ecosystems. In Section 6, we explore trust-building strategies and\nspecialized explainability techniques tailored to Agentic AI, while Section 7 details advanced security mechanisms\nand privacy-preserving methods. Section 8 synthesizes our findings and outlines future research directions and policy\nrecommendations. Section 9 concludes the paper. A list of key terms used throughout this paper is provided in Table 9.\n1.4 Related Work\nLLM-Based Multi-Agent Surveys (Technical Focus): Prior surveys on LLM-driven multi-agent systems primarily\nemphasize system architectures, agent capabilities, and domain-specific applications of agentic AI, but often overlook\ncritical TRiSM considerations. For instance, existing reviews [ 25,26] focus on simulated environments, inter-agent\ncommunication, and performance benchmarks, yet fail to address adversarial threats, lifecycle governance, or system-\nwide explainability. Other targeted surveys, such as [ 27] on natural language communication and [ 28] on collaboration\nmechanisms, explore interaction protocols and use cases in domains like QA systems, 5G, and Industry 5.0. While the\nformer briefly acknowledges security concerns and the latter highlights deployment contexts, both narrowly center on\ncoordination strategies without examining robustness, governance structures, or explainability at scale.\nTrustworthy/Responsible AI Surveys (Broad Trust Focus): Surveys in the TRiSM domain address ethical and risk-\nrelated concerns such as alignment, fairness, and privacy attacks [ 30], but typically overlook explainability, governance,\nand multi-agent dynamics. While they offer actionable recommendations for general ML systems, they do not account\nfor LLM-based coordination in high-stakes domains.\nCurrent Survey (LLM Multi-Agent + TRiSM in High-Stakes Domains): This survey uniquely integrates TRiSM\nprinciples into the analysis of LLM-based multi-agent systems, specifically targeting high-stakes domains such as\nhealthcare, science, and finance. In contrast to earlier surveys (shown in Table 1) that underemphasize governance\nand security, this work foregrounds adversarial threats (e.g., vulnerabilities in agent interaction), end-to-end lifecycle\ngovernance (from training data to deployment oversight), and large-scale explainability. These concerns are treated as\n3\n--- Page 4 ---\nAPREPRINT\nintegral to system design, not peripheral. Additionally, this survey provides actionable recommendations for researchers,\nengineers, and policymakers, bridging gaps left by both technical and responsible-AI surveys.\n2 Literature Review Methodology\nTo ensure a comprehensive review of the literature on trust, risk, and security in Agentic AI systems, we adopted a\nstructured methodology inspired by best practices in systematic reviews [ 31,32]. This section outlines our research\nobjectives, data sources, inclusion criteria, and classification strategy.\nResearch Objectives The review was guided by the following questions:\n•RQ1: What are the key trust, risk, and security challenges posed by LLM-enabled Agentic AI systems?\n•RQ2: What technical and governance strategies have been proposed to address these challenges?\n•RQ3: How do existing approaches map onto the TRiSM pillars: explainability, model operations, security, and\nprivacy?\n•RQ4: What gaps remain in current research, and what directions are promising for future work?\nData Sources and Search Strategy We searched the following major digital libraries: IEEE Xplore, ACM Digital\nLibrary, SpringerLink, arXiv, ScienceDirect, and Google Scholar, covering publications from January 2020 to May\n2025. The search was executed using Boolean combinations of keywords relevant to Agentic AI and TRiSM, including:\n\"agentic AI\" OR multi-agent systems \"(AMAS)\" OR \"multi-agent LLMs\" OR \"AI agents\" OR\n\"autonomous agents\" OR \"intelligent agents\") AND (\"trust\" OR \"trustworthiness\" OR \"risk\" OR\n\"security\" OR \"safety\" OR \"governance\" OR \"oversight\" OR \"compliance\" OR \"explainability\" OR\n\"interpretability\" OR \"transparency\" OR \"privacy\" OR \"data protection\")\nIn total, 150 unique papers were initially retrieved.\nInclusion and Exclusion Criteria We applied the following criteria during the screening process:\n•Inclusion:\n–Studies published between 2022 and 2025 , to capture developments following the introduction of LLMs\n(mostly post-chatgpt).\n–Studies that explicitly discuss agentic ,multi-agent , orLLM-powered AI systems in the context of at least one\nTRiSM dimension: trust, risk, security, governance, explainability, or privacy.\n–Peer-reviewed articles, preprints, or whitepapers from credible sources (e.g., arXiv, NeurIPS, IEEE, ACM,\nNature, or governmental standards bodies).\n•Exclusion:\n–Papers focused solely on traditional, rule-based, or symbolic agents without any integration of LLMs or\nemergent coordination.\n–Studies addressing only low-level ML components (e.g., training optimization, architecture design) without\nreference to agentic behavior or TRiSM concerns.\n–Non-English papers or those lacking sufficient metadata (e.g., missing abstracts or publication details).\nAfter screening titles and abstracts, we shortlisted 250 papers. A subsequent full-text review narrowed the selection to\n180 primary studies that directly addressed one or more TRiSM pillars. In addition to peer-reviewed literature, we also\nincorporated relevant white papers, technical blogs, and practitioner reports to contextualize and map the emerging\nlandscape of Agentic AI within the TRiSM framework.\nQuality Assurance To ensure the rigor and reliability of included studies, we conducted a quality assessment adapted\nfrom established systematic review guidelines [ 33,32]. Each paper was evaluated against the following criteria: (1)\nDoes the study clearly define its objectives in the context of agentic or LLM-based systems? (2) Are the methods,\nexperiments, or architectural designs well-documented and reproducible? (3) Does the paper substantively engage with\nat least one of the trust, risk, security, explainability, or privacy aspects? (4) Does the study offer empirical, theoretical,\nor normative contributions relevant to Agentic AI governance? Each criterion was rated as low,medium , orhigh.\nStudies scoring “low” in more than two dimensions were excluded or flagged for contextual relevance only. Quality\nratings were assigned independently by two reviewers, with discrepancies resolved through discussion.\n4\n--- Page 5 ---\nAPREPRINT\nFundamentalsTRiSM Threats and Risks\nTrust and Explainability\nMetricsSecurity and PrivacyGovernance for \nAgentic LLMsExplainability in \nMulti -Agent \nDecisions\nModelOps: Lifecycle \nmanagement for \nLLM Agent Application \nSecurity for LLMs\nModel Privacy and \nData Protection\nPrivacy Mechanism Privacy Preserving\nCompliance and Governance\nFundamentalsFrameworks\nThreats\n & \nRisks\nTrust \n&\nExplainabilitySecurity\n&\n Privacy\nEvaluation \nMetrics\nFigure 1: Taxonomy of Agentic AI systems presented in this review. It includes five major sections: Fundamentals\nof Agentic AI Systems (Agentic AI vs. Traditional AI Agents, LLM-Based Architecture), The TRISM Framework\n(Governance, Explainability, ModelOps, Application Security, Privacy), Threats and Risks (Unique Threat Vectors,\nTaxonomy of Risks, Case Studies), Trust and Explainability, and Evaluation Metrics (including Human-Centered and\nComposite Metrics).\n3 Fundamentals of Agentic AI Systems\n3.1 Agentic AI vs. Traditional AI Agents\nThe distinction between traditional AI agents and Agentic AI systems represents a paradigm shift in autonomous system\ndesign. Traditional agents operate through predefined rules [ 34], heuristic workflows [ 35,36], or deterministic logic\n[37], excelling in narrow, bounded environments. In contrast, Agentic AI systems leverage foundation models (LLMs)\nto achieve adaptive goal-oriented behavior through multi-agent coordination and emergent reasoning capabilities.\nAgentic AI systems fundamentally redefine autonomy through three core innovations: 1) Multi-agent coordination\nwhere specialized agents (e.g., planners, verifiers) collaborate via structured protocols [ 18,20] 2)Persistent context\nthrough memory architectures that maintain state across workflows [ 38] 3)Dynamic meta-orchestration that delegates\ntasks and resolves conflicts [39] We present a comparison of AI agents and Agentic AI in Table 2.\nThis architecture enables longitudinal task execution and cross-domain generalization, overcoming the context frag-\nmentation inherent in traditional approaches [ 40]. Systems like AutoGen [ 19] demonstrate how LLM-backed agents\ncollectively decompose complex problems through multi-step reasoning [ 41], marking a qualitative leap from reactive\nagents to proactive problem-solving systems.\n5\n--- Page 6 ---\nAPREPRINT\nTable 2: Traditional AI Agents vs. Agentic AI Systems\nDimension Traditional AI Agents Agentic AI Systems\nAutonomy Model ×Reactive execution via scripts/rules ✓Goal-driven planning & adaptation\n•Fixed action sequences •Recursive self-improvement\nCognitive Foundation ×Symbolic logic & FSMs ✓Foundation models (LLMs/LIMs)\n•Hand-coded KBs •Emergent reasoning\nIntelligence Scope ×Narrow & task-specific ✓Broad & compositional\n•Single-domain focus •Cross-domain transfer\nReasoning Approach ×Deterministic/single-step ✓Multi-step & contextual\n•Rule-based inference •Chain-of-Thought (CoT)\nCollaboration ×Isolated execution ✓Role-specialized coordination\n•Manual decomposition •Automated hierarchy\nTemporal Context ×Episodic & stateless ✓Persistent memory\n•Session-bound •VectorDB/LTM\nOrchestration ×Hard-coded pipelines ✓Dynamic meta-agents\n•Sequential workflows •Conflict resolution\nTool Utilization ×Static integrations ✓Planned API invocation\n•Handcrafted interfaces •Function calling\nContext Awareness ×Bounded inputs ✓Memory-augmented\n•Limited context •RAG architectures\nScalability ×Domain-specific ✓Cross-domain transfer\n•High maintenance •Modular composition\nExemplars ▷MYCIN ▷ELIZA ▷Finite-state bots ▷AutoGen ▷ChatDev ▷MetaGPT ▷AgentVerse\n✓= Core strength ×= Fundamental limitation •= Implementation characteristic ▷= Modern framework ▷= Classical system\n3.2 LLM-Based Agentic AI System Architecture\nAgentic Multi-Agent System (AMAS) represent an emerging paradigm in AI where multiple LLM-powered agents\noperate semi-autonomously, interact with external tools, and collaborate to achieve complex tasks. As illustrated in\nFigure 2, a typical AMAS architecture comprises several key components that together form a flexible yet highly\ndynamic ecosystem. At the core are multiple LLM-Based Agents, each capable of reasoning, planning, and tool\ninvocation [ 42]. These agents access a Shared Toolchain Interface [ 43] to execute code, perform searches, or interact\nwith domain-specific APIs.\nCommunication and coordination are facilitated through a Communication Middleware [ 44], allowing agents to\nshare goals, observations, or intermediate results. A Task Manager or Orchestrator [ 45] governs high-level planning,\ndelegating subtasks to agents based on their roles or specializations. Agents can read from and write to a World Model\nor Shared Memory [ 25], which stores contextual knowledge, system state, or evolving task data. Human oversight is\nsupported through a Human-in-the-Loop Interface [36], enabling users to prompt, correct, or halt agent behavior.\nTo ensure accountability, a Trust and Audit Module monitors agent actions, logs tool usage, and generates behavioral\ntraces [ 46]. However, this modular and distributed structure introduces significant TRiSM [ 47] challenges. With\nmultiple autonomous agents accessing external resources, the Security Gateway becomes critical for enforcing access\ncontrols, authentication, and sandboxing [48].\nLikewise, a dedicated Privacy Management Layer is essential to prevent leakage of sensitive or personally identifiable\ninformation [ 49], especially when data traverses multiple agents or tools. Finally, an Explainability Interface must\nprovide interpretable rationales for multi-agent decisions, supporting transparency and trust calibration [ 50]. Together,\nthese architectural elements make AMAS powerful yet complex, raising unique and urgent questions about how to\nensure their trustworthiness, mitigate systemic risks, and secure them against adversarial behaviors. Below, we discuss\nthe architecture of AMAS.\nLanguage Model Core (Agent Brain). At the center of an Agentic AI system lies a LLM serving as the primary\ndecision-making controller or “brain” [ 51]. The core LLM is initialized with a user goal and a structured agent\nprompt (defining its role, capabilities, and tool access). It then generates step-by-step decisions or actions, interpreting\ninstructions, producing reasoning traces, and selecting next steps in either natural language or structured action formats.\nIn many agent frameworks, such as [ 52], Baby AGI [ 53] and GPT Engineer [ 54], the LLM governs the full control loop,\norchestrating the overall system behavior.\nPlanning and Reasoning Module. To handle complex goals, an explicit planning mechanism decomposes tasks\ninto manageable sub-goals. This can be done internally via chain-of-thought (CoT) or tree-of-thoughts prompting\n[55], where the model performs intermediate reasoning before arriving at a final decision. Some implementations\nemploy external planning systems by translating goals into structured planning languages and using classical planners\nfor long-term decision-making. Planning is often interleaved with execution and feedback: the agent refines its plan\nbased on outcomes, alternating between reasoning, acting, and integrating observations. Techniques like ReAct [ 56]\nexemplify this loop by guiding the agent through repeated reasoning–action–observation cycles, improving performance\non complex tasks.\n6\n--- Page 7 ---\nAPREPRINT\nPerception/Input \nLayer\nCommunication & \nCollaboration\nDecision -Making\nLearning \nModule\nFigure 2: Architecture of LLM-based Agentic Multi-Agent System (AMAS), illustrating key functional layers: Percep-\ntion/Input Layer (text, image, audio processing), Cognitive/Reasoning Layer (goal-setting, planning, decision-making),\nAction/Execution Layer (digital and physical task execution), Learning Module (supervised and reinforcement learning),\nCommunication and Collaboration (agent messaging and coordination), Data Storage & Retrieval (centralized/distributed\ndatabases), and Monitoring & Governance (ethical oversight, observability, and compliance mechanisms). The modular\ndesign highlights adaptive intelligence and inter- agent synerg\nMemory Module. Agentic AI systems integrate memory to maintain context across iterations. This includes short-term\nmemory (recent interactions held within the prompt context) and long-term memory (accumulated knowledge or\nexperiences) [ 7]. Long-term memory is often implemented using vector databases, where key facts or past events are\nstored and retrieved by similarity search. By reintegrating past data into the LLM’s prompt, the agent can recall relevant\ninformation across sessions, avoid repetition, and support coherent long-term planning. Effective memory management\nenables adaptive, learning-driven behavior.\nTool-Use Interface. To extend its capabilities beyond text generation, the agent is equipped with a tool-use interface\n[57]. This layer allows invocation of external tools such as web search, APIs, code interpreters, or databases. The\navailable tools are specified in the agent prompt with command schemas. When the LLM determines a tool is needed, it\nemits a structured command, which is executed externally. The result is fed back into the LLM as a new observation.\nThis mechanism enables the agent to access real-time information, perform computations, and interact with external\nsystems dynamically. Tool-use frameworks like MRKL [ 58] illustrate this design by routing queries to different expert\nmodules (symbolic or learned) with an LLM as the router. Similarly, approaches like Toolformer [ 57] train the model to\ninsert API calls in its text generation.\nPerception and Environment Interface. For agents interacting with dynamic environments, such as web interfaces,\nsimulated worlds, or physical systems, an observation - action interface is essential [ 59]. Perception modules translate\nraw inputs (e.g., sensor data, images, or textual states) into representations the LLM can process. Conversely, the\nagent’s chosen actions are executed within the environment and the resulting state changes are returned to the agent as\n7\n--- Page 8 ---\nAPREPRINT\nTable 3: Representative LLM-Based Agentic AI Frameworks\nFramework (Year) Core LLM Planning Memory Tool Use Notable Features\nAutoGPT [60] GPT-4 Self-looped CoT Vector DB OS shell + web Fully autonomous goal loop\nBabyAGI [53] GPT-3.5/4 Task queue In-memory Web search Minimal task generator\nGPT Engineer [61] GPT-4V Spec-to-code File cache Python REPL End-to-end code generation\npipeline\nLangGraph [62] Model-\nagnosticFSM via graph Persistent nodes Custom modules Visual orchestration of agent\ngraphs\nAutoGen [19] GPT-4 Multi-agent\nPDDLJSON/DB API calls Modular, reusable agent templates\nMRKL [58] LLaMA/GPT Prompt router N/A Math + search\ntoolsNeuro-symbolic expert routing\nReflexion [63] GPT-3.5/4 Retry–reflect\nloopEpisodic buffer Same as base\nagentVerbal self-improvement via\nreflection\nMetaGPT [64] GPT-4 SOP workflow YAML state Git CLI Structured roles for software\nengineering\nV oyager [65] GPT-4 Auto skill tree Task DB Minecraft API Lifelong open-ended learning in\nenvironments\nWebV oyager [66] LLaV A-1.6 ReAct JSON store Browser actions Multimodal web interaction via\nvision and text\nHuggingGPT [67] ChatGPT Task-plan-select Log store Hugging Face\nmodelsExternal model orchestration by\nLLMs\nCAMEL [68] GPT-4 Role-play CoT Dialogue log Chat only Multi-agent role simulation via\ndialogue\nChatDev [69] GPT-4 Chat chain File repo Unix tools Simulated software development\nworkflow\nCrewAI [70] Any Declarative plan Optional DB Python modules Lightweight, LangChain-free\nframework\nAgentVerse [71] Model-\nagnosticConfig graph Redis/KV store Plugin API Multi-agent simulation and task\nsolving\nOpenAgents [72] Model-\nagnosticAgent scripts MongoDB Web plugins Open platform with public hosting\nSuperAGI [73] GPT-4 DAG workflow Postgres Pinecone Concurrent agents for production\nuse\nobservations. This loop supports sense-plan-act cycles that continue until the task is completed or halted. In robotic or\nmultimodal settings, the interface may include additional sensory models (e.g., vision transformers), but the core flow\nremains consistent.\nIntegration and Autonomy. These modules together form a closed-loop architecture. The LLM plans and reasons over\ntasks, guided by memory and tools, and interfaces with the environment to execute decisions and observe results [ 17].\nEach iteration enriches the agent’s context, enabling it to self-prompt, generate subtasks, assess progress, and adapt\nstrategies over time. This integrated design empowers agentic systems to operate autonomously, pursue long-range\ngoals, and exhibit adaptive behavior across dynamic environments.\nTable 3 presents representative systems and shows how each one maps these five design axes (LLM core, planning/\nreasoning, memory, tool-use, environment interface) into practice. For example, AutoGPT pairs a GPT -4 core with\na self -looped chain -of-thought planner and a vector -database memory, while V oyager couples the same core with an\nembodied Minecraft API that serves as both tool layer and environment interface. We comparing current Agentic AI\nimplementations and identifying open design trade-offs in this table.\n4 The TRISM Framework for Agentic AI\n4.1 Governance for Agentic LLM Systems\nAI Trust, Risk, and Security Management (AI TRISM) is a comprehensive governance framework designed to ensure\nthat AI systems are trustworthy, robust, and compliant with safety standards [ 24]. Originally highlighted in industry\nguidelines for AI governance [ 74], TRISM provides a structured approach to manage the unique challenges of LLM-\nbased “agentic” AI systems . Such systems consist of autonomous LLM agents that can make independent decisions,\n8\n--- Page 9 ---\nAPREPRINT\ncollaborate with other agents, and adapt their behavior over time [ 46]. These properties: autonomy, multi-agent\ninteraction, and evolving behavior, introduce new risks not seen in traditional single-model deployments [75].\nFor example, an agent acting in isolation might be benign, but when interacting with others across organizational or\ntrust boundaries, it could manipulate peers or leak confidential information. The TRISM framework addresses these\nconcerns by focusing on four key pillars: Explainability, Model Operations (ModelOps), Application Security, and\nModel Privacy [ 46,24]. Each pillar targets a critical aspect of safety or risk management, ensuring that an agentic LLM\nsystem remains transparent, reliable, secure, and compliant with ethical and regulatory requirements. Below, we define\neach pillar and explain how it applies to LLM-based agentic systems, grounding the discussion in current research and\nbest practices.\n4.2 Explainability in Multi-Agent Decision Making\nDefinition and Importance Explainability refers to making the inner workings and decisions of AI agents interpretable\nto humans. In the context of agentic LLM systems, explainability is paramount for building user trust, as outcomes often\nemerge from complex interactions among multiple agents rather than a single model’s prediction [ 76]. Accordingly, the\nTRISM framework elevates explainability as a core pillar to ensure that each agent’s actions and the overall system’s\nbehavior can be understood and audited.\nTechniques for Explainability in Agentic Systems Achieving explainability in multi-agent LLM systems is challenging\nbecause one must interpret not only individual model decisions, but also the inter-agent dynamics that lead to final\noutcomes. Established explainable AI techniques provide a starting point. For instance, Local Interpretable Model-\nAgnostic Explanations (LIME ) [77] and S hapley Additive Explanations (SHAP) [78] can be adapted to analyze LLM\ndecisions. These techniques identify which features or input factors most influenced an agent’s output, offering insight\ninto why a particular action or response was taken. In an agentic setting, a “feature” might be a component of the\nagent’s input context or a signal from another agent.\nBeyond local explanations, counterfactual analysis is increasingly important for multi-agent explainability. Counterfac-\ntual techniques examine how the system’s behavior would change if certain conditions were altered [ 79], for example,\nif a particular agent’s contribution were removed or modified. This approach, rooted in causal inference, helps isolate\neach agent’s role in collaborative decision-making. For instance, by systematically toggling an agent off or varying its\noutputs, one can observe changes in the collective outcome and thus determine that Agent Xwas critical in influencing\ndecision Y. Such analyses surface the dependencies and influences among agents, effectively explaining emergent\nbehaviors at the system level. Moreover, recent research on explainable AMAS suggests logging the intermediate\nreasoning steps (e.g. chain-of-thought prompts or dialogue between agents) can further enhance transparency. Human\nauditors can also help with a trace of how agents arrived at a decision, e..g, which agent contributed what information\nand why, to produce narrative explanations for its outcomes.\nSummary In summary, the explainability pillar of TRISM compels the use of these techniques (surrogate models,\nfeature attributions, counterfactual testing, and transparent reasoning traces) to ensure that even highly autonomous\nLLM agents remain interpretable and accountable to human oversight.\n4.3 ModelOps : Lifecycle Management for LLM Agents\nDefinition and Scope ModelOps is the discipline of managing AI models through their entire lifecycle, from develop-\nment and deployment to monitoring, maintenance, and eventual retirement [ 80]. It extends the principles of MLOps\n(Machine Learning Operations) to focus specifically on model governance and reliable operation in production. Within\nagentic LLM systems, ModelOps encompasses not just individual models, but the orchestration of multiple agents\nand the supporting infrastructure that keeps them running safely [ 46]. Effective ModelOps is crucial for maintaining\nconsistency, performance, and regulatory compliance as LLM agents evolve or as new agents are added to a system.\nApplication to Agentic Systems LLM-based agents require rigorous lifecycle governance because their behavior can\nchange with model updates, prompt adjustments, or environmental drift. A cornerstone of ModelOps in this context is\nversion control , i.e., tracking and managing versions of each agent’s model and prompt configurations. Additionally,\nrobust CI/CD pipelines (Continuous Integration/Continuous Deployment) are employed to automatically test agents’\nperformance and safety whenever a model is fine-tuned or an agent’s logic is modified. Before deployment, multi-agent\nsimulations and unit tests validate that new agent behaviors do not introduce regressions or unsafe interactions. This\naligns with best practices in LLM operations (LLMOps) [ 81], which integrate MLOps principles tailored to the\nchallenges of large language models\nChallenge One challenge with this method is model drift, i.e., over time, an agent’s responses may become less accurate\nor relevant as data distributions shift or real-world conditions change. Continuous monitoring is therefore required\n9\n--- Page 10 ---\nAPREPRINT\nto detect performance degradation or deviations from expected behavior, triggering retraining or recalibration when\nneeded. Moreover, real-time monitoring and logging are fundamental in multi-agent settings. Each agent’s actions\n(e.g. API calls, decisions taken, errors encountered) are logged and analyzed to provide observability into the system’s\nfunctioning.\nIn a large-scale agent ecosystem, orchestration services may oversee the agents, scheduling their tasks and managing\ninter-agent communication. ModelOps must govern these orchestration layers as well, ensuring, for instance, that if one\nagent fails or produces suspicious output, it can be isolated or shut down without collapsing the whole system.\nSummary In line with TRISM objectives, robust ModelOps thereby ensures that an Agentic AI system remains reliable\nand maintainable. It formalizes change management (so updates do not introduce new risks), provides continuous\nvalidation of model behavior, and supports compliance by logging data for audits and enforcing policies (e.g. preventing\nunauthorized model changes).\n4.4 Application Security for LLM Agents\nThreats in Agentic Environments Application security in the TRISM framework focuses on protecting AI agents and\ntheir ecosystem from malicious attacks and misuse. LLM-based agents are susceptible to a range of novel security\nthreats that exploit their language-based interfaces and cooperative behaviors. One well-documented threat is prompt\ninjection [82], wherein an attacker designs input data that contains hidden or malicious instructions . Recent studies\n[83] have shown that in AMAS, such prompt injections can propagate from one agent to another, a phenomenon dubbed\n“prompt infection” is analogous to a computer virus spreading across networks [ 84]. In a prompt infection scenario, a\nmalicious prompt introduced to Agent A might covertly modify Agent A’soutput, which then becomes part of Agent B’s\ninput, thereby tricking Agent B as well, and so on. This cascading attack can lead to widespread data leaks, fraudulent\ntransactions, misinformation, or coordinated misbehavior across an agent society.\nAnother critical vulnerability is identity spoofing andimpersonation [85]. In a multi-agent system, agents often\ncommunicate or coordinate tasks, and they may rely on credentials or tokens to authenticate one another. For example,\nif an adversary steals an agent’s API key or tricks the system into treating a rogue model as a trusted peer, they could\nissue commands or receive information under a false identity.\nDefensive Measures: To mitigate these threats, TRISM’s security pillar mandates a defense-in-depth approach tailored\nto LLM agents:\n1.Prompt hygiene : agents should treat inputs defensively by sanitizing and filtering prompts, and using\nguardrails or content policies to detect and refuse suspicious instructions. Prompt hardening (e.g., adding\nsecure prefixes or validation steps) is one such method to make agents less susceptible to injection [86].\n2.Strong authentication and access controls Each agent and human user must be securely authenticated, and\nleast-privilege principles [87] should constrain what actions an agent can perform autonomously.\n3.Continuous monitoring If an agent suddenly starts issuing unusual requests or deviates from its normal\npattern of behavior, automated monitors can flag this for investigation or trigger an automatic shutdown of the\nagent’s actions.\nRecent frameworks, such as LangChain/LangFlow [ 88], AutoGen [ 89], CrewAI [ 70], introduce the idea of trust scores\nor reputation among agents, where agents verify each other’s outputs and cross-check decisions to catch inconsistencies\nor signs of compromise. Moreover, training LLM agents with adversarial robustness in mind (e.g. fine-tuning on\nadversarial examples, employing adversarial training regimes) [84] can improve their resistance to malicious inputs\nSummary In sum, the Application Security pillar of TRISM emphasizes proactive safeguards against both external\nattackers and potential rogue agents. By implementing strict authentication, input validation, encrypted communication,\nsandboxing of execution (for agents that can use tools or code), and comprehensive monitoring, organizations can\nsignificantly reduce the risk of prompt-based exploits, impersonation, and other lateral vulnerabilities that are unique to\nautonomous multi-agent AI systems hal.science. This layered security approach is essential to maintain the integrity\nand reliability of agentic LLM deployments in adversarial environments.\n4.5 Model Privacy and Data Protection\nChallenges in Data Sharing: The Model Privacy pillar addresses the protection of sensitive data within AI agent\nsystems, ensuring that the use of personal or confidential information complies with privacy regulations and ethical\nnorms. LLM-based agents often need to handle user data, proprietary business information, or other sensitive inputs to\nfulfill their tasks [ 75]. In a multi-agent context, this challenge is amplified by the fact that agents may share information\n10\n--- Page 11 ---\nAPREPRINT\nTable 4: TRISM pillars for LLM-based Agentic AI and associated governance references.\nPillar Primary Objective Key Practices & Agentic-LLM Con-\nsiderationsExemplar Standards / Metrics\nExplainability Transparent multi-agent\ndecisionsSHAP, LIME, counterfactuals;\nreasoning-trace logs; cross-agent\ndependency mappingNIST AI RMF “Explain” [96]\nISO/IEC 24029-1 [97]\nMetrics: fidelity, sufficiency, human-\ntrust score\nModelOps Lifecycle governance of\nagents, models, orches-\ntrationVersion-controlled prompts; CI/CD w/\nmulti-agent sims; drift monitoring; roll-\nback policiesISO/IEC 42001 (AI MS) [98]\nSRE MTTR/MTTD; ∆-accuracy\n(model drift); pipeline pass-rate\nApplication Security Prevent prompt injec-\ntion, spoofing, lateral\nexploitsPrompt sanitation; least-privilege\nscopes; encrypted comms; sandboxed\ntool calls; anomaly detectorsOWASP Top-10 for LLM Apps [99]\nJailbreak-success%; mean-time-to-\ndetect; exploit CVSS\nModel Privacy Safeguard sensitive data\nand shared memoryDifferential-privacy training; data mini-\nmization; homomorphic encryption; se-\ncure enclaves; audit logsGDPR Art. 25 [100]\nHIPAA §164 [101]\nMetrics: ϵ-DP budget, leakage rate,\naccess-audit pass-rate\nwith each other (e.g. via a shared memory store or message passing) to collaborate. Without strict privacy controls, there\nis a risk that an agent could inadvertently expose private data to unauthorized parties or that sensitive information could\n“leak” through the language model’s outputs. Therefore, TRISM’s privacy pillar compels organizations to institute\nmeasures that safeguard data throughout the AI lifecycle, from training and inference to inter-agent communication.\nTechniques for Privacy Preservation\n•Differential Privacy (DP): Injects calibrated noise during model training to prevent memorization of individual\ndata entries, ensuring no single record significantly influences the output [90].\n•Data Anonymization and Minimization: Limits inter-agent data sharing to only what is necessary [ 91], often\nusing aggregated or pseudonymized formats (e.g., “age bracket 30-40” instead of exact birthdate).\n•Secure Multi-Party Computation (SMPC): Enables agents to compute joint functions without exposing private\ninputs, useful in cross-organization tasks like collaborative fraud detection [92].\n•Homomorphic Encryption (HE): Allows agents to compute on encrypted data [ 93]. With Fully Homomorphic\nEncryption (FHE), even the plaintext query and response remain unseen by the agent.\n•Trusted Execution Environments (Secure Enclaves): Hardware-based isolation ensures that even privileged\nsystem users cannot access the data being processed by agents [ 94]. Useful for secure memory sharing and execution.\n•Model Privacy Policies and Compliance: Enforces data retention limits, maintains audit logs, and ensures\ncompliance with regulations (e.g., GDPR, HIPAA) governing agent behavior and data usage [95].\nSummary By implementing these layers of privacy defense , from differential privacy in model training to homomorphic\nencryption for data sharing, and stringent access control policies, Agentic AI systems can protect user data and\nproprietary information even as they leverage that data for intelligent decision-making.\nThe TRISM framework offers a comprehensive governance model for LLM-based Agentic AI, integrating Explainability,\nModelOps, Application Security, and Model Privacy to manage the complexity of autonomous agent systems. Grounded\nin proven methods, such as SHAP, CI/CD, adversarial defenses, and homomorphic encryption, TRISM enhances safety,\ntransparency, and trust. As AI systems evolve, TRISM provides a stable foundation to ensure responsible and secure\nagent behavior, aligning advanced capabilities with human values and operational integrity.\nTable 4 condenses each pillar into its governing goal.\n5 Threats and Risks in LLM-based (Agentic) AMAS\nIn this section, we discuss the threats and risks in AMAS.\n5.1 Unique Threat Vectors\nAgentic AI systems introduce a distinct set of security and reliability concerns compared to traditional single-agent\nLLM architectures [ 102]. These risks primarily arise from agents’ autonomy, persistent state management, and the\ncomplex demands of multi-agent coordination [4]. Below, we discuss these threats\n11\n--- Page 12 ---\nAPREPRINT\n•Autonomy abuse The foremost threat is autonomy abuse , whereby agents with significant decision-making authority\nmight misinterpret objectives or implement harmful plans due to erroneous reasoning or manipulated inputs\n[103,104]. Unlike deterministic models, agentic systems dynamically generate actions, complicating efforts to\ndefine and enforce safe operational states [105, 106].\n•Persistent memory Another threat is the persistent memory, which while, crucial for context retention, introduces\nunique vulnerabilities through potential adversarial injections and accumulations [ 107,108]. Such contamination\ncan propagate subtly via shared memory, especially in the absence of detailed version control and robust audit\nmechanisms [103, 102].\n•Agent orchestration This risk involves central or distributed control mechanisms for role assignment and work-\nflow mediation. A compromised orchestrator could distort task distribution or misroute information, triggering\ncascading failures, issues exemplified by documented vulnerabilities in MetaGPT [ 20] and AutoGen [ 19]. These\norchestration vulnerabilities differentiate agentic systems markedly from conventional stateless, single-threaded\nLLM deployments.\n5.2 Taxonomy of Risks\nTo systematically understand the security landscape in Agentic AI, we categorize risks into four broad classes:\nadversarial attacks, data leakage, agent collusion, and emergent behaviors.\n•Adversarial Attacks Agents remain vulnerable to prompt injections, gradient-based manipulations, and engineered\nreasoning traps, risks that are magnified in AMAS due to propagation across agent interactions [ 109,110]. An\nillustrative instance is the role-swapping attack observed in ChatDev [18].\n•Data Leakage Persistent memory and extensive inter-agent communication elevate the likelihood of accidental\nexposure of sensitive information [ 111,112]. In sensitive domains like financial services and HR, inadequate\nboundary enforcement and ineffective sanitization amplify these leakage risks [113, 114].\n•Agent Collusion and Mode Collapse Coordination mechanisms can inadvertently lead agents to reinforce mutual\nerrors, precipitating groupthink or echo chambers [ 115,116]. AutoGen experiments illustrate how iterative dialogues\namong agents can amplify flawed designs, highlighting the risk of emergent misalignment [19, 117, 118].\n•Emergent Behavior Complex interactions among agents, memory components, tools, and tasks yield unpredictable\nbehaviors that evade traditional testing and validation methods [ 119,120]. Agents optimizing for efficiency may\nunintentionally bypass critical verification steps or suppress contradictory information, scenarios exemplified in\nblockchain [121] and audio verification contexts [122].\n5.3 Case Studies\nSeveral real-world and research-based examples illustrate the tangible impact of these risks in deployed or experimental\nagentic systems.\nCase Study 1: Prompt Leakage in Agentic Systems. Instances of prompt leakage have been observed in LLM-based\nagent frameworks such as AutoGPT, where recursive prompt augmentation and insufficient memory controls can\nlead to the unintentional exposure of sensitive information. In one reported scenario, sensitive tokens were stored in\npersistent memory and later surfaced in planning summaries or external logs. Such vulnerabilities underscore the critical\nimportance of implementing memory sanitization, access controls, and prompt boundary protections to safeguard\nagentic systems from cascading information leaks [123].\nCase Study 2: Collusive Failure in ChatDev. In a collaborative code generation session involving planner, coder,\nand tester agents within the ChatDev framework, an error in a shared planning module led to the propagation of faulty\ndesign assumptions. Due to the absence of external ground-truth or objective feedback loops, all agents validated each\nother’s outputs, resulting in a feedback loop of erroneous confirmations. This scenario underscores the necessity of\nincorporating diverse information sources and adversarial checks within agent loops to prevent such collusive failures\n[69, 83].\nCase Study 3: Simulation Attack in Swarm Robotics. In a simulated swarm robotics experiment utilizing LLM-based\nplanning strategies, an agent was provided with a misleading environmental assumption, leading to a coordination\nfailure characterized by spatial congestion and task incompletion. This incident underscores the potential vulnerabilities\nin real-world deployments, particularly in critical infrastructure or logistics, where such failures could have significant\nconsequences. The case highlights the importance of robust validation mechanisms and the integration of diverse\ninformation sources to ensure reliable swarm behavior [124, 125].\nCase Study 4: Memory Poisoning in Multi-Agent Chatbots. In a multi-agent customer support system, a customer-\nfacing agent injected sarcastic feedback into a persistent feedback buffer. This buffer was later utilized by the\n12\n--- Page 13 ---\nAPREPRINT\npolicy improvement agent to adapt dialogue strategies, resulting in responses with inappropriate tones. This incident\nunderscores the importance of implementing validation filters, sentiment monitoring, and robust feedback loop\ngovernance in self-adapting systems to prevent such memory poisoning vulnerabilities [126].\nCase Study 5: System Prompt Drift in Autonomous Memory Agents In experiments with agents using system-level\nmemory (e.g., LangGraph or BabyAGI), over time, system prompts began drifting due to self-appended contextual\nmemory that wasn’t properly versioned or validated. This led to hallucinated goals and emergent behaviors misaligned\nwith initial intentions [ 127]. Points to risks from prompt accumulation and the need for memory version control, audit\ntrails, and reset mechanisms.\nThese cases illustrate that the introduction of autonomy, memory, and orchestration into LLM-based AI introduces an\nexpanded threat surface that cannot be mitigated with traditional LLM security protocols alone. As agentic systems\nevolve, new methodologies are needed for rigorous, system-wide threat modeling and runtime assurance that span\nmultiple agents, roles, and memory contexts.\n6 Trust and Explainability in Agentic AI\nAgentic AI systems are highly autonomous agents capable of making decisions and taking actions without continuous\nhuman oversight. These systems also pose unique challenges and opportunities for human trust. Ensuring that users and\nstakeholders have confidence in such systems is crucial for their adoption in real-world settings. Two key factors that\ninfluence trust in Agentic AI are the transparency of the agent’s decision-making processes and the ability to explain or\njustify its actions in human-understandable terms.\n6.1 Building Trust in Autonomous Agents\nIn Agentic AI systems, building trust is foundational to user acceptance, system reliability, and responsible deployment\nespecially as these systems begin to make autonomous decisions in critical domains such as healthcare, finance, and\nscientific research [ 128]. Unlike traditional software agents, autonomous LLM-based agents are characterized by\nself-directed reasoning, adaptive memory, and dynamic collaboration, which make their operations opaque and often\nunpredictable [ 129]. Establishing trust in such systems, therefore, requires a combination of technical transparency,\nuser feedback integration, and robust oversight mechanisms.\nTable 5: Trust-Enabling Mechanisms in Agentic AI Systems\nMechanism Purpose Example System(s) Trust Contribution\nTransparency via\nReasoning TraceabilityMake agent decisions\nobservable through\nintermediate steps or\nexplanationsSciAgent [130],\nMetaGPT [131]Enhances interpretability and\ndebuggability\nStatus Reporting and\nIntent DisclosureProvide real-time updates or\nclarification of progress and\nerrorsChatDev [18],\nAutoGen [89]Mimics human team transparency;\nreduces uncertainty\nHuman-in-the-Loop\n(HITL)Enable user oversight over\nhigh-stakes or irreversible\nactionsChemCrow [132],\nenterprise dashboardsPrevents unsafe execution; reinforces\ncontrol\nBehavioral Consistency &\nBounded AutonomyEnsure agents operate within\npredefined roles and limitsAdaptive tutors [ 133,134]Improves predictability; prevents\noverreach\nSocial Trust Cues Use polite language,\nuncertainty expression, and\nturn-takingGPT-4 safety-tuned\nprofiles [104, 135]Enhances user comfort and trust in\nuncertain scenarios\nTransparency and Decision Traceability Transparency is one of the core enablers of trust. For users to understand\nand evaluate agentic decisions, the reasoning chains, decision states, and action triggers of agents must be made\nobservable. Several agentic systems are now integrating decision traceability through mechanisms such as CoT\nprompting and self-explanation modules. For example, SciAgent [ 130] generates scientific summaries and provides\njustifications by linking outputs to source documents retrieved via retrieval-augmented generation (RAG). Similarly,\nMetaGPT[ 131] structures its reasoning using role-based outputs, where each agent (e.g., planner, coder) explicitly\nstates the logic behind its task execution, creating modular interpretability.\n13\n--- Page 14 ---\nAPREPRINT\nStatus Reporting and Progress Visibility Clear reporting of intent and intermediate status is also essential. Human\ncollaborators often require updates about what an agent is doing, why a task is taking longer, or how an agent interprets\nambiguous instructions. Tools like AutoGen[ 89] and ChatDev[ 18] have incorporated structured chat interfaces where\nagents summarize their intermediate progress, decisions, and encountered errors.\nHuman-in-the-Loop Oversight Human oversight and intervention mechanisms further reinforce trust. Allowing\nhuman users to review, edit, or approve agent-generated outputs not only prevents missteps but also signals that the\nsystem respects user authority [ 136,137,138]. Many systems adopt a human-in-the-loop (HITL) paradigm [ 139],\nwhere agents request confirmation before executing high-risk or irreversible actions.\nBehavioral Consistency and Bounded Autonomy Trust requires predictability. Agents should follow defined roles,\noutput in expected formats, and remain within delegated authority. Example In enterprise AI platforms used for\nautomated data analytics, agents may generate insights or dashboards but defer publishing until a domain expert reviews\nthe material. Similarly, in autonomous research assistants like ChemCrow [ 132], agents pause to allow chemists to\nvalidate proposed reactions or data pipelines before proceeding, reinforcing safe deployment.\nSocial Trust Cues and Language Behaviors Beyond system-level mechanisms, behavioral consistency and bounded\nautonomy are crucial. If an agent behaves unpredictably or inconsistently, even if technically correct, users are less\nlikely to trust it. Behavioral alignment mechanisms such as predefined role protocols, output style consistency, and\nlanguage modeling constraints help standardize responses [ 140,17,141].Example In adaptive educational platforms\nusing AI tutors, agents may be allowed to revise lesson plans but not change grading criteria, preserving institutional\ntrust boundaries [ 133,134]. Lastly, social trust cues, such as polite language, turn-taking, and cooperative gestures, have\nshown promise in reinforcing user trust even in non-expert settings [ 142]. Studies have found that users trust agents\nmore when they express uncertainty (\"I’m not sure, but here’s what I found\") rather than overconfidence [ 104,135]. This\nhas been implemented in models like GPT-4 when configured with safety-tuned instruction sets, improving reliability\nperception without undermining capability.\nTogether, these mechanisms form a layered trust strategy for Agentic AI, as summarized in Table 5. As autonomy\nand complexity increase, combining transparency, oversight, and social alignment will be essential to sustain user\nconfidence.\n6.2 Explainability Techniques\nExplainability remains a cornerstone in fostering trust, accountability, and reliability in Agentic AI systems, particularly\nas they operate in high-stakes environments where multi-agent coordination and autonomous decisions directly impact\nhuman lives [ 17]. In contrast to conventional AI systems, Agentic AI introduces unique challenges for explainability due\nto its decentralized architecture, dynamic role assignment, and evolving task decomposition among multiple interacting\nagents [4].\nTable 6: Explainability Techniques in Agentic AI Systems\nTechnique Purpose Challenges in Agentic AI Example Use Cases\nLIME [77] Local post-hoc explanation\nusing surrogate modelsFails to capture cross-agent\ndependencies in multi-agent\nsetupsFeature-level auditing in fraud\ndetection\nSHAP [78] Attribution of prediction to\ninput features via Shapley\nvaluesDifficult to aggregate across\nagents with divergent contextsFinancial risk analysis, regulatory\nexplanations\nDecision Provenance\nGraphs [143, 144]Trace inter-agent decision\npaths and data flowsCan be complex to interpret at\nscaleCollaborative document\ngeneration, planning chains\nMulti-agent SHAP [145] Attribution extended to\nagents and shared memoryHigh complexity, lack of standard\nframeworksStrategic gameplay, collaborative\nwriting\nSymbolic/Rule-Based\nAgents [4]Built-in interpretability via\nlogic or rule-based modelsLimited flexibility compared to\nLLMsHybrid planning systems,\neducational tutors\nAttention\nMaps [146, 147]Visualize focus of\nlanguage/image modelsOnly reflects internal focus, not\nlogic or intentMultimodal VQA, image\ncaptioning\nPrompt Audit\nTrails [148, 149]Log input prompts, outputs,\nand agent actionsRequires infrastructure, may miss\nimplicit state changesDebugging multi-agent sequences,\nfine-tuning\n14\n--- Page 15 ---\nAPREPRINT\nLocal Post-hoc Techniques (LIME and SHAP) Local Interpretable Model-Agnostic Explanations (LIME) [ 77] and\nSHapley Additive exPlanations (SHAP) [ 78] are widely adopted techniques that offer post-hoc interpretability. LIME\napproximates a black-box model locally using an interpretable surrogate model, while SHAP attributes predictions to\ninput features via Shapley values. These techniques have been integrated into agentic pipelines, particularly in finance\nand multi-agent fraud detection systems, where feature-level transparency supports regulatory compliance. However,\ntheir direct application in Agentic AI is limited. Each agent may operate with its own objectives, context, and tool\naccess, leading to divergent decision paths that local techniques struggle to reconcile [76, 150].\nExplainability in AMAS Emergent behavior poses another challenge: the interpretability of an individual agent does\nnot necessarily imply the interpretability of the overall system. In platforms such as ChatDev [ 18] or AutoGen [ 19],\nagents emulate specialized roles (e.g., engineer, reviewer), and tracing a final action back to its source agent is often\nnon-trivial. To address this, researchers have proposed composite frameworks that combine local explanations with\nglobal decision traceability [ 151,152,153]. For example, decision provenance graphs visualize communication flows\nand interdependencies across agents [ 143,144], while causal influence chains track how actions propagate between\nroles [ 154,155]. Adapted SHAP techniques for multi-agent setups now aim to attribute outcomes to shared memory\nand agent collaboration [145].\nSymbolic and Hybrid Architectures Another promising direction is the use of inherently interpretable modules\nsuch as rule-based planners and decision trees within hybrid architectures [ 4]. These agents offer built-in explainability\nbut retain the generative capabilities of LLMs for broader context understanding. Such designs are increasingly used in\ndomains where structure and interpretability are prioritized, such as educational AI or mission planning.\nLightweight Interpretability: Attention and Prompts Attention map visualizations have been used to highlight\nfocus areas in multimodal language agents [ 146,156,147], offering lightweight but informative views into model\nbehavior. Prompt audit trails logging prompt history, agent actions, and response metadata have also gained traction\n[148,149]. These mechanisms support system debugging, safety evaluations, and human-in-the-loop fine-tuning in\nmulti-agent environments.\nOngoing Challenges and Research Directions Despite these advancements, achieving robust explainability in\nAgentic AI systems remains an open research problem. Many techniques focus on isolated predictions or modules\nand fail to capture system-level dynamics. Future work should prioritize longitudinal interpretability across agent\ninteractions, causal reasoning pipelines, and interactive querying interfaces that support transparency in real time. A\ncomparative summary of current techniques is shown in Table 6.\n6.3 Evaluation Metrics for Agentic AI Systems\nAgentic AI systems demand a comprehensive evaluation across multiple dimensions beyond traditional accuracy. We\noutline five key categories of metrics: trustworthiness ,explainability ,user-centered performance ,coordination , and\ncomposite scores , each capturing a distinct aspect of an Agentic AI’s performance and its real-world implications..\nBelow we discuss these metrics and summarize in Table 7.\nTrustworthiness: This category evaluates the reliability, safety, and ethical alignment of the AI agent. A trustworthy\nagent consistently produces correct and unbiased results, adheres to constraints, and avoids harmful or unpredictable\nbehavior. Metrics for trustworthiness include success rates on tasks under varying conditions (measuring robustness),\nviolation rates of safety or ethical guidelines (which should be minimal), and calibration of the agent’s confidence (how\nwell the agent’s self-reported confidence aligns with actual accuracy) [ 157]. Some approaches combine such factors\ninto an overall trust score. For example, one model defines a trustworthiness score Tas\nT=C+R+I\nS, (1)\nwhere Cis the agent’s credibility (accuracy and correctness of its outputs), Ris reliability (consistency of performance\nover time), Iis the level of user alignment or rapport (analogous to “intimacy” in trust modeling), and Sis self-\norientation (the degree to which the agent pursues its own goals over the user’s goals). A higher Tindicates an agent\nthat is accurate, consistent, user-aligned, and not self-serving, which corresponds to greater trustworthiness. In practice,\nachieving high trustworthiness means the agent behaves predictably and transparently in accordance with ethical AI\nprinciples (such as fairness and accountability).\nExplainability: Explainability metrics assess how well the agent’s decisions can be understood and traced by humans\n[158]. These metrics focus on the clarity and completeness of the rationale the agent provides for its actions. For\n15\n--- Page 16 ---\nAPREPRINT\ninstance, one can measure the explanation coverage (the percentage of decisions or outputs that come with an adequate\nexplanation) and the fidelity of explanations (how accurately the explanation reflects the true reasoning or model logic).\nConsistency of explanations for similar scenarios is another important metric: the agent should explain comparable\ndecisions in a similar way, indicating a stable reasoning process. Quantitatively, methods like OpenXAI provide a suite\nof metrics to evaluate explanation quality across dimensions such as faithfulness, stability, and fairness of explanations\n[159]. High explainability builds user trust, as users can follow whythe agent made a decision, and it aids debugging by\nrevealing the agent’s internal decision process. In regulated domains (e.g., healthcare or finance), explainability is often\nessential for compliance and user acceptance.\nUser-Centered Performance: User-centered metrics capture how effectively the AI agent interacts with and satisfies\nthe end-user’s needs. These criteria emphasize the user’s experience and outcomes [ 160]. Key metrics include user\nsatisfaction ratings , typically collected via surveys or feedback after interaction, which reflect whether the user’s goals\nwere met and how comfortable they felt with the agent’s behavior. Task success from the user’s perspective (did the\nagent fulfill the user’s request or solve the user’s problem?) is a fundamental measure.\nAdditionally, interaction metrics like the number of back-and-forth clarification queries needed (fewer indicates the\nagent understood the user well) and the coherence or naturalness of the conversational flow contribute to user-centered\nevaluation. Human-in-the-loop evaluations are often employed here: for example, user studies might rate the agent\non criteria such as helpfulness, clarity and naturalness of language, and adherence to user instructions. Ultimately, a\nuser-centered agentic system should align its actions with user intent and preferences.\nBenchmarks like ChatDev [ 18], which simulates a multi-agent software development team interacting via natural\nlanguage, implicitly evaluate how well agents fulfill user-defined roles and requirements in a collaborative project. This\nhighlights the importance of user-oriented success in complex, realistic tasks.\nTable 7: Summary of key evaluation metrics for Agentic AI systems. Each category addresses a different facet of an\nautonomous agent’s performance and behavior.\nMetric Aspect Evaluation Focus (Examples of Metrics)\nTrustworthiness Reliability and safety of the agent’s behavior. Example metrics: task success rate across diverse scenarios;\nfrequency of rule or safety violations (lower is better); calibration of confidence vs. outcomes; fairness\nand bias indices.\nExplainability Transparency and interpretability of the agent’s decisions. Example metrics: percentage of outputs with\nan accompanying explanation; explanation fidelity to actual model reasoning; consistency of explanations\nfor similar cases; human interpretability ratings of explanations.\nUser-Centered User satisfaction and alignment with user needs. Example metrics: user satisfaction score (post-\ninteraction survey); rate of fulfilling user-defined goals; number of clarification questions needed (lower\nis better); naturalness and coherence of dialog from the user’s perspective.\nCoordination Effective collaboration in multi-agent or modular systems. Example metrics: multi-agent task completion\nrate; communication overhead (e.g., messages exchanged); consistency of shared plans or beliefs among\nagents; synergy score quantifying complementary actions.\nComposite Overall performance across metrics. Example metrics: weighted aggregate score encompassing trust,\nexplainability, user-centric, and coordination measures; specialized composite indices (e.g., TUE for tool-\nuse efficacy); cross-domain benchmark results (aggregate success across diverse tasks, as in AgentBench).\nCoordination (Multi-Agent or Modular): In scenarios where an Agentic AI system consists of multiple cooperating\nagents or modular components, coordination metrics gauge how effectively these parts work together. Good coordination\nmeans agents share information, divide labor without conflict or redundancy, and converge on solutions efficiently.\nQuantitative measures include the team success rate on collaborative tasks (whether the group of agents achieves the\noverall goal) and communication efficiency metrics (e.g., the number of messages or iterations required among agents\nto reach a decision, with fewer often indicating more efficient interaction).\nOne specific example is the Component Synergy Score (CSS) , which counts or weights effective interactions between\nagents, reflecting how well each agent’s actions complement the others (a higher CSS means agents are synergistic\nrather than working at cross-purposes). Multi-agent frameworks such as ChatDev and MetaGPT provide practical\ntestbeds for these metrics: they orchestrate specialized agents (e.g., different roles in a software engineering pipeline)\nthat must cooperate to complete complex projects.\nEvaluations on such frameworks examine whether agents maintain a consistent shared plan, handle inter-agent\ndependencies smoothly, and recover from misunderstandings. For instance, if one agent generates a plan and another\n16\n--- Page 17 ---\nAPREPRINT\nexecutes it, a coordination metric would assess if the executing agent follows the planner’s intent correctly and whether\nboth agents remain in agreement throughout the process. High coordination scores indicate that the agentic system\nfunctions as a cohesive unit, which is crucial for complex tasks beyond the capability of any single agent.\nComposite Metrics: Composite metrics aggregate multiple evaluation aspects into a single overall score. These are\nuseful for summarizing an agent’s performance holistically, especially when comparing different systems. A composite\nmetric is often a weighted combination of the above categories, for example:\nMcomposite =wTMT+wEME+wUMU+wCMC, (2)\nwhere MT, ME, MU, MCare the normalized scores (on a common scale) for trustworthiness, explainability, user-\ncentered performance, and coordination respectively, and wT, wE, wU, wCare weights reflecting the relative importance\nof each aspect for a given application. The choice of weights wican be domain-specific (for instance, in healthcare\napplications, trustworthiness and explainability might be weighted more heavily than raw efficiency). An example\nof a specialized composite metric in agentic contexts is the Tool Utilization Efficacy (TUE) score, which combines\nsub-metrics evaluating how correctly and efficiently an agent uses external tools (including proper tool selection and\ncorrect parameter usage in tool calls) into one measure. By condensing multiple criteria, composite metrics enable\nhigh-level comparison and benchmarking of agentic systems. For instance, AgentBench [ 161] is a comprehensive\nbenchmark that evaluates agents across a diverse range of tasks and environments (from operating system manipulation\nto web shopping), effectively providing a composite performance profile of an agent. Such aggregated scores highlight\nif an agent performs strongly across the board or if it excels in some dimensions while underperforming in others. It is\nimportant to interpret composite scores in light of their components: a single number can mask specific weaknesses\n(e.g., an agent might achieve a high overall score by doing well in task completion and coordination, yet still have poor\nexplainability). Therefore, composite metrics are most informative when accompanied by a breakdown of the agent’s\nperformance per category.\n7 Security and Privacy in LLM-based Agentic Multi-Agent Architectures\n7.1 Security Mechanisms\nAgentic AI systems, composed of loosely coupled yet collaboratively functioning LLM-based agents, introduce an\nexpanded attack surface relative to conventional AI agents [ 162]. Ensuring the security of such systems necessitates\na multi-layered defense architecture that addresses data protection, execution integrity, inter-agent communication,\nand model robustness [ 163]. Among the foundational techniques employed are encryption [164,165],access control\n[166,167],adversarial defense [168,169], and runtime monitoring [170,171] each adapted to the unique demands of\ndecentralized multi-agent environments.\nEncryption plays an important role in safeguarding data exchanged between agents, especially when sensitive or\nregulated content (e.g., healthcare records, financial data) is involved [ 165,49,172]. Agentic workflows often include\ninter-agent handoff of partially processed results, models, or prompts. Implementations such as SSL/TLS, homomorphic\nencryption [ 173,172], and secure enclaves (e.g., Intel SGX ) are increasingly integrated into Agentic AI pipelines to\nensure confidentiality across message-passing protocols.\nAccess control becomes crucial when orchestrators or shared memory modules manage permissions for agents with\ndistinct capabilities and responsibilities. For instance, in systems like AutoGen[ 19] and CrewAI [ 70] where agents\ntake on specialized roles (e.g., summarizer, planner, coder) [ 174], enforcing principle-of-least-privilege access prevents\nprivilege escalation and unauthorized tool invocation [ 19,170,175]. Agent-based access control policies often aligned\nwith Role-Based Access Control (RBAC) [ 176,177] and Attribute-Based Access Control (ABAC) [ 178,177] paradigms\ncan dynamically restrict which agents may access sensitive APIs, files, or memory buffers, based on contextual trust\nlevels [4].\nAdversarial robustness is a growing concern as LLM-based agents are susceptible to prompt injection, manipulation\nthrough poisoned tool outputs, or coordination disruption via malformed intermediate results [ 179,180]. Recent\nstudies have shown that multi-agent LLM frameworks can be destabilized by adversarially crafted outputs from one\ncompromised agent propagating misleading information to others [ 180,163,102,42,103,181]. Adversarial training\nmethods, such as input perturbation, reward shaping, and contrastive learning, can partially mitigate these vulnerabilities\n[182,102]. Integrating safety constraints and verifying tool responses before execution are also effective mitigation\nstrategies.\nhttps://www.intel.com/content/www/us/en/products/docs/accelerator-engines/\nsoftware-guard-extensions.html\n17\n--- Page 18 ---\nAPREPRINT\nRuntime monitoring systems support the detection of anomalous agent behaviors, especially in high-stakes domains\nlike automated healthcare or cybersecurity [ 183]. Log-based auditing, anomaly detection with LSTM or autoencoder-\nbased detectors, and trust scoring among agents are becoming essential components of real-time surveillance layers\n[184,185,114]. For example, Microsoft’s Copilot governance layers monitor anomalous agent behavior across sessions\nto ensure compliant execution and flag potentially harmful interactions.\nAs Agentic AI continues to expand into mission-critical domains, developing standardized, scalable security mechanisms\nwill be paramount. Future approaches must include zero-trust frameworks, secure multi-party computation, and formal\nverification of inter-agent protocols to ensure safe and resilient operation across decentralized autonomous agent\ncollectives.\n7.2 Privacy-Preserving Techniques\nThe decentralized and interactive nature of LLM-based Agentic AI systems introduces new challenges for preserving\nprivacy, especially as agents continuously communicate, access external data sources, and store episodic or shared\nmemory. To ensure data confidentiality and protect personally identifiable information (PII), Agentic AI systems must\nadopt robust privacy-preserving techniques such as differential privacy ,data minimization , and secure computation .\nDifferential privacy (DP) offers mathematically grounded guarantees by injecting statistical noise into outputs, ensuring\nthat individual user contributions cannot be re-identified. In multi-agent LLM systems, DP can be applied during\ntraining or at inference-time when agents exchange information. For example, Google’s implementation of DP in\nfederated learning frameworks can be extended to distributed agentic systems, where agents collaboratively train or\nfine-tune local models without exposing raw data [ 186,187]. DP-SGD and privacy budgets ( ϵ-differentials) can regulate\ninformation exposure during policy updates or collaborative planning in real-time decision-making agents.\nData minimization is another cornerstone of privacy preservation. Agentic AI systems can mitigate exposure risks by\nlimiting the scope, granularity, and duration of data collected or retained during task execution. For instance, temporary\nmemory buffers used in systems like ChatDev or ReAct-based pipelines are cleared once subgoals are completed,\npreventing persistent storage of unnecessary user data. Furthermore, anonymization and pseudony [ 187], techniques\ncan help remove identifying features before data is passed between agents or stored in shared memory repositories.\nSecure computation techniques including secure multi-party computation (SMPC), homomorphic encryption, and\ntrusted execution environments (TEEs) enable agents to perform computations over encrypted or obfuscated data\nwithout compromising privacy. In scenarios where agents collaborate across different organizational boundaries (e.g.,\nfederated medical agents or cross-silo industrial agents), SMPC allows joint computations such as diagnostics or\nanomaly detection without data leakage. Homomorphic encryption, while computationally expensive, is increasingly\nbeing explored to allow arithmetic operations on encrypted vectors used in RAG workflows.\nPrivacy-by-design principles are becoming central to the engineering of next-generation agentic systems. Architectures\nnow embed user consent layers, configurable privacy settings, and memory redaction modules that allow end-users or\nsystem administrators to control what agents can remember or share. As Agentic AI expands into domains such as\npersonalized education, healthcare, and finance, ensuring privacy-respecting behaviors will be essential for regulatory\ncompliance (e.g., GDPR, HIPAA) and public trust.\n7.3 Compliance and Governance\nAs Agentic AI systems grow in capability and autonomy, ensuring regulatory compliance and instituting robust\ngovernance mechanisms become imperative. Unlike traditional AI agents, agentic systems operate with greater\nautonomy, persistent memory, and complex decision flows necessitating layered oversight to manage legal, ethical,\nand societal implications. Effective governance in this context spans three critical dimensions: adherence to regulatory\nstandards, system-level auditability, and enforceable policy frameworks.\nRegulatory standards provide the baseline requirements that all AI systems, including agentic architectures, must\nmeet. Frameworks such as the NIST AI Risk Management Framework (AI RMF) and the EU AI Act define principles\nfor trustworthy AI including transparency, accountability, and fairness. These standards are especially relevant for\nLLM-based AMAS that interact in high-stakes domains like healthcare, finance, defense, or transportation. For\nexample, the EU AI Act classifies certain autonomous systems as “high-risk,” requiring continuous risk monitoring,\ndocumentation of decision logic, and human oversight mechanisms attributes directly relevant to Agentic AI.\nAuditability is critical in ensuring transparency and facilitating post-hoc accountability. Each decision, plan, or\ninteraction within an agentic system should be logged with timestamps, context, agent role, and justification. Techniques\nsuch as decision provenance andaction traceability enable this, allowing regulators or internal auditors to reconstruct\n18\n--- Page 19 ---\nAPREPRINT\nTable 8: Governance Dimensions for Agentic AI Systems\nGovernance Component Description Example Tools or Methods\nRegulatory Compliance Aligning with legal and industry standards for\nresponsible AI [188]NIST AI RMF [96], EU AI Act [189], GDPR [100],\nHIPAA [101]\nAuditability and Logging Capturing traceable records of agent actions\nand decisions [167]Decision provenance, immutable logs,\nblockchain-based traceability\nPolicy Enforcement Enforcing system-level operational, ethical,\nand security constraints [40, 190]RBAC/ABAC policies [191], formal logic rules,\ndynamic sandboxing [48, 192, 193]\nHuman Oversight Human-in-the-loop [139] decision\ncheckpoints and override capabilitiesHuman review agents, interactive dashboards,\ncompliance checkpoints\nRisk Monitoring Continuous identification and mitigation of\nsystemic or emergent risks [194]Model risk scanners, out-of-distribution detectors,\nreinforcement learning audit modules\nExplainability Governance Ensuring interpretability standards are met in\ndecision pipelines [195]LIME/SHAP [196, 197, 198], decision\ntrees [199, 200], interpretable surrogate\nmodels [201, 202, 203]\nAdaptive Governance Updating governance mechanisms based on\nmodel evolution and deployment\ncontext [104, 128]Governance-as-code [95, 204], auto-updated rule\nengines [205, 206, 207], learning-based policy\nadaptation [115, 208]\nIncident Response and\nRecoveryProcedures for handling, logging, and\nrecovering from failures or breachesReal-time alerting, kill-switch mechanisms, secure\nrollback protocols\nhow decisions were reached. For instance, in systems like AutoGen or MetaGPT, where agents assume specialized\nroles (e.g., researcher, coder, reviewer), audit trails can capture role-specific actions and flag inconsistencies, bias\namplification, or security violations. Blockchain-based audit logs are also being explored to ensure immutability and\nverifiability of multi-agent interactions.\nPolicy enforcement governs what agentic systems can and cannot do, and under which conditions. These policies must\nbe codified into the orchestration layers or meta-agent governance modules that manage agent interactions. Examples\ninclude enforcing memory expiration policies to avoid data retention violations or restricting access to external tools\nbased on role and authentication level. Role-based access control (RBAC) and attribute-based access control (ABAC)\nare essential for enforcing differentiated privileges across agent subcomponents. Furthermore, real-time monitoring\nsystems can halt or flag agent activity that deviates from pre-specified ethical constraints or operational bounds, using\nformal verification tools like TLA+ or symbolic execution engines.\nEmerging best practices also include the creation of AI governance boards , the adoption of governance-as-code\nplatforms, and the integration of adaptive governance layers that evolve as the agentic system scales or changes\ncontext. These practices are designed to meet not only current standards like ISO/IEC 42001 for AI management\nsystems, but also prepare for future regulatory evolutions. Below, we summarize the core governance components\nrequired for secure and compliant Agentic AI deployment.\nThus, governance in Agentic AI systems is not a static compliance checkbox but a dynamic, adaptive layer embedded\ninto the orchestration and operational pipelines. As these systems become more autonomous and integrated across\nsectors, aligning governance with system behavior, human values, and evolving legal standards is imperative for safe\nand trustworthy deployment, as summarized in Table 8.\n8 Discussion\nour exploration of TRiSM-based governance for LLM-powered Agentic AI systems reveals critical insights into\ntechnical design, ethical oversight, regulatory alignment, and future challenges. Below, we discuss the broader\nimplications of our findings, structured into key areas for clarity.\n8.1 Technical Implications of TRiSM for LLM Agent Design\nThe AI TRiSM framework (Trust, Risk, and Security Management) imposes concrete technical requirements on how\nautonomous LLM-driven agents are built and deployed. A core implication is the need to embed real-time monitoring\nand control mechanisms into agent architectures. Rather than treating LLMs agents as black-box decision-makers,\nTRiSM encourages instrumenting them with continuous oversight “guardrails” [ 209]. For example, there is discusison\non designing specialized “guardian agents” within AMAS [ 210]. In this paradigm, such agents serve as proactive\nmonitors that filter sensitive data and establish baselines of normal behavior, while operator agents dynamically enforce\n19\n--- Page 20 ---\nAPREPRINT\npolicies at runtime (e.g. blocking disallowed actions such as outputting personally identifiable information). This\nlayered control strategy transforms the technical architecture: an autonomous LLM agent is now supplemented by\nmeta-agents that supervise its inputs, outputs, and tool use in real time.\nPrior research highlights risks like “excessive agency” [ 87] where an LLM given too much autonomy or tool access can\nproduce unintended harmful actions (for instance, via hallucination or misinterpreted goals). TRiSM-driven agentic\ndesign mitigates these failure modes by constraining agent autonomy within well-defined safety bounds. Likewise,\nemerging threats specific to Agentic AI such as prompt injection [ 82] attacks, memory poisoning [ 126], or cascading\nhallucinations [ 211]underscore the need for built-in risk controls. By incorporating anomaly detection and policy-\nchecking modules (as per the Sentinel/Operator model), an LLM agent can detect deviations from normal behavior\nand either alert humans or automatically neutralize the threat (e.g. masking a sensitive datum or stopping an unsafe\naction). This aligns with calls in the security community for proactive measures in Agentic AI, blending traditional\ncybersecurity techniques (access control, logging, sandboxing) with AI-specific safeguards like LLM firewalls and\nadversarial robustness checks [212].\nIn summary, TRiSM’s technical implications mean that autonomous LLM agents should no longer be deployed as\nstand-alone intelligent actors; instead, they operate under an active governance fabric of monitors, validators, and\nenforcement agents that ensure trustworthiness and safety by design.\n8.2 Ethical and Societal Ramifications of Multi-Agent AI\nBeyond technical matters, deploying networks of autonomous LLM agents raises pressing ethical and societal questions.\nApplying TRiSM in this context emphasizes principles of accountability, human oversight, and fairness [ 188] all of\nwhich are vital for public trust in AI systems. A central concern is accountability: when AI agents make autonomous\ndecisions that affect humans, who is answerable for the outcomes? TRiSM-based governance insists that organizations\nretain clear responsibility for their AI’s actions, rather than obscuring blame behind algorithmic “black boxes”. This\nimplies implementing audit trails and explicable decision logs so that any harmful or biased outcome can be traced and\nattributed [ 148]. Recent guidance on trustworthy AI [ 190] often highlights accountability and explainability as key\npillars of trust.\nIn practice, our approach means each autonomous agent’s decisions should be transparent enough to be understood\nand challenged by human reviewers when necessary. Human oversight is another ethical imperative tightly coupled\nwith accountability. TRiSM does not seek to eliminate humans from the loop; rather, it provides a structured way for\nhumans and AI agents to collaborate under defined governance. Human operators or “AI managers” must have the\nability to intervene or override when an agent’s behavior deviates from acceptable bounds or when moral judgment is\nrequired. Indeed, high-level policy frameworks (such as the EU’s AI ethics guidelines [ 189]) explicitly call for “human\nagency and oversight” in AI systems.\nIn multi-agent setups, this may involve dashboard interfaces where humans can monitor agent swarms in real time,\npause or shut down agents exhibiting anomalies, and adjust policies on the fly [ 115]. The risk of “user complacency”,\ntrusting an autonomous agent too much , has been noted as a hazard [ 213]. TRiSM governance counteracts this by\nformalizing oversight roles and ensuring no AI operates without appropriate human or regulatory supervision. Fairness\nand bias mitigation are also critical societal considerations.\nOur governance approach therefore incorporates bias audits and fairness checks throughout the agent lifecycle [ 214].\nTechniques like pre-deployment bias testing, continuous monitoring for disparate impacts, and diverse stakeholder\nevaluation panels can be employed. These measures echo regulatory expectations; the EU’s AI Act [ 189] and related\nguidelines enumerate “diversity, non-discrimination and fairness” as core requirements for trustworthy AI. In deploying\nLLM-based agents, we must ensure they do not treat individuals or groups inequitably, for example, content-filtering\nagents should apply policies uniformly across demographic groups, and task-planning agents should not propagate\nhistorical biases in resource allocation decisions.\nIn sum, TRiSM-oriented governance extends beyond preventing technical failures: it seeks to uphold ethical norms and\nhuman rights, ensuring that autonomy in AI does not come at the expense of justice, transparency, or human dignity.\n8.3 Alignment with Emerging AI Regulations and Standards\nThe principles embedded in TRiSM align closely with emerging regulatory frameworks for AI. This convergence means\nthat adopting TRiSM-based governance can help organizations meet new legal obligations and industry standards.\nFor example, the European Union’s AI Act [ 215] (set to fully apply in 2026) mandates rigorous risk management,\ntransparency, data governance, and human oversight for “high-risk ” AI systems. These are precisely the capabilities\nthat a TRiSM approach cultivates.\n20\n--- Page 21 ---\nAPREPRINT\nBy instituting continuous risk assessment, documentation of AI decision processes, and oversight mechanisms, a\nTRiSM-governed multi-agent system inherently addresses many of the EU Act’s requirements (e.g. having a risk\nmanagement system and post-market monitoring for AI. Notably, the Act also stresses accuracy, robustness, and\ncybersecurity for high-risk AI, qualities that TRiSM’s security management component is designed to ensure (through\nadversarial resilience, access control, etc.). Similarly, international AI governance standards are emerging that mirror\nTRiSM’s tenets. ISO/IEC 42001:2023, the first global standard for AI management systems, highlights requirements\nsuch as transparency, accountability, bias mitigation, safety, and privacy in AI development\nTRiSM’s trust and risk management focus naturally encompasses these elements: for instance, trust in TRiSM relates\nto reliable, truthful outputs (promoting transparency), while explicit risk management aligns with accountability for\nnegative outcomes. By implementing TRiSM, organizations essentially put in place the processes that ISO 42001 [ 98]\nand similar standards call for (e.g. leadership oversight, documented risk controls, ongoing monitoring and improvement\ncycles). Another example is the U.S. NIST AI Risk Management Framework [ 96], which emphasizes many of the same\nconcepts: identifying risks, embedding governance, and cultivating trustworthiness in AI.\nBy following TRiSM guidelines: for example, maintaining an “AI catalog” of all models/agents in use and their\npurposes, enforcing policies via sentinel/operator agents, and logging every AI decision, organizations create an\naudit-ready environment. In the event of an incident or an inquiry by authorities, they can demonstrate traceability and\ncontrol over their autonomous agents, which will be crucial for regulatory compliance and liability management.\n8.4 Limitations and Current Research Gaps\nWhile the TRiSM-based approach appears promising, our work also revealed several limitations and open challenges in\ncurrent research.\nFirst, limited benchmark evaluations pose a problem. The AI safety and agent governance community lacks widely\naccepted benchmarks to quantitatively assess trustworthiness or risk in multi-agent LLM systems. Unlike classical AI\ndomains (vision, NLP) that have standard test suites, there is no consensus on how to measure an “AI agent” ability to\noperate safely under TRiSM principles. This makes it difficult to compare different governance strategies or to track\nprogress objectively. We encourage future work to develop evaluation frameworks, possibly extending from adversarial\nattack simulations [ 182] or “red-teaming” exercises [ 216] that can stress-test agentic systems and score their resilience\n(e.g. measuring success rates of prompt injection attacks or frequency of policy violations caught by oversight agents).\nSecondly, there is a paucity of real-world validation for many TRiSM-inspired controls. Much of the existing literature\nand tooling for LLM agent safety has been demonstrated in laboratory settings or on narrowly scoped tasks [ 47].\nIt remains uncertain how these governance mechanisms perform in complex, open-ended real-world environments.\nAdditionally, integrating TRiSM with legacy systems poses practical challenges , for example, the earlier work notes\ncompatibility issues when bolting on trust/security layers to existing AI pipelines. This suggests a limitation in how\neasily current AI deployments can fit TRiSM controls, a topic that merits further engineering research.\nAnother critical gap is adversarial robustness. As we improve defenses, attackers will inevitably adapt. Recent findings\nshow that LLM-based systems remain vulnerable to cleverly crafted attacks (for example, hidden prompt injections or\nsubtle data poisoning) that can bypass superficial guardrails [ 84]. TRiSM solutions today are not foolproof against\nthese. For example, an agent designed to mask secret data might itself be tricked into revealing it if the oversight logic\nfails to anticipate a new attack pattern. The literature identifies “evolving threats” and “adversarial attacks” as ongoing\nobstacles to trustworthy AI [ 210]. This underlines a need for continuous updates and adaptive security in any TRiSM\nimplementation.\nFinally, organizational and human factors present limitations: implementing TRiSM requires interdisciplinary expertise\n(AI specialists, security experts, ethicists, legal advisors) and clear governance structures. Many organizations lack the\nnecessary skill sets or frameworks, making TRiSM adoption superficial or inconsistent. Without a strong organizational\ncommitment, even the best technical framework can falter.\n8.5 Future Roadmap for Agentic AI TRiSM\nDrawing on our findings and best practices from multiple disciplines, we propose several actionable directions for\nfuture research and implementation, as shown in Figure 3. These recommendations span both technical system design\nimprovements and governance-level policy initiatives:\n•Develop Standardized Evaluation Benchmarks: The community should create open benchmarks and challenge\nenvironments to test multi-agent AI governance. For instance, a suite of scenario-based tasks (with built-in threats\nand ethical dilemmas) could be used to evaluate how well a TRiSM-governed agent system performs relative to one\n21\n--- Page 22 ---\nAPREPRINT\nFuture\nRoadmap\nfor Agentic AI\nScalable\nAgent Architectures\nSecure\nMulti-Agent\nCollaborationExplainable\nMulti-Agent DecisionsHuman-Centered\nTrust DesignAutonomous\nLifecycle ManagementEthics & GovernanceBenchmarking\n& Evaluation\nCognitive\nCapability Expansion\nFigure 3: Strategic roadmap for LLM-enabled Agentic AI, grouped into eight priority research and development\ndomains.\nwithout such controls. This will enable direct comparisons and drive progress on measurable metrics of trust (e.g.\nfrequency of prevented failures or fairness outcomes).\n•Advance Adversarial Robustness Techniques: Future system design must anticipate a continually evolving threat\nlandscape. Techniques from cybersecurity (e.g. adversarial training, AI model “penetration testing” [ 217], and\nformal verification) should be integrated into the LLM agent development pipeline. Cross-disciplinary collaboration\nwith security experts can yield LLM-specific hardening methods, such as dynamic prompt anomaly detectors or\nrobust tool APIs that constrain agent actions. Additionally, creating red-team/blue-team exercises for AMAS , akin\nto cyber wargames [218] , can help discover vulnerabilities in a controlled way before real adversaries do.\n•Human-Centered Oversight Tools: We encourage designing better interfaces and protocols for human oversight\nof Agentic AI. Borrowing from human-computer interaction [ 219]and cognitive engineering [ 220], researchers\ncould devise dashboards that visualize an agent society’s state, flag important decisions, and allow intuitive human\nintervention (pausing agents, rolling back actions, etc.).\n•Regulatory Sandboxes and Compliance-by-Design: Policymakers and industry should collaborate to create\nregulatory sandboxes for multi-agent AI trials. These would be controlled environments where innovators can\ndeploy Agentic AI under supervision, demonstrating TRiSM controls to regulators. Insights from such pilots can\ninform refinements in both technical standards and regulations. Moreover, adopting a compliance-by-design mindset\nis key: future AI system designs should bake in the requirements of frameworks like the EU AI Act and ISO 42001\nfrom the startrather than retrofit them.\n•Cross-Domain Best Practices and Ethical Governance: There is much to learn from other high-stakes domains.\nFor example, the safety engineering field (e.g. aerospace, automotive) has mature practices for redundant controls\nand failure mode analysis; these could inspire analogous practices in AI agent design. Likewise, ethics boards in\nbiomedical research provide a template for AI ethics committees that review agent behaviors and approve high-risk\ndeployments. We advocate establishing multidisciplinary governance boards that include ethicists, legal experts,\ndomain specialists, and community representatives to oversee significant deployments of autonomous AI.\n9 Conclusion\nTRiSM-based governance offers a promising scaffold to ensure that autonomous LLM-powered agents are trustworthy,\naccountable, and secure. Our discussion has analyzed how this framework influences technical design decisions,\nmandates ethical guardrails, and dovetails with emerging regulatory regimes. While current research is nascent and\n22\n--- Page 23 ---\nAPREPRINT\nnot without limitations, the path forward is clear. By rigorously testing these systems, strengthening them against\nadversaries, and crafting policies and standards in tandem with technological advances, we can enable powerful\nmulti-agent AI systems to operate beneficially under robust oversight. The stakes are high but with a proactive,\ninterdisciplinary approach, we can achieve a balance where innovation in AI goes hand-in-hand with responsibility and\ntrust. As future work tackles the open challenges identified, we anticipate that TRiSM principles will transition from\nconceptual best-practice to standard operating procedure for Agentic AI, ensuring these systems earn and maintain the\nconfidence of all stakeholders involved.\nReferences\n[1] Chris Miller. 30+ powerful ai agents statistics in 2025: Adoption & insights, 2025. Accessed: 2025-06-02.\n[2] Lyzr AI. The state of ai agents in enterprise: H1 2025, 2025. Accessed: 2025-06-02.\n[3] Stuart J Russell. Rationality and intelligence. Artificial intelligence , 94(1-2):57–77, 1997.\n[4]Ranjan Sapkota, Konstantinos I Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: A conceptual taxonomy,\napplications and challenge. arXiv preprint arXiv:2505.10468 , 2025.\n[5]I De Zarzà, J De Curtò, Gemma Roig, Pietro Manzoni, and Carlos T Calafate. Emergent cooperation and strategy\nadaptation in multi-agent systems: An extended coevolutionary theory with llms. Electronics , 12(12):2722, 2023.\n[6] Cristiano Castelfranchi. Modelling social action for ai agents. Artificial intelligence , 103(1-2):157–182, 1998.\n[7]Yingxuan Yang, Qiuying Peng, Jun Wang, and Weinan Zhang. Multi-llm-agent systems: Techniques and business\nperspectives. arXiv preprint arXiv:2411.14033 , 2024.\n[8]Ivey Chiu and LH Shu. Biomimetic design through natural language analysis to facilitate cross-domain\ninformation retrieval. Ai Edam , 21(1):45–59, 2007.\n[9]Michael S Lew, Nicu Sebe, Chabane Djeraba, and Ramesh Jain. Content-based multimedia information retrieval:\nState of the art and challenges. ACM Transactions on Multimedia Computing, Communications, and Applications\n(TOMM) , 2(1):1–19, 2006.\n[10] Doru Tanasa and Brigitte Trousse. Advanced data preprocessing for intersites web usage mining. IEEE Intelligent\nSystems , 19(2):59–65, 2005.\n[11] David DeVault, Ron Artstein, Grace Benn, Teresa Dey, Ed Fast, Alesia Gainer, Kallirroi Georgila, Jon Gratch,\nArno Hartholt, Margaux Lhommet, et al. Simsensei kiosk: A virtual human interviewer for healthcare decision\nsupport. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems ,\npages 1061–1068, 2014.\n[12] Dapeng Hao, Tao Ai, Frank Goerner, Xuemei Hu, Val M Runge, and Michael Tweedle. Mri contrast agents:\nbasic chemistry and safety. Journal of Magnetic Resonance Imaging , 36(5):1060–1071, 2012.\n[13] David Griol, Javier Carbó, and José M Molina. An automatic dialog simulation technique to develop and evaluate\ninteractive conversational agents. Applied Artificial Intelligence , 27(9):759–780, 2013.\n[14] Michael R Genesereth and Nils J Nilsson. Logical foundations of artificial intelligence . Morgan Kaufmann,\n2012.\n[15] Pieter Spronck, Marc Ponsen, Ida Sprinkhuizen-Kuyper, and Eric Postma. Adaptive game ai with dynamic\nscripting. Machine Learning , 63:217–248, 2006.\n[16] Daniel Szafir and Bilge Mutlu. Pay attention! designing adaptive agents that monitor and improve user\nengagement. In Proceedings of the SIGCHI conference on human factors in computing systems , pages 11–20,\n2012.\n[17] Deepak Bhaskar Acharya, Karthigeyan Kuppan, and B Divya. Agentic ai: Autonomous intelligence for complex\ngoals–a comprehensive survey. IEEE Access , 2025.\n[18] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su,\nXin Cong, et al. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924 ,\n2023.\n[19] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\nZhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint\narXiv:2308.08155 , 2023.\n[20] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven\nKa Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative\nframework. arXiv preprint arXiv:2308.00352 , 3(4):6, 2023.\n23\n--- Page 24 ---\nAPREPRINT\n[21] Bernard P Zeigler. Object-oriented simulation with hierarchical, modular models: intelligent agents and\nendomorphic systems . Academic press, 2014.\n[22] Mei Yii Lim. Memory models for intelligent social companions. In Human-computer interaction: The agency\nperspective , pages 241–262. Springer, 2012.\n[23] Alice Gomstyn and Alexandra Jonker. What is ai trism?, March 2025. Accessed: 2025-06-02.\n[24] Adib Habbal, Mohamed Khalif Ali, and Mustafa Ali Abuzaraida. Artificial intelligence trust, risk and security\nmanagement (ai trism): Frameworks, applications, challenges and future research directions. Expert Systems\nwith Applications , 240:122442, 2024.\n[25] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V . Chawla, Olaf Wiest, and\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024.\n[26] Shuaihang Chen, Yuanxing Liu, Wei Han, Weinan Zhang, and Ting Liu. A survey on llm-based multi-agent\nsystem: Recent advances and new frontiers in application, 2025.\n[27] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and Chaozhuo Li. Beyond\nself-talk: A communication-centric survey of llm-based multi-agent systems, 2025.\n[28] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D. Nguyen.\nMulti-agent collaboration mechanisms: A survey of llms, 2025.\n[29] Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung yi Lee, and\nYun-Nung Chen. Creativity in llm-based multi-agent systems: A survey, 2025.\n[30] Xingli Fang, Jianwei Li, Varun Mulchandani, and Jung-Eun Kim. Trustworthy ai on safety, bias, and privacy: A\nsurvey, 2025.\n[31] Angela Boland, Gemma Cherry, and Rumona Dickson. Doing a systematic review: a student s guide. 2017.\n[32] Staffs Keele et al. Guidelines for performing systematic literature reviews in software engineering. Technical\nreport, Technical report, ver. 2.3 ebse technical report. ebse, 2007.\n[33] Barbara Kitchenham. Procedures for performing systematic reviews. Keele, UK, Keele University , 33(2004):1–26,\n2004.\n[34] Nicola Muscettola, P Pandurang Nayak, Barney Pell, and Brian C Williams. Remote agent: To boldly go where\nno ai system has gone before. Artificial intelligence , 103(1-2):5–47, 1998.\n[35] Markus Hannebauer. From formal workflow models to intelligent agents. In Proceedings of the AAAI-99\nWorkshop on Agent Based Systems in the Business Context , pages 19–24, 1999.\n[36] Justin Cranshaw, Emad Elwany, Todd Newman, Rafal Kocielnik, Bowen Yu, Sandeep Soni, Jaime Teevan, and\nAndrés Monroy-Hernández. Calendar. help: Designing a workflow-based scheduling agent with humans in the\nloop. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , pages 2382–2393,\n2017.\n[37] Paul J Govaerts, Bart Vaerenberg, Geert De Ceulaer, Kristin Daemers, Carina De Beukelaer, and Karen Schauwers.\nDevelopment of a software tool using deterministic logic for the optimization of cochlear implant processor\nprogramming. Otology & Neurotology , 31(6):908–918, 2010.\n[38] Uwe M Borghoff, Paolo Bottoni, and Remo Pareschi. Human-artificial interaction in the age of agentic ai: a\nsystem-theoretical approach. Frontiers in Human Dynamics , 7:1579166, 2025.\n[39] Johannes Schneider. Generative to agentic ai: Survey, conceptualization, and challenges. arXiv preprint\narXiv:2504.18875 , 2025.\n[40] Christian Schroeder de Witt. Open challenges in multi-agent security: Towards secure systems of interacting ai\nagents. arXiv preprint arXiv:2505.02077 , 2025.\n[41] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie,\nFei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information\nProcessing Systems , 37:114843–114871, 2024.\n[42] Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo\nYan, Hanjun Luo, et al. A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment.\narXiv preprint arXiv:2504.15585 , 2025.\n[43] April Speight. Build ai agents end-to-end in vs code. https://techcommunity.microsoft.com/blog/\nazuredevcommunityblog/build-ai-agents-end-to-end-in-vs-code/4418117 , May 2025. Microsoft\nDeveloper Community Blog.\n24\n--- Page 25 ---\nAPREPRINT\n[44] Danny Weyns, Alexander Helleboogh, Tom Holvoet, and Michael Schumacher. The agent environment in\nmulti-agent systems: A middleware perspective. Multiagent and Grid Systems , 5(1):93–108, 2009.\n[45] Microsoft Corporation. Langgraph: Agent orchestration framework for llms. https://www.langchain.com/\nlanggraph , 2024. Accessed: 2025-06-03.\n[46] Jagreet Kaur. Building trust with ai trism: Managing risks in the era of agentic ai. Akira AI Blog , December\n2024. Accessed: 2025-06-03.\n[47] CharliAI Inc. Trust, Risk and Security Management in AI Systems. https://charliai.com/wp-content/\nuploads/2024/01/TRiSM-Position-Paper-January-2024.pdf , January 2024. Accessed: 2025-06-03.\n[48] Bingzhuo Zhong, Hongpeng Cao, Majid Zamani, and Marco Caccamo. Towards safe ai: Sandboxing dnns-based\ncontrollers in stochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37,\npages 15340–15349, 2023.\n[49] Georgios Feretzakis, Konstantinos Papaspyridis, Aris Gkoulalas-Divanis, and Vassilios S Verykios. Privacy-\npreserving techniques in generative ai and large language models: A narrative review. Information , 15(11):697,\n2024.\n[50] Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, and Mark O Riedl. Explainable reinforcement learning\nagents using world models. arXiv preprint arXiv:2505.08073 , 2025.\n[51] Elvis Saravia. Llm agents. https://www.promptingguide.ai/research/llm-agents , 2024. Accessed:\n2025-06-03.\n[52] Significant Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT , 2023. Accessed:\n2025-06-03.\n[53] Yohei Nakajima. Babyagi (archived version). https://github.com/yoheinakajima/babyagi_archive ,\n2024. Accessed: 2025-06-03.\n[54] Anton Osika. gpt-engineer. https://github.com/AntonOsika/gpt-engineer , 2023. Accessed: 2025-06-\n03.\n[55] Elvis Saravia. Prompt engineering guide. https://www.promptingguide.ai/ , 2024. Accessed: 2025-06-03.\n[56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models, 2023.\n[57] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.\n[58] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata,\nYoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-\nShwartz, Amnon Shashua, and Moshe Tenenholtz. Mrkl systems: A modular, neuro-symbolic architecture that\ncombines large language models, external knowledge sources and discrete reasoning, 2022.\n[59] Kaiming Liu, Xuanyu Lei, Ziyue Wang, Peng Li, and Yang Liu. Agent-environment alignment via automated\ninterface generation. arXiv preprint arXiv:2505.21055 , 2025.\n[60] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional\nopinions. arXiv preprint arXiv:2306.02224 , 2023.\n[61] Anton Osika. gpt-engineer: Cli platform to experiment with codegen. https://github.com/AntonOsika/\ngpt-engineer , 2023. MIT License.\n[62] Jialin Wang and Zhihua Duan. Agent ai with langgraph: A modular framework for enhancing machine translation\nusing large language models. arXiv preprint arXiv:2412.03801 , 2024.\n[63] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion: Language agents with verbal reinforcement learning, 2023.\n[64] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and\nJürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth\nInternational Conference on Learning Representations , 2024.\n[65] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima\nAnandkumar. V oyager: An open-ended embodied agent with large language models, 2023.\n[66] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu.\nWebvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919 ,\n2024.\n25\n--- Page 26 ---\nAPREPRINT\n[67] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving\nai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems ,\n36:38154–38180, 2023.\n[68] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:\nCommunicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on\nNeural Information Processing Systems , 2023.\n[69] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su,\nXin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software\ndevelopment, 2024.\n[70] João Moura and contributors. Crewai: Framework for orchestrating role-playing, autonomous ai agents.\nhttps://github.com/crewAIInc/crewAI , 2023. MIT License.\n[71] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi\nLu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in\nagents. arXiv preprint arXiv:2308.10848 , 2023.\n[72] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian\nLiu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents:\nAn open platform for language agents in the wild, 2023.\n[73] TransformerOptimus and contributors. Superagi: A dev-first open source autonomous ai agent framework.\nhttps://github.com/TransformerOptimus/SuperAGI , 2023. MIT License.\n[74] Avivah Litan. Ai trust and ai risk: Tackling trust, risk and security in ai models. Gartner , December 2024.\nAccessed: 2025-06-03.\n[75] Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, and Wonyong Shin.\nSeven security challenges that must be solved in cross-domain multi-agent llm systems, 2025.\n[76] Avi Rosenfeld and Ariella Richardson. Explainability in human–agent systems. Autonomous agents and\nmulti-agent systems , 33:673–705, 2019.\n[77] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions\nof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining , pages 1135–1144, 2016.\n[78] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings of the\n31st International Conference on Neural Information Processing Systems , pages 4765–4774. Curran Associates\nInc., 2017.\n[79] Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances in Neural\nInformation Processing Systems , 2017-Decem:4067–4077, 2017.\n[80] Kay Lefevre, Chetan Arora, Kevin Lee, Arkady Zaslavsky, Mohamed Reda Bouadjenek, Ali Hassani, and Imran\nRazzak. Modelops for enhanced decision-making and governance in emergency control rooms. Environment\nSystems and Decisions , 42(3):402–416, September 2022.\n[81] Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl. Machine learning operations (mlops): Overview,\ndefinition, and architecture, 2022.\n[82] Matthew Kosinski and Amber Forrest. What is a prompt injection attack? IBM Think , March 2024. Accessed:\n2025-06-03.\n[83] Donghyun Lee and Mo Tiwari. Prompt infection: Llm-to-llm prompt injection within multi-agent systems, 2024.\n[84] Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, and Christos Emmanouilidis. Just\nas humans need vaccines, so do models: Model immunization to combat falsehoods, 2025.\n[85] Jay Chen and Royce Lu. Ai agents are here. so are the threats. Unit 42 Blog , May 2025. Accessed: 2025-06-03.\n[86] Ante Gojsali ´c. System prompt hardening: The backbone of automated ai security. SplxAI Blog , December 2024.\nAccessed: 2025-06-03.\n[87] OWASP Agentic Security Initiative. Agentic ai – threats and mitigations. Technical report, Open Worldwide\nApplication Security Project (OWASP), February 2025. Accessed: 2025-06-03.\n[88] LangChain Team. Langchain and langgraph: Comparing function and tool calling capabilities. LangChain Blog ,\nApril 2024. Accessed: 2025-06-03.\n[89] Microsoft Open Source. Agents — autogen agentchat user guide. https://microsoft.github.io/autogen/\nstable//user-guide/agentchat-user-guide/tutorial/agents.html , 2024. Accessed: 2025-06-02.\n26\n--- Page 27 ---\nAPREPRINT\n[90] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data\nanalysis. In Theory of Cryptography Conference , pages 265–284. Springer, 2006.\n[91] Latanya Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness\nand Knowledge-Based Systems , 10(05):557–570, 2002.\n[92] Yehuda Lindell and Benny Pinkas. Secure two-party computation via cut-and-choose oblivious transfer. In\nAdvances in Cryptology—EUROCRYPT 2007 , pages 329–346. Springer, 2007.\n[93] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the 41st annual ACM\nsymposium on Theory of computing , pages 169–178. ACM, 2009.\n[94] Victor Costan and Srinivas Devadas. Intel sgx explained. In IACR Cryptology ePrint Archive , volume 2016,\npage 86, 2016.\n[95] Google AI. Responsible AI practices. \\url{https://ai.google/responsibilities/responsible-ai-practices/}, 2021.\n[96] Ravit Dotan, Borhane Blili-Hamelin, Ravi Madhavan, Jeanna Matthews, and Joshua Scarpino. Evolving ai risk\nmanagement: A maturity model based on the nist ai risk management framework, 2024.\n[97] International Organization for Standardization. Iso/iec tr 24029-1:2021 – artificial intelligence (ai) – assessment\nof the robustness of neural networks – part 1: Overview. Technical report, ISO/IEC, 2021. Available at\nhttps://www.iso.org/standard/77608.html .\n[98] International Organization for Standardization. Iso/iec 42001:2023 – artificial intelligence management system\n(ai ms) – requirements. Technical report, ISO/IEC, 2023. Available at https://www.iso.org/standard/\n81230.html .\n[99] Open Worldwide Application Security Project (OWASP). OWASP Top 10 for Large Language Model Ap-\nplications. https://owasp.org/www-project-top-10-for-large-language-model-applications/ ,\n2024. Accessed: 2025-06-03.\n[100] European Union. General Data Protection Regulation (GDPR) – Article 25: Data protection by design and by\ndefault. https://gdpr-info.eu/art-25-gdpr/ , 2016. Accessed: 2025-06-03.\n[101] U.S. Department of Health and Human Services. HIPAA Privacy Rule – 45 CFR Part 164: Security and\nPrivacy Protections for Health Information. https://www.ecfr.gov/current/title-45/subtitle-A/\nsubchapter-C/part-164 , 2003. Accessed: 2025-06-03.\n[102] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. Ai\nagents under threat: A survey of key security challenges and future pathways. ACM Computing Surveys ,\n57(7):1–36, 2025.\n[103] Atharv Singh Patlan, Peiyao Sheng, S Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath. Real ai agents\nwith fake memories: Fatal context manipulation attacks on web3 agents. arXiv preprint arXiv:2503.16248 , 2025.\n[104] Noam Kolt. Governing ai agents. arXiv preprint arXiv:2501.07913 , 2025.\n[105] Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler\nSmith, Wolfram Barfuss, Jakob Foerster, Tomáš Gaven ˇciak, et al. Multi-agent risks from advanced ai. arXiv\npreprint arXiv:2502.14143 , 2025.\n[106] Robb Wilson and Josh Tyson. Age of Invisible Machines: A Guide to Orchestrating AI Agents and Making\nOrganizations More Self-Driving, Revised and Updated . John Wiley & Sons, 2025.\n[107] Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong,\nand Jeff Z Pan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint\narXiv:2505.00675 , 2025.\n[108] Chad DeChant. Episodic memory in ai agents poses risks that should be studied and mitigated. arXiv preprint\narXiv:2501.11739 , 2025.\n[109] Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, and Yi Yang. Autonomous llm-enhanced adversarial attack for\ntext-to-motion. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pages 6144–6152,\n2025.\n[110] Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li,\nJunqi Zhang, Wenbo Zhou, et al. Compromising llm driven embodied agents with contextual backdoor attacks.\nIEEE Transactions on Information Forensics and Security , 2025.\n[111] Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, and Shing-Chi\nCheung. Ip leakage attacks targeting llm-based multi-agent systems. arXiv preprint arXiv:2505.12442 , 2025.\n27\n--- Page 28 ---\nAPREPRINT\n[112] Junyuan Mao, Fanci Meng, Yifan Duan, Miao Yu, Xiaojun Jia, Junfeng Fang, Yuxuan Liang, Kun Wang, and\nQingsong Wen. Agentsafe: Safeguarding large language model-based multi-agent systems via hierarchical data\nmanagement. arXiv preprint arXiv:2503.04392 , 2025.\n[113] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D Nguyen.\nMulti-agent collaboration mechanisms: A survey of llms. arXiv preprint arXiv:2501.06322 , 2025.\n[114] Minrui Xu, Jiani Fan, Xinyu Huang, Conghao Zhou, Jiawen Kang, Dusit Niyato, Shiwen Mao, Zhu Han,\nKwok-Yan Lam, et al. Forewarned is forearmed: A survey on large language model-based agents in autonomous\ncyberattacks. arXiv preprint arXiv:2505.12786 , 2025.\n[115] Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, and Gang Wang.\nMulti-agent embodied ai: Advances and future directions. arXiv preprint arXiv:2505.05108 , 2025.\n[116] Jiaxun Cui, Chen Tang, Jarrett Holtz, Janice Nguyen, Alessandro G Allievi, Hang Qiu, and Peter Stone.\nTowards natural language communication for cooperative autonomous driving via self-play. arXiv preprint\narXiv:2505.18334 , 2025.\n[117] Chilukuri Riktha Reddy, B Madhavi Devi, Kura Pranav Reddy, and Lahari Medarametla. Enhancing mental\nwellbeing among college students using autogen. In 2025 IEEE International Students’ Conference on Electrical,\nElectronics and Computer Science (SCEECS) , pages 1–7. IEEE, 2025.\n[118] Peter Cihon, Merlin Stein, Gagan Bansal, Sam Manning, and Kevin Xu. Measuring ai agent autonomy: Towards\na scalable approach with code inspection. arXiv preprint arXiv:2502.15212 , 2025.\n[119] Herbert Dawid, Philipp Harting, Hankui Wang, Zhongli Wang, and Jiachen Yi. Agentic workflows for economic\nresearch: Design and implementation. arXiv preprint arXiv:2504.09736 , 2025.\n[120] Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M Zhang, Elizabeth Black, Micheal Luck, Philip Torr,\nand Michael Wooldridge. Large language models miss the multi-agent mark. arXiv preprint arXiv:2505.21298 ,\n2025.\n[121] Md Monjurul Karim, Dong Hoang Van, Sangeen Khan, Qiang Qu, and Yaroslav Kholodov. Ai agents meet\nblockchain: A survey on secure and scalable collaboration for multi-agents. Future Internet , 17(2):57, 2025.\n[122] Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang,\nHanjun Luo, et al. Audiotrust: Benchmarking the multifaceted trustworthiness of audio large language models.\narXiv preprint arXiv:2505.16211 , 2025.\n[123] Divyansh Agarwal, Alexander Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, and Chien-Sheng Wu. Prompt\nleakage effect and mitigation strategies for multi-turn LLM applications. In Franck Dernoncourt, Daniel Preo¸ tiuc-\nPietro, and Anastasia Shimorina, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track , pages 1255–1275, Miami, Florida, US, November 2024. Association for\nComputational Linguistics.\n[124] V olker Strobel, Marco Dorigo, and Mario Fritz. Llm2swarm: Robot swarms that responsively reason, plan, and\ncollaborate through llms. In NeurIPS 2024 Workshop on Open-World Agents (OWA-2024) , 2024.\n[125] Antoine Ligot and Mauro Birattari. On using simulation to predict the performance of robot swarms. Scientific\nData , 9(1):788, 2022.\n[126] Conor Atkins, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Ian Wood, and Mohamed Ali Kaafar. Those\naren’t your memories, they’re somebody else’s: Seeding misinformation in chat bot memories. arXiv preprint\narXiv:2304.05371 , 2023.\n[127] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\nLost in the middle: How language models use long contexts, 2023.\n[128] Laurie Hughes, Yogesh K Dwivedi, Tegwen Malik, Mazen Shawosh, Mousa Ahmed Albashrawi, Il Jeon, Vincent\nDutot, Mandanna Appanderanda, Tom Crick, Rahul De’, et al. Ai agents and agentic systems: a multi-expert\nanalysis. Journal of Computer Information Systems , pages 1–29, 2025.\n[129] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer\nScience , 18(6):186345, 2024.\n[130] Alireza Ghafarollahi and Markus J Buehler. Sciagents: Automating scientific discovery through multi-agent\nintelligent graph reasoning. arXiv preprint arXiv:2409.05556 , 2024.\n[131] Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Meta-design matters: A\nself-design multi-agent system. arXiv preprint arXiv:2505.14996 , 2025.\n28\n--- Page 29 ---\nAPREPRINT\n[132] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. Chem-\ncrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376 , 2023.\n[133] Diana-Margarita Córdova-Esparza. Ai-powered educational agents: Opportunities, innovations, and ethical\nchallenges. Information , 16(6):469, 2025.\n[134] Wadim Strielkowski, Veronika Grebennikova, Alexander Lisovskiy, Guzalbegim Rakhimova, and Tatiana\nVasileva. Ai-driven adaptive learning for sustainable educational transformation. Sustainable Development ,\n33(2):1921–1947, 2025.\n[135] Michael Kirchhof, Gjergji Kasneci, and Enkelejda Kasneci. Position: Uncertainty quantification needs reassess-\nment for large-language model agents. arXiv preprint arXiv:2505.22655 , 2025.\n[136] Han Wang, An Zhang, Nguyen Duy Tai, Jun Sun, Tat-Seng Chua, et al. Ali-agent: Assessing llms’ alignment with\nhuman values via agent-based evaluation. Advances in Neural Information Processing Systems , 37:99040–99088,\n2024.\n[137] Jam Kraprayoon, Zoe Williams, and Rida Fayyaz. Ai agent governance: A field guide. arXiv preprint\narXiv:2505.21808 , 2025.\n[138] Joseph Sifakis, Dongming Li, Hairong Huang, Yong Zhang, Wenshuan Dang, River Huang, and Yijun Yu. A\nreference architecture for autonomous networks: An agent-based approach. arXiv preprint arXiv:2503.12871 ,\n2025.\n[139] Hongpeng Chen, Shufei Li, Junming Fan, Anqing Duan, Chenguang Yang, David Navarro-Alarcon, and Pai\nZheng. Human-in-the-loop robot learning for smart manufacturing: A human-centric perspective. IEEE\nTransactions on Automation Science and Engineering , 2025.\n[140] Isabella Seeber, Lena Waizenegger, Stefan Seidel, Stefan Morana, Izak Benbasat, and Paul Benjamin Lowry.\nCollaborating with technology-based autonomous agents: Issues and research opportunities. Internet Research ,\n30(1):1–18, 2020.\n[141] Amit K Shukla, Vagan Terziyan, and Timo Tiihonen. Ai as a user of ai: Towards responsible autonomy. Heliyon ,\n10(11), 2024.\n[142] Giulio Sandini, Alessandra Sciutti, and Pietro Morasso. Artificial cognition vs. artificial intelligence for next-\ngeneration autonomous robotic agents. Frontiers in Computational Neuroscience , 18:1349408, 2024.\n[143] Jose Tupayachi, Haowen Xu, Olufemi A Omitaomu, Mustafa Can Camur, Aliza Sharmin, and Xueping Li.\nTowards next-generation urban decision support systems through ai-powered construction of scientific ontology\nusing large language models—a case in optimizing intermodal freight transportation. Smart Cities , 7(5):2392–\n2421, 2024.\n[144] Jiaru Bai, Kok Foong Lee, Markus Hofmeister, Sebastian Mosbach, Jethro Akroyd, and Markus Kraft. A derived\ninformation framework for a dynamic knowledge graph and its application to smart cities. Future Generation\nComputer Systems , 152:112–126, 2024.\n[145] Fangqiao Tian, An Luo, Jin Du, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Jiawei Zhou, Jayanth\nSrinivasa, Ashish Kundu, et al. An outlook on the opportunities and challenges of multi-agent ai systems. arXiv\npreprint arXiv:2505.18397 , 2025.\n[146] Shusen Liu, Haichao Miao, Zhimin Li, Matthew Olson, Valerio Pascucci, and P-T Bremer. Ava: Towards\nautonomous visualization agents through visual perception-driven decision-making. In Computer Graphics\nForum , volume 43, page e15093. Wiley Online Library, 2024.\n[147] Kai Hu, Keer Xu, Qingfeng Xia, Mingyang Li, Zhiqiang Song, Lipeng Song, and Ning Sun. An overview:\nAttention mechanisms in multi-agent reinforcement learning. Neurocomputing , page 128015, 2024.\n[148] Hanchi Gu, Marco Schreyer, Kevin Moffitt, and Miklos Vasarhelyi. Artificial intelligence co-piloted auditing.\nInternational Journal of Accounting Information Systems , 54:100698, 2024.\n[149] Jia Hui Chin, Pu Zhang, Yu Xin Cheong, and Jonathan Pan. Automating security audit using large language\nmodel based agent: An exploration experiment. arXiv preprint arXiv:2505.10732 , 2025.\n[150] Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu. A survey on\ninterpretable reinforcement learning. Machine Learning , 113(8):5847–5890, 2024.\n[151] Mostafa Yossef, Mohamed Noureldin, and Aghyad Alqabbany. Explainable artificial intelligence framework for\nfrp composites design. Composite Structures , 341:118190, 2024.\n[152] Bhanu Chander, Chinju John, Lekha Warrier, and Kumaravelan Gopalakrishnan. Toward trustworthy artificial\nintelligence (tai) in the context of explainability and robustness. ACM Computing Surveys , 57(6):1–49, 2025.\n29\n--- Page 30 ---\nAPREPRINT\n[153] Bingcheng Wang, Tianyi Yuan, and Pei-Luen Patrick Rau. Effects of explanation strategy and autonomy of\nexplainable ai on human–ai collaborative decision-making. International Journal of Social Robotics , 16(4):791–\n810, 2024.\n[154] Atul Rawal, Adrienne Raglin, Danda B Rawat, Brian M Sadler, and James McCoy. Causality for trustworthy\nartificial intelligence: status, challenges and perspectives. ACM Computing Surveys , 57(6):1–30, 2025.\n[155] Yunxiang Qiu, Yuting Tang, Liangguo Chen, Shuyu Jiang, Ying Huang, and Xingshu Chen. Reinforcement\nlearning-driven temporal knowledge graph reasoning for secure data provenance in distributed networks. Peer-to-\nPeer Networking and Applications , 18(4):1–16, 2025.\n[156] Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. Generative ai for\nvisualization: State of the art and future directions. Visual Informatics , 2024.\n[157] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation\nusing gpt-4 with better human alignment, 2023.\n[158] Cristian Munoz, Kleyton da Costa, and Franklin Cardenoso Fernandez. Enhancing transparency in ai: Explain-\nability metrics for machine learning predictions. Holistic AI Blog , 2024.\n[159] Chirag Agarwal, Dan Ley, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri,\nMarinka Zitnik, and Himabindu Lakkaraju. Openxai: Towards a transparent evaluation of model explanations,\n2024.\n[160] Kerry Rodden, Hilary Hutchinson, and Xin Fu. Measuring the user experience on a large scale: user-centered\nmetrics for web applications. In Proceedings of the SIGCHI conference on human factors in computing systems ,\npages 2395–2398, 2010.\n[161] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men,\nKejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688 , 2023.\n[162] Raihan Khan, Sayak Sarkar, Sainik Kumar Mahata, and Edwin Jose. Security threats in agentic ai system. arXiv\npreprint arXiv:2410.14728 , 2024.\n[163] Yuntao Wang, Yanghe Pan, Quan Zhao, Yi Deng, Zhou Su, Linkang Du, and Tom H Luan. Large model agents:\nState-of-the-art, cooperation paradigms, security and privacy, and future trends. arXiv preprint arXiv:2409.14457 ,\n2024.\n[164] Yifeng He, Ethan Wang, Yuyang Rong, Zifei Cheng, and Hao Chen. Security of ai agents. arXiv preprint\narXiv:2406.08689 , 2024.\n[165] Sumeet Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip Torr, Lewis Hammond, and\nChristian Schroeder de Witt. Secret collusion among ai agents: Multi-agent deception via steganography.\nAdvances in Neural Information Processing Systems , 37:73439–73486, 2024.\n[166] Ken Huang, Vineeth Sai Narajala, Idan Habler, and Akram Sheriff. Agent name service (ans): A universal\ndirectory for secure ai agent discovery and interoperability. arXiv preprint arXiv:2505.10609 , 2025.\n[167] Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Deslandes Whitney, Dazza Green-\nwood, Alan Chan, and Alex Pentland. Authenticated delegation and authorized ai agents. arXiv preprint\narXiv:2501.09674 , 2025.\n[168] Ghadeer Ghazi Shayea, Mohd Hazli Mohammed Zabil, Mustafa Abdulfattah Habeeb, Yahya Layth Khaleel, and\nAS Albahri. Strategies for protection against adversarial attacks in ai models: An in-depth review. Journal of\nIntelligent Systems , 34(1):20240277, 2025.\n[169] Jasmita Malik, Raja Muthalagu, and Pranav M Pawar. A systematic review of adversarial machine learning\nattacks, defensive controls and technologies. IEEE Access , 2024.\n[170] Vineeth Sai Narajala and Om Narayan. Securing agentic ai: A comprehensive threat model and mitigation\nframework for generative ai agents. arXiv preprint arXiv:2504.19956 , 2025.\n[171] Chi-Min Chan, Jianxuan Yu, Weize Chen, Chunyang Jiang, Xinyu Liu, Weijie Shi, Zhiyuan Liu, Wei Xue, and\nYike Guo. Agentmonitor: A plug-and-play framework for predictive and secure multi-agent systems. arXiv\npreprint arXiv:2408.14972 , 2024.\n[172] RM Bommi, TJ Nandhini, et al. Enhancing healthcare data security and privacy through ai-driven encryption\nand privacy-preserving techniques. In 2025 International Conference on Data Science, Agents & Artificial\nIntelligence (ICDSAAI) , pages 1–6. IEEE, 2025.\n[173] I Jency and RP Anto Kumar. Finquaxbot: enhancing trust and security in personalized investment and tax\nforecasting using homomorphic encryption and meta-reinforcement learning with explainability. Expert Systems\nwith Applications , page 128136, 2025.\n30\n--- Page 31 ---\nAPREPRINT\n[174] Will Epperson, Gagan Bansal, Victor C Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema Amershi.\nInteractive debugging and steering of multi-agent ai systems. In Proceedings of the 2025 CHI Conference on\nHuman Factors in Computing Systems , pages 1–15, 2025.\n[175] Ismail, Rahmat Kurnia, Zilmas Arjuna Brata, Ghitha Afina Nelistiani, Shinwook Heo, Hyeongon Kim, and\nHowon Kim. Toward robust security orchestration and automated response in security operations centers with a\nhyper-automation approach using agentic artificial intelligence. Information , 16(5):365, 2025.\n[176] Javed Akhtar Khan. Role-based access control (rbac) and attribute-based access control (abac). In Improving\nsecurity, privacy, and trust in cloud computing , pages 113–126. IGI Global Scientific Publishing, 2024.\n[177] Ken Huang, Vineeth Sai Narajala, John Yeoh, Ramesh Raskar, Youssef Harkati, Jerry Huang, Idan Habler, and\nChris Hughes. A novel zero-trust identity framework for agentic ai: Decentralized authentication and fine-grained\naccess control. arXiv preprint arXiv:2505.19301 , 2025.\n[178] Subash Neupane, Shaswata Mitra, Sudip Mittal, and Shahram Rahimi. Towards a hipaa compliant agentic ai\nsystem in healthcare. arXiv preprint arXiv:2504.17669 , 2025.\n[179] Shuyuan Liu, Jiawei Chen, Shouwei Ruan, Hang Su, and Zhaoxia Yin. Exploring the robustness of decision-level\nthrough adversarial attacks on llm-based embodied models. In Proceedings of the 32nd ACM International\nConference on Multimedia , pages 8120–8128, 2024.\n[180] Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, and Yang Zhang.\nBreaking agents: Compromising autonomous llm agents through malfunction amplification. arXiv preprint\narXiv:2407.20859 , 2024.\n[181] Atharv Singh Patlan, Peiyao Sheng, S Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath. Ai agents in\ncryptoland: Practical attacks and no silver bullet. Cryptology ePrint Archive , 2025.\n[182] Maxwell Standen, Junae Kim, and Claudia Szabo. Adversarial machine learning attacks and defences in\nmulti-agent reinforcement learning. ACM Computing Surveys , 57(5):1–35, 2025.\n[183] Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke,\nNitarshan Rajkumar, David Krueger, Noam Kolt, et al. Visibility into ai agents. In Proceedings of the 2024 ACM\nConference on Fairness, Accountability, and Transparency , pages 958–973, 2024.\n[184] Yewei Zhen, Donghai Tian, Xiaohu Fu, and Changzhen Hu. A novel malware detection method based on audit\nlogs and graph neural network. Engineering Applications of Artificial Intelligence , 152:110524, 2025.\n[185] Ranjan Sapkota, Konstantinos I Roumeliotis, and Manoj Karkee. Vibe coding vs. agentic coding: Fundamentals\nand practical implications of agentic ai. arXiv preprint arXiv:2505.19443 , 2025.\n[186] Konstantinos Lazaros, Dimitrios E Koumadorakis, Aristidis G Vrahatis, and Sotiris Kotsiantis. Federated\nlearning: Navigating the landscape of collaborative intelligence. Electronics , 13(23):4744, 2024.\n[187] M Victoria Luzón, Nuria Rodríguez-Barroso, Alberto Argente-Garrido, Daniel Jiménez-López, Jose M Moyano,\nJavier Del Ser, Weiping Ding, and Francisco Herrera. A tutorial on federated learning from theory to practice:\nFoundations, software frameworks, exemplary use cases, and selected trends. IEEE/CAA Journal of Automatica\nSinica , 11(4):824–850, 2024.\n[188] Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota,\nAditya Jain, Anas Zafar, Muneeb Ul Hassan, et al. Who is responsible? the data, models, users or regulations? a\ncomprehensive survey on responsible generative ai for a sustainable future. arXiv preprint arXiv:2502.08650 ,\n2025.\n[189] European Parliament and Council. Regulation (EU) 2024/1244 of the European Parliament and of the\nCouncil laying down harmonised rules on artificial intelligence (AI Act). https://eur-lex.europa.eu/\nlegal-content/EN/TXT/?uri=CELEX%3A32024R1244 , 2024. Official EU legislation adopted in 2024 estab-\nlishing the legal framework for AI systems.\n[190] Chen Chen, Xueluan Gong, Ziyao Liu, Weifeng Jiang, Si Qi Goh, and Kwok-Yan Lam. Trustworthy, responsible,\nand safe ai: A comprehensive architectural framework for ai safety with challenges and mitigations. arXiv\npreprint arXiv:2408.12935 , 2024.\n[191] American Bar Association. Aba house adopts 3 guidelines to improve use of artificial intelligence.\nhttps://www.americanbar.org/advocacy/governmental_legislative_work/publications/\nwashingtonletter/may-23-wl/ai-0523wl/ , May 2023. Resolution 604 outlines guidelines for\naccountability, transparency, and human oversight in AI systems.\n[192] Muhammad Ali, Stavros Shiaeles, Maria Papadaki, and Bogdan V Ghita. Agent-based vs agent-less sandbox for\ndynamic behavioral analysis. In 2018 Global Information Infrastructure and Networking Symposium (GIIS) ,\npages 1–5. IEEE, 2018.\n31\n--- Page 32 ---\nAPREPRINT\n[193] Satvik Golechha and Adrià Garriga-Alonso. Among us: A sandbox for measuring and detecting agentic deception.\narXiv preprint arXiv:2504.04072 , 2025.\n[194] Peter Slattery, Alexander K. Saeri, Emily A. C. Grundy, Jess Graham, Michael Noetel, Risto Uuk, James Dao,\nSoroush Pour, Stephen Casper, and Neil Thompson. The ai risk repository: A comprehensive meta-review,\ndatabase, and taxonomy of risks from artificial intelligence, 2025.\n[195] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto\nBarbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. Explainable artificial\nintelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion ,\n58:82–115, 2020.\n[196] Ahmed M Salih, Zahra Raisi-Estabragh, Ilaria Boscolo Galazzo, Petia Radeva, Steffen E Petersen, Karim Lekadir,\nand Gloria Menegaz. A perspective on explainable artificial intelligence methods: Shap and lime. Advanced\nIntelligent Systems , 7(1):2400304, 2025.\n[197] Turker Berk Donmez, Mustafa Kutlu, Mohammed Mansour, and Mustafa Zahid Yildiz. Explainable ai in action:\na comparative analysis of hypertension risk factors using shap and lime. Neural Computing and Applications ,\n37(5):4053–4074, 2025.\n[198] Patrick Knab, Sascha Marton, Udo Schlegel, and Christian Bartelt. Which lime should i trust? concepts,\nchallenges, and solutions. arXiv preprint arXiv:2503.24365 , 2025.\n[199] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David\nHa. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint\narXiv:2504.08066 , 2025.\n[200] Guneet Kaur Walia and Mohit Kumar. Computational offloading and resource allocation for iot applications\nusing decision tree based reinforcement learning. Ad Hoc Networks , page 103751, 2025.\n[201] Luis L Fonseca, Lucas Böttcher, Borna Mehrad, and Reinhard C Laubenbacher. Optimal control of agent-based\nmodels via surrogate modeling. PLOS Computational Biology , 21(1):e1012138, 2025.\n[202] Shekhar Mahmud, Mustafa Kutlu, and Alper Turan Alan. Optimizing multi-agent system swarm performance\nthrough explainable ai. In 2025 11th International Conference on Mechatronics and Robotics Engineering\n(ICMRE) , pages 175–180. IEEE, 2025.\n[203] Zhang Xi-Jia, Yue Guo, Shufei Chen, Simon Stepputtis, Matthew Gombolay, Katia Sycara, and Joseph Campbell.\nModel-agnostic policy explanations with large language models. arXiv preprint arXiv:2504.05625 , 2025.\n[204] Busra Ozdenizci Kose. Modern software challenges: Innovations in data governance and devsecops. In\nData Governance, DevSecOps, and Advancements in Modern Software , pages 113–130. IGI Global Scientific\nPublishing, 2025.\n[205] Tuan-Jun Goh, Lee-Ying Chong, Siew-Chin Chong, and Pey-Yun Goh. A campus-based chatbot system using\nnatural language processing and neural network. Journal of Informatics and Web Engineering , 3(1):96–116,\n2024.\n[206] Jingru Yu, Yi Yu, Xuhong Wang, Yilun Lin, Manzhi Yang, Yu Qiao, and Fei-Yue Wang. The shadow of fraud:\nThe emerging danger of ai-powered social engineering and its possible cure. arXiv preprint arXiv:2407.15912 ,\n2024.\n[207] Bhada Yun, Dana Feng, Ace S Chen, Afshin Nikzad, and Niloufar Salehi. Generative ai in knowledge work:\nDesign implications for data navigation and decision-making. In Proceedings of the 2025 CHI Conference on\nHuman Factors in Computing Systems , pages 1–19, 2025.\n[208] Tongyang Li, Jiageng Ruan, and Kaixuan Zhang. The investigation of reinforcement learning-based end-to-end\ndecision-making algorithms for autonomous driving on the road with consecutive sharp turns. Green Energy and\nIntelligent Transportation , page 100288, 2025.\n[209] Guardrails AI. Guardrails AI | Your Enterprise AI needs Guardrails — guardrailsai.com, February 2024.\n[210] Securiti. What is AI TRiSM and why it’s essential in the era of genai, May 2025. Accessed: 2025-06-03.\n[211] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models:\nPrinciples, Taxonomy, Challenges, and Open Questions, November 2023. arXiv:2311.05232 [cs].\n[212] Shaina Raza, Syed Raza Bashir, and Usman Naseem. Accuracy meets Diversity in a News Recommender System.\nInProceedings of the 29th International Conference on Computational Linguistics , pages 3778–3787, Gyeongju,\nRepublic of Korea, 10 2022. International Committee on Computational Linguistics.\n32\n--- Page 33 ---\nAPREPRINT\n[213] Raja Parasuraman and Dietrich H. Manzey. Complacency and bias in human use of automation: An attentional\nintegration. Human Factors , 52(3):381–410, 2010.\n[214] Shaina Raza, Shardul Ghuge, Chen Ding, and Deval Pandya. FAIR Enough: How Can We Develop and Assess a\nFAIR-Compliant Dataset for Large Language Models’ Training? arXiv preprint arXiv:2401.11033 , 2024.\n[215] European Commission. AI Act | Shaping Europe’s digital future, 2024. Accessed: 2025-06-03.\n[216] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova\nDasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan,\nDanny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer,\nEli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan,\nand Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons\nLearned, November 2022. arXiv:2209.07858 [cs].\n[217] Aileen G. Bacudio, Xiaohong Yuan, Bei-Tseng Bill Chu, and Monique Jones. An overview of penetration testing.\nInternational Journal of Network Security & Its Applications (IJNSA) , 3(6):19–38, November 2011.\n[218] Jacquelyn Schneider. Wargaming cyber security. War on the Rocks , September 2020.\n[219] Saleema Amershi, Daniel S. Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina\nSuh, Shamsi T. Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. Guidelines\nfor human-ai interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems .\nACM, 2019.\n[220] Henry Hexmoor, Johan Lammens, Guido Caicedo, and Stuart C Shapiro. Behaviour based AI, cognitive processes,\nand emergent behaviors in autonomous agents , volume 1. WIT Press, 2025.\n33\n--- Page 34 ---\nAPREPRINT\nAI Agents\n(percieves + act)\nAgentic AI\nLLM Core\n Tooling\nPersistent\nMemory\nFigure 4: Traditional AI Agent vs. Agentic AI.\nTable 9: Key terminologies for LLM-based agentic AI systems.\nTerm Definition\nAgentic AI System A multi-agent architecture powered by large language models (LLMs), where autonomous agents\ncollaborate, plan, and execute tasks over extended horizons with persistent memory and dynamic\nrole assignment.\nAutonomy Model The mechanism by which an agent decides and acts without direct human intervention, often using\ngoal-driven planning and chain-of-thought reasoning.\nChain of Thought (CoT) A prompting strategy in which an LLM generates intermediate reasoning steps before producing a\nfinal answer or action, enhancing interpretability and multi-step planning.\nCounterfactual Analysis An interpretability technique that examines how altering certain inputs or agent contributions would\nchange the overall system outcome, revealing causal dependencies among agents.\nExplainability The capacity of an AI system (or individual agent) to produce human-understandable justifications\nor rationales for its decisions and actions, often via LIME, SHAP, or decision provenance graphs.\nFoundation Model (LLM) A pretrained large language model (e.g., GPT-4, LLaMA) that serves as the “brain” of each agent,\nproviding generative capabilities, reasoning, and tool-calling support.\nShared Memory (Persis-\ntent Memory)A centralized or distributed store (often a vector database) where agents write and retrieve contextual\ninformation, enabling long-term planning and consistency across iterations.\nModelOps The practice of managing AI models (and agent prompts) throughout their lifecycle—development,\ndeployment, monitoring, and retirement—with version control, CI/CD testing, and drift detection.\nApplication Security Safeguards and best practices (e.g., prompt sanitation, authentication, sandboxing) designed to\nprotect agentic systems from prompt injection, identity spoofing, and lateral exploits.\nModel Privacy Techniques (e.g., differential privacy, homomorphic encryption, secure enclaves) that ensure sensi-\ntive data—either during training or inter-agent communication—remains protected in multi-agent\nworkflows.\nPrompt Injection A security exploit in which an attacker crafts input containing hidden instructions that corrupt an\nagent’s reasoning or propagate malicious commands through agent interactions (“prompt infection”).\nRetrieval-Augmented Gen-\neration (RAG)A framework where agents query an external knowledge store (e.g., vector database) to fetch\nrelevant documents or facts, then condition their LLM responses on that retrieved context.\nRole-Specialized Coordi-\nnationAn architectural pattern in which each agent is assigned a specific function (e.g., planner, verifier,\ncoder) and collaborates via structured communication protocols to achieve complex tasks.\nDecision Provenance\nGraphA graph-based representation that traces data flows and decision steps across multiple agents,\nenabling post-hoc auditing and system-level interpretability.\nTool-Use Interface The mechanism by which an agent issues structured commands (e.g., API calls, code execution) to\nexternal services or environments and incorporates the results back into its reasoning.\nTrust Score A composite metric that quantifies an agent’s reliability, alignment with user goals, and consistency\nover time, often combining accuracy, safety-violation rates, and calibration of confidence.\nComposite Metric An aggregate evaluation score (e.g., a weighted sum of trustworthiness, explainability, user-centered\nperformance, and coordination metrics) used to benchmark different agentic systems.\n34",
  "text_length": 150161
}