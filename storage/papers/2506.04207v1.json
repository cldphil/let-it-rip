{
  "id": "http://arxiv.org/abs/2506.04207v1",
  "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
  "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
  "authors": [
    "Shuang Chen",
    "Yue Guo",
    "Zhaochen Su",
    "Yafu Li",
    "Yulun Wu",
    "Jiacheng Chen",
    "Jiayu Chen",
    "Weijie Wang",
    "Xiaoye Qu",
    "Yu Cheng"
  ],
  "published": "2025-06-04T17:51:08Z",
  "updated": "2025-06-04T17:51:08Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04207v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04207v1  [cs.LG]  4 Jun 2025Advancing Multimodal Reasoning: From Optimized\nCold Start to Staged Reinforcement Learning\nShuang Chen1∗, Yue Guo2∗, Zhaochen Su3, Yafu Li4, Yulun Wu1, Jiacheng Chen4,\nJiayu Chen2,Weijie Wang1,Xiaoye Qu4†,Yu Cheng5†\n1Zhejiang University2Fudan University3Soochow University\n4Shanghai AI Laboratory5The Chinese University of Hong Kong\nCode Page: https://github.com/CSfufu/Revisual-R1\nMathVerse MathVision DynaMath WeMath LogicVista AIME24 AIME25 GPQA MATH500 Average020406080100Accuracy (%)53.6\n48.8\n27.542.052.353.3\n43.347.589.2\n53.1Accuracy Comparison Across Open-Source Reasoning Models\nOpenVLThinker-7B\nMM-Eureka-Qwen-7B\nMMR1-Math-v0\nThinkLite-7B-VL\nVLAA-Thinker-Qwen2.5VL-7B\nVL-Rethinker-7B\nReVisual-R1 (Ours)\nFigure 1: Overall performance across five multimodal reasoning benchmarks (MathVerse, MathVision, Dy-\nnaMath, WeMath, and LogicVista) and four textual reasoning benchmarks (AIME24, AIME25, GPQA, and\nMATH500). Our ReVisual-R1 achieves better performance than existing works.\nAbstract\nInspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in Multimodal\nLarge Language Models (MLLMs) by directly applying reinforcement learning\n(RL). However, they still struggle to activate complex reasoning. In this paper,\nrather than examining multimodal RL in isolation, we delve into current training\npipelines and identify three crucial phenomena: 1) Effective cold start initialization\nis critical for enhancing MLLM reasoning. Intriguingly, we find that initializing\nwith carefully selected text data alone can lead to performance surpassing many re-\ncent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO\napplied to multimodal RL suffers from gradient stagnation, which degrades train-\ning stability and performance. 3) Subsequent text-only RL training, following the\nmultimodal RL phase, further enhances multimodal reasoning. This staged training\napproach effectively balances perceptual grounding and cognitive reasoning devel-\nopment. By incorporating the above insights and addressing multimodal RL issues,\nwe introduce ReVisual-R1 , achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.\n∗Equal contributions.\n†Corresponding authors.\nPreprint. Under review.\n--- Page 2 ---\n1 Introduction\nRecently, the field of large language models (LLMs) has witnessed significant advancements in\ncomplex cognitive reasoning [ 1,2], notably exemplified by reasoning models like DeepSeek-R1 [ 3].\nThese models successfully leveraged Reinforcement Learning (RL) to facilitate the self-emergence\nof intricate reasoning abilities in text-only models. Inspired by this success, a natural extension has\nbeen to apply similar RL paradigms to Multimodal Large Language Models (MLLMs) with the goal\nof incentivizing multimodal reasoning capabilities [4, 5, 6, 7, 8, 9, 10, 11].\nDespite these promising efforts, current methods often struggle to fully unlock complex reasoning\nwithin MLLMs. This difficulty suggests that directly transplanting RL techniques from text-only\ndomains may not adequately address the unique dynamics and requirements inherent in multimodal\nlearning and reasoning. Motivated by this gap, our work comprehensively studies the training\npipelines of the multimodal reasoning models. Through this investigation, we identify three crucial\nphenomena that significantly influence the efficacy of multimodal training:\nFirst, we observe that sufficient cold start initialization is indispensable for effectively cultivating the\nreasoning ability of MLLMs. Conventional cold-start phases for MLLMs, often relying on simplistic\nvisual and textual pre-training corpora, appear to insufficiently prepare models for the demands of\ncomplex problem-solving, a challenge evident in the reasoning limitations of various existing models\n[12,4,13,14,15]. This initial deficit critically hinders the subsequent RL stages from eliciting\nsophisticated, self-critical reasoning patterns. To unlock deeper deliberative reasoning in MLLMs, an\nenriched cold-start initialization is therefore not merely beneficial, but indispensable. Specifically,\ninitializing with carefully selected text data that instills foundational reflective capabilities and\nthe capacity for extended Chain-of-Thought (CoT) reasoning proves to be a powerful strategy.\nIntriguingly, such targeted textual initialization allows our model to surpass the multimodal reasoning\nperformance of many recent multimodal reasoning models.\nSecond, we identify that the standard Group Relative Policy Optimization (GRPO) algorithm [ 16],\ncommonly applied for multimodal RL, suffers from a gradient stagnation problem. This issue\nsignificantly degrades both the training stability and the ultimate performance of the multimodal\nRL phase. To address this fundamental limitation and improve the efficacy of multimodal RL, we\npropose Prioritized Advantage Distillation (PAD). PAD is designed to mitigate gradient stagnation by\nstrategically filtering out zero-advantage samples and re-weighting informative trajectories, thereby\nfocusing the learning process on more impactful data and improving training stability.\nThird, we discover that conducting further post-training using text RL after the multimodal RL\ntraining phase further enhances multimodal reasoning ability. This observation highlights the complex\ninterplay between visual grounding and linguistic fluency in achieving superior multimodal reasoning.\nBased on these three key findings, we devise a three-stage curriculum that effectively balances\nvisual and textual competencies for multimodal reasoning. This curriculum comprises: (1) a textual\ncold-start to explicitly instill complex reasoning templates; (2) a multimodal RL stage to connect\nlinguistic reasoning with visual perception; and (3) a text-only RL refinement stage aims to restore\nlinguistic fluency and refine reasoning expression without eroding the newly gained visual grounding\nskills, effectively consolidating the multimodal reasoning capabilities.\nTo this end, we introduce ReVisual-R1, the first 7B-parameter open-source MLLM designed with\nthis staged curriculum. As shown in Figure 1, ReVisual-R1 presents strong performance in chal-\nlenging multimodal reasoning tasks, while simultaneously preserving strong general-purpose textual\nskills. Extensive experiments on a suite of challenging benchmarks, including MathVerse [ 17],\nMathVision [ 18], MathVista [ 19], DynaMath [ 20], WeMath [ 21], and LogicVista [ 22], as well as the\nAIME24/25 [ 23], GPQA [ 24], MATH-500 [ 25] benchmark, confirm that ReVisual-R1 significantly\noutperforms much larger public models.\nTo summarize, our contributions are as follows:\n•We systematically investigate MLLM cold-start initialization, revealing the insufficiency of existing\nmultimodal pre-training datasets and demonstrating that a text-centric, high-difficulty cold-start\nphase is crucial for unlocking complex multimodal reasoning capabilities.\n•We identify and address the critical issue of gradient stagnation in GRPO for multimodal RL by\nproposing Prioritized Advantage Distillation (PAD), a novel technique that ensures more stable\ntraining and sample-efficient learning for MLLMs.\n2\n--- Page 3 ---\n•We present ReVisual-R1, an open-source 7B MLLM developed through a principled three-stage\ncurriculum. This approach uniquely cultivates deep, self-reflective reasoning and robust visual\ngrounding, enabling ReVisual-R1 to achieve state-of-the-art performance on complex multimodal\nreasoning tasks, rivaling even larger or proprietary models.\n2 Preliminaries\nIn this section, we first formulate the task setting and key concepts in the multimodal reasoning\nproblem. Then, we describe the base training algorithm framework used in our method.\n2.1 Multimodal Reasoning Formulation\nIn multimodal reasoning tasks, the input can be represented as x= (v, q), where vdenotes the visual\ncontent, and qdenotes the textual query. Our work aims to guide a MLLM to generate a multi-step,\nself-reflective reasoning process t, which ultimately assists the model in producing a solution ythat\ncorrectly answers the query based on the multimodal input.\nFormally, we aim to learn a policy πθ(y|x), parameterized by θ, which maps the input question space\nXto the solution space Y. Our objective is to optimize the model parameters such that the expected\nreward r(y, x)over the output distribution is maximized:\nθ∗= arg max\nθEx∼DEy∼πθ(y|x)[r(y, x)] (1)\nwhere Drepresents the distribution of multimodal reasoning tasks. Similar to Deepseek R1 [ 3], in\nour work, we mainly use rule-based reward, r(x, y) = 1 ifyis correct, otherwise r(x, y) = 0 .\n2.2 Group Relative Policy Optimization\nGroup Relative Policy Optimization (GRPO) extends traditional policy optimization methods by\norganizing training samples into groups and optimizing policies relative to reference models within\neach group, offering several advantages for training language models on complex reasoning tasks.\nFormally, given a batch of samples B, GRPO divides them into Kgroups {G1,G2, . . . ,GK}based\non certain criteria. For each group Gi, we maintain both a policy model πθand a reference model\nπθref. The GRPO objective for each group is formulated as:\nEx∼GiEy∼πθ(y|x)\u0014\nmin\u0012πθ(y|x)\nπθref(y|x)ˆA(x, y),clip\u0012πθ(y|x)\nπθref(y|x),1−ϵ,1 +ϵ\u0013\nˆA(x, y)\u0013\u0015\n(2)\nwhere ϵis a hyperparameter controlling the size of the trust region, and ˆA(x, y)is the group-specific\nadvantage function. For each input xwithGgenerated responses {y1, . . . , y G}within a group, the\nadvantage for response yiis defined as:\nˆA(x, yi) =r(x, yi)−mean ({r(x, y1), . . . , r (x, yG)})\nstd({r(x, y1), . . . , r (x, yG)}) +ϵ(3)\nwhere ϵis a small constant for numerical stability. This relative advantage is then used within a\nclipped surrogate objective function.\nHere, r(x, yi)represents the reward for response yito input x. This advantage function measures how\nmuch better a specific response is compared to the average performance within its group, normalized\nby the group’s reward variance. By using group-specific advantages, GRPO encourages the model to\nimprove responses within each group while maintaining diversity across groups.\n3 GRAMMAR: Generalized Multimodal Reasoning Dataset\nIn this section, we first show an intriguing finding involving the multimodal reasoners in Section 3.1,\npaving the way for the strategy of our data curation pipeline in Section 3.2.\n3\n--- Page 4 ---\nTable 1: Textual and multimodal reasoning datasets source of our GRAMMAR.\nMultimodal Text-only\nSource Samples Source Samples Source Samples Source Samples\nFigureQA [26] 100K Super-CLEVR [27] 30K Big-Math-RL [28] 251K GAIR_LIMO [29] 0.8K\nMA VIS [30] 218K TabMWP [31] 38K Big-Math-RL-U 35K s1K-1.1 [32] 1K\nGeoQA [33] 5K UniGeo [34] 16K OpenThoughts [35] 114K OpenMathR [36] 3,200K\nGeometry3K [37] 2.1K MultiMath [38] 300K DeepMath [39] 103K OrcaMath [40] 200K\nIconQA [41] 107K OpenR1-220k [42] 220K NuminaMath-CoT [43] 859K\n3.1 Preliminary Study of Cold Start\nVision-R1 R1-One-Vision DeepMath OpenR1-Math0510152025Absolute Performance Improvement5.67.612.8\n11.3\n4.47.214.3\n12.413.3\n10.026.6\n23.3\n14.4\n11.820.922.6MM Cold Start Avg.\nText Cold Start Avg.\nMathVerse\nMathVision\nAIME2024\nMATH500\nFigure 2: Absolute performance improvement on\nQwen2.5-VL-7B-Instruct across textual and multimodal\nreasoning tasks. The red and purple dashed lines rep-\nresent the average absolute gains of VisionR1/R1-One-\nVision and DeepMath/OpenR1-Math over the baseline,\nrespectively, across four reasoning tasks.Currently, the leading text and multimodal rea-\nsoning models in the community [ 44,3] pri-\nmarily rely on cold-start training with exten-\nsive self-critique data or large-scale multimodal\nreinforcement learning based on verifiable re-\nwards [ 3]. While cold-start training is essen-\ntial for improving the reasoning capabilities\nof models [ 3], its effect on the reasoning abil-\nities of multimodal models remains underex-\nplored. To investigate this, we collect two open-\nsource cold-start multimodal datasets, Vision-\nR1 [4] and R1-One-Vision [ 12], along with two\ncold-start textual datasets, DeepMath [ 39] and\nOpenR1-Math [ 42]. Then we randomly sam-\nple 40,000 instances from these datasets to fine-\ntune Qwen2.5-VL-7B-Instruct [ 45]. The fine-\ntuned models are subsequently evaluated on mul-\ntimodal reasoning benchmarks (MathVerse and\nMathVision) as well as text reasoning bench-\nmarks (AIME24 and Math500). The experimen-\ntal outcomes and average performance enhancements from the multimodal and textual cold-start\ndatasets are illustrated in Figure 2.\nThe results in Figure 2 reveal that models trained with text-only cold start data exhibit substantial\nimprovements in both textual and multimodal reasoning tasks. In contrast, models trained solely on\nmultimodal datasets, such as Vision-R1 and R1-One-Vision, show limited gains in both multimodal\nand textual reasoning. This suggests that the complexity and patterns presented by textual cold start\ndata may better stimulate the models’ reasoning capabilities.\nTo further investigate this observation, we perform an analysis using a subset of 100 examples\nsampled from the Vision-R1 [ 4] and DeepMath [ 39] datasets. Specifically, we analyze the response\nlengths and pass rates of the doubao-1.5-thinking-pro-vision model [ 46] on these samples. Responses\nto textual prompts from DeepMath averaged 8,207.76 tokens, which is substantially longer than the\n821.48 tokens generated in response to multimodal prompts from Vision-R1. Moreover, the pass\nrate for Vision-R1 is 96.00%, whereas DeepMath achieve a pass rate of only 75.0%. These findings\nfurther indicate that current multimodal cold start datasets may lack sufficient complexity to inspire\nadvanced reasoning capabilities of reasoning models. It indicates that existing multimodal cold start\ndatasets in the community may not be sufficiently challenging to enhance the complex reasoning\ncapabilities and generalization of policy models, i.e.,they may not provide effective initial strategies\nduring cold start training. Therefore, it is desirable to develop a data curation pipeline for multimodal\npolicy model to improve the generalization capabilities.\n3.2 Data Curation\nInformed by Section 3.1 regarding the variability in open-source reasoning data [ 47], [48], [49], [50],\n[51], we develop GRAMMAR, a new dataset designed to enhance the generalization of reasoning\ncapabilities in multimodal models. GRAMMAR comprises 47k diverse textual thought samples with\nexplicit reasoning paths, augmented by 31k complex textual examples and 21k multimodal questions\nwith ground truth annotations suitable for rule-based reinforcement learning.\n4\n--- Page 5 ---\nThe construction of GRAMMAR involved a multi-stage curation pipeline. We begin by amassing\na wide array of open-source reasoning data, spanning various difficulty levels (details in Table 1).\nThis initial collection underwent rule-based filtering to ensure answer verifiability, excluding items\nlike proof problems and those with difficult-to-verify ground truths. Subsequently, Qwen2.5-VL-7B-\nInstruct was employed for initial pruning of overly simple or complex questions. Qwen2.5-VL-32B-\nInstruct is then used to assess the remaining samples to classify them into ten difficulty levels. To\nmaximize data diversity and minimize redundancy, we encoded questions using NV-Embedding-V2\n[52], applied HDBSCAN [ 53] for clustering, assigned topics to clusters via Qwen2.5-7B-Instruct,\nand performed balanced sampling across both topics and difficulty strata.\n4 Staged Reinforcement Optimization (SRO)\nOur data investigations (Section 3.1) and the curation of the GRAMMAR dataset highlight the\nnecessity of high-quality, reasoning-focused data for developing advanced MLLM capabilities.\nBuilding directly on these data-centric foundations, we introduce Staged Reinforcement Optimization\n(SRO), a framework designed to systematically cultivate robust reasoning and diverse competencies\nin MLLMs. SRO achieves this through a sequence of distinct learning phases, each tailored to address\nspecific training challenges and leverage appropriate components of the GRAMMAR dataset. This\nsection details the SRO architecture and its constituent techniques.\n4.1 Stage 1: Multimodal RL\nAfter the cold start training, the SRO framework commences with a dedicated Multimodal Reinforce-\nment Learning (MRL) phase. This initial stage is pivotal for enabling the MLLM to ground textual\nconcepts in visual information and execute cross-modal reasoning, primarily using the multimodal\nsamples from our GRAMMAR dataset. We employ GRPO as the core RL algorithm for this phase.\nTo ensure stable and effective learning, particularly when dealing with complex tasks and potentially\nsparse rewards common in multimodal settings, we integrate two key enhancements: Prioritized\nAdvantage Distillation (PAD) to improve gradient quality by addressing specific GRPO limitations,\nand an efficient-length reward function for more controlled and stable response generation.\n4.1.1 Prioritized Advantage Distillation (PAD)\nIn this paper, we discover a significant challenge when applying GRPO in complex multimodal\nsettings is “Gradient Stagnation”. This phenomenon refers to a reduction in learning efficacy due to a\npredominance of near-zero advantage estimates, which is particularly acute when dealing with sparse\nbinary rewards. Essentially, if entire groups of generated responses yield uniform rewards (e.g., all\ncorrect or all incorrect), the resulting advantage signals become null, leading to zero policy gradients\nand thereby halting learning for those samples. This issue, also noted in concurrent works [ 13,54],\ncan severely impede training progress. To specifically counteract gradient stagnation and enhance the\nefficiency of GRPO, we introduce Prioritized Advantage Distillation (PAD). PAD refines the training\nprocess by strategically focusing updates on the most informative samples within each batch, namely\nthose exhibiting significant, non-zero advantage signals. This approach optimizes computational\nresource allocation and promotes more consistent learning. The PAD mechanism, detailed below,\noperates on each batch after initial advantage estimation:\n•Per-Sequence Advantage Calculation: Compute the absolute advantage |˜Ai|for each sequence i\nin the original batch B, representing its learning signal magnitude.\n•Effective Sample Filtering: Form an “effective set” Eby selecting sequences iwhose absolute\nadvantage |˜Ai|falls within a specified informative range [Tlow, Thigh]. Critically, Tlow>0filters\nout stagnant (near-zero advantage) samples, ensuring that candidates for sub-sampling provide\npotentially useful learning signals.\n•Prioritized Sub-sampling from Effective Set: From this effective set E,k′= min( ρ|B|,|E|)\nsequences are drawn to form a distilled mini-batch. Selection is prioritized based on sequences’\nabsolute advantages ( ˆAifori∈ E), with the probability for selecting sequence idetermined by a\ntemperature-controlled Softmax distribution:\nPr(iis selected |i∈ E) =exp( ˆAi/τ)P\nj∈Eexp( ˆAj/τ)(4)\n5\n--- Page 6 ---\nThe temperature τgoverns sampling concentration and is typically decayed during training (e.g.,\nlinearly from 1.0to0.3) to shift from exploration towards exploitation. This enriches the mini-batch\nwith the most informative samples from E.\nPAD thus directly counteracts gradient stagnation via a dual mechanism: first, by filtering out\nstagnant samples, and second, by prioritizing updates using informative, non-zero advantages from\nthe remaining set. This selective optimization of the learning process ensures efficient computational\nresource allocation towards high-value samples. Consequently, PAD leads to enhanced training\nstability, improved learning efficiency, and more effective acquisition of complex reasoning skills,\nparticularly in challenging scenarios with sparse or binary rewards.\n4.1.2 Efficient-Length Reward Function\nWhile complex reasoning tasks often necessitate extended outputs, excessively long sequences can be\nsuboptimal [ 55,56]. Therefore, in this paper, besides the primary reward signal for task accuracy, we\nintroduce an efficient-length reward to regulate the verbosity of generated responses. Specifically, let\nLybe the token length of the generated sequence and Lbudget be a pre-defined target length budget. A\nraw reward score, Rraw, is first computed as a linear function of the deviation:\nRraw=α(Lbudget−Ly) +δ (5)\nwhere αis a positive scaling factor controlling the penalty magnitude for length deviation, and δis a\nbaseline reward (e.g., 0.5) assigned when Ly=Lbudget . The final Efficient-Length Reward, Rlen, is\nobtained by clipping Rrawto the range [0,1]:\nRlen(Ly, Lbudget, α, δ) = max(0 .0,min(1 .0, Rraw)) (6)\nIn our experiments, we typically use a small α(e.g., 0.005) to ensure a gentle slope for the reward\nadjustment. This formulation provides a continuous and bounded reward signal. Specifically, a\nreward of δis assigned if the generated length Lymatches the budget Lbudget . The reward proportion-\nally increases (up to a maximum of 1.0) as Lybecomes shorter than Lbudget , thereby encouraging\nconciseness. This Efficient-Length Reward guides the model towards producing responses that are\nnot only accurate but also parsimonious, without prematurely curtailing potentially valuable, longer\nreasoning paths, thus fostering more robust and efficient learning of complex multimodal reasoning.\n4.2 Stage 2: Textual RL\nWhile MRL is indispensable for grounding reasoning across visual and textual inputs, intensive\nMRL training can inadvertently lead to a decline in purely textual capabilities, which we define\nas “textual capability decay”. To further elevate the model’s capacity for sophisticated abstract\nreasoning, we integrate a subsequent Textual Reinforcement Learning (TRL) phase. This stage aims\nto achieve both robust linguistic fluency and advanced reasoning. Linguistic fluency is restored\nand enhanced by fine-tuning on high-quality, text-only corpora focused on instruction-following\nand conversational abilities. Simultaneously, to foster advanced reasoning, the TRL phase exposes\nthe model to complex, text-centric problem-solving tasks. This compels the model to refine and\ngeneralize intricate reasoning patterns, articulate multi-step thought processes with greater clarity,\nand master linguistic nuances essential for higher-order cognition.\nFor policy optimization during this TRL phase, we employ GRPO, augmented with our proposed\nPAD mechanism for efficient sample utilization. The reward function is multifaceted, designed to\npromote linguistic excellence, which encompasses both fluency and conciseness. Conciseness, in\nparticular, is also encouraged by the efficient-length reward (Equation 6).\n5 Experiments\n5.1 Experiments Setup\nDatasets The training of ReVisual-R1 follows our proposed three-stage methodology, utilizing\ncarefully curated datasets for each phase. The cold-start phase employed approximately 40k pure text\nentries focused on establishing foundational language understanding. Subsequently, the Multimodal\nReinforcement Learning (MRL) phase used approximately 26k diverse multimodal entries from our\nGRAMMAR dataset (see Section 3.2 for data curation details) to develop cross-modal reasoning.\n6\n--- Page 7 ---\nFinally, the text-based RL (TRL) phase consisted of approximately 30k text entries designed to refine\nnuanced understanding and generation capabilities.\nBenchmarks We evaluate ReVisual-R1 on a comprehensive suite of benchmarks, selected to\ntest diverse reasoning skills. For visual-mathematical reasoning, we employed MathVerse [ 17],\nMathVision [ 18], WeMath [ 21], and DynaMath [ 20]. Broader multimodal reasoning was assessed\nusing MathVista [ 19] and LogicVista [ 22]. Performance on challenging text-based mathematical\nreasoning was measured on AIME24/25 [ 23] and MATH-500 [ 25], while general question answering\nwas tested with GPQA [ 24]. We report pass@1 accuracy for performance on the evaluated benchmarks\nexcept AIME. For the AIME24/25 benchmark, performance is measured using pass@32 accuracy.\nBaselines The performance of ReVisual-R1 is benchmarked against several categories of models,\nwith detailed results presented in Table 2. These baselines include: (1) leading closed-source\nmodels such as doubao-1.5-vision-pro-32k [ 57], OpenAI-GPT-4o [ 58], Claude-3.7-Sonnet [ 59],\nand Gemini-2.0-Flash [ 60]). (2) diverse open-source general-purpose MLLMs like the InternVL3-\n8B [61], LLaV A-OneVision [ 62], and the Qwen2.5-VL-7B [ 45]; and (3) specialized open-source\nreasoning MLLMs, including VLAA-Thinker-7B [ 63], OpenVLThinker-7B [ 14], MMR1-Math-v0\n[64], MM-Eureka [65], and VL-Rethinker-7B [13].\nImplementation Our ReVisual-R1 model is based on the Qwen-2.5-VL-7B-Instruct model. Its\ntraining comprised three distinct stages. The process begins with a cold-start phase utilizing LLaMA\nFactory [ 66] and pure text data to establish foundational language understanding. Following this,\nMultimodal Reinforcement Learning (MRL) is implemented using Easy R1 [ 67]. In this stage,\nthe GRPO Kullback-Leibler (KL) divergence constraint is omitted to encourage broader policy\nexploration. The final stage involves Text-based Reinforcement Learning (TRL), also conducted via\nEasy R1. During TRL, the vision tower is frozen to concentrate learning on textual reasoning, and\na small KL penalty is incorporated alongside entropy annealing to enhance training stability. All\nexperiments are conducted on a setup of 8 NVIDIA A100-80G GPUs. Detailed prompt settings and\ntraining hyperparameters are provided in the Appendix.\n5.2 Main Results\nAs shown in Table 2, our model achieves state-of-the-art (SOTA) results on math-related benchmarks\namong open-source reasoning multimodal models and even beats some commercial large MLLM.\nThese results prove the effectiveness of our method beneficial to the open-source community.\nSpecifically, ReVisual-R1 achieves an impressive average score of 53.1%, a significant improvement\nof +16.8 percentage points over the previous open-source SOTA average. Specifically, ReVisual-R1\nsecures the top position among open-source contenders in nine out of ten individual benchmarks:\nMathVerse (+5.4% ∆), MathVision (+13.9% ∆), DynaMath (+9.8% ∆), WeMath (+0.2% ∆),\nLogicVista (+9.6% ∆), AIME24 (+44.6% ∆), AIME25 (+15.4% ∆), GPQA (+10.1% ∆), and\nMATH500 (+23.4% ∆). The most substantial gains are observed in the challenging AIME24,\nMATH500, and AIME25 benchmarks, underscoring ReVisual-R1’s advanced mathematical and\ninferential reasoning prowess. On MathVista, ReVisual-R1 achieves the second-best open-source\nscore, narrowly trailing the leading open-source model by only -0.6%.\nWhen compared to closed-source commercial models, ReVisual-R1 also exhibits highly competitive\nperformance. For instance, its average score (53.1%) surpasses that of OpenAI-GPT-4o (41.6%). On\nspecific demanding benchmarks such as MATH500, ReVisual-R1 (89.2%) outperforms both doubao-\n1.5-vision-pro-32k (85.2%) and OpenAI-GPT-4o (74.6%). Similarly, on AIME24 and AIME25,\nReVisual-R1 demonstrates substantial leads over these commercial offerings. While some closed-\nsource models like doubao-1.5-vision-pro-32k show a higher overall average (55.8%), ReVisual-R1’s\nability to outperform them on several key reasoning tasks highlights its specialized strengths.\nCollectively, these results validate the efficacy of our proposed training method, including the\nstructured three-stage curriculum and enhancements like Prioritized Advantage Distillation. The\nstrong performance of ReVisual-R1 not only pushes the boundaries for open-source 7B MLLMs in\ncomplex reasoning but also provides a valuable contribution to the broader research community.\n7\n--- Page 8 ---\nTable 2: Performance comparison of various MLLMs on diverse out-of-domain benchmarks. The best scores are\nbold ; the second best are underlined (among open-source models). AIME24 andAIME25 results are averaged\nover eight independent inference runs to reduce score variance.\nMultimodal Reasoning Benchmarks Textual Reasoning Benchmarks\nModel MathVerse MathVision MathVista DynaMath WeMath LogicVista AIME24 AIME25 GPQA MATH500 Avg.\nClose-Source\ndoubao-1.5-vision-pro-32k 64.7 51.5 78.6 44.9 64.2 65.7 26.7 20.0 56.1 85.2 55.8\nOpenAI-GPT-4o 40.6 31.1 59.9 34.5 42.9 64.4 9.3 8.3 49.9 74.6 41.6\nClaude-3.7-Sonnet 52.0 41.3 66.8 39.7 58.2 49.3 20.0 13.3 61.1 80.4 48.2\nGemini-2.0-Flash 43.6 47.8 70.4 42.1 47.4 52.3 33.3 36.7 35.4 69.0 47.8\nOpen-Source General Models\nInternVL-3-8B 33.9 28.6 70.5 23.0 37.5 43.6 3.3 6.7 34.8 75.2 35.7\nLLaV A-OV-7B 17.6 17.6 62.6 9.0 17.7 32.0 0.0 0.0 0.1 45.2 20.2\nQwen-2.5-VL-7B 38.7 26.6 69.1 12.6 24.5 35.6 10.0 6.7 32.8 67.2 32.4\nOpen-Source Reasoning Models\nOpenVLThinker-7B 38.1 23.0 65.3 16.8 35.2 44.5 5.0 1.7 28.3 51.0 30.9\nMM-Eureka-Qwen-7B 45.4 28.1 72.6 23.0 21.8 46.3 6.7 3.3 34.3 66.6 31.5\nMMR1-Math-v0 42.8 30.7 69.8 17.4 31.9 46.8 5.4 0.8 19.2 65.8 33.1\nThinkLite-7B-VL 42.9 24.6 71.6 16.5 41.8 42.7 8.8 27.9 24.8 61.4 36.3\nVLAA-Thinker-7B 44.9 24.2 71.7 17.8 35.7 45.9 0.8 12.6 30.8 30.8 31.5\nVL-Rethinker-7B 46.4 28.4 73.7 17.8 36.3 42.7 2.9 2.9 37.4 47.0 33.6\nOur model\nReVisual-R1 53.6 48.8 73.1 27.5 42.0 52.3 53.3 43.3 47.5 89.2 53.1\n∆(Ours–Open SoTA) +7.2 +18.1 -0.6 +4.5 +0.2 +5.5 +44.5 +15.4 +10.1 +23.4 +16.8\nTable 3: Ablation study of different training stage combinations applied to the ReVisual-R1 model, building\nupon a Cold Start. Best results per column are bold and second-best are underlined .\nTraining Stages Applied MathVerse MathVision MathVista DynaMath WeMath LogicVista Avg\nCold Start (CS) only 51.9 47.9 70.5 26.5 35.8 50.1 47.1\nCS + MRL 50.9 47.6 71.9 25.7 38.8 51.2 47.7\nCS + TRL 47.3 47.3 71.0 25.2 33.7 44.7 44.9\nCS + MRL + TRL 53.6 48.8 73.1 27.5 42.0 52.3 49.6\nCS + TRL + MRL 47.5 48.0 70.3 24.2 35.0 48.2 45.5\n5.3 Ablation Study\n5.3.1 Ablation on SRO\nTo validate our Staged Reinforcement Optimization (SRO) framework, we conducted ablation studies\non different combinations of Multimodal RL (MRL) and Text-based RL (TRL) phases, all building\nupon our optimized text-centric cold-start (CS). This investigation, with results detailed in Table 3,\naimed to empirically determine the most effective sequence for cultivating a balance between robust\nvisual grounding and advanced textual proficiency.\nThe empirical evidence strongly supports our proposed CS + MRL + TRL (ReVisual-R1-MTR)\nsequence, which consistently yielded the highest average performance (49.6 Avg). This outcome\naffirms our core hypothesis: an initial MRL phase dedicated to establishing strong visual grounding,\nfollowed by a TRL phase to refine textual fluency and abstract reasoning, is crucial for developing\nsuperior multimodal capabilities without degrading the foundational cross-modal understanding.\nIn a more detailed analysis, the CS + MRL only model (47.7 Avg), while performing well on visually\nintensive tasks such as MathVista (71.9), did not reach the overall performance of the full MTR\nsequence. This suggests that MRL, while vital, can lead to a “textual capability decay”, which the\nsubsequent TRL stage effectively mitigates. The alternative SRO ordering, CS + TRL + MRL (45.5\nAvg), also proved less effective than our MTR approach. This finding indicates that establishing\nstrong visual grounding before intensive textual refinement allows for more synergistic learning,\nwhere the TRL phase can enhance reasoning that is already connected across modalities.\n8\n--- Page 9 ---\nTable 4: Ablation results demonstrating the impact of Prioritized Advantage Distillation (PAD) and its core\ncomponents. Performance metrics are reported for various mathematical reasoning benchmarks, averaged across\ndatasets. Best results per column are bold and second-best are underlined .\nModel Configuration Strategy MathVerse MathVision MathVista DynaMath WeMath LogicVista Avg\nReVisual-R1-Stage2 PAD 50.9 47.6 71.9 25.7 38.8 51.2 47.7\nw/o PAD Components:\n- Full PAD (Baseline) GRPO-Baseline 47.6 45.8 68.8 25.2 34.8 48.6 45.1\n- No Prioritized Sub-sampling GRPO-Filter 47.7 46.7 71.2 25.5 35.1 49.7 46.0\n- No Effective Sample Filtering Random-Sampling 47.9 46.4 70.7 26.1 37.1 49.3 46.2\nIn conclusion, these ablation results provide compelling justification for the MRL-then-TRL ordering\nwithin our SRO framework. This strategic sequencing first grounds the model multimodally and\nthen sharpens its linguistic and abstract reasoning faculties, culminating in a more comprehensively\ncapable and high-performing MLLM.\n5.3.2 Ablation study on PAD\nWe conduct ablation studies to evaluate Prioritized Advantage Distillation (PAD), examining its\noverall efficacy, the contribution of its components, and its sensitivity to key hyperparameters.\n0 5 10 15 20 25 30\nStep0.220.240.260.280.300.320.34Reward/Accuracy\nReward/Accuracy Comparison\nPAD sampling\nrandom sampling\ngrpo ﬁlter\ngrpo baseline\nFigure 3: Training reward of different strategies: PAD-\nsampling, GRPO-Baseline, GRPO-Filter, and Random-\nSampling. PAD consistently reaches higher accuracy\nand converges faster.To assess PAD’s impact, its full implementation\nwas compared against GRPO-Baseline, GRPO-\nFilter-only, and Random-Sampling strategies.\nTable 4 demonstrates that full PAD achieved su-\nperior performance on mathematical reasoning\nbenchmarks, highlighting the importance of its\ncore components: effective sample filtering and\nprioritized sub-sampling. Training dynamics\n(Figure 3) further corroborate PAD’s effective-\nness, with its sampling strategy yielding higher\nreward accuracy and faster convergence, thereby\nenhancing learning efficiency.\n5.3.3 Ablation on Efficient-Length Reward\nIn this paper, in multimodal RL, we devise an\nEfficient-Length Reward. As depicted in Fig-\nure 4, the Efficient-Length Reward significantly\nimpacts training. The regularized model main-\ntained stable and higher reward accuracy (Fig. 4a) and consistently low entropy (Fig. 4b). In contrast,\nthe baseline model suffered an accuracy decline and a dramatic entropy increase. Furthermore, the\nEfficient-Length Reward helped maintain a stable mean response length (Fig. 4c) and a low clip\nratio (Fig. 4d), unlike the baseline, which exhibited uncontrolled growth in response length and a\nconsequently higher clip ratio.\nIn summary, the Efficient-Length Reward is crucial for stabilizing training, preventing accuracy\ndegradation, maintaining low model entropy, and controlling verbosity.\n6 Related Work\n6.1 Multimodal Large Language Model\nSophisticated reasoning in Multimodal Large Language Models (MLLMs), inspired by Large Lan-\nguage Model (LLM) advancements, is a key research area. Initial progress involved integrating visual\nencoders with LLMs (e.g., CLIP [ 68], MiniGPT4 [ 69]), with notable advancements driven by visual\ninstruction tuning in series like LLaV A [ 70] [71]. While leading closed-source models (e.g., GPT-o3\n[72], Kimi-VL [ 73]) excel at long Chain-of-Thought (CoT) reasoning, open-source contributions\nhave focused on CoT adaptations [ 74,75,76] and Supervised Fine-Tuning (SFT) with reasoning\n9\n--- Page 10 ---\n0 10 20 30 40 50152025303540Accuracy (\n(a) Reward Accuracy\nw efﬁcent length\nw/o efﬁcent length\n0 10 20 30 40 500123456Entropy\n(b) Entropy Loss\nw efﬁcent length\nw/o efﬁcent length\n0 10 20 30 40 50\nTraining Step25003000350040004500Tokens\n(c) Response Length (mean)\nw efﬁcent length\nw/o efﬁcent length\n0 10 20 30 40 50\nTraining Step0.050.100.150.200.250.300.350.400.45Ratio\n(d) Clip Ratio\nw efﬁcent length\nw/o efﬁcent lengthFigure 4: Training dynamics comparing models with (purple lines, “w efficient-length”) and without (green\nlines, “w/o efficient-length”) the Efficient-Length Reward.\ntraces [ 77] [78]. The open-sourcing of DeepSeek-R1 [ 3] has further spurred Reinforcement Learning\n(RL) applications for visual reasoning [ 4] [6] and specialized domains like mathematical reasoning.\nNevertheless, many MLLMs reasoning models [ 13] [14] [12] [15] are limited by generating relatively\nshort responses, which often curtail genuine reflection, thorough visual exploration, and consequently,\ndeep multimodal reasoning. Our work, in contrast, introduces a novel framework to enable MLLMs to\ngenerate significantly longer, reflective responses with explicit visual references, thereby facilitating\nlong CoT reasoning to unlock more comprehensive multimodal reasoning capabilities.\n6.2 Reinforcement Learning in Reasoning\nReinforcement learning (RL) significantly advances LLM reasoning, evolving from methods like\nReinforcement Learning from Human Feedback (RLHF) [ 79,80]. Current LLM research further\nexplores direct RL fine-tuning, specialized cold-start datasets for long-form reasoning, and advanced\nalgorithms like Group Relative Policy Optimization (GRPO) [ 16] and its refinements (e.g., DAPO\n[54], DR.GRPO [ 81], GPG [ 82]) to elicit deeper reasoning. However, RL application to multimodal\nreasoning in MLLMs is nascent. Initial MLLMs efforts focus on subdomains like math reasoning\n[5,9] or generative reward models [ 83], often utilizing data from commercial models. Nonetheless,\nsuccesses such as DeepSeek-R1’s [ 3] rule-based RL are spurring similar MLLMs investigations,\nindicating growing interest in RL for unlocking sophisticated multimodal reasoning.\n7 Conclusion\nThis paper introduces ReVisual-R1, a 7B open-source MLLM designed to address prevalent chal-\nlenges in cultivating sophisticated multimodal reasoning. By systematically integrating a strategic,\nhigh-difficulty text-only cold-start phase for foundational reasoning, a Multimodal RL stage em-\nploying GRPO stabilized by our novel Prioritized Advantage Distillation (PAD) mechanism and\nguided by rule-based rewards including an Efficient-Length Reward, and a final TextRL refinement\nphase, our structured three-stage curriculum demonstrates that thoughtful data strategy and targeted\nalgorithmic optimizations are pivotal. ReVisual-R1 achieves state-of-the-art performance among\nopen-source 7B models on a suite of challenging visuo-mathematical and reasoning benchmarks.\nThis work underscores that careful curriculum design and algorithmic enhancements, rather than\nsheer model scale, can unlock robust, self-reflective multimodal reasoning.\n10\n--- Page 11 ---\nReferences\n[1]Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b\nmodel and 8k examples: Emerging reasoning with reinforcement learning is both effective and\nefficient.\n[2]Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang.\nLearning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945 , 2025.\n[3]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n[4]Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and\nShaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models.\narXiv preprint arXiv:2503.06749 , 2025.\n[5]Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian\nShi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment\nwith rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365 , 2025.\n[6]Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and\nZiwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language\nmodels. arXiv preprint arXiv:2411.14432 , 2024.\n[7]Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. Surf:\nTeaching large vision-language models to selectively utilize retrieved information. arXiv\npreprint arXiv:2409.14083 , 2024.\n[8]Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang,\nJames Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision\nlanguage models. In The Thirteen International Conference on Learning Representations .\n[9]Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang,\nXingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning\nabilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536 , 2025.\n[10] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie\nChen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via\nvisual tool reinforcement learning. arXiv preprint arXiv:2505.08617 , 2025.\n[11] Chuming Shen, Wei Wei, Xiaoye Qu, and Yu Cheng. Satori-r1: Incentivizing multimodal\nreasoning with spatial grounding and verifiable rewards. arXiv preprint arXiv:2505.19094 ,\n2025.\n[12] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu,\nDacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized\nmultimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615 ,\n2025.\n[13] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-\nrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning.\narXiv preprint arXiv:2504.08837 , 2025.\n[14] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Open-\nvlthinker: An early exploration to complex vision-language reasoning via iterative self-\nimprovement, 2025.\n[15] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generaliza-\ntion ability in vision-language models with less than $3. https://github.com/Deep-Agent/\nR1-V , 2025. Accessed: 2025-02-02.\n[16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\n11\n--- Page 12 ---\n[17] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun\nZhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see\nthe diagrams in visual math problems? In European Conference on Computer Vision , pages\n169–186. Springer, 2024.\n[18] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan,\nand Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset.\nAdvances in Neural Information Processing Systems , 37:95095–95169, 2024.\n[19] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao\nCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical\nreasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 , 2023.\n[20] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: A\ndynamic visual benchmark for evaluating mathematical reasoning robustness of vision language\nmodels, 2025.\n[21] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma\nGongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong,\nYida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large\nmultimodal model achieve human-like mathematical reasoning?, 2024.\n[22] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical\nreasoning benchmark in visual contexts, 2024.\n[23] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang,\nKashif Rasul, Longhui Yu, Albert Q. Jiang, Ziju Shen, et al. Numinamath: The largest public\ndataset in ai4maths with 860k pairs of competition math problems and solutions. https:\n//huggingface.co/datasets/Numinamath , 2024. Hugging Face repository, 13:9.\n[24] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark. In First Conference on Language Modeling , 2024.\n[25] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXiv preprint arXiv:2103.03874 , 2021.\n[26] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and\nYoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Workshop Track Proceedings . OpenReview.net, 2018.\n[27] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van\nDurme, and Alan L. Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness\nin visual reasoning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2023, Vancouver, BC, Canada, June 17-24, 2023 , pages 14963–14973. IEEE, 2023.\n[28] Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato,\nAnikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: A\nlarge-scale, high-quality math dataset for reinforcement learning in language models. CoRR ,\nabs/2502.17387, 2025.\n[29] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is\nmore for reasoning, 2025.\n[30] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Yichi Zhang, Chengzhuo Tong, Jiaming\nLiu, Aojun Zhou, Shanghang Zhang, Peng Gao, and Hongsheng Li. MA VIS: mathematical\nvisual instruction tuning with an automatic data engine. In The Thirteenth International Confer-\nence on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net,\n2025.\n12\n--- Page 13 ---\n[31] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. In The Eleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n[32] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\nLuke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple\ntest-time scaling, 2025.\n[33] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang\nLin. Geoqa: A geometric question answering benchmark towards multimodal numerical\nreasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings\nof the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021 , volume ACL/IJCNLP 2021 of Findings of ACL , pages 513–523. Association for\nComputational Linguistics, 2021.\n[34] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang.\nUnigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , pages 3313–3323. Association for Computational Linguistics,\n2022.\n[35] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025.\n[36] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt\nSchifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art math-\nematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891 ,\n2025.\n[37] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun\nZhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic\nreasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021 , pages 6774–6786. Association for Computational\nLinguistics, 2021.\n[38] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath:\nBridging visual and mathematical reasoning for large language models. CoRR , abs/2409.00147,\n2024.\n[39] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian\nYu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi,\nand Dong Yu. Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable\nmathematical dataset for advancing reasoning, 2025.\n[40] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking\nthe potential of slms in grade school math. CoRR , abs/2402.14830, 2024.\n[41] Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan\nLiang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding\nand visual language reasoning. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings\nof the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS\nDatasets and Benchmarks 2021, December 2021, virtual , 2021.\n[42] Hugging Face. Open r1: A fully open reproduction of deepseek-r1, January 2025.\n[43] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa\nHuang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong,\nLi Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath.\n[https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/\nproject-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf) ,\n2024.\n13\n--- Page 14 ---\n[44] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\narXiv:2505.07062 , 2025.\n[45] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,\n2025.\n[46] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang,\nChengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb\nreasoning models with reinforcement learning. arXiv e-prints , pages arXiv–2504, 2025.\n[47] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,\nand Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual\nlanguage reasoning, 2022.\n[48] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin.\nGeoqa: A geometric question answering benchmark towards multimodal numerical reasoning,\n2022.\n[49] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming\nLiu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction\ntuning. arXiv e-prints , pages arXiv–2407, 2024.\n[50] Adam Dahlgren Lindström and Savitha Sam Abraham. Clevr-math: A dataset for compositional\nlanguage, visual and mathematical reasoning, 2022.\n[51] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: A\ndataset for question answering on choropleth maps, 2022.\n[52] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan\nCatanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist\nembedding models. arXiv preprint arXiv:2405.17428 , 2024.\n[53] Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering based on\nhierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data\nmining , pages 160–172. Springer, 2013.\n[54] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan,\nGaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning\nsystem at scale. arXiv preprint arXiv:2503.14476 , 2025.\n[55] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong\nLiu, Shuxian Liang, Junxian He, et al. A survey of efficient reasoning for large reasoning\nmodels: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614 , 2025.\n[56] Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, and Yu Cheng. Scaling reasoning, losing control:\nEvaluating instruction following in large reasoning models. arXiv preprint arXiv:2505.14810 ,\n2025.\n[57] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\narXiv:2505.07062 , 2025.\n[58] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv\npreprint arXiv:2410.21276 , 2024.\n[59] Anthropic. Claude 3.7 sonnet. https://www.anthropic.com , 2024.\n[60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.\n14\n--- Page 15 ---\n[61] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shen-\nglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source\nmultimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271 ,\n2024.\n[62] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang,\nYanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR ,\nabs/2408.03326, 2024.\n[63] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and\nCihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language\nmodels. arXiv preprint arXiv:2504.11468 , 2025.\n[64] Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Hang Zhang,\nYuming Jiang, Xin Li, Deli Zhao, Fan Wang, Yu Rong, Aixin Sun, and Shijian Lu. Mmr1:\nAdvancing the frontiers of multimodal reasoning. https://github.com/LengSicong/MMR1 ,\n2025.\n[65] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu,\nBotian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang,\nand Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale\nreinforcement learning. CoRR , abs/2503.07365, 2025.\n[66] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and\nYongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 3: System Demonstrations) , Bangkok, Thailand, 2024. Association for Computational\nLinguistics.\n[67] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen\nXiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.\ncom/hiyouga/EasyR1 , 2025.\n[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning\n(ICML) , pages 8748–8763. PMLR, 2021.\n[69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4:\nEnhancing vision-language understanding with advanced large language models. In The Twelfth\nInternational Conference on Learning Representations , 2024.\n[70] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326 , 2024.\n[71] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan\nLi. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,\n2024.\n[72] OpenAI. Introduction to chatgpt-o3, 2025.\n[73] Kimi Team. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint\narXiv:2501.12599 , 2025.\n[74] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[75] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig,\nWenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction\ntuning at scale, 2024.\n[76] Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo:\nTowards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192 ,\n2024.\n15\n--- Page 16 ---\n[77] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-\nthought prompting for large multimodal models, 2024.\n[78] Minghe Gao, Shuang Chen, Liang Pang, Yuan Yao, Jisheng Dang, Wenqiao Zhang, Juncheng Li,\nSiliang Tang, Yueting Zhuang, and Tat-Seng Chua. Fact :teaching mllms with faithful, concise\nand transferable rationales, 2024.\n[79] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems ,\n35:27730–27744, 2022.\n[80] Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. Test-time preference optimization:\nOn-the-fly alignment via iterative textual feedback. arXiv preprint arXiv:2501.12895 , 2025.\n[81] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee,\nand Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint\narXiv:2503.20783 , 2025.\n[82] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: A simple and\nstrong reinforcement learning baseline for model reasoning, 2025.\n[83] Minghe Gao, Xuqi Liu, Zhongqi Yue, Yang Wu, Shuang Chen, Juncheng Li, Siliang Tang,\nFei Wu, Tat-Seng Chua, and Yueting Zhuang. Benchmarking multimodal cot reward model\nstepwise by visual program, 2025.\n[84] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 9556–9567, 2024.\n[85] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities.\narXiv preprint arXiv:2308.02490 , 2023.\n[86] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng\nLi, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal\nllm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint\narXiv:2408.13257 , 2024.\nA Appendix\nA.1 Training settings\nThe training process can be divided into three distinct phases: cold start, multimodal reinforcement\nlearning, and text-only reinforcement learning. Key hyperparameters for each training phase are\ndetailed in Table 5.\nA.2 Algorithm in Prioritized Advantage Distillation (PAD)\nThe PAD mechanism, introduced conceptually in the main text, is detailed in Algorithm 1 to clarify\nits step-by-step operation in refining training batches for more effective learning.\nInitially, PAD filters the original batch Bto create an “effective set” Eof sample indices and a\ncorresponding map ˆAEfor their advantages (Lines 2-10 in Algorithm 1). For each sequence iin\nB, its absolute advantage |˜Ai|is computed. If this value falls within a specified informative range\n[Tlow, Thigh], where Tlow>0is crucial for excluding stagnant (near-zero advantage) samples, the\nindex iis added to E, and its absolute advantage ˆAi,abs is stored in ˆAE.\nIf this effective set Eis non-empty, prioritized sub-sampling is performed (Lines 12-29). This\nmulti-step process involves: (a) Calculating sampling probabilities Pjfor each sequence index j∈ E\n16\n--- Page 17 ---\nTable 5: Key Hyperparameters for Training Stages.\nComponent Hyperparameter Component Hyperparameter\nCold StartLearning Rate = 2.0×10−5\nActorGlobal Batch Size = 128\nGradient Accumulation = 8 Micro Batch Rollout = 4\nNumber of Epochs = 5 Max Grad Norm = 1.0\nLR Scheduler = Cosine Learning Rate (lr) = 1×10−6\nWarmup Ratio = 0.05 Weight Decay = 1×10−2\nMax Sequence Length = 32768 Entropy Coef Init ( β0) = 0.02\nPrecision = BF16 Entropy Coef Min ( βmin) = 0.0\nDeepSpeed = Zero2 Entropy Decay Rate ( λ) = 0.985 (exp)\nEntropy Warmup Steps ( τw) = 140\nTotal Updates = 200000\nGRPOAdv Estimator = grpo\nKL Penalty Type = low var kl\nKL Coef = 2×10−3\nτ= 0.3Model SettingsMax Prompt Length = 8192\nMax Response Length = 8192\nRollout Batch Size = 512\nGeneration Temperature = 1.0\nGeneration Top P = 0.95\nFigure 5: Prompt Template for ReVisual-R1 in both inference and training stages.\nvia a temperature-controlled Softmax distribution over their stored absolute advantages ˆAE[j](Lines\n14-21). A uniform probability distribution across Eserves as a fallback mechanism should the\nSoftmax normalization term Zbe zero. (b) Determining the sub-sample size k′asmin(⌈ρN⌉,|E|),\nwhere ρis the sub-sampling ratio and Nis the original batch size (Line 23). (c) Sampling k′indices\nfromEaccording to the calculated probabilities Pdist(Line 25). (d) Constructing the final distilled\nmini-batch Bdistilled by retrieving the original sequences corresponding to these k′sampled indices\n(Lines 27-29). If Eis void, an empty batch is returned. This entire procedure ensures that training\nbatches are enriched by systematically filtering out uninformative data and prioritizing samples\nanticipated to yield more substantial learning signals.\nA.3 Performance on Multimodal General Benchmarks\nIn this section, we provide an analysis in Table 6 to reveal that ReVisual-R1 demonstrates strong\nand competitive performance across these general MLLM benchmarks. Specifically, on the MMMU\nbenchmark [ 84], Revisual-R1 secures the second-best score ( 50.55 ), closely following ThinkLite-\n7B-VL. It achieves leading performance on MM-Vet [ 85] with a top score of 49.81. In the MME-\nRealWorld benchmark [ 86], Revisual-R1 (62.68) delivers a solid performance, though it is surpassed\nby mmr1-mathv0 and ThinkLite-7B-VL. These results underscore Revisual-R1’s robust and well-\nrounded reasoning capabilities on these general multimodal tasks, particularly its notable strength on\n17\n--- Page 18 ---\nAlgorithm 1 Prioritized Advantage Distillation (PAD)\nRequire: Original batch B={seq1, . . . , seq N}withN=|B|; advantage estimates Aest={˜A1, . . . , ˜AN};\nthresholds Tlow, Thigh; temperature τ; sub–sampling ratio ρ\nEnsure: Distilled mini-batch Bdistilled\n▷Steps 1 & 2: Per-Sequence Advantage Calculation and Effective Sample Filtering\n1:E ← ∅ ▷Set of indices for effective samples\n2:ˆAE← {} ▷Map: original index i∈ E → its absolute advantage ˆAi\n3:fori←1toNdo\n4: ˆAi,abs← |˜Ai| ▷Absolute advantage of current sequence i\n5: ifTlow≤ˆAi,abs≤Thighthen\n6: E ← E ∪ { i} ▷Add index to effective set\n7: ˆAE[i]←ˆAi,abs ▷Store absolute advantage for effective sample i\n8: end if\n9:end for\n10:Bdistilled ← ∅\n11:if|E|>0then\n▷Step 3: Prioritized Sub-sampling from the Effective Set\n▷a. Calculate sampling probabilities Pjfor each j∈ E\n12: Z←X\nj∈Eexp( ˆAE[j]/τ) ▷Normalization term (Softmax denominator over E)\n13: Pdist← {} ▷Map: original index j∈ E → its sampling probability Pj\n14: for all j∈ Edo\n15: ifZ >0then\n16: Pdist[j]←exp( ˆAE[j]/τ)/Z\n17: else\n18: Pdist[j]←1/|E| ▷Uniform fallback if Z= 0\n19: end if\n20: end for\n▷b. Determine actual sub-sample size k′\n21: k′←min\u0000\n⌈ρN⌉,|E|\u0001\n▷c. Sample k′indices from Eaccording to probabilities Pdist\n22: Ssampled _indices←SAMPLE (E, Pdist, k′) ▷ Ssampled _indices is a list of k′indices from E\n▷d. Form the distilled mini-batch\n23: for all idx∈Ssampled _indices do\n24: Bdistilled ← B distilled ∪ {seqidx}\n25: end for\n26:end if\n27:return Bdistilled\nMM-Vet, within a competitive field where no single open-source 7B model consistently dominates\nall evaluated benchmarks.\nTable 6: Performance of open-source multimodal reasoning 7B models on general MLLM benchmarks\nModel MMMU MM-Vet MME-RealWorld\nVL-Rethinker 42.22 47.59 61.43\nOpenVLThinker 19.37 40.73 13.46\nMMR1-Math-v0 47.33 42.75 69.12\nR1-VL-7B 48.23 45.64 59.31\nThinkLite-7B-VL 51.22 37.44 68.49\nVLAA-Thinker-7B 50.44 41.46 64.44\nRevisual-R1 50.55 49.81 66.68\nA.4 Reasoning Example\n18\n--- Page 19 ---\nQuestion: \nWhich of the boxes comes next \nin the sequence? \nSelect answers fr om A-E\nOurs Revisual-R1 7B Model: \n<think>  \nOkay , let's try to figure out the sequence here. So there are five options labeled A to E, each with some black squares in dif ferent positions.  \nThe problem is asking which one comes next in the sequence.\nFirst, I need to look at the given figur es and see if ther e's a pattern . Let me list them out again to make sure I have them right, Top row:\n1. A: A square divided by a diagonal, with a black square in the bottom right corner .\n2. B: Two vertical stripes, each with a black square on the right side.\n3. C: A square with a diagonal from top left to bottom right, and a black square in the top left quadrant.\n4. D: Two vertical stripes, with a black square in the right stripe's upper part.\n5. E: A square split into two vertical parts, with a black square in the left part.\n[Reflection] Wait, maybe I should number them as per their order in the problem. The first row is options 1 to 5, then the second row is A to \nE. Wait, the pr oblem shows  the first row as the original five, then the next row labeled A to E. \n[Revisit Image]W ait, looking back :The user presented the problem with two rows. The first row has five images (probably labeled 1 to 5),  \nbut the user then labels the second row as A to E. Wait, the actual question's options are A to E, each in the second row . \nThe sequence seems to  involve the placement of the black squares relative to the diagonals or divisions of the square. Let me analyze each  \noption :\nOriginal top row (before the line):……\nThen the next row is labeled A to E: A: Square split vertically , left third has a black square.……\n[Self-Critic] Wait, perhaps the original pr oblem's figur es ar e arranged as follows :\nFirst row (top): ……\nSecond row (labeled A to E): A: Vertical division, left part has a black square.……\n[Logic] Now , trying to see the pattern. Let's look at the original figur es (befor e the line) and then the options A-E.  The task is to  \ndetermine which comes next. Looking at  the first row's figures (original positions):……\n[Self-Critic] Wait, maybe the pattern alternates between diagonal and vertical splits.  Let's check :\nOriginal sequence (top row):1. Diagonal, lower right.……\nThen the next row (A-E): A: Vertical split (three parts?), left part black.……\nIf the pattern is alternating between diagonal and vertical splits, then after figure 5 (diagonal), the next would be a vertical split. \n[Hypothesis] Looking at the original vertical splits: figures 2, 4, and then maybe  A, D, E. \nLet's see: Original vertical splits (two parts) are figures 2 (lower right), 4 (upper right). Then after figure 5 (diagonal), next could be another  \nvertical split. Among options A-E, which are vertical splits: A: vertical split into three columns? Or two columns?  The image for A is a \nsquare split vertically into left and right, with a black square in the left part. D is vertical split into two, with black in the right upper . E is  \nvertical split into two, black in left lower .\nSo after figure 5 (diagonal), the next might be a vertical split. The last vertical split in the original sequence (figure 4) has the black square in  \nthe upper right. Then the next vertical split could be in the lower left (E) or upper left (A?), but need to check the order .\n[Hypothesis & Analysis] Alternatively , looking at the positions:\\n\\nOriginal vertical splits:\\n\\nFigure 2: right lower .\\n\\nFigure 4: right  \nupper .\\n\\nSo if the pattern for  vertical splits is alternating between lower  and upper  in the right section, then next could be lower  \nright again?  But  figure 2 is right lower , figure 4 is right upper . So maybe  the next vertical split would be right lower again. Then option E  \nis left lower . Hmm, not sur e.\nAlternatively , maybe the sequence is moving in a certain direction. Let's think of the figures as arranged in rows. The first row has five  \nfigures, and the second row has A-E. The correct answer is among A-E, which are the second row .\n[Hypothesis] Alternatively , the sequence might be based on r otation or  the position of the black squar e. For example, starting with  \ndiagonal, then vertical splits. Let's see:…… Wait, the options A-E are all vertical splits except B and C, which have diagonals. Wait, \noriginal figures 1-5 have a mix: 1,3,5 have diagonals; 2,4 have vertical splits. The options A-E: A is vertical split left; B is diagonal with  \nblack in upper right; C is diagonal upper left; D is vertical split right upper; E is vertical split left lower .\nSo after figure 5 (diagonal lower right), the next figure is likely a vertical split. Now , in the original vertical splits (figures 2 and 4), figure 2  \nis right lower , figure 4 is right upper . So maybe the next vertical split alternates.  If figure 4 was right upper , then next could be right lower  \nagain (figure 2's position). \n[Reflection]  But option E is left lower . Hmm. Alternatively , the vertical splits in the original sequence go fr om right lower  to right  \nupper . If the next vertical split is right lower again, but that's figure 2.  But E is left lower . \n[Hypothesis] Alternatively , maybe the dir ection is moving left .\nWait, maybe  the pattern is that after two diagonal figures (1,3,5), the vertical splits alternate between right and left. Let's see:\\n\\nOriginal  \nvertical splits: figure 2 (right lower), figure 4 (right upper). Then the next vertical split might be left lower (E) or left upper (A). But figure A \nis left black square, which is a vertical split left. If the pattern is moving from right to left, then after figure 4 (right upper), next would be  \nleft lower (E). \n[Self-Critic] But I need to check . Alternatively , looking at the positions of the black squares in the vertical splits:\\n\\nFigure 2: right third,  \nlower part.\\n\\nFigure 4: right third, upper part.\nSo the vertical split's black square moves from lower to upper in the same right third. Then, if the next vertical split is in the left third, \nmaybe lower  part (E) . So after moving right upper , next is left lower . That would make the pattern: right lower , right upper , left lower . So \nE. Alternatively , the answer is E.\n[Self-Critic]  Let's check the options: Original sequence: diagonal, vertical split (right lower), diagonal, vertical split (right upper),  \ndiagonal. Then next is vertical split (left lower , E). That seems plausible.Another way: looking at the positions of the black squares in the  \nvertical splits. The vertical splits in figures 2 and 4 have the black square in the right third. Figure 2's black square is at the lower part of the  \nright third, figure 4's is at the upper part. \nSo if we follow that pattern, the next vertical split would be in the left third, perhaps at the lower  part (E).  So the answer is E.\\n\n</think>\n<summary >\n The sequence involves alternating between diagonal and vertical split figures. After the diagonal figure (5), the next is a vertical split. The \nvertical splits in the original sequence (figures 2 and 4) have black squares in the right third, moving from lower to upper . The next vertical  \nsplit follows this pattern but in the left third, at the lower part. \nAnswer:  \\boxed{E}\n</summary>\nFigure 6: Our Revisual-R1 model reasoning case, showcasing its exceptional reasoning ability. The model\ngenerates long responses, continuously hypothesizing, reflecting, verifying, and correcting to arrive at the final\nanswer, while also providing a summary answer.\n19",
  "text_length": 72388
}