{
  "id": "http://arxiv.org/abs/2505.24722v1",
  "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
  "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
  "authors": [
    "Neil He",
    "Rishabh Anand",
    "Hiren Madhu",
    "Ali Maatouk",
    "Smita Krishnaswamy",
    "Leandros Tassiulas",
    "Menglin Yang",
    "Rex Ying"
  ],
  "published": "2025-05-30T15:42:42Z",
  "updated": "2025-05-30T15:42:42Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24722v1",
  "full_text": "arXiv:2505.24722v1 [cs.LG] 30 May 2025 HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts Neil He⋆Rishabh Anand⋆Hiren Madhu Ali Maatouk Smita Krishnaswamy Leandros Tassiulas Menglin Yang†Rex Ying Yale University, USA⋆Equal Contribution Open-source code: github.com/Graph-and-Geometric-Learning/helm Abstract Frontier large language models (LLMs) have shown great success in text modeling and generation tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations such as dot- products and norms. Furthermore, recent studies have shown that not respecting the underlying geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. To this end, we introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of- Curvature Experts model, HELM-M ICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM- D. For HELM-M ICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA ) for efficient, reduced-KV-cache training and inference. For both models, we further develop essential hyperbolic equivalents of rotary positional encodings and root mean square normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures – up to 4% – over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale language model pretraining. 1 Introduction Contemporary Large Language Models (LLMs) [ 18,42,9,1] fundamentally operate within Euclidean space. This manifests in their reliance on Euclidean operations such as dot products and norms applied to token embeddings. However, this architecture presents a potential mismatch with the intrinsic structure of natural language data. Existing works have shown that textual data, particularly token inputs to LLMs, exhibit an inherent semantic hierarchy [ 48,47,21,35], thus requiring a space that can naturally accommodate these relationships. An ideal LLM architecture would possess geometric alignment with the underlying structure of the data it aims to represent. †Work done while at Yale University, USA. Preprint. Under review. 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Ollivier-Ricci Curvature0.000.020.040.060.080.100.120.14ProbabilityLlama-2-7B Gemma-2-2B DeepSeek-MoEFigure 1: Ricci curvature distribution of token embeddings from decoder-only LLMs showing substantial variation of negative curvature, implying higher local hyperbolicity.To further illustrate the unique geometry of text data, we show in Figure 1 the distribution of Ricci Curvature of token embeddings from popular decoder-only LLMs on 1000 diverse samples from RedPajama [ 46]2. We ob- serve that the vast majority of tokens exhibit a wide range of negative curvatures. This has also been observed by Robinson et al., who investigate the subspace of to- ken embeddings and its inherent non-Euclidean structure. As Ricci curvature measures the local geometry of a man- ifold, these empirical observations suggest hierarchical token structures, while the variation in curvature values suggest complex token geometry that cannot be captured by single curvature approaches. Robinson et al. also show that not respecting the geometry of tokens will harm a Transformer-based LLM’s generative capabilities while introducing undue training instabilities. He et al. also document power law distributions of token frequencies, implying hierarchy among tokens. We thus propose a geometric rethinking of the transformer-based LLM: operate fully in hyperbolic space, where the negative curvature results in exponentially increasing volume w.r.t. distance. Hyperbolic space is expansive, scale-free, and yields low distortion, and has shown success in numerous settings, particularly in Transformer architectures [48,5,19,6,39]. It provides token embeddings more “breathing room” in the network, better aligning with the underlying structure of the text data. However, previous works that study hyperbolic Transformers and pre-trained models have several major shortcomings: (1) Inflexible geometric spaces. They assign each Transformer block a single hyperbolic manifold, embedding entire token sequences in fixed-curvature spaces. This approach does not align with the observed substantial variant in curvature values as noted above, thus limiting the expressiveness of hidden representations. (2) Lack of essential operations. They omit widely-used LLM components such as rotary positional encoding and RMS normalization, and lack theoretical guarantees of LLM modules in Euclidean settings; (3) Poor scalability. They focus on low-dimensional settings and use quadratic hyperbolic self-attention mechanisms that do not scale comparably to modern Euclidean foundation models [ 18,10]. In this work, we address the limitations of both Euclidean LLMs and prior hyperbolic Transformers through the following contributions: •To alleviate limitation (1), we introduce hyperbolic Mixture-of- Curvature Experts ( MICE), where each expert operates in a distinct curvature space, enabling the model to encode fine-grained geometric structure from text. The mixed-curvature strategy employed by MICEcaptures the range of negative curvatures prevalent among token embeddings, mitigating previous hyperbolic Transformers’ representational inflexibility. •To resolve limitation (2), we introduce several novel hyperbolic modules to develop hyperbolic LLMs: Hyperbolic Rotary Positional Encodings ( HOPE) and Hyperbolic RMSNorm, bridging the gap between hyperbolic Transformers and modern Euclidean LLMs. Additionally, we provide extensive theoretical analysis that provides similar guarantees as in the Euclidean case. •To address limitation (3), we propose Hyperbolic Multi-head Latent Attention ( HMLA ) to perform efficient inference with a reduced-footprint KV cache and reduce active memory during training, thereby bridging the scalability gap between previous hyperbolic Transformers and current Euclidean LLMs. •Finally, we introduce HELM, the first attempt at building a family of fully HypErbolic Large Language Models. We are the first to train at billion parameter scale and outperform popular Euclidean architectures on a diverse set of benchmarks. 2 Related Work Hyperbolic Transformers. Hyperbolic geometry has emerged as a powerful embedding space for representational learning, particularly in domains characterized by hierarchical and scale-free structures [ 16,34,15]. Hyperbolic Neural Networks (HNN) [ 15] and their extension HNN++ [ 39] 2Ricci Curvature is a metric of how strongly or loosely connected a region of space is. The more negative the curvature, the more hyperbolic the space is. We describe the metric in Appendix A.1. 2 have shown that operating in hyperbolic spaces increases representational capacity. Several works have incorporated hyperbolic geometry into Transformers to better capture semantic relationships. HAT [ 19], HNN++ [ 39], and HyboNet [ 5] all propose equivalent formulations of hyperbolic attention in different models of hyperbolic space. HypFormer [ 48] developed several essential modules that were lacking in previous works, showing improved performance for structured and hierarchical datasets. Although work on hyperbolic pre-trained language models exist [ 6], they ignore essen- tial components required to train large language models, such as normalization layers, residual connections, roatry positional encodings. Furthermore, these works suffer from the limitations mentioned in Section 1. Previous work have also considered mixed-curvature Transformers [ 7], however, they only using different curvature values in each attention head and relying on tangent space methods that are prone to mapping errors and numerical stability [ 5,48,20,3]. Further related works include LResNet [ 23] who introduces an efficient hyperbolic residual connection method that mitigated numerical instability and tangent space mappings. Some works have devised hybrid approaches by incorporating hyperbolic components to existing pre-trained Euclidean LLMs and vision models [ 12,36,47,17,49,31]. However, these works do not develop and pre-train hyperbolic LLMs from scratch. Our work advances this line of research by developing and scaling hyperbolic architectures to large-scale pretraining setups, and additionally introducing novel components for efficient and expressive language modeling in hyperbolic space. Open Large Language Models. The recent surge in open LLMs has democratized access to cutting- edge NLP capabilities, enabling broader research and application development. Notable among these is LLaMA [ 44], which introduced a family of efficient and powerful models trained on diverse, large-scale corpora. Llama models employ several optimizations such as rotary positional embeddings and grouped query attention, making them competitive across various downstream tasks. Building on these ideas, Gemma [ 42] introduced further improvements for better data curation, advanced pertaining techniques, and careful model scaling strategies. In parallel, Mixture-of-Experts (MoE) architectures have emerged as a prominent design to enhance model capacity without a proportional increase in computation cost. DeepSeek-MoE [ 11] introduces an efficient routing mechanism to dynamically activate a subset of experts per input, significantly improving inference throughput compared to other MoE models such as Mixtral [ 27]. However, all these models are inherently Euclidean, and while effective, may not align well with the geometry of the token subspace. 3 Preliminary We introduce the relevant background required to understand the building blocks of HELM, including hyperbolic geometry, hyperbolic self-attention, and other useful hyperbolic modules. 3.1 Lorentz Hyperbolic Geometry There are several isometric models of hyperbolic space employed in previous research [ 45,19,43, 34,48,5,3]. In this study, we choose the Lorentz model: its special space-time interactions allow forfully hyperbolic operations, offering enhanced expressiveness and empirical stability from an optimization perspective [ 5,48,33]. Nevertheless, our methods can be easily reformulated for other models of hyperbolic geometry via isometric mappings. Lorentz model. Ann-dimensional Lorentz model, denoted as LK,n, is a Riemannian manifold Ln equipped with the Riemannian metric tensor gK n= diag( −1,1,..., 1)and defined by a constant negative curvature K < 0. Each point x∈LK,nhas a parametrized form [xt,xs]T, where xt∈R is the time-like dimension and xs∈Rnis the space-like dimension. For points x,y∈LK,n, their Lorentzian inner product ⟨x,y⟩Lis given by ⟨x,y⟩L=−xtyt+xT sys=xTgK ny.Hence, the Lorentzian norm |∥x∥|L:=p |⟨x,x⟩L|. Formally, the point set Ln:={x∈Rn+1:⟨x,x⟩L= 1/K, x t>0}. The origin o∈LK,nis the point [p −1/K,0,..., 0]T. Tangent space. The tangent space at a point x∈LK,nis set of points orthogonal to x, defined as TxLK,n={y∈Rn+1:⟨x,y⟩L= 0}.Notably, the tangent space is isomorphic to Euclidean space. 3.2 Hyperbolic Neural Network Modules Extensive work has been done to develop hyperbolic neural network modules [ 48,5,3,45,23,15, 39, 19], which we introduce below. 3 Lorentz Linear Layer. Given curvatures K1, K2, and parameters W∈R(n+1)×mandb∈Rm, the Lorentzian linear transformation with curvature  is the map HLT: LK1,n→LK2,mgiven by, HLT(x;W,b) =r K2 K1·\u0014q ∥W⊤x+b∥2−1/K2,W⊤x+b\u0015. (1) Lorentz Residual Connection. Letx, f(x)∈LK,nwhere xis an input vector and f(x)is the output of a neural network f. Then, the Lorentzian residual connection  is given by, x⊕Lf(x) =α1x+α2y, αi=wi/\u0010√ −K|∥w1x+w2f(x)∥L|\u0011 fori∈ {0,1}, (2) where α1, α2are weights parametrized by constants (w1, w2)∈R2\\ {(0,0)}. Hyperbolic self-attention. Hyperbolic self-attention is defined equivalently in different models through manifold midpoints [39,19,5]. Given Ttokens Xwhere xi∈LK,n,WQ,WK,WV∈ R(n+1)×d, then the Lorentzian self-attention [5, 48] is given by AttL(x)i=PT j=1νi,jvj√ −K|∥PT l=1νi,kvl∥|L, where νi,j=exp\u0000 −d2 L(qi,kj)/√m\u0001 PT l=1exp (−d2 L(qi,kl)/√m)(3) where dLis the Lorentzian distance and Q= HLT( X;WQ,bQ),K= HLT( X;WK,bK),V= HLT(X;WV,bV)are the queries, keys, and values respectively. 4 Method In this section, we propose several novel hyperbolic modules that serve as building blocks for HELM. We then introduce the overall architecture of HELM: a Mixture-of-Curvature variant, HELM-M ICE, and a dense variant, HELM-D, for comparison. 4.1 Hyperbolic Rotary Positional Encoding Previous works that proposed positional encoding methods in hyperbolic space [ 5,48,22] are confined to only learning relative encodings. However, contemporary LLMs [ 18,10,4] have instead shifted towards Rotary Positional Encodings (RoPE) [ 40], a scalable alternative that incorporates aspects from both absolute and relative encoding methods, improving length generalization and downstream performance. We thus propose a novel hyperbolic rotary positional encoding (HOPE) to construct positional encodings in hyperbolic space, and prove the same theoretical guarantees as RoPE. Given Ttokens X, where xi∈LK,d(deven), let Q,Kbe the queries and keys as in Equation (3). The hyperbolic rotary positional encoding applied to the i-th token is, HoPE( zi) =\u0014p ∥Ri,Θ(zi)s∥2−1/K Ri,Θ(zi)s\u0015;Ri,Θ= Ri,θ1 0 0... 0 0Ri,θ20... 0..... 0 0 0...... Ri,θd/2 ,(4) where Ri,θlis the 2D rotation matrix parameterized by angle iθlandzcan be a query qior key kj. Next, we study the theoretical aspects of HOPE; all proofs can be found in Appendix A. First note that sinceRi,Θis an Euclidean rotation matrix, it isometrically preserves the (Euclidean) norm of vectors. Given the definition of the Lorentz model (Section 3.1), an equivalent expression for Equation (4) is HoPE( zi) =\u0012 1 0 0Ri,Θ\u0013 zi, making HoPE a valid Lorentzian rotation operation (see, e.g., ). Validity. A defining characteristic for the Euclidean RoPE is that the inner product of the encoded keys and queries is a function of only the word embeddings and their relative position. Thus, only the relative positional information is encoded [ 40]. For hyperbolic attention in Equation (3), the analogous is defined with −d2 Linstead, which we formalize below. Proposition 4.1. LetXbeTtokens with xi∈LK,d. LetQ,Kbe queries and keys as in Equation (3). Then−d2 L(HoPE ( qa),HoPE ( kb))) = g(xa,xb;a−b)for some function g. 4 HOPEonly encodes relative positional information based on Proposition 4.1 similar to RoPE, which establishes its validity as a RoPE operation. Long-term decay. A desiring property of RoPE is long-term decay, where the attention score between a key-query pair decays when the relative position increases. HOPEhas the same property as well, as shown by the following proposition. Proposition 4.2. LetQ,Kbe as defined in Equation (3), then the negative square Lorentz distance −dL(HoPE ( qi),HoPE( kj))can be upper bounded by f(qi,kj)g(i−j)<0, where fhas no dependencies on position, and gdepends entirely on relative position and scales inversely w.r.t. i−j. Thus, HOPEensures far-apart tokens have weaker connections based on Proposition 4.2, a property not guaranteed by previous learned encoding methods. Robustness to arbitrary token distances. Barbero et al. recently show that decaying token dependency (Proposition 4.2) may not be the primary reason for RoPE’s success, but rather it enables LLMs to attend to specific relative distances. Our formulation of H OPE also ensures this property: Proposition 4.3. LetQ,Kbe as defined in Equation (3), then HOPEcan be maximal at an arbitrary distance, i.e., for any relative distance r∈Z, there exists a key kjsuch that the softmax value is maximum at distance r. HOPEthus provides hyperbolic transformers the flexibility to decay token dependencies while also ensuring robust attention across arbitrary relative distances. Positional attention. Barbero et al. also prove that using RoPE can help capture purely positional relationships via diagonal attention patterns, where tokens only attend to themselves, and off-diagonal attention patterns, where tokens attend only to their preceding neighbor. H OPE also allows for this: Proposition 4.4. Attention heads with HOPEcan learn diagonal or off-diagonal attention patterns. We provide ablations comparing H OPE to prior hyperbolic positional encodings in Appendix D. 4.2 Hyperbolic Mixture-of-Curvature Module Previous hyperbolic Transformer architectures fix each block to a single hyperbolic manifold, forcing the entire sequence to be embedded with a fixed curvature, restricting the flexibility of the hidden representations. Mixture-of-Experts (MoE) [ 26] used in Euclidean LLMs [ 10,30,13,27,9], where the model selectively activates only a subset of specialized \"experts\" for each token, enables LLMs to learn more diverse data patterns while remaining computationally efficient. However, the experts are still limited to one geometric space, restricting the collective granularity the experts can learn. Accommodating variable curvatures calls for a more flexible treatment: we propose the first Mixture- of-Curvature Experts (MICE) module, where each expert operates on a distinct curvature space. Letxt∈LK1,nbe the t-th token input, then MiCENs Nr:xt∈LK,n→xt∈LK,n, where Nris the number of routed experts and Nsis the number of shared experts. First, we pass xtthrough a gating module to obtain the gating scores for each routed expert, denoted as gt,ifor1≤Nr, given as, gt,i=g′ t,iPNr j=1gt,j;st,j= act(( xt)⊤ syj);g′ t,j=\u001ast,j, st,j∈Topk({st,k}k≤Nr, Kr) 0otherwise.(5) Here, st,jis the token-expert affinity with activation function act,yjis the centroid vector of thei-th routed expert, Topk( S, A)picks the top Avalues from set S, and Kris the number of activated experts. Then, the token is passed through each shared and routed expert. Let HFFN r,i: LKr,i,m→LKr,i,mbe the routed experts and HFFN s,i:LKs,i,m→LKs,i,mbe the shared experts, defined through hyperbolic feedforward networks. Here, the value of Kr,iandKs,ican vary for each expert, i.e., each expert lives on a distinct manifold. To align the input’s manifold and the experts’ manifolds, first we project the tokens to the expert manifolds via st,i=p K/K s,ixtand rt,i=p K/K r,ixt. The projected token is passed through each expert and projected back to the input manifold, where we obtain yt,i=p Ks,i/KHFFN r,i(st,i)andzt,i=p Kr,i/KHFFN r,i(rt,i). The output of MiCENs Nris given by, MiCENs Nr(xt) =xt⊕L PNs i=1yt,i+PNr i=1zt,i√ −K|∥PNs i=1yt,i+PNr i=1zt,i∥|L!. (6) 5 (a)MICEmodule architecture. Routed experts are selected through a gating module. The token are project from input manifold to expert manifold and then passed through each expert. The output of each expert are then project back to the input manifold and merged together through Lorentzian centroid. This modules allows experts to learn from distinct curva- ture spaces to allow for more granularity. (b)HMLA framework. The embeddings are projected into latent space and then upward projected into queries, keys, and values. Ad- ditional decoupled queries and a shared key are created for hyperbolic positional encoding through HOPE. The queries and keys are con- catenated together before performing hyperbolic self-attention. Figure 2: Mixture-of-Curvature Experts ( MICE) and hyperbolic Multi-Head Latent Attention (HMLA). The constantsp Ks,i/K,p Kr,i/Kproject from the experts’ manifolds to the input manifold, ensuring that the output of each shared and routed expert lives on the same manifold. The outputs are then combined through the Lorentzian centroid [ 29], before performing a Lorentzian residual connection. Note that MICEis indeed a valid hyperbolic module, which we expand on in Appendix B. 4.3 Efficiency via Hyperbolic Multi-Head Latent Attention Previous hyperbolic Transformers for natural language applications focus mainly on hyperbolic self- attention, synonymous to naive dot-product attention mechanism in Euclidean LLMs. However, recent Euclidean LLMs have gradually adopted more efficient attention methods for enhanced scalability. The quadratic attention mechanism and the extant hyperbolic self-attention formulation suffer from heavy memory challenges for large-scale training. In this section, we propose hyperbolic Multi-Head Latent Attention ( HMLA )to alleviate this bottleneck. Inspired by Euclidean MLA [ 11,10],HMLA reduces the size of the KV cache during inference and the active memory consumption during training. We provide a high-level description of HMLA; detailed formulation can be found in Appendix B.2. Letxt∈LK,nh nbe the t-token, where nis the embedding dimension and hnis the number of heads. First, we project xtviaHLT to latent queries and key-value vectors, obtaining cQ t,cKV tof dimen- sions nq, nkvrespectively such that nq, nkv≪nhn. We then upward-project the latent vectors back tohnheads of dimension neach via HLT, obtaining [kC t,i]i≤hn,[vC t,i]i≤hnfromcKV t, and [qC t,i]i≤hn fromcQ t. Then the projected keys are processed by positional encodings. Following previous works [11,10], as RoPE is incompatible with MLA due to position coupling, we employ a decoupled HOPEscheme with HMLA. Through HLT, we project cQ tto additional query vectors, [qR t,i]i≤hn, andcKV tto a shared key, kR t, of dimension nrhnandnrrespectively ( nr≪nhn). The queries vectors, qC t,i,qR t,i, and the key vectors, kC t,i,kR t, are then concatenated together through Lorentzian concatenation [ 37,48] for each i, where we obtain the final query and key vectors qt,i,kt,i. The atten- tion score and output are computed using Equation (3) with [qt,i]t≤hn,[kt,i]t≤hn,[vC t,i]t≤hnas the queries, keys, and values, before concatenating all the heads together with Lorentzian concatenation. The concatenated result is passed through a final projection layer with HLT. Memory complexity. During inference, HMLA only requires caching the latent key-value pairs. As a result, the memory footprint for HMLA isO((nkv+nr)L), where Lis the number of layers. In contrast, the hyperbolic self-attention used in previous hyperbolic Transformers (Equation (3)) requires storing the full-sized keys and values, resulting in a memory complexity of O(2nnhL). By choosing nkv, nr≪nnh, we have (nkv+nr)≪2nnh, resulting in a significantly smaller memory footprint while maintaining the same time complexity of O((nnh)2). Additionally, the latent query 6 INPUT Text tok ens Hyperbolic EmbeddingsHyperbolic Multi-head Latent Attention Hyperbolic Multi-head AttentionorLorentz RMSNorm Lorentz RMSNormMixture-of - Curvature Experts (MiCE) Hyperbolic Feed-F orward Networkor Lorentz RMSNorm LogitsFigure 3: HELM architecture. The input tokens are mapped to hyperbolic word embeddings before being processed by a series of Ldecoder blocks, comprising an attention block and an FFN block. The attention block (blue) can either be hyperbolic self-attention or HMLA, while the FFN block (yellow) can either be a HFFN or MICElayer. The output of the decoder blocks is mapped to logits. Residual connections are omitted for brevity. projection also results in smaller active footprint during training. This collective mechanism enables far greater scalability. 4.4 Hyperbolic RMSNorm Previous works have not implemented hyperbolic root mean square normalization (RMSNorm), which is widely used in popular Euclidean LLMs [ 18,11,10] due to its stability and robustness in both forward and backward passes. Here, we propose hyperbolic RMSNorm to fill the gap: RMSNorm L(x) =hp ∥RMSNorm( xs)∥ −1/K,RMSNorm( xs)i⊤. (7) Here, RMSNorm denotes the corresponding Euclidean operation. Equation (7) is equivalent to passing RMSNorm into the HRC functions from Hypformer [ 48]. This formulation is chosen over other methods of defining normalization layers through hyperbolic midpoint and tangent space operations [45, 3] for better numerical stability and computational efficiency. Invariance to input scaling. Our formulation of hyperbolic RMSNorm is invariant to a scaling of inputs, giving us similar guarantees as Euclidean RMSNorm in terms of gradient stability during backpropagation and enhanced robustness to perturbations. We provide proofs for these guarantees in Appendix A.7. Proposition 4.5. RMSNorm Lis invariant to scaling of inputs xduring both the forward and backward passes. 4.5 Overall Architecture for Hyperbolic Large Language Models (HELM) We introduce the framework for hyperbolic LLMs ( HELM ) based on the modules we introduced and developed in Section 4. In particular, we develop hyperbolic LLMs with Mixture-of-Curvature Experts (HELM-M ICE), a class of hyperbolic LLMs with MoE modules where each expert functions in a distinct curvature space. We also construct hyperbolic dense LLMs (HELM-D ), which resembles classic decoder-only LLMs such as Llama. The overall architecture is as follows (see Figure 3): tokenized text is first mapped to learned hyper- bolic word embeddings. The resulting embeddings are passed through a series of hyperbolic decoder blocks, each consisting of two components: 1) the attention component, where the embeddings are normalized by a RMSNorm Llayer, then processed by an attention block such as HMLA or self-attention, and finally added to the embeddings through Equation (2); and 2) the HFFN compo- nent, where the processed embeddings are again normalized by RMSNorm Lbefore being passed through a HFFN block and residually added to the output of the attention block (Equation (2)). For HELM-M ICE, the HFFN block can either be a dense block such as HFFN SGor aMICEblock as defined in Section 4.2, where HFNN SGis a hyperbolic SwiGLU FFN we built to be consistent with Euclidean LLMs (see Appendix B.3 for details). HELM-D contains only dense HFFN layers. The output of the final decoder block is then normalized once again before projected to logits for next-token prediction. 7 Table 1: Multichoice question-answering accuracy (%) of HELM models and their Euclidean counterparts. Bold denotes highest accuracy and underline denotes second-highest. HELM models consistently outperform their Euclidean counterparts, with HELM-M ICEachieving the highest accuracy overall. Model # Params CommonsenseQA HellaSwag OpenbookQA MMLU ARC-Challenging Avg 0-Shot 0-Shot 0-Shot 5-Shot 5-Shot - LLAMA 115M 21.1 25.3 25.3 23.8 21.0 23.3 HELM-D 115M 20.1 25.9 27.0 25.8 21.2 24.0 DEEPSEEKV3 120M 19.2 25.2 23.4 24.2 21.8 22.8 HELM-M ICE 120M 19.3 26.0 27.4 24.7 23.5 24.2 DEEPSEEKV3 1B 19.5 26.2 27.4 23.6 22.7 23.9 HELM-M ICE 1B 19.8 26.5 28.4 25.9 23.7 24.9 5 Experiments We evaluate both HELM variants’ ability to answer MCQ questions in two popular benchmarks, MMLU [ 24] and ARC [ 8]. Additionally, we train an ablation HELM-M ICEwith constant curvature across experts, comparing with HELM-M ICE models with varying curvature across experts. 5.1 Multichoice Benchmarking We evaluate both HELM-M ICEandHELM-D at 100M-parameter scales, across a variety of benchmarks spanning STEM problem-solving, general knowledge, and commonsense reasoning. The dense models also serve as an ablation comparison with the MICEmodels. We further scale theHELM-M ICEto 1B parameters as the smaller HELM-M ICEmodel outperformed HELM-D overall. Additional details regarding implementation and datasets can be found in Appendix B. Training Setup. We use the LLaMA3.1-8B tokenizer [ 18] for all models, with a vocabulary size of 128K. For HELM-D, we use hyperbolic self-attention and HFFN SGfor the decoder block. We use 6 heads and 6 layers for the 100M model. For HELM-M ICE, we use HMLA and a mixture of dense and MICElayers, each with 2 active experts and one shared expert. We use 6 heads, 6 layers, and 4 experts per layer for the 100M model, and we use 14 heads, 16 layers, and 8 experts per layer for the 1B model. The experts have curvature initiated uniformly from −0.1to−2.0. Additionally, we incorporate the auxiliary-loss-free load balancing scheme and complementary sequence-wise auxiliary loss from DeepSeekV3 [ 10] to encourage load balancing among the experts. Each model was trained on a cluster of 4 NVIDIA A6000 and 4 NVIDIA A800 GPUs with model and data parallelism, where at most 4 GPUs were used by each model. We use the English portion of the Wikipedia dataset [ 14] for training, comprising ∼6.4M rows of raw text, or roughly 5B tokens. Hyperbolic word embedding. Previous works [ 5,39] directly map input tokens to trained hyperbolic embeddings. However, we experienced model instability when training the 1B models with this method. Therefore, we only train the space-like dimension of the Lorentz word embeddings. Baselines. We test against two popular Euclidean models: one dense model and one MoE model. For the dense model, we test HELM-D against LLaMA [ 18]. For the MoE model, we test HELM-M ICE against DeepSeekV3 [ 10]. We train both baselines from scratch at the same parameter scales as their HELM counterparts, with the same dataset, tokenizer, and training setup. Benchmarks. We evaluate on a variety of benchmarks, including STEM and general knowledge reasoning benchmarks such as MMLU [ 24], ARC-Challenging [ 8], and OpenbookQA [ 32], and commonsense reasoning benchmarks such as CommonsenseQA [ 41],such HellaSwag [ 50]. For MMLU and ARC, we use 5-shot predictions. For CommonsenseQA, OpenbookQA, and HellaSwag, we use 0-shot prediction. Results. The results are shown in Table 1. We report the accuracy of the models’ abilities to answer multiple choice questions from the benchmarks. We mainly focus on comparing models within the same architectural sub-family, i.e., dense models and MoE models are separately tested against each other. Both HELM variants consistently outperform their Euclidean counterparts. In particular, the smaller HELM-D model achieves higher accuracy than LLaMA on four out of the five benchmarks, whereas the smaller HELM-M ICEmodel outperforms the smaller DeepSeekV3 model on all five 8 Table 2: Ablation accuracy, where we compare HELM-M ICEwith a variant where all experts have the same curvature value, denoted as MICE-C ONST. Bolding denotes the highest accuracy and underline denotes the second-highest. Euclidean DEEPSEEKV3results are shown for reference. Overall, HELM-M ICEconsistently achieves the highest accuracy, while both hyperbolic models still outperform the Euclidean counterpart. Model # Params CommonsenseQA HellaSwag OpenbookQA MMLU ARC-Challenging Avg 0-Shot 0-Shot 0-Shot 5-Shot 5-Shot - DEEPSEEKV3 120M 19.2 25.2 23.4 24.2 21.8 22.8 MICE-C ONST 120M 20.0 25.6 27.0 23.5 22.3 23.7 HELM-M ICE 120M 19.3 26.0 27.4 24.7 23.5 24.2 benchmarks. When comparing the ∼100M-scale HELM-D andHELM-M ICEmodels, the latter nearly always performs better and achieves higher accuracy overall. This reflects the effectiveness of using more flexible geometry, coupled with the overall fewer active parameters in HELM-M ICE. For the larger 1B-parameter models, HELM-M ICEconsistently outperforms the 1B DeepSeekV3 model, achieving the highest accuracy overall. In all cases, the hyperbolic LLMs achieve better overall scores across the five benchmarks. The HELM models also always achieve higher accuracy on the more difficult reasoning benchmarks, namely MMLU and ARC-Challenging. This suggests better reasoning capability afforded by in- corporating more suitable geometries in the embedding space. Overall, our results demonstrate the superiority of hyperbolic LLMs – in particular, the Mixture-of-Curvature Experts framework – in answering complex multiple-choice questions across a wide range of domains. 5.2 Ablating Distinct Curvature Learning with HELM-M ICE To assess the effectiveness of HELM-M ICEwhen each expert operates in a distinct curvature space, we additionally train a 120M-parameter HELM-M ICEmodel where the curvature of each expert is fixed to −1.0, which we denote as M ICE-C ONST. Consequently, M ICE-C ONST embeds the entire token sequence into a fixed space of curvature −1.0similar to a dense model. MICE-C ONST is trained with the same setup as the preceding models. We show the results in Table 2. HELM-M ICE outperforms the constant-curvature MICE-C ONST in 4 out of the 5 benchmarks and achieves the higher overall accuracy, demonstrating the effectiveness of learning more expressive presentation by setting each expert to learn within a distinct manifold. Notably, MICE-C ONST still outperforms the Euclidean DeepSeekV3 baseline on all 5 of the benchmarks, further demonstrating the effectiveness of hyperbolic LLMs over their Euclidean counterparts. 6 Conclusion In this work, we introduce HELM, a family of fully hyperbolic large language models trained at hundred-million and billion-parameter scales. Operating entirely in hyperbolic space, HELM models are better aligned with the variable geometric structure of text and token distributions. We develop MICEmodules to construct HELM-M ICEvariants, enabling fine-grained geometric learning and more expressive and geometrically flexible hidden representations. We further introduce HMLA mechanism to enable HELM models to be memory efficient and improve scalability. We also introduce the HOPEandRMSN ORMLmodules, which are fundamental to building modern hyperbolic LLMs, and support them with extensive theoretical analysis and guarantees. Trained on 5B tokens, HELM models outperform their Euclidean counterparts across benchmarks in STEM reasoning, commonsense reasoning, and general knowledge. Nevertheless, the research presented has a few limitations. Due to computational constraints, our experiments only compare HELM to Euclidean LLMs trained on the same 5B tokens, which have less representational capacity when compared to the commercially available LLMs trained on much more extensive data [ 4,1,18,42,10]. Additionally, we chose the Wikipedia dataset for its widely accepted reliability. However, the trained models might be under-exposed to areas such as mathematical reasoning as a result. Future work could explore incorporating scaling laws [ 28,25] for hyperbolic LLMs across larger compute and data frontiers to investigate their potential. 9 Acknowledgments This work was supported in part by the National Science Foundation (NSF) IIS Div Of Information & Intelligent Systems 2403317. We also gratefully acknowledge support in part from the Silicon Valley Community Foundation, an Amazon research award, the Yale AI Engineering Research Grant from Yale Office of the Provost, and an LEAP-U Sponsored Research from Samsung Research America. Additionally, this research has greatly benefited from the discussions and research talks held at the IMS-NTU Joint Workshop on Applied Geometry for Data Sciences. We also thank Ngoc Bui (Yale Univeristy) for useful feedback and discussion. References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovi ´c. Round and round we go! what makes rotary positional encodings useful?, 2025. URL https://arxiv.org/abs/2410.06205. Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. Fully hyperbolic convolutional neural networks for computer vision. In ICLR, 2024. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT- NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Work- shop on Challenges & Perspectives in Creating Large Language Models, 2022. Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. arXiv preprint arXiv:2105.14686, 2021. Weize Chen, Xu Han, Yankai Lin, Kaichen He, Ruobing Xie, Jie Zhou, and Zhiyuan Liu. Hyperbolic pre-trained language model. IEEE TASLP, 32, 2024. Sungjun Cho, Seunghyuk Cho, Sungwoo Park, Hankook Lee, Honglak Lee, and Moontae Lee. Curve your attention: Mixed-curvature transformers for graph representation learning. arXiv preprint arXiv:2309.04082, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL https://arxiv.org/abs/2401.06066. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412. 19437. DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024. Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakr- ishna Vedantam. Hyperbolic image-text representations. In ICML, pages 7694–7731. PMLR, 2023. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23:1–40, 2022. 10  Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic neural networks. In NeurIPS, pages 5345–5355, 2018. Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In ICML, pages 1646–1655. PMLR, 2018. Songwei Ge, Shlok Kumar Mishra, Simon Kornblith, Chun-Liang Li, and David Jacobs. Hyper- bolic contrastive learning for visual representations beyond objects. ArXiv, abs/2212.00653, 2022. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv:2407.21783, 2024. Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. In ICLR, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. Neil He, Jiahong Liu, Buze Zhang, Ngoc Bui, Ali Maatouk, Menglin Yang, Irwin King, Melanie Weber, and Rex Ying. Position: Beyond euclidean–foundation models should embrace non-euclidean geometries. arXiv preprint arXiv:2504.08896, 2025. Neil He, Menglin Yang, and Rex Ying. Hypercore: The core framework for building hyperbolic foundation models with comprehensive modules. arXiv preprint arXiv:2504.08912, 2025. Neil He, Menglin Yang, and Rex Ying. Lorentzian residual neural networks. In KDD, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):79–87, 1991. doi: 10.1162/neco.1991.3.1. 79. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Marc Law, Renjie Liao, Jake Snell, and Richard Zemel. Lorentzian distance learning for hyperbolic representations. In ICML, pages 3672–3681. PMLR, 2019. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi- tional computation and automatic sharding. In International Conference on Learning Represen- tations (ICLR), 2021. Paolo Mandica, Luca Franco, Konstantinos Kallidromitis, Suzanne Petryk, and Fabio Galasso. Hyperbolic learning with multimodal large language models. arXiv preprint arXiv:2408.05097, 2024. 11  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, pages 2381–2391. Association for Computational Linguistics, 2018. Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. The numerical stability of hyperbolic representation learning, 2024. URL https://arxiv.org/abs/2211.00181. Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical represen- tations. In NeurIPS, pages 6338–6347, 2017. Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In ICML, pages 3779–3788, 2018. Avik Pal, Max van Spengler, Guido Maria D’Amely di Melendugno, Alessandro Flaborea, Fabio Galasso, and Pascal Mettes. Compositional entailment learning for hyperbolic vision-language models. ICLR, 2025. Eric Qu and Dongmian Zou. Autoencoding hyperbolic representation for adversarial generation. arXiv preprint arXiv:2201.12825, 2022. Michael Robinson, Sourya Dey, and Shauna Sweet. The structure of the token space for large language models, 2024. URL https://arxiv.org/abs/2410.08993. Ryohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. In ICLR, 2020. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In NAACL, pages 4149–4158, 2019. Gemma Team. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. URL https://arxiv.org/abs/2403.08295. Alexandru Tifrea, Gary Bécigneul, and Octavian-Eugen Ganea. Poincaré glove: Hyperbolic word embeddings. arXiv preprint arXiv:1810.06546, 2018. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Max van Spengler, Erwin Berkhout, and Pascal Mettes. Poincaré resnet. CVPR, 2023. Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexan- drov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URL https://arxiv.org/abs/2411.12372. Menglin Yang, Aosong Feng, Bo Xiong, Jihong Liu, Irwin King, and Rex Ying. Hyperbolic fine-tuning for large language models. ICML LLM Cognition Workshop, 2024. Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King, and Rex Ying. Hypformer: Exploring efficient transformer fully in hyperbolic space. In KDD, pages 3770– 3781, 2024. Yun Yue, Fangzhou Lin, Kazunori D. Yamada, and Ziming Zhang. Hyperbolic contrastive learning. arXiv preprint arXiv:2302.01409, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In ACL, pages 4791–4800, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. URL https: //arxiv.org/abs/1910.07467. 12 Appendix A Proofs and Details of Theoretical Results 13 A.1 Ollivier-Ricci Curvature............................... 13 A.2 H OPE is a Lorentz Rotation............................. 14 A.3 Proposition 4.1: H OPE is a function of embeddings and relative position only... 14 A.4 Proposition 4.2: H OPE decays with increasing relative position........... 15 A.5 Proposition 4.3: H OPE enables tokens to attend across distances.......... 16 A.6 Proposition 4.4: Attention heads with H OPE learn special positional patterns... 16 A.7 Proposition 4.5: Invariance guarantees of Hyperbolic RMSNorm.......... 18 B Additional Details 19 B.1 M ICE as a Lorentzian Module............................ 19 B.2 Hyperbolic Multi-Head Latent Attention....................... 19 B.3 Hyperbolic SwiGLU Feedforward Network..................... 20 C Training and Evaluation Details 20 C.1 Models Setup..................................... 21 C.2 Training Details.................................... 21 C.3 Evaluation Details................................... 22 D Ablation Studies 22 D.1 Ablation for HMLA................................. 23 D.2 Ablation for H OPE.................................. 23 A Proofs and Details of Theoretical Results A.1 Ollivier-Ricci Curvature Ricci curvature is a geometric object that measures average geodesic dispersion on a Riemannian manifold, i.e., whether straight paths on a surface remain parallel (zero curvature), converge (positive curvature), or diverge (negative curvature). Ollivier-Ricci curvature is the discrete analog for graphs that do not have a notion of tangent structure. Suppose we have a graph G(V, E). For a node i∈V, we define probability measures µiby: µi(j) =( 1 deg(i)(i, j)∈E 0 otherwise. Fori, j∈V, the Ollivier-Ricci curvature is given by, κG(i, j) = 1−WG 1(µi, µj) d(i, j). Here, WG 1is the 1-Wasserstein distance between measures, µiandµj. Intuitively, the curvature is defined as random walks between iandj. If the random walks tend to stay at equal distances, κ(i, j) = 0; if they diverge, κ(i, j)<0; and if they converge, κ(i, j)>0. Since curvature is a local property of a Riemannian surface, for graphs, we examine local neighborhoods with a step size of 1. We thus choose µi(j) =1 deg(j)if(i, j)∈E, otherwise 0. 13 In our preliminary analysis of popular decoder-only LLMs (Figure 1), we draw k-nearest-neighbors graphs with the final-layer token embeddings for a collection of prompts from RedPajama [ 46] to ascertain their geometric structure. We observe a high variability of negative curvature values across tokens, hinting at their non-Euclidean nature. Robinson et al. further allude to the non-Euclidean token subspace, necessitating language models that can accommodate this unique geometry. A.2 H OPE is a Lorentz Rotation Here, we expand on HOPEbeing a Lorentz rotation. Since Ri,Θis a Euclidean rotation, we have ∥z∥=∥Ri,Θz∥. Then, since for any Lorentz vector z∈LK,nwe have zt=p −1/K+∥zs∥ ∈R. Computing (Ri,Θz)t=p −1/K+∥Ri,Θzs∥=p −1/K+∥zs∥=zt. Thus, HOPEdoes not affect the time-like dimension of z, so we have, HoPE( z) =\u0012 1 0 0Ri,Θ\u0013 z, making H OPE a valid Lorentz rotational operation. A.3 Proposition 4.1: H OPE is a function of embeddings and relative position only Proposition. LetXbeTtokens with xi∈LK,d. LetQ,Kbe queries and keys as in Equation (3). Then−d2 L(HoPE ( qa),HoPE ( kb))) = g(xa,xb;a−b)for some function g. Proof. We will denote HoPE( qa),HoPE( kb)asfq(xa), fk(xb)where fq, fkdenotes the function that projects the word embeddings to queries and keys, and then applying HOPE. In practice, the projection is done through a hyperbolic linear layer, which we take to be HLT from Section 2. It suffices to prove this proposition for the case of d= 2, since HOPEdoes not affect the time-like dimension of the inputs and Ri,Θacts independently on each 2D block. First, note that we have −d2 L(fq(xa), fk(xb)) =2 K−2⟨fq(xa), fk(xb)⟩L =2 K+ 2 (fq(xa)tfk(xb)t)−2⟨fq(xa)s, fk(xb)s⟩, where ⟨·,·⟩Ldenotes Lorentzian inner product and ⟨·,·⟩denotes the regular Euclidean inner prod- uct. Since HOPEis a Lorentz rotation, the term 2 (fq(xa)tfk(xb)t)is simply 2((qa)t,(kb)t) = 2\u0010p WQxa−1/Kp WKxb−1/K\u0011, so we focus on the inner product ⟨fq(xa)s, fk(xb)s⟩. Then, by assuming that d= 2, we have (fq(xa))s,(fk(xb))s∈R2; hence, we can parametrize these vec- tors by their radial and angular components. For simplicity, denote (fq(xa))s,(fk(xb))sasa,b respectively. Then, write ⟨a,b⟩as a function g′. Afterwards, we parametrize the vectors as a=φq(xa, a)eiϑq(xa,a) b=φk(xb, b)eiϑk(xb,b) g′=φgeϑg′,(8) where φ{q,k,g′}denote the radial component and ϑ{q,k,g′}denote the angular component. Note that it suffice to show that under HOPE, we can express φg′, ϑg′as a function of the word embeddings and relative position. To see this, note that by definition of g′we have φq(xa, a)φk(xb, b) =φg′ ϑq(xa, a)−ϑk(xb, b) =ϑg′.(9) Now, given the fact that HOPEacts via Euclidean rotation on the time-like dimension of any vector, we have φq(xa, a) =φq(xa, a′), φk(xb, b) =φk(xb, b′)for any a′, b′. In particular, when a′=b′= 0, HOPE acts via identity since all rotation angles become 0. Hence, we have φq(xa, a) =∥(qa)s∥ φk(xb, b) =∥(kb)s∥.(10) Furthermore, we have φg′=∥(qa)s∥∥(kb)s∥=∥WQxa∥∥WKxb∥by plugging back into Equa- tion (9), which is a function of just the word embeddings. Next, for the angular component, note 14 that given the definition of HOPE, the rotation on any 2D block of the space-like dimension of the input at position pis simply a scaling of a fixed rotation angle by p. Letting this fixed angle be σ, the rotation is precisely pσ. Therefore, we have ϑq(xa, a) =θq+aσ ϑk(xb, b) =θk+bσ,(11) where θq, θkdenote the angular components of qa,kb. Next, given Equation (9), we have ϑg′= (a−b)(σ) + (θq−θk). Note that we have eiθq=WQxa ∥WQxa∥andeiθk=WKxb ∥WKxb∥. Consequently, ϑg′ is a function of the word embeddings and the relative position a−b. All in all, −d2 L(fq(xa), fk(xb)) can be expressed with a function g(xa,xb;a−b)as desired. A.4 Proposition 4.2: H OPE decays with increasing relative position Proposition. LetQ,Kbe as defined in Equation (3), then the negative square Lorentz distance −dL(HoPE ( qa),HoPE( kb))can be upper bounded by f(qa,kb)g(a−b)<0, where fhas no dependencies on position, and gdepends entirely on relative position and scales inversely w.r.t. a−b. Proof. For simplicity, we denote qa=q,kb=k. Recall that −d2 L(HoPE ( q),HoPE( k)) =−2 K−2(qtkt) + 2q⊤ sks. Next, for simplicity, we denote qs,ksasa,brespectively. We group together entries of the queries and keys, where a[2k:2k+1],b[2k:2k+1]as the 2k-th and the (2k+ 1) -th entries of a,brespectively. With this in mind, note that since we take query and key projects to be with HLT as given in Section 2, we obtain a=WQxaandb=WKxb. To that end, we can assert that a⊤b=\u0000 Ra,ΘWQxa\u0001⊤\u0000 Rb,ΘWKxb\u0001 =x⊤ aWQRb−a,ΘWKxb = Re n/2X k=0a[2k,2k+1]b∗ [2k,2k+1]ei(b−a)θk , (*) where Re(x)denotes the real component of x∈C. Now, recall Abel’s Transformation, which allows one to rewrite the sum of the product of two sequences as the product of the partial sums. Denote one of the a[2k,2k+1]b∗[2k,2k+1]asAk, and denote the sequencekX l=0ei(b−a)θlasEl. With this in mind, (*) can be written as Re n/2X k=0Ak(Ek+1−Ek) . Consequently, we obtain (recall that boundary termAn/2= 0) a⊤b =  n/2X k=0Ak(Ek+1−Ek)  =  n/2X k=0(Ak+1−Ak)Ek  (Abel Transformation) ≤n/2X k=0|(Ak+1−Ak)||Ek| 15 Now, note that the term Ak+1−Akhas no dependency on position. The sumn/2X k=0|Ek|scales inversely with b−a, as shown by Su et al.. To that end, we have −d2 L(HoPE ( q),HoPE( k)) =−2 K−2(qtkt) + 2q⊤ sks ≤ −2 K−2(qtkt) + 2 max k|(Ak+1−Ak)|n/2X k=0|Ek|. Note that Ak, qt, ktdepends on only the word embeddings and Ekdepends only on the position. Thus, we have the desired result. A.5 Proposition 4.3: H OPE enables tokens to attend across distances Proposition. LetQ,Kbe as defined in Equation (3), then HOPEcan be maximal at an arbitrary distance, i.e., for any relative distance r∈Z, there exists a key kjsuch that the softmax value is maximum at distance r. Proof. We first restate Lemma A.1. from Barbero et al.. The remainder of our proof follows a similar layout to that of Proposition 3.1. in Barbero et al.. Lemma A.1. (Barbero et al. )Consider g∈Qwithg̸= 0andn∈Z. Then, ng≡0 (mod 2 π) only when n= 0. In particular, this also holds if gis algebraic. Consider a distance r, a non-trivial query Qp=ψ∈LK,n, as well as a key K= HoPE( Qp). We can represent ψas a combination of a time and space dimension on the Lorentzian manifold, [ψt, ψs]∈LK,n. Using our definition of H OPE in Section 4.1, we have Kq=\"r ∥Rp,Θψs∥2−1 K,Rp,Θψs# ∈LK,n. Recall that the operator Rp,Θis a valid Euclidean rotation in Rnwhile HOPEremains a valid Lorentzian operation. Therefore, Rp,Θdoes not affect the Euclidean norm in the time dimension as it is an isometry. Instead, we can focus on the space dimension ψs∈Rn, on which we can use the Euclidean dot product. Assume the query is at position iand the key is at some j≤i. We then compute the following dot product: ψ⊤ s,pHoPE( ψs,p) =ψ⊤ sR(j−i)+r p,Θ ψs =X l=1,···,n/2\u0010 ψ(l) s\u0011⊤ R(j−i)+r p,θl\u0010 ψ(l) s\u0011 =X l=1,···,n/2 ψ(l) s 2 cos (( j−i+r)θl). (12) Using Lemma A.1. from Barbero et al., we observe the maximum can be achieved when j−i=−r forj−i≤0since we are using causal masking, j≤i. This ensures cos (j−i+r)θl= cos (0) = 1, concluding the proof. A.6 Proposition 4.4: Attention heads with H OPE learn special positional patterns Proposition. Attention heads with HOPEcan learn diagonal or off-diagonal attention patterns. Proof. The proof follows a similar layout as that of Proposition 5.3. from Barbero et al.. We start with the diagonal case. Suppose Qi=Kj=ψ∈LK,d, for non-trivial ψ= [ψt, ψs]. We assume embedding dimension d= 2, i.e., only a single rotation block Ri,θacts on the embeddings. 16 Recall the squared Lorentzian distance for any a, b∈LK,d, d2 L(a, b) =∥a−b∥2 L =2 K−2⟨a, b⟩L =2 K−2(−atbt+a⊤ sbs). Without H OPE, −d2 L(Qi,Kj) =−\" 2 K−2(−ψtψt+ψ⊤ sψs)# =−\" 2 K+ 2ψtψt−2ψ⊤ sψs#. Using H OPE, −d2 L(Ri,θQi,Rj,θKj) =−\" 2 K+ 2ψ2 t−2\u0010 (Ri,θψs)⊤(Rj,θψs)\u0011# =−\" 2 K+ 2ψ2 t−2\u0010 ψ⊤ sRj−i,θψs\u0011# =−\" 2 K+ 2ψ2 t |{z} C−2∥ψs∥2cos (( j−i)θ)# =−h C−2∥ψs∥2cos (( j−i)θ)i. (13) Using Lemma A.1. from [ 2], when j=i, we have (j−i)θ≡0 (mod2π). Next, let us define ai,i as−[C−2∥ψs∥2]. This means cos (( j−i)θ) = cos (0) = 1, andai,j<ai,i. This gives us the following self-attention score: νi,i=exp (ai,i)P k<iexp (ai,k) + exp ( ai,i) =exp (−[C−2∥ψs∥2])P k<iexp (−[C−2∥ψs∥2cos (( k−i)θ)]) + exp ( −[C−2∥ψs∥2]) =1 1 +P k<iexp (−[C−2∥ψs∥2cos (( k−i)θ)]−(−[C−r∥ψ2s∥2])) =1 1 +P k<iexp (−C+ 2∥ψs∥2cos (( k−i)θ) +C−2∥ψs∥2) =1 1 +P k<iexp (2∥ψs∥2(cos (( k−i)θ)−1)), for all k̸=i,cos (( k−i)θ)<1. This means 2∥ψs∥2(cos (( k−i)θ)−1)<0. To that end, we obtain sup ∥ψs∥2→∞1 1 +P k<iexp (2∥ψs∥2(cos (( k−i)θ)−1))= 1. This guarantees νi,i>1−ϵforϵ >0, where ϵis a function of 2∥ψs∥2. We now consider the off-diagonal pattern. Set Qi=ψfor non-trivial ψ= [ψt, ψs]∈LK,d. Set keys Ki=R1,θψand define ai,i−1as the off-diagonal input to the softmax when computing νi,i−1. To 17 that end, we have ai,i−1=−d2 L(Ri,θQi,Ri−1,θKi) =−h2 K+ 2ψ2 t−2\u0010 (Ri,θQi)⊤(Ri−1,θKi)\u0011# =−\" 2 K+ 2ψ2 t−2\u0010 (R1,θψs)⊤(Ri−1,θR1,θψs)\u0011# =−\" 2 K+ 2ψ2 t−2\u0010 (Ri,θψs)⊤(Ri,θψs)\u0011# =−\" 2 K+ 2ψ2 t−2\u0010 ∥ψs∥2cos (( i−i)θ)\u0011# =−h2 K+ 2ψ2 t−2∥ψs∥2i. Use the same reasoning from the diagonal case to show that attention head with HOPEcan learn off-diagonal patterns. This concludes the proof. A.7 Proposition 4.5: Invariance guarantees of Hyperbolic RMSNorm Proposition. RMSNorm Lis invariant to scaling of inputs xduring both the forward and backward passes. Proof. Euclidean RMSNorm is invariant to input-scaling, both during the forward and backward pass. We observe similar guarantees from our formulation of hyperbolic RMSNorm. To that end, we first prove the input-scaling invariance of Euclidean RMSNorm. Given an input x∈Rnand and a feed-forward network with parameters W∈Rn×m, y=σ W⊤x RMS( W⊤x)⊙g+b!, RMS( a) =vuut1 mmX k=ia2 i. Here, gis a learnable gain parameter, initially set to 1, that re-scales the standardized inputs and bis a bias term. Suppose the weights are scaled by a small factor, W′=δW. First, observe that the root mean squared operation, RMS, is input-scaling invariant: RMS( αa) =αRMS( a). It is then evident that the final output of RMSNorm is also scale-invariant: y′=σ (W′)⊤x RMS(( W′)⊤x)⊙g+b! =σ δW⊤x RMS( δW⊤x)⊙g+b! =σ \u0001δW⊤x \u0001δRMS( W⊤x)⊙g+b! =y (14) A similar argument can be made for a scaling of the inputs x. Since hyperbolic RMSNorm uses Euclidean RMSNorm internally, it offers the same invariance guarantees as it operates solely on the space dimension of the Lorentzian input. 18 Given an input x= [xt,xs]∈LK,n, we know RMSNorm( δx) = RMSNorm( x)for some scaling factor δ. As such, y′= RMSNorm L(δx) =hp ∥RMSNorm( δxs)∥ −1/K,RMSNorm( δxs)i⊤ =hp ∥RMSNorm( xs)∥ −1/K,RMSNorm( xs)i⊤ =y. Next, we analyze the gradient stability of hyperbolic RMSNorm. In Euclidean RMSNorm, for a given loss L, we are interested in computing three gradients:∂L ∂gfor the gain parameter,∂L ∂bfor the bias, and∂L ∂Wfor the weights. We compute∂L ∂gand∂L ∂bas follows: ∂L ∂b=∂L ∂v·∂v ∂b∂L ∂g=∂L ∂v⊙W⊤x RMS( W⊤x), where vdenotes the inputs to the activation σ. These gradients are invariant to the scaling of Euclidean inputs xand weights W, trivially for∂L ∂b, and due to the linearity established in Equation (14) for∂L ∂g. Computing∂L ∂Wis more involved due to the quadratic computation in RMS, but also provides invariance to input scaling as shown by Zhang and Sennrich. Given an input x= [xt,xs]∈LK,n, we know∂L ∂g,∂L ∂g, and∂L ∂Ware scaling-invariant in the backward pass since hyperbolic RMSNorm uses Euclidean RMSNorm. Thus, for any scaled hyperbolic input δx∈LK,n, we get scaling invariance both in the time and space dimension during the backward pass. B Additional Details B.1 M ICE as a Lorentzian Module In this section, we expand on the fact that MICEis indeed a valid hyperbolic module throughout. Note that since Equation (6) consists of the combination of a Lorentzian residual connection [ 23] and Lorentzian centroid [ 29], it suffices to show that the projection from input manifold to expert manifold, and the reverse projection, are valid projections between Lorentz hyperbolic spaces. In fact, it suffices to show that given x∈LK1,n, we havep K1/K2x∈LK2,n. To see this, note that Dp K1/K2x,p K1/K2xE L=K1 K2⟨x,x⟩L =K1 K2·1 K1(x∈LK1,n) =1 K2 Thus,p K1/K2x∈LK2,nas desired. Then, each projection via scaling byp K/K s,iandp K/K r,i indeed map the input vector xto the expert manifold, and the projection viap Ks,i/Kandp Kr,i/K maps the output of the experts back to the input manifold. As a result, every vector in Equation (6) lives on the input manifold, hence the output lives in LK,nas desired. Additionally, note that since the squared Lorentzian distance is given by d2 L(x,y) = 2 /K−2⟨x,y⟩= 2/K+ 2xtyt−2x⊤ sys, it scales inversely w.r.t. x⊤ sys. As a result, the gating score obtained through Equation (5) is minimizing the the squared hyperbolic distance between the input token vector xtand the vector yj(by viewing centroid vector yjas the space-like dimension of a Lorentz hyperbolic vector). Therefore, the gating module is in fact a hyperbolic module as well. B.2 Hyperbolic Multi-Head Latent Attention In this section, we provide the details for hyperbolic Multi-Head Latent Attention ( HMLA ). Let xt∈LK,nh nbe the t-token, where nis the embedding dimension and hnis the number of heads. LetWDKV∈R(nhn+1)×nkvbe the downward projection matrix of the keys and values, and 19 WDQ∈R(nnh+1)×nqbe the downward projection matrix of the query ( nkv, nq≪nhn). We first compress the token into the latent spaces via cKV t= HLT( xt;WKV,bKV)∈LK,nkv,cQ t= HLT(xt;WQ,bQ)∈LK,nq. We then project the latent query, key, and value vectors back to the higher dimensional spaces. Specifically, let WUV,WUK∈R(nkv+1)×nhnbe the upward projection matrix of the keys and values, and let WUQ∈R(nq+1)×nhnbe the upward projection matrix of the query. Then, the final projected keys, values, and queries are [kC t,1;...;kC t,hn] = HLT\u0000 cKV t;WUK,bUK\u0001; [vC t,1;...;vC t,hn] = HLT\u0000 cKV t;WUV,bUV\u0001 [qC t,1;...;qC t,hn] = HLT\u0010 cQ t;WUQ,bUQ\u0011. (15) Following previous works [ 11,10], as RoPE is incompatible with MLA due to position coupling, we employ a decoupled HOPEscheme with HMLA, where we use additional query vectors with a shared key. Let WQR∈R(nq+1)×(hnnr)andWKR∈R(nkv+1)×nrbe the upward projection matrix of the decoupled queries and the shared key respectively, where nris the dimension per head. We apply H OPE to these vectors to obtain the position-encoded vectors [qR t,1;...;qR t,hn] = HoPE\u0010 HLT\u0010 cQ t;WQR,bQR\u0011\u0011;kR t= HoPE\u0000 HLT\u0000 cK t;WKR,bKR\u0001\u0001. (16) Then, we obtain the final query and key vectors as qt,i= HCat( qC t,i;qR t,i);kt,i= HCat( kC t,i;kR t), (17) where HCat denotes hyperbolic concatenation [ 48,37]. The attention score is computed based on negative squared Lorentz distance similar to Equation (3) as ot,i=PN j=1αt,i,jvC t,j√ −K|∥PN k=1αt,i,kvC t,j∥|L;αt,i,j=exp\u0000 −d2 L(qt,i,kt,j)/√hn+nr\u0001 PN k=1exp\u0000 −d2 L(qt,i,kt,k)/√hn+nr\u0001.(18) The final output of HMLA can be expressed as the concatenation of the hyperbolic vector HMLA( Xt;hn, n, n r, nq, nkv) = HLT\u0012hp ∥ot∥ −1/K,oti⊤;WO,bO\u0013, (19) where ot= [ot,1,...,ot,hn]andWO∈Rhn(n+1)×hnnis the out-project matrix. HMLA enables HELM models to improve computational efficiency during training and inference compared to the regular hyperbolic self-attention in Equation (3). B.3 Hyperbolic SwiGLU Feedforward Network In this section, we introduce hyperbolic SwiGLU feedforward networks (FFNs), whose Euclidean formulation is widely used in LLMs [ 18,10]. This differs from previous FNNs used in hyperbolic Transformers in the need for feature multiplication and activation function [ 5,6,48]. Let x∈LK,n be the input tokens, W1,W3∈R(n+1)×mbe the weights of internal projection layers and W2∈ R(m+1)×nbe the weight matrix of the outward projection layer. Then, the hyperbolic SwiGLU FNN HFFN SG:LK,n→LK,nis given by HFNN SG(x) = HLT\u0012hp ∥y∥ −1/K,yi⊤;W2,b2\u0013 y= SiLU L(HLT( x;W1,b1))⊗sHLT(x;W3,b3),(20) where SiLU Ldenotes SiLU activation using the HRC activation operations from Hypformer [ 48] and⊗sdenotes multiplication on the space dimention of a Lorentz vector, i.e. x⊗sy=xsys. C Training and Evaluation Details In this section we detail the training and evaluation setup for the experiments. 20 C.1 Models Setup Here we detail the model setup for all the models we used in the experiments. HELM-M ICEmodel setup. We follow the notation in Appendix B.2 and Section 4.2. For HELM- MICE, we used HMLA as the attention mechanism in the attention block, MICEas the sparse feedforward network, and HFNN SGas the dense feedforward network. For both sizes, only the first decoder block uses the dense layer and the rest of the blocks using the MICElayer. The MICElayers useHFNN SGas well for its feedforward component. For the dense layer, we set the intermediate dimension to be 4hnn. For the MICElayers, we set the intermediate dimension of HFNN SGas 2hnn. For the ∼100Msized model, we used 6 total layers, 6 heads ( nh= 6) each with n= 64, and we setnkv= 64,nr= 16. For MICElayers, we employ 4 experts with 2 active experts per token (Nr= 4, Kr= 2). We use one shared expert ( Ns= 1). For the curvatures of the routed experts, we set them to be uniform from −0.1to2.0. The curvature of the shared expert is set to be −1. The curvature of the entire model is set to −1as well. For HMLA layers, in practice, the upward projection matrices do not need to project back to the full dimension of hnn. Due to compute constraints, we instead employ a reduction in dimensionality during the upward projection, where WUK,WUV∈R(nkv+1)×hnn/2, and the outward projection matrix WOprojects back to the full dimensionality of the input with WO∈Rhn(n/2+1)×hnn. For the ∼1Bsized model, we use 16 total layers, 14 heads ( nh= 14 ) each with n= 64, and we setnkv= 256,nr= 64. For MICElayers, we employ 4 experts with 2 active experts per token (Nr= 8, Kr= 2). We use one shared expert ( Ns= 1). For the curvatures of the routed experts, we set them to be uniform from −0.1to2.0. The curvature of the shared expert is set to be −1. The curvature of the entire model is set to −1as well. We do not use the same reduction in dimensionality during upward projection as we did in the ∼100Mcase to enable for more expressive attention modules. HELM-D model setup. For the HELM-D model, we only train the 100Msized model. Here, we use 6 layers, 6 heads each with dimension 64, and we set the intermediate dimension of the HFNN SG feedforward networks to be 4 times the total model dimension. We set the overall curvature of the model to −1. All hyperbolic models are built on top of HyperCore He et al.. Baseline models setup. For the baseline models, we set them up to have identical dimensionality as the HELM models. In particular, for the LLaMA model we train, we use the same number of layers, heads, and dimensionality per head as the feedforward network. For the DeepSeek models we train, we use the same number of layers, heads, dimensionality per head, dimensionality for the feedforward network, dimensionality for the MoE modules, number of routed and shared experts, and the same dimensionality in the MLA layers. Hyperbolic work embeddings. For the smaller HELM models, we map the input tokens directly to Lorentz hyperbolic vectors, which are then trained as hyperbolic parameters via Riemannian optimizers. The parameters are initialized via wrapped Gaussian normal distribution on the manifold. However, when training the ∼1BHELM-M ICEmodel, we found this to cause training instability. As a result, for the larger model, we first map the tokens to the space-like dimension of Lorentz hyperbolic vectors, and then compute the time-like dimension of the vectors afterwards. We found this to stabilize model training. C.2 Training Details Dataset. For the training dataset, we use the English portion of the Wikipedia dataset [ 14]. This dataset consists of ∼6.4Mrows of data. We download the dataset directly from Huggingface. The raw text data is then passed through the LLaMA3.1-8B tokenizer [ 18], which has a vocabulary size of∼128K. We use a sequence length of 2048 for all models. Samples longer than 2048 tokens were broken up into multiple samples, with the trailing tailed dropped. The tokenized dataset consist of roughly 4.5B∼5Btokens. For training efficiency, as we measured the average number of tokens per sample is ∼700across the dataset, we used sample packing with a packing ratio of 3.0. Then packed samples shorted than 2048 tokens are then padded on the right. 21 Table 3: Ablation accuracy, where we compare HELM-M ICEwith a variant using hyperbolic Multi- Head self-Attention instead of HMLA, denoted as MICE-HMHA. Bolding denotes the highest accuracy and underline denotes the second-highest. Euclidean DEEPSEEKV3results are shown for reference. Overall, HELM-M ICEconsistently achieves the highest accuracy, while both hyperbolic models still outperform the Euclidean counterpart. Model # Params CommonsenseQA HellaSwag OpenbookQA MMLU ARC-Challenging Avg 0-Shot 0-Shot 0-Shot 5-Shot 5-Shot - DEEPSEEKV3 120M 19.2 25.2 23.4 24.2 21.8 22.8 MICE-HMHA 120M 19.3 25.7 26.0 23.8 25.3 23.7 HELM-M ICE 120M 19.3 26.2 27.4 24.7 24.1 24.2 Table 4: Ablation accuracy, where we compare HELM with a variants using learned relative positional encoding instead of HOPE, denoted as HELM-D-L andHELM-M ICE-L. Bolding denotes the highest accuracy and underline denotes the second-highest. Euclidean results are shown for reference. Overall, HELM-M ICEandHELM-D consistently achieves the higher accuracy, while both hyperbolic models still outperform the Euclidean counterpart. Model # Params CommonsenseQA HellaSwag OpenbookQA MMLU ARC-Challenging Avg 0-Shot 0-Shot 0-Shot 5-Shot 5-Shot - LLAMA 115M 21.1 25.3 25.3 23.8 21.0 23.3 HELM-D-L 115M 19.7 25.5 28.6 23.0 21.8 23.7 HELM-D 115M 20.1 25.9 27.0 25.8 21.2 24.0 DEEPSEEKV3 120M 19.2 25.2 23.4 24.2 21.8 22.8 HELM-M ICE-L 120M 19.0 25.5 27.0 23.0 25.7 24.0 HELM-M ICE 120M 19.3 26.0 27.4 24.7 23.5 24.2 Pipeline setup. For training, we set up data-parallelism with Hugginface Accelerate. We use an effective batch size of ∼2Mtokens (including padding). To ensure a fair comparison between the hyperbolic and Euclidean models, we use a learning rate of 2e-4 for all dense models and a learning rate of 4e-4 for the MoE and MICEmodels. A weight decay rate of 0.01 was used for all models. For the HELM-M ICEmodels and the DeepSeek models, in order to balance the load between each expert, we utilize the auxiliary-loss-free load balancing strategy and the complementary sequence-wise auxiliary loss during training. The former punishes extreme load imbalance among the experts by dynamically updating a bias term during the gating module, while not needing an explicit auxiliary loss computation for better training efficiency. The latter punishes extreme load imbalance for any particular sequence. All training used a cosine annealing learning rate scheduler with a final target learning rate of 0.1×the initial learning rate, with 3%of the gradient update steps used as warmup steps. Runtime. We empirically observe that HELM models take roughly 1.5 to 1.8 times the training of their Euclidean counterparts. For example, the larger ∼1BHELM-M ICEmodel takes roughly 72 hours to train on 4 NVIDIA A800s while the similarly sized DeepSeekV3 model takes roughly 40 hours on the same machine. C.3 Evaluation Details We use the Language Model Evaluation Harness library (github.com/EleutherAI/lm-evaluation- harness) for all evaluations, where the framework prompts the models with the answers choices to each question and picks the one with the highest likelihood value. For OpenbookQA, we convert the answer choices from full sentences to letter choices for all models, to make up for the relatively smaller model and training dataset sizes. D Ablation Studies In this section, we perform additional ablation studies to access the effectiveness of HOPEand HMLA. 22 D.1 Ablation for HMLA Past works have found that in the Euclidean case, Multi-Head Latent Attention can achieve comparable and in some cases even superior performance compared to regular Multi-Head Attention [ 10]. Here we assess the effectiveness of HMLA against hyperbolic Multi-Head self-Attention. We train HELM-M ICEwith the same setup, where we replace the HMLA layers with a hyperbolic Multi- Head self-Attention layer as given in Equation (3). We denote the this model as MICE-HMHA. The results are shown in Table 3. HELM-M ICEoutperforms MICE-HMHA in 3 out of the 5 tasks, achieving the same accuracy for 1 task, with the MICE-HMHA achieving better accuracy in the last task. The results demonstrate the effectiveness of HELM-M ICEwhile significantly reducing the memory footprint of the KV-cache. Both hyperbolic models still outperform the Euclidean model, demonstrating the effectiveness of HELM in general. D.2 Ablation for H OPE In this section we assess the effectiveness of HOPEagainst other hyperbolic positional encoding methods, namely the learned relative positional encoding from Hypformer [ 48]. We devise a variant ofHELM-D, denoted as HELM-D-L and a variant of HELM-M ICE, denoted as HELM-M ICE-L, where each model uses the learned positional encoding instead of HOPE. The results are shown in Table 4. Overall both HELM-M ICEandHELM-D outperform their counterparts that use learned positional encoding instead of HOPE. Interestly, however, HELM-M ICE-L and HELM-D - L outperformed HELM-M ICEandHELM-D respectively on the ARC-Challenging benchmark, possibly due to better alignment with reasoning prompts with non-uniform encodings. Nevertheless, the results demonstrate the effectiveness of HOPEover learned positional encodings in 4 out of the 5 tasks. 23",
  "text_length": 74505
}