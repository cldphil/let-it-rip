{
  "id": "http://arxiv.org/abs/2505.24769v1",
  "title": "Generalization Dynamics of Linear Diffusion Models",
  "summary": "Diffusion models trained on finite datasets with $N$ samples from a target\ndistribution exhibit a transition from memorisation, where the model reproduces\ntraining examples, to generalisation, where it produces novel samples that\nreflect the underlying data distribution. Understanding this transition is key\nto characterising the sample efficiency and reliability of generative models,\nbut our theoretical understanding of this transition is incomplete. Here, we\nanalytically study the memorisation-to-generalisation transition in a simple\nmodel using linear denoisers, which allow explicit computation of test errors,\nsampling distributions, and Kullback-Leibler divergences between samples and\ntarget distribution. Using these measures, we predict that this transition\noccurs roughly when $N \\asymp d$, the dimension of the inputs. When $N$ is\nsmaller than the dimension of the inputs $d$, so that only a fraction of\nrelevant directions of variation are present in the training data, we\ndemonstrate how both regularization and early stopping help to prevent\noverfitting. For $N > d$, we find that the sampling distributions of linear\ndiffusion models approach their optimum (measured by the Kullback-Leibler\ndivergence) linearly with $d/N$, independent of the specifics of the data\ndistribution. Our work clarifies how sample complexity governs generalisation\nin a simple model of diffusion-based generative models and provides insight\ninto the training dynamics of linear denoisers.",
  "authors": [
    "Claudia Merger",
    "Sebastian Goldt"
  ],
  "published": "2025-05-30T16:31:58Z",
  "updated": "2025-05-30T16:31:58Z",
  "categories": [
    "stat.ML",
    "cond-mat.dis-nn",
    "cs.LG",
    "math.ST",
    "stat.TH"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24769v1",
  "full_text": "arXiv:2505.24769v1 [stat.ML] 30 May 2025Generalization Dynamics of Linear Diffusion Models Claudia Merger and Sebastian Goldt∗ International School of Advanced Studies (SISSA), Trieste, Italy 2nd June 2025 Abstract Diffusion models trained on finite datasets with Nsamples from a target distribution exhibit a transition from memorisation, where the model reproduces training examples, to generalisation, where it produces novel samples that reflect the underlying data distribution. Understanding this transition is key to characterising the sample efficiency and reliability of generative models, but our theoretical understanding of this transition is incomplete. Here, we analytically study the memorisation-to-generalisation transition in a simple model using linear denoisers, which allow explicit computation of test errors, sampling distributions, and Kullback-Leibler divergences between samples and target distribution. Using these measures, we predict that this transition occurs roughly when N≍d, the dimension of the inputs. When Nis smaller than the dimension of the inputs d, so that only a fraction of relevant directions of variation are present in the training data, we demonstrate how both regularization and early stopping help to prevent overfitting. For N > d, we find that the sampling distributions of linear diffusion models approach their optimum (measured by the Kullback-Leibler divergence) linearly with d/N, independent of the specifics of the data distribution. Our work clarifies how sample complexity governs generalisation in a simple model of diffusion-based generative models and provides insight into the training dynamics of linear denoisers. 1 Introduction Diffusion models [ 1,2,3] have become the state-of-the-art paradigm in generative AI, where they are trained to sample from an unknown distribution ρbased on a finite set of training data, sampled from ρ. Diffusion models sample by implementing a function that maps a random seed into a new sample. While the behavior of diffusion models that have learned this mapping accurately has been studied recently [4, 5, 6, 7, 8, 9], much less is known about their behavior when trained on finite datasets. In this context, Kadkhodaie et al. [ 10] recently made a remarkable observation: diffusion models trained on finite datasets with Nsamples from a target distribution exhibit a transition from mem- orisation, where the model reproduces training examples, to generalisation, where it produces novel samples that reflect the underlying data distribution. We illustrate this phenomenon in fig. 1a), where we show samples generated by diffusion models optimized on training sets of increasing size N. For each sampled image, we show the most similar image from the training set in fig. 1b). We find that for the CelebA dataset [ 11], for training set sizes up to N= 3200, new samples can be identical to the training examples, meaning that the latter are essentially memorized, before genuinely new samples emerge atN≥6400. Beyond comparing the samples manually, we quantify memorization by measuring the similarity between elements of the training set and the generated samples, which decreases with N, see fig. 1 c). But does the absence of memorization mean the model has found a general solution? A crucial observation by Kadkhodaie et al. [ 10] was that two models trained on large, but independent sets of ∗{cmerger, sgoldt}@sissa.it 1 101102103104 # training examples102 101 100difference from best modeld) Generalization Linear U-neta) Generated 80 320 640 1280 3200 6400 12800 b) Closest training ex. 1520408032064012803200640012800 # training examples0.51.0similarityc) MemorizationFigure 1: a) Samples generated from diffusion models with a U-net architecture at different training set sizes N. b) Most similar elements of the corresponding training set. d) Average similarity (averaged over102samples, error bars report one standard deviation) between samples generated from diffusion models with U-net architectures and training set, measured with emphasis on details in the images (for details, see appendix A). All training data sets of all models are disjoint. a) Squared differences between the mappings of diffusion models trained from a reference model trained on Nmax= 6·104training examples, averaged over 10test examples in the linear and 102test examples in the non-linear case. Error bars reporting standard deviations are smaller than the symbol sizes. data can produce the same sample given the same random seed, meaning that they must essentially implement the same mapping. We measure the similarity between the mappings by comparing diffusion models of different architectures (Linear and U-net) trained on subsets of increasing size of CelebA to a reference model trained on N= 6·104samples. In fig. 1d), we show that the difference from the reference model on test examples from a the reference model trained on decays with N. This indicates that the models implement a solution that is increasingly independent of the details of the training set, but rather general, as it depends on characteristic statistics of ρ. These observations raise two intriguing questions: First, how many samples are required such that the model no longer memorizes the data? Second, how many samples are necessary for the mappings of different diffusion models to align? In this work, we present a theory of the memorization-generalization transition for linear diffusion models. Linear neural networks have helped elucidate overfitting in supervised learning in the past [ 12, 13] by providing a fully tractable case in which key mechanisms can be understood. Here, assuming that the denoiser is a linear neural network allows to connect data structure, learning dynamics, regularization, the training outcomes at finite Nin an analytical theory. Specifically, our main contributions are the following: •We derive the test and training loss as well as the Kullback-Leibler divergence between the sampling and target distribution for a diffusion model with linear denoiser. •We show that overfitting is most severe when N < d, where relevant directions of variation in ρ are not present in the training set, and we show how both early stopping and regularization are effective measures to prevent overfitting •We show that in the N > d regime, the Kullback-Leibler divergence between the sampling distribution and ρdecays linearly ind N, independent of the details of the target distribution. •We finally observe that the mappings implemented by non-linear denoisers are similar to the mappings learnt by linear denoisers for a wide range of diffusion times and eigenmodes of the input covariance, and that the differences grow with the number of samples N. 2 Further related work A growing body of work studies memorization and generalization in generative models. A common hypothesis is that diffusion models learn a distribution where each training sample is the center of a mode [ 14,15,16,17,18]. Generalization then occurs when the capacity of the model is insufficient to memorize all training samples, or when the sampling procedure is stopped early, before each generated sample has collapsed onto one of the modes. However, it is unclear how such a multimodal solution is learned dynamically, especially in the presence of architectural constraints and regularization. Here, we consider a unimodal learned distribution instead, and predict how it is learned from samples. The dynamics of learning have been studied in [ 19], who predict precise learning curves for random feature models, finding that generalization occurs when Nis on the order of the number of features. Li et al. have highlighted the importance of early stopping in training one-layer diffusion models, asserting that when optimally stopped, the Kullback-Leibler divergence scales at most with N−2/5and the model capacity for unimodal distributions. However, it is not yet clear how learning outcomes are related to the structure in the data distributions. Favero et al. [ 21] assume that ρis a random hierarchy model [ 22], and assert that diffusion models generalize when Nis polynomial in d. Other works have assessed that if the data lie on a lower-dimensional manifold, the required number of samples scales exponentially [ 23,15] or linearly [ 24] with the dimension of the manifold, rather than the embedding dimension. The authors of  establish the first results on linear diffusion models, asserting that generalization measures on Σ,Σ0 converge at Nlinear or quadratic in d, depending on whether the measure concerns the eigenvalues or also the eigenvectors of the matrices. Our work can include the case of a lower-dimensional manifold (by setting Σto be a low-rank matrix). In contrast to the above, we predict readily measurable quantities such as the test and training loss and their dependence on N, the training dynamics and regularization. 2 Diffusion Models In this section, we will provide a short introduction to diffusion models based on [ 3,26]. Diffusion models consist of an iterative noising and denoising process. The noising process simplifies the distribution of a sample x0from ρthrough the addition of noise xt=p 1−βtxt−1+p βtϵ, ϵ ∼ N (0,Id) for a number of noising steps t∈ {1,..., T }andβt∈(0,1), typically βt<1andT∼103. As tincreases, the original signal x0is gradually suppressed compared to the isotropic Gaussian noise, until one obtains xTwhose distribution is assumed to be close to N(0,Id). Defining αt= 1−βtand ¯αt=Qt s>0αsone can achieve the noising process up to tin a single step xt(ϵt) =√¯αtx0+√ 1−¯αtϵt, ϵ t∼ N (0,Id), (1) which enables an efficient implementation of the noising process. Diffusion models are optimized to reverse the noising process: they implement a mapping ϵθ(xt, t) whose paramaters θare optimized to predict the noise vector ϵt. For a dataset Dconsisting of Nsamples inRd, this amounts to minimizing L=1 dT|D|X tX x0∈DEϵt∥ϵt−ϵθ(xt(ϵt), t)∥2, (2) which we will refer to as the (training) loss. Once trained, the denoiser is then used to iteratively generate new samples. The generation process follows the reverse direction to the noising process, starting from pure white noise u0∼ N (0,Id)and culminating in a new sample uT. For sampling, we will use s=T−tas an iteration index and uas a dynamical variable to distinguish the noising process from the denoising process. The iteration reads us+1=µθ(us, T−s) +σT−sξ, ξ ∼ N (0,Id). (3) 3 where we defined µθ(xt, t):=xt−p 1−¯αtϵθ(xt, t)√αt+q (1−σ2 t)−¯αt−1ϵθ(xt, t). (4) These two equations can be understood as first predicting x0≈xt−√1−¯αtϵθ √¯αt, then adding back \"noise\" in the form of σtξ+p (1−σ2 t)−¯αt−1ϵθ. 2.1 Affine linear denoisers Typical architectures for ϵθare very complex, including U-nets [ 27] and tranformers. To build a theory of diffusion models, we instead focus on linear denoisers. Linear neural networks [ 28,29] have a long history of successfully elucidating various phenomena in machine learning theory, such as the double descent phenomenon [ 12] or the dynamics of learning semantic information [ 30]. Here, we consider the case where for each t, the denoiser is implemented by an affine linear mapping ϵθ(xt, t) =Wt\u0000 xt−√¯αtbt\u0001 (5) where Wt∈Rd×dis a weight matrix and bt∈Rda bias term. Training the denoiser then amounts to op- timizing eq. (2) with respect to {Wt, bt}t. We will also add a standard regularization termP tγtTrWtWT t to the training objective, where the prefactor γtallows us to apply different levels of regularization to different noising stages t. With these definitions in hand, we now move to our first results on the generalization abilities of linear diffusion models. 3 Optimal linear diffusion models on unstructured data In this section, we will explore the general mechanisms underlying generalization with Nin the linear case. Ultimately, we will compute the Kullback-Leibler divergence, which compares the distribution of the samples from the model to the ground truth. However, in practice, this quantity is often not accessible. Hence, we will first characterize the test loss, as it is readily measured also in the case of non-linear models and non-Gaussian data. We then make a connection between these two quantities, highlighting that they both penalize the absence of relevant directions of variation in the training data at small N. Since Wt,btare not coupled in eq. (2) across different values of t, we can find their optima inde- pendently for each noising stage. Inserting eq. (5) into eq. (2), we find that the contribution for each t decomposes into a data-dependent term and one which depends only on the additive noise Eϵt∥ϵt−ϵθ(xt, t)∥2= ¯αt∥Wt(x0−bt)∥2+Eϵt \u0000 1−√ 1−¯αtWt\u0001 ϵt 2. (6) We will not consider variations which arise due to the fact that only a finite number of noised samples are used at each training step, hence in eq. (6) we take the average over infinitely many realizations of ϵt. Eq.(6)shows that the loss contains only terms either linear or quadratic in the training data x0. Consequently, only the first two moments of the data will contribute to eq. (2). We therefore define the empirical mean µ0and covariance Σ0 µ0:=1 NX x0∈Dx0, Σ0:=1 NX x0∈D(x0−µ0) (x0−µ0)T. (7) To evaluate the minimum of the training loss, we add the regularization termP tγtTrWtWT tto the loss and evaluate both the average over ϵtand the sum over the data set. Eq. (6)is quadratic in Wt, bt, hence it is convex with the unique minimum b∗ t=µ0, W∗ t=√1−¯αt ¯αtΣ0+ (1−¯αt+γt)Id. (8) 4 3.1 Test and residual loss Using eq. (8), we now evaluate the difference between the the optimal training loss and the test loss, where the average runs over ρinstead of the training data. This will help us understand how the mapping obtained from finite Nperforms compared to the optimal linear denoiser, the reference model we expect or architecture to converge to. To understand how test and training loss evolve as we vary N, we express both quantities using the spectral properties of Σ0. Since Σ0is real and symmetric, it has an eigendecomposition Σ0=P νλ0 νe0 ν⊗e0 ν, with{λ0 ν}νits eigenvalues, {e0 ν}νits normalized eigenvectors and ⊗the outer product. At the optimum, W∗ t, b∗ t, we find the irreducible, or residual, loss R=X tX ν\u0014¯αtλ0 ν+γt (¯αtλ0ν+ (1−¯αt+γt))\u0015. (9) which is the minimal value of the training loss which a linear denoiser can achieve. Since the test loss is a quadratic function of the data, the only statistics of ρneeded to evaluate it are the population mean µ and the population covariance Σ. Using the shorthand notation Σµ,ν=\u0000 e0 µ\u0001TΣe0 νfor the entries of Σin the eigenbasis of Σ0, we find the difference between Rand the test loss Ltest−R=X t\u0000 ¯αt−¯α2 t\u0001X ν\" Σνν−λ0 ν (¯αtλ0ν+ (1−¯αt+γt))2+(µ−µ0)2 ν (¯αtλ0ν+ (1−¯αt+γt))2#. (10) Eq.(10)shows explicitly that the gap between RandLtestarises whenever there is a mismatch between µ0,Σ0andµ,Σ. The terms that contribute the most strongly to Ltest−Rare those for which the denominator under the sum is minimized. This occurs for the smallest values of λνand the smallest values of t, as there 1−¯αtis minimized. When the number of data in the training set, N, is smaller than the dimension d, at least N−deigenvalues of Σ0are exactly zero, giving rise to large contributions in eq. (10). In fig. 2 we exemplify the overfitting phenomenon using a standard normal distribution ρ=N(0,Id). We average over draws of the training data using the Marčenko-Pastur law [ 31] which predicts the spectral density of Σ0in the N, d→ ∞, N/d =const. limit. We show examples of the Marčenko-Pastur distribution for several values ofN din fig. 2. For N < d, the distribution acquires a Dirac delta peak at λ0= 0corresponding to the nullspace of Σ0and a bulk of non-zero eigenvalues. AsN dincreases above one, the delta peak vanishes and the bulk eventually concentrates around 1. The presence of zero eigenvalues in the spectrum of Σ0is reflected in a very large gap between training and test loss in fig. 2 b) when N < d. However, this gap can effectively be reduced by regularization, through the presence of γtin the denominator of 10. A promising choice is γt= ¯αtc, withc∈R, c > 0, which regularizes the denoiser for lower tmore strongly than for larger t, befitting our earlier observation that overfitting is most severe at small t. In 2 we see that even minor levels ( c= 0.01) of regularization significantly reduce overfitting. This choice of γtcorresponds to replacing Σ0in eq. (8) by¯αtΣeff. 0, where Σeff. 0= Σ 0+cIdis an effective covariance matrix. Therefore, one can interpret the presence of a regularization term as an assumption on the minimal level of variability cin an arbitrary direction. On the other hand, we can see it as a cutoff imposed on the eigenvalues of Σ0, below which the structure of the distribution of ρNcannot be resolved. 3.2 Sample statistics at the optimum In the previous section, we have compared empirically accessible quantities such as test and training loss. Now, we move to a measure which directly compares the distributions of the generated samples andρ, the Kullback-Leibler divergence (DKL). Affine linear denoisers sample from Gaussian distributions: Starting from an initial Gaussian random variable u0∼ N (0,Id),uTis a linear map of Gaussian random variables and constants, hence it is also Gaussian. Up to orders of T−1, affine linear networks reproduce the mean µ0and covariance Σ0of 5 102 100N/d=0.3a) Marc enko Pastur 102 100N/d=1.0 0 100 102 100N/d=10.0 101 100N/d102 100102loss b) =(0,Id) c=0 c=102 c=101 101102103104 training steps24 lossd) training dynamics train test 101102103104 training steps100101test losse) test loss dynamics per t t=1 t=10t=50 t=500101 100101 N/d101 1001 dDKL(N|) c) Kullbeck-Leibler div.c=104 c=102 c=101 Figure 2: a) Marčenko-Pastur distribution for varying fractions N/d. b) Prediction for test loss (full lines) and residual loss (dashed lines) over N/d. Empty symbols are simulation results for the residual loss, full symbols denote the test loss, averaged over 5random draws of training sets per data point. c) Kullback-Leibler divergence between sampling distribution and ρ. Lines are prediction, squares report averages over 10draws of the data. d) Training and test loss over training steps for one draw of the training data in addition to prediction (dashed lines) for N/d= 0.5andc= 0. e) Same as d), but per denoising step t. All results were obtained for ρ=N(0,Id)andd= 26. Error bars for simulation results report one standard deviation. the training data (for a derivation, see appendix C.3). Consequently, the distribution of samples from the linear denoiser is ρN=N(µ0,Σ0+cId), where cis a small parameter. The presence of ccan be interpreted as originating either from the corrections due to the finite number of sampling steps (see appendix C for details), or from the regularization term γt=√¯αtc. We now impose a Gaussian hypothesis on the data ρ=N(µ,Σ). This assumption lets us treat exclusively the deviations between ρ, ρNwhich arise due to finite N, because ρis in principle reachable with a linear denoiser (opposed to non-Gaussian distributions). To characterize the deviations due to finite N, we compute the DKL between the distribution of samples and ρ DKL(ρN|ρ) =1 2\u0014 ln|Σ| |Σ0+cId|+ (µ−µ0)TΣ−1(µ−µ0) + Tr Σ−1(Σ0+cId)−d\u0015, (11) which is a measure of distance between distributions, in particular DKL(ρ′|ρ) = 0 ⇔ρ′=ρ. The most dominant term at small NisDKL(ρN|ρ)isln|Σ| |Σeff. 0|=P νlnλν λ0ν+c, where λνare the eigenvalues of Σ. This term in the DKL heavily penalizes the presence of a nullspace in Σ0for small c, analogous to the test loss. We show the DKL in 2c) finding the same two relevant regimes as in the test loss: first, when N < d, regularization is effective to reduce the Kullback-Leibler diverence. Second, when N > d, the DKL(ρN|ρ)diminishes rapidly. This decrease in 11 not mirrored in the test loss, which plateaus due to the presence of the residual loss R. 3.3 The speed of learning We solve the learning dynamics of matrices Wtfor centered datasets ( µ0= 0) in appendix B. We find that the elements of Wtexponentially relax towards 8 at a different rates ¯αtλ0 ν+ 1−¯αt+γtcorresponding to different spatial directions. The rate ¯αtλ0 ν+ 1−¯αt+γtcorresponds precisely to the denominator of the terms in eq. (10), whose minimal values lead to the most severe overfitting. This means that the 6 101103 104 101 102 a) EigenvaluesCelebA MNIST 102 100 N/d102 100lossb) CelebA Linear U-net 102 101 100 N/d100DKL(N|)/d e) 102 100 N/d102 101 100lossc) MNIST 101 101 N/d101 100DKL(N|)/d f) 101 100101 N/d102 101 100DKL(N|)/d d) k k=0k=1k=2d/(4N)Figure 3: a) Eigenvalues of CelebA and MNIST dataset, sorted by rank. b) and c) show test loss (filled symbols simulation, dark gray curves prediction) and training loss (empty symbols simulation, light gray curves prediction) for over number of training samples. In the linear case, test and training loss are averaged over Gaussian samples with the same mean and covariance as the data, averaged over 100 draws of the training set. In the nonlinear case, they are averaged over min {100, N}training and 200 test samples. d) Kullback-Leibler divergence between sample distribution of linear diffusion models with regularization c= 10−4andρ, where the eigenvalues of Σfollow a powerlaw. Symbols are averages over10random draws of the training sets, error bars report one standard deviation, but are typically smaller than the symbol size. e) and f) are equivalent to d), but for Σoriginating from the CelebA and MNIST datasets, respectively. most precarious directions in the sense of overfitting are also the ones which are learned the slowest. At the same time, regularization speeds up the learning process in all directions, as it increases the rate of convergence to the optimum. We compare our analytical results to the training curves of linear neural networks in fig. 2 d). While the training loss approaches Rexponentially, we see that the test loss initially decays, then increases again, as the model overfits the data. Furthermore, we see in fig. 2 d) that, the smaller tis, the later overfitting occurs. This makes early stopping an effective strategy to prevent overfitting. Throughout this section, we have considered the case of Σ = Id. For realistic data, we must treat the case of general Σ, which we will do in the following. 4 Structured data To move towards a more realistic model of data, we now consider two widely used image-datasets: the CelebA dataset [ 11], a dataset of celebrity portraits, down-sampled to 80×80grayscale pixels, and the MNIST dataset, a dataset of handwritten digits [ 32], up-sampled to 32×32pixels. We provide the code used to produce these results in. Recall that for linear diffusion models, the relevant structure of ρenters via the mean µand its covariance Σ. For many realistic datasets, including CelebA and MNIST, we find that the spectra of Σ are strongly hierarchical, meaning that there are few eigendirections with large variance and many eigendirections with comparatively small variance, see fig. 3a). This hierarchy in the eigenvalues can be quantified e.g. using the participation ratio, DPR=(Tr Σ)2 Tr Σ2, a measure of effective dimensionality. For example, for the CelebA dataset, d= 6400, butDPR≈7.8, in the MNIST dataset, we find d= 1024, butDPR≈25. To evaluate the test loss or the Kullback-Leibler divergence, we must average over the different realizations of Σ0originating from independent draws of the data from ρ. This is far from straight- forward, as the random matrix Σ0appears e.g. in the denominator of the test loss. In appendix D, we 7 use the replica trick to compute a potential whose derivatives create the necessary averages. The use of this trick dates back to [ 34], and has been used e.g. to compute the spectra of sparse random matrices [35,36]. We find that the averages depend on a quantity qwhich must be determined self-consistently from the spectrum of Σ. We will express our result using qand the weighted sum Rk(q), which are determined by qt=1 dX νλν 1 +λνˆαtN dqˆαt+NdRt k=d−1X νλk ν\u0010 qt+N dˆαt+N dλν\u00112. (12) where ˆαt= ¯αt/(1−¯αt+γt). 4.1 Test loss Using eq. (12) we can then express the test loss via Ltest= 1 +1 TX t1 1 +γt 1−¯αt\"\u0000 qt+N dˆα\u00012 1−¯αt+γt Rt 0+Rt 1+\u0010 Rt 3 2\u00112 + (Rt 1)2 1−N dRt 2 −2 dTrId Id+Nˆαt dˆαtqt+NΣ# (13) In fig. 3, we compare test and training loss computed using both linear models at the optimum eq. (8) and nonlinear models. While the data points originating from the nonlinear models are in general more noisy and do not follow the trajectory of the linear models, we find that the gap between the training and test closes approximately at the same number of training data. For the MNIST data, the test loss computed on different architectures does not converge to the same value, with the U-net architecture outperforming the linear model. While the U-net architecture is far more flexible and is expected to outperform the linear model, we believe that this deviation is mostly due to the MNIST data being almost binary: almost all pixels in the MNIST images saturate to either +1or−1, meaning that a non-linear model which predicts the noise to be the difference to the closest binary variable will naturally outperform a linear model. 4.2 Alignment with the reference model While the test loss is readily evaluated across datasets and architectures, it plateaus when N > d, even when other generalization measures, such as the Kullback-Leibler divergence, continue to improve. We hence define the following measure, which does not saturate in this regime ∆ϵN=1 TX t ||ϵN(xtest)−ϵ∞(xtest)t)||2 (xtest), where (xtest)are noised test samples, ϵNis the mapping obtained from a finite dataset of Nsamples and ϵ∞is the mapping obtained from an infinite number of samples. In practice, when only finite number of data are available, we choose ϵ∞as the mapping optimized on the largest subset of the data. ∆ϵN, obtained from disjoint data sets, then measures if the mapping has generalized towards a solution which is independent of the training set. For linear diffusion models, we find ∆ϵN=Ltest−1 +1 TX t1−¯αt (1−¯αt+γt)Tr (Id+ ˆαtΣ)−1. (14) Note the similarity between ∆ϵNand the test loss eq. (13): these two measures differ only by terms independent of N, which are responsible for the plateau of the test loss. In the unsaturated regime, the test loss is a good proxy for ∆ϵN. For N > d, increasing Ncontinues to improve generalization (measured by ∆ϵN), but this effect is no longer reflected in the test loss. In fig. 1 a), we show ∆ϵNfor the CelebA dataset, both for linear and non-linear diffusion models, as well as a prediction in the average over draws of the dataset from replica theory. We find that both for linear and non-linear diffusion models, the curves show a kink around N∼103, which is precisely where the gap between training and test loss closes (see fig. 2 c)) and memorization diminishes (see fig. 1 d)). 8 102104 N1.01.52.0dissimilarity from linear referencea) rel. error 101103 100101b) per eigenmode 20 80 640 3200 12800 1 6000 T 0tc) N=20 1 6000 d) N=1280 1 6000 e) N=12800 6 3 03Figure 4: Relative difference of non-linear denoisers from best linear model, per noising step tand direction ν, trained on inecreasing numbers of data. a) averaged over νandt, b) averaged over t, c) - e) logdt,νperν, t. All data are averaged over 100test samples per t, ν. 4.3 Kullback-Leibler divergence As a final measure of generalization, we compare the Kullback-Leibler divergence between our Gaussian hypothesis on the data ρ=N(µ,Σ)and the sampling distribution ρN=N(µ0,Σ0+cId). We then find 1 dDKL(ρN|ρ) =1 2q c d Ncq+ 1−1 2dX iln c λi+1 d Ncq+ 1 −N 2dln\u0012d Ncq+ 1\u0013 +d+ 2√cTr Σ−1 2+cTr Σ−1 2Nd+c 2dTr Σ−1(15) where the first line in eq. (15) originates from the term ln|Σ|/|Σ0+cId|andqis computed from 12 with ˆαt=c−1. When cis very small, the terms in the first line eq. (15) dominate the expression. In fig. 3 d) e), f) we compare measurements of DKL(ρN|ρ)to the outcome of eq. (15) for Σmeasured on MNIST and CelebA, and case where the eigenvalues of Σare follow a power-law. Analogously to the case of unstructured data, the first line of eq. (15) penalizes the presence of a nullspace of Σ0, although this can be mediated by the presence of very small eigenvalues in Σ. For N < d, the presence of a hierarchy in the eigenvalues of Σ(controlled e. g. by increasing kin fig. 3d)) can decrease the DKL. Intuitively, this is because the absence of variation in Σ0is not as significant when the corresponding variation in Σis also small. We find that for N > d, the DKL collapses on to the same line independently of the specifics of Σ. The independence of the DKL on Σhas been noted in [ 37], who argued that this makes the DKL a good measure for the similarity of Σ0toΣ. In D.8, we show that when cis much smaller than the smallest eigenvalue of ΣandN > d the DKL is approximately given by d/(4N), where we have neglected terms of order (d/N)2and√c. In the N > d regime, we find this scaling of the DKL across realizations of Σ(see fig. 3 d) - f)), up to deviations which originate from c >0, which causes a saturation of the DKL above zero. 5 Differences between linear and non-linear models We now test whether the mappings encoded by different architectures are indeed similar. Prior studies have observed increasing similarity between non-linear and linear models with t, see [ 38,39]. We compute the relative distance of their mappings in the eigenspace of Σ. We define a direction - and t dependent distance dt,νmeasure dt,ν=\u0000 ϵN(xt, t)−ϵ∗ ∞(xt, t)\u00012 ν |ϵN(xt, t) +η|ν|ϵ∗∞(xt, t) +η|ν(16) where ϵNis a U-net trained on Nexamples and ϵ∗ ∞is a linear model with modest regularization c= 10−2, trained on the maximal amount of available data and η= 10−3prevents divergences. In fig. 4, we show dt,νfor the CelebA dataset. Overall, we find that the relative error decays with N,νand t. Indeed for a large extent of t, ν, the difference between linear and non-linear models becomes very 9 small. However, for leading eigenmodes (small ν), the differences between linear and non-linear models grow with N. This (small ν, small t) is also the regime where we expect the non-Gaussianity of the data to have the largest effect. 6 Discussion We have identified two relevant regimes for generalization in linear diffusion models, N > d andN < d. When N < d, the model overfits due to a lack of variability in the training set, namely the low-rank structure of the empirical covariance matrix. Both the Kullback-Leibler divergence and test loss strongly penalize this lack of variability. However, overfitting can be mitigated by applying early stopping and regularization. Regularization acts both as a prior on the minimal amount of variability in any direction of the data and as a cutoff on the spectrum of Σbelow which the structure of the distribution cannot be resolved. For N > d, structure in the data plays a minor role and generalization measured by the Kullback-Leibler divergence decays linearly with d/N, even for data which are believed to be intrinsically low-dimensional. Intriguingly, we found that a highly hierarchical structure in the data usually associated with a lower effective dimensionality has no significant effect on the emergence of the two regimes. This suggests that linear diffusion models place emphasis on learning all directions with finite variability, not only those with the highest levels of variation. A similar effect has previously been observed in [ 40], where learning in the supervised setting was contrasted with diffusion models. In the supervised case, an effect called benign overfitting occurs: if the data consist of a signal that is corrupted by noise, the model may overfit to the signal, ignoring the noise. In diffusion models, however, both the signal and the noise are faithfully represented, meaning that all variability in the data is taken into account. This is intuitive, given that the objective in training diffusion models is precisely to draw from a distribution with the same level of variability. One important open issue remains to identify the “correct” metric to measure generalization in diffusion models. The authors of [ 25] have argued that even for linear models, the relevant scaling of N withdis either linear (as in the Kullbeck-Leibler divergence considered here) or quadratic, depending on the metric. Nevertheless, the practical consequences we draw from our analysis are simple: When N < d, then either regularization or early stopping are effective to prevent overfitting, whereas N > d is naturally favored. Finally, we found that linear diffusion models are very similar to their non-linear counterparts in a significant portion of the sampling trajectory and data space. Accounting for the non-linearity of the mapping especially for leading eigendirections of Σand low levels of noise will be a fruitful direction of future research. Acknowledgements We are grateful to Alessio Giorlandino for helpful discussions. CM and SG gratefully acknowledge funding from Next Generation EU, in the context of the National Recovery and Resilience Plan, In- vestment PE1 – Project FAIR “Future Artificial Intelligence Research” (CUP G53C22000440006). SG additionally acknowledges funding from the European Research Council (ERC) for the project “beyond2”, ID 101166056, and from the European Union–NextGenerationEU, in the framework of the PRIN Project SELF-MADE (code 2022E3WYTY – CUP G53D23000780001). 10 References Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsuper- vised Learning using Nonequilibrium Thermodynamics. Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribu- tion. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020. arXiv:2006.11239 [cs, stat]. Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling. In Advances in Neural Information Processing Systems, volume 34, pages 17695–17709. Curran Associates, Inc., 2021. Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative Modeling with Denoising Auto-Encoders and Langevin Sampling, October 2022. arXiv:2002.00107 [stat]. Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis, May 2023. arXiv:2208.05314 [stat]. Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us Build Bridges: Understanding and Extending Diffusion Generative Models, August 2022. arXiv:2208.14699 [cs]. Holden Lee, Holden Lee, Jhu Edu, Jianfeng Lu, Yixin Tan, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. Jakiw Pidstrigach. Score-Based Generative Models Detect Manifolds, October 2022. arXiv:2206.01018 [stat]. Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, and Stéphane Mallat. Generalization in diffu- sion models arises from geometry-adaptive harmonic representations, April 2024. arXiv:2310.02557 [cs]. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. pages 3730–3738. IEEE Computer Society, December 2015. ISSN: 2380-7504. Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of general- ization error in neural networks. Neural Networks, 132:428–446, 2020. Kirsten Fischer, Alexandre René, Christian Keup, Moritz Layer, David Dahmen, and Moritz Helias. Decomposing neural networks as mappings of correlation functions. Physical Review Research, 4(4):043143, November 2022. Publisher: American Physical Society. Luca Ambrogioni. In search of dispersed memories: Generative diffusion models are associative memory networks, November 2023. arXiv:2309.17290 [cs, stat]. Beatrice Achilli, Luca Ambrogioni, Carlo Lucibello, Marc Mézard, and Enrico Ventura. Memoriza- tion and Generalization in Generative Diffusion under the Manifold Hypothesis, February 2025. arXiv:2502.09578 [cond-mat]. Giulio Biroli, Tony Bonnaire, Valentin de Bortoli, and Marc Mézard. Dynamical Regimes of Diffusion Models, February 2024. arXiv:2402.18491 [cond-mat]. Anand Jerry George, Rodrigo Veiga, and Nicolas Macris. Analysis of Diffusion Models for Manifold Data, February 2025. arXiv:2502.04339 [math]. 11  Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, and Dmitry Krotov. Memorization to Generalization: The Emergence of Diffusion Models from Associative Memory. November 2024. Anand Jerry George, Rodrigo Veiga, and Nicolas Macris. Denoising Score Matching with Ran- dom Features: Insights on Diffusion Models from Precise Learning Curves, February 2025. arXiv:2502.00336 [cs]. Puheng Li, Zhong Li, Huishuai Zhang, and Jiang Bian. On the Generalization Properties of Diffusion Models, March 2025. arXiv:2311.01797 [cs]. Alessandro Favero, Antonio Sclocchi, Francesco Cagnetta, Pascal Frossard, and Matthieu Wyart. How compositional generalization and creativity improve as diffusion models are trained, March 2025. arXiv:2502.12089 [stat]. Francesco Cagnetta, Leonardo Petrini, Umberto M. Tomasini, Alessandro Favero, and Matthieu Wyart. How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. Physical Review X, 14(3):031001, July 2024. Publisher: American Physical Society. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data. ArXiv, abs/2302.07194:null, 2023. Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, and Qing Qu. Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering, December 2024. arXiv:2409.02426 [cs]. Giulio Biroli and Marc Mézard. Generative diffusion in very large dimensions. Journal of Statistical Mechanics: Theory and Experiment, 2023(9):093402, September 2023. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models, October 2022. arXiv:2010.02502. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomed- ical Image Segmentation, May 2015. arXiv:1505.04597 [cs]. Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53–58, 1989. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR, 2014. Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic de- velopment in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537– 11546, 2019. V A Marčenko and L A Pastur. DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES. Mathematics of the USSR-Sbornik, 1(4):457–483, April 1967. Li Deng. The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]. IEEE Signal Processing Magazine, 29(6):141–142, November 2012. Claudia Merger and Sebastian Goldt. Code for Generalization Dynamics of Linear Diffusion models, May 2025. S F Edwards and R C Jones. The eigenvalue spectrum of a large symmetric random matrix. Journal of Physics A: Mathematical and General, 9(10):1595–1603, October 1976. 12  Reimer Kuehn. Spectra of Sparse Random Matrices. Journal of Physics A: Mathematical and Theoretical, 41(29):295002, July 2008. arXiv:0803.2886 [cond-mat]. G. J. Rodgers and A. J. Bray. Density of states of a sparse random matrix. Physical Review B, 37(7):3557–3562, March 1988. Michele Tumminello, Fabrizio Lillo, and Rosario N. Mantegna. Kullback-Leibler distance as a measure of the information filtered from multivariate data. Physical Review E, 76(3):031123, September 2007. Publisher: American Physical Society. Binxu Wang and John J. Vastola. The Hidden Linear Structure in Score-Based Models and its Application, November 2023. arXiv:2311.10892 [cs]. Xiang Li, Yixiang Dai, and Qing Qu. Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure. November 2024. Andi Han, Wei Huang, Yuan Cao, and Difan Zou. On the Feature Learning in Diffusion Models, December 2024. arXiv:2412.01021 [stat]. 13 a) Eigenvectors 1 2 3 4 5 400 800 1600 3200 6400 0 0 b) Fourier spectra 0 0 0 0 0 0 0 0 0 Figure A.1: a) First five leading eigenvectors ν= 1,..., 5of the covariance matrix of the CelebA data, as well as sub-leading eigenvectors ν= 400,..., 6400. b) Corresponding Fourier spectra of the eigenvectors. A A detail-based similarity measure for CelebA For the models trained on CelebA, computing the cosine similarity c(x, y) =xTy |x||y|between a generated sample and one from the training set yields a very high similarity ∼0.9, even if the generated images are genuinely different. Upon manual inspection of the corresponding images, we find that this occurs due to a large portion of the image, such as the background, being uniformly dark or light. In fig. A.1 we compare the eigenvectors of the covariance matrix of the CelebA data to their corresponding Fourier spectra. We find that leading eigenvectors ν= 1,..., 5have a more homogeneous spatial distribution of light and dark pixels, correspondingly their Fourier spectra are concentrated around small frequencies (small |ω|). Asνincreases, however, the spectra of the eigenvectors become more broad, and small frequencies are suppressed. On the basis of these observations, we construct a similarity measure which is oriented more towards the details of the images: We first project the images into the space spanned only by sub-leading eigenvectors ν >10. We then compute the cosine similarities of the resulting vectors. We find that this measure is then more sensitive to changes in the details of the images, which leave the background uniform (e.g. for the generated image and closest training set examples in fig. 1 for N≥6400 ). B Learning dynamics of linear denoisers Throughout this section, we will assume that all data sets are centered, hence that µ0=µ= 0and that the bias terms btare initialized at zero, corresponding to their optimal value in this case. We introduce a training time τand a learning rate η. At each training step, we will update the parameters of the linear network θaccording to θ(τ+dτ)−θ(τ)dτ=−η∇θL We will treat the dynamics of Wtit in the eigenbasis of the empirical covariance matrix, Wt(τ) =X νwµ,ν,t(τ)e0 µ⊗e0 ν Inserting this expression into the training loss, we find L=1 dTX tX µ,ν\u0002\u0000 ¯αtλ0 ν+γt+ 1−¯αt\u0001 w2 µ,ν,t−2√ 1−¯αtwµ,ν,t\u0003 +d 14 This expression shows that all entries in wµ,ν,t(τ)decouple and we can treat the evolution of the weight matrices elementwise. Taking the derivative and using the definition of W∗ T(eq. (8)) we find, that in the limitdτ→0 wµ,ν,t(τ) =w∗ µ,ν,t+ expn −2η dT\u0002 ¯αtλ0 ν+ (1−¯αt+γt)\u0003 τo\u0000 wµ,ν,t(0)−w∗ µ,ν,t\u0001. (B.1) This expression shows that through training, the entries of Wtapproach their optimal value exponentially with rate 2η dT\u0002 ¯αtλ0 ν+ (1−¯αt+γt)\u0003. The expression for the training and test loss, shown in 2 follows from inserting eq. (B.1) into L. C Sampling dynamics of affine linear denoisers In this section we compute how the mean and covariance of the samples evolve under the the iterative denoising process specified in eq. (3). Before we do so, we note that to sample from a given Gaussian distribution with mean µ0and covariance Σ0, no iterative process is necessary. Rather, given u0∼ N(0,Id), we can generate a sample uwith the aforementioned statistics with a simple linear transform uT=p Σ0u0+µ0. (C.1) we will see in the following that the iterative sampling process approaches the same statistics of uT. In this manuscript, we do not consider the fact that the noising process is discrete, rather we will treat sampling time as continuous. However, we will highlight corrections which arise due to ¯α(0)̸= 1and ¯α(T)̸= 0. C.1 Continuous time limit For the following calculations, it will be useful to write a continuous time version of the denoising process. To this end, we also consider a rescaled time in which the time arguments in the sampling process are incremented by h=T−1,rather than increments of 1as in eq. (3), thus both denoising timesand noising time trun from zero to one with t= 1−s. We now assume β(t) =O(h),∀t. This is reasonable, since we expect the changes in every step of the diffusion process to be small. We now define ˆβ(t):=βt hˆσ(t):=σt√ h. (C.2) For¯α(t),we use that ¯α(t+h) =\u0010 1−ˆβ(t)h\u0011 ¯α(t), so in the limit T→ ∞,or equivalently h→0, d¯α(t) dt=−ˆβ(t)¯α(t). (C.3) This differential equation admits a formal solution, namely ¯α(t) =e−ζ(t), ζ(t):=Zt 0dsˆβ(s). (C.4) C.2 Fokker-Planck equation With the definition of the continuous noising/denoising time limit in hand, we now write eq. (3) as a linear stochastic differential equation. We find du(s) = (m(s)u(s) +c(s))ds+ ˆσ(1−s)dZs 15 where ms, csare found by inserting eq. (4) and eq. (8) into eq. (3) and Zsis a Wiener process. We find m(s) =ˆβ(1−s) 2Id−1 2\u0010 ˆσ2(1−s) +ˆβ(1−s)\u0011 ¯α(1−s)Σ0+ (1−¯α(1−s))Id, c(s) =1 2\u0010 ˆσ2(1−s) +ˆβ(1−s)\u0011p ¯α(1−s) ¯α(1−s)Σ0+ (1−¯α(1−s))Idµ0. Observe that m(s)is diagonal in the eigenbasis of Σ0. Moving into this basis, we can now solve for the statistics of the sampling process in a decoupled manner, as in this basis, all entries of u(s)are statistically independent. In the following calculation, we will keep one direction uνfixed, dropping the index νfo brevity. We use the Fokker-Planck equation to write down the differential equation for the density ρ(u, t)which describes this variable. We have ∂sρ(u, s) =−∂u[(m(s)u(s) +c(s))ρ(u, s)] +1 2∂2 u\u0002 ˆσ(1−s)2ρ(u, s)\u0003. (C.5) Since we know that this is a Gaussian process, we make the following Ansatz for the density ρ(u, s) =1p 2πσu(s)2exp\" −(u(s)−µu(s))2 2σu(s)2# defining µu(s), σu(s)2as the mean and variance of the samples at sampling time s, respectively. With this Ansatz we find that ∂s(σu(s)2) = 2 m(s)σu(s)2+ ˆσ(1−s)2∂sµu(s) =m(s)µu(s) +c(s). (C.6) which admit the solutions µu(s) = exp\u0012Zs 0dv m(v)\u0013\u0014Zs 0dvexp\u0012 −Zv 0dw m (w)\u0013 c(v) +µu(0)\u0015 (C.7) σu(s)2= exp\u0012 2Zs 0dv m(v)\u0013\u0014Zs 0dvexp\u0012 −2Zv 0dw m (w)\u0013 ˆσ(1−v)2+σu(0)2\u0015 (C.8) The initial conditions are σu(0) = 1,µu(0) = 0 since u(0)∼ N (0,Id). We will find closed form solutions for these integrals for two choices of ˆσin the next step. C.3 Solutions for mean and covariance of samples We will first simplify some of the integrals to solve these two equations. To find the variance σu(s)2, we first solveZs 0dvˆβ(1−v) = ln\u0012¯α(1−s) ¯α(1)\u0013 Zs 0dv−ˆβ(1−v) ¯α(1−v)(λ0−1) + 1=−ln\u0012¯α(1−s) ¯α(1)\u0013 + ln\u0012¯α(1−s)(λ0−1) + 1 ¯α(1)(λ0−1) + 1\u0013 Second, we have c(s) =p ¯α(1−s)\u0010 m(s)−ˆβ(1−s) 2\u0011 µ0. With Zs 0dvexp\u0012 −Zv 0dw m (w)\u0013p ¯α(1−v) m(v)−ˆβ(1−v) 2! =p ¯α(1−s) exp\u0012 −Zs 0dw m (w)\u0013 −p ¯α(1) we then find µu(s) =p ¯α(1−s)µ0−exp\u0012Zs 0dv m(v)\u0013p ¯α(1)µ0 We now treat two different scenarios, ˆσ2(t) =ˆβ(t)andˆσ(t) = 0. 16 C.3.1 ˆσ(t) = 0 In this case, we haveRs 0dv m(v) =1 2ln\u0010 ¯α(1−s)(λ0−1)+1 ¯α(1)(λ0−1)+1\u0011, hence µu(s) =p ¯α(1−s)µ0−s ¯α(1)¯α(1−s)(λ0−1) + 1 ¯α(1)(λ0−1) + 1µ0 (C.9) σu(s)2=¯α(1−s)(λ0−1) + 1 ¯α(1)(λ0−1) + 1 Since ¯α(1)vanishes exponentially with ζ, and ¯α(0) = 1+ O(h), we find that for a long noising trajectory of many steps, Σu= Σ 0+O(h)andµu=µ0+O(h), reproduce the empirical mean and covariance of the training set to good approximation. C.3.2 ˆσ2(t) =ˆβ(t) In this caseRs 0dv m(v) = ln\u0010 ¯α(1−s)(λ0−1)+1 ¯α(1)(λ0−1)+1\u0011 +1 2ln\u0010 ¯α(1) ¯α(1−s)\u0011, hence we find the sampling mean to be µu(s) =p ¯α(1−s)µ0−s ¯α(1)2 ¯α(1−s)¯α(1−s)(λ0−1) + 1 ¯α(1)(λ0−1) + 1µ0. To evaluate eq. (C.8) for the covariance we first solve the following integral Zs 0dvexp\u0012 −2Zv 0dw m (w)\u0013 ˆβ(1−v) =\u0000 ¯α(1)(λ0−1) + 1\u00012 ¯α(1) ·\u0012¯α(1−s) ¯α(1−s)(λ0−1) + 1−¯α(1) ¯α(1)(λ0−1) + 1\u0013. Inserting this into eq. (C.8), we find for the sampling covariance σu(s)2= ¯α(1−s)(λ0−1) + 1 +¯α(1)2(λ0−1) ¯α(1−s)\u0012¯α(1−s)(λ0−1) + 1 ¯α(1)(λ0−1) + 1\u00132 Again, since ¯α(1)vanishes exponentially with ζ, and ¯α(0) = 1 + O(h), we find that for a long noising trajectory of many steps, Σu(1) = Σ 0+O(h)andµu(1) = µ0+O(h), meaning that the sampling mean and covariance reproduce the empirical mean and covariance of the training set to good approximation. In the case of finite regularization γt=c√¯αt, we must replace λ0with λ0+cin all formulae. This shows that both regularization ¯α(0)̸= 1bias the sampler towards a covariance matrix with an additional, spherical term. D Replica theory for linear denoisers at finite N In this appendix, we derive summary statistics for linear denoisers optimized using the empirical covariance matrix Σ0for different sample sizes N. We will assume that the training data originates from a centered Gaussian ρ∼ N (0,Σ), where we define the \"true\" covariance matrix Σto be Σ =RΛRT,Λ = λ1... 0... 0... λ d  (D.1) 17 withRa fixed rotation matrix, therefore |R|= 1, RTR=Id. We parametrize the empirical covariance matrix Id + ˆαtΣ0in the following way Σ0=1 NNX β=1Σ1 2xβ\u0010 Σ1 2xβ\u0011T, xβ∼ N (0,Id)∀β= 1,..., N We are now interested in statistics of the inverse of the related random matrix Id+ ˆαΣ0. We define the following quantities fg(J) =1 dZY βdρ\u0010 xβ\u0011 lnZ(J,Σ0) Z(J,Σ0):= Id+ ˆαΣ0+RJg(Λ)RT −1 2 =Zdη √ 2πdexp −1 2ηT\" Id+ˆα NNX β=1Σ1 2xβ\u0010 Σ1 2xβ\u0011T +RJg(Λ)RT# η  (D.2) The function fthen plays the role of a generating functional for the moments of the inverse of Id+ ˆαtΣ0, e.g. 1 dD (Id+ ˆαtΣ0)−1E Σ0=R\u0012 −2d dJf(J)|J=0,g(Λ)= Id\u0013 RT For the relevant quantities computed in this manuscript, it will be sufficient to compute ffor diagonal J,Jij:=δijJi. This is because all quantities can be written as traces of matrix products, and choosing Jthus corresponds to choosing the basis in which we evaluate the trace to be given by R. The difficulty in computing fthen arises from the fact that all the integrals in xβare coupled in the logarithm. To evaluate the integral, we now use the replica trick, which consists of re-writing the logarithm as ⟨lnZ(J,Σ0)⟩Σ0= lim n→01 n\u0000 ⟨Z(J,Σ0)⟩n Σ0−1\u0001 (D.3) The replica trick then consists of evaluating ⟨Z(J,Σ0)⟩n Σ0for integer values of nand then taking the limit n→0. To compute ⟨Z(J,Σ0)⟩n Σ0, we first write the power as an integral over nindependent variables ηα, where αis the replica index, ⟨Z(J,Σ0)⟩n Σ0=Z nY α=1dηα √ 2πd!* exp nX α=1−1 2(ηα)T(Id+ ˆαtΣ0+RJRTf(Σ))ηα!+ Σ0(D.4) In the following section, we will simplify this expression via a change of variables to a set of summary statistics. D.1 Introducing auxiliary variables We first isolate the terms depending on xβfrom the expression. ⟨Z⟩n Σ0=Z nY α=1dηα √ 2πd! exp nX α=1\u0014 −1 2(ηα)T\u0000 Id+RJf(Λ)RT\u0001\u0015! ·Z NY β=1dxβ √ 2πd exp −1 2X β\u0010 xβ\u0011T ˆα NX αΣ1 2ηα\u0010 Σ1 2ηα\u0011T +Id! xβ  (D.5) To simplify the expression a bit, we now change variables to µα= Σ1 2ηα, we then find that we can isolate one factor in which Σappears, but not the samples xβ, and vice versa. Additionally, note that for 18 given µα,the second line is just the N−th power of the first line. Both factors are coupled together by the fact that µαappear in both factors. We thus simplify to ⟨Z⟩n Σ0=Z nY α=1dµα √ 2πd! exp −1 2nX α=1\u0000 RTµα\u0001TΛ−1 2(Id+Jg(Λ)) Λ−1 2\u0000 RTµα\u0001 −n 2ln|Λ|! · Zdx √ 2πdexp −1 2xT ˆα NX αµα(µα)T+Id! x! | {z } =:G N Another rotation of both xβandµαbyRTthen eliminates Rfrom the expression, leaving all other terms unchanged. We now simplify the latter integral, G. Our goal is to have all directions iofµα decouple. To this end, we define our first auxiliary variable Rα=1√ dxTµα(D.6) and enforce this definition with a Dirac delta in the integral, using that δ(r−m) =1 2πR d˜rexp (i˜r[r−m]). This yields G=Zdx √ 2πdY αdRαd˜Rα 2πexp −1 2dˆα NX αR2 α−x2 2+i˜Rα\u0012 Rα−1√ dxTµα\u0013! =ZY αdRαd˜Rα 2πexp −1 2dˆα NX αR2 α+i˜RαRα!Zdx √ 2πdexp\u0012 −x2 2−i1√ d˜RαxTµα\u0013 We see that the integral over xis a Gaussian integral which can be solved exactly, yielding G=ZY αdRαd˜Rα 2πexp X α\u0012 −1 2dˆα NR2 α+i˜RαRα\u0013 −X α1,α2˜Rα1˜Rα2 2d(µα1)Tµα2! Importantly, this quantity depends only on the replica overlaps (µα1)Tµα2, which brings us to our second auxiliary variable: Qα1,α2:=1 d(µα1)Tµα2(D.7) Using the Hubbard-Strantonivic transform backwards to also eliminate the integrals over all Rα, we find G=Zr N dˆαnZY αd˜Rα√ 2πexp\u0012 −1 2˜RT\u0012Q d+IdN dˆα\u0013 R\u0013 =r N dˆαn Q+IdN dˆα −1 2 Inserting the result for Ginto the expression as well as enforcing the definition of Qwith another Dirac delta, we find ⟨Z⟩n Σ0=Y idρi(λi) nY α1=1dµα1α1Y α2=1dQα1,α2d˜Qα1,α2 2π! exp S\u0010 λ,{µα}α, Q,˜Q\u0011! S\u0010 λ,{µα}α, P,˜P\u0011 =−1 2X i\" λ−1 i(1 +Jig(λi))X α(µα i)2# +iX α1≤α2˜Qα1,α2\u0012 Qα1,α2−1 d(µα1)Tµα2\u0013 +NlnG(Q)−n 2ln|λi| 19 Here we have also explicitly used the fact that we chose Jto be diagonal in the eigenspace of Σ. We now also solve the integral over µαby exploiting that all spatial directions in the expression are decoupled. Theµαdependent part of the integral is then given by Si(˜Q) = lnZY αdµα i√ 2πexp −1 2λi(1 +Jif(λ))X α(µα i)2−n 2ln|λi| −i dX α1≤α2˜Qα1,α2µα1 iµα2 i  = lnp λi−n λ−1 iId(1 +Jig(λi)) +i ddiag˜Q+i d˜Q −1 2 With this, we find that the only remaining integrals are in Qand˜Q. Assuming that N,˜Q=O(d), we now pull out a factor d ¯Zn(j) =Z nY α1≤α2dQα1,α2d˜Qα1,α2 2πexp\u0010 dS\u0010 Q,˜Q\u0011\u0011 S\u0010 Q,˜Q\u0011 = iX α1≤α21 d˜Qα1,α2Qα1,α2+1 dX iSi(˜Q) +N dlnG(Q)  (D.8) We will not solve these integrals explicitly. Rather, we will approximate the integral by exp\u0010 dS\u0010 Q∗,˜Q∗\u0011\u0011, where Q∗,˜Q∗are the maxima of S. This is because due to the common prefactor d, the integral is assumed to concentrate around a single point for d→ ∞, the Saddle Point. D.2 Saddle point approximation for any n Before we find Q∗,˜Q∗, we introduce simplification in the form of a replica symmetric Ansatz Qα1,α2=qδα1,α2+p(1−δα1,α2) (D.9) i d˜Q= ˜qδα1,α2+ ˜p(1−δα1,α2). (D.10) Parameterizing Q,˜Qin this way implicitly assumes all replicas are equivalent. With this simplification, we can explicitly diagonalize Q=npe1eT 1+(q−p)Idwith∀i= 1,..., n, where eα 1=1√n∀α= 1,... n. Likewise we findi ddiag˜Q+i d˜Q=n˜pe1eT 1+ (2˜q−˜p)Id. Inserting this into the matrix determinant, we find G=r N dˆαn\u0012 q−p+N dˆα\u0013−n 2 1 +np q−p+N dˆα!−1 2 and Si(˜Q) =−n−1 2ln|1 +Jig(λi) +λi(2˜q−˜p)| −1 2ln|1 +Jig(λi) +λi(2˜q+ (n−1)˜p)| −nlnλi We now find and solve the Saddle Point equations:d daS= 0fora∈ {q, p,˜q,˜p}forn∈(0,∞). This 20 yields the following four conditions: ˜q=N 2dn (n−1) q−p+N dˆα+1 q+ (n−1)p+N dˆα! ˜p=N dn −1 q−p+N dˆα+1 q+ (n−1)p+N dˆα! q=1 dnX i\u0014(n−1)λi 1 +Jig(λi) +λi(2˜q−˜p)+λi 1 +Jig(λi) +λi(2˜q+ (n−1)˜p)\u0015 p=1 ndX i\u0014−λi 1 +Jig(λi) +λi(2˜q−˜p)+λi 1 +Jig(λi) +λi(2˜q+ (n−1)˜p)+\u0015 To simplify these expressions, we make the following observations. First ˜q,˜ponly depend on u:=q−p andw:=q+ (n−1)p. Second q, ponly depend on ˜u= 2˜q−˜pand˜w= 2˜q+ (n−1)˜p. It is hence possible to re-parametrize the problem and thereby decouple some of the variables. Concretely, using the first two equations, we find ˜u=N d1 u+N dˆα˜w=N d 1 w+N dˆα! The second two equations yield u=1 dX i\u0014λi 1 +Jig(λi) +λi˜u\u0015 w=1 dX i\u0014λi 1 +Jig(λi) +λi˜w\u0015 This defines a self-consistency equation each for u, w. Interestingly, both pairs of self-consistency equations are the same. Assuming that the solution is unique, we find that u=w,˜u= ˜wand hence p,˜p= 0. With this, we find that qmust be found self-consistently from q(J) =1 dX i λi 1 +Jig(λi) +λi\u0012 N d1 q+N dˆα\u0013 (D.11) and ˜q=N 2dn1 q+N dˆα D.3 Taking the limit n→0 Inserting the Saddle-point values for q,˜q, p,˜p, we find ln¯Zn(J) =dn\" N 2dq(J) q(J) +N dˆα−1 2dX i\" ln 1 +Jig(λi) +λi N d1 q(J) +N dˆα! # +N 2dlnN dˆα−N 2dln\u0012 q(J) +N dˆα\u0013# Due to the linear appearance of ninln¯Zn(J), we can finally take the limit n→0in eq. (D.3) and find the expression for f f(J) =N 2dq(J) q(J) +N dˆα−\" 1 2dX iln 1 +Jig(λi) +λi N d1 q(J) +N dˆα! # −N 2dln\u0012dˆα Nq(J) + 1\u0013. 21 103 100q0.800.850.900.951.00g N d=0.2 103 100q0.000.250.500.751.00N d=1.0 103 100q0.000.250.500.751.00N d=2.0 k=0 k=1 k=2 k=3Figure D.1: Relation between ¯gandq. Full lines are predictions using eq. (D.16), markers show simulations for different fractions of N/d, and varying values of ˆαbetween 2−9,29. Colors and marker styles distinguish and different spectra of the true covariance matrix Σ, namely λi=i−k∀i= 1,..., d. D.4 Consistency checks To validate our result, we perform a consistency check, using on prior knowledge on the Stieltjes transform g, defined by g(z) = lim d→∞1 dTr1 zId−Σ1 2WΣ1 2 where Wis a Wishart with parameter qS=d N. It is known that in this case, gfulfills the following self-consistency equation: g(z) =Z dρΣ(λ)1 z−λ\u0000 1−d N+d Nzg(z)\u0001 (D.12) where ρΣ(λ)is the spectral density of Σ,ρΣ(λ) =1 dP iδ(λ−λi)in our case. Defining ¯g=−1 ˆαg\u0012 −1 ˆα\u0013 = lim d→∞1 dTr1 Id+ ˆαΣ1 2WΣ1 2 this variable then fulfills the self-consistency equation. ¯g=1 dX i1 1 + ˆαλi\u0000 1−d N+d N¯g\u0001 (D.13) We will now relate the self-consistency relation for gto the self-consistency equation for q. Observe that, from the replica calculation, it follows that ¯g=X id dJif(J) J=0=1 dX i1 1 +λi\u0012 1 ˆαd Nq+1\u0013=:s(q) Using the replica self-consistency relation for q, forJ= 0,eq. (D.11), we find s(q):=ˆα\u0000d N−1\u0001 q+ 1 ˆαd Nq+ 1, (D.14) Furthermore, it follows from the definition eq. (D.14), that 1−d N+d Ns(q) =1 ˆαd Nq+ 1 which, inserted into the definition of s, yields a self-consistency equation for s(q), s(q) =1 dX i1 1 + ˆαλi\u0000 1−d N+d Ns(q)\u0001 (D.15) 22 which is identical to the self-consistency equation for ¯g, eq. (D.13), meaning that the replica calcu- lation produces a variant of the known self-consistency relation eq. (D.12) for the Stieltjes trans- form. Furthermore the replica calculation yields a relation between q=1 dD Tr\u0010 Σ1 1+ˆαΣ0\u0011E Σ0and ¯g=1 dD Tr\u0010 1 1+ˆαΣ0\u0011E Σ0which is independent of the spectrum of Σ, namely ¯g=s(q) (D.16) We test this relation in fig. D.1, finding excellent agreement with simulations. D.5 Squared difference on test examples We compute the generalization measure for fixed t: ⟨ \u0000 W∗ t−Woracle t\u0001 xt 2⟩xt,Σ0=1−¯αt (1−¯αt+γt)\u0012ψ1,1+ψ1,2 (1−¯αt+γt)−2ψ2+ψ3\u0013 where we defined ψt 1,1= Tr\u00141−¯αt (Id+ ˆαtΣ0)2\u0015 ψt 1,2= Tr\u0014¯αt (Id+ ˆαtΣ0)2Σ\u0015 ψt 2= Tr\u00141 (Id+ ˆαtΣ0)\u0015 (D.17) We first compute ψ2, using that ψ2=−2X id dJif(J) J=0,g=1 We now find∂f(J) ∂q= 0, henced f dJi=∂f(J) ∂Ji, which is equal to d f dJi=−1 2dg(λi) 1 +Jig(λi) +λi\u0012 N d1 q+N dˆα\u0013 Where qis found, e.g. by solving eq. (D.13). Forψ1,1andψ1,2, we first note that for precision matrix A, and diagonal J,Λ, we have that d2 dJidJjlnZdη √ 2πdexp\u0012 −1 2ηT[A+Jg(Λ)]η\u0013 J=0=f(Λ)iif(Λ)jj 2\u0010 A−1 ij\u00112 Where we used Wick’s theorem to evaluate the Gaussian moments. For the specific functions of interest eq. (D.17), we find ψ1,1= 2X ij\u0012d2 dJidJjf(J)\u0013 J=0,g(x)=1ψ1,2= 2X ij\u0012d2 dJidJjf(J)\u0013 J=0,g(x)=√x the only difference between the two being the function g. Hence, we compute the second derivatives d2f dJidJj J=0=δij1 2dg2(λi)\u0012 1 +λi\u0012 N d1 q+N dˆα\u0013\u00132−N 2d2g(λi)λi\u0000 q+N dˆα+N dλi\u00012dq dJj J=0 23 Using the self-consistency equation, we find dq dJi J=0=−\u0000 q+N dˆα\u00012 d\u0000 1−N dR2(q)\u0001λig(λi) \u0000 q+N dˆα+N dλi\u00012 where we defined Rk(q) =1 dX jλk j\u0000 q+N dˆα+N dλj\u00012. Putting it all together, for the specific functions of interest, we find ψ1,1=\u0012 q+N dˆα\u00132\" R0+N dR1(q)2 \u0000 1−N dR2(q)\u0001# ψ1,2=\u0012 q+N dˆα\u00132\" R1+N dR3 2(q)2 \u0000 1−N dR2(q)\u0001# and ψ2=1 dX i1 1 +λi\u0012 N d1 q+N dˆα\u0013. D.6 Residual and test loss at finite N To compute the residual loss, we employ the following identity: d f dˆαt J=0,g=1=−1 2NdNX β=1*X ij\u0010 Σ1 2xβ\u0011 i\u00121 1 + ˆαΣ0\u0013 ij\u0010 Σ1 2xβ\u0011 j+ =−1 2d Tr Σ 01 Id+ ˆαΣ0 Inserting this into the equation for the residual loss, eq. (9), which yields R=−2 TX t¯αt 1−¯αt+γtd f dˆαt J=0,g=1+γt 1−¯αt+γtψ2 =1 TX t¯αt 1−¯αt+γtq dˆαt Nq+ 1+γt 1−¯αt+γt1 dX i1 1 +λi\u0012 N d1 q+N dˆαt\u0013 (D.18) Second, we find that the test loss simplifies to Ltest= 1 +1 TX t1−¯αt (1−¯αt+γt)\u0012ψ1,1+ψ1,2 (1−¯αt+γt)−2ψ2\u0013 which contains only functions which we have already computed in appendix D.5. D.7 Kullback-Leibler divergence We compare ρ=N(µ,Σ)toρN=N(µ0,Σ0+γId)The DKL between two Gaussians is given by eq. (11). We now average this expression over draws of the data set term by term. First, note that Tr Σ−1(⟨Σ0⟩+γId)−d=γTr Σ−1(D.19) 24 Second, we compute (µ−µ0)TΣ−1(µ−µ0) =1 N2dX i,j,k,l =1NX β1,β2=1D xβ1 ixβ2 jE\u0010 Σ1 2+√cId\u0011 kiΣ−1 kl\u0010 Σ1 2+√cId\u0011 lj =1 N2dX i,j=1NX β1,β2=1δβ1β2δij\u0012 1 + 2√cΣ−1 2 ij+cΣ−1 ij\u0013 =d+ 2√cTr Σ−1 2+cTr Σ−1 N withxβ∼ N (0,Id). Finally, we have that when γ=1 ˆα −1 2⟨ln|Σ0+γId|⟩=d f(J= 0)−d 2lnγ=d f(J= 0) +d 2ln ˆα (D.20) we have that d f(0) +d 2ln ˆα=N 2dq q+N dˆα−\" 1 2dX iln 1 ˆα+λi N d1 qˆα+N d! # −N 2dln\u0012dˆα Nq+ 1\u0013 such that all in all, the DKL simplifies to eq. (15). D.8 qatN≫d We now seek an approximation for qin the regime N≫d. To do so, we first examine ¯g= 1 dD Tr\u0010 1 1+ˆαΣ0\u0011E Σ0, which we found to relate to qvia the relation D.16. We now additionally as- sume that ˆαis much larger than\u0000 λ0 min\u0001−1, where λ0 minis the smallest eigenvalue in Σ0. This assumption is only valid for at least N≥das otherwise Σ0has zero eigenvalues. Then it follows that ¯gis of order ˆα−1. We now invert the relation between qand¯g, finding that q=1 ˆαd N 1 1−d N+d N¯g−1! ≈1 ˆαd N 1 1−d N−1! (D.21) which is independent of Σ. Inserting this into eq. (15), we find DKL(ρN|ρ) d=1 +N dln\u0000 1−d N\u0001 + ln\u0000 1−d N\u0001 +N d2 2+O\u0010√ ˆα−1\u0011, (D.22) which, for N≫d, scales asd 4N. 25",
  "text_length": 62189
}