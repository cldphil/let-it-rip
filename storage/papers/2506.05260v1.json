{
  "id": "http://arxiv.org/abs/2506.05260v1",
  "title": "LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs",
  "summary": "Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs.",
  "authors": [
    "Xiaodong Wang",
    "Jinfa Huang",
    "Li Yuan",
    "Peixi Peng"
  ],
  "published": "2025-06-05T17:21:16Z",
  "updated": "2025-06-05T17:21:16Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05260v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05260v1  [cs.CV]  5 Jun 2025LEANPO: Lean Preference Optimization for\nLikelihood Alignment in Video-LLMs\nXiaodong Wang1,2Jinfa Huang1Li Yuan1,2Peixi Peng1,2∗\n1Peking University2Peng Cheng Laboratory\n{wangxiaodong21s@stu., pxpeng}@pku.edu.cn\nCode: https://github.com/Wang-Xiaodong1899/LeanPO\nAbstract\nMost Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO [ 1], to optimize the reward margin between a winning re-\nsponse ( yw) and a losing response ( yl). However, the likelihood displacement\nobserved in DPO indicates that both logπθ(yw|x)andlogπθ(yl|x)often\ndecrease during training, inadvertently boosting the probabilities of non-target\nresponses. In this paper, we systematically revisit this phenomenon from LLMs to\nVideo-LLMs, showing that it intensifies when dealing with the redundant complex-\nity of video content. To alleviate the impact of this phenomenon, we propose Lean\nPreference Optimization (LEANPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to the\npolicy model. A key component of LEANPOis the reward-trustworthiness cor-\nrelated self-generated preference data pipeline, which carefully infuses relevant\nprior knowledge into the model while continuously refining the preference data via\nself-reflection. This allows the policy model to obtain high-quality paired data and\naccurately estimate the newly defined reward, thus mitigating the unintended drop.\nIn addition, we introduce a dynamic label smoothing strategy that mitigates the\nimpact of noise in responses from diverse video content, preventing the model from\noverfitting to spurious details. Extensive experiments demonstrate that LEANPO\nsignificantly enhances the performance of state-of-the-art Video-LLMs, consis-\ntently boosting baselines of varying capacities with minimal additional training\noverhead. Moreover, LEANPOoffers a simple yet effective solution for align-\ning Video-LLM preferences with human trustworthiness, paving the way toward\nreliable and efficient Video-LLMs.\n1 Introduction\nRecent advanced Multi-modal Large Language Models (MLLMs) are built on powerful Large\nLanguage Models (LLMs) with visual encoders, capable of multi-modal understanding, reasoning,\nand interaction [ 2,3,4,5,6,7,8]. These models show remarkable improvement in images, videos, and\nother multi-modality tasks, continuously boosting the open-source MLLMs’ development. However,\nsince video content is more complex and diverse than images, Video-LLMs’ responses may not be\nproperly grounded in the video, which makes the development of Video-LLMs challenging.\nDue to the learned bias in Video-LLMs’ large-scale pre-training and supervised fine-tuning (SFT),\nVideo-LLMs often face misalignment and hallucinations when they are reasoning information from\nvideos, which affects the trustworthiness in many practical applications [ 10]. Inspired by LLMs\nalignment works, most Video-LLMs alignment works adopt Direct Preference Optimization (DPO) [ 1]\nto align Video-LLMs with human values or intentions [ 11,12,13,14]. DPO involves using human\nor AI’s preference feedback for the model’s responses, to collect preference pairs consisting of the\n∗Corresponding author.\nPreprint.\n--- Page 2 ---\nLLM alignment\nVideo -LLM alignment\nDPO\n(a) LLM alignment vs. Video -LLM alignment (b) Video -LLM DPO vs. Video -LLM LEANPOLEANPOFigure 1: (a). Comparison between a LLM alignment based on Llama3-8B-Instruct [ 9] and a\nVideo-LLM alignment based on LLaV A-NeXT-Video [ 6] using DPO algorithm. (b). Comparison\nof the implicit reward between DPO and our proposed LEANPOon a Video-LLM (LLaV A-NeXT-\nVideo-7B). The proposed LEANPOis immune to the likelihood displacement, making the reward in\npreference alignment training more stable and promoting efficient alignment.\nwinning response ywand the losing response yl, and the loss is to maximize the margin between the\nreward of the winning and the losing responses for the policy model. However, both the likelihood of\nwinning response logπθ(yw|x)and the losing response logπθ(yl|x)often decrease simultaneously\nduring the DPO training. This phenomenon is widely known as the likelihood displacement problem\nand has been discussed in many prior works on LLM alignment [ 15,16,17,18]. This causes an\nunexpected increase in the probabilities of non-target responses. This phenomenon is attributed\nto various factors, such as model capacity [ 16], multi-token interference [ 19] or the sub-optimal\ntraining [ 18]. However, likelihood displacement phenomenon has not been explored for Video-LLMs.\nIn this paper, we systematically revisit likelihood displacement phenomenon from LLMs to Video-\nLLMs. First, we compare the likelihood curves of winning and losing responses between an LLM\nalignment based on Llama3-8B-Instruct [ 9] and a Video-LLM alignment based on LLaV A-NeXT-\nVideo-7B [ 6], both trained using DPO algorithm, as shown in Fig. 1. (a). From LLMs to Video-LLMs,\nthe likelihood of winning and losing responses drops more dramatically. Next, considering the reward\ndefinition in DPO, where the reward is given by log(πθ(y|x)/πsft(y|x)). We observe the\ncorresponding implicit reward curve for Video-LLM DPO alignment in the upper part of Fig. 1. (b).\nDue to the more severe likelihood displacement phenomenon, the rewards for both responses also\ndecline sharply. The winning reward should have remained the same or increased, but it actually\ndecreased. This interplay between likelihood and reward affects the training dynamics, ultimately\nmaking the optimization of Video-LLMs inefficient. As previously discussed for LLMs [ 16,19], the\nexacerbation of this phenomenon in Video-LLMs during the DPO process may stem from the higher\nmodel capacity required for video understanding.\nRewards are susceptible to the phenomenon of decreasing likelihood. Our intuition is that during\ntraining, the likelihood of a response to the policy model drops rapidly, which is too far from the\nlikelihood of the response in the SFT model, making the ratio-type reward used by DPO unstable.\nThus, to mitigate the negative effect of the likelihood displacement phenomenon, in this paper, we\nreformulate the implicit reward as the average likelihood of the response generated by the policy\nmodel, and such a definition removes the dependence on reference models. The reward is defined\nas1\n|y|logπθ(y), and the reward curves of winning responses become more stable in the training, as\nshown in the bottom of Fig. 1. (b). This simple reformulation makes the model optimization objective\nbecome maximizing the likelihood margin. So what kind of data is more suitable for maximizing\nthe likelihood margin? Previous methods [ 11,12,13] select pair data from multiple responses of the\nmodel itself, but the difference in the likelihood of such pair data is not obvious, which may make\nthe optimization inefficient. To this end, we propose the Lean Preference Optimization ( LEANPO)\nframework, and a key component is a reward-trustworthiness correlated self-generated preference\n2\n--- Page 3 ---\nVideo Large Language Model\nVideo Large Language Model\nGT answer : Behind the two men, there is a wall featuring rectangular \npatterns and an old -fashioned radio to the right side on a shelf. \nWinning : In the video , behind the two men, there is a wall with a \ndecorative panel and an old -fashioned radio to the right side on a shelf.  \nThe background also includes some pipes that are part of the room's \nheating system.  The room appears to be a vintage setting, which \nsuggests this could be from an older television show or movie.\nLosing : In the video , behind the two men, there is a window with \nmetal bars.  The bars are horizontal and appear to be part of a balcony \nor railings outside the window.  The barrier suggests this could be a \nview of a rooftop or a similar outdoor area with an outside structure.\nPrior\n__  ___ ____  ___ \n_____ ___ _____ __\n__  __  _ _  ___  _\n__ _ _ _ _ ________(1) Self -generated Preference Data Pipeline\n(2) Efficient Preference Optimization \nInitial response\nUser query : What can be seen \nbehind the two men?Reflection\nLikelihood Reward\nDynamic\nPreference\nLearningFigure 2: Overview of the LEANPO(Lean Preference Optimization) framework. (1) Given an input\nvideo, the system prompt, and the user query, a novel method that injects the prior (the ground-truth\nanswer with high trustworthiness) into the Video-LLM to generate the winning response, which\ncaptures the trustworthiness from the standard answer, and an losing response is generated by\naugmented video without prior injection. (2) Paired responses are assigned rewards by likelihoods,\nand we optimize the model by maximizing the reward discrepancy with dynamic preference labels.\ndata pipeline, which injects relevant prior knowledge into the model while continuously refining\nthe training data via self-reflection. This allows the policy model to estimate response likelihoods\nprecisely. By ensuring that the collected responses maintain a consistent level of reward (likelihood)\nand trustworthiness, our approach fosters more reliable and well-calibrated preference learning.\nSpecifically, given public video-text training data (e.g., Video-QA, Video-Caption), an intuitive idea is\nto feed the ground-truth answers to the Video-LLMs as hints, giving the model more trustworthiness\nin its responses. We feed the ground-truth answer as the hint to the model along with the video to\nprompt the model to make a response first and then improve the response through self-reflection, to\nobtain a preferred (winning) response. Further, to expose the differences in trustworthiness between\nresponses, when generating the inferior (losing) responses, we apply video augmentation to the\noriginal videos, making the responses have some randomness and fuzziness. Besides, since the\nvideo contents are always diverse and contain more task-independent information, a dynamic label\nsmoothing method is introduced to mitigate the impact of potential noise. As shown at the bottom\nof Fig. 1. (b), LEANPOmakes reward trend in preference learning more stable, and the reward\nmargin between winning and losing responses exists at the beginning and continues to expand, thus\nprompting the efficient alignment of Video-LLMs. Overall, our contributions are as follows:\n• We revisit the likelihood displacement phenomenon in Video-LLMs, and propose a simple\nyet effective LEANPOthat makes the rewards learning more stable and expands the margin\nbetween winning and losing responses;\n•We reformulate the reward of Video-LLMs as the average likelihood to mitigate likelihood\ndisplacement, and propose a reward-trustworthiness-correlated preference data pipeline that\nsynergizes well with this new formulation to achieve efficient alignment;\n•Extensive experiments demonstrate the effectiveness of LEANPOon six challenging video\nbenchmarks. Specifically, implementing our method with LLaV A-NeXT-Video-7B yields a\nsignificant improvement of 10.8% on NeXT-QA and 12.4% on Video-MME.\n3\n--- Page 4 ---\n2 Related Work\n2.1 Likelihood Displacement Phenomenon\nPreference alignment methods like DPO [ 1] optimize reward margins between chosen ( yw) and\nrejected ( yl) responses. However, studies show both logπθ(yw|x)andlogπθ(yl|x)often decrease\nduring training, which a phenomenon termed likelihood displacement [17]. This contradicts the\nexpected divergence between preferred and dispreferred outputs, while unintentionally increasing\nprobabilities for non-target responses. Theoretical analyses attribute this behavior to three primary\nfactors: (1) Model capacity bottlenecks that restrict simultaneous optimization of preference margins\nand likelihood preservation [ 20]; (2) Multi-token interference from conflicting gradient signals across\ndiverse training samples [ 19]; and (3) Suboptimal initialization inherited from supervised fine-tuning\n(SFT) phases [ 18]. Critically, likelihood displacement not only diminishes the absolute probabilities\nof target responses but also inadvertently elevates the likelihood of non-target responses , which are\nunrelated to the preference pairs—thereby degrading generalization performance [ 21,22]. While\nrecent work in LLMs proposes dataset filtering [ 17] or regularization techniques [ 22] to mitigate this\nissue, these solutions compromise either training efficiency or the simplicity that defines DPO-based\nmethods. We argue that video data with temporal redundancies amplify likelihood displacement\nthrough preference alignment strategy in Video-LLMs. This unaddressed challenge likely hinders\npreference alignment efficiency in Video-LLMs, demanding an urgent investigation.\n2.2 Preference Optimization for LLMs\nLarge Language Models (LLMs) are typically pre-trained in an unsupervised manner on extensive\ntextual corpora, followed by supervised fine-tuning (SFT) on high-quality instruction datasets to\nacquire task-specific capabilities [ 23,24,25,26]. While SFT enhances alignment with human prefer-\nences, it suffers from high computational costs and potential propagation of undesirable behaviors\nfrom the training data. Reinforcement learning (RL)-based preference optimization methods, such as\nRLHF [ 27] and RLAIF [ 28], address these limitations by aligning models with human preferences\nthrough feedback signals, significantly improving conversational and coding abilities. Despite their\neffectiveness, RL-based approaches require complex pipelines involving multiple LLMs and substan-\ntial computational resources. DPO [ 1] pioneers a reward-model-free framework by reformulating\nRLHF objectives into a binary cross-entropy loss. Subsequent advancements have introduced more\nstreamlined preference optimization approaches: PRO [ 29] and LiPO [ 30] leverage ranked response\nlists rather than pairwise comparisons, while ORPO [ 31] and SimPO [ 32] eliminate reference model\ndependencies, aligning optimization with generation dynamics. KTO [ 33] operates without pairwise\npreference data by leveraging absolute quality judgments. β-DPO [ 34] introduces a dynamic βfor all\nlikelihoods into DPO. In this paper, we treat the likelihood of winning and losing differently and use\nlabel-smoothing to achieve dynamic preference alignment. In general, the role of different algorithms\nor data in Video-LLMs has not been fully explored.\n2.3 Preference Optimization for Multimodal LLMs\nMultimodal LLMs (MLLMs) face exacerbated alignment challenges, as misalignment often manifests\nas visual hallucinations, responses ungrounded in input images or videos. Recent work adapts LLM\npreference optimization techniques to MLLMs through direct architectural inheritance, exemplified\nby RLHF-V [ 35] and RLAIF-V [ 36]. Current approaches for image-based MLLMs focus on two\nstrategies: (1) Enhanced preference data curation through diverse response collection (Silkie [ 37]),\nhallucination-aware annotations (RLHF-V [ 35]), or hybrid scoring (CSR [ 38]), and (2) Adversarial\ntraining via image distortion (POVID [ 39], BPO [ 40]) or augmentation-based negative sampling\n(SeVa [ 41], STIC [ 42], V-DPO [ 43]). While effective for hallucination reduction, these methods\nshow limited impact on general capability enhancement [ 44]. Video-based MLLMs present unique\nchallenges due to temporal dynamics and computational constraints. Initial efforts like VLM-\nRLAIF [ 11] employ Proximal Policy Optimization (PPO) with self-evaluated rewards, while LLaV A-\nHound [ 13] leverages GPT-4V [ 45] for automated sentence-level scoring. Current video alignment\nmethods [ 12,14] predominantly adopt DPO, but suffer from misalignment between training objectives\n(pairwise preference ranking) and inference metrics (absolute generation quality). Our work addresses\nthis gap by reformulating rewards of Video-LLMs as normalized likelihood of policy models,\nsynergizing with self-generated preference data for efficient temporal alignment.\n4\n--- Page 5 ---\n3 Lean Preference Optimization\nMotivation. In Fig. 1. (b), the upper part illustrates the likelihood displacement phenomenon in\nVideo-LLM alignment, where the rewards for both winning and losing responses decline, uninten-\ntionally elevating the rewards of non-target responses. As the policy model’s likelihood for given\nresponses continues to drop, the gap with the SFT model’s likelihood widens. To mitigate this issue,\nwe reformulate the reward as the average likelihood of the response under the policy model. The data\ndistribution determines the likelihood distribution. To explore the likelihood of different responses,\nwe analyze LLaV A-NeXT-Video [ 6] using the LLaV A-Hound [ 13] dataset. As shown in Fig. 3,\nwe plot reward (average likelihood) curves for ground-truth (GT), direct inference, hallucination\n(generated with misleading text [ 42]), and Hound’s original winning and losing responses. Despite\nhigh trustworthiness of GT responses, they receive the lowest rewards due to low model likelihood.\nSince preference training aims to enlarge the reward margin between winning and losing, we propose\nselecting training pairs by considering both reward and trustworthiness. Concretely, we guide the\nmodel to generate a high-reward, GT-informed response (blue arrow), and use video augmentation to\nproduce a slightly weaker one (red arrow), and details of the pipeline are in Fig. 2.\n3.1 Reward-Trustworthiness Correlated Self-generated Preference Data Pipeline\nFigure 3: Implicit rewards for different responses.Preference learning needs feedback collected in\nthe form of comparison pairs, where each pair\nincludes a winning response ywand a losing re-\nsponse ylto the same input including the video\nVand prompt q. The widely used method to\ncollect paired data in LLMs is randomly sam-\npling several candidates from LLMs with a large\ntemperature, a high top-p parameter, or different\nrandom seeds. Recently, some work [ 36,13]\non multi-modal LLMs’ preference optimization\nalso utilizes the same strategy. Random sampling can collect numerous responses from the model\nitself, but a key problem lies in how to label the winning and losing responses from candidates. So,\nsome work use human feedbacks [ 35], GPT judgment [ 13], open-source proprietary models [ 36],\nor attention values inside models [ 46]. However, these methods are expensive or inefficient. In\nthe meantime, in the video understanding scenario, only random sampling makes it hard to expose\nthe genuine difference between responses. Also, these methods only consider the trustworthiness,\nso the selected winning responses may have low likelihood, resulting in poor training efficiency.\nTable 1: Prompt for winning response generation.\nPrompt for Winning Response Generation\nQuestion: A chat between a curious user and an artificial\nintelligence assistant. The assistant gives helpful, ...\nUSER: Here are some hints: {GT answer: a}\nPlease respond based on the given hints and video content.\n{VIDEO_TOKEN}\n{query: q}\nASSISTANT:\nResponse: {Initial Response: yinit}\n// Video-LLM Reflection\nUSER: Your previous reply to me was: {Initial Response:\nyinit}. This response can continue to be improved.\nNow, please align your response with the information below:\n{GT answer: a}\nYou need to reflect the given information as best you can,\noptimize your response, and enrich your answer. I’ll ask you\nthe question again:\n{VIDEO_TOKEN}\n{query: q}\nASSISTANT:\nResponse: {winning response: yw}To address these concerns, we propose a\nreward-trustworthiness-correlated prin-\nciple to generate preference data,\nwhereby the responses should have the\nsame level of reward (likelihood) and\ntrustworthiness. We leverage prior\nknowledge and a reflection method to\nenhance the accuracy of winning re-\nsponses’ information, and also try to\nincrease the difference between paired\nresponses. Since there is a large amount\nof video-text paired data in previous\nwork, such as Video-LLM pre-training\nand fine-tuning [ 5,7,13], we can di-\nrectly use the annotation data, including\nvideo caption or video QA data as raw\nmaterial to construct preference data. A\nvideo-text instance can be denoted as\n(V, q, a ), where Vdenotes a video, qde-\nnotes a query, and adenotes the ground-\ntruth answer.\n5\n--- Page 6 ---\nWe then introduce our data generation pipeline. To pursue a winning response with high likelihood\nand trustworthiness, we believe that important factors include the accuracy of the response and\nwhether it follows the output distribution of the model. Thus, we propose a novel prior injection\nmethod, that is, we concatenate the answer awith query qinto a simple formatted prompt as shown\nin the upper part of Tab. 1, which is fed into the Video-LLM with the video V:\nyinit=πsft(V,[a, q]), (1)\nThis simple operation prompts the Video-LLM to have prior knowledge to respond to the question.\nThen, to avoid Video-LLM misunderstanding the information of answers or directly copy-pasting\nthem, we encourage the model to reflect and correct its initial response yinit, which better ensures\nthe rationality and information content of final output yw. The detailed prompts are shown in Table 1.\nAfter the prior injection and reflection, the reward of a winning response ywwill be raised effectively,\nwhere the reward is defined as the average log likelihood as follows:\nr(V, q, y w) =β\n|yw||yw|X\ni=1logπsft(yw,i|[V, q], yw,<i), (2)\nwhere πsftis the SFT model, and βis a constant to control the scaling of the reward.\nAs for generating losing responses ylwith low level reward and trustworthiness. We apply some\naugmentations to the original videos Vand prompt the Video-LLM to respond to the same query.\nDue to the random and noise augmentation, the losing response ylwill have lower trustworthiness,\nand it will slightly deviate from the output distribution produced by the model when it sees normal\nvideos. The reward of losing ylis defined as follows:\nr(V, q, y l) =β\n|yl||yl|X\ni=1logπsft(yl,i|[V, q], yl,<i). (3)\nIn this way, our pipeline using existing video-text data, can prompt any Video-LLM itself to generate\npaired preference data, and there is an obvious difference between winning and losing responses,\nincluding reward and trustworthiness. For almost any triplet (V, yw, yl), it satisfies the r(V, q, y w)>\nr(V, q, y l). The reward curve of our generated paired data can be seen in Fig. 3.\n3.2 Dynamic Preference Learning\nThe data pairs generated by our data generation framework have significant differences, including in\nreward and trustworthiness. Thus, maximizing the reward discrepancy is easier than maximizing the\ndifference between comparable rewards, thus benefiting the stability of optimization. Additionally,\nwe add a reward margin term γ, where γ≥0in our training. The Bradley-Terry objective is defined\nas follows:\np(yw≻yl|[V, q]) =σ(r(V, q, y w)−r(V, q, y l)−γ) (4)\nDue to the diversity of videos and the influence of some noise data, winning responses may sometimes\nbe of worse quality than losing responses. The intuitive result is that the likelihood of the winning\nresponse is lower than the likelihood of the losing response, which may make it difficult to directly\noptimize the objective of Eq. 4. To alleviate the problem, we introduce a dynamic label smoothing\nmethod to correct the potential noise, where the pseudo label zqis:\nzq=(\n1,if\u0010\nr(V, q, y w)−r(V, q, y l)\u0011\n> d,\n0,else.(5)\nwhere dis a hyper-parameter. Using pseudo labels, we define the dynamic preference learning\nobjective as below:\n˜p(yw≻yl) = (1 −zq·α)·p(yw≻yl) +zq·α·p(yl≻yw), (6)\nwhere p(yw≻yl|[V, q])is abbreviated to p(yw≻yl), and αdenotes the label smoothing factor.\nFinally, using the self-generated data D, we define the final learning objective of our framework:\nLLEANPO(πθ) =−E(V,q,y w,yl)∼D\u0002\n˜p(yw≻yl)\u0003\n, (7)\nBy optimizing the LEANPOloss on self-collected preference dataset, the model extracts richer\ninformation from input videos to enhance the trustworthiness, thus better aligning with winning\nresponses. Consequently, Video-LLM can achieve alignment optimization more efficiently.\n6\n--- Page 7 ---\nTable 2: Results on LongVB [ 48], MLVU [ 49], NeXT-QA [ 50], and Video-MME [ 47] compared\nwith state-of-the-art models. The Video-MME results are in the format “w.o subs\".\nModel Size LongVB MLVU NeXT-QAVideo-MME\n(val) (M-avg) (mc) Short Medium Long Overall\nVideo-LLaV A [53] 7B 39.1 47.3 - 45.3 38.0 36.2 39.9\nQwen-VL-Max [54] - - 42.2 - 55.8 49.2 48.9 51.3\nShareGPT4Video [55] 8B 39.7 46.4 - 48.3 36.3 35.0 39.9\nInternVL-Chat-V1.5 [56] 20B 51.2 50.4 - 50.7 60.2 46.4 45.6\nVideoChat2 [57] 7B 39.3 47.9 - 48.3 37.0 33.2 39.5\nLongLLaV A [58] 7B - 56.3 - 61.9 51.4 45.4 52.9\nVideo-CCAM [59] 14B - 63.1 - 62.2 50.6 46.7 53.2\nLongV A [60] 7B 51.3 58.8 68.3 61.1 50.4 46.2 52.6\nLLaV A-Video-TPO [14] 7B 59.0 62.9 77.6 71.3 56.9 49.0 59.1\nLLaV A-NeXT-Video [6] 7B 40.1 43.4 53.9 44.0 38.0 34.4 38.8\nLLaV A-NeXT-Video-L EANPO (ours )7B 44.0 45.1 59.7 51.4 43.0 36.6 43.6\n∆(LEANPO) - 10.0% ↑ 3.9% ↑ 10.8% ↑ 16.8% ↑ 13.2% ↑ 6.4% ↑ 12.4% ↑\nLLaV A-Video [8] 7B 58.5 62.6 79.4 71.3 57.4 49.6 59.4\nLLaV A-Video-L EANPO (ours ) 7B 58.9 63.3 81.2 71.6 57.9 50.2 59.9\n∆(LEANPO) - 0.7% ↑ 1.1% ↑ 2.3% ↑ 0.4% ↑ 0.9% ↑ 1.2% ↑ 0.8% ↑\n4 Experiment\n4.1 Setup\nImplementation Details To verify the effectiveness of our method, we utilize two video-LLMs, in-\ncluding LLaV A-NeXT-Video-7B [ 6] and LLava-Video-7B [ 8], which have different LLM backbones.\nWe freeze the visual encoder and multi-modal projection and fine-tune all the parameters of LLM\nbackbones. All experiments are completed on A100 GPUs, and we utilize 16 frames for full-training\nVideo-LLMs due to the limited GPU memory. More details are shown in Appendix A.\nData & Benchmarks The annotation data are sampled from the video QA or caption data from [ 13],\nincluding 17k data. For each SFT model, we utilize all 17k triplets (video, query, answer) to generate\npaired responses. For the study on preference data size, we also randomly sample annotation data\nfrom LLaV A-Video-178k [ 8]. For all-round evaluation, we consider 4 multi-choice QA benchmarks:\nVideo-MME [ 47], LongVideoBench (LongVB) [ 48], MLVU [ 49], NeXT-QA [ 50], and 2 open-ended\nQA benchmarks: DREAM-1K [51] for detailed video description and VideoChatGPT [52] for chat.\n4.2 Quantitative Results\nMultiple-choice QA benchmarks Tab. 2 presents the comparison results with current state-of-\nthe-art video-LLMs on 4 multiple-choice QA benchmarks. With the introduction of LEANPO, both\nLLaV A-NeXT-Video and LLaV A-Video significantly outperform their corresponding baselines. It\nbrings significant improvements of 10% on LongVB, 3.9% on MLVU, 10.8% on NeXT-QA, and\n12.4% on Video-MME. Based on the state-of-the-art LLaV A-Video, our method achieves better\nimprovements against TPO [ 61]. Our model outperforms all 7B baselines on MLVU, NeXT-QA, and\nVideo-MME, and achieves a closer score to GPT-4o on MLVU. The results consistently indicate our\nLEANPOpotential to improve the capability on multiple-choice QA.\nOpen-ended QA benchmarks Tab. 3 presents the comparison of detailed video description on\nDREAM-1K. Our method brings improvements of +3.2 and +1.2 on the F1 score for two baseline\nmodels, respectively, and our model outperforms all 7B baselines. Tab. 4 presents the results\ncompared with current state-of-the-art models on VideoChatGPT. CI, DO, CU, TU, and CO refer to\nthe correctness of information, detail orientation, contextual understanding, temporal understanding,\nand consistency, respectively. Our method surpasses DPO on 3 out of 5 tasks, achieves a higher\naverage performance, and significantly outperforms GPT-4V on the consistency (CO) task.\nThe consistent improvements over two baseline models across various multiple-choice and open-\nended QA benchmarks validate the effectiveness of our method and highlight its ability to enhance\nthe comprehensive video understanding capabilities of pretrained Video-LLMs.\n7\n--- Page 8 ---\nTable 3: GPT-based evaluation for video de-\ntailed captioning. Results on DREAM-1K.\nModel F1 Precision Recall\nVideo-LLaV A [53] 20.4 28.1 16.0\nVideoChat2 [62] 26.6 31.0 23.3\nMiniGPT-4Video [63] 24.0 26.1 22.2\nPLLaV A-34B [64] 28.2 38.4 22.3\nLLaV A-OV-7B [7] 31.7 - -\nLLaV A-OV-72B [7] 33.2 - -\nLLaV A-NeXT-Video [6] 23.3 31.1 18.6\nLLaV A-NeXT-Video-L EANPO (ours )26.5 34.6 21.5\nLLaV A-Video [8] 31.7 33.6 29.9\nLLaV A-Video-L EANPO (ours ) 32.9 33.5 32.4Table 4: GPT-based evaluation for video chat.\nResults on VideoChatGPT [52].\nMethods CI DO CU TU CO Avg.\nVideoChat [65] 2.23 2.50 2.53 1.94 2.24 2.29\nVideo-ChatGPT [52] 2.50 2.57 2.69 2.16 2.20 2.42\nVista-LLaMA[66] 2.44 2.64 3.18 2.26 2.31 2.57\nMoiveChat [67] 2.76 2.93 3.01 2.24 2.42 2.67\nChat-UniVi [68] 2.89 2.91 3.46 2.40 2.81 2.89\nVideoChat2 [62] 3.02 2.88 3.51 2.66 2.81 2.98\nPLLaV A [64] 3.21 2.86 3.62 2.33 2.93 2.99\nCAT [69] 3.08 2.95 3.49 2.81 2.89 3.07\nST-LLM [70] 3.23 3.05 3.74 2.93 2.81 3.15\nLLaV A-NeXT-Video [6] 3.39 3.29 3.92 2.60 3.12 3.26\nLLaV A-NeXT-Video-DPO [1] 3.64 3.45 4.17 2.95 4.08 3.66\nLLaV A-NeXT-Video-L EANPO (ours )3.77 3.51 4.14 2.72 4.26 3.68\nQuery: Which object is depicted as a small flying black dot at the start of the video?\nA.Dust mites in the lungs.\nB.Virus.\nC.Protein Receptors.\nD.Black bacteria.\n__________________________________ ________________________________________________ _\nLLaV A -NeXT -Video :  A.\n_________________________________________________________________________ __________\nLLaV A -NeXT -Video -LEANPO(Ours):  B. \nQuery: According to the video, what is the right pose demonstrated in the animation when sitting on the \nbike before starting to ride? \nA.Sitting backwards on the bike with both feet on the handlebars, ready to push off and start riding. \nB.Sitting sideways on the bike with the left foot on the ground and the right foot resting on the pedal. \nC.Sitting on the bike with both feet on the ground, not engaging with the pedals. \nD.Sitting on the bike seat with the right foot on the ground and the left foot resting on the pedal.\n__________________________________ ________________________________________________ _\nLLaV A -NeXT -Video :  A.\n_________________________________________________________________________ __________\nLLaV A -NeXT -Video -LEANPO(Ours):  D. \n(a) Video QA: The correct answer is marked in green.\nQuery: Please provide a detailed description of the video, focusing on the main subjects, their actions, \nand the background scenes.\n___________________________________________________________________________________        \nLLaV A -NeXT -Video :  The video depicts a woman wearing a white robe and a white turban , standing in front \nof a white fence. she is holding a white cloth in her hands and appears to be engaged in some sort of activity . \nthe woman's attire and the white fence suggest that she might be in a religious or ceremonial setting . In the \nbackground, there is a person wearing a red turban, who seems to be observing the woman's actions. the person's red \nturban contrasts with the woman's white attire, drawing attention to them. The video's background is relatively \nsimple, with a clear focus on the woman and the person in the red turban. the white fence and the white cloth in the \nwoman's hands add to the simplicity of the scene, suggesting a calm and peaceful atmosphere. Overall, the video \nseems to capture a moment of quiet contemplation or ritua l, with the woman in the white robe and turban and the \nperson in the red turban as the main subjects. the simplicity of the background and the actions of the subjects create a \nsense of tranquility and focus on the subjects themselves.\n_________________________________________________________________________ __________\nLLaV A -NeXT -Video -LEANPO(Ours):  The video depicts a scene where two individuals are engaged in a \nfood preparation activity . the main subject is a person wearing a white robe and a turban , who is actively \ninvolved in the process. this individual is seen holding a piece of food, possibly a piece of bread or a dough ball, \nand appears to be in the act of serving or distributing the food . the person's attire, which includes a white robe \nand a turban, suggests a cultural or religious context to the scene. In the background, there is another person \nwearing a blue shirt and a face mask, indicating a concern for hygiene and health safety . this person is not the \nmain focus of the video but is present in the scene, possibly observing or assisting in the food preparation . The \nsetting seems to be outdoors, with a fence visible in the background, and there are trees and a clear sky, suggesting a \npleasant day. the presence of the fence and the outdoor setting might imply that this is a casual or informal gathering \nrather than a formal or commercial event. The video's overall tone is calm and unhurried, with the focus on the \nfood preparation and the interaction between the individuals involved . the video's purpose could be to document \na cultural or religious event, a community gathering, or a simple moment of everyday life.\n___________________________________________________________________________________\nReference GT :  a person dressed in white clothing and wearing a white headscarf is distributing food to people on \nthe left. after the people on the left receive the food, they walk away, and someone on the right brings out new food \nto give to the person in white clothing. they continue to hand out food to the new arrivals on the left.\n (b) Video Detail Captioning\nFigure 4: Qualitative comparison for Video Detail Captioning and Video QA. Red indicates irrelevant\nresponses, while blue indicates well-grounded responses.\n4.3 Ablation Study\nComparison of different algorithms and different data In Tab. 5, we compare our method with\nSFT, DPO, SimPO on various settings, using 17k data from [ 13]. When comparing methods using GT\ndata, DPO and SimPO under the GT-as-Winning setting achieve lower scores on Video-MME than\nSFT. In contrast, our framework outperforms DPO and SimPO, achieving the highest scores on Video-\nMME, LongVB, MLVU, and DREAM-1K. Additionally, by leveraging our data generation pipeline,\nboth DPO and SimPO have significant improvements on these benchmarks, further validating the\neffectiveness of our data generation pipeline. In Tab. 8, we test LEANPOusing different responses.\nCompared with responses of GT or other models [ 13], or use hallucination response (using misleading\ntext) [42], LEANPOachieves the best performance using data from our data generation pipeline.\nTable 5: Comparison between other preference learning methods and our proposed LEANPO.\nMethod Data source Win LoseVideo-MMELongVB MLVU DREAM-1K\nShort Medium Long Overall (val) (M-avg) (test)\nBase - - - 44.0 38.0 34.4 38.8 40.1 43.4 23.3\n- SFT Hound GT - 46.2 40.4 34.9 40.5 40.8 43.0 24.8\n- DPO Hound Win Lose 48.7 39.6 35.9 41.4 42.0 43.6 19.0\n- DPO Hound GT Lose 43.9 38.7 33.6 38.7 40.6 42.8 25.8\n- DPO Our pipe Win Lose 49.4 42.8 35.0 42.4 43.0 44.2 26.3\n- SimPO Hound Win Lose 44.9 39.7 34.1 39.6 40.9 45.0 26.1\n- SimPO Hound GT Lose 43.7 38.6 32.0 38.1 43.0 43.8 26.0\n- SimPO Our pipe Win Lose 50.6 42.1 35.0 42.6 43.5 40.8 26.1\n-LEANPO Our pipe Win Lose 51.4 43.0 36.6 43.6 44.0 45.1 26.5\n8\n--- Page 9 ---\nTable 6: Ablation study of LEANPOwith LLaV A-NeXT-Video-7B on various benchmarks.\nMethod Self-gen data Reflect DynamicVideo-MMELongVB MLVU DREAM-1K\nShort Medium Long Overall (val) (M-avg) (test)\nBase - - - 44.0 38.0 34.4 38.8 40.1 43.4 23.3\nDPO - - - 48.7 39.6 35.9 41.4 42.0 43.6 19.0\nSimPO - - - 44.9 39.7 34.1 39.6 40.9 45.0 26.1\nLEANPO✓ 50.6 42.1 35.0 42.6 43.5 40.8 26.1\n✓ ✓ 51.0 42.4 36.3 43.2 43.9 44.5 25.3\n✓ ✓ ✓ 51.4 43.0 36.6 43.6 44.0 45.1 26.5\nTable 7: Label smoothing factor αanalysis.\nModelVideo-MME\nShort Medium Long Overall\nBase 44.0 38.0 34.4 38.8\nLEANPOα=0.5 50.8 42.1 36.0 43.0\nLEANPOα=0.3 50.8 42.7 36.0 43.2\nLEANPOα=0.1 51.4 43.0 36.6 43.6Table 8: Different response effect analysis.\nWin LoseVideo-MME\nShort Medium Long Overall\nHound Win Hound Lose 48.3 40.2 36.0 41.5\nHound GT Hound Lose 49.8 39.9 34.3 41.3\nInference Text-hallu 47.0 39.2 34.8 40.3\nOur Win Our Lose 51.4 43.0 36.6 43.6\nFramework components Tab. 6 illustrates the impact of the data generation pipeline, reflection,\nand dynamic labels. Using our data, LEANPOgets better scores than other baselines on Video-MME\nand LongVB. By incorporating reflection and dynamic techniques, it achieves overall improvements\nrather than enhancing only specific capabilities. Tab. 7 presents the effects the the smoothing factor\nfor dynamic labels, where the most commonly used value of 0.1 has the best effect.\nEffect of preference dataset size In Fig. 5, we examine the scalability of our LEANPO. To empha-\nsize the importance of data quantity, we traverse datasets of different sizes only once during training.\n5k 10k 17k 25k 30k\nDataset Size35.037.540.042.545.047.550.0Acc. on Video-MME (%)\nShort\nMedium\nLong\nFigure 5: Impact of self-generated preference\ndataset size based on LLaV A-NeXT-Video-7B [ 6]\nfor on Video-MME. The present results are in the\nformat “w.o subs\". The model only traverses the\npreference dataset once during training.The intuitive result is that increasing the\ndata generally improves average performance\non Video-MME. However, with larger video\ndatasets, certain specific capabilities may im-\nprove further, as evidenced by higher scores in\nmedium-duration video understanding.\n4.4 Qualitative Results\nThe qualitative comparison results between our\nmethod and baseline on Video QA and detailed\nvideo description are shown in Fig. 4a and\nFig. 4b. In the first example, which requires\nmodels to link the flying black dot in the for-\nmer frames to the virus in the latter frames, our\nmodel successfully comprehends and links the\nrelated content. In the second example, which requires models to recognize the action correctly, the\nbaseline misidentifies continuous actions in the video. In contrast, our model accurately identifies the\nactions and gives accurate responses. Fig. 4b shows an example of detailed video description from\nDREAM-1K. For the original LLaV A-NeXT-Video, it only sees the appearance features of the main\nobject, and the rest of the information means a lot of hallucinations. In contrast, our model not only\nsees the appearance features of the main object and other objects, but also infers a very accurate and\nrelevant description of the video content.\n4.5 Generalization of LenPO on Video-LLMs\nDetails We conduct experiments on the latest Qwen-VL series Video-LLMs to validate the general-\nization ability of our proposed LEANPO. The training video data originates from LLaV A-Hound [ 13].\nIn Table. 9, we present the results of Qwen2-VL-7B [ 71] and Qwen2.5-VL-7B [ 72] using different\npost-training methods. All training and evaluation experiments use 32 video frames. We test these\nmodels on three benchmarks, including Video-MME [ 47], MLVU [ 49], LongVideoBench [ 48]. Video-\nMME has three types of duration (short, medium, long), and focuses more on temporal reasoning\ntasks. MLVU and LongVideoBench focus more on fine-grained details and long video understanding.\n9\n--- Page 10 ---\nTable 9: Qwen-VL Series Video-LLMs performance using our proposed LEANPO.\nMethod Data source Win LoseVideo-MMEMLVU LongVideoBench\nShort Medium Long Overall (val) (test)\nQwen2-VL-7B [71] - - - 68.1 52.3 47.8 56.0 59.8 55.9\n- SFT Hound GT - 67.5 54.7 48.9 57.0 60.0 53.1\n- DPO Hound Win Lose 70.2 54.9 48.4 57.8 59.5 55.2\n-LEANPO Ours Win Lose 70.8 56.4 48.8 58.7 60.3 55.9\nQwen2.5-VL-7B [72] - - - 70.6 56.8 49.2 58.9 61.5 59.0\n- SFT Hound GT - 70.4 57.6 51.1 59.7 60.0 57.1\n- DPO Hound Win Lose 72.3 57.1 49.9 59.8 60.9 59.2\n-LEANPO Ours Win Lose 73.9 59.7 51.5 61.7 61.7 59.1\nTable 10: Generalization results of our method on Image-LLMs. We show a comparison with other\npreference algorithms on LLaV A-v1.5-7B [ 73]. We report the results on 4 comprehensive multimodal\nbenchmarks for reference. The best and second-best results are shown in bold andunderlined\nrespectively.\nMethod #Prompts WildVision LLaV A-W LiveBench L-Wilder MMEPMMECMMB-en MM-Vet MMStar\nLLaV A-v1.5-7B – 24.0 63.4 39.0 53.0 1510.7 348.2 64.0 31.1 33.3\n+ SIMA [74] 17k 17.6 66.1 43.9 52.3 1507.7 379.3 64.9 31.6 34.7\n+ CSR [38] 15k 20.0 71.1 42.6 55.9 1524.2 367.9 65.4 33.9 33.6\n+ RLAIF-V [36] 33.8k 19.2 72.7 44.8 56.4 1362.7 302.9 62.6 26.7 35.4\n+ RLHF [75] 9.4k 19.8 63.7 - 54.5 1508.2 360.2 60.4 31.1 33.0\n+ LLaV A-Critic [44] 9.4k 29.2 73.5 - 57.2 1500.4 350.7 64.1 32.2 34.2\n+LEANPO 9.4k 32.1 71.1 42.6 54.7 1480.6 326.4 64.9 36.7 34.2\nCompared with SFT and DPO [ 1], our proposed LEANPOachieves large improvements over the\nperformance of the two baselines. In Video-MME, which focuses more on temporal reasoning tasks,\nLEANPOhas significant improvements. However, in MLVU and LongVideoBench, which focus\nmore on details and long video understanding, training the above methods with limited frames has no\nobvious improvement. Nevertheless, LEANPOmaintains the performance better than SFT and DPO.\n4.6 Generalization of LeanPO on Image-LLMs\nGeneralization results of our method on Image-LLMs. We show a comparison with other preference\nalgorithms on LLaV A-v1.5-7B [ 73]. We report the results on 9 comprehensive multimodal bench-\nmarks for reference. Compared with other methods, our method can achieve comparable performance\nimprovements with less data. In particular, we achieve the best scores on both WildVision and\nMM-Vet. Compared with the baseline, our method has a significant performance improvement on 7\nbenchmarks, which shows the generalization of our method.\n5 Conclusion\nIn this paper, we present Lean Optimization Alignment ( LEANPO) to creatively address the limitation\nof preference alignment in Video-LLMs. We first revisit the likelihood displacement in Video-LLMs\nalignment. To alleviate the adverse effects, we reformulate rewards and leverage ground-truth\nanswers and combine prior knowledge injection and reflection to optimize model trustworthiness.\nLEANPOprovides a promising alternative for aligning model preferences with human expectations.\nOur method achieves substantial performance gains on six video understanding benchmarks, while\nmaintaining minimal training costs. This work not only enhances the alignment but also offers a\nscalable framework for improving the trustworthiness and effectiveness of Video-LLMs.\nReferences\n[1]Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nNIPS , 36, 2024.\n10\n--- Page 11 ---\n[2]Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In CVPR , pages 14398–14409, 2024.\n[3]Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila:\nOn pre-training for visual language models. In CVPR , pages 26689–26699, 2024.\n[4]Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. In CVPR , pages 26296–26306, 2024.\n[5]Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Improved reasoning, ocr, and world knowledge, January 2024.\n[6]Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu,\nand Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.\n[7]Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\nLi, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326 , 2024.\n[8]Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video\ninstruction tuning with synthetic data. arXiv preprint arXiv:2410.02713 , 2024.\n[9] AI@Meta. Llama 3 model card. 2024.\n[10] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large\nvision language models. In AAAI , volume 38, pages 18135–18143, 2024.\n[11] Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. Tuning large\nmultimodal models for videos using reinforcement learning from AI feedback. In Lun-Wei\nKu, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok,\nThailand, August 11-16, 2024 , pages 923–940. Association for Computational Linguistics,\n2024.\n[12] Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. i-srt:\nAligning large multimodal models for videos by iterative self-retrospective judgment. arXiv\npreprint arXiv:2406.11280 , 2024.\n[13] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu,\nChunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of\nvideo large multimodal models from language model reward. arXiv preprint arXiv:2404.01258 ,\n2024.\n[14] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference\noptimization for long-form video understanding. arXiv preprint arXiv:2501.13919 , 2025.\n[15] Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, and Xiao Li. Dpo-shift: Shifting the\ndistribution of direct preference optimization. arXiv preprint arXiv:2502.07599 , 2025.\n[16] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie,\nStefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage\nsuboptimal, on-policy data. arXiv preprint arXiv:2404.14367 , 2024.\n[17] Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, and Boris Hanin.\nUnintentional unalignment: Likelihood displacement in direct preference optimization. arXiv\npreprint arXiv:2410.08847 , 2024.\n[18] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From rtoq∗: Your language model\nis secretly a q-function. arXiv preprint arXiv:2404.12358 , 2024.\n[19] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.\nSmaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint\narXiv:2402.13228 , 2024.\n11\n--- Page 12 ---\n[20] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie,\nStefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage\nsuboptimal, on-policy data. arXiv preprint arXiv:2404.14367 , 2024.\n[21] Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, and Xiao Li. Dpo-shift: Shifting the\ndistribution of direct preference optimization. arXiv preprint arXiv:2502.07599 , 2025.\n[22] Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet,\nand Zhaoran Wang. Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an\nadversarial regularizer. arXiv preprint arXiv:2405.16436 , 2024.\n[23] OpenAI. Introducing chatgpt. CoRR , 2022.\n[24] OpenAI. GPT-4 technical report. CoRR , 2023.\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n[26] Qwen team. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n[27] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems , 30, 2017.\n[28] Yuntao Bai, , et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073 , 2022.\n[29] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 38, pages 18990–18998, 2024.\n[30] Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao,\nMohammad Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization\nthrough learning-to-rank. arXiv preprint arXiv:2402.01878 , 2024.\n[31] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without\nreference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural\nLanguage Processing , pages 11170–11189, 2024.\n[32] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a\nreference-free reward. arXiv preprint arXiv:2405.14734 , 2024.\n[33] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:\nModel alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 , 2024.\n[34] Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang,\nand Xiangnan He. β-dpo: Direct preference optimization with dynamic β.Advances in Neural\nInformation Processing Systems , 37:129944–129966, 2024.\n[35] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan\nLiu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior\nalignment from fine-grained correctional human feedback. In CVPR , pages 13807–13816, 2024.\n[36] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen\nHe, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai\nfeedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220 , 2024.\n[37] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou\nWang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models.\narXiv preprint arXiv:2312.10665 , 2023.\n[38] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao\nWang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language\nmodels. arXiv preprint arXiv:2405.14622 , 2024.\n12\n--- Page 13 ---\n[39] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modali-\nties in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411 ,\n2024.\n[40] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang.\nStrengthening multimodal large language model with bootstrapped preference optimization. In\nEuropean Conference on Computer Vision , pages 382–398. Springer, 2025.\n[41] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. Self-supervised visual preference\nalignment. arXiv preprint arXiv:2404.10501 , 2024.\n[42] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei\nWang. Enhancing large vision language models with self-training on image comprehension.\narXiv preprint arXiv:2405.19716 , 2024.\n[43] Yuxi Xie, Guanzhen Li, Xiao Xu, and Min-Yen Kan. V-dpo: Mitigating hallucination in\nlarge vision language models via vision-guided direct preference optimization. arXiv preprint\narXiv:2411.02712 , 2024.\n[44] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang,\nand Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint\narXiv:2410.02712 , 2024.\n[45] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_\nCard.pdf , 2023.\n[46] Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He,\nYuanjun Xiong, Dahua Lin, and Jiaqi Wang. Mia-dpo: Multi-image augmented direct preference\noptimization for large vision-language models. arXiv preprint arXiv:2410.17637 , 2024.\n[47] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive\nevaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075 ,\n2024.\n[48] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for\nlong-context interleaved video-language understanding, 2024.\n[49] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang,\nTiejun Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video\nunderstanding. arXiv preprint arXiv:2406.04264 , 2024.\n[50] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-\nanswering to explaining temporal actions. In CVPR , pages 9777–9786, 2021.\n[51] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and\nevaluating large video description models. arXiv preprint arXiv:2407.00634 , 2024.\n[52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models. arXiv preprint\narXiv:2306.05424 , 2023.\n[53] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united\nvisual representation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023.\n[54] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\nlocalization, text reading, and beyond. arXiv preprint arXiv:2308.12966 , 2023.\n[55] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong\nDuan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.\nSharegpt4video: Improving video understanding and generation with better captions. arXiv\npreprint arXiv:2406.04325 , 2024.\n13\n--- Page 14 ---\n[56] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 24185–24198, 2024.\n[57] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,\nand Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 ,\n2023.\n[58] Yin Song and Chen Wu and Eden Duthie. aws-prototyping/long-llava-qwen2-7b, 2024.\n[59] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam:\nEnhancing video-language understanding with causal cross-attention masks for short and long\nvideos. arXiv preprint arXiv:2408.14023 , 2024.\n[60] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue\nWang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision.\narXiv preprint arXiv:2406.16852 , 2024.\n[61] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal alignment networks for long-term\nvideo. In CVPR , pages 2906–2916, 2022.\n[62] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo\nChen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark.\narXiv preprint arXiv:2311.17005 , 2023.\n[63] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding,\nand Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding\nwith interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413 , 2024.\n[64] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava:\nParameter-free llava extension from images to videos for video dense captioning. arXiv preprint\narXiv:2404.16994 , 2024.\n[65] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,\nand Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 ,\n2023.\n[66] Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing\nhallucination in video language models via equal distance to visual tokens. In CVPR , pages\n13151–13160, 2024.\n[67] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu,\nHaozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse\nmemory for long video understanding. In CVPR , pages 18221–18232, 2024.\n[68] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified\nvisual representation empowers large language models with image and video understanding.\narXiv preprint arXiv:2311.08046 , 2023.\n[69] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: enhancing\nmultimodal large language model to answer questions in dynamic audio-visual scenarios. arXiv\npreprint arXiv:2403.04640 , 2024.\n[70] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language\nmodels are effective temporal learners. arXiv preprint arXiv:2404.00308 , 2024.\n[71] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing\nLiu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception\nof the world at any resolution. arXiv preprint arXiv:2409.12191 , 2024.\n[72] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,\n2025.\n14\n--- Page 15 ---\n[73] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485 , 2023.\n[74] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi\nZhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language\nmodality alignment in large vision language models via self-improvement. arXiv preprint\narXiv:2405.15973 , 2024.\n[75] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang\nGan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models\nwith factually augmented rlhf. arXiv preprint arXiv:2309.14525 , 2023.\n15\n--- Page 16 ---\nA Dataset Details 16\nB Visualization of Implicit Reward 16\nC Preference Data Pipeline: DPO vs. LeanPO 16\nD More Case Analysis 17\nA Dataset Details\nFor a comprehensive evaluation, we conduct our experiment on six several popular video under-\nstanding benchmarks, including Video-MME [ 47], LongVideoBench (LongVB) [ 48], MLVU [ 49],\nNeXT-QA [50], DREAM-1K [51] and VideoChatGPT [52].\nVideo-MME is a large-scale benchmark for evaluating multimodal understanding in video-based\nLLMs, covering tasks like question answering, captioning, and spatiotemporal reasoning. It includes\na diverse set of real-world videos with comprehensive annotations for objective evaluation.\nLongVideoBench (LongVB) focuses on testing the ability of the models to handle extended video\ncontent, requiring both fine-grained moment analysis and higher-level summarization. Its annotated\nvideos often span many minutes or even hours, challenging existing context-length limits.\nMulti-Level Video Understanding (MLVU) presents a dataset covering tasks from basic object-\naction recognition to high-level story comprehension. It tests a model’s capacity for reasoning across\nsemantic levels in diverse real-world video clips.\nNeXT-QA (Narrative and Explainable Temporal QA) emphasizes temporal and causal reasoning\nin everyday video scenarios. Questions and annotations target multi-step reasoning over sequences of\nevents rather than mere visual recognition.\nDREAM-1K is an open-ended video QA dataset with around 1,000 clips, encouraging free-form,\ndetailed responses. Its tasks promote rich descriptive answers and deeper inference beyond simple\nfactual recall.\nVideoChatGPT is a multi-turn, conversational framework for discussing and reasoning over video\ncontent. It tests a system’s capacity to maintain context and coherence across extended visual\ndialogues.\nB Visualization of Implicit Reward\nTo verify the self-generated preference data quality from Sec. 3.1, as shown in Fig. 6, we visualize\nthe implicit reward for each video sample with different response inputs. These preferred winning\nresponses have both high trustworthiness and high likelihood, which has higher trustworthiness than\nthose responses without prior, and also has higher likelihood than GT answers because the latter\ndoes not follow the Video-LLM’s output distribution. Although the ground-truth answers are correct\nfor the video queries (and we consider them with the highest trustworthiness), their corresponding\nlikelihood curve is much lower than the other two responses as they fall outside the output distribution\nof Video-LLMs. Consequently, treating ground-truth answers as equally preferable to the winning\nresponses in preference learning proves inefficient, leading to suboptimal experimental results. This\nmotivated us to develop the above framework to collect winning responses that achieve both high\ntrustworthiness and high likelihood.\nC Preference Data Pipeline: DPO vs. LeanPO\nAs shown in Fig 7. (a), most existing methods sample multiple responses from Video-LLM itself,\nand require human or additional strong AI models (such as GPT-4V [ 45]) to label the paired data [ 13,\n12,11], which is expensive and inefficient. To develop a cheaper, effective, and flexible framework\nto build the own preference data of Video-LLMs, we obey the reward-trustworthiness-correlated\nprinciple to construct preference data, as shown in the upper of Fig. 7. (b).\n16\n--- Page 17 ---\nFigure 6: Visualization of average likelihood (implicit reward) for each video sample with different\nresponse inputs to the LLaV A-NeXT-Video-7B model [ 6]. Samples are randomly chosen from our\nself-generated preference data.\n(a) DPO on Video-LLM alignment(b) LEANPOon Video-LLM alignmentVideoVideo-LLMQueryResponse 1Response 1Response N…\n•GT video caption•GT answerWinningresponseLosingresponseVideoVideoQueryQueryVideo-LLMGT answerFirst responseWinningresponseLosingresponseReflectionAugmentation\n(a) DPO on Video-LLM alignment(b) LEANPOon Video-LLM alignmentVideoVideo-LLMQueryResponse 1Response 1Response N…\n•GT video caption•GT answerWinningresponseLosingresponseVideoVideoQueryQueryVideo-LLMGT answerFirst responseWinningresponseLosingresponseReflectionAugmentation\nFigure 7: Comparison of different methods for Video-LLM alignment. We compare the preference\ndata generation pipeline between DPO and our proposed LEANPOon a Video-LLM.\nD More Case Analysis\nWe present more qualitative results for Video Multiple-choice QA and video detailed description in\nFig. 8, Fig. 9, Fig. 10.\n17\n--- Page 18 ---\nQuery: In which river's bank is it possible to see snow?\nA. Lena River.\nB.Yangze River.\nC.Ob-Irtysh River.\nD.SyrDarya River.\"\n__________________________________ ________________________________________________ _\nLLaV A -NeXT -Video :  A.\n_________________________________________________________________________ __________\nLLaV A -NeXT -Video -LEANPO(Ours):  C. \nQuery: How many hours of sleep does this video recommend adults aim to achieve each day, as \ndepicted in the animation?\nA.5-7\nB.7-9\nC.9-11\nD.11-13\n__________________________________ ________________________________________________ _\nLLaV A -NeXT -Video :  A.\n_________________________________________________________________________ __________\nLLaV A -NeXT -Video -LEANPO(Ours):  B. \nFigure 8: More Video QA results compared with baseline.\n18\n--- Page 19 ---\nQuery: Please provide a detailed description of the video, focusing on the main subjects, their actions, \nand the background scenes.\n___________________________________________________________________________________        \nLLaV A -NeXT -Video:  The video depicts a group of people in a dark room, with a focus on a man who is holding \na torch. the man is standing in the center of the room, and he appears to be the main subject of the video. he is \nholding the torch up high, and it is illuminating the surrounding area. the room is dark, and there are no other sources \nof light visible. in the background, there are several other people present. they are standing around the room, and \nsome of them are holding weapons. the weapons appear to be swords, and they are being held in a way that suggests \nthey are ready for combat. the people in the background are dressed in medieval clothing, which suggests that the \nvideo may be set in a historical period. the man with the torch is the only one who is clearly visible, and he is the \nonly one who is actively holding a weapon . the other people in the room are not as clearly visible, and their actions \nare not as distinct. however, their presence and the fact that they are holding weapons suggest that they are part of \nthe same group as the man with the torch. overall, the video appears to be a scene from a historical movie or play, \nwith a focus on the man with the torch and the group of people in medieval clothing . the dark room and the \npresence of weapons suggest that the scene is set in a time of conflict or danger.\n_________________________________________________________________________ _____\nLLaV A -NeXT -Video -LEANPO(Ours):  The video portrays a dynamic scene set in a medieval -style \nenvironment. the main subjects are a group of animated characters, each with distinct costumes and \nexpressions. the characters are engaged in a fierce battle, with some wielding swords and others using fire as a \nweapon . the fire adds a dramatic element to the scene, illuminating the characters and creating a sense of urgency. in \nthe foreground, there are two characters who are the primary focus. one character is holding a sword, poised \nand ready for action, while the other character is holding a torch, which is emitting flames. the flames are \nvisible, suggesting that the torch is being used as a weapon , possibly to ignite the opponents or to create a \ndistraction. in the background, there are additional characters, some of whom are also engaged in the battle. they are \ndressed in medieval armor, which adds to the historical and fantasy atmosphere of the scene. the armor varies in \ndesign, indicating a diverse group of characters. the setting appears to be a wooden structure, possibly a ship or a \nfort, with a wooden floor and a wooden wall in the background . this suggests that the battle is taking place in a \nconfined space, which is a common setting for medieval battles. the wooden elements and the medieval -style \ncostumes contribute to the overall atmosphere of the scene, creating a sense of a bygone era. overall, the video \ncaptures a moment of intense action and drama, with the characters' costumes, expressions, and the use of fire as a \nweapon adding to the narrative and visual appeal of the scene. \n___________________________________________________________________________________\nReference GT :  a character is suspended in midair, making gestures and speaking as the camera pans down to a \ngroup of characters gathered in front of a door, among them one wielding a flaming sword. then, the crowd begins to \nfight, with the one holding the flaming sword swinging it around and knocking a person to the ground.\nFigure 9: More Video detailed description results compared with baseline.\n19\n--- Page 20 ---\nQuery: Please provide a detailed description of the video, focusing on the main subjects, their actions, \nand the background scenes.\n___________________________________________________________________________________        \nLLaV A -NeXT -Video:  Thevideo features a person riding a dirt bike on a trail surrounded by trees. the rider is \nwearing a helmet and appears to be in motion, navigating the twists and turns of the path. the trail is covered in dirt \nand rocks, adding to the challenging nature of the ride. the surrounding trees provide a natural backdrop, with their \nleaves rustling in the wind as the bike speeds by. the rider's focus is on the path ahead, with their body leaning into \nthe turns as they maneuver the bike. the sound of the bike engine and the rider's breathing can be heard in the \nbackground , adding to the immersive experience of the video. overall, the video captures the thrill and excitement \nof dirt bike riding in a scenic natural setting. \n______________________________________________________________________________\nLLaV A -NeXT -Video -LEANPO(Ours):  Thevideo captures a thrilling scene of a mountain biker navigating a \ndirt trail. the main subject is a person dressed in a white top and black pants, with a black helmet and gloves, \nriding a mountain bike. the rider is skillfully maneuvering the bike, showcasing their expertise in handling the bike \non the challenging terrain. the trail is surrounded by a dense forest, with trees on both sides, creating a natural and \nserene backdrop for the action. the rider is seen jumping over small hills and navigating through the dirt path , \ndemonstrating their agility and control over the bike. the video provides a dynamic view of the rider's movements, \nhighlighting the speed and precision involved in mountain biking. the overall atmosphere is one of adventure and \nexcitement, as the rider skillfully tackles the challenging terrain. \n___________________________________________________________________________________\nReference GT :  in a forest clearing, a person rides a mountain bike, soaring down from a high slope and swinging \nleft and right as they plunge down the curving gully. the camera then cuts to another spot in the forest where a \ndifferent rider, also on a mountain bike, leaps into the air from behind a dirt mound and quickly zooms down the \nslope. as they descend, the rider swings from side to side to maintain the bike's speed and balance, before the final \nshot switches to a stretch of woodland.\nFigure 10: More Video detailed description results compared with baseline.\n20",
  "text_length": 69610
}