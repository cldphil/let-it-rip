{
  "id": "http://arxiv.org/abs/2506.00915v1",
  "title": "3D Skeleton-Based Action Recognition: A Review",
  "summary": "With the inherent advantages of skeleton representation, 3D skeleton-based\naction recognition has become a prominent topic in the field of computer\nvision. However, previous reviews have predominantly adopted a model-oriented\nperspective, often neglecting the fundamental steps involved in skeleton-based\naction recognition. This oversight tends to ignore key components of\nskeleton-based action recognition beyond model design and has hindered deeper,\nmore intrinsic understanding of the task. To bridge this gap, our review aims\nto address these limitations by presenting a comprehensive, task-oriented\nframework for understanding skeleton-based action recognition. We begin by\ndecomposing the task into a series of sub-tasks, placing particular emphasis on\npreprocessing steps such as modality derivation and data augmentation. The\nsubsequent discussion delves into critical sub-tasks, including feature\nextraction and spatio-temporal modeling techniques. Beyond foundational action\nrecognition networks, recently advanced frameworks such as hybrid\narchitectures, Mamba models, large language models (LLMs), and generative\nmodels have also been highlighted. Finally, a comprehensive overview of public\n3D skeleton datasets is presented, accompanied by an analysis of\nstate-of-the-art algorithms evaluated on these benchmarks. By integrating\ntask-oriented discussions, comprehensive examinations of sub-tasks, and an\nemphasis on the latest advancements, our review provides a fundamental and\naccessible structured roadmap for understanding and advancing the field of 3D\nskeleton-based action recognition.",
  "authors": [
    "Mengyuan Liu",
    "Hong Liu",
    "Qianshuo Hu",
    "Bin Ren",
    "Junsong Yuan",
    "Jiaying Lin",
    "Jiajun Wen"
  ],
  "published": "2025-06-01T09:04:12Z",
  "updated": "2025-06-01T09:04:12Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00915v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00915v1  [cs.CV]  1 Jun 20253D Skeleton-Based Action Recognition: A Review\nMengyuanLiua, HongLiua, QianshuoHua, BinRenb, JunsongYuanc, JiayingLinaand JiajunWend\naState Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China\nbDepartment of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy\ncUniversity at Buffalo, The State University of New York, Buffalo, NY, USA\ndTsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\nARTICLE INFO\nKeywords :\nAction Recognition\n3D Skeleton\nComputer VisionABSTRACT\nWiththeinherentadvantagesofskeletonrepresentation,3Dskeleton-basedactionrecognitionhasbe-\ncomeaprominenttopicinthefieldofcomputervision.However,previousreviewshavepredominantly\nadopted a model-oriented perspective, often neglecting the fundamental steps involved in skeleton-\nbased action recognition. This oversight tends to ignore key components of skeleton-based action\nrecognition beyond model design and has hindered deeper, more intrinsic understanding of the task.\nTobridgethisgap,ourreviewaimstoaddresstheselimitationsbypresentingacomprehensive,task-\noriented framework for understanding skeleton-based action recognition. We begin by decomposing\nthe task into a series of sub-tasks, placing particular emphasis on preprocessing steps such as\nmodality derivation and data augmentation. The subsequent discussion delves into critical sub-\ntasks, including feature extraction and spatio-temporal modeling techniques. Beyond foundational\naction recognition networks, recently advanced frameworks such as hybrid architectures, Mamba\nmodels,largelanguagemodels(LLMs),andgenerativemodelshavealsobeenhighlighted.Finally,a\ncomprehensive overview of public 3D skeleton datasets is presented, accompanied by an analysis of\nstate-of-the-art algorithms evaluated on these benchmarks. By integrating task-oriented discussions,\ncomprehensive examinations of sub-tasks, and an emphasis on the latest advancements, our review\nprovidesafundamentalandaccessiblestructuredroadmapforunderstandingandadvancingthefield\nof 3D skeleton-based action recognition.\n1. Introduction\nActionrecognitionisacriticalcomponentandoneofthe\nmost actively researched topics in computer vision, having\nbeen explored for several decades. Its capability to identify\nhumanactionsholdsimmensepromiseacrossdiverseappli-\ncations, including intelligent surveillance, human-computer\ninteraction, virtual reality, robotics, and more [1, 2, 3, 4].\nSignificant advancements have been achieved using various\ndata representations [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], such\nas RGB image sequences, depth images, and optical flows.\nHowever, these modalities often involve higher computa-\ntional costs, reduced robustness in complex environments,\nandincreasedsensitivitytovariationslikebodyscale,view-\npoint changes, and motion speeds [15]. Moreover, they can\nintroduceprivacyconcernsinpracticalapplications.Incon-\ntrast, skeleton data offers a topological and abstract repre-\nsentation of the human body through joints and bones. This\nmodality is not only computationally efficient but also less\naffected by environmental complexities. Recent advance-\nmentsinsensors,suchasMicrosoftKinect[16],andstate-of-\nthe-art human pose estimation algorithms [17, 18, 19, 20],\nhave enabled the accurate capture of 3D skeleton data [21],\nmaking it a promising alternative for action recognition.\nSkeletonsequencesexhibitthreenotablecharacteristics:\n(1) Spatial human pose: Strong correlations between joint\nnodes and their neighbors reveal rich structural informa-\ntion within each frame. (2) Temporal human action: Inter-\nframe correlations capture significant temporal dynamics.\n(3) Spatio-temporal co-occurrence: Joint and bone interac-\ntionslinkspatialandtemporaldomains.Thesefeatureshave\ngarnered significant attention in human action recognitionanddetection,drivingthegrowingadoptionofskeletondata\nin the field.\nHuman action recognition based on skeleton sequences\nis primarily a temporal pattern recognition problem, with\ntraditionalmethodsoftenfocusingonextractingmotionpat-\nterns from skeleton data. This has led to extensive research\non handcrafted features [22, 23, 24], commonly involving\nrelative3Drotationsandtranslationsbetweenjointsorbody\nparts[25,26].However,studieshaveshownthathandcrafted\nfeaturestypicallyperformwellonlyonspecificdatasets[27],\nhighlighting the issue that features designed for one dataset\nmay not transfer effectively to others, especially in wild\nenvironments. This limits the generalizability and broader\napplicability of action recognition algorithms.\nWith the rapid advancements in deep learning, methods\nsuchasRNNs,CNNs,GCNs,Transformers,hybridmethods\nandgenerativemethods,havebeenappliedtoskeleton-based\naction recognition, as Fig.3. RNN-based models [28, 29,\n30, 31], like LSTMs and GRUs, are strong in temporal\nmodeling, treating skeleton sequences as time-series data.\nCNNs [32, 33, 34] excel at capturing spatial relationships\namong joints by encoding skeleton sequences as pseudo-\nimages. However, both RNNs and CNNs fail to model the\nintrinsic relationships among joints. To address this, GCNs\n[35, 36, 37] represent skeleton data as graphs, where joints\nare vertices and bones are edges, offering better representa-\ntionofstructuralinteractions.Transformers[38,39,40,41],\nwiththeirself-attentionmechanism,havegainedtractionfor\nmodeling joint relationships and improving skeleton data\nanalysisbyreplacinghandcraftedadjacencymatrices.More\nPage 1 of 30\n--- Page 2 ---\n3D Skeleton-Based Action Recognition: A Review\nGeometric Features\nGeneralized \nFeatures\nJoint Bone\nJoint\nMotionBone\nMotionJoint Bone\nBone-Motion Joint-Motion1\n4\n5\n9832\n67\n10\n111\n4\n5\n9832\n67\n10\n11\nGraph-Structured Representation\nt\n4\n5\n9832\n671\n1\n10\n114 24 4\n14 24 4\n16 26 6\n16 26 6\n18 28 8\n18 28 8jointframe1...\n......\n......\n......11\n11\n11\n11\n11\n11t. . .1 t. . .1 t. . .1\nPseudo-Image RepresentationSpatial \nModuleTemporal\nModule\nSpatial Module\nTemporal Module\nSpatial-\nTemporal \nModuleSpatial-\nTemporal \nModuleParallel Structure\nFusion StructureSerial Structuret-1 t t+1\nSpatial Feature Extraction\nTemporal Feature Extraction\nRunning\nSkeletal Data \nModalititesSkeleton-Specific \nAugmentationsSkeletal Data\nRepresentationSkeletal Feature \nExtractionSpatial-Temporal\nModeling\nNormal \nAugmentations\nExtreme \nAugmentations\nMixing \nAugmentations\nViewpoint-Invariant \nTransformSequential \nRepresentation\nPseudo-Image \nRepresentation\n Graph-Structure \nRepresentation\n Token-Based\nRepresentationTemporal Feature\nExtraction\nSpatial \nFeature Extraction\nAssisted \nFeature ExtractionSpatio-temporal\nSerial Structure\nSpatio-temporal\nParallel Structure\nSpatio-temporal \nFusion Structure\nFigure 1: A comprehensive workflow for 3D skeleton-based action recognition using deep learning\nSkeleton Sequence Skeleton Pseudo-Image Feature… …t\n4\n5\n9832\n671\n1\n10\n114 24 4\n14 24 4\n16 26 6\n16 26 6\n18 28 8\n18 28 8jointframe1...\n...\n...\n...\n...\n...\n...11\n11t. . .1 t. . .1 t. . .111\n11\n11\n11\n(a) CNN-based Feature Extraction\nSkeleton SequenceSkeleton Graph\n(c) Graph-based Feature Extraction…\nFeatureSkeleton Sequence Featuret1t2t3\nJointTime\nH(j,t)\n(j,t)H(j-1,t) H(j,t)\nH(j,t-1)\n(b) RNN-based Feature Extraction\nSkeleton Sequence Embedding Feature\n(d) Transformer-based Feature ExtractionSelf-AttentionFeed-Forward\nSelf-AttentionEncoder-Decoder AttentionFeed-Forward×N Transformer\nEncoder Decoder\nSkeleton Sequence Embedding\nFigure 2: Main feature extraction methods\nrecently, hybrid methods [42, 43, 44, 45, 46] seek to com-\nbine the strengths of two or more backbone architectures,\nsuch as RNNs, CNNs, GCNs, and Transformers, to more\ncomprehensivelymodelbothspatialandtemporalaspectsof\nskeletondata.Generativemethods[47,48,49,50]havebeen\nadapted to pre-train and regularize skeleton representations\nby reconstructing or predicting masked joints, thereby cap-\nturing intrinsic spatio-temporal patterns in an unsupervised\nmanner.\nSkeleton-based action recognition using deep learning\nmethods has been extensively summarized and discussed\nin several surveys [51, 52, 41, 53]. Sun et al. [52] and\nShin et al. [53] provide detailed discussions on the Action\nRecognition task under diverse data modalities, but they\nonly briefly cover 3D skeleton-based methods without in-\ndepth analysis. Xin et al. [41] specifically focus on 3D\nskeleton-based methods, but their discussion is limited to\nTransformer-based approaches. Ren et al. [51] offer a com-\nprehensiveoverview,coveringmethodsrangingfromRNNs\ntoTransformers.Thesesurveystypicallycategorizeresearcheffortsbasedonmodelarchitectures,suchasRNNs,CNNs,\nGNNs, and Transformers.\nHowever, existing reviews still exhibit several short-\ncomings. On one hand, although categorizing methods by\narchitectureeffectivelytracesthefield’sevolutionandhigh-\nlights focal points over time, recent advances have given\nrise to more flexible hybrid designs [42, 43, 44, 45, 46] that\nexploit the unique properties of skeletal action sequences\nand transcend the constraints of traditional models. More-\nover, these surveys have yet to cover emerging approaches\nsuch as those based on large language model prompting or\nMamba architectures. On the other hand, as illustrated in\nFig. 1, each stage, from skeleton data processing through to\nspatio-temporal feature modeling, critically influences final\nperformance.Yetexistingreviewstendtoconcentratealmost\nexclusively on the modeling phase, without offering a com-\nprehensive overview of the data preprocessing steps, which\nwebelievedeservesequalattention.Theseobservationslead\ntothreekeyinsights:(1)classificationbymodelarchitecture\naloneisinsufficient;instead,asystematicreviewofthetask’s\nessential stages can provide a more holistic perspective\nPage 2 of 30\n--- Page 3 ---\n3D Skeleton-Based Action Recognition: A Review\nRNN-Based MethodsSkeleMotion\n (Caetano et al.)Enhanced Skeleton\nVisualization\n(Liu et al.)\nBayesian GC-LSTM\n(Du et al.)Clips+CNN+MTLN\n(Ke et al.)\n4s Shift-GCN  \n(Cheng et al.)Hyperformer  \n(Zhou et al.)JTM\n(Wang et al.)PoseC3D\n(Duan et al.)\nDG-STGCN   \n(Duan et al.)\nKShapeNet  \n(Friji et al.)GCA-LSTM \n(Liu et al.)InfoGCN   \n(Chi et al.)TSGCNeXt  \n(Liu et al.)\nKA-AGTN  \n(Liu et al.)HBRNN-L   \n(Du et al.)\nElse-Net  \n(Li et al.)IndRNN  \n(Li et al.)\nMANs    \n(Li et al.)ST-GCN\n(Yan et al.)\n2021 2022 2023\nCNN-Based Methods\nGCN-Based Methods\nTransformer-Based Methods\nHybrid MethodsRotClips+M\nTCNN\n(Du et al.) AGC-LSTM\n(Si et al.)2s-AGCN\n(shi et al.)\nDC-GCN+ADG\n(Cheng et al.)MS-G3D\n(Liu et al.)DSTA-Net\n(Shi et al.)ST-TR\n(Plizzari et al.)\nIIP-Transformer\n(Zhang et al.)\nTrust Gate ST-LSTM\n(Ke et al.)20152016 2017 2018\nRes-TCNs\n(Kim et at.)GCA-LSTM \n(Liu et al.)\nSR-TSL\n(Si et al.)2019\nHCN\n(Li et al.)\nAS-GCN\n(Li et al.)Fuzzy Integral-\nBased CNN\n (Banerjee et al.)SGN\n(Zhang et al.)\nCTR-GCN\n(Chen et al.)PSUMNet\n(Trivedi et al.)STAR-Transformer\n(Ahn et al.)\nTemPose\n(Ibh et al.)LA-GCN\n(Xu et al.)2020\nVA-CNN\n(Zhang et al.)2024BlockGCN\n(Zhou et al.)\nDS-GCN\n(Xie et al.)PURLS\n(Zhu et al.)SkateFormer\n(Do et al.)\nSkeleton-in\n-Context\n(Wang et al.)\n2025\nGenerative MethodsSA-DVAE\n(Li et al.)MacDiff\n(Wu et al.)\nFigure 3: Chronological overview of the most relevant deep learning-based action recognition methods.\nand help readers delve into the core modeling challenges;\n(2) skeleton data preprocessing has a profound impact on\nsubsequentspatio-temporalmodelingandthereforewarrants\ndetailed examination; and (3) given the limited number of\ncurrent surveys, our review must encompass the very latest\ndevelopments,includingMambaarchitecturesandlargelan-\nguage model–based prompting.\nTherefore,thispapershiftsthefocusfrommodelarchitecture-\nbased classification to the more fundamental challenges\nof skeleton-based action recognition: how to transform\nunstructured data into structured representations and ef-\nfectively model the spatio-temporal features of skeletal se-\nquences. Specifically, we organize our discussion along the\nhierarchical tasks involved in deep learning-based skeletal\naction sequence modeling: derived modalities ,data aug-\nmentation ,data representation ,temporal/spatial mod-\neling, andspatio-temporal co-modeling . This paradigm\nnot only outlines the entire pipeline of this task but also\nprovides a comprehensive analysis of the core modeling\nchallenges in this domain. It aims to offer readers a deeper\nunderstanding of the characteristics of skeletal data and the\ninherent research difficulties of the task.\nIn summary, our paper contains four contributions:\n•A more essential framework: This review adopts\na task-oriented framework, departing from the tra-\nditional model architecture-centered discussions. As\nshown in Fig. 1, we organize the discussion based on\nthe key stages of the task, with a particular emphasis\nondatarepresentation,skeletonmodeling,andspatio-\ntemporal co-modeling. This unique viewpoint helps\nreaders gain a deeper understanding of the intrinsic\nnature of the task.\n•Amorecomprehensiveframework: Webelievethat\nthepreprocessingofskeletondataplaysacrucialrole\nin subsequent spatio-temporal modeling. Therefore,\nwefocusonandanalyzetaskssuchasskeleton-based\nderived modalities and skeleton-specific augmenta-\ntions in the data preprocessing phase. This allows us\nto refine the framework of existing surveys, which\ntypically focus solely on spatio-temporal modeling\nmethods, and provides readers with a more compre-\nhensive and holistic view of the entire process.•Core Issue Refinement: By breaking down the task\ninto multiple progressively detailed subtasks, this re-\nview provides a clear research roadmap. As shown in\nFig.2,thisreviewmakesamoreessentialanddetailed\ndiscussionofkeyprocessessuchasfeatureextraction\nand modeling, which helps identify common chal-\nlenges in the entire task and encourages researchers\nto explore model design and optimization from the\nperspective of data understanding.\n•Exploring Cutting-Edge Methods: This review fo-\ncuses on the latest and most advanced deep learning\nmethodsforskeleton-basedhumanactionrecognition,\nwith particular attention to emerging hybrid architec-\nture models, Mamba architecture models, and large\nlanguage models (LLMs). A systematic analysis and\nevaluation of current state-of-the-art (SOTA) models\nare presented, exploring their performance and appli-\ncabilitytospecifictasks.Thisin-depthdiscussionalso\nprovidesinsightsintothefuturedirectionsofresearch\nin this field.\nThe structure of the remaining sections is organized as\nfollows. Section 2 analyzes the theoretical modeling of the\noverall process. Sections 3 to 7 are structured in the se-\nquentialstagesoftheskeleton-basedactionrecognitiontask.\nSection 3 explores the generation and application of multi-\nmodal skeleton data. Section 4 introduces skeleton-specific\ndata augmentation methods. Section 5 discusses techniques\nfor converting skeleton data into representations suitable\nfor model input. Section 6 provides a detailed account of\nfeature extraction methods. Section 7 focuses on spatio-\ntemporal modeling techniques. Section 8 lists commonly\nuseddatasetsandtheperformanceofstate-of-the-artmodels\non these datasets. Finally, Section 9 discusses the potential\nfuture developments in the field of skeleton-based action\nrecognition.\n2. Overall Process Theoretical Modeling\nIn this section, we will model the overall skeleton ac-\ntion recognition process in combination with Fig.1, aiming\nto provide readers with a clear and intuitive framework\nthrough mathematical representation and step-by-step anal-\nysis. Starting from the input skeleton data modality, we\nwill discuss key links such as data enhancement, feature\nPage 3 of 30\n--- Page 4 ---\n3D Skeleton-Based Action Recognition: A Review\nrepresentation,featureextraction,andspatio-temporalmod-\neling. Then, we will reveal the overall process and internal\nmechanism of skeleton action recognition based on deep\nlearning.\n2.1. Skeletal Data Modalities\nIn skeleton-based action recognition, the input data is\ntypically represented as a skeleton sequence, consisting of\njoint coordinates evolving over time, as shown in Eq.1.\n={𝑋𝑡∣𝑡=1,2,…, 𝑇}, 𝑋𝑡∈ℝ𝐽×𝐷(1)\nwhere 𝑇is the total number of time steps, 𝐽represents\nthe number of joints, and D denotes the dimensionality\nof each joint (e.g., 2D or 3D coordinates). The features\ncan be categorized into two main types: geometric features\nand generalized features. The generalized features are cate-\ngorized into four distinct modalities: Joint ( 𝑋joint\n𝑡), Bone\n(𝑋bone\n𝑡), Joint Motion ( 𝑋jm\n𝑡), and Bone Motion ( 𝑋bm\n𝑡).\nEachmodalityrepresentsaspecificcharacteristicofskeletal\ndata.TheJointmodalitycapturesthepositionalinformation\nof key body joints, while the Bone modality encodes the\nrelativerelationshipsbetweenconnectedjoints.JointMotion\nand Bone Motion modalities describe the dynamic changes\ninjointsandbonesovertime.TheBonemodalityisdefined\nby Eq. 2, the Joint Motion modality by Eq. 3, and the Bone\nMotion modality by Eq. 4, where 𝑋joint1\n𝑡and𝑋joint2\n𝑡denote\nthe two joints at the ends of a bone, and 𝑋𝑡−1indicates the\nposition of a joint or bone at the previous time step.\n𝑋bone\n𝑡=𝑋joint1\n𝑡−𝑋joint2\n𝑡(2)\n𝑋jm\n𝑡=𝑋joint\n𝑡−𝑋joint\n𝑡−1(3)\n𝑋bm\n𝑡=𝑋bone\n𝑡−𝑋bone\n𝑡−1(4)\n2.2. Skeleton-specific Augmentations\nTo improve the generalization ability and robustness of\nthe model, skeleton-specific data enhancements are applied\nto the input data , denoted as ′=(), where \nrepresents the transformation function. These augmenta-\ntion methods include Normal Augmentations ,Extreme\nAugmentations ,Mixing Augmentations , andViewpoint-\nInvariant Transformations . Normal augmentations en-\nhance data diversity through simple perturbations, while\nextremeaugmentationsfurtherperformspatialandtemporal\ntransformations such as cropping, rotation, time reversal,\nand Gaussian noise addition. Mixing augmentations com-\nbine different samples to generate new data, and viewpoint-\ninvariant transformations ensure the stability of skeleton\ndata under varying observation angles.2.3. Skeletal Data Representation\nThe representation of skeleton data is critical for sub-\nsequent feature extraction and modeling, denoted as =\n(′), where is the representation function. The main\ndatarepresentationmethodsincludeSequentialRepresenta-\ntion,Pseudo-ImageRepresentation,Graph-StructureRepre-\nsentation and Token-based Representation.\nSequential Representation treats skeleton data as a\ntime-series input and is particularly suited to RNN-based\nmethods. The sequential representation can be formally de-\nfined as Eq.5.\nseq={𝑋𝑡∣𝑡=1,2,…, 𝑇} (5)\nPseudo-Image Representation converts skeleton data\nintoatwo-dimensionalimageformatbymappingthetempo-\nral dimension 𝑇and joint dimension 𝑉to the image height\n𝐻and width 𝑊, making it compatible with CNN-based\nmodels. The pseudo-image representation is given by Eq.6.\nimg=𝑓img()∈ℝ𝑇×𝑉×𝐶(6)\nGraph-Structure Representation models the skeleton\nas a graph 𝐺=(𝑉 , 𝐸), where 𝑉denotes joint nodes and 𝐸\ndenotes bones connecting them, as shown in Eq.7.\ngraph=𝐺=(𝑉 , 𝐸) (7)\nToken-based Representation transforms each joint’s\ndata into independent tokens via the embedding function\n𝑇 𝑜𝑘𝑒𝑛𝐸𝑚𝑏𝑒𝑑 , then applies position encoding 𝑃 𝑜𝑠𝐸𝑚𝑏𝑒𝑑\ntopreserveanddistinguishtemporalandpositionalrelation-\nships between tokens. The token representation can be for-\nmallydefinedasEq.8,where 𝐳𝑡,𝑣iscomputedasEq.9.This\nrepresentationiswellsuitedtoTransformer-basedmethods.\ntoken={𝐳𝑡,𝑣∣𝑡=1,2,…, 𝑇 , 𝑣=1,2,…, 𝑉}(8)\n𝐳𝑡,𝑣=𝑇 𝑜𝑘𝑒𝑛𝐸𝑚𝑏𝑒𝑑 (𝑋𝑡[𝑣])+𝑃 𝑜𝑠𝐸𝑚𝑏𝑒𝑑(𝑡, 𝑣)(9)\nThese representations enable RNNs, CNNs, GCNs and\nTransformers to effectively model the structure and dynam-\nics of skeleton data in subsequent stages.\n2.4. Feature Extraction\nFeatureextractionisacrucialstepinskeleton-basedac-\ntion recognition, aimed at extracting discriminative spatio-\ntemporal features from the data to enhance recognition per-\nformance.Specifically,itincludesTemporalFeatureExtrac-\ntion and Spatial Feature Extraction, denoted as Φ()→\n𝐹temporal , 𝐹spatial, whereΦrepresents the feature extraction\nfunction applied to the input data .Temporal feature\nextraction focuses on capturing the dynamic changes of\nthe skeleton data over the temporal dimension, such as the\nPage 4 of 30\n--- Page 5 ---\n3D Skeleton-Based Action Recognition: A Review\nspeed and continuity of actions. Spatial feature extrac-\ntionemphasizes the static relationships, geometric arrange-\nments,andstructuresamongdifferentjointsorboneswithin\nthe skeleton. Different network approaches, such as RNNs,\nCNNs,GCNs,andTransformers,exhibitdistinctadvantages\natthisstage,ensuringtheeffectiveextractionofkeyfeatures\nrelevant to the action categories.\n2.5. Spatial-Temporal Modeling\nSpatio-temporal modeling is the core step in skeleton-\nbased action recognition, aiming to jointly model the spa-\ntial structure and temporal dynamics of skeleton data. It\nis expressed as 𝐹final= Φst(𝐹temporal , 𝐹spatial), whereΦst\nrepresents the spatio-temporal modeling method. Generally\ndepending on the approach, spatio-temporal modeling falls\ninto the following three main structures.\nSpatio-temporal serial structure , as shown in Eq.10,\nextracts spatial features first and then models the temporal\nrelationshipssequentially,makingitsuitableforstagedpro-\ncessing.\n𝐹serial=Φtemporal(Φspatial()) (10)\nSpatio-temporalparallelstructure ,asshowninEq.11,\nsimultaneously models spatial and temporal information to\nenable joint learning.\n𝐹parallel=Φtemporal()+Φspatial() (11)\nSpatio-temporal fusion structure , as shown in Eq.12,\ncombines spatial and temporal features through flexible\nmechanisms, such as weighted summation, concatenation,\nor attention mechanisms, to enhance overall modeling per-\nformance. The fusion function Fusion (⋅)can adopt methods\nsuch as weighted summation, concatenation, or attention\nmechanisms.\n𝐹fusion=Fusion(Φtemporal(),Φspatial())(12)\nFinally, the output prediction is performed based on a\nfully connected layer with parameters 𝑊and𝑏, and the\npredicted action category ̂ 𝑦is obtained using the softmax\nfunction as Eq.13.\n̂ 𝑦=Softmax(𝑊⋅𝐹final+𝑏). (13)\n3. Skeleton-based Modalities Generation\nSkeleton-based action recognition relies on skeleton se-\nquences as the primary input data, which consist of a series\nof skeleton coordinates. Based on this raw joint coordinate\ndata, it is natural to derive additional representations by\ndefining various relationships within the skeleton structure.\nFor instance, the edges formed between pairs of joints can\nserveasbonemodalityinformation,whileplanesdefinedby\nthree joints can represent another type of modality. Thesemultiple modalities are essentially supplementary data de-\nrived from skeleton sequences through predefined meth-\nods.Notably,manystudieshavedemonstratedthatdifferent\npatterns of data exhibit distinct characteristics, which can\ncomplement each other in action recognition tasks. This\nsectionfirstreviewsseveralclassicapproachesthatleverage\nsuch modalities and then discusses the current mainstream\nparadigms of modality design based on deep learning tech-\nniques.\n3.1. Geometric Features\nEarly research focused on recognizing actions by mea-\nsuring skeleton parameters like angles, positions, orienta-\ntions,velocities,andaccelerations[54,55,56,57,58].Chen\net al. [59] identified nine geometric features and combined\nthem to represent pose and motion. Yao et al. [60] devel-\nopedpose-basedfeatures,suchasjointdistances,joint-plane\ndistances, and joint velocities. Ofli et al. [57] used angles\nbetween bones to select informative joints for classifica-\ntion.Vemulapalli[23]usedrotationalrelationshipsbetween\nbody parts in Lie groups. These geometric features are\nintuitivelyunderstandable,describingrelationshipsbetween\njoints, lines and planes.\nWith the rise of deep learning, many works have ex-\ntended geometric features for action recognition by em-\nploying various advanced deep learning methods. Huang\net al. [61] extended [23] by incorporating Lie group struc-\nture into a deep network, enabling it to effectively learn\nrotation-based features. Zhang et al. [62, 63] explored com-\nplex geometric relationships between joints, feeding rich\nspatial domain features into a three-layer LSTM network\nand designing eight relational features, as shown in Fig. 4.\nExtensive experiments demonstrated that the joint-to-line\ndistance feature outperformed other geometric features. Li\net al. [64] proposed view-invariant shape motion represen-\ntations derived from skeleton sequences using geometric\nalgebra,whichsignificantlyimprovedrecognitionaccuracy.\nLiuetal.[65]introducedanovelfeaturedescriptorbasedon\nrotationalrelationshipsbetweenjointsinskeletons,leverag-\ning geometric algebra to compute and derive these rotation\nrelationsusingaspecificoperatorknownastherotorinGA,\nthusofferingmorerobustandmeaningfulfeaturesforaction\nrecognition.\n3.2. Generalized Features\nAlthough geometric features can outperform methods\nrelying solely on joint coordinates, the diverse and intricate\ndesigns of these features often hinder the model’s general-\nization performance. Some geometric features show clear\nadvantagesonlyonspecificdatasets.Asaresult,researchers\nhave shifted their focus towards more generalized modal\nrepresentations. The four predominant modalities namely\nJoint, Bone, Joint-Motion and Bone-Motion have gradually\nbecome mainstream.\nBone.Withtheexceptionoforiginaljoints,manyworks\n[62,37,66]haveprovedthatthelines(bones)betweenpair-\nwise joints are also important geometric structures in the\nskeletonandcontainrichstructuralorrelationalinformation.\nPage 5 of 30\n--- Page 6 ---\n3D Skeleton-Based Action Recognition: A Review\nName Symbol Calculation Description\nJoint Coordinate JcJc(J)=(Jx,Jy,Jz) The 3D coordinate of the joint J.\nJoint-Joint Distance JJdJJd(J1,J2)=/vextenddouble/vextenddouble/vextenddouble− − →J1J2/vextenddouble/vextenddouble/vextenddoubleThe Euclidean distance between joint J1to\nJ2.\nJoint-Joint Orienta-\ntionJJoJJo(J1,J2)= unit(− − →J1J2)The orientation from joint J1toJ2, repre-\nsented by the unit length vector− − →J1J2.\nJoint-Line Distance JLdJLd(J,LJ1→J2)=2S/triangleJJ1J2/JJd(J1,J2)The distance from joint Jto lineLJ1→J2.\nThe calculation is accelerated with Helen\nformula.\nLine-Line Angle LLaLLa(LJ1→J2,LJ3→J4)\n= arccos( JJo(J1,J2)T⊙JJo(J3,J4))The angle ( 0toπ) from line LJ1→J2to\nLJ3→J4.\nJoint-Plane Distance JPdJPd(J,PJ1→J2→J3)\n=(Jc(J)−Jc(J1))⊙JJo(J1,J2)⊗JJo(J3,J4)The distance from joint Jto plane\nPJ1→J2→J3.\nLine-Plane Angle LPaLPa(LJ1→J2,PJ3→J4→J5)\n= arccos( JJo(J1,J2))⊙JJo(J3,J4)⊗JJo(J3,J5)The angle (0 to π) between line LJ1→J2\nand the normal vector of plane PJ3→J4→J5.\nPlane-Plane Angle PPaPPa(PJ1→J2→J3,PJ4→J5→J6)\n= arccos( JJo(J1,J2)⊗JJo(J1,J3)\n⊙JJo(J3,J4)⊗JJo(J3,J5))The angle (0 to π) between the nor-\nmal vectors of planes PJ1→J2→J3and\nPJ4→J5→J6.\nTable 2: Deﬁnitions of eight geometric features. Note that Hips coordinate is excluded as it is ﬁxed as (0,0,0). On the other\nhand, the ycoordinate of Hip in the world coordinate frame reﬂects the absolute height of body and is informative in some\ncases (e.g., discerning jumping in the air), and hence is included. ⊙is the dot product. ⊗is the cross product of two vectors.\nJc(J) JJd(J1,J2) JJo(J1,J2) JLd(J,L)\nLLa(L1,L2) JPd(J,P) LPa(L,P ) PPa(P1,P2)\nFigure 5: Eight feature types. Note that for each feature only the relevant joints, lines, and planes are drawn in red.\nv2 in varied views containing 4different data modalities\nfor each sample. It consists of 56,880 action samples of\n60different classes including daily activities, interactions\nand medical conditions performed by 40subjects aged be-\ntween 10and35.A25joints human model is provided. In\norder to evaluate effectiveness of scale-invariant and view-\ninvariant features, it provides two types of evaluation proto-\ncols, cross-subject and cross-view.\nUT-Kinect Dataset [29]. The UT-Kinect dataset iscaptured by a single stationary Kinect containing 200 se-\nquences of 10classes performed by 10subjects in varied\nviews. Each action is recorded twice for every subject and\neach frame in a sequence contains 20skeleton joints. We\nfollow the half-vs-half protocol proposed in [38], where half\nof the subjects are used for training and the remaining for\ntesting.\nBerkeley MHAD [19]. Berkeley MHAD is captured by\na motion capture system containing 659 sequences of 11\n152\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on December 07,2024 at 02:54:06 UTC from IEEE Xplore.  Restrictions apply. \nFigure 4: Designed eight geometric relational features: Joint\nCoordinate, Joint-Joint Distance, Joint-Joint Orientation,\nJoint-Line Distance, Line-Line Angle, Joint-Plane Distance,\nLine-Plane Angle, Plane-Plane Angle.\nInparticular,sinceeachboneisconnectedtotwojoints,\nShi et al. [66] firstly defined the joint closest to the center\nof mass of the skeleton as the source joint, and the joint\nfartherfromthecenterofmassasthetargetjoint.Eachbone\nisthenrepresentedasavectorpointingfromthesourcejoint\ntothetargetjoint,whichcapturesbothlengthanddirectional\ninformation.Forinstance,givenabonewithitssourcejoint\n𝐽1= (𝑥1, 𝑦1, 𝑧1)and its target joint 𝐽2= (𝑥1, 𝑦1, 𝑧1), the\nbone vector is computed as Eq.14.\n𝐵𝐽1,𝐽2=(𝑥2−𝑥1, 𝑦2−𝑦1, 𝑧2−𝑧1) (14)\nSpecifically, joints emphasize the absolute position,\nwhich can figure out discriminative moving parts of body\nintheactionbyanalyzingthedistributionorlocaldensityof\njoints. While bones emphasize relative position, which can\nbuild angles to help figure out specific poses and actions.\nThus, there exists potential complementarity for action\nrecognition between both geometric structures.\nTemporal Motion. Inspired by the work of Simonyan\nand Zisserman [12], where RGB images and optical flow\nfields are utilized as two input streams of a network, Li et\nal. [67] extended this concept by defining temporal motion\nextracted from skeleton data as an additional input. Specifi-\ncally, given a 3D joint coordinate 𝑱=(𝑥, 𝑦, 𝑧), the skeleton\nof an individual is represented as a set of joint coordinates\n𝑿𝑡= {𝑱1\n𝑡,𝑱2\n𝑡,…,𝑱𝑁\n𝑡}where 𝑁denotes the number of\njoints in the skeleton for each frame. The skeleton motion\nbetween two consecutive frames is computed as Eq.15.\n𝑴𝑡=𝑿𝑡−𝑿𝑡−1\n={𝑱1\n𝑡−𝑱1\n𝑡−1,𝑱2\n𝑡−𝑱2\n𝑡−1,…,𝑱𝑁\n𝑡−𝑱𝑁\n𝑡−1}(15)\nwhere 𝑡is the frame index. Temporal motion, also re-\nferred to as joint-motion, is informative in discriminating\nfine-grainedactions,suchas\"putonjacket\"versus\"takeoff\njacket.\"Temporalmotioncanalsobecomputedforthebone\nstream in the same manner as joint motion, yielding bone-\nmotion.\nMainstream Trends. Different types of data exhibit\ndistinct characteristics, and fusing multiple modalities can\nenhance performance. Leveraging the simple yet effective\n3\n3D skeleton data. In [ 11], Zheng et al. propose a genera-\ntive adversarial network (GAN) based encoder-decoder for\nsequential reconstruction, and exploit the learned intermediate\nrepresentation to recognize different actions. Su et al. [15]\npropose a decoder-weakening strategy for the encoder-decoder\nmodel, so as to drive the encoder to learn discriminative action\nfeatures. Linet al. [16] devise multiple pretext tasks ( e.g.,\nmotion prediction, identifying temporal order) to drive the\nunsupervised learning with encoder-decoder architecture , and\ncombine them to encourage the Bi-GRU encoder to capture\nmore action patterns.\nThere are a few key differences between our method and\nprevious skeleton-based methods: (1) We propose a novel\ncontrastive action learning paradigm to learn effective action\nrepresentations from unlabeled skeleton data. We do NOT\nrequire feature engineering like [ 5], [6], [28] or designing\ntask-speciﬁc models ( e.g., GAN [ 11], encoder-decoder [ 15]) to\nimplement corresponding pretext tasks like reconstruction. By\ncontrast, we exploit different novel skeleton data augmentation\nstrategies to drive the contrastive learning, which encourages\nthe model to learn inherent action patterns from different\nskeleton transformations. Besides, the proposed contrastive\nlearning paradigm is highly ﬂexible and scalable, which could\nbe extended to different pretext tasks and encoders. (2) The\nproperty of consistence is exploited to achieve better con-\ntrastive learning: We not only propose a momentum LSTM to\nlearn more consistent action representations but also involve\na queue to build a consistent and memory-efﬁcient dictionary\nto improves the performance of the unsupervised contrastive\naction learning.\nB. Contrastive Learning\nContrastive learning [ 20], [23], [33]–[36] is an effective un-\nsupervised learning method that can be applied on various pre-\ntext tasks via a contrastive loss. Pretext tasks ( e.g., motion re-\nconstruction, frame prediction) can be used to learn useful data\nrepresentation beforehand and later to be applied to the tasks\nof real interest like action recognition. Some works design\npretext tasks based on auto-encoders to denoise images [ 37], or\nachieve plausible image colorization [ 38]. The contrastive loss\n[39] is associated with tasks and it measures the similarities\nof sample pairs in a representation space. For example, in\ninstance discrimination task [ 33], the noise-contrastive esti-\nmation (NCE) related contrastive loss [ 40] pulls closer the\naugmented samples from the same instance, and pushes apart\nones from different instances. The contrastive multiview cod-\ning (CMC) [ 20] aims to maximize mutual information between\ndifferent views, while the momentum contrastive paradigm\n(MoCo) [ 34], [36] facilitates contrastive unsupervised learn-\ning by queue-based dictionary look-up mechanism and the\nmomentum-based update. Compared with Moco, SwA V [41]\nincorporates the online clustering into contrastive learning,\nwhich runs with small batches and requires less memory for\nstorage of features. In [35], Chen et al. propose SimCLR\nwith the multi-layer perceptron (MLP) projection head and\nstronger color augmentation to further improve the quality of\nunsupervised learned representation ( Likewise, Moco v2 [36]\n(a) Original\n (b) Rotation\n (c) Shear\n (d) Reverse\n(e) Gaussian  Noise\n (f) Gaussian  Blur\n (g) Joint  Mask\n (h) Channel  Mask\nFig. 2. Visualization of data augmentations (b)-(h) for the same skeleton\nsequence (a).\nbeneﬁts from the MLP and the stronger color augmentation ).\nSimCLRv2 [42]rather adopts larger ResNet models, deeper\nprojection heads, and memory mechanism from MoCo to\nachieve superior performance. In asimpler way, SimSiam [43]\nrequires neither negative pairs nor a momentum encoder ( i.e.,\ncan be viewed as “SimCLR” without negative pairs) to obtain\ncompetitive outcomes . [34]–[36], [42], [43] could be viewed\nas an instance discrimination method to perform unsupervised\nvisual representation learning. This work is the ﬁrst attempt to\nexplore contrastive learning based on instance discrimination\nfor learning an effective action representation directly from\nunlabeled 3D skeleton sequences.\nIII. P ROPOSED APPROACH\nSuppose that an input skeleton sequence x= (x1,...,xT)\ncontainsTconsecutive skeleton frames, where xi∈RM×J×3\ncontains 3D coordinates of Jdifferent body joints for M\nactors. The training set Φ ={x(i)}N\ni=1containsNskeleton\nsequences of different actions collected from multiple views\nand persons. Each skeleton sequence x(i)corresponds to a\nlabelyi, whereyi∈{a1,···,ac},airepresents the ithaction\nclass, andcis the number of action classes. Our goal is to learn\nan effective action representation qfromx(i)without using\nany skeleton label. Then, the effectiveness of learn features q\nis validated by the linear evaluation protocol: Leaned features\nqand labels are used to train a linear classiﬁer for action\nrecognition (note that qis frozen and NOT tuned at the\nrecognition stage). The overview of the proposed approach\nis given in Fig. 3, and we present the details of each technical\ncomponent below.\nA. Data Augmentation for Skeleton Sequences\nAs the goal of contrastive learning is to learn shared pattern\ninformation between different augmented instances of the\nsame example [ 34]–[36], it is natural to consider the prop-\nerty of “pattern-invariance” in skeleton sequences: Random\ntransformations of the same skeleton sequence under a speciﬁc\naugmentation strategy or strategy composition ( e.g., rotation)\nKEEP similar action patterns, which can be contrasted and\nlearned to achieve an effective action representation. TakeFigure 5: Visualization of normal augmentations.\nmodal designs of Joint, Bone, Joint-Motion and Bone-\nMotion, an increasing number of studies [68, 69, 70, 71,\n72, 73, 74, 75] have adopted these four modalities as inputs\nfor modeling. Specifically, most models are trained using\nthese four modalities separately, with results obtained by\naveraging the outputs from each modality.\n4. Skeleton-specific Augmentations\nMost deep learning methods require large amounts of\ndata to achieve optimal performance. However, most ex-\nisting skeleton-based action recognition algorithms have\nfocused on network architectures [51, 52], and one critical\nissue that has received little attention is the inadequacy of\nskeleton data. The lack of annotated training data often\nleads to overfitting, reducing the model’s generalization\nability. Therefore, leveraging data augmentation techniques\nto expand limited skeleton data can enhance both model\nperformance and generalization. This process is essential\nin the data preprocessing stage before model development.\nData augmentation requires distinct techniques depending\nonthemodalityandtask.Forinstance,imageandvideodata\nmay use techniques like dropout [76, 77], image rotation\n[78], random cropping [78], occlusion [79], and jittering\n[80],whilenaturallanguageprocessingmayusenoise-based\nmethods [81, 82].\nOn the one hand, skeleton sequences are non-Euclidean\ndata, so augmentation methods used for images and videos\ncannot be directly transferred to them. On the other hand,\nskeleton sequences exhibit minimal redundancy and carry\na high degree of semantic information. Consequently, to\neffectively learn representations for 3D skeleton data, stan-\ndard RGB-based augmentations, such as color distortion or\nGaussianblurring,areinadequate.Instead,skeleton-specific\naugmentation strategies are required to ensure the learned\nfeatures capture the spatiotemporal dynamics of the joints.\n4.1. Normal Augmentations\nNormal Augmentations [83, 84], an early approach\nto data augmentation, include techniques such as Spatial\nFlip,Rotation,GaussianNoise,GaussianBlur,andChannel\nMasking. This paradigm leverages the pattern invariance\nPage 6 of 30\n--- Page 7 ---\n3D Skeleton-Based Action Recognition: A Review\nin skeleton sequences, encouraging models to learn in-\ntrinsic action patterns across various transformations. Due\nto its robustness, this approach has been widely adopted\nin skeleton-based action recognition models [35, 85, 69,\n66, 71, 72, 44, 31]. Yan et al. [35] used random moving\nand sampling during training to augment skeleton data and\nmitigate overfitting. Subsequent works [85, 69, 66] used\nsimilartechniques.SinceZhangetal.[86]proposedrandom\nrotation of 3D skeletons at the sequence level, state-of-the-\nart methods [71, 72, 44] have adopted this technique.\nInaddition,inself-supervisedtasksbasedoncontrastive\nlearning, numerous studies [83, 84, 49, 87, 88, 89, 90, 91]\nhaveexploredinnovativeskeletondataaugmentationstrate-\ngies to enhance contrastive learning. These augmentations\ndrivethemodeltolearnintrinsicactionpatternsbyleverag-\ning diverse skeleton transformations, further improving its\nabilitytocapturemeaningfulrepresentations.Raoetal.[83]\nproposed innovative skeleton data augmentation strategies\ntogeneratequeryandkeyskeletonsequencesasaugmented\ninstancesforcontrastivelearning.Theyintroducesevenaug-\nmentationtechniques,includingrotation,shearing,flipping,\nGaussian noise, Gaussian blur, joint masking, and channel\nmasking. These methods aim to simulate diverse viewpoint\nchanges and noise conditions through random transforma-\ntionsanddataperturbations,therebyenhancingthemodel’s\nrobustness for action recognition and enabling it to capture\nintrinsic action patterns. Similarly, Thoker et al. [84] pre-\nsentedasuiteofspatialandtemporalskeletonaugmentation\ntechniques to generate positive pairs for 3D skeleton-based\naction sequences. Their approach includes pose augmen-\ntation, joint jittering, and temporal crop-resize, which are\ncombinedtoformacomprehensivespatio-temporalskeleton\naugmentation pipeline. This strategy encourages the model\nto focus on the spatiotemporal dynamics of skeleton-based\nactionsequenceswhilemitigatingtheinfluenceofconfound-\ning factors such as viewpoint variations and exact joint\npositions.\n4.2. Extreme Augmentations\nHowever,thosenormalaugmentationslimittheencoder\nfrom exploring novel patterns exposed by other augmenta-\ntions. Extreme augmentations [92] introduce more move-\nment patterns for learning general feature representations,\nincluding four spatial augmentations: Shear, Spatial Flip,\nRotate, Axis Mask, two temporal augmentations: Crop,\nTemporal Flip, and two spatio-temporal augmentations:\nGaussian Noise and Gaussian Blur. While normal augmen-\ntations show robustness, they lack effective design to fully\nleverage strong augmentations, leaving their utility under-\nexplored. To address this, Zhang et al. [93] proposed a pro-\ngressive strategy generating ordered positive pairs through\nincremental augmentation, incorporating strong augmen-\ntations to enhance representation learning. Balancing the\nextraction of rich augmented patterns with the preservation\nof motion information is crucial, particularly when dealing\nwithextremeaugmentations.Toaddressthis,Linetal.[94]\nproposed motion-adaptive data transformations based on\n__\nPool\nchannelspatial\ntemporalSAFPiXi\ntransXXi\nData 2t+ 1t+ t\nh∆ctvihctvihctvi\niX Data 2t+ 1t+ t_\nX Average Motion 2t+ 1t+ t\nc h∆ctvihctvi\niα PoolMultiply\ni\ntvA Actionlet2t+ 1t+ tActionlet SelectionUnsupervised \nkg\n_)z,izsim(\nkgkfkf\ni\nactX Actionlet Part \n2t+ 1t+ ti\nnonX Actionlet Part -Non\n2t+ 1t+ tactT\nKLL\nkg MLPqg MLP\nkf GCNqf GCN\ni\nkXactMotion-Adaptive Data Transformation\nT\nFigure 2. The pipeline of actionlet-dependent contrastive learning. In unsupervised actionlet selection, we employ the difference from the\naverage motion to obtain the region of motion. For contrastive learning, we employ two streams, i.e., the online stream and the offline\nstream. The above stream is the online stream, which is updated by gradient. The below is the offline stream, which is updated by\nmomentum. We get the augmented data Xi\ntransby performing motion-adaptive data transformation (MATS) on the input data Xi\nqwith the\nobtained actionlet. In offline feature extraction, we employ semantic-aware feature pooling (SAFP) to obtain the accurate feature anchor.\nFinally, utilizing similarity mining, we increase the similarity between positives and decrease the similarity between negatives.\n3.2. Actionlet-Guided Contrastive Learning\nTo take full advantage of the actionlet, we propose an\nactionlet-dependent contrastive learning method, shown in\nFig.2. We impose different data transformations for differ-\nent regions by a motion-adaptive data transformation strat-\negy module (MATS). Moreover, the semantic-aware feature\npooling module (SAFP) is proposed to aggregate the fea-\ntures of actionlet region for better action modeling.\nMotion-Adaptive Transformation Strategy (MATS). In\ncontrastive learning, data transformation Tis crucial for\nsemantic information extraction and generalization capac-\nity. How to design more diverse data transformations while\nmaintaining relevant information for downstream tasks is\nstill a challenge. Too simple data transformation is limited\nin numbers and modes and cannot obtain rich augmented\npatterns. However, data transformations that are too diffi-\ncult may result in loss of motion information. To this end ,\nwe propose motion-adaptive data transformations for skele-\nton data based on actionlet. For different regions, we pro-\npose two transformations, actionlet transformation and non-\nactionlet transformation.\n•Actionlet Transformation Tact: Actionlet data trans-\nformations are performed within the actionlet regions. In-\nspired by the previous work [ 8], we adopt four spatial data\ntransformations {Shear, Spatial Flip, Rotate, Axis Mask };\ntwo temporal data transformations {Crop, Temporal Flip };\nand two spatio-temporal data transformations {Gaussian\nNoise, Gaussian Blur }.\nBesides, Skeleton AdaIN is proposed as a mixingmethod of global statistics. We randomly select two skele-\nton sequences and then swap the spatial mean and tempo-\nral variance of the two sequences. This transformation is\nwidely used in style transfer [ 12]. Here, we are inspired by\nthe idea of style and content decomposition in style trans-\nfer and regard the motion-independent information as style\nand the motion-related information as content. Therefore,\nwe use Skeleton AdaIN to transfer this motion independent\nnoise between different data. The noisy pattern of the data\nis thus augmented by this transfer method. This transforma-\ntion can be formalized as:\nXi\nadain=σ(Xj)/parenleftbiggXi−µ(Xi)\nσ(Xi)/parenrightbigg\n+µ(Xj), (6)\nwhere σ(·)is the temporal variance ,µ(·)is the spatial mean,\nandXjis a randomly selected sequence. All these data\ntransformations maintain the action information.\n•Non-Actionlet Transformation Tnon: To obtain\nstronger generalization, several extra data transformations\nare applied to the non-actionlet regions in addition to the\nabove data transformation.\nWe apply an intra-instance data transformation {Random\nNoise}and an inter-instance data transformation {Skeleton\nMix}. The random Noise has larger variance. Skele-\nton Mix is an element-wise data mixing method, includ-\ning Mixup [ 44], CutMix [ 43], and ResizeMix [ 25]. Be-\ncause these transformations are performed on non-actionlet\nregions, they do not change the action semantics. There-\nfore, the transformed data are used as positive samples with\nthe original data.\n2366TnonFigure 6: Thepipelineofmotion-adaptivedatatransformation.\nactionlets,applyingextremeaugmentationstonon-actionlet\nregions, as shown in Fig. 6. These methods enhance the\nmodel’s ability to learn intrinsic patterns and improve the\neffectiveness of contrastive learning.\n4.3. Mixing Augmentations\nCurrent methods for normal [83, 35, 86] or extreme\n[92, 93] augmentations primarily apply global rotations or\nslight perturbations to skeleton data, ensuring that the se-\nmantic information remains intact during the data augmen-\ntation process. Given the inherent challenge in augmenting\nindividual skeleton data samples, a feasible approach is\nto consider data augmentation across multiple samples. In\nrecentyears,variousmix-baseddataaugmentationmethods\n[95, 96, 97, 98, 99, 100] have emerged in domains such as\nimage classification, fine-grained image recognition, graph\nclassification, and object detection, aiming to improve the\ngeneralization capabilities of neural networks and perfor-\nmance in other complex tasks. Therefore, inspired by the\nsuccessofmix-baseddataaugmentationtechniquesinother\ndomains, several studies [101, 102, 103] have proposed\nmixing-based augmentation methods tailored for skeleton-\nbased action recognition.\nZhang et al. [101] pioneered the application of mix-\nbased augmentation methods to skeleton data, introduc-\ning techniques such as CutMix [99], ResizeMix [97], and\nMixup [100] as inter-skeleton transformations to generate\nmixed skeleton datasets. Building on this, they addressed\nthe challenge of long-tailed distributions prevalent in deep\nlearning by proposing a spatial-temporal mixing strategy\n[102].Thisapproachexplicitlylinkscriticalmotionpatterns\nwith high-level semantics, facilitating more robust decision\nboundary learning for minority classes. Similarly, Xiang et\nal.[103]proposedJointMixingDataAugmentation(JMDA)\nforskeleton-basedactionrecognition,incorporatingTempo-\nralMix (TM) and SpatialMix (SM). Recently, to facilitate\nstabletrainingoftransformer-basedmodels,whichtypically\nrequirelargeamountsofdata,Doetal.[75]proposedacom-\nprehensive set of augmentation techniques. These include\nintra-instance augmentations, which apply standard trans-\nformations,andinter-instanceaugmentations,whichinvolve\nmixingdataacrossinstances.Thesemethodsaimtoenhance\nmodelperformancebymitigatingoverfittingandimproving\ngeneralization, effectively addressing the limitations posed\nby the scarcity of skeleton data.\nPage 7 of 30\n--- Page 8 ---\n3D Skeleton-Based Action Recognition: A Review\n4.4. Viewpoint-Invariant Transform\nHuman actions are often observed from arbitrary cam-\nera viewpoints in real-world scenarios, which results in\nsome datasets containing data captured from multiple per-\nspectives. One of the primary challenges in skeleton-based\nhuman action recognition lies in the complex viewpoint\nvariations inherent in the data. Skeletal representations of\nthe same posture can vary significantly when viewed from\ndifferent angles, making action recognition a particularly\ndifficult task in practice [104, 105]. To better mitigate the\nchallenges posed by viewpoint variations, it is often neces-\nsarytopreprocesstherawskeletondatabeforeconvertingit\ninto a structured format.\nOne common approach to addressing this challenge of\nviewpoint variations is to apply a preprocessing step that\ntransforms 3D joint coordinates from the camera’s coordi-\nnate system to a person-centered coordinate system. This is\ntypically achieved by aligning the body center to the origin\nandrotatingtheskeletonsothatthebodyplaneisparallelto\nthe(𝑥, 𝑦)plane,therebymakingtheskeletaldatainvariantto\nabsolute position and body orientation [106, 107, 108, 30].\nFor instance, Lee etal. [109] converted the original skeletal\nsequenceintoahuman-perception-basedcoordinatesystem,\nmitigating confusion caused by inconsistent orientations in\nthe skeletal sequence and improving robustness to scaling,\nrotation, and translation of skeletal data.\nHowever,theapproachesmentionedabovehavenotable\nlimitations. First, the method relies on defining the body\nplane using joints such as the \"hip\", \"shoulder\" or \"neck\"\nwhich may not always suit alignment purposes due to the\nnon-rigid nature of the human body. Second, these trans-\nformations can result in the loss of critical motion infor-\nmation, such as the trajectory and speed of the body center\nor the dynamic changes in body orientation. For example,\nthe action of walking may be reduced to walking in place,\nand a dance involving body rotation may appear as dancing\nwith a fixed orientation. Finally, the preprocessing steps\n(e.g.,translationandrotation)aredesignedbasedonhuman-\ndefined heuristics rather than being optimized explicitly for\naction recognition tasks, limiting the potential to exploit\noptimal viewpoints effectively. Hence, Zhang et al. [110]\ndesigned a view adaptive recurrent neural network (RNN)\nwith LSTM architecture, which enables the network itself\nto adapt to the most suitable observation viewpoints from\nend to end. Friji et al. [111] proposed a geometry-based\ndeep learning method, which optimizes rigid and non-rigid\ntransformationstofilterouttheeffectsofposition,scale,and\nrotation.\n5. Skeletal Data Representation\nGenerally, data augmentation techniques are directly\napplied to the unstructured raw skeleton sequences, which\naretypicallyinaformthatmodelscannotdirectlyinterpret.\nTo enable models to effectively learn from skeleton data,\nthe next step involves converting the raw skeleton data into\na format compatible with input requirements for RNNs,\nCNNs, GCNs or Transformers. In this process, the key\nt\n4\n5\n9832\n671\n1\n10\n114 24 4\n14 24 4\n16 26 6\n16 26 6\n18 28 8\n18 28 8jointframe1...\n......\n......\n......11\n11\n11\n11\n11\n11t. . .1 t. . .1 t. . .1\n(b) Pseudo-image Representation\nID\n5\n4\n...\n10\n111\n4\n5\n9832\n67\n12\n310\n11t\n1...\nt1\n4\n5\n9832\n67\n10\n111\n4\n5\n9832\n67\n10\n11\n(c) Graph-structured Representation1...(a) Sequential Representation\n1\n4\n5\n9832\n67\n10\n11\nK\nQi j\ni\nj???6,7\n(d) Token-based Representation1...tFigure 7: Unstructured skeleton sequences are converted into\nfour distinct input formats for further processing.\nchallengeistoefficientlytransformthesparse,unstructured\nskeletonactionsequencesintoastructuredformatthatdeep\nlearningmodelscanunderstandwhilepreservingasmuchof\nthe original spatial and temporal dependencies as possible.\nInparticular, graph-structuredrepresentation withGCNs\nconvert the skeleton into a graph that mirrors the human\nanatomicaltopologyandwillbeintroducedingreaterdetail.\n5.1. Raw Skeleton Sequences: 2D vs. 3D\nSkeleton data can be represented in both 2D and 3D\nformats, and both forms are capable of supporting action\nrecognition tasks [112, 113]. However, compared to 2D\nskeletons, 3D skeletons provide additional depth informa-\ntion,whichsignificantlyenhancesthepreservationofspatial\ndetails that are crucial for the quality of subsequent model-\ning.Furthermore,withtheadvancementofdepthimagesen-\nsors,inertialsensors,andhumanposeestimationalgorithms,\nobtaining 3D skeleton data has become increasingly easier.\nAs a result, 3D skeleton-based benchmarks have gained\ngreater popularity in human action recognition tasks, and\nmethodsrelyingon3Dskeletondataarenowmoreprevalent.\nUnless otherwise stated, this paper primarily focuses on 3D\nskeleton data.\n5.2. Sequential Representation with RNNs\nIn the early stages of research, models such as stan-\ndard RNNs, LSTMs, and GRUs have demonstrated signif-\nicant effectiveness in processing sequential data [29, 114,\n31, 115, 68, 62, 63, 109, 110, 111]. Fig.7 (a) illustrates\ninterpreting the skeleton sequence as a sequential repre-\nsentation. Consequently, a critical initial consideration is\nhow to effectively transform skeletal data into a sequen-\ntial format suitable for these models while preserving the\nspatiotemporal information inherent in the original skeletal\ndata. However, most early works simply arrange joints in\na linear sequence, overlooking the kinematic dependencies\nbetween them. This approach introduces false connections\nbetween body joints that are not strongly related, leading\nto inaccuracies in modeling the human body’s structure.\nPage 8 of 30\n--- Page 9 ---\n3D Skeleton-Based Action Recognition: A Review\nLiu et al. [29, 114] considered the natural structure and\nspatial relationships of human joints by modeling them\nas tree-structured sequential representations. This approach\neffectivelypreservestheinherenthierarchicalandadjacency\nrelationshipsbetweenjoints,therebycapturingricherspatial\ndependency features. Similarly, Wang et al. [31] proposed\ntwo methods for converting the skeleton structure into a se-\nquencebeforeapplyinganRNNtocapturespatialdependen-\ncies. One of these methods is graph traversal-based, where\njoints are accessed sequentially based on their adjacency\nrelationships,ensuringthatthespatialstructureofthegraph\nis preserved.\n5.3. Pseudo-image Representation with CNNs\nThe representation of skeleton data for CNN-based\nmethods has evolved significantly to effectively encode\nspatiotemporalinformation.Fig.7(b)illustratesinterpreting\nthe skeleton sequence as a pseudo-image representation.\nTo meet the input requirements of CNNs, 3D skeleton\nsequence data is often transformed from a vector sequence\ninto a pseudo-image. However, creating a representation\nthat captures both spatial and temporal information can\nbe challenging. To address this, many researchers have\nencoded skeleton joints into multiple 2D pseudo-images,\nwhich are then fed into CNNs for efficient feature learning\n[116, 117, 118, 119, 64, 25, 120, 64].\nWang [118] proposed the Joint Trajectory Maps (JTM),\nwhich represent spatial configuration and dynamics of joint\ntrajectories into three texture images through color encod-\ning. However, this method is a little complicated and also\nloses importance during the mapping procedure. To tackle\nthis shortcoming, Li et al. [119] used a translation-scale\ninvariantimagemappingstrategy,whichfirstdividedhuman\nskeleton joints in each frame into five main parts according\nto the human physical structure, and then those parts were\nmappedto2Dform.Thismethodmakestheskeletonimage\nconsist of both temporal and spatial information. However,\nthough the performance was improved, there is no reason\nto treat skeleton joints as isolated points, because in the\nreal world, an intimate connection exists among our body\nparts. For example, when waving the hands, not only the\njoints directly within the hand should be considered, but\nalsootherpartssuchasshouldersandlegsareimportant.Li\net al. [64] proposed the shape-motion representation from\ngeometric algebra, which addressed the importance of both\njoints and bones and fully utilized the information provided\nbytheskeletonsequence.Similarly,Liuetal.[25]alsoused\ntheenhancedskeletonvisualizationtorepresenttheskeleton\ndata,andCaetanoetal.[120]proposedanewrepresentation\nnamed SkeleMotion based on motion information, which\nencodes the temporal dynamics by explicitly computing the\nmagnitudeandorientationvaluesoftheskeletonjoints.Fig.\n8(a)showstheshape-motionrepresentation[64],whileFig.\n8(b) illustrates the SkeleMotion representation. Moreover,\nsimilar to SkeleMotion, Caetano et al. [121] adopted the\nSkeleMotion framework, but based it on a tree structureandreferencejointsforskeletonimagerepresentation.Previ-\nousmethodsusuallyutilizebodyjoint-basedrepresentation,\nwhile leaving edge-based movement poorly investigated.\nThe movement of the skeleton edge provides richer infor-\nmation than body joints for recognizing a particular action.\nSome human actions, such as playing the guitar, involve\nrepeatedly swinging one part of the body, which is not\neasy to identify from the perspective of motion because the\ncoordinatesofthebodyjointsdonotchangemuch.Wanget\nal.[122]proposedthenovelskeletonedgemotionnetworks\n(SEMN)tofurtherexplorethemotioninformationofhuman\nbody parts. Specifically, SEMN addresses the movement of\ntheskeletonedgebyusingtheanglechangesoftheskeleton\nedge and the movement of the corresponding body joints.\n5.4. Graph-structured Representation with GCNs\nInspired by the fact that human 3D skeleton data natu-\nrallyformsatopologicalgraph,ratherthanasequencevector\norpseudo-imageasusedinRNN-basedorCNN-basedmeth-\nods, Graph Convolutional Networks (GCNs) have recently\nbeen adopted for skeleton-based action recognition. Fig.7\n(c) illustrates interpreting the skeleton sequence as a graph-\nstructuredrepresentation.Thisapproachleveragestheinher-\nent graph structure of the data [35, 123, 66, 86, 124]. The\ncoreideaistomodeltheskeletonasaspatiotemporalgraph,\nwherenodesrepresentjointsandedgesencoderelationships\nboth within the body and across time. This representation\nhas demonstrated compelling results, further validated by\nstrong performance on benchmark datasets. Specifically,\nin practical implementation, the skeleton topology is rep-\nresented as an adjacency matrix. For clarity, we use this\nadjacency matrix to depict the connections between human\nbody joints in this section. By repeatedly multiplying the\nadjacencymatrix 𝑙times,weobtainahigh-orderpolynomial\nof the adjacency matrix. The 𝑙-th order polynomial reflects\nthe aggregation of information from 𝑙-hop neighbor nodes.\nStructural links are derived by combining polynomials of\ndifferent orders in varying proportions.\nFixed Topological Representation. The most straight-\nforward approach is to construct a fixed adjacency matrix\nbased on the topological connectivity between joints to\nrepresent the graph structure. However, using a naturally\nconnected skeleton topology as the adjacency matrix may\nnot always be the optimal choice. Consequently, consider-\nableefforthasbeendedicatedtooptimizingthestructureof\nthe skeleton topology.\nYanetal.[35]firstintroducedanovelmodelforskeleton-\nbased action recognition, called Spatial-Temporal Graph\nConvolutional Networks (ST-GCN). In this approach, they\nconstructed a spatiotemporal graph, where the joints are\nrepresented as graph vertices, and the natural connectivity\nwithin the human body structure and across time serves\nas graph edges. The adjacency matrix partitioning strategy\nused in ST-GCN divides the matrix into three parts: the\ncenter node itself, nodes close to the center of gravity, and\nnodesfurtherawayfromthecenter.Thispartitioningscheme\nrestricts nodes from interacting only with their immediate\nPage 9 of 30\n--- Page 10 ---\n3D Skeleton-Based Action Recognition: A Review\nwhereRt\nlis a time-dependant rotor. In this manner we have\nplaced the rotational motion of bt\nlinRt\nl, which is given by\nRt\nl=e x p/parenleftbigg\n−ψ\n2Ωt\nl/parenrightbigg\n(13)\nwhereψ=a r c o s/parenleftbig\nbt\nl·bt+1\nl/slashbig\n|bt\nl|/vextendsingle/vextendsinglebt+1\nl/vextendsingle/vextendsingle/parenrightbig\n, and the bivetor Ωt\nl=\nbt\nl∧bt+1\nldeﬁnes the plane of the rotation in G/prime\n3.\nThe Rotors {Rt\nl,t∈(1,2,···,T−1)} form a\ncontinues group called Lie group , and the bivectors\n{Ωt\nl,t∈(1,2,···,T−1)}form their Lie algebra . There-\nfore, we take Ωt\nlas the motion vector of bt\nl. However,\nclassifying {Ωt\nl,t∈(1,2,···,T−1)}of a skeleton se-\nquence in G/prime\n3is not a trivial task. To overcome this difﬁculty,\nwe project Ωt\nlto bivector basis in G/prime\n3for parameterization,\nwhich is detailed as follows.\nSuppose that two bones bt\nl=3/summationtext\nk=1yl,t,kekandbt+1\nl=\n3/summationtext\nk=1yl,(t+1),kek, thenΩt\nlcan be written as follows.\nΩt\nl=bt\nl∧bt+1\nl\n=αt\nl(e/prime\n1∧e/prime\n2)+βt\nl(e/prime\n2∧e/prime\n3)+γt\nl(e/prime\n3∧e/prime\n1)(14)\nwhereαt\nl=yl,t,1yl,(t+1),2−yl,t,2yl,(t+1),1,βt\nl=\nyl,t,2yl,(t+1),3−yl,t,3yl,(t+1),2andγt\nl=yl,t,3yl,(t+1),1−\nyl,t,1yl,(t+1),3.\nHence, the motion vector of bt\nlis given by Ωt\nlΔ=\n[αt\nl,βt\nl,γt\nl]. AndΩ= ∪\nl∈(1,2,···,N−1),t∈(1,2,···,T−1)Ωt\nlis de-\nnoted as Bone-Motion.\nSumming up the above, we obtain four shape-motion rep-\nresentations of a skeleton sequence, including Joint-Shape P,\nJoint-Motion V, Bone-Shape Band Bone-Motion Ω,w h i c h\nare complementary to each other and provide the complete\ninformation for describing skeleton actions.\nInspired by [11], we encode {P,V,B,Ω}to color images\n{In,n=1,2,3,4}, which are beneﬁcial to subsequent CNN\nmodels to learn more abundant and distinctive discriminan-\nt features for the task of skeleton-based action recognition.\nFig. 2 shows the skeleton sequence shape-motion representa-\ntions generated from the action “pick up with one hand” on\nNorthwestern-UCLA dataset.\nGiven a skeleton sequence I, four encoding images\n{In,n=1,2,3,4}are generated and fed into a multi-stream\nCNN model. For each image In, the posterior probability\noutputted by the CNN is denoted as prob(l|In). Then the\ndecision-level average fusion strategy is used for the score\ncalculation of I, shown as follows.\nscore(l|I)=1\nNN/summationdisplay\nn=1prob(l|In) (15)\nwhich indicates the ﬁnal score of Ibelonging to the l-th ac-\ntion class, and Nis the number of CNN channels participating\nin the fusion.\nJoint-Shape\n Joint-Motion\n Bone-Shape\nFig. 2 . Skeleton sequence shape-motion representations gen-\nerated from “pick up with one hand” on Northwestern-UCLABone-Motion\ndataset.\n4. EXPERIMENTS\n4.1. Datasets and Protocols\nNTU RGB+D dataset (NTU) [6] is currently the largest\nskeleton-based action recognition dataset. It contains 60 ac-\ntions performed by 40 subjects from three cameras, gener-\nating total 56880 skeleton sequences. We follow the cross-\nsubject (CS) and cross-view (CV) evaluation protocols in [6].\nFor the CS protocol, we split the 40 subjects into training and\ntesting groups. For the CV protocol, we use samples of cam-\neras 2 and 3 for training and samples of camera 1 for testing.\nNorthwestern-UCLA dataset [3] contains 1494 skeleton se-\nquences of 10 classes performed 1 to 6 times by 10 subjects\nfrom three views. Following the evaluation protocol in [3],\nwe use all the samples captured from the ﬁrst two views as\ntraining data, and the samples of the third view are used for\ntesting.\n4.2. Implementation Details\nIn our multi-stream CNN model, each CNN contains ﬁve con-\nvolutional layers and three FC layers. And we use pretrained\nVGG model which have been preliminarily trained on Ima-\ngeNet. The number of neurons in the last FC layer is equal to\nthe total number of human action classes for different dataset-\ns. The dropout with a probability of 0.5 is used to alleviate\noverﬁtting. The momentum value is set to 0.9 and the weight\ndecay is set to 0.00005. Initial learning rate is set to 0.001\nand the network parameters are learned using the mini-batch\nstochastic gradient descent. The batch sizes are 50 and 16 for\nNTU and N-UCLA datasets, respectively.\n4.3. Experimental Results and Discussions\n4.3.1. Evaluation of Skeleton Sequence Shape-Motion Rep-\nresentations\nTable 1 shows the results of different skeleton sequence\nshape-motion representations, i.e., Joint-Shape ( JS), Joint-\nMotion ( JM), Bone-Shape ( BS) and Bone-Motion ( BM), as\nwell as the fusion by averaging the predicted scores on NTU\nand N-UCLA datasets.\n1069\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on December 11,2024 at 07:00:44 UTC from IEEE Xplore.  Restrictions apply. \n(a)\n82\n14\n3\n9 5\n6\n7\n(a) (d)T\n1\n(b)\n (c)\n𝜃′𝑧𝑥\n𝜃′𝑦𝑧\n𝜃′𝑥𝑦\n123456. . . t= 0\n123456. . . t= 1\n123456. . . t= 2\n123456. . . t= 3\n123456. . . t= T-1\n123456. . . t= T2425\n2425\n2425\n2425\n2425\n2425\n...\n𝑀123456. . . t= 0\n123456. . . t= 1\n123456. . . t= 2\n123456. . . t= 3\n123456. . . t= T-1\n123456. . . t= T2425\n2425\n2425\n2425\n2425\n2425\n...Compute\nOrientation (θ’)\nCompute\nMagnitude (M)2512\n24 2217 13\n18 14\n19 15\n20 1621\n1110\n23Figure 1. SkeleMotion representation. (a) Skeleton data sequence of Tframes. (b) Computation of the magnitude and orientation from the\njoint movement. (c) θ/primeandMarrays: each row encodes the spatial information (relation between joint movements) while each column\ndescribes the temporal information for each joint movement. (d) Skeleton image after resizing and stacking of each axes.\nnumber of frames may vary depending on the skeleton se-\nquence of each video. Figure 1 gives an overview of our\nmethod for building the SkeleMotion representation.\n3.1.1 Temporal Scale Aggregation (TSA)\nSkeleton image representations in the literature basically\nencodes joint coordinates as channels. In view of that, it\nmay cause a problem that the co-occurrence features are ag-\ngregated locally, being not able to capture long-range joint\ninteractions involved in actions [14]. Moreover, one draw-\nback of encoding motion values of joints is the noisy values\nthat can be introduced to the representation due to small\ndistancedbetween two frames. For instance, if the compu-\ntation is performed considering two consecutive frames, it\ncould add to the representation unnecessary motion of joints\nthat are irrelevant to predict a speciﬁc action (e.g., motion\nof the head joint on a handshake action).\nTo overcome the aforementioned problems, we also pro-\npose a variation of our SkeleMotion representation by pre-\ncomputing the motion structure Dconsidering different d\ndistances. For each of the motion structures D, we com-\npute its respective magnitude skeleton representation M\nand then stack them all into one single representation. The\nsame is applied to compute the orientation skeleton repre-\nsentationθ, however a weighting scheme is applied during\nthe ﬁltering process explained before, as\nθ/prime\nc,t=/braceleftbigg0,ifMc,t<m×d\nθc,t,otherwise. (5)\nSuch technique adds more temporal dynamics to the repre-\nsentation by explicitly showing temporal scales to the net-\nwork. In this way, the network can learn which movements\nare really important for the action learning and also being\nable to capture long-range joint interactions.4. Experimental Results\nIn this section we present the experimental results ob-\ntained with the proposed skeleton image representation for\nthe 3D action recognition problem. We compare it to\nother skeleton representations in the literature. Besides the\nclassical skeleton image representation of Du et al. [4],\nwe compare with other representations used by state-of-\nthe-art approaches [32, 10, 13, 14, 34] as baselines on\nNTU RGB+D 60 [25]. We also compare our approach to\nsate-of-the-art methods on the NTU RGB+D 120 [15]. To\nisolate only the contribution brought by SkeleMotion to the\naction recognition problem, all other representations were\ntested on the same datasets with the same split of training\nand testing data and using the same CNN.\n4.1. Datasets\nTheNTU RGB+D 60 [25] is publicly available 3D ac-\ntion recognition dataset. It consists of 56,880 videos from\n60 action categories which are performed by 40 distinct sub-\njects. The videos were collected by three Microsoft Kinect\nsensors. The dataset provides four different data informa-\ntion: (i) RGB frames; (ii) depth maps; (iii) 395 infrared\nsequences; and (iv) skeleton joints. There are two different\nevaluation protocols: cross-subject, which split the 40 sub-\njects into training and testing; and cross-view, which uses\nsamples from one camera for testing and the other two for\ntraining. The performance is evaluated by computing the\naverage recognition across all classes.\nTheNTU RGB+D 120 [15] is a large-scale 3D action\nrecognition dataset captured under various environmental\nconditions. It consists of 114,480 RGB+D video samples\ncaptured using the Microsoft Kinect sensor. As in NTU\nRGB+D 60 [25], the dataset provides RGB frames, depth\nmaps, infrared sequences and skeleton joints. It is com-\n(b)\nFigure 8: Examples of the proposed representation skeleton\nimage. (a) shows Skeleton sequence shape-motion represen-\ntations generated from \"pick up with one hand\" action. (b)\nshows the SkeleMotion representation workflow.\nneighbors, which fails to capture the broader interactions of\nmanynodesinhumanmovement.Toaddressthislimitation,\nWang et al. [125] extended the adjacency matrix into five\nparts, incorporating two additional sets of nodes—one near\nthe central node and the other farther away. This modifica-\ntionenableseachnodetointeractwiththoseuptotwohops\naway, thus providing a more comprehensive representation\nof long-term and global motion characteristics. To cap-\nture richer dependencies, Li et al. [123] introduced Action-\nStructural Graph Convolutional Networks (AS-GCN). In\naddition to using Actional Links to model specific latent\ndependenciesrelatedtoactions,theyalsoextendedtheexist-\ning skeleton graph to represent higher-order dependencies,\nknown as Structural Links. These two types of links are\ncombinedintoageneralizedskeletongraph,whichimproves\nfeature learning and enables the model to construct a more\ngeneralized representation of the skeleton.\nHowever, approaches mentioned above may introduce\nnoise from uncorrelated nodes, which can negatively affect\nthe neural network. To mitigate this issue and capture de-\npendenciesbetweennon-physicallyconnectedjoints,Haoet\nal.[126]proposedHyper-GNN.Inthismethod,hypernodes\nare selected from the top 50 %of joints with the longest\nmovementdistanceswithinthevideosequence.Hyperedges\nconnect these distant, non-adjacent nodes, enabling Hyper-\nGNN to extract complex higher-order spatial-temporal fea-\ntures.Similarly,Leeetal.[127]introducedHD-GCN,which\ndecomposes the human skeleton graph into six hierarchical\nsubsets to emphasize the distance relationships between\njoints. The HD-Graph structure, shown in Fig. 9, includes\nboth physically connected edges and fully connected edges,\nallowingnodesthatarenotnaturallyconnectedtobelinked\nwithin subgraphs. This improves the transmission of infor-\nmation between joints with hierarchical distances. Qin et\nal.[128]proposedamethodthatdown-samplestheoriginal\nskeleton topology into six compressed topologies in stages.Eachcompressionstepaggregateshigher-orderneighborin-\nformation,enhancingGCN’sabilitytoidentifyrelationships\nbetween joints at varying distances.\nDynamic Topological Representation. However, the\nskeleton topology graph used by ST-GCN is handcrafted\nand fixed, limiting the neural network to learning only the\nrelationships between naturally connected joints. This con-\nstraint impedes the ability of GCNs to fully extract and\naggregate relevant features from the skeleton data. Another\nfundamental challenge with GCNs is their limited capacity\nto model long-term dependencies between distant joints.\nFor example, in the ’clapping’ action, the joints on the two\npalms are spatially distant in the skeleton topology, making\nit difficult for the graph convolution operation to capture\nthe relationships between these joints. These challenges in\nGCNs can be summarized as: (1) how to extract richer\nfeature representations from the skeleton topology, and\n(2)howtomodellong-termdependenciesbetweenjoints.\nAddressing these issues calls for more dynamic topological\nrepresentations of skeleton data.\nTo extract and learn richer feature representations, Shi\n[66] proposed 2s-AGCN, which constructs an adaptive\ntopology graph. This graph is updated automatically via\nthe backpropagation algorithm, enabling it to more effec-\ntivelycapturethejointconnectionstrength.Specifically,the\nadaptiveadjacencymatrixconsistsofthreecomponents:(1)\nan adjacency matrix representing the physical connections\nbetween body joints, (2) a purely parameterized adjacency\nmatrix that is jointly optimized with the network’s other\nparameters during training, and (3) a sample-specific adja-\ncencymatrixtailoredtoeachindividualinputsample.Yeet\nal.[124] proposedDynamic GCN,which utilizesacontext-\nencodingnetworktomodeladynamicadjacencymatrixthat\nincorporates global contextual information. The context-\nencodingnetworkconsistsofthree 1×1convolutionallayers,\nwhich compress the skeleton feature maps along both the\nchannel and temporal dimensions, and then reshape them\nto match the size of the adjacency matrix. Chen et al. [71]\nproposedtheCTR-GCNwhichmodelschannel-wisetopolo-\ngies by dividing the channels into groups and only sharing\nthe skeleton topology between groups. CTR-GCN learns\na group-shared topology and channel-specific correlations\nsimultaneously to enhance the feature extraction and aggre-\ngation ability of GCNs without significantly increasing the\nparameters.Liuetal.[129]furtherproposedDynamic-Static\nMulti-Graph Convolution (TSGCNeXt) which expands the\nchannelnumberofskeletonfeaturesbyfourtimes,mapping\nskeleton features into higher-dimensional feature spaces to\ncapture discriminative potential spatial dependencies.\n5.5. Token-based Representation with\nTransformers\nIn recent years, with the impressive performance of\nTransformersacrossvariousresearchfields,manyTransformer-\nbased methods have emerged [39, 38, 130, 40, 131]. Fig.7\n(d) illustrates interpreting the skeleton sequence as a token-\nbased representation. The Transformer architecture is built\nPage 10 of 30\n--- Page 11 ---\n3D Skeleton-Based Action Recognition: A Review\nSet  ଵ\nSet  ଶ\nSet  ଷ\nSet  ସ\nSet  ଵ Set  ଶ Set  ଷ Set  ସ\n(a)Set  ଵ\nSet  ଶ\nSet  ଷ\nSet  ସ\n(b)Set  ଵ Set  ଶ Set  ଷ Set  ସ\nFigure 2. (a) Structure of HD-Graph with physically connected (PC) edges. The human skeleton graph is decomposed into a rooted\ntree, where PC edges are included in hierarchy sets. (b) Structure of HD-Graph with fully connected (FC) edges. Edges between all\nnodes in the same semantic space are obtained by connecting all the nodes in adjacent hierarchy edge sets. Blue and red lines stand for PC\nand FC edges, respectively.\nfeatures representing the relationships between physically\nconnected edges among human skeleton, and they outper-\nform other methods by using them to construct the major\nrelationships between joint nodes in the human skeleton.\nIn particular, [25] and [2] propose adaptive attention-based\ngraph structures to learn the sample-wise topological fea-\ntures. However, they might fall into suboptimality because\nthey do not consider the physical prior of the human skeletal\nstructure and allow too much flexibility in network training.\nTo address this issue, we introduce a novel HD-Graph, ref-\nerencing the known tendencies of human perception\n2.2. Attention Modules for Action Recognition\nThe attention mechanism is an essential element for\nconstructing a deep neural network. Using recent at-\ntention modules [10, 34], networks emphasize important\ninformation along a specific dimension. For example,\nHuet al. [10] applies channel-wise attention, and Woo et\nal. [34] applies both channel-wise and spatial-wise atten-\ntions. These techniques are divided into two categories\nfor GCNs: (1) attention-based graph construction [24, 2]\nwhich is a method of forming topologies using a non-\nlocal block [32] or customized correlation matrices, and (2)\nspatial-wise, temporal-wise, channel-wise attention, which\nare commonly used attentions in [25, 27], and several other\nnetworks.\n3. Methodology\nIn Sec. 3.2, we detail the HD-Graph convolution to\nsolve the problems of the conventional human-skeletongraph [35], which includes only PC edges. We also explain\nthe A-HA module in Sec. 3.3 to highlight dominant hier-\narchical features. In Sec. 3.4, we replace the widely used\nfour-stream ensemble method [25, 2, 4] with a six-way en-\nsemble without motion data streams. Finally, we introduce\nthe HD-GCN, which uses these proposed methods.\n3.1. Preliminaries\nNotations. The spatio-temporal graph for human skeleton\nis represented by G(V,E), where VandEdenote the joint\nand edge groups, respectively. Physically connected edges\nand fully-connected edges used in Sec. 3.2 are denoted as\nPC-edges and FC-edges, respectively.\nGraph Convolutional Networks. 3D time-series skele-\ntal data are represented by X∈R3×T×V, where Vand\nTare the number of joint nodes and the temporal window\nsize, respectively. GCN’s operation with input feature map\nFin∈RC×T×Vis as follows:\nFout=/summationdisplay\ns∈S/hatwideAsFinΘs, (1)\nwhere S={sid, scf, scp}denotes graph subsets, and sid,\nscf, and scpindicate identity, centrifugal, and centripetal\njoint subsets, respectively. Θsdenotes the pointwise con-\nvolution operation. The normalized adjacency matrix /hatwideAis\ninitialized as Λ−1\n2AΛ−1\n2∈RNS×V×V, where Λis a di-\nagonal matrix for normalization and NS= 3.\nFigure 9: Structure of the HD-Graph with physically connected\nedges and fully connected edges.\non the Self-Attention mechanism, which receives input as\na 1D sequence of token embeddings and computes the\nrelationships between all input vectors. However, since\nthe Transformer model itself does not inherently handle\nsequential data, it is necessary to apply position encoding\nto the input skeleton data to provide spatial information\nabout the joints within the sequence. Firstly, the input\njoints are weighted with additional information through\nposition embedding to preserve spatial relationships. The\nmostcommonapproachinskeletondataprocessingemploys\nabsolutepositionencoding,whereeachjointislabeledusing\nsine and cosine functions with varying frequencies as the\nencoding functions, as Eq.16.\n𝑷𝑬(𝒑, 𝒊)=⎧\n⎪\n⎨\n⎪⎩sin(\n𝑝\n10000𝑖∕𝐶𝑖𝑛)\nif𝑖is even,\ncos(\n𝑝\n10000𝑖∕𝐶𝑖𝑛)\nif𝑖is odd.(16)\nIn Eq.16, 𝑝and𝑖denote the position of a joint and the\ndimensionofitspositionencoding(PE)vector,respectively,\nthe resulting PE vector is then added to the original joint\nvector.Thisensureseachjointisassignedauniqueidentifier.\nSubsequently, the PE vectors are projected into the embed-\ndingsubspacethroughamappingoperation,akintotheword\nembeddings in NLP or the patch embeddings in VIT[132].\n6. Feature Extraction\nIn the field of skeleton-based human action recognition,\nfeature extraction is one of the most crucial core links of\ntheentireprocess.Thisprocessnotonlyrequiresaccurately\ncapturing the temporal dynamic changes in the skeleton se-\nquencebutalsopaysfullattentiontothespatialrelationship\nbetween joint nodes and their connection topology to facil-\nitate subsequent spatio-temporal feature modeling. Recent\nmethods have also begun to apply Large Language Models\n(LLMs), Generative Models, and other advanced learning\nmethods to the feature extraction stage. These methods can\nassist the system in automatically mining deeper and more\nabstract action patterns, thereby significantly enhancing the\nmodel’s learning ability and generalization performance.\nThe main feature extraction method is shown in Fig. 2.6.1. Temporal Feature Extraction\nTemporal Feature Extraction Using RNNs. RNN and\nits variants (e.g., LSTM) are widely used in skeleton-based\naction recognition due to their strong temporal sequence\nprocessing capabilities. Many studies focus on optimizing\nRNN and LSTM models for improved temporal feature ex-\ntraction.Kimetal.[133]proposeddRNN,amodeldesigned\nto capture salient spatio-temporal information by quantify-\ning information gain changes caused by significant motion\nbetween frames. Zheng et al. [68] developed an attention-\nbased recurrent relational LSTM network that employs a\nmulti-layer LSTM structure to capture temporal features in\nskeleton sequences. Liao et al. [134] introduced the mathe-\nmatical tool of log-signature into RNN, creating the Logsig\nRNNmodel.Thisapproachsignificantlyreducesthenumber\noftimestepsrequiredbyRNNswhileenablingthemodelto\nhandlevariable-lengthtimeserieswithoutpadding,offering\nrobustness against data loss.\nTemporal Feature Extraction Using CNNs. Temporal\nfeature extraction poses a significant challenge for convo-\nlutional neural networks (CNNs) in skeleton-based action\nrecognition. Kim et al. [135] utilized Temporal Convolu-\ntional Networks (TCN) [136], providing a clear and inter-\npretablespatio-temporalrepresentation.Liuetal.[137]pro-\nposed Skepxel, which hierarchically captured fine-grained\ntemporal relationships among joints within frames. In con-\ntrast,Rahmanietal.[138]proposedFourierTemporalPyra-\nmid, emphasizes capturing coarse-grained temporal rela-\ntionships for a broader temporal perspective. To further en-\nhancespatio-temporalfeaturerepresentation,Jiaetal.[139]\nproposed TS-TCN, which integrated inter-frame and intra-\nframe vector features for improved sensitivity to skeletal\ndynamicscomparedtotraditionaldistance-andangle-based\nfeatures. Inter-frame vector features represent the motion\nof skeletal joints between consecutive frames, while intra-\nframe vector features capture the relative positions of joints\nwithinasingleframe.Moreover,TS-TCNredesignsresidual\nblocks with varying strides along the network depth, en-\nhancing TCN’s ability to model long-term dependencies in\ncomplex action sequences effectively.\nTemporalFeatureExtractionUsingTemporalGraphs.\nGraph convolutional networks (GCNs) have been shown to\nexcelinmodelingskeletonspace,andbyincorporatingtem-\nporalinformationintographconstruction,dynamicchanges\nin the temporal dimension can be modeled. This method\nusually builds a spatio-temporal graph based on skeleton\nsequence data, where joints are treated as graph vertices\nandthespatialandtemporalrelationshipsbetweenjointsare\nrepresented as graph edges. Yan et al. [140] first introduced\nthe concept of spatio-temporal graph and proposed the\nspatio-temporal graph convolutional network (ST-GCN). In\nthismodel,thejointsintheskeletonsequencearedefinedas\ngraph vertices, the spatial connections are derived from the\nhumanbodystructure,andthetemporalconnectionsarecap-\ntured through inter-frame edges, enabling joint modeling of\nspatial and temporal features. This framework significantly\nenhances the network’s ability to represent spatio-temporal\nPage 11 of 30\n--- Page 12 ---\n3D Skeleton-Based Action Recognition: A Review\nfeatures. Li et al. [141]further refined the construction of\ntemporal graphs by introducing temporal graph routers. By\nfeeding the trajectories of skeleton sequence nodes into this\nrouter, they generate a more flexible and adaptive skeleton\njoint connection graph. This method effectively improves\nthe accuracy of temporal feature extraction and enhances\nthe model’s ability to capture complex motion patterns.\nFurthermore,Shift-GCN[85]adoptsanewmethodthatcom-\nbines shift map operations with point-wise convolutions.\nThis technology provides a more flexible receptive field\nfor temporal graphs, optimizes the aggregation of temporal\nfeatures, and further enhances spatio-temporal graph mod-\neling. Graph Convolutional Networks (GCNs) are highly\neffectiveinmodelingskeletonstructures,representingskele-\nton sequence data as a spatio-temporal graph in which\nthe joints serve as graph vertices, and the spatial-temporal\nrelationshipsarerepresentedbygraphedges.Yanetal.[140]\nfirst introduced the spatio-temporal graph concept and pro-\nposedthespatio-temporalgraphconvolutionalnetwork(ST-\nGCN). Li et al. [141] refined temporal graph construction\nby introducing temporal graph routers, which adaptively\ngenerateskeletonjointconnectionsusingskeletonsequence\ntrajectories.Cheng[85]combinedshiftmapoperationswith\npoint-wiseconvolutions,providingamoreflexiblereceptive\nfield for temporal graphs.\nTemporal Feature Extraction Using Attentions. Un-\nlike traditional methods, attention mechanisms capture de-\npendencies between time points globally, offering advan-\ntages in processing long sequences and complex spatio-\ntemporal interactions. Cho et al. [39] proposed three self-\nattention networks (SAN) to extract long-term temporal se-\nmanticfeaturesbycapturinglong-rangecorrelationsthrough\nself-attention.Plizzarietal.[130]designedaspatio-temporal\nTransformernetwork(ST-TR)thatlearnedmotiondynamics\nbetween frames through a temporal self-attention module,\ncapturingtemporalandspatialdependenciesforcomprehen-\nsiveactionrepresentation.Zhang[40]proposedaspace-time\ndedicated Transformer (STST) that integrated a direction-\ntime Transformer block with a direction-aware strategy to\nmodellong-termdependencieseffectively,whileleveraging\nmulti-task self-supervised learning to enhance robustness\nagainst skeleton data noise and improve temporal feature\nextraction.Inadditiontoexplicittemporalfeatureextraction,\nWu et al. [142] introduced a frequency-aware attention\nmodule leveraging discrete cosine transform (DCT) for\nfrequency-based temporal feature representation. However,\nattention mechanisms require significant storage for global\ninformation, leading to high computational costs. Shi et al.\n[143]addressedthiswithSTAR,asparseTransformerusing\npiecewise linear attention for efficient temporal dynamics\nand variable frame-length support.\nTemporal Feature Extraction Hierarchically. Hierar-\nchical temporal feature extraction, which decomposes dy-\nnamics into components with varied receptive fields, cap-\ntures multi-scale patterns in skeletal sequences, offering a\ncomprehensive temporal understanding of motion. Lee etal. [109] proposed TS-LSTM, integrating short-, medium-\n, and long-term networks to capture multi-scale motion\nfeatures. Wang et al. [144] used a multi-stream LSTM to\nprocesssingle-frame,short-term,andlong-termfeaturesfor\naholisticview.Toaddressnon-adjacenttemporaldependen-\ncies,Liu[145]proposedamulti-scaletemporalconvolution\nmodule with diverse receptive fields, outperforming local\nconvolutions.Duan[146]furtherimprovedefficiencybyin-\ntegratingchannelgrouping.Shietal.[38]developedDSTA-\nNet, decoupling skeleton data into slow and fast temporal\nstreams. Chen et al. [131] proposed Hi-TRS, a hierarchi-\ncal Transformer framework that extracts spatio-temporal\nfeatures at frame, clip, and video levels, with hierarchical\nsupervision during pretraining.\nTemporal Feature Extraction by Partition. Directly\ninputtingallinformationwithoutsegmentationoftenleadsto\ncoarseandimprecisetemporalfeatureextraction.Toaddress\nthis,researchershaveexploreddividingtemporalsequences\nintosmaller,moremanageablecomponentsforfiner-grained\nfeature learning. For instance, Qiu et al. [73] proposed the\nSpatio-Temporal Tuple Transformer (STTFormer), which\nsegments skeletal sequences into non-overlapping tuples,\nwith each tuple representing a sub-action. Features are ex-\ntracted from these sub-actions using a self-attention mod-\nule and then aggregated through an inter-frame feature ag-\ngregation mechanism. Similarly, Do et al. [75] partitioned\njoints and frames based on skeletal temporal relationships,\ndistinguishing between adjacent and distant frames. This\napproachappliesskeletaltemporalself-attentionwithineach\nregion, effectively capturing both local and global temporal\ndynamics.\n6.2. Spatial Feature Extraction\nSpatial Feature Extraction Using RNNs. Recurrent\nNeural Networks (RNNs) excel in temporal modeling but\nare relatively weaker in spatial feature extraction. To ad-\ndress this limitation, RNNs are often combined with spatial\npartitioning and hierarchical processing [28], or integrated\nwith other models such as CNNs, GCNs, and Transformers\nfor auxiliary spatial modeling. Liu et al. [108] proposed a\ntree-structured skeleton traversal approach to enhance spa-\ntial information extraction, incorporating an LSTM with\ngates to effectively handle noise and occlusions. In their\nsubsequentwork[147],theyintroducedtheGlobalContext-\nAware Attention LSTM (GCA-LSTM), an attention-based\nnetworkthatleveragesglobalcontexttoselectivelyfocuson\ninformative joints. The GCA-LSTM architecture comprises\ntwo LSTM layers: the first encodes skeleton sequences to\ngenerate global context memory, while the second refines\nthis memory using attention-based representations. Zhang\net al. [110] addressed viewpoint variations in skeleton se-\nquencesbyintegratingatemporalattentionresidualmodule\nwithin an LSTM framework. This module adjusts skeleton\nsequences to align viewpoints before the main LSTM net-\nwork extracts further features. Additionally, Li et al. [148]\ncombined attention-based RNNs with CNNs to enhance\nPage 12 of 30\n--- Page 13 ---\n3D Skeleton-Based Action Recognition: A Review\nFigure 10: Architecture of the channel-wise topology refine-\nment graph convolution.\nspatial modeling, demonstrating the potential of hybrid ar-\nchitectures in overcoming spatial modeling limitations.\nSpatial Feature Extraction Using CNNs. Convolu-\ntional Neural Networks (CNNs) excel in spatial feature ex-\ntraction from 2D images but face challenges in skeleton-\nbasedHAR,where3Dskeletondatamustbeconvertedinto\npseudo images for processing. Liu [25] utilized enhanced\nskeletalvisualizationtorepresentskeletondata.Wangetal.\n[118] proposed the Joint Trajectory Map (JTM), which en-\ncodesthespatialstructureanddynamicsofjointtrajectories\ninto three texture images using color encoding, though this\napproach risks complexity and information loss. Li et al.\n[64]introducedashape-motionrepresentationderivedfrom\ngeometric algebra, emphasizing the importance of joints\nandbonestomaximizetheinformationencodedinskeleton\nsequences. Caetano et al. [120] developed SkeleMotion, a\nmotion-based representation that explicitly encodes tempo-\nral dynamics by calculating the magnitude and direction of\nskeletal joint movements. Additionally, based on SkeleMo-\ntion, Caetano [121] extended the framework by incorporat-\ningatreestructureandreferencejointstorepresentskeleton\nimages. Despite these advancements, pseudo-image-based\nmethods often face limitations. The convolution kernels\ntypically learn co-occurrence features only from adjacent\njoints within their receptive field, potentially overlooking\nlatent correlations between distant joints. To address this\nissue,partitioningorhierarchicalfeatureextractionmethods\nhavebeenproposedtoenhancelearning,asdemonstratedby\n[139, 34].\nSpatial Feature Extraction Using Spatial Graphs.\nIn recent years, due to the excellent expressive power of\ngraph structures, graph-based learning models have gar-\nnered widespread attention [149, 150]. Researchers have\nfocused on two main areas of improvement: better graph\nconstruction and more effective feature aggregation.\nImprovements in graph construction typically rely on\nadjacency matrices that have been refined to capture not\nonly physical connections but also more nuanced dynamic\nand semantic relationships. Shi et al. [66] introduced Two-\nstream Adaptive GCN (2sAGCN), learning graph topology\nvia backpropagation and integrating joint coordinates with\nbone-based information to enhance action modeling. Li et\nal. [123] proposed AS-GCN, combining action-specific and\nstructurallinksintoageneralizedskeletongraph.Chen[71]\ndeveloped CTR-GCN, grouping channels to share skeleton\ntopology, learning shared topologies, and channel-specific\ncorrelationsforefficientfeatureextraction,asshowninFig.10. Duan [146] presented DG-GCN, dynamically learning\njoint correlations with an affinity matrix, refining joint re-\nlations without predefined graphs, and enhancing precision\nthrough dot-product and normalization.\nImprovementsinfeatureaggregationfocusoncapturing\nboth local and global spatial dependencies while enhancing\nextraction flexibility and efficiency. Wu et al. [151] used\ncross-domain spatial residual layers and densely connected\nblocks to learn global features. Attention mechanisms were\napplied in [21, 152] to extract discriminative information\nand capture global dependencies. Traditional graph convo-\nlutions, limited by fixed receptive fields, were addressed by\nShift-GCN[85],whichexpandedeachnode’sreceptivefield\nto the entire skeleton graph via shift operations, improving\nflexibility.Chietal.[72]introducedInfoGCN,incorporating\naninformationbottleneckandattention-basedgraphconvo-\nlution for dynamic, context-aware skeleton topologies. To\ncapture long-term spatial dependencies, Peng et al. [153]\nproposedtheTripooloperationandlossfunction,enhancing\nefficiency. Liu et al. [129] advanced the field with TSGC-\nNeXt, expanding skeleton feature channels fourfold to map\nfeatures into a higher-dimensional space for more distinct\nspatial dependencies.\nSpatial Feature Extraction Using Attentions. By em-\nploying spatial self-attention modules, the model captures\ncorrelations between nodes within a frame and dynamic\nrelationships across frames [130, 40]. Zhang et al. [154]\nproposed RE-DMT, which captures global spatial features\nthroughdynamicmaskingandrearrangement.Liuetal.[74]\ndesigned PAINet, which learns discriminative features by\ncombining intra-sequence and inter-sequence correlations,\ncapturing co-motion and semantic relationships through a\ntopologyencodingmodule.ThecombinationofTransformer\nandGCNisakeydirectionforspatialfeatureextraction.Shi\n[143] used sparse multi-order topology matrix multiplica-\ntionandself-attentiontolearnspatialrelationships.Liuetal.\n[42] introduced the kernel attention adaptive graph Trans-\nformer network (KA-AGTN), which captures high-order\ndependencies between joints. Zheng et al. [155] proposed\nthe spatio-temporal Dijkstra Attention (STDA) mechanism\nto enhance topological integration and optimize feature ag-\ngregationbystrengtheninglocalneighborhoodconnections.\nSpatial Feature Extraction Hierarchically. Similar to\ntemporalhierarchicalfeatureextraction,Spatialfeaturescan\nbe extracted by partitioning the space into different recep-\ntive fields [73], enabling the learning of spatial features at\nvaryingscales.Lietal.[34]proposedahierarchicalmethod\nto learn co-occurrence features. It independently learned\npoint-level features for each joint, which were then used\nas channels for convolution layers to learn hierarchical co-\noccurrence features. Liu et al. [145] integrated a disentan-\ngledmulti-scaleaggregationschemewithaspatio-temporal\ngraph convolution operator called G3D, which served as a\npowerfulfeatureextractor.Wang[156]formedahypergraph\nto simulate hyperedges between graph nodes, introducing\nvarious pairings such as ’channel-temporal block’, ’order-\nchannel-body joint’, and ’channel-hyper-edge (any order)’,\nPage 13 of 30\n--- Page 14 ---\n3D Skeleton-Based Action Recognition: A Review\nas well as ’channel-only’ pairs to capture more complex\nspatial and temporal relationships. Do et al. [75] utilized\nthe spatial relationships of the human skeleton to differ-\nentiate between proximal and distant joints. By combining\nthis spatial distinction with temporal segmentation based\non proximity, they defined four types of spatiotemporal\nrelationships. This approach enables spatial self-attention\nwithin each region, effectively capturing both local and\nglobal temporal dependencies.\nSpatial Feature Extraction by Partition. Human mo-\ntioniscomposedofindependentorcoordinatedmovements\nof various body parts, such as arms, legs, and the torso.\nFor example, activities like running and swimming require\nprecise coordination among these parts. To effectively rec-\nognize diverse human actions, it is essential to model the\nmovements of individual body parts while capturing their\ncollaborativedynamics.Methodssuchas[31,157,119,158,\n28, 38] decompose the skeleton into five parts (two arms,\ntwo legs, and the torso), modeling each part independently\nandthenintegratingtheirspatio-temporalfeaturestoextract\ncollaborative motion patterns. Similarly, Avola et al. [115]\ndivides the body into upper and lower sections for separate\nprocessing.\nHowever, this simple partitioning approach has limi-\ntations. It may overlook the overall common features of\nactions. To address this, Behera et al. [159] proposed the\nCo-LSTMmodel,whichusesaregularizeddeepLSTMnet-\nwork to learn co-occurrence features among skeletal joints,\nenhancingrepresentationalcapacity.Additionally,part-level\nembeddings are often underutilized, especially for fine-\ngrained actions. Wang et al. [160] proposed a Transformer-\nbased network to model intra- and inter-part dependencies\nin the spatial domain for detailed feature representation.\nChen et al. [131] introduced a hierarchical Transformer for\nunsupervised learning, leveraging attention mechanisms on\nbody part divisions.\n6.3. Assisted Feature Extraction\nRecently,numerousstudieshaveexploredapplyinglarge\nlanguagemodels(LLMs)andgenerativemodelstoskeleton-\nbased action recognition to aid feature extraction. These\napproaches demonstrate vast potential, offering more in-\nnovative pathways for feature learning while significantly\nenhancing the robustness and adaptability of models.\nAssistedFeatureExtractionUsingLLMs. LLMswith\ntheir extensive knowledge base and powerful semantic rea-\nsoning,areincreasinglyvitalinskeleton-basedactionrecog-\nnition.Theybridgethegapbetweenvisualfeaturesandhigh-\nlevelactionsemanticsbyprovidingsemanticpriorsandgen-\neratingmultimodalinformationtoenhancefeaturelearning.\nFor example, action descriptions generated by LLMs, such\nasbodypartmovements,serveassupervisorysignalsinmul-\ntimodal training, improving action feature representations\nand achieving state-of-the-art performance [161]. Xu et al.\n[162]combinedLLM-generatedsemanticpriorswithmulti-\nhopattentiongraphconvolutionnetworkstointegrateglobalandcategory-levelpriorrelationships,improvingactionrep-\nresentationquality.LLMscanalsoactasauxiliarynetworks\nin multimodal frameworks, optimizing multimodal features\nduring training while requiring only skeleton data during\ninference,balancingefficiencyandperformance[163].Quet\nal.[164]demonstratedthatLLMscandirectlyrecognizeac-\ntionsbyconvertingskeletonsequencesintoactionsentences,\nleveraging their language processing capabilities.\nIn tasks requiring recognition of unseen samples (e.g.,\nzero-shot and few-shot), LLMs combine semantic reason-\ning with visual feature analysis to extract spatio-temporal\npatterns and semantic information. Zhou et al. [165] used\nglobalalignmentandtemporalconstraintmodulestocapture\nstatistical correlations between visual and semantic spaces,\nenhancing recognition by incorporating temporal informa-\ntion.Yanetal.[166]proposedaglobal-local-globalfeature-\nguided strategy, generating high-level textual descriptions\nenrichedwithhumanknowledgetoimprovetherecognition\nof unseen actions.\nAssisted Feature Extraction Using Generative Mod-\nels.Generative models have also been widely applied to\nskeleton-based action recognition, particularly by model-\ning spatio-temporal dependencies through the prediction\nor reconstruction of masked skeleton data. For instance,\nYan et al. [47] introduced the Skeleton Sequence Learning\n(SSL) framework, which uses the generative model Skele-\ntonMAE to pre-train skeleton representations. This model\nreconstructs masked joints and edges by combining graph-\nbased encoding with human body topology knowledge, ef-\nfectively extracting skeleton features when combined with\nspatio-temporallearningmodules.However,traditionalgen-\nerativemodelsoftenfacethechallengeofredundantfeatures\nduringpre-training.Toaddressthis,Linetal.[49]proposed\nthe Idempotent Generative Model (IGM), which introduced\nidempotencyconstraintstoenforcestrongerconsistencyreg-\nularization in the feature space, resulting in more compact\nand effective feature representations.\nMoreover,diffusionmodels,atypeofgenerativemodel,\nhave recently been adapted for skeleton-based action recog-\nnition. These models progressively map noise to the target\ndistribution and have been modified for skeleton data with\ninnovations such as specialized skeleton loss functions and\nthe incorporation of Transformer architectures. Wu et al.\n[48] proposed the Masked Conditional Diffusion (MacDiff)\nframework, where the diffusion decoder conditions on rep-\nresentations from a semantic encoder. By applying random\nmaskingtotheencoder’sinput,MacDiffreducesredundancy\nand introduced an information bottleneck, enhancing the\nquality and generalizability of skeleton representations.\n7. Spatial-temporal Modeling\nThemostcriticalfactorsforskeleton-basedactionrecog-\nnition lie in two key aspects: intra-frame representations\nthat capture joint co-occurrences and inter-frame represen-\ntationsthatmodelthetemporalevolutionofskeletons.Early\nresearch in this domain often focused on modeling either\nPage 14 of 30\n--- Page 15 ---\n3D Skeleton-Based Action Recognition: A Review\nSpatial \nModuleTemporal\nModule\nSpatial Module\nTemporal Module\nSpatial-\nTemporal \nModuleSpatial-\nTemporal \nModuleSpatio-Temporal Parallel StructureSpatial \nModuleTemporal\nModule\nSpatial Module\nTemporal Module\nSpatial-\nTemporal \nModuleSpatial-\nTemporal \nModuleSpatio-Temporal Serial Structure\nSpatio-Temporal Fusion Structure\nFigure 11: Schematic diagram of three types of spatial-\ntemporal modeling architecture.\nspatial or temporal features in isolation, neglecting the in-\ntegrationofbothaspects.However,asthefieldhasevolved,\nresearchers have increasingly recognized the importance of\njointly modeling spatial and temporal information.\nIn recent years, numerous studies have aimed to capture\nthecontextualdependenciesinboththetemporalandspatial\ndomains, leveraging the spatial and temporal relationships\namong skeleton joints [167, 168, 72]. This dual-focus ap-\nproach has significantly advanced the development of the\nfield. Consequently, current models typically incorporate\narchitecturaldesignsthatintegratebothspatialandtemporal\nnetworks for skeleton-based action recognition.\nBased on the arrangement of spatial and temporal mod-\neling modules within their overall frameworks, these mod-\nels can be broadly categorized into three types: spatio-\ntemporalserialstructures ,spatio-temporalparallelstruc-\ntures,andspatio-temporalfusionstructures .Theschematic\ndiagramsofthesethreearchitecturescanbefoundinFig.11.\n7.1. Spatio-temporal Serial Structure\nRNN-BasedMethods. Duetal.[28]dividedthehuman\nskeleton into five parts based on its physical structure, and\nthenseparatelyfedthesepartsintofivedistinctsubnets.The\nrepresentationsextractedbythesesubnetsarehierarchically\nfused to form the inputs for higher layers, enabling spatial\nmodelingofthedifferentbodyparts.Thetemporaldynamics\nof the entire body representation are then modeled using a\nbidirectional recurrent neural network (BRNN) [169]. Ulti-\nmately,thesestackedBRNNscanbeseenasextractingboth\nspatial and temporal features from the skeleton sequences.\nSong et al. [30] proposed an end-to-end multi-layer LSTM\nnetwork incorporating both spatial and temporal attention\nmechanismsforactionrecognition.Thenetworkisdesigned\ntoautomaticallyselectthemostprominentjointswithineach\nframe using the spatial attention module, while the tempo-\nral attention module assigns varying levels of importance\nto different frames. Both modules are constructed using\nLSTM networks. Zheng et al. [68] proposed a two-streamARRN-LSTM model, consisting of a Joints-stream and a\nBone-stream. For each stream, they employed a Recurrent\nRelational Network (RRN) [170] as a robust framework to\nlearn spatial features from a single skeleton, followed by a\nmulti-layer LSTM to capture the temporal dynamics within\nskeleton sequences. This approach enables the extraction\nof spatio-temporal features from both the Joints and Bone\nmodalities.\nGCN-BasedMethods. Asoneofthepioneeringworks,\nST-GCN by Yan et al. [35] is the first to apply graph con-\nvolutionalnetworks(GCNs)toskeleton-basedactionrecog-\nnition. They construct a skeleton graph with spatial edges\nand temporal edges. The model then applies spatial graph\nconvolution and temporal graph convolution sequentially\nto effectively model the dynamics of skeleton sequences.\nSubsequent works have employed the same ST-GCN back-\nbone [66, 85, 86, 71, 167], a classic paradigm for serial\nmodeling along both the spatial and temporal dimensions.\nShi et al. [66] proposed an adaptive graph convolutional\nnetwork to more effectively learn the topology of the graph\nacrossdifferent GCNlayers.Furthermore, Chengetal.[85]\nproposed two types of spatial shift graph operations for\nspatialgraphmodelingandtwotypesoftemporalshiftgraph\noperations for temporal modeling. This approach further\nreduces computational complexity while enhancing the re-\nceptive field for spatiotemporal modeling. Considering that\nsemantic information, such as joint type and frame index,\nplays a crucial role in action recognition, Zhang et al. [86]\nproposed a Semantics-Guided Neural Network (SGN) that\nexplicitly incorporates both semantics and dynamics for\nhighlyefficientactionrecognition.Chenetal.[71]proposed\na Channel-Wise Topology Refinement Graph Convolution\n(CTR-GC)thatdynamicallyandeffectivelymodelschannel-\nwise topology. Yussif et al. [167] proposed an instance-\nbased self-relation modeling graph convolution that learns\nthe connection strength between joints by determining their\ncorresponding proximity in the feature space.\nTransformer-BasedMethods. Observingthatthetem-\nporalandspatialdimensionsarefundamentallydifferent,Shi\net al. [38] introduced a novel decoupled spatial-temporal\nattention network (DSTA-Net) for skeleton-based action\nrecognition. The skeleton sequence is first fed into a spatial\nattention module to model the spatial relationships between\njoints, and then sequentially passed through a temporal\nattention module to capture the temporal relationships be-\ntween frames. Sun et al. [171] proposed the MSST-RT\nframework,incorporatingalightweightrelativetransformer\nmodule for efficient spatial and temporal modeling. In the\nspatial dimension, the Spatial Relative Transformer (SRT)\nis designed to capture long-range dependencies while pre-\nserving the original skeleton topology. For the temporal\ndimension,theTemporalRelativeTransformer(TRT)mod-\nels long-range interactions between nonadjacent frames,\nensuring that the sequential order of the skeleton remains\nintact.Ijazetal.[168]designedadual-modalitytransformer\nthat leverages both acceleration and skeletal joint data. In\neachbranch,aSpatialTransformerEncoderandaTemporal\nPage 15 of 30\n--- Page 16 ---\n3D Skeleton-Based Action Recognition: A Review\nTransformerEncoderareconnectedsequentiallytocompute\nspatial and temporal features from the skeletal joints. To\nsimultaneouslyfusehumanjointandpartinteractions,Wang\net al. [160] proposed a novel spatial-temporal transformer\nnetwork (IIP-Former), which efficiently captures both joint-\nlevel(intra-part)andpart-level(inter-part)dependencies.By\nintegrating spatial and temporal dependencies into a single\ntransformer, IIP-Former reduces model complexity while\nenhancing generalization performance.\nHybrid-Network Based Methods. Recurrent Neural\nNetworks(RNNs)demonstratestrongcapabilitiesincaptur-\ningtemporaldependenciesbutfacelimitationsineffectively\nmodeling the spatial coordination among different body\nparts.Hence,Sietal.[36]proposedaSpatialReasoningand\nTemporal Stack Learning (SR-TSL) model, which consists\na a Spatial Reasoning Network (SRN) to extract high-level\nspatialstructuralinformationwithineachframeandaTem-\nporal Stack Learning Network (TSLN) employing a com-\nposition of multiple skip-clip LSTMs to model fine-grained\ntemporaldynamicsacrossskeletonsequences,thusprovieds\na comprehensive solution for spatiotemporal representation\nlearning.\nRecognizinghumanactionseffectivelyrequiresleverag-\ning the intrinsic topology of joints, which provides critical\ncontextual information. Chi et al. [72] proposed a novel\napproachthatintroducesaSelf-Attention-basedGraphCon-\nvolution (SA-GC) module. This module dynamically ex-\ntractstheintrinsicgraphstructurewhileencodingasequence\nof skeleton data. Following this, a Multi-Scale Temporal\nConvolution(MS-TC)moduleisemployedtomodeltempo-\nraldynamics,enablingrobustspatiotemporalrepresentation\nlearning for action recognition. Liu et al. [42] introduced a\nSkeleton Graph Transformer (SGT) block featuring graph\ntransformer operators designed to model higher-order spa-\ntial dependencies among joints. Compared to traditional\nfeature extractors, the SGT block effectively alleviates the\noversmoothing problem while capturing long-range depen-\ndencies with greater precision. Additionally, they propose a\nTemporal Kernel Attention (TKA) block, an efficient tem-\nporal feature enhancement module that enables the SGT\nblockstofocusonsalienttemporalfeatures,therebyimprov-\ning the overall spatiotemporal modeling capability. Yang et\nal. [43] proposed the Stream-GCN network, which incor-\nporates multi-stream components and channel attention to\nenhance spatial-temporal modeling. The multiple streams\noffercomplementaryrepresentationsbyaddressingdifferent\nmodalities, while the attention mechanism emphasizes the\nmost critical channels. Each stream includes a novel cross-\nchannelattentionmoduleforadaptivelyweightingchannels,\nservingasthespatialmodelingcomponent,andintegratesa\nmulti-scaletemporalmodelingmoduletoeffectivelycapture\ntemporal dynamics.\n7.2. Spatio-temporal Parallel Structure\nRNN-Based Methods. Wang et al. [31] proposed a\nnovel two-stream RNN architecture to model both temporaldynamics and spatial configurations for skeleton based ac-\ntion recognition. The temporal model, consisting of stacked\nLSTM for capturing spatial information of skeleton joints\nand hierarchical LSTM for processing temporal dynamics\nof individual body parts, integrates its outputs with those\nfromthespatialmodeltoproducethefinalactionrecognition\nresults. Similarly, Cui et al. [172] proposed a multi-source\nmodelthatintegratestemporalandspatialmodels.Thetem-\nporalmodelisfurtherdividedintothreedistinctbranchesto\ncapture information at the global, local, and detailed levels,\nrespectively, enabling a more comprehensive perception of\naction dynamics.\nCNN-Based Methods. Liu et al. [173] are the first to\napply 3D CNNs to skeleton-based action recognition. To\nenable3DCNNstolearnrobustfeatures,skeletonjointsare\nencoded into spatial and temporal volumes separately, cap-\nturing spatial and temporal information. Two independent\n3D CNN models are trained individually for the spatial and\ntemporal streams, which are fused only during the forward\npropagation phase. Similarly, Xu et al. [117] designed four\ndistinct sub-networks to extract diverse features, including\nglobal, local, focus, and motion features of actions. Among\nthese,aTwo-streamEntiretyNetisproposedtocaptureboth\nspatial and temporal features of the entire skeleton. In this\narchitecture, the spatial stream extracts spatial features by\nsliding convolutional kernels in the spatial domain, while\nthe temporal stream extracts temporal features by sliding\nthe kernels in the temporal domain. The two streams are\nfused by computing cross-domain losses, enabling end-to-\nend training of this sub-network.\nGCN-Based Methods. Considering the intrinsic high-\norder correlations among skeleton joints is crucial for ac-\ntion recognition, Li et al. [141] proposed a novel spatio-\ntemporal graph routing (STGR) scheme, consisting of two\ncomponents: the Spatial Graph Router (SGR), which aims\nto discover the connectivity relationships among joints\nthrough sub-group clustering along the spatial dimension,\nand the Temporal Graph Router (TGR), which explores the\nstructural information by measuring the correlation degrees\nbetween temporal trajectories. This framework effectively\nadaptstolearntheintrinsichigh-orderconnectivityrelation-\nships between physically distant skeleton joints. Due to the\ncomplexity of human activities, coarse modelling features\nwillleadtoconfusionbetweenambiguousactionswithsim-\nilarspatialappearancesortemporaltransformations.Hence,\nZhou et al. [174] proposed a Feature Refinement Head\nwhich first decouples the hidden feature maps into spatial\nand temporal components and then applies a contrastive\nlearning loss with global class prototypes and ambiguous\nsamples.Suchspatial-temporaldecouplemodulethatmines\nthe spatial and temporal information simultaneously to\nimprovethediscriminativeabilityofactionrepresentations.\nTransformer-Based Methods. Zhang et al. [40] intro-\nducedtheSpatial-TemporalSpecializedTransformer(STST),\nwhich independently models the temporal and spatial infor-\nmation of skeleton sequences while maintaining robustness\nto various abnormal scenarios. Specifically, this approach\nPage 16 of 30\n--- Page 17 ---\n3D Skeleton-Based Action Recognition: A Review\nextracts coordinate information, semantic information, and\ntemporal information into three distinct types of tokens to\nfully utilize the skeleton data without losing critical details.\nBuilding on a similar concept, Shi et al. [143] proposed a\nnovel skeleton-based action recognition model that applies\nsparseattentiontothespatialdimensionandsegmentedlin-\near attention to the temporal dimension, further refining the\nefficiency and performance of attention mechanisms in this\ndomain.Moreover,toalignspatialandtemporalinformation\nbetter, Liu et al. [74] proposed the Cross spatial alignment\nand the Cross temporal alignment branches to exploit inter-\nskeletonandintra-skeletonsemanticinformation.Compared\nto previous works, their approach involves the alignment\nof the spatial and temporal domains within two parallel\nbranches, enabling complementary attention to informative\nregions.\nHybrid-Network Based Methods. Building upon the\nresearch foundation of ST-GCN, Plizzari et al. [130] in-\ncorporated the concept of self-attention and proposed the\nST-TR model, which incorporates a Spatial Self-Attention\n(SSA) module to capture intra-frame interactions between\ndifferent body parts and a Temporal Self-Attention (TSA)\nmodule to model inter-frame correlations. The model em-\nploysatwo-brancharchitecture,whereeachbranchistrained\nindependently, and the final scores are fused to generate\npredictions. To address the limitations of entangled spatio-\ntemporal feature representation, Bai et al. [175] proposed a\nnovelarchitecturenamedHierarchicalGraphConvolutional\nSkeleton Transformer (HGCT). This model combines the\nstrengths of Graph Convolutional Networks, including their\nability to capture local topology, temporal dynamics, and\nhierarchicalstructure,withtheglobalcontextmodelingand\ndynamic attention capabilities of Transformers. By inte-\ngratingthesecomplementaryfeatures,HGCTfacilitatesthe\ndisentanglement of spatio-temporal representations, rather\nthan solely focusing on refining graph topology design.\nUnsupervised Learning. Zhou et al. [88] proposed a\nparallel spatiotemporal masking strategy to leverage local\nrelationships within partial skeleton sequences, enhancing\nrobustness to noise and data omissions. Dong et al. [176]\nintroducedaninstance-levelrepresentation,whichcombines\ntemporal and spatial feature representations through con-\ncatenation, forming a unified and comprehensive represen-\ntation to describe the entire skeleton sequence. Since the\ncomplex spatiotemporal information extracted by most ex-\nisting models is highly entangled, it becomes challenging\ntoprovideclearinsightsforsubsequentanalysis.Toaddress\nthisissue,Wuetal.[91]designednoveldecouplingencoders\nthat extract clean spatial and temporal representations from\nthe otherwise entangled information, enabling more effec-\ntive interpretation and comparison of the learned features.\n7.3. Spatio-temporal Fusion Structure\nRNN-Based Methods. In addition to leveraging RNNs\noverthetemporaldomaintouncoverdiscriminativedynam-\nics and motion patterns for 3D action recognition, valu-\nable discriminative information is also encoded in the staticpostures represented by the 3D locations of joints within\nindividual frames. Furthermore, the sequential nature of\nskeletondataallowsfortheapplicationofRNN-basedlearn-\ning in the spatial domain. Building on this observation,\nLiu et al. [29, 114] proposed a spatio-temporal LSTM (ST-\nLSTM)model.Inthespatialdomain,thebodyjointswithin\na frame are processed sequentially, while in the temporal\ndomain, the corresponding joints’ locations are fed across\ntime. Each unit in the model receives hidden representa-\ntions from both preceding joints within the same frame and\nprevious frames of the same joint, enabling the effective\nintegration of spatial and temporal contextual information.\nDespite its utility, LSTM exhibits limited attention capa-\nbility for 3D action recognition. This limitation primarily\narises from LSTM’s inability to effectively perceive global\ncontextualinformation,whichisoftencrucialforaddressing\nglobalclassificationtaskssuchas3Dactionrecognition.To\novercome this constraint, Liu et al. [147, 177] extended the\noriginal LSTM network by proposing the Global Context-\nAwareAttentionLSTM(GCA-LSTM),amodeldesignedto\nenhance attention capabilities for 3D action recognition. In\nthe GCA-LSTM framework, global contextual information\nis introduced at every step of the network. This enables\nthe model to evaluate the informativeness of new inputs\nat each step and adjust attention weights accordingly. This\nmechanismallowsGCA-LSTMtofocusmoreeffectivelyon\ndiscriminative features critical to 3D action recognition.\nGCN-Based Methods. In previous work, a spatiotem-\nporal layer typically combined spatial graph convolution\nand temporal convolution. Wu et al. [178] introduced a\ncross-domain spatial residual layer as a residual branch to\ncomplementtheoriginalspatialgraphconvolutionandtem-\nporal convolution branches. Liu et al. [69] proposed G3D,\na novel unified spatiotemporal graph convolution module\nthat directly models cross-spacetime joint dependencies.\nThey achieve this by introducing graph edges across the\n\"3D\"spatial-temporaldomainasskipconnections,enabling\nunobstructed information flow and significantly enhancing\nspatial-temporal feature learning. Observing that many ex-\nisting GCN methods use a pre-defined graph structure that\nremains fixed throughout the entire network, which can\nresult in the loss of implicit joint correlations, Peng et al.\n[179] focused on reducing the manual effort required. They\nreplacethefixedgraphstructurewithadynamicgraphstruc-\nture through automated neural architecture search (NAS)\n[180],andexploredifferentgraphgenerationmechanismsat\nvarious semantic levels. Inspired by the SlowFast network\n[7] for RGB video recognition, Fang et al. [181] proposed\nthe Spatial-Temporal SlowFast Graph Convolutional Net-\nwork (STSF-GCN). This model incorporates a fastpathway\nto capture short-range dependencies and a slow pathway\nto capture long-range dependencies, enabling the effective\nmodeling of skeleton data within a unified spatiotemporal\nframework. Similar to MS-G3D [69] and GR-GCN [182],\nSTSF-GCN leverages this dual-pathway approach to en-\nhance its capability in capturing diverse spatiotemporal re-\nlationships. Most previous works [66, 183, 71] treated all\nPage 17 of 30\n--- Page 18 ---\n3D Skeleton-Based Action Recognition: A Review\njoints and edges as the same type, failing to capture the se-\nmanticpropertiesofactions.Toaddressthis,Xieetal.[184]\nproposed a dynamic semantic-based graph convolutional\nnetwork (DS-GCN) for skeleton-based action recognition.\nIn this approach, the joint and edge types are implicitly\nencodedintheskeletontopology.Specifically,twosemantic\nmodules are introduced: joint-type-aware adaptive topol-\nogy and edge-type-aware adaptive topology. By integrating\nthese semantic modules with temporal convolution, DS-\nGCN forms a powerful framework for action recognition.\nTransformer-BasedMethods. Bothrecurrentandcon-\nvolutional operations are local, neighborhood-based meth-\nods [185], operating in either space or time. These meth-\nods repeatedly extract and propagate local information to\ncapture long-range dependencies. Despite the adoption of\nhierarchical networks [28, 109, 186] to obtain deeper se-\nmantic representations, the challenge of capturing bidirec-\ntional semantic dependencies persists. To address this, Cho\net al. [39] introduced three novel self-attention networks\n(SAN), namely SAN-v1, SAN-v2, and SAN-v3, integrating\nTemporal Segment Networks (TSN) with these SAN vari-\nants. By capturing long-range correlations, their approach\neffectively extracts high-level semantics, allowing model to\novercomethechallengeofacquiringlong-termsemanticin-\nformation.Focusingsolelyonone-orfew-hopgraphneigh-\nborhoodsmayoverlookdependenciesbetweenunconnected\nbody joints. To address this, Wang et al. [156] proposed\nusing hypergraphs to model higher-order hyperedges be-\ntweengraphnodes(e.g.,third-andfourth-orderhyperedges\ncaptureinteractionsamongthreeorfournodes),enablingthe\ncapture of more complex motion patterns involving groups\nof body joints. Kim et al. [187] proposed a 3D deformable\nattention mechanism within a cross-modal learning frame-\nwork, which addresses the spatio-temporal feature fusion\nchallenge in skeleton-based action recognition. By leverag-\ning an adaptive spatio-temporal receptive field and a cross-\nmodallearningapproach,theirmethodeffectivelyintegrates\nspatial and temporal features across modalities.\nMost transformer-based methods [188, 44, 189, 190]\nprimarily focus on improving configuration and learning\nspatiotemporalcorrelations,withoutleveragingskeletalmo-\ntion patterns in the frequency domain. This limitation hin-\nderstheirabilitytolearndiscriminativerepresentationsthat\ncapturesubtlemotionsimilarities.Toaddressthisissue,Wu\net al. [142] proposed the Frequency-aware Mixed Trans-\nformer (FreqMixFormer), which is specifically designed to\nrecognizesimilarskeletalactionswithsubtle,discriminative\nmotions.However,capturingcorrelationsbetweenalljoints\nin all frames requires substantial memory resources. Do et\nal. [75] recently introduced the Skeletal-Temporal Trans-\nformer(SkateFormer),anovelapproachthatpartitionsjoints\nand frames based on different types of skeletal-temporal\nrelationships (Skate-Type). This method performs skeletal-\ntemporal self-attention (Skate-MSA) within each partition,\ntherebyreducingmemoryusagewhilemaintainingeffective\nlearning of correlations.Hybrid-NetworkBasedMethods. Thesemethodstypi-\ncallyintegrateattentionmechanismstoenableunifiedmod-\neling of spatio-temporal features [21, 148, 191, 44, 45,\n155, 192]. Focus on how to effectively extract discrimina-\ntive spatial and temporal features, Si et al. [21] proposed\na novel Attention Enhanced Graph Convolutional LSTM\nNetwork(AGCLSTM),consistingofatemporalhierarchical\narchitecture to increase temporal receptive fields and an\nattention mechanism to enhance information of key joints.\nLi et al. [148] proposed the Temporal Attention Recali-\nbration Module (TARM), which weights each frame and\njoint based on their importance within the action sequence.\nThe Spatio-Temporal Convolution Module (STCM) is then\nused to model these enriched spatiotemporal features. This\napproach demonstrates the effectiveness of a novel \"mem-\nory attention + convolution network\" scheme for capturing\nthe complex spatiotemporal variations in skeleton joints.\nObserving that the recent State-Of-The-Art (SOTA) mod-\nels for this task tends to be exceedingly sophisticated and\nover-parameterized, Song et al. [191] embedded recent ad-\nvanced separable convolutional layers into an early fused\nMultiple Input Branches (MIB) network, constructing an\nefficient Graph Convolutional Network (GCN) baseline for\nskeleton-based action recognition. Xin et al. [44] proposed\ntheSpatial-MixModuletodynamicallycapturemultivariate\ntopological relationships, and the Temporal-Mix Module,\nwhichemploysmultipletemporalmodelstoensuretherich-\nness of global differential expressions. Duan et al. [45] tar-\ngetedmoregeneralscenariosthattypicallyinvolveavariable\nnumber of people and various forms of interaction between\npeople and present SkeleTR, a unified solution for multiple\nskeleton-based action tasks.\nMamba Based Methods. Mamba is an efficient state-\nspace model (SSM) designed to handle sequential data\nand capture long-range dependencies. Martinel et al. [193]\nproposed SkelMamba, a skeleton-based action recognition\nmodel built on Mamba, which introduces the Time-Space\nMamba Block (TSMB) to model temporal and spatial fea-\ntures in groups. The extraction of temporal features is\naccomplished through grouped 1D convolutions, incorpo-\nratingtimedynamicsatvariousscales.SkelMambaextends\nthe traditional Mamba’s Selective Scan Mechanism (S6) by\nenabling four scanning directions: \"time-to-space,\" \"space-\nto-time,\" \"reverse time-to-space,\" and \"reverse space-to-\ntime.\" This enhancement allows the model to outperform\nGCN and Transformer-based methods in capturing long-\nrange dependencies, subtle motion features, and global\ndynamics.Additionally,theefficientSSMreducescomputa-\ntional complexity, making SkelMamba particularly suitable\nfor skeleton action recognition, especially in medical appli-\ncations that require detailed dynamic analysis.\nOver the past decade, the field has accumulated many\nclassic datasets, summarized in Table 1. Skeleton sequence\ndatasets such as MSR Actions 3D [195], 3D Action Pairs\n[203], and MSR Daily Activity 3D [197] have been exten-\nsively analyzed in numerous previous surveys. In addition\nto these well-known and widely used datasets, this survey\nPage 18 of 30\n--- Page 19 ---\n3D Skeleton-Based Action Recognition: A Review\nTable 1\nThe representative benchmark datasets with skeleton data\nmodalities.\nDataset Year Class Subject Sample Viewpoint\nHDM0 [194] 2007 130 5 2337 1\nMSR-Action3D [195] 2010 20 10 567 1\nCAD-6 [196] 2011 12 4 60 -\nMSR DailyActivity3D [197] 2012 16 10 320 1\nUTKinect [54] 2012 10 10 200 1\nSBU Kinect Interactions [198] 2012 8 7 282 1\nBerkeley MHAD [199] 2013 12 12 660 4\nCAD-120 [200] 2013 10 4 120 -\nIAS-lab [201] 2013 15 12 540 1\nJ-HMDB [202] 2013 21 - 31838 -\nMSRAction-Pair [203] 2013 12 10 360 1\nUCFKinect [204] 2013 16 16 1280 1\nMulti-View TJU [205] 2014 20 22 7040 2\nNorthwestern-UCLA [206] 2014 10 10 1475 3\nUPCV [207] 2014 10 20 400 1\nUWA3D Multiview [208] 2014 30 10 900 4\nSYSU 3D HOI [22] 2014 12 40 480 1\nTJU [209] 2015 15 20 1200 1\nUTD-MHAD [210] 2015 27 8 861 1\nUWA3D Multiview II [211] 2015 30 10 1075 4\nLarge Scale Combined(LSC) [212] 2016 88 - 3898 -\nNTU RGB+D [107] 2016 60 40 56880 80\nPKU-MMD [213] 2017 51 66 1076 3\nKinetics400 [214] 2017 400 - 300000 -\nRGB-D Varying-view [215] 2018 40 118 25600 8+1(360◦)\nDHP19 [216] 2019 33 17 - 4\nDrive&Act [217] 2019 83 15 - 6\nMMAct [218] 2019 37 20 36764 4+Egocentric\nNTU RGB+D 120 [219] 2019 120 106 114480 155\nETRI-Activity3D [220] 2020 55 100 112620 -\nEV-Action [221] 2020 20 70 7000 9\nIKEA ASM [222] 2020 33 48 16764 3\nUAV-Human [223] 2021 155 119 67428 -\nprimarilyfocusesonthecurrentmainstreamandchallenging\ndatasets.\nSYSU3DHuman-ObjectInteractionSet(SYSU) [22].\nThe Kinect-captured dataset consists of 12 actions per-\nformed by 40 subjects, with a total of 480 sequences. Each\nsubject is represented by 20 joints. This dataset presents a\nchallenge due to the high similarity among the activities,\nmaking it difficult to distinguish between them based solely\non skeletal data.\nUWA3D Multi-view Activity II (UWA3D) [211]. This\nKinect-captureddatasetcontains30actionsperformedby10\ndifferent subjects. The videos are captured from 4 different\nviews: front view (V1), left side view (V2), right side view\n(V3),andtopview(V4).Ithas1075sequencesintotal.This\ndatasetischallengingbecauseofthediversityofviewpoints,\nself-occlusion, and high similarity among activities.\nNorthwestern-UCLA (N-UCLA) [206].This Kinect-\ncaptured dataset contains 1494 videos of 10 actions. These\nactions are performed by 10 subjects and repeated 1 to 6\ntimes. There are three views. Each subject has 20 joints.\nSBU Kinect Interaction (SBU) [198]. This Kinect-\ncaptured dataset is an interaction dataset with each action\nperformed by two subjects. The dataset contains 282 se-\nquencesacross8actionclasses,witheachsubjecthaving15\njoints. It’s important to note that both the SYSU and SBU\ndatasetsarecapturedusingasinglecamerafromoneprimary\nviewpoint.However,actionsareperformedbydifferentsub-\njects at varying locations, distances from the camera, and\norientations, which introduces additional variability in the\ndata.\nSHREC’17 Track [258]. The dataset contains 2,800\ngesture sequences, performed in two variations: using oneTable 2\nPerformance of skeleton-based deep learning HAR methods on\nKinetics400 dataset.\nMethods Year AccuracyGCN-Based MethodsAR-GCN [224] 2019 33.5\nST-GR [141] 2019 33.6\nPR-GCN [225] 2020 33.7\nPeGCN [226] 2020 34.8\nAS-GCN [123] 2019 34.8\nSLnL-rFA [227] 2019 36.6\nDGNN [37] 2019 36.9\nJB-AAGCN [228] 2019 37.4\nGCN-NAS [179] 2020 37.1\nCGCN [229] 2020 37.5\nMS-AAGCN [228] 2020 37.8\nDynamic GCN [124] 2020 37.9\nMS-G3D [145] 2020 38.0\nDualHead-Net [230] 2021 38.4\n2s-AGCN+TEM [231] 2021 38.6\nPoseConv3D [232] 2022 49.1\nDS-GCN [184] 2024 50.6\nProtoGCN [233] 2024 51.9Other MsethodsOurs-Conv-Chiral [234] 2019 30.9\nSLnL-rFA [227] 2019 36.6\nST-TR-agcn [235] 2020 37.4\nSTF [236] 2022 39.9\nPYSKL [237] 2022 49.1\nStructured Keypoint Pooling [238] 2023 52.3\nfingerandthewholehand.Eachgestureisrepeatedbetween\n1 and 10 times by 28 participants, with the hand skeleton\nconsisting of 22 joints. The dataset follows the same evalu-\nation protocol as in [10, 31]: training data consists of 1,960\nsequences, while the remaining 840 sequences are used for\ntesting. The gesture sequences can be labeled into 14 or 28\nclasses, depending on the number of fingers used and the\ngesture represented.\nDHG-14/28 [259]. The DHG-14/28 dataset is collected\nusing the Intel Real-Sense camera and contains 2,800 se-\nquencesof14gestures,performed5timesby20participants.\nEvaluationfollowsaleave-one-subjectcross-validationstrat-\negy.\nHuman3.6M [260].Ithas15typesofactionsperformed\nby 7 actors (S1, S5, S6, S7, S8, S9, and S11). Each pose\nhas 32 joints in the format of an exponential map. We\nconvert them to 3D coordinates and angle representations\nand discard 10 redundant joints. The global rotations and\ntranslations of poses are excluded. The frame rate is down-\nsampledfrom50fpsto25fps.S5andS11areusedfortesting\nandvalidationrespectively,whiletheremainingareusedfor\ntraining.\nThe Kinetics-400 [214]. The dataset is a large-scale\nvideo dataset collected from YouTube videos with 400 ac-\ntion classes. It contains 250K training and 19K validation\n10-second video clips. The performance of all deep learn-\ning methods on the Kinetics-400 dataset is summarized as\nshown in Table 2.\nPage 19 of 30\n--- Page 20 ---\n3D Skeleton-Based Action Recognition: A Review\nTable 3\nPerformance of skeleton-based deep learning HAR methods on\nNTU RGB+D and NTU RGB+D 120 datasets. ‘CS’, ‘CV’,\nand ‘CP’ denote Cross-Subject, Cross-View, and Cross-Setup\nevaluation criteria.\nMethods YearDataset\nNTU RGB+D 60 NTU RGB+D 120\nCS CV CS CPRNN-Based MethodsHBRNN-L [239] 2015 59.1 64.0 - -\nP-LSTM [107] 2016 62.9 70.3 25.5 26.3\nTrust Gate ST-LSTM [177] 2016 69.2 77.7 58.2 60.9\nLSTM with Geometric Features [62] 2017 70.3 82.4 - -\nTwo-stream RNN [31] 2017 71.3 79.5 - -\nSTA-LSTM [30] 2017 73.4 81.2 - -\nGCA-LSTM [147] 2017 74.4 82.8 58.3 59.2\nEnsemble TS-LSTM [109] 2017 74.6 81.3 - -\nBayesian GC-LSTM [240] 2018 81.8 89.0 - -\nMANs [148] 2018 82.7 93.2 - -\nIndRNN [241] 2018 86.7 93.7 - -\nVA-RNN (aug.) [110] 2019 79.8 88.9 - -\nARRN-LSTM [68] 2019 81.8 89.6 - -\nAGC-LSTM [21] 2019 89.2 95.0 - -\nLogsig-RNN [134] 2021 - - 68.3 67.2\nKShapeNet [111] 2021 84.2 89.7 74.8 76.9CNN-Based MethodsJTM [242] 2016 73.4 75.6 - -\nRes-TCNs [33] 2017 74.3 83.1 67.5 75.6\nSkeletonNet [158] 2017 75.9 81.2 - -\nJDMs [243] 2017 76.2 82.3 - -\nClips+CNN+MTLN [32] 2017 79.6 84.8 58.4 57.9\nEnhanced Skeleton Visualization [25] 2017 80.0 87.2 60.3 63.2\nTranslation-Scale Invariant Mapping [119] 2017 85.0 92.3 - -\nRotClips+MTCNN [244] 2018 81.1 87.4 62.2 61.8\nEnsem-NN [117] 2018 84.8 91.2 - -\nHCN [34] 2018 86.5 91.1 - -\nTSRJI [121] 2019 73.3 80.3 67.9 62.8\nSkeleMotion [120] 2019 76.5 84.7 67.7 66.9\nSkepxels [137] 2019 81.3 89.2 - -\nShape-motion [64] 2019 82.9 90.0 - -\nVA-CNN (aug.) [110] 2019 88.7 94.3 - -\nGimme Signals [245] 2020 - - 71.9 83.5\nTS-TCNs [139] 2020 82.4 90.2 71.9 83.5\nFuzzy Integral-Based CNN [246] 2020 84.2 89.7 74.8 76.9\nSEMN [122] 2021 82.0 85.8 - -GCN-Based MethodsST-GCN [35] 2018 81.5 88.3 - -\nSR-TSL [36] 2018 84.8 92.4 - -\nAS-GCN [123] 2019 86.8 94.2 - -\n2s-AGCN [66] 2019 88.5 95.1 - -\nDGNN [37] 2019 89.9 96.1 - -\nDC-GCN+ADG [183] 2020 88.2 95.2 90.8 96.6\nSGN [86] 2020 89.0 94.5 79.2 81.5\nGCN-NAS [179] 2020 89.4 95.7 - -\n4s Shift-GCN [85] 2020 90.7 96.5 85.9 87.6\nMS-G3D [145] 2020 91.5 96.2 86.9 88.4\nSym-GNN [70] 2021 90.1 96.4 - -\nElse-Net [247] 2021 91.6 96.4 - -\nCTR-GCN [71] 2021 92.4 96.8 88.9 90.\nSATD-GCN [248] 2022 89.3 95.5 - -\nEfficientGCN-B4 [191] 2022 92.1 96.1 88.7 88.9\nTD-GCN [249] 2022 92.8 96.8 89.8 91.2\nTCA-GCN [125] 2022 92.8 97.0 89.4 90.8\nPSUMNet [250] 2022 92.9 96.7 89.4 90.6\nLanguage Supervised Training [251] 2022 92.9 97.0 89.9 91.1\nInfoGCN [72] 2022 93.0 97.1 89.8 91.2\nDG-STGCN [146] 2022 93.2 97.5 89.6 91.3\nHD-GCN [127] 2022 93.4 97.2 90.1 91.6\nPoseC3D [232] 2022 94.1 97.1 - -\nTSGCNeXt [129] 2023 93.1 97.0 90.2 91.7\nLA-GCN [162] 2023 93.5 97.2 90.7 91.8\nBlockGCN [252] 2024 93.1 97.0 90.3 91.5\nDeGCN [253] 2024 93.6 97.4 91.0 92.1Transformer-Based MethodsTS-SAN [39] 2020 87.2 92.7 - -\nDSTA-Net [38] 2020 91.5 96.4 86.6 89.0\nSparse Transformer [143] 2021 83.4 84.2 78.3 78.5\nST-TR [130] 2021 89.9 96.1 81.9 84.1\nSTST [40] 2021 91.9 96.8 - -\nIIP-Transformer [254] 2021 92.3 96.4 88.4 89.7\nKA-AGTN [42] 2022 90.4 96.1 86.1 88.0\nSTTFormer [73] 2022 92.3 96.5 88.3 89.2\nHi-TRS [131] 2022 90.0 95.7 85.3 87.4\nHyperformer [255] 2022 92.9 96.5 89.9 91.3\nSTAR-Transformer [256] 2023 92.0 96.5 90.3 92.7\nTemPose [257] 2023 92.7 95.2 88.5 87.0\nSkateFormer [75] 2024 93.5 97.8 89.8 91.4\nFreqMixFormer [142] 2024 93.6 97.4 90.5 91.9\nNTU-RGB+D [107]. Proposed in 2016, the dataset\ncontains 56,880 video samples collected using Microsoft\nKinectv2,makingitoneofthelargestdatasetsforskeleton-\nbased action recognition. It provides 3D spatial coordinates\nfor 25 joints in each human action. Two evaluation proto-\ncols are recommended: Cross-Subject and Cross-View. Inthe Cross-Subject protocol, which includes 40,320 samples\nfor training and 16,560 for evaluation, the 40 subjects are\nsplit into training and evaluation groups. In the Cross-View\nprotocol,itincludes37,920samplesfortrainingand18,960\nforevaluation,usingcameras2and3fortrainingandcamera\n1 for evaluation.\nNTU-RGB+D120 [219].Recently,anextendedversion\nof the original NTU-RGB+D dataset, known as NTU-\nRGB+D 120, has been introduced. This dataset comprises\n120 action classes and 114,480 skeleton sequences, with\nan expanded range of 155 viewpoints. Specifically, CS\nrefers to Cross-Subject, CV denotes Cross-View in NTU-\nRGB+D,andCSet(Cross-Setting)isusedinNTU-RGB+D\n120. Table 3 summarizes the performance of various deep\nlearningmethodsonbothNTU-RGB+DandNTU-RGB+D\n120 datasets. While existing algorithms have achieved im-\npressiveresultsontheoriginalNTU-RGB+Ddataset,NTU-\nRGB+D120remainsasignificantchallenge.Currentskeleton-\nbased action recognition methods perform relatively poorly\non this dataset, making it one of the most demanding\nbenchmarks in the field.\nUCF101 [261]. The UCF101 dataset is a relatively\nsmall benchmark dataset, building on the UCF50 dataset. It\nconsists of 13,320 video clips categorized into 101 action\nclasses, which are further grouped into five main types:\nBody Motion, Human-Human Interactions, Human-Object\nInteractions, Playing Musical Instruments, and Sports. The\ntotaldurationofthevideoclipsexceeds27hours.Allvideos\nwere sourced from YouTube, featuring a consistent frame\nrate of 25 FPS and a resolution of 320 ×240.\nHMDB51 [262].Thisdatasetcontains6,766videoclips\nacross 51 action categories, sourced from movies and web\ncontent.Eachcategoryincludesatleast101clips,makingit\na diverse collection of realistic videos. The dataset follows\nanoriginalevaluationprotocolinvolvingthreedistincttrain-\ning/testing splits. In each split, 70 clips per action category\nare designated for training, and 30 clips for testing.\n8. Future Work\nIn the previous sections, we review the various deep\nlearning methods and datasets for HAR with skeleton data\nmodality.Next,wewilldiscusssomepotentialandimportant\nresearch directions, which need further development and\nexploration.\nHand gesture recognition. The joints in the human\nhand are numerous and very dense. Hand gestures are clear\nsigns that express individual action intentions and emo-\ntions well. It also has a strong auxiliary role in recognizing\nother non-gesture actions. Although GCN-based methods\n[249] [263] achieved high success, two problems in gesture\nrecognition need to be solved. (1) Most existing databases\ncondense the hand skeleton into 1 or 2 skeleton points,\nwhich is far from accurate gesture recognition. Although\nseveral datasets dedicated to gesture recognition have been\nproposed, they tend to contain only gestures, which is too\nnarrowintheapplicationscope.(2)Ifthejointsofthehand\nPage 20 of 30\n--- Page 21 ---\n3D Skeleton-Based Action Recognition: A Review\nwere fully incorporated into the human skeleton topology,\nthe existing methods would incur considerable additional\ncomputationaloverheadandredundantinformation,because\nmost of the time the joints of the hand are less active than\nthose of the rest of the human body.\nFew-shot/one-shotactionrecognition. Innaturalsitua-\ntions,peopleperformawiderangeofactionsatvaryingfre-\nquencies. Some actions may be rare and difficult to capture,\nyet they can be highly significant. For instance, a \"robbery\"\nis an infrequent event in daily life, but when it occurs in\nvideosurveillance,itmustberecognizedimmediately.One-\nshot recognition aims to find a method to classify new\ninstances with a single reference sample. Although there\nhavebeensomeattempts[264][265],theystillcannotsolve\nthe problem of data scarcity in certain practical scenarios.\nPossible approaches for solving problems of this category\nare metric learning [266, 267], or meta-learning [268]. In\naction recognition, this suggests that a novel action can be\nlearned from a single reference demonstration. However,\nunlike one-shotimageclassification,skeleton-basedactions\ninvolve sequential data, where a single frame often lacks\nsufficient context to recognize a new activity. Current so-\nlutions [269] primarily focus on static images, overlooking\nthe importance of image sequences. Static images are inad-\nequate for distinguishing between similar actions, such as\nsitting down and standing up, as they fail to capture the\ntemporal dynamics that differentiate these actions. Except\nfor accurately distinguishing between categories of actions\nwhen data is scarce, the difficulty with one-shot action\nrecognition is to distinguish between unknown classes that\nhaveneverbeenseenbefore.Berti[270]addsadiscriminator\ntodeterminetheunknownactionclass.Besides,video-based\noccluded one-shot recognition [271] is still attractive to be\nresearched.\nHumanMotionPrediction Humanactionsexhibitcon-\ntinuityandcausality.Motionpredictionaimstoobservepar-\ntiallycompletedbehaviorstoinfersubsequentactions,which\nis crucial for preventing potentially risky behaviors. Earlier\nmethods [272] [273] mainly used RNN-based approaches,\nwhich were difficult to train due to discontinuity and error\naccumulation problems. The current mainstream approach\nis based on GCN [274] and Transformer [275, 276]. It is\ncommon practice to copy the last observable frame action\nmultipletimes(thelengthofthepredictingaction)toconvert\ntheoriginalinputsequenceintoanextendedinputsequence.\nThen the ground truth of future poses is also appended to\nthe observable poses to obtain the extended ground truth\nof predicting action. Experiments show that the prediction\nbetween the extended sequences is easier than between the\noriginalsequencesandtheformermakesthemodel’spredic-\ntion more accurate. Although existing methods are capable\nof predicting a single action, they need to be improved for\npredictingalongtimecomplexaction.Thekeypointishow\nto simulate the smooth transition between two continuous\nactions.Real-timeactionrecognition. TheexistingHARmeth-\nods achieve excellent results based on many model param-\neters and computational overhead. In real application sce-\nnarios, such as embedded devices, devices have to quickly\nobtain real-time human action information. Therefore how\nto improve the recognition efficiency of HAR methods and\nreduce resource consumption is eager for further research.\nTwo-person interaction recognition and group ac-\ntionsrecognition. Mostexistingdatasetsrecordactionsfora\nsingleperson,andfewstudies[189][277][278][279]focus\non the recognition of two-person interaction recognition.\nThe reality is that people gather together, and most actions\ncaptured by the camera are group actions. For two-person\ninteraction or group actions, the urgent problem is how\nto capture the information of individual interaction. While\nconventional approaches focus on modeling temporal and\nspatial information within individuals, modeling informa-\ntion between individuals requires new skeleton topologies\nor additional neural networks.\nUnsupervised Learning. Manually annotating data is\ntedious work, so labeled data is expensive. However, deep\nlearning methods usually require large amounts of data\nfor training models Lacking the amount of data leads to\nover-fitting problems and worse performance. Meanwhile,\nunsupervised learning techniques can train models with\nunlabeled data which greatly reduces the need for labeled\ndata.Duetotherelativelyeasycollectionofunlabeleddata,\nunsupervisedlearningtechniqueshavebecomeasignificant\nresearch direction that is worthy of exploring in the future.\nEarlier unsupervised learning methods for skeleton-\nbasedactionrecognitioncanbedividedintotwocategories:\nusing RNN-based encoder-decoders and contrastive learn-\ning schemes. Several existing methods utilize RNN-based\nencoder-decodernetworks[280][281].Thedecoderofthese\nnetworksperformsapre-trainingtasktoinducetheencoder\nto extract an appropriate representation for action recogni-\ntion. RNN-based models suffer from long-range dependen-\ncies, and the GCN-based models have a similar challenge\nbecause they deliver information sequentially along a fixed\npath [130]. Therefore, the RNN and GCN-based methods\nhave limitations in extracting global representations from\nthe motion sequence, especially from long motions. The\ntransformer is more popular because of its ability to model\nthe local dynamics of joints and capture the global context\nfrom motion sequences. How to use pre-training strategies\nmoreefficientlytogeneratemotionsequencerepresentations\nthat match the natural structure of the human body remains\ntobestudied.Othermethodsexploitthecontrastivelearning\nscheme[83][282][94].Thesemethodsaugmenttheoriginal\nmotion sequence and regard it as a positive sample while\nconsidering other motion sequences as negative samples.\nThemodelisthentrainedtogeneratesimilarrepresentations\nbetween the positive samples using contrastive loss. How\nto construct and match positive and negative samples and\ncalculate their similarity is critical.\nPage 21 of 30\n--- Page 22 ---\n3D Skeleton-Based Action Recognition: A Review\n9. Conclusions\nIn this paper, we presented a comprehensive review\nof skeleton-based action recognition, focusing on the core\nchallengesoftransformingunstructureddataintostructured\nrepresentationsandmodelingthespatialfeaturesofskeletal\nsequences. Moving beyond traditional model architecture\nclassifications, we adopted a task-oriented framework that\nemphasizes key stages such as data representation, joint\nmodeling, and spatiotemporal feature modeling. This ap-\nproach provides a more holistic view of the task, offering\nvaluable insights into the intrinsic challenges and the re-\nsearch opportunities they present. We highlighted the cru-\ncial role of skeleton data preprocessing, including derived\nmodalities and data augmentation, which directly influence\nthe success of subsequent spatiotemporal modeling. Addi-\ntionally, we explored emerging cutting-edge methods such\nas hybrid architectures, Mamba models, and large language\nmodels (LLMs), offering a fresh perspective on the future\ndirectionsofresearch.Byrefiningthetaskintoprogressively\ndetailed sub-tasks, our review encourages researchers to\naddress common challenges from a data-driven perspec-\ntive,fosteringinnovationinmodeldesignandoptimization.\nUltimately, this review aims to deepen the understanding\nof skeleton-based action recognition and to guide future\nadvancementsinthefield,especiallyasitcontinuestoevolve\nwith new technologies and methodologies.\nReferences\n[1] M.A.Khan,K.Javed,S.A.Khan,T.Saba,U.Habib,J.A.Khan,and\nA.A.Abbasi,“Humanactionrecognitionusingfusionofmultiview\nanddeepfeatures:anapplicationtovideosurveillance,” Multimedia\ntools and applications , vol. 83, no. 5, pp. 14885–14911, 2024.\n[2] H. Gammulle, D. Ahmedt-Aristizabal, S. Denman, L. Tychsen-\nSmith, L. Petersson, and C. Fookes, “Continuous human action\nrecognition for human-machine interaction: a review,” ACM Com-\nputing Surveys , vol. 55, no. 13s, pp. 1–38, 2023.\n[3] X.Li,H.Chen,S.He,X.Chen,S.Dong,P.Yan,andB.Fang,“Ac-\ntionrecognitionbasedonmultimodefusionforvronlineplatform,”\nVirtual Reality , vol. 27, no. 3, pp. 1797–1812, 2023.\n[4] M.Dallel,V.Havard,D.Baudry,andX.Savatier,“Inhard-industrial\nhumanactionrecognitiondatasetinthecontextofindustrialcollabo-\nrativerobotics,”in 2020IEEEInternationalConferenceonHuman-\nMachine Systems (ICHMS) . IEEE, 2020, pp. 1–6.\n[5] J.Lin,C.Gan,andS.Han,“Tsm:Temporalshiftmoduleforefficient\nvideo understanding,” in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , 2019, pp. 7083–7093.\n[6] H. Liu, B. Ren, M. Liu, and R. Ding, “Grouped temporal en-\nhancement module for human action recognition,” in 2020 IEEE\nInternationalConferenceonImageProcessing(ICIP) . IEEE,2020,\npp. 1801–1805.\n[7] C.Feichtenhofer,H.Fan,J.Malik,andK.He,“Slowfastnetworksfor\nvideo recognition,” in Proceedings of the IEEE/CVF international\nconference on computer vision , 2019, pp. 6202–6211.\n[8] D.Tran,H.Wang,L.Torresani,J.Ray,Y.LeCun,andM.Paluri,“A\ncloserlookatspatiotemporalconvolutionsforactionrecognition,”in\nProceedingsoftheIEEEconferenceonComputerVisionandPattern\nRecognition , 2018, pp. 6450–6459.\n[9] C. Xu, L. N. Govindarajan, Y. Zhang, and L. Cheng, “Lie-x: Depth\nimagebasedarticulatedobjectposeestimation,tracking,andaction\nrecognition on lie groups,” International Journal of Computer Vi-\nsion, vol. 123, no. 3, pp. 454–478, 2017.[10] S. Baek, Z. Shi, M. Kawade, and T.-K. Kim, “Kinematic-layout-\naware random forests for depth-based action recognition,” arXiv\npreprint arXiv:1607.06972 , 2016.\n[11] R. Poppe, “A survey on vision-based human action recognition,”\nImage and vision computing , vol. 28, no. 6, pp. 976–990, 2010.\n[12] K. Simonyan and A. Zisserman, “Two-stream convolutional net-\nworks for action recognition in videos,” Advances in neural infor-\nmation processing systems , vol. 27, 2014.\n[13] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-\nstreamnetworkfusionforvideoactionrecognition,”in Proceedings\noftheIEEEconferenceoncomputervisionandpatternrecognition ,\n2016, pp. 1933–1941.\n[14] J.Liu,R.Ding,Y.Wen,N.Dai,F.Meng,F.-L.Zhang,S.Zhao,and\nM. Liu, “Explore human parsing modality for action recognition,”\nCAAI Transactions on Intelligence Technology (CAAI TIT) , 2024.\n[15] G. Johansson, “Visual perception of biological motion and a model\nforitsanalysis,” Perception&psychophysics ,vol.14,no.2,pp.201–\n211, 1973.\n[16] Z.Zhang,“Microsoftkinectsensoranditseffect,” IEEEmultimedia ,\nvol. 19, no. 2, pp. 4–10, 2012.\n[17] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang,\n“Multi-contextattentionforhumanposeestimation,”in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2017, pp. 1831–1840.\n[18] W. Yang, W. Ouyang, H. Li, and X. Wang, “End-to-end learning of\ndeformablemixtureofpartsanddeepconvolutionalneuralnetworks\nforhumanposeestimation,”in ProceedingsoftheIEEEConference\nonComputerVisionandPatternRecognition ,2016,pp.3073–3082.\n[19] Z.Cao,G.Hidalgo,T.Simon,S.-E.Wei,andY.Sheikh,“Openpose:\nrealtime multi-person 2d pose estimation using part affinity fields,”\nIEEE transactions on pattern analysis and machine intelligence ,\nvol. 43, no. 1, pp. 172–186, 2019.\n[20] L. Song, G. Yu, J. Yuan, and Z. Liu, “Human pose estimation and\nits application to action recognition: A survey,” Journal of Visual\nCommunicationandImageRepresentation ,vol.76,p.103055,2021.\n[21] C. Si, W. Chen, W. Wang, L. Wang, and T. Tan, “An attention\nenhanced graph convolutional lstm network for skeleton-based ac-\ntion recognition,” in proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , 2019, pp. 1227–1236.\n[22] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang, “Jointly learning het-\nerogeneous features for rgb-d activity recognition,” in Proceedings\noftheIEEEconferenceoncomputervisionandpatternrecognition ,\n2015, pp. 5344–5352.\n[23] R. Vemulapalli, F. Arrate, and R. Chellappa, “Human action recog-\nnition by representing 3d skeletons as points in a lie group,” in\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition , 2014, pp. 588–595.\n[24] M. E. Hussein, M. Torki, M. A. Gowayyed, and M. El-Saban,\n“Humanactionrecognitionusingatemporalhierarchyofcovariance\ndescriptorson3djointlocations,”in Twenty-thirdinternationaljoint\nconference on artificial intelligence , 2013.\n[25] M. Liu, H. Liu, and C. Chen, “Enhanced skeleton visualization\nfor view invariant human action recognition,” Pattern Recognition ,\nvol. 68, pp. 346–362, 2017.\n[26] R. Vemulapalli and R. Chellapa, “Rolling rotations for recognizing\nhuman actions from 3d skeletal data,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2016, pp.\n4471–4479.\n[27] L. Wang, D. Q. Huynh, and P. Koniusz, “A comparative review of\nrecent kinect-based action recognition algorithms,” IEEE Transac-\ntions on Image Processing , vol. 29, pp. 15–28, 2019.\n[28] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural\nnetwork for skeleton based action recognition,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition ,\n2015, pp. 1110–1118.\n[29] J. Liu, A. Shahroudy, D. Xu, and G. Wang, “Spatio-temporal lstm\nwith trust gates for 3d human action recognition,” in European\nconference on computer vision . Springer, 2016, pp. 816–833.\nPage 22 of 30\n--- Page 23 ---\n3D Skeleton-Based Action Recognition: A Review\n[30] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “An end-to-end\nspatio-temporal attention model for human action recognition from\nskeleton data,” in Proceedings of the AAAI conference on artificial\nintelligence , vol. 31, no. 1, 2017.\n[31] H. Wang and L. Wang, “Modeling temporal dynamics and spatial\nconfigurations of actions using two-stream recurrent neural net-\nworks,”in ProceedingsoftheIEEEConferenceonComputerVision\nand Pattern Recognition , 2017, pp. 499–508.\n[32] Q. Ke, M. Bennamoun, S. An, F. Sohel, and F. Boussaid, “A new\nrepresentation of skeleton sequences for 3d action recognition,” in\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition , 2017, pp. 3288–3297.\n[33] T. S. Kim and A. Reiter, “Interpretable 3d human action analysis\nwith temporal convolutional networks,” in 2017 IEEE conference\non computer vision and pattern recognition workshops (CVPRW) .\nIEEE, 2017, pp. 1623–1631.\n[34] C.Li,Q.Zhong,D.Xie,andS.Pu,“Co-occurrencefeaturelearning\nfrom skeleton data for action recognition and detection with hierar-\nchical aggregation,” arXiv preprint arXiv:1804.06055 , 2018.\n[35] S.Yan,Y.Xiong,andD.Lin,“Spatialtemporalgraphconvolutional\nnetworks for skeleton-based action recognition,” in Proceedings of\nthe AAAI conference on artificial intelligence , vol. 32, no. 1, 2018.\n[36] C.Si,Y.Jing,W.Wang,L.Wang,andT.Tan,“Skeleton-basedaction\nrecognition with spatial reasoning and temporal stack learning,”\ninProceedings of the European conference on computer vision\n(ECCV), 2018, pp. 103–118.\n[37] L.Shi,Y.Zhang,J.Cheng,andH.Lu,“Skeleton-basedactionrecog-\nnition with directed graph neural networks,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\n2019, pp. 7912–7921.\n[38] ——, “Decoupled spatial-temporal attention network for skeleton-\nbasedaction-gesturerecognition,”in ProceedingsoftheAsianCon-\nference on Computer Vision , 2020.\n[39] S.Cho,M.Maqbool,F.Liu,andH.Foroosh,“Self-attentionnetwork\nforskeleton-basedhumanactionrecognition,”in Proceedingsofthe\nIEEE/CVF Winter Conference on Applications of Computer Vision ,\n2020, pp. 635–644.\n[40] Y.Zhang,B.Wu,W.Li,L.Duan,andC.Gan,“Stst:Spatial-temporal\nspecialized transformer for skeleton-based action recognition,” in\nProceedings of the 29th ACM International Conference on Multi-\nmedia, 2021, pp. 3229–3237.\n[41] W.Xin,R.Liu,Y.Liu,Y.Chen,W.Yu,andQ.Miao,“Transformer\nforskeleton-basedactionrecognition:Areviewofrecentadvances,”\nNeurocomputing , vol. 537, pp. 164–186, 2023.\n[42] Y. Liu, H. Zhang, D. Xu, and K. He, “Graph transformer network\nwith temporal kernel attention for skeleton-based action recogni-\ntion,”Knowledge-Based Systems , vol. 240, p. 108146, 2022.\n[43] Y. Yang, H. Chen, Z. Liu, Y. Lyu, B. Zhang, S. Wu, Z. Wang,\nand K. Ren, “Action recognition with multi-stream motion mod-\neling and mutual information maximization,” arXiv preprint\narXiv:2306.07576 , 2023.\n[44] W. Xin, Q. Miao, Y. Liu, R. Liu, C.-M. Pun, and C. Shi, “Skeleton\nmixformer:Multivariatetopologyrepresentationforskeleton-based\naction recognition,” in Proceedings of the 31st ACM International\nConference on Multimedia , 2023, pp. 2211–2220.\n[45] H.Duan,M.Xu,B.Shuai,D.Modolo,Z.Tu,J.Tighe,andA.Berg-\namo, “Skeletr: Towards skeleton-based action recognition in the\nwild,”inProceedingsoftheIEEE/CVFInternationalConferenceon\nComputer Vision , 2023, pp. 13634–13644.\n[46] J. Liu, B. Yin, J. Lin, J. Wen, Y. Li, and M. Liu, “Hdbn: A\nnovel hybrid dual-branch network for robust skeleton-based action\nrecognition,” in Proceedings of the IEEE International Conference\non Multimedia and Expo Workshop (ICMEW) , 2024.\n[47] H.Yan,Y.Liu,Y.Wei,Z.Li,G.Li,andL.Lin,“Skeletonmae:graph-\nbased masked autoencoder for skeleton sequence pre-training,” in\nProceedings of the IEEE/CVF International Conference on Com-\nputer Vision , 2023, pp. 5606–5618.[48] L. Wu, L. Lin, J. Zhang, Y. Ma, and J. Liu, “Macdiff: Unified\nskeletonmodelingwithmaskedconditionaldiffusion,”in European\nConference on Computer Vision . Springer, 2025, pp. 110–128.\n[49] L. Lin, L. Wu, J. Zhang, and J. Liu, “Idempotent unsupervised\nrepresentation learning for skeleton-based action recognition,” in\nEuropean Conference on Computer Vision . Springer, 2025, pp.\n75–92.\n[50] S.-W. Li, Z.-X. Wei, W.-J. Chen, Y.-H. Yu, C.-Y. Yang, and J. Y.-j.\nHsu, “Sa-dvae: Improving zero-shot skeleton-based action recogni-\ntionbydisentangledvariationalautoencoders,”in EuropeanConfer-\nence on Computer Vision . Springer, 2025, pp. 447–462.\n[51] B. Ren, M. Liu, R. Ding, and H. Liu, “A survey on 3d skeleton-\nbasedactionrecognitionusinglearningmethod,” CyborgandBionic\nSystems, vol. 5, p. 0100, 2024.\n[52] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu,\n“Humanactionrecognitionfromvariousdatamodalities:Areview,”\nIEEE transactions on pattern analysis and machine intelligence ,\nvol. 45, no. 3, pp. 3200–3225, 2022.\n[53] J. Shin, N. Hassan, A. S. M. Miah, and S. Nishimura, “A compre-\nhensivemethodologicalsurveyofhumanactivityrecognitionacross\ndivers data modalities,” arXiv preprint arXiv:2409.09678 , 2024.\n[54] L. Xia, C.-C. Chen, and J. K. Aggarwal, “View invariant human\nactionrecognitionusinghistogramsof3djoints,”in 2012IEEEcom-\nputersocietyconferenceoncomputervisionandpatternrecognition\nworkshops . IEEE, 2012, pp. 20–27.\n[55] E. Ohn-Bar and M. Trivedi, “Joint angles similarities and hog2\nfor action recognition,” in Proceedings of the IEEE conference on\ncomputervisionandpatternrecognitionworkshops ,2013,pp.465–\n470.\n[56] R. Chaudhry, F. Ofli, G. Kurillo, R. Bajcsy, and R. Vidal, “Bio-\ninspired dynamic 3d discriminative skeletal features for human\naction recognition,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops , 2013, pp.\n471–478.\n[57] F.Ofli,R.Chaudhry,G.Kurillo,R.Vidal,andR.Bajcsy,“Sequence\nof the most informative joints (smij): A new representation for hu-\nman skeletal action recognition,” Journal of Visual Communication\nand Image Representation , vol. 25, no. 1, pp. 24–38, 2014.\n[58] M. Müller, T. Röder, and M. Clausen, “Efficient content-based\nretrievalofmotioncapturedata,”in ACMSIGGRAPH2005Papers ,\n2005, pp. 677–685.\n[59] C.Chen,Y.Zhuang,F.Nie,Y.Yang,F.Wu,andJ.Xiao,“Learning\na 3d human pose distance metric from geometric pose descriptor,”\nIEEE transactions on visualization and computer graphics , vol. 17,\nno. 11, pp. 1676–1689, 2010.\n[60] A. Yao, J. Gall, and L. Van Gool, “Coupled action recognition\nand pose estimation from multiple views,” International journal of\ncomputer vision , vol. 100, pp. 16–37, 2012.\n[61] Z.Huang,C.Wan,T.Probst,andL.VanGool,“Deeplearningonlie\ngroupsforskeleton-basedactionrecognition,”in Proceedingsofthe\nIEEEconferenceoncomputervisionandpatternrecognition ,2017,\npp. 6099–6108.\n[62] S. Zhang, X. Liu, and J. Xiao, “On geometric features for skeleton-\nbased action recognition using multilayer lstm networks,” in 2017\nIEEE Winter Conference on Applications of Computer Vision\n(WACV). IEEE, 2017, pp. 148–157.\n[63] S.Zhang,Y.Yang,J.Xiao,X.Liu,Y.Yang,D.Xie,andY.Zhuang,\n“Fusing geometric features for skeleton-based action recognition\nusingmultilayerlstmnetworks,” IEEETransactionsonMultimedia ,\nvol. 20, no. 9, pp. 2330–2343, 2018.\n[64] Y. Li, R. Xia, X. Liu, and Q. Huang, “Learning shape-motion\nrepresentations from geometric algebra spatio-temporal model for\nskeleton-basedactionrecognition,”in 2019IEEEInternationalCon-\nference on Multimedia and Expo (ICME) . IEEE, 2019, pp. 1066–\n1071.\n[65] X. Liu, Y. Li, and R. Xia, “Rotation-based spatial–temporal feature\nlearning from skeleton sequences for action recognition,” Signal,\nImage and Video Processing , vol. 14, no. 6, pp. 1227–1234, 2020.\nPage 23 of 30\n--- Page 24 ---\n3D Skeleton-Based Action Recognition: A Review\n[66] L. Shi, Y. Zhang, J. Cheng, and H. Lu, “Two-stream adaptive graph\nconvolutional networks for skeleton-based action recognition,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2019, pp. 12026–12035.\n[67] C. Li, Q. Zhong, D. Xie, and S. Pu, “Skeleton-based action recog-\nnition with convolutional neural networks,” in 2017 IEEE inter-\nnational conference on multimedia & expo workshops (ICMEW) .\nIEEE, 2017, pp. 597–600.\n[68] W. Zheng, L. Li, Z. Zhang, Y. Huang, and L. Wang, “Relational\nnetworkforskeleton-basedactionrecognition,”in 2019IEEEInter-\nnationalconferenceonmultimediaandexpo(ICME) . IEEE,2019,\npp. 826–831.\n[69] Z.Liu,H.Zhang,Z.Chen,Z.Wang,andW.Ouyang,“Disentangling\nand unifying graph convolutions for skeleton-based action recog-\nnition,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , 2020, pp. 143–152.\n[70] M. Li, S. Chen, X. Chen, Y. Zhang, Y. Wang, and Q. Tian, “Sym-\nbiotic graph neural networks for 3d skeleton-based human action\nrecognition and motion prediction,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 44, no. 6, pp. 3316–3333,\n2021.\n[71] Y. Chen, Z. Zhang, C. Yuan, B. Li, Y. Deng, and W. Hu, “Channel-\nwise topology refinement graph convolution for skeleton-based ac-\ntion recognition,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 13359–13368.\n[72] H.-g. Chi, M. H. Ha, S. Chi, S. W. Lee, Q. Huang, and K. Ramani,\n“Infogcn: Representation learning for human skeleton-based action\nrecognition,”in ProceedingsoftheIEEE/CVFConferenceonCom-\nputer Vision and Pattern Recognition , 2022, pp. 20186–20196.\n[73] H. Qiu, B. Hou, B. Ren, and X. Zhang, “Spatio-temporal tuples\ntransformer for skeleton-based action recognition,” arXiv preprint\narXiv:2201.02849 , 2022.\n[74] X.Liu,S.Zhou,L.Wang,andG.Hua,“Parallelattentioninteraction\nnetworkforfew-shotskeleton-basedactionrecognition,”in Proceed-\ningsoftheIEEE/CVFInternationalConferenceonComputerVision ,\n2023, pp. 1379–1388.\n[75] J. Do and M. Kim, “Skateformer: skeletal-temporal transformer for\nhuman action recognition,” in European Conference on Computer\nVision. Springer, 2025, pp. 401–420.\n[76] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR.Salakhutdinov,“Dropout:asimplewaytopreventneuralnetworks\nfromoverfitting,” Thejournalofmachinelearningresearch ,vol.15,\nno. 1, pp. 1929–1958, 2014.\n[77] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler, “Ef-\nficient object localization using convolutional networks,” in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition , 2015, pp. 648–656.\n[78] S. Yang, W. Xiao, M. Zhang, S. Guo, J. Zhao, and F. Shen, “Image\ndata augmentation for deep learning: A survey,” arXiv preprint\narXiv:2204.08610 , 2022.\n[79] Z.Zhong,L.Zheng,G.Kang,S.Li,andY.Yang,“Randomerasing\ndata augmentation,” in Proceedings of the AAAI conference on\nartificial intelligence , vol. 34, no. 07, 2020, pp. 13001–13008.\n[80] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassifica-\ntion with deep convolutional neural networks,” Advances in neural\ninformation processing systems , vol. 25, 2012.\n[81] J. Wei and K. Zou, “Eda: Easy data augmentation techniques for\nboosting performance on text classification tasks,” arXiv preprint\narXiv:1901.11196 , 2019.\n[82] B.Li,Y.Hou,andW.Che,“Dataaugmentationapproachesinnatural\nlanguage processing: A survey,” Ai Open, vol. 3, pp. 71–90, 2022.\n[83] H. Rao, S. Xu, X. Hu, J. Cheng, and B. Hu, “Augmented skeleton\nbasedcontrastiveactionlearningwithmomentumlstmforunsuper-\nvised action recognition,” Information Sciences , vol. 569, pp. 90–\n109, 2021.\n[84] F. M. Thoker, H. Doughty, and C. G. Snoek, “Skeleton-contrastive\n3d action representation learning,” in Proceedings of the 29th ACM\ninternational conference on multimedia , 2021, pp. 1655–1663.[85] K. Cheng, Y. Zhang, X. He, W. Chen, J. Cheng, and H. Lu,\n“Skeleton-based action recognition with shift graph convolutional\nnetwork,”in ProceedingsoftheIEEE/CVFconferenceoncomputer\nvision and pattern recognition , 2020, pp. 183–192.\n[86] P. Zhang, C. Lan, W. Zeng, J. Xing, J. Xue, and N. Zheng,\n“Semantics-guided neural networks for efficient skeleton-based hu-\nman action recognition,” in Proceedings of the IEEE/CVF Confer-\nenceonComputerVisionandPatternRecognition ,2020,pp.1112–\n1121.\n[87] Y.Hua,W.Wu,C.Zheng,A.Lu,M.Liu,C.Chen,andS.Wu,“Part\naware contrastive learning for self-supervised action recognition,”\narXiv preprint arXiv:2305.00666 , 2023.\n[88] Y. Zhou, H. Duan, A. Rao, B. Su, and J. Wang, “Self-supervised\nactionrepresentationlearningfrompartialspatio-temporalskeleton\nsequences,” in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 37, no. 3, 2023, pp. 3825–3833.\n[89] L.Franco,P.Mandica,B.Munjal,andF.Galasso,“Hyperbolicself-\npacedlearningforself-supervisedskeleton-basedactionrepresenta-\ntions,”arXiv preprint arXiv:2303.06242 , 2023.\n[90] A. Shah, A. Roy, K. Shah, S. Mishra, D. Jacobs, A. Cherian, and\nR. Chellappa, “Halp: Hallucinating latent positives for skeleton-\nbased self-supervised learning of actions,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2023, pp. 18846–18856.\n[91] C.Wu,X.-J.Wu,J.Kittler,T.Xu,S.Ahmed,M.Awais,andZ.Feng,\n“Scd-net: Spatiotemporal clues disentanglement network for self-\nsupervisedskeleton-basedactionrecognition,”in Proceedingsofthe\nAAAI Conference on Artificial Intelligence , vol. 38, no. 6, 2024, pp.\n5949–5957.\n[92] T.Guo,H.Liu,Z.Chen,M.Liu,T.Wang,andR.Ding,“Contrastive\nlearning from extremely augmented skeleton sequences for self-\nsupervised action recognition,” in Proceedings of the AAAI Confer-\nence on Artificial Intelligence , vol. 36, no. 1, 2022, pp. 762–770.\n[93] J. Zhang, L. Lin, and J. Liu, “Hierarchical consistent contrastive\nlearning for skeleton-based action recognition with growing aug-\nmentations,” in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 37, no. 3, 2023, pp. 3427–3435.\n[94] L.Lin,J.Zhang,andJ.Liu,“Actionlet-dependentcontrastivelearn-\ningforunsupervisedskeleton-basedactionrecognition,”in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2023, pp. 2363–2372.\n[95] D. Hendrycks, A. Zou, M. Mazeika, L. Tang, B. Li, D. Song, and\nJ. Steinhardt, “Pixmix: Dreamlike pictures comprehensively im-\nprovesafetymeasures,”in ProceedingsoftheIEEE/CVFConference\non Computer Vision and Pattern Recognition , 2022, pp. 16783–\n16792.\n[96] Z.Liu,S.Li,D.Wu,Z.Liu,Z.Chen,L.Wu,andS.Z.Li,“Automix:\nUnveiling the power of mixup for stronger classifiers,” in European\nConference on Computer Vision . Springer, 2022, pp. 441–458.\n[97] J. Qin, J. Fang, Q. Zhang, W. Liu, X. Wang, and X. Wang, “Re-\nsizemix: Mixing data with preserved object information and true\nlabels,”arXiv preprint arXiv:2012.11101 , 2020.\n[98] V.Verma,A.Lamb,C.Beckham,A.Najafi,I.Mitliagkas,D.Lopez-\nPaz, and Y. Bengio, “Manifold mixup: Better representations by\ninterpolatinghiddenstates,”in Internationalconferenceonmachine\nlearning. PMLR, 2019, pp. 6438–6447.\n[99] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix:\nRegularization strategy to train strong classifiers with localizable\nfeatures,”in ProceedingsoftheIEEE/CVFinternationalconference\non computer vision , 2019, pp. 6023–6032.\n[100] H. Zhang, “mixup: Beyond empirical risk minimization,” arXiv\npreprint arXiv:1710.09412 , 2017.\n[101] J.Zhang,L.Lin,andJ.Liu,“Promptedcontrastwithmaskedmotion\nmodeling: Towards versatile 3d action representation learning,” in\nProceedingsofthe31stACMInternationalConferenceonMultime-\ndia, 2023, pp. 7175–7183.\n[102] ——,“Shap-mix:Shapleyvalueguidedmixingforlong-tailedskele-\nton based action recognition,” arXiv preprint arXiv:2407.12312 ,\nPage 24 of 30\n--- Page 25 ---\n3D Skeleton-Based Action Recognition: A Review\n2024.\n[103] L.XiangandZ.Wang,“Jointmixingdataaugmentationforskeleton-\nbased action recognition,” ACM Transactions on Multimedia Com-\nputing, Communications and Applications , 2024.\n[104] J. K. Aggarwal and L. Xia, “Human activity recognition from 3d\ndata: A review,” Pattern Recognition Letters , vol. 48, pp. 70–80,\n2014.\n[105] X. Ji and H. Liu, “Advances in view-invariant human motion analy-\nsis: A review,” IEEE Transactions on Systems, Man, and Cybernet-\nics, Part C (Applications and Reviews) , vol. 40, no. 1, pp. 13–24,\n2009.\n[106] W. Zhu, C. Lan, J. Xing, W. Zeng, Y. Li, L. Shen, and X. Xie, “Co-\noccurrence feature learning for skeleton based action recognition\nusing regularized deep lstm networks,” in Proceedings of the AAAI\nconference on artificial intelligence , vol. 30, no. 1, 2016.\n[107] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+ d: A large\nscale dataset for 3d human activity analysis,” in Proceedings of the\nIEEEconferenceoncomputervisionandpatternrecognition ,2016,\npp. 1010–1019.\n[108] J. Liu, A. Shahroudy, D. Xu, and G. Wang, “Spatio-temporal\nlstm with trust gates for 3d human action recognition,” in Com-\nputer Vision–ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part III 14 .\nSpringer, 2016, pp. 816–833.\n[109] I. Lee, D. Kim, S. Kang, and S. Lee, “Ensemble deep learning\nfor skeleton-based action recognition using temporal sliding lstm\nnetworks,” in Proceedings of the IEEE international conference on\ncomputer vision , 2017, pp. 1012–1020.\n[110] P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng, “View\nadaptive recurrent neural networks for high performance human\naction recognition from skeleton data,” in Proceedings of the IEEE\ninternational conference on computer vision , 2017, pp. 2117–2126.\n[111] R. Friji, H. Drira, F. Chaieb, H. Kchok, and S. Kurtek, “Geometric\ndeep neural network using rigid and non-rigid transformations for\nhuman action recognition,” in Proceedings of the IEEE/CVF inter-\nnational conference on computer vision , 2021, pp. 12611–12620.\n[112] D. Avola, M. Cascio, L. Cinque, G. L. Foresti, C. Massaroni, and\nE. Rodolà, “2-d skeleton-based action recognition via two-branch\nstacked lstm-rnns,” IEEE Transactions on Multimedia , vol. 22,\nno. 10, pp. 2481–2496, 2019.\n[113] P. Elias, J. Sedmidubsky, and P. Zezula, “Understanding the gap\nbetween2dand3dskeleton-basedactionrecognition,”in 2019IEEE\nInternational Symposium on Multimedia (ISM) . IEEE, 2019, pp.\n192–1923.\n[114] J. Liu, A. Shahroudy, D. Xu, A. C. Kot, and G. Wang, “Skeleton-\nbased action recognition using spatio-temporal lstm network with\ntrust gates,” IEEE transactions on pattern analysis and machine\nintelligence , vol. 40, no. 12, pp. 3007–3021, 2017.\n[115] D. Avola, M. Cascio, L. Cinque, G. L. Foresti, C. Massaroni, and\nE. Rodolà, “2-d skeleton-based action recognition via two-branch\nstacked lstm-rnns,” IEEE Transactions on Multimedia , vol. 22,\nno. 10, pp. 2481–2496, 2019.\n[116] Z. Ding, P. Wang, P. O. Ogunbona, and W. Li, “Investigation of\ndifferent skeleton features for cnn-based 3d action recognition,”\nin2017 IEEE International Conference on Multimedia & Expo\nWorkshops (ICMEW) . IEEE, 2017, pp. 617–622.\n[117] Y. Xu, J. Cheng, L. Wang, H. Xia, F. Liu, and D. Tao, “Ensemble\none-dimensional convolution neural networks for skeleton-based\naction recognition,” IEEE Signal Processing Letters , vol. 25, no. 7,\npp. 1044–1048, 2018.\n[118] P.Wang,W.Li,C.Li,andY.Hou,“Actionrecognitionbasedonjoint\ntrajectory maps with convolutional neural networks,” Knowledge-\nBased Systems , vol. 158, pp. 43–53, 2018.\n[119] B.Li,Y.Dai,X.Cheng,H.Chen,Y.Lin,andM.He,“Skeletonbased\naction recognition using translation-scale invariant image mapping\nand multi-scale deep cnn,” in 2017 IEEE International Conference\non Multimedia & Expo Workshops (ICMEW) . IEEE, 2017, pp.\n601–604.[120] C. Caetano, J. Sena, F. Brémond, J. A. Dos Santos, and W. R.\nSchwartz, “Skelemotion: A new representation of skeleton joint\nsequences based on motion information for 3d action recognition,”\nin201916thIEEEInternationalConferenceonAdvancedVideoand\nSignal Based Surveillance (AVSS) . IEEE, 2019, pp. 1–8.\n[121] C. Caetano, F. Brémond, and W. R. Schwartz, “Skeleton image\nrepresentation for 3d action recognition based on tree structure and\nreference joints,” in 2019 32nd SIBGRAPI conference on graphics,\npatterns and images (SIBGRAPI) . IEEE, 2019, pp. 16–23.\n[122] H. Wang, B. Yu, K. Xia, J. Li, and X. Zuo, “Skeleton edge motion\nnetworksforhumanactionrecognition,” Neurocomputing ,vol.423,\npp. 1–12, 2021.\n[123] M.Li,S.Chen,X.Chen,Y.Zhang,Y.Wang,andQ.Tian,“Actional-\nstructural graph convolutional networks for skeleton-based action\nrecognition,”in ProceedingsoftheIEEE/CVFConferenceonCom-\nputer Vision and Pattern Recognition , 2019, pp. 3595–3603.\n[124] F. Ye, S. Pu, Q. Zhong, C. Li, D. Xie, and H. Tang, “Dynamic gcn:\nContext-enrichedtopologylearningforskeleton-basedactionrecog-\nnition,”in Proceedingsofthe28thACMInternationalConferenceon\nMultimedia , 2020, pp. 55–63.\n[125] Q. Wang, K. Zhang, and M. A. Asghar, “Skeleton-based st-gcn\nfor human action recognition with extended skeleton graph and\npartitioning strategy,” IEEE Access , vol. 10, pp. 41403–41410,\n2022.\n[126] X. Hao, J. Li, Y. Guo, T. Jiang, and M. Yu, “Hypergraph neural\nnetwork for skeleton-based action recognition,” IEEE Transactions\non Image Processing , vol. 30, pp. 2263–2275, 2021.\n[127] J. Lee, M. Lee, D. Lee, and S. Lee, “Hierarchically decomposed\ngraph convolutional networks for skeleton-based action recogni-\ntion,”arXiv preprint arXiv:2208.10741 , 2022.\n[128] X. Qin, H. Li, Y. Liu, J. Yu, C. He, and X. Zhang, “Multi-stage\npart-aware graph convolutional network for skeleton-based action\nrecognition,” IET Image Processing , vol. 16, no. 8, pp. 2063–2074,\n2022.\n[129] D. Liu, P. Chen, M. Yao, Y. Lu, Z. Cai, and Y. Tian, “Tsgcnext:\nDynamic-staticmulti-graphconvolutiozheng2024spatioforefficient\nskeleton-based action recognition with long-term learning poten-\ntial,”arXiv preprint arXiv:2304.11631 , 2023.\n[130] C. Plizzari, M. Cannici, and M. Matteucci, “Spatial temporal trans-\nformer network for skeleton-based action recognition,” in Pattern\nRecognition. ICPR International Workshops and Challenges: Vir-\ntualEvent,January10–15,2021,Proceedings,PartIII . Springer,\n2021, pp. 694–701.\n[131] Y. Chen, L. Zhao, J. Yuan, Y. Tian, Z. Xia, S. Geng, L. Han, and\nD. N. Metaxas, “Hierarchically self-supervised transformer for hu-\nman skeleton representation learning,” in Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part XXVI . Springer, 2022, pp. 185–202.\n[132] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[133] V. Veeriah, N. Zhuang, and G.-J. Qi, “Differential recurrent neural\nnetworks for action recognition,” in Proceedings of the IEEE inter-\nnational conference on computer vision , 2015, pp. 4041–4049.\n[134] S. Liao, T. Lyons, W. Yang, K. Schlegel, and H. Ni, “Logsig-\nrnn: a novel network for robust and efficient skeleton-based action\nrecognition,” arXiv preprint arXiv:2110.13008 , 2021.\n[135] T. Soo Kim and A. Reiter, “Interpretable 3d human action analysis\nwith temporal convolutional networks,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops ,\n2017, pp. 20–28.\n[136] C.Lea,M.D.Flynn,R.Vidal,A.Reiter,andG.D.Hager,“Temporal\nconvolutional networks for action segmentation and detection,” in\nproceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition , 2017, pp. 156–165.\n[137] J. Liu, N. Akhtar, and A. Mian, “Skepxels: Spatio-temporal image\nrepresentation of human skeleton joints for action recognition.” in\nPage 25 of 30\n--- Page 26 ---\n3D Skeleton-Based Action Recognition: A Review\nCVPR workshops , 2019, pp. 10–19.\n[138] H. Rahmani and A. Mian, “3d action recognition from novel view-\npoints,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 1506–1515.\n[139] J.-G. Jia, Y.-F. Zhou, X.-W. Hao, F. Li, C. Desrosiers, and C.-M.\nZhang, “Two-stream temporal convolutional networks for skeleton-\nbasedhumanactionrecognition,” JournalofComputerScienceand\nTechnology , vol. 35, pp. 538–550, 2020.\n[140] S.Yan,Y.Xiong,andD.Lin,“Spatialtemporalgraphconvolutional\nnetworks for skeleton-based action recognition,” in Proceedings of\nthe AAAI conference on artificial intelligence , vol. 32, no. 1, 2018.\n[141] B. Li, X. Li, Z. Zhang, and F. Wu, “Spatio-temporal graph routing\nfor skeleton-based action recognition,” in Proceedings of the AAAI\nConferenceonArtificialIntelligence ,vol.33,no.01,2019,pp.8561–\n8568.\n[142] W.Wu,C.Zheng,Z.Yang,C.Chen,S.Das,andA.Lu,“Frequency\nguidance matters: Skeletal action recognition by frequency-aware\nmixed transformer,” in Proceedings of the 32nd ACM International\nConference on Multimedia , 2024, pp. 4660–4669.\n[143] F. Shi, C. Lee, L. Qiu, Y. Zhao, T. Shen, S. Muralidhar, T. Han, S.-\nC. Zhu, and V. Narayanan, “Star: Sparse transformer-based action\nrecognition,” arXiv preprint arXiv:2107.07089 , 2021.\n[144] L. Wang, X. Zhao, and Y. Liu, “Skeleton feature fusion based on\nmulti-stream lstm for action recognition,” IEEE Access , vol. 6, pp.\n50788–50800, 2018.\n[145] Z.Liu,H.Zhang,Z.Chen,Z.Wang,andW.Ouyang,“Disentangling\nand unifying graph convolutions for skeleton-based action recog-\nnition,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , 2020, pp. 143–152.\n[146] H.Duan,J.Wang,K.Chen,andD.Lin,“Dg-stgcn:Dynamicspatial-\ntemporal modeling for skeleton-based action recognition,” arXiv\npreprint arXiv:2210.05895 , 2022.\n[147] J.Liu,G.Wang,P.Hu,L.-Y.Duan,andA.C.Kot,“Globalcontext-\naware attention lstm networks for 3d action recognition,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2017, pp. 1647–1656.\n[148] C. Li, C. Xie, B. Zhang, J. Han, X. Zhen, and J. Chen, “Memory\nattention networks for skeleton-based action recognition,” IEEE\nTransactions on Neural Networks and Learning Systems , vol. 33,\nno. 9, pp. 4800–4814, 2021.\n[149] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng, and\nH. Li, “T-gcn: A temporal graph convolutional network for traffic\nprediction,” IEEEtransactionsonintelligenttransportationsystems ,\nvol. 21, no. 9, pp. 3848–3858, 2019.\n[150] F.Monti,K.Otness,andM.M.Bronstein,“Motifnet:amotif-based\ngraphconvolutionalnetworkfordirectedgraphs,”in 2018IEEEdata\nscience workshop (DSW) . IEEE, 2018, pp. 225–228.\n[151] C. Wu, X.-J. Wu, and J. Kittler, “Spatial residual layer and dense\nconnection block enhanced spatial temporal graph convolutional\nnetworkforskeleton-basedactionrecognition,”in proceedingsofthe\nIEEE/CVFinternationalconferenceoncomputervisionworkshops ,\n2019, pp. 0–0.\n[152] Y.-H. Wen, L. Gao, H. Fu, F.-L. Zhang, and S. Xia, “Graph cnns\nwith motif and variable temporal block for skeleton-based action\nrecognition,” in Proceedings of the AAAI conference on artificial\nintelligence , vol. 33, no. 01, 2019, pp. 8989–8996.\n[153] W. Peng, X. Hong, and G. Zhao, “Tripool: Graph triplet pooling\nfor 3d skeleton-based action recognition,” Pattern Recognition , vol.\n115, p. 107921, 2021.\n[154] C. Zhang, Y. Hu, M. Yang, C. Li, and X. Hu, “Skeletal spatial-\ntemporal semantics guided homogeneous-heterogeneous multi-\nmodal network for action recognition,” in Proceedings of the 31st\nACM International Conference on Multimedia , 2023, pp. 3657–\n3666.\n[155] Y.Zheng,H.Huang,X.Wang,X.Yan,andL.Xu,“Spatio-temporal\nfusion for human action recognition via joint trajectory graph,”\ninProceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 38, no. 7, 2024, pp. 7579–7587.[156] L. Wang and P. Koniusz, “3mformer: Multi-order multi-mode\ntransformer for skeletal action recognition,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2023, pp. 5620–5631.\n[157] Y.Du,Y.Fu,andL.Wang,“Skeletonbasedactionrecognitionwith\nconvolutional neural network,” in 2015 3rd IAPR Asian conference\non pattern recognition (ACPR) . IEEE, 2015, pp. 579–583.\n[158] Q. Ke, S. An, M. Bennamoun, F. Sohel, and F. Boussaid, “Skele-\ntonnet: Mining deep part features for 3-d action recognition,” IEEE\nsignal processing letters , vol. 24, no. 6, pp. 731–735, 2017.\n[159] R. K. Behera, M. Jena, S. K. Rath, and S. Misra, “Co-lstm: Con-\nvolutional lstm model for sentiment analysis in social big data,”\nInformation Processing & Management , vol. 58, no. 1, p. 102435,\n2021.\n[160] Q. Wang, S. Shi, J. He, J. Peng, T. Liu, and R. Weng, “Iip-\ntransformer: Intra-inter-part transformer for skeleton-based action\nrecognition,” in 2023 IEEE International Conference on Big Data\n(BigData) . IEEE, 2023, pp. 936–945.\n[161] W.Xiang,C.Li,Y.Zhou,B.Wang,andL.Zhang,“Generativeaction\ndescription prompts for skeleton-based action recognition,” in Pro-\nceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 10276–10285.\n[162] H. Xu, Y. Gao, Z. Hui, J. Li, and X. Gao, “Language knowledge-\nassisted representation learning for skeleton-based action recogni-\ntion,”arXiv preprint arXiv:2305.12398 , 2023.\n[163] J.Liu,C.Chen,andM.Liu,“Multi-modalityco-learningforefficient\nskeleton-basedactionrecognition,”in Proceedingsofthe32ndACM\nInternational Conference on Multimedia , 2024, pp. 4909–4918.\n[164] H. Qu, Y. Cai, and J. Liu, “Llms are good action recognizers,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2024, pp. 18395–18406.\n[165] Y. Zhou, W. Qiang, A. Rao, N. Lin, B. Su, and J. Wang, “Zero-shot\nskeleton-basedactionrecognitionviamutualinformationestimation\nand maximization,” in Proceedings of the 31st ACM International\nConference on Multimedia , 2023, pp. 5302–5310.\n[166] T. Yan, W. Zeng, Y. Xiao, X. Tong, B. Tan, Z. Fang, Z. Cao,\nand J. T. Zhou, “Crossglg: Llm guides one-shot skeleton-based\n3d action recognition in a cross-level manner,” arXiv preprint\narXiv:2403.10082 , 2024.\n[167] S.B.Yussif,N.Xie,Y.Yang,andH.T.Shen,“Self-relationalgraph\nconvolution network for skeleton-based action recognition,” in Pro-\nceedings of the 31st ACM International Conference on Multimedia ,\n2023, pp. 27–36.\n[168] M.Ijaz,R.Diaz,andC.Chen,“Multimodaltransformerfornursing\nactivity recognition,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 2022, pp. 2065–2074.\n[169] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural\nnetworks,” IEEEtransactionsonSignalProcessing ,vol.45,no.11,\npp. 2673–2681, 1997.\n[170] R.Palm,U.Paquet,andO.Winther,“Recurrentrelationalnetworks,”\nAdvances in neural information processing systems , vol. 31, 2018.\n[171] Y.Sun,Y.Shen,andL.Ma,“Msst-rt:Multi-streamspatial-temporal\nrelativetransformerforskeleton-basedactionrecognition,” Sensors,\nvol. 21, no. 16, p. 5339, 2021.\n[172] R. Cui, A. Zhu, S. Zhang, and G. Hua, “Multi-source learning for\nskeleton-based action recognition using deep lstm networks,” in\n201824thInternationalConferenceonPatternRecognition(ICPR) .\nIEEE, 2018, pp. 547–552.\n[173] H. Liu, J. Tu, and M. Liu, “Two-stream 3d convolutional neu-\nral network for skeleton-based action recognition,” arXiv preprint\narXiv:1705.08106 , 2017.\n[174] H. Zhou, Q. Liu, and Y. Wang, “Learning discriminative represen-\ntationsforskeletonbasedactionrecognition,”in Proceedingsofthe\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2023, pp. 10608–10617.\n[175] R. Bai, M. Li, B. Meng, F. Li, M. Jiang, J. Ren, and D. Sun,\n“Hierarchical graph convolutional skeleton transformer for action\nrecognition,”in 2022IEEEInternationalConferenceonMultimedia\nPage 26 of 30\n--- Page 27 ---\n3D Skeleton-Based Action Recognition: A Review\nand Expo (ICME) . IEEE, 2022, pp. 01–06.\n[176] J. Dong, S. Sun, Z. Liu, S. Chen, B. Liu, and X. Wang, “Hierarchi-\ncal contrast for unsupervised skeleton-based action representation\nlearning,” in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 37, no. 1, 2023, pp. 525–533.\n[177] J.Liu,G.Wang,L.-Y.Duan,K.Abdiyeva,andA.C.Kot,“Skeleton-\nbasedhumanactionrecognitionwithglobalcontext-awareattention\nlstm networks,” IEEE Transactions on Image Processing , vol. 27,\nno. 4, pp. 1586–1599, 2017.\n[178] C. Wu, X.-J. Wu, and J. Kittler, “Spatial residual layer and dense\nconnection block enhanced spatial temporal graph convolutional\nnetworkforskeleton-basedactionrecognition,”in proceedingsofthe\nIEEE/CVFinternationalconferenceoncomputervisionworkshops ,\n2019, pp. 0–0.\n[179] W. Peng, X. Hong, H. Chen, and G. Zhao, “Learning graph con-\nvolutional network for skeleton-based human action recognition\nby neural searching,” in Proceedings of the AAAI conference on\nartificial intelligence , vol. 34, no. 03, 2020, pp. 2669–2676.\n[180] B. Zoph, “Neural architecture search with reinforcement learning,”\narXiv preprint arXiv:1611.01578 , 2016.\n[181] Z. Fang, X. Zhang, T. Cao, Y. Zheng, and M. Sun, “Spatial-\ntemporal slowfast graph convolutional network for skeleton-based\naction recognition,” IET Computer Vision , vol. 16, no. 3, pp. 205–\n217, 2022.\n[182] X. Gao, W. Hu, J. Tang, J. Liu, and Z. Guo, “Optimized skeleton-\nbased action recognition via sparsified graph regression,” in Pro-\nceedings of the 27th ACM international conference on multimedia ,\n2019, pp. 601–610.\n[183] K. Cheng, Y. Zhang, C. Cao, L. Shi, J. Cheng, and H. Lu, “Decou-\nplinggcnwithdropgraphmoduleforskeleton-basedactionrecogni-\ntion,”inComputerVision–ECCV2020:16thEuropeanConference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16 .\nSpringer, 2020, pp. 536–553.\n[184] J. Xie, Y. Meng, Y. Zhao, A. Nguyen, X. Yang, and Y. Zheng,\n“Dynamic semantic-based spatial graph convolution network for\nskeleton-based human action recognition,” in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 38, no. 6, 2024, pp.\n6225–6233.\n[185] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural\nnetworks,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2018, pp. 7794–7803.\n[186] S.ChoandH.Foroosh,“Spatio-temporalfusionnetworksforaction\nrecognition,” in Asian conference on computer vision . Springer,\n2018, pp. 347–364.\n[187] S. Kim, D. Ahn, and B. C. Ko, “Cross-modal learning with 3d\ndeformable attention for action recognition,” in Proceedings of the\nIEEE/CVF international conference on computer vision , 2023, pp.\n10265–10275.\n[188] Z. Gao, P. Wang, P. Lv, X. Jiang, Q. Liu, P. Wang, M. Xu, and\nW. Li, “Focal and global spatial-temporal transformer for skeleton-\nbased action recognition,” in Proceedings of the Asian Conference\non Computer Vision , 2022, pp. 382–398.\n[189] Y. Pang, Q. Ke, H. Rahmani, J. Bailey, and J. Liu, “Igformer:\nInteraction graph transformer for skeleton-based human interaction\nrecognition,”in ComputerVision–ECCV2022:17thEuropeanCon-\nference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nXXV. Springer, 2022, pp. 605–622.\n[190] H. Liu, Y. Liu, Y. Chen, C. Yuan, B. Li, and W. Hu, “Transkeleton:\nHierarchical spatial–temporal transformer for skeleton-based action\nrecognition,” IEEE Transactions on Circuits and Systems for Video\nTechnology , vol. 33, no. 8, pp. 4137–4148, 2023.\n[191] Y.-F.Song,Z.Zhang,C.Shan,andL.Wang,“Constructingstronger\nand faster baselines for skeleton-based action recognition,” IEEE\ntransactions on pattern analysis and machine intelligence , vol. 45,\nno. 2, pp. 1474–1488, 2022.\n[192] X. Yun, C. Xu, K. Riou, K. Dong, Y. Sun, S. Li, K. Subrin, and\nP. Le Callet, “Behavioral recognition of skeletal data based on tar-\ngeted dual fusion strategy,” in Proceedings of the AAAI Conferenceon Artificial Intelligence , vol. 38, no. 7, 2024, pp. 6917–6925.\n[193] N. Martinel, M. Serrao, and C. Micheloni, “Skelmamba: A state\nspacemodelforefficientskeletonactionrecognitionofneurological\ndisorders,” arXiv preprint arXiv:2411.19544 , 2024.\n[194] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. Krüger, and\nA. Weber, “Documentation mocap database hdm05,” Computer\nGraphics Technical Report CG-2007-2, Universität Bonn , 2007.\n[195] W. Li, Z. Zhang, and Z. Liu, “Action recognition based on a bag of\n3d points,” in 2010 IEEE computer society conference on computer\nvision and pattern recognition-workshops . IEEE, 2010, pp. 9–14.\n[196] J. Sung, C. Ponce, B. Selman, and A. Saxena, “Human activity\ndetection from rgbd images.” plan, activity, and intent recognition ,\nvol. 64, 2011.\n[197] J.Wang,Z.Liu,Y.Wu,andJ.Yuan,“Miningactionletensemblefor\nactionrecognitionwithdepthcameras,”in 2012IEEEconferenceon\ncomputer vision and pattern recognition . IEEE, 2012, pp. 1290–\n1297.\n[198] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras,\n“Two-person interaction detection using body-pose features and\nmultiple instance learning,” in 2012 IEEE computer society confer-\nenceoncomputervisionandpatternrecognitionworkshops . IEEE,\n2012, pp. 28–35.\n[199] F.Ofli,R.Chaudhry,G.Kurillo,R.Vidal,andR.Bajcsy,“Berkeley\nmhad: A comprehensive multimodal human action database,” in\n2013 IEEE workshop on applications of computer vision (WACV) .\nIEEE, 2013, pp. 53–60.\n[200] H.S.Koppula,R.Gupta,andA.Saxena,“Learninghumanactivities\nandobjectaffordancesfromrgb-dvideos,” TheInternationaljournal\nof robotics research , vol. 32, no. 8, pp. 951–970, 2013.\n[201] M. Munaro, G. Ballin, S. Michieletto, and E. Menegatti, “3d flow\nestimationforhumanactionrecognitionfromcoloredpointclouds,”\nBiologically Inspired Cognitive Architectures , vol. 5, pp. 42–51,\n2013.\n[202] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards\nunderstandingactionrecognition,”in ProceedingsoftheIEEEinter-\nnational conference on computer vision , 2013, pp. 3192–3199.\n[203] O. Oreifej and Z. Liu, “Hon4d: Histogram of oriented 4d normals\nforactivityrecognitionfromdepthsequences,”in Proceedingsofthe\nIEEEconferenceoncomputervisionandpatternrecognition ,2013,\npp. 716–723.\n[204] C. Ellis, S. Z. Masood, M. F. Tappen, J. J. LaViola, and R. Suk-\nthankar, “Exploring the trade-off between accuracy and observa-\ntional latency in action recognition,” International Journal of Com-\nputer Vision , vol. 101, pp. 420–436, 2013.\n[205] A.-A. Liu, Y.-T. Su, P.-P. Jia, Z. Gao, T. Hao, and Z.-X. Yang,\n“Multiple/single-view human action recognition via part-induced\nmultitask structural learning,” IEEE transactions on cybernetics ,\nvol. 45, no. 6, pp. 1194–1208, 2014.\n[206] J. Wang, X. Nie, Y. Xia, Y. Wu, and S.-C. Zhu, “Cross-view action\nmodeling, learning and recognition,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2014, pp.\n2649–2656.\n[207] I.Theodorakopoulos,D.Kastaniotis,G.Economou,andS.Fotopou-\nlos,“Pose-basedhumanactionrecognitionviasparserepresentation\nindissimilarityspace,” JournalofVisualCommunicationandImage\nRepresentation , vol. 25, no. 1, pp. 12–23, 2014.\n[208] H. Rahmani, A. Mahmood, D. Huynh, and A. Mian, “Action clas-\nsification with locality-constrained linear coding,” in 2014 22nd\nInternationalConferenceonPatternRecognition . IEEE,2014,pp.\n3511–3516.\n[209] A.-A. Liu, W.-Z. Nie, Y.-T. Su, L. Ma, T. Hao, and Z.-X. Yang,\n“Coupled hidden conditional random fields for rgb-d human action\nrecognition,” Signal Processing , vol. 112, pp. 74–82, 2015.\n[210] C. Chen, R. Jafari, and N. Kehtarnavaz, “Utd-mhad: A multimodal\ndataset for human action recognition utilizing a depth camera and a\nwearableinertialsensor,”in 2015IEEEInternationalconferenceon\nimage processing (ICIP) . IEEE, 2015, pp. 168–172.\nPage 27 of 30\n--- Page 28 ---\n3D Skeleton-Based Action Recognition: A Review\n[211] H. Rahmani, A. Mahmood, D. Huynh, and A. Mian, “Histogram of\noriented principal components for cross-view action recognition,”\nIEEE transactions on pattern analysis and machine intelligence ,\nvol. 38, no. 12, pp. 2430–2443, 2016.\n[212] J.Zhang,W.Li,P.Wang,P.Ogunbona,S.Liu,andC.Tang,“Alarge\nscalergb-ddatasetforactionrecognition,”in UnderstandingHuman\nActivities Through 3D Sensors: Second International Workshop,\nUHA3DS 2016, Held in Conjunction with the 23rd International\nConference on Pattern Recognition, ICPR 2016, Cancun, Mexico,\nDecember4,2016,RevisedSelectedPapers2 . Springer,2018,pp.\n101–114.\n[213] C. Liu, Y. Hu, Y. Li, S. Song, and J. Liu, “Pku-mmd: A large scale\nbenchmark for continuous multi-modal human action understand-\ning,”arXiv preprint arXiv:1703.07475 , 2017.\n[214] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a\nnew model and the kinetics dataset,” in proceedings of the IEEE\nConferenceonComputerVisionandPatternRecognition ,2017,pp.\n6299–6308.\n[215] Y. Ji, F. Xu, Y. Yang, F. Shen, H. T. Shen, and W.-S. Zheng, “A\nlarge-scale rgb-d database for arbitrary-view human action recogni-\ntion,” inProceedings of the 26th ACM international Conference on\nMultimedia , 2018, pp. 1510–1518.\n[216] E.Calabrese,G.Taverni,C.AwaiEasthope,S.Skriabine,F.Corradi,\nL. Longinotti, K. Eng, and T. Delbruck, “Dhp19: Dynamic vision\nsensor 3d human pose dataset,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition workshops ,\n2019, pp. 0–0.\n[217] M.Martin,A.Roitberg,M.Haurilet,M.Horne,S.Reiß,M.Voit,and\nR.Stiefelhagen,“Drive&act:Amulti-modaldatasetforfine-grained\ndriverbehaviorrecognitioninautonomousvehicles,”in Proceedings\nof the IEEE/CVF International Conference on Computer Vision ,\n2019, pp. 2801–2810.\n[218] Q. Kong, Z. Wu, Z. Deng, M. Klinkigt, B. Tong, and T. Murakami,\n“Mmact:Alarge-scaledatasetforcrossmodalhumanactionunder-\nstanding,” in Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision , 2019, pp. 8658–8667.\n[219] J.Liu,A.Shahroudy,M.Perez,G.Wang,L.-Y.Duan,andA.C.Kot,\n“Ntu rgb+ d 120: A large-scale benchmark for 3d human activity\nunderstanding,” IEEEtransactionsonpatternanalysisandmachine\nintelligence , vol. 42, no. 10, pp. 2684–2701, 2019.\n[220] J. Jang, D. Kim, C. Park, M. Jang, J. Lee, and J. Kim, “Etri-\nactivity3d: A large-scale rgb-d dataset for robots to recognize daily\nactivitiesoftheelderly,”in 2020IEEE/RSJInternationalConference\nonIntelligentRobotsandSystems(IROS) . IEEE,2020,pp.10990–\n10997.\n[221] L. Wang, B. Sun, J. Robinson, T. Jing, and Y. Fu, “Ev-action:\nElectromyography-visionmulti-modalactiondataset,”in 202015th\nIEEE International Conference on Automatic Face and Gesture\nRecognition (FG 2020) . IEEE, 2020, pp. 160–167.\n[222] Y.Ben-Shabat,X.Yu,F.Saleh,D.Campbell,C.Rodriguez-Opazo,\nH. Li, and S. Gould, “The ikea asm dataset: Understanding people\nassembling furniture through actions, objects and pose,” in Pro-\nceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision , 2021, pp. 847–859.\n[223] T. Li, J. Liu, W. Zhang, Y. Ni, W. Wang, and Z. Li, “Uav-human: A\nlargebenchmarkforhumanbehaviorunderstandingwithunmanned\naerial vehicles,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , 2021, pp. 16266–16275.\n[224] X. Ding, K. Yang, and W. Chen, “An attention-enhanced recurrent\ngraphconvolutionalnetworkforskeleton-basedactionrecognition,”\ninProceedings of the 2019 2nd International Conference on Signal\nProcessing and Machine Learning , 2019, pp. 79–84.\n[225] S. Li, J. Yi, Y. A. Farha, and J. Gall, “Pose refinement graph\nconvolutionalnetworkforskeleton-basedactionrecognition,” IEEE\nRoboticsandAutomationLetters ,vol.6,no.2,pp.1028–1035,2021.\n[226] Y. Yoon, J. Yu, and M. Jeon, “Predictively encoded graph convolu-\ntional network for noise-robust skeleton-based action recognition,”\nApplied Intelligence , pp. 1–15, 2022.[227] G. Hu, B. Cui, and S. Yu, “Skeleton-based action recognition\nwith synchronous local and non-local spatio-temporal learning and\nfrequency attention,” in 2019 IEEE International conference on\nmultimedia and expo (ICME) . IEEE, 2019, pp. 1216–1221.\n[228] L.Shi,Y.Zhang,J.Cheng,andH.Lu,“Skeleton-basedactionrecog-\nnition with multi-stream adaptive graph convolutional networks,”\nIEEE Transactions on Image Processing , vol. 29, pp. 9532–9545,\n2020.\n[229] D. Yang, M. M. Li, H. Fu, J. Fan, Z. Zhang, and H. Leung,\n“Unifying graph embedding features with graph convolutional\nnetworks for skeleton-based action recognition,” arXiv preprint\narXiv:2003.03007 , 2020.\n[230] T. Chen, D. Zhou, J. Wang, S. Wang, Y. Guan, X. He, and\nE. Ding, “Learning multi-granular spatio-temporal graph network\nfor skeleton-based action recognition,” in Proceedings of the 29th\nACMinternationalconferenceonmultimedia ,2021,pp.4334–4342.\n[231] Y. Obinata and T. Yamamoto, “Temporal extension module for\nskeleton-basedactionrecognition,”in 202025thInternationalCon-\nferenceonPatternRecognition(ICPR) . IEEE,2021,pp.534–540.\n[232] H.Duan,Y.Zhao,K.Chen,D.Lin,andB.Dai,“Revisitingskeleton-\nbasedactionrecognition,”in ProceedingsoftheIEEE/CVFConfer-\nenceonComputerVisionandPatternRecognition ,2022,pp.2969–\n2978.\n[233] H. Liu, Y. Liu, M. Ren, H. Wang, Y. Wang, and Z. Sun, “Re-\nvealing key details to see differences: A novel prototypical per-\nspective for skeleton-based action recognition,” arXiv preprint\narXiv:2411.18941 , 2024.\n[234] R. Yeh, Y.-T. Hu, and A. Schwing, “Chirality nets for human pose\nregression,” Advances in Neural Information Processing Systems ,\nvol. 32, 2019.\n[235] C. Plizzari, M. Cannici, and M. Matteucci, “Skeleton-based action\nrecognition via spatial and temporal transformer networks,” Com-\nputer Vision and Image Understanding , vol. 208, p. 103219, 2021.\n[236] L.Ke,K.-C.Peng,andS.Lyu,“Towardsto-atspatio-temporalfocus\nfor skeleton-based action recognition,” in Proceedings of the AAAI\nconference on artificial intelligence , vol. 36, no. 1, 2022, pp. 1131–\n1139.\n[237] H. Duan, J. Wang, K. Chen, and D. Lin, “Pyskl: Towards good\npractices for skeleton action recognition,” in Proceedings of the\n30thACMInternationalConferenceonMultimedia ,2022,pp.7351–\n7354.\n[238] R. Hachiuma, F. Sato, and T. Sekii, “Unified keypoint-based action\nrecognitionframeworkviastructuredkeypointpooling,”in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2023, pp. 22962–22971.\n[239] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural\nnetwork for skeleton based action recognition,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition ,\n2015, pp. 1110–1118.\n[240] R. Zhao, K. Wang, H. Su, and Q. Ji, “Bayesian graph convolution\nlstm for skeleton based action recognition,” in Proceedings of the\nIEEE/CVFInternationalConferenceonComputerVision ,2019,pp.\n6882–6892.\n[241] S.Li,W.Li,C.Cook,C.Zhu,andY.Gao,“Independentlyrecurrent\nneural network (indrnn): Building a longer and deeper rnn,” in\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition , 2018, pp. 5457–5466.\n[242] P. Wang, Z. Li, Y. Hou, and W. Li, “Action recognition based on\njoint trajectory maps using convolutional neural networks,” in Pro-\nceedings of the 24th ACM international conference on Multimedia ,\n2016, pp. 102–106.\n[243] C. Li, Y. Hou, P. Wang, and W. Li, “Joint distance maps based\nactionrecognitionwithconvolutionalneuralnetworks,” IEEESignal\nProcessing Letters , vol. 24, no. 5, pp. 624–628, 2017.\n[244] Q.Ke,M.Bennamoun,S.An,F.Sohel,andF.Boussaid,“Learning\ncliprepresentationsforskeleton-based3dactionrecognition,” IEEE\nTransactions on Image Processing , vol. 27, no. 6, pp. 2842–2855,\n2018.\nPage 28 of 30\n--- Page 29 ---\n3D Skeleton-Based Action Recognition: A Review\n[245] R. Memmesheimer, N. Theisen, and D. Paulus, “Gimme signals:\nDiscriminativesignalencodingformultimodalactivityrecognition,”\nin2020 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS) . IEEE, 2020, pp. 10394–10401.\n[246] A. Banerjee, P. K. Singh, and R. Sarkar, “Fuzzy integral-based cnn\nclassifier fusion for 3d skeleton action recognition,” IEEE transac-\ntionsoncircuitsandsystemsforvideotechnology ,vol.31,no.6,pp.\n2206–2216, 2020.\n[247] T. Li, Q. Ke, H. Rahmani, R. E. Ho, H. Ding, and J. Liu, “Else-\nnet: Elastic semantic network for continual action recognition from\nskeleton data,” in Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , 2021, pp. 13434–13443.\n[248] J. Zhang, G. Ye, Z. Tu, Y. Qin, Q. Qin, J. Zhang, and J. Liu, “A\nspatial attentive and temporal dilated (satd) gcn for skeleton-based\naction recognition,” CAAI Transactions on Intelligence Technology ,\nvol. 7, no. 1, pp. 46–55, 2022.\n[249] J. Liu, X. Wang, C. Wang, Y. Gao, and M. Liu, “Temporal de-\ncoupling graph convolutional network for skeleton-based gesture\nrecognition,” IEEE Transactions on Multimedia , 2023.\n[250] N. Trivedi and R. K. Sarvadevabhatla, “Psumnet: Unified modality\npartstreamsareallyouneedforefficientpose-basedactionrecogni-\ntion,”inComputerVision–ECCV2022Workshops:TelAviv,Israel,\nOctober 23–27, 2022, Proceedings, Part V . Springer, 2023, pp.\n211–227.\n[251] W. Xiang, C. Li, Y. Zhou, B. Wang, and L. Zhang, “Language\nsupervised training for skeleton-based action recognition,” arXiv\npreprint arXiv:2208.05318 , 2022.\n[252] Y. Zhou, X. Yan, Z.-Q. Cheng, Y. Yan, Q. Dai, and X.-S. Hua,\n“Blockgcn: Redefine topology awareness for skeleton-based action\nrecognition,”in ProceedingsoftheIEEE/CVFConferenceonCom-\nputer Vision and Pattern Recognition , 2024, pp. 2049–2058.\n[253] W. Myung, N. Su, J.-H. Xue, and G. Wang, “Degcn: Deformable\ngraph convolutional networks for skeleton-based action recogni-\ntion,”IEEE Transactions on Image Processing , vol. 33, pp. 2477–\n2490, 2024.\n[254] Q. Wang, J. Peng, S. Shi, T. Liu, J. He, and R. Weng, “Iip-\ntransformer: Intra-inter-part transformer for skeleton-based action\nrecognition,” arXiv preprint arXiv:2110.13385 , 2021.\n[255] Y. Zhou, C. Li, Z.-Q. Cheng, Y. Geng, X. Xie, and M. Keuper,\n“Hypergraph transformer for skeleton-based action recognition,”\narXiv preprint arXiv:2211.09590 , 2022.\n[256] D.Ahn,S.Kim,H.Hong,andB.C.Ko,“Star-transformer:Aspatio-\ntemporal cross attention transformer for human action recognition,”\ninProceedingsoftheIEEE/CVFWinterConferenceonApplications\nof Computer Vision , 2023, pp. 3330–3339.\n[257] M. Ibh, S. Grasshof, D. Witzner, and P. Madeleine, “Tempose: A\nnew skeleton-based transformer model designed for fine-grained\nmotionrecognitioninbadminton,”in ProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition ,2023,pp.\n5198–5207.\n[258] Q.DeSmedt,H.Wannous,J.-P.Vandeborre,J.Guerry,B.LeSaux,\nand D. Filliat, “Shrec’17 track: 3d hand gesture recognition using a\ndepth and skeletal dataset,” in 3DOR-10th Eurographics Workshop\non 3D Object Retrieval , 2017, pp. 1–6.\n[259] Q. De Smedt, H. Wannous, and J.-P. Vandeborre, “Skeleton-based\ndynamichandgesturerecognition,”in ProceedingsoftheIEEECon-\nference on Computer Vision and Pattern Recognition Workshops ,\n2016, pp. 1–9.\n[260] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.\n6m: Large scale datasets and predictive methods for 3d human\nsensing in natural environments,” IEEE transactions on pattern\nanalysis and machine intelligence , vol. 36, no. 7, pp. 1325–1339,\n2013.\n[261] K. Soomro, “Ucf101: A dataset of 101 human actions classes from\nvideos in the wild,” arXiv preprint arXiv:1212.0402 , 2012.\n[262] H.Kuehne,H.Jhuang,E.Garrote,T.Poggio,andT.Serre,“Hmdb:\na large video database for human motion recognition,” in 2011\nInternational conference on computer vision . IEEE, 2011, pp.2556–2563.\n[263] J.-H. Song, K. Kong, and S.-J. Kang, “Dynamic hand gesture\nrecognition using improved spatio-temporal graph convolutional\nnetwork,” IEEE Transactions on Circuits and Systems for Video\nTechnology , vol. 32, no. 9, pp. 6227–6239, 2022.\n[264] L. Wang and P. Koniusz, “Temporal-viewpoint transportation plan\nforskeletalfew-shotactionrecognition,”in ProceedingsoftheAsian\nConference on Computer Vision , 2022, pp. 4176–4193.\n[265] N. Ma, H. Zhang, X. Li, S. Zhou, Z. Zhang, J. Wen, H. Li, J. Gu,\nand J. Bu, “Learning spatial-preserved skeleton representations for\nfew-shotactionrecognition,”in ComputerVision–ECCV2022:17th\nEuropean Conference, Tel Aviv, Israel, October 23–27, 2022, Pro-\nceedings, Part IV . Springer, 2022, pp. 174–191.\n[266] E.HofferandN.Ailon,“Deepmetriclearningusingtripletnetwork,”\ninSimilarity-BasedPatternRecognition:ThirdInternationalWork-\nshop,SIMBAD2015,Copenhagen,Denmark,October12-14,2015.\nProceedings 3 . Springer, 2015, pp. 84–92.\n[267] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu, “Learning fine-grained image similarity with\ndeep ranking,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2014, pp. 1386–1393.\n[268] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning\nforfastadaptationofdeepnetworks,”in Internationalconferenceon\nmachine learning . PMLR, 2017, pp. 1126–1135.\n[269] R.Memmesheimer,S.Häring,N.Theisen,andD.Paulus,“Skeleton-\ndml:Deepmetriclearningforskeleton-basedone-shotactionrecog-\nnition,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision , 2022, pp. 3702–3710.\n[270] S. Berti, A. Rosasco, M. Colledanchise, and L. Natale, “One-shot\nopen-set skeleton-based action recognition,” in 2022 IEEE-RAS\n21st International Conference on Humanoid Robots (Humanoids) .\nIEEE, 2022, pp. 765–772.\n[271] K. Peng, A. Roitberg, K. Yang, J. Zhang, and R. Stiefelhagen,\n“Delving deep into one-shot skeleton-based action recognition with\ndiverse occlusions,” IEEE Transactions on Multimedia , 2023.\n[272] Y. Tang, L. Ma, W. Liu, and W. Zheng, “Long-term human motion\nprediction by modeling motion context and enhancing motion dy-\nnamic,”arXiv preprint arXiv:1805.02513 , 2018.\n[273] H.-F. Sang, Z.-Z. Chen, and D.-K. He, “Human motion prediction\nbasedonattentionmechanism,” MultimediaToolsandApplications ,\nvol. 79, pp. 5529–5544, 2020.\n[274] T.Ma,Y.Nie,C.Long,Q.Zhang,andG.Li,“Progressivelygenerat-\ningbetterinitialguessestowardsnextstagesforhigh-qualityhuman\nmotionprediction,”in ProceedingsoftheIEEE/CVFConferenceon\nComputer Vision and Pattern Recognition , 2022, pp. 6437–6446.\n[275] W. Mao, M. Liu, and M. Salzmann, “Weakly-supervised action\ntransition learning for stochastic human motion prediction,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2022, pp. 8151–8160.\n[276] W. Guo, Y. Du, X. Shen, V. Lepetit, X. Alameda-Pineda, and\nF.Moreno-Noguer,“Backtomlp:Asimplebaselineforhumanmo-\ntionprediction,”in ProceedingsoftheIEEE/CVFWinterConference\non Applications of Computer Vision , 2023, pp. 4809–4819.\n[277] M. Perez, J. Liu, and A. C. Kot, “Interaction relational network\nfor mutual action recognition,” IEEE Transactions on Multimedia ,\nvol. 24, pp. 366–376, 2021.\n[278] Y. Wen, Z. Tang, Y. Pang, B. Ding, and M. Liu, “Interactive\nspatiotemporal token attention network for skeleton-based general\ninteractive action recognition,” in 2023 IEEE/RSJ International\nConferenceonIntelligentRobotsandSystems(IROS) . IEEE,2023,\npp. 7886–7892.\n[279] Y. Wen, M. Liu, S. Wu, and B. Ding, “Chase: Learning convex hull\nadaptive shift for skeleton-based multi-entity action recognition,”\narXiv preprint arXiv:2410.07153 , 2024.\n[280] N. Zheng, J. Wen, R. Liu, L. Long, J. Dai, and Z. Gong, “Unsuper-\nvised representation learning with long-term dynamics for skeleton\nbased action recognition,” in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 32, no. 1, 2018.\nPage 29 of 30\n--- Page 30 ---\n3D Skeleton-Based Action Recognition: A Review\n[281] K.Su,X.Liu,andE.Shlizerman, “Predict&cluster:Unsupervised\nskeletonbasedactionrecognition,”in ProceedingsoftheIEEE/CVF\nconference on computer vision and pattern recognition , 2020, pp.\n9631–9640.\n[282] P. Wang, J. Wen, C. Si, Y. Qian, and L. Wang, “Contrast-\nreconstruction representation learning for self-supervised skeleton-\nbasedactionrecognition,” IEEETransactionsonImageProcessing ,\nvol. 31, pp. 6224–6238, 2022.\nPage 30 of 30",
  "text_length": 206699
}