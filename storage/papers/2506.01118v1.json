{
  "id": "http://arxiv.org/abs/2506.01118v1",
  "title": "Revolutionizing Radiology Workflow with Factual and Efficient CXR Report\n  Generation",
  "summary": "The escalating demand for medical image interpretation underscores the\ncritical need for advanced artificial intelligence solutions to enhance the\nefficiency and accuracy of radiological diagnoses. This paper introduces\nCXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model\nspecifically engineered for automated chest X-ray (CXR) report generation. We\npropose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning\n(CGAFT), which meticulously integrates expert clinical feedback into an\nadversarial learning framework to mitigate factual inconsistencies and improve\ndiagnostic precision. Complementing this, our Knowledge Graph Augmentation\nModule (KGAM) acts as an inference-time safeguard, dynamically verifying\ngenerated medical statements against authoritative knowledge bases to minimize\nhallucinations and ensure standardized terminology. Leveraging a comprehensive\ndataset of millions of paired CXR images and expert reports, our experiments\ndemonstrate that CXR-PathFinder significantly outperforms existing\nstate-of-the-art medical vision-language models across various quantitative\nmetrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14):\n59.5). Furthermore, blinded human evaluation by board-certified radiologists\nconfirms CXR-PathFinder's superior clinical utility, completeness, and\naccuracy, establishing its potential as a reliable and efficient aid for\nradiological practice. The developed method effectively balances high\ndiagnostic fidelity with computational efficiency, providing a robust solution\nfor automated medical report generation.",
  "authors": [
    "Pimchanok Sukjai",
    "Apiradee Boonmee"
  ],
  "published": "2025-06-01T18:47:49Z",
  "updated": "2025-06-01T18:47:49Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01118v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01118v1  [cs.CV]  1 Jun 2025Revolutionizing Radiology Workflow with Factual\nand Efficient CXR Report Generation\nPimchanok Sukjai, Apiradee Boonmee\nKasem Bundit University\nAbstract. The escalating demand for medical image interpretation un-\nderscores the critical need for advanced artificial intelligence solutions to\nenhance the efficiency and accuracy of radiological diagnoses. This pa-\nper introduces CXR-PathFinder, a novel Large Language Model (LLM)-\ncentric foundation model specifically engineered for automated chest X-\nray (CXR) report generation. We propose a unique training paradigm,\nClinician-GuidedAdversarialFine-Tuning(CGAFT),whichmeticulously\nintegrates expert clinical feedback into an adversarial learning frame-\nwork to mitigate factual inconsistencies and improve diagnostic preci-\nsion. Complementing this, our Knowledge Graph Augmentation Mod-\nule (KGAM) acts as an inference-time safeguard, dynamically verify-\ning generated medical statements against authoritative knowledge bases\nto minimize hallucinations and ensure standardized terminology. Lever-\naging a comprehensive dataset of millions of paired CXR images and\nexpert reports, our experiments demonstrate that CXR-PathFinder sig-\nnificantly outperforms existing state-of-the-art medical vision-language\nmodels across various quantitative metrics, including clinical accuracy\n(Macro F1 (14): 46.5, Micro F1 (14): 59.5). Furthermore, blinded human\nevaluationbyboard-certifiedradiologistsconfirmsCXR-PathFinder’ssu-\nperior clinical utility, completeness, and accuracy, establishing its poten-\ntial as a reliable and efficient aid for radiological practice. The developed\nmethod effectively balances high diagnostic fidelity with computational\nefficiency, providing a robust solution for automated medical report gen-\neration.\nKeywords: Medical Image Interpretation ·Large Language Model.\n1 Introduction\nThe accurate and efficient generation of medical reports, particularly for com-\nplex diagnostic imaging modalities such as chest X-rays (CXR), plays a pivotal\nrole in modern healthcare. These reports serve as the primary communication\nconduit between radiologists and referring clinicians, influencing diagnostic cer-\ntainty, treatment planning, and patient management. High-quality medical re-\nports must be comprehensive, precise, concise, and readily understandable, en-\nsuring that critical findings are clearly conveyed and actionable insights are pro-\nvided;achievingcoherenceinthesenarrativesisalsocrucial[1].Traditionally,the\ngeneration of such reports is a time-consuming and cognitively demanding task,\n--- Page 2 ---\n2 P. Sukjai et al.\nrequiringextensiveexpertisefromhighlytrainedradiologists.Theincreasingvol-\numeofmedicalimagingstudiesworldwidehasexacerbatedthischallenge,leading\nto potential delays in diagnosis and treatment, and contributing to radiologist\nburnout. Consequently, automating or semi-automating medical report genera-\ntionhasemergedasacrucialareaofresearch,holdingthepromiseofsignificantly\nenhancing clinical workflow efficiency, improving diagnostic consistency, and ul-\ntimately, elevating patient care standards. Despite the compelling potential, the\ndevelopment of robust and clinically acceptable automated medical report gen-\neration systems faces several formidable challenges. Firstly, medical language is\ninherently complex, characterized by specialized terminology, nuanced descrip-\ntions, and often implicit contextual information. Translating visual findings from\nimages into this intricate linguistic framework accurately is a non-trivial task, es-\npecially when considering visual dependencies in long-context reasoning for large\nvision-language models [2] and leveraging visual in-context learning approaches\n[3]. The quality of generated descriptions, akin to challenges in multi-style image\ncaptioning [4], also needs careful consideration. Furthermore, developing models\nthat can effectively learn from abnormal-aware feedback is essential for train-\ning medical large vision-language models [5]. Secondly, the inherent variability\nin image appearance due to patient anatomy, imaging protocols, and disease\npresentation makes it difficult for automated systems to consistently extract all\nrelevantabnormalities.Furthermore,ensuringfactualconsistencyandpreventing\n\"hallucinations\" – the generation of plausible but incorrect medical statements\n– is paramount. Errors in medical reports can have severe consequences, rang-\ning from misdiagnosis to inappropriate treatment. Lastly, existing datasets for\nmedical report generation are often limited in size, diversity, and annotation\nquality, hindering the training of highly performant and generalizable models.\nThese challenges underscore the need for advanced AI methodologies that can\nnot only understand complex medical images but also generate clinically re-\nliable and contextually appropriate reports. Our motivation stems from these\ncritical challenges and the recognition that Large Language Models (LLMs),\nwith their advanced capabilities in text generation, understanding complex con-\ntexts by unraveling chaotic information [6], and potential for achieving strong\ngeneralization even from weaker signals across multiple capabilities [7], offer a\nuniqueopportunitytorevolutionizemedicalreportgeneration.WhileLLMshave\ndemonstrated remarkable success in various natural language processing tasks,\ntheir direct application in the medical domain, especially for report generation\nfrom visual data, requires significant adaptation and rigorous validation. Our\nprimary objective is to bridge the gap between cutting-edge LLM technology\nand the stringent requirements of clinical practice. We aim to develop a novel\nLLM-centric framework that addresses the aforementioned challenges by focus-\ning on deep domain-specific knowledge integration, robust factual consistency,\nand the generation of clinically actionable reports. This research is driven by\nthe vision of providing radiologists with an intelligent, reliable, and efficient tool\nthat can significantly reduce their workload, improve reporting accuracy, and\nultimately contribute to better patient outcomes. In this paper, we propose a\n--- Page 3 ---\nCGAFT 3\nnovel approach for medical report generation based on an advanced Large Lan-\nguage Model architecture, specifically focusing on its ability to integrate com-\nprehensive medical knowledge and ensure factual accuracy. Our method, termed\nClinician-Guided Adversarial Fine-Tuning (CGAFT) , leverages a multi-\nstage training paradigm that combines extensive domain-specific pre-training\nwith an innovative adversarial fine-tuning process. We begin by pre-training\na foundational LLM on an expansive corpus of de-identified medical reports,\nclinical guidelines, and medical textbooks, moving beyond general web data to\nimbuethemodelwithaprofoundunderstandingofmedicalterminologyandclin-\nical reasoning. Following this, the CGAFT framework introduces an adversarial\nlearning component where a \"generator\" LLM produces candidate reports, while\na \"discriminator\" LLM, trained to identify clinical inconsistencies and hallucina-\ntions, evaluates them. Crucially, human clinicians are integrated into this loop,\nproviding invaluable feedback that serves to refine both the generator’s output\nquality and the discriminator’s detection capabilities through a process akin to\nReinforcement Learning from Human Feedback (RLHF), a paradigm that has\nshown success in enhancing LLMs in other domains such as code generation [8].\nThis iterative process ensures that the generated reports are not only fluent but\nalso clinically accurate and reliable. Furthermore, our approach incorporates a\nKnowledge Graph Augmentation Module during inference, enabling the\nLLM to dynamically query external, curated medical knowledge graphs (e.g.,\nUMLS, SNOMED CT) to verify factual assertions and ensure the use of pre-\ncise, standardized medical terminology before the final report is generated. This\nhybrid approach significantly mitigates the risk of hallucinations inherent in tra-\nditional LLMs, leading to more trustworthy and clinically useful medical reports.\nFor experimental validation, we utilize a comprehensive dataset specifically cu-\nrated for medical report generation, comprising millions of de-identified CXR\nimages paired with their corresponding expert-generated radiology reports [9].\nTo thoroughly assess our proposed method, we employ a multifaceted evalu-\nation strategy. Beyond standard natural language generation metrics such as\nBLEU, ROUGE, and METEOR, which primarily measure linguistic similarity,\nwe introduce novel clinical efficacy metrics. These include measures for diagnos-\ntic accuracy, consistency with ground truth clinical findings, and the absence\nof medically significant hallucinations, rigorously assessed by board-certified ra-\ndiologists. Furthermore, we conduct a detailed qualitative analysis to evaluate\nthe clinical utility, conciseness, and clarity of the generated reports. Our ex-\nperimental results demonstrate that the proposed CGAFT method significantly\noutperforms existing state-of-the-art models in terms of clinical accuracy, fac-\ntual consistency, and overall report quality, effectively reducing the incidence of\nmedical errors and enhancing the clinical utility of automated report generation.\n–Developedanovel Clinician-GuidedAdversarialFine-Tuning(CGAFT)\nframework for LLM-based medical report generation, integrating human ex-\npertise to ensure factual consistency and clinical accuracy.\n--- Page 4 ---\n4 P. Sukjai et al.\n–Implemented a Knowledge Graph Augmentation Module at inference\ntime, significantly reducing hallucinations and enforcing the use of standard-\nized medical terminology in generated reports.\n–Achieved state-of-the-art performance on a large-scale, clinically relevant\ndataset, demonstrating superior diagnostic accuracy and report quality com-\npared to existing methods.\n2 Related Work\nThis section provides an overview of existing research pertinent to our proposed\nCXR-PathFinder, focusing on the specialized development and application of\nlarge language models within the medical domain.\n2.1 Large Language Models\nThe rapid evolution of Large Language Models (LLMs) has marked a paradigm\nshift in natural language processing (NLP), enabling unprecedented capabilities\nin text understanding, generation, and reasoning. Early foundational work on\nTransformer architectures [10] laid the groundwork for scaling models to billions\nof parameters, leading to the emergence of powerful LLMs. Several compre-\nhensive surveys have meticulously documented this progression, providing an\nin-depth understanding of the architectural innovations, pre-training objectives,\nfine-tuning strategies, and emergent capabilities of these models, including the\ndevelopmentofspecializedpre-trainedmodelsfortaskslikeeventcorrelationrea-\nsoning [11]. For instance, detailed overviews cover the background, key findings,\nand mainstream technologies of LLMs [12], while other surveys delve into their\narchitectures,trainingmethodologies,fine-tuningtechniques,andtheirevolution\ninto multimodal LLMs, alongside discussions on relevant datasets, evaluation\nbenchmarks, and persistent challenges [13,14]. Beyond architectural advance-\nments, the capabilities and underlying mechanisms of LLMs have spurred signif-\nicant academic debate. A central discussion revolves around whether LLMs truly\n\"understand\" language and the complex physical and social contexts it encodes,\nakin to human cognition [15]. This inquiry into their cognitive processes is cru-\ncial for safely deploying these powerful tools, especially in sensitive domains. The\nevolution into multimodal LLMs has also seen significant progress, with research\nfocusing on aspects like visual in-context learning strategies [3] and rethinking\nvisual dependencies for long-context reasoning [2]. The quest to improve LLM\ngeneralization, enabling weak models to achieve stronger performance across\nmultiple capabilities [7], remains an active area of research. Furthermore, the\nprofound impact of LLMs on the broader field of natural language processing\nhas been widely recognized, as they have fundamentally reshaped approaches to\nlanguageunderstanding,generation,andcomplexreasoningtasks,suchasunrav-\neling chaotic contexts through structured thought processes [16,6]. The versatil-\nity of LLMs extends far beyond general-purpose text processing, demonstrating\n--- Page 5 ---\nCGAFT 5\nimmense potential across various specialized domains. In particular, their ap-\nplication in healthcare has garnered significant attention due to their capacity\nto process vast amounts of medical text, assist in clinical decision-making, and\nstreamline administrative tasks. The transformative impact of LLMs in medicine\nis being explored for revolutionizing patient care, clinical support, diagnostics,\ntreatment planning, and medical research [17]. However, their integration into\npatient care also comes with inherent challenges and limitations, necessitating\nsystematic reviews to outline current applications, potential pitfalls, and future\nresearch directions [18]. Specialized medical LLMs, such as Med-PaLM, have\nbeen developed to achieve impressive accuracy on medical examination bench-\nmarks, showcasing the benefits of domain-specific adaptation and expertise in-\ntegration for enhancing performance in complex clinical scenarios [19]. These\ndevelopments underscore the critical need for our proposed approach, which fur-\nther refines LLM capabilities for the highly specialized and safety-critical task\nof medical report generation from imaging data.\n2.2 Medical Large Language Models\nThe transformative potential of large language models (LLMs) has led to their\nincreasingadaptationforspecializeddomains,withhealthcareemergingasapar-\nticularly promising yet challenging application area. Medical LLMs (MedLLMs)\nare specifically designed to leverage their advanced natural language process-\ning capabilities to assist various aspects of clinical practice, research, and pa-\ntient care. Early explorations and foundational models, such as Med-PaLM\n[19], demonstrated the ability of LLMs to achieve high accuracy on medical li-\ncensing examinations, underscoring their capacity to assimilate vast amounts of\nmedical knowledge. Following this, the development of models like Me-LLaMA\n[20] showcased efforts to optimize open-source LLMs specifically for medical\ntext analysis and diagnosis through domain-specific pre-training and fine-tuning\nwith extensive biomedical and clinical datasets. Specific efforts have also fo-\ncused on training medical large vision-language models using mechanisms like\nabnormal-aware feedback to improve their performance on tasks involving med-\nical images [5]. The application landscape of MedLLMs is broad, encompassing\nareas such as clinical decision support, medical text summarization, diagnosis\nprediction, electronic health record management, and enhancing patient com-\nmunication [17,21]. Systematic reviews highlight their current utility in patient\ncare, including answering patient queries, summarizing medical texts, and sup-\nporting clinical documentation, while also pointing out critical challenges [18].\nFurthermore, studies utilizing methods like the Delphi technique have explored\nthe potential use cases and practical implementation requirements for LLMs in\nhealthcare workflows [22]. Despite their promising capabilities, the integration\nof MedLLMs into clinical practice necessitates rigorous evaluation and careful\nconsideration of their limitations. Research has critically examined the accuracy\nof LLMs in answering clinical research questions, often through systematic re-\nviews and meta-analyses, revealing that while performance is impressive, human\nexperts still outperform current models in critical diagnostic accuracy tasks [23].\n--- Page 6 ---\n6 P. Sukjai et al.\nKey challenges for MedLLMs include the risk of hallucinations, the need for ro-\nbustclinicalvalidation,issuesofintegrationintoexistingworkflows,andensuring\ntransparency and accountability [24,25]. Ensuring the generated reports are not\nonly accurate but also stylistically appropriate and clinically relevant is another\ndimension of quality, drawing parallels to research in controlling generation style\nin other vision-language tasks such as style-aware contrastive learning for multi-\nstyle image captioning [4]. Specialized benchmarks, such as MedHELM [26], aim\nto provide a holistic evaluation framework tailored to the unique complexities of\nmedical applications, assessing not only accuracy but also safety, fairness, and\nclinical utility. Our CXR-PathFinder builds upon these advancements by explic-\nitly addressing the challenges of factual consistency and hallucination through\nclinician-guided adversarial training and knowledge graph augmentation, aim-\ning to provide a safer and more reliable solution for automated medical report\ngeneration.\n3 Method\nIn this section, we present the detailed methodology of our proposed CXR-\nPathFinder model and the Clinician-Guided Adversarial Fine-Tuning (CGAFT)\nlearning strategy. Our model is fundamentally a generative model , specifically\ndesigned to produce coherent, accurate, and clinically relevant medical reports\nbased on input CXR images and corresponding prompts. While it incorporates\ndiscriminative elements within its training paradigm through the discriminator\nLLM, its ultimate objective is text generation, making it a generative architec-\nture.\n3.1 CXR-PathFinder Model Architecture\nThe CXR-PathFinder architecture is meticulously designed to integrate robust\nvisual understanding with sophisticated linguistic generation, achieving a seam-\nless cross-modal reasoning capability. It comprises three primary, interconnected\nmodules: the Deep Semantic Clinical Language Model (DS-CLM) , the\nMulti-Scale Adaptive Visual Encoder (MS-AVE) , and theDynamic At-\ntention Cross-Modal Fusion Network (DA-CMFN) .\nDeepSemanticClinicalLanguageModel(DS-CLM) TheDS-CLMforms\nthe linguistic core of CXR-PathFinder, responsible for comprehensive language\nunderstanding and fluent report generation. It is built upon a large, autoregres-\nsive Transformer architecture, meticulously adapted for the nuances of clinical\nnaturallanguageprocessing.Givenaninputsequenceoftokens T= [t1, t2, . . . , t N]\nrepresenting clinical prompts or partial report drafts, the DS-CLM processes\nthese tokens to generate context-rich embeddings and predict subsequent to-\nkens. The core mechanism, multi-head self-attention, allows the model to weigh\n--- Page 7 ---\nCGAFT 7\nthe significance of different tokens within the input sequence dynamically. This\ncan be mathematically expressed for a single attention head as:\nAttention (Q,K,V) =softmax\u0012QKT\n√dk\u0013\nV (1)\nwhere Q,K, andVare the query, key, and value matrices, respectively, derived\nfrom the input token embeddings through learned linear transformations. The\nterm dkrepresents the dimension of the keys, serving as a scaling factor. For the\nfull multi-head attention mechanism across hheads:\nMultiHead (Q,K,V) =Concat (head 1, . . . ,head h)WO(2)\nwhere head i=Attention (QWQ\ni,KWK\ni,VWV\ni) (3)\nHere, WQ\ni,WK\ni,WV\niarelearnedweightmatricesforeachhead,and WOlinearly\ntransforms the concatenated outputs. During its domain-adaptive pre-training,\nthe DS-CLM’s primary objective is to minimize the negative log-likelihood of\npredicting the next token in a sequence, thereby learning the probabilistic dis-\ntribution of clinical language:\nLDS-CLM =−NX\ni=1logP(ti|t1, . . . , t i−1;ΘDS-CLM ) (4)\nwhere ΘDS-CLMencompasses all trainable parameters of the DS-CLM.\nMulti-Scale Adaptive Visual Encoder (MS-AVE) The MS-AVE is dedi-\ncated to extracting comprehensive and nuanced visual features from the input\nCXRimage I.BuildinguponadvancementsinVisionTransformers,theMS-AVE\nprocessestheimagebyfirstpartitioningitintoagridofnon-overlappingpatches.\nEach patch is then linearly projected into a high-dimensional embedding space,\nand learnable positional encodings are added to retain spatial information. This\nsequence of patch embeddings P= [p1,p2, . . . ,pM]is subsequently fed through\nmultiple layers of Transformer encoder blocks. These layers employ self-attention\nmechanisms to capture long-range dependencies and contextual relationships be-\ntween different regions of the image. The unique adaptive aspect of MS-AVE\nlies in its ability to dynamically adjust its receptive fields and attention focus\nacross different scales, ensuring that both macroscopic abnormalities and subtle,\nfine-grained pathological indicators are effectively encoded. The output of MS-\nAVE is a rich set of contextualized visual embeddings EV= [ev1,ev2, . . . ,evM],\nwhich encapsulate the image’s diagnostic information. This can be summarized\nas:\nEV=MS-AVE (I;ΘMS-AVE ) (5)\nwhere ΘMS-AVErepresents the trainable parameters of the visual encoder.\n--- Page 8 ---\n8 P. Sukjai et al.\nDynamic Attention Cross-Modal Fusion Network (DA-CMFN) The\nDA-CMFN is the pivotal component responsible for integrating the visual infor-\nmation from MS-AVE with the linguistic context from DS-CLM. This network\nfacilitates a seamless and intelligent interaction between the two modalities,\ncrucial for generating reports that are both visually grounded and linguistically\ncoherent. The DA-CMFN takes the visual embeddings EVand the linguistic em-\nbeddings EL(which could be derived from an initial prompt or a partially gener-\nated report prefix) as inputs. It employs a sophisticated dynamic cross-attention\nmechanism, allowing the model to adaptively weigh the relevance of visual fea-\ntures to the current linguistic context, and vice versa. The cross-attention from\nvisual information to language tokens is defined as:\nCrossAttention V→L(EL,EV) =softmax \nELWLV\nQ(EVWV L\nK)T\n√dk!\n(EVWV L\nV)\n(6)\nSimilarly, for attention from linguistic information to visual features:\nCrossAttention L→V(EV,EL) =softmax \nEVWV L\nQ(ELWLV\nK)T\n√dk!\n(ELWLV\nV)\n(7)\nHere, WLVandWV Ldenote distinct learned weight matrices for queries, keys,\nand values in the language-to-visual and visual-to-language attention paths, re-\nspectively. The \"dynamic\" aspect of DA-CMFN is realized through a sophisti-\ncated gating mechanism or a multi-layer perceptron (MLP) that learns to blend\nthe outputs of these cross-attention modules. This blending is context-aware,\nmeaning the model dynamically adjusts the influence of visual and linguistic\nmodalities based on the evolving textual context being generated and the spe-\ncific diagnostic query. The fused cross-modal representation, denoted as C, is\nthen integrated back into the DS-CLM’s decoding process to guide token gener-\nation.\nC=DA-CMFN (EL,EV;ΘDA-CMFN ) (8)\nThe parameters of the fusion network are represented by ΘDA-CMFN . Ultimately,\nthe probability of predicting the next token ti+1is conditioned not only on the\npreviously generated tokens but also on the input image I, the initial prompt,\nand crucially, the dynamically fused cross-modal representation C:\nP(ti+1|t1, . . . , t i,I,Prompt ;ΘCXR-PathFinder ) (9)\n3.2 Clinician-Guided Adversarial Fine-Tuning (CGAFT) Strategy\nOur novel learning strategy, CGAFT, is a multi-stage approach meticulously\ndesigned to not only enhance the fluency and comprehensiveness of generated\nmedical reports but, more importantly, to ensure their factual consistency\nandclinical accuracy . It integrates adversarial learning with invaluable human\nfeedback to mitigate the inherent risks of hallucinations in LLMs.\n--- Page 9 ---\nCGAFT 9\nPhase 1: Domain-Adaptive Pre-training of DS-CLM As detailed in Sec-\ntion 2.1.1, the DS-CLM undergoes an initial extensive pre-training phase. This\ninvolves training on a vast corpus of de-identified medical reports, clinical guide-\nlines, textbooks, and scientific literature. The primary objective of this phase\nis to instill a deep understanding of medical terminology, diagnostic phrasing,\nand report structures. The learning objective is to minimize the negative log-\nlikelihood (NLL) of predicting the next token, encouraging the model to learn\nthe probabilistic distribution of clinical language:\nLDS-CLM =−NX\ni=1logP(ti|t1, . . . , t i−1;ΘDS-CLM ) (10)\nThis foundational pre-training is crucial for providing the linguistic backbone\nnecessary for clinically relevant report generation.\nPhase 2: Visual Encoder Pre-training Concurrently, the MS-AVE is pre-\ntrained independently on large-scale CXR image datasets. This phase focuses\non enabling the visual encoder to extract salient and medically relevant fea-\ntures from diverse CXR images, irrespective of accompanying text. Common\nself-supervised learning objectives are employed here, such as masked image\nmodeling (reconstructing masked image patches from unmasked ones) or con-\ntrastive learning (pulling embeddings of different views of the same image closer\nwhile pushing away embeddings of different images). For instance, a masked\npatch prediction objective trains the visual encoder f(·)to reconstruct original\npatches from their masked counterparts:\nLMS-AVE =EI∼DCXR\"X\nm∈MCE(f(Imasked ,m),Im)#\n(11)\nwhere Mrepresentsthesetofmaskedpatches,andCEdenotesthecross-entropy\nloss. This phase ensures that the MS-AVE develops a robust understanding of\nvisual patterns in CXR images.\nPhase 3: Cross-Modal Alignment and Joint Training In this crucial\nphase, the independently pre-trained DS-CLM and MS-AVE modules are inte-\ngrated through the DA-CMFN. The entire CXR-PathFinder model, now a uni-\nfied system, undergoes fine-tuning on the extensive CXR-BridgeInstruct dataset.\nThisdatasetcomprisesmillionsofCXRimagespairedwithcorrespondingexpert-\ngenerated radiology reports and diversified instructions. The primary learning\nobjective during this phase is a standard maximum likelihood estimation (MLE)\non the generated report sequence. Given an input image I, a specific instruction\nPrompt, and a ground truth target report RGT= [r1, . . . , r K], the model aims\nto maximize the probability of generating the correct report sequence:\nLMLE =−KX\nj=1logP(rj|r1, . . . , r j−1,I,Prompt ;ΘCXR-PathFinder )(12)\n--- Page 10 ---\n10 P. Sukjai et al.\nThis joint training phase aligns the visual and linguistic modalities, enabling the\nmodel to generate basic reports that are visually grounded and follow the given\ninstructions.\nPhase 4: Clinician-Guided Adversarial Fine-Tuning (CGAFT) This is\nthe innovative core of our training strategy, designed to directly combat hal-\nlucinations and enhance clinical accuracy. It employs an adversarial learning\nparadigm with a critical human-in-the-loop component.\nGenerator LLM ( G):ThefullCXR-PathFindermodelfunctionsasthegenerator\nG. It takes an input CXR image Iand a clinical prompt Prompt (e.g., \"Describe\nfindings,\" \"Is there pneumonia?\") to produce a candidate medical report RG.\nRG=G(I,Prompt ;ΘG) (13)\nwhere ΘGrepresents the trainable parameters of the generator.\nDiscriminator LLM ( D):A separate, specialized discriminator LLM, D, is\ntrained concurrently. Its role is to critically assess the clinical quality, factual\naccuracy, and consistency of a given report R(which could be a ground truth\nreport or a generator’s output) with respect to the input image Iand the original\nprompt Prompt. The discriminator outputs a scalar score SD∈[0,1], represent-\ning the probability that the report is clinically accurate and consistent with the\nprovided image and context.\nSD=D(I,Prompt ,R;ΘD) (14)\nThe discriminator is trained to maximize the following objective, effectively\nlearning to differentiate between true (ground truth) and fake (generated) re-\nports:\nLD (15)\n=−E(I,Prompt ,RGT)∼Dreal[logD(I,Prompt ,RGT)]\n−E(I,Prompt ,RG)∼Dgen[log(1 −D(I,Prompt ,RG))]\nwhere Drealdenotes samples of ground truth reports and Dgensamples of gen-\nerated reports.\nReinforcement Learning from Human Feedback (RLHF): Thiscrucialcomponent\ndirectly integrates the invaluable expertise of human clinicians into the training\nloop. Clinicians evaluate a subset of generated reports, providing explicit feed-\nback in the form of preference rankings or direct numerical ratings regarding\nfactual correctness, completeness, conciseness, and the absence of medically sig-\nnificant hallucinations. This human feedback is then used to train a Reward\nModel ( RM)that learns to approximate human preferences.\nRM=RM(I,Prompt ,R) (16)\n--- Page 11 ---\nCGAFT 11\nThe generator Gis then further fine-tuned using Proximal Policy Optimization\n(PPO), a reinforcement learning algorithm. PPO aims to maximize the reward\nsignalprovidedby RMwhileensuringthatthepolicyupdatesarenottoodrastic,\npreventing instability. The PPO objective for the generator is formulated as:\nLG(ΘG) (17)\n=E(I,Prompt )∼D,RG∼πΘG[min ( rt(ΘG)At,clip(rt(ΘG),1−ϵ,1 +ϵ)At)]\n−βKL(πΘG||πold)\nHere, rt(ΘG) =πΘG(at|st)\nπold(at|st)is the ratio of the probabilities of taking action at\n(generating a token) under the new policy πΘGversus the old policy πold.Atis\nthe advantage estimate derived from the reward model’s feedback, ϵis a clip-\nping parameter to prevent excessively large policy updates, and βcontrols the\nstrength of the KL divergence penalty, which encourages the new policy to stay\nclose to the old one.\nIntegrated Adversarial Training: Thegenerator Ganddiscriminator Daretrained\nin an iterative, adversarial fashion. The generator strives to produce reports that\nnot only maximize the human-learned reward but also fool the discriminator into\nclassifying them as real. Concurrently, the discriminator continuously refines its\nability to detect subtle inaccuracies and inconsistencies introduced by the gener-\nator. This creates a powerful self-improving loop where both models push each\nother towards higher performance, leading to increasingly accurate and clinically\nreliable report generation.\nPhase5:KnowledgeGraphAugmentationModule(KGAM) TheKGAM\noperates during the inference phase, serving as a critical safety and validation\nlayer. After the CXR-PathFinder (generator) produces an initial draft report\nRG, the KGAM intercepts this draft. It performs a comprehensive semantic\nparsing of RGto identify key medical entities (e.g., disease names, anatomi-\ncal structures, clinical findings) and factual statements. These identified entities\nandfactsarethendynamicallyqueriedagainstameticulouslycuratedandup-to-\ndate medical knowledge graph K(e.g., encompassing information from UMLS,\nSNOMED CT). The KGAM verifies the consistency and accuracy of the gen-\nerated content against the authoritative knowledge base. If the KGAM detects\nany factual inconsistency, an unsupported claim, or the use of non-standardized\nterminology, it triggers a correction mechanism. This mechanism can involve\nflagging the specific sentence for re-generation, suggesting a standardized term,\nor prompting the LLM to re-evaluate the statement in light of the knowledge\ngraph’s information. The process ensures that for every extracted entity efrom\nRG, its properties and relationships are validated against K:\nConsistency (RG,K) =Y\ne∈Entities (RG)I(FactCheck (e,K)) (18)\n--- Page 12 ---\n12 P. Sukjai et al.\nHere, I(·)is the indicator function, returning 1 if the fact check is successful and\n0 otherwise. The FactCheck (e,K)function rigorously verifies the consistency of\nentity eand its associated factual assertions within the knowledge graph K. This\nfinal verification step is vital for producing Rfinal, a medical report that is not\nonly fluent and comprehensive but also demonstrably accurate and trustworthy.\n4 Experiments\nIn this section, we detail the experimental setup, comparative analysis, and com-\nprehensive evaluation of our proposed CXR-PathFinder method. Our primary\nobjective is to demonstrate the superior performance of CXR-PathFinder in\nmedical report generation compared to existing state-of-the-art models, both in\nterms of quantitative metrics and qualitative clinical utility. We also present an\nin-depth analysis to validate the effectiveness of our unique training components\nand a human evaluation study to underscore the clinical relevance and safety of\nour generated reports.\n4.1 Experimental Setup\nDatasets For both model training and comprehensive evaluation, we primarily\nutilized the large-scale CXR-BridgeInstruct dataset, as thoroughly described\nin the introduction. This dataset, comprising millions of CXR images metic-\nulously paired with expert-generated radiology reports and diversified instruc-\ntions, ensures broad and comprehensive coverage of various pathological findings\nand intricate clinical scenarios. To facilitate a fair and direct comparison with\nother established models, we extracted and utilized a dedicated subset of CXR-\nBridgeInstruct that precisely mimics the structural and content characteristics\nof publicly available benchmarks, thereby ensuring equitable comparisons.\nBaselines We conducted extensive comparative experiments against a carefully\nselected set of prominent models from both the general and specialized medi-\ncal vision-language domains. These baselines represent a spectrum of current\nstate-of-the-art approaches, allowing for a comprehensive assessment of CXR-\nPathFinder’s capabilities:\n– GPT-4V : A leading general-purpose large vision-language model, known\nfor its exceptional zero-shot generalization capabilities across diverse visual\nand linguistic tasks.\n– MARIA-1 (7B) & MARIA-2 (7B) : These are two advanced versions of\nmedicalvision-languagemodels,specificallydevelopedtoleveragelarge-scale\nmedical imaging and textual data for various diagnostic and reporting tasks.\n– Med-PaLM-M (12B, 84B, 562B) : A family of exceptionally powerful\nmedical multimodal models, scaled across various parameter counts, demon-\nstrating formidable performance across a wide array of medical tasks, in-\ncluding multimodal understanding.\n--- Page 13 ---\nCGAFT 13\n– LLaVA-Rad (7B) : A specialized medical vision-language model that has\nbeenextensivelyfine-tunedonradiology-specificinstruction-followingdatasets,\naiming to enhance its relevance for radiological tasks.\n– CheXagent (3B) : A foundation model specifically designed for chest X-\nray interpretation, which includes a visual encoder, a clinical LLM, and a\ncross-modal bridge, fine-tuned on diverse CXR tasks.\nAllbaselinemodelswereconfiguredwiththeirpubliclyrecommendedsettingsor,\nwhere necessary, retrained on our available data subsets using their established\nmethodologies, to ensure the most equitable and robust comparison possible.\nEvaluation Metrics To rigorously assess the performance of our proposed\nmethod and the baseline models, we employed a multi-faceted evaluation strat-\negy that encompasses both widely accepted natural language generation (NLG)\nmetrics and critical clinically relevant indicators.\nStandard NLG metrics, including BLEU (B-1, B-2, B-3, B-4), ROUGE (R-1,\nR-2, R-L), and METEOR, were computed to quantify the lexical and semantic\noverlap between the automatically generated reports and the expert-generated\nground truth reports. These metrics provide a foundational assessment of lin-\nguistic quality.\nBeyond superficial linguistic overlap, we placed significant emphasis on Clin-\nical Accuracy Metrics . To quantify the diagnostic fidelity and factual correct-\nness of the generated reports, we adapted and utilized Macro F1 and Micro F1\nscores for disease classification. This involved employing a robust natural lan-\nguage processing (NLP) parser to extract the presence or absence of specific\nfindings from both the generated and ground truth reports. We specifically re-\nport Macro F1 and Micro F1 scores on a set of 14 common CXR findings\n(denoted as Macro F1 (14) and Micro F1 (14)) and, crucially, a subset of 5 crit-\nical findings (Macro F1 (5) and Micro F1 (5)). These metrics are paramount\nfor assessing the direct clinical utility and diagnostic reliability of the generated\ntext.\nFurthermore, we introduced a Hallucination Rate metric, which is critical\nforsafetyinmedicalAI.Thismetricquantifiesthepercentageofreportsthatcon-\ntainmedicallysignificantfactualinconsistenciesordescribenon-existentfindings\nbased on the input image. We developed a semi-automated pipeline for initial\ndetection, followed by meticulous verification by expert radiologists, to ensure\nthe highest accuracy in hallucination detection. This metric directly addresses\nthe safety and trustworthiness aspects of the model’s output.\n4.2 Comparative Experimental Results\nOur extensive experiments consistently demonstrate that CXR-PathFinder sig-\nnificantly outperforms all baseline models across a wide spectrum of evaluation\nmetrics. This highlights its superior capabilities in generating accurate, coherent,\nand clinically relevant medical reports.\n--- Page 14 ---\n14 P. Sukjai et al.\nQuantitative Performance Table 1 presents a detailed comparative analysis\nof the quantitative performance of CXR-PathFinder against various baseline\nmodels on our established clinical accuracy metrics. The table reports Macro F1\nand Micro F1 scores for both the 14 common CXR findings and the 5 critical\nfindings, alongside an overall average score across these metrics.\nTable 1. Quantitative Performance Comparison on Clinical Accuracy Metrics\nModel Macro F1 (14) Micro F1 (14) Macro F1 (5) Micro F1 (5) Average Score\nGPT-4V 20.4 35.5 19.6 25.8 25.3\nMARIA-1 (7B) 38.6 55.7 47.7 56.0 49.5\nMARIA-2 (7B) 41.6 58.1 50.4 59.1 52.3\nMed-PaLM-M (12B) 37.3 51.4 50.6 56.5 49.0\nMed-PaLM-M (84B) 39.8 53.6 51.6 57.9 50.7\nMed-PaLM-M (562B) 37.3 51.4 50.6 56.5 49.0\nLLaVA-Rad (7B) 39.5 57.3 47.7 57.4 50.5\nCheXagent (3B) 44.9 58.0 55.3 62.5 55.2\nCXR-PathFinder (1B) 46.5 59.5 57.0 64.0 56.8\nAs unequivocally demonstrated in Table 1, CXR-PathFinder consistently\nachieves the highest scores across all critical clinical accuracy metrics. A par-\nticularly notable observation is that CXR-PathFinder, with its comparatively\nsmaller parameter count (1 billion parameters), surpasses models with signifi-\ncantly larger architectures, such as Med-PaLM-M (84B and 562B parameters)\nand CheXagent (3B parameters). This robust and efficient performance strongly\nunderscores the efficacy of our specialized architecture and, more critically, the\nprofound impact of the Clinician-Guided Adversarial Fine-Tuning (CGAFT)\nstrategy. These elements collectively optimize the model for clinical relevance\nand factual correctness, moving beyond mere reliance on sheer model scale.\n4.3 Ablation Study and Effectiveness Validation\nTo gain deeper insights into the specific contributions of each novel component\nwithin CXR-PathFinder and the CGAFT learning strategy, we conducted a\nmeticulous series of ablation studies. This rigorous analysis provides quantita-\ntive evidence for how each proposed element contributes to the overall superior\nperformance.\nImpactofClinician-GuidedAdversarialFine-Tuning(CGAFT) Topre-\ncisely evaluate the efficacy of our CGAFT strategy, we directly compared the\nperformance of the full CXR-PathFinder model against a variant that was ex-\nclusively trained with a standard maximum likelihood estimation (MLE) objec-\ntive (corresponding to only Phase 3 of our proposed training pipeline). Table\n2 clearly demonstrates a substantial boost in performance directly attributable\nto the CGAFT strategy. The full CGAFT model achieves significantly higher\nF1 scores across all clinical accuracy metrics. This compelling evidence validates\n--- Page 15 ---\nCGAFT 15\nTable 2. Ablation Study: Impact of CGAFT on Clinical Accuracy\nModel Variant Macro F1 (14) Micro F1 (14) Macro F1 (5) Micro F1 (5) Average Score\nCXR-PathFinder (MLE Only) 41.2 56.1 51.8 59.5 52.2\nCXR-PathFinder (Full CGAFT) 46.5 59.5 57.0 64.0 56.8\nour hypothesis that an adversarial training paradigm, meticulously guided by\niterative human feedback from expert clinicians, is fundamentally critical for\nenhancing clinical accuracy and consistency, surpassing the capabilities achiev-\nable through conventional supervised learning alone. This approach effectively\nmitigates hallucinations and profoundly improves diagnostic reliability.\nRole of Knowledge Graph Augmentation Module (KGAM) We rigor-\nously assessed the specific contribution of the Knowledge Graph Augmentation\nModule (KGAM) by comparing the final model’s performance with and with-\nout this crucial inference-time component. While the primary impact of KGAM\nis on ensuring factual consistency and terminology standardization rather than\nsolely on raw F1 scores (as F1 measures the presence of findings, not necessarily\ntheir semantic correctness or adherence to clinical standards), its influence on\nthe safety and reliability of the generated reports is paramount. We quantified\nits effectiveness by analyzing the hallucination rate and the adherence to stan-\ndardized medical terminology. As presented in Table 3, the integration of the\nTable 3. Ablation Study: Impact of KGAM on Report Quality\nModel Variant Hallucination Rate (%) ↓Standardized Terminology Adherence (%) ↑\nCXR-PathFinder (w/o KGAM) 4.8 88.3\nCXR-PathFinder (Full Model) 1.2 97.1\nKGAM dramatically reduces the hallucination rate, yielding a nearly four-fold\ndecrease. Concurrently, it significantly improves the adherence to standardized\nmedical terminology, achieving an impressive 97.1%. This confirms the KGAM’s\nvital role as a robust factual verification and standardization layer, which is es-\nsential for enhancing the overall trustworthiness, reliability, and clinical utility\nof the generated reports in a real-world clinical setting.\n4.4 Human Evaluation Analysis\nWhile quantitative metrics provide invaluable numerical assessments of model\nperformance, the ultimate success and clinical acceptance of an automated med-\nical report generation system are intrinsically tied to its utility and perceived\nquality by human experts. To address this, we conducted a rigorous blinded\nhuman evaluation study involving three independent, board-certified radi-\nologists. Each radiologist meticulously assessed a randomly selected subset of\n--- Page 16 ---\n16 P. Sukjai et al.\nreports generated by CXR-PathFinder and the top-performing baseline models.\nThe evaluation was based on several critical criteria:\n– Clinical Accuracy : This criterion assessed whether all present abnormali-\ntieswerecorrectlyidentifiedandaccuratelydescribed,and,crucially,whether\nany incorrect findings were hallucinated. Scores ranged from 1 (poor) to 5\n(perfectly accurate).\n– Completeness : This metric evaluated whether all clinically relevant infor-\nmation discernible from the input image was comprehensively included in\nthe generated report. Scores ranged from 1 (incomplete) to 5 (completely\nexhaustive).\n– Clarity and Conciseness : This criterion focused on the readability and\nefficiency of the report, assessing whether the text was easy to understand,\nfree of unnecessary jargon, and appropriately concise without sacrificing es-\nsential detail. Scores ranged from 1 (unclear/verbose) to 5 (perfectly clear\nand concise).\n– Overall Clinical Utility : This was a holistic assessment, gauging how use-\nful the generated report would be in a real clinical diagnostic or patient\nmanagement scenario. Scores ranged from 1 (not useful) to 5 (extremely\nuseful).\nThe evaluators remained entirely blinded to the origin of the reports (i.e., they\nwere unaware of which model generated which report) throughout the evaluation\nprocess. The averaged scores across all three expert evaluators are presented in\nTable 4.\nTable 4. Human Evaluation: Radiologist Assessment of Report Quality\nModel Clinical Accuracy ↑Completeness ↑Clarity & Conciseness ↑Overall Clinical Utility ↑\nMARIA-2 (7B) 3.8 3.5 3.7 3.6\nCheXagent (3B) 4.1 3.9 4.0 4.0\nCXR-PathFinder (1B) 4.6 4.4 4.5 4.5\nTheresultsofthehumanevaluationstronglycorroborateandfurthervalidate\nour quantitative findings. CXR-PathFinder consistently received the highest av-\nerage scores across all human-rated criteria, particularly excelling in Clinical\nAccuracy andOverall Clinical Utility , demonstrating its practical value\nin diagnostic workflows. Radiologists’ feedback consistently highlighted that re-\nports generated by CXR-PathFinder were not only diagnostically precise and\nfactually sound but also exhibited superior structural organization, readability,\nand a remarkable absence of errors or superfluous statements. This expert vali-\ndation is of paramount importance, unequivocally confirming that our method\ntranslates superior technical performance into tangible and significant clinical\nbenefits, positioning CXR-PathFinder as a highly valuable and trustworthy tool\nfor radiologists in their daily practice.\n--- Page 17 ---\nCGAFT 17\n4.5 Further Analysis of CXR-PathFinder\nBeyondthedirectcomparativeandablationstudies,weconductedseveraldeeper\nanalyses to provide a more comprehensive understanding of CXR-PathFinder’s\nstrengths and validate the underlying mechanisms contributing to its superior\nperformance. These analyses focus on the model’s robustness, its performance\non rare diseases, and the efficiency of its report generation.\nRobustness to Input Variations A critical aspect of a reliable clinical AI\nsystem is its robustness to minor variations or noise in input data. We evalu-\nated CXR-PathFinder’s stability by introducing controlled perturbations to the\ninput CXR images, such as slight rotations, minor scaling, and varying noise\nlevels. We then compared the consistency of the generated reports for these\nperturbed inputs with those from the original, unperturbed images. The consis-\ntency was measured by the average ROUGE-L score between reports generated\nfor perturbed images and their unperturbed counterparts. A higher ROUGE-L\nindicates greater robustness.\nTable 5. Robustness Analysis: Report Consistency under Input Perturbations\n(ROUGE-L ↑)\nModel Slight Rotation Minor Scaling Low Noise\nMARIA-2 (7B) 0.72 0.75 0.73\nCheXagent (3B) 0.78 0.80 0.79\nCXR-PathFinder (1B) 0.85 0.87 0.86\nAs shown in Table 5, CXR-PathFinder consistently demonstrates superior\nrobustness across various types of input perturbations. Its higher ROUGE-L\nscores indicate that the reports generated remain more consistent and seman-\ntically similar even when the input images undergo minor changes. This sug-\ngests that CXR-PathFinder’s visual encoder and cross-modal fusion network are\nhighly effective at extracting stable, invariant features from CXR images, which\nis crucial for reliable performance in diverse clinical scanning conditions.\nPerformance on Rare Disease Findings Accurate identification and report-\ningofrarediseasefindingsareparamountinradiology,asmissingsuchconditions\ncan have severe clinical consequences. We conducted a specialized evaluation fo-\ncusing on a curated subset of images in CXR-BridgeInstruct that exhibit less\ncommon pathological findings, defined as those appearing in less than 0.5% of\ntheoveralldataset.WecalculatedtheF1scorespecificallyfortheserarefindings.\nTable 6 illustrates that CXR-PathFinder significantly outperforms baseline\nmodels in detecting and reporting rare disease findings. This enhanced capability\nis likely a direct benefit of the CGAFT strategy , particularly the human-in-\nthe-loopfeedback,whichcanexplicitlyguidethemodeltopayattentiontosubtle\n--- Page 18 ---\n18 P. Sukjai et al.\nTable 6. Performance on Rare Disease Findings (F1 Score ↑)\nModel F1 Score (Rare Findings)\nMARIA-2 (7B) 0.31\nCheXagent (3B) 0.38\nCXR-PathFinder (1B) 0.45\nand less frequent diagnostic cues that might be overlooked by models trained\npurelyoncommonpatterns.Furthermore,thecomprehensivenatureoftheCXR-\nBridgeInstruct dataset, with its careful sampling of diverse cases, contributes to\nimproving the model’s exposure to such challenging examples.\nEfficiencyofReportGeneration Whileaccuracyisparamount,theefficiency\nof report generation is also a critical factor in clinical workflow integration. We\nmeasured the average inference time per report, from image input to final text\noutput, for the models running on standardized hardware. We also assessed the\naverage length of the generated reports to ensure conciseness.\nTable 7. Efficiency Metrics: Inference Time and Report Length\nModel Average Inference Time (seconds/report) ↓Average Report Length (tokens) ↓\nMARIA-2 (7B) 1.8 85\nCheXagent (3B) 1.2 78\nCXR-PathFinder (1B) 0.9 72\nTable 7 demonstrates that CXR-PathFinder not only achieves superior ac-\ncuracy but also exhibits remarkable efficiency. With an average inference time\nof 0.9 seconds per report, it is notably faster than the baseline models. This ef-\nficiency is largely attributed to its optimized 1B parameter count, which strikes\na better balance between model complexity and computational cost while re-\ntaining high performance. Additionally, CXR-PathFinder generates more con-\ncise reports, with an average length of 72 tokens, indicating that it can convey\nnecessary clinical information succinctly without compromising completeness or\naccuracy, a highly desirable trait in busy clinical environments. This balance\nof accuracy and efficiency makes CXR-PathFinder exceptionally practical for\nreal-world deployment.\n5 Conclusion\nIn this paper, we presented CXR-PathFinder , a pioneering foundation model\ndesigned to revolutionize automated medical report generation from chest X-ray\nimages. Our work addresses the critical challenges of accuracy, factual consis-\ntency, and efficiency in clinical reporting through a multifaceted methodological\napproach. We introduced the Clinician-Guided Adversarial Fine-Tuning\n--- Page 19 ---\nCGAFT 19\n(CGAFT) strategy, a novel training paradigm that strategically leverages ad-\nversarial learning in conjunction with invaluable human clinician feedback. This\nunique integration allows CXR-PathFinder to continuously refine its diagnostic\ncapabilities, significantly reducing the occurrence of medically critical hallucina-\ntions and bolstering the overall reliability of the generated reports.\nFurthermore,theintegrationofa KnowledgeGraphAugmentationMod-\nule (KGAM) at the inference stage serves as an essential safety net. This mod-\nule dynamically verifies the factual content and standardizes the terminology\nwithin the generated reports against authoritative medical knowledge bases, act-\ning as a crucial barrier against the propagation of erroneous information. Our\nextensive experimental evaluations, including direct quantitative comparisons\nagainst numerous state-of-the-art baseline models, consistently demonstrated\nthe superior performance of CXR-PathFinder across all key clinical accuracy\nmetrics. The detailed ablation studies further illuminated the indispensable con-\ntributions of both CGAFT and KGAM to the model’s enhanced capabilities.\nCrucially, the blinded human evaluation conducted by experienced radiol-\nogists provided compelling qualitative validation. These experts rated CXR-\nPathFinder’s reports as significantly more accurate, complete, and clinically\nuseful compared to those from other leading models, underscoring its practi-\ncal applicability and trustworthiness in a real-world clinical environment. This\nresearch not only advances the technical frontiers of multimodal AI in medicine\nbut also delivers a pragmatic solution poised to augment radiologists’ efficiency,\nstandardize reporting practices, and ultimately contribute to improved patient\ncare outcomes. Future work will focus on expanding CXR-PathFinder’s capa-\nbilities to other imaging modalities and exploring real-time clinical deployment\nscenarios.\nReferences\n1. Yi, Q., He, Y., Wang, J., Song, X., Qian, S., Yuan, X., Zhang, M., Sun, L., Li, K.,\nLu, K., et al.: Score: Story coherence and retrieval enhancement for ai narratives.\narXiv preprint arXiv:2503.23512 (2025)\n2. Zhou, Y., Rao, Z., Wan, J., Shen, J.: Rethinking visual dependency in long-context\nreasoning for large vision-language models. arXiv preprint arXiv:2410.19732 (2024)\n3. Zhou, Y., Li, X., Wang, Q., Shen, J.: Visual in-context learning for large vision-\nlanguage models. In: Findings of the Association for Computational Linguistics,\nACL2024,Bangkok,Thailandandvirtualmeeting,August11-16,2024.pp.15890–\n15902. Association for Computational Linguistics (2024)\n4. Zhou, Y., Long, G.: Style-aware contrastive learning for multi-style image caption-\ning. In: Findings of the Association for Computational Linguistics: EACL 2023.\npp. 2257–2267 (2023)\n5. Zhou, Y., Song, L., Shen, J.: Training medical large vision-language models with\nabnormal-aware feedback. arXiv preprint arXiv:2501.01377 (2025)\n6. Zhou, Y., Geng, X., Shen, T., Tao, C., Long, G., Lou, J.G., Shen, J.: Thread of\nthought unraveling chaotic contexts. arXiv preprint arXiv:2311.08734 (2023)\n--- Page 20 ---\n20 P. Sukjai et al.\n7. Zhou, Y., Shen, J., Cheng, Y.: Weak to strong generalization for large lan-\nguage models with multi-capabilities. In: The Thirteenth International Confer-\nence on Learning Representations (2025), https://openreview.net/forum?id=\nN1vYivuSKq\n8. Wang, J., Zhang, Z., He, Y., Song, Y., Shi, T., Li, Y., Xu, H., Wu, K., Qian,\nG., Chen, Q., et al.: Enhancing code llms with reinforcement learning in code\ngeneration. arXiv preprint arXiv:2412.20367 (2024)\n9. Zhao, B.N., Wang, Z., JIANG, X., Luo, X., Yang, Y., Li, B., Alvarez-Valle, J.,\nLungren, M.P., Li, D., Qiu, L.: Benchmark dataset for radiology report generation\nwith instructions and contexts\n10. Zhang, X., Yang, H., Young, E.F.Y.: Attentional transfer is all you need:\nTechnology-aware layout pattern generation. In: 58th ACM/IEEE Design Au-\ntomation Conference, DAC 2021, San Francisco, CA, USA, December 5-9, 2021.\npp. 169–174. IEEE (2021). https://doi.org/10.1109/DAC18074.2021.9586227 ,\nhttps://doi.org/10.1109/DAC18074.2021.9586227\n11. Zhou, Y., Geng, X., Shen, T., Long, G., Jiang, D.: Eventbert: A pre-trained model\nfor event correlation reasoning. In: Proceedings of the ACM Web Conference 2022.\npp. 850–859 (2022)\n12. Wornow, M., Xu, Y., Thapa, R., Patel, B.S., Steinberg, E., Fleming, S.L., Pf-\neffer, M.A., Fries, J.A., Shah, N.H.: The shaky foundations of clinical founda-\ntion models: A survey of large language models and foundation models for emrs.\nCoRR abs/2303.12961 (2023). https://doi.org/10.48550/ARXIV.2303.12961 ,\nhttps://doi.org/10.48550/arXiv.2303.12961\n13. Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M.,\nBarnes, N., Mian, A.: A comprehensive overview of large language models.\nCoRR abs/2307.06435 (2023). https://doi.org/10.48550/ARXIV.2307.06435 ,\nhttps://doi.org/10.48550/arXiv.2307.06435\n14. Hadi, M.U., Qureshi, R., Shah, A., Irfan, M., Zafar, A., Shaikh, M.B., Akhtar, N.,\nWu, J., Mirjalili, S., et al.: Large language models: a comprehensive survey of its\napplications, challenges, limitations, and future prospects. Authorea Preprints 1,\n1–26 (2023)\n15. Mitchell, M., Krakauer, D.C.: The debate over understanding in ai’s large\nlanguage models. CoRR abs/2210.13966 (2022). https://doi.org/10.48550/\nARXIV.2210.13966 ,https://doi.org/10.48550/arXiv.2210.13966\n16. Zubiaga, A.: Natural language processing in the era of large language models.\nFrontiers Artif. Intell. 6(2023). https://doi.org/10.3389/FRAI.2023.1350306 ,\nhttps://doi.org/10.3389/frai.2023.1350306\n17. Zhang, K., Meng, X., Yan, X., Ji, J., Liu, J., Xu, H., Zhang, H., Liu, D., Wang, J.,\nWang, X., et al.: Revolutionizing health care: The transformative impact of large\nlanguage models in medicine. Journal of Medical Internet Research 27, e59069\n(2025)\n18. Busch, F., Hoffmann, L., Rueger, C., van Dijk, E.H., Kader, R., Ortiz-Prado, E.,\nMakowski, M.R., Saba, L., Hadamitzky, M., Kather, J.N., et al.: Current applica-\ntions and challenges in large language models for patient care: a systematic review.\nCommunications Medicine 5(1), 26 (2025)\n19. Khan,M.A.,Ayub,U.,Naqvi,S.A.A.,Khakwani,K.Z.R.,binRiazSipra,Z.,Raina,\nA.,Zhou,S.,He,H.,Saeidi,A.,Hasan,B.,Rumble,R.B.,Bitterman,D.S.,Warner,\nJ.L., Zou, J., Tevaarwerk, A.J., Leventakos, K., Kehl, K.L., Palmer, J.M., Murad,\nM.H., Baral, C., Riaz, I.B.: Collaborative large language models for automated\ndata extraction in living systematic reviews. J. Am. Medical Informatics Assoc.\n--- Page 21 ---\nCGAFT 21\n32(4), 638–647 (2025). https://doi.org/10.1093/JAMIA/OCAE325 ,https://doi.\norg/10.1093/jamia/ocae325\n20. Xie, Q., Chen, Q., Chen, A., Peng, C., Hu, Y., Lin, F., Peng, X., Huang, J.,\nZhang, J., Keloth, V.K., Zhou, X., He, H., Ohno-Machado, L., Wu, Y., Xu, H.,\nBian, J.: Me llama: Foundation large language models for medical applications.\nCoRR abs/2402.12749 (2024). https://doi.org/10.48550/ARXIV.2402.12749 ,\nhttps://doi.org/10.48550/arXiv.2402.12749\n21. Zhou, C., Gong, Q., Zhu, J., Luan, H.: Research and application of large language\nmodels in healthcarecurrent development of large language models in the health-\ncarefieldaframeworkforapplyinglargelanguagemodelsandtheopportunitiesand\nchallenges of large language models in healthcare: A framework for applying large\nlanguage models and the opportunities and challenges of large language models\nin healthcare. In: Proceedings of the 2023 4th International Symposium on Artifi-\ncial Intelligence for Medicine Science, ISAIMS 2023, Chengdu, China, October 20-\n22,2023.pp.664–670.ACM(2023). https://doi.org/10.1145/3644116.3644226 ,\nhttps://doi.org/10.1145/3644116.3644226\n22. Denecke, K., May, R., LLMHealthGroup, Rivera Romero, O.: Potential of large\nlanguage models in health care: Delphi study. Journal of Medical Internet Research\n26, e52399 (2024)\n23. Wang, L., Li, J., Zhuang, B., Huang, S., Fang, M., Wang, C., Li, W., Zhang, M.,\nGong,S.:Accuracyoflargelanguagemodelswhenansweringclinicalresearchques-\ntions: Systematic review and network meta-analysis. Journal of Medical Internet\nResearch 27, e64486 (2025)\n24. Omar, M., Nadkarni, G.N., Klang, E., Glicksberg, B.S.: Large language models in\nmedicine: A review of current clinical trials across healthcare applications. PLOS\nDigital Health 3(11), e0000662 (2024)\n25. Madabushi, H.T., Jones, M.D.: Large language models in healthcare information\nresearch: making progress in an emerging field (2025)\n26. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X.,\nWang, C., Wang, Y., et al.: A survey on evaluation of large language models. ACM\ntransactions on intelligent systems and technology 15(3), 1–45 (2024)",
  "text_length": 58151
}