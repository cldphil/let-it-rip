{
  "id": "http://arxiv.org/abs/2506.05183v1",
  "title": "TreeRPO: Tree Relative Policy Optimization",
  "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.",
  "authors": [
    "Zhicheng Yang",
    "Zhijiang Guo",
    "Yinya Huang",
    "Xiaodan Liang",
    "Yiwei Wang",
    "Jing Tang"
  ],
  "published": "2025-06-05T15:56:38Z",
  "updated": "2025-06-05T15:56:38Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05183v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05183v1  [cs.LG]  5 Jun 2025TREERPO: T REE RELATIVE POLICY OPTIMIZATION\nZhicheng Yang1Zhijiang Guo1,2Yinya Huang4Xiaodan Liang5,6\nYiwei Wang3Jing Tang1,2\n1The Hong Kong University of Science and Technology (Guangzhou)\n2The Hong Kong University of Science and Technology\n3University of California, Merced4ETH Zurich5Sun Yat-sen University6MBZUAI\n{yangzhch6, cartusguo, xdliang328, wangyw.evan }@gmail.com\nyinya.huang@hotmail.com, jingtang@ust.hk\nABSTRACT\nLarge Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. How-\never, a key limitation of existing approaches is that rewards defined at the full tra-\njectory level provide insufficient guidance for optimizing the intermediate steps of\na reasoning process. To address this, we introduce TREERPO , a novel method\nthat estimates the mathematical expectations of rewards at various reasoning steps\nusing tree sampling. Unlike prior methods that rely on a separate step reward\nmodel, T REERPO directly estimates these rewards through this sampling process.\nBuilding on the group-relative reward training mechanism of GRPO, T REERPO\ninnovatively computes rewards based on step-level groups generated during tree\nsampling. This advancement allows T REERPO to produce fine-grained and dense\nreward signals, significantly enhancing the learning process and overall perfor-\nmance of LLMs. Experimental results demonstrate that our T REERPO algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0% to 35.5%. Furthermore, T REERPO signifi-\ncantly outperforms GRPO by 2.9% in performance while simultaneously reducing\nthe average response length by 18.1%, showcasing its effectiveness and efficiency.\nOur code will be available at https://github.com/yangzhch6/TreeRPO.\n0.180.20.220.240.260.280.30.320.340.36\n0 80 160 240 320Pass@1 Accuracy (%)\nTraing StepsTreeRPO\nGRPO\nFigure 1: The average Pass@1 accuracy of T REERPO and GRPO with Qwen-2.5-Math-1.5b on\nfour mathematical benchmarks: MATH-500, OlympiadBench, Minerva, and AIME.\n1 I NTRODUCTION\nRecent advancements in test-time scaling with reinforcement learning methods bring milestone\nprogress to Large Language Models (LLMs). Reasoning models such as OpenAI O1 (OpenAI,\n2024), DeepSeek R1 (Guo et al., 2025), and QwQ (Qwen, 2024) have demonstrated significantly\n1\n--- Page 2 ---\nsuperior performance in complex reasoning tasks. Reinforcement Learning with Verifiable Rewards\n(RLVR) plays a pivotal role in this progress, which enhances the model’s performance by continu-\nously exploring reasoning paths toward correct answers on verifiable problems.\nIn the realm of LLM-RL integration for complex reasoning, existing approaches can be broadly\ncategorized into two paradigms: reward model-based methods (Ouyang et al., 2022; Shen et al.,\n2025; Schulman et al., 2017) and reward model-free methods (Rafailov et al., 2023; Shao et al.,\n2024; Zeng et al., 2025; Luo et al., 2025b). Among reward model-based methods, reward models are\ntypically divided into outcome reward models (ORMs; Cobbe et al. 2021; Yu et al. 2023) and process\nreward models (PRMs; Lightman et al. 2023; Wang et al. 2024; Lu et al. 2024a; Chen et al. 2025).\nORMs provide a single scalar reward for the entire generation sequence, while PRMs offer step-\nwise evaluations of the reasoning path. The fine-grained, dense reward signals from PRMs generally\nyield superior RL performance compared to ORMs. However, acquiring high-quality training data\nfor PRMs remains challenging, as accurately annotating the correctness of intermediate reasoning\nsteps requires substantial domain expertise. This data scarcity significantly hinders the scalability of\nPRM-based approaches.\nRecent breakthroughs in enhancing LLM reasoning capabilities, such as GRPO (Shao et al., 2024)\nand its variants (Yu et al., 2025; Yue et al., 2025), have adopted a reward model-free paradigm.\nThese methods leverage verifiable reward functions trained on complex reasoning datasets, where\nrewards are determined by whether the model’s final output matches the ground-truth numerical\nanswer or passes predefined unit tests in programming tasks. This approach achieves remarkable\nscalability by eliminating the need for human annotations or reward models. However, similar to\nORMs, these rule-based methods only provide trajectory-level rewards, offering limited guidance for\noptimizing intermediate reasoning steps. Consequently, the question of how to deliver dense, fine-\ngrained reward signals without relying on reward models presents an important research direction.\nTo address this challenge, we propose TREERPO , a novel approach that estimates the mathematical\nexpectations of rewards at various reasoning steps through tree sampling. Unlike previous methods\nthat require explicit step-level reward models, T REERPO employs a tree-based sampling mech-\nanism to approximate these expectations. Building upon GRPO’s group-relative reward training\nframework, T REERPO innovatively computes rewards based on step-level groups within the sam-\npled tree structure. This design enables the generation of fine-grained, dense reward signals that\nguide the model’s reasoning process more effectively while maintaining the scalability advantages\nof verifiable reward functions. Through this approach, T REERPO achieves more efficient and ef-\nfective optimization of LLM reasoning capabilities.\nTo summarize, our main contributions are as follows:\n• To the best of our knowledge, T REERPO is the first reward model-free method that pro-\nvides dense process reward signals through tree sampling and group relative reward com-\nputation, significantly enhancing the efficiency of RL-based reasoning optimization.\n• Through extensive experimentation, T REERPO was found to significantly increase Qwen-\n2.5-Math’s Pass@1 accuracy on various benchmarks from 19.0% to 35.5%, including a\n2.9% lead over GRPO.\n• Detailed analysis demonstrates that T REERPO achieves higher accuracy and reduces to-\nken consumption. Specifically, it cuts the average response length on test benchmarks by\n18.1% compared to GRPO, showcasing more efficient and precise reasoning.\n2 R ELATED WORKS\n2.1 E LICITING COMPLEX REASONING ABILITY\nComplex reasoning tasks (Hendrycks et al., 2021; He et al., 2024; Lewkowycz et al., 2022; Zeng\net al., 2024; Yang et al., 2025; Xiang et al., 2025) such as mathematical problem solving are one of\nthe most challenging tasks for LLMs. Various methods are proposed to elicit the reasoning ability\nof LLMs. These approaches can be divided into two groups:\n1)In-context learning : These methods aim to improve the reasoning ability of LLMs by design-\ning various prompting strategies and frameworks without updating the model parameters. Chain-\nof-thought (CoT; Wei et al. 2022) prompting shows that intermediate reasoning steps can greatly\n2\n--- Page 3 ---\nimprove model performance. Subsequent research (Zhang et al., 2023; Yao et al., 2023; Bi et al.,\n2023; Yang et al., 2024b) has further enhanced CoT through various methods.\n2)Fine-tuning : This line of approaches (Yang et al., 2022; Yu et al., 2024; Lu et al., 2024b; Huang\net al., 2024; Tong et al., 2024) involve finetuning on extensive and high-quality datasets to improve\nreasoning capabilities. The core of these methods is to construct high-quality question-response\npairs with chain-of-thought reasoning processes. MetaMath (Yu et al., 2024) focuses on data aug-\nmentation for both questions and answer texts. MathGenie (Lu et al., 2024b) collects a vast amount\nof data through open-source language models. DART-Math (Tong et al., 2024) generates diverse\nsolutions with the difficulty-aware rejection sampling. Recent studies (Shao et al., 2024; Hu et al.,\n2025; Zeng et al., 2025; Luo et al., 2025b; Yu et al., 2025; Yue et al., 2025) have explored reinforce-\nment learning in complex reasoning tasks and have acquired great achievements. Inspired by recent\nsuccesses in reinforcement learning for complex reasoning tasks, we propose T REERPO, an innova-\ntive reinforcement learning method that leverages tree sampling to further enhance LLM reasoning\nability.\n2.2 R EINFORCEMENT LEARNING WITH LLM S\nReinforcement Learning from Human Feedback (RLHF; Ouyang et al. 2022) has been widely used\nin LLM alignments. Direct Preference Optimization (DPO; Rafailov et al. 2023) is further proposed\nto simplify the training pipeline of RLHF, which directly uses pair-wise preference data for model\noptimization. Recent studies (OpenAI, 2024; Guo et al., 2025; XAI, 2024; DeepMind, 2024; Qwen,\n2024; Team et al., 2025) have shown that reinforcement learning can significantly improve the rea-\nsoning ability of models. This type of work can roughly be divided into two categories:\n1)Reward model-based : There are two primary types of reward models: the Outcome Reward\nModel (ORM) and the Process Reward Model (PRM). Prior effort (Lightman et al., 2023) sug-\ngests that PRM outperforms ORM due to the fine-grained step-by-step reward signals. Math-\nShepherd (Wang et al., 2024) trains a PRM by estimating the potential for a given reasoning step.\nHowever, training a reward model requires extensive, high-quality annotated data, especially for\nPRMs. This hinders the scaling of reward models in the field of complex reasoning.\n2)Reward model-free : DPO is one of these methods, but it requires the elaborate construction of\npairwise data for training. Step-DPO (Lai et al., 2024) constructs a pipeline to generate pair-wise\nstep-level data and surpasses the performance of DPO. The other line of research (Shao et al., 2024;\nHu et al., 2025; Zeng et al., 2025; Luo et al., 2025b) has shown that verification functions are ef-\nfective in improving the reasoning capabilities of LLMs. They avoid the need for reward models,\noffering a simple yet effective approach. The typical methods are GRPO (Shao et al., 2024) and its\nvariants DAPO (Yu et al., 2025) and V APO (Yue et al., 2025). However, rule-based reward is similar\nto ORM, providing trajectory-level reward signals rather than fine-grained process reward signals.\nUnlike existing efforts, T REERPO achieves fine-grained, dense reward signals without relying on\na separate reward model. T REERPO can offer a more scalable solution for obtaining dense reward\nsignals in complex reasoning tasks.\n3 T REERPO: M ETHODOLOGY\nIn this section, we elaborate on the proposed T REERPO. First, we present tree sampling in Sec-\ntion 3.1, which is designed to construct step step-level group to enhance long-chain reasoning abil-\nities with GRPO. Next, in Section 3.2, we introduced the pruning strategy to improve the sampling\nand training efficiency in T REERPO. In Section 3.3, we discuss the numerical influence of standard-\nized binary rewards and continuous rewards on advantage computation and propose a new advantage\ncomputation method for continuous rewards.\n3.1 T REE SAMPLING\nWhile GRPO has been proven to be effective and suitable for scaling in complex reasoning tasks\nwith verifiable reward, it only provides the trajectory-level reward by evaluating the final answer of\nthe generated sequences. Instead, to provide step-level reward estimation without using a reward\nmodel, we designed tree sampling.\n3\n--- Page 4 ---\n+1.0 +1.0 0.0 +1.0 0.0 0.00.0 0.0 +1.0 0.0+1.0 0.0+1.0 +1.0 +1.0 0.0 0.0 0.0\n+0.667+0.111 +0.556+0.037 +0.444 +0.852\n0.0+0.333+0.333question\nStep 1\nStep 2\nStep 3\nStep 4format\ntrain batch\n……group 3group 1\ngroup 2\ngroup NFigure 2: The sampling process of our T REERPO. T REERPO starts from the question, sampling N\nnodes at each step until generation is completed or the maximum depth limit Dis reached. Then,\na verifiable reward function is used to evaluate all leaf nodes and then back-propagates the rewards\nto their parent nodes, thereby obtaining intermediate step rewards, which achieves process reward\nsignaling. We traverse each node and aggregate all children steps of a node into a group to compute\nadvantages, which are finally formatted into the training batch.\nGiven an input question q, the language model generates an N-ary tree through iterative sampling,\ngoverned by the following constraints:\n•Branching Factor : At each decoding step, the model samples Ncandidate continuations,\nexpanding Nnew branches from the current node.\n•Depth Limit : The tree expansion terminates when any path reaches the maximum depth\nD, ensuring tractability.\n•Step Segmentation : We directly divide the steps according to the token length. Each step\nproduces at most Lsteptokens per branch. Generation halts for a branch if a stop token is\ngenerated, or the branch violates reaches depth limit. A more precise step division method\nis our future work.\nAs shown in Figure 2, the tree’s reward computation follows a bottom-up recursive expectation\nscheme, where:\n•Leaf Evaluation : For each leaf node vleaf, the verification function ϕtakes the entire path\nP= [vroot, . . . , v leaf]as input and computes the reward:\nrleaf=ϕ(P) =ϕ\u0000\n[vroot, . . . , v leaf]\u0001\n,\n•Parent Propagation : Non-leaf nodes aggregate rewards from their children:\nrnode=Ec∈Children (vnode)\u0002\nrc\u0003\n.\nThis propagates bottom-up, weighting all viable completion paths.\nIn conclusion, our tree sampling framework estimates the reward of each step as its potential to\ndeduce the correct final answer.\n3.2 D ATA PRUNING\nSimilar to the Dynamic Sampling strategy of DAPO, we filter out the samples to keep all data\nsamples in the training batch with effective gradients.\nIn the data construction pipeline of T REERPO, a group Gis formally defined as the set of child\nnodes c1, . . . , c noriginating from a common parent node p, as illustrated in Figure 2. Adopting a\nstrategy analogous to the dynamic sampling approach in DAPO, we perform group-level filtering\nbased on reward distribution characteristics.\n∆RG= max\nci∈GR(ci)−min\ncj∈GR(cj) (1)\n4\n--- Page 5 ---\nwhere R(ci)denotes the reward associated with child node ci. We introduce a variance threshold τ\nsuch that a group Gis included in the training batch Bif and only if:\nG ∈ B ⇐⇒ ∆RG> τ (2)\nThe threshold τoperates as a hyperparameter controlling the trade-off between sample diversity and\nlearning signal strength in the batch construction process.\nThis data selection criterion ensures all samples in the batch with effective gradients and improves\nthe efficiency of the training process.\n3.3 A DVANTAGE COMPUTATION\nIn the vanilla GRPO framework, the advantage estimation is derived by normalizing binary rewards:\nˆAi,t=ri−mean ({Ri}G\ni=1)\nstd({Ri}G\ni=1). (3)\nHowever, when applied to continuous rewards, this approach introduces significant bias. For in-\nstance, two reward sequences, R1= [0,0,1,1]andR2= [0.49,0.49,0.51,0.51], produce identical\nnormalized advantages [−1,−1,1,1], despite their distinct reward distributions. While R1exhibits\na clear bimodal separation, R2contains only minor variations (a maximal difference of 0.02). This\nindicates that standard normalization fails to properly scale advantages for continuous rewards, lead-\ning to misleading policy updates.\nTo mitigate this bias, we propose an alternative advantage computation that preserves the statistical\nproperties of binary reward normalization while accommodating continuous rewards. Instead of\ncomputing the empirical variance from R, we define the normalization factor as σ=µ(1−µ),\nwhere µis the mean reward. This formulation maintains consistency with the variance of Bernoulli-\ndistributed rewards (Var [R] =µ(1−µ)) while generalizing to continuous settings.\nFor a given reward sequence R= [R1, R2, . . . , R n], the advantage is computed as:\nµ=1\nnnX\ni=1Riσ=µ(1−µ)\nAi=Ri−µ\nσ(4)\nBy fixing the variance term σtoµ(1−µ), we ensure that advantage values remain interpretable and\nstable, avoiding the overamplification of small differences in continuous rewards. This approach\nbridges the gap between binary and continuous reward normalization while maintaining the original\nscaling behavior of GRPO.\n3.4 O BJECTIVE OF TREERPO\nWe adopt the clipped objective of GRPO, together with a directly imposed KL penalty term: Ad-\nditionally, the KL-regularization between current policy πθand the reference policy πrefis directly\nadded to the loss function:\nJTreeRPO (θ) =E(q∼D,{oi}G\ni=1∼πθold(q)\n\"\n1\nGGX\ni=11\n|oi||oi|X\nt=1 \nmin\u0010\nri,t(θ)ˆAi,t,clip\u0010\nri,t(θ),1−ε,1 +ε\u0011\nˆAi,t\u0011\n−βD KL(πθ||πref)!#\n,(5)\nwhere\nri,t(θ) =πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t). (6)\n5\n--- Page 6 ---\n4 E XPERIMENTS\nDatasets. We conduct the evaluation of our experiments using 4 widely used mathematical rea-\nsoning benchmarks: MATH-500 (Lightman et al., 2023), OlympiadBench (He et al., 2024), Minver-\nvaMath (Lewkowycz et al., 2022), and AIME24. Among them, Math-500 are 500 items screened\nout from the original MATH test split. The subset consists of 500 representative problems, and the\nevaluation produces similar results to the full-set evaluation. In the training scenario, we use the\ntraining split of MATH dataset, which contains 7.5k high-quality training samples. In the future,\nwe will extend the experiment to the DeepScaler (Luo et al., 2025b) training data, which is a more\nchallenging dataset for mathematical reasoning.\nParameter Setting. Our experiments are based on Qwen2.5-Math series language models (Yang\net al., 2024a). In the evaluation procedure, we set the temperature as 0.6 to sample 8 candidate re-\nsponses for each question. In the reinforcement learning training procedure, we set the temperature\nas 0.6 and roll out 8 responses for each question. The learning rate is 1e-6 for both GRPO and\nTREERPO. The coefficients for KL divergence and entropy loss are β= 0.001andα=−0.001,\nrespectively. For GRPO, the training batch size is 128 and the mini-batch size is 64. For our\nTREERPO, the training batch size is 128. Since the training data size of each step of T REERPO is\nfloating, the size of our mini-batch is obtained as half of the training data size. By default, the max-\nimum prompt length is 512, and the maximum response length is 1152 for GRPO. For T REERPO,\nthe maximum prompt length is 512, the maximum step length Lstepis 384, the maximum depth D\nof tree sampling is set as 3, and the N-ary is set as 8. For better efficiency, we set the data pruning\ncoefficient τto 0.1 as described in Sec. 3.2.\nImplementation Details. We follow the rllm (Luo et al., 2025a;b) framework which is derived\nfrom the verl (Sheng et al., 2024) pipeline. Both rllm and verl integrate the vllm (Kwon et al., 2023)\nframework for efficient inference of models. All of our experiments are conducted on A800 GPUs.\nAt present, the LLM of our experiment is the Qwen2.5-Math series. Due to the limitations of time\nand computation resources, we have only reported the 1.5b model. We plan to conduct experiments\non 7b and 32b as soon as possible\nMetrics. We use the same verification function in rllm to evaluate the performance of LLMs.\nCompared with other repositories, the reward function implemented by rllm is more complete and\nsystematic. For the test results, the accuracy rate we report is pass@1(avg@8) performance for all\ntested benchmarks.\n4.1 M AINRESULTS\nTREERPO demonstrates significant performance improvements. We conduct T REERPO and\nGRPO on Qwen2.5-Math-1.5b model with the training split of the MATH dataset, and conduct the\nevaluation on four selected benchmarks: Math-500, MinervaMath, OlympiadBench, and AIME. As\nshown in Figure 3, our T REERPO outperform GRPO on all of the tested benchmarks. After training\n360 steps, our T REERPO outperforms GRPO by 2.7% on MATH-500, 3.5% on MinervaMath, 2.4%\non OlympiadBench, and 3.0% on AIME, respectively. As illustrated in Figure 1, T REERPO out-\nperforms the overall performance of GRPO by 2.9% . In conclusion, T REERPO has demonstrated\nconsistent superiority on multiple benchmarks.\nTREERPO demonstrates efficiency advantage in token usage. We conduct T REERPO and\nGRPO on the Qwen2.5-Math-1.5b model with the training split of the MATH dataset, and compute\nthe average response length on four selected benchmarks: Math-500, MinervaMath, Olympiad-\nBench, and AIME. As illustrated in Figure 4, compared to GRPO, our T REERPO achieves a 17.1%\nreduction in token usage on MATH, 22.3% on MinervaMath, 18.0% on OlympiadBench, and 15.3%\non AIME. On average, T REERPO demonstrates a 18.1% decrease in token usage across the four\nbenchmarks compared to GRPO, showcasing its superior efficiency.\nWe show the response case of a simple question in Figure 5. It can be seen that in this simple case,\nTREERPO’s response is more concise\n6\n--- Page 7 ---\n0.360.410.460.510.560.610.660.71\n0 100 200 300Pass@1 Accuracy (%)\nTraining StepsTreeRPO\nGRPO\n0.120.140.160.180.20.220.24\n0 100 200 300Pass@1 Accuracy (%)\nTraining StepsTreeRPO\nGRPO\n0.20.220.240.260.280.30.32\n0 100 200 300Pass@1 Accuracy (%)\nTraining StepsTreeRPO\nGRPO00.050.10.150.20.25\n0 100 200 300Pass@1 Accuracy (%)\nTraining StepsTreeRPO\nGRPO(a) MATH -500 Pass@1 Accuracy (b) MinervaMath Pass@1 Accuracy\n(c) OlympiadBench Pass@1 Accuracy (d) AIME Pass@1 AccuracyFigure 3: Performance comparison of our T REERPO and GRPO on the four selected bench-\nmarks: Math-500, MinervaMath, OlympiadBench, and AIME. The experiments are conducted with\nQwen2.5-Math-1.5b, an LLM pretrained with a large amount of mathematical corpus.\n400450500550600650700\n0 100 200 300Response Length\nTraining StepsTreeRPO\nGRPO\n400450500550600650700750800850900\n0 100 200 300Response Length\nTraining StepsTreeRPO\nGRPO\n500550600650700750800\n0 100 200 300Response Length\nTraining StepsTreeRPO\nGRPO\n600650700750800850900\n0 100 200 300Response Length\nTraining StepsTreeRPO\nGRPO(a) Avg Response Length on MATH -500 (b) Avg Response Length on MinervaMath\n(c) Avg Response Length on OlympiadBench (d) Avg Response Length on AIME\nFigure 4: Response Length comparison of our T REERPO and GRPO on the four selected bench-\nmarks: Math-500, MinervaMath, OlympiadBench, and AIME. The experiments is conducted with\nQwen2.5-Math-1.5b\n7\n--- Page 8 ---\nQuestion : Solve1\n𝑥−5>0. Enter your answer using interval notation.\nTreeRPO :\nTo solve the inequality 1\n𝑥−5>0, we need to \ndetermine when the expression 1\n𝑥−5is \npositive. The fraction 1\n𝑥−5is positive when \nthe denominator 𝑥−5is positive. Therefore, \nwe need to solve the inequality 𝑥−5>0.\nAdding 5 to both sides of the inequality, we \nget:\n𝑥>5\nSo, the solution to the inequality 1\n𝑥−5>0is \n(5,+∞).\nThe final answer in interval notation is:\n\\boxed{(5,+∞)}.GRPO :\nTo solve the inequality 1\n𝑥−5>0,we need to \ndetermine the values of 𝑥for which the \nexpression 1\n𝑥−5is positive. The expression \n1\n𝑥−5is positive when the denominator 𝑥−5is \npositive. Therefore, we need to solve the \ninequality 𝑥−5>0.\nLet's solve this step -by-step:\n1. Start with the inequality:\n𝑥−5>0\n2. Add 5 to both sides of the inequality:\n𝑥>5\nSo, the solution to the inequality 1\n𝑥−5>0is \n𝑥>5. In interval notation, this is written as \n(5,+∞).\nThe final answer is: \\boxed{(5,+∞)}.Figure 5: Responses of T REERPO and GRPO of a simple question.\nThe performance of T REERPO under different hyperparameters. In the experiments, we con-\nduct experimental analyses using different batch sizes, and the results are shown in Figure 6. For\nGRPO and T REERPO, the batch size bsz= 16 /128has very little influence on the final perfor-\nmance. Our T REERPO significantly outperforms GRPO in both two Settings.\nIn addition, there are 3 hyperparameters to control the generation procedure of tree sampling:\n• Branching Factor N: At each decoding step, the model samples N candidates.\n• Maximum Depth Limit D: The tree expansion terminates when any path reaches the max-\nimum depth.\n• Maximum Step Length Lstep: Each step produces at most Lsteptokens per branch.\nWe will conduct more detailed experimental analyses on these 3 hyperparameters in the future.\n0.170.220.270.320.37\n0100 200 300 400 500 600 700 800 900Pass@1 Accuracy (%)\nTraining StepsGRPO\nTreeRPO\n500550600650700750800\n0100 200 300 400 500 600 700 800 900Avg Response Length\nTraining StepsGRPO\nTreeRPO\nFigure 6: Comparison of T REERPO and GRPO with bsz= 16 . The pass@1 accuracy and the\nresponse length are calculated by taking the average on the four tested benchmarks.\n5 C ONCLUSION\nIn this paper, we propose T REERPO, which conducts tree sampling to construct step-level groups\nbased on vanilla GRPO. T REERPO obtains the reward of the current step by estimating the reward\nof the subsequent sampling paths of the current step. This is a method that can obtain dense re-\nward signals without the need for process reward models (PRMs). The experimental results show\n8\n--- Page 9 ---\nthat T REERPO demonstrates both effectiveness and efficiency. In the future, we will continuously\nimprove the algorithm based on the current version and expand the scale of LLM training.\n6 F UTURE WORK AND LIMITATIONS\nRemove Redundant Steps. Yuan et al. (2023) uses Rejection Sampling to collect correct reason-\ning paths for training LLMs. They find that the sampled redundant responses degrade the perfor-\nmance of LLMs. We consider that this phenomenon may also exist in RL. In vanilla GRPO, each\nresponse is treated equally, so responses with high similarity are repeatedly trained, which may cause\nperformance disturbances. We believe that eliminating redundant rollouts can enhance performance\nwhile improving training efficiency through pruning.\nPrecise Step Segmentation. The step division of generated sequences in this article is imple-\nmented based on a specific token length. Give priority to exploring more precise step division\nmethods.\n• One solution to be implemented is to add the step special token and train the language\nmodel to segment different steps by itself.\n• Sampling at the tokens where branch paths are more likely to be generated (Wang et al.,\n2025).\nWe believe that more precise step cutting will provide more accurate fine-grained reward signals and\nfurther enhance the model’s performance.\nScaling on Larger Model Sizes. Due to the limitations of time and GPU resources, our experi-\nment can only report the 1.5b model for the time being. The experimental results of larger-sized\nmodels, such as 7b and 32b, will be updated in the future.\nEngineering Efficiency Optimization of Tree Sampling. Tree sampling is time-consuming, and\nthe tree sampling strategy implemented in this paper is not optimized from the perspective of the\nKV cache. We believe that the engineering optimization of tree sampling will significantly improve\nthe efficiency of the training procedure.\nREFERENCES\nZhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. When do\nprogram-of-thoughts work for reasoning? arXiv preprint arXiv:2308.15452 , 2023.\nJiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li,\nand Kwan-Yee K. Wong. Spc: Evolving self-play critic via adversarial games for llm reasoning,\n2025. URL https://arxiv.org/abs/2504.19162 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\nGoogle DeepMind. Gemini 2.0 flash thinking, 2024. URL https://deepmind.google/\ntechnologies/gemini/flash-thinking/ .\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench:\nA challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scien-\ntific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the\n62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers) , pp. 3828–3850, Bangkok, Thailand, August 2024. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.\nacl-long.211/ .\n9\n--- Page 10 ---\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS ,\n2021.\nJingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum.\nOpen-reasoner-zero: An open source approach to scaling up reinforcement learning on the base\nmodel, 2025. URL https://arxiv.org/abs/2503.24290 .\nDong Huang, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao Qing, Heming Cui, Zhi-\njiang Guo, and Jie M Zhang. Effi-code: Unleashing code efficiency in language models. arXiv\npreprint arXiv:2410.10209 , 2024.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles , 2023.\nXin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-\nwise preference optimization for long-chain reasoning of llms, 2024. URL https://arxiv.\norg/abs/2406.18629 .\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with lan-\nguage models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Ad-\nvances in Neural Information Processing Systems , volume 35, pp. 3843–3857. Curran Associates,\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\narXiv:2305.20050 , 2023.\nJianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang\nGuo. Autopsv: Automated process-supervised verifier. In Amir Globersons, Lester Mackey,\nDanielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.),\nAdvances in Neural Information Processing Systems 38: Annual Conference on Neural Infor-\nmation Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15,\n2024 , 2024a. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n9246aa822579d9b29a140ecdac36ad60-Abstract-Conference.html .\nZimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and\nHongsheng Li. MathGenie: Generating synthetic data with question back-translation for en-\nhancing mathematical reasoning of LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Sriku-\nmar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pp. 2732–2747, Bangkok, Thailand, August 2024b. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.151. URL https:\n//aclanthology.org/2024.acl-long.151/ .\nMichael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi,\nRachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica.\nDeepcoder: A fully open-source 14b coder at o3-mini level, 2025a. Notion Blog.\nMichael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y . Tang, Manan Roongta, Colin Cai,\nJeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview\nwith a 1.5b model by scaling rl, 2025b. Notion Blog.\nOpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-\nlow instructions with human feedback. Advances in neural information processing systems , 35:\n27730–27744, 2022.\n10\n--- Page 11 ---\nQwen. Qwq-32b: Embracing the power of reinforcement learning, 2024. URL https://\nqwenlm.github.io/blog/qwq-32b/ .\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin Neural Information Processing Systems , 36:53728–53741, 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y . K. Li, Y . Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathe-\nmatical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.\n03300 .\nWei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan.\nExploring data scaling trends and effects in reinforcement learning from human feedback. arXiv\npreprint arXiv:2503.22230 , 2025.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint\narXiv: 2409.19256 , 2024.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with\nllms. arXiv preprint arXiv:2501.12599 , 2025.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-\naware rejection tuning for mathematical problem-solving. In The Thirty-eighth Annual Confer-\nence on Neural Information Processing Systems , 2024. URL https://openreview.net/\nforum?id=zLU21oQjD5 .\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhi-\nfang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 9426–9439,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/ .\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen,\nJianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen\nYu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive\neffective reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/\n2506.01939 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems , 35:24824–24837, 2022.\nXAI. Grok 3 beta — the age of reasoning agents, 2024. URL https://x.ai/news/grok-3 .\nKun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi\nChen, Yu-Jie Yuan, Jianhua Han, Hang Xu, Hanhui Li, Mrinmaya Sachan, and Xiaodan Liang.\nSeephys: Does seeing help thinking? – benchmarking vision-based physics reasoning, 2025.\nURL https://arxiv.org/abs/2505.19099 .\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,\nJianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical ex-\npert model via self-improvement, 2024a. URL https://arxiv.org/abs/2409.12122 .\n11\n--- Page 12 ---\nZhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, and Xiaodan Liang. LogicSolver: To-\nwards interpretable math word problem solving with logical prompt-enhanced learning. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022 , pp. 1–13, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.1. URL\nhttps://aclanthology.org/2022.findings-emnlp.1/ .\nZhicheng Yang, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, and Jing Tang.\nAlignedCoT: Prompting large language models via native-speaking demonstrations. In Yaser\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Com-\nputational Linguistics: EMNLP 2024 , pp. 2857–2896, Miami, Florida, USA, November 2024b.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.163. URL\nhttps://aclanthology.org/2024.findings-emnlp.163/ .\nZhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng,\nLinqi Song, Xiaodan Liang, and Jing Tang. Optibench meets resocratic: Measure and improve\nLLMs for optimization modeling. In The Thirteenth International Conference on Learning Rep-\nresentations , 2025. URL https://openreview.net/forum?id=fsDZwS49uY .\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. In\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in\nNeural Information Processing Systems , volume 36, pp. 11809–11822. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf .\nFei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathe-\nmatical reasoning. arXiv preprint arXiv:2311.09724 , 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhen-\nguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions\nfor large language models. In The Twelfth International Conference on Learning Representations ,\n2024. URL https://openreview.net/forum?id=N8N0hgNDRt .\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai,\nTiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guang-\nming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu,\nJiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao\nZhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingx-\nuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL\nhttps://arxiv.org/abs/2503.14476 .\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,\nand Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language\nmodels, 2023.\nYu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi\nWang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu,\nLingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang\nZhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and\nreliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.\norg/abs/2504.05118 .\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-\nzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025.\nURL https://arxiv.org/abs/2503.18892 .\nZhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan\nYao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang\nChen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. Mr-ben: A meta-\nreasoning benchmark for evaluating system-2 thinking in llms. In Amir Globersons, Lester\nMackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang\n12\n--- Page 13 ---\n(eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural\nInformation Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 -\n15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\nd81cb1f4dc6e13aeb45553f80b3d6837-Abstract-Conference.html .\nYifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with\nlarge language models. arXiv preprint arXiv:2308.04371 , 2023.\n13",
  "text_length": 40515
}