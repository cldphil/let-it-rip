{
  "id": "http://arxiv.org/abs/2506.01157v1",
  "title": "Source Tracing of Synthetic Speech Systems Through Paralinguistic\n  Pre-Trained Representations",
  "summary": "In this work, we focus on source tracing of synthetic speech generation\nsystems (STSGS). Each source embeds distinctive paralinguistic features--such\nas pitch, tone, rhythm, and intonation--into their synthesized speech,\nreflecting the underlying design of the generation model. While previous\nresearch has explored representations from speech pre-trained models (SPTMs),\nthe use of representations from SPTM pre-trained for paralinguistic speech\nprocessing, which excel in paralinguistic tasks like synthetic speech\ndetection, speech emotion recognition has not been investigated for STSGS. We\nhypothesize that representations from paralinguistic SPTM will be more\neffective due to its ability to capture source-specific paralinguistic cues\nattributing to its paralinguistic pre-training. Our comparative study of\nrepresentations from various SOTA SPTMs, including paralinguistic, monolingual,\nmultilingual, and speaker recognition, validates this hypothesis. Furthermore,\nwe explore fusion of representations and propose TRIO, a novel framework that\nfuses SPTMs using a gated mechanism for adaptive weighting, followed by\ncanonical correlation loss for inter-representation alignment and\nself-attention for feature refinement. By fusing TRILLsson (Paralinguistic\nSPTM) and x-vector (Speaker recognition SPTM), TRIO outperforms individual\nSPTMs, baseline fusion methods, and sets new SOTA for STSGS in comparison to\nprevious works.",
  "authors": [
    " Girish",
    "Mohd Mujtaba Akhtar",
    "Orchid Chetia Phukan",
    "Drishti Singh",
    "Swarup Ranjan Behera",
    "Pailla Balakrishna Reddy",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "published": "2025-06-01T20:32:10Z",
  "updated": "2025-06-01T20:32:10Z",
  "categories": [
    "eess.AS",
    "cs.SD"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01157v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01157v1  [eess.AS]  1 Jun 2025SOURCE TRACING OF SYNTHETIC SPEECH\nSYSTEMS THROUGH PARALINGUISTIC\nPRE-TRAINED REPRESENTATIONS\nGirish∗‡†, Mohd Mujtaba Akhtar††, Orchid Chetia Phukan‡†, Drishti Singh‡†\nSwarup Ranjan Behera§, Pailla Balakrishna Reddy¶, Arun Balaji Buduru∗, Rajesh Sharma∥∗∗\n∗UPES, India ,†V .B.S.P .U, India ,‡IIIT-Delhi, India ,§Independent Researcher, India ,¶Reliance AI, India ,\n∥University of Tartu, Estonia ,∗∗Plaksha university,India\nCorrespondence:mmakhtar.research@gmail.com,orchidp@iiitd.ac.in\nAbstract —In this work, we focus on source tracing of syn-\nthetic speech generation systems (STSGS). Each source embeds\ndistinctive paralinguistic features—such as pitch, tone, rhythm,\nand intonation—into their synthesized speech, reflecting the\nunderlying design of the generation model. While previous\nresearch has explored representations from speech pre-trained\nmodels (SPTMs), the use of representations from SPTM pre-\ntrained for paralinguistic speech processing, which excel in\nparalinguistic tasks like synthetic speech detection, speech\nemotion recognition has not been investigated for STSGS. We\nhypothesize that representations from paralinguistic SPTM will\nbe more effective due to its ability to capture source-specific\nparalinguistic cues attributing to its paralinguistic pre-training.\nOur comparative study of representations from various SOTA\nSPTMs, including paralinguistic, monolingual, multilingual, and\nspeaker recognition, validates this hypothesis. Furthermore, we\nexplore fusion of representations and propose TRIO , a novel\nframework that fuses SPTMs using a gated mechanism for\nadaptive weighting, followed by canonical correlation loss for\ninter-representation alignment and self-attention for feature\nrefinement. By fusing TRILLsson (Paralinguistic SPTM) and\nx-vector (Speaker recognition SPTM), TRIO outperforms indi-\nvidual SPTMs, baseline fusion methods, and sets new SOTA for\nSTSGS in comparison to previous works.\nIndex Terms —Source Tracing, Paralinguistic Pre-Trained\nModels, Synthetic Speech Generators\nI. INTRODUCTION\nAdvancements in audio manipulation technology have\nblurred the line between real and synthetic speech. Modern\ntext-to-speech (TTS) and voice conversion (VC) systems can\nproduce highly realistic voices, enabling malicious actors to\nmanipulate speech with remarkable accuracy. This growing\nchallenge highlights the urgent need for reliable methods to\ndetect and attribute synthetic speech. This advancement poses\nsignificant risks, as malicious entities can exploit synthetic\nspeech for impersonation, fraud, and misinformation. As such\nthe urgent need for robust synthetic speech detection (SSD)\nsolutions becomes undeniable to safeguard trust in digital\ncommunication. As a remedy there has been sufficient re-\nsearch into SDD [1], [2], [3]. Also, the use of representations\nfrom speech pre-trained models (SPTMs) such as Wav2vec2,\n†Contributed equally as first authorsWavLM, Whisper have captured recent attention within the\ncommunity as these SPTMs provide performance benefit\n[4], [5], [6]. These SPTMs are either fine-tuned or used\nas feature extractors for extracting representations. Despite\nmuch advancement in synthetic speech detection, most of the\nprevious research has mostly focused on distinguishing real\nand synthetically generated speech i.e. binary classification,\nbut it is not sufficient to predict and mitigate misuse and\nimprove forensic analysis.\nTo further enhance synthetic speech detection from foren-\nsic analysis, it is important to understand the exact tool used\nto generate the speech and this task is known as Source\nTracing of Synthetic Speech Generation Systems (STSGS).\nIt has recently captured attention within the community\nand plays a crucial role in improving the explainability of\ndetection systems, enforcing accountability, and developing\ntargeted countermeasures against malicious deepfake appli-\ncations [7], [8], [9]. Each source (TTS, VC) imprint distinc-\ntive paralinguistic features—including pitch, tone, rhythm,\nand intonation—onto their synthesized speech, mirroring the\nunderlying design principles and processing mechanisms of\nthe respective generation models.\nAs such previous research on STSGS have investigated\nrepresentations from various state-of-the-art (SOTA) SPTMs\n[10], [11], [12], [13] for understanding their capability for\ncapturing such source-specific paralinguistic cues. However,\nthey haven’t investigated the usage of representations from\nSPTM pre-trained for paralinguistic speech processing such\nas TRILLsson [14] which have shown SOTA behavior for\ndifferent paralinguistic tasks including tasks such as synthetic\nspeech detection and speech emotion recognition. In this\nwork, we solve this research gap and explore representa-\ntions from paralinguistic SPTM for STSGS. We hypothesize\nthat paralinguistic SPTM representations will be the most\neffective for STSGS, as their specialized paralinguistic pre-\ntraining enables them to capture paralinguistic cues unique\nto each source more effectively than other SPTM represen-\ntations . To test this hypothesis, we conduct a comprehensive\ncomparative study of various SOTA SPTMs, including par-\nalinguistic, monolingual, multilingual, and speaker recogni-\n--- Page 2 ---\ntion. Our findings validate our hypothesis.\nAdditionally, inspired by prior research demonstrating\nperformance gains through SPTMs representations fusion\nin related areas such as synthetic speech detection [15]\nand speech emotion recognition [16], we also explore this\ndirection for STSGS. Phukan et al. [13] have made the\ninitial exploration for fusion of SPTMs representation for\nSTSGS, however, they have considered only a handful of\nSPTMs representations, here, in our study, we consider a\nwide range of SOTA SPTMs representations and also the\ninclusion of paralinguistic SPTM representations that has\nbeen missing and a major drawback in their study. To this\nend, we introduce TRIO (GaTed Canonical Cor RelatIOn\nAttention Network), a novel framework for fusing SPTMs.\nTRIO employs a gated mechanism for adaptive weighting\nof representations, incorporates canonical correlation loss for\nbetter alignment between the representations, and utilizes\nself-attention for enhanced feature refinement. By fusing\nTRILLsson (a paralinguistic SPTM) with x-vector (a speaker\nrecognition SPTM), TRIO achieves superior performance,\noutperforming individual SPTMs, baseline fusion techniques,\nand setting a new SOTA benchmark for STSGS in compari-\nson to previous works.\nIn summary, the key contributions of this work are as\nfollows:\n•We carry out a comprehensive comparative analysis of\nvarious SOTA SPTMs representations to understand the\ncapability of paralinguistic SPTM representations for\nSTSGS. We show that representations from TRILLsson\nachieves the topmost performance amongst all other\nSPTMs representations.\n•We introduce a novel framework, TRIO for effective\nfusion of SPTMs representations. TRIO uses a gated\nmechanism for adaptive representation weighting, ap-\nplies canonical correlation loss for improved alignment,\nand employs self-attention for refined feature enhance-\nment. By fusing TRILLsson and x-vector, TRIO sur-\npasses individual SPTMs and baseline fusion methods,\nsetting a new SOTA benchmark for STSGS compared\nto prior works.\nTo make our work more accessible and reproducible, we’ve\nshared the code and models at1.\nII. S PEECH PRE-TRAINED REPRESENTATIONS\nIn this section, we present a brief overview of the SPTMs\nused in our study. Wav2Vec22[17], WavLM3[18], and\nUnispeech-SAT4[19] are monolingual SPTMs and we con-\nsider their base versions pre-trained on LibriSpeech (960\nhours of English). Wav2Vec2 was trained to solve a con-\ntrastive learning objective, WavLM was pre-trained for solv-\ning masked speech modeling and speech denoising simulta-\nneously while Unispeech-SAT was trained in a multi-task\nspeaker-aware format. Both WavLM and Unispeech-SAT\n1https://github.com/Helix-IIIT-Delhi/TRIO-Source Tracing\n2https://huggingface.co/facebook/wav2vec2-base\n3https://huggingface.co/microsoft/wavlm-base\n4https://huggingface.co/microsoft/unispeech-sat-basehave reported SOTA performance in SUPERB. We consider\nXLS-R5[20], Whisper6[21], and MMS7[22] for multilingual\nSPTMs. We consider their 300M, 74M, and 1B parameters\nversion for XLS-R, Whisper, and MMS respectively. XLS-R,\nWhisper, MMS were pre-trained on 128, 96, and over 1400\nlanguages respectively. XLS-R and MMS follows Wav2vec2\narchitecture and pre-trained in a contrastive learning approach\nwhile Whisper is a vanilla transformer encoder-decoder\narchitecture and trained in a multi-task manner. We also\nconsider speaker recognition SPTMs such as x-vector8[23]\nand ECAPA9[24] as they have shown its effectiveness for\nsynthetic speech detection [15] as well as STSGS [13].\nHowever, Phukan et al. [13] only considered x-vector in\ntheir study and here, in our study, we included, ECAPA,\nwhich shows further improvement over x-vector in speaker\nrecognition tasks. Both x-vector and ECAPA are trained\non V oxceleb1 + V oxceleb2. As paralinguistic SPTM, we\nconsider TRILLsson10[14]. It is a distilled model from the\nSOTA universal paralinguistic conformer (CAP12). TRILLs-\nson representations shows SOTA performance across various\nparalinguistic tasks such as speech emotion recognition,\nsynthetic speech detection, speaker recognition, and we use\nthe version with 63M parameters. Additionally, we also add\nWav2Vec2-emo11, a SPTM fine-tuned for SER because SER\nis inherently a paralinguistic application. Before passing the\nspeech samples to SPTMs, we resample them to 16KHz and\nextract representations from the last hidden state of the frozen\nSPTMs by mean pooling. We extract representations of 192\nfor ECAPA; 512 for x-vector, Whisper (We use its encoder);\n768 for Wav2vec2, WavLM, Unispeech-SAT, Wav2vec2-\nemo; 1024 for TRILLsson; 1280 for XLS-R, MMS.\nIII. M ODELING\nIn this section, we discuss the downstream models used\nwith individual representations followed by the proposed\nframework, TRIO for fusion of SPTMs representations. We\nuse fully connected network (FCN) and CNN as downstream\nmodels as they have preferred by previous research as ef-\nfective downstream networks [15], [13]. The CNN model\nconsists of two convolutional blocks that receives SPTMs\nrepresentations as input with 1D-CNN layers of 128 and 64\nfilters of kernel size 3 with each 1D-CNN layer followed by\nmaxpooling. Then we flatten the outputs and use a FCN block\nthat consists of two dense layers with 90 and 45 neurons\neach followed by the final output layer that uses softmax\nas activation function and outputs probabilities of the source\nclasses. The FCN model follows the same modeling paradigm\nas used for the FCN block in the CNN model. The number of\ntrainable parameters in FCN models ranges 0.6 to 0.8M while\nfor CNN models, it varies between 0.8 to 1.2M, depending\non the input representations dimensionality.\n5https://huggingface.co/facebook/wav2vec2-xls-r-300m\n6https://huggingface.co/openai/whisper-base\n7https://huggingface.co/facebook/mms-1b\n8https://huggingface.co/speechbrain/spkrec-xvect-voxceleb\n9https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb\n10https://www.kaggle.com/models/google/trillsson\n11https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP\n--- Page 3 ---\nMaxpooling\n1D-CNN2x\nMaxpooling\n1D-CNN\nRepresentation RepresentationConcatenationFCNOutput\nCCALCE\nL = LCE + (-LCCA)\nLCCAMSA\nSPTM SPTM\nInputσ σσSigmoid\nMultiplication\n2xFig. 1: Proposed Framework: TRIO\nA.TRIO\nThe architecture diagram of the proposed framework,\nTRIO for fusion of SPTMs representations is shown in Figure\n1.TRIO leverages a gated mechanism to adaptively weight\nrepresentations, integrates canonical correlation loss to im-\nprove alignment between them, and applies self-attention for\nmore effective feature refinement. First, the SPTMs represen-\ntations are passed through to two convolutional blocks that\nuses same modeling as used for individual representational\nmodeling above. Suppose, XandYare features from two\nSPTMs branches after the flattening them. Then the flattened\nfeatures are passed through gated mechanism that consists of\na sigmoid gate and the outputs are GXandGY. After that,\nwe perform element wise-multiplication with the original\nfeatures, ˆX=GX⊙X, ˆY=GY⊙Yto extract the most\nrelevant features. Next, the refined features are aligned using\ncanonical correlation analysis (CCA) as a novel loss function,\nwhich maximizes the correlation between ˆXandˆY. Higher\nCCA means better alignment. The CCA loss is formulated\nas:\nLCCA=tr\u0010\n(ΣˆXˆX)−1/2ΣˆXˆY(ΣˆYˆY)−1/2\u0011\nwhere ΣˆXˆXandΣˆYˆYare the covariance matrices of ˆX\nandˆY,ΣˆXˆYis the cross-covariance matrix between ˆXand\nˆY. tr(·)denotes the trace operation. LCCA ensures that the\nrepresentations ˆXandˆYare maximally correlated, thereby\nimproving their alignment. After aligning the features to a\njoint representational space, we concatenate the features from\nthe two SPTMs representation networks. Following this, we\nuse a self-attention mechanism, which computes the queries\nQ, keys K, and values Vas:Q=XconcatWQ, K =XconcatWK, V =XconcatWVwhere Xconcat represents the\nconcatenated features from SPTMs representations branches.\nThe attention scores are then computed using the scaled dot-\nproduct attention:\nAttention (Q, K, V ) =softmax\u0012QKT\n√dk\u0013\nV\nThen the features are passed through a FCN block of two\ndense layers with 90 and 45 neurons followed by a output\nlayer with softmax activation that outputs probabilities. We\nperform joint optimization with the cross entropy loss LCCA .\nFinally, the total loss Lis given as: L=LCE+(−λ·LCCA)\nwhere λis a hyperparameter controlling the importance of\nLCCA . The negative sign before LCCA is used because\nLCCA is formulated to maximize correlation, while loss\nfunctions are typically minimized in optimization. By using\na negative sign, we effectively encourage the model to\nmaximize correlation while jointly minimizing LCE. The\ntrainable parameters range from 1.3 to 1.5M.\nIV. E XPERIMENTS\nA. Dataset\nWe use two benchmark synthetic speech detection\ndatabases: ASVSpoof 2019 (ASV) [25] and FAD Chinese\nDataset (CFAD) [26]. ASV contains both real and synthetic\nspeech samples from 19 generative systems, recorded at 16\nkHz. Real recordings feature diverse speakers with varying\naccents and speaking styles, while synthetic samples were\ngenerated using SOTA VC and TTS methods. We merged\nthe train, validation, and test splits for ASV , resulting in 19\nsynthetic speech source classes (A01 to A19). We followed\n5-fold cross-validation for ASV , with 4 folds used for training\nand one fold for testing. CFAD is a chinese dataset and\nfeatures real and synthetic samples from 12 speech synthesis\ntechniques including SOTA TTS and VC systems. We use the\nofficial dataset split for training, validating and evaluation of\nthe models.\nTraining and Hyperparameter Details : The models are\ntrained for 50 epochs with a batch size of 32, utilizing\nAdam as optimizer and cross-entropy as the loss function.\nFor experiments with TRIO , we kept the value of λfixed\nas 0.3 throughout the experiments as preliminary exploration\nyielded optimal results. Dropout and Early stopping are used\nfor mitigating overfitting.\nB. Experimental Results\nTable I presents the evaluation scores of downstream\nnetworks trained on different SPTMs representations. We use\naccuracy and equal error rate (EER) as the evaluation metrics\nfollowing previous research on STSGS [13]. We report EER\nby computing the average scores using a one-vs-all approach.\nFor ASV , we report the average of five folds scores and\nfor CFAD, we report the scores obtained in the official\nevaluation set. Our findings indicate that representations from\nTRILLsson (paralinguistic SPTM) consistently achieve the\nhighest attribution accuracy and the lowest EER, significantly\noutperforming other representations. This reinforces their\n--- Page 4 ---\nPTMs ASV CFAD\nFCN CNN FCN CNN\nA↑EER↓A↑EER↓A↑EER↓A↑EER↓\nW2V 46.24 15.44 63.76 6.78 51.37 24.20 76.59 9.19\nWV 35.42 14.22 47.49 11.53 34.29 25.63 37.83 21.20\nUS 45.68 23.19 56.48 10.04 45.93 34.91 73.65 17.64\nXR 65.67 10.53 80.24 5.03 52.43 17.32 77.98 8.54\nWP 76.92 8.88 88.43 4.61 72.17 13.85 86.41 7.91\nMMS 82.21 7.90 89.29 4.17 73.54 13.51 89.78 6.95\nXV 89.46 5.43 96.63 2.13 76.85 11.63 92.33 4.47\nEP 85.17 4.87 93.79 4.04 74.29 10.45 88.61 4.93\nW2V-emo 72.59 9.65 82.29 6.32 70.53 12.96 85.39 12.51\nT 92.53 4.79 97.16 1.69 78.91 8.63 92.81 3.37\nTABLE I: Accuracy and EER in %; Abbreviations used:\nWav2vec2 (W2V), WavLM (WV), Unispeech (US), XLS-R\n(XR), Whisper (WP), MMS (MMS), x-vector (XV), ECAPA\n(EP), Wav2vec2-emo (W2V-emo), TRILLsson (T); The abr-\nreviations used here are kept same for Table II\nPairs ASV CFAD\nConcat TRIO Concat TRIO\nA↑EER↓A↑EER↓A↑EER↓A↑EER↓\nW2V + WV 94.31 7.69 95.97 7.62 88.79 4.89 94.28 4.61\nW2V + US 92.55 8.34 94.85 7.58 85.25 9.28 91.39 8.85\nW2V + XR 95.64 8.73 97.14 7.69 93.86 7.95 94.13 8.63\nW2V + WP 96.79 7.56 95.96 7.59 94.01 8.94 93.59 7.94\nW2V + MMS 94.60 7.62 96.28 6.28 89.97 8.55 93.54 8.51\nW2V + XV 96.21 7.36 95.17 7.39 86.54 7.49 89.73 7.37\nW2V + EP 93.08 6.59 95.25 7.14 86.21 9.54 92.62 7.56\nW2V + W2V-emo 92.85 6.48 96.64 6.59 88.67 8.58 92.47 7.58\nW2V + T 97.50 5.86 98.21 5.08 95.85 6.08 96.23 5.89\nWV + US 86.79 6.22 88.57 4.93 78.61 9.05 89.28 8.19\nWV + XR 85.91 5.30 87.32 4.78 91.59 8.79 91.68 7.54\nWV + WP 93.46 6.76 95.35 5.04 93.66 8.89 95.73 7.71\nWV + MMS 90.31 6.40 92.39 4.97 90.55 8.01 93.23 8.59\nWV + XV 94.89 5.49 94.72 4.30 90.27 8.81 93.79 8.69\nWV + EP 93.84 5.06 94.29 5.87 93.21 9.81 93.85 7.29\nWV + W2V-emo 88.69 4.99 93.51 4.29 94.67 8.63 94.89 7.63\nWV + T 95.81 4.55 95.16 4.33 95.29 7.86 95.21 7.21\nUS + XR 89.28 5.36 84.61 5.23 79.20 8.11 84.62 7.06\nUS + WP 91.59 6.04 93.82 5.27 81.82 9.24 85.06 7.29\nUS + MMS 90.55 5.22 92.38 4.51 89.63 7.99 91.50 5.72\nUS + XV 92.29 5.54 97.63 4.69 88.26 8.14 92.72 5.85\nUS + EP 91.97 5.66 94.27 4.76 87.72 8.39 93.50 6.53\nUS + W2V-emo 92.32 5.49 94.93 4.47 90.28 8.06 92.85 6.25\nUS + T 93.52 4.92 95.25 4.22 91.63 7.02 94.23 4.86\nXR + WP 94.81 5.06 95.53 4.11 90.62 5.49 95.36 5.31\nXR + MMS 94.59 5.72 95.83 5.14 92.36 6.52 94.82 5.17\nXR + XV 94.27 4.95 95.37 5.35 90.89 5.30 93.81 5.52\nXR + EP 93.51 4.92 94.43 4.26 91.76 5.87 92.29 4.49\nXR + W2V-emo 93.84 5.29 94.11 4.64 93.83 5.19 94.08 4.21\nXR + T 94.62 4.38 96.89 4.05 94.05 4.84 95.13 3.91\nWP + MMS 93.59 4.93 94.44 4.48 92.52 6.29 92.89 4.96\nWP + XV 95.13 5.31 96.01 4.29 95.11 5.19 97.16 4.14\nWP + EP 93.81 4.89 94.06 4.21 93.66 4.81 92.28 4.23\nWP + W2V-emo 94.26 4.73 95.24 4.02 92.98 4.51 94.08 3.85\nWP + T 94.89 3.95 95.31 3.24 95.21 4.09 97.52 3.09\nMMS + XV 96.89 3.84 97.51 3.53 93.53 5.27 94.48 4.61\nMMS + EP 95.67 3.18 96.11 2.96 91.13 3.59 92.34 3.38\nMMS + W2V-emo 96.91 3.08 97.86 2.93 92.89 4.19 93.82 3.91\nMMS + T 97.17 2.84 98.21 2.72 93.86 3.53 94.28 3.01\nXV + EP 97.16 4.24 98.04 4.15 94.22 4.21 95.68 4.03\nXV + W2V-emo 97.25 4.31 98.14 4.01 95.14 4.63 96.55 4.14\nXV + T 98.38 0.36 99.56 0.19 97.28 1.29 99.04 0.95\nEP + W2V-emo 87.61 8.53 89.93 7.21 76.28 10.64 79.94 8.28\nEP + T 92.28 2.10 94.81 1.83 79.24 7.61 82.38 5.53\nW2V-emo + T 97.39 0.45 97.56 0.39 96.38 1.49 97.16 0.99\nTABLE II: Accuracy and EER in %\n(a) MMS\n (b) TRILLsson\nFig. 2: t-SNE Plots for CFAD\nFig. 3: Confusion Matrix for CFAD using TRIO\n(x-vector + TRILLsson)\nability to capture source-specific paralinguistic cues, which\nare crucial for distinguishing synthetic speech sources. This\nvalidates our hypothesis that paralinguistic SPTM represen-\ntations will be the most effective for STSGS attributing to\ntheir paralinguistic pre-training. Among all the other SPTMs,\nspeaker recognition SPTMs (x-vector and ECAPA) showed\ncomparatively good performance. This suggests that their\npre-training for speaker recognition tasks enhances their\nability to capture source-specific cues, contributing to im-\nproved performance in STSGS. Additionally, we observe that\nmonolingual SPTMs reports the lowest performance for both\nthe datasets showing its inability to capture source specific\ncues. Overall, the CNN models showed better performance\nthan its FCN counterparts. We also plot the t-SNE plots\nof raw representations of MMS and TRILLsson in Figure\n2. We observe better cluster across the source classes for\nTRILLsson and this supports our obtained results and further\namplify the credibility of the proposed hypothesis.\nTable II presents the results of fusion of various SPTMs\nrepresentations. We use concatention-based fusion as the\nbaseline fusion technique. We keep the same network for\nconcatenation-based fusion technique as the proposed frame-\nwork, TRIO . However, we remove the gated mechansim,\nCCA loss and self-attention refinement block. We keep the\ntraining details as same as for experiments with TRIO . Our\nresults indicate that fusion of representations through TRIO\noutperforms the baseline fusion technique, demonstrating its\neffectiveness in integrating diverse SPTM representations.\nNotably, the best performance across both the datasets was\nachieved by fusing x-vector and TRILLsson representations\n--- Page 5 ---\nusing TRIO , highlighting the complementary nature of these\nrepresentations. Further, we observe that fusion of TRILLs-\nson with speaker recognition and multilingual SPTMs shows\ncomparatively good performance than fusion of monolin-\ngual SPTMs with each other. Overall, fusion of SPTMs\nrepresentations improved performance than the performance\nwith individual representations. We also plot the confusion\nmatrices of CNN trained with TRIO with fusion of x-vector\nand TRILLsson in Figure 3.\nComparison with SOTA : We compare our best performing\nmodel TRIO with x-vector and TRILLsson with previous\nSOTA work [13]. They reported accuracy and EER of 98.91%\nand 0.26% on ASV , while for CFAD, they reported 99.01%\nand 1.07%. While we report accuracy and EER: 99.56%\nand 0.19% on ASV , 99.04% and 0.95% on CFAD. This top\nperformance shows that our work sets the new SOTA for\nSTSGS.\nV. C ONCLUSION\nIn our study, we show the effectiveness of utilizing par-\nalinguistic SPTMs representations for STSGS. By capturing\nsource-specific paralinguistic cues, these representations out-\nperform representations from various other SOTA SPTMs.\nFurther, we propose TRIO , a novel framework for fusion of\nrepresentations. By integrating TRILLsson and x-vector rep-\nresentations through TRIO , we show topmost performance\nsurpassing individual SPTMs representations and baseline\nfusion methods as well as report SOTA results in STSGS\ncompared to previous SOTA work. Our findings serve as a\nvaluable reference for future studies in selecting appropriate\nSPTMs representations for STSGS and highlight the potential\nof combining SPTMs representations for further enhancing\nSTSGS.\nREFERENCES\n[1] Z. Wu, T. Kinnunen, N. Evans, J. Yamagishi, C. Hanilc ¸i et al. ,\n“ASVspoof 2015: The first automatic speaker verification spoofing and\ncountermeasures challenge,” in Proc. of INTERSPEECH , 2015.\n[2] M. Todisco, X. Wang, V . Vestman, M. Sahidullah, and K. Lee,\n“ASVspoof 2019: Future horizons in spoofed and fake audio detec-\ntion,” in Proc. of INTERSPEECH , 2019.\n[3] J.-w. Jung, H.-S. Heo, H. Tak, H.-j. Shim, J. S. Chung, B.-J. Lee, H.-\nJ. Yu, and N. Evans, “Aasist: Audio anti-spoofing using integrated\nspectro-temporal graph attention networks,” in ICASSP 2022-2022\nIEEE international conference on acoustics, speech and signal pro-\ncessing (ICASSP) . IEEE, 2022, pp. 6367–6371.\n[4] J. M. Mart ´ın-Do ˜nas and A. ´Alvarez, “The vicomtech audio deepfake\ndetection system based on wav2vec2 for the 2022 add challenge,”\ninICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2022, pp. 9241–\n9245.\n[5] P. Kawa, M. Plata, M. Czuba, P. Szyma ´nski, and P. Syga, “Improved\ndeepfake detection using whisper features,” in Interspeech 2023 , 2023,\npp. 4009–4013.\n[6] Y . Guo, H. Huang, X. Chen, H. Zhao, and Y . Wang, “Audio deep-\nfake detection with self-supervised wavlm and multi-fusion attentive\nclassifier,” in ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp.\n12 702–12 706.\n[7] X. Yan, J. Yi, J. Tao, C. Wang, H. Ma, T. Wang, S. Wang, and\nR. Fu, “An initial investigation for detecting vocoder fingerprints of\nfake audio,” in Proc. of the 1st International Workshop on Deepfake\nDetection for Audio Multimedia , 2022.[8] C. Y . Zhang, J. Yi, J. Tao, C. Wang, and X. Yan, “Distinguishing\nneural speech synthesis models through fingerprints in speech\nwaveforms,” ArXiv , vol. abs/2309.06780, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:261705832\n[9] T. Zhu, X. Wang, X. Qin, and M. Li, “Source tracing: Detecting voice\nspoofing,” in Proc. Asia-Pacific Signal and Information Processing\nAssociation Annual Summit and Conference (APSIPA ASC) , 2022.\n[10] N. Klein, T. Chen, H. Tak, R. Casal, and E. Khoury, “Source tracing of\naudio deepfake systems,” in Interspeech 2024 , 2024, pp. 1100–1104.\n[11] X. Yan, J. Yi, J. Tao, and J. Chen, “Audio deepfake attribution:\nAn initial dataset and investigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2208.10489\n[12] K. Bhagtani, A. K. S. Yadav, P. Bestagini, and E. J. Delp, “Attribution\nof diffusion based deepfake speech generators,” in 2024 IEEE Interna-\ntional Workshop on Information Forensics and Security (WIFS) , 2024,\npp. 1–6.\n[13] O. C. Phukan, D. Singh, S. R. Behera, A. B. Buduru, and R. Sharma,\n“Investigating prosodic signatures via speech pre-trained models for\naudio deepfake source attribution,” arXiv preprint arXiv:2412.17796 ,\n2024.\n[14] J. Shor and S. Venugopalan, “Trillsson: Distilled universal paralinguis-\ntic speech representations,” in Interspeech 2022 , 2022, pp. 356–360.\n[15] O. Chetia Phukan, G. Kashyap, A. B. Buduru, and R. Sharma,\n“Heterogeneity over homogeneity: Investigating multilingual speech\npre-trained models for detecting audio deepfake,” in Findings of the\nAssociation for Computational Linguistics: NAACL 2024 , K. Duh,\nH. Gomez, and S. Bethard, Eds. Mexico City, Mexico: Association\nfor Computational Linguistics, Jun. 2024, pp. 2496–2506. [Online].\nAvailable: https://aclanthology.org/2024.findings-naacl.160\n[16] Y . Wu, P. Yue, C. Cheng, and T. Li, “Investigation of ensemble of\nself-supervised models for speech emotion recognition,” in 2023 Asia\nPacific Signal and Information Processing Association Annual Summit\nand Conference (APSIPA ASC) . IEEE, 2023, pp. 988–995.\n[17] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representations,”\nAdvances in neural information processing systems , vol. 33, pp.\n12 449–12 460, 2020.\n[18] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li, N. Kanda,\nT. Yoshioka, X. Xiao et al. , “Wavlm: Large-scale self-supervised pre-\ntraining for full stack speech processing,” IEEE Journal of Selected\nTopics in Signal Processing , vol. 16, no. 6, pp. 1505–1518, 2022.\n[19] S. Chen, Y . Wu, C. Wang, Z. Chen, Z. Chen, S. Liu, J. Wu, Y . Qian,\nF. Wei, J. Li et al. , “Unispeech-sat: Universal speech representation\nlearning with speaker aware pre-training,” in ICASSP 2022-2022 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2022, pp. 6152–6156.\n[20] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh,\nP. von Platen, Y . Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli,\n“Xls-r: Self-supervised cross-lingual speech representation learning at\nscale,” in Interspeech 2022 , 2022, pp. 2278–2282.\n[21] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, “Robust speech recognition via large-scale weak supervi-\nsion,” in International conference on machine learning . PMLR, 2023,\npp. 28 492–28 518.\n[22] V . Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu,\nA. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al. , “Scaling speech\ntechnology to 1,000+ languages,” Journal of Machine Learning Re-\nsearch , vol. 25, no. 97, pp. 1–52, 2024.\n[23] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur,\n“X-vectors: Robust dnn embeddings for speaker recognition,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP) , 2018, pp. 5329–5333.\n[24] B. Desplanques, J. Thienpondt, and K. Demuynck, “Ecapa-tdnn:\nEmphasized channel attention, propagation and aggregation in tdnn\nbased speaker verification,” 2020.\n[25] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch, N. Evans,\nM. Sahidullah, V . Vestman, T. Kinnunen, K. A. Lee et al. , “Asvspoof\n2019: A large-scale public database of synthesized, converted and\nreplayed speech,” Computer Speech & Language , vol. 64, p. 101114,\n2020.\n[26] H. Ma, J. Yi, C. Wang, X. Yan, J. Tao, T. Wang, S. Wang, and\nR. Fu, “Cfad: A chinese dataset for fake audio detection,” Speech\nCommunication , vol. 164, p. 103122, 2024.",
  "text_length": 28961
}