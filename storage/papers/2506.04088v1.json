{
  "id": "http://arxiv.org/abs/2506.04088v1",
  "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
  "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.",
  "authors": [
    "Jun-Peng Jiang",
    "Yu Xia",
    "Hai-Long Sun",
    "Shiyin Lu",
    "Qing-Guo Chen",
    "Weihua Luo",
    "Kaifu Zhang",
    "De-Chuan Zhan",
    "Han-Jia Ye"
  ],
  "published": "2025-06-04T15:46:30Z",
  "updated": "2025-06-04T15:46:30Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04088v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04088v1  [cs.LG]  4 Jun 2025Multimodal Tabular Reasoning with Privileged\nStructured Information\nJun-Peng Jiang1,2,3∗Yu Xia3Hai-Long Sun1,2,3∗Shiyin Lu3Qing-Guo Chen3\nWeihua Luo3Kaifu Zhang3De-Chuan Zhan1,2Han-Jia Ye1,2†\n1School of Artificial Intelligence, Nanjing University, China\n2National Key Laboratory for Novel Software Technology, Nanjing University, China\n3AI Business, Alibaba Group.\nAbstract\nTabular reasoning involves multi-step information extraction and logical inference\nover tabular data. While recent advances have leveraged large language models\n(LLMs) for reasoning over structured tables, such high-quality textual representa-\ntions are often unavailable in real-world settings, where tables typically appear as\nimages. In this paper, we tackle the task of tabular reasoning from table images,\nleveraging privileged structured information available during training to enhance\nmultimodal large language models (MLLMs). The key challenges lie in the com-\nplexity of accurately aligning structured information with visual representations,\nand in effectively transferring structured reasoning skills to MLLMs despite the in-\nput modality gap. To address these, we introduce TabUlar Reasoning with Bridged\ninfOrmation ( TURBO ), a new framework for multimodal tabular reasoning with\nprivileged structured tables. TURBO benefits from a structure-aware reasoning trace\ngenerator based on DeepSeek-R1, contributing to high-quality modality-bridged\ndata. On this basis, TURBO repeatedly generates and selects the advantageous rea-\nsoning paths, further enhancing the model’s tabular reasoning ability. Experimental\nresults demonstrate that, with limited ( 9k) data, TURBO achieves state-of-the-art\nperformance ( +7.2%vs. previous SOTA) across multiple datasets.\n1 Introduction\nTabular understanding focuses on the extraction of information from various forms of tables, such as\nstructured tables and table images [ 15,29,60,69]. Going a step further, tabular reasoning [ 85,71,83]\ninvolves multi-step extraction, integration, and logical inference over tables to derive the final answer,\ndemonstrating its potential in fields such as finance [ 89], healthcare [ 59], and scientific research [ 23].\nWith the advancement of large language models (LLMs) [ 4,45,7,17], particularly their increasing\nreasoning capabilities [ 44,51,20], performing tabular reasoning tasks becomes feasible. Current\nefforts primarily focus on structured tables, where the input is typically the table’s text or structured\nrepresentations such as Markdown [ 79,71,83]. In contrast, in real-world scenarios, we can only have\naccess to table images or screenshots rather than clean, structured tables [ 46,84,37]. Given the gap\nbetween training inputs and deployment conditions, a natural question arises: can structured tables be\nleveraged as privileged information [68, 67] to improve the reasoning capabilities of MLLMs?\nStructured tables inherently contain rich information, allowing for fast and precise extraction of the\ncontent in each row and column [ 25,70]. However, they often struggle to capture the full structural\ncontext, particularly in cases where the layout and relationships between rows and columns are\n∗Work done during an internship at AI Business, Alibaba Group.\n†Corresponding author, email: yehj@lamda.nju.edu.cn.\nPreprint. Under review.\n--- Page 2 ---\ncomplex [ 37]. On the other hand, while table images provide clear visual cues, current MLLMs still\nface challenges in effectively extracting and integrating corresponding information, particularly for\ncomplex tasks that require multi-step understanding [31, 74].\nBeyond the gap between structured tables and images, while recent MLLMs have shown progress\nin domains such as mathematical reasoning [ 58,62], their performance on tabular reasoning tasks\nremains limited [ 61,86]. In our setting, the model is required to extract relevant content from table\nimages, integrate information based on row and column relationships, and then perform logical\ninference or numerical computation to arrive at the correct answer. This multi-step process places\nhigh demands on both perception and reasoning, exposing clear limitations in current models.\nTherefore, to enhance multimodal tabular reasoning capability with privileged structured tables, there\nare two main challenges. Given the complexity of structured tables, not all content is equally relevant\nto a given question. It is therefore crucial to identify and extract the most relevant information to\nguide model learning. Furthermore, once relevant content is identified, the next challenge is to\neffectively leverage it to improve the reasoning ability of MLLMs, enabling them to progressively\nconstruct accurate, multi-step reasoning paths.\nTo address the challenges, we propose TabUlar Reasoning with Bridged infOrmation ( TURBO ),\na framework that integrates structured table information during training to enhance multimodal\ntabular reasoning. Our approach leverages structured tables as bridged information, facilitating\nthe knowledge transfer from LLMs to MLLMs. Additionally, we employ reinforcement learning\ntechniques to further enhance the model’s reasoning capabilities, allowing it to progressively improve\nits performance on complex tabular reasoning tasks.\nSpecifically, we construct a new pipeline tailored for tabular reasoning. We start by feeding structured\nmarkdown tables and their corresponding QA pairs into DeepSeek-R1 [ 20], the SOTA reasoning\nLLM, to generate structure-aware chains of thought to bridge structured tables and images. This\nprocess yields high-quality reasoning data in the form of [question, reasoning, answer] triples, which\neffectively filter out redundant content and emphasize question-relevant reasoning traces. We then\nconduct supervised fine-tuning (SFT), equipping the MLLM with initial tabular reasoning capabilities.\nBuilding on this foundation, by iteratively sampling and selecting the relative advantages in reasoning\npaths as GRPO method [ 57], we further enhance the reasoning capability, progressively strengthening\nthe reasoning performance. Experimental results demonstrate that our TURBO achieves significant\nimprovements ( +7.2%over baselines) with limited data, showing high generalization ability and\ninterpretability. Our contributions are threefold:\n•We highlight the practical value of using structured tables as privileged information during training\nto enhance reasoning over table images, as such structured tables are often unavailable at inference\ntime in real-world scenarios.\n•We propose a new framework that utilizes privileged structured information to bridge the modality\ngap and enhance the tabular reasoning capabilities of MLLMs, combining structure-aware reasoning\ntrace generation with iterative optimization.\n•Experimental results show that our method achieves substantial performance improvements over\ndatasets with limited training data. Furthermore, the resulting model demonstrates strong robustness\nand interpretability in its reasoning process.\n2 Related Work\n2.1 Tabular Reasoning\nTabular data is of great importance due to its broad applicability in real-world domains, encompassing\na variety of learning tasks from standard classification and regression tasks [ 76,78,27] to open-world\nchallenges [ 28,5,26,36,77]. Besides, tabular reasoning is a part of tabular understanding, which\ninvolves comprehending the information contained within a table and can be broken down into several\ntasks, such as Table Structure Recognition (TSR) [ 55,54], Table Detection (TD) [ 18,33], and Table\nQuestion Answering (TQA) [ 8,65,29]. Traditional methods, whether OCR-based [ 2,12,19] or\nOCR-free [ 42,30,16,69,84], have made significant strides in recognizing the structure and content\nof tables. However, we focus on more challenging tabular reasoning tasks.\nTabular reasoning involves multi-step information extraction and logical inference over tabular data.\nIn the field of tabular reasoning, several recent works have made notable progress. OPENRT [ 85] is\n2\n--- Page 3 ---\na open-source framework designed for reasoning over tabular data. It enables the reproduction of\nexisting table pre-training models for fair performance comparison and supports rapid development\nof new models. [ 14] explores different table representations and directly prompts LLMs and MLLMs,\ninvestigating how structural formats influence reasoning capabilities. Chain-of-Table [ 71] employs\nin-context learning to iteratively generate operations and update the table, allowing LLMs to construct\nreasoning chains where each step builds upon the previous output. HIPPO [ 37] represents tables using\nboth textual and visual modalities, and optimizes MLLMs to learn richer and more comprehensive\ninformation with DPO [ 53]. However, these works primarily focus on reasoning over structured\ntables and achieve only limited performance. Since structured tables are rarely available in real-world\nsettings, reasoning over table images becomes a more practical choice.\n2.2 MLLMs for Reasoning\nThe field of multimodal large language models (MLLMs) has advanced significantly, particularly in\nintegrating visual and textual processing. Modern MLLMs combine visual encoders [ 52,64,81,24,\n63], LLMs, and fusion modules. Models like BLIP-2 [ 32] and InstructBLIP [ 13] use a Q-Former\nto bridge vision encoders and frozen LLMs. MiniGPT-4 [ 87] combines Q-Former with a linear\nprojector for better alignment between vision and LLMs. LLaV A [ 35] employs an MLP projector to\nimprove vision-LLM alignment. Alongside these open-source advancements, proprietary models like\nGPT-4o/o1 [ 43,1], Gemini2.5pro [ 66], and Qwen2.5-VL-Plus/MAX [ 3] have excelled in benchmarks\nand real-world applications.\nRecent developments in MLLMs have led to significant improvements in reasoning tasks involving\nboth textual and multimodal data [ 44,20,50,62,49]. Most current approaches rely on Chain-\nof-Thought (CoT) techniques [ 72] to train MLLMs for sequential reasoning. Notable data-driven\ninitiatives include Math-LLaV A [ 58], which introduced the MathV360K dataset, and MAmmoTH-\nVL [ 21], which builds large multimodal CoT datasets at scale. Another research direction focuses\non improving vision-text alignment. MA VIS [ 82] fine-tunes a math-specific vision encoder with a\ncurated set of captions, while Math-PUMA [ 90] aligns modalities by utilizing the KL divergence\nin next-token prediction distributions. In our paper, we focus on reasoning over table images,\nwhere structural information is not explicitly available and must be inferred through visual cues and\nmulti-step reasoning to reach the correct answer.\n3 Preliminaries\nIn this section, we first introduce the notation of multimodal tabular reasoning with privileged\nstructured information task, followed by some preliminary experiments and analyses.\n3.1 Multimodal Tabular Reasoning with Privileged Structured Information\nTabular reasoning refers to the task of answering questions that require understanding and logical\ninference over tabular data. This often involves multi-step reasoning, including locating relevant cells,\nintegrating information across rows and columns, and performing operations such as comparison,\naggregation, or arithmetic.\nIn real-world applications, high-quality structured tables T, such as markdown tables, are often\ndifficult to obtain. Instead, what is more readily available are table images V, such as screenshots or\nscanned documents. These image-based tables present significant challenges for reasoning, as the\nmodel must perceive both the content and the visual layout before performing any logical inference.\nTo address this, our goal is to enhance multimodal large language models Mto generate a response\nXafor the tabular reasoning question Qover table images Vby leveraging structured tables T\nas privileged information during training. While structured tables are not accessible at inference\ntime, they offer rich semantics and precise structure that can guide the model during learning. By\ncombining visual table inputs with structured supervision, we aim to improve the MLLM’s ability to\nperform accurate and interpretable reasoning over table images. Our goal is for the final answer, Xa,\nto be as consistent as possible with the correct answer Ya.\nmin\nMNX\ni=1ℓ(M(V, Q|T), Ya). (1)\n3\n--- Page 4 ---\n1.000.750.500.250.00TABMWPQwen2.5-VLInternVL2.5Ovis2ACC\nWTQw/o STw/o STw/o STFigure 1: Impact of structured tables (ST).\nWe evaluate the effect of providing struc-\ntured tables or not as input across Qwen2.5-\nVL [ 3], InternVL2.5 [ 10], and Ovis2 [ 39] on the\nTABMWP [ 38] and WTQ [ 48] datasets. The re-\nsults show a consistent performance drop when\nstructured tables are removed, highlighting their\nimportance for effective tabular reasoning.\nQuestion: A business magazine surveyed its readers about their commute times. How many commutes are less than 51 minutes?Qwen2.5-VL: From stem 3: 2 commute times (30, 38)  From stem 4: 2 commute times (41, 44) From stem 5: 1 commute time (50) Total: (2 + 2 + 1 = 5). Final Answer: 5Ovis2:Based on the table, the final answer is5.InternVL-2.5:Count all the leaves in the rows with stems 3, 4 and 5.\\n\\nYoucounted 5 leaves less than 51 minutes.\nStemLeaf34560 8 1 4 40 1 2 7 725799Commute time(minutes):Figure 2: Challenge of current MLLMs on tabu-\nlar reasoning. This example illustrates the limita-\ntions of current MLLMs in performing multi-step\nreasoning and numerical computation. The task\nrequires sequential reasoning based on the image,\nbut existing models often lack such capabilities or\nmake errors during intermediate steps, ultimately\nleading to incorrect answers.\nℓis the loss function that measures the discrepancy between prediction and ground-truth. “ |” indicates\nthat the answer is generated conditioned on the corresponding structured table T. Our objective is to\nenhance the M’s reasoning capability with the privileged structured information. In situations where\nstructured tables is not available in real-world scenarios, we expect Mcan still provide accurate\nanswers given only the table image V. Therefore, during the inference phase, we measure the\nperformance of Mbased on its prediction accuracy given any test image.\n3.2 Analysis on MLLMs in Tabular Reasoning\nTo conduct our preliminary study, we select several of the most advanced and widely adopted open-\nsource MLLMs, including Qwen2.5-VL [ 3], InternVL2.5 [ 10], and Ovis2 [ 39], and evaluate them on\nTABMWP [ 38] and WikiTableQuestions (WTQ) [ 48]. We randomly sample 100 examples from each\ndataset for testing. Since tabular reasoning tasks involve open-ended question answering, it is often\nchallenging to directly judge the correctness of model outputs. We employ Qwen2.5-72B-Instruct [ 73]\nas an external evaluator. Specifically, we provide the LLM with the question, ground-truth answer,\nand the model’s prediction. By converting the evaluation into a single-modality setting, this approach\nreduces ambiguity and improves the reliability of the evaluation, offering a more accurate assessment\nof MLLMs’ reasoning capabilities.\nTo investigate the role of structured tables in tabular reasoning, we conduct preliminary experiments\nusing MLLMs. Specifically, we compare the model’s performance when reasoning is performed with\naccess to structured tables versus relying solely on table images during inference.\nStructured Information Matters in Tabular Reasoning. Figure 1 presents a comparison between\ninference results with and without access to structured tables. We observe a significant drop in\nperformance across all models when structured tables are not provided. This highlights the difficulty\nMLLMs face when relying solely on table images, as they must implicitly recover the table’s structure\nand relevant relationships before performing reasoning. In contrast, structured tables offer clean and\nexplicit content, which greatly facilitates accurate information extraction. These results underscore\nthe importance of leveraging structured representations to guide model reasoning, even if they are\nonly available during training.\nMLLMs Lack Explicit Reasoning Traces. Both WTQ and TABMWP contain complex, reasoning-\nintensive questions that require multi-step inference and arithmetic operations to reach the correct\nanswer, as in the case in Figure 2. However, our analysis reveals that current MLLMs often fall\nshort in these scenarios. Despite having access to table images, these models tend to produce final\nanswers directly, frequently bypassing any clear intermediate reasoning steps. In the absence of\nexplicit prompting for chain-of-thought generation, their outputs rarely exhibit clear reasoning traces.\nThis lack of step-by-step thinking not only undermines interpretability but also frequently results in\nincorrect predictions. Furthermore, even Qwen shows potential in reasoning traces, there are critical\nintermediate mistakes, leading to the final wrong answer. These observations highlight the limitations\nof current MLLMs in tabular reasoning and underscore the need for methods that can explicitly\nleverage structured reasoning capability and align it with visual representations.\n4\n--- Page 5 ---\nQuestion: What is the total numberof wins listedfor the United States?Answer: 22\nReasoningTrace: First,let’s list out the golfers from the United States. Looking at the\"Country\" column:-Tiger Woods: United States, 18 wins.-Hunter Mahan: United States, 2 wins.-Phil Mickelson: United States, 2 wins.Let me calculate that. 18 plus 2 is 20, plus another 2 is 22.Let me check again: …So the final answeris 22. \nStructuredTable\nTableImage\nSFTthink 1, answer 1think 2, answer 2think n, answer n…\nreward \n…relative advantage\ngeneratea group of responsesData Generation Pipeline\nTraining Flow\nreject samplingFigure 3: Data generation pipline and training flow of our TURBO framework. We first input the\nstructured table, question, and answer into DeepSeek to generate a reasoning trace, and apply reject\nsampling to improve data quality. Then, supervised fine-tuning (SFT) is performed on the collected\ndata for privileged information alignment. Finally, we generate a group of answers for each question\nand compute their relative advantages based on their reward to further reinforce the reasoning ability.\n4 Multimodal Tabular Reasoning with Bridged Information\nIn this section, we introduce our TURBO framework (as shown in Figure 3) of multimodal tabular\nreasoning with privileged structured information through a structure-aware reasoning trace generator.\nThen we sample and select the advantageous reasoning paths, further enhancing the reasoning ability.\n4.1 Bridging Structured Tables and Images\nA central challenge in multimodal tabular reasoning arises from the intrinsic modality gap between\nstructured tables and table images. These two modalities, though semantically aligned, are processed\nvery differently by MLLMs, leading to differences in their reasoning behavior.\nStructured tables, such as those in Markdown or HTML, explicitly encode row-column relationships,\nenabling accurate and fine-grained access to cell contents [ 6,48]. This makes them particularly\nsuitable for tasks that require semantic-level reasoning. However, they often fail to capture complex\nlayout nuances such as merged cells, multi-level headers, or visual hierarchies, which are common\nin real-world tables [ 14,37]. In contrast, table images naturally preserve these structural cues\nthrough visual patterns like borders, spacing, and alignment. Yet, due to OCR errors, visual noise,\nor resolution limits, accurately extracting textual content from table images remains a significant\nchallenge [ 40]. This inherent modality gap highlights the complementary nature of structured tables\nand table images—each captures essential but distinct aspects of tabular information.\nUnified Reasoning across Modalities. However, despite their representational differences, both\nstructured tables and table images ultimately convey the same underlying information relevant to\nreasoning tasks. From a reasoning perspective, the inference trajectory— i.e., the logical path leading\nfrom question to answer—would remain consistent across both modalities. A correct reasoning chain,\nonce established, can guide models to identify the relevant content and apply appropriate reasoning\nsteps, regardless of whether the input is a structured table or its visual counterpart. This highlights\nthat the essence of tabular reasoning lies not in the input format but in the ability to extract and\ncompose the necessary information into a coherent, accurate reasoning process.\n5\n--- Page 6 ---\nTherefore, it is natural to leverage the strong reasoning capabilities of current large language models\nto extract high-quality reasoning chains for tabular data. These structured chains of thought are\nmodality-independent and can serve as valuable supervision signals for improving multimodal tabular\nreasoning. To generate structure-aware reasoning traces, we leverage the available structured tables\nduring training. Specifically, for each markdown-formatted table Tand its associated question-\nanswer pair (Q, A), we input (T, Q, A )into DeepSeek-R1 [ 20], a state-of-the-art reasoning LLM.\nBy providing the ground-truth answer Aalongside the question and table, we guide the model to\nproduce a coherent and accurate reasoning process R={r1, r2, ..., r n}that logically connects the\nquestion to the correct answer.\nReject Sampling over Reasoning. This approach produces high-quality supervision triplets\n(Q, R, A ), where the reasoning trace Rcaptures the key logical steps, relevant table elements,\nand necessary operations to arrive at the correct answer. To further improve data quality, we apply\nadditional reject sampling: overly verbose or contradictory reasoning traces—often caused by in-\nconsistencies between DeepSeek’s generated answer and the provided ground-truth answer—are\nremoved. We also employ an auxiliary strong LLM ( e.g., Qwen2.5-72B-Instruct) to assess the\ncoherence and correctness of each trace, ensuring that only faithful and focused reasoning chains\nare retained. After reject sampling, we obtain approximately 9k high-quality examples that serve as\nreliable supervision for enhancing multimodal tabular reasoning.\nIn summary, we introduce the bridged information that successfully aligns structured tables with table\nimages through reasoning chains. Our structure-aware reasoning trace generator provides high-quality\nreasoning traces, facilitating the transfer of reasoning from structured inputs to images.\n4.2 Enhancing MLLMs with Reasoning Capability\nPreliminary Exploration of Reasoning Capability. The most straightforward way to enhance\nMLLMs with reasoning ability is to use supervised fine-tuning (SFT) on our high-quality reasoning\ndata. We construct the following system prompt to guide the model’s learning process: You are\na helpful assistant. For each question, first think through your reasoning,\nthen provide an answer. Format your response as: <think>Your reasoning\nprocess</think><answer>Your final answer</answer> . Each training ground truth is\nformatted as follows: <think> R</think><answer> A</answer> . Through SFT, this process\nenables MLLMs to learn the “reasoning-first, answer-later” paradigm. Moreover, since the reasoning\ntraces are modality-invariant, the reasoning ability can be preliminarily transferred from LLMs to\nMLLMs across modalities.\nReinforcing Multimodal Reasoning Capability in MLLMs. Inspired by [ 20], we adopt Group\nRelative Policy Optimization (GRPO) as the reinforcement learning algorithm to further enhance the\nreasoning capability. Unlike SFT, which minimizes token-level prediction errors, GRPO leverages\npolicy gradients derived from reward signals. This enables the model to explore a broader range of\npossible solutions, encouraging more diverse and complex reasoning behaviors [20, 34].\nSpecifically, we define questions as Qand the current policy as πθold. For each question q∈Q, a\ngroup of responses {o1, o2, ..., o G}is generated from πθold. A fixed reference policy πrefis also\nused to regularize training. The optimization objective for the updated policy πθis defined as:\nJ(θ) =Eq∼Q,{oi}G\ni=1∼πθold\"\n1\nGGX\ni=1min\u0012πθ(oi|q)\nπθold(oi|q)Ai,clip\u0012πθ(oi|q)\nπθold(oi|q),1−ϵ,1 +ϵ\u0013\nAi\u0013\n−βDKL(πθ∥πref)#\n(2)\nHere, ϵis the clipping threshold, and βis the weight for the KL divergence penalty. The advantage\nAifor each response oiis computed relative to the group as:\nAi=ri−mean ( {r1, r2, . . . , r G})\nstd ({r1, r2, . . . , r G}). (3)\nThis approach normalizes the reward across the group to obtain a relative advantage signal. The KL\ndivergence term is defined as:\nDKL(πθ∥πref) =πref(oi|q)\nπθ(oi|q)−log\u0012πref(oi|q)\nπθ(oi|q)\u0013\n−1. (4)\n6\n--- Page 7 ---\nBy leveraging relative advantage, the model is able to perform self-comparison across its own set of\ngenerated responses, identifying which responses or reasoning paths are relatively more effective.\nMeanwhile, the KL divergence term acts as a regularizer, ensuring that the updated policy does\nnot deviate excessively from the reference model in a single optimization step. This constraint\nhelps maintain training stability by preventing abrupt policy shifts, enabling the model to improve\nprogressively in a controlled and stable manner.\nUnlike standard PPO [ 56], GRPO removes the need for a critic network by estimating the relative\nadvantage within each sampled group. This not only simplifies implementation but also significantly\nreduces computational overhead. In our tabular reasoning setting, the reward function is composed of\ntwo parts: format reward and accuracy reward.\nThe format reward encourages the model to adhere to a structured reasoning format by using\n<think></think> and<answer></answer> tags in its response. This design promotes a two-\nstep process where the model first articulates a chain of reasoning based on its understanding of\nthe table, and then explicitly states the final answer derived from that reasoning. By rewarding\noutputs that follow this format, we guide the model to develop more interpretable and systematic\nreasoning behavior. The accuracy reward is rule-based and evaluates whether the content in the\n<answer></answer> tags matches the ground truth labels. This component directly incentivizes\ncorrectness in the final answer, ensuring that the reasoning process ultimately leads to accurate\noutcomes.\nIn summary, reward-guided GRPO efficiently enhances the model’s reasoning ability in tabular tasks\nby selecting superior responses within a group, enabling self-improvement without the overhead of\na value function. Through this two-stage learning framework, our model develops a strong tabular\nreasoning capability, effectively bridging structured and unstructured modalities while enabling\nrobust, interpretable reasoning across table images.\n5 Experiments\nIn this section, we first present the experimental setup, including datasets, baselines, and imple-\nmentation details. We then demonstrate the effectiveness of our approach through comprehensive\nexperiments and ablation studies. Finally, case studies further highlight the interpretability and\nrobustness of our method in complex tabular reasoning scenarios.\n5.1 Experimental Setup\nImplementation Details. We conduct all experiments based on the Ovis2 [ 39]. Our training pipeline\nconsists of two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). During the SFT\nstage, we train our model using 4 ×A100 GPUs for 1 hour with a batch size of 128 and a learning rate\nof 2e-6. This stage allows the model to learn structure-aware reasoning patterns from high-quality\n[question, reasoning, answer] triples. In the RL stage, we further enhance the model’s reasoning\nability by applying GRPO with 8 ×A100 GPUs for 24 hours. We use a smaller learning rate of 5e-7\nwith the same batch size of 128. For each question, the model generates 16 candidate responses,\nwhich are then used to compute relative advantages and guide the optimization process.\nEvaluation Benchmarks. Following HIPPO [ 37], we evaluate the tabular reasoning capability of\nour model on a diverse set of publicly available benchmarks. Specifically, for Tabular Question\nAnswering (TQA) tasks, we use five representative datasets: TABMWP [ 38], WTQ [ 48], HiTab [ 11],\nand TAT-QA [ 88]. We export FeTaQA [ 41] due to its evaluation metric being BLEU [ 47], which\ndoes not focus on the accuracy of question answering. For Table Fact Verification (TFV) tasks, we\ninclude TabFact [ 9] and InfoTabs [ 22]. To further assess the robustness of our approach, we conduct\nadditional experiments on MMMU [ 80], evaluating our model on all table-related questions within\nthis challenging multimodal benchmark. This allows us to verify whether the reasoning capabilities\nlearned through our framework generalize to unseen and diverse table images in complex real-world\nsettings. For TQA, we evaluate model performance using Accuracy (ACC), and in TFV , we use the\nbinary classification accuracy for TabFact (true/false outputs) and multi-class accuracy for InfoTabs\n(entail/contradict/neutral outputs). We employ Qwen2.5-72B-Instruct [ 73] as an external evaluator.\nSpecifically, we provide the LLM with the question, ground-truth answer, and the model’s prediction.\n7\n--- Page 8 ---\nTable 1: Accuracy comparison across tabular reasoning benchmarks. Our model, TURBO , consistently\nachieves the best overall performance, demonstrating strong reasoning ability and effective integration\nof privileged structured table information. Even when compared to the strongest baseline Qwen2.5-\nVL, T URBO achieves an average improvement of 7.2%.\nMethodQuestion Answering Fact VerificationMMMU AverageTABMWP WTQ HiTab TAT-QA TabFact InfoTabs\nInternVL-2.5 [10] 90.88 43.19 45.94 34.97 66.46 55.50 41.46 54.06\nQwen2.5-VL [3] 92.48 65.85 67.09 70.54 83.01 77.91 42.07 71.28\nMiniCPM-V-2.6 [75] 83.68 47.97 56.53 51.55 78.48 73.03 31.10 60.33\nHIPPO [37] 87.34 55.71 63.13 61.40 82.29 75.70 - -\nHIPPO w/o ST 85.83 49.10 57.23 56.22 80.20 72.74 35.98 62.47\nTable-LLaV A [86] 53.20 16.62 7.87 10.49 57.62 66.78 17.68 32.89\nTabPedia [84] 10.66 23.53 6.54 13.08 35.49 2.43 2.44 13.45\nOvis2 [39] 92.00 58.76 68.59 47.67 80.80 74.11 48.17 67.16\nOvis2-CoT 92.12 60.80 66.43 48.70 81.61 72.46 50.61 67.53\nTURBO 96.75 67.80 72.15 73.21 85.81 81.89 57.32 76.42\nComparison Methods. For comprehensive comparisons, we select leading open-source MLLMs\nand those in tabular understanding, including Qwen2.5-VL-7B [ 3], InternVL-2.5-8B [ 10], MiniCPM-\nV-2.6-8B [75], Ovis2-8B [39], Table-LLaV A-7B [86], TabPedia-7B [84], and HIPPO-8B [37].\n5.2 Main Results\nIn this experiment, we evaluate the table reasoning effectiveness of our TURBO and baseline models\non both the TQA, TFV tasks, and MMMU. For the HIPPO method, which leverages both structured\ntables and table images during training and inference, we report its performance under the original\nsetting with structured tables and table images as input. Additionally, we include HIPPO without\nstructured tables as “HIPPO w/o ST”, a variant that takes only table images as input, which better\naligns with our setting and serves as a fairer comparison for evaluating purely from table images.\nTo ensure a fair comparison with HIPPO, we follow the same data sampling strategy by randomly\nselecting 10k examples as our training set. We then apply our structure-aware reasoning trace\ngenerator to produce reasoning traces. To maintain high data quality, we further conduct reject\nsampling on incorrect or low-quality generations. As a result, we retain approximately 9k high-\nquality training samples for SFT and RL for 1 epoch. Results are in Table 1.\nAcross all datasets, our model TURBO achieves the best overall performance, demonstrating clear and\nconsistent improvements over existing baselines. Specifically, TURBO surpasses the strong tabular\nunderstanding baseline HIPPO up to a relative improvement of 21.7%over TQA datasets (in WTQ),\nand by 8.2%over TFV datasets (in InfoTabs). Notably, compared to the variant HIPPO “w/o ST”\n(which only uses table images and is thus more comparable to our real-world setting), on average,\nour method achieves a 26.8%improvement on TQA and 9.8%on TFV , highlighting the effectiveness\nof our reasoning supervision and privileged structured information.\nFurthermore, on the challenging MMMU benchmark, which evaluates table reasoning in real-world\nmultimodal scenarios, TURBO reaches 57.32, outperforming the previous best method Ovis2-CoT\nby13%, and surpassing HIPPO by 59.3%relative improvement, showcasing the robustness and\ngeneralization of our model under diverse and complex table formats.\nThese results validate that our two-stage learning framework not only equips MLLMs with strong\nreasoning capability, but also highlights its effectiveness in bridging the gap between structured tables\nand visual table inputs, enabling the model to absorb and utilize privileged structural information.\n5.3 Further Studies\nAblation Study. To better understand the contribution of each component in our two-stage framework,\nwe conducted an ablation study on our baseline model. As shown in Figure 4, we compare the\nfollowing variants: Baseline: The base model Ovis2 without reasoning supervision. “w/ SFT”:\nSupervised Fine-tuned with our structure-aware reasoning traces. “w/ RL”: Optimized with reward-\ndriven reinforcement learning stage. T URBO : The full model trained with both SFT and RL stages.\n8\n--- Page 9 ---\nQuestion: Maxwell Software, Inc., has the following mutually exclusive projects.Supposethe company uses the NPV rule to rank these two projects. Which project should be chosen if the appropriate discount rate is 15 percent?A: Project AB: Project BTurbo: <think>Okay, let's see. The problem is about choosing between Project A and Project B using the NPV rule with a 15% discount rate. First, I need to calculate the NPV for each project. Looking at the table, the initial investment for Project A is -$20,000,and for Project B, it's -$24,000. The cash flows for each year. The NPV formula is …  discounted by the discount rate (15% or 0.15).For Project A:\\nYear0: -$20,000 (initial investment)\\nYear1: $13,200 / (1+0.15)^1 = $13,200 / 1.15 ≈ $11,478.26… NPV_A = -$20,000 + $11,478.26 + $6,279.07 + $2,104.53 ≈ -$138.14For Project B:\\nYear0: -$24,000 (initial investment)\\nYear1: $14,100 / (1+0.15)^1 = $14,100 / 1.15 ≈ $12,260.87… NPV_B = -$24,000 + $12,260.87 + $7,401.71 + $4,997.45 ≈ $659.31Comparing the NPVs, Project B has a higher NPV than Project A. Therefore, Project B should be chosen.</think><answer>B</answer>Year      Project A      Project B0123-$20,00013,2008,3003,200-$24,00014,1009,8007,600Figure 5: Case study of Turbo. Our method accurately extracts relevant information from the\ntable image (highlighted in blue), formulates the appropriate calculation expressions (in pink), and\nperforms step-by-step reasoning to arrive at the correct final answer (in green). This demonstrates the\neffectiveness of T URBO in both visual grounding, mathematics, and logical reasoning.\n/uni00000037/uni00000024/uni00000025/uni00000030/uni0000003a/uni00000033 /uni0000003a/uni00000037/uni00000034 /uni0000002b/uni0000004c/uni00000037/uni00000044/uni00000045 /uni00000037/uni00000024/uni00000037/uni00000010/uni00000034/uni00000024 /uni00000037/uni00000044/uni00000045/uni00000029/uni00000044/uni00000046/uni00000057 /uni0000002c/uni00000051/uni00000049/uni00000052/uni00000037/uni00000044/uni00000045/uni00000056 /uni00000030/uni00000030/uni00000030/uni00000038/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000032/uni00000059/uni0000004c/uni00000056\n/uni0000005a/uni00000012/uni00000003/uni00000036/uni00000029/uni00000037\n/uni0000005a/uni00000012/uni00000003/uni00000035/uni0000002f\n/uni00000037/uni00000058/uni00000055/uni00000045/uni00000052\nFigure 4: Ablation study of components in TURBO\non each dataset. TURBO achieves consistent and\nsignificant improvements over the Ovis baseline\non all datasets, demonstrating the effectiveness of\neach introduced component.From the results, we observe consistent improve-\nments across all evaluation tasks. Compared to\nthe baseline, adding SFT alone brings 10.1%\nrelative improvement on average, demonstrating\nthat structure-aware reasoning traces help the\nmodel better understand table semantics. The\nRL stage also introduces 11.8%performance\ngains, particularly on complex QA datasets like\nTAT-QA, showing that relative advantage can\nrefine reasoning fidelity and answer consistency.\nFinally, the full TURBO model achieves the best\nperformance across the board, 13.8%on aver-\nage. This confirms that both stages in our frame-\nwork are complementary and jointly crucial for\nmaximizing reasoning ability.\nCase Studies. To further demonstrate the effec-\ntiveness of our approach, we present a case in\nFigure 5 that showcases the reasoning capability of our TURBO . In this example, the task involves\ninterpreting a table image and answering a complex question that requires multiple-step reasoning,\nincluding visual grounding, mathematics, and logical reasoning. More cases and experiment results\ncan be found in Appendix B.\nIn the baseline model, answers are generated directly without an interpretable reasoning process.\nAfter the SFT stage, the model begins to exhibit basic reasoning behaviors but still makes shortcuts,\nsuch as incorrect calculations. With RL tuning, TURBO chooses advantage answers in a group of\nresponses, generates step-by-step reasoning, accurately locates key table elements, and produces\ninterpretable answers, demonstrating clear improvements in both reasoning depth and accuracy.\n6 Conclusion\nIn this paper, we propose TURBO , a new multimodal large language model tailored for real-world\ntabular reasoning tasks, where structured tables are used as privileged information. Our method\nleverages a two-stage training paradigm that integrates structured reasoning supervision into MLLMs.\nSpecifically, we first introduce a high-quality, structure-aware reasoning trace generation mechanism,\nenabling the model to internalize a \"think-then-answer\" paradigm during supervised fine-tuning. We\nthen further refine the reasoning capabilities via reinforcement learning, aligning the model’s outputs\nwith more accurate and interpretable reasoning behaviors. Extensive experiments show that TURBO\nconsistently achieves state-of-the-art performance among open-source MLLMs. We hope this work\ninspires future research into multimodal reasoning, especially in developing more systematic and\ninterpretable frameworks for real-world tasks.\n9\n--- Page 10 ---\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023.\n[2]Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. Doc-\nformer: End-to-end transformer for document understanding. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 993–1003, 2021.\n[3]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,\n2025.\n[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\n[5]Hao-Run Cai and Han-Jia Ye. Understanding the limits of deep tabular methods with temporal\nshift. arXiv preprint arXiv:2502.20260 , 2025.\n[6]Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike\nBurrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. Bigtable: A distributed storage\nsystem for structured data. ACM Transactions on Computer Systems (TOCS) , 26(2):1–26, 2008.\n[7]Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,\nXiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language\nmodels. ACM transactions on intelligent systems and technology , 15(3):1–45, 2024.\n[8]Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, and William W Cohen. Open\nquestion answering over tables and text. arXiv preprint arXiv:2010.10439 , 2020.\n[9]Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou\nZhou, and William Yang Wang. Tabfact: A large-scale dataset for table-based fact verification.\narXiv preprint arXiv:1909.02164 , 2019.\n[10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shen-\nglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source\nmultimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271 ,\n2024.\n[11] Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang\nLou, and Dongmei Zhang. Hitab: A hierarchical table dataset for question answering and\nnatural language generation. arXiv preprint arXiv:2108.06712 , 2021.\n[12] Cheng Da, Peng Wang, and Cong Yao. Multi-granularity prediction with learnable fusion for\nscene text recognition. arXiv preprint arXiv:2307.13244 , 2023.\n[13] Wenliang Dai, Junnan Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi.\nInstructblip: Towards general-purpose vision-language models with instruction tuning. arxiv\n2023. arXiv preprint arXiv:2305.06500 , 2, 2023.\n[14] Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and\nRada Mihalcea. Tables as texts or images: Evaluating the table reasoning ability of llms and\nmllms. In Findings of the Association for Computational Linguistics ACL 2024 , pages 407–426,\n2024.\n[15] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. Turl: Table understanding\nthrough representation learning. ACM Special Interest Group on Management of Data Record ,\n51(1):33–40, 2022.\n[16] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.\nUnidoc: A universal large multimodal model for simultaneous text detection, recognition,\nspotting and understanding. arXiv preprint arXiv:2308.11592 , 2023.\n[17] Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, and Xu Yang. Speculative ensemble:\nFast large language model ensemble via speculation. arXiv preprint arXiv:2502.01662 , 2025.\n[18] Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table detection using deep\nlearning. In 2017 14th IAPR international conference on document analysis and recognition ,\nvolume 1, pages 771–776. IEEE, 2017.\n[19] Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu, and Liqing\nZhang. Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document\nunderstanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4583–4592, 2022.\n[20] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n10\n--- Page 11 ---\n[21] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig,\nWenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction\ntuning at scale. arXiv preprint arXiv:2412.05237 , 2024.\n[22] Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. Infotabs: Inference on tables\nas semi-structured data. arXiv preprint arXiv:2005.06117 , 2020.\n[23] Vivek Gupta, Shuo Zhang, Alakananda Vempala, Yujie He, Temma Choji, and Vivek Srikumar.\nRight for the right reason: Evidence extraction for trustworthy tabular reasoning. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pages 3268–3283, 2022.\n[24] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on\npattern analysis and machine intelligence , 45(1):87–110, 2022.\n[25] Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Mar-\ntin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint\narXiv:2004.02349 , 2020.\n[26] Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin\nHoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with a\ntabular foundation model. Nature , 637(8045):319–326, 2025.\n[27] Jun-Peng Jiang, Si-Yang Liu, Hao-Run Cai, Qile Zhou, and Han-Jia Ye. Representation learning\nfor tabular data: A comprehensive survey. arXiv preprint arXiv:2504.16109 , 2025.\n[28] Jun-Peng Jiang, Han-Jia Ye, Leye Wang, Yang Yang, Yuan Jiang, and De-Chuan Zhan. Tabular\ninsights, visual impacts: transferring expertise from tables to images. In Forty-first International\nConference on Machine Learning , 2024.\n[29] Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question an-\nswering: recent advances. In China Conference on Knowledge Graph and Semantic Computing ,\npages 174–186. Springer, 2022.\n[30] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document understanding trans-\nformer without ocr. arXiv preprint arXiv:2111.15664 , 7(15):2, 2021.\n[31] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering\nbenchmark on multiple table domains. arXiv preprint arXiv:2404.19205 , 2024.\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning , pages 19730–19742. PMLR, 2023.\n[33] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank:\nTable benchmark for image-based table detection and recognition. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference , pages 1918–1925, 2020.\n[34] Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, and Kaipeng Zhang. Think or not think:\nA study of explicit thinking in rule-based visual reinforcement fine-tuning. arXiv preprint\narXiv:2503.16188 , 2025.\n[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems , 36, 2024.\n[36] Si-Yang Liu and Han-Jia Ye. Tabpfn unleashed: A scalable and effective solution to tabular\nclassification problems. arXiv preprint arXiv:2502.02527 , 2025.\n[37] Zhenghao Liu, Haolan Wang, Xinze Li, Qiushi Xiong, Xiaocui Yang, Yu Gu, Yukun Yan, Qi Shi,\nFangfang Li, Ge Yu, et al. Hippo: Enhancing the table understanding capability of large language\nmodels through hybrid-modal preference optimization. arXiv preprint arXiv:2502.17315 , 2025.\n[38] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. arXiv preprint arXiv:2209.14610 , 2022.\n[39] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye.\nOvis: Structural embedding alignment for multimodal large language model. arXiv preprint\narXiv:2405.20797 , 2024.\n[40] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa:\nVisual question answering by reading text in images. In 2019 international conference on\ndocument analysis and recognition , pages 947–952. IEEE, 2019.\n[41] Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech\nKry´sci´nski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, et al. Fetaqa: Free-form table\nquestion answering. Transactions of the Association for Computational Linguistics , 10:35–49,\n2022.\n11\n--- Page 12 ---\n[42] Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, and Peter Staar. Tableformer: Table\nstructure understanding with transformers. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 4614–4623, 2022.\n[43] OpenAI. Gpt-4o: Hello gpt-4o. Technical report, OpenAI, 2024.\n[44] OpenAI. Learning to reason with llms. Technical report, OpenAI, 2024.\n[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems ,\n35:27730–27744, 2022.\n[46] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig.\nTablenet: Deep learning model for end-to-end table detection and tabular data extraction\nfrom scanned document images. In 2019 international conference on document analysis and\nrecognition (ICDAR) , pages 128–133, 2019.\n[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics , pages 311–318, 2002.\n[48] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables.\narXiv preprint arXiv:1508.00305 , 2015.\n[49] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang,\nXingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning\nabilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536 , 2025.\n[50] QwenTeam. Qvq: To see the world with wisdom. Technical report, QwenTeam, Alibaba, 2024.\n[51] QwenTeam. Qwq: Reflect deeply on the boundaries of the unknown. Technical report,\nQwenTeam, Alibaba, 2024.\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning ,\npages 8748–8763. PMLR, 2021.\n[53] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems , 36:53728–53741, 2023.\n[54] Mahmoud Salaheldin Kasem, Abdelrahman Abdallah, Alexander Berendeyev, Ebrahem Elkady,\nMohamed Mahmoud, Mahmoud Abdalla, Mohamed Hamada, Sebastiano Vascon, Daniyar\nNurseitov, and Islam Taj-Eddin. Deep learning for table detection and structure recognition: A\nsurvey. ACM Computing Surveys , 56(12):1–41, 2024.\n[55] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt:\nDeep learning for detection and structure recognition of tables in document images. In 2017\n14th IAPR international conference on document analysis and recognition , volume 1, pages\n1162–1167. IEEE, 2017.\n[56] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n[57] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\n[58] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and\nRoy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large\nlanguage models. arXiv preprint arXiv:2406.17294 , 2024.\n[59] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho,\nCarl Yang, and May D Wang. Ehragent: Code empowers large language models for few-shot\ncomplex tabular reasoning on electronic health records. arXiv preprint arXiv:2401.07128 , 2024.\n[60] Alexey Shigarov. Table understanding: Problem overview. Wiley Interdisciplinary Reviews:\nData Mining and Knowledge Discovery , 13(1):e1482, 2023.\n[61] Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu,\nHaobo Wang, Haokai Xu, Hao Chen, et al. Tablegpt2: A large multimodal model with tabular\ndata integration. arXiv preprint arXiv:2411.02059 , 2024.\n[62] Hai-Long Sun, Zhun Sun, Houwen Peng, and Han-Jia Ye. Mitigating visual forgetting via\ntake-along visual conditioning for multi-modal long cot reasoning. In ACL, 2025.\n[63] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua\nLuo, Kaifu Zhang, De-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. In\nInternational conference on machine learning , 2025.\n12\n--- Page 13 ---\n[64] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training\ntechniques for clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\n[65] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco,\nHannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over\ntext, tables and images. arXiv preprint arXiv:2104.06039 , 2021.\n[66] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.\n[67] Vladimir Vapnik, Rauf Izmailov, et al. Learning using privileged information: similarity control\nand knowledge transfer. J. Mach. Learn. Res. , 16(1):2023–2049, 2015.\n[68] Vladimir Vapnik and Akshay Vashist. A new learning paradigm: Learning using privileged\ninformation. Neural networks , 22(5-6):544–557, 2009.\n[69] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai,\nCong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information\nextraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 15641–15653, 2024.\n[70] Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. Tuta:\nTree-based transformers for generally structured table pre-training. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining , pages 1780–1790, 2021.\n[71] Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng\nWang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, et al. Chain-of-table:\nEvolving tables in the reasoning chain for table understanding. arXiv preprint arXiv:2401.04398 ,\n2024.\n[72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems , 35:24824–24837, 2022.\n[73] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[74] Bohao Yang, Yingji Zhang, Dong Liu, André Freitas, and Chenghua Lin. Does table source\nmatter? benchmarking and improving multimodal scientific table understanding and reasoning.\narXiv preprint arXiv:2501.13042 , 2025.\n[75] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu\nLi, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv\npreprint arXiv:2408.01800 , 2024.\n[76] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. A closer look at\ndeep learning on tabular data. arXiv preprint arXiv:2407.00956 , 2024.\n[77] Han-Jia Ye, Si-Yang Liu, and Wei-Lun Chao. A closer look at tabpfn v2: Strength, limitation,\nand extension. arXiv preprint arXiv:2502.17361 , 2025.\n[78] Han-Jia Ye, Huai-Hong Yin, De-Chuan Zhan, and Wei-Lun Chao. Revisiting nearest neighbor\nfor tabular data: A deep tabular baseline two decades later. In International Conference on\nLearning Representations , 2025.\n[79] Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language\nmodels are versatile decomposers: Decomposing evidence and questions for table-based rea-\nsoning. In Proceedings of the 46th international ACM SIGIR conference on research and\ndevelopment in information retrieval , pages 174–184, 2023.\n[80] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 9556–9567, 2024.\n[81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for\nlanguage image pre-training. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 11975–11986, 2023.\n[82] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming\nLiu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction\ntuning. arXiv e-prints , pages arXiv–2407, 2024.\n[83] Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, and Wanxiang Che. A survey\nof table reasoning with large language models. Frontiers of Computer Science , 19(9):199348,\n2025.\n[84] Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Binghong Wu, Lei Liao, Shu Wei, Yongjie Ye,\nHao Liu, Wengang Zhou, et al. Tabpedia: Towards comprehensive visual table understanding\n13\n--- Page 14 ---\nwith concept synergy. In Advances in Neural Information Processing Systems , pages 7185–7212,\n2024.\n[85] Yilun Zhao, Boyu Mi, Zhenting Qi, Linyong Nan, Minghao Guo, Arman Cohan, and Dragomir\nRadev. Openrt: an open-source framework for reasoning over tabular data. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System\nDemonstrations) , pages 336–347, 2023.\n[86] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping\nWang. Multimodal table understanding. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) , pages 9102–9124, 2024.\n[87] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 , 2023.\n[88] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli\nFeng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and\ntextual content in finance. arXiv preprint arXiv:2105.07624 , 2021.\n[89] Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, and Tat Seng Chua. Tat-llm: A\nspecialized language model for discrete reasoning over financial tabular and textual data. In\nProceedings of the 5th ACM International Conference on AI in Finance , pages 310–318, 2024.\n[90] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up-\nward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI\nConference on Artificial Intelligence , pages 26183–26191, 2025.\n14\n--- Page 15 ---\nA Training Details\nIn this section, we will introduce some training details about our experiment, including the datasets\nwe use and the hyperparameters in our experiment.\nA.1 Dataset Details\nFollowing HIPPO [ 37], we use Table Question Answering (TQA) and Table Fact Verification (TFV)\ntasks for training and evaluation.\nAll data statistics are shown in Table 2. we use five representative datasets: TABMWP [ 38], WTQ [ 48],\nHiTab [ 11], and TAT-QA [ 88]. We export FeTaQA [ 41] due to its evaluation metric being BLEU [ 47],\nwhich does not focus on the accuracy of question answering. For Table Fact Verification (TFV) tasks,\nwe include TabFact [ 9] and InfoTabs [ 22]. To further assess the robustness of our approach, we\nconduct additional experiments on MMMU [ 80], evaluating our model on all table-related questions\nwithin this challenging multimodal benchmark.\nTable 2: Dataset Statistics.\nDataset Train Test\nQuestion Answering\nTABMWP 30,745 7,686\nWTQ 17,689 4344\nHiTab 8,941 1,586\nTAT-QA 5,920 772\nFact Verification\nTabFact 31,321 6,845\nInfoTabs 18,383 5,400\nMMMU - 165\nThe construction of the TURBO training dataset utilized TABMWP, WTQ, TAT-QA, TabFact, and\nInfoTabs. The HiTab dataset is excluded because it involves multi-level tables, which present\nformatting challenges when converted to Markdown. From each of the chosen datasets, we randomly\nextract 2,000 instances, resulting in a combined dataset of 10,000 training instances. Each instance\nconsists of a structured table, a question, and the corresponding answer, which are then fed into a\nstructure-aware reasoning tracer generator to generate high-quality reasoning traces. Then, reject\nsampling is used to further filter the data, resulting in 9,000 training instances in total. This dataset\nforms the foundation for supervised fine-tuning and subsequent reinforcement learning stages in the\nTURBO framework.\nA.2 Hyperparameters.\nAs shown in Table 3, we provide the training hyperparameters for TURBO . Throughout all stages of\ntraining, we pre-train for one epoch in 9k generated data, with a batch size of 128.\nDuring the SFT (supervised fine-tuning) stage, we adopt a learning rate of 2e-6 with a cosine decay\nschedule, a per-GPU batch size of 2, and 16 gradient accumulation steps across 4 A100-80G GPUs.\nThe model is trained with bf16 precision and gradient checkpointing enabled. The maximum text and\nmultimodal input lengths are set to 1500 and 4096 tokens, respectively.\nIn the RL stage, we fine-tune using a lower learning rate of 5e-7 with a constant schedule. The setup\ninvolves 8 A100-80G GPUs with a per-GPU batch size of 1 and the same accumulation steps. Each\ninput prompt generates 16 candidate completions, and we apply a CLIP range of 0.2 to stabilize\nreward scaling. The sequence lengths are extended to 2000 (text) and 4596 (multimodal). We retain\nthe AdamW optimizer and a warmup ratio of 0.1 across both stages, with no weight decay.\n15\n--- Page 16 ---\nTable 3: Training hyperparameters.\nConfig SFT Stage RL Stage\nEpoch 1\nOptimizer AdamW\nLearning rate 2e-6 5e-7\nLearning rate scheduler Cosine Constant\nWeight decay 0.0\nWarmup ratio 0.1\nText max length 1500 2000\nMultimodal max length 4096 4596\nGeneration number - 16\nCLIP range - 0.2\nBatch size per GPU 2 1\nGradient accmulation steps 16\nGPU 4 × A100-80G 8 × A100-80G\nPrecision Bf16\nGradient checkpoint True\nB More Experiments\nIn this section, we present additional experimental results to further validate the effectiveness of\nTURBO . We also include qualitative examples to illustrate the model’s behavior and capabilities\nacross various scenarios.\nB.1 Comparison with DeepSeek\nWe also compare our method with DeepSeek-R1 [ 20]. We also show some text-only results for\nreference based on our model. The result is shown in Table 4.\nTable 4: Comparison with DeepSeek and the text-only results over datasets.\nMethodQuestion Answering Fact VerificationAverageTABMWP WTQ HiTab TAT-QA TabFact InfoTabs\nDeepSeek-R1 98.31 85.03 82.86 86.79 91.61 79.46 87.34\nBaseline-textonly 94.52 62.44 75.11 66.96 82.04 72.43 75.58\nTURBO -textonly 96.29 66.50 71.31 73.21 87.74 82.44 79.58\nTURBO 96.75 67.80 72.15 73.21 85.81 81.89 79.60\nDeepSeek-R1 leverages structured tables as input and benefits from its massive scale—671B\nparameters—and extensive training corpus, achieving the strongest overall performance across\nbenchmarks. In contrast, our proposed TURBO model operates with a much smaller parameter size\n(8B) and is trained on only 9kexamples. Remarkably, despite relying solely on table images during\ninference, TURBO attains competitive results and even surpasses DeepSeek-R1 on the InfoTabs\ndataset. This demonstrates the effectiveness and robustness of our training framework in extracting\nand reasoning over information from visual tables.\nFurthermore, we evaluate the generalization capability of our approach by comparing performance\nunder a text-only setting. Specifically, we run both baseline and our TURBO model using structured\ntables as input. Results show that TURBO , though primarily trained in a multimodal setting, achieves\nstrong performance even when only textual input is provided. The comparable results between\nTURBO and its text-only variant indicate that our training strategy successfully distills and transfers\nstructured knowledge from tables into the model. This highlights the broader applicability of our\napproach and its ability to enhance both visual and textual reasoning in a unified framework.\nB.2 More Cases\nWe present detailed more cases in Figure 6. These cases illustrate the diverse reasoning types\nand challenges, including multi-step arithmetic, compositional conditions, and fine-grained table\n16\n--- Page 17 ---\nQuestion: A candy dispenser put various numbers of orange candies into bags. How many bags had at least 22 orange candies but fewer than 86 orange candies?Turbo:<think>Okay, let's see. The problem is asking how many bags had at least 22 orange candies but fewer than 86. The data is given in a stem-and-leaf plot.First, I need to understand how stem-and-leaf plots work. The stem represents the tens place, and the leaf is the ones place. So, for example, a stem of 1 with leaves 6, 8, 9 would be 16, 18, 19. Looking at the table, … the stems that fall within this range are 2, 3, 4, 5, 6, and 7. Stem 8 is 89, which is above 86, so that's excluded.Now, let's countthe leaves for each of these stems:\\n\\n-Stem 2: Leaves are 1, 6, 9. That's 21, 26, 29, but 21 should be excluded. So 2 bags …Adding these up: 2 + 1 + 1 + 5 + 2 = 11. So the total number of bags with at least 22 but fewer than 86 candies is 11.</think><answer>11</answer>\nQuestion: The results of a compaction test on samples of soil that are to be used for an embankment on a highway project are listed below. Determine the optimum moisture content.  A: 10%.B: 8%.C: 9%.Turbo: <think>Okay, let's see. The question is asking for the optimum moisture content based on the compaction test results provided in the table. The table lists five samples with their respective moisture content and bulk density.First, I need to understand what \\\"optimum moisture content\\\" means. In soil compaction, the optimum moisture content is the percentage at which the soil can be compacted to achieve the maximum dry density. …Looking at the table, the samples are numbered 1 to 5. The moisture content increases from 4.8% to 9.7%. The bulk density values also increase, starting from 2.164 g/cc and going up to 2.351 g/cc. …The key here is to find the sample where the bulk density is highest, as that would correspond to the optimum moisture content. Let's check the bulk density values:\\n\\n-Sample 1: ….\\n\\nSample3 has the highest bulk density at 2.351 g/cc, which is the maximum value in the table. …Therefore,the optimum moisture content is 7.8%, which is closest to option B: 8%.</think><answer>B</answer>Figure 6: More cases of Turbo.\ncomprehension. These cases demonstrate that our TURBO can do well in both visual grounding,\nmathematics, and logical reasoning.\nC Limitations and Future Works\nWhile this work focuses on scenarios where structured tables are unavailable at test time—requiring\ninference purely from table images—there exist many practical situations where both the image and\nits underlying structured representation can be accessed. Exploring how to effectively incorporate\nstructured tables as additional inputs during inference presents a promising direction for future\nresearch.\nAnother limitation lies in the type of tables considered. Most of our experiments involve relatively\nclean and regular tables, where each row corresponds to a distinct entity and each column to a specific\nattribute. However, real-world tables are often more diverse and complex. For example, datasets\nlike HiTab and MMMU include tables with merged cells, nested headers, and irregular layouts\nthat challenge current MLLMs. A more systematic analysis of how different table structures affect\nmultimodal reasoning performance would be a valuable extension of this work.\nLastly, while our proposed TURBO framework significantly improves the reasoning abilities of\nopen-source MLLMs, a substantial performance gap remains when compared with state-of-the-art\nproprietary LLMs such as DeepSeek. We hope this paper inspires the community to further explore\nreal-world multimodal reasoning scenarios and to continue narrowing the gap between open- and\nclosed-source models, contributing to more robust and accessible MLLMs.\n17",
  "text_length": 69000
}