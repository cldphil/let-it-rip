{
  "id": "http://arxiv.org/abs/2506.04203v1",
  "title": "Cascadia: A Cascade Serving System for Large Language Models",
  "summary": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality.",
  "authors": [
    "Youhe Jiang",
    "Fangcheng Fu",
    "Wanru Zhao",
    "Stephan Rabanser",
    "Nicholas D. Lane",
    "Binhang Yuan"
  ],
  "published": "2025-06-04T17:48:38Z",
  "updated": "2025-06-04T17:48:38Z",
  "categories": [
    "cs.DC"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04203v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04203v1  [cs.DC]  4 Jun 2025CASCADIA : A Cascade Serving System for Large\nLanguage Models\nYouhe Jiang∗\nHKUST\nyouhejiang@gmail.comFangcheng Fu∗\nPeking University\nccchengff@pku.edu.cnWanru Zhao∗\nUniversity of Cambridge\nwz341@cam.ac.uk\nStephan Rabanser\nUniversity of Toronto\nstephan@cs.toronto.eduNicholas D. Lane\nUniversity of Cambridge\nndl32@cam.ac.ukBinhang Yuan†\nHKUST\nbiyuan@ust.hk\nAbstract\nRecent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality outputs. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this latency–quality\ntrade-off using model cascades, which route simpler queries to smaller models and\nmore complex ones to larger models. However, enabling efficient cascade serving\nremains challenging. Current frameworks lack effective mechanisms for handling\n(i) the huge and varying resource demands of different LLMs, (ii) the inherent het-\nerogeneity of LLM workloads, and (iii) the co-optimization of system deployment\nand routing strategy. Motivated by these observations, we introduce CASCADIA , a\nnovel cascade serving framework designed explicitly to schedule request routing\nand deploy model cascades for fast, quality-preserving LLM serving. C ASCADIA\nemploys a bi-level optimization method: at the inner level, it uses a mixed-integer\nlinear program to select resource allocations and parallelism strategies based on\nLLM information and workload characteristics; at the outer level, it applies a\nweighted Tchebycheff algorithm to iteratively co-optimize the routing strategy and\nthe system deployment produced by the inner level. Our extensive evaluation on\ndiverse workload traces and different model cascades (DeepSeek and the Llama\nseries) demonstrates that CASCADIA significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to 4 ×\n(2.3×on average) tighter latency SLOs and up to 5 ×(2.4×on average) higher\nthroughput while maintaining target answer quality.\n1 Introduction\nLarge language models (LLMs) such as DeepSeek-R1 [ 9], OpenAI o3 [ 27], Claude [ 3], Gemini [ 32]\nand Llama-3 [ 6] have demonstrated outstanding performance across a wide range of real-world\napplications (e.g., chatbots, healthcare and education) [ 12,29,8], largely influence human lives.\nHowever, serving LLMs can be costly [ 13,16,25], since significant computational resources (e.g.,\nGPUs) are required to meet certain service demands, such as meeting certain latency deadlines (i.e.,\nSLO attainment—the proportion of requests served within a specified response-time target) and\ngeneration throughput. In this paper, we explore an alternative solution that strategically utilizes\nmodel cascades to better balance the response latency and quality trade-offs inherent in LLM serving.\n∗Equal contribution\n†Correspondence to: Binhang Yuan <biyuan@ust.hk>\nPreprint. Under review.\n--- Page 2 ---\nDifferent Models406080100 Quality92\n78\n67\nDifferent Models261014 Latency (s)11.1\n2.30.4DeepSeek-671B\nDeepSeek-70BDeepSeek-7BFigure 1: Average response quality and\nlatencies of different DeepSeek models.\nQuality is judged by GPT-4o using the\nLLM-as-a-Judge framework [44].Cascade model serving refers to a serving architecture\nwhere multiple models of varying sizes and capabilities\nare arranged in a sequential pipeline, creating a hierarchy\nof models that process requests with increasing levels of\nsophistication [ 1,4,18,17,19,36]. As shown in Figure 1,\nlarger models typically provide higher response quality but\nalso incur greater latency, which in turn leads to increased\nenergy consumption and compute usage [ 33]. In this ap-\nproach, incoming requests are initially handled by smaller,\ncomputationally efficient models that can rapidly process\nsimpler requests. Only when these lightweight models de-\ntermine that a request exceeds their capabilities or requires\nhigher-quality responses does the system escalate the request to larger, more powerful models in\nthe cascade. This progressive delegation mechanism enables service providers to optimize system\nperformance by matching request complexity with appropriate model capacity, thereby significantly\nreducing computational costs while maintaining high-quality responses for complex request. Several\nrecent studies have focused on optimizing LLM serving using model cascades [4, 1, 18, 10, 26].\nThe cascade model serving architecture, which adaptively routes simpler and more complex requests\nto smaller and larger models, respectively, presents significant opportunities for optimizing the\ncost-efficiency of LLM serving. In this work, we focus specifically on the setting where service\nproviders host and manage every model in the cascade themselves. However, effectively adapting this\nparadigm to LLM scenarios is much harder to implement than to propose, as we enumerate below:\n•Model heterogeneity. LLMs require large amounts of compute and memory, and different models\nhave varying resource demands for efficient serving [ 14,5]. With a fixed resource pool, suboptimal\nallocation across models in the cascade can degrade overall serving efficiency.\n•Workload heterogeneity. LLM workloads exhibit considerable heterogeneity [ 28,37,39,43,\n41]. Models within the cascade often face incoming requests with varying characteristics (e.g.,\ninput/output lengths, arrival rates) and favor different deployment strategies (e.g., replication,\nparallel configuration), further adding complexity to optimal system deployment.\n•Cascade-aware load balancing. The request routing strategy directly impacts the system load of\neach model in the cascade. For instance, if more requests are routed to a particular model, its load\nincreases; the resource allocation and deployment strategy for that model should then be adjusted\nto balance loads across all models. Consequently, the deployment of multiple models must be\nco-optimized with the routing strategy to manage load across the cascade.\nIn order to overcome these challenges, we propose CASCADIA , a novel cascade serving system that\nis optimized for LLM characteristics and that co-optimizes the deployment of multiple models in the\ncascade together with the request routing strategy. Our contributions are as follows:\n•Contribution 1. We formulate cascade serving—covering system deployment and request rout-\ning—as a constrained optimization problem. To solve it efficiently, we propose a bi-level approach\nthat jointly optimizes deployment and routing. The inner level uses mixed-integer linear program-\nming (MILP) to determine the optimal deployment plan given a routing strategy, while the outer\nlevel applies a weighted Tchebycheff method to optimize routing, balancing latency and quality.\n•Contribution 2. We implement CASCADIA , an efficient cascade serving system tailored to LLMs.\nCASCADIA enables an adaptive model cascade paradigm that allocates resources and routes requests\nacross a hierarchy of model sizes (e.g., small, medium, and large), thereby balancing response\nlatency and output quality. Within each cascade stage, CASCADIA supports various parallelism\nstrategies (e.g., tensor and pipeline parallelism), which allows it to automatically select the optimal\nstrategy based on model size, incoming workload, and routing decisions.\n•Contribution 3. We empirically evaluate CASCADIA by comparing it to both single-model and\nexisting cascade serving systems across a variety of scenarios, including diverse workload traces\n(e.g., coding and mathematics), different model cascades (DeepSeek and the Llama series), and\nmultiple evaluation metrics (SLO attainment and throughput). The results show that, compared with\nstate-of-the-art non-cascade and cascade solutions, CASCADIA achieves up to 4 ×lower latency\ndeadlines (2.3 ×on average) and boosts system throughput by up to 5 ×(2.4×on average).\n2\n--- Page 3 ---\n2 Preliminary and Related Work\nLLM inference phases and workload heterogeneity. There are two phases within LLM inference:\nprefill anddecoding . During the prefill phase, the model processes the input prompt to compute\nthe key-value (KV) cache and generates the first token in a single step. In contrast, the decoding\nphase uses the last generated token and the KV cache as inputs to generate subsequent tokens in a\ntoken-by-token manner. Generally, the prefill phase is compute-bound, while the decoding phase is\nmemory-bound [ 28,46,2]. LLM inference workloads exhibit heterogeneity in input, output token\nlengths and request arrival rate, which is called workload heterogeneity . For instance, conversation\nworkloads (short input and long output lengths) typically require more memory resources to handle\nthe memory-bound decoding phase, while coding workloads (long input and short output lengths)\ndemand more compute resources to manage the compute-bound prefill phase. Therefore, appropriately\nallocating resources based on workload demands is critical for optimal performance [42, 15].\nCascade model inference. Current LLMs come in various sizes and configurations, offering a\nbroad spectrum of choices. Effectively leveraging this diversity can balance trade-offs between\nresponse latency and quality during inference. Recent efforts propose cascade model inference to\nutilize models of differing complexities. In such architectures, an input prompt is processed through\nincreasingly complex models, using threshold-based routing that stops computation once a cheaper\nmodel produces a confident enough answer. For instance, FrugalGPT [ 4] employs a dynamic LLM\ncascade strategy that routes queries through progressively stronger models (e.g., GPT-3.5 →GPT-4)\nbased on real-time difficulty estimation, optimizing cost-efficiency without sacrificing accuracy.\nSimilarly, AutoMix [ 1] uses intelligent layer-wise token routing to dynamically allocate computation\nbased on input difficulty. CascadeServe [ 18] automates and optimizes end-to-end inference with\ncascades, adjusting model deployment and request routing based on real-time system loads. However,\nexisting systems overlook key LLM-specific workload characteristics and neglect the importance of\nco-optimizing system deployment with request routing (i.e., system-algorithm co-design).\nSpeculative decoding and early-exit in LLM inference. Speculative decoding uses a lightweight\ndraft model to generate token blocks, which a larger target model verifies—leveraging model hetero-\ngeneity to reduce computation and latency [ 20,24,23]. Similarly, early-exit networks add decision\nbranches at intermediate layers, enabling inference to stop early when confidence is high—cascading\ncomputation within a single model [ 38,31]. In contrast, we focus firmly on cascade model inference.\nreq_rate=4\nlong outputs0.004.679.3414.01Throughput (req/s)DeepSeek 7B\nreq_rate=8\nlong outputs0.006.4212.8419.26DeepSeek 7B\nreq_rate=4\nlong outputs0.000.871.742.61DeepSeek 70B\nreq_rate=8\nlong outputs0.001.502.994.49DeepSeek 70B\nreq_rate=4\nshort outputs0.003.296.589.87DeepSeek 7B\nreq_rate=4\nshort outputs0.000.931.862.79DeepSeek 70B7B (4,1,1) 7B (2,2,1) 7B (2,1,2) 70B (4,2,1) 70B (4,1,2) 70B (2,4,1)\nFigure 2: Benchmarked performance of different parallelism strategies across different workloads and\nmodel sizes. Long and short outputs represent two different workloads with average output sequence\nlength to be 512 and 1024; the three-element array represents the DP, TP, and PP degrees.\nLimitations of existing cascade serving systems. We summarize the limitations of existing cascade\nserving systems: ( i) Ineffective resource allocation for different model types within a cascade.\nDifferent model types have distinct memory and computation resource needs. For example, DeepSeek-\n671B typically requires more allocated resources than DeepSeek-70B due to its larger memory and\ncomputational demands. Current systems ignore the importance of adjusting resource allocation\naccording to the needs of different model types, leading to unbalanced system loads. ( ii) Inadequate\nadaptation of parallelism strategies to varying workloads and model sizes. The optimal parallelism\nstrategies vary across different workloads (e.g., different input and output request sequence lengths\nand request arrival rates) and model sizes. As shown in Figure 2, choosing the optimal parallelism\nstrategy can achieve up to 3 ×higher system throughput. Current systems do not optimize parallelism\nstrategies according to specific workload and model size, resulting in degraded overall system\nperformance. ( iii) Insufficient co-optimization between system deployment and routing strategy. The\nrouting strategy decides the request portion processed by each model type within a cascade, which in\nturn determines the system loads for different model types. Existing systems neglect to adapt system\ndeployment configurations based on routing outcomes, resulting in suboptimal resource usage. To\n3\n--- Page 4 ---\naddress these challenges, a cascade serving system tailored for LLMs is necessary. Such a system\nmust optimize end-to-end performance and ensure stringent SLO adherence.\n3 Scheduling Algorithm in C ASCADIA\n3.1 Problem Formulation\nTo optimize the cascade serving system under fluctuating LLM workloads, the scheduling algorithm\nshould determine two essential components: ( i)The model deployment plan , which specifies the\nresource allocations and parallelism strategies for multiple model types (e.g., small, medium, large)\nwithin the cascade to minimize the system response latency (e.g., p95 latency—the response time\nthreshold below which 95% of all requests complete); and ( ii)the routing strategy , which balances\nthe trade-off between system response latency and quality to decide the appropriate model path for\neach incoming query. We term a solution addressing these two components as a cascade plan .\n𝐑𝐨𝐮𝐭𝐢𝐧𝐠\t𝐒𝐭𝐫𝐚𝐭𝐞𝐠𝐲DeterminesWorkloadDistribution𝐈𝐧𝐧𝐞𝐫\t𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐚𝐭𝐢𝐨𝐧SolvesResourceAllocation&ParallelismStrategyWorkloadDistribution𝐎𝐮𝐭𝐞𝐫\t𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐚𝐭𝐢𝐨𝐧EvaluatesTrade-offs(Latencyvs.Quality)𝐃𝐞𝐩𝐥𝐨𝐲𝐦𝐞𝐧𝐭\t𝐏𝐥𝐚𝐧DeterminesSystemResponseLatencySystemResponseLatency\nFigure 3: Our bi-level optimization workflow.Note that the routing strategy determines the request\ndistribution over different model types, which in turn\ndictates the optimal model deployment plan, while\nthe model deployment plan defines the system re-\nsponse latency that feeds back into the routing de-\ncision. Given the interdependent and exponentially\nlarge search space, determining the optimal cascade\nplan is an NP-hard problem. To solve this problem,\nwe adopt a bi-level optimization method that enables\nsystem–algorithm co-design, which is shown in Figure 3, and can be summarized as:\n•MILP-based inner optimization : Given the routing strategy, the inner optimization (§3.2) employs\nan mixed-integer linear programming (MILP) formulation to capture system resource constraints\nand compute the optimal model deployment plan that minimizes system response latency.\n•Weighted Tchebycheff outer optimization : Based on the latency outcome from the inner opti-\nmization, the outer optimization (§3.3) applies the weighted Tchebycheff method to minimize the\nmaximum weighted deviation from an ideal trade-off point between system response latency and\nquality, thereby generating a well-distributed set of Pareto-optimal routing strategies.\n3.2 MILP-Based Inner Optimization\nThe inner optimization of our scheduling algorithm determines the optimal model deployment plan\nbased on the routing strategy and resource constraints. An example deployment plan is shown\nin Figure 4. Assume a total of NGPUs serve a model cascade with Cmodel types, {c1, c2, . . . , c C},\nwhere cidenotes the i-th model type. The outer-layer routing strategy provides incoming workload\ninformation W={w1, w2, . . . , w C}for each model type, including average input/output sequence\nlengths and request arrival rate. We use F=f1, f2, . . . , f Cto denote the number of GPUs allocated\nper model. The total allocation must not exceed available GPUs, i.e.,PC\ni=1fi≤N.\nModelReplicaPipelineParallelismTensorParallelismWorkload𝐰𝟏𝐰𝟐𝐰𝟑\n𝐂𝐚𝐬𝐜𝐚𝐝𝐞\t𝟏:\t𝐜𝟏𝐂𝐚𝐬𝐜𝐚𝐝𝐞\t𝟐:\t𝐜𝟐𝐂𝐚𝐬𝐜𝐚𝐝𝐞\t𝟑:\t𝐜𝟑\n𝐃𝐏=𝟐;𝐓𝐏=𝟐;𝐏𝐏=𝟐𝐃𝐏=𝟐;𝐓𝐏=𝟐,𝐏𝐏=𝟐;𝐓𝐏=𝟐𝐃𝐏=𝟏;𝐓𝐏=𝟐,𝐏𝐏=𝟑\nFigure 4: Illustration of a model deployment plan.Parallelism strategy search. Given\nthe workload information wiand re-\nsource allocation fi, we determine\nthe optimal parallelism strategy and\nthe corresponding simulated system\nresponse latency for the model type\ni.CASCADIA provides three forms\nof parallelism: data parallelism (i.e.,\nmodel replication, DP) [ 21], tensor\nmodel parallelism (TP) [ 34], and\npipeline parallelism (PP) [ 11]. Denot-\ning the degrees of data, tensor, and pipeline parallelism for the model type by dp,tp, and pp, any\nfeasible parallelism strategy must satisfy the following resource constraint: (Pdp i\nj=1tpi,j×ppi,j)≤fi,\ni.e., one model type can be replicate into multiple replicas, each replica can have varied tensor and\npipeline parallelism degrees, as shown in Figure 4, the summation of different parallelism degrees\nshould be less or equal than the total number of GPUs assigned. Based on the workload information\n4\n--- Page 5 ---\nwand the resource constraint f, we iterate over all feasible parallelism combinations to select the\nstrategy that minimizes the response latency lifor the model type i. The latency liis computed using\nthe simulator S(·)asli=S(wi, fi)3.\nMILP problem formulation. Our MILP problem formulation aims to minimize the maximum system\nresponse latency among all model types in the cascade. Let Lbe a continuous variable representing\nthe maximum latency across all model types. Given the workload information wi, we discretize the\npossible GPU resource allocations f∈ {1,2, . . . , N }and precompute the corresponding latencies\nliusing the simulator S(wi, f). We introduce binary assignment variables xi,f, defined such that\nxi,f= 1 if model type iis allocated fGPUs, and 0otherwise, for each type i∈ {1, . . . , C }\nand each feasible allocation f. The constraints of our MILP include: ( i) For each model type i,\nexactly one GPU allocation fmust be selected, i.e.,PN\nf=1xi,f= 1,∀i= 1, . . . , C ; (ii) the total\nnumber of GPUs assigned across all model types should be equal to the available GPUs N, i.e.,PC\ni=1PN\nf=1f xi,f=N; and ( iii) the maximum latency Lmust be at least as large as the latency\nli(f)corresponding to each selected allocation, i.e., L≥PN\nf=1li(f)xi,f,∀i= 1, . . . , C . We\nexplicitly enforce variable domains and integrality constraints as follows: xi,f∈ {0,1},∀i, fand\nL≥0. If certain GPU allocations fare infeasible for specific model types—such as when the total\nmemory of the allocated fGPUs is less than the minimum memory required by the model type—we\nexplicitly set xi,f= 0 for these allocation pairs. Our goal is to minimize the maximum system\nresponse latency L, used in the outer layer optimization.\n3.3 Weighted Tchebycheff Outer Optimization\nThe outer optimization of our scheduling algorithm aims to optimize the trade-off between system\nresponse latency and quality by generating a Pareto front.\n…\n𝐋𝐌𝟏𝐋𝐌𝟐𝐋𝐌𝐂𝐎𝐮𝐭𝐩𝐮𝐭𝐬𝐎𝐮𝐭𝐩𝐮𝐭𝐬𝐎𝐮𝐭𝐩𝐮𝐭𝐬𝐑𝐨𝐮𝐭𝐞𝐫𝐈𝐧𝐩𝐮𝐭𝐬…𝐑𝐨𝐮𝐭𝐢𝐧𝐠\t𝐌𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭𝐑𝐞𝐪𝐮𝐞𝐬𝐭𝐬\t𝐀𝐜𝐜𝐞𝐩𝐭𝐞𝐝𝐂𝐚𝐬𝐜𝐚𝐝𝐞𝐈𝐧𝐟𝐞𝐫𝐞𝐧𝐜𝐞\nFigure 5: Threshold-based cascade routing\nworkflow. The router determines whether a\nrequest is accepted or forwarded to the next\nmodel type based on predefined thresholds.Thresholds tuning and request routing. We adopt\nthe threshold-based cascade routing workflow con-\nsistent with prior works [ 1,4] (Figure 5). Initially,\nevery incoming request is sent to the first (small-\nest) model type c1in the cascade. A judger then\nevaluates the quality of the output responses from\nmodel types c1tocC−1, and a set of thresholds H=\n{h1, h2, . . . , h C−1}is defined to decide whether the\nrequests at each model type should be accepted or\nforwarded to the next model type. In this framework,\nthe routing strategy θis directly determined by the\nthresholds H, i.e., θ=θ(H). Each routing strategy\nθis associated with a system response latency L(θ)\n(determined by the inner layer optimization) and a\nquality metric Q(θ)(determined by the judger).\nWeighted Tchebycheff optimization. Given a routing strategy θwith a corresponding system\nresponse latency L(θ)and a quality metric Q(θ), we employ the weighted Tchebycheff method [ 35]\nto balance these competing objectives. First, we define an utopia point z∗= (z∗\n1, z∗\n2)representing\nthe best achievable system response latency and quality, where z∗\n1denotes the minimum latency,\ncorresponding to the scenario where all requests are processed by the smallest model type c1, and\nz∗\n2denotes the maximum quality, corresponding to the scenario where all requests are processed\nby the largest model type cC. Then, we formulate the scalarized objective function as: T(θ) =\nmax{λ1(L(θ)−z∗\n1), λ2(z∗\n2−Q(θ))}, where λ1andλ2are positive weights reflecting the relative\nimportance of latency and quality. Next, we explore different trade-off regions of the Pareto front: (i)\nFor each specific λ1andλ2, we solve the single-objective optimization problem, i.e., minT, which\nyeilds a routing strategy that is Pareto-optimal for that particular trade-off between latency and quality;\n(ii) we vary (λ1, λ2)over a logarithmic scale (e.g., 0.1 to 10) to generate a set of Pareto-optimal\nsolutions, as shown in Figure 6, each reflecting a different trade-off between latency and quality.\nFinally, the optimal strategy is selected according to user-specific requirements, such as stringent\nlatency constraints or the need for higher-quality responses.\n3We adopt the inference task simulator from ETH EASL Scratchpad [ 7], which estimates the system p95\nlatency according to workload information and resource allocation.\n5\n--- Page 6 ---\nPareto-optimalpointsDominatepointsPareto-optimalfontConvergence𝛉𝟏𝛉𝟐𝐐𝐮𝐚𝐥𝐢𝐭𝐲𝐒𝐲𝐬𝐭𝐞𝐦\t𝐑𝐞𝐬𝐩𝐨𝐧𝐬𝐞\t𝐋𝐚𝐭𝐞𝐧𝐜𝐲Figure 6: Pareto-optimal solutions.Illustrative Example. Assume the utopia point is\ndefined by a minimum achievable latency z∗\n1= 10\nms and a maximum quality z∗\n2= 0.95. Using the\nweighted Tchebycheff method with weights (λ1=\n0.6, λ2= 0.4), consider a non-optimal strategy θ1\nwith latency L(θ1) = 12 ms and quality Q(θ1) =\n0.90. The weighted deviations from the ideal are\n0.6×(12−10) = 1 .2and0.4×(0.95−0.90) = 0 .02,\nyielding T(θ1) = max {1.2,0.02}= 1.2. Another\nPareto-optimal strategy θ2with latency 11 ms and\nquality 0.92 results in T(θ2) = max {0.6,0.012}=\n0.6, which is preferred under this weight setting, as illustrated in Figure 6. Additionally, by varying\nthe weights (λ1, λ2), the optimization emphasizes different objective preferences, allowing the\nexploration of diverse trade-off solutions along the Pareto front.\nImpact of LLM workloads on Pareto-optimal strategy selection. The characteristics of incoming\nLLM workloads strongly influence which Pareto-optimal points are selected. Two key factors\ncontribute: ( i) Average input/output lengths and arrival rates affect system latency—longer sequences\nor higher loads increase compute demand, raising latency under limited resources and shifting\nthe latency–quality trade-off; ( ii) Request complexity impacts response quality—if small models\nconfidently handle most queries, fewer escalations to larger models are needed, preserving quality at\nlower latency. Our bi-level framework considers system constraints (e.g., resource allocation) and\nalgorithmic behavior (e.g., routing), enabling efficient, adaptive co-optimization across workloads.\n4 Evaluation\nTo evaluate the design and implementation of C ASCADIA , we ask the following essential questions:\n•RQ1: What is the end-to-end performance comparison between CASCADIA and state-of-the-art\nLLM serving systems in SLO attainment and throughput under configurable quality guarantees?\n•RQ2: What model deployment plans and routing strategies are used for different test cases to\noptimize system performance?\n•RQ3: How effective is our scheduling algorithm in practice?\n4.1 Experimental Setup\nEnvironments. Our experiments are conducted on 4 GPU servers, where each server is equipped\nwith 8 NVIDIA H100-80GB GPUs. Within each server, the GPUs are connected via NVLink with a\nbandwidth of 400GB/s, and the servers are connected via Inifiband with a bandwidth of 200GB/s.\nModel cascade construction. We construct a model cascade using the DeepSeek series models for\nCASCADIA , which are representative and popular open-source transformer models. Specifically, we\nuse DeepSeek-7B, DeepSeek-70B (distilled version), and DeepSeek-671B AWQ with INT4 quantized\nweights [ 22] as three model types within our system. We employ a GPT-4o (LLM-as-a-Judge) [ 44]\nas the judger mentioned in §3.3, which assesses the output responses of each model type within the\ncascade and assigns scores between 0 and 100.\nBaselines. We compare C ASCADIA with two baselines:\n•Compare with stand-alone LLMs served by SGLang. We compare CASCADIA against stand-\nalone LLMs that are directly served on SGLang [ 45] under various response quality constraints\n(e.g., 90, 85, 80, 70) to demonstrate the effectiveness of LLM serving with model cascades. For\nquality requirement of 90 and 85, we choose stand-alone DeepSeek-671B for comparison, and for\nquality reqirement of 80 and 70, we choose stand-along DeepSeek-70B for comparison. To achieve\na fair comparison, we tune the parallelism strategy using our MILP algorithm mentioned in §4 for\neach of the stand-alone model and report the best values in all experiments.\n•Compare with cascade model serving system CascadeServe. We compare CASCADIA against\nan existing cascade model serving system CascadeServe. It chooses model cascade deployment\nplan based on system load (e.g., request arrival rate), enables model replication on hardware and\nadaptively dispatches incoming requests. We tune the parallelism and request routing strategies for\nCascadeServe based on the real-time system load and report the best values in all experiments.\n6\n--- Page 7 ---\n10 15 20 25 3058687990100SLO Attainment (%)\n11.73 17.39 20.00Trace 1 | avg_quality=90\n6.0 9.5 13.0 16.5 20.054657788100\n7.38 11.43 15.42Trace 1 | avg_quality=85\n2.0 3.2 4.5 5.8 7.051637688100\n3.153.70 5.70Trace 1 | avg_quality=80\n1 2 3 4 544587286100\n1.44 2.83 4.40Trace 1 | avg_quality=70\n4 9 14 19 2446607386100SLO Attainment (%)\n7.08 11.50 16.12Trace 2 | avg_quality=90\n3.0 5.8 8.5 11.2 14.044587286100\n3.57 5.16 13.08Trace 2 | avg_quality=85\n2.0 3.4 4.8 6.1 7.554667789100\n2.44 3.79 4.55Trace 2 | avg_quality=80\n0.5 1.6 2.8 3.9 5.036526884100\n0.97 2.71 3.93Trace 2 | avg_quality=70\n3.0 6.2 9.5 12.8 16.0\nSLO Scale43577286100SLO Attainment (%)\n4.51 6.62 13.87Trace 3 | avg_quality=90\n3.0 5.2 7.5 9.8 12.0\nSLO Scale46597386100\n3.75 6.31 11.88Trace 3 | avg_quality=85\n1 2 3 4 5\nSLO Scale44587286100\n1.34 2.39 4.21Trace 3 | avg_quality=80\n0.5 1.4 2.2 3.1 4.0\nSLO Scale36526884100\n0.86 1.57 3.47Trace 3 | avg_quality=70Cascadia CascadeServe DeepSeek-70B/671BFigure 7: End-to-end SLO attainment results evaluating CASCADIA against two baseline systems.\nEach row corresponds to a particular LLM workload trace, and each column corresponds to a specific\nquality requirement. The stars indicate the 95% SLO attainment for each system.\nTraces. We follow prior work to generate workload traces based on real-world data [ 13,46]. Our\ntesting traces are subsampled from MT-Bench [ 44], a multi-turn conversation benchmark that contains\nmultiple types of LLM workloads (e.g., coding, mathematics and reasoning). Each of our subsampled\ntraces have different workload characteristics and different complexities as mentioned in §3.3.\nEvaluation metrics. Following previous evaluation setups [ 21,5,2], we evaluate system performance\nbased on SLO attainment and system throughput. The SLO is determined empirically based on the\nsystem’s average single-request processing latency, and we scale it to various multiples (SLO Scale\nin Figure 7) to assess performance under different levels of operational stringency. We focus on\nidentifying the minimum SLO Scale at which the system achieves 95% SLO attainment.\n4.2 End-to-end Experimental Results (RQ1)\nEnd-to-end system performance. We evaluate the SLO attainment and throughput of CASCADIA\nacross multiple traces and quality requirements, comparing it with two baselines. Results in Figure 7\nand Figure 8 show that C ASCADIA outperforms all baselines:\n•CASCADIA achieves up to 4 ×and on average 2.8 ×lower latency deadlines, and up to 5 ×and\non average 3 ×higher system throughput compared with stand-alone LLMs. For instance, when\ntesting on trace 3 with an average quality requirement of 85, stand-alone DeepSeek-671B requires\n11.88 SLO scales to achieve 95% attainment, while CASCADIA with different model types that\nuses smaller models to process simpler requests only requires 3.75 SLO scales.\n•CASCADIA achieves up to 2.5 ×and on average 1.7 ×lower latency deadlines, and up to 3.3 ×\nand on average 1.7 ×higher throughput than CascadeServe. While CascadeServe optimizes model\ndeployment and routing based on real-time load, it overlooks LLM-specific workload characteristics\n(e.g., input/output lengths) and request complexity, leading to sub-optimal parallelism and routing.\nFor example, on trace 1 with an average quality requirement of 90, CascadeServe needs 17.3 SLO\nscales to reach 95% SLO attainment, whereas C ASCADIA requires only 11.73.\nSystem performance with different model cascades. We further evaluate CASCADIA using a\ndifferent model cascade by replacing the DeepSeek series with the Llama series (Llama3-8B and\nLlama3-70B). As shown in Figure 9, CASCADIA outperforms baselines by up to 3.8 ×and on average\n2.6×, demonstrating strong performance across LLM cascades.\n7\n--- Page 8 ---\n0.000.080.150.230.30Throughput (req/sec)1.39×\n1.50×\nTrace 1 | avg_quality=90\n0.000.140.280.420.56\n1.55×\n2.28×\nTrace 2 | avg_quality=90\n0.000.190.380.580.77\n1.29×\n2.59×\nTrace 3 | avg_quality=90\n0.000.140.280.410.55\n1.70×\n2.09×\nTrace 1 | avg_quality=85\n0.000.280.560.841.13\n1.45×\n3.91×\nTrace 2 | avg_quality=85\n0.000.260.530.791.06\n1.68×\n3.27×\nTrace 3 | avg_quality=85\n0.000.300.600.901.21Throughput (req/sec)1.12×\n1.76×\nTrace 1 | avg_quality=80\n0.000.400.801.201.59\n1.57×\n1.93×\nTrace 2 | avg_quality=80\n0.000.781.572.353.14\n1.90×\n3.53×\nTrace 3 | avg_quality=80\n0.00.71.42.12.8\n1.89×\n3.24×\nTrace 1 | avg_quality=70\n0.001.192.383.574.76\n3.25×\n4.70×\nTrace 2 | avg_quality=70\n0.001.322.643.965.28\n1.93×\n4.99×\nTrace 3 | avg_quality=70Cascadia CascadeServe DeepSeek-70B/671BFigure 8: End-to-end throughput results evaluating CASCADIA against two baseline systems across\ndifferent LLM workload traces and quality requirements.\n3.5 5.1 6.8 8.410.0\nSLO Scale56677889100\n4.6 6.2 8.2Trace 1 | avg_quality=80\n2.0 3.4 4.8 6.1 7.5\nSLO Scale48617487100\n2.8 4.8 6.5Trace 2 | avg_quality=80\n1.0 2.5 4.0 5.5 7.0\nSLO Scale38546984100\n1.62.6 6.0Trace 3 | avg_quality=80\n1.5 3.1 4.8 6.4 8.0\nSLO Scale44587286100\n2.13.1 6.4Trace 1 | avg_quality=70\n1.0 2.8 4.5 6.2 8.0\nSLO Scale40557085100\n1.6 4.95.5Trace 2 | avg_quality=70\n1.0 2.8 4.5 6.2 8.0\nSLO Scale40557085100\n1.61.8 5.1Trace 3 | avg_quality=70SLO Attainment (%)Cascadia CascadeServe Llama-70B\nFigure 9: End-to-end SLO attainment results evaluating CASCADIA against two baselines using a\nLlama cascade (Llama3-8B; Llama3-70B) across LLM workload traces and quality requirements.\n4.3 Case studies on Model Deployment Plans and Routing Strategies (RQ2)\n(90, 1) (85, 1) (80, 1) (80, 2) (80, 3) (70, 3)0.00.20.40.60.81.01.2Processing Latency\n(Normalized by c1)c1c2c3\nFigure 10: Benchmarked processing latency of each model\ntype within the cascade across different testing cases.Case study on resource allocation\nand routing strategies. We bench-\nmarked the thresholds, processing ra-\ntios and allocated resources for differ-\nent model types across different test-\ning cases. For instance, when testing\non trace 1 with an average quality re-\nquirement of 90, model types c1toc3\nprocess 100%, 94% and 50% of the\ntotal requests, and the assigned GPU\nnumbers are 4, 8 and 20. When the quality requirement changes to 85, less requests are required\nto be processed by the largest model c3(from 50% to 21%), and less resources are allocated to c3\naccordingly (from 20 to 16). This algorithm and system co-optimization enables CASCADIA to adjust\nsystem resource allocation and request routing based on user requirements, ensuring balanced load\nacross different model types to boost system performance. Additionally, when testing on trace 3 with\nan average quality requirement of 70, CASCADIA deploys a subset of model types (DeepSeek-7B\nand -70B) to minimize the latencies required for requests processing. As shown in Figure 10, across\ndifferent testing cases, CASCADIA always balances the loads among different model types to ensure\noptimized system performance. Table 1 in Appendix C demonstrates the thresholds, processing ratios\nand allocated resources for different model types across different testing cases.\nCase study on parallelism strategies. We benchmarked the parallelism strategies for different model\ntypes across different testing cases. For example, when testing on trace 1 with an average quality\nrequirement of 90, the optimal parallelism strategy s2forc2is (DP=2, TP=4). In this case, if we\nchange the parallelism strategy to (DP=4, TP=2), the performance of this model type would drop by\n33.7%. Additionally, when the quality requirement drops to 85, the optimal parallelism strategy s2for\nc2shifts to (DP=6, TP=2). This adjustment occurs because the change in quality requirements alters\nthe LLM workloads, the request complexity routed and the resource allocated to c2. Consequently,\ns2is updated to optimize the single model type’s performance while balancing loads across all model\ntypes within the cascade. Table 2 in Appendix C presents the parallelism strategies for each model\ntype within the cascade across different test cases.\n8\n--- Page 9 ---\n0.000.080.150.230.30Throughput (req/sec)1.23×\n2.10×\nTrace 1 | avg_quality=90\n0.000.140.280.420.56\n1.46×\n1.86×\nTrace 2 | avg_quality=90\n0.000.190.380.580.77\n1.32×\n1.43×\nTrace 3 | avg_quality=90\n0.000.140.280.410.55\n1.58×\n1.71×\nTrace 1 | avg_quality=85\n0.000.280.560.841.13\n1.42×\n 1.38×\nTrace 2 | avg_quality=85\n0.000.260.530.791.06\n1.63×\n1.47×\nTrace 3 | avg_quality=85Cascadia Uniform Parallelism Strategy Uniform Resource AllocationFigure 11: Ablation study on resource allocation and parallelism strategy.\nAblation study. We disable individual optimizations in CASCADIA to evaluate their impact, as shown\nin Figure 11: ( i) Replacing our parallelism strategy optimization with a uniform parallelism strat-\negy—tensor parallelism within each server and data parallelism across servers—reduces performance\nby up to 1.6 ×(1.4×on average). For example, DeepSeek-7B and DeepSeek-671B requires higher\ndegrees of data and tensor parallelism to maximize throughput and parameter sharding; a uniform\napproach fails to accommodate these needs. ( ii) Replacing our resource allocation optimization with\nuniform resource allocation reduces performance by up to 2.1 ×(1.7×on average). For instance, in\ntrace 1 with an average quality requirement of 90, DeepSeek-671B was originally allocated 20 GPUs,\nbut uniform allocation assigns only 12, causing load imbalance.\n4.4 Effectiveness of the Scheduling Algorithm (RQ3)\nOverall scheduling process. During scheduling, our weighted Tchebycheff optimization (§3.3)\nexplores parameters λ1,λ2,h1, andh2to balance response latency and quality. Simultaneously, our\nMILP-based optimization (§3.2) searches for resource allocations and parallelism strategies to balance\nload across model types and minimize latency. CASCADIA then selects the optimal plan—including\nthresholds, resource allocations, and parallelism strategies—based on quality requirements. Figure 13\nin Appendix D shows the explored scheduling points across different traces.\n32 GPUs 64 GPUs 128 GPUs050100150200Time Cost (s)Trace 1 Trace 2 Trace 3\nFigure 12: Algorithm running time under\nour experimental setup (32 GPUs) and when\nscaled to larger clusters (64 and 128 GPUs).Scheduling algorithm runtime and scalability. Fig-\nure 12 shows the runtime performance of CASCA -\nDIA’s scheduling algorithm, evaluated on a 12-core\nCPU instance. In our setup (32 GPUs), scheduling\ncompletes within one minute. For larger clusters (64\nand 128 GPUs), it finishes within 2 and 4 minutes, re-\nspectively. These results demonstrate the algorithm’s\nefficiency and scalability across test cases and cluster\nsizes. Moreover, the algorithm is highly paralleliz-\nable, as resource allocations, parallelism, and routing\nstrategies are independent—allowing execution time to scale down with more CPU cores.\nRe-scheduling to adapt to workload changes. As discussed in §3.3, LLM workload characteristics\n(e.g., average input and output lengths, request rate and complexity) significantly affect the optimal\nmodel deployment plan and routing strategy. Thus, CASCADIA implement a re-scheduling mechanism\nto accommodate dynamic LLM workloads. Concretely, the system ( i) subsample and record the\nreal-time characteristics of the incoming LLM workloads (e.g., subsample 100 requests every 10\nminutes and record the workload characteristics), ( ii) upon detecting a significant shift in workload\ncharacteristics (e.g., an increase in request arrival rate or request complexity), the scheduling algorithm\nis executed again, incorporating recent historical data to produce an updated model deployment plan\nand routing strategy. Note that the re-scheduling and model reloading process take only minutes—far\nshorter than the hourly scale at which real-world workload variations tend to occur.\n5 Conclusion\nThis paper proposes CASCADIA , a cascade serving system tailored for LLMs. Its core component is\na scheduling algorithm that jointly optimizes resource allocation, parallelism, and routing within the\ncascade system. Extensive experiments on diverse workload traces and multiple model cascades show\nthat this co -design substantially reduces request latency and boosts system throughput compared with\nboth single-model and existing cascade baselines, while maintaining the target answer quality.\n9\n--- Page 10 ---\nReferences\n[1]Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra,\nPei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, et al. Automix:\nAutomatically mixing language models. Advances in Neural Information Processing Systems ,\n37:131000–131034, 2024.\n[2] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-\nvani, Alexey Tumanov, and Ramachandran Ramjee. Taming {Throughput-Latency }tradeoff in\n{LLM}inference with {Sarathi-Serve }. In18th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 24) , pages 117–134, 2024.\n[3]Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL\nhttps://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/\nModel_Card_Claude_3.pdf .\n[4]Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models\nwhile reducing cost and improving performance. Transactions on Machine Learning Research .\n[5]Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion\nStoica, and Hao Zhang. Muxserve: flexible spatial-temporal multiplexing for multiple llm\nserving. In Proceedings of the 41st International Conference on Machine Learning , pages\n11905–11917, 2024.\n[6]Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783 , 2024.\n[7] ETH-EASL. Scratchpad, 2025. URL https://github.com/eth-easl/Scratchpad .\n[8]GitHub. The world’s most widely adopted ai developer tool, 2024. URL https://github.\ncom/features/copilot .\n[9]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n[10] Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna\nMenon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In\nThe Twelfth International Conference on Learning Representations .\n[11] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, Hy-\noukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant\nneural networks using pipeline parallelism. Advances in neural information processing systems ,\n32, 2019.\n[12] Jaeho Jeon and Seongyong Lee. Large language models in education: A focus on the com-\nplementary relationship between human teachers and chatgpt. Education and Information\nTechnologies , 28(12):15873–15892, 2023.\n[13] Youhe Jiang, Ran Yan, Xiaozhe Yao, Yang Zhou, Beidi Chen, and Binhang Yuan. Hexgen:\ngenerative inference of large language model over heterogeneous environment. In Proceedings\nof the 41st International Conference on Machine Learning , pages 21946–21961, 2024.\n[14] Youhe Jiang, Fangcheng Fu, Xiaozhe Yao, Guoliang He, Xupeng Miao, Ana Klimovic, Bin Cui,\nBinhang Yuan, and Eiko Yoneki. Demystifying cost-efficiency in llm serving over heterogeneous\ngpus. arXiv preprint arXiv:2502.00722 , 2025.\n[15] Youhe Jiang, Fangcheng Fu, Xiaozhe Yao, Taiyi Wang, Bin Cui, Ana Klimovic, and Eiko\nYoneki. Thunderserve: High-performance and cost-efficient llm serving in cloud environments.\narXiv preprint arXiv:2502.09334 , 2025.\n[16] Youhe Jiang, Ran Yan, and Binhang Yuan. Hexgen-2: Disaggregated generative inference of\nllms in heterogeneous environment. arXiv preprint arXiv:2502.07903 , 2025.\n10\n--- Page 11 ---\n[17] Steven Kolawole, Don Dennis, Ameet Talwalkar, and Virginia Smith. Revisiting cascaded\nensembles for efficient inference. In Workshop on Efficient Systems for Foundation Models II@\nICML2024 .\n[18] Ferdi Kossmann, Ziniu Wu, Alex Turk, Nesime Tatbul, Lei Cao, and Samuel Madden. Cas-\ncadeserve: Unlocking model cascades for inference serving. arXiv preprint arXiv:2406.14424 ,\n2024.\n[19] Luzian Lebovitz, Lukas Cavigelli, Michele Magno, and Lorenz K Muller. Efficient inference\nwith model cascades. Transactions on Machine Learning Research , 2023.\n[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\nspeculative decoding. In International Conference on Machine Learning , pages 19274–19286.\nPMLR, 2023.\n[21] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang,\nZhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. {AlpaServe }: Statistical multiplexing\nwith model parallelism for deep learning serving. In 17th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 23) , pages 663–679, 2023.\n[22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan\nXiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization\nfor on-device llm compression and acceleration. Proceedings of Machine Learning and Systems ,\n6:87–100, 2024.\n[23] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao\nZhang. Online speculative decoding. In Forty-first International Conference on Machine\nLearning .\n[24] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang,\nRae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large\nlanguage model serving with tree-based speculative inference and verification. In Proceedings of\nthe 29th ACM International Conference on Architectural Support for Programming Languages\nand Operating Systems, Volume 3 , pages 932–949, 2024.\n[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia. Spot-\nserve: Serving generative large language models on preemptible instances. In Proceedings of\nthe 29th ACM International Conference on Architectural Support for Programming Languages\nand Operating Systems, Volume 2 , pages 1112–1127, 2024.\n[26] Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta,\nAditya Krishna Menon, and Sanjiv Kumar. Faster cascades via speculative decoding. arXiv\npreprint arXiv:2405.19261 , 2024.\n[27] OpenAI. Openai o3, 2025. URL https://platform.openai.com/docs/models/o3 .\n[28] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and\nRicardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In 2024\nACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA) , pages\n118–132. IEEE, 2024.\n[29] Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa,\nCheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, et al. A study of generative large\nlanguage model for medical research and healthcare. NPJ digital medicine , 6(1):210, 2023.\n[30] You Peng, Youhe Jiang, Chen Wang, and Binhang Yuan. Hexgen-text2sql: Optimizing llm\ninference request scheduling for agentic text-to-sql workflow. arXiv preprint arXiv:2505.05286 ,\n2025.\n[31] Haseena Rahmath P, Vishal Srivastava, Kuldeep Chaurasia, Roberto G Pacheco, and Rodrigo S\nCouto. Early-exit deep neural network-a comprehensive survey. ACM Computing Surveys , 57\n(3):1–37, 2024.\n11\n--- Page 12 ---\n[32] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-\nbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv\npreprint arXiv:2403.05530 , 2024.\n[33] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to\nwatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pages 1–9. IEEE, 2023.\n[34] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[35] Ralph E Steuer and Eng-Ung Choo. An interactive weighted tchebycheff procedure for multiple\nobjective programming. Mathematical programming , 26:326–344, 1983.\n[36] Matthew Streeter. Approximation algorithms for cascading prediction models. In International\nconference on machine learning , pages 4752–4760. PMLR, 2018.\n[37] Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin.\nLlumnix: Dynamic scheduling for large language model serving. In 18th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 24) , pages 173–191, 2024.\n[38] Surat Teerapittayanon and Bradley McDanel. Branchynet: Fast inference via early exiting from\ndeep neural networks. In 2016 23rd international conference on pattern recognition (ICPR) ,\npages 2464–2469. IEEE, 2016.\n[39] Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhenheng Tang, Xin He, Rui Guo, Xin Wang,\nQiang Wang, Amelie Chi Zhou, et al. Burstgpt: A real-world workload dataset to optimize llm\nserving systems. arXiv preprint arXiv:2401.17644 , 2024.\n[40] Ran Yan, Youhe Jiang, Wangcheng Tao, Xiaonan Nie, Bin Cui, and Binhang Yuan. Flashflex:\nAccommodating large language model training over heterogeneous environment. arXiv preprint\narXiv:2409.01143 , 2024.\n[41] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:\n1m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning\nRepresentations .\n[42] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou, Jiarong Xing,\nand Ion Stoica. Blendserve: Optimizing offline inference for auto-regressive large models with\nresource-aware batching. arXiv preprint arXiv:2411.16102 , 2024.\n[43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: A large-scale real-world\nllm conversation dataset. In The Twelfth International Conference on Learning Representations .\n[44] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in Neural Information Processing Systems , 36:46595–46623, 2023.\n[45] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu,\nShiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution\nof structured language model programs. Advances in Neural Information Processing Systems ,\n37:62557–62583, 2024.\n[46] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang. {DistServe }: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving. In 18th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 24) , pages 193–210, 2024.\n12\n--- Page 13 ---\nA Limitations\nLLM-specific. CASCADIA is designed specifically for LLMs and leverages LLM-specific character-\nistics (e.g., LLMs’ resource needs and workloads); it may not directly extend to classic deep neural\nnetworks or vision models without adaptation.\nExperimental scale. All our experiments were conducted on a 32-GPU cluster—while this scale\ncaptures many real-world deployment scenarios, it precludes evaluation on larger GPU pools, which\nwe leave for future work.\nHomogeneous cluster. Our framework assumes a homogeneous cluster in which all GPUs share\nidentical capabilities; supporting heterogeneous deployments with mixed-capacity hardware is an\nimportant avenue for future investigation.\nB Extended Related Work\nParallelism strategies. LLMs with huge memory and computational resource requirements typically\nrely on parallelization across multiple GPUs [ 21]. There are three prevalent forms of parallelism:\ndata parallelism (DP, i.e., model replication), tensor parallelism (TP) [ 34], and pipeline parallelism\n(PP) [ 11]. DP replicates the model into multiple replicas, enabling parallel processing of requests. TP\ndivides model weights and computationally intensive operations such as matrix multiplication across\nvarious GPUs, thereby splitting data scanning and computation to minimize LLM inference latency.\nPP divides the layers of a model into multiple stages. These stages are assigned to distinct GPUs for\nexecution and they establish a pipeline. Only inter-layer activations are needed to be communicated\nbetween stages.\nC Case studies on Model Deployment Plans and Routing Strategies\nCase study on resource allocation and routing strategies. Table 1 demonstrates the case study of\nthresholds, processing ratios and allocated resources for different model types across different testing\ncases.\nTable 1: Case study of the thresholds ( h1,h2), processing ratios ( p1,p2,p3), and allocated resources ( f1,f2,\nf3) for each model type within the cascade across different testing cases. (90, 1) denotes testing on Trace 1 with\nan average quality requirement of 90.\nh1 h2 p1 p2 p3 f1 f2 f3\n(90, 1) 99 91 100% 94% 50% 4 8 20\n(85, 1) 74 64 100% 62% 21% 4 12 16\n(80, 1) 69 25 100% 54% 11% 6 14 12\n(80, 2) 61 18 100% 31% 3% 8 16 8\n(80, 3) 32 0 100% 23% 0% 18 14 0\n(70, 3) 10 0 100% 5% 0% 24 8 0\nCase study on parallelism strategies. Table 2 presents a case study on parallelism strategies for\neach model type within the cascade across different test cases.\nTable 2: Case study of the parallelism strategies for each model type within the cascade ( s1,s2,s3) across\ndifferent testing cases.\nParallelism Strategies\n(90, 1) s1: (DP=4), s2: (DP=2, TP=4), s3: (TP=4, PP=3), (TP=8)\n(85, 1) s1: (DP=2, TP=2), s2: (DP=6, TP=2), s3: (DP=2, TP=8)\n(80, 1) s1: (DP=6), s2: (DP=5, TP=2), (TP=4), s3: (TP=4, PP=3)\n(80, 2) s1: (DP=6), (TP=2), s2: (DP=8, TP=2), s3: (TP=8)\n(80, 3) s1: (DP=10), (DP=4, TP=2), s2: (DP=2, TP=4), (DP=3, TP=2), s3: -\n(70, 3) s1: (DP=16), (DP=4, TP=2), s2: (DP=4, TP=2), s3: -\nD Overall scheduling process.\nOverall scheduling process. During scheduling, our weighted Tchebycheff optimization mentioned\nin §3.3 explores different parameters λ1,λ2,h1andh2, aims at optimizing the trade-off between\n13\n--- Page 14 ---\n134 439 744 104960718292Average QualityTrace 1\n131 412 692 973\nEstimated System Response Latency (s)64748595Trace 2\n138 395 651 90868788898Trace 3Dominated Points Pareto-Optimal PointsFigure 13: Pareto-optimal strategy search across different traces by systematically varying the parameters λ1,\nλ2,h1,h2mentioned in §3.3, as well as exploring different resource allocation and parallelism strategies.\nsystem response latency and quality. Meanwhile, our MILP-based optimization discussed in §3.2\nevaluates different resource allocation and parallelism strategies, aims at balancing the system loads\namong different model types and minimizing system response latency. Figure 13 demonstrates the\nexplored scheduling algorithm points across different traces. CASCADIA then selects the optimal\npoint, which includes information such as thresholds, resource allocations and parallelism strategies\nfor each model type within the cascade, based on specific quality requirements.\n14",
  "text_length": 53416
}