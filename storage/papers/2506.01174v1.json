{
  "id": "http://arxiv.org/abs/2506.01174v1",
  "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question\n  Answering",
  "summary": "Structured scene representations are a core component of embodied agents,\nhelping to consolidate raw sensory streams into readable, modular, and\nsearchable formats. Due to their high computational overhead, many approaches\nbuild such representations in advance of the task. However, when the task\nspecifications change, such static approaches become inadequate as they may\nmiss key objects, spatial relations, and details. We introduce GraphPad, a\nmodifiable structured memory that an agent can tailor to the needs of the task\nthrough API calls. It comprises a mutable scene graph representing the\nenvironment, a navigation log indexing frame-by-frame content, and a scratchpad\nfor task-specific notes. Together, GraphPad serves as a dynamic workspace that\nremains complete, current, and aligned with the agent's immediate understanding\nof the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a\n+3.0% increase over an image-only baseline using the same vision-language\nmodel, while operating with five times fewer input frames. These results show\nthat allowing online, language-driven refinement of 3-D memory yields more\ninformative representations without extra training or data collection.",
  "authors": [
    "Muhammad Qasim Ali",
    "Saeejith Nair",
    "Alexander Wong",
    "Yuchen Cui",
    "Yuhao Chen"
  ],
  "published": "2025-06-01T21:13:38Z",
  "updated": "2025-06-01T21:13:38Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01174v1",
  "full_text": "--- Page 1 ---\nGraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question\nAnswering\nMuhammad Qasim Ali*\nUniversity of Waterloo\nm45ali@uwaterloo.caSaeejith Nair*\nUniversity of Waterloo\nsmnair@uwaterloo.caAlexander Wong\nUniversity of Waterloo\na28wong@uwaterloo.ca\nYuchen Cui\nUniversity of California, Los Angeles\nyuchencui@cs.ucla.eduYuhao Chen\nUniversity of Waterloo\nyuhao.chen1@uwaterloo.ca\nAbstract\nStructured scene representations are a core compo-\nnent of embodied agents, helping to consolidate raw sen-\nsory streams into readable, modular, and searchable for-\nmats. Due to their high computational overhead, many ap-\nproaches build such representations in advance of the task.\nHowever, when the task specifications change, such static\napproaches become inadequate as they may miss key ob-\njects, spatial relations, and details. We introduce Graph-\nPad, a modifiable structured memory that an agent can tai-\nlor to the needs of the task through API calls. It comprises a\nmutable scene graph representing the environment, a navi-\ngation log indexing frame-by-frame content, and a scratch-\npad for task-specific notes. Together, GraphPad serves as\na dynamic workspace that remains complete, current, and\naligned with the agent’s immediate understanding of the\nscene and its task. On the OpenEQA benchmark, Graph-\nPad attains 55.3 % accuracy— +3.0 pp over an image-only\nbaseline using the same vision–language model—while op-\nerating with five times fewer input frames. These results\nshow that allowing online, language-driven refinement of\n3-D memory yields more informative representations with-\nout extra training or data collection.\n1. Introduction\nA household robot has just scanned the kitchen when a user\nasks, “Is the red mug back inside the upper cupboard?”\nBecause the scene graph was built from only a handful of\nearlier keyframes, it lacks both a node for the mug and the\nrelation linking it to the cupboard interior. With this criti-\ncal information missing, the agent must either guess, rescan\nthe entire scene, or refuse the query altogether. This sce-\nnario highlights a fundamental limitation in current embod-ied AI: structured 3D memories are typically finalized\nbefore the downstream task is known, frequently omit-\nting objects and spatial relations that later prove essen-\ntial for action or reasoning .\nInherent limitations of static memories. Modern em-\nbodied systems commonly pair a vision-language model\n(VLM) with pre-computed structured scene representations\nsuch as 3D scene graphs [10, 11]. These structured scene\nrepresentations help to consolidate large amounts of dense,\nnoisy RGB-D frames into a shared scene representation. By\nmodeling the scene as distinct entities and relations, Scene\nGraphs help VLMs structure their complex multi-step spa-\ntial reasoning while performing navigation and manipula-\ntion tasks [2, 11].\nA core challenge with such memory systems is that\ncrucial compression decisions—specifically, which objects\nto retain and what aspects of the scene to describe—are\nmade once. When the subsequent task shifts, these pre-\nconfigured, static representations can become insufficient,\nincomplete, or inaccurate for the new task at hand.\nThis fundamental flaw is vividly demonstrated in Em-\nbodied Question Answering (EQA) [4]. Here, generic scene\ngraphs, built without any foresight into the specific ques-\ntions that will be asked, consistently perform poorly when\nattempting to provide answers [14]. This underscores a crit-\nical point: a memory system optimized for one general pur-\npose simply can’t adapt effectively to the nuanced demands\nof a different, more specific task.\nCurrent solutions follow two primary approaches. Over-\nprovisioning retains nearly every candidate detection, incur-\nring substantial memory and computational overhead [1].\nOffline enhancement pipelines append heuristically chosen\naffordances or relations for anticipated tasks [11], but each\nnew domain demands additional engineering effort and can-\nnot recover information never initially detected. NeitherarXiv:2506.01174v1  [cs.AI]  1 Jun 2025\n--- Page 2 ---\nstrategy scales effectively when user requests vary widely.\nOur approach: language-guided memory updates. We\npropose that an embodied agent should be able to dynam-\nically revise its structured scene memory when reasoning\nreveals a gap in its knowledge. Rather than discarding an\ninadequate structured representation—and thereby losing\nits benefits—or recreating it from scratch at high compu-\ntational cost, the agent should be able to modify and en-\nhance the existing representation. This allows the agent to\nincorporate task-relevant information without starting over.\nMoreover, the agent should be able to do this through tar-\ngeted perceptual queries on specific keyframes, and seam-\nlessly integrate the newly discovered information into its\nscene graph.\nGraphPad. We introduce GraphPad , a modifiable 3D\nscene graph memory whose content can be updated at in-\nference time through language-level commands issued by\nthe same VLM that performs high-level reasoning. When-\never uncertainty arises or missing information is identi-\nfied, the VLM can: (i) retrieve additional keyframes from\nits navigation log, (ii) insert previously undetected objects\nor spatial relationships, and (iii) annotate existing nodes\nwith task-specific attributes. These operations are exposed\nas language-callable functions that the reasoning agent it-\nself can invoke, eliminating the need for task-specific rea-\nsoning code.1Through successive targeted updates, the\ngraph evolves from a coarse initial representation into a\ntask-conditioned model containing precisely the informa-\ntion needed for the current query or action planning.\nContributions.\n• We formulate language-driven online editing of struc-\ntured 3D memories as a solution to the task–memory mis-\nmatch inherent in fixed scene graphs.\n• We present GraphPad , a system in which a single VLM\nboth identifies knowledge gaps and updates its 3D repre-\nsentation via language function calls during inference.\n• On the OpenEQA benchmark [14], we demonstrate\nthat GraphPad improves spatial question answering from\n52.3% to 55.3% while reducing the number of processed\nframes from 25 to 5, outperforming both image-only and\nstatic-graph baselines without additional training.\nBy recasting perception as an interactive, language-\nmediated process rather than a static preprocessing step,\nGraphPad enables more efficient and effective reasoning\nabout complex 3D environments—a critical capability for\nagents that must bridge language understanding with phys-\nical action.\n1Generic vision modules—object detector, mask extractor, depth back-\nprojection—remain necessary but are notspecialized for any particular\ndownstream task.2. Background and Related Work\nCurrent embodied AI systems struggle with a fundamental\nlimitation: their memory representations often lack critical\ninformation needed during task execution. This section ex-\namines key research areas relevant to this challenge.\n2.1. Embodied Memory Representations\nFrame-level memories store raw RGB-D streams with\nlearned descriptors. Systems like ReMEmbR [1] and\nEmbodied-RAG [28] index thousands of images, allowing\nretrieval of relevant observations during inference. While\nflexible, these approaches face computational overhead\nfrom processing large image collections and delegate all\nspatial reasoning to language models.\nSemantic metric maps address these limitations by em-\nbedding visual features within geometric reconstructions.\nVL-Maps [9], PLA [5], and OpenScene [15] support spatial\ngrounding but lack explicit object boundaries and relation-\nships essential for symbolic reasoning.\n3D scene graphs compress perception into discrete\nentities and relations. Hydra [10] pioneered real-time\nscene graph construction, with extensions for monocular\ninputs [20], multi-robot fusion [6], and open-vocabulary\nlabeling [7]. These structures align with structured VLM\nprompting but remain largely static after construction.\n2.2. Memory Update Mechanisms\nSeveral existing methodologies employ updatable memo-\nries; however, they typically fall short by not dynamically\nrefining scene graphs with new semantic information cru-\ncial for the task at hand but absent in the initial recording:\nDynamic object tracking systems like DynaMem [12],\nOpenIN [24], and Kimera [20] update spatial relations when\nobjects move but rely on predefined update policies rather\nthan reasoning-triggered modifications.\nIncremental discovery methods like Moma-LLM [8]\nand Search3D [23] can integrate new objects but employ\nfixed detection heuristics rather than responding to specific\nreasoning gaps.\nMemory selection approaches like 3D-Mem [29] and\nKARMA [25] retain informative views but cannot add\nstructure absent from initial observations. Similarly,\nexpanded-context approaches [2] can reference more im-\nages but cannot insert missing relations.\n2.3. Task-Memory Alignment\nEnsuring memory representations contain task-relevant in-\nformation remains a central challenge:\nGraph adaptation techniques attempt to tailor exist-\ning memories to specific tasks. Information-theoretic\napproaches [13] compress graphs while preserving task-\nrelevant nodes. SayPlan [19] and SayNav [17] dynamically\ncontract graphs during planning. Neither approach handles\n--- Page 3 ---\nFigure 1. Overview of GraphPad for embodied question answering. The top row illustrates the Structured Scene Memory (SSM) com-\nponents: Scene Graph with associated Graphical Scratch Pad, Frame Memory containing sparse keyframes, and Navigation Log indexing\nframe metadata. The bottom row demonstrates the inference process: given the question about a white object above the TV , the VLM\nfirst examines the initial memory state (left), identifies missing information and calls the analyze frame API on a promising frame\n(middle), then integrates the newly detected air conditioner into the scene graph and scratch pad before providing the answer (right). This\nprocess enables dynamic, task-specific memory updates without exhaustive preprocessing.\nadding new information. LLM-enhanced scene graphs [11]\naugments an existing scene graph with task-specific affor-\ndances for the household rearrangement task. However,\nthey rely on a complex scene graph enhancement pipeline\nthat is domain-specific and not adaptable to diverse, unfore-\nseen downstream tasks.\nImage-level reasoning systems like Bumble [22] and\nTagMap [30] provide direct visual access but sacrifice ex-\nplicit relational structure. Graph-based reasoning meth-\nods [28] provide structured context but treat graphs as read-\nonly, requiring new exploration rather than memory editing.\nEmpirical analyses [26] confirm that missing nodes and\nrelations—not reasoning failures—frequently cause perfor-\nmance errors in embodied tasks.\n2.4. Embodied Question Answering\nEmbodied Question Answering (EQA) benchmarks offer a\nstandardized evaluation of memory adequacy. The episodic\nmemory variant (EM-EQA) [14] provides agents with fixed\nobservations, isolating memory representation quality from\nexploration.Recent approaches to EM-EQA highlight representation\ncompleteness as a critical factor. GraphEQA [21] uses se-\nmantic graphs for viewpoint selection, while 3D-Mem [29]\ncurates informative snapshots. Both systems nevertheless\nfail when queried entities are absent from their representa-\ntions.\nGraphPad addresses the task-memory alignment prob-\nlem through targeted scene graph updates during inference.\nUnlike previous work, our approach enables the reasoning\nVLM to detect knowledge gaps and modify its own mem-\nory representation through three specific operations: frame\nretrieval, entity/relation insertion, and semantic annotation.\nThis maintains the efficiency advantages of structured rep-\nresentations while addressing their typically static nature.\n3. Methodology\nGraphPad builds a Structured Scene Memory (SSM) from\na sparse set of RGB-D keyframes and gives the vision-\nlanguage model (VLM) that answers questions three\ncallable functions to editthat memory during inference. We\nfirst describe the agentic reasoning loop (Sec. 3.1), then\n--- Page 4 ---\nthe four data structures that constitute the SSM (Sec. 3.2),\nand finally the Modifiability APIs (Sec. 3.3).\nThroughout we follow the episodic-memory EQA pro-\ntocol [14]: the agent is given every k-th RGB-D frame of a\npre-recorded scan together with camera poses; no new sens-\ning is possible after deployment.\n3.1. Agentic Reasoning Loop\nAt test time, the VLM receives both the initial SSM and a\nnatural language query q. The reasoning process unfolds\niteratively: the VLM analyzes what information it needs to\nanswer the question and repeatedly invokes Modifiability\nAPIs to augment its understanding of the scene.\nIn each iteration, the VLM examines the current scene\nmemory, identifies knowledge gaps relevant to the question,\nand selects: (1) which API to call, (2) which frame to ana-\nlyze, and (3) what specific information to seek (expressed as\na natural language query). The selected API returns new ob-\njects, relations, or semantic annotations that are integrated\ninto the scene memory. This process continues until the\nVLM determines it has sufficient information or reaches a\nmaximum of mallowed API calls.\nTo ensure that the responses are grounded in all mem-\nory components, we prompt the VLM that it must support\nits final answer with dual evidence—visual evidence from\nframes in the Frame Memory and semantic evidence from\nnotes in the Scratch-Pad. This constraint ensures that the\nVLM adequately utilizes its scene memory and searches the\nscene thoroughly prior to responding.\nUnlike static memory systems that rely on pre-built rep-\nresentations, GraphPad enables a VLM to dynamically up-\ndate its memory based on task requirements and its under-\nstanding of the scene. In our experiments, most questions\nare answered with just 2-3 targeted API calls, demonstrat-\ning the utility and efficiency of the reasoning loop.\n3.2. Structured Scene Memory\nThe SSM contains four mutually linked structures:\nScene Graph\nA directed multigraph G= (N,E)whose nodes rep-\nresent object tracks . Each node nistores a point cloud\nPi, pooled visual embedding Vi, pooled language em-\nbedding Li, caption Ci, room/floor ID, and a list of\nkeyframes in which the object is visible. Edges encode\nfour view-invariant spatial relations critical for planning\nand manipulation: on top of ,subpart of ,contained in ,\nandattached to .\nGraphical Scratch-Pad\nMirrors Nbut adds a free-form notes field initialized\nempty; the APIs write task-specific information here\nduring reasoning.Frame Memory\nAn initial set of nimgevenly spaced keyframes. Addi-\ntional frames requested by the APIs are appended (no\neviction is used in our experiments).\nNavigation Log\nFor each keyframe: room, textual field-of-view tag, ego-\ncentric motion label (from pose deltas), and the set of\nvisible node IDs. The log serves as a structured index,\nguiding the VLM in selecting candidate frames likely\nto contain information about specific objects or spatial\nrelationships.\nInitial construction. For every k-th RGB image It, we\nrun a VLM detector that outputs bounding boxes and cap-\ntions. Each box is passed to SAM to obtain a mask; the\nmask is back-projected with depth into a point cloud and\nvoxel-downsampled to 0.02 m. Noise is removed by keep-\ning only the largest DBSCAN cluster (default sklearn\nparameters). Visual embeddings are extracted using CLIP\nViT-L/14; language embeddings come from BGE [27].\nTrack association. A new detection Diis matched to an\nexisting track Tjwhen the vote\nSij=1[Vi·Vj>0.7]+1[Li·Lj>0.8]+1[Gij>0.4](1)\nexceeds 2, where Gijis the fraction of points in Diwithin\nδgofTj(we use δg= 5 cm). This voting scheme requires\nat least two of the following conditions to hold with suffi-\ncient confidence: visual feature similarity, caption semantic\nsimilarity, or spatial overlap. Visual and language embed-\ndings of the matched track are updated by an exponential\nmoving average with α= 0.5; unmatched detections start\nnew tracks.\nEdge discovery. Every three frames we prompt the\nVLM with the current frame plus the JSON list of visible\n⟨bbox , caption ⟩pairs; the model predicts all pairwise rela-\ntions among the four relation types. Each predicted edge is\nstored with a subject-ID, object-ID, relation label, and the\nVLM’s free-form justification string.\nCaption consolidation. Accumulated captions on a\ntrack are periodically compressed by prompting the VLM\nwith the list and asking for a single sentence that faithfully\nparaphrases all entries.\nRoom/floor labels. We adopt the HOV-SG pipeline [26]:\nfloors from height-histogram modes and rooms via water-\nshed segmentation on wall skeletons; room labels derive\nfrom CLIP similarity to a fixed set of class names.\nWhen the SSM is passed to the VLM for reasoning, the\nScene Graph, Scratch-Pad, and Navigation Log are serial-\nized to JSON, while Frame Memory is supplied as inter-\nleaved images with their Frame IDs.\n--- Page 5 ---\n3.3. Modifiability APIs\nAll three APIs receive a Frame ID and a natural-language\nquery . Each returns a JSON patch containing new nodes,\nedges, or scratch-pad notes plus evidence pointers that are\nincorporated into the SSM.\nfind objects\nDetects previously unseen instances relevant to query in\nthe specified frame and fuses them into G. The func-\ntion leverages the VLM’s bounding box detection ca-\npabilities to identify query-relevant objects and gener-\nate corresponding query-relevant notes. These detec-\ntions are incorporated into the scene graph by associat-\ning them with existing tracks or creating new ones. The\nquery-relevant notes are used to update the correspond-\ning scratch-pad entries.\nanalyze objects\nFor each node in the user-supplied list that is visible in\nthe frame, the VLM analyzes its appearance and an-\nswers query , storing the result in the node’s notes .\nThe function employs the VLM to examine each visi-\nble node’s bounding box and generate descriptive notes\npertaining to the query. These notes are stored in the\ncorresponding nodes’ scratch-pad entries. If specified\nnodes are not visible in the frame, the function defaults\ntofind objects .\nanalyze frame\nA frame-level variant that jointly discovers undetected\nobjects and annotates existing ones with respect to\nquery . This consolidated approach can both identify\nnew perceptual elements and enrich the semantic under-\nstanding of known objects in a single operation.\nThese APIs enable a critical capability: the reasoning\nagent itself can identify and remedy gaps in its scene rep-\nresentation during inference. Rather than preprocessing\nexhaustively for anticipated questions, GraphPad builds a\nminimal initial representation and lets task requirements\nguide targeted perceptual refinement. This approach aligns\nwith real-world robotic scenarios where complete scene un-\nderstanding is computationally intractable, but targeted per-\nception can efficiently support specific goals.\nBy invoking these functions, the agent patches omis-\nsions in its memory in real time, yielding a task-conditioned\ngraph that improves both answer accuracy and confidence\nwithout requiring rescanning of the physical scene.\n4. Results\nWe evaluate GraphPad on the OpenEQA benchmark [14] to\nassess how language-guided scene graph updates affect spa-\ntial reasoning performance. Our experiments use Gemini\n2.0 Flash [16] as both the reasoning agent and detector. AllTable 1. OpenEQA performance comparison across methods\n# Method Accuracy (%)\nBlind LLMs\n1 GPT-4 33.5\nSocratic LLMs w/ Frame Captions\n2 GPT-4 w/ LLaV A-1.5 43.6\nSocratic LLMs w/ Scene-Graph Captions\n3 GPT-4 w/ CG 36.5\n4 GPT-4 w/ SVM 38.9\nMulti-Frame VLMs\n5 GPT-4V 55.3\n6 Gemini-2.0 Flash 52.3\n7 3D-Mem (w/ GPT4V) 57.2\nHuman Agent\n8 Human 86.8\nOur Results\n9 GraphPad (w/ Gemini 2.0 Flash) 55.3\nevaluations use the episodic-memory variant of OpenEQA,\nwhere systems receive fixed keyframes from 3D scene scans\n(HM3D [18] and ScanNet [3]).\n4.1. Baseline Comparison on OpenEQA\nTable 1 presents GraphPad’s performance against estab-\nlished baselines. Using an initial frame memory size of\nnimg= 5, search depth of m= 20 , and Frame-Level API,\nGraphPad achieves 55.3% accuracy. This represents a 3.0\npercentage point increase over using the same VLM (Gem-\nini 2.0 Flash) with image-only input (52.3%). GraphPad\nprocesses 5 initial frames compared to the 25 frames (every\nk-th frame with k= 5for HM3D and k= 20 for ScanNet)\nused in the image-only baseline.\nGraphPad scores higher than static scene graph methods\n(CG: 36.5%, SVM: 38.9%), suggesting potential benefits\nof dynamic scene graph updates during inference. The sys-\ntem performs identically to GPT-4V (55.3%) despite using\na different base VLM. 3D-Mem [29] achieves 57.2% accu-\nracy, outperforming our approach by 1.9 percentage points,\nthough it uses GPT-4V rather than Gemini 2.0.\nThe comparison suggests that structured memory with\ntargeted updates can help bridge the performance gap be-\ntween different vision-language models while potentially\nreducing the number of frames that need to be processed.\n4.2. Component Analysis\nTo understand the contribution of different GraphPad com-\nponents, we conducted an ablation study (Table 2) using a\nsubset of 184 OpenEQA questions. Starting with just four\nraw frames (32.9% accuracy), we progressively added com-\nponents to measure their individual impact.\n--- Page 6 ---\nTable 2. Ablation study of system components\nMethod Accuracy (%)\nFrame Memory 32.9\nFrame Memory + Scene Graph 34.6\nFrame Memory + Navigation Log 42.5\nFrame Memory + SG + Navigation Log 46.9\nFrame Memory + Navigation Log + Image\nAPI45.1\nFrame Memory + SG + Navigation Log +\nNode-level API47.1\nFrame Memory + SG + Navigation Log +\nFrame-level API50.5\nTable 3. Performance as a function of search depth\nSearch Depth Accuracy (%)\n0 46.9\n1 45.8\n2 46.3\n3 48.1\n4 45.9\n5 49.7\n20 51.2\nThe Navigation Log provides the largest individual im-\nprovement (+9.6 percentage points over frame-only), sug-\ngesting that structured information about frame contents\nhelps guide the VLM’s attention. Adding a static scene\ngraph yields a modest improvement (+1.7 percentage points\nover frame-only), while the Modifiability APIs add another\n3.6 percentage points over the static scene representation.\nFrame-level APIs (50.5%) outperformed node-level vari-\nants (47.1%). We observed that when using node-level\nAPIs, the VLM tended to rely more on find objects\nthananalyze objects , often using the former in ways\nsimilar to analyze frame .\n4.3. Effect of Search Depth and Frame Count\nWe examined how search depth ( m) affects accuracy using\nthe OpenEQA184 subset with nimg= 4 initial frames and\nthe Frame-Level API. Table 3 shows that accuracy generally\nincreases with search depth, reaching 51.2% at m= 20 . In-\nterestingly, limited search ( m= 1 orm= 2) sometimes\nperforms worse than no search ( m= 0), possibly because\npreliminary updates without follow-up refinement can in-\ntroduce misleading information.\nThe initial number of frames in Frame Memory also\ninfluences performance (Table 4). Accuracy improves as\nmore frames are included, though gains diminish beyond\n5 frames. We observed that with fewer initial frames, the\nmodel made more API calls on average, potentially com-\npensating for the limited initial context.Table 4. Performance as a function of initial frame count\nInitial Frames Accuracy (%)\n2 48.2\n3 47.4\n4 49.9\n5 50.3\n6 50.5\nTable 5. Performance by question category: GraphPad vs. Gemini\n2.0 with images only\nCategory GraphPad (%) Gemini (%)\nAttribute Recognition 66.8 46.5\nObject State Recognition 69.6 66.5\nFunctional Reasoning 59.2 53.5\nWorld Knowledge 55.9 52.0\nObject Recognition 58.4 62.2\nSpatial Understanding 47.7 52.4\nObject Localization 31.3 34.3\n4.4. Performance by Question Category\nBreaking down performance by question category (Table 5)\nreveals variation in how GraphPad (with k= 5) compares\nto using Gemini-2.0 with only images (every 5th frame in\nthe scene).\nGraphPad shows larger improvements in attribute recog-\nnition (+20.3 percentage points), functional reasoning (+5.7\npercentage points), and object state recognition (+3.1 per-\ncentage points). These categories often require detailed\nanalysis of specific object properties. The image-only base-\nline performs better in object recognition (-3.8 percentage\npoints), spatial understanding (-4.7 percentage points), and\nobject localization (-3.0 percentage points), which may rely\nmore on raw visual processing capabilities.\n4.5. API Call Distribution\nAnalysis of GraphPad’s API usage (Fig. 2) shows that 95%\nof questions are answered with 5 or fewer API calls, with an\naverage of 1.9 calls per question. The distribution indicates\nthat in most cases, the system requires relatively few up-\ndates to answer questions, though we observe a long tail of\nmore complex queries requiring additional refinement steps.\nThe peak at zero calls represents questions that could be\nanswered using only the initial scene representation without\nany additional API calls, suggesting that for some question\ntypes, the initial structured memory provides sufficient con-\ntext.\n5. Limitations\nWhile GraphPad demonstrates the value of editable 3D\nscene representations, several important limitations affect\n--- Page 7 ---\nFigure 2. Distribution of API calls per query on OpenEQA\nits current implementation:\n•Error propagation in detection. Our current implemen-\ntation lacks a verification mechanism for object detection\nquality. When the VLM misidentifies an object or rela-\ntion, this error becomes part of the scene graph and can\nnegatively affect downstream reasoning.\n•API design constraints. The three operations (find, ana-\nlyze objects, analyze frame) represent our initial attempt\nat a minimal API set, but we have not rigorously eval-\nuated whether this is optimal. Different reasoning tasks\nmight benefit from specialized APIs not explored in this\nwork.\n•Computational overhead. Each API call requires a full\nVLM inference pass, adding significant latency (typically\n2-3 seconds per call in our implementation). This latency\ncurrently limits GraphPad’s applicability to real-time sys-\ntems.\n•Domain generalization. Our APIs were designed specif-\nically for question answering about static scenes. We\nhave not tested their effectiveness for other domains like\nmanipulation planning or navigation in dynamic environ-\nments.\n•Scalability limits. As scene graphs grow larger, both the\nprompt size and reasoning complexity increase. Our ex-\nperiments were limited to medium-sized home environ-\nments; performance in larger spaces remains untested.\n6. Conclusion\nWe presented GraphPad, a system that enables VLMs to\nupdate 3D scene graphs during inference through language-\ncallable functions. On OpenEQA, GraphPad improved ac-\ncuracy by 3.0 percentage points over an image-only base-\nline using the same VLM while requiring fewer input\nframes.\nOur experiments suggest several insights for 3D\nlanguage-vision systems. First, static structured represen-\ntations appear to benefit from targeted, task-specific re-\nfinement. Second, language-guided perception may of-\nfer a middle ground between exhaustive preprocessing andpurely reactive vision. Third, the efficiency of GraphPad\n(averaging under 2 API calls per question) indicates that\ntargeted scene exploration can be a practical strategy.\nFor future 3D vision-language-action systems, these re-\nsults suggest investigating how reasoning agents might di-\nrect their own perception in service of task goals. Extend-\ning GraphPad’s approach to manipulation planning, naviga-\ntion, and dynamic scenes could help bridge the gap between\nlanguage understanding and effective action in 3D environ-\nments.\nReferences\n[1] Abrar Anwar, John Welsh, Joydeep Biswas, Soha Pouya, and\nYan Chang. Remembr: Building and reasoning over long-\nhorizon spatio-temporal memory for robot navigation. arXiv\npreprint arXiv:2409.13682 , 2024. 1, 2\n[2] Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu,\nMithun George Jacob, Tingnan Zhang, Tsang-Wei Ed-\nward Lee, Wenhao Yu, Connor Schenck, David Rendleman,\nDhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete\nFlorence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani,\nCarolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, and\nJie Tan. Mobility vla: Multimodal instruction navigation\nwith long-context vlms and topological graphs. arXiv\npreprint arXiv:2407.07775 , 2024. 1, 2\n[3] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nProc. Computer Vision and Pattern Recognition (CVPR),\nIEEE , 2017. 5\n[4] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,\nDevi Parikh, and Dhruv Batra. Embodied question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2018. 1\n[5] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,\nSong Bai, and Xiaojuan Qi. Pla: Language-driven open-\nvocabulary 3d scene understanding. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 7010–7019, 2023. 2\n[6] Elias Greve, Martin B ¨uchner, Niclas V ¨odisch, Wolfram\nBurgard, and Abhinav Valada. Collaborative dynamic 3d\nscene graphs for automated driving. In 2024 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA) ,\npages 11118–11124, 2024. 2\n[7] Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy\nJatavallabhula, Bipasha Sen, Aditya Agarwal, Corban\nRivera, William Paul, Kirsty Ellis, Rama Chellappa, et al.\nConceptgraphs: Open-vocabulary 3d scene graphs for per-\nception and planning. In 2024 IEEE International Confer-\nence on Robotics and Automation (ICRA) , pages 5021–5028.\nIEEE, 2024. 2\n[8] Daniel Honerkamp, Martin B ¨uchner, Fabien Despinoy, Tim\nWelschehold, and Abhinav Valada. Language-grounded dy-\nnamic scene graphs for interactive object search with mobile\nmanipulation. IEEE Robotics and Automation Letters , 2024.\n2\n--- Page 8 ---\n[9] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram\nBurgard. Visual language maps for robot navigation. In Pro-\nceedings of the IEEE International Conference on Robotics\nand Automation (ICRA) , London, UK, 2023. 2\n[10] Nathan Hughes, Yun Chang, and Luca Carlone. Hy-\ndra: A real-time spatial perception system for 3d scene\ngraph construction and optimization. arXiv preprint\narXiv:2201.13360 , 2022. 1, 2\n[11] Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan,\nChenyang Zhu, Ruizhen Hu, and Kai Xu. Llm-enhanced\nscene graph learning for household rearrangement. In SIG-\nGRAPH Asia 2024 Conference Papers , pages 1–11, 2024. 1,\n3\n[12] Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala,\nChris Paxton, Nur Muhammad Mahi Shafiullah, and Lerrel\nPinto. Dynamem: Online dynamic spatio-semantic mem-\nory for open world mobile manipulation. arXiv preprint\narXiv:2411.04999 , 2024. 2\n[13] Dominic Maggio, Yun Chang, Nathan Hughes, Matthew\nTrang, Dan Griffith, Carlyn Dougherty, Eric Cristofalo,\nLukas Schmid, and Luca Carlone. Clio: Real-time\ntask-driven open-set 3d scene graphs. arXiv preprint\narXiv:2404.13696 , 2024. 2\n[14] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav\nPutta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal,\nPaul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al.\nOpeneqa: Embodied question answering in the era of foun-\ndation models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 16488–\n16498, 2024. 1, 2, 3, 4, 5\n[15] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea\nTagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.\nOpenscene: 3d scene understanding with open vocabularies.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 815–824, 2023. 2\n[16] Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu.\nIntroducing Gemini 2.0: our new AI model for the agen-\ntic era. https://blog.google/technology/\nai / introducing - gemini - 2/ , 2024. Google\nBlog, https://blog.google/technology/ai/\nintroducing-gemini-2/ . 5\n[17] Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee,\nHan pang Chiu, and Alvaro Velasquez. Saynav: Grounding\nlarge language models for dynamic planning to navigation\nin new environments. In 34th International Conference on\nAutomated Planning and Scheduling , 2024. 2\n[18] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-\njmans, Oleksandr Maksymets, Alexander Clegg, John M\nTurner, Eric Undersander, Wojciech Galuba, Andrew West-\nbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv\nBatra. Habitat-matterport 3d dataset (HM3d): 1000 large-\nscale 3d environments for embodied AI. In Thirty-fifth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track , 2021. 5\n[19] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-\nChakra, Ian Reid, and Niko Suenderhauf. Sayplan: Ground-\ning large language models using 3d scene graphs for scalabletask planning. In 7th Annual Conference on Robot Learning ,\n2023. 2\n[20] A. Rosinol, A. Violette, M. Abate, N. Hughes, Y . Chang,\nJ. Shi, A. Gupta, and L. Carlone. Kimera: from SLAM to\nspatial perception with 3D dynamic scene graphs. In arXiv\npreprint arXiv:2101.06894 , 2021. https://arxiv.\norg/pdf/2101.06894.pdf . 2\n[21] Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing\nChen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Fran-\ncis, and Oliver Kroemer. Grapheqa: Using 3d semantic scene\ngraphs for real-time embodied question answering. arXiv\npreprint arXiv:2412.14480 , 2024. 3\n[22] Rutav Shah, Albert Yu, Yifeng Zhu, Yuke Zhu, and Roberto\nMart ´ın-Mart ´ın. Bumble: Unifying reasoning and acting with\nvision-language models for building-wide mobile manipula-\ntion. arXiv preprint arXiv:2410.06237 , 2024. 3\n[23] Ayca Takmaz, Alexandros Delitzas, Robert W Sumner,\nFrancis Engelmann, Johanna Wald, and Federico Tombari.\nSearch3d: Hierarchical open-vocabulary 3d segmentation.\narXiv preprint arXiv:2409.18431 , 2024. 2\n[24] Yujie Tang, Meiling Wang, Yinan Deng, Zibo Zheng,\nJingchuan Deng, and Yufeng Yue. Openin: Open-vocabulary\ninstance-oriented navigation in dynamic domestic environ-\nments. arXiv preprint arXiv:2501.04279 , 2025. 2\n[25] Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou,\nShuai Liang, Xing Hu, Yinhe Han, and Yiming Gan. Karma:\nAugmenting embodied ai agents with long-and-short term\nmemory systems. arXiv preprint arXiv:2409.14908 , 2024.\n2\n[26] Abdelrhman Werby, Chenguang Huang, Martin B ¨uchner,\nAbhinav Valada, and Wolfram Burgard. Hierarchical open-\nvocabulary 3d scene graphs for language-grounded robot\nnavigation. In First Workshop on Vision-Language Models\nfor Navigation and Manipulation at ICRA 2024 , 2024. 3, 4\n[27] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff,\nDefu Lian, and Jian-Yun Nie. C-pack: Packed resources for\ngeneral chinese embeddings, 2024. 4\n[28] Quanting Xie, So Yeon Min, Tianyi Zhang, Kedi Xu, Aarav\nBajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson,\nand Yonatan Bisk. Embodied-rag: General non-parametric\nembodied memory for retrieval and generation. arXiv\npreprint arXiv:2409.18313 , 2024. 2, 3\n[29] Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen,\nHongxin Zhang, Yilun Du, and Chuang Gan. 3d-mem:\n3d scene memory for embodied exploration and reasoning.\narXiv preprint arXiv:2411.17735 , 2024. 2, 3, 5\n[30] Mike Zhang, Kaixian Qu, Vaishakh Patil, Cesar Cadena, and\nMarco Hutter. Tag map: A text-based map for spatial rea-\nsoning and navigation with large language models. arXiv\npreprint arXiv:2409.15451 , 2024. 3",
  "text_length": 35530
}