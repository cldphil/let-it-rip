{
  "id": "http://arxiv.org/abs/2506.04108v1",
  "title": "Rectified Sparse Attention",
  "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
  "authors": [
    "Yutao Sun",
    "Tianzhu Ye",
    "Li Dong",
    "Yuqing Xia",
    "Jian Chen",
    "Yizhao Gao",
    "Shijie Cao",
    "Jianyong Wang",
    "Furu Wei"
  ],
  "published": "2025-06-04T16:01:48Z",
  "updated": "2025-06-04T16:01:48Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04108v1",
  "full_text": "--- Page 1 ---\nRectified Sparse Attention\nYutao Sun‚àó12Tianzhu Ye‚àó12Li Dong‚àó1Yuqing Xia‚àó1\nJian Chen1Yizhao Gao13Shijie Cao1Jianyong Wang2Furu Wei1‚ãÑ\n1Microsoft Research2Tsinghua University\n3The University of Hong Kong\nhttps://aka.ms/GeneralAI\nAbstract\nEfficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with periodic\ndense rectification. By refreshing the KV cache at fixed intervals using a dense\nforward pass, ReSA bounds error accumulation and preserves alignment with the\npretraining distribution. Experiments across math reasoning, language modeling,\nand retrieval tasks demonstrate that ReSA achieves near-lossless generation quality\nwith significantly improved efficiency. Notably, ReSA delivers up to 2.42 √óend-\nto-end speedup under decoding at 256K sequence length, making it a practical\nsolution for scalable long-context inference. Code is available at https://aka.\nms/ReSA-LM .\n1 Introduction\nThe ability to process long contexts has become a core requirement for Large Language Models, with\ncontext lengths up to millions of tokens [ 19,25]. In particular, long sequence generation has received\ngrowing attention, especially due to the demand for test-time scaling [8, 12].\nDespite this progress, efficient long-sequence generation remains a significant challenge. In standard\nautoregressive decoding, each token must attend to the full KV cache, leading to frequent memory\naccess and increased IO pressure. This bottleneck severely limits throughput, especially in long-\ncontext scenarios where memory access dominates latency.\n0 1000 2000 3000 4000\nDecoding Length0.760.770.780.790.80T op-3 Accuracy\n(Language Modeling)\nDense Attention\nBlock Sparse Attention\nFigure 1: Sparse decoding perfor-\nmance becomes worse with increas-\ning decoding length due to error ac-\ncumulation of KV cache.Recent works [ 17,22] used sparse decoding to alleviate this\nissue. These methods selectively attend to a subset of the con-\ntext, achieving accuracy comparable to dense attention on long\ninputs while reducing computational cost. However, as shown\nin Figure 1, they often suffer from worse performance with\nincreasing length. Since computation errors accumulate in\nthe KV cache during sparse decoding , the attention compu-\ntation will suffer from the misalignment between training and\ninference, contributing to performance degradation.\nIn this work, we propose Rectified Sparse Attention (ReSA), a\nsimple yet effective approach that achieves near-lossless long-\nsequence generation quality while maintaining high inference\nefficiency. ReSA leverages block-sparse attention [22] for fast\n‚àóEqual contribution. ‚ãÑCorresponding author.\nPreprint. Under review.arXiv:2506.04108v1  [cs.CL]  4 Jun 2025\n--- Page 2 ---\nPrefilling\nBlock Sparse Decoding\nDense Rectification\nBlock Sparse Decodingùë°=1\nùë°=2\nùë°=3\nùë°=4\nAttended Token Ignored Token Cached by Dense Attention Cached by Sparse AttentionRefresh KV caches \nwith dense attention \nevery ùëìtimesteps  (only \nfor the last ùëìtokens )Figure 2: Overview of ReSA. After completing the prefill stage, the model enters sparse decoding.\nOnce the number of generated tokens reaches the rectification frequency, a rectification step is\nperformed to construct a lossless compact KV cache, after which sparse decoding resumes.\nretrieval and further improves memory efficiency by applying\nshared grouping [ 27], allowing query heads to reuse attention patterns. To address the error accumula-\ntion issue, we introduce dense rectification, where the sparse KV cache is periodically refreshed with\na parallel dense forward pass. This ensures that approximation errors are bounded within a constant\nrange, preventing long-term degradation.\nWe conduct comprehensive experiments to demonstrate the effectiveness of ReSA. On math reasoning\nbenchmarks, ReSA achieves strong test-time scaling and matches dense attention in long-sequence\nsettings. In language modeling, ReSA significantly closes the quality gap between sparse and dense\ndecoding. On the efficiency side, our approach yields up to 2.42 √óend-to-end speedup under INT4\ndecoding at 256K context length, showing strong practical utility for real-world deployment.\n2 Rectified Sparse Attention\nReSA primarily involves two alternating phases, sparse decoding and periodic rectification. During\nthe decoding phase, we employ the group block sparse attention mechanism, which significantly\nreduces computational and memory overhead, enabling fast autoregressive inference. During the\nrectification stage, the decoding tokens are forwarded in parallel to correct approximation errors\nin KV cache introduced by sparse decoding. By alternating between sparse generation and dense\nrectification, ReSA enables scalable long-context inference while ensuring the generation quality.\n2.1 Group Block Sparse Attention\nSelf-attention mechanisms are the core component of Transformer architectures, enabling each token\nto attend to all previous tokens. Formally, in Group-Query Attention (GQA) [ 2], given a sequence\nofntokens, we compute the query Q‚ààRh√óg√ón√ód, key K‚ààRh√ón√ód, and value V‚ààRh√ón√ód\nmatrices through learned projections. The attention output is computed as:\nAttention( Q, K, V )ij= softmax\u0012QijK‚ä§\ni‚àö\nd\u0013\nVi (1)\nwhere softmax( ¬∑)is applied along each query row. The pairwise computation requires O(n2d)\noperations, making standard attention prohibitively expensive for long-context inference.\nWe adopt a block-sparse attention design that selectively attends to a small number of relevant\nmemory blocks rather than the entire context. Given the block size band block sparse mask\nM‚àà {0,1}h√ón√ón/b, the block-sparse attention is computed as:\nGBSA( Q, K, V, M )ij= softmax\u0012QijK‚ä§\ni¬∑Mi‚àö\nd\u0013\nVi,Mijk=Mij‚åäk/b‚åã (2)\n2\n--- Page 3 ---\nBlock 1Block 2Block 3Block N-1Block N\n¬∑¬∑¬∑\nSelect Top-!Blocks for Sparse AttentionLocal WindowCurrent QueryHead 1Head 2Head 3Head 4PoolingKV CacheGroup \"Figure 3: Overview of Group Block Sparse Attention. For each group of query heads, we perform\naverage pooling and enforce the selection of the same KV blocks across all heads within the group.\nGBSA adopts a query-dependent sparsity pattern, where each query attends to a limited set of key\nblocks determined by M. Since each selected key block corresponds to a contiguous memory region\nin the KV cache, this design ensures both high performance and memory efficiency during inference.\nNote that we further accelerate decoding by maintaining a shared sparse pattern within each GQA\ngroup [27].\nBlock Representation Following the Quest algorithm [ 22], we represent the key-value memory\nusing blocks to enable efficient retrieval. Specifically, given a key matrix k‚ààRn√ód, we partition it\ninto non-overlapping blocks of size b, where each block contains bconsecutive tokens. For the i-th\nblock, we compute two block descriptors:\nkblock_min ,i= min( kib:(i+1)b), kblock_max ,i= max( kib:(i+1)b) (3)\nwhere min(¬∑)andmax(¬∑)are applied element-wise across the block dimension.\nEach block is thus summarized by a pair of vectors (kblock min ,i, kblock max ,i), which compactly describe\nthe distribution range of keys within the block. This representation allows efficient approximate\nmatching without exhaustively scanning all individual tokens. During decoding, newly generated\nkeys can be incrementally incorporated by updating the block key statistics, enabling an online update\nmechanism without recomputing from scratch.\nNotably, the block representation is entirely training-free, relying solely on statistical descriptors. Our\nmethod remains compatible with more advanced block representation strategies, such as SeerAtten-\ntion [ 7], where block keys are fine-tuned jointly with the model to achieve higher retrieval precision\nif needed.\nBlock Selection During decoding, given a pooling query q‚ààRdfor each GQA group and a set of\nblock descriptors {(kblock_min ,i, kblock_max ,i)}M\ni=1, we compute similarity scores following the Quest\nalgorithm [22]. Specifically, the score between the pooling query and block iis calculated as:\nscore i=dX\nj=1max(qj√ó(kblock_max ,i)j, qj√ó(kblock_min ,i)j) (4)\nwhere qjdenotes the j-th dimension of the pooling query, and (kblock min ,i)j,(kblock max ,i)jare the\nj-th dimensions of the minimum and maximum vectors of block i, respectively.\nTo select the attended blocks, we adopt a dynamic top- nstrategy. First, a fixed number of recent\nblocks, denoted as nlocal, are always preserved by setting their scores to +‚àû, ensuring that the\nlatest context is available for local coherence. Second, we enforce a minimal block number nminto\navoid significant performance degradation on short sequences. Finally, the value of nis dynamically\ndetermined based on a active ratio p, following:\nn= max ( nmin,‚åàM√óp‚åâ), (5)\nwhere Mis the total number of available memory blocks. Attention computation is restricted to the\nselected blocks, significantly reducing memory accesses while maintaining retrieval quality.\n2.2 Dense Rectification\nTransformer inference implicitly consists of two distinct phases: context encoding , realized through\nthe construction of the KV cache, and next-token prediction , realized through the forward pass of\n3\n--- Page 4 ---\nAlgorithm 1 Rectified Sparse Decoding\nRequire: Initial prompts P, model M, rectification frequency f, maximum generation steps T\nEnsure: Generated tokens G\nInitialize KV cache KbyPrefill (P,K)\nInitialize block key cache B\nInitialize output sequence G ‚Üê empty\nfori= 1toTdo\nt‚ÜêSparseForward (G[i‚àí1],K,B)\nAppend ttoG\nUpdate KV cache Kwitht\nUpdate block key cache Bwitht\nifimodf= 0then\nK,B ‚Üê DenseForward (G[i‚àíf:i],K,B)\nUpdate block key cache B\nend if\nend for\nthe current token. While sparse attention effectively approximates the next-token prediction phase, it\ninevitably introduces errors. Crucially, these prediction errors accumulate in the KV cache during\ndecoding, leading to compounding inaccuracies over long sequences. To mitigate this issue, we\npropose Dense Rectification , a lightweight mechanism that periodically refreshes the KV cache to\nmaintain its quality. This design constrains error accumulation within a constant window size and\nenables efficient sparse decoding without compromising generation consistency.\nRectification Algorithm Given a rectification frequency f, we perform standard sparse decoding\nfor up to ftokens, appending newly generated tokens into the KV cache. After every ftoken, we\nbatch these recent tokens and re-encode them using dense attention to reconstruct an updated KV\ncache. This two-phase approach ‚Äî serial sparse decoding followed by parallel rectification ‚Äî ensures\nthat errors introduced by approximate attention are corrected at regular intervals, keeping the memory\nquality close to that of dense decoding. Importantly, the rectification step amortizes efficiently\nover large batches, maintaining high throughput even when dense recomputation is involved. To\nmaintain consistency, we also refresh the associated block keys during rectification. otherwise, the\nmisalignment between the block keys and the updated KV cache would degrade subsequent sparse\nretrieval accuracy.\nCompatibility with LLM Serving Systems Dense Rectification is naturally compatible with\nmodern LLM serving optimizations such as continuous batching [ 26] and chunked prefill [ 1,11].\nSince rectification only requires periodic batched re-encoding, it seamlessly fits into systems that\ndynamically group decoding and prefill workloads to maximize GPU utilization. By maintaining a\nfixed rectification frequency per request, our method can operate within the batching and scheduling\npipelines without introducing special synchronization barriers or inefficiencies.\n2.3 Decoding Procedure\nOur decoding procedure alternates between sparse decoding and periodic rectification to achieve\na balance between efficiency and generation quality. The process begins with a standard dense\nprefill phase, where the initial prompt is encoded into a complete key-value memory for subsequent\ndecoding. During the decoding phase, tokens are generated sequentially using sparse attention,\nwhich restricts memory access to a dynamically selected subset of context blocks. This enables fast\nautoregressive generation with reduced computational and memory costs.\nTo correct for approximation errors introduced by sparse attention, we periodically perform rectifi-\ncation. Specifically, after a fixed number of decoding steps, we batch the recently generated tokens\nand re-encode them using dense attention. This refreshes the key-value memory and ensures that\naccumulated errors are bounded within a constant window, maintaining memory quality close to\ndense decoding. The full decoding procedure is summarized in Algorithm 1.\n4\n--- Page 5 ---\nThe pipeline continues by alternating between sparse generation and rectification until the genera-\ntion process completes. This design enables scalable long-context inference while preserving the\nconsistency and reliability of the generated outputs.\nMemory Access Analysis We further analyze the memory efficiency of the proposed decoding\npipeline. In each sparse decoding step, the memory access consists of two parts: retrieving block keys\nfor selection, proportional to mem( KV cache )/b, and performing sparse attention, proportional to\nmem( KV cache )√óp, where bdenotes the block size and pdenotes the sparsity ratio. In addition, for\nevery fstep, a dense rectification is performed, whose amortized cost per step is mem( KV cache )/f.\nTherefore, the average memory access per decoding step can be approximated as:\nAvg(mem) = mem( KV cache )√ó\u00121\nb+p+1\nf\u0013\n. (6)\nCompared to dense decoding, which requires accessing the entire KV cache at every step, our design\nachieves a theoretical memory access reduction factor of1\nb+p+1\nf. By adjusting b,p, andf, the\npipeline can flexibly trade-off between memory efficiency and generation fidelity.\n2.4 Kernel Implementation\nWe develop a custom kernel optimized for the decoding phase, following a split-execution strategy\nsimilar to Flash Decoding and incorporating shared KV fetching techniques [ 27]. The key design\nprinciple is to assign each GQA group to an individual streaming multiprocessor (SM), ensuring\nefficient resource utilization and minimal inter-SM communication.\nThe decoding workload is batch _size√ónum _kv_heads . Given the total number of SMs available on\nthe GPU, the workload is split accordingly to balance the computation between SMs. The splitting is\nperformed at the level of block indices. For each decoding step, a batch of queries typically activates\nkmemory blocks. We partition these kactive blocks evenly across the available SMs, so that each\nSM is responsible for approximately k/split blocks. Each SM independently fetches the required\nKV entries corresponding to its assigned blocks and performs sparse attention locally. The kernel\nimplementation details are described in ??.\nThe design achieves high decoding throughput by minimizing memory contention, maximizing SM\noccupancy, and fully exploiting intra-GQA key sharing during sparse decoding.\n3 Experiments\n3.1 Setup\nWe evaluate ReSA from different perspectives. First, we make test-time scaling inference on math\nreasoning tasks (Section 3.2). Second, we simulate inference-time attention pattern on language\nmodeling (Section 3.3). Third, we verify the effectiveness on retrieval (Section 3.4) tasks. Fourth, we\nanalyze the inference advantages (Section 3.5, including kernel-level and end-to-end accelerations.\nWe choose Qwen2.5 [ 24], a widely-used standard Transformer pre-trained model as evalutaion\narchitectures. We apply ReSA on all of the layers, rather than skipping the first two layers in\nQuest [ 22]. The block size is 16 and the minimal selected block number is nmin= 16, nlocal= 1to\navoid performance degradation in short context. For longer sequences, the default sparsity ratio is\np= 0.9. The default rectification frequency is f= 32 .\n3.2 Long Reasoning\nWe evaluate test-time scaling performance on math reasoning tasks. The validation dataets inclue\nMinerva Math [ 14], Gaokao 2023 En [ 16], OlympiadBench [ 9], AIME24, and AMC23. We exclude\nsome well-known math datasets such as GSM8K [ 5], and MATH [ 10] since these datasets‚Äô average\ninference length is below 512. We choose DeepSeek-R1-Qwen-Distill 7B [ 8] as the evaluation model.\nThe number of attention head is 28 and kv head is 4. The hidden size is 3584 and the number of\nlayers is 28.\nThe results in Table 1 show that while ReSA achieves performance comparable to the dense baseline,\nSparse Decoding alone consistently underperforms. ReSA maintains near-lossless performance in\n5\n--- Page 6 ---\nMinerva Gaokao2023En OlympiadBench AIME24 AMC23 Avg\nR1-Qwen-Distill 1.5B\nDense 28.7 71.6 40.8 27.4 65.6 46.82\nSparse 29.0 67.9 38.7 21.3 60.6 43.50\nReSA 28.1 71.8 39.5 23.0 65.4 45.56\nAvg Length 6390.8 4915.8 8991.6 12126.4 7866.4 8058.2\nR1-Qwen-Distill 7B\nDense 40.4 73.8 52.3 48.1 89.0 60.72\nSparse 38.1 72.9 48.4 46.1 83.1 57.72\nSparse dense2 37.9 72.5 48.8 44.6 83.1 57.38\nReSA 39.7 73.5 52.3 51.1 86.0 60.52\nAvg Length 4018.7 2889.9 7520.0 10474.5 5732.2 6127.1\nTable 1: Performance comparison on math reasoning tasks. While simple sparse decoding methods\nshow a gap with dense decoding, ReSA achieves near lossless long-sequence generation.\n8K 16K 32K 48K 64K\nSequence Length0.700.720.740.760.780.80T op-3 Accuracy\nDense\nDecode Only\nReSA (Ours)\nSparse\nFigure 4: Top-3 next-token prediction accu-\nracy with different rectification frequency.\n8K 16K 32K 48K 64K\nSequence Length0.700.720.740.760.780.80T op-3 Accuracy\nDense\nReSAp=0.2\nReSAp=0.1\nReSAp=0.05Figure 5: Top-3 next-token prediction accu-\nracy with different sparsity ratio.\nlong-context reasoning tasks, whereas Sparse Decoding leads to performance degradation as decoding\nprogresses. Additionally, manually enforcing dense layers for the first two layers does not result in a\nsignificant improvement in math-reasoning tasks.\n3.3 Language Modeling\nSe evaluate language modeling performance under simulated sparse decoding patterns. Specifically,\nwe divide each input sequence into two parts. Given a total sequence length L, we split it into a prefix\nof length L‚àíxand a suffix of length x. The prefix is processed using dense attention, while the\nsuffix uses sparse attention. Here, xeffectively controls the rectification frequency. When x=L, it\ncorresponds to the sparse decoding baseline, where no rectifying is performed and the entire sequence\nis encoded using sparse attention.\nWe conduct our experiments using long-sequence book data. For each target sequence length, we\nuse the same data and truncate from the left to ensure that the prediction tokens are perfectly aligned\nacross all settings. We report the top-3 accuracy computed over the final 32 tokens of each sequence\nto focus on the model‚Äôs performance in the later decoding stages.\nAs shown in Figure 4, we compare the impact of different rectification frequencies on model perplexity.\nThe setting labeled Decode Only corresponds to the case where all KV cache entries are generated\nusing dense attention, and sparse attention is only used for decoding. This serves as the upper bound\nfor ReSA. We observe that ReSA significantly reduces the performance gap between dense and sparse\ndecoding. Notably, when x= 32 , the model‚Äôs performance almost approaches the upper bound,\n6\n--- Page 7 ---\nSetting QA MultiQuery FWE VT MultiKey MultiValue CWE Single Avg\nDense 0.563 0.211 0.833 0.719 0.688 0.246 0.134 1.000 0.549\nReSA p=0.95 0.500 0.180 0.740 0.719 0.750 0.238 0.125 1.000 0.531\nReSA p=0.9 0.625 0.203 0.760 0.719 0.750 0.234 0.178 1.000 0.559\nReSA p=0.8 0.594 0.195 0.771 0.719 0.719 0.246 0.175 1.000 0.552\nTable 2: RULER benchmarks under different sparsity ratios. Dense represents the fully-attended\nbaseline, while ReSA p=xdenotes our method with sparsity level x.\ndemonstrating the effectiveness of rectification in mitigating the error accumulation issue inherent in\nsparse decoding.\nIn Figure 5, we further examine the effect of different sparsity ratios under a fixed rectification\nfrequency of x= 32 . We find that there is a noticeable performance gap between the p= 0.98and\np= 0.95. Although p= 0.8sparsity achieves perplexity comparable to the dense setting, we adopt\np= 0.9as the default due to its better trade-off between performance and efficiency. Additionally,\nsince effective block selection strategies can lead to higher achievable sparsity, our method can be\nfurther combined with advanced attention selection mechanisms such as SeerAttention [ 7] to enhance\nruntime efficiency.\n3.4 Long-Sequence Retrieval\nWe conduct experiments on the RULER benchmark to further evaluate the impact of different sparsity\nlevels. Unlike the long-sequence generation tasks, where rectification plays a critical role in mitigating\ncumulative error, the RULER benchmark focuses on relatively short output sequences. As a result,\nthe final accuracy is primarily determined by the quality of the sparse attention estimation.\nResults are presented in Table 2. We observe that as the sparsity ratio increases from p= 0.95top=\n0.9, there is a consistent improvement in average accuracy, with ReSA p=0.9achieving comparable\nperformance to the dense baseline (0.559 vs. 0.549). The performance under p= 0.8remains similar\nto that under p= 0.9, indicating that moderate increases in sparsity do not substantially degrade\naccuracy in short-generation settings. Considering that a lower sparsity ratio generally leads to\nfaster inference, ReSA p=0.9represents a better trade-off between performance and efficiency on the\nRULER benchmark.\n3.5 Inference Efficiency\nWe evaluate the efficiency of ReSA on standard GPU hardware. Specifically, we use Qwen-2.5 7B\nas the evaluation model and conduct all experiments on NVIDIA A100-80G GPUs. The primary\nbaseline is FlashAttention, a highly optimized dense attention implementation. To ensure a fair\ncomparison and prevent memory overflow issues caused by excessively large KV caches during\nlong-sequence evaluation, we adopt a shared KV cache strategy across all layers during efficiency\nmeasurements. The batch size is fixed at 8 by default throughout all experiments.\nFor latency measurement, we report the CUDA kernel execution time, excluding CPU-side scheduling\noverhead. This setup more accurately reflects the real-world inference scenario, as the CPU overhead\ncan be effectively optimized away through techniques such as CUDA graph capture.\n3.5.1 Attention Efficiency\nFigure 6 shows the detailed latency breakdown across different sequence lengths ( 16k,64k, and 256k\ntokens). We compare ReSA, and dense attention under the same settings. The latency is decomposed\ninto three parts: sparse estimation, attention computation, and rectification overhead.\nCompared to dense attention, ReSA significantly reduces the total latency, especially at longer\nsequence lengths. As the sequence grows, dense attention exhibits longer latency with increasing\ncontext length, leading to substantial latency increase, while ReSA maintains much flatter scaling\ndue to its sparsified attention computation.\n7\n--- Page 8 ---\n0.0 2.5 5.0 7.5 10.0\nLatency (ms)DenseReSASequence Length: 16384\n0 10 20\nLatency (ms)Sequence Length: 65536\n0 20 40 60\nLatency (ms)Sequence Length: 262144\nSparse Estimation Attention RectificationFigure 6: Kernel-level latency breakdown across different sequence lengths. While Sparse Decoding\nachieves effective acceleration, rectification only requires a small additional overhead.\nMoreover, sparse estimation and attention computation consume comparable amounts of time,\nbecause the memory access pattern for sparse estimation scales with mem (KV cache )/block, while\nfor attention it scales with mem(KV cache )√óp. Given our experimental settings ( block = 16 ,\np= 0.9), both operations operate on similar memory volumes. Notably, under fixed block size,\nfurther increasing the sparsity ratio can not bring significant speed-up.\nThe overhead of rectification is relatively small compared with sparse decoding part. Specifically, the\nrectification module accounts for up to 32.7% of the total attention-related latency at 256k lengths,\nwhile at 64k, this proportion drops to 28.9%. When the sequence length is scaling, the latency ratio\nwill converge to the memory access ratio 1/f. These results indicate that while sparse estimation and\nattention computation remain efficient, the rectification does not bring big overhead.\n3.5.2 End-to-End Efficiency\nWe further evaluate the end-to-end throughput of ReSA in both FP16 and INT4 precision settings.\nFor the INT4 experiments, we leverage the Marlin kernel [ 6] for low-bit matmul. The matmul weight\nis quantized with group-wise scaling. The group size is 128.\nFigure 7 and Figure 8 report the throughput across different context lengths (4K, 16K, 64K, and 256K\ntokens) under FP16 and INT4 settings, respectively. Consistent with the kernel-level results, ReSA\nsignificantly improves the overall throughput as the sequence length grows, achieving up to 2.28√ó\nspeedup over dense attention in FP16 and 2.44√óin INT4 at 256K context length.\nNotably, the benefits of ReSA become more prominent at longer sequences due to the quadratic\nscaling bottleneck of dense attention, while the overhead of sparse estimation and rectification remains\nmodest even under quantized inference. These results demonstrate that ReSA is highly effective in\nimproving real-world end-to-end generation speed across different precision levels.\n4K 16K 64K 256K\nContext Length020406080Latency\n1.02x1.11x1.47x2.18xDense\nReSA\nFigure 7: End-to-end latency with FP16.\n4K 16K 64K 256K\nContext Length020406080Latency\n1.03x1.18x1.7x2.44xDense\nReSA Figure 8: End-to-end latency with INT4.\n8\n--- Page 9 ---\n0.98 0.95 0.90\nSparsity Ratio70.072.5Accuracy (%)\nGaokao2023En\n0.98 0.95 0.90\nSparsity Ratio4550\nOlympiadBench\n0.98 0.95 0.90\nSparsity Ratio304050\nAIME24\n0.98 0.95 0.90\nSparsity Ratio758085\nAMC23\nDense Sparse ReSAf=16 ReSAf=32 ReSAf=64 ReSAf=128Figure 9: Ablation studies on different rectification frequencies fand sparsity ratios pacross five\nmath reasoning benchmarks. ReSA consistently improves over the sparse baseline. Frequencies\nf= 32 orf= 64 achieve the best trade-off between performance and overhead.\n3.6 Ablation Studies\nWe conduct ablation studies to examine the effect of rectification frequency and sparsity ratio on\nperformance. As shown in Figure 9, we evaluate ReSA across five math reasoning benchmarks under\nvarying sparsity levels ( p‚àà {0.9,0.95,0.98}) and rectification frequencies ( f‚àà {16,32,64,128}).\nCompared to the sparse decoding baseline, ReSA consistently outperforms the baseline across all\nsparsity levels. Notably, when the attention computation ratio is reduced to 0.1, ReSA achieves\naccuracy that is remarkably close to the dense decoding upper bound. This demonstrates that ReSA\neffectively mitigates the quality drop typically associated with sparse decoding while maintaining\nhigh computational efficiency.\nAmong the frequencies, f= 32 achieves accuracy close to the dense baseline on most datasets,\nstriking a favorable balance between quality and efficiency. While f= 16 offers marginal gains,\nit incurs higher rectification overhead and is therefore less practical. Notably, even with f= 128 ,\na large portion of the performance gain is retained, highlighting the robustness of the rectification\nmechanism under infrequent updates.\n4 Related Work\nSparse Attention Recent efforts in sparse decoding for large language models can be broadly cate-\ngorized into training-free and training-aware approaches. Training-free methods enhance inference\nefficiency without substantial retraining. Quest [ 22] and InfLLM [ 23] both adopt query-aware block-\nsparse attention, selectively retrieving critical memory blocks based on query relevance. MagicPig [ 4]\nand ClusterKV (Tactic) [ 17] employ similarity-based techniques, using hashing or clustering to\napproximate attention relevance. In contrast, training-aware architectures such as NSA [ 27] and\nMoBA [ 18] integrate sparsity into model design, aligning structures with hardware during pretrain-\ning. Our method complements training-free sparse attention by improving memory quality through\nlightweight rectification, avoiding the high retraining cost required by training-aware approaches.\nSpeculative Decoding Speculative decoding [ 13] accelerates generation by drafting multiple tokens\nand verifying them with the target model. Methods like Medusa [ 3] and EAGLE [ 15] reuse the\ntarget model‚Äôs hidden states for drafting. TriForce [ 21] and MagicDec [ 20] propose self-speculation,\nusing the model‚Äôs own sparse KV cache for drafting and a dense cache for verification. While\nsharing similar compute characteristics with sparse KV-based self-speculation, ReSA avoids per-\ntoken accept/reject decisions and resampling overhead. In Appendix ??, we compare ReSA and\nself-speculation in detail.\n5 Conclusion\nIn this paper, we introduced Rectified Sparse Attention, a simple yet effective method for efficient\nlong-sequence generation. ReSA combines group block sparse attention for decoding latency, and\ndense rectification to bound error accumulation. Extensive experiments on math reasoning and\nlanguage modeling tasks demonstrate that ReSA achieves near-lossless performance compared to\ndense decoding, delivering up to 2.42 √óinference speedup at 256K context length. These results\nhighlight ReSA‚Äôs practical effectiveness in long-context language model deployment.\n9\n--- Page 10 ---\nAcknowledgments\nWe would like to thank Lei Wang and Yu Cheng for their valuable help implementing group block\nsparse attention with the TileLang library.\nReferences\n[1]Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and\nRamachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked\nprefills. arXiv preprint arXiv:2308.16369 , 2023.\n[2]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr√≥n,\nand Sumit Sanghai. Training generalized multi-query transformer models from multi-head\ncheckpoints. arXiv preprint arXiv:2305.13245 , 2023.\n[3]Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri\nDao. Medusa: Simple llm inference acceleration framework with multiple decoding heads.\narXiv preprint arXiv:2401.10774 , 2024.\n[4]Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte,\nYuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, et al. Magicpig: Lsh sampling for\nefficient llm generation. arXiv preprint arXiv:2410.16179 , 2024.\n[5]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n[6]Elias Frantar, Roberto L Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. Marlin:\nMixed-precision auto-regressive parallel inference on large language models. arXiv preprint\narXiv:2408.11743 , 2024.\n[7]Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai,\nHayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse\nattention in your llms. arXiv preprint arXiv:2410.13276 , 2024.\n[8]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n[9]Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu,\nXu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for\npromoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint\narXiv:2402.14008 , 2024.\n[10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2) , 2021.\n[11] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam\nRajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, et al.\nDeepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference.\narXiv preprint arXiv:2401.08671 , 2024.\n[12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv\npreprint arXiv:2412.16720 , 2024.\n[13] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\nspeculative decoding. In International Conference on Machine Learning , pages 19274‚Äì19286.\nPMLR, 2023.\n10\n--- Page 11 ---\n[14] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving\nquantitative reasoning problems with language models. Advances in Neural Information\nProcessing Systems , 35:3843‚Äì3857, 2022.\n[15] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling\nrequires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 , 2024.\n[16] Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code\ninterpreter output‚Äìa reproducible pipeline. arXiv preprint arXiv:2401.08190 , 2024.\n[17] Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, and Minyi Guo. Clusterkv: Manipulating\nllm kv cache in semantic space for recallable compression. arXiv preprint arXiv:2412.03213 ,\n2024.\n[18] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran\nHe, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms.\narXiv preprint arXiv:2502.13189 , 2025.\n[19] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-\nbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv\npreprint arXiv:2403.05530 , 2024.\n[20] Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi,\nIan En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latency-\nthroughput tradeoff for long context generation with speculative decoding. arXiv preprint\narXiv:2408.11049 , 2024.\n[21] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless\nacceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint\narXiv:2404.11912 , 2024.\n[22] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest:\nQuery-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774 ,\n2024.\n[23] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan\nLiu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an\nefficient context memory. arXiv preprint arXiv:2402.04617 , 2024.\n[24] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[25] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang,\nJianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint\narXiv:2501.15383 , 2025.\n[26] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.\nOrca: A distributed serving system for Transformer-based generative models. In 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22) , pages 521‚Äì538, 2022.\n[27] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda\nXie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and\nnatively trainable sparse attention. arXiv preprint arXiv:2502.11089 , 2025.\n11",
  "text_length": 36416
}