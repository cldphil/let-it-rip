{
  "id": "http://arxiv.org/abs/2506.05345v1",
  "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
  "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
  "authors": [
    "Adrian ÅaÅ„cucki",
    "Konrad Staniszewski",
    "Piotr Nawrot",
    "Edoardo M. Ponti"
  ],
  "published": "2025-06-05T17:59:55Z",
  "updated": "2025-06-05T17:59:55Z",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05345v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05345v1  [cs.LG]  5 Jun 2025\nInference-Time Hyper-Scaling with KV Cache\nCompression\nAdrian ÅaÅ„cuckiâ€ Konrad Staniszewskiâ€ Piotr Nawrotâ—‡*Edoardo M. Pontiâ€ â—‡\nâ€ NVIDIAâ—‡University of Edinburgh\nInference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel\nsequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the keyâ€“value\n(KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling : by\ncompressing the KV cache, we can generate more tokens within the same compute budget and further improve\nthe accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce\nDynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K\ntraining steps to achieve 8 Ã—compression, while maintaining better accuracy than training-free sparse attention.\nInstead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations\nand preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with\nDMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and\nmemory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA,\nand 9.6 on LiveCodeBench across compute budgets.\n1. Introduction\nScaling inference-time computeâ€”employed in mod-\nels such as OpenAIâ€™s o1 (OpenAI et al., 2024) or\nDeepSeekâ€™sR1(Guoetal.,2025)â€”tradesoffincreased\ninference time and memory for higher reasoning ac-\ncuracy in large language models (LLMs). Models\nreason by generating intermediate steps that explore\nthe problem before reaching an answer. Adjusting\nthe depth and breadth of this explorationâ€”known as\nsequential and parallel scaling, respectively (Muen-\nnighoff et al., 2025)â€”controls the inference-time com-\npute budget (Yao et al., 2023; Uesato et al., 2022;\nWang et al., 2023; Lightman et al., 2024).\nDespite its success, scaling inference-time compute\nis fundamentally bottlenecked in Transformer LLMs\nby the number of tokens from the keyâ€“value (or KV)\ncache that are attended to during auto-regressive\ngeneration. This cache grows linearly with respect\nto the length and number of reasoning chains, as\nthe new keyâ€“value representations are appended to\nit. Hence, it can easily exhaust the memory of the\naccelerator and slow down each generation step, as\nattention is memory-bound: its cost is dominated\nby the time needed to retrieve the cache from mem-\nory. Fortunately, several methods can mitigate these\nissues during post-training or inference. These rely\non training-free heuristics to sparsify the KV cache\n(Oren et al., 2024; Li et al., 2024), selectively retrieve\nsubsets of tokens (Tang et al., 2024), or retrofit LLMs\nwith the ability to choose whether to merge or append\nitems to the cache (Nawrot et al., 2024).\n         GPQA \nLiveCodeBench\nAIME24         \nMATH 50024681012R1-Qwen 2.5 1.5B\nR1-Qwen 2.5 7B\nR1-Qwen 2.5 32BFigure 1|Average absolute improvement of DMS over\nthe original LLMs during inference-time scaling on\nreasoning tasks within the same KV cache budget,\nwhich is a proxy for runtime and memory load.\nIn this work, we investigate for the first time\nwhether efficient attention methods enhance inference-\ntime scaling. In principle, by exploring more con-\ncurrent reasoning threads or longer threads for the\nsame memory or runtime budget, efficient models can\nachieve higher-quality predictions than their original\ncounterparts. However, this hinges upon the crucial\nassumption that efficient attention does not degrade\nthe modelâ€™s reasoning abilities, which unfortunately\nis often the side effect of training-free sparsification\nmethods (Zhang et al., 2023a; Oren et al., 2024). On\nthe other hand, KV cache compression during post-\n*Work done as an intern at NVIDIA.\n--- Page 2 ---\nInference-Time Hyper-Scaling with KV Cache Compression\ntraining usually better preserves the modelâ€™s qual-\nity, but also demands costly retrofitting procedures\n(Nawrot et al., 2024).\nIn order to overcome these limitations, as a sec-\nond main contribution, we propose Dynamic Memory\nSparsification (DMS), a new method that combines\nthe best of both worlds by retrofitting LLMs to spar-\nsify the KV cache through an inexpensive procedure.\nWe thus demonstrate that sparsificationâ€”rather than\nmore complex token merging proposed in Dynamic\nMemory Compression (DMC; Nawrot et al., 2024)â€”is\nsufficient to retain performance at high compression\nratios if the model is made aware of the eviction de-\ncisions a certain number of steps in advance. This,\nin turn, allows us to retrofit LLMs with KV cache\ncompression in a much more sample-efficient way\nthan DMC, achieving 8 Ã—compression with only 1K\ntraining steps. On the other hand, the superior perfor-\nmance of DMS highlights the benefits of retrofitting\nLLMs over training-free heuristics.\nWe evaluate inference-time scaling capabilities of\nefficient attention (including DMS) on reasoning\ndatasets such as MATH-500 (Hendrycks et al., 2021b)\nand AIME 2024 for math, GPQA Diamond (Rein\net al., 2024) for hard sciences, and LiveCodeBench\n(Jain et al., 2025) for coding. We find that DMS\nsignificantly improves the Pareto frontiers across var-\nious model sizes and datasets, outperforming vanilla\nLLMs in both memory reads (which is a proxy for\nruntime) and peak memory use. Notably, DMS consis-\ntently dominates other baselines for efficient attention,\nwhich we also verify on a broader set of tasks outside\nof inference-time scaling. DMS variants even sur-\npass the corresponding vanilla LLMs on long-context\ntasks, such as needle-in-a-haystack and variable state\ntracking (Hsieh et al., 2024), while achieving higher\nefficiency. Overall, this validates the effectiveness of\nefficient attentionâ€”unlocked by DMSâ€”for inference-\ntime scaling, which improves the reasoning capabili-\nties of models under any given inference-time budget.\n2. Background\n2.1. Inference-time Scaling\nInference-time scaling allows a model to â€˜think longer\nor more broadlyâ€™ about a problem to enhance the\nquality of its solution, by leveraging extra compute\nduring generation (Du et al., 2024; Madaan et al.,\n2023; Yao et al., 2023). In practice, when presented\nwith a prompt x, a Large Language Model ğ‘“LLMcan\nexplore ğ‘›chains of reasoning [z1, . . . ,zğ‘›]to gener-\nate corresponding answers [y1, . . . ,yğ‘›]. While some\nstrategies involve guiding this exploration through a\nProcess Reward Model (PRM; Li et al., 2023; Fenget al., 2023; Lightman et al., 2024; Uesato et al.,\n2022; Wang et al., 2024; Snell et al., 2024) by scoring\neach reasoning step, recent systematic comparisons\nestablished that simpler PRM-free strategies such as\nmajority voting (Wang et al., 2025b) remain the most\ncompetitive.\nHence, scaling can be easily achieved in two ways:\nincreasingthemaximumlengthforchainsofreasoning\n(known as sequential scaling) or increasing the number\nof chains (known as parallelscaling). These two quan-\ntities can be controlled to determine a â€˜token budgetâ€™\nfor inference-time computation (Muennighoff et al.,\n2025), which directly translates into a proportional\nmemory load and runtime. In fact, in Transformer\nLLMs, the keyâ€“value cache grows linearly with the\nnumber of generated tokens. Crucially, the KV cache\nis stored in VRAM on GPU accelerators, contributing\nsignificantly to the overall memory load and limiting\nscalability. At the same time, retrieving the KV cache\nthrough high-bandwidth memory access dominates\nruntime during generation. As a result, the KV cache\nconstitutes a bottleneck for inference-time scaling,\nwhich leads to the natural question: by making the\nKV cache leaner and scaling to even longer sequences\nor more parallel ones, could we increase the down-\nstream performance of reasoning LLMs for the same\ncompute budget?\n2.2. KV Cache Sparsity\nAn intuitive strategy to reduce the size of the KV\ncache is to evict tokens, i.e., dynamically remove\nthe keyâ€“value pairs of the least relevant tokens dur-\ning inference. Recent methods have addressed this\nchallenge by selectively managing tokens within a\nsliding window of context of size ğ‘¤. For instance, for\neach time step ğ‘¡, TOVA (Oren et al., 2024) evicts\nthe token with the lowest attention weight such that\nğ‘–TOVA =minğ‘–âˆ‘ï¸€\nâ„âˆˆğ»ğ‘â„(ğ‘¡)ğ‘–where ğ‘â„(ğ‘¡)ğ‘–denotes the\nattention weight assigned to token ğ‘–by attention head\nâ„at time step ğ‘¡. Similarly, Heavy-Hitter Oracle (H20;\nZhang et al., 2023a) evicts the token with the lowest\ncumulative attention, additionally keeping a sliding\nwindow of recent tokens. These approaches incur\nminimal computational overhead due to their efficient\nheuristics for eviction scores, while retaining a maxi-\nmum KV cache size of ğ‘¤and speeding up generation\nas a consequence.\nAn entirely different strategy is adopted by meth-\nods like Quest (Tang et al., 2024), which fully retains\nthe entirety of the KV cache but only retrieves the\nmost relevant pages(i.e., fixed-size blocks of contigu-\nous KV items) from memory. Relevant pages are\ndetermined through a heuristic that approximates at-\ntention scores from the highest-magnitude dimensions\n2\n--- Page 3 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nof their KV items. While this approach accelerates\ngeneration by reducing memory transfers without per-\nmanently evicting tokens, it does not reduce memory\nload. In fact, to efficiently perform page selection, the\nmethod requires storing additional page representa-\ntions, resulting in a slight memory overhead rather\nthan savings.\n2.3. KV Cache Compression\nWhile mitigating runtime or memory load, training-\nfree KV cache sparsity methods often incur perfor-\nmance degradation at high compression ratios. To\novercome this trade-off, Dynamic Memory Compres-\nsion (DMC; Nawrot et al., 2024) reduces KV cache\nsize by dynamically compressing representations, po-\ntentially extracting and retaining vital information.\nAt each timestep ğ‘¡, for every attention head sepa-\nrately, DMC models decide to either appendthe new\nkeyâ€“value pair to KV cache as in standard Transform-\ners, oraccumulate it into the most recent cache entry\nusing weighted averaging. As a result, every attention\nhead produces a uniquely compressed KV sequence\nwith a distinct compression ratio (CR). This flexibility\nallows the model to preserve critical information while\naggressively compressing redundant representations,\nunlike sparse attention methods that impose uniform\ncompression budgets (Nawrot et al., 2025).\nApplying DMC requires continued training (known\nas â€˜retrofittingâ€™), during which the discrete decisions\nare relaxed into continuous variables via stochastic\nreparameterization (Louizos et al., 2018), enabling\ngradient-based optimization. Although it requires a\nfraction of the pre-training budget, the computational\ncost is still significant. Moreover, DMC by default\ndoes not accelerate the prefilling phase, since all inter-\nmediate, partially accumulated tokens are retained.\n3. Dynamic Memory Sparsification\nToken eviction strategies effectively reduce KV cache\nsize, but degrade downstream performance at higher\neviction rates. Conversely, DMC offers stronger com-\npression at the cost of expensive retraining. To scale\ninference-time efficiency even further, it is therefore\nessential to develop a KV cache compression method\nthat is inexpensive, easy to integrate, and maintains\naccuracy at high compression ratios. To this end,\nwe propose Dynamic Memory Sparsification (DMS),\na method of teaching pre-trained models a simple,\nadaptive token eviction policy. As such, it combines\nthe advantages of eviction and trained compression,\nwith significantly higher data efficiency than DMC.3.1. Multi-Head Self-Attention\nWe start from LLMs with vanilla multi-head self-\nattention. Formally, let h1:ğ‘‡âˆˆRğ‘‡Ã—ğ‘‘represent the\ninputsequenceofhiddenstatestoaTransformerlayer,\nwhere ğ‘‡denotes the number of tokens, and ğ‘‘is the\nhidden dimension. Multi-Head Self-Attention (hence-\nforth, simply â€˜attentionâ€™) divides the hidden states\nintoğ‘›â„attention heads, each processed independently\nto capture distinct input relationships. For a single at-\ntention head, the model applies distinct linear projec-\ntions using weight matrices ğ‘Šğ‘, ğ‘Šğ‘˜, ğ‘Šğ‘£âˆˆR(ğ‘‘/ğ‘›â„)Ã—ğ‘‘,\nobtaining queries, keys, and values:\nq1:ğ‘‡=ğ‘Šğ‘h1:ğ‘‡,k1:ğ‘‡=ğ‘Šğ‘˜h1:ğ‘‡,v1:ğ‘‡=ğ‘Šğ‘£h1:ğ‘‡.\nThe attention weights and output vector for the ğ‘–-th\ntoken are computed as:\nğ‘ğ‘–ğ‘—=1ğ‘—â‰¤ğ‘–exp(qâŠ¤\nğ‘–kğ‘—/âˆšğ‘‘â„)âˆ‘ï¸€ğ‘–\nğ‘¡=1exp(qâŠ¤\nğ‘–kğ‘¡/âˆšğ‘‘â„),oğ‘–=ğ‘–âˆ‘ï¸\nğ‘—=1ğ‘ğ‘–ğ‘—vğ‘—,\nwhere 1denotes the indicator function and constrains\nthe token dependencies to be autoregressive. Finally,\nthe outputs from all heads are concatenated and pro-\njected by a matrix ğ‘Šğ‘œâˆˆRğ‘‘Ã—ğ‘‘to yield the output.\n3.2. Retrofitting Models with DMS\nTo integrate DMS into pre-trained models, we adapt\nthe retrofitting strategy from DMC, introducing two\ncrucial modifications. First, whereas DMC weights\nand aggregates tokens, DMS simply evicts them. Sec-\nond, we separate the time of eviction decisions from\nthe time of their execution: when eviction is predicted\nfor a token, the model is given time to integrate infor-\nmation from the token scheduled for eviction before it\nis actually removed. Below we describe the procedure\nfor a single attention head, though the same process\nis applied across all KV heads independently.\nGiven an input hidden vector to an attention layer\nhğ‘¡at inference time step ğ‘¡, DMS predicts a binary\neviction decision ğ›¼ğ‘¡which controls the eviction of\n(kğ‘¡,vğ‘¡). To maintain differentiability during training,\nğ›¼ğ‘¡is learned through stochastic reparametrization\nwith a Gumbel-sigmoid distribution:\nğ›¼ğ‘¡âˆ¼Gumbel-sigmoid (hğ‘¡wâŠ¤+ğ‘, ğœ)âˆˆ[0,1],(1)\nwhere wâˆˆRğ‘‘is a vector of trainable weights initially\nset to w= [0, . . . , 0]âŠ¤. We set a low temperature ğœ\nto encourage discrete eviction decisions and ğ‘=âˆ’5\nin order to offset the logits and initiate training with\nğ›¼ğ‘¡â‰ˆ0, preventing eviction early in training. Empir-\nically, this configuration prevents initial loss spikes,\nwhich might cause catastrophic forgetting (Nawrot\net al., 2024).\n3\n--- Page 4 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nÎ±bin=0\nÎ±bin=0evict k2 upon exitingtimeÎ±bin=1\nÎ±bin=1schedule k2\nfor eviction\nschedule k4\nfor evictionk1k2\nk1 k2\nk1k2\nk1the sliding windowk1\nÎ±bin=0\nk3\nk3k4\nk3k4k5\nk2sliding window\n1\n2\n3\n4\n5\n(a) DMS key cache management during inference.k0 k1 k2 . . . kn\nâ¡\nâ¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£â¤\nâ¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦q0 0âˆ’âˆ âˆ’âˆ . . .âˆ’âˆ\nq1 0 0 âˆ’âˆ âˆ’âˆ\nq2 0 0 0 âˆ’âˆ\nq3 0 0 0 âˆ’âˆ\nq4log(19ğ›¼1) 0 0 âˆ’âˆ\nq5log(19ğ›¼1) log(1 9ğ›¼2) 0 âˆ’âˆ\n............\nqnlog(19ğ›¼1) log(1 9ğ›¼2) log(1 9ğ›¼3). . . 0\n(b) Attention mask ğ‘€ğ›¼during training.\nFigure 2|During each inference step ( left) the incoming keyâ€“value pair (kğ‘¡,vğ‘¡)might be selected for later\neviction, based on predicted binary decisions ğ›¼binâˆˆ{0,1}(we show only a sequence of keys for clarity). The\neviction takes place as soon as the pair falls out of the sliding window. During training ( right), this behavior\nis induced with an additive attention mask. Eviction decisions are relaxed from binary to continuous ğ›¼âˆˆ[0,1].\nDuring training, a sequence of eviction decisions\nğ›¼1:ğ‘‡is used to construct a mask ğ‘€ğ›¼âˆˆ(âˆ’âˆ,0]ğ‘‡Ã—ğ‘‡\n(Figure 2b), which is added to unnormalized attention\nscores ğ‘„ğ¾âŠ¤. It selectively modulates token visibility:\nâˆ’âˆfully masks a token, 0 indicates no masking, and\nsmall negative values have a partial effect. It follows\nthat evicting a particular kğ‘–automatically means\nevicting the corresponding vğ‘–.\nDelayed Eviction via Sliding Window Imme-\ndiate eviction can harm the modelâ€™s abilities by pre-\nmaturely discarding useful context. To mitigate this,\nwe propose delaying the execution of eviction deci-\nsions. Specifically, the eviction decision ğ›¼ğ‘¡is made at\ntimestep ğ‘¡, but the token selected for eviction remains\navailable until a future timestep ğ‘¡+ğ‘¤. This delay\ncreates a sliding window of size ğ‘¤and is implemented\nthrough the construction of ğ‘€ğ›¼.\nPrevious work indicates that decoder-only models\nheavily attend to recent tokens (Xiao et al., 2024;\nJiang et al., 2024). Consequently, delayed eviction en-\nables the model to extract relevant information from\nthe eviction candidates before their removal, even\nif those tokens are recent. In contrast, immediate\neviction of recent tokens has a negative impact. We\nconfirm this empirically in Section 5.3, demonstrat-\ning that immediate eviction leads to rapid perfor-\nmance degradation for all tested sliding window sizes,\nwhereas delayed eviction maintains stable training\nand substantially improves sample efficiency, dramat-\nically reducing the number of training tokens needed\nto achieve a given CR.\nTraining Objective During training we follow\nDMC and apply a one-sided L1 loss term which forcesthe model to match the average value of predicted ğ›¼\nfor a given input to the target compression ğ›¼â‹†, i.e.,\nâ„’aux=max(ï¸€\nğ›¼â‹†ğ¿ ğ» ğ‘‡âˆ’âˆ‘ï¸€\nğ‘™âˆˆğ¿âˆ‘ï¸€\nâ„âˆˆğ»âˆ‘ï¸€\nğ‘¡âˆˆğ‘‡ğ›¼ğ‘™â„ğ‘¡,0)ï¸€\n,\nwhere ğ¿, ğ», ğ‘‡denote the number of layers, KV atten-\ntion heads, and sequence length, respectively. Over\nthe course of training, the target compression ğ›¼â‹†is\nlinearly annealed from 0 to(ï¸€\n1âˆ’1\nCR)ï¸€\n. We train the\nmodel using a logit distillation loss â„’Dloss (Hinton\net al., 2015), which is added to the auxiliary loss â„’aux:\nâ„’=â„’D+â„’aux.\nSince we do not enforce any constraints on com-\npression for individual attention heads, they adopt\npossibly different compression ratios and produce KV\nsequences of possibly different lengths.\nPerformance Considerations The overhead of\nDMS on the attention mechanism comes solely from\nconstructing and applying the additive attention\nmask, which never needs to be materialized. For\neach attention head, it can be compactly passed as a\nvector of eviction decisions ğ›¼1:ğ‘‡, and is implementable\nwith existing tools (Wang et al., 2025a; Dong et al.,\n2024). Implementation-wise, a neuron is re-purposed\nfrom qğ‘¡orkğ‘¡to predict ğ›¼ğ‘¡instead of adding a pa-\nrameter vector wfor every attention head (Nawrot\net al., 2024). Hence, no extra parameters are added.\n3.3. Inference\nFigure 2a shows the inference time operation of DMS.\nThe decision variables are rounded to the nearest in-\nteger ğ›¼bin\nğ‘¡=âŒŠsigmoid (hğ‘¡wâŠ¤+ğ‘)âŒ‰âˆˆ{ 0,1}. Ifğ›¼bin\nğ‘¡=1,\nthen the (kğ‘¡,vğ‘¡)pair needs to be evicted at time ğ‘¡+ğ‘¤.\nThe sparsity introduced by DMS can also be leveraged\nduring the prefilling phase to eliminate unnecessary\ncomputation (Wang et al., 2025a).\n4\n--- Page 5 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nPerformance-wise, DMS does not introduce any\nnew read/write operations on the KV cache, since\nthe evicted tokens could be simply overwritten by in-\ncoming ones, under the assumption that the keys are\nstored in the KV cache with positional information.\nPagedAttention (Kwon et al., 2023) facilitates storing\nthe sparsified KV cache in memory, where pages are\nallocated to individual attention heads. This formu-\nlation enables our reuse of existing, efficient kernels\nthat support PagedAttention.\n4. Experimental Setup\nModels and Baselines To evaluate inference-time\nscaling through KV cache compression, we primarily\nfocus on reasoning models of different sizes distilled\nfrom DeepSeek R1 reasoning traces (Guo et al., 2025),\nincluding Qwen 2.5 1.5B, 7B, and 32B. In addition,\nas a sanity check on other families of models and for\nablations on method design, we test the accuracy of\nefficient attention methods also on Llama 3.2 1B In-\nstruct (Grattafiori et al., 2024). We retrofit all these\nmodels with DMS and compare them against the\noriginal models, DMC, and training-free KV cache\nsparsificationmethodsdescribedinSection2.2: Token\nOmission via Attention (TOVA; Oren et al., 2024),\nHeavy-Hitter Oracle (H2O; Zhang et al., 2023a), and\nQuest (Tang et al., 2024).1Crucially, all the LLMs\nincluded in the experiments use Grouped Query At-\ntention (GQA; Ainslie et al., 2023), hence KV tokens\nare shared among multiple query heads, which exac-\nerbates the destructive effects of token eviction.\nLogit Distillation and Retrofitting In contrast\nto conventional retrofitting methods employing stan-\ndard language modeling loss (Nawrot et al., 2024),\nwe retrofit all models through logit distillation (Hin-\nton et al., 2015). In particular, the original LLM\nacts as the teacher and the DMS-retrofitted one as\nthe student. As previously observed in other set-\ntings (Sreenivas et al., 2024; Minixhofer et al., 2025),\nwe found that logit distillation provides greater ro-\nbustness to shifts in training data (since the original\npre-training and post-training mixtures are rarely\npublic) and is especially beneficial for fragile LLMs\nwith lower parameter counts. We provide information\non the training data for distillation in Appendix C.\nThe retrofitting process is inspired by DMC\n(Nawrot et al., 2024). The amount of data required\nfor equipping LLMs with DMS depends directly on\nthe context length of retrofitted models and the tar-\nget KV cache compression ratio: higher compres-\n1Our baseline implementations closely follow the publicly\navailable reference implementations.sion ratios necessitate larger datasets. Specifically,\nwe employ a linear training schedule that runs for\n100 training steps for each unit of compression ratio:\nCR(ğ‘¡) =ğ‘¡\n100+ 1. Unless otherwise stated, we train\nDMS models with a sliding windowâ€”and equivalently\nan eviction delayâ€”of 256 tokens.\n5. Results\n5.1. Inference Time Hyper-Scaling\nOur main goal is to determine if KV cache com-\npression increases downstream performance by ef-\nfectively leveraging a larger â€˜token budgetâ€™ for an\nequivalent runtime and memory load compared with\nvanilla Transformers. We run our experiments for\ninference-time hyper-scaling on a selection of datasets\nthatrequireadvancedreasoningcapabilities, following\nSnell et al. (2024) and Guo et al. (2025): specifically,\nAIME 24 and MATH 500 (Hendrycks et al., 2021a)\nfor math problems, GPQA Diamond (Rein et al.,\n2024) for physics, chemistry, and biology problems,\nand LiveCodeBench (Jain et al., 2025) for coding.\nAs a metric for performance, we use exact match\nafter mapping the output to a unified math repre-\nsentation (for MATH 500 and AIME 24) or one of\nthe 4 available choices (for GPQA Diamond). For\nLiveCodeBench, we report pass@all, i.e., if any of the\ngenerated sequences pass the tests.\nAs metrics for the effective budget in terms of time\nand memory, we focus on two metrics: (i) KV cache\ntoken reads , which represent the total number of\nitems in the KV cache attended to in each generation\nstep, summed across steps. Therefore, it reflects the\nruntime efficiency, as KV cache loading from memory\nis the main bottleneck during generation, contributing\na share of the inference latency that increases with\nthe sequence length (Tang et al., 2024; Nawrot et al.,\n2025). For instance, as detailed in Appendix G, for a\nbatch size of 256 and for the range of sequence lengths\nconsidered in our experiments (8Kâ€“32K), this share\nexceeds 90% of the latency in Qwen-R1 1.5B and\n80% for Qwen-R1 7B. Secondly, (ii) peak tokens in\nmemory , which represents the maximum KV cache\nsize, critical for memory-constrained hardware, such\nas GPUs or edge devices.\nTo establish a variety of trade-offs between down-\nstreamperformanceandcompute, werunexperiments\nwith varying budget configurations in terms of max-\nimum sequence length(L), number of parallel rea-\nsoning chains or width(W), and compression ratio\n(CR). Hence, each configuration can be defined by a\ntuple L-W-CR, where CR is 1 for vanilla models. By\nidentifying the Pareto frontier for each method, we\ncan determine which ones offer superior performance\n5\n--- Page 6 ---\nInference-Time Hyper-Scaling with KV Cache Compression\n2252272291620242832364044AIME 24\n8-4-416-4-4\n32-1-4\n8-8-816-8-8\n32-1-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-4-8\n8-8-816-4-8\n16-8-8\n32-1-832-4-832-8-8\n8-1-1 8-2-18-4-116-1-1\n16-2-132-1-1Qwen-R1 1.5B\n2242262284045505560657075\n8-4-416-4-4\n32-1-48-8-816-8-8\n32-1-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-8-8\n16-4-816-8-832-8-8\n8-1-1 8-2-18-4-1 16-1-116-2-132-1-1Qwen-R1 7B\n2252272295055606570758085\n8-4-416-4-4\n32-1-4\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-8-816-4-816-8-8\n32-8-8\n8-1-18-2-18-4-116-1-116-2-1\n32-1-1Qwen-R1 32B\n22322522780828486889092MATH 500\n8-4-416-4-4\n32-1-48-8-816-8-8\n32-1-88-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-8-816-4-816-8-8\n32-1-832-8-8\n8-1-1\n8-2-18-4-1\n16-1-1\n16-2-132-1-1\n22222422691929394959697\n8-4-416-4-4\n32-1-48-8-816-8-8\n32-1-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-8-816-4-816-8-8\n32-8-8\n8-1-18-2-18-4-116-1-116-2-132-1-1\n222223224225929394959697\n8-4-416-4-4\n32-1-48-4-4 8-8-416-4-4\n32-1-48-8-816-4-8\n8-1-18-2-18-4-116-1-1\n16-2-1 32-1-1\n224226228 303234363840GPQA Diamond\n8-4-416-4-4\n32-1-48-8-816-8-8\n32-1-88-4-4\n8-8-4\n16-4-416-8-4\n32-1-4\n32-4-48-8-8\n16-4-816-8-832-8-8\n8-1-18-2-18-4-116-1-1\n16-2-132-1-1\n22422622846485052545658\n8-4-416-4-4\n32-1-48-8-8\n16-8-8\n32-1-88-4-4 8-8-416-4-4 16-8-4\n32-1-432-4-4 8-8-8\n16-4-816-8-8\n8-1-1\n8-2-18-4-116-1-1\n16-2-132-1-1\n22422522622722858606264666870\n8-4-4\n16-4-4\n32-1-48-4-48-8-4\n16-4-416-8-432-1-432-4-4\n8-8-816-4-8\n8-1-1 8-2-1 8-4-116-1-116-2-132-1-1\n2242262281416182022242628LiveCodeBench\n8-4-4\n16-1-416-4-4\n8-8-8\n16-1-816-8-8\n8-4-4\n16-1-416-4-4\n8-8-8\n16-1-816-8-8\n8-1-116-1-1\n2242262282428323640444852\n8-4-4\n16-1-416-4-4\n8-8-8\n16-1-816-8-8\n8-1-48-4-4\n16-1-416-4-4\n8-1-88-4-88-8-8\n16-1-816-4-816-8-8\n8-1-116-1-1\n22422522622739424548515457606366\n8-4-416-1-416-4-4\n8-4-416-1-416-4-4\n16-1-8\n8-1-116-1-1\nCumulative KV cache memory readsDMS Quest Vanilla\nFigure 3|Inference-time scaling results comparing exact-match performance ( ğ‘¦-axis) and KV cache reads\nas a measure of runtime ( ğ‘¥-axis). We evaluate Qwen-R1-distilled models at different scales (columns) and 4\ndatasets (rows). Point colors indicate results for DMS (green), vanilla models (purple), and the state-of-the-art\nsparse attention baseline, Quest (yellow). Colored lines indicate the respective Pareto frontier. Annotations\nindicate the scaling strategy as a L-W-CR tuple in terms of sequence length L (times 1024 tokens), width\nW (number of sampled reasoning threads), and compression ratio CR. The horizontal black line shows the\nperformance reported by Guo et al. (2025) for the vanilla model based on a 32-1-1 configuration. From\nthe plots, it emerges that not only DMS achieves the best Pareto frontier, but that in general, KV cache\ncompression is an effective strategy for improving inference-time scaling.\nfor the same budget. We report the results for each ef-\nficiency metric separately: KV cache memory reads in\nFigure 3 and peak memory usage in Figure 4. For sim-\nplicity, asbaselines, wereportonlythestate-of-the-art\ntraining-free method in terms of accuracyâ€“speedup\n(Quest) and accuracyâ€“memory (TOVA), respectively.\nFrom Figure 3, we observe that KV cache compres-\nsion methods generally yield superior Pareto frontiers\ncompared to vanilla LLMs, across model sizes and\ndatasets. Specifically, the best-performing method ineach datasetâ€“size combination substantially improves\nthe scores at comparable memory transfer budgets\n(proportional to runtime). Averaging Pareto frontier\nmargins across budgets, as detailed in Appendix E\nTable 5 and summarized in Figure 1, we find average\ngainsforDMSof11.5forAIME24, 2.3forMATH500,\n5.5 for GPQA Diamond, and 8.3 for LiveCodeBench.\nVariability across datasets primarily reflects their sat-\nuration levels; for instance, models achieve very high\nperformance on MATH 500 even with limited budgets.\nNotably, performance gains from DMS decrease with\n6\n--- Page 7 ---\nInference-Time Hyper-Scaling with KV Cache Compression\n21121221321421551015202530354045AIME 24\n8-4-416-4-4\n32-1-4\n8-8-816-8-8\n32-1-88-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-4-88-8-816-4-816-8-8 32-1-832-4-832-8-8\n8-1-18-2-18-4-116-1-116-2-132-1-1Qwen-R1 1.5B\n2112132152432404856647280\n8-4-416-4-4\n32-1-4 8-8-816-8-8\n32-1-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-8-816-4-816-8-832-8-8\n8-1-18-2-18-4-116-1-1\n16-2-1 32-1-1Qwen-R1 7B\n21121221321421548525660646872768084\n8-4-416-4-4\n32-1-4\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n8-1-18-2-18-4-116-1-116-2-1\n32-1-1Qwen-R1 32B\n21021121221321480828486889092MATH 500\n8-4-416-4-4\n32-1-48-8-816-8-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n16-4-816-8-8\n32-1-832-8-8\n8-1-1\n8-2-18-4-116-1-1\n16-2-132-1-1\n21021221491929394959697\n8-4-416-4-4\n32-1-48-8-816-8-8\n32-1-8\n8-4-48-8-416-4-416-8-4\n32-1-432-4-4\n16-4-816-8-832-8-8\n8-1-18-2-18-4-1 16-1-116-2-132-1-1\n210211212213214929394959697\n8-4-416-4-4\n32-1-4\n8-4-48-8-416-4-416-8-4\n32-1-4 32-4-4\n8-1-18-2-18-4-1 16-1-116-2-1 32-1-1\n211213215303234363840GPQA Diamond\n8-4-4\n16-4-4\n32-1-48-8-816-8-8\n32-1-88-4-48-8-4\n16-4-416-8-4\n32-1-432-4-4\n8-8-8\n16-4-816-8-832-8-88-1-18-2-18-4-1 16-1-1\n16-2-132-1-1\n2112132154446485052545658\n8-4-4\n16-4-4\n32-1-48-8-8\n16-8-8\n32-1-8\n8-8-416-4-416-8-4\n32-1-432-4-4\n32-8-8\n8-1-18-2-18-4-1 16-1-1\n16-2-132-1-1\n211212213214215606264666870\n8-4-4\n16-4-4\n32-1-416-4-4\n16-8-4 32-1-432-4-4\n8-1-1 8-2-18-4-116-1-116-2-132-1-1\n21021121221321469121518212427LiveCodeBench\n8-4-4\n16-1-416-4-4\n8-8-8\n16-1-816-8-8\n8-1-48-4-4\n16-1-416-4-4\n8-1-88-8-816-1-816-8-8\n8-1-1 16-1-1\n210211212213214101520253035404550\n8-4-4\n16-1-416-4-4\n8-8-8\n16-1-816-8-8\n8-1-48-4-416-1-416-4-4\n8-1-88-8-816-1-816-8-8\n8-1-1 16-1-1\n2112122132143035404550556065\n8-4-416-1-416-4-4\n8-1-48-4-416-1-416-4-4\n8-1-116-1-1\nPeak tokens in memoryDMS TOV A Vanilla\nFigure 4|Inference-time scaling results comparing exact-match performance ( ğ‘¦-axis) and peak tokens in\nmemory ( ğ‘¥-axis). We refer to Figure 3 for full details. These results clearly indicate that KV cache compression\nmethods (and especially DMS) incur a substantially reduced number of peak tokens in memory compared to\nvanilla LLMs, achieving higher performance for comparable memory needs.\nincreasing model scale on MATH 500, yet increase\nwith scale on GPQA Diamond and LiveCodeBench.\nSimilar trends emerge for the â€˜peak tokens in memoryâ€™\nmetric shown in Figure 3 and summarized in Table 6.\nOverall, these findings indicate that KV cache com-\npression exhibits more favorable scaling properties\nthan dense attention, highlighting its potential for\nadvancing the reasoning capabilities of current LLMs.\nMoreover, comparing DMS with other KV cache\ncompression methods, it emerges how its Pareto fron-\ntier clearly dominates the best baselines for both\nefficiency metrics, namely Quest for KV cache mem-\nory reads and TOVA for peak tokens in memory\n(Figures 3 and 4). This is even more remarkable con-\nsidering that Quest sacrifices memory efficiency, fullypreservingtheKVcache, inordertomitigateaccuracy\ndegradationâ€”and yet DMS manages to offer a better\nruntimeâ€“accuracy trade-off. Datasets like MATH 500,\nwhere Questâ€™s Pareto frontier mostly overlaps with\nVanilla at all scales, illustrate that gains from larger\ntoken budgets can be eaten away, unless performance\nis retained even at high CRs. DMS meets this desider-\natum in a data-efficient way, thus offering inexpensive\nhyper-scaling with existing LLMs.\nZooming in on specific results, we can assess which\nL-W-CR configurations tend to lie on the Pareto\nfrontier for DMS. For most tasks, these consist of a\ncombination of sequential and parallel scaling, hinting\nat the necessity to use both for inference-time scaling.\n7\n--- Page 8 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nTable 1|Evaluation of Llama 3.2 1B Instruct on a broader array of tasks, reflecting different combinations\nof KV cache compression methods and compression ratios (CR). We note that due to full-dense attention\nprefill, Quest is equivalent to vanilla on MMLU and HellaSwag. The DMS model used in this comparison was\ntrained with a sliding window of just 16 tokens.\nCR 1Ã— 2Ã— 3Ã— 4Ã—\nMethod VanillaH2O TOVA Quest DMCwin=16\nDMSH2O TOVA Quest DMCwin=16\nDMSH2O TOVA Quest DMCwin=16\nDMS\nGSM8K 47.044.0 45.0 45.1 31.9 46.932.9 40.1 44.7 6.4 46.514.7 20.2 39.9 3.6 42.3\nMMLU 47.945.7 43.4 47.9 34.9 48.037.6 38.1 47.926.3 45.2 32.7 35.2 47.925.6 40.2\nHellaS 43.442.9 42.8 43.442.2 43.3 42.1 42.5 43.440.0 43.3 41.3 41.8 43.439.443.4\nNIAH 96.434.0 65.2 95.8 0.0 97.817.2 40.2 95.61.8 93.6 13.4 28.0 95.8 0.0 96.8\nVT55.827.4 56.2 53.0 0.0 63.217.6 45.2 50.4 0.2 69.212.6 33.8 49.6 4.0 67.6\n5.2. DMS for General-purpose LLMs\nMoreover, we aim to establish whether DMS is effec-\ntive beyond inference-time scaling settings, so that\na model retrofitted with DMS can be reliably de-\nployed as a general-purpose LLM. To this end, we\nfirst compare DMS with respect to vanilla models for\nequivalentgeneratedtokenlengths(ratherthanactual\ncompute budget). We focus again on the same mod-\nels and datasets as Section 5.1. From Tables 7 to 9,\nit emerges that DMS mostly preserves the original\naccuracy at CR 4 Ã—and yields minimal degradations\n(~3.5 points on average) at CR 8 Ã—.\nNext, we benchmark DMS on a broader set of\ntasks, as a way to ensure that its sparse prefilling\ndoes not affect performance in short generation set-\ntings: GSM8K (Cobbe et al., 2021) for grade-school\nmath, MMLU (Hendrycks et al., 2021a) for factuality,\nHellaSwag (Zellers et al., 2019) for zero-shot common-\nsense question answering, and Needle in a Haystack\n(NIAH; Kamradt, 2023) and Variable Tracking (VT;\nHsieh et al., 2024) for long context processing. We\nselect Llama 3.2 Instruct, which does not rely on long\nreasoning and is therefore compatible with short gen-\neration. Technical details for the experimental setup\nare provided in Appendices E and F.\nTable 1 compares downstream performance at 2 Ã—,\n3Ã—, and 4Ã—compression ratios.2DMS stands out\nas the most robust method, achieving higher scores\nthan both training-free and retrofitted baselines in\nmost combinations of tasks and CRs, with Quest\nas a second-best contender. While in short-context\ntasks, DMS performance is close to the original LLM,\nin long-context tasks (such as NIAH and VT) DMS\neven surpasses it. We speculate this is due to over-\ncoming limitations of dense attention over long se-\nquences, such as over-squashing (Barbero et al., 2024)\nand lost-in-the-middle (Liu et al., 2024a). Moreover,\n2While H2O and TOVA are designed for long context tasks\nwith large sliding windows, we consciously evaluate them with\nshort sliding windows to meet the target CRs.long-context performance provides evidence thatâ€”\ncompared with DMCâ€”DMS is more successful at\nextrapolating compression to lengths beyond those\nobserved during retrofitting, albeit only up to a cer-\ntain limit (see Appendix D). Among learned compres-\nsion methods, DMC collapses quickly, likely due to its\nmore challenging training objective amplified by the\nlimited 1B model capacity.3Overall, DMS emerges\nas a general-purpose method, whose performance re-\ntention at high compression ratios makes it suitable\nnot only for inference-time hyper-scaling but also for\nshort-context and long-context tasks.\n5.3.Ablationsfor DMSand In-DepthAnalysis\nThe design choices in DMS were informed by results\nof small-scale experiments. We present ablations on\neviction policy and data efficiency during retrofitting\nof the Llama 3.2 1B Instruct models. To evaluate\nthe impact of delayedeviction, we trained additional\nmodels with immediate eviction, which aligns more\nclosely with existing token eviction methods:\nDelayed\nevictionğ›¼ğ‘¡determines the eviction of (kğ‘¡,vğ‘¡)\nat a future time step ğ‘¡+ğ‘¤\nImmediate\nevictionğ›¼ğ‘¡+ğ‘¤determines the eviction of past\n(kğ‘¡,vğ‘¡)at time step ğ‘¡+ğ‘¤\nBothpoliciesweretestedwithdifferentslidingwindow\nsizes. Remarkably, DMSretainsreasoningcapabilities\nwith a window of only 16 tokens up to a compression\nratio of 4Ã—, as shown in Figure 5. Larger sliding win-\ndow sizes better preserve reasoning capabilities, which\nis expected in a zero-shot setting. However, switching\nto immediate eviction drastically deteriorates scores\nfor every tested sliding window length.\nRegarding data efficiency, the right panel of Fig-\nure 5 shows how scores vary when retrofitting with\n3Nevertheless, in Appendix D we show that, while still\nlagging behind, this collapse does not occur for shorter contexts\nand a larger non-GQA model.\n8\n--- Page 9 ---\nInference-Time Hyper-Scaling with KV Cache Compression\n02040\n1x\n2x 3x4xDMS win=0 (immediate)\n 1x\n2x\n3x4x2x\n3x4x2x\n3x4xDMC\n02040\n1x 2x 3x 4x\n3x\n4xDMS win=16\nDMS win=16 (immediate)\n1x 3x 4x\n5x6x3x4x\n5x\n6x3x4x\n5x\n6xDMS win=16\n02040\n1x 2x 3x 4x\n3x 4x\nDMS win=64\nDMS win=64 (immediate)\n1x 3x 4x\n5x\n6x3x 4x\n5x6x3x4x\n5x6x\nDMS win=64\n0 5 10 15 20 25 30 35 40\nTraining tokens (B)02040\n1x 2x 3x 4x\n3x\n4xDMS win=256\nDMS win=256 (immediate)\n0 10 20 30 40 50 60 70\nTraining tokens (B)\n1x 3x 4x 5x 6x 3x 4x5x 6x 3x4x5x6x\nDMS win=256GSM8K Score\nFigure 5|GSM8K 0-shot scores of Llama 3.2 1B Instruct across different compression variants. Left:\ndelayed eviction (default) with a 16-token window consistently preserves reasoning abilities of the model,\nwhile immediate eviction causes rapid degradation. The quality gap only widens as the compression gets\nstronger.Right:DMS requires an order of magnitude less data to train than DMC. This was also observed\nfor Qwen 2.5 R1 models with 1.5B, 7B, and 32B parameter scales.\ndifferent training token budgets. Crucially, DMS\nachieves higher scores than DMC while using 8Ã—\nfewer training tokens. In practice, the reasoning mod-\nels described in Section 5.1 were trained with 60Ã—less\ntraining data,4achieving CR 4Ã—within 300 training\nsteps and CR 8Ã—within 700 steps.\nFinally, we measured how the CR varies for differ-\nent lengths of the sequences generated through DMS\n(Figure 6 left). The resulting pattern closely matches\nthat reported in Nawrot et al. (2024). The model\nsparsifies less than the target CR in early parts of\na sequence, but even more aggressively than speci-\nfied beyond 10K tokens. This behavior stems from\nthe training objective and from the tendency of the\nconditional entropy rate of natural-language text to\ndecrease as the context length grows. In Figure 6\n(right), we also observe that early layers are com-\npressed to a smaller degree than later layers.\n6. Related Work\nKV cache size reduction The challenge of KV\ncache reduction has garnered significant interest in\nrecent years, with approaches falling into three main\ncategories: attention sparsification, quantization, and\ndecomposition. In addition to the sparse attention\nbaselines considered in Section 2.2, Landmark At-\n4DMC was reported to require 44K training steps to reach\nCR8, with performance deteriorating when the amount of data\nis halved (Nawrot et al., 2024).tention (Mohtashami and Jaggi, 2023) and Native\nSparse Attention (Yuan et al., 2025) create represen-\ntations for each KV cache chunk and retrieve only\nthe most important chunks for attention computation,\neffectively reducing the amount of data transferred\nfrom the device HBM memory. Compared to these\nmethods, DMS not only accelerates inference but also\nreduces memory load and allows for dynamically se-\nlecting different compression ratios across layers and\nheads based on the input. Moreover, DMS improves\non other retrofitting methods, such as DMC (Nawrot\net al., 2024), both in terms of data efficiency and\ndownstream accuracy.\nAnother strategy for KV cache size reduction is\nquantization, exemplified by methods such as KIVI\n(Liu et al., 2024b) and KVQuant (Hooper et al., 2024),\nwhich quantize keys per channel and values per to-\nken. Finally, KV cache reduction can be achieved\nvia SVD-based decomposition. LoRC (Zhang et al.,\n2024) directly reduces the ranks of key and value ma-\ntrices, whereas Eigen Attention (Saxena et al., 2024)\nmoves the attention computation into a truncated\nspace induced by SVD. While being less expressive\nthan DMS as they assume uniform compression, both\nquantization and decomposition are orthogonal to\nDMS and can be potentially combined with it to\nfurther improve efficiency.\nInference-time scaling Research on inference-\ntime scaling has so far mostly assumed an equivalence\n9\n--- Page 10 ---\nInference-Time Hyper-Scaling with KV Cache Compression\n0 10000 20000 30000\nContext length123456Compression ratio\n101102103104\nContext length (log-scale)\n1\n4CR2\n4 8 12 16 20 24 28\nLayer1\n4CR4\n0.20.40.60.81.0KV Head\nFigure 6|Left:The measured compression ratio for Qwen-R1 7B, trained with DMS CR 4Ã—, while\nprocessing AIME 24, MATH 500, and GPQA Diamond problem instances. Right: Average per-head\ncompression learned by the model, as a percentage of retained tokens sorted for every layer.\nbetween compute budget and generated tokens, in\nterms of sequence length or parallel samples (Brown\net al., 2024; Zhang et al., 2023b; Wang et al., 2023).\nThis budget can be allocated adaptively to the com-\nplexity of the task (Snell et al., 2024) or forced to meet\nan amount pre-defined by the user (Muennighoff et al.,\n2025). To the best of our knowledge, our work is the\nfirst to fully disentangle generated tokens from the\neffective compute (runtime and peak memory load)\nwhen reasoning in the discrete language space. In\nfact, we show how KV cache compression methods\ncan effectively expand the token budget for the same\ncompute budget. A separate family of strategies are\nbased on latent space reasoning (Geiping et al., 2025),\nwhich add a recurrent block on top of Transformer\nLLMs; however, this effectively requires a separate\narchitecture rather than boosting existing LLMs, and\nit remains unclear whether these scale similarly to\nreasoning in the discrete token space.\nWhile in this work we opt for verifier-free scaling\nstrategies, adopting the recommendations of Wang\net al. (2025b), inference-time scaling can rely on pro-\ncess reward models (PRMs) to verify intermediate\nreasoning steps. This allows effective self-critique\nloops and re-ranking candidate solutions (Uesato\net al., 2022; Lightman et al., 2024; Liang et al., 2024).\nNonetheless, we remark that hyper-scaling can be ex-\ntended to PRM strategies, too. In particular, the ver-\nifierâ€™s complexity is quadratic in the sequence length;\nto complement the benefits of KV cache compression\nof the LLM, the PRM would need to be accelerated\nby prefilling-time sparse attention methods, such as\nMInference (Jiang et al., 2024). We leave this possible\ndirection to future work.\n7. Conclusions\nWe introduce inference-time hyper-scaling : by com-\npressing the keyâ€“value cache of Transformer LLMs via\nsparse attention, we improve downstream reasoning\naccuracy by enabling longer token sequences or more\nparallel sequences at the same compute budgetâ€”interms of runtime or memoryâ€”compared to the orig-\ninal LLM. A fundamental requirement of inference-\ntime hyper-scaling is to increase efficiency without\nsacrificing accuracy. To achieve this, we propose Dy-\nnamic Memory Sparsification (DMS), a novel, train-\nable KV cache reduction method that delays evic-\ntion decisions, while remaining highly data-efficient.\nEmpirically, we observe large gains on benchmarks\ninvolvingadvancedmath, scientificproblems, andcod-\ning, demonstrating the effectiveness of hyper-scaling.\nOverall, our approach provides an inexpensive strat-\negy for converting LLMs into more effective reasoners,\npushing inference-time scaling to new frontiers.\nAcknowledgments\nThe authors would like to thank Marcin Chochowski,\nDavid Tarjan, and Andrzej SuÅ‚ecki for helpful discus-\nsions, Szymon Migacz for his assistance with the com-\nputing infrastructure, as well as PrzemysÅ‚aw Strzel-\nczyk, Krzysztof Pawelec, Daniel Korzekwa, Alex Fit-\nFlorea, and Michael Lightstone for support in releas-\ning this paper.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong,\nYury Zemlyanskiy, Federico LebrÃ³n, and Sumit\nSanghai. 2023. GQA: Training generalized multi-\nquery transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing .\nFederico Barbero, Andrea Banino, Steven Kaptur-\nowski, DharshanKumaran, JoÃ£oG.M.AraÃºjo, Alex\nVitvitskyi, Razvan Pascanu, and Petar VeliÄkoviÄ‡.\n2024. Transformers need glasses! Information over-\nsquashing in language tasks. In Advances in Neural\nInformation Processing Systems , volume 37, pages\n98111â€“98142.\nEdward Beeching, Lewis Tunstall, and Sasha Rush.\n2024. Scaling test-time compute with open models.\n10\n--- Page 11 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nBradley Brown, Jordan Juravsky, Ryan Ehrlich,\nRonald Clark, Quoc V. Le, Christopher RÃ©, and\nAzalia Mirhoseini. 2024. Large language monkeys:\nScaling inference compute with repeated sampling.\nPreprint, arXiv:2407.21787.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar\nKhot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. 2018. Think you have solved ques-\ntion answering? Try ARC, the AI2 reasoning chal-\nlenge.Preprint, arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve\nmath word problems. Preprint, arXiv:2110.14168.\nDeepSeek-AI. 2024. DeepSeek-V2: A strong, eco-\nnomical, and efficient mixture-of-experts language\nmodel.Preprint, arXiv:2405.04434.\nJuechu Dong, Boyuan Feng, Driss Guessous, Yanbo\nLiang, and Horace He. 2024. Flex Attention: A pro-\ngramming model for generating optimized attention\nkernels.Preprint, arXiv:2412.05496.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\nTenenbaum, and Igor Mordatch. 2024. Improv-\ning factuality and reasoning in language models\nthrough multiagent debate. In Forty-first Interna-\ntional Conference on Machine Learning .\nXidong Feng, Ziyu Wan, Muning Wen, Ying Wen,\nWeinan Zhang, and Jun Wang. 2023. AlphaZero-\nlike tree-search can guide large language model de-\ncoding and training. In NeurIPS 2023 Foundation\nModels for Decision Making Workshop .\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-\nman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Alain Le Noacâ€™h,\nHaonan Li, Kyle McDonell, Niklas Muennighoff,\nChris Ociepa, Jason Phang, Laria Reynolds, Hailey\nSchoelkopf, Aviya Skowron, Lintang Sutawika, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and\nAndy Zou. 2024. The language model evaluation\nharness.\nJonas Geiping, Sean McLeish, Neel Jain, John\nKirchenbauer, Siddharth Singh, Brian R Bartold-\nson, Bhavya Kailkhura, Abhinav Bhatele, and Tom\nGoldstein. 2025. Scaling up test-time compute\nwith latent reasoning: A recurrent depth approach.\nPreprint, arXiv:2502.05171.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav\nJauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, et al. 2024. The Llama\n3 herd of models. Preprint, arXiv:2407.21783.Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-\nR1: Incentivizing reasoning capability in LLMs via\nreinforcement learning. Preprint, arXiv:2501.12948.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021a. Measuring massive multitask lan-\nguage understanding. In International Conference\non Learning Representations .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021b. Measuring mathematical\nproblem solving with the MATH dataset. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round\n2).\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nPreprint, arXiv:1503.02531.\nColeman Hooper, Sehoon Kim, Hiva Moham-\nmadzadeh, Michael W. Mahoney, Yakun Sophia\nShao, Kurt Keutzer, and Amir Gholami. 2024.\nKvquant: Towards 10 million context length llm in-\nference with kv cache quantization. In Advances in\nNeural Information Processing Systems , volume 37,\npages 1270â€“1303.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman,\nShantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. 2024. RULER: Whatâ€™s\nthe real context size of your long-context language\nmodels? In First Conference on Language Model-\ning.\nNamanJain, KingHan, AlexGu, Wen-DingLi, Fanjia\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. 2025. Live-\ncodebench: Holistic and contamination free eval-\nuation of large language models for code. In The\nThirteenth International Conference on Learning\nRepresentations .\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang,\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua\nHan, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin,\nYuqing Yang, and Lili Qiu. 2024. MInference 1.0:\nAccelerating pre-filling for long-context LLMs via\ndynamic sparse attention. In Advances in Neural\nInformation Processing Systems , volume 37, pages\n52481â€“52515.\nG. Kamradt. 2023. LLMTest_NeedleInAHaystack .\nhttps://github.com/gkamradt/LLMTest_\nNeedleInAHaystack .\n11\n--- Page 12 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with PagedAttention. In Proceedings of the 29th\nSymposium on Operating Systems Principles .\nHynek KydlÃ­Äek and Greg Gandenberger. 2025. Math-\nverify.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei\nChen, Jian-Guang Lou, and Weizhu Chen. 2023.\nMakinglargelanguagemodelsbetterreasonerswith\nstep-aware verifier. Preprint, arXiv:2206.02336.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai,\nPatrick Lewis, and Deming Chen. 2024. SnapKV:\nLLM knows what you are looking for before genera-\ntion. InAdvances in Neural Information Processing\nSystems, volume 37, pages 22947â€“22970.\nZhenwen Liang, Ye Liu, Tong Niu, Xiangliang Zhang,\nYingbo Zhou, and Semih Yavuz. 2024. Improv-\ning LLM reasoning through scaling inference com-\nputation with collaborative verification. Preprint,\narXiv:2410.05318.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2024. Letâ€™s verify step by step. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024a. Lost in the middle: How language\nmodels use long contexts. Transactions of the Asso-\nciation for Computational Linguistics , 12:157â€“173.\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,\nZhaozhuo Xu, Vladimir Braverman, Beidi Chen,\nandXiaHu.2024b. KIVI:Atuning-freeasymmetric\n2bit quantization for KV cache. In Proceedings\nof the 41st International Conference on Machine\nLearning , volume 235 of Proceedings of Machine\nLearning Research , pages 32332â€“32344.\nChristos Louizos, Max Welling, and Diederik P.\nKingma. 2018. Learning sparse neural networks\nthrough ğ¿0regularization. In International Con-\nference on Learning Representations .\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,Katherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Advances in\nNeural Information Processing Systems , volume 36,\npages 46534â€“46594.\nBenjamin Minixhofer, Ivan VuliÄ‡, and Edoardo Maria\nPonti. 2025. Cross-tokenizer distillation via\napproximate likelihood matching. Preprint,\narXiv:2503.20083.\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\nLandmark Attention: Random-access infinite con-\ntext length for transformers. In Advances in neural\ninformation processing systems .\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xi-\nang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and\nTatsunori Hashimoto. 2025. s1: Simple test-time\nscaling.Preprint, arXiv:2501.19393.\nPiotr Nawrot, Adrian ÅaÅ„cucki, Marcin Chochowski,\nDavid Tarjan, and Edoardo Ponti. 2024. Dynamic\nmemory compression: Retrofitting LLMs for ac-\ncelerated inference. In Forty-first International\nConference on Machine Learning .\nPiotr Nawrot, Robert Li, Renjie Huang, Sebastian\nRuder, KellyMarchisio, andEdoardoMPonti.2025.\nThe sparse frontier: Sparse attention trade-offs in\ntransformer LLMs. Preprint, arXiv:2504.17768.\nNVIDIA.2024. Megatron-LM:Ongoingresearchtrain-\ning transformer models at scale.\nOpenAI, Aaron Jaech, Adam Kalai, Adam Lerer,\nAdam Richardson, Ahmed El-Kishky, Aiden Low,\net al. 2024. OpenAI o1 system card. Preprint,\narXiv:2412.16720.\nMatanel Oren, Michael Hassid, Nir Yarden, Yossi Adi,\nand Roy Schwartz. 2024. Transformers are multi-\nstate RNNs. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Process-\ning, pages 18724â€“18741, Miami, Florida, USA.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland,\nJackson Petty, Richard Yuanzhe Pang, Julien Di-\nrani, Julian Michael, and Samuel R. Bowman. 2024.\nGPQA: A graduate-level Google-proof Q&A bench-\nmark. In First Conference on Language Modeling .\nUtkarsh Saxena, Gobinda Saha, Sakshi Choudhary,\nand Kaushik Roy. 2024. Eigen attention: Attention\nin low-rank space for KV cache compression. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2024 , pages 15332â€“15344, Miami,\nFlorida, USA.\n12\n--- Page 13 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In International\nConference on Learning Representations .\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral\nKumar. 2024. Scaling LLM test-time compute op-\ntimally can be more effective than scaling model\nparameters. Preprint, arXiv:2408.03314.\nSharath Turuvekere Sreenivas, Saurav Muralidharan,\nRaviraj Joshi, Marcin Chochowski, Ameya Sunil\nMahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia\nChen, Yoshi Suhara, Shizhe Diao, et al. 2024. LLM\npruning and distillation in practice: The Minitron\napproach. Preprint, arXiv:2408.11796.\nJiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan\nXiao, Baris Kasikci, and Song Han. 2024. QUEST:\nQuery-aware sparsity for efficient long-context LLM\ninference. In Proceedings of the International Con-\nference on Machine Learning (ICML) .\nHugo Touvron, Louis Martin, Kevin Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Ying-\nhai Lu, Yuning Mao, Xavier Martinet, Todor\nMihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi\nRungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina\nWilliams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,\nIliyan Zarov, Yuchen Zhang, Angela Fan, Melanie\nKambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. Preprint, arXiv:2307.09288.\nJonathan Uesato, Nate Kushman, Ramana Ku-\nmar, Francis Song, Noah Siegel, Lisa Wang, An-\ntonia Creswell, Geoffrey Irving, and Irina Hig-\ngins. 2022. Solving math word problems with\nprocess- and outcome-based feedback. Preprint,\narXiv:2211.14275.\nGuoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu,\nJiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian,\nDianhai Yu, and Haifeng Wang. 2025a. Flashmask:Efficient and rich mask extension of flashatten-\ntion. InThe Thirteenth International Conference\non Learning Representations .\nJunlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athi-\nwaratkun, QingyangWu, JueWang, ShuaiwenLeon\nSong, Ce Zhang, Bhuwan Dhingra, and James Zou.\n2025b. Think deep, think fast: Investigating effi-\nciency of verifier-free inference-time-scaling meth-\nods.Preprint, arXiv:2504.14047.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\n2024. Math-shepherd: Verify and reinforce LLMs\nstep-by-step without human annotations. In Pro-\nceedings of the 62nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1:\nLong Papers) , pages 9426â€“9439, Bangkok, Thai-\nland.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. InThe Eleventh International Conference on\nLearning Representations .\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2024. Efficient streaming lan-\nguage models with attention sinks. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2023. Tree of thoughts: Deliberate problem solving\nwith large language models. In Advances in Neural\nInformation Processing Systems , volume 36, pages\n11809â€“11822.\nJingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo,\nLiang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X.\nWei, Lean Wang, Zhiping Xiao, Yuqing Wang,\nChong Ruan, Ming Zhang, Wenfeng Liang, and\nWangding Zeng. 2025. Native sparse attention:\nHardware-aligned and natively trainable sparse at-\ntention.Preprint, arXiv:2502.11089.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics .\nRongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang\nWang, Hao Cheng, Chao Zhang, and Yelong Shen.\n2024. LoRC: Low-rank compression for LLMs KV\ncache with a progressive compression strategy. In\nWorkshop on Machine Learning and Compression,\nNeurIPS 2024 .\n13\n--- Page 14 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tian-\nlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,\nYuandong Tian, Christopher RÃ©, Clark Barrett,\nZhangyang â€œAtlasâ€ Wang, and Beidi Chen. 2023a.\nH2O: Heavy-hitter oracle for efficient generative\ninference of large language models. In Advances in\nNeural Information Processing Systems 36 .\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nKaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreed-\nhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn\nSong, and Xin Eric Wang. 2025. The hidden risks\nof large reasoning models: A safety assessment of\nR1.Preprint, arXiv:2502.12659.\n14\n--- Page 15 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nA. Limitations, Future Work and Impact\nLarger Model Sizes, Longer Contexts, and Higher Compression Ratios. In this work, we focus on\nmodels ranging from 1B to 32B parameters, context lengths up to 32K tokens, and compression ratios up to\n8Ã—. Exploring even larger models, longer contexts, and higher compression ratios remains an exciting avenue\nfor future research.\nIntegration with Other Efficient Attention Mechanisms. We demonstrated DMS with the standard\nmulti-head attention mechanism used in Transformer-based models such as Llama and Qwen 2.5. Extending\nDMS to alternative attention variants, such as Multi-head Latent Attention (DeepSeek-AI, 2024) represents a\npromising direction for future investigation. Moreover, DMS compresses the KV cache, whereas Quest (Tang\net al., 2024) selectively retrieves cache items. Hence, the two are orthogonal and could be combined to further\npush the Pareto frontier for inference time scaling.\nBroader Impact. Our approach does not introduce novel risks; however, it may amplify existing concerns\nassociated with large-scale reasoning models. For a detailed analysis of these risks, we refer readers to Zhou\net al. (2025).\nB. Additional Details for Retrofitting\nDMS Implementation Unlike (Nawrot et al., 2024), which extracts ğ›¼ğ‘¡from key representations affecting\nall query heads in a group, we â€˜borrowâ€™ the first neuron from the first query head in each query group and use\nit to extract ğ›¼ğ‘¡, eliminating the need for additional parameters while minimizing the impact on attention\ncomputation. This requires a short continued training, during which we gradually zero out the first dimension\nof the first query head in each group: qğ‘¡,first[0]â†qğ‘¡,first[0]Ã—(ï¸\n1âˆ’ğ‘¡\nğ‘›ğ‘¡)ï¸\n, where ğ‘¡denotes the current training\nstep and ğ‘›ğ‘¡= 2000. After this initial stage, the models are ready for the main DMS retrofitting phase, where\nthey learn to dynamically evict tokens. After we extract ğ›¼ğ‘¡from the first query head, we set qğ‘¡,first[0] = 0\nto avoid ğ›¼ğ‘¡influence on the result of attention calculation, while leaving other query heads in the group\nunaffected.\nTraining Configuration We use a batch size of 1024 following the original Llama recipe (Touvron et al.,\n2023). Context lengths are set to 4096 tokens for Llama 3.2 1B Instruct and Llama 2 7B models, and 8192\ntokens for Llama 3.1 8B and R1-distilled models to accommodate the longer sequences required by AIME and\nMATH 500 benchmarks.\nDefault DMS Configuration Unless otherwise specified, all DMS models use delayed eviction with a\nsliding window of 256 tokens and increment the compression ratio by 1 every 100 training steps. We denote\ndifferent DMS variants using the notation DMSwin=ğ‘¦, where ğ‘¥represents the sliding window size. Unlike\nDMC (Nawrot et al., 2024), we omit the third fine-tuning phase (training with fixed compression ratio) as it\nprovided negligible benefits for DMS.\nInfrastructure and Computational Requirements All models are trained on NVIDIA H100 GPUs\nusing Megatron-LM (NVIDIA, 2024) in bfloat16 precision, with optimizer states stored in fp32. For the\n32B Qwen-R1 model, each retrofitting step (batch size 256, context length 8192) requires approximately 18\nseconds on 256 H100 GPUs using tensor parallelism 8 and pipeline parallelism 2. Model checkpoints, including\noptimizer states, occupy approximately 430GB of storage. The complete project consumed roughly 200K\nGPU hours, including preliminary experiments.\nC. Training Data\nFor the Qwen-R1 models, we utilize logit distillation leveraging the OpenR1-Math-220k dataset. This dataset\ncontains high-quality reasoning traces sampled from DeepSeek R1. To further enhance data quality, we apply\n15\n--- Page 16 ---\nInference-Time Hyper-Scaling with KV Cache Compression\na filtering step using Math-Verify (KydlÃ­Äek and Gandenberger, 2025), retaining only traces resulting in correct\nmathematical solutions.\nFor the Llama 3.2 1B Instruct model, the training corpus comprises two main components: (1) a carefully\ncurated set of programming language examples covering languages such as Python, C, and C++, and (2)\nsynthetic data generated by prompting the model. In particular, we utilize the Llama 3.2 1B Instruct model\nitself to produce completions for the one-dimensional linear algebra subset of the DeepMind mathematics\ndataset (Saxton et al., 2019), which follows the structured format:\nTask format in one-dimensional linear algebra\nSolve aX + b = cX + d for X.\nLlama 3.2 1B prompt for generating responses\n<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 23 July 2024\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: Solve 5*b - 2355 = -50*b - 2740 for b.\nYour response should end with \"The final answer is [answer]\" where [answer] is the\nresponse to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|> Ë“â†’\nIn contrast with the data mixture for Qwen-R1 models, we do not perform correctness filtering on this\nsynthetic, model-generated dataset.\nD. Additional Downstream Evaluations for DMC and DMS\nIn Table 2 we show that DMS can extrapolate beyond the retrofitting context length of 4K, whereas DMC\nmay fail to do so. In Table 3, we show a comparison between Vanilla model, DMS, Quest, and DMC on Llama\n27B.\nE. Downstream Results Significance\nWe provide further analysis regarding the statistical significance and robustness of our experimental results.\nSpecifically, we report standard deviations for the Llama 3.2 1B Instruct models in Table 4, and quantify the\naverage Pareto improvement in Tables 5 and 6. To precisely measure the Pareto improvement, we extract\nPareto frontiers for DMS, the best KV cache reduction baseline, and the vanilla baseline from Figures 3 and 4.\nThen, for each task and model size, we identify the largest common budget interval ğ¼shared by each pair of\nmethods A and B, and compute the average improvement as:\nâˆ«ï¸€\nğ‘¥âˆˆğ¼(ğ´(ğ‘¥)âˆ’ğµ(ğ‘¥))ğ‘‘ğ‘¥\n|ğ¼|\nwhere ğ´(ğ‘¥)andğµ(ğ‘¥)denote the best accuracy achieved by method A and B, respectively, at budget ğ‘¥.\nFor budget values not explicitly measured, we employ linear interpolation.\n16\n--- Page 17 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nTable 2|Needle in the Haystack and Variable Tracking results for 1B parameter Llama 3.2 Instruct model.\nWe note that in contrast to DMC, DMS can extrapolate beyond the retrofitting context length. Note that on\nthe heavily compressible Variable Tracking task, DMS beats even the vanilla model.\nMethod/Task NIAH VT\nContext 3K 4K 8K 3K 4K 8K\nVanilla 99.4 96 .4 97 .261.4 55 .8 41 .2\nCR2\nTOVA 62.8 65 .2 75 .056.0 56 .2 49 .8\nH2O 29.0 34 .0 37 .025.6 27 .4 21 .4\nQuest 99.295.8 97 .460.0 53 .0 36 .4\nDMC 99.0 0 .0 0 .062.4 0 .0 0 .0\nDMSwin=16 99.097.8 99 .472.0 63 .2 56 .0\nCR3\nTOVA 25.8 40 .2 41 .638.4 45 .2 40 .6\nH2O 16.6 17 .2 19 .815.6 17 .6 13 .4\nQuest 99.095.6 97 .060.4 50 .4 31 .8\nDMC 99.0 1 .8 0 .046.4 0 .2 1 .2\nDMSwin=16 99.293.6 24 .276.2 69 .2 58 .8\nCR4\nTOVA 16.8 28 .0 26 .431.4 33 .8 30 .2\nH2O 9.4 13 .4 12 .811.8 12 .6 11 .0\nQuest 98.4 95 .897.657.4 49 .6 32 .4\nDMC 97.0 0 .0 0 .048.6 4 .0 0 .8\nDMSwin=16 99.4 96 .812.274.8 67 .6 57 .2\nTable 3|Results for base Llama 2 7B parameter models. Both DMS and DMC were trained using LM-loss\nwithout logit distillation. Since these models are not instruction-tuned, we evaluate with 8-shot prompting on\nGSM8K, 5-shot on MMLU, 1-shot Needle in a Haystack, and zero-shot on ARC-Challenge and HellaSwag.\n(Nawrot et al., 2024).\nMethod ARC-C GSM8K HS MMLU NIAH\nVanilla 45.6 14 .9 75 .5 45 .4 100 .00\nCR4\nDMSwin=16 45.8 14 .2 76 .0 43 .7 100 .0\nQuest 45.6 14 .5 75 .5 45 .4 100 .0\nDMC 46.2 12 .2 76 .3 43 .9 100 .0\nCR8\nDMSwin=16 46.2 10 .5 76 .4 40 .2 60 .0\nQuest 45.6 11 .6 75 .5 45 .4 100 .0\nDMC 44.7 10 .0 75 .4 41 .7 100 .0\nF. Evaluation Details\nF.1. Implementation of TOVA, H2O, Quest, and DMC\nFor TOVA (Oren et al., 2024), H2O (Zhang et al., 2023a), and Quest (Tang et al., 2024), we calculate\nthe KV-budget by summing the input length and the maximum generation length, then dividing by the\n17\n--- Page 18 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nTable 4|Results from Table 1 expanded with standard deviation as computed by Language Model Evaluation\nHarness (Gao et al., 2024).\nMethod ARC-C GPQA GSM8K HS\nVanilla 31.2Â±1.425.0Â±2.0 44.9Â±1.443.4Â±0.5\nCR2\nDMSwin=16 31.3Â±1.425.7Â±2.1 46.6Â±1.443.3Â±0.5\nTOVA 29.6Â±1.325.2Â±2.1 45.0Â±1.442.8Â±0.5\nH2O 31.1Â±1.426.8Â±2.1 44.0Â±1.442.9Â±0.5\nQuest 31.2Â±1.425.0Â±2.0 45.1Â±1.443.4Â±0.5\nCR3\nDMSwin=16 31.1Â±1.424.6Â±2.0 45.5Â±1.443.3Â±0.5\nTOVA 30.0Â±1.323.7Â±2.0 40.1Â±1.442.5Â±0.5\nH2O 31.2Â±1.424.3Â±2.0 32.9Â±1.342.1Â±0.5\nQuest 31.2Â±1.425.0Â±2.0 44.7Â±1.443.4Â±0.5\nCR4\nDMSwin=16 31.1Â±1.424.3Â±2.0 41.0Â±1.443.4Â±0.5\nTOVA 29.0Â±1.323.7Â±2.0 20.2Â±1.141.8Â±0.5\nH2O 27.5Â±1.323.7Â±2.0 14.7Â±1.041.3Â±0.5\nQuest 31.2Â±1.425.0Â±2.0 39.9Â±1.343.4Â±0.5\nTable 5|Compute-Accuracy Pareto frontier difference. We use linear interpolation for the unknown values of\nthe frontier. NA denotes that the projections of the Pareto frontiers on the budget axis are disjoint.\nMethod AIME 24 MATH 500 GPQA â™¢LiveCodeBench\n1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B\nDMS vs Vanilla 10.6 15 .0 9 .1 4.2 1 .0 1 .6 4.8 4 .1 7 .6 7.3 7 .9 9 .6\nQuest vs Vanilla âˆ’6.8 3 .1 2 .0âˆ’1.8âˆ’0.6NANA NA 2.3 2.5 5 .0 3 .8\nDMS vs Quest 18.8 13 .5 2 .6 6.2 2 .1 1 .4NA NA NA 4.9 3 .4 5 .4\nTable 6|Memory-Accuracy Pareto frontier difference. We use linear interpolation for the unknown values of\nthe frontier. NA denotes that the projections of the Pareto frontiers on the budget axis are disjoint.\nMethod AIME 24 MATH 500 GPQA â™¢LiveCodeBench\n1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B\nDMS vs Vanilla 17.3 15 .7 10 .6 3.3 0 .5 1 .5 4.9 4 .2 7 .6 7.4 8 .4 12 .0\nTOVA vs Vanilla 5.3âˆ’0.2 2 .6 1.3âˆ’1.4 1 .8âˆ’1.1âˆ’2.1 2 .1 3.8 3 .2 4 .9\nDMS vs TOVA 9.6 15 .6 5 .1 2.3 2 .0âˆ’0.1 5.6 6 .5 5 .7 4.0 6 .0 7 .3\ncompression ratio. For H2O, the KV-budget is evenly split between the recent cache and the heavy-hitter\ncache. During evaluation, memory-optimising methods such as TOVA and H2O first perform a standard\nprefill phase until the KV-budget is reached and subsequently switch to their respective memory-efficient\nmechanisms.\nQuest (Tang et al., 2024), unlike TOVA, H2O, DMC, and DMS, does not reduce the KV cache memory\nfootprint. Thus, following the authorsâ€™ recommendations, we permit Quest to perform prefilling using full\ndense attention and set the block size to max(16,2cr). This configuration provides Quest with an advantage\n18\n--- Page 19 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nTable 7|Results from Figure 3 at specified max Length and Width=1 configurations. Those points allow for\na direct comparison with Vanilla model.\nTask Model Size Length Vanilla DMS CR4 Quest CR4\nAIME 24 Qwen-R11.5B 32k 30.0 30.0 26.7\n7B 32k 53.3 53.3 55.5\n32B 32k 70.0 73.3 73.3\nMATH 500 Qwen-R11.5B 32k 84.8 84.0 84.1\n7B 32k 94.0 92.8 93.2\n32B 32k 94.8 94.6 94.6\nGPQA Diamond Qwen-R11.5B 32k 36.5 36.9 37.0\n7B 32k 51.5 48.5 50.2\n32B 32k 63.1 63.6 65.4\nLiveCodeBench Qwen-R11.5B 16k 17.3 17.0 15.8\n7B 16k 35.9 34.4 33.1\n32B 16k 57.0 54.8 54.5\nTable 8|Results from Figure 4 at specified max Length and Width=1 configurations. Those points allow for\na direct comparison with Vanilla model.\nTask Model Size CTX Vanilla DMS CR4 TOVA CR4\nAIME 24 Qwen-R11.5B 32k 30.0 30.0 30.0\n7B 32k 53.3 53.3 46.7\n32B 32k 70.0 73.3 70.0\nMATH 500 Qwen-R11.5B 32k 84.8 84.0 84.3\n7B 32k 94.0 92.8 91.8\n32B 32k 94.8 94.6 95.2\nGPQA Diamond Qwen-R11.5B 32k 36.5 36.9 34.3\n7B 32k 51.5 48.5 47.5\n32B 32k 63.1 63.6 63.1\nLiveCodeBench Qwen-R11.5B 16k 17.3 17.0 14.9\n7B 16k 35.9 34.4 30.7\n32B 16k 57.0 54.8 51.1\nover the other methods. Additionally, we employ a separate top-k for each query head, which can result in\nan increased number of memory transfers for Quest compared to DMS, DMC, TOVA, and H2O. However,\nthe computational cost remains similar. We use this approach as Quest was originally designed for models\nwithout GQA, and we wanted to avoid a custom modification that could potentially degrade the performance.\nIn plots regarding kv-cache memory reads we calculate the total number of different blocks retrieved from a\nsingle key-head. That is we assume optimal implementation that makes use of topk intersections between\nquery heads and retrieves each block only once.\nFor DMC, we follow the implementation described in the original paper (Nawrot et al., 2024).\nF.2. Downstream Tasks\nWe evaluate all downstream tasks in a zero-shot setting, unless stated otherwise.\nFor GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2021a), ARC-Challenge (Clark et al., 2018),\nand HellaSwag (Zellers et al., 2019), we use the Language Model Evaluation Harness (Gao et al., 2024),\n19\n--- Page 20 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nTable 9|Results from Figure Figure 3 comparing DMS wit CR8 to Vanilla (CR1) on specified max Length\nand Width=1 configurations.\nTask Model Size CTX Vanilla DMS CR8\nAIME 24 Qwen-R11.5B 32k 30.0 23.3\n7B 32k 53.3 50.0\nMATH 500 Qwen-R11.5B 32k 84.8 80.0\n7B 32k 94.0 93.0\nGPQA Diamond Qwen-R11.5B 32k 36.5 31.3\n7B 32k 51.5 46.5\nLiveCodeBench Qwen-R11.5B 16k 17.3 16.1\n7B 16k 35.9 33.4\nversion 0.4.3.\nFor Needle in a Haystack (NIAH) (Kamradt, 2023) and Variable Tracking (VT), we adopt the evaluation\nimplementation provided by RULER (Hsieh et al., 2024) and use the context length defined in the retrofitting\nprocedure. For NIAH, we utilize the essay version with a single needle, whereas for VT, we utilize the version\nwith 40 variable chains and 0 hops, filled with repeating text.\nFor AIME24,5GPQA â™¢(Rein et al., 2024), and MATH 500 (Lightman et al., 2024), we evaluate models using\nthe Search and Learn framework (version 0.1.0) (Snell et al., 2024; Beeching et al., 2024), with math-parsing\nfunctionality derived from MathVerify (version 1.0.0) (KydlÃ­Äek and Gandenberger, 2025). For LiveCodeBench\nwe utilize the tasks from 2024-08-01 till 2025-01-31.\nFor few-shot tasks from Language Model Evaluation Harness we directly utilize the framework to provide\nthe few-shot examples. For RULER (Hsieh et al., 2024), we use the example generator to sample few-shot\nexamples. Below we present remaining prompts that were used during the evaluation, except the prompts\nto base models, which were set to unaltered task input, and prompts for zero-shot evaluation of instruction\ntuned models, which were set to task inputs wrapped with HuggingFace tokenizer chat template.6\nFor GSM8K zero-shot evaluation, we adopt the prompt from Meta.\nGSM8K zero-shot prompt\n<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 23 July 2024\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: ___problem_text___\nYour response should end with \"The final answer is [answer]\" where [answer] is the\nresponse to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|> Ë“â†’\nFor Qwen-R1 AIME 24, MATH 500 and GPQA â™¢we adopt the prompts from Open-R1 repository7.\n5https://huggingface.co/datasets/HuggingFaceH4/aime_2024\n6https://huggingface.co/docs/transformers/en/chat_templating\n7https://github.com/huggingface/open-r1\n20\n--- Page 21 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nAIME 24 and MATH 500 prompts\n<|User|>Solve the following math problem efficiently and clearly. The last line of\nyour response should be of the following format: 'Therefore, the final answer\nis: $\\boxed{ANSWER}$. I hope it is correct '(without quotes) where ANSWER is\njust the final number or expression that solves the problem. Think step by step\nbefore answering.Ë“â†’\nË“â†’\nË“â†’\nË“â†’\n___problem_text___<|Assistant|><think>\nGPQA Diamond prompt\n<|User|>Answer the following multiple choice question. The last line of your\nresponse should be of the following format: 'Answer: $LETTER '(without quotes)\nwhere LETTER is one of ABCD. Think step by step before answering.Ë“â†’\nË“â†’\n___problem_text___<|Assistant|><think>\nFor coding tasks we utilize the following prompt adopted from LiveCodeBench(Jain et al., 2025) DeepSeek-R1\nsetting:\nLiveCodeBench prompt\nA conversation between User and Assistant. The user asks a question, and the\nAssistant solves it. The assistant first thinks about the reasoning process in\nthe mind and then provides the user with the answer. The reasoning process and\nanswer are enclosed within <think> </think> and <answer> </answer> tags,\nrespectively, i.e., <think> reasoning process here </think> <answer> answer here\n</answer>.<|User|>You will be given a question (problem specification) and will\ngenerate a correct Python program that matches the specification and passes all\ntests.Ë“â†’\nË“â†’\nË“â†’\nË“â†’\nË“â†’\nË“â†’\nË“â†’\nQuestion: ___problem_text___\n<|Assistant|><think>\n21\n--- Page 22 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nG. Influence of KV Cache on Inference Latency\nIn this section, we provide a simplified analysis estimating the proportion of inference latency introduced by\nreading from the keyâ€“value cache to the entire latency of the step, during a single auto-regressive inference\nstep of an LLM on a GPU. Our calculations are based on the architecture of the Llama 3 model family.\nSpecifically, we derive sample equations for Llama 3.1 8B, parameters of which are listed below.\nParameter Value Description\nğ‘› 32 Number of layers\nğ‘‘ 4096 Hidden dimension\nğ‘‘ff 14336 Internal dimension of the MLP layers\nğ‘‘kv 1024 Dimension of the Key/Value sequences\nğ‘‰ 128256 Vocabulary size\nThe estimates can be expressed in terms of batch size ğµand sequence length ğ¿, which determine the\nnumber of tokens in the KV cache. The number of floating-point operations (FLOPs) can be approximated as\nFLOPS (ğµ, ğ¿)â‰ˆğ‘›ğµ(ï¸€\n6ğ‘‘ğ‘‘ff+ 4ğ‘‘2+ 4ğ‘‘ğ‘‘kv+ 4ğ‘‘ğ¿)ï¸€\n+ 2ğµğ‘‘ğ‘‰. (2)\nThis calculation considers only major matrix-vector multiplications (assuming two FLOPs per multiply-\naccumulate operation), omitting minor operations such as normalization and pointwise non-linearities.\nSimilarly, we estimate the number of reads from the High Bandwidth Memory as:\nReads (ğµ, ğ¿)â‰ˆğ‘›(ï¸€\n6ğ‘‘ğ‘‘ff+ 4ğ‘‘2+ 4ğ‘‘ğ‘‘ff+ 4ğµğ¿ğ‘‘kv)ï¸€\n+ 2ğ‘‘ğ‘‰, (3)\nassuming 2 bytes per parameter (16-bit precision). Note that only the KV cache ( 4ğ‘›ğµğ¿ğ‘‘kv) scales with batch\nsize and sequence length. As a sanity check, we confirm that Reads (1,0)/2â‰ˆ7.5ğµapproximate the modelâ€™s\nparameter count (without 0.5ğµfor the input embedding table, which does not have to be entirely read during\nan inference step). Finally, the approximations for Llama 3.1 8B have the following form:\nFLOPS (ğµ, ğ¿)â‰ˆ1.45Â·109ğµ+ 5.24Â·105ğµğ¿ (4)\nReads (ğµ, ğ¿)â‰ˆ1.50Â·1010+ 1.31Â·105ğµğ¿. (5)\nFor the remaining calculations, we use the peak performance values for NVIDIA H100 SXM ( https:\n//www.nvidia.com/en-us/data-center/h100/ ) for 16-bit calculations without 2:4 sparsity:\nBFLOAT16 Tensor Core performance 989.5TFLOPS\nGPU Memory bandwith 3.35TB/s\nSince memory reads are significantly slower than computations, the latency contribution from KV cache\nreads ( 1.31Ã—105ğµğ¿term) dominates at larger sequence lengths and batch sizes. Thus, KV cache size is a\ncritical factor in inference latency for long sequences.\nThe inference latency per step can be approximated as\nmax(ï¸‚FLOPS (ğµ, ğ¿)\n989.5TFLOPS,Reads (ğµ, ğ¿)\n3.35TB/s)ï¸‚\n, (6)\nassuming ideal overlap between computation and memory operations. Approximating KV cache reads as\n4ğ‘›ğµğ¿ğ‘‘kvand following the same calculations for other Llama and Qwen models, we visualize the fraction of\nlatency contributed by KV cache reads to the latency of entire inference steps (Figure 7).\n22\n--- Page 23 ---\nInference-Time Hyper-Scaling with KV Cache Compression\nFigure 7|Percentage of total latency attributed to KV cache reads. Those reads clearly dominate latency as\nbatch size and sequence length increase. When the KV cache is compressed (CR 4 Ã—and 8Ã—), more tokens\ncan be accommodated before the latency of reading the KV cache becomes an issue.\n23",
  "text_length": 78677
}