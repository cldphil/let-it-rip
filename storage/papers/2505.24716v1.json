{
  "id": "http://arxiv.org/abs/2505.24716v1",
  "title": "Towards Scalable Schema Mapping using Large Language Models",
  "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
  "authors": [
    "Christopher Buss",
    "Mahdis Safari",
    "Arash Termehchy",
    "Stefan Lee",
    "David Maier"
  ],
  "published": "2025-05-30T15:36:56Z",
  "updated": "2025-05-30T15:36:56Z",
  "categories": [
    "cs.DB",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24716v1",
  "full_text": "arXiv:2505.24716v1 [cs.DB] 30 May 2025Towards Scalable Schema Mapping using Large Language Models Christopher Buss∗ bussch@oregonstate.edu Oregon State University Corvallis, Oregon, USAMahdis Safari∗ safarim@oregonstate.edu Oregon State University Corvallis, Oregon, USAArash Termehchy termehca@oregonstate.edu Oregon State University Corvallis, Oregon, USA Stefan Lee leestef@oregonstate.edu Oregon State University Corvallis, Oregon, USADavid Maier maier@pdx.edu Portland State University Portland, Oregon, USA ABSTRACT The growing need to integrate information from a large number of diverse sources poses significant scalability challenges for data integration systems. These systems often rely on manually written schema mappings, which are complex, source-specific, and costly to maintain as sources evolve. While recent advances suggest that large language models (LLMs) can assist in automating schema matching by leveraging both structural and natural language cues, key challenges remain. In this paper, we identify three core issues with using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to input phrasing and structure, which we propose methods to address through sampling and aggregation techniques; (2) the need for more expressive mappings (e.g., GLaV), which strain the limited context windows of LLMs; and (3) the computa- tional cost of repeated LLM calls, which we propose to mitigate through strategies like data type prefiltering. 1 INTRODUCTION There is a recognized need to collect and connect information from a variety of data sources [ 8,10,11]. As an example, we have recently worked in a large-scale NIH-funded project to augment the information of biomedical entities by querying other biomedical data sources [ 30]. The main focus of this project is to repurpose current drugs to treat or mitigate the symptoms of diseases for which there is insufficient time or resources to develop effective treatments (e.g., new or rare diseases) [ 3]. To support such a system, its developers must find and combine a patchwork of data sources to get a full picture of a drug (e.g., clinical trials, research literature, and adverse effects). Collecting all this information is resource- intensive and can be a barrier to important discoveries. Data integration systems are complex and often ingest data from a large number of diverse sources. For each source, programmers must manually write mappings that reconcile structural differences. Writing the correct mappings often requires understanding the semantics of the source. Thus, programmers must cross-reference natural-language descriptions (e.g., database documentation), the logical model, and the actual representation of data. Due to the complexity of mappings, the large number of sources, and the fact that sources evolve over time, integration systems have major scalability problems. Often, mappings are source-specific and cannot be reused. Systems not only become more complex as mappings are added, but with each new source, there is a higher ∗Both authors contributed equally to this research.probability that any one source will change, disrupting the system until developers repair the affected mappings. This results in high maintenance costs. To contend with the growing number of sources, we must develop new tools to reduce the manual effort required to build and maintain data integration systems. Due to the complicated nature of data integration, it requires human-in-the-loop approaches. Many older works have focused on rule-based tools [ 5,7,23], which lack semantic understanding, limiting their usefulness. More recent work has proposed training models to expand their ability beyond basic rules [ 34]. However, doing so requires preparing labeled data specific to certain domains. Recent studies have explored how large language models (LLMs) can be used to generate schema mappings [ 12,18,19,22,24,25,31]. Since LLMs can incorporate a wide range of supporting information, including schema metadata and natural language descriptions, they are especially suited to generating mappings. In this paper we describe three challenges with using LLMs for generating schema mappings. •Inconsistent Outputs: LLMs are highly sensitive to input phrasing and structure, leading to unpredictable and diverse results. We propose techniques for sampling and combining multiple outputs, increasing our mapping coverage and providing effective ways to filter out unlikely mappings. We provide preliminary results that illustrate the effectiveness of our methods. •GLaV with Limited Contexts: Existing LLM-based meth- ods focus on limited mapping types, which are often insuf- ficient for many real-world integration scenarios. Towards supporting more integration systems, we consider the chal- lenges associated with generating more expressive map- pings (GLaV). Supporting such mappings would increase complexity, requiring more sophisticated representations and careful design to avoid overwhelming the LLM’s con- text. •Challenge: Efficient Prompting: LLM-based schema map- ping is computationally expensive due to repeated model calls, especially with large datasets. This, in large part, is thanks to its large input, which only becomes more difficult to manage as we consider more expressive mappings. 2 SCHEMA MAPPING ASSISTANT Our discussion is inspired by tools [ 5,7,23], meant to assist the user in the schema mapping process. Generally speaking, these Buss et al. Local Drugs Drugs Clinical_TrialsTrials meds m_id generic_name trial m_id findings month day year Adverse Effectsid name class uses drug_id outcome dateadverse_compounds formula adverse_effectsside_effects Figure 1: Example integration scenario with a target (left) and two sources (right). Green dashed lines represent seman- tic correspondences between attributes (schema alignment). Dotted represent inter-schema references (foreign keys). systems take the full set of possible mappings and filter them down to a candidate set. The candidate set is then shown to the user so they may verify them, selecting which mappings to implement. This work focuses on mappings between relational databases, but we assert that the challenges highlighted here apply to schema mapping broadly, regardless of the logical models used. It is expected that users will need to verify the output mappings as the underlying model used to generate them will not know the user’s latent intent. In essence, there is nearly always expectations (e.g., business rules not represented within the schema) for which the underlying model is not privy to. 2.1 Preliminaries Data Integration System. Following previous works [ 16], we de- scribe a data integration system Ias a triple⟨G,S,M⟩, where • G is the target schema, which describes a unified view of sources. • S is the source schema, which specifies the structure of the sources to integrate. For the sake of definitional simplicity, we do not distinguish between different sources; instead, we considerSto simply be the union of all source schemas. • M is the mapping between GandS, constituted by a set ofrules, each describing how a subset of Gsemantically corresponds to a subset of S. BothSandGcontain relations 𝑆1,𝑆2,...𝑆 𝑛and𝐺1,𝐺2,...𝐺 𝑚respec- tively. In turn, each relation contains a set of attributes denoted as 𝑎𝑡𝑡𝑟(𝑆𝑖)={𝑠1,𝑠2,...,𝑠 𝑙}and𝑎𝑡𝑡𝑟(𝐺𝑗)={𝑔1,𝑔2,...,𝑔 𝑘}. Example 1. Figure 1 represents an integration scenario within the drug domain. Specifically, to better understand whether certain drugs can be used to treat certain rare diseases, we want to integrate information about each drug’s clinical trials and adverse affects, and likely many other sources not picture here. In our running example, we focus on integrating information from the Trials source: given S={𝑚𝑒𝑑𝑠,𝑡𝑟𝑖𝑎𝑙}andG={𝐷𝑟𝑢𝑔𝑠,𝐶𝑙𝑖𝑛𝑖𝑐 _𝑇𝑟𝑖𝑎𝑙𝑠}, we must define a mapping (M).Mapping Rules (st-tgds). We formally express rules as Source-to- Target Tuple-Generating Dependencies (st-tgds), ∀®𝑥\u0000𝜙(®𝑥)→∃®𝑦𝜓(®𝑥,®𝑦)\u0001 where •𝜙(®𝑥)is a conjunction of atoms over the source S. •𝜓(®𝑥,®𝑦)is a conjunction of atoms over the target G. • ®𝑥are universally quantified variables. • ®𝑦are existentially quantified variables. Both𝜙(®𝑥)and𝜓(®𝑥,®𝑦)may include additional predicates (e.g., for fil- tering tuples). In essence, each rule asserts a pattern over the source, that, if matched, generates tuples adhering to the corresponding pattern in the target. Example 2. Continuing our example in 1, we indicate which tuples in Trials should trigger the generation of tuples in Local Drugs. Further, we indicate which attributes within the new target tuples should be populated, and how that population is determined by the attributes within the triggering source tuples. ∀𝑖,𝑔,𝑓,𝑚,𝑑,𝑦\u0000meds(𝑖,𝑔)∧trial(𝑖,𝑓,𝑚,𝑑,𝑦),𝑦>1990 →∃𝑥1,𝑥2,𝑥3,𝑥4\u0000Drugs(𝑥1,𝜏1[𝑔],𝑥2,𝑥3,𝑥4)∧ Clinical_Trials(𝑥1,𝑓,𝜏 2[𝑚,𝑑,𝑦])\u0001\u0001(1) Target attributes are populated based on their semantic counter- parts in the source. In some instances, value-level transformations are necessary, such as translating a drug’s generic name to its brand name (𝜏1) and concatenating date-parts ( 𝜏2). However, we make a distinction between value-level transformations and schema-level transformations. This work focuses on the latter, though, in the long-term, we foresee it being useful, especially for complex value- level transformations, to tackle the former problem as a separate step from that of schema-level transformations. Referential Dependencies. Often, rules contain referential depen- dencies which condition the existence of rows in one relation upon the existence of join-able rows in another. In the case of rule 1, we specify referential dependencies over both the source and target. Over the source, the shared variable 𝑖in𝑚𝑒𝑑𝑠 and𝑡𝑟𝑖𝑎𝑙 forces tuples from these relations to fall within the same equi join on m_id. Over the target, the shared variable 𝑥1implies the creation of a surrogate key for each answer within the source, ensuring that rows in 𝐷𝑟𝑢𝑔𝑠 are connected to their corresponding rows in 𝐶𝑙𝑖𝑛𝑖𝑐𝑎𝑙 _𝑇𝑟𝑖𝑎𝑙𝑠. Im- portantly, referential dependencies are expressed via the presence of the same variable ( 𝑖and𝑥1) within multiple relational predicates on the left-hand side ( 𝑚𝑒𝑑𝑠 and𝑡𝑟𝑖𝑎𝑙𝑠 ) and right-hand side ( 𝐷𝑟𝑢𝑔𝑠 and𝐶𝑙𝑖𝑛𝑖𝑐𝑎𝑙 _𝑇𝑟𝑖𝑎𝑙𝑠 ) of the same rule. 2.2 Rule Expressiveness Rules are commonly divided into three classes, each of which is defined by the number of relational predicates allowed over the source and target. Formally, these classes are called Global-as-View (GaV), Local-as-View (LaV), and Global-Local-as-View (GLaV). We refer interested readers to [ 16] for a more detailed comparison of these three classes. For the sake of our exposition, we simply make the distinction between those classes that limit either side of a rule to one-and-only-one relational predicate (i.e., GaV and LaV) and the class that does not (i.e., GLaV). Henceforth refer to the former class Towards Scalable Schema Mapping using Large Language Models aslimited referential dependencies (LRD) and the latter class as full referential dependencies (FRD). The rule written in Equation 1 falls strictly within the FRD class as more than one relational predicate appears on both sides. Example 3. To demonstrate the limitations of LRD, we translate rule 1 (written as one FRD rule) into a mapping containing only LRD rules, ∀𝑖,𝑔(meds(𝑖,𝑔)→∃𝑥1,𝑥2,𝑥3,𝑥4Drugs(𝑥1,𝜏1[𝑔],𝑥2,𝑥3,𝑥4)) ∀𝑖,𝑓,𝑚,𝑑,𝑦\u0000trial(𝑖,𝑓,𝑚,𝑑,𝑦),𝑦>1990 →∃𝑥5Clinical_Trials(𝑥5,𝑓,𝜏 2[𝑚,𝑑,𝑦])\u0001 Note that the translation from FRD to LRD requires two rules, isolating the source and target relations. This is problematic since the variables 𝑖and𝑥1do not share the same scope across rules. In other words, the LRD mapping will result in a target instance which does not specify which clinical trials concern which drugs. Further, the instance will contain all drugs in 𝑚𝑒𝑑𝑠 regardless of their most recent clinical trial. Schema Alignments. Rather than generating schema mappings directly, many works focus on the simpler task of generating schema alignments, which can eliminate many undesired mappings from consideration. A schema alignment is a set of pairs, {(𝑠𝑙,𝑔𝑘)|𝑠𝑙∈ 𝑎𝑡𝑡𝑟(𝑆𝑖),𝑔𝑘∈𝑎𝑡𝑡𝑟(𝐺𝑗)}where a pair asserts that source attribute 𝑠𝑙semantically corresponds to target attribute 𝑔𝑘. In limited cases, algorithms can produce the exact correct mapping rules given the alignments as input [ 23]. Figure 1 includes schema alignments for our running example. 2.3 Problem Definition Given a source schema S, a global schemaG, and a set of hints, a Schema Mapping Assistant must produce a candidate set of map- pings𝐶of some class as described in Section 2.2. From the schemata itself, we are guaranteed to have certain information, including re- lation names, attribute names, and any constraints stated within the schema (e.g., primary keys, foreign keys, data types, etc,.). 2.3.1 Hints. In addition to the information given by the schemata, we may also have additional contextual information, which we call hints, that can be leveraged for determining 𝐶. In current work, we consider natural language descriptions of tables 𝑡𝑑𝑒𝑠𝑐(·)and attributes𝑎𝑑𝑒𝑠𝑐(·), as well as sample data values 𝑣𝑎𝑙(·). Example 4. For example, the Drugs in Figure 1 might have the following hints, 𝑛𝑎𝑚𝑒(𝑇)=”𝐷𝑟𝑢𝑔𝑠 ” 𝑡𝑑𝑒𝑠𝑐(𝑇)=”𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛𝑜𝑛𝑐𝑙𝑎𝑠𝑠, 𝑢𝑠𝑒𝑠, 𝑎𝑛𝑑𝑠𝑖𝑑𝑒𝑒𝑓𝑓𝑒𝑐𝑡𝑠 ” 𝑑𝑎𝑡𝑎𝑡𝑦𝑝𝑒(𝑇)={𝑖𝑛𝑡,𝑆𝑡𝑟𝑖𝑛𝑔,𝑆𝑡𝑟𝑖𝑛𝑔..., } 𝑎𝑑𝑒𝑠𝑐(𝑇)={”𝑈𝑛𝑖𝑞𝑢𝑒𝐼𝐷 𝑓𝑜𝑟 𝐷𝑟𝑢𝑔. ”, ”𝐵𝑟𝑎𝑛𝑑𝑛𝑎𝑚𝑒𝑜𝑓 𝑑𝑟𝑢𝑔. ”,...,} Not pictured are other hints that are given by the schema definition. The availability of additional hints depends on the sources them- selves. Data values may not be available, either because the relations are empty or the data itself is restricted due to privacy concerns. In practice, natural language descriptions may be derived from documentation, but even this is not guaranteed to exist.2.3.2 Candidate Set Quality. The goal is to provide users with a high-quality candidate set. Ultimately, what makes a candidate set \"high-quality\" depends on many factors, including user preference. However, we often prioritize recall over precision: maximize the number of true mappings while simultaneously minimizing the number of false mappings. This is because it often takes less effort for a human to confirm that a candidate mapping is wrong than it does for them to determine the correct mapping on their own. 3 CHALLENGE: INCONSISTENT OUTPUTS The output of LLMs depend heavily on how their input is phrased. The space of possible outputs is often diverse and varies signif- icantly in terms of quality. Further, it is difficult to predict the relative impact that different phrasings will have on the output. For example, minor adjustments in the ordering of content—such as rearranging rows or columns in a table—can impact accuracy, as LLMs are sensitive to the structure of the input data. However, most existing schema matching approaches overlook these sensitivities. They typically rely on a single, static prompt and do not account for structural variations in the input. For ex- ample, Parciak et al [ 22] repeat the same prompt multiple times and aggregate the outputs using majority voting to approximate a high-confidence result, rather than varying the prompt itself. In contrast, we treat prompt variation as a key mechanism for ex- ploring the model’s output space more effectively and improving overall quality of the final candidate set. 3.1 Sampling Outputs Instead of relying on a single prompt, we treat this as a sampling problem. More specifically, we view the model as a black-box from which we can sample mappings by providing different prompt phrasings. More formally, we start with a reasonably effective prompt template 𝑃(·), and then apply a set of transformations that exploit symmetric properties of our problem. We prompt the LLM𝑛times to produce candidate sets 𝑐1,𝑐2,...𝑐 𝑛. We then combine these sets using a function 𝐴(·)to produce the final candidate set 𝐶′=𝐴(𝑐1,𝑐2,...𝑐 𝑛). Sampling multiple outputs for the same input can benefit us in two key ways. First, it expands our coverage of the hypothesis space, increasing the chance of discovering high-quality alignments that might otherwise be missed with a single static prompt. Second, it provides insight into the relative likelihood of different mappings: if a particular output appears consistently across multiple samples, it may serve as a proxy for confidence in its quality. 3.1.1 Symmetric Transformations. Starting from a well-performing base prompt, we apply transformations that alter the input for- mat without changing the underlying task. Specifically, we intro- duce variation by randomly permuting the order of columns in the prompt, sampling different data instances, and swapping the source and target tables. These transformations preserve the semantic equivalence of the task while encouraging the model to explore different parts of the output space. 3.1.2 Sample Merging. To combine the candidates generated from each prompt group (e.g., prompts using the original table orienta- tion or the swapped orientation), we apply aggregation functions Buss et al. such as union, majority vote, or intersection. The choice of function depends on the desired trade-off between recall and precision. For example, union maximizes recall by including all possible matches, majority vote refines the output by selecting the most frequent align- ments, and intersection focuses on the most consistent matches across all outputs. Chen et al. [ 6] explore LLM-based consistency sampling by prompting the model to select the most coherent response among a set of candidates. While their approach is designed for free-form generation, applying it directly to schema matching would require an additional LLM call over a long, concatenated prompt, which can be problematic for structured outputs with high token length. More- over, selecting a single response is incompatible with our setting, where multiple candidate mappings per attribute must be consid- ered and further analyzed. Instead, we apply logical operations such as majority vote over structured JSON outputs, providing a lightweight and scalable alternative without added inference cost or context limitations. 3.2 Bidirectional Schema Matching To account for alignments from both the original and swapped table perspectives, simple aggregation functions like majority vote or union are not ideal. These methods fail to consider the differing confidence levels that the LLM may have for matches from each perspective. Instead, we propose estimating a confidence score for each candidate match in both directions before merging the results, leading to a more reliable combination. MatchMaker estimates the LLM’s confidence in alignments by prompting it to score candidate matches for each attribute. However, these scores may not accurately reflect true probabilities. Inspired by, we approximate the LLM’s confidence by first asking it to select the best match among the candidates, then using its output logits to compute confidence scores. Combining results from the original and swapped table perspec- tives presents a challenge, as its not clear which merging method is most effective. We explore different techniques for this task. As a starting point, we test two simple approaches: averaging and multiplying confidence scores. Averaging reduces the confidence if an alignment appears in only one direction, while multiplication removes alignments that are missing from either direction. In addition, we apply the stable matching algorithm [ 2] to the ranked matches from both directions. This approach conceptually aligns with the table-swapping process, as it reflects the bidirec- tional nature of preferences and ensures a stable and consistent matching between attributes. We define Stable Schema Matching as follows: Definition 3.1 (Stable Schema Matching). Given two schemas 𝐴and𝐵, the input consists of two sets of attributes, 𝐴[attr]= {𝑎1,𝑎2,...,𝑎 𝑛}and𝐵[attr]={𝑏1,𝑏2,...,𝑏 𝑚}, along with ranked preference lists for each attribute in 𝐴[attr]over the attributes in 𝐵[attr], and vice versa. The goal is to compute a stable matching between attributes in 𝐴and𝐵such that no unmatched pair of at- tributes would prefer each other over their current matches. The output is a set 𝑀={(𝑎𝑖,𝑏𝑗)|𝑎𝑖∈𝐴[attr],𝑏𝑗∈𝐵[attr]}contain- ing the stable matches between attributes in 𝐴and𝐵, ensuring that the matching is mutually acceptable and stable. The number ofmatches can be constrained by a parameter 𝐾, which specifies the top𝐾stable matches for each attribute. An overview of our bidirectional matching design is shown in Figure 2. For more details on our prompts, how we compute the confidence score, and the stable matching algorithm, please refer to the technical report1. Figure 2: Bidirectional schema matching process. Matching is performed in both directions by swapping the roles of source and target tables. Each direction involves aggregation and ranking of candidate matches, which are then merged to produce the final results. 3.3 Preliminary Results In this section, we evaluate the impact of our proposed prompting strategies and bidirectional approach with symmetric transforma- tions, demonstrating how these methods help an open-source model achieve competitive performance compared to proprietary models. Problem Definition. We focus on the simplest form of mapping, which is nonetheless a difficult task. Specifically, given relations 𝑆𝑖and𝐺𝑗, we want to to produce a set of pairs, each representing an alignment between a source attribute and a global attribute. Formally,𝐶={(𝑠𝑙,𝑔𝑘)|𝑠𝑙∈𝑎𝑡𝑡𝑟(𝑆𝑖),𝑔𝑘∈𝑎𝑡𝑡𝑟(𝐺𝑗)}indicating that the attribute 𝑠𝑙semantically corresponds to the attribute 𝑔𝑘. Prompt Template and Techniques. We use reasoning-based prompt- ing strategies to improve LLM performance in zero-shot settings. Specifically, we prompt the LLM three times with transformed inputs using a fixed seed. To ensure consistent output formatting and enable efficient processing, we constrain the model to produce structured JSON output [ 29]. Each prompt follows an 𝑁-1 format, where𝑁source attributes and one target attribute are serialized in JSON to improve structural understanding [ 26,27]. This𝑁-1 strategy has been shown to outperform other settings in terms of matching effectiveness [ 22]. We conducted experiments to evaluate the impact of different types of metadata. Our results show that natural language descriptions of attributes consistently provide the largest boost to schema matching performance. Even with basic schema details such as table names, attribute names, and data types, the LLM-based method outperforms traditional approaches such 1The technical report can be found at https://research.engr.oregonstate.edu/ idea/sites/research.engr.oregonstate.edu.idea/files/scalable_schema_mapping- technical_report.pdf Towards Scalable Schema Mapping using Large Language Models as COMA in the single-prompt setting. We included data values based on the idea that example values could help the model better understand attribute meaning. Interestingly, data values caused a slight drop in performance in single-prompt setups but proved more helpful when aggregating results across multiple prompts—each varying in column order and sampled values. For more details and results, please refer to our technical report. To improve the semantic reasoning of the LLM, we include all available schema metadata in the prompt, including attribute names, data types, descriptions, ten randomly sampled unique data values, and relation descriptions. Datasets. We conduct our experiments on two widely used schema matching datasets: MIMIC-OMOP and Synthea-OMOP. To popu- late OMOP with data values, we used sample data from MIMIC-IV formatted under the OMOP model. MIMIC-IV differs enough from MIMIC-III to avoid record overlap but remains conceptually similar. However, some aligned columns lack sufficient sample data due to privacy restrictions or missing data. Many columns also lack descriptions, which adds complexity to schema matching. MIMIC- OMOP includes 26 schema pairs that align real-world healthcare databases: MIMIC-III and the OMOP Common Data Model. It covers 268 source attributes, 203 target attributes, and 155 ground truth matches. This dataset reflects the complexity of real-world medical data. In contrast, Synthea-OMOP is based on synthetic healthcare records generated by Synthea and aligned to OMOP. It contains 12 schema pairs with 101 source attributes, 134 target attributes, and 105 matches. Although synthetic, Synthea captures realistic data variability and schema ambiguity. This makes it a challenging and widely used benchmark for schema matching tasks. Baselines. We compare our methods with three baselines that require no training data. The first is COMA [ 7], a widely used schema-matching method known for its efficiency and flexibility, which has been refined through several iterations [ 4,20]. COMA is a rule-based approach that lacks semantic understanding, with the schema-based version considering only schematic information, and the instance-based version incorporating both schematic in- formation and data values. As a result, COMA can miss important semantic nuances and struggle with the complexities of real-world schemas like those in MIMIC-OMOP and Synthea-OMOP, which affects its overall matching performance. We evaluate both versions using Valentine’s Python wrapper for COMA 3.02. For language model baselines, we use the N-1 prompting method from Parciak et al. [ 22]. This method is simple and effective. It works by aggregating multiple prompts, similar to our own aggregation approach. However, It does not provide ranked match suggestions, which are often useful in practice. It also restricts mappings to one- to-one correspondences, overlooking scenarios where an attribute may align with multiple counterparts. We also evaluate Match- Maker [ 24], which refines alignments with pre- and post-filtering steps. Although MatchMaker improves performance by filtering, its multi-step prompting pipeline introduces inefficiencies and can fail if intermediate language model outputs deviate from expected patterns. Both methods are implemented based on their respective papers, and for a fair comparison, we exclude the pre-filtering step in MatchMaker, as our method does not include it 2https://github.com/delftdata/valentineMetrics. We report the average Precision@k, Recall@k, and F1@k for both our approach and the LLM baselines, averaged across three different random seeds. For methods that do not rank the alignments, we report metrics at 𝑘=max. For the bidirectional methods using stable matching and multiplication, the value of k is limited based on the pipeline, as the final alignments must be present in the aggregated candidates from both directions. There- fore, we report a limited k for these methods. For the method using averaging, we report the maximum k obtained from the Match- Maker method. In real-world data integration scenarios, automated matching candidates require manual validation. Therefore, we aim to achieve high precision and recall simultaneously to reduce man- ual effort while ensuring high-quality matches. Many studies across various tasks, including schema matching, demonstrate that larger language models perform better [ 13,22, 28,33]. However, the most advanced models are typically available only via APIs, which raises significant privacy concerns. In these experiments, we use the Meta- Llama-3.1-70B-Instruct-GPTQ-INT43, the largest model we could run on the server. More details on the evaluation setup can be found in our technical report. Method Experiment k P@k R@k F1@k AggregationOriginal tables max 0.35 0.79 0.47 Swapped tables max 0.47 0.67 0.54 BidirectionalStable Matching1 0.68 0.62 0.64 2 0.66 0.63 0.64 Average1 0.37 0.78 0.49 2 0.31 0.82 0.44 3 0.30 0.83 0.43 Multiply1 0.67 0.62 0.64 2 0.66 0.63 0.64 BaselineMatchMaker1 0.25 0.24 0.23 2 0.15 0.30 0.19 3 0.11 0.31 0.15 COMA Sch. max 0.14 0.11 0.10 COMA Inst. max 0.21 0.14 0.16 Parciak et al max 0.30 0.15 0.18 Table 1: Precision@k (P@k), Recall@k (R@k), and F1@k for different methods on the MIMIC dataset. 3.3.1 Evaluation Results.The results in this paper use the ma- jority vote aggregation method, as it offers the best balance be- tween recall and precision. Results for other aggregation methods are available in our technical report. As shown in Tables 1 and 2, our aggregation method outperforms all baselines. In addition to our symmetric transformations, the key difference from Parciak et al. lies in the output format and schema serialization used in the prompt. Their method struggles with incomplete responses, where the model often skips the final decision4. MatchMaker also fails when the LLM does not follow the required format in intermediate 3The \"70B\" refers to the model’s size, with 70 billion parameters. \"Instruct\" indicates that the model is fine-tuned for instruction-following tasks. \"GPTQ\" is a quantization method that optimizes memory efficiency and improves inference speed. \"INT4\" refers to 4-bit integer quantization, a specific technique used within GPTQ to further reduce memory usage while maintaining performance. 4Example: The first attribute to consider is {𝑠𝑜𝑢𝑟𝑐𝑒 _𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒}. Does {𝑠𝑜𝑢𝑟𝑐𝑒 _𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒}semantically match {𝑡𝑎𝑟𝑔𝑒𝑡 _𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒}? Buss et al. steps. While both MatchMaker and Parciak et al. used GPT-4 in their studies, we employed a significantly smaller model in our experiments. As a result, our prompt is more effective for smaller models, which we attribute to its clear output format and structured JSON schema serialization. On the MIMIC dataset, LLM-based methods outperform COMA, highlighting that LLMs excel in domains where domain knowl- edge is crucial, thanks to their ability to process natural language descriptions. In contrast, on the Synthea dataset—where vocabu- lary and attribute names require less domain knowledge—Parciak et al.’s method does not outperform COMA. This suggests that when domain knowledge is less critical, the choice of pipeline and prompting method, especially for smaller models, becomes more important. We also observe that the bidirectional method using multiplica- tion outperforms the others, achieving the best F1@1 score. The bidirectional method using stable matching closely follows. The difference lies in our use of confidence score ranking in stable matching, while multiplication computes final alignment confi- dence based on the actual values of scores from each direction. These methods are effective for tasks where both precision and recall are important. The bidirectional method using averaging, though lower in precision, excels in recall and is preferred when high recall is prioritized. We compared our methods against LLM baselines originally de- signed for GPT-4. MatchMaker is directly comparable, as it also eval- uates on the MIMIC and Synthea datasets. We assess performance by comparing our bidirectional method, based on an open-source model, with the full MatchMaker pipeline using their reported ac- curacy@1. As shown in Table 3, our bidirectional method using stable matching and multiplication achieves accuracy compara- ble to MatchMaker on the Synthea dataset. It also outperforms MatchMaker on the MIMIC dataset. Our bidirectional approach, combined with symmetric transfor- mations, delivers strong results on clinical datasets like Synthea Method Experiment k P@K R@K F1@K AggregationOriginal tables max 0.57 0.95 0.70 Swapped tables max 0.52 0.56 0.51 BidirectionalStable Matching1 0.78 0.52 0.60 2 0.77 0.55 0.62 Average1 0.58 0.95 0.71 2 0.50 0.95 0.65 3 0.48 0.96 0.63 Multiply1 0.77 0.55 0.62 2 0.77 0.55 0.62 BaselineMatchMaker1 0.45 0.21 0.27 2 0.23 0.21 0.21 3 0.15 0.21 0.17 COMA Sch. max 0.30 0.15 0.19 COMA Inst. max 0.30 0.16 0.20 Parciak et al max 0.53 0.11 0.17 Table 2: Precision@k (P@k), Recall@k (R@k), and F1@k for different methods on the Synthea dataset.Dataset Method Accuracy@1 MIMICMatchMaker 62.20±2.40 Bidirectional (Stable Matching) 0.78±0.00 Bidirectional (Average) 0.49±0.01 Bidirectional (Multiply) 0.77±0.01 SyntheaMatchMaker 70.20±1.70 Bidirectional (Stable Matching) 0.69±0.01 Bidirectional (Average) 0.64±0.01 Bidirectional (Multiply) 0.70±0.01 Table 3: Comparison of our proposed method using Llama-3.1-70B- Instruct-GPTQ-INT4 against MatchMaker’s using GPT-4. and MIMIC, achieving accuracy comparable to or exceeding GPT- 4-based models such as MatchMaker. While their results were ob- tained using a significantly larger model, our approach, built on a smaller open-source model, performs competitively. This highlights that strong performance can be achieved not just through model scale or domain knowledge, but through careful pipeline design. In addition to improved prompting strategies, such as more effec- tive schema serialization, our method introduces order variations at the table, column, and data value levels to encourage broader exploration of the alignment space. These design choices highlight the impact of pipeline structure in maximizing the effectiveness of LLMs for schema matching, even without access to proprietary models. 3.3.2 Sampling Techniques for FRD Mappings. Our evaluation indi- cates that, with the correct sampling techniques, open source LLMs are highly competitive with at least one major, proprietary model (GPT-4). However, these techniques depend on decomposing the LLM’s responses into a fairly limited number of atomic elements. For example, the final answer in the LLM’s response for schema alignment is pairs of source columns and target columns. It is easy to break the output into sets of these pairs, making the ordering irrelevant, Further, each pair is reflexive in the sense that (A, B) is identical to (B, A), allowing us to leverage bidirectional techniques. In essence, sampling techniques require outputs that are simple enough such that they can be broken down into atomic elements and overlap between responses can be calculated. Extending these techniques to schema mappings is not as easy do to the increased complexity in the output language. A mapping is essentially a collection of queries, and it is not immediately clear how one might effectively partition such an output to test for over- lap. One way to partition the output is on a per-query basis, testing for the repetition of queries within outputs. However, using exact string comparison would be much too restrictive, likely leading to very little overlap, even if there are logically-equivalent queries. Simultaneously, the logical-equivalency of queries is not guaran- teed to be decidable [ 1]. Further, if a given rule is not invertible, then the bidirectional technique cannot be used. This makes such techniques incorrect in a theoretical sense, but empirically, such techniques may still be useful if employed correctly. Clearly, this is an important subject for future research. Towards Scalable Schema Mapping using Large Language Models 4 CHALLENGE: REPRESENTATION AND LARGE INPUT As discussed in Section 2.2, existing research does not consider FRD rules making the associated techniques insufficient for many com- mon mapping scenarios. In this section, we consider two immediate challenges associated with Zero-Shot generation of FRD mappings using LLMs. First, we discuss the difficulties with representing FRD rules. Second, we discuss the overall complexity of generating a full FRD mapping. We close this section with an empirical study meant to help us better understand these challenges and establish future research. 4.1 Output Representation LLMs have show great success at generating SQL [ 17], making it a promising candidate for representing rules. However, an SQL query can only represent a single GaV rule because it’s output is always only a single table. In fact, most popular query languages can only produce individual GaV rules. That being said, multiple SQL statements can be used in tandem to represent a GLaV rule. Whether an LLM will produce such scripts is another issue. That being said, it is possible to translate SQL into other classes. For example, in the context of importing tabular data into a relational target, one work prompts an LLM to generate a query over the target and then inverts it to produce a LaV rule [ 12]. However, this approach only works if the GaV counterpart is invertible itself, a theoretically needless limitation. 4.2 Input Given some unknown rule, the LLM can only generate it if it is provided, at minimum, the schema fragments appearing in the rule. On the other head, existing works have shown that filtering input to that which is relevant often improves performance. It is reasonable to believe that we would also benefit from filtering out irrelevant schema. However, such a task is not straightforward. A mapping could be quite large, with rules covering the entirety of the source and target schemas. In such an instance, it perhaps makes more sense to break the full mapping down into parts. A sensible approach would be to segment the full mapping at the rule level. However, since we do not know the underlying rules, we would need to predict their contents related to source and target relations. Relations within Rule k. As the same relation may appear multiple times within a conjunction, we define S𝑘andT𝑘to be the unique relations appearing in 𝜙(®𝑥)(i.e., source relations appearing in left- hand side of the rule) and 𝜓(®𝑥,®𝑦)(i.e., target relations appearing in right-hand side of the rule), respectively, for the kth rule. We do not know either S𝑘orT𝑘for any given rule (much less the rule itself). And finding S𝑘andT𝑘is, itself, a form of schema filtering. However, knowing neither set of relations makes filtering a potentially multi-step process where we must predict subgraphs of the target and the source and then pair those subgraphs across the source and target correctly. This may be quite hard, perhaps motivating solutions that are very relaxed in how they filter schema. When developing a schema filter, we must consider its restrictive- ness: we want a filter that maintains as many relevant schema partsas possible while also reducing the size of the context. This presents an optimization problem where we must decide how aggressively to filter such that the LLM’s performance does not significantly degrade either due to a lack of context (information) or an overly noisy context (too much irrelevant information). 4.3 Preliminary Results In this section, we present experiments meant to provide some preliminary insight into the two questions raised in this section: 1) in what ways is SQL sufficient (insufficient) for generating GLaV rules? and 2) How does the input size affect the final set of map- pings? For the later question, we assume that we have already found S𝑘andT𝑘for each kth rule. The question is, how do we chunk these specifications into prompts. Prompt Template and Techniques. Similar to our prompt discussed in Section 3.3, we assume a zero-shot setting and encourage the model to reason about its mapping prior to producing its final script. Each prompt can contain multiple source relations and target rela- tions, each of which is serialized as JSON. We include all available and relevant metadata. For each relation, we include its name, pri- mary key, and any foreign key relationships. For each attribute, we include its name, type, whether it is NULLable, and up to 10 instance data values, uniformly sampled without replacement. Each sampled data value is truncated to 100 characters. Dataset. We use a subset of Amalgam [ 21], a commonly-cited schema mapping benchmark. Of Amalgam’s four bibliography databases, we take S1 as our source and S2 as our target. The gold mapping contains 7 rules. Generally speaking, it involves decompos- ing publication-type relations (S1) into attribute-specific relations (S2). During our research, we have noticed a surprising lack of schema mapping benchmarks despite the prevalence of the prob- lem. Unfortunately, many benchmarks have broken links. We have pieced Amalgam together from a few places: we use the schema definitions from the original source [ 21], the data provided by [ 15], and the ground truth as provided by the iBench scenario Github page5. Notably, the ground truth mappings are specified using a proprietary format. We translate these to SQL for our purposes. We hope that the inaccessibility of the original dataset helps cir- cumvent any data leakage issues. Amalgam does not contain any natural language descriptions of attributes or relations. However, given its common domain, we hypothesize that an LLM should have a general understanding of the domain. Metrics. We borrow a metric commonly used in Text-to-SQL called execution accuracy which measures the overlap between the results obtained from predicted queries and ground-truth queries. However, instead of only reporting either 1 (full overlap) or 0 (any- thing less than full overlap), we report the percentage of overlap. As discussed in Section 2, most data integration systems follow a human-in-the-loop approach, where a user can validate and fix predicted mappings. Thus, it is useful to report the proximity of a predicted mapping to that of the ground truth. To test overlap, we need to compare the effect of both map- pings given the same input target-instance. For evaluation data, 5https://github.com/RJMillerLab/ibenchScenarioCollection Buss et al. we generate 100 rows for each table in the target database. When schematically valid, we insert NULLs into attributes for some rows and remove some foreign key references, resulting in some parent rows with no children (i.e., some authors with no publications and some publications with no authors). We apply both the gold and predicted mapping to produce a gold target instance 𝐼′and a pre- dicted target instance 𝐼. We then apply an exhaustive set of test queries to both instances. For each test query 𝑞𝑖, we calculate false positive rows (FP), false negative rows (FN), and true positive rows (TP) as follows, 𝐹𝑃𝑖=𝑞𝑖[𝐼]−𝑞𝑖[𝐼′]𝐹𝑁𝑖=𝑞𝑖[𝐼′]−𝑞𝑖[𝐼]𝑇𝑃𝑖=𝑞𝑖[𝐼′]∩𝑞𝑖[𝐼] We then calculate recall (R), precision (P) as, 𝑅𝑖=|𝑇𝑃𝑖|/(|𝑇𝑃𝑖|+|𝐹𝑁𝑖|)𝑃𝑖=|𝑇𝑃𝑖|/(|𝑇𝑃𝑖|+|𝐹𝑃𝑖|) Finally, F1-score is calculated as 𝐹1𝑖=(2.0∗𝑅𝑖∗𝑃𝑖)/(𝑅𝑖+𝑃𝑖). We test for two kinds of overlap: Table Overlap and Join Over- lap. In both cases, we project over all columns except for arbitrary primary keys and foreign key references. For our dataset, these keys have no semantic meaning. Thus, correctness depends on upon establishing the correct references between rows and not on the particular values used to do so. our only expectation is that, no matter the values assigned to these keys, the correct references are established between rows. Table Overlap. Let𝑇1(𝑥1,¯𝑥1),𝑇2(𝑥2,¯𝑥2),...,𝑇 𝑛(𝑥𝑛,¯𝑥𝑛)be relations in the target schema, where 𝑥1,𝑥2,...,𝑥 𝑛is each relation’s respective set of attributes (columns), and ¯𝑥1,¯𝑥2,...,¯𝑥𝑛are the primary key columns and foreign key references for their respective relation. For each𝑇𝑖, we test for overlap using 𝑞𝑖(𝑥𝑖):−𝑇𝑖(𝑥𝑖∩¯𝑥𝑖)if both 𝑞𝑖[𝐼′]and𝑞𝑖[𝐼]return no results, then we do not consider 𝑞𝑖in our final calculation. This prevents the overlap from being inflated by target tables that are not touched by any mapping. Join Overlap. We indicate the target relations appearing in the kth gold mapping with T𝑘=𝑇1(𝑥1,¯𝑥1),𝑇2(𝑥2,¯𝑥2),...,𝑇 𝑙(𝑥𝑙,¯𝑥𝑙). For eachT𝑘, we test for overlap using 𝑞𝑘(Ð𝑙 𝑗=1𝑥𝑗):−𝑇1(𝑥1∪¯𝑥1)⊲⊳ 𝑇2(𝑥2∪¯𝑥2)⊲⊳...⊲⊳𝑇𝑙(𝑥𝑙∪¯𝑥𝑙). In instances where |T𝑘|=1, we drop query𝑞𝑘as it devolves to a query with no joins, which is already covered by Table Overlap. In instances where we have multiple, identical queries due to target-relation overlap in our gold mapping, we remove all but one of the queries. After calculating individual metrics, we take the average of all queries. It is worth noting that perfect Join Overlap (i.e., F1 = 1.0) also implies perfect Table Overlap, but perfect Table Overlap does not imply perfect Join Overlap. In fact, low Table Overlap is likely connected with very low table overlap scores given the nature of the metrics. Producing high Join Overlap requires that the model effectively generate the correct references between rows, sometimes requiring the model to invent new keys. 4.3.1 Evaluation Results. We conduct an empirical evaluation to answer two questions. First, we want to know how does chunking affect performance; namely, are there benefits of producing more chunks at a time. What are the drawbacks of having the LLM do more work with fewer prompts? Second, we want to better un- derstand the limitations of a baseline approach, which will help establish promising research directions.Rules / Prompt Input Tokens Output Tokens 1 3910 1104 2 5024 1484 3 6425 1838 4 7596 2053 5 8839 2489 6 9983 2849 7 11259 2639 Table 4: The average number of input and output tokens according to the number of rules a prompt is given specification for (i.e., the underlying source and target relations). Each average is calculated over 20 random prompt/response pairs. To understand the effect of input size on mapping quality, we vary the input size of prompts with respect to how many (S𝑘,T𝑘) pairs we supply in the prompt. As more pairs are added, the model will not only need to parse more information, but will also need to generate more code. We treat the Max. Rules per Prompt (MRPP) as a hyperparameter and vary it from 1 to 7. For example, if MRPP is 5, we will prompt the model twice: once with a specification for 5 rules and again with a specification for 2 rules (one prompt will have fewer rules if the number of mappings is not divisible by MRPP). Rules are uniformly sampled without replacement, and as we add rules, we remove overlapping relations. We use the same LLM here (Meta-Llama-3.1-70B-Instruct-GPTQ- INT4). For each MRPP, we prompt the model 20 times using different seeds, controlling the order in which relations and attributes within those relations are presented and which data values are sampled. Unlike the previous section, we do not combine the outputs of these different models, but rather use this as a way to get a more robust measurement of performance that is not dependent on a single representation of the input. As discussed, combining these outputs is a viable technique for producing better programs, but it is saved for future works. We report 95% confidence intervals for our metrics. Figure 3 shows the performance averaged over seeds for each setting for MRPP. Though we see a decrease in performance overall, it is most drastic for recall, implying that the model omits parts the rules whenever more than one rule is included in the prompt. Table 4 gives us an idea of how the complexity of the input and output grows as more rules are added. 5 CHALLENGE: REDUCING POOR-QUALITY MAPPINGS In Section 3, we proposed methods for filtering the candidate set based on sampling multiple outputs from an LLM. Furthermore, we empirically showed how such methods can improve the candidate set of mappings. However, sample-based filtering will not eliminate false positives if they consistently appear in the output. Thus, it is worth considering other filtering strategies that can complement these sample-based methods. We propose additional methods for filtering the candidate set based on markers of mapping quality. Towards Scalable Schema Mapping using Large Language Models /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni00000030/uni00000044/uni0000005b/uni00000011/uni00000003/uni00000035/uni00000058/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f /uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 Figure 3: Average precision, recall, and F1-Score plotted against the maximum rules per prompt (MRPP). Error bars represent a 95% confidence interval. 5.1 Constraint-Based Filtering Mappings must be consistent with both the explicit and implicit (semantic) constraints of the global schema. Explicit constraints include those stored in the schema and are often enforced by data- base management systems. For example, mappings must popu- late required attributes and respect attribute data types. Moreover, mappings must respect semantic constraints, which are often not enforced by the database, but are still generally followed by its users. Some semantic constraints are commonsense regardless of the domain. For example, each row within a relation must have some information content. This constraint can be violated when a relation has an arbitrary primary key, which is a common database design practice. This can create situations where the primary key is populated but all of the attributes of the row are NULL, functionally producing a row with no real information content. Another source of semantic constraints are business rules, which are often documented (likely as an entity-relationship diagram) as part of the global schema’s design. Some semantic constraints are commonsense given the domain (e.g., each child must have exactly two biological parents) while others may be esoteric and purely process-driven. Regardless, many such constraints are often not stored in the schema. Instead, they are very likely enforced by the end-user when they inspect the final mapping set. In fact, the existence of latent constraints is one important reason for why humans must validate candidate rules. Specifying such constraints could be valuable in eliminating unfavorable mappings from the candidate set. Further, since such constraints specify how data should be organized within the target schema, theoretically, they could be specified once and then be used to produce higher-quality candidate sets for all sources, the very definition of a scalable technique. Of course, it is likely that even these constraints will need to be edited as the target itself evolves.Using semantic constraints is a promising technique for enabling scalable data integration systems, but there are important questions: how would one explicitly define these constraints and how could they be used in practice? It would tedious and difficult to write out and manage all of these underlying constraints, so a more practical approach would be to derive them from other sources. For example, if we already have data in our target, we can use techniques to derive constraints from that data. Further, some such constraints could be learned through user preferences over candidate mappings. 5.2 Model-Based Filtering Users may not want to semantic constraints into logical statements as it, admittedly, may require significant overhead. Alternatively, one can leverage models of what consistent data \"looks like\". When adding new sources or maintaining existing ones, we can material- ize the data (i.e., run the candidate mapping) and check the quality of the instance. Assuming that the current instance data in the global up to this point is a good representation of \"high-quality data. \" We can derive characteristics of that data that end up being indicators of quality. In some way, building a model that, through unsupervised learning, constructs its own non-logical black-box constraints of data quality based on the current instance. Reward models could essentially measure the similarity of of existing data and new data (proposed data from candidate mapping)–how well do they \"mix\". Of course, this raises questions of how we might prevent the model from becoming biased towards our current instance. For example, if our current instance only contains publications from VLDB, the model might associate publication[title]=\"VLDB\" as an important indicator of quality, and conversely, would assume that instances where, for some publications, publication[title]!=\"VLDB\" indicates a poor mapping. Buss et al. Max. Rules/Prmpt Input Output Total Reduction 1 27577 8135 35712 – 2 19204 5282 24486 1.46 3 16635 4633 21268 1.68 4 14087 3777 17864 2.00 5 14083 4068 18151 1.97 6 13884 4006 17890 2.00 7 11259 2639 13898 2.57 Table 5: How chunking affects the number of tokens processed. \"Reduction\" specifies efficiency relative the first row (i.e., using a separate prompt for each rule) 6 CHALLENGE: EFFICIENT PROMPTING A major challenge in LLM-based schema mapping is its high compu- tational cost: for large datasets, many tokens need to be processed. Though computation is generally cheaper than human attention, it is necessary to consider the trade-off between performance and computational costs. imprecisely speaking, techniques should pro- duced the desired results without excessive computation. This is especially important for those who must pay a third party for computational resources. We discuss three strategies for reducing computational costs (i.e., tokens processed). Reducing Unnecessary Comparisons (Don’t ask LLM trivial things). One way to reduce unnecessary comparisons is through data type prefiltering. By categorizing attributes into broad types—such as Numeric, Text, Date/Time, and Boolean—we can reduce the number of source attributes that need to be compared to a target attribute. In an N-1 matching setup, where each prompt compares one target attribute to multiple source attributes, prefiltering ensures that only source attributes with the same data type as the target attribute are included in the comparison. This reduces the pool of source at- tributes from N to a smaller subset, k, improving both the efficiency and accuracy of the model by eliminating irrelevant comparisons. Another approach is to use the results from the first round of pre- dictions to assess confidence. If the top match has a much higher score than the others, there may be no need for additional rounds of comparison, further reducing the number of calls. By refining the selection process based on confidence and narrowing down the pool of candidate pairs, we can make the schema matching process more efficient and scalable. Efficient Chunking. As discussed, one tunable parameter is the amount of work induced (i.e., code written) by each prompt. We observed in section 4 that the performance of our baseline approach does worsen as we ask it to generate more rules. However, as shown in Table 5, we also observe that the total number of tokens processed (input/output) is drastically reduced when we have the LLM produce two rules instead of one. Further, the total tokens processed is reduced by more than 50% when we ask the model to generate the full mapping within one prompt. The non-linear nature of this reduction implies that we can be more efficient with our techniques while also maintaining good performance–basically, the problem is more nuanced than simply reducing the number of prompts. One reason for this non-linear relationship is that there is a constant overhead that must be paidwith each prompt; namely, the static text which sets up our specific task for the LLM. The other major reason has to do with high rule overlap, where underlying rules share many source and target relations. Chunking two high-overlap rules together in one prompt has the affect of completing more work while adding relatively little to the input (i.e., the very few relations that do not appear in both rules). In other words, some rules can be chunked together without adding significant complexity (i.e., number of tokens) to either the input or the output rule. A simple technique, then, would be to chunk as many rules together under different prompts such that their overlap is high. REFERENCES  Serge Abiteboul, Richard Hull, and Victor Vianu. 1995. Foundations of databases. Vol. 8. Addison-Wesley Reading. Menatalla Abououf, Shakti Singh, Hadi Otrok, Rabeb Mizouni, and Anis Ouali. 2019. Gale-Shapley Matching Game Selection—A Framework for User Satisfac- tion. IEEE Access 7 (2019), 3694–3703. doi:10.1109/ACCESS.2018.2888696 Ted T. Ashburn and Karl B. Thor. 2004. Drug repositioning: identifying and developing new uses for existing drugs. Nature Reviews Drug Discovery 3, 8 (2004), 673–683. David Aumueller, Hong-Hai Do, Sabine Massmann, and Erhard Rahm. 2005. Schema and ontology matching with COMA++. In Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data (Baltimore, Mary- land) (SIGMOD ’05). Association for Computing Machinery, New York, NY, USA, 906–908. doi:10.1145/1066157.1066283  Angela Bonifati, Giansalvatore Mecca, Alessandro Pappalardo, Salvatore Raunich, and Gianvito Summa. 2008. Schema mapping verification: the spicy way. In Proceedings of the 11th international conference on Extending database technology: Advances in database technology. 85–96. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal Self-Consistency for Large Language Model Generation. arXiv:2311.17311 [cs.CL] https://arxiv.org/abs/2311.17311 Hong-Hai Do and Erhard Rahm. 2002. Chapter 53 - COMA — A system for flexible combination of schema matching approaches. In VLDB ’02: Proceedings of the 28th International Conference on Very Large Databases, Philip A. Bernstein, Yannis E. Ioannidis, Raghu Ramakrishnan, and Dimitris Papadias (Eds.). Morgan Kaufmann, San Francisco, 610–621. doi:10.1016/B978-155860869-6/50060-3  AnHai Doan, Alon Halevy, and Zachary Ives. 2012. Principles of Data Integration (1st ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. Ronald Fagin, Phokion G Kolaitis, Renée J Miller, and Lucian Popa. 2005. Data exchange: semantics and query answering. Theoretical Computer Science 336, 1 (2005), 89–124. National Science Foundation and National Institutes of Health. 2021. Smart Health and Biomedical Research in the Era of Artificial Intelligence and Advanced Data Science (SCH). https://www.nsf.gov/pubs/2021/nsf21530/nsf21530.htm  Behzad Golshan, Alon Y. Halevy, George A. Mihaila, and Wang-Chiew Tan. 2017. Data Integration: After the Teenage Years. In PODS. Zezhou Huang, Jia Guo, and Eugene Wu. 2024. Transform Table to Database Using Large Language Models. Proceedings of the VLDB Endowment. ISSN 2150 (2024), 8097. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 [cs.CL] https://arxiv.org/abs/2205.11916  Christos Koutras, George Siachamis, Andra Ionescu, Kyriakos Psarakis, Jerry Brons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, and Asterios Katsi- fodimos. 2021. Valentine: Evaluating Matching Techniques for Dataset Discovery. In2021 IEEE 37th International Conference on Data Engineering (ICDE). 468–479. doi:10.1109/ICDE51399.2021.00047  Sebastian Kruse, Paolo Papotti, and Felix Naumann. 2015. Estimating Data Integration and Cleaning Effort.. In EDBT. 61–72. Maurizio Lenzerini. 2002. Data integration: A theoretical perspective. In Proceed- ings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 233–246. Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang, and Yuyu Luo. 2024. A Survey of NL2SQL with Large Language Models: Where are we, and where are we going? arXiv preprint arXiv:2408.05109 (2024). Xuanqing Liu, Runhui Wang, Yang Song, and Luyang Kong. 2024. GRAM: Gen- erative Retrieval Augmented Matching of Data Schemas in the Context of Data Security. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Dis- covery and Data Mining (Barcelona, Spain) (KDD ’24). Association for Computing Towards Scalable Schema Mapping using Large Language Models Machinery, New York, NY, USA, 5476–5486. doi:10.1145/3637528.3671602  Yurong Liu, Aecio Santos, Eduardo HM Pena, Roque Lopez, Eden Wu, and Juliana Freire. [n. d.]. Enhancing Biomedical Schema Matching with LLM-based Training Data Generation. In NeurIPS 2024 Third Table Representation Learning Workshop. Sabine Massmann, Salvatore Raunich, David Aumüller, Patrick Arnold, Erhard Rahm, et al.2011. Evolution of the COMA match system. Ontology Matching 49 (2011), 49–60. Renée J. Miller, Daniel Fisla, Mary Huang, David Kymlicka, Fei Ku, and Vi- vian Lee. 2001. The Amalgam Schema and Data Integration Test Suite. (2001). www.cs.toronto.edu/ miller/amalgam. Marcel Parciak, Brecht Vandevoort, Frank Neven, Liesbet M. Peeters, and Stijn Vansummeren. 2024. Schema Matching with Large Language Models: an Experi- mental Study. arXiv:2407.11852 [cs.DB] https://arxiv.org/abs/2407.11852  Lucian Popa, Yannis Velegrakis, Renee J Miller, Mauricio A Hernandez, and Ronald Fagin. 2002. Translating web data. In VLDB’02: Proceedings of the 28th International Conference on Very Large Databases. Elsevier, 598–609. Nabeel Seedat and Mihaela van der Schaar. 2024. Matchmaker: Self-Improving Compositional LLM Programs for Table Schema Matching. In NeurIPS 2024 Third Table Representation Learning Workshop. https://openreview.net/forum?id= KCklcYUlLb  Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2024. ReMatch: Retrieval Enhanced Schema Matching with LLMs. arXiv:2403.01567 [cs.DB] https://arxiv.org/abs/2403.01567  Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, and Chris Parnin. 2023. Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs. arXiv:2310.10358 [cs.CL] https://arxiv.org/abs/ 2310.10358  Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. In Proceedings of the 17th ACM Interna- tional Conference on Web Search and Data Mining (Merida, Mexico) (WSDM’24). Association for Computing Machinery, New York, NY, USA, 645–654. doi:10.1145/3616855.3635752  Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs.CL] https://arxiv.org/abs/2203.11171  Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv:2302.11382 [cs.SE] https://arxiv.org/abs/2302.11382  E. C. Wood, Amy K. Glen, Lindsey G. Kvarfordt, Finn Womack, Liliana Acevedo, Timothy S. Yoon, Chunyu Ma, Veronica Flores, Meghamala Sinha, Yodsawalai Chodpathumwan, Arash Termehchy, Jared C. Roach, Luis Mendoza, Andrew S. Hoffman, Eric W. Deutsch, David Koslicki, and Stephen A. Ramsey. 2021. RTX- KG2: a system for building a semantically standardized knowledge graph for translational biomedicine. bioRxiv (2021). https://www.biorxiv.org/content/ early/2021/11/01/2021.10.17.464747  Yongqin Xu, Huan Li, Ke Chen, and Lidan Shou. 2024. KcMF: A Knowledge- compliant Framework for Schema and Entity Matching with Fine-tuning-free LLMs. arXiv:2410.12480 [cs.CL] https://arxiv.org/abs/2410.12480  Yifan Zeng, Ojas Tendolkar, Raymond Baartmans, Qingyun Wu, Lizhong Chen, and Huazheng Wang. 2024. LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking. arXiv:2406.00231 [cs.IR] https://arxiv.org/abs/2406.00231  Haochen Zhang, Yuyang Dong, Chuan Xiao, and Masafumi Oyamada. 2024. Large Language Models as Data Preprocessors. arXiv:2308.16361 [cs.AI] https: //arxiv.org/abs/2308.16361  Jing Zhang, Bonggun Shin, Jinho D Choi, and Joyce C Ho. 2021. SMAT: An attention-based deep learning solution to the automation of schema matching. InAdvances in Databases and Information Systems: 25th European Conference, ADBIS 2021, Tartu, Estonia, August 24–26, 2021, Proceedings 25. Springer, 260–274.",
  "text_length": 64659
}