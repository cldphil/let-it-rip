{
  "id": "http://arxiv.org/abs/2506.04141v1",
  "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
  "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
  "authors": [
    "Kejian Zhu",
    "Zhuoran Jin",
    "Hongbang Yuan",
    "Jiachun Li",
    "Shangqing Tu",
    "Pengfei Cao",
    "Yubo Chen",
    "Kang Liu",
    "Jun Zhao"
  ],
  "published": "2025-06-04T16:33:41Z",
  "updated": "2025-06-04T16:33:41Z",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04141v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04141v1  [cs.CV]  4 Jun 2025\nMMR-V: What’s Left Unsaid? A Benchmark for\nMultimodal Deep Reasoning in Videos\nKejian Zhu1,2, Zhuoran Jin1,2, Hongbang Yuan1,2, Jiachun Li1,2, Shangqing Tu3\nPengfei Cao1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2\n1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems,\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences3Tsinghua University\nzhukejian2025@ia.ac.cn {zhuoran.jin, hongbang.yuan} @nlpr.ia.ac.cn\n{pengfei.cao, yubo.chen, kliu, jzhao} @nlpr.ia.ac.cn\nAbstract\nThe sequential structure of videos poses a challenge to the ability of multimodal\nlarge language models (MLLMs) to locate multi-frame evidence and conduct\nmultimodal reasoning. However, existing video benchmarks mainly focus on un-\nderstanding tasks, which only require models to match frames mentioned in the\nquestion (hereafter referred to as “question frame”) and perceive a few adjacent\nframes. To address this gap, we propose MMR-V: A Benchmark for Multimodal\nDeep Reasoning in Videos . The benchmark is characterized by the following\nfeatures. (1) Long-range, multi-frame reasoning : Models are required to infer\nand analyze evidence frames that may be far from the question frame. (2) Beyond\nperception : Questions cannot be answered through direct perception alone but\nrequire reasoning over hidden information. (3) Reliability : All tasks are manu-\nally annotated, referencing extensive real-world user understanding to align with\ncommon perceptions. (4) Confusability : Carefully designed distractor annotation\nstrategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257\ntasks. Our experiments reveal that current models still struggle with multi-modal\nreasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy.\nAdditionally, current reasoning enhancement strategies (Chain-of-Thought and\nscaling test-time compute) bring limited gains. Further analysis indicates that\nthe CoT demanded for multi-modal reasoning differs from it in textual reasoning,\nwhich partly explains the limited performance gains. We hope that MMR-V can\ninspire further research into enhancing multi-modal reasoning capabilities.\nProject https://mmr-v.github.io/\n1 Introduction\nRecent models like OpenAI’s o1 [ 1] and Deepseek-R1 [ 2] have significantly improved text reasoning\nability through reinforcement learning. This has sparked growing interest in multimodal reasoning [ 3].\nModels like o3 and o4-mini [ 4] have achieved impressive results on image reasoning tasks through\ntool use, integrating visual information into the reasoning process to enable deep reflection and\nevidence mining. However, most of these studies focus on images, with limited exploration of\nmore challenging video reasoning tasks. Video naturally involves sequential and richer multimodal\ninformation, requiring models to perform reasoning and mine evidence over long-range, multi-frame.\nSince this capability is essential for real-world applications such as embodied intelligence and\nintelligent security monitoring [ 5;6], it naturally raises an important question: can current MLLMs\nperform deep multimodal reasoning and mine evidence on complex videos like o3 on image tasks?\nPreprint. Under review.\n--- Page 2 ---\nFigure 1: Examples showing the MMR-V tasks and the difference from previous video benchmarks.\nHowever, existing video benchmarks primarily focus on perception and understanding tasks [ 7;8].\nThese tasks often only require locating frames mentioned in the question and understanding adjacent\nframes. For example, at the bottom of Figure 1, noticing the boy being hit by the metal frame\nis enough to understand why he ran into the girl. Such tasks fall short in evaluating multimodal\nreasoning abilities. We summarize their limitations as follows: (1) Limited frame context : Even for\nlong videos, existing tasks often rely on just a few adjacent frames, failing to exploit the long-range\nsequential structure of the video. (2) Lack of reasoning : Many questions can be answered through\ndirect perception. (3) Unrealistic task : Simple perception and adjacent-frame understanding tasks\ndo not meet the real-world demands for AI system strong capabilities.\nTo address these shortcomings, we propose MMR-V Bench: A Benchmark for Multi-modal Deep\nReasoning in Videos . We present two examples to illustrate the key differences with previous video\nunderstanding benchmarks in Figure 1. MMR-V offers the following features: (1) Long-range,\nmulti-frame reasoning: tasks involve multimodal reasoning over non-adjacent video frames to\nlocate and analyze multiple evidences; (2) Beyond perception: questions cannot be answered by\ndirect perception of question frame directly, requiring reasoning and the extraction of implications;\n(3) Reliability: All tasks are annotated manually, and potential subjective bias is reduced by cross-\nreferencing the most popular video comments. (4) Confusability: We employ carefully designed\nannotation strategies to craft model-aligned distractor options, thereby ensuring confusability.\n2\n--- Page 3 ---\nInspired by cognitive and psychological theories [ 9;10;11], such as Kahneman’s Dual Process\nTheory [ 12], we categorize the tasks in MMR-V into implicit reasoning andexplicit reasoning . The\nkey distinction lies in whether the question requires reasoning beyond surface-level information to\ninfer underlying implications . Explicit reasoning is defined as questions that can be solved using\nperceivable information from the video. For example, the task shown in Figure 1 requires noticing the\ntwo lighters hidden in the hand. Implicit reasoning requires extracting and interpreting the underlying\nsubtext behind visual information. For example, in the implicit reasoning case shown in Figure 1, it\nrequires inferring the underlying implication that the girl’s room number 7 symbolizes good luck.\nThis is more of an assessment of EQ, testing whether the model can use its deep understanding of the\nworld knowledge to make implicit and subconscious reasoning paths like humans.\nMMR-V comprises 317 videos and 1257 tasks . The videos span six major categories, with lengths\nranging from 7 to 3771 seconds, with an average of 277 seconds. Tasks are further divided into 10\ncategories and subcategories. Each task is in multiple-choice format with approximately ten options\non average. Tasks typically require reasoning over average 12 video frames, covering about 60% of\nvideo duration. All questions and correct answers are human-annotated and reviewed. Distractors are\ngenerated using a carefully designed annotation strategy (Details in Section 3.2).\nWe evaluated 9 proprietary models and 11 open-source models on MMR-V . The results reveal that\neven the best-performing model, o4-mini, achieved only 52.5% accuracy , highlighting the significant\nchallenge MMR-V poses to current multimodal large language models. Our key findings are as\nfollows. (1) Multimodal reasoning challenge: Our findings in Section 4.2 show that reasoning\nenhancement strategies (e.g., CoT and scaling test-time compute) yield limited improvements,\nindicating that MMR-V presents a greater challenge to current multimodal reasoning models. Further\nerror analysis in Section 4.5 shows that the CoT demanded in multimodal reasoning differs from those\nin textual reasoning. Current models tend to rely on textual reasoning based on visual information\nfrom the question frame and few adjacent frames, lacking the multimodal reasoning needed to\nlocate and analyze evidence from long-range frames. This limitation hinders the overall reasoning\nperformance. (2) More modality will benefit: We found that for models that support all modalities,\nadding additional audio modalities will improve the performance (Accuracy improved by 1.4%,\n1.0%, and 1.0% for Gemini 2.0-Flash, Gemini 2.0-Flash-Thinking, and Phi-4-Multimodal-Instruct,\nrespectively). (3) Human-model gap: In human experiments, we found that although models exhibit\nhuman-level performance on text reasoning tasks, there is still a significant gap between model and\nhuman on multimodal, especially video, reasoning tasks. We hope MMR-V will inspire further\nresearch into enhancing multimodal reasoning capabilities in AI systems.\n2 Task Overview\nThe tasks in MMR-V require deeper multimodal reasoning. Unlike previous tasks such as math\nand puzzle problems [ 13;14;15], we argue that the scope of multimodal reasoning should be more\nbroadly defined. Previous work focuses more on text-oriented reasoning based on perceived visual\ninformation. In contrast, our task requires integrating the various forms of visual evidences, such\nas artistic style, lighting, and depth, into the reasoning process. Even more challenging, it involves\nreasoning over long-range, multi-frame visual evidence . Videos have a temporal dimension, which\nputs a greater challenge on the ability to find clues in different frames through multimodal reasoning.\n2.1 Definition for Implicit and Explicit Reasoning.\nWe categorize reasoning tasks in MMR-V into Implicit Reasoning andExplicit Reasoning , inspired\nby Kahneman’s Dual Process Theory [ 12] and other cognitive theories [ 9;10;11]. The most obvious\ndifference is whether or not one needs to understand the subtext beneath the surface information.\nSecondly, implicit reasoning for human is often achieved by experience based on world knowledge,\nthus consuming little attention resources. Tasks are further divided into 10 categories and 33\nsubcategories. Six categories are shown in Figure 2, with the first row belonging to implicit and the\nsecond row is explicit. Further explanations and examples can be found in Appendix D.\nImplicit Reasoning focuses on incorporating hidden meanings behind visual information into\nreasoning. In these tasks, surface-level visual cues often conceal deeper layers of meaning, such as\nmetaphor. Besides, for human, “(implicit) operates automatically and quickly, with little or no effort\nand no sense of voluntary control. ” - Dual Process Theory.\n3\n--- Page 4 ---\nVideo Type: AnimationⅠ. Metaphor Understanding\nQuestion: What does the brown \ncoat in the video symbolize?\nOptions: \n(A) It is said to represent the family's long - lost fortune that \nthey are still searching for.\n......\n(D) It symbolizes the father in a family, who protects his \nfamily in times of difficulty.\n(H) The bad luck that has been following the family for \ngenerations.\nVideo Type: AnimationⅡ. Theme Understanding\nQuestion: What social issues does \nthis video imply?\nOptions: \n(A) It implies that everyone should invest all their money in \nthe stock market to get rich quickly.\n(B) People face great pressure in buying homes, as housing \nprices increase while they are saving money. ......\n(G) It shows that the social issue is that renting a house is \nalways a waste of money, and buying a house is the only way \nto have a stable life.\n(H) Color of the house can determine a person's social status.\nVideo Type: FilmⅢ. Emotion Recognition\nQuestion: The man eventually lost \nhis job. Was he happy in the end?\nOptions: \n(A) He was relieved to leave a stressful environment at work.\n(B) He was not happy, as losing the job led to a significant \ndownturn in his life.\n(C) He was happier because he pursued a passion in art.\n(D) He felt neutral since he anticipated the job loss.\n(E) He was happy and felt an unprecedented sense of relief.\n......\n(K) He was happy as he finally took a long vacation.\nVideo Type: FilmⅣ. Causal Reasoning\nQuestion: What might the girl be \nwriting a greeting card for?\nOptions: \n(A) Valentine's Day celebration.\n(B) Milo's birthday.\n(C) A graduation celebration.\n(D) Apology for a mistake.\n......\n(I) A shared achievement celebration.\n(J) To visit her seriously ill boyfriend and wish him well.\n(K) A farewell for a move to a new city.\nVideo Type: LifeⅤ. Sequential Structure Reasoning\nQuestion: Is the video played in \nreverse? Why?\nOptions: \n(A) The entire video is not in reverse; the card tower \ncollapsing is played forward.\n......\n(I) It is not played in reverse; you can tell from the man \nspraying away the letter 'P' from the wall.\n(J) It is played in reverse; The man's actions align with \nforward motion when the fruit basket is filled.\nVideo Type: LifeⅥ. Counterintuitive Reasoning\nQuestion: How does the man \nmake the pen disappear?\nOptions: \n(A) He palms the pen and makes it as though it has vanished.\n(B) Camera cut to give the illusion of the pen disappearing.\n......\n(H) He drops the pen into his lap while gesturing forward \nwith his hands.\n(I) He slipped the pen into the black pocket in front of him.\n(J) The pen is hollow and collapses into his hand.\n(K) He uses a false thumb to secretly store the pen.\nExplicit ReasoningImplicit ReasoningFigure 2: Overview of six tasks in MMR-V Bench.\nExplicit Reasoning evaluates whether a model can perform reasoning based on multimodal details\nexplicitly presented across long-range, multi-frame of a video. However, solving these tasks demands\nfine-grained perception and rigorous logical reasoning. “(explicit) allocates attention to the effortful\nmental activities that demand it, including complex computations. ” - Dual Process Theory.\n2.2 Implicit Reasoning Tasks\nMetaphor Understanding (MU) : MU tasks evaluate the ability to reason about metaphors for\nentities or environment. For example, the case in Figure 2 I interprets the metaphor of the brown coat.\nTheme Understanding (TU) : TU assesses the ability to infer the main idea and attitude of the author\nthrough the full video. For example, the case in Figure 2 II asks what social issue the video reveals.\nEmotion Recognition (ER) : ER tasks evaluate the ability to analyze character emotional states, as\nwell as higher-level emotions such as the author’s attitude and the audience’s emotional response. For\nexample, the case in Figure 2 III involves inferring whether the character feels happy at the end.\nComment Matching (CM) : CM task is to predict the most fitting audience comments for a video\nbased on a criteria. For example, selecting which comment would be the most humorous after\nwatching the video. Detailed example can be found in Appendix D.1.\nImplicit Symbol (IS) : IS task is to interpret implicit symbols in the video, such as cultural elements.\nFor example, inferring the ethnicity of the filming location. Details can be found in Appendix D.1.\n2.3 Explicit Reasoning Tasks\nCausal Reasoning (CAR) : CAR assesses the ability to reason about causal relationships in the video.\nFor example, in Figure 2 IV , it involves inferring the reason why the girl is making a card.\nSequential Structure Reasoning (SSR) : SSR tasks assess reasoning about temporal structure in\nvideo editing and storytelling. In the example from Figure 2 V , the task is to infer if the video is\nreversed. However, the creator of this video explains the video is played normally.\nCounterintuitive Reasoning (CIR) : CIR tasks evaluate the ability to analyze information that\ncontradicts common sense, requiring detailed cross-frame analysis. In the example from Figure 2 VI,\nthe task is to reason the principle behind the counterintuitive magic trick.\n4\n--- Page 5 ---\nCross-modal Transfer Reasoning (CTR) : To reason and match information out of the video that\nshares similar meaning. For example, find the quote with same theme of the video.\nVideo Type and Intent (VTI) : VTI tasks test the ability to infer key meta-level information such as\nthe genre and communicative intent of the video from a global perspective. For example, the case in\nAppendix D.2 infers the release time by reasoning the video is set during COVID-19.\n3 MMR-V Bench\nTo ensure that MMR-V effectively evaluate multimodal reasoning abilities, we follow three principles\nduring construction: P1. Multi-frame: Questions require reference to long-range, multi-frame\ninformation, prompting the model to reason across multiple visual cues. P2. Deep reasoning:\nAnswers should not be directly perceivable from the video; instead, they should demand understanding\nof the subtext or multimodal reasoning, reflecting a deep comprehension of the content. P3. Realistic:\nTasks should align with real-world question-answering needs, ensuring answers are consistent with\ncommon user understanding and free from individual cognitive biases or prejudices.\n3.1 Video Collection\nWe manually curated a diverse original videos from Youtube with following checklist: (1) Avoidance\nof linear, descriptive content : We excluded videos with straightforward structures, such as daily\nrecordings or sports broadcasts, in order to ensure that the tasks require deep reasoning over multi-\nframes (For Principle P1). (2) Creative and thematically rich videos : We selected videos that are\nintentional designed and edited by creators, often conveying well-crafted themes. This ensures that the\nquestions require interpretation beyond surface-level visual content (For Principle P2). (3) Alignment\nwith real-world : Highly Popular Videos were preferred, which are indicated by active comment\nsections and audience engagement. This helps avoid biases introduced by niche content and ensures\nalignment with general user cognition (For Principle P3). (4) Diverse coverage : To further promote\ngeneralizability, we ensured broad coverage across video types, topics, and durations, allowing\nMMR-V to reflect the diversity of real-world video content (For Principle P3). As a result, our final\nbenchmark comprises 317 videos spanning six major categories : Animation, Film, Philosophy, TV ,\nLife, and Art. The specific categories are shown in the Appendix C. Furthermore, for problems where\naudio might be helpful, we ensure that the videos include audio.\n3.2 Data Annotation & Quality Assurance\nAll tasks in MMR-V Bench are designed in a multiple-choice format. There is one correct option and\nseveral wrong options. Make sure there are carefully crafted distractors among the wrong options.\nTo ensure the quality and plausibility of these distractors, we designed three distinct distractors\nannotation strategies. (1) Str. 1: We prompt a strong model GPT-4o [ 16] to directly answer the\nmanually annotated question. If the model generate an incorrect answer (as verified by human\nannotators), that answer is retained as a high-quality distractor. If correct, we combine human-written\ndistractors with incorrect options generated by GPT-4o as distractors. (2) Str. 2: Given the question\nand correct answer annotated manually, GPT-4o is prompted to generate distractors. (3) Human\nannotators construct distractors manually.\nModels Str. 1 Str. 2 Str. 3\nGPT-4o 59% 70% 62%\nQwen-VL-7B 37% 51% 42%\nTable 1: Performance on 100 questions anno-\ntated with different strategies (str.).We conducted a test using 100 questions, using\nthree strategies to form three test-set with 100\nmultiple-choice tasks. As shown in Table 1, dis-\ntractors generated by strategy 1 are more confus-\ning, significantly increasing the difficulty and qual-\nity of our tasks. It is worth noting that in the above\ntest process, when GPT-4o directly answered 100\ntasks, the accuracy rate verified by humans was\nonly 17%. This reflects the limitations of the current model in multimodal reasoning capabilities.\nTo ensure high quality, we also developed an checklist based on the construction principles and\ninvited human annotators to verify the accuracy and difficulty of the tasks using this checklist. We\ninvited five annotators with at least a bachelor’s degree to participate in the annotation and review\n5\n--- Page 6 ---\nprocess. The checklist of MMR-V is shown in the Appendix B. The overall annotation process and\nthe annotation platform can be found in Figure 7 and Figure 8 in the Appendix B.\n3.3 Data StatisticsTable 2: Dataset Statistic of MMR-V .\nDataset Statistic\nTask\nQuestion Count 1257\nAverage Option Count 10\nAverage Question Words 14\nAverage Option Words 10\nVideo\nVideo Count 317\nMinimum Length (s) 7\nMaximum Length (s) 3771\nAverage Length (s) 277MMR-V comprises a total of 317 videos spanning a wide\nrange of content types, and includes 1,257 multiple-choice\nreasoning tasks. Each question is annotated with 7 to 11\ncandidate answers, with only one correct answer guaran-\nteed. As illustrated in Figure 9a, the videos are categorized\ninto six major domains, each encompassing fine-grained\nsubcategories to ensure diversity in content, style, and\nsemantics. The reasoning tasks in our benchmark are orga-\nnized across three levels of granularity, reflecting different\ndimensions of reasoning complexity and modality. The\ndistribution of task types across these levels is shown in\nFigure 9b. More information is shown in Table 2.\n4 Experiments\n4.1 Settings\nWe conducted extensive evaluations on 9 proprietary and 11 open-source models as detailed in the\nAppendix E.1. Our main experiments were conducted under two settings: zero-shot and zero-shot +\nCoT [ 17], in order to examine whether reasoning enhances performance. For further analysis, we\nintroduced the following categories of comparative models: (1) Models with different scales. (2)\n“Thinking” model and its base version. (e.g., Gemini-2.0-Flash and Gemini-2.0-Flash-Thinking).\nMultimodal Inputs : For models supporting full-modal inputs (e.g., Gemini-2.0-flash), we further\ncompare their performance with and without audio input to evaluate its influence on reasoning results.\nFrame Selection : Since some models only support multiple images or short video clips, we standard-\nized the number of input frames. Details of frame sampling are provided in Appendix E.\nHuman Experiment: To provide a meaningful upper bound for MMR-V and to examine the human-\nmodel gap, we invited participants with at least bachelor degree to conduct human experiment. We\nsampled 100 tasks GPT-4o answered incorrectly and 100 tasks it answered correctly for experiment.\n4.2 Main Results\nWe report the evaluation results in Table 3. Results indicate that the MMR-V Bench poses a significant\nchallenge to current multimodal large models. Even the best-performing model, o4-mini, achieves\nonly 52.5% accuracy. Among open-source models, Gemma-3-27b-it performs the best, demonstrating\nrelatively strong performance. However, there remains a gap compared to proprietary models.\nCurrent reasoning enhancements have limitations on MMR-V . Results in Table 3 show that\ncurrent reasoning enhancement strategies, which are relatively effective in textual domains, such as\nCoT prompt reasoning and scaling test-time compute (i.e., \"Thinking\" models), offer only limited\ngains on MMR-V . CoT brings only a 0.57% average gain, and \"Thinking\" model improves just\n2.4%. This indicates that MMR-V presents a significant challenge to the multimodal reasoning\ncapabilities of existing models. Analysis of sampled model responses shows that visual analysis\naccounts for only about 10% of the CoTs. This reveals that reasoning process of current model is\nmostly text-based (reasoning on questions and options), relying on visual perception of question\nframe, instead of integrating visual reasoning and evidence mining into CoTs. Several examples are\nprovided in Appendix H, and further analysis in Section 4.5 supports similar findings.\nModel performance on MMR-V Bench exhibits a clear scaling law effect. Smaller models under\nthe same architecture perform poorly on tasks that require complex reasoning. For instance, larger\nmodels like Qwen2.5-VL-72B (39.1%) and GPT-4o (44%) outperform their smaller versions Qwen2.5-\nVL-7B (30.1%) and GPT-4o-mini (34.8%), showing relative gains of 9% and 9.2%, respectively.\nModel performance across different tasks on MMR-V Bench.\n6\n--- Page 7 ---\nTasks Video Categories\nModel Overall Implicit Explicit Art Life TV Film Ani. Phi.\nOpen-source models\nLLaV A-Onevision 6.5 8.8 7.0 9.6 5.4 6.6 6.5 3.4 9.5 3.8 9.8 1.2\nLLaV A-Video 18.4 17.6 19.1 18.1 15.4 16.3 14.4 11.2 13.2 17.4 21.4 12.8\nNVILA-8B-Video 25.5 25.3 26.2 24.2 23.9 25.9 17.3 21.3 23.5 21.6 38.0 21.8\nPhi-4-multimodal-instruct 26.7 27.6 29.4 31.2 19.4 18.1 19.4 19.2 25.9 26.4 33.9 24.4\nCogvlm2-video-llama3 25.6 26.1 25.4 26.2 26.1 25.7 15.5 18.3 24.7 19.1 43.2 20.8\nQwen2.5-VL-7B 30.1 32.4 33.7 36.2 20.8 22.5 20.9 18.1 29.6 21.2 48.4 19.8\nIntern3-8B 33.6 32.9 35.5 33.4 28.6 31.4 23.0 22.6 31.7 24.3 52.9 23.2\nGemma-3-12b-it 34.0 34.2 37.8 37.6 24.0 25.4 19.4 24.9 25.9 31.3 51.9 24.4\nInternVL2.5-38B 39.9 39.7 43.8 43.7 29.9 29.4 30.4 28.8 30.4 37.2 57.4 29.1\nQwen2.5-VL-72B 39.1 40.4 41.3 42.8 33.4 34.3 28.9 28.2 29.1 36.5 55.6 37.2\nGemma-3-27b-it 42.0 41.1 46.5 44.7 30.3 32.0 31.7 32.2 35.5 41.3 56.1 33.7\nProprietary models\nGPT-4o-mini-2024-07-18 34.8 35.2 38.0 38.6 26.3 26.3 29.5 25.4 29.6 33.0 48.7 18.6\nGemini-2.0-Flash (16 frames) 42.6 44.3 44.3 45.9 38.3 40.0 30.9 32.2 40.7 40.6 58.5 24.4\nClaude-3.5-Sonnet-20241022 43.3 44.2 45.0 46.1 38.9 39.1 33.8 31.1 41.3 41.3 55.8 44.4\nGPT-4o-2024-11-20 44.0 46.1 46.6 46.9 37.6 44.0 38.1 37.3 34.9 41.0 61.6 32.6\nGemini-2.0-Flash-thinking 45.0 43.5 46.6 46.0 40.6 37.1 34.5 31.6 38.6 48.3 60.1 25.6\nGPT-4.1-2025-04-14 46.6 48.9 49.1 51.7 40.3 41.7 43.2 35.6 43.9 46.5 57.1 34.9\nGemini-2.0-Flash (512 frames) 48.0 49.9 50.5 52.6 41.6 42.9 36.7 36.7 39.7 46.2 66.7 31.4\nGemini-2.5-Flash 51.2 50.5 52.9 52.3 46.9 45.7 45.3 39.5 50.3 47.9 65.6 34.9\no4-mini-2025-04-16 52.5 52.1 54.6 54.5 47.1 46.0 48.2 40.1 54.0 51.7 65.3 27.9\nBaseline\nBest Performance of Models 52.5 54.6 47.1 48.2 40.1 54.0 51.7 65.6 44.4\nHuman 86.0 80.6 91.2 57.7 92.3 90.6 92.3 90.7 70.0\nTable 3: Evaluation results (%) on MMR-V . Results under CoT prompting are highlighted in gray.\nThe random accuracy on MMR-V Bench is approximately 10%. Bold andunderlined values indicate\nthe best performance among proprietary and open-source models, respectively.\n/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000018/uni00000026/uni00000030/uni0000002c/uni00000036 /uni00000026/uni00000024/uni00000035\n/uni00000036/uni00000036/uni00000035\n/uni00000026/uni0000002c/uni00000035\n/uni00000026/uni00000037/uni00000035\n/uni00000039/uni00000037/uni0000002c /uni00000030/uni00000038/uni00000037/uni00000038/uni00000028/uni00000035/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000052\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni00000016/uni00000010/uni00000018/uni00000010/uni00000036/uni00000052/uni00000051/uni00000051/uni00000048/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni0000004c/uni00000051/uni0000004c/uni00000010/uni00000015/uni00000011/uni00000018/uni00000010/uni00000029/uni0000004f/uni00000044/uni00000056/uni0000004b/uni0000002a/uni00000048/uni00000050/uni0000004c/uni00000051/uni0000004c/uni00000010/uni00000015/uni00000011/uni00000013/uni00000010/uni00000029/uni0000004f/uni00000044/uni00000056/uni0000004b/uni00000010/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000015/uni00000011/uni00000018/uni00000010/uni00000039/uni0000002f/uni00000010/uni0000001a/uni00000025\n/uni00000052/uni00000017/uni00000010/uni00000050/uni0000004c/uni00000051/uni0000004c\nFigure 3: Performance on different tasks.Firstly, the models performed better on implicit\ntasks than on explicit tasks (with an average gain\nof +7.9%). Through analysis of tasks and model\nresponses, we found that in implicit tasks, video cre-\nators often embed implicit meanings throughout the\nentire video, resulting in abundant visual cues that\ncan support reasoning. This reduces the requirements\nfor multi-modal reasoning and clue localization. In\ncontrast, explicit tasks demand finer-grained reason-\ning and the ability to identify specific evidence. For\nexample, in the implicit task at the bottom of Figure\n1, many frames provide clues suggesting that the girl\nsymbolizes good luck (e.g., room number, flowers,\nlighting, weather, etc.). In contrast, the explicit task\nat the top contains only a few key frames where the\nhidden lighter in magician’s hand can be seen.\nSecondly, the models performed particularly\npoorly on Counterintuitive Reasoning (CIR) ,Se-\nquential Structure Reasoning (SSR) , and Comment\nMatching (CM) tasks . For CIR and SSR tasks, poor performance mainly stems from the limited\nability of current models to perform multi-frame reasoning. These two tasks require the model\nto reason on long-range videos, rather than relying on internal knowledge. However, instead of\nanalyzing to locate evidences in other frames, models often rely on surface-level visual perception\nof the question frame, followed by textual reasoning over question and options. For CM tasks,\nthe results highlight a significant gap between model and human capabilities in implicit reasoning.\nWhile humans can infer underlying information such as humor and emotion with minimal cognitive\neffort [18], current models consistently fail to capture such subtleties.\n7\n--- Page 8 ---\nTasks Categories\nOverall Imp. Exp. Art Life TV Film Ani. Phi.\nGemini-2.0 42.6 44.3 38.3 30.9 32.2 40.7 40.6 58.5 24.4\n+audio 44.0↑1.446.2↑1.938.3−0.031.0↑0.131.6↓0.642.3↑1.641.0↑0.461.1↑2.629.1↑4.7\nGemini-2.0-thinking 45.0 46.6 40.6 34.5 31.6 38.6 48.3 60.1 25.6\n+audio 46.0↑1.048.4↑1.839.7↓0.931.7↓2.833.9↑2.344.4↑5.842.7↓5.662.4↑2.332.6↑7.0\nPhi-4-multimodal-instruct 26.7 29.4 19.4 19.4 19.2 25.9 26.4 33.9 24.4\n+audio 27.7↑1.031.3↑1.918.1↓1.315.4↓3.019.7↑0.524.5↓1.427.8↑1.437.3↑3.426.7↑2.3\nTable 4: The impact of adding audio modality on the performance (accuracy %) on different tasks.\nHuman Performance. Humans achieved an average score of 86%, which highlights a significant\nhuman-model gap. Although studies suggest that models achieved human-level performance on text\ntasks [ 2;19], models still lag behind on multimodal reasoning tasks. Humans can identify clues in\nvideos easily, while models tend to focus on question frames rather than exploring other evidence\nframes. Specially, unlike models, humans perform slightly worse on implicit tasks, which is mainly\ndue to the challenges posed by highly abstract implicit understanding in art and philosophy.\n4.3 Influence of Frames Count\n/uni00000014/uni00000019/uni00000016/uni00000015 /uni00000019/uni00000017 /uni0000001c/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000015/uni00000018/uni00000019 /uni00000018/uni00000014/uni00000015\n/uni00000029/uni00000055/uni00000044/uni00000050/uni00000048/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000016/uni0000001b/uni00000017/uni00000013/uni00000017/uni00000015/uni00000017/uni00000017/uni00000017/uni00000019/uni00000017/uni0000001b/uni00000018/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000029/uni00000055/uni00000044/uni00000050/uni00000048/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n/uni00000032/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004f\n/uni0000002c/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000028/uni0000005b/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\nFigure 4: Accuracy with the increase of\ninput frame counts.For Gemini-2.0-Flash, which supports long video in-\nputs, we evaluated performance changes as the number\nof frames increases. As shown in Figure 4, accuracy\nimproves with more frames, but the rate of improve-\nment gradually slows. After sampling and observing\nthe CoTs, it is found that the initial gains come from\nthe addition of evidence frames, while the slowdown is\nmainly due to limited multi-frame reasoning ability of\nthe model. Performance on implicit tasks continues to\nimprove in later stages, as visual cues for such tasks are\noften dispersed throughout the video (as discussed in\nSection 4.2); more frames tend to provide more clues.\nIn contrast, explicit clues are fewer and more localized.\n4.4 Influence of Audio Input\nFor models that support full-modal input, we compared their performance before and after incorpo-\nrating the audio modality. As shown in Table 4, overall performance improved with the addition of\naudio. Specifically, Gemini 2.0-Flash, Gemini 2.0-Flash-Thinking, and Phi-4-multimodal-instruct\nshowed improvements of 1.4%, 1.0%, and 1.0%, respectively. This suggests that advancing research\non fully multimodal models is a promising direction.\n4.5 Error Analysis\n47%\n26%13%13%2%\n6%\nLack of Visual Reasoning Implicit Misinterpretation\nKnowledge Insufficiency Reasoning Error\nOutput Formatting Issue Hallucination\nFigure 5: Error analysis of GPT-4o.We sampled 100 incorrect responses from GPT-4o for\nerror analysis. The main sources of errors can be cate-\ngorized as follows: (1) Lack of Visual Reasoning: the\nmodel often failed to locate the correct evidence frames\nand lack of long-range, multi-frame visual reasoning. (2)\nImplicit Misinterpretation: revealing a significant under-\nstanding gap between the model and human cognition. (3)\nKnowledge Insufficiency: the model lacks some intrinsic\nknowledge (4) Reasoning Error: during the multi-step de-\nduction process. (5) Hallucination: the model introduced\nfake or unsupported information. (6) Output Formatting\nIssue: model refusals or formatting errors prevent answer\nextraction. Among error cases, Lack of Visual Reasoning\naccounts for the largest proportion. This indicates that\n8\n--- Page 9 ---\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000032/uni00000058/uni00000057/uni00000053/uni00000058/uni00000057/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000033/uni0000004b/uni0000004c/uni00000010/uni00000017/uni00000010/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000050/uni00000052/uni00000047/uni00000044/uni0000004f/uni00000010/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057\n/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055/uni00000003/uni00000029/uni00000055/uni00000044/uni00000050/uni00000048\n/uni00000034/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni00000055/uni00000044/uni00000050/uni00000048\n/uni00000039/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000032/uni00000058/uni00000057/uni00000053/uni00000058/uni00000057/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000052\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000032/uni00000058/uni00000057/uni00000053/uni00000058/uni00000057/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000002a/uni00000048/uni00000050/uni0000004c/uni00000051/uni0000004c/uni00000010/uni00000015/uni00000011/uni00000018/uni00000010/uni00000053/uni00000055/uni00000048/uni00000059/uni0000004c/uni00000048/uni0000005a\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000032/uni00000058/uni00000057/uni00000053/uni00000058/uni00000057/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000052/uni00000017/uni00000010/uni00000050/uni0000004c/uni00000051/uni0000004c/uni00000010/uni00000015/uni00000013/uni00000015/uni00000018/uni00000010/uni00000013/uni00000017/uni00000010/uni00000014/uni00000019Figure 6: CoT content across different stages. The y-axis indicates the ratio of the 500 sampled CoTs\nthat include analysis of these four types of content at each stage.\ncurrent models still lack genuine multimodal reasoning capabilities. They tend to rely on text-based\nreasoning after briefly perceiving frames adjacent to the question, rather than engaging in deep,\nlong-range, multi-frame video reasoning. Most existing reasoning models remain inadequate in\nintegrating multimodal information into the reasoning process and performing thorough analysis. In\ncontrast, o4-mini exhibits a better reasoning paradigm, as shown in Figure 11 for comparison.\nWe further analyzed model CoTs by categorizing each step into video or text analysis (e.g. options),\nwith video analysis divided into question frame and other frame analysis (details in Appendix F). We\nsampled 500 CoTs from models, split each into 10 equal-length segments, and used GPT-4.1 to label\neach segment. As shown in Figure 6, where models further to the right perform better on MMR-V ,\nmodels with better performance on MMR-V show more video analysis, especially on other frames\n(red line). Notably, 4o-mini stands out with strong analysis of non-question frames, highlighting the\nvalue of enhanced visual reasoning and tool use in multi-frame video reasoning tasks.\n5 Related Work\nVideo Understanding Benchmark. Existing video benchmarks primarily focus on evaluating models’\nperception and intuitive understanding of visual elements in videos, such as action recognition [ 20;21;\n22;23] and video description [ 24;25]. Recent notable works, such as Video-MME [ 8], MVBench [ 26]\nand MMBench-Video [ 27], have extended video understanding to multiple task types and video\ntypes, enabling a more comprehensive assessment of video understanding capabilities. Additionally,\nbenchmarks like LVBench [ 28] and LongVideoBench [ 29] have introduced long-video question-\nanswering tasks. However, these tasks mainly evaluate whether a model can accurately extract\nrelevant information from long videos based on the given questions, while the subsequent steps\nremain largely perception-oriented. MMR-V is designed to assess whether a model can perform\nmulti-frame, long-span, multimodal autonomous reasoning on videos based on the given questions.\nMultimodal Reasoning. Recent advancements have greatly enhanced LLM reasoning [ 2;1;30;31].\nMany top LLMs perform well on complex reasoning tasks, but their evaluation focuses on text-\nbased reasoning [ 32;33;34;35;36;37]. MLLMs still lack thorough assessment in this area.\nCurrent multimodal reasoning benchmarks mainly involve mathematical or coding tasks in image\nform [ 14;38;39], which primarily test visual recognition followed by text reasoning. True multimodal\nreasoning requires integrating details like depth, texture, and audio for complex inference. MMR-V\nBench aims to evaluate multimodal sequential reasoning in video tasks.\n6 Conclusion\nThis paper introduces MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos . All\ntasks are annotated by human experts and designed to evaluate abilities of multimodal reasoning.\nMMR-V presents a significant challenge to current models, with the best model performance still\nlagging 33.5% accuracy behind human. This highlights a human-model gap in interpreting and\nreasoning about video information. Notably, o4-mini achieves the best results on MMR-V , suggesting\nthat integrating visual reasoning into CoT and leveraging tool use is a promising direction for tackling\nvideo reasoning tasks. We hope MMR-V will serve as a reliable evaluation benchmark for the\ndevelopment of MLLMs and offer valuable insights into advancing multimodal reasoning research.\n9\n--- Page 10 ---\nReferences\n[1]A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry,\nA. Beutel, A. Carney, et al. , “Openai o1 system card,” arXiv preprint arXiv:2412.16720 , 2024.\n[2]D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. ,\n“Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,” arXiv\npreprint arXiv:2501.12948 , 2025.\n[3]Y . Wang, S. Wu, Y . Zhang, S. Yan, Z. Liu, J. Luo, and H. Fei, “Multimodal chain-of-thought\nreasoning: A comprehensive survey,” arXiv preprint arXiv:2503.12605 , 2025.\n[4] OpenAI, “Openai: Introducing openai o3 and o4-mini,” 2025.\n[5]J. Hou, C. Wu, Z. Yuan, J. Tan, Q. Wang, and Y . Zhou, “Research of intelligent home secu-\nrity surveillance system based on zigbee,” in 2008 International Symposium on Intelligent\nInformation Technology Application Workshops , pp. 554–557, IEEE, 2008.\n[6]J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie, “Thinking in space:\nHow multimodal large language models see, remember, and recall spaces,” arXiv preprint\narXiv:2412.14171 , 2024.\n[7]J. Zhou, Y . Shu, B. Zhao, B. Wu, S. Xiao, X. Yang, Y . Xiong, B. Zhang, T. Huang, and Z. Liu,\n“Mlvu: A comprehensive benchmark for multi-task long video understanding,” arXiv preprint\narXiv:2406.04264 , 2024.\n[8]C. Fu, Y . Dai, Y . Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y . Shen, M. Zhang, et al. ,\n“Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video\nanalysis,” arXiv preprint arXiv:2405.21075 , 2024.\n[9]J. S. B. Evans, “Heuristic and analytic processes in reasoning,” British Journal of Psychology ,\nvol. 75, no. 4, pp. 451–468, 1984.\n[10] R. Sun, “The clarion cognitive architecture: Extending cognitive modeling to social simulation,”\nCognition and multi-agent interaction , pp. 79–99, 2006.\n[11] M. Polanyi, Personal knowledge . Routledge, 2012.\n[12] D. Kahneman, Thinking, fast and slow . macmillan, 2011.\n[13] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and\nJ. Gao, “Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts,”\narXiv preprint arXiv:2310.02255 , 2023.\n[14] K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li, “Measuring multimodal\nmathematical reasoning with math-vision dataset,” Advances in Neural Information Processing\nSystems , vol. 37, pp. 95095–95169, 2024.\n[15] F. Zhang, L. Wu, H. Bai, G. Lin, X. Li, X. Yu, Y . Wang, B. Chen, and J. Keung, “Humaneval-v:\nEvaluating visual understanding and reasoning abilities of large multimodal models through\ncoding tasks,” arXiv preprint arXiv:2410.12381 , 2024.\n[16] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda,\nA. Hayes, A. Radford, et al. , “Gpt-4o system card,” arXiv preprint arXiv:2410.21276 , 2024.\n[17] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou, et al. , “Chain-of-\nthought prompting elicits reasoning in large language models,” Advances in neural information\nprocessing systems , vol. 35, pp. 24824–24837, 2022.\n[18] K. Krishna, Y . Chang, J. Wieting, and M. Iyyer, “RankGen: Improving text generation with\nlarge ranking models,” in Proceedings of EMNLP , pp. 199–232, 2022.\n[19] OpenAI, “Gpt-4 technical report,” arXiv preprint arxiv:2303.08774 , 2023.\n10\n--- Page 11 ---\n[20] M. U. Khattak, M. F. Naeem, J. Hassan, M. Naseer, F. Tombari, F. S. Khan, and S. Khan,\n“How good is my video lmm? complex video reasoning and robustness evaluation suite for\nvideo-lmms,” arXiv preprint arXiv:2405.03690 , 2024.\n[21] K. Mangalam, R. Akshulakov, and J. Malik, “Egoschema: A diagnostic benchmark for very\nlong-form video language understanding,” Advances in Neural Information Processing Systems ,\nvol. 36, pp. 46212–46244, 2023.\n[22] V . Patraucean, L. Smaira, A. Gupta, A. Recasens, L. Markeeva, D. Banarse, S. Koppula,\nM. Malinowski, Y . Yang, C. Doersch, et al. , “Perception test: A diagnostic benchmark for\nmultimodal video models,” Advances in Neural Information Processing Systems , vol. 36,\npp. 42748–42761, 2023.\n[23] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, “Next-qa: Next phase of question-answering to\nexplaining temporal actions,” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pp. 9777–9786, 2021.\n[24] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y . Zhuang, “Video question answering\nvia gradually refined attention over appearance and motion,” in Proceedings of the 25th ACM\ninternational conference on Multimedia , pp. 1645–1653, 2017.\n[25] J. Xu, T. Mei, T. Yao, and Y . Rui, “Msr-vtt: A large video description dataset for bridging\nvideo and language,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pp. 5288–5296, 2016.\n[26] K. Li, Y . Wang, Y . He, Y . Li, Y . Wang, Y . Liu, Z. Wang, J. Xu, G. Chen, P. Luo, et al. ,\n“Mvbench: A comprehensive multi-modal video understanding benchmark,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 22195–22206,\n2024.\n[27] X. Fang, K. Mao, H. Duan, X. Zhao, Y . Li, D. Lin, and K. Chen, “Mmbench-video: A long-\nform multi-shot benchmark for holistic video understanding,” Advances in Neural Information\nProcessing Systems , vol. 37, pp. 89098–89124, 2024.\n[28] W. Wang, Z. He, W. Hong, Y . Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y . Dong, et al. ,\n“Lvbench: An extreme long video understanding benchmark,” arXiv preprint arXiv:2406.08035 ,\n2024.\n[29] H. Wu, D. Li, B. Chen, and J. Li, “Longvideobench: A benchmark for long-context interleaved\nvideo-language understanding,” Advances in Neural Information Processing Systems , vol. 37,\npp. 28828–28857, 2024.\n[30] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. ,\n“Kimi k1. 5: Scaling reinforcement learning with llms,” arXiv preprint arXiv:2501.12599 , 2025.\n[31] Y . Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang, W. Luo, and K. Zhang, “Marco-o1:\nTowards open reasoning models for open-ended solutions,” arXiv preprint arXiv:2411.14405 ,\n2024.\n[32] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Stein-\nhardt, “Measuring mathematical problem solving with the math dataset,” arXiv preprint\narXiv:2103.03874 , 2021.\n[33] Y . Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y . Dong, et al. , “Long-\nbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks,”\narXiv preprint arXiv:2412.15204 , 2024.\n[34] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, C. Hesse, and J. Schulman, “Training verifiers to solve math word\nproblems,” ArXiv preprint , vol. abs/2110.14168, 2021.\n[35] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y . Pang, J. Dirani, J. Michael, and S. R.\nBowman, “Gpqa: A graduate-level google-proof q&a benchmark,” in First Conference on\nLanguage Modeling , 2024.\n11\n--- Page 12 ---\n[36] Y . Wang, X. Ma, G. Zhang, Y . Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang,\net al. , “Mmlu-pro: A more robust and challenging multi-task language understanding bench-\nmark,” in The Thirty-eight Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track , 2024.\n[37] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan, “Swe-bench:\nCan language models resolve real-world github issues?,” arXiv preprint arXiv:2310.06770 ,\n2023.\n[38] W. Shi, Z. Hu, Y . Bin, J. Liu, Y . Yang, S.-K. Ng, L. Bing, and R. K.-W. Lee, “Math-llava:\nBootstrapping mathematical reasoning for multimodal large language models,” arXiv preprint\narXiv:2406.17294 , 2024.\n[39] K. Ying, F. Meng, J. Wang, Z. Li, H. Lin, Y . Yang, H. Zhang, W. Zhang, Y . Lin, S. Liu, et al. ,\n“Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language\nmodels towards multitask agi,” arXiv preprint arXiv:2404.16006 , 2024.\n[40] G. Lakoff and M. Johnson, Metaphors we live by . University of Chicago press, 2008.\n[41] OpenAI, “Openai: Hello gpt-4o,” 2024.\n[42] OpenAI, “Gpt-4o mini: advancing cost-efficient intelligence,” 2024.\n[43] OpenAI, “Introducing gpt-4.1 in the api.” https://openai.com/index/gpt-4-1/ , 2025.\n[44] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut,\nA. Lazaridou, O. Firat, J. Schrittwieser, et al. , “Gemini 1.5: Unlocking multimodal understand-\ning across millions of tokens of context,” arXiv preprint arXiv:2403.05530 , 2024.\n[45] Google DeepMind, “Gemini 2.5: Our most intelligent ai model,” March 2025.\n[46] Anthropic, “Anthropic: Introducing claude 3.5 sonnet,” 2024.\n[47] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin,\nJ. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang,\nL. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y . Fan,\nY . Su, Y . Zhang, Y . Wan, Y . Liu, Z. Cui, Z. Zhang, and Z. Qiu, “Qwen2.5 technical report,”\nCoRR , vol. abs/2412.15115, 2024.\n[48] A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé,\nM. Rivière, L. Rouillard, T. Mesnard, G. Cideron, J. Grill, S. Ramos, E. Yvinec, M. Casbon,\nE. Pot, I. Penchev, G. Liu, F. Visin, K. Kenealy, L. Beyer, X. Zhai, A. Tsitsulin, R. Busa-Fekete,\nA. Feng, N. Sachdeva, B. Coleman, Y . Gao, B. Mustafa, I. Barr, E. Parisotto, D. Tian, M. Eyal,\nC. Cherry, J. Peter, D. Sinopalnikov, S. Bhupatiraju, R. Agarwal, M. Kazemi, D. Malkin,\nR. Kumar, D. Vilar, I. Brusilovsky, J. Luo, A. Steiner, A. Friesen, A. Sharma, A. Sharma, A. M.\nGilady, A. Goedeckemeyer, A. Saade, A. Kolesnikov, A. Bendebury, A. Abdagic, A. Vadi,\nA. György, A. S. Pinto, A. Das, A. Bapna, A. Miech, A. Yang, A. Paterson, A. Shenoy,\nA. Chakrabarti, B. Piot, B. Wu, B. Shahriari, B. Petrini, C. Chen, C. L. Lan, C. A. Choquette-\nChoo, C. Carey, C. Brick, D. Deutsch, D. Eisenbud, D. Cattle, D. Cheng, D. Paparas, D. S.\nSreepathihalli, D. Reid, D. Tran, D. Zelle, E. Noland, E. Huizenga, E. Kharitonov, F. Liu,\nG. Amirkhanyan, G. Cameron, H. Hashemi, H. Klimczak-Plucinska, H. Singh, H. Mehta, H. T.\nLehri, H. Hazimeh, I. Ballantyne, I. Szpektor, and I. Nardini, “Gemma 3 technical report,”\nCoRR , vol. abs/2503.19786, 2025.\n[49] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y . Duan, H. Tian, W. Su, J. Shao, et al. ,\n“Internvl3: Exploring advanced training and test-time recipes for open-source multimodal\nmodels,” arXiv preprint arXiv:2504.10479 , 2025.\n[50] B. Li, Y . Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y . Li, Z. Liu, et al. ,\n“Llava-onevision: Easy visual task transfer,” arXiv preprint arXiv:2408.03326 , 2024.\n[51] B. Lin, Y . Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan, “Video-llava: Learning united visual\nrepresentation by alignment before projection,” arXiv preprint arXiv:2311.10122 , 2023.\n12\n--- Page 13 ---\n[52] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai,\nV . Chaudhary, C. Chen, D. Chen, D. Chen, J. Chen, W. Chen, Y . Chen, Y . Chen, Q. Dai,\nX. Dai, R. Fan, M. Gao, M. Gao, A. Garg, A. Goswami, J. Hao, A. Hendy, Y . Hu, X. Jin,\nM. Khademi, D. Kim, Y . J. Kim, G. Lee, J. Li, Y . Li, C. Liang, X. Lin, Z. Lin, M. Liu, Y . Liu,\nG. Lopez, C. Luo, P. Madan, V . Mazalov, A. Mitra, A. Mousavi, A. Nguyen, J. Pan, D. Perez-\nBecker, J. Platin, T. Portet, K. Qiu, B. Ren, L. Ren, S. Roy, N. Shang, Y . Shen, S. Singhal,\nS. Som, X. Song, T. Sych, P. Vaddamanu, S. Wang, Y . Wang, Z. Wang, H. Wu, H. Xu, W. Xu,\nY . Yang, Z. Yang, D. Yu, I. Zabir, J. Zhang, L. L. Zhang, Y . Zhang, and X. Zhou, “Phi-4-mini\ntechnical report: Compact yet powerful multimodal language models via mixture-of-loras,”\nCoRR , vol. abs/2503.01743, 2025.\n[53] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y . Wang, Y . Cheng, S. Huang, J. Ji, Z. Xue,\net al. , “Cogvlm2: Visual language models for image and video understanding,” arXiv preprint\narXiv:2408.16500 , 2024.\n[54] Z. Liu, L. Zhu, B. Shi, Z. Zhang, Y . Lou, S. Yang, H. Xi, S. Cao, Y . Gu, D. Li, et al. , “Nvila:\nEfficient frontier visual language models,” arXiv preprint arXiv:2412.04468 , 2024.\n13\n--- Page 14 ---\n1. Video Collection\nRejectChecklist\n(1) High recognition\ninterpretation?\n(2) Is it Non-\nstraightforward ?NOYES\nVideo Pool\nVisuals\n+\nAudios\n2.Data Annotation\nChecklist\n(1) Is long-span, multi-frame reasoning required?\n(2) Is  a deeper understanding of video  required  ? \nThe answer cannot be obtained directly through\nperception\n(3) Is the QA  consistent with real-world and common\nunderstanding?\nManually written QAs\nQuestion : Why did the broken umbrella\nin the boy's hand, damaged by the\nwind,  suddenly get fixed while in the air ?\nCorrect Answer:  Because the boy held\nthe girl's hand,  gained good luck , and as a\nresult, the umbrella was restored.   \nWatch the video and answer the question\nand give a corr ect answer .\nThen generate several misleading wr ong\noptions .Generate False Options\nCorrect answer of GPT-4o and\nManual the same meaning?\nYESManually written\nmisleading optionsNOCombination\nQuestioin\nCorrect Option\nFalse Options3. Quality Assurance\nManually\nQuality\nCheck\nQ: Why did the broken umbrella in the boy's hand,\ndamaged by the wind,  get fixed while in the air ?\n... \n (B) Because the boy held the girl's hand,  gained good luck , and\nas a result, the umbrella was restored.   \n... \n(H) The  video was reversed  during that part, making it look like\nthe broken umbrella was getting fixed.\n(I) Aerodynamics, changes in wind strength and direction\ncontributed  to the umbrella being repaired.\nCorrect answer of GPT -4o\nas misleading option  Figure 7: MMR-V Construction Pipeline.\nA Limitations\nDespite our efforts to improve our work, several limitations remain. (1) Scaling MMR-V is challeng-\ning due to the high cost of manual annotation and verification, as all tasks and correct answers are\ncurated and reviewed by human annotators. (2) Although we strive to cover a wide range of video\nand task types, certain real-world categories (such as mystery, puzzle-solving, and gaming) are still\nunderrepresented. (3) The majority of videos in MMR-V are in English, with only a small proportion\nin other languages such as Chinese, French, Thai, and German, which constrains its multilingual\napplicability. We will further study and try to solve this issue in the future.\nB MMR-V Construction\nB.1 Checklist\nAccording to the MMR-V construction principles introduced in the main text Section 3 , we wrote\nthe following annotation checklist:\n(1) You are expected to watch the entire video before formulating any questions or answers.\n(2) Each question must require long-distance, multi-frame reasoning and cannot be answered\nthrough direct perception (ensuring compliance with Principles 1 and 2).\n(3) To ensure consistency with real-world user perception (Principle 3), annotators are encour-\naged to refer to the official interpretation of the original video author and user consensus (highly\npraised comments in the comment section) when writing or verifying the correct answer. This\nhelps mitigate annotator bias and ensures that the reasoning task reflects the understanding of a\nwider audience.\nB.2 Construction Pipeline\nIn this section, we present the construction process of MMR-V Bench in a macro sense. The whole\nprocess is divided into three stages: video collection ,data annotation , and quality assurance . For\nvideo collection , we designed a checklist to ensure the quality and diversity of videos in the Bench.\n\"High recognition interpretation?\" ensures that the questions raised and the annotated answers based\n14\n--- Page 15 ---\nFigure 8: Annotation Platform of MMR-V .\nArtAnimationFilm\nLifePhilosophyTVDanceMusic MV\nStage Play\nPhotography\nSocial Issues\nDaily Theme\nPersonification\nHistory\nComedy\nScience Fiction\nShort FilmClassicHumorShort VideoTravelAnti-Cut EditingConcept IntroSelf-ReflectionPsychologyPhilosophyCommercialPublic Service AdTV ShowMagic\n(a) Video categories.\nImplicit\nReasoningMetaphor\nExplicit\nReasoningUnderstanding\nEmotionTheme\nUnderstanding\nRecognitionComment MatchingImplicit SymbolSequential\nCausal ReasoningStructure ReasoningCounterintuitiveReasoning Cross-modal TransferVideo Typeand Intent\n (b) Proportion of different tasks.\nFigure 9: (a) Video categories in MMR-V Bench. (b) Proportion of different tasks in MMR-V Bench.\non the video have references that are consistent with public cognition (official interpretations or highly\npraised comments) to alleviate the subjective bias of the annotator. \"Is it Non-straightforward?\"\nensures that the video is not a straightforward narrative, which is conducive to increasing the reasoning\ndifficulty of the question. For data annotation , as described in section 3.2 of the main text, we use\ngpt-4o to assist in annotation with interference options. Let the model generate the correct answer\nbased on the question, and manually review to ensure that the correct answer generated by the model\nis different from the manual annotation. If they are different, the answer generated by gpt-4o is used\nas the interference item, otherwise the interference item is manually written. For quality assurance ,\nwe designed a checklist for human reviewers to check the correctness and difficulty of the tasks. The\nannotation platform is shown in Figure 8.\nC Diversity of MMR-V\nIn this section, we show the diversity of MMR-V Bench, including video diversity and task diversity.\nFor video, we show the six categories of videos in MMR-V in Figure 9a, including Life, Animation,\nFilm, Art, TV , and Philosophy. At the same time, for each category, we divide it into several\n15\n--- Page 16 ---\nAbility Type L1 Ability Type L2 Ability Type L3\nImplicit ReasoningMetaphor Understanding (MU)Structural Metaphor, Orientational Metaphor,\nOntological Metaphor, Creative Metaphor\nTheme Understanding (TU)Philosophical Concepts, Social Issues, Personal Reflection,\nEveryday Topics, Video Naming\nEmotion Recognition (ER)Explicit Emotion, Implicit Emotion,\nMeta-emotion, Audience Emotion\nComment Matching (CM) Humorous, Thought-provoking, Trending\nImplicit Symbol (IS) Cultural Symbols, Art Symbols, Other Symbols\nExplicit ReasoningCausal Reasoning (CAR) Forward Reasoning, Backward Reasoning\nSequential Structure Reasoning (SSR)Narrative Structure, Core Connecting Elements,\nInference on Editing Techniques, Hallucination\nCounterintuitive Reasoning (CIR)Magic Deconstruction or Special Effects Editing,\nArtistic Techniques, Humor and Exaggeration\nCross-modal Transfer Reasoning (CTR) Video-to-Text, Video-to-Audio, Video-to-Video\nVideo Type and Intent (VTI) Video Type, Video Intent\nTable 5: Three-level classification of tasks in MMR-V .\nsubcategories to better understand the classification of video categories. Secondly, in section 3.3 we\nshow the diversity of video length, ranging from 7 seconds to 3771 seconds. For tasks, we divide\nthem into two parts, ten categories and 33 subcategories, three levels. The division of the first and\nsecond levels, as well as the proportion of different types of tasks, can be seen in Figure 9b.\nD Task Details\nThe tasks in MMR-V can be divided into three levels. Level 1: Implicit Reasoning & Explicit\nReasoning. Level 2: Contains ten task classes. Level 3: Contains 33 task subclasses. Next, we will\nintroduce these tasks with some task examples.\nD.1 Implicit Reasoning Tasks\nI. Metaphor Understanding (MU)\nFor the definition of subclasses of the metaphor understanding task, we mainly refer to the book\nMetaphors We Live [ 40] By by George Lakoff and Mark Johnson, which introduces metaphor-related\nconcepts in detail.\nI.1. Structural Metaphor\nTask Description : There are structural similarities between the subject and object. For example, time\ncan be compared to flowing water, both of which have the structure of flow and passing away.\nExample Question :\nQuestion : What does the brown coat in the video symbolize?\nOptions :\n(A) It is said to represent the family’s long - lost fortune that they are still searching for.\n(B) The brown coat symbolizes the lost hope of the family because it was worn during a difficult\ntime.\n(C) It refers to a coat that has been washed and taken out to dry, likely worn by the father.\",\n(D) It symbolizes the father in a family, who protects his family in times of difficulty.\n(E) It represents the fear of the outside world.\n(F) The unfulfilled dreams of the children in the family as they always saw it as a sign of\nsomething unattainable.\n(G) The brown coat in the video represents a raincoat, used to protect the clothes inside from\ngetting wet.\n16\n--- Page 17 ---\n(H) The bad luck that has been following the family for generations.\nCorrectAnswer : (D)\nVideo : father - 1 minute emotional award winning - video_url\nI.2. Orientational Metaphor\nTask Description : There are similarities in direction or composition between the subject and the\nmetaphor, for example, walking up a staircase is compared to ambition.\nExample Question :\nQuestion : Why does the dance, which is filled with artistry and beauty throughout, end with a\ndescent?\nOptions :\n(A) There is a connection between the fall and the creation at some point.\n(B) It represents a dive to explore new depths, both literal and metaphorical.\n(C) It indicates the dancer’s exhaustion, capturing a moment of fatigue.\n(D) It reflects the calmness of the ocean, evoking a sense of tranquility.\n(E) It highlights the theme of rebirth, symbolizing renewal and transformation.\n(F) It represents the beauty of underwater life, showcasing its unique allure.\n(G) It symbolizes being weighed down by emotions, expressing inner turmoil.\n(H) It symbolizes the end of a dream, marking a moment of conclusion.\n(I) It shows the dancer’s connection to water, emphasizing fluidity and grace.\n(J) It symbolizes a return to nature and surrender to life’s forces, embracing the natural flow.\n(K) It signifies the end of the dance’s energy, indicating a point of culmination.\nCorrectAnswer : (A)\nVideo : Falling - Underwater dance - video_url\nI.3. Ontological Metaphor\nTask Description : This metaphor involves viewing an abstract concept as a concrete entity. Usually,\nthe core concept of the entire video is turned into a concrete entity to tell the story.\nExample Question :\nQuestion : The scene around 1:00 metaphorically represents what aspect of communities?\nOptions :\n(A) Communities can build their resilience to setbacks by working together and adapting to new\nchallenges.\n(B) Promoting individual success in competitive environments.\n(C) Building resilience through community partnerships.\n(D) Overcoming challenges for community progress.\n(E) Celebrating the individual achievements of community members.\n(F) Developing sustainable practices for environmental harmony.\n(G) Decision-making processes of a community.\n(H) The interconnectedness of global communities.\n(I) Isolation of communities for self-sufficiency.\n(J) The role of external aid in community development.\n(K) Highlighting the diversity of cultures within a community.\nCorrectAnswer : (A)\nVideo : Resilience: Anticipate, organise, adapt - video_url\nI.4. Creative Metaphor\nTask Description : This metaphor is usually carefully designed by the author for a specific video and\nneeds to be understood in the context of the video.\nExample Question :\nQuestion : What is the pink fairy ball in the film?\nOptions :\n17\n--- Page 18 ---\n(A) It’s a toy the boy picked up on the street, having no special connection to his condition.\n(B) They are the microorganisms in this world, living in every corner.\n(C) The pink fairy ball represents the boy’s childhood dream of becoming a fairy.\n(D) It’s a hallucination caused by lack of sleep, not related to antidepressants at all.\n(E) They are the boy’s toys, which he bought to help treat his depression.\n(F) It is the effect of the antidepressants the boy is taking, which helps him see many things with\nvitality and positive effects.\n(G) It’s an advertisement prop for a new product in the background of the scene.\n(H) The ball is a sign of the boy’s wish to escape from his daily work routine.\n(I) The pink fairy ball is a symbol of the city’s upcoming festival decorations\nCorrectAnswer : (F)\nVideo : Soft Rain | Animated Short Film (2023) - video_url\nII. Theme Understanding (TU)\nII.1. Philosophical Concepts\nTask Description : The themes of the videos are usually about concepts and principles related to\nphilosophy and psychology.\nExample Question :\nQuestion : What is the overall message that the animation aims to convey?\nOptions :\n(A) It suggests happiness comes solely from financial achievements.\n(B) The animation emphasizes the need to avoid all responsibilities.\n(C) The animation aims to illustrate the ways to relieve stress.\n(D) It illustrates the mechanical process of water flow.\n(E) The animation encourages saving water to prevent wastage.\n(F) The animation conveys the importance of managing stress through self-care practices.\n(G) The animation highlights achieving success through hard work.\n(H) The animation suggests that ignoring stress leads to happiness.\n(I) The video underlines the significance of collective teamwork.\n(J) It depicts progress and growth through constant work.\nCorrectAnswer : (C)\nVideo : The Stress Bucket - video_url\nII.2. Social Issues\nTask Description : The theme of the video is usually to reflect some problems existing in today’s\nsociety and express a strong appeal of the author.\nExample Question :\nQuestion : What social reality does this video satirize?\nOptions :\n(A) The rise of environmental awareness in urban settings.\n(B) The video represents the bystander effect in society.\n(C) The economic disparities in urban vs. rural areas.\n(D) The challenges of modern relationship dynamics.\n(E) The impact of fashion trends on daily life.\n(F) The increasing complexity of urban development planning.\n(G) The need for infrastructure improvement and road safety.\n(H) The influence of social media on public behavior.\n(I) The rapid pace of technological advancement in transportation.\n(J) The shift in societal values towards individualism.\nCorrectAnswer : (B)\nVideo : Stone | 1 Minute Short Film | Hot Shot - video_url\n18\n--- Page 19 ---\nII.3. Personal Reflection\nTask Description : The author hopes that the video will inspire people to reflect on and resonate with\nthings in their lives.\nExample Question :\nQuestion : What is the core concept that the film aims to convey?\nOptions :\n(A) Romantic relationships in adolescence.\n(B) The importance of education institutions.\n(C) Overcoming supernatural challenges.\n(D) The dynamics of family disagreements.\n(E) Exploration of technological advancement.\n(F) Not to judge others too quickly.\n(G) Journey of a superhero in saving the city.\n(H) Inter-species relations on Earth.\n(I) Power struggles in political leadership.\n(J) Historical recount of a famous personality.\nCorrectAnswer : (F)\nVideo : Award Winning SHORT FILMS Don’t Judge | BATTI Hindi Heart Touching Short\nMovies | Content Ka Keeda - video_url\nII.4. Everyday Topics\nTask Description : The themes expressed in the videos are usually the sublimation of the insights and\nthemes in daily life, such as praising maternal love, friendship, etc.\nExample Question :\nQuestion : What is implied by the contrast between the scenes around 0:47 and 1:11?\nOptions :\n(A) The contrast shows that the mother is indecisive and can’t make up her mind in a crisis.\n(B) It demonstrates the father’s sense of responsibility and bravery, praising paternal love.\n(C) The contrast between the beginning and the end conveys a sense of tragedy, criticizing the\ndestruction of the ecological environment by humans.\n(D) It shows that the father wants to abandon the child when facing danger.\n(E) It shows the bravery of the bird in the background, facing authority head-on, and praises\ncourage.\n(F) The mother still protects her child at all costs even in the face of danger, which praises\nmaternal love.\n(G) It implies that the father is doing it for self - preservation rather than out of love for the child.\n(H) It shows that even when there are many birds, they do not appear very united, and in the face\nof danger, they become a disorganized mess.\nCorrectAnswer : (F)\nVideo : Mother 1 minute Sad Emotional Award Winning Iranian Short Film Animation Animated\n- video_url\nII.5. Video Naming\nTask Description : Come up with a suitable title for this video or the core content of the video (dance,\netc.). This tests the model’s control over the overall content and whether it can get the subtleties of\nthe title like humans.\nExample Question :\nQuestion : \"Please come up with a suitable name for this dance.\",\nOptions :\n(A) The Dance of the Butterfly.\",\n(B) The Rhythm of the Phoenix.\",\n(C) The Grace of the Swan.\",\n19\n--- Page 20 ---\n(D) The Spirit of the Dragon.\",\n(E) The Charm of the Peony.\",\n(F) The Step of the Tiger.\",\n(G) The Soul of Peacock\",\n(H) The Beat of the Forest.\",\n(I) The Leap of the Deer.\",\n(J) The Spin of the Star.\",\n(K) The Waltz of the Moon.\"\nCorrectAnswer : (G)\nVideo : Yang Liping - The Soul of Peacock - Peacock Dance - Traditional Dance - video_url\nIII. Emotion Recognition (ER)\nIII.1. Explicit Emotion\nTask Description : Analyze the emotions of the characters in the video. Explicit emotions can usually\nbe directly understood through facial expressions, body movements, etc.\nExample Question :\nQuestion : Summarize the boy’s emotional changes between 6:00 and 7:00.\noptions :\n(A) Anger - Fear - Surprise and happiness\n(B) Sadness - Excitement - Helplessness\n(C) Disappointment - Let - down - Sorrow\n(D) Loneliness - Isolation - Solitude\n(E) Sadness - Grief - Mourning\n(F) Sadness - Shock - Surprise and happiness\n(G) Disappointment - Astonishment - Stupefaction\n(H) Loneliness - Isolation - Sorrow\n(I) Disappointment - Excitement - Helplessness\ncorrectAnswer : (F)\nVideo : CGI Animated Short Film: \"Crunch\" by Gof Animation | CGMeetup - video_url\nIII.2. Implicit Emotion\nTask Description : Analyze the emotions of characters in the video. Implicit emotions usually need\nto be analyzed indirectly through the environment, style, etc.\nExample Question :\nQuestion : What kind of emotional atmosphere does the stage lighting create?\noptions :\n(A) Solemn and sorrowful atmosphere.\n(B) Neutral and unemotional atmosphere.\n(C) Intense and dramatic atmosphere.\n(D) Joyful and festive atmosphere.\n(E) Sadness and loss.\n(F) Confident and empowering atmosphere.\n(G) Chaotic and confusing atmosphere.\n(H) Calm and serene atmosphere.\n(I) Playful and whimsical atmosphere.\n(J) Romantic and loving atmosphere.\nCorrectAnswer : (E)\nVideo : Stages of Grief- A V ANTGARDE SHOW 2023 - video_url\n20\n--- Page 21 ---\nIII.3. Meta-emotion\nTask Description : This part refers to the high-level emotions in the video, such as the emotions\nexpressed by the author through the video, and the emotions expressed by the entire video.\nExample Question :\nQuestion : Summarize the meaning of this short film in one word.\nOptions : [\n(A) Creation\n(B) Transformation\n(C) Stress\n(D) Mutation\n(E) Metamorphosis\n(F) Growth\n(G) Rebirth\"\n(H) Destruction\n(I) Erosion\n(J) Development\n(K) Isolation\n(L) Conversion\nCorrectAnswer : (C)\nStress - Shortfilm - video_url\nIII.4. Audience Emotion\nTask Description : Analyze the emotions that viewers are most likely to feel after watching the video.\nThis is more advanced and relatively easy for humans to sense. Including the perception of humor.\nExample Question :\nQuestion : What are the reasons for the high number of views on this video?\nOptions :\n(A) The video features a well-known celebrity who has a large fan base, drawing a lot of\nattention.\n(B) The dance style is extremely unique and has never been seen before, sparking curiosity.\n(C) People are under a lot of stress and need videos that can help them unwind.\n(D) The background music is a popular hit song that many people recognize and enjoy.\n(E) The video was released during a major holiday season when people are more likely to watch\nvideos.\n(F) The choreography is incredibly complex and impressive, showcasing the dancers’ skills.\n(G) The video has a strong and inspiring message that resonates with a wide audience.\n(H) The video was featured on a popular TV show or news segment, driving more views.\n(I) The video was shared by a large number of dance schools and communities, spreading its\nreach.\n(J) The video was part of a viral challenge that encouraged people to share it.\n(K) The video has high-quality production values that make it stand out from other content.\nCorrectAnswer : \"(C)\nSatisfying and Relaxing Kinetic Sand ASMR shorts - video_url\nIV . Comment Matching (CM)\nIV .1. Humorous\nTask Description : The video will spark laughter because of certain comments, making the audience\nfeel funny, testing whether the model can match it correctly.\nExample Question :\nQuestion : Based on this video, which of the following comments is likely to make people laugh?\nOptions :\n(A) Did he just audition for a water ballet?\n21\n--- Page 22 ---\n(B) How many fish does it take to catch a man?\n(C) Is there a Walmart beneath the river?\n(D) The fish are holding a grudge, watch out!\n(E) Now that’s what I call a splash of creativity.\n(F) I came for the fishing tips and stayed for the synchronized swimming.\n(G) That water has more personality than my neighbor!\n(H) I’m starting to think he’s part fish.\n(I) I think the fish caught him instead.\n(J) That’s definitely a land fish champion.\n(K) That fish will never trust humans again.\nCorrectAnswer : \"(C)\",\nHe DI Lao - video_url\nIV .2. Thought-provoking\nTask Description : Some comments under the video will enhance people’s thinking and test whether\nthe model can accurately understand.\nExample Question :\nQuestion : Which of the following statements can better explain the social reality expressed in\nthis animation?\nOptions :\n(A) The animation showcases an idealized view of advancement within a corporate ladder.\n(B) The depiction highlights the dehumanization and mechanization of individuals in a powerful\nsocial system.\n(C) It portrays the joy of discovering one’s true passions through societal pressures.\n(D) The scenes show a man achieving happiness through daily routine.\n(E) It represents personal ambition and the drive for success in individual careers.\n(F) The animation indicates the triumph of an individual’s spirit in the face of adversity.\n(G) It reflects the disintegration of traditional family roles.\n(H) The animation shows the importance of family support in work-life balance.\n(I) It emphasizes the challenge of maintaining personal identity in urban settings.\n(J) We are all working for others without realizing it due to our own needs.\n(K) The animation illustrates the struggle with contemporary health issues.\nCorrectAnswer : (J)\nEL EMPLEO - video_url\nIV .3. Trending\nTask Description : It is relatively difficult to test whether the model can accurately infer and analyze\nthe most popular comments under the video.\nExample Question :\nQuestion : Which of the following comments best summarizes the content conveyed by this\nfilm?\nOptions :\n(A) Material possessions define one’s value.\n(B) Selfless acts lead to rewards that surpass material wealth.\n(C) Loneliness is a desirable state.\n(D) Personal gains are the ultimate goal of helping others.\n(E) Isolation is the path to personal growth.\n(F) True happiness is found through wealth accumulation.\n(G) Success comes from competitive behavior.\n(H) Sharing leads to financial prosperity.\n(I) He receives what money can’t buy.\n(J) Adversity breeds stronger individuals.\nCorrectAnswer : (I)\nUnsung Hero - video_url\n22\n--- Page 23 ---\nV . Implicit Symbol (IS)\nV .1. Cultural Symbols\nTask Description : Test whether the model can infer and analyze the cultural characteristics hidden\nunder the surface visual elements of the video (such as nationality, festivals, customs, religion, etc.).\nExample Question :\nQuestion : The plaque inscribed with “Dominating Three Continents” that appears in the video\nis most likely to be found in the architecture of which of the following religions?\nOptions :\n(A) Taoism\n(B) Shinto\n(C) Sikhism\n(D) Judaism\n(E) Islam\n(F) Christianity\n(G) Buddhism\n(H) Hinduism\n(I) Jainism\n(J) Zoroastrianism\nCorrectAnswer : (G)\n[4K] Hangzhou 2024 in the misty rain | West Lake, Lingyin Temple, Night walking in\nHefang Street - video_url\nV .2. Art Symbols\nTask Description : Test whether the model can infer and analyze the art-related characteristics hidden\nunder the surface visual elements of the video (such as dance style, artistic skills, imitation, etc.).\nExample Question :\nQuestion : What is the shadow that appears in our view at 1:40 imitating?\nOptions :\n(A) The shadow is imitating a pole dancer.\n(B) The shadow is imitating a person washing a dog.\n(C) The shadow is imitating a person brushing their hair.\n(D) The shadow is imitating someone playing a violin.\n(E) The shadow is imitating two people engaged in a conversation.\n(F) The shadow is imitating someone painting a wall.\n(G) The shadow is imitating a person feeding a horse.\n(H) The shadow is imitating a person washing their car.\n(I) The shadow is imitating a dog barking at a person.\n(J) The shadow is imitating someone performing a magic trick.\n(K) The shadow is imitating a person holding an umbrella.\n(L) The shadow is imitating someone walking a large dog.\nCorrectAnswer : (A)\nLEAKED! Hilarious Shadow Puppets - AGT 2023 Early Release - video_url\nV .3. Other Symbols\nTask Description : Test whether the model can infer and analyze other special symbols (such as\ncommercial advertisements, etc.) hidden under the surface visual elements of the video.\nExample Question :\nQuestion : \"What do you think the chimpanzee that appears multiple times in the film symbol-\nizes?\",\nOptions :\n(A) The chimpanzee symbolizes chaos and disruption in everyday life. (B) The chimpanzee\nsymbolizes a childhood fear. (C) The chimpanzee symbolizes technology invading personal\n23\n--- Page 24 ---\nspace. (D) The chimpanzee symbolizes the unpredictability of fate. (E) The chimpanzee\nsymbolizes a glue company. (F) The chimpanzee symbolizes lost opportunities. (G) The\nchimpanzee symbolizes an obsession with social status. (H) The chimpanzee symbolizes\nenvironmental degradation. (I) The chimpanzee symbolizes the desire for freedom. (J) The\nchimpanzee symbolizes misunderstanding between people. (K) The chimpanzee symbolizes\nreliability and trust in friendships.\nCorrectAnswer : (E)\nAll Gorilla glue ads - video_url\nD.2 Explicit Reasoning Tasks\nI. Causal Reasoning (CAR)\nI.1. Forward Reasoning\nTask Description : Forward reasoning can also be understood as the prediction of future events,\nincluding prediction of outcomes, prediction of content that has not yet appeared, etc.\nExample Question :\nQuestion : What is the speculated ending of the film?\nOptions :\n(A) The movie concludes with an unexpected twist where the flowers reveal a hidden secret.\n(B) The ending is a cliffhanger, leaving the audience uncertain about the characters’ fate.\n(C) Her boyfriend passed away due to illness, leaving the girl devastated with grief.\n(D) The film wraps up with a joyous family reunion.\n(E) The film ends with a dramatic breakup as one character leaves with a heavy heart.\n(F) The movie concludes with a comedic mishap involving the flowers.\n(G) The ending shows a tragic farewell as one character moves to a new city.\n(H) The film ends with the revelation of a long-lost sibling.\n(I) The story concludes with the characters embarking on a spontaneous road trip.\n(J) The film ends on a melancholic note, reflecting on lost opportunities.\n(K) The video closes with a heartwarming reconciliation between the main characters after\nexchanging heartfelt notes and gestures.\nCorrectAnswer : (C)\nFor Milo - A WARD WINNING 1 Minute Short film (2020) - video_url\nI.2. Backward Reasoning\nTask Description : Backward reasoning means finding the cause from the effect and inferring the\nreason why an event occurred.\nExample Question :\nQuestion : Why was the elderly black man warned by security at the beginning of the film?\nOptions :\n(A) Mobile phones are not allowed for recording during magic shows.\n(B) He was trying to sell unauthorized merchandise.\n(C) He was recognized as a local celebrity causing disruptions.\n(D) He was accused of stealing a bicycle.\n(E) He was creating loud music disturbing the peace.\n(F) He was believed to have lost his entrance ticket.\n(G) He was inadvertently blocking the pathway.\n(H) He was associated with another person causing trouble nearby.\n(I) He was engaged in card tricks that security found suspicious.\n(J) He was loitering without a purpose.\nCorrectAnswer : (A)\nNow You See Me Official Opening Scene (2013) - Mark Ruffalo, Morgan Freeman Movie\nHD- video_url\n24\n--- Page 25 ---\nII. Sequential Structure Reasoning (SSR)\nII.1. Narrative Structure\nTask Description : Reasoning and analyzing the narrative order of the entire video, including the\nediting order, such as sequential, flashback, and interpolation.\nExample Question :\nQuestion : What kind of narrative sequence does the film employ?\nOptions :\n(A) non-linear flashback sequence, where events are shown out of chronological order, often\nrevealing backstory\n(B) parallel overlapping sequences, showing multiple storylines happening simultaneously with\nsome overlap\n(C) cyclical narrative structure, repeating events or themes in a circular pattern\n(D) linear narrative sequence, following a straightforward progression from beginning to end\n(E) random jumps in the timeline, moving unpredictably between different points in time\n(F) interwoven thematic structure, weaving together different themes and ideas throughout the\nstory\n(G) reverse chronological order, starting with the end and moving backwards in time\n(H) fragmented narrative, presenting the story in disjointed or broken segments\n(I) begins with a flashback and then proceeds in chronological order\n(J) episodic progression, advancing the story through a series of distinct episodes or chapters\n(K) multi-perspective narrative, telling the story from multiple characters’ points of view\nCorrectAnswer : (I)\nIdentity SHORT FILM (Award Winning Inspirational Short) - video_url\nII.2. Core Connecting Elements\nTask Description : Videos with this type of question usually have a key connecting element that runs\nthrough the entire video. It is carefully designed by the producer and tests the model’s inductive\nreasoning of the visual information of the entire video.\nExample Question :\nQuestion : What is the recurring element in the video, summarized in one word?\nOptions :\n(A) Pareidolia\n(B) Smile\n(C) Alarm\n(D) Work\n(E) Mirror\n(F) Mundane\n(G) Routine\n(H) Suit\n(I) Coffee\n(J) Sleep\n(K) Bedroom\n(L) Portrait\nCorrectAnswer : (B)\nPAREIDOLIA - 1 Minute Short Film | Award Winning - video_url\nII.3. Inference on Editing Techniques\nTask Description : These tasks evaluate the models’ deep analysis and multimodal reasoning about\nvideo editing strategies.\nExample Question :\n25\n--- Page 26 ---\nQuestion : \"Please guess how many videos were needed to record the moment the man punched\nthe punctured water ball at the beginning of the video?\",\nOptions :\n(A) At least two separate takes would be needed.\n(B) At least one single take is needed.\n(C) Three separate takes are needed.\n(D) Four separate takes are needed.\n(E) Each scene can be captured in a single continuous take.\n(F) Five separate takes are needed.\n(G) Six separate takes are needed.\n(H) Eight separate takes are needed.\n(I) Ten separate takes are required.\n(J) Twenty separate takes are necessary.\n(K) At least ten separate takes are needed.\nCorrectAnswer : (C)\nPlaying With Time - video_url\nNote: The reasoning and analysis process of this question can refer to this disassembly video .\nII.4. Hallucination\nTask Description : Evaluate whether the model perceives various types of hallucinations when\nperceiving video content.\nExample Question :\nQuestion : How many dancers are there in the video?\nOptions :\n(A) 0\n(B) 1\n(C) 2\n(D) 3\n(E) 4\n(F) 5\n(G) 6\n(H) 7\n(I) 8\n(J) 9\n(K) options before are all false\nCorrectAnswer : (B)\nRat dance with falling body parts - video_url\nIII. Counterintuitive Reasoning (CIR)\nIII.1. Magic Deconstruction or Special Effects Editing\nTask Description : This type of video usually creates some impossible magical effects, but some are\nmagic tricks, and some are editing and special effects, which require deeply reasoning.\nExample Question :\nQuestion : Starting at 4:35, how did the man achieve this magical effect in the magic trick?\nOptions : (A) Sleight of hand technique with a hidden ring, using dexterity to conceal and reveal\nthe ring.\n(B) Utilizing a mirror to confuse the audience, creating optical illusions through reflection.\n(C) A distraction technique with a smoke bomb, diverting attention with a sudden burst of smoke.\n(D) A special ring that retracts into a fake thumb, using a concealed mechanism to make the ring\ndisappear.\n(E) Using a magnet hidden in the sleeve, manipulating objects with magnetic force.\n(F) A camera trick with video editing, altering footage to create the illusion of magic.\n(G) Sleight of hand technique with a hidden string, using a concealed thread to control objects.\n26\n--- Page 27 ---\n(H) The bottle inside the paper bag had already been altered to leave only the outer plastic skin.\n(I) Employing a twin assistant to swap the ring, using a look-alike to deceive the audience.\n(J) The use of an invisible thread, employing a nearly undetectable line to move objects.\n(K) A sound cue to mislead the audience’s attention, using noise to distract from the real action.\nCorrectAnswer : (H)\nLevel 1 to 100 Magic Tricks Anyone Can Do - video_url\nIII.2. Artistic Techniques\nTask Description : This type of video usually creates some impossible scenes, but it is usually an\nartistic expression deliberately designed by the author.\nExample Question :\nQuestion : Why is the shadow on the boy’s face illuminated by sunlight at 1:06?\nOptions : (A) Because the boy moves to a position where a strong light source is directly above\nhim, not related to the girl.\n(B) It’s just a coincidence that the angle of the sun changes suddenly at that moment, and has\nnothing to do with any special meaning.\n(C) The sunlight illuminates the shadow because the cameraman adjusts the lighting equipment\nto create a better visual effect.\n(D) The girl’s appearance brings good luck, and the sunlight representing good fortune clears\naway the gloom of bad luck in his world.\n(E) This is because the boy has walked into a neighborhood with better weather and climate.\n(F) The sunlight lights up the shadow because there is a hidden light - emitting device in the\nscene that is turned on at 1:06.\n(G) It’s a result of the special lens filter used during filming, which makes the shadow on the\nboy’s face appear to be lit by sunlight.\n(H) Because the boy didn’t get hurt after falling and his mood improved, the sunlight is used to\nrepresent his improved mood.\nCorrectAnswer : (D)\nCGI Animated Short Film HD \"Jinxy Jenkins & Lucky Lou\" by Mike Bidinger & Michelle\nKwon | CGMeetup - video_url\nIII.3. Humor and Exaggeration\nTask Description : A common technique in humorous videos is to use exaggerated expressions that\nseem unreasonable, but there are some clues to understand the meaning. This type of question tests\nthe model’s ability to reason about exaggerations and unusual techniques.\nExample Question :\nQuestion : Why does the first half of the scene look sunny but also show rain?\nOptions :\n(A) It is a sunshower, when rain falls while the sun is shining.\n(B) The character is dreaming of being both wet and warm.\n(C) There are rainclouds directly above while sunlight comes from the side.\n(D) It is snow instead of rain, reflecting the sunlight.\n(E) The effect is caused by morning fog and light refraction.\n(F) It’s a visual illusion caused by mist.\n(G) The character moved to a different location quickly.\n(H) A rainbow is forming which intensifies the sunlight.\n(I) Dew drops from trees reflect sunlight.\n(J) There are two unrelated weather animations merged together.\n(K) The man wet the bed, which caused the presence of water in his dream.\nCorrectAnswer : (K)\nIt now makes sense - video_url\n27\n--- Page 28 ---\nIV . Cross-modal Transfer Reasoning (CTR)\nEvaluate the ability to transfer reasoning from video to text, audio, video or image (for example,\nvideo-to-text: the theme of a video may have the same meaning as a famous quote) Task Description :\nEvaluate the ability to transfer reasoning from video to text (for example, the theme of a video may\nhave the same meaning as a famous quote)\nExample Question :\nQuestion : Which of the following proverbs best explains the theme of this short film?\nOptions :\n(A) When one door closes, another opens.\n(B) Opportunity knocks only once.\n(C) Time heals all wounds.\n(D) The early bird catches the worm.\n(E) Never judge a book by its cover.\n(F) All that glitters is not gold.\n(G) The grass is always greener on the other side.\n(H) Actions speak louder than words.\n(I) A stitch in time saves nine.\n(J) Beauty is in the eye of the beholder.\n(K) Absence makes the heart grow fonder.\n(L) A penny saved is a penny earned.\nCorrectAnswer : (E)\nVideo : Award Winning SHORT FILMS Don’t Judge | BATTI Hindi Heart Touching Short\nMovies | Content Ka Keeda - video_url\nV . Video Type and Intent (VTI)\nV .1. Video Type\nTask Description : Evaluate the model’s ability to analyze video types, such as commercials, science\nfiction films, comedies, etc.\nExample Question :\nQuestion : What type of video is this most likely to be?\nOptions :\n(A) A documentary about airplane technology\n(B) Advertisement for an ice-cream\n(C) A drama set on an airplane\n(D) A comedy film featuring an airline\n(E) An in-flight safety demonstration video\n(F) A travel vlog featuring aerial views\n(G) A science fiction movie on a spaceship\n(H) This is an advertisement.\n(I) A video tour of an airplane factory\n(J) A virtual reality experience of flying\n(K) A news segment on turbulence incidents\nCorrectAnswer : (H)\nLeo Messi vs Kobe Bryant - Legends on Board - Turkish Airlines - video_url\nV .2. Video Intent\nTask Description : Reasoning and analyzing the purpose and production intention of the video (e.g.\nwhat kind of product performance is promoted in a commercial advertisement, etc.)\nExample Question :\nQuestion : Which year do you think this video was most likely released?\nOptions :\n(A) 2018\n28\n--- Page 29 ---\n(B) 2017\n(C) 2016\n(D) 2015\n(E) 2023\n(F) 2019\n(G) 2023\n(H) 2020\n(I) 2014\n(J) 2013\nCorrectAnswer : (H)\nLockdown | One Minute Short Film Challenge | Film Riot - video_url\nE Evaluation Details\nE.1 Baselines\nThe baselines include closed-source models: (1) GPT series: GPT-4o [ 41], GPT-4o-mini [ 42],\nand GPT-4.1 [ 43]; (2) Gemini series: Gemini-2.0-flash, Gemini-2.0-flash-thinking-01-21 [ 44], and\nGemini-2.5-flash [45]; (3) Claude-3-5-Sonnet-20241022 [46]; (4) o4-mini [4]; open-source models:\n(1) Qwen series: Qwen2.5-VL (7B/72B-Instruct) [ 47]; (2) Gemma series: Gemma-3 (12B/27B) [ 48];\n(3) InternVL series: Intern3-VL (8B/38B) [ 49]; (4) LLava series: LLava-Onevision-7B [ 50], Video-\nLLava-7B [ 51]; (5) Phi-4-multimodal-Instruct [ 52]; (6) Other video models: Cogvlm2-video-llama3-\nchat [53], NVILA-8B-Video [54]. All local experiments are conducted on 4×A100 80GB GPUs.\nE.2 Frame Selection\nWe followed the official configurations of models that support multi-image input, as well as settings\nin previous works [ 8;28], to define the number of input frames for each model. Specifically, we fixed\nthe number of frames per model and sampled them evenly across the video duration. We sampled\n8 frames for LLaV A-OneVision, Video-LLaV A, and NVILA-8B-Video, Phi-4-multimodal-instruct;\n16 frames were sampled for Qwen2.5-VL-7B, Qwen2.5-Omni-7B, CogVLM2-Video-LLaMA3-\nChat, InternVL-8B, Gemma-3-it-12B and Gemini-2.0-Flash-Thinking; 32 frames were sampled\nfor Qwen2.5-VL-72B, InternVL-38B, Gemma-3-it-27B , GPT-4o, GPT-4o-Mini, o4-mini, Gemini-\n2.5-Flash-preview and Claude-3.5-Sonnet. Exceptionally, since Gemini-2.0-Flash supports long\nvideo and multimodal context inputs, we sampled one frame per second across each video, with a\nmaximum cap of 512 frames to ensure API stability. Additionally, to enable a fair comparison with\nGemini-2.0-Flash-Thinking, we also tested a version of Gemini-2.0-Flash with 16 frames.\nF Details of CoT Analysis Experiment\nThis section presents the CoT analysis experiments discussed in Section 4.5. We use a representative\nmodel CoT to illustrate 4 categories of analysis in Figure 10. Specifically, Text Analysis refers to\nthe examination of textual information such as the question and options; Video Analysis focuses on\nthe content of the video; Question Frame targets the specific frame referenced in the question—for\ninstance, the frame where the magician controls two flames; and Other Frame pertains to frames\noutside the scope of the question. In Figure 10, yellow, red, and blue represent text, question frame,\nand other frame analysis respectively. Red, blue, and green all represent video analysis.\n29\n--- Page 30 ---\nPrompt for CoT Annotation\nYou will be given a model’s textual reply to a video-based question along with the video frames. Your task is to\ndetermine four boolean labels for each chunk of the reply:\n1. ‘other frame desc‘: Does this chunk describe visual information from frames other than question frame?\n2. ‘question frame desc‘: Does this chunk correctly describe visual information from the question frame specified\nin the question?\n3. ‘video analysis‘: Does this chunk perform analysis of the video content?\n4. ‘text analysis‘: Does this chunk perform analysis of the text (e.g., question text, options) rather than visual\ncontent\n- The question frame refers to the specific frame(s) referenced by the question prompt.\n- Other-frame descriptions are visual details not present in the question frame but from other frames.\n- Video analysis includes describing trends, motions, or visual inference beyond plain description.\n- Text analysis includes reasoning over question text, options, or external text context.\nRespond strictly in JSON:\n{\n\"other frame desc\": true or false,\n\"question frame desc\": true or false,\n\"video analysis\": true or false,\n\"text analysis\": true or false\n}\nQuestion: {question}\nReply Chunk: {chunk}\nWhole CoT Reply: {CoT}\nTable 6: CoT analysis prompt.\n30\n--- Page 31 ---\nCoT Analysis Example\nQuestion: How does the man at the beginning of the video pick up and casually \ncontrol the flame on the lighter?\nOptions: \n(A) He used a holographic projector to simulate the flame.\n(B) He used a special flame-retardant chemical on his hand to create the illusion.\n(C) He possessed an innate immunity to fire.\n(D) He practiced yoga meditation to withstand any flame heat.\n(E) A quick extinguishing spray was applied that halted the flame.\n(F) He surrounded the flame with an invisible film.\n(G) He mastered the art of fire manipulation.\n(H) The flame was made of non-flammable gas.\n(I) He applied a hidden cooling technology under his sleeve.\n(J) The flame was actually an LED light.\n(K) A hidden lighter in his hand, a sleight of hand trick.\nMy thinking process to arrive at the answer is as follows:\\n\\n1. **Understand the Question:** \nThe question is specifically asking about *how* the man at the beginning of the video picks up and \n*casually controls* the flame on a lighter.  The key words are \\\"how\\\" and \\\"casually controls,\\\" \nimplying a method or technique rather than a supernatural ability or complex technology.\\n\\n2. \n**Analyze the Video**I noticed that the \\u201cfloating\\u201d flame always stayed just above \nhis palm in a way that perfectly hid the source, and that when he distracted us he slipped a small \nbutane lighter into position. In the close\\u2010up reveal later, you can see how the second \nlighter remains concealed until he produces the flame on his fingers. It\\u2019s all done by classic \npalm concealment and sleight of hand with a hidden lighter. \\n\\n2. **Analyze the Options:** I \nreviewed each option to see which one is most plausible and aligns with the context of a magic \ntrick video. I considered the practicality and likelihood of each explanation.\\n\\n    *   **(A) \nHolographic Projector, (J) LED Light, (H) Non-flammable gas:** These options suggest the flame \nisn't real. While possible for illusions, they are less likely for a *casual* control and might be \nvisually detectable as fake in a video focusing on close-up magic.\\n\\n    *   **(C) Innate immunity \nto fire, (D) Yoga meditation, (G) Mastered fire manipulation:** These options are fantastical or \nmystical and don't explain a practical method.  They are less likely to be the intended explanation \nin a video about magic *tricks*.\\n\\n    4. **Prioritize Simplicity and Plausibility:** In the context \nof magic tricks, simpler explanations are often better. Option (K) - sleight of hand - is the \nsimplest and most direct explanation that fits the description of \\\"casual control.\\\"  It's a well-\nknown method for creating illusions with small objects like lighters.\\n\\n5. **Eliminate Less Likely \nOptions:** I eliminated the options that were:\\n    *   Too complex or technologically advanced (A, \nI).\\n    *   Unrealistic or fantastical (C, D, G, H).\\n    *   Less direct or slightly less probable for \n\\\"casual control\\\" in a close-up magic context compared to sleight of hand (B, E, F, J).\\n\\n6. \n**Select the Best Fit:** Option (K) - \\\"A hidden lighter in his hand, a sleight of hand trick\\\" - \nemerges as the most likely and best-fitting answer because it is:\\n    *   A common and effective \nmagic technique.\\n    *   A simple and direct explanation.\\n    *   Consistent with the idea of \n\\\"casual control.\\\"\\n    *   Plausible in the context of a magic trick video.\\n\\n\\nTherefore, based \non the question, the options, and the likely context of a magic trick video, option (K) is the most                                       \naccurate answer.\\n\\n**Final Answer:** [[K]]\nFigure 10: CoT example of experiments in Section 4.5. Yellow, red, and blue represent text, question\nframe, and other frame analysis respectively. Red, blue, and green all represent video analysis.\n31\n--- Page 32 ---\nG Evaluation Prompt\nWe evaluated two settings in the main experiment, zero shot and zero shot + CoT. The prompts used\nare as follows.\nPrompt for Zero-Shot Setting\n[[INSTRUCTION ]]\nPlease select the best answer to the following multiple-choice question based on the video.\nOnly one option is the most accurate answer in relation to the question and the video.\nWhat is the correct answer to this question\n{Question}\nOptions:\n{Options}\n[[ENDOFINSTRUCTION ]]\n[[O UTPUT FORMAT ]]\nFormat your answer as follows:\nPlease directly output the answer letter without thinking and explanation.\nIf the correct option letters (A, B, C, D... ) for the multiple-choice question is X, give the final correct option\nnumber in the following format: \"[[X]]\"\n[[ENDOFOUTPUT FORMAT ]]\nTable 7: Evaluation prompt for the Zero-Shot Setting.\nPrompt for Zero-Shot + CoT Setting\n[[INSTRUCTION ]]\nPlease select the best answer to the following multiple-choice question based on the video.\nOnly one option is the most accurate answer in relation to the question and the video.\nWhat is the correct answer to this Question:\n{Question}\nOptions:\n{Options}\nLet’s think step by step.\n[[ENDOFINSTRUCTION ]]\n[[O UTPUT FORMAT ]]\nFormat your answer as follows:\nYour thinking process.\nIf the correct option letters (A, B, C, D... ) for the multiple-choice question is X, give the final correct option\nnumber in the following format: \"[[X]]\"\n[[ENDOFOUTPUT FORMAT ]]\nTable 8: Evaluation prompt for the Zero-Shot + CoT Setting.\nH Case Study\nIn this section, we present reasoning processes and results from selected models on the MMR-V\nbenchmark. Through these case studies, we aim to better illustrate the current shortcomings of models\nin multimodal reasoning tasks and provide insights that may inspire future research and advancements\nin this area.\nFirstly, there is a comparison between a good CoT and a poor CoT in 11. Yellow highlights indicate\ntext-based analysis, while green highlights denote video-based analysis. As shown, the upper CoT\nengages in deep multimodal reasoning grounded in the video content, ultimately leading to the correct\nanswer. In contrast, the lower CoT performs only shallow perception of the video and relies heavily\non deep reasoning over the textual content, resulting in a text-dominant reasoning process that yields\nan incorrect answer.\nNote: The reasoning and analysis process of example Figure 15 can refer to this disassembly video .\n32\n--- Page 33 ---\nGood MCoT vs Bad MCoT\nQuestion: What does the object being chased by the people refer to?\nOptions: \n(A) Difficulties in life.\n(B) His fully automatic house that can walk autonomously.\n(C) The house that this person is saving money to buy.\n(D) The old house that the family decided to renovate.\n(E) The villa that the rich man bought as a vacation home.\n(F) The mansion that the celebrity owns in the city center.\n(G) The cottage that the couple wants to rent for their honeymoon.\n(H) The enchanted house.\n      o4-mini-CoT: The animation shows people in hot pursuit of a \nwalking\\u2010house\\u2014literally chasing after a \\u201chome.\\u201d In the course \nof the film we see the house slipping through rain, snow, across seas and canyons, \nalways just out of reach, until in the end it stands on the far side of a great divide \nbetween a crowd of people and a cluster of buildings. This is a metaphor for the \nendless struggle to afford and finally own a home. Thus the chased object refers to \nthe house the person is saving up to buy. [[C]]\n      Gemini-2.0-Flash-Thinking-CoT: The**Thinking Process:**\\n\\n1. **Identify \nthe question:** The question asks about the meaning of \\\"the object being chased by the \npeople\\\".\\n\\n2. **Watch the video and observe the object:** The video clearly depicts a \nhouse with legs walking and being chased by people.\\n\\n3. **Analyze the options and \ncompare them to the video:**\\n    * **(A) Difficulties in life:**  While metaphorical \ninterpretations are possible, the video literally shows a house being chased. This option is \ntoo abstract and doesn't directly relate to the visual content.\\n    * **(B) His fully \nautomatic house that can walk autonomously:** This option directly describes what is seen \nin the video \\u2013 a house that can walk on its own. This is a strong contender.\\n    * **(C) \nThe house that this person is saving money to buy:**  There is no visual or narrative \nelement in the short video clip about saving money or buying a house. This option is not \nsupported by the video.\\n    * **(D) The old house that the family decided to renovate:**  \nNo renovation or family context is present in the clip.  This option is irrelevant.\\n    * **(E) \nThe villa that the rich man bought as a vacation home:** The video does not suggest wealth, \nvacation, or a villa specifically. This option is not supported.\\n    * **(F) The mansion that \nthe celebrity owns in the city center:**  No celebrity, mansion, or city center is depicted. \nThis option is irrelevant.\\n    * **(G) The cottage that the couple wants to rent for their \nhoneymoon:**  There is no mention of a couple, honeymoon, or cottage rental in the video \nclip. This option is irrelevant.\\n    * **(H) The enchanted house:** While the walking house \ncould be considered \\\"enchanted,\\\" option (B) is more precise and descriptive of the \n*mechanism* of the house's unusual behavior (autonomously walking). \\\"Enchanted\\\" is a \nmore general and less accurate description compared to \\\"fully automatic house that can \nwalk autonomously.\\\"\\n\\n4. **Select the best option:** Option (B) is the most direct, \naccurate, and visually supported answer based on the video. It literally describes the \nobject being chased.\\n\\n**Final Answer:**\\n\\n[[B]] Figure 11: A comparison of CoTs from two models on the same task. Yellow and green indicate text\nand video analysis, respectively. As shown, o4-mini’s reasoning paradigm demonstrates a deeper\nanalysis of the video content.\n33\n--- Page 34 ---\nQuestion: Why did the broken umbrella in the boy's hand, damaged by the wind, suddenly get fixed while the boy \nand the girl were in the air?Lack of Visual Reasoning\nGemini-2.0-Flash-Thinking: [[Thinking Process]]\\nThe user is asking for the best answer to the question \\\"Why did the \nbroken umbrella in the boy's hand, damaged by the wind, suddenly get fixed while the boy and the girl were in the air?\\\" \nbased on the provided video and options.\\n\\nAfter watching the video, it's clear that:\\n1. The umbrella is initially broken \nby the wind on the ground.\\n2. The boy and girl are lifted into the air by pianos attached to balloons.\\n3. While in \nthe air, the broken umbrella appears to become fixed.\\n\\nNow let's evaluate each option against what's observed in the \nvideo and consider the most logical explanation within the context of the animation:\\n\\n* ...... **(B) Because the boy held \nthe girl's hand, gained good luck, and as a result, the umbrella was restored.** - This is a whimsical, magical explanation. \nWhile the video has a fantastical element, \\\"good luck\\\" causing physical repair is not the most direct or likely intended \nanswer.\\n\\n  ......(I) Aerodynamics, changes in wind strength and direction contributed to the umbrella being repaired.** - \nThis is a more detailed and scientifically grounded explanation.  As they ascend, changes in wind strength, direction, and \nthe aerodynamic forces acting on the umbrella could allow it to reshape itself. This is very plausible.\\n\\n* - While the video \n*is* computer-generated animation, this option describes *how* it's shown, not *why* it happens within the video's \nnarrative. It doesn't explain the in-universe reason for the repair.\\n\\n\\nComparing options (C) and (I), option (I) is more \ncomprehensive as it includes \\\"aerodynamics\\\" in addition to changes in wind, which is a more complete physics-based \nexplanation relevant to an umbrella in motion in the air. Option (C) is a simplified version of (I).\\n\\nOption (I) is the most \nscientifically plausible and best explanation from the given options for why the umbrella appears to get fixed within the \ncontext of the animation.\\n\\n[[I]]\nHuman Error Annotation:The model only analyzes visual information in the first small orange part of the analysis, and \nonly analyzes the frame corresponding to the question. After that, most of the CoT is doing text analysis (comparing the pros \nand cons of each option)Options: \n(A) Because the boy initially used too much force, causing the umbrella to be blown apart, and it was repaired after he \nadjusted his effort.\n(B) Because the boy held the girl's hand, gained good luck, and as a result, the umbrella was restored.\n(C) Because the wind in the air happened to die down, the umbrella returned to normal.\n(D) Because the girl held the boy's hand, making him feel extremely happy, which in turn allowed the umbrella to be \nrepaired smoothly.\n(E) It's just a visual illusion in the video, and the umbrella wasn't actually blown apart in the first place.\n(F) The umbrella was designed to be a self - repairing model, and it activated its repair function automatically.\n(G) It was a pre - arranged magic trick, and the umbrella was never truly damaged.\n(H) The video was reversed during that part, making it look like the broken umbrella was getting fixed.\n(I) Aerodynamics, changes in wind strength and direction contributed to the umbrella being repaired.\n(J) It was a computer - generated special effect added during post - production to create a magical look.\"\nVideo Type: Animation\nTask Type L1: Explicit Reasoning.\nTask Type L2: Counterintuitive Reasoning\nTask Type L3: Artistic TechniquesFigure 12: Error Case: Lack of Visual Reasoning.\n34\n--- Page 35 ---\nQuestion: How did the man manage to insert the top card into the deck and then reveal the top card again, only to \nfind it back in its original position?Lack of Visual Reasoning\nGemini-2.0-Flash-Thinking: My thinking process to arrive at the correct answer, option (I), is as follows:\\n\\n1. \n**Understand the Question:** The question is asking for the *method* used by the man to make the top card reappear on \ntop of the deck after being inserted into the deck.  This is a classic magic trick scenario.\\n\\n2. **Watch the Video \nCarefully:** I watched the video multiple times, paying close attention to the man's hand movements, especially \nwhen he \\\"inserts\\\" the card and then reveals it again.\\n\\n3. **Analyze the Actions in the Video:**\\n    * He shows \nthe top card (2 of Clubs).\\n    * He *appears* to insert the 2 of Clubs into the deck.\\n    * He immediately reveals the \ntop card again, and it's still the 2 of Clubs.\\n\\n4. **Evaluate Each Option Against the Video and Common Magic \nPrinciples:** I went through each option, considering if it's plausible based on what I saw and general knowledge of magic \ntricks.\\n\\n    * **(A) Hidden Compartment:**  Possible in theory, but decks with hidden compartments are less common for \nsimple card tricks.  It's also not visually indicated in the video.  Less likely for this type of quick trick.\\n  ......***(I) Sleight \nof Hand:** This is the most probable and general explanation for card magic. Sleight of hand encompasses various \ntechniques to manipulate cards secretly. This option directly addresses the core of card magic and fits the observed actions.  \n*This is the most likely answer.*\\n    * **(J) Rigged Springs:**  Mechanically complex and unnecessary. Springs would be \nhard to conceal and control for this specific effect. Unlikely.\\n    * **(K) Turned Over Two Cards:**  Doesn't explain how \nthe card returns to the top after insertion.  Also, visually, it looks like he's only turning over one card.  Unlikely and \nirrelevant to the question.\\n\\n5. **Select the Best Fit:** After evaluating all options, option (I) \\\"The man used sleight of \nhand to secretly return the card to the top\\\" stands out as the most general, accurate, and likely explanation.  It encompasses \nthe core principle of card magic and directly addresses the question of how the card returned to the top. Option (B) is a \n*type* of sleight of hand, but (I) is broader and more encompassing as a general explanation.  Option (H) is also related to \nsleight of hand but option (I) is more direct to the question.\\n\\n6. **Final Answer:** Therefore, option (I) is the best \nanswer.\\n\\n[[I]]\nHuman Error Annotation: The model only analyzes the actions in the frames adjacent to the question frame in the video. \nIn fact, at the end of the video, the magician explains that he turned over two cards at the beginning and showed the second \ncard. The model did not observe the evidence frame of the decryption part through visual reasoning.Options: \n\"(A) The card deck was equipped with a hidden compartment.\",\n(B) He had two identical cards, and he switched them.\",\n(C) The card deck was magnetic, allowing precise control.\",\n(D) The cards were fitted with a special adhesive.\",\n(E) The man used camera tricks to manipulate what was shown.\",\n(F) He used invisible threads to control the card's position.\",\n(G) The cards had sensors that automatically rearranged them.\",\n(H) He inserted the card in a fake cut, not the real deck.\",\n(I) The man used sleight of hand to secretly return the card to the top.\",\n(J) The deck was rigged with springs to pop the card back up.\",\n(K) When showed the cards, he actually turned over two cards.\"\nVideo Type: TV\nTask Type L1: Explicit Reasoning.\nTask Type L2: Counterintuitive Reasoning\nTask Type L3: Magic Deconstruction\nFigure 13: Error Case: Lack of Visual Reasoning.\n35\n--- Page 36 ---\nQuestion: What is the main message that this video wants to convey?Implicit Misinterpretation\nGPT-4o: [Analyze the best option for question] \\nThe video showcases a brown coat protecting other clothes from rain on a \nclothesline. The actions and symbolism suggest the brown coat's sacrifice, highlighting the theme of protection or \nsacrifice for others, akin to a familial role. \\n\\n[Justification for your final choice based on the thinking \nprocess.]\\nThrough the coat's protective measures, the video illustrates the theme of sacrifice by the brown coat to keep \nother clothes dry. This aligns closely with option C, where the sacrifice of the brown coat ensures that the other clothes do \nnot get wet.\\n\\nGive the final correct option number in the following format: \\\"[[C]]\\\"\nHuman Error Annotation: The model only analyzes the actions in the frames adjacent to the question frame in the video. \nIn fact, at the end of the video, the magician explains that he turned over two cards at the beginning and showed the second \ncard. The model did not observe the evidence frame of the decryption part through visual reasoning.Options: \n(A) The video is mainly about showing how easy it is to solve family problems without much effort.\n(B) It praises the mother's strength and kindness, and how she teaches her children to interact with others.\n(C) It praises the sacrifice of the brown coat, ensuring that the other clothes do not get wet.\n(D) It aims to illustrate that family members should always rely on external help when in trouble.\n(E) The video tries to convey that family is not important compared to personal success.\n(F) The video praises the father's sense of responsibility and commitment.\n(G) The main point is that fathers always make mistakes and children have to fix them.\n(H) It aims to show that fathers should focus more on their careers than on family.\nVideo Type: Animation\nTask Type L1: Implicit Reasoning.\nTask Type L2: Theme Understanding\nTask Type L3: Everyday Topics\nFigure 14: Error Case: Implicit Misinterpretation.\n36\n--- Page 37 ---\nQuestion: Please guess how many videos were needed to record the moment the man punched the punctured water \nball at the beginning of the video?Reasoning Error\nGemini-2.0-Flash-Thinking: [[Thinking Process]]\\nThe question asks to guess the number of videos needed to record the \nwater balloon punch scene at the beginning of the video.  This is a question about the *process* of video making, \nspecifically related to achieving a good take.\\n\\nBased on common video production knowledge and the nature of capturing \na dynamic event like a water balloon bursting in slow motion, it is highly unlikely that such a scene would be captured \nperfectly in a single take. There are many variables such as timing of the punch, the balloon's burst pattern, camera \nfocus, and framing.\\n\\nConsidering the options:\\n* **Options suggesting a single take (B) and (E)** are less likely. \nCapturing a dynamic and potentially messy action like a water balloon burst perfectly on the first try is improbable, \nespecially for a visually appealing slow-motion shot.\\n* **Options suggesting multiple takes (A), (C), (D), (F), (G), (H), (I), \n(J), (K)** are more reasonable. Filmmaking often involves multiple takes to ensure the best possible shot.\\n* **Option (A) \n'At least two separate takes would be needed.'** This is the most conservative option indicating that more than one \ntake is probable. It's a very reasonable minimum guess.\\n* **Options with higher numbers of takes (C), (D), (F), (G), \n(H), (I), (J), (K)** are also possible, but they are more specific guesses.  Without explicit information from the video, these \nare harder to justify as the *best* guess compared to the minimal \\\"at least two\\\".\\n* **Option (K) 'At least ten separate \ntakes are needed.'**  and **(J) 'Twenty separate takes are necessary.'** are likely overestimations for a relatively simple \nshot like punching a water balloon.  While possible, they are less probable as the *best* guess without more \ninformation.\\n\\nGiven that we are asked to *guess* and choose the *best* answer, the most reasonable and safe guess, \nbased on general video production practices and the nature of the scene, is that **at least two takes would be needed**.  \nThis accounts for the likelihood of needing to adjust something or re-perform the action for a better result.\\n\\nTherefore, \nOption (A) is the most accurate and reasonable answer from the provided options.\\n\\n[[Correct Option]]\\n[[A]]\nHuman Error Annotation: The model accurately found the frame corresponding to the problem and perceived the \nevidence frame required for reasoning, so it is not a Lack of Visual Reasoning. However, the model made mistakes in \nthe orange part during reasoning.Options: \n(A) At least two separate takes would be needed.\n(B) At least one single take is needed.\n(C) Three separate takes are needed.\n(D) Four separate takes are needed.\n(E) Each scene can be captured in a single continuous take.\n(F) Five separate takes are needed.\n(G) Six separate takes are needed.\n(H) Eight separate takes are needed.\n(I) Ten separate takes are required.\n(J) Twenty separate takes are necessary.\n(K) At least ten separate takes are needed.\nVideo Type: TV\nTask Type L1: Explicit Reasoning.\nTask Type L2: Counterintuitive Reasoning\nTask Type L3: Magic Deconstruction\nFigure 15: Error Case: Reasoning Error.\n37\n--- Page 38 ---\nQuestion: The entire video is viewed through a black circle. What is this?Correct Case \nclaude-3-5-sonnet: Let me think about this step by step:\\n\\n1) Looking at the video, I notice the view is consistently \nshown through a circular frame with crosshairs in it.\\n\\n2) The crosshairs (reticle) are a key identifying feature - \nthey have the characteristic \\\"+\\\" shape with a circle.\\n\\n3) Throughout the video, the view is used to target/aim at \nbirds in the trees.\\n\\n4) The combination of the circular view, crosshairs, and targeting function strongly indicates \nthis is a scope used for aiming.\\n\\n5) Among the options, C mentions \\\"the scope of a gun\\\" - this matches perfectly with \nall the visual evidence.\\n\\n6) While other options like telescopes or cameras might have circular views, they typically don't \ninclude targeting crosshairs like we see here.\\n\\n7) The clear targeting/hunting context and reticle design confirms this is \nspecifically a gun scope rather than any other optical device.\\n\\n[[C]]\nHuman Analysis:This is a relatively good answer. The model's thinking process before answering the question is mainly \ncentered around the visual information of the video, rather than the entire CoT being spent analyzing the textual reasoning \nof the options.Options: \n(A) It's a circular hole in a piece of paper the viewer is looking through.\n(B) It's a decorative frame added for aesthetic purposes.\n(C) It is the scope of a gun.\n(D) It is the view through a telescope.\n(E) It is a special pair of observation glasses used to observe the behavior of wild animals.\n(F) It is an astronomical telescope.\n(G) It is the lens of a telephoto DSLR camera.\n(H) It is the view through a kaleidoscope.\n(I) It's the lens of a vintage camera.\n(J) It's a magic hoop that gives special vision.\nVideo Type: Animation\nTask Type L1: Explicit Reasoning.\nTask Type L2: Causal Reasoning\nTask Type L3\": Backward Reasoning\nFigure 16: MMR-V Construction Pipeline.\n38",
  "text_length": 120700
}