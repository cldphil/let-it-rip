{
  "id": "http://arxiv.org/abs/2506.04171v1",
  "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard\n  Constraints",
  "summary": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.",
  "authors": [
    "Utkarsh Utkarsh",
    "Pengfei Cai",
    "Alan Edelman",
    "Rafael Gomez-Bombarelli",
    "Christopher Vincent Rackauckas"
  ],
  "published": "2025-06-04T17:12:37Z",
  "updated": "2025-06-04T17:12:37Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CE",
    "cs.NA",
    "math.NA"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04171v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04171v1  [cs.LG]  4 Jun 2025Physics-Constrained Flow Matching: Sampling\nGenerative Models with Hard Constraints\nUtkarsh‚àó\nMassachusetts Institute of TechnologyPengfei Cai‚àó\nMassachusetts Institute of Technology\nAlan Edelman\nMassachusetts Institute of TechnologyRafael Gomez-Bombarelli‚Ä†\nMassachusetts Institute of Technology\nChristopher Vincent Rackauckas‚Ä†\nMassachusetts Institute of Technology\nAbstract\nDeep generative models have recently been applied to physical systems governed by\npartial differential equations (PDEs), offering scalable simulation and uncertainty-\naware inference. However, enforcing physical constraints, such as conservation\nlaws (linear and nonlinear) and physical consistencies, remains challenging. Exist-\ning methods often rely on soft penalties or architectural biases that fail to guarantee\nhard constraints. In this work, we propose Physics-Constrained Flow Matching\n(PCFM ), a zero-shot inference framework that enforces arbitrary nonlinear con-\nstraints in pretrained flow-based generative models. PCFM continuously guides the\nsampling process through physics-based corrections applied to intermediate solu-\ntion states, while remaining aligned with the learned flow and satisfying physical\nconstraints. Empirically, PCFM outperforms both unconstrained and constrained\nbaselines on a range of PDEs, including those with shocks, discontinuities, and\nsharp features, while ensuring exact constraint satisfaction at the final solution.\nOur method provides a general framework for enforcing hard constraints in both\nscientific and general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.\n1 Introduction\nDeep generative modeling provides a powerful and data-efficient framework for learning complex\ndistributions from finite samples. By estimating an unknown data distribution and enabling sample\ngeneration via latent-variable models, deep generative methods have achieved state-of-the-art per-\nformance in a wide range of domains, including image synthesis [ 1‚Äì4], natural language generation\n[5, 6], and applications in molecular modeling and materials simulation [7‚Äì9].\nInspired by these successes, researchers have begun applying generative modeling to physical\nsystems governed by Partial Differential Equations (PDEs) [ 10‚Äì13]. In these settings, generative\nmodels offer unique advantages, including efficient sampling, uncertainty quantification, and the\ncapacity to model multimodal solution distributions [ 10,12]. However, a fundamental challenge\nin this context is ensuring that generated samples respect the governing physical constraints of the\nsystem [ 12,14]. In traditional domains like vision or text, domain structure is often incorporated\nthrough soft constraints ‚Äîclassifier guidance [ 3], score conditioning [ 2], or architectural priors such\n‚àóEqual contribution. Order decided by coin toss.\n‚Ä†Corresponding authors: rafagb@mit.edu ,crackauc@mit.edu\nPreprint. Under review.\n--- Page 2 ---\nas equivariance [ 15]. Manifold-based approaches further constrain generations to lie on known\ngeometric spaces [ 16‚Äì18]. While such methods can align the model with geometric priors, they\ncannot be easily adapted for enforcing physical laws in dynamical systems.\nCrucially, constraint enforcement in generative modeling for PDEs follows a different paradigm.\nPhysical invariants such as mass, momentum, and energy [ 19,20] often arise from underlying\nsymmetries [ 21]. Prior work to incorporate physics into neural networks has largely relied on\ninductive biases in training and regression-based tasks: encoding conservation laws as soft penalties\n(e.g., Physics-Informed Neural Networks (PINNs)) [ 22], imposing architectural priors [ 23,24].\nHowever, soft constraints can lead to critical failure modes, particularly when exact constraint\nsatisfaction is essential for stability or physical plausibility [ 25‚Äì27]. To address this, recent efforts\nhave explored hard constraint enforcement, through learning conservation laws [ 28,29], constraint\nsatisfaction at inference [14, 30], and differentiable physics [31‚Äì33].\nDespite this progress, hard constraint enforcement in generative models, particularly for PDEs,\nremains a nascent area [ 12,34]. Enforcing hard constraints in generative models is particularly\nchallenging due to the inherent stochasticity of the sampling process, and the constraints must be\nsatisfied exactly in the final denoised solution but need not be preserved throughout the sampling\nprocess. DiffusionPDE [ 10] and D-Flow [ 35] propose gradient-based constraint enforcement during\nsampling, but these methods often require backpropagation through expensive PDE operators and may\nfail to exactly satisfy the target constraints. The ECI framework [ 12] introduces a novel mixing-based\ncorrection process for zero-shot constraint satisfaction, but only empirically evaluates on simple linear\nconstraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities.\nPhysics-Constrained Flow MatchingVanilla Flow MatchingFlow matching stepsùúè=0ùúè=1\nFigure 1: Evolution of generated solutions for\nthe Burgers equation using vanilla Flow Match-\ning (bottom) and our Physics-Constrained Flow\nMatching (top). Burgers‚Äô equation exhibits sharp\nshock fronts (top left in the figure), which standard\nFFM fails to capture accurately, resulting in overly\nsmoothed or smeared solutions. In contrast, PCFM\nefficiently incorporates physical constraints during\nsampling, enabling accurate shock resolution and\nphysically consistent final outputs.In this work, we introduce Physics-\nConstrained Flow Matching (PCFM) , a\nframework that bridges modern generative\nmodeling with classical ideas from numerical\nPDE solvers. PCFM enables zero-shot hard\nconstraint enforcement for pretrained flow\nmatching models, projecting intermediate flow\nstates onto constraint manifolds at inference\ntime, without requiring gradient information\nduring training. Unlike prior methods, PCFM can\nenforce arbitrary nonlinear equality constraints ,\nincluding global conservation laws, nonlinear\nresiduals, and sharp boundary conditions. It\nrequires no retraining and no architectural\nmodification, operating entirely post-hoc. While\nwe focus on PDE-constrained generation in\nthis work, PCFM provides a general framework\nfor enforcing hard constraints for flow-based\ngenerative models and may be extended to work\nfor other domains such as molecular design\nand scientific simulations beyond PDEs. To\nsummarize, we list our contributions as follows:\n1.We introduce a general framework PCFM for enforcing arbitrary and multiple physical\nconstraints inFlow Matching -based generative models. These constraints include satisfying\nconservation laws, boundary conditions, or even arbitrary non-linear constraints. Our method\nenforces these constraints as hard requirements at inference time, without modifying the\nunderlying training objective.\n2.Our approach is zero-shot : it operates directly on any pre-trained flow matching model\nwithout requiring gradient information for the constraints during training. This makes the\nmethod broadly applicable and computationally efficient, especially in scenarios where\nconstraint gradients are expensive or unavailable.\n3.We demonstrate significant improvements in generating solutions to partial differential\nequations (PDEs), outperforming state-of-the-art baselines by up to 99.5% in standard\nmetrics, such as mean squared error, while ensuring zero constraint residual .\n2\n--- Page 3 ---\nTable 1: Comparison of generation methods motivated by constraint guidance or enforcement.\nZero-shot Continuous Guidance Hard Constraint Complex Constraints\nConditional FFM [11] ‚úó ‚úì ‚úì ‚úó\nDiffusionPDE [10] ‚úì ‚úó ‚úó ‚úì\nD-Flow [35] ‚úì ‚úì ‚úó ‚úì\nECI [12] ‚úì ‚úì ‚úì ‚úó\nPCFM (Ours) ‚úì ‚úì ‚úì ‚úì\n4.We evaluate our method on challenging PDEs exhibiting shocks, discontinuities, and sharp\nspikes ‚Äîsettings in which standard flow matching models typically fail. Our approach\nimproves the accuracy of such models at inference time only , without the need for retraining\nor fine-tuning, by retrofitting physical consistency into generated samples.\n5.To enable practical deployment, we develop a custom, batched and differentiable solver\nthat projects intermediate flow states onto the constraint manifold. This solver integrates\nseamlessly with modern deep learning pipelines and enables end-to-end differentiability\nthrough the constraint enforcement mechanism.\n2 Related Work\nFlow-based Generative Models. Flow-based generative models [ 4,36,37] have emerged as a\nscalable alternative to diffusion models by defining continuous normalizing flows (CNF) using\nordinary differential equations (ODEs) [ 38,39,36] parameterized by a time-dependent vector\nfield. In flow matching models, samples from a tractable prior distribution are transported to a\ntarget distribution via a learned vector field through a simulation-free training of CNFs. Stochastic\ninterpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions,\nenabling flexible interpolations between distributions [ 37]. Furthermore, methods like rectified flows\nproposed efficient sampling with fewer integration steps by straightening the flow trajectories [ 36].\nFunctional flow matching (FFM) [ 11] and Denoising Diffusion Operator (DDO) [ 40] extend this\nparadigm to spatiotemporal data, learning flow models directly over function spaces such as partial\ndifferential equations (PDEs) solutions.\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom-\ning direction used to improve fidelity in physics-based domains [ 12,10,41]. Constraint information is\ntypically exploited via gradient backpropagation [ 42,10,41,35] and has been successively applied to\ndomains such as inverse problems [ 43‚Äì45]. Gradient backpropagation through an ODE solver can be\nprohibitively expensive for functional data such as PDEs [ 46‚Äì48]. Manifold-based flows and diffusion\nmodels [ 16,18] capture known geometric priors. However, they are not suitable for PDEs having\nwith data-driven or implicit constraints. For PDE-constrained generation, DIFFUSION PDE [10]\napplies PINN-like soft penalties during sampling, while D-F LOW [35] optimizes a noise-conditioned\nobjective. Both approaches incur a high computational cost and offer only approximate constraint\nsatisfaction. ECI [12] introduces a novel gradient-free, zero-shot, and hard constraint method on\nPDE solutions. However, its empirical evaluation is limited to simple linear and non-overlapping\nconstraints‚Äîe.g., pointwise or regional equalities‚Äîwith known closed-form projections. It lacks a\ngeneral roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve\nPDEs with shocks. Furthermore, while labeled ‚Äúgradient-free‚Äù, its reliance on analytical projections\nrestricts extensibility to nonlinear or coupled constraints, and practical enforcement still implicitly\nrelies on nonlinear optimization, which often requires gradient information [ 49]. We summarize our\ndifferences compared to generative methods motivated with hard constraints in Table 1.\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning in-\ncorporates physical laws as inductive biases in machine learning models, typically through soft\npenalties as in PINNs [ 22,50] or Neural Operators [ 23,51,52]. While effective for regression, these\nmethods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via\ndifferentiable layers [ 53,31,54], inference-time projections [ 14,30], and structured architectures\n[32,33,28]. Constraint-aware integration methods offer complementary insights. constrained neural\nODEs [ 55,56,29], differential algebraic equations [ 57,58], and geometric integrators [ 59,59]\nenforce feasibility through projection-based updates. Though underexplored in generative modeling,\nthese methods motivate principled approaches to constrained sampling. Our method combines this\nnumerical perspective with flow-based generation, enabling exact constraint enforcement without\nretraining.\n3\n--- Page 4 ---\n3 Methodology and Setup\n3.1 Problem Setup\nWe consider physical systems governed by parameterized conservation laws of the form\n‚àÇtu(x, t) +‚àá ¬∑ F œï(u(x, t)) = 0 , ‚àÄx‚àà‚Ñ¶, t‚àà[0, T], (1)\nu(x,0) = Œ±0(x), ‚àÄx‚àà‚Ñ¶, (2)\nBu(x, t) = 0 , ‚àÄx‚àà‚àÇ‚Ñ¶, t‚àà[0, T], (3)\nwhere ‚Ñ¶‚äÇRdis a bounded domain, u: ‚Ñ¶√ó[0, T] :X ‚Üí Rnis the solution field, F(u;œï)\nis a flux function parameterized by œï‚ààŒ¶,Œ±0specifies the initial condition, and Bdenotes the\nboundary operator. For a fixed PDE family and parameter set Œ¶, we define the associated solution\nsetUF:={u‚àà U:‚àÉœï‚ààŒ¶such that usatisfies (1) ‚àí(3)}representing all physically admissible\nsolutions generated by varying œï. We assume access to a pretrained generative model like FFM [ 11],\nthat approximates this solution set, e.g., via a flow-based model trained on simulated PDE solutions.\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce\non the solution u(x, t)through a constraint operator Hu(x, t) = 0 defined on a subdomain XH‚äÜ\n‚Ñ¶√ó[0, T], and let UH:={u‚àà U :Hu(x, t) = 0 for all (x, t)‚àà XH}denote the constraint-\nsatisfying solution set. Our objective is to generate samples from the intersection UF|H:=UF‚à© UH,\ni.e., functions that satisfy both the PDE and the constraint exactly. Importantly, we seek to impose H\nat inference time, without retraining the pre-trained model, thus narrowing the generative support\nfromUFtoUF|Hin a zero-shot manner.\n3.2 Generative Models via Flow-Based Dynamics\nLetUdenote the space of candidate functions u:X ‚Üí Rn, where X:= ‚Ñ¶√ó[0, T]is the\nspatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map\nœïœÑ:U √ó[0,1]‚Üí U called flow, indexed by the flow time œÑ‚àà[0,1]. A vector field vtwhich\ndefines the evolution of the flow œïœÑaccording to the ordinary differential equation:d\ndœÑœïœÑ(u) =\nvœÑ(œïœÑ(u)), œï 0(u0) =u0.This yields a continuous path of measures through a push-forward map\nœÄœÑ= (œïœÑ)#œÄ0, connecting the prior noise measure œÄ0to the predicted target measure œÄ1. Chen et al.\n[38] proposed to learn the vector field vtwith a deep neural network parameterized by Œ∏. Later in\nthis text, we will denote this parameterized vector field as vŒ∏:=vœÑ(x;Œ∏). This naturally induces\nparameterization for œïtcalled Continuous Normalizing Flows (CNFs). Lipman et al. [4]introduced a\nsimulation and likelihood free training method for CNFs called Flow Matching along with with other\nsimilar works [36, 39] (see Appendix G).\nIn the context of functional data such as PDE solutions, the Functional Flow Matching (FFM)\nframework [ 11] extends this idea to functional spaces. Instead of modeling samples in finite-\ndimensional Euclidean space, FFM learns flows between functions u0, u1‚àà U via interpolants\n[4,37,36], trained on trajectories with u0‚àºœÄ0, u1‚àºŒΩ, where ŒΩis the target probability measure\nfrom where wish to sample the PDE solutions. The vector field vŒ∏is parameterized by a Neural\nOperator [ 23,24] which is forward compatible to operate on functional spaces. The resulting\ngenerative model approximates the target measure œÄ1‚âàŒΩover the support UFthrough sufficient\ntraining (see Appendix G.2 for further details). In this work, we focus on augmenting such pre-trained\nmodels to enforce additional physical constraints at inference time, without requiring retraining.\n3.2.1 Constraint Types in PDE Systems\nConstraints in PDE-governed physical systems arise from a variety of sources, including bound-\nary conditions, conservation laws [ 20,60], and physical admissibility requirements [ 61]. These\nconstraints can be local (e.g., pointwise Dirichlet or Neumann conditions) or global (e.g., integral\nconservation of mass or energy), may be linear or nonlinear in the solution field u(x, t), and may not\nbe implicitly satisfied by the generated solution. Table 2 summarizes representative forms of com-\nmonly encountered constraints, categorized by their mathematical structure. This taxonomy guides\nthe design of appropriate enforcement mechanisms in generative modeling pipelines. A detailed\ndescription of the constraints, particularly those in our experiments, is provided in Appendix D.\n3.3 PCFM: Physics-Constrained Flow Matching\nGiven a pretrained generative flow model vŒ∏(u, œÑ), the flow dynamics define a pushforward map\nœïœÑthat transports samples u0‚àºœÄ0to solution-like outputs u1=œï1(u0)‚àºœÄ1. In our setting,\n4\n--- Page 5 ---\nTable 2: Summary of constraint types commonly encountered in PDE-based physical systems,\ncategorized by their mathematical form and scope.\nConstraint Type Representative Form Linearity\nDirichlet IC / BC Hu=Au‚àíb= 0 Linear\nGlobal mass conservation (periodic BCs) Hu=R\n‚Ñ¶u(x, t)dx‚àíC= 0 Linear\nNonlinear conservation law Hu=d\ndtR\n‚Ñ¶œÅ(u(x, t))dx= 0 Nonlinear\nNeumann or flux boundary condition Hu=‚àÇnu(x, t)‚àíg(x, t) = 0 Potentially nonlinear\nCoupled or implicit constraints Nonlinear spatial/temporal relationships Nonlinear\nAlgorithm 1 PCFM: Physics-Constrained Flow Matching\nRequire: Flow model vŒ∏(u, œÑ), constraint residual h(u), initial state u0, steps N, penalty Œª\nEnsure: Final state u1such that h(u1) = 0\n1:‚àÜœÑ‚Üê1/N,u‚Üêu0\n2:fork= 0, . . . , N ‚àí1do\n3: œÑ‚Üêk¬∑‚àÜœÑ,œÑ‚Ä≤‚ÜêœÑ+ ‚àÜœÑ\n4: u1‚ÜêODESolve( u, vŒ∏, œÑ,1, Œ∏)\n5: J‚Üê ‚àáh(u1)‚ä§\n6: uproj‚Üêu1‚àíJ‚ä§(JJ‚ä§)‚àí1h(u1)\n7: ÀÜuœÑ‚Ä≤‚ÜêODESolve( uproj,‚àí(uproj‚àíu0),1, œÑ‚Ä≤)\n8: uœÑ‚Ä≤‚Üêarg min u‚à•u‚àíÀÜuœÑ‚Ä≤‚à•2+Œª‚à•h(u+ (1‚àíœÑ‚Ä≤)vŒ∏(uœÑ‚Ä≤, œÑ‚Ä≤))‚à•2\n9:if‚à•h(u)‚à•> œµthen\n10: u‚Üêarg min u‚à•u‚àíu1‚à•2s.t.h(u) = 0\n11:return u\nconstraints are imposed on the final sample u1, via a nonlinear function Hu:=h(u1) = 0 , which we\nwish to enforce exactly during inference. Our goal is to generate samples u(œÑ)along the generative\ntrajectory such that the terminal output u1satisfies h(u1) = 0 , while remaining aligned with the\nlearned flow vŒ∏. To achieve this, we develop a constraint-guided sampling algorithm that interleaves\nlightweight constraint corrections with marginally consistent flow updates. The core procedure is\noutlined below.\nForward Shooting and Projection. At each substep œÑ‚ÜíœÑ+Œ¥œÑ, we first perform a forward\nsolve (shooting) to the final flow time œÑ= 1,u1= ODESolve\u0000\nu(œÑ), vŒ∏, œÑ,1\u0001\n,typically using\nan inexpensive integrator (e.g., Euler with large step size) since high precision is not required for\nintermediate inference. We then apply a single Gauss‚ÄìNewton projection to softly align u1to the\nconstraint manifold,\nr=h(u1), J =‚àáh(u1)‚ä§, u proj=u1‚àíJ‚ä§(JJ‚ä§)‚àí1r, (4)\nwhich shifts u1minimally to a linearized feasible point. As formalized in Proposition E.1, this\nprojection step corresponds to a projection onto the tangent space of the constraint manifold at u1,\nand recovers the exact solution when his affine.\nReverse Update via OT Interpolant. To propagate this correction back to œÑ‚Ä≤=œÑ+Œ¥œÑ, we\nconsider a reverse ODE solve uœÑ‚Ä≤= ODESolve\u0000\nu(œÑ),‚àívŒ∏,1, œÑ‚Ä≤\u0001\n. ODEs are inherently reversible in\ntheory, however in practice it may have O(1)error in the backward integration [ 62] mainly because\nof the flip of the signs of the eigenvalues of the Jacobian of vŒ∏[47]. Gholami et al. showcases this\ncan be unstable for neural ODEs with a large absolute value of the Lipschitz constant of vŒ∏[47,46].\nHowever, robust options, such as implicit or adjoint methods [ 48,63], are computationally prohibitive\nduring inference. To alleviate these issues during inference, instead, we approximate the reverse flow\nusing the Optimal Transport (OT) [4, 11] displacement interpolant used during flow matching:\nÀÜuœÑ‚Ä≤= ODESolve\u0000\nuproj,‚àí(uproj‚àíu0),1, œÑ‚Ä≤\u0001\n. (5)\nThis avoids instability and is justified by Proposition 3.1, which shows that the straight-line dis-\nplacement approximates the true marginal flow in the small-step limit. This perspective is further\nsupported by recent findings in Rectified Flows [ 36], where generative paths become increasingly\nlinear as flow resolution improves.\n5\n--- Page 6 ---\nProposition 3.1 (Reversibility under OT Displacement Interpolant) .LetvŒ∏(u, œÑ)be a pre-trained\nmarginal velocity field learned via deterministic flow matching, with pushforward map œïœÑsuch that\nu1=œï1(u0), for samples u0‚àºœÄ0andu1‚àºœÄ1. Suppose vŒ∏(u, œÑ)is Lipschitz continuous in both u\nandœÑ. Then under numerical integration with step size Œ¥œÑ, the flow satisfies\nvŒ∏(u(œÑ), œÑ) =u1‚àíu0+O(Œ¥œÑp),\nwhere pis the order of the integrator. Moreover, defining the OT displacement interpolant ¬Øv(u) :=\nu1‚àíu0, the reverse update ÀÜuœÑ‚Ä≤= ODESolve\u0000\nuproj,‚àí¬Øv(u),1, œÑ‚Ä≤\u0001\nproduces a numerically stable\nand solver-invariant approximation that is unconditionally reversible as Œ¥œÑ‚Üí0.\nRelaxed Constraint Correction. Due to discretization and the nonlinearity of h, the point ÀÜuœÑ‚Ä≤may\nstill incur residual error. We thus perform a penalized correction:\nuœÑ‚Ä≤= arg min\nu‚à•u‚àíÀÜuœÑ‚Ä≤‚à•2+Œª\r\rh(u+Œ≥ vŒ∏(u, œÑ‚Ä≤))\r\r2, Œ≥ = 1‚àíœÑ‚Ä≤, (6)\nwhich encourages constraint satisfaction at the extrapolated point while preserving local flow align-\nment. As Œ¥œÑ‚Üí0, this step aligns with the OT-based marginal objective, and offers robustness in\nsettings with coarse discretization, for e.g., necessitated by the need of lower number of function\nevaluations. We provide an ablation in Appendix L.2 highlighting its effect.\nFinal Projection and Runtime. We set uœÑ+Œ¥œÑ:=uœÑ‚Ä≤, and iterate. If constraint residuals remain\nhigh at œÑ= 1, we apply a final full projection:\nu1= arg min\nu‚Ä≤‚à•u‚Ä≤‚àíu1‚à•2subject to h(u‚Ä≤) = 0 . (7)\nTo this end, we have developed a custom, batched differentiable solver that performs these updates\nentirely in parallel: at each iteration it assembles and inverts only an m√ómSchur complement system\n(with m= dim h‚â™n), then applies a Newton-style correction to the full state (see Appendix I for\nthe specific algorithm details). Despite the extra iterations, the overall cost remains O(n)per sample,\nsince (i) each Schur solve is O(m3)withm‚â™n, (ii) the backward flow solve in Eq. (5)is a single\nEuler step, and (iii) the relaxed-penalty correction in Eq. (6)typically converges in 3‚Äì5 gradient steps.\nWe note that ECI [ 12] emerges as a special case of this framework when constraints are linear and\nnon-overlapping, and the penalty is omitted ( Œª= 0) (see Appendix F). Our method generalizes this\nby enabling strict enforcement of nonlinear and overlapping constraints while preserving generative\nconsistency.\n3.4 Reversibility and Interpolant Compatibility\nOur method naturally extends to other interpolants, including variance-preserving (VP) Score-Based\nDiffusion Models (SBDMs) [ 2,4]. While OT interpolants yield straight-line paths, VP flows benefit\nfrom bounded Lipschitz constants due to Gaussian smoothing, making them more stable in reverse\n[64]. The key requirement is access to a consistent displacement direction (e.g., u1‚àíu0), which\nenables constraint enforcement via reversible approximations regardless of the underlying interpolant.\n3.4.1 Tradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with sufficiently many flow steps, the penalty term becomes less critical,\nand the dynamics naturally stay closer to the OT path [ 36]. We present detailed ablation results in\nAppendix L.2, showing how PCFM adapts across this tradeoff.\n4 Experiments and Results\nIn this section, we highlight the key results and compare our approach with representative baselines.\nFor every problem, we construct a PDE numerical solution dataset with two degrees of freedom by\nvarying initial (IC) and boundary conditions (BC) (see Appendix B), and pre-train an unconditional\nFFM model [ 11] on this dataset (see Appendix C). To evaluate generative performance and constraint\n6\n--- Page 7 ---\nTable 3: Generative performance for zero-shot methods on constrained PDEs with linear and nonlinear\nconstraints. Heat and Navier-Stokes enforce global conservation laws (CL) as linear constraints,\nalong with initial condition (IC) constraints. In contrast, Burgers and Reaction-Diffusion apply CL as\nnonlinear constraints along with IC or BC constraints. Lower values indicate better performance, and\nbest results are highlighted in bold.\nDataset Metric PCFM ECI DiffusionPDE D-Flow FFM\nHeat EquationMMSE / 10‚àí20.241 0.697 4.49 1.97 4.56\nSMSE / 10‚àí20.937 0.973 3.93 1.14 3.51\nCE (IC) / 10‚àí20 0 599 102 579\nCE (CL) / 10‚àí20 0 2.06 64.8 2.11\nFPD 1.22 1.34 1.70 2.70 1.77\nNavier-StokesMMSE / 10‚àí24.59 5.23 17.4 ‚Äì 16.5\nSMSE / 10‚àí24.17 7.28 9.48 ‚Äì 7.90\nCE (IC) / 10‚àí20 0 288 ‚Äì 328\nCE (CL) / 10‚àí20 0 21.4 ‚Äì 18.6\nFPD 1.00 1.04 3.70 ‚Äì 2.81\nReaction-Diffusion ICMMSE / 10‚àí20.026 0.324 3.16 0.318 2.92\nSMSE / 10‚àí20.583 0.060 2.54 6.86 2.54\nCE (IC) / 10‚àí20 0 451 215 445\nCE (CL) / 10‚àí20 6.00 3.82 29.7 3.87\nFPD‚Ä†15.7 136 44.1 28.3 24.9\nBurgers BCMMSE / 10‚àí20.335 0.359 5.42 0.224 4.86\nSMSE / 10‚àí20.123 0.089 1.30 0.948 1.38\nCE (BC) / 10‚àí20 20.3 426 95.7 409\nCE (CL) / 10‚àí20 15.7 6.20 15.0 6.91\nFPD 0.292 0.307 25.9 1.44 24.7\nBurgers ICMMSE / 10‚àí20.052 10.0 14.3 9.97 13.7\nSMSE / 10‚àí20.272 6.65 8.06 7.91 7.90\nCE (IC) / 10‚àí20 0 471 397 462\nCE (CL) / 10‚àí20 205 6.22 8.66 6.91\nFPD 0.101 1.31 35.8 22.1 33.5\n‚Ä†Pretrained Poseidon requires spatially square inputs; we bilinearly interpolated solution grids to 128 √ó128,\nwhich may introduce artifacts in FPD evaluation. D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of PDEs with different constraint requirements, ranging from easier linear constraints\nto harder nonlinear constraints, we evaluate different sampling methods using the same pretrained\nFFM model. The specific linear and nonlinear constraints used in our PCFM approach are outlined\nin Appendix D, while the parameters and set-up for other methods are outlined in Appendix H. To\nevaluate the similarity between the ground truth and generated solution distributions, we follow\nKerrigan et al. [11] and Cheng et al. [12] to compute the pointwise mean squared errors of the\nmean ( MMSE ) and standard deviation ( SMSE ), along with the Fr ¬¥echet Poseidon Distance ( FPD ).\nThe FPD quantifies distributional similarity in feature space using a pretrained PDE foundation\nmodel (Poseidon) [ 65], analogous to Fr ¬¥echet Inception Distance (FID) used in image generation [ 66].\nImportantly, we evaluate the constraint errors by taking the ‚Ñì2norm of the residuals for initial\ncondition (IC), boundary condition (BC), and global mass conservation (CL) over time t, averaged\nacross all Ngenerated samples: CE(‚àó) =1\nNPN\nn=1\r\rR‚àó\u0000\nÀÜu(n)\u0001\r\r\n2,where ‚àó ‚àà { IC,BC,CL}\n7\n--- Page 8 ---\nGenerated solution meanMass residual meanùë•ùë°\nFigure 2: Comparison of mean ¬±1 std. of mass residuals across samples. generated solutions and\nmass conservation errors for the Reaction-Diffusion problem with IC fixed. By enforcing both IC\nand nonlinear mass conservation constraints, PCFM improves quality of generated solutions while\nsatisfying both constraints exactly.\n4.1 Linear Hard Constraints\nWe first focus on linear hard constraints by considering the 2-D Heat equation ( u(x, t)) and the 3-D\nNavier-Stokes ( u(x, y, t )) equation with periodic boundary conditions. The global conservation laws\nsimplify to a linear integral over the solution field, where the conserved mass quantity reduces to a\nsum or mean of the solution across the spatial domain (see Appendix D). We aim to constrain the\ngeneration to satisfy a selected IC and global mass conservation. In Table 3, PCFM outperforms all\nthe compared methods in the MMSE, SMSE, and FPD, achieving machine-level precision for hard\nconstraints on both IC and global mass conservation. Due to the simplicity of the linear constraints,\nECI is also capable of achieving constraint satisfaction in generated samples.\nPCFM further enables us to improve the performance by applying relaxed constraint corrections\n(Equation 6), achieving higher solution fidelity with fewer flow matching steps. For high-dimensional\nsettings such as Navier‚ÄìStokes, we adopt a stochastic interpolant perspective by randomizing u0\nacross batches [ 37]. This enhances alignment with the OT displacement path while preserving hard\nconstraint satisfaction at generation time.\n4.2 Nonlinear Hard Constraints\n4.2.1 Nonlinear Global and Boundary Constraints\nReaction-Diffusion (nonlinear mass & Neumann flux). We first consider a reaction-diffusion\nPDE with nonlinear mass conservation and Neumann boundary fluxes. When constrained on the\nfixed IC and nonlinear mass, PCFM achieves the lowest constraint errors and best generation fidelity\n(Table 3, Figure 2). ECI meets the IC constraint by exact value enforcement in their correction step\nbut its framework cannot enforce nonlinear mass conservation. DiffusionPDE and D-Flow, despite\nusing a combined loss function on IC and PINN-loss, likewise fail to enforce both hard constraints\nsimultaneously. In contrast, PCFM satisfies IC and mass conservation to machine precision for all t\n(Figure 2), which in turn enforces Neumann boundary fluxes and improves overall solution quality.\nBurger‚Äôs Equation (nonlinear mass & Dirichlet BCs). Constraining Dirichlet BC at x= 0and\nzero-flux Neumann BC at x= 1, generated samples should satisfy both BC and nonlinear mass\nconservation constraints for all t. PCFM achieves constraint satisfaction while matching similar\nfidelity of other methods (Figure 5).\n4.2.2 Nonlinear Local Dynamics and Shock Constraints\nWe demonstrate PCFM‚Äôs ability to tackle a more challenging task of enforcing global nonlinear\nand local dynamical constraints, achieving improved generation quality while enforcing physical\nconsistencies. Specifically, for Burgers‚Äô equation constrained on ICs, PCFM outperforms all baselines\nacross metrics while satisfying both IC and mass conservation. In addition to global constraints,\nwe incorporate 5unrolled local flux collocation points in the residual (see Appendix D for further\ndetails). Projecting intermediate u1to the constraint manifold helps capture the localized shock\n8\n--- Page 9 ---\nstructure (see top left corner of Figure 3), improving solution fidelity to match the shock dynamics in\nBurgers. In contrast, other methods cannot satisfy hard constraints nor capture the shock dynamics.\nGenerated solution meanMass residual mean\nùë•ùë°\nFigure 3: Comparison of mean generated solutions and mass conservation errors for the Burger‚Äôs\nproblem with IC fixed. By enforcing nonlinear conservation constraints via PCFM, our method\ncaptures the Burgers‚Äô shock phenomenon, ensures global mass conservation in the generated solution,\nwhile improving solution quality. Shaded bands show ¬±1 std. of mass residuals across samples.\n4.3 Enhancing Fidelity through Additional Physical Constraints\nIn our ablation study (Figure 4), we investigate the effect of imposing more constraints, specifically\nthe effect of constrained collocation points, in addition to the IC and mass conservation constraints in\nthe Burgers‚Äô problem. We show that adding more constrained collocation points improves generation\nquality without worsening the satisfaction of IC and mass constraints (see Appendix D for constraint\nset details). Interestingly, while stacking multiple constraints increases computational cost in each\nGauss-Newton projection, we find that complementary constraints (local flux, IC, and global mass)\ncan improve performance, unlike soft-constraint approaches such as PINNs, where adding more\nconstraints can degrade performance due to competing objectives [ 26,25]. However, in cases of\nconflicting constraints, tradeoffs will arise that require careful balance of their effects on generation\nfidelity and physical consistency. Indeed, the ability to chain different hard constraints on a pretrained\ngenerative model makes PCFM flexible and practical for diverse applications.\n100 200 300 400 500\nConstrained collocation points103\n102\nMMSE\n100 200 300 400 500\nConstrained collocation points102\n101\nSMSE\n100 200 300 400 500\nConstrained collocation points1010\n109\n108\nIC Residual\n100 200 300 400 500\nConstrained collocation points106\n105\n104\nMass Residual\nFlow matching steps\n10 20 50 100 200\nFigure 4: Increasing the number of constraints (constraint collocation points) can improve solution\nfidelity while maintaining strong satisfaction of other constraints (IC and global mass conservation),\ndemonstrating the ability of PCFM to handle chaining of multiple constraints.\nWe also explored total variation diminishing (TVD) constraints to promote smoother solution profiles,\ndemonstrating on the heat equation that TVD improves both smoothness and fidelity while preserving\nIC and global mass constraints (see Appendix L). This highlights PCFM‚Äôs flexibility to incorporate\nmultiple variety of constraints to enhance generative quality.\n5 Conclusion\nWe presented PCFM , a zero-shot inference framework for enforcing arbitrary nonlinear equality\nconstraints in pretrained flow-based generative models. PCFM combines flow-based integration,\ntangent-space projection, and relaxed penalty corrections to strictly enforce constraints while stay-\ning consistent with the learned generative trajectory. Our method supports both local and global\nconstraints, including conservation laws and consistency constraints, without requiring retraining or\n9\n--- Page 10 ---\narchitectural changes. Empirically, PCFM outperforms state-of-the-art baselines across diverse PDEs,\nincluding systems with shocks and nonlinear dynamics, achieving lower error and exact constraint\nsatisfaction. Notably, we find that enforcing additional complementary constraints improves gen-\neration quality, contrary to common limitations observed in soft-penalty methods such as PINNs\n[25,26]. While PCFM currently focuses on equality constraints, extending it to inequality constraints\nand leveraging structure in constraint Jacobians are promising directions for improving scalability\n[67,49]. Our work offers a principled approach for strictly enforcing physical feasibility in generative\nmodels, with broad impact on scientific simulation and design.\n6 Acknowledgments\nWe acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing\nHPC resources. We also acknowledge the Delta GPU system at the National Center for Supercomput-\ning Applications (NCSA), supported by the National Science Foundation and the ACCESS program.\nP.C. is supported by the Regenerative Energy-Efficient Manufacturing of Thermoset Polymeric\nMaterials (REMAT) Energy Frontier Research Center, funded by the U.S. Department of Energy,\nOffice of Science, Basic Energy Sciences under award DE-SC0023457.\nThis material is based upon work supported by the U.S. National Science Foundation under award Nos\nCNS-2346520, PHY-2028125, RISE-2425761, DMS-2325184, OAC-2103804, and OSI-2029670, by\nthe Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112490488,\nby the Department of Energy, National Nuclear Security Administration under Award Number DE-\nNA0003965 and by the United States Air Force Research Laboratory under Cooperative Agreement\nNumber FA8750-19-2-1000. Neither the United States Government nor any agency thereof, nor\nany of their employees, makes any warranty, express or implied, or assumes any legal liability or\nresponsibility for the accuracy, completeness, or usefulness of any information, apparatus, product,\nor process disclosed, or represents that its use would not infringe privately owned rights. Reference\nherein to any specific commercial product, process, or service by trade name, trademark, manufacturer,\nor otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring\nby the United States Government or any agency thereof. The views and opinions of authors expressed\nherein do not necessarily state or reflect those of the United States Government or any agency thereof.‚Äù\nThe views and conclusions contained in this document are those of the authors and should not be\ninterpreted as representing the official policies, either expressed or implied, of the United States Air\nForce or the U.S. Government.\nReferences\n[1]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems , 33:6840‚Äì6851, 2020.\n[2]Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456 , 2020.\n[3]Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems , 34:8780‚Äì8794, 2021.\n[4]Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow\nmatching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.\n[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877‚Äì1901, 2020.\n[6]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.\n[7]John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ÀáZ¬¥ƒ±dek, Anna Potapenko, et al.\nHighly accurate protein structure prediction with alphafold. nature , 596(7873):583‚Äì589, 2021.\n10\n--- Page 11 ---\n[8]Gabriele Corso, Hannes St ¬®ark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock:\nDiffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776 ,\n2022.\n[9]Juno Nam, Sulin Liu, Gavin Winter, KyuJung Jun, Soojung Yang, and Rafael G ¬¥omez-Bombarelli.\nFlow matching for accelerated simulation of atomic transport in materials, 2025. URL https:\n//arxiv.org/abs/2410.01464 .\n[10] Jiahe Huang, Guandao Yang, Zichen Wang, and Jeong Joon Park. Diffusionpde: Generative\npde-solving under partial observation. arXiv preprint arXiv:2406.17763 , 2024.\n[11] Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. arXiv\npreprint arXiv:2305.17209 , 2023.\n[12] C. Cheng, B. Han, D.C. Maddix, A.F. Ansari, A. Stuart, M.W. Mahoney, and Y . Wang. Gradient-\nfree generation for hard-constrained systems. In International Conference on Learning Repre-\nsentations , 2025.\n[13] Gefan Yang and Stefan Sommer. A denoising diffusion model for fluid field prediction. arXiv\npreprint arXiv:2301.11661 , 2023.\n[14] Derek Hansen, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, and Michael W. Mahoney.\nLearning physical models that can respect conservation laws. In Proceedings of the 40th\nInternational Conference on Machine Learning , volume 202, pages 12469‚Äì12510. PMLR,\n2023.\n[15] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International\nconference on machine learning , pages 2990‚Äì2999. PMLR, 2016.\n[16] Ricky TQ Chen and Yaron Lipman. Flow matching on general geometries. arXiv preprint\narXiv:2302.03660 , 2023.\n[17] Mevlana C Gemici, Danilo Rezende, and Shakir Mohamed. Normalizing flows on riemannian\nmanifolds. arXiv preprint arXiv:1611.02304 , 2016.\n[18] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. Advances\nin Neural Information Processing Systems , 33:2503‚Äì2515, 2020.\n[19] Lawrence C Evans. Partial differential equations , volume 19. American Mathematical Society,\n2022.\n[20] Randall J. LeVeque. Numerical Methods for Conservation Laws . Lectures in mathematics ETH\nZ¬®urich. Birkh ¬®auser Verlag, 1990.\n[21] Emmy Noether. Invariant variation problems. Transport theory and statistical physics , 1(3):\n186‚Äì207, 1971.\n[22] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations. Journal of Computational physics , 378:686‚Äì707, 2019.\n[23] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,\nAndrew Stuart, and Anima Anandkumar. Fourier Neural Operator for Parametric Partial\nDifferential Equations. In International Conference on Learning Representations , 2021.\n[24] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning\nnonlinear operators via DeepONet based on the universal approximation theorem of operators.\nNature Machine Intelligence , 3(3):218‚Äì229, 2021.\n[25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow\npathologies in physics-informed neural networks. SIAM Journal on Scientific Computing , 43\n(5):A3055‚ÄìA3081, 2021.\n11\n--- Page 12 ---\n[26] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney.\nCharacterizing possible failure modes in physics-informed neural networks. In Advances in\nneural information processing systems , volume 34, pages 26548‚Äì26560, 2021.\n[27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural\ntangent kernel perspective. Journal of Computational Physics , 449:110768, 2022.\n[28] Jack Richter-Powell, Yaron Lipman, and Ricky T. Q. Chen. Neural conservation laws: A\ndivergence-free perspective. arXiv preprint arXiv:2210.01741 , 2022.\n[29] Takashi Matsubara and Takaharu Yaguchi. Finde: Neural differential equations for finding and\npreserving invariant quantities. In International Conference on Learning Representations , 2023.\n[30] S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart,\nMichael W. Mahoney, and Yuyang Wang. Using Uncertainty Quantification to Character-\nize and Improve Out-of-Domain Learning for PDEs. In Proceedings of the 40th International\nConference on Machine Learning , volume 235, pages 36372‚Äì36418. PMLR, 2024.\n[31] Geoffrey N ¬¥egiar, Michael W. Mahoney, and Aditi S. Krishnapriyan. Learning differentiable\nsolvers for systems with hard constraints. In International Conference on Learning Representa-\ntions , 2023.\n[32] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In\nAdvances in neural information processing systems , volume 32, 2019.\n[33] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley\nHo. Lagrangian neural networks. arXiv preprint arXiv:2003.04630 , 2020.\n[34] Lyle Regenwetter, Giorgio Giannone, Akash Srivastava, Dan Gutfreund, and Faez Ahmed.\nConstraining generative models for engineering design with negative data. Transactions on\nMachine Learning Research , 2024.\n[35] Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow:\nDifferentiating through flows for controlled generation. arXiv preprint arXiv:2402.14017 , 2024.\n[36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate\nand transfer data with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.\n[37] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A\nunifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023.\n[38] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. Advances in neural information processing systems , 31, 2018.\n[39] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic\ninterpolants. arXiv preprint arXiv:2209.15571 , 2022.\n[40] Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Az-\nizzadenesheli, Jean Kossaifi, Vikram V oleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al.\nScore-based diffusion models in function space. arXiv preprint arXiv:2302.07400 , 2023.\n[41] Dule Shu, Zijie Li, and Amir Barati Farimani. A physics-informed diffusion model for high-\nfidelity flow field reconstruction. Journal of Computational Physics , 478:111972, 2023.\n[42] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solv-\ning inverse problems with latent diffusion models via hard data consistency. arXiv preprint\narXiv:2307.08123 , 2023.\n[43] Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. Flow-\ngrad: Controlling the output of generative odes with gradients. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 24335‚Äì24344, 2023.\n[44] Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, and Ju Sun. Dm-\nplug: A plug-in method for solving inverse problems with diffusion models. arXiv preprint\narXiv:2405.16749 , 2024.\n12\n--- Page 13 ---\n[45] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models\nfor inverse problems using manifold constraints. Advances in Neural Information Processing\nSystems , 35:25683‚Äì25696, 2022.\n[46] Amir Gholami, Kurt Keutzer, and George Biros. Anode: Unconditionally accurate memory-\nefficient gradients for neural odes. arXiv preprint arXiv:1902.10298 , 2019.\n[47] Suyong Kim, Weiqi Ji, Sili Deng, Yingbo Ma, and Christopher Rackauckas. Stiff neural ordinary\ndifferential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science , 31(9), 2021.\n[48] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit\nSupekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations\nfor scientific machine learning. arXiv preprint arXiv:2001.04385 , 2020.\n[49] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society , 48\n(3):334‚Äì334, 1997.\n[50] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu\nYang. Physics-informed machine learning. Nature Reviews Physics , 3(6):422‚Äì440, 2021.\n[51] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power\nof neural networks: A view from the width. In Advances in Neural Information Processing\nSystems , volume 30, 2017.\n[52] Pengfei Cai, Sulin Liu, Qibang Liu, Philippe Geubelle, and Rafael Gomez-Bombarelli. Towards\nlong rollout of neural operators with local attention and flow matching-inspired correction: An\nexample in frontal polymerization pdes. NeurIPS 2024 Workshop on Machine Learning and the\nPhysical Sciences, 2024.\n[53] Priya L Donti, David Rolnick, and J Zico Kolter. Dc3: A learning method for optimization with\nhard constraints. In International Conference on Learning Representations , 2021.\n[54] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico\nKolter. Differentiable convex optimization layers. In Advances in neural information processing\nsystems , volume 32, 2019.\n[55] Aaron Lou, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser Nam Lim, and\nChristopher M De Sa. Neural manifold ordinary differential equations. Advances in Neural\nInformation Processing Systems , 33:17548‚Äì17558, 2020.\n[56] Muhammad Firmansyah Kasim and Yi Heng Lim. Constants of motion network. In Advances\nin Neural Information Processing Systems , volume 35, pages 25295‚Äì25305, 2022.\n[57] Uri M Ascher and Linda R Petzold. Computer methods for ordinary differential equations and\ndifferential-algebraic equations . SIAM, 1998.\n[58] Linda Petzold. Differential/algebraic equations are not ode‚Äôs. SIAM Journal on Scientific and\nStatistical Computing , 3(3):367‚Äì384, 1982.\n[59] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical\nintegration. Oberwolfach Reports , 3(1):805‚Äì882, 2006.\n[60] Randall J LeVeque. Finite volume methods for hyperbolic problems , volume 31. Cambridge\nuniversity press, 2002.\n[61] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal\nalgorithms. Physica D: nonlinear phenomena , 60(1-4):259‚Äì268, 1992.\n[62] Gerhard Wanner and Ernst Hairer. Solving ordinary differential equations II , volume 375.\nSpringer Berlin Heidelberg New York, 1996.\n[63] Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435 , 2022.\n[64] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods.\narXiv preprint arXiv:2305.16860 , 2023.\n13\n--- Page 14 ---\n[65] Maximilian Herde, Bogdan Raoni ¬¥c, Tobias Rohner, Roger K ¬®appeli, Roberto Molinaro, Em-\nmanuel de B ¬¥ezenac, and Siddhartha Mishra. Poseidon: Efficient foundation models for pdes,\n2024. URL https://arxiv.org/abs/2405.19101 .\n[66] Jae Hyun Lim, Nikola B. Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzade-\nnesheli, Jean Kossaifi, Vikram V oleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based\ndiffusion models in function space, 2023. URL https://arxiv.org/abs/2302.07400 .\n[67] Stephen P Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press,\n2004.\n[68] Carl T Kelley. Iterative methods for linear and nonlinear equations . SIAM, 1995.\n[69] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, and Luca Antiga. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural information processing\nsystems , 32, 2019.\n[70] Utkarsh Utkarsh, Valentin Churavy, Yingbo Ma, Tim Besard, Prakitr Srisuma, Tim Gymnich,\nAdam R Gerlach, Alan Edelman, George Barbastathis, Richard D Braatz, et al. Automated\ntranslation and accelerated solving of differential equations on multiple gpu platforms. Computer\nMethods in Applied Mechanics and Engineering , 419:116591, 2024.\n[71] Kai Fukami, Koji Fukagata, and Kunihiko Taira. Super-resolution reconstruction of turbulent\nflows with machine learning. Journal of Fluid Mechanics , 870:106‚Äì120, 2019.\n[72] Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and\nNils Thuerey. Weatherbench: a benchmark data set for data-driven weather forecasting. Journal\nof Advances in Modeling Earth Systems , 12(11):e2020MS002203, 2020.\n14\n--- Page 15 ---\nA Proof of Hard Constraint Satisfaction\nWe now show that, under standard regularity assumptions, PCFM (see Algorithm 1) returns a final\nsample u1satisfying the hard constraint h(u1) = 0 to numerical precision.\nTheorem A.1 (Exact Constraint Enforcement) .Leth:Rn‚ÜíRmbe a twice continuously differen-\ntiable constraint function, and suppose the Jacobian Jh(u) :=‚àáh(u)‚ààRm√ónhas full row rank\nm‚â§nin a neighborhood of the constraint manifold M:={u‚ààRn:h(u) = 0}. Then the final\nsample u1produced by PCFM satisfies\nh(u1) = 0\nto machine precision, provided the final projection step in Algorithm 1 is solved using sufficiently\nmany Newton‚ÄìSchur iterations.\nProof. We consider the final correction step in Algorithm 1:\nu1:= arg min\nu‚Ä≤‚à•u‚Ä≤‚àíu1‚à•2s.t.h(u‚Ä≤) = 0 ,\nwhich seeks to project the current sample u1(obtained after flow integration and relaxed penalty\ncorrection) onto the feasible manifold.\nLetu(0):=u1denote the initial guess to the Newton solver. The constrained root-finding problem\nh(u) = 0 is nonlinear, but under the full-rank Jacobian assumption, the implicit function theorem\nensures a locally unique root u‚àó‚àà M nearu(0). We solve the nonlinear system using the following\nNewton‚ÄìSchur iteration:\nu(k+1)=u(k)‚àíJh(u(k))‚ä§h\nJh(u(k))Jh(u(k))‚ä§i‚àí1\nh(u(k)). (A.1)\nThis corresponds to the batched Schur-complement Newton update implemented in our differentiable\nsolver. Standard convergence theory (e.g., Theorem 10.1 in [ 68]) guarantees that, for sufficiently\nsmall‚à•h(u(0))‚à•, the iterates u(k)converge quadratically tou‚àó. Hence, for any tolerance Œµ >0, there\nexists a finite Ksuch that\n‚à•h(u(K))‚à•< Œµ.\nSince we continue iterations until ‚à•h(u(K))‚à•< Œµ tol(with Œµtoltypically set to machine precision), we\nconclude\nh(u1) =h(u(K))‚âà0.\nFinally, we verify that the initial guess u(0)=u1is within the basin of convergence . Each prior\nstep of PCFM includes: (i) a Gauss‚ÄìNewton projection, (ii) an OT-based backward solve, and (iii) a\nrelaxed penalty correction. These ensure the incoming residual ‚à•h(u1)‚à•is already small (typically\n<10‚àí3). Combined with the well-conditioned Jacobian Jh, we ensure convergence of the final\nNewton‚ÄìSchur projection.\nB PDE Dataset Details\nWe evaluate our method on four representative PDE systems commonly studied in scientific machine\nlearning: the Heat Equation ,Navier-Stokes Equation ,Reaction-Diffusion Equation , and Burgers‚Äô\nEquation . These problems cover linear and nonlinear dynamics, have smooth or discontinuous\nsolutions, and include a variety of initial and boundary condition types.\nB.1 Heat Equation\nWe consider the one-dimensional heat equation with periodic boundary conditions, following the\nsetting in Hansen et al. [14]:\nut=Œ±uxx, x‚àà[0,2œÄ], t‚àà[0,1], (8)\nwith the sinusoidal initial condition\nu(x,0) = sin( x+œÜ), (9)\n15\n--- Page 16 ---\nand periodic boundary conditions u(0, t) =u(2œÄ, t). The analytical solution is given by u(x, t) =\nexp(‚àíŒ±t) sin(x+œÜ). To pretrain our FFM model, we construct the dataset by varying the diffusion\ncoefficient Œ±‚àº U(1,5)and phase œÜ‚àº U(0, œÄ). All solutions are on a 100√ó100spatiotemporal\ngrid. During constrained sampling, we restrict to the same initial condition by fixing œÜ=œÄ/4. The\nglobal mass conservation constraint is enforced via\nZ2œÄ\n0u(x, t)dx= 0,‚àÄt‚àà[0,1]. (10)\nB.2 Navier-Stokes Equation\nWe consider the 2D incompressible Navier‚ÄìStokes (NS) equation in vorticity form with periodic\nboundary conditions, following Li et al. [23]:\n‚àÇtw(x, t) +u(x, t)¬∑ ‚àáw(x, t) =ŒΩ‚àÜw(x, t) +f(x), (11)\n‚àá ¬∑u(x, t) = 0 , (12)\nwhere w=‚àá √ó udenotes the vorticity and ŒΩ= 10‚àí3is the viscosity. The initial vorticity\nw0is sampled from a Gaussian random field, and the forcing function is defined as: f(x) =\n0.1‚àö\n2 sin\u0000\n2œÄ(x1+x2) +œï\u0001\n, œï‚àº U(0, œÄ/2). We solve the NS system using a Crank‚ÄìNicolson\nspectral solver on a 64√ó64periodic grid, recording 50 uniformly spaced temporal snapshots over\nthe interval T= 49 . For training, we generate 10000 simulations by sampling 100 random initial\nvorticities and 100 forcing phases. For test dataset, we sample an additional 10 vorticities and 100\nforces, yielding 1000 solutions. We fix to 1 initial vorticity for comparison during sampling.\nHere, we prove the global mass conservation for this NS problem set-up. Under periodic boundary\nconditions, global vorticity is conserved. Specifically, integrating the vorticity equation over the\nspatial domain ‚Ñ¶and applying the divergence theorem yields:\nd\ndtZ\n‚Ñ¶w(x, t)dx=Z\n‚Ñ¶ŒΩ‚àÜw(x, t) +f(x)‚àíu¬∑ ‚àáw(x, t)dx= 0,\nsince the divergence and Laplacian terms vanish due to periodicity and the incompressibility condition\n‚àá ¬∑u= 0. Thus, the total vorticityR\n‚Ñ¶w(x, t)dxremains constant for all t.\nB.3 Reaction-Diffusion Equation\nWe consider a nonlinear 1D reaction-diffusion equation with Neumann boundary conditions:\nut=œÅu(1‚àíu)‚àíŒΩ‚àÇxu, (13)\non the domain x‚àà[0,1], t‚àà[0,1], where (œÅ, ŒΩ) = (0 .01,0.005). The boundary fluxes at the left\nand right ends are specified as gLandgR, respectively. The initial condition u(x,0)is sampled from\na randomized combination of sinusoidal and localized bump functions (see Appendix D for full\nspecification). The training dataset is constructed by pairing 80 initial conditions with 80 boundary\nconditions, producing 6400 PDE solutions, all discretized on a (nx, nt ) = (128 ,100) space-time\ngrid. We use a semi-implicit finite difference solver with CFL-controlled time stepping. During\nconstrained sampling, we fix one initial condition and vary the boundary fluxes to enforce constraint\nsatisfaction.\nAs this system involves nonlinear source and flux terms, global conservation is no longer trivial.\nIntegrating both sides of the PDE over the domain yields:\nd\ndtZ1\n0u(x, t)dx=œÅZ1\n0u(1‚àíu)dx+gL(t)‚àígR(t), (14)\nwhich depends nonlinearly on the state u(x, t)and boundary fluxes gL, gR. Thus, conservation must\nbe tracked through both the nonlinear reaction term and boundary-driven transport, making hard\nconstraint enforcement via our framework necessary.\nB.4 Inviscid Burgers‚Äô Equation\nWe consider the 1D inviscid Burgers‚Äô equation with mixed Dirichlet and Neumann boundary condi-\ntions:\nut+1\n2(u2)x= 0, x‚àà[0,1], t‚àà[0,1], (15)\n16\n--- Page 17 ---\nwith initial condition defined as a smoothed step function centered at a random location ploc‚àº\nU(0.2,0.8):\nu(x,0) =1\n1 + exp\u0000x‚àíploc\nœµ\u0001, œµ= 0.02. (16)\nWe apply a Dirichlet condition on the left, u(0, t) =ubcwithubc‚àº U(0,1), and a Neumann\ncondition (zero-gradient) on the right. Solutions are computed using a Godunov finite volume method\non a(nx, nt ) = (101 ,101) grid. For pretraining the FFM model, we vary both initial and boundary\nconditions, generating 80 variants each to create 6400 solutions in the training set. For constrained\nsampling, we use two configurations: (1) fixing the initial condition and varying the boundary\ncondition, and (2) fixing the boundary condition and varying the initial condition.\nThe conservation constraint is nonlinear and particularly sensitive due to shock formation. Integrating\nthe PDE yields\nd\ndtZ1\n0u(x, t)dx=‚àí\u00021\n2u(1, t)2‚àí1\n2u(0, t)2\u0003\n,\nso global conservation requires u(1, t)2=u(0, t)2, making the constraint highly sensitive to\nboundary-induced discontinuities.\nC FFM Model Pretraining\nFor all PDE benchmarks, we employ the functional flow matching (FFM) [ 11] framework and\nparameterize the underlying time-dependent vector field with a Fourier neural operator (FNO)\nbackbone [ 23]. The FNO encoder takes the input the concatenation of the current state uœÑ, a sinusoidal\npositional embedding for the spatial grid and Fourier time embedding. For each benchmark, we adopt\nthe following training scheme unless otherwise specified:\n‚Ä¢Optimizer: Adam optimizer with learning rate 3√ó10‚àí4,Œ≤1= 0.9,Œ≤2= 0.999, and no\nweight decay.\n‚Ä¢Learning rate scheduler: Reduce-on-plateau scheduler with a factor of 0.5, patience of 10\nvalidation steps, and a minimum learning rate of 1√ó10‚àí4.\n‚Ä¢Batch size: 256for 1D problems (Heat, Reaction-Diffusion, Burgers) and 24for the 2D\nNavier-Stokes problem.\nFor Heat, we follow Cheng et al. [12] to train on 5000 analytical solutions of the heat equation. For\nReaction-Diffusion and Burgers, we train each model on 6400 numerical PDE solutions. For these 3\nproblems, the FNO is configured with 4Fourier layers with 32Fourier modes for both spatial and\ntemporal dimensions, 64hidden channels, and 256projection channels. For Navier-Stokes equation,\nwe train a 3D FFM on 10,000numerical solutions. The FNO has 2Fourier layers with 16Fourier\nmodes per dimension, 32hidden channels, and 256projection channels, following Cheng et al. [12].\nFFM models for Heat, Reaction-Diffusion, and Burgers are each trained over 20,000steps on 1\nNVIDIA V100 GPU and the Navier-Stokes model is trained for 500,000steps on 4 NVIDIA A100\nGPUs.\nFollowing Kerrigan et al. [11], all FFM models use a Gaussian prior noise P(u0). For all 1D\nproblems, we adopt a Matern Gaussian Process prior with smoothness parameter ŒΩ= 0.5, kernel\nlength 0.001, and variance 1.0, implemented using GPyTorch . For Navier-Stokes (2D), we adopt\nstandard Gaussian white noise as the prior, P(u0) =N(0, I). Importantly, PCFM is applied post-hoc\nat inference time, allowing us to reuse the pretrained unconditional FFM models across constraint\nconfigurations.\nD Constraints Enforcement via PCFM\nWe focus on physical and geometric constraints of the form Hu= 0, which must be satisfied\nas equality conditions on the solution field u. First, we describe common equality constraints\nencountered in solving PDEs. Next, we describe specific constraint setup for our PDE datasets.\n17\n--- Page 18 ---\nD.1 Linear Equality Constraints\nDirichlet Initial and Boundary Conditions Constraints such as u(x,0) = Œ±0(x)oru(x, t) =\ng(x, t)on‚àÇ‚Ñ¶√ó[0, T]define fixed-value conditions that are linear in u. These can be encoded in the\ngeneral form\nHu=Au‚àíb= 0, (17)\nwhere Ais a sampling or interpolation operator applied to u, and bis the prescribed target value.\nLinear Global Conservation Laws (Periodic Systems) In systems with periodic boundary condi-\ntions, additive invariants such as total mass are often linearly conserved. A representative example\nis\nHu=Z\n‚Ñ¶u(x, t)dx‚àíC= 0, (18)\nwhere C‚ààRis the conserved quantity. Such constraints can be exactly enforced via analytically\nderived projections [14], geometric integrators [59], and numerical optimization [67, 49].\nD.2 Nonlinear Equality Constraints\nMany physical systems exhibit global conservation laws where the conserved quantity depends\nnonlinearly on u. A representative form is:\nHu=d\ndtZ\n‚Ñ¶œÅ(u(x, t))dx= 0, (19)\nwhere œÅ(u)denotes a nonlinear density, such as total energy or entropy. While such constraints\nare often not conserved exactly by standard numerical integrators, structure-preserving methods,\nsuch as symplectic or variational integrators [ 59], can conserve invariants approximately over long\ntime horizons. In our setting, however, we seek to enforce such nonlinear conservation laws exactly\nat inference time and moreover only at the final denoised sample. Thus, we rely on applying\niterative projection methods such as Newton‚Äôs method for constrained least squares [ 67] to satisfy\nEquation (19) up to numerical tolerance.\nNeumann-Type Boundary Constraints Boundary constraints involving fluxes, such as\nHu=‚àÇnu(x, t)‚àíg(x, t) = 0 , (20)\nare typically linear under standard semi-discretizations, as the derivative operator is linear. However,\nthese constraints can become effectively nonlinear in practice when implemented with upwinding or\nother nonlinear flux-discretization schemes, particularly in hyperbolic PDEs [ 60]. Additionally, if\nthe prescribed flux gdepends nonlinearly on u, the constraint becomes explicitly nonlinear. In both\ncases, we handle them using differentiable correction procedures or projection-based updates within\nour framework.\nIn our PCFM framework, the residual operator H(u)specifies the equality constraints to be enforced\non the generated solution u. We enforce these constraints on the generated solution state in our PCFM\nprojection operator, ensuring that the final solution strictly satisfies the required physical constraints\nwithout retraining the underlying flow model. We summarize the specific constraint formulations\nused for each PDE task below.\nD.3 Heat Equation (IC and Mass Conservation)\nThe residual is a concatenation of two linear constraints:\nHHeat(u) =\u0014\nu(x, t= 0)‚àíuIC(x)R\nu(x, t)dx‚àíR\nu(x, t= 0)dx\u0015\n(21)\nwhere the first term enforces the initial condition and the second term ensures mass conservation over\ntime.\n18\n--- Page 19 ---\nD.4 Navier-Stokes Equation (IC and Mass Conservation)\nFor the 2D Navier-Stokes problem, we impose the initial vorticity and the global mass conservation\nconstraint over the spatial domain:\nHNS(u) =\"u(x, y, t = 0)‚àíuIC(x, y)ZZ\nu(x, y, t )dx dy‚àíZZ\nu(x, y, t = 0)dx dy#\n(22)\nwhere u(x, y, t )represents the vorticity field over spatial coordinates (x, y)and time t. The mass\nconservation term enforces that the total vorticity integrated over the spatial domain remains consistent\nwith the initial condition over all time steps.\nD.5 Reaction-Diffusion (IC and Nonlinear Mass Conservation)\nWe enforce both the initial condition and nonlinear mass conservation which accounts for reaction\nand boundary flux terms:\nHRD(u) =\"u(x, t= 0)‚àíuIC(x)\nm(t)‚àí\u0010\nm(0) +Rt\n0œÅ(u)dt+Rt\n0(gL‚àígR)dt\u0011#\n(23)\nwhere m(t) =R\nu(x, t)dxis the total mass, œÅ(u) =œÅu(1‚àíu)is the nonlinear reaction source term,\nandgL,gRare the Neumann boundary fluxes.\nD.6 Burgers (BC and Mass Conservation)\nWe enforce both the boundary conditions (Dirichlet and Neumann) and nonlinear mass conservation:\nHBurgers-BC (u) =\"u(x= 0, t)‚àíuL\nu(x=‚àí1, t)‚àíu(x=‚àí2, t)R\nu(x, t)dx‚àíR\nu(x, t= 0)dx#\n(24)\nwhere uLis the Dirichlet boundary value at the left boundary, and the Neumann BC enforces a\nzero-gradient at the right boundary.\nD.7 Burgers (IC, Mass Conservation, and Local Flux)\nWe impose three complementary constraints for the Burgers equation: (i) the initial condition, (ii)\nglobal nonlinear mass conservation, and (iii) a sequence of local conservation updates based on\nGodunov‚Äôs flux method. Specifically, the residual is formulated as:\nHBurgers-IC (u) =Ô£Æ\nÔ£ØÔ£ØÔ£∞u(x, t= 0)‚àíuIC(x)\nR(k)\nFlux(u)Z\nu(x, t)dx‚àíZ\nu(x, t= 0)dxÔ£π\nÔ£∫Ô£∫Ô£ª(25)\nwhere R(k)\nFlux(u)applies kunrolled discrete updates (collocation points) based on Godunov flux:\nF(uL, uR) =Ô£±\nÔ£≤\nÔ£≥min\u00001\n2u2\nL,1\n2u2\nR\u0001\nifuL‚â§uR\n1\n2u2\nL ifuL> uRanduL+uR\n2>0\n1\n2u2\nR ifuL> uRanduL+uR\n2‚â§0(26)\nWe apply k‚àà {1, . . . , 5}unrolled steps to incrementally enforce local conservation dynamics\nalongside global mass conservation and initial condition satisfaction.\nE Details of the PCFM\nWe provide additional theoretical context for the PCFM algorithm. In Proposition E.1, we formalize\nthe projection step as a tangent-space update in Hilbert spaces and show that it corresponds to an\northogonal projection onto the linearized constraint manifold, justifying its use for enforcing both\nlinear and nonlinear constraints.\n19\n--- Page 20 ---\nProposition E.1 (Tangent-Space Projection in Hilbert Spaces) .LetHbe a real Hilbert space and let\nh:H ‚ÜíRmbe a Fr ¬¥echet-differentiable constraint operator. Consider a point u1‚àà H, and define\nthe Jacobian J:=Dh(u1)‚àà L(H,Rm), the bounded linear operator representing the Fr ¬¥echet\nderivative.\nThen, the update\nuproj=u1‚àíJ‚àó(JJ‚àó)‚àí1h(u1)\nis the unique solution to the constrained minimization problem\nmin\nu‚ààH‚à•u‚àíu1‚à•2\nHsubject to h(u1) +J(u‚àíu1) = 0 ,\nand corresponds to the orthogonal projection of u1onto the affine subspace defined by the lineariza-\ntion of hatu1, i.e., the tangent space to the constraint manifold at u1.\nIn the special case where h(u) =Au‚àíbis affine, with A‚àà L(H,Rm), the update becomes the\nexact projection onto the feasible set {u‚àà H:Au=b}.\nF Connection to ECI as a Special Case\nThe ECI (Extrapolate‚ÄìCorrect‚ÄìInterpolate) framework [ 12] only considers and showcases empirical\nresults on constraints such as fixed initial conditions, Dirichlet boundary values, and global mass\nintegrals. These are all linear and non-overlapping, and are applied at isolated subsets of the solution\nfield. To be used with the ECI framework, such constraints must admit closed-form oblique projections\nand act independently across disjoint regions. Let us denote any value or regional constraint [ 12] as a\nlinear constraint h(u) =Au‚àíb, for some matrix A‚ààRm√ón, vector b‚ààRm, and for simplicity and\ncomparison purposes, the ECI hyperparameters such as re-sampling is set to its default value (0) and\nmixing to be 1.\nIn this regime, our Gauss‚ÄìNewton update during PCFM guidance simplifies to an exact constraint\nsatisfaction step for linear constraints:\nuproj=u1‚àíA‚ä§(AA‚ä§)‚àí1(Au1‚àíb),\nwhich is precisely the ‚Äùcorrection‚Äù step ECI employs. Moreover, by disabling the relaxed penalty\n(Œª= 0) and skipping intermediate corrections, PCFM reduces exactly to ECI.\nHence, ECI is a special case of PCFM, tailored to simple linear constraints, while PCFM provides\na general and principled framework for enforcing arbitrary nonlinear equality constraints during\ngenerative sampling.\nG Pre-trained Models with Functional Flow Matching\nG.1 Flow Matching\nFlow Matching (FM) [ 4,36,39] formulates generative modeling as learning a time-dependent velocity\nfieldvŒ∏(u, œÑ)that transports a prior sample u0‚àºœÄ0to a target sample u1‚àºœÄ1. The induced flow\nœïœÑsatisfies the ODE\nd\ndœÑœïœÑ(u) =vŒ∏(œïœÑ(u), œÑ), œï 0(u0) =u0,\nand defines a trajectory u(œÑ) =œïœÑ(u0)connecting œÄ0toœÄ1. The learning objective minimizes the\nsquared deviation between the model field and a known target field ÀÜv, evaluated along interpolants\nuœÑ= (1‚àíœÑ)u0+œÑu1. This yields the standard Flow Matching loss:\nLFM(Œ∏) =EœÑ‚àºU[0,1], u0‚àºœÄ0, u1‚àºœÄ1\u0002\n‚à•vŒ∏(uœÑ, œÑ)‚àíÀÜv(uœÑ, œÑ)‚à•2\u0003\n, (27)\nwhere ÀÜv(uœÑ, œÑ)is the true vector field, which is typically unknown.\nConditional Flow Matching (CFM). When the conditional velocity field ut(u1) :=‚àÇœÑœàœÑ(u1)is\nknown in closed form, as under optimal transport, one can minimize the conditional flow matching\nloss:\nLCFM(Œ∏) =EœÑ,u0,u1\u0002\n‚à•vŒ∏(uœÑ, œÑ)‚àí(u1‚àíu0)‚à•2\u0003\n, (28)\nwhere uœÑ= (1‚àíœÑ)u0+œÑu1is a linear interpolant. This CFM objective forms the basis for most\nrecent flow-based generative models due to its tractability and effectiveness.\n20\n--- Page 21 ---\nG.2 Functional Flow Matching\nTo handle infinite-dimensional generative tasks such as PDE solutions, Functional Flow Matching\n(FFM) [11] extends CFM to Hilbert spaces Uof functions u:X ‚ÜíR. The FFM flow œàœÑsatisfies\nd\ndœÑœàœÑ(u) =vŒ∏(œàœÑ(u), œÑ), u 0‚àº¬µ0,\nwhere ¬µ0is a base measure over function space. Under regularity assumptions, FFM minimizes an\nanalogous loss over the path of measures:\nLFFM(Œ∏) =EœÑ,u0,u1h\n‚à•vŒ∏(uœÑ, œÑ)‚àí(u1‚àíu0)‚à•2\nL2(X)i\n, (29)\nwhere uœÑ= (1‚àíœÑ)u0+œÑu1and the L2-norm is used to evaluate function-valued velocities over\nthe domain X. This loss generalizes CFM to the functional setting, and serves as the pretraining\nobjective for PCFM in this work.\nH Sampling Setups for PCFM and Other Baselines\nFor all comparisons, we adopt the explicit Euler integration scheme unless otherwise specified.\nWe use 100Euler update (flow matching) steps for the simpler heat problem and 200steps for\nNavier-Stokes, Reaction-Diffusion, and Burgers (IC or BC).\nVanilla FFM. We perform unconstrained generation by integrating the learned flow vector field\nusing explicit Euler steps, following the standard flow matching procedure [ 4,11]. At each step, the\nmodel applies the forward update ut+‚àÜt=ut+ ‚àÜt vŒ∏(t, ut)without any constraint enforcement.\nPCFM (Ours). PCFM performs explicit Euler integration combined with constraint correction at\nevery step. We apply 1Newton update to project intermediate states onto the constraint manifold.\nAdditionally, we optionally apply guided interpolation with Œª= 1.0, step size 0.01, and20refinement\nsteps to further refine interpolated states, although we use Œª= 0by default (no interpolation guidance)\nfor computational efficiency (see Appendix L.2 for further ablations on Œª). Interpolation guidance is\nonly applied for the heat equation case to obtain results in Table 3. For Navier-Stokes, we follow the\nstochastic interpolant idea and randomize noise over batches [ 37] (i.e., different noise samples over\nbatches). Finally, after all flow matching steps, we apply a final projection on the solution to enforce\nthe constraints.\nECI. We follow the ECI sampling procedure introduced by Cheng et al. [12] where it performs\niterative extrapolation-correction-interpolation steps as well as several mixing and noise resampling\nsteps throughout the trajectory. For the simpler heat equation case, we did not do mixing and noise\nresampling. For more challenging PDEs (Navier-Stokes, Reaction-Diffusion, and Burgers), we apply\nnmix= 5mixing updates per Euler step, and resample the Gaussian prior every 5steps. We adopt\ntheir value enforcements in IC and Dirichlet BC cases, and also constant mass integral enforcement\nin the heat and Navier-Stokes problems, where the periodic boundary conditions lead to trivial linear\nconstraints.\nD-Flow. Following Ben-Hamu et al. [35], D-Flow differentiates through an unrolled Euler integra-\ntion process. Following their paper and the set-up in Cheng et al. [12], we optimize the initial noise\nvia LBFGS with 20iterations and a learning rate of 1.0. The optimization minimizes the constraint\nviolation at the final state, requiring gradient computation through the full unrolled trajectory. We\nadopt the adjoint method shipped from torchdiffeq [38] to differentiate through the ODE solver.\nDepending on the problem, we adopt an IC or BC loss as well as a PINN-loss based on the differential\nform of the PDE to form the constraint loss function to be optimized. We use 10iterations and a\nlearning rate of 10‚àí2for the Navier-Stokes dataset to avoid NaNs in the optimization loop.\nDiffusionPDE. We adopt a gradient-guided sampling method proposed by Huang et al. [10] on the\nsame pretrained FFM model. The generation process is augmented with explicit constraint correction\nwhere a composite loss of IC/BC and PINN [ 22] (differential form of the PDE), like in the D-Flow\nset-up, is used to guide the vector field at each flow step. We apply a correction coefficient Œ∑= 1.0\nand set the PINN loss weight to 10‚àí2for stable generation. We find that a higher PINN weight or\nhigher learning rate lead to unstable generation.\n21\n--- Page 22 ---\nI Batched Differentiable Solver for Nonlinear Constraints\nWe describe our solver for projecting a predicted sample u1‚ààRnonto the nonlinear constraint\nmanifold M:={u‚ààRn:h(u) = 0}, where h:Rn‚ÜíRm. The projection is formulated as the\nconstrained optimization problem:\nmin\nu‚ààRn1\n2‚à•u‚àíu1‚à•2subject to h(u) = 0 . (30)\nThe corresponding Lagrangian is:\nL(u, Œª) =1\n2‚à•u‚àíu1‚à•2+Œª‚ä§h(u),\nwith first-order optimality conditions:\n‚àáuL(u, Œª) =u‚àíu1+J(u)‚ä§Œª= 0, h (u) = 0 ,\nwhere J(u) :=‚àáh(u)‚ààRm√ónis the constraint Jacobian.\nThese yield the full nonlinear KKT system:\n\u001au‚àíu1+J(u)‚ä§Œª= 0,\nh(u) = 0 .(31)\nNewton-Based Update. At iteration k, we linearize the KKT system around u(k), Œª(k)and solve\nfor updates Œ¥u, Œ¥Œª . The full Newton step solves:\n\u0014\nI+Pm\ni=1Œª(k)\ni‚àá2hi(u(k))J(u(k))‚ä§\nJ(u(k)) 0\u0015\u0014\nŒ¥u\nŒ¥Œª\u0015\n=‚àí\u0014\nu(k)‚àíu1+J(u(k))‚ä§Œª(k)\nh(u(k))\u0015\n. (32)\nHere, the upper-left block contains the Hessian of the Lagrangian:\n‚àá2\nuuL(u(k), Œª(k)) =I+mX\ni=1Œª(k)\ni‚àá2hi(u(k)).\nAfter solving, we update the primal and dual variables:\nu(k+1)=u(k)+Œ¥u, Œª(k+1)=Œª(k)+Œ¥Œª.\nThis Newton system converges quadratically under standard regularity assumptions (e.g., full-rank\nJacobian and Lipschitz-continuous second derivatives). In practice, we often omit the second-order\nterms to yield a Gauss‚ÄìNewton approximation that is more stable and efficient in high-dimensional\nsettings.\nApproximate KKT Solve via Schur Complement. For inference-time projection, we adopt a\nsimplified and batched update using the Schur complement [ 67]. At each iteration, we set Œª= 0, and\nsolve only for the primal update. Eliminating Œ¥ufrom Equation (32), we obtain:\n\u0000\nJJ‚ä§\u0001\nŒª=h(u), Œ¥u =‚àíJ‚ä§Œª. (33)\nThis gives the Gauss‚ÄìNewton-style update:\nu‚Üêu‚àíJ‚ä§(JJ‚ä§)‚àí1h(u). (34)\nWe iterate this procedure until convergence or until the constraint residual ‚à•h(u)‚à•falls below a set\ntolerance. The matrix JJ‚ä§‚ààRm√ómis small and typically well-conditioned for local or sparse\nconstraints, enabling efficient solves.\nRestarting the dual variable with Œª= 0 at each iteration avoids stale gradient accumulation and\nimproves numerical stability. This leads to a robust and memory-efficient projection routine that\nsupports batched execution and reverse-mode differentiation.\n22\n--- Page 23 ---\nBatched and Differentiable Implementation. We implement the solver in a batched and differ-\nentiable fashion to support inference across samples. For a batch of inputs {ui\n1}B\ni=1, we evaluate\nJacobians Ji, residuals h(ui), and solve the corresponding Schur systems in parallel using vector-\nized operations and autodiff-compatible backends (e.g., PyTorch with batched Cholesky or linear\nsolvers) [ 69]. Potentially, specific GPU kernels can be built for ODE integration to ensure optimal\nperformance [38, 70].\nComputational Complexity. The per-sample cost includes:\n‚Ä¢O(m2n)for computing JandJ‚ä§,\n‚Ä¢O(m3)for solving the Schur complement system,\n‚Ä¢O(n)for applying the update.\nSince typically m‚â™n, the overall cost scales as O(n)per sample.\nJ Evaluation Metrics\nWe evaluate each method using 512generated samples for all 1D problems and 100samples for the\n2D Navier-Stokes problem. For each setting, we use an equivalent number of ground truth PDE\nsolutions with a fixed IC or BC configuration used during sampling for direct comparison.\nMMSE and SMSE. Following Kerrigan et al. [11], Cheng et al. [12], we evaluate generation\nfidelity with mean of the MSE and the standard deviation of the MSE as:\nMMSE =‚à•¬µgen‚àí¬µgt‚à•2\n2,SMSE =‚à•œÉgen‚àíœÉgt‚à•2\n2,\nwhere ¬µgen,¬µgtdenote the mean across generated and true PDE solutions, and œÉgen, œÉgtthe standard\ndeviations.\nConstraint Error. To evaluate physical consistency, we compute the ‚Ñì2norm of residuals from\nconstraint functions R‚àóapplied to each sample ÀÜu(n), then average across all Nsamples:\nCE(‚àó) =1\nNNX\nn=1\r\r\rR‚àó\u0010\nÀÜu(n)\u0011\r\r\r\n2,‚àó ‚àà { IC,BC,CL}.\nwhere the residuals are computed following Appendix D to measure the constraint or conservation\nviolation.\nFr¬¥echet Poseidon Distance (FPD). To assess distributional similarity beyond mean and variance,\nwe adopt the Fr ¬¥echet Poseidon Distance (FPD) introduced in Cheng et al. [12], where we measure\nthe Fr ¬¥echet distance between the hidden state distributions extracted from a pretrained foundation\nmodel (Poseidon [ 65]) applied to both generated and true solutions. We pass both generated and true\nsolutions through the Poseidon base model (157M parameters) and extract the last hidden activations\nof the encoder to eventually obtain a 784-dimension feature vector for FPD calculation:\nFPD2=‚à•¬µ1‚àí¬µ2‚à•2+ Tr\u0010\nŒ£1+ Œ£ 2‚àí2 (Œ£ 1Œ£2)1/2\u0011\n, (35)\nwhere ¬µ1,Œ£1and¬µ2,Œ£2are the empirical mean and covariance of the Poseidon embeddings from\nthe generated and true solutions‚Äô distributions, respectively. FPD is computed either per frame u(x, y)\nfor 2D PDEs and averaged over all frames (across t) or over the full spatiotemporal solution u(x, t)\nfor 1D PDEs.\n23\n--- Page 24 ---\nK Further Results\nùë•ùë°Generated solution std dev\nGenerated solution meanùë•ùë°\nMass residual meanBC residual mean\nFigure 5: Solution profiles for the Inviscid Burgers equation with fixed BC. We plot the various\nconstraint guidance methods and compare the mean solution profile and standard deviation. While\nPCFM yields slightly worse MMSE and SMSE and better FPD, it ensures global mass conservation\nand maintains low constraint errors for both Dirichlet and Neumann BCs over time.\nGenerated solution meanùë•ùë°ùë•ùë°Generated solution std dev\nFigure 6: Solution profiles for the Heat equation with fixed IC. We plot the various constraint guidance\nmethods and compare the mean solution profile and standard deviation. PCFM outperforms all other\nmethods by visually being the most similar to the ground truth.\n24\n--- Page 25 ---\nùë•ùë°Generated solution meanGenerated solution std dev\nFigure 7: Alternative view of the Reaction-Diffusion equation with fixed IC. We plot the pointwise\nmean and standard deviation across generated samples for each method. PCFM outperforms all other\nmethods by visually being the most similar to the ground truth.\nùë•ùë°Generated solution meanGenerated solution std dev\nFigure 8: Alternative view of the Inviscid Burgers equation with fixed IC. We plot the pointwise\nmean and standard deviation across generated samples for each method. PCFM outperforms all other\nmethods by visually being the most similar to the ground truth.\nL Further Ablations\nL.1 Total Variation (TV) Constraints on the Heat Equation\nTotal Variation Diminishing (TVD) constraints encode the principle that diffusive systems, such as\nthose governed by the heat equation, smooth spatial fluctuations over time. Mathematically, this is\nexpressed by the fact that the total variation of the solution should not increase in time:\nTV(u(t)) :=Z\f\f\f\f‚àÇu\n‚àÇx\f\f\f\fdx is non-increasing in t.\nTo enforce this property in a data-driven or generative setting, we adopt a hard constraint that relates\nthe total variation at final time Tto the total variation at the initial time t= 0. Specifically, we\nimpose the condition:\nTV(uT) =Œ≥¬∑TV(u0),\nwhere Œ≥‚àà(0,1)is a decay factor that can either be fixed (e.g., Œ≥= 0.5) or estimated from unprojected\nsamples. This constraint can be implemented using discrete spatial differences as:\nTV(uj)‚âànx‚àí2X\ni=0|ui+1,j‚àíui,j|,\napplied at selected time slices (typically t= 0andt=T).\n25\n--- Page 26 ---\nTable 4: Test metrics on the Heat Equation dataset with TVD constraints. Lower is better for all\nmetrics.\nMetric PCFM ECI DiffusionPDE D-Flow FFM\nMMSE / 10‚àí20.684 0.697 4.49 1.97 4.56\nSMSE / 10‚àí20.962 0.973 3.93 1.14 3.51\nCE (IC) / 10‚àí20 0 599 102 579\nCE (CL) / 10‚àí20 0 2.06 64.8 2.11\nFPD 1.28 1.34 1.70 2.70 1.77\nPower MSE 488.42 500.12 1420.92 3996.11 932.20\nWe encode the constraint as a differentiable function:\nhTV(u) := TV(uT)‚àíŒ≥¬∑TV(u0) = 0 .\nThis constraint is compatible with mass conservation and initial condition enforcement, and can be\ncombined with other physically informed conditions such as energy decay or curvature regularity. In\npractice, we observe that enforcing TVD constraints reduces spurious oscillations in the generative\noutput and improves structural fidelity without significantly altering statistical accuracy.\nPower MSE. To further quantify the effect of this constraint, we introduce the Power MSE metric,\nwhich measures the deviation in spatial frequency content between the generated and ground truth\nsolutions. Formally, for a model‚Äôs mean output ¬Øu(x, t), we compute the spatial power spectrum\n|ÀÜu(k, t)|2and average over time to obtain a per-frequency energy profile. The Power MSE is then\ndefined as:\nPower MSE :=1\nnxX\nk\u0010\n|ÀÜugen(k)|2‚àí|ÀÜuref(k)|2\u00112\n.\nThis metric captures discrepancies in frequency modes and is especially sensitive to unphysical\nsharpness or over-smoothing. Variants of power-spectrum-based error metrics have been used in\nturbulence modeling [ 71], climate downscaling [ 72], and Fourier neural operators training [ 23]. In\nour setting, we find that Power MSE effectively complements pointwise metrics (e.g., MMSE, CE),\nas it evaluates structural fidelity in the spectral domain. As shown in Table 4, this metric reveals\nhow enforcing a TVD constraint not only improves calibration but also leads to closer alignment in\nfrequency space.\nEffect of TVD Constraint. To isolate the benefit of total variation control, we mirror the ECI\nsetting by setting the constraint penalty weight to zero and enforcing only initial condition and mass\nconservation constraints. When we additionally include the TVD constraint ( Œ≥= 0.3) in our PCFM\nmodel (with no penalty-based regularization), we observe a consistent improvement in structural\nfidelity. This is most evident in the Power MSE metric, where PCFM achieves the lowest error among\nall baselines (488.42 vs. 500.12 for ECI), indicating better alignment with the true spatial energy\ndistribution. We also see gains in MMSE and SMSE, without introducing constraint violations (e.g.,\nCE remains zero). This confirms that the TVD constraint acts as a meaningful structural prior even\nin the absence of learned penalties, improving generative realism without sacrificing calibration or\nefficiency.\nL.2 Effect of Relaxed Constraint Correction with Flow Matching Steps\nFigure 9 shows the effect of the relaxed constraint penalty weight Œªon the generation error, mea-\nsured via MMSE and SMSE, for the Reaction-Diffusion dataset. We evaluate on a 128√ó100\nspatial-temporal grid, as described in Appendix B.3, and solve the relaxed optimization objective in\nEquation (6) using the Adam optimizer.\nWith only 10 flow steps (left), increasing Œªsignificantly improves performance, as the relaxed\nconstraint correction effectively compensates for the coarser integration of the flow. In contrast, with\n100 flow steps (right), the flow is more precise, and additional penalty yields only marginal benefit.\nThis illustrates that relaxed correction is particularly valuable when inference is constrained to a small\n26\n--- Page 27 ---\n104\n103\n102\n101\n100\nPenalty weight \n0.0050.0060.0070.0080.0090.0100.0110.012Error\nMMSE and SMSE vs \nMMSE\nSMSE\n104\n103\n102\nPenalty weight \n0.0020.0040.0060.0080.0100.0120.014Error\nMMSE and SMSE vs \nMMSE\nSMSEFigure 9: Effect of penalty weight Œªon MMSE and SMSE for the Reaction-Diffusion dataset. Left:\n10 flow matching steps. Right: 100 flow matching steps.\nnumber of steps‚Äîfor instance, in scenarios where evaluating the vector field vŒ∏is computationally\nexpensive.\nHowever, overly large values of Œªcan harm performance by distorting the reverse update and breaking\nalignment with the OT interpolant (see Proposition 3.1), as discussed in Equation (5). Hence, choosing\nŒªrequires balancing constraint enforcement with consistency along the learned generative path.\n27",
  "text_length": 84324
}