{
  "id": "http://arxiv.org/abs/2506.00886v1",
  "title": "Toward a Theory of Agents as Tool-Use Decision-Makers",
  "summary": "As Large Language Models (LLMs) evolve into increasingly autonomous agents,\nfundamental questions about their epistemic foundations remain unresolved: What\ndefines an agent? How should it make decisions? And what objectives should\nguide its behavior? In this position paper, we argue that true autonomy\nrequires agents to be grounded in a coherent epistemic framework that governs\nwhat they know, what they need to know, and how to acquire that knowledge\nefficiently. We propose a unified theory that treats internal reasoning and\nexternal actions as equivalent epistemic tools, enabling agents to\nsystematically coordinate introspection and interaction. Building on this\nframework, we advocate for aligning an agent's tool use decision-making\nboundary with its knowledge boundary, thereby minimizing unnecessary tool use\nand maximizing epistemic efficiency. This perspective shifts the design of\nagents from mere action executors to knowledge-driven intelligence systems,\noffering a principled path toward building foundation agents capable of\nadaptive, efficient, and goal-directed behavior.",
  "authors": [
    "Hongru Wang",
    "Cheng Qian",
    "Manling Li",
    "Jiahao Qiu",
    "Boyang Xue",
    "Mengdi Wang",
    "Heng Ji",
    "Kam-Fai Wong"
  ],
  "published": "2025-06-01T07:52:16Z",
  "updated": "2025-06-01T07:52:16Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00886v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00886v1  [cs.AI]  1 Jun 2025Toward a Theory of Agents as\nTool-Use Decision-Makers\nHongru Wangα†Cheng Qianβ†Manling LiδJiahao QiuσBoyang Xueα\nMengdi WangσHeng JiβKam-Fai Wongα\nαThe Chinese University of Hong Kong,βUniversity of Illinois Urbana-Champaign\nδNorthwestern University,σPrinceton University\nhrwang@se.cuhk.edu.hk, chengq9@illinois.edu\nAbstract\nAs Large Language Models (LLMs) evolve into increasingly autonomous agents,\nfundamental questions about their epistemic foundations remain unresolved: What\ndefines an agent? How should it make decisions? And what objectives should guide\nits behavior? In this position paper, we argue that true autonomy requires agents\nto be grounded in a coherent epistemic framework that governs what they know,\nwhat they need to know, and how to acquire that knowledge efficiently. We propose\na unified theory that treats internal reasoning and external actions as equivalent\nepistemic tools, enabling agents to systematically coordinate introspection and\ninteraction. Building on this framework, we advocate for aligning an agent’s tool\nuse decision-making boundary with its knowledge boundary, thereby minimizing\nunnecessary tool use and maximizing epistemic efficiency. This perspective shifts\nthe design of agents from mere action executors to knowledge-driven intelligence\nsystems, offering a principled path toward building foundation agents capable of\nadaptive, efficient, and goal-directed behavior.\n1 Introduction\nLarge Language Models (LLMs) have rapidly evolved beyond text generation into autonomous agents\ncapable of independently planning and executing complex tasks with minimal human oversight [ 1].\nThese emerging capabilities have enabled a broad range of real-world applications, including travel\nplanning [ 2], human-computer interaction [ 3–5], and scientific research [ 6–9]. However, as these\nsystems grow increasingly agentic, foundational questions remain unresolved: What is an agent?\nWhat constitutes its optimal behavior? And how can such optimality be realized in practice?\nFrom a conceptual standpoint, the dominant view frames agents as LLMs that interleave internal\nreasoning and external actions to complete tasks. While functionally effective, this pragmatic framing\nlacks a principled account of how such behaviors should be coordinated or optimized. From an\nempirical standpoint, existing agentic systems primarily rely on prompting [ 10,11] or supervised\nfine-tuning [ 12,13], but seldom investigate how these training paradigms relate to the optimality\nof agent behavior, leaving opaque the reasons behind agentic success or failure. To address this\ntheoretical and empirical gap, we propose a bottom-up formalization of agency grounded in four key\nconstructs: what is a tool, what is an agent, what constitutes optimal behavior, and how to achieve\nit. Specifically, we posit that: (1) A tool is any process or interface, whether internal reasoning or\nexternal interaction, that contributes to knowledge acquisition towards goal completion. (2) An agent\nis a decision-maker that dynamically coordinates internal and external tools in pursuit of specific\nobjectives. (3) Optimal behavior occurs when an agent aligns its tool use decisions with its knowledge\nboundary. (4) This alignment can be operationalized by minimizing the agent’s unnecessary tool use.\n†Equal contributions.\nPreprint. Under review.\n--- Page 2 ---\nCoTReflectionDecomposition...\nActionsModelsFunctions\nAlign to\nGuides\nAgent: Goal-oriented Decision MakerCoordinating\nCoordinating\nInside:Necessitates\nOutside:NecessitatesOutside:ControlsInside:ControlsInternalCognitiveToolsTool Use Decision BoundaryKnowledge BoundaryExternalPhysicalToolsFigure 1: Conceptual framework of agent decision-making based on tool use and knowledge bound-\naries. The agent is modeled as a goal-directed decision-maker that coordinates internal cognitive\ntools (e.g., chain-of-thought, reflection) and external physical tools (e.g., actions, models, functions)\nacross a tool use decision boundary. Optimal behavior emerges when tool use decisions align with\nthe knowledge boundary, ensuring the agent invokes only the tools necessary to acquire missing\nknowledge and efficiently achieve task objectives.\nIn summary, we argue an optimal agent is one that adaptively coordinates internal reasoning and\nexternal action to acquire only the knowledge it needs, achieving goals efficiently by minimizing\nunnecessary tool use.\nTo arrive at these positions, we systematically introduce the Theory of Agent , inspired by the cognitive\nconcept of Theory of Mind , which characterizes an agent’s capacity to model not only external\nenvironments but also its own internal knowledge state. This theory is grounded in the core insight\nthat reasoning and acting are not distinct behaviors but rather epistemically equivalent tools for\nacquiring task-relevant knowledge. This allows us to conceptualize agents as knowledge-driven tool-\nuse decision-makers that adaptively choose between internal introspection and external interaction (As\nshown in Figure 1). Our theoretical framework is developed in two stages: (1) a unified behavioral\nframework (Section 2) that models reasoning and acting under a shared decision-making logic,\ntreating them as internal and external tools for retrieving different sources of knowledge. This\nunification enables us to frame all agent interactions as tool use decisions; (2) three core principles\nof knowledge (Section 3) that define the structure and dynamics of an agent’s epistemic state and\ndecision-making process, providing theoretical lemmata for what constitutes optimal behavior.\nBuilding on this theoretical foundation, we first articulate the importance of aligning an agent’s\nknowledge boundary with its decision boundary, and explore how this alignment is manifested\nthrough training and inference (Section 4.1). Furthermore, we identify four distinct behavior modes\nof agents, and argue that an autonomous agent should learn to accomplish its predefined goals with\nthe minimal number of external tool calls (Section 4.2): an objective that aligns closely with expert’s\nvision for autonomous machine intelligence [ 14], where “a truly autonomous machine intelligence\nis designed to minimize the number of actions a system needs to take in the real world to learn a\ntask.” In our framework, the world model is embodied by the LLM itself, as proposed in recent\nstudies [ 15,16]. Finally, we outline a general roadmap toward building foundation agents that realize\nthese properties and principles in practice (Section 4.3).\n2 Foundations\n2.1 The Unification of Reasoning and Acting\nIt is widely recognized that reasoning and acting constitute the two fundamental capabilities of\nintelligent agent behavior [ 10,17]. Reasoning enables an agent to plan, infer, reflect, and monitor\nits internal cognitive state, while acting allows it to engage with the external environment to gather\n2\n--- Page 3 ---\nnew information or carry out tasks. Rather than viewing these modalities as distinct or sequential\nprocesses, we propose that:\nPosition: A Unified View of Reasoning and Acting\nReasoning and acting should be treated as equivalent epistemic tools within a unified framework, where\nreasoning entails an internal cognitive tool for manipulating information within the agent’s parametric\nknowledge space, while acting entails an external physical tool for acquiring information beyond the\nagent’s internal capabilities.\nThis unified perspective aligns with the affordance theory [ 18], which suggests that actions arise from\nthe interplay between perception and interaction. From this perspective, reasoning and acting are not\nhierarchically ordered or merely sequential but are co-equal capabilities of decision-making. Each\nplays a complementary role in enabling agents to resolve uncertainty and make progress toward task\ncompletion. Embracing this integrated view encourages the development of intelligent systems that\ncan seamlessly coordinate their internal cognitive mechanisms and external interactive capabilities,\nbased on their current knowledge state and the epistemic demands of the task at hand.\nInternal cognitive tools. Cognitive tools refer to internal cognitive mechanisms that support sys-\ntematic or investigative thinking to solve problems [ 19,20]. In the context of intelligent agents,\nvarious reasoning modules [ 11,21], such as Chain-of-Thought [ 22], reflection, decomposition, and\nalternative-thinking, function as cognitive processes that enable the retrieval and manipulation of\ninternal knowledge to guide problem-solving. For instance, Reasoning via Planning (RAP) [ 15] con-\nceptualizes the language model as both a world model and a reasoning engine, incrementally accumu-\nlating knowledge through iterative reasoning steps. Similarly, Self-Discover [ 11] constructs abstract\nreasoning structures and then instantiates them to address complex tasks, mirroring the approach of\ntool-based agents that first generate plans for tool use and then execute them sequentially [ 23,24].\nBeyond these, other cognitive tools appear in diverse applications, such as conversational strategies\nin dialogue systems [ 25] and psychologically inspired mechanisms designed to model uncertainty,\nemotion, or user intent [ 26]. Despite their varied forms, these tools share a common function: they\nserve as triggers for internal knowledge retrieval, allowing the model to reason and act based on its\nembedded understanding of the world.\nExternal physical tools. External physical tools refer to modules or interfaces outside the model\nthat are invoked through specific triggers, such as rules, actions, or special tokens, whose outputs\nare then incorporated into the model’s context to inform subsequent reasoning [ 27,24]. These\ntools function as vital interfaces between the agent and its environment, enabling the acquisition of\ntask-relevant knowledge that lies beyond the agent’s internal parameters. Importantly, external tools\nspan a wide spectrum of interactions, capturing how agents, like humans, leverage their surroundings\nto reduce uncertainty or complete tasks. Examples include querying a search engine, calling an API,\nprocessing sensor input, or performing physical actions [ 27,28]. For instance, clicking a button in a\nuser interface may be represented as an external tool call, where the input parameter is the button’s\nlocation and the resulting webpage serves as the observation. Similarly, in embodied settings, actions\nsuch as “MoveTo(Room A)” can be interpreted as tool invocations, with “Room A” as the parameter\nand the resulting sensory output as the feedback. This perspective enables a unified treatment of\ndiverse forms of interaction as structured tool use: they serve as interfaces for external knowledge\nacquisition, allowing the model to access and interact with knowledge beyond its epistemic capacity.\n2.2 Tool-Integrated Agents\nBuilding on the unification of reasoning and acting, we further propose a redefinition of the agent\ngrounded in this integrated perspective:\nPosition: Definition of Agent\nAn agent is an entity that coordinates internal cognitive tools (e.g., reflection) and external physical\ntools (e.g., function callings) to acquire knowledge in order to achieve a specific goal.\nFrom this viewpoint, an agent is fundamentally a knowledge-driven decision-maker that navigates a\ntask by alternating between internal reasoning and external interaction. Formally, a tool-integrated\n3\n--- Page 4 ---\nagent trajectory can be described as τ= (t1, k1, t2, k2, ..., t n, kn), where each tiis either an internal\nor external tool invocation, and each kirepresents the corresponding knowledge retrieved. Here,\n“knowledge” is broadly defined as any information that advances the agent’s problem-solving state .\nAt each step, the agent must choose the most epistemically valuable tool based on its current state,\naiming to progressively bridge the knowledge gap toward a complete solution. The process concludes\nwhen the agent has accumulated sufficient knowledge to achieve the pre-defined goal.\nThis unified framework offers several key advantages: (1) It generalizes prior approaches such as\nReAct [ 10], which can be viewed as special cases where internal tool steps (e.g., reasoning) are\ntreated as monolithic thought units ri, leveraging the model’s pre-trained cognitive abilities without\nrequiring explicit tool separation. (2) It aligns with findings from large reasoning models (LRMs),\nwhich show that outcome-based reinforcement learning (RL) can effectively train agents to discover\nand utilize internal cognitive tools [ 29]. The same principle applies to external physical tools, as\nshown in recent studies on tool-augmented agents [ 30]. Thus, the framework provides a coherent\nfoundation for agentic learning across both domains. (3) Most importantly, this perspective leads\nto a new learning paradigm: next tool prediction . Just as next-token prediction enables LLMs\nto learn a compressed representation of the world from text, next-tool prediction allows agents\nto learn procedural knowledge through interaction. By learning to choose the right tool, agents\ncan dynamically update their internal representations and evolve through experience, mimicking\nhuman-like adaptation and learning.\n3 Principles of Knowledge and Decision in Model\nAs established earlier, an agent functions as a knowledge-driven decision-maker regarding the use\nof internal or external tools. This implies the existence of a knowledge space defined by the agent’s\nown knowledge boundary, which informs its decisions, and a corresponding decision boundary that\ndetermines whether internal or external tools are employed. Understanding these boundaries is crucial\nfor analyzing agent behavior and serves as a basis for optimizing it. In this section, we formalize the\nconcepts of knowledge and decision boundaries (Section 3.1), and, based on their definitions, we\nintroduce three key principles that highlight optimal agent behavior (Section 3.2 to Section 3.4).\n3.1 The Definition of Knowledge and Decision Boundaries\nKnowledge Boundary. At any time step t, letWrepresent the complete set of world knowledge.\nWe define the model m’s internal and external knowledge as:\nKint(m, t)⊆ W andKext(m, t) =W \\ K int(m, t)\nwhere Kint(m, t)denotes the internal knowledge embedded in m, andKext(m, t)represents the\nexternal knowledge accessible from the world. The knowledge boundary is defined as the frontier\nbetween the two:\n∂K(m, t) =∂Kint(m, t) =∂Kext(m, t)\nThis boundary marks the epistemic limit of the model’s internal knowledge. We assume all internal\nor external knowledge is accurate, leaving discussion of overlap or conflict to Appendix B.\nDecision Boundary. Given a time step t, letTint={t1\nint, ..., tn\nint}be the set of internal cognitive\ntools and Text={t1\next, ..., tm\next}the set of external physical tools. The decision boundary ∂D(m, t)\nis the point at which the model decides whether to use internal or external tools to acquire additional\ntask-relevant knowledge:\n∂D(m, t) =∂Tint(m, t) =∂Text(m, t)\nwhere Tint(m, t)andText(m, t)denote tool choices leading to internal or external knowledge acqui-\nsition, respectively.\nIn summary, the knowledge boundary defines the model’s epistemic limits, while the decision\nboundary governs how the model navigates these limits through tool use. Each point in the knowledge\nspace corresponds to a point in the decision space, reflecting how the model chooses to engage with\nthat knowledge through tool use. In this way, the decision boundary operationalizes the knowledge\nboundary, shaping the model’s policy for knowledge acquisition in pursuit of its goals.\n4\n--- Page 5 ---\n3.2 Principle 1: Foundations\nMost existing models are pre-trained on large-scale corpora in an unsupervised manner, embedding\nsubstantial world knowledge into their parameters [ 31–33]. However, not all knowledge is internalized\nduring pre-training, and external knowledge may still need to be acquired through continual learning\nstrategies such as SFT or RL [ 34,35]. We introduce two key lemmas that ground the distinction and\ndynamics of internal and external knowledge, forming the basis of the knowledge boundary.\nLemma 1.1: Over long horizons, scaling laws enable the expansion of Kint; i.e., the knowledge\nboundary ∂Kmoves outward. This expansion reflects the model’s increasingly comprehensive\ninternal representation of the world across modalities and domains. For instance, Sora2demonstrates\nthe acquisition of rich physical knowledge, enabling the generation of realistic, coherent long-form\nvideos. With sufficient training data, architecture, and optimization, the model effectively compresses\nthe external world into its internal parameter space [ 36,37]. As ∂Kexpands with scale, the model\nmay ultimately support real-time abstraction of the world, or even autonomously discover knowledge\nbeyond existing human understanding [38–40], leading toward AI for scientific discovery.\nLemma 1.2: Continual learning methods such as SFT can reshape both the knowledge boundary\nand the decision boundary. To stay current and improve performance, models must update out-\ndated knowledge or acquire new information through continual learning, including prompting [ 41],\nsupervised fine-tuning [ 42,43], and knowledge editing [ 44,45]. These processes naturally shift\nthe knowledge boundary to reflect updated internal states. In parallel, decision boundaries can be\nadjusted to improve tool use behavior, such as encouraging external tool invocation only when neces-\nsary [ 46,47]. Central to this is the model’s meta-cognitive ability to assess its current knowledge and\ndecide which tool to use accordingly, which we will discuss in Section 4.1.\n3.3 Principle 2: Uniqueness and Diversity\nAcross open-source and proprietary models, both unique and shared characteristics emerge. To better\nunderstand model capabilities and limitations, we posit that while each model has distinct boundaries,\nthere also exist universal properties common to all.\nLemma 2.1: Each model has its own knowledge boundary and decision boundary. These boundaries\ndiffer due to variations in model size, architecture, training data, and learning objectives. Larger\nmodels trained on more diverse corpora tend to internalize a broader scope of world knowledge [ 33,\n37]. In contrast, decision boundaries are primarily shaped through explicit tool use training [ 13],\nleading to variation in how models interact with tools to acquire knowledge.\nLemma 2.2: There exist minimal and maximal knowledge (and decision) boundaries across all\nmodels. Theminimal knowledge boundary ∂Kmin=TN\ni=1∂K(i)represents the smallest common\nset of internalized knowledge shared by all models, regardless of their training setup. Conversely,\nthemaximal knowledge boundary ∂Kmax =SN\ni=1∂K(i)reflects the union of all internal knowledge\nacross models, encompassing even niche or domain-specific knowledge found only in specialized sys-\ntems. Analogously, minimal and maximal decision boundaries exist, though they are best interpreted\nas normative alignment goals rather than fixed, objective thresholds.\n3.4 Principle 3: Dynamic Conservation\nKnowledge is inherently dynamic, continuously evolving as new facts emerge and old ones become\nobsolete. To capture this temporality, we propose the principle of dynamic conservation of knowledge,\nemphasizing how models must adapt to an ever-changing epistemic landscape.\nLemma 3.1: At any time step t, the total world knowledge Wtis fixed and identical across all\nmodels. Ideally, a model would internalize the entire knowledge set, i.e., Kint(m, t) =Wt, requiring\nno external tool use. This entails an aspirational endpoint for fully autonomous intelligence [ 14].\nPractically, however, as Wtexpands over time, models must also evolve to keep pace. If a model’s\nepistemic growth outpaces that of the external world, this ideal state becomes theoretically attainable.\n2https://openai.com/sora/\n5\n--- Page 6 ---\nInternal\nknowledgeExternal\nknowledgeKnowledge\nboundary\nInternal\nCognitive\nToolsExternal\nPhysical\nToolsT ool Use Decision\nboundary\nDecides\nMonitor :Self-aware Knowledge Boundary Control :Self-aware Tool Utilization\nFigure 2: The tool use decision boundary of agent should align with its knowledge boundary. This\nalignment represents the optimal behavior of agent that only invoke external physical tools when\nnecessary.\nLemma 3.2: For any task or query qand model m, there exists a minimal and fixed epistemic\neffort N(q, m ), allocated between internal and external sources, that is necessary to solve the task.\nThis can be decomposed as N(q, m ) =kint+kext, where kintreflects knowledge retrieved from the\nmodel’s internal parameters and kextrepresents knowledge acquired through external tools. This\nformulation reveals several insights: (1) N(q, m )is jointly determined by the complexity of the\ntask and the capabilities of the model, indicating stronger models may satisfy most or all of N\nthrough internal reasoning ( kint→N), while weaker models may depend more on external assistance\n(kext→N) [48]. (2) Even models with limited internal capacity can achieve high performance by\ndynamically offloading reasoning or retrieval steps to more capable tools or agents. This suggests\na form of capability equivalence, where optimal tool use policies allow weaker models to simulate\nstronger ones. (3) The objective is not merely task completion, but the development of behavior\npolicies that minimize epistemic effort while managing latency, cost, and cognitive load. In this\nview, intelligent behavior is defined not just by the correctness of outputs, but by the efficiency and\nadaptiveness of the pathways taken to reach them. We expand on these implications in Appendix A.\n4 Agents: From Knowing to Reasoning and Acting\nBuilding on the principles we discussed, we now explore how these principles shape the design and\nbehavior of intelligent agents. As outlined in Section 2.2, we define an agent as a decision-making\nentity that coordinates internal cognitive tools to retrieve internal knowledge (i.e., reason) and external\nphysical tools to acquire external knowledge (i.e., act). At each step in a task, the agent must decide\nwhich tool to invoke based on its current state and what knowledge is needed to move closer to a\nsolution. This iterative process continues until the agent accumulates sufficient knowledge to produce\na final answer or achieve its goal. Drawing on the concept of the agent’s knowledge boundary and\ndecision boundary, we arrive at the central position of this paper:\nPosition: Decision-Knowledge Alignment Principle\nFor an agent to achieve decision optimality, its tool use decision boundary should align with its\nknowledge boundary. This alignment represents the most efficient way to producing correct answers.\nIn other words, an intelligent agent should invoke internal tools when the needed knowledge lies\nwithin its parametric capacity and turn to external tools when that knowledge must be acquired from\nthe environment. This alignment ensures that the agent’s behavior is both efficient and epistemically\ngrounded, leading to more robust and adaptive decision-making. In this section, we first justify\nour position by examining how alignment between decision and knowledge boundaries can be\noperationalized during both agent training and inference (Section 4.1). We then inspect what\nconstitutes optimal agent behavior under this principle (Section 4.2), and finally, we outline practical\npathways for building agents that achieve optimality in practice (Section 4.3).\n4.1 Alignment of Decision and Knowledge Boundary\nMeta-Cognition. Meta-cognition refers to the ability to monitor and regulate one’s own cognitive\nprocesses: knowing what one knows, recognizing uncertainty, and selecting appropriate strategies\naccordingly [ 49]. In the context of intelligent agents, meta-cognition is the agent’s capacity to assess\n6\n--- Page 7 ---\nInternalKnowledgeExternalKnowledge\nDecideto useExternal Tool\nDecideto useInternal ToolKnowledge outside boundaryTraining Time:\nDecideto useExternal Tool\nFalls inside boundaryMapping\nKnowledge inside boundaryFalls outside boundaryMappingExternal Tool Underuse / Internal Tool Overuse\nOriginal DecisionBoundaryNew Decision BoundaryOriginal DecisionBoundaryNew Decision BoundaryTrainingAligning decision boundary with knowledge boundaryDecideto useInternalToolTrainingAligning decision boundary with knowledge boundaryInternal Tool Underuse / External Tool OveruseFigure 3: Training should dynamically adjust the decision boundary relative to the fixed knowledge\nboundary to optimize efficiency, minimize hallucinations, and prevent tool overuse or underuse.\nwhether the knowledge required to progress lies within its internal parametric space or must be\nacquired through external tools. Just as humans are often governed by an implicit heuristic to draw\non external help when uncertain and reason internally when confident, agents must also learn to make\nsuch distinctions contextually. Therefore, achieving alignment between the knowledge and decision\nboundary ultimately requires cultivating accurate and adaptive meta-cognition, both during training\nand at inference time.\nTraining-Time Alignment Dynamic. After pretraining, a model’s parametric knowledge boundary\nbecomes relatively static, reflecting what its known knowledge that can be elicited. In contrast, the\ndecision boundary remains adjustable during the model alignment phase. As shown in Figure 3,\nmisalignment between these boundaries leads to two primary failure modes. If a model uses internal\ntools for knowledge it does not actually possess, this results in hallucinations or incorrect reasoning\ndue to internal tool overuse [ 50]. Conversely, if the model defers to external tools despite already\nknowing the answer, it wastes computation and time, an inefficiency stemming from external tool\noveruse [ 47]. These conclusions actually prove our position from the opposite side that aligning the\ndecision boundary with the knowledge boundary is the most efficient strategy for an agent to arrive at\nthecorrect answer.\nTo realize this efficient and accurate behavior, alignment training must enable the model to make\ntool use decisions based on a calibrated understanding of its own knowledge limits. This involves\nfostering meta-cognition: the ability to recognize what is known versus unknown. Techniques such\nas supervised fine-tuning with explicit tool labels or reinforcement learning with task-based feedback\ncan guide the model toward more effective tool use strategies, which we will elaborate in Section 4.3.\nOverall, our goal is to shape a dynamic decision boundary that aligns closely with the model’s\nknowledge boundary, enabling more accurate and resource efficient problem solving.\nInference-Time Alignment Dynamic. During inference, the knowledge required to answer a\nspecific query is limited and often partially unknown. As shown in Figure 4, the agent begins with an\nincomplete picture of what it needs to know. By interacting with external tools, such as making API\ncalls or executing actions, the agent retrieves missing information and integrates it into its context.\nThis process incrementally expands the model’s effective knowledge boundary, forming a temporary,\ntask-specific epistemic frontier that evolves as inference progresses.\nReasoning during inference thus becomes a dynamic feedback loop, where the agent alternates\nbetween internal cognition and external exploration. Meta-cognition plays a central role in this\nprocess: the agent must continually assess whether it possesses sufficient knowledge to proceed or\nshould gather more. Without this adaptive self-assessment, the agent risks terminating prematurely or\ninefficiently overusing tools. Robust inference-time meta-cognition enables agents to regulate this\nloop effectively: balancing accuracy, efficiency, and completeness in real-time decision-making.\n7\n--- Page 8 ---\nDecideto useExternal Tool\nDecideto useInternal Tool\nExternalKnowledgeInternalKnowledge\nTotal knowledge needed for a particular queryShould be similar but may not exactly aligncertain knowledge\nDecision for this certain knowledgeMappingOriginalknowledgeboundaryInternalKnowledge\nNew KnowledgeBoundary\nDecide to use External Tool \nInteractionRetrieved External KnowledgeIncorporationInference Time:Figure 4: Inference-time alignment depends on real-time expansion of the knowledge boundary\nthrough interaction, requiring adaptive meta-cognition to balance completeness and efficiency.\n4.2 Optimal Agent Behavior\nAligning an agent’s decision boundary with its knowledge boundary is empirically challenging,\nas the knowledge boundary is inherently abstract and often difficult to detect without extensive\nprobing [ 38,39]. Therefore, we shift our focus from detecting the boundary itself to evaluating the\nbehavior of the agent - specifically, what we consider optimal from a human perspective. While\ncorrectness is a primary goal, as discussed in the previous section, an agent can still produce\ncorrect answers while inefficiently overusing external tools. Thus, correctness alone is not sufficient\nevidence of alignment. To better characterize optimal agent behavior, we argue that efficiency should\naccompany correctness, as an ideal agent not only solves the problem but does so with judicious\ncoordination of its internal and external tools. In this section, we examine four representative agent\nbehaviors based on their patterns of tool use, evaluating the strengths and weaknesses of each in\nterms of alignment and efficiency in agent decision-making.\n•Maximizing both internal and external physical tool use. In this case, the agent produces the\ncorrect answer but does so through excessive use of both internal and external tools, regardless\nof necessity. This behavior is inefficient, consumes unnecessary resources, and increases the\nrisk of error propagation or tool misuse. It also obscures the decision-making process, reducing\ntransparency and trust. Rather than reflecting strategic reasoning, this mirrors brute-force search,\nwhich is misaligned with the goals of scalable and interpretable AI systems.\n•Maximizing external tool and minimizing internal tool use. This entails the agent over-relies on\nexternal tools while underutilizing its internal reasoning capacity. This may yield correct results,\nespecially for smaller models, but it results in inefficiency and increased dependence on external\nsystems. More importantly, it conflicts with the core aim of model scaling: to internalize knowledge\nwithin parameters. By deferring to external tools, the agent misses opportunities to reinforce and\ngeneralize its own representations, limiting long-term autonomy and adaptability.\n•Maximizing internal tools and minimizing external tool use. In this setup, the agent leans\nheavily on internal reasoning and avoids external tool use [ 51]. This behavior promotes autonomy\nand efficiency, especially in constrained environments, and aligns with the principle of maximizing\nmodel capacity. However, excessive internal deliberation can lead to overthinking, producing\nunnecessarily long reasoning chains. While this reflects strong use of internal knowledge, it may\noverlook more efficient external solutions in certain cases (See $ B), indicating a need for better\ntool use calibration.\n•Minimizing both internal and external physical tool use. This represents the most efficient\ntrajectory: solving tasks with minimal use of tools, internal [ 52] or external [ 51]. It reflects optimal\nbehavior of using tools only when necessary, guided by precise self-monitoring and calibrated\ndecision-making. However, extreme minimalism can risk underthinking or skipping essential steps,\n8\n--- Page 9 ---\nespecially in complex tasks. In addition, empirically training agents toward this behavior is difficult,\nas it requires balancing correctness with efficiency, which is a more delicate optimization than\ncorrectness alone.\n4.3 Paths for Agent Optimality\nAmong the four agent behaviors discussed, the third and fourth configurations offer more promising\ndirections for building efficient and autonomous agents. While they differ in their reliance on internal\nreasoning, both aim to minimize reliance on external systems while preserving task success. As\ndiscussed in Section 4.1, this objective implicitly reflects an alignment between the decision and\nknowledge boundaries, yet provides a more actionable proxy that avoids the need to explicitly probe\nthe agent’s often abstract and difficult-to-measure knowledge boundary.\nThis perspective also aligns with the expert vision that autonomous machine intelligence should\nminimize the number of real-world actions required to solve a task [14]. However, achieving such\nminimization remains an open challenge. In this section, we examine existing approaches that aim\nto reduce external tool use, analyzing their strategies, assumptions, and limitations in the context of\nscalable and efficient agent design.\nAgentic Pretraining. Next-token prediction has been foundational in compressing world knowledge\ninto a model’s parametric space [ 36]. However, this alone does not equip models to acquire new\nknowledge through interaction. We propose augmenting this paradigm with next-tool prediction :\na learning objective where the model learns to predict the most appropriate external tool to invoke\nat each step. This transforms interaction itself into a first-class modeling target, allowing the agent\nto learn how to gather information it doesn’t already possess. As research trends toward unified\nagent architectures, modeling all forms of interaction (API calls, UI navigation, or environment\nmanipulation) as structured, learnable outputs opens the door to a new kind of scaling law: one that\ngoverns knowledge acquisition, not just compression. This shift is essential for building adaptive,\nself-improving agents in open-ended, dynamic environments [3, 53].\nAgentic Supervised Fine-tuning. Supervised fine-tuning (SFT) remains the most common ap-\nproach for teaching agents tool use, using small task-specific datasets (e.g., math, code) to demon-\nstrate when and how to call external tools [ 12,13]. However, it often assumes a uniform knowledge\nboundary across models, which is unrealistic. As discussed in Lemma 2.1, this mismatch leads to\ninefficiencies: what is helpful for a small model may be redundant or even distracting for larger ones.\nOne solution is to create custom SFT datasets tailored to each model’s knowledge boundary, but this\nis resource-intensive and hard to scale. A more practical alternative, as outlined in Lemma 2.2, is\nto approximate a maximal knowledge boundary and train agents to defer intelligently when faced\nwith unfamiliar content [ 47]. While this approach offers greater generality, it may lack the precision\nneeded for fine-grained domains, highlighting a trade-off between scalability and behavioral fidelity.\nAgentic Reinforcement Learning. Reinforcement learning (RL) offers a more promising path\nfor aligning a model’s decision-making with its own knowledge boundary, as agents can learn\nfrom experience how to adaptively use tools. The key challenge lies in designing reward functions\nthat go beyond correctness. While many RL agents are trained to maximize answer accuracy, this\nignores how the answer is reached, including whether reasoning is efficient, whether tool use is\njustified, and whether the trajectory is optimal [ 30,54]. Recent work like OTC-PO [ 51] addresses\nthis by balancing correctness with penalties for unnecessary tool calls, encouraging agents to act\nwith restraint and self-awareness. By optimizing not only for outcomes but for processes, RL can\nproduce agents that are not only accurate but also efficient, interpretable, and better aligned with\nreal-world deployment constraints. We further envision a cyclical training paradigm - RL →SFT→\nRL→SFT - where reinforcement learning uncovers high-quality trajectories aligned with the agent’s\nknowledge boundary, and supervised fine-tuning consolidates these behaviors for improved stability\nand generalization. This iterative process can gradually refine both the agent’s policy and decision\nboundary, moving toward increasingly adaptive and self-aware agentic behavior3.\nAgentic Prompting. Once the model is trained, numerous studies utilize prompt engineering\nto develop task-specific agentic workflows across various domains - achieving adaptation without\n3We left some discussion ragarding future directions in Appendix C.\n9\n--- Page 10 ---\nadditional fine-tuning [ 55–57]. Despite achieving exceptional performance on complex tasks, few of\nthese approaches rigorously evaluate behavioral optimality, such as internal cognitive tool overuse\n(i.e., overthinking) or external physical tool overuse (i.e., overacting). For examples, Wang et al.\n[58] propose Agent Workflow Memory (AWM) to enable the agent to learn reusable task workflows\nfrom past experiences and using them to guide future actions, while Alita [ 56] empower the agent to\nself-generate and refine the Model Context Protocol (MCP), aiming to discover more effective and\nefficient path to achieve the pre-defined goal. There is another line of work utilize prompting to collect\nhigh-quality agent trajectories for further supervised fine-tuning to enhance the performance [ 59,60].\n5 Conclusion\nIn this position paper, we introduced a unified epistemic theory of agents that reframes reasoning\nand acting as equivalent tools. By aligning an agent’s decision-making boundary with its knowledge\nboundary, we advocate for the design of agents that are not only capable of completing tasks but do\nso with epistemic efficiency - minimizing unnecessary interactions while maximizing knowledge\ngain to achieve task success. This perspective moves beyond the current paradigm of tool-augmented\nLLMs and points toward a future in which agents exhibit genuine autonomy grounded in principled\ndecision-making.\nLooking ahead, this framework offers a roadmap for developing foundation agents that can operate\neffectively in open-ended environments, learn efficiently with minimal supervision, and generalize\nacross domains. By emphasizing knowledge - driven intelligence, this theory invites a rethinking of\nhow we measure and build capable AI agents - not by their frequency of action, but by their ability to\nknow when to reason and when to act. We believe this epistemic perspective will play a foundational\nrole in the next generation of AI systems, shaping agents that are not only more capable but also safer,\nmore reliable, and better aligned with human values and long-term goals.\nReferences\n[1] Noam Kolt. Governing ai agents. arXiv preprint arXiv:2501.07913 , 2025.\n[2]Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\nand Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\npreprint arXiv:2402.01622 , 2024.\n[3]Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J\nHua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal\nagents for open-ended tasks in real computer environments. Advances in Neural Information\nProcessing Systems , 37:52040–52094, 2024.\n[4]Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Z. Pan, and\nKam-Fai Wong. AppBench: Planning of multiple APIs from various APPs for complex user\ninstruction. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the\n2024 Conference on Empirical Methods in Natural Language Processing , pages 15322–15336,\nMiami, Florida, USA, November 2024. Association for Computational Linguistics.\n[5]Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang,\nJiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin,\nLongxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei\nZheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua\nYang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated\ngui interaction with native agents, 2025.\n[6]Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the\ndeep research. arXiv preprint arXiv:2502.04644 , 2025.\n[7]Thao Nguyen, Tiara Torres-Flores, Changhyun Hwang, Carl Edwards, Ying Diao, and Heng\nJi. Glad: Synergizing molecular graphs and language descriptors for enhanced power conver-\nsion efficiency prediction in organic photovoltaic devices. In Proc. 33rd ACM International\nConference on Information and Knowledge Management (CIKM 2024) , 2024.\n10\n--- Page 11 ---\n[8]Carl Edwards, Aakanksha Naik, Tushar Khot, Martin Burke, Heng Ji, and Tom Hope. Synergpt:\nIn-context learning for personalized drug synergy prediction and drug design. In Proc. 1st\nConference on Language Modeling (COLM2024) , 2024.\n[9]Carl Edwards, Chi Han, Gawon Lee, Thao Nguyen, Chetan Kumar Prasad, Sara Szymkuc,\nBartosz A. Grzybowski, Bowen Jin, Ying Diao, Jiawei Han, Ge Liu, Hao Peng, Martin D. Burke,\nand Heng Ji. mclm: A function-infused and synthesis-friendly modular chemical language\nmodel. In arxiv , 2025.\n[10] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In International Conference\non Learning Representations (ICLR) , 2023.\n[11] Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed Chi,\nDenny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language\nmodels self-compose reasoning structures. Advances in Neural Information Processing Systems ,\n37:126032–126058, 2024.\n[12] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen,\net al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In The Twelfth\nInternational Conference on Learning Representations .\n[13] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen\nYu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools, 2025.\n[14] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.\nOpen Review , 62(1):1–62, 2022.\n[15] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhit-\ning Hu. Reasoning with language model is planning with world model. arXiv preprint\narXiv:2305.14992 , 2023.\n[16] Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, and Junge Zhang. Position: Foundation agents as\nthe paradigm shift for decision making. arXiv preprint arXiv:2405.17009 , 2024.\n[17] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on\nlarge language model based autonomous agents. Frontiers of Computer Science , 18(6), March\n2024.\n[18] James J Gibson. The ecological approach to visual perception: classic edition . Psychology\npress, 2014.\n[19] David H Jonassen. What are cognitive tools? In Cognitive tools for learning , pages 1–6.\nSpringer, 1992.\n[20] Piet AM Kommers, David H Jonassen, and J Terry Mayes. Cognitive tools for learning . Springer,\n1992.\n[21] WANG Hongru, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z Pan, Zeming Liu, and Kam-Fai\nWong. Self-reasoning language models: Unfold hidden reasoning chains with few reasoning\ncatalyst. In Workshop on Reasoning and Planning for Large Language Models , 2025.\n[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems , 35:24824–24837, 2022.\n[23] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language\nmodels. Advances in Neural Information Processing Systems , 36:43447–43478, 2023.\n[24] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octo-\ntools: An agentic framework with extensible tools for complex reasoning. arXiv preprint\narXiv:2502.11271 , 2025.\n11\n--- Page 12 ---\n[25] Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Yongfeng\nHuang, and Kam-Fai Wong. Tpe: Towards better compositional reasoning over cognitive tools\nvia multi-persona collaboration. In Natural Language Processing and Chinese Computing: 13th\nNational CCF Conference, NLPCC 2024, Hangzhou, China, November 1–3, 2024, Proceedings,\nPart II , page 281–294, Berlin, Heidelberg, 2024. Springer-Verlag.\n[26] Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, and\nKam-Fai Wong. Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue\nquestions with LLMs. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of\nthe Association for Computational Linguistics: EMNLP 2023 , pages 12047–12064, Singapore,\nDecember 2023. Association for Computational Linguistics.\n[27] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen\nlanguage models with massive tools via tool embeddings. Advances in neural information\nprocessing systems , 36:45870–45894, 2023.\n[28] Dongge Han, Trevor McInroe, Adam Jelley, Stefano V . Albrecht, Peter Bell, and Amos Storkey.\nLLM-personalize: Aligning LLM planners with human preferences via reinforced self-training\nfor housekeeping robots. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-\nKhalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st Interna-\ntional Conference on Computational Linguistics , pages 1465–1474, Abu Dhabi, UAE, January\n2025. Association for Computational Linguistics.\n[29] DeepSeek-AI Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025.\n[30] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1:\nTraining llms to reason and leverage search engines with reinforcement learning, 2025.\n[31] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowledge bases? In Kentaro Inui, Jing\nJiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 2463–2473, Hong Kong, China, November\n2019. Association for Computational Linguistics.\n[32] Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin\nXu. Knowledgeable or educated guess? revisiting language models as knowledge bases. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 1860–1874,\nOnline, August 2021. Association for Computational Linguistics.\n[33] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu,\neditors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP) , pages 5418–5426, Online, November 2020. Association for Computational\nLinguistics.\n[34] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual\nlearning: Theory, method and application. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 46(8):5362–5383, 2024.\n[35] Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian\nYang, Jiangyue Yan, Yao Su, Zhenhan Dai, Yifeng Xie, Yihan Cao, Lichao Sun, Pan Zhou,\nLifang He, Hechang Chen, Yu Zhang, Qingsong Wen, Tianming Liu, Neil Zhenqiang Gong,\nJiliang Tang, Caiming Xiong, Heng Ji, Philip S. Yu, and Jianfeng Gao. A survey on post-training\nof large language models, 2025.\n[36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels, 2020.\n12\n--- Page 13 ---\n[37] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity\nscaling laws. In The Thirteenth International Conference on Learning Representations , 2025.\n[38] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and\nHaifeng Wang. Investigating the factual knowledge boundary of large language models with\nretrieval augmentation. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa,\nBarbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International\nConference on Computational Linguistics , pages 3697–3715, Abu Dhabi, UAE, January 2025.\nAssociation for Computational Linguistics.\n[39] Moxin Li, Yong Zhao, Yang Deng, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng,\nand Tat-Seng Chua. Knowledge boundary of large language models: A survey, 2024.\n[40] Xunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary\nfor large language models: A different perspective on model evaluation. In Lun-Wei Ku,\nAndre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 2270–2286,\nBangkok, Thailand, August 2024. Association for Computational Linguistics.\n[41] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang.\nCan we edit factual knowledge by in-context learning? In Houda Bouamor, Juan Pino, and\nKalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 4862–4876, Singapore, December 2023. Association for Computa-\ntional Linguistics.\n[42] Siyuan Cheng, Ningyu Zhang, Bozhong Tian, Xi Chen, Qingbin Liu, and Huajun Chen. Editing\nlanguage model-based knowledge graph embeddings. In Proceedings of the AAAI conference\non artificial intelligence , volume 38, pages 17835–17843, 2024.\n[43] XiaoQi Han, Ru Li, Xiaoli Li, Jiye Liang, Zifang Zhang, and Jeff Pan. InstructEd: Soft-\ninstruction tuning for model editing with hops. In Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024 , pages\n14953–14968, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n[44] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge\nediting for large language models: A survey, 2024.\n[45] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen,\nand Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , pages 10222–10240, Singapore, December\n2023. Association for Computational Linguistics.\n[46] Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang,\nGuanhua Chen, and Kam-Fai Wong. Self-DC: When to reason and when to act? self divide-and-\nconquer for compositional unknown questions. In Luis Chiruzzo, Alan Ritter, and Lu Wang,\neditors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Volume 1: Long Pa-\npers) , pages 6510–6525, Albuquerque, New Mexico, April 2025. Association for Computational\nLinguistics.\n[47] Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür,\nGokhan Tur, and Heng Ji. Smart: Self-aware agent for tool overuse mitigation, 2025.\n[48] Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed F. AbouElhamayed, Yueying Li, and\nMohamed S. Abdelfattah. Splitreason: Learning to offload reasoning, 2025.\n[49] Rakefet Ackerman and Valerie A. Thompson. Meta-reasoning: Monitoring and control of\nthinking and reasoning. Trends in Cognitive Sciences , 21(8):607–617, 2017.\n[50] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan\nHerzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint\narXiv:2405.05904 , 2024.\n13\n--- Page 14 ---\n[51] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin,\nMengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning,\n2025.\n[52] Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025.\n[53] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem\nGokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking\nllms for embodied decision making. Advances in Neural Information Processing Systems ,\n37:100428–100534, 2024.\n[54] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl, 2025.\n[55] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\nand Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\narXiv:2309.17288 , 2023.\n[56] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang,\nZixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan\nHuang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent enabling scalable\nagentic reasoning with minimal predefinition and maximal self-evolution, 2025.\n[57] Jiahao Qiu, Fulian Xiao, Yimin Wang, Yuchen Mao, Yijia Chen, Xinzhe Juan, Siran Wang, Xuan\nQi, Tongcheng Zhang, Zixin Yao, Jiacheng Guo, Yifu Lu, Charles Argon, Jundi Cui, Daixin\nChen, Junran Zhou, Shuyao Zhou, Zhanpeng Zhou, Ling Yang, Shilong Liu, Hongru Wang,\nKaixuan Huang, Xun Jiang, Yuming Cao, Yue Chen, Yunfei Chen, Zhengyi Chen, Ruowei\nDai, Mengqiu Deng, Jiye Fu, Yunting Gu, Zijie Guan, Zirui Huang, Xiaoyan Ji, Yumeng Jiang,\nDelong Kong, Haolong Li, Jiaqi Li, Ruipeng Li, Tianze Li, Zhuoran Li, Haixia Lian, Mengyue\nLin, Xudong Liu, Jiayi Lu, Jinghan Lu, Wanyu Luo, Ziyue Luo, Zihao Pu, Zhi Qiao, Ruihuan\nRen, Liang Wan, Ruixiang Wang, Tianhui Wang, Yang Wang, Zeyu Wang, Zihua Wang, Yujia\nWu, Zhaoyi Wu, Hao Xin, Weiao Xing, Ruojun Xiong, Weijie Xu, Yao Shu, Xiao Yao, Xiaorui\nYang, Yuchen Yang, Nan Yi, Jiadong Yu, Yangyuxuan Yu, Huiting Zeng, Danni Zhang, Yunjie\nZhang, Zhaoyu Zhang, Zhiheng Zhang, Xiaofeng Zheng, Peirong Zhou, Linyan Zhong, Xiaoyin\nZong, Ying Zhao, Zhenxin Chen, Lin Ding, Xiaoyu Gao, Bingbing Gong, Yichao Li, Yang Liao,\nGuang Ma, Tianyuan Ma, Xinrui Sun, Tianyi Wang, Han Xia, Ruobing Xian, Gen Ye, Tengfei\nYu, Wentao Zhang, Yuxi Wang, Xi Gao, and Mengdi Wang. On path to multimodal historical\nreasoning: Histbench and histagent, 2025.\n[58] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen\nZhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow\ngeneration. arXiv preprint arXiv:2410.10762 , 2024.\n[59] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+\nreal-world apis. In The Twelfth International Conference on Learning Representations .\n[60] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Train-\ning language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425 ,\n2025.\n[61] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the national academy of\nsciences , 114(13):3521–3526, 2017.\n[62] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan\nHerzig. Does fine-tuning LLMs on new knowledge encourage hallucinations? In Yaser Al-\nOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing , pages 7765–7784, Miami, Florida, USA,\nNovember 2024. Association for Computational Linguistics.\n14\n--- Page 15 ---\n[63] Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen,\nHeng Ji, and Tong Zhang. R-tuning: Instructing large language models to say ‘I don‘t know’.\nIn Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers) , pages 7113–7139, Mexico City, Mexico,\nJune 2024. Association for Computational Linguistics.\n[64] Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia\nTsvetkov. Don‘t hallucinate, abstain: Identifying LLM knowledge gaps via multi-LLM col-\nlaboration. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the\n62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers) , pages 14664–14690, Bangkok, Thailand, August 2024. Association for Computational\nLinguistics.\n[65] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language\nmodels (mostly) know what they know. arXiv preprint arXiv:2207.05221 , 2022.\n[66] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do\nlarge language models know what they don‘t know? In Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 ,\npages 8653–8665, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[67] Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang.\nKnowledge of knowledge: Exploring known-unknowns uncertainty with large language models.\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for\nComputational Linguistics: ACL 2024 , pages 6416–6432, Bangkok, Thailand, August 2024.\nAssociation for Computational Linguistics.\n[68] Amos Azaria and Tom Mitchell. The internal state of an LLM knows when it‘s lying. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023 , pages 967–976, Singapore, December 2023. Association for\nComputational Linguistics.\n[69] Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, and Pascale\nFung. LLM internal states reveal hallucination risk faced with a query. In Yonatan Belinkov,\nNajoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors,\nProceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks\nfor NLP , pages 88–104, Miami, Florida, US, November 2024. Association for Computational\nLinguistics.\n[70] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances\nfor uncertainty estimation in natural language generation, 2023.\n[71] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334 , 2022.\n[72] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can\nllms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv\npreprint arXiv:2306.13063 , 2023.\n[73] Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-\nbox hallucination detection for generative large language models. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing , pages 9004–9017, Singapore, December 2023. Association for\nComputational Linguistics.\n[74] Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing\nGao. SaySelf: Teaching LLMs to express confidence with self-reflective rationales. In Yaser\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing , pages 5985–5998, Miami, Florida,\nUSA, November 2024. Association for Computational Linguistics.\n15\n--- Page 16 ---\n[75] Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu,\nand Kam-Fai Wong. Ualign: Leveraging uncertainty estimations for factuality alignment on\nlarge language models, 2024.\n[76] Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring\ncollaboration mechanisms for LLM agents: A social psychology view. In Lun-Wei Ku, Andre\nMartins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers) , pages 14544–14607, Bangkok,\nThailand, August 2024. Association for Computational Linguistics.\n[77] OpenAI Team. Openai o1 system card, 2024.\n[78] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025.\n[79] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chen-\nguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language\nmodels are strong context generators. In The Eleventh International Conference on Learning\nRepresentations .\n[80] Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei\nXu. Knowledge conflicts for LLMs: A survey. In Yaser Al-Onaizan, Mohit Bansal, and\nYun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural\nLanguage Processing , pages 8541–8565, Miami, Florida, USA, November 2024. Association\nfor Computational Linguistics.\n[81] Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang,\nXuanli He, Kam-Fai Wong, and Pasquale Minervini. Steering knowledge selection behaviours\nin LLMs via SAE-based representation engineering. In Luis Chiruzzo, Alan Ritter, and\nLu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter\nof the Association for Computational Linguistics: Human Language Technologies (Volume\n1: Long Papers) , pages 5117–5136, Albuquerque, New Mexico, April 2025. Association for\nComputational Linguistics.\n16\n--- Page 17 ---\nInternal\nknowledgeExternal\nknowledgeKnowledge\nboundary\nExternal\nknowledgeKnowledge\nboundary\nInternal\nknowledgeExternal\nknowledge\nNew\nKnowledgeInternal\nknowledge\nKnowledge\nboundary\n(a)Knowledge Boundary (b)Knowledge Expansion (c)New Knowledge DiscoveryFigure 5: A high-level illustration of Lemma 1.1 for a specific model m.\nA Further Discussion: Principles of Knowledge and Decision in Model\nA.1 Principle 1: Foundation of Knowledge and Decision Boundary\nLemma 1.1: Some may argue that it is possible that the scaling law does not work and the expansion\nmay stop at a specific timestep. In this case, there always some knowledge that the models can not\ncapture or master, and the model must learn these via another ways, such as learn from interaction, or\nhuman experts, to expand the knowledge boundary [38].\nLemma 1.2: It is always desired that the model can gain or update knowledge in specific domains\nwithout affecting other domains. However, this is extremely challenging, as knowledge in the real\nworld is deeply interconnected rather than isolated. During learning on new experience, models are\nprone to catastrophic forgetting [ 61] or hallucinate [ 62,63], where previously acquired knowledge\nfades.\nIn summary, Lemma 1.1 and Lemma 1.2 call for a scalable lifelong learning paradigm that can\ndynamically expand and redistribute the model’s knowledge boundary ∂Kin response to new data\nand evolving tasks across the time.\nA.2 Principle 2: Uniqueness and Diversity of Knowledge and Decision Boundary\nThere are lots of models from both open-source and close-source sides. To better understand the\ndiversity and limitations of model capabilities, we posit that there exist both unique and universal\nproperties shared across all models [64].\nLemma 2.1: There are several lines of research direction being affected by this lemma. One line\nof research is the knowledge boundary identification through prompting [ 65–67], probing [ 68,69],\nuncertainty estimation [ 70–72], and self-consistency checks [ 73]. For instance, several studies try to\ncollect model-specific supervised fine-tuning dataset to teach the model to say “I do not know\" for\nthe unknown questions and only provide answer for known questions [ 63,74,75]. Another line of\nwork involves model specialization and collaborative inference where models with complementary\nboundaries (e.g., generalist vs. domain experts) are orchestrated to jointly solve complex tasks\n[64,76]. This is particularly relevant in modular systems or tool-integrated agents, where models\nselectively offload tasks based on different specialists.\nExternal\nknowledgeMaximal knowledge\nboundary\nExternal\nknowledgeMinimal\nknowledge\nboundary\nFigure 6: A high-level illustration of Lemma 2.2 for the all models M={m0, ..., m n}.\n17\n--- Page 18 ---\nLemma 2.2: Both minimal and maximal knowledge boundaries play key roles to better understand\nthe mental world of the models. On the minimal side, ∂Kmincaptures the foundational knowledge\nconsistently learned by all models, such as basic language structures, common facts, and widely\nshared cultural concepts. It effectively defines a shared epistemic core - a common worldview shaped\nby dominant patterns in pretraining data. This boundary reflects the most universal priors across\nmodels and has important implications for alignment, fairness, and generalization. On the maximal\nside, understanding ∂Kmaxhelps reveal the outer limits of what any existing model can know. This\ninsight motivates strategies like one-fits-all supervised fine-tuning, where models are trained to defer\nor trigger specific actions, such as tool use or human intervention, when a task requires knowledge\nbeyond this boundary. For example, several studies collect the well-designed dataset to finetune the\nmodel to only call tools when the required knowledge is outside the inherent parametric knowledge\nof LLMs, and therefore the dataset can apply for all model [47].\nA.3 Principle 3: Dynamic Conservation of Knowledge\nKnowledge is inherently dynamic—the world is constantly evolving as new information is discovered\nand outdated knowledge becomes obsolete. To capture this temporal nature of knowledge, we propose\nthe principle of dynamic conservation of knowledge.\nLemma 3.1: Current mainstream models, however, primarily function as knowledge distributors\nrather than knowledge discoverers. They optimize for efficiency and effectiveness in retrieving,\nsynthesizing and applying existing knowledge, for example, improving task automation, boosting\nproductivity, and supporting human decision making. The emerging paradigm of AI for Science seeks\nto bridge this gap by leveraging models not only to encode and apply existing knowledge but to\ngenerate novel hypotheses, identify hidden patterns, and accelerate scientific discovery.\nLemma 3.2: There are several RL-based approaches focus exclusively on optimizing final answer\ncorrectness, without accounting for the underlying reasoning trajectory. In detail, several large\nreasoning models (LRMs), such as OpenAI’s o1 [ 77], DeepSeek-R1 [ 29], and QwQ [ 78], achieve\nexceptional performance by being optimized solely for final answer correctness, regardless of the\nnumber of reasoning tokens used or the utility of the reasoning process itself. Moreover, few studies\ntry to re-produce the success of RL in tool-integrated reasoning which also only focus on the final\nanswer correctness [ 30,54]. As a result, models often develop inefficient or suboptimal behaviors,\nincluding overthinking, where excessive internal reasoning leads to inflated tint, and tool overuse,\nwhere models make frequent, unproductive calls to external tools, increasing text. Although few\nstudies focus on the this issue, they mainly focus on one side either the internal or external. In contrast,\nwe argue that a more principled and holistic framework is required, one that views internal and external\ntools as complementary actions within a unified decision-making process. Such a framework should\naim not just to maximize correctness, but also to approximate the minimal epistemic effort N(q, m )\nrequired for task completion.\nTogether, these three principles establish a unified theoretical framework for understanding, analyzing,\nand improving the knowledge structures and behaviors of models and agents, offering foundational\nguidance for developing models that are not only powerful and effective, but also efficient, adaptive,\nand epistemically aware. It is believed to guide the design of next-generation models or agents\nthat can reason more effectively, learn continuously, collaborate strategically, and act responsibly in\nopen-ended, evolving environments.\nBOther Relationship Between Internal Knowledge and External Knowledge\nIn the main pages, we assume that the internal knowledge and external knowledge are two separated\nparts for simplicity and generalization. In practice, it may not hold since the internal knowledge may\noverlap with external knowledge, and there may exist knowledge conflict between these two sources.\nWe discuss these situations as follows:\nKnowledge Overlap. This highlights an important possibility: internal cognitive tools and certain\nexternal physical tools can retrieve overlapping or even identical pieces of knowledge, implying a\npotential for epistemic transferability between the two. For example, a model may answer a factual\nquestion either by recalling internalized knowledge from its parameters or by querying an external\n18\n--- Page 19 ---\ntool such as a search engine, both pathways leading to the same correct answer [ 79,46]. A similar\nphenomenon occurs with tasks like simple mathmatical operation, where the model may either\ncompute the result internally or delegate it to an external calculator API. This interchangeability\nsuggests that internal and external tools can act as substitutes under certain conditions, raising\nfurther questions about when and how agents should transfer, balance, or even fuse internal reasoning\nwith external interaction for optimal epistemic efficiency. In these cases, minimizing external physical\ntools is also maximizing internal cognitive tools as evidenced by recent study [ 51]. We emphasize\nthe opposite is not necessarily true: maximizing the use of external physical tools does not imply\nminimizing the use of internal cognitive tools since not all external tools can map to specific internal\ntools. In many cases, excessive reliance on external tools may reflect insufficient internal reasoning\nrather than optimized epistemic behavior.\nKnowledge Conflict. In some cases, internal and external knowledge sources may conflict [ 80,81],\nleading to inconsistent or contradictory information. This typically arises when the model retrieves\noutdated, incomplete, or hallucinated content from its internal memory that contradicts more up-to-\ndate or accurate external sources. Such conflict is especially pronounced when the model attempts to\ngenerate knowledge beyond its internal boundary, often resulting in hallucinations [ 50]. For instance,\na model may confidently generate an incorrect answer based on memorized but obsolete knowledge,\neven when a correct answer is accessible via an external tool like a search engine or database. These\nsituations highlight the importance of epistemic calibration: the model must learn not only what it\nknows, but also when its internal knowledge is unreliable and should be overridden by external input.\nAddressing knowledge conflict requires mechanisms for knowledge arbitration, where agents resolve\ndiscrepancies by evaluating the reliability, recency, and epistemic certainty of each source - an open\nchallenge for building robust decision boundaries under uncertainty.\nC Future Directions\nVision Agent. Vision agents extend our unified framework of reasoning and acting by incorporating\nvisual affordances as part of the decision-making loop. In our definition, external physical tools\nare invoked based on an agent’s knowledge gaps; in vision agents, visual input becomes a direct\nmeans of detecting such gaps and informing tool use decisions. To realize this, future systems should\ntreat visual understanding not as passive recognition but as actionable epistemic input. This involves\nembedding affordance-aware modules into vision-language models that not only recognize objects\nbut predict possible interactions. Moreover, meta-cognitive control should guide visual attention: the\nagent must actively attend to regions most likely to resolve its uncertainty. Training in simulation\nwith reinforcement learning can allow agents to learn the utility of visual exploration for acquiring\nexternal knowledge, enabling more precise tool invocation grounded in perception.\nEmbodied Agent. Embodied agents concretize the external physical tool dimension by extending\nit into the physical world, where the agent’s own body becomes a tool, and the environment imposes\ndynamic constraints. Within our framework, this embodiment means that the agent’s knowledge\nboundary is not only cognitive but also physically bounded (e.g., what can be seen, reached, or\nmanipulated). To operationalize this, agents should be equipped with real-time sensorimotor feedback\nloops and control modules that treat actions as epistemic moves: physical actions (e.g., MoveTo,\nPickUp) should be treated like external tool calls that yield knowledge from the environment.\nLearning here must be closed-loop and incremental—using reinforcement signals from physical\ninteraction to adjust the decision boundary over time. Physical meta-cognition, such as failure\ndetection or confidence in execution, should guide whether to reason further, retry an action, or\nexplore alternatives.\nMulti-Agent Coordination. Multi-agent coordination extends our framework from individual\nagents aligning their decision and knowledge boundaries to a collective setting where these boundaries\nare distributed across multiple agents. In this paradigm, each agent operates with a local view (its\nown knowledge and decision boundaries), but contributes to a shared task by reasoning about and\ninteracting with other agents. The key challenge is aligning these distributed boundaries to form\na coherent collective intelligence. To achieve this, agents must be equipped with mechanisms\nto communicate epistemic state, and dynamically delegate subtasks to peers whose knowledge\nboundaries better match the problem context. This requires structured communication protocols, role\n19\n--- Page 20 ---\ninference strategies, and shared meta-cognitive modules that manage when to ask, respond, or act.\nPractically, this can be developed through multi-agent reinforcement learning in environments where\ncooperation is required for successful task completion, with reward functions encouraging efficient\ndivision of cognitive and physical labor.\n20",
  "text_length": 75384
}