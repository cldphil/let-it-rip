{
  "id": "http://arxiv.org/abs/2506.00893v1",
  "title": "Affordance Benchmark for MLLMs",
  "summary": "Affordance theory posits that environments inherently offer action\npossibilities that shape perception and behavior. While Multimodal Large\nLanguage Models (MLLMs) excel in vision-language tasks, their ability to\nperceive affordance, which is crucial for intuitive and safe interactions,\nremains underexplored. To address this, we introduce A4Bench, a novel benchmark\ndesigned to evaluate the affordance perception abilities of MLLMs across two\ndimensions: 1) Constitutive Affordance}, assessing understanding of inherent\nobject properties through 1,282 question-answer pairs spanning nine\nsub-disciplines, and 2) Transformative Affordance, probing dynamic and\ncontextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs.\nEvaluating 17 MLLMs (nine proprietary and eight open-source) against human\nperformance, we find that proprietary models generally outperform open-source\ncounterparts, but all exhibit limited capabilities, particularly in\ntransformative affordance perception. Furthermore, even top-performing models,\nsuch as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag\nbehind human performance (best: 85.34%, worst: 81.25%). These findings\nhighlight critical gaps in environmental understanding of MLLMs and provide a\nfoundation for advancing AI systems toward more robust, context-aware\ninteractions. The dataset is available in\nhttps://github.com/JunyingWang959/A4Bench/.",
  "authors": [
    "Junying Wang",
    "Wenzhe Li",
    "Yalun Wu",
    "Yingji Liang",
    "Yijin Guo",
    "Chunyi Li",
    "Haodong Duan",
    "Zicheng Zhang",
    "Guangtao Zhai"
  ],
  "published": "2025-06-01T08:26:34Z",
  "updated": "2025-06-01T08:26:34Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00893v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00893v1  [cs.CL]  1 Jun 2025Affordance Benchmark for MLLMs\nJunying Wang\nwangjunying@pjlab.org.cn\nFudan University\nShanghai AI LabWenzhe Li\nliwenzhe@pjlab.org.cn\nShanghai AI Lab\nShanghai, ChinaYalun Wu\nwuyalun1@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai, China\nYingji Liang\nliangyingji@stu.ecnu.edu.cn\nEast China Normal University\nShanghai, ChinaYijin Guo\nguoyijin@pjlab.org.cn\nShanghai Jiao Tong University\nShanghai AI LabChunyi Li\nlichunyi@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nHaodong Duan\nduanhaodong@pjlab.org.cn\nShanghai AI Lab\nShanghai, ChinaZicheng Zhang†\nzhangzicheng@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong UniversityGuangtao Zhai†\nzhaiguangtao@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nWhat is affordance ? \nThe \"action possibilities\" provided \nby the object for organisms.\nDoes the machine understand the \naffordance of this door?\nThis door can be unscrewed by \nthe doorknob, and also \nslammed by foot.\nConstitutive Affordance \nTransformative Affordance\nThe affordance of this object? \n \nA): Used as kindling to assist fire ignition\nB): Extracting liquid from this object for consumption\nC): Natural food for feeding small animals such as squirrels\nD): Used as material for handicrafts to make decorative items\nE): Used in gardening as mulch to maintain soil moisture\nCorrect choice: A, C, D, E\nWhich one is the best matches the \nprimary use of the object? \nA): Handheld fan\nB): Concert glow stick\nC): Indoor decoration ornament\nD): Desktop candle holder\nE): Drink stirrer\nCorrect choice: A\nIntuition\nReasoning\nAffordance\nCategory\nWhy affordance important ? \nMore efficient task \nexecution, enhanced \nenvironmental adaptability. \nMulti-modal \nLLMsIndustrial \nRobotics\nAutonomous \nVehiclesAR/VR\nDevices\nRehabilitation\nRobotics\nHow to evaluate affordance \nunderstanding ?\nEducational\nRobotics\nFigure 1: The motivation of the A4Bench. The affordance theory proposed by James J. Gibson[ 15] defines the action possibilities\nprovided by the object for organisms. Evaluating the affordance perception abilities of MLLMs can help enable more efficient\ntask execution and improved adaptability to diverse environments for AI systems.\nABSTRACT\nAffordance theory posits that environments inherently offer action\npossibilities that shape perception and behavior. While Multimodal\nLarge Language Models (MLLMs) excel in vision-language tasks,\ntheir ability to perceive affordance, which is crucial for intuitive\nand safe interactions, remains underexplored. To address this, we\nintroduce A4Bench , a novel benchmark designed to evaluate the\naffordance perception abilities of MLLMs across two dimensions:\n1)Constitutive Affordance , assessing understanding of inherent ob-\nject properties through 1,282 question-answer pairs spanning nine\nsub-disciplines, and 2) Transformative Affordance , probing dynamic\nand contextual nuances (e.g., misleading, time-dependent, cultural,or individual-specific affordance) with 718 challenging question-\nanswer pairs. Evaluating 17 MLLMs (nine proprietary and eight\nopen-source) against human performance, we find that proprietary\nmodels generally outperform open-source counterparts, but all\nexhibit limited capabilities, particularly in transformative affor-\ndance perception. Furthermore, even top-performing models, such\nas Gemini-2.0-Pro (18.05% overall exact match accuracy), signifi-\ncantly lag behind human performance (best: 85.34%, worst: 81.25%).\nThese findings highlight critical gaps in environmental understand-\ning of MLLMs and provide a foundation for advancing AI systems\ntoward more robust, context-aware interactions. The dataset is\navailable in https://github.com/JunyingWang959/A4Bench/.\n--- Page 2 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY Wang et al.\nCCS CONCEPTS\n•Computing methodologies →Computer vision ;Spatial and\nphysical reasoning .\nKEYWORDS\nAffordance, MLLM, Benchmark\n1 INTRODUCTION\n‘What we perceive when we look at objects are their affordances,\nnot their qualities ’. This bold assertion by James J. Gibson en-\ncapsulates the essence of affordance theory , which posits that\nenvironments inherently offer animals a spectrum of action possi-\nbilities that shape perception and behavior [ 15]. Unlike traditional\ntheories that focus on discerning physical attributes such as shape\nor color, this framework suggests that organisms directly appre-\nhend the action possibilities embedded within environmental\nfeatures. For instance, a door is perceived not merely as a structure\nbut as an entity affording passage: it can be unlocked with a key or\nforcefully opened with a kick. Defined as the inherent values and\nmeanings of objects, affordance form a vital link between ecological\nfeatures and behavioral responses, challenging dualistic paradigms\nand promoting a unified ecological approach to perception.\nWhy affordance is important?\nUnderstanding affordance is vital for intelligent agents, whether\nbiological or artificial, to engage meaningfully with their envi-\nronments . For artificial intelligence systems navigating complex\nsettings, understanding affordance ensures robust and intuitive in-\nteractions while enhancing safety. For example, a robot perceiving\na surface as affording support can navigate terrain securely, while\none identifying an object as graspable can manipulate it effectively.\nMoreover, effective affordance perception enables industrial robots\ntoexecute tasks with greater efficiency and allows rehabilitation\nrobots to enhance human-machine interaction by adapting to\nuser needs. The capacity to discern affordance, whether beneficial\nor harmful, underpins behaviors ranging from survival to complex\nsocial interactions. Gibson underscored this by noting how humans\nmodify environments ’to change what it affords’, emphasizing the\nprofound link between affordance and intentional action.\nWhat Affordance Perception Should MLLMs Possess?\nAccurate perception of an object affordance enables recognition\nof its true utility, as Gibson asserts, ‘ If the affordance of a thing are\nperceived correctly, we say that it looks like what it is ’. Yet discerning\ngenuine affordance often demands experiential learning, since a\nleaf appearing benign may conceal a nettle sting or a politician\nseeming helpful may mask deceptive demagoguery.\nTo evaluate this capability rigorously, we introduce a novel\nbenchmark ( A4Bench ) to assess affordance perception across two\nprimary dimensions. First, constitutive affordance examines how\nMLLMs apprehend inherent object and environmental properties\nsuch as shape, size, or material that determine whether a surface\naffords walking or an object affords grasping, using 1282 question-\nanswer pairs across nine sub-disciplines. Second, transformative\naffordance probes comprehension of dynamic affordance, includ-\ningmisleading affordance (like a glass barrier appearing as open\nair but affording collision), time-independent affordance (e.g.,\nfruit ripening changes its food value affordance, specified by color),cross-cluture affordance (e.g., a postbox affording letter-mailing\nonly within a postal system), and individual affordance (e.g., a\nledge being sit-on-able depending on leg length).\nA4Bench pioneers comprehensive affordance perception eval-\nuation, highlighting contextual and dynamic nuances to reveal\nMLLM capabilities and steer future development. Testing 17 lead-\ning MLLMs, both open-source and closed-source, on A4Bench\nreveals that even the top-performing model significantly trails hu-\nman performance, leading to a critical conclusion:\nMLLMs are still poor at affordance perception.\nMLLMs exhibit significant limitations in perceiving affordance\nwith human-like proficiency, with marked disparities between open-\nsource and closed-source variants. This deficiency spans constitu-\ntive and transformative affordance. Challenges arise from limited\ncontextual understanding, notably in medical disciplines within\nconstitutive affordance and agent-specific dimensions within trans-\nformative affordance. Conversely, human observers consistently\nexcel across all dimensions, highlighting inadequate comprehension\nby MLLMs of object-human-environment interactions essential for\nrobust affordance perception (poor at affordance perception).\nEvaluating affordance perception in MLLMs shapes AI advance-\nments by driving A4Bench development, as shown in Fig. 1. And\nour contributions can be summarized as follows:\n•We extend affordance theory to the context of MLLMs and\nintroduce a systematic evaluation framework for affordance that\nrigorously tests the understanding of MLLMs on understanding\nconstitutive and transformative affordance.\n•We propose A4Bench , the first comprehensive benchmark\nspecifically designed to assess the affordance perception ca-\npabilities of MLLMs. It encompasses 2,000 multimodal question-\nanswer pairs, covering both Constitutive Affordance (1,282 pairs\nacross nine sub-disciplines) and Transformative Affordance (718\npairs addressing misleading, time-dependent, cross-cultural,\nand individual-specific affordance).\n•By evaluating 17 MLLMs (9 proprietary and 8 open-source)\nagainst human performance, we provide a detailed analysis\nof their affordance perception capabilities. Our findings reveal\nsignificant limitations in existing models, with even top per-\nformers lagging far behind human benchmarks, thus offering a\nroadmap for future improvements in context-aware AI systems.\n2 RELATED WORKS\nMultimodal Large Language Models. Multimodal Large Lan-\nguage Models (MLLMs) have demonstrated remarkable capabilities\nin diverse vision-language tasks. Among them, diverse proprietary\n(close-source ) models [ 3,6,16,29,30,35] and representative open-\nsource models [ 2,10–13,18,21–24,27,28,36,37,39,42,44] have\nexhibited impressive superiority from embodied agents [ 34] to other\nreal-world applications [ 19]. However, whether these MLLMs are\nmasters at percepting the affordance of object is still question-\nable, which needs further investigation.\nMultimodal Benchmarks. Current benchmarks provide valu-\nable insights, assessing capabilities ranging from bilingual percep-\ntion and reasoning (MMBench [ 26]), hierarchical comprehension\n(SEED-Bench [ 20]), expert-level multimodal tasks (MMT-Bench\n--- Page 3 ---\nAffordance Benchmark for MLLMs Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\n[41]), and complex embodied scenarios (PCA-Bench [ 9]), to crit-\nical nuances like visual dependency (MMStar [ 8]), safety against\nadversarial inputs (MM-SafetyBench [ 25]), and understanding of\nAI-generated images (A-Bench [ 43]). Despite these efforts, there is\nstill a gap in explicitly assessing the affordance perception abilities\nof MLLMs. Since understanding affordance is fundamental for any\nintelligent agent to interact effectively, meaningfully, and safely\nwith its surroundings, A4Bench is developed to bridge this gap.\n3 CONSTRUCTING THE A4BENCH\n3.1 Key Principles\nCovering Constitutive and Transformative Affordance . The\nhuman brain, when visually recognizing action interactions, pro-\ncesses static forms alongside dynamic movements [7]. Inspired by\nthis, assessing MLLMs in perceiving object affordance involves un-\nderstanding inherent object properties and their empowering potential .\nAssessing whether these models meet such criteria requires examin-\ning proficiency in ( constitutive affordance ) and ( transformative\naffordance ) perception. As shown in Fig. 2, constitutive affordance\ncovers a diverse range of scenarios, while transformative affordance\nincludes misleading affordance, individual-variant affordance, time-\ndependent affordance, and cross-cultural affordance.\nGuaranteeing Benchmarking Difficulty . Rigorous bench-\nmarking effectively evaluates rapid advancements in MLLMs at the\nforefront of human knowledge [33]. To ensure benchmark difficulty,\ndiverse strategies are implemented: a quality control mechanism,\na vision-language prompt removal approach, and an unspecified\noptions and answers strategy. 1) Quality control mechanism\nemploys a human-MLLM mixed-obfuscation adaptation process.\nThis iterative method starts with human-generated problems, fol-\nlowed by alternating revisions between models and experts until\nthe model fails to respond correctly, ensuring a challenging bench-\nmark. 2) Vision-language prompt removal approach enhances\naffordance comprehension assessment by replacing explicit object\nnames in images, questions, and options with ‘this object.’ This\nmethod replicates real-world multimodal perception environments\ncritical for affordance evaluation while increasing difficulty for\nrobust model assessment. 3) Unspecified options and answers\nstrategy heightens challenge by concealing the exact number of\ncorrect answers from models and participants, thus reducing re-\nliance on guessing while prioritizing deep comprehension. Quanti-\ntative distributions of answers and options appear in Fig. 2.\n3.2 Focused Aspects\nEvaluating affordance perception examines inherent properties\nsuch as structure and material alongside potential dynamic action\ninteractions, encompassing constitutive and transformative affor-\ndance dimensions. Representative examples of these multifaceted\ndimensions are clearly and visually illustrated in Fig. 3.\nConstitutive Affordance Perception .We propose A4Bench ,\na robust benchmark designed to assess affordance perception in\nmultimodal large language models. To evaluate basic affordance\ncomprehension, the Constitutive Affordance component includes\n1282 multimodal question-answer pairs across four disciplines and\nnine sub-disciplines, emphasizing inherent object properties such as\n~1770\n5-Option~155\n2-Option~75\n4-OptionNatural & \nRural\nEnvironments\n~423Medical & Scientific  Environments~245 Urban & Industrial Environments~388 \nFarmland \n& Rural\n~87\nNatural\n~220Urban~170\nTransportation~118\nMisleading\nAffordance\n~187Cross-\nCultural\nAffordance\n~118Individual-Variant \nAffordance\n~203Daily Life \nEnvironments\n~226 \nUnderwater\n/Marine\n~116Industrial\n~100\nTime-\nDependent \nAffordance\n~210 \nLaboratory\n~119 Medical~126\nConstitutive\nAffordance\n~1282\n~718\nTransformative \nAffordance~737\n3-Answer\n~91\n5-Answer~308\n4-Answer~563\n2-Answer\n~311\n1-AnswerQuantitative Distribution of Answers\nQuantitative Distribution of OptionsFigure 2: The focused aspects and the corresponding amount\ndistributions of A4Bench. The focused aspects and the\namount of multimodal question-answer pairs employed are\nshown in leftof the figure. The Quantitative Distribution of\nAnswers and Quantitative Distribution of Options are shown\ninright-top andright-buttom of the figure, respectively.\nshape, size, and material that determine walkability or graspability.\nThese pairs probe static environmental cues critical for accurate af-\nfordance detection. The disciplines Daily Life Environments (226\npairs), Natural and Rural Environments (423 pairs), Urban and\nIndustrial Environments (388 pairs), and Medical and Scien-\ntific Environments (245 pairs) comprise 64.1% of the benchmark.\nAnd the details of sub-discipline are illustrated in Fig. 2.\nTransformative Affordance Perception. To evaluate compre-\nhension of dynamic object potential in MLLMs, the Transforma-\ntive Affordance component of A4Bench employs 718 challeng-\ning multimodal question-answer pairs designed to probe complex\ncontext-sensitive affordance perception across diverse real-world\nscenarios. This component rigorously tests the ability of MLLMs to\ninterpret nuanced interactions beyond static properties, ensuring\nrobust evaluation of adaptive affordance understanding critical for\npractical applications. These pairs, categorized into four distinct\naspects, address high-difficulty dynamic interactions: 1) Mislead-\ning Affordance Perception examines visually deceptive objects\nsuch as pillows resembling fire hydrants or phone cases mimick-\ning slippers, testing perceptual accuracy under visual ambiguity\nin everyday contexts. 2) Time-Dependent Affordance Percep-\ntion investigates affordance evolving over time, exemplified by\na concrete surface transitioning from impassable to walkable af-\nter six months of curing or fruit ripening to edibility, reflecting\ntemporal dynamics in natural processes. 3) Cross-Cultural Af-\nfordance Perception analyzes variations due to cultural contexts,\nwhere a thumbs-up gesture conveys approval in some cultures but\nsignals taxi-hailing in others, highlighting cultural specificity in\nglobal interactions. 4) Individual-Variant Affordance Percep-\ntion explores affordance differing by individual, such as a stroller\nproviding seating for infants but not adults, emphasizing tailored\ninteraction potential across populations with distinct needs.\n--- Page 4 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY Wang et al.\nDaily Life Environments\nQuestion: Which of the following options describe \nthe affordance of this object? \n \nOptions: \n(A) Altering a photo's color, brightness, or style \nto achieve visual effects\n(B) Used for filtering impurities in air purifiers\n(C) Used in photography to control the effect of \nlight entering the lens\n(D) Protecting lens surfaces from dust and \nscratches\n(E) Used for filtering tea leaves or food particles\nCorrect choice: A, C\nNatural & Rural Environments\nQuestion: Which of the following options describe the \naffordance of this object?   \nOptions: \n(A) Used for studying geological history and rock formation\n(B) Serves as a venue for outdoor adventure and extreme \nsports\n(C) Used for constructing hydroelectric power stations\n(D) Serves as a nature reserve and biodiversity research \nsite\n(E) Used for testing sound propagation and echo \nphenomena\nCorrect choice: A, B, C, D, E\nUrban & Industrial Environments\nQuestion: Which of the following options describe the \naffordance of this object? \n  \nOptions: \n(A) Used for real-time communication between \nemployees in factory workshops\n(B) Used as an alarm device in emergency situations\n(C) Used for playing music in high-noise environments\n(D) Used for coordination and communication between \ndifferent trades on construction sites\n(E) Used as an alternative tool for office internal mail\nCorrect choice: A, B, D\nMedical & Scientific Environments\nQuestion: Which of the following options describe the \naffordance of this object? \n  \nOptions: \n(A) Observes fine details of tiny objects or organisms\n(B) Used in scientific research and experiments\n(C) Functions as a magnifying glass\n(D) Takes long-distance photographs\n(E) Detects air quality\nCorrect choice: A, B, C\nMisleading Affordance\nQuestion: Which of the following uses can this \nobject serve? \n \nOptions: \n(A) As an office building\n(B) As a warehouse for storing items\n(C) As gym equipment\n(D) As a tool for moving items\n(E) As a residence\nCorrect choice: A, B, E\nTime-Dependent Affordance\nQuestion: Regarding the affordance of this object, \nwhich of the following judgments are accurate?\nOptions: \n(A) It can be safely touched when the paint is dry\n(B) Special lighting is required for it to dry\n(C) It allows for long-term display after fully drying\n(D) A hairdryer must be used to speed up drying\n(E) It will not fade in any state\nCorrect choice: A, C\nIndividual-Variant Affordance\nQuestion: Regarding the main purpose of this item, \nwhich of the following statements are correct?  \nOptions: \n(A) Specially used for baby travel\n(B) Can be used instead of shopping carts\n(C) Suitable for transporting large items\n(D) Some models can be converted into car seats\n(E) Can be used as a fitness equipment\nCorrect choice: A, D\nCross-Cultural Affordance\nQuestion: Regarding the affordance of this object, which \nof the following judgments are accurate?\nOptions: \n(A) Being a core element of traditional Chinese opera \n(B) This object affords adoption as a standard makeup \ntechnique by drama performances worldwide\n(C) Often referenced in Japanese anime character designs\n(D) Used as daily etiquette makeup in Chinese workplace\n(E) The pattern symbolic features of it are partially \nechoed in traditional Japanese masks\nCorrect choice: A, E\nDirverse\nConstitutive \nCases\nFirst \nAffordance \nBenchmark \nfor \nMLLMsChallenging\nTransformative \nCases2k multi-modal question\n-answer pairs \nAffordance\nis Important\nFigure 3: Typical samples from the A4Bench. Each sample is accompanied by a image-question-answer pair. A4Bench evaluates\nmodels across diverse discplines (Constitutive Affordance) and challenging dimensions (Transformative Affordance), ensuring\na comprehensive evaluation of the affordance perception capabilities.\n3.3 Question Collection\nQuestion Type .In the A4Bench , two distinct question formats\nare employed: Yes-or-No andWhat questions. Yes-or-No questions\n(7.8%) evaluate fundamental judgment capabilities in MLLMs, while\nWhat questions (92.2%) demand deeper affordance comprehension\ndue to their inherent complexity, facilitating a comprehensive and\nrobust assessment of nuanced conceptual understanding.\nHuman Expert Annotation .A team of 60 human annotators,\ncategorized by professional expertise into 10 senior experts, 20 ju-\nnior researchers, and 30 general researchers, develops questions\nforA4Bench . Senior experts formulate transformative questions\nleveraging their extensive experience, while junior and general re-\nsearchers focus on foundational questions, ensuring a comprehen-\nsive benchmark. The annotation process, conducted in controlled\nlaboratory and online settings for consistency, involves sourcing\nor generating relevant images, designing precise questions, and\ndefining their content and structure using specialized knowledge.\nEach question undergoes rigorous review by at least five additional\nexpert annotators, whose critical feedback ensures accuracy, clarity,\nand alignment with the benchmark objectives, safeguarding the\nintegrity and utility of the A4Bench dataset.\nQuestion Response .Specifically, the example input query to\nMLLMs can be exemplified as:\n#User: Which of the following options can describe the affordance\nof this object as shown in the image?\nA. This object can be used as a tool for determining directional position\nB. This object can be used as a tool for measuring precise timeC. Essential navigational equipment for wilderness exploration\nD. Used as a tool for drawing accurate circles in drafting\nE. A small device that operates using Earth’s magnetic field\nAnswer with the option’s letter from the given choices directly.\nDuring evaluation, answer candidates and correct responses are\nrandomized to ensure impartiality. Given that MLLM responses\nmay vary in format (e.g., ‘ C’, ‘Expressionism ’, or ‘ The painting style\nof the image is expressionism ’ for correct choice C), we implement a\nprompt-human progressive choice evaluation technique to rigor-\nously validate the accuracy and consistency of these responses.\n4 EXPERIMENT RESULTS\n4.1 Benchmark Candidates\nIn A4Bench, 9 proprietary MLLMs (closed-source ) and 8 open-\nsource MLLMs are choosen for benchmarking. The Proprietary\nMLLMs (closed-source ) include o3 (2025-04-16) [ 31], ChatGPT-4o\n(latest) [ 30], GPT-4.1 (2025-04-14) [ 32], GPT-4.1-Mini (2025-04-14)\n[32], GPT-4o (2024-11-20) [ 1], Claude-3.5-Sonnet (2024-10-22) [ 3],\nClaude-3.7-Sonnet (2025-02-19) [ 4], Gemini-2.0-Flash [ 17], Gemini-\n2.0-Pro (exp-02-05) [ 14], which are all up-to-date and popular MLLMs.\nTheopen-source MLLMs cover DeepSeek-VL2 [ 38], DeepSeek-\nVL2-Small [ 38], DeepSeek-VL2-Tiny [ 38], InternVL3-14B[ 45], Intern-\nVL3-38B [ 45], MpLUG-OWL3-7B (2024-11-01) [ 40], Qwen2.5-VL-\n32B (Instruct) [ 5], Qwen2.5-VL-72B (Instruct) [ 5], which are all\ncompetitive MLLMs. All MLLMs are tested with zero-shot setting.\nIt is noteworthy that the instruction prompts (e.g., concept and\nformat prompts in this study) may vary slightly across different\nMLLMs, tailored to their respective official configurations.\n--- Page 5 ---\nAffordance Benchmark for MLLMs Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nPerformance (%)Gemini-2.0-Pro (exp-02-05)\nClaude-3.7-Sonnet (2025-02-19)\nChatGPT-4o (latest)\no3 (2025-04-16)\nClaude-3.5-Sonnet (2024-10-22)\nGPT-4.1 (2025-04-14)\nGPT-4o (2024-11-20)\nGemini-2.0-Flash\nQwen2.5-VL-72B (Instruct)\nGPT-4.1-Mini (2025-04-14)\nDeepSeek-VL2\nInternVL3-14B\nInternVL3-38B\nQwen2.5-VL-32B (Instruct)\nDeepSeek-VL2-Small\nDeepSeek-VL2-Tiny\nMpLUG-OWL3-7B (2024-11-01)\nRandom Guess18.05%\n17.63%\n16.78%\n15.99%\n15.69%\n15.61%\n15.52%\n15.40%\n15.12%\n14.59%\n14.34%\n13.72%\n13.58%\n13.52%\n13.48%\n13.47%\n13.06%\n5.55%Comparison of Overall Performance Across Different MLLMs\nProprietary\nOpen-source\nRandom Guess\n(a) Overall results of A4Bench.\n (b) Detailed results of A4Bench.\nFigure 4: A Quick Look of the A4Bench Outcomes. (a) showcases a comparative analysis of the overall match score between\n17 selected MLLMs (both closed-source and open-source), and random guess. (b) displays a radar chart that details the match\nscore performance of the top-7 MLLMs across diverse discplines (Constitutive Affordance) and challenging dimensions\n(Transformative Affordance excluding Misleading dimensions) of A4Bench.\n4.2 Human Performance\nTo assess human performance on A4Bench , a user study is con-\nducted in a controlled laboratory environment with carefully se-\nlected participants. Participants are initially familiarized with the\ntask structure through exposure to representative examples, ensur-\ning comprehension of the evaluation format. They subsequently\nprovide responses to questions presented in A4Bench . To maintain\nexperimental conditions analogous to those of MLLMs, question\norder is randomized, and participants receive only the concept\nprompt, format prompt, questions, and answer options, without\nsupplementary information. Both the bestandworst performance\noutcomes are documented for comparative analysis. This rigorous\nmethodology ensures reliable and results. Statistical measures are\napplied to evaluate response consistency and accuracy.\n4.3 Findings of A4Bench\nProprietary MLLMs vs. Open-Source MLLMs vs. Human .A\nconcise overview of the A4Bench results is provided in Fig. 4,\nwhich offering several key insights: 1) Proprietary MLLMs consis-\ntently outperform their open-source counterparts, with models like\nGemini-2.0-Pro achieving an overall score of 18.05%, surpassing\nthe best open-source model, Qwen2.5-VL-72B, at 15.12%. This gap\nunderscores the advantage proprietary models have, likely due to\naccess to more extensive and diverse training datasets. Although\nthe performance margin is not always vast and the first open-\nsource models demonstrate competitiveness, proprietary systems\ncurrently maintain an advantage in overall affordance understand-\ning as gauged by this benchmark. 2) All evaluated MLLMs show\nsignificantly limited capabilities in affordance perception, which issubstantially below human-level performance. The top-performing\nMLLM, Gemini-2.0-Pro, lags behind the human best by 14.62%, and\neven the human worst at 81.25% surpasses the best MLLMs by 3.39%.\nThis stark disparity highlights that current MLLMs, despite their\nadvancements, struggle to replicate human-level understanding\nof affordance-related tasks, necessitating further development in\nthis area. 3) All models outperform the random guess baseline, in-\ndicating some level of learned capacity, although insufficient for\ncomplex affordance percception and understanding.\nFindings of Constitutive Affordance .In the constitutive affor-\ndance category, which evaluates the ability to perceive affordance in\nstatic contexts, performance varies significantly across disciplines.\n1) Proprietary models demonstrate a clear edge, with ChatGPT-4o\nand GPT-4-1 scoring 16.41% and 18.18% in the Daily discipline ,\nrespectively, indicating robust performance in understanding affor-\ndance in everyday scenarios. Similarly, in the Natural discipline ,\nboth models achieve 10.45%, reflecting an ability to handle natural\nenvironments effectively. However, their performance dips in more\nspecialized disciplines, such as Medical (9.63% for ChatGPT-4o)\nandUrban (10.98% for GPT-4.1), suggesting limitations in domain-\nspecific contextual understanding. 2) In contrast, open-source mod-\nels such as DeepSeek-VL2 and InternVL3-38B demonstrate consis-\ntent yet comparatively lower performance across these disciplines,\nwith scores ranging from 8.65% to 9.63% and 8.60% to 9.30%, re-\nspectively. This uniformity suggests a balanced but less specialized\ncapability relative to proprietary models. 3) All MLLMs demonstrate\nsuboptimal performance relative to human capabilities, exposing\na substantial gap in interpreting constitutive affordance with the\ndepth, nuance, and precision inherent in human understanding.\n--- Page 6 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY Wang et al.\nTable 1: Benchmark results on the A4Bench, displaying Exact match Accuracy , with the best performance marked in bold and\nthe second-best underlined for both proprietary and open-source MLLMs. The Overall score of Constitutive Affordance is the\nweighted average of Daily, Natural, Medical, and Urban discplines. The Overall score of Transformative Affordance averages\nTime-Indenpendent, Cross-Culture, and Individual dimensions. The final Overall score encompasses all pairs.\nCategories Constitutive Affordance Transformative Affordance Overall ↑\nMLLM (MLLM) Daily ↑Natural↑Medical↑Urban↑Overall↑Misleading↑Time-Depen↑Culture↑Individual↑Overall↑\nHuman (Best) 83.63% 87.35% 83.39% 87.50% 85.99% 87.17% 83.67% 84.75% 81.66% 83.14% 85.34%\nHuman (Worst) 79.20% 85.71% 78.83% 81.55% 81.88% 83.96% 81.43% 78.81% 76.03% 78.78% 81.25%\nProprietary MLLMs:\no3 (2025-04-16) 14.64% 11.45% 10.28% 11.37% 11.76% 63.64% 10.95% 10.17% 7.39% 9.42% 15.99%\nChatGPT-4o (latest) 16.41% 10.45% 9.63% 10.98% 11.52% 67.38% 13.33% 16.10% 7.39% 11.68% 16.78%\nGPT-4.1 (2025-04-14) 18.18% 10.45% 9.95% 9.99% 11.57% 68.98% 8.10% 10.17% 2.96% 6.59% 15.61%\nGPT-4.1-Mini (2025-04-14) 12.87% 10.86% 9.45% 9.79% 10.59% 63.10% 7.24% 14.58% 2.81% 7.18% 14.59%\nGPT-4o (2024-11-20) 19.95% 10.45% 8.98% 9.39% 11.49% 65.24% 9.52% 9.32% 4.93% 7.72% 15.52%\nClaude-3.5-Sonnet (2024-10-22) 14.19% 13.71% 9.63% 10.38% 11.92% 61.50% 7.62% 14.41% 6.40% 8.66% 15.69%\nClaude-3.7-Sonnet (2025-02-19) 17.29% 17.80% 11.26% 12.96% 14.86% 45.99% 10.48% 24.58% 12.32% 14.31% 17.63%\nGemini-2.0-Flash 17.29% 10.04% 9.95% 9.59% 11.15% 68.45% 5.71% 11.02% 5.91% 6.97% 15.40%\nGemini-2.0-Pro (exp-02-05) 18.26% 13.71% 14.19% 14.35% 14.82% 67.38% 9.52% 13.56% 4.43% 8.47% 18.05%\nOpen-source MLLMs:\nDeepSeek-VL2 9.33% 9.63% 8.65% 8.79% 9.11% 69.50% 8.10% 12.71% 3.97% 7.54% 14.34%\nDeepSeek-VL2-Small 9.77% 8.82% 8.65% 8.79% 8.95% 65.78% 6.19% 11.86% 2.46% 6.03% 13.48%\nDeepSeek-VL2-Tiny 9.77% 8.82% 8.65% 8.79% 8.95% 65.78% 6.13% 11.80% 2.43% 5.98% 13.47%\nInternVL3-14B 8.44% 8.41% 9.30% 9.19% 8.84% 60.43% 11.43% 15.25% 2.96% 9.04% 13.72%\nInternVL3-38B 9.33% 9.63% 9.30% 8.60% 9.17% 62.03% 7.62% 13.56% 2.96% 7.16% 13.58%\nMpLUG-OWL3-7B (2024-11-01) 10.21% 9.63% 8.33% 9.79% 9.54% 57.75% 7.62% 11.02% 0.99% 5.84% 13.06%\nQwen2.5-VL-32B (Instruct) 8.88% 10.04% 9.30% 8.40% 9.15% 66.31% 7.14% 8.47% 1.97% 5.46% 13.52%\nQwen2.5-VL-72B (Instruct) 10.65% 13.31% 9.95% 9.98% 11.10% 64.71% 8.10% 9.32% 5.42% 7.34% 15.12%\nRandom guess 1.33% 1.55% 1.22% 2.60% 1.79% 44.92% 1.42% 0.85% 0.05% 0.77% 5.55%\nFindings of Transformative Affordance .In the transforma-\ntive affordance category category evaluates the understanding of\ndynamic and contextually shifting affordance. The overall score\nfor this category is calculated as a weighted average of the Time-\nIndependent, Cross-Culture, and Individual dimensions. The exclu-\nsion of the Misleading dimension arises because its random guess\nrate is notably high, necessitating the subtraction of this baseline\nto accurately reflect the model true performance in that dimension.\nPerformance disparities across dimensions emerge, as depicted in\nTab. 1. Specifically, 1) proprietary models like Gemini-2.0-Pro lead\nwith a Time-Dependent score of 15.2% and a Culture score of\n13.56%, demonstrating some proficiency in handling temporal trans-\nformations and cultural nuances. However, they struggle signifi-\ncantly in the Misleading dimension (67.38%) and the Individual\ndimension (4.43%), indicating challenges in detecting deceptive af-\nfordance and interpreting individual-specific contexts. Similarly,\nClaude-3.5-Sonnet achieves a balanced performance with 14.86%\noverall but shows weaknesses in Individual at 6.40%. 2) Open-\nsource models, such as Qwen2.5-VL-72B, display more uniform\nbut lower scores across these dimensions, ranging from 8.10% in\nMisleading to 9.32% in Culture , with a particularly poor Indi-\nvidual score of 5.42%. This suggests that open-source models lack\nthe depth required for nuanced transformative affordance percep-\ntion. 3) In comparison, human performance remains consistently\nhigh, ranging from 83.67% (Time-Dependent) to 84.75% (Culture),highlighting a significant gap. The underperformance of MLLMs in\ntransformative affordance tasks, especially in dimensions requiring\ncomplex reasoning like Misleading andIndividual , highlights\nthe imperative for enhanced training strategies to improve their\ncapacity to address dynamic and subjective affordance scenarios.\n5 CONCLUSION\nThis paper introduces A4Bench , a novel and comprehensive bench-\nmark featuring 2,000 multimodal question-answer pairs, specifi-\ncally designed to systematically assess the affordance perception\ncapabilities of MLLMs across two critical dimensions: constitu-\ntive affordance and transformative affordance. Our comprehensive\nevaluation of 17 MLLMs compared to human performance reveals\nsubstantial limitations in current model capabilities. All models\n(including leading proprietary ones) significantly underperform\nrelative to human understanding, particularly in the nuanced do-\nmain of transformative affordance. These findings underscore the\nsubstantial challenges MLLMs currently face in achieving robust\nenvironmental interaction understanding. Overcoming these de-\nficiencies is thus crucial for developing MLLMs capable of more\nsophisticated and safer real-world interaction. By this A4Bench\nbenchmark, we offer a robust foundational tool and clear roadmap\nto guide future research towards developing MLLMs with more\ncontext-aware and ultimately safer interaction capabilities.\n--- Page 7 ---\nAffordance Benchmark for MLLMs Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nREFERENCES\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2]Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra\nChaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh\nGarg, Theophile Gervet, et al .2024. Pixtral 12B. arXiv preprint arXiv:2410.07073\n(2024).\n[3]Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku. https:\n//www.anthropic.com/news/claude-3-family.\n[4]Anthropic. 2025. Claude 3.7 Sonnet. https://www.anthropic.com/news/claude-3-\n7-sonnet.\n[5]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai\nDang, Peng Wang, Shijie Wang, et al .2025. Qwen2.5-VL Technical Report.\narXiv:2502.13923 [cs.CV] https://arxiv.org/abs/2502.13923\n[6] BailingMM-Pro-0120 2025. https://github.com/wwbin2017/bailing/.\n[7] Randolph Blake and Maggie Shiffrar. 2007. Perception of human motion. Annu.\nRev. Psychol. 58, 1 (2007), 47–73.\n[8]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen,\nHaodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. 2024. Are We\non the Right Way for Evaluating Large Vision-Language Models?. In Advances\nin Neural Information Processing Systems , A. Globerson, L. Mackey, D. Belgrave,\nA. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates,\nInc., 27056–27087. https://proceedings.neurips.cc/paper_files/paper/2024/file/\n2f8ee6a3d766b426d2618e555b5aeb39-Paper-Conference.pdf\n[9]Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang,\nPeiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. 2024. PCA-Bench:\nEvaluating Multimodal Large Language Models in Perception-Cognition-Action\nChain. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-\nWei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational\nLinguistics, Bangkok, Thailand, 1086–1104. doi:10.18653/v1/2024.findings-acl.64\n[10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie,\nXingkai Yu, and Chong Ruan. 2025. Janus-Pro: Unified Multimodal Understanding\nand Generation with Data and Model Scaling. arXiv preprint arXiv:2501.17811\n(2025).\n[11] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker,\nTuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024.\nNvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402\n(2024).\n[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP:\nTowards General-purpose Vision-Language Models with Instruction Tuning.\narXiv preprint arXiv:2305.06500 (2023). arXiv:2305.06500 [cs.CV] https://arxiv.\norg/abs/2305.06500 Originally announced May 2023.\n[13] Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, and Jiao Ran.\n2025. Scalable vision language model training via high quality data curation.\narXiv preprint arXiv:2501.05952 (2025).\n[14] Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al .2024. Gemini 1.5:\nUnlocking Multimodal Understanding Across Millions of Tokens of Context.\narXiv preprint arXiv:2403.05530 (2024).\n[15] James J Gibson. 2014. The ecological approach to visual perception: classic edition .\nPsychology press.\n[16] Google. 2024. Gemini. https://gemini.google.com. Large language model.\n[17] Google. 2024. Introducing Gemini 2.0: Our New AI Model for the Agentic\nEra. https://blog.google/technology/google-deepmind/google-gemini-ai-update-\ndecember-2024/.\n[18] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi\nFan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al .2025. Seed1. 5-vl technical\nreport. arXiv preprint arXiv:2505.07062 (2025).\n[19] Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Sunil Aryal, Imran Razzak, and\nHakim Hacid. 2025. Visual Question Answering: From Early Developments to\nRecent Advances – A Survey. arXiv preprint arXiv:2501.03939 (2025). https:\n//arxiv.org/abs/2501.03939 cs.CV, cs.MM.\n[20] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang,\nand Ying Shan. 2024. SEED-Bench: Benchmarking Multimodal Large Language\nModels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) . 13299–13308.\n[21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen\nZhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al .2024. Llava-onevision: Easy\nvisual task transfer. arXiv preprint arXiv:2408.03326 (2024).\n[22] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao,\nJan Kautz, Mohammad Shoeybi, and Song Han. 2023. VILA: On Pre-training for\nVisual Language Models. arXiv:2312.07533 [cs.CV]\n[23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Im-\nproved Baselines with Visual Instruction Tuning. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition . IEEE, 26296–\n26306. https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_\nBaselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html CVPR 2024\nHighlight.\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-\ntion Tuning. In Advances in Neural Information Processing Systems , Vol. 36. Curran\nAssociates, Inc., 26924–26958. https://proceedings.neurips.cc/paper_files/paper/\n2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html\nNeurIPS 2023 Oral Presentation.\n[25] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. 2024. Mm-\nsafetybench: A benchmark for safety evaluation of multimodal large language\nmodels. In European Conference on Computer Vision . Springer, 386–403.\n[26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo\nZhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al .2024. Mmbench:\nIs your multi-modal model an all-around player?. In European conference on\ncomputer vision . Springer, 216–233.\n[27] Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou.\n2024. Points: Improving your vision-language model with affordable strategies.\narXiv preprint arXiv:2409.04828 (2024).\n[28] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and\nHan-Jia Ye. 2024. Ovis: Structural embedding alignment for multimodal large\nlanguage model. arXiv preprint arXiv:2405.20797 (2024).\n[29] MUG-U-7B 2025. MUG-U. https://github.com/Shopee-MUG/MUG-U/.\n[30] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\n[31] OpenAI. 2025. Introducing OpenAI o3 and o4-mini. https://openai.com/index/\nintroducing-o3-and-o4-mini/.\n[32] OpenAI. 2025. OpenAI Models Documentation. https://platform.openai.com/\ndocs/models.\n[33] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang,\net al.2025. Humanity’s Last Exam. arXiv:2501.14249 [cs.LG] https://arxiv.org/\nabs/2501.14249\n[34] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun\nChao, and Yu Su. 2023. LLM-Planner: Few-Shot Grounded Planning for Embodied\nAgents with Large Language Models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) . https://arxiv.org/abs/2212.04088\n[35] step-1o 2024. http://www.eecs.harvard.edu/mdw/ proj/codeblue/.\n[36] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang,\nCheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al .2025. Kimi-vl\ntechnical report. arXiv preprint arXiv:2504.07491 (2025).\n[37] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng\nPan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al .2024. Janus: Decoupling\nvisual encoding for unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848 (2024).\n[38] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai\nDai, Huazuo Gao, Yiyang Ma, et al .2024. DeepSeek-VL2: Mixture-of-\nExperts Vision-Language Models for Advanced Multimodal Understanding.\narXiv:2412.10302 [cs.CV] https://arxiv.org/abs/2412.10302\n[39] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi\nCai, Haoyu Li, Weilin Zhao, Zhihui He, et al .2024. MiniCPM-V: A GPT-4V Level\nMLLM on Your Phone. arXiv preprint arXiv:2408.01800 (2024).\n[40] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei\nHuang, and Jingren Zhou. 2024. mPLUG-Owl3: Towards Long Image-Sequence\nUnderstanding in Multi-Modal Large Language Models. arXiv:2408.04840 [cs.CV]\nhttps://arxiv.org/abs/2408.04840\n[41] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao\nZhang, et al .2024. MMT-bench: a comprehensive multimodal benchmark for\nevaluating large vision-language models towards multitask AGI. In Proceedings of\nthe 41st International Conference on Machine Learning (Vienna, Austria) (ICML’24) .\nJMLR.org, Article 2359, 83 pages.\n[42] Bo Zhang, Shuo Li, Runhe Tian, Yang Yang, Jixin Tang, Jinhao Zhou, and Lin\nMa. 2025. Flash-VL 2B: Optimizing Vision-Language Model Performance for\nUltra-Low Latency and High Throughput. arXiv preprint arXiv:2505.09498 (2025).\n[43] Zicheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang.\n2024. A-Bench: Are LMMs Masters at Evaluating AI-Generated Images? arXiv\npreprint arXiv:2406.03070.\n[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\n2023. MiniGPT-4: Enhancing Vision-Language Understanding with Ad-\nvanced Large Language Models. arXiv preprint arXiv:2304.10592 (2023).\narXiv:2304.10592 [cs.CV] https://arxiv.org/abs/2304.10592 Originally announced\nApril 2023.\n[45] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu,\nHao Tian, Yuchen Duan, et al .2025. InternVL3: Exploring Advanced Training and\nTest-Time Recipes for Open-Source Multimodal Models. arXiv:2504.10479 [cs.CV]\nhttps://arxiv.org/abs/2504.10479",
  "text_length": 44354
}