{
  "id": "http://arxiv.org/abs/2506.05209v1",
  "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
  "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
  "authors": [
    "Nikhil Kandpal",
    "Brian Lester",
    "Colin Raffel",
    "Sebastian Majstorovic",
    "Stella Biderman",
    "Baber Abbasi",
    "Luca Soldaini",
    "Enrico Shippole",
    "A. Feder Cooper",
    "Aviya Skowron",
    "John Kirchenbauer",
    "Shayne Longpre",
    "Lintang Sutawika",
    "Alon Albalak",
    "Zhenlin Xu",
    "Guilherme Penedo",
    "Loubna Ben Allal",
    "Elie Bakouch",
    "John David Pressman",
    "Honglu Fan",
    "Dashiell Stander",
    "Guangyu Song",
    "Aaron Gokaslan",
    "Tom Goldstein",
    "Brian R. Bartoldson",
    "Bhavya Kailkhura",
    "Tyler Murray"
  ],
  "published": "2025-06-05T16:21:30Z",
  "updated": "2025-06-05T16:21:30Z",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05209v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05209v1  [cs.CL]  5 Jun 2025The Common Pile v0.1: An 8TB Dataset of Public\nDomain and Openly Licensed Text\nNikhil Kandpal∗1,2Brian Lester∗1,2Colin Raffel∗1,2,3\nSebastian Majstorovic4Stella Biderman4Baber Abbasi4Luca Soldaini5Enrico Shippole6\nA. Feder Cooper†7Aviya Skowron4John Kirchenbauer8Shayne Longpre9Lintang\nSutawika4,10Alon Albalak‡11Zhenlin Xu12Guilherme Penedo3Loubna Ben Allal3Elie\nBakouch3John David Pressman4Honglu Fan4,13Dashiell Stander4Guangyu Song4Aaron\nGokaslan7Tom Goldstein8Brian R. Bartoldson14Bhavya Kailkhura14Tyler Murray5\n1University of Toronto2Vector Institute3Hugging Face4EleutherAI5The Allen Institute for\nArtificial Intelligence6Teraflop AI7Cornell University8University of Maryland, College Park\n9MIT10CMU11Lila Sciences12Independent13poolside14Lawrence Livermore National\nLaboratory\nAbstract\nLarge language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible intellectual\nproperty infringement and ethical concerns. Training LLMs on openly licensed\ntext presents a first step towards addressing these issues, but prior data collection\nefforts have yielded datasets too small or low-quality to produce performant LLMs.\nTo address this gap, we collect, curate, and release the Common Pile v0.1, an\neight terabyte collection of openly licensed text designed for LLM pretraining.\nThe Common Pile comprises content from 30 sources that span diverse domains\nincluding research papers, code, books, encyclopedias, educational materials,\naudio transcripts, and more. Crucially, we validate our efforts by training two\n7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and\nComma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models\nattain competitive performance to LLMs trained on unlicensed text with similar\ncomputational budgets, such as Llama 1 and 2 7B. In addition to releasing the\nCommon Pile v0.1 itself, we also release the code used in its creation as well as\nthe training mixture and checkpoints for the Comma v0.1 models.\n1 Introduction\nA critical stage of large language model (LLM) development is pretraining [ 72,136,142], where an\nLLM is trained to predict the next token (i.e., word or subword unit) in a corpus of unstructured text.\nPretraining is widely regarded as the foundation for strong downstream performance, as it enables\nLLMs to learn the structure of natural language [ 32,110,154] and accumulate a broad base of world\nknowledge [ 133,152]. In an effort to push the capabilities of LLMs, pre-training datasets have grown\nsteadily over time [ 143], with modern datasets containing trillions of tokens [ 132,167,193]. To meet\nthis increasing demand for pre-training data, the de facto approach has been to leverage the public\nInternet as a source of text [57, 95, 108, 132, 142].\nWhile the web provides a diverse and continuously growing supply of text, much of this content—\nunder most legal frameworks—is protected by copyright. Yet, this text is routinely used to pretrain\n∗Equal contribution. For a list of author contributions, see Appendix A.†Work done while a graduate\nstudent at Cornell University.‡Work done while at SynthLabs.\nPreprint.\n--- Page 2 ---\nStack V2\nPEPs\nUSPTO\nCAP\nUSGPO\nUK Hansard\nRegulations.gov\nWikiteam\nWikimedia\nCCCC\nNews\nFoodista\nPDR\npeS2o\nPubMed\nArXiv Papers\nArXiv Abstracts\nStack Exchange\nGitHub Archive\nUbuntu IRC\nBHL\nPre-1929 Books\nLibrary of Congress\nProject Gutenberg\nCC YouTube\nDPI\nDOAB\nPressBooks\nLibreTexts\nOER Commons1MB100MB10GB1TBDataset SizeCode\n(4775 GB) Government & Legal\n(1172 GB)Wikis\n(528 GB)Web\n(260 GB)Academic\nPapers\n(370 GB)Online Forums\n(165 GB)Public Domain\nBooks\n(244 GB) Other\n(29 GB)Educational\nResources\n(15 GB)Figure 1: The Common Pile is an 8TB dataset of openly licensed text curated from 30 diverse\nsources. The sources comprising the Common Pile are shown above, categorized by textual domain.\nLLMs, often without compensation to the creators of this content. Recent estimates suggest that\ncompensating the authors of pre-training data, even at conservatively low wage rates, would cost\nbillions of US dollars [ 82]. While copyright exemptions for text and data mining exist in some\njurisdictions [ 69,79,92,130,156], many rights holders have objected to the uncompensated use of\ntheir work, resulting in numerous lawsuits against LLM developers [ 24,191] that could carry financial\ndamages in the billions [ 40,96,159]. Beyond questions of intellectual property (IP) law, the use of\nweb-scraped data also raises ethical concerns [ 9], as content creators rarely explicitly consent to the\ndownstream use of their work for LLM training. In fact, recent evidence suggests that many content\nowners may notconsent to its use as LLM training data, as shown by a sharp mid-2023 increase in\nwebsites blocking AI crawlers [ 107], following growing awareness of web data being used to train\nmodels. Finally, while open models trained on publicly released pre-training datasets [ 18,64,103]\nsupport research into the study of learning dynamics [ 50,76,84], memorization [ 17,22], data\nauditing [ 47,128,145], and more, the use of unlicensed training data heavily limits the ability of\nmodel trainers to share their datasets, and has previously resulted in DMCA takedowns of datasets\nsuch as the Pile [57].\nThe current landscape reflects a growing divide between LLM developers and content creators. We\nsubmit that a natural first step toward resolving this tension is to ask: Is it possible to train performant\nlanguage models using only public domain and openly licensed text? We define “openly licensed”\ntext as content that follows the Open Knowledge Foundation’s Open Definition 2.1 (further detailed\nin section 2 and Appendix C), which refers to content where the copyright holder has granted explicit\npermission for the content to be freely accessed, used, modified, and shared for any purpose. Our\nprimary contribution in this paper is to demonstrate that this is indeed possible by collecting, curating,\nand releasing the Common Pile v0.1 , an 8TB dataset that—to our knowledge—constitutes the largest\ncollection of openly licensed text to date. The Common Pile comprises 30 text sources (detailed in\nsection 3), covering diverse domains including research publications, open-source code, government\ndocuments, historical books, educational resources, audio transcripts, and more. Crucially, we\ndemonstrate that after appropriate filtering, deduplication, and reweighting, the Common Pile v0.1\ncan be used as the foundation for competitive LLMs. Specifically, we train Comma v0.1-1T and\nComma v0.1-2T, a pair of 7-billion-parameter models with comparable performance to budget-\nmatched models trained on unlicensed datasets such as Llama 1 and 2 7B. In the spirit of openness\nand transparency, we release the Common Pile v0.1, both Comma v0.1 models and their filtered and\ndeduplicated pre-training dataset, and all data collection and processing code.\n2 What do we mean by “openly licensed”?\nCopyright law grants content creators certain rights, such as exclusive rights (with certain exceptions)\nto reproduce, distribute, and create derivatives of their original works. Although copyright laws vary\nacross jurisdictions, original, creative works (that are “fixed” in a tangible medium, such as physically\nor digitally [see, e.g., 1]) typically fall within the scope of copyright. Works in the public domain [38]\nhave had their copyrights expire (after a legally dictated time period), were never eligible for copyright\nprotection due to specific carve-outs (e.g., government documents in the U.S. [ 2]), or were otherwise\ndedicated to the public domain by their copyright owners (e.g., with a CC0 license [ 35]). Copyright\n2\n--- Page 3 ---\nowners can license their protected works, allowing others to adapt and reuse them under specified\nterms. For example, Creative Commons (CC) Licenses (except CC0) grant the right to “reproduce\nand Share the Licensed Material, in whole or in part; and produce, reproduce, and Share Adapted\nMaterial” [ 36]. For a more in-depth and accessible discussion about licenses and generative AI, see\nLee et al. [96, Parts II.I–II.J].\nFor the Common Pile, we collect and curate public domain and openly licensed text, where we\nconsider “openly licensed” to mean any license that meets the Open Knowledge Foundation’s Open\nDefinition 2.1. Some prominent examples of licenses that are considered to be “open” under this\ndefinition include CC BY [ 37], CC BY-SA [ 39], and software licenses certified by the Blue Oak\nCouncil (e.g., the MIT license) [ 20]. We note that CC NC (non-commercial) and ND (no derivatives)\nlicenses are not considered open under this definition and we therefore do not include content\ndistributed under these licenses. While the use of an open license does not necessarily imply that the\nrights holder has specifically contemplated use of their content to train LLMs, most open licenses\ninclude text like “the above rights may be exercised in all media and formats whether now known or\nhereafter devised” [ 37]. Overall, we consider our use of openly licensed data to be a substantial first\nstep towards ethical pre-training dataset curation.\n2.1 License due diligence\nLicense laundering There is a large quantity of data on the internet with incorrect, ambiguous, or\nmissing licensing metadata [ 96,106]. A common pitfall is “license laundering,” where a copyrighted\nwork is redistributed (typically by a non-rights holder) with an incorrect license. License laundering\ncan undermine our ability to confidently source openly licensed content since it implies that we\ncannot always trust the license distributed with a piece of content. To address this issue, we set\nstrict standards for data sourcing, only including data from sources where we were confident that\nthe licensing information was provided by the copyright holder, which ultimately led us to exclude\ncertain sources such as OpenAlex [ 80,126], YouTube Commons [ 74], and the Hacker News dataset\non Kaggle.\nUse of collection licenses A related issue is the licensing status of compilations of existing works.\nMany training corpora are released under open licenses, but these licenses do not necessarily align\nwith the licensing status of the underlying documents [ 96, Part II.A]. As an example, the ODC-By\nlicense has been commonly used for large-scale web corpora such as Dolma [ 167], FineWeb [ 132],\nand TxT360 [ 173]. ODC-By, by definition, does not extend to individual documents within the\ncorpus; therefore, the copyright of documents in these collections is still controlled by the document\nauthors, and does notimply that the text itself is openly licensed.\nLLM-generated synthetic datasets Datasets containing text generated by LLMs trained on un-\nlicensed data have been released under open licenses [e.g. 195]. It has not yet been established\nwhether it is permissible to apply arbitrary licenses to the generations of an LLM that was trained on\nunlicensed data [ 96]. We therefore take a conservative stance and avoid synthetic content that was\ngenerated by an LLM.\nCaveats Despite our best efforts at due diligence, data that falls outside of our curatorial principles\nand choices may have still ended up in our dataset. License laundering is a notoriously hard\nproblem to identify exhaustively in practice [ 96]. Copyright owners may also change the license they\nassociate with their content. Since we collected and curated the Common Pile v0.1 in late 2024, the\nlicensing information we include and rely on may not be completely aligned with more recent updates.\nFurther, some documents that we collect that are in the public domain or are openly licensed may\ncontain material with unclear status (e.g., quoted snippets of in-copyright books in public domain\nU.S. government publications). Finally, we note that while it is relatively straightforward to obey\nattribution requirements when redistributing data, attributing model predictions back to the training\ndata points that influenced them remains an active area of research [129, 28].\n2.2 Comparisons with related work\nOur work is not the first that aims to construct a dataset of openly licensed and/or public domain data\nfor the purposes of training machine learning models. Past efforts include CommonCanvas [ 61], a\ncollection of approximately 70 million Creative Commons-licensed images designed for training\nimage generation models, the PG19 dataset [ 140] of public domain novels sourced from Project\n3\n--- Page 4 ---\nGutenberg used for benchmarking language models, the C4Corpus tools for sourcing Creative\nCommons text from Common Crawl snapshots [ 68], and many datasets comprising CC BY-SA-\nlicensed text from Wikipedia [66, 115].\nMore relevant to our work are the recent Open License Corpus (OLC) [ 119], Common Corpus [ 74,91],\nand KL3M [ 78] datasets, which were constructed for use as LLM pre-training data. On the whole,\nOLC uses similar selection criteria to ours, including text that is in the public domain or is openly\nlicensed. However, OLC also includes conversations scraped from Hacker News, which does not have\nan open license. Additionally, OLC is considerably smaller than the Common Pile v0.1, comprising\ndata from 12 sources (vs. 30 for Common Pile) totaling 0.85 TB of text (vs. 7.6 TB for Common\nPile). Common Corpus also uses a similar set of allowable licenses/copyright statuses (e.g. CC\nBY , CC BY-SA, public domain, MIT-style, etc.) although the specific licenses/statuses are not\nclear because Common Corpus does not retain full per-document licensing information across all\nsources. Additionally, Common Corpus incorporates data from OpenAlex [ 126] which is known\nto provide inaccurate licensing information [e.g., 80]. Furthermore, while the Common Pile and\nCommon Corpus are similar in size (7.6 TB vs. 7.4 TB), Common Corpus targets a broader set of\nlanguages and therefore contains significantly less English text. Conversely, KL3M does not consider\nCC BY-SA to be acceptable and, as a result, almost exclusively consists of government documents.\nAccordingly, the Common Pile is much larger than KL3M (3 TB), and is built from significantly\nmore diverse data sources (Figure 1 & Section 3). In subsection 4.3, we compare the Common Pile\nv0.1 to these datasets in a controlled setting, ultimately showing that it produces substantially more\nperformant LLMs.\n3 Composition of the Common Pile\nThe Common Pile comprises content drawn from a wide range of domains, including scholarly\npublications, government documents, online discussions, books, open educational resources, and\nmore. In this section, we provide an overview of each of the domains contained in the Common Pile\nand briefly discuss their constituent data sources. In-depth discussion of each source is provided in\nAppendix B.\nScientific and scholarly texts , which are often distributed under open licenses due to open access\nmandates, appear in many LLM pre-training datasets [e.g. 57,167,185] since they expose models to\ntechnical terminology, formal reasoning, and long-range document structure. To attain broad coverage\nof scholarly text, we filter peS2o [ 166] (a collection text extracted from open-access scientific PDFs\nbased on S2ORC [ 104]) to only retain openly licensed research papers. For medical-domain text,\nwe collect text from openly licensed articles in the U.S. National Institutes of Health’s National\nLibrary of Medicine’s PubMed Central archive. Additionally, we collect data from ArXiv, which\ncontains over 2.4 million articles in the quantitative sciences, most of which are uploaded as LaTeX\nsource and may be distributed under various licenses chosen by a given article’s author. We include\nopenly licensed articles sourced from ArXiv’s bulk-access S3 bucket and parsed using L ATEXML\nand Trafilatura [ 10]. Furthermore, according to ArXiv’s licensing policy, all metadata (including\nabstracts) of articles posted to ArXiv are distributed under the CC0 license; we therefore include the\nabstracts for allArXiv papers in the Common Pile, regardless of the paper’s full-text license.\nOnline discussion forums comprise multi-turn question-answer pairs and discussions and therefore\ncan be useful for training language models to follow conversational structure as well as for improving\nperformance on question answering and dialogue-centric tasks. StackExchange is a collection of\nwebsites that host user-provided questions and answers and allow their redistribution under a CC\nBY-SA license. We leverage the user-provided StackExchange dumps from the Internet Archive and\nformat questions/answers in the same order they appear on StackExchange, using PyMarkdown to\nconvert each comment into plain text. Additionally, we collect text from issues, pull requests, and\ncomments on GitHub, which, according to GitHub’s terms of service, inherit the license of their\nassociated repository. We extract this content from repositories with Blue Oak Council-approved\nlicenses from the GitHub Archive. Finally, we include logs of all discussions on the Ubuntu-hosted\nInternet Relay Chat (IRC) since 2004, which are released into public domain.\nGovernment and legal texts are often published directly into the public domain or under open\nlicenses. For example, in the US, text written by federal government employees is considered to be in\nthe public domain. We therefore include all plain-text documents made available through the United\n4\n--- Page 5 ---\nStates Government Publishing Office (USGPO)’s GovInfo.gov developer API. Additionally, we\ninclude all plain text regulatory documents published by U.S. federal agencies from Regulations.gov,\nan online platform that hosts newly proposed rules and regulations from federal agencies. The\nCommon Pile also incorporates US Patents and Trademark Office (USPTO) patent documents\nsourced from the Google Patents Public Data dataset [ 77], containing millions of public domain\npatents and published patent applications dating back to 1782. Similarly, the Hansard (the official\nrecord of parliamentary proceedings) of the United Kingdom is distributed under the Open Parliament\nLicense, which stipulates similar terms to the CC BY license. We source UK Hansard data from\nParlParse [ 131], covering Commons debates from 1918 forward and Lords proceedings from the\n1999 reform. For legal text, we leverage the Caselaw Access Project (comprising 40 million pages of\nU.S. federal and state court decisions and judges’ opinions from the last 365 years) and Court Listener\n(including 900 thousand cases scraped from 479 courts). Only legal texts in the public domain were\nselected for the Common Pile.\nCurated task datasets are typically designed for fine-tuning on specific downstream tasks such as\nquestion answering, summarization, or text classification. To source datasets that are distributed\nunder an open license and only contain content owned by the dataset’s rights holder (to avoid license\nlaundering), we use metadata and redistributed datasets from the Data Provenance Initiative [ 106,109].\nFull details on the datasets we include are available in Appendix D.\nBooks , particularly historic text, can fall into the public domain due to copyright expiration—for\nexample, in the United States, books published prior to 1929 are currently in the public domain. We\nsource public domain books from various sources, including the Biodiversity Heritage Library (BHL),\nan open-access digital library for biodiversity literature and archives; pre-1929 books digitized by the\nInternet Archive on behalf of HathiTrust member libraries; the collection of public domain books\ncalled “Selected Digitized Books” released by the Library of Congress; and select books from Project\nGutenberg, an online collection of over 75,000 digitized books, most of which are in the public\ndomain.\nOpen Educational Resources (OERs) are educational materials (e.g. textbooks, lecture notes, lesson\nplans, etc.), typically published under Creative Commons licenses that support free and equitable\naccess to education. We collect data from multiple OER repositories, including the Directory of\nOpen Access Books (DOAB), an online index of over 94,000 peer-reviewed books curated from\ntrusted open-access publishers; PressBooks, a searchable catalog of over 8,000 open access books;\nOERCommons, an online platform where educators share open-access instructional materials; and\nLibreTexts, a catalog of over 3,000 open-access textbooks.\nWikis are topic- or domain-specific encyclopedic websites that are collaboratively written, maintained,\nand moderated. Historical and cultural precedent has led many wikis to have an open license. We\ndownloaded the official database dumps of wikitext (Mediawiki’s custom markup language) of\nthe English-language wikis that are directly managed by the Wikimedia foundation and converted\nwikitext to plain text using wtf_wikipedia . For wikis not managed by Wikimedia, we make use of\nwikiteam’s unofficial database dumps and apply the same conversion process.\nSource code has proven to be a useful part of LLM pre-training corpora, not only to support coding\nabilities but also to improve reasoning [ 7,113,120]. Due to the Free and Open Source Software\n(FOSS) movement, a great deal of source code is distributed with an open license. We leverage\nprior work done by the Software Heritage Foundation and BigCode to compile the openly licensed\nsubset of the Stack V2 [ 113], based on the license detection performed by the creators of Stack\nV2. Additionally we collected all Python Enhancement Proposals (PEPs)—design documents that\ngenerally provide a technical specification and rationale for new features of the Python programming\nlanguage—that were released into the public domain.\nYouTube allows users to upload content under a CC BY license. We therefore sourced and transcribed\nspeech-heavy CC BY videos from YouTube. To avoid license laundering and focus on high-quality\nspeech-based textual content, we manually curated a set of over 2,000 YouTube channels that release\noriginal openly licensed content containing speech. From these channels, we retrieved and transcribed\n(using Whisper [ 138]) over 1.1 million openly licensed videos comprising more than 470,000 hours\nof content.\nWeb text is a common source of LLM pre-training data. A small fraction of content on the web\nis distributed under open licenses. To recover a portion of this content, we process 52 Common\n5\n--- Page 6 ---\nCrawl snapshots using a regular expression (regex) adapted from the C4Corpus project [ 68] to retain\npages that include a CC BY, CC BY-SA, or CC0 marker. This regex naturally results in many false\npositives (e.g., it would retain a page that included and provided attribution for a CC BY image but\notherwise contained unlicensed content), so we manually verified the top 1000 domains by content\nvolume, retaining only those for which all content was assigned a Creative Commons license. Text\nwas extracted using a pipeline similar to the one used for Dolma [ 167]. We provide more details\non the composition of our web-sourced text, called CCCC, in Appendix G. Apart from CCCC,\nwe additionally manually collected data from a few select sites, including Foodista, a community-\nmaintained site with recipes and food-related news as well as nutrition information; news sites that\npublish content under CC BY or CC BY-SA according to Open Newswire; and the Public Domain\nReview, an online journal dedicated to exploration of works of art and literature that have aged into\nthe public domain.\n4 Assessing the Common Pile v0.1’s quality\nThe utility of an LLM pre-training dataset is mostly assessed in terms of whether or not it can be\nused to train performant LLMs. To validate our efforts in curating the Common Pile, we use it as\nthe basis of an LLM pre-training dataset created through additional filtering (subsection 4.1) and\nrebalancing (subsection 4.2). Then, we perform a controlled data ablation study (subsection 4.3)\nwhere we train otherwise-identical LLMs on different pre-training datasets, including prior datasets\ncomprised of openly licensed text mentioned in subsection 2.2 as well as a selection of representative\npre-training datasets of unlicensed text. Finally, we train Comma v0.1-1T and Comma v0.1-2T, 7\nbillion parameter LLMs trained on 1 and 2 trillion tokens (respectively) of Common Pile-sourced\ncontent, and compare them to models with a similar parameter count and training budget that were\ntrained on unlicensed text (subsection 4.4).\n4.1 Dataset preprocessing and filtering\nBefore training a language model, it is considered important to “clean” data in hopes of retaining only\nhigh-quality text under some notion of quality [ 4,105]. Consequently, before training on data from\nthe Common Pile (which is distributed in a relatively “raw” format), we independently preprocessed\neach of the Common Pile’s non-code datasets using pipelines implemented with the Dolma data\nprocessing toolkit [167].\nSince the Common Pile v0.1 focuses primarily on English content, we apply language identification\nusing a FastText classifier [ 81] to filter out non-English text. When processing web text from\nCCCC, we employ the text quality classifier adapted from DataComp-LM [ 99] with an extremely\nlow threshold to remove noisy text. We remove documents with pervasive OCR errors using the\nlikelihood-based filtering approach from [ 166], which removes documents that are assigned an\nexcessively low log-likelihood under a unigram language model constructed from the Trillion Word\nCorpus [ 117]. To reduce the prevalence of toxic or inappropriate content, we apply a pair of FastText\ntoxicity classifiers implemented in Dolma [ 167] that were trained on the Jigsaw Toxic Comment\nClassification Challenge dataset [ 30]. We apply regex-based personally identifiable information\n(PII) redaction to remove email addresses, phone numbers, and IP addresses, and replace them\nwith <EMAIL_ADDRESS> ,<PHONE_NUMBER> , and <IP_ADDRESS> respectively. Finally, we perform\nsource-specific regex filtering to remove repetitive or boilerplate text (e.g., page numbers, document\npreambles, license statements, etc.). For a detailed breakdown of the pre-processing applied to each\ndataset, see Table 5 (appendix).\nAfter filtering, we perform global document-level fuzzy deduplication across all sources, as excessive\ndata duplication is known to harm language modeling performance [94] and increase memorization\n[83]. We use the bloom filter-based deduplication functionality from Dolma [ 167] and deem two\ndocuments duplicates if they share more than 90% of their 20-grams.\nFor code data from the Stack v2, we apply the Red Pajama V1 [ 185] code filtering heuristics.\nThese include filters based on the mean and maximum line length in a document, the proportion of\nalphanumeric characters, and the ratio of alphabetical characters to tokens. After this initial filter, we\nadopt the process used by SmolLM2 [ 5] where we keep only code in Python, C, C++, SQL, Java,\nPHP, Rust, Javascript, Typescript, Go, Ruby, Markdown, C#, Swift, or shell and filter this set using\nlanguage-specific quality classifiers to retain only educational and well-documented code. We use\n6\n--- Page 7 ---\nFigure 2: The Common Pile consistently outperforms other openly licensed corpora as a pre-\ntraining dataset. Following the setup from Penedo et al. [132] , we train and evaluate 1.7B parameter\nmodels on 28B tokens of data from each dataset. Stars denote benchmarks on which the model\ntrained using the Common Pile outperforms all other models.\na lower threshold to filter out low-quality code than was used for SmolLM2, resulting in a larger\nset of post-filtered text. Finally, we extract plaintext from HTML documents in the Stack V2 using\nTrafilatura [ 10] and apply our standard plaintext filtering pipeline including language, length, toxicity,\nand PII filtering.\n4.2 Data mixing\nRecent work [ 3,178,189] has shown that up- or down-weighting pre-training data sources in\naccordance with some notion of data quality can produce more performant models. Indeed, the\nsources in the Common Pile vary drastically in their characteristics, and we don’t necessarily expect\nthat our largest sources contain the highest quality text. For example, patent text sourced from\nthe USPTO (our second-largest source) exhibits substantially different wording, terminology, and\nrepetition than typical natural language. Consequently, we anticipate that appropriately mixing the\nsources in the Common Pile (rather than simply combining all sources, i.e., mixing in proportion to\nsource size) is of particular importance. Additionally, while LLM pre-training datasets have been\ncontinuously scaled to avoid the diminishing returns that result from repeating data [ 94], recent work\nhas highlighted that repeating high-quality data can be preferable to avoiding repetition by training\non low-quality data [120, 52].\nTo determine mixing weights, we first trained per-source language models using the procedure\noutlined in subsection 4.3 below for 28 billion tokens on all sources that were sufficiently large to\nbe repeated less than four times at this data budget. Based on the performance of these per-source\nmodels, we heuristically set mixing weights to up- and down-weight high- and low-performance\nsources respectively while targeting a maximum of six repetitions over the course of a 1 trillion token\ntraining run. Additionally, we assumed that our smaller sources were high quality and set their mixing\nrates such that they were also repeated six times over the course of 1 trillion tokens. The resulting\nmixture and per-source repetition rates are given in Table 7 (appendix). We also experimented with\nusing MixMin [ 178] to automatically determine mixing weights but found that it did not improve\nover our heuristically determined mixture.\nBecause we use this dataset mixture to train the Comma v0.1 models (subsection 4.4) and because\nit comprises a heavily filtered and remixed version of the Common Pile v0.1, we refer to it as the\n“Comma dataset” to distinguish it from the Common Pile itself.\n4.3 Controlled dataset quality experiments\nAs a preliminary measure of the Common Pile’s quality, we adopt the experimental setting of Penedo\net al. [132] and identically train models on the Comma dataset and various preexisting datasets. By\nusing a controlled setting across datasets, we can assert that differences in model performance stem\nprimarily from the quality of each dataset. Specifically, we train 1.7 billion parameter decoder-only\nTransformer models [ 182] that follow the Llama architecture [ 179] on 28 billion tokens of data from\neach dataset, tokenized using the GPT-2 tokenizer [ 137]. We follow the hyperparameters and setup\n7\n--- Page 8 ---\nof Penedo et al. [132] exactly, except that we used a weight decay of 0.2 instead of 0.1 due to slightly\nimproved performance (possibly due to the large amount of repetition in the Comma dataset).\nEach model was then evaluated using the set of “early signal” tasks identified by Penedo et al. [132]\nwhich cover commonsense reasoning and knowledge capabilities; specifically, we evaluate zero-shot\nperformance on ARC [33], MMLU [70], HellaSwag (HSwag) [192], OpenBookQA (OBQA) [118],\nCommonSenseQA (CSQA) [ 171], PIQA [ 19], and SocialIQA (SIQA) [ 160]. We omit Winogrande\n(which was used in Penedo et al. [132] ) because it is included in the set of datasets we sourced from\nthe Data Provenance Initiative; consequently all of the tasks we evaluate on are “unseen” by all\nmodels. We highlight that a significant portion of the Comma dataset is code, but none of the tasks\nwe evaluate on measure code capabilities. While it is possible that we could improve performance\nby omitting code data in this setting, we retained code for reliable reporting of the Comma dataset’s\nperformance.\nAs baselines, we compare to the prior datasets that aim to provide open licensed text discussed in\nsubsection 2.2: OLC [ 119], Common Corpus [ 91], and KL3M [ 78]. We additionally compare to the\nPile, as it one of the only LLM pre-training datasets that contains a comparable number of diverse\nsources to the Common Pile (22 vs. 30). Finally, we report the performance of two web text-based\nunlicensed pre-training datasets: OSCAR [ 169], which incorporates relatively little filtering; and\nFineWeb [ 132], an recent dataset that reflects current best practice for LLM pre-training dataset\ncuration.\nThe resulting performance of each model is shown in Figure 2, with detailed results in Table 9\n(appendix). Notably, the Comma dataset-based model outperforms the models trained OLC, Common\nCorpus, and KL3M across all benchmarks and outperforms the Pile-based model on all but two\nbenchmarks. While the performance of the FineWeb-based model is the best on most benchmarks,\nthe Comma dataset-based model performs best on the scientific and scholarly knowledge-based\nbenchmarks MMLU and ARC, possibly due to the Common Pile’s large proportion of domain-\nrelevant text. On the other hand, on the commonsense reasoning datasets HellaSwag, PIQA, and\nCommonSenseQA, the model trained on the Comma dataset has significantly worse performance\nthan models trained on the Pile, OSCAR, and FineWeb, possibly indicating a lack of relevant data\nin the Common Pile. We additionally note that recent work [ 188] highlights that performance on\nHellaSwag is most heavily influenced by coverage of certain domains and topics such as personal\nblogs, tutorials, hobbies, and sports, which are poorly represented in the Common Pile. Overall,\nthese findings confirm that the Comma dataset performs best among datasets that aim to contain only\nopenly licensed data and is also a strong candidate in general, particularly when targeting scientific\nand scholarly applications.\nWe note that the Comma dataset is the only dataset we evaluate that explicitly includes task-like data\ndue to inclusion of data from the Data Provenance Initiative (DPI). To verify that this does not confer\nan unfair advantage, we trained an additional model on the Comma dataset with all sources retained\nexcept for the DPI-sourced data. Removing this source had a minimal impact on model performance\n(full results in Table 9), with a notable decrease only on HellaSwag, possibly suggesting that the DPI\ndata contains domain-relevant data for this benchmark that other sources lack.\n4.4 Comma v0.1\nHaving established that Comma’s dataset produces models with competitive performance when\ncompared to other datasets, we now validate our efforts at larger, more realistic scales. Specifically,\nwe train Comma v0.1-1T and Comma v0.1-2T, a pair of 7 billion-parameter LLMs trained on 1 and 2\ntrillion tokens of text respectively, and compare with other models trained using similar computational\nbudgets.\nTokenization While training a tokenizer on unlicensed text is less likely to raise ethical or IP-related\nissues than training an LLM, we nevertheless trained a custom tokenizer on the Comma dataset to\nensure that our entire modeling pipeline was based on openly licensed data. In addition, the different\ncharacteristics of our dataset likely makes existing tokenizers (which are often trained on web text)\nsuboptimal. We therefore trained a BPE-based [ 55] tokenizer using the Hugging Face tokenizers\nlibrary using a vocabulary size of 64,000. We follow the same splitting regex as Llama 3.2 [ 62] and\nthe Hugging Face ByteLevel preprocessor; no Unicode normalization was used. The tokenizer was\ntrained on a 600GB sample [150] of text from the Comma dataset.\n8\n--- Page 9 ---\nARC-C ARC-E MMLU BoolQ HSwag OBQA CSQA PIQA SIQA HumEval MBPP020406080100Performance\nKnowledge/Reasoning CodingComma v0.1-1T LLaMA MPT RPJ-INCITE Qwen3Figure 3: Compared to models trained with similar resources (7 billion parameters, 1 trillion\ntokens), Comma v0.1-1T is the strongest model on several standard benchmarks. To contextual-\nize these results, we include Qwen3 8B (trained on 36 trillion tokens) as a “current best-practices”\nupper bound. Stars denote benchmarks on which Comma v0.1-1T outperforms all other compute-\nmatched models (i.e., all models other than Qwen3). Full numerical results are provided in Table 10\n(appendix).\nTraining setup We trained Comma v0.1-1T and -2T using the lingua framework [ 183]. We base\nour model architecture and training hyperparameters on lingua ’s Llama-7B configuration, which\nclosely follows the conventions set by the Llama series of models [ 179,62]. For Comma v0.1-1T, we\ntrained with an effective batch size of 512 length-4096 sequences using the AdamW [ 111] optimizer\nand a weight decay of 0.2. For the Comma v0.1 variant trained on 2 trillion tokens, we increased the\nbatch size to 2048 length-4096 sequences.\nWe performed two stage training, with a first stage following a cosine learning rate schedule and the\nsecond stage being a ‘cool-down” [ 73], where we train only on a subset of high-quality sources using\nthe mixing weights provided in Table 8 (appendix) while decaying the learning rate linearly to 0.\nComma v0.1-1T had a first stage of 460,000 steps with 2,000 steps of warmup, an initial learning\nrate of 1e−3, a minimum learning rate of 1e−9, a cosine schedule period of 500,000 steps, and\n18,000 steps of decay. For Comma v0.1-2T, the first stage instead had 230,000 steps with a maximum\nand minimum learning rate of 2e−3and2e−9respectively and a period of 250,000 steps, with\n9,000 steps of decay in the second stage. For both models, we average together ten evenly spaced\ncheckpoints from the cool-down phase to produce a final model as suggested by Grattafiori et al.\n[62]. Apart from our main Comma v0.1-1T and -2T training runs, we completed several additional\nruns to better understand how hyper-parameters impact the model, including using a different batch\nsize and following a three-stage (rather than two-stage) curriculum. Overall, the results of these runs\nwere consistent with the findings from our main training runs. Additional details can be found in\nAppendix O.\nEvaluation We evaluate the Comma v0.1 models on the suite of benchmarks used by Groeneveld\net al. [64] in addition to two additional code benchmarks. Specifically, we evaluate models on\nARC [ 33], MMLU [ 70], BoolQ [ 31], HellaSwag [ 192], OpenBookQA [ 118], CommonsenseQA [ 171],\nPIQA [ 19], and SIQA [ 160] to probe world knowledge and reasoning and HumanEval [ 25] and\nMBPP [ 8] to evaluate coding capabilities. Following Groeneveld et al. [64], we evaluate using\nOLMES [ 65], using a zero-shot format for all tasks except MMLU, which uses a 5-shot format. For\nthe coding tasks, we report pass@10 accuracies.\nBaseline models For fairness, we primarily compare to prior models with the same parameter count\nand token budget. Since we are not aware of any such models trained on openly licensed data, we\ncompare only to models trained on unlicensed data. For Comma v0.1-1T, we compare to Llama 1\n7B [179], MPT-7B [ 175], RPJ-INCITE-7B [ 185], StableLM-7B [ 12], and OpenLLaMA-7B [ 58]. For\nthe two trillion token variant, we compare to OLMo Twin (specifically OLMo-7B-Twin-2T) [ 64],\nLlama 2 7B [ 180], and DeepSeekLLM [ 16]. Over time, the token budgets of open pre-trained LLMs\nhave continually grown [ 143], and current standard practice is to pretrain on significantly more than\n1 or 2 trillion tokens. Consequently, recent models tend to outperform our baselines, which were\nreleased in 2023 and 2024. To provide a state-of-the-art point of reference, we additionally include\nresults for the recently released Qwen3 8B [ 176], which was trained for 36 trillion tokens. We\nemphasize that we cannot reliably compare to a model with a 36 ×or 18×larger training budget and\nwe primarily include it as a point of reference.\n9\n--- Page 10 ---\nARC-C ARC-E MMLU BoolQ HSwag OBQA CSQA PIQA SIQA HumEval MBPP020406080100Performance\nKnowledge/Reasoning CodingComma v0.1-2T OLMo Twin LLaMA 2 DeepSeekLLM Qwen3Figure 4: Comma v0.1-2T is also competitive with budget-matched models (7 billion parameters,\n2 trillion tokens) trained on unlicensed data. We additionally include Qwen3 8B as a higher budget\nupper bound. Stars denote benchmarks where Comma v0.1-2T outperforms budget-matched models.\nFull numerical results are provided in Table 11 (appendix).\nResults As shown in Figure 3, Comma v0.1-1T outperforms budget-matched baseline models on\nover half of the benchmarks tested. In line with our results from subsection 4.3, we observe that\nComma v0.1-1T excels on knowledge-based benchmarks like ARC-C and MMLU, but lags behind\non HellaSwag and PIQA. Comma v0.1-1T is also particularly strong at code-related tasks where it\noutperforms baseline models by a wide margin. Comparisons to StableLM and OpenLLama can be\nfound in Table 10 (appendix), but show similar trends.\nOur promising results from training on 1 trillion tokens motivated us to experiment with longer\ntraining durations. To test whether the filtered dataset supports training durations beyond 1T, we\ntrained Comma v0.1-2T simply by repeating the same data mixture used for Comma v0.1-1T\napproximately twice. We note that this pre-training mixture involves repeating certain sources an\nexcessive number of times (up to 16 passes for some sources). Prior work suggests that these extreme\nlevels of data repetition may result in diminishing returns [ 121]. However, these experiments still\ngive us a preliminary picture of the performance achievable under a larger budget.\nThe performance of Comma v0.1-2T is compared with budget-matched models in Figure 4. Notably,\nwe find that Comma v0.1-2T is competitive with OLMo, Llama 2, and DeepSeekLLM, with especially\nstrong performance on MMLU, SIQA, ARC-E, and the coding tasks. We emphasize that the Comma\nv0.1-2T result here is likely nota best-case 2T-token run using the Common Pile v0.1 due to\nexcessive repetition, and better performance could likely be attained through a 2T-specific mixture\nand curriculum. Nevertheless, this result highlights the promise of larger scale training runs based on\nthe Common Pile. Qwen3 8B’s superior performance across most benchmarks confirms the benefit\nof larger training budgets and motivates future efforts on scaling up the Common Pile.\n5 Conclusion\nWe release Common Pile v0.1 , an 8TB corpus that—to our knowledge—constitutes the largest dataset\nbuilt exclusively from openly licensed text. Alongside our dataset, we release Comma v0.1-1T and\n-2T, two performant 7-billion-parameter LLMs trained on text from the Common Pile, as well as\nthe filtered and rebalanced data mixture we used for training. Our results demonstrate that not only\nis the Common Pile the strongest dataset for pretraining under an open-license constraint, but also\nthat it produces models comparable to those trained on an equivalent amount of unlicensed data.\nThis positive result holds promise for future of open-license pretraining, especially if the research\ncommunity invests in collecting larger quantities of openly licensed text data in the future. Ultimately,\nwe believe that the Common Pile v0.1 represents the first step on the path towards a more ethical\nlanguage model ecosystem, where performance need not come at the cost of creator rights and legal\ntransparency.\nAcknowledgments\nWe thank Chris Maddison, Anvith Thudi, Pierre-Carl Langlais, Alec Radford, Adam Roberts, Sewon\nMin, and Weijia Shi for fruitful discussions and constructive feedback. An early draft of this work\n10\n--- Page 11 ---\nwas shared at the Dataset Convening hosted by the Mozilla Foundation and EleutherAI. We thank the\nparticipants for their discussion and feedback.\nThis work was supported by funding from the Mozilla Foundation and Sutter Hill Ventures. We\nacknowledge the support of the Natural Sciences and Engineering Research Council of Canada\n(NSERC). Researchers funded through the NSERC-CSE Research Communities Grants do not\nrepresent the Communications Security Establishment Canada or the Government of Canada. Any\nresearch, opinions or positions they produce as part of this initiative do not represent the official\nviews of the Government of Canada.\nParts of this work were performed under the auspices of the U.S. Department of Energy by Lawrence\nLivermore National Laboratory under Contract DE-AC52-07NA27344 and was supported by the\nLLNL-LDRD Program under Project No. 24-ERD-010 and Project No. 24-SI-008 (LLNL-CONF-\n2006420).\nReferences\n[1]17 U.S. Code § 102. Subject matter of copyright: In general, December 1990. URL https:\n//www.law.cornell.edu/uscode/text/17/102 .\n[2]17 U.S. Code § 105. Subject matter of copyright: United States Government works, December\n2024. URL https://www.law.cornell.edu/uscode/text/17/105 .\n[3]Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data\nmixing for language model pre-training. arXiv preprint arXiv:2312.02406 , 2023.\n[4]Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi\nWang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on\ndata selection for language models. Transactions on Machine Learning Research , 2024.\n[5]Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme\nPenedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlí ˇcek, Agustín Piqueres Lajarín, Vaibhav\nSrivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben\nBurtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro\nvon Werra, and Thomas Wolf. Smollm2: When smol goes big – data-centric training of a\nsmall language model, 2025. URL https://arxiv.org/abs/2502.02737 .\n[6]Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability\nof monolingual representations. In Annual Meeting of the Association for Computational\nLinguistics , pages 4623–4637, 2019.\n[7]Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli,\nMarzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact of\ncode in pre-training. arXiv preprint arXiv:2408.10914 , 2024.\n[8]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732 , 2021.\n[9]Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bom-\nmarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl,\nSebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang,\nLeandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, Lisa\nGutermuth, Hynek Kydlí ˇcek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Long-\npre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu,\nAndrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White,\nand Thomas Wolf. Towards best practices for open datasets for llm training, 2025. URL\nhttps://arxiv.org/abs/2501.08365 .\n[10] Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text\nDiscovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing: System Demonstrations , pages 122–131. Association for\n11\n--- Page 12 ---\nComputational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.\n15.\n[11] Max Bartolo, A. Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the\nai: Investigating adversarial human annotation for reading comprehension. Transactions of the\nAssociation for Computational Linguistics , 8:662–678, 2020.\n[12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth\nAdithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6\nb technical report. arXiv preprint arXiv:2402.17834 , 2024.\n[13] Jonathan Berant, A. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Conference on Empirical Methods in Natural Language Processing ,\npages 1533–1544, 2013.\n[14] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir:\nSearch Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury,\nGabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40th\nEuropean Conference on IR Research (ECIR 2018) , Lecture Notes in Computer Science, Berlin\nHeidelberg New York, March 2018. Springer.\n[15] Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-Scale\nWeb Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and Stefan\nV oigt, editors, 3rd International Symposium on Open Search Technology (OSSYM 2021) .\nInternational Open Search Symposium, October 2021.\n[16] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\nDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language\nmodels with longtermism. arXiv preprint arXiv:2401.02954 , 2024.\n[17] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language\nmodels. Advances in Neural Information Processing Systems , 36:28072–28090, 2023.\n[18] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,\nAviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large\nlanguage models across training and scaling, 2023. URL https://arxiv.org/abs/2304.\n01373 .\n[19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language, 2019. URL https://arxiv.org/abs/\n1911.11641 .\n[20] Blue Oak Council. License List (version 15), 2025. URL https://blueoakcouncil.org/\nlist .\n[21] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Nat-\nural language inference with natural language explanations. In Neural Information Processing\nSystems , pages 9560–9572, 2018.\n[22] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\nChiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[23] Ilias Chalkidis, Abhik Jana, D. Hartung, M. Bommarito, Ion Androutsopoulos, D. Katz, and\nNikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english.\nInAnnual Meeting of the Association for Computational Linguistics , pages 4310–4330, 2021.\n[24] Chat GPT Is Eating the World, 2024. URL https://chatgptiseatingtheworld.com .\n12\n--- Page 13 ---\n[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,\nPeter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code, 2021.\n[26] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang\nWang. HybridQA: A dataset of multi-hop question answering over tabular and textual data. In\nTrevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 1026–1036, Online, November 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.91. URL https://\naclanthology.org/2020.findings-emnlp.91/ .\n[27] Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, and\nWilliam Yang Wang. Logic2text: High-fidelity natural language generation from logical forms.\nArXiv , abs/2004.14579, 2020.\n[28] Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung,\nAdithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, Eduard\nHovy, Roger Grosse, and Eric Xing. What is your data worth to gpt? llm-scale data valuation\nwith influence functions, 2024. URL https://arxiv.org/abs/2405.13954 .\n[29] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and\nLuke Zettlemoyer. Quac: Question answering in context. In Conference on Empirical Methods\nin Natural Language Processing , pages 2174–2184, 2018.\n[30] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will\nCukierski. Toxic comment classification challenge. https://kaggle.com/competitions/\njigsaw-toxic-comment-classification-challenge , 2017. Kaggle.\n[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044 , 2019.\n[32] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does\nBERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , 2019.\n[33] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\n[34] Karl Cobbe, V . Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. ArXiv , abs/2110.14168, 2021.\n[35] Creative Commons. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, 2025. URL\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n[36] Creative Commons. Creative Commons Attribution 4.0 International License § 2(a)(1)(A),\n2025. URL https://creativecommons.org/licenses/by/4.0/legalcode .\n[37] Creative Commons. Creative Commons Attribution 4.0 International License § 3(a)(1)(A)(i),\n2025. URL https://creativecommons.org/licenses/by/4.0/legalcode .\n[38] Creative Commons. Public Domain Mark 1.0, 2025. URL https://creativecommons.\norg/publicdomain/mark/1 .\n13\n--- Page 14 ---\n[39] Creative Commons. Creative Commons Attribution-ShareAlike 4.0 International License,\n2025. URL https://creativecommons.org/licenses/by-sa/4.0/ .\n[40] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright,\nMemorization, and Generative AI. arXiv preprint arXiv:2404.12590 , 2024.\n[41] A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley,\nDaniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books from\nopen-weight language models. arXiv preprint arXiv:2505.12546 , 2025.\n[42] Yiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang,\nand Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. In\nEMNLP-IJCNLP , pages 5882–5888, 2019.\n[43] Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen tau Yih, and Peter Clark. Tracking state\nchanges in procedural text: a challenge dataset and models for process paragraph comprehen-\nsion. In North American Chapter of the Association for Computational Linguistics , pages\n1595–1604, 2018.\n[44] Pradeep Dasigi, Nelson F. Liu, Ana Marasovi ´c, Noah A. Smith, and Matt Gardner. Quoref: A\nreading comprehension dataset with questions requiring coreferential reasoning. In Conference\non Empirical Methods in Natural Language Processing , volume abs/1908.05803, 2019.\n[45] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao\nZheng, and Zhiyuan Liu. Few-nerd: A few-shot named entity recognition dataset. ArXiv ,\nabs/2105.07464, 2021.\n[46] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. Drop: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In North American Chapter of the Association for Computational Linguistics ,\npages 2368–2378, 2019.\n[47] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettle-\nmoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership\ninference attacks work on large language models? arXiv preprint arXiv:2402.07841 , 2024.\n[48] S. Dumitrescu, Petru Rebeja, Beáta L ˝orincz, Mihaela G ˘aman, M. Ilie, Andrei Pruteanu,\nAdriana Stan, Luciana Morogan, Traian Rebedea, and Sebastian Ruder. Liro: Benchmark and\nleaderboard for romanian language tasks. In NeurIPS Datasets and Benchmarks , 2021.\n[49] Yanai Elazar and Yoav Goldberg. Where’s my head? definition, data set, and models for\nnumeric fused-head identification and resolution. Transactions of the Association for Compu-\ntational Linguistics , 7:519–535, 2019.\n[50] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius\nMosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring causal effects\nof data statistics on language model’sfactual’predictions. arXiv preprint arXiv:2207.14251 ,\n2022.\n[51] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moral\nstories: Situated reasoning about norms, intents, actions, and their consequences. ArXiv ,\nabs/2012.15738, 2020.\n[52] Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, Ludwig\nSchmidt, and Tom Gunter. Datasets, documents, and repetitions: The practicalities of unequal\ndata quality. arXiv preprint arXiv:2503.07879 , 2025.\n[53] James Ferguson, Matt Gardner, Tushar Khot, and Pradeep Dasigi. Iirc: A dataset of incomplete\ninformation reading comprehension questions. In Conference on Empirical Methods in Natural\nLanguage Processing , pages 1137–1147, 2020.\n[54] Nancy Fulda, Nathan Tibbetts, Zachary Brown, and D. Wingate. Harvesting common-sense\nnavigational knowledge for robotics from uncurated text corpora. In Conference on Robot\nLearning , pages 525–534, 2017.\n14\n--- Page 15 ---\n[55] Philip Gage. A new algorithm for data compression. The C Users Journal archive , 12:23–38,\n1994. URL https://api.semanticscholar.org/CorpusID:59804030 .\n[56] N. Gale, G. Heath, E. Cameron, S. Rashid, and S. Redwood. Using the framework method for\nthe analysis of qualitative data in multi-disciplinary health research. BMC Medical Research\nMethodology , 13:117 – 117, 2013.\n[57] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.\nThe pile: An 800gb dataset of diverse text for language modeling, 2020. URL https:\n//arxiv.org/abs/2101.00027 .\n[58] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\nhttps://github.com/openlm-research/open_llama .\n[59] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. Did\naristotle use a laptop? a question answering benchmark with implicit reasoning strategies.\nTransactions of the Association for Computational Linguistics , 9:346–361, 2021.\n[60] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking nli systems with sentences that\nrequire simple lexical inferences. ArXiv , abs/1805.02266, 2018.\n[61] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir\nPatel, Jonathan Frankle, Cory Stephenson, and V olodymyr Kuleshov. CommonCanvas: Open\nDiffusion Models Trained on Creative-Commons Images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 8250–8260, June\n2024.\n[62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang,\nAngela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravanku-\nmar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Char-\nlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,\nDamien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,\nDhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis\nAnderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,\nHailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,\nIsabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer\nBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\nWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun,\nJoshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik,\nKuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten,\nLawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat\nSingh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita,\nMaya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,\nNaman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning\nZhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar\nVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,\nRicardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Gird-\nhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean\nBell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,\nSheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,\n15\n--- Page 16 ---\nSpencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,\nTamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vi-\ngnesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish V ogeti, Vítor Albiero,\nVladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet,\nXiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia,\nXuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,\nYuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing\nChen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam\nShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma,\nAlex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo,\nAnam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,\nAndrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal,\nAparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,\nAzadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,\nBeto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti,\nBrandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton,\nCatalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin,\nDana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia\nDavid, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland,\nEdward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily\nWood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet,\nFrank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil\nHalpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan\nInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison\nRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,\nIgor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,\nJames Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff\nTang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian\nJin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,\nJosh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal,\nKatayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran\nJagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A,\nLeandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca\nWehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson,\nMatthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan\nKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik\nVyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso,\nMo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks,\nNatasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,\nNikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar\nSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,\nPhilip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish\nYuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\nBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\nMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-\nsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang\nZhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,\nStephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad,\nSumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury,\nSydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson,\nTianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun V ontimitta,\nVictoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad\nIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,\nWenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang,\n16\n--- Page 17 ---\nXilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin\nZhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi\nQian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu\nYang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL\nhttp://arxiv.org/abs/2407.21783 . arXiv:2407.21783 [cs].\n[63] Grobid. Grobid. https://github.com/kermitt2/grobid , 2008–2025.\n[64] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkin-\nson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar,\nYuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff,\nAakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander,\nDustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell\nWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge,\nKyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the\nscience of language models, 2024. URL https://arxiv.org/abs/2402.00838 .\n[65] Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh\nHajishirzi. Olmes: A standard for language model evaluations, 2025. URL https:\n//arxiv.org/abs/2406.08446 .\n[66] Mandy Guo, Zihang Dai, Denny Vrande ˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual\nlanguage model dataset. In Proceedings of the Twelfth Language Resources and Evaluation\nConference , pages 2440–2452, 2020.\n[67] Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui. Disfl-\nqa: A benchmark dataset for understanding disfluencies in question answering. ArXiv ,\nabs/2106.04016, 2021.\n[68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\nwith free license. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi,\nMarko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan\nOdijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC‘16) , pages 914–922, Portorož, Slovenia, May\n2016. European Language Resources Association (ELRA). URL https://aclanthology.\norg/L16-1146/ .\n[69] Seth Hays. AI Training and Copyright Infringement: Solutions from Asia, Octo-\nber 2024. URL https://www.techpolicy.press/ai-training-and-copyright-\ninfringement-solutions-from-asia/ .\n[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300 , 2020.\n[71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp\ndataset for legal contract review. ArXiv , abs/2103.06268, 2021.\n[72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classi-\nfication. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics , 2018.\n[73] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei\nFang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language\nmodels with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024.\n[74] HuggingFace: Common Corpus, 2025. URL https://huggingface.co/datasets/\nPleIAs/common_corpus .\n[75] Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine\nBosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsense\nknowledge graphs. In AAAI Conference on Artificial Intelligence , pages 6384–6392, 2020.\n17\n--- Page 18 ---\n[76] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L Richter, Quentin Anthony, Timothée\nLesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually\npre-train large language models. arXiv preprint arXiv:2403.08763 , 2024.\n[77] IFI CLAIMS Patent Services and Google. Google patents public data. https://patents.\ngoogle.com/ , 2023. Licensed under a Creative Commons Attribution 4.0 International\nLicense.\n[78] Michael J Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project:\nCopyright-clean training resources for large language models, 2025. URL https://arxiv.\norg/abs/2504.07854 .\n[79] Infocomm Media Development Authority of Singapore (IMDA), Aicadium, and AI Verify\nFoundation. Model AI Governance Framework for Generative AI: Fostering a Trusted Ecosys-\ntem, May 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/\n05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf .\n[80] Najko Jahn, Nick Haupka, and Anne Hobert. Analysing and reclassifying open access informa-\ntion in OpenAlex, 2023. URL https://subugoe.github.io/scholcomm_analytics/\nposts/oalex_oa_status/?utm_source=chatgpt.com .\n[81] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\nefficient text classification. In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 2, Short Papers , pages 427–431.\nAssociation for Computational Linguistics, April 2017.\n[82] Nikhil Kandpal and Colin Raffel. Position: The most expensive part of an llm should be its\ntraining data. arXiv preprint arXiv:2504.12427 , 2025.\n[83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy\nrisks in language models, 2022. URL https://arxiv.org/abs/2202.06539 .\n[84] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large\nlanguage models struggle to learn long-tail knowledge. In International Conference on\nMachine Learning , pages 15696–15707. PMLR, 2023.\n[85] Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, and Kentaro\nInui. When choosing plausible alternatives, clever hans can be clever. ArXiv , abs/1911.00225,\n2019.\n[86] Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from\nscience question answering. In AAAI Conference on Artificial Intelligence , pages 5189–5197,\n2018.\n[87] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg,\nAlexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman\nCohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey\nFeldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian\nKohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\nKelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi,\nPaul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian,\nA. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline\nWu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. The\nSemantic Scholar Open Data Platform. ArXiv , abs/2301.10140, 2023. URL https://api.\nsemanticscholar.org/CorpusID:256194545 .\n[88] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam,\nK. Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich’ard Nagyfi,\nES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph\nSchuhmann, Huu Nguyen, and A. Mattick. Openassistant conversations - democratizing large\nlanguage model alignment. ArXiv , abs/2304.07327, 2023.\n18\n--- Page 19 ---\n[89] T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nD. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V . Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–466, 2019.\n[90] Faisal Ladhak, Esin Durmus, Claire Cardie, and K. McKeown. Wikilingua: A new benchmark\ndataset for multilingual abstractive summarization. ArXiv , abs/2010.03093, 2020.\n[91] Pierre-Carl Langlais. Releasing Common Corpus: the largest public domain dataset for training\nLLMs, 2024. URL https://huggingface.co/blog/Pclanglais/common-corpus .\n[92] LDP Headquarters for the Promotion of Digital Society and Project Team on\nthe Evolution and Implementation of AIs. AI White Paper 2024: New Strate-\ngies in Stage II, Toward the world’s most AI-friendly country, April 2024.\nURL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-\nGovernance-Framework-for-Generative-AI-May-2024-1-1.pdf .\n[93] R. Lebret, David Grangier, and Michael Auli. Neural text generation from structured data\nwith application to the biography domain. In Conference on Empirical Methods in Natural\nLanguage Processing , pages 1203–1213, 2016.\n[94] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\nbetter, 2022. URL https://arxiv.org/abs/2107.06499 .\n[95] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:\nThe Next Generation. SSRN , 2023. http://dx.doi.org/10.2139/ssrn.4580739.\n[96] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin’ ’Bout AI Generation:\nCopyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133 , 2023.\n[97] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes,\nSebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and Christian Bizer. Dbpedia - a\nlarge-scale, multilingual knowledge base extracted from wikipedia. Semantic Web , 6:167–195,\n2015.\n[98] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In AAAI Spring\nSymposium: Logical Formalizations of Commonsense Reasoning , 2011.\n[99] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik\nBansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff,\nReinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon\nAlbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,\nJosh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel\nIlharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,\nThao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri,\nSewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander\nToshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas\nKollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal\nShankar. Datacomp-lm: In search of the next generation of training sets for language models,\n2025. URL https://arxiv.org/abs/2406.11794 .\n[100] Xin Li and D. Roth. Learning question classifiers. In International Conference on\nComputational Linguistics , pages 1–7, 2002.\n[101] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\nhuman falsehoods. In Annual Meeting of the Association for Computational Linguistics , pages\n3214–3252, 2021.\n[102] Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham Neubig. Testing the ability of\nlanguage models to interpret figurative language. ArXiv , abs/2204.12632, 2022.\n19\n--- Page 20 ---\n[103] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua\nTao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller,\nYonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang\nShen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov,\nTim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.\nURL https://arxiv.org/abs/2312.06550 .\n[104] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC:\nThe semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pages 4969–4983, Online, July 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL\nhttps://www.aclweb.org/anthology/2020.acl-main.447 .\n[105] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash\nKapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al. The responsible foundation\nmodel development cheatsheet: A review of tools & resources. Transactions on Machine\nLearning Research , 2024.\n[106] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William\nBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi (Alexis)\nWu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, and\nSara Hooker. A large-scale audit of dataset licensing and attribution in AI. Nature Machine\nIntelligence , 6(8):975–987, August 2024. doi: 10/gt8f5p.\n[107] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William\nBrannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman,\nChristopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh,\nCaroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad\nAlghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary,\nLester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff,\nSeonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh\nChien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne\nIppolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. Consent in crisis: The rapid decline\nof the AI data commons. Advances in Neural Information Processing Systems , 37, 2024.\n[108] Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy\nGero, Sandy Pentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai are\nall broken: what will it take to fix them? arXiv preprint arXiv:2404.12691 , 2024.\n[109] Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,\nWilliam Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy,\net al. Bridging the data provenance gap across text, speech and video. arXiv preprint\narXiv:2412.17847 , 2024.\n[110] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph,\nDenny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to training\ndata: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 1: Long Papers) , pages 3245–3276, 2024.\n[111] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations , 2019. URL https:\n//openreview.net/forum?id=Bkg6RiCqY7 .\n[112] Annie Louis, D. Roth, and Filip Radlinski. “i’d rather just go to bed”’: Understanding indirect\nanswers. In Conference on Empirical Methods in Natural Language Processing , volume\nabs/2010.03450, 2020.\n[113] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,\nNouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,\nDenis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,\nIndraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,\n20\n--- Page 21 ---\nEvgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan\nSu, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru\nTang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank\nMishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry,\nCanwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson,\nCarolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,\nCarlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro\nvon Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.\n[114] Robert Mahari and Shayne Longpre. Discit ergo est: Training data provenance and fair use.\nRobert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use,\nDynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review,\nWinter , 2023.\n[115] Matt Mahoney. Large text compression benchmark, 2011.\n[116] Stephen Merity, Caiming Xiong, James Bradbury, and R. Socher. Pointer sentinel mixture\nmodels. ArXiv , abs/1609.07843, 2016.\n[117] Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres,\nMatthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg,\nDan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and\nErez Lieberman Aiden. Quantitative analysis of culture using millions of digitized\nbooks. Science , 331(6014):176–182, 2011. doi: 10.1126/science.1199644. URL\nhttps://www.science.org/doi/abs/10.1126/science.1199644 .\n[118] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\n2018.\n[119] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A.\nSmith, and Luke Zettlemoyer. SILO language models: Isolating legal risk in a nonparametric\ndatastore. In The Twelfth International Conference on Learning Representations , 2024. URL\nhttps://openreview.net/forum?id=ruk0nyQPec .\n[120] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi,\nAleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained\nlanguage models. Advances in Neural Information Processing Systems , 36, 2023.\n[121] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Alek-\nsandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained\nlanguage models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\neditors, Advances in Neural Information Processing Systems , volume 36, pages 50358–50376.\nCurran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/\npaper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf .\n[122] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: A\nchallenge dataset for measuring social biases in masked language models. In Conference on\nEmpirical Methods in Natural Language Processing , pages 1953–1967, 2020.\n[123] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challenges\nfor end-to-end generation. ArXiv , abs/1706.09254, 2017.\n[124] Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and S. Ananiadou. Open-domain anatomical\nentity mention detection. In Annual Meeting of the Association for Computational Linguistics ,\npages 27–36, 2012.\n[125] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Creak: A dataset for\ncommonsense reasoning over entity knowledge. ArXiv , abs/2109.01653, 2021.\n[126] OpenAlex, 2025. URL https://openalex.org .\n21\n--- Page 22 ---\n[127] Vassil Panayotov, Guoguo Chen, Daniel Povey, and S. Khudanpur. Librispeech: An asr corpus\nbased on public domain audio books. 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 5206–5210, 2015.\n[128] Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A Choquette-Choo, and Prateek Mittal.\nPrivacy auditing of large language models. arXiv preprint arXiv:2503.06808 , 2025.\n[129] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and\nAleksander Madry. Trak: Attributing model behavior at scale, 2023. URL\nhttps://arxiv.org/abs/2303.14186 .\n[130] European Parliament and Council of the European Union. Directive (eu) 2019/790,\n2019. URL https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:\n32019L0790#art_3 .\n[131] ParlParse. Parser for uk parliament proceedings. https://parser.theyworkforyou.com/ ,\n2025. Accessed: 2025-05-09.\n[132] Guilherme Penedo, Hynek Kydlí ˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\nLeandro V on Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for the\nfinest text data at scale. Advances in Neural Information Processing Systems , 37, 2024.\n[133] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing , 2019.\n[134] E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, and A. Korhonen. Xcopa:\nA multilingual dataset for causal commonsense reasoning. In Conference on Empirical\nMethods in Natural Language Processing , pages 2362–2376, 2020.\n[135] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamic\nbenchmark for sentiment analysis. ArXiv , abs/2012.15349, 2020.\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\n[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019.\n[138] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust speech recognition via large-scale weak supervision, 2022. URL\nhttps://arxiv.org/abs/2212.04356 .\n[139] Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi. Coached conversational\npreference elicitation: A case study in understanding movie preferences. In SIGDIAL\nConferences , pages 353–360, 2019.\n[140] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\nCompressive transformers for long-range sequence modelling. arXiv preprint , 2019. URL\nhttps://arxiv.org/abs/1911.05507 .\n[141] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-\nnigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne\nHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth\nDathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell,\nNat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-\nraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, An-\ngeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama,\nCyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G.\n22\n--- Page 23 ---\nJohnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward\nLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff\nStanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling\nlanguage models: Methods, analysis & insights from training gopher. ArXiv , abs/2112.11446,\n2021. URL https://api.semanticscholar.org/CorpusID:245353475 .\n[142] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research , 21(140), 2020.\n[143] Robi Rahman and David Owen. The size of datasets used to train language models\ndoubles approximately every seven months, 2024. URL https://epoch.ai/data-\ninsights/dataset-size-trend . Accessed: 2025-05-08.\n[144] Nazneen Rajani, Bryan McCann, Caiming Xiong, and R. Socher. Explain yourself! leveraging\nlanguage models for commonsense reasoning. ArXiv , abs/1906.02361, 2019.\n[145] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight:\nDesigning a third party audit ecosystem for ai governance. In Proceedings of the 2022\nAAAI/ACM Conference on AI, Ethics, and Society , pages 557–571, 2022.\n[146] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. In Conference on Empirical Methods in Natural\nLanguage Processing , pages 2383–2392, 2016.\n[147] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\nquestions for squad. In Annual Meeting of the Association for Computational Linguistics ,\nvolume abs/1806.03822, 2018.\n[148] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.\nSchema-guided dialogue state tracking task at dstc8. ArXiv , abs/2002.01359, 2020.\n[149] Abhilasha Ravichander, Matt Gardner, and Ana Marasovi ´c. Condaqa: A contrastive reading\ncomprehension dataset for reasoning about negation. ArXiv , abs/2211.00295, 2022.\n[150] Varshini Reddy, Craig W. Schmidt, Yuval Pinter, and Chris Tanner. How much\nis enough? the diminishing returns of tokenization training data, 2025. URL\nhttps://arxiv.org/abs/2502.20273 .\n[151] Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani\nAljunied, Luong Chi Mai, V . Thang, N. Thai, Vichet Chea, Rapid Sun, Sethserey Sam,\nSopheap Seng, K. Soe, K. Nwet, M. Utiyama, and Chenchen Ding. Introduction of the asian\nlanguage treebank. In Oriental COCOSDA International Conference on Speech Database\nand Assessments , pages 1–6, 2016.\n[152] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , 2020.\n[153] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to\nai complete question answering: A set of prerequisite real tasks. In AAAI Conference on\nArtificial Intelligence , pages 8722–8731, 2020.\n[154] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know\nabout how BERT works. Transactions of the association for computational linguistics , 8, 2021.\n[155] Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes,\nRonan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference\nin natural language. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the As-\nsociation for Computational Linguistics: EMNLP 2020 , pages 4661–4675, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418.\nURL https://aclanthology.org/2020.findings-emnlp.418/ .\n23\n--- Page 24 ---\n[156] Matthew Sag and Peter K. Yu. The globalization of copyright exceptions for ai train-\ning. Emory Law Journal , 74, 2025. doi: http://dx.doi.org/10.2139/ssrn.4976393. URL\nhttps://ssrn.com/abstract=4976393 .\n[157] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. Conjnli: Natural language inference over\nconjunctive sentences. In Conference on Empirical Methods in Natural Language Processing ,\npages 8240–8252, 2020.\n[158] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande.\nCommunications of the ACM , 64:99 – 106, 2019.\n[159] Pamela Samuelson. How to Think About Remedies in the Generative AI Copyright Cases.\nLawfare , February 2024. URL https://www.lawfaremedia.org/article/how-to-\nthink-about-remedies-in-the-generative-ai-copyright-cases .\n[160] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.\nSocialiqa: Commonsense reasoning about social interactions, 2019. URL\nhttps://arxiv.org/abs/1904.09728 .\n[161] A. Sboev, A. Naumov, and R. Rybka. Data-driven model for emotion detection in russian\ntexts. In BICAAI , pages 637–642, 2020.\n[162] Tal Schuster, Adam Fisch, and R. Barzilay. Get your vitamin c! robust fact verification\nwith contrastive evidence. In North American Chapter of the Association for Computational\nLinguistics , pages 624–643, 2021.\n[163] Emily Sheng and David C. Uthus. Investigating societal biases in a poetry composition system.\nArXiv , abs/2011.02686, 2020.\n[164] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and\nMatthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive\nlearning. ArXiv , abs/2010.03768, 2020.\n[165] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson, Abinaya Mahendiran,\nWei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike\nZhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura,\nDominik Krzemi’nski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan\nMudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda,\nEmad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia\nKreutzer, A. Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access\ncollection for multilingual instruction tuning. ArXiv , abs/2402.06619, 2024. URL\nhttps://api.semanticscholar.org/CorpusID:267617144 .\n[166] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical\nreport, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o .\n[167] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,\nAnanya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob\nMorrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha\nRavichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind\nTafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,\nDirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens\nfor language model pretraining research. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics , 2024.\n[168] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine\ntranslation. ArXiv , abs/1906.00591, 2019.\n[169] Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. Asynchronous pipeline for\nprocessing huge corpora on medium to low resource infrastructures. In 7th Workshop on the\nChallenges in the Management of Large Corpora (CMLC-7) . Leibniz-Institut für Deutsche\nSprache, 2019.\n24\n--- Page 25 ---\n[170] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of\nqualitative relationship questions. In Conference on Empirical Methods in Natural Language\nProcessing , volume abs/1909.03553, 2019.\n[171] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:\nA question answering challenge targeting commonsense knowledge. arXiv preprint\narXiv:1811.00937 , 2018.\n[172] Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal\nGuerquin, Kyle Richardson, and E. Hovy. A dataset for tracking entities in open domain\nprocedural text. ArXiv , abs/2011.08092, 2020.\n[173] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar\nRao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe\nMa, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: A top-quality llm pre-training\ndataset requires the perfect blend, 2024.\n[174] Ishan Tarunesh, Somak Aditya, and M. Choudhury. Trusting roberta over bert: Insights from\nchecklisting the natural language inference task. ArXiv , abs/2107.07229, 2021.\n[175] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially\nusable llms, 2023. URL www.mosaicml.com/blog/mpt-7b . Accessed: 2023-05-05.\n[176] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/ .\n[177] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a\nlarge-scale dataset for fact extraction and verification. ArXiv , abs/1803.05355, 2018.\n[178] Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, and Chris J. Mad-\ndison. Mixmin: Finding data mixtures via convex minimization, 2025. URL\nhttps://arxiv.org/abs/2502.10510 .\n[179] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient\nfoundation language models, 2023. URL https://arxiv.org/abs/2302.13971 .\n[180] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n[181] UK Parliament. Open parliament license. https://www.parliament.uk/site-\ninformation/copyright-parliament/open-parliament-licence/ , Unknown.\nAccessed: 2025-05-09.\n[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.\n[183] Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier\nTeytaud, and David Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library, 2024.\nURL https://github.com/facebookresearch/lingua .\n[184] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert,\nOlivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev.\nHelpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv , abs/2311.09528, 2023.\n[185] Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,\nXiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul\nChalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish,\nand Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URL\nhttps://arxiv.org/abs/2411.12372 .\n25\n--- Page 26 ---\n[186] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Resolving gendered\nambiguous pronouns with bert. ArXiv , abs/1906.01161, 2019.\n[187] Wei Wei, Quoc V . Le, Andrew M. Dai, and Jia Li. Airdialogue: An environment for\ngoal-oriented dialogue research. In Conference on Empirical Methods in Natural Language\nProcessing , pages 3844–3854, 2018.\n[188] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini.\nOrganize the web: Constructing domains enhances pre-training data curation. arXiv preprint\narXiv:2502.10341 , 2025.\n[189] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang,\nQuoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up\nlanguage model pretraining. Advances in Neural Information Processing Systems , 36, 2023.\n[190] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, R. Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. In Conference on Empirical Methods in Natural Language Processing , pages\n2369–2380, 2018.\n[191] Cat Zakrzewski, Nitasha Tiku, and Elizabeth Dwoskin. OpenAI prepares to fight for its life\nas legal troubles mount. The Washington Post , 2024.\n[192] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\n[193] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny\nPan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du,\nYiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang,\nJunting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu,\nSine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo\nWang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao,\nJiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. MAP-Neo: Highly capable\nand transparent bilingual large language model series. arXiv preprint arXiv:2405.19327 , 2024.\n[194] Hongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: A deep diagnosis of essential\ncommonsense knowledge for answering winograd schema challenge. In Annual Meeting of\nthe Association for Computational Linguistics , pages 5736–5745, 2020.\n[195] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:\n1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning\nRepresentations , 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM .\n[196] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and D. Roth.\nTemporal reasoning on implicit events from distant supervision. ArXiv , abs/2010.12753, 2020.\n26\n--- Page 27 ---\nAppendix\nTable of Contents\nA Contributions 28\nB Detailed Description of Sources 28\nB.1 Scientific and Scholarly Text . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nB.2 Online Discussions and Forums . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nB.3 Government and Legal Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nB.4 Curated Task Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nB.5 Books in the Public Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nB.6 Open Educational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nB.7 Wikis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nB.8 Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nB.9 Transcribed Audio Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nB.10 Web Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nC Additional insights on licensing 33\nC.1 Why we can’t always trust automatic license detection . . . . . . . . . . . . . . 34\nD List of Data Provenance Initiative sources 34\nE List of News sources 44\nF List of WikiMedia wikis 44\nG CCCC Source Statistics 44\nH PeS2o Source Statistics 46\nI Growth rates of openly licensed data 47\nJ Details on filtering pipelines 47\nK Details on Comma’s pre-training data mixture 49\nL Details on Comma’s cool-down data mixture 51\nM Details on small-scale data ablations 52\nN Additional Comma results 52\nO Additional training runs 53\nO.1 Ablations at 1T Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n27\n--- Page 28 ---\nA Contributions\nFigure 5: Author contributions to this work. Large squares indicate a major contribution and small\nsquares indicate a supporting contribution.\nB Detailed Description of Sources\nBelow, we give a more in-depth overview of the sources that make up the Common Pile, including\nspecific license decisions and tools used during collection.\nB.1 Scientific and Scholarly Text\nScientific and scholarly texts are a staple of modern LLM pretraining corpora, appearing in nearly all\nlarge-scale datasets [e.g. 57,185,167] since they expose models to technical terminology, formal\nreasoning, and long-range document structure—skills that are essential for downstream tasks in\nscience, education, and question answering. Thanks to open access mandates and academic cultural\nnorms, many scholarly texts are either in the public domain or are distributed under open licenses.\npeS2o To ensure broad coverage across many scientific disciplines, we include a version of\npeS2o [ 166] restricted to openly licensed articles. pes2o is derived from S2ORC [ 104], a cor-\npus of openly licensed abstract and full-text papers that have been converted to a structured format\nusing Grobid [ 63]. Starting from Grobid’s XML output, peS2o filters papers that are too short, have\nincorrect metadata, are in languages other than English, and contain OCR errors using a combination\nof heuristic- and model-based filtering steps. We refer the reader to the datasheet and code for more\ndetails on this processing pipeline. The subset of peS2o included in the Common Pile starts from\nv3of the corpus, which contains documents from January 1, 1970 to October 6, 2024. We retain\nfull-text papers with CC BY, CC BY-SA, or CC0 licenses, or that have been labeled as public domain;\nmetadata is provided by the Semantic Scholar APIs [ 87]. After filtering, this set contains 6.3 million\n28\n--- Page 29 ---\npapers, or 35.7 billion whitespace-separated segments. We provide more details on the composition\nof this subset in Appendix H.\nPubMed PubMed Central (PMC) is an open-access archive of biomedical and life sciences research\npapers maintained by the U.S. National Institutes of Health’s National Library of Medicine. We\ncollected papers from PMC whose metadata indicated that the publishing journal had designated a\nCC BY, CC BY-SA, or CC0 license. PMC stores the text content of each article as a single XML file,\nwhich we convert to markdown using pandoc.\nArXiv Papers ArXiv is an online open-access repository of over 2.4 million scholarly papers\ncovering fields such as computer science, mathematics, physics, quantitative biology, economics,\nand more. When uploading papers, authors can choose from a variety of licenses. We included\ntext from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses in the Common Pile\nthrough a three-step pipeline: first, the latex source files for openly licensed papers were downloaded\nfrom ArXiv’s bulk-access S3 bucket; next, the L ATEXML conversion tool was used to convert these\nsource files into a single HTML document; finally, the HTML was converted to plaintext using the\nTrafilatura [10] HTML-processing library.\nArXiv Abstracts Each paper uploaded to ArXiv includes structured metadata fields, including an\nabstract summarizing the paper’s findings and contributions. According to ArXiv’s licensing policy,\nthe metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless of\nthe license of the paper itself. Thus, we include as an additional source the abstracts for every paper\nsubmitted to ArXiv. We source the abstracts from ArXiv’s API via the Open Archives Initiative\nProtocol for Metadata Harvesting endpoint and reproduce them as-is.\nB.2 Online Discussions and Forums\nOnline forums are a rich source of multi-turn, user-generated dialogue covering a wide range of\ntopics. These platforms often feature question–answer pairs, problem-solving discussions, and\ninformal explanations of technical and non-technical concepts. The Common Pile incorporates online\ndiscussions from sources that distribute content under an open license.\nStackExchange While StackExchange formerly provided structured XML dumps of all of their\ncontent, since July of 2024, StackExchange has stopped publishing dumps to the Internet Archive.\nInstead, each site can provide a logged-in user with a custom URL to download the dump for that\nsite. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible.\nAdditionally, in dumps produced by the new export tool, many questions that are available in past\ndumps (and accessible on the site) are not present. We therefore extract all questions and answers\nfrom community uploaded dumps from December of 2024 from the Internet Archive and additionally\nextract missing questions and answers from the last official dumps in July of 2024 to account for the\ndeficiencies listed above. We use a question, its comments, its answers and the comments on each\nanswer as a single document. Following the display order on StackExchange, answers are ordered by\nthe number of votes they received, with the exception that the “accepted answer” always appears first.\nPyMarkdown was used to convert each comment into plain text.\nGitHub Archive According to GitHub’s terms of service, issues and pull request descriptions—\nalong with their comments—inherit the license of their associated repository. To collect this data,\nwe used the GitHub Archive’s public BigQuery table of events to extracted all issue, pull request,\nand comment events since 2011 and aggregated them into threads. The table does not include “edit”\nevents so the text from each comment is the original from when it was first posted. We filtered\nout comments from bots. This resulted in approximately 177 million threads across 19 million\nrepositories. We then removed threads whose repositories did not have a Blue Oak Council-approved\nlicense. License information for each repository comes from either 1) the “public-data:github_repos”\nBigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 million\nrepositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. When\nparsing failed, the raw markdown was kept.\nUbuntu IRC Logs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004\nhave been archived and released into the Public Domain. We downloaded all chats from all channels\nup until March of 2025. We consider all messages for given channel on a given day as a single\ndocument. We removed system messages as well as those from known bots.\n29\n--- Page 30 ---\nB.3 Government and Legal Texts\nGovernments produce a vast amount of informational text, ranging from legislation and legal opinions\nto scientific reports, public communications, and regulatory notices. This content is explicitly intended\nto inform the public, and as such, in many jurisdictions it is published directly into the public domain\nor under open licenses. In the United States, for example, works authored by federal employees as\npart of their official duties are not subject to copyright. Government and legal texts offer language\nmodels exposure to formal argumentation, legal reasoning, and procedural language.\nUS Government Publishing Office The United States Government Publishing Office (USGPO) is\na federal agency responsible for disseminating official documents authored by the U.S. government.\nThe Common Pile v0.1 includes all plain-text documents made available through the USGPO’s\nGovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issues\nof the Federal Register, congressional hearing transcripts, budget reports, economic indicators, and\nother federal publications.\nUS Patents and Trademark Office In the US, patent documents are released into the public\ndomain as government works. Patents follow a highly standardized format with distinct required\nsections for background, detailed description, and claims. We include parents from the US Patents\nand Trademark Office (USPTO) as provided by the Google Patents Public Data dataset [ 77], which\nincludes millions of granted patents and published patent applications dating back to 1782. We\nprocessed these documents to extract clean text while preserving this structured format. Mathematical\nexpressions and equations were converted into L ATEX.\nCaselaw Access Project and Court Listener The Common Pile contains 6.7 million cases from\nthe Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40\nmillion pages of U.S. federal and state court decisions and judges’ opinions from the last 365 years.\nIn addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The Caselaw\nAccess Project and Court Listener source legal data from a wide variety of resources such as the\nHarvard Law Library, the Law Library of Congress, and the Supreme Court Database. From these\nsources, we only included documents that were in the public domain. Erroneous OCR errors were\nfurther corrected after digitization, and additional post-processing was done to fix formatting and\nparsing.\nUK Hansard Hansard represents the official record of parliamentary proceedings across the\nUnited Kingdom’s legislative bodies. The Common Pile incorporates records from multiple sources,\nincluding debates and written answers from the UK Commons and Lords, devolved legislatures\n(Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), London\nMayor’s Questions, and ministerial statements. Data was sourced from ParlParse [ 131], covering\nCommons debates from 1918 forward and Lords proceedings from the 1999 reform. Each document\nwas processed to preserve complete parliamentary sessions as cohesive units, maintaining the natural\nflow of debate. All content is published under the Open Parliament License [181].\nRegulations.gov Regulations.gov is an online platform operated by the U.S. General Services\nAdministration that collates newly proposed rules and regulations from federal agencies along with\ncomments and feedback from the general public. The Common Pile includes all plain-text regulatory\ndocuments published by U.S. federal agencies on this platform, acquired via the bulk download\ninterface provided by Regulations.gov.\nB.4 Curated Task Data\nCurated datasets that cover specific tasks such as question answering, summarization, or text classifi-\ncation are often released via open licenses to the research community. While not traditionally part\nof pretraining corpora, including a small amount of task-oriented data during pretraining can help\nmodels acquire early familiarity with task formats and prompt–completion structures.\nData Provenance Initiative The Data Provenance Initiative is a digital library of supervised\ndatasets that have been manually annotated with their source and license information [ 106,109].\nWe leverage their tooling to filter HuggingFace datasets, based on a range of criteria, including\ntheir licenses, which may be particularly relevant for supervised datasets [ 114]. Specifically, we\nfilter the data according to these criteria: contains English language or code data, the text is not\nmodel-generated, the dataset’s audit yielded a open license and the original sources of the data are\nonly from recognized public domain sources.\n30\n--- Page 31 ---\nB.5 Books in the Public Domain\nBooks represent a time-tested resource for language model pretraining, offering carefully edited,\nlong-form prose that supports learning of narrative coherence and long-range dependency modeling.\nFor these reasons, many large-scale pretraining corpora—including the Pile [ 57], Dolma [ 167],\nand RedPajama [ 185]—include content from books [ 41]. In the United States, as of 2024, books\npublished prior to 1929 are in the public domain. Thus, the Common Pile includes public domain\nbooks drawn from curated collections, covering topics such as literature, science, and history.\nBiodiversity Heritage Library The Biodiversity Heritage Library (BHL) is an open-access digital\nlibrary for biodiversity literature and archives. The Common Pile contains over 42 million public\ndomain books and documents from the BHL collection. These works were collected using the bulk\ndata download interface provided by the BHL and were filtered based on their associated license\nmetadata. We use the optical character recognition (OCR)-generated text distributed by BHL.\nPre-1929 Books Books published in the US before 1929 passed into the public domain on January\n1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitized\nbooks which were published in the US before 1929. The collection contains over 130,000 books\ndigitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCR\nplain text files were downloaded directly from the Internet Archive website.\nLibrary of Congress The Library of Congress (LoC) curates a collection of public domain books\ncalled “Selected Digitized Books”. We downloaded over 130,000 English-language books from this\npublic domain collection as OCR plain text files using the LoC APIs.\nProject Gutenberg Project Gutenberg is an online collection of over 75,000 digitized books\navailable as plain text. We use all books that are 1) English and 2) marked as in the Public Domain\naccording to the provided metadata. Additionally, we include any books that are part of the pg19 [ 140]\ndataset, which only includes books that are over 100 years old. Minimal preprocessing is applied\nto remove the Project Gutenberg header and footers, and many scanned books include preamble\ninformation about who digitized them.\nB.6 Open Educational Resources\nOpen Educational Resources (OERs) are educational materials, typically published under open\nlicenses, to support free and equitable access to education. These resources include educational\nartifacts such as textbooks, lecture notes, lesson plans, syllabi, and problem sets. For language\nmodels, OERs offer exposure to instructional formatting and domain-specific information, making\nthem valuable for improving performance on knowledge-based downstream tasks. The Common Pile\nincludes a range of such materials sourced from major OER repositories, including collections of\nopen-access books and structured teaching resources.\nDirectory of Open Access Books The Directory of Open Access Books (DOAB) is an online\nindex of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collect\nthe openly licensed content from DOAB, we retrieve metadata using their official metadata feed.\nWe then filter the collection to include only English-language books released under CC BY and CC\nBY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext using\nthe Marker PDF-to-text converter. As an additional validation step, we manually create a whitelist of\nopen license statements and retain only texts explicitly containing one of these statements in their\nfront- or back-matter.\nPressBooks PressBooks is a searchable catalog of over 8,000 open access books. To collect openly\nlicensed content from PressBooks we construct a search query to retrieve URLs for all books written\nin English and listed as public domain or under CC BY or CC BY-SA licenses. For each matched\nbook, we collect its contents directly from the publicly available web version provided by PressBooks.\nOERCommons OERCommons is an online platform where educators share open-access instruc-\ntional materials—such as textbooks, lesson plans, problem sets, course syllabi, and worksheets—with\nthe goal of expanding access to affordable education. To collect the openly licensed content available\non OERCommons, we construct a search query to retrieve English-language content released into the\npublic domain or under CC BY or CC BY-SA licenses. The resulting documents are converted to\nplain text directly from the HTML pages hosted on the OERCommons website.\n31\n--- Page 32 ---\nLibreTexts LibreTexts is an online platform that provides a catalog of over 3,000 open-access\ntextbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks in\nthe catalog and check each textbook section for a license statement indicating that it is in the public\ndomain or under a CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plain\ntext from these textbook sections directly from the HTML pages hosted on the LibreTexts website.\nB.7 Wikis\nWikis are collaboratively maintained websites that organize information around specific topics or\ndomains. Their crowd-sourced nature, coupled with community moderation and citation requirements,\noften results in text that is both informative and well-structured. Prominent examples such as\nWikipedia have become staples in large-scale language model pretraining corpora due to their breadth\nof coverage and high quality. In addition, most major wikis are distributed under open licenses such\nas CC BY and CC BY-SA. The Common Pile includes content from a range of openly licensed wikis\nto provide models with structured and well-researched informational text.\nWikimedia We downloaded the official database dumps from March 2025 of the English-language\nwikis that are directly managed by the Wikimedia foundation (see Appendix F for a complete list).\nThese database dumps include the wikitext—Mediawiki’s custom markup language—for each page as\nwell as talk pages, where editors discuss changes made to a page. We only use the most recent version\nof each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments in\nformatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitext\nmath into L ATEX math using our custom code. Finally, any remaining HTML tags were removed via\nregexes.\nWikiteam There are many wikis on the internet that are not managed by the Wikimedia foundation,\nbut do use their MediaWiki software to power their wiki. Many of these wikis have been archived\nby wikiteam, a collection of volunteers that create unofficial database dumps of wikis and upload\nthem to the Internet Archive. We download all dumps made by wikiteam when the metadata indicates\nthe wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the Internet\nArchive in September of 2024. This results in downloading approximately 330,000 wikis. When\nmultiple dumps of the same wiki exists, we use the most recent dump. The wikitext was converted\nto plain text following the same steps as with Wikimedia wikis. After preprocessing, we removed\ndocuments from wikis that appeared to contain large amounts of license laundering, e.g. those that\nwere collections of song lyrics or transcripts.\nB.8 Source Code\nSource code has become an increasingly important component of large-scale language model pretrain-\ning corpora, as it enables models to learn syntax, program structure, and problem solving strategies\nuseful for both code generation and reasoning tasks. Thanks to the Free and Open Source Software\n(FOSS) movement, code also happens to be one of the most openly licensed forms of text, with\nmany software repositories distributed under open licenses such as MIT, BSD, Apache 2.0, and the\nGNU Free Documentation License (GFDL). The Common Pile includes high-quality, openly licensed\nsource code from large-scale public code datasets and documentation standards, enabling models\ntrained on it to perform better on coding and technical writing tasks.\nThe Stack V2 The Stack V2 [ 113] consists of a mixture of openly licensed and unlicensed work.\nWe use the tooling that the Software Heritage Foundation and BigCode created to build our dataset.\nIn particular, we relied on the license detection performed by the creators of Stack V2. When multiple\nlicenses are detected in a single repository, we make sure that allof them meet our definition of\n“openly licensed”.\nPython Enhancement Proposals Python Enhancement Proposals, or PEPs, are design documents\nthat generally provide a technical specification and rationale for new features of the Python program-\nming language. There are been 661 PEPs published. The majority of PEPs are published in the Public\nDomain, but 5 were published under the “Open Publication License” and omitted. PEPs are long,\nhighly-polished, and technical in nature and often include code examples paired with their prose.\nPEPs are authored in ReStructured Text; we used pandoc, version 3.5, to convert them to plain text.\n32\n--- Page 33 ---\nB.9 Transcribed Audio Content\nA historically underutilized source of text data is speech transcribed from audio and video content.\nSpoken language in educational videos, speeches, and interviews provide an opportunity for models\nto learn conversational speech patterns.\nCreative Commons YouTube YouTube is large-scale video-sharing platform where users have the\noption of uploading content under a CC BY license. To collect high-quality speech-based textual\ncontent and combat the rampant license laundering on YouTube, we manually curated a set of over\n2,000 YouTube channels that consistently release original openly licensed content containing speech.\nThe resulting collection spans a wide range of genres, including lectures, tutorials, reviews, video\nessays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensed\nvideos comprising more than 470,000 hours of content. Finally, each video was transcribed to text\nusing the Whisper speech recognition model [138].\nB.10 Web Text\nThe success of modern LLM pre-training relies on text scraped indiscriminately from the web, as\nweb text covers an extremely diverse range of textual domains. In the Common Pile, we restrict this\napproach to only include web content with clear public domain status or open license statements.\nCreative Commons Common Crawl We sourced text from 52 Common Crawl snapshots, covering\nabout half of Common Crawl snapshots available to date and covering all years of operations of\nCommon Crawl up to 2024. We found a higher level of duplication across this collection, suggesting\nthat including more snapshots would lead to a modest increase in total token yield. From these\nsnapshots, we extract HTML content using FastWarc [ 15]. Then, using a regular expression adapted\nfrom the C4Corpus project [ 68], we retain only those pages where a CC BY, CC BY-SA, or CC0\nlicense appears. To ensure license accuracy, we manually verified the top 1000 domains by content\nvolume, retaining only the 537 domains with confirmed licenses where the Creative Commons\ndesignation is applied to all text content rather than only embedded media or a subset of the text\non the domain. We extract the main content of these documents and remove boilerplate using\nResiliparse [ 14]. We perform URL-level exact deduplication and use Bloom filters to remove near-\nduplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma [ 167];\nnamely, we use C4-derived heuristics [ 142] to filter pages containing Javascript, Lorem Ipsum, and\ncurly braces {}. We also apply all Gopher rules [ 141] to remove low-quality pages. We provide more\ndetails on the composition of this subset in Appendix G.\nFoodista Foodista is a community-maintained site with recipes, food-related news, and nutrition\ninformation. All content is licensed under CC BY. Plain text is extracted from the HTML using a\ncustom pipeline that includes extracting title and author information to include at the beginning of\nthe text. Additionally, comments on the page are appended to the article after we filter automatically\ngenerated comments.\nNews We scrape the news sites that publish content under CC BY or CC BY-SA according to\nopennewswire. A full list of sites can be found in Appendix E. Plain text was extracted from\nthe HTML using our custom pipeline, including extraction of the title and byline to include at the\nbeginning of each article.\nPublic Domain Review The Public Domain Review is an online journal dedicated to exploration\nof works of art and literature that have aged into the public domain. We collect all articles published\nin the Public Domain Review under a CC BY-SA license.\nC Additional insights on licensing\nThere are many standards we could have chosen for what licenses to include in our dataset. The open\nsource, knowledge, and culture movements have harmonized on the high level principles described in\nsection 1: “open” means that permission is granted for content to be freely used, studied, modified,\nand shared for any purpose. This language is found in the Open Knowledge Definition we follow\nas well as the Open Source Institute’s Open Definition, Creative Commons’s statement on Open\nCulture, Wikimedia’s Acceptable licenses policy and more. Our work was also developed to be\nconsistent with the Open movement’s work in the specific context of AI technologies such as the\n33\n--- Page 34 ---\nOpen Source Initiative’s Open Source AI Definition and in consultations with leading members of\nthe community [9].\nC.1 Why we can’t always trust automatic license detection\nThere are many reasons why identifying the licensing status of internet text with automatic tooling\ncan be challenging. In this section, we briefly discuss some major themes from our experience.\nThere are many ways to say the same thing. While there exist standards for how to express a\nlicense, people don’t always follow those standards and failure to follow the standards doesn’t mean\nthat the license is invalid. For example, simple string matching on “CC BY” misses a huge amount of\nCC BY licensed text because a very common way to denote Creative Commons licenses is using an\nimage badge. Current web-processing tools are substantially stronger at identifying text than images,\nand the failure rate on sites using image badges is quite high.\nLack of understanding of licenses. Most people are not lawyers and do not understand the full\nlegal scope and meaning of the licenses that they attempt to put on their text. Developers routinely\ntweak boilerplate to produce ambiguous language like (“Licensed under MIT-ish terms”) or write\ncontradictory statements (“All rights reserved / CC-BY”). In general, it is common for people to write\nquasi-legal language along side a more traditional license. Non-standard licenses require substantial\namounts of work to interpret and are not always valid or meaningful.\nLicensing signals can be noisy. Even when a developer intends to clearly communicate a specific\nlicense, contradictions and errors can occur in practice. For example, Longpre et al. [107] found that\nthere were substantial disagreements between the terms of service of a website and the restrictions\nfound in a robots.txt file. We have not yet found a reliable way to have an automatic system identify\nlicensed text and therefore frequently resort to manual review by humans.\nD List of Data Provenance Initiative sources\nThe openly licensed supervised datasets included in the Common Pile are listed in Table 1. These\ndatasets were identified and collected using metadata from the Data Provenance Initiative. For more\ninformation on these datasets, consult the Data Provenance Initiative Dataset Explorer.\nTable 1: Supervised datasets included in the Common Pile from the Data Provenance Initiative\ncollection.\nCollection Dataset Identifier Licenses\nAgentInstruct AgentInstruct-alfworld[164] MIT License\nHelpSteer HelpSteer[184] CC BY 4.0\nAya Dataset aya-english[165] Apache License 2.0\nCommitPackFT commitpackft-abap[165] MIT License\nCommitPackFT commitpackft-agda[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-apl[165] MIT License, ISC License\nCommitPackFT commitpackft-arc[165] MIT License\nCommitPackFT commitpackft-aspectj[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-ats[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-blitzmax[165] MIT License\nCommitPackFT commitpackft-bluespec[165] MIT License\nCommitPackFT commitpackft-boo[165] MIT License\nContinued on next page\n34\n--- Page 35 ---\nCollection Dataset Identifier Licenses\nCommitPackFT commitpackft-brainfuck[165] Apache License 2.0, BSD 2-Clause\nLicense, MIT License\nCommitPackFT commitpackft-bro[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-cartocss[165] MIT License\nCommitPackFT commitpackft-chapel[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-clean[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-coldfusion[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-creole[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-crystal[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-dns-zone[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-dylan[165] MIT License\nCommitPackFT commitpackft-eiffel[165] MIT License\nCommitPackFT commitpackft-emberscript[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-fancy[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-flux[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-forth[165] MIT License\nCommitPackFT commitpackft-g-code[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-gdscript[165] Apache License 2.0, CC0 1.0, MIT\nLicense\nCommitPackFT commitpackft-genshi[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-graphql[165] Apache License 2.0, BSD 3-Clause\nLicense, CC0 1.0, MIT License\nCommitPackFT commitpackft-harbour[165] MIT License\nCommitPackFT commitpackft-hlsl[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-http[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-idris[165] MIT License, BSD 3-Clause\nLicense, BSD 2-Clause License\nCommitPackFT commitpackft-igor-pro[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-inform-7[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-ioke[165] MIT License\nCommitPackFT commitpackft-isabelle[165] MIT License, BSD 2-Clause License\nCommitPackFT commitpackft-jflex[165] MIT License\nCommitPackFT commitpackft-json5[165] MIT License, BSD 3-Clause\nLicense, BSD 2-Clause License\nCommitPackFT commitpackft-jsonld[165] Apache License 2.0, BSD 3-Clause\nLicense, CC0 1.0, MIT License\nCommitPackFT commitpackft-krl[165] MIT License\nCommitPackFT commitpackft-latte[165] MIT License\nCommitPackFT commitpackft-lean[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-lfe[165] Apache License 2.0, MIT License\nContinued on next page\n35\n--- Page 36 ---\nCollection Dataset Identifier Licenses\nCommitPackFT commitpackft-lilypond[165] MIT License\nCommitPackFT commitpackft-liquid[165] Apache License 2.0, CC0 1.0, MIT\nLicense\nCommitPackFT commitpackft-literate-agda[165] MIT License\nCommitPackFT commitpackft-literate-\ncoffeescript[165]MIT License\nCommitPackFT commitpackft-literate-haskell[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-llvm[165] Apache License 2.0, BSD 3-Clause\nLicense, BSD 2-Clause License,\nMIT License\nCommitPackFT commitpackft-logos[165] Apache License 2.0, BSD 3-Clause\nLicense, BSD 2-Clause License,\nMIT License, ISC License\nCommitPackFT commitpackft-lsl[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-maple[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-mathematica[165] MIT License, CC0 1.0\nCommitPackFT commitpackft-metal[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-mirah[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-monkey[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-moonscript[165] MIT License\nCommitPackFT commitpackft-mtml[165] MIT License\nCommitPackFT commitpackft-mupad[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-nesc[165] MIT License\nCommitPackFT commitpackft-netlinx[165] MIT License\nCommitPackFT commitpackft-ninja[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-nit[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-nu[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-ooc[165] MIT License\nCommitPackFT commitpackft-openscad[165] MIT License, CC0 1.0, BSD\n2-Clause License\nCommitPackFT commitpackft-oz[165] MIT License, BSD 2-Clause License\nCommitPackFT commitpackft-pan[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-piglatin[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-pony[165] MIT License, BSD 2-Clause License\nCommitPackFT commitpackft-propeller-spin[165] MIT License\nCommitPackFT commitpackft-pure-data[165] MIT License\nCommitPackFT commitpackft-purebasic[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-purescript[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-ragel-in-ruby-\nhost[165]MIT License\nCommitPackFT commitpackft-rebol[165] Apache License 2.0, MIT License\nContinued on next page\n36\n--- Page 37 ---\nCollection Dataset Identifier Licenses\nCommitPackFT commitpackft-red[165] MIT License, BSD 2-Clause License\nCommitPackFT commitpackft-rouge[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-sage[165] MIT License\nCommitPackFT commitpackft-sas[165] MIT License\nCommitPackFT commitpackft-scaml[165] MIT License, BSD 2-Clause License\nCommitPackFT commitpackft-scilab[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-slash[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-smt[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-solidity[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-sourcepawn[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-squirrel[165] MIT License\nCommitPackFT commitpackft-ston[165] MIT License\nCommitPackFT commitpackft-systemverilog[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-unity3d-asset[165] Apache License 2.0, BSD 3-Clause\nLicense, BSD 2-Clause License,\nMIT License, ISC License, CC0 1.0\nCommitPackFT commitpackft-uno[165] MIT License\nCommitPackFT commitpackft-unrealscript[165] MIT License\nCommitPackFT commitpackft-urweb[165] MIT License, BSD 3-Clause License\nCommitPackFT commitpackft-vcl[165] Apache License 2.0, BSD 3-Clause\nLicense, MIT License\nCommitPackFT commitpackft-xbase[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-xpages[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-xproc[165] Apache License 2.0, MIT License\nCommitPackFT commitpackft-yacc[165] MIT License, ISC License, BSD\n2-Clause License\nCommitPackFT commitpackft-zephir[165] MIT License\nCommitPackFT commitpackft-zig[165] MIT License\nDolly 15k dolly-brainstorming[165] CC BY-SA 3.0\nDolly 15k dolly-classification[165] CC BY-SA 3.0\nDolly 15k dolly-closedqa[165] CC BY-SA 3.0\nDolly 15k dolly-creative_writing[165] CC BY-SA 3.0\nDolly 15k dolly-infoextract[165] CC BY-SA 3.0\nDolly 15k dolly-openqa[165] CC BY-SA 3.0\nDolly 15k dolly-summarization[165] CC BY-SA 3.0\nDialogStudio ds-ABCD[165] Apache License 2.0, MIT License\nDialogStudio ds-ATIS[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-ATIS-NER[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-AirDialogue[165] Apache License 2.0\nDialogStudio ds-AntiScam[165] Apache License 2.0, CC0 1.0\nDialogStudio ds-BANKING77[165] Apache License 2.0, CC BY 4.0\nContinued on next page\n37\n--- Page 38 ---\nCollection Dataset Identifier Licenses\nDialogStudio ds-BANKING77-OOS[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-BiTOD[165] Apache License 2.0\nDialogStudio ds-CLINC-Single-Domain-OOS-\nbanking[165]Apache License 2.0, CC BY 3.0\nDialogStudio ds-CLINC-Single-Domain-OOS-\ncredit_cards[165]Apache License 2.0, CC BY 3.0\nDialogStudio ds-CLINC150[165] Apache License 2.0, CC BY-SA 3.0\nDialogStudio ds-CaSiNo[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-CoQA[165] Apache License 2.0, MIT License\nDialogStudio ds-CoSQL[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-ConvAI2[165] Apache License 2.0\nDialogStudio ds-CraigslistBargains[165] Apache License 2.0, MIT License\nDialogStudio ds-DART[165] Apache License 2.0, MIT License\nDialogStudio ds-DSTC8-SGD[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-DialogSum[165] Apache License 2.0, MIT License\nDialogStudio ds-Disambiguation[165] Apache License 2.0, MIT License\nDialogStudio ds-FeTaQA[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-GECOR[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-GrailQA[165] Apache License 2.0\nDialogStudio ds-HDSA-Dialog[165] Apache License 2.0, MIT License\nDialogStudio ds-HH-RLHF[165] Apache License 2.0, MIT License\nDialogStudio ds-HWU64[165] Apache License 2.0, CC BY-SA 3.0\nDialogStudio ds-HybridQA[165] Apache License 2.0, MIT License\nDialogStudio ds-KETOD[165] Apache License 2.0, MIT License\nDialogStudio ds-MTOP[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-MULTIWOZ2_2[165] Apache License 2.0, MIT License\nDialogStudio ds-MulDoGO[165] Apache License 2.0, CDLA\nPermissive 1.0\nDialogStudio ds-MultiWOZ_2.1[165] Apache License 2.0, MIT License\nDialogStudio ds-Prosocial[165] Apache License 2.0, MIT License\nDialogStudio ds-RESTAURANTS8K[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-SGD[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-SNIPS[165] Apache License 2.0\nDialogStudio ds-SNIPS-NER[165] Apache License 2.0\nDialogStudio ds-SParC[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-SQA[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-STAR[165] Apache License 2.0, MIT License\nDialogStudio ds-Spider[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-TOP[165] Apache License 2.0, CC BY-SA\nDialogStudio ds-TOP-NER[165] Apache License 2.0, CC BY-SA\nDialogStudio ds-Taskmaster1[165] Apache License 2.0, CC BY 4.0\nContinued on next page\n38\n--- Page 39 ---\nCollection Dataset Identifier Licenses\nDialogStudio ds-Taskmaster2[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-Taskmaster3[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-ToTTo[165] Apache License 2.0, CC BY-SA 3.0\nDialogStudio ds-TweetSumm[165] Apache License 2.0, CC0 1.0\nDialogStudio ds-WOZ2_0[165] Apache License 2.0\nDialogStudio ds-WebQSP[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-WikiSQL[165] Apache License 2.0, BSD 3-Clause\nLicense\nDialogStudio ds-WikiTQ[165] Apache License 2.0, CC BY-SA 4.0\nDialogStudio ds-chitchat-dataset[165] Apache License 2.0, MIT License\nDialogStudio ds-wizard_of_internet[165] Apache License 2.0, CC BY 4.0\nDialogStudio ds-wizard_of_wikipedia[165] Apache License 2.0, CC BY 4.0\nFlan Collection (Chain-of-Thought) fc-cot-cot_gsm8k[34] MIT License\nFlan Collection (Chain-of-Thought) fc-cot-cot_strategyqa[59] CC BY-SA 3.0\nFlan Collection (Chain-of-Thought) fc-cot-stream_creak[125] MIT License, CC BY-SA 4.0\nFlan Collection (Chain-of-Thought) fc-cot-stream_esnli[21] MIT License, CC BY-SA 4.0\nFlan Collection (Flan 2021) fc-flan-drop[46] CC BY 4.0\nFlan Collection (Flan 2021) fc-flan-e2e_nlg[123] CC BY-SA 4.0\nFlan Collection (Flan 2021) fc-flan-natural_questions[89] Apache License 2.0, CC BY-SA 3.0\nFlan Collection (Flan 2021) fc-flan-quac[29] CC BY-SA 4.0\nFlan Collection (Flan 2021) fc-flan-squad_v1[147] CC BY-SA 4.0\nFlan Collection (Flan 2021) fc-flan-squad_v2[147] CC BY-SA 4.0\nFlan Collection (Flan 2021) fc-flan-trec[100] CC0 1.0\nFlan Collection (Flan 2021) fc-flan-true_case[100] CC0 1.0\nFlan Collection (Flan 2021) fc-flan-wiki_lingua_english_en[90] CC BY 3.0\nFlan Collection (Flan 2021) fc-flan-winogrande[158] Apache License 2.0, CC BY 4.0\nFlan Collection (Flan 2021) fc-flan-wnli[98] CC BY 4.0\nFlan Collection (Flan 2021) fc-flan-word_segment[98] CC0 1.0\nFlan Collection (Flan 2021) fc-flan-wsc[98] CC BY 4.0\nFlan Collection (P3) fc-p3-adversarial_qa[11] CC BY-SA 3.0\nFlan Collection (P3) fc-p3-cos_e[144] BSD 3-Clause License\nFlan Collection (P3) fc-p3-dbpedia_14[97] CC BY-SA 3.0\nFlan Collection (P3) fc-p3-hotpotqa[190] Apache License 2.0, CC BY-SA 4.0\nFlan Collection (P3) fc-p3-quarel[153] CC BY 4.0\nFlan Collection (P3) fc-p3-quartz[170] CC BY 4.0\nFlan Collection (P3) fc-p3-quoref[44] CC BY 4.0\nFlan Collection (P3) fc-p3-web_questions[13] CC BY 4.0\nFlan Collection (P3) fc-p3-wiki_bio[93] CC BY-SA 3.0\nFlan Collection (P3) fc-p3-wiki_hop[93] CC BY-SA 3.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-adversarial_qa[11] CC BY-SA 3.0\nContinued on next page\n39\n--- Page 40 ---\nCollection Dataset Identifier Licenses\nFlan Collection\n(Super-NaturalInstructions)fc-sni-adverserial_qa[11] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-air_dialogue[187] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-ancora_ca_ner[187] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-anem[124] MIT License, CC BY-SA 3.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-argkp Apache License 2.0, CC BY-SA 3.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-asian_language_-\ntreebank[151]CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-atomic[75] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-bard[54] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-cedr[161] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-circa[112] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-clue_cmrc2018[42] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-coached_conv_pref[139] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-copa_hr BSD 2-Clause License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-crows_pairs[122] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-cuad[71] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-defeasible_nli_atomic[155] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-disfl_qa[67] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-e_snli[21] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-gap[186] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-hotpotqa[190] Apache License 2.0, CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-human_ratings_of_natural_-\nlanguage_generation_outputs[190]CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-hybridqa[26] CC BY 4.0, MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-iirc[53] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-jigsaw[53] CC0 1.0\nContinued on next page\n40\n--- Page 41 ---\nCollection Dataset Identifier Licenses\nFlan Collection\n(Super-NaturalInstructions)fc-sni-librispeech_asr[127] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-logic2text[27] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-numeric_fused_head[49] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-offenseval_dravidian[49] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-open_pi[172] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-paper_reviews_data_set[172] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-poem_sentiment[163] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-propara[43] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-quarel[153] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-quartz[170] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-quoref[44] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-ro_sts_parallel[48] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-schema_guided_dstc8[148] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-scitail[86] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-scitailv1.1[86] Apache License 2.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-semeval_2020_task4[86] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-sms_spam_collection_v.1[86] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-splash[86] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-squad2.0[147] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-squad_1.1[146] CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-strategyqa[59] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-universal_dependencies___-\nenglish_dependency_treebank[59]CC BY-SA 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-web_questions[13] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-wiki_hop[93] CC BY-SA 3.0\nContinued on next page\n41\n--- Page 42 ---\nCollection Dataset Identifier Licenses\nFlan Collection\n(Super-NaturalInstructions)fc-sni-wikitext[116] CC BY-SA 3.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-winograd_wsc[98] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-winomt[168] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-winowhy[194] MIT License\nFlan Collection\n(Super-NaturalInstructions)fc-sni-wsc; enhanced_wsc[194] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-wsc_fiexed[194] CC BY-SA 3.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-xcopa[134] CC BY 4.0\nFlan Collection\n(Super-NaturalInstructions)fc-sni-xquad[6] CC BY-SA 4.0\nOpen Assistant oasst-en[88] Apache License 2.0, CC BY 4.0\nOpen Assistant OctoPack oasst-en-octopack[88] Apache License 2.0, CC BY 4.0\nOpen Assistant v2 oasst2-en[88] Apache License 2.0\nOIG oig-unified_canadian_-\nparliament[88]Apache License 2.0\nOIG oig-unified_cuad[88] Apache License 2.0, CC BY 4.0\nOIG oig-unified_grade_school_math_-\ninstructions[88]Apache License 2.0, MIT License\nOIG oig-unified_nq[88] Apache License 2.0, CC BY-SA 3.0\nOIG oig-unified_sqlv1[88] Apache License 2.0, CC BY-SA 4.0\nOIG oig-unified_sqlv2[88] Apache License 2.0, CC BY-SA 4.0\nOIG oig-unified_squad_v2_more_-\nneg[88]Apache License 2.0, CC BY-SA 4.0\nTasksource Instruct tsi-balanced_copa[85] BSD 2-Clause License\nTasksource Instruct tsi-breaking_nli[60] CC BY-SA 4.0\nTasksource Instruct tsi-cladder[60] MIT License\nTasksource Instruct tsi-condaqa[149] Apache License 2.0\nTasksource Instruct tsi-conj_nli[157] MIT License\nTasksource Instruct tsi-defeasible_nli-atomic[155] MIT License\nTasksource Instruct tsi-defeasible_nli-snli[155] MIT License\nTasksource Instruct tsi-dynasent-\ndynabench.dynasent.r1.all-r1[135]CC BY 4.0\nTasksource Instruct tsi-dynasent-\ndynabench.dynasent.r2.all-r2[135]CC BY 4.0\nTasksource Instruct tsi-fever_evidence_related-mwong_-\n_fever_related[177]CC BY-SA 4.0\nTasksource Instruct tsi-few_nerd-supervised[45] CC BY-SA 4.0\nTasksource Instruct tsi-fig_qa[102] MIT License\nTasksource Instruct tsi-fracas[56] MIT License\nContinued on next page\n42\n--- Page 43 ---\nCollection Dataset Identifier Licenses\nTasksource Instruct tsi-hyperpartisan_news[56] CC BY 4.0\nTasksource Instruct tsi-lex_glue-case_hold[23] Apache License 2.0\nTasksource Instruct tsi-lonli[174] MIT License\nTasksource Instruct tsi-moral_stories-full[51] MIT License\nTasksource Instruct tsi-neqa[51] CC BY 4.0\nTasksource Instruct tsi-prost Apache License 2.0\nTasksource Instruct tsi-quote_repetition CC BY 4.0\nTasksource Instruct tsi-recast-recast_factuality CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_megaveridicality CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_ner CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_puns CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_sentiment CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_verbcorner CC BY-SA 4.0\nTasksource Instruct tsi-recast-recast_verbnet CC BY-SA 4.0\nTasksource Instruct tsi-redefine_math CC BY 4.0\nTasksource Instruct tsi-tracie[196] Apache License 2.0\nTasksource Instruct tsi-truthful_qa-multiple_-\nchoice[101]Apache License 2.0\nTasksource Instruct tsi-vitaminc-tals__vitaminc[162] MIT License\nTasksource Instruct tsi-winowhy[194] MIT License\nTasksource Symbol-Tuning tsy-breaking_nli[60] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-cladder[60] MIT License\nTasksource Symbol-Tuning tsy-condaqa[149] Apache License 2.0\nTasksource Symbol-Tuning tsy-conj_nli[157] MIT License\nTasksource Symbol-Tuning tsy-defeasible_nli-atomic[155] MIT License\nTasksource Symbol-Tuning tsy-defeasible_nli-snli[155] MIT License\nTasksource Symbol-Tuning tsy-dynasent-\ndynabench.dynasent.r1.all-r1[135]CC BY 4.0\nTasksource Symbol-Tuning tsy-dynasent-\ndynabench.dynasent.r2.all-r2[135]CC BY 4.0\nTasksource Symbol-Tuning tsy-fever_evidence_related-\nmwong__fever_related[177]CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-fracas[56] MIT License\nTasksource Symbol-Tuning tsy-hyperpartisan_news[56] CC BY 4.0\nTasksource Symbol-Tuning tsy-lonli[174] MIT License\nTasksource Symbol-Tuning tsy-recast-recast_factuality[174] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-recast-recast_-\nmegaveridicality[174]CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-recast-recast_ner[174] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-recast-recast_puns[174] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-recast-recast_sentiment[174] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-recast-recast_verbcorner[174] CC BY-SA 4.0\nContinued on next page\n43\n--- Page 44 ---\nCollection Dataset Identifier Licenses\nTasksource Symbol-Tuning tsy-recast-recast_verbnet[174] CC BY-SA 4.0\nTasksource Symbol-Tuning tsy-tracie[196] Apache License 2.0\nTasksource Symbol-Tuning tsy-vitaminc-tals__vitaminc[162] MIT License\nTasksource Symbol-Tuning tsy-winowhy[194] MIT License\nE List of News sources\nThe Common Pile contains a variety of openly licensed news sources released under CC BY and\nCC BY-SA licenses. The sources licensed under CC BY include: 360info, Africa is a Country, Alt\nNews, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global V oices,\nMeduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New Canadian\nMedia, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, and ZimFact. The\nsources licensed under CC BY-SA include: Oxpeckers, Propastop, and The Public Record.\nF List of WikiMedia wikis\nOfficial Wikimedia wikis are released under a CC BY-SA license. The Common Pile includes the\nfollowing Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity,\nWikivoyage, and Wiktionary.\nG CCCC Source Statistics\nWe provide additional statistics on the CCCC subset of the Common Pile, including the number of\nunicode words and documents sourced from each Common Crawl snapshot, in Table 2.\nTable 2: Counts of words and documents extracted from 52 snapshots after filtering with our pipeline.\nSnapshot Unicode Words Documents\nCC-MAIN-2013-20 3,851,018,197 5,529,294\nCC-MAIN-2013-48 4,544,197,252 6,997,831\nCC-MAIN-2014-10 4,429,217,941 6,682,672\nCC-MAIN-2014-15 4,059,132,873 5,912,779\nCC-MAIN-2014-23 5,193,195,765 8,253,690\nCC-MAIN-2014-35 4,254,690,945 6,551,673\nCC-MAIN-2014-41 4,289,814,449 6,558,170\nCC-MAIN-2014-42 3,986,284,741 6,144,797\nCC-MAIN-2014-49 3,316,075,452 4,699,472\nCC-MAIN-2014-52 4,307,765,289 6,338,983\nCC-MAIN-2015-06 3,675,982,679 5,181,955\nCC-MAIN-2015-11 3,932,442,900 5,438,533\nCC-MAIN-2015-14 3,658,107,765 4,954,273\nContinued on next page\n44\n--- Page 45 ---\nSnapshot Unicode Words Documents\nCC-MAIN-2015-18 4,451,734,946 6,319,757\nCC-MAIN-2015-22 4,285,945,319 5,949,267\nCC-MAIN-2015-27 3,639,904,128 4,975,152\nCC-MAIN-2016-07 1,588,496,703 3,798,207\nCC-MAIN-2016-18 3,228,754,200 4,446,815\nCC-MAIN-2016-22 3,217,827,676 4,242,762\nCC-MAIN-2017-04 3,852,699,213 5,239,605\nCC-MAIN-2017-09 4,186,915,498 5,119,171\nCC-MAIN-2017-13 4,950,110,931 5,923,670\nCC-MAIN-2017-17 4,684,050,830 5,645,725\nCC-MAIN-2017-22 4,683,569,278 5,514,717\nCC-MAIN-2017-26 4,744,689,137 5,514,047\nCC-MAIN-2017-51 1,981,004,306 2,529,289\nCC-MAIN-2018-13 4,816,417,930 5,520,099\nCC-MAIN-2018-22 3,921,533,251 4,401,956\nCC-MAIN-2018-26 4,506,583,931 4,916,546\nCC-MAIN-2018-30 4,936,722,403 5,282,886\nCC-MAIN-2018-34 3,865,953,978 3,808,725\nCC-MAIN-2018-47 3,933,439,841 3,637,947\nCC-MAIN-2018-51 4,745,124,422 4,616,832\nCC-MAIN-2019-04 4,475,679,190 4,140,277\nCC-MAIN-2019-09 4,287,868,800 4,142,190\nCC-MAIN-2019-13 3,966,330,348 3,849,631\nCC-MAIN-2019-30 4,179,526,188 4,430,572\nCC-MAIN-2019-35 5,144,426,270 5,048,106\nCC-MAIN-2019-39 4,572,972,457 4,527,430\nCC-MAIN-2020-29 5,200,565,501 4,984,248\nCC-MAIN-2020-34 4,458,827,947 4,297,009\nCC-MAIN-2021-17 1,768,757,386 1,824,942\nCC-MAIN-2021-39 4,599,961,675 4,287,356\nCC-MAIN-2021-43 5,337,349,331 5,304,846\nCC-MAIN-2021-49 3,980,018,773 4,050,641\nCC-MAIN-2022-05 4,517,850,019 4,503,863\nCC-MAIN-2023-06 5,135,614,227 4,959,915\nCC-MAIN-2023-14 5,117,143,765 4,675,097\nCC-MAIN-2023-23 5,461,486,807 4,869,627\nCC-MAIN-2023-50 5,881,860,014 4,901,306\nCC-MAIN-2024-10 5,164,171,562 4,335,071\nCC-MAIN-2024-18 4,745,457,054 3,949,186\nTotal 221,715,271,483 259,728,610\n45\n--- Page 46 ---\nH PeS2o Source Statistics\nAdditional statistics on the composition of the peS2o subset of the Common Pile can be found in\nTable 3 and Table 4.\nTable 3: Distribution of licenses in the peS2o subset.\nLicense Train Split Validation Split\nCC BY 6,088,325 37,754\nCC BY-SA 120,150 1,231\nCC0 36,373 121\nPublic domain 10,060 6\nTable 4: Distribution of papers across 23 fields of study, as identified by the Semantic Scholar\nAPI [87]. A paper may belong to one or more fields of study.\nField of Study Train Split Validation Split\nMedicine 2,435,244 23,734\nBiology 1,518,478 8,879\nEnvironmental\nScience993,499 7,601\nEngineering 656,021 5,005\nComputer Science 462,320 3,003\nMaterials Science 416,045 3,166\nPhysics 413,461 1,285\nChemistry 406,429 2,781\nPsychology 364,441 2,126\nEducation 220,014 1,532\nBusiness 193,536 946\nEconomics 185,716 921\nAgricultural and Food\nSciences333,776 2,013\nSociology 137,257 1,535\nMathematics 135,676 199\nPolitical Science 106,748 378\nGeology 67,258 217\nGeography 44,269 257\nLinguistics 41,737 228\nHistory 36,848 192\nLaw 30,888 251\nPhilosophy 27,518 148\nArt 26,658 75\n46\n--- Page 47 ---\nI Growth rates of openly licensed data\nOver time, the volume of openly licensed data continues to grow as more creators release content\nunder open licenses. In Figure 6, we quantify this growth between 2010 and 2024 by analyzing\nsubsets of the Common Pile for which reliable creation date metadata is available. We plot the\ncumulative proportion of data created up to various cutoff dates and find that approximately half\nof the Common Pile (around 3.8TB) was created since 2020. This trend provides insight into the\ngrowing availability of openly licensed data and suggests a promising trajectory for future LLMs\ntrained entirely on openly licensed sources.\n2010 2012 2014 2016 2018 2020 2022 2024\nCutoff Date020406080100Percentage of Total SizeQuantity of Openly Licensed Data Over Time\nAll\nCode\nEducational ResourcesOther\nOnline Forums\nWebWikis\nGovernment\nAcademic Papers\nFigure 6: The amount of openly licensed text grows steadily over time. We visualize the cumulative\nproportion of data created up to various cutoff dates for sources in the Common Pile with reliable\ncreation date metadata. This includes all sources except for the Caselaw Access Project, Data\nProvenance Initiative, and the sources covering early 20th century Public Domain books.\nJ Details on filtering pipelines\nIn subsection 4.1, we detail the steps used to produce the Comma v0.1 training dataset from the\nraw text in the Common Pile. These include applying filters based on language, text quality, length,\nlikelihood, and toxicity; removing various forms of PII; and removal of source-specific boilerplate\ntext using regular expressions. The Common Pile contains a diverse range of sources and we therefore\ndesign separate filtering thresholds for each source. The exact source-specific thresholds used to\npost-process the Common Pile can be found in Table 5. Additionally, statistics on the pre- and\npost-filtered sizes of each source can be found in Table 6.\nTable 5: Pre-processing pipelines applied to each source in the Common Pile to construct the Comma\ndataset.\nSource Language Text Quality Doc Length Log-Likelihood Toxicity PII Regex Filter\nArXiv Abstracts – – – – – Y N\nArXiv Papers > 0.5 – – – – Y N\nBiodiversity\nHeritage Library> 0.5 – > 100 > -20 – N Y\nCaselaw Access\nProject– – > 100 – > 0.1 Y N\nCC Common\nCrawl> 0.5 > 0.0001 > 100 – > 0.1 Y N\nContinued on next page\n47\n--- Page 48 ---\nSource Language Text Quality Doc Length Log-Likelihood Toxicity PII Regex Filter\nData Provenance\nInitiative– – – – – N N\nDatabase of\nOpen Access\nBooks> 0.5 – > 200 – > 0.1 Y N\nFoodista > 0.5 – > 100 – – N N\nGitHub Archive > 0.5 – > 100 – > 0.1 Y N\nLibrary of\nCongress– – – > -20 > 0.1 N Y\nLibreTexts > 0.5 – > 700 – > 0.1 Y N\nNews > 0.5 – > 100 – – Y N\nOERCommons > 0.5 – > 300 – > 0.1 Y N\npeS2o – – – – – Y N\nPre-1929 Books – – – > -20 > 0.1 N Y\nPressBooks > 0.5 – > 600 – > 0.1 Y N\nProject\nGutenberg> 0.5 – – > -20 – N N\nPublic Domain\nReview– – > 100 – – Y N\nPubMed > 0.5 – > 100 – – Y N\nPEPs – – – – – Y N\nRegulations.gov – – > 100 – – Y Y\nStackExchange > 0.5 – – – – Y N\nUbuntu IRC > 0.5 – > 100 – > 0.1 Y N\nUK Hansard > 0.5 – – – – Y N\nUSGPO – – – – – N Y\nUSPTO – – > 100 > -20 – Y N\nWikimedia > 0.5 – > 100 – – Y N\nWikiteam > 0.5 – > 700 – > 0.1 Y N\nCC YouTube > 0.5 – > 100 – > 0.1 Y N\nTable 6: Raw and filtered sizes of the Common Pile’s constituent datasets.\nDocument Count Size (GB)\nSource Raw Filtered Raw Filtered\nArXiv Abstracts 2,538,935 2,504,679 2.4 2.4\nArXiv Papers 321,336 304,048 21 19\nBiodiversity Heritage\nLibrary42,418,498 15,111,313 96 35\nCaselaw Access Project 6,919,240 6,735,525 78 77\nContinued on next page\n48\n--- Page 49 ---\nDocument Count Size (GB)\nSource Raw Filtered Raw Filtered\nCC Common Crawl 51,054,412 6,852,137 260 58\nData Provenance Initiative 9,688,211 3,508,518 7 3\nDirectory of Open Access\nBooks474,445 403,992 12.5 12\nFoodista 72,090 65,640 0.09 0.08\nGitHub Archive 30,318,774 23,358,580 54.7 40.4\nLibrary of Congress 135,500 129,052 47.8 35.6\nLibreTexts 62,269 40,049 5.3 3.6\nNews 172,308 126,673 0.4 0.3\nOERCommons 9,339 5,249 0.1 0.05\npeS2o 6,294,020 6,117,280 188.2 182.6\nPre-1929 Books 137,127 124,898 73.8 46.3\nPressBooks 106,881 54,455 1.5 0.6\nProject Gutenberg 71,810 55,454 26.2 20.1\nPublic Domain Review 1,412 1,406 0.007 0.007\nPubMed 4,068,867 3,829,689 158.9 147.1\nPEPs 656 655 0.01 0.01\nRegulations.gov 225,196 208,301 6.1 5.1\nStackExchange 33,415,400 30,987,814 103.7 89.7\nStack V2 218,364,133 69,588,607 4774.7 259.9\nUbuntu IRC 329,115 234,982 6.3 5.3\nUK Hansard 51,552 47,909 10 9.6\nUSGPO 2,732,677 2,148,548 74.5 36.1\nUSPTO 20,294,152 17,030,231 1003.4 661.1\nWikimedia 63,969,938 16,311,574 90.5 57.4\nWikiteam 219,139,368 26,931,807 437.5 13.7\nCC YouTube 1,129,692 998,104 21.5 18.6\nTotal 692,854,953 233,817,169 7557.9 1838.3\nK Details on Comma’s pre-training data mixture\nWe estimated the quality of each source in the Common Pile by training a 1.7B-parameter model for\n28B tokens on each source individually and evaluating the resulting models on the set of “early signal”\ntasks from [ 132]. In doing so, we found that the amount of text in each source was poorly correlated\nwith text quality, motivating the use of heuristic mixing weights to up-/down-weight different sources\nin our pre-training mix. In Table 7 we list the pre-training mixture weights for each of the sources in\nthe Common Pile.\n49\n--- Page 50 ---\nTable 7: Overview of the data mixing used to up/down-weight individual sources in the Common\nPile to construct the Comma v0.1-1T pre-training dataset. Comma v0.1-2T simply repeats this full\nmixture twice.\nSource Size (GB) Repeats Effective Size\n(GB)Tokens\n(Billions)Percentage\nArXiv\nAbstracts2.4 6 14.4 3.6 0.360%\nArXiv Papers 19.5 6 117 29.3 2.932%\nBiodiversity\nHeritage\nLibrary35.5 0.25 8.9 2.2 0.220%\nCaselaw\nAccess Project77.5 1 77.5 19.4 1.941%\nCC Common\nCrawl58.1 6 348.6 87.1 8.716%\nData\nProvenance\nInitiative3.4 6 20.4 5.1 0.510%\nDatabase of\nOpen Access\nBooks12 6 72 18 1.801%\nFoodista 0.08 6 0.48 0.12 0.012%\nGitHub\nArchive40.4 6 242.4 60.6 6.064%\nLibrary of\nCongress35.6 0.25 8.9 2.2 0.220%\nLibreTexts 3.6 6 21.6 5.4 0.540%\nNews 0.25 6 1.5 0.38 0.038%\nOERCommons 0.05 6 0.3 0.08 0.008%\npeS2o 182.6 6 1,095.6 273.9 27.409%\nPre-1929\nBooks46.3 1 46.3 11.6 1.161%\nPressBooks 0.6 6 3.6 0.9 0.090%\nProject\nGutenberg20.1 1 20.1 5 0.500%\nPublic Domain\nReview0.007 6 0.04 0.01 0.001%\nPubMed 147.1 1 147.1 36.8 3.683%\nPEPs 0.01 6 0.06 0.02 0.002%\nRegulations.gov 5.1 6 30.6 7.6 0.761%\nStackExchange 89.7 6 538.2 134.6 13.469%\nStack V2 259.9 2 519.8 130 13.009%\nContinued on next page\n50\n--- Page 51 ---\nSource Size (GB) Repeats Effective Size\n(GB)Tokens\n(Billions)Percentage\nUbuntu IRC 5.3 6 31.8 7.9 0.791%\nUK Hansard 9.6 6 57.6 14.4 1.441%\nUSGPO 36.1 0.25 9 2.3 0.230%\nUSPTO 661.1 0.25 165.3 41.3 4.133%\nWikimedia 57.4 6 344.4 86.1 8.616%\nWikiteam 13.7 4 54.8 13.7 1.371%\nCC YouTube 18.6 1 18.6 4.7 0.470%\nTotal 1838.3 – 3997.4 999.3 100%\nL Details on Comma’s cool-down data mixture\nFollowing Hu et al. [73], we end training with a “cool-down” where we train on 37.7B tokens of\nhigh-quality data while linearly decaying the learning rate to 0. We provide the source mixture\nweights for this cool-down phase in Table 8.\nTable 8: Overview of the data mixing used to up/down-weight individual sources in the Common\nPile to construct the training distribution for Comma v0.1-1T’s cool-down phase. Comma v0.1-2T\nsimply repeats this full mixture twice.\nSource Size (GB) Repeats Effective Size\n(GB)Tokens\n(Billions)Percentage\nArXiv Papers 19.5 0.5 9.8 2.4 6.50%\nCC Common\nCrawl58.1 0.3 17.4 4.4 11.63%\nData\nProvenance\nInitiative3.4 2 6.8 1.7 4.55%\nDatabase of\nOpen Access\nBooks12 2 24 6 16.04%\nFoodista 0.08 2 0.16 0.04 0.11%\nLibreTexts 3.6 2 7.2 1.8 0.48%\nNews 0.25 2 0.5 0.13 0.33%\nOERCommons 0.05 2 0.1 0.03 0.07%\npeS2o 182.6 0.1 18.3 4.6 12.18%\nPressBooks 0.6 2 1.2 0.3 0.77%\nPublic Domain\nReview0.007 2 0.014 0.004 0.01%\nContinued on next page\n51\n--- Page 52 ---\nSource Size (GB) Repeats Effective Size\n(GB)Tokens\n(Billions)Percentage\nPEPs 0.01 2 0.02 0.005 0.02%\nStackExchange 89.7 0.25 22.4 5.6 14.96%\nStack V2 259.9 0.1 26.0 6.5 17.04%\nWikimedia 57.4 0.4 23 5.7 15.32%\nTotal 679.4 – 149.9 37.5 100%\nM Details on small-scale data ablations\nIn subsection 4.3 we report results from a series of small-scale data ablations where we identically\ntrained 1.7B parameter models on various openly licensed and unlicensed datasets and evaluate their\nperformance on the “early signal” tasks from Penedo et al. [132] to compare their data quality against\nthe Common Pile. In Figure 7 we show how the performance of these models evolve over the course\nof their training run, highlighting that differences in data quality become apparent very early in\ntraining. Additionally, we provide exact numerical results for each model in Table 9, showing that\nthe Common Pile has higher data quality than any previously released openly licensed datasets and\nthe Pile, and nearly matches the data quality of the OSCAR dataset. To validate that this is not purely\ndue to the presence of high-quality supervised fine-tuning data from the Data Provenance Initiative\n(DPI) data source, we also perform an ablation on the Common Pile excluding the DPI data and find\nthat the final performance of this model is largely unchanged.\nFigure 7: A model trained on the Comma dataset consistently outperforms models trained on\nother corpora of openly licensed text and outperforms the Pile on all but two tasks. We train\nidentical 1.7B parameter models on 28B tokens from each dataset following Penedo et al. [132].\nN Additional Comma results\nWe provide exact numerical results for Comma v0.1-1T and -2T alongside baseline models results\nacross a variety of knowledge, reasoning, and coding tasks in Table 10 and Table 11 respectively. We\nfind that particularly on knowledge-based benchmarks (such as MMLU) and coding benchmarks,\n52\n--- Page 53 ---\nTable 9: Comma’s training dataset has higher quality than previous openly-licensed datasets\nand unlicensed datasets like the Pile. In the small-scale (1.7B parameter) data ablation setting,\nwe find that Comma’s training dataset yields better models than previous openly licensed datasets\nand the Pile, and nearly matches the performance of models trained on OSCAR. Additionally, we\nfind that removing the high-quality supervised data from the Data Provenance Initiative has marginal\naffect on the Comma dataset’s overall quality.\nDataset ARC MMLU HS OBQA CSQA PIQA SIQA Avg.\nKL3M 31.8 26.3 29.9 28.4 26.8 58.2 38.0 36.2\nOLC 33.1 27.5 33.8 27.4 27.7 59.4 38.5 37.3\nCommon Corpus 34.2 27.0 33.6 30.2 26.4 61.0 37.7 37.6\nComma (no DPI) 37.7 28.7 37.6 31.0 30.8 63.8 39.8 40.0\nComma 38.0 29.5 39.9 32.4 29.6 65.8 39.4 40.8\nThe Pile 37.0 27.8 35.8 28.6 31.5 66.8 38.2 39.6\nOSCAR 35.4 27.6 40.8 30.4 32.1 69.7 39.7 40.9\nFineWeb 38.0 29.1 48.2 34.2 33.6 73.4 40.3 43.7\nComma v0.1-1T and -2T outperform baseline models trained on an equivalent amount (1T or 2T\ntokens, respectively) of unlicensed text.\nTable 10: Comparison between Comma v0.1-1T and baseline models trained with similar resources (7\nbillion parameters, 1 trillion tokens) across a variety of knowledge, reasoning, and coding benchmarks.\nModel ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg.\nRPJ-INCITE 42.8 68.4 27.8 68.6 70.3 49.4 57.7 76.0 46.9 11.1 15.9 48.6\nLLaMA 44.5 67.9 34.8 75.4 76.2 51.2 61.8 77.2 50.3 19.9 27.9 53.4\nStableLM 50.8 65.4 45.2 71.7 75.6 48.2 57.2 77.0 48.2 23.1 32.0 54.0\nMPT 46.5 70.5 30.2 74.2 77.6 48.6 63.3 77.3 49.1 27.3 33.2 54.3\nOpenLLaMA 44.5 67.2 40.3 72.6 72.6 50.8 62.8 78.0 49.7 27.6 33.9 54.5\nComma v0.1-1T 52.8 68.4 42.4 75.7 62.6 47.0 59.4 70.8 50.8 36.5 35.5 54.7\nQwen3 57.2 74.5 77.0 86.1 77.0 50.8 66.4 78.2 55.0 94.5 67.5 71.3\nTable 11: Performance of Comma v0.1-2T and a variety of budget-matched baseline models.\nModel ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg.\nOLMo Twin 45.2 67.5 28.2 71.7 73.4 48.0 61.8 77.9 48.5 18.2 27.5 51.6\nLlama 2 48.5 69.5 45.8 80.2 76.2 48.4 62.8 76.7 50.8 26.1 28.5 55.8\nComma v0.1 2T 45.8 71.8 49.8 78.6 64.4 46.2 64.0 72.5 52.3 44.2 41.5 57.4\nDeepSeekLLM 49.5 67.7 48.5 71.7 74.1 52.0 66.6 77.8 51.6 43.1 43.8 58.8\nO Additional training runs\nTo explore the sensitivity of our Comma v0.1 results to hyperparameter choices, we perform a series\nof additional 7B parameter/1T token training runs on AMD MI300A GPUs with slight alterations\nto the training recipe. Due to both a desire to reach the same 1T token target rapidly, and the lower\nsingle-GPU throughput on the system available for these ablations, for all additional runs the the\ntraining batch size is 8.3M ( 223) versus the 2.1M ( 221) tokens per step of Comma v0.1. Unless\notherwise specified, we did not use the two phase training process described in subsection 4.4 (i.e. no\nseparate high-quality cooldown phase is run and we do not perform checkpoint averaging at the end\nof training and before evaluation).\nO.1 Ablations at 1T Tokens\nWe first performed a set of training runs for 125,000 steps, resulting in 1.048T total tokens (referred\nto as “1T” for brevity).\n53\n--- Page 54 ---\n“8M Batch” We perform a run with nearly the same training hyperparameters as Comma v0.1-1T,\nexcept with a larger 8M token batch size. We also use a single phase training setup; the base data\nmixture (Table 7) is run for the entire duration to 1T tokens. The learning rate schedule is 2,000 steps\nof warm-up from 0 to a peak of 1e−3with 123,000 steps of decay to a minimum of 1.8e−9.\n“Curriculum” In this experimental run, a different data mixture is used in each of three training\nstages of equal duration (we also use the modified hyperparameters from “8M Batch” ablation\nabove). The first stage of the curriculum comprises data from only the Common Pile’s largest sources\n(mostly USPTO, Table 13). The second stage uses the same data mixture as Comma v0.1’s main\npre-training phase (“phase I”), but run for only 1/3 of the duration. Finally, the third and last stage of\nthe curriculum up-weights Common Pile’s highest quality, benchmark-relevant sources (Table 14).\nWe provide exact numerical results for Comma v0.1 and alternate Comma runs performed with\ndifferent hyperparameters and data mixture curricula across a variety of knowledge, reasoning, and\ncoding benchmarks in Table 12. We find that the 8M Batch and Curriculum ablations are roughly\ncomparable on average to the main Comma v0.1-1T run, with the notable exception that both ablations\nslightly outperform Comma v0.1-1T on the coding benchmarks. We conclude that the benchmark\nresults reported for Comma v0.1-1T in subsection 4.4 seem relatively robust to minor changes in\ntraining hyperparameters, dataset mixture curriculum (assuming similar amounts of most data splits\nappear at some time during training), and the software environment and GPU hardware used to train\nthe model.\nTable 12: Comparison between our main Comma v0.1-1T training run and alternate runs performed\nwith different hyperparameters and data mixture curricula across a variety of knowledge, reasoning,\nand coding benchmarks. For “Main”, we report the performance of Comma v0.1-1T without averaging\nthe cooldown checkpoints so that it is a fair comparison.\nModel ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg.\nCurriculum 45.2 69.1 41.4 74.7 60.8 46.8 59.1 70.5 48.6 38.1 34.6 53.5\n8M Batch 47.2 69.6 42.9 69.9 62.9 47.0 56.9 70.4 50.5 36.8 37.2 53.8\nMain 50.8 68.4 40.2 72.9 62.3 46.2 59.5 71.0 51.2 32.1 34.6 53.6\nTable 13: Overview of the data mixing used to up/down-weight individual sources for the Stage 1 of\nthe Curriculum ablation run. In this table we omit the size columns for brevity.\nSource Repeats Tokens\n(Billions)Percentage\nUSPTO 1.4125 233.5 66.81%\nPre-1929\nBooks5.65 65.4 18.71%\nStack V2\n(HTML)11.3 12.8 3.65%\nUSGPO 1.41 12.8 3.65%\nLibrary of\nCongress1.41 12.6 3.59%\nBiodiversity\nHeritage\nLibrary1.41 12.52 3.58%\nTotal – 349.4 100%\n54\n--- Page 55 ---\nTable 14: Overview of the data mixing used to up/down-weight individual sources for the Stage 3 of\nthe Curriculum ablation run. In this table we omit the size columns for brevity.\nSource Repeats Tokens\n(Billions)Percentage\nStack V2 1 63.8 18.519%\nDatabase of\nOpen Access\nBooks6 18 5.230%\nWikimedia 6 86.1 24.981%\nStackExchange 2.5 56.1 16.259%\npeS2o 1 45.6 13.241%\nCC Common\nCrawl3 43.6 12.638%\nArXiv Papers 5 24.4 7.063%\nData\nProvenance\nInitiative6 5.1 1.485%\nPressBooks 6 0.87 0.251%\nLibreTexts 6 0.54 0.157%\nNews 6 0.37 0.108%\nFoodista 6 0.12 0.036%\nOERCommons 6 0.08 0.023%\nPEPs 6 0.02 0.005%\nPublic Domain\nReview6 0.01 0.003%\nTotal – 344.7 100%\n55",
  "text_length": 177574
}