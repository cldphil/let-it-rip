{
  "id": "http://arxiv.org/abs/2506.04166v1",
  "title": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion",
  "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.",
  "authors": [
    "Caleb Chin",
    "Aashish Khubchandani",
    "Harshvardhan Maskara",
    "Kyuseong Choi",
    "Jacob Feitelberg",
    "Albert Gong",
    "Manit Paul",
    "Tathagata Sadhukhan",
    "Anish Agarwal",
    "Raaz Dwivedi"
  ],
  "published": "2025-06-04T17:04:34Z",
  "updated": "2025-06-04T17:04:34Z",
  "categories": [
    "cs.LG",
    "stat.CO",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04166v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04166v1  [cs.LG]  4 Jun 2025N2: A Unified Python Package and Test Bench for\nNearest Neighbor-Based Matrix Completion\nCaleb Chin\nCornell UniversityAashish Khubchandani\nCornell UniversityHarshvardhan Maskara\nCornell University\nKyuseong Choi\nCornell UniversityJacob Feitelberg\nColumbia UniversityAlbert Gong\nCornell UniversityManit Paul\nUniversity of Pennsylvania\nTathagata Sadhukhan\nCornell UniversityAnish Agarwal\nColumbia UniversityRaaz Dwivedi\nCornell University\nAbstract\nNearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical guar-\nantees, including entry-wise error bounds, confidence intervals, and minimax\noptimality. Despite their simplicity, recent work has shown that NN approaches are\nrobust to a range of missingness patterns and effective across diverse applications.\nThis paper introduces N2, a unified Python package and testbed that consolidates a\nbroad class of NN-based methods through a modular, extensible interface. Built\nfor both researchers and practitioners, N2supports rapid experimentation and\nbenchmarking. Using this framework, we introduce a new NN variant that achieves\nstate-of-the-art results in several settings. We also release a benchmark suite of\nreal-world datasets—from healthcare and recommender systems to causal inference\nand LLM evaluation—designed to stress-test matrix completion methods beyond\nsynthetic scenarios. Our experiments demonstrate that while classical methods\nexcel on idealized data, NN-based techniques consistently outperform them in\nreal-world settings.\n1 Introduction\nNearest neighbor methods are a class of non-parametric algorithms widely used for regression,\nclassification and pattern recognition. Due to their scalability and success under models with min-\nimal assumptions, nearest neighbor methods have recently been adopted for practical fields such\nas matrix completion and counterfactual inference in panel data settings. Matrix completion is a\nwell-established field that supplies practitioners with many tools to recover underlying matrices\nusing partial or even noisy observations [ HMLZ15 ,Cha15 ,KMO10 ], with recommendation sys-\ntems [ KBV09 ,Rec11 ] as an important use-case. Panel data counterfactual inference aims at learning\nthe treatment effect of policies across time [ Bai09 ,BN21 ,ABD+21]. One important example is\nindividualized healthcare predictions [ KSS+19]. Nearest neighbor methods were recently recognized\nas effective in providing granular inference guarantees for both matrix completion and counter-\nfactual inference when either the missingness or the policy treatment are not completely random\nand confounded [ MC19 ,DTT+22a,ADSS23 ]. They have also been recently leveraged to tackle\ndistributional matrix completion settings [FCAD24, CFC+24]\nDespite nearest neighbor methods popularity, there is no unified package that lets a user easily switch\nbetween different kinds of nearest neighbor algorithms for matrix completion and counterfactual\nPreprint. Under review.\n--- Page 2 ---\ninference. In this paper, we present a package ( N2) to unify several nearest neighbor methods under a\nsingle interface, so users can easily choose the method that suits their data the best, for both scalar and\ndistributional missing data problems. Next, we review two broad applications of nearest neighbors\nand matrix completion: recommendation systems and panel data settings.\nRecommendation systems In the problem of a recommendation system, multiple users submit\nratings / preferences for a subset of items, and the vendor recommends items based on the partial\nrating information [ RS05 ]. Prediction of unseen ratings is the key to a good recommendation, and\nthe predictions are based on the patterns of the partially observed ratings which can be formalized\ninto a matrix completion problem. A popular instance is the Netflix problem, an open competition for\nthe best prediction algorithm of user ratings of films. Users (rows of matrix) have the option to rate\nthe films (columns of matrix). However, users typically rate only a subset of films, and the ratings\nare commonly biased towards their preferences. Under the common belief that only a few factors\ndrive the preference and ratings of the users, many matrix completion algorithms have resorted to\nthe low rank assumption on the ratings matrix [ Rec11 ,CR12 ]. Variants of nearest neighbors (in\nshort, NN) recently were introduced to correct for the potential bias originating from the fact that\nobservation patterns are influenced by the matrix entries (e.g. ratings) themselves and by unmeasured\nconfounders [ADSS23, AADS24].\nPanel data Panel data constitutes measurements of multiple units that are tracked over time (hence\na type of longitudinal data) as they undergo different types of interventions at different times [ Bai09 ].\nPanel data is used to analyze the causal effects of any policies, treatments, or business decisions that\naffect the subjects in the data, making this data type essential in econometrics and health studies.\nMatrix completion algorithms enable estimation of these causal effects at the unit ×time level, the\nmost granular scale where causal effects can be estimated [ ABD+21,ADSS23 ]. For example,\nconsider a mobile health app trying to learn the effectiveness of two exercise routines. Suppose the\napp alternates two routines across Ndifferent users repeatedly over Tweeks, where their health\nactivities (say physical step counts) throughout each week are recorded. Then, we can use matrix\ncompletion to impute the counterfactual step counts under treatment and control, allowing us to\nestimate causal effects at the most granular scale.\nOur contributions We make the following contributions: First, we present a unified framework for\nnearest neighbor algorithms that facilitates extending to new variants, especially for matrix completion\nproblems. We leverage this framework to introduce then a new nearest neighbors algorithm, auto\nnearest neighbors (in short AutoNN ), which aims both to improve on the existing methods and\ndemonstrate the ease of developing new variants with our library. Next, we introduce a unified,\neasy-to-implement nearest neighbor library that contains a breadth of nearest neighbor algorithms\nfor matrix comple2tion problems. Next, we present a test bench called N2-Bench that comprises of\nseveral real-world data sets across applications. Finally, we illustrate our library’s wide applicability\non the test benchmark.\n1.1 Related work\nWe contextualize our contributions in the context of both nearest neighbors as a general algorithm\nand matrix completion specifically.\nNearest neighbors As introduced above, nearest neighbor methods are widely used for non-\nparametric regression, classification, and pattern recognition [ CH67 ,CBS20 ]. Recently, nearest\nneighbor methods were introduced as an effective algorithm for matrix completion problems [ LSSY19 ,\nADSS23 ,DTT+22a], especially when the missingness depended on observations and unobserved\nconfounders. The fact that nearest neighbor methods target a single entry at a time via matching\nmakes them effective against various types of missing patterns. The class of algorithms has grown\nto account for a wide range of applications involving scalar or distributional settings; for instance,\nnearest neighbors are used for evaluating the personalized recommendation effect of a healthcare\napp on the average and the distribution of physical step counts [ DTT+22a,CFC+24,FCAD24 ,\nSPD24 ] and are also used for policy evaluations [ ADSS23 ]. Our library, N2, is designed to easily\nimplement the vanilla scalar versions of nearest neighbors [ LSSY19 ,DTT+22a] as well as its\nunweighted [ DTT+22b,SPD24 ] and weighted [ SPD25 ] variants. Finally, N2is also capable of\ndistributional matrix completion [CFC+24, FCAD24].\n2\n--- Page 3 ---\nOther matrix completion methods Universal singular value thresholding (USVT), proposed in\n[Cha15 ,BC22 ], is a classical spectral-based method for performing matrix completion; its core\nfunctionality is based on a singular value decomposition of the matrix and thresholding the singular\nvalues. SoftImpute, introduced by [ HMLZ15 ], is another widely used optimization-based algorithm\nfor matrix completion. The algorithm computes ridge-regression updates of the low-rank factors\niteratively and finally soft-thresholds the singular values to impose a nuclear norm penalty. No-\ntably USVT and SoftImpute have provable guarantees when missingness is completely at random,\nbut empirically fail when the missing pattern depends on the observed entries or the unobserved\nconfounders [ADSS23]. Our real-world analysis in Sec. 4 once again demonstrates this point.\nExisting software for matrix completion and nearest neighbors Scikit-Learn [ PVG+11], a\npopular Python package for machine learning tools, implements a simple k-nearest neighbor algorithm\nfor imputing missing values in a feature matrix. However, their implementation is designed for the\nfeature matrix setting. So, neighbors are only defined across samples (row-wise). Additionally, they\ndo not provide any implementation for more advanced nearest neighbor algorithms, nor does their\npackage allow for easy extendability like our proposed package.\nOrganization Sec. 2 contains an overview of the existing nearest neighbor algorithms implemented\nin our library, N2. Sec. 2.3 introduces a new nearest neighbor algorithm, AutoNN , that is also\nimplemented in N2. Sec. 3 touches upon the high-level (class) structure of N2, as well as the\ninterface for practitioners. Sec. 4 tests different variants of nearest neighbor methods and classical\nmethods on our new test bench of diverse datasets called N2-Bench . Finally, in Sec. 5, we provide\nconcluding remarks and outline future directions of research.\n2 Nearest Neighbors for Matrix Completion\nWe now introduce the mathematical model for matrix completion:\nfori∈[N], t∈[T] :Zi,t:=\u001aX1(i, t), ..., X n(i, t)∼µi,t ifAi,t= 1,\nunknown ifAi,t= 0.(1)\nIn other words, for matrix entries where Ai,t= 1, we observe nmeasurements Zi,tthat takes\nvalue Xi,trealized from distribution µi,t. When n= 1, i.e., Zi,t=X1(i, t), we refer to (1) as the\nscalar matrix completion model; scalar matrix completion is the most common problem posed in\nthe literature [ CR12 ,Rec11 ,KBV09 ,HMLZ15 ,Cha15 ,DTT+22a,DTT+22b,ADSS23 ], where the\ngoal is to learn the mean of the underlying distributions {θi,t=R\nxdµi,t(x)}i∈[N],t∈[T].When there\nare more than one observed measurements per entry, i.e., Zi,t= [X1(i, t), ..., X n(i, t)]forn≥2,\nwe refer to (1) as the distributional matrix completion problem, the goal being the recovery of the\ndistributions as a whole. We refer the readers to App. A for a detailed discussion on the structural\nassumptions imposed on the model (1).\n2.1 Unified framework\nWe introduce two general modules (namely DISTANCE andAVERAGE ) from which the variants of\nnearest neighbors are constructed. We introduce several shorthands used in the modules. Denote the\ncollection of measurements, missingness, and weights:\nZ:=\b\nZj,s\t\nj∈[N],s∈[T],A:={Aj,s}j∈[N],s∈[T],andW:={wj,s}j∈[N],s∈[T].\nLetφ(x, x′)be a metric between x, x′∈ X for some space X. Further define bφ(Zi,t, Zj,s)as a\ndata-dependent distance between any two observed entries (i, t)and(j, s)of the matrix (1). The two\nmodules can now be defined:\n(i) D ISTANCE (bφ,Z,A): Additional input is the data-dependent distance between entries of\nmatrix bφand output is the collection of row-wise and column-wise distance of matrix:\nρrow\ni,j:=P\ns̸=tAi,sAj,sbφ(Zi,s, Zj,s)P\ns̸=tAi,sAj,sand ρcol\nt,s:=P\nj̸=iAj,tAj,sbφ(Zj,t, Zj,s)P\nj̸=iAj,tAj,s,\n3\n--- Page 4 ---\n(ii) A VERAGE (φ,W,Z,A): Additional input are the weights W, metric φand output is the\noptimizer\nbθ= argmin\nx∈XX\nj∈[N],s∈[T]wj,sAj,sφ(x, Zj,s).\nTheDISTANCE module calculates the row-wise and column-wise distance of the matrix, by taking the\naverage of the observed entry-wise distance bφ(·,·). The AVERAGE module calculates the weighted\naverage of observed measurements, where the notion of average depends on the metric φand the\nspaceXon which the metric φis defined. Notably, the weights Win the AVERAGE module encodes\nthe entry information of the estimand.\nRemark 1 The vanilla row-wise nearest neighbors [ LSSY19 ] that targets the mean θi,t= R\nxdµi,t(x)of entry (i, t)is recovered by first applying DISTANCE withbφ(Zj,s, Zj′,s′) =\n(Zj,s−Zj′,s′)2, applying AVERAGE with the non-smooth weight wj,s=1(ρrow\ni,j≤η1)·1(ρcol\ns,t≤0),\nand using the metric φ(x, y) = ( x−y)2. Note that the non-smooth weight satisfies\nwj,t=1(ρrow\ni,j≤η1), whereas wj,s= 0 fors̸=t; by defining the nearest neighbor set\nNt,η1:={j∈[N] :ρrow\nj,t≤η1}, the AVERAGE module output can be rewritten as\nargminx∈RP\nj∈Nt,η1Aj,t(x−Zj,t)2=|Nt,η1|−1P\nj∈Nt,η1Zj,t.\n2.2 Existing methods\nWe present existing variants of nearest neighbors using the two modules introduced Sec. 2.1; all the\nmethods presented here are recovered by sequentially applying DISTANCE andAVERAGE with the\nappropriate specification of bφ,φandW.\nAll methods except AWNN and our newly proposed AutoNN , have binary weights i.e., wj,s∈ {0,1}.\nAutoNN , detailed in Sec. 2.3, uses weights to carefully pool together the benefits of TSNN and\nDRNN .AWNN [SPD25 ] improves upon RowNN by adaptively choosing the weights which optimally\nbalances the bias-variance tradeoff of RowNN as follows\u0000\nw⋆\n1(i, t), ..., w⋆\nN(i, t)\u0001\n:= argmin\n(v1,...,v N)∈∆N2 log(2 N)bσ2X\nk∈[N]v2\nk+X\nk∈[N]vkAk,tρrow\ni,k. (2)\nwherebσ2is the estimated error and ∆Nis a simplex in RN; see [ SPD25 ] for details of (2). Tab. 1\ncontains a concise summary of the existing nearest neighbor variants; see App. B for a detailed\nexposition for each methods.\nTable 1: Variants of nearest neighbors for matrix completion.\nType Method bφ(x, y) φ(x, y) wj,s\nn= 1 RowNN [LSSY19] (Alg. 1) (x−y)2(x−y)21(ρrow\ni,j≤η1, ρcol\ns,t≤0)\nColNN [LSSY19] (Alg. 1) (x−y)2(x−y)21(ρrow\ni,j≤0, ρcol\ns,t≤η2)\nTSNN [SPD24] (Alg. 2) (x−y)2(x−y)21(ρrow\ni,j≤η1, ρcol\ns,t≤η2)\nAWNN [SPD25] (Alg. 5) (x−y)2(x−y)2w⋆\nj(i, t)·1(ρcol\ns,t≤0)\nDRNN [DTT+22b] (Alg. 3) RowNN +ColNN −TSNN\nAutoNN (Sec. 2.3) α·DRNN + (1−α)·TSNN\nn >1KernelNN [CFC+24] (Alg. 4) \\MMD2\nk(x, y)MMD2\nk(x, y)1(ρrow\ni,j≤η1, ρcol\ns,t≤0)\nW2NN [FCAD24] (Alg. 4) bW2\n2(x, y) W2\n2(x, y) 1(ρrow\ni,j≤η1, ρcol\ns,t≤0)\nUnder the distributional matrix completion setting ( n >1in (1)), the methods KernelNN andW2NN\nin Tab. 1 take µ, ν∈ X as square integrable probability measures, and φ(µ, ν)as either the squared\nmaximum mean discrepency (i.e. MMD2\nk(µ, ν), see [ MFS+17]) or squared Wasserstein metric (i.e.,\nW2(µ, ν), see [ Big20 ]). Further, the entry-wise distance bφ(x, y)in this case is either the unbiased\nU-statistics estimator \\MMD2\nk(Zi,t, Zj,s)forMMD2\nk(µi,t, µj,s)(see [ MFS+17]) or the quantile based\nestimator bW2(Zi,t, Zj,s)forW2(µi,t, µj,s)(see [Big20]).\n4\n--- Page 5 ---\n2.3 New variant: Auto nearest neighbors\nTSNN is a generalization of RowNN andColNN by setting one of the tuning parameters to zero (see\nTab. 1), whereas the idea underlying DRNN is fundamentally different from that of TSNN ;DRNN\ndebiases a naive combination of RowNN andColNN whereas TSNN simply boosts the number of\nmeasurements averaged upon, thereby gaining from lower variance. So we simply interpolate the two\nmethods for some hyper-parameter α∈[0,1]; see Tab. 1. Notably the hyper-parameter ηfor both\nDRNN andTSNN are identical when interpolated.\nSuppose µi,t=θi,t+εi,tin (1) where εi,tare centered i.i.d. sub-Gaussian distributions across i\nandt. When σis large in magnitude, TSNN denoises the estimate by averaging over more samples,\nhence providing a superior performance compared to DRNN in a noisy scenario. When σis small\nso that bias of nearest neighbor is more prominent, DRNN effectively debiases the estimate so as to\nprovide a superior performance compared to TSNN . The linear interpolator AutoNN automatically\nadjusts to the underlying noise level and debiases or denoises accordingly; such property is critical\nwhen applying nearest neighbors to real world data set where the noise level is unknown. We refer to\nFig. 1 for visual evidence.\n24252627\n# Columns (T)2−52−42−3Absolute error\nDRNN:T−0.83\nTSNN:T−0.43\nAutoNN:T−0.75\n24252627\n# Columns (T)2−32−22−1Absolute error\nDRNN:T−0.54\nTSNN:T−0.44\nAutoNN:T−0.54\n(a) High SNR (b) Low SNR\nFigure 1: Error scaling for certain NN variants in synthetic experiments. See App. D.1 for\ndetails on the data-generating process and how the signal-to-noise ratio (SNR) is defined. Each point\ncorresponds to the mean absolute error ± 1 standard error across 30 trials.\n3N2Package and Interface\nWe now present our unified Python package, N2, for nearest neighbor algorithms for matrix com-\npletion. In particular, we provide a class structure which abstracts the estimation procedure utilized\nin each different nearest neighbor method as well as the the DISTANCE andAVERAGE modules\ndescribed above in Sec. 2. On top of that, our library facilitates easy extensions to other nearest\nneighbors algorithms and other data types on top of scalars and distributions. For example, as long as\na distance and average notion are well defined, our library can be easily applied to a matrix of images\nor text strings.\nClass structure. The core functionality of N2is based on two abstract classes:\nEstimationMethod andDataType .\nEstimationMethod classes contain the logic to impute a missing entry such as how to use calculated\ndistances. In the context of the modules in Sec. 2, this class determines the weighting function or, in\nthe context of DRNN andAutoNN , how to compose estimates. We separate this from the DataType\nabstraction because several estimation methods can be used for multiple data types. For example,\nRowRowEstimator implements the RowNN procedure for any data type given to it, such as scalars\nor distributions.\nDataType classes implement the DISTANCE andAVERAGE modules for any kind of data type (e.g.\nscalars which use squared distance and simple averaging). This abstract class allows for our package\n5\n--- Page 6 ---\nto extend to any data types beyond the ones we tested. For instance, a practitioner can easily add\naDataType for text strings which uses vector embeddings to find distances and averages between\nbetween strings without needing to rewrite any of the estimation procedure.\nInterface. To use our library, a user simply has to instantiate a composite class\nNearestNeighborImputer with their EstimationMethod andDataType of choice. We pro-\nvide constructor functions to automatically create popular NearestNeighborImputer classes such\nas a two-sided nearest neighbor estimator with the scalar data type. From a design pattern point of\nview, this is known as a Composite design pattern [ GHJV93 , pg. 163]. We use this design pattern\nso that anyone looking to customize the estimation procedure can do so for any kind of data type\nsimultaneously. Similarly, with the exception of doubly robust estimators, each estimation procedure\nworks out of the box with any data type that implements the DataType abstract class. The Doubly\nrobust estimation method does not work out of the box with distributions because a subtraction\noperation is not well defined in the distribution space.\nFinally, the user simply needs to input (i) a data matrix, (ii) a mask matrix which specifies which\nvalues are missing, and (iii) the row and column to impute. Thus, a user can test out different\nestimation procedures by changing just one line of code. Separately from the core functionality, we\nhave also implemented several cross-validation classes which take in a NearestNeighborImputer\nclass and find the best hyperparameters to use (e.g., distance thresholds and weights).\n4N2-Bench and Results\nIn this section, we evaluate several nearest neighbor algorithms provided by our library, N2, on\nreal-world data. As part of our package, we include data loaders which automatically download the\nnecessary datasets and format them for evaluation. These datasets and loaders comprise our proposed\nbenchmark for nearest neighbor matrix completion algorithms, N2-Bench . We also test several\nexisting popular matrix completion techniques ([ HMLZ15 ,Cha15 ]). For details on our experimental\nsetup, computing hardware, and boxplot generation, see App. D.\n4.1 Personalized healthcare: HeartSteps\nThe HeartSteps V1 study (HeartSteps study for short) is a clinical trial designed to measure the\nefficacy of the HeartSteps mobile application for encouraging non-sedentary activity [ KSS+19]. The\nHeartSteps V1 data and its subsequent extensions have been widely used for benchmarking a variety\nof tasks including counterfactual inference of treatment effect [ DTT+22a,CFC+24], reinforcement\nlearning for intervention selection [ LGKM20 ], and micro-randomized trial design [ QWC+22]. In the\nHeartSteps study, N= 37 participants were under a 6-week period micro-randomized trial, where\nthey were provided with a mobile application and an activity tracker. Participants independently\nreceived a notification with probability p= 0.6for5pre-determined decision points per day for 40\ndays ( T= 200 ). We denote observed entries Zi,tas the mean participant step count for one hour\nafter a notification was sent and unobserved entries as the unknown step count for decision points\nwhere no notification was sent. Our task is to estimate the counterfactual outcomes: the participant’s\nstep count should they have received a different treatment (notification or no notification) than they\ndid at specific time points during the study.\nResults & Discussion. We benchmark the performance of the matrix completion methods by\nmeasuring absolute error on held-out observed step counts across 10 participants in the last 50\ndecision points. We use the remaining data to find nearest neighbor hyperparameters using cross-\nvalidation. To benchmark the distributional nearest neighbors methods ( KernelNN andW2NN)\nagainst the scalar methods, we first set each entry to have the number of samples n= 60 , where\neach sample is the 1 minute step count before imputation. Then, we take the mean of the imputed\nempirical distribution as the estimate.\nIn Fig. 2(a), we compare the absolute error of the imputed values across the nearest neighbor and\nbaseline methods. The scalar nearest neighbor methods far out-perform USVT and are on par with\nSoftImpute. The two distributional nearest neighbor methods far outperform all methods operating in\nthe scalar setting; it suggests that matching by distributions collect more homogeneous neighbors,\n6\n--- Page 7 ---\nUSVT Soft\nImputeCol-\nNNRow-\nNNDR-\nNNTS-\nNNAuto-\nNNAW-\nNNKernel-\nNNW2-\nNN0123Absolute error\n0.0 2.5 5.0 7.5\nStep count0.00.20.40.60.8ProportionGround truth\nKernelNN\nW2NN(a) Absolute error of mean step count prediction (b) KernelNN vs.W2NN\nFigure 2: HeartSteps: estimating step count under scalar and distributional matrix completion\nsettings. Panel (a) shows the absolute error of predicted step count of the nearest neighbor methods\nagainst matrix completion baselines (SoftImpute, USVT). Panel (b) shows an example of an imputed\nentry in the distributional matrix completion setting.\nthereby decreases the bias of the method, compared to matching only the first moments as done in\nmost scalar matrix nearest neighbor methods.\nIn Fig. 2 panel (b), we show an example of an imputed entry in the distributional nearest neighbors\nsetting. In this case, the ground truth distribution is bimodal, as the participant was largely sedentary\n(0 steps) with small amounts of activity. While both KernelNN andW2NNcapture the sedentary\nbehavior of the participant, KernelNN is able to recover the bimodality of the original distribution\nwhereas W2NNcannot.\n4.2 Movie recommendations: MovieLens\nUSVT Soft\nImputeCol-\nNNRow-\nNNDR-\nNNTS-\nNNAuto-\nNNAW-\nNN01234Absolute error\nFigure 3: MovieLens: Estimation error for a\nrandom subsample of size 500. For experimental\nsettings and discussion see Sec. 4.2.The MovieLens 1M dataset [ HK15 ] contains\n1 million ratings (1–5 stars) from 6,040 users\non 3,952 movies. Collaborative filtering on\nMovieLens has long been a benchmark for\nmatrix-completion methods: neighborhood-\nbased algorithms [ SKKR01 ], latent-factor mod-\nels [KBV09 ], and, more recently, nearest neigh-\nbors interpreted as blind regression under a la-\ntent–variable model [ LSSY19 ]. These assist\npractitioners in data-driven recommendation sys-\ntems, since more accurate rating imputation\ndirectly drives better personalized suggestions\nand user engagement. This is a standard scalar\nmatrix completion problem with N= 6,040\nandT= 3,952. Each rating is an integer in\n{1, . . . , 5}. The dataset has a very high per-\ncentage of missing values: 95.53% missing.\nOur task is to estimate unobserved ratings us-\ning various matrix completion algorithms. We\nbenchmark the performance of nearest neigh-\nbors against matrix factorization by measuring absolute error on held -out ratings. See App. D.3 for\nadditional details on the dataset.\nResults & Discussion. We fit the nearest neighbor methods using a random sample of size 100\nfrom the first 80% of the dataset to choose nearest neighbor hyperparameters via cross-validation.We\nthen test the method on a random subsample of size 500 from the last 20% of the dataset. As observed\nin Fig. 3, all nearest neighbor methods have a lower average error than USVT and a much lower\nstandard deviation of errors, with ColNN ,RowNN ,DRNN , and AutoNN performing the best out of\nthe nearest neighbor methods. SoftImpute performs on par with the nearest neighbor methods. Note\n7\n--- Page 8 ---\n1970 1980 1990 2000\nYear5075100125150Cigarette ConsumptionTSNN\nObs.SCRowNNUSVT\nAutoNN\nSIColNN\nAWNNDRNNProp. 99 (1989)\nUSVT Soft\nImputeCol-\nNNRow-\nNNDR-\nNNTS-\nNNAuto-\nNNAW-\nNNSC020406080Absolute error(a) Synthetic controls for California (b) Absolute error on control states\nin post-intervention period\nFigure 4: Nearest neighbor methods generate high-fidelity synthetic controls in counterfactual\ninference for panel data. For exact settings and further discussion see Sec. 4.3.\nthat the nearest neighbor methods perform well even while only being trained on a tiny subset of the\ndata of size 100 out of the 1 million ratings available.\n4.3 Counterfactual inference for panel data: Proposition 99\nNext we consider a panel data setting, where our goal is to estimate the effect of the California\nTobacco Tax and Health Protection Act of 1988 (a.k.a. Proposition 99) on annual state-level cigarette\nconsumption1. By definition, the counterfactual cigarette consumption in California—had Proposition\n99 never been enacted—is not observed. [ ADH10 ] introduce the notion of a “synthetic control” to\nserve as a proxy for this unobserved value based on “neighboring” control states that never instituted\na tobacco tax. These states are not close in a geographical sense, but rather close due to similarities\nin other covariates2. We take a different approach and use only the observed cigarette consumption\nlevels from the control states, of which there are 38 in total. Thus, we frame our problem as a\nscalar matrix completion problem with N= 39 andT= 31 (see (1)). The last row in the matrix\ncorresponds to the state of California.\nResults & Discussion. For each method, we use a 64-16-20 train-validation-test split and use cross\nvalidation to fit any hyperparameters. Fig. 4 plots the various synthetic controls for California (left)\nand absolute error of each method on the 38 control states, for which we do observe the no-treatment\nvalues (right). From Fig. 4(a), we see that nearest neighbor methods, in particular TSNN andRowNN ,\nare roughly on par with the gold-standard synthetic control method of [ ADH10 ] (“SC”) for estimating\nCalifornia’s counterfactual cigarette consumption in the post-intervention period (after 1989). This is\ndespite the fact that the nearest neighbor methods rely on less information for the estimation task.\nFrom Fig. 4(b), we see that all nearest neighbor methods, with the exception of ColNN , achieve\nsimilar error levels as the synthetic control baseline. RowNN achieves even lower error levels. See\nsupplementary experiment details in App. D.4.\n4.4 Efficient LLM evaluation: PromptEval\nThe rapid advancement of LLMs have placed them at the center of many modern machine learning\nsystems, from chatbots to aids in medical education [ GHC+25]. In practice, system architects want\nto strike the right balance of real-world performance and cost, but navigating this Pareto frontier is a\ndaunting task. 2024 alone saw at least 10 new models from Anthropic, Google, Meta, and OpenAI,\nnot even counting the multitude of open-source fine-tuned models built on top of these. On specific\n1measured as per capita cigarette sales in packs\n2GDP per capita, beer consumption, percent aged 15–24, and cigarette retail prices\n8\n--- Page 9 ---\n0.3 0.5 0.7\nPropensity (p)0.50.60.70.80.9Kolmogorov-Smirnov distance\nKernelNN (col)\nKernelNN (row)\nW2NN (col)\nW2NN (row)\n0.00 0.25 0.50 0.75 1.00\nScore0.00.10.20.30.4ProportionGround\nTruth\nKernelNN\n(col)\nW2NN\n(row)(a) Mean KS distance between estimated (b) KernelNN vs.W2NN\nand ground-truth distributions\nFigure 5: Distributional nearest neighbor methods enable efficient LLM evaluation on MMLU.\nWe estimate LLM score distributions across all models and tasks given only a limited number of\nmodel-task evaluations, determined by the propensity p. See Sec. 4.4 for a detailed discussion.\ntasks, smaller, fine-tuned models may even outperform the latest frontier models, in addition to being\nmore cost effective.\nWe investigate how matrix completion, specifically nearest neighbor methods, can alleviate some of\nthese burdens. We use the PromptEval dataset [ PXW+24], which evaluates 15open-source language\nmodels (ranging in size from 3B to 70B parameters) and 100different prompting techniques across\nthe 57 tasks of the MMLU benchmark [ HBB+20]. In practice, the performance of a model depends—\nsometimes dramatically—on the precise input prompt. This suggests that we need to consider the\nperformance of a model across a wide range of prompts, rather than any one prompt in particular.\nThus, we model this problem as a distributional matrix completion problem with N= 15 ,T= 57 ,\nandn= 100 . Given one of 57 tasks, we aim to accurately characterize the performance of each\nmodel without resorting to exhaustive evaluation. Nearest neighbors leverage commonalities across\nmodels and tasks to estimate the performance distribution of each entry, which was otherwise not\nconsidered in [ PXW+24]; previous literature achieves efficient evaluation per model and task in\nisolation without leveraging any across model / task information.\nResults & Discussion. We randomly include each entry in the matrix independently with probability\np∈ {0.3,0.5,0.7}and impute the missing entries using the KernelNN andW2NNmethods of Tab. 1.\nFor each method, we consider both the the row-wise and column-wise variants. Fig. 5(a) reports the\nmean Kolmogorov-Smirnov (KS) distance between the imputed and ground-truth distributions across\nthe entries in the test set for varying missingness values. As expected, estimation error decreases as p\nincreases. Fig. 5(b) visualizes the imputed distributions using row-wise KernelNN and column-wise\nW2NN(atp= 0.7) for a select entry, along with the ground-truth distribution. Even with 30% of\nmatrix entries missing, distributional NN methods are able to recover the underlying distribution.\n5 Conclusion\nIn this paper, we present a unified framework, Python library ( N2), and test bench ( N2-Bench ) for\nnearest neighbor-based matrix completion algorithms for both scalar and distributional settings. We\ndemonstrate how our library supports a diverse set of datasets spanning recommendation systems\n(MovieLens), patient-level healthcare causal inference (HeartSteps), counterfactual inference for\npanel data (Proposition 99), and LLM evaluation (PromptEval). Our framework and library facilitate\nresearchers and practitioners to easily try out different nearest neighbor methods on their dataset of\nchoice as well as extend the library to more complex nearest neighbor methods.\nSeveral future directions are natural from our work. Here we focused on unweighted and specific\nweighing schemes; several other weighting strategies can be easily incorporated into the library. Next,\nbeing able to deal with larger matrices requires handling distributed datasets as well as speeding up\n9\n--- Page 10 ---\nthe runtime of N2, e.g., via efficient implementations using matrix multiplication or approximate\nnearest neighbors strategies.\nReferences\n[AADS24] Alberto Abadie, Anish Agarwal, Raaz Dwivedi, and Abhin Shah. Doubly robust\ninference in causal latent factor models. arXiv preprint arXiv:2402.11652 , 2024.\n[ABD+21]Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar\nKhosravi. Matrix completion methods for causal panel data models. Journal of the\nAmerican Statistical Association , 116(536):1716–1730, 2021.\n[ADH10] Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for\ncomparative case studies: Estimating the effect of california’s tobacco control program.\nJournal of the American statistical Association , 105(490):493–505, 2010.\n[ADSS23] Anish Agarwal, Munther Dahleh, Devavrat Shah, and Dennis Shen. Causal matrix\ncompletion. In The Thirty Sixth Annual Conference on Learning Theory , pages 3821–\n3826. PMLR, 2023.\n[AI22] Susan Athey and Guido W Imbens. Design-based analysis in difference-in-differences\nsettings with staggered adoption. Journal of Econometrics , 226(1):62–79, 2022.\n[Bai09] Jushan Bai. Panel data models with interactive fixed effects. Econometrica , 77(4):1229–\n1279, 2009.\n[BBBK11] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for\nhyper-parameter optimization. Advances in neural information processing systems , 24,\n2011.\n[BC22] Sohom Bhattacharya and Sourav Chatterjee. Matrix completion with data-dependent\nmissingness probabilities. IEEE Transactions on Information Theory , 68(10):6762–\n6773, 2022.\n[BGKL17] Jérémie Bigot, Raúl Gouet, Thierry Klein, and Alfredo López. Geodesic PCA in the\nWasserstein space by convex PCA. Annales de l’Institut Henri Poincaré, Probabilités et\nStatistiques , 53(1):1 – 26, 2017.\n[Big20] Bigot, Jérémie. Statistical data analysis in the wasserstein space*. ESAIM: ProcS ,\n68:1–19, 2020.\n[BN21] Jushan Bai and Serena Ng. Matrix completion, counterfactuals, and factor analysis of\nmissing data. Journal of the American Statistical Association , 116(536):1746–1763,\n2021.\n[BPV+24]Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv,\nMatan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen,\net al. Unitxt: Flexible, shareable and reusable data preparation and evaluation for\ngenerative ai. arXiv preprint arXiv:2401.14019 , 2024.\n[BYC13] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search:\nHyperparameter optimization in hundreds of dimensions for vision architectures. In\nInternational conference on machine learning , pages 115–123. PMLR, 2013.\n[CAD20] Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of\nmeasures in high dimensions. arXiv preprint arXiv:2007.07105 , 2020.\n[CBS20] Timothy I Cannings, Thomas B Berrett, and Richard J Samworth. Local nearest\nneighbour classification with applications to semi-supervised learning. The Annals of\nStatistics , 48(3):1789–1814, 2020.\n[CFC+24]Kyuseong Choi, Jacob Feitelberg, Caleb Chin, Anish Agarwal, and Raaz Dwivedi.\nLearning counterfactual distributions via kernel nearest neighbors. arXiv preprint\narXiv:2410.13381 , 2024.\n10\n--- Page 11 ---\n[CH67] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions\non information theory , 13(1):21–27, 1967.\n[Cha15] Sourav Chatterjee. Matrix estimation by Universal Singular Value Thresholding. The\nAnnals of Statistics , 43(1):177 – 214, 2015.\n[CR12] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimiza-\ntion. Communications of the ACM , 55(6):111–119, 2012.\n[DTT+22a] Raaz Dwivedi, Katherine Tian, Sabina Tomkins, Predrag Klasnja, Susan Murphy, and\nDevavrat Shah. Counterfactual inference for sequential experiments. arXiv preprint\narXiv:2202.06891 , 2022.\n[DTT+22b] Raaz Dwivedi, Katherine Tian, Sabina Tomkins, Predrag Klasnja, Susan Murphy, and\nDevavrat Shah. Doubly robust nearest neighbors in factor models. arXiv preprint\narXiv:2211.14297 , 2022.\n[DZCM22] Raaz Dwivedi, Kelly Zhang, Prasidh Chhabaria, and Susan Murphy. Deep dive into\npersonalization. Working paper , 2022.\n[FCAD24] Jacob Feitelberg, Kyuseong Choi, Anish Agarwal, and Raaz Dwivedi. Distributional\nmatrix completion via nearest neighbors in the wasserstein space. arXiv preprint\narXiv:2410.13112 , 2024.\n[GHC+25]Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah\nCornes, Rene Kizilcec, and Dennis Shung. Benchmarking generative ai for scoring\nmedical student interviews in objective structured clinical examinations (osces). arXiv\npreprint arXiv:2501.13957 , 2025.\n[GHJV93] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. Design patterns:\nAbstraction and reuse of object-oriented design. In ECOOP’93—Object-Oriented\nProgramming: 7th European Conference Kaiserslautern, Germany, July 26–30, 1993\nProceedings 7 , pages 406–431. Springer, 1993.\n[GTA+23]Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi,\nCharles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle\nMcDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey\nSchoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang,\nKevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12\n2023.\n[HBB+20]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. Measuring massive multitask language understanding. arXiv\npreprint arXiv:2009.03300 , 2020.\n[HK15] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context.\nACM Trans. Interact. Intell. Syst. , 5(4), December 2015.\n[HMLZ15] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion\nand low-rank svd via fast alternating least squares. The Journal of Machine Learning\nResearch , 16(1):3367–3402, 2015.\n[Hun07] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science &\nEngineering , 9(3):90–95, 2007.\n[JSM+23]Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume\nLample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\n[KBV09] Yehuda Koren, Robert Bell, and Chris V olinsky. Matrix factorization techniques for\nrecommender systems. Computer , 42(8):30–37, 2009.\n[KMO10] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion\nfrom a few entries. IEEE transactions on information theory , 56(6):2980–2998, 2010.\n11\n--- Page 12 ---\n[KSS+19]Predrag Klasnja, Shawna Smith, Nicholas J Seewald, Andy Lee, Kelly Hall, Brook\nLuers, Eric B Hekler, and Susan A Murphy. Efficacy of contextually tailored suggestions\nfor physical activity: a micro-randomized optimization trial of heartsteps. Annals of\nBehavioral Medicine , 53(6):573–582, 2019.\n[LGKM20] Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized\nheartsteps: A reinforcement learning algorithm for optimizing physical activity. Proc.\nACM Interact. Mob. Wearable Ubiquitous Technol. , 4(1), March 2020.\n[LR19] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data , volume\n793. John Wiley & Sons, 2019.\n[LSSY19] Yihua Li, Devavrat Shah, Dogyoon Song, and Christina Lee Yu. Nearest neighbors\nfor matrix estimation interpreted as blind regression for latent variable model. IEEE\nTransactions on Information Theory , 66(3):1760–1784, 2019.\n[MC19] Wei Ma and George H Chen. Missing not at random in matrix completion: The effec-\ntiveness of estimating missingness probabilities under a low nuclear norm assumption.\nAdvances in neural information processing systems , 32, 2019.\n[Met24] Meta. Introducing meta llama 3: The most capable openly available llm to date.\nhttps://ai.meta.com/blog/meta-llama-3 , 2024.\n[MFS+17]Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Schölkopf,\net al. Kernel mean embedding of distributions: A review and beyond. Foundations and\nTrends® in Machine Learning , 10(1-2):1–141, 2017.\n[OW23] Orzechowski and Walker. The Tax Burden on Tobacco, 1970-2019 | Data | Cen-\nters for Disease Control and Prevention — data.cdc.gov. https://data.cdc.gov/\napi/views/7nwe-3aj9/rows.csv?accessType=DOWNLOAD , 2023. [Accessed 16-\n05-2025].\n[PVG+11]F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blon-\ndel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.\nJournal of Machine Learning Research , 12:2825–2830, 2011.\n[PXW+24]Felipe Maia Polo, Ronald Xu, Lucas Weber, Mírian Silva, Onkar Bhardwaj, Leshem\nChoshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, and Mikhail Yurochkin.\nEfficient multi-prompt evaluation of llms. In The Thirty-eighth Annual Conference on\nNeural Information Processing Systems , 2024.\n[QWC+22]Tianchen Qian, Ashley E Walton, Linda M Collins, Predrag Klasnja, Stephanie T Lanza,\nInbal Nahum-Shani, Mashfiqui Rabbi, Michael A Russell, Maureen A Walton, Hyesun\nYoo, et al. The microrandomized trial for developing digital interventions: Experimental\ndesign and data analysis considerations. Psychological methods , 27(5):874, 2022.\n[Rec11] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine\nLearning Research , 12(12), 2011.\n[RS05] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization\nfor collaborative prediction. In Proceedings of the 22nd international conference on\nMachine learning , pages 713–719, 2005.\n[SKKR01] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collabo-\nrative filtering recommendation algorithms. In Proceedings of the 10th International\nConference on World Wide Web , WWW ’01, page 285–295, New York, NY , USA, 2001.\nAssociation for Computing Machinery.\n[SPD24] Tathagata Sadhukhan, Manit Paul, and Raaz Dwivedi. On adaptivity and minimax\noptimality of two-sided nearest neighbors. arXiv preprint arXiv:2411.12965 , 2024.\n[SPD25] Tathagata Sadhukhan, Manit Paul, and Raaz Dwivedi. Adaptively-weighted nearest\nneighbors for matrix completion. arXiv preprint arXiv:2505.09612 , 2025.\n12\n--- Page 13 ---\n[TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love,\net al. Gemma: Open models based on gemini research and technology. arXiv preprint\narXiv:2403.08295 , 2024.\n13\n--- Page 14 ---\n1 Introduction 1\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2 Nearest Neighbors for Matrix Completion 3\n2.1 Unified framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Existing methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3 New variant: Auto nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3N2Package and Interface 5\n4N2-Bench and Results 6\n4.1 Personalized healthcare: HeartSteps . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2 Movie recommendations: MovieLens . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.3 Counterfactual inference for panel data: Proposition 99 . . . . . . . . . . . . . . . 8\n4.4 Efficient LLM evaluation: PromptEval . . . . . . . . . . . . . . . . . . . . . . . . 8\n5 Conclusion 9\nA Structural assumptions 14\nA.1 Factor model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA.2 Missingness pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nB Nearest neighbor algorithms 15\nB.1 Vanilla nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nB.2 Two-sided and doubly-robust nearest neighbors . . . . . . . . . . . . . . . . . . . 16\nB.3 Distributional nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nB.4 Adaptively weighted nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . 18\nC Cross-Validation 19\nD Case Study Details 19\nD.1 Synthetic data generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD.2 HeartSteps V1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD.3 MovieLens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nD.4 Proposition 99 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nD.5 PromptEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA Structural assumptions\nProvable guarantees of nearest neighbors in matrix settings (1) can be shown when structural\nassumptions are imposed on the distributions µi,tand the missingness Ai,t. We collect existing\nresults from [ LSSY19 ,DTT+22b,CFC+24,FCAD24 ,SPD24 ,SPD25 ]. Given data with missing\nobservations from (1), the practitioner is interested in learning information of the distributions, e.g.,\nmean of the distributions {θi,t=R\nxdµi,t(x)}.\n14\n--- Page 15 ---\nThe first assumption specifies the factor structure on the mean; that is, there exists latent factors\nui, vtthat collectively characterize the signal of each entry (i, t)of the matrix [ LSSY19 ,DTT+22a,\nADSS23 ,CFC+24,FCAD24 ]. Such a factor model is analogous to the low rank assumptions\ncommonly imposed in matrix completion [ CR12 ]. The second assumption specifies how the missing\npattern Ai,twas generated; for instance missing completely at random (MCAR) assumes that Ai,t\nare independent to all other randomness present in the model (1) and that all entries have positive\nprobability of being observed.\nA.1 Factor model\nFor the scalar matrix completion problem, i.e., (1) with n= 1, the main goal is to learn (or\nimpute) the mean of the underlying distribution θi,tfor any missing entries [ LSSY19 ,DTT+22b,\nDTT+22a,ADSS23 ,SPD24 ,SPD25 ]. The majority of this literature assumes (i) an additive noise\nmodel µi,t=θi,t+εi,tfor centered i.i.d. sub-Gaussian noise εand (ii) mean factor model, i.e.,\nθi,t=f(ui, vt)for some latent factors ui, vtand real valued function f.\nFor the distributional matrix completion problem (i.e., (1) with n >1) the main goal is to learn the\nunderlying distribution itself [ CFC+24,FCAD24 ]; a factor model is imposed on the distribution as a\nwhole. For instance, a factor model is assumed on the kernel mean embedding of distributions; that\nis, there exist latent factors uiandvtand an operator gsuch thatR\nk(x,·)dµi,t(x) =g(ui, vt).\nA.2 Missingness pattern\nFor both the scalar and distributional matrix completion problem (1), the missing pattern (i.e., how\nthe missingness Aj,swas generated) can be categorized into three classes using the taxonomy of\n[LR19 ]: missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-\nrandom (MNAR). MCAR assumes that the missingness Ai,tis exogenous (independently generated\nfrom all the randomness in the model) and i.i.d. with propensity P(Ai,t= 1) = p > 0for all\n(i, t). MAR is a more challenging scenario compared to MCAR as missingness is not exogenous,\nbut its randomness depends on the observations. Further, propensities pi,tmay differ for entries\n(i, t)but positivity still holds, i.e., mini∈[N],t∈[T]pi,t>0. An important instance for MAR is the\nadaptive randomized policies [ DZCM22 ]. The MNAR setup is the most challenging as it assumes\nthe missingness depends on the unobserved latent confounders, while positivity may also be violated,\ni.e.,mini∈[N],t∈[T]pi,t= 0. The staggered adoption pattern, where a unit remains treated once a\nunit is treated at some adoption time, is a popular example of MNAR, mainly because positivity is\nviolated. See [ABD+21, AI22] for more details on staggered adoption.\nWe briefly outline the structural assumptions existing nearest neighbor methods were shown to work\nwith provable guarantees; for all the existing methods, factor models (with slightly different details;\ncompare the mean factorization [ LSSY19 ] and the distribution factorization [ CFC+24,FCAD24 ])\nare all commonly assumed.\n•(Scalar matrix completion) The vanilla versions of nearest neighbors ( RowNN ) in [ LSSY19 ,\nDTT+22a] are shown to work for MCAR and MAR setup; the latter shows that simple\nnearest neighbors can provably impute the mean when the missingness is fully adaptive\nacross all users and history. The variants of vanilla nearest neighbors DRNN [DTT+22b] is\nproven to work under MCAR, while TSNN [SPD24 ] is proven to work under unobserved\nconfounding, i.e., MNAR.\n•(Distributional matrix completion) The KernelNN [CFC+24] is shown to recover the un-\nderlying distribution under MNAR, whereas W2NN[FCAD24 ] is shown to work under\nMCAR.\nB Nearest neighbor algorithms\nThe nearest neighbor methods introduced in Tab. 1 are elaborated in this section. We present two\nversions of each method; the first version explicitly constructs neighborhoods instead of subtly\nembedding them in the weights Wof the AVERAGE module, and the second version specifies how\neach methods can be recovered by applying the two modules, DISTANCE andAVERAGE , sequentially.\n15\n--- Page 16 ---\nB.1 Vanilla nearest neighbors\nWe elaborate on the discussion in Rem. 1 and provide here a detailed algorithm based on the explicit\nconstruction of neighborhoods, which is essentially equivalent to RowNN in Tab. 1. The inputs are\nmeasurements Z, missingness A, the target index (i, t), and the radius η.\nStep 1: (Distance between rows) Calculate the distance between row iand any row j∈[N]\\ {i}\nby averaging the squared Euclidean distance across overlapping columns:\nρi,j:=P\ns̸=tAi,sAj,s(Zi,s−Zj,s)2\nP\ns̸=tAi,sAj,s.\nStep 2: (Construct neighborhood) Construct a neighborhood of radius ηwithin the tth column using\nthe distances {ρi,j:j̸=i}:\nNt,η:=\b\nj∈[N]\\ {i}:ρi,j≤η\t\nStep 3: (Average across observed neighbors) Take the average of measurements within the neigh-\nborhood:\nbθi,t,η:=1\n|Nt,η|X\nj∈Nt,ηAj,tZj,t.\nIn practice, the input ηforRowNN should be optimized via cross-validation; we refer the reader to\nApp. C for a detailed implementation.\nWe specify the exact implementation of the two modules DISTANCE ,AVERAGE to recover RowNN :\nAlgorithm 1: RowNN for scalar nearest neighbor\nInput: Z,A, η,(i, t)\n1Initialize entry-wise metric bφ(Zj,s, Zj′,s′)←(Zj,s−Zj′,s′)2and metric φ(x, y)←(x−y)2\n2Initialize hyper-parameter η←(η1,0)\n3Calculate row-wise metric\b\nρrow\ni,j:j̸=i\t\n←DISTANCE (bφ,Z,A)\n4Initialize weight wj,s←1(ρrow\ni,j≤η1, ρcol\ns,t≤η2)\n5Calculate average bθi,t←AVERAGE (φ,W, Z, A )\n6returnbθi,t\nThe discussion for RowNN here can be identically made for ColNN as well.\nB.2 Two-sided and doubly-robust nearest neighbors\nWe elaborate on the variants of the vanilla nearest neighbors algorithm TSNN andDRNN in Tab. 1;\nwe first elaborate on an equivalent version of each of the methods which explicitly constructs\nneighborhoods.\nIn the following three step procedure, DRNN andTSNN differs in the last averaging step: the inputs\nare the measurements Z, missingness A, the target index (i, t), and the radii η= (η1, η2).\nStep 1: (Distance between rows) Calculate the distance between row iand any row j∈[N]\\ {i}\nand the distance between column tand any column s∈[T]\\ {t}:\nρrow\ni,j:=P\ns̸=tAi,sAj,s(Zi,s−Zj,s)2\nP\ns̸=tAi,sAj,sand ρcol\nt,s:=P\nj̸=iAj,tAj,s(Zj,t−Zj,s)2\nP\nj̸=iAj,tAj,s\nStep 2: (Construct neighborhood) Construct a row-wise and column-wise neighborhood of radius η1\nandη2respectively,\nNrow\nt,η1:=\b\nj∈[N]\\ {i}:ρrow\ni,j≤η\t\nandNcol\ni,η2:=\b\ns∈[T]\\ {t}:ρcol\nt,s≤η\t\n16\n--- Page 17 ---\nStep 3: (Average across observed neighbors) Take the average of measurements within the neigh-\nborhood; the first and the second averaging correspond to DRNN andTSNN respectively:\nbθDR\ni,t,η:=P\nj∈Nrow\nt,η1,s∈Ncol\ni,η2Aj,tAi,sAj,s\u0000\nZj,t+Zi,s−Zj,s\u0001\nP\nj∈Nrow\nt,η1,s∈Ncol\ni,η2Aj,tAi,sAj,sand\nbθTS\ni,t,η:=P\nj∈Nrow\nt,η1,s∈Ncol\ni,η2Aj,sZj,s\nP\nj∈Nrow\nt,η1,s∈Ncol\ni,η2Aj,s.\nNext, we specity the exact implemention of the two modules DISTANCE andAVERAGE to recover\nTSNN andDRNN :\nAlgorithm 2: TSNN for scalar matrix completion\nInput: Z,A, η,(i, t)\n1Initialize entry-wise metric bφ(Zj,s, Zj′,s′)←(Zj,s−Zj′,s′)2and metric φ(x, y)←(x−y)2\n2Initialize tuning parameter η←(η1, η2)\n3Calculate row-wise and column-wise metric\b\nρrow\ni,j:j̸=i\t\n,\b\nρcol\nt,s:s̸=t\t\n←DISTANCE (bφ, Z, A )\n4Initialize weight wj,s←1(ρrow\ni,j≤η1, ρcol\ns,t≤η2)\n5Calculate average bθi,t←AVERAGE (φ,W,Z,A)\n6returnbθi,t\nForDRNN algorithm below, we consider ZandAto beN×Tsized matrices, so that their transpose\nis well defined. Then note that ColNN is simply applying Alg. 1 with transposed observation matrices.\nAlgorithm 3: DRNN for scalar matrix completion\nInput: Z,A, η,(i, t)\n1Initialize RowNN ←Alg. 1 with inputs (Z,A, η,(i, t))andη←(η1,0)\n2Initialize ColNN ←Alg. 1 with input (ZT,AT, η,(i, t))andη←(η1,0)\n3Initialize TSNN ←Alg. 2 with inputs (Z,A, η,(i, t))andη←(η1, η2)\n4Calculate bθi,t←RowNN +ColNN −TSNN\n5returnbθi,t\nB.3 Distributional nearest neighbors\nUnlike the scalar nearest neighbor methods, distributional nearest neighbors necessitate a distribu-\ntional notion of distance between rows and columns of matrix and a distributional analog of averaging.\n[CFC+24] and [ FCAD24 ] use maximum mean discrepency (in short MMD ) of kernel mean em-\nbeddings [ MFS+17] and Wasserstein metric (in short W2) [Big20 ] respectively both for defining\nthe distance between rows / columns and for averaging. The corresponding barycenters of MMD\nandW2[CAD20 ,BGKL17 ] are used for averaging, and so the methods are coined kernel nearest\nneighbors (in short KernelNN ) and Wasserstein nearest neighbors (in short W2NN) respectively.\nWe elaborate on a vanilla version three step procedure of KernelNN ,W2NNthat explicitly constructs\nneighborhoods. The input are measurements Z, missingness A, the target index (i, t)and the radius\nη,\nStep 1: (Distance between rows) Calculate the distance between row iand any row j∈[N]\\ {i}\nby averaging the estimator of distribution metric bϱ:\nρMMD\ni,j :=P\ns̸=tAi,sAj,s\\MMD2\nk(Zi,s, Zj,s)P\ns̸=tAi,sAj,sand ρW2\ni,j:=P\ns̸=tAi,sAj,sbW2\n2(Zi,s, Zj,s)P\ns̸=tAi,sAj,s.\nStep 2: (Construct neighborhood) Construct a neighborhood of radius ηwithin the tth column using\nthe distances {ρi,j:j̸=i}:\nNMMD\nt,η :=\b\nj∈[N]\\ {i}:ρMMD\ni,j≤η\t\nandNW2\nt,η:=\b\nj∈[N]\\ {i}:ρW2\ni,j≤η\t\n17\n--- Page 18 ---\nStep 3: (Average across observed neighbors) Set µZ\ni,t=n−1Pn\nℓ=1δXℓ(i,t)as the empirical measure\nof the multiple measurements Zi,t. Take the barycenter within the neighborhood:\nbµMMD\ni,t,η :=1\n|NMMD\nt,η|X\nj∈NMMD\nt,ηAj,tµZ\nj,t and\nbµW2\ni,t,η:= argmin\nµX\nj∈∈NW2\nt,ηW2\n2(µ, µZ\nj,t).\nFor further details on the W2andMMD algorithms see [FCAD24] and [CFC+24], respectively.\nAlgorithm 4: Vanilla (row-wise) distributional nearest neighbor\nInput: Z,A,k, η,(i, t)\n1Initialize entry-wise metric bφ(Zj,s, Zj′,s′)←\\MMD2\nk(Zj,s, Zj′,s′)orbW2\n2(Zj,s, Zj′,s′)\n2Initialize metric φ(x, y)←MMD2\nk(x, y)orW2\n2(x, y)\n3Initialize tuning parameter η←(η1,0)\n4Calculate row-wise metric\b\nρrow\ni,j:j̸=i\t\n←DISTANCE (bφ, Z, A )\n5Initialize weight wj,s←1(ρrow\ni,j≤η1, ρcol\ns,t≤η2)\n6Calculate average bµi,t←AVERAGE (φ,W,Z,A)\n7returnbµi,t\nB.4 Adaptively weighted nearest neighbors\nWe elaborate on the adaptive variant of the vanilla nearest neighbor algorithm AWNN as mentioned\nin Sec. 2.2 and Tab. 1. The input are measurements Z, and missingness A. Note that there is no need\nfor radius parameter ηand hence no CV .\nStep 1: (Distance between rows and initial noise variance estimate) Calculate an estimate for noise\nvariance and then the distance between any pair of distinct rows i, j∈[N]by averaging the\nsquared Euclidean distance across overlapping columns:\nρi,j:=P\ns̸=tAi,sAj,s(Zi,s−Zj,s)2\nP\ns̸=tAi,sAj,s, Z←P\nj,s∈[N]×[T]Aj,sZj,sP\nj,s∈[N]×[T]Aj,s,\nbσ2←P\nj,s∈[N]×[T]Aj,s(Zj,s−Z)2\nP\nj,s∈[N]×[T]Aj,s\nStep 2: (Construct weights) For all rows and columns (i, t)∈[N]×[T], evaluate w(i,t)=\n(w1,t,···, wn,t), the weights that optimally minimizes the following loss involving an\nestimate of the noise variance bσ2,\nw(i,t)=arg minbw(i,t)\n2 log(2 m/δ)bσ2∥bw(i,t)∥2\n2+X\ni′∈[N]bwi′,tAi′,tbρi′,i\n, (3)\nwherebw(i,t)= (bw1,t,···,bwn,t)is a non-negative vector that satisfyPn\ni′=1bwi′,tAi′,t= 1.\nStep 3: (Weighted average) Take the weighted average of measurements:\nbθi,t=X\ni′∈[N]bwi′,tAi′,tXi′,t,∀(i, t)∈[N]×[T]\nStep 4: (Fixed point iteration over noise variance) Obtain new estimate of noise variance and stop if\ndifference between old and new bσ2is small.\nbσ2←1P\ni∈[N],t∈[T]Ai,tX\ni∈[N],t∈[T]\u0010\nZi,t−bθi,t\u00112\nAi,t\n18\n--- Page 19 ---\nNo cross-validation in A WNN The optimization problem in (3) can be solved exactly in linear time\n(worst case complexity) using convex optimization [ SPD25 ].AWNN doesn’t rely on radius parameter\nη. Not only it automatically assigns neighbors to (i, t)thentry during its weight calculation(non-\nneighbors get zero weight), but also takes into account the distance of the neighbors from the (i, t)th\nentry. The closer neighors get higher weights and vice - versa.\nWe further specify the exact implementation of the two modules DISTANCE ,AVERAGE to recover\nAWNN :\nAlgorithm 5: AWNN for scalar nearest neighbor\nInput: Z,A,(i, t)\n1Initialize entry-wise metric bφ(Zj,s, Zj′,s′)←(Zj,s−Zj′,s′)2and metric φ(x, y)←(x−y)2\n2Initialize noise - variance estimate σ2\nϵ←Variance\u0010\n{Zi,t}(i,t)∈[N]×[T]\u0011\n3Calculate row-wise metric\b\nρrow\ni,j:j̸=i\t\n←DISTANCE (bφ,Z,A)\n4Initialize weight {w1,t, . . . , w n,t} ← arg minbw(i,t)h\n2 log(2 m/δ)bσ2∥bw(i,t)∥2\n2+P\ni′∈[N]bwi′,tAi′,tbρi′,ii\n5Calculate average bθi,t←AVERAGE (φ,W, Z, A )\n6returnbθi,t\nC Cross-Validation\nFor each nearest neighbor method, we use cross-validation to optimize hyperparameters including\ndistance thresholds and weights, depending on which nearest neighbor algorithm is chosen. Specif-\nically, for each experiment, we choose a subset of the training test to optimize hyperparameters\nby masking those matrix cells and then estimating the masked values. We utilize the HyperOpt\nlibrary [ BYC13 ] to optimize (possibly multiple) hyperparamters using the Tree of Parzen Estimator\n[BBBK11 ], a Bayesian optimization method. Our package supports both regular distance thresholds\nand percentile-based thresholds, which adapt to the distances calculated within the specific dataset.\nD Case Study Details\nThe boxplots are generated using matplotlib’s [ Hun07 ] standard boxplot function. The box shows\nthe first, second, and third quartiles. The bottom line shows the first quartile minus the 1.5 ×the\ninterquartile range. The top line shows the third quartile plus 1.5 ×the interquartile range. All\nexperiments are run on standard computing hardware (MacBook Pro with an M2 Pro CPU with 32\nGB of RAM).\nD.1 Synthetic data generation\nGenerate Zi,t=Xi,t∼N(θi,t, σ2), i.e., scalar matrix completion setting, with a linear factor\nstructure θi,t=uivt. Row latent factors ui∈R4are i.i.d. generated across i= 1, ..., N , where each\nentry of uifollow a uniform distribution with support [−0.5,0.5]; column latent factors vt∈R4are\ngenerated in an identical manner. The missingness is MCAR with propensity pi,t= 0.5for all iand\nt. Further, the size of column and rows are identical N=T. For the left panel in Fig. 1, the noise\nlevel is set as σ= 0.001and for the right panel σ= 1.\nD.2 HeartSteps V1\nThe mobile application was designed to send notifications to users at various times during the day\nto encourage anti-sedentary activity such as stretching or walking. Participants could be marked as\nunavailable during decision points if they were in transit or snoozed their notifications, so notifications\nwere only sent randomly if a participant was available and were never sent if they were unavailable.\nTo process the data in the framework of (1), we let matrix entry Zi,tbe the average one hour\nstep count for participant iand decision point twhen a notification is sent (i.e. Ai,t= 1) and\nunknown when a notification is not sent (i.e. Ai,t= 0). The treatment assignment pattern is\n19\n--- Page 20 ---\n0 25 50 75 100 125 150 175\nDecision point0\n10\n20\n30UnitsFigure 6: HeartSteps V1 data notification pattern. The dark blue entries indicate that the app sent\na notification to a sedentary participant—the entry has value Ai,t= 1. The white entries indicate that\nthe participant was available but did not receive a notification or they were active immediately prior\nto the decision point. The light blue entries indicate the participant was unavailable. We assign the\nvalue Ai,t= 0for all the white and light blue entries.\nrepresented as the 37 x 200 matrix visualized in Fig. 6. We use the dataset downloaded from\nhttps://github.com/klasnja/HeartStepsV1 (CC-BY-4.0 License).\nD.3 MovieLens\nWe load MovieLens via a custom MovieLensDataLoader that (i) downloads and caches the\nml-1m.zip archive, (ii) reads ratings.dat into a user ×movie pivot table, and (iii) constructs\nthe binary mask where observed entries correspond to rated user–movie pairs. The data matrix is\nZ∈ {1, . . . , 5}6040×3952and mask matrix is A∈ {0,1}6040×3952. The data can be downloaded\nfrom https://grouplens.org/datasets/movielens/1m/ . See https://files.grouplens.\norg/datasets/movielens/ml-1m-README.txt for the usage license.\nD.4 Proposition 99\nData comes primarily from the Tax Burden on Tobacco compiled by Orzechowski and Walker [ OW23 ]\n(ODC-By License). Using synthetic control methods, Abadie et al. construct a weighted combination\nof control states that closely resembles California’s pre-1988 characteristics and cigarette consumption\npatterns. The optimal weights produce a synthetic California primarily composed of Colorado (0.164),\nConnecticut (0.069), Montana (0.199), Nevada (0.234), and Utah (0.334), with all other states\nreceiving zero weight. The treatment effect is estimated as the difference between actual California\nper-capita cigarette sales and those of synthetic California after Proposition 99’s implementation. By\n2000, this analysis revealed that annual per-capita cigarette sales in California were approximately\n26 packs lower than what they would have been without Proposition 99, representing about a 25%\nreduction in cigarette consumption. To validate these findings, the authors conducted placebo tests by\napplying the same methodology to states not implementing tobacco control programs, confirming\nthat California’s reduction was unusually large and statistically significant (p = 0.026).\nProposition 99, the California Tobacco Tax and Health Protection Act of 1988, dataset spans from\n1970 to 2000, providing 19 years of pre-intervention data before Proposition 99 was implemented\nin 1988 and 12 years of post-intervention data. It provides annual state-level cigarette consumption\nmeasured as per capita cigarette sales in packs based on tax revenue data. This data serves as a\nreal data benchmark for many of the variants of synthetic controls [ ABD+21]. We use the CDC\ndataset for the Nearest Neighbors methods and only use the target variable (i.e., cigarette consumption\nmeasured in packs per capita), and the dataset from SyntheticControlMethods library3for the SC\nbaseline, since it relies on additional covariates.\n3https://github.com/OscarEngelbrektson/SyntheticControlMethods/tree/master (Apache-\n2.0 License)\n20\n--- Page 21 ---\nD.5 PromptEval\nMMLU is a multiple choice Q&A benchmark with 57tasks, with a total of near 14K examples4.\n15different models, e.g., variants of Llama 3 [ Met24 ], Mistral [ JSM+23] and Gemma [ TMH+24].\nThe examples are fed into the models with 100different varying prompting templates. The prompt\ntemplates are created by traversing between 3node modules, namely a separator , aspace and an\noperator (see [ PXW+24, Alg. 3, App. J] for details), from which 100unique prompt templates are\ncreated. The unitxt [ BPV+24] preprocessing library is used to construct the dataset and evaluation\nis done by LM-Eval-Harness [ GTA+23] library. The number of examples differ per task and each\nexamples are evaluated on a model (verifiable, so assigned 0or1for correctness) by wrapping the\nexamples with 100different prompt templates.\n4https://github.com/felipemaiapolo/prompteval (MIT License)\n21",
  "text_length": 64583
}