{
  "id": "http://arxiv.org/abs/2506.01206v1",
  "title": "Mamba Drafters for Speculative Decoding",
  "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.",
  "authors": [
    "Daewon Choi",
    "Seunghyuk Oh",
    "Saket Dingliwal",
    "Jihoon Tack",
    "Kyuyoung Kim",
    "Woomin Song",
    "Seojin Kim",
    "Insu Han",
    "Jinwoo Shin",
    "Aram Galstyan",
    "Shubham Katiyar",
    "Sravan Babu Bodapati"
  ],
  "published": "2025-06-01T22:52:47Z",
  "updated": "2025-06-01T22:52:47Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01206v1",
  "full_text": "--- Page 1 ---\nMamba Drafters for Speculative Decoding\nDaewon Choi1, Seunghyuk Oh1, Saket Dingliwal2, Jihoon Tack1,\nKyuyoung Kim1,Woomin Song1,2,†,Seojin Kim3,‡,Insu Han1,Jinwoo Shin1,\nAram Galstyan2,Shubham Katiyar2,Sravan Babu Bodapati2\n1KAIST2Amazon AGI3Seoul National University\nAbstract\nSpeculative decoding has emerged as a promis-\ning approach to accelerating large language\nmodel (LLM) generation using a fast drafter\nwhile maintaining alignment with the target\nmodel’s distribution. However, existing ap-\nproaches face a trade-off: external drafters offer\nflexibility but can suffer from slower drafting,\nwhile self-speculation methods use drafters tai-\nlored to the target model but require re-training.\nIn this paper, we introduce novel drafters based\non Mamba, a state-of-the-art state space model\n(SSM), as a solution that combines the best as-\npects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids\nthe quadratic complexity inherent in traditional\nTransformer-based methods, enabling faster\ndrafting and lower memory usage while main-\ntaining the flexibility to work across different\ntarget models. We further enhance efficiency\nwith a novel test-time tree search algorithm for\ngenerating high-quality draft candidates. Our\nempirical evaluation demonstrates that Mamba-\nbased drafters not only outperform existing ex-\nternal drafting methods but are also comparable\nto state-of-the-art self-speculation approaches\nwhile using less memory and maintaining their\ncross-model adaptability.\n1 Introduction\nRecent breakthroughs in large language models\n(LLMs) have been largely driven by Transformer\narchitectures (Vaswani et al., 2017), which have en-\nabled exceptional performance across a wide range\nof tasks (Achiam et al., 2023; Singhal et al., 2025;\nKim et al., 2024a). However, their capabilities of-\nten come with significant computational overhead,\nprimarily due to the autoregressive nature of se-\nquential token generation, while the quadratic com-\nplexity of the attention mechanism further exacer-\nbates scalability challenges. Speculative decoding\n†Work done during an internship at Amazon.‡Work\ndone at KAIST.(SD) (Stern et al., 2018; Leviathan et al., 2023; Xia\net al., 2023; Chen et al., 2023) has emerged as a\npromising approach to addressing the inefficien-\ncies of autoregressive models by generating multi-\nple candidate tokens with an efficient drafter and\nverifying them in parallel with the target model,\nensuring identical output with greater efficiency.\nThis approach enables simultaneous decoding of\nmultiple tokens within a single forward pass.\nExisting SD methods are mainly categorized into\ntwo types: (i) using an external drafter (Leviathan\net al., 2023) applicable to multiple target mod-\nels, and (ii) adopting a self-speculation approach,\nwhere a drafter is trained to align with the target\nmodel itself, showing faster drafting speed com-\npared to external drafters (Cai et al., 2024; Li et al.,\n2024d). For instance, self-speculation involves\ntraining a small Transformer head on top of the\ntarget model’s large Transformer block to gener-\nate multiple candidate tokens, which are then used\nas a draft for the target model (Li et al., 2024c).\nWhile showing a faster drafting speed compared\nto using an external drafter, training a separate\ndrafter for each target model is computationally ex-\npensive. More importantly, to handle distribution\nshift—i.e., to process novel inputs unseen during\ntraining—the drafter should be trained on a large\ncorpus, which is particularly challenging because\nmodifying only the last layer, a common practice,\nstill requires forwarding all lower layers of the large\ntarget model during training, leading to significant\ncomputational overhead.\nThis raises a key question: How can we develop\nan external drafter to have cross-model adaptabil-\nity while also avoiding the limitation of the Trans-\nformer’s quadratic computation for fast drafting?\nThis naturally leads us to explore non-quadratic se-\nquential models such as state-space models (SSMs)\n(Gu et al., 2022b), as external drafters. SSMs lever-\nage a linear recurrence structure with a fixed state\nsize, ensuring per-token computation and memory\n1arXiv:2506.01206v1  [cs.CL]  1 Jun 2025\n--- Page 2 ---\n(a) Encoding Memory\n (b) Decoding Time\n (c) Decoding Memory\nFigure 1: Comparison of drafting time & peak memory usage during encoding and decoding. Mamba drafter\nmaintains nearly constant decoding speed and memory usage, whereas both EAGLE, which employs self-speculation\nwith a single-layer Transformer within the large target model, and the Mistral-based external drafter, which also\nutilizes a Transformer, exhibit substantially higher memory requirements as the context length increases. Here, the\ntarget model is Mistral-7B, and we consider a 160M-sized Mistral and a 130M-sized Mamba. Measurements are\ntaken on an NVIDIA H100.\ncomplexity remain constant during inference, mak-\ning them more effective drafters than Transformers.\nSpecifically, we use Mamba (Gu and Dao, 2023), a\nstate-of-the-art SSM, as a drafter in SD and make\nthe following key observations:\n•Mamba is an efficient external drafter, show-\ning comparable results with self-speculation :\nMamba’s drafting latency is comparable to\nself-speculation, with both latency and mem-\nory usage remaining low even for significantly\nlonger input contexts, in contrast to alternative\nexternal drafters.\n•A smaller Mamba can often be more effec-\ntive than a larger Transformer as an exter-\nnal drafter : Despite its size, a small Mamba\nmodel achieves a comparable acceptance\nlength to larger Transformers, with higher\noverall throughput due to its fast drafting.\nTo further leverage Mamba’s efficiency, we pro-\npose simple yet effective tree decoding strategies\nwith Mamba drafters by formulating decoding as a\nmulti-armed bandit (MAB) (Slivkins et al., 2019)\nproblem. Specifically, we introduce a test-time tree\nsearch algorithm that dynamically optimizes the\ndraft tree structure based on the input query. With\nMamba’s low latency, we observe that it benefits\nfrom trees of varying widths and lengths (see Ta-\nble 5). By framing the selection of the optimal\ntree structure as an MAB problem, we enable sta-\nble and adaptive adjustments of the drafting tree to\naccommodate different query types.\nWe conduct a comprehensive set of experiments\nto evaluate our Mamba-based approach, focusingon practical SD scenarios across a wide range of\ntasks (Narayan et al., 2018; Zheng et al., 2023;\nChen et al., 2021). Our results demonstrate that\nMamba-based drafting can significantly outper-\nform traditional Transformer-based approaches.\nFor example, our approach surpasses their through-\nput by 2x while having similar acceptance length.\nFurthermore, our approach achieves throughput\ncomparable to EAGLE (Li et al., 2024d), a recent\nsingle-layer Transformer drafter designed for a spe-\ncific target model, in long-context scenarios while\nconsuming up to 20 GB less memory. This is no-\ntable as our target-agnostic Mamba drafter works\nwith an arbitrary target model without re-training,\nwhereas EAGLE, a self-speculation method, re-\nquires re-training of the drafter whenever the target\nmodel is updated. Moreover, advances in SSMs,\ne.g., Mamba-2 (Dao and Gu, 2024), directly benefit\nour approach, further enhancing the advantages of\nusing an effective, target-agnostic drafter.\n2 Related Work\nState-space models (SSMs). State-space models\nare strong linear models that combine the classi-\ncal state-space representation (Kalman, 1960) with\nrecurrent networks (Elman, 1990). In contrast\nto models with quadratic scaling, such as Trans-\nformers (Vaswani et al., 2017), which use self-\nattention and experience increasing computational\ncosts with sequence length, SSMs leverage linear\nrecurrence (Gu et al., 2022b,a; Mehta et al., 2023),\nenabling more efficient training and inference. This\nefficiency enables SSMs to excel, particularly in\nprocessing long sequences (Tay et al., 2021).\n2\n--- Page 3 ---\nRecent advancements, such as Mamba (Gu and\nDao, 2023), leverage hardware-aware algorithms\nand selection mechanisms to enhance SSMs fur-\nther. These developments have enabled SSMs to\ndemonstrate effectiveness in complex tasks across\ndiverse domains, including language, audio, and\nvideo (Zhu et al., 2024; Li et al., 2024a,b). Build-\ning on this foundation, our research aims to utilize\nMamba’s efficiency for drafting, enabling faster\nand more effective speculative decoding.\nSpeculative decoding. Speculative decoding fol-\nlows a draft-and-verify framework (Stern et al.,\n2018), where a smaller drafter generates candidate\ntokens that are verified by the target model. This\nmethod accelerates generation by increasing par-\nallelism while ensuring alignment with the target\nmodel’s distribution. Later advancements extended\nthis approach to sampling settings (Leviathan et al.,\n2023; Chen et al., 2023) and incorporated various\noptimization techniques to improve efficiency.\nSpeculative decoding approaches can be broadly\ncategorized into two types. One approach utilizes\nexternal drafter models (Leviathan et al., 2023),\nwhich provide high flexibility, allowing a single\ndrafter to be directly used for different target mod-\nels. On the other hand, self-speculation (Cai et al.,\n2024; Li et al., 2024d) takes a different approach\nby utilizing a very small model that uses the target\nmodel’s internal hidden states for drafting. While\nbeing faster, they require expensive re-training of\nthe drafter for every target model, showing lim-\nited flexibility. In this work, we demonstrate that\nMamba can get the best of both worlds, being an\nexternal drafter with very fast drafting speed.\nTo further improve the acceptance probability\nof the drafts, recent works move from sequential\ndrafting to tree-structured drafting (Miao et al.,\n2024; Yang et al., 2024; Li et al., 2024c), which\nallows verifying multiple draft candidates in paral-\nlel. Additionally, researchers have explored non-\nTransformer drafters (He et al., 2023; Fu et al.,\n2024). While Wang et al. (2024) introduced hard-\nware optimizations for applying SSMs to specu-\nlative decoding, we explored the effectiveness on\nTransformer-based target models (which is a de\nfacto architecture) and developed an efficient infer-\nence scheme (i.e., tree-drafting) for Mamba. Fur-\nthermore, we present a more thorough comparison\nand analysis across different drafter models.3Why Mamba for Speculative Decoding?\nIn this section, we demonstrate that Mamba can\nserve as powerful external drafters for speculative\ndecoding (SD). We examine this in terms of both ef-\nficiency and effectiveness. For efficiency , we com-\npare latency and peak memory usage during draft-\ning, which includes encoding (prefill), initial for-\nwarding of the given input sequence, and per-token\ngeneration during decoding (in Figure 1). For effec-\ntiveness , we report throughput, the average number\nof tokens per unit time, and acceptance length, the\naverage number of tokens accepted per forward\npass of the target model (in Figures 2 and 3).\nPreliminaries: Speculative decoding. Given a\ntarget model Mp, an efficient drafter Mqand an\ninput sequence xprefix, SD starts by drafter Mq\ngenerates candidate tokens with a length of γto-\nkens, denoted as ˜x1,˜x2, ...,˜xγ, where each are\nsampled from drafter distribution, ˜xi∼qi(x|\n˜x1, ...,˜xi−1, xprefix). After drafting, candidate to-\nkens along with xprefix are passed to the target\nmodel Mqin parallel and corresponding target dis-\ntribution pi(x|˜x1, ...,˜xi−1, xprefix)and tail distri-\nbution pγ+1is obtained. Finally, each candidate\ntoken ˜xiis verified by criterion sequentially from\ni= 1toγ. Here, the criterion is determined based\nonpiandqi. Once a token is rejected at the posi-\ntion of i, a new token is sampled from the adjusted\ndistribution p′\ni. If all candidates are accepted, an\nextra token is sampled from pγ+1.\n3.1 Efficiency of Mamba as a drafter\nTo evaluate the efficiency of Mamba as the drafter,\nwe compare it against two baselines: an external\nTransformer drafter and a self-speculation drafter.\nSpecifically, we use Mistral-7B (Jiang et al., 2023)\nas the target model and Mistral-160M, a smaller\nmodel with the same architecture, as the external\nTransformer drafter. For self-speculation, we em-\nploy EAGLE (Li et al., 2024d), while Mamba-\n130M serves as the Mamba-based drafter. All\ndrafters are trained from scratch, with additional\ntraining details provided in Appendix A.3. All\nmodels are instruction-tuned.\nAs Figure 1a illustrates, Mamba offers signifi-\ncant drafting efficiency compared to both baselines.\nFor example, as the input length increases, prefill\nmemory for both Mistral and EAGLE grow nearly\nquadratically, while Mamba maintains memory us-\nage with its efficient selectivity algorithm (Gu and\nDao, 2023). Furthermore, as in Figure 1b, Mamba\n3\n--- Page 4 ---\nFigure 2: Comparison of draft efficiency on GSM-8K.\nMamba drafters achieve substantially higher throughput\nthan a Transformer drafter due to their faster drafting\nspeed and favorable acceptance length. SD is run with a\ntemperature of 1.0 and a draft length of 5.\nexhibits significantly lower decoding latency than\nMistral of similar size, and is even comparable to\nEAGLE. Lastly, Figure 1c shows that Mamba’s\nuse of a single state enables it to maintain constant\nmemory usage independent of the input length. In\ncontrast, other drafters require KV cache for decod-\ning, causing the cache size to grow linearly with\nthe input length, leading to high memory overhead.\nThese results demonstrate that Mamba drafters can\nmake SD more adaptable to varying input lengths\nmore effectively than Transformer drafters.\n3.2 Effectiveness of Mamba as a drafter\nIn this section, we show that Mamba is an effec-\ntive drafter for SD. For effective SD, there is an\nimportant trade-off between the drafter’s speed and\nsize, i.e., a larger drafter may achieve a higher ac-\nceptance length than smaller models by generating\ncandidate tokens that are better aligned with the\ntarget model’s distribution but increase the latency\ndue to the larger size. In this context, we observe\nthat a small Mamba model can be a more effec-\ntive drafter than a Transformer and, depending on\nthe task, even larger Mamba models. As shown in\nFigure 2, the smallest Mamba achieves the highest\nthroughput among all drafters due to its fast draft-\ning and reasonable acceptance length. Notably, the\nsmall Mamba achieves a higher acceptance length\nthan the Transformer of similar size. This can be at-\ntributed to its better alignment with the distribution\nof the larger Transformer model, as illustrated in\nFigure 3. Most interestingly, we found that smaller\nMamba can be a stronger drafter than larger Mam-\nbas, as smaller Mamba shows significant drafting\nspeed compared to larger sizes, generating more\nFigure 3: Comparison of draft model calibration.\nReliability diagrams show that a small Mamba drafter\naligns better with the target model, Pythia-6.9B, than the\nTransformer drafter, Pythia-160M, achieving a lower\nexpected calibration error (ECE) on the XSum dataset.\ncandidates with a slightly lower acceptance rate\nthan larger models. We believe this highlights the\nexceptional drafting speed of Mamba, where such a\nphenomenon is not observed in Transformer-based\ndrafters (see Table 1a). To leverage this fast draft-\ning speed further, we suggest a tree search algo-\nrithm to generate a better candidate and improve\nthe acceptance length.\n4 Tree-Structured Drafting with Mamba\nIn this section, we introduce an effective draft-\ning strategy for Mamba drafters by using tree-\nstructured decoding , i.e., hierarchically expanding\nmultiple candidate nodes at each step instead of\nsequentially generating tokens. Specifically, we\nsuggest an efficient way to implement tree search\nfor Mamba decoding (in Section 4.1) and introduce\na test-time tree searching algorithm to adaptively\noptimize draft tree structure (in Section 4.2).\n4.1 Tree-structured drafting with Mamba\nTo improve the effectiveness of the drafter Mq,\nprevious approaches (Yang et al., 2024; Li et al.,\n2024d) sample multiple candidates from qiat each\ndrafting step iby constructing a draft tree. Espe-\ncially in Transformer drafter, this process is acceler-\nated by tree attention (Miao et al., 2024), a special-\nized attention algorithm that represents the causal\nrelationships between all tokens in the tree, thereby\neliminating overlapped token forwarding, e.g., in-\nput sequence xprefix. Here, we suggest an efficient\ntree-structured drafting specialized for Mamba.\nEfficient tree-structured drafting with batch\ngeneration. We demonstrate that Mamba can per-\nform tree-structured drafting efficiently by using\n4\n--- Page 5 ---\nbatch generation. Specifically, as Mamba only re-\nquires the current state to predict the next token\n(as it is a recurrent network), generating multiple\nnodes from the current node only requires copying\nthe current state and then performing sampling. In\ncontrast, Transformers are required to copy the cur-\nrent sequence length of the KV cache to predict the\nnext token, and this overhead grows with the se-\nquence length, making it crucial to eliminate such\nduplication using tree attention.\nFormally, given a tree configuration T=\n(N1, N2, ..., N γ), where γis the draft length, and\nNican be understood as the number of new nodes\nobtained by sampling from each node at the ith\ngeneration, one can view tree-structured drafting\nas a batch generation of a total batch size Bi=\nN1×N2× ··· × Ni.\nEfficient cache utilization for batch generation.\nWhile efficient, batch generation indeed increases\nthe computation complexity by a factor of Biper\ngeneration, compared to sequential drafting, i.e.,\nN1=...=Nγ= 1. To alleviate overheads from\nthe batch size Bduring tree-structured drafting for\nMamba, we propose a batch-wise cache implemen-\ntation for Mamba. Specifically, given a tree config-\nuration T, we determine the possible batch sizes\nB1, ...,Bγfor each drafting positions. Using these\ncalculated sizes, we create a state cache per each\nbatch size and allocate memory in advance, prevent-\ning the memory re-allocation during duplication.\nNext, we leverage a graph cache (Nguyen et al.,\n2021) to accelerate the GPU computation flow for\neach batch size. This cache stores the graph struc-\nture of intermediate computations for each batch\nsize, enabling efficient reuse of the computational\ngraph across multiple executions. The reason this\nis feasible is that Mamba receives a fixed size of in-\nput(B1,1),(B2,1), ...,(Bγ,1), owing to its linear\nrecurrence structure.\n4.2 Test-time dynamic tree search using\nmulti-armed bandit\nWe now present a way to systematically allocate\nthe given budget to find the effective tree config-\nuration T, based on the observation that Mamba\nbenefits from different tree configurations across\ntasks (see Section 5.5 for details). To this end, we\nsuggest to formalize the tree configuration search\nproblem as a multi-armed bandit (MAB) problem\nand dynamically optimize the tree configuration at\ninference time.Decoding as multi-armed bandit. Following a\nprevious work (Kim et al., 2024b), we define each\ndrafting and verification step as a round in the multi-\narmed bandit (MAB) framework. Specifically, in\neach round t, drafter Mqfollows an policy πthat\nchoose kthtree configuration T(t)\nkfrom the pre-\ndefined tree configuration set S={T1,T2, ...,TK}.\nThen it performs γgenerations and obtains a re-\nward r(t), e.g., the number of accepted tokens. The\ngoal of the MAB problem is to design an opti-\nmal policy π∗that maximizes the expected cumula-\ntive reward EhPT\nt=1r(t)i\nover a total of Trounds,\nwhere Tis determined by the completion of gener-\nation for a given query.\nOptimization. To balance exploration and stable\nconvergence in MAB, we utilize the UCB algo-\nrithm (Auer, 2002) as our policy π. It chooses tree\nconfiguration T(t)\nk∗at round tas follows:\nk∗= arg max\nk∈{1,..,K}ˆr(t)\nk+λUCBs\n2 lnt\nn(t)\nk, (1)\nwhere ˆr(t)\nkis a cumulative reward mean, i.e.,Pt\nt=1r(t)\nkandn(t)\nkis the count numbers of kthcon-\nfiguration is selected up to round t. For reward r(t)\nk,\nwe define it as follows:\nr(t)\nk:=− \n1\nNaccept+λγγ(Tk)\nNaccept!\n·I, (2)\nwhere Iis an indicator function, which is 1when\nthekthconfiguration is selected and 0otherwise,\nandNaccept is the number of accepted tokens at\nround t. Especially, γ(T(t)\nk)represent draft length\nof selected tree T(t)\nkto penalize increase of draft\ntimes, as γis increase. We notice this reward di-\nrectly originated from the SD speedup objective\n(see Appendix A.4 for more details).\n5 Experiments\nIn this section, we present a comprehensive evalua-\ntion of our proposed Mamba drafter framework.\nBaselines. We use a wide range of Transformer-\nbased drafters as baselines, applying tree drafting\nfor a fair comparison with our Mamba drafter. Ad-\nditionally, we consider EAGLE (Li et al., 2024d),\na recent single-layer Transformer drafter that lever-\nages tree drafting and is directly trained to align\nwith the target model, as a baseline for self-\nspeculation methods.\n5\n--- Page 6 ---\nTable 1: Comparison with Transformer-based external drafters and self-speculation. We evaluate SD using\n(a) pre-trained and (b) instruction-tuned models with both greedy decoding (temperature = 0) and sampling\n(temperature = 1). All drafters leverage tree-structured drafting and our method additionally uses the proposed tree\nsearch algorithm. Throughput is reported along with the acceptance length shown in parentheses. The best results\nare shown in bold.\n(a) Pre-trained model\nDrafter Greedy (Temp=0) Sampling (Temp=1)\nTarget Method Size XSum CNN-DM GSM-8k XSum CNN-DM GSM-8k\nPythia-6.9BNo drafter − 53.30 49.29 54.69 52.51 45.33 53.81\nPythia70M47.31 46.99 57.36 41.86 45.30 47.96\n(1.52) (1.54) (1.68) (1.67) (1.76) (1.77)\n160M50.05 49.53 67.89 46.67 47.17 55.40\n(2.23) (2.26) (2.72) (2.28) (2.30) (2.63)\n410M70.53 70.08 75.97 53.50 56.64 63.64\n(4.62) (4.73) (4.64) (3.60) (3.80) (4.01)\n138.80 131.97 149.46 108.68 105.01 119.67Ours 130M(4.55) (4.38) (4.57) (3.53) (3.53) (3.73)\nMistral-7BNo drafter − 51.15 49.55 50.31 53.49 47.40 52.92\nMistral 160M61.55 61.04 49.38 53.91 50.50 62.29\n(3.13) (3.05) (2.21) (2.74) (2.68) (2.94)\n76.71 65.23 77.50 79.18 70.95 82.63Ours 130M(2.39) (2.13) (2.25) (2.73) (2.65) (2.73)\n(b) Instruction-tuned model\nDrafter Greedy (Temp=0) Sampling (Temp=1)\nTarget Method External? MT-bench Alpaca Human-Eval MT-bench Alpaca Human-Eval\nPythia-6.9BNo drafter − 54.51 55.28 54.76 53.89 54.72 54.21\nPythia ✓70.71 60.77 109.51 65.73 62.07 109.52\n(3.10) (2.65) (4.68) (3.03) (2.82) (4.25)\nEAGLE ✗125.61 117.17 122.44 87.01 78.58 83.05\n(3.85) (3.53) (4.71) (2.67) (2.40) (2.97)\n128.21 114.08 172.38 110.20 108.54 143.55Ours ✓(3.91) (3.41) (5.41) (3.65) (3.51) (4.82)\nMistral-7BNo drafter − 52.97 53.58 52.30 52.39 53.02 52.34\nMistral ✓67.47 61.40 100.23 57.19 51.05 80.94\n(3.04) (2.73) (4.53) (2.84) (2.40) (3.92)\nEAGLE ✗107.16 94.03 132.69 94.03 86.60 122.11\n(3.22) (2.79) (3.98) (2.90) (2.63) (3.78)\n102.48 96.83 118.04 88.68 82.75 87.81Ours ✓(3.16) (2.96) (3.69) (2.95) (2.71) (2.94)\nEvaluation metrics. To compare the gains in\ndecoding acceleration from SD across different\ndrafter types, we focus on throughput, which is\nthe number of tokens generated per second during\ninference, as a measure of overall inference speed.\nFor a more comprehensive evaluation of effective-\nness, we also report the average acceptance length,\nwhich indicates the average number of tokens ac-\ncepted per forward pass of the target model.\n5.1 Comparison with transformer drafters\nFirst, we compare our Mamba drafter with Trans-\nformer external drafters.Pre-trained models. We evaluate the perfor-\nmance of the pre-trained Mamba drafter across\nvarious language modeling tasks. Specifically, we\nconsider XSum (Narayan et al., 2018) and CNN-\nDailyMail (Hermann et al., 2015) for general lan-\nguage modeling tasks, as well as GSM-8K (Cobbe\net al., 2021) for mathematical language modeling.\nAs summarized in Table 1a, while larger drafters\nlike Pythia-410M achieve slightly better through-\nput gains on some datasets due to increased ac-\nceptance length, small Transformer drafters (e.g.,\nPythia-70M) show minimal improvement over the\n6\n--- Page 7 ---\nTable 2: Comparisons on LongBench. Throughput (tokens/s) is the primary metric, with acceptance length shown\nin parentheses. We also report peak memory, calculated by summing the memory consumption of both the target\nand drafter models during the prefill phase. Bold indicates the best result, while the runner-up is underlined.\nSingle-Document QA Multi-Document QA Peak Memory (GB)\nMethod External? 1k 2k 4k 8k 1k 2k 4k 8k 1k 2k 4k 8k\nNo drafter - 31.02 27.89 24.35 19.30 28.17 24.22 19.01 14.83 15 16 20 36\nMistral ✓25.30 23.28 19.48 15.23 24.64 21.06 16.49 12.1831 33 38 59(2.43) (2.37) (2.24) (2.21) (2.53) (2.48) (2.44) (2.39)\nEAGLE ✗53.13 47.00 37.27 26.12 42.48 35.38 25.10 17.3632 34 42 72(2.73) (2.81) (2.76) (2.71) (2.60) (2.61) (2.68) (2.64)\n55.09 45.65 36.32 24.92 47.91 36.40 26.27 17.77Ours ✓(2.91) (2.77) (2.77) (2.80) (2.94) (2.86) (2.88) (2.87)31 32 36 52\nvanilla autoregressive baseline without SD, with\nonly marginal benefits in the sampling case. In\ncontrast, Mamba significantly improves through-\nput across datasets and temperature settings. For\ninstance, in GSM-8K, Mamba achieves nearly 2x\nthe throughput of Pythia-410M on sampling setup.\nNotably, even when the Mamba drafter has a lower\nacceptance length than Transformer drafters, it still\noutperforms them due to its fast drafting speed.\nInstruction-tuned models. Next, we evaluate\nthe Mamba drafter in instruction-following sce-\nnarios using MT-bench (Zheng et al., 2023) for\nmulti-turn dialogues, Alpaca (Taori et al., 2023)\nfor general instruction-following tasks, and Hu-\nmanEval (Chen et al., 2021) for code generation.\nAs shown in Table 1b, the Mamba drafter outper-\nforms Transformer drafters across all instruction-\nfollowing datasets. For example, in HumanEval,\nwhile Pythia and Mistral drafters improve through-\nput over the vanilla autoregressive baseline by\n54.75 and 47.93 for their respective target mod-\nels, Mamba achieves even more significant gains of\n117.62 and 65.74 for these models. These results\nhighlight Mamba’s flexibility and superior general-\nization to diverse instruction-following tasks com-\npared to Transformer drafters.\n5.2 Comparison with self-speculation\nWe demonstrate that the Mamba drafter, which\nis an external drafter, can achieve competitive\nthroughput even against recent approaches that\ntrain drafters with direct access to target models.\nSpecifically, we consider EAGLE, which uses a\nsingle-layer Transformer drafter trained to generate\ntokens from the target model’s last hidden states\nfor better alignment with the target model. Ta-\nble 1b reports the results on instruction-followingtasks, where our Mamba drafter achieves through-\nput gains comparable to EAGLE across the datasets\nand target models. Notably, on MT-bench, the\nMamba drafter achieves a throughput of 125.61,\nwhich is comparable to EAGLE’s 128.21 for Pythia-\n6.9B. These results highlight not only Mamba’s fast\ndrafting speed (see Figure 1b), but also its effec-\ntiveness in achieving comparable acceptance length\nwithout requiring access to target models.\n5.3 Long-context scenarios\nTo evaluate Mamba’s scalability in long-context\nscenarios, we conduct SD experiments on Long-\nBench (Bai et al., 2023) using input lengths rang-\ning from 1k to 8k, with all drafters trained with\nthe same context limit. *Additionally, we apply\nYaRN (Peng et al., 2023) to extend the context\nlimit for both Transformer drafters and EAGLE.\nAs shown in Table 2, which presents the results\non LongBench, Mamba maintains a higher accep-\ntance length on longer inputs compared to both\nTransformer drafters and EAGLE, even when the\nlatter utilizes YaRN to extend the context length.\nIn the Single-Document QA task, as the input\nlength increases from 1k to 8k, Mistral’s accep-\ntance length decreases from 2.43 to 2.21, while\nMamba remains more stable, changing from 2.91\nto 2.80. This stability reflects Mamba’s ability to\nextrapolate effectively via recurrence (Gu and Dao,\n2023). Moreover, Mamba generalizes well to un-\nseen complex distributions. In the Multi-Document\nQA task (which is a complex problem compared\nto single-document QA as the answer is located\nacross multiple documents), Mamba consistently\nachieves throughput gains comparable to EAGLE.\nHere, we notice that the gains are obtainable more\n*Following EAGLE, we fine-tune pre-trained Mamba and\nMistral on ShareGPT with a context limit of 2k.\n7\n--- Page 8 ---\nTable 3: Cross-target model performance on MT-\nbench. Experiments are run with a temperature of 0 and\nsequential drafting with a length of 5.\nMethod Setup Accept length Throughput\nEAGLEPythia→Pythia 2.59 94.75\nMistral →Pythia N/A N/A\nPythia→Pythia 3.08 112.69OursMistral →Pythia 2.45 93.20\nTable 4: Tree-structured drafting on MT-bench. Ex-\nperiments are run with a temperature of 0 and a fixed\ntree configuration of (3,2,2,1,1). Tree drafting yields\nnotable improvements in all performance metrics.\nTree? Accept length Latency Throughput\n✗ 3.08 6.62 112.69\n✓ 3.91 8.30 127.37\nefficiently: when applying to 8k, Mamba only con-\nsumes memory up to 52GB, compared to EAGLE,\nwhich reaches up to 72GB (with both including the\nmemory needed for drafter and target verification).\n5.4 Cross-target model performance\nUsing an external drafter enables plug-and-play\nintegration with new target models without the\nneed to re-train the drafter for each specific model.\nTo evaluate Mamba’s performance as an exter-\nnal drafter, we use the Mamba drafter with a tar-\nget model that the drafter has not been explicitly\ntrained to align with. Specifically, we use the\ninstruction-tuned variant of Pythia-6.9B as the tar-\nget model and Mamba trained with the Mistral-7B\ntokenizer. As shown in Table 3, even without ex-\nplicit training, Mamba achieves throughput compa-\nrable to EAGLE, which is specifically trained for\nthe target model. This highlights Mamba’s flexibil-\nity as an external drafter, enabling efficient deploy-\nment without the need for costly re-training.\n5.5 Ablations and analysis\nWe further evaluate the contributions of individ-\nual components in our framework to the gains in\ndecoding acceleration. Here we mainly consider\nthroughput (tokens/sec) as the performance metric.\nTree-structured drafting. In Table 4, we show\nthe impact of tree-structured drafting on perfor-\nmance. Our approach improves acceptance length\nwith minimal drafting latency overhead, resulting\nin higher throughput gains. This effect is similar to\nthe tree-attention mechanism used in Transformer-\nbased drafters. These results demonstrate that ourTable 5: Throughput by tree configuration. Through-\nput (tokens/s) for different tree configurations given as\n(N1, ..., N γ), where Niindicates the number of samples\natithgeneration in drafting step.\nMethod (3,3,2,1) (3,2,2,1,1) (2,2,2,1,1,1)\nPythia 75.38 70.71 63.75\nOurs 124.99 127.37 124.37\nTable 6: Effects of test-time tree search on through-\nput. Experiments are run with a temperature of 0.\nSearch? MT-bench Alpaca HumanEval Avg.\n✗ 124.99 116.12 149.15 130.09\n✓ 128.21 114.08 172.38 138.22\nbatch generation enables Mamba to benefit from\ntree-structured drafting, which has not been ex-\nplored in the field to the best of our knowledge.\nTree configurations. Table 5 shows that Mamba\nmaintains stable throughput across diverse config-\nurations, whereas Transformer drafters exhibit a\ndecline as tree length increases. This is due to\nMamba’s very fast drafting, which effectively miti-\ngates the overhead of using longer trees.\nTest-time tree search. We further analyze the\nimpact of the test-time tree search algorithm.\nSpecifically, we use as tree candidates (3,3,2,1),\n(3,2,2,1,1), and (2,2,2,1,1,1)and compare\nthem with naive tree-structured drafting that uti-\nlizes a fixed tree configuration, i.e., (3,2,2,1). As\nshown in Table 6, our multi-armed bandit (MAB)-\nbased algorithm often improves throughput signifi-\ncantly on several datasets (e.g., HumanEval) com-\npared to the naive approach.\n6 Conclusion\nIn this work, we present Mamba-based drafters\nas an effective solution to the challenges of ex-\nisting speculative decoding methods. Leveraging\nthe linear structure of state-space models, Mamba\nsignificantly improves drafting speed and mem-\nory efficiency. To further enhance drafting quality,\nwe introduce a novel tree-based search algorithm.\nOur experimental results show that Mamba-based\ndrafters not only outperform existing external draft-\ning techniques but also match the performance of\nadvanced self-speculation approaches, particularly\nin long-context scenarios.\n8\n--- Page 9 ---\nLimitations\nWhile our Mamba-based drafting approach demon-\nstrates significant improvements in inference speed,\nthere are several opportunities for further enhance-\nment. Although Mamba-based drafters require\nmemory for hidden state maintenance, this over-\nhead could potentially be optimized through effi-\ncient memory management techniques. While our\ntree-based search method’s performance depends\non hyperparameter settings, this flexibility allows\nfor customization across different use cases, and\nfuture work could develop adaptive optimization\nstrategies. Additionally, while beyond the scope\nof this work, investigating self-drafting capabilities\nwithin a single Mamba model—leveraging its effi-\ncient architecture to serve as both drafter and veri-\nfier—represents an intriguing direction for future\nresearch. These limitations and opportunities point\nto exciting directions that could further advance the\nefficiency of large language model inference.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nP Auer. 2002. Finite-time analysis of the multiarmed\nbandit problem.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2023. Longbench: A bilingual, mul-\ntitask benchmark for long context understanding.\nPreprint , arXiv:2308.14508.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence , volume 34,\npages 7432–7439.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\nJason D Lee, Deming Chen, and Tri Dao. 2024.\nMedusa: Simple llm inference acceleration frame-\nwork with multiple decoding heads. arXiv preprint\narXiv:2401.10774 .\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318 .\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde De Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question\nanswering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457v1 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168 .\nTri Dao and Albert Gu. 2024. Transformers are SSMs:\nGeneralized models and efficient algorithms through\nstructured state space duality. In International Con-\nference on Machine Learning (ICML) .\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science , 14(2):179–211.\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.\n2024. Break the sequential dependency of llm in-\nference using lookahead decoding. arXiv preprint\narXiv:2402.02057 .\nAlbert Gu and Tri Dao. 2023. Mamba: Linear-time\nsequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752 .\nAlbert Gu, Karan Goel, Ankit Gupta, and Christopher\nRé. 2022a. On the parameterization and initializa-\ntion of diagonal state space models. In Advances in\nNeural Information Processing Systems .\nAlbert Gu, Karan Goel, and Christopher Ré. 2022b. Ef-\nficiently modeling long sequences with structured\nstate spaces. In International Conference on Learn-\ning Representations .\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,\nand Di He. 2023. Rest: Retrieval-based speculative\ndecoding. arXiv preprint arXiv:2311.08252 .\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS , pages 1693–1701.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b.arXiv preprint arXiv:2310.06825 .\nRudolph Emil Kalman. 1960. A new approach to linear\nfiltering and prediction problems.\nAlex Kim, Maximilian Muhn, and Valeri Nikolaev.\n2024a. Financial statement analysis with large lan-\nguage models. arXiv preprint arXiv:2407.17866 .\n9\n--- Page 10 ---\nTaehyeon Kim, Hojung Jung, and Se-Young Yun. 2024b.\nA unified framework for speculative decoding with\nmultiple drafters as a bandit. In Proceedings of the\nFourth Workshop on Efficient Natural Language and\nSpeech Processing (ENLSP-IV): Highlighting New\nArchitectures for Future Foundation Models .\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning , pages 19274–19286. PMLR.\nKunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali\nWang, Limin Wang, and Yu Qiao. 2024a. Video-\nmamba: State space model for efficient video under-\nstanding. arXiv preprint arXiv:2403.06977 .\nShufan Li, Harkanwar Singh, and Aditya Grover. 2024b.\nMamba-nd: Selective state space modeling for multi-\ndimensional data. arXiv preprint arXiv:2402.05892 .\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024c. Eagle-2: Faster inference of language\nmodels with dynamic draft trees. arXiv preprint\narXiv:2406.16858 .\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024d. Eagle: Speculative sampling re-\nquires rethinking feature uncertainty. arXiv preprint\narXiv:2401.15077 .\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and\nBehnam Neyshabur. 2023. Long range language\nmodeling via gated state spaces. In International\nConference on Learning Representations .\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao\nCheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al.\n2024. Specinfer: Accelerating large language model\nserving with tree-based speculative inference and\nverification. In Proceedings of the 29th ACM Interna-\ntional Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems, Vol-\nume 3 , pages 932–949.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. ArXiv , abs/1808.08745.\nVinh Nguyen, Michael Carilli, Sukru Burc Eryilmaz,\nVartika Singh, Michelle Lin, Natalia Gimelshein, Al-\nban Desmaison, and Edward Yang. 2021. Accelerat-\ning pytorch with cuda graphs. Accessed: 2025-01-\n27.\nGuilherme Penedo, Hynek Kydlí ˇcek, Anton Lozhkov,\nMargaret Mitchell, Colin Raffel, Leandro V on Werra,\nThomas Wolf, et al. 2024. The fineweb datasets:\nDecanting the web for the finest text data at scale.\narXiv preprint arXiv:2406.17557 .\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context window\nextension of large language models. arXiv preprint\narXiv:2309.00071 .Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, et al.\n2025. Toward expert-level medical question answer-\ning with large language models. Nature Medicine ,\npages 1–8.\nAleksandrs Slivkins et al. 2019. Introduction to multi-\narmed bandits. Foundations and Trends ®in Machine\nLearning , 12(1-2):1–286.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. Advances in Neural Information\nProcessing Systems , 31.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena: A benchmark for efficient transformers.\nInInternational Conference on Learning Representa-\ntions .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems .\nJunxiong Wang, Daniele Paliotta, Avner May, Alexan-\nder M Rush, and Tri Dao. 2024. The mamba in\nthe llama: Distilling and accelerating hybrid models.\narXiv preprint arXiv:2408.15237 .\nHeming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu\nWei, and Zhifang Sui. 2023. Speculative decod-\ning: Exploiting speculative execution for accelerating\nseq2seq generation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023 , pages\n3909–3925.\nSen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen.\n2024. Multi-candidate speculative decoding. arXiv\npreprint arXiv:2401.06706 .\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830 .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\nPreprint , arXiv:2306.05685.\nLianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong\nWang, Wenyu Liu, and Xinggang Wang. 2024. Vi-\nsion mamba: Efficient visual representation learning\nwith bidirectional state space model. arXiv preprint\narXiv:2401.09417 .\n10\n--- Page 11 ---\nA Experimental Details\nA.1 Datasets\nWe evaluate six benchmarks: three for pre-trained models and three for instruction-tuned models. These\ninclude XSum (Narayan et al., 2018) and CNN-DailyMail (Hermann et al., 2015) for general language\nmodeling, GSM-8K (Cobbe et al., 2021) for mathematical reasoning, MT-Bench (Zheng et al., 2023) for\nmulti-turn dialogues, Alpaca (Taori et al., 2023) for general instruction-following, and HumanEval (Chen\net al., 2021) for code generation. Following EAGLE (Li et al., 2024d), we subsample each dataset to\napproximately 80 samples.\nFor evaluating longer-context scenarios, we use six tasks from LongBench (Bai et al., 2023) for\ndocument-based question answering: three (NarrativeQA, Qasper, MultifieldQA-en) for Single-Document\nQA and three (HotpotQA, 2WikiMultihopQA, MuSiQue) for Multi-Document QA. While LongBench\noriginally includes Chinese-language tasks (MultifieldQA-zh, DuReader), we observe that the target\nmodel produced poor outputs on these tasks and therefore exclude them. For pre-processing, we first filter\nout data samples with input lengths exceeding 8k tokens and truncate them to specific input lengths, such\nas 1k, 2k, 4k, and 8k.\nA.2 Architectures\nPre-train models. For the pre-trained target model, we consider EleutherAI/pythia-6.9b and\nmistralai/Mistral-7B-v0.1. For external Transformer drafters, we use smaller models from the same\nfamily as the target model, specifically Pythia-70M, 160M, 410M and Mistral-160M. For Mamba drafters,\nwe use two versions of Mamba-130M: Mamba-Pythia-130M and Mamba-Mistral-130M, which share\ntokenizers with Pythia and Mistral, respectively. In cases where no official pre-trained checkpoints are\navailable, e.g., Mistral-160M and Mamba-Mistral, we pre-train them from scratch (see Appendix A.3 for\ndetails).\nInstruction-tuned models. For the instruction-tuned target model, we consider allenai/open-\ninstruct-pythia-6.9b-tulu and mistralai/Mistral-7B-Instruct-v0.1, which are instruction-tuned from\nEleutherAI/Pythia-6.9b and mistralai/Mistral-7B-v0.1, respectively. To obtain instruction-tuned drafters,\nwe supervised fine-tune (SFT) the pre-trained drafters on ShareGPT, following the training dataset used\nfor EAGLE (see Appendix A.3 for details). Additionally, we obtain corresponding EAGLE for Pythia and\nMistral by following their official released training code.\nA.3 Training Details.\nFollowing a common pre-training recipe†, we pre-train Mistral-160M and Mamba-Mistral-130M on\nFineWeb-Edu (Penedo et al., 2024) for 5,000 training steps using a batch size of 4,096 and a context limit\nof 2k. Next, we fine-tune (SFT) the pre-trained drafters on ShareGPT for 2 epochs with a batch size of\n128 and a context limit of 2k, following the standard SFT procedure‡. For validation, we use HellaSwag\n(Zellers et al., 2019), ARC-Easy (Clark et al., 2018), and PIQA (Bisk et al., 2020). We select the best\nmodel by testing various learning rates, specifically {2e-3, 2e-4, 2e-5}.\nA.4 Implementation\nTree-structured drafting. Following previous work (Yang et al., 2024), we implement tree-structured\ndrafting for the external Transformer drafter. We use a tree configuration with a depth of 5, i.e., (3,2,2,1,1),\nto align the draft length with EAGLE. For EAGLE, we directly follow its official tree-structured drafting\nimplementation.\nReward modeling for MAB. To obtain the reward function in Equation (2), we directly use the speed\nup formula from SD per drafting step. Given a target model and drafter’s decoding time, i.e., per-token\ngeneration time, as Ttarget andTdraft, the total time of SD TSD\ntotalper drafting step is as follows:\nTSD\ntotal=Ttarget(γ) +γ·Tdraft, (3)\n†https://github.com/facebookresearch/lingua\n‡https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/README.md\n11\n--- Page 12 ---\nwhere γis draft length, and Ttarget(γ)is verification time for forwarding γdraft tokens. Then, we compute\nSD’s decoding time TSD\nAvgby dividing the number of accepted tokens Naccept , i.e., TSD\nAvg=TSD\nTotal\nNaccept. Finally,\nthe speed up of SD per drafting step is as follows:\nspeedup =Ttarget\nTSD\nAvg=Naccept·Ttarget\nTtarget(γ) +γ·Tdraft(4)\nThen, the inverse of speedup is as follows:\n1\nspeedup=1\nNaccept·Ttarget(γ)\nTtarget+γ\nNaccept·Tdraft\nTtarget(5)\nGenerally, we can assumeTtarget(γ)\nTtarget≃1, asγis not larger value, andTdraft\nTtarget≃λγas it is constant during\ndrafting. Then, our reward function ris derived as follows:\nr=1\nNaccept+λγ·γ\nNaccept(6)\nThis formula originated from the inverse of speed up, so we need to minimize this function.\nA.5 Greedy Decoding and Sampling\nAlgorithm 1 outlines the verification process for draft token acceptance using two decoding strategies:\n1.Greedy decoding (red, lines 7, 15, 22): This method selects tokens deterministically by setting the\ntemperature to zero, effectively forcing the model to choose the most probable token at each step.\nThis is equivalent to using the one-hot version of the target model, pone-hot .\n2.Sampling-based approach (blue, lines 8, 16, 23): In contrast, this method introduces stochasticity\nby sampling tokens from the probability distribution given by the target model p. This allows for\nmore diverse outputs.\nThe verification algorithm works by comparing the probabilities assigned by the target model and the\ndraft model. The acceptance of a draft token ˜xtdepends on the ratio of these probabilities. If the draft\ntoken is rejected, a new token is sampled based on either the greedy or sampling-based approach.\nA.6 Computational Resources\nWe conduct most experiments on a single NVIDIA RTX 4090 24GB GPU, except for longer-context\nexperiments in Table 2, where we use a single NVIDIA H100 80GB GPU to efficiently handle input\nlengths of up to 8k tokens. For pre-training, we leverage 8 NVIDIA H200 141GB GPUs, which takes\napproximately one day. For instruction-tuning of external drafters and training EAGLE, we use 8 NVIDIA\nRTX 4090 24GB GPUs, requiring approximately two hours and one day, respectively. Here, we remark\nthat training EAGLE incurs additional computational cost, as it requires extracting hidden states from the\ntraining data via forward passes through the target model.\n12\n--- Page 13 ---\nAlgorithm 1 Verification Algorithm.\n1:Given target model p, one-hot version of target model pone-hot , and draft model q.\n2:Given input sequence xprefix, and draft sequence ˜xof length γ.\n3:fort= 1toγdo\n4: Sample ufrom a uniform distribution:\n5: u∼U[0,1]\n6: Get the probability of each model for the draft token ˜xt\n7: pt=pone-hot (˜xt|xprefix, x1, . . . , x t−1)\n8: pt=p(˜xt|xprefix, x1, . . . , x t−1)\n9: qt=q(˜xt|xprefix, x1, . . . , x t−1)\n10: ifu <min\u0010\n1,pt\nqt\u0011\nthen\n11: Accept the token ˜xt:\n12: xt←˜xt.\n13: else\n14: Reject the draft token ˜xtand sample a new one:\n15: xt∼pone-hot (x|xprefix, x1, . . . , x t−1).\n16: xt∼\u0000\np(x|xprefix, x1, . . . , x t−1)−q(x|xprefix, x1, . . . , x t−1)\u0001\n+.\n17: break\n18: end if\n19:end for\n20:ifall tokens are accepted then\n21: Sample an extra token xγ+1:\n22: xγ+1∼pone-hot (x|xprefix, x1, . . . , x γ).\n23: xγ+1∼p(x|xprefix, x1, . . . , x γ).\n24:end if\n25:Output the accepted token sequence x1, . . . , x n, where nis the accepted token length.\n13",
  "text_length": 49804
}