{
  "id": "http://arxiv.org/abs/2506.05167v1",
  "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
  "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\n\\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing\nretrieved documents based on evidentiality, ensuring whether answer generation\nis supported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
  "authors": [
    "Yeonseok Jeong",
    "Jinsu Kim",
    "Dohyeon Lee",
    "Seung-won Hwang"
  ],
  "published": "2025-06-05T15:43:49Z",
  "updated": "2025-06-05T15:43:49Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05167v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05167v1  [cs.CL]  5 Jun 2025ECoRAG: Evidentiality-guided Compression for Long Context RAG\nYeonseok Jeong1, Jinsu Kim2, Dohyeon Lee3, Seung-won Hwang3*\nIPAI, Seoul National University1, Korea University2, Seoul National University3\n{jys3136, waylight3, seungwonh}@snu.ac.kr\ntonmmy222@korea.ac.kr\nAbstract\nLarge Language Models (LLMs) have shown\nremarkable performance in Open-Domain\nQuestion Answering (ODQA) by leverag-\ning external documents through Retrieval-\nAugmented Generation (RAG). To reduce RAG\noverhead, from longer context, context com-\npression is necessary. However, prior com-\npression methods do not focus on filtering out\nnon-evidential information, which limit the\nperformance in LLM-based RAG. We thus\npropose Evidentiality-guided RAG, or EC-\noRAG framework. ECoRAG improves LLM\nperformance by compressing retrieved docu-\nments based on evidentiality, ensuring whether\nanswer generation is supported by the cor-\nrect evidence. As an additional step, EC-\noRAG reflects whether the compressed con-\ntent provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments\nshow that ECoRAG improves LLM perfor-\nmance on ODQA tasks, outperforming existing\ncompression methods. Furthermore, ECoRAG\nis highly cost-efficient, as it not only reduces\nlatency but also minimizes token usage by re-\ntaining only the necessary information to gen-\nerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.\n1 Introduction\nLLMs (OpenAI, 2023; Touvron et al., 2023) have\nexcelled in tasks such as ODQA by leveraging ex-\nternal knowledge through RAG (Lewis et al., 2020;\nRam et al., 2023). However, RAG inevitably in-\ncreases context length, which incurs higher com-\nputational cost and also hinders generation qual-\nity (Liu et al., 2024; Hsieh et al., 2024; Li et al.,\n2024).\nWhile adopting existing context compression (Li\net al., 2023) may look promising, such a baseline\npresents two main challenges. First, LLMs are\nknown to be vulnerable to irrelevant contents that\n*Corresponding Author\n36.037.038.039.040.041.042.043.044.0\n3 6 9 12Exact Match (EM)\nthe number of documents\nRaw Prepend RECOMP ECoRAG (ours)\n41.042.043.044.045.0Figure 1: Comparison of performance between prepend-\ning retrieved documents (standard RAG) (Karpukhin\net al., 2020), applying RECOMP (Xu et al., 2024),\nand applying ECoRAG on the Natural Questions\n(Kwiatkowski et al., 2019) test set. Experiments were\nconducted using Flan-UL2 (Tay et al., 2023).\ncannot provide evidence for answer generation (Shi\net al., 2023; Qian et al., 2024; Wu et al., 2024), and\nexisting compression methods (Xu et al., 2024;\nJiang et al., 2024; Yoon et al., 2024) do not effec-\ntively filter them out. As a result, a naive baseline\nsimply prepending retrieved documents, ‘standard\nRAG’ in Figure 1, outperforms a baseline compres-\nsor RECOMP (Xu et al., 2024). As the number of\ndocuments increases, a baseline compressor fails\nto filter out increasing irrelevant contents, causing\nperformance to decline.\nSecond, it is challenging to determine the desir-\nable compression ratio for each question. Failure\nto do so may lead to compressing too much, which\nresults in losing crucial information, or compress-\ning too little, which produces overly long contexts\nthat degrade generation quality (Liu et al., 2024;\n1\n--- Page 2 ---\nHsieh et al., 2024; Li et al., 2024) and increase com-\nputational costs. Thus, it is necessary to find the\ndesirable compression ratio that enables the LLM\nto generate the correct answer for each question.\nOur distinction is using evidentiality to ad-\ndress both challenges and proposing Evidentiality-\nguided Compression and Retrieval- Augmented\nGeneration ( ECoRAG ) framework: Ours com-\npresses retrieved documents to retain only the in-\nformation necessary to support the answer. To\novercome the first challenge, evidentiality (Lee\net al., 2021; Asai et al., 2022) is used to determine\nwhether each sentence in the retrieved documents\nsupports the correct answer to a question. It can\nbe quantified for each sentence by measuring how\nmuch it contributes to the model to generate the\ncorrect answer. We train the compressor using this\nas training signals.\nTo address the second challenge, ECoRAG re-\nflects on compression as a collective, where it con-\ntains sufficient evidence. We begin by forming the\nsmallest possible collective unit of compression\nand assess whether it is evidential. If not, it means\nthat it is compressed too much, which we adjust\nadaptively by collecting more, until it is sufficient.\nThrough this reflection process, ECoRAG finds the\ndesirable compression ratio that enables the LLM\nto generate the correct answer with minimal tokens.\nBy applying these methods, ECoRAG has two\nadvantages when dealing with long contexts as\nthe number of documents increases. First, EC-\noRAG improves performance by retaining only the\ninformation necessary for generating the correct\nanswer and removing distracting content. This re-\nsults in gains on ODQA datasets such as Natural\nQuestions (NQ) (Kwiatkowski et al., 2019), Triv-\niaQA (TQA) (Joshi et al., 2017), WebQuestions\n(WQ) (Berant et al., 2013). Second, by compress-\ning the long context to only what is needed, it re-\nduces computational costs.\nOur contributions to this work can be summa-\nrized as follows: (1) Evidentiality-guided Com-\npression: We developed a method that compresses\nretrieved documents based on evidentiality. (2) Ev-\nidentiality Reflection for Adaptive Compression:\nOur framework evaluates compressed content for\nevidentiality and adaptively adjusts the length of\ncompression. (3) Experiments show that our ap-\nproach significantly improves retrieval-augmented\nLLM performance on ODQA datasets. (4) Our\napproach is also cost-efficient, as it quickly com-\npresses long context, reducing latency and tokens.2 Related Work\n2.1 Evidentiality-guided RAG\nDense retrievers (Karpukhin et al., 2020; Izacard\net al., 2022) focus on lexical answerability, but may\nmislabel documents as relevant when they lack con-\ntextual evidence, leading to the need for evidential-\nity. In prior work (Lee et al., 2021), evidentiality\nrefers to whether a document supports generating\nthe correct answer to a question. Unlike answer-\nability, evidentiality is more challenging to mine\ndirectly as it reflects the contextual relationship\nbetween a question and a document. To measure\nevidentiality, previous work checks whether the\nremoval of the document is critical for answering\nthe question (Asai et al., 2022), utilizes attention\nscores (Niu et al., 2020), or considers the change\nin confidence scores (Song et al., 2024). Our work\nintroduces evidentiality in LLMs, enhancing RAG\nby prioritizing contextually rich documents for gen-\nerating correct answers.\n2.2 Prompt Compression\nNumerous studies (Mu et al., 2024; Li et al., 2023;\nKim et al., 2024) have focused on prompt compres-\nsion to address both cost and performance chal-\nlenges, as shown in prior research (Shi et al., 2023;\nLiu et al., 2024; Hsieh et al., 2024). RECOMP (Xu\net al., 2024) provides both extractive and generative\nsummaries of documents, considering whether the\nsummaries helped answer the given question. LLM-\nLingua (Jiang et al., 2023b) uses conditional proba-\nbilities of LLMs to guide fine-grained prompt com-\npression. Building on this, LongLLMLingua (Jiang\net al., 2024) compresses prompts in long context\nscenarios by using a question-aware coarse-to-fine\ncompression and document reordering mechanism.\nSimilarly, CompAct (Yoon et al., 2024) employs an\nadaptive compression strategy to iteratively com-\npress documents while retaining key information\nrelevant to the query. However, existing methods\nstruggle to compress long context, which prevents\nthem from fully utilizing the retrieval results.\n2.3 Retrieval Evaluation for RAG\nLLMs may evaluate the quality of retrieved re-\nsults for enhancing RAG, as seen in Madaan et al.\n(2024), where models iteratively improve their re-\nsponses; this concept has been applied to RAG.\nSelf-RAG (Asai et al., 2024) trains LLM to evalu-\nate retrieved documents and its output by predicting\nreflection tokens that assess the need for retrieval\n2\n--- Page 3 ---\nand the quality of the generated text. Labruna et al.\n(2024) dynamically determines whether to retrieve\nadditional context when needed by using a trained\nreader LLM. CRAG (Yan et al., 2024) employs a re-\ntrieval evaluator to assess document relevance and\ntriggers corrective actions to refine retrieved infor-\nmation, by using lexical overlap between questions\nand documents. In our ECoRAG framework, we\nevaluate whether the evidence is sufficient to gen-\nerate the correct answer by leveraging evidentiality\nas defined by the LLM.\n3 Proposed Method\nIn this section, we describe how ECoRAG adap-\ntively adjusts the compression length to ensure\nthat the LLM generates the correct answer. To\nachieve this, we focus on: (1) compressing re-\ntrieved documents by sorting them based on evi-\ndentiality (Section 3.1), and (2) evaluating whether\nthe compressed documents is sufficiently eviden-\ntial, and if not, adaptively incorporating more in-\nformation (Section 3.2), and Figure 2 provides an\noverview.\n3.1 Evidentiality-guided Compressor\nThis section explains how retrieved documents are\ncompressed while preserving the evidence that en-\nables the LLM to generate the correct answer. We\ndecompose documents into sentences inspired by\nXu et al. (2024) and compress them guided by ev-\nidentiality. To retain the necessary content and\nremove irrelevant parts during the compression pro-\ncess, we first extract evidential sentences from the\nretrieved documents (Section 3.1.1) and then use\nthem to train the compressor (Section 3.1.2).\n3.1.1 Definition of Evidentiality\nWe define the evidentiality of a sentence based on\nits contribution to generating the correct answer\nwhile penalizing distractors that interfere with this\nprocess. The degree of evidentiality is categorized\nhierarchically based on two conditions. We find\nsentences that enable the LLM to generate the cor-\nrect answer. If a sentence does not, we then check\nif it interferes with other evidence.\nFirst, when assessing whether each sentence\nhelps generate the correct answer, it is important to\nconsider that the LLM contains parametric knowl-\nedge (Wang et al., 2020; Yu et al., 2023; Luo et al.,\n2023). Prior work (Lee et al., 2021; Asai et al.,\n2022) has focused on whether the language model\ncould contribute to generating the correct answerusing given document. However, it is challeng-\ning to distinguish whether the correct answer was\ngenerated using the document or parametric knowl-\nedge, especially in larger models. If the correct an-\nswer was generated solely using parametric knowl-\nedge, regardless of the given document, it is unclear\nto determine whether the document serves as key\nevidence. Therefore, we propose the following first\ncondition: 1Without the sentence the LLM can-\nnot generate the correct answer alone, but with the\nsentence it can.\nSecond, it is also crucial for the compressor to\nfilter out distractors that hinder the evidence from\ngenerating the correct answer. While robustness\nto distractors can be improved through fine-tuning\n(Liu et al., 2024), training LLMs often requires sub-\nstantial costs for training and closed LLMs often\nimpossible to train. If the compressor can remove\ndistractors, it can be applied to any LLM without re-\nquiring additional training. To identify distractors,\nwe introduce a second condition for sentences that\ndo not satisfy 1:2The sentence does not inter-\nfere with the evidence defined in 1in generating\nthe correct answer.\nBased on the aforementioned conditions, we hi-\nerarchically define evidentiality as depicted in Fig-\nure 3. Sentences satisfying condition 1are labeled\nasstrong evidence . Sentences failing to meet con-\ndition 1are further classified based on condition\n2: those satisfying condition 2are labeled as\nweak evidence , while those that do not are classi-\nfied as distractor . Following these conditions, we\nuse an LLM to label sentences in retrieved docu-\nments for each question in the training data.\n3.1.2 Learning Objective for Compressor\nGiven labeled sentences D={d1, d2,···, d|D|},\nfor a question q, we train our compressor based\non dual encoders (Izacard et al., 2022) to differen-\ntiate between strong and weak evidence, as well\nas distractor. Using dual encoders, EQfor ques-\ntions and EDfor sentences, we calculate the sim-\nilarity score between qand sentences in D(i.e.,\nsim(q, di) =EQ(q)·ED(di)). Sentences are cate-\ngorized into strong ( d∗) or weak ( d+) evidence, and\ndistractor ( d−) based on our hierarchical definition.\nWe define similarity scores as s∗=sim(q, d∗),\ns+=sim(q, d+), ands−=sim(q, d−). The sim-\nilarity scores are utilized to train two inequalities:\n(s+> s−),(s∗> s+, s−) (1)\n3\n--- Page 4 ---\n⋮𝑑𝑖\n𝑑𝑖+1\n𝑑|𝐷|\nEvidentiality -\nguided\nCompressor\ndecompose\n𝑞: Who got the first nobel prize?\nEvidentiality\nEvaluator\n𝑑2′𝑑1′\n𝑑|𝐷|′⋮retrieval\nECoRAG\n𝐶Final \ncompressionFinal compression\nLLM\nyes\nsort sentences\nby evidentiality⋮\n𝑑𝑘′Collective\n𝐶 𝑑𝑘+1′Retrieved\ndocuments\n(n = 100)\n⋮𝑑2′𝑑1′\n𝑑|𝐷|′⋮⋮\n𝑑𝑘′\n𝑑𝑘+1′\nno collect more evidence ( 𝑑𝑘+1′)Evidentiality Reflection Evidentiality -guided Compression\nIs 𝐶 \nevidential\nto 𝑞?\n[End]Figure 2: This figure illustrates the overall framework of ECoRAG. First, the evidentiality-guided compressor\ncompresses the retrieved documents by sorting decomposed sentences based on evidentiality, producing an ordered\nset of evidences d′\n1, d′\n2, . . . , d′\n|D|. Second, evidentiality reflection starts with the top-ranked sentence ( n= 1, i.e.,\nC=d′\n1), and the evidentiality evaluator determines whether Cis evidential. If not, more evidence is added\niteratively ( n=k→n=k+ 1) until the evaluator judges Cas evidential. Once evidential, it is used for final\ncompression (green line); otherwise, additional evidence is collected (red line).\nThese inequalities ensure that strong evidence is\nranked above weak evidence, which in turn is\nranked above distractor, guiding the training of\nour compressor.\nThe weak evidentiality loss Lweuses the In-\nfoNCE loss to distinguish weak evidence d+from\ndistractor d−. The loss function is formulated as:\nLwe=−logexp(s+/τ)\nexp(s+/τ) +P\nd−\nj∈D−exp(s−\nj/τ)\n(2)\nHere, s−\nj=sim(q, d−\nj)represents the similarity\nscore for each distractor in the set D−, and τis a\ntemperature parameter.\nThe strong evidentiality loss Lsealso utilizes the\nInfoNCE loss to prioritize strong evidence d∗. The\nloss function is formulated as:\nLse=−logexp(s∗/τ)\nexp(s∗/τ) +P\nd±\nj∈D−∪D+exp(s±\nj/τ)\n(3)\nHere, s±\nj=sim(q, d−\nj)is the similarity score for\neach sentence in the combined sets of distractors\nD−and weak evidences D+.\nThe final loss Lis defined as the sum of the\nstrong and weak evidentiality losses:\nL=Lse+Lwe (4)\nOur compressor is trained using this loss L, and\nranks sentences d′\n1, d′\n2, . . . , d′\n|D|by evidentiality,selecting high-scoring ones for compression. The\nnumber of sorted evidence required can vary de-\npending on the difficulty of each question. How-\never, providing too little evidence may omit impor-\ntant information, while too much increases com-\nputational costs for each question. Thus, balanced\ncompression ratio is necessary for each question to\naddress both issues.\n3.2 Evidentiality Reflection for Adaptive\nCompression\nOnce a collective of evidential sentences is formed,\nwe need to determine whether the compression ra-\ntio is appropriate. To achieve this, we reflect on the\nevidentiality of compressed documents using a lan-\nguage model (Section 3.2.1). Then, if compressed\ntoo much, we adaptively adjust the compression\nratio by collecting more (Section 3.2.2).\n3.2.1 Training Evidentiality Evaluator\nWe develop an effective evidentiality evaluator\nMevalthat assesses whether the compressed docu-\nments are strong evidence enough to generate the\ncorrect answer. In prior work, CompAct (Yoon\net al., 2024) trained the evaluator by prompting\nGPT-4o (OpenAI, 2023) to determine if the evi-\ndence is sufficient to answer the question. However,\nthis approach can introduce bias (Chiang and Lee,\n2023) when GPT-4o evaluates through prompting,\nleading to inaccurate supervision. Accurate super-\nvision requires verifying if the document actually\nenables the reader LLM to generate the correct\n4\n--- Page 5 ---\n𝑞: When is the next deadpool  movie being released?\n𝑎: May 18, 2018\n𝑑1: “Deadpool 2” was released on May 18, 2018. \n𝑑2: “Deadpool 2” is the next movie of Deadpool.\n𝑞\n𝑞\n𝑞\n𝑞𝑑12017\nMay 18, 2018\n2017\n𝑑1strong evidence𝑑3: Spider -Man and Deadpool often team up in Marvel.\n𝑑2\n𝑑3 2017\n𝑑2weak evidence 𝑑3distractor𝑞\n𝑞 𝑑3𝑑2 May 18, 2018 𝑑1\n𝑑1 2017\nLLM\n𝑑2,𝑑3sent to (c)\n(b) strong evidentiality mining (a) Example of question, answer, \nand sentences for evidentiality mining\nLLM\n(c) weak evidentiality mining Figure 3: This figure illustrates the evidentiality mining\nstrategy of ECoRAG.\nanswer. To achieve this, we reuse our evidential-\nity labels obtained from the LLM in Section 3.1.1\nand distill them from our reader LLM into smaller\nmodel, Flan-T5-large (Chung et al., 2022), to build\nthe evaluator. Comparison between CompAct and\nour evaluator is discussed in Section 5.2.\nWe train Mevalusing our evidentiality labeled\ndataset (d∗, d+, d−)to determine if compressed\ndocuments are sufficient for correct answer gener-\nation. The evaluator is trained to classify whether\nthe given compressed documents is strong evi-\ndence. To facilitate this, we add 2 special tokens\nt∈[<EVI> ,<NOT> ]and train Mevalto gener-\nate ‘<EVI>’ for strong evidence d∗, and ‘<NOT>’\nfor other sentences d+, d−. Subsequently, next-\ntoken prediction loss Levalis used for this training\nstage to predict whether compressed documents are\nstrong evidence.\nLeval=−log pMeval(t|q, d) (5)\n3.2.2 Adaptive Compression\nIn adaptive compression, the compression ratio is\nadaptively adjusted by our evaluator, which reflects\non whether the current compression is evidential,as described in Figure 2. Initially, our evaluator as-\nsesses the evidentiality of compressed documents\nCcontaining only the first evidence, d′\n1, from our\nordered evidences d′\n1, d′\n2, . . . , d′\n|D|. If the evalua-\ntor determines that Cis evidential, it becomes the\nfinal compression provided to LLM. If Cis not\nevidential, we add the next piece of evidence d′\n2\ntod′\n1to build new compressed documents. If the\nk-th iteration fails, d′\nk+1is added to the previously\ncompressed documents. This process is repeated\nuntil the desirable compression is found, with a to-\nken limit set to avoid infinite loop. Since retrieved\ndocuments do not always include gold evidence for\nall queries, a token limit is necessary to prevent\ninfinite loops from continuously adding evidence.\nThe final compression is then used as input for the\nLLM, which generates the final answer.\nAlthough iterative adjustment can increase la-\ntency compared to using raw documents, ECoRAG\nreduces it efficiently. Prior work (Yoon et al., 2024),\neach iteration required LLM (7B) to generate a new\ncompression by using the previous compression\nand the next piece of evidence. Thus, with each\niteration, LLM reads different contents and gen-\nerates compression of multiple tokens, increasing\nlatency time. However, ECoRAG reduces redun-\ndancy by ordering evidence just once and adding\nit iteratively. Moreover, our framework utilized\na lightweight evaluator (0.77B) that adjusts com-\npression length by generating just a single special\ntoken, resulting in rapid compression speed; the\nactual results are shown in Section 5.4.\n4 Experiments\n4.1 Experimental Settings\nDatasets We evaluate our framework through\nNQ (Kwiatkowski et al., 2019), TQA (Joshi et al.,\n2017), and WQ (Berant et al., 2013), which are\nODQA datasets. We use the 100 documents re-\ntrieved from DPR (Karpukhin et al., 2020)1.\nModels We initialize our evidentiality compres-\nsor from Contriever (Izacard et al., 2022) and use\nit to compare its performance with RECOMP (Xu\net al., 2024). For evidentiality evaluator, we utilize\nFlan-T5-large (Chung et al., 2022), because pre-\nvious RAG and document-assessment work (Han\net al., 2023; Yan et al., 2024) have successfully em-\nployed it. Detailed justification for this choice can\n1Since enhancing the retriever is beyond the scope of this\nstudy, we conduct our experiments under the assumption that\nthe retrieved documents are already provided.\n5\n--- Page 6 ---\nMethodsNQ TQA WQ\n#tokens ↓ EM F1 #tokens ↓ EM F1 #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 31.88 44.10 0 64.78 73.10 0 24.51 42.73\nstandard RAG (100 documents) 13905 36.09 50.18 14167 56.21 64.22 13731 21.11 38.72\nRAG with 100 documents compressed\nLLMLingua (Jiang et al., 2023b) 635 26.84 38.30 630 50.81 57.91 641 22.98 39.77\nLLMLingua-2 (Pan et al., 2024) 1315 30.11 42.52 1324 53.19 60.46 1113 23.52 40.61\nLongLLMLingua (Jiang et al., 2024) 1370 32.96 45.32 1402 55.75 63.75 1355 21.51 39.13\nRECOMP (extractive) (Xu et al., 2024) 662 32.85 44.54 672 51.66 59.08 658 19.54 36.83\nRECOMP (abstractive) (Xu et al., 2024) 14 27.59 39.19 26 39.95 46.68 19 20.47 36.90\nCompAct (Yoon et al., 2024) 106 35.71 47.14 96 63.96 73.87 75 29.77 44.25\nECoRAG (ours) 632 36.48 49.81 441 65.34 75.37 560 30.17 46.13\nTable 1: Compression methods performance comparison on NQ, TQA, and WQ. The table shows the results using\nGPT-4o-mini as the reader model, given 100 retrieved documents (Karpukhin et al., 2020). It reports the number of\ntokens after compression, along with EM and F1-score, illustrating the impact of different compression methods on\nmodel performance.\nbe found in Section B.2. For the reader model, we\nuse GPT-4o-mini (OpenAI, 2023), as it supports a\ncontext length of 128K tokens, sufficient to process\nall 100 retrieved documents.\nEvaluation Metrics We report results on the test\nsets of NQ, TQA, and WQ using EM and word-\nlevel F1-score to assess the question-answering\ntask performance. We also report the average num-\nber of input tokens given to the reader LLM to\nevaluate the efficiency of our compression step.\nBaseline We report two types of baselines.\nRAG without compression : As a baseline, we\nreport the results using only the question and raw\nretrieved documents. The ‘closed-book’ setting,\nwhere no retrieval is used, shows that the model\nrelies solely on its internal knowledge. In the ‘stan-\ndard RAG’ setting, we simply concatenate the top\n100 retrieved documents without any compression\nfor evaluation.2This is the approach used in con-\nventional RAG without compression.\nRAG with 100 compressed documents : We\nalso reproduce several retrieval augmentation\nmethods for comparison. To better understand\nthe effect of different compression methods, we\nevaluated several baselines including LLMLin-\ngua (Jiang et al., 2023b), LLMLingua-2 (Pan et al.,\n2024), LongLLMLingua (Jiang et al., 2024), Com-\npAct (Yoon et al., 2024), and RECOMP which\noffers both extractive and abstractive variants.\nIn addition to our compression and non-\ncompression baselines, we include BGE-M3 (Chen\net al., 2024) and BGE-reranker (Xiao et al., 2024)\n2We also evaluated the effect of reducing the number of\nretrieved documents ( k= 5,10,20) for both DPR and Con-\ntriever in Table 16; results are explained in Section A.9.under equal token budgets. However, since these\nare not compression methods, their comparison\nresults are addressed in Section A.4.\n4.2 Results\nIn this section, we report the results of our model\nand compare them with both compression-based\nand non-compression baselines for ODQA in Table\n1. Accuracy, such as EM and F1-score, is a more\nimportant metric than token reduction for evaluat-\ning compression quality because simply reducing\ntokens without preserving necessary information\nis meaningless. A method is more efficient if it\nreduces more tokens while maintaining higher ac-\ncuracy than another.\nIn terms of accuracy, ECoRAG outperforms all\nbaselines, including standard RAG, where the LLM\nreads all retrieved information. In the long context\nsetting, retrieving many documents often brings in\nthose with low relevance scores, introducing noise.\nHowever, previous compression methods fail to\nfilter out this noise, leading to performance degra-\ndation compared to uncompressed approaches. No-\ntably, ECoRAG surpasses all these methods, even\nwith fewer tokens than some of them. The strength\nof ECoRAG lies in compressing only the necessary\ncontent, focusing solely on the information essen-\ntial for generating the correct answer. As a result,\nECoRAG outperforms the strongest compression\nbaseline in NQ (+0.77%p), TQA (+1.38%p), and\nWQ (+0.40%p) in EM. As further detailed in Sec-\ntion A.10, ECoRAG maintains this advantage even\non much longer retrieved documents, confirming\nits robustness in another long context setting (Bai\net al., 2024).\nFrom a token efficiency perspective, ECoRAG\n6\n--- Page 7 ---\nMethods NDCG@1 NDCG@10\nAnswerability (baseline) 67.82 79.20\nLeave-One-Out (Asai et al., 2022) 70.67 80.80\nECoRAG (ours) 75.53 81.92\nTable 2: Comparison of NDCG@1 and NDCG@10 on\nHotpotQA dataset using different training signals\nuses more tokens than RECOMP (abstractive) and\nCompAct but still outperforms them, while com-\npressing with fewer tokens than other methods.\nAccording to Xu et al. (2024), abstractive RE-\nCOMP performs well in the 5-document setting\nbut struggles in long contexts due to input size limi-\ntations. CompAct suffers from inaccurate compres-\nsion evaluation, failing to retain essential informa-\ntion, which lowers performance. In contrast, EC-\noRAG can handle long context and retain only the\nnecessary content to generate the correct answer,\nwhich results in superior performance across differ-\nent datasets. Excluding the two compressors that\nfail to preserve necessary information, ECoRAG\nachieves higher accuracy with fewer tokens than\nother methods, demonstrating its token efficiency.\n5 Analysis\nIn addition to the main results, we verified the ef-\nfectiveness of our framework by addressing the\nfollowing research questions:\n•RQ1 : Does our compressor effectively cap-\nture human-annotated evidence?\n•RQ2 : How accurately does our evaluator pre-\ndict evidentiality?\n•RQ3 : What is the impact of each component\nin ECoRAG?\n•RQ4 : Is ECoRAG efficient compression?\n5.1 RQ1: Alignment with Human-annotated\nEvidentiality\nIn this section, we assess whether our compres-\nsor can effectively sort sentences by evidentiality\nfor next step. Although our compressor improves\nLLM performance by learning LLM-defined ev-\nidentiality, it is essential to verify whether it ef-\nfectively captures ground-truth evidence. Thus,\nwe conducted experiments using HotpotQA (Yang\net al., 2018), which provides human-annotated evi-\ndence. We compared how well prior methods and\nour compressor assign higher scores to ground-\ntruth evidence. For evaluation, we use NormalizedDiscounted Cumulative Gain (NDCG) as a metric\nto evaluate how effectively evidentiality-focused\nmethods, including ours, rank evidence higher.\nAs shown in Table 2, ECoRAG achieved the\nhighest performance, demonstrating strong align-\nment with human-annotated evidentiality. The ‘An-\nswerability’ baseline trains the compressor by treat-\ning passages containing the correct answer as pos-\nitive and those without as negative. The ‘Leave-\nOne-Out’ (Asai et al., 2022) considers a passage\nas positive if removing it prevents the model from\ngenerating the correct answer, and negative if the\nmodel still succeeds. ECoRAG outperforms prior\nevidentiality baselines, achieving improvements in\nNDCG@1 (+4.86%p) and NDCG@10 (+1.12%p)\nThis result indicates that our compressor effectively\ncaptures evidence and aligns well with human anno-\ntations. Thus, our compressor provides well-sorted\nevidences to our evaluator, then we need to verify\nthe evaluator, the other component of ECoRAG.\n5.2 RQ2: Evaluator Performance on\nEvidentiality Prediction\n30405060708090100\nAccuracy Precision Recall F1TQA\nCompAct Flan-UL2 Ours\nFigure 4: Evidentiality evaluation metrics using differ-\nent evaluator, including ours, measured on the TQA.\nWe also need to verify the evidentiality evalua-\ntor to accurately evaluate whether the compressed\ndocuments enable the LLM to generate the correct\nanswer. To assess its accuracy, we conducted exper-\niments on the TQA test set. For each question, we\ndefine ground-truth labels for retrieved documents\nas either <EVI>, which lead to generating the cor-\nrect answer as in Section 3.2.1, or <NOT>. We\nthen measured how well our evaluator and other\nevaluators predicted these labels using accuracy,\nprecision, recall, and F1-score. The results are\nshown in Figure 4.\nAcross all metrics, our evidentiality evalua-\ntor effectively predicts evidentiality, even though\nit has significantly fewer parameters than other\n7\n--- Page 8 ---\nNQ TQA\nEM R20 EM R20\n(A) ECoRAG (ours) 36.48 75.18 65.43 80.38\nCompressor\n(B) w/o answerability 31.25 49.53 63.86 70.84\n(C) w/o evidentiality 35.46 74.93 64.90 80.59\nAdaptive Compression\n(D) w/o evaluator 35.71 - 63.63 -\nTable 3: Ablation study of ECoRAG, showing the im-\npact of compressor and adaptive compression methods.\nevaluators. It outperforms the CompAct evalua-\ntor (7B) (Yoon et al., 2024) by +13.96%p in F1\nscore. The CompAct evaluator is based on Mistral-\n7B (Jiang et al., 2023a) and trained with supervi-\nsion from GPT-4o. As Asai et al. (2024) noted,\nthe reader LLM evaluates whether documents sup-\nport the correct answer, making it a strong baseline.\nWe used Flan-UL2 (Tay et al., 2023) (20B) as our\nreader LLM, as described in Section B.3. Notably,\nour evidentiality evaluator, despite its much smaller\nsize (770M), closely approximates the performance\nof Flan-UL2 (-0.08p%).\n5.3 RQ3: Ablation Study\nIn Table 3, we present the results of our ablation\nstudy, assessing the impact of each component in\nour framework by comparing EM across different\nsettings. We also report R20, checking if the gold\nanswer is in the top 20 sentences.\nForCompressor , we compare (A) ECoRAG with\ntwo inferior compressors, (B) and (C). In (B), the\ncompressor uses a pretrained Contriever check-\npoint without additional training, while in (C), it is\ntrained with answerability labels. As shown, our\ncompressor trained with evidentiality labels out-\nperforms both alternatives. Comparing (A) and\n(C) shows that evidentiality labels increase EM\n(+1.02%p, +0.53%p) while maintaining R20 at a\ncomparable level. Since R20 measures lexical over-\nlap, (C), trained with answerability, performs simi-\nlarly to or better than (A). The results demonstrate\nthe superiority of our evidentiality labels over an-\nswerability labels, as they prioritize contextually\nrich information.\nForEvaluator , we consider a no-evaluator set-\nting (D), where the initial compression from the\ncompressor is used without evaluating its eviden-\ntiality. The EM gap between (A) and (D) (+0.77%p,\n+1.80%p) highlights the impact of the evidential-\nity evaluator. These results highlight the impor-\ntance of adaptively adjusting the amount of evi-MethodsCompression\nTimeInference\nTimeTotal\nTimeThroughput\n(example/sec)\nclosed-book - 3.79h 3.79h 0.26\nstandard RAG - 12.28h 12.28h 0.08\nRECOMP 0.27h 4.08h 4.35h 0.23\nCompAct 10.10h 4.83h 14.94h 0.07\nECoRAG (ours) 0.73h 4.23h 4.96h 0.20\nTable 4: Inference time and compression time for NQ\ntest.\ndence through evidentiality evaluation.\n5.4 RQ4: Total Latency\nECoRAG is cost-efficient not only because it re-\nduces the number of tokens but also because it de-\ncreases total latency in the RAG process. In RAG\nwithout compression, computational costs increase\nas more documents are retrieved. By applying com-\npression and retaining only the necessary informa-\ntion, ECoRAG reduces total processing time.\nTable 4 presents the total latency3, including\nboth compression and inference time, to show the\nefficiency of our approach. For long context, the\nLLM-based abstractive compressor CompAct took\nlonger than the ‘standard RAG’ setting, whereas the\nextractive compressors RECOMP and ECoRAG\nwere faster. ECoRAG uses the lightweight eval-\nuator that generates only a single token per itera-\ntion, stopping the reflection process once the com-\npressed document is evidential or the token limit\nis reached, thereby preventing excessive compres-\nsion time. While ECoRAG had similar speed to\nRECOMP, it achieved better performance by re-\ntaining only the information necessary to generate\nthe correct answer, as described in Table 1. Thus,\nECoRAG is effective in handling long contexts in\nterms of both performance and efficiency.\nECoRAG is a two-step design that achieves both\nspeed and performance. Single-step aggregation\nwith LLMs, as demonstrated by CompAct in Ta-\nble 1, struggles with length dependency for list-\nwise evaluation due to the “lost-in-the-middle” is-\nsue (Liu et al., 2024). In contrast, ECoRAG sep-\narates the process by first assessing sentences in-\ndividually with an extractive compressor and then\nevaluating them collectively. This separation over-\ncomes challenges in handling long contexts and im-\nproves compression effectiveness. Our lightweight\ncomponents ensure efficiency while achieving ef-\nfective compression.\n3Since GPT-4o-mini does not provide latency measure-\nments, we conducted the latency experiments using Flan-UL2.\n8\n--- Page 9 ---\n6 Conclusion\nECoRAG is a framework designed to compress\nlong context by focusing on evidentiality in LLMs,\ndefined as whether information supports generating\nthe correct answer. Evidentiality-guided compres-\nsion effectively filters out irrelevant content and\nretains necessary evidence. Through adaptive com-\npression, ECoRAG determines the optimal com-\npression length for each question, ensuring efficient\nuse of context. As a result, ECoRAG demonstrates\nboth superior performance and efficiency in han-\ndling long context, outperforming other compres-\nsion methods.\n7 Limitation\nEvidentiality provides an effective indicator for\ndetermining whether information is necessary for\nan LLM to generate the correct answer. However,\nmining evidentiality labels is computationally ex-\npensive, leading to increased costs. Since multiple\ninferences are required for each question, it results\nin significant time consumption. Nevertheless, as\nmore time is spent, more evidentiality labels can\nbe obtained, which can contribute to the training\nof the compressor. Evidentiality labels can also be\nreused to train the evidentiality evaluator, optimiz-\ning resource usage. Once the compressor is fully\ntrained and applied, the LLM inference process\nbecomes faster.\nBuilding upon this efficiency improvement, the\napplication of this system can be extended beyond\nODQA to address broader real-world scenarios.\nExtending it to tasks like summarization may be\nnecessary due to context length limits when pro-\ncessing full content with LLMs. Selecting and\nsummarizing only the most important parts can\nimprove performance (Saxena and Keller, 2024;\nJeong et al., 2025), requiring evidentiality to be\nredefined based on summarization metrics. Investi-\ngating such adaptations is a potential direction for\nfuture work.\nAcknowledgements\nThis work was supported by the National Research\nFoundation of Korea(NRF) grant funded by the Ko-\nrea government(MSIT) (No. RS-2024-00414981),\nInstitute of Information & communications Tech-\nnology Planning & Evaluation (IITP) grant funded\nby the Korea government (MSIT) (No. 2022-0-\n00077/RS-2022-II220077, AI Technology Devel-\nopment for Commonsense Extraction, Reasoning,and Inference from Heterogeneous Data), and Insti-\ntute of Information & communications Technology\nPlanning & Evaluation (IITP) grant funded by the\nKorea government(MSIT) [NO.RS-2021-II211343,\nArtificial Intelligence Graduate School Program\n(Seoul National University)].\nReferences\nAkari Asai, Matt Gardner, and Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nknowledge-intensive nlp tasks. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2226–2243.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\nretrieve, generate, and critique through self-reflection.\nInThe Twelfth International Conference on Learning\nRepresentations .\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, et al. 2024. Longbench:\nA bilingual, multitask benchmark for long context\nunderstanding. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 3119–3137.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing , pages 1533–1544.\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nLuo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2024 ,\npages 2318–2335, Bangkok, Thailand. Association\nfor Computational Linguistics.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models. Preprint , arXiv:2210.11416.\n9\n--- Page 10 ---\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 .\nSang-eun Han, Yeonseok Jeong, Seung-won Hwang,\nand Kyungjae Lee. 2023. On monotonic aggrega-\ntion for open-domain qa. In Proc. Interspeech 2023 ,\npages 3432–3436.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-\ntanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-\nburg. 2024. RULER: What’s the real context size of\nyour long-context language models? In First Confer-\nence on Language Modeling .\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research .\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nYeonseok Jeong, Minsoo Kim, Seung-won Hwang, and\nByung-Hak Kim. 2025. Agent-as-judge for factual\nsummarization of long narratives. arXiv preprint\narXiv:2501.09993 .\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023a. Mistral\n7b.arXiv preprint arXiv:2310.06825 .\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing\nYang, and Lili Qiu. 2023b. LLMLingua: Compress-\ning prompts for accelerated inference of large lan-\nguage models. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 13358–13376, Singapore. Association\nfor Computational Linguistics.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dong-\nsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\n2024. LongLLMLingua: Accelerating and enhanc-\ning LLMs in long context scenarios via prompt com-\npression. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1658–1677, Bangkok,\nThailand. Association for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 6769–6781,\nOnline. Association for Computational Linguistics.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,\nand Jinwoo Shin. 2024. Sure: Improving open-\ndomain question answering of LLMs via summarized\nretrieval. In The Twelfth International Conference on\nLearning Representations .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: A benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:452–\n466.\nTiziano Labruna, Jon Ander Campos, and Gorka\nAzkune. 2024. When to retrieve: Teaching llms to\nutilize information retrieval effectively. Preprint ,\narXiv:2404.19705.\nKyungjae Lee, Seung-won Hwang, Sang-eun Han, and\nDohyeon Lee. 2021. Robustifying multi-hop qa\nthrough pseudo-evidentiality training. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 6110–6119.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems , 33:9459–9474.\nTianle Li, Ge Zhang, Quy Duc Do, Xiang Yue,\nand Wenhu Chen. 2024. Long-context llms strug-\ngle with long in-context learning. arXiv preprint\narXiv:2404.02060 .\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.\n2023. Compressing context to enhance inference ef-\nficiency of large language models. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing , pages 6342–6353, Singa-\npore. Association for Computational Linguistics.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics , 12:157–173.\nZiyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang\nTao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.\nAugmented large language models with parametric\nknowledge guiding. Preprint , arXiv:2305.04757.\n10\n--- Page 11 ---\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2024. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Pro-\ncessing Systems , 36.\nJesse Mu, Xiang Li, and Noah Goodman. 2024. Learn-\ning to compress prompts with gist tokens. Advances\nin Neural Information Processing Systems , 36.\nYilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, Jing-\nfang Xu, and Minlie Huang. 2020. A self-training\nmethod for machine reading comprehension with soft\nevidence extraction. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 3916–3927, Online. Association\nfor Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report. Preprint ,\narXiv:2303.08774.\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin\nXia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor\nRuhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao,\nLili Qiu, and Dongmei Zhang. 2024. LLMLingua-\n2: Data distillation for efficient and faithful task-\nagnostic prompt compression. In Findings of the\nAssociation for Computational Linguistics ACL 2024 ,\npages 963–981, Bangkok, Thailand and virtual meet-\ning. Association for Computational Linguistics.\nCheng Qian, Xinran Zhao, and Tongshuang Wu. 2024.\n”merge conflicts!”’ exploring the impacts of exter-\nnal knowledge distractors to parametric knowledge\ngraphs. In First Conference on Language Modeling .\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics , 11:1316–1331.\nRohit Saxena and Frank Keller. 2024. Select and sum-\nmarize: Scene saliency for movie script summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: NAACL 2024 , pages 3439–3455.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning , pages\n31210–31227. PMLR.\nYongho Song, Dahyun Lee, Myungha Jang, Seung-won\nHwang, Kyungjae Lee, Dongha Lee, and Jinyoung\nYeo. 2024. Evidentiality-aware retrieval for overcom-\ning abstractiveness in open-domain question answer-\ning. In Findings of the Association for Computa-\ntional Linguistics: EACL 2024 , pages 1930–1943.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil\nHoulsby, and Donald Metzler. 2023. UL2: Unifyinglanguage learning paradigms. In The Eleventh Inter-\nnational Conference on Learning Representations .\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\narXiv preprint arXiv:2408.00118 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 .\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. ♪musique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics , 10:539–554.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs. arXiv\npreprint arXiv:2010.11967 .\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022. Super-\nnaturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 5085–5109.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003–4012, Marseille, France. European Language\nResources Association.\nKevin Wu, Eric Wu, and James Zou. 2024. How faith-\nful are rag models? quantifying the tug-of-war be-\ntween rag and llms’ internal prior. arXiv preprint\narXiv:2404.10198 .\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of the 47th international ACM SIGIR\nconference on research and development in informa-\ntion retrieval , pages 641–649.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. Re-\ncomp: Improving retrieval-augmented lms with con-\ntext compression and selective augmentation. In The\nTwelfth International Conference on Learning Repre-\nsentations .\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.\n2024. Corrective retrieval augmented generation.\nPreprint , arXiv:2401.15884.\n11\n--- Page 12 ---\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nInProceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nChanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Min-\nbyul Jeong, and Jaewoo Kang. 2024. CompAct:\nCompressing retrieved documents actively for ques-\ntion answering. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 21424–21439, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations .\n12\n--- Page 13 ---\nAppendices\nA Further Analysis\nA.1 Comparative Analysis of Compression\nMethods\nIn this section, we will provide a more detailed\ncomparison of our approach with other baselines\nbased on Table 1. Table 5 provides an overview of\nhow each method differs. Based on this compari-\nson, we discuss how large-scale documents can be\ncompressed efficiently and effectively.\nIn ODQA, since the model must provide an an-\nswer to a given question, the compression process\nneeds to consider the question. LLMLingua (Jiang\net al., 2023b) and LLMLingua-2 (Pan et al., 2024),\nwhich do not consider the question during compres-\nsion, often include irrelevant information, leading\nto suboptimal performance. On the other hand, the\nmethods other than LLMLingua and LLMLingua-2\nare question-aware, allowing them to more effec-\ntively capture the necessary content, resulting in\nhigher performance compared to question-agnostic\nmethods.\nThe amount of evidence needed varies for each\nquestion, and one solution to address this is adap-\ntive compression, where the compression ratio\nis adjusted for each question. By applying this\nmethod, only the necessary tokens are used, lead-\ning to high performance with fewer tokens. As seen\nin Table 1, both CompAct (Yoon et al., 2024) and\nECoRAG achieve high performance with a reduced\nnumber of tokens.\nHowever, there are two main challenges when\ndealing with long context. First, while using nu-\nmerous retrieval results increases the amount of\nnecessary information available, it also includes\ndocuments with lower relevance scores, resulting\nin considerable noise. Second, the overall length\nof the documents is too long, which makes the\ncompression process time-consuming.\nTo address the first challenge mentioned above,\nthe concept of evidentiality is necessary. As dis-\ncussed in Section 3.1.1, by prioritizing strong evi-\ndence for correct answer generation and penalizing\ndistractors, we have been able to create a compres-\nsor that is robust against noise. Consequently, this\napproach allows ECoRAG to demonstrate the high-\nest performance in large-scale document settings.\nTo address the second challenge, the compres-\nsor must be an extractive compressor that evalu-\nates each content pointwise and extracts only thenecessary information. Language model-based ab-\nstractive compressor is hindered by limited context\nlength, which leads to truncation and fails to han-\ndle entire large-scale documents. Moreover, LLM-\nbased abstractive compressor often requires sub-\nstantial time for inference and may suffer from po-\nsitional biases (Liu et al., 2024), which can lead to\ninaccurate assessments of evidentiality. However,\nextractive compressors such as ECoRAG and RE-\nCOMP (extractive) (Xu et al., 2024) are lightweight\nmodels that can quickly calculate scores, as seen in\nTable 4, and process each document in parallel for\neach document, thus avoiding positional biases.\nBased on these observations, we conclude that\nECoRAG, which combines all the characteristics\nfrom Table 5, is appropriate for compressing large-\nscale documents effectively.\nA.2 Evaluator Performance on NQ\n30405060708090100\nAccuracy Precision Recall F1NQ\nCompAct Flan-UL2 Ours\nFigure 5: Evidentiality evaluation metrics using differ-\nent evaluator, including ours, measured on the NQ.\nWe conducted same experiments on\nNQ (Kwiatkowski et al., 2019), as described in\nSection 5.2, observed similar trends to those in\nTQA (Joshi et al., 2017). As shown in Figure 5, our\nevidentiality evaluator consistently outperforms\nCompAct and demonstrates comparable results\nto Flan-UL2, further validating its effectiveness\nacross different datasets.\nA.3 Compression Effectiveness with More\nLong Context\nTo explore performance of ECoRAG with more\ndocuments, we conducted additional experiments\nusing 1000 retrieved documents in Table 6. Previ-\nous compression work, such as CompAct, focused\non up to 30 documents, while our experiments used\n100 documents, a common setting in RAG mod-\nels like FiD (Izacard and Grave, 2021). To verify\n13\n--- Page 14 ---\nMethodsQuestion\n-awareAdaptive\nCompressionEvidentiality\n-guidedExtractive\nCompression\nLLMLingua, LLMLingua-2 ✗ ✗ ✗ ✓\nLongLLMLingua ✓ ✗ ✗ ✓\nRECOMP (extractive) ✓ ✗ ✗ ✓\nRECOMP (abstractive) ✓ ✗ ✗ ✗\nCompAct ✓ ✓ ✗ ✗\nECoRAG (ours) ✓ ✓ ✓ ✓\nTable 5: The table compares different methods based on their key characteristics. Our approach, ECoRAG, integrates\nall these features for fast and effective large-scale document compression.\nMethods #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 21.33 28.71\nstandard RAG (1000 documents) 127,880 0.44 0.63\nRAG with 1000 documents compressed\nRECOMP (extractive) 661 31.39 42.29\nECoRAG (ours) 659 35.51 48.63\nTable 6: Experimental results on the NQ test dataset\nusing GPT-4o-mini, comparing performance with\nand without compression for 1000 retrieved docu-\nments (Karpukhin et al., 2020).\nwhether our method consistently improves perfor-\nmance even with more documents, we tested with\n1000 documents. Due to limited budget, we used\ndocuments already retrieved by a DPR setting that\nwas searched, differing from our top-100 DPR set-\nting. We compared ECoRAG with RECOMP, an\nextractive method with a similar structure, and ex-\ncluded abstractive compressors such as CompAct\ndue to its too high latency in longer context com-\npression.\nWith 1000 documents, ECoRAG remained\nhighly effective in compressing and preserving es-\nsential information. The context length became too\nlong for GPT-4o-mini to effectively utilize the in-\nformation (Hsieh et al., 2024), as shown in Table 6.\nHowever, our compression effectively reduced the\nlength, maintaining high performance. Addition-\nally, ECoRAG outperformed other extractive com-\npressors, demonstrating its superiority in handling\nextensive document sets.\nECoRAG remains the most effective compressor\neven for extremely long contexts. Without compres-\nsion, excessive context length can degrade perfor-\nmance or exceed the context limit. In contrast, our\nretriever-based compressor efficiently compresses\nextended inputs regardless of length.A.4 A Comparative Study with Reranker\nECoRAG fundamentally differs from reranking\nmethods like BGE-M3 (Chen et al., 2024) and\nRECOMP by adaptively determining the rank and\ncompression ratio needed for each query. While\nreranking models focus on relevance, they lack our\nability to iteratively refine compression based on\nevidentiality. To ensure a fair comparison with our\napproach in terms of token usage, we conducted ad-\nditional experiments with both BGE-M34and BGE-\nreranker5(Xiao et al., 2024) by using its reranked\ntop-10 and top-20 sentences. As shown in Table 7,\nECoRAG achieves better performance, demonstrat-\ning the importance of selecting the appropriate con-\ntext over simply increasing or reducing the amount\nof information.\nUnlike other sentence reranking methods, EC-\noRAG evaluates the initial compression and adap-\ntively adjusts the compression ratio through a re-\nflection process to determine how much infor-\nmation is required. This capability moves EC-\noRAG closer to true compression rather than sim-\nple reranking. Furthermore, our research extends\nbeyond proposing a compressor—it introduces a\ncomplete framework. While we used Contriever to\nensure fair comparisons with RECOMP, our frame-\nwork is flexible and capable of training models like\nBGE-M3 and BGE-reranker to learn LLM-based\nevidentiality, further enhancing performance.\nA.5 Adaptive Compression Ratio Analysis\nTo validate the claim of our adaptive compression\ncapabilities, we analyzed the distribution of com-\npression ratios across datasets. The compression\nratio is defined as the number of compressed tokens\ndivided by the number of original tokens. Table 8\nsummarizes the minimum, maximum, mean, me-\n4BAAI/bge-m3\n5BAAI/bge-reranker-large\n14\n--- Page 15 ---\nMethodsNQ TQA WQ\n#tokens EM F1 #tokens EM F1 #tokens EM F1\nBGE-M3 (top 10) 330 33.02 45.47 370 64.12 74.34 322 20.77 38.27\nBGE-M3 (top 20) 670 33.99 46.82 746 65.15 75.14 645 20.77 38.00\nBGE-reranker (top 10) 436 34.16 47.73 – – – – – –\nBGE-reranker (top 20) 838 34.82 47.95 – – – – – –\nECoRAG (ours) 632 36.48 49.81 441 65.34 75.37 560 30.17 46.13\nTable 7: Performance on NQ, TQA and WQ using GPT-4o-mini, comparing dense retriever BGE-M3, BGE-reranker\n(results shown only for NQ), and our ECoRAG.\nDatasetMin Compression\nRatioMax Compression\nRatioMean Compression\nRatioMedian Compression\nRatioStandard\nDeviation\nNQ 0.0036 1 0.0401 0.0446 0.0247\nTQA 0.0034 1 0.0267 0.0161 0.0221\nTable 8: Compression ratio statistics for NQ and TQA datasets.\nCompressor Evaluator Reader\nVRAM usage 110M 770M ≥8B\nLatency 0.70h 0.03h 4.23h\nTable 9: VRAM usage and latency for each component\nin ECoRAG on the NQ test set.\ndian, and standard deviation of compression ratios\nfor the NQ and TQA datasets.\nThe results highlight differences between\ndatasets, with higher mean and median compres-\nsion ratios observed for NQ. This reflects complex-\nity of dataset, requiring the extraction of answers\nfrom lengthy Wikipedia documents through reason-\ning and comprehensive understanding. In contrast,\nTQA involves documents with explicitly positioned\nanswers, making the task primarily about filtering\nirrelevant information. Consequently, ECoRAG re-\ntrieves more evidence for NQ to address its higher\ninformation needs, demonstrating its ability to ad-\njust compression ratios adaptively based on dataset\ncomplexity and information requirements.\nA.6 Further Analysis on Efficiency\nECoRAG has demonstrated efficiency over tradi-\ntional RAG, as shown in Table 1 and 4, but further\nanalysis is required to verify its resource and la-\ntency efficiency. To compare resource usage, we\nrefer to Table 9. While traditional RAG requires at\nleast 8B VRAM in our experiments, ECoRAG only\nadds additional 880M VRAM. Furthermore, since\nthe compressor and evaluator can operate sequen-\ntially as well as simultaneously with the reader,\nECoRAG remains feasible in traditional RAG envi-\nronments.MethodsCompression\nTimeInference\nTimeTotal\nTimeThroughput\n(example/sec)\nstandard RAG - 8.55h 8.55h 0.08\nECoRAG (ours) 0.51h 2.94h 3.45h 0.20\nTable 10: Inference time and compression time for NQ\ntest under worst case scenarios.\nIn terms of latency, Table 4 shows that ECoRAG\nis more efficient than traditional RAG, but addi-\ntional verification is needed across different cases.\nThe additional modules—compressor and evalu-\nator—may seem to increase system complexity.\nHowever, traditional RAG must process the entire\nlong context, while ECoRAG reduces latency by\n7.32h, as shown in Table 4. Table 9 shows that EC-\noRAG requires little time for compression, reduc-\ning the risk of bottleneck as the preceding modules\nprocess efficiently. In the worst case, ECoRAG\nevaluates compression multiple times, leading to\nlonger latency than in the best case. However, even\nin the worst case, Table 10 demonstrates that EC-\noRAG is still faster than traditional RAG.\nA.7 Case study of evidentiality-guided\ncompression\nTable 11 illustrates an example of evidentiality-\nguided compression. For the given question, who\ndies at the end of Den of Thieves? with the correct\nanswer Merrimen , the initial document set before\ncompression includes the correct answer. But it\nalso contains irrelevant information, which mis-\nleads the LLM into generating the wrong answer,\nDonnie. After compression, irrelevant content con-\ntaining the word Donnie is effectively suppressed,\nleaving only the evidential (highlighted) sentences.\n15\n--- Page 16 ---\nQuestion Gold answers\nwho dies at the end of den of thieves Merrimen\nType In-context documents Prediction\nNone Donnie\nretrieved\ndocumentsDen of Thieves (film) Nick, forcing Nick to shoot him. AsMerrimen liesontheground\ndying,Nick kneels andconsoles him. When Nick inspects Merrimen ’s SUV , he only finds\nbags with shredded paper; he also finds that Donnie has escaped custody. Nick later goes to\nDonnie ’s bar and sees pictures of him with some of the crew members from the heist. It is\nrevealed Donnie masterminded the heist to keep all of the stolen cash for himself in a second\ngarbage truck. After the passage of some time, Donnie is working in a London bar, planning\na new heist. The film was in Den of Thieves (film) is currently in development. In Los\nAngeles, a team of robbers led by Ray Merrimen make a violent armed attack and hijack an\narmored truck. Police officers arrive on the scene and engage in a shootout with the robbers.\nEventually, Merrimen and his crew escape with the empty armored truck. In the morning,\nDetective Nick O’Brien investigates the crime scene, having been monitoring Merrimen and\nhis crew for a while. Suspecting a local bartender named Donnie for involvement, Nick finds\nhim at the bar and kidnaps him for interrogation. Donnie reveals Merrimen is planning\nto rob the Federal Reserve on Den of Thieves (film) garbage truck that removes shredded\nbills. Nick’s team catches up to Donnie and seizes him, beating him until he tells them\nwhere Merrimen is going. Merrimen , Bosco, and Levi try to make their escape with the\nmoney bags from the waste truck but hit a traffic jam and are blocked. Nick’s team spots\nthem and attempt to shoot them as the robbers try to escape. A shootout occurs initiated\nbyMerrimen , killing one of Nick’s men. Levi and Bosco are eventually shot dead, but\nMerrimen gets away. Nick chases and shoots Merrimen , wounding him. Merrimen raises\nan empty gun to Den of Thieves (film) is currently in development. In Los Angeles, a team\nof robbers led by Ray Merrimen make a violent armed attack and hijack an armored truck.\nPolice officers arrive on the scene and engage in a shootout with the robbers. Eventually,\nMerrimen and his crew escape with the empty armored truck. In the morning, Detective\nNick O’Brien investigates the crime scene, having been monitoring Merrimen and his crew\nfor a while. Suspecting a local bartender named Donnie for involvement, Nick finds him\nat the bar and kidnaps him for interrogation. Donnie reveals Merrimen is planning to rob\nthe Federal Reserve on Den of Thieves (film) Friday of that week by covertly removing\nabout $30 million in old bills which are scheduled to be shredded after their serial numbers\nare deleted from computer records. At their hideout, Merrimen has one of his crew, Levi,\nroughly interrogate Donnie to ensure he didn’t disclose anything about the plan. Meanwhile,\nNick goes to a strip club and finds Merrimen ’s stripper girlfriend, hiring her for the night to\nfind out where the heist is going to happen. The next morning, Nick makes an effort to see\nhis daughter at her school. As the day of the heist comes, Merrimen andDonnie\nCompression Den of Thieves (film) AsMerrimen liesontheground dying,Nick kneels andconsoles him.\nDen of Thieves (film) Eventually, Merrimen and his crew escape with the empty armored\ntruck. Den of Thieves (film) Merrimen , Bosco, and Levi try to make their escape with\nthe money bags from the waste truck but hit a traffic jam and are blocked. Den of Thieves\n(film) In the morning, Detective Nick O’Brien investigates the crime scene, having been\nmonitoring Merrimen and his crew for a while. Den of Thieves (film) Meanwhile, Nick\ngoes to a strip club and finds Merrimen ’s stripper girlfriend, hiring her for the night to find\nout where the heist is going to happen.Merrimen\nTable 11: Case study of how the compression of the retrieved documents helps the model to identify the correct\nanswer from NQ test set. The highlighted part is the evidential sentence that directly gives useful information for\ngenerating the correct answer Merrimen , rather than the incorrect answer Donnie .\nA.8 Generalizability across Readers\nTo evaluate the generalizability of our compres-\nsion framework, we conducted experiments using\nFlan-UL2 (Tay et al., 2023) (20B), Llama3 (Dubey\net al., 2024) (8B), and Gemma2 (Team et al., 2024)\n(9B) as the reader LLMs. These models were cho-\nsen to investigate how our method performs across\ndiverse architectures and parameter sizes.\nFlan-UL2 was selected because RECOMP also\nutilizes it, as we intend to directly compare with\nit. Furthermore, additional experiments were con-\nducted with Llama3 and Gemma2 to extend theevaluation. Since Llama3 has large context length,\nit can conduct ‘standard RAG’ experiment, unlike\nFlan-UL2 and Gemma2.\nResults show that our evidentiality-guided com-\npression method consistently outperforms other\ncompression baselines on all three models. Specifi-\ncally, with Flan-UL2 in Table 12, which was used\nto define evidentiality during training, the model\ndemonstrated a clear improvement across all met-\nrics. Similarly, as shown in Table 13. Gemma2,\ndespite being trained without its own evidentiality\nmining, also showed improved performance with\n16\n--- Page 17 ---\nMethodsNQ TQA WQ\n#tokens ↓ EM F1 #tokens ↓ EM F1 #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 21.33 28.71 0 46.48 52.47 0 32.97 42.33\nstandard RAG (100 documents) 15456 - - 15943 - - 15135 - -\nRAG with 100 documents compressed\nLLMLingua 725 19.17 25.48 726 42.97 48.93 868 31.10 40.87\nLLMLingua-2 1475 24.63 32.19 1518 53.07 59.42 1580 30.61 41.76\nLongLLMLingua 1516 38.03 46.94 1570 65.79 73.88 1629 32.78 45.27\nRECOMP (extractive) 727 38.06 46.18 750 62.49 69.68 857 31.25 43.18\nRECOMP (abstractive) 16 22.22 29.56 30 43.50 49.88 157 38.15 38.56\nCompAct 252 42.16 51.05 253 64.37 72.25 218 33.07 44.45\nECoRAG (ours) 693 44.38 53.56 501 66.45 74.02 671 33.71 46.08\nTable 12: Comparison of compression methods on NQ, TQA, and WQ using Flan-UL2 (Tay et al., 2023) with 100\nretrieved documents (Karpukhin et al., 2020).\nMethodsNQ TQA WQ\n#tokens ↓ EM F1 #tokens ↓ EM F1 #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 27.84 38.35 0 57.11 66.39 0 26.77 43.24\nstandard RAG (100 documents) 14260 - - - - - 14075 - -\nRAG with 100 documents compressed\nLLMLingua 643 26.90 37.90 638 60.71 68.09 649 25.04 42.08\nLLMLingua-2 1403 28.56 38.95 1393 59.95 67.84 1401 24.36 40.52\nLongLLMLingua 1411 37.67 49.40 1436 63.17 70.28 1399 27.02 44.23\nRECOMP (extractive) 165 37.65 48.24 687 63.19 70.38 680 26.03 42.22\nRECOMP (abstractive) 17 27.98 38.00 28 58.78 65.74 21 25.20 41.60\nCompAct 111 38.67 49.87 100 65.88 73.29 78 26.67 43.04\nECoRAG (ours) 684 39.20 50.24 448 66.32 74.25 504 27.41 44.00\nTable 13: Comparison of compression methods on NQ, TQA, and WQ using Gemma2 (Team et al., 2024) with 100\nretrieved documents (Karpukhin et al., 2020).\nMethodsNQ TQA WQ\n#tokens ↓ EM F1 #tokens ↓ EM F1 #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 22.16 32.36 0 60.89 67.80 0 21.79 35.81\nstandard RAG (100 documents) 14263 0.27 0.97 14574 0.24 2.70 14147 0.25 4.48\nRAG with 100 documents compressed\nLLMLingua 641 15.20 22.31 636 52.11 59.23 646 17.62 30.92\nLLMLingua-2 1346 3.91 7.19 1366 48.08 55.91 1337 4.28 11.44\nLongLLMLingua 1388 20.30 28.85 1423 58.34 68.49 1372 18.70 32.12\nRECOMP (extractive) 160 22.33 31.12 683 36.69 44.08 667 16.19 27.80\nRECOMP (abstractive) 16 18.75 27.85 27 42.73 50.94 21 18.80 33.25\nCompAct 107 28.01 38.52 99 56.01 64.69 76 21.41 35.21\nECoRAG (ours) 519 30.22 42.55 445 59.25 69.32 588 21.60 35.43\nTable 14: Comparison of compression methods on NQ, TQA, and WQ using Llama3 (Dubey et al., 2024) with 100\nretrieved documents (Karpukhin et al., 2020).\n17\n--- Page 18 ---\nMethods #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 31.88 44.10\nstandard RAG (100 documents) 13,847 37.11 50.82\nRAG with 100 documents compressed\nLLMLingua 645 25.79 37.56\nLLMLingua-2 1,319 29.95 44.40\nLongLLMLingua 1,364 33.44 46.20\nRECOMP (extractive) 659 33.21 45.98\nRECOMP (abstractive) 16 30.45 43.01\nCompAct 75 37.69 51.65\nECoRAG (ours) 641 41.43 54.02\nTable 15: Experimental results on the NQ dataset using\nGPT-4o-mini, comparing performance with and without\ncompression for documents retrieved by Contriever.\nour compression method, further validating its ef-\nfectiveness.\nIn the case of Llama3, as presented in Ta-\nble 14, our compression approach outperformed\nother baselines, including naive prepend. However,\nin certain instances, it was outperformed by the\n‘closed book’ approach. This suggests that paramet-\nric knowledge embedded within the reader LLM\ncan occasionally align well with specific datasets,\nleading to variations in performance across models.\nNonetheless, our framework ECoRAG is model-\nagnostic, as we have excluded the influence of the\nparametric knowledge of the reader LLM in mining\nevidentiality labels. These results emphasize that\nour compression method consistently outperforms\nother compression approaches, further validating\nits effectiveness across diverse models and configu-\nrations.\nA.9 Generalizability across Retrievers\nTo verify that our compression approach general-\nizes beyond DPR, we conducted additional experi-\nments using another retriever. Our initial choice of\nDPR was intentional, in order to demonstrate the\nrobustness of our compression approach even un-\nder challenging conditions where a weaker retriever\ncould introduce significant noise. In Table 15, we\nthen evaluated our method with Contriever (Izacard\net al., 2022), a stronger dense retriever. The results\nshow an even larger performance gain when paired\nwith Contriever than with DPR, indicating that\nECoRAG synergizes especially well with higher-\nquality retrieval.\nTo compare against a simple baseline of re-\ntrieving fewer documents, we evaluated ECoRAG\nagainst varying retrieval sizes. We initially chose\n100 documents to align with standard practice in\nprior work, such as Fusion-in-Decoder (IzacardMethods#tokens\n(DPR)EM\n(DPR)#tokens\n(Contriever)EM\n(Contriever)\nReduced retrieval size\n# docs (k) = 5 693 35.53 690 33.48\n# docs (k) = 10 1,386 35.95 1,381 34.76\n# docs (k) = 20 2,774 36.33 2,762 36.47\nAdaptive compression\nECoRAG (ours) 632 36.48 641 41.43\nTable 16: Experimental results on the NQ dataset us-\ning GPT-4o-mini, comparing reduced retrieval sizes for\nDPR and Contriever against adaptive compression via\nECoRAG.\nMethods #tokens ↓ EM F1\nRAG without compression\nclosed-book 0 26.19 36.71\nstandard RAG (100 documents) 14,313 34.52 44.69\nRAG with 100 documents compressed\nLLMLingua 636 22.57 31.54\nLLMLingua-2 1,330 26.66 37.00\nLongLLMLingua 1,406 27.45 38.07\nRECOMP (extractive) 688 28.05 38.87\nRECOMP (abstractive) 12 24.27 33.88\nCompAct 74 31.21 42.42\nECoRAG (ours) 647 34.69 45.13\nTable 17: Experimental results on the HotpotQA dataset\nusing GPT-4o-mini, comparing performance with and\nwithout compression for 100 documents (Karpukhin\net al., 2020).\nand Grave, 2021). In Table 16, we then conducted\nexperiments on varying numbers of retrieved docu-\nments ( k= 5, 10, and 20) for both DPR and Con-\ntriever, measuring token count and EM without\ncompression. Our results show that reducing k\ndoes not improve accuracy as effectively as adap-\ntive compression via ECoRAG, emphasizing the\nbenefit of our evidentiality-guided approach.\nA.10 Evaluation in Multi-hop QA\nTo assess the effectiveness of ECoRAG in multi-\nhop QA tasks requiring multiple evidence sources,\nwe conducted experiments in Table 17. ECoRAG\nclassifies evidentiality into three categories and\ndefines weak evidence that supports the correct\nanswer without directly generating the answer.\nThis enables ECoRAG to perform effectively in\ntasks requiring partial evidence, such as multi-hop\nQA. Furthermore, according to CompAct, adap-\ntively adjusting evidence can collect the partial evi-\ndence needed for multi-hop QA, ECoRAG achieves\nthrough Evidentiality Reflection.\nTable 17 shows that ECoRAG outperformed\nboth non-compressed and other compression base-\nlines in HotpotQA (Yang et al., 2018). CompAct\nand other baselines did not outperform the “stan-\ndard RAG” approach, which uses all 100 docu-\n18\n--- Page 19 ---\nMethod HotpotQA MusiQue\nstandard RAG 49.76 23.91\nRECOMP 49.12 23.08\nECoRAG (ours) 52.29 24.60\nTable 18: Experiments on LongBench multi-hop\ndatasets (HotpotQA and MusiQue) evaluating F1-score\nperformance of standard RAG, RECOMP, and ECoRAG\nusing Llama3-8B.\nments without compression. In contrast, ECoRAG\nimproved performance by removing distractors and\nkeeping necessary evidence. These results show\nthat ECoRAG is effective for complex scenarios\nsuch as multi-hop QA.\nTo evaluate ECoRAG in scenarios where the\nchallenge lies not only in the number but also in\nthe length of retrieved documents, we applied our\nmethod to the LongBench (Bai et al., 2024) bench-\nmark. LongBench is a long-context understand-\ning benchmark covering tasks such as HotpotQA6\nand MuSiQue (Trivedi et al., 2022). In Table 18,\nwe compared standard RAG, RECOMP, and EC-\noRAG (using Llama3-8B) across these tasks within\nLongBench. Consistent with our multi-hop results,\nECoRAG outperformed both non-compressed and\ncompression baselines in this long-document set-\nting, further demonstrating its robustness and effec-\ntiveness.\nB Experimental Details\nB.1 Implementation Details\nWe used 8 Nvidia RTX3090 GPUs to train all mod-\nels. For mining evidentiality labels for all sentences\nin retrieved documents, we used the NLTK library7\nto split DPR (Karpukhin et al., 2020) retrieved top-\n100 documents into sentences. To reduce costs,\nwe used the open LLM Flan-UL28(Tay et al.,\n2023), which was also used in our experiments and\nRECOMP (Xu et al., 2024), to label evidentiality\nbased on the definition in Section 3.1.1.\nOur evidentiality compressor was trained from\nContriever (Izacard et al., 2022) checkpoint pre-\ntrained on CC-net (Wenzek et al., 2020) and En-\nglish Wikipedia (Izacard et al., 2022).. We trained\nit using the AdamW optimizer with a batch size of\n6LongBench does not include the full original datasets, so\nour result in HotpotQA results may differ from those reported\nin Table 17.\n7www.nltk.org\n8google/flan-ul2Subset Accuracy Precision Recall F1-score\nexposed subset 68.52 81.47 64.17 72.13\nnon-exposed subset 71.77 78.64 76.22 77.42\nTable 19: Evaluation results of the Flan-T5-based evi-\ndentiality evaluator on TQA, comparing performance\nbetween the exposed and non-exposed subsets.\n64 and a learning rate of 5·10−5for 4 epochs on\nNQ (Kwiatkowski et al., 2019) and WQ (Berant\net al., 2013), and 2 epochs on TQA (Joshi et al.,\n2017). While training with LweandLselosses,\nwe we used 8 positive contexts and 56 negative\ncontexts per batch. When calculating the Lseloss,\nwe used negative set with weak evidence to dis-\ntractor ratio of 0.15:0.85, treating weak evidence\nas hard negative. We set the temperature τfor the\ncontrastive loss to 1.0.\nOur evidentiality evaluator was trained from\na pretrained Flan-T5-large checkpoint9using the\nAdamW optimizer. We trained it with a batch size\nof 40 and a learning rate of 1·10−5for 4 epochs\nwith all datasets. We included ‘<NOT>’ sentences\nwith high compressor scores in the training stage\nto make the evidentiality evaluator distinguish only\nthe genuinely strong evidence ‘<EVI>’ from the\nseemingly plausible ones. We constructed the train-\ning data for the evaluator with a ratio of 1:3 be-\ntween ‘<EVI>’ and ‘<NOT>’ sentences. For adap-\ntive compression, a limit on the number of evidence\npieces was necessary to avoid infinite loops, which\nwe set at 20. We set this limit to 20 to achieve\na compression level similar to RECOMP, but it\ncan be increased for tasks that require more evi-\ndence. Additionally, to prevent high latency due\nto overly frequent evaluations, we incrementally\nadded 4 evidence pieces at a time. For experiments\non the test set, we used GPT-4o-mini10, Flan-UL2,\nGemma211, and Llama312.\nB.2 Selection for Evidentiality Evaluator\nWe chose Flan-T5-large as the basis for our Evi-\ndentiality Evaluator due to its strong instruction-\nfinetuning and robust performance on classifica-\ntion tasks. T5-large has been widely used in prior\nRAG research (Han et al., 2023; Yan et al., 2024)\nfor document-based evaluation. For our eviden-\ntiality scoring task, we employ Flan-T5-large as it\ndemonstrates enhanced instruction-following capa-\n9google/flan-t5-large\n10gpt-4o-mini-2024-07-18\n11google/gemma-2-9b-it\n12meta-llama/Meta-Llama-3-8B-Instruct\n19\n--- Page 20 ---\nbilities that are well-suited for this classification\ntask. However, a potential concern arises regard-\ning Flan-T5-large’s prior exposure to datasets such\nas NQ, TQA, and WQ, which might lead to mem-\norization rather than genuine evidentiality learn-\ning. As reported in research related to Flan-T5-\nlarge (Chung et al., 2022; Wang et al., 2022), its\nexposure was clearly separated into exposed and\nnon-exposed subsets, and our comparison experi-\nments (Section 5.2) demonstrate negligible perfor-\nmance differences between these groups. When\nwe applied a two-proportion Z test to the results in\nTable 19, the analysis at the 0.05 significance level\nconfirmed that the observed differences are not sta-\ntistically significant. Therefore, these observation\nindicates that the model has learned evidentiality\nprinciples rather than simply memorizing evidence\nfrom prior exposure.\nB.3 Input Prompts for LLM\nWe report two examples of input prompts for reader\nLLMs. In Figure 6, we report the input prompt\nused for evidentiality mining and test set experi-\nments to answer a given question when provided\nwith the question and the compressed documents.\nThis prompt was also utilized during the evidential-\nity mining process, as described in Section 3.1.1.\nFigure 7 presents the input prompt for mining the\nground truth label of compressed documents us-\ning Flan-UL2 as the evidentiality evaluator in the\nexperiments detailed in Section 5.2.\nC Usage of AI Assistants\nWe utilized ChatGPT to improve the clarity and\ngrammatical accuracy of my writing. It provided\nsuggestions for rephrasing sentences and correct-\ning grammatical errors to make the text flow more\nnaturally.\n20\n--- Page 21 ---\nQuestion Answering Prompt\nwho won a million on deal or no deal\nAnswer: Tomorrow Rodriguez\nwho is the woman washing the car in cool hand luke\nAnswer: Joy Harmon\nwho is the actor that plays ragnar on vikings\nAnswer: Travis Fimmel\nwho said it’s better to have loved and lost\nAnswer: Alfred , Lord Tennyson\nname the first indian woman to be crowned as miss world\nAnswer: Reita Faria\nDocuments\nQuestion\nAnswer:\nFigure 6: An input prompt for LLM for question answering, including few-shot examples, input documents, and a\nquestion.\n21\n--- Page 22 ---\nEvidentiality Evaluation Prompt\nYou are an expert at determining whether a document provides evidential support for a given\nquestion. You will receive a question and a document, and your task is to evaluate whether the\ndocument is evidential, partially evidential, or non-evidential in relation to the question.\nAssess the support provided by the document using the following scale:\n- [Evidential] - The document fully supports the question, providing clear and direct evidence that\nanswers or addresses the query completely.\n- [Non-Evidential] - The document does not provide relevant information or evidence related to the\nquestion, making it unrelated or insufficient to support the query.\nPlease provide your assessment and briefly justify your reasoning based on the content of the\ndocument in relation to the question.\nQuestion: what is the temperature of dry ice in kelvin?\nEvidence: At atmospheric pressure, sublimation/deposition occurs at or 194.65 K. The density of\ndry ice varies, but usually ranges between about.\nScore: [Evidential]\nQuestion: when did north vietnam unify with the south?\nEvidence: The distinctive synthesizer theme was performed by the then-little-known Thomas\nDolby, and this song also marked a major departure from their earlier singles because their\nprevious singles were mid to upper tempo rock songs while this song was a softer love song with\nthe energy of a power ballad.\nScore: [Non-Evidential]\nQuestion: who played all the carly ’s on general hospital?\nEvidence: Throughout the 2000s, Carly, then Tamara Braun (2001–05) goes on to become one of\nthe\nScore: [Non-Evidential]\nQuestion: who sang the original blinded by the light?\nEvidence: Light of Day (song) \"Light of Day\", sometimes written as \"(Just Around the Corner to\nthe) Light of Day\", is a song written by Bruce Springsteen and performed initially by Joan Jett and\nMichael J.\nScore: [Non-Evidential]\nQuestion: who was the rfc editor until 1998 just provide the family name?\nEvidence: Perhaps his most famous legacy is from RFC 760, which includes a robustness principle\noften called \"Postel’s law\": \"an implementation\nScore: [Non-Evidential]\nQuestion: Question\nEvidence: Compressed Documents\nScore:\nFigure 7: An input prompt for LLM for evidentiality evaluation, including few-shot examples, compressed\ndocuments, and a question.\n22",
  "text_length": 82329
}