{
  "id": "http://arxiv.org/abs/2506.01049v1",
  "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
  "summary": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.",
  "authors": [
    "Siyuan Li",
    "Juanxi Tian",
    "Zedong Wang",
    "Xin Jin",
    "Zicheng Liu",
    "Wentao Zhang",
    "Dan Xu"
  ],
  "published": "2025-06-01T15:30:37Z",
  "updated": "2025-06-01T15:30:37Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01049v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01049v1  [cs.LG]  1 Jun 2025Taming LLMs by Scaling Learning Rates with Gradient Grouping\nSiyuan Li1,2⋆, Juanxi Tian2,4⋆, Zedong Wang3⋆, Xin Jin2,\nZicheng Liu1,2†,Wentao Zhang4,Dan Xu3\n1Zhejiang University,2Westlake University,\n3The Hong Kong University of Science and Technology,4Peking University\nAbstract\nTraining large language models (LLMs) poses\nchallenges due to their massive scale and het-\nerogeneous architectures. While adaptive opti-\nmizers like AdamW help address gradient varia-\ntions, they still struggle with efficient and effec-\ntive parameter-wise learning rate estimation, re-\nsulting in training instability, slow convergence,\nand poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work intro-\nduces Scaling with Gradient Grouping ( SGG ),\nan optimizer wrapper that improves adaptive\nlearning rate estimation by dynamic grouping\nand group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters\nand then applies cluster-specific scaling to cali-\nbrate learning rates for each parameter, thus im-\nposing collective group-wise constraints while\nmaintaining precise per-parameter adaptation.\nExperiments on diverse (M)LLM benchmarks\nshow that SGG integrates seamlessly with exist-\ning optimizers, and offers consistent gains and\nfaster convergence over baselines, with various\nmodel sizes. Its stability across varying batch\nsizes and learning rates establishes SGG as a\nrobust choice for LLM optimization.\n1 Introduction\nOptimization algorithms have long been the corner-\nstone of deep learning systems. Among these, adap-\ntive optimizers (Kingma and Ba, 2015; Loshchilov\nand Hutter, 2019; You et al., 2020) stand out for\ntheir ability to adjust individual learning rates for\neach parameter, enabling effective training of large\nlanguage models (LLMs) with heterogeneous archi-\ntecture (Liu et al., 2020b; Zhang et al., 2025b). Yet,\ntheir reliance on per-parameter statistics ( e.g., first\nand second moments of gradient) incurs substantial\nmemory overhead, which limits their application,\nespecially in resource-constrained scenarios.\n⋆Equal contribution\n†Corresponding author: Zicheng Liu\nmn\nCluster 1       Cluster 2        Cluster 3GradientDeviation3 Scale Factors for i-th LayerLayerMean…All Network Layers of LLM\nGlobalMedian Dev.i-thLayerMinMaxMeanGlobalStatisticsFigure 1: Scaling with Gradient Grouping. Illustra-\ntion of SGG with online grouping and group-specific\nlearning rate (LR) scaling upon adaptive LR optimizers.\nTo address this, parameter-efficient fine-tuning\n(PEFT) (Hu et al., 2021; Dettmers et al., 2024)\nhas garnered increasing attention, which reduces\ntrainable parameters via low-rank updates. While\nmemory-efficient, PEFT incurs performance degra-\ndation compared to full-rank training (Table 4) and\nrequires architecture modification. In parallel, ef-\nforts have been made to compress optimizer states\ndirectly, e.g., by low-bit quantization or approxi-\nmating gradient statistics (Shazeer and Stern, 2018;\nLuo et al., 2023; Zhu et al., 2024a). However, these\ngeneric approaches typically rely on heuristic pri-\nors that might discard crucial information, resulting\nin inconsistent efficacy across tasks (Table 5). This\nleaves practitioners at a deadlock: the compromise\nof performance in LLM training seems inevitable.\nRecent studies light the way by revealing that dif-\nferent layers in LLMs ( e.g., attention and MLP) ex-\nhibit distinct yet internally consistent optimization\nbehaviors (Li et al., 2024c; Zhang et al., 2025b),\nsuggesting potential redundancy in existing adap-\ntive methods. Adam-mini (Zhang et al., 2024) thus\ndivides model parameters into pre-defined groups –\neach assigned an average learning rate instead, with\nminimal performance drop versus prior works.\n--- Page 2 ---\nGradient(Momentum)DensityAdamSGG(Cluster1)SGG(Cluster2)(a) Gradiant Distribution\nLearningRateDensityAdamSGG(Cluster1)SGG(Cluster2) (b) Learning Rate Distribution\nAdamSGG(Cluster1)SGG(Cluster2)\nGradNormDensity (c) Grad Norm Distribution\nFigure 2: Clusters of gradient statistics with LLaMA-1B pre-training on C4. Distributions of (a)parameter-wise\ngradients gtand(b)learning rates αtfor12-th FFN layer at 5kiterations. SGG identifies diverse clusters compared\nto Adam (in gray), introducing group constraints while maintaining parameter-wise adaptation. (c)Distribution of\ngradient L2-norms across layers, showcasing SGG’s ability to adapt to LLMs’ heterogeneity (Zhang et al., 2025b).\nIn this work, we propose to scale learning rates\nwith grouping constraints rather than replace them.\nWe first conduct pilot studies on LLaMA-1B pre-\ntraining to gain intuition. As shown in Figure 2, dis-\ntinct clustering patterns are observed in layer-wise\ngradient statistics, which aligns with previous find-\nings. However, they also exhibit noticeable charac-\nteristics ( e.g., significant parameter-wise variations\nwithin each cluster, Sec. 2.2). This suggests that,\nwhile grouping is viable, retaining parameter-wise\nadaptation could still be beneficial for effective\nLLM training, especially in terms of performance,\ninstead of replacing it with a single learning rate per\ngroup. Thus, we introduce Scaling with Gradient\nGrouping ( SGG ), an optimizer wrapper to bridge\nper-parameter and group-wise learning rate control.\nAs illustrated in Figure 1 and Algorithm 1, SGG\ndynamically clusters gradient statistics (specifically\nmomentum vectors ml) in each layer l(Sec. 2.2)\nand then performs cluster-specific scaling accord-\ning to their deviation relative to that layer’s and the\nentire model’s global statistics (Sec. 2.3), which\nthus imposes grouping constraints for homogeniza-\ntion while maintaining parameter-wise adaptability.\nExperiments demonstrate that SGG consistently\ndelivers performance gains and accelerates conver-\ngence across different LLM (Sec. 3.2) and MLLM\n(Sec. 3.3) benchmarks, such as pre-training on C4,\nsupervised fine-tuning (SFT) on GLUE, PEFT on\ncommonsense reasoning tasks, and Direct Pref-\nerence Optimization (DPO). For instance, Adam\ncombined with SGG could surpass recent opti-\nmizers on C4 pre-training across diverse model\nsizes (from 60M to 1B). More importantly, SGG\nenables low-rank pre-training to match full-rank\nperformance without modifications to the training\npipeline, yielding up to 30.4%lower validation per-\nplexity over LoRA baselines – a huge step forward\nas previous low-rank optimizers often struggled.\nOur contributions are as follows:•We present SGG, a flexible optimizer wrapper\nthat scales adaptive learning rates with online\ngrouping constraints rather than replace them in\npre-defined groups, balancing parameter-wise\ndynamics and collective optimization behavior.\n•In practice, SGG integrates seamlessly with ex-\nisting optimizers and PEFT techniques, requir-\ning no changes to the training pipeline or model\narchitectures. We also provide CPU, GPU, and\nhybrid implementations for different demands.\n•SGG’s consistent effectiveness shows the po-\ntential of scaling adaptive learning rates with\ngroup-wise constraints. While SGG offers an\nintuitive instantiation of this scheme, different\ngrouping and scaling strategies are conceivable\nand might inspire future studies along this line.\n2 Methodology\n2.1 Preliminaries and Problem Definition\nTo demonstrate the plug-and-play integration of our\nSGG, we first outline the essential steps in gradient-\nbased optimizers, marked in blue in Algorithm 1.\nThe process typically begins with gradient com-\nputation. At iteration t, the gradient gt\nlof objective\nLw.r.t. parameters θt−1\nlof layer lis calculated as:\ngt\nl=∇θt−1\nlL(θt−1\nl,D) (1)\nwhere Ddenotes the training dataset, and θt−1\nlis\nfrom previous iteration. Subsequently, historical\ngradient information is incorporated to stabilize the\nupdate, commonly referred to as momentum mt\nl.\nWhile vanilla SGD (Sinha and Griscik, 1971) uses\nthe current gradient instead ( mt\nl=gt\nl), momentum-\nbased methods often employ an exponential mov-\ning average (EMA) to smooth estimates over time:\nmt\nl=MomentumEstimate (gt\nl, mt−1\nl, β1)(2)\nwhere mt−1\nlis from the last iteration, and the EMA\ndecay β1controls the retention of past gradients.\n--- Page 3 ---\nTable 1: Overview of typical optimizers (Opt.), PEFT techniques, and plug-and-play optimizer wrapper. We\nconsider a neural net layer W∈Rm×n(m≤n)with LoRA rank r≪mand SGG clusters K≪m. Both weights\nand optimizer states are included. (i)Optimization states. We compare the adaptive Learning Rate (LR) costs, i.e.,\nthe extra state for its estimation ( e.g., second moment, Non-negative Matrix Factorization (NMF), and SGG’s cluster\nindices). (ii)Different low-rank integration. (iii)Performance. We report the averaged PPL (%) ↓for C4 pre-training\nin Table 4 as the illustration of performance gains with the relative GPU memory to Adam (full-rank) in PyTorch.\nCategory Method Adaptive LR Basic State Extra State Low-Rank Plugin Extra Branch C4↓GPU Memory\nClassical Opt. SGD ✗ Weight & Grad. ✗ ✗ ✗ ✗ − 2mn\nAdaptive LR Opt. Adam Param-wise mn Weight & Grad. 2nd-Moment mn ✗ ✗ ✗ 23.36 3mn\nEfficient Opt. CAME Param-wise mn Weight & Grad. NMF 2(m+n) NMF ✗ ✗ -1.64 2mn+2(m+n)\nPEFT LoRA ✗ Full-rank Grad. ✗ LoRA ✓ r(m+n)+5.06 +3r(m+n)\nOpt. Wrapper SGG Group-wise K Base Opt. Indices (mn+K)Clustering ✓ ✗ -1.99 +0\nAlgorithm 1 Scaling with Gradient Grouping\nRequire: Parameters {θl}L\nl=1, global learning rate sched-\nuleη, optimizer hyperparameters (β1, β2), objective L,\ndataset D. SGG hyperparameters: cluster number K,\nrecluster interval T, scaling EMA decay β3.\nEnsure: Optimized model parameters θ.\n1:Initialize:\n2:RandomInit ({θ0\nl}L\nl=1) ▷Model parameters\n3:{α0\nl}L\nl=1←η ▷ Adaptive learning rates\n4:{Cl}L\nl=1←0 ▷Cluster assignment\n5:{Sl}L\nl=1←1 ▷Cluster scaling factor\n6:foreach iteration t= 1,2, . . . do\n7: ηt←LRScheduler( η,t)\n8: foreach layer l= 1,2, . . . , L do\n9: // — Standard Gradient-based Update Steps —\n10: Gradient Computation\n11: gt\nl← ∇θt−1\nlL(θt−1,D)\n12: Momentum Estimation\n13: mt\nl←MomentumEstimate (gt\nl, mt−1\nl, β1)\n14: Adaptive Learning Rate Estimation\n15: αt\nl←LREstimate (αt−1\nl, mt\nl, β2, ηt)\n16: // — SGG Specific Steps —\n17: iftmod T== 0 then ▷Re-clutering\n18: Assign Gradient Clusters\n19: Ct\nl←GradCluster (mt\nl, K)\n20: Update Cluster Scaling Factors\n21: St\nl←ScaleUpdate (Ct\nl, mt\nl, β3)\n22: end if\n23: Apply Learning Rate Scaling\n24: αt\nl←αt\nl· St\nl[Ct\nl] ▷Cluster-specific scaling\n25: Parameter Update\n26: θt\nl←θt−1\nl−αt\nl·mt\nl\n27: end for\n28: end for\nAdaptive learning rate algorithms ( e.g., Adam\nand AdaGrad) further refine the process by calculat-\ning parameter-wise or layer-wise second-moment\nestimates of gradients to calibrate step sizes:\nαt\nl=LREstimate (αt−1\nl, mt\nl, β2, ηt) (3)\nwhere ηtindicates the global learning rate set by\nscheduler at iteration t, and β2is the EMA decay\nlikeβ1. Non-adaptive methods, in contrast, simply\nuse the global one instead ( αt\nl=ηt). Note that this\nlearning rate adaptation typically increases memory\noverhead, particularly for large-scale models – a\nkey challenge that most prior works aim to address.In the last step, model parameters θlare updated\nby the learning rate scaled momentum ( αt\nl·mt\nl):\nθt\nl=θt−1\nl−αt\nl·mt\nl (4)\nThis paradigm is common in existing optimizers,\nmostly differing in how mt\nlandαt\nlare derived. No-\ntably, SGG (highlighted in green in Algorithm 1)\nbuilds upon this by leveraging these pre-computed\nstates from the base optimizer to impose grouping\nconstraints on parameter-wise αt\nl, ensuring effort-\nless integration with diverse optimizers, from SGD\nto APOLLO (Zhu et al., 2024a). In the follow-\ning sections, we discuss the specific grouping and\ngroup-wise learning rate scaling strategies in SGG.\n2.2 Gradient Grouping via Online Clustering\nIt has been observed that parameters in LLMs ex-\nhibit non-independent optimization behaviors, in-\nherently forming intra-correlated groups (Li et al.,\n2024c; Zhang et al., 2025b). To build intuitions for\nthis work, we first conduct an empirical analysis of\ngradient statistics with LLaMA pre-training on C4.\nPilot Studies. Figure 2 shows that gradient statis-\ntics, whether measured by layers or parameters,\nexhibit distinct clustering patterns, which aligns\nwith previous findings. However, a crucial aspect\nof these clusters is their internal diversity – they ex-\nhibit considerable parameter-wise variations within\neach group. Second, subtle yet significant deviation\ncan be identified in these cluster distributions when\nexamining different statistics, such as gradients in\nFigure 2(a) and learning rates in Figure 2(b).\nThese findings lead to the following considera-\ntions: (i)Since the clustering patterns differ across\noptimization statistics ( e.g., gradients vs. learning\nrates), methods relying on pre-defined fixed groups,\nsuch as Adam-mini (Zhang et al., 2024), might not\neffectively capture these distinct behaviors, sug-\ngesting the need for dynamic grouping strategies.\n(ii)While grouping has proven effective, replacing\n--- Page 4 ---\nparameter-wise learning rates simply with a single,\naggregated one per group (either pre-defined or dy-\nnamically derived) might not adapt to the observed\nparameter-wise variation, thus discarding essential\noptimization signals for effective LLM training.\nTo this end, we propose to scale learning rates\nthrough dynamic grouping rather than replace them\nin static groups, thereby imposing group constraint\nwhile maintaining parameter-wise adaptation.\nOnline Clustering. SGG begins with dynamic\ngrouping as GradCluster (mt\nl, K)in Algorithm 1,\nwhich partitions momentum vectors mt\nlwithin each\nlayer l∈LintoKgroups with related indices Ct\nl\naccording to their similarity. To achieve this, online\nclustering stands out as a straightforward solution,\nand the choice of specific clustering algorithms is\nthen crucial for both effectiveness and efficiency.\nAs such, we evaluate several potential strategies,\nincluding K-means, mini-batch K-means, Gaussian\nMixture Models (GMM), and DBSCAN. Ablation\nstudies in Figure 3 (perplexity versus training time)\nand Table 9 (hyper-parameters) show that mini-\nbatch K-means offer the most favorable trade-off\nbetween clustering quality and computational effi-\nciency. Thus, we select this as the default clustering\nimplementation of GradCluster (mt\nl, K)in SGG.\n2.3 Cluster-specific Learning Rate Scaling\nWe introduce ScaleUpdate (Ct\nl, gt\nl, β3)to calculate\nthe scaling factor St\nl[c]for each cluster c∈Kafter\ngrouping, which modulates learning rate αt\nl. This\ninvolves two sub-tasks: (i)measuring the statistics\nfor different levels of partitions ( e.g., clusters, lay-\ners, and even the global one); (ii)updating cluster-\nspecific scales St\nl[c]based on the above statistics.\nThis contrasts with the previous Adam-mini (Zhang\net al., 2024), which replaces per-parameter adaptive\nlearning rates with their group-wise means directly.\nTable 2: Group Statistics for SGG Scaling. Param.\nrefers to Adam-like baselines. Validation PPL ↓is re-\nported with LLaMA on C4. MDA yields the best result.\nStatistic Var. Var. Sign(Var.) Grad. Grad. Grad.\nMethod Param. Mean Mean L2-norm MAD MDA\n130M 25.08 22.76 22.67 22.58 24.62 22.18\n1B 15.56 14.63 14.68 14.66 14.58 14.30\nTo determine an effective measure for each clus-\ntercat layer l∈L, we examine several candidates\nin Table 2. These include: (1) Within-cluster met-\nrics: Mean in Adam-mini (Zhang et al., 2024), Vari-\nance, Sign of Variance in SignSGD (Bernstein et al.,\n2018), L2-norm in LARS (Ginsburg et al., 2018),and (2)Layer-aware metrics: Median Absolute De-\nviation (MAD). More importantly, recent studies\nshow that severe discrepancies exist between the\ntraining dynamics of shallow and deep LLM layers,\nresulting in sudden loss spikes (Chowdhery et al.,\n2023; Molybog et al., 2023), exploding/vanishing\ngradients (Wang et al., 2024; Zhu et al., 2025),\nwhere the model’s performance can deteriorate dra-\nmatically. This inspires us to incorporate a global\nperspective, beyond groups and layers, into SGG’s\nscaling process to promote training homogeneity.\nTo achieve this, we adopt Median of Deviation\nto Average (MDA). For each cluster c∈Kin layer\nlat iteration t, its MDA, denoted Dt\nl,c, quantifies\nthe median deviation of its constituent momentum\nmt\nl· Ct\nl[c]from the average of that layer, as:\nDt\nl,c=median\u0000\n|mt\nl· Ct\nl[c]−mean(mt\nl)|\u0001\n,(5)\nwhere Ct\nl[c]denotes the selection mask for param-\neters in cluster cof layer l. To obtain a robust ref-\nerence for homogenization, we compute a global\nMDADtwhich characterizes the typical parameter-\nwise deviation throughout the model. The scaling\nfactor Sl[c]of cluster cis then defined as the ratio\nof this global Dtto the cluster’s specific Dt\nl,cas:\nSt\nl[c] =Dt\nDt\nl,c+ϵ, (6)\nwhere ϵ= 10−8ensures numerical stability. Thus,\nclusters with a lower Dt\nl,c(more stable dynamics\nrelative to global Dt) receive a proportionally larger\nfactor St\nl[c]. Conversely, clusters with higher, diver-\ngent MDAs are scaled down. Hence, this promotes\nlearning homogeneity across layers and clusters,\nwhich mitigates discrepancies and suppresses diver-\ngent behavior that could lead to disruptive updates.\nTable 3: Gains vs Costs. Relative gains↑in PPL and\ncost↓in training time and peak GPU memory increase\nfor GPU, CPU, hybrid versions with LLaMA-1B on C4.\nMethod PPL Training Time Memory\nAdam 15.56 110h 7.8G\nAdam+ SGG (GPU) +6.5% (-1.00) +1.8% (+2h) +4.3G\nAdam+ SGG (CPU) +6.5% (-1.00) +8.2% (+9h) +0.0G\nAdam+ SGG (Hybrid) +6.5% (-1.00) +4.1% (+4h) +2.1G\nThese factors are then clamped to [0.1,10]and\nare updated periodically per Titerations using an\nEMA to smooth out short-term fluctuations:\nSt\nl[c] =β3·St−1\nl[c] + (1−β3)·Dt\nDt\nc,l+ϵ,(7)\n--- Page 5 ---\n05101520253035\nK-MeansMB K-MeansGMMDBSCANPPL (%)Training Time (x10h)\nK-Means      Mini-batch K-Means       GMM                DBSCAN14.2514.3015.1415.37122h119h140h134hFigure 3: Grouping Methods PPL-efficiency trade-\noffwith LLaMA-1B on C4. Blue bars show validation\nPerplexity (PPL ↓), and pink bars show training time.\nMini-batch K-means achieves the best trade-off.\nwhere β3is the EMA decay rate. Subsequently, per-\nparameter adaptive learning rates are multiplied by\ntheir corresponding group-wise scaling factors as\nαt\nl· St\nl[Ct\nl]in Algorithm 1. Table 2 shows the effec-\ntiveness of using MDA for global homogenization\nwhile maintaining parameter-wise adaptation ( e.g.,\n22.18 and 14.30 PPL for 130M and 1B models).\nAs for implementation, we consider two key\ntrade-offs between performance and efficiency. (i)\nClustering Strategies and Frequency: We eval-\nuate four common approaches: K-means (Mac-\nQueen et al., 1967), mini-batch K-means (Scul-\nley, 2010), GMM (Kambhatla and Leen, 1994),\nand DBSCAN (Ester et al., 1996). As depicted in\nFigure 3, mini-batch K-means offers the best bal-\nance between accuracy and computational cost. We\ntherefore adopt it as our default clustering strategy.\nWe also empirically set the interval Tas 10% of the\ntotal training iterations, as verified in Figure 6. (ii)\nStorage and Computation: As shown in Table 3,\nwe compare the performance, training time, and\nGPU memory of putting them on the GPU or CPU.\nWhile keeping {Cl}L\nl=1and{Sl}L\nl=1in CPU would\nnot slow the training, the peak GPU memory for\nonline clustering is significant. Consequently, as\nstated in Table 1, we utilize the CPU implementa-\ntion that stores additional optimization states on the\nCPU, which does not require extra GPU memory\nand increases the overall training time negligibly.\n3 Experiments\n3.1 Experimental Setup\nDatasets and Tasks. To evaluate the effective-\nness and versatility of SGG, we conducted ex-\nperiments on 20 public datasets, including large-\nscale natural language datasets, Visual Question\nAnswering (VQA), and multimodal LLM (MLLM)\nevaluation benchmarks. (1) Pre-training on C4:\nWe used the ensubset of the C4 dataset, a largeTable 4: C4 Pre-training with diverse LLaMA sizes\n(from 60M to 1B). Comparison of full-rank, memory-\nefficient, and low-rank optimizers. Validation Perplexity\n(PPL% ↓: lower is better) is reported. Bold andgreen\ntypes denote the best results and gains ↓of SGG ( blue\nback ground) over related baselines ( gray back ground).\nNote that †denotes the results borrowed from GaLore,\nwhile the others were reimplemented in this work.\nMethod Venue 60M 130M 350M 1B\nPre-training with Full-Rank Optimizers\nAdam†ICLR’15 34.06 25.08 18.80 15.56\nNAdam ICLR’18 35.86 28.88 19.24 15.78\nRAdam ICLR’20 30.43 25.17 19.13 15.65\nLAMB ICLR’20 33.04 24.37 18.26 15.84\nAdan TPAMI’23 32.01 23.14 17.32 14.70\nAdam+ SGG Ours 30.31 22.18 17.28 14.30\n∆Gains -3.75 -2.90 -1.52 -1.26\nPre-training with Memory-efficient Optimizers\nAdam-mini†ICLR’25 34.10 24.85 19.05 16.07\nAdafactor†ICML’18 32.57 23.98 17.74 15.19\nLow-Rank†arXiv’22 78.18 45.51 37.41 34.53\nCAME ACL’23 31.37 23.38 17.45 14.68\nCAME+ SGG Ours 30.15 22.91 17.09 14.35\n∆Gains -1.22 -0.46 -0.36 -0.33\nAPOLLO†MLSys’25 31.55 22.94 16.85 14.20\nAPOLLO+ SGG Ours 30.18 22.52 16.54 13.95\n∆Gains -1.37 -0.42 -0.31 -0.25\nLow-Rank Pre-training\nLoRA†ICLR’22 34.99 33.92 25.58 19.21\nReLoRA†ICLR’23 37.04 29.37 29.08 18.33\nGaLore†ICML’24 34.88 25.36 18.95 15.64\nLoRA+ SGG Ours 30.62 23.62 17.86 14.73\n∆Gains -4.37 -10.30 -7.72 -4.48\nTraining Tokens 1.1B 2.2B 6.4B 13.1B\ncleaned web corpus from Common Crawl filtered\nfor safety (Köpf et al., 2023), to assess SGG in\nLLM pre-training. (2) SFT on GLUE: We fine-\ntuned RoBERTa-base models on GLUE bench-\nmark. GLUE comprises a collection of NLP\ntasks, such as sentiment analysis, question an-\nswering, and textual entailment (Wang, 2018),\nproviding a standard measurement of generaliza-\ntion in the understanding capabilities of common\nlanguages. (3) PEFT on Commonsense Rea-\nsoning: Leveraging the LLM-Adapters frame-\nwork (Hu et al., 2023), we evaluated SGG’s com-\npatibility and performance with PEFT methods on\nLLaMA architecture across 8 Commonsense Rea-\nsoning (CS) datasets: BoolQ (Clark et al., 2019),\nPIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),\nHellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC (ARC-Easy and ARC-\nChallenge) (Clark et al., 2018), and OBQA (Mi-\nhaylov et al., 2018). (4) Direct Preference Opti-\nmization (DPO): To evaluate SGG in human pref-\nerence alignment tasks, we implemented DPO us-\n--- Page 6 ---\n0.0 0.5 1.0 1.5 2.0 2.2\nToken Seen (Billions)20253035404550Validation Perplexity (%)\nAdam\nAdam (LoRA)\nAdam+SGG\nAdam+SGG (LoRA)(a) LLaMA-130M Pre-training\n0.0 2.5 5.0 7.5 10.0 12.5\nToken Seen (Billions)15202530Validation Perplexity (%)\nAdam\nAdam (LoRA)\nAdam+SGG\nAdam+SGG (LoRA) (b) LLaMA-1B Pre-training\n60M 130M 350M 1B\nParameter Scales1520253035404550Validation Perplexity (%)\nLoRA\nLow-Rank\nReLoRA\nGaLore\nLoRA+SGG (c) Parameter Scaling-up\nFigure 4: Convergence and Scaling-up on C4 Pre-training . Validation Perplexity (PPL% ↓: lower is better) vs\nTraining Tokens/Parameters. (a)LLaMA-130M and (b)LLaMA-1B training curves demonstrate faster convergence\nand lower PPL of SGG compared to baselines in both low-rank ( Adam vsAdam+SGG ) and full-rank ( Adam vs\nAdam+SGG ) settings. (c) LoRA+SGG consistently outperforms other low-rank methods as model size increases.\ning the TRL library. The Qwen2.5 0.5B model was\ntrained on the ultrafeedback_binarized dataset,\nwhich includes binary preference labels (von Werra\net al., 2020). (5) MLLM Validation: (i) VQA\nbenchmarks such as GQA (Hudson and Manning,\n2019), TextVQA (Singh et al., 2019), SciVQAI\n(evaluation on the imageset of ScienceVQA) (Lu\net al., 2022b), VQAv2 (Goyal et al., 2017), and\nVizwiz (Gurari et al., 2018). (ii) MLLM evalua-\ntion benchmarks including POPE (Li et al., 2023b),\nMMBench (Liu et al., 2025b), MMBench-Chinese\n(MMBenchCN) (Liu et al., 2025b), SEEDI(Li et al.,\n2023a), and MME (Perception) (Yin et al., 2023).\nImplementation Details We implemented SGG\nin PyTorch, ensuring compatibility with standard\noptimizers through minimal code integration. Its\nkey hyper-parameters were empirically tuned for\noptimal performance-efficiency trade-off as the\ndefault setups: cluster number K= 3, interval\nT= 500 (nearly 1 ∼5%of total iterations), and\ndecay coefficient β3= 0.99. To minimize GPU\nmemory demands, cluster indices Cand scaling fac-\ntorsScan be optionally stored on the CPU. Table 3\nconfirms SGG’s negligible training time increase\nand preserved GPU memory footprint. Reproduced\nresults are marked with gray and blue backgrounds,\nwhile the others are cited from their original pa-\npers. All experiments are conducted using NVIDIA\nA100-80G GPUs with three independent runs.\n3.2 Comparison Results with LLMs\nAcross PT, SFT, PEFT, and DPO, SGG consistently\nimproves performance with efficiency, highlighting\nits value as a versatile optimizer wrapper for LLMs.\nPre-training on C4. Following GaLore (Zhao\net al., 2024a), we employ LLaMA-based architec-\ntures (60M to 1B) for both full-rank and low-rank\npre-training. We keep consistent hyper-parameters,\ntuning learning rates within a fixed budget, and useBF16 precision for efficiency. Table 4 shows that\napplying SGG consistently reduces validation per-\nplexity ( -3.75% and-1.26% for AdamW in 60M\nand 1B; -10.30% and-4.48% for LoRA in 130M\nand 1B) and it accelerates convergence (Figure 4)\ncompared to baselines. Notably, SGG for the first\ntime enables low-rank pre-training (LoRA+SGG)\nto achieve performance comparable to full-rank\ntraining across model sizes (e.g.,14.73 vs14.30 in\n1B;30.62 vs30.31 in 60M), a huge step forward as\nprevious low-rank optimizers typically lagged be-\nhind in performance. View Appendix A for details.\nSFT on GLUE. We fine-tuned the pre-trained\nRoBERTa-base on various GLUE tasks. Table 5\nshows that applying SGG yields consistent gains\nover baselines in both full-rank and low-rank (ranks\n4 and 8) SFT scenarios. Notably, AdamW+SGG\nyields substantial average gains ( +1.00% full-rank,\n+1.27% rank 4), with significant task-specific im-\nprovements ( e.g., MRPC full-rank +1.35% , MNLI\nrank 4 +1.36% ), demonstrating SGG’s versatility\nand robustness across different SFT scenarios.\nPEFT on Commonsense Reasoning. Following\nLLM-Adapters, we assess SGG in CS tasks with\ntop-1 accuracy and GPU memory, where LLaMA-\n7B is fine-tuned by AdamW+LoRA ( r= 32 ) on\na unified training dataset, followed by evaluation\non each specific subset. As shown in Table 6, SGG\nimproves LoRA by an average of +2.9% across all\ntasks, with up to +4.2% gains on specific tasks like\nOBQA. It matches or surpasses PEFT baselines,\nsuch as Prefix (Li and Liang, 2021), Series(Houlsby\net al., 2019), and Parallel (He et al., 2021), and\nmore recent DoRA, GaLore, and Fira (Chen et al.,\n2024). View Table A4 and Appendix A for details.\nDPO. We verify SGG’s effectiveness in aligning\nLLMs with human preferences using DPO, adher-\ning to standard TRL library settings. SGG again\n--- Page 7 ---\nTable 5: GLUE Benchmark Results with RoBERTa-base. Top-1 accuracy (% ↑: higher is better) is reported.\nComparison across both full-rank and low-rank (LoRA r= 4,r= 8) settings. Bold andgreen types denote the\nbest results and performance gains ↑of SGG ( blue back ground) compared to related baselines ( gray back ground).\nOptimizer Rank CoLA STS-B MRPC RTE SST2 MNLI QNLI QQP Average\nFull-Rank SFT\nSGD Full 62.12 90.73 87.74 79.06 94.26 87.53 92.29 92.22 85.74\nAdamW Full 62.24 90.92 91.30 79.42 94.57 87.18 92.33 92.28 86.24\nLAMB Full 62.09 90.59 88.72 75.45 94.72 87.71 92.42 91.46 85.40\nCAME Full 62.16 90.43 89.02 75.94 94.61 87.13 92.31 91.54 85.39\nAPOLLO Full 62.45 90.70 90.36 77.53 94.58 87.57 92.40 92.12 85.96\nAdamW+ SGG Full 63.36 +1.12 91.22 +0.30 92.65 +1.35 80.87 +1.45 95.58 +1.01 88.32 +1.14 92.88 +0.55 93.32 +1.04 87.28 +1.00\nLAMB+ SGG Full 62.47 +0.38 90.90 +0.31 89.46 +0.74 76.53 +1.08 94.95 +0.23 87.81 +0.10 92.89 +0.47 91.78 +0.32 85.85 +0.45\nLow-Rank SFT (rank 4)\nSGD (LoRA) 4 60.32 90.31 87.75 79.06 94.27 87.39 92.16 91.89 85.39\nAdamW (LoRA) 4 61.38 90.57 91.07 78.70 92.89 86.82 92.18 91.29 85.61\nLAMB (LoRA) 4 61.51 90.33 89.46 74.73 94.27 87.51 92.48 91.57 85.23\nDoRA 4 60.38 90.50 88.24 74.73 93.69 − 92.59 − −\nGaLore (LoRA) 4 60.35 90.73 92.25 79.42 94.04 87.00 92.24 91.06 85.89\nAdamW+ SGG 4 62.36 +0.98 91.10 +0.53 92.12 +1.05 80.51 +1.81 95.06 +2.17 88.18 +1.36 92.62 +0.44 93.06 +1.77 86.88 +1.27\nLAMB+ SGG 4 62.47 +0.96 90.90 +0.57 89.46 +0.30 75.53 +0.80 94.95 +0.34 87.73 +0.12 92.92 +0.41 91.78 +0.36 85.72 +0.49\nLow-Rank SFT (rank 8)\nSGD (LoRA) 8 60.57 90.29 88.48 79.42 94.32 87.44 92.23 92.10 85.61\nAdamW (LoRA) 8 61.83 90.80 91.90 79.06 93.46 86.94 92.25 91.22 85.93\nLAMB (LoRA) 8 61.89 90.78 89.21 79.42 94.61 87.61 92.51 91.42 85.35\nDoRA 8 58.36 90.63 88.97 75.09 93.81 − 92.68 − −\nGaLore (LoRA) 8 60.06 90.82 92.01 79.78 94.38 87.17 92.20 91.11 85.94\nAdamW+ SGG 8 62.36 +0.53 91.10 +0.30 92.12 +0.22 80.51 +1.45 95.06 +1.60 88.17 +1.23 92.65 +0.40 92.85 +1.63 86.85 +0.92\nLAMB+ SGG 8 62.47 +0.58 90.90 +0.12 89.46 +0.25 76.53 +1.80 94.95 +0.34 87.85 +0.24 92.87 +0.36 91.78 +0.36 85.85 +0.50\nTable 6: LLaMA-7B PEFT Results on Commonsense\nReasoning. Comparison of LoRA+SGG ( blue back -\nground) against baselines. Top-1 accuracy (% ↑: higher\nis better) of selected tasks and all tasks on average (Avg.)\nare reported. Bold andgreen types denote the best re-\nsults and gains ↑compared to LoRA ( gray background).\nMethod BoolQ PIQA SIQA WG Arc-E OBQA Avg.\nParallel 67.9 76.4 78.8 78.9 73.7 75.2 72.2\nLoRA 68.9 80.7 77.4 78.8 77.8 74.8 74.7\nDoRA 69.7 83.4 78.6 81.0 81.9 79.2 78.4\nGaLore 69.5 82.0 75.1 18.0 80.7 78.0 62.7\nFira 69.4 82.6 78.0 81.2 82.2 80.8 76.9\nLoRA+ SGG 70.3 83.6 78.8 80.9 81.5 79.0 77.6\n∆Gains +1.4 +2.9 +1.4 +2.1 +3.7 +4.2 +2.9\nDoRA+ SGG 71.4 84.8 79.5 82.8 83.8 81.2 79.6\n∆Gains +1.7 +1.4 +0.9 +1.8 +1.9 +2.0 +1.2\ndemonstrates clear advantages. As shown in Ta-\nble 7, AdamW+SGG achieves the highest accuracy\n(72.02% ) under LoRA training, improving signifi-\ncantly ( +1.80% ) over AdamW and even surpass-\ning its full rank counterpart (72.02% vs71.85% ),\nshowcasing SGG’s potential to substantially im-\nprove alignment methods with favorable efficiency.\n3.3 Comparison Results with MLLMs\nWe validate SGG’s effectiveness in MLLMs, fol-\nlowing LLaV A-v1.5 with a pretrained Vicuna-v1.5-\n7B (Chiang et al., 2023), pretrained 2 ×MLP, and a\npretrained CLIP (Radford et al., 2021), supervised\nfine-tuned for one epoch with a batch size of 64. (i)\nFull-Rank SFT: AdamW, Adafactor, and LAMB\nare considered as baselines, with details of hyperpa-\nrameters and settings provided in Table A5, and dis-Table 7: Qwen2.5-0.5B DPO Results with full-rank and\nLoRA setups. Top-1 accuracy(%) ↑is reported. Bold\nandgreen types denote best results and relative gains.\nOptimizer Full-Rank LoRA\nSGD 70.10 69.73\nAdamW 71.39 70.22\nLAMB 70.82 70.39\nSGD+ SGG 70.82 +0.72 70.76 +1.03\nAdamW+ SGG 71.85 +0.47 72.02 +1.80\nLAMB+ SGG 71.32 +0.50 71.28 +0.89\nplay results of mainstream MLLM methods. The\nresults in Table 8 show that SGG boosts AdamW\nby+1.0% on average. When paired with Adafactor,\nSGG could offer +0.6% gains compared to base-\nline. Notably, SGG delivers an impressive +2.4%\nimprovement on VizWiz. (ii) PEFT and Quanti-\nzation: To rigorously evaluate SGG in resource-\nconstrained scenarios, we conduct PEFT (LoRA)\nand 8-bit Quantization LoRA (Q-LoRA (Dettmers\net al., 2024)) with rank r= 128 and scaling fac-\ntorα= 256 . Table 8 shows that SGG achieves\n65.1% average accuracy and yields +2.2% gains\nover LoRA on VizWiz. Furthermore, SGG also\nenhances QLoRA (8-bit) SFT by +0.6% on aver-\nage. All these results demonstrate SGG’s versatility\nand effectiveness in boosting MLLM performance\nacross SFT, PEFT, and quantized FT (Table A6).\n3.4 Robustness to Learning Rate Scaling-up\nAdam-like optimizers often struggle with the in-\nterplay between learning rate (LR) and batch size,\n--- Page 8 ---\nTable 8: MLLM performance comparison on diverse\nbenchmarks with LLaV A variants and different opti-\nmizers. Top-1 accuracy (%) ↑for selected tasks and\nall-task averaged (Avg.) results are reported. MMB and\nMMBCNdenote MMbench and MMbench (Chinese).\nBold andgreen types denote the best results and gains ↓\nof SGG ( blue back ground) over related baselines ( gray\nback ground). Please view Table A6 for the full results.\nOptimizerImage Question Answering BenchmarksAvg.GQA VizWiz SciVQAIVQATMMB MMBCNPOPE\nBLIP-2 41.0 19.6 61.0 42.5 − − 85.3 −\nInstructBLIP 49.2 34.5 60.5 50.1 36.0 23.7 79.8 47.7\nQwen-VL 59.3 35.2 67.1 63.8 38.2 7.4 − −\nTinyLLaV A 62.0 − 69.1 59.1 66.9 − 86.4 −\nMoE-LLaV A 62.6 − 70.3 57.0 68.0 − 85.7 −\nLLaV A-Phi − − 68.4 48.6 59.8 − 85.0 −\nLLaV A-NeXT 64.2 57.6 70.1 64.9 67.4 60.6 86.5 67.3\nLLaV A-MOD 58.7 39.2 68.0 58.5 66.3 61.9 87.0 62.8\nLLaV A-KD-2B 62.3 44.7 64.7 53.4 64.0 63.7 86.3 62.7\nLLaVA-v1.5 Full-Rank SFT\nAdamW 62.0 50.0 66.8 58.2 64.3 58.3 85.9 63.6\nAdafactor 62.7 48.2 70.7 57.1 66.1 60.4 86.0 64.5\nLAMB 43.8 53.3 61.5 43.4 43.2 41.8 81.2 52.6\nAdamW+ SGG 62.4 50.2 69.8 57.4 65.9 60.1 86.3 64.6\n∆Gains +0.4 +0.2 +3.0 -0.8 +1.6 +1.8 +0.4 +1.0\nAdafactor+ SGG 62.8 50.6 71.6 57.3 66.3 60.8 86.0 65.1\n∆Gains +0.1 +2.4 +0.9 +0.2 +0.2 +0.4 +0.0 +0.6\nLAMB+ SGG 44.0 53.3 61.8 43.5 43.3 41.9 81.3 52.7\n∆Gains +0.2 +0.0 +0.3 +0.1 +0.1 +0.1 +0.1 +0.1\nLLaVA-v1.5 Low-Rank SFT (AdamW)\nLoRA 63.0 47.8 68.4 58.2 66.1 58.9 86.4 64.1\nLoRA+ SGG 63.4 51.0 70.1 58.6 66.7 59.4 86.6 65.1\n∆Gains +0.4 +2.2 +1.5 +0.4 +0.6 +0.5 +0.2 +1.0\nLLaVA-v1.5 8-bit Low-Rank SFT (AdamW)\nQ-LoRA 54.3 50.7 66.4 52.5 56.0 49.8 82.9 58.9\nQ-LoRA+ SGG 55.1 51.3 66.7 53.0 56.1 51.0 83.4 59.5\n∆Gains +0.8 +0.6 +0.3 +0.5 +0.1 +0.2 +0.5 +0.6\nleading to training instability ( e.g., the surge phe-\nnomenon (Li et al., 2024b)) and meticulous tuning.\nIn contrast, SGG shows exceptional robustness in\nthis regard. During SFT on Alpaca (Taori et al.,\n2023) with Adam (Figure 5), SGG maintains sta-\nble validation loss across a wide spectrum of batch\nsizes ( 128to4096 ) and learning rates, even under\nextreme conditions like batch sizes of 4096 and\nLR of 0.1. This suggests that SGG effectively mit-\nigates gradient outliers and dynamically adapts\nLRs, ensuring reliable training across diverse con-\nfigurations. Please refer to Appendix C for details.\n3.5 Ablation Studies\nWe analyze the three key hyper-parameters in SGG.\n(i) Cluster number: Table 9 shows that Kcan be\neasily set to {2,3} for diverse tasks according to\nthe mini-batch K-means diagnostics. (ii) Interval:\nThe interval Tcan be set as 5% of the total training\niterations, e.g.,T= 500 for LLaMA-60M yields\nstrong results, as shown in Figure 6(a). (iii) LR\n105\n104\n103\n102\n101\nSFT Learning Rate0246810121416Validation Perplexity (%)\nAdam (Batch Size=128)\nAdam (Batch Size=512)\nAdam (Batch Size=1024)\nAdam (Batch Size=2048)\nAdam (Batch Size=4096)\nAdam+SGG (Batch Size=128)\nAdam+SGG (Batch Size=512)\nAdam+SGG (Batch Size=1024)\nAdam+SGG (Batch Size=2048)\nAdam+SGG (Batch Size=4096)Figure 5: Learning Rate and Batch Size Scaling-up\nwith Qwen2.5-0.5B SFT on Alpaca. Validation loss ↓vs\nSFT Learning Rate for Adam andAdam+SGG across\nvarious batch sizes (128 to 4096). SGG offers consistent\nrobustness over a wider range of hyper-parameters.\n30.030.531.031.532.0\n100 250 500 100023.023.524.024.5\nReclustering T IntervalValidation Perplexity (%)LLaMA-60M\nLLaMA-130M\n(a) Recluster Interval T\n30.030.531.031.532.032.5\n0.9 0.99 0.999 0.999922.022.523.023.5\nScaling Decay Beta3Validation Perplexity (%)LLaMA-60M\nLLaMA-130M (b) Scaling Decay β3\nFigure 6: Ablation of Hyperparameters with LLaMA-\n60M and LLaMA-130M pre-training on C4. Validation\nPerplexity (PPL % ↓: lower is better) vs. (a)Recluster\nInterval T(% total iterations) and (b)EMA Decay β3.\nThe results demonstrate that T≈500andβ3= 0.99\nare the most favorable settings for SGG upon Adam.\nscaling decay: Figure 6(b) demonstrates that SGG\nis insensitive to the precise value of scaling decay\nβ3, with β3= 0.99proving a robust choice.\nTable 9: Ablation studies of the number of SGG clus-\ntersK(task-relevant) across different tasks and models.\nERR denotes the mini-batch K-means running errors.\nTask Model K=1 (Adam) K=2 K=3 K=4\nC4↓ LLaMA-60M 34.1 30.3 30.8 ERR\nC4↓ LLaMA-130M 25.1 23.3 23.5 ERR\nGLUE (MNLI) ↑RoBERT-Base 87.2 88.3 87.9 ERR\nMLLM ↑ LLaV A-1.5-7B 63.6 64.2 64.5 64.3\n4 Related Work\nEfficient Optimizers. Adaptive learning rate op-\ntimizers (Loshchilov and Hutter, 2019) are preva-\nlent in training LLMs due to their balance of con-\nvergence speed and generalization. However, their\neffectiveness might diminish at scale because of the\nreliance on global gradient statistics, which over-\nlook the inherent heterogeneity in LLMs (Zhao\net al., 2024b; Zhang et al., 2025b). This hetero-\ngeneity, combined with the low-rank properties\nof LLMs, often leads to inefficient parameter up-\ndates and suboptimal convergence (Chen et al.,\n--- Page 9 ---\n2024; Zhao et al., 2024a). Traditional methods like\nAdam exhibit limitations in handling gradient dy-\nnamics under LLM low-rank constraints (Li et al.,\n2024a), prompting the development of memory-\nefficient optimizers such as BAdam (Luo et al.,\n2025) and LISA (Pan et al., 2025). Techniques\nlike Adam-mini (Zhang et al., 2024) and APOLLO\n(Zhu et al., 2024a) further demonstrate that reduced\nlearning rates or SGD-like memory footprints can\nachieve competitive performance. Nevertheless,\nchallenges persist, particularly in scaling optimiza-\ntion for large models, as evidenced by the surge\nphenomenon in optimal learning rate and batch size\nscaling (Li et al., 2024b). Recent studies like SPAM\n(Huang et al., 2025) and CAME (Luo et al., 2023)\nintroduce momentum reset and confidence-guided\nstrategies to stabilize training. SGG addresses these\nissues by grouping gradients and applying group-\nspecific scaling, ensuring tailored learning rates.\nParameter-efficient Fine-tuning. Parameter-\nefficient Fine-tuning (PEFT) has become essential\nfor adapting LLMs to downstream tasks efficiently.\nLoRA (Hu et al., 2021) as a foundational PEFT\ntechnique significantly reduces the computational\ncosts by training only a small number of low-rank\nperturbation matrices added to the pre-trained\nweights. Recent extensions like DoRA variants\n(Liu et al., 2024c; Nasiri and Garraghan, 2025)\nfurther improve adaptation efficiency while\nmaintaining performance. Despite their success,\nLoRA-based methods exhibit limitations: reliance\non Dropout for regularization can be ineffective,\nespecially in short training regimes (Kamalakara\net al., 2022). Suboptimal initialization can impede\nconvergence in sparse data scenarios, and static\nscaling factors hinder adaptive tuning of learning\nrates (Dettmers et al., 2023). While recent efforts\nlike LoRA+ (Hayou et al., 2024) and LoRA-XS\n(Bałazy et al., 2024) attempt to mitigate some\nof these issues, challenges persist, particularly\nin complex multi-modality perception tasks (Ma\net al., 2024) and broader PEFT applications (Zhang\net al., 2025a). These limitations underscore the\nneed for low-rank optimization that is migratable\nand can adjust learning rates with the low-rank\nproperty, which could be addressed by the gradient\ngrouping-based learning rate scaling in SGG.\n5 Conclusion\nThis paper presents SGG, an optimizer wrapper\nto address the challenges in LLM training. SGGclusters momentum vectors in each layer and com-\nputes cluster-specific scaling factors to modulate\nparameter-wise learning rates. Experiments demon-\nstrate SGG’s versatility and effectiveness with con-\nsistent performance gains and faster convergence\nwhen integrated with other optimizers and LoRA.\n6 Discussion and Limitations\nBorder Impact. The expanding application of\nLLMs underscores the need for efficient and effec-\ntive optimization. SGG offers a distinct approach:\nrather than relying on global approximation tech-\nniques ( e.g., NMF) or architectural modifications\n(e.g., LoRA variants), SGG employs intra-layer\ngradient clustering and performs cluster-specific\nscaling to parameter-wise adaptive learning rates.\nThis scheme allows seamless integration with main-\nstream optimizers and LoRA, yielding consistent\nperformance gains across diverse (M)LLM appli-\ncations with negligible additional cost.\nLimitations and Discussion. While SGG shows\ngreat promise, its implementation shows avenues\nfor future studies along this line: (1) Grouping\nStrategies: SGG’s reliance on online clustering\nfor dynamic grouping, though intuitive, represents\nonly one specific choice. The adaptive learning\nrate scaling paradigm itself is flexible, which could\ninclude broader grouping designs, such as more pre-\ncise online clustering, heuristic-based static parti-\ntioning, or even learned grouping functions, any of\nwhich might offer different performance-efficiency\ntrade-offs for diverse scenarios and demands. (2)\nComputational Efficiency: While the CPU of-\nfloading in SGG mitigates the GPU burden, its on-\nline clustering still brings significant costs, which\npresents a huge concern in resource-constrained\nscenarios. Future work could focus on lightweight\ngrouping methods or operations that could approxi-\nmate grouping benefits without explicit clustering,\nthereby further enhancing the LLM optimizer’s ef-\nficiency and applicability. (3) Evaluation Scope:\nThe validation in this work covers diverse bench-\nmarks, yet extending the evaluation to a wider array\nof scenarios such as image generation (Yu et al.,\n2023; Li et al., 2025), multi-modalities (Lu et al.,\n2022a; Xu et al., 2025), architectures ( e.g., vision\nbackbones (Liu et al., 2022; Li et al., 2024d) and\nMixture-of-Experts (Cai et al., 2024a)), and vari-\nous data scales could provide deeper insights into\nSGG’s generalization capabilities and potentially\nuncover new avenues for effective LLM training.\n--- Page 10 ---\nAcknowledgement\nThis research is supported in part by the Early Ca-\nreer Scheme of the Research Grants Council (RGC)\nof the Hong Kong SAR under grant No. 26202321\nand HKUST Startup Fund No. R9253. This work\nwas done when Juanxi Tian and Xin Jin interned\nat Westlake University and Peking University. The\nauthors thank the Westlake University AI Station\nfor supporting GPUs.\nReferences\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966 .\nKlaudia Bałazy, Mohammadreza Banaei, Karl Aberer,\nand Jacek Tabor. 2024. Lora-xs: Low-rank adap-\ntation with extremely small number of parameters.\narXiv preprint arXiv:2405.17604 .\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzade-\nnesheli, and Anima Anandkumar. 2018. signsgd:\ncompressed optimisation for non-convex problems.\nInInternational Conference on Machine Learning .\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of\nthe AAAI conference on artificial intelligence , pages\n7432–7439.\nWeilin Cai, Juyong Jiang, Fan Wang, Jing Tang,\nSunghun Kim, and Jiayi Huang. 2024a. A survey on\nmixture of experts in large language models. IEEE\nTransactions on Knowledge and Data Engineering .\nYuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He,\nAo Tong, Zhenye Gan, Chengjie Wang, and Xiang\nBai. 2024b. Llava-kd: A framework of distilling\nmultimodal large language models. arXiv preprint\narXiv:2410.16236 .\nXi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xi-\nangyu Yue, Ye Yuan, and Guoren Wang. 2024. Fira:\nCan we achieve full-rank training of llms under low-\nrank constraint? arXiv preprint arXiv:2410.01623 .\nXiangning Chen, Chen Liang, Da Huang, Esteban Real,\nKaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Lu-\nong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. 2023.\nSymbolic discovery of optimization algorithms. In\nThirty-seventh Conference on Neural Information\nProcessing Systems .\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023) , 2(3):6.Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research , 24(240):1–113.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500 .\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. Advances in neural information\nprocessing systems , 36:10088–10115.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2024. Qlora: Efficient finetuning\nof quantized llms. Advances in Neural Information\nProcessing Systems , 36.\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, and Xi-\naowei Xu. 1996. A density-based algorithm for dis-\ncovering clusters in large spatial databases with noise.\nInKnowledge Discovery and Data Mining .\nBoris Ginsburg, Igor Gitman, and Yang You. 2018.\nLarge batch training of convolutional networks with\nlayer-wise adaptive rate scaling. In International\nConference on Learning Representations (ICLR) .\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages\n6904–6913.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. 2018. Vizwiz grand challenge: Answering\nvisual questions from blind people. In Conference on\nComputer Vision and Pattern Recognition (CVPR) ,\npages 3608–3617.\nSoufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024.\nLora+: Efficient low rank adaptation of large models.\narXiv preprint arXiv:2402.12354 .\n--- Page 11 ---\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\nInternational Conference on Learning Representa-\ntions (ICLR) .\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. ArXiv .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685 .\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-\nPeng Lim, Lidong Bing, Xing Xu, Soujanya Po-\nria, and Roy Ka-Wei Lee. 2023. Llm-adapters:\nAn adapter family for parameter-efficient fine-\ntuning of large language models. arXiv preprint\narXiv:2304.01933 .\nTianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu,\nZhangyang Wang, and Shiwei Liu. 2025. Spam:\nSpike-aware adam with momentum reset for stable\nllm training. arXiv preprint arXiv:2501.06842 .\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 6700–6709.\nSiddhartha Rao Kamalakara, Acyr Locatelli, Bharat\nVenkitesh, Jimmy Ba, Yarin Gal, and Aidan N\nGomez. 2022. Exploring low rank training of deep\nneural networks. arXiv preprint arXiv:2209.13569 .\nNanda Kambhatla and Todd K. Leen. 1994. Classifying\nwith gaussian mixtures and clusters. In Advances in\nNeural Information Processing Systems (NeurIPS) ,\npage 681–688, Cambridge, MA, USA. MIT Press.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR) .\nAndreas Köpf, Yannic Kilcher, Dimitri V on Rütte,\nSotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Nguyen, Oliver Stan-\nley, Richárd Nagyfi, et al. 2023. Openassistant\nconversations-democratizing large language model\nalignment. Advances in Neural Information Process-\ning Systems , 36:47669–47681.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. 2023a. Seed-bench: Bench-\nmarking multimodal llms with generative compre-\nhension. arXiv preprint arXiv:2307.16125 .\nGuangyan Li, Yongqiang Tang, and Wensheng Zhang.\n2024a. Lorap: Transformer sub-layers deserve differ-\nentiated structured compression for large language\nmodels. arXiv preprint arXiv:2404.09695 .Junnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International conference on ma-\nchine learning , pages 12888–12900. PMLR.\nShuaipeng Li, Penghao Zhao, Hailin Zhang, Xingwu\nSun, Hao Wu, Dian Jiao, Weiyan Wang, Chengjun\nLiu, Zheng Fang, Jinbao Xue, et al. 2024b. Surge\nphenomenon in optimal learning rate and batch size\nscaling. arXiv preprint arXiv:2405.14578 .\nSiyuan Li, Juanxi Tian, Zedong Wang, Luyuan Zhang,\nZicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, and\nStan Z Li. 2024c. Unveiling the backbone-optimizer\ncoupling bias in visual representation learning. arXiv\npreprint arXiv:2410.06373 .\nSiyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan,\nHaitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng,\nand Stan Z. Li. 2024d. Moganet: Multi-order gated\naggregation network. In International Conference on\nLearning Representations (ICLR) .\nSiyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian,\nCheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie,\nHaonan Lu, Haoqian Wang, and Zhen Lei. 2025.\nMergevq: A unified framework for visual generation\nand representation with disentangled token merging\nand quantization. In Conference on Computer Vision\nand Pattern Recognition (CVPR) .\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing , pages 4582–4597.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin\nZhao, and Ji-Rong Wen. 2023b. Evaluating object\nhallucination in large vision-language models. In The\n2023 Conference on Empirical Methods in Natural\nLanguage Processing .\nVladislav Lialin, Sherin Muckatira, Namrata Shiva-\ngunde, and Anna Rumshisky. 2023. Relora: High-\nrank training through low-rank updates. In The\nTwelfth International Conference on Learning Repre-\nsentations .\nBin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu,\nPeng Jin, Junwu Zhang, Munan Ning, and Li Yuan.\n2024. Moe-llava: Mixture of experts for large vision-\nlanguage models. arXiv preprint arXiv:2401.15947 .\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. 2024a. Llava-\nnext: Improved reasoning, ocr, and world knowledge.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2024b. Visual instruction tuning. Advances in\nneural information processing systems , 36.\n--- Page 12 ---\nJingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang,\nGuokun Lai, Yulun Du, Yidao Qin, Weixin Xu, En-\nzhe Lu, Junjie Yan, et al. 2025a. Muon is scalable\nfor llm training. arXiv preprint arXiv:2502.16982 .\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020a. On the variance of the adaptive learning rate\nand beyond. In International Conference on Learn-\ning Representations .\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen,\nand Jiawei Han. 2020b. Understanding the difficulty\nof training transformers. In Conference on Empirical\nMethods in Natural Language Processing .\nShih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo\nMolchanov, Yu-Chiang Frank Wang, Kwang-Ting\nCheng, and Min-Hung Chen. 2024c. Dora: Weight-\ndecomposed low-rank adaptation. arXiv preprint\narXiv:2402.09353 .\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. 2025b. Mm-\nbench: Is your multi-modal model an all-around\nplayer? In European conference on computer vi-\nsion, pages 216–233. Springer.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph\nFeichtenhofer, Trevor Darrell, and Saining Xie. 2022.\nA convnet for the 2020s. In Conference on Computer\nVision and Pattern Recognition (CVPR) .\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations (ICLR) .\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. 2022a. Unified-\nio: A unified model for vision, language, and multi-\nmodal tasks. In International Conference on Learn-\ning Representations (ICLR) .\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022b. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems , 35:2507–2521.\nQijun Luo, Hengxu Yu, and Xiao Li. 2025. Badam: A\nmemory efficient full parameter optimization method\nfor large language models. Advances in Neural Infor-\nmation Processing Systems , 37:24926–24958.\nYang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo\nJiang, Xin Jiang, and Yang You. 2023. Came:\nConfidence-guided adaptive memory efficient opti-\nmization. arXiv preprint arXiv:2307.02047 .\nFeipeng Ma, Hongwei Xue, Guangting Wang, Yizhou\nZhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siy-\ning Wu, Mike Zheng Shou, and Xiaoyan Sun. 2024.\nVisual perception by large language model’s weights.\narXiv preprint arXiv:2405.20339 .James MacQueen et al. 1967. Some methods for classi-\nfication and analysis of multivariate observations. In\nProceedings of the fifth Berkeley symposium on math-\nematical statistics and probability , pages 281–297.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789 .\nIgor Molybog, Peter Albert, Moya Chen, Zachary De-\nVito, David Esiobu, Naman Goyal, Punit Singh\nKoura, Sharan Narang, Andrew Poulton, Ruan Silva,\nBinh Tang, Diana Liskovich, Puxin Xu, Yuchen\nZhang, Melissa Hall Melanie Kambadur, Stephen\nRoller, and Susan Zhang. 2023. A theory on adam\ninstability in large-scale machine learning. ArXiv ,\nabs/2304.09871.\nHamid Nasiri and Peter Garraghan. 2025. Edora:\nEfficient weight-decomposed low-rank adaptation\nvia singular value decomposition. arXiv preprint\narXiv:2501.12067 .\nRui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng\nZhang, Chi Han, and Tong Zhang. 2025. Lisa: layer-\nwise importance sampling for memory-efficient large\nlanguage model fine-tuning. Advances in Neural\nInformation Processing Systems , 37:57018–57049.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning , pages 8748–8763. PMLR.\nSashank J. Reddi, Satyen Kale, and Surinder Kumar.\n2018. On the convergence of adam and beyond. In\nInternational Conference on Learning Representa-\ntions .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM , 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728 .\nD. Sculley. 2010. Web-scale k-means clustering. In\nInternational Conference on World Wide Web .\nNoam M. Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nArXiv , abs/1804.04235.\nFangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei\nZhang, Guanghao Zhang, Haonan Shi, Long Chen,\nTao Zhong, Wanggui He, et al. 2024. Llava-mod:\nMaking llava tiny via moe knowledge distillation.\narXiv preprint arXiv:2408.15881 .\n--- Page 13 ---\nAmanpreet Singh, Vivek Natarajan, Meet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards vqa models\nthat can read. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition ,\npages 8317–8326.\nNaresh K. Sinha and Michael P. Griscik. 1971. A\nstochastic approximation method. IEEE Transac-\ntions on Systems, Man, and Cybernetics , SMC-\n1(4):338–344.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, Nathan Lambert,\nShengyi Huang, Kashif Rasul, and Quentin Gal-\nlouédec. 2020. Trl: Transformer reinforcement learn-\ning. https://github.com/huggingface/trl .\nNikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira,\nDavid Brandfonbrener, Lucas Janson, and Sham M.\nKakade. 2024. Soap: Improving and stabilizing\nshampoo using adam. ArXiv , abs/2409.11321.\nAlex Wang. 2018. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461 .\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,\nDongdong Zhang, and Furu Wei. 2024. Deepnet:\nScaling transformers to 1,000 layers. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence .\nJin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting\nHe, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan,\nKai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and\nJunyang Lin. 2025. Qwen2.5-omni technical report.\nArXiv , abs/2503.20215.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, An-\nwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei\nHuang. 2024. mplug-owl2: Revolutionizing multi-\nmodal large language model with modality collabo-\nration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages\n13040–13051.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing\nSun, Tong Xu, and Enhong Chen. 2023. A survey on\nmultimodal large language models. arXiv preprint\narXiv:2306.13549 .\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining BERT in 76 minutes. In International Con-\nference on Learning Representations (ICLR) .\nLijun Yu, José Lezama, Nitesh B Gundavarapu, Luca\nVersari, Kihyuk Sohn, David Minnen, Yong Cheng,\nVighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al.2023. Language model beats diffusion–tokenizer is\nkey to visual generation. In International Conference\non Learning Representations (ICLR) .\nHuizhuo Yuan, Yifeng Liu, Shuang Wu, Xun Zhou, and\nQuanquan Gu. 2025. Mars: Unleashing the power of\nvariance reduction for training large models. In Inter-\nnational Conference on Machine Learning (ICML) .\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830 .\nDan Zhang, Tao Feng, Lilong Xue, Yuandong Wang,\nYuxiao Dong, and Jie Tang. 2025a. Parameter-\nefficient fine-tuning for foundation models. arXiv\npreprint arXiv:2501.13787 .\nYushun Zhang, Congliang Chen, Tian Ding, Ziniu Li,\nRuoyu Sun, and Zhiquan Luo. 2025b. Why trans-\nformers need adam: A hessian perspective. Ad-\nvances in Neural Information Processing Systems ,\n37:131786–131823.\nYushun Zhang, Congliang Chen, Ziniu Li, Tian Ding,\nChenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu\nSun. 2024. Adam-mini: Use fewer learning rates to\ngain more. arXiv preprint arXiv:2406.16793 .\nJiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang\nWang, Anima Anandkumar, and Yuandong Tian.\n2024a. Galore: Memory-efficient llm training\nby gradient low-rank projection. arXiv preprint\narXiv:2403.03507 .\nRosie Zhao, Depen Morwani, David Brandfonbrener,\nNikhil Vyas, and Sham Kakade. 2024b. Deconstruct-\ning what makes a good optimizer for language mod-\nels.arXiv preprint arXiv:2407.07972 .\nBaichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo,\nXien Liu, Ji Wu, and Lei Huang. 2024. Tinyllava: A\nframework of small-scale large multimodal models.\narXiv preprint arXiv:2402.14289 .\nHanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu,\nSem Park, Vikas Chandra, Bo Long, David Z Pan,\nZhangyang Wang, and Jinwon Lee. 2024a. Apollo:\nSgd-like memory, adamw-level performance. arXiv\npreprint arXiv:2412.05270 .\nJiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun,\nand Zhuang Liu. 2025. Transformers without nor-\nmalization. arXiv preprint arXiv:2503.10622 .\nYichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and\nYaxin Peng. 2024b. Llava-phi: Efficient multi-modal\nassistant with small language model. In Proceed-\nings of the 1st International Workshop on Efficient\nMultimedia Computing under Limited , pages 18–22.\n--- Page 14 ---\nAppendix\nA Implementation Details\nSGG is implemented in PyTorch and designed for\nseamless integration with mainstream adaptive op-\ntimizers such as Adam variants (Kingma and Ba,\n2015). It requires no modifications to model archi-\ntectures and only minimal additions to the optimiza-\ntion loop. SGG introduces a few key hyperparame-\nters, which are empirically tuned to balance com-\nputational overhead with performance. It includes\nthe number of clusters K∈ {2,3}, the recluster\ninterval T∈[200,1000] which is typically set to\n1∼5% of the total training iterations, and the\nscaling factor EMA decay β3= 0.99. These hyper-\nparameters are empirically tuned to balance com-\nputational efficiency and optimization performance.\nTo minimize GPU memory footprint, especially for\nlarge-scale models, clustering indices, and scaling\nfactors can be stored in CPU memory. This en-\nsures that SGG remains scalable without imposing\nsignificant additional GPU memory demands.\nThe SGG wrapper operates in two main stages\nwithin each optimization step for layers desig-\nnated for scaling: (i) gradient grouping by on-\nline clustering, and (ii) cluster-specific learning\nscaling for each parameter. For optimizers like\nAdam (Kingma and Ba, 2015), the momentum es-\ntimates mt\nl(which provide a smoothed representa-\ntion of the gradients) are flattened and clustered\ninstead. The clustering is performed using the\nMiniBatchKMeans algorithm from the sklearn\nlibrary, which is efficient and suitable for large\ndatasets. During clustering, the flattened gradients\nor momentum estimates are reshaped into a 2D ar-\nray of shape (N,1), where Nis the total number\nof elements in the gradient tensor. After clustering,\neach gradient element is assigned to a cluster, and\nthe scaling factors Slare updated using an EMA of\nthe median gradient magnitudes within each clus-\nter. These scaling factors are then applied to the\nlearning rates during the parameter update step, en-\nabling adaptive and cluster-specific optimization.\nThe immigration of SGG to other adaptive learning\nrate optimizers (Shazeer and Stern, 2018; You et al.,\n2020; Liu et al., 2025a; Luo et al., 2023) should be\nsimilar to this case. The entire process is overall\ncomputationally efficient, with the extra clustering\nperformed on the CPU and only the final scaling\nfactors transferred to the GPU for parameter up-\ndates (the costs are nearly ignorable). Moreover,\nthe proposed scaling operations will not be em-Table A1: Hyperparameters of LLaMA models for pre-\ntraining and evaluation.\nModel 60M 130M 350M 1B\nEmbedding dim. 512 768 1024 2048\nIntermediate dim. 1376 2048 2736 5461\nHeads 8 12 16 24\nLayers 8 12 24 32\nSteps 10K 20K 60K 100K\nWarmup 1K 2K 6K 10K\nData Amount 1.3B 2.6B 7.8B 13.1B\nployed on the scalar and vector parameters like\nnormalization layers and bias, as Muon, because\nthese parameters do not have low-rank properties\nand are scale sensitive.\nB Experimental Setups and Results\nB.1 LLM Pre-training on C4\nWe conducted extensive pre-training experiments\non LLaMA-based large language models using the\nC4 dataset. The C4 dataset, a meticulously cleaned\nand processed version of Common Crawl’s web\ncorpus, serves as a benchmark for pre-training lan-\nguage models and learning word representations.\nTo closely replicate real-world pre-training con-\nditions, we implemented a no-repetition training\nprotocol over a substantial data volume, scaling our\nexperiments across model sizes up to 7 billion pa-\nrameters. We provide a comprehensive overview of\nthe LLaMA architecture and the specific hyperpa-\nrameters employed during pre-training (Table A1).\nThe hyperparameters are standardized across all\nmodel sizes, with a maximum sequence length of\n256 tokens and a batch size of 131,000 tokens ( i.e.,\nthe total batch size of 512 samples). For experi-\nments of all optimizers, we implemented a learning\nrate warmup phase for the initial 10% of the to-\ntal training steps, followed by a cosine annealing\nschedule that gradually reduces the learning rate to\n10% of its initial value.\nFor each model size (ranging from 60 million\nto 1 billion parameters), we performed a system-\natic hyperparameter search to identify the optimal\nlearning rate from the set {1e-2, 5e-3, 1e-3, 5e-\n4, 1e-4}, with selection criteria based on valida-\ntion perplexity. Notably, SGG demonstrated re-\nmarkable robustness to hyperparameter variations,\nmaintaining stable performance across different\nmodel sizes with a consistent learning rate. As\nshown in Table A2 and Figure A1, we provided\nfull benchmark results for the C4 pre-training ex-\n--- Page 15 ---\nTable A2: Full Comparison Results of LLaMA Pre-training on C4 using full-rank and memory-efficient training\nwith the model sizes ranging from 60M to 1B. The validation perplexity (PPL ↓: lower is better) and GPU memory\n(Mem.) ↓are reported, where only the weights and optimization states are considered. Bold andgreen types denote\nthe best results and performance gains ↓of SGG ( blue back ground) over related baselines ( gray back ground). Note\nthat†denotes results borrowed from previous papers, while others were reproduced by us.\nMethod Venue 60M 130M 350M 1B\nPPL Mem. PPL Mem. PPL Mem. PPL Mem.\nAdam†ICLR’15 34.06 0.36G 25.08 0.76G 18.80 2.06G 15.56 7.80G\nNAdam ICLR’18 35.86 0.36G 28.88 0.76G 19.24 2.06G 15.78 7.80G\nRAdam ICLR’20 30.43 0.36G 25.17 0.76G 19.13 2.06G 15.65 7.80G\nLAMB ICLR’20 33.04 0.36G 24.37 0.77G 18.26 2.07G 15.84 7.81G\nAdan TPAMI’23 32.01 0.36G 23.14 0.77G 17.32 2.31G 14.70 15.78G\nMuon arXiv’24 28.93 0.36G 22.34 0.76G 17.09 2.06G 14.52 7.80G\nAdam+ SGG Ours 30.31 0.36G 22.18 0.76G 17.28 2.06G 14.30 7.80G\n∆Gain -3.75 +0.00 -2.90 +0.00 -1.52 +0.00 -1.26 +0.00\nAdafactor†ICML’18 32.57 0.24G 23.98 0.61G 17.74 1.53G 15.19 6.65G\nLION arXiv’23 50.89 0.34G 30.67 0.73G 21.28 1.98G 15.72 5.51G\nLow-Rank†arXiv’22 78.18 0.26G 45.51 0.54G 37.41 1.08G 34.53 3.57G\nAdam-mini†ICLR’25 34.10 0.23G 24.85 0.48G 19.05 1.32G 16.07 4.75G\nCAME ACL’23 31.37 0.25G 23.38 0.62G 17.45 1.55G 14.68 6.70G\nCAME+ SGG Ours 30.15 0.25G 22.91 0.62G 17.09 1.55G 14.35 6.70G\n∆Gain -1.22 +0.00 -0.46 +0.00 -0.36 +0.00 -0.33 +0.00\nAPOLLO†MLSys’25 31.55 0.24G 22.94 0.52G 16.85 1.22G 14.20 4.38G\nAPOLLO+ SGG Ours 30.18 0.24G 22.52 0.52G 16.54 1.22G 13.95 4.38G\n∆Gain -1.37 +0.00 -0.42 +0.00 -0.31 +0.00 -0.25 +0.00\nLoRA†ICLR’22 34.99 0.36G 33.92 0.80G 25.58 1.76G 19.21 6.17G\nReLoRA†ICLR’23 37.04 0.36G 29.37 0.80G 29.08 1.76G 18.33 6.17G\nGaLore†ICML’24 34.88 0.24G 25.36 0.52G 18.95 1.22G 15.64 4.38G\nGaLore+SPAM†ICLR’25 32.39 0.24G 23.98 0.52G 18.28 1.22G 14.73 6.17G\nLoRA+ SGG Ours 30.62 0.36G 23.62 0.80G 17.86 1.76G 14.73 6.17G\n∆Gain -4.37 +0.00 -10.30 +0.00 -7.72 +0.00 -4.48 +0.00\nTraining Tokens 1.1B 2.2B 6.4B 13.1B\nperiments. We borrowed results of popular base-\nlines from the previous studies, including Adam,\nAdam-mini (Zhang et al., 2024), APOLLO (Zhu\net al., 2024a), Low-Rank (Kamalakara et al., 2022),\nLoRA (Hu et al., 2021), ReLoRA (Lialin et al.,\n2023), GaLore (Zhao et al., 2024a), SPAM (Huang\net al., 2025), while reproducing more popular opti-\nmizers with the aforementioned experiments setups,\nincluding Adafactor (Shazeer and Stern, 2018),\nNAdam (Reddi et al., 2018), RAdam (Liu et al.,\n2020a), LAMB (You et al., 2020), LION (Chen\net al., 2023), CAME (Luo et al., 2023), and\nMuon (Liu et al., 2025a).\nB.2 LLM SFT on GLUE Benchmark\nThe GLUE benchmark, a widely used evaluation\nframework for NLP tasks such as sentiment anal-\nysis, question answering, and textual entailment\n(Wang, 2018), serves as a robust platform for as-\nsessing model performance. In this study, we fine-\ntuned the pre-trained RoBERTa-Base model on the\nGLUE benchmark using the Hugging Face imple-mentation. The model was trained for 30 epochs\nwith a batch size of 16 for all tasks except for\nCoLA, which utilized a batch size of 32. We metic-\nulously tuned the learning rate and scale factor\nfor the SGG optimization technique. Table A3 de-\ntails the hyperparameters employed for fine-tuning\nRoBERTa-Base with SGG.\nThe results, as presented in Table 5, demon-\nstrate the effectiveness of SGG in enhancing model\nperformance across various GLUE sub-tasks. No-\ntably, SGG consistently improves the top-1 accu-\nracy when applied to different optimizers (AdamW\nand LAMB) with full-rank and low-rank settings.\nThese enhancements underscore the advantage of\nSGG in stabilizing and accelerating the conver-\ngence of gradient-based optimization methods, par-\nticularly in low-rank settings where computational\nefficiency is crucial. The consistent performance\ngains across multiple tasks and optimizers high-\nlight SGG’s potential as a robust technique for fine-\ntuning large-scale language models, making it a\nvaluable addition to the NLP toolkit.\n--- Page 16 ---\n60M 130M 350M 1B\nParameter Scales15.017.520.022.525.027.530.032.535.0Validation Perplexity (%)\nAdam\nAdam-mini\nLAMB\nLION\nAdam+SGG(a) Full-Rank Optimizers\n60M 130M 350M 1B\nParameter Scales15.017.520.022.525.027.530.032.5Validation Perplexity (%)\nAdafactor\nCAME\nAPOLLO\nAPOLLO-mini\nAPOLLO+SGG (b) Memory-efficient Optimizers\nFigure A1: Model Parameter Scaling-up Analysis on C4 pre-training with different optimization algorithms.\nTable A3: Hyperparameters of fine-tuning RoBERTa base on GLUE benchmark.\nMNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\nBatch Size 16 16 16 32 16 16 16 16\n# Epochs 30 30 30 30 30 30 30 30\nLearning Rate 2e-05 1e-05 3e-05 3e-05 1e-05 1e-05 1e-05 2e-05\nRank Config. Full\nMax Seq. Len. 512\nBatch Size 16 16 16 32 16 16 16 16\n# Epochs 30 30 30 30 30 30 30 30\nLearning Rate 2e-05 1e-05 3e-05 3e-05 1e-05 1e-05 1e-05 2e-05\nRank Config. r= 4\nMax Seq. Len. 512\nBatch Size 16 16 16 32 16 16 16 16\n# Epochs 30 30 30 30 30 30 30 30\nLearning Rate 2e-05 2e-05 2e-05 3e-05 1e-05 2e-05 2e-05 3e-05\nRank Config. r= 8\nMax Seq. Len. 512\nB.3 LLM PEFT with Commonsense\nReasoning Tasks\nFollowing LLM-Adaptor (Hu et al., 2023), we\nevaluate eight Commonsense Reasoning tasks\nwith top-1 accuracy (%) and GPU memory con-\nsumption, including BoolQ (Clark et al., 2019),\nPIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),\nHellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC-Easy (ARC-E) and\nARC-Challenge (ARC-C) (Clark et al., 2018),\nand OBQA (Mihaylov et al., 2018). As SFT se-\ntups in LLM-Adaptor, we combine the training\ndatasets from all sub-tasks to fine-tune the pre-\ntrained LLaMA-7B for 3 epochs using AdamW\noptimizer with a basic learning rate of 1e-4, a batch\nsize of 32, and the rank r= 32 . Then, we evalu-\nate each sub-task individually using its respective\ntesting dataset. Three classical PEFT baselines,\nPrefix-tuning (Prefix) (Li and Liang, 2021), Series\nAdapter (Series) (Houlsby et al., 2019), and Paral-\nlel Adapter (Parallel) (He et al., 2021), and three\npopular PEFT methods, DoRA (Liu et al., 2024c),GaLore (Zhao et al., 2024a), and Fira (Chen et al.,\n2024), are compared in Table A4. Our SGG con-\nsistently improves eight sub-tasks over LoRA by\n+2.9% without extra GPU memory, achieving com-\npetitive performances with well-designed PEFT\nmethods with LoRA+SGG.\nB.4 LLM RLHF with DPO\nIn our experiments, we employed the Direct Prefer-\nence Optimization (DPO) approach to fine-tune\nthe Qwen2.5-0.5B model using the ultrafeed-\nback_binarized dataset, which contains binary pref-\nerence labels that facilitate the alignment of the\nmodel with human preferences (von Werra et al.,\n2020). The training process was conducted using\nboth full-rank and LoRA strategies, with the latter\nbeing typically effective in reducing the number\nof trainable parameters while maintaining competi-\ntive performance. Hyperparameters include a learn-\ning rate of 5.0×10−7for full-rank training and\n5.0×10−6for LoRA, with a single training epoch\nand a batch size of 2 per device. Gradient accumula-\ntion was set to 8 steps, and gradient checkpointing\n--- Page 17 ---\nTable A4: Full Comparison Results of LLaMA PEFT on eight commonsense reasoning datasets with the accuracy\n(%↑: higher is better) and the GPU memory ↓, where only the weights and optimization states are considered.\nChatGPT results are obtained by Zero-shot CoT with gpt-3.5-turbo API. Bold andgreen types denote the best results\nand performance gains ↑of SGG ( blue background) compared to corresponding LoRA baselines ( gray background).\nModel PEFT Memory BoolQ PIQA SIQA HellaSwag WinoGrande Arc-E Arc-C OBQA Average\nChatGPT − − 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0\nPrefix 0.05G 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6\nSeries 0.42G 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8\nParallel 1.49G 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2\nLoRA 0.35G 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7\nLLaMA-7B DoRA 0.26G 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4\nGaLore 0.26G 69.5 82.0 75.1 32.2 18.0 80.7 65.8 78.0 62.7\nFira 0.26G 69.4 82.6 78.0 76.8 81.2 82.2 64.4 80.8 76.9\nLoRA+ SGG 0.35G 70.3 83.6 78.8 81.7 80.9 81.5 65.3 79.0 77.6\n∆Gain +0.00 +1.4 +2.9 +1.4 +3.6 +2.1 +3.7 +4.0 +4.2 +2.9\nwas enabled to optimize memory usage.\nThe optimization process utilized several opti-\nmizers, including SGD, AdamW, and LAMB, with\nand without the addition of the SGG (Stochastic\nGradient with Gain) technique. As shown in Ta-\nble 7, the inclusion of SGG consistently improved\nthe Top-1 accuracy across all optimizers. For in-\nstance, AdamW with SGG achieved a Top-1 accu-\nracy of 71.85% in full-rank training, representing\na gain of 0.47% over the baseline AdamW. Simi-\nlarly, in LoRA training, AdamW with SGG reached\n72.02%, a significant improvement of 1.80% com-\npared to the baseline. These results underscore the\nadvantage of SGG in enhancing the optimization\nprocess, particularly in scenarios where both com-\nputational efficiency and performance are critical.\nThe LoRA configuration used a rank ( r) of\n32 and alpha ( α) of 16, which provided a bal-\nance between model complexity and performance.\nThe evaluation strategy was set in steps, with\nevaluations conducted every 50 steps, and log-\nging was performed every 25 steps to moni-\ntor the training progress. The output direc-\ntory was designated as Qwen2-0.5B-DPO , and the\nno_remove_unused_columns flag was enabled to\nretain all columns in the dataset during training.\nB.5 MLLM SFT with LLaV A Variants\nTo validate the generalization capability of the\nSGG-equipped optimizer, we also verify it on some\nvariants of LLaV A (Liu et al., 2024b). i.e.LLaV A-\nv1.5-7b, LLaV A-LoRA, LLaV A-v1.3. And we\nchoose some mainstream multi-modal LLMs at Ta-\nble 8, e.g.BLIP (Li et al., 2022), InstructBLIP (Dai\net al., 2023), Qwen-VL (Bai et al., 2023), Qwen-\nVL-Chat, mPLUG-Owl2 (Ye et al., 2024), and\nsome variant of LLaV A, Tiny-LLaV A (Zhou et al.,2024), MoE-LLaV A (Lin et al., 2024), LLaV A-\nPhi (Zhu et al., 2024b), LLaV A-NeXT (Liu et al.,\n2024a), LLaV A-MOD (Shu et al., 2024), and\nLLaV A-KD-2B (Cai et al., 2024b).\nSetup and Settings: Following the LLaV A-\nv1.5, we use a pre-trained Vicuna-v1.5-7B (Chiang\net al., 2023) as the language decoder. A pre-trained\n2×MLP is used as the connector to align the visual\ntokens to text tokens. The connector was trained by\ntheLCS-558K datasets for one epoch. For the visual\nencoder, CLIP (Radford et al., 2021) encodes and\nextracts the visual representation from the images.\nIn our experiments, we validate three different op-\ntimizers: AdamW, Adafactor, and LAMB. We also\nreproduced results of popular optimizers like Muon,\nSOAP (Vyas et al., 2024), and MARS (Yuan et al.,\n2025) as the extension. The details of the opti-\nmizer hyperparameters and some training settings\nare shown in Table A5.\nSupervised Fine-tuning: We keep the visual\nencoder frozen and update the parameters of the\nconnector and LLM for training. For the Full-Rank\nSupervised Fine-Tuning (SFT), the learning rate\nwas set to 2e-5, the batch size was 64, and train-\ning one epoch on llava-v1.5-mix665k dataset.\nTo further validate the effectiveness of SGG in\nthe light parameters and low-bit quantization sce-\nnario, we conducted an experiment to train the\nLow-Rank (LoRA) and 8-bit Quantization LoRA\n(Q-LoRA (Dettmers et al., 2024)) SFT method.\nThese methods have unique advantages in param-\neter efficiency and training speed. For the LoRA\nand Q-LoRA SFT, the rank rof LoRA is 128, the\nlearning rate scaling factor αis 256, the batch size\nset is 64, and training one epoch. These low-rank\nmethods are based on the LLaV A-v1.5.\nResults: Table 8 and Table A6 present the re-\n--- Page 18 ---\nTable A5: Details of the hyperparameters for the in-\ncluded optimizers and experiment settings.\nMethod AdamW Adafactor LAMB\nModules and datasets\nLLM Vicuan-v1.5-7B\nVision encoder CLIP-L-336px\nConnector 2×MLP\nPretrain data LCS-558K\nSFT data llava-v1.5-mix665k\nBasic SFT settings\nLearning rate 2e−52e−52e−5\nBatch size 64 64 64\nBetas (0.9, 0.999) ✗ (0.9, 0.999)\nEpsilon 1e−8(1e−30, 1e−3) 1e−6\nWeight decay ✗ ✗ ✗\nLR scheduler Cosine Cosine Cosine\nWarmup ratio 0.03 0.03 0.03\nClip threshold ✗ 1.0 ✗\nClamp value ✗ ✗ 10\nCluster number 3 3 2\nRecluster interval 1,000 1,000 1,000\nDecay rate (0.95, 0.9) (0.95, 0.9) (0.95, 0.9)\nLow-Rank hyperparameters\nLoRA ( r=128, α=256) ✓ ✗ ✗\n8bit LoRA ( r=128, α=256) ✓ ✗ ✗\nsults of SGG on VQA and benchmark tasks. Ta-\nble 8 shows the results of seven representative tasks,\nwhile Table A6 displays the full results of nine\ntasks. For the Full-Rank SFT, on the AdamW op-\ntimizer, SGG achieves 64.5 average performance\non the 7 different tasks, which brings +0.9% per-\nformance compared to the AdamW baseline. On\nthe Adafactor, SGG could get extra +0.5% per-\nformance compared to the vanilla Adafactor, espe-\ncially on the VizWzi VQA task, SGG could bring\n+2.4% capability. With LAMB+SGG, our perfor-\nmance can reach 52.7. For the LoRA SFT, our SGG\ncould achieve 65.1 scores, and on the VizWiz task,\nit brings additional performance gains of +2.2% .\nFor the 8-bit experiments, Table 8 shows that SGG\nwith AdamW could also bring some performance.\nC Empirical Analysis\nC.1 Analysis of Gradient Clustering\nFigure 2 illustrates the gradient clustering phe-\nnomenon observed during the pre-training of the\nLLaMA-1B model on the C4 dataset, focusing\non gradients, adaptive learning rates, and gradient\nnorms. LLMs exhibit unique gradient dynamics\ndue to their massive scale, sparse activations, and\nhierarchical structure. SGG leverages these char-\nacteristics to improve optimization efficiency and\nconvergence. Gradients in LLMs often follow a\nheavy-tailed distribution, with a small fraction of\nparameters contributing disproportionately to theoverall gradient magnitude. SGG addresses this by\nflattening gradients into high-dimensional vectors\nand applying clustering algorithms ( e.g.,k-means)\nto group parameters with similar behaviors. This\nresults in two distinct clusters: one for parameters\nwith large gradients (associated with salient fea-\ntures or rare tokens) and another for those with\nsmaller gradients (associated with frequent but less\ninformative tokens). Adaptive learning rates are\nthen computed separately for each cluster, ensuring\nstability for parameters with large gradients and\nfaster convergence for those with smaller gradients.\nThis contrasts with baselines that apply uniform\nlearning rates, failing to account for the heavy-\ntailed gradient distributions typical of LLMs.\nFigure 4(c) depicts the layer-wise L2-gradient\nnorm distributions across all layers of the LLaMA-\n1B model. Gradient norms vary significantly across\nlayers due to the hierarchical nature of LLMs. Ear-\nlier layers ( e.g., embedding and low-level trans-\nformer layers) exhibit smaller gradient norms, as\nthey focus on general syntactic and semantic pat-\nterns. In contrast, deeper layers ( e.g., higher-level\ntransformer layers) tend to have larger gradient\nnorms, as they model complex, context-dependent\nrelationships. SGG captures these patterns by\ngrouping parameters based on gradient norms and\napplying layer-wise learning rate scaling. This en-\nsures that earlier layers receive larger updates for\nfaster learning of general patterns, while deeper lay-\ners receive smaller updates to maintain stability and\nprevent overfitting. Baseline methods, which lack\nsuch adaptive scaling, often struggle to optimize\nall layers simultaneously, leading to suboptimal\nconvergence and poor generalization.\nThe clustering of gradients, adaptive learning\nrates, and gradient norms in LLMs are deeply inter-\nconnected phenomena. The heavy-tailed gradient\ndistribution directly influences adaptive learning\nrates, as parameters with large gradients are as-\nsigned smaller learning rates to prevent instability.\nThis, in turn, affects gradient norms, as learning\nrate scaling impacts the magnitude of parameter\nupdates. SGG’s ability to capture these relation-\nships and adaptively scale learning rates based on\ngradient clustering and norm distributions leads to\nmore stable and efficient optimization compared\nto baseline methods. Furthermore, the hierarchi-\ncal structure of LLMs introduces additional com-\nplexity, as different layers exhibit distinct gradient\nbehaviors. SGG addresses this by leveraging layer-\nwise clustering and scaling, ensuring each layer\n--- Page 19 ---\nTable A6: Full Comparison Results with Mainstream MLLMs . Compared with their counterparts, Top-1 accuracy\n(%↑: higher is better) is reported. A VG: The average of the nine benchmarks for comprehensive comparison, except\nfor MME.†: reproduced results using the official code. Green types denote the performance gains ↑of SGG ( blue\nback ground) over related baselines ( gray back ground). Most results are reported from LLaV A-KD (Cai et al.,\n2024b).\nMethod LLM OptimizerImage Question Answering BenchmarksA VGVQAv2 GQA VizWiz SciVQAITextVQA MME MMBench MMBenchCNPOPE SEEDI\nBLIP-2 Vicuna-13B AdamW 65.0 41.0 19.6 61.0 42.5 − − − 85.3 − −\nInstructBLIP Vicuna-7B AdamW − 49.2 34.5 60.5 50.1 − 36.0 23.7 79.8 − −\nQwen-VL Qwen-7B AdamW 78.8 59.3 35.2 67.1 63.8 − 38.2 7.4 − − −\nQwen-VL-Chat Qwen-7B AdamW 78.2 57.5 38.9 68.2 61.5 − 60.6 56.7 − − −\nmPLUG-Owl2 LLaMA2-7B AdamW 79.4 56.1 54.5 68.7 54.3 − 66.5 − 85.8 − −\nTinyLLaV A†Qwen1.5-4B AdamW 79.9 63.4 46.3 72.9 59.0 − 67.9 67.1 85.2 − −\nTinyLLaV A Phi2-2.7B AdamW 79.9 62.0 − 69.1 59.1 − 66.9 − 86.4 − −\nBunny Phi2-2.7B AdamW 79.8 62.5 43.8 70.9 56.7 − 68.6 37.2 − − −\nImp-3B Phi2-2.7B AdamW − 63.5 54.1 72.8 59.8 − 72.9 46.7 − − −\nMobileVLM MLLaMA-2.7B AdamW − 59.0 − 61.0 47.5 − 59.6 − 84.9 − −\nMobileVLMv2 MLLaMA-2.7B AdamW − 61.1 − 70.0 57.5 − 63.2 − 84.7 − −\nMoE-LLaV A Phi2-2.7B AdamW 79.9 62.6 − 70.3 57.0 − 68.0 − 85.7 − −\nLLaV A-Phi Phi2-2.7B AdamW 71.4 − − 68.4 48.6 − 59.8 − 85.0 − −\nLLaV A-NeXT Vicuna-1.5-7B AdamW 81.8 64.2 57.6 70.1 64.9 1519.0 67.4 60.6 86.5 70.2 69.3\nLLaV A-NeXT Vicuna-1.5-13B AdamW 82.8 65.4 60.5 73.6 67.1 1575.0 70.0 64.4 86.2 71.9 71.3\nMiniCPM-V MiniCPM-2.4B AdamW − 51.5 50.5 74.4 56.6 − 64.0 62.7 79.5 − −\nMiniCPMv2 MiniCPM-2.4B AdamW − 52.1 60.2 76.3 73.2 − 68.5 67.2 86.3 − −\nLLaV A-MOD Qwen1.5-1.8B AdamW − 58.7 39.2 68.0 58.5 − 66.3 61.9 87.0 − −\nLLaV A-KD-2B Qwen1.5-1.8B AdamW 79.0 62.3 44.7 64.7 53.4 − 64.0 63.7 86.3 − −\nLLaVA-v1.5/1.6 Full-Rank SFT\nLLaV A-v1.5 Vicuna-1.5-7B AdamW 78.5 62.0 50.0 66.8 58.2 1510.7 64.3 58.3 85.9 66.2 65.6\nLLaV A-v1.5 Vicuna-1.5-7B Adafactor 79.0 62.7 48.2 70.7 57.1 1462.5 66.1 60.4 86.0 66.8 66.3\nLLaV A-v1.5 Vicuna-1.5-7B LAMB 63.9 43.8 53.3 61.5 43.4 1090.9 43.2 41.8 81.2 50.4 53.6\nLLaV A-v1.5 Vicuna-1.5-7B Muon 79.3 62.6 50.3 69.1 57.7 1461.7 67.1 59.8 85.9 67.0 66.5\nLLaV A-v1.5 Vicuna-1.5-7B SOAP 79.4 62.5 47.8 69.7 57.9 1457.1 66.6 60.1 86.2 67.4 66.4\nLLaV A-v1.5 Vicuna-1.5-7B MARS 79.3 62.8 49.2 69.1 56.4 1451.1 66.7 59.4 86.1 67.5 66.3\nLLaV A-v1.5 Vicuna-1.5-7B AdamW+ SGG 79.1 62.4 50.2 69.8 57.4 1476.9 65.9 60.1 86.3 66.9 66.5\n∆Gains compared to AdamW +0.6 +0.4 +0.2 +2.0 -0.8 -33.8 +1.6 +1.8 +0.4 +0.7 +0.9\nLLaV A-v1.5 Vicuna-1.5-7B Adafactor+ SGG 79.2 62.8 50.6 71.6 57.3 1477.2 66.3 60.8 86.0 67.3 66.8\n∆Gains compared to Adafactor +0.1 +0.1 +2.4 +0.9 +0.2 +14.7 +0.2 +0.4 +0.0 +0.5 +0.5\nLLaV A-v1.5 Vicuna-1.5-7B LAMB+ SGG 64.3 44.0 53.3 61.8 43.5 1122.9 43.3 41.9 81.3 50.4 53.8\n∆Gains compared to LAMB +0.4 +0.2 +0.0 +0.3 +0.1 +32.0 +0.1 +0.1 +0.1 +0.1 +0.2\nLLaVA-v1.5 Low-Rank SFT (AdamW)\nLLaV A-v1.5 Vicuna-1.5-7B LoRA 79.1 63.0 47.8 68.4 58.2 1466.2 66.1 58.9 86.4 67.8 66.2\nLLaV A-v1.5 Vicuna-1.5-7B LoRA+ SGG 79.1 63.4 51.0 70.1 58.6 1477.8 66.7 59.4 86.6 68.2 67.0\n∆Gains compared to LoRA +0.0 +0.4 +2.2 +1.5 +0.4 +11.6 +0.6 +0.5 +0.2 +0.4 +0.8\nis optimized according to its specific role. This\nis particularly critical for LLMs, where the inter-\nplay between low-level and high-level features is\nessential for capturing the nuances of natural lan-\nguage. By preserving the inherent structure of the\noptimization landscape, SGG not only improves\nconvergence but also enhances the model’s ability\nto generalize to unseen data.\nC.2 Analysis of Learning Rate Scaling\nWe analyze the impact of learning rate scaling\non the validation perplexity of the Qwen2.5-0.5B\nmodel fine-tuned on the Alpaca dataset. The ex-\nperiments were conducted with varying batch sizes{128, 512, 1024, 2048, 4096} and learning rates\n{1e-1, 1e-2, 1e-3, 1e-4, 1e-5}, using both the Adam\noptimizer and Adam with SGG. The model was\ntrained for 3 epochs with LoRA (rank=8) and fol-\nlowed the official settings of the Alpaca framework.\nThe results, as depicted in Figure 5, demonstrate\nseveral key trends. First, as the batch size increases,\nthe validation perplexity generally decreases, indi-\ncating that larger batch sizes contribute to more\nstable and efficient training. This effect is par-\nticularly pronounced when SGG is applied, sug-\ngesting that SGG enhances the model’s ability to\ngeneralize even under extreme batch size settings.\nSecond, lower learning rates ( e.g., 1e-4, 1e-5) con-\n--- Page 20 ---\nsistently yield better performance, especially when\ncombined with larger batch sizes, highlighting the\nimportance of balancing these hyperparameters.\nNotably, SGG provides robust performance gains\nacross all configurations, significantly reducing val-\nidation perplexity compared to standard Adam opti-\nmization. This improvement is attributed to SGG’s\nability to guide the optimization process more ef-\nfectively, particularly in scenarios with large batch\nsizes and varying learning rates. Overall, the results\nunderscore the effectiveness of SGG in enhancing\nmodel performance, even in challenging training\nconditions, and emphasize the critical role of hy-\nperparameter tuning in achieving optimal results.",
  "text_length": 87751
}