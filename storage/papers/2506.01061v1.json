{
  "id": "http://arxiv.org/abs/2506.01061v1",
  "title": "AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation",
  "summary": "Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task\nthat synthesizes intermediate frames between existing ones while maintaining\nspatial and temporal coherence. VFI techniques have evolved from classical\nmotion compensation-based approach to deep learning-based approach, including\nkernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently\ndiffusion model-based approach. We introduce AceVFI, the most comprehensive\nsurvey on VFI to date, covering over 250+ papers across these approaches. We\nsystematically organize and describe VFI methodologies, detailing the core\nprinciples, design assumptions, and technical characteristics of each approach.\nWe categorize the learning paradigm of VFI methods namely, Center-Time Frame\nInterpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze\nkey challenges of VFI such as large motion, occlusion, lighting variation, and\nnon-linear motion. In addition, we review standard datasets, loss functions,\nevaluation metrics. We examine applications of VFI including event-based,\ncartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by\noutlining promising future research directions to support continued progress in\nthe field. This survey aims to serve as a unified reference for both newcomers\nand experts seeking a deep understanding of modern VFI landscapes.",
  "authors": [
    "Dahyeon Kye",
    "Changhyun Roh",
    "Sukhun Ko",
    "Chanho Eom",
    "Jihyong Oh"
  ],
  "published": "2025-06-01T16:01:24Z",
  "updated": "2025-06-01T16:01:24Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01061v1",
  "full_text": "--- Page 1 ---\n1\nAceVFI: A Comprehensive Survey of\nAdvances in Video Frame Interpolation\nDahyeon Kye, Changhyun Roh, Sukhun Ko, Chanho Eom, and Jihyong Oh†\nhttps://github.com/CMLab-Korea/Awesome-Video-Frame-Interpolation\nAbstract —Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task that synthesizes intermediate\nframes between existing ones while maintaining spatial and temporal coherence. VFI techniques have evolved from\nclassical motion compensation-based approach to deep learning-based approach, including kernel-, flow-, hybrid-, phase-,\nGAN-, Transformer-, Mamba-, and more recently diffusion model-based approach. We introduce AceVFI, the most\ncomprehensive survey on VFI to date, covering over 250+ papers across these approaches. We systematically organize\nand describe VFI methodologies, detailing the core principles, design assumptions, and technical characteristics of each\napproach. We categorize the learning paradigm of VFI methods namely, Center-Time Frame Interpolation (CTFI) and\nArbitrary-Time Frame Interpolation (ATFI). We analyze key challenges of VFI such as large motion, occlusion, lighting\nvariation, and non-linear motion. In addition, we review standard datasets, loss functions, evaluation metrics. We examine\napplications of VFI including event-based, cartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by\noutlining promising future research directions to support continued progress in the field. This survey aims to serve as a\nunified reference for both newcomers and experts seeking a deep understanding of modern VFI landscapes. We maintain\nan up-to-date project page: https://github.com/CMLab-Korea/Awesome-Video-Frame-Interpolation.\nIndex Terms —Video Frame Interpolation, Generative Inbetweening, Video Generation, Low-Level Vision\n✦\n1 I NTRODUCTION\nVIDEO Frame Interpolation (VFI) aims to increase\nthe temporal resolution ( i.e., frame rate) of a\nvideo sequence by synthesizing one or more inter-\nmediate frames between given consecutive frames.\nThis task serves a broad range of applications, in-\ncluding novel view synthesis [1]–[4], slow-motion gen-\neration [5]–[10], video compression [11]–[14], video\nprediction [13], [15]–[17], and diverse generation tasks\nsuch as co-speech reenactment [18], human motion\nsynthesis [19], and facial animation [20]. A key advan-\ntage of VFI lies in its ability to synthesize perceptu-\nally smooth and temporally coherent motion, aligning\nwell with the temporal characteristics of the human\nvisual system (HVS). High-frame-rate (HFR) content\nreduces artifacts such as motion blur and judder [21],\n[22], thereby enhancing the visual quality in high-\nresolution (HR) and immersive media. This makes VFI\n•Dahyeon Kye, Changhyun Roh, Sukhun Ko and Jihyong Oh are\nwith the Department of Imaging Science, GSAIM, Chung-Ang\nUniversity, Seoul, South Korea (e-mail: rpekgus@cau.ac.kr;\nchanghyunroh@cau.ac.kr; looloo330@cau.ac.kr; jihyon-\ngoh@cau.ac.kr).\n•Chanho Eom is with the Department of Metaverse Convergence,\nGSAIM, Chung-Ang University, Seoul, South Korea (e-mail:\ncheom@cau.ac.kr).\n†denotes corresponding author.\n!!\t\t!!\"\t⋱\t!#\"\t\t⋱\t\t\t\t\t\t\t\t\t\t\t\t!\"$!\"\t\t!\"VFI Model (\tℱ\t)×&frame interpolation!!!\"\n'=1\n0!\"⋱\t\t\t\t\t\t\t\t\t\t\t\t\t#\"\t\t\t⋱\t\"$!\"\t\t1Fig. 1. General process of VFI. Given two input frames I0\nandI1, the VFI model Fsynthesizes one or more intermediate\nframes. ×ninterpolation denotes synthesizing n−1intermediate\nframes to increase the frame rate by a factor of n.\nparticularly valuable in latency-sensitive and fidelity-\ncritical scenarios such as sports broadcasting, interac-\ntive gaming, and virtual reality. Finally, in streaming\npipelines, VFI also enables bandwidth-efficient video\ntransmission by reconstructing intermediate frames\nlocally, reducing the need to transmit full frame se-\nquences [21].\nFormally, given two frames I0andI1, a VFI model\nFestimates the interpolated frame ˆItat time t∈(0,1):\nˆIt=F(I0, I1, t). (1)\nAs shown in Fig. 1, interpolating n−1frames between\neach input pair increases the frame rate by a factor ofarXiv:2506.01061v1  [cs.CV]  1 Jun 2025\n--- Page 2 ---\n2\nTemporalAlignment\nTemporalAlignment\n!!\n!\"\"!\n\"\"!#!→$ / \"$!→$ !#$Feature Extraction\n!#\"→$ / \"$\"→$ Feature Extraction\n%\nℱ\nFrame SynthesisMotion Estimation\nFig. 3\nFig. 2. General pipeline of VFI. Given two input frames I0andI1, the feature representations F0andF1are extracted, which are\nthen aligned to the target time tusing the estimated motion. The temporal alignment can be performed on either input RGB pixels\nor features, resulting in ˆI0→t,ˆI1→torˆF0→t,ˆF1→t. Finally, a Frame Synthesis module blends the aligned inputs to produce the\ninterpolated frame ˆIt. This pipeline highlights the four core stages of VFI: Feature Extraction, Motion Estimation, Temporal Alignment,\nand Frame Synthesis.\nn. For example, generating seven intermediate frames\nper interval transforms a 30fps video into 240fps.\n1.1 General Pipeline of VFI\nAs shown in Fig. 2, the general VFI pipeline consists\nof four stages. (i) Feature Extraction: Input frames I0\nandI1are first passed through a feature extraction\nnetwork [23]–[25] to obtain deep features F0andF1.\nThese features encode spatial and semantic informa-\ntion suitable for subsequent motion reasoning [26].\n(ii) Motion Estimation: The temporal correspondence,\ncommonly referred to as motion , is estimated. Mo-\ntion estimation is performed either explicitly ( e.g.,\noptical flow [27]) or implicitly via kernels [8], [16],\n[28]–[47], phase [48], [49], attention maps [50]–[57],\nor cost volumes [35], [53], [58], [59]. (iii) Temporal\nAlignment: The estimated motion is used to tempo-\nrally align the input pixels or features to the target\ntime t, resulting in ˆI0→t,ˆI1→torˆF0→t,ˆF1→t. There\nare four types of alignment strategies. Kernel-based\nalignment (Fig. 3 (a)) aggregates local or non-local\ninformation from the inputs using learned, spatially-\nadaptive kernels. These kernels implicitly encode mo-\ntion by adapting their spatial weights based on local\ncontext, allowing motion-aware alignment without ex-\nplicit flow estimation. Flow-based alignment (Fig. 3 (b))\nwarps inputs guided by the estimated flow. Forward\nwarping [60] maps source pixels ( i.e., input pixels) to\ntheir estimated locations in the target frame. Back-\nward warping [61] samples from the source based\non coordinates in the target frame, effectively pulling\ninformation from the source toward the desired time.\nAttention-based alignment (Fig. 3 (c)) replaces explicit\ngeometric warping with attention-weighted aggrega-\ntion [51], [53]. By computing soft correspondences be-\nTemporalAlignment\nTemporalAlignment(a)(b)(c)!\n(a) Kernel(b) Flow(c) Attention Map(d) Cost Volume\n(d)!\"\"!/$!\n\"\"/$\"\"%!→$/$&!→$\n\"%\"→$/$&\"→$Motion EstimationFig. 3. Temporal alignment process. The input frames (I0, I1)\nor their features (F0, F1)are temporally aligned toward a target\ntimet. (a) kernel-based alignment using spatially-adaptive filters,\n(b) flow-based alignment guided by optical flow, (c) attention-\nbased alignment using attention-weighted correspondences, and\n(d) cost volume-based alignment through pixel- or feature-level\nsimilarity estimation.\ntween elements across input frames, this approach can\nadaptively focus on semantically relevant regions and\nalign contents even across large spatial-temporal gaps.\nCost volume-based alignment (Fig. 3 (d)) constructs\ndense similarity volumes between feature maps, en-\nabling precise correspondence modeling across space\nand time. (iv) Frame Synthesis: Finally, the aligned\ninputs are blended to synthesize the final interpolated\nframe using either simple averaging, weighted blend-\ning or synthesis networks [62].\n--- Page 3 ---\n3\nVideo Frame InterpolationI. Introduction (Sec. 1)General Pipeline (Sec. 1.1)\nMethodology Overview (Sec. 1.2)\nII. Methodology (Sec. 2)Motion Compensation-based (Sec. 2.1)\nDeep Learning-based (Sec. 2.2)Kernel-based (Sec. 2.2.1)\nFlow-based (Sec. 2.2.2)\nKernel- and Flow-based\nCombined (Sec. 2.2.3)\nPhase-based (Sec. 2.2.4)\nGAN-based (Sec. 2.2.5)\nTransformer-based (Sec. 2.2.6)\nMamba-based (Sec. 2.2.7)Diffusion-based (Sec. 2.3)\nIII. Learning\nParadigm (Sec. 3)Center-Time Frame Interpolation\n(CTFI) (Sec. 3.1)\nArbitrary-Time Frame Interpolation\n(ATFI) (Sec. 3.2)\nTraining Strategy (Sec. 3.3)CTFI Training Strategy\n(CTFI-TS) (Sec. 3.3.1)\nATFI Training Strategy\n(ATFI-TS) (Sec. 3.3.2)\nLoss Functions (Sec. 3.4)Reconstruction Loss (Sec. 3.4.1)\nPerceptual Loss (Sec. 3.4.2)\nAdversarial Loss (Sec. 3.4.3)\nFlow Loss (Sec. 3.4.4)\nIV. VFI Challenges (Sec. 4)Large Motion (Sec. 4.1)\nOcclusion (Sec. 4.2)\nLighting Variation (Sec. 4.3)\nNon-linear Motion (Sec. 4.4)\nV. Datasets and\nEvaluation Metrics (Sec. 5)Datasets (Sec. 5.1)Triplet Datasets (Sec. 5.1.1)\nMulti-frame Datasets (Sec. 5.1.2)\nData Augmentation (Sec. 5.2)\nEvaluation Metrics (Sec. 5.3)Image-level Metrics (Sec. 5.3.1)\nPerceptual Metrics (Sec. 5.3.2)\nVideo-level Metrics (Sec. 5.3.3)\nVI. Applications (Sec. 6)Event-based VFI (Sec. 6.1)\nCartoon VFI (Sec. 6.2)\nMedical Image VFI (Sec. 6.3)\nJoint Task VFI (Sec. 6.4)\nVII. Future Research\nDirections (Sec. 7)Video Streaming Service (Sec. 7.1)\nAll-in-One\nLLV Video Restoration (Sec. 7.2)\n3D and 4D\nScene Understanding (Sec. 7.3)\nFig. 4. Overview of the survey structure. The figure summarizes the hierarchical organization of the survey, including methodological\ncategories (Sec. 2), learning paradigms (Sec. 3), key challenges (Sec. 4), datasets and evaluation metrics (Sec. 5), applications\n(Sec. 6), and future research directions (Sec. 7).\n1.2 Methodology Overview\nVFI methodologies can be broadly classified into three\nmajor categories: motion compensation-based [63]–\n[72], deep learning-based [7], [16], [28]–[46], [48]–[62],\n[73], [74], [74]–[100], and diffusion models (DMs)-\nbased approach [96], [101]–[119].\nThe motion compensation-based approach domi-\nnated the pre-deep-learning era, offering a straightfor-\nward two-stage strategy: estimating motion explicitly\nand warping frames accordingly. While effective un-\nder simple motion, its reliance on hand-crafted rules\nand block-based assumptions limits its ability to han-dle occlusions and complex, non-rigid dynamics.\nWith the advent of convolutional neural networks\n(CNNs) [120], the field shifted toward deep learning-\nbased approach. This approach replaces heuristic\npipelines with end-to-end frameworks that learn mo-\ntion patterns and appearance features directly from\ndata. As a result, they significantly improve robustness\nunder diverse and challenging conditions. Further\nmethodological details are discussed in Sec. 2.2.\nMore recently, DMs have been introduced as a\ngenerative perspective to VFI, framing it as a con-\nditional denoising process rather than a determinis-\n--- Page 4 ---\n4\ntic prediction task. This has expanded the scope of\nVFI into the broader paradigm of Generative Inbe-\ntweening [111], [112], enabling uncertainty-aware in-\nterpolation and semantically diverse frame synthesis.\nThis shift not only enhances robustness in ambiguous\nmotion scenarios but also opens the door to multi-\nmodal guidance ( e.g., text, depth, or motion priors),\nredefining the role of VFI in creative and interactive\nvideo generation.\nOverview. Fig. 4 shows the overall structure of this\npaper. Sec. 2 analyzes methodological taxonomies of\nVFI. Sec. 3 introduces and compares the two principal\nlearning paradigms of VFI, and further examines their\ncorresponding training strategies and loss functions.\nSec. 4 discusses major challenges in VFI, along with\nhow recent methods address them. Sec. 5 reviews com-\nmon datasets and evaluation metrics. Sec. 6 explores\napplications of VFI across diverse domains. Finally,\nSec. 7 presents future research directions of VFI.\n2 M ETHODOLOGY\n2.1 Motion Compensation-based\nBefore the advent of deep-learning, VFI was primarily\ntackled using Motion-Compensated Frame Interpolation\n(MCFI) [64], [67], [68], [70] or Frame Rate Up-Conversion\n(FRUC) [63], [65], [66], [69], [71], [72], which domi-\nnated the field from the late 1990s through the early\n2000s. These approaches explicitly estimate motion,\ntypically via block matching or global parametric\nmodels, and synthesize the intermediate frame by\nwarping the input frames according to the estimated\nmotion fields. A typical MCFI pipeline involves two\nsteps: (i) block-based motion estimation and (ii) pixel-\nlevel warping for frame synthesis. In block-based\nestimation, the frame is partitioned into fixed-size\nrectangular blocks, assuming uniform motion within\neach region. While this formulation offers computa-\ntional efficiency, it fails to capture non-rigid or object-\nspecific motion, often resulting in artifacts such as\nholes (due to occlusions) and overlaps (due to many-\nto-one mappings). Rooted in classical video coding\nframeworks [121], MCFI methods emphasize speed\nand simplcity, but inherently lack the capacity to\nhandle fine-grained, non-linear motion. To address\nthese issues, several extensions are proposed, includ-\ning multi-stage estimation [70], adaptive motion mod-\nels [66], and occlusion-aware warping [72]. Interme-\ndiate frame synthesis is typically achieved via block-\nwise projection or forward guided by the estimated\nmotion.\nDespite their limited robustness under complex\ndynamics, MCFI and FRUC methods [63]–[72] lay\nthe conceptual foundation for modern VFI. This core\nprinciple, which involves explicit motion estimation\n!\"!($,&)\n(a) Standard convolution\n!\"($+),&+*)+,\"#,%+,\"#,%($,&)(b) Deformable convolution!\"($+)+⍺#,%,&+*+\t/#,%)\n(c) Dynamic convolution!\"($+)+⍺#,%,&+*+\t/#,%)\n!\"!($,&)!\"!($,&)+,\"#,%Eq. (2) Eq. (4) Eq. (5)\nFig. 5. Comparison of different convolution types. (a) Stan-\ndard convolution samples at a fixed grid location (x+k, y+l). (b)\nDeformable convolution introduces learnable offsets (αk,l, βk,l),\nenabling adaptive sampling at (x+k+αk,l, y+l+βk,l). (c)\nDynamic convolution further generalizes this by predicting the\nkernel weights Wk,l\ni(x, y)dynamically for each output position,\nallowing for spatially-variant filtering.\nfollowed by motion-guided warping, remains cen-\ntral to many modern learning-based models and\nis now enhanced with deep feature representations\nand end-to-end training. Importantly, classical motion-\ncompensated strategies offer valuable insights into the\ninductive biases that shape modern VFI architectures.\nConcepts such as motion locality, piecewise rigidity,\nand spatial warping, which originated from block-\nbased estimation, are implicitly retained in modern\nmechanisms including deformable convolutions [32],\n[122] and local attention [123]. Furthermore, the chal-\nlenges encountered in this approach, such as occlu-\nsion handling and motion discontinuity, have directly\ninfluenced the design of occlusion-aware blending\nmodules and bidirectional flow formulations in re-\ncent models. In this light, traditional motion models\nserve as both a historical foundation and a conceptual\nframework for the progressive development of VFI\narchitectures.\n2.2 Deep Learning-based\n2.2.1 Kernel-based\nKernel-based VFI methods [8], [16], [28]–[47], [56] syn-\nthesize intermediate frames by predicting spatially-\nadaptive convolutional kernels , which are subse-\nquently applied to local patches extracted from the in-\nput frames. Motion information is implicitly encoded\nin the kernel weights, thereby enabling motion-aware\npixel aggregation without explicit motion estimation.\nAs shown in Fig. 5 (a), a standard kernel-based inter-\npolation can be mathematically formulated as:\nˆI(x, y) =N−1X\ni=0R−1X\nk=0R−1X\nl=0Wk,lIi(x+k, y+l),(2)\nwhere Ndenotes the number of input frames, Rde-\nnotes the kernel size, and Wk,lrepresents the learned\nkernel weight at offset (k, l). This approach adopts\na simple single-stage formulation that combines mo-\ntion estimation and frame synthesis into a one-step\n--- Page 5 ---\n5\nprocess. AdaConv [28] utilizes a U-Net-like archi-\ntecture [24] to predict spatially-varying 2D kernels\nfor each output pixel. This enables local, pixel-wise\nmotion-aware aggregation that can implicitly handle\nboth alignment and occlusion [46]. SepConv [29] fur-\nther reduces the computational overhead by decom-\nposing the 2D kernel into separable 1D kernels:\nW=Wv∗Wh, (3)\nwhere Wv∈RR×1andWh∈R1×Rare vertical and\nhorizontal 1D kernels respectively. The ∗denotes the\nouter product between the two 1D kernels, resulting\nin a full 2D kernel W∈RR×R. This decomposition\nreduces the number of learnable parameters from R2\nto2R, while maintaining a comparable receptive field.\nDespite their conceptual simplicity, these methods are\ninherently limited in handling large displacements\ndue to their fixed receptive fields [74]. Such constraints\nstem from the content-agnostic nature of CNNs, which\nuniformly apply learned filters across spatial loca-\ntions [51]. While this weight-sharing inductive bias\nproves effective in recognition tasks, it becomes sub-\noptimal in VFI, where fine-grained motion modeling\nis essential. To overcome this problem, deformable\nkernel-based methods [8], [32], [34], [37], [39], [40],\n[42], [44], [51], [56] introduce learnable offsets [122]\nas shown Fig. 5 (b), which allow sampling outside the\nregular convolution grid:\nˆI(x, y) =N−1X\ni=0R−1X\nk=0R−1X\nl=0Wk,l\n·Ii(x+k+αk,l, y+l+βk,l), (4)\nwhere (αk,l, βk,l)are the learnable offsets. Ada-\nCoF [36] estimates both kernel weights and sampling\noffsets for each output pixel, though it employs a\nfixed offset pattern, limiting its expressiveness under\ncomplex motion. To enhance spatial adaptivity further\nas shown in Fig. 5 (c), dynamic kernel-based meth-\nods [35], [41], [44], [58], [124] make location-dependent\nkernel weights:\nˆI(x, y) =N−1X\ni=0R−1X\nk=0R−1X\nl=0Wk,l(x, y)\n·Ii(x+k+αk,l, y+l+βk,l), (5)\nwhere Wk,l(x, y)denotes a dynamically predicted ker-\nnel at location (x, y). Methods such as CDFI [41] and\nMSEConv [44] jointly learn spatially-varying weights\nand offsets, offering enhanced flexibility and im-\nproved interpolation accuracy.\nOverall, kernel-based methods forego explicit mo-\ntion supervision, instead leveraging learned spatial\npriors for synthesizing intermediate frames. As ker-\nnel prediction and convolution-based synthesis aretightly coupled, motion estimation and frame synthe-\nsis are implicitly fused into a single-stage. This implicit\nformulation offers robustness in noisy or uncertain\nmotion settings and eliminates the dependency on\naccurate optical flow. However, these models often\nlack temporal generalizability, as the learned kernels\nare typically conditioned on a fixed interpolation time\n(e.g.,t= 0.5). Consequently, most kernel-based meth-\nods are constrained to CTFI (Sec. 3.1) and fail to gen-\neralize to ATFI (Sec. 3.2), limiting their applicability in\nreal-world scenarios requiring temporal flexibility.\n2.2.2 Flow-based\nFlow-based methods [16], [30], [33], [35], [37]–[39],\n[47], [53]–[55], [58]–[60], [62], [73]–[85], [87]–[90],\n[125] explicitly estimate dense motion in the form\nofoptical flow , a dense motion field representing\nthe pixel-wise displacements between two frames,\nto temporally align input frames and synthesize in-\ntermediate frames. Recent advances in optical flow\nestimation [126]–[136] have directly propelled the\nperformance of flow-based VFI models. A typical\npipeline comprises: (1) estimating either anchor flows\n(V0→t,V1→t) orintermediate flows (Vt→0,Vt→1), (2) ap-\nplying flow-guided warping [60], [61] of input frames\nor features ( I0, I1orF0, F1), and (3) synthesizing\nthe target frame ( ˆIt) by blending the warped results\n(ˆI0→t/ˆF0→tandˆI1→t/ˆF1→t).\nThe accuracy of the flow critically impacts in-\nterpolation quality in this approach, as misalign-\nment directly causes blur and artifacts. Many earlier\nworks [7], [30], [60] adopt off-the-shelf optical flow\nnetworks [126]–[136] to estimate the initial flows. Al-\nthough these networks offer reliable motion estima-\ntion, they are not specifically optimized for the VFI\ntask and often introduce unnecessary architectural\ncomplexity. Moreover, they tend to have large model\nsizes and struggle to handle extreme motions that lie\noutside the training distribution [125]. To better adapt\nthe motion estimation to the VFI task, a number of\nmethods [5], [6], [16], [35], [53]–[55], [57], [59], [62],\n[74], [81], [83], [97], [100], [125] propose to estimate\ntheir own task-oriented flow within their framework,\nwhich is optimized jointly with the frame interpo-\nlation objective. For example, BiM-VFI [137] distills\nflow knowledge from an ensemble of flow predictors\ninto a lightweight network tailored for interpolation.\nGIMM-VFI [88] addresses the noise in flows from pre-\ntrained optical flow estimator ( e.g., RAFT [134], Flow-\nFormer [135]) by refining them through a coordinate-\nbased implicit networks. Pseudo ground-truth (GT)\nstrategies are also common, where pseudo GT flow is\ngenerated by existing flow networks and used as weak\nsupervision to bootstrap VFI training [16], [79]. These\nhelp produce temporally consistent and semantically\naligned flows customized for interpolation. With the\n--- Page 6 ---\n6\n(a) Forward warping(b) Backward warping𝐼!𝐼\"!→#\n❗collisions\n❗holes\n𝐼!𝐼\"!→#\nEq. (7) Eq. (11)\nFig. 6. Comparison of forward and backward warping strate-\ngies. (a) Forward warping [60] projects source pixels ( I0) to their\nestimated positions in the target frame using V0→t. This may\nintroduce holes (unmapped pixels) or collisions (multiple pixels\nmapped to the same location). (b) Backward warping [61] sam-\nples each pixel in the target frame from the source using Vt→0.\nSince sampling is performed at every target location, backward\nwarping naturally produces dense and complete outputs.\nestimated flow, warping is implemented via either\nforward [60] or backward [61] warping operation. De-\npending on the warping operation, it requires different\ntypes of flows.\nForward warping. Most forward warping-based\nmethods [6], [30], [35], [38], [60], [62], [73], [74],\n[80], [85], [125] first estimate bidirectional flows\n(V0→1,V1→0), from which the intermediate flows\n(V0→t,V1→t)are linearly interpolated:\nˆV0→t=t· V0→1,ˆV1→t= (1−t)· V1→0.(6)\nThese flows are used to project source pixels to the\nintermediate frame:\nˆI0→t=→\nWf(I0,ˆV0→t),ˆI1→t=→\nWf(I1,ˆV1→t).(7)\nHowever, forward warping introduces structural arti-\nfacts such as holes (unmapped regions) and collisions\n(multiple pixels mapping to the same target position)\nparticularly near motion boundaries [5] as shown in\nFig. 6 (a). Unlike backward warping, which ensures\ndense sampling by mapping each pixel in the target\ndomain, forward warping does not guarantee full\ncoverage due to its source-driven formulation. This\nintrinsic asymmetry stems from the lack of inverse\nconsistency in optical flow, i.e.,Vi→j̸=−Vj→iin\ngeneral, particularly under occlusions or non-rigid\nmotion. To address this, SoftSplat [60] proposes a\nsoftmax-based splatting mechanism:\n→\nWf(I0,V0→t) =⃗P(exp( Z)·I0,V0→t)\n⃗P(exp( Z),V0→t), (8)\nwhere Zdenotes a learned importance map ( e.g.,\ndepth), and the operator ⃗Pdenotes a differentiable\nsplatting with soft aggregation. The soft aggregation\nscheme in SoftSplat not only mitigates hole/collision\nartifacts but also improves the gradient flow by mak-\ning warping fully differentiable, in contrast to stan-\ndard splatting operations which are piecewise con-stant and non-smooth. Despite this, the inherent ar-\ntifacts make naive forward warping a less favored\nprimary choice.\nBackward warping. In contrast, backward warp-\ning [5], [33], [55], [59], [76], [81] samples each pixel\nin the target frame by mapping it back to the input\nusing estimated intermediate flows (Vt→0,Vt→1). In-\ntermediate flows denote the flows from the unknown\ntarget frame to the input frames. Since the target frame\nis unavailable, it is not straightforward to obtain the\nintermediate flows. These flows can be approximated\nvia direct prediction [6], [16], [35], [52]–[55], [57]–\n[59], [81], [90], [100], flow interpolation [5], [7], or\nflow reversal [76], [77], [79], [89]. For instance, Super-\nSloMo [5] employs such linear approximations and\nfurther refines them via dedicated subnetworks. The\nflow interpolation for intermediate flows is defined as:\nˆVt→0=−t· V0→1ort· V1→0 (9)\nˆVt→1= (1−t)· V0→1or−(1−t)· V1→0. (10)\nTo enhance robustness, XVFI [79] introduces Comple-\nmentary Flow Reversal (CFR), a weighted aggregation\nstrategy that fuses multiple reversed and complemen-\ntary flows to construct robust intermediate motion\nfields. This strategy complements the shortcomings\nof both linear flow approximation and naive flow\nreversal [76], offering robustness against ambiguities\nnear motion boundaries. Given the intermediate flows,\nbackward warping is applied as:\nˆI0→t=←\nWb(I0,ˆVt→0),ˆI1→t=←\nWb(I1,ˆVt→1),(11)\nwhere←\nWbdenotes the backward warping oper-\nator [61]. The warped results are blended using\nocclusion-aware mask Mand residual refinement R:\nIt=M⊙ˆI0→t+ (1−M)⊙ˆI1→t+R, (12)\nThe operator ⊙denotes element-wise multiplication,\nor the Hadamard product, which blends the warped\nframes proportionally based on the occlusion-aware\nconfidence map. Additionally, some methods [5], [75],\n[76], [79] further incorporates (1−t)andtas scalar\nweights into Mto guide time-aware blending. Several\nmethods also exploit auxiliary priors such as depth [7],\ncontextual features [7], [30], [33], [35], [39], [60], or\nedge information [37], [73], [77] to further guide inter-\npolation. Also, learnable synthesis networks [138] or\nadditional post-processing can further improve sharp-\nness and correct residual artifacts.\nModeling non-linear motion. Many early methods\nassume linear motion and brightness constancy [5]–\n[7], [16], [30], [33], [35], [60], [73], [74], [76], [85],\nmeaning that objects move along a straight trajectories\nat constant speed, and pixel intensities remain un-\nchanged. However, these assumptions often fail under\n--- Page 7 ---\n7\nreal-world scenarios. Quadratic [76], [77], [86], [139]\nor cubic [62] motion modeling has been proposed\nto account for acceleration. QVI [76] and EQVI [77]\nestimate acceleration-aware flows utilizing four input\nframes. While recent works [90], [137] further explore\nvelocity ambiguity [90], which refers to the ill-posed na-\nture of intermediate motion inference where multiple\ntrajectories yield the same intermediate position, es-\npecially under occlusion or acceleration. BimVFI [137]\nand Zhong et al [90] introduce bidirectional motion\nfields and time-aware reasoning mechanisms to dis-\nambiguate such cases, enabling robust interpolation\nunder occlusion, acceleration, and non-linear motion.\nOverall, flow-based methods remain one of the\nmost extensively explored and practically adopted\napproaches in VFI, owing to their explicit and inter-\npretable modeling of motion trajectories. Their abil-\nity to flexibly generate intermediate frames for ar-\nbitrary timestamps makes them well-suited for vari-\nous real-world scenarios requiring variable frame-rate\nsynthesis. Despite these strengths, their performance\nis sensitive to flow estimation accuracy, particularly\nunder conditions of occlusion, large motion, lighting\nvariation or non-linear motion. As research in optical\nflow continues to evolve [140], [141], flow-based VFI\nis expected to further benefit from these developments\nand remain a foundational component of future VFI\napproach.\n2.2.3 Kernel- and Flow-based Combined\nKernel- and flow-based approaches each offer distinct\nstrengths in VFI. Flow-based methods [7], [16], [30],\n[33], [35], [37], [38], [74], [97] estimate dense motion\nfields to align frames in a temporally coherent manner,\nbut their performance degrades due to inaccurate flow\nestimation or the presence of occlusions. In contrast,\nkernel-based methods [7], [8], [16], [28], [29], [31]–[44],\n[46], [47], [56], [74], [97] directly synthesize pixels us-\ning learned, spatially adaptive convolutional kernels,\noffering greater robustness in regions with complex\nmotion. However, they are limited by their local recep-\ntive field and thus struggle with large displacements.\nHybrid methods combine these complementary\napproaches by using flow estimation to guide the\nplacement and orientation of learned kernels, yielding\nboth global motion alignment and localized refine-\nment. This combined approach [7], [16], [30], [33], [35]–\n[38], [74], [97] typically begins by estimating optical\nflows using dedicated flow networks. Some meth-\nods adopt off-the-shelf optical flow networks [126]–\n[136] to guide the sampling location or trajectory of\nadaptive kernels. The kernels are then applied along\nflow-aligned paths to aggregate motion-aware pixel\nneighborhoods. MEMC-Net [33] exemplifies this de-\nsign by integrating PWC-Net for flow estimation and\ndeformable convolution [122] for localized refinement.In this setup, flow fields define the sampling offsets,\nwhile the kernel weights are learned to capture resid-\nual motion and restore high-frequency content. The\npredicted flows determine the sampling offsets for\neach pixel, while the learnable kernels capture resid-\nual motion and texture details. More recently, LAD-\nDER [47] introduces a lightweight encoder-decoder ar-\nchitecture that jointly estimates motion-aware features\nand spatially adaptive kernels, reducing complexity\nwhile maintaining hybrid modeling capacity.\nDespite their accuracy, hybrid approach typically\nintroduces significant computational costs due to the\ndual pipelines for flow and kernel prediction [41]. To\nalleviate this, several works [47], [97] adopt encoder-\nsharing strategies to reduce redundancy and latency.\nThese designs enhance interpolation robustness in sce-\nnarios with large displacements, motion ambiguities,\nor complex occlusion, where single approach-based\nmodels often fail. As hybrid architectures continue\nto evolve, balancing the performance and efficiency\nremains a central challenge and a promising direction.\n2.2.4 Phase-based\nAn alternative direction in VFI explores the use of\nphase information to implicitly capture motion cues.\nIn the frequency domain, pixel-wise representations\ncan be decomposed into amplitude and phase com-\nponents, where temporal phase shifts across frames\nencode the apparent motion of underlying structures.\nPhase-based methods [48], [49] exploit this prop-\nerty by estimating motion through local phase varia-\ntions, rather than relying on explicit correspondence\nor pixel displacement. To extract and manipulate\nphase information, most methods adopt multi-scale\nfrequency representations such as complex steerable\npyramids [142]–[144]. Within this framework, motion\nis modeled by interpolating both phase and amplitude\nat each pyramid level. Meyer et al . [48] solves this\noptimization problem explicitly, while later method\nPhaseNet [49] adopts end-to-end learning strategies.\nHowever, the effectiveness of these methods is\nfundamentally constrained by the assumption that\nmotion can be approximated as local phase shifts.\nWhile this holds for small or moderate motion magni-\ntudes, the assumption breaks down in the presence of\nlarge motion, leading to phase ambiguity and aliasing\nartifacts [145], [146]. As a result, phase-based meth-\nods often struggle to preserve fine spatial details or\nsharp boundaries in high-speed motion scenarios, lim-\niting their applicability in unconstrained, real-world\nsettings. Still, phase representations remain a valu-\nable signal modality and, when combined with other\nlearning-based methods, may help enhance robust-\nness against photometric and structural distortions.\n--- Page 8 ---\n8\n2.2.5 GAN-based\nA major limitation of conventional learning-based VFI\napproaches lies in their reliance on pixel-level loss\nfunctions such as ℓ1,ℓ2, or perceptual losses based\non deep features ( e.g., VGG [147]). While these ob-\njectives are effective for minimizing reconstruction\nerrors, they often produce perceptually unsatisfying\nresults, characterized by over-smoothed textures and\ndiminished realism [148], [149]. To mitigate this gap,\nseveral methods adopt Generative Adversarial Net-\nworks (GANs) [150], which demonstrate remarkable\nperformance in synthesizing visually plausible con-\ntent [151], [152]. These GAN-based VFI methods [36],\n[74], [93]–[97], [101]–[103] employ a generator Gto\nsynthesize the intermediate frame ˆIt, and a discrimi-\nnator Dto differentiate between the GT ItandˆIt. The\ngenerator is optimized using both a reconstruction loss\nand an adversarial loss, enabling it to produce frames\nthat are structurally coherent with the inputs while\nexhibiting high perceptual fidelity. Such formulations\nare particularly effective in hallucinating plausible\ncontent in disoccluded regions [153] and enhancing\nvisual details in blurry or textureless areas [74].\nDespite their potential, GAN-based models intro-\nduce new challenges. They are notoriously difficult\nto train, often suffering from instability, mode col-\nlapse [154], and poor generalization when exposed to\nmotion patterns or scene layouts not well represented\nin the training data. In such cases, the generator may\nfail to generalize, leading to artifacts or unrealistic\ninterpolations. As a result, domain adaptation or fine-\ntuning is often required when applying GAN-based\nmethods to novel environments [155], limiting their\nscalability in practical deployment.\n2.2.6 Transformer-based\nOriginally proposed for sequence modeling in natu-\nral language processing (NLP) [123], the Transformer\narchitecture has been successfully adapted to VFI [50]–\n[57], [113], [117], [124] owing to its strong capacity\nfor capturing long-range dependencies through the\nattention mechanism [123], [156]. In the context of\nVFI, where motions often span large spatial and tem-\nporal regions with occlusions and deformations, this\ncapability is particularly advantageous. The attention\nmechanism adaptively weighs features by their rele-\nvance to selectively attend to distant yet semantically\nrelevant regions. This is an essential property for syn-\nthesizing temporally coherent intermediate frames.\nThe core attention operation is defined as:\nAttn(Q, K, V ) =Softmax \nQK⊤\n√\nd!\nV, (13)\nwhere Q,K, and Vdenote the query, key, and value\nmatrices, respectively, and dis the dimensionality ofthe feature space. This formulation enables the model\nto focus on spatial-temporal regions that are infor-\nmative for interpolation, while effectively handling\nocclusions and appearance changes [37].\nTransformer-based VFI methods [50]–[57], [113],\n[117], [124] primarily differ in how they structure\nattention and encode temporal dependencies. VFI-\nFormer [52] introduces a cross-scale window-based\nattention (CSWA) mechanism to capture multi-scale\ndependencies without relying on flow-based motion\nestimation. Queries are computed from features at the\ntarget time, while keys and values are derived from\nneighboring input frames, enabling direct temporal\nassociations. The multi-scale windowing expands the\nreceptive field, enhancing robustness to complex mo-\ntion. VFIT [51] employs a hierarchical Transformer\noperates on multi-resolution features and predicts\nspatially adaptive blending kernels for fine-grained\nsynthesis. EMA-VFI [53] integrates attention modules\nwith CNNs to reduce overhead, using inter-frame at-\ntention to jointly extract motion and appearance cues\nwith improved efficiency.\nDespite their effectiveness, this approach suffers\nfrom high computational costs. The standard self-\nattention scales quadratically with the input resolu-\ntion, posing a bottleneck for HR video inputs. To\novercome this, efficient attention designs have been\nproposed. Swin Transformer [156] reduces complexity\nvia windowed self-attention and shifted windows,\nwhile Restormer [157] introduces transposed attention\nto achieve linear complexity with respect to spatial\ndimensions. These developments point to a promising\ndirection in which Transformer-based architectures\nmay effectively balance global context modeling with\ncomputational efficiency, enabling real-time HR frame\ninterpolation in practical applications.\n2.2.7 Mamba-based\nStructured State Space Models (SSMs) [98] offer a\nprincipled framework for sequence modeling through\ncontinuous-time dynamical systems. Among them,\nMamba [99] introduces a selective state-space pa-\nrameterization that combines the recurrent efficiency\nof recurrent neural networks (RNNs) [158] with the\nglobal context modeling capabilities of Transformers.\nBy leveraging linear recurrence and input-dependent\ngating, Mamba enables long-range dependency mod-\neling with linear complexity. The core of SSM-based\nmodeling is the continuous-time linear time-invariant\n(LTI) system defined as:\nh′(t) =Ah(t) +Bx(t),\ny(t) =Ch(t) +Dx(t),(14)\nwhere h(t)∈RNis the latent state, x(t)∈Ris the\ninput, and y(t)∈Ris the output. Here, the state\n--- Page 9 ---\n9\nsize is N, with system parameters A∈RN×N, B∈\nRN×1, C ∈R1×N, D ∈R. To incorporate this\nformulation into deep learning models, the system is\ntypically discretized using the zero-order hold (ZOH)\nmethod with a step size ∆. The resulting discrete\nparameters are computed as:\n¯A= exp(∆ A),\n¯B= (∆ A)−1(exp(∆ A)−I)∆B,(15)\nwhich yields the following discrete-time recurrence:\nhk=¯Ahk−1+¯Bxk,\nyk=Chk+Dxk.(16)\nwhich defines the fundamental update rule for state-\nspace sequence modeling.\nVFIMamba [100] is the first work to incorporate\nMamba into VFI. It introduces a hierarchical architec-\nture based on the S6-based Mixed-SSM Block (MSB) to\nmodel temporal dynamics across spatial resolutions.\nThis design enables bidirectional propagation of mo-\ntion features through structured recurrence, effectively\ncapturing both short- and long-range dependencies.\nCompared to Transformer-based models [50]–[57],\n[113], [117], [124], VFIMamba achieves lower memory\nconsumption and faster inference while maintaining\ncompetitive accuracy, particularly in handling large\ndisplacements and preserving high-frequency texture\ndetails. MambaFlow [140] presents a Mamba-centric\nframework for end-to-end optical flow estimation. It\nextends the modeling capacity of Mamba through two\ncore mechanisms. Self-Mamba captures long-range\nintra-frame dependencies by applying bidirectional\nstate updates to enrich spatial features with global\ncontext. Cross-Mamba, inspired by cross-attention,\nmodels inter-frame interactions to improve motion\ncorrespondence. Together, these modules collectively\nimprove robustness to occlusion, motion discontinu-\nities, and ambiguous flow regions, making the archi-\ntecture a promising backbone for VFI. Additionally,\nother SSM-based models such as MambaIR [159] and\nMambaIRv2 [160] demonstrate strong performance in\nimage restoration tasks by capturing local detail and\nglobal structure with low complexity. The success of\nthese models suggests that structured recurrence of-\nfers a compelling alternative to attention mechanisms\nfor spatiotemporal modeling in VFI.\nOverall, Mamba provides a promising design\nspace for future VFI frameworks. Its low architectural\ncomplexity and reduced sensitivity to hyperparame-\nter tuning offer practical advantages over attention-\nbased designs. However, its ability to handle long-\nterm motion dependencies, occlusion, and non-rigid\ndeformation remains underexplored. Future directions\nmay include exploring hybrid architectures that com-\nbine Mamba with local attention or design adaptive\n⋯\n!!!\"\nInputkeyframes!\"#Outputframes\n⋱⋱    ℱ\n⋯ℰ#×%\nConditionsFig. 7. General structure of DM-based VFI framework. The\nframework receives input keyframes ( I0,I1) and generates in-\ntermediate frames ( It) through a denoising process. In addi-\ntion to input keyframes, the model can accept various auxiliary\nconditioning signals such as images, text, audio, optical flow,\nor semantic maps via lightweight adapter modules or attention\nmechanisms.\nrecurrence mechanisms conditioned on motion com-\nplexity and occlusion patterns.\n2.3 Diffusion Model-based\nTraditional deep learning-based VFI methods are\npredominantly deterministic, assuming a one-to-one\nmapping between the input frames and the interpo-\nlated output [113]. While such models demonstrate\nstrong performance under moderate motion, they en-\ncounter fundamental limitations when faced with se-\nvere occlusions, significant displacements, or rapid\nappearance changes, where the underlying motion is\nambiguous. In such cases, a deterministic framework\ncannot fully capture the range of plausible transitions\nbetween two frames, often leading to results that\ndiverge from human perceptual expectations [161].\nThese limitations have prompted the exploration of\na generative approach that embraces uncertainty and\nseeks to synthesize diverse yet semantically coherent\ninterpolations.\nDMs [162]–[167] have emerged as a powerful\ngenerative model, achieving state-of-the-art (SOTA)\nperformance across image [164], [168], video [165],\n[166], and multimodal generation [169], [170]. Unlike\nGANs [150] or VAEs [171], which suffer from adver-\nsarial instability and posterior collapse respectively,\nDMs offer stable training, high-fidelity samples, and\nstrong temporal consistency. Inspired by their success\nin text-to-video (T2V)) [172]–[174] and image-to-video\n(I2V) [169], [175], researchers have recently adapted\nDMs for VFI [104]–[107], [114], [116], [117], expand-\ning the scope of VFI from deterministic interpolation\nto conditional generative modeling. This paradigm\nshift redefines VFI as a conditional denoising process,\naligning it with the broader concept of Generative\nInbetweening [111], [112], which focuses on synthe-\nsizing plausible and temporally coherent transitions\nbetween sparse keyframes under uncertainty. In this\nformulation, VFI is modeled as a denoising process\n--- Page 10 ---\n10\nconditioned on keyframes, typically denoted as I0and\nI1. Starting from Gaussian noise, a denoising network\ngradually synthesizes intermediate frames via learned\ndenoising steps. For example, Stable Video Diffusion\n(SVD) [167] adopts a latent diffusion framework. It\nfirst encodes video sequences into compact represen-\ntations via an encoder E(·), adds noise in the latent\nspace, and then denoises them using a 3D U-Net [25].\nThe denoising objective, such as the v-prediction\nloss [176], encourages accurate reconstruction from\nnoisy inputs:\nL=Ez,cimage,ϵ,th\r\rv−fθ(zt,cimage, t)\r\r2\n2i\n, (17)\nwhere v=αtϵ−σtzt,ztis the noisy latent at timestep\nt,ϵis the GT noise, and αt,σtare variance schedule\nparameters that define the weighting between signal\nand noise. cimage denotes the conditioning keyframes.\nInitial DM-based VFI models [104], [105] directly\npredict intermediate content without relying on ex-\nplicit motion estimation. Building upon this founda-\ntion, more recent methods [111], [114] extend VFI into\na semantically aware generation task, emphasizing\ncoherent scene evolution over time. To enhance tem-\nporal alignment, TRF [112] and ViBiDSampler [115]\npropose bidirectional sampling and trajectory fusion,\nfacilitating robust interpolation from both forward\nand backward perspectives without task-specific train-\ning. Beyond inference strategies, architectural inno-\nvations improve motion control and temporal con-\nsistency. EDEN [117] augments the denoising net-\nwork with a spatio-temporal encoder for global con-\nsistency, while MoG [107] integrates motion priors\nusing flow-guided warping in the latent space. As\nshown in Fig. 7, one of the key strengths of DM-\nbased frameworks lies in their inherent flexibility to\nincorporate diverse conditioning modalities beyond\ninput keyframes. DMs can seamlessly integrate aux-\niliary signals such as depth, semantic maps, audio,\ntext, or motion priors via adapter-based [108], [109]\nor attention-based conditioning pathways. This allows\nfor rich user guidance and semantic control, enabling\nuse cases like such a story-driven animation [108],\ncross-modal interpolation, and interactive video gen-\neration [161]. Framer [161] injects spatial priors into\nthe U-Net via attention mechanisms, while MoG [107]\nand FCVG [114] adopt ControlNet-like structures [177]\nto condition the generative process at multiple scales,\nimproving alignment and spatial consistency.\nOverall, DM-based VFI provides a new perspec-\ntive on VFI by decoupling interpolation from deter-\nministic regression and introducing generative mod-\neling as a robust alternative. The ability of DMs\nto integrate diverse conditioning modalities and to\nmodel uncertainty enables flexible and perceptually\nplausible frame synthesis. However, their computa-\ntional cost and sampling latency remain notable chal-\n!=1/2\n(b) Arbitrary-Time Frame Interpolation\n!=1!=0\n!=1!=1/2!=0\n!=1/4\n!=7/8⋯⋯⋯⋯⋯⋯\n(a) Center-Time Frame InterpolationFig. 8. Comparison of CTFI and ATFI. (a) CTFI only generates\na single center-frame at t=0.5given two inputs. (b) ATFI can\nsynthesize frames at arbitrary t∈(0,1).\nlenges. Future research directions may include hybrid\nframeworks that combine explicit motion estimation\nwith generative denoising, as well as curriculum- or\ncascade-based denoising strategies tailored for HR\ninputs or temporally long-range interpolation tasks.\n3 L EARNING PARADIGM\n3.1 Center-Time Frame Interpolation (CTFI)\nCenter-Time Frame Interpolation (CTFI) as shown in\nFig. 8 (a), also known as fixed-time interpolation , is\na widely adopted learning paradigm in VFI. Here,\nmodels are trained on triplets (I0, I1\n2, I1)[6], [50],\n[178]–[180], with I0andI1as inputs and I1\n2as the GT\ncenter-frame. Owing to the simplicity of supervision\nand precise GT alignment, this paradigm has been\ndominant in earlier works [16], [28], [29], [36], [50],\n[51], [59], [76], [92], [113].\nDespite its ease of implementation, CTFI suffers\nfrom major limitations in real-world scenarios where\nintermediate frames are required at arbitrary times-\ntamps. Since models are trained to generate only the\ncenter-frame at t=1\n2, they inherently lack temporal\nflexibility for generating frames at other timestamps.\nFor example, to generate a frame at t=1\n4, the model\nfirst synthesizes the center-frame, and then recursively\ngenerates ˆI1\n4conditioned on (I0,ˆI1\n2). This recursive\nstrategy is inherently sequential, introducing two key\ndrawbacks [5], [79], [181]. First, it increases computa-\ntional latency and prevents parallel generation, as each\nintermediate frame depends on the previously syn-\nthesized result. Second, it leads to cumulative errors\nwhere artifacts in earlier frames propagate through the\ninference chain, degrading temporal consistency and\noverall quality. Additionally, CTFI restricts the tempo-\nral upsampling factor to powers of two ( 2n), thereby\nlimiting adaptability in diverse frame-rate conversion\nscenarios such as real-time video streaming or arbi-\ntrary slow-motion synthesis.\n3.2 Arbitrary-Time Frame Interpolation (ATFI)\nIn contrast, Arbitrary-Time Frame Interpolation (ATFI)\normulti-frame interpolation as shown in Fig. 8 (b),\ngeneralizes the task by enabling interpolation at any\n--- Page 11 ---\n11\narbitrary t∈(0,1)between two given frames [5], [7],\n[30], [35], [45], [53], [54], [59], [62], [75], [76], [79],\n[114], [161]. This paradigm explicitly receives tas\ninput during training and inference, allowing direct\nsynthesis of frames at specified timestamps and sup-\nporting continuous-time interpolation. While some\nearlier methods [7], [73] perform iterative ATFI in\na frame-by-frame fashion, such methods often suffer\nfrom temporal jitter due to a lack of continuity mod-\neling. In contrast, temporally-aware models [62], [75]\npredict multiple intermediate frames in one pass, pro-\nmoting sequence-level coherence and computational\nefficiency.\nDespite its flexibility, ATFI also presents chal-\nlenges. Training requires HFR datasets to supervise\nintermediate frames at diverse timestamps. Addition-\nally, ATFI inherently prone to the velocity ambiguity\nproblem, where multiple plausible motion trajectories\ncan lead to the same intermediate position. This often\nleads models to average over alternatives, resulting\nin temporal blur. Furthermore, ATFI must account for\nnon-linear motion such as acceleration or abrupt di-\nrection changes, phenomena not easily handled under\nconstant-velocity assumptions. These issues are dis-\ncussed in depth in Sec. 4.4. Despite these challenges,\nATFI remains a versatile and powerful paradigm for\nreal-world applications, offering improved flexibility\nfor slow-motion generation, dynamic frame-rate adap-\ntation, and user-controllable playback.\n3.3 Training Strategy\n3.3.1 CTFI Training Strategy (CTFI-TS)\nCTFI-TS builds training triplets (I0, It, I1)with Itpo-\nsitioned precisely at the center-point between I0and\nI1. These triplets can be generated by sampling three\nconsecutive frames which are uniformly sampled as\nshown in Fig. 9 (a). This enables the construction of\nlarge-scale training datasets without dense manual\nannotation. During training, models are supervised\nexclusively at t=0.5, and no explicit temporal encod-\ning is involved. At inference, the model is similarly\nevaluated by predicting center-frames at each stride.\nWhile efficient, this strategy inherently limits gener-\nalization to other timestamps and requires recursive\nprocessing for arbitrary-time synthesis.\n3.3.2 ATFI Training Strategy (ATFI-TS)\nATFI-TS constructs training samples from (n+1) con-\nsecutive frames, using the first and last as inputs\n(I0, I1)and the (n−1)intermediate frames as supervi-\nsion targets for their respective times t∈(0,1). Each t\nis either provided directly or encoded via temporal\nembeddings [54], [81], [106]. When HFR videos are\navailable, training data can be flexibly constructed by\nuniformly sub-sampling frames at a desired interval as\n(a) Center-Time Frame Interpolationframe0frame0frame0\nframe0frame0frame0!!!\"!#$!.&frame0\nframe0frame0frame0frame0frame0!!!\"!#$!.&\nframe0frame0\nframe0frame0frame0frame0!!!\"!#$!.&(b) Arbitrary-Time Frame Interpolationframe0frame0\nframe0frame0frame0frame0!!!\"!#$\"'frame0\nframe0frame0frame0frame0frame0!!!\"!#$!.&\nframe0frame0frame0\nframe0frame0frame0!!!\"!=0.5only !Any !∈(0,1) possible !!#$'(Fig. 9. Comparison of CTFI-TS and ATFI-TS. (a) CTFI-TS sam-\nples exactly three uniformly spaced frames per training example,\nwith only the center-frame used as supervision. (b) ATFI-TS uses\n(n+1) uniformly spaced frames from HFR videos, allowing an\nintermediate frame at arbitrary timestamp t∈(0,1)to serve as\nsupervision target.\nshown in Fig. 9 (b). As long as the original frame rate\nof the video is divisible by the desired interpolation\nfactor, any pair of frames can be selected as inputs, and\nthe frames that lie temporally between them can serve\nas GT supervision targets. This strategy allows models\nto learn from a wide distribution of motions and time\nintervals. Inference is fully parallelizable, frames at\nanyt∈(0,1)can be generated independently, making\nthis approach highly efficient and scalable for real-\ntime and high-frame-rate applications. By explicitly\nmodeling time and enabling continuous supervision,\nATFI-TS forms the backbone of modern interpolation\nframeworks seeking generalizability, temporal coher-\nence, and fine-grained control.\n3.4 Loss Functions\nLoss functions play a critical role in guiding VFI mod-\nels toward producing temporally coherent and percep-\ntually realistic outputs. They are broadly categorized\ninto reconstruction, perceptual, adversarial, and flow-\nbased losses, each addressing different aspects of the\ninterpolation objective.\n3.4.1 Reconstruction Loss\nReconstruction losses supervise the model to mini-\nmize the pixel-wise discrepancy between the predicted\nintermediate frame ˆItand the GT frame IGT\nt. These\nlosses are typically applied in the RGB space.\nL1Loss is defined as:\nL1=\r\r\rˆIt−IGT\nt\r\r\r\n1, (18)\n--- Page 12 ---\n12\nwhich computes the pixel-wise absolute difference\nbetween frames.\nL2loss is defined as:\nL2=\r\r\rˆIt−IGT\nt\r\r\r2\n2, (19)\nthis loss computes the squared error, yielding\nsmoother gradients but often producing overly\nsmoothed outputs, particularly in high-frequency re-\ngions or under motion-induced misalignments [106].\nCharbonnier Loss [182] is a differentiable variant of\ntheL1loss:\nLchar=ρ(IGT\nt−ˆIt), (20)\nwhere ρ(x) = (x2+ϵ2)αis the Charbonnier function,\nwith a small constant ϵ(typically 10−3) for numerical\nstability and α= 0.5. The loss provides smoother\ngradients than the L1loss. Owing to its smooth gra-\ndient profile and outlier resilience, Charbonnier loss is\nfrequently adopted in VFI for its balanced sensitivity\nto both sharp detail and robust training stability.\nLaplacian loss [183] compares the Laplacian pyramid\ndecompositions of the interpolated and GT frames\nto supervise frame synthesis across multiple spatial\nscales:\nLlap=lX\ni=12i−1\r\r\rLi(ˆIt)−Li((IGT\nt)\r\r\r\n1, (21)\nwhere Li(·)is the i-th pyramid level. This encourages\nalignment of both global structure and fine detail, and\nis often used in conjunction with L1loss.\nCensus loss [184], also referred to as ternary loss, eval-\nuates the structural consistency of local image patches\nunder census transformation [185]. It is defined as:\nLcen=ψ(IGT\nt,ˆIt), (22)\nwhere ψ(·,·)is a Hamming-like distance function over\ncensus-encoded patches. Due to its robustness against\nillumination and photometric noise, census loss im-\nproves particularly effective in unsupervised or self-\nsupervised VFI frameworks.\n3.4.2 Perceptual Loss\nTo enhance perceptual realism, VFI models often\nincorporate high-level perceptual losses in addition\nto pixel-wise criteria. A widely adopted formulation\ncomputes feature-level distances using a pre-trained\nVGG network [23]:\nLper=\r\r\rϕ(ˆIt)−ϕ(IGT\nt)\r\r\r2\n2, (23)\nwhere ϕdenotes the feature extractor. This loss pro-\nmotes structural consistency and encourages synthesis\nof semantically aligned textures, especially in chal-\nlenging visual regions.3.4.3 Adversarial Loss\nTo further improve realism, adversarial learning\nframeworks employ a discriminator Dtrained to dis-\ntinguish interpolated frames from real ones. The stan-\ndard GAN objective is:\nLGAN=EIGT\nt[logD(IGT\nt)]+EˆIt[log(1−D(ˆIt))].(24)\nBy optimizing this objective jointly with reconstruc-\ntion losses, the generator learns to produce sharper\nand more plausible frames. To enforce temporal con-\nsistency, recent works also adopt temporal discrim-\ninators, which operate on sequences to distinguish\ncoherent dynamics [62], [74].\n3.4.4 Flow Loss\nGiven that many VFI models rely on motion estima-\ntion as an intermediate step, flow supervision becomes\ncritical for improving temporal alignment. Several loss\nterms are used to regularize or supervise flow predic-\ntion.\nSmoothness Loss [16] encourages piecewise smooth\nflow by penalizing abrupt spatial changes:\nLsmooth =∥∇V 0→1∥1+∥∇V 1→0∥1. (25)\nWarping Loss [5] measures the reconstruction error\nafter warping one frame to the other using estimated\nflow:\nLwarp =∥I0− W(I1,V)∥1+∥I1− W(I0,V)∥1,(26)\nwhere Wdenotes the warping operator.\nFirst-order Edge-aware Smoothness Loss [79] is de-\nsigned to preserve sharp motion discontinuities, this\nloss attenuates regularization near edges:\nLedge=X\ni=0,1exp \n−e2X\nc|∇xI0\ntc|!⊤\n· |∇xV0\nti|,(27)\nwhere edge strengths are computed via image gradi-\nents and used to modulate the smoothness penalty.\n4 VFI C HALLENGES\nDespite extensive progress in VFI, several representa-\ntive challenges consistently remain difficult across ap-\nproaches, limiting real-world performance. As shown\nin Fig. 10, these include large motion [58], [60], oc-\nclusion [87], lighting variation, and non-linear mo-\ntion [90], [137].\n--- Page 13 ---\n13\n(a)Large motion\n𝐼!\n𝐼\"𝐼!𝐼\"\n𝐼\"𝐼!𝐼!𝐼\"(b) Occlusion\n(c)Lighting variation(d)Non-linear motion\nFig. 10. Representative challenges in VFI. (a) Large motion makes it difficult to establish accurate correspondences across frames,\nespecially in cases involving fast-moving objects, deformable structures, or significant camera motion. (b) Occlusion introduces\nambiguity, as some regions in the intermediate frame are not visible in either of the input frames, making it unclear what content\nshould be synthesized. (c) Lighting variations, such as shadows, reflections, or changes in illumination, violate brightness constancy\nassumptions and hinder accurate motion estimation. (d) Non-linear motion refers to changes in motion speed or direction over time,\nmaking it difficult to infer intermediate positions.\n4.1 Large Motion\nLarge motion refers to scenarios where objects un-\ndergo substantial displacement between consecutive\nframes. As shown in Fig. 10 (a), this includes articu-\nlated movements ( e.g., a person leaning left to right)\nor abrupt camera motion, which result in wide spatial\nshifts across the image plane. Such motion is preva-\nlent in real-world videos and presents a fundamental\nchallenge in VFI due to the difficulty of establishing\naccurate correspondences over long spatial ranges.\nTo accurately synthesize an intermediate frame, the\nmodel must identify where each pixel from the first\nframe (I0)has moved in the following frame (I1)\nwhich denotes motion or correspondence estimation.\nWhen the motion is small, this is relatively straight-\nforward because corresponding pixels remain close.\nHowever, large motion induces long-range dependen-\ncies that exceed the receptive field of standard net-\nworks. Moreover, appearance changes and occlusions\nfurther hinder accurate estimation by introducing dis-\ncontinuities in motion and visibility. To address this,\nmany VFI models adopt a coarse-to-fine hierarchical\nframework, where large displacements are estimated\nat low-resolution (LR) feature maps and progressively\nrefined at higher resolutions. RIFE [59] employs multi-\nscale residual flow refinement, enabling robust align-\nment under wide motion ranges. FILM [83] leverages\na feature pyramid for flow estimation and lightweight\nsynthesis, explicitly targeting fast motion and blur\nscenarios. Similarly, IFRNet [81] improves motion en-\ncoding through a motion-aware feature extractor and\nan intermediate flow refinement block. In addition to\nthese designs, some models further enhance alignment\nunder large displacements by leveraging bidirectional\nmotion modeling [5], [56]–[58] or attention mecha-\nnisms [55], [97], [161]. ABME [58] proposes asymmet-ric bilateral estimation, predicting forward and back-\nward flows independently to improve robustness un-\nder occlusion. BiFormer [55] incorporates deformable\nattention across bidirectional contexts, enabling the\nmodel to dynamically attend to semantically relevant\nbut spatially distant regions, an effective strategy for\ncapturing non-local motion patterns.\nDespite architectural differences, these models all\nshare a common objective of expanding the receptive\nfield effectively while maintaining spatial precision. To\nthis end, many methods combine multi-scale refine-\nment, attention-based global matching, and motion-\naware modules, enabling them to handle wide-range\nmotion more effectively. Such designs have demon-\nstrated strong performance on benchmarks involving\nextreme motion, most notably X4K1000FPS [79], which\nprovides 4K videos at 1000fps along with dense GT for\nfine-grained evaluation. Following the introduction of\nX4K1000FPS, several HR datasets [186], [187] have\nbeen proposed to further benchmark performance\nunder high-speed and large-displacement conditions.\nBy providing more realistic and challenging settings,\nthese datasets have enabled better training and eval-\nuation of VFI models in unconstrained environments.\nAs a result, the availability of such benchmarks has\naccelerated the development of more robust architec-\ntures capable of preserving motion detail and fidelity\nunder large displacements.\n4.2 Occlusion\nAchieving high-quality (HQ) interpolation demands\naccurate motion estimation as well as a proper un-\nderstanding of occlusions. Otherwise, severe artifacts\nare likely to appear in the interpolated frames, par-\nticularly near motion boundaries. For two consecutive\ninput frames, certain pixels in the intermediate frame\n--- Page 14 ---\n14\nmay not correspond to any observable region in either\ninput, creating ambiguity in determining the correct\ncontent for these occluded regions [188]. As shown in\nFig. 10 (b), such occlusions can occur when previously\nhidden objects become visible or when objects move\ntoward the camera, revealing regions that were not\nseen in either input. Naively blending warped inputs\noften results in severe artifacts, most notably ghosting\nartifacts [87], where an object is not only incorrectly\nprojected from its previous location but also appears\nas a duplicate at its correct position due to the lack\nof sufficient visual cues. This is especially problematic\nin disoccluded regions, areas newly revealed in the\nintermediate frame but absent in both inputs, such as\nwhen an object emerges from behind another or moves\ndirectly toward the viewpoint. In these cases, the ab-\nsence of visual evidence introduces ambiguity, making\nit unclear what content should be synthesized. To\nresolve this, modern VFI methods incorporate explicit\nocclusion reasoning to guide the synthesis process.\nA common approach involves estimating soft oc-\nclusion masks that weight the pixel contributions\nfrom each frame [5]–[7], [33], [74]. SuperSloMo [5]\njointly predicts bidirectional flow and occlusion masks\nto exclude unreliable pixels during frame blending.\nSoftSplat [60] improves upon this by introducing\na differentiable softmax visibility map that enables\nconfidence-weighted forward warping. OCAI [87]\nfurther incorporates forward-backward consistency\nchecks [184] to identify unreliable flow regions and ap-\nplies targeted masking and flow inpainting to recover\nmissing structures. In addition to visibility maps, aux-\niliary cues such as context and depth also improve\nocclusion handling. CtxSyn [30] integrates warped\ncontext features alongside frames to guide synthesis\nwith spatial awareness. DAIN [7] estimates occlusion\nareas using depth information and leverages neigh-\nboring contextual cues to fill the missing regions.\nOverall, occlusion-aware VFI remains a criti-\ncal challenge, particularly in dynamic scenes with\ndepth discontinuities or disoccluded motion. As such,\nSOTA models increasingly combine multiple strate-\ngies, such as masking, depth priors, feature similarity,\nor forward-backward consistency [87], [184] to recover\nplausible content in ambiguous regions and maintain\ntemporal coherence in the output.\n4.3 Lighting Variation\nLighting variation refers to temporal changes in il-\nlumination, shadows, reflections, or exposure across\nconsecutive frames as shown in Fig. 10 (c). These\nvariations can significantly degrade the quality of\ninterpolation, as they violate the basic assumption\nof brightness constancy [27], [189], which is widely\nadopted in many optical flow and motion estimationmethods. This assumption presumes that the inten-\nsity of a surface patch remains constant across time\nas it moves, allowing pixel-wise correspondences to\nbe inferred from photometric similarity. However, in\npractice, lighting changes can cause the same object to\nappear drastically different between frames, resulting\nin erroneous motion estimation and visually inconsis-\ntent interpolations.\nTo mitigate this, alternative representations have\nbeen proposed. Phase-based methods [48], [49] oper-\nate in the frequency domain, where motion is encoded\nas phase shifts rather than intensity differences. These\nmodels leverage phase information that remains stable\nunder lighting fluctuations, yielding temporally coher-\nent interpolations even in the presence of flickering or\nexposure variation. More recently, Transformer-based\narchitectures have shown robustness to photometric\ninconsistencies. TTVFI [124] aligns motion features\nacross temporal trajectories using attention, enabling\nthe model to blend semantically aligned tokens rather\nthan relying on raw pixel intensities. This higher-\nlevel representation effectively helps suppress errors\ninduced from inconsistent lighting, producing percep-\ntually coherent results.\nAlthough lighting variation has received less at-\ntention than large motion or occlusion problem, ex-\nisting methods suggest that photometric-invariant\nfeatures, frequency-domain modeling, and attention-\nbased alignment provide viable solutions. Continued\nexploration of these strategies could further enhance\nthe robustness of VFI models in unconstrained envi-\nronments.\n4.4 Non-linear Motion\nMany early VFI methods [5]–[7], [16], [30], [33], [35],\n[60], [73], [74], [76], [85] assume linear or uniform\nmotion between input frames. Under this assumption,\nobjects move along straight trajectories at constant\nvelocity, allowing motion estimation based on simple\ntemporal interpolation. Flow-based [5], [6], [16], [33],\nkernel-based [33], and even phase-based models [49]\noften rely on this assumption implicitly. However,\nin real-world scenarios, motion is frequently non-\nlinear due to acceleration, deceleration, or directional\nchange. As shown in Fig. 10 (d), a sliding ball acceler-\nates along a curved path, violating the linear motion\nprior and introducing significant estimation error.\nTo address these limitations, researchers have pro-\nposed higher-order motion modeling that extends be-\nyond linear assumptions. Since most existing methods\noperate on only two input frames, they are inherently\nunder-constrained and forced to assume simple mo-\ntion. To overcome this, several methods incorporate\nmultiple input frames (typically four) to capture richer\ntemporal variations and better approximate non-linear\n--- Page 15 ---\n15\nmotion. QVI [76] introduces a quadratic motion model\nthat fits second-order trajectories over four consecu-\ntive input frames. Specifically, it takes (I−1, I0, I1, I2)\nas inputs and predicts an intermediate frame Itfor ar-\nbitrary t∈(0,1). By modeling both velocity and accel-\neration from surrounding frames, QVI enables the net-\nwork to better handle curved or time-varying motion\npaths. This parametric formulation allows the model\nto explicitly account for motion curvature. EQVI [77]\nfurther refines by combining offset-based warping\nwith temporal embeddings, improving precision and\nrobustness under complex motions. More recently, IQ-\nVFI [86] introduces an implicit motion representation\nusing a coordinate-based MLP that adapts to arbitrary\nmotion patterns without requiring predefined trajec-\ntory assumptions. These works collectively empha-\nsizes the importance of modeling non-linear motion\ndirectly, especially in multi-frame settings. However,\nthese simple mathematical models cannot completely\ncapture the complexities and irregularities of real-\nworld motions.\nAs the field progresses, a new challenge velocity\nambiguity [90] appears. When only two frames are\navailable, multiple plausible motion trajectories can\nexplain the observed displacement, making the un-\nderlying motion inherently under-constrained. This\nambiguity becomes especially pronounced in scenes\ninvolving curved motion or directional switches, such\nas bouncing balls or rotating limbs. To tackle this,\nZhong et al. [90] introduces a velocity embedding\nmodule that learns to disambiguate temporal dynam-\nics by jointly reasoning over motion direction and tem-\nporal consistency. It separates appearance modeling\nfrom motion estimation, which enhances robustness in\ncomplex scenes. BiM-VFI [137] takes a complementary\nperspective by designing an explicit bidirectional mo-\ntion descriptor. Its Bidirectional Motion Fields (BiM)\nencode angular and magnitude differences relative to\nthe intermediate time, enabling accurate modeling of\ncurved, asymmetric, or velocity-changing trajectories.\nBiM-VFI further integrates these representations into a\nBiM-guided flow estimator and motion-aware refine-\nment network, yielding temporally coherent results\nin non-linear regimes. These recent advances signal\na shift from rigid linear motion priors toward flex-\nible, context-aware motion modeling. By extending\ntemporal supervision and refining motion representa-\ntions, either through quadratic formulations, implicit\nembeddings, or directional velocity fields, modern\nVFI methods now offer significantly improved perfor-\nmance in complex motion scenarios that were previ-\nously underexplored.5 D ATASETS AND EVALUATION\n5.1 Datasets\nTo facilitate training and evaluation across vary-\ning temporal resolutions and motion complexities,\nnumerous VFI datasets have been developed. Ta-\nble 1 provides a high-level summary of commonly\nused datasets categorized into triplet and multi-frame\ntypes. We describe each dataset in detail below.\n5.1.1 Triplet Datasets\nEarly learning-based VFI approaches primarily rely on\ntriplet datasets , where two input frames are used to pre-\ndict the temporally centered GT frame. This configura-\ntion aligns with CTFI settings (Sec. 3.1). Some datasets\nare further extended to seven-frame sequences [6] for\nevaluating frame-rate upsampling.\n•Middlebury [189]: Originally designed for op-\ntical flow, Middlebury contains short video\nclips with moderate complexity. Its small size\nlimits scalability, but it remains a standard\nbenchmark for consistency evaluation.\n•UCF101 [16], [178]: A human action dataset\nfrom which a small subset of triplets is used\nfor VFI. Due to its LR and simple motion, it is\nmainly used for training or sanity checks.\n•Vimeo90K [6]: A widely adopted benchmark\nwith diverse scenes and consistent format. It\noffers clean supervision and balanced motion\ncomplexity, making it ideal for comparative\nanalysis.\n•SNU-FILM [50]: Constructed from high-speed\nfootage and categorized by motion difficulty,\nSNU-FILM enables evaluation across varying\nlevels of motion, occlusion, and blur.\n•ATD-12K [180]: A large-scale animation\ndataset with rich stylistic diversity. Its varia-\ntion in artistic textures and motion patterns\nsupports both general-purpose and domain-\nspecific evaluation.\n5.1.2 Multi-frame Datasets\nMulti-frame datasets enable dense temporal supervi-\nsion and are commonly used in both CTFI and ATFI\n(Sec. 3.2) settings. They support flexible frame sam-\npling and facilitate evaluation under diverse temporal\nintervals.\n•Xiph [60], [195]: A curated set of 4K video\nsequences designed for assessing interpolation\nfidelity in subtle motion settings.\n•KITTI [190]: Captured in autonomous driving\nscenarios, KITTI poses unique challenges with\nsparse GT and large ego-motion.\n•Sintel [192]: A synthetic dataset rendered from\ntheSintel film, offering photorealistic motion\nand structured flow annotations.\n--- Page 16 ---\n16\nTABLE 1\nSummary and comparison of popular datasets for VFI.\nThe dataset types Trepresents Triplet dataset, Mrepresents Multi-frame dataset.\nDataset Venue Type Resolution Split#Videos /\n#TripletsURL\nMiddlebury[189]IJCV’11 T ≤640×480 (VGA)train -\ntest 12\nUCF101[178]CRCV’12 T 256×256train -\ntest 379\nVimeo90K[6]IJCV’19 T 448×256train 51,312\ntest 3,782\nSNU-FILM[50]AAAI’20 T ≤1280×720 (HD)train -test 1,240\nATD-12K[180]CVPR’21 T 1280×720,1920×1080 (FHD)train 10,000\ntest 2,000\nXiph[60]- M 2048×1080 (2K),4096×2160 (4K)train -\ntest 8\nKITTI[190]CVPR’12 M 1240×376train 194\ntest 195\nDAVIS[191]CVPR’16 M 1920×1080train 30\ntest 20\nHD[33]TPAMI’19 M 960×544,1280×720,1920×1080train -\ntest 11\nSintel[192]ECCV’12 M 1024×436train 23\ntest 12\nAdobe240[179]CVPR’17 M 1280×720train 61\ntest 10\nGOPRO[193]CVPR’17 M 1280×720train 22\ntest 11\nX4K1000FPS[79]ICCV’21 M 4096×2160train 4,408\ntest 15\nWebVid-10M[194]ICCV’21 M variedtrain 10M\ntest -\nLAVIB[187]NeurIPS’24 M 4096×2160train 188,644\ntest 53,494\nOpenVid[186]ICLR’25 M ≥512×512,1920×1080train 1M\ntest -\n•DAVIS [191]: Originally for segmentation,\nDAVIS features complex object motion, occlu-\nsion, and deformation, offering rich dynamics\nfor interpolation.\n•Adobe240 [179]: Collected at 240fps, this\ndataset captures real-world motion blur and\nlighting changes, ideal for fine-grained tempo-\nral modeling.\n•GOPRO [193]: Featuring high-frame-rate\nrecordings with handheld cameras, GOPRO\nprovides realistic non-linear motion and\ndefocus blur.\n•HD [33]: A subset of HR content from Xiph,\nwith sharper motion content suited for realistic\nevaluation.\n•X4K1000FPS [79]: A premier benchmark forultra-slow motion and long-range interpola-\ntion, thanks to its dense 1000fps and 4K capture\nsettings.\n•WebVid-10M [194]: A large-scale web video\ncorpus originally built for text-video tasks. Its\nsize and diversity support generative VFI when\nproperly filtered.\n•LAVIB [187]: Designed for large-scale, diverse-\ndomain evaluation with balanced splits and\ncurated subsets for out-of-distribution testing.\n•OpenVid [186]: A text-video dataset support-\ning multi-modal VFI and DM-based interpola-\ntion research via dense, aligned samples.\n--- Page 17 ---\n17\n5.2 Data Augmentation\nModern VFI models incorporate spatial and tem-\nporal data augmentation to improve generalization\nand prevent overfitting. A widely adopted strategy is\npatch-based cropping, where fixed-size patches ( e.g.,\n128×128 or256×256) are randomly extracted\nfrom HR inputs [29], [54], [62], [81]. This not only\nreduces memory and computational costs but also\nencourages localized motion learning while mitigating\nspatial overfitting to scene layout or object positioning.\nFurthermore, random cropping prevents the model\nfrom overfitting to spatial priors such as background\nlayout or object location, thereby improving robust-\nness across spatial contexts [29]. Additional spatial\naugmentations, such as horizontal/vertical flipping\nand random rotation, enhance appearance diversity\nand promote invariance to orientation and perspec-\ntive changes. These augmentations enable the model\nto remain invariant to directional biases and better\ngeneralize to unseen spatial transformations.\nTemporal augmentation is equally critical in se-\nquential modeling. Frame order reversal [54], [81] is\ncommonly applied, wherein sequences like (I0, I1, I2)\nare reversed to (I2, I1, I0). In CTFI, this augmenta-\ntion preserves the center-frame I1while exposing the\nmodel to symmetric motion trajectories [5], [50]. Sim-\nilarly, in ATFI settings, reversing sequences ensures\ntemporal consistency under bidirectional motion. For\nexample as shown in Fig. 9 (b), consider an input\ntriplet (I0, I1\n3, I1)used to supervise interpolation at\nt=1\n3. By reversing the sequence to (I1, I1\n3, I0), the\nrelative time becomes (1−1\n3)=2\n3. This simple yet ef-\nfective strategy enables the model to learn tempo-\nrally symmetric representations, thereby improving\ngeneralization across motion directions and enhancing\nrobustness in bidirectional synthesis.\nOverall, these augmentation act as effective reg-\nularizers, enabling VFI models to generalize across\ndiverse motion scales, temporal patterns, and visual\nvariations. Integrating these schemes has become a\nfoundational component of both CTFI and ATFI train-\ning pipelines.\n5.3 Evaluation Metrics\nTo facilitate comprehensive assessment of VFI models,\nvarious metrics have been proposed to capture dif-\nferent aspects of visual quality and temporal coher-\nence. Table 2 summarizes commonly used evaluation\nmetrics categorized into image-level, perceptual, and\nvideo-level types.\n5.3.1 Image-level Metrics\nImage-level metrics assess the quality of individual\ninterpolated frames with respect to GT references.TABLE 2\nSummary of evaluation metrics for VFI.\nArrows (/) indicate whether higher or lower values\ncorrespond to better interpolation quality. A checkmark ( /)\nindicates that the metric requires GT frames. Colored rows\ndenote perceptual metrics.\nCategory MetricInterpolation\nQualityReference\nFrame\n/ctre\nImage-level\nMetricsPSNR /\nSSIM [196] /\nIE [189] /\nNIQE [197] \nFID [198] /\nLPIPS [199] /\nFloLPIPS [200] /\nSTLPIPS [201] /\nDISTS [202] \nVideo-level\nMetricsVSFA [203] \ntOF [204] /\nFVD [205] /\nFVMD [206] /\nVBench [207] \nThese pixel-centric evaluations focus on spatial ac-\ncuracy without considering temporal dependencies\nacross video sequences.\nPeak Signal-to-Noise Ratio (PSNR) quantifies recon-\nstruction fidelity based on the mean squared error\n(MSE) between interpolated frame and GT frame.\nWhile higher PSNR reflects better numerical simi-\nlarity, it often fails to align with human perception,\nespecially for high-frequency or perceptually salient\nregions.\nStructural Similarity Index (SSIM) [196] evaluates\nlocal structural integrity by comparing luminance,\ncontrast, and texture patterns. SSIM values range in\n[−1,1], with higher values indicating stronger struc-\ntural alignment. Though more perceptually aligned\nthan PSNR, SSIM may still overrate visually implausi-\nble outputs if global structure is preserved.\nInterpolation Error (IE) [189] computes the root-\nmean-square error (RMSE) between interpolated\nframe and the GT frame. Despite being intuitive, IE\nshares limitations with PSNR in terms of perceptual\nrelevance.\n5.3.2 Perceptual Metrics\nPerceptual metrics aim to assess the semantic plausi-\nbility, texture fidelity, and structural realism of interpo-\nlated frames, often aligning better with human visual\npreferences.\nNatural Image Quality Evaluator (NIQE) [197] is a\nno-reference score derived from deviations to natural\nimage statistics. Lower values reflect more natural, HQ\nframes.\nFr´ echet Inception Distance (FID) [198] measures the\nFr´echet distance between the feature distributions of\n--- Page 18 ---\n18\ngenerated frames and GT frames using a pre-trained\nInception network [208]. Lower FID indicates better\nsemantic alignment.\nLearned Perceptual Image Patch Similarity\n(LPIPS) [199] measures perceptual similarity using\ndeep features from pretrained networks. It is robust\nto minor misalignment and sensitive to semantic\ndifferences. Lower LPIPS signifies better perceptual\nquality.\nFloLPIPS [200] extends LPIPS by applying motion-\naware weighting based on optical flow. It emphasizes\nvisual fidelity in regions undergoing large displace-\nment.\nSTLPIPS [201] improves LPIPS by incorporating shift-\ntolerant feature matching, enhancing robustness to\nslight misalignments.\nDISTS (Deep Image Structure and Texture Similar-\nity) [202] separately evaluates texture and structure\nsimilarity using deep features. It balances local detail\nand global consistency.\n5.3.3 Video-level Metrics\nThese metrics assess spatiotemporal coherence over\nvideo sequences, which is essential for realistic and\ntemporally consistent interpolation.\nVSFA [203] is a no-reference model trained on human\nlabels. It estimates perceptual quality by aggregating\ndeep features with a recurrent network. Lower scores\nsuggest better perceived video quality.\ntOF [204] computes temporal optical flow consistency\nacross frames. Lower tOF values indicate smoother\nmotion continuity.\nFr´ echet Video Distance (FVD) [205] measures the\nFr´echet distance between distributions of deep fea-\ntures extracted from real and generated videos using\na pre-trained Inflated 3D ConvNet (I3D) [209]. Lower\nFVD values reflect stronger temporal and perceptual\nrealism.\nFr´ echet Video Motion Distance (FVMD) [206] im-\nproves upon FVD by disentangling motion and ap-\npearance, focusing more explicitly on dynamic consis-\ntency.\nVBench [207] is a multi-dimensional benchmark that\nscores video models across motion fidelity, coherence,\nand realism. It enables large-scale reference-free eval-\nuation using semantic video representations.\n6 A PPLICATIONS\n6.1 Event-based VFI\nEvent-based Video Frame Interpolation (EVFI) [14],\n[210]–[223] aims to improve interpolation accuracy\nby leveraging the unique advantages of event cam-\neras. Unlike conventional frame-based cameras that\ncapture full images at fixed intervals, event cam-\neras [224], which are bio-inspired vision sensors [225],asynchronously record per-pixel brightness changes,\nreferred to as “ events ”, triggered when a contrast\nthreshold is exceeded. These sensors offer key benefits\nsuch as ultra-high temporal resolution, high dynamic\nrange, and low latency, making them ideal for sce-\nnarios involving rapid motion or challenging lighting.\nConsequently, event cameras have gained traction in\nVFI research, especially where traditional RGB frames\nsuffer from motion blur or low temporal fidelity [212],\n[216], [218].\nOne of the early models, TimeLens [212], estimates\noptical flow directly from event streams and synthe-\nsizes intermediate frames accordingly. Later models\nsuch as TimeReplayer [216] and EGVD [223] improve\nperformance by jointly estimating motion and appear-\nance. TimeLens-XL [221] enhances any-time interpola-\ntion capability by optimizing flow and frame synthesis\niteratively. Despite these advances, EVFI models re-\nmain sensitive to synthesis errors, as inaccuracies can\naccumulate over time, leading to temporal artifacts.\nDespite their strengths, EVFI models face practi-\ncal challenges. Capturing real event streams requires\nspecialized neuromorphic sensors, which are often ex-\npensive and less accessible than conventional cameras.\nMoreover, collecting large-scale event datasets with\ndense GT labels is especially challenging due to the\nasynchronous nature of event recordings. As a result,\nseveral studies [226]–[229] exploit event simulation\nfrom standard camera, simulating the event stream\nfrom continuous images or video sequences. Kaiser\net al. [226] simulates positive or negative events by\nthresholding the intensity change between consecutive\nframes. Pix2NVS [227] estimates per-pixel luminance\nfrom video to synthesize event-like representations,\naligned to frame intervals.\n6.2 Cartoon VFI\nProducing traditional 2D animation is labor-\nintensive [230], requiring artists to manually draw\nmultiple in-between frames. VFI offers a means of\nautomating this process by generating plausible\nintermediate frames, thereby reducing production\ntime and cost [230], [231].\nHowever, cartoon videos exhibit distinct charac-\nteristics compared to real-domain videos: they fea-\nture exaggerated motion, minimal texture, flat color\nregions, and sharp contours, which pose challenges\nto correspondence-based methods. To address this,\ndomain-specific models have been proposed [109],\n[180], [232]–[235]. Notably, ToonCrafter [109] adopts\na generative framework rather than relying on explicit\nmotion estimation. Recent efforts aim to build models\nthat generalize across both cartoon and real domains\nby leveraging diverse training data or domain adapta-\ntion techniques [114], [116], [161].\n--- Page 19 ---\n19\nA major bottleneck in cartoon VFI research is the\nabsence of standardized, HQ datasets. While ATD-\n12K [180] provides a useful benchmark, its triplet-\nonly format restricts its utility in ATFI settings. As\na result, future progress will depend on the release\nof open, multi-frame cartoon datasets that enable fair\nand reproducible evaluation.\n6.3 Medical Image VFI\nVFI is also increasingly applied in medical imag-\ning to reconstruct temporally dense 4D sequences\nfrom sparsely acquired volumetric scans [236]–[238].\nModalities like CT and MRI face acquisition con-\nstraints due to radiation exposure and long scanning\ntimes [237], leading to coarse temporal sampling.\nVFI offers a means to generate intermediate volumes\nthat enhance temporal resolution without incurring\nadditional scan overhead. Medical VFI models must\naccount for subtle anatomical motions and preserve\nfine structural detail critical for clinical interpreta-\ntion. CPT-Interp [238] uses continuous motion field\nmodeling, while DU4D [237] proposes an unsuper-\nvised interpolation framework that does not rely on\nGT annotations. These approaches enhance applica-\nbility in settings where labeled 4D medical datasets\nare scarce. Nonetheless, challenges remain. Ensuring\nclinical validity, minimizing hallucinated content, and\nestablishing domain-specific evaluation metrics are\nongoing concerns. Future research will likely explore\nphysiology-aware modeling, uncertainty quantifica-\ntion, and benchmark design specific to 4D medical\nimaging tasks.\n6.4 Joint Task\nRecent studies have explored jointly performing VFI\nwith other LLV tasks such as super-resolution (SR) [8],\n[239]–[243] and deblurring [181], [244]–[248]. Such\njoint formulations exploit the inherent correlation\nbetween spatial and temporal cues in video se-\nquences [240]. For instance, space-time video super-\nresolution (STVSR) jointly upsamples resolution and\nframe rate by leveraging spatial details to enhance\nmotion estimation and vice versa [240]. Shared rep-\nresentations enable efficient feature reuse, reduce re-\ndundancy, and facilitate joint optimization. Models\nsuch as FISR [240] and MOTIF [243] exemplify this\nintegrated approach. Joint deblurring and interpola-\ntion addresses scenarios involving both motion blur\nand low frame rates. Instead of applying deblurring\nfollowed by VFI in a cascade, end-to-end models [139],\n[181], [244]–[248] simultaneously estimate clean and\ninterpolated frames, resulting in improved temporal\nconsistency and visual clarity. These multitask designs\nimprove robustness and efficiency, particularly under\nreal-world degradation, and suggest promising direc-\ntions for unified LLV modeling.7 F UTURE RESEARCH DIRECTIONS\n7.1 Video Streaming Service\nThe widespread adoption of real-time video services,\nincluding video conferencing and adaptive streaming,\npresents a growing need for bandwidth-aware video\ndelivery under constrained networks [249]. VFI of-\nfers a promising solution by enabling keyframe-only\ntransmission while synthesizing intermediate frames\non the client side, thus maintaining visual fluidity at\nlower bitrates. While early methods confirm its poten-\ntial for rate reduction, practical deployment remains\nscarce due to model complexity, inference latency,\nand platform limitations. Future directions include the\ndevelopment of ultra-lightweight architectures that\ncan operate on mobile or edge devices with limited\ncompute resources. Moreover, adaptive interpolation\nstrategies that jointly consider network bandwidth,\nscene motion complexity, and perceptual saliency are\nneeded. Learning-based rate control, where the in-\nterpolation fidelity is dynamically modulated, could\nenable bitrate–quality trade-offs tuned in real-time.\nJoint optimization pipelines that integrate VFI models\ninto codecs or reinforcement learning-based streaming\nagents may unlock robust low-latency video systems.\nIn particular, methods that unify frame interpolation\nwith residual-based encoding and decoding schemes\ncould blur the boundary between generation and com-\npression, laying the groundwork for next-generation\nstreaming protocols.\n7.2 All-in-One LLV Video Restoration\nWhile all-in-one models have shown promising results\nin image restoration [250]–[252], equivalent progress\nin the video domain particularly in unified LLV\nframeworks, remains limited. Current LLV restoration\npipelines remain fragmented, with VFI, denoising,\ndeblurring, and SR often treated as separate tasks. This\nmodularity, while convenient for controlled bench-\nmarking, limits model robustness under real-world\ndegradations that involve complex mixtures of tem-\nporal and spatial artifacts. A promising direction is\nthe development of unified, all-in-one architectures\nthat perform multiple LLV tasks jointly, where VFI\nis not treated as a standalone module but as an in-\ntegral part of a broader restoration framework. The\ninterpolated frames can offer temporally consistent\nguidance for denoising or deblurring, while super-\nresolved outputs can enhance motion estimation accu-\nracy. Cross-task consistency losses or multi-task learn-\ning objectives can foster synergistic improvements.\nMoreover, transformer- or diffusion-based architec-\ntures with spatio-temporal attention mechanisms are\nnaturally suited to this multi-task paradigm, as they\ncan encode long-range dependencies and modulate\ntask-specific pathways via conditioning.\n--- Page 20 ---\n20\n7.3 3D and 4D Scene Understanding\nVFI research remains largely grounded in 2D image\nspace, often assuming planar motion and flat appear-\nance fields. However, the increasing prevalence of 3D-\naware applications in AR/VR, robotics, and multiview\nrendering calls for VFI methods that explicitly ac-\ncount for the underlying geometry of dynamic scenes.\nRecent works in 4D scene modeling using tempo-\nral neural fields [253], dynamic Gaussians [254], and\nneural point representations [255] suggest that tem-\nporally coherent synthesis is possible when motion\nis modeled in 3D space. Integrating VFI into such\npipelines enables physically plausible interpolation\nthat respects depth, occlusion, and parallax. Depth-\nconditioned flow, pose-aware synthesis, or geometry-\naware latent spaces may serve as intermediate repre-\nsentations. Furthermore, VFI models can be extended\nto generate novel viewpoints, enabling geometry-\nconsistent interpolation across spatial and temporal\ndomains. Applications span from time-synchronized\nmultiview interpolation to immersive scene recon-\nstruction from sparse video inputs. Future work may\nexplore co-training paradigms that fuse interpolation\nand view synthesis losses, jointly supervising geom-\netry, appearance, and motion fields across space-time\nvolumes.\nREFERENCES\n[1] R. Szeliski, “Prediction error as a quality metric for motion\nand stereo,” in Proceedings of IEEE International Conference\non Computer Vision , vol. 2. IEEE, 1999, pp. 781–788.\n[2] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros,\n“View synthesis by appearance flow,” in Proceedings of the\nProceedings of the European Conference on Computer Vision .\nSpringer, 2016, pp. 286–301.\n[3] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, “Deep-\nstereo: Learning to predict new views from the world’s\nimagery,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2016, pp. 5515–5524.\n[4] Z. Li, S. Niklaus, N. Snavely, and O. Wang, “Neural\nscene flow fields for space-time view synthesis of dynamic\nscenes,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2021, pp. 6498–6508.\n[5] H. Jiang, D. Sun, V . Jampani, M.-H. Yang, E. Learned-\nMiller, and J. Kautz, “Super slomo: High quality estima-\ntion of multiple intermediate frames for video interpo-\nlation,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 9000–9008.\n[6] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman, “Video\nenhancement with task-oriented flow,” International Jour-\nnal of Computer Vision , vol. 127, pp. 1106–1125, 2019.\n[7] W. Bao, W.-S. Lai, C. Ma, X. Zhang, Z. Gao, and M.-\nH. Yang, “Depth-aware video frame interpolation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2019, pp. 3703–3712.\n[8] X. Xiang, Y. Tian, Y. Zhang, Y. Fu, J. P . Allebach, and\nC. Xu, “Zooming slow-mo: Fast and accurate one-stage\nspace-time video super-resolution,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2020, pp. 3370–3379.[9] B.-U. Jeon and K. Chung, “Dynamic framerate slow-\nfast network for improving autonomous driving perfor-\nmance,” IEIE Transactions on Smart Processing & Computing ,\nvol. 12, no. 3, pp. 261–268, 2023.\n[10] Z. Huang, A. Huang, X. Hu, C. Hu, J. Xu, and\nS. Zhou, “Scale-adaptive feature aggregation for efficient\nspace-time video super-resolution,” in Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , 2024, pp. 4228–4239.\n[11] C.-Y. Wu, N. Singhal, and P . Krahenbuhl, “Video compres-\nsion through image interpolation,” in Proceedings of the\nProceedings of the European Conference on Computer Vision ,\n2018, pp. 416–431.\n[12] D. Chun, T. S. Kim, K. Lee, and H.-J. Lee, “Compressed\nvideo restoration using a generative adversarial network\nfor subjective quality enhancement,” IEIE Transactions on\nSmart Processing & Computing , vol. 9, no. 1, pp. 1–6, 2020.\n[13] Z. Jia, Y. Lu, and H. Li, “Neighbor correspondence match-\ning for flow-based video frame synthesis,” in Proceedings\nof the ACM International Conference on Multimedia , 2022, pp.\n5389–5397.\n[14] H. Takahashi, T. Nagumo, K. Jo, A. Andreas, S. Rad,\nR. C. Daudt, Y. Miyatani, H. Wakabayashi, and C. Bran-\ndli, “Coupled video frame interpolation and encoding\nwith hybrid event cameras for low-power high-framerate\nvideo,” arXiv preprint arXiv:2503.22491 , 2025.\n[15] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale\nvideo prediction beyond mean square error,” International\nConference on Learning Representations , 2015.\n[16] Z. Liu, R. A. Yeh, X. Tang, Y. Liu, and A. Agarwala, “Video\nframe synthesis using deep voxel flow,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision ,\n2017, pp. 4463–4471.\n[17] S. Hirose, K. Kotoyori, K. Arunruangsirilert, F. Lin, H. Sun,\nand J. Katto, “Real-time video prediction with fast video\ninterpolation model and prediction training,” in IEEE In-\nternational Conference on Image Processing . IEEE, 2024, pp.\n2015–2021.\n[18] H. Liu, X. Yang, T. Akiyama, Y. Huang, Q. Li, S. Kuriyama,\nand T. Taketomi, “Tango: Co-speech gesture video reenact-\nment with hierarchical audio motion embedding and dif-\nfusion interpolation,” International Conference on Learning\nRepresentations , 2025.\n[19] H. Liu, Z. Xu, F.-T. Hong, H.-P . Huang, Y. Zhou,\nand Y. Zhou, “Video motion graphs,” arXiv preprint\narXiv:2503.20218 , 2025.\n[20] A. Bigata, R. Mira, S. Bounareli, K. Vougioukas, Z. Land-\ngraf, N. Drobyshev, M. Zieba, S. Petridis, M. Pantic et al. ,\n“Keyface: Expressive audio-driven facial animation for\nlong sequences via keyframe interpolation,” Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2025.\n[21] Q. Hou, A. Ghildyal, and F. Liu, “A perceptual quality\nmetric for video frame interpolation,” in Proceedings of the\nEuropean Conference on Computer Vision . Springer, 2022,\npp. 234–253.\n[22] D. Danier, F. Zhang, and D. R. Bull, “Bvi-vfi: a video\nquality database for video frame interpolation,” IEEE\nTransactions on Image Processing , vol. 32, pp. 6004–6019,\n2023.\n[23] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” 2015.\n[24] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convolu-\ntional networks for biomedical image segmentation,” in\nMedical Image Computing and Computer-Assisted interven-\ntion. Springer, 2015, pp. 234–241.\n[25] ¨O. C ¸ ic ¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and\nO. Ronneberger, “3d u-net: learning dense volumetric\nsegmentation from sparse annotation,” in Medical Image\nComputing and Computer-Assisted Intervention . Springer,\n2016, pp. 424–432.\n--- Page 21 ---\n21\n[26] M. Nottebaum, S. Roth, and S. Schaub-Meyer, “Efficient\nfeature extraction for high-resolution video frame interpo-\nlation,” British Machine Vision Conference , 2022.\n[27] B. K. Horn and B. G. Schunck, “Determining optical flow,”\nArtificial intelligence , vol. 17, no. 1-3, pp. 185–203, 1981.\n[28] S. Niklaus, L. Mai, and F. Liu, “Video frame interpolation\nvia adaptive convolution,” in Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , 2017, pp.\n670–679.\n[29] ——, “Video frame interpolation via adaptive separable\nconvolution,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2017, pp. 261–270.\n[30] S. Niklaus and F. Liu, “Context-aware synthesis for video\nframe interpolation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2018, pp. 1701–\n1710.\n[31] T. Peleg, P . Szekely, D. Sabo, and O. Sendik, “Im-net for\nhigh resolution video frame interpolation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2019, pp. 2398–2407.\n[32] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets\nv2: More deformable, better results,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2019, pp. 9308–9316.\n[33] W. Bao, W.-S. Lai, X. Zhang, Z. Gao, and M.-H. Yang,\n“Memc-net: Motion estimation and motion compensa-\ntion driven neural network for video interpolation and\nenhancement,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence , vol. 43, no. 3, pp. 933–948, 2019.\n[34] X. Cheng and Z. Chen, “Video frame interpolation via\ndeformable separable convolution,” in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 34, no. 07,\n2020, pp. 10 607–10 614.\n[35] J. Park, K. Ko, C. Lee, and C.-S. Kim, “Bmbc: Bilateral\nmotion estimation with bilateral cost volume for video\ninterpolation,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2020, pp. 109–125.\n[36] H. Lee, T. Kim, T.-y. Chung, D. Pak, Y. Ban, and S. Lee,\n“Adacof: Adaptive collaboration of flows for video frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2020, pp. 5316–\n5325.\n[37] S. Gui, C. Wang, Q. Chen, and D. Tao, “Featureflow:\nRobust video interpolation via structure-to-texture genera-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 14 004–14 013.\n[38] S. Niklaus, L. Mai, and O. Wang, “Revisiting adaptive con-\nvolutions for video frame interpolation,” in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer\nVision , 2021, pp. 1099–1109.\n[39] Z. Shi, X. Liu, K. Shi, L. Dai, and J. Chen, “Video frame\ninterpolation via generalized deformable convolution,”\nIEEE Transactions on Multimedia , vol. 24, pp. 426–439, 2021.\n[40] X. Cheng and Z. Chen, “Multiple video frame interpo-\nlation via enhanced deformable separable convolution,”\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence , vol. 44, no. 10, pp. 7029–7045, 2021.\n[41] T. Ding, L. Liang, Z. Zhu, and I. Zharkov, “Cdfi:\nCompression-driven network design for frame interpola-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2021, pp. 8001–8011.\n[42] Z. Chen, R. Wang, H. Liu, and Y. Wang, “Pdwn: Pyramid\ndeformable warping network for video interpolation,”\nIEEE Open Journal of Signal Processing , vol. 2, pp. 413–424,\n2021.\n[43] D. Danier, F. Zhang, and D. Bull, “Enhancing deformable\nconvolution based video frame interpolation with coarse-\nto-fine 3d cnn,” in IEEE International Conference on Image\nProcessing . IEEE, 2022, pp. 1396–1400.\n[44] X. Ding, P . Huang, D. Zhang, and X. Zhao, “Video frame\ninterpolation via local lightweight bidirectional encodingwith channel attention cascade,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing . IEEE,\n2022, pp. 1915–1919.\n[45] T. Kalluri, D. Pathak, M. Chandraker, and D. Tran, “Flavr:\nFlow-agnostic video representations for fast frame inter-\npolation,” in Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision , 2023, pp. 2071–2082.\n[46] K. Zhou, W. Li, X. Han, and J. Lu, “Exploring motion\nambiguity and alignment for high-quality video frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2023, pp. 22 169–\n22 179.\n[47] T. Shen, D. Li, Z. Gao, L. Tian, and E. Barsoum, “Ladder:\nAn efficient framework for video frame interpolation,”\narXiv preprint arXiv:2404.11108 , 2024.\n[48] S. Meyer, O. Wang, H. Zimmer, M. Grosse, and A. Sorkine-\nHornung, “Phase-based frame interpolation for video,” in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2015, pp. 1410–1418.\n[49] S. Meyer, A. Djelouah, B. McWilliams, A. Sorkine-\nHornung, M. Gross, and C. Schroers, “Phasenet for video\nframe interpolation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2018, pp. 498–\n507.\n[50] M. Choi, H. Kim, B. Han, N. Xu, and K. M. Lee, “Channel\nattention is all you need for video frame interpolation,” in\nProceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 34, no. 07, 2020, pp. 10 663–10 671.\n[51] Z. Shi, X. Xu, X. Liu, J. Chen, and M.-H. Yang, “Video\nframe interpolation transformer,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2022, pp. 17 482–17 491.\n[52] L. Lu, R. Wu, H. Lin, J. Lu, and J. Jia, “Video frame inter-\npolation with transformer,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022,\npp. 3532–3542.\n[53] G. Zhang, Y. Zhu, H. Wang, Y. Chen, G. Wu, and L. Wang,\n“Extracting motion and appearance via inter-frame atten-\ntion for efficient video frame interpolation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2023, pp. 5682–5692.\n[54] Z. Li, Z.-L. Zhu, L.-H. Han, Q. Hou, C.-L. Guo, and M.-M.\nCheng, “Amt: All-pairs multi-field transforms for efficient\nframe interpolation,” in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , 2023, pp.\n9801–9810.\n[55] J. Park, J. Kim, and C.-S. Kim, “Biformer: Learning bilateral\nmotion estimation via bilateral transformer for 4k video\nframe interpolation,” in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , 2023, pp.\n1568–1577.\n[56] D. Zhang, P . Huang, X. Ding, F. Li, W. Zhu, Y. Song,\nand G. Yang, “L2bec2: Local lightweight bidirectional\nencoding and channel attention cascade for video frame\ninterpolation,” ACM Transactions on Multimedia Computing,\nCommunications and Applications , vol. 19, no. 2, pp. 1–19,\n2023.\n[57] C. Liu, G. Zhang, R. Zhao, and L. Wang, “Sparse global\nmatching for video frame interpolation with large mo-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2024, pp. 19 125–19 134.\n[58] J. Park, C. Lee, and C.-S. Kim, “Asymmetric bilateral mo-\ntion estimation for video frame interpolation,” in Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision , 2021, pp. 14 539–14 548.\n[59] Z. Huang, T. Zhang, W. Heng, B. Shi, and S. Zhou,\n“Real-time intermediate flow estimation for video frame\ninterpolation,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2022, pp. 624–642.\n[60] S. Niklaus and F. Liu, “Softmax splatting for video frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference\n--- Page 22 ---\n22\non Computer Vision and Pattern Recognition , 2020, pp. 5437–\n5446.\n[61] M. Jaderberg, K. Simonyan, A. Zisserman et al. , “Spatial\ntransformer networks,” vol. 28, 2015.\n[62] Z. Chi, R. Mohammadi Nasiri, Z. Liu, J. Lu, J. Tang, and\nK. N. Plataniotis, “All at once: Temporally adaptive multi-\nframe interpolation with advanced motion modeling,” in\nEuropean conference on computer vision -¿ proceedings of the\neuropean conference on computer vision . Springer, 2020, pp.\n107–123.\n[63] P . Haavisto, J. Juhola, and Y. Neuvo, “Fractional frame\nrate up-conversion using weighted median filters,” IEEE\nTransactions on Consumer Electronics , vol. 35, no. 3, pp. 272–\n278, 1989.\n[64] Y. Nakaya and H. Harashima, “Motion compensation\nbased on spatial transformations,” IEEE Transactions on\ncircuits and Systems for Video Technology , vol. 4, no. 3, pp.\n339–356, 1994.\n[65] R. Castagno, P . Haavisto, and G. Ramponi, “A method for\nmotion adaptive frame rate up-conversion,” IEEE Trans-\nactions on circuits and Systems for Video Technology , vol. 6,\nno. 5, pp. 436–446, 1996.\n[66] S.-H. Lee, Y.-C. Shin, S. Yang, H.-H. Moon, and R.-H.\nPark, “Adaptive motion-compensated interpolation for\nframe rate up-conversion,” IEEE Transactions on Consumer\nElectronics , vol. 48, no. 3, pp. 444–450, 2002.\n[67] T. Ha, S. Lee, and J. Kim, “Motion compensated frame\ninterpolation by new block-based motion estimation algo-\nrithm,” IEEE Transactions on Consumer Electronics , vol. 50,\nno. 2, pp. 752–759, 2004.\n[68] B.-D. Choi, J.-W. Han, C.-S. Kim, and S.-J. Ko, “Motion-\ncompensated frame interpolation using bilateral motion\nestimation and adaptive overlapped block motion com-\npensation,” IEEE Transactions on Circuits and Systems for\nVideo Technology , vol. 17, no. 4, pp. 407–416, 2007.\n[69] S.-J. Kang, K.-R. Cho, and Y. H. Kim, “Motion compen-\nsated frame rate up-conversion using extended bilateral\nmotion estimation,” IEEE Transactions on Consumer Elec-\ntronics , vol. 53, no. 4, pp. 1759–1767, 2008.\n[70] A.-M. Huang and T. Q. Nguyen, “A multistage mo-\ntion vector processing method for motion-compensated\nframe interpolation,” IEEE Transactions on Image Processing ,\nvol. 17, no. 5, pp. 694–708, 2008.\n[71] D. Wang, L. Zhang, and A. Vincent, “Motion-compensated\nframe rate up-conversion—part i: Fast multi-frame mo-\ntion estimation,” IEEE Transactions on Broadcasting , vol. 56,\nno. 2, pp. 133–141, 2010.\n[72] D. Wang, A. Vincent, P . Blanchfield, and R. Klepko,\n“Motion-compensated frame rate up-conversion—part ii:\nNew algorithms for frame interpolation,” IEEE Transac-\ntions on Broadcasting , vol. 56, no. 2, pp. 142–149, 2010.\n[73] Y.-L. Liu, Y.-T. Liao, Y.-Y. Lin, and Y.-Y. Chuang, “Deep\nvideo frame interpolation using cyclic frame generation,”\ninProceedings of the AAAI Conference on Artificial Intelli-\ngence , vol. 33, no. 01, 2019, pp. 8794–8802.\n[74] L. Yuan, Y. Chen, H. Liu, T. Kong, and J. Shi, “Zoom-in-to-\ncheck: Boosting video interpolation via instance-level dis-\ncrimination,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 12 183–\n12 191.\n[75] F. A. Reda, D. Sun, A. Dundar, M. Shoeybi, G. Liu, K. J.\nShih, A. Tao, J. Kautz, and B. Catanzaro, “Unsupervised\nvideo interpolation using cycle consistency,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision ,\n2019, pp. 892–900.\n[76] X. Xu, L. Siyao, W. Sun, Q. Yin, and M.-H. Yang,\n“Quadratic video interpolation,” vol. 32, 2019.\n[77] Y. Liu, L. Xie, L. Siyao, W. Sun, Y. Qiao, and C. Dong,\n“Enhanced quadratic video interpolation,” in Proceedings\nof the European Conference on Computer Vision . Springer,\n2020, pp. 41–56.[78] H. Zhang, Y. Zhao, and R. Wang, “A flexible recurrent\nresidual pyramid network for video frame interpolation,”\ninProceedings of the European Conference on Computer Vision .\nSpringer, 2020, pp. 474–491.\n[79] H. Sim, J. Oh, and M. Kim, “Xvfi: extreme video frame\ninterpolation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 14 489–14 498.\n[80] P . Hu, S. Niklaus, S. Sclaroff, and K. Saenko, “Many-to-\nmany splatting for efficient video frame interpolation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2022, pp. 3553–3562.\n[81] L. Kong, B. Jiang, D. Luo, W. Chu, X. Huang, Y. Tai,\nC. Wang, and J. Yang, “Ifrnet: Intermediate feature refine\nnetwork for efficient frame interpolation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2022, pp. 1969–1978.\n[82] W. Shangguan, Y. Sun, W. Gan, and U. S. Kamilov, “Learn-\ning cross-video neural representations for high-quality\nframe interpolation,” in Proceedings of the European Con-\nference on Computer Vision . Springer, 2022, pp. 511–528.\n[83] F. Reda, J. Kontkanen, E. Tabellion, D. Sun, C. Pantofaru,\nand B. Curless, “Film: Frame interpolation for large mo-\ntion,” in Proceedings of the European Conference on Computer\nVision . Springer, 2022, pp. 250–266.\n[84] S. Niklaus, P . Hu, and J. Chen, “Splatting-based synthe-\nsis for video frame interpolation,” in Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , 2023, pp. 713–723.\n[85] X. Jin, L. Wu, J. Chen, Y. Chen, J. Koo, and C.-h. Hahm,\n“A unified pyramid recurrent network for video frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2023, pp. 1578–\n1587.\n[86] M. Hu, K. Jiang, Z. Zhong, Z. Wang, and Y. Zheng, “Iq-\nvfi: implicit quadratic motion estimation for video frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2024, pp. 6410–\n6419.\n[87] J. Jeong, H. Cai, R. Garrepalli, J. M. Lin, M. Hayat, and\nF. Porikli, “Ocai: Improving optical flow estimation by\nocclusion and consistency aware interpolation,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2024, pp. 19 352–19 362.\n[88] Z. Guo, W. Li, and C. C. Loy, “Generalizable implicit\nmotion modeling for video frame interpolation,” vol. 37,\n2024, pp. 63 747–63 770.\n[89] G. Wu, X. Tao, C. Li, W. Wang, X. Liu, and Q. Zheng,\n“Perception-oriented video frame interpolation via asym-\nmetric blending,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2024, pp. 2753–\n2762.\n[90] Z. Zhong, G. Krishnan, X. Sun, Y. Qiao, S. Ma, and J. Wang,\n“Clearer frames, anytime: Resolving velocity ambiguity in\nvideo frame interpolation,” in Proceedings of the European\nConference on Computer Vision . Springer, 2024, pp. 346–\n363.\n[91] X. Jin, L. Wu, J. Chen, I. Cho, and C.-H. Hahm, “Unified\narbitrary-time video frame interpolation and prediction,”\ninIEEE International Conference on Acoustics, Speech and\nSignal Processing . IEEE, 2025, pp. 1–5.\n[92] G. Long, L. Kneip, J. M. Alvarez, H. Li, X. Zhang, and\nQ. Yu, “Learning image matching by simply watching\nvideo,” in Proceedings of the European Conference on Com-\nputer Vision . Springer, 2016, pp. 434–450.\n[93] J. Van Amersfoort, W. Shi, A. Acosta, F. Massa, J. Totz,\nZ. Wang, and J. Caballero, “Frame interpolation with\nmulti-scale deep loss functions and generative adversarial\nnetworks,” arXiv preprint arXiv:1711.06045 , 2017.\n[94] J. Xiao and X. Bi, “Multi-scale attention generative ad-\nversarial networks for video frame interpolation,” IEEE\nAccess , vol. 8, pp. 94 842–94 851, 2020.\n--- Page 23 ---\n23\n[95] W. Xue, H. Ai, T. Sun, C. Song, Y. Huang, and L. Wang,\n“Frame-gan: Increasing the frame rate of gait videos with\ngenerative adversarial networks,” Neurocomputing , vol.\n380, pp. 95–104, 2020.\n[96] Q. N. Tran and S.-H. Yang, “Efficient video frame inter-\npolation using generative adversarial networks,” Applied\nSciences , vol. 10, no. 18, p. 6245, 2020.\n[97] D. Danier, F. Zhang, and D. Bull, “St-mfnet: A spatio-\ntemporal multi-flow network for frame interpolation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2022, pp. 3521–3531.\n[98] A. Gu, K. Goel, and C. R ´e, “Efficiently modeling long\nsequences with structured state spaces,” International Con-\nference on Learning Representations , 2021.\n[99] A. Gu and T. Dao, “Mamba: Linear-time sequence mod-\neling with selective state spaces,” Conference on Language\nModeling , 2024.\n[100] G. Zhang, C. Liu, Y. Cui, X. Zhao, K. Ma, and L. Wang,\n“Vfimamba: Video frame interpolation with state space\nmodels,” vol. 37, 2024, pp. 107 225–107 248.\n[101] M. Koren, K. Menda, and A. Sharma, “Frame interpolation\nusing generative adversarial networks,” Tech. Rep., 2017.\n[102] Q. N. Tran and S.-H. Yang, “Video frame interpolation via\ndown–up scale generative adversarial networks,” Com-\nputer Vision and Image Understanding , vol. 220, p. 103434,\n2022.\n[103] S. Wen, W. Liu, Y. Yang, T. Huang, and Z. Zeng, “Gen-\nerating realistic videos from keyframes with concatenated\ngans,” IEEE Transactions on Circuits and Systems for Video\nTechnology , vol. 29, no. 8, pp. 2337–2348, 2018.\n[104] V . Voleti, A. Jolicoeur-Martineau, and C. Pal, “Mcvd-\nmasked conditional video diffusion for prediction, gener-\nation, and interpolation,” vol. 35, 2022, pp. 23 371–23 385.\n[105] D. Danier, F. Zhang, and D. Bull, “Ldmvfi: Video frame\ninterpolation with latent diffusion models,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 38, no. 2,\n2024, pp. 1472–1480.\n[106] S. Jain, D. Watson, E. Tabellion, B. Poole, J. Kontkanen\net al. , “Video interpolation with diffusion models,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2024, pp. 7341–7351.\n[107] Z. Huang, Y. Yu, L. Yang, C. Qin, B. Zheng, X. Zheng,\nZ. Zhou, Y. Wang, and W. Yang, “Motion-aware latent dif-\nfusion models for video frame interpolation,” in Proceed-\nings of the 32nd ACM International Conference on Multimedia ,\n2024, pp. 1043–1052.\n[108] J. Xing, M. Xia, Y. Zhang, H. Chen, W. Yu, H. Liu, G. Liu,\nX. Wang, Y. Shan, and T.-T. Wong, “Dynamicrafter: Ani-\nmating open-domain images with video diffusion priors,”\ninProceedings of the European Conference on Computer Vision .\nSpringer, 2024, pp. 399–417.\n[109] J. Xing, H. Liu, M. Xia, Y. Zhang, X. Wang, Y. Shan, and T.-\nT. Wong, “Tooncrafter: Generative cartoon interpolation,”\nACM Transactions on Graphics , vol. 43, no. 6, pp. 1–11, 2024.\n[110] L. Shen, T. Liu, H. Sun, X. Ye, B. Li, J. Zhang, and Z. Cao,\n“Dreammover: Leveraging the prior of diffusion models\nfor image interpolation with large motion,” in Proceedings\nof the European Conference on Computer Vision . Springer,\n2024, pp. 336–353.\n[111] X. Wang, B. Zhou, B. Curless, I. Kemelmacher-Shlizerman,\nA. Holynski, and S. M. Seitz, “Generative inbetweening:\nAdapting image-to-video models for keyframe interpola-\ntion,” International Conference on Learning Representations ,\n2025.\n[112] H. Feng, Z. Ding, Z. Xia, S. Niklaus, V . Abrevaya, M. J.\nBlack, and X. Zhang, “Explorative inbetweening of time\nand space,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2024, pp. 378–395.\n[113] Z. Lyu, M. Li, J. Jiao, and C. Chen, “Frame interpolation\nwith consecutive brownian bridge diffusion,” in Proceed-ings of the ACM International Conference on Multimedia , 2024,\npp. 3449–3458.\n[114] T. Zhu, D. Ren, Q. Wang, X. Wu, and W. Zuo, “Genera-\ntive inbetweening through frame-wise conditions-driven\nvideo generation,” Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2025.\n[115] S. Yang, T. Kwon, and J. C. Ye, “Vibidsampler: Enhancing\nvideo interpolation using bidirectional diffusion sampler,”\nInternational Conference on Learning Representations , 2025.\n[116] G. Zhang, Y. Zhu, Y. Cui, X. Zhao, K. Ma, and L. Wang,\n“Motion-aware generative frame interpolation,” arXiv\npreprint arXiv:2501.03699 , 2025.\n[117] Z. Zhang, H. Chen, H. Zhao, G. Lu, Y. Fu, H. Xu,\nand Z. Wu, “Eden: Enhanced diffusion for high-quality\nlarge-motion video frame interpolation,” Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2025.\n[118] Y. Hai, G. Wang, T. Su, W. Jiang, and Y. Hu, “Hierarchical\nflow diffusion for efficient frame interpolation,” Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2025.\n[119] J. Hur, C. Herrmann, S. Saxena, J. Kontkanen, W.-S. Lai,\nY. Shih, M. Rubinstein, D. J. Fleet, and D. Sun, “High-\nresolution frame interpolation with patch-based cascaded\ndiffusion,” in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 39, no. 4, 2025, pp. 3868–3876.\n[120] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet\nclassification with deep convolutional neural networks,”\nvol. 60, no. 6. AcM New York, NY, USA, 2017, pp. 84–90.\n[121] J. Jain and A. Jain, “Displacement measurement and its\napplication in interframe image coding,” IEEE Transactions\non communications , vol. 29, no. 12, pp. 1799–1808, 1981.\n[122] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n“Deformable convolutional networks,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2017,\npp. 764–773.\n[123] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is\nall you need,” vol. 30, 2017.\n[124] C. Liu, H. Yang, J. Fu, and X. Qian, “Ttvfi: Learning\ntrajectory-aware transformer for video frame interpola-\ntion,” IEEE Transactions on Image Processing , vol. 32, pp.\n4728–4741, 2023.\n[125] X. Jin, L. Wu, G. Shen, Y. Chen, J. Chen, J. Koo, and C.-\nh. Hahm, “Enhanced bi-directional motion estimation for\nvideo frame interpolation,” in Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision , 2023,\npp. 5049–5057.\n[126] S. Jiang, D. Campbell, Y. Lu, H. Li, and R. Hartley,\n“Learning to estimate hidden motions with global motion\naggregation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 9772–9781.\n[127] P . Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid,\n“Deepflow: Large displacement optical flow with deep\nmatching,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2013, pp. 1385–1392.\n[128] A. Dosovitskiy, P . Fischer, E. Ilg, P . Hausser, C. Hazirbas,\nV . Golkov, P . Van Der Smagt, D. Cremers, and T. Brox,\n“Flownet: Learning optical flow with convolutional net-\nworks,” in Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision , 2015, pp. 2758–2766.\n[129] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and\nT. Brox, “Flownet 2.0: Evolution of optical flow estimation\nwith deep networks,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2017, pp. 2462–\n2470.\n[130] A. Ranjan and M. J. Black, “Optical flow estimation using\na spatial pyramid network,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2017,\npp. 4161–4170.\n--- Page 24 ---\n24\n[131] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “Pwc-net: Cnns\nfor optical flow using pyramid, warping, and cost vol-\nume,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 8934–8943.\n[132] T.-W. Hui, X. Tang, and C. C. Loy, “Liteflownet: A\nlightweight convolutional neural network for optical flow\nestimation,” in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2018, pp. 8981–8989.\n[133] A. Bar-Haim and L. Wolf, “Scopeflow: Dynamic scene\nscoping for optical flow,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020,\npp. 7998–8007.\n[134] Z. Teed and J. Deng, “Raft: Recurrent all-pairs field trans-\nforms for optical flow,” in Proceedings of the European\nConference on Computer Vision . Springer, 2020, pp. 402–\n419.\n[135] Z. Huang, X. Shi, C. Zhang, Q. Wang, K. C. Cheung,\nH. Qin, J. Dai, and H. Li, “Flowformer: A transformer\narchitecture for optical flow,” in Proceedings of the European\nConference on Computer Vision . Springer, 2022, pp. 668–685.\n[136] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao,\n“Gmflow: Learning optical flow via global matching,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2022, pp. 8121–8130.\n[137] W. Seo, J. Oh, and M. Kim, “Bim-vfi: directional mo-\ntion field-guided frame interpolation for video with non-\nuniform motions,” Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2025.\n[138] D. Fourure, R. Emonet, E. Fromont, D. Muselet,\nA. Tremeau, and C. Wolf, “Residual conv-deconv grid net-\nwork for semantic segmentation,” British Machine Vision\nConference , 2017.\n[139] Y. Zhang, C. Wang, and D. Tao, “Video frame interpolation\nwithout temporal priors,” Advances in Neural Information\nProcessing Systems , vol. 33, pp. 13 308–13 318, 2020.\n[140] J. Du, Y. Sun, Z. Zhou, P . Chen, R. Zhang, and K. Mao,\n“Mambaflow: A mamba-centric architecture for end-to-\nend optical flow estimation,” Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , 2025.\n[141] Q. Dong and Y. Fu, “Memflow: Optical flow estima-\ntion and prediction with memory,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2024, pp. 19 068–19 078.\n[142] E. P . Simoncelli, W. T. Freeman, E. H. Adelson, and D. J.\nHeeger, “Shiftable multiscale transforms,” IEEE transac-\ntions on Information Theory , vol. 38, no. 2, pp. 587–607, 1992.\n[143] E. P . Simoncelli and W. T. Freeman, “The steerable pyra-\nmid: A flexible architecture for multi-scale derivative com-\nputation,” in Proceedings of International Conference on Image\nProcessing , vol. 3. IEEE, 1995, pp. 444–447.\n[144] J. Portilla and E. P . Simoncelli, “A parametric texture\nmodel based on joint statistics of complex wavelet coef-\nficients,” International journal of Computer Vision , vol. 40,\npp. 49–70, 2000.\n[145] N. Wadhwa, M. Rubinstein, F. Durand, and W. T. Freeman,\n“Phase-based video motion processing,” ACM Transactions\non Graphics , vol. 32, no. 4, pp. 1–10, 2013.\n[146] P . Didyk, P . Sitthi-Amorn, W. Freeman, F. Durand, and\nW. Matusik, “Joint view expansion and filtering for au-\ntomultiscopic 3d displays,” ACM Transactions on Graphics ,\nvol. 32, no. 6, pp. 1–8, 2013.\n[147] S. Liu and W. Deng, “Very deep convolutional neural\nnetwork based image classification using small training\nsample size,” in IAPR Asian conference on pattern recogni-\ntion. IEEE, 2015, pp. 730–734.\n[148] H. Men, V . Hosu, H. Lin, A. Bruhn, and D. Saupe, “Visual\nquality assessment for interpolated slow-motion videos\nbased on a novel database,” in International Conference on\nQuality of Multimedia Experience . IEEE, 2020, pp. 1–6.\n[149] D. Danier, F. Zhang, and D. Bull, “A subjective qualitystudy for video frame interpolation,” in IEEE International\nConference on Image Processing . IEEE, 2022, pp. 1361–1365.\n[150] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n“Generative adversarial networks,” Communications of the\nACM , vol. 63, no. 11, pp. 139–144, 2020.\n[151] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and\nA. C. Courville, “Improved training of wasserstein gans,”\nvol. 30, 2017.\n[152] D. Berthelot, T. Schumm, and L. Metz, “Began: Bound-\nary equilibrium generative adversarial networks,” arXiv\npreprint arXiv:1703.10717 , 2017.\n[153] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and\nO. Winther, “Autoencoding beyond pixels using a learned\nsimilarity metric,” in International Conference on Machine\nLearning . PMLR, 2016, pp. 1558–1566.\n[154] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gen-\nerative adversarial networks,” in International Conference\non Machine Learning . PMLR, 2017, pp. 214–223.\n[155] J. Chen, Y. Li, K. Ma, and Y. Zheng, “Generative adver-\nsarial networks for video-to-video domain adaptation,” in\nProceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 34, no. 04, 2020, pp. 3462–3469.\n[156] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin,\nand B. Guo, “Swin transformer: Hierarchical vision trans-\nformer using shifted windows,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021,\npp. 10 012–10 022.\n[157] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan,\nand M.-H. Yang, “Restormer: Efficient transformer for\nhigh-resolution image restoration,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2022, pp. 5728–5739.\n[158] M. Arjovsky, A. Shah, and Y. Bengio, “Unitary evolution\nrecurrent neural networks,” in International Conference on\nMachine Learning . PMLR, 2016, pp. 1120–1128.\n[159] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia,\n“Mambair: A simple baseline for image restoration with\nstate-space model,” in Proceedings of the European Confer-\nence on Computer Vision . Springer, 2024, pp. 222–241.\n[160] H. Guo, Y. Guo, Y. Zha, Y. Zhang, W. Li, T. Dai, S.-T. Xia,\nand Y. Li, “Mambairv2: Attentive state space restoration,”\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2025.\n[161] W. Wang, Q. Wang, K. Zheng, H. Ouyang, Z. Chen,\nB. Gong, H. Chen, Y. Shen, and C. Shen, “Framer: In-\nteractive frame interpolation,” International Conference on\nLearning Representations , 2025.\n[162] J. Ho, A. Jain, and P . Abbeel, “Denoising diffusion proba-\nbilistic models,” vol. 33, 2020, pp. 6840–6851.\n[163] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and\nT. Salimans, “Cascaded diffusion models for high fidelity\nimage generation,” Journal of Machine Learning Research ,\nvol. 23, no. 47, pp. 1–33, 2022.\n[164] R. Rombach, A. Blattmann, D. Lorenz, P . Esser, and B. Om-\nmer, “High-resolution image synthesis with latent diffu-\nsion models,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 10 684–\n10 695.\n[165] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi,\nand D. J. Fleet, “Video diffusion models,” vol. 35, 2022,\npp. 8633–8646.\n[166] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W.\nKim, S. Fidler, and K. Kreis, “Align your latents: High-\nresolution video synthesis with latent diffusion models,”\ninProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2023, pp. 22 563–22 575.\n[167] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch,\nM. Kilian, D. Lorenz, Y. Levi, Z. English, V . Voleti, A. Letts\net al. , “Stable video diffusion: Scaling latent video diffusion\n--- Page 25 ---\n25\nmodels to large datasets,” arXiv preprint arXiv:2311.15127 ,\n2023.\n[168] P . Dhariwal and A. Nichol, “Diffusion models beat gans\non image synthesis,” vol. 34, 2021, pp. 8780–8794.\n[169] S. Zhang et al. , “I2vgen-xl: High-quality image-to-video\nsynthesis via cascaded diffusion models,” arXiv preprint\narXiv:2311.04145 , 2023.\n[170] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss,\nS. Zada, A. Ephrat, J. Hur, G. Liu, A. Raj et al. , “Lumiere: A\nspace-time diffusion model for video generation,” in ACM\nSIGGRAPH Asia Conference Papers , 2024, pp. 1–11.\n[171] D. P . Kingma, M. Welling et al. , “Auto-encoding variational\nbayes.” Banff, Canada, 2013.\n[172] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu,\nY. Shan, X. Qie, and M. Z. Shou, “Tune-a-video: One-\nshot tuning of image diffusion models for text-to-video\ngeneration,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 7623–7633.\n[173] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu,\nY. Yang, W. Hong, X. Zhang, G. Feng et al. , “Cogvideox:\nText-to-video diffusion models with an expert trans-\nformer,” International Conference on Learning Representa-\ntions , 2025.\n[174] S. Yuan et al. , “Identity-preserving text-to-video genera-\ntion by frequency decomposition,” Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2025.\n[175] W. Ren et al. , “Consisti2v: Enhancing visual consistency\nfor image-to-video generation,” Transactions on Machine\nLearning Research , 2024.\n[176] T. Salimans and J. Ho, “Progressive distillation for fast\nsampling of diffusion models,” International Conference on\nLearning Representations , 2022.\n[177] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional\ncontrol to text-to-image diffusion models,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision ,\n2023, pp. 3836–3847.\n[178] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset\nof 101 human actions classes from videos in the wild,”\nConference on Robots and Vision , 2012.\n[179] S. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich, and\nO. Wang, “Deep video deblurring for hand-held cameras,”\ninProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2017.\n[180] L. Siyao, S. Zhao, W. Yu, W. Sun, D. Metaxas, C. C.\nLoy, and Z. Liu, “Deep animation video interpolation in\nthe wild,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2021, pp. 6587–\n6595.\n[181] J. Oh and M. Kim, “Demfi: deep joint deblurring and\nmulti-frame interpolation with flow-guided attentive cor-\nrelation and recursive boosting,” in Proceedings of the Eu-\nropean Conference on Computer Vision . Springer, 2022, pp.\n198–215.\n[182] P . Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Bar-\nlaud, “Two deterministic half-quadratic regularization al-\ngorithms for computed imaging,” in Proceedings of 1st\ninternational conference on image processing , 1994.\n[183] P . Bojanowski, A. Joulin, D. Lopez-Paz, and A. Szlam,\n“Optimizing the latent space of generative networks,”\nInternational Conference on Learning Representations , 2018.\n[184] S. Meister, J. Hur, and S. Roth, “Unflow: Unsupervised\nlearning of optical flow with a bidirectional census loss,”\ninProceedings of the AAAI Conference on Artificial Intelli-\ngence , 2018.\n[185] R. Zabih and J. Woodfill, “Non-parametric local trans-\nforms for computing visual correspondence,” in Proceed-\nings of the European Conference on Computer Vision , 1994.\n[186] K. Nan, R. Xie, P . Zhou, T. Fan, Z. Yang, Z. Chen, X. Li,\nJ. Yang, and Y. Tai, “Openvid-1m: A large-scale high-\nquality dataset for text-to-video generation,” 2024.[187] A. Stergiou, “Lavib: A large-scale video interpolation\nbenchmark,” 2024.\n[188] Y. Wang, Y. Yang, Z. Yang, L. Zhao, P . Wang, and W. Xu,\n“Occlusion aware unsupervised learning of optical flow,”\ninProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2018, pp. 4884–4893.\n[189] S. Baker, D. Scharstein, J. P . Lewis, S. Roth, M. J. Black, and\nR. Szeliski, “A database and evaluation methodology for\noptical flow,” International Journal of Computer Vision , 2011.\n[190] A. Geiger, P . Lenz, and R. Urtasun, “Are we ready for\nautonomous driving? the kitti vision benchmark suite,”\ninProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2012.\n[191] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,\nM. Gross, and A. Sorkine-Hornung, “A benchmark dataset\nand evaluation methodology for video object segmenta-\ntion,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2016.\n[192] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A nat-\nuralistic open source movie for optical flow evaluation,”\ninProceedings of the European Conference on Computer Vision\n-¿ proceedings of the Proceedings of the European Conference on\nComputer Vision , 2012.\n[193] S. Nah, T. Hyun Kim, and K. Mu Lee, “Deep multi-scale\nconvolutional neural network for dynamic scene deblur-\nring,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2017.\n[194] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, “Frozen\nin time: A joint video and image encoder for end-to-\nend retrieval,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 1728–1738.\n[195] C. Montgomery, “Xiph.org video test me-\ndia (derf’s collection),” Online, Available:\nhttps://media.xiph.org/video/derf/, 1994.\n[196] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P . Simoncelli,\n“Image quality assessment: from error visibility to struc-\ntural similarity,” IEEE Transactions on Image Processing ,\n2004.\n[197] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a\n“completely blind” image quality analyzer,” IEEE Signal\nProcessing Letters , vol. 20, no. 3, pp. 209–212, 2012.\n[198] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter, “Gans trained by a two time-scale update\nrule converge to a local nash equilibrium,” Advances in\nNeural Information Processing Systems , 2017.\n[199] R. Zhang, P . Isola, A. A. Efros, E. Shechtman, and O. Wang,\n“The unreasonable effectiveness of deep features as a\nperceptual metric,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2018.\n[200] D. Danier, F. Zhang, and D. Bull, “Flolpips: A bespoke\nvideo quality metric for frame interpolation,” in Picture\nCoding Symposium , 2022.\n[201] A. Ghildyal and F. Liu, “Shift-tolerant perceptual simi-\nlarity metric,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2022, pp. 91–107.\n[202] K. Ding, K. Ma, S. Wang, and E. P . Simoncelli, “Image\nquality assessment: Unifying structure and texture simi-\nlarity,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence , vol. 44, no. 5, pp. 2567–2581, 2020.\n[203] D. Li, T. Jiang, and M. Jiang, “Quality assessment of in-the-\nwild videos,” in Proceedings of the 27th ACM international\nconference on multimedia , 2019, pp. 2351–2359.\n[204] M. Chu, Y. Xie, J. Mayer, L. Leal-Taix ´e, and N. Thuerey,\n“Learning temporal coherence via self-supervision for\ngan-based video generation,” ACM Transactions on Graph-\nics, vol. 39, no. 4, pp. 75–1, 2020.\n[205] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier,\nM. Michalski, and S. Gelly, “Towards accurate generative\nmodels of video: A new metric & challenges,” International\nConference on Learning Representations Workshop , 2019.\n--- Page 26 ---\n26\n[206] J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, and R. Liao,\n“Fr\\’echet video motion distance: A metric for evaluating\nmotion consistency in videos,” International Conference on\nMachine Learning Workshop , 2024.\n[207] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang,\nT. Wu, Q. Jin, N. Chanpaisit et al. , “Vbench: Comprehen-\nsive benchmark suite for video generative models,” in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 2024.\n[208] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wo-\njna, “Rethinking the inception architecture for computer\nvision,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2016, pp. 2818–2826.\n[209] J. Carreira and A. Zisserman, “Quo vadis, action recogni-\ntion? a new model and the kinetics dataset,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition , 2017, pp. 6299–6308.\n[210] Z. W. Wang, W. Jiang, K. He, B. Shi, A. Katsaggelos,\nand O. Cossairt, “Event-driven video frame synthesis,”\ninProceedings of the IEEE/CVF International Conference on\nComputer Vision Workshops , 2019, pp. 0–0.\n[211] S. Lin, J. Zhang, J. Pan, Z. Jiang, D. Zou, Y. Wang, J. Chen,\nand J. Ren, “Learning event-driven video deblurring and\ninterpolation,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2020, pp. 695–710.\n[212] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig,\nY. Li, and D. Scaramuzza, “Time lens: Event-based video\nframe interpolation,” in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , 2021, pp.\n16 155–16 164.\n[213] Z. Yu, Y. Zhang, D. Liu, D. Zou, X. Chen, Y. Liu, and J. S.\nRen, “Training weakly supervised video frame interpo-\nlation with events,” in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , 2021, pp. 14 589–\n14 598.\n[214] X. Zhang and L. Yu, “Unifying motion deblurring and\nframe interpolation with events,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2022, pp. 17 765–17 774.\n[215] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li,\nand D. Scaramuzza, “Time lens++: Event-based frame\ninterpolation with parametric non-linear flow and multi-\nscale fusion,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 17 755–\n17 764.\n[216] W. He, K. You, Z. Qiao, X. Jia, Z. Zhang, W. Wang,\nH. Lu, Y. Wang, and J. Liao, “Timereplayer: Unlocking\nthe potential of event cameras for video interpolation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2022, pp. 17 804–17 813.\n[217] S. Wu, K. You, W. He, C. Yang, Y. Tian, Y. Wang,\nZ. Zhang, and J. Liao, “Video interpolation by event-\ndriven anisotropic adjustment of optical flow,” in Pro-\nceedings of the European Conference on Computer Vision .\nSpringer, 2022, pp. 267–283.\n[218] T. Kim, Y. Chae, H.-K. Jang, and K.-J. Yoon, “Event-based\nvideo frame interpolation with cross-modal asymmetric\nbidirectional motion fields,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2023,\npp. 18 032–18 042.\n[219] G. Lin, J. Han, M. Cao, Z. Zhong, and Y. Zheng, “Event-\nguided frame interpolation and dynamic range expan-\nsion of single rolling shutter image,” in Proceedings of the\n31st ACM International Conference on Multimedia , 2023, pp.\n3078–3088.\n[220] Y. Liu, Y. Deng, H. Chen, and Z. Yang, “Video frame\ninterpolation via direct synthesis with the event-based\nreference,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2024, pp. 8477–\n8487.[221] Y. Ma, S. Guo, Y. Chen, T. Xue, and J. Gu, “Timelens-\nxl: Real-time event-based video frame interpolation with\nlarge motion,” in Proceedings of the European Conference on\nComputer Vision . Springer, 2024, pp. 178–194.\n[222] J. Chen et al. , “Repurposing pre-trained video diffusion\nmodels for event-based video interpolation,” Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition , 2025.\n[223] Z. Zhang et al. , “Egvd: Event-guided video diffusion\nmodel for physically realistic large-motion frame interpo-\nlation,” arXiv preprint arXiv:2503.20268 , 2025.\n[224] P . Lichtsteiner, C. Posch, and T. Delbruck, “A 128 ×128 120\ndb 15 µs latency asynchronous temporal contrast vision\nsensor,” IEEE Journal of Solid-state Circuits , vol. 43, no. 2,\npp. 566–576, 2008.\n[225] A. Niwa, F. Mochizuki, R. Berner, T. Maruyarma, T. Ter-\nano, K. Takamiya, Y. Kimura, K. Mizoguchi, T. Miyazaki,\nS. Kaizu et al. , “A 2.97 µm-pitch event-based vision sen-\nsor with shared pixel front-end circuitry and low-noise\nintensity readout mode,” in IEEE International Solid-State\nCircuits Conference . IEEE, 2023, pp. 4–6.\n[226] J. Kaiser, J. C. V . Tieck, C. Hubschneider, P . Wolf, M. We-\nber, M. Hoff, A. Friedrich, K. Wojtasik, A. Roennau,\nR. Kohlhaas et al. , “Towards a framework for end-to-\nend control of a simulated vehicle with spiking neural\nnetworks,” in IEEE International Conference on Simulation,\nModeling, and Programming for Autonomous Robots . IEEE,\n2016, pp. 127–134.\n[227] Y. Bi and Y. Andreopoulos, “Pix2nvs: Parameterized con-\nversion of pixel-domain video frames to neuromorphic\nvision streams,” in IEEE International Conference on Image\nProcessing . IEEE, 2017, pp. 1990–1994.\n[228] A. Z. Zhu, Z. Wang, K. Khant, and K. Daniilidis, “Event-\ngan: Leveraging large scale image datasets for event\ncameras,” in IEEE international conference on computational\nphotography . IEEE, 2021, pp. 1–11.\n[229] Z. Zhang, S. Cui, K. Chai, H. Yu, S. Dasgupta, U. Mah-\nbub, and T. Rahman, “V2ce: Video to continuous events\nsimulator,” in IEEE International Conference on Robotics and\nAutomation . IEEE, 2024, pp. 12 455–12 461.\n[230] Y. Meng, H. Ouyang, H. Wang, Q. Wang, W. Wang, K. L.\nCheng, Z. Liu, Y. Shen, and H. Qu, “Anidoc: Animation\ncreation made easier,” Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2025.\n[231] V . F. Chavez, C. Esteves, and J.-B. Hayet, “Time-adaptive\nvideo frame interpolation based on residual diffusion,”\nACM SIGGRAPH , 2025.\n[232] S. Chen and M. Zwicker, “Improving the perceptual qual-\nity of 2d animation interpolation,” in Proceedings of the\nEuropean Conference on Computer Vision . Springer, 2022,\npp. 271–287.\n[233] X. Li, B. Zhang, J. Liao, and P . V . Sander, “Deep sketch-\nguided cartoon video inbetweening,” IEEE Transactions on\nVisualization and Computer Graphics , vol. 28, no. 8, pp. 2938–\n2952, 2021.\n[234] Y. Yang, L. Fan, Z. Lin, F. Wang, and Z. Zhang, “Layerani-\nmate: Layer-specific control for animation,” arXiv preprint\narXiv:2501.08295 , 2025.\n[235] T. Xie, Y. Zhao, Y. Jiang, and C. Jiang, “Physanimator:\nPhysics-guided generative cartoon animation,” Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition , 2025.\n[236] Y. Guo, L. Bi, E. Ahn, D. Feng, Q. Wang, and J. Kim, “A\nspatiotemporal volumetric interpolation network for 4d\ndynamic medical image,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020,\npp. 4726–4735.\n[237] J. Kim, H. Yoon, G. Park, K. Kim, and E. Yang, “Data-\nefficient unsupervised interpolation without any inter-\nmediate frame for 4d medical images,” in Proceedings of\n--- Page 27 ---\n27\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2024, pp. 11 353–11 364.\n[238] X. Li, R. Yang, X. Li, A. Lomax, Y. Zhang, and J. Buh-\nmann, “Cpt-interp: Continuous spatial and temporal mo-\ntion modeling for 4d medical image interpolation,” arXiv\npreprint arXiv:2405.15385 , 2024.\n[239] E. Shechtman, Y. Caspi, and M. Irani, “Increasing space-\ntime resolution in video,” in Proceedings of the European\nConference on Computer Vision . Springer, 2002, pp. 753–\n768.\n[240] S. Y. Kim, J. Oh, and M. Kim, “Fisr: Deep joint frame inter-\npolation and super-resolution with a multi-scale temporal\nloss,” in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 34, no. 07, 2020, pp. 11 278–11 286.\n[241] M. Haris, G. Shakhnarovich, and N. Ukita, “Space-time-\naware multi-resolution video enhancement,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2020, pp. 2859–2868.\n[242] G. Xu, J. Xu, Z. Li, L. Wang, X. Sun, and M.-M.\nCheng, “Temporal modulation network for controllable\nspace-time video super-resolution,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2021, pp. 6388–6397.\n[243] Y.-H. Chen, S.-C. Chen, Y.-Y. Lin, and W.-H. Peng, “Mo-\ntif: Learning motion trajectories with local implicit neu-\nral functions for continuous space-time video super-\nresolution,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 23 131–23 141.\n[244] W. Shen, W. Bao, G. Zhai, L. Chen, X. Min, and Z. Gao,\n“Video frame interpolation and enhancement via pyramid\nrecurrent framework,” IEEE Transactions on Image Process-\ning, vol. 30, pp. 277–292, 2020.\n[245] ——, “Blurry video frame interpolation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2020, pp. 5114–5123.\n[246] Z. Zhong, X. Sun, Z. Wu, Y. Zheng, S. Lin, and I. Sato,\n“Animation from blur: Multi-modal blur decomposition\nwith motion guidance,” in Proceedings of the European\nConference on Computer Vision . Springer, 2022, pp. 599–\n615.\n[247] W. Shang, D. Ren, Y. Yang, H. Zhang, K. Ma, and W. Zuo,\n“Joint video multi-frame interpolation and deblurring\nunder unknown exposure time,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition , 2023, pp. 13 935–13 944.\n[248] Y. Yang, J. Liang, B. Yu, Y. Chen, J. S. Ren, and B. Shi, “La-\ntency correction for event-guided deblurring and frame\ninterpolation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2024, pp. 24 977–\n24 986.\n[249] Z. Yan, J. Pei, H. Wu, H. Tabassum, and P . Wang,\n“Semantic-aware adaptive video streaming using latent\ndiffusion models for wireless networks,” arXiv preprint\narXiv:2502.05695 , 2025.\n[250] B. Li, X. Liu, P . Hu, Z. Wu, J. Lv, and X. Peng, “All-in-one\nimage restoration for unknown corruption,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2022, pp. 17 452–17 462.\n[251] G. Wu, J. Jiang, K. Jiang, and X. Liu, “Content-aware trans-\nformer for all-in-one image restoration,” arXiv preprint\narXiv:2504.04869 , 2025.\n[252] Y. Ai, H. Huang, X. Zhou, J. Wang, and R. He, “Multi-\nmodal prompt perceiver: Empower adaptiveness general-\nizability and fidelity for all-in-one image restoration,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2024, pp. 25 432–25 444.\n[253] S. Park, M. Son, S. Jang, Y. C. Ahn, J.-Y. Kim, and N. Kang,\n“Temporal interpolation is all you need for dynamic neu-\nral radiance fields,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 2023, pp.\n4212–4221.[254] S. Nag, D. Cohen-Or, H. Zhang, and A. Mahdavi-Amiri,\n“In-2-4d: Inbetweening from two single-view images to\n4d generation,” arXiv preprint arXiv:2504.08366 , 2025.\n[255] Z. Zheng, D. Wu, R. Lu, F. Lu, G. Chen, and C. Jiang,\n“Neuralpci: Spatio-temporal neural field for 3d point\ncloud multi-frame non-linear interpolation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2023, pp. 909–918.",
  "text_length": 143162
}