{
  "id": "http://arxiv.org/abs/2506.01048v1",
  "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item\n  Response Theory",
  "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of natural language tasks. However, selecting the optimal LLM to\nrespond to a user query often necessitates a delicate balance between\nperformance and cost. While powerful models deliver better results, they come\nat a high cost, whereas smaller models are more cost-effective but less\ncapable. To address this trade-off, we propose IRT-Router, a multi-LLM routing\nframework that efficiently routes user queries to the most suitable LLM.\nInspired by Item Response Theory (IRT), a psychological measurement\nmethodology, IRT-Router explicitly models the relationship between LLM\ncapabilities and user query attributes. This not only enables accurate\nprediction of response performance but also provides interpretable insights,\nsuch as LLM abilities and query difficulty. Additionally, we design an online\nquery warm-up technique based on semantic similarity, further enhancing the\nonline generalization capability of IRT-Router. Extensive experiments on 20\nLLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline\nmethods in terms of effectiveness and interpretability. Its superior\nperformance in cold-start scenarios further confirms the reliability and\npracticality of IRT-Router in real-world applications. Code is available at\nhttps://github.com/Mercidaiha/IRT-Router.",
  "authors": [
    "Wei Song",
    "Zhenya Huang",
    "Cheng Cheng",
    "Weibo Gao",
    "Bihan Xu",
    "GuanHao Zhao",
    "Fei Wang",
    "Runze Wu"
  ],
  "published": "2025-06-01T15:14:58Z",
  "updated": "2025-06-01T15:14:58Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01048v1",
  "full_text": "--- Page 1 ---\nIRT-Router: Effective and Interpretable Multi-LLM Routing\nvia Item Response Theory\nWei Song1, Zhenya Huang1,2*, Cheng Cheng1, Weibo Gao1, Bihan Xu1,\nGuanhao Zhao1, Fei Wang3, Runze Wu4\n1State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China\n2Institute of Artificial Intelligence Comprehensive National Science Center\n3School of Computing, National University of Singapore\n4NetEase Fuxi Al Lab\n{sw2, doublecheng, weibogao, xbh0720, ghzhao0223}@mail.ustc.edu.cn ,huangzhy@ustc.edu.cn ,\nwang-fei@nus.edu.sg ,wurunze1@corp.netease.com\nAbstract\nLarge language models (LLMs) have demon-\nstrated exceptional performance across a wide\nrange of natural language tasks. However, se-\nlecting the optimal LLM to respond to a user\nquery often necessitates a delicate balance be-\ntween performance and cost. While powerful\nmodels deliver better results, they come at a\nhigh cost, whereas smaller models are more\ncost-effective but less capable. To address this\ntrade-off, we propose IRT-Router, a multi-LLM\nrouting framework that efficiently routes user\nqueries to the most suitable LLM. Inspired by\nItem Response Theory (IRT), a psychological\nmeasurement methodology, IRT-Router explic-\nitly models the relationship between LLM ca-\npabilities and user query attributes. This not\nonly enables accurate prediction of response\nperformance but also provides interpretable in-\nsights, such as LLM abilities and query dif-\nficulty. Additionally, we design an online\nquery warm-up technique based on semantic\nsimilarity, further enhancing the online gen-\neralization capability of IRT-Router. Exten-\nsive experiments on 20 LLMs and 12 datasets\ndemonstrate that IRT-Router outperforms most\nbaseline methods in terms of effectiveness and\ninterpretability. Its superior performance in\ncold-start scenarios further confirms the reli-\nability and practicality of IRT-Router in real-\nworld applications. Code is available at https:\n//github.com/Mercidaiha/IRT-Router .\n1 Introduction\nIn recent years, large language models (LLMs)\nhave demonstrated exceptional capabilities across\na wide range of natural language tasks (Liu et al.,\n2024a; Yang et al., 2024; Liu et al., 2024b; Zhao\net al., 2024; Xue et al., 2024), rapidly becoming a\ndominant force in the field of natural language pro-\ncessing. Generative applications based on LLMs,\nsuch as ChatGPT, have attracted widespread usage\n*Corresponding author\nMMLU MATH CEVAL HumanEval0.00.20.40.60.81.0Average PerformanceGPT-4o ($10/M)\nQWQ-32B-Preview ($1.2/M)Llama3.1-8B-Instruct ($0.2/M)\nGLM-4-Flash ($0.0137/M)Figure 1: Four representative LLMs’ output pricing and\ntheir performance on four different datasets.\nfrom various industries due to their outstanding\naccessibility. Users can input queries in natural\nlanguage and receive responses without the need\nfor specialized data or code. As user demands\nbecome increasingly complex, new LLMs are re-\nleased almost daily. These models vary signifi-\ncantly in terms of reasoning ability, performance,\ncomputational resource requirements, and cost, as\nshown in Figure 1. Generally, larger models tend to\nprovide stronger performance but come with higher\ncomputational costs, while smaller models, though\nmore affordable, often exhibit weaker performance.\nThis diverse LLM ecosystem presents a dilemma\nfor practical applications: How can user queries be\neffectively routed to the most appropriate LLM?\nWhile routing all queries to the largest and most\npowerful model ensures high-quality results, this\napproach is costly and unnecessary. For simpler\nqueries, smaller models are often sufficient, of-\nfering cheaper and faster solutions. On the other\nhand, using large models like o1 or enhanced LLMs\nthrough complex strategies may result in “over-\nthinking” (Chen et al., 2024b), leading to signifi-\ncant resource waste and even reducing answer qual-\nity (Jeong et al., 2024; Xu et al., 2024; Ma et al.,\n2025). Therefore, finding the optimal balance be-\ntween response quality and cost has become a key\nchallenge for the practical application of LLMs.\nLLM Router offers an efficient solution by as-arXiv:2506.01048v1  [cs.AI]  1 Jun 2025\n--- Page 2 ---\nsigning each user query to the most appropriate\nLLM through a custom router (see Figure 2). This\napproach aims to maximize response performance\nwithin cost constraints or minimize cost while\nmaintaining target quality. Early LLM routers\nused static strategies, routing queries to progres-\nsively costlier models until the desired quality was\nachieved (Chen et al., 2023), resulting in resource\nwastage from ineffective requests. Recent data-\ndriven routing methods use pre-trained response\nperformance predictors to predict the response qual-\nity from each LLM after receiving a query and\nrouting the query to the optimal LLM (Ding et al.,\n2024; Hu et al., 2024). Due to their low routing\ncosts and high efficiency, data-driven routing has\nbecome the mainstream approach in LLM router\nresearch.\nHowever, existing data-driven routing methods\nstill face limitations in effectiveness andinter-\npretability : (1) In terms of effectiveness, cur-\nrent approaches often rely on existing models like\nBERT to predict LLM response quality. However,\nthey lack a systematic and rational framework tai-\nlored to the LLM router field, which hinders their\nability to fully exploit the underlying relationships\nbetween LLMs and queries. Furthermore, the ran-\ndomness and openness of user queries cause dis-\ncrepancies between queries in the online environ-\nment and those during the training phase, leading to\na cold-start problem that further limits the effective-\nness of LLM router. (2) In terms of interpretability,\ncurrent methods only output performance scores\nof LLM responses, without providing the rationale\nbehind the predictions. Providing a clear expla-\nnation, such as “Routing the math query to QwQ-\n32B-Preview because it performs better on math\nproblem” (as shown in Figure 1), would signifi-\ncantly enhance the reliability of predictions. Trans-\nparent explanations not only help users understand\nthe decision-making process of the model but also\nincrease trust and acceptance.\nTo address these challenges, we propose IRT-\nRouter , a multi-LLM routing framework built on\nItem Response Theory (IRT). IRT is a psycholog-\nical theory widely used to measure human test-\ntakers’ abilities, which assumes that test-takers\nhave a “latent ability” to answer questions, while\nquestions possess attributes such as “latent diffi-\nculty.” By modeling the probability of test-takers\nproviding correct answers, IRT explicitly captures\nthe implicit relationship between human abilities\nand test item attributes. In our context, each LLM\nQuery\nRouter\n…Candidate LLMs\nQueryQueryQueryQuery\nResponse\nResponse\nResponse\nResponse\nResponseFigure 2: LLM Router. Queries are assigned to different\nLLMs for responses through the trained router.\nis treated as a “test-taker” with a latent, multidi-\nmensional ability to respond to user queries, while\neach query is treated as a “question” with latent\nattributes. Based on IRT, we explicitly model the\nrelationship between the multidimensional abilities\nof LLMs and query attributes (e.g., response diffi-\nculty) to predict the performance of each LLM on\nspecific queries. Combined with cost optimization\nobjectives (e.g., output pricing), IRT-Router selects\nthe most suitable LLM for response. Compared\nto existing methods, IRT-Router is specifically de-\nsigned for this domain, grounded in psychological\nmeasurement theory, making its modeling more\nprincipled. Additionally, IRT-Router quantifies at-\ntributes such as the latent abilities of LLMs and\nthe response difficulty of queries, providing inter-\npretable justifications for routing decisions. To\nmitigate the cold-start problem for new queries af-\nter online deployment, we warm up new queries\nusing semantically similar existing queries. In par-\nticular, we design our model based on two concrete\nimplementations of IRT families, namely MIRT-\nRouter based on Multidimensional IRT (Reckase,\n2009) and NIRT-Router based on NCDM (Wang\net al., 2020). Extensive experiments validate the\neffectiveness and interpretability of our approach.\nThe main contributions of this work are summa-\nrized as follows:\n•We innovatively apply psychological measure-\nment theory to the LLM routing field, exploring\na rational way to combine data mining techniques\nwith LLM routing tasks.\n•IRT-Router is a novel framework tailored to\nLLM routing. It can explicitly establish the rela-\ntionship between query attributes and LLM ability,\nensuring both effectiveness and interpretability.\n•Extensive experiments with 20 LLMs and 12\ndatasets demonstrate that IRT-Router outperforms\n--- Page 3 ---\nmost baseline methods in terms of effectiveness\nand interpretability. Its superiority in cold-start\nscenarios confirms that IRT-Router is more realistic\nand reliable for practical applications.\n2 Related Work\nItem Response Theory. Item Response Theory\n(IRT) (Woodruff and Hanson, 1996) is a widely\nused psychological theory for measuring human\ntest-takers’ abilities. It assumes that test-takers pos-\nsess a “latent ability” to answer questions, while\nthe questions themselves have attributes, such as\n“latent difficulty”. By modeling the probability that\ntest-takers provide correct answers, IRT explicitly\ncaptures implicit relationship between human abili-\nties and the attributes of questions. Specially, IRT\nrelies on the psychological Monotonicity assump-\ntion, which states that the probability of a test-taker\nanswering a question correctly is proportional to\ntheir proficiency in the skill associated with the\nitem, thus ensuring interpretability.\nIn machine learning, IRT has been implemented\nin various forms to diagnose human abilities by\nmodeling response data (Gao et al., 2021, 2023;\nLi et al., 2025; Zhang et al., 2023, 2024; Liu\net al., 2019). For instance, IRT and MIRT mod-\nels (Woodruff and Hanson, 1996; Reckase, 2009)\nuse logistic-like functions to model unidimensional\nand multidimensional learner abilities, respectively.\nMeanwhile, the NCDM model (Wang et al., 2020)\nleverages neural networks to capture higher-order\ninteractions between learners and test items, en-\nabling the evaluation of multidimensional abilities.\nRecently, due to its effectiveness and interpretabil-\nity in human measurement, IRT has been applied to\nassess machine learning models (Liu et al., 2024c;\nMartínez-Plumed et al., 2019), evaluate sample dif-\nficulty (Martínez-Plumed et al., 2022), enhance\nrecommendation systems (Liu et al., 2023), rank\nleaderboards (Rodriguez et al., 2021), and evaluate\nLLM capabilities (Guinet et al., 2024; Gor et al.,\n2024; Liu et al., 2024e).\nWe are motivated to model the relationship be-\ntween LLM and query based on IRT to enhance the\neffectiveness and interpretability of LLM router.\nLLM Router. The LLM Router field remains\nin an exploratory phase (Šakota et al., 2024; Liu\net al., 2024d; Ramírez et al., 2024; Hari and Thom-\nson, 2023; Mohammadshahi et al., 2024; Dai et al.,\n2024; Stojkovic et al., 2024). Unlike Mixture of Ex-\nperts (MoE)(Cai et al., 2024), which requires load-ing all expert parameters on a single machine, or\nLLM ensembling, which requires outputs from all\ncandidate models, LLM routers only assign queries\nto the most suitable model, improving performance\nwhile reducing costs.\nEarlier approaches like FrugalGPT and AutoMix\n(Chen et al., 2023; Aggarwal et al., 2023) cas-\ncade queries through models ordered by cost, ob-\ntaining responses until one is deemed sufficient.\nOther methods use data-driven techniques to train\nlightweight routers for optimal LLM assignment.\nHybridLLM (Ding et al., 2024) uses a BERT-based\nrouter, while RouteLLM (Ong et al., 2024) intro-\nduces routers like SW-ranking, MF, and BERT clas-\nsifiers, focusing mainly on binary routing (large vs.\nsmall models). Zooter (Lu et al., 2023) and Rou-\nterDC (Chen et al., 2024a) leverage smaller models\nwith reward mechanisms or contrastive learning,\nachieving performance similar to larger models.\nGraphRouter (Feng et al., 2024) uses a GNN-based\nrouter but requires prior task knowledge, which\ncan be challenging in real-world use. RouterBench\n(Hu et al., 2024) and (Shnitzer et al., 2023) propose\nKNN-based routing.\nLLM routers are crucial in commercial sys-\ntems that reduce costs for LLM applications,\nsuch as Martian ( withmartian.com ) and Neutrino\nAI (neutrinoapp.com ). Martian claims to “beat\nGPT-4 on performance and reduce costs by 20%-\n97%” through dynamic model routing. LLM API\nproviders like OpenRouter ( openrouter.ai ) also of-\nfer similar capabilities.\n3 Preliminary\n3.1 Problem Definition\nGiven a set of large language models (LLMs)\nM={M1, M 2, . . . , M n}and a set of queries\nQ={q1, q2, . . . , q m}, our goal is to assign each\nquery qi∈ Q to the most suitable LLM Mj∈ M ,\nachieving higher performance and lower cost.\nFor each query qi, we define a scoring function:\nS(qi, Mj) =α·ˆP(qi, Mj)−β· C(Mj),(1)\nwhere:\n•ˆPis the predicted performance of model Mjon\nquery qi, obtained from a trained model.\n•C(Mj)represents the fixed cost of using LLM\nMj. Here, to unify the measurement, we define\nit as the linear mapping of LLM output pricing\n--- Page 4 ---\nCandidate LLMs\nQuery\nAbout:miscellaneousQuestion: A piece of paper that appears blue in sunlight is illuminated solely by a red light that is passed through a green filter. What color does the paper appear under this illumination?Choices:A. Blue    B. Green    C. Red.   D. Black\n+COT\n…\nBERT\nBERT\nLLM ProfileQuery Embedding\nLLM Embedding\nRouter\nTraining Queries\n❄\n🔥Similarity\n🔍Interactive LayerQuery DiscriminationQuery DifficultyRelevance VectorLLM Abilityℳ!ℳ\"ℳ#ℳ!ℳ\"ℳ#𝒞(ℳ#)𝒞(ℳ\")𝒞(ℳ!)LLM Fixed Cost%𝒫(ℳ#)%𝒫(ℳ\")%𝒫(ℳ!)PredictedPerformance_𝒮(ℳ!)𝒮(ℳ\")𝒮(ℳ#)…Score𝓢(𝓜𝟐)𝒮(ℳ%)𝒮(ℳ&)…Ranking\n🏆\nResponseSoftmax\nLearnable Weights\nFixed Parameters\nRouting Lineℳ\"DeepSeek-ChatGPT-4o is released by OpenAI on May 13, 2024. GPT-4o is a flagship multimodal language model capable ofprocessing text, images, and audio inputs…\nFigure 3: Framework of IRT-Router. The left side represents query and LLM embedding, the middle performs\nIRT-based prediction, and the right side outputs the routing decision.\n(see Table 5) to the range [0,1]1. Here, we ap-\nproximate C(Mj)as a fixed cost just for simplic-\nity, which is the most basic representation of cost.\nActually, C(Mj)can be adjusted based on user-\ndefined settings. For instance, if a user has ample\ncomputational resources, the cost for self-hosted\nopen-source LLMs can even be set to 0.\n•αandβare predefined trade-off parameters con-\ntrolling the relative importance of performance and\ncost, with α+β= 1. A larger αindicates a higher\nemphasis on performance, whereas a larger βpri-\noritizes cost efficiency.\nThe optimal model assignment is determined by\nselecting the model that maximizes the score:\nM∗(qi) = arg max\nMj∈MS(qi, Mj). (2)\nThus, the key challenge lies in accurately learn-\ning the performance prediction function ˆP(qi, Mj)\nto ensure effective query routing.\n3.2 Item Response Theory\nIRT is a psychometric theory used to measure\nthe ability of human test-takers based on their re-\nsponses to test items (Woodruff and Hanson, 1996).\nIn the context of our work, the LLMs act as the\ntest-takers, and the queries represent the items. For\na given query qiand a LLM Mj, The predicted\nperformance of Mjonqican be modeled as:\nˆP(qi, Mj) =IRT(θMj;bi, ai, ...), (3)\nwhere, IRT(·)is a general form and has various\nimplementations, such as logistic functions in the\n1For example, the most expensive candidate LLM, GPT-\n4o’s output pricing is $10/M. So C(GPT-4o) is 10/10 = 1 ,\nwhileC(DeepSeek-Chat) is 0.28/10 = 0 .028.IRT model and MIRT model (Woodruff and Han-\nson, 1996; Reckase, 2009), and neural networks in\nthe NCDM model (Wang et al., 2020). For LLM\nmodeling , the key parameter is θMj, representing\nthe model’s ability. For query modeling , there are\nmore parameters. For instance, biis the difficulty\nparameter, which captures the inherent difficulty\nof query qi, andaiis the discrimination parameter,\nwhich controls how sharply the predicted perfor-\nmance changes as θMjincreases.\nSpecially, IRT relies on the psychological Mono-\ntonicity assumption, which states that as the LLM’s\nability θMjincreases, the predicted performance\non query increases. This assumption aligns with\nthe intuitive idea that more capable LLMs are more\nlikely to perform well on more difficult queries,\nensuring the interpretability of LLM router.\n4 Methods\nAs shown in Figure 3, the proposed framework\nof IRT-Router operates as follows: Initially, we\nobtain the embeddings of both the query and the\ncandidate LLMs. Next, the performance of each\nLLM is predicted using an IRT-based model. These\nperformance predictions are then combined with\nthe LLM’s fixed costs to compute ranking scores.\nFinally, the query is routed to the LLM with the\nhighest score for generating the response.\n4.1 Query and LLM Embeddings\nEach query qiis first transformed into a query em-\nbedding eqi∈Rdqusing a pre-trained embedding\nmodel (e.g., BERT). This embedding captures the\nsemantic meaning of the query.\nSimilarly, each LLM Mj(e.g., GPT-4o) is repre-\n--- Page 5 ---\nsented by a corresponding embedding eMj∈RdM,\nwhich is derived from its profile. The profile in-\ncludes metadata such as the model’s release date,\ndeveloper, type, key features, and a brief descrip-\ntion (see Table 10). This profile is then encoded to\nform the LLM’s embedding.\n4.2 IRT-based Prediction\nWe have mentioned in Eq.(3) that there are multiple\nimplementation forms of IRT. Here, we will intro-\nduce a lightweight version, Multidimensional IRT-\nRouter, followed by a more interpretable version,\nNeural IRT-Router.\n4.2.1 MIRT-Router\nInspired by Multidimensional Item Response The-\nory (Reckase, 2009), MIRT-Router models the in-\nteraction between queries and LLMs using a logis-\ntic function, where each LLM is described by its la-\ntent ability θMj∈RNin multiple dimensions, and\neach query is characterized by its discrimination\nai∈RNand difficulty bi∈R. These parameters\nare all obtained through transformation layers:\nθMj=WθeMj,ai=Waeqi, bi=Wbeqi,(4)\nwhere Wθ,WaandWbare all learnable weights.\nInteractive Function. The predicted perfor-\nmance of Mjonqifollows the logistic function:\nˆP(qi, Mj) =1\n1 + exp( −a⊤\niθMj+bi).(5)\nTraining. We train MIRT-Router on a dataset\nDtraincontaining tuples (qi, Mj, yij), where yijis\nthe empirical performance score of Mjonqi. This\nscore is computed by comparing the LLM’s re-\nsponse to the ground truth.\nTo learn the weights, we minimize the binary\ncross-entropy loss:\nL=−X\n(qi,Mj,yij)∈D trainh\nyijlogˆP(qi, Mj)\n+(1−yij) log(1 −ˆP(qi, Mj))i\n.(6)\n4.2.2 NIRT-Router\nWhile MIRT-Router focuses on latent abilities,\nNIRT-Router extends it by incorporating an explicit\nrelevance vector rqithat associates each dimension\nwith specific ability which is predefined (see Ap-\npendix C.1).Relevance Vector The relevance vector rqi∈\nRNrepresents the degree to which a query qiis\nassociated with different ability dimensions.\nForQtrain: To define the relevance vector rqi, we\nfirst perform clustering on the question embeddings\nusing UMAP (McInnes et al., 2018) for dimension-\nality reduction followed by HDBSCAN (McInnes\net al., 2017) clustering, which can adaptively iden-\ntify clusters through density analysis. Each cluster\nrepresents a set of questions that share similar abil-\nity requirements. For each cluster, we identify the\nrelevant abilities by considering the abilities re-\nquired by num (= 5) sample questions within the\ncluster, which are done through a LLM for conve-\nnience (see Appendix C.2). The relevance vector\nrqifor a given query qiis then constructed by as-\nsigning 1 or 0 to each ability dimension, indicating\nwhether the ability is relevant to the query.\nForQtest: Since the true relevance vectors are\nnot available, we approximate rqiusing the mean\nrelevance vectors of its 5-nearest neighbors (5-NN)\nin the embedding space. This ensures that unseen\nqueries still have reasonable relevance estimations.\nInteractive Function. To predict performance of\nMjonqi, NIRT-Router applies a neural interaction\nlayer:\nxij=rqi⊙(θMj−bi)×ai, (7)\nˆP(qi, Mj) =σ(ϕ(W1x⊤\nij+b1)), (8)\nwhere:\nθMj=σ(WθeMj)∈RN,\nai=Waeqi∈R,bi=σ(Wbeqi)∈RN,\nrqi=softmax (rqi)∈RN,(9)\nwhere σ(·)is the sigmoid function, and W1,Wθ,\nWa,Wbare learnable weights.\nTraining. We train NIRT-Router on a dataset\nD′traincontaining tuples (qi, Mj,rqi, yij), where\nrqiis the relevance vector of the query.\nThe loss function remains the same as in MIRT-\nRouter, using a binary cross-entropy loss (Eq.(6)).\n4.3 Routing Decision\nAfter obtaining the predicted performance\nˆP(qi, Mj), we combined it with the LLM fixed\ncostC(Mj)using the score function (Eq.(1)). The\nLLM with the highest score is the one to which the\nquery is routed.\n--- Page 6 ---\n4.4 Warm-Up for Query Cold-Start\nIn real-world scenarios, test queries are typically\nunseen during training, leading to the cold-start\nproblem. Although text-based semantic embed-\ndings somewhat alleviate this issue, we have im-\nplemented a further improvement through refining\nthe query vector by incorporating information from\nsimilar known queries. Specifically, given a test\nquery , we update its vector as\neqi= (1−λ)·eqi+λ·ewarm\nqi, (10)\nwhere ewarm\nqiis an adjustment vector obtained by\naveraging the embeddings of its k-nearest neigh-\nbors in the training set, identified using a similarity\nsearch in the query embedding space.\n5 Experimental Setup\nThis section will provide a detailed explanation of\nthe data construction and setup. As mentioned in\nSection 4.2.1, the training set is defined as Dtrain=\n{(qi, Mj, yij), qi∈ Q train, Mj∈ M , yij∈[0,1]},\nand the test set follows a similar structure.\nSpecifically, we construct interaction data be-\ntween 12 different types of datasets and 20 different\nLLMs. For each query, we generate responses from\nall 20 LLMs. The response quality is then evaluated\nagainst the ground truth using the corresponding\nevaluation metrics described in Table 7, producing\nthe performance score of Mjonqi, which is yij.\n5.1 Datasets\nIn-distribution (ID). In this scenario, we utilized\n8 datasets. For each dataset, we randomly split\nit into a training set (70%) and a test set (30%).\nAll training sets were combined to form the over-\nall training query set Qtrainfor learning the router.\nSimilarly, all test sets were combined to form the\noverall test set Qtest, which was used to evaluate\nthe router in an in-distribution scenario. Since the\ntraining and test query sets were partitioned be-\nfore interacting with the LLMs, all queries in the\ntest set were unseen. The 8 datasets are as fol-\nlows: (1) MMLU (Hendrycks et al., 2021a): A\nbenchmark including 57 tasks across diverse do-\nmains. (2) CMMLU (Li et al., 2024): A Chinese\nmultitask evaluation benchmark covering 67 tasks.\n(3) ACLUE (Zhang and Li, 2023): An ancient\nChinese language understanding benchmark. (4)\nARC_C (Clark et al., 2018): A dataset designed\nto measure advanced reasoning capabilities. (5)Hotpot_QA (Yang et al., 2018): A dataset that re-\nquires multi-hop reasoning across documents. (6)\nSQUAD (Rajpurkar et al., 2018): A reading com-\nprehension dataset consisting of questions posed\nby crowdworkers. (7)MATH (Hendrycks et al.,\n2021b): A dataset of lots of challenging compe-\ntition mathematics problems. (8) MBPP (Austin\net al., 2021): A dataset of programming tasks.\nOut-of-distribution (OOD). We also evaluate\nthe trained router on 4 OOD datasets, which are:\n(1) CEV AL (Huang et al., 2024): A Chinese dataset\nspanning 52 disciplines. (2) Commonsense_QA\n(Talmor et al., 2018): tests commonsense reasoning.\n(3) GSM8K (Cobbe et al., 2021): contains grade-\nschool math word problems. (4) HumanEval:\nevaluates code generation capabilities.\n5.2 Candidate LLMs\nAs mentioned in Section 1, there are various types\nof LLMs available. Here, we select 20 representa-\ntive models (listed in Table 5) as candidate LLMs.\n5.3 Baselines\nWe compare our proposed methods (MIRT-Router\nand NIRT-Router) with Small LLM (i.e., Ministral-\n8B-Instruct-2410) and Large LLM (i.e., GPT-\n4o), which always route queries to the small\nand large models, respectively, as well as with\nsome recent representative works, as follows: Hy-\nbridLLM (Ding et al., 2024) and RouteLLM (Ong\net al., 2024) use DeBERTa-v3 (He et al., 2023) and\nMatrix Factorization (MF) as binary classifiers to\nselect the better-performing model between a pair\nof large and small models, respectively. We de-\nfine the small model as Ministral-8B-Instruct-2410\nand the large model as GPT-4o for both. Router-\nBench (Hu et al., 2024) designs multiple routing\nstrategies to respond to user queries and route them\nto the best LLM from a set of candidates. We\nadopt the “Predictive Router” strategy for our im-\nplementation, as other strategies require obtaining\nresponses from each candidate LLM first, which\nincurs higher costs.\n5.4 Metrics\nFollowing GraphRouter (Feng et al., 2024), we\nevaluate all routing methods using three metrics:\nPerformance The average response performance\nacross all test queries, which is evaluated against\nthe ground truth (see Section 5 and Table 7).\n--- Page 7 ---\nTable 1: Testing results in In-distribution scenario. Performance, Total Cost($) and Reward( ×10−2) are three\nmetrics mentioned in Section 5.4. The best results on each setting are highlighted. The bigger αis, the more\nrequirements for performance. ↓indicates the lower, the better. ↑indicates the higher, the better.\nSetting α= 0.8,β= 0.2 α= 0.5,β= 0.5 α= 0.2,β= 0.8\nPerformance ↑Total Cost ↓Reward ↑Performance ↑Total Cost ↓Reward ↑Performance ↑Total Cost ↓Reward ↑\nSmall LLM 48.70% 0.31 38.48 48.70% 0.31 23.14 48.70% 0.31 7.81\nLarge LLM 77.53% 12.93 42.02 77.53% 12.93 -11.24 77.53% 12.93 -64.49\nHybridLLM 54.37% 1.98 40.43 52.42% 1.54 20.27 56.65% 2.78 -5.84\nRouteLLM 77.25% 12.80 42.00 73.59% 11.15 -6.32 66.24% 7.51 -33.17\nRouterBench 80.01% 1.15 62.23 79.48% 0.53 37.67 78.36% 0.37 13.35\nMIRT-Router 80.67% 0.42 63.89 80.65% 0.42 38.69 80.03% 0.39 13.59\nNIRT-Router 80.69% 0.55 63.70 80.41% 0.43 38.53 79.37% 0.41 13.37\nTable 2: Testing results in Out-of-distribution scenario.\nSetting α= 0.8,β= 0.2 α= 0.5,β= 0.5 α= 0.2,β= 0.8\nPerformance ↑Total Cost ↓Reward ↑Performance ↑Total Cost ↓Reward ↑Performance ↑Total Cost ↓Reward ↑\nSmall LLM 59.83% 0.11 45.67 59.83% 0.11 28.88 59.83% 0.11 10.31\nLarge LLM 84.90% 5.30 47.92 84.90% 5.30 -7.55 84.90% 5.30 -63.02\nHybridLLM 63.34% 0.73 47.93 62.08% 0.41 27.19 63.79% 0.65 2.98\nRouteLLM 84.39% 5.25 47.70 79.90% 4.74 -4.83 75.06% 3.48 -37.58\nRouterBench 85.50% 0.26 67.42 85.75% 0.16 41.37 84.62% 0.12 15.09\nMIRT-Router 87.12% 0.14 69.17 87.12% 0.14 42.25 87.18% 0.13 15.45\nNIRT-Router 87.37% 0.15 69.32 87.24% 0.14 42.30 87.46% 0.13 15.50\nTotal Cost The total expenditure incurred for all\ntest queries (measured in USD). It is computed as:\nX\nq∈Q test[input_pricing (M∗(q))×input_tokens (q)+\noutput_pricing (M∗(q))×output_tokens (q, M∗(q))].(11)\nReward To unify the measurement, we follow\nthe normalization approach in Section 3.1; the\nTotal Cost is linearly mapped to the range [0,1].\nThe scaling factor is determined by the maximum\nobserved Total Cost. The final reward function\nbalances performance and cost:\nReward =α·Performance −β·linear (Total Cost ).(12)\n5.5 Implementation Details\nWe use bert-base-uncased2as embedding model\nfor both queries and LLMs. The k in warm-up\nmechanism is set to 5. And the Dimension Nof\nMIRT-Router and NIRT-Router are both set to 25.\nThe router is trained using the Adam optimizer\nwith a learning rate of 0.002 and a batch size of\n512. All experiments are run on 1 NVIDIA A100\n40GB GPU.\n6 Experimental Results\n6.1 Main Results\nIn-Distribution Results. From Table 1, we can\nobserve that in all three different settings, our IRT-\n2https://huggingface.co/google-bert/bert-base-uncasedRouter achieves the highest performance. The aver-\nage answer accuracy is 3% higher than when using\nGPT-4o alone, yet the Total Cost is only 1/30 of\nGPT-4o’s. When compared to using only small\nmodels, our method improves the Performance by\n32%, with a comparable Total Cost. Moreover, un-\nder all settings that balance performance and cost\n(Metric Reward), IRT-Router also achieves the opti-\nmal results. Through comparisons with IRT-Router\nand RouterBench against other baselines that only\nuse two candidate LLMs, it is clear that multi-LLM\nrouting significantly outperforms binary-LLM rout-\ning. This indicates that binary-LLM routing fails\nto fully exploit the complementary capabilities of\ndifferent LLMs. We also observe that while IRT-\nRouter outperforms RouterBench, especially in the\ncase of α= 0.8(performance priority), where it\nnot only provides better performance but also costs\nonly half of RouterBench. But RouterBench ad-\nheres more strictly to variations in α. We think\nthis is related to the measurement or definition of\nthe LLM’s fixed cost, C(Mj). Finally, we find that\nthe performance of MIRT-Router is very similar to\nNIRT-Router, with MIRT-Router slightly outper-\nforming NIRT-Router in the ID scenario.\nOut-of-Distribution Results. In the OOD sce-\nnario, IRT-Router also demonstrates strong perfor-\nmance, achieving the highest Reward. Its Perfor-\nmance is 2% higher than the best-performing base-\nline. Notably, NIRT-Router outperforms MIRT-\nRouter in this scenario, suggesting that NIRT-\n--- Page 8 ---\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24Llama3.1-8B-Instruct\nLlama3.1-70B-Instruct\nGPT-4o Mini\nGPT-4o Mini+COT-0.74 0.37 0.30 0.40 0.28 0.33 -0.64 0.31 -0.79 0.32 0.34 -0.63 -0.66 0.30 0.32 0.33 0.22 0.15 0.31 0.38 0.39 -0.73 -0.49 0.36 -0.10\n-0.73 0.43 0.36 0.44 0.37 0.38 -0.63 0.38 -0.79 0.39 0.41 -0.61 -0.59 0.33 0.34 0.40 0.24 0.25 0.35 0.43 0.44 -0.63 -0.49 0.42 0.03\n-0.85 0.48 0.40 0.49 0.41 0.43 -0.74 0.41 -0.91 0.43 0.46 -0.71 -0.65 0.35 0.37 0.44 0.26 0.28 0.39 0.47 0.49 -0.68 -0.54 0.46 0.05\n-1.09 0.61 0.50 0.62 0.53 0.54 -0.95 0.53 -1.16 0.55 0.58 -0.92 -0.83 0.43 0.45 0.57 0.30 0.34 0.49 0.60 0.63 -0.86 -0.71 0.59 0.07\n1.0\n0.5\n0.00.5\nValueFigure 4: Four LLMs’ ability values across 25 dimensions. From top to bottom, the 4 average ability values are\n0.0257 ,0.0763 ,0.0841 , and 0.0971 . Higher values indicate stronger capability.\nType: Number Theory Level: Level 2 Problem: Find the integer $n$, $0 \\le n \\le 5$, that satisfies \\[n \\equiv12345 \\pmod{6}.\\]Type: Prealgebra Level: Level 4 Problem: How many times does the digit 6 appear in the list of all integers from 1 to 100?Difficulty: -0.047Difficulty: -0.117\nFigure 5: Two queries from the MATH dataset. The\nhigher the values of Level and Difficulty, the more chal-\nlenging the query.\nRouter may have slightly better generalization abil-\nity compared to MIRT-Router. We believe this\nis due to the more complex network structure of\nNIRT-Router.\n6.2 Interpretability of IRT-Router\nLLM Ability We select two pairs of models from\nthe same series (Llama3.1-8B-Instruct vs Llama3.1-\n70B-Instruct, GPT-4o Mini vs GPT-4o Mini+COT)\nfor ability comparison. As shown in Figure 4,\nthe ability values for all four models are obtained\nfrom the trained MIRT-Router. It can be seen that\nLlama3.1-70B-Instruct is either equal to or sur-\npasses Llama3.1-8B-Instruct in each dimension,\nwhich aligns with the expected relationship be-\ntween larger and smaller models, and is consis-\ntent with their average performance on ID test set\n(71% and 32%). The other pair involves the COT-\nenhanced and non-enhanced versions. It is evident\nthat the average ability value of GPT-4o Mini+COT\nis higher than that of GPT-4o Mini.\nQuery Difficulty We randomly select 2 queries\nfrom the MATH dataset, each labeled with a ques-\ntion level. As shown in Figure 5, the query diffi-\nculty derived from MIRT-Router matches the level\nlabels.\nRouting Analysis The ability values of 20\nLLMs obtained from MIRT-Router are ranked,\nwith the top 10 being DeepSeek-Chat (81%),\nDeepSeek-Coder (81%), Gemini-1.5-Flash (73%),\nGLM-4-Plus (79%), GPT-4o (78%), GPT-4o Mini\n(71%), GPT-4o Mini+COT (72%), Llama3.1-405B-\nInstruct (78%), Qwen2.5-32B-Instruct-GPTQ-Int4\n(78%), Qwen2.5-72B-Instruct (80%), where (·)Table 3: Top- krouting accuracy of MIRT-Router.\nScenario Top-1 Top-2 Top-3 Top-4 Top-5\nID 2.72% 9.88% 32.51% 38.78% 47.85%\nOOD 2.15% 7.50% 27.29% 31.94% 39.47%\nrepresents their average performance on the ID\ntraining set. In fact, the abilities exhibited by\nthese 10 models are not significantly different,\nbut there is a large gap compared to models\nlike QwQ-32B-Preview (60%) and Llama3.1-8B-\nInstruct (32%). Among the top 10, the low-\nest cost models are DeepSeek-Chat ($0.28/M),\nDeepSeek-Coder ($0.28/M) and Qwen2.5-32B-\nInstruct-GPTQ-Int4 ($0.2/M). We sort all queries\nof the ID test set by their difficulty (obtained from\nMIRT-Router) and select the top 30% and bottom\n30% of queries. We then observe the actual assign-\nment of queries under the setting α= 0.8. We\nfind that, in the top 30%, 80% of the queries are\nrouted to DeepSeek-Chat. In the bottom 30%, 99%\nof the queries are routed to Qwen2.5-32B-Instruct-\nGPTQ-Int4. This shows that queries with higher\ndifficulty tend to be routed to models with stronger\nabilities, while queries with lower difficulty are\nrouted to slightly weaker but sufficiently capable\nand more cost-effective models. It demonstrates\nthe effectiveness and rationality of the IRT-Router.\nRouting Accuracy To further evaluate the ef-\nfectiveness of IRT-Router, we additionally assess\nhow accurately MIRT-Router routes unseen queries\nto the Top- kmost optimal LLMs. The routing\naccuracy under both in-distribution and out-of-\ndistribution scenarios (with a total of 20 candidate\nLLMs and α= 0.8) is reported in Table 3. Al-\nthough the Top-1 accuracy is relatively low, this\nis mainly due to two factors. First, the routing ob-\njective considers both performance and cost. With\nmany candidate LLMs available, several models\n(e.g., DeepSeek-Coder and DeepSeek-Chat) per-\nform similarly. And even smaller and larger mod-\nels may get comparable scores, making limiting\n--- Page 9 ---\nTable 4: Results in ID scenario when α= 0.8.\nRMSE ↓MAE ↓AUC ↑ACC ↑\nRouterBench 0.62 0.55 0.50 0.34\nMIRT-Router 0.45 0.43 0.62 0.67\nNIRT-Router 0.45 0.42 0.62 0.68\nMIRT-Router NIRT-Router69.069.269.4Reward (×102)\nAll w/o Warm-Up\nMIRT-Router NIRT-Router42.042.242.4Reward (×102)\nAll w/o Warm-Up\nFigure 6: Reward of IRT-Router (All and w/o Warm-up)\nWhen α= 0.8(left) and α= 0.5(right).\nto just the Top-1 model less meaningful. Second,\nthe current IRT-Router is intentionally lightweight,\nwith a very small number of trainable parameters.\nWith more high-quality training data and refined\nconstraints in the future, allowing IRT-Router to\nevolve and train continually, we expect its accuracy\nto improve further.\n6.3 Generalization Ability\nNew LLM We also conduct experiments on the\nnew LLM, Claude 3.5 Haiku 20241022. Here,\nwe focus on evaluating how different routers per-\nform in predicting the quality of the new LLM’s\nresponses on the ID test set. We select four met-\nrics: regression (MAE, RMSE) and classification\n(AUC, ACC). As shown in Table 4, RouterBench\nhas very low prediction accuracy, almost random,\nwhereas MIRT-Router and NIRT-Router perform\nmuch better. Nevertheless, the current IRT-Router\nstill shows limited generalization to unseen LLMs,\nwith an ACC of 0.67, indicating significant room\nfor improvement. We believe improving this aspect\nis also an important future direction, such as lever-\naging few-shot learning or similarity to warm up\nLLM cold-start.\nWarm up Query Cold-Start We conduct experi-\nments to study the effectiveness of warm-up for the\nquery cold-start. Figure 6 and 7 shows the Reward\nof MIRT-Router and NIRT-Router (with or with-\nout warm-up) on the OOD test set when α= 0.8,\nα= 0.5andα= 0.2. We find that when the warm-\nup module is removed, all rewards decrease, with\nthis effect being more pronounced in NIRT-Router.\nThis indicates that the warm-up mechanism has a\nmore significant impact on improving the perfor-\nmance of NIRT-Router.7 Conclusion\nIn this work, we introduced IRT-Router, an inter-\npretable and effective LLM router based on Item\nResponse Theory. Our method effectively achieved\nhigher performance and lower cost by selecting the\nmost suitable LLM for a given query. Through ex-\ntensive experiments on 20 LLMs and 12 datasets,\nwe found that IRT-Router outperformed multiple\nbaselines, demonstrating the superiority of our\nmethod. Additionally, our warm-up mechanism\nfor query cold-starts enhanced generalization to\nunseen queries.\nLimitations\nThe datasets currently used are common bench-\nmark datasets with ground truth labels. However,\nthe queries in these datasets are relatively short\nand do not cover the wide variety encountered in\nreal-world usage. We recognize this as a common\nchallenge in the LLM Router field. Moving for-\nward, it would be valuable to continuously gather\ndynamic data based on human preferences (Zheng\net al., 2023; Chiang et al., 2024), which could better\nreflect real-world query distributions.\nAdditionally, our router appears insufficiently\nsensitive to changes in α, suggesting that a more re-\nfined measurement approach is needed, such as in-\ncreasing the value of the LLM’s fixed cost C(Mj).\nMoreover, we have not imposed additional con-\nstraints on the relationship between query attributes\nand LLM abilities. For instance, if we assume that\nlarger models generally exhibit higher average abil-\nity levels than smaller models, we could introduce\nan ordering constraint on the average values of\ntheir learned ability vectors during training. This\nconstraint could guide the optimization process, ac-\ncelerate convergence, and lead to more reasonable\nand accurate training outcomes.\nAcknowledgments\nThis research was partially supported by the Na-\ntional Science and Technology Major Project\n(No.2022ZD0117103), the National Natural Sci-\nence Foundation of China (Grants No.62477044),\nthe Fundamental Research Funds for the Cen-\ntral Universities (No.WK2150110038), and CCF-\nNetEase ThunderFire Innovation Research Fund-\ning (NO. CCF-Netease 202306). Zhenya Huang\ngratefully acknowledges the support of the Young\nElite Scientists Sponsorship Program by CAST\n(No. 2024QNRC001).\n--- Page 10 ---\nReferences\nPranjal Aggarwal, Aman Madaan, Ankit Anand, Sriv-\nidya Pranavi Potharaju, Swaroop Mishra, Pei Zhou,\nAditya Gupta, Dheeraj Rajagopal, Karthik Kappa-\nganthu, Yiming Yang, et al. 2023. Automix: Auto-\nmatically mixing language models. arXiv preprint\narXiv:2310.12963 .\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732 .\nWeilin Cai, Juyong Jiang, Fan Wang, Jing Tang,\nSunghun Kim, and Jiayi Huang. 2024. A survey on\nmixture of experts. arXiv preprint arXiv:2407.06204 .\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nFrugalgpt: How to use large language models while\nreducing cost and improving performance. arXiv\npreprint arXiv:2305.05176 .\nShuhao Chen, Weisen Jiang, Baijiong Lin, James T\nKwok, and Yu Zhang. 2024a. Routerdc: Query-\nbased router by dual contrastive learning for as-\nsembling large language models. arXiv preprint\narXiv:2409.19886 .\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He,\nJianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\nLiu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang,\nZhaopeng Tu, Haitao Mi, and Dong Yu. 2024b. Do\nnot think that much for 2+3=? on the overthinking of\no1-like llms. Preprint , arXiv:2412.21187.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-\nsios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nHao Zhang, Banghua Zhu, Michael Jordan, Joseph E\nGonzalez, et al. 2024. Chatbot arena: An open plat-\nform for evaluating llms by human preference, 2024.\nURL https://arxiv. org/abs/2403.04132 , 2(10).\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nXiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, and John\nLui. 2024. Cost-effective online multi-llm selec-\ntion with versatile reward models. arXiv preprint\narXiv:2405.16587 .\nDujian Ding, Ankur Mallick, Chi Wang, Robert Sim,\nSubhabrata Mukherjee, Victor Ruhle, Laks VS Laksh-\nmanan, and Ahmed Hassan Awadallah. 2024. Hybrid\nllm: Cost-efficient and quality-aware query routing.\narXiv preprint arXiv:2404.14618 .Tao Feng, Yanzhen Shen, and Jiaxuan You. 2024.\nGraphrouter: A graph-based router for llm selections.\narXiv preprint arXiv:2410.03834 .\nWeibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi,\nMu-Chun Wang, Jianhui Ma, Shijin Wang, and Yu Su.\n2021. Rcd: Relation map driven cognitive diagnosis\nfor intelligent education systems. In Proceedings\nof the 44th international ACM SIGIR conference on\nresearch and development in information retrieval ,\npages 501–510.\nWeibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin,\nLinan Yue, Zheng Zhang, Rui Lv, and Shijin Wang.\n2023. Leveraging transferable knowledge concept\ngraph embedding for cold-start cognitive diagnosis.\nInProceedings of the 46th international ACM SIGIR\nconference on research and development in informa-\ntion retrieval , pages 983–992.\nMaharshi Gor, Hal Daumé III, Tianyi Zhou, and Jor-\ndan Boyd-Graber. 2024. Do great minds think\nalike? investigating human-ai complementarity in\nquestion answering with caimira. arXiv preprint\narXiv:2410.06524 .\nGauthier Guinet, Behrooz Omidvar-Tehrani, Anoop\nDeoras, and Laurent Callot. 2024. Automated\nevaluation of retrieval-augmented language models\nwith task-specific exam generation. arXiv preprint\narXiv:2405.13622 .\nSurya Narayanan Hari and Matt Thomson. 2023.\nTryage: Real-time, intelligent routing of user\nprompts to large language model. arXiv preprint\narXiv:2308.11601 .\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. Preprint , arXiv:2111.09543.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021a. Measuring massive multitask language under-\nstanding. Preprint , arXiv:2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021b. Measuring mathemati-\ncal problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874 .\nQitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang,\nBenjamin Keigwin, Gaurav Ranganath, Kurt Keutzer,\nand Shriyash Kaustubh Upadhyay. 2024. Router-\nbench: A benchmark for multi-llm routing system.\narXiv preprint arXiv:2403.12031 .\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024.\nC-eval: A multi-level multi-discipline chinese evalua-\ntion suite for foundation models. Advances in Neural\nInformation Processing Systems , 36.\n--- Page 11 ---\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented large language mod-\nels through question complexity. arXiv preprint\narXiv:2403.14403 .\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang,\nHai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. 2024. Cmmlu: Measuring massive mul-\ntitask language understanding in chinese. Preprint ,\narXiv:2306.09212.\nMingjia Li, Hong Qian, Jinglan Lv, Mengliang He, Wei\nZhang, and Aimin Zhou. 2025. Foundation model en-\nhanced derivative-free cognitive diagnosis. Frontiers\nof Computer Science , 19(1):191318.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024a.\nDeepseek-v3 technical report. arXiv preprint\narXiv:2412.19437 .\nJiayu Liu, Zhenya Huang, Tong Xiao, Jing Sha, Jinze\nWu, Qi Liu, Shijin Wang, and Enhong Chen. 2024b.\nSocraticlm: Exploring socratic personalized teaching\nwith large language models. Advances in Neural\nInformation Processing Systems , 37:85693–85721.\nQi Liu, Zheng Gong, Zhenya Huang, Chuanren Liu,\nHengshu Zhu, Zhi Li, Enhong Chen, and Hui Xiong.\n2024c. Multi-dimensional ability diagnosis for ma-\nchine learning algorithms. Science China Informa-\ntion Sciences , 67(12):1–2.\nQi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui\nXiong, Yu Su, and Guoping Hu. 2019. Ekt: Exercise-\naware knowledge tracing for student performance\nprediction. IEEE Transactions on Knowledge and\nData Engineering , 33(1):100–115.\nYang Liu, Alan Medlar, and Dorota Glowacka. 2023.\nWhat we evaluate when we evaluate recommender\nsystems: Understanding recommender systems’ per-\nformance using item response theory. In Proceedings\nof the 17th ACM Conference on Recommender Sys-\ntems, pages 658–670.\nYueyue Liu, Hongyu Zhang, Yuantian Miao, Van-Hoang\nLe, and Zhiqiang Li. 2024d. Optllm: Optimal assign-\nment of queries to large language models. arXiv\npreprint arXiv:2405.15130 .\nYunting Liu, Shreya Bhandari, and Zachary A Pardos.\n2024e. Leveraging llm-respondents for item eval-\nuation: a psychometric analysis. arXiv preprint\narXiv:2407.10899 .\nKeming Lu, Hongyi Yuan, Runji Lin, Junyang Lin,\nZheng Yuan, Chang Zhou, and Jingren Zhou. 2023.\nRouting to the expert: Efficient reward-guided en-\nsemble of large language models. arXiv preprint\narXiv:2311.08692 .Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui\nWang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu,\nChen Zhang, et al. 2025. Debate on graph: a flexible\nand reliable reasoning framework for large language\nmodels. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 39, pages 24768–\n24776.\nFernando Martínez-Plumed, David Castellano, Carlos\nMonserrat-Aranda, and José Hernández-Orallo. 2022.\nWhen ai difficulty is easy: The explanatory power of\npredicting irt difficulty. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36,\npages 7719–7727.\nFernando Martínez-Plumed, Ricardo BC Prudêncio,\nAdolfo Martínez-Usó, and José Hernández-Orallo.\n2019. Item response theory in ai: Analysing machine\nlearning classifiers at the instance level. Artificial\nintelligence , 271:18–42.\nLeland McInnes, John Healy, Steve Astels, et al. 2017.\nhdbscan: Hierarchical density based clustering. J.\nOpen Source Softw. , 2(11):205.\nLeland McInnes, John Healy, and James Melville. 2018.\nUmap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint\narXiv:1802.03426 .\nAlireza Mohammadshahi, Arshad Rafiq Shaikh, and\nMajid Yazdani. 2024. Routoo: Learning to route\nto large language models effectively. Preprint ,\narXiv:2401.13979.\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin\nChiang, Tianhao Wu, Joseph E Gonzalez, M Waleed\nKadous, and Ion Stoica. 2024. Routellm: Learning\nto route llms with preference data. arXiv preprint\narXiv:2406.18665 .\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. arXiv preprint arXiv:1806.03822 .\nGuillem Ramírez, Alexandra Birch, and Ivan Titov.\n2024. Optimising calls to large language models with\nuncertainty-based two-tier selection. arXiv preprint\narXiv:2405.02134 .\nMark D Reckase. 2009. Multidimensional item re-\nsponse theory models. In Multidimensional item\nresponse theory , pages 79–112. Springer.\nPedro Rodriguez, Joe Barrow, Alexander Miserlis\nHoyle, John P Lalor, Robin Jia, and Jordan Boyd-\nGraber. 2021. Evaluation examples are not equally in-\nformative: How should that change nlp leaderboards?\nInProceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 4486–\n4503.\n--- Page 12 ---\nMarija Šakota, Maxime Peyrard, and Robert West. 2024.\nFly-swat or cannon? cost-effective language model\nchoice via meta-modeling. In Proceedings of the\n17th ACM International Conference on Web Search\nand Data Mining , pages 606–615.\nTal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule,\nYuekai Sun, Justin Solomon, Neil Thompson, and\nMikhail Yurochkin. 2023. Large language model\nrouting with benchmark datasets. arXiv preprint\narXiv:2309.15789 .\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Tor-\nrellas, and Esha Choukse. 2024. Dynamollm: De-\nsigning llm inference clusters for performance and\nenergy efficiency. arXiv preprint arXiv:2408.00741 .\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937 .\nFei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying\nChen, Yu Yin, Zai Huang, and Shijin Wang. 2020.\nNeural cognitive diagnosis for intelligent education\nsystems. In Proceedings of the AAAI conference on\nartificial intelligence , volume 34, pages 6153–6161.\nDavid J Woodruff and Bradley A Hanson. 1996. Estima-\ntion of item response models using the em algorithm\nfor finite mixtures.\nMayi Xu, Yongqi Li, Ke Sun, and Tieyun Qian. 2024.\nAdaption-of-thought: Learning question difficulty\nimproves large language models for reasoning. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing , pages\n5468–5495.\nShangzi Xue, Zhenya Huang, Jiayu Liu, Xin Lin, Yuting\nNing, Binbin Jin, Xin Li, and Qi Liu. 2024. Decom-\npose, analyze and rethink: Solving intricate problems\nwith human-like reasoning cycle. Advances in Neu-\nral Information Processing Systems , 37:357–385.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115 .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600 .\nYixuan Zhang and Haonan Li. 2023. Can large language\nmodel comprehend ancient chinese? a preliminary\ntest on aclue. Preprint , arXiv:2310.09550.\nZheng Zhang, Qi Liu, Hao Jiang, Fei Wang, Yan\nZhuang, Le Wu, Weibo Gao, and Enhong Chen. 2023.\nFairlisa: fair user modeling with limited sensitive\nattributes information. In Proceedings of the 37th\nInternational Conference on Neural Information Pro-\ncessing Systems , pages 41432–41450.Zheng Zhang, Wei Song, Qi Liu, Qingyang Mao, Yiyan\nWang, Weibo Gao, Zhenya Huang, Shijin Wang, and\nEnhong Chen. 2024. Towards accurate and fair cog-\nnitive diagnosis via monotonic data augmentation.\nAdvances in Neural Information Processing Systems ,\n37:47767–47789.\nHongke Zhao, Likang Wu, Yuqing Shan, Zonghan Jin,\nYuanpei Sui, Zipeng Liu, Nan Feng, Minqiang Li,\nand Wei Zhang. 2024. A comprehensive survey of\nlarge language models in management: Applications,\nchallenges, and opportunities. Challenges, and Op-\nportunities (August 14, 2024) .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems , 36:46595–46623.\n--- Page 13 ---\nMIRT-Router NIRT-Router15.215.415.6Reward (×102)\nAll w/o Warm-UpFigure 7: Reward of IRT-Router (All and w/o Warm-up)\nWhen α= 0.2.\nA Experimental details\nA.1 LLM Profile\nTable 10 presents the profiles used for LLM em-\nbeddings. Here, we obtained each LLM’s pro-\nfile by utilizing ChatGPT’s web search mode with\nprompts. Of course, the final results were manually\ncorrected.\nA.2 Cadidate LLMs and their Pricing\nCadidate LLMs are categorized into four groups\n(exists overlapping):\n•API-based Models: Large models accessible\nvia API calls. For closed-source models (e.g.,\nGPT-4o), pricing is based on official API rates,\nwhile for open-source models (e.g., Llama3.1-\n405B-Instruct), we refer to pricing from Together\nAI3.\n•Deployable Models: Smaller models, such as\n7B, 8B, or quantized versions(e.g., Ministral-8B-\nInstruct-2410, Qwen2.5-32B-Instruct-GPTQ-Int4),\nwhich can be deployed locally. We standardize\ninference on a single A100 40GB GPU, with pric-\ning set at $0.2 per million output tokens, following\nTogether AI.\n•Specialized Models: Models tailored for spe-\ncific proprietary tasks (e.g., QwQ-32B-Preview,\nDeepSeek-Coder).\n•Enhanced Models: Models such as GPT-4o\nMini with Chain-of-Thought (CoT) prompting. It\nshares the same pricing as GPT-4o Mini but differs\nin prompting strategies.\nThe first 20 LLMs in Table 5 are used in the main\nexperiments, and the last LLM is for validating the\ngeneralization of IRT-Router on new LLMs.\nA.3 Datasets Details\nTable 7 shows the details of datasets. So the overall\nsize of Qtrainis24430 . And the size of Dtrainfor\ntraning the router is 24430×20 = 488600 .\n3https://www.together.ai/LLM Iutput $/1M Output $/1M\nDeepSeek-Chat 0.14 0.28\nDeepSeek-Coder 0.14 0.28\nGemini-1.5-Flash 0.075 0.3\nGLM-4-Air 0.137 0.137\nGLM-4-Flash 0.0137 0.0137\nGLM-4-Plus 6.85 6.85\nGPT-4o 2.5 10\nGPT-4o Mini 0.15 0.6\nGPT-4o Mini+COT 0.15 0.6\nLlama3.1-8B-Instruct 0.1 0.2\nLlama3.1-70B-Instruct 0.792 0.792\nLlama3.1-405B-Instruct 3.15 3.15\nMinistral-8B-Instruct-2410 0.1 0.2\nMistral-7B-Instruct-v0.2 0.1 0.2\nMixtral-8x7B-Instruct 0.54 0.54\nQwen2.5-32B-Instruct-GPTQ-Int4 0.1 0.2\nQwen2.5-7B-Instruct 0.1 0.2\nQwen2.5-72B-Instruct 1.08 1.08\nQwen2.5-Math-7B-Instruct 0.1 0.2\nQwQ-32B-Preview 1.2 1.2\nClaude 3.5 Haiku 20241022 (New) 0.8 4\nTable 5: Cadidate LLMs and their pricing. In practice,\nthe pricing can be adjusted as needed at any time.\nTable 6: Embedding Models.\nDimension Pricing\nbge-m3 1536 free\ntext-embedding-3-small 1024 $0.02 / 1M\nbert-base-uncased 768 free\nzhipu-embedding-3 512 $0.0685 / 1M\nB Sensitivity Analysis\nEmbedding Models In the previous experiments,\nwe use the bert-base-uncased as embedding model\nfor both queries and LLMs. However, there are\nnow many available embedding models, each with\ndifferent dimensions and pricing. We cannot guar-\nantee that BERT is the optimal choice for every\nscene. Therefore, we conducted experiments to\nevaluate the impact of four commonly used embed-\nding models (listed in Table 6) on the router.\nWe select four common embedding models:\ntwo are provided by large model vendors and re-\nquire a fee (OpenAI’s text-embedding-3-small and\nZhipu AI’s embedding-3), while the other two are\nwidely used pre-trained language models that can\nbe deployed independently (bge-m3 and bert-base-\nuncased). The embedding output dimensions for\nthese models are also shown in the Table 6.\nFor models that are not free, we also included the\nembedding cost in the Total Cost calculation. Our\nfindings reveal that while paid embedding mod-\nels generally offer higher performance, their as-\nsociated costs are also higher. When consider-\ning the trade-off between performance and cost,\nBERT strikes a favorable balance, yielding rela-\n--- Page 14 ---\nIn-distribution\nDataset Type Evaluation Metric Train Size Test Size\nACLUE Ancient Chinese accuracy 1400 600\nARC_C Reasoning accuracy 1400 600\nCMMLU Chinese Multitask accuracy 7000 3000\nHotpot_QA Multi-Hop EM 1400 600\nMATH Math accuracy 1400 600\nMBPP Code pass@1 630 270\nMMLU Multitask accuracy 9800 4200\nSQUAD Reading Comprehension f1 1400 600\nOut-of-distribution\nDataset Task type Evaluation Metric Train Size Test Size\nCEV AL Chinese Multitask accuracy - 1000\nCommonsense_QA Commonsense Reasoning accuracy - 1000\nGSM8K Math accuracy - 1000\nHumanEval Code pass@1 - 160\nTable 7: Datasets Details.\nTable 8: Results of MIRT-Router on ID test set when\nα= 0.8.\nPerformance ↑Total Cost ↓Reward ↑\nbge-m3 80.65% 0.47 63.47\ntext-embedding-3-small 80.99% 0.86 63.53\nbert-base-uncased 80.67% 0.42 63.89\nzhipu-embedding-3 80.78% 0.59 63.73\ntively higher rewards in our experimental setup.\nDimension N The dimension Nrepresents the\nnumber of dimensions used for modeling the abil-\nities. A larger Nindicates a finer granularity in\nthe ability modeling. But how does Naffect the\nrouter’s performance? We experiment with five\ndifferent values of Nand obtain the correspond-\ning results. Figure 8 shows that the Performance\nfluctuates with changes in N, with no consistent\nupward or downward trend. On the other hand, the\nTotal Cost tends to be higher when Nis either very\nsmall or very large, and it reaches its minimum\nwhenN= 25 .\nCold-Start Parameter λWe also conduct abla-\ntion studies on the cold-start parameter λunder\nboth ID and OOD scenarios. As shown in Table 9,\nwe vary λfrom 0 to 0.4 and report the performance,\ntotal cost, and reward. For the ID scenario, differ-\nentλvalues yield very similar results, with λ= 0.2\nor0.3achieving slightly higher reward. In contrast,\nfor the OOD scenario, where cold-start issues are\nmore pronounced, larger λvalues (e.g., 0.3or0.4)\nconsistently improve reward. This aligns with intu-\n5 15 25 35 45\nDimension \n80.580.680.780.880.981.0Performance(%)\nPerformance\n0.400.410.420.430.440.45\nTotal Cost($)\nTotal CostFigure 8: Results of MIRT-Router on ID test set when\nα= 0.8with different dimension N.\nTable 9: Impact of cold-start parameter λon MIRT-\nRouter under ID and OOD scenarios.\nλID OOD\nPerf.↑Total Cost ↓Reward ↑Perf.↑Total Cost ↓Reward ↑\n0 80.66 0.42 63.88 87.05 0.14 69.12\n0.1 80.66 0.42 63.88 87.08 0.14 69.14\n0.2 80.67 0.42 63.89 87.08 0.14 69.14\n0.3 80.67 0.42 63.89 87.12 0.14 69.17\n0.4 80.64 0.42 63.86 87.12 0.14 69.17\nition, as cold-start issues are more severe in OOD\nsettings.\nC Interpretability of NIRT-Router\nC.1 Predefined Abilities\nWe first predefine the actual meanings correspond-\ning to the Ndimensions of the ability vector\nθMj∈RNin NIRT-Router. In our experiments,\nN= 25 . Therefore, we refer to both LLM evalua-\ntion and human comment to define the 25 specific\n--- Page 15 ---\nabilities corresponding to these 25 dimensions of\nθMj. Notably, these definitions can be adjusted as\nneeded.\nThe 25 predefined specific ability are as follows:\n•0: Reasoning\n•1: Understanding\n•2: Generation\n•3: Information retrieval\n•4: Multidisciplinary knowledge\n•5: Emotion understanding and expression\n•6: Adaptability and robustness\n•7: Interactivity\n•8: Ethical and moral consideration\n•9: Mathematical calculation\n•10: Data analysis\n•11: Symbolic processing\n•12: Geometric and spatial reasoning\n•13: Programming and algorithms\n•14: Scientific knowledge application\n•15: Technical documentation understanding\n•16: Current affairs and common knowledge\n•17: Cultural understanding\n•18: Language conversion\n•19: Music and art understanding\n•20: Editing and proofreading\n•21: Prediction and hypothesis testing\n•22: Inference\n•23: Decision support\n•24: Content summarization\nC.2 Prompt for Getting Relevance Vector\nIn Section 4.2.2, we have mentioned that we use\na LLM to identify the relevant abilities by consid-\nering the abilities required by 5 sample questions\nwithin the cluster. Specifically, we use GPT-4o\nMini to complete the simple task. And the prompt\nis shown in Figure 9.\nPromptYou will be provided with the following query: {query}Identify which of the following abilities it requires from the LLM: {', '.join(abilities)}.Output the abilities as a comma-separated list.\nFigure 9: Prompt for getting relevance vector.\nC.3 LLM Ability Visualization\nFigure 10 shows four LLMs’ ability values across\n25 dimensions obtained from the trained NIRT-\nRouter. From the comparison between Llama3.1-\n0123456 7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18 1920212223240.420.440.460.480.500.520.54Llama3.1-8B-Instruct\nLlama3.1-70B-Instruct\n0123456 7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18 1920212223240.420.440.460.480.500.520.54DeepSeek-Chat\nMinistral-8B-Instruct-2410Figure 10: Four LLMs’ ability values across 25 dimen-\nsions obtained from the trained NIRT-Router.\n8B-Instruct and 70B, the larger model outperforms\nthe smaller one in almost every dimension. The\ngap is even more pronounced when comparing\nDeepSeek-Chat and Ministral-8B-Instruct-2410,\nespecially in dimensions 0: Reasoning, 1: Un-\nderstanding, 9: Mathematical calculation and 11:\nSymbolic processing, where the larger model sig-\nnificantly surpasses the smaller one. However,\nin dimension 8: Ethical and moral consideration,\nthe larger model unexpectedly under-performs the\nsmaller one. We speculate that this could be due\nto insufficient training data in this specific aspect,\nleading to inadequate learning. Another possi-\nble explanation is that the larger model’s broader\nknowledge base makes it more prone to generating\nunbounded content.\n--- Page 16 ---\nLLM Profile\nDeepSeek-Chat DeepSeek-Chat: Released by DeepSeek AI (China) in December 2024, it is an advanced conversational AI model\ndesigned to facilitate human-like interactions. Leveraging the architecture of DeepSeek V3, a 685 billion parameter\nmodel, DeepSeek-Chat offers versatile and cost-efficient performance, outperforming models like GPT-4o and Llama\n3.1. It is trained on a diverse dataset, enabling it to understand and generate human-like text across various domains.\nDeepSeek-Coder DeepSeek-Coder: Released by DeepSeek AI (China) in June 2024, it is an open-source series of code language\nmodels designed to enhance code intelligence in software development. The models range from 1.3 billion to 33\nbillion parameters and are trained from scratch on 2 trillion tokens, with a composition of 87% code and 13% natural\nlanguage in both English and Chinese. They feature a context window of 16,000 tokens and support project-level code\ncompletion and infilling. DeepSeek-Coder has demonstrated performance comparable to GPT-4 Turbo in code-specific\ntasks.\nGemini-1.5-Flash Gemini-1.5-Flash, released by Google in 2024, is a lightweight model optimized for speed and efficiency. It excels in\nhandling high-volume, high-frequency tasks such as summarization, chat applications, and data extraction from long\ndocuments and tables, making it suitable for applications requiring quick processing and scalability.\nGLM-4-Air GLM-4-Air is released on 2024-06-05 by Zhipu AI, a Chinese AI startup. Best cost-performance model, similar overall\nperformance to GLM-4, with 128k context, fast and affordable. Pre-training corpus consists of multilingual (mostly\nEnglish and Chinese) documents from a mixture of different sources.\nGLM-4-Flash GLM-4-Flash is released for free use in 2024 by Zhipu AI, a Chinese AI startup. Suitable for simple tasks, fastest\nspeed, most affordable version, with 128k context.\nGLM-4-Plus GLM-4-Plus is released for use on 2024-08-30 by Zhipu AI, a Chinese AI startup. It’s latest base Large Model of\nZhipu AI, representing a comprehensive enhancement in language understanding, instruction following, and long-\ntext processing, maintaining an internationally leading level of performance, which demonstrates powerful visual\ncapabilities comparable to OpenAI GPT-4.\nGPT-4o GPT-4o is released by OpenAI on May 13, 2024. GPT-4o is a flagship multimodal language model capable of\nprocessing text, images, and audio inputs. It features a context window of 128,000 tokens and supports up to 16,000\noutput tokens per request. The model’s training data includes diverse sources up to October 2023, enhancing its\nlanguage understanding and generation capabilities.\nGPT-4o Mini GPT-4o Mini is introduced by OpenAI on July 18, 2024. GPT-4o Mini is a streamlined, cost-effective version of\nGPT-4o. It maintains a 128,000-token context window and supports up to 16,000 output tokens per request. Designed\nfor efficiency, GPT-4o Mini offers competitive performance in reasoning, coding, and multimodal tasks, making it\nsuitable for a wide range of applications.\nGPT-4o Mini+COT **This version (GPT-4o Mini COT) is enhanced by using Chain of thoughts**. GPT-4o Mini is introduced by OpenAI\non July 18, 2024. GPT-4o Mini is a streamlined, cost-effective version of GPT-4o. It maintains a 128,000-token context\nwindow and supports up to 16,000 output tokens per request. Designed for efficiency, GPT-4o Mini offers competitive\nperformance in reasoning, coding, and multimodal tasks, making it suitable for a wide range of applications.\nLlama3.1-8B-Instruct Llama3.1-8B-Instruct: Released by Meta AI in September 2024, this 8-billion parameter instruction-tuned model is\ndesigned to excel in various natural language understanding and generation tasks. It supports a context length of up to\n128,000 tokens, enabling the processing of extensive inputs. The model has been fine-tuned on a diverse dataset to\nenhance its performance in following instructions across multiple domains.\nLlama3.1-70B-Instruct Llama3.1-70B-Instruct: Also released by Meta AI in September 2024, this 70-billion parameter instruction-tuned\nmodel offers enhanced capabilities in understanding and generating human-like text. With a context length support of\nup to 128,000 tokens, it is well-suited for complex tasks requiring deep comprehension and extended context handling.\nThe model has been fine-tuned to improve its instruction-following abilities across various applications.\nLlama3.1-405B-Instruct Llama3.1-405B-Instruct: This is the largest model in the Llama3.1 series, released by Meta AI in September 2024,\nfeaturing 405 billion parameters. It is designed to provide state-of-the-art performance in natural language processing\ntasks, with a focus on instruction-following capabilities. The model supports a context length of up to 128,000 tokens,\nmaking it suitable for highly complex and extended tasks. Fine-tuned on an extensive and diverse dataset, it aims to\ndeliver superior performance across a wide range of applications.\nMinistral-8B-Instruct-2410 Ministral-8B-Instruct-2410: Released by Mistral AI on October 16, 2024, Ministral-8B-Instruct-2410 is an 8-billion\nparameter instruction fine-tuned language model. It is designed to significantly outperform existing models of similar\nsize, offering a context window of up to 128,000 tokens. The model has been trained on a diverse dataset, including a\nsubstantial proportion of multilingual and code data, enhancing its versatility across various domains.\nMistral-7B-Instruct-v0.2 Mistral-7B-Instruct-v0.2: Released by Mistral AI on March 23, 2024, Mistral-7B-Instruct-v0.2 is an instruction-tuned\nlanguage model with 7 billion parameters. It has been fine-tuned to understand and execute specific instructions\neffectively, making it suitable for applications such as chatbots, virtual assistants, and task-oriented dialogue systems.\nThe model supports a context window of up to 32,000 tokens, enabling it to process and maintain coherence over\nlonger text sequences.\nMixtral-8x7B-Instruct Mixtral-8x7B-Instruct: Released by Mistral AI in December 2023, Mixtral-8x7B-Instruct is an ensemble model\ncomprising eight 7-billion parameter models, totaling 56 billion parameters. It is designed to deliver enhanced\nperformance across various natural language processing tasks by leveraging the strengths of multiple models. This\narchitecture allows for improved accuracy and robustness in understanding and generating human-like text.\nQwen2.5-32B-Instruct-GPTQ-Int4 Qwen2.5-32B-Instruct-GPTQ-Int4: Released by Alibaba Cloud’s Qwen team (China), this is a 32.5 billion parameter\ninstruction-tuned model, quantized to 4-bit precision using GPTQ for efficient inference. It features a context length of\nup to 131,072 tokens, facilitating long-context understanding and generation.\nQwen2.5-7B-Instruct Qwen2.5-7B-Instruct: This 7.61 billion parameter instruction-tuned model from Alibaba Cloud’s Qwen team (China)\nis designed for general-purpose language understanding and generation. It incorporates advanced architectural features\nand supports a context length of up to 131,072 tokens, enabling effective handling of long texts.\nQwen2.5-72B-Instruct Qwen2.5-72B-Instruct: Part of the Qwen2.5 series by Alibaba Cloud (China), this 72 billion parameter instruction-tuned\nmodel is tailored for complex language tasks requiring extensive understanding and generation capabilities. It benefits\nfrom a large-scale pretraining dataset and advanced architectural design, supporting long-context processing.\nQwen2.5-Math-7B-Instruct Qwen2.5-Math-7B-Instruct: This 7 billion parameter instruction-tuned model from Alibaba Cloud’s Qwen team is\ndesigned specifically for mathematical problem-solving and reasoning tasks. It is trained on a specialized mathematical\ndataset to enhance its proficiency in handling complex mathematical queries.\nQwQ-32B-Preview QwQ-32B-Preview is an experimental research model developed by the Qwen Team (China), released in November\n2024. It focuses on advancing AI reasoning capabilities, particularly in mathematics and programming. The model\nhas 32.5 billion parameters and a context window of 32,768 tokens. It employs a dense transformer architecture\nwith advanced components such as Rotary Position Embedding (RoPE), SwiGLU activation functions, RMSNorm\nnormalization, and Attention QKV bias.\nClaude 3.5 Haiku 20241022 (New) Claude 3.5 Haiku 20241022 is a large language model developed by Anthropic, released on October 22, 2024. It\nfeatures advanced natural language understanding and generation capabilities, supports multiple languages, and is\ndesigned to provide efficient and accurate text processing for a wide range of applications, including customer service,\ncontent creation, and educational tools.\nTable 10: LLMs’ profiles. The first 20 LLMs are used in the main experiments, and the last LLM is for validating\nthe generalization of IRT-Router on new LLMs.",
  "text_length": 69619
}