{
  "id": "http://arxiv.org/abs/2506.04041v1",
  "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events",
  "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.",
  "authors": [
    "Claire Barale",
    "Leslie Barrett",
    "Vikram Sunil Bajaj",
    "Michael Rovatsos"
  ],
  "published": "2025-06-04T15:06:27Z",
  "updated": "2025-06-04T15:06:27Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04041v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04041v1  [cs.CL]  4 Jun 2025LEXTIME: A Benchmark for Temporal Ordering of Legal Events\nClaire Barale1Leslie Barrett2Vikram Sunil Bajaj2Michael Rovatsos1\n1School of Informatics, University of Edinburgh2Bloomberg\nclaire.barale@ed.ac.uk\nAbstract\nTemporal reasoning in legal texts is important\nfor applications like case law analysis and com-\npliance monitoring. However, existing datasets\nlack expert language evaluation, leaving a gap\nin understanding how LLMs manage event or-\ndering in legal contexts. We introduce LEX-\nTIME, the first dataset designed to evaluate\nLLMs’ event ordering capabilities in legal lan-\nguage, consisting of 512 instances from U.S.\nFederal Complaints with annotated event pairs\nand their temporal relations. Our findings show\nthat (1) LLMs are more accurate on legal event\nordering than on narrative (up to +10.5%); (2)\nlonger input contexts and implicit events boost\naccuracy, reaching 80.8% for implicit-explicit\nevent pairs; (3) legal linguistic complexities\nand nested clauses remain a challenge. We\ninvestigate how context length, explicit vs im-\nplicit event pairs, and legal language features\naffect model performance, demonstrating the\nneed for specific modeling strategies to en-\nhance temporal event reasoning. The code\nandLEXTIMEdataset are available at: https:\n//github.com/clairebarale/LexTime\n1 Introduction\nUnderstanding the temporal relationships between\nevents is fundamental for natural language under-\nstanding. In the legal domain, temporal reasoning\nis important for tasks such as case law analysis,\ncontract interpretation, and compliance monitoring.\nAccurately determining the timeline of events helps\nassess liability, obligations, and procedural validity.\nHowever, the unique linguistic characteristics of\nlegal texts raise questions about the effectiveness\nof current temporal reasoning approaches.\nLanguage models (LMs) face notable challenges\nwith temporal reasoning due to its complex logic,\nambiguities, and the necessity to process long con-\ntextual dependencies (Jain et al., 2023). While\nseveral benchmarks exist for temporal reasoning\nwith large language models (LLMs), none focus\nFigure 1: Overview of the task of legal event ordering\nin L EXTIME\non expert languages. Much current research on\nevent extraction and ordering focuses on narratives,\nsuch as the EVENT STORY LINECORPUS (Caselli\net al., 2018), the FINEGRAINED TEMPORAL RE-\nLATIONS DATASET (Vashishtha et al., 2019), and\ntheTRACIE dataset (Zhou et al., 2021a). As a re-\nsult, LMs’ proficiency on expert types of language,\nsuch as legal language, is largely unknown. How\ndo LMs address the unique features of legal lan-\nguage for event ordering?\nIn this work, we conduct the first study tar-\ngeting temporal event ordering within legal lan-\nguage. Besides its implications for legal work,\nevent ordering is a foundation for downstream legal\nNLP tasks, such as summarization, drafting, and\nquestion-answering. We start by assembling LEX-\nTIME, a novel dataset composed of 512 instances\nconstructed from labor-related U.S. Federal Com-\nplaints (Sec. 2). We extract events relevant to both\nthe case and its procedural timeline, providing a\nstructured resource for studying temporal reasoning\nin legal texts. An example instance from LEXTIME\nis shown in Fig. 1.\nWe show that legal language used in LEXTIME\nexhibits specific lexical, syntactic, and discourse\n1\n--- Page 2 ---\ncharacteristics, making it a distinct genre (Sec. 3.\nSince event temporal ordering has been extensively\nstudied in narrative contexts, we use TRACIE as a\nbaseline for comparison due to its similarity in task\ndesign and focus on event ordering.\nNext, we conduct a systematic evaluation of\nLMs’ ability to perform event ordering, defined as\nidentifying temporal relations between event pairs\n(Sec. 4, 5). Our analysis focuses on key aspects\nrelevant to the legal domain, such as the impact\nof context length, the distinction between explicit\nand implicit events, and a comparative study of\nnarrative versus legal language. An implicit event\nis an event whose trigger is not explicitly stated in\nthe text but can be inferred from the surrounding\ncontext such as negotiation in Figure 1.\nOur experiments assess Chain-of-Thought (CoT)\nprompting (Wei et al., 2024), a method previously\nsuccessful in reasoning over complex and long in-\nputs. However, we find that CoT prompting is\nineffective for event ordering, leading to a 3.6%\naccuracy drop for LEXTIMEcompared to standard\nfew-shot prompting which yields the best accuracy.\nOverall, we have three core findings: (1) LLMs\nshow improved performance on legal event order-\ning in L EXTIMEcompared to narrative, achieving\nenhancements of up to +10.5% (GPT-4o, few-shot).\nThis shows that legal texts offer clearer temporal\nstructures for models to utilize. (2) LLMs perform\nbetter with longer input contexts (+2.3% for GPT-4\nTurbo, few-shot) and with event pairs that include\nimplicit types. The highest accuracy of 80.8% is\nachieved using pairs with explicit-implicit events\n(GPT-4 Turbo, few-shot), surpassing pairs of two\nexplicit events by 2.6% and the overall dataset av-\nerage by 3.2% (Sec. 5). (3) However, linguistic\ncomplexities specific to legal language contribute\nto model errors (Sec. 6). Features typical of the\nlegal genre are overrepresented in misclassified\ninstances, such as paraphrases (+89% in error sam-\nples) and events occurring in subordinate clauses\n(+84%).\nIn sum, we make the following contributions:\n1.We introduce LEXTIME, the first dataset curated\nfor legal event ordering, enabling the systematic\nstudy of temporal reasoning in legal texts.\n2.We use LEXTIME to analyze the abilities of\nlarge LLMs on legal event ordering, a funda-\nmental task in legal NLP. Our findings reveal\nthat while LLMs achieve higher accuracy on le-\ngal texts than narrative texts, their performanceis still constrained by linguistic complexities\nunique to legal language.\n3.We identify effective strategies for improving\nmodel accuracy, showing that longer context\nwindows enhance accuracy for large models,\nwhile shorter context windows benefit smaller\nmodels, providing practical insights for optimiz-\ning model configurations in legal NLP tasks.\n2 Datasets and Experimental Design\nWe create a dataset focused on temporal reasoning\nin legal language, derived from labor-related U.S.\nFederal Complaints. For comparison, we use TRA-\nCIE, which consists of short stories and has been\ncurated for the same task, offering a natural choice.\n2.1 Task Overview and L EXTIME Dataset\nLEXTIMEis a dataset built to measure the ability\nof LMs to perform temporal event ordering on a\nspecific type of language: legal language. Each\ninstance contains: (1) a context paragraph taken\nfrom a complaint document (2) a pair of events,\nwhich can be either one implicit event and one\nexplicit event or two explicit events. An event\nis an occurrence or action triggered by a verb or\nnoun that takes place at a specific time and can be\nordered relative to other events based on temporal\nrelationships. (3) a query consisting of a pair of\nevents and a temporal relationship between them,\ncreating a triplet following this template: {event\nA, TR, event B} (4) a binary label, yesorno. For\ninstance, in the positive instance shown in Fig. 1,\nentering a settlement agreement andCompanyX\nappealed are two explicit events taken from the\ncontext paragraph ,follows is the TRthat links them\nin the query , and the answer ( label ) to the query\naccording to the context isYes.\nPre-Labelling We use Mistral-Large-Instruct-\n2407 to help construct instances and then manu-\nally review all generated instances. We employ a\nprompt-chaining approach to minimize errors that\nmay occur during the multiple steps involved (Wu\net al., 2022).\nWe filter paragraphs with at least two -edto-\nkens (event triggers) and merge every three for\ncontext, averaging 172 tokens. Next, we extract\nexplicitly mentioned events using an initial prompt.\nWe manually review these to select the most sig-\nnificant case or procedural timeline events. Inter-\nmediate cases, where temporal relations are am-\n2\n--- Page 3 ---\nRepr esentation Allen's Relations LexT ime's Relations\nBefore, MeetsPrecedes, Happens before,\nStarts before, Arises before, Is\nfollowed by\nOverlaps, Finished by ,\nContains, Starts, Equals,\nStarted by , During,\nOverlapped byIs simultaneous, Happens\nsimultaneously\nFinishes, Met by , AfterFollows, Happens after , Starts\nafter, Comes afterFigure 2: LEXTIME’s temporal relations and their cor-\nrespondence to Allen’s interval relations (Allen, 1983).\nEvent A green, while Event B is in yellow.\nbiguous, are resolved during this annotation stage\nthrough manual review. We then identify five im-\nplicit events per paragraph and generate sentences\nlinking events with temporal relationships (TR),\nindicating whether one event occurs before, at the\nsame time, or after another. All possible TR vari-\nations are provided in the last column of Figure 2.\nEach TR is reversed to create a contradictory query\n(e.g., happens after tohappens before ).\nFig. 2 compares the TR used in LEXTIMEwith\nAllen’s relations (Allen, 1983), which are foun-\ndational to temporal reasoning. We simplify the\nTR to three categories for practical and theoretical\nreasons: (1) Strict event order: in legal reasoning,\nimportant TRs primarily depend on precedence\nand causality (e.g., The lawsuit was filed before the\nhearing ), and finer-grained relations like Meets are\nless relevant; (2) Ambiguous Boundaries: Allen’s\nrelations need precise interval boundaries, which\nare often vague. Simplifying these TRs improves\ninterpretability, consistency, and reduces annota-\ntion errors; (3) Consistency with TRACIE : we em-\nploy three key relationships to ensure alignment\nand facilitate comparison with (Zhou et al., 2021a).\nLabel Collection We manually review the 512\npre-labelled instances and correct their labels if\nneeded. In total, we found that 23.24% of the sam-\nples were wrongly labelled. Among those errors,\n54.55% were an error on the temporal relationship,\n37.87% on the event extraction, and 7.58% on both.\nWe collect a balanced dataset of 257 instances la-\nbelled yes(TR is correct) and 255 instances la-\nbelled no(TR is incorrect).\n2.2 T RACIE Dataset\nWe randomly select 512 instances from the TRACIE\ndataset (Zhou et al., 2021a) to match the number\nof samples in LEXTIME.TRACIE contains short\nstory paragraphs and implicit events. We modify\nthe original TRACIE dataset to match the formatofLEXTIME by converting instances to a query\ninstead of the original entailment task format. We\nrerun all experiments as previous benchmarks using\nTRACIE have employed older models (Chu et al.,\n2024).\n3 Specific Features of Legal Events\nThis section examines the linguistic and structural\ncharacteristics that differentiate legal language\nfrom narrative, using LEXTIME andTRACIE as\nrepresentative datasets. We show that legal lan-\nguage constitutes a distinct genre characterized by\nspecialized event structures and temporal represen-\ntations.\nPrior work has shown that legal language fol-\nlows distinct linguistic conventions (Yunus et al.,\n2016), including lengthy nominalized expressions,\nfrequent passive forms, and specialized vocabu-\nlary, making its syntactic structure more complex\nthan that of general prose or news texts (Kuzman\nand Ljubeši ´c, 2023). In an empirical study of the\ntext in the US Code, (Martinez et al., 2024) found\nhigher rates of complex syntactic structures relative\nto six baseline genres of English. Similarly, Mar-\ntinez et al. (2022) noted that US case law contains\ncenter-embedded clauses at double the rate of other\ntext genres. Guided by the features identified in\n(Yunus et al., 2016), we divide our analysis into:\n(1) a lexical analysis and (2) a syntax and discourse\nanalysis (Fig. 3).\nWe employ a combination of methods, each se-\nlected for its suitability to a specific feature of inter-\nest: (1) named-entity recognition to extract dates;\n(2) a set of rules and keyword search to extract tem-\nporal markers (Appendix A.2); (3) part-of-speech\nand dependency parsing to extract modal verbs, the\ntense of the verbs, negations; (4) and further the\ndependency parsing with manual annotation for\nnominalized events, passive voice, citations, subor-\ndinate and conditional clauses.\n3.1 Vocabulary\nDomain-specific event triggers – An event trig-\nger is a verb or noun that signals an event, such\nasfiled infiled a lawsuit . InLEXTIME, event trig-\ngers are mostly legal terms (Fig. 4), and they vary\nsignificantly from those in TRACIE . None of the\n10 most frequent event triggers overlap between\nthe datasets, and we compute a low lexical over-\nlap of 0.09 using all unique event triggers found in\neach dataset (Jaccard similarity). TRACIE shows\n3\n--- Page 4 ---\nDate Mentions Temporal CuesModalsNegations01020304050\n37\n24\n15\n6\n320\n42Lexical\nSubordinate Clauses Conditional ClausesNoun EventsPassivesParaphrases\nNon-Past EventsFuture Speculative01020304050\n16\n1052\n28\n735\n96\n2 23012\n5Syntax and Discourse\nLexTime\nTracieFigure 3: Comparison of linguistic features in LEXTIMEandTRACIE . Values (y-axis) represent the percentage of\nsentences that contain each feature, indicating its frequency in the text.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0ﬁled\nengaged\nadvised\nagreed\nconcerted\namended\npick\nstore\nposted\nrequested\nFigure 4: Top-10 triggers in LEXTIME by frequency\n(%).\na higher diversity of event triggers with a ratio of\n0.148 and 701 unique triggers, compared to 0.035\nand 403 unique triggers for LEXTIME(unique trig-\ngers divided by the total number of triggers).\nMany terms in LEXTIMEhave specific meanings\nthat differ from their general usage (semantic shift).\nFor example, charged in legal terms means a formal\naccusation, while in everyday language, it can refer\ntocharging a phone . In contrast, TRACIE includes\nevent triggers that often describe common actions\n(e.g., started, asked ).\nPrecise and explicit temporal cues – LEXTIME\ncontains 1028% more date mentions than TRACIE ,\nreflecting reliance on precise and explicit temporal\nexpressions (e.g., on June 4, 2021 ) and statutory\ndeadlines (e.g., within 30 days of filing ). In contrast,\nnarrative texts use vague temporal expressions, as\nevidenced by fewer dates but a higher occurrence\nof frequency adverbs (+2.43%, Appendix A.3).\nHigher density of temporal markers – LEX-\nTIMEcontains a higher density of temporal markers\nthan TRACIE (+3.99%), highlighting a structural\nand functional difference in the representation of\nevents (Fig. 3).\nFrequent use of modalities – LEXTIME has\n11% more modal verbs than TRACIE , except for\ncould , which shows a decrease of -1.96% (Ap-\npendix A.4). The use of modalities in legal writing\nindicates binding obligations ( must, shall ), permis-sions ( may, can ), and advisory language ( should ).\nIn contrast, narratives employ modality to express\nuncertainty ( could ).\nFrequent use of negation – Negation appears\n4.73% more in LEXTIME than in TRACIE . This\nreflects its structural and functional role in legal rea-\nsoning (restrictions, prohibitions, and exceptions).\nLEXTIMEincludes double negatives, characteristic\nof legal discourse. For example: A decision shall\nnot be invalid solely because of a procedural error .\n3.2 Syntax, Discourse, and Tenses\nEvents in subordinate or conditional clauses –\nSubordinate and conditional clause events are more\nfrequent in LEXTIMEthan in TRACIE (+10.5% and\n+9.56%). This reflects complex hierarchical depen-\ndencies, where an event’s occurrence can depend\non another event or condition.\nSentence length and event density – LEXTIME\nhas longer sentences than TRACIE (Fig. 5), averag-\ning 34.72 vs. 8.86 tokens, resulting in higher event\ndensity (4.01 vs. 1.60 events per sentence).\n0 10 20 30 40 50 60 70LexTime\nTracieQ1: 20Med: 28\nQ3: 41\nQ1: 7Med: 9\nQ3: 11\nFigure 5: Boxplot showing sentence length distributions\n(in tokens). Median (Med) and quartiles (Q1, Q3) are\nnoted, outliers are excluded for clarity.\nNominalization, passive voice, and paraphras-\ning – Legal writing often uses nouns instead of\nverbs to express events, for instance: The revoca-\ntion of the contract was issued yesterday . Nominal-\nized events appear in 52.50% of sentences (+50.4%\ncompared to TRACIE ). Legal texts prioritize nomi-\nnalization and passive constructions to emphasize\noutcomes and obligations. Legal texts use the\n4\n--- Page 5 ---\npassive voice (+24.76%), and paraphrase reported\nevents (+6.56%) more frequently, enhancing their\nimpersonal and authoritative tone. This creates im-\nplicit event structures by omitting the agent. For\nexample, in The revocation of the contract , the ac-\ntor is unstated, making it difficult to identify who\nacted and when.\nCitations – Legal texts frequently include ci-\ntations (23.68% of sentences), anchoring argu-\nments through references to past court decisions\nand statutes, often encoding past events.\nBroader range of verb tenses and speculative\nevents – InTRACIE , most events appear in the\npast simple (87.62%), while LEXTIMEshows more\ntense variation, with 24.24% in the present simple\nand 5.57% in the present perfect (Appendix A.5).\nAdditionally, speculative events are more common\ninLEXTIME(9.83%, +4.1%). Tense variation af-\nfects temporal reasoning, as some tenses signal\nrules and precedents. For instance, the present per-\nfect marks past events with ongoing relevance (e.g.,\nThe plaintiff has submitted an appeal ).\n4 Benchmark Methodology\nWe conduct evaluations using two different prompt-\nbased approaches: standard prompting and chain-\nof-thought (CoT) prompting. We perform exper-\niments in zero( ZS), one (1S), and few-shot (FS)\nsettings (3 examples). Prompts and examples can\nbe found in Appendix D.\nStandard Prompting – In a ZS setting, the\nmodel is prompted to answer a query containing\nthe triplet {event A, TR, event B} based on a con-\ntext paragraph. In 1S and FS settings, models are\nprovided with respectively one and three examples\n(context paragraphs and question-answer pairs).\nCoT Prompting – Instructions for CoT prompts\nare the same as those for standard prompting. CoT\nprompting guides a LM to reason through steps\nbefore giving a final answer. We don’t use CoT in\na ZS setting, as we consider examples necessary.\nWe manually annotate up to three examples for\nstep-by-step reasoning in 1S and FS settings.\nEvaluation We evaluate our benchmark using ac-\ncuracy. The evaluation compares long vs. short in-\nputs to reflect a key characteristic of legal language\n(sentence length), which captures finer linguistic\ncomplexities such as conditional clauses, subordi-\nnation, and event density. There are 230 paragraphswith fewer than 150 tokens (short) and 282 para-\ngraphs with more than 150 tokens (long). Inspired\nbyTRACIE and based on our linguistic analysis\nand which found that legal texts frequently utilize\nnon-agentive structures, we address the importance\nof implicit event reasoning in legal NLP. We eval-\nuated the influence of implicit events considering\n210 explicit-implicit pairs and 288 explicit-explicit\npairs (and 14 implicit-implicit pairs).\nModels and Experimental Setup We evaluate\na range of popular LMs, including both open-\nsource and proprietary models (Appendix B), span-\nning from smaller models (LLaMA 3.2 1B, Flan-\nT5780M) to larger ones (GPT-4). Proprietary\nmodels are accessed through their APIs, non-\nproprietary models are accessed through Hugging-\nFace and utilized in lower precision formats (16-bit\nfloating-point and 4-bit quantization).\n5 Experimental Results\nGPT-4o, GPT-4 Turbo and Mistral outperform\nLLaMA and FLAN models across all evaluated\nprompts and settings.\nAcross the five studied subsets, GPT-4 Turbo\nachieves the highest accuracy in four cases (up\nto 80.8% accuracy for explicit-implicit pairs of\nevents), including three in a FS setting and one in\na 1S CoT setting (all data). The best accuracy for\nthe short-context subset is obtained by Mistral 123B\nin a FS setting with 77.8%. Overall, CoT appears\namong the top three results only three times across\nall categories (all data, long context, and explicit-\nimplicit), suggesting its limited impact on perfor-\nmance in this task.\nCompared to GPT-4o, Mistral 123Bexhibits\nlower accuracy across all prompts, with perfor-\nmance differences ranging from 0.2% to 8%.\nLLaMA 3.1 70Balso shows reduced accuracy, with\na performance gap between 0.7% and 5.5%. Flan-\nT5780Mdemonstrates a significant decline, with\naccuracy differences ranging from 10.4% to 21.9%.\nWhen considering the other LLaMA models collec-\ntively, their accuracy is lower by 20.2% to 28.5%.\nCompared to GPT-4 Turbo, Mistral 123Bshows\nlower accuracy across all prompts, with differences\nranging from 1.9% to 9.8%. LLaMA 3.1 70Bex-\nhibits a decline in accuracy ranging from 4% to\n8.3%, while Flan-T5 780Mshows a more significant\ndrop, with differences between 13% and 21.7%.\n5\n--- Page 6 ---\nModelSubsetAll Long Context Short Context Explicit-Explicit Explicit-Implicit\nZS 1S FS ZS 1S FS ZS 1S FS ZS 1S FS ZS 1S FS\nGPT-4o 69.9 74.9 77.4 70.3 74.6 77.4 71.6 71.2 77.0 68.8 74.8 76.8 71.5 76.9 79.0\n+CoT - 68.6 72.6 - 69.0 72.6 - 68.7 73.2 - 69.3 72.7 - 69.2 74.0\nGPT-4 Turbo 70.2 75.1 77.2 70.1 76.6 79.9 70.1 73.5 76.4 71.0 76.6 78.2 71.1 74.3 80.8\n+CoT - 77.6 74.3 - 77.7 71.3 - 70.7 75.2 - 76.0 73.1 - 78.7 75.0\nMistral 123B 61.9 70.1 73.9 62.2 69.6 70.8 63.3 71.6 77.8 63.5 71.5 74.7 62.6 69.6 73.6\n+CoT - 67.8 72.4 - 62.3 70.1 - 69.6 74.6 - 65.5 72.0 - 68.1 71.2\nLLaMA 3.1 70B 66.1 71.3 71.9 68.8 70.3 73.9 67.8 71.7 73.3 65.1 70.9 75.3 68.7 73.0 69.7\n+CoT - 69.3 66.6 - 71.1 66.3 - 72.0 72.8 - 76.1 75.5 - 69.7 70.2\nLLaMA 3.1 8B 51.9 60.5 55.9 51.2 59.1 51.1 51.7 59.1 58.4 57.5 56.7 53.7 52.6 62.6 57.3\n+CoT - 48.4 54.6 - 49.9 55.1 - 47.0 56.1 - 49.7 51.6 - 46.7 60.8\nLLaMA 3.1 8B(Base) 49.6 50.5 50.2 50.4 51.7 50.0 49.3 53.6 50.3 49.6 50.8 50.1 47.6 48.2 48.8\n+CoT - 52.3 50.8 - 49.9 53.2 - 52.6 50.9 - 53.0 51.9 - 51.7 52.4\nLLaMA 3.2 3B 52.2 53.5 51.0 51.7 52.3 47.2 55.9 55.1 51.0 54.7 51.2 51.3 53.5 52.6 51.7\n+CoT - 53.8 57.2 - 53.7 57.1 - 53.0 55.9 - 53.6 59.5 - 49.9 56.5\nLLaMA 3.2 3B(Base) 50.6 52.3 52.9 50.0 50.0 46.8 45.7 49.6 53.0 49.8 54.9 49.5 49.3 45.8 49.8\n+CoT - 52.9 48.8 - 53.5 49.3 - 55.2 54.3 - 51.6 52.4 - 50.2 50.2\nLLaMA 3.2 1B 49.9 47.7 48.9 48.7 48.6 51.1 51.6 48.8 51.2 49.6 48.4 48.7 49.8 46.7 52.3\n+CoT - 49.9 50.1 - 45.3 52.1 - 45.7 48.7 - 46.7 50.2 - 47.6 49.0\nLLaMA 3.2 1B(Base) 48.8 54.9 49.0 46.1 53.2 50.0 51.7 50.9 54.8 52.7 50.5 45.1 48.5 49.8 46.7\n+CoT - 50.2 51.0 - 51.8 52.5 - 47.4 51.7 - 51.3 50.5 - 50.2 49.3\nFlan-T5 780M 57.2 56.3 55.5 55.3 54.6 53.5 57.4 58.3 57.8 55.3 54.5 52.4 60.4 59.0 59.9\n+CoT - 58.2 55.9 - 56.0 54.3 - 59.6 57.8 - 55.3 54.2 - 61.4 59.0\nTable 1: LEXTIMEexperimental results (accuracy) under zero-shot (ZS), one-shot (1S), and few-shot (FS) settings\nfor standard and CoT prompting. All results are averaged on three runs. A gradient-based color scheme highlights\nhigher accuracy scores, with darker shades indicating better performance.\nThe other LLaMA models collectively show accu-\nracy reductions ranging from 21.4% to 29.2%.\n5.1 Chain-of-Thought in Event Ordering\nLexTime Tracie02040608072.8\n64.870.864.275.167.671.5\n63.4\n1S\n1S CoT\nFS\nFS CoT\nFigure 6: Performance gap (accuracy) with and without\nCoT prompting. Results are averaged from GPT-4, GPT-\n4 Turbo, LLaMA 3.1 70Band Mistral 123B.\nCoT prompting does not improve accuracy for event\nordering.\nPrior research has demonstrated that CoT\nprompting can improve a model’s reasoning skills\n(Wei et al., 2024). As illustrated in Fig. 6, CoT\nprompting does not improve performance for LEX-\nTIMEorTRACIE . The best results are achieved us-\ning a FS approach with a standard prompt (75.1%).\nEmploying CoT in FS reduces performance by\n3.6%. In a 1S setting, we also observe a decrease\nin accuracy of 2%. Similar results are seen with the\nTRACIE dataset. Table 1 shows that these findings\nhold true regardless of context length, and implicit\nevents. This indicates that incorporating eliciting\nreasoning steps did not improve the model’s capa-\nbility to retrieve temporal relationships, possiblydue to the increased length of the prompt when\nincluding reasoning for multiple examples.\n5.2 Impact of Context Length\nLarger LMs benefit from longer contexts, achieving\na 2.3% improvement over the entire dataset. Smaller\nLMs perform better with shorter context windows.\nGPT-4 Turbo achieves the highest accuracy in\nlong-context settings (79.9%), while Mistral 123B\nperforms best in short-context settings (77.8%),\nboth with a FS prompting setup.\nFor GPT-4o, GPT-4 Turbo, and LLaMA 3.1 70B,\nincreasing the input context length leads to im-\nproved performance, with GPT-4 Turbo achieving\nup to a 3.5% gain on a FS setting. In contrast,\nall other evaluated models exhibit higher accuracy\nwith shorter context lengths (below 150 tokens),\nwith Mistral 123Bdemonstrating a 7% improvement\nand LLaMA 3.1 8Bachieving up to a 7.3% increase.\nThis observation suggests that larger models, such\nas GPT-4 Turbo, leverage the additional contex-\ntual information to capture temporal structures and\nevent relationships, ultimately leading to more ac-\ncurate predictions. In settings other than FS, we\nobserve that shorter context lengths also enhance\nperformance. Specifically, GPT-4 Turbo achieves a\n3.9% improvement with shorter context windows\nin the CoT few-shot setting, while LLaMA 3.1 70B\ndemonstrates a performance gain of 6.5%.\n6\n--- Page 7 ---\n5.3 Impact of Implicit Events\nGPT-4 Turbo achieves the highest overall accuracy at\n80.8% for explicit-implicit pairs, outperforming the\nbest result on the full dataset by +3.2%.\nGPT-4 Turbo achieves the highest accuracy\nacross both subsets (78.8% and 80.8%, FS). Sev-\neral hypotheses could explain this: (1) implicit\nevents may force the model to rely on a broader\ncontext, and (2) explicit-explicit event pairs may\nintroduce ambiguity, making it more challenging to\ndetermine precise temporal relationships. This ob-\nservation aligns with previous findings that GPT-4\nmodels benefit from longer context windows.\nGPT-4o, GPT-4 Turbo, and Flan-T5 780Myield\nbetter accuracy on explicit-implicit event pairs, im-\nproving by +2.2%, +2.6%, and +7.5%, respectively,\nin their FS settings. This suggests that these models\nare capable of inferring missing temporal informa-\ntion. In contrast, Mistral 123Band LLaMA 3.1 70B\nperform better on explicit-explicit pairs, achieving\nimprovements of +1.1% and +5.6% in their FS set-\ntings. This suggests that these models rely on direct\nlexical and temporal cues.\nWhen considering all LLaMA models except\nLLaMA 3.1 70B, base models exhibit a larger per-\nformance improvement on explicit-explicit pairs\n(+1.68%) compared to explicit-implicit pairs, sur-\npassing the increase observed in aligned models\n(+0.91%) — on average across all prompt types.\nOverall, for the aligned LLaMA models (exclud-\ning 70B), the average accuracy for explicit-implicit\npairs is 52.64%, compared to 49.23% for base mod-\nels. This suggests that alignment enhances implicit\nreasoning capabilities.\n5.4 Comparison with T RACIE\nThe highest accuracy achieved on TRACIE at 73.1%\n(GPT-4 Turbo) is 4.2% lower than the highest accu-\nracy on L EXTIME.\nGPT-4o, LLaMA 3.1 70B, and FLAN-T5 con-\nsistently perform better on LEXTIME across all\nprompt settings, with improvements of up to 10.5%\n(GPT-4o, FS). GPT-4 Turbo and Mistral 123Bgen-\nerally perform better on LEXTIME, except in the\nZS setting. For the smaller LLaMA models (all\nexcept 70B), LEXTIMEoutperforms TRACIE by\nup to 5.6% in the 1S setting (LLaMA 3.1 8B). How-\never, in the 1S CoT, TRACIE surpasses LEXTIME\nby 6.2%, indicating that task performance varies\ndepending on the prompting strategy.ZS 1S FS\nGPT-4o 67.0 ( ↓2.9) 67.4 ( ↓7.5) 66.9 ( ↓10.5)\n+CoT - 61.8 ( ↓6.8) 64.0 ( ↓8.6)\nGPT-4 Turbo 70.8 ( ↑0.6) 66.5 ( ↓8.6) 73.1 (↓4.1)\n+CoT - 73.6 (↓4.0) 65.9 ( ↓8.4)\nMistral 123B 63.9 ( ↑2) 61.1 ( ↓9) 64.6 ( ↓9.3)\n+CoT - 58.4 ( ↓9.4) 72.4 (=0.0)\nLLaMA 3.1 70B 63.0 ( ↓3.1) 64.3 ( ↓7.0) 65.9 ( ↓6.0)\n+CoT - 63.1 ( ↓6.2) 66.0 ( ↓0.6)\nLLaMA 3.1 8B 53.5 ( ↑1.6) 54.9 ( ↓5.6) 52.7 ( ↓3.2)\n+CoT - 54.6 ( ↑6.2) 56.4 ( ↑1.8)\nLLaMA 3.1 8B(Base) 50.0 ( ↑0.4) 48.9 ( ↓1.6) 50.1 ( ↓0.1)\n+CoT - 49.5 ( ↓2.8) 51.8 ( ↑1.0)\nLLaMA 3.2 3B 53.4 ( ↑1.2) 53.5 ( =0.0) 53.5 ( ↑2.5)\n+CoT - 51.6 ( ↓2.2) 51.2 ( ↓6.0)\nLLaMA 3.2 3B(Base) 49.7 ( ↓0.9) 52.1 ( ↓0.2) 51.1 ( ↓1.8)\n+CoT - 51.3 ( ↓1.6) 50.9 ( ↑2.1)\nLLaMA 3.2 1B 50.3 ( ↑0.9) 50.2 ( ↑2.5) 51.1 ( ↑2.2)\n+CoT - 49.1 ( ↓0.8) 49.2 ( ↓0.9)\nLLaMA 3.2 1B(Base) 47.1 ( ↓1.7) 49.8 ( ↓5.1) 50.3 ( ↑1.3)\n+CoT - 50.6 ( ↑0.4) 51.4 ( ↑0.4)\nFlan-T5 780M 53.9 ( ↓3.3) 53.9 ( ↓2.4) 54.3 ( ↓1.2)\n+CoT - 54.6 ( ↓3.6) 54.9 ( ↓1.0)\nTable 2: TRACIE experimental results (accuracy) under\nzero-shot (ZS), one-shot (1S), and few-shot (FS) set-\ntings for standard prompting and CoT. Top 3 results are\nin bold. Values in parentheses indicate the absolute dif-\nference between LEXTIMEand TRACIE .↑signifies that\naccuracy is higher on TRACIE compared to LEXTIME.\n↓indicates that accuracy is lower on TRACIE .\nThe accuracy difference suggests distinct chal-\nlenges between legal and narrative event ordering.\nOne possible explanation for the lower accuracy on\nTRACIE may be longer contexts in LEXTIME(Fig.\n5), which provides more contextual information. A\nsecond hypothesis relates to less explicit and po-\ntentially more challenging language for temporal\nreasoning (Fig. 3) in TRACIE.\n6 Errors Analysis\nWe manually analyze 100 erroneous predictions by\nGPT-4, GPT-4 Turbo and LLaMA 3.1 70B. Visual-\nization of errors is shown in Fig. 7.\nTR in Queries – We analyze the distribution of\nerrors by categorizing according to the three de-\nfined temporal relationship types. Errors are evenly\ndistributed between After and Before TR, with\nfewer Simultaneous errors. This pattern reflects\nthe initial dataset composition (Sec. 2), indicating\nno bias toward any specific temporal relation type.\nReasoning Errors – We analyze context para-\ngraphs’ characteristics that led to error in event\nordering. Comparing this error set to Fig. 3, we\nobserve that, except for noun-based events (un-\nderrepresented in the error set by 9%), all other\nlinguistic features identified in Sec. 3 as character-\nistic of legal language are overrepresented among\n7\n--- Page 8 ---\nAfter Before\nSimultaneous050100\n46 44\n10Temporal Relations in Queries\nModalsNegations\nSubordinate Clauses Conditional ClausesNoun EventsPassivesParaphrases\nNon-Past EventsFuture Speculative050100\n21 18100\n12435296 98\n21Reasoning ErrorsFigure 7: Error analysis for LEXTIMEon 100 misclassified samples from the few-shot setting across GPT-4, GPT-4\nTurbo and LLaMA 3.1 70B. The left chart shows the TR distribution in the queries, and the right plot categorizes\nerrors by counting features found in the context paragraphs.\nerrors. Specifically, errors are associated with a\n+89% increase in paraphrases, +84% in subordi-\nnate clauses, +63% in events not in the past simple\ntense, +24% in passive voice, +12% in negations\nand future speculative constructions, +6% in modal-\nity markers, and +2% in conditional clauses. This\nsuggests that these linguistic features contribute to\nthe difficulty of event ordering in legal texts.\n7 Related Work\nBenchmarking LLMs – Previous datasets and\nbenchmarks (Hosokawa et al., 2023; Wenhu Chen,\n2024) study the performance of LLMs on temporal\nreasoning, showing that those tasks pose a chal-\nlenge and that the limitations of LLMs in temporal\nreasoning are not well understood. They show that\nLMs generally struggle with temporal reasoning, as\nevidenced by Feng et al. (2023), who demonstrate\nthat LLMs often take random guesses. Jain et al.\n(2023) show that LLMs perform better on event\nduration tasks than on event ordering tasks and that\nreasoning over longer contexts is more difficult,\ntwo aspects we study in L EXTIME.\nQiu et al. (2024) indicate that LLMs lag behind\nhuman performance and small-scale but specialized\nLMs. The performance gap is only marginally\nreduced by introducing in-context learning, fine-\ntuning, and CoT prompting. Their analysis shows\nthat current LLMs struggle with having a consistent\ntemporal model due to insufficient exposure during\npretraining. We investigate the effectiveness of\nCoT prompting within the legal domain.\nTemporal Reasoning and Understanding –\nGrounding events in a temporal context has a\nnotable history in NLP. Pustejovsky et al. (2005)\ndemonstrated that temporal grounding affects\nquestion-answering model quality and defined an\nevent as a situation that occurs or holds true at aspecific time point or interval. Recent temporal rea-\nsoning benchmarks advanced our understanding of\nLMs’ capabilities (Wenhu Chen, 2024; Hosokawa\net al., 2023). However, existing datasets primar-\nily focus on general or narrative texts (Zhou et al.,\n2021b). Temporal reasoning consists of subtasks\nsuch as event frequency, duration, or ordering\n(Zhou et al., 2019; Jain et al., 2023). Event or-\ndering is particularly important for constructing\ntimelines in the legal domain. Despite its signifi-\ncance, no prior work has systematically evaluated\nLMs’ ability to perform temporal event ordering in\nlegal texts.\nExisting datasets mainly several approaches: nat-\nural language inference and relation extraction\n(Thukral et al., 2021; Hosokawa et al., 2023; Zhou\net al., 2021b) or question-answering (QA), such\nas and MC-TACO (Zhou et al., 2019), TimeDial\nfor dialogue settings (Qin et al., 2021), TempRea-\nson (Tan et al., 2023) and TimeQA (Wenhu Chen,\n2024). Our work is inspired by those methods,\nframing the task as a simplified QA format with\nbinary labels.\n8 Conclusion\nWe present LEXTIME, the first benchmark for eval-\nuating temporal event ordering in legal language,\nand conduct a systematic study on LLMs’ ability\nto reason over legal temporal structures. Our find-\nings show that while LLMs outperform narrative\nbenchmarks, accuracy is constrained by the com-\nplexities of legal discourse, such as paraphrasing.\nWe show that longer input contexts and implicit-\nexplicit event pairs improve model accuracy, offer-\ning practical insights for optimizing LLM-based\nlegal NLP applications. These results highlight the\nneed for domain adaptation, with future work po-\ntential in exploiting identified errors and linguistic\nstructure to .\n8\n--- Page 9 ---\n9 Limitations\nOur study has several limitations. First, we do not\nprovide a comprehensive evaluation of ambiguous\ncases, as these are classified and resolved during\nthe annotation stage. Second, our temporal rela-\ntions are limited to three main categories, whereas a\nmore detailed approach, such as Allen’s interval re-\nlations, could capture richer temporal distinctions.\nAdditionally, the generalizability of our findings\nis constrained by the specificity of the legal domain\nin our dataset and by the size of the LEXTIME\n(512 samples). While our benchmark represents\nan initial step in evaluating temporal reasoning in\nlegal texts, it primarily focuses on event ordering.\nFuture research should extend this evaluation to\ninclude other temporal reasoning tasks, such as\nevent frequency and duration analysis, to achieve\na more comprehensive understanding. Expanding\nthe availability of annotated legal datasets across\nvarious legal domains would also enhance the ro-\nbustness of this research.\nFinally, computational constraints limited our\nability to evaluate the larger LLaMA 70B model\nin both its instructed and base versions. Instead,\nwe focused on smaller models, such as the LLaMA\n8B instructed and base versions, to explore the\nperformance differences between aligned and base\nmodels at a more manageable scale. Future re-\nsearch should investigate whether larger models\ndemonstrate improved temporal reasoning in legal\ncontexts.\nReferences\nJames F Allen. 1983. Maintaining knowledge about\ntemporal intervals. Communications of the ACM ,\n26(11):832–843.\nTomasso Caselli, Ben Miller, Marieke van Erp, Piek\nV ossen, Martha Palmer, Eduard Hovy, Teruko Mita-\nmura, David Caswell, Susan W Brown, and Claire\nBonial. 2018. Events and stories in the news: Pro-\nceedings of the workshop. In Events and Stories in\nthe News . Association for Computational Linguistics\n(ACL).\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\nYu, Haotian Wang, Ming Liu, and Bing Qin. 2024.\nTimeBench: A comprehensive evaluation of tempo-\nral reasoning abilities in large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1204–1228, Bangkok, Thailand.\nAssociation for Computational Linguistics.Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2024. Scaling instruction-finetuned language models.\nJournal of Machine Learning Research , 25(70):1–53.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 .\nYu Feng, Ben Zhou, Haoyu Wang, Helen Jin, and Dan\nRoth. 2023. Generic temporal reasoning with dif-\nferential analysis and explanation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 12013–12029, Toronto, Canada. Association\nfor Computational Linguistics.\nTaishi Hosokawa, Adam Jatowt, and Kazunari\nSugiyama. 2023. Temporal natural language infer-\nence: Evidence-based evaluation of temporal text\nvalidity. In Advances in Information Retrieval:\n45th European Conference on Information Retrieval,\nECIR 2023, Dublin, Ireland, April 2–6, 2023, Pro-\nceedings, Part I , page 441–458, Berlin, Heidelberg.\nSpringer-Verlag.\nRaghav Jain, Daivik Sojitra, Arkadeep Acharya, Sri-\nparna Saha, Adam Jatowt, and Sandipan Dandapat.\n2023. Do language models have a common sense\nregarding time? revisiting temporal commonsense\nreasoning in the era of large language models. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6750–\n6774, Singapore. Association for Computational Lin-\nguistics.\nTaja Kuzman and Nikola Ljubeši ´c. 2023. Automatic\ngenre identification: a survey. Language Resources\nand Evaluation , pages 1–34.\nEric Martinez, Francis Mollica, and Edward Gibson.\n2022. Poor writing, not specialized concepts, drives\nprocessing difficulty in legal language. Cognition ,\n224.\nEric Martinez, Francis Mollica, and Edward Gibson.\n2024. Even lay people use legalese. PNAS , 121.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\nJames Pustejovsky, Robert Knippen, Jessica Littman,\nand Roser Saurí. 2005. Temporal and event informa-\ntion in natural language text. Language resources\nand evaluation , 39:123–164.\nLianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng\nHe, Yejin Choi, and Manaal Faruqui. 2021. TIME-\nDIAL: Temporal commonsense reasoning in dialog.\nInProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n9\n--- Page 10 ---\n7066–7076, Online. Association for Computational\nLinguistics.\nYifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen,\nEdoardo Ponti, and Shay Cohen. 2024. Are large\nlanguage model temporally grounded? In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume\n1: Long Papers) , pages 7064–7083, Mexico City,\nMexico. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research ,\n21(140):1–67.\nQingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023.\nTowards benchmarking and improving the temporal\nreasoning capability of large language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 14820–14835, Toronto, Canada.\nAssociation for Computational Linguistics.\nShivin Thukral, Kunal Kukreja, and Christian Kavouras.\n2021. Probing language models for understanding of\ntemporal expressions. In Proceedings of the Fourth\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP , pages 396–406, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nSiddharth Vashishtha, Benjamin Van Durme, and\nAaron Steven White. 2019. Fine-grained temporal re-\nlation extraction. arXiv preprint arXiv:1902.01390 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2024. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nProceedings of the 36th International Conference on\nNeural Information Processing Systems , NIPS ’22,\nRed Hook, NY , USA. Curran Associates Inc.\nWilliam Yang Wang Wenhu Chen, Xinyi Wang. 2024.\nA dataset for answering time-sensitive questions.\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff\nGray, Alejandra Molina, Michael Terry, and Car-\nrie J Cai. 2022. Promptchainer: Chaining large lan-\nguage model prompts through visual programming.\nPreprint , arXiv:2203.06566.\nKamariah Yunus, Radzuwan Ab Rashid, et al. 2016.\nColligations of prepositions: Essential properties of\nlegal phraseology. International Journal of Applied\nLinguistics and English Literature , 5(6):199–208.\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth.\n2019. “going on a vacation” takes longer than “go-\ning for a walk”: A study of temporal commonsenseunderstanding. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 3363–3369, Hong Kong, China. Association\nfor Computational Linguistics.\nBen Zhou, Kyle Richardson, Qiang Ning, Tushar Khot,\nAshish Sabharwal, and Dan Roth. 2021a. Temporal\nreasoning on implicit events from distant supervision.\nPreprint , arXiv:2010.12753.\nBen Zhou, Kyle Richardson, Qiang Ning, Tushar Khot,\nAshish Sabharwal, and Dan Roth. 2021b. Temporal\nreasoning on implicit events from distant supervision.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1361–1371, Online. Association for Computa-\ntional Linguistics.\nAppendix\nA Specific Features of Legal Events\nA.1 T RACIE Event Triggers\n0.0 0.5 1.0 1.5 2.0 2.5 3.0decided\nwent\nwanted\nget\ngo\nfound\ntold\nstarted\nneeded\nbought\nFigure 8: Top-10 triggers in TRACIE by frequency (%).\nA.2 Keywords for Temporal Markers\nExtraction\n•General time: now, then, before, after, later,\nsoon, earlier, previously, recently, already, im-\nmediately, eventually, finally, formerly, subse-\nquently, yet, ever, until, till, since, while, when,\nwhenever, as soon as, as long as, afterwards,\nmeanwhile, in the meantime, up to now, up until\nnow.\n•Frequency Adverbs: always, usually, often,\nsometimes, occasionally, rarely, seldom, never,\nfrequently, constantly, continuously, periodically,\ninfrequently, repeatedly\nAdverbs, conjunctions or prepositions that don’t\nappear in Fig. 9 were not found in the datasets.\nA.3 Frequency of Temporal Cues\nSee Fig. 9.\n10\n--- Page 11 ---\nafter when later\nrecentlysince beforethen\nsubsequentlyuntilﬁnally\neventuallyyet\nformerlypreviouslynow soonalready\nimmediatelywhile\nafterwardseveralwaysnever\nsometimesoftenusually01234555\n4\n2\n1 11 111\n0 0 0 00 0 0 0 0 0 000 01\n034\n1\n0112\n001\n1\n0 0 02\n1\n001\n0 012\n00 0Frequency Adverbs Legal TracieFigure 9: Comparison of the frequency of temporal adverbs in LEXTIME(blue) and TRACIE (green). Values (y-axis)\nrepresent the percentage of sentences that contain each adverb, indicating its frequency in the text.\nwould shouldmay must couldcan willmight shall02468 7.68\n2.85\n1.34 1.2 1.060.78 0.560.14 0.141.32\n0.0 0.03 0.02.95\n0.17 0.410.03 0.0Legal\nTracie\nFigure 10: Comparison of the frequency of the modal verbs in LEXTIME(blue) and TRACIE (green). Values (y-axis)\nrepresent the percentage of sentences that contain each modal, indicating its frequency in the text.\nA.4 Frequency of Modal Verbs\nSee Fig. 10.\nA.5 Frequency of Verb Tenses\nFig. 11 shows the frequency of the tenses found in\nthe context paragraph, for both datasets.\npast simplepresent simple present perfect future simplepast continuouspresent continuouspast perfect020406064.24\n24.24\n5.570.99 0.14 0.11 0.11Legal\npast simplepresent simple present perfect future simplepast continuouspresent continuouspast perfect02040608087.62\n8.96\n0.44 0.68 1.29 0.1 0.85Tracie\nFigure 11: Distribution of the tenses of the verbs in\nLEXTIMEandTRACIE . The y-axis is the frequency, in\n%.B Models\nB.1 Impact of Model Size\n0.5 1.0 3.0 8.0 70.0 123.0\nModel Size (B)0.400.450.500.550.600.650.700.75\n0.5000.5250.5420.6900.692\n0.472LLaMA models\nMistral-large\nFlan-T5-large\nFigure 12: Scaling effect of model size and overall\ntemporal reasoning performance. The y-axis shows\nthe average accuracy across all prompt types for each\nmodel. The x-axis (model size) is shown in the log scale.\nResults show a log-linearity between parameter size and\nperformance.\nB.2 Models Used\nGPT-4o and GPT-4-turbo GPT-4-turbo refers to\ngpt-4-turbo-2024-04-09 , and GPT-4o refers to gpt-\n4o-2024-08-06 . GPT-4o provides faster inference\ncompared to turbo (OpenAI, 2023).\nLlama 3.1 and Llama 3.2 We use the Hug-\ngingFace implementation of Llama-3.1-8B, Llama-\n3.1-8B-Instruct, Llama-3.1-70B, Llama-3.1-70B-\nInstruct, Llama-3.2-1B, Llama-3.2-1B-Instruct,\nLlama-3.2-3B, Llama-3.2-3B-Instruct . Llama 3.1\nand 3.2 fine-tuned versions use supervised fine-\ntuning (SFT) and reinforcement learning from hu-\n11\n--- Page 12 ---\nman feedback (RLHF), Rejection Sampling (RS),\nand Direct Preference Optimization (DPO) (Dubey\net al., 2024).\nMistral We use the large version, 123B parame-\nters, Mistral-Large-Instruct-2407 .\nFLAN-T5-large We use the large version of flan-\nt5 (Chung et al., 2024). Flan-t5 is built on top of T5\n(Raffel et al., 2020) through instruction fine-tuning.\nB.3 GPU Resources\nThis work was made possible by the use of: Nvidia\nTesla V100-SXM2, Nvidia Tesla K80, Nvidia\nA100 40GB and 80GB, and Nvidia RTX A6000\n40GB.\nC Dataset Construction\nC.1 Examples from the L EXTIME\nC.2 Prompts for Dataset Generation\nD Prompts for Evaluation\n12\n--- Page 13 ---\nContext Query (Event A, TR, Event B) Answer (Label)\nRather than pursue the grievance through the contracts\ngrievance and arbitration procedure, the union directed\nhospital employees to refuse to pick up extra shifts\nbeyond their normal scheduled work [...]Does the hospital being forced to hire\ntraveller nurses and pay an extraordinary\npremium precede  the union members'\nconcerted refusal to pick up extra shifts ?No\nOn November 25, 2020, the district court granted the\nunions summary judgment motion and denied\ncompany X . Company X subsequently appealed the\nmatter to the United States Court of Appeals [...]Does the company and the union\nexperiencing a dispute precede  the district\ncourt denying Company X a summary\njudgement motion ?Yes\nIn February of 2023, a union representative engaged in\nphysically intimidating conduct  with a hospital manager .\nIn response to the incident, the hospital limited the\nunion representatives access to the hospital [...]Does the union disputing the hospital's\nresponse follow  the union representative\nengaging in physically intimidating conduct ?YesFigure 13: Examples from LEXTIME, showing three instances composed of a context paragraph, a query, and a\nbinary label. Event B (explicit) is highlighted in green in the context and the query. Event A from the query occurs\nlater in the context, beyond the visible portion.\nDescription Prompt\nPrompt 1: Get paragraphs Your task is to find if {context_paragraph} contains events. The events should be verbs. If it does\nnot contain relevant events, just answer with \"None\" (no explanation). If it contains relevant events,\nplease answer \"relevant\" and list them as verbs only (no explanation).\nPrompt 2: Get implicit events Your task is to find 5 implicit events in {context_paragraph}. Explicit events have already been\nidentified in {explicit_events}. Implicit events are events that are not explicitly written in the\nparagraph, but that the reader understands happened.\nHere is an example: {example}.\nReturn the events as verbs, in a numbered list, followed by 2 explicit events that are the closest to\nthe start and end time of the implicit event.\nPrompt 3: Construct sentences Context Paragraph: {context_paragraph} Implicit and Explicit Events: {instance} Your task is\nto construct pairs of events initially extracted from the {context_paragraph}: events have already\nbeen extracted and are contained in {instance}. One instance includes one implicit event and two\nexplicit events. Construct 3 sentences that demonstrate the temporal relationship between the events.\nUse phrases like \"Event A precedes Event B,\" \"Event A is simultaneous with Event B\", \"Event A\nfollows Event B\" or \"Event A happens after Event B\" to indicate the temporal order.\nHere is an example: {example}.\nTable 3: Prompts use for generating pre-labelled instances of L EXTIME\n13\n--- Page 14 ---\nDescription Prompt\nZeroshot Given the context, the task is to answer the query with \"yes\" or \"no\".\ncontext: \"{context}\"\nquery: \"{query}\"\nanswer:\nOneshot Given the context, the task is to answer the query with \"yes\" or \"no\".\nexample: {example1}\ncontext: \"{context}\"\nquery: \"{query}\"\nanswer:\nFewshot Given the context, the task is to answer the query with \"yes\" or \"no\".\nexample 1: {example1}\nexample 2: {example2}\nexample 3: {example3}\ncontext: \"{context}\"\nquery: \"{query}\"\nanswer:\nCoT oneshot Given the context, the task is to answer the query with \"yes\" or \"no\".\nLet’s reason through it step-by-step:\nexample: {example1_cot}\ncontext: \"{context}\"\nquery: \"{query}\"\nanswer:\nCoT fewshot Given the context, the task is to answer the query with \"yes\" or \"no\" .\nLet’s reason through it step-by-step:\nexample 1: {example1_cot}\nexample 2: {example2_cot}\nexample 3: {example3_cot}\ncontext: {context}\nquery: {query}\nanswer:\nTable 4: Prompt templates for evaluation, on all five types of prompts used: zeroshot, oneshot, fewshot, CoT oneshot,\nCoT fewshot. These templates are used for evaluation on both LEXTIMEandTRACIE , examples used differ and are\nextracted from each dataset respectively\n14\n--- Page 15 ---\nDescription Examples\nExample 1 context: \"Before assigning the plaintiff to work, the defendant hired employees, including the plaintiff. The plaintiff was hired as a\ntrackman.\"\nquery: \"Does hiring employees happen before assigning the plaintiff to work?\"\nanswer: \"yes\"\nExample 2 context: \"ms. X advised ms. Y to issue the written discipline and to speak with mr. R (as well as the other roll tunnel operators)\nregarding the defaced guidelines. accordingly, ms. Y , along with John Doe (the union steward) approached mr. R to present him\nwith the disciplinary warning and to discuss the defacement of the posted guidelines. mr. R was on his clamp truck and ms. Y\nadvised that mr. R that she needed to speak with him. without stopping, mr. R told her that he did not have time to speak with her\n(his own supervisor) because he was working and drove by her.\"\nquery: \"Does ms. X learning about the defaced guidelines precedes her advising ms. Y to issue the written discipline?\"\nanswer: \"yes\"\nExample 3 context: \"rather than pursue the grievance through the contracts grievance and arbitration procedure, the union directed hospital\nemployees to refuse to pick up extra shifts beyond their normal scheduled work. specifically, prior to the dispute over employees\nobligation to work holidays, employees would regularly and voluntarily pick up extra shifts beyond their normal scheduled work.\nbut once the dispute over holiday work arose, employees stopped picking up shifts beyond their normal scheduled work. after the\naforementioned dispute arose, a member of the union and a hospital employee provided to a member of the hospitals administration\na text message stating, in part: per union unless you had originally picked up ot or st when it was first offered we should be giving\nback those hours for this coming week and they are asking us not to pick up anything through the end of may and the first 2 weeks of\njune because they still are not allowing people to take vacation on a holiday unless they have adequate staffing. upon information and\nbelief, the references to ot and st in the text message mean overtime and straight time, respectively. the union members concerted\nrefusal to pick up extra shifts is a violation of the contracts and has caused the hospital great hardship in achieving adequate staffing\nfor the hospitals operations. as a result of the union and its members conduct, the hospital was forced to hire so-called traveler nurses\nand to pay an extraordinary premium, in the millions of dollars, for adequate staffing compared with the cost to the hospital had the\nunion not directed its members to engage in this concerted effort and had the union members picked up extra shifts as they had in the\npast.\"\nquery: \"Does the hospital being forced to hire traveler nurses and pay an extraordinary premium precedes the union members’\nconcerted refusal to pick up extra shifts?\"\nanswer: \"no\"\nExample 1 CoT context: \"Before assigning the plaintiff to work, the defendant hired employees, including the plaintiff. The plaintiff was hired as a\ntrackman.\"\nquery: \"Does hiring employees happens before assigning the plaintiff to work?\"\nanswer:\" Reasoning: 1. The defendant hired employees, including the plaintiff. 2. The plaintiff was hired as a trackman. 3. The\nemployees including the plaintiff were all assigned to work.\nThe correct answer is: \"yes\"\nExample 2 CoT context: \"ms. X advised ms. Y to issue the written discipline and to speak with mr. R (as well as the other roll tunnel operators)\nregarding the defaced guidelines. accordingly, ms. Y , along with John Doe (the union steward) approached mr. R to present him\nwith the disciplinary warning and to discuss the defacement of the posted guidelines. mr. R was on his clamp truck and ms. Y\nadvised that mr. R that she needed to speak with him. without stopping, mr. R told her that he did not have time to speak with her\n(his own supervisor) because he was working and drove by her.\"\nquery: \"Does ms. X learning about the defaced guidelines precedes her advising ms. Y to issue the written discipline?\"\nanswer:\" Reasoning:\n1. Ms. X learned about the defaced guidelines. 2. She then advised Ms. Y to issue the written discipline.\nSince Ms. X’s learning about the defaced guidelines happened before advising Ms. Y , the correct answer is \"yes\".\nExample 3 CoT context: \"rather than pursue the grievance through the contracts grievance and arbitration procedure, the union directed hospital\nemployees to refuse to pick up extra shifts beyond their normal scheduled work. specifically, prior to the dispute over employees\nobligation to work holidays, employees would regularly and voluntarily pick up extra shifts beyond their normal scheduled work.\nbut once the dispute over holiday work arose, employees stopped picking up shifts beyond their normal scheduled work. after the\naforementioned dispute arose, a member of the union and a hospital employee provided to a member of the hospitals administration\na text message stating, in part: per union unless you had originally picked up ot or st when it was first offered we should be giving\nback those hours for this coming week and they are asking us not to pick up anything through the end of may and the first 2 weeks of\njune because they still are not allowing people to take vacation on a holiday unless they have adequate staffing[.] upon information\nand belief, the references to ot and st in the text message mean overtime and straight time, respectively. the union members concerted\nrefusal to pick up extra shifts is a violation of the contracts and has caused the hospital great hardship in achieving adequate staffing\nfor the hospitals operations. as a result of the union and its members conduct, the hospital was forced to hire so-called traveler nurses\nand to pay an extraordinary premium, in the millions of case 3:23-cv-00466 document 1 filed 04/13/23 page 5 of 10 6 dollars, for\nadequate staffing compared with the cost to the hospital had the union not directed its members to engage in this concerted effort\nand had the union members picked up extra shifts as they had in the past.\"\nquery: \"Does the hospital being forced to hire traveler nurses and pay an extraordinary premium precedes the union members’\nconcerted refusal to pick up extra shifts?\"\nanswer:\" Reasoning:\n1. The union members’ concerted refusal to pick up extra shifts occurred as a result of the dispute. 2. This refusal led to the hospital\nbeing forced to hire traveler nurses and pay an extraordinary premium.\nSince the hospital was forced to hire traveler nurses and pay a premium as a result of the union members’ refusal, the correct answer\nis \"no\".\nTable 5: Examples used for one-shot and few-shot setting for evaluation on L EXTIME\n15\n--- Page 16 ---\nDescription Examples\nExample 1 context: \"My boss pulled me aside at work. I thought he was going to tell me Happy Birthday. Instead, he told me I wasn’t needed\ntoday. I sulked home, not sure whether I was getting fired or not. When I arrived home, I walked into a surprise birthday party!\"\nquery: \"Does the event i felt worried start after my boss told me i wasn’t needed?\"\nanswer: \"yes\"\nExample 2 context: \"John got a new cat and was worried how it would take to the old one. He tried everything but the old cat would always act\nmean. One day the old cat was stuck in a box and the young cat helped it. Ever since they became best friends. \"John was so happy\nwith the result, he got another cat.\"\"\nquery: \"Does the event john brought a cat home ends before the old cat stuck in a box?\"\nanswer: \"yes\"\nExample 3 context: \"Javier has always wanted to try instant ramen. He decides that today is his day to try it. He goes to the store to buy a few\npackets. On his way home Javier is in a car accident. Javier never got to cook his instant ramen.\"\nquery: \"Does the event javier hasn’t had instant ramen before. ends before he go to the store to buy a few packets ?\"\nanswer: \"no\"\nExample 1 CoT context: \"My boss pulled me aside at work. I thought he was going to tell me Happy Birthday. Instead, he told me I wasn’t needed\ntoday. I sulked home, not sure whether I was getting fired or not. When I arrived home, I walked into a surprise birthday party!\"\nquery: \"Does the event i felt worried start after my boss told me i wasn’t needed?\"\nanswer:\" Reasoning: 1. The boss informed the speaker that they weren’t needed today. 2. After hearing this news, the speaker felt\nworried about their job status. 3. The feeling of worry is a response to the boss’s statement.\nThe correct answer is: \"yes\"\nExample 2 CoT context: \"John got a new cat and was worried how it would take to the old one. He tried everything but the old cat would always act\nmean. One day the old cat was stuck in a box and the young cat helped it. Ever since they became best friends. \"John was so happy\nwith the result, he got another cat.\"\"\nquery: \"Does the event john brought a cat home ends before the old cat stuck in a box?\"\nanswer:\" Reasoning:\n1. John brought a new cat home and was concerned about how the old cat would react. 2. The old cat initially acted mean towards\nthe new cat. 3. At some point, the old cat got stuck in a box. 4. The event of bringing the new cat home occurs first and precedes the\nincident of the old cat getting stuck.\nSince the correct answer is \"yes\".\nExample 3 CoT context: \"Javier has always wanted to try instant ramen. He decides that today is his day to try it. He goes to the store to buy a few\npackets. On his way home Javier is in a car accident. Javier never got to cook his instant ramen.\"\nquery: \"Does the event javier hasn’t had instant ramen before. ends before he go to the store to buy a few packets ?\"\nanswer:\" Reasoning:\n1. The context states that Javier has always wanted to try instant ramen, indicating he has not had it before. 2. Javier decides to try\ninstant ramen today. 3. He goes to the store to buy instant ramen, but he has not yet cooked or eaten it. 4. Therefore, the event of not\nhaving instant ramen continues until he purchases the packets and cooks, meaning it does not end before he goes to the store.\nSince, the correct answer is \"no\".\nTable 6: Examples used for one-shot and few-shot setting for evaluation on T RACIE\n16",
  "text_length": 60773
}