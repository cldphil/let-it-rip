{
  "id": "http://arxiv.org/abs/2506.00942v1",
  "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task\n  Understanding",
  "summary": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs.",
  "authors": [
    "Haitao Li",
    "Ziyu Li",
    "Yiheng Mao",
    "Ziyi Liu",
    "Zhoujian Sun",
    "Zhengxing Huang"
  ],
  "published": "2025-06-01T10:17:13Z",
  "updated": "2025-06-01T10:17:13Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "eess.SP"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00942v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00942v1  [cs.CL]  1 Jun 2025anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input\nand Multi-Task Understanding\nHaitao Li\nZhejiang University\nHangzhou, China\nlihaitao@zju.edu.cnZiyu Li\nZhejiang University\nHangzhou, China\nliziyu@zju.edu.cnYiheng Mao\nZhejiang University\nHangzhou, China\nyihengmao@zju.edu.cn\nZiyi Liu\nTranstek Medical Electronics Co., Ltd.\nZhongshan, China\n11313008@zju.edu.cnZhoujian Sun\nZhejiang Lab\nHangzhou, China\nsunzhoujian@zju.edu.cnZhengxing Huang\nZhejiang University\nHangzhou, China\nzhengxinghuang@zju.edu.cn\nAbstract\nThe advent of multimodal large language models (MLLMs) has\nsparked interest in their application to electrocardiogram (ECG)\nanalysis. However, existing ECG-focused MLLMs primarily focus\non report generation tasks, often limited to single 12-lead, short-\nduration (10s) ECG inputs, thereby underutilizing the potential of\nMLLMs. To this end, we aim to develop a MLLM for ECG analysis\nthat supports a broader range of tasks and more flexible ECG in-\nputs. However, existing ECG-QA datasets are often monotonous.\nTo address this gap, we first constructed the anyECG dataset, which\nencompasses a wide variety of tasks, including report generation,\nabnormal waveform localization, and open-ended question answer-\ning. In addition to standard hospital ECGs, we introduced long-\nduration reduced-lead ECGs for home environments and multi-\nple ECG comparison scenarios commonly encountered in clinical\npractice. Furthermore, we propose the anyECG-chat model, which\nsupports dynamic-length ECG inputs and multiple ECG inputs. We\ntrained the model using a three-stage curriculum training recipe\nwith the anyECG dataset. A comprehensive evaluation was con-\nducted, demonstrating that anyECG-chat is capable of supporting\nvarious practical application scenarios, including not only common\nreport generation tasks but also abnormal waveform localization\nfor long-duration reduced-lead ECGs in home environments and\ncomprehensive comparative analysis of multiple ECGs.\nCCS Concepts\n•Applied computing →Life and medical sciences ;Health\ninformatics .\nKeywords\nMulti-Modal Large Language Model, ECG\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, Woodstock, NY\n©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\nHaitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, and Zhengxing\nHuang. 2018. anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input\nand Multi-Task Understanding. In Proceedings of Make sure to enter the correct\nconference title from your rights confirmation email (Conference acronym ’XX).\nACM, New York, NY, USA, 14 pages. https://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nElectrocardiograms (ECGs) play a pivotal role in clinical practice,\nserving as a cornerstone for diagnosing and monitoring various\ncardiac conditions [ 3,36,38]. Traditional methods for ECG analysis\noften rely on task-specific models, which are typically constrained\nto single tasks such as arrhythmia detection or report generation\n[10,17,33,44,49]. These approaches lack the flexibility to handle\ndiverse tasks and do not support interactive human-machine com-\nmunication. The emergence of multimodal large language models\n(MLLMs) [ 1,7,28,45] has introduced a transformative paradigm\nfor ECG analysis. By leveraging their powerful natural language\nreasoning capabilities and multimodal perception abilities, MLLMs\ncan perform a wide range of tasks while enabling natural language-\nbased human interaction. Consequently, ECG-focused MLLMs have\ngarnered significant attention as a promising research direction.\nPrevious research on ECG MLLMs has explored various ap-\nproaches. A straightforward method involves using an external\nclassifier or feature extractor to convert ECG signals into a series\nof textual labels, which are then passed to the LLM [ 24,34,47].\nHowever, this approach often results in significant information loss.\nAnother line of work focuses on native ECG MLLMs, where ECG\nrepresentations are directly fed into the LLM. For instance, MEIT\n[43] employs a projection mechanism to align ECG embeddings\nwith the semantic space of the LLM, enabling the generation of\nECG reports. Similarly, PULSE [ 29] addresses real-world scenarios\ninvolving ECG images by constructing an ECG image-based MLLM\nthat supports tasks such as report generation, waveform classifica-\ntion, and rhythm analysis. Building on these advancements, [ 50]\nintroduces multi-turn dialogue capabilities, enabling iterative inter-\nactions. Additionally, it leverages retrieval-augmented generation\n(RAG) [18] to enrich the LLM with ECG-specific knowledge.\nDespite the advancements in ECG-MLLMs, their current appli-\ncations are predominantly limited to single-task scenarios such as\nreport generation or label classification [ 20,29,43,50]. In essence,\nECG reports are composed of a series of labels related to rhythm,\n--- Page 2 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\nTable 1: anyECG dataset examples.\nReportGen\nECG (10s 12leads) :\nQuestion : Please provide the report for the following ECG.\nAnswer : Report: Sinus rhythm, Poor R wave progression - probable\nnormal variant, Borderline ECG\nLocalization\nECG (15s 2leads) :\nQuestion : Examine this ECG and point out where the Premature\nventricular contraction is located.\nAnswer : Duration: 1.9s-3.1s, 6.8s-8.1s, 14.3s-15.0s\nMultiECG\nThree ECGs (10s 12leads) :\nQuestion : How do the ECGs collected 0 days ago, 1323 days ago, and\n1924 days ago compare in terms of rhythm and overall assessment?\nThe ECGs collected at these times show sinus rhythm as the primary\nrhythm, with one instance of sinus bradycardia, and all are assessed\nas normal or normal except for rate, indicating a stable cardiac\ncondition over time.\nAnswer : The ECGs collected at these times show sinus rhythm as\nthe primary rhythm, with one instance of sinus bradycardia, and all\nare assessed as normal or normal except for rate, indicating a stable\ncardiac condition over time.\nmorphology, and diagnosis, making report generation and label\nclassification fundamentally the same task [ 12,42]. However, the\ncore objective of MLLMs is to address diverse, multi-task challenges\nrather than being confined to a single task [45]. Consequently, ex-\nisting ECG-MLLMs fail to fully harness the potential of MLLMs.\nMoreover, these models are typically restricted to processing single,\n12-lead, 10-second ECG inputs [ 29,43,50], which are inadequate\nfor modern use cases. For instance, they cannot effectively han-\ndle the long-duration, reduced-lead ECGs commonly generated\nin home environments [ 15] or the multi-ECG comparison scenar-\nios frequently encountered in clinical practice. To bridge this gap,\nthere is a pressing need for a more versatile ECG-MLLM capable\nof supporting a broader range of tasks, particularly fine-grained\nlocalization tasks, and accommodating more flexible ECG inputs,\nincluding long-duration ECGs, reduced-lead ECGs, and multiple\nECGs.\nHowever, existing ECG question-answering datasets [ 29,34,43]\nare often overly simplistic and fail to meet the requirements for di-\nverse tasks and flexible input scenarios. To address these limitations,\nwe developed a novel dataset named anyECG, which comprises\nthree subsets: ReportGen, Localization, and MultiECG. These sub-\nsets encompass a wide range of tasks, including report generation,abnormal waveform localization, and open-ended question answer-\ning. Additionally, we introduced long-duration ECGs, reduced-lead\nECGs, and multi-ECG inputs to better align with modern clinical\nand home-monitoring scenarios. To support these diverse tasks\nand flexible input formats, we propose the anyECG-chat Model\nwhich supports dynamic-length ECG inputs and multiple ECG in-\nputs seamlessly. We employed a three-stage curriculum learning\n[11,45] approach to train the model, enabling it to evolve from\ncoarse perception to fine-grained understanding, and ultimately to\ninstruction-following and multi-ECG comparison tasks.\nWe evaluated our model across three tasks: ReportGen, Localiza-\ntion, and MultiECG. For the ReportGen task, we conducted out-of-\ndomain testing using six unseen ECG classification datasets to more\naccurately assess the model’s generalization capabilities. Our model\noutperformed all existing ECG-MLLMs in this task. For the Localiza-\ntion task, we utilized a reserved test set for evaluation. Unlike other\nECG-MLLMs, which lack the fine-grained perception required for\nsecond-level abnormality localization, our model not only surpassed\nthe performance of traditional dedicated segmentation models but\nalso demonstrated the ability to handle dynamic-length ECG inputs.\nFurthermore, despite anyECG-Localization dataset being limited to\n2-lead ECGs, our model exhibited strong zero-shot capabilities in\nsingle-lead scenarios, making it well-suited for home monitoring\napplications. For the MultiECG task, we evaluated the model using\nthe MIMIC Multi-ECG QA dataset and the ECG-QA dataset. Our\nmodel consistently demonstrated dominant performance among\nECG-MLLMs. Additionally, it exhibited robust multi-turn dialogue\ncapabilities, highlighting its potential as a valuable teaching tool\nfor medical students.\n2 Related Work\nECG Understanding : In recent years, the paradigm of ECG under-\nstanding has gradually shifted from traditional supervised learning\n[37] to self-supervised learning [ 10,17,33,44,49], which leverages\nlarge amounts of unlabeled data for pretraining. Self-supervised\nECG learning can be broadly categorized into two approaches:\ncontrastive self-supervised learning [ 10,44], which typically in-\nvolves augmenting the same ECG signal into two different views as\npositive samples while treating different ECG signals as negative\nsamples; and generative self-supervised learnin [ 17,33,49], which\nmasks a portion of the ECG signal and attempts to reconstruct\nthe masked part using the unmasked portion. Both approaches,\nhowever, require fine-tuning on downstream task data and are not\ninherently suited for zero-shot scenarios.\nInspired by CLIP [ 35], several multimodal contrastive learning\nmethods for ECG-report pairs [ 21,23,25,48] have emerged. These\nmethods are trained by minimizing the distance between paired\nECG and report embeddings while maximizing the distance be-\ntween non-paired ones. However, these models are fundamentally\ndiscriminative, limiting their application to tasks such as comput-\ning similarity between ECGs and reports. They are not well-suited\nfor diverse generative tasks, such as report generation, and cannot\nhandle multiple tasks within a single model.\nIn contrast, this paper introduces anyECG-chat, a generative\nMLLM capable of performing a wide range of tasks guided by\n--- Page 3 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 2: Overview of the anyECG Dataset\nDataset Source Duration Leads ECGs per QA QA Pairs\nReportGen\nMIMIC-ECG ReportGen MIMIC-ECG [12] 10s 12 1 773,268\nLocalization\nEuropean ST-T Localization European ST-T Database [39] 10s 2 1 39,110\nEuropean ST-T Long Localization European ST-T Database [39] 10–60s 2 1 19,555\nMIT-BIH ST Change Localization MIT-BIH ST Change Database [2] 10s 2 1 6,500\nMIT-BIH ST Change Long Localization MIT-BIH ST Change Database [2] 10–60s 2 1 3,250\nMIT-BIT Arrhythmia Localization MIT-BIT Arrhythmia Database [30] 10s 2 1 54,440\nMIT-BIT Arrhythmia Long Localization MIT-BIT Arrhythmia Database [30] 10–60s 2 1 27,220\nMultiECG\nMIMIC Multi-ECG QA MIMIC-ECG [12] 10s 12 2–6 135,094\nECG-QA(10%) [34] PTB-XL [42] 10s 12 1–2 33,220\ntextual instructions. This approach unlocks the potential for diverse\nand flexible applications in ECG understanding.\nECG-MLLMs Inspired by advancements in vision-language\nmodels [ 1,7,28,45], ECG-MLLMs have emerged as a promising\ndirection for ECG analysis. A straightforward approach involves\nusing external classifiers or feature extractors to convert ECG sig-\nnals into a series of textual labels, which are then fed into the LLM\n[24,34,47]. However, this method often results in significant in-\nformation loss. An alternative approach focuses on building native\nECG MLLMs, where ECG embeddings are directly input into the\nLLM. For instance, MEIT [ 43] employs a projection mechanism to\nmap ECG embeddings into the semantic space of the LLM, enabling\ntasks like report generation. Similarly, PULSE [ 29] addresses real-\nworld scenarios involving ECG images by constructing an ECG\nimage-based MLLM capable of supporting tasks such as report gen-\neration, waveform classification, and rhythm analysis. Building on\nthese advancements, [ 50] introduces multi-turn dialogue capabili-\nties, allowing for iterative interactions. Additionally, it incorporates\nRAG [18] to enhance the LLM with ECG-specific knowledge.\nDespite these successes, existing ECG-MLLMs are often limited\nto single-task applications, such as report generation or label clas-\nsification [ 29,43,50], which contradicts the core objective of multi-\nmodal models to address diverse tasks within a single framework\n[7,45]. Furthermore, current ECG-MLLMs are typically restricted\nto processing single, 12-lead, 10-second ECG inputs. This limita-\ntion makes them inadequate for modern scenarios, such as the\nlong-duration, reduced-lead ECGs commonly generated in home\nenvironments, or the multi-ECG comparison tasks frequently en-\ncountered in clinical practice. To address these challenges, we pro-\npose anyECG-chat, a model designed to support dynamic-length\nECG inputs and multiple ECG inputs seamlessly.\n3 AnyECG Dataset\nThe existing ECG-QA datasets [ 29,34,43] are relatively monoto-\nnous and fail to meet the requirements for supporting multi-task\nand flexible ECG inputs. To address this limitation, we constructed\na novel dataset named anyECG. In terms of tasks, prior ECG-QAdatasets are often restricted to report generation or label classifica-\ntion, which significantly underestimates the potential of MLLMs.\nTherefore, we introduced a broader range of tasks, including not\nonly report generation but also more complex tasks requiring fine-\ngrained perception, such as waveform localization, as well as di-\nverse open-ended question answering facilitated by LLMs. Regard-\ning ECG signals, previous datasets typically utilize single, short-\nduration (10s), 12-lead ECG. This setup is inadequate for modern\nscenarios, such as the large volume of long-duration, reduced-lead\nECGs generated in home environments, and the multi-ECG compar-\nison scenarios commonly encountered in clinical practice. To this\nend, we incorporated long-duration ECGs, reduced-lead ECGs, and\nmulti-ECG inputs into our dataset. Specifically, anyECG consists of\nthree components: anyECG-ReportGen, anyECG-Localization, and\nanyECG-MultiECG. We provide an example from each component\nin the Table 1 and summarize the dataset statistics in Table 2. No-\ntably, we standardized the sampling frequency of all ECG datasets\nto 100 Hz and normalized the ECG signals to a range of -1 to 1.\n3.1 anyECG-ReportGen\nanyECG-ReportGen is a report generation QA dataset derived from\nthe MIMIC-ECG dataset. MIMIC-ECG [ 12] contains approximately\n800,000 ECGs and corresponding reports collected from around\n160,000 individuals. Each ECG consists of 12 leads, has a duration of\n10 seconds, and is sampled at 500 Hz. To enhance data quality, we\nexcluded samples with empty reports or reports containing fewer\nthan three words, removed reports lacking meaningful informa-\ntion, and discarded ECGs with unexpected anomalies. After these\npreprocessing steps, a total of 773,268 ECGs remained. We orga-\nnized the ECGs and their corresponding reports into a QA format\nsuitable for training MLLMs. The questions were formulated as\n\"Please provide the report for the following ECG\" and its various\nparaphrased forms (see Appendix A.1). The answers correspond to\nthe respective reports. In total, 773,268 QA pairs were generated,\nall of which were used for training.\n--- Page 4 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\n...\n... ...........61 tokens\n241 tokens\n61 tokens 61 tokens\nECG EncoderMLP Connector\nPlease provide the report for the \nfollowing ECG(s).Text TokenizerText EmbeddingLoRA AdapterLLaMa LLM\nShort 12 leads ECG (10s)\nLong reduced leads ECG (40s)Two ECGs Comparison\n<ECG_start>\n<ECG_end>\nECG Embedding\nText Embedding<Padding> (a) ECG Encoder Pre-training \nUsing ECG-Report Pairs\n(b) AnyECGchat Training Using \nanyECG dataset\nFigure 1: The overview of anyECG-chat architecture.\n3.2 anyECG-Localization\nanyECG-Localization is a waveform localization dataset derived\nfrom three long-duration, 2-lead ECG datasets collected in home\nsettings: the European ST-T Database [ 39], the MIT-BIH ST Change\nDatabase [ 2], and the MIT-BIT Arrhythmia Database [ 30]. These\ndatasets are meticulously annotated by physicians to identify abnor-\nmal waveforms and rhythms, including features such as Left Bun-\ndle Branch Block (LBBB) beats, Right Bundle Branch Block (RBBB)\nbeats, and Premature Ventricular Contractions (PVCs). Specifically,\nthe European ST-T Database contains 90 ECG recordings, each\nlasting 120 minutes. The MIT-BIH ST Change Database includes\n28 ECG recordings, each lasting between 20 and 70 minutes. The\nMIT-BIT Arrhythmia Database comprises 48 ECG recordings, each\nlasting 30 minutes. While all these datasets consist of 2-lead ECGs,\nthe leads are not identical across datasets.\nanyECG-Localization is further divided into two subsets: short-\nduration and long-duration. For the short-duration subset, ECGs\nare segmented into 10-second clips. For the long-duration subset,\nECGs are segmented into clips of dynamic lengths ranging from\n10 to 60 seconds. For each region where abnormalities occur, we\nresample 10 times for short-duration and 5 times for long-duration\nclips around the abnormal region, introducing a random time shift\nto enhance dataset diversity and robustness. To prevent the model\nfrom generating hallucinated responses (e.g., predicting abnormal\nregions when none exist), we included \"Not Found\" samples, where\nthe queried feature is absent in the ECG. This ensures the model can\ncorrectly respond with \"Not Found\" instead of providing random\ntime segments.\nThe dataset was reformatted into a QA structure. Questions are\nphrased as \"Can you show me where the [abnormal] occurred on\nthis ECG?\" along with various paraphrased forms (see AppendixA.2). Answers correspond to the localized waveform regions or\n\"Not Found.\" Ultimately, anyECG-Localization comprises 100,050\nshort-duration ECG localization QA pairs and 50,025 long-duration\nECG localization QA pairs. A portion of the dataset was reserved as\na test set, ensuring that the same ECG (entire recording level, not\nsegments level) does not appear in both the training and test sets.\n3.3 anyECG-MultiECG\nanyECG-MultiECG is a multi-ECG comparison dataset designed\nto address scenarios in clinical practice where physicians compare\nmultiple ECGs from the same patient over time. It consists of two\ncomponents: MIMIC Multi-ECG QA and ECG-QA [ 34] constructed\nfrom PTB-XL [42].\nThe MIMIC Multi-ECG QA dataset is derived from the MIMIC-\nECG dataset [ 12], which contains nearly 800,000 ECGs. Since the\nability to compare multiple ECGs builds upon the model’s under-\nstanding of single ECGs, only a small number of multi-ECG QA\npairs are required for instruction tuning once the model has been\ntrained on single ECG task. To construct this dataset, we selected the\nfirst 200,000 ECGs from MIMIC-ECG and grouped them by patient,\nidentifying individuals with 2 to 6 ECGs. The distribution of the\nnumber of ECGs per patient and the sampling time intervals are de-\ntailed in Appendix A.4. Open-ended QA pairs were generated using\nLlama-3.3-70B-Instruct [ 13,41], a pure language model. To provide\nthe model with ECG information, we supplied the corresponding\nreports and the sampling times for each ECG. Six example ques-\ntions were used as few-shot samples, covering various scenarios: (1)\ngenerating a report for each ECG, (2) providing a comprehensive\ndiagnosis based on all ECGs, (3) identifying trends, and (4) predict-\ning potential future changes. These four scenarios assume that the\nuser provides only the order of the ECGs without specifying their\n--- Page 5 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nsampling times. Additionally, we considered cases where sampling\ntimes are provided, including (5) absolute sampling times and (6)\nrelative sampling times. The specific prompts are detailed in Ap-\npendix A.3. For each patient, eight questions and corresponding\nanswers were generated, resulting in a total of 135,094 multi-ECG\nQA pairs.\nThe second component, ECG-QA [ 34], is constructed from PTB-\nXL by [ 42]. Since the answers in ECG-QA are often overly simplistic\n(e.g., yes/no or a list of tags), we aimed to prevent the model from\noverfitting to this concise answering style. To achieve this, we used\nonly one-tenth of the training set and appended the prompt \"Please\nanswer briefly. \" to the original questions. This subset contains 33,220\nQA pairs.\n4 AnyECG-chat Architecture\nThe architecture of our model is illustrated in Figure 1. It consists\nof an ECG encoder, a large language model (LLM), a modality\nalignment module, and LoRA adapters. Previous ECG MLLMs were\noften limited to single 12-lead, short-duration (10s) ECG inputs.\nTo enable our model to handle the diverse scenarios and flexible\nECG inputs in the anyECG dataset, we introduced a Dynamic ECG\nInput mechanism. We will elaborate on each component and the\nDynamic ECG Input mechanism in detail below.\n4.1 ECG encoder\nThe performance of multimodal large language models (MLLMs)\nin question answering (QA) tasks heavily relies on the perceptual\ncapabilities of the ECG encoder. Instead of training the ECG encoder\nfrom scratch using the anyECG dataset, we opted to pre-train it\nusing contrastive learning [ 23] on the MIMIC-ECG [ 12] dataset,\nwhich contains 800,000 ECGs and their corresponding reports.\nWe employed a ViT-base [ 9] architecture as the ECG encoder.\nHowever, since ViT-base is originally designed for image data, we\nmade several modifications to adapt it to the characteristics of multi-\nchannel 1D ECG signals: (1) we redefined the patching mechanism\nto accommodate the temporal and multi-lead nature of ECG signals,\nand (2) we introduced lead embeddings [ 33] and adjusted positional\nembeddings to capture the spatiotemporal structure of ECG data.\nSpecifically, let an ECG signal be represented as 𝑋∈R𝐿×𝑇,\nwhere𝐿is the number of leads and 𝑇is the signal length, computed\nas𝑇=sampling duration×sampling frequency . First, we standard-\nized the sampling frequency to 100 Hz and normalized each lead\nto the range[−1,1]to mitigate measurement biases from different\ndevices and enhance generalization.\nTo adapt the patching mechanism, we applied spatio-temporal\npatchifying with a patch size of (1,200). For example, given a pre-\nprocessed ECG from MIMIC-ECG 𝑋∈R12×1000, each lead is di-\nvided into 5 patches, resulting in a total of 60 patches across all\nleads. Additionally, we introduced a [CLS] token to capture global\nfeatures.\nTraditional ViT models rely solely on positional embeddings,\nwhich are insufficient for capturing inter-lead relationships in ECG\ndata. To address this limitation, we introduced lead embeddings,\ndenoted as𝐸lead, to encode the spatial relationships between leads.\nPatches from the same lead share the same lead embedding, whilepatches from different leads at the same time share the same po-\nsitional embedding. The final input embedding for each patch is\ncomputed as:\n𝐸=𝐸signal+𝐸pos+𝐸lead,\nwhere𝐸posrepresents the positional embedding, and 𝐸signal is the\npatch embedding derived from the ECG signal.\nThe ViT encoder consists of alternating layers of multi-headed\nself-attention (MSA) and feed-forward blocks. Layer normalization\n(LN) is applied before every block, and residual connections are\nused after every block. The feed-forward block contains two layers\nwith a GELU non-linearity.\n𝑧0=[CLS ;E1;E2;···;E𝑛],E𝑖∈R𝐷,\n𝑧′\nℓ=MSA(LN(𝑧ℓ−1))+𝑧ℓ−1, ℓ=1,...,𝐿,\n𝑧ℓ=FeedForward(LN(𝑧′\nℓ))+𝑧′\nℓ, ℓ=1,...,𝐿,\ny=LN(𝑧0\n𝐿).\n4.2 Large Language Model.\nIn this paper, we utilize the Meta-Llama-3-8B-Instruct [ 13] as our\nLLM. LLaMA is pre-trained on a diverse corpus comprising both\nnatural language and programming language data using a self-\nsupervised approach. The instruct version, derived from LLaMA, is\nfine-tuned specifically for dialogue tasks. This additional training\non instruction-following prompts significantly enhances its perfor-\nmance in reasoning and generation tasks across various domains.\nTo prevent overfitting and catastrophic forgetting, which could\nsignificantly degrade the model’s ability to respond to general\nqueries, we opted for Low-Rank Adaptation (LoRA) [ 16] instead of\nfull parameter fine-tuning. LoRA introduces a small set of auxiliary\nlearnable weights, referred to as LoRA adapters, which are added on\ntop of the pre-trained LLM while keeping all its original parameters\nfrozen. Each LoRA adapter is associated with a specific model layer\nand modifies its frozen parameters by adding a low-rank learnable\nmatrix of the same size. In anyECG-chat, we inject LoRA adapters\n(rank=8 and 𝛼=16) to the projection layers for query and key in\nall self-attention layers of the LLaMA model.\n4.3 Modality Connector\nVarious modality connectors have been explored in prior research\non vision-language models (VLMs), including cross-attention mech-\nanisms [ 1], Q-formers [ 22], and simple linear projections [ 28]. In\nthis work, to balance effectiveness and efficiency, we adopt a two-\nlayer MLP with GELU activation as the modality connector, inspired\nby LLaVA 1.5 [27].\n4.4 Dynamic ECG Input\nThe Dynamic ECG Input mechanism is designed to empower anyECG-\nchat with the ability to handle diverse scenarios and flexible ECG\ninputs, including varying-length ECGs, reduced-lead ECGs, and\nmulti-ECG inputs. To achieve this, two key challenges must be\naddressed: (1) embedding dynamic-length and reduced-lead ECGs\neffectively, and (2) ensuring that multiple ECG embeddings can be\ninput into the LLM while maintaining clear distinctions between\ndifferent ECGs.\n--- Page 6 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\nTable 3: Overview of the Training Recipe\nStage Trained Params Training Task Samples LR Batch Size Epochs\n1 Connector + ECG encoder ReportGen 773,268 1×10−4256 2\n2 Connector + ECG encoder + LoRA ReportGen + Localization 923,343 1×10−464 2\n3 Connector + ECG encoder + LoRA ReportGen + Localization + MultiECG 1,091,657 1×10−464 1\nFor the first challenge, since our ECG encoder is pre-trained on\nthe MIMIC-ECG dataset using 10-second, 100 Hz, 12-lead ECGs,\nwe adopt the following strategies: For ECGs shorter than 10 sec-\nonds, zero-padding is applied to match the required length. For\nECGs longer than 10 seconds, they are first padded to the nearest\nmultiple of 10 seconds and then segmented into 10-second clips.\nThese clips are individually processed by the ECG encoder, and the\nresulting embeddings are concatenated. The [CLS] tokens from\neach segment are averaged to produce the final embedding for the\nlong-duration ECG. For reduced-lead ECGs, missing leads are sim-\nilarly zero-padded to ensure compatibility with the encoder. As\nmentioned above, since our ECG encoder incorporates lead em-\nbeddings, it can capture the relationships between leads even for\nmissing leads.\nTo address the second challenge, and to ensure the LLM can dis-\ntinguish between multiple ECG inputs without conflating them into\na single long-duration ECG, we introduce special tokens <ECG_start>\nand<ECG_end> . These tokens are added before and after each ECG\nembedding, enabling the LLM to clearly identify and differentiate\nbetween individual ECG inputs.\nThis Dynamic ECG Input mechanism equips anyECG-chat with\nthe flexibility to process a wide variety of ECG inputs, making it\nwell-suited for the diverse practical scenarios represented in the\nanyECG dataset.\n5 Training Recipe\nWe designed a three-stage curriculum learning approach tailored to\nthe varying complexity of tasks in the anyECG dataset. Inspired by\n[11,45], this approach comprises pretraining, fine-grained pretrain-\ning, and open-ended instruction tuning. Notably, the ECG encoder\nwas pre-trained on the MIMIC-ECG dataset using contrastive learn-\ning prior to these three stages.\nIn the first stage, we trained the model on the anyECG-ReportGen\ndataset while freezing the parameters of the LLM. Only the Con-\nnector and ECG encoder were updated during this phase. The goal\nof this stage was to effectively align the ECG embeddings with\nthe LLM embeddings through the Connector. The second stage\nintroduced the more challenging anyECG-Localization dataset in\naddition to anyECG-ReportGen. This dataset requires the model\nto perform fine-grained waveform localization. During this phase,\nwe jointly trained the ECG encoder, the Connector, and fine-tuned\nthe LLM using LoRA. The objective of this fine-grained pretraining\nstage was to enhance the model’s ability to perform precise local-\nization tasks. In the third stage, we utilized the complete anyECG\ndataset, focusing on open-ended QA tasks. The training parameters\nare the same as those in the second stage. Compared to the earlier\nstages, this phase introduced a significant number of open-ended\nQA tasks, aiming to improve the model’s instruction-followingcapabilities. Additionally, multi-ECG inputs were incorporated to\nenable the model to leverage its perceptual abilities for more com-\nplex comparative tasks.\nThis progressive three-stage training strategy allowed the model\nto evolve from coarse perception to fine-grained understanding, and\nfinally to instruction-following and multi-ECG comparison tasks.\nBy gradually increasing task complexity, the approach mitigates the\nrisk of the model relying excessively on textual reasoning, which\ncould lead to hallucinations, especially when its ECG perception\ncapabilities are underdeveloped. The training recipe is summarized\nin Table 3.\n6 Experiments\nTo evaluate the performance of anyECG-chat in handling diverse\ntasks and flexible ECG inputs, we conducted experiments on three\ncomponents of the anyECG dataset: Report Generation, Localiza-\ntion, and Multi-ECG. For the Report Generation task, since the\nentire MIMIC-ECG dataset was used for contrastive pretraining\nand Stage 1 training, we performed out-of-domain testing on six\nunseen ECG datasets to ensure fairness. For the Localization task,\nwe reserved a subset of ECGs from unseen individuals as the test\nset during the construction of the anyECG dataset. Additionally,\nalthough anyECG-Localization only includes 2-lead ECGs, we also\nevaluated the model’s zero-shot capability in single-lead scenarios.\nFor the Multi-ECG task, we constructed an additional test set for\nMIMIC Multi-ECG QA, ensuring no individual overlap with the\ntraining set, and evaluated ECG-QA using its official test set. In-\nterestingly, we observed that although anyECG-chat was trained\nsolely on single-turn QA tasks, it demonstrated strong multi-turn\ninstruction-following capabilities. This highlights its potential as\na valuable teaching tool for medical students. An overview of the\nevaluation datasets is provided in Table 4.\n6.1 Report Generation\nFor the report generation task, anyECG-chat was trained on the\nanyECG-ReportGen dataset, which is derived from the MIMIC-ECG\ndataset. Instead of evaluating our model directly on MIMIC-ECG,\nwe opted for an out-of-domain setup to assess the generalization ca-\npability of anyECG-chat. Specifically, we conducted experiments on\nsix ECG classification datasets: CPSC [ 26], CSN [ 51], PTBXL-Form,\nPTBXL-Rhythm, PTBXL-Super, and PTBXL-Sub [ 42]. Notably, ECG\nreports are essentially composed of labels, and using classification\ntasks to evaluate the model provides a more accurate measure of\nits understanding of ECGs compared to traditional text similarity\nmetrics like BLEU or ROUGE. To benchmark and compare anyECG-\nchat with existing models, we prompted the anyECG-chat with\nthe query, \"Please provide the report for the following ECG.\" The\nreports generated by anyECG-chat and the dataset label names\n--- Page 7 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 4: Evaluation Dataset Overview\nEvaluation Test QA Setting\nReportGEN\nPTBXL-Super 2,158 OOD\nPTBXL-Sub 2,158 OOD\nPTBXL-Form 880 OOD\nPTBXL-Rhythm 2,098 OOD\nCPSC 1,382 OOD\nCSN 9,031 OOD\nLocalization\nEuropean ST-T Localization 5,710 2 leads (ID), 1 lead (ZS)\nEuropean ST-T Long Localization 2,855 2 leads (ID), 1 lead (ZS)\nMIT-BIH ST Change Localization 1,110 2 leads (ID), 1 lead (ZS)\nMIT-BIH ST Change Long Localization 555 2 leads (ID), 1 lead (ZS)\nMIT-BIT Arrhythmia Localization 10,230 2 leads (ID), 1 lead (ZS)\nMIT-BIT Arrhythmia Long Localization 5,115 2 leads (ID), 1 lead (ZS)\nMulti-ECG\nMIMIC Multi-ECG QA 1152 ID\nECG-QA 8,214 10% train data used\nNotes: ID: in-domain, OOD: out-of-domain, ZS: zero-shot. All datasets\nuse the test subset.\nwere then encoded using a text encoder (BioBERT [ 8]). Finally, the\ncosine similarity between the text embeddings of anyECG-chat’s\noutput and each label was computed to derive the prediction scores.\nWe compared anyECG-chat against several supervised methods\n[4,5,14,33,44], discriminative zero-shot methods [ 25], and other\ngenerative zero-shot methods [ 19,29] using AUC as the evaluation\nmetric. The results, presented in Table 5, demonstrate that anyECG-\nchat achieved the best performance among generative zero-shot\nmethods, except on PTBXL-Form. Additionally, for PTBXL-Rhythm,\nPTBXL-Sub, and CSN, anyECG-chat delivered performance compa-\nrable to supervised methods and discriminative zero-shot methods.\nThe former are task-specific models tailored for each dataset, while\nthe latter require prior knowledge of dataset labels. In contrast, our\napproach directly generates labels without relying on such prior\ninformation.\nTable 5: Results of Classification.\nmacro-AUCPTBXL\nSuperPTBXL\nSubPTBXL\nFormPTBXL\nRhythmCPSC CSN\nSupervised: dedicated model tailored for each dataset\nSimCLR [4] 73.53 73.39 62.52 77.73 76.54 73.20\nBYOL [14] 76.45 71.64 70.82 77.17 78.75 74.69\nSimSiam [5] 75.63 76.38 71.31 75.92 75.31 77.41\nASTCL [44] 81.02 76.51 66.99 76.05 79.51 75.79\nST-MEM [33] 71.36 63.59 66.07 74.85 70.39 71.36\nDiscriminative Zero-Shot: requires pre-defined labels\nMERL [25] 74.20 75.70 65.90 78.50 82.80 74.40\nGenerative Zero-Shot: directly outputs labels without requiring pre-defined labels\nLLaVa-Med [19] 51.21 58.33 69.12 75.77 56.07 60.54\nPULSE [29] 66.61 61.32 63.82 73.91 66.15 64.18\nanyECG-chat 67.84 72.48 62.08 77.62 68.70 70.096.2 Localization Task\n6.2.1 Results. For the localization task, we reserved a subset of\nunseen ECGs as the test set during the construction of the anyECG-\nLocalization dataset. We evaluated the performance of anyECG-\nchat on this test set using the Intersection over Union (IoU) metric,\nas localization can be modeled as a segmentation task. We also\ncompared its performance against other supervised methods [ 32]\nand existing ECG-MLLMs [ 19,29]. As expected, other ECG-MLLMs\n[19,29] lacked the fine-grained temporal perception required for\nsecond-level localization. When asked to identify the location of\nabnormal waveforms, they could only provide lead-level answers\n(see Appendix B.1 for detail). In contrast, anyECG-chat was able to\ndeliver precise second-level answers. Detailed results are presented\nin Figure 2.\nFor short-duration ECGs, the results demonstrate that although\nUnet [ 32] is a dedicated model for segmentation tasks, anyECG-\nchat significantly outperformed Unet on the European ST-T and\nMIT-BIH ST Change datasets and achieved comparable perfor-\nmance on the MIT-BIH Arrhythmia dataset. Interestingly, we ob-\nserved that Unet exhibited consistent performance across different\ndatasets, whereas anyECG-chat showed varying performance. This\ndiscrepancy may be attributed to the diverse training data used\nfor anyECG-chat, beyond the anyECG-localization dataset, which\nlikely enhanced its ability to perceive different types of abnormal-\nities across datasets. For long-duration ECGs, Unet was unable\nto handle dynamic-length ECGs due to architectural limitations,\nwhereas anyECG-chat successfully processed these inputs, further\nshowcasing its flexibility and robustness.\n6.2.2 Single Lead ECG Localization. Although the anyECG-localization\ndataset only includes 2-lead ECGs, we also evaluated the model’s\nzero-shot capability in single-lead scenarios. Three single-lead cases\nwere tested: masking the first lead, masking the second lead, and\nmasking a random lead, with the masked lead values set to zero.\nThe results, shown in Figure 2, indicate that anyECG-chat achieves\ncomparable performance in both short-duration and long-duration\nECGs when the first lead is masked in the European ST-T dataset\nand when the second lead is masked in the MIT-BIH ST Change\nand MIT-BIH Arrhythmia datasets. This demonstrates the model’s\nzero-shot capability in single-lead scenarios. However, performance\ndrops significantly when the other lead is masked, likely because\nthe queried abnormal waveform features are present only in the\nmasked lead.\nEuropean ST-T MIT-BIH Arrhythmia MIT-BIH ST Change010203040506070IOU70.2\n46.669.1\n66.1\n32.541.8\n34.443.166.7\n49.9\n37.854.2\n49.5651.51\n46.20Short Duration\nEuropean ST-T MIT-BIH Arrhythmia MIT-BIH ST Change59.0\n32.659.6\n54.4\n22.435.2\n22.429.957.6\n38.7\n26.447.6Long DurationanyECG-chat mask first lead mask second lead mask random lead U-Net\nFigure 2: Results of Localization and Zero-Shot Sigle Lead\nECG Localization.\n--- Page 8 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\n6.3 Multi-ECG Comparison\nFor the multi-ECG comparison task, we evaluated our model using\ntwo datasets: MIMIC Multi-ECG QA and ECG-QA. The former\nincludes scenarios involving comparisons of 2 to 6 ECGs, while the\nlatter focuses solely on comparisons between 2 ECGs. As previously\nmentioned, since the answers in ECG-QA are relatively concise, we\nlimited the training data to 10% of the original dataset to prevent the\nmodel from overfitting to short responses. The detailed results are\npresented below. Additionally, although anyECG is a single-turn\nQA dataset, we also tested anyECG-chat’s performance in multi-\nturn QA scenarios, demonstrating its potential as a teaching tool\nfor physicians.\n6.3.1 MIMIC Multi-ECG QA. Since the MIMIC Multi-ECG QA\ndataset is constructed using Llama-3.3-70B-Instruct [ 13,41] for\nopen-ended QA tasks, it lacks explicit metrics for direct evaluation.\nTo address this, we employed a LLM, QwQ-32B [ 40,46], as the\nevaluation model. To ensure fairness, we did not use the outputs of\nLlama-3.3-70B-Instruct as the gold standard for QwQ’s evaluation.\nInstead, we provided QwQ with the questions and the correspond-\ning reports for each ECG, allowing it to assess the quality of the\noutputs from anyECG-chat and other models based solely on this\ninformation. The evaluation scores ranged from 0 to 5. The detailed\nprompt is provided in Appendix B.2. This approach ensures that\nthe outputs of Llama-3.3-70B-Instruct were used only for training\nand not as the evaluation gold standard.\nWe compared the outputs of anyECG-chat, LLaVa-Med [ 19], and\nPULSE [ 29]. It is worth noting that since LLaVa-Med and PULSE\nwere not trained to handle multi-ECG inputs, we adapted their\nusage to support multi-ECG comparison tasks while maintaining\nconsistency with their training setup. Specifically, we first processed\neach ECG individually to generate its corresponding report. These\nreports were then concatenated, along with an image combining\nall the ECGs, and provided as input to LLaVa-Med and PULSE to\nanswer multi-ECG comparison questions. The score distributions\nfor these models are shown in Figure 3. Notably, anyECG-chat\nachieved significantly higher scores compared to the other two\nmodels. Additionally, we analyzed the average scores of each model\nacross different numbers of ECG inputs, as well as the number\nof times each model achieved the highest score among the three\nmodels. The results, as shown in Table 6, indicate that anyECG-\nchat maintains notable robustness as the number of input ECGs\nincreases. Furthermore, anyECG-chat secured the highest score in\n816 out of 1,152 questions, demonstrating a substantial performance\nadvantage over the other two models.\nTable 6: Average Scores for MIMIC Multi-ECG QA Across\nDifferent Numbers of ECG Inputs.\n# ECGs 2 3 4 5 6 All Highest (#)\nanyECG-chat 3.28 2.80 2.61 2.61 2.98 3.01 816/1152\nLLaVa-Med 2.75 2.19 2.34 2.09 2.24 2.48 553/1152\nPULSE 1.66 1.40 1.13 1.46 1.84 1.53 264/1152\n0 1 2 3 4 5\nScore050100150200250300350400Count\n124149176169213321\n282\n177\n115131175272422\n273\n169\n109\n76103Score Distribution Across Models\nanyECG-chat\nLLaVa-Med\nPULSEFigure 3: Results of MIMIC Multi-ECG QA.\n6.3.2 ECG-QA. For the ECG-QA dataset, we compared anyECG-\nchat with several discriminative models [ 6,31] and other ECG-\nMLLMs [ 19,29]. As mentioned earlier, the answers in ECG-QA\nare relatively concise, often limited to a few short phrases. Conse-\nquently, discriminative methods model the QA task as a multi-label\nclassification problem, which requires predefined possible labels\nas prior knowledge. In contrast, ECG-MLLMs, as generative meth-\nods, directly produce answers without relying on predefined labels.\nSimilar to the MIMIC Multi-ECG QA evaluation, LLaVa-Med and\nPULSE were not specifically trained for multi-ECG inputs. To ensure\ncompatibility, we employed the same approach used in the MIMIC\nMulti-ECG QA evaluation to adapt these models for multi-ECG in-\nput testing. We used exact match accuracy as the evaluation metric,\nand the results are presented in Table 7. Although anyECG-chat\ndoes not outperform discriminative models that leverage prede-\nfined labels, it achieves the best performance among generative\nECG-MLLMs, even when trained on only 10% of the training data.\nNotably, it excels in CI-Verify and CC-Verify tasks, achieving ac-\ncuracies of 70.1% and 67.9%, respectively, demonstrating its strong\ncapability in multi-ECG comparison tasks.\nTable 7: Performance Comparison Across Different Question\nTypes on ECG-QA.\nEM Acc.S\nVerifyS\nChooseS\nQueryCC\nVerifyCC\nQueryCI\nVerifyCI\nQuery\nDiscriminative Model: requires possible labels\nM3AE [6] 74.6 57.1 41.0 75.5 20.1 75.3 4.2\nMedViLL [31] 73.9 54.1 40.4 74.3 22.0 77.5 3.5\nFusion Transformer 72.1 46.4 37.4 71.9 18.4 68.1 2.2\nGenerative ECG-MLLM: directly outputs answers without requiring pre-defined labels\nLLaVa-Med(0%) [19] 34.7 0 0 11.9 0 36.8 0\nPULSE(100%) [29] 64.6 56.1 2.4 52.9 3.9 57.1 0\nanyECG-chat(10%) 69.0 49.6 18.8 67.9 5.6 70.1 0.4\nS: Single, CC: Comparison-Consecutive, CI: Comparison-Irrelevant.\n6.3.3 Multi-Turn QA. Although the anyECG dataset only includes\nsingle-turn QA scenarios, we hypothesize that anyECG-chat re-\ntains the ability to handle multi-turn QA. This is because we fine-\ntuned the model using LoRA rather than performing full parame-\nter fine-tuning, which avoids catastrophic forgetting of the LLM’s\n--- Page 9 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\npre-trained capabilities. A multi-turn QA example is shown in Ap-\npendix B.3. While we lack quantitative metrics for evaluation, we\nobserved that the model demonstrates strong multi-turn instruction-\nfollowing capabilities, suggesting its potential as a teaching tool\nfor physicians.\n7 Conclusion\nIn this paper, we introduced anyECG-chat, a multimodal large lan-\nguage model designed for diverse ECG analysis tasks. By leveraging\nthe novel anyECG dataset and a three-stage curriculum training\nstrategy, anyECG-chat demonstrated strong performance across\nreport generation, waveform localization, and multi-ECG compari-\nson tasks. The proposed Dynamic ECG Input mechanism further\nenhanced the model’s flexibility, enabling it to handle varying-\nlength, reduced-lead, and multi-ECG inputs seamlessly. Experi-\nmental results showed that anyECG-chat outperformed existing\nECG-MLLMs in multiple scenarios and exhibited robust zero-shot\ncapabilities. Additionally, its multi-turn instruction-following abil-\nity highlights its potential as a valuable tool for medical education\nand clinical applications.\nReferences\n[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al .\n2022. Flamingo: a visual language model for few-shot learning. Advances in\nneural information processing systems 35 (2022), 23716–23736.\n[2]P. Albrecht. 1983. ST Segment Characterization for Long Term Automated ECG\nAnalysis . Massachusetts Institute of Technology, Department of Electrical En-\ngineering and Computer Science. https://books.google.com.hk/books?id=8NB-\ntgAACAAJ\n[3]Yehualashet Megersa Ayano, Friedhelm Schwenker, Bisrat Derebssa Dufera, and\nTaye Girma Debelee. 2022. Interpretable machine learning techniques in ECG-\nbased heart disease classification: a systematic review. Diagnostics 13, 1 (2022),\n111.\n[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning . PMLR, 1597–1607.\n[5]Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition . 15750–15758.\n[6]Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and\nTsung-Hui Chang. 2022. Multi-modal masked autoencoders for medical vision-\nand-language pre-training. In International Conference on Medical Image Comput-\ning and Computer-Assisted Intervention . Springer, 679–689.\n[7]Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui,\nWenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al .2024. How Far Are\nWe to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-\nSource Suites. arXiv preprint arXiv:2404.16821 (2024).\n[8]Pritam Deka, Anna Jurek-Loughrey, et al .2022. Evidence Extraction to Validate\nMedical Claims in Fake News Detection. In International Conference on Health\nInformation Science . Springer, 3–15.\n[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[10] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong\nKwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning via\ntemporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021).\n[11] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass.\n2023. Listen, think, and understand. arXiv preprint arXiv:2305.10790 (2023).\n[12] Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody,\nChrystinne Fernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber,\nParastou Eslami, et al .[n. d.]. MIMIC-IV-ECG-Diagnostic Electrocardiogram\nMatched Subset. ([n. d.]).\n[13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek\nKadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex\nVaughan, et al .2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783\n(2024).[14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan\nGuo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new\napproach to self-supervised learning. Advances in neural information processing\nsystems 33 (2020), 21271–21284.\n[15] Hai Yue Gu, Jun Huang, Xu Liu, Shu Qian Qiao, and Xi Cao. 2024. Effectiveness of\nsingle-lead ECG devices for detecting atrial fibrillation: An overview of systematic\nreviews. Worldviews on Evidence-Based Nursing 21, 1 (2024), 79–86.\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\n[17] Rui Hu, Jie Chen, and Li Zhou. 2023. Spatiotemporal self-supervised represen-\ntation learning from multi-lead ECG signals. Biomedical Signal Processing and\nControl 84 (2023), 104772.\n[18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\net al.2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in neural information processing systems 33 (2020), 9459–9474.\n[19] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei\nYang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med: Train-\ning a large language-and-vision assistant for biomedicine in one day. Advances\nin Neural Information Processing Systems 36 (2023), 28541–28564.\n[20] Haitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, and Zhengxing Huang.\n2024. De-biased Multimodal Electrocardiogram Analysis. arXiv preprint\narXiv:2411.14795 (2024).\n[21] Haitao Li, Che Liu, Zhengyao Ding, Ziyi Liu, and Zhengxing Huang. 2025. Fine-\nGrained ECG-Text Contrastive Learning via Waveform Understanding Enhance-\nment. arXiv preprint arXiv:2505.11939 (2025).\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language\nmodels. In International conference on machine learning . PMLR, 19730–19742.\n[23] Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. 2024. Frozen\nlanguage model helps ecg zero-shot learning. In Medical Imaging with Deep\nLearning . PMLR, 402–415.\n[24] Chunyu Liu, Yongpei Ma, Kavitha Kothur, Armin Nikpour, and Omid Kavehei.\n2023. BioSignal Copilot: Leveraging the power of LLMs in drafting reports for\nbiomedical signals. medRxiv (2023), 2023–06.\n[25] Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, and Rossella\nArcucci. 2024. Zero-Shot ECG Classification with Multimodal Learning and\nTest-time Clinical Knowledge Enhancement. arXiv preprint arXiv:2403.06659\n(2024).\n[26] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al .2018. An open\naccess database for evaluating the algorithms of electrocardiogram rhythm and\nmorphology abnormality detection. Journal of Medical Imaging and Health\nInformatics 8, 7 (2018), 1368–1373.\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved Baselines\nwith Visual Instruction Tuning.\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-\ntion Tuning.\n[29] Ruoqi Liu, Yuelin Bai, Xiang Yue, and Ping Zhang. 2024. Teach Multimodal LLMs\nto Comprehend Electrocardiographic Images. arXiv preprint arXiv:2410.19008\n(2024).\n[30] George B Moody and Roger G Mark. 2001. The impact of the MIT-BIH arrhythmia\ndatabase. IEEE engineering in medicine and biology magazine 20, 3 (2001), 45–50.\n[31] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, and Edward\nChoi. 2022. Multi-modal understanding and generation for medical images and\ntext via vision-language pre-training. IEEE Journal of Biomedical and Health\nInformatics 26, 12 (2022), 6070–6080.\n[32] Viktor Moskalenko, Nikolai Zolotykh, and Grigory Osipov. 2020. Deep learning\nfor ECG segmentation. In Advances in neural computation, machine learning,\nand cognitive research III: selected papers from the XXI international conference\non neuroinformatics, October 7-11, 2019, Dolgoprudny, Moscow Region, Russia .\nSpringer, 246–254.\n[33] Yeongyeon Na, Minje Park, Yunwon Tae, and Sunghoon Joo. 2024. Guiding\nMasked Representation Learning to Capture Spatio-Temporal Relationship of\nElectrocardiogram. arXiv preprint arXiv:2402.09450 (2024).\n[34] Jungwoo Oh, Seongsu Bae, Gyubok Lee, Joon-myoung Kwon, and Edward Choi.\n2023. ECG-QA: A Comprehensive Question Answering Dataset Combined With\nElectrocardiogram. arXiv preprint arXiv:2306.15681 (2023).\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al.2021. Learning transferable visual models from natural language supervision.\nInInternational conference on machine learning . PmLR, 8748–8763.\n[36] Adyasha Rath, Debahuti Mishra, Ganapati Panda, and Suresh Chandra Satapathy.\n2021. Heart disease detection using deep learning methods from imbalanced\nECG samples. Biomedical Signal Processing and Control 68 (2021), 102820.\n--- Page 10 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\n[37] Antônio H Ribeiro, Manoel Horta Ribeiro, Gabriela MM Paixão, Derick M Oliveira,\nPaulo R Gomes, Jéssica A Canazart, Milton PS Ferreira, Carl R Andersson, Peter W\nMacfarlane, Wagner Meira Jr, et al .2020. Automatic diagnosis of the 12-lead ECG\nusing a deep neural network. Nature communications 11, 1 (2020), 1760.\n[38] S Sahoo, M Dash, S Behera, and S Sabut. 2020. Machine learning approach to\ndetect cardiac arrhythmias in ECG signals: A survey. Irbm 41, 4 (2020), 185–194.\n[39] Alessandro Taddei, G Distante, M Emdin, P Pisani, GB Moody, C Zeelenberg, and\nC Marchesi. 1992. The European ST-T database: standard for evaluating systems\nfor the analysis of ST-T changes in ambulatory electrocardiography. European\nheart journal 13, 9 (1992), 1164–1172.\n[40] Qwen Team. 2025. QwQ-32B: Embracing the Power of Reinforcement Learning.\nhttps://qwenlm.github.io/blog/qwq-32b/\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[42] Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I\nLunze, Wojciech Samek, and Tobias Schaeffter. 2020. PTB-XL, a large publicly\navailable electrocardiography dataset. Scientific data 7, 1 (2020), 154.\n[43] Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie\nFu, Rossella Arcucci, Huaxiu Yao, and Mi Zhang. 2024. MEIT: Multi-modal elec-\ntrocardiogram instruction tuning on large language models for report generation.\narXiv preprint arXiv:2403.04945 (2024).\n[44] Ning Wang, Panpan Feng, Zhaoyang Ge, Yanjie Zhou, Bing Zhou, and Zongmin\nWang. 2023. Adversarial spatiotemporal contrastive learning for electrocar-\ndiogram signals. IEEE Transactions on Neural Networks and Learning Systems\n(2023).\n[45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin\nChen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al .2024. Qwen2-vl: Enhancing\nvision-language model’s perception of the world at any resolution. arXiv preprint\narXiv:2409.12191 (2024).\n[46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang\nLin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue,\nPei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical\nReport. arXiv preprint arXiv:2412.15115 (2024).\n[47] Han Yu, Peikun Guo, and Akane Sano. 2023. Zero-shot ECG diagnosis with large\nlanguage models and retrieval-augmented generation. In Machine learning for\nhealth (ML4H) . PMLR, 650–663.\n[48] Han Yu, Peikun Guo, and Akane Sano. 2024. ECG Semantic Integrator (ESI):\nA Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text.\narXiv preprint arXiv:2405.19366 (2024).\n[49] Huaicheng Zhang, Wenhan Liu, Jiguang Shi, Sheng Chang, Hao Wang, Jin He,\nand Qijun Huang. 2022. Maefe: Masked autoencoders family of electrocardio-\ngram for self-supervised pretraining and transfer learning. IEEE Transactions on\nInstrumentation and Measurement 72 (2022), 1–15.\n[50] Yubao Zhao, Tian Zhang, Xu Wang, Puyu Han, Tong Chen, Linlin Huang, Youzhu\nJin, and Jiaju Kang. 2024. ECG-Chat: A Large ECG-Language Model for Cardiac\nDisease Diagnosis. arXiv preprint arXiv:2408.08849 (2024).\n[51] J Zheng, H Guo, and H Chu. 2022. A large scale 12-lead electrocardiogram\ndatabase for arrhythmia study (version 1.0. 0). PhysioNet 2022Available online:\nhttp://physionet. org/content/ecg-arrhythmia/1.0. 0/(accessed on 23 November 2022)\n(2022).\n--- Page 11 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nA anyECG Dataset\nA.1 Template for ReportGen\nThe question templates used for constructing the anyECG-ReportGen\ndataset are provided in Table 8.\nQuestions for ECG Report Generation.\n•Please provide the report for the following ECG.\n•Give me the report of this ECG.\n•I need a report on the following ECG.\n•Could you send me the ECG report?\n•Provide me with the report of this ECG.\n•Please generate a report for the ECG below.\n•I’d like to receive the report for this ECG.\n•Can you share the report of the following ECG?\n•Give me a detailed report on this ECG.\n•May I have the official report for the ECG provided?\nTable 8: Questions for ECG Report Generation.\nA.2 Template for Localization\nThe question templates used for constructing the anyECG-\nLocalization dataset are provided in Table 9. The {abnormal} in the\nquestions refers to the specific abnormality present in the ECG, such\nas Premature ventricular contraction (PVC), Left bundle branch\nblock beat (LBBB) and Right bundle branch block beat (RBBB).\nQuestions for Localization.\n•Can you show me where the {abnormal} occurred on this ECG?\n•Locate the {abnormal} on this ECG for me, please.\n•Could you identify where the {abnormal} is on this ECG?\n•Tell me where to find the {abnormal} on this ECG.\n•Please locate the specific location of the {abnormal} on this ECG.\n•Check this ECG and tell me where the {abnormal} appears.\n•Determine where the {abnormal} is on this electrocardiogram.\n•Help me find where the {abnormal} shows up on this ECG.\n•Examine this ECG and point out where the {abnormal} is located.\n•Assess this ECG and specify the location of the {abnormal}.\n•Where does the {abnormal} appear in this ECG?\n•On this ECG, where can I see the {abnormal}?\n•Can you locate the {abnormal} on this ECG?\n•Where is the {abnormal} located in this ECG?\n•Locate the {abnormal} on this ECG for me, please.\n•Could you point out where the {abnormal} is on this ECG?\n•Where should I look to find the {abnormal} on this ECG?\n•I need to find the {abnormal} on this ECG; where should I look?\n•Help me locate the {abnormal} on this ECG.\n•Determine where the {abnormal} is located on this electrocardio-\ngram.\nTable 9: Questions for Localization.\nA.3 Prompt for MIMIC Multi-ECG QA\nThe prompt used to generate the MIMIC Multi-ECG QA dataset\nis provided in Table 11. The prompt is designed to elicit complexopen-ended questions and answers based on the ECG reports and\nacquisition times.\nA.4 MIMIC Multi-ECG QA\nThe distribution of the number of ECGs per patient and the sam-\npling time intervals are shown in Figure 4.\n2 3 4 5 6\nNumber of ECGs01000200030004000500060007000Number of Patients7357\n4251\n2769\n1944\n1441Distribution of Number of ECGs per Patient\n2 3 4 5 6\nNumber of ECGs0100020003000400050006000Sampling Interval (days)Sampling Interval by ECG Count\nFigure 4: Statistics of the MIMIC Multi-ECG QA dataset.\nB Case Study\nB.1 Localization\nA case study of ECG localization is presented in Table 10. As ex-\npected, LLaVa-Med and PULSE lack the fine-grained perceptual\ncapability required for second-level abnormal waveform localiza-\ntion. When asked to identify the location of abnormal waveforms,\nthey can only provide lead-level answers.\nTable 10: Case Study of ECG Localization.\nECG:\nQuestion: Examine this ECG and point out where the Premature\nventricular contraction is located.\nTruth: Duration: 2.0s-3.7s\nanyECG-chat: Duration: 1.9s-3.7s\nOthers fail to localize at the second level.\nLLaVa-Med: The Premature ventricular contraction (PVC) is lo-\ncated in the V1-V2 region of the ECG.\nPULSE: V2\nB.2 MIMIC Multi-ECG QA\nA detailed case study of MIMIC Multi-ECG QA is presented in Ta-\nble 12. We utilized QwQ-32B to assess the performance of anyECG-\nchat, LLaVa-Med, and PULSE. The evaluation prompt is provided\nat the bottom of Table 12. To ensure fairness, we did not use the an-\nswers generated by Llama as the gold standard. Instead, we supplied\nthe questions and corresponding reports as references, allowing\n--- Page 12 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\nThe prompt used to generate multi-ECG QA pairs.\nBased on the following ECGs, generate 8 different types of complex open-ended questions that require step-by-step thinking, and\ncorresponding step-by-step answers. The following information is provided: the reports of each ECG and acquisition time. Questions\nshould be about the ECG, in the question, you can choose to indicate the collection time of ECG or not. I need you to ask more\nquestions. The more complex and diverse the question, the better. When the question q or answer a involves time, you need to\nprovide the absolute or relative acquisition time of the ECG in the question.\nFor example, given reports: [[’Sinus tachycardia with PACs’, ’Possible inferior infarct - age undetermined’,\n’Abnormal ECG’], [’Sinus arrhythmia’], [’Sinus rhythm’, ’Probable left ventricular hypertrophy’]] and acqui-\nsition time [’2148-11-12’, ’2149-06-06’, ’2149-12-24’] ,[0, 205, 406] days, generate the following questions:\n% ECG acquisition times are not provided, but the ECGs are presented in sequential order.\nq: Provide a report for each electrocardiogram\na: ECG1: Sinus tachycardia with PACs, possible inferior infarct - age undetermined, abnormal ECG. ECG2:\nSinus arrhythmia. ECG3: Sinus rhythm, probable left ventricular hypertrophy.\nq: What can be found by combining these ECGs\na: Combining these ECGs shows evolving cardiac patterns: initial tachycardia with possible infarct, followed\nby arrhythmia, then normalized rhythm with signs of left ventricular hypertrophy.\nq: What changes occur in the ECGs\na: The ECGs show a shift from sinus tachycardia with PACs and possible infarct to sinus arrhythmia, then to\nnormal sinus rhythm with probable left ventricular hypertrophy.\nq: Possible trends in the future\na: Future ECGs may show progression of left ventricular hypertrophy or stabilization if underlying conditions\nare managed effectively.\n% Absolute Absolute ECG acquisition times are provided.\nq: These electrocardiograms were taken on 2148-11-12, 2149-06-06, and 2149-12-24. Please help me take a look\na: These ECGs from 2148-11-12 to 2149-12-24 show initial abnormalities, transient arrhythmia by mid-2149,\nand possible left ventricular hypertrophy by end of 2149.\n% Relative ECG acquisition times are provided.\nq: The first ECG was collected 400 days ago, the second was collected 200 days ago, and the third was\ncollected most recently. What changes have occurred?\na: Over the past 400 days, ECGs show improvement from sinus tachycardia and possible infarct to normal\nrhythm, with recent signs of left ventricular hypertrophy.\nGiven reports {reports} , and acquisition time {acquisition_time} ,{acquisition_time_relative} , generate 8 different\ntypes of complex open-ended questions that require step-by-step thinking, and corresponding step-by-step answers. Format each\nQA pair in a single line as a JSON dictionary (key “q” for question, and “a” for answer). Do not include any other explanation.\nTable 11: The prompt used to generate multi-ECG QA pairs.\nQwQ to evaluate the quality of the model outputs based solely on\nthis information.B.3 Multi-Turn QA\nA case study of multi-turn QA is presented in Table 13. The example\ndemonstrates how anyECG-chat can be used as a teaching tool for\nphysicians, providing detailed explanations and suggestions based\non the ECG data.\n--- Page 13 ---\nanyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 12: Case Study of MIMIC Multi-ECG QA.\nECG:\nReports from MIMIC-ECG:\n1. Sinus rhythm, Normal ECG\n2. Sinus rhythm, Normal ECG\n3. Sinus rhythm, Anterior T wave changes are nonspecific, Borderline ECG.\nQuestion: Given the ECGs were collected 0, 716, and 1258 days apart, what can be inferred about the cardiac health over this\nperiod?\nAnswer (Llama) (given reports):\nOver 1258 days, the cardiac health appears to have remained largely stable, with sinus rhythm persisting, but with the\nemergence of nonspecific T wave changes, suggesting a need for continued monitoring.\nPredictions (only ECG input)\nanyECG-chat (Score 4):\nOver 1258 days, there was a progression from a normal ECG to a borderline ECG with possible left atrial abnormality and\nnonspecific T wave changes, suggesting a possible decline in cardiac health.\nLLaVa-Med (Score 2):\nIt is difficult to draw specific conclusions about the cardiac health over this period without more information about the patient’s\nmedical history, symptoms, and any treatments or interventions they may have received during this time. However, the fact\nthat the ECGs were collected at different time points can provide insights into the patient’s cardiac health over time. By\ncomparing the ECGs, healthcare professionals can assess changes in the patient’s heart function, identify any potential issues,\nand monitor the effectiveness of any treatments or interventions. It is important to consult with a healthcare professional for a\nthorough evaluation and proper interpretation of the ECGs in the context of the patient’s overall health.\nPULSE (Score 1):\nBased on the ECGs, it can be inferred that there is no significant change in the cardiac health over this period, as the sinus\ntachycardia is consistent across all three ECGs. The presence of sinus tachycardia indicates that the heart is beating at an\nabnormally fast rate, but the underlying rhythm remains normal. The lack of any other abnormal findings or changes in the\nECGs suggests that the cardiac health has not significantly deteriorated or improved over this period.\nPrompt used for QwQ to score:\n% Answer generated by Llama are not used as gold standard for evaluation.\nFor the given question {question} about multiple ECG-QA, and the report {reports} corresponding to each ECG, score the\nanswer below, where 0 means completely incorrect and 5 means completely correct. The answer is: <prediction>.\n--- Page 14 ---\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY xx et al.\nTable 13: Multi-Turn QA Example.\nECG:\nTurn 1:\nUser: Given the ECGs were collected 0, 716, and 1258 days apart, what can be inferred about the cardiac health over this\nperiod?\nanyECG-chat: Over 1258 days, there was a progression from a normal ECG to a borderline ECG with possible left atrial\nabnormality and nonspecific T wave changes, suggesting a possible decline in cardiac health.\nTurn 2:\nUser: What is the reason for the T wave change?\nanyECG-chat: The T wave changes are nonspecific, meaning that they do not have a clear diagnostic significance. They could\nbe due to a variety of factors, including electrolyte imbalance, cardiac ischemia, or other conditions.\nTurn 3:\nUser: If this patient is already 70 years old, based on the current ECG condition, do you have any suggestions?\nanyECG-chat: Yes, considering the patient’s age and the presence of nonspecific T wave changes, it’s essential to monitor\ntheir cardiac health closely and consider preventive measures such as:\n1. Regular follow-up appointments with a cardiologist.\n2. Lifestyle modifications, such as a balanced diet, regular exercise, and stress management.\n3. Control of underlying medical conditions that may contribute to cardiac health.\n4. Consideration of additional diagnostic tests, such as echocardiography or stress testing, to further assess cardiac function\nand identify potential issues early.\nEarly detection and intervention can help prevent or mitigate the progression of cardiac disease in older adults.",
  "text_length": 69286
}