{
  "id": "http://arxiv.org/abs/2506.01097v1",
  "title": "Generic Token Compression in Multimodal Large Language Models from an\n  Explainability Perspective",
  "summary": "Existing Multimodal Large Language Models (MLLMs) process a large number of\nvisual tokens, leading to significant computational costs and inefficiency.\nPrevious works generally assume that all visual tokens are necessary in the\nshallow layers of LLMs, and therefore token compression typically occurs in\nintermediate layers. In contrast, our study reveals an interesting insight:\nwith proper selection, token compression is feasible at the input stage of LLM\nwith negligible performance loss. Specifically, we reveal that explainability\nmethods can effectively evaluate the importance of each visual token with\nrespect to the given instruction, which can well guide the token compression.\nFurthermore, we propose to learn a mapping from the attention map of the first\nLLM layer to the explanation results, thereby avoiding the need for a full\ninference pass and facilitating practical deployment. Interestingly, this\nmapping can be learned using a simple and lightweight convolutional network,\nwhose training is efficient and independent of MLLMs. Extensive experiments on\n10 image and video benchmarks across three leading MLLMs (Qwen2-VL,\nLLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,\ne.g., pruning 50% visual tokens while retaining more than 96% of the original\nperformance across all benchmarks for all these three MLLMs. It also exhibits\nstrong generalization, even when the number of tokens in inference far exceeds\nthat used in training.",
  "authors": [
    "Lei Lei",
    "Jie Gu",
    "Xiaokang Ma",
    "Chu Tang",
    "Jingmin Chen",
    "Tong Xu"
  ],
  "published": "2025-06-01T17:44:16Z",
  "updated": "2025-06-01T17:44:16Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01097v1",
  "full_text": "--- Page 1 ---\nGeneric Token Compression in Multimodal Large\nLanguage Models from an Explainability Perspective\nLei Lei1∗Jie Gu2†Xiaokang Ma2Chu Tang2Jingmin Chen2Tong Xu1\n1University of Science and Technology of China2Rightly Robotics\n{lily168,tongxu}@ustc.edu.cn\n{jgu,xma,chu.tang,jingmin.chen}@rightly.ai\nAbstract\nExisting Multimodal Large Language Models (MLLMs) process a large number of\nvisual tokens, leading to significant computational costs and inefficiency. Previous\nworks generally assume that all visual tokens are necessary in the shallow layers\nof LLMs, and therefore token compression typically occurs in intermediate layers.\nIn contrast, our study reveals an interesting insight: with proper selection, token\ncompression is feasible at the input stage of LLM with negligible performance\nloss. Specifically, we reveal that explainability methods can effectively evaluate the\nimportance of each visual token with respect to the given instruction, which can well\nguide the token compression. Furthermore, we propose to learn a mapping from the\nattention map of the first LLM layer to the explanation results, thereby avoiding the\nneed for a full inference pass and facilitating practical deployment. Interestingly,\nthis mapping can be learned using a simple and lightweight convolutional network,\nwhose training is efficient and independent of MLLMs. Extensive experiments\non10image and video benchmarks across three leading MLLMs (Qwen2-VL,\nLLaV A-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,\ne.g., pruning 50% visual tokens while retaining more than 96% of the original\nperformance across all benchmarks for all these three MLLMs. It also exhibits\nstrong generalization, even when the number of tokens in inference far exceeds\nthat used in training.\n1 Introduction\nWith large language models (LLMs) providing a strong foundation [ 5,29,35,30,3], research\non multimodal large language models (MLLMs) has gained significant momentum [ 27,11,50,2].\nConsiderable progress has been achieved in various image- and video-related tasks [ 10,1]. A common\nparadigm among existing MLLMs is to jointly feed visual tokens (generated by a vision encoder) and\ntextual tokens into the LLM for cross-modal alignment and integration [ 27,50,24]. This paradigm\nintroduces substantial memory and computational overhead due to the high volume of visual tokens,\nwhich grows rapidly with higher resolutions or frame rates [ 39,46]. Consequently, there is a pressing\nneed for effective token compression techniques.\nPrevious exploration of visual token compression methods can be roughly divided into two categories.\nThe first aims to obtain more compact and fewer visual representations (especially for videos) in\na task- or instruction-agnostic manner (independent of LLM) [ 4,42,33,37,32]. We argue that\nvisual representations are an integral part of MLLMs and serve as the foundation for achieving\nstrong performance and generalization. Therefore, it may be more appropriate to design compact\ngeneral-purpose visual representations during the construction of MLLMs, rather than applying\n∗Interns at Rightly Robotics.\n†Corresponding author.\nPreprint. Under review.arXiv:2506.01097v1  [cs.CV]  1 Jun 2025\n--- Page 2 ---\nseparate compression techniques afterward [ 39,38,26]. The second category focuses on selecting\ntokens that are most relevant to the given instruction. FastV [ 8] is a pioneering work that highlights\nthe importance of retaining all shallow-layer visual tokens in LLMs for lossless compression. While\nthis assumption has been adopted by many subsequent studies [ 47,49,41,34,20], we believe it\nremains open to question: are all visual tokens in the shallow layers of LLM truly essential?\nThis paper seeks to answer the question of whether an effective token compression approach prior to\nthe LLM exists but remains undiscovered, or whether it is inherently infeasible. To this end, we first\nexplore the use of explainability methods to assess visual token importance with respect to the given\ninstruction. Explainability methods for transformer-based architecture generally iteratively update a\nrelevance map across layers using gradient-weighted multi-head attentions [ 6,7]. Relevance scores\nindicating the contributions of input tokens to output can be used to rank and prune less important\nvisual tokens for compression. Systematic and detailed experiments conducted on both image and\nvideo data across three representative MLLMs demonstrate the effectiveness of such a compressor.\nThe results indicate that, with appropriate selection, pruning tokens that are not critical to the task at\nthe LLM input stage is indeed feasible. Moreover, unlike previous works motivated by observations\nderived from specific network architectures ( e.g., LLaV A) [ 8,34], which limits their generality and\ntransferability, our explainability-based approach is broadly applicable. Rather than relying on the\nbehaviors of specific models, it leverages the inherent characteristics of the applied model.\nAfter validating that the explanation results are effective compression indicators, a lightweight model\ncapable of generating an alternative to the relevance map is further needed to enable efficient and\npractical deployment. Interestingly, this goal can be achieved by training a simple fully convolutional\nnetwork that predicts relevance based on the first-layer attention map of the LLM. The training\nprocess is highly efficient ( e.g., training a 5-layer network using only 10K image data) and does\nnot involve any changes to the MLLM itself. Using the predicted relevance, token compression can\nbe performed prior to the prefill phase with negligible extra computational cost. As a result, both\ncomputational and memory overhead during inference are significantly reduced, with no modifications\nrequired to either the prefill or decode phases. Last but not least, our approach generalizes well across\nvarious architectures, benefiting from the broadly applicable nature of explainability methods and the\nMLLM-agnostic design of the auxiliary training.\nTo thoroughly assess the capability of our approach, we apply it to three prominent models with\ndifferent architectures and visual representations: VILA1.5, LLaV A-OneVision, and Qwen2-VL. We\ninclude ten widely used image and video benchmarks that span a wide range of visual complexities and\ntasks, ensuring a comprehensive evaluation. Notably, our method achieves significant compression\nby pruning 75% of video tokens while retaining more than 97% of the original performance across all\nbenchmarks for both VILA1.5 and LLaV A-OneVision. It also performs well on image tasks, where\nup to 50% of image tokens can be removed with only a minimal performance drop: maintaining over\n96% of baseline performance for Qwen2-VL and LLaV A-OneVision.\nIn summary, the contributions of the work are threefold: (i) reveal that explainability methods can well\nevaluate the importance of visual tokens, enabling effective token compression. (ii) propose a highly\nefficient token compressor by learning from explanation results. It allows token compression to be\nperformed before the LLM, significantly reducing inference costs at both the prefill and decode phases.\n(iii) Validate the effectiveness and generalization of our method through extensive experiments on a\nwide range of image and video benchmarks across different MLLMs.\n2 Related Work\nMultimodal Large Language Models. Benefiting from advancements in large language models\n(LLMs) [ 29,35,3], multimodal large language models (MLLMs) have gained considerable attention\ndue to the powerful ability in multi-modal understanding and reasoning [ 27,11,2,10,1]. Recent\nadvances [ 22,39,46] tend to handle images with higher resolution and videos with more frames,\nwhich significantly increases the number of visual tokens and thus the computational burden. This\nreveals the necessity for token compression strategies that can balance efficiency and effectiveness.\nOur work proposes a generic token compression method at the LLM input stage which significantly\nreduces computational costs without sacrificing performance.\nVisual Token Compression. Existing visual token compression methods for MLLMs can be broadly\ncategorized into: task/instruction-agnostic compression [ 4,42,33,37,32] and task/instruction-related\n2\n--- Page 3 ---\nFigure 1: Overview of our method . The top portion illustrates the details of our explainability-based com-\npression approach: an explainability method can reveal the important visual tokens (first row, Section 3.2); a\nlightweight model can then be trained to approximate this explainability and serve as a compression indicator\n(second row, Section 3.3). The bottom portion shows a general inference framework for MLLMs, where the\nresulting compressor is applied at the input stage of the LLM.\ncompression [ 8,49,41,34,20]. The first category of methods typically introduces additional modules\nto merge redundant visual tokens based on the similarities between them, addressing the limitations\nof existing models. However, many recent works have developed techniques to obtain more compact\nvisual representations when building MLLMs [ 39,38,26]. We believe that task/instruction-related\ncompression offers greater potential for reducing the number of visual tokens. FastV [ 8] represents a\ntypical method of the second category, which rely on shallow-layer attention maps of the LLM for\ncompression. In this work, we explores the feasibility of an effective token compression prior to the\nLLM. Such a method is not only task/instruction-related, but also remains independent of the MLLM\narchitecture, making it broadly applicable and generalizable.\n3 Method\n3.1 Background and Motivation\nCurrent Multimodal Large Language Models (MLLMs) typically follow a framework in which\na vision encoder is incorporated to encode visual signals into a sequence of tokens [ 27,2,11].\nSpecifically, multiple frames or patches are sampled from a video or an image, and their corresponding\nvisual tokens are encoded. These visual tokens are then flattened and concatenated with textual prompt\ntokens before being fed into a Large Language Model (LLM) to generate a response. Formally, let V\nbe the video or image, and let VM andLM represent the vision encoder and the language model,\nrespectively. The visual token embeddings Evcan be represented as Ev=VM(V)∈RNv×C,\nwhere Nvis the number of visual tokens and Cis the feature dimension.3LetEs∈RNs×Cand\nEu∈RNu×Cdenote the token embeddings of the system prompt and user instruction, respectively.\nBy feeding Evtogether with EsandEuinto the LLM, a textual response is generated, i.e.,Y=\nLM(Es, Ev, Eu).\nEvcan be considered as general-purpose representations of visual signals that are task/instruction-\nagnostic. Recent advances have developed techniques to reduce the number of visual tokens to obtain\na more compact Evwhen building MLLMs [ 39,38,26]. Therefore, instead of further compressing\nEvin isolation (as in [ 4,32]), our objective is to assess the importance of each token in Evwith\n3A cross-modal projector is commonly employed in such architectures. For notational simplicity, we denote\nboth the vision encoder and the projector by VM .\n3\n--- Page 4 ---\nFigure 2: Visualization of Rvobtained via the explainability method (left) and the corresponding token\npruning results (right) . Based on Rv, the top 50% of visual tokens are retained, while the remaining 50% are\npruned (masked in white). All three MLLMs generate the correct answer using only the retained tokens.\nrespect to a given instruction, and subsequently prune those that are less essential. Moreover, we\ninvestigate how to perform token compression prior to LLM computation, i.e., first compressing\nEvtoˆEv∈RˆNv×Cand then computing Y=LM(Es,ˆEv, Eu), where ˆNvis much smaller than\nNv. In contrast to previous approaches [ 8,41,20], our method does not require any modifications\nto the prefill and decode phases during inference, and computational and memory overhead can be\nsignificantly reduced in both phases.\nThe details of our approach are presented below. In Section 3.2, we introduce explainability methods\nto assess the importance of visual tokens and guide token compression. A learning mechanism is\nthen proposed to predict the explanation results in Section 3.3, which ultimately enables effective\ntoken compression at the LLM input stage.\n3.2 Token Compression with Explainability\nTo reduce instruction-agnostic redundancy at the token level, we need to estimate the contribution\nof each visual token to the model response. Explainability methods for LLMs facilitate this goal by\ngenerating a relevance map through the integration of attention weights and corresponding gradients,\neffectively revealing where the model genuinely focuses. The resulting relevance map highlights the\ncontributions, enabling us to rank and prune these visual tokens accordingly. The pipeline for this\nsection is shown in the first row of Figure 1.\nRelevance Maps by Explainability Method. We adopt a generic explainability method similar to\n[43,6] to compute the relevance of the response-to-vision. The relevance values reveal the distribution\nof importance across visual tokens utilized by the LLM. Without loss of generality, assume that\nthe LLM in an MLLM has Llayers, and denote the generated sequence of textual tokens as Y=\n{y0, y1, . . . , y T−1}. Specifically, we trace back the semantic relevance flow from generated tokens to\nraw visual inputs. For each ytat the t-th generation step, the relevance map Rtis first initialized as\nan identity matrix and then iteratively updated across layers. Denote Al\ntand∇Al\ntas the multi-head\nattention map and the corresponding gradients in the l-th layer, obtained during the forward and\nbackward passes, respectively. Rtis updated as\nRt=Rt+Eh(Al\nt⊙ ∇Al\nt)·Rt, (1)\nwhere⊙represents Hadamard product, and Ehis the mean across the heads dimension. The update is\nperformed from the 0-th layer to the last layer. In the end, the relevance of ytto visual signals can be\nextracted by indexing the corresponding positions in the last row of Rt, that is, Rt[−1, Ns:Ns+Nv].\nFinally, we aggregate visual relevance across all time steps tby averaging, obtaining the overall visual\nrelevance scores Rv∈R1×Nvwith respect to the current response. This well-grounded importance\nassessment Rvcan then be used to rank and select visual tokens.\nVisual Token Compression Using Relevance Scores. The importance of visual tokens related to the\ninstruction can be ranked according to Rv. We can prune the less important visual tokens down to a\ntarget count of ˆNv, resulting in compressed token embeddings ˆEvas LLM input.\n4\n--- Page 5 ---\nObservation. We visualize Rvand the corresponding token pruning results for LLaV A-OneVision,\nQwen2-VL, and VILA1.5 in Figure 2.4Although differences exist in the visualizations due to varia-\ntions in how each MLLM processes visual input, there are notable commonalities. All three MLLMs\nfocus on the textual regions within the video, as these are most relevant to the question (querying\nkeywords appearing in the video). Moreover, experimental results show that retaining 50% of the\noriginal visual tokens based on Rvpreserves over 98% of the performance on image benchmarks and\n99% on video benchmarks (see Section 4.2 for details). We draw the following conclusion: the expla-\nnation results faithfully capture the visual information essential for the MLLM to answer the question,\nand retaining only the corresponding visual tokens does not compromise model performance.\n3.3 Explainability-based Compressor Learning\nThe relevance map offers valuable insights into achieving token compression at the LLM input level.\nHowever, its practical application is limited by the fact that Rvis derived post-hoc – only after the\nmodel has already generated the output. To address this limitation, we propose to approximate Rv\nusing a standalone module trained independently. This module learns to capture attention patterns\nand generate relevance estimates ˜Rv, ultimately allowing token compression to be performed before\nLLM inference. The pipeline for this section is shown in the second row of Figure 1.\nModel Architecture. As shown in Eq. 1, the relevance map is essentially obtained by aggregating\nattention maps. Consequently, learning a mapping from attention maps to relevance maps could be\na promising approach. Interestingly yet reasonably, we find in practice that this mapping can be\nsatisfactorily learned using a simple convolutional network based on the first-layer attention of LLMs.\nFormally, let A0be the first-layer attention map. Due to the nature of causal attention, A0is a lower\ntriangular matrix. Similar to [ 8,51,49], we focus specifically on the attention scores that visual\ntokens receive from textual instruction tokens. Accordingly, we extract the submap A0\nu→v∈RNu×Nv\nby indexing the corresponding positions. We then average the Nuscores for each visual token to\nobtain a compact representation, resulting in A0\nv∈R1×Nv.5\nThis averaged attention vector A0\nvis subsequently fed into a 1D convolutional model fθto predict\nvisual relevance:\n˜Rv=fθ(A0\nv). (2)\nNote that a softmax operation is applied at the end of fθ, making ˜Rva probability distribution. In\naddition, a separate instance of fθis used for each MLLM, because it is trained to approximate the\nexplainability patterns specific to that particular MLLM.\nTraining Objectives. Rvis processed through masking and normalization to form the training label\nR∗\nv. Specifically, we first set the values at the bottom 50% of Rvto zero, which provides a clearer\nsupervisory signal and reduces interference [ 18]. Normalization is then applied to ensure a valid\nprobability distribution. Since the raw values in Rvare close to each other, applying softmax would\nresult in a near-uniform distribution, which weakens the supervision signal. Instead, we normalize\nRvthrough division of each score by the total, better preserving the relative differences. Finally,\ngiven R∗\nvand ˜Rv, the Kullback–Leibler (KL) divergence is used to measure the difference, defining\nthe loss function:\nLKL=KL(R∗\nv||˜Rv). (3)\nOberservation. The learned fθcan be seamlessly integrated into the MLLM inference pipeline to\ngenerate ˜Rv, which can guide the token compression. As shown in Figure 1, a visualization of Rvand\n˜Rvis given in the first and second rows, along with their corresponding pruning results, respectively.\nOne can see that ˜Rvclosely resembles Rv. Important visual regions related to the question ( i.e., the\ntextual regions) are highlighted in both maps. This observation provides evidence that the lightweight\nmodel fθcan indeed be efficiently and effectively trained to approximate Rv, allowing lossless token\ncompression at the LLM input stage. Quantitative experimental results further support this conclusion\n(see Section 4.3 for details).\n4More visualization cases are presented in Supplemental Material.\n5We omit the head dimension for notational simplicity.\n5\n--- Page 6 ---\n4 Experiments\n4.1 Experimental Setup\nModels. Experiments are conducted on three leading MLLMs with different architectures for\nextensive validation, i.e., LLaV A-OneVision-7B [ 22], Qwen2-VL-7B [ 39] and VILA1.5-8B [ 28].\nThese models are all highly representative. LLaV A-OneVision and Qwen2-VL take a significant step\ntoward processing visual inputs of arbitrary resolution and length. In particular, Qwen2-VL achieves\nmore compact visual representations by introducing a dynamic resolution mechanism and designing\ntoken aggregation modules. VILA1.5 represents a class of methods that encode images or video\nframes into a fixed number of tokens.\nBenchmarks. We thoroughly evaluate our method on 10 widely used image and video benchmarks.\nFor image tasks, MME [ 16] (all-round capability), MMStar [ 9] (data contamination), MMVet [ 44]\n(subjective evaluation), and SEED-Bench [ 23] (all-round capability) are included, covering various\naspects of MLLM performance.\nFor video evaluation, we select Video-MME(wo sub.) [ 17], MVBench [ 25], MMBench-Video [ 15],\nNExT-QA [ 40], and ActivityNetQA [ 45], covering a wide range of dimensions. Video-MME contains\nvideos with varying durations from diverse domains. MVBench evaluates temporal understanding\nthrough dynamic video tasks that cannot be solved with static frames. MMBench-Video com-\nprises long YouTube videos paired with open-ended questions. NExT-QA features multiple-choice\nand open-ended questions, focusing on causal and temporal action reasoning, and common scene\ncomprehension. ActivityNetQA consists of 58,000 QA pairs derived from 5,800 complex web videos.\nImplemantation Details. Generating Rv. Our implementation employs eager attention, allowing\naccess to full-layer attention maps required by the explainability method [ 6]. Compared to FlashAt-\ntention [ 13] and inference based on KV cache [ 31], eager attention requires more memory. To avoid\nout-of-memory errors and ensure efficient data generation, we limit the number of visual tokens\nto approximately 1500 per sample. Specifically, for video inputs, LLaV A-OneVision, VILA and\nQwen2-VL are all set to sample 8 frames, resulting in 1569, 1568 and 1296 visual tokens per video,\nrespectively. For image inputs, LLaV A-OneVision and Qwen2-VL use similar image resolutions,\nresulting in 1500 and 1849 visual tokens per image, respectively. VILA always processes an image as\n196 tokens, eliminating the need for additional configuration. The generated Rvcan be used directly\nto guide token pruning or to train fθ.\nTraining fθ.fθis implemented as a five-layer fully convolutional network with channel dimensions\nof32,64,128,256,and512. Each layer employs a 1D depthwise separable convolution [ 12],i.e., a\ndepthwise convolution with a kernel size of 3 followed by a pointwise convolution. An additional\npointwise convolution layer is applied at the end for channel aggregation. The network is trained\nby using Adam [ 21] with default settings and a batch size of 128. Training data is collected from\nopen-source datasets: a subset of LLaV A-Video [ 48] for videos and a subset of Infinity-MM [ 19]\nfor images, each containing approximately 10K samples. Note that fθis specific to MLLM, so each\nMLLM generates its own A0\nvandRvbased on the input image- or video-text pair for training. Refer\nto the Supplemental Material for more details about the training data. The training is performed for\nroughly 100 epochs, taking about half an hour for image data and less than four hours for video data\non a single A100 GPU.\nInference . The learned fθcan be seamlessly integrated into existing inference pipelines (no modifi-\ncations are required for the prefill and decode phases of LLM inference). More interestingly, fθis\ncapable of processing longer A0\nvthanks to the fully convolution design. That is, our compression\nmethod can handle larger images and longer videos, even though the visual token number is limited\nto approximately 1500 during training. Corresponding experiments have been conducted. In these ex-\nperiments, Qwen2-VL dynamically processes both images (with ‘max_pixels’ set to half of its default\nvalue) and videos (with ‘VIDEO_MAX_PIXELS’ and ‘FPS_MAX_FRAMES’ set to 384×28×28\nand 32, respectively). These configurations are set to accommodate hardware resource constraints.\nLLaV A-OneVision also processes images dynamically with default settings, while sampling 32 frames\nper video as in [ 20] for a fair comparison. For VILA, the input image size cannot be changed, and\nthe number of input video frames is set to 16. All evaluations are performed using VLMEvalKit [ 14].\n6\n--- Page 7 ---\nModel MethodRetention\nRatioImage BenchmarkAvg.(%)Video BenchmarkAvg.(%)\nMME MMStar MMVet Video-MME MVBench MMBench-V\nLlava-\nOneVisionVanilla 100% 1997.7 60.5 48.7 100 53.6 41.2 0.41 100\nGAE50% 1974.2 59.7 47.2 98.1 54.3 41.1 0.40 99.5\n25% 1977.3 59.3 47.0 97.8 53.8 40.9 0.40 99.1\nQwen2-VLVanilla 100% 2295.1 60.4 54.0 100 50.4 51.0 1.23 100\nGAE50% 2297.1 60.3 53.2 99.5 51.0 50.7 1.19 99.1\n25% 2299.1 58.7 51.7 97.7 50.3 49.7 1.17 97.5\nVILA1.5Vanilla 100% 1700.3 38.7 39.3 100 47.3 34.0 1.29 100\nGAE50% 1740.5 37.2 38.0 98.4 47.9 34.2 1.26 99.8\n25% 1722.1 35.7 35.6 94.7 47.1 35.1 1.28 100.7\nTable 1: The relevance Rveffectively guides token compression under different retention ratios. Avg.\nmeans the average of performance preservation ratios across all image benchmarks.\nModel MethodRetention\nRatioMME MMStar MMVet SEED Avg.(%)\nLlava-\nOneVisionVanilla 100% 1997.7 60.5 48.7 76.7 100\nFastV50%1974.0 56.8 46.1 75.2 96.3\nOurs 1980.8 57.5 46.2 75.3 96.8\nFastV25%1940.2 51.7 36.8 71.1 87.7\nOurs 1965.9 52.1 41.8 72.7 91.3\nQwen2-VLVanilla 100% 2295.1 60.4 54.0 75.8 100\nFastV50%2283.4 55.5 52.2 73.2 96.1\nOurs 2288.3 55.9 51.9 73.2 96.2\nFastV25%2276.5 51.6 45.5 68.4 89.8\nOurs 2280.9 51.8 47.3 67.9 90.6\nTable 2: Compare explainability-based compressor on image benchmarks. FastV performs token compres-\nsion at the 4-th layer of LLM (as suggested by its optimal configuration), while our method compresses tokens\nbefore feeding them into LLM. As a result, even under the same retention ratio, our method achieves a noticeably\nlower average number of retained tokens across all LLM layers, leading to higher computational efficiency.\n4.2 Effectiveness of Compression with Explainability\nWe conduct experiments to verify whether the explanation results can guide token compression, i.e.,\ncompressing EvtoˆEvaccording to Rvand then feeding ˆEvintoLM to generate a response. To\nthoroughly evaluate effectiveness and generalization, we apply the compression method to three\nstate-of-the-art MLLMs and test them on three image and three video benchmarks.\nIn Table 1, we report the quantitative results of MLLMs with the retention ratio of visual tokens set\nto 50% and 25% after compression. The strong performance across multiple models and datasets\ndemonstrates the effectiveness and broad applicability of such an explainability-based token compres-\nsor. For Qwen2-VL, a reduction of 50% in visual tokens maintains more than 99% of the original\nperformance on both image and video tasks. For LLaV A-OneVision, the model retains 99.1% of its\nvanilla performance on video tasks even when only 25% of the tokens are retained. VILA reduces\nthe number of visual tokens to just 98 per image or frame with 50% retention, yet it still achieves\n98% of the original performance on images and nearly unchanged performance on videos. These\nobservations indicate that token compression based on relevance Rveffectively preserves the visual\ntokens essential for MLLMs to answer the question. In addition, it can be seen that post-compression\nperformance tends to be better preserved on video tasks than on image tasks. This is probably\nbecause videos contain more redundant visual content that is irrelevant to the instruction compared to\nimages. The higher redundancy in videos implies greater room for visual token reduction. Similar\nobservations can be found in [8].\n4.3 Effectiveness of Explainability-based Compressor Learning\nThe performance of the ˜Rv-guided token compressor is evaluated in this section. ˜Rvis generated by\nthe learned fθ, and the token pruning is performed accordingly before the LLM computation. Four\nimage and six video benchmarks are included for evaluation.\n7\n--- Page 8 ---\nModel MethodRetention\nRatioVideo-MME MVBenchMMBench-\nVideoNext-QAActivity-QA Avg.(%)multi-choice open-ended\nLlava-\nOneVisionVanilla 100% 53.6 41.2 0.41 79.2 49.0 56.9 100\nFastV50%53.4 39.5 0.43 78.6 49.4 56.5 99.9\nOurs 53.4 40.5 0.43 78.6 49.7 56.5 100.4\nFastV25%51.1 39.0 0.40 77.6 48.6 53.9 96.6\nOurs 51.3 39.0 0.42 77.0 49.0 54.5 97.3\nFastV10%47.6 37.9 0.34 75.0 46.2 49.5 90.0\nOurs 47.1 37.4 0.40 76.5 45.6 51.6 92.8\nQwen2-VLVanilla 100% 50.4 51.0 1.23 76.8 45.5 53.6 100\nFastV50%49.7 50.2 1.17 76.6 45.9 51.4 98.1\nOurs 50.0 49.8 1.18 75.6 45.9 52.4 98.3\nFastV25%48.0 48.3 1.08 75.4 43.0 45.5 92.6\nOurs 48.1 46.7 1.11 74.2 44.3 50.5 94.2\nFastV10%45.9 42.8 0.97 72.5 42.9 45.3 87.9\nOurs 46.1 42.5 1.00 72.0 43.3 47.5 88.9\nVILA1.5Vanilla 100% 47.3 34.0 1.29 69.9 46.2 55.6 100\nFastV50%46.4 34.8 1.28 69.7 45.7 55.0 99.6\nOurs 47.6 35.2 1.25 70.3 46.4 55.4 100.3\nFastV25%45.3 34.7 1.24 68.8 45.4 54.2 98.0\nOurs 45.5 35.6 1.22 69.4 46.4 54.8 99.0\nFastV10%43.9 34.2 1.13 66.3 43.6 52.1 94.0\nOurs 43.6 35.0 1.14 67.0 44.7 53.0 95.3\nTable 3: Compare explainability-based compressor on video benchmarks. Compared to FastV , which\ncompresses tokens in the shallow layer of LLM, our prior-to-LLM compression leads to fewer visual tokens\nacross all layers and even better performance.\nPerformance Comparison. FastV [ 8] is selected for a comprehensive comparison due to its excellent\nperformance and wide applicability. The token pruning is performed at the 4-th layer of LLM in\nour setup. Table 2 presents the results of LLaV A-OneVision and Qwen2-VL under different token\ncompression retention ratios on image benchmarks. We exclude VILA here because it uses a fixed\nand relatively small number of image tokens, making compression less meaningful. As shown in\nthe table, at a retention rate of 50%, our compressor demonstrates overall superiority over FastV ,\nachieving average improvements of 0.5% and 0.1% across all benchmarks for Llava-OneVision and\nQwen2-VL, respectively. When the retention rate is further reduced to 25%, the performance gains\nincrease to 3.6% and 0.8%, indicating enhanced robustness under higher compression rates.\nIn Table 3, we evaluate the compression performance of LLaV A-OneVision, Qwen2-VL, and VILA\non video benchmarks. A lower retention ratio ( i.e., 10%) is also considered, as videos usually contain\nhigher information redundancy. We make several observations. First, our compressor consistently\noutperforms FastV , regardless of the model and retention ratio. Both LLaV A-OneVision and VILA\nare able to maintain 100% performance when 50% of the visual tokens are pruned. Second, among\nthe three models, VILA exhibits the smallest performance degradation, while Qwen2-VL shows the\nlargest. This is intriguing and may be because the attention patterns in Qwen2-VL are relatively\nharder to capture. Finally, comparing the results in Tables 1, 2, and 3, the performance degradation\nfrom the Rv-guided compressor to the ˜Rv-guided compressor is more pronounced in image tasks.\nThis is likely also due to the greater redundancy in videos, which reduces the learning difficulty.\nApplying to Larger Images and Longer Videos. Figure 3 presents the results of this experiment.\nThe first two sub-figures show the average compression performance on 4 image benchmarks and\n6 video benchmarks, respectively. Our method still consistently outperforms FastV , demonstrating\nits capability to handle larger images and longer videos. For example, although it is trained only\non videos with 8 frames, it can be directly applied to token compression for videos with 32 frames,\nachieving excellent performance (see Section 4.1 for implementation details). Detailed comparison\nresults on these 10 benchmarks are provided in the Supplemental Material.\nThe last two sub-figures show the comparisons on two challenging benchmarks, i.e., MMStar and\nMVBench, respectively. The experiment is conducted on LLaV A-OneVision, with original-resolution\nimages and 32-frame videos, at a retention rate of 25%. Several concurrent methods are introduced for\n8\n--- Page 9 ---\nFigure 3: Comparison results on larger images and longer videos . Performance preservation ratio denotes\nthe proportion of the performance retained relative to the Vanilla model. The average retention ratio refers to\nthe mean proportion of retained tokens across all LLM layers. The first two sub-figures illustrate the average\nperformance preservation of MLLMs across image and video benchmarks. The last two sub-figures show the\ncomparisons with competitive methods based on LLaV A-OneVision on two challenging benchmarks.\nModel MethodRetention\nRatioImage Benchmark Video BenchmarkAvg.(%)MME MMStar MMVet Video-MME MVBench MMBench-V\nLlava-\nOneVisionVanilla 100% 1997.7 60.5 48.7 53.6 41.2 0.41 100\nMean-weighted50%1974.5 58.5 45.9 53.6 40.8 0.39 97.3\nGrad-weighted 1974.2 59.7 47.2 54.3 41.1 0.40 98.8\nQwen2-VLVanilla 100% 2295.1 60.4 54.0 50.4 51.0 1.23 100\nMean-weighted50%2300.6 58.2 49.2 49.9 49.9 1.15 96.3\nGrad-weighted 2297.1 60.3 53.2 51.0 50.7 1.19 99.3\nVILA1.5Vanilla 100% 1700.3 38.7 39.3 47.3 34.0 1.29 100\nMean-weighted50%1720.8 38.0 34.2 48.0 34.1 1.20 96.9\nGrad-weighted 1740.5 37.2 38.0 47.9 34.2 1.26 99.1\nTable 4: Ablation study on the aggregation strategies in explainability methods. We evaluate two strategies\nfor aggregating multi-head attention maps—gradient-weighted summation and simple averaging—to generate\nrelevance maps for guiding token compression, across both video and image benchmarks.\ncomparison: PruneVID, FastVID, VisionZip, and PyramidDrop. Our approach achieves state-of-the-\nart performance, even when compared with methods specifically designed for videos. These methods\nare not included in the above comprehensive comparison because they are either too specialized\n(designed exclusively for videos or incompatible with certain aggregation modules in MLLMs), or\nlack publicly available code that prevents us from evaluating their performance across more MLLMs.\nBeyond the superior performance, it is worth noting that our lightweight compressor significantly\nboosts the efficiency of MLLM inference while introducing negligible additional computational costs.\nAn efficiency evaluation is provided in the Supplemental Material.\n4.4 Ablation Study\nAs shown in Eq. 1, the relevance map is updated based on the aggregation of the attention maps\nin each layer. This aggregation involves averaging over the head dimension and can take the form\nof either a simple average (as used in [ 49]), or a weighted average using gradients (used in our\napproach). Table 4 shows the performance comparison between these two aggregation strategies.\nOne can see that employing gradient-weighted aggregation to generate Rvfor token compression\nperforms consistently better, whether on image or video benchmarks. This suggests that gradient-\nweighted aggregation produces higher-quality relevance assessments of visual tokens with respect to\nthe response. A reasonable explanation is that attention heads differ in their importance and relevance,\nand taking a simple average across heads may result in distorted relevance maps [36].\n5 Conclusion\nIn this work, we demonstrate the feasibility of visual token compression at the LLM input stage.\nExplainability methods generate relevance scores of visual tokens to output quantifying the contri-\nbution of each visual token. Experimental results indicate that the relevance scores well evaluate\nthe importance of visual tokens, which can be used for effective token compression. To enable\nefficient and practical deployment, we employ a simple convolutional network to learn a mapping\nfrom the first-layer attention maps of the LLM to the explainability-derived relevance scores. Using\n9\n--- Page 10 ---\nthe predicted relevance scores from lightweight model, token compression can be performed prior to\nthe LLM with no modifications to MLLMs. Extensive experiments demonstrate the effectiveness and\ngeneralizability of our generic token compression method. Since the relevance scores are obtained\nvia backward computations, their generation is resource-intensive. This poses a challenge in scaling\nthe compressor training to high-resolution images or long video sequences. In future work, we aim to\nleverage stronger compressor models to improve performance and further explore the use of relevance\nscores to guide token compression during training.\nReferences\n[1]Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, et al. Gemini: A family\nof highly capable multimodal models. arXiv , 2023.\n[2]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, et al. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv , 2023.\n[3]Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, et al. Deepseek LLM: scaling open-\nsource language models with longtermism. arXiv , 2024.\n[4]Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, et al. Token merging: Your vit\nbut faster. In ICLR , 2023.\n[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are\nfew-shot learners. arXiv , 2020.\n[6]Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting\nbi-modal and encoder-decoder transformers. In ICCV , 2021.\n[7]Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization.\nInCVPR , pages 782–791, 2021.\n[8]Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, et al. An image is worth 1/2 tokens after\nlayer 2: Plug-and-play inference acceleration for large vision-language models. In ECCV , 2024.\n[9]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, et al. Are we on the right way for evaluating\nlarge vision-language models? In NeurIPS , 2024.\n[10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, et al. How far are we to gpt-4v? closing the\ngap to commercial multimodal models with open-source suites. arXiv , 2024.\n[11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, et al. Internvl: Scaling up vision foundation\nmodels and aligning for generic visual-linguistic tasks. arXiv , 2023.\n[12] François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR ,\n2017.\n[13] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, et al. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In NeurIPS , 2022.\n[14] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, et al. Vlmevalkit: An open-source\ntoolkit for evaluating large multi-modality models. In MM, 2024.\n[15] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, et al. Mmbench-video: A long-form\nmulti-shot benchmark for holistic video understanding. In NeurIPS , 2024.\n[16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, et al. MME: A comprehensive evaluation\nbenchmark for multimodal large language models. arXiv , 2023.\n[17] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, et al. Video-mme: The first-ever comprehensive\nevaluation benchmark of multi-modal llms in video analysis. arXiv , 2024.\n[18] Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, et al. Exploiting behavioral consistence for\nuniversal user representation. In AAAI , 2021.\n[19] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, et al. Infinity-mm: Scaling multimodal\nperformance with large-scale and high-quality instruction data. arXiv , 2024.\n[20] Xiaohu Huang, Hao Zhou, and Kai Han. Prunevid: Visual token pruning for efficient video\nlarge language models. arXiv , 2024.\n[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR ,\n2015.\n10\n--- Page 11 ---\n[22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, et al. Llava-onevision: Easy visual task\ntransfer. arXiv , 2024.\n[23] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, et al. Seed-bench: Benchmarking multimodal\nllms with generative comprehension. arXiv , 2023.\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In ICML , 2023.\n[25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, et al. Mvbench: A comprehensive multi-modal\nvideo understanding benchmark. In CVPR , 2024.\n[26] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large\nlanguage models. In ECCV , 2024.\n[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In\nNeurIPS , 2023.\n[28] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, et al. NVILA: efficient frontier visual\nlanguage models. arXiv , 2024.\n[29] OpenAI. GPT-4 technical report. arXiv , 2023.\n[30] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, et al. Instruction tuning with GPT-4.\narXiv , 2023.\n[31] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, et al. Efficiently scaling\ntransformer inference. In Conf. Mach. Learn. Syst. , 2023.\n[32] Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, et al. Fastvid: Dynamic density pruning for\nfast video large language models. arXiv , 2025.\n[33] Leqi Shen, Tianxiang Hao, Tao He, Sicheng Zhao, et al. Tempme: Video temporal token\nmerging for efficient text-video retrieval. In ICLR , 2025.\n[34] Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, et al. Tokencarve: Information-preserving\nvisual token compression in multimodal large language models. arXiv , 2025.\n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, et al. Llama: Open and\nefficient foundation language models. arXiv , 2023.\n[36] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, et al. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL, 2019.\n[37] Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, et al. FOLDER: accelerating\nmulti-modal large language models with enhanced performance. arXiv , 2025.\n[38] Han Wang, Yuxiang Nie, Yongjie Ye, Guanyu Deng, et al. Dynamic-vlm: Simple dynamic\nvisual token compression for videollm. arXiv , 2024.\n[39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, et al. Qwen2-vl: Enhancing vision-language\nmodel’s perception of the world at any resolution. arXiv , 2024.\n[40] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-\nanswering to explaining temporal actions. In CVPR , 2021.\n[41] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, et al. Pyramiddrop: Accelerating your large\nvision-language models via pyramid visual redundancy reduction. arXiv , 2024.\n[42] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, et al. Visionzip: Longer is better\nbut not necessary in vision language models. arXiv , 2024.\n[43] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, et al. Deco: Decoupling token compression from\nsemantic abstraction in multimodal large language models. arXiv , 2024.\n[44] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, et al. Mm-vet: Evaluating large\nmultimodal models for integrated capabilities. In ICML , 2024.\n[45] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, et al. Activitynet-qa: A dataset for understanding\ncomplex web videos via question answering. In AAAI , 2019.\n[46] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Internlm-xcomposer-2.5: A versatile\nlarge vision language model supporting long-contextual input and output. arXiv , 2024.\n[47] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, et al. Sparsevlm: Visual token\nsparsification for efficient vision-language model inference. arXiv , 2024.\n11\n--- Page 12 ---\n[48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, et al. Video instruction tuning with synthetic data.\narXiv , 2024.\n[49] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, et al. A stitch in time saves nine: Small\nVLM is a precise guidance for accelerating large vlms. arXiv , 2024.\n[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, et al. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. In ICLR , 2024.\n[51] Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, and Sheng Guo. Focusllava: A coarse-to-fine\napproach for efficient and effective visual token compression. arXiv , 2024.\n12\n--- Page 13 ---\nA More Visualization Results\nA.1 Visualization Results Across Different MLLMs\nWe present visualization results for LLaV A-OneVision, Qwen2-VL, and VILA1.5 on both video and\nimage inputs in Figures 4-8. Given an input image or video V, we first show the visual relevance\nscores Rvwith respect to the current response obtained using an explainability method. Based on\nRv, we visualize the results of token pruning at 50% and 25% retention ratios (labeled as Top-50%\ncompressed ˆVand Top-25% compressed ˆVin the figures). Then, we visualize the pruning results\nproduced by our trained compressor ( fθ) under the same compression ratios (labeled as Top-50%\ncompressedˆ˜Vand Top-25% compressedˆ˜Vin the figures).\nFigure 4: Video Input Visualizations for LLaV A-OneVision .\nFigure 5: Image Input Visualizations for Llava-OneVision .\n13\n--- Page 14 ---\nFigure 6: Video Input Visualizations for Qwen2-VL .\nFigure 7: Image Input Visualizations for Qwen2-VL .\nA.2 Case Study: Explainability Reveals Instruction-Related Visual Tokens\nTo demonstrate the effectiveness of explainability methods in identifying visual tokens that are highly\nrelevant to user instructions, we present two case studies covering both video and image inputs.\nGiven the same input V, the explainability method generates visual relevance scores Rvthat se-\nlectively emphasize different visual tokens according to varying user instructions. As shown in\nFigure 9, when the user instruction specifically targets clothing-related information, the visual tokens\ncorresponding to the person’s clothing in the video obtain higher relevance scores compared to\ninstructions requesting a general summary. Similarly, in Figure 10, visual tokens relevant to the user\ninstruction exhibit higher relevance scores. When the user instruction specifies excluding the Ford\nF150, the visual attention shifts primarily to the other two columns. In contrast, when the instruction\nhighlights the highest fueling cost, the Ford F150 column attracts nearly all the attention.\nFrom a visualization standpoint, we further corroborate that the explanation results faithfully reflect\nthe critical visual information required by the MLLM to answer the question.\n14\n--- Page 15 ---\nFigure 8: Video Input Visualizations for VILA .\nB Details of Data for Training fθ\nWe train our explainability-based compressor based on subsets sampled from high-quality open-source\ndatasets. First, the details of the sampling are as follows:\nImage Dataset. For training the compressor used in image tasks, we sample a subset of Infinity-MM\nthat ensures high quality and diversity. The training set primarily consists of data used during Stage\n4, including 9k samples randomly sampled from the Data Generated by GPT-4 subset and 4k from\nSynthetic Data .\nVideo Dataset. For training the compressor used in video tasks, we sample a subset of LLaV A-Video.\nSpecifically, we include 7k samples from LLaVA-Video , 6k from NeXT-QA and 4k from ActivityNetQA .\nNote that the training sets of NeXT-QA andActivityNetQA have no overlap with the testing sets used\nin the evaluation. During sampling, since LLaV A-Video contains several parts categorized by task\ntype (open-ended and multi-choice) and video duration (0–30s, 30-60s, 1–2min and 2-3min), we\nensure a balanced distribution by randomly selecting an equal number of training examples from\neach part.\nMoreover, we assume that the visual attention distributions ( Rv) associated with correct answers\nexhibit higher quality than those that lead to incorrect answers. Therefore, when training fθfor\na specific MLLM, the sampled data are evaluated by this MLLM, and the samples with incorrect\nanswers are filtered out. Only samples for which the MLLM produces correct answers are retained\nand used as training data. The number of the retained samples ranges from 8K to 12K.\nC Detailed comparison results on Generalization\nWe provide full tables of results corresponding to the generalization experiments shown in the first\ntwo sub-figures of Figure 3 in the main text (Applying to Larger Images and Longer Videos), with\ndetailed results for the image and video benchmarks listed in Table 5 and Table 6, respectively.\n15\n--- Page 16 ---\nFigure 9: Case Study 1 .\nFigure 10: Case Study 2 .\nD Efficiency Analysis in Inference\nTo evaluate computational efficiency during inference, we follow FastV and PyramidDrop and report\nthe FLOPs of the visual token part. Specifically, we consider the FLOPs of the multihead attention\nand the feed-forward network (FFN) modules as:\nFLOPs layer= 4nd2+ 2n2d+lnm, (4)\nwhere nis the number of visual tokens, dis the hidden state size, mis the intermediate size of the FFN,\nandlis the number of layers in the FFN. To compute the total FLOPs for the entire LLM, we simply\nmultiply Eq. 4 by the number of Transformer layers NL, i.e., FLOPs LLM=NL(4nd2+2n2d+lnm).\nAt the input stage of the LLM, our compressor introduces additional computation. First, we consider\nthe FLOPs introduced by the first-layer attention map:\nFLOPs attn=nd2+nd. (5)\nNote that only the key projection computation for visual tokens and the attention computation from\ntextual tokens to visual tokens are required, corresponding to the term nd2andnd, respectively. Only\nthe FLOPs incurred by the visual part are included.\n16\n--- Page 17 ---\nModel MethodRetention\nRatioMME MMStar MMVet SEED Avg.(%)\nLlava-\nOneVisionVanilla 100% 2002.0 62.0 52.0 76.7 100\nFastV50%1990.3 57.3 48.4 75.7 95.9\nOurs 1988.0 57.8 50.2 75.4 96.8\nFastV25%1953.7 52.0 43.2 71.5 89.4\nOurs 1985.8 52.6 45.8 73.1 91.9\nQwen2-VLVanilla 100% 2316.6 61.1 51.7 76.4 100\nFastV50%2295.8 57.7 52.4 74.8 98.2\nOurs 2311.7 57.9 53.9 73.9 98.9\nFastV25%2288.2 55.0 49.3 71.1 94.3\nOurs 2283.1 55.8 50.8 71.0 95.3\nTable 5: Compare generalization performance of explainability-based compressor on image benchmarks.\nModel MethodRetention\nRatioVideo-MME MVBenchMMBench-\nVideoNext-QAActivity-QA Avg.(%)multi-choice open-ended\nLlava-\nOneVisionVanilla 100% 59.3 37.1 0.38 80.9 52.5 58.4 100\nFastV50%58.8 36.1 0.38 80.5 51.4 58.2 98.9\nOurs 58.8 37.2 0.38 80.2 52.0 58.1 99.5\nFastV25%57.0 35.4 0.35 79.7 50.9 57.2 96.2\nOurs 56.5 36.9 0.36 79.2 51.0 58.0 97.3\nQwen2-VLVanilla 100% 57.1 52.7 1.42 80.7 49.5 57.6 100\nFastV50%55.4 51.3 1.40 79.6 49.0 55.7 97.9\nOurs 55.7 51.4 1.41 79.5 48.7 56.3 98.2\nFastV25%53.0 49.6 1.30 78.6 47.3 52.0 93.6\nOurs 53.2 48.6 1.30 78.4 47.6 54.3 94.1\nVILA1.5Vanilla 100% 48.7 31.7 1.30 70.4 45.8 55.2 100\nFastV50%48.1 31.5 1.31 70.1 46.5 55.1 100.0\nOurs 48.4 34.3 1.34 70.0 47.0 56.0 102.4\nFastV25%46.3 31.8 1.26 69.6 45.6 54.6 98.3\nOurs 47.4 35.0 1.29 70.0 46.7 55.7 101.5\nTable 6: Compare generalization performance of explainability-based compressor on video benchmarks.\nNext, we account for the FLOPs introduced by the 1D depthwise separable convolution:\nFLOPs conv=LX\nl=1n(Cl\nink+Cl\ninCi\nout), (6)\nwhere Cl\ninandCl\noutdenote the number of input and output channels of the l-th layer, respectively.\nWe ensure that the output shape of each convolutional layer remains the same as its input by applying\nappropriate padding with respect to the kernel size k. As a result, the number of visual tokens n\nremains constant across all layers. Then the total FLOPs is computed as the sum of the operations\nacross all Lconvolutional layers.\nTo intuitively understand the additional computational cost introduced by our method, we adopt a\ntypical parameter configuration used in MLLMs. Specifically, we set the number of visual tokens\nnto 1568, the hidden dimension dto 3584, the intermediate size mto 18944, and assume 3 layers\nper FFN block ( l= 3). For the full LLM, we consider a 28-layer Transformer blocks ( NL= 28 ).\nForfθ, we follow the configuration described in Section 4.1 (Experimental Setup). Concretely, the\nconvolutional network consists of 5 layers ( L= 5) with kernel size k= 3, and channel dimensions\nincreasing across layers: 32, 64, 128, 256, and 512. Based on these settings, FLOPs attnamounts to\napproximately 0.02 trillion, FLOPs conv is approximately 0.0003 trillion, while FLOPs LLM reaches\napproximately 11.69 trillion. It can be observed that the computational overhead introduced by our\ncompressor is negligible. The computational costs of these two parts account for only 0.17% and\n0.0026% of the total computational cost, respectively.\nFinally, we proceed to evaluate the overall efficiency and performance of our method in comparison\nwith recent methods (supplement to the experiment presented in the last two sub-figures of Figure\n3 in the main text (Applying to Larger Images and Longer Videos)). We present the comparative\n17\n--- Page 18 ---\nModel MethodRetention\nRatio(%)FLOPs(T)Performance\nPreservation(%)\nLlava-\nOneVisionVanilla 100 12.9 100\nFastV25.0 4.2 83.9\n25.0 3.5 66.3\nPDrop30.0* 3.8 85.2\n25.4* 3.2 80.2\nOurs 25.0 3.1 84.8\nQwen2-VLVanilla 100 9.6 100\nFastV 25.0 3.1 90.0\nOurs 25.0 2.4 91.3\nTable 7: Efficiency and performance comparison across different methods on MMStar. Values marked\nwith * indicate that the retention ratio refers to the average proportion of retained tokens across all LLM layers,\ndue to multi-stage compression in PDrop. For FastV , the same retention ratio corresponds to different FLOPs\nwhen compression is applied at different layers (2nd and 4th).\nModel MethodRetention\nRatio(%)FLOPs(T)Performance\nPreservation(%)\nLlava-\nOneVisionVanilla 100 52.7 100\nFastVID 25.0 11.7 99.3\nPruneVID 17.0* 11.9 99.1\nFastV 25.0 16.1 95.4\nVisionZip 25.0 11.7 94.4\nOurs 25.0 11.7 99.5\nQwen2-VLVanilla 100 48.4 100\nFastV 25.0 14.9 94.1\nOurs 25.0 10.9 92.2\nVILA1.5Vanilla 100 27.0 100\nFastV 25.0 8.2 100.3\nOurs 25.0 6.3 110.4\nTable 8: Efficiency and performance comparison across different methods on MVBench. Values marked\nwith * indicate that the retention ratio is reported from the original paper.\nresults in Table 7 and Table 8 on two challenging benchmarks, MMStar and MVBench. The FLOPs\nreported in the table are computed using a standardized input setting. For image input, FLOPs are\ncomputed using a 384×512input image as the reference (the number of visual tokens nis 1728\nfor LLaV A-OneVision and 1302 for Qwen2-VL). For video input, LLaV A-OneVision and VILA1.5\nsample 32 and 16 frames, respectively, resulting in visual token counts nof 6272 and 3136. We fix\nQwen2-VL’s input to 32 frames at 720×1280 resolution ( n=5824) for FLOPs calculation.\nOur method achieves competitive performance compared to generic methods (FastV , PyramidDrop,\nand Visionzip) and even methods specifically designed for videos (FastVID and PruneVID) while\nachieving lower FLOPs. For instance, on LLaV A-OneVision, our method achieves superior perfor-\nmance while maintaining FLOPs that are lower than or comparable to other methods. With lower\nFLOPs of 3.1T, our method achieves a performance preservation of 84.8% on MMStar, outperforming\nPyramidDrop and FatsV that have higher FLOPs (3.2T and 3.5T) but lower performance preservation\n(80.2% and 66.3%). Similarly, our method achieves the highest performance preservation of 99.5%\non MVBench with only 11.7T FLOPs. Notably, when applied on VILA, our method surpasses the\nperformance of the vanilla model by 10% on MVBench, even while reducing FLOPs by approxi-\nmately 77%. These results highlight the effectiveness of our approach in achieving a good trade-off\nbetween accuracy and computational efficiency.\n18",
  "text_length": 54705
}