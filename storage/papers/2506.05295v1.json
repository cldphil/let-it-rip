{
  "id": "http://arxiv.org/abs/2506.05295v1",
  "title": "Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms",
  "summary": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.",
  "authors": [
    "Baihe Huang",
    "Shanda Li",
    "Tianhao Wu",
    "Yiming Yang",
    "Ameet Talwalkar",
    "Kannan Ramchandran",
    "Michael I. Jordan",
    "Jiantao Jiao"
  ],
  "published": "2025-06-05T17:48:19Z",
  "updated": "2025-06-05T17:48:19Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05295v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05295v1  [cs.LG]  5 Jun 2025Sample Complexity and Representation Ability of\nTest-time Scaling Paradigms\nBaihe Huang1* Shanda Li2Tianhao Wu1Yiming Yang2\nAmeet Talwalkar2Kannan Ramchandran1Michael I. Jordan1Jiantao Jiao1\n1University of California, Berkeley2Carnegie Mellon University\nAbstract\nTest-time scaling paradigms have significantly advanced the capabilities of large language\nmodels (LLMs) on complex tasks. Despite their empirical success, theoretical understanding\nof the sample efficiency of various test-time strategies—such as self-consistency, best-of- n, and\nself-correction—remains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires Θ(1/∆2)samples to produce the\ncorrect answer, while best-of- nonly needs Θ(1/∆), where ∆<1denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an expressiveness result for\nthe self-correction approach with verifier feedback: it enables Transformers to simulate online\nlearning over a pool of experts at test time. Therefore, a single Transformer architecture can\nprovably solve multiple tasks without prior knowledge of the specific task associated with a user\nquery, extending the representation theory of Transformers from single-task to multi-task settings.\nFinally, we empirically validate our theoretical results, demonstrating the practical effectiveness of\nself-correction methods.\n1 Introduction\nOver the past several years, Large Language Models (LLMs) have witnessed remarkable advances,\nachieving unprecedented performance across a broad spectrum of application [ 12,13,20]. Driven\nby the paradigm of chain-of-thought (CoT) reasoning [ 87], the outputs of LLMs have not only\ngrown in length but also in structural complexity. In particular, recent studies have demonstrated\nthat scaling up computational resources during test time significantly enhances the problem-solving\ncapabilities LLMs—a phenomenon termed as the test-time scaling law [ 11,89,36,66]. Various\nmethods have been proposed to effectively utilize additional test-time compute, including self-\nconsistency [ 84,11,63,17], best-of- nsampling [ 42,77,62,70,73], Monte Carlo Tree Search\n(MCTS) [ 80,101,31,83,15,56], and self-correction [ 59,88,18,35,100,48]. Powered by test-time\nscaling paradigms, several reasoning models, such as OpenAI-o1 [ 65] and Deepseek-R1 [ 24], have\nachieved remarkable success in many complex tasks [34, 21, 38, 75, 22, 41, 97].\nDespite these empirical advancements, the theoretical foundations of test-time scaling remain\nunderdeveloped. While recent progress has been made in understanding the expressiveness and\nlearnability of chain-of-thought reasoning [ 29,61,53,44], two fundamental challenges remain\nunresolved:\n*baihe_huang@berkeley.edu .\n1\n--- Page 2 ---\n1.Many test-time scaling approaches rely on repeated sampling from the same LLM to select\na final answer [ 84,11,42,77,63,17,91,46,62,70,73]. Two dominant paradigms are:\nself-consistency, which marginalizes reasoning paths and selects the most frequent answer;\nand best-of- n, which chooses the answer with the highest reward score. However, a rigorous\nunderstanding of their sample complexities is lacking. This raises the first question:\nWhat is the sample complexity of repeated sampling methods,\nparticularly self-consistency and best-of- n?\n2.Theoretical analyses of Transformers’ expressiveness have largely focused on their ability\nto represent individual tasks [ 95,8,9,25,68,26,27,54,3,102,94,5,7,32,81,6,64,51,\n32,3,6,82,57,86,60,55], while the ability of Transformers to express multiple tasks at\nthe same has been under-studied. In contrast, practical LLMs are expected to perform across\ndiverse tasks at inference time—often using more tokens and computation than theory accounts\nfor [19]. This gap in theory limits our understanding of test-time scaling approaches that go\nbeyond CoT, such as self-correction [ 59,88,18,35,100,48] which uses reward information.\nAs a result, we are motivated to pose the second central question:\nHow can we characterize the expressiveness under test-time scaling methods,\nespecially in multi-task settings?\nOur Contributions. This work addresses the challenges outlined above through two key contribu-\ntions. First, we analyze the sample complexity of two prominent decoding strategies: self-consistency\nand best-of- n, in terms of the probability gap between the most likely (correct) and the second most\nlikely model outputs. Our results reveal a fundamental separation in sample efficiency that highlights\nthe advantage of the best-of- napproach.\nProposition 1.1 (Informal statement of Theorem 3.1 and Theorem 3.2) .Let∆∈(0,1)denote the\ndifference between the Transformer’s probability of producing the correct answer and the probability\nof the second most likely answer. Then, self-consistency requires Θ(1/∆2)samples to reliably\nproduce the correct answer, whereas best-of- nachieves the same with only Θ(1/∆)samples.\nProof Sketch. For best-of- n, correctness is achieved if the correct answer appears at least once\namong nindependent samples. Since the correct answer occurs with probability at least ∆, we have:\nP(Best-of- noutputs correct answer )≥1−(1−∆)n.\nTo ensure high probability, it suffices to take n≍1/∆.\nIn contrast, self-consistency relies on the correct answer being the most frequently sampled\nresponse. Let n1andn2be the counts of the correct and second most likely answers among n\nsamples, respectively. Using the Berry-Esseen theorem, the difference\nX=n1−n2−n∆√n\napproximately follows a normal distribution with constant mean and variance. To ensure n1> n2\nwith high probability, we require P(X >−∆√n)≈1, or equivalently n≍1/∆2.\nSecond, we investigate Transformer’s capacity for self-correction. We demonstrate that a Trans-\nformer equipped with verifier feedback at test time can implement online learning algorithms over\na pool of expert models, enabling it to adaptively identify the most suitable expert and ultimately\ngenerate a response that maximizes the reward. This process is illustrated in Figure 1: given the user\nquery (e.g. solve the PDE1\nc(x)2∂2u\n∂t2−∆u= 0inΩ×(0, T)with some boundary conditions), the\n2\n--- Page 3 ---\nTransformer fautoregressively generates a sequence of actions (e.g., selecting the sixth expert) and\nresponses (e.g., constructing and applying a spectral method solver), conditioned on the history of\nprevious action-response pairs and their corresponding rewards (e.g., solution error). Notably, this\nprocess relies solely on the Transformer f—whose architecture encapsulates the capabilities of all\nexperts—and the reward function, distinguishing it from traditional routing algorithms that explicitly\nquery experts. As such, this mechanism allows a single Transformer architecture to solve multiple\ntasks without prior knowledge of the specific task associated with a user query.\nFigure 1: An example from [ 50] of test-time online learning, where the Transformer progressively\nlearns that finite-element method solves the partial differential equation with higher accuracy.\nProposition 1.2 (Informal statement of Theorem 4.7) .There exists a generic way to construct a\nwider transformer ffrom any Transformer-based expert models f1, . . . , f Esuch that, when provided\nwith reward-based feedback, fcan generate a sequence of responses where the t-th response has\nregret o(1).\nProof Sketch. We first construct a Transformer f0that implements an online learning algorithm\nwith regret o(1). At each layer of the unified Transformer f, we stack the attention blocks from\nthe corresponding layers of experts f0, f1, . . . , f E. When generating the i-th action, our goal is to\nactivate only the attention blocks associated with expert f0; when generating the i-th response, our\ngoal is to activate only the attention blocks associated with expert fk, where kis the expert selected\nby action i. To achieve the above, we add an attention block and develop a generalized position\nencoding scheme to induce attention sink behavior [92]: the attentions of all non-selected experts\nsink to the token representing action i(being one at <action i> and zero elsewhere) and attentions\nof the k-th expert are identical to the attentions computed by fk. We illustrated this mechanism in\nFigure 2. As a result, the action sequence achieves o(1)regret and the response sequence is generated\nfrom the corresponding expert selected by the latest action. Therefore, the response sequence also\nachieves regret o(1).\nProposition 1.2 has two key implications. First, it demonstrates that a Transformer can express\nmultiple tasks within a single architecture, extending beyond prior theoretical results that focus on\nsingle-task expressiveness. Importantly, the construction is task-agnostic and independent of the spe-\ncific expert Transformers used, making both the result and the underlying techniques of independent\ntheoretical interest. Second, Proposition 1.2 reveals a fundamental distinction between self-correction\nand repeated-sampling paradigms. While repeated-sampling methods generate identically distributed\nresponses across attempts, self-correction provably allows the model to update its attempts based\non verifier feedback, thereby increasing the probability of producing the correct answer as inference\nprogresses. We further validate this results through controlled experiments.\n2 Preliminaries\nTransformers. In this work, we consider attention-only Transformers defined as follows.\n3\n--- Page 4 ---\nFigure 2: Illustration of the attention sink behavior in the self-correcting Transformer.\nDefinition 2.1 (Transformer) .We define a Transformer model over vocabulary Vas a tuple\n(θ,pe,(K(l)\nh,Q(l)\nh,V(l)\nh)h∈[H],l∈[L], ϑ,V)\nwhere θ:V →Rdis the tokenizer, pe :Rd× Vω→Rdis a position encoder, K(l)\nh,Q(l)\nh,V(l)\nh∈\nRd×dare the key, query, value matrices over Llayers and Hheads each layer, and ϑis the output\nfeature. The computation of a Transformer rolls out as follows:\n1. For each i= 1, . . . , n\nX(1)\ni= pe( θ(vi);v1, . . . , v i).\n2. For each l= 1, . . . , L , compute each X(l+1)\ni fori= 1, . . . , n by\nX(l+1)\ni =HX\nh=1iX\nj=1exp\u0010\ns(l)\nh(Xi, Xj)\u0011\nZ(l)\nh·V(l)\nhX(l)\nj, (1)\nwhere s(l)\nh(·)is the attention score defined by s(l)\nh(Xi, Xj) = (Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj)and\nZ(l)\nh=Pi\nj=1exp\u0010\ns(l)\nh(Xi, Xj)\u0011\nis the normalizing constant.\n3. The output probability is given by\npf(y|v1, . . . , v n) = Softmax( ϑ(y)⊤X(L)\nn), y∈ V.\nIn particular, we assume the softmax attention layer has precision ϵ: if two attention scores s1, s2\nsatisfy es1< ϵ·es2, then es1is treated as zero in the attention computation of Eq. (1).\nWhile classical positional encoders is solely dependent on the index of the current token (i.e. we\nmay write pe(θ(vi);v1, . . . , v i) = pe( θ(vi);i)), recent advance [ 37,98,33] has extended this notion\nto incorporate set membership information of preceding tokens. This generalization proves crucial\nfor enhancing the long-context capability required for effective self-correction. Motivated by this\ninsight, we introduce the following notion of a generalized position encoder.\nDefinition 2.2 (Generalized Position Encoder) .We say that pe :Rd× Vω→Rdis a generalized\nposition encoder w.r.t. a partition V1, . . . ,VKofVif it maps an input feature in Rdand a token\nsequence (of arbitrary length) v1,···, vito a vector in Rd, so that it only depends on the input feature\nand the membership of each viin the sets V1, . . . ,VK, i.e.\npe(θ(vi);v1, . . . , v i) = pe\u0010\nθ(vi); ( 1(vj∈ Vk))j∈[i],k∈[K]\u0011\n.\n4\n--- Page 5 ---\nTest-time scaling. In this work, we study the following three strategies for test-time scaling.\n1.Self-consistency samples ni.i.d. responses from the language model and chooses the most\nconsistent answer, while marginalizing over the reasoning paths.\n2.Best-of- nsamples ni.i.d. responses from the language model and chooses the answer with the\nhighest score given by the reward model.\n3.In the Self-Correction paradigm, the Transformer autonomously generates a sequence of n\nresponses, each conditioned on the previous responses and their respective reward scores.\n3 Separation between Self-Consistency and Best-of-n\nIn this section, we study the sample complexity of self-consistency and best-of- n. Letqdenote the\nuser query (e.g. a math problem) and Odenote the answer space; then for each answer o∈ O we\ndefine p(o)as the marginalized probability of generating oover all possible reasoning paths\np(o) =X\nreasoning pathpf(reasoning path , o|q)\nwhere pfdenotes the probability distribution of Transformer f.\nTo understand the sample complexity, we focus on the dependence on the following probability\ngap:\n∆ := p(o∗)−max\no∈O,o̸=o∗p(o)\nwhere o∗denotes the correct answer1. If∆≤0, then self-consistency fails to find the correct answer\nwith high probability and the separation becomes trivial. Therefore, we focus on the setting where\n∆>0(i.e., the most likely answer is correct), which is also considered in prior theoretical work [ 40].\nUnder this setting, we may assume without loss of generality that the reward function ris maximized\n(only) at the correct answer, because pitself is such a reward function satisfying this condition. Note\nthat since p(o)is marginalized over reasoning paths, ∆>0does not imply that the correct answer\ncan be derived easily from greedy decoding.\nTheorem 3.1 (Sample Complexity of Self-Consistency) .When n≥2 log(1 /δ)\n∆2, self-consistency with\nni.i.d. samples is able to produce the correct answer with probability at least 1−δ; When n≤1\n∆2,\nthere exists a hard instance where self-consistency with ni.i.d. samples fails to produce the correct\nanswer with constant probability.\nTheorem 3.2 (Sample Complexity of Best-of- n).When n≥2 log(1 /δ)\n∆, best-of- nwithni.i.d. samples\nis able to produce the correct answer with probability at least 1−δ; When n≤1\n∆, there exists a\nhard instance where best-of- nwithni.i.d. samples fails to produce the correct answer with constant\nprobability.\nBy providing matching (up to logarithmic factors) upper and lower bounds on the number of\nsamples, the above results establishes the separation between self-consistency and best-of- n. While\nself-consistency requires Θ(1/∆2)samples to produce the correct answer, best-of- nshows advantage\nby only requiring Θ(1/∆)samples. Therefore, this theory corroborates the empirical findings that\nbest-of- ngenerally leads to better problem solving accuracy on reasoning tasks compared with\nself-consistency [79, 90].\n1If there are multiple correct answers, we can let o∗to denote the set, and our results continue to hold in this setting.\n5\n--- Page 6 ---\n4 Expressiveness under Self-Correction\nA key distinction between self-correction and the repeated sampling strategies discussed in the\nprevious section lies in the dependence structure of the generated responses: unlike repeated sampling,\nthe outputs produced by self-correction are not i.i.d.. Consequently, to analyze the sample efficiency\nof self-correction, we must first address a fundamental question: can a large language model (LLM),\nthrough self-correction, increase the likelihood of generating the correct answer? At its core, this\nquestion is one of expressiveness—whether the Transformer architecture’s representation capacity is\nsufficient to support such improvement.\nIn this section, we take a first step toward analyzing the expressiveness of Transformers under the\nself-correction paradigm. Unlike prior work that focuses on expressiveness in the context of a single\ntask, we study what we call general-purpose expressiveness : the ability to solve a broad range of\ntasks. To this end, we introduce the concept of a General-Purpose Transformer—a construction that\nmaps any collection of task-specific Transformers (experts) into a single unified Transformer.\nDefinition 4.1 (General-Purpose Transformer) .We say that ϕis a General-Purpose Transformer of\ntype(t1, t2)if it maps any set of Transformers with hidden size dand depth Linto another ‘unified’\nTransformer with hidden size t1·d+t2and depth L+O(1).\nA general-purpose Transformer provides a principled framework for constructing more powerful\nTransformer architectures by composing simpler, task-specific components. This meta-architecture\nenables a single model to solve multiple tasks at inference time, representing a significant advancement\nin our theoretical understanding of the expressive power of modern machine learning systems. Our\ngoal is to investigate the general-purpose expressiveness of self-correction paradigms through the\nlens of general-purpose Transformers: specifically, how a Transformer can adaptively solve different\ntasks during inference without prior knowledge of the task identity.\n4.1 General-purpose expressiveness\nIn this section, we present two auxiliary results that serve as building blocks for constructing general-\npurpose Transformers capable of solving multiple tasks. These results may also be of independent\ninterest beyond expressiveness of self-correction.\nFigure 3: Illustration of the general-purpose Transformer that combines Transformers over different\ntoken spaces. In the first query, since the last token ‘is’ belongs to the blue space, f2is called to solve\nthe common sense problem by attending to only blue tokens. In the second query, since the last token\n‘=’ belongs to the red space, f1is called to solve the arithmetic problem by attending to only red\ntokens. Importantly, these “function calls” occur implicitly within the internal computation of the\nunified Transformer architecture.\n6\n--- Page 7 ---\nThe first result addresses the setting in which multiple Transformers operate over distinct vo-\ncabularies, with each vocabulary corresponding to a specific task. The objective is to construct a\nunified Transformer that uses the final token in the input sequence to infer which task to perform,\nand subsequently solves the task by attending only to the task-relevant tokens. This paradigm is\nillustrated in Figure 3.\nProposition 4.2 (General-purpose Expressiveness over Different Token Spaces) .For any H, L, K, N max∈\nZ+,Vi∩ Vj=∅(∀i̸=j∈ {0} ∪[K]), there exists a general-purpose Transformer ϕof type\n(O(K), O(logNmax))such that for any Transformers fk= (θ,pe,(K(l)\nk;h,Q(l)\nk;h,V(l)\nk;h)h∈[H],l∈[L], ϑ,Vk)\nfork∈[K], the Transformer ef=ϕ(f1, . . . , f K)satisfies the following property: for any token\nsequence v=v1···vnsuch that n≤Nmaxand there exists one vi0∈ V0, we have\npef(·|v) =pfκ(·|u)\nwhere κis the task indicated by the last token: i.e., vn∈ Vκ, and u=vi1···vim, where {i1<···<\nim}={i:vi∈ Vκ}, is the sequence of tokens relevant to task κ.\nRemark 4.3. The existence of vi0which does not belong to any {Vi}i∈[K]serves the technical\npurpose of inducing attention sink of all irrelevant experts to vi0. It may be achieve by assuming the\nuser query always ends with the special token <eos> .\nThe following result considers a more challenging scenario in which multiple Transformers\noperate across different tasks but share a common vocabulary space. A set of indicator tokens,\ndenoted by Ω, is used to specify the intended task. The objective is to determine which task to execute\nbased on the most recent indicator token. It then proceeds to solve the task by attending exclusively to\nthe task-relevant tokens appearing before the first indicator token and after the last indicator token in\nthe input sequence. This paradigm is closely related to self-correction, and is illustrated in Figure 4.\nFigure 4: Illustration of the general-purpose Transformer that combines Transformers over the same\ntoken spaces. In the first query, since the last indicator token ‘(2)’ points to the second expert, f2\nis called to solve the history problem by attending to only blue tokens. In the second query, since\nthe last indicator token ‘(1)’ points to the first expert, f1is called to solve the chemistry problem by\nattending to only red tokens. Importantly, these “function calls” occur implicitly within the internal\ncomputation of the unified Transformer architecture.\nProposition 4.4 (Multi-Task Representation over the Same Token Space) .For any H, L, K, N max∈\nZ+, token spaces Ω∩V=∅, there exists a general-purpose Transformer ϕof type (O(K), O(logNmax))\nsuch that for any Transformers fk= (θ,pe,(K(l)\nk;h,Q(l)\nk;h,V(l)\nk;h)h∈[H],l∈[L], ϑ,V), k∈[K]over\nV, the Transformer ef=ϕ(f1, . . . , f K)satisfies the following property: for any token sequence\nv=v1···vnsuch that\n{ξ1<···< ξm}={j:vj∈Ω}, ξm< n≤Nmax\n7\n--- Page 8 ---\nthen we have\npef(·|v) =pfκ(·|u) (2)\nwhere u=v1···vξ1−1vξm+1···vnis the token sequence obtained by omitting tokens from position\nξ1toξm, and κis the task indicated by token vξm.\nRemark 4.5. We observe that in both results above, reducing the type parameters is generally\nnot feasible. The dependence on Karises from the need to compute features for all Kexperts\ncorresponding to the user query. Since the model lacks prior knowledge of the task, it must encode all\ntask-relevant information to preserve the ability to invoke any expert at inference time. The log(Nmax)\nscaling stems from the positional encoding: in order to construct Nmaxnearly orthogonal vectors,\nthe positional embedding must have dimension at least log(Nmax).\n4.2 General-purpose expressiveness of Transformers with self-correction\nIn this section we state the main result that establishes general-purpose expressiveness of Transformers\nwith self-correction. We rely on the following notion of regret-minimization Transformer, which\nexpresses the single task of finding the most rewardable action.\nDefinition 4.6 (Regret-Minimization Transformer) .We say that a Transformer fachieves simple\nregret reg(·)over reward function rand action space A, if for any T∈Z+, we have max a∗∈Ar(a∗)−\nE[r(aT)]≤reg(T)where a1, . . . , a Tare generated in the following way:\nat∼pf(·|a1, r1, . . . , a t−1, rt−1),∀t= 1, . . . , T,\nrt=r(at),∀t= 1, . . . , T.\nEssentially, the goal of a regret-minimization Transformer is to learn from a reward oracle\nand ultimately recommend an action that is near-optimal, which is related to a concept commonly\nreferred to as simple regret in the bandit literature [ 28,14,43]. To achieve this, the Transformer may\nimplement strategies such as mirror descent, upper confidence bounds, or search-based algorithms,\ndepending on the problem structure. As these procedures rely only on basic arithmetic operations,\nsuch Transformers can be constructed by applying the universal approximation capabilities of\nTransformers [ 95,58,29,53]: for example, [ 55] provides constructions to approximate upper\nconfidence bounds and Thompson sampling algorithms with regret O(√\nT). Consequently, their\nconstruction is not the primary focus of this work.\nThe following theorem establishes the existence of a general-purpose Transformer that can\nsimulate the behavior of a set of expert Transformers (not necessarily over the same token space)\nthrough self-correction. Specifically, it shows that such a unified Transformer can, at inference time,\nidentify and invoke the appropriate expert to solve any task that the original experts can solve. The\nself-correction protocol is described in Algorithm 1, wherein the unified Transformer autoregressively\ngenerates actions and responses, after which the verifier is queried to obtain reward signals. Through\nthis process of trial and error, the model effectively “learns” at inference time, using the verifier to\nminimize regret and adaptively select the correct expert.\nTheorem 4.7 (Regret Minimization via Self-Correction) .For any H, L, K, N max∈Z+, token\nspaces V0,V1, . . . ,VK,A(|A|=K)such that V0,V= (∪K\nk=1Vk),andAare disjoint, and reward\nfunction r, there exists a general-purpose Transformer ϕof type (O(K), O(logNmax))such that\ngiven any set of Transformers denoted as follows,\n•Kexpert Transformers :fk= (θ,pe,(K(l)\nk;h,Q(l)\nk;h,V(l)\nk;h)h∈[H],l∈[L], ϑ,Vk)fork∈[K], such\n8\n--- Page 9 ---\nAlgorithm 1 Self-correction with verifier\n1:procedure GENERATION (q) ▷ q=q1. . . q n0denotes the user query.\n2: prompt ←q\n3: fort= 1, . . . , T do\n4: a(t)∼pef(· |prompt) ▷ a(t)designates which expert to use in t-th iteration\n5: prompt ←prompt |a(t)▷Update the prompt autoregressively, |represents token concatenation.\n6: fori= 1, . . . do\n7: u(t)\ni∼pef(· |prompt) ▷Generate t-th response autoregressively\n8: prompt ←prompt |u(t)\ni ▷Update the prompt autoregressively\n9: ifu(t)\ni= EOS then\n10: Break\n11: end if\n12: end for\n13: r(t)←r(q, u(t)),prompt ←prompt |r(t)▷Query verifier to obtain reward of t-th response\n14: end for\n15: Return\n16:end procedure\nthat one of the expert fk∗achieves λ-suboptimal reward:\nEu∼fk∗(·|q)[r(q, u)]≥max\nu∗∈Vωr(q, u∗)−λ\n•Regret-Minimization Transformer :f0= (θ,pe,K(l)\n0;h,Q(l)\n0;h,V(l)\n0;h)h∈[H],l∈[L], ϑ,V0∪ A)\nthat implements a bandit algorithm over the reward function r0and action space Awith simple\nregret reg(t), where r0(a) =Eu∼fa(·|q)[r(q, u)]denotes the average reward of responses\ngenerated by the a-th expert,\nthen the Transformer ef=ϕ(f0, f1, . . . , f K)satisfies the following property: for any prompt v=\nv1···vn, if the response sequence u(1), . . . , u(T)generated by the protocol in Algorithm 1 has total\nlength ≤Nmax, then we have\nmax\nu∗∈Vωr(q, u∗)−E[r(q, u(T))]≤λ+ reg( T)\nRemark 4.8. While the general-purpose Transformer ϕcan be applied to construct the brutal-force\nTransformer efthat simply tries every expert, we note that the generality of Definition 4.6 allows us to\nconstruct more powerful Transformers beyond brutal search. Leveraging the structures in the problem\nand the expert pool, it is entirely possible to identify the correct expert using ≪Ktrials [72, 30].\nAs a consequence of Theorem 4.7, we obtain a Transformer architecture that can provably\nproduce a final answer that nearly maximizes the reward. This means that the unified transformer\ncan solve Kdistinct tasks at inference time, without requiring prior knowledge of which task the\nuser query pertains to. Notably, the construction of such an architecture is general-purpose , in\nthat it is independent of the specific tasks, reward functions, or expert policies. To the best of\nour knowledge, this constitutes the first theoretical result of its kind in the study of Transformer\narchitectures. Furthermore, our theory aligns with the empirical finding that LLMs are able to\nprogressively optimize outcome rewards during test-time [71].\n9\n--- Page 10 ---\n5 Experiments\nIn this section, we conduct synthetic experiments to show that Transformers can self-correct with\nverifier feedback.\n5.1 Experimental Setup\nData generation. We aim to construct a test problem with complex prompts such that correctly\nsolving the problem in the single-term generation is challenging. In this case, self-correction can play\na critical role if Transformers have such capacities. Specifically, in our synthetic problem, the prompt\nis the concatenation of the following two components:\n•Instruction : A 3-SAT problem, e.g.,\n(∼x3∨ ∼x1∨ ∼x2)∧(∼x1∨ ∼x3∨x2)∧(∼x4∨x2∨ ∼x3)∧ ···\n•Data : A string composed of characters from the set { a,b}.\nModel Depth Heads Width\nGPT-nano 3 3 48\nGPT-micro 4 4 128\nGPT-mini 6 6 192\nGopher-44M 8 16 512\nTable 1: Model configuration hyperparameters.The ground truth target is defined as follows: If\nthe 3-SAT problem in the instruction is satisfiable,\nthe model should copy the string in the data part\nin the output; otherwise, the model should reverse\nthe string in the output.\nModel configuration. We train Transformer\nmodels of various sizes. The configurations are\ndetailed in Table 1.\nImplementation details. Our code are implemented based on PyTorch [67] and minGPT2. All\nthe models are trained on one NVIDIA GeForce RTX 2080 Ti GPU with 11GB memory.\nGPT-nano GPT-micro GPT-mini Gopher-44M020406080100Accuracy (%)\n2.5693.0998.57 99.15\n1.2363.19 63.19 63.19T est accuracy comparison\nW/o self-correction\nW/ self-correction\nFigure 5: Accuracy comparisons of different\nmodels with/without self-correction at test time.In our experiment, we construct datasets using\n3-SAT problems with 4 variables and 20 clauses.\nThe lengths of the data strings are set to 5. We gen-\nerate 10000 instances for training and 512 instances\nfor evaluation. In the training set, we control the\nratio of satisfiable and unsatisfiable 3-SAT instruc-\ntions to 9:1, while in the test set, the ratio is set to\n1:1.\nAll our models are trained with the Adam opti-\nmizer [ 47] for 5 epochs. Following common prac-\ntice, the learning rate goes through the warm-up\nstage in the first 5% of training iterations, and then\ndecays linearly to 0 until training finishes. We set\nthe peak learning rate to 10−4and find that all the\nmodels are stably trained under this learning rate\nschedule. We do not apply drop out or weight decay during training. We repeat the experiments for 3\ntimes under different random seeds and report the average accuracy with error bars.\n2https://github.com/karpathy/minGPT (MIT license).\n10\n--- Page 11 ---\n5.2 Results\nTest set accuracy across different inference settings is shown in Figure 5. We note that model\nperformance plateaus at 63.19% when there is no self-correction at test time, with no improvement\nfrom increased model size. By contrast, when models are equipped with verifier signals to enable\nself-correction, test accuracy improves substantially, demonstrating the efficacy of this mechanism.\nCrucially, larger models – such as GPT-mini and Gopher-44M – achieve near-perfect accuracy under\nself-correction, suggesting that sufficiently expressive Transformers are capable of implementing\neffective self-correction strategies. This empirical result supports our theoretical findings.\n6 Related Works\nTheories of Transformers and Large Language Models. The success of Transformers and LLMs\nhas motivated the study on their expressiveness. Existing research has shown that Transformers\ncan implement simple functions such as sparse linear functions, two-layer neural networks, and\ndecision trees [ 32], gradient descent [ 3,6,82], automata [ 57,102], Dyck languages [ 8,94], Turing\nmachines [ 25,9,96,68,86], variational inference [ 60], and bandit algorithms [ 55]. [95,58,4,69]\nestablish universal approximation results under various settings. [ 26,27,49,54] study representational\ncapabilities and properties of self-attention, the core component in Transformers. [ 29,53] study\nthe expressiveness of auto-regressive Transformers with chain-of-thought. [ 26,52,10] studies the\nsample complexity of Transformers. Recently, a growing body of work has begun to explore the\ntheoretical foundations of self-improvement in large language models (LLMs). [ 78] introduces\nthe generation-verification gap as a key quantity governing scaling behavior. [ 40] proposes a\nprogressive sharpening framework in which the policy gradually shifts toward more confident\nresponses. [ 74] draws on reinforcement learning theory to formally establish the advantages of\nverifier-based methods. In contrast to these works, our results provide explicit sample complexity\nrates and tangible representation architectures, enabling a more concrete understanding of the\nfundamental capabilities and limitations of test-time scaling paradigms.\nTest-time scaling. Recent research has established the test-time scaling law of LLMs, illuminating\na new scaling axis beyond training-time scaling laws [ 45,39]. Existing approaches of scaling up\ntest-time compute of LLMs can be broadly classified into two categories: (1) applying test-time\nalgorithms (aka inference-time algorithms) during LLM decoding [ 11,90,76]; and (2) explicitly\ntraining LLMs to output long chain-of-thought traces [ 36,46,66,93]. Many recent works focus on\nunderstanding and improving the effectiveness of test-time scaling empirically: [ 19,1,23,85] study\nunder-thinking, over-thinking, and length control in LLM reasoning. [ 16] proposes to integrates\nself-verification and self-correction into sampling. [ 71] analyzes optimizing test-time compute by\nintroducing a meta reinforcement learning formulation. [ 74] demonstrates that verification/RL is\nimportant for optimal test-time scaling. [ 99] provides an extensive review of the test-time scaling\nlandscape. In contrast, our work focuses on theoretical analyses of test-time scaling.\n7 Discussions\nIn this work, we present a theoretical analysis of test-time scaling paradigms, focusing on two core\naspects: sample efficiency and representational capacity. Our investigation reveals a fundamental sepa-\nration in sample complexity between self-consistency and best-of- n, providing theoretical support for\nthe empirically observed superiority of the latter method. Furthermore, by introducing the framework\n11\n--- Page 12 ---\nofgeneral-purpose expressiveness , we construct generic Transformer architectures capable of emulat-\ning online learning algorithms at test time. This capability enables a single model to provably solve\nmultiple tasks without task-specific adaptation, thus extending our understanding of expressiveness\nto multi-task settings. Our results highlight the theoretical advantage of self-correction paradigms,\nwhich iteratively refine predictions to increase the likelihood of correct answers—surpassing the\nlimitations of i.i.d. responses by repeated sampling approaches. This finding is validated through\nexperiments and we observe that it requires additional model capacities for Transformer to implement\nself-correction.\nDespite these contributions, our work comes with limitations: our construction in Theorem 4.7\nonly applies to attention-only Transformers and relies on a slightly generalized position encoding\nmethod. Relaxing these constraints constitutes interesting problems for future research.\n12\n--- Page 13 ---\nReferences\n[1]P. Aggarwal and S. Welleck. L1: Controlling how long a reasoning model thinks with\nreinforcement learning. In arXiv , 2025.\n[2]S. Agrawal and R. Jia. Optimistic posterior sampling for reinforcement learning: worst-case\nregret bounds. Advances in neural information processing systems , 30, 2017.\n[3]E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is\nin-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 ,\n2022.\n[4]S. Alberti, N. Dern, L. Thesing, and G. Kutyniok. Sumformer: Universal approximation for\nefficient transformers. In T. Doster, T. Emerson, H. Kvinge, N. Miolane, M. Papillon, B. Rieck,\nand S. Sanborn, editors, Proceedings of 2nd Annual Workshop on Topology, Algebra, and\nGeometry in Machine Learning (TAG-ML) , volume 221 of Proceedings of Machine Learning\nResearch , pages 72–86. PMLR, 28 Jul 2023.\n[5]C. Anil, Y . Wu, A. Andreassen, A. Lewkowycz, V . Misra, V . Ramasesh, A. Slone, G. Gur-Ari,\nE. Dyer, and B. Neyshabur. Exploring length generalization in large language models. arXiv\npreprint arXiv:2207.04901 , 2022.\n[6]Y . Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable\nin-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637 ,\n2023.\n[7]B. Barak, B. Edelman, S. Goel, S. Kakade, E. Malach, and C. Zhang. Hidden progress in deep\nlearning: Sgd learns parities near the computational limit. Advances in Neural Information\nProcessing Systems , 35:21750–21764, 2022.\n[8]S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to\nrecognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pages 7096–7116, 2020.\n[9]S. Bhattamishra, A. Patel, and N. Goyal. On the computational power of transformers and its\nimplications in sequence modeling. In Proceedings of the 24th Conference on Computational\nNatural Language Learning , pages 455–475, 2020.\n[10] E. Botta, Y . Li, A. Mehta, J. T. Ash, C. Zhang, and A. Risteski. On the query complexity of\nverifier-assisted language generation. arXiv preprint arXiv:2502.12123 , 2025.\n[11] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V . Le, C. Ré, and A. Mirhoseini. Large\nlanguage monkeys: Scaling inference compute with repeated sampling. arXiv preprint\narXiv:2407.21787 , 2024.\n[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages\n1877–1901. Curran Associates, Inc., 2020.\n13\n--- Page 14 ---\n[13] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee,\nY . Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.\narXiv preprint arXiv:2303.12712 , 2023.\n[14] A. Carpentier and M. Valko. Simple regret for infinitely many armed bandits. In International\nConference on Machine Learning , pages 1133–1141. PMLR, 2015.\n[15] G. Chen, M. Liao, C. Li, and K. Fan. Alphamath almost zero: Process supervision without\nprocess. In The Thirty-eighth Annual Conference on Neural Information Processing Systems ,\n2024.\n[16] J. Chen, J. Ren, X. Chen, C. Yang, R. Sun, and S. Ö. Arık. Sets: Leveraging self-verification\nand self-correction for improved test-time scaling. arXiv preprint arXiv:2501.19306 , 2025.\n[17] L. Chen, J. Q. Davis, B. Hanin, P. Bailis, I. Stoica, M. Zaharia, and J. Zou. Are more LLM\ncalls all you need? towards the scaling properties of compound AI systems. In Conference on\nNeural Information Processing Systems , 2024.\n[18] X. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug. In\nInternational Conference on Learning Representations , 2024.\n[19] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu, M. Zhou, Z. Zhang, et al.\nDo not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint\narXiv:2412.21187 , 2024.\n[20] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311 , 2022.\n[21] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168 , 2021.\n[22] codeforce. Codeforces, 2025.\n[23] A. Cuadron, D. Li, W. Ma, X. Wang, Y . Wang, S. Zhuang, S. Liu, L. G. Schroeder, T. Xia,\nH. Mao, et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic\ntasks. arXiv preprint arXiv:2502.08235 , 2025.\n[24] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. In arXiv , 2025.\n[25] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv\npreprint arXiv:1807.03819 , 2018.\n[26] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-\nattention mechanisms. In International Conference on Machine Learning , pages 5793–5831.\nPMLR, 2022.\n[27] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y . Bai, A. Chen,\nT. Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits\nThread , 1:1, 2021.\n14\n--- Page 15 ---\n[28] E. Even-Dar, S. Mannor, Y . Mansour, and S. Mahadevan. Action elimination and stopping\nconditions for the multi-armed bandit and reinforcement learning problems. Journal of machine\nlearning research , 7(6), 2006.\n[29] G. Feng, B. Zhang, Y . Gu, H. Ye, D. He, and L. Wang. Towards revealing the mystery\nbehind chain of thought: a theoretical perspective. Advances in Neural Information Processing\nSystems , 36:70757–70798, 2023.\n[30] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive\ndecision making. arXiv preprint arXiv:2112.13487 , 2021.\n[31] Z. Gao, B. Niu, X. He, H. Xu, H. Liu, A. Liu, X. Hu, and L. Wen. Interpretable contrastive\nmonte carlo tree search reasoning. In arXiv , 2024.\n[32] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a\ncase study of simple function classes. Advances in Neural Information Processing Systems ,\n35:30583–30598, 2022.\n[33] O. Golovneva, T. Wang, J. Weston, and S. Sukhbaatar. Contextual position encoding: Learning\nto count what’s important. arXiv preprint arXiv:2405.18719 , 2024.\n[34] Google. Aime problems and solutions, 2025.\n[35] Z. Gou, Z. Shao, Y . Gong, yelong shen, Y . Yang, N. Duan, and W. Chen. CRITIC: Large\nlanguage models can self-correct with tool-interactive critiquing. In International Conference\non Learning Representations , 2024.\n[36] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948 , 2025.\n[37] Z. He, G. Feng, S. Luo, K. Yang, L. Wang, J. Xu, Z. Zhang, H. Yang, and D. He. Two\nstones hit one bird: Bilevel positional encoding for better length extrapolation. arXiv preprint\narXiv:2401.16421 , 2024.\n[38] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the MATH dataset. In Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track , 2021.\n[39] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las\nCasas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den\nDriessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae,\nand L. Sifre. An empirical analysis of compute-optimal large language model training. In\nA. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information\nProcessing Systems , 2022.\n[40] A. Huang, A. Block, D. J. Foster, D. Rohatgi, C. Zhang, M. Simchowitz, J. T. Ash, and\nA. Krishnamurthy. Self-improvement in language models: The sharpening mechanism. arXiv\npreprint arXiv:2412.01951 , 2024.\n[41] Z. Huang, Z. Wang, S. Xia, X. Li, H. Zou, R. Xu, R.-Z. Fan, L. Ye, E. Chern, Y . Ye, Y . Zhang,\nY . Yang, T. Wu, B. Wang, S. Sun, Y . Xiao, Y . Li, F. Zhou, S. Chern, Y . Qin, Y . Ma, J. Su, Y . Liu,\nY . Zheng, S. Zhang, D. Lin, Y . Qiao, and P. Liu. Olympicarena: Benchmarking multi-discipline\n15\n--- Page 16 ---\ncognitive reasoning for superintelligent AI. In Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track , 2024.\n[42] R. Irvine, D. Boubert, V . Raina, A. Liusie, Z. Zhu, V . Mudupalli, A. Korshuk, Z. Liu, F. Cremer,\nV . Assassi, C.-C. Beauchamp, X. Lu, T. Rialan, and W. Beauchamp. Rewarding chatbots for\nreal-world engagement with millions of users. In arXiv , 2023.\n[43] K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil’ucb: An optimal exploration algorithm\nfor multi-armed bandits. In Conference on Learning Theory , pages 423–439. PMLR, 2014.\n[44] N. Joshi, G. Vardi, A. Block, S. Goel, Z. Li, T. Misiakiewicz, and N. Srebro. A theory of\nlearning with autoregressive chain of thought. arXiv preprint arXiv:2503.07932 , 2025.\n[45] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-\nford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361 , 2020.\n[46] Kimi. Kimi k1.5: Scaling reinforcement learning with llms. In arXiv , 2025.\n[47] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n[48] A. Kumar, V . Zhuang, R. Agarwal, Y . Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal,\nC. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement\nlearning. arXiv preprint arXiv:2409.12917 , 2024.\n[49] S. Li, X. Chen, D. He, and C.-J. Hsieh. Can vision transformers perform convolution? arXiv\npreprint arXiv:2111.01353 , 2021.\n[50] S. Li, T. Marwah, J. Shen, W. Sun, A. Risteski, Y . Yang, and A. Talwalkar. Codepde: An\ninference framework for llm-driven pde solver generation. arXiv preprint arXiv:2505.08783 ,\n2025.\n[51] S. Li, Z. Song, Y . Xia, T. Yu, and T. Zhou. The closeness of in-context learning and weight\nshifting for softmax regression. arXiv preprint arXiv:2304.13276 , 2023.\n[52] Y . Li, A. Kirchmeyer, A. Mehta, Y . Qin, B. Dadachev, K. Papineni, S. Kumar, and A. Risteski.\nPromises and pitfalls of generative masked language modeling: theoretical framework and\npractical guidelines. arXiv preprint arXiv:2407.21046 , 2024.\n[53] Z. Li, H. Liu, D. Zhou, and T. Ma. Chain of thought empowers transformers to solve inherently\nserial problems. In The Twelfth International Conference on Learning Representations , 2024.\n[54] V . Likhosherstov, K. Choromanski, and A. Weller. On the expressive power of self-attention\nmatrices. arXiv preprint arXiv:2106.03764 , 2021.\n[55] L. Lin, Y . Bai, and S. Mei. Transformers as decision makers: Provable in-context reinforcement\nlearning via supervised pretraining. arXiv preprint arXiv:2310.08566 , 2023.\n[56] Q. Lin, B. Xu, Z. Li, Z. Hao, K. Zhang, and R. Cai. Leveraging constrained monte carlo tree\nsearch to generate reliable long chain-of-thought for mathematical reasoning. In arXiv , 2025.\n[57] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to\nautomata. arXiv preprint arXiv:2210.10749 , 2022.\n16\n--- Page 17 ---\n[58] S. Luo, S. Li, S. Zheng, T.-Y . Liu, L. Wang, and D. He. Your transformer may not be as\npowerful as you expect. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances\nin Neural Information Processing Systems , 2022.\n[59] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,\nS. Prabhumoye, Y . Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh,\nand P. Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference\non Neural Information Processing Systems , 2023.\n[60] S. Mei and Y . Wu. Deep networks as denoising algorithms: Sample-efficient learning of\ndiffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420 ,\n2023.\n[61] W. Merrill and A. Sabharwal. The expressive power of transformers with chain of thought.\narXiv preprint arXiv:2310.07923 , 2023.\n[62] T. Munkhbat, N. Ho, S. H. Kim, Y . Yang, Y . Kim, and S.-Y . Yun. Self-training elicits concise\nreasoning in large language models. In arXiv , 2025.\n[63] A. Nguyen, D. Mekala, C. Dong, and J. Shang. When is the consistent prediction likely to be\na correct prediction? In arXiv , 2024.\n[64] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann,\nA. Askell, Y . Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint\narXiv:2209.11895 , 2022.\n[65] OpenAI. Openai o1 system card. In arXiv , 2024.\n[66] OpenAI. Openai o3-mini, 2024.\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. Advances in neural information processing systems , 32:8026–8037, 2019.\n[68] J. Pérez, P. Barceló, and J. Marinkovic. Attention is turing-complete. Journal of Machine\nLearning Research , 22(75):1–35, 2021.\n[69] A. Petrov, P. H. Torr, and A. Bibi. Prompting a pretrained transformer can be a universal\napproximator. In Proceedings of the 41st International Conference on Machine Learning ,\npages 40523–40550, 2024.\n[70] J. Qiu, Y . Lu, Y . Zeng, J. Guo, J. Geng, H. Wang, K. Huang, Y . Wu, and M. Wang. Treebon:\nEnhancing inference-time alignment with speculative tree-search and best-of-n sampling.\narXiv preprint arXiv:2410.16033 , 2024.\n[71] Y . Qu, M. Y . Yang, A. Setlur, L. Tunstall, E. E. Beeching, R. Salakhutdinov, and A. Ku-\nmar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint\narXiv:2503.07572 , 2025.\n[72] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. Operations\nResearch , 66(1):230–252, 2018.\n17\n--- Page 18 ---\n[73] P. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret, N. Vieillard, A. Ramé, B. Shariari, S. Perrin,\nA. Friesen, G. Cideron, S. Girgin, P. Stanczyk, A. Michi, D. Sinopalnikov, S. Ramos, A. Héliou,\nA. Severyn, M. Hoffman, N. Momchev, and O. Bachem. Bond: Aligning llms with best-of-n\ndistillation. In arXiv , 2024.\n[74] A. Setlur, N. Rajaraman, S. Levine, and A. Kumar. Scaling test-time compute without\nverification or rl is suboptimal. arXiv preprint arXiv:2502.12118 , 2025.\n[75] B. Shi, M. Tang, K. R. Narasimhan, and S. Yao. Can language models solve olympiad\nprogramming? In Conference on Language Modeling , 2024.\n[76] C. V . Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM test-time compute optimally can\nbe more effective than scaling parameters for reasoning. In The Thirteenth International\nConference on Learning Representations , 2025.\n[77] Y . Song, G. Wang, S. Li, and B. Y . Lin. The good, the bad, and the greedy: Evaluation of llms\nshould not ignore non-determinism. In arXiv , 2024.\n[78] Y . Song, H. Zhang, C. Eisenach, S. Kakade, D. Foster, and U. Ghai. Mind the gap: Examining\nthe self-improvement capabilities of large language models. arXiv preprint arXiv:2412.02674 ,\n2024.\n[79] Z. Sun, L. Yu, Y . Shen, W. Liu, Y . Yang, S. Welleck, and C. Gan. Easy-to-hard generalization:\nScalable alignment beyond human supervision. In The Thirty-eighth Annual Conference on\nNeural Information Processing Systems , 2024.\n[80] Y . Tian, B. Peng, L. Song, L. Jin, D. Yu, L. Han, H. Mi, and D. Yu. Toward self-improvement\nof LLMs via imagination, searching, and criticizing. In Conference on Neural Information\nProcessing Systems , 2024.\n[81] J. V on Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov,\nand M. Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint\narXiv:2212.07677 , 2022.\n[82] J. V on Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov,\nand M. Vladymyrov. Transformers learn in-context by gradient descent. In International\nConference on Machine Learning , pages 35151–35174. PMLR, 2023.\n[83] Z. Wan, X. Feng, M. Wen, S. M. McAleer, Y . Wen, W. Zhang, and J. Wang. Alphazero-like\ntree-search can guide large language model decoding and training. In Forty-first International\nConference on Machine Learning , 2024.\n[84] X. Wang, J. Wei, D. Schuurmans, Q. V . Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou.\nSelf-consistency improves chain of thought reasoning in language models. In The Eleventh\nInternational Conference on Learning Representations , 2023.\n[85] Y . Wang, Q. Liu, J. Xu, T. Liang, X. Chen, Z. He, L. Song, D. Yu, J. Li, Z. Zhang, et al.\nThoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint\narXiv:2501.18585 , 2025.\n[86] C. Wei, Y . Chen, and T. Ma. Statistically meaningful approximation: a case study on ap-\nproximating turing machines with transformers. Advances in Neural Information Processing\nSystems , 35:12071–12083, 2022.\n18\n--- Page 19 ---\n[87] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou, et al. Chain-of-\nthought prompting elicits reasoning in large language models. Advances in neural information\nprocessing systems , 35:24824–24837, 2022.\n[88] S. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y . Choi. Generating\nsequences by learning to self-correct. In The Eleventh International Conference on Learning\nRepresentations , 2023.\n[89] Y . Wu, Z. Sun, S. Li, S. Welleck, and Y . Yang. Scaling inference computation: Compute-\noptimal inference for problem-solving with language models. In Workshop on Mathematical\nReasoning and AI at NeurIPS’24 , 2024.\n[90] Y . Wu, Z. Sun, S. Li, S. Welleck, and Y . Yang. Inference scaling laws: An empirical analysis\nof compute-optimal inference for LLM problem-solving. In The Thirteenth International\nConference on Learning Representations , 2025.\n[91] Y . Wu, Y . Wang, T. Du, S. Jegelka, and Y . Wang. When more is less: Understanding chain-of-\nthought length in llms. In arXiv , 2025.\n[92] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with\nattention sinks. arXiv preprint arXiv:2309.17453 , 2023.\n[93] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv,\nC. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu,\nJ. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng,\nM. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li,\nT. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y . Fan, Y . Su, Y . Zhang, Y . Zhang,\nY . Wan, Y . Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report.\narXiv preprint arXiv:2505.09388 , 2025.\n[94] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process\nbounded hierarchical languages. arXiv preprint arXiv:2105.11115 , 2021.\n[95] C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar. Are transformers universal\napproximators of sequence-to-sequence functions? In International Conference on Learning\nRepresentations , 2020.\n[96] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula,\nQ. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural\ninformation processing systems , 33:17283–17297, 2020.\n[97] D. Zhang, S. Zhoubian, Z. Hu, Y . Yue, Y . Dong, and J. Tang. ReST-MCTS*: LLM self-training\nvia process reward guided tree search. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems , 2024.\n[98] K. Zhang, G. Li, H. Zhang, and Z. Jin. Hirope: Length extrapolation for code models using\nhierarchical position. arXiv preprint arXiv:2403.19115 , 2024.\n[99] Q. Zhang, F. Lyu, Z. Sun, L. Wang, W. Zhang, Z. Guo, Y . Wang, I. King, X. Liu, and C. Ma.\nWhat, how, where, and how well? a survey on test-time scaling in large language models.\narXiv preprint arXiv:2503.24235 , 2025.\n19\n--- Page 20 ---\n[100] Y . Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang. Small language\nmodels need strong verifiers to self-correct reasoning. In ACL (Findings) , 2024.\n[101] Y . Zhang, S. Wu, Y . Yang, J. Shu, J. Xiao, C. Kong, and J. Sang. o1-coder: an o1 replication\nfor coding. In arXiv , 2024.\n[102] H. Zhao, A. Panigrahi, R. Ge, and S. Arora. Do transformers parse while predicting the masked\nword? In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , pages 16513–16542, Singapore, Dec.\n2023. Association for Computational Linguistics.\n20\n--- Page 21 ---\nA Proofs\nA.1 Proof of Theorem 3.1\nProof. WriteO={1, . . . , O }(O∈Z+) where iis the i-th most likely answer and let nidenote the\nnumber of occurrences of i. Then we have\nˆp=1\nn(n1, . . . , n O)∼1\nnMultinomial( n, p),\nwhere p= (p(1), . . . , p (O)).\nUpper bound. When n≥2 log(1 /δ)\n∆2 we apply Claim A.5 to obtain that with probability at least\n1−δ,\n∥ˆp−p∥1≤r\n2 ln(1 /δ)\nn≤∆.\nUnder this event, we have that for any i >1\nn1−ni=n·(ˆp1−ˆpi)\n≥n·(p1−pi− ∥ˆp−p∥1)\n≥0\nand hence the correct answer 1is the most consistent answer. It follows that self-consistency can\nproduce the correct answer with probability at least 1−δ.\nLower bound. When n≤1\n∆2, we construct the hard instance where p1= (1 + ∆) /2, p2=\n(1−∆)/2and∆<0.00001 . Ifn≤1\n∆then by the proof of Theorem 3.2, with constant probability\nthe correct answer is not generated at all and hence self-consistency fails to produce the correct\nanswer. Otherwise n≥1\n∆≥10000 . We may write X:=n1−n2−n∆√nas a sum of i.i.d. random\nvariables divided by√n:\nX=Pn\ni=1Yi√n,\nwhere E(Yi) = 0 , σ2=E(Y2\ni)≥1/2, ρ=E(|Yi|3)≤1. By Claim A.6, we have that\nP(n1< n2) =P(X <−1)\n≥Φ(−1)−8ρ\nσ3√n\n≥0.01.\nThus in both cases, self-consistency fails to produce the correct answer with constant probability.\nA.2 Proof of Theorem 3.2\nProof. WriteO={1, . . . , O }where iis the i-th most likely answer and let nidenote the number of\noccurrences of i. Then we have\np(1)≥p(2) + ∆ ≥∆.\nNote that for best-of- n, correctness is achieved if the correct answer appears at least once among n\nindependent samples.\n21\n--- Page 22 ---\nUpper bound. When n≥2 log(1 /δ)\n∆, we have\nP(Best-of- noutputs correct answer ) = 1−(1−p(1))n\n≥1−(1−∆)2 log(1 /δ)\n∆\n≥1−δ.\nThis confirms that best-of- nachieves the correct answer with 1−δprobability.\nLower bound. When n≤1\n∆, we construct the hard instance where p(1) = ∆+(1 −∆)/O, p (2) =\n···=p(O) = (1 −∆)/Oand∆<0.0000001 . Since the correct answer occurs with probability at\nleast∆, we have:\nP(Best-of- noutputs correct answer ) = 1−(1−p(1))n\n≤1−(1−2∆)1\n∆\n≤0.99.\nThis confirms that best-of- nfails to produce the correct answer with constant probability.\nA.3 Proof of Proposition 4.2\nWe first introduce the following result that extends any Transformer to a larger vocabulary, so that it\nonly attends to tokens in its original vocabulary.\nProposition A.1 (Extended Representation to Multiple Token Spaces) .For any H, L, N max∈Z+,\nV1∩ V0=∅, there exists a general-purpose Transformer ϕof type (O(1), O(logNmax))such\nthat for any Transformers f= (θ,pe,(K(l)\nh,Q(l)\nh,V(l)\nh)h∈[H],l∈[L], ϑ,V1)over vocabulary V1, the\nTransformer ef=ϕ(f1)satisfies the following property: for any token sequence v=v1···vnsuch\nthatn≤Nmax, denote {i1<···< im}={i:vi∈ V1}, then we have\npef(·|v) =pf(·|u),\nwhere u=vi1···vim.\nProof. Set constants Bv, Bqk, Bθsuch that for any layer land head h, it holds that\r\r\r(Q(l)\nh)⊤K(l)\nh\r\r\r\n2≤\nBqk,\r\r\rV(l)\nh\r\r\r\n2≤Bv, and∥θ(v)∥2≤Bθholds for all v∈ V. LetB= (HBv)LBqkBθ, C= 4B2+\nlog(1/ϵ), C0= 4C. By Lemma A.3, there exists α1, . . . , α Nmax, β0, β1∈Rd0andA0, A1, A∈\nRd0×d0ford0≤O(logNmax)such that\n1. For any i≥j1, j2, j3:\n(αi+β1)⊤A0(αj1+β1) = (αi+β1)⊤A0(αj2+β1)≥(αi+β1)⊤A0(αj1+β0) +C0\n(αi+β0)⊤A0(αi+β0)≥(αi+β0)⊤A0(αj1+β1) +C0, (3)\n2. For any i > j\n(αi+β1)⊤A(αi+β1)≥(αi+β1)⊤A(αj+β1) +C0\n≥(αi+β1)⊤A(αj+β0) + 2C0, (4)\n3. For any i≥j, j1\n(αi+β1)⊤A1(αj+β0) = (αi+β1)⊤A1(αj1+β1) +C0\n(αi+β1)⊤A1(αi+β1)≥max{(αi+β1)⊤A1(αj1+β1),(αi+β1)⊤A1(αj1+β0)}+C0.\n(5)\n22\n--- Page 23 ---\nWe define ϕas follows: for any Transformers f= (θ,pe,(K(l)\nh,Q(l)\nh,V(l)\nh)h∈[H],l∈[L], ϑ,V1),\nthe Transformer ef=ϕ(f)is given by\n(eθ,fpe,(eK(l)\nh,eQ(l)\nh,eV(l)\nh)h∈[H+1],l∈[L],eϑ,V1∪ V0),\nwhere the tokenizer is given by\neθ(v) = 1(v∈ V1)·\u0012θ(v)\nβ1\u0013\n+ 1(v∈ V0)·\u00120\nβ0\u0013\n,\nthe positional encoder is given by\nfpe\u0012\u0012x\ny\u0013\n;v1, . . . , v i\u0013\n=\u0012pe (x;u)\nαi+y\u0013\n,\nwhere u=vi1···vimandx∈Rd; forl= 1, . . . , L the key, query, value matrices are given by\neK(l)\nh=\u0012\nK(l)\nh\nA0\u0013\n,eQ(l)\nh=\u0012\nQ(l)\nh\nI\u0013\n,\neV(l)\nh=\u0012\nV(l)\nh\n0\u0013\n,\neK(l)\nH+1=\u00120\nA\u0013\n,eQ(l)\nH+1=\u00120\nI\u0013\n,eV(l)\nH+1=\u00120\nI\u0013\n.\nThe output feature is given by eϑ(y) =\u0012\nϑ(y)\n0\u0013\n. Since i1, . . . , i monly depends on whether vi’s\nbelong to the set V1, the generalized position encoding peis well-defined. It can be verified that ϕis\nindeed a general-purpose Transformer of type (O(1), O(logNmax)).\nWe show that for any l= 1, . . . , L ,\neX(l)\ni=\u0012\nX(l)\ni\neαi\u0013\n,∀i=i1, . . . , i m (6)\nwhere X(l)\niis the l-th layer of Transformer fat position i(attending only to positions i1, . . . , i m)\nsuch that\n∥X(l)\ni∥2≤Bθ(HBv)l, (7)\nand\neX(l)\nj=\u00120\neαj\u0013\n,∀j /∈ {i1, . . . , i m} (8)\nwhereeαi=αi+ 1(v∈ V0)·β0+ 1(v∈ V1)·β1.\nWe prove these results by induction. The case l= 1folows directly from the definitions of the\ntokenizer.\n23\n--- Page 24 ---\nProve Eq. (6).Suppose Eq. (6) and Eq. (8) hold for 1, . . . , l −1-th layer, and consider l-the layer.\nWe have\neX(l+1)\ni =HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj)\u0011\neZ(l)\nh·eV(l)\nheX(l)\nj\n| {z }\nterm 1\n+iX\nj=1exp\u0010\n(eQ(l)\nH+1eX(l)\ni)⊤(eK(l)\nH+1eX(l)\nj)\u0011\neZ(l)\nH+1·eV(l)\nH+1eX(l)\nj\n| {z }\nterm 2.\nEq. (3) ensures that for any i, i′∈ {i1, . . . , i m}, j /∈ {i1, . . . , i m}:\n(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\ni′) = (Q(l)\nheX(l)\ni)⊤(K(l)\nheX(l)\ni′) + (αi+β1)⊤A0(αi′+β1)\n≥(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj) + (αi+β1)⊤A0(αj+β0) +C\n= (eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj) +C,\nand if i, j1, j2∈ {i1, . . . , i m}\n(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj1)−(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj2)\n= (Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj1) + (αi+β1)⊤A0(αj1+β1))−(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj2)−(αi+β1)⊤A0(αj2+β1)\n= (Q(l)\nhX(l)\ni)⊤(K(l)\nheX(l)\nj1)−(Q(l)\nheX(l)\ni)⊤(K(l)\nhX(l)\nj2),\nwhere we use the fact that C0≥C+ 2 max h,l,i,j\f\f\f(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj)\f\f\f. Since the transformers\nhave precision ϵandC≥2 max h,l,i,j\f\f\f(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj)\f\f\f+ log(1 /ϵ), it follows that the\nattention weights of head (k−1)H+his identical to the attention weights of expert k, i.e.\nexp\u0010\n(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj)\u0011\neZ(l)\nh= 1(j∈ {i1, . . . , i m})·exp\u0010\n(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj)\u0011\nZ(l)\nh.\nTherefore\nterm 1 =HX\nh=1X\nj=i1,...,imexp\u0010\n(Q(l)\nhX(l)\ni)⊤(K(l)\nhX(l)\nj)\u0011\nZ(l)\nh·\u0012\nV(l)\nhX(l)\nj\n0\u0013\n=\u0012\nX(l+1)\nj\n0\u0013\n.\nFurthermore, by Eq. (4) we have for any j < i\n(eQ(l)\nH+1eX(l)\ni)⊤(eK(l)\nH+1eX(l)\ni) =eα⊤\niAeαi\n≥eα⊤\niAeαj+C\n= (eQ(l)\nH+1eX(l)\ni)⊤(eK(l)\nH+1eX(l)\nj) +C,\nand hence the attention weighs concentrates on iitself. Thus\nterm 2 =\u0012\n0\nI\u0013\n·\u0012\nX(l)\ni\neαi\u0013\n=\u0012\n0\neαi\u0013\n.\nCombining, we derive Eq.(6) for (l+ 1) -th layer.\n24\n--- Page 25 ---\nProve Eq. (7).From above,\n∥X(l+1)\ni∥2=\r\r\r\r\r\rHX\nh=1iX\nj=1exp\u0010\n(eQ(l)\nheX(l)\ni)⊤(eK(l)\nheX(l)\nj)\u0011\neZ(l)\nh·V(l)\nhX(l)\nj\r\r\r\r\r\r\n2\n≤HBv·max\nj≤i∥X(l)\nj∥2\n≤Bθ(HBv)l+1.\nThis confirms Eq. (24) for l+ 1.\nProve Eq. (8).Notice that Eq. (3) ensures that for any j, j′/∈ {i:vi∈ V1}andi∈ {i:vi∈ V1}:\n(eQ(l)\nheX(l)\nj)⊤(eK(l)\nheX(l)\nj′) = (Q(l)\nhX(l)\nj)⊤(K(l)\nhX(l)\nj′) + (αj+β0)⊤A0(αj′+β0)\n≥(Q(l)\nhX(l)\nj)⊤(K(l)\nhX(l)\ni) + (αj+β0)⊤A0(αi+β1) +C\n= (eQ(l)\nheX(l)\nj)⊤(eK(l)\nheX(l)\ni) +C.\nIt follows that the attention weights is concentrated on the compliment of {i:vi∈ V1}itself, and\ntherefore Eq. (8) follows by a simple induction argument.\nFinally, at the output layer\npef(y|v1, . . . , v n) = Softmax( eϑ(y)⊤eX(L)\nn)\n= Softmax( ϑ(y)⊤X(L)\nm)\n=pfκ(y|u).\nThis establishes the desired statement.\nNow we return to the proof of Proposition 4.2.\nProof. By Proposition A.1, it suffices to construct general-purpose Transformer ϕsuch that\npef(·|v) =pfκ(·|u),\nwhere u=v1···vi0−1vi0+1···vn, because then the eϕgiven by\neϕ(f1, . . . , f K) =ϕ(ϕe(f1), . . . , ϕ e(fK))\nsatisfies the requirement, where ϕeis the general-purpose Transformer that extends the KTransform-\ners to the larger vocabulary V:=∪K\nk=1Vkas given by Proposition A.1.\nSet constants Bv, Bqk, Bθsuch that for any layer land head h, it holds that\r\r\r(Q(l)\nh)⊤K(l)\nh\r\r\r\n2≤\nBqk,\r\r\rV(l)\nh\r\r\r\n2≤Bv, and∥θ(v)∥2≤Bθholds for all v∈ V. Let B= (KHB v)LBqkBθ, C=\n4B2+ log(1 /ϵ), C0= 4C. By Lemma A.3, there exists α1, . . . , α N, β0, β1, . . . , β K∈Rd0and\nA1, . . . , A K∈Rd0×d0ford0≤O(K+ log Nmax)such that\n1. For any i≥j1, j2, j3andk, k′, k′′̸= 0:\n(αi+βk)⊤A0(αj1+βk′) = (αi+βk)⊤A0(αj2+βk′′)≥(αi+βk)⊤A0(αj1+β0) +C0\n(αi+β0)⊤A0(αi+β0)≥(αi+β0)⊤A0(αj1+βk) +C0, (9)\n2. For any i > j andk̸=k′̸= 0\n(αi+βk)⊤A(αi+βk)≥(αi+βk)⊤A(αj+βk′) +C0\n≥(αi+βk)⊤A(αj+β0) + 2C0, (10)\n25\n--- Page 26 ---\n3. For any i≥j, j1andk̸=k′, k′′\n(αi+βk)⊤Ak′(αj+β0) = (αi+βk)⊤Ak′(αj1+βk′′) +C0\n(αi+βk)⊤Ak(αi+βk)≥max{(αi+βk)⊤Ak(αj1+βk′′),(αi+βk)⊤Ak′(αj1+β0)}+C0,\n(11)\nWe define ϕas follows: for any Transformers\nfk= (θk,pek,(K(l)\nk;h,Q(l)\nk;h,V(l)\nk;h)h∈[H],l∈[L], ϑk,Vk),\noverVk, k∈[K], the Transformer ef=ϕ(f1, . . . , f K)is given by\n(eθ,fpe,(eK(l)\nh,eQ(l)\nh,eV(l)\nh)h∈[KH+1],l∈[L+1],eϑ,V),\nwhere the tokenizer is given by\neθ(v) = 1(v /∈ V0)·\nθ1(v)\n...\nθK(v)\n0\n+\n0\n...\n0\nβE(v)\n,\nwhere E(v) =kiffv∈ Vk. Let the positional encoder be given by\nfpe\u0012\u0012x\ny\u0013\n;v1, . . . , v i\u0013\n=\npe1(x;u)\n...\npeK(x;u)\nαi+y\n,\nwhere x∈Rdanduis the sub-sequence of vthat omits vi0(if any); for l= 1, . . . , L the key, query,\nvalue matrices are given by\neK(l)\n(k−1)H+h=\n0\n...\nK(l)\nk;h\n...\nA0\n,eQ(l)\n(k−1)H+h=\n0\n...\nQ(l)\nk;h\n...\nI\n,\neV(l)\n(k−1)H+h=\n0\n...\nV(l)\nk;h\n...\n0\n,\neK(l)\nKH+1=\n0\n...\n0\nA\n,eQ(l)\nKH+1=\n0\n...\n0\nI\n,eV(l)\nKH+1=\n0\n...\n0\nI\n,\n26\n--- Page 27 ---\nwhere the submatrices K(l)\nk;h,Q(l)\nk;h,V(l)\nk;hare located in the k-th diagonal block, and for the final layer\neK(L+1)\nk=\n0\n...\n0\nAk\n,eQ(L+1)\nk=\n0\n...\n0\nI\n,eV(L+1)\nk=\n0\n...\nI\n...\n0\n,\nwhere the identity sub-matrix in eV(L+1)\nkis located in the k-th block. The output feature is given by\neϑ(y) =\nϑ1(y)\n...\nϑK(y)\n0\n. Since u(k)’s only depend on set membership information of vi’s, the generalized\nposition encoding peis well-defined. We can easily verify that ϕis indeed a general-purpose\nTransformer of type (O(K), O(logNmax)).\nWe show that for any l= 1, . . . , L ,\neX(l)\ni=\nX(l)\n1;i\n...\nX(l)\nK;i\neαi\n,∀i̸=i0 (12)\nwhere X(l)\nk;iis the l-th layer of Transformer kat position i(attending to all positions but i0) such that\n∥X(l)\nk;i∥2≤Bθ(KHB v)l. (13)\nand\neX(l)\ni0=\n0\n...\n0\neαi0\n(14)\nwhereeαi=αi+βE(vi).\nWe prove these results by induction. The case l= 1folows directly from the definitions of the\ntokenizer.\nProve Eq. (12).Suppose Eq. (12) and Eq. (14) hold for 1, . . . , l −1=th layer, and consider l-the\nlayer. We have\neX(l+1)\ni =KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·eV(l)\n(k−1)H+heX(l)\nj\n| {z }\nterm 1\n+iX\nj=1exp\u0010\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj)\u0011\neZ(l)\nKH+1·eV(l)\nKH+1eX(l)\nj\n| {z }\nterm 2.\n27\n--- Page 28 ---\nEq. (9) ensures that for any j1< j2≤isuch that i0/∈ {i, j1, j2}:\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj1) = (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j1) + (αi+βE(i))⊤A0(αj1+βE(j1))\n≥(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j1) + (αi+βE(i))⊤A0(αi0+βE(i0)) +C\n= (eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\ni0) +C.\nand\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj1)−(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj2)\n= (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j1) + (αi+βE(i))⊤A0(αj1+βE(j1))\n−(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j2)−(αi+βE(i))⊤A0(αj2+βE(j2))\n= (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j1)−(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j2).\nIt follows from the precision ϵof the transformers that the attention weights of head (k−1)H+h\nis identical to the attention weights of expert k, i.e.\nexp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h=exp\u0010\n(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j)\u0011\nZ(l)\nk;h.\nTherefore\nterm 1 =KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j)\u0011\nZ(l)\nk;h·\n0\n...\nV(l)\nk;hX(l)\nk;j\n...\n0\n=\nX(l)\n1;i\n...\nX(l)\nK;i\n0\n.\nFurthermore, by Eq. (10) we have for any j < i\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\ni) =eα⊤\niAeαi\n≥eα⊤\niAeαj+C\n= (eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj) +C\nand hence the attention weighs concentrates on iitself. Thus\nterm 2 =\n0\n...\n0\nI\n·\nX(l)\n1;i\n...\nX(l)\nK;i\neαi\n=\n0\n...\n0\neαi\n\nCombining these two terms, we confirm that Eq.(12) holds for (l+ 1) -th layer.\n28\n--- Page 29 ---\nProve Eq. (13).From above,\n∥X(l+1)\nk;i∥2=\r\r\r\r\r\rKX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·V(l)\nk;hX(l)\nk;j\r\r\r\r\r\r\n2\n≤KHB v·max\nj≤i∥X(l)\nk;j∥2\n≤Bθ(KHB v)l+1.\nThis confirms Eq. (13) for l+ 1.\nProve Eq. (14).Notice that Eq. (9) ensures that for any j≤i0:\n(eQ(l)\n(k−1)H+heX(l)\ni0)⊤(eK(l)\n(k−1)H+heX(l)\ni0) = (Q(l)\nk;hX(l)\nk;i0)⊤(K(l)\nk;hX(l)\nk;i0) + (αi0+βE(i0))⊤A0(αi0+βE(i0))\n≥(Q(l)\nk;hX(l)\nk;i0)⊤(K(l)\nk;hX(l)\nk;j) + (αi0+βE(i0))⊤A0(αj+βE(j)) +C\n= (eQ(l)\n(k−1)H+heX(l)\ni0)⊤(eK(l)\n(k−1)H+heX(l)\nj) +C.\nIt follows that the attention weights of head (k−1)H+his concentrated on i0itself, therefore\nterm 1 =KX\nk=1HX\nh=1\n0\n...\nV(l)\nk;h·0\n...\n0\n= 0.\nBy the same argument, for i=i0we have\nterm 2 =\n0\n...\n0\nI\n·\n0\n...\n0\neαi0\n=\n0\n...\n0\neαi0\n.\nCombining these confirms Eq. (14).\nNext, we show that the last layer satisfies\neX(L+1)\nn =\n0\n...\nX(L+1)\nκ;n\n...\n0\n(15)\nwhere X(L+1)\nκ;n is the κ-th block. To see this, we notice that Eq. (11) implies the followings (the\nproofs are identical to the above):\n1. Attention sink to dummny token vi0for mismatch expert: for any k′̸=κandj≤nwe have\n(eQ(L)\n(k′−1)H+heX(L)\nn)⊤(eK(L)\n(k′−1)H+heX(L)\nj) = (αn+βE(n))⊤Ak′(αj+βE(j))\n≤(αn+βE(n))⊤Ak′(αi0+βE(i0))−C\n= (eQ(L)\n(k′−1)H+heX(L)\nn)⊤(eK(L)\n(k′−1)H+heX(L)\ni0)−C.\n(16)\n29\n--- Page 30 ---\n2. Attention to oneself for matching expert: for any j̸=i0we have\n(eQ(L)\n(κ−1)H+heX(L)\nn)⊤(eK(L)\n(κ−1)H+heX(L)\nj) = (αn+βE(n))⊤Aκ(αj+βE(j))\n≥(αn+βE(n))⊤Aκ(αi0+βE(i0)) +C\n= (eQ(L)\n(κ−1)H+heX(L)\nn)⊤(eK(L)\n(κ−1)H+heX(L)\ni0) +C,\n(17)\nand\n(eQ(L)\n(κ−1)H+heX(L)\nn)⊤(eK(L)\n(κ−1)H+heX(L)\nn) = (αn+βE(n))⊤Aκ(αn+βE(n))\n≥(αn+βE(n))⊤Aκ(αj+βE(j)) +C\n= (eQ(L)\n(κ−1)H+heX(L)\nn)⊤(eK(L)\n(κ−1)H+heX(L)\nj) +C.\n(18)\nCombining Eq. (16), Eq. (17), and Eq. (18), we have\nexp\u0010\n(eQ(L)\n(k−1)H+heX(L)\nn)⊤(eK(L)\n(k−1)H+heX(L)\nj)\u0011\nZ(l)\nk=(\nδi0\nj, k̸=κ\nδn\nj, k =κ\nIt follows that\neX(L+1)\nn =eV(L)\n(κ−1)H+h·eX(L)\nn+X\nk̸=κV(L)\n(κ−1)H+h·eX(L)\ni0\n=\n0\n...\nI\n...\n0\n·\nX(L)\n1;i\n...\nX(L)\nK;i\neαi\n=\n0\n...\nX(L)\nκ;n\n...\n0\n.\nTherefore we establish Eq. (15).\nFinally, at the output layer\npef(y|v1, . . . , v n) = Softmax( eϑ(y)⊤eX(L+1)\nn )\n= Softmax( ϑ(y)⊤Y(L)\nn−1)\n=pfκ(y|u).\nThis establishes the desired statement.\nA.4 Proof of Proposition 4.4\nProof. Set constants Bv, Bqk, Bθsuch that for any layer land head h, it holds that\r\r\r(Q(l)\nh)⊤K(l)\nh\r\r\r\n2≤\nBqk,\r\r\rV(l)\nh\r\r\r\n2≤Bv, and∥θ(v)∥2≤Bθholds for all v∈ V. Let B= (KHB v)LBqkBθ, C=\n2B2+ log(1 /ϵ), C0= 4C. Define ι(i) = uiffξu≤i < ξ u+1(ξ0=−1, ξm+1=∞by\ndefault). Let E(·)denote the task id indicated by the special token. By Lemma A.2, there exists\nα1, . . . , α N, β1, . . . , β K∈Rd0andA1, . . . , A K∈Rd0×d0ford0≤O(K+ log Nmax)such that\nfor any n≤Nwe have\n30\n--- Page 31 ---\n1. For any k̸=k′:\nα⊤\nnAk(αn+βk′)≥C0+\n\nα⊤\nnAkαn\nα⊤\nnAkαj\nα⊤\nnAk(αj+βk′′),∀0≤j≤n,1≤k′′≤K. (19)\n2. For any k∈[K]:\nα⊤\nnAkαn=α⊤\nnAkα0≥C0+\n\nα⊤\nnAk(αn+βk)\nα⊤\nnAkαj\nα⊤\nnAk(αj+βk′),∀0< j < n, k′̸=k. (20)\n3. For any k, k′, k′′∈[K]:\n(αn+βk′)⊤Ak(αn+βk′)≥C0+ (αn+βk′)⊤Akαj,∀0≤j≤n. (21)\n4. For any 0< j < n :\nα⊤\nnAαn≥α⊤\nnA(αn+βk) +C0\n≥C0+ max {α⊤\nnAαj, α⊤\nnA(αj+βk′)},∀k, k′′∈[K]. (22)\nWe define ϕas follows: for any Transformers\nfk= (θk,pek,(K(l)\nk;h,Q(l)\nk;h,V(l)\nk;h)h∈[H],l∈[L], ϑk,V), k∈[K]\noverV, the Transformer ef=ϕ(f1, . . . , f K)is given by\n(eθ,fpe,(eK(l)\nh,eQ(l)\nh,eV(l)\nh)h∈[KH+1],l∈[L],eϑ,V ∪Ω),\nwhere the tokenizer is given by\neθ(v) =\nθ1(v)\n...\nθK(v)\n0\n, v∈ V,eθ(ω) =\n0\n...\n0\nβE(ω)\n, ω∈Ω,\nthe positional encoder is given by\nfpe\u0012\u0012\nx\ny\u0013\n;v1, . . . , v i\u0013\n=\npe1(x;v1,···, vξ1−1, vξm+1,···, vn)\n...\npeK(x;v1,···, vξ1−1, vξm+1,···, vn)\nαι(i)+y\n,\n31\n--- Page 32 ---\nwhere x∈Rd; forl= 1, . . . , L the key, query, value matrices are given by\neK(l)\n(k−1)H+h=\n0\n...\nK(l)\nk;h\n...\nAk\n,eQ(l)\n(k−1)H+h=\n0\n...\nQ(l)\nk;h\n...\nI\n,\neV(l)\n(k−1)H+h=\n0\n...\nV(l)\nk;h\n...\n0\n,\neK(l)\nKH+1=\n0\n...\n0\nA\n,eQ(l)\nKH+1=\n0\n...\n0\nI\n,eV(l)\nKH+1=\n0\n...\n0\nI\n,\nwhere the submatrices K(l)\nk;h,Q(l)\nk;h,V(l)\nk;hare located in the k-th diagonal block.\nThe output feature is given by eϑ(y) =\nϑ1(y)\n...\nϑK(y)\n0\n. Since ξ1, ξmonly depends on whether vi’s\nbelong to the set Ω, the generalized position encoding peis well-defined. We can easily verify that ϕ\nis indeed a general-purpose Transformer of type (O(K), O(logNmax)).\nLeteX(l)\n1, . . . ,eX(l)\nnrepresent the l-th hidden layer. Our goal is to show that for any l= 1, . . . , L ,\neX(l)\nican be written as:\neX(l)\ni=\nX(l)\n1;i\n...\nX(l)\nK;i\neαi\n, i= 1, . . . , n, (23)\nwhereeαi=αι(i)+ 1(ι(i) =i)·βE(vi)andX(l)\nk;i∈Rdsuch that\n∥X(l)\nk;i∥2≤Bθ(KHB v)l. (24)\nIn particular, for i= 1, . . . , m we have\nX(l)\nk;ξi= 0,∀k= 1, . . . , K, (25)\nand for j= 1, . . . , ξ 1we have\nX(l)\nk;j=Y(l)\nk;j,∀k= 1, . . . , K, (26)\nand for j= 1, . . . , ξ 1−1, ξm+ 1, . . . , n we have\nX(l)\nκ;j=Y(l)\nκ,j−ξm−1+ξ1, X(l)\nk′;j= 0,∀k′̸=κ, (27)\nwhere Y(l)\nk;jis the l-th hidden layer of fk(attending only to positions 1, . . . , ξ 1−1, ξm+ 1, . . . , n ) .\n32\n--- Page 33 ---\nThus we apply induction on l. The case l= 1 holds trivially from the definition of eθandfpe.\nSuppose the above relationship holds for all layers 1, . . . , l , consider layer l+ 1. We have\neX(l+1)\ni =KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·eV(l)\n(k−1)H+heX(l)\nj\n| {z }\nterm 1\n+iX\nj=1exp\u0010\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj)\u0011\neZ(l)\nKH+1·eV(l)\nKH+1eX(l)\nj\n| {z }\nterm 2,\nwhere\neZ(l)\n(k−1)H+h=iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\n.\nBy induction hypothesis,\neX(l)\ni=\nX(l)\n1;i\n...\nX(l)\nK;i\neαi\n,\nandX(l)\nk;i=Y(l)\nζ(i)fori= 1, . . . , ξ 1−1, ξm+ 1, . . . , n , where ζ(i) :=(\ni, i < ξ 1\ni−ξm−1 +ξ1, i > ξ m.\nNotice that for j≤i:\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj) = (X(l)\nk;i)⊤(Q(l)\nk;h)⊤K(l)\nk;hX(l)\nk;j+eα⊤\niAkeαj,\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj) =eα⊤\niAeαj.\nProve Eq (23).By properties of α, β, A , for any j2< ξu< j1< i < ξ u+1notice that:\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj1)≥(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nξu) +C\n≥(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj2) + 2C.\nDue to ϵ-precision of transformers, this implies that\nexp\u0010\n(eQ(l)\nKH+1eX(l)\ni)⊤(eK(l)\nKH+1eX(l)\nj)\u0011\nZ(l)\nKH+1=(1(j>ξu)\ni−ξu, ξ u< i < ξ u+1\nδj\nξl, i =ξu,\n33\n--- Page 34 ---\nand hence for ξu< i < ξ u+1\neX(l+1)\ni =KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·eV(l)\n(k−1)H+h\n...\nX(l)\nk;j\n...\n0\n\n+iX\nj=ξu+1·1\ni−ξu·\n0\n...\n0\nαι(i)\n\n=\nX(l+1)\n1;i\n...\nX(l+1)\nK;i\neαi\n,\nand for i=ξu\neX(l+1)\ni =KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·eV(l)\n(k−1)H+h\n...\nX(l)\nk;j\n...\n0\n+\n0\n...\n0\nαι(i)+βE(vi)\n\n=\nX(l+1)\n1;i\n...\nX(l+1)\nK;i\neαi\n,\nwhere\nX(l+1)\nk;i=KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·V(l)\nk;hX(l)\nk;j. (28)\nThis confirms Eq. (23) for l+ 1.\nProve Eq. (24).From above,\n∥X(l+1)\nk;i∥2=\r\r\r\r\r\rKX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h·V(l)\nk;hX(l)\nk;j\r\r\r\r\r\r\n2\n≤KHB v·max\nj≤i∥X(l)\nk;j∥2\n≤Bθ(KHB v)l+1.\nThis confirms Eq. (24) for l+ 1.\n34\n--- Page 35 ---\nProve Eq. (25).We first show X(l)\nk;ξ1= 0. Indeed, by the properties of αt, βk, for any j≤ξ1\n(eQ(l)\n(k−1)H+heX(l)\nξ1)⊤(eK(l)\n(k−1)H+heX(l)\nξ1)\n= (X(l)\nk;ξ1)⊤(Q(l)\nk;h)⊤K(l)\nk;hX(l)\nk;ξ1+ (α0+βE(vξ1))⊤Ak(α0+βE(vξ1))\n≥(X(l)\nk;ξ1)⊤(Q(l)\nk;h)⊤K(l)\nk;hX(l)\nk;ξ1+ (α0+βE(vξ1))⊤Akα0+C\n= (eQ(l)\n(k−1)H+heX(l)\nξ1)⊤(eK(l)\n(k−1)H+heX(l)\nj) +C\nIt follows from Eq. (28) that\nX(l+1)\nk;ξ1=KX\nk=1HX\nh=1V(l)\nk;hX(l)\nk;ξ1= 0.\nForξi(i >1), we apply the same argument again to obtain that for any j≤ξisuch that j /∈ {ξ1<\n···< ξι(n)}and any i′< i,\n(eQ(l)\n(k−1)H+heX(l)\nξi)⊤(eK(l)\n(k−1)H+heX(l)\nξk′)\n≥(eQ(l)\n(k−1)H+heX(l)\nξ1)⊤(eK(l)\n(k−1)H+heX(l)\nj) +C\nThis implies that the attention weights are supported on {ξ1<···< ξi}, and therefore\nX(l+1)\nk;ξi=KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(eQ(l)\n(k−1)H+heX(l)\nξi)⊤(eK(l)\n(k−1)H+heX(l)\nξj)\u0011\neZ(l)\n(k−1)H+h·V(l)\nk;hX(l)\nk;ξj= 0\nwhere we apply the induction hypothesis k;X(l)\nξj= 0for all j= 1, . . . , i −1. This thus completes\nthe proof of Eq. (25).\nProve Eq. (26).When j1< j2≤i < ξ 1, we have\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj1)−(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+hX(l)\nj2)\n= (X(l)\nk;i)⊤(Q(l)\nk;h)⊤K(l)\nk;hX(l)\nk;j1+α⊤\n0Akα⊤\n0\n−(X(l)\nk;i)⊤(Q(l)\nk;h)⊤K(l)\nk;hX(l)\nk;j2−α⊤\n0Akα⊤\n0\n= (Q(l)\nk;hY(l)\nk;i)⊤(K(l)\nk;hY(l)\nk;ji)−(Q(l)\nk;hY(l)\nk;i)⊤(K(l)\nk;hY(l)\nk;j2).\nIt follows that\neZ(l)\n(k−1)H+h=iX\nj=1exp\u0010\n(Q(l)\nk;hY(l)\nk;i)⊤(K(l)\nk;hY(l)\nk;j)\u0011\n,\nand\nX(l+1)\nk;i=KX\nk=1HX\nh=1iX\nj=1exp\u0010\n(Q(l)\nk;hY(l)\nk;i)⊤(K(l)\nk;hY(l)\nk;j)\u0011\neZ(l)\n(k−1)H+h·V(l)\nk;hY(l)\nk;j\n=Y(l+1)\nk;i.\nThis confirms Eq. (26).\nProve Eq. (27).When i > ξ m, we rely on the following properties:\n35\n--- Page 36 ---\n1. Attention sink to vξmfor mismatch expert: for any k′̸=κandj≤iwe have\n(eQ(l)\n(k′−1)H+heX(l)\ni)⊤(eK(l)\n(k′−1)H+heX(l)\nj)≤(eQ(l)\n(k′−1)H+heX(l)\ni)⊤(eK(l)\n(k′−1)H+heX(l)\nξm)−C.\n(29)\n2.Attention to task-relevant tokens for matching expert: for j∈ {1, . . . , ξ 1−1, ξm+ 1, . . . , n },\nandξ1≤j′≤ξmwe have\n(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj)≥(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj′) +C.\n(30)\nand for j1< j2∈ {1, . . . , ξ −1−1, ξm+ 1, . . . , n }\n(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj1)−(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj2)\n= (Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nζ(j1))−(Q(l)\nκ;hY(l)\ni−ξm−1+ξ1)⊤K(l)\nκ;hY(l)\nκ;ζ(j2)), (31)\nTo see Eq. (29), we notice that\n(eQ(l)\n(k′−1)H+heX(l)\ni)⊤(eK(l)\n(k′−1)H+heX(l)\nj)\n= (X(l)\nk′;i)⊤(Q(l)\nk′;h)⊤K(l)\nk′;hX(l)\nk′,j+α⊤\nmAk′(αι(j)+βE(vj)· 1(ι(j) =j))\n≤(X(l)\nk′;i)⊤(Q(l)\nk′;h)⊤K(l)\nk′;hX(l)\nk′;ξm+α⊤\nmAk′(αm+βE(vξm))−C\n= (eQ(l)\n(k′−1)H+heX(l)\ni)⊤(eK(l)\n(k′−1)H+heX(l)\nξm)−C,\nwhere we use Eq. (19) with k′̸=κ.\nTo see Eq. (30), we notice that\n(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj) = (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j) +α⊤\nmAκα0\n≥(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j′) +α⊤\nmAκ(αι(j′)+βE(vj′)) +C\n= (eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj′) +C,\nand\n(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj) = (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j) +α⊤\nmAκα0\n≥(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j′) +α⊤\nmAkαι(j′)+C\n= (eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj′) +C,\nwhere we use Eq. (20) and Eq. (22).\nWhen ξm< j1< j2, Eq. (31) follows directly from\n(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj1)−(eQ(l)\n(κ−1)H+heX(l)\ni)⊤(eK(l)\n(κ−1)H+heX(l)\nj2)\n= (Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j1) +α⊤\nmAkα⊤\nm\n−(Q(l)\nk;hX(l)\nk;i)⊤(K(l)\nk;hX(l)\nk;j2) +α⊤\nmAkα⊤\nm\n= (Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj1−ξm−1+ξ1)−(Q(l)\nκ;hY(l)\ni−ξm−1+ξ1)⊤K(l)\nκ;hY(l)\nκ;j2−ξm−1+ξ1).\nThe other cases follow similarly due to Eq. (22).\n36\n--- Page 37 ---\nWe have hence confirmed Eq. (29), Eq. (30), Eq. (31), and therefore\nexp\u0010\n(eQ(l)\n(k−1)H+heX(l)\ni)⊤(eK(l)\n(k−1)H+heX(l)\nj)\u0011\neZ(l)\n(k−1)H+h=\n\nδξm\nj, k ̸=κ\nexp\u0010\n(Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj)\u0011\neZ(l)\n(k−1)H+h, k =κ, j < ξ 1\n0, k =κ, ξ1≤j≤ξm\nexp\u0010\n(Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj−ξm−1+ξ1)\u0011\neZ(l)\n(k−1)H+h, k =κ, j > ξ m\nand\neZ(l)\n(k−1)H+h=X\nj=1,...,ξ 1−1,ξm+1,...,nexp\u0010\n(Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj)\u0011\n.\nIt follows that\nX(l+1)\nκ;i=ξ1−1X\nj=1exp\u0010\n(Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj)\u0011\neZ(l)\n(k−1)H+hV(l)\nk;hY(l)\nj\n+iX\nj=ξm+1exp\u0010\n(Q(l)\nκ;hY(l)\nκ;i−ξm−1+ξ1)⊤(K(l)\nκ;hY(l)\nj−ξm−1+ξ1)\u0011\neZ(l)\n(k−1)H+hV(l)\nk;hY(l)\nj−ξm−1+ξ1,\n=Y(l+1)\nκ;i−ξm−1+ξ1\nX(l+1)\nk′;i=X(l)\nk′;ξm= 0,∀k′̸=κ.\nTherefore we establish Eq. (27). This completes the induction.\nAt the output layer, we have\npef(y|v1, . . . , v n) = Softmax( eϑ(y)⊤eX(L)\nn)\n= Softmax( ϑ(y)⊤Y(L)\nn−ξm−1+ξ1)\n=pfκ(y|u1, . . . , u n−ξm−1+ξ1).\nThis establishes the desired Eq. (2).\nA.5 Proof of Theorem 4.7\nProof. Letϕs, ϕm, ϕedenote the general-purpose Transformers in Proposition 4.4 (with Kexperts),\n4.2 (with K= 3 token spaces), and A.1 (extending to V) respectively. We construct a dummy\nTransformer fdthat outputs BOS immediately after a token in A. Then we claim that the general-\npurpose Transformer eϕdefined by\neϕ(f0, f1, . . . , f K) =ϕm(ϕs(ϕe(f1), . . . , ϕ e(fK)), fd, f0)\nachieves the desired property.\nIndeed, let g1=ϕs(ϕe(f1), . . . , ϕ e(fK)), by Proposition 4.4, we have\n1.Expert following : Att-th iteration,\npg1\u0010\n·\f\f\fprompt\u0011\n∼pfa(t)\u0010\n·\f\f\fq|u(t)\n1:i−1\u0011\n,\nwhere q|u(t)\n1:i−1is the token sequence obtained by concatenating the user query qand prior\ngenerated part in response t:u(t)\n1:i−1.\n37\n--- Page 38 ---\n2.Regret minimization :\nmax\na∗∈Ar0(a∗)−E[r0(a(T))]≤reg(T).\nTherefore by Proposition 4.2, we have\nu(t)\ni∼pfa(t)\u0010\n·\f\f\fq|u(t)\n1:i−1\u0011\n.\nIt follows that\nmax\nu∗∈Vωr(q, u∗)−E[r(q, u(T))]≤λ+Eu∼fk∗(·|p)[r(q, u)]−Ea(T)h\nEu(T)∼fa(t)(·|q)[r(q, u(T))]i\n≤λ+ max\na∗∈Ar0(a∗)−E[r0(a(T))]\n≤λ+ reg( T).\nFinally, eϕhas type ϕof type (O(K), O(log(Nmax)))because ϕshas type (O(K), O(log(Nmax)))\nandϕm, ϕehas type (O(1), O(log(Nmax))). This completes the proof.\nA.6 Attention Sink Positional Encoding\nIn this section, we introduce positional encoding mechanisms that induce attention sink behaviors\nused by Theorem 4.7.\nLemma A.2 (Attention Sink Positional Encoding, Type 1) .For any C∈R+,K, N∈Z+, there exist\nvectors α1, . . . , α N, β1, . . . , β K∈Rdand matrices A, A 1, . . . , A K∈Rd×dford≤O(K+ log N)\nsuch that for any n∈[N]the followings hold\n1. For any k̸=k′:\nα⊤\nnAk(αn+βk′)≥C+\n\nα⊤\nnAkαn\nα⊤\nnAkαj\nα⊤\nnAk(αj+βk′′),∀0≤j≤n,1≤k′′≤K.\n2. For any k∈[K]:\nα⊤\nnAkαn=α⊤\nnAkα0≥C+\n\nα⊤\nnAk(αn+βk)\nα⊤\nnAkαj\nα⊤\nnAk(αj+βk′),∀0< j < n, k′̸=k.\n3. For any k, k′, k′′∈[K]:\n(αn+βk′)⊤Ak(αn+βk′)≥C+ (αn+βk′)⊤Akαj,∀0≤j≤n.\n4. For any 0< j < n :\nα⊤\nnAαn≥α⊤\nnA(αn+βk) +C\n≥C+ max {α⊤\nnAαj, α⊤\nnA(αj+βk′)},∀k, k′′∈[K].\n38\n--- Page 39 ---\nProof. Notice that the following relations are sufficient to guarantee the desired properties\nα⊤\nnAkαn=α⊤\nnAkα0,\nα⊤\nnAkβk′=C,\nα⊤\nnAkαn≥α⊤\nnAkαj+α⊤\nnAkβk′+C,\nα⊤\nnAkβk=−C,\nα⊤\nnAβk=−C,\nβ⊤\nk′Akβk′= 9C.\nBy Lemma A.4, we can find γ1, . . . , γ N∈R¯dsuch that ¯d=O(logN),γ⊤\niγj≤1/2for any\ni̸=j∈[N], and γ⊤\niγi≥1for any i∈[N]. Define\nBk=eke⊤\nk, ηk=−ek.\nwhere e1, . . . , e Kform the standard basis of RK.\nWe thus let\nαi=\naγi\nb1E\nc1\nc1\n0\n, βk=\n0\nfηk\ne\n−e\nh\n, α0=\n0\n0\ng1\n−g1\n0\n\nAk=\nI\nBk\n1\n−1\n1\n, A=\nI\nI/K\n0\n0\n0\n,\nwhere b=c=f=√\nC, e=√\nC/2, a=√\n3C, g= 2√\nC, h = 3√\nC. The dimension can be\nbounded by d=¯d+K+ 3 = O(K+ log N).\nLemma A.3 (Attention Sink Positional Encoding, Type 2) .For any C∈R+,K, N∈Z+, there exist\nvectors α1, . . . , α N, β0, . . . , β K∈Rdand matrices A, A 1, . . . , A K∈Rd×dford≤O(K+ log N)\nsuch that for any n∈[N]the followings hold\n1. For any i≥j1, j2, j3andk, k′, k′′̸= 0:\n(αi+βk)⊤A0(αj1+βk′) = (αi+βk)⊤A0(αj2+βk′′)≥(αi+βk)⊤A0(αj1+β0) +C\n(αi+β0)⊤A0(αi+β0)≥(αi+β0)⊤A0(αj1+βk) +C.\n2. For any i > j andk̸=k′̸= 0\n(αi+βk)⊤A(αi+βk)≥(αi+βk)⊤A(αj+βk′) +C\n≥(αi+βk)⊤A(αj+β0) + 2C.\n3. For any i≥j, j1andk̸=k′, k′′\n(αi+βk)⊤Ak′(αj+β0)≥(αi+βk)⊤Ak′(αj1+βk′′) +C\n(αi+βk)⊤Ak(αi+βk)≥max{(αi+βk)⊤Ak(αj1+βk′′),(αi+βk)⊤Ak′(αj1+β0)}+C.\n39\n--- Page 40 ---\nProof. Following the notations in Lemma A.2, let\nαi=\nγi\n0\n0\n0\n, βk=\n0\nγ\nek\n1\n, β0=\n0\nγ\n1\nf\n,\nand\nA=\n0\na·I\n0\n0\n, Ak=\nb·I\n0\nc·eke⊤\nk\n1\n, A=\ne·I\n0\n0\n0\n,\nwhere a=c=e=C, f= 3.5C, d= 4C. The dimension can be bounded by d=¯d+K+ 3 =\nO(K+ log N).\nA.7 Technical Claims\nClaim A.4 (Johnson-Lindenstrauss Lemma) .Given 0< ε < 1, a set XofNpoints in Rn, and an\ninteger k >8(lnN)\nε2, there is a linear map f:Rn→Rksuch that\n(1−ε)∥u−v∥2≤ ∥f(u)−f(v)∥2≤(1 +ε)∥u−v∥2\nholds for all u, v∈X.\nClaim A.5 (Concentration of Multinomial Distributions, adapted from [ 2]).Letp∈∆Sand\nˆp∼1\nnMultinomial (n, p). Then, for any δ∈[0,1]:\nP \n∥ˆp−p∥1≥r\n2 ln(1 /δ)\nn!\n≤δ.\nClaim A.6 (Berry-Esseen theorem) .IfX1, X2, . . . are i.i.d. random variables with E(X1) = 0 ,\nE(X2\n1) =σ2>0, andE(|X1|3) =ρ <∞, we define\nYn=X1+X2+···+Xn\nn\nas the sample mean, with Fnthe cumulative distribution function ofYn√n\nσandΦthe cumulative\ndistribution function of the standard normal distribution, then for all xandn,\n|Fn(x)−Φ(x)| ≤8ρ\nσ3√n.\n40\n--- Page 41 ---\nB Detailed Experiment Results\nIn Table 2, we report detailed test accuracy comparisons among different models with/without\nself-correction at test time. We note that:\n• Self-correction significantly boosts models’ test performances.\n•Larger models benefit more from self-correction, indicating that model expressiveness plays an\nimportant role in implementing self-correction.\nThose empirical findings corroborate our theoretical results.\nModel Accuracy with self-correction (%) Accuracy without self-correction (%)\nGPT-nano 1.23±1.07 2 .56±0.43\nGPT-micro 63.19±0.16 93 .09±9.70\nGPT-mini 63.19±0.16 98 .57±1.85\nGopher-44M 63.19±0.16 99 .15±0.23\nTable 2: Detailed test accuracy comparisons.\n41",
  "text_length": 83642
}