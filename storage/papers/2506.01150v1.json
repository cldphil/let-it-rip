{
  "id": "http://arxiv.org/abs/2506.01150v1",
  "title": "Flexible Selective Inference with Flow-based Transport Maps",
  "summary": "Data-carving methods perform selective inference by conditioning the\ndistribution of data on the observed selection event. However, existing\ndata-carving approaches typically require an analytically tractable\ncharacterization of the selection event. This paper introduces a new method\nthat leverages tools from flow-based generative modeling to approximate a\npotentially complex conditional distribution, even when the underlying\nselection event lacks an analytical description -- take, for example, the\ndata-adaptive tuning of model parameters. The key idea is to learn a transport\nmap that pushes forward a simple reference distribution to the conditional\ndistribution given selection. This map is efficiently learned via a normalizing\nflow, without imposing any further restrictions on the nature of the selection\nevent. Through extensive numerical experiments on both simulated and real data,\nwe demonstrate that this method enables flexible selective inference by\nproviding: (i) valid p-values and confidence sets for adaptively selected\nhypotheses and parameters, (ii) a closed-form expression for the conditional\ndensity function, enabling likelihood-based and quantile-based inference, and\n(iii) adjustments for intractable selection steps that can be easily integrated\nwith existing methods designed to account for the tractable steps in a\nselection procedure involving multiple steps.",
  "authors": [
    "Sifan Liu",
    "Snigdha Panigrahi"
  ],
  "published": "2025-06-01T20:05:20Z",
  "updated": "2025-06-01T20:05:20Z",
  "categories": [
    "stat.ME",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01150v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01150v1  [stat.ME]  1 Jun 2025Flexible Selective Inference with Flow-based\nTransport Maps\nSifan Liu1and Snigdha Panigrahi∗2\n1Center for Computational Mathematics, Flatiron Institute\n2Department of Statistics, University of Michigan\nJune 2025\nAbstract\nData-carving methods perform selective inference by conditioning the\ndistribution of data on the observed selection event. However, existing data-\ncarving approaches typically require an analytically tractable characterization\nof the selection event. This paper introduces a new method that leverages tools\nfrom flow-based generative modeling to approximate a potentially complex\nconditional distribution, even when the underlying selection event lacks an\nanalytical description—take, for example, the data-adaptive tuning of model\nparameters. The key idea is to learn a transport map that pushes forward a\nsimple reference distribution to the conditional distribution given selection. This\nmap is efficiently learned via a normalizing flow, without imposing any further\nrestrictions on the nature of the selection event. Through extensive numerical\nexperiments on both simulated and real data, we demonstrate that this method\nenables flexible selective inference by providing: (i) valid p-values and confidence\nsets for adaptively selected hypotheses and parameters, (ii) a closed-form\nexpression for the conditional density function, enabling likelihood-based and\nquantile-based inference, and (iii) adjustments for intractable selection steps\nthat can be easily integrated with existing methods designed to account for\nthe tractable steps in a selection procedure involving multiple steps.\n∗The author gratefully acknowledges support by NSF CAREER Award DMS-2337882.\n1\n--- Page 2 ---\n1 Introduction\nA primary goal in data science is to extract models and formulate hypotheses from\nobserved data. The practice of drawing inference from such selected models or\nhypotheses, without deferring the task to future data, is known as post-selection or\nselective inference. Whether it is basing inference on the most predictive model selected\nfrom a set of candidate models, testing hypotheses formulated after querying the\ndata, or seeking inference in models where data-adaptive choices, such as the degrees\nof freedom or the model complexity parameter used in fitting, are involved—these\nare all examples of selective inference straight out of the data scientist’s playbook.\nA practical strategy to addressing selective inference in such problems is to account\nfor the selection procedure by using a conditional distribution. This distribution is\nderived by conditioning on the selection event observed in the data, or on a suitable\nsubset of that event. A well-known example of this strategy is data-splitting, where\nthe data is divided into two parts, one used for selection and the other reserved for\ninference. In this approach, inference is derived by conditioning on the data used\nfor selection. This includes the sample splitting approach for independently and\nidentically distributed data (Cox, 1975), as well as more recent approaches for certain\nparametric distributions, such as data-fission (Rasines and Young, 2023; Dharamshi\net al., 2025; Leiner et al., 2025), which involve dividing individual observations into\ntwo parts. When the two parts obtained by splitting the data are independent, the\nconditional distribution used for selective inference coincides with the unconditional\ndistribution of the holdout data reserved exclusively for inference.\nConditioning on all the data used in selection, as is done in data-splitting, often\namounts to conditioning on more information than is actually necessary. Instead\nof adopting a data-splitting strategy, adjustments for the selection procedure can\nbe made by conditioning on less information—ideally, only the event of selection\nitself. This conditioning approach, which discards only the information involved in\nselection, was introduced by Fithian et al. (2014); Lee et al. (2016) for applications\nin variable selection. In this paper, we refer to the class of conditional methods,\nwhich, unlike data-splitting, reuse data from selection, as “data-carving”. When\nfeasible, a data-carving approach can offer significant advantages over data-splitting\nmethods, both in terms of selection accuracy and inferential power. This is because,\nin a data-splitting approach, selection tends to be less accurate when performed only\non a subset of the data, and inference based solely on the holdout portion is less\nefficient at the same time, leading to tests with low power and unnecessarily wide\nconfidence intervals.\nRecent work have developed data-carving methods in various problems for\n2\n--- Page 3 ---\nregression, classification, and dimension reduction. This includes, for example, the\nwork by Le Duy and Takeuchi (2022); Liu (2023); Panigrahi et al. (2023); Huang\net al. (2023); Gao et al. (2024); Perry et al. (2024); Panigrahi et al. (2024); Pirenne\nand Claeskens (2024); Bakshi et al. (2024). The adjustments based on a conditional\ndistribution typically depend on an analytically tractable description of the selection\nevent or, at the very least, utilize some knowledge of the geometry of this event.\nIn practice, however, the process of extracting models and hypotheses from data\ncan be far more complex than the types of selection events these adjustments can\nhandle. For example, if a data scientist adaptively selects tuning parameters to\ncontrol the complexity of model fit or balance the bias-variance tradeoff during fitting,\ndescribing the selection of these parameters is a notoriously difficult task. While\ncomputing the necessary adjustment for such selection events is beyond the reach of\ncurrent data-carving methods, we develop a new method in this paper that computes\ninference from the complex, otherwise intractable conditional distribution based on\nthe full data.\nThe key idea of our approach is to construct a transport map that transforms\nthe conditional distribution of the relevant statistic to its pre-selection distribution,\neffectively acting as a debiasing transformation that removes the effect of selection.\nTo obtain such a transport map, we adopt tools from flow-based generative modeling\nand learn this map in a data-driven manner. Importantly, our approach imposes no\nrestrictions on the selection event or the algorithm leading to this event, applying\neven when the event lacks a tractable description. Instead, we only require that the\nselection procedure can be repeatedly applied to synthetically generated data, whose\noutputs are used as training data to fit the flow-based generative model.\nOur main contributions are summarized below:\n(i)We show how transport maps can be used to construct valid hypothesis tests\nand confidence sets for adaptively chosen parameters. We provide guarantees\non the selective Type Ierror and selective coverage probability in terms of the\naccuracy of an approximate transport map.\n(ii)The transport map directly yields an approximation to the conditional density\nfunction given the observed selection event. This allows for a range of inference\nprocedures, such as those based on quantiles and conditional maximum likelihood\nestimation (MLE), as in Panigrahi and Taylor (2023).\n(iii)When the selection procedure involves multiple steps, our method can be easily\nintegrated with existing data-carving tools that adjust for some parts of the\nprocess with tractable descriptions. For example, when applying the lasso for\n3\n--- Page 4 ---\nvariable selection at a fixed regularization parameter, several types of data-\ncarving tools exist to adjust for this type of selection. If the regularization\nparameter is chosen adaptively (e.g., via cross-validation), our method can\naccount for this intractable tuning step and combine with these existing tools\nto ensure valid inference.\nThe remainder of the paper is organized as follows. In Section 2, we present the\nconceptual framework for the data-carving approach, illustrate it with a concrete\nexample, and review related work. In Section 3, we describe how transport maps can\nbe used to perform selective inference and provide selective inferential guarantees\nfor our approach. Section 4 presents our method for learning transport maps via\nnormalizing flows. Section 5 discusses several extensions, including handling nuisance\nparameters and integrating our approach with existing data-carving tools. In Section 6,\nwe illustrate the application of our method on several examples that are beyond the\nreach of existing data-carving approaches, including an application to single-cell data\nanalysis. Section 7 concludes the paper with a discussion of future directions.\n2 Data-carving: framework and review\nIn this section, we introduce the framework for data-carving methods, illustrate our\nproposed method with a first example, and provide a review of other related work.\n2.1 Framework\nSelection procedure Suppose a data scientist is provided with a dataset D∈ D\nand applies a selection procedure on this data to select a model. This procedure may\ninvolve additional randomness, independent of the data, such as from a data-splitting\nprocedure or the addition of independent, external noise. For example, this additional\nrandomness may arise from train-test splits used during a model fitting procedure\nwhen tuning parameters are chosen via cross-validation, or from externally added noise\nto the selection algorithm to facilitate computationally efficient and more powerful\nselective inference, as done in Tian and Taylor (2018); Panigrahi et al. (2024, 2023);\nHuang et al. (2023). We represent this external randomness by a randomization\nvariable W∈ W with distribution Q. If no additional randomness is involved in\nthe selection procedure, we take W= 0. We denote by DoandWothe observed\nrealizations of DandW, respectively.\nThe selection procedure generates a model Mas a function of the data and\nrandomization variable, which we denote as M=cM(D, W ). The notion of a “model”\n4\n--- Page 5 ---\nis context-dependent but can be broadly understood as a collection of plausible\ndata-generating distributions. For example, in regression settings, a model might\ncorrespond to a linear model involving a selected subset of predictors. In this case,\nthe selection procedure determines which predictors to include in our model. Our\nframework here is quite general, and applies to a wide range of selection procedures,\nincluding those that can be formulated as algorithms. These include, for example,\nthe choice of regularization parameters, degrees of freedom or other model complexity\nparameters, and the number of dimensions to reduce the data to—such as principal\ncomponents—in dimension-reduction techniques, all of which are illustrated with\nnumerical examples later in the paper.\nSelective inference: goal and guarantees Given a model, a fundamental task\nis to draw inference about parameters based on observed data. However, when the\nmodel itself is selected in a data-dependent manner, classical inference procedures are\nno longer valid due to selection bias. To correct for this bias, data-carving methods\nbase inference for the selected model Mo=cM(Do, Wo) on distributions conditioned\non the selection event.\nHereafter, we denote the target post-selection parameter in the selected model by\nθMo=θ(Mo), where the superscript Moemphasizes its dependence on the selected\nmodel. Common inferential tasks include testing the null hypothesis H0:θMo=θ0or\nconstructing a confidence set for θMo. For now, we assume that Mois a parametric\nmodel fully specified by the parameter θMo. We address the issue with nuisance\nparameters in Section 5. Had the model Mobeen fixed in advance, inference could\nproceed using a statistic T∼PθMo, where PθMois the “pre-selection” distribution of\nT. However, such a na¨ ıve method fails to account for the data-dependent nature of\nthe model and typically results in overly optimistic, invalid inference.\nTo adjust for selection, we seek to provide valid inference conditioned on the\nselected model, as outlined in Fithian et al. (2014). For hypothesis testing, a test\nϕ(T) is said to control the selective Type Ierror at level αif\nEθ0h\nϕ(T)|cM=Moi\n≤α, (1)\nwhere the expectation is taken under the joint distribution of D∼Pθ0andW∼Q,\nconditional on the selection event cM(D, W ) =Mo. Similarly, a confidence set C(T)\nforθMoachieves selective coverage probability 1−αif\nPθMoh\nθMo∈C(T)|cM=Moi\n≥1−α. (2)\nBy the law of iterated expectations, the selective inferential guarantees in (1)and(2)\nimply unconditional validity, holding on average over the possible outcomes of cM.\n5\n--- Page 6 ---\nConditional distribution under data-carving We now turn to the conditional\ndistribution of Tunder data-carving, which is derived by conditioning on the selection\nevent. Let the density of Tunder the pre-selection distribution PθMobepθMo. We\ndenote the conditional distribution of T| {cM(D, W ) =Mo}under PθMoasP∗\nθMo,\nwith density given by\np∗\nθMo(t)∝pθMo(t)×PθMoh\ncM=Mo|T=ti\n. (3)\nHere,PθMoh\ncM=Mo|T=ti\nrepresents the probability of selecting model Mogiven\nthe statistic T=t, marginalizing over any remaining randomness in the data Dand\nW. A formal derivation is provided in Appendix A. The selective Type Ierror and\ncoverage probability guarantee in (1) and (2) can be equivalently expressed as\nEP∗\nθ0[ϕ(T)]≤αandP∗\nθMo\u0002\nθMo∈C(T)\u0003\n≥1−α,\nrespectively, using these notations.\nWhether or not randomization is involved in the selection procedure, obtaining the\nselective distribution P∗\nθMoin(3)in closed form can be infeasible unless the selection\nevent{(D, W )∈ D × W :cM(D, W ) =Mo}admits a tractable description. We now\npresent a motivating data example in which the selection event is difficult to describe\nanalytically, and preview how our proposed method enables valid selective inference\nwithout requiring an explicit description of the selection event.\n2.2 A first example and related work\nIn the example below, we consider the problem of fitting a regression spline, where\nthe number of knots is determined in a data-adaptive manner. Regression splines are\ncommonly used to model nonlinear relationships between a response variable yand a\npredictor variable x. The number of knots acts as a tuning parameter that controls\nthe complexity of the model fit and is typically chosen using data-adaptive tools such\nas cross-validation (CV).\nIn particular, we perform regression using a natural cubic spline, with the number\nof knots selected through a 10-fold CV procedure. In our specific instance, 5 knots\nare chosen, yielding 6 spline basis functions {b1, . . . , b 6}. Additionally, we include the\nconstant function b0(x) = 1 to serve as an intercept. The selected nonlinear model\nMois given by\nyi=6X\nk=0βkbk(xi) +εi, i∈ {1,2, . . . , n }, (4)\n6\n--- Page 7 ---\n0.00.20.41/1\n 2/2\n 3/3\n 4/4\n 5/5\n 6/6\n3\n030.00.20.41()1\n3\n031()2\n3\n031()3\n3\n031()4\n3\n031()5\n3\n031()6\nFigure 1: Top panel: histograms of ˆβj/σjconditioned on selecting 5 knots under the\nglobal null. Bottom panel: histograms of τ−1(ˆβ)j, where τ−1is the learned transport\nmap that pulls back the conditional distribution of ˆβto the standard Gaussian\ndistribution. The red dashed curves represent the density function of N(0,1). Due to\nthe selection of knots via CV, prior to inference, the distribution of ˆβj/σjis distorted\nand is no longer N(0,1). The learned transport map τpulls back the post-selection\ndistribution of ˆβto the standard Gaussian, yielding a new test statistic τ−1(ˆβ)jfor\nvalid selective inference.\nwhere the errors εiiid∼ N(0, σ2). As our selective inferential task, we consider testing\nthe global null hypothesis that the predictor is not associated with the response, i.e.,\nβk= 0 for all k= 1, . . . , 6.\nIf the selection of the number of knots is ignored in this problem, inference for\nthe coefficients βkcan be carried out using the least squares estimator T=ˆβ1:6\nofβ1:6. Under the na¨ ıve approach, the statistic Tfollows a multivariate normal\ndistribution N(0,Σ) under the global null, where Σ = σ2((X⊤X)−1)1:6,1:6andX=\n(b0(x), b1(X), . . . , b K(X))∈Rn×7denotes the design matrix formed with the selected\nbasis functions.\nHowever, when we condition on the event that the selected number of knots is\n5, the null distribution of T=ˆβ1:6is no longer N(0,Σ). To illustrate the effect\nof knot selection on the distribution of ˆβ, we generate 2000 datasets under the\nglobal null model, in which the output of the CV procedure is 5 knots. In each\nreplicate, we compute the least squares estimator ˆβ. The top panel of Figure 1\ndisplays histograms of each coordinate of ˆβ, standardized by its standard deviationp\nΣk,k. These empirical distributions show clear deviations from the standard normal,\nrepresented by the red dashed curves.\n7\n--- Page 8 ---\nUsing our method, we learn a pushforward transport map ˆτfrom the pre-selection\ndistribution Pθ0to the conditional distribution P∗\nθ0, which we formally define in Section\n3. As a consequence, we have that if T∼P∗\nθ0, then τ∗−1(T)∼ N(0,Σ) (see Lemma\n1). The bottom panel of Figure 1 displays histograms of each coordinate of τ∗−1(T),\nstandardized by their corresponding standard deviations. These distributions closely\nmatch the standard normal distribution. This result demonstrates that, despite the\nanalytical intractability of the selection event, the learned transport map in our\napproach successfully corrects for selection bias and ensures valid selective inference.\nOur method can be seen related to approaches introduced for likelihood-free or\nsimulation-based inference, which enable inference when the likelihood function is\nintractable, but generating data from the underlying model is feasible. Work in\nthis area include approximate Bayesian computation (Marin et al., 2012), synthetic\nlikelihood approaches (Price et al., 2018) and simulation-based approaches (Xie\nand Wang, 2022; Awan and Wang, 2024). Recent machine learning techniques\nthat have made likelihood-free inference feasible include neural posterior estimation\n(Papamakarios and Murray, 2016), neural likelihood estimation (Papamakarios et al.,\n2019), and ratio estimation methods (Cranmer et al., 2015; Thomas et al., 2022).\nSimilar to the work in Papamakarios et al. (2019), where an autoregressive flow is\ntrained to approximate the likelihood of data given parameter values, our approach\napplies flow-based techniques to estimate a conditional likelihood function.\nIn the selective inference literature, Liu et al. (2022) learn the probability of a\nselection event given the data, which in turn facilitates a pivot for scalar-valued\nparameters. In contrast, our method takes a different approach by using transport\nmaps to learn the conditional distribution, and offers greater versatility in the range of\ninferential tasks it can perform. This includes, for example, inference for vector-valued\nparameters, joint likelihood-based inference, as well as the ability to combine with\nexisting corrections for the more tractable parts of the selection procedure, which\nwe discuss in Sections 3 and 5. Furthermore, as shown in Guglielmini et al. (2025),\nthe MLE from the conditional likelihood function—readily obtained from our current\nmethod—can be used to make inference on potentially complex functionals of the\npost-selection parameter when coupled with tools like the delta method and the\nbootstrap. In this sense, our approach may be extended to facilitate inference beyond\njust linear functionals. A different type of guarantee is offered by simultaneous\nmethods for selective inference, such as those in Berk et al. (2013); Zrnic and Fithian\n(2024), which aim at validity over a set of plausible targets instead of the one observed\nin our data. In contrast, the conditional guarantees in our work not only ensure valid\ninference for the selected model, but also enable data-adaptive inference, i.e., in cases\nwhere selection bias is either minimal or absent, inference from our approach matches\n8\n--- Page 9 ---\nwith the na¨ ıve approach.\n3 Data-carving using transport maps\nIn this section, we present the idea of using transport maps for performing selective\ninference. We describe how to construct tests and confidence sets for an Rd-valued\nparameter θMo, when a transport map, or a sufficiently accurate approximation to\nit, is available. As discussed earlier in Section 2, we consider a statistic Twhose\npre-selection distribution is denoted by PθMo.\nDefinition 3.1. Letτ∗\nθMo:Rd→Rdbe a diffeomorphism, i.e., a continuously\ndifferentiable and invertible map whose inverse τ∗−1\nθMois also differentiable. The map\nτ∗\nθMois said to push forward the pre-selection distribution PθMoofTto its conditional\n(post-selection) distribution P∗\nθMoif\np∗\nθMo(t) =pθMo(τ∗−1\nθMo(t))· |∇τ∗−1\nθMo(t)|,\nwhere pθMoandp∗\nθModenote the densities of PθMoandP∗\nθMo, respectively, and |∇τ∗−1\nθMo(t)|\ndenotes the determinant of the Jacobian of the inverse map. We denote this\npushforward relationship by\nτ∗\nθMo#PθMo=P∗\nθMo, (5)\nand call τ∗\nθMoa transport map from PθMotoP∗\nθMo.\nFrom Definition 3.1, the inverse map τ∗−1\nθMocan be interpreted as a pullback from\nthe conditional distribution to the pre-selection distribution. This is formalized in\nthe following lemma.\nLemma 1. Suppose τ∗\nθMo:Rd→Rdsatisfies the pushforward relation (5). Then, if\nT∼P∗\nθMo, it follows that τ∗−1\nθMo(T)∼PθMo.\nProof. This result follows from the definition of τ∗\nθMo.\nPut another way, the inverse map τ∗−1\nθMoacts as a debiasing transformation that\ncorrects for the effect of selection. Under this transformation, the pulled-back statistic\nτ∗−1\nθMo(T) follows the pre-selection distribution. Consequently, any inference procedure\nthat is valid under the pre-selection distribution can be directly applied to the\npulled-back statistic. We elaborate on this idea in the remainder of this section.\n9\n--- Page 10 ---\n3.1 Hypothesis tests\nGiven a valid test under the pre-selection distribution, a conditionally valid test—one\nthat controls the selective Type Ierror as defined in (1)—can be obtained immediately\nby using the pullback of a transport map from PθMotoP∗\nθMo.\nTheorem 3.1. Consider testing the null hypothesis H0:θMo=θ0. Let ϕ(T)be a\nlevel- αtest under Pθ0, i.e.,EPθ0[ϕ(T)]≤α. Suppose τ∗\nθ0is a transport map satisfying\nthe pushforward condition (5)when θMo=θ0. Then, ϕ◦τ∗−1\nθ0(T)controls the selective\nType Ierror at level α, i.e.,\nEP∗\nθ0\u0002\nϕ◦τ∗−1\nθ0(T)\u0003\n≤α.\nProof. Lemma 1 implies that eT=τ∗−1\nθ0(T)∼Pθ0under the null hypothesis. Therefore,\nEP∗\nθ0\u0002\nϕ◦τ∗−1\nθ0(T)\u0003\n=EPθ0h\nϕ(eT)i\n≤α.\nIn practice, the ideal transport map τ∗\nθ0is typically not available. As described\nin Section 4, our proposed method estimates an approximate map ˆτθ0to serve\nas a surrogate for the ideal map τ∗\nθ0. While ˆτθ0may not satisfy the exact\npushforward condition in Equation (5), the induced distribution ˆτθ0#Pθ0can still\nclosely approximate the target distribution P∗\nθ0, allowing for approximately valid\nselective inference.\nTo quantify the discrepancy between the approximate and target distributions,\nwe consider the Kullback-Leibler (KL) divergence. For two probability measures P\nandQdefined on a common measurable space, the KL divergence is given by\nKL(P∥Q) =EP\u0014\nlogdP\ndQ\u0015\n.\nThe following theorem provides a quantitative bound on the selective Type Ierror in\nterms of the KL divergence between P∗\nθ0and ˆτθ0#Pθ0.\nTheorem 3.2 (Type Ierror bound) .Consider testing the null hypothesis H0:\nθMo=θ0. Let ϕ(T)be a valid level- αtest under Pθ0, and let ˆτθ0be an invertible and\ndifferentiable map such that KL(P∗\nθ0∥ˆτθ0#Pθ0)≤ε. Then the selective Type Ierror\nof the approximate test ϕ◦ˆτ−1\nθ0(T)is bounded by\nEP∗\nθ0\u0002\nϕ◦ˆτ−1\nθ0(T)\u0003\n≤α+p\nε/2.\n10\n--- Page 11 ---\nProof of Theorem 3.2. Since ϕ(T)∈[0,1], the difference in expectations under two\ndistributions can be bounded by their total variation distance:\n\f\fEˆτ−1\nθ0#P∗\nθ0[ϕ(T)]−EPθ0[ϕ(T)]\f\f≤TV(ˆτ−1\nθ0#P∗\nθ0,Pθ0),\nwhere TVdenotes the total variation distance between two probability distributions.\nBy Pinsker’s inequality,\nTV(ˆτ−1\nθ0#P∗\nθ0,Pθ0)≤r\n1\n2KL(ˆτ−1\nθ0#P∗\nθ0∥Pθ0) =r\n1\n2KL(P∗\nθ0∥ˆτθ0#Pθ0)≤p\nε/2,\nwhere the equality follows from the fact that ˆτθ0is invertible and differentiable, and\nthat KL divergence is invariant under such transformations. Combined with the\nprevious inequality, we obtain\nEP∗\nθ0\u0002\nϕ◦ˆτ−1\nθ0(T)\u0003\n=Eˆτ−1\nθ0#P∗\nθ0[ϕ(T)]≤EPθ0[ϕ(T)] +p\nε/2≤α+p\nε/2,\nwhere the last inequality holds since ϕis a level- αtest under Pθ0.\nTheorem 3.2 establishes that if the KL divergence between the target measure\nP∗\nθ0andˆτθ0#Pθ0is at most ε, then the selective Type Ierror is inflated by no more\nthanp\nε/2. This result provides both a theoretical guarantee for selective inference\nas well as a principled, data-driven criterion for learning the transport map, based\non minimizing the KL divergence KL(P∗\nθ0∥ˆτθ0#Pθ0). We describe this estimation\nprocedure in Section 4.\n3.2 Confidence sets\nConfidence sets for θMowith the selective coverage guarantee, as defined in (2), can\nbe obtained by inverting the level- αtests established in Theorem 3.1. This leads to\nthe following result.\nProposition 3.2. For each θ0∈Θ, assume that the transport map τ∗\nθ0satisfies the\npushforward condition (5), and let ϕθ0∈ {0,1}denote a level- αtest for H0:θMo=θ0\nunder the distribution Pθ0. Define the confidence set as\nC∗(T) =\b\nθ0∈Θ :ϕθ0◦τ∗−1\nθ0(T) = 0\t\n. (6)\nThenC∗(T)achieves selective coverage at level 1−α, i.e.,\nP∗\nθMo\u0002\nθMo∈ C∗(T)\u0003\n≥1−α.\n11\n--- Page 12 ---\nProof. The proof follows by noting that\nP∗\nθMo\u0002\nθMo∈ C∗(T)\u0003\n= 1−P∗\nθMo\u0002\nϕθMo◦τ∗−1\nθMo(T) = 1\u0003\n= 1−PθMo[ϕθMo(T) = 1] ≥1−α.\nAnalogous to Theorem 3.2, we establish a lower bound on the selective coverage\nprobability of confidence sets constructed using approximate maps ˆτθ0. Specifically, if\neach ˆτθ0induces a distribution such that the KL divergence from the exact selective\ndistribution P∗\nθ0is at most ϵfor all θ0∈Θ, then the resulting confidence sets achieve\nnear-nominal selective coverage.\nTheorem 3.3 (Coverage probability bound) .Suppose that for all θ0∈Θ, the\napproximate map ˆτθ0satisfies KL(P∗\nθ0∥ˆτθ0#Pθ0)≤ε. Then the selective coverage\nprobability of the confidence set bCsatisfies\nP∗\nθMoh\nθMo∈bC(T)i\n≥1−α−p\nε/2,\nwherebCis defined analogously to C∗in(6), with τ∗\nθ0replaced by ˆτθ0.\nProof. The proof follows directly from Theorem 3.2.\n3.3 Inference based on conditional density\nA transport map τ∗\nθMoin(5)also provides an expression for the selective density of\nthe statistic Tunder the conditional distribution P∗\nθMo, as stated in Definition 3.1.\nAccordingly, an approximate map ˆτθMo≈τ∗\nθMoyields an approximate selective density\nof the form\nq∗\nθMo(t) =pθMo(ˆτ−1\nθMo(t))· |∇ˆτ−1\nθMo(t)|. (7)\nThis approximate selective density can now serve as the basis for conducting selective\ninference. In what follows, we discuss two approaches that directly use q∗\nθMofor\nselective inference.\nSelective MLE The approximate selective density (7)gives rise to the following\nnegative log-likelihood function\nℓ∗(θ) :=−logq∗\nθ(T) =−logpθ(ˆτ−1\nθ(T))−log|ˆ∇τ−1\nθ(T)|. (8)\n12\n--- Page 13 ---\nMinimizing the negative log-likelihood ℓ∗(θ) yields a natural point estimate, the\nselective MLE, defined as\nbθMo\nmle= argmin\nθ∈Θℓ∗(θ). (9)\nIn the presence of external randomization, Panigrahi and Taylor (2023) obtained\nselective inference for θMobased on approximate normality of the selective MLE. This\napproach constructs Wald-type intervals centered at the selective MLE, with variance\nestimated using the observed Fisher information matrix, both of which are computed\nfrom the conditional likelihood. We can apply the same procedure here, using the\nselective likelihood density defined in Equation (7).\nAs an example, suppose we wish to construct an equi-tailed confidence interval\nfor the jthcomponent of θMo. We compute the selective MLE bθMo\nmleas in (9), and\nestimate the Fisher information matrix as\nbI=I(bθMo\nmle) =∇2\nθℓ∗(θ)\f\f\fbθMo\nmle.\nBased on the approximate normality of bθMo\nmle, a (1−α) confidence interval for θMo\njis\ngiven by\n[bθMo\nmle]j±q\n[bI−1]j,j·z1−α/2,\nwhere [ bθMo\nmle]jis the jthcomponent of the selective MLE, [ bI−1]j,jis the ( j, j)th\ncomponent of the inverse Fisher information matrix bI−1, and z1−α/2is the (1 −α/2)\nquantile of the standard normal distribution.\nQuantile-based inference IfθMois a scalar-valued parameter, one can perform\nselective inference based on the quantiles of the conditional density (7). Concretely,\nsuppose we want to test the null hypothesis H0:θMo=θ0against one-sided or\ntwo-sided alternatives. In this case, valid p-values controlling the selective Type I\nerror can be computed as\nZT\n−∞q∗\nθ0(t)dt,Z∞\nTq∗\nθ0(t)dt,2·min\u0012ZT\n−∞q∗\nθ0(t)dt,Z∞\nTq∗\nθ0(t)dt\u0013\n,\ncorresponding to left-tailed, right-tailed, and two-sided tests, respectively.\nAnalogous to Theorem 3.2, if ˆτθ0#Pθ0is close to P∗\nθ0in KL divergence, then the\np-values based on the corresponding densities are also close. Specifically, we have the\nbound:\n\f\f\f\fZT\n−∞p∗\nθ0(t)dt−ZT\n−∞q∗\nθ0(t)dt\f\f\f\f≤TV(P∗\nθ0,ˆτθ0#Pθ0)≤r\n1\n2KL(P∗\nθ0∥ˆτθ0#Pθ0).\n13\n--- Page 14 ---\nIfKL(P∗\nθ0∥ˆτθ0#Pθ0)≤ε, the approximate p-values computed using q∗\nθ0differ from the\nexact p-values by at mostp\nϵ/2.\n4Learning transport maps using normalizing flows\nWe now describe our approach for learning the transport map τ∗\nθMothat pushes\nforward the pre-selection distribution PθMoto the conditional distribution P∗\nθMo. We\nfirst present our method in the context of hypothesis testing, and subsequently extend\nit to constructing confidence sets and obtaining an approximation to the selective\ndensity.\n4.1 Minimizing the KL divergence\nConsider the problem of testing the null hypothesis H0:θMo=θ0. As discussed\nin Section 3, an approximate transport map can be obtained by minimizing the\nKL divergence between the target conditional distribution P∗\nθ0and the pushforward\ndistribution ˆτθ0#Pθ0. As shown in Theorem 3.2, minimizing the KL divergence\nbetween the two distributions enables us to control the selective Type Ierror. This\nmakes KL minimization not only a natural learning objective but also a theoretically\njustified approach for performing valid selective inference.\nUsing the formula for the density of ˆτθ0#Pθ0in(7), we note that the KL divergence\ncan be expressed as\nKL(P∗\nθ0∥ˆτθ0#Pθ0) =EP∗\nθ0\u0002\nlogp∗\nθ0(T)−logpθ0(ˆτ−1\nθ0(T))−log|∇ˆτ−1\nθ0(T)|\u0003\n.\nTo evaluate the expectation, we draw training samples T(b)for 1≤b≤Bfrom the\ntarget distribution P∗\nθ0. Ignoring the constant term which doesn’t depend on the\ntransport map, we arrive at the empirical version of the objective function\n1\nBBX\nb=1−logpθ0(ˆτ−1\nθ0(T(b)))−log|∇ˆτ−1\nθ0(T(b))|. (10)\nHowever, directly optimizing over arbitrary diffeomorphisms is generally intractable.\nIn practice, we adopt a variational approach to solve this problem by parameterizing\nthe transport map within a flexible class of functions, which we describe in the\nfollowing section.\n14\n--- Page 15 ---\n4.2 Parametrizing transport maps via normalizing flows\nWe propose to parametrize the transport maps using normalizing flows, a class of\ngenerative models that transform a simple reference distribution into a complex target\ndistribution through a sequence of invertible, differentiable maps. In our setting, the\nreference distribution is the pre-selection distribution Pθ0, and our target distribution\nis the conditional (post-selection) distribution P∗\nθ0.\nSince the objective function (10) involves only the inverse map, we directly\nparameterize the inverse transport map ˆτ−1\nθ0using a normalizing flow family {ηθ0(·;ψ) :\nψ∈Ψ}, where ψ∈Ψ denotes the (unknown) flow parameters to be optimized. This\nleads to the following optimization problem\nbψ= argmin\nψ∈Ψ1\nBBX\nb=1−logpθ0(ηθ0(T(b);ψ))−log|∇ηθ0(T(b);ψ)|. (11)\nThe solution to (11)yields the approximate inverse transport map bτ−1\nθ0(·) :=ηθ0(·;bψ).\nVarious flow architectures have been proposed in the literature on normalizing\nflows, offering different trade-offs between expressiveness and computational efficiency.\nIn all examples presented in this paper, we employ the real-valued non-volume\npreserving (RealNVP) flows in Dinh et al. (2017). This choice ensures that the\nJacobian of the transformation—appearing in the second term of the objective in\nEquation (11)—is a triangular matrix, allowing its determinant to be computed\nefficiently. Implementation details and parameterization of the RealNVP flows used\nin our experiments are provided in Appendix B.\nTo summarize, our approach to learning the transport map involves two main\nsteps. In Step 1, we generate training data {T(b)∼P∗\nθ0,1≤b≤B}from the\nconditional distribution under the null. To do so, we employ rejection sampling that\nis carried out as follows:\n(i) Generate D(b)∼Pθ0andW(b)∼Q.\n(ii) Apply the selection algorithm cMto (D(b), W(b)).\n(iii) If cM(D(b), W(b)) =Mo, accept T(b)=T(D(b)) as a sample.\nIn Step 2, we parametrize the inverse transport map using the RealNVP flow, as\ndescribed above, and solve the optimization problem in (11)over the flow parameter\nspace via gradient descent. By combining the flow-based (inverse) transport map ˆτ−1\nθ0\nwith Theorem 3.1, we obtain a valid testing procedure. Our approach is summarized\nin Figure 2.\n15\n--- Page 16 ---\nGenerating\ntraining samples\nDraw ( D(b), W(b))∼(Pθ0×\nQ); ifcM(D(b), W(b)) =Mo,\naccept T(b)=T(D(b))\nas a training sample.Learning\ntransport map\nOptimizing flow parameter\nψby solving problem (11).Selective inference\nTestH0:θMo=θ0by\nϕ◦ˆτ−1\nθ0(T), where ϕis a\nvalid level α-test under Pθ0.\nFigure 2: Pipeline of the proposed approach for hypothesis testing.\n4.3 Conditional normalizing flows for efficient inference\nAs discussed in Section 3, confidence sets for the parameter θMocan be constructed by\ninverting hypothesis tests. We have described how to learn a transport map τ∗\nθ0for a\nfixed parameter value θ0∈Θ by generating data from P∗\nθ0and training a normalizing\nflow. However, repeating this process for many different parameter values can be\ncomputationally expensive. To reduce this cost, we propose a conditional normalizing\nflow approach that learns a single family of transport maps {ˆτθ, θ∈Θ}, indexed by\nθ, enabling efficient construction of confidence sets and more generally, estimation of\nthe conditional density p∗\nθ(T) across a range of parameter values.\nRevisiting Theorem 3.3, one can construct confidence sets with near-nominal\ncoverage by minimizing the KL divergence between the pushforward measure induced\nbyˆτθand the target distribution P∗\nθ, for all θ∈Θ. Toward this goal, we minimize\nthe expected KL divergence:\nEθMo∼π[KL(P∗\nθMo∥ˆτθMo#PθMo)],\nwhere πis a distribution supported on Θ. In practice, this distribution is user-specified\nto generate plausible parameter values from the support set Θ.\nTo optimize this objective, we consider a conditional normalizing flow family\n{η(·,·;ψ), ψ∈Ψ}, where, for fixed ψ,η(T, θ;ψ) takes as input both the statistic T\nand the parameter value θ, with ψdenoting the shared parameters of the conditional\nflow. We train this model using paired samples {(T(b), θ(b)),1≤b≤B}, where\nθ(b)∼π, T(b)|θ(b)∼P∗\nθ(b). In our experiments, the distribution πis chosen to be a\nmultivariate Gaussian distribution, with details provided in Section 6. Given these\ntraining samples, we solve the optimization problem\nbψ= argmin\nψ∈Ψ1\nBBX\nb=1−logpθ(b)(η(T(b), θ(b);ψ))−log|∇η(T(b), θ(b);ψ)|, (12)\n16\n--- Page 17 ---\nyielding an estimate bψand the approximate inverse transport map bτ−1\nθMo(·) :=\nη(·, θMo;bψ), which can be used to construct confidence sets of θMoas described\nin Section 3.2. The procedure for constructing confidence interval is summarized in\nFigure 3.\nGenerating\ntraining samples\nDraw θ(b)∼πand\n(D(b), W(b))∼(Pθ(b)×Q);\nifcM(D(b), W(b)) =Mo,\naccept ( T(D(b)), θ(b))\nas a training sample.Learning\ntransport map\nOptimize the conditional\nflow parameter ψby\nsolving problem (12).Selective inference\nConstruct confidence\nset for θMoas{θ∈\nΘ :ϕθ◦ˆτ−1\nθ(T) = 0}\nwhere ϕθis a valid\nlevel α-test under Pθ.\nFigure 3: Pipeline of the proposed approach for constructing confidence intervals.\nAdditionally, the conditional normalizing flow provides a surrogate for the selective\nlikelihood function defined in (8). By substituting the learned inverse map bτ−1\nθinto the\nlikelihood function, we obtain an approximation to the selective likelihood, which then\nfacilitates maximum likelihood inference using the procedure described in Section 3.3,\nor quantile-based inference for scalar-valued parameters.\nIn our description of the method, the transport maps use the pre-selection\ndistribution as the reference distribution. However, as noted in the following remark,\nthe reference distribution need not be the pre-selection distribution.\nRemark 4.1 (Choice of reference distribution) .When training the normalizing flow,\nthe reference distribution does not have to be the pre-selection distribution PθMo. It\ncan, in fact, be any probability distribution for which the log-density can be computed\nin a tractable form. In the variational inference and normalizing flow literature, the\nstandard normal distribution N(0, Id)is a common choice for the reference measure.\nHowever, using PθMoinstead of N(0, Id)may offer practical advantages. Since PθMo\nmay be closer to the target distribution P∗\nθMowhen the effect of selection is less, a\nsimpler transformation may suffice to approximate the target in such examples, thereby\nmaking the transport map easier to learn and potentially improving training efficiency.\n5 Extensions\nIn this section, we describe extensions of our method to accommodate different\nscenarios. In particular, we discuss how to condition on extra information from the\n17\n--- Page 18 ---\nselection procedure to address issues such as nuisance parameters, and how to combine\nour method with existing methods for selective inference that provide corrections for\nparts of the selection procedure with analytically tractable selection events.\n5.1 Conditioning on extra information\nIn selective inference, it can be beneficial to condition not only on the selection event\n{cM=Mo}, but also on additional statistics. Let A=A(D) denote such a statistic.\nThen any inference procedure that is valid conditional on {cM=Mo, A=Ao}remains\nvalid when conditioning only on {cM=Mo}, by the law of iterated expectation.\nThe additional conditioning on Atypically serves the following purposes. First, it\ncan help eliminate nuisance parameters from the model, as done in the construction\nof uniformly most powerful unbiased (UMPU) tests for exponential families (Fithian\net al., 2014) or in forming the selective likelihood function within the M-estimation\nframework (Huang et al., 2023). The approximate conditional density q∗\nθ(7)obtained\nfrom our approach can be easily used to implement this strategy for handling nuisance\nparameters. Second, conditioning on additional information can improve the efficiency\nof the rejection sampling that we described in Section 4. Since the selection event\n{cM=Mo}holds for the observed data, additionally conditioning on part of the data,\nin this case A=Ao, can increase the chance of re-selecting Mo, thereby improving\nacceptance rates. While marginalizing over Amay be an option, conditioning on its\nobserved value often provides a more computationally efficient alternative, though\npossibly at the expense of some statistical power.\nIn our framework, conditioning on extra information requires only minor\nmodification to our learning approach in Section 4. When generating the training\nsamples, we draw datasets D(b)∼Pθ(· |A=Ao), and then accept a sample T(b)if\ncM(D(b), W(b)) =Mo. Here is an example illustrating how this works. In a fixed X-\nregression setting, consider the linear model y∼ N(XMoβMo, σ2In), where the design\nmatrix XMois determined by the selection procedure cM. In this case, additionally\nconditioning on the statistic A= (In−XMo(X⊺\nMoXMo)−1X⊺\nMo)y, which is independent\nof the least squares estimator, is likely to increase the probability of re-selecting the\nsame model Mo. Since yfollows a Gaussian distribution and Ais a linear function\nofy, simulating from y(b)|A=Aois straightforward. Similarly, when additional\nrandomization is involved, such as a train-test split used during model fitting, we can\ncondition on these randomization variables, in this case the random choice of split, at\nthe time of inference. Once the training samples are generated under this conditional\ndistribution, the subsequent steps—training the normalizing flow and conducting\ninference—proceed exactly as before.\n18\n--- Page 19 ---\n5.2 Integrating with existing selective inference methods\nIn practice, data analysis pipelines may involve multiple stages of selection. Our\nproposed method is particularly well-suited for handling selection steps with\nintractable selection events, and at the same time, it can be easily combined with\nexisting selective inference methods that apply to the tractable steps of the selection\nprocedure.\nAs a motivating example, consider the well-studied problem of variable selection\nvia the lasso. A long line of prior work, including Lee et al. (2016); Liu (2023);\nPanigrahi et al. (2024); Kivaranovic and Leeb (2021), have studied selective inference\nmethods after lasso selection with a fixed regularization parameter λ. However, in\nmost applications, λis selected in a data-dependent manner (e.g., via cross-validation),\nintroducing an additional layer of selection that is typically not accounted for.\nTo formally illustrate the above example, let bλ(D, W(1)) denote the selection\nprocedure for the regularization parameter, and let cMλ(D, W(2)) denote the lasso\nvariable selection procedure with regularization parameter λ. Both procedures may\ninvolve randomization, represented by the randomization variables W(1)andW(2),\nrespectively. Suppose that for the observed data, the selected regularization parameter\nisλo=bλ(Do, W(1)\no), and the subsequent lasso with regularization parameter λo\nselects the model Mo=cMλo(Do, W(2)\no). This results in the selected linear model\ny∼ N (XMoβMo, σ2In), where XMois the design matrix composed of covariates\nindexed by Mo. The test statistic is chosen to be the least squares estimator in the\nselected model T=bβMo= (X⊺\nMoXMo)−1X⊺\nMoy. In line with existing methods for\nthe lasso, we consider conditioning on the additional statistic A= (In− P XMo)y,\nwhere PXModenotes the projection onto the column span of XMo, as well as on the\nsign vector Sof the lasso solution. Conditioning on this extra information enables\nefficient computation of the lasso selection probability at a fixed value of the tuning\nparameter.\nTo perform valid inference for θMo, the post-selection parameter of interest after\nselecting Mo, we must now account for both steps of selection. This parameter is\nformally defined in (14) in the following section. The result below characterizes\nthe conditional density of T, given the selected regularization parameter, the active\nvariable set, and the associated additional statistics.\nProposition 5.1. The density of the conditional distribution of Tgiven {bλ=\nλo,cMλo=Mo, A=Ao, S=So}, evaluated at t, is given by\np∗\nθMo(t)∝pθMo(t|bλ=λ, A=Ao)| {z }\n(I)·Ph\ncMλo=Mo, S=So|T=t, A=Aoi\n| {z }\n(II),(13)\n19\n--- Page 20 ---\nwhere (I)is the conditional density of Tgiven{bλ=λ, A=Ao}, and (II)is the lasso\nselection probability conditioned on both TandAat regularization parameter λo.\nProof of Proposition 5.1. We can write the conditional density of Tas\np∗\nθMo(t) =pθMo(t|bλ=λo,cMλo=Mo, A=Ao, S=So)\n∝pθMo(t|bλ=λo, A=Ao)·Ph\ncMλo=Mo, S=So|T=t, A=Ao,bλ=λoi\n.\nBecause the lasso selection cMλ0is conditionally independent of bλwhen conditioned\nonTandA—lasso selection is completely characterized by Tand ( In− P XMo)y—we\nhave\nPh\ncMλo=Mo, S=So|T=t, A=Ao,ˆλ=λoi\n=Ph\ncMλo=Mo, S=So|T=t, A=Aoi\n.\nThe probability is over the possible randomness in the external randomness W(2),\nand does not depend on the model parameter θMo. This finishes the proof.\nThe second term ( II) in the conditional density (13)is the probability of the lasso\nselection (including the sign) with a fixed regularization parameter. The lasso selection\nevent{cMλo=Mo, S=So}is known to be a polyhedron if no external randomization\nis applied. In this case, the selection probability is an indicator function and can be\ncomputed exactly (Lee et al., 2016). When external randomization W(2)is introduced,\nthe selection probability can be expressed as the probability of a Gaussian distribution\nover a polyhedral region. Efficient Monte Carlo estimators for this probability have\nbeen developed in Liu (2023), or the exact probability can be computed with some\nmore additional conditioning in Panigrahi et al. (2024).\nOn the other hand, the first term ( I), which is the conditional density of T|\nbλ=λo, A=Ao, is harder to characterize, particularly when bλis selected via some\ncomplex tuning procedure, such as cross-validation. This is precisely where our\nproposed method proves especially useful, offering a practical tool to account for the\nadaptive selection of the regularization parameter. Specifically, we apply the proposed\nmethod to learn a transport map that pushes forward the pre-selection distribution\nPθMoto the conditional distribution of T|ˆλ=λo, A=Ao. The transport map\nprovides an approximation of the conditional density ( I), analogous to the expression\ngiven in Equation (7). By combining the approximate conditional density ( I) with\nthe computed lasso selection probability ( II), we obtain a valid selective inference\nprocedure that accounts for both steps of selection.\nWe present this example in Section 6.3 and demonstrate how our method, combined\nwith existing approaches, delivers valid selective inference on the full dataset. For\n20\n--- Page 21 ---\n0.00.10.20.30.4\nDensity conditional on  (I)\nUnadjusted density0.00.20.40.60.81.0\nLasso selection probability (II)\nObserved 1\n0.00.20.40.60.8\nConditional density (I) × (II) \nConditional density for fixed \nFigure 4: Left: pre-selection density (orange dashed) and conditional density ( I) of\nˆβ1|ˆλ=λo, A=Ao(blue solid), obtained via the learned transport map. Middle:\nlasso selection probability ( II), estimated using Liu (2023). Right: conditional density\nignoring λselection (orange dashed) vs. full conditional density accounting for both\nselection steps (blue solid). The red vertical line marks the observed value of ˆβ1.\nnow, we illustrate the conditional density ( I) and the lasso selection probability\n(II) in Figure 4, highlighting how our method successfully corrects for the complex\nprocess of tuning parameter selection. The left panel plots the conditional density\nof (I) (blue solid line), computed using the learned transport map, alongside the\npre-selection density of ˆβ1(orange dashed line), where ˆβ1denotes the least squares\nestimate for a variable included in the selected model. The middle panel shows the\nlasso selection probability ( II) as a function of ˆβ1, computed using the Monte Carlo\nmethod of Liu (2023). In the right panel, the blue solid line represents the product of\n(I) and ( II)—that is, the estimated conditional density accounting for both stages of\nselection—while the orange dashed line represents the pre-selection density multiplied\nby (II), corresponding to inference that ignores the data-adaptive nature of λ.\nThe red dotted vertical line in the plots indicates the observed value of ˆβ1. Under\nthe orange curve in the right panel, the observed value lies to the right of the 0.99\nquantile, leading to a false rejection of the null hypothesis β1= 0. In contrast, under\nthe blue curve—which incorporates the selection of λ—the observed value falls within\nthe 0.95 quantile, and the null is not rejected. This example illustrates that failing to\naccount for the selection of the regularization parameter leads to invalid inference for\nthe regression coefficients in the selected model.\n21\n--- Page 22 ---\n6 Applications\nIn this section, we illustrate the performance of our proposed method on several\nsimulation experiments and a single-cell data analysis. In all the examples below, we\nfit a generalized linear model (GLM) to the data, using a feature matrix ZMo∈Rn×d\nthat is constructed adaptively based on the observed data, treating the response\ny∈Rnas random and the predictor matrix Xas fixed. We consider four different\nselection procedures for constructing the feature matrix:\n1.In Section 6.1, the feature matrix is constructed using a polynomial basis\nexpansion, where the degree of the polynomial fit is selected based on an\nanalysis of variance (ANOVA) criterion.\n2.In Section 6.2, the feature matrix consists of natural cubic splines, where the\nnumber of knots in the nonlinear fit is selected via cross-validation (CV).\n3.In Section 6.3, the feature matrix is composed of a subset of predictors selected\nby the lasso, with the regularization parameter first chosen via CV.\n4.In Section 6.4, the feature matrix consists of the principal components of\nthe predictor matrix, following the standard approach in principal component\nregression (PCR), with the number of principal components chosen via CV.\nGiven the adaptively constructed feature matrix ZMo, the selected GLM has\ndensity given by\npMo(y|ZMo, βMo) = exp\u0010y⊺ZMoβMo−A(βMo)\nσ2\u0011\n·h(y;σ),\nwhere βMo∈Rdis the vector of regression coefficients. We assume that the dispersion\nparameter σ2is either known or can be consistently estimated from the data, allowing\nus to use this estimate as a plug-in value in our inference procedure. The examples\nin Sections 6.1, 6.2, and 6.3 involve a Gaussian linear model, and the example in\nSection 6.4 considers a logistic regression model.\nFollowing (Berk et al., 2013; Lee et al., 2016), the target post-selection parameter\nin our simulations is defined as the best linear regression coefficients in the selected\nmodel, and is given by\nθMo= argmin\nβ∈RdE\"nX\ni=1ℓ(β;yi, ZMo,i)#\n, (14)\n22\n--- Page 23 ---\nwhere ℓ(β;y, Z) =A(β)−yZ⊤βis the negative log-likelihood of the selected linear\nmodel. The expectation in (14)is taken with respect to the true data-generating\ndistribution. Note that the selected model Mois treated as a working model, and we\nmake no assumptions about the quality of the selection, allowing Moto be potentially\nmisspecified.\nIn the proposed method, when constructing confidence intervals for individual\nparameters, we train a conditional normalizing flow as described in Section 4.3. If\nthe goal is to test a specific null hypothesis (as in Section 6.2), we instead train an\nunconditional normalizing flow as outlined in Section 4.2. The normalizing flows are\nparametrized using RealNVP (Dinh et al., 2017) with 12 affine coupling layers. Each\ncoupling layer involves a shift and a scaling parameter, both of which are outputs of\nneural networks with 1 hidden layer containing 8 neurons. For conditional normalizing\nflows, these neural networks take the parameter value θas an additional input. For\neach simulation, we generate 2,000 training samples and 500 validation samples. The\nnormalizing flows are trained using full-batch Adam with learning rate 10−4for 10000\niterations. The validation loss—KL divergence computed on the validation set—is\ncomputed every 1000 iterations, and the flow parameter corresponding to the lowest\nvalidation loss is selected as the final parameter. More details about the normalizing\nflow and the training procedure are provided in Appendix B. Code for reproducing\nthe experiments is available at https://github.com/liusf15/transport_selinf .\n6.1 Selecting the degree of polynomial fit\nWhen fitting a polynomial regression model with yas the response and xas the\npredictor, the degree of the polynomial is typically unknown a priori. To determine\nthis unknown degree, we apply an ANOVA procedure, in which we start from a\npolynomial of degree 0, and sequentially increase the degree in a stepwise manner.\nSuppose Mpis the model with polynomial degree p. An F-test is used to compute\nthe p-value for comparing the model Mpwith the model Mp+1, treating the smaller\nmodel as the null hypothesis. If the p-value is larger than 0.05, we stop and choose\nMpas the final model; otherwise, we continue the procedure to compare Mp+1and\nMp+2. The procedure is stopped when the maximum degree is reached. For a detailed\ndescription of this procedure, we refer the readers to (James et al., 2013, Chapter 7).\nWhen a degree p >0 is selected, we call the selected model Moand construct\nthe corresponding n×(p+ 1) feature matrix ZMo, where the k-th column contains\nxkfor 0≤k≤p. We then fit a least squares regression using this feature matrix\nand construct confidence intervals for each regression coefficient in the model. In this\nsimulation, we set n= 100 and generate xiiid∼ N(0,1),yi∼ N(f∗(xi),1) (1≤i≤n),\n23\n--- Page 24 ---\nSR-1 SR-2 SR-30.20.40.60.81.0Coverage (2=0)\nSR-1 SR-2 SR-302468Length (2=0)\nSR-1 SR-2 SR-30.00.20.40.60.81.0Coverage (2=0.1)\nSR-1 SR-2 SR-368101214Length (2=0.1)\nNaive\nSplitting\nProposedFigure 5: Coverage proportions and average interval lengths for the coefficients in\npolynomial regression, where the polynomial degree is selected by an ANOVA criterion.\nThe first two plots show the coverage and interval lengths for non-randomized selection\nwith ν2= 0, and the last two plots are for ν2= 0.1. The dashed line indicates the\ntarget coverage probability 0.95.\nwhere f∗(xi) =c·(x3\ni+x4\ni) with the signal strength cvaried over {0,0.1,0.2}. We\nrefer to these as signal regimes SR-1, SR-2, SR-3, respectively. We set the maximum\ndegree of the polynomial fit to be 4.\nOur method enables valid inference in the selected model by explicitly accounting\nfor the selection of the polynomial degree pfrom the ANOVA procedure. We compare\nthe proposed method to the following approaches:\n(i)Na¨ ıve : which performs inference based on the least squares estimator from the\nselected model, but is clearly invalid as it ignores the data-adaptive choice of p.\n(ii)Splitting : which uses the UV method by Rasines and Young (2023). The UV\nmethod performs selection on a perturbed version of the response, in this case\ny+W, where W∼ N(0, ν2In) for a pre-specified value of ν2. It then conducts\ninference on y−σ2\nν2W, the holdout portion of the response, which is independent\nof the data used for selection. This approach is a variant of data splitting for\nfixed Xregression, and falls under the broader class of data fission or thinning\nschemes described in Leiner et al. (2025); Dharamshi et al. (2025).\nWe consider two settings for our simulations: ν2= 0 (no randomization) and\nν2= 0.1 (a small amount of randomization). In the latter case, the polynomial fit is\nexpected to be very close to that of the non-randomized procedure based on the full\ndata. While our approach applies to both randomized and non-randomized selection,\nthe splitting approach is applicable only to the latter case.\n24\n--- Page 25 ---\nFigure 5 shows the coverage probabilities and interval lengths based on 2000\nsimulation repetitions for each signal regime. A non-zero degree is selected in\napproximately 100 simulations for SR-1, and in about 400 simulations for SR-3. As\ncan be seen from the empirical coverage probabilities in this plot, na¨ ıve inference\nleads to severe under-coverage, whether or not randomization is used in the selection\nprocedure. In contrast, our method produces valid confidence intervals that achieve\nthe nominal coverage probability across all scenarios. In the case where ν2= 0.1, our\nmethod, following the principles of data carving, uses the full data for inference and,\nas a result, produces significantly shorter confidence intervals for the post-selection\nparameters. In particular, the splitting intervals are nearly 1 .5−2 times longer than\nthose produced by our data-carving method.\n6.2 Selecting the number of knots\nThis example extends the analysis from Section 2, where we fit regression splines to\nthe data. The number of knots Kin the fit is a hyperparameter, which is selected\nusing a CV procedure. As described in Section 2, we test the null hypothesis that\nthere is no association between yandx. However, since Kis selected adaptively\nbased on the data, the classical F-test for this task fail to control the Type Ierror.\nWe select the number of knots from {2,3,4,5}using a 10-fold CV procedure. We\ndenote by Mothe model corresponding to the selected K, which uses the feature\nmatrix ZMoconsisting of ( K+ 1) basis functions along with an intercept. We then\nperform inference using the nonlinear model fitted with this adaptively constructed\nfeature matrix. For our simulations, we fix n= 100, and generate the predictors\nxiiid∼Unif(0,1) for 1 ≤i≤n. The response variables are then generated as\nyi∼ N (f∗(xi),1) where f∗(x) is the natural cubic spline function with 3 knots\npositioned at the quartiles. The coefficients for the 4 basis functions are given\nbyc·(1,1,−1,1), with the signal strength cvarying over the set {0,0.1,0.2,0.3}.\nThis leads to four signal regimes, which we denote by SR-0, SR-1, SR-2, and SR-3,\nrespectively. When c= 0, f∗does not depend on xand the null hypothesis is true.\nWhen c >0, the alternative hypothesis is true.\nTo test the null hypothesis that yhas no significant association with xusing our\ndata-carving method, we follow the pipeline outlined in Figure 2. In this case, we\nchoose the standard normal distribution as the reference distribution. Specifically,\nwe obtain a transport map ˆτsuch that ˆτ#N(0, Id) approximates P∗\n0, the conditional\ndistribution of the least squares estimator Tunder the null. We then compute the\npulled-back statistic ˆτ−1(T), using the inverse transport map, and reject the null\nhypothesis if ∥ˆτ−1(T)∥2\n2> χ2\n(K+1),1−α, where χ2\n(K+1),1−αis the 1 −αquantile of the\n25\n--- Page 26 ---\nSR-00.00.20.40.60.8Type I error (2=0)\nSR-1 SR-1 SR-30.00.20.40.60.8Power (2=0)\nSR-00.00.20.40.60.8Type I error (2=0.1)\nSR-1 SR-1 SR-30.00.20.40.60.8Power (2=0.1)\nNaive\nSplitting\nProposedFigure 6: Results for testing the dependence between yandxby fitting a spline\nregression model, where the number of knots is chosen by CV. The first two plots\nshow the Type Ierror and power for the non-randomized selection with ν2= 0, and\nthe last two plots are for ν2= 0.1. The dashed line indicates the nominal Type I\nerror 0.05.\nchi-squared distribution with K+ 1 degrees of freedom.\nIn line with the previous example, we conduct our simulations under two\ntypes of selection procedures: one without additional randomization and one with\nrandomization, corresponding to ν2= 0 and ν2= 0.1, respectively. We compare our\nmethod to the invalid na¨ ıve approach, which performs a classical F-test to compare\nthe intercept-only model with the selected linear model, without accounting for the\nprior selection of K. When ν2= 0.1, we also compare our method to the data-splitting\napproach, which applies na¨ ıve inference to the holdout portion of the response, as\ndescribed in Section 6.1.\nFigure 6 presents the empirical selective Type Ierror and the power of the tests\nfrom 500 simulation runs. As expected, our method offers valid control on the Type\nIerror, and achieves substantially higher power than the splitting method.\n6.3 Lasso with cross-validated regularization parameter\nWe consider the example of the lasso with a cross-validated regularization parameter,\nfollowing the discussion in Section 5.2. Given a regularization parameter λ >0, the\nlasso solves the optimization problem\nˆβλ= argmin\nβ∈Rp1\n2∥y−Xβ∥2\n2+λ∥β∥1,\nwhere y∈RnandX∈Rn×pdenote the response vector and the predictor matrix,\nrespectively. The ℓ1-penalty induces sparse solutions, leading to a selected set of\n26\n--- Page 27 ---\nvariables:\ncMλ(y, X) ={j∈[p] :ˆβλ\nj̸= 0}.\nGiven that we observe Mo=cMλ(y, X), we perform inference in the linear model\ny∼ N(ZMoβMo, σ2In), where ZMo=XMois the submatrix of Xcorresponding to the\nvariables in the subset Mo.\nInference after the lasso at a fixed regularization parameter has been extensively\nstudied in the selective inference literature, with many approaches proposed to tackle\nthis problem. If one conditions additionally on the sign of ˆβλ, then Lee et al. (2016)\nshowed that the selection event can be characterized as a polyhedron, and in this\ncase, selective inference can be based on the distribution of a univariate truncated\nnormal variable. But this method often leads to excessively conservative confidence\nintervals, as shown by Kivaranovic and Leeb (2021).\nTo increase power and avoid infinitely long confidence intervals, the randomized\nlasso of the form\nˆβλ= argminβ∈Rp1\n2∥y−Xβ∥2\n2+λ∥β∥1−W⊺β, (15)\nwas used in Panigrahi and Taylor (2023); Liu (2023); Panigrahi et al. (2024), where\nW∼ N(0, ν2X⊺X) is a p-dimensional randomization variable, and ν2controls the\nrandomization level. Conditional on the sign of ˆβλ, the distribution of the least\nsquares estimator in the selected model is a soft-truncated normal distribution, after\nmarginalizing over all or part of the randomness in W. Here, we consider two\nselective inference methods after solving (15)at fixed λ: (i) the separation-of-variable\n(SOV) method by Liu (2023), which computes the quantiles of the soft-truncated\ndistribution with Monte Carlo sampling; (ii) the bivariate normal (Bivnormal) method\nby Panigrahi et al. (2024), which computes an exact pivot by conditioning on additional\ninformation.\nA closely related randomization scheme involves solving the lasso using a\nrandomized response y+W, where W∼ N(0, ν2In). This is introduced by Tian and\nTaylor (2018) and is equivalent to solving the randomized lasso in (15). Using the\nvariables selected with the noisy response vector, the UV method of Rasines and\nYoung (2023) can be used to obtain splitting-type inference based on y−σ2\nν2W.\nIn our simulations, we set n= 100 and p= 20. The predictor matrix X∈Rn×pis\ngenerated as Xiiid∼ N(0,ΣX) for 1 ≤i≤n, where Σ X,jk= 0.9|j−k|, and the response\nvector yis generated from N(0, In), an all-noise model. We perform a 10-fold CV to\nselect λover a pre-specified grid ranging from 0 .01p\nlogp/nto 5p\nlogp/n, equally\n27\n--- Page 28 ---\n0.60.70.80.91.0Coverage (2=0)\n0510152025Length (2=0)\n0.60.70.80.91.0Coverage (2=0.1)\n1819202122Length (2=0.1)\nNaive\nSplitting\nPolyhedral\nBivnormal\nSOV\nAdj-Polyhedral\nAdj-Bivnormal\nAdj-SOVFigure 7: Lasso with cross-validation. The first two panels show the coverage\nproportions and interval lengths for the scenario ν2= 0, while the last two panels\nare for ν2= 0.1. The horizontal dashed line indicates the target coverage probability\n0.95.\nspaced on the log scale. We consider both non-randomized ( ν2= 0) selection and\nselection with a small amount of randomization ( ν2= 0.1). When ν2= 0.1, both\ncross-validation and lasso selection are performed using the perturbed data y+W,\nwhere W∼ N(0, ν2In).\nObviously, the na¨ ıve approach which assumes ˆβ∼ N (βMo,Σ) and ignores all\nselection steps preceding inference, is clearly invalid. Furthermore, we consider the\nfollowing methods that treat λoas fixed and offer only partial corrections for the\nvariable selection process:\n1.Polyhedral : inference is based on the truncated normal distribution of ˆβ\nconditioned on the lasso selection event (Lee et al., 2016).\n2.Bivnormal : quantiles of ˆβjare computed based on a bivariate normal\ndistribution (Panigrahi et al., 2024), which marginalizes over an appropriately-\nchosen linear projection of W.\n3.SOV : quantiles of ˆβjare computed using the sampling method from Liu (2023),\nwhich marginalizes over all the randomness in W.\nThese methods are partially invalid because they ignore the selection of λo. We\ncan adjust for the selection of λoby combining these methods with our proposed\nmethod, as described in Section 5.2. These adjusted methods are referred to as\nAdj-Polyhedral ,Adj-Bivnormal ,Adj-SOV . To implement Adj-Polyhedral , we\nmultiply the conditional density for the jthregression coefficient—accounting for the\nselection of λvia our method, that corresponds to the first term ( I) in Proposition 5.1—\nwith 1[Ij\n1,Ij\n2](t), where [ Ij\n1, Ij\n2] denotes the truncation interval obtained with Polyhedral .\n28\n--- Page 29 ---\nForAdj-Bivnormal , we multiply this conditional density, obtained using the learned\ntransport map, with H(t) = Φ( ajt+bj\n2)−Φ(ajt+bj\n1), where aj, bj\n1, bj\n2are scalars\nobtained with Bivnormal . Similarly, Adj-SOV is obtained by multiplying this\nconditional density, based on the learned transport map, with an estimate of the lasso\nselection probability using Monte Carlo sampling obtained with SOV . In the case\nwhen ν2= 0.1, we include the splitting-based UV method, which performs inference\nusing simply the holdout data y−σ2\nν2W.\nThe simulation is repeated 1000 times, out of which about 270 times the selected\nmodel is nonempty. If the selected model is nonempty, we compute the average\ncoverage proportions of all the selected coefficients and the average interval lengths.\nThe results are reported in Figure 7. Methods that do not account for the selection of\nλoconsistently exhibit under-coverage. However, when combined with our proposed\napproach, all these methods—both with and without additional randomization—are\nable to achieve the target 95% coverage probability. As expected, the valid methods\nproduce wider intervals, taking into account the additional uncertainty from the\nselection of the regularization parameter. Consistent with findings from the literature\non randomized selective inference, the data-carving intervals from Adj-Bivnormal\nandAdj-SOV , obtained using the randomized lasso, are shorter than those from the\nother valid approaches, including Splitting andAdj-Polyhedral .\n6.4 Selecting the number of principal components\nBelow, we consider performing a principal component regression (PCR) analysis with\na Bernoulli random variable, where the number of principal components (PCs) is first\nselected via CV. Let Kdenote the number of PCs, which we select using a 5-fold CV\non the data. Let ZMo∈Rn×Kdenote the feature matrix consisting of the top KPCs\nofX. We then fit the logistic regression model\nyi|Zi∼Bernoulli((1 + exp( −β0−Z⊺\nMo,iβ))−1),\nwhere β0represents the intercept. We begin with a simulation study, followed by an\napplication in single-cell data analysis.\nSimulations For our simulations, the predictors are generated as xiiid∼ N p(0,Σ),\nwhere the covariance matrix Σ has entries Σ ij=ρ|i−j|. The responses are generated\nasyiiid∼Bernoulli (1/2), with no dependence between yandx. We fix n= 100,\np= 50, and vary ρ∈ {0.3,0.6,0.9}, which are labeled as Corr-1, Corr-2, and Corr-3,\nrespectively, in the plots. Inference is only performed when the selected K > 0. After\n29\n--- Page 30 ---\nCorr-1 Corr-2 Corr-30.70.80.91.0Coverage proportion\nCorr-1 Corr-2 Corr-30.00.10.20.30.40.5Interval length\nCorr-1 Corr-2 Corr-30.00.10.20.3Type I error (global null)\nNaive\nProposedFigure 8: Results for the principal component regression example. Left panel: average\ncoverage proportions of regression coefficients βjin the selected PCR model. Middle\npanel: average interval lengths for the confidence intervals of βj. Right panel: Type\nIerror for testing the global null.\nselection, we construct confidence intervals for the regression coefficients, and test\nthe global null hypothesis that all βj= 0 for 1 ≤j≤K.\nIgnoring the selection step, one would construct Wald-type confidence intervals\nfor the individual regression coefficients βjfor 1≤j≤K, and perform a likelihood\nratio test (LRT) for the global null hypothesis. However, due to selection bias, the\nresulting confidence intervals would fail to achieve the nominal coverage probabilities,\nand the LRT would not properly control the Type Ierror. Our method, by contrast,\naccounts for the selection of the number of principal components, providing valid\ntests and confidence intervals for the post-selection parameter.\nThe simulation is repeated 1000 times for each scenario, among which about\n300-400 simulations select a non-zero K. The results are shown in Figure 8. The\nconfidence intervals constructed by our data-carving method achieve the target\ncoverage probability, with lengths only slightly longer than those of the na¨ ıve Wald\nintervals. Additionally, our method provides a valid test for the global null hypothesis,\nwith proper control of the selective Type Ierror as expected.\nApplication in single-cell gene expression analysis Single-cell RNA sequencing\n(scRNA-seq) enables researchers to profile gene expression at the resolution of\nindividual cells, allowing for the discovery and characterization of highly specialized\ncell types (Regev et al., 2017). Due to the high dimensionality of gene expression data,\nprincipal component analysis (PCA) is widely used for dimensionality reduction, and\nis applied as a standard preprocessing step in popular tools for analyzing scRNA-seq\ndata, such as the Seurat R package (Satija et al., 2015; Stuart et al., 2019; Hao et al.,\n30\n--- Page 31 ---\n2021). One common application involves using PCA to derive features before fitting\na model in a supervised analysis. For example, the prediction algorithm scPred\n(Alquicira-Hernandez et al., 2019) trains classifiers using the first few PCs to perform\ntasks such as predicting cell types in peripheral blood mononuclear cells (PBMCs).\nThe number of PCs to retain is typically selected using heuristics like the elbow\nplot. More principled methods, such as data thinning or cross-validation, can also be\napplied for selection, as demonstrated in Neufeld et al. (2024).\nFollowing a similar approach to (Alquicira-Hernandez et al., 2019), we focus on\ndistinguishing memory B cells from na¨ ıve B cells in a PBMC dataset. We use the\npublicly available 10X Genomics dataset1and fit a logistic PCR model, as described\nearlier in this section. The number of PCs included in the logistic regression fit is\nselected using a five-fold CV. Preprocessing steps follow the guidelines in the Seurat\ntutorial2, with details provided in Appendix B.4. After preprocessing, we retain 2000\ngenes and 233 cells annotated as either memory B cells (140 cells) or na¨ ve B cells (93\ncells).\nCV selects the top six PCs, which are then used to construct features for the\nlogistic regression model. Our focus is on constructing p-values and confidence\nintervals for the coefficients in the fitted logistic regression model, as commonly\nreported in inference summary tables. As discussed earlier, the na¨ ıve inferential\napproach that ignores the presence of the CV procedure used to construct the feature\nmatrix will likely produce overly optimistic and misleading conclusions. In contrast,\nour proposed method offers a principled approach to valid inference by accounting\nfor the selection procedure while utilizing the entire dataset for this task.\nTable 1 summarizes our results. The na¨ ıve method reports the first three PCs\nas statistically significant, whereas our method identifies only the first two. For\nthe remaining coefficients, our method produces wider confidence intervals than the\nna¨ ıve method, reflecting the additional uncertainty introduced by the data-driven\nconstruction of features.\n7 Concluding remarks\nIn this paper, we introduce a data-carving method that enables powerful and flexible\nselective inference with conditional guarantees. On the one hand, unlike approaches\nsuch as data-splitting—that condition on the data used for selection, equivalent\nto conditioning on much more than necessary—our method reuses data from the\n1https://www.10xgenomics.com/datasets/5k_Human_Donor1_PBMC_3p_gem-x\n2https://satijalab.org/seurat/articles/pbmc3k_tutorial\n31\n--- Page 32 ---\nNa¨ ıve Proposed\nPC p-value CI p-value CI\n1 0.000 ( −1.913,−0.581) 0.005 ( −1.840,−0.359)\n2 0.000 ( −4.117,−2.375) 0.001 ( −4.590,−2.676)\n3 0.022 ( −1.609,−0.125) 0.783 ( −2.947,1.170)\n4 0.516 ( −0.810,0.407) 0.834 ( −1.932,1.106)\n5 0.588 ( −0.379,0.668) 0.264 ( −0.353,1.175)\n6 0.136 ( −0.162,1.186) 0.889 ( −4.660,1.323)\nTable 1: P-values and 95% confidence intervals for the regression coefficients from\nthe selected logistic PCR in the PBMC data analysis.\nselection steps by conditioning on less. On the other hand, unlike existing data-\ncarving methods—which rely on an explicit analytical characterization of the selection\nevent—our method can handle selection events for which no such description is\navailable.\nOur data-carving method applies to selection procedures both with and without\nadditional randomization. In cases where certain types of selection events are known\nto suffer a loss in power without external randomization, our method, similar to\napproaches in randomized selective inference, can take into account the randomized\nselection procedure to improve power. However, in contrast to these existing methods,\nwhich rely on a specific form of randomization to make selective inference feasible,\nour approach in this paper does not rely on the form of the randomization used. For\nexample, in our simulations, we used a standard CV procedure based on sample\nsplitting to select tuning parameters. However, we note that our method can be\nreadily applied to alternative CV techniques, such as the antithetic CV proposed\nin Liu et al. (2024), which employs a correlated Gaussian randomization scheme to\nselect tuning parameters.\nThe key statistical idea underlying our approach is simple: we use a pushforward\ntransport map from a simple reference density to the conditional distribution, and\nthen apply the inverse map—the pullback map—to perform selective inferential tasks\nsuch as hypothesis testing and interval estimation. To efficiently learn the pullback\nmap, we employ a normalizing flow. More broadly, our work demonstrates how\npowerful tools from generative modeling can be utilized to broaden the scope of\nselective inference methodology, while still ensuring strong conditional guarantees,\nsuch as control of the selective Type Ierror and selective coverage probability.\nFinally, we identify several promising directions for future research. For example,\n32\n--- Page 33 ---\nin the spirit of simulation-based inference, other types of parameterizations of the\ntransport map, such as the generative neural networks in Liu et al. (2021), could be\nused in place of normalizing flows for learning the target conditional distribution.\nPotential extensions of our work include developing selective inference within a\nBayesian framework, which involves sampling from a posterior formed by appending\na prior to the selective likelihood. However, similar to the frequentist line of\nwork, existing methods have primarily focused on selection events with analytical\ncharacterizations. By contrast, new extensions could enable selective inference for\npotentially intractable posteriors, allowing data-scientists to leverage the full benefits\nof the Bayesian framework.\nA Derivation of the conditional density\nProposition A.1. The conditional density of T|cM=Mois proportional to\npθMo(t)×PθMoh\ncM=Mo|T=ti\n.\nProof. The joint density of TandWis given by pθMo(t)×pW(w). The joint density\nconditional on cM(D, W ) =Mois proportional to\np(t, w|cM(D, w) =Mo)∝pθMo(t)·pW(w)·Ph\ncM(D, w) =Mo|T=t, W =wi\n.\nIntegrating over w, we obtain\np∗\nθMo(t)∝pθMo(t)·Ph\ncM(D, W ) =Mo|T=ti\n,\nwhere the expectation is taken over WandD|T=t.\nB Details of the experiments\nB.1 Normalizing flow architecture\nWe construct the normalizing flow τ(·) =τ(·;ψ) by stacking Laffine coupling layers\nDinh et al. (2017). Given a subset u⊂1:d, let−udenote the complement of u. Let\nxandx′denote the input and output of an affine coupling layer, with the mapping\ngiven by\nx′\nu=xu,\nx′\n−u= softplus( s(xu))⊙x−u+t(xu).\n33\n--- Page 34 ---\nThe scaling function s(·) and shifting function t(·) are functions of xu, and are\nparametrized by multi-layer perceptrons (MLP). The softplus operator log(1 +ex) is\nused to ensure that the scaling is positive.\nThe coupling layer keeps the input variables xuunchanged, while transforming\nthe remaining variables x−uusing a componentwise affine transformation, whose\nscale and shift are determined by the values of xu. This structure ensures that the\nJacobian of this transformation is a triangular matrix, and the determinant can be\ncomputed efficiently. The inverse of the transformation can be computed by applying\nthe inverse of the affine transformation to x′\n−uand keeping xuunchanged.\nFor conditional normalizing flows, we concatenate the parameter value θwith the\ninput xuin every scaling and shifting function, and they become s(xu, θ) and t(xu, θ),\nrespectively. In all the experiments, we use L= 12 coupling layers. Each scaling and\nshifting MLP has one hidden layer with 8 neurons, and uses the ReLU activation.\nFor the one-dimensional case, we parametrize the one-dimensional map τ:R→R\nby a monotonic rational-quadratic spline Durkan et al. (2019), where the knots and\nthe derivatives at the internal points are the parameters to be learned. If a conditional\nflow is trained, the knots and derivatives are outputs of an MLP, whose input is the\nparameter θ. In all the experiments, we use 20 bins for the rational-quadratic spline.\nB.2 Training details\nWe generate 2000 training samples and 500 validation samples. The training samples\nare used to compute the loss function in (10). We run full-batch Adam with 10000\niterations, and compute the validation loss using the validation samples every 1000\niterations. The parameter corresponding to the lowest validation loss is selected as\nthe final parameter. The learning rate is set to 10−4for all the experiments initially.\nIf training diverges, the learning rate is changed to 10−5.\nB.3 Generating training data\nIn all the experiments except the example of spline regression, we need to train a\nconditional normalizing flow ˆτθthat pushes forward PθtoP∗\nθforθ∈Θ. Therefore,\nwe need to generate pairs of ( T(b), θ(b)) such that T(b)∼P∗\nθ(b)for 1≤b≤B. In\nthe implementation, we draw θ(b)from a normal distribution, whose center is the\n(unconditional) MLE of θand whose covariance is the covariance of the MLE. Given\nevery θ(b), we draw T(b)∼P∗\nθ(b)by rejection sampling. If the rejection sampling fails\nto produce a sample after 100 tries, we stop and continue to generate the next θ(b+1).\n34\n--- Page 35 ---\nB.4 PBMC data preprocessing\nWe use the Seurat package to preprocess the gene expression data. We first filtered\nout low-quality cells by retaining only those that expressed more than 200 and\nfewer than 5000 genes, and had less than 5% of their total expression coming from\nmitochondrial genes. Gene expression counts were then normalized to account for\ndifferences in sequencing depth across cells. From the normalized data, we identified\nthe 2000 most variable genes across all cells and standardized their expression levels to\nhave zero mean and unit variance. Finally, we only keep the cells that are annotated as\neither “memory B cell” or “naive B cell”, resulting in a data matrix of size 233 ×2000.\nPrincipal components are computed based on this matrix.\nReferences\nAlquicira-Hernandez, J., Sathe, A., Ji, H. P., Nguyen, Q., and Powell, J. E. (2019).\nscPred: accurate supervised method for cell-type classification from single-cell\nRNA-seq data. Genome biology , 20:1–17.\nAwan, J. and Wang, Z. (2024). Simulation-based, finite-sample inference for privatized\ndata. Journal of the American Statistical Association , pages 1–14.\nBakshi, S., Huang, Y., Panigrahi, S., and Dempsey, W. (2024). Inference with\nrandomized regression trees. arXiv preprint arXiv:2412.20535 .\nBerk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. (2013). Valid post-selection\ninference. The Annals of Statistics , 41(2):802–837.\nCox, D. R. (1975). A note on data-splitting for the evaluation of significance levels.\nBiometrika , pages 441–444.\nCranmer, K., Pavez, J., and Louppe, G. (2015). Approximating likelihood ratios with\ncalibrated discriminative classifiers. arXiv preprint arXiv:1506.02169 .\nDharamshi, A., Neufeld, A., Motwani, K., Gao, L. L., Witten, D., and Bien, J. (2025).\nGeneralized data thinning using sufficient statistics. Journal of the American\nStatistical Association , 120(549):511–523.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation using Real\nNVP. In International Conference on Learning Representations .\n35\n--- Page 36 ---\nDurkan, C., Bekasov, A., Murray, I., and Papamakarios, G. (2019). Neural spline\nflows. Advances in neural information processing systems , 32.\nFithian, W., Sun, D., and Taylor, J. (2014). Optimal inference after model selection.\narXiv preprint arXiv:1410.2597 .\nGao, L. L., Bien, J., and Witten, D. (2024). Selective inference for hierarchical\nclustering. Journal of the American Statistical Association , 119(545):332–342.\nGuglielmini, S., Claeskens, G., and Panigrahi, S. (2025). Selective inference in\ngraphical models via maximum likelihood. arXiv preprint arXiv:2503.24311 .\nHao, Y., Hao, S., Andersen-Nissen, E., III, W. M. M., Zheng, S., Butler, A., Lee,\nM. J., Wilk, A. J., Darby, C., Zagar, M., Hoffman, P., Stoeckius, M., Papalexi,\nE., Mimitou, E. P., Jain, J., Srivastava, A., Stuart, T., Fleming, L. B., Yeung, B.,\nRogers, A. J., McElrath, J. M., Blish, C. A., Gottardo, R., Smibert, P., and Satija,\nR. (2021). Integrated analysis of multimodal single-cell data. Cell.\nHuang, Y., Pirenne, S., Panigrahi, S., and Claeskens, G. (2023). Selective inference\nusing randomized group lasso estimators for general models. arXiv preprint\narXiv:2306.13829 .\nJames, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). An introduction to\nstatistical learning , volume 112. Springer.\nKivaranovic, D. and Leeb, H. (2021). On the length of post-model-selection confidence\nintervals conditional on polyhedral constraints. Journal of the American Statistical\nAssociation , 116(534):845–857.\nLe Duy, V. N. and Takeuchi, I. (2022). More powerful conditional selective inference\nfor generalized lasso by parametric programming. Journal of Machine Learning\nResearch , 23(300):1–37.\nLee, J., Sun, D. L., Sun, Y., and Taylor, J. (2016). Exact post-selection inference,\nwith application to the lasso. The Annals of Statistics , 44(3):907–927.\nLeiner, J., Duan, B., Wasserman, L., and Ramdas, A. (2025). Data fission: splitting\na single data point. Journal of the American Statistical Association , 120(549):135–\n146.\nLiu, Q., Xu, J., Jiang, R., and Wong, W. H. (2021). Density estimation using deep\ngenerative neural networks. Proceedings of the National Academy of Sciences ,\n118(15):e2101344118.\n36\n--- Page 37 ---\nLiu, S. (2023). An exact sampler for inference after polyhedral model selection. arXiv\npreprint arXiv:2308.10346 .\nLiu, S., Markovic, J., and Taylor, J. (2022). Black-box selective inference via\nbootstrapping. arXiv preprint arXiv:2203.14504 .\nLiu, S., Panigrahi, S., and Soloff, J. A. (2024). Cross-validation with antithetic\ngaussian randomization. arXiv preprint arXiv:2412.14423 .\nMarin, J.-M., Pudlo, P., Robert, C. P., and Ryder, R. J. (2012). Approximate\nBayesian computational methods. Statistics and computing , 22(6):1167–1180.\nNeufeld, A., Dharamshi, A., Gao, L. L., and Witten, D. (2024). Data thinning for\nconvolution-closed distributions. Journal of Machine Learning Research , 25(57):1–\n35.\nPanigrahi, S., Fry, K., and Taylor, J. (2024). Exact selective inference with\nrandomization. Biometrika , 111(4):1109–1127.\nPanigrahi, S., MacDonald, P. W., and Kessler, D. (2023). Approximate post-selective\ninference for regression with the group lasso. Journal of machine learning research ,\n24(79):1–49.\nPanigrahi, S. and Taylor, J. (2023). Approximate selective inference via maximum\nlikelihood. Journal of the American Statistical Association , 118(544):2810–2820.\nPapamakarios, G. and Murray, I. (2016). Fast ε-free inference of simulation models\nwith Bayesian conditional density estimation. Advances in neural information\nprocessing systems , 29.\nPapamakarios, G., Sterratt, D., and Murray, I. (2019). Sequential neural likelihood:\nFast likelihood-free inference with autoregressive flows. In The 22nd international\nconference on artificial intelligence and statistics , pages 837–848. PMLR.\nPerry, R., Panigrahi, S., Bien, J., and Witten, D. (2024). Inference on the\nproportion of variance explained in principal component analysis. arXiv preprint\narXiv:2402.16725 .\nPirenne, S. and Claeskens, G. (2024). Parametric programming-based approximate\nselective inference for adaptive lasso, adaptive elastic net and group lasso. Journal\nof Statistical Computation and Simulation , pages 1–24.\n37\n--- Page 38 ---\nPrice, L. F., Drovandi, C. C., Lee, A., and Nott, D. J. (2018). Bayesian synthetic\nlikelihood. Journal of Computational and Graphical Statistics , 27(1):1–11.\nRasines, D. G. and Young, G. A. (2023). Splitting strategies for post-selection\ninference. Biometrika , 110(3):597–614.\nRegev, A., Teichmann, S. A., Lander, E. S., Amit, I., Benoist, C., Birney, E.,\nBodenmiller, B., Campbell, P., Carninci, P., Clatworthy, M., et al. (2017). The\nhuman cell atlas. elife, 6:e27041.\nSatija, R., Farrell, J. A., Gennert, D., Schier, A. F., and Regev, A. (2015). Spatial\nreconstruction of single-cell gene expression data. Nature Biotechnology , 33:495–502.\nStuart, T., Butler, A., Hoffman, P., Hafemeister, C., Papalexi, E., III, W. M. M., Hao,\nY., Stoeckius, M., Smibert, P., and Satija, R. (2019). Comprehensive integration of\nsingle-cell data. Cell, 177:1888–1902.\nThomas, O., Dutta, R., Corander, J., Kaski, S., and Gutmann, M. U. (2022).\nLikelihood-free inference by ratio estimation. Bayesian Analysis , 17(1):1–31.\nTian, X. and Taylor, J. (2018). Selective inference with a randomized response. The\nAnnals of Statistics , 46(2):679–710.\nXie, M.-g. and Wang, P. (2022). Repro samples method for finite-and large-sample\ninferences. arXiv preprint arXiv:2206.06421 .\nZrnic, T. and Fithian, W. (2024). Locally simultaneous inference. The Annals of\nStatistics , 52(3):1227–1253.\n38",
  "text_length": 85643
}