{
  "id": "http://arxiv.org/abs/2506.01047v2",
  "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification",
  "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.\nOur dataset is publicly available at: https://github.com/menamerai/cheer-ekman.",
  "authors": [
    "Phan Anh Duong",
    "Cat Luong",
    "Divyesh Bommana",
    "Tianyu Jiang"
  ],
  "published": "2025-06-01T15:13:59Z",
  "updated": "2025-06-03T03:33:33Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01047v2",
  "full_text": "--- Page 1 ---\narXiv:2506.01047v2  [cs.CL]  3 Jun 2025CHEER-Ekman: Fine-grained Embodied Emotion Classification\nPhan Anh Duong, Cat Luong, Divyesh Bommana, Tianyu Jiang\nUniversity of Cincinnati\n{duongap, luongcn, bommandh}@mail.uc.edu, tianyu.jiang@uc.edu\nAbstract\nEmotions manifest through physical experi-\nences and bodily reactions, yet identifying such\nembodied emotions in text remains understud-\nied. We present an embodied emotion classi-\nfication dataset, CHEER-Ekman,1extending\nthe existing binary embodied emotion dataset\nwith Ekman’s six basic emotion categories. Us-\ning automatic best-worst scaling with large\nlanguage models, we achieve performance su-\nperior to supervised approaches on our new\ndataset. Our investigation reveals that sim-\nplified prompting instructions and chain-of-\nthought reasoning significantly improve emo-\ntion recognition accuracy, enabling smaller\nmodels to achieve competitive performance\nwith larger ones.\n1 Introduction\nEmotions are not merely abstract mental states;\nthey are deeply intertwined with somatic experi-\nences. When we feel joy, our faces light up with\nsmiles; when we are scared, our hearts race and our\nhands tremble. These physical reactions are more\nthan just side effects—they are part of how we expe-\nrience and express emotions. This concept, known\nas embodied emotion, suggests that our bodies play\na key role in how we feel, perceive, and under-\nstand emotions (Lakoff and Johnson, 1999; Nieden-\nthal, 2007). In natural language, these connections\nsurface as descriptions of physiological reactions\n(e.g., “my stomach churned in disgust”) or unin-\ntentional physical actions (e.g., “she stomped her\nfeet in frustration”)—phenomena termed embodied\nemotions . Recognizing such expressions is pivotal\nfor understanding implicit emotional cues in narra-\ntives. While recent advances in NLP have focused\nmore on explicit emotion classification (Moham-\nmad et al., 2018) or sentiment analysis (Rosenthal\net al., 2017), the subtler task of identifying em-\n1https://github.com/menamerai/cheer-ekman\nFear\nSurprise\nJoy\nAnger\nDisgust\nSadness\nJust the thought of public speaking makes my heart want to pound out of my chest.Seeing the balloon with Happy Birthday written on it made my heart race.Figure 1: Illustration of embodied emotions classified\ninto six categories.\nbodied emotions remains less explored, despite its\npsychological grounding and practical relevance.\nThe CHEER dataset (Zhuang et al., 2024) filled\na gap in this field by providing a collection of sen-\ntences where body parts are used to express emo-\ntions. The dataset includes 7,300 human-annotated\nsentences containing body part references, propos-\ning a binary classification task which we will refer\nto as “embodied emotion detection.” However, one\nlimitation of this work is that it does not distinguish\nbetween different types of emotions—for instance,\nwhether a racing heart signals fear or excitement.\nThe framework of Ekman’s (1992) basic emotions\noffers a potential solution to this limitation. By\nlinking embodied expressions to these specific emo-\ntions, we can build systems that better understand\nhuman emotional experiences.\nTo achieve this goal, we extend the CHEER\ndataset by annotating all its 1,350 positive sam-\nples with six Ekman emotion labels ( Joy,Sadness ,\nAnger ,Disgust ,Fear, and Surprise ), creating a new\ndataset, CHEER-Ekman , as illustrated in Figure 1.\nFor clarity, we refer to the novel classification task\nproduced by this dataset as “embodied emotion\nclassification ,” as compared to the binary task dis-\ncussed above. We adopt the automatic best-worst\nscaling (BWS) technique (Kiritchenko and Mo-\nhammad, 2017; Bagdon et al., 2024) with large\n--- Page 2 ---\nlanguage models (LLMs) to tackle the task. Our\nexperiments show that using Llama 3.1 8B with\nBWS significantly outperforms zero-shot prompt-\ning. The best BWS experiment achieved a 50.6\nF1-score, surpassing supervised BERT (49.6) and\nbeating zero-shot approaches by around 20 points.\nBuilding on Zhuang et al.’s (2024) investigation\nof LLMs’ capability for embodied emotion detec-\ntion, we further explore prompting techniques that\nenhanced the detection task. Our experimental re-\nsults reveal that LLMs can make better recognition\nwhen instructions are rephrased in plain, easily un-\nderstood language, which boosts F1 by nearly 30\npoints compared to technical definitions. Moreover,\nchain-of-thought reasoning enables an 8B parame-\nter model to nearly match a 70B model, closing the\nperformance gap to within 7 F1 points. In summary,\nour contributions are three-fold:\n1.We present CHEER-Ekman, an extension\nof the CHEER dataset that enriches embod-\nied emotion expressions with fine-grained\nEkman emotion labels, addressing a criti-\ncal gap in understanding how specific emo-\ntions manifest through bodily expressions.\nOur dataset is available at: https://github.\ncom/menamerai/cheer-ekman .\n2.We demonstrate that the automatic best-\nworst scaling technique enables LLMs to per-\nform emotion classification without any task-\nspecific training, achieving performance that\nexceeds supervised approaches.\n3.We reveal that counterintuitively, simplified\neveryday language in prompts dramatically\noutperforms technical definitions for embod-\nied emotion tasks, and that structured rea-\nsoning through chain-of-thought can allow\nsmaller language models to perform at a level\ncloser to larger language models on our task.\n2 Related Work\nEmotion recognition in natural language process-\ning has been extensively studied, with researchers\nfocusing more on explicit emotion using datasets\nlike SemEval (Strapparava and Mihalcea, 2007;\nMohammad et al., 2018) and GoEmotions (Dem-\nszky et al., 2020). Recent research has expanded\nto explore nuanced aspects of emotion expres-\nsion (Li et al., 2021), including emotion intensity\nprediction (Mohammad, 2018; Bagdon et al., 2024)\nand the detection of subtle emotional cues in dia-logues (Poria et al., 2019; Ghosal et al., 2020; Li\net al., 2022). The advent of large language models\n(LLMs) has catalyzed significant advances in emo-\ntion understanding capabilities (Lee et al., 2024;\nSabour et al., 2024; Zhao et al., 2024; Liu et al.,\n2024).\nWhile these works contribute to a deeper under-\nstanding of emotion detection in text, the embodied\nnature of emotions—how physical sensations and\nactions encode affective states—has received com-\nparatively less attention. The concept of embodied\nemotion is rooted in cognitive science, particularly\nin the works of Lakoff and Johnson (1999) and\nNiedenthal (2007), which suggest that emotional\nexperiences are closely tied to bodily states and\nactions. Despite the psychological grounding of\nembodied emotions, computational approaches to\ncapturing them in text remain limited. Zhuang et al.\n(2024) introduced the CHEER dataset, which pro-\nvides a collection of sentences where body parts\nare explicitly used to express emotions. Our work\nextends theirs by incorporating Ekman’s six ba-\nsic emotions into embodied emotion recognition,\noffering a more fine-grained classification system.\nAnother relevant line of work is the study of\nemotion taxonomy. Although recent advances in\npsychology have offered newer granular categories\nof emotions such as 27 emotions by Cowen and\nKeltner (2017), which has been adopted in both\ntextual emotion datasets (Demszky et al., 2020)\nand visual emotion dataset (Kosti et al., 2019),\nwe follow the vast majority of existing emotion\ndatasets (Strapparava and Mihalcea, 2007; Moham-\nmad et al., 2018; Poria et al., 2019) by utilizing the\nsix basic emotions ( Joy,Sadness ,Anger ,Disgust ,\nFear , and Surprise ) proposed by Ekman (1992),\nwhich remain foundational due to their universal-\nity and simplicity. Future research may explore\nintegrating alternative taxonomies into embodied\nemotion classification to enhance both granularity\nand coverage.\n3 Methods\nOur methodological approach comprises three key\ncomponents that build upon and extend the work\nof Zhuang et al. (2024). First, we explore prompt-\ning strategies to enhance LLMs’ capability to\ndetect embodied emotions. Second, we intro-\nduce CHEER-Ekman, a refinement of the original\nCHEER dataset that adds fine-grained emotion la-\nbels. Finally, we adopt the BWS framework for\n--- Page 3 ---\nemotion classification that leverages comparative\njudgments to improve classification accuracy.\n3.1 Prompting LLMs for Embodied Emotion\nDetection\nTo address the gap in prompt design within the\nZhuang et al.’s (2024) framework, we first sought\nto enhance the embodied emotion detection task\nusing state-of-the-art LLMs and explore the im-\npact of prompt engineering on performance. Our\napproach centers on two key strategies: prompt\nsimplification to mitigate linguistic complexity and\nchain-of-thought (CoT) prompting.\nPrompt Simplification. We investigated the ef-\nfects of linguistic and domain complexity by\nconducting experiments with the base Llama-\n3.1 (Grattafiori et al., 2024) and the recently re-\nleased DeepSeek-R1 distilled version (DeepSeek-\nAI et al., 2025). Specifically, we compared two\nprompts: the base prompt used in Zhuang et al.\n(2024) and a simplified prompt, which reduces syn-\ntactic and lexical complexity to minimize potential\ncomprehension barriers for LLMs.\nChain-of-Thought Prompting. We further ex-\nplored eliciting reasoning from the model by imple-\nmenting chain-of-thought (CoT) prompting. Based\non Zhuang et al.’s (2024) annotation criteria for\nembodied emotion detection, we developed three\nCoT variants: a 2-step variant that evaluates emo-\ntional causation and purposeless expression, a 3-\nstep variant that adds body part identification, and\na simplified 2-step variant with reduced linguistic\ncomplexity. These variants allowed us to exam-\nine how explicit causal reasoning affects both the\nmodel’s emotion detection performance and its un-\nderstanding of body-emotion relationships.\n3.2 Dataset Creation\nWhile embodied emotion detection identifies emo-\ntional expressions through bodily movements, un-\nderstanding the specific emotions conveyed re-\nquires more fine-grained annotation. To address\nthis need, we propose CHEER-Ekman , a re-\nfined dataset extending the original CHEER corpus\n(Zhuang et al., 2024) by annotating its 1,350 pos-\nitive embodied emotion instances with Ekman’s\n(1992) six basic emotions ( Joy,Sadness ,Anger ,\nDisgust ,Fear , and Surprise ). Our adoption of Ek-\nman’s basic emotions taxonomy balances granular-\nity with practical considerations, as recent research\nby Liu et al. (2024) demonstrates that finer-grained\nFigure 2: CHEER-Ekman dataset distribution of emo-\ntions.\nEmotion Sentence\nJoy ... watched the fireflies with a loving look on his face.\nSadness ... frowning and scuffing his feetalong the floor.\nFear Marty nervously runs his fingers through his hair...\nAnger ... makes me want to hit my head against the wall.\nDisgust Dean snorted incredulously, shaking his head in disbelief.\nSurprise ... my eyes almost fell out of my head.\nTable 1: Examples in our CHEER-Ekman datasets.\nemotion taxonomies often face sparsity issues, even\nin bigger datasets like GoEmotions (Demszky et al.,\n2020).\nThis approach also maintains consistency with\nZhuang et al.’s (2024) methodology, which utilized\nemotion-associated adverbs derived from these ba-\nsic emotions for weak supervision. We have also\nelected not to include the weakly labeled positive\nsamples from the CHEER dataset, prioritizing our\nfinal dataset quality and reliability over quantity.\nWe recruited two annotators to label the 1,350\nembodied emotion sentences from the original\nCHEER dataset. For each sentence, we provide\nthe annotators with the sentence, the relevant body\npart, and up to three preceding sentences for con-\ntext. We then ask the annotators to select one of\nthe six emotions that best match the physical expe-\nrience described through the body part. The pair-\nwise inter-annotator agreement by Cohen’s Kappa\nis 0.64, indicating good agreement. Finally, the\nannotators adjudicated their disagreements to pro-\nduce the final gold labels. We show some example\nsentences with their annotated emotions in Table 1.\nFigure 2 illustrates the emotion distribution of\nsentences in our newly constructed CHEER-Ekman\ndataset. Specifically, Fear is the most preva-\nlent emotion (24.7%), followed by Joy(21.2%),\nSadness (19.3%), Surprise (13.3%), and Disgust\n(12.5%). Anger appeared last at 9.0%.\n--- Page 4 ---\n3.3 BWS for Emotion Classification\nTo address the fine-grained emotion classification\ntask proposed with CHEER-Ekman, we first tested\nzero-shot LLM prompting. However, the model\noften fails to adhere to the instructions and is prone\nto erroneous behaviors, leading to incorrect out-\nputs. Inspired by recent work on automatic emotion\nintensity annotation using LLMs (Bagdon et al.,\n2024), we adopted the same best-worst scaling tech-\nnique. The methodology involves presenting the\nLLM with tuples of four different sentences, in-\nstructing it to identify the body part instances that\nmost and least represent a specific Ekman emotion.\nThen, the equation#Best−#Worst\n#Totalis used to calcu-\nlate a score per sentence per emotion. Finally, we\nchoose the emotion that receives the highest score\nto be the prediction for the sentence.\nTo examine how the number of comparisons af-\nfects classification accuracy, we tested a broader\nrange of tuple counts, from 2Nto72N, increas-\ning by 50% at each step of expansion (where Nis\nthe number of instances to be classified). While\nBagdon et al. (2024) found that more tuples im-\nproved accuracy in their experiments up to 12N,\nwe expanded this investigation to 72N to further\nexplore performance gain behaviors from scaling\ncomparative rounds.\n4 Evaluation\nWe conduct experiments to tackle both the embod-\nied emotion detection and emotion classification\ntasks. The embodied emotion detection CHEER\ndataset contains 7,300 sentences, and our CHEER-\nEkman dataset contains 1,350 sentences. We ex-\nplored various strategies and models, and reported\ntheir F1-scores on both datasets.\n4.1 Embodied Emotion Detection\nSimple Prompting Analysis. To obtain the bi-\nnary classification results, we directly compare the\nlogit probabilities of “ True” and “ False ” tokens\ninstead of using text generation. This approach en-\nsures deterministic outputs by avoiding the random-\nness inherent in sampling-based decoding methods,\nwhile also preventing potential output format vio-\nlations that can occur during free-form generation.\nTable 2 shows that simplified prompts led to sub-\nstantial performance improvements for the 70B\nparameter models. The F1-score increased by 29.5\npoints for Llama-3.1-70B, and by 41.6 points for\nDeepSeek-R1-70B (distilled on Llama), surpassingModelMacro\nF1EE Neutral\nPre Rec F1 Pre Rec F1\nLlama base 37.2 21.5 99.6 35.3 99.6 24.2 39.0\nLlama simple 66.7 37.5 89.3 52.8 96.9 68.9 80.6\nDeepSeek base 32.6 20.3 99.4 33.7 99.3 18.7 31.5\nDeepSeek simple 74.2 51.2 69.3 58.9 93.1 86.2 89.5\nGPT 3.5 base 70.2 44.0 68.3 53.5 92.5 81.9 86.9\nBERT 83.5 73.2 72.1 72.6 94.2 94.5 94.4\nTable 2: Results comparison for Embodied Emotion De-\ntection. Llama: Llama-3.1-70B. DeepSeek: DeepSeek-\nR1-Distilled-Llama-70B. GPT 3.5 and fine-tuned BERT\nnumbers are from Zhuang et al. (2024). The base and\nsimple subscripts indicate the type of prompts, which\ncan be found in Table 5.\nModelMacro\nF1EE Neutral\nPre Rec F1 Pre Rec F1\nLlama 2-step 53.4 26.2 80.8 39.6 93.0 52.7 67.2\nLlama 3-step 54.8 24.9 53.4 34.0 87.3 66.5 75.5\nLlama 2-step-simple 60.1 31.5 44.5 36.9 87.4 79.8 83.4\nDeepSeek 2-step 52.2 26.4 90.8 40.9 96.1 47.3 63.4\nDeepSeek 3-step 57.4 27.9 62.0 38.5 89.4 66.7 76.4\nDeepSeek 2-step-simple 67.5 40.1 65.2 49.7 91.7 79.8 85.3\nTable 3: CoT results for Embodied Emotion Detec-\ntion. Llama: Llama-3.1-8B. DeepSeek: DeepSeek-R1-\nDistilled-Llama-8B. The 2-step ,2-step-simple , and 3-\nstepsubscripts indicate the type of prompt accompany-\ning the model in that run. Prompt details are in Table 6.\nGPT 3.5 results reported in Zhuang et al. (2024).\nChain-of-Thought (CoT) Analysis. Table 3\nshows that CoT prompting enhanced performance\nto competitive levels with larger models in the ex-\nperiments of Table 2, particularly benefiting dis-\ntilled reasoning models like DeepSeek-R1-8B (dis-\ntilled on Llama). The DeepSeek 8B model us-\ning simple 2-step prompts (DeepSeek 2-step-simple )\nachieved results within 6.7 F1-points of its larger\n70B counterpart (DeepSeek simple ) and 2.7 F1-\npoints of GPT 3.5. Deeper reasoning processes\nproved more effective, with 3-step CoT consis-\ntently outperforming 2-step variants across both\nmodels. Finally, simplified prompting substantially\nimproved CoT performance, yielding F1-score in-\ncreases of 6.7 (Llama 2-step-simple vs. Llama 2-step)\nand 15.3 (DeepSeek 2-step-simple vs. DeepSeek 2-step).\nError Analysis. To investigate model failures in\nthe zero-shot experiments using the simple prompt\nsetting with Llama-3.1-70B, we analyzed incorrect\npredictions. We found a pronounced false-positive\nbias, accounting for 93.3% of all errors. A man-\nual inspection of 100 false-positive cases revealed\n--- Page 5 ---\nModel F1 F1-J F1-Sa F1-F F1-A F1-D F1-Su\nLlama 31.6 39.4 43.6 26.6 32.2 19.1 28.5\nDeepSeek 28.4 43.3 35.7 33.1 23.1 14.8 20.2\nBWS 4N 41.8 62.3 57.7 37.9 28.1 30.1 33.9\nBWS 12N 44.6 67.1 59.2 44.3 38.6 19.0 39.3\nBWS 36N 50.6 66.7 64.7 48.0 53.2 22.0 48.9\nBWS 48N 49.8 68.0 62.8 48.2 51.3 24.8 43.8\nBWS 72N 49.5 68.5 64.5 46.2 51.6 20.5 45.6\nBERT 49.6 68.2 57.5 50.1 30.2 56.1 35.7\nTable 4: Results for Emotion Classification. Llama:\nLlama-3.1-8B. DeepSeek: DeepSeek-R1-Distilled-\nLlama-8B. BWS: Automatic BWS with Llama-3.1-8B.\nThe first column F1 is the macro-averaged score, fol-\nlowed by F1-score F1-x, where J - Joy, Sa - Sad, F -\nFear, A - Anger , D - Disgust , and Su - Surprise .\nthree main patterns. First, 17% of cases involved\nreferenced body parts that were present in the ex-\nperience or expression without acting, as in “ tears\nfalling down the face.” Second, 42% of errors\nstemmed from body parts performing functional or\nphysiological roles within emotional contexts, such\nas eyes closing when “ blackness crept across his\neyes,” a natural physiological reaction associated\nwith the character passing away within the context.\nFinally, 41% of errors involved metaphorical or\nidiomatic expressions. These included cases where\nemotional embodiment was implied but not explic-\nitly stated (“ I couldn’t believe my eyes,” implying\nwidened eyes in surprise but not explicitly describ-\ning this action), expressions symbolically referring\nto emotional states without literal physical embodi-\nment (“ a straw that broke my back ”), or purely lo-\ncational expressions involving body parts without\nany action (“ thoughts racing through my head ”).\nThese nuanced distinctions highlight the model’s\nchallenges in accurately interpreting metaphorical,\nsymbolic, and non-embodied references.\n4.2 Embodied Emotion Classification\nOur experimental results demonstrate notable per-\nformance disparities across prompting strategies\nand model architectures. We use the Llama-3.1-\n8B as the LLM interpreter for best-worst scaling\n(BWS). In Table 4, the first section shows the per-\nformance of zero-shot large language models, in-\ncluding Llama-3.1-8B and DeepSeek-R1-8B. The\nsecond section shows BWS results with different\nnumbers of tuples. And the last section shows a\nfine-tuned BERT model for comparison (details in\nAppendix C). We see that BWS exhibits superior\nperformance even with smaller tuple configurations(4N), exceeding Llama-3.1-8B by 10.2 points. Per-\nformance improves consistently as the number of\ntuples increases from 2Nto36N(40.2 to 50.6),\nsuggesting enhanced classification from expanded\npairwise comparisons. We hypothesize that this\nexpansion helps the model better weigh emotional\nsignificance in text, improving classification accu-\nracy. Notably, the best result comes from the expan-\nsion to 36Ntuples, with the F1-score beating the\nsupervised method BERT by 1 point. Our experi-\nmental results show that as we keep increasing N,\nthe performance will reach a plateau as evidenced\nby48Nand72N(see Appendix B).\nError Analysis. To better understand model lim-\nitations, we conducted a qualitative error analysis\non the misclassified cases. We identified several\nconsistent failure modes, including the model’s dif-\nficulty in interpreting emotionally complex inputs\nor making reliable distinctions between closely re-\nlated emotional states. In one case, the model pre-\ndicted Joy, even though the embodied expression\n“Ryan ducks his head down to his notebook” sig-\nnaled Fear . This misclassification likely resulted\nfrom the influence of nearby positive context, such\nas“Brendon waves and smiles” , which distracted\nthe model from the emotion-relevant phrase. In an-\nother case, the vivid scene “the age-old rock tradi-\ntion of holding up lighters spread across the 28,000\nperson deep crowd . . . lighting up the entire audi-\nence . . . the hair on my arms started to raise” was\nmisclassified as Joy, despite strong physiological\ncues such as raised arm hair that more closely re-\nflect Surprise . This highlights the model’s tendency\nto prioritize surface-level celebratory language over\nconflicting embodied cues.\n5 Conclusion\nThis work advances embodied emotion recognition\nthrough three main contributions. First, we created\na new dataset called CHEER-Ekman by extending\nthe CHEER dataset with Ekman emotion labels\nto better understand the connection between bod-\nily expressions and emotional states. Second, we\ndemonstrated that best-worst scaling outperforms\nboth prompted LLMs and fine-tuned BERT, show-\ning the potential for emotion classification without\ntask-specific training. Finally, we found that sim-\nplified language and chain-of-thought reasoning\nsignificantly improve LLM performance in embod-\nied emotion detection, enabling smaller models to\nachieve competitive results.\n--- Page 6 ---\nLimitations\nWhile our approach demonstrates a promising ad-\nvancement in embodied emotion detection using\nLLMs and the best-worst scaling technique, several\nlimitations warrant consideration.\nFirst, a key observation in our embodied emotion\ndetection task was that simplifying prompts signifi-\ncantly improved model performance. While these\nfindings may suggest enhanced efficiency through\nlinguistic streamlining, they simultaneously intro-\nduce concerns about potential overfitting to these\nsimplified phrasings. Simplified prompts may in-\nadvertently prioritize more explicit expressions of\nembodied emotion over subtler or more figurative\nlanguage, meaning the models might learn to recog-\nnize patterns specific to the prompt structure rather\nthan generalizing to a wide variety of natural lan-\nguage expressions.\nSecond, the CHEER-Ekman dataset is relatively\nsmall, consisting of only 1,350 sentences. This\nlimited size stems from our decision to annotate\nonly the sentences already identified as containing\nembodied emotions in the original CHEER dataset.\nThis selective annotation was intended to efficiently\nfocus our efforts on instances most relevant to em-\nbodied emotion, but it may introduce a bias towards\npositive examples.\nFinally, when addressing emotion classification\nvia best-worst scaling, the scalability and computa-\ntional overhead of this methodology present chal-\nlenges: while higher tuple quantities lead to higher\naccuracy, they also impose significant computa-\ntional costs. Due to time and computational con-\nstraints, we utilize a smaller model, which may\nlead to suboptimal results for higher-order tuples.\nAdditionally, the limited context window prevents\nus from effectively implementing a few-shot set-\nting, further impacting performance in scenarios\nrequiring extended context understanding.\nAcknowledgments\nWe thank the CincyNLP group for their construc-\ntive comments. We also thank the anonymous ACL\nreviewers for their valuable suggestions and feed-\nback.References\nChristopher Bagdon, Prathamesh Karmalkar, Harsha\nGurulingappa, and Roman Klinger. 2024. “you are\nan expert annotator”: Automatic best–worst-scaling\nannotations for emotion intensity modeling. In Pro-\nceedings of the 2024 Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL 2024) .\nAlan Cowen and Dacher Keltner. 2017. Self-report\ncaptures 27 distinct categories of emotion bridged by\ncontinuous gradients. Proceedings of the National\nAcademy of Sciences of the United States of America ,\n114.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, and Peiyi Wang. 2025. Deepseek-r1:\nIncentivizing reasoning capability in llms via rein-\nforcement learning. Preprint , arXiv:2501.12948.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2020) .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL 2019) .\nPaul Ekman. 1992. Are there basic emotions? Psycho-\nlogical Review , 99(3).\nDeepanway Ghosal, Navonil Majumder, Alexander Gel-\nbukh, Rada Mihalcea, and Soujanya Poria. 2020.\nCOSMIC: COmmonSense knowledge for eMotion\nidentification in conversations. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020 (Findings of EMNLP 2020) .\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nand Alex Vaughan. 2024. The llama 3 herd of models.\nPreprint , arXiv:2407.21783.\nSvetlana Kiritchenko and Saif Mohammad. 2017. Best-\nworst scaling more reliable than rating scales: A case\nstudy on sentiment intensity annotation. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (ACL 2017) .\nRonak Kosti, Jose M Alvarez, Adria Recasens, and\nAgata Lapedriza. 2019. Context based emotion\nrecognition using emotic dataset. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n42(11).\nGeorge Lakoff and Mark Johnson. 1999. Philosophy\nin the flesh: the embodied mind and its challenge of\nwestern thought . Basic Books. OCLC: 302020239.\n--- Page 7 ---\nGyeongeun Lee, Christina Wong, Meghan Guo, and\nNatalie Parde. 2024. Pouring your heart out: In-\nvestigating the role of figurative language in online\nexpressions of empathy. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (ACL 2024) .\nQintong Li, Piji Li, Zhaochun Ren, Pengjie Ren, and\nZhumin Chen. 2022. Knowledge bridging for em-\npathetic dialogue generation. In Proceedings of the\n36th AAAI conference on artificial intelligence (AAAI\n2022) .\nZhengyan Li, Yicheng Zou, Chong Zhang, Qi Zhang,\nand Zhongyu Wei. 2021. Learning implicit sentiment\nin aspect-based sentiment analysis with supervised\ncontrastive pre-training. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP 2021) .\nZhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang,\nand Sophia Ananiadou. 2024. Emollms: A series\nof emotional large language models and annotation\ntools for comprehensive affective analysis. In Pro-\nceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining (KDD 2024) .\nSaif Mohammad. 2018. Obtaining reliable human rat-\nings of valence, arousal, and dominance for 20,000\nenglish words. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2018) .\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. SemEval-\n2018 task 1: Affect in tweets. In Proceedings of the\n12th International Workshop on Semantic Evaluation\n(SemEval 2018) .\nPaula M. Niedenthal. 2007. Embodying Emotion. Sci-\nence, 316(5827):1002–1005.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. MELD: A multimodal multi-party\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL 2019) .\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nInProceedings of the 11th International Workshop\non Semantic Evaluation (SemEval 2017) .\nSahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu,\nJinfeng Zhou, Alvionna Sunaryo, Tatia Lee, Rada Mi-\nhalcea, and Minlie Huang. 2024. EmoBench: Eval-\nuating the emotional intelligence of large language\nmodels. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2024) .\nCarlo Strapparava and Rada Mihalcea. 2007. SemEval-\n2007 task 14: Affective text. In Proceedings of the\nFourth International Workshop on Semantic Evalua-\ntions (SemEval 2007) .Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang,\nYulin Hu, Yanyan Zhao, Chen Wei, and Bing Qin.\n2024. Both matter: Enhancing the emotional intel-\nligence of large language models without compro-\nmising the general intelligence. In Findings of the\nAssociation for Computational Linguistics ACL 2024\n(Findings of ACL 2024) .\nYuan Zhuang, Tianyu Jiang, and Ellen Riloff. 2024. My\nheart skipped a beat! recognizing expressions of em-\nbodied emotion in natural language. In Proceedings\nof the 2024 Annual Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL 2024) .\n--- Page 8 ---\nA LLM Prompt for Embodied Emotion\nDetection & Embodied Emotion\nClassification\nTable 5-7 presents the complete prompt templates\nemployed in our experimental methodology. Ta-\nble 5 details the prompts used for zero-shot em-\nbodied emotion detection experiments. The Base\nprompt closely replicates the methodology of\nZhuang et al. (2024), with the sole modification\nbeing the use of “True” and “False” as decision\ntokens rather than “Yes” and “No”. The Simple\nprompt maintains the fundamental logic while em-\nploying more straightforward language and struc-\nture to reduce cognitive complexity.\nTable 6 presents our chain-of-thought (CoT)\nprompt variants. The 2-step implementation in-\ncorporates the dual criteria from the Base prompt\nas explicit reasoning steps. The 3-step variant aug-\nments this with an initial body movement identifi-\ncation phase, designed to establish concrete context\nand facilitate more comprehensive reasoning. The\n2-step simple variant examines the effectiveness of\nlinguistic simplification within the CoT framework.\nTable 7 outlines the large language model (LLM)\nprompt utilized for emotion classification experi-\nments with Llama-3.1-8B and DeepSeek-R1-8B\nmodels.\nThroughout our prompt templates, we employ\nthe following placeholder semantics:\n•“<sentence|> ” denotes the target sentence\ncontaining the body part for evaluation.\n•“<bdypart|> ” indicates the specific body part\ninstance within the sentence.\n•“<preceed|> ” represents up to three preced-\ning context sentences, when available.\nThis placeholder convention remains consistent\nacross all experimental tasks presented in this re-\nsearch.\nB Best-Worst Scaling\nExperiment Setup. Best-worst scaling (BWS) is a\ncomparative annotation method where annotators\nselect the best and worst items from a given set,\ntypically a 4-tuple. This approach efficiently de-\nrives pairwise comparisons, as selecting the best\nand worst items provides information about most\nitem relationships within the set. A single anno-\ntation with best-worst scaling is equivalent to anannotation with 6 pairwise comparisons. Hence, us-\ning BWS allows for fewer inferences with the same\nresult. This is particularly beneficial for identifying\nemotion intensity or emotion classification.\nThese 4-tuples are assembled from the test data\nand then are presented to LLMs using the prompt\nin Table 8. The model then picks one instance\nthat most represents and one instance that least\nrepresents some property (in our case, this would\nbe one of the six Ekman’s (1992) emotions).\nOnce multiple 4-tuples are annotated, a simple\ncounting procedure generates numerical scores, al-\nlowing items to be ordered according to their rel-\nevance to the given property. The score is cal-\nculated using the formula#Best−#Worst\n#Overall, where\nthe#Best and#Worst represent the number of\ntimes a sentence is ranked Best or Worst, respec-\ntively; and #Overall denotes the number of oc-\ncurrences of the sentence across all 4-tuples. This\napproach captures a continuous measure, reflecting\nthe relative intensity of a sentence within the given\ncategory.\nWith this method, the LLMs can perform accu-\nrate annotations. The results from the annotations\nwill be calculated to get a BWS intensity score\nacross 6 Ekman emotions. The emotion with the\nhighest intensity score will be chosen as the pre-\ndicted label. This classification process can be\nrepresented by the following expression:\nˆe= arg max\nei∈ES(ei)\nwhere eicorresponds to each emotion, Eis the set\nof all emotions, and S(ei)is the intensity score\nw.r.t. to such emotion. The resulting predictions\nare compared with the labels from the CHEER-\nEkman dataset to assess performance using several\nmetrics, with a particular focus on the F1-score.\nThe approach of increasing the number of tuples\nto enhance performance was proposed by Bagdon\net al. (2024).\nAlong with the embodied emotion classification\nprompt, we also incorporate two additional place-\nholder semantics:\n•“<textid|> ” denotes the unique instance ID\nfrom the dataset. This id helps the model\neasily pick out its answer from the sentence\ntuple when inferencing.\n•“<emo|> ” denotes the specific emotion re-\nquired for ranking.\n--- Page 9 ---\nThis placeholder convention remains consistent\nacross all experimental tasks presented in this re-\nsearch.\nPerformance Plateau. Figure 7 illustrates the re-\nlationship between tuple count and F1-score per-\nformance in our BWS experiments. The results\ndemonstrate significant performance improvements\nup to 24Ntuples, reaching optimal performance at\n36N. Beyond this threshold, performance degrada-\ntion is observed, with decreased F1-scores at both\n48Nand72Ntuple configurations. This pattern\nsuggests a clear upper bound for effective tuple\nscaling in BWS implementations.\nC BERT Experiment Setup\nFor the embodied emotion classification task dis-\ncussed in Section 3.3, in addition to BWS, we\nfine-tuned BERT as a reference benchmark (Devlin\net al., 2019). Inputs were constructed by concate-\nnating the preceding context, main text, and refer-\nenced body part. We set the maximum sequence\nlength to 512 and use a batch size of 16. The model\nwas trained with the AdamW optimizer, a learn-\ning rate of 2e-5, and evaluated over 15 training\nepochs. Cross-entropy loss was used as the objec-\ntive, with tokenization performed using truncation\nand padding. All runs were conducted with 5 seed\nvalues starting from 41 to 45 for reproducibility,\nand final results were averaged across these five\nruns.\nD Data and Results Analysis\nFigure 3 - 6 present a more detailed analysis of the\nCHEER-Ekman dataset and evaluation of models’\nperformance. Figure 3 illustrates the frequency of\nthe top 10 body parts associated with each emo-\ntion, where the size of the bubble reflects the co-\noccurrence of the body part and emotion pair. No-\ntably, the body parts face,eye,head ,hand , and\nthroat appear consistently across the top 10 body\nparts across all emotions, with the highest fre-\nquency observed in face,eye, and head .\nFigure 4 illustrates the classification perfor-\nmance of the 10 most frequent body parts,\nwith their frequency and corresponding accuracy\nacross the three language models: Llama-3.1-8B,\nDeepSeek-R1-8B, and fine-tuned BERT. As ex-\npected, the fine-tuned BERT model consistently\noutperforms both Llama and DeepSeek for the\nmost frequent body parts. Generally, the fine-tuned\nBERT model outperforms both zero-shot Llamaand Deepseek, achieving an average accuracy in-\ncrease of 11.7 over Llama and 14.8 over DeepSeek\nacross the top 10 frequent body parts.\nFigure 5 and 6 present confusion matrices com-\nparing the models’ predicted emotions against the\nground truth emotion for Llama and DeepSeek,\nrespectively. When comparing the two figures, a\nnotable pattern emerges. In Figure 5, strong activa-\ntions across the diagonal indicate Llama’s attempt\nto predict emotions accurately without bias towards\nany one emotion. In Figure 6, however, we observe\na prominent concentration in Joyin the DeepSeek\nmodel.\n--- Page 10 ---\nTask Prompt Template\nBase Please determine if a body part is involved in any embodied emotion. Specifically, a body part is\ninvolved in some embodied emotion if both conditions below are satisfied:\n1) The physical movement or physiological arousal involving the body part is evoked by emotion.\n2) The physical movement, if there is any, has no other purpose than emotion expression.\nAnswer \"True\" if the body part is involved in any embodied emotion, and \"False\" otherwise.\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nAnswer:\nSimple Decide if a body part is used purely to express emotion. Ask:\n- Did emotion cause the body part’s movement/response?\n- Was the movement ONLY for expressing emotion (no other reason)?\nIf both are true, say \"True.\" Else, say \"False.\"\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nAnswer:\nTable 5: Zero-shot templates for different tasks.\nSetting Prompt Template\n2-Step Please determine if a body part is involved in any embodied emotion.\nFirst, answer Condition 1: Is the body part’s movement/arousal caused by emotion?\nThen, answer Condition 2: Does the movement lack non-emotional purposes?\nIf both of those conditions are true, answer \"True.\" Otherwise, answer \"False.\" Please reason\nstep-by-step for your answer.\nHere is the question:\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\n3-Step Please determine if a body part is involved in any embodied emotion. Specifically, a body part\nis involved in some embodied emotion if both conditions below are satisfied: Before answering,\nreasoning step-by-step\n1. Identify the body part mentioned.\n2. Check if emotion directly caused its movement/arousal.\n3. Verify if the movement has no functional purpose.\nOnly if all of the above are true, answer \"True.\" Otherwise, answer \"False.\"\nHere is the question:\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\n2-Step Simple Decide if a body part is used purely to express emotion. Ask:\n- Did emotion cause the body part’s movement/response?\n- Was the movement ONLY for expressing emotion (no other reason)?\nIf both are true, say \"True.\" Else, say \"False.\" Before answering, give your reasoning step-by-step.\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nTable 6: Chain of Thought (CoT) prompt templates for different settings.\n--- Page 11 ---\nPrompt Template\nClassify the emotion expressed by the body part in a sentence into one of six categories: \"Joy\", \"Sadness\", \"Anger\", \"Fear\",\n\"Surprise\", or \"Disgust\".\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nAnswer:\nTable 7: Emotion classification prompt template.\nPrompt Template\nYou are an expert annotator specializing in emotion recognition. Rank the following examples based on how much <emo|>\nthe specified body part exudes in the text.\nInstructions:\n- Use only the Preceding Text for context.\n- Identify which example conveys the MOST <emo|> and which conveys the LEAST <emo|> based on the body part mentioned.\n- Do not repeat the text. Only provide the Example numbers in the specified format.\nExample: <textid|>\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nExample: <textid|>\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nExample: <textid|>\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nExample: <textid|>\nPreceding Context: <preceed|>\nSentence: <sentence|>\nBody part: <bdypart|>\nFormat your response as:\nMost <emo|> Example:\nLeast <emo|> Example:\nTable 8: BWS-Emotion classification prompt template.\n--- Page 12 ---\nFigure 3: Frequency of top 10 body parts for each emotion.\n--- Page 13 ---\nFigure 4: Embodied emotion classification of top 10 frequent body parts with frequency (left) and accuracy (right)\ncomparing zeroshot with Llama, DeepSeek and finetuned BERT.\nFigure 5: Confusion matrix of Llama predictions.\n--- Page 14 ---\nFigure 6: Confusion matrix of DeepSeek predictions.\nFigure 7: F1-score trends for BWS when increasing the number of tuples from 2N to 72N (where N is the total\nnumber of instances to be classified). BERT’s performance is shown as a reference baseline.",
  "text_length": 40935
}