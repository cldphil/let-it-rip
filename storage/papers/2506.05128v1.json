{
  "id": "http://arxiv.org/abs/2506.05128v1",
  "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning",
  "summary": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.",
  "authors": [
    "Tanmay Parekh",
    "Kartik Mehta",
    "Ninareh Mehrabi",
    "Kai-Wei Chang",
    "Nanyun Peng"
  ],
  "published": "2025-06-05T15:16:14Z",
  "updated": "2025-06-05T15:16:14Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05128v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05128v1  [cs.CL]  5 Jun 2025DiCoRe: Enhancing Zero-shot Event Detection via\nDivergent-Convergent LLM Reasoning\nTanmay Parekh†Kartik Mehta‡Ninareh Mehrabi‡\nKai-Wei Chang†Nanyun Peng†\n†Computer Science Department, University of California, Los Angeles\n‡Amazon AGI Foundations\n{tparekh, kwchang, violetpeng}@cs.ucla.edu\nAbstract\nZero-shot Event Detection (ED), the task of\nidentifying event mentions in natural language\ntext without any training data, is critical for doc-\nument understanding in specialized domains.\nUnderstanding the complex event ontology, ex-\ntracting domain-specific triggers from the pas-\nsage, and structuring them appropriately over-\nloads and limits the utility of Large Language\nModels (LLMs) for zero-shot ED. To this end,\nwe propose DICORE, a divergent-convergent\nreasoning framework that decouples the task\nof ED using Dreamer and Grounder. Dreamer\nencourages divergent reasoning through open-\nended event discovery, which helps to boost\nevent coverage. Conversely, Grounder intro-\nduces convergent reasoning to align the free-\nform predictions with the task-specific instruc-\ntions using finite-state machine guided con-\nstrained decoding. Additionally, an LLM-\nJudge verifies the final outputs to ensure high\nprecision. Through extensive experiments on\nsix datasets across five domains and nine LLMs,\nwe demonstrate how DICOREconsistently out-\nperforms prior zero-shot, transfer-learning, and\nreasoning baselines, achieving 4–7% average\nF1 gains over the best baseline – establishing\nDICOREas a strong zero-shot ED framework.\n1 Introduction\nEvent Detection (ED) is the task of identifying\nevents by extracting and labeling event triggers\n(Sundheim, 1992; Doddington et al., 2004). ED\naids in various downstream applications, including\nnews monitoring (Tanev et al., 2008), biomedical\nliterature mining (Pyysalo et al., 2012), epidemic\nforecasting (Parekh et al., 2024a,b), and legal un-\nderstanding (Francesconi et al., 2010). Training\neffective ED models requires large amounts of\nexpert-annotated domain-specific data, which is\nhighly costly and labor-intensive. This underlines\nthe need to develop zero-shot systems that can per-\nform ED robustly without using any training data.\nThe family was heading to New Hampshire from Lakeland.[][(“Transport”, “heading”)]His friend, Martha Stewart, pleaded not guilty last week.[(“Charge”, “pleaded”), (“Hearing”, “pleaded”)][]Refusing access would mean Turkey would lose USD $15 billion U.S. aid package.{[“Loss”, “lose”]}[(“Refusal”, “Refusing”), (“Loss”, “lose”)]Recall MissPrecision MissConstraint ViolationsSentenceLlama3-70BGold[][]\nQwen2.5-72BGPT4o[(“Charge”, “pleaded”)][(“Acquit”, “not guilty”)][(“Refusal”, “Refusing”), (“Loss”, “$15 billion USD”)][(“Refusing”, “Refusal”)]\nFigure 1: (top) Illustration of how prompting LLMs\ndirectly for Event Detection (ED) with all the task con-\nstraints can lead to precision, recall, and constraint vio-\nlations (incorrect JSON, trigger not in sentence) across\nvarious LLMs. The errors are highlighted in bold .\n(bottom) Highlighting the superior model performance\n(green bars ) of our proposed DICOREwith minimal\ninference cost ( red bars ) relative to reasoning baselines.\nRecently, large language models (LLMs) have\nshown strong zero-shot performance across vari-\nous tasks (Ouyang et al., 2022a; Li et al., 2023b).\nHowever, their effectiveness on ED remains lim-\nited (Gao et al., 2023; Huang et al., 2024), due to\nthe requirement of extensive domain knowledge\nand the complex structural nature of ED. ED re-\nquires deep reasoning and imposes several inter-\ntwined constraints: study of the large, closed event\nontology and ensuring the event types must be cho-\nsen from it; semantic understanding of the input\n1\n--- Page 2 ---\npassage and precisely identifying domain-specific\ntriggers within it; and conforming the output to a\nstrict, machine-parsable structured format. Encod-\ning these constraints as natural language instruc-\ntions in the prompt overloads the LLM cognitively,\nmaking it harder to effectively apply its reasoning\nskills (Tam et al., 2024). This increased difficulty in\nreasoning causes failures, such as missing relevant\nevents, predicting irrelevant ones, and struggling to\nfollow the expected format, as shown in Figure 1.\nTo this end, we propose DICORE, a novel\npipeline introducing Divergent- Convergent\nReasoning, that facilitates better ED performance\nby reducing the cognitive burden of constraint\nadherence on the LLM. DICOREcomprises two\nmajor components in a pipeline: Dreamer and\nGrounder. (1) Dreamer fosters divergent reasoning\nby prompting in an unconstrained, open-ended\nmanner. This encourages broad semantic explo-\nration of potential event mentions by removing\nrigid task constraints and, in turn, boosts the recall.\n(2) Grounder introduces convergent reasoning by\nmapping Dreamer’s free-form predictions to the\ntask-specific closed event ontology. To alleviate\nthe constraint adherence burden on the LLM, we\nemploy a finite-state machine (FSM) to encode\nstructural and task-specific constraints. This FSM\nguides the generation process through constrained\ndecoding, ensuring that the output adheres to the\ntask requirements. Finally, we add an LLM-Judge\nto verify the grounded predictions against the\noriginal task instructions, ensuring high precision\nby filtering irrelevant predictions.\nWe conduct extensive experiments on six\ndatasets from five domains across nine LLMs.\nCompared with various existing LLM inference\nworks (Gao et al., 2023; Wang et al., 2023; Parekh\net al., 2025), we show how DICOREperforms the\nbest with average improvements of 4-5% F1 Trig-\nger Classification and 5.5-6.5% F1 Event Identifica-\ntion over the best baselines. DICORE, without any\ntraining, also consistently improves over transfer-\nlearning baselines (Hsu et al., 2022; Sainz et al.,\n2023) fine-tuned on 15-30k datapoints by at least\n5-12% F1. Furthermore, we demonstrate that DI-\nCOREprovides 1-2% F1 gains while using 15-55x\nfewer inference tokens relative to strong thinking-\nbased models and chain-of-thought (CoT), high-\nlighting the significance of our proposed divergent-\nconvergent reasoning.\nIn summary, we make the following contribu-\ntions: (1) We propose Dreamer, introducing di-\nThere should not be any demonstration in times of war.Event Type: DemonstrateEvent Type: AttackFigure 2: Illustration example for the task of Event\nDetection. Here, the blue box is the input sentence, and\nthe green boxes are the event mentions. The underlined\nwords indicate the event triggers.\nvergent reasoning to improve event coverage. (2)\nWe develop Grounder, performing convergent rea-\nsoning to align free-form predictions to the event\nontology. (3) We design FSM-guided decoding\nto enforce task-specific structure during inference.\nThrough extensive evaluations across six datasets,\nfive domains, and nine LLMs, we demonstrate\nthe generalizability and efficacy of DICORE, es-\ntablishing it as a robust zero-shot ED framework.\nWe will release the code at https://github.com/\nPlusLabNLP/DiCoRe .\n2 Background and Related Works\nEvent Detection (ED) is the task of identifying\nevent mentions from input text/document Xbased\non a pre-defined ontology (Sundheim, 1992; Gr-\nishman and Sundheim, 1996; Doddington et al.,\n2004). We follow previous works (Doddington\net al., 2004) to define an event as something that\nhappens or describes a change of state. Each event\nis labeled by an event type eand the list of pre-\ndefined event types constitutes an event ontology\nE. An event trigger tis defined as the most distinc-\ntive word/phrase that indicates the event’s presence\nin the text X. The trigger-event type pair (t, e)is\njointly referred to as the event mention . The extrac-\ntion of trigger words from the text and classifying\nthem into one or more event types from the event\nontology is the task of Event Detection, described\nbyfbelow.\n[(e1, t1), ...(en, tn)] =f(X;E)\nWe provide an illustration of the task in Figure 2,\nwherein demonstration andwarindicate the men-\ntions of Demonstrate andAttack events, respec-\ntively, in the sentence.\nEvent Detection: Traditionally, ACE05 (Dod-\ndington et al., 2004) and ERE (Song et al., 2015)\nhave been traditionally utilized for developing\nvarious sequence-tagging (Wadden et al., 2019;\nHsu et al., 2023a) and generative (Li et al., 2021;\nHsu et al., 2023b) models. However, procuring\n2\n--- Page 3 ---\nexpert-annotated data in specialized domains like\nbiomedicine, law, cybersecurity, etc. is an ex-\npensive and labor-intensive task, leading to explo-\nrations in zero-shot and low-resource ED.\nZero-shot Event Detection: Recently, various\ndiverse datasets such as MA VEN (Wang et al.,\n2020), FewEvent (Deng et al., 2019), GENEV A\n(Parekh et al., 2023) in general domain, GE-\nNIA2011 (Kim et al., 2011), GENIA2013 (Kim\net al., 2013) in biomedical, CASIE (Satyapanich\net al., 2020) in cybersecurity, PHEE (Sun et al.,\n2022) in pharmacovigilance, SPEED (Parekh et al.,\n2024b), SPEED++ (Parekh et al., 2024a) in epi-\ndemiology, etc. have been developed. To explore\ngeneralizability across these domains/datasets, ini-\ntial works posed ED as a question-answering (Du\nand Cardie, 2020) or machine-reading comprehen-\nsion problem (Liu et al., 2020). Various works\nexplored transfer and joint learning across various\nIE tasks to build more universal IE models (Lu\net al., 2022; Fei et al., 2023; Li et al., 2024). Some\nworks have explored posing ED as a generative\ntext-to-text approach with event-based templates\n(Lu et al., 2021; Li et al., 2021; Hsu et al., 2022),\neven for zero-shot cross-lingual transfer (Huang\net al., 2022; Parekh et al., 2024b). However, these\nworks require source data training for zero-shot\ntransfer, limiting their utility. Recent works have\nalso explored the utility of zero-shot prompting\nwith LLMs - concluding their sub-par performance\n(Gao et al., 2023; Li et al., 2023a). Other works\nhave explored utilizing LLMs for data generation\n(Ma et al., 2024; Zhang et al., 2024b; Parekh et al.,\n2025) to aid better generalizability. In our work,\nwe focus on improving LLMs’ zero-shot task gen-\neralizability to ED without any model fine-tuning.\nUnconstraining LLMs for Better Reasoning:\nLLMs show immense language understanding and\ngeneration capabilities, but they need instructions\nand constraints to aid in meaningful human tasks\n(Ouyang et al., 2022b). However, imposing con-\nstraints also reduces LLM reasoning capabilities\n(Tam et al., 2024; Banerjee et al., 2025). To this\nend, works have explored constrained decoding by\naltering the output probability distribution (Willard\nand Louf, 2023; Netz et al., 2024; Zhang et al.,\n2024a). Some works explore grammar-aligned de-\ncoding strategies (Geng et al., 2023; Park et al.,\n2024). However, such strict enforcement has been\nshown to hurt LLMs’ generations. Recently, Tam\net al. (2024) explored better prompt design on mathreasoning to unburden the constraints on the LLM.\nWith similar inspiration, we explore decoupling\nLLMs from constraints to improve reasoning for\nIE tasks, specifically Event Detection, in our work.\n3 Methodology\nIn our work, we frame ED through a generative\noutlook fgen(Paolini et al., 2021; Huang et al.,\n2022) as they provide stronger zero-shot perfor-\nmance (Hsu et al., 2022) and are better suited for\nLLMs. We consider a structured list of tuples as\nthe output generation as they provide stronger per-\nformance (§ C.1) and are easy to parse (Wang et al.,\n2023). However, these considerations introduce\nconstraints (encoded as task instructions in LLM\nprompt) like the predicted event is from the pro-\nvided list, the predicted trigger phrase is in the\ninput text, and the output generation follows the\nJSON format, as technically described below.\nY=fgen(X;E) where\nY= “[(e1, t1), ...(en, tn)]” (1)\nt∈X ∀t∈ {t1, ...t n} (2)\ne∈ E ∀ e∈ {e1, ...e n} (3)\nWe argue that these structured constraints inher-\nent to ED (Eq. 1-3) increase the cognitive load\non LLMs, making reasoning more difficult (Tam\net al., 2024). This is one of the contributing fac-\ntors to LLMs’ subpar performance for ED (Huang\net al., 2024). To address this, we propose DICORE,\na novel pipeline that decouples and reduces con-\nstraint adherence through divergent open-ended\ndiscovery, convergent alignment, and constrained\ndecoding. DICOREis lightweight, does not require\nadditional training, and can be seamlessly applied\nto any LLM. Specifically, DICOREcomprises a\nthree-stage pipeline of a Dreamer-Grounder-Judge,\nas illustrated in Figure 3, and described below.\n3.1 Dreamer\nOur first component, Dreamer aka Divergent open-\nended thinker , is designed to promote open-ended\ndivergent discovery, encouraging the LLM to\nachieve high recall by freely identifying potential\nevents without being constrained by the predefined\nevent ontology. Specifically, the Dreamer compo-\nnentfdremoves the task-specific event constraint\n(Eq. 3), relaxes the trigger constraint (Eq. 2), and\nprompts the LLM to extract event mentions directly\nfrom the input sentence Xas\nYd= “[(e′\n1, t1), ...(e′\nn, tn)]” = fd(X)\n3\n--- Page 4 ---\nDREAMERDivergent open-ended thinkerGROUNDERConvergent constraint alignerJUDGEHigh precision verifierCabinet has advised people who experience symptoms of fever, flu, or rashes, to report to their nearest facility\nLLM[(“Advice”, “advised”),(“symptom reporting”, “experience”), (“disease reporting”, “report”)]VerbsFever flu …cabinetreportingEvent Ontology:Control – collective efforts to impede the spread of a pandemic…\nLLM[(“Control”, “advised”),(“Symptom”, “experience”), (“Control”, “report”)]\nLLM\nFSM[(“Control”, “advised”),(“Symptom”, “experience”)]Control = collective efforts to impede the spread of a pandemicCabinet has advised people who experience …Figure 3: Illustration of our DICOREpipeline. First, the Dreamer reasons divergently in an open-ended uncon-\nstrained manner about all potential events in the text and generates free-form event names. Next, the Grounder reads\nthe event ontology and convergently grounds the free-form predictions to one of the event types. It uses finite-state\nmachine (FSM) guided constrained decoding to enforce task-specific constraints. Finally, the Judge evaluates each\nprediction and verifies its validity at a holistic scale.\nwhere each e′\niis a free-form LLM-generated nat-\nural language event name. Notably, e′\nineed not\nadhere to the event ontology E. We provide an\nillustration of the LLM prompt in Figure 5.\nBy removing explicit references to the event on-\ntology, the instructions become less restrictive and\nmore semantically intuitive for the LLM. This sim-\nplification enables the model to divergently reason\non the underlying semantics of the text, rather than\nrigidly aligning with predefined categories. This\nopen-ended setup encourages broader event dis-\ncovery, improving recall by allowing the model to\nidentify diverse or implicit event types. Though it\nmay lower precision, it produces a rich candidate\nset for downstream refinement.\n3.2 Grounder\nThe second component, Grounder aka Conver-\ngent constraint aligner , convergently aligns the\nDreamer’s open-ended predictions Ydwith the\nclosed, task-specific event ontology E, while filter-\ning the events that are not mappable. Technically,\nthe Grounder component fginfuses the original\ntask-specific constraints into the prompt to gener-\nate the grounded event mentions Ygas\nYg= “[(e1, t1), ...(em, tm)]” = fg(X;E, Yd)\nAn illustration of the Grounder prompt and ex-\npected output is shown in Figure 6.\nFSM-guided decoding for constraint enforce-\nment: To reduce the burden of constraint-\nfollowing on the LLM and ensure strict adherence\nJSONFormatEmpty / Non-zero ListSelect Event TypeSelect TriggerEnd / Next Event MentionEndState[(“[]𝑒1𝑒|𝜀|. . .𝑤1𝑤|𝑋|. . .”, “”), (“”)]AB\nCDFigure 4: Finite state machine (FSM) illustration\nfor guiding decoding to enforce constraints. Here\ne1, . . . , e |E|∈ E represent all the possible event types\nandw1, . . . , w |X|∈Xrepresent the atomized phrases\nin the sentence X.\nto the task format, we incorporate a constrained de-\ncoding mechanism guided by a finite-state machine\n(FSM). Inspired by recent work (Willard and Louf,\n2023; Zhang et al., 2024a), the FSM is designed to\nencode key constraints (Eq. 1–3) within the decod-\ning process. We construct and demonstrate an FSM\nto encode constraints for our ED task in Figure 4.\nThe states of the FSM represent possible deci-\nsion points (e.g., choice of event type, choice of\ntrigger word, etc.). The state transitions denote pos-\nsible LLM generations/choices (e.g., list of event\ntypes in the ontology E) and are guided by their\ncorresponding LLM generation probabilities. In\nour FSM in Figure 4, we first decide if there is any\nevent or if it is an event-free sentence (state A).\nNext, we decide our first event type e∈ E(state B)\n4\n--- Page 5 ---\nLLMPrompt MA VEN (168) FewEvent (100) ACE (33) GENIA (9) SPEED (7) CASIE (5) Average\nStyle TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI\nLlama3-8BChatIE 33.7 7.3 13.8 20.8 10.2 27.6 30.6 24.9 46.8 8.6 3.2 11.3 28.4 15.5 43.3 10.8 3.6 20.4 22.2 10.8 27.2\nGEE 19.1 1.9 6.8 11.7 5.9 14.0 30.0 21.3 27.4 25.4 15.8 26.7 35.9 27.7 38.7 11.5 9.2 45.8 22.3 13.6 26.6\nDEE 33.7 6.0 9.2 21.1 10.6 17.8 26.9 19.8 36.1 25.3 16.9 32.5 29.1 20.3 39.2 8.7 7.6 48.3 24.1 13.5 30.5\nBD 54.5 10.7 12.3 22.3 9.9 15.0 34.2 19.5 31.4 28.1 11.2 30.2 35.3 24.7 37.2 16.8 7.4 44.5 31.9 13.9 28.4\nMD 45.9 2.8 4.0 25.2 9.5 15.2 35.6 22.4 30.1 22.8 15.3 25.4 34.9 27.8 42.4 10.3 8.8 47.9 29.1 14.4 27.5\nMS 46.2 10.3 11.2 20.2 10.2 17.0 26.7 17.6 23.1 27.6 19.7 30.5 34.1 27.3 40.6 11.9 10.3 48.3 27.8 15.9 28.4\nDICORE53.5 14.4 17.4 26.1 15.7 25.0 40.3 36.3 47.9 25.8 15.4 30.0 35.5 23.6 42.4 18.5 16.8 58.8 33.3 20.4 36.9\nLlama3-70BChatIE 47.9 19.8 24.8 33.3 20.8 40.6 45.5 37.9 47.0 14.6 6.4 17.3 41.8 31.0 50.9 12.9 10.2 48.9 32.7 21.0 38.3\nGEE 28.3 15.7 17.5 26.2 16.3 31.1 47.0 42.3 52.2 32.5 24.2 38.5 43.7 34.7 46.0 11.1 10.7 43.2 31.5 24.0 38.1\nDEE 60.8 14.8 16.4 34.0 21.3 33.6 47.4 38.3 45.4 39.2 30.5 46.0 41.7 32.2 44.7 16.6 16.4 63.1 40.0 25.6 41.5\nBD 63.0 13.9 15.2 34.0 14.5 22.6 49.1 36.6 41.7 39.4 26.5 45.4 49.2 33.6 45.7 16.5 11.7 48.8 41.9 22.8 36.6\nMD 63.5 14.2 14.7 34.0 20.9 32.6 51.2 40.2 46.8 36.8 28.9 43.0 45.4 36.8 49.0 13.9 13.7 64.4 40.8 25.8 41.8\nMS 33.9 21.6 22.3 35.3 24.9 39.9 49.9 42.8 46.9 37.4 31.0 45.0 43.8 35.5 49.6 14.0 14.0 59.5 35.7 28.3 43.9\nDICORE62.5 27.8 30.6 40.4 25.1 36.1 57.2 49.5 55.1 38.6 31.0 48.5 45.0 36.5 51.8 17.3 16.6 66.6 43.5 32.8 48.1\nTable 1: Main results comparing the zero-shot ED performance of our proposed DICOREwith all other baselines\nfor the Llama3-8B-Instruct and Llama3-70B-Instruct LLMs. TI: Trigger Identification, TC: Trigger Classification,\nEI: Event Identification. bold = best performance. (XX) = number of distinct event types.\nand then the corresponding trigger w∈X(state\nC). Finally, we decide if we want to list another\nevent mention or end the predictions (state D). To\nensure that the generations are natural, the FSM\nstates are partitioned in alignment with the LLM\ntokenizer, i.e., the states are chosen such that the\nsequence of transition tokens is the most probable\ntokenization of the output text Yg.\nAt each FSM state, we generate using the LLM\nwhile constraining its output space by zeroing out\nthe probability mass of all tokens not correspond-\ning to valid state transitions. This enforces that the\nLLM can only generate tokens permitted by the\nFSM at each step, effectively guiding the genera-\ntion process according to the task-specific grammar.\nAs a result, all structural constraints are directly en-\nforced during decoding, ensuring well-formed and\nontology-compliant outputs.\n3.3 Judge\nThe final component of our pipeline, Judge aka\nHigh precision verifier , serves to ensure each pre-\ndicted event mention adheres to the original task\ninstructions. Specifically, for each candidate pair\n(ei, ti), the Judge fjevaluates the hypothesis that\nthe trigger tiexpresses the event type eiin the con-\ntext of the input sentence Xas\nyi\nj= “Y es/No ” =fj(ei, ti, X;E)\nAll predictions with yi\nj= “Y es”are accepted into\nthe final output, while the others are rejected. We\nprovide an illustration of the prompt in Figure 7.\nThis verification step plays a crucial role in en-\nsuring the semantic validity and task alignment ofpredictions at a holistic level. By filtering out irrel-\nevant or uncertain outputs, the Judge substantially\nimproves the precision of the overall system with-\nout requiring additional supervision or training.\n4 Experimental Setup\nIn this section, we describe our experimental setup\ncomprising the datasets, baselines, evaluation met-\nrics, and implementation details. Additional setup\nand implementation details are provided in § B.\nDatasets: We benchmark our model across six\nED datasets spanning five diverse domains, listed\nas: (1) MA VEN (Wang et al., 2020) and (2) Few-\nEvent (Deng et al., 2019) from the general do-\nmain, (3) ACE (Doddington et al., 2004) from\nthe news domain, (4) GENIA (Kim et al., 2011),\nfrom the biomedical domain, (5) SPEED (Parekh\net al., 2024b), from the epidemiological/social me-\ndia domain, (6) CASIE (Satyapanich et al., 2020),\nfrom the cybersecurity domain. To avoid any dis-\ntributional biases, following TextEE (Huang et al.,\n2024), we uniformly sample 250 datapoints from\nthe complete dataset for evaluation.1\nBaselines: We consider two major baselines, de-\nscribed below: (1) Multi-event Direct (MD) (Gao\net al., 2023) directly prompts the LLM to provide\nthe final output in a single pass, and (2) Multi-event\nStaged (MS) (Parekh et al., 2025) decomposes the\ntask into two stages, where the first stage identifies\nthe event and the second stage extracts the cor-\nresponding triggers. We also compare with other\nworks like: (3) Binary-event Direct (BD) (Lyu et al.,\n1We will release the test splits for reproduction.\n5\n--- Page 6 ---\nLLMPrompt MA VEN (168) FewEvent (100) ACE (33) GENIA (9) SPEED (7) CASIE (5) Average\nStyle TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI\nQwen2.5-14BMD 53.0 17.6 20.9 28.8 21.1 34.2 28.3 24.5 42.1 24.8 18.8 26.7 37.7 33.0 51.2 15.8 15.8 61.5 31.4 21.8 39.5\nMS 46.5 20.8 24.6 24.8 18.9 32.1 33.6 26.3 32.5 25.4 19.2 27.7 38.9 34.3 46.1 16.3 16.1 54.5 30.9 22.6 36.2\nDICORE53.1 23.3 27.6 29.7 19.3 30.4 38.4 37.7 48.8 29.9 22.6 38.6 42.9 35.3 46.5 19.7 19.5 58.8 35.8 26.1 41.8\nQwen2.5-72BMD 49.4 21.6 24.1 17.0 12.3 21.0 28.8 25.8 30.3 30.5 27.0 36.3 41.4 37.4 45.4 11.0 10.4 57.9 29.7 22.4 35.8\nMS 39.9 23.6 25.4 25.0 21.0 34.2 42.5 40.4 42.5 26.7 23.6 34.1 40.6 35.5 45.2 10.5 10.5 49.1 30.9 25.8 38.4\nDICORE54.1 27.5 30.2 30.8 22.3 32.9 46.8 44.8 47.8 33.6 29.8 43.9 40.6 34.7 41.4 15.9 15.8 59.3 37.0 29.2 42.6\nGPT3.5-turboMD 50.9 17.4 20.4 23.2 14.6 27.0 40.9 36.2 42.5 27.0 19.9 31.4 36.5 30.6 41.8 10.0 9.9 51.1 31.4 21.4 35.7\nMS 48.2 15.5 17.2 23.7 15.9 29.8 40.7 37.4 42.3 23.2 19.0 26.3 33.0 23.7 35.5 7.7 7.1 44.4 29.4 19.8 32.6\nDICORE48.1 21.6 26.1 25.3 15.6 31.1 41.7 41.7 48.9 26.2 19.5 36.3 32.4 27.2 49.0 11.4 10.6 55.7 30.9 22.7 41.2\nGPT4oMD 61.8 28.9 31.9 30.6 23.9 35.4 52.3 52.3 52.3 41.0 36.5 49.5 44.1 40.2 48.0 10.1 10.1 55.7 40.0 32.0 45.5\nMS 49.4 30.8 33.3 25.6 20.6 32.2 36.2 36.2 38.3 36.6 33.2 45.0 45.7 40.4 50.1 13.4 13.4 46.9 34.5 29.1 41.0\nDICORE58.5 32.2 35.6 36.1 28.4 38.5 54.9 54.9 56.6 40.7 35.4 51.2 43.3 37.3 46.1 16.7 16.7 58.8 41.7 34.2 47.8\nTable 2: Generalization results for zero-shot ED performance comparing DICOREwith the best baselines for four\nother LLMs of Qwen2.5-14B-Instruct, Qwen2.5-72B-Instruct, GPT3.5-turbo, and GPT4o. bold = best performance.\n(XX) = number of distinct event types.\n2021; Li et al., 2023c) prompts the LLM to do bi-\nnary classification for each event, (4) Decompose-\nEnrich-Extract (DEE) (Shiri et al., 2024) utilizes\ninstruction enrichment with schema information\nfor ED, (5) GuidelineEE (GEE) (Srivastava et al.,\n2025), similar to Code4Struct (Wang et al., 2023),\nconverts ED into a code-generation problem using\nPython classes and instantiations, and (6) ChatIE\n(Wei et al., 2023) decomposes ED via multi-turn\nconversations. We ensure consistent, structured out-\nputs for each baseline to maintain fair comparisons\n(analysis in § C.1). Furthermore, we add the Judge\ncomponent to each baseline, if not already present,\nto ensure robust benchmarking of D ICORE.\nBase LLMs: We use the following LLMs for our\nbase experiments: Llama3-8B-Instruct and Llama3-\n70b-Instruct from the Llama3 family (Dubey et al.,\n2024) and Qwen2.5-14B-Instruct; Qwen2.5-72B-\nInstruct from the Qwen2.5 (Yang et al., 2024) LLM\nfamily; and GPT3.5-turbo and GPT-4o (Brown\net al., 2020; OpenAI, 2023) from OpenAI.\nEvaluation Metrics: Following Ahn (2006);\nParekh et al. (2025) we report the F1 scores for\nthe following three metrics: (1) Trigger Identifica-\ntion (TI) - correct identification of triggers, and (2)\nEvent Identification (EI) - correct classification of\nevent types, and (3) Trigger Classification (TC) -\ncorrect identification of the trigger-event pair (event\nmention). To maintain consistency with traditional\nspan-based evaluations, we used string matching to\nmap the generated outputs to input spans.\nImplementation Details: We use TextEE\n(Huang et al., 2024) for our benchmarking,\ndatasets, and evaluation setup. To restrict LLM’sgeneration choices for the FSM-guided constrained\ndecoding, we utilize Outlines (Willard and Louf,\n2023) over vLLM inference (Kwon et al., 2023).\nWe use Curator (Marten et al., 2025) for querying\nthe GPT family LLMs. We deploy a temperature\nof 0.4 and top-p of 0.9 for decoding. We report\nthe averaged results over three runs for robust\nbenchmarking.\n5 Results and Analysis\nIn this section, we provide our main results and\nfindings, and later provide supporting evidence\nthrough our analyses.\n5.1 Main Results\nWe present the main zero-shot results for all base-\nlines on the six datasets for Llama3 LLMs in Ta-\nble 1. As seen from the average results (last three\ncolumns), DICOREperforms the best, surpassing\nthe best baseline of multi-event staged (MS) by a\nsignificant margin of 5.5-8% TI, 4-8.5% EI, and 4-\n5% TC. The performance disparity across different\ntask decomposition methods of ChatIE, MS, and\nDICOREhighlights how our divergent-convergent\ndecomposition of Dreamer-Grounder provides a\nstronger inductive bias. Other baselines perform\nrelatively better on the fewer-event and simpler\ndatasets like GENIA/SPEED, but DICOREshows\nstrong dominance on the larger event datasets like\nMA VEN/FewEvent/ACE.\nGeneralization across LLMs: To demonstrate\nthe generalizability of DICORE, we benchmark\nit with the top-performing baselines on four addi-\ntional LLMs from the Qwen and GPT families and\nshow our results in Table 2. We note how DICORE\n6\n--- Page 7 ---\nModel Setting Average F1\nTI TC EI\nTest on GENIA, SPEED, CASIE\nGOLLIE-7B 6.0 5.3 15.3\nGOLLIE-34B 15.6 11.7 29.4\nLlama3-8B D ICORE 26.6 18.6 43.7\nLlama3-70B D ICORE 33.6 28.0 55.6\nTest on all but ACE dataset\nACE-trained DEGREE 20.9 11.0 21.3\nLlama3-8B D ICORE 31.9 17.2 34.7\nLlama3-70B D ICORE 40.8 27.4 46.7\nTest on all but MA VEN dataset\nMA VEN-trained DEGREE 31.8 25.0 38.6\nLlama3-8B D ICORE 29.2 21.6 40.8\nLlama3-70B D ICORE 39.7 31.7 51.6\nTable 3: Comparison of pure zero-shot DICOREwith\nfine-tuned transfer-learning baselines. Underline indi-\ncates scenarios of D ICOREimprovements.\nperforms the best across all LLMs with an overall\naverage improvement of 5.5% TI, 6.5% EI, 4% TC\nover the multievent-staged baseline and 3.3% TI,\n5.4%, 4.6% TC over the multievent-direct base-\nline. Across different LLMs, we note the strongest\nperformance on GPT4o, followed by Llama3-70B-\nInstruct and Qwen2.5-72B, indicating how more\nparameters help better reasoning with D ICORE.\n5.2 Comparison with Fine-tuned\nTransfer-learning Methods\nVarious works utilize transfer-learning and univer-\nsal Information Extraction (IE) training for zero-\nshot cross-dataset ED (Cai et al., 2024; Li et al.,\n2024). These works train on selected IE datasets\nand show performance on unseen IE datasets. We\nprovide a comparison of DICOREwith two such\ntransfer-learning approaches: (1) DEGREE (Hsu\net al., 2022), a generative framework utilizing text-\nbased event templates to generalize, (2) GOLLIE\n(Sainz et al., 2023), a universal IE framework, fine-\ntuning LLMs on various IE instruction datasets.\nFor DEGREE, we consider two versions where the\nsource data is ACE and MA VEN, respectively. For\nGOLLIE, we consider the fine-tuned GOLLIE-7B\nand GOLLIE-34B models. We provide the aver-\naged results across target datasets (not included in\nthe source data) in Table 3, with detailed results in\n§ C.2. Through these results, we demonstrate how,\ndespite no fine-tuning, DICOREconsistently out-\nperforms the fine-tuned transfer-learning baselines\nacross all settings. On average, DICOREimproves\nby 3-10% F1 using Llama3-8B-Instruct and 10-Base LLM Prompt Average F1\nStyle TI TC EI\nChain-of-thought Baselines\nLlama3-8B MD + CoT 25.0 13.5 27.1\nLlama3-8B MS + CoT 28.4 17.6 31.9\nLlama3-70B MD + CoT 41.0 30.9 48.0\nLlama3-70B MS + CoT 40.5 31.6 47.1\nQwen2.5-72B MD + CoT 34.9 27.1 43.6\nQwen2.5-72B MS + CoT 36.2 28.8 40.8\nThinking-based model Baselines\nDS-Qwen-32B MD 39.2 30.0 46.3\nDS-Qwen-32B MS 39.5 30.4 45.2\nDS-Llama3-70B MD 29.0 23.3 36.1\nDS-Llama3-70B MS 33.3 27.0 37.8\nO1-mini MD 40.2 32.5 44.7\nDICOREbase model results\nLlama3-8B D ICORE 33.3 20.4 36.9\nLlama3-70B D ICORE 43.5 32.8 48.1\nQwen2.5-72B D ICORE 37.0 29.2 42.6\nGPT4o D ICORE 41.7 34.2 47.8\nDICOREimprovements with reasoning\nLlama3-8B D ICORE+ CoT 33.1 21.1 36.2\nLlama3-70B D ICORE+ CoT 43.0 33.1 49.8\nQwen2.5-72B D ICORE+ CoT 37.0 29.1 43.5\nDS-Qwen-32B D ICORE 43.1 33.3 49.5\nDS-Llama3-70B D ICORE 41.4 33.0 48.3\nTable 4: Comparison of DICOREwith reasoning-based\nbaselines like Chain-of-thought (CoT) and thinking-\nbased models. Underline indicates DICOREimprove-\nments over reasoning baselines.\n22% F1 using Llama3-70B-Instruct and GPT4o.\n5.3 Comparison with Reasoning baselines\nReasoning by verbalizing thoughts using addi-\ntional tokens has commonly helped improve perfor-\nmance across a wide range of tasks (Kojima et al.,\n2022; Latif et al., 2024). We evaluate the utility\nof reasoning, specifically Chain-of-thought (CoT)\n(Wei et al., 2022), along with thinking-based mod-\nels like Deepseek-R1-Distilled-Qwen-32B (DS-\nQwen-32B), Deepseek-R1-Distilled-Llama3-70B\n(DS-Llama3-70B) (DeepSeek-AI et al., 2025) and\nO1-mini (Jaech et al., 2024) on our task of zero-\nshot ED in Table 4 (complete results in § C.3). We\ndemonstrate how the baselines improve with addi-\ntional reasoning; however, DICOREwith the base\nmodels (Llama3-70B) consistently outperforms all\nthese reasoning baselines (even O1-mini) while us-\ning 15-55x fewer tokens on average (§ C.3). We\nalso show how our method is complementary to\nreasoning by demonstrating further improvements\nup to 1-2% F1 using reasoning with D ICORE.\n7\n--- Page 8 ---\nSentence Best Baseline Dreamer Grounder Judge\nPrediction Prediction Prediction Prediction\ncass apd ra gave birth to\nher first daughter.[(\"Life:Be-Born\" ,\n\"gave\" )][(\"Birth\", \"gave\"),\n(\"Birth\", \"birth\")][(\"Life:Be-Born\",\n\"birth\")][(\"Life:Be-Born\",\n\"birth\" )]\nAfter passing the island,\nthe hurricane turned to\nthe northeast, and be-\ncame extratropical on\nSeptember 8, before dis-\nsipating two days later.[(\"Change\",\n\"turned\" ),\n(\"Change\",\n\"became\" ), (\"Dis-\nsipating\" ,\"dissi-\npating\" )][(\"Movement\",\n\"turned\"), (\"Tran-\nsition\", \"became\"),\n(\"Dissipation\", \"dissi-\npating\")][(\"Change_event_time\",\n\"turned\"), (\"Becom-\ning_a_member\",\n\"became\"), (\"Disper-\nsal\", \"dissipating\")][(\"Dispersal\",\n\"dissipating\" )]\nCovid-19 has led to so-\ncial distancing, but we\ncan still be together\nthrough the quarantine\nwith online gaming![] [(\"Social_Distancing\",\n\"distancing\"), (\"Quar-\nantine\", \"quarantine\"),\n(\"Gaming\", \"gam-\ning\")][(\"prevent\", \"dis-\ntancing\"), (\"control\",\n\"quarantine\")][(\"prevent\",\n\"distancing\") ,\n(\"control\", \"quar-\nantine\" )]\nTable 5: Qualitative examples comparing DICORE’s predictions (per component) with the best baseline. We\nhighlight the correct predictions in green and incorrect ones in red.\nComponent TI TC\nP R F P R F\nLlama3-8B-Instruct\nDreamer 8.5 64.3 15.0 0.0 0.0 0.0\n+ Grounder 20.4 47.9 28.6 15.5 37.1 21.9\n+ FSM Decoding 22.3 56.8 32.1 16.2 42.3 23.4\n+ Judge 41.8 39.0 40.3 37.5 35.2 36.3\nMD Baseline 48.4 28.2 35.6 30.2 17.8 22.4\nMS Baseline 22.0 33.8 26.7 14.4 22.5 17.6\nLlama3-70B-Instruct\nDreamer 15.5 77.5 25.8 0.0 0.0 0.0\n+ Grounder 28.6 65.7 40.4 22.5 53.4 31.8\n+ FSM Decoding 32.3 66.7 43.5 26.2 54.0 35.3\n+ Judge 52.8 62.5 57.2 45.7 54.0 49.5\nMD Baseline 57.2 46.5 51.2 44.0 37.1 40.2\nMS Baseline 66.4 39.9 49.9 57.0 34.3 42.8\nTable 6: Ablation Study on the ACE dataset highlighting\nthe significance and contribution of each component of\nDICORE. P: Precision, R: Recall, F: F1 score.\n5.4 Ablation Study\nTo demonstrate the role of each component of our\npipeline, we ablate and show the model perfor-\nmance as we add each component in DICOREfor\nthe ACE dataset for Llama3-8B and Llama3-70B\nLLMs in Table 6. For reference, we also show the\nprecision/recall splits of the baselines. Dreamer\nachieves a high recall for TI (albeit a low preci-\nsion) - demonstrating the utility of divergent un-\nconstrained reasoning. Grounder helps align the\npredictions, causing a slight drop in recall while im-\nproving the precision. FSM Decoding helps largely\nimprove the recall for Llama3-8B-Instruct by im-\nproving the mapping, and precision for Llama3-\n70B-Instruct by fixing any constraint violations.Finally, Judge largely boosts the precision of the\nmodel. Analysis of the baselines reveals that they\nare conservative, making a low number of high-\nprecision predictions. In comparison, DICORE\nmakes many more predictions, largely improving\nrecall while maintaining reasonably high precision.\nQualitative Study: We provide some qualitative\nexamples for each component of DICORE, while\ncomparing the best baseline across the datasets in\nTable 5 (more examples in § D). We see how the\nbest baseline often reasons incorrectly, leading to\nprecision loss, or remains conservative, predicting\nnothing, leading to recall errors. The split across\nthe three components shows how Dreamer gen-\nerates many plausible event mentions, Grounder\naligns and removes some, while Judge verifies and\nfilters irrelevant ones. These examples provide the\ninternal workings of DICORE, highlighting the sig-\nnificance of divergent-convergent reasoning.\n6 Conclusion and Future Work\nIn our work, we introduce DICORE, a novel\ndivergent-convergent reasoning pipeline of\nDreamer-Grounder-Judge, aimed at decoupling the\nLLM from task-specific constraints, and indirectly\nbetter exploiting LLMs’ reasoning. Through\nexperimentation on six ED datasets from five\ndomains across nine LLMs, we confirm how\nDICOREprovides a stronger inductive bias, im-\nproving over other zero-shot baselines, fine-tuned\ntransfer learning methods, and reasoning-focused\napproaches. Future works can explore this\nparadigm on broader tasks and study to better elicit\ndivergent-convergent reasoning.\n8\n--- Page 9 ---\nLimitations\nIn our work, we focus on improving zero-shot LLM\ninference for Event Detection. This work is easily\nextendable to other low-resource settings as well\nas other Information Extraction (IE) tasks - but\nwe leave these for future explorations. To keep\nexperimentation consistent with prior works, we\nutilized/sampled 250 datapoints from each dataset\nas our test set. If working with a different data\nsplit, one might get different absolute model per-\nformance, but we believe the general trends should\nremain the same. Finally, there are various lines\nof work on improving the use of retrieval to select\ngood in-context examples, or teaching the LLM\nto learn the schema. We believe these works are\northogonal and complementary to our work, and\nwe do not compare/include them in our study.\nEthical Considerations\nOur work focuses on using LLMs through the in-\nductive bias of our method DICORE. Since we do\nnot train the LLM, there could be inherent biases in\nthe LLM that can crop up when using our pipeline.\nWe do not study or provide methods to mitigate\nsuch biases, as it’s not in the scope of our work.\nWe would like to acknowledge that we used AI\nassistants and chatbots for writing some parts of the\npaper, helping with coding up plots, and searching\nfor related works. For each application, a human\nexpert verified to ensure we do not add any spuri-\nous/harmful content.\nReferences\nDavid Ahn. 2006. The stages of event extraction. In\nProceedings of the Workshop on Annotating and Rea-\nsoning about Time and Events , pages 1–8, Sydney,\nAustralia. Association for Computational Linguistics.\nDebangshu Banerjee, Tarun Suresh, Shubham Ugare,\nSasa Misailovic, and Gagandeep Singh. 2025.\nCRANE: reasoning with constrained LLM genera-\ntion. CoRR , abs/2502.09061.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, and 12 others. 2020. Lan-\nguage models are few-shot learners. CoRR ,\nabs/2005.14165.Zefan Cai, Po-Nien Kung, Ashima Suvarna, Mingyu\nMa, Hritik Bansal, Baobao Chang, P. Jeffrey Brant-\ningham, Wei Wang, and Nanyun Peng. 2024. Improv-\ning event definition following for zero-shot event de-\ntection. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 2842–2863, Bangkok,\nThailand. Association for Computational Linguistics.\nRuirui Chen, Chengwei Qin, Weifeng Jiang, and\nDongkyu Choi. 2024. Is a large language model a\ngood annotator for event extraction? In Thirty-Eighth\nAAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applica-\ntions of Artificial Intelligence, IAAI 2024, Fourteenth\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2014, February 20-27, 2024, Van-\ncouver, Canada , pages 17772–17780. AAAI Press.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 81 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. CoRR ,\nabs/2501.12948.\nShumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi\nZhang, Wei Zhang, and Huajun Chen. 2019. Meta-\nlearning with dynamic-memory-based prototypi-\ncal network for few-shot event detection. CoRR ,\nabs/1910.11621.\nGeorge Doddington, Alexis Mitchell, Mark Przybocki,\nLance Ramshaw, Stephanie Strassel, and Ralph\nWeischedel. 2004. The automatic content extrac-\ntion (ACE) program – tasks, data, and evaluation. In\nProceedings of the Fourth International Conference\non Language Resources and Evaluation (LREC‘04) ,\nLisbon, Portugal. European Language Resources As-\nsociation (ELRA).\nXinya Du and Claire Cardie. 2020. Event extraction by\nanswering (almost) natural questions. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 671–683,\nOnline. Association for Computational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, and 82\nothers. 2024. The llama 3 herd of models. CoRR ,\nabs/2407.21783.\nHao Fei, Shengqiong Wu, Jingye Li, Bobo Li, Fei Li,\nLibo Qin, Meishan Zhang, Min Zhang, and Tat-Seng\nChua. 2023. Lasuie: Unifying information extrac-\ntion with latent adaptive structure-aware generative\nlanguage model. CoRR , abs/2304.06248.\nEnrico Francesconi, Simonetta Montemagni, Wim Pe-\nters, and Daniela Tiscornia, editors. 2010. Semantic\n9\n--- Page 10 ---\nProcessing of Legal Texts: Where the Language of\nLaw Meets the Law of Language , volume 6036 of\nLecture Notes in Computer Science . Springer.\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023. Exploring the feasibility of chatgpt for event\nextraction. CoRR , abs/2303.03836.\nSaibo Geng, Martin Josifoski, Maxime Peyrard, and\nRobert West. 2023. Grammar-constrained decoding\nfor structured NLP tasks without finetuning. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pages 10932–\n10952, Singapore. Association for Computational\nLinguistics.\nRalph Grishman and Beth Sundheim. 1996. Message\nUnderstanding Conference- 6: A brief history. In\nCOLING 1996 Volume 1: The 16th International\nConference on Computational Linguistics .\nI-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee,\nScott Miller, Prem Natarajan, Kai-Wei Chang, and\nNanyun Peng. 2022. DEGREE: A data-efficient\ngeneration-based event extraction model. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1890–1908, Seattle, United States. Association for\nComputational Linguistics.\nI-Hung Hsu, Kuan-Hao Huang, Shuning Zhang, Wenxin\nCheng, Prem Natarajan, Kai-Wei Chang, and Nanyun\nPeng. 2023a. TAGPRIME: A unified framework\nfor relational structure extraction. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 12917–12932, Toronto, Canada. Association\nfor Computational Linguistics.\nI-Hung Hsu, Zhiyu Xie, Kuan-Hao Huang, Prem Natara-\njan, and Nanyun Peng. 2023b. AMPERE: AMR-\naware prefix for generation-based event argument\nextraction model. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 10976–\n10993, Toronto, Canada. Association for Computa-\ntional Linguistics.\nKuan-Hao Huang, I-Hung Hsu, Prem Natarajan, Kai-\nWei Chang, and Nanyun Peng. 2022. Multilin-\ngual generative language models for zero-shot cross-\nlingual event argument extraction. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 4633–4646, Dublin, Ireland. Association for\nComputational Linguistics.\nKuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu\nXie, Zixuan Zhang, Prem Natarajan, Kai-Wei Chang,\nNanyun Peng, and Heng Ji. 2024. TextEE: Bench-\nmark, reevaluation, reflections, and future challenges\nin event extraction. In Findings of the Association\nfor Computational Linguistics: ACL 2024 , pages\n12804–12825, Bangkok, Thailand. Association for\nComputational Linguistics.Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney,\nAlex Iftimie, Alex Karpenko, Alex Tachard Passos,\nAlexander Neitz, Alexander Prokofiev, Alexander\nWei, Allison Tam, Ally Bennett, Ananya Kumar, and\n80 others. 2024. Openai o1 system card. CoRR ,\nabs/2412.16720.\nJin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-\nnori Yonezawa. 2011. Overview of Genia event task\nin BioNLP shared task 2011. In Proceedings of\nBioNLP Shared Task 2011 Workshop , pages 7–15,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nJin-Dong Kim, Yue Wang, and Yamamoto Yasunori.\n2013. The Genia event extraction shared task, 2013\nedition - overview. In Proceedings of the BioNLP\nShared Task 2013 Workshop , pages 8–15, Sofia, Bul-\ngaria. Association for Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. CoRR ,\nabs/2205.11916.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-\nlez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-\nory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, SOSP 2023,\nKoblenz, Germany, October 23-26, 2023 , pages 611–\n626. ACM.\nEhsan Latif, Yifan Zhou, Shuchen Guo, Yizhu Gao,\nLehong Shi, Matthew Nayaaba, Gyeong-Geon Lee,\nLiang Zhang, Arne Bewersdorff, Luyang Fang, Xi-\nantong Yang, Huaqin Zhao, Hanqi Jiang, Haoran Lu,\nJiaxi Li, Jichao Yu, Weihang You, Zhengliang Liu,\nVincent Shung Liu, and 8 others. 2024. A systematic\nassessment of openai o1-preview for higher order\nthinking in education. CoRR , abs/2410.21287.\nBo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei\nYe, Wen Zhao, and Shikun Zhang. 2023a. Evaluating\nchatgpt’s information extraction capabilities: An as-\nsessment of performance, explainability, calibration,\nand faithfulness. CoRR , abs/2304.11633.\nDongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu,\nZiyang Chen, Baotian Hu, Aiguo Wu, and Min\nZhang. 2023b. A survey of large language models\nattribution. CoRR , abs/2311.03731.\nSha Li, Heng Ji, and Jiawei Han. 2021. Document-level\nevent argument extraction by conditional generation.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 894–908, Online. Association for Computa-\ntional Linguistics.\n10\n--- Page 11 ---\nSha Li, Qiusi Zhan, Kathryn Conger, Martha Palmer,\nHeng Ji, and Jiawei Han. 2023c. GLEN: General-\npurpose event detection for thousands of types. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , pages\n2823–2838, Singapore. Association for Computa-\ntional Linguistics.\nXiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen,\nZiji Zhang, Yingying Zhuang, Narayanan Sadagopan,\nand Anurag Beniwal. 2025. When thinking fails: The\npitfalls of reasoning for instruction-following in llms.\nCoRR , abs/2505.11423.\nZixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren,\nWenxuan Liu, Miao Su, Yucan Guo, Yantao Liu, Lix-\niang Lixiang, Zhilei Hu, Long Bai, Wei Li, Yidan\nLiu, Pan Yang, Xiaolong Jin, Jiafeng Guo, and Xueqi\nCheng. 2024. KnowCoder: Coding structured knowl-\nedge into LLMs for universal information extraction.\nInProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 8758–8779, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nJian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiaojiang\nLiu. 2020. Event extraction as machine reading com-\nprehension. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 1641–1651, Online. Association\nfor Computational Linguistics.\nYaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong\nTang, Annan Li, Le Sun, Meng Liao, and Shaoyi\nChen. 2021. Text2Event: Controllable sequence-to-\nstructure generation for end-to-end event extraction.\nInProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n2795–2806, Online. Association for Computational\nLinguistics.\nYaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu\nLin, Xianpei Han, Le Sun, and Hua Wu. 2022. Uni-\nfied structure generation for universal information\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 5755–5772, Dublin,\nIreland. Association for Computational Linguistics.\nQing Lyu, Hongming Zhang, Elior Sulem, and Dan\nRoth. 2021. Zero-shot event extraction via transfer\nlearning: Challenges and insights. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers) , pages 322–332, Online.\nAssociation for Computational Linguistics.\nMingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung,\nP. Jeffrey Brantingham, Nanyun Peng, and Wei Wang.\n2024. STAR: boosting low-resource information\nextraction by structure-to-text data generation withlarge language models. In Thirty-Eighth AAAI Con-\nference on Artificial Intelligence, AAAI 2024, Thirty-\nSixth Conference on Innovative Applications of Ar-\ntificial Intelligence, IAAI 2024, Fourteenth Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, EAAI 2014, February 20-27, 2024, Vancouver,\nCanada , pages 18751–18759. AAAI Press.\nRyan* Marten, Trung* Vu, Charlie Cheng-Jie Ji, Kartik\nSharma, Shreyas Pimpalgaonkar, Alex Dimakis, and\nMaheswaran Sathiamoorthy. 2025. Curator: A Tool\nfor Synthetic Data Creation. https://github.com/\nbespokelabsai/curator .\nLukas Netz, Jan Reimer, and Bernhard Rumpe. 2024.\nUsing grammar masking to ensure syntactic validity\nin llm-based modeling tasks. CoRR , abs/2407.06146.\nOpenAI. 2023. GPT-4 technical report. CoRR ,\nabs/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022a.\nTraining language models to follow instructions with\nhuman feedback. CoRR , abs/2203.02155.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022b.\nTraining language models to follow instructions with\nhuman feedback. CoRR , abs/2203.02155.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai,\nCícero Nogueira dos Santos, Bing Xiang, and Ste-\nfano Soatto. 2021. Structured prediction as transla-\ntion between augmented natural languages. In 9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021 . OpenReview.net.\nTanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin\nKim, I-Hung Hsu, Kai-Wei Chang, and Nanyun\nPeng. 2025. FIG: forward-inverse generation for\nlow-resource domain-specific event detection. CoRR ,\nabs/2502.17394.\nTanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-\nWei Chang, and Nanyun Peng. 2023. GENEV A:\nBenchmarking generalizability for event argument\nextraction with hundreds of event types and argument\nroles. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3664–3686, Toronto,\nCanada. Association for Computational Linguistics.\nTanmay Parekh, Jeffrey Kwan, Jiarui Yu, Sparsh Johri,\nHyosang Ahn, Sreya Muppalla, Kai-Wei Chang, Wei\n11\n--- Page 12 ---\nWang, and Nanyun Peng. 2024a. SPEED++: A mul-\ntilingual event extraction framework for epidemic\nprediction and preparedness. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12936–12965, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nTanmay Parekh, Anh Mac, Jiarui Yu, Yuxuan Dong,\nSyed Shahriar, Bonnie Liu, Eric Yang, Kuan-Hao\nHuang, Wei Wang, Nanyun Peng, and Kai-Wei\nChang. 2024b. Event detection from social media\nfor epidemic prediction. In Proceedings of the 2024\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers) ,\npages 5758–5783, Mexico City, Mexico. Association\nfor Computational Linguistics.\nKanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick,\nNadia Polikarpova, and Loris D’Antoni. 2024.\nGrammar-aligned decoding. In Advances in Neural\nInformation Processing Systems 38: Annual Confer-\nence on Neural Information Processing Systems 2024,\nNeurIPS 2024, Vancouver, BC, Canada, December\n10 - 15, 2024 .\nSampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-\nCheol Cho, Junichi Tsujii, and Sophia Ananiadou.\n2012. Event extraction across multiple levels of bio-\nlogical organization. Bioinform. , 28(18):575–581.\nOscar Sainz, Iker García-Ferrero, Rodrigo Agerri,\nOier Lopez de Lacalle, German Rigau, and Eneko\nAgirre. 2023. Gollie: Annotation guidelines im-\nprove zero-shot information-extraction. CoRR ,\nabs/2310.03668.\nTaneeya Satyapanich, Francis Ferraro, and Tim Finin.\n2020. CASIE: extracting cybersecurity event infor-\nmation from text. In The Thirty-Fourth AAAI Con-\nference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020 , pages 8749–8757. AAAI Press.\nFatemeh Shiri, Farhad Moghimifar, Reza Haffari, Yuan-\nFang Li, Van Nguyen, and John Yoo. 2024. De-\ncompose, enrich, and extract! schema-aware event\nextraction using llms. In 27th International Confer-\nence on Information Fusion, FUSION 2024, Venice,\nItaly, July 8-11, 2024 , pages 1–8. IEEE.\nZhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese,\nJustin Mott, Joe Ellis, Jonathan Wright, Seth Kulick,\nNeville Ryant, and Xiaoyi Ma. 2015. From light\nto rich ERE: Annotation of entities, relations, and\nevents. In Proceedings of the 3rd Workshop on\nEVENTS: Definition, Detection, Coreference, and\nRepresentation , pages 89–98, Denver, Colorado. As-\nsociation for Computational Linguistics.Saurabh Srivastava, Sweta Pati, and Ziyu Yao. 2025.\nInstruction-tuning llms for event extraction with an-\nnotation guidelines. CoRR , abs/2502.16377.\nZhaoyue Sun, Jiazheng Li, Gabriele Pergola, Byron\nWallace, Bino John, Nigel Greene, Joseph Kim, and\nYulan He. 2022. PHEE: A dataset for pharmacovigi-\nlance event extraction from text. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing , pages 5571–5587, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nBeth M. Sundheim. 1992. Overview of the fourth Mes-\nsage Understanding Evaluation and Conference. In\nFourth Message Understanding Conference (MUC-\n4): Proceedings of a Conference Held in McLean,\nVirginia, June 16-18, 1992 .\nZhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-\nYen Lin, Hung-yi Lee, and Yun-Nung Chen. 2024.\nLet me speak freely? a study on the impact of format\nrestrictions on large language model performance. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing: Industry\nTrack , pages 1218–1236, Miami, Florida, US. Asso-\nciation for Computational Linguistics.\nHristo Tanev, Jakub Piskorski, and Martin Atkinson.\n2008. Real-time news event extraction for global\ncrisis monitoring. In Natural Language and Infor-\nmation Systems, 13th International Conference on\nApplications of Natural Language to Information Sys-\ntems, NLDB 2008, London, UK, June 24-27, 2008,\nProceedings , volume 5039 of Lecture Notes in Com-\nputer Science , pages 207–218. Springer.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nInProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5784–\n5789, Hong Kong, China. Association for Computa-\ntional Linguistics.\nXiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong\nHan, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin,\nand Jie Zhou. 2020. MA VEN: A Massive General\nDomain Event Detection Dataset. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pages 1652–\n1671, Online. Association for Computational Linguis-\ntics.\nXingyao Wang, Sha Li, and Heng Ji. 2023. Code4Struct:\nCode generation for few-shot event structure predic-\ntion. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3640–3663, Toronto,\nCanada. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\n12\n--- Page 13 ---\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR , abs/2201.11903.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-shot information extraction via\nchatting with chatgpt. CoRR , abs/2302.10205.\nBrandon T. Willard and Rémi Louf. 2023. Efficient\nguided generation for large language models. CoRR ,\nabs/2307.09702.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayi-\nheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Ji-\naxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and\n22 others. 2024. Qwen2.5 technical report. CoRR ,\nabs/2412.15115.\nHonghua Zhang, Po-Nien Kung, Masahiro Yoshida,\nGuy Van den Broeck, and Nanyun Peng. 2024a.\nAdaptable logical control for large language models.\nInAdvances in Neural Information Processing Sys-\ntems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024 .\nWeiyan Zhang, Wanpeng Lu, Jiacheng Wang, Yating\nWang, Lihan Chen, Haiyun Jiang, Jingping Liu, and\nTong Ruan. 2024b. Unexpected phenomenon: LLMs’\nspurious associations in information extraction. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2024 , pages 9176–9190, Bangkok,\nThailand. Association for Computational Linguistics.\n13\n--- Page 14 ---\nA D ICOREPrompts\nWe described our modeling paradigm of divergent-\nconvergent reasoning through the Dreamer-\nGrounder-Judge paradigm in § 3. Here we provide\nsome additional details and also share the prompts\nthat we used for each component.\nDreamer: The Dreamer component induces di-\nvergent thinking, encouraging the model to think\nmore widely. We induce this behavior by removing\nthe event-based constraints from the task instruc-\ntions and adding additional inductive bias to pro-\nvide this encouragement inthe form of additional\ntask instructions asking the model to be super lib-\neral. We provide an illustration of this prompt in\nFigure 5. Specifically, sentences like \"Try to be lib-\neral and increase the coverage as much as possible.\nI will filter and improve the precision in the next\nstep.\" and \"Be very open and output all possible\nevents that are potentially mentioned.\" provide this\nstronger divergent reasoning inductive bias.\nGrounder: The Grounder component aligns the\nopen-ended predictions of the Dreamer with the\nclosed event ontology using convergent reasoning.\nTo this end, we add the various task-specific con-\nstraints in the form of natural language instructions\nas well as use a finite-state machine (FSM) guided\ngeneration to aid with this convergent reasoning.\nHere, we describe the prompt and the inductive\nbiases in it, as illustrated in Figure 6. Specifically,\nwe first add all the verbalized constraints, including\nthe ontology details in the form of event names and\ninformation. To provide more inductive bias, we\nalso add a sentence like \"Be conservative in your\noutputs - If a trigger word cannot be mapped, skip\nthe trigger word. If the mapped event does not\nhappen in the sentence, skip the trigger word.\".\nJudge: The Judge is tasked with the evaluation\nof the prediction to ensure that the trigger word\ntriggers the specific event in the given sentence.\nWe run the Judge for each prediction separately. To\nmake this lightweight, we ensure that the output\nspace is simple \"Yes\" or \"No\" without any explana-\ntion, which makes the parsing easier as well. We\nprovide an illustration of this prompt in Figure 7.\nThis component is very generic and can be easily\napplied to other methods/LLMs as well.\nYou are an event extraction model, looking to extract events from a sentence.This is an event extraction task where the goal is to extract structured events from the text. A structured event contains an event trigger word and an event type.Below is a sentence from which you need to extract the events if any. Only output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...] for each event in the sentence. Try to be liberal and increase the coverage as much as possible. I will filter and improve the precision in the next step. Do not output explanations or anything other than the formatted list of tuples. If there are no events in the sentence, output empty list []. Be very open and output all possible events that are potentially mentioned.People who live in crowded or poorer areas are more likely to test positive for Covid - 19 , according to a ( url ) study of infection in the general population ( url )System PromptUser InstructionsUser QueryFigure 5: Illustration of the prompt utilized for Dreamer.\nTo encourage divergent thinking, we remove event-\nbased constraints from the model instructions. Further-\nmore, we add sentences that encourage the model to be\nliberal and open in its predictions.\nB Additional Experimental Details\nIn § 4, we provided brief details about our experi-\nmental and implementation details. Here, we pro-\nvide more intricate details about our different ex-\nperimental setups and baseline models.\nB.1 Dataset Statistics\nOur experimental setup is a pure zero-shot setup\nwhere we do not use any training data. We pro-\nvide statistics about the evaluation splits of the\ndifferent datasets in Table 7. We follow TextEE\n(Huang et al., 2024) for the evaluation setup and\nconsider a uniform random split of 250 test sam-\nples from each dataset to avoid any train-test split\nbias. Since CASIE is a smaller dataset, we only\nuse 50 test samples for this dataset. The table high-\nlights the domain diversity of the datasets covering\ncommon domains like news and general, while also\nfocusing on technical domains like biomedical and\nepidemiology. The datasets also show variation\nin the density, with ACE, FewEvent, and SPEED\nbeing sparse with 1 event mention/sentence. On\nthe other hand, MA VEN, CASIE, and GENIA are\n14\n--- Page 15 ---\nYou are an event extraction model, looking to map provided trigger words to potential event types.This is an event extraction task where the goal is to extract structured events from the text. Given the sentence and possible event triggers, map these triggers to corresponding events from the provided event list. Omit triggers which are not mappable or if the mapped event is not mentioned in the sentence.The event list comprises 7 events. These events are:Infect … Spread … …Below is the sentence and the list of trigger words. Map each trigger word from this list to a single event from above and output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...]. Be conservative in your outputs - If a trigger word cannot be mapped, skip that trigger word. If the mapped event does not happen in the sentence, skip that trigger word. Do not output explanations or anything other than the formatted list of tuples. If no triggers can be mapped, output empty list [].Sentence: I hope this pandemic ends soon …Trigger List: ['ends']\"System PromptUser InstructionsUser QueryFigure 6: Illustration of the prompt utilized for\nGrounder. To encourage convergent thinking and align-\nment, we add event-based constraints in the model in-\nstructions. Furthermore, we add sentences that encour-\nage the model to be more conservative in its predictions.\ndenser with 2.5-10 event mentions/passage. Fi-\nnally, we also show the variation in token length,\nwith ACE being the lowest with 13 average tokens,\nwhile GENIA and CASIE are longer with 250-280\naverage tokens per document.\nB.2 Additional Implementation Details\nIn this section, we provide additional implementa-\ntion details for DICOREand the various baselines.\nFor open-source models, we ran them locally on\nNVIDIA RTX A6000/A100 machines with support\nfor 8 GPUs.\nB.2.1 D ICORE\nTrigger Atomization Adaptation for FSM-\nguided Decoding: Different datasets have var-\nied annotation instructions and definitions for the\ntrigger spans. Some datasets are strictly adhering\nto only single-word triggers (e.g., SPEED), while\nothers are largely loose and support multi-word\ntriggers (e.g., CASIE). We provide a small study\nof measuring multi-word triggers in Table 8, high-\nlighting this disparity across datasets. To account\nYou are an event extraction verification model, looking to verify the provided trigger word triggers the event type in the given sentence.This is an event extraction verification task where the goal is to verify if the extracted structured event is mentioned in the text. Given the sentence, a possible event mention with its trigger, verify if the event mention is correct or not.Event Definition: The event of interest is infect. The event is related to the process of a disease/pathogen invading host(s).Event Trigger: infectionBelow is the sentence. Verify if the above trigger word triggers the above mentioned event in this given sentence. If yes, then output 'Yes' else output 'No' . Do not output explanations or anything other than 'Yes/No' .People who live in crowded or poorer areas are more likely to test positive for Covid - 19 , according to a ( url ) study of infection in the general population ( url )System PromptUser InstructionsUser QueryFigure 7: Illustration of the prompt utilized for Judge.\nTo encourage convergent thinking and alignment, we\nadd event-based constraints in the model instructions.\nFurthermore, we add sentences that encourage the\nmodel to be more conservative in its predictions.\nDataset Domain # Doc # Event Avg. Doc\nMentions Length\nMA VEN General 250 623 24.5\nFewEvent General 250 250 30.5\nACE News 250 71 13.2\nGENIA Biomedical 250 2472 251.3\nSPEED Epidemiology 250 258 32.4\nCASIE Cybersecurity 50 291 283.1\nTable 7: Data Statistics of the various ED datasets used\nin our experimental setup.\nfor these varied definitions, we infuse a customiz-\nable atomization unit in our FSM-guided decoding.\nSpecifically, state C from Figure 4 is customiz-\nable wherein for stricter datasets (SPEED, ACE,\nFewEvent), we impose an additional constraint\nof single-word trigger, while for other datasets\n(CASIE, GENIA, MA VEN), we apply a looser con-\nstraint of substring match with the query sentence.\nB.2.2 Multi-event Direct (MD)\nMulti-event direct (MD) (Gao et al., 2023; Huang\net al., 2024; Chen et al., 2024) is the most common\nand simplest prompting technique used for ED. It\nprompts the model directly to reason across all the\nevents and provide the relevant triggers based on\n15\n--- Page 16 ---\nDataset % Multi-word Triggers\nMA VEN 8%\nFewEvent 3%\nACE 2.8%\nGENIA 8.5%\nSPEED 0%\nCASIE 54.6%\nTable 8: Measuring the percentage of multi-word trig-\ngers across the different ED datasets.\nYou are an event extraction model, looking to extract events from a sentence.This is an event extraction task where the goal is to extract structured events from the text. A structured event contains an event trigger word and an event type.Here are 7 events that we are interested in:Infect … Spread … …Below is a sentence from which you need to extract the events if any. Only output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...] for each event in the sentence. Do not output explainations or anything other than the formatted list of tuples. If there are no events in the sentence, output empty list [].People who live in crowded or poorer areas are more likely to test positive for Covid - 19 , according to a ( url ) study of infection in the general population ( url )System PromptUser InstructionsUser Query\nFigure 8: Illustration of the prompt utilized for multi-\nevent direct baseline.\nthe query text. We try various prompt versions and\nillustrate the best engineered prompt based on a\nsmall study in Figure 8. Majorly, we include all\ntask-specific instructions and constraints in a single\nverbalized prompt, which can overload the LLM’s\nreasoning capability.\nB.2.3 Multi-event Staged (MS)\nMulti-event staged (MS) (Parekh et al., 2025) was\nintroduced as a way of forward generation to en-\nsure higher trigger quality. We extend that in our\nwork to build a strong task decomposition baseline.\nSimply, this model first extracts the event types\nfrom the texts in Stage 1 and then extracts triggers\nspecific to these event types in Stage 2. We provide\nan illustration of the two stages of MS in Figures 9\nand 10. In this case, the first stage majorly only\nfocuses on the event-specific constraints, while the\nYou are an event extraction model, looking to extract events from a sentence.This is an event extraction task where the goal is to extract structured events from the text. Given the sentence, figure if you find any event mention of the possible events.Here are 7 events that we are interested in:Infect … Spread … …Below is a sentence from which you need to extract the events if any. Only output a list of events in the form [\\\"event_type_1\\\" , \\\"event_type_2\\\" , ...] that you find in the sentence. Do not output explanations or anything other than the formatted list. If there are no events in the sentence, output empty list [].People who live in crowded or poorer areas are more likely to test positive for Covid - 19 , according to a ( url ) study of infection in the general population ( url )System PromptUser InstructionsUser QueryFigure 9: Illustration of the Stage-1 prompt utilized for\nmulti-event staged baseline.\nYou are an event extraction model, looking to extract event triggers for given events from a sentence.This is an event extraction task where the goal is to extract structured events from the text. Given the sentence and possible events, find corresponding event triggers for the event. Event triggers are usually one word, many times verbs, and most indicative of the event presence.Here are 1 events that are possibly present in the sentence: ['infect’]The event of interest is infect. The event is related to the process of a disease/pathogen invading host(s).Below is a sentence. Identify the trigger word for the above listed events and output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...] for all the events mentions. Omit events which are not present or do not have a prominent trigger. Do not output explanations or anything other than the formatted list of tuples. If there are no events in the sentence, output empty list [].Children can catch COVID - 19 .System PromptUser InstructionsUser Query\nFigure 10: Illustration of the Stage-2 prompt utilized for\nmulti-event staged baseline.\n16\n--- Page 17 ---\nYou are an event extraction model, looking to extract event triggers for the given event from a sentence.This is an event extraction task where the goal is to extract structured events from the text. Given the sentence and the event definition, find corresponding event triggers for the event. Event triggers are usually one word, many times verbs, and most indicative of the event presence.The event of interest is infect. The event is related to the process of a disease/pathogen invading host(s).Below is a sentence. Identify the trigger word for the above event of interest. Output a list in the form [\\\"trigger1\\\" , \\\"trigger2\\\" ...] for all the events mentions. If the event is not present, output a empty list []. Do not output explanations or anything other than the output list.Children can catch COVID - 19 .System PromptUser InstructionsUser QueryFigure 11: Illustration of the prompt utilized for binary-\nevent direct baseline.\nsecond stage is focused on the trigger-specific ones.\nB.2.4 Binary-event Direct (BD)\nBinary-event direct (BD) (Lyu et al., 2021; Li et al.,\n2023c) has been a popular paradigm pre-dating\nLLMs when smaller generative text-to-text mod-\nels were used. It drastically reduces the LLM’s\nconstraints by making the LLM focus on a single\nevent type at a time, i.e., it prompts the LLM in a\nmulti-event direct manner, but for each event type\nseparately. Finally, the predictions are aggregated\nand output as the final prediction. We provide an\nillustration of the prompt in Figure 11. Overall,\nthis is a highly expensive method, especially for\nlarger event datasets.\nB.2.5 Decompose-Enrich-Extract (DEE)\nDecompose-Enrich-Extract (DEE) (Shiri et al.,\n2024) is a variation of the multi-event direct (MD)\nmodel, wherein it prompts the model to make pre-\ndictions while enhancing the input schema. It also\nputs down additional rules to make the extraction\nmore accurate, but we posit this also adds more\nconstraints, restricting the model’s reasoning. We\nprovide an illustration of the prompt for this base-\nline in Figure 12.\nB.2.6 GuidelineEE (GEE)\nGuidelineEE (GEE) (Srivastava et al., 2025) is the\nmethod focused on providing extensive guidelines\nYou are an event extraction model, looking to extract events from a sentence.Task Description: You are an assistant that helps extract the list of event types and their trigger words from input text.Extraction Rules:-The instance can contain any number of events.-Limit responses to event types and their triggers only.-Refrain from providing additional explanations.-Do not enumerate the list.Event Type Definitions: The possible event types and their definitions are as follows:Infect … Spread … …Output Format: Output a list of events [{'event_type': <event_type_1>, 'trigger': <event_trigger_1>}, {'event_type': <event_type_2>, 'trigger': <event_trigger_2>}, ...]. Each event contains an event type and its trigger.People who live in crowded or poorer areas are more likely to test positive for Covid - 19 , according to a ( url ) study of infection in the general population ( url )System PromptUser InstructionsUser QueryFigure 12: Illustration of the prompt utilized for\nDecompose-Enrich-Extract baseline.\nYou are an event extraction model, looking to extract events from a sentence.This is an event extraction task where the goal is to extract structured events from the text. A structured event contains an event trigger word and an event type. For each different event type, please output the instances of the corresponding classes with the appropriate trigger i.e. <Event_Name>(trigger='<Trigger_name>’)#  The following lines describe the events as python classes:@dataclassclass infect():\"\"\" The event of interest is infect. The event is related to the process of a disease/pathogen invading host(s).\"\"\"def __init__(self, trigger: str):        self.trigger = trigger…# This is the text to analyzetext = “Children can catch COVID - 19 .”System PromptUser InstructionsUser Query# The list called result should contain the instances for the events in the above text according to the guidelines above (i.e. [ event_name1(trigger='trigger1'), event_name2(trigger='trigger2'), ...]):result = \nFigure 13: Illustration of the prompt utilized for Guide-\nlineEE baseline.\n17\n--- Page 18 ---\nto the LLM to improve its task understanding capa-\nbility. This work is similar to Code4Struct (Wang\net al., 2023), wherein the input and output are more\ncode-oriented using Python class-like structures.\nThe definition is provided as a docstring, and the\ntrigger is extracted as an attribute of the class. The\noutput is mainly instantiations of the right set of\nclasses. We provide an illustration of the prompt\nfor this baseline in Figure 13.\nB.2.7 ChatIE\nChatIE (Wei et al., 2023) is a simple variation of\nmulti-event staged (MS), but uses multi-turn con-\nversation with the LLM. Specifically, stage-1 (Fig-\nure 9) is used as the initial prompt, and based on the\noutput, stage-2 (Figure 10) is used as the second\nturn of the prompt.\nB.2.8 GPT Runs\nFor the GPT models (i.e., GPT3.5-turbo, GPT4o,\nO1-mini), we utilized Curator (Marten et al., 2025)\nfor the API calls. We noticed how the GPT models\nare already super conservative in their predictions,\neven when explicitly asked not to be. The Judge\ncomponent was indeed hurting model performance\nby making the pipeline more conservative. Thus,\nwe removed the Judge from all runs of the GPT\nLLMs.\nC Additional Experimental Results\nHere we provide additional and complementary\nresults to the ones discussed in the main paper.\nC.1 Structured v/s Unstructured Output\nIn our work, we largely maintain the output to be\nstructured to ensure easy parsing and get stronger\nmodel performance as noted in Wang et al. (2023).\nTo provide more evidence, we conducted a small\nexperiment with different output formats: (1) Struc-\ntured JSON output (the base version that we have\ncurrently) using a JSON list of tuples as the out-\nput, (2) Structured text wherein we ask the LLM\nto produce natural language text but in a structured\nway, and (3) Free-form text and re-structuring (Tam\net al., 2024), wherein the LLM generates free-form\ntext in the first generation and later restructures into\nJSON format using an additional LLM generation.\nWe provide an illustration of these output formats\nin Figure 14.\nWe ablate these three output formats using the\nMulti-event Direct (MD) prompt setting for the\nBelow is a sentence from which you need to extract the events if any. Only output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...] for each event in the sentence. Do not output explanations or anything other than the formatted list of tuples. If there are no events in the sentence, output empty list [].Below is a sentence from which you need to extract the events if any. Output the event and trigger information as natural sentences like “The event <event_name> is triggered by the trigger <trigger>.” for each event type on a new line. Do not output explanations. If there are no events in the sentence, output “No events found.”.Below is a sentence from which you need to extract the events if any. Output the event and trigger information in natural language as you wish. Do not output any explanations.\nStructured JSONStructured TextFree-form & RestructuringBelow is the text from which you need to extract the structured event-related information. Only output a list of tuples in the form [(\\\"event_type_1\\\" , \\\"event_trigger_word_1\\\"), (\\\"event_type_2\\\" , \\\"event_trigger_word_2\\\"), ...] for each event in the sentence. Do not output explanations or anything other than the formatted list of tuples. If there are no events in the sentence, output empty list [].Figure 14: Illustration of the prompts utilized for the\ndifferent output formats for ablating why the structured\noutput format is better.\nOutput Format TI TC EI\nStructured JSON 35.6 22.4 30.1\nStuctured Text 14.9 11.0 31.8\nFree-form & Restructuring 16.7 12.7 20.8\nTable 9: Ablation Study on the ACE dataset using\nLlama3-8B-Instruct, highlighting the significance of\nutilizing structured JSON output compared to text out-\nputs.\nACE dataset using Llama3-8B-Instruct. We pro-\nvide the results of the average of 3 runs in Table 9.\nAs clearly evidenced, any kind of text-based output\nformat is quite poor for TI and TC metrics, high-\nlighting the significance of JSON-based output.\nC.2 Complete Results for Transfer Learning\nBaselines\nWe discussed and compared DICOREwith exist-\ning zero-shot cross-dataset transfer-learning ap-\nproaches in § 5.2. We provide complete results\nfor each dataset in Table 12 for a deeper analysis.\nWe exclude results for MA VEN and FewEvent for\nGOLLIE as the generations were degenerate and\nled to 0 F1 performance. Across the three settings\nof various source-target datasets, we see how our\npure zero-shot DICOREconsistently outperforms\nall the fine-tuned transfer learning baselines by a\nconsiderable margin. In fact, DICORE, based on\nthe smaller Llama3-8B-Instruct LLM is stronger\nthan most of these transfer-learning baselines. This\nhighlights the superior zero-shot generalization of\n18\n--- Page 19 ---\nLLM Prompt Style Avg. Words\nLlama3-8BMD + CoT 36.8\nMS + CoT 82.4\nLlama3-70BMD + CoT 87.4\nMS + CoT 107.9\nQwen2.5-72BMD + CoT 96.3\nMS + CoT 184.4\nDS-Qwen-32BMD 247.8\nMS 525.5\nDS-L3-70BMD 258.9\nMS 484.4\nLlama3-8B D ICORE 11.6\nLlama3-70B D ICORE 6.6\nQwen2.5-72B D ICORE 5.1\nTable 10: Efficiency analysis in terms of average number\nof words per query (Avg. Words) of DICOREwith other\nreasoning-based baselines on the ACE dataset.\nour proposed method.\nC.3 Complete Results for Reasoning Baselines\nIn § 5.3, we discuss and compare DICOREwith\nreasoning-based approaches and models. Here, we\nprovide complete results of that comparison across\ndatasets in Table 14. In comparison to the non-\nCoT numbers, we note how CoT provides gains\nfor the baseline models, and larger gains for the\nlarger LLMs. This indicates how reasoning im-\nproves model performance, but also requires more\nparameters and longer context handling. Thinking-\nbased models somehow show poorer performance\ncompared to CoT, and our observations align with\nLi et al. (2025). Next, we show how the base non-\nCoT performance of DICOREis better than the\nCoT-based baselines. This can also be seen when\ncomparing thinking-based model baselines. This\nstrongly indicates how the strong inductive bias of\nDICOREbeats the reasoning-based improvements.\nAdditionally, we also infuse reasoning with DI-\nCORE, specifically only in the Grounder stage.\nReasoning in the Dreamer stage makes the model\nmore conservative and harms the divergent reason-\ning we want to encourage. We note how this addi-\ntional reasoning provides further improvements of\nup to 1-2% F1 over the base DICOREperformance.\nEfficiency analysis: Apart from performance,\nwe also analyze the effectiveness in terms of ef-\nficiency of the various methods. We measure effi-\nciency by the average number of output words gen-\nerated per query (which should be equivalent to the\naverage number of output tokens). We provide this\ncomparison for the different methods and LLMs\nfor the ACE dataset in Table 10. As evident, CoTComponent/LLM EI\nP R F\nLlama3-8B-Instruct\nDreamer 0.0 0.0 0.0\n+ Grounder 19.1 45.6 26.9\n+ FSM Decoding 21.1 54.9 32.3\n+ Judge 49.5 46.5 47.9\nMD Baseline 40.5 23.9 30.1\nMS Baseline 18.9 29.6 23.1\nLlama3-70B-Instruct\nDreamer 0.0 0.0 0.0\n+ Grounder 25.4 61.5 36.0\n+ FSM Decoding 29.1 60.1 39.2\n+ Judge 50.8 60.1 55.1\nMD Baseline 51.2 43.2 46.8\nMS Baseline 62.5 37.5 46.9\nTable 11: Ablation Study using Trigger Identification\n(TI) on the ACE dataset highlighting the significance\nand contribution of each component of DICORE. P:\nPrecision, R: Recall, F: F1 score.\nand thinking-based models expend a large amount\nof tokens on token-based reasoning, which is zero\nin the case of DICORE. On average, DICOREre-\nduces the output words by 15x compared to CoT\nand by up to 55x compared to the thinking-based\nmodels. This highlights the practical utility of DI-\nCOREwhere it can provide higher performance at\nvastly reduced token generation cost.\nC.4 Additional results for Ablation Study\nWe provided an ablation study for DICORE’s com-\nponents in § 5.4. Here we provide additional re-\nsults for the same study, specifically for the Event\nIdentification (EI) evaluation metric in Table 11.\nWe conclude observations similar to those noted in\nthe main paper, highlighting how DICOREhelps\nincrease the recall without much decreasing the pre-\ncision of the model. Dreamer has a 0% score since\nthe event names are free-form text generations in\nthis stage.\nD Broader Qualitative Study\nWe provided a brief qualitative study eliciting some\ncommon errors of previous baselines and how\nDICOREfixes them in § 5.4. Here, we provide\nsome more examples to highlight the various er-\nrors made by previous baselines in Table 13. Next,\nwe also show some more examples to elicit the\ninternal component-wise predictions of DICORE\nin Table 15. Overall, these examples demonstrate\nthe utility of the divergent-convergent reasoning\n19\n--- Page 20 ---\nLM/LLMPrompt MA VEN (168) FewEvent (100) ACE (33) GENIA (9) SPEED (7) CASIE (5) Average\nStyle TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI\nTrained on ACE data* → Tested on other datasets\nBART-large DEGREE 29.4 11.0 13.8 42.6 22.5 27.2 - - - 5.1 3.5 11.6 23.4 16.2 26.7 3.8 2.0 27.0 20.9 11.0 21.3\nLlama3-8B D ICORE53.5 14.4 17.4 26.1 15.7 25.0 - - - 25.8 15.4 30.0 35.5 23.6 42.4 18.5 16.8 58.8 31.9 17.2 34.7\nLlama3-70B D ICORE62.5 27.8 30.6 40.4 25.1 36.1 - - - 38.6 31.0 48.5 45.0 36.5 51.8 17.3 16.6 66.6 40.8 27.4 46.7\nGPT4o D ICORE58.5 32.2 35.6 36.1 28.4 38.5 - - - 40.7 35.4 51.2 43.3 37.3 46.1 16.7 16.7 58.8 39.1 30.0 46.0\nTrained on MA VEN data* → Tested on other datasets\nBART-large DEGREE - - - 31.1 18.7 25.0 43.3 36.6 38.2 33.9 27.6 46.2 44.8 37.1 44.8 6.1 5.2 38.6 31.8 25.0 38.6\nLlama3-8B D ICORE - - - 26.1 15.7 25.0 40.3 36.3 47.9 25.8 15.4 30.0 35.5 23.6 42.4 18.5 16.8 58.8 29.2 21.6 40.8\nLlama3-70B D ICORE - - - 40.4 25.1 36.1 57.2 49.5 55.1 38.6 31.0 48.5 45.0 36.5 51.8 17.3 16.6 66.6 39.7 31.7 51.6\nGPT4o D ICORE - - - 36.1 28.4 38.5 54.9 54.9 56.6 40.7 35.4 51.2 43.3 37.3 46.1 16.7 16.7 58.8 38.3 34.5 50.2\nTrained on ACE data* → Tested on GENIA, SPEED, CASIE\nGOLLIE-7B GOLLIE - - - - - - - - - 3.2 2.2 7.1 12.6 11.6 24.3 2.1 2.1 14.4 6.0 5.3 15.3\nGOLLIE-34B GOLLIE - - - - - - - - - 26.5 22.8 40.4 15.9 10.9 19.1 4.5 1.5 28.6 15.6 11.7 29.4\nLlama3-8B D ICORE - - - - - - - - - 25.8 15.4 30.0 35.5 23.6 42.4 18.5 16.8 58.8 26.6 18.6 43.7\nLlama3-70B D ICORE - - - - - - - - - 38.6 31.0 48.5 45.0 36.5 51.8 17.3 16.6 66.6 33.6 28.0 55.6\nGPT4o D ICORE - - - - - - - - - 40.7 35.4 51.2 43.3 37.3 46.1 16.7 16.7 58.8 33.6 29.8 52.0\nTable 12: Complete results for comparison of DICOREwith other fine-tuned transfer-learning approaches for\nzero-shot ED. *Training done for models other than DICORE.DICOREresults are pure zero-shot, i.e., without any\ntraining. \"-\" indicates training data or where results were degenerate. (XX) = number of distinct event types.\nparadigm for ED.\nSentence Baseline Prediction\nPrecision Errors\nIn the near future we will be\nexpanding this to include all\nthe other organizations that we\ncan contact, but we are just\nkeeping things safe for now.[(\"Phone-Write\",\n\"contact\" )]\nThe Holocaust of the Jews\nand Zigeuner was motivated\nby racial prejudices.[(\"Attack\" ,\"Holo-\ncaust\" )]\nMy friend, an ER physician\nhas said over 70% of people\nwho test positive for covid\nNEVER have a fever.[(\"symptom\",\n\"fever\" )]\nOn 4 April 2013, a build-\ning collapsed on tribal land in\nMumbra.[(\"Destroying\" ,\"col-\nlapsed\" )]\nRecall Errors\nPasko was released in January\nfor good behavior after serv-\ning more than two-thirds of the\nsentence.[(\"Release-Parole\",\n\"released\" )]\nMissed: (\"Sentence\",\n\"sentence\")\nPeople who live in crowded or\npoorer areas are more likely to\ntest positive for Covid - 19[]\nMissed: (\"infect\",\n\"positive\")\nWOW debuted on January 18\nas part of AXS’s Friday Night\nFights schedule[]\nMissed: (\"Pro-\ncess_start\", \"de-\nbuted\")\nHe is got it pretty easy Id\nsay even with the international\ntravel[]\nMissed: (\"Transport-\nperson\", \"travel\")\nTable 13: Qualitative examples highlighting the various\nerrors by zero-shot LLM baselines. We highlight the\ncorrect predictions in green and incorrect ones in red.\n20\n--- Page 21 ---\nLLMPrompt MA VEN (168) FewEvent (100) ACE (33) GENIA (9) SPEED (7) CASIE (5) Average\nStyle TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI TI TC EI\nChain-of-thought\nLlama3-8BMD 45.9 2.8 4.0 25.2 9.5 15.2 35.6 22.4 30.1 22.8 15.3 25.4 34.9 27.8 42.4 10.3 8.8 47.9 29.1 14.4 27.5\n+ CoT 35.4 3.2 4.8 15.4 6.8 13.8 30.6 18.7 27.6 24.3 15.9 26.9 34.6 27.8 42.1 9.8 8.7 47.1 25.0 13.5 27.1\nMS 46.2 10.3 11.2 20.2 10.2 17.0 26.7 17.6 23.1 27.6 19.7 30.5 34.1 27.3 40.6 11.9 10.3 48.3 27.8 15.9 28.4\n+ CoT 35.9 7.2 8.2 20.5 11.1 19.3 34.3 23.4 32.9 27.2 20.1 29.6 39.4 31.9 46.6 13.1 12.2 54.8 28.4 17.6 31.9\nDICORE53.5 14.4 17.4 26.1 15.7 25.0 40.3 36.3 47.9 25.8 15.4 30.0 35.5 23.6 42.4 18.5 16.8 58.8 33.3 20.4 36.9\n+ CoT 53.6 15.5 17.9 27.5 15.4 24.7 39.8 36.6 45.0 25.8 16.4 31.9 35.1 26.6 41.5 16.7 15.9 56.0 33.1 21.1 36.2\nLlama3-70BMD 63.5 14.2 14.7 34.0 20.9 32.6 51.2 40.2 46.8 36.8 28.9 43.0 45.4 36.8 49.0 13.9 13.7 64.4 40.8 25.8 41.8\n+ CoT 56.0 29.4 32.5 37.1 25.3 37.2 54.9 48.5 57.1 35.4 28.2 45.5 47.1 39.5 50.3 15.7 14.8 65.4 41.0 30.9 48.0\nMS 33.9 21.6 22.3 35.3 24.9 39.9 49.9 42.8 46.9 37.4 31.0 45.0 43.8 35.5 49.6 14.0 14.0 59.5 35.7 28.3 43.9\n+ CoT 55.7 29.5 32.6 34.9 25.4 38.6 56.1 51.3 56.5 31.8 26.4 37.7 49.7 42.5 56.6 14.8 14.6 60.6 40.5 31.6 47.1\nDICORE62.5 27.8 30.6 40.4 25.1 36.1 57.2 49.5 55.1 38.6 31.0 48.5 45.0 36.5 51.8 17.3 16.6 66.6 43.5 32.8 48.1\n+ CoT 61.2 34.1 36.4 40.9 27.3 37.5 55.4 51.7 58.5 37.9 31.7 48.1 44.3 36.5 50.8 18.0 17.4 67.1 43.0 33.1 49.8\nQwen2.5-72BMD 49.4 21.6 24.1 17.0 12.3 21.0 28.8 25.8 30.3 30.5 27.0 36.3 41.4 37.4 45.4 11.0 10.4 57.9 29.7 22.4 35.8\n+ CoT 54.0 27.9 33.8 26.7 20.5 33.3 46.1 41.6 47.3 29.5 26.1 38.9 42.6 36.8 48.1 10.3 9.9 60.0 34.9 27.1 43.6\nMS 39.9 23.6 25.4 25.0 21.0 34.2 42.5 40.4 42.5 26.7 23.6 34.1 40.6 35.5 45.2 10.5 10.5 49.1 30.9 25.8 38.4\n+ CoT 54.2 28.0 31.1 28.3 21.5 33.6 48.5 46.3 48.9 30.7 26.5 38.7 44.9 39.7 47.9 10.6 10.6 44.5 36.2 28.8 40.8\nDICORE54.1 27.5 30.2 30.8 22.3 32.9 46.8 44.8 47.8 33.6 29.8 43.9 40.6 34.7 41.4 15.9 15.8 59.3 37.0 29.2 42.6\n+ CoT 54.2 29.7 33.8 31.7 23.5 35.5 45.4 42.2 45.4 34.2 29.2 43.6 40.5 34.6 44.8 16.8 16.7 60.0 37.1 29.3 43.8\nThinking-based models\nDS-Qwen-32BMD 55.3 26.7 30.1 34.0 23.7 36.8 56.3 51.8 60.2 33.2 27.5 41.2 45.5 39.0 54.5 11.1 11.1 54.9 39.2 30.0 46.3\nMS 55.0 25.8 29.6 33.8 23.3 38.5 50.6 48.9 59.6 30.5 25.0 36.6 52.7 44.7 54.7 14.6 14.6 51.9 39.5 30.4 45.2\nDICORE60.1 30.2 32.6 38.5 26.1 36.8 56.3 53.9 60.5 36.3 30.4 47.6 48.6 41.1 55.2 18.5 17.8 64.4 43.1 33.3 49.5\nDS-L3-70BMD 48.3 31.2 32.5 13.7 9.6 17.3 31.5 27.8 34.5 24.5 21.6 31.9 45.3 38.9 50.6 10.5 10.5 50.0 29.0 23.3 36.1\nMS 50.3 28.3 31.3 23.9 18.5 28.3 36.8 33.7 38.0 27.8 24.6 35.3 48.2 44.2 49.2 12.6 12.6 44.7 33.3 27.0 37.8\nDICORE59.5 34.7 37.2 36.2 25.9 35.0 53.0 51.3 55.8 32.3 28.6 42.7 49.3 39.8 53.4 18.0 17.9 65.9 41.4 33.0 48.3\nO1-mini MD 59.1 32.8 35.7 36.8 28.0 40.3 53.9 48.5 53.0 35.8 33.7 43.8 44.2 40.2 48.1 11.5 11.5 47.5 40.2 32.5 44.7\nTable 14: Complete results for comparison of DICOREwith reasoning approaches like Chain-of-thought (CoT) and\nthinking-based models for zero-shot ED. bold = best performance. (XX) = number of distinct event types.\nSentence Dreamer Grounder Judge\nPrediction Prediction Prediction\nPolice also arrested two Moroc-\ncan men suspected of traffick-\ning in human beings and nav-\nigating the Zodiac boat across\nfrom Africa, Efe said.[(\"arrest\", \"arrested\"),\n(\"trafficking\", \"trafficking\"),\n(\"navigating\", \"navigating\"),\n(\"said\", \"said\")][(\"Arrest-Jail\", \"arrested\"),\n(\"Charge-Indict\", \"traffick-\ning\")][(\"Arrest-Jail\", \"ar-\nrested\" )]\nOnly 4 men have competed\nwithout eliminating a single\nopponent Fire, Mini Maximo,\nSombrita and Stukita.[(\"compete\", \"competed\"),\n(\"eliminate\", \"eliminating\")][(\"Competition\", \"com-\npeted\")][(\"Competition\",\n\"competed\") ]\nWeird as hell: the Covid-19 pa-\ntients who have symptoms for\nmonths | Coronavirus outbreak |\nThe Guardian (url)[(\"Disease_Spread\", \"out-\nbreak\"), (\"Infection\", \"pa-\ntients\"), (\"Symptom_Show\",\n\"symptoms\")][(\"symptom\", \"symptoms\"),\n(\"spread\", \"outbreak\")][(\"symptom\", \"symp-\ntoms\" ), ( \"spread\",\n\"outbreak\" )]\nThe time he has spent inside\nroughly equates to 2 years per\nwoman he killed[(\"Kill\", \"killed\"), (\"Spend\",\n\"spent\"), (\"Equate\",\n\"equates\")][(\"Life.Die\", \"killed\")] [(\"Life.Die\",\n\"killed\" )]\nTable 15: Qualitative examples eliciting DICORE’s predictions per component for various input sentences. We\nhighlight the correct predictions in green and incorrect ones in red.\n21",
  "text_length": 91617
}