{
  "id": "http://arxiv.org/abs/2506.04139v1",
  "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in\n  Low-Resource Flemish?",
  "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.",
  "authors": [
    "Ratna Kandala",
    "Katie Hoemann"
  ],
  "published": "2025-06-04T16:31:37Z",
  "updated": "2025-06-04T16:31:37Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04139v1",
  "full_text": "--- Page 1 ---\nAre Lexicon -Based Tools Still the Gold Standard for Valence Analysis in Low -Resource \nFlemish?  \nRatna Kandala, Katie Hoemann  \nDepartment of Psychology, University of Kansas, USA  \nn038k926 @ku.edu , hoemann@ku.edu  \n \nABSTRACT   \nUnderstanding the nuances in everyday language is pivotal for advancements in computational \nlinguistics and emotions research. Traditional lexicon -based tools such as Linguistic Inquiry and \nWord Count (LIWC) (Pennebaker et al., 20 07) and Pattern (De Smedt & Daelemans, 2012) have \nlong served as foundational instruments in this domain. LIWC is the most extensively validated \nword -count -based text analysis tool in the social sciences and Pattern is an open -source Python \nlibrary offering functionalities for N atural Language Processing (NLP) . However, everyday \nlanguage is inherently spontaneous, richly expressive, and deeply context dependent. To ex plore \nthe capabilities of Large Language Models (LLMs) in capturing the valences of daily narratives in \nFlemish (Belgian Dutch), a low -resource language, we first conducted a study involving \napproximately 25,000 textual responses from 102 Dutch -speaking participants. Each participant \nprovided narratives prompted by the question, “What is happening right now and how do you feel \nabout it?”, accompanied by self -assessed valence ratings on a continuous scale from –50 to +50. \nWe then assessed the per formance of three Dutch -specific LLMs ChocoLlama -8B-Instruct, \nReynaerde -7B-chat, and GEITje -7B-ultra in predicting these valence scores, and compared their \noutputs to those generated by LIWC and Pattern. Our findings indicate that, despite advancements \nin LLM architec tures, these Dutch -tuned models currently fall short in accurately capturing the \nemotional valence present in spontaneous, real -world narratives. This study underscores the \nimperative for developing culturally and linguistically tailored models/NLP tools t hat can adeptly \nhandle the complexities of natural language use. Enhancing automated valence analysis is not only \npivotal for advancing computational methodologies but also holds significant promise for \nenriching psychological research with ecologically va lid insights into human daily experiences. \nWe advocate for increased efforts in creating comprehensive datasets and finetuning LLMs for \nlow-resource languages like Flemish, aiming to bridge the gap between computational linguistics \nand emotion research.   \n  \n--- Page 2 ---\n1. Introduction  \nThe increasing use of Large Language Models (LLMs) in everyday applications has raised \nimportant questions about their ability to understand and reflect emotional nuances in natural \nlanguage communication. As users increasingly seek empathetic interactions with LLMs \nperceiving them as trustworthy and emotionally resonant agents, the ability to decode valence \n(emotional pleasantness/unpleasantness , communicated via  language) becomes critical. Yet, \ncurrent LLMs lack the psychological mechanisms to generate genuine empathy, as noted in a \nrecent Nature Intelligence editorial (Shteynberg, 2024), underscoring the urgency of culturally and \nlinguistically tailored solution s. This can have long -term effects on how AI systems are \nexperienced by humans. This gap is especially pronounced in low -resource languages like Flemish \n(Belgian -Dutch), where culturally nuanced valence detection is essential  to develop Natural \nLanguage Processing (NLP)  tools for Flemish speakers.   \nLexicon -based sentiment analysis employs predefined dictionaries where each word is associated \nwith a sentiment label such as positive, negative, or neutral or a sentiment score indicating its \nemotional intensity. These dictionaries enable the extraction o f sentiment from text by mapping \nwords to their corresponding emotional values. This approach is widely utilized in psychological \nresearch to examine emotional content in text and analyze verbal behavior (Pennebaker  et al.,  \n2007; Tausczik & Pennebaker, 201 0). Lexicon -based methods have  also been particularly useful \nin studying emotional references in natural language (Mohammad & Turney, 2013). Traditional \nlexicon -based tools like Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) have \nlong dominated valence analysis in psychological research. These methods rely on predefined \nword -emotion mappings, offering interpretability. For instance, LIWC, the widely used tool for \ntext analysis in emotion research, excels in validated dictionar y categorie s but faces limitations in \ncapturing spontaneous, context -rich narratives common in daily language.  For example, in a study \nsurvey analyzing  responses about workplace satisfaction, LIWC’s predefined \"PosEmo\" \ndictionary might correctly flag words like  \"fulfilled,\"  \"collaborative,\"  or \"rewarding\"  as positive, \noffering interpretable insights into employee morale. However, in informal narratives like  \"This \n‘amazing’ Monday has me stuck in traffic for hours  - what a joy!\" , LIWC would \nmisclassify  \"amazing\"  and \"joy\" as positive, failing to detect the sarcastic tone and contextual \nfrustration.  Similarly,  another tool  - Pattern’s (De Smedt & Daelemans, 2012) rule-based sentiment \nanalysis, while robust for structured text, falters with free -form expressions due to its reliance on \nsyntactic patterns.  For instance, Pattern’s syntactic rules might accurately score the senten ce \"The \nproduct works flawlessly and is incredibly user -friendly\"  as positive by recognizing adjectives \nlike \"flawlessly\"  and \"user -friendly.\"  Yet, in a free -form narrative like  \"It’s wild how this ‘perfect’ \napp crashes every time I need it most  - truly next -level reliability,\"  Pattern would assign a positive \nscore based on  \"perfect\"  and \"next -level,\"  missing the irony and negative sentiment conveyed \nthrough context . \nMost  work in the sentiment analysis domain has been done in languages such as English, French , \nGerman, Mandarin, Spanish  etc. which benefit from extensive annotated datasets . The amount of \n--- Page 3 ---\nliterature on English outweighs other languages in sentiment analysis (Ligthart et al., \n2021).   Recent advances in LLMs like GPT -4o (Martinez, 2024) have shown promise in valence \nprediction for English. However, their efficacy in low -resource languages remains unproven, \nparticularly for Flemish, which exhibits lexical, grammatical, and sociolinguistic features distinct \nfrom standard (Netherlandic) Dutch. For example, Flemish incorporates regional vocabulary (e.g., \n“fuif” for “party”), divergent pronoun usag e (“gij” vs. “jij”), and multilingual influences from \nFrench  (and, like other variants of Dutch, increasingly English) . These nuances challenge lexicon -\nbased tools, risking inaccurate valence detection in Flemish texts.   \nThis study addresses three critical gaps:   \n1. Linguistic Inequity: Flemish, spoken by 6.5 million in Belgium, remains underrepresented \nin NLP research (Meeus  et al. , 2024).   \n2. Ecological Validity: Prior work predominantly relies on social media data (e.g., tweets) or \nonline surveys, which impose character limits and linguistic constraints that distort natural \nemotional expression leading to a performance decrease in sentiment an alysis (Reusens, \n2022; Roberts et al., 2014) in contrast to open -ended responses.  \n3. Methodological Rigor: Existing evaluations of LLMs for valence analysis often use \nexternal annotators, introducing biases absent in self -reported ground truth.   \nWe present a comparison of lexicon -based tools (LIWC, Pattern) and Dutch -tuned LLMs \n(ChocoLlama -8B-Instruct, Reynaerde -7B-chat, GEITje -7B-ultra) using nearly 25,000 open -ended \nFlemish narratives collected via ambulatory assessment (where participants repea tedly report \nexperiences in real time within their natural environments , minimizing  the retrospective bias \ninherent to lab surveys) from 102 Dutch  speakers in Belgium. Participants described their real-\ntime experiences using  self-rated valence scores (−50 to +50). Our findings reveal that even state -\nof-the-art Dutch LLMs underperform lexicon -based methods in aligning with human valence \nratings, underscoring the need for linguistically tailored models.   This gap carries profound \nimplications for low-resource NLP studies, especially for emotion science researchers.  Unlike  \nsocial media data, these narratives reflect free -flowing, context -rich expressions of valence, \noffering ecologically valid insights into momentary well -being. As such this is an attempt to br idge \ncomputational linguistics and psychology for low -resource languages, enabling models that \nrespect linguistic diversity. Our dataset, the first of its kind for Flemish, also paves the way for \nLLMs that genuinely understand cultural nuance, a prerequisi te for applications in customer \nsentiment analysis, social science research, and personalized mental health interventions.  This \nstudy underscores the societal stakes of equitable AI development: inaccurate valence detection \nrisks alienating Flemish speaker s from mental health support tools, exacerbating disparities in AI -\ndriven care.  \nThis paper is structured as follows. First, we situate our work within the broader literature on \nsentiment analysis and emotion detection, focusing on low -resource languages and methodological \n--- Page 4 ---\ngaps in NLP.  Next, we provide a concise overview of the tools central to this study: the lexicon -\nbased frameworks  LIWC  and Pattern , and Dutch -tuned large language models. Following this, we \noutline our methodology, detailing the collection of nearly 25,000 Flemish daily narratives, the \nself-reported valence rating protocol, and the comparative evaluation framework for LLMs and \nlexicons. Subsequently, we present results quantifying the alignment between model predictions \nand human valence ratings, emph asizing coverage disparities and statistical significance.  We then \nprovide a discussion contextualizing the performance disparities between lexicon -based tools and \nDutch -tuned LLMs .  Finally, we conclude with practical recommendations for advancing equitab le \nNLP tools in under -resourced linguistic contexts and propose future research directions to bridge \ncomputational and psychological inquiry.  \n2. Prior Work  \nSentiment analysis (SA), or  the detection of  valence  in language  [(un)pleasantness], is widely \napplied in healthcare (Qiu et al., 2023), customer service (Ali, 2024), and misinformation detection \n(Liu, 2023). Traditional lexicon -based tools like LIWC (Pennebaker et al., 2015), Pattern (De \nSmedt & Daelemans, 2012), and V ADER (Hutto & Gilbert, 2014)  which map words to predefined \nsentiment scores  have dominated psychological research due to their interpretability. However, \nthese tools lack contextual awareness , struggle  with sarcasm, multilingual code -switching, and \ndomain -specific valence cues (Mohammad & Turney, 2013).  Recent studies highlight LLMs’ \npotential in valence detection , particularly GPT -4o for English texts (Martinez, 2023). While \nLLMs  also show promise in psychiatry (Obradovich et al., 2024) and psychotherapy (Mishra et \nal., 2023), their performance in low -resource languages remains understudied. Untuned LLMs \noften fail to capture emotion intensity ( Liu et al., 2024 ), and their reliance on English -centric \ntraining data limits cross -linguistic generalizability (Ligthart  et al., 2021).   \nFor languages such as Dutch and its regional variant Flemish , spoken by 6.5 million in Belgium , \nwith distinct vocabulary (e.g.,  fuif for “party”) and pronoun usage ( gij vs. jij), SA research  is \nsparse. Recently, Reusens (2022) compared lexicon methods with BERT -based models on Flemish \ntweets, finding transformers superior for short -text analysis. However, this work omitted LIWC, \nthe psychology community’s gold standard,  and Pattern , and relied on social media data \nconstrained by character limits and linguis tic norms (Roberts et al., 2014).  Most SA datasets use \nexternal annotators to label valence (Dashtipour et al., 2021; Alexandridis et al., 2021), introducing \nbiases from third -party interpretations. Social media posts further distort emotional expression due \nto self -censorship and platform -specific constraints (Reusens, 2022). In contrast, ambulatory \nassessment  - collecting real -time, open -ended narratives with self -reported valence  offers \necologically valid ground truth (Pennebaker, 1997; Frisina et al., 2 004).  We address these gaps \nby: (a) Evaluating three Dutch -tuned LLMs against LIWC and Pattern on  nearly 25,000 open -\nended Flemish narratives  with self-reported valence scores  (−50 to +50).   (b) Using the \nuntranslated  (to English),  context -rich daily narratives to preserve linguistic authenticity  (c) \nProviding a comparison of lexicon -based methods and LLM s for low -resource Flemish.  \n--- Page 5 ---\n3. Lexicon -Based Text Analyses  \nA lexicon -based method employs a dictionary specifically created to detect sentiments, with its \ncore being a lexicon resource characterized by accuracy and comprehensive vocabulary coverage. \nWithin the landscape of computational linguistics tools, lexicon -based approaches are valued for \ntheir interpretability and ease of implementation. Below, we briefly discuss LIWC and Pattern.  \n3.1 LIWC  \nLinguistic Inquiry and Word Count (LIWC), introduced by Pennebaker et al.  (2001),  is among the \nearliest and most widely adopted frameworks. Although initially developed for English, LIWC has \nbeen adapted for Dutch and other languages (Boot et al., 2017). Its distinguishing feature is a \nmultidimensional classification of language, mappi ng Dutch lexicon words across more than 90 \ncategories, including psycholinguistic features like emotional tone (PosEmo and NegEmo) and \ngeneral metrics such as word frequency a nd punctuation use. While the PosEmo (positive emotion) \nand NegEmo (negative emotion) categories are commonly leveraged for SA, LIWC’s extensive \ncategorization also supports applications in psychology (e.g., identifying markers of depression) \nand sociolinguistics (e.g., examining gender -related communication patterns). To estimate the \nvalence [ (un)pleasantness] of texts using LIWC, researchers calculate the percentage of dictionary -\nmatched words within a given text.  The Dutch version of LIWC  2015  (Boot et a l., 2017) contains \n6,614 words  (of which 1227 words belong to  PosEmo and 1474 words belong to  NegEmo).  \nHowever, as a proprietary closed -source product, LIWC lacks transparency for customization, and \nits methodology does not incorporate rules for detecting negation, sarcasm, or intensifiers.  \n3.2 Pattern  \nPattern is an openly available Python package developed by the CLiPS Computational Linguistics \ngroup, University of Antwerp  (De Smedt & Daelemans, 2012) . Although it is often introduced as \na web -mining library, it bundles a broad Natural Language Processing (NLP) stack: web -scrapping \nhelpers, part -of-speech taggers, n -gram utilities, and WordNet access for six languages (English, \nSpanish, German, French, Italian, Dutch). Its advantages , such as easy installation , have become \na convenient tool for researchers who do not want to juggle multiple specialized libraries. It also \nprovides a dedicated sub -module for Dutch ( Pattern.nl) to perform SA without any machine \nlearning knowledge. Unlike recent studies using large language models (LLMs) for SA that \ndemand GPU resources, fine -tuning data and coding expertise, Pattern.nl is genuinely “plug -and-\nplay.” Installing the package and calling a single function (sentiment(text)) is sufficient, \nsignificantly lowering the threshold for scholars in psycholog y who need scalable sentiment \nestimates without the need to invest in a full ML  pipeline to answer their research questions.  To \nperform SA, Pattern.nl relies on a handcrafted polarity lexicon containing 3,304 distinct Dutch \nlemmas, approximately 97% of which are adjectives, the part -of-speech  (POS)  that carries the \nmost evaluative meaning. Each lemma is annotated with a polarity value in the interval [ -1,1] and \nauxiliary metadata such as subjectivity strength and short glosses. The algorithm works in three \nmajor steps: chunking, chunk -level scoring,  and sentence -level aggregation. In  chunking, a \n--- Page 6 ---\nsentence is divided into chunks (e.g. nominal, verbal, adjectival chunks). In chunk -level scoring, \nwithin each chunk, every token that appears in the lexicon contributes to  its polarity. Through \nhand -coded rules, tokens flagged as intensifiers, diminishers or negators adjust the values \naccordingly. Importantly, Pattern distinguishes between degree -modifying adverbs and regular \nsentiment words based on POS tags. For example, ‘verschrikkelijk’ can act as an adjective \n(“horrible”) or as an adverbial intensifie r (“terribly nice”). In sentence -level aggregation, the final \nsentence polarity is the average of the regulated chunk scores.  However, the lexicon was originally \ncompiled from product -review corpora and has been reported to score texts as neutral for sente nces \nwith affective words outside the domain on product reviews, simply because the lemmas were \nabsent from the resource.    \n4. LLMs  Chosen  \nTo evaluate valence estimation in Flemish texts, we selected three open -weight LLMs fine -tuned \nfor Dutch: ChocoLlama -8B-Instruct, GEITje -7B, and Reynaerde -7B-Chat. These models were \nchosen for their architectural diversity and training methodologies, aimin g to assess their \ncapabilities in understanding and processing Flemish emotional content. The selection of open -\nsource models was also driven by considerations for data privacy, ensuring that participant data \nremains secure.  All checkpoints are released un der the Apache -2.0 licence and load with the \nstandard HuggingFace chat template.  \nLlama -3-ChocoLlama -8B-Instruct  (Meeus  et al. , 2024): Built upon Meta’s Llama -3-8B \narchitecture, this model employs rank -16 low -rank adaptation (LoRA) on 32B Dutch tokens, \nfollowed by supervised fine -tuning (SFT) and direct -preference optimization (DPO) alignment. \nHugging Face It adheres to a languag e-adaptation strategy that retains the original tokenizer, a \nmethod previously shown to enhance Dutch performance in Llama -2 variants. A dedicated \nevaluation set, ChocoLlama -Bench, accompanies the release.   \nGEITje -7B-Ultra  (Vanroy, 2024): Based on Mistral 7B architecture, GEITje -7B-Ultra undergoes \nfull-parameter pretraining on 10 billion Dutch tokens, followed by  supervised fine -tuning  (SFT) \nand direct preference optimization  (DPO ) alignment for conversational use. Hugging Face Project \ndocumentation indicates that GEITje -Ultra was built on publicly available Llama -2-7B-hf weights \n(Touvron et al., 2023), reflecting an  open -foundation philosophy. Among the GEITje family, the \nUltra var iant is reported as the best overall performer.   \nReynaerde -7B-Chat  (ReBatch, 2024): Rather than further pretraining, Reynaerde -7B-Chat \nattaches rank -8 quantized LoRA (QLoRA) adapters to a Llama -2-7B-hf base. It is fine -tuned on 4 \n× 10⁵ Dutch chat pairs and concludes with DPO alignment, producing a safe, concise dialogue \nstyle that can run on a single NVIDIA A100 GPU.  \nIt has been demonstrated that both GEITje -7B-Ultra and Reynaerde -7B-Chat have outperformed \nChocoLlama -2-7B-tokentrans -instruct in Dutch reasoning, comprehension, and writing \n--- Page 7 ---\nbenchmarks, though their efficacy for psychological research remains to be evaluated (Meeus  et \nal., 2024) . \n5. Methodology  \n5.1 Data   \nThe dataset comprises  24,854 open -ended Dutch -language text entries  (medium -scale) collected \nlongitudinally over  70 days  from  102 participants  (age 18 –65, μ=26.47, σ=8.87) in Belgium. \nParticipants were native Dutch speakers with smartphone access, recruited via online/posted flyers \nand referrals  (most were native Belgians, a few were from the Netherlands and living in Belgium) . \nUsing a dedicated app, they received  four daily prompts  asking  (in Dutch) , “What is going on now \nor since the last prompt, and how do you feel about it?” Responses were either short text snippets \n(3-4 sentences) or 1 -minute voice recordings, yielding a temporally structured dataset with \npotential for time -series or multimodal analysis (text + audio). For robustness, eligibility criteria \nensured linguistic consistency (native speakers) and device relia bility (smartphone ownership). \nFurther methodological details, including ethical protocols, are provided in the Supplementary \nMaterial. The dataset can be made available to interested pers ons after acceptance.   \n5.2 Prompt  for the LLMs   \nTo enable sentiment analysis on this dataset, we designed a standardized prompt to elicit valence \nratings from various Dutch -language large language models (LLMs):  \n\"You are a Dutch language expert analyzing the valence of Belgian Dutch texts. Participants \nresponded to:  \n \n‘What is going on now or since the last prompt, and how do you feel about it?’  \n \nCarefully read the response of the participant: {text}.  Your task is to rate its sentiment from 1 \n(very negative) to 7 (very positive). Return ONLY a single numerical rating enclosed in brackets, \ne.g. [X], with no additional text.   \n \n Output Format: [number]”   \n \nThe placeholders {text} are filled with the texts.    \n \n6. Results:  Performance of LLMs versus Lexical Tools  \nThis section presents the results of the models and their comparison with self -reported valences \nof the users.   \n6.1 Coverage of Valence Predictions  \n--- Page 8 ---\nWe first assessed the practical applicability of each method by evaluating coverage —the \nproportion of texts (N = 24,852) for which models generated a numeric valence score  because \noutput coverage varied substantially across models . Lexicon -based tools demonstrated near -\ncomplete coverage: LIWC and Pattern produced scores for 24,848 texts (99.9%). In stark contrast, \nDutch -tuned LLMs exhibited substantial variability. ChocoLlama -8B-Instruct generated \npredictions for 17,378 texts (69.9 %), GEITje -7B-ultra for 9,445 texts (38.0%), and Reynaerde -\n7B-chat for only 446 texts (1.8%).   These disparities underscore a critical challenge: while lexicon \ntools universally processed inputs, LLMs’ coverage gaps risk introducing selection bias, \ncomplicating fair performance evaluations.   \n 6.2 Alignment with Self -Reported Valence  \nNext, we evaluated how well each model’s predictions align with the user’s/author’s self -reported \nvalence ratings using correlation metrics. Higher correlation meant the model’s predicted \nsentiment closely tracks the writer’s own valence ratings which rang e from –50 for extremely \nnegative sentiment to +50 for extremely positive sentiment. This was done using two correlations: \n(a) Pearson’s r, the standard measure of linear association between two continuous variables (b) \nPolyserial correlation, a statistica l measure used to assess the relationship between an ordinal \nvariable (i.e. the model’s prediction) to a continuous variable (user’s rating). All analyses were \nlimited  to the subset of texts processed by each model.   \n 6.3 Model Performance Evaluation: Lexicon -Based Valence Prediction   \nIn an analysis of valence prediction performance, the following correlation coefficients were \nobserved for lexicon -based methods:  Table  1 compares LIWC’s ( PosEmo and NegEmo), and \nPattern.nl against the user’s self -reported valence. Pattern showed the highest correlation (Pearson \nr = 0.31, Polyserial r = 0.31) when compared to LIWC15 -PosEmo (Pearson r = 0.21, Polyserial r \n= 0.23) and LIWC15 -NegEmo (Pearson r = -0.23, Polyserial r = -0.23). Thus, between LIWC15 \nand Pattern, Pattern turned out to be the robust  lexicon -tool for valence prediction for these Flemish \ntexts.   \nTable 1: Correlation and coverage metrics across sentiment models  \nModel  Coverage (N/%)  Pearson r Polyserial r \nLIWC15 (posemo)  24,848 / 99.9%  0.21 0.23 \nLIWC 15 (negemo)  24, 848 / 99.9%  -0.23 -0.23 \nPattern.nl  24,848 / 99.9%  0.31 0.31 \nChocollama -8B-instruct  17,378 / 69.9%  0.35 0.40 \nGEITje -7B-ultra 9,445 / 38%  0.35 0.44 \nReynaerde -7B-Chat  446 / 1.8%  0.18 0.24 \n \n \n--- Page 9 ---\nComing to the large language models, on the 17,378 texts for which ChocoLlama -8B-Instruct \nreturned a numeric valence score, the Pearson correlation was 0.35 and a polyserial correlation \nwas 0.40 with the user’s self -reported valence. On this same subset of texts, the lexicon baseline \nLIWC yielded substantially lower linear associations. LIWC positive -emotion counts correlated \nat r = 0.23 and LIWC -negative emotion counts at r = -0.23, and th e Pattern composite score at r = \n0.32. Paired significance texts/Pai red samples t -test confirmed that Pattern outperformed the \nLIWC NegEmo feature (t = 10.57, p < 0.001). Crucially, ChocoLlama’s correlation was \nsignificantly higher than Pattern’s (t = 4.02, p < 0.001). Together, these results suggested \nChocoLlama’s stronger alignment with the author’s continuous valence ratings than indivi dual \nLIWC and Pattern for these subset s of texts .  \nSpeaking of the next model, GEITje -7B-ultra, across the 9445 texts it scored, its valence \npredictions exhibited a Pearson correlation of r = 0.35 and a polyserial correlation of 0.44 with the \nuser’s self -reported ratings. When evaluated on this identical s ubset, the lexicon methods delivered \nweak associations: LIWC PosEmo yielded r =0.23, LIWC NegEmo r = -.26, and Pattern r =0.34. \nA paired comparison confirmed that Pattern’s correlation significantly exceeded LIWC’s negative -\nemotion score (t = 7.14, p < 0.0 01), whereas GEITje’s correlation did not significantly differ from \nPattern’s (t = - 1.20, p = 0.23). Thus, although both GEITje and Pattern performed better than the \nindividual LIWC dimensions, GEITje does not provide a significantly reliable improvement over \nthe best lexicon baseline on the texts it has covered.   \nNow coming to Reynarrde -7B-chat, in a comparative analysis for the 446 texts it returned values \nfor, the model achieved a Pearson correlation of 0.18 and a Polyserial correlation of 0.24 with the \nuser self -reported valence scores in a scale from –50 to +50 . These results were benchmarked \nagainst LIWC and Pattern which yielded the following correlations: LIWC -PosEmo: 0.25, LIWC -\nNegEmo: -0.19, Pattern: 0.28. Statistical significance of performance differences was assessed via \npaired t -tests (n = 446). The Pat tern’s method’s correlation did not significantly surpass LIWC -\nNegEmo's (t = 1.48, p = 0.14) or Reynaerde -7B-chat's (t = 1.57, p =0.12). While Pattern exhibited \nnominally higher correlations than Reynaerde, the difference lacked statistical reliability at this \nsample size.   \n To summarize, ChocoLlama -8B-Instruct and GEITje -7B-ultra exhibited higher correlations \n(Pearson r = 0.35) than lexicon tools on their respective subsets, but their incomplete coverage \n(69.9% and 38.0%) limits their applicability. Lexicon methods, by contrast, achieved n ear-\nuniversal coverage (99.9%) with moderate correlations (Pattern: r = 0.31).   \n \n7. Discussion  \nOur study was an attempt to evaluate how well Dutch -finetuned large language models (LLMs) \ncapture emotional valence in daily Flemish narratives compared to lexicon -based tools. While \nLLMs like ChocoLlama -8B-Instruct and GEITje -7B-ultra showed moderate correlations with \n--- Page 10 ---\nhuman ratings on subsets of texts, their performance was constrained by critical limitations: \nincomplete coverage and possibly, a systemic domain mismatch between training data and real -\nworld narratives. In contrast, lexicon -based tools like Pattern.nl, though less contextually nuanced, \nprovided reliable valence estimates across the entire corpus. These findings challenge the \nassum ption that language -specific LLMs inherently outperform lexicon methods in low -resource \nsettings and highlight the need for cult urally tailored solutions.  \nHere we discuss the plausible reasons for the fine -tuned LLMs on Dutch to yield the observed \nresults for our daily narratives Flemish dataset.  Most of the LLMs’ general training data are based \non web and one -third of all data is in English (Mattheus, 2024). For example, Llama -2's pretraining \ndata comprises 89.7% English and only 10.3% non -English,  leaving colloquial and pragmatic \nfeatures of Flemish underrepresented (Li et al., 2024). GPT -3 exhibits a similar skew, with roughly \n92.65% of its tokens in Engli sh, underscoring a systemic English -centric bias across major \nLLMs.   Adaptation efforts like ChocoLlama which incorporated 32 billion Dutch token for \ncontinued pretraining demonstrated sizeable gains on Dutch benchmarks confirming that \nlanguage -specific co rpora are essential to overcome these blind spots (Mattheus, 2024). However, \noverreliance on publicly available corpora (e.g. legal documents, social media texts) imbues LLMs \nwith a formal, technical register that rarely features colloquialism , first -perso n constructions, and \nimplicit emotional cues characteristic of everyday narratives, leading to blind spots and inaccurate \nvalence estimates in informal texts. This domain mismatch undermines LLM performance in terms \nof its accuracy on valence estimation si mply because these models lack exposure to the lexical, \nsyntactic, and pragmatic cues that signal emotion in everyday narratives (Blitzer et al., 2007).   \nIn contrast, LIWC and Pattern.nl are grounded in psycholinguistic or manually curated word lists \nto retain sensitivity to informal affective expressions (Pennebaker et al., 2001; Gatti & van \nStegeren, 2020). LIWC’s validated dictionaries map words directly  onto positive and negative \naffect, producing interpretable scores that align closely with human judgments across registers \n(Pennebaker et al., 2001). Notably, Pattern.nl outperformed LIWC in our study, likely due to its \ncomposite polarity scoring rather t han LIWC’s reliance on raw emotion -word counts.  \nAddressing this gap requires augmenting training data with narrative -style corpora or combining \nLLM outputs with lexicon -driven scores to create hybrid models better suited for daily Flemish \nnarratives.  They are not specifically developed datasets for capturing the valences.  Pretrained \nLLM embeddings offer general distributional semantics learned from massive corpora, which can \nbenefit downstream tasks but also perpetuate the formal -domain bias and embed cultural or social \nprejudices (Bordia & Bowman, 2019) . Supervised fine -tuning on annotated narrative datasets has \nbeen shown to substantially improve valence accuracy by aligning model parameters with the \ntarget register’s features (Demszky, 2023). However, this process demands sizeable, high -quality \nannotations of Flemish narratives, data that are currently scarce. Furthermore, LL Ms risk \namplifying biases in sentiment judgments, differentiating affective language based on gendered or \ndemographic stereotypes unless rigorous bias auditing and inclusive data cu ration practices are \nenforced (Bordia & Bowman, 2019). Future research should prioritize the creation of standardized \n--- Page 11 ---\nFlemish narrative valence benchmarks based on manually annotated corpora of everyday Flemish \ntexts that enable systematic evaluation of LLM -based approaches (Augustyniak et al., 2023)  which \nhave the potential to bridge computational and psychological research . This urgency is amplified \nin mental health research, where daily narratives , critical for capturing nuanced emotional states,  \nremain underrepresented in NLP studies . Until LLMs overcome domain mismatches and achieve \ncomparable robustness, lexicon tools remain indispensable for scalable, ecologically valid valence \nanalysis in Flemish.  \n8. Conclusion   \nThis study demonstrates that lexicon -based tools like Pattern.nl remain the gold standard for \nvalence analysis in low -resource Flemish, outperforming Dutch -tuned LLMs in coverage and \nfairness despite their contextual limitations.  Moving forward , establishing standardized \nbenchmarks for Flemish valence analysis , grounded in manually annotated, ecologically valid \nnarratives  will enable targeted improvements in data augmentation, bias mitigation, and hybrid \nmodeling. Simultaneously, the call for personalized L LMs (Nature Machine Intelligence, 2024) \nhighlights a critical tension: while LLMs cannot replicate human empathy, their integration with \nlexicon -driven frameworks may bridge the gap between scalable automation and culturally \ngrounded emotion research. Unti l such synergies are realized, lexicon tools will continue to anchor \nvalence analysis in low -resource contexts, ensuring both fairness and methodological rigor.  \n9. Limitations   \nWhile this study advances understanding of valence analysis in Flemish, its scope is constrained \nby several factors. First, Dutch and Flemish remain critically under -represented in both training \ndata and available LLMs, reflecting broader systemic gaps in low-resource language NLP. Though \nrecent efforts (Mattheus, 2024) aim to bridge this gap, developing state -of-the-art systems for \nFlemish is hampered by sparse language resources, limited computational infrastructure, and a \nshortage of technically proficie nt users , challenges endemic to under -resourced languages. Our \nevaluation of three Dutch -tuned LLMs, while informative  does not  account for  larger models  (e.g., \nLLaMA -2 70B) , newer architectures or hybrid approaches that may outperform current \nbenchmarks. Second, while our corpus of ~25,000 narratives provides  rich ecological validity, its \nreliance on a single prompt structure risks conflating valence expression with elicitation method, \nlimiting generalizability to other narrative contexts.  Finally, while lexicon tools circumvent LLMs’ \ntechnical limitations, they lack contextual nuance (e.g., resolving sarcasm or cultural idioms) and \ncannot replicate human empathy , a gap underscored by critiques of LLMs’ inability to interpret \nemotional subtext ( Nature Machine Intelligence, 2024 ). \n \n  \n--- Page 12 ---\nREFERENCES  \nAlexandridis, G., Varlamis, I., Korovesis, K., Caridakis, G., & Tsantilas, P. (2021). A survey on \nsentiment analysis and opinion mining in Greek social media. Information, 12 , Article 331. \nhttps://doi.org/10.3390/info12080331  \nAli, H., Hashmi, E., Yildirim, S. Y ., & Shaikh, S. (2024). Analyzing Amazon products sentiment: \nA comparative study of machine and deep learning, and transformer -based techniques. \nElectronics, 13 (7), Article 1305.  \nAugustyniak, Ł., Woźniak, S., Gruza, M., Gramacki, P., Rajda, K., Morzy, M., & Kajdanowicz, \nT. (2023). Massively multilingual corpus of sentiment datasets and multi -faceted sentiment \nclassification benchmark. arXiv Preprint  arXiv:2306.07902. https://arxiv.org/abs/2306.07902  \nBlitzer, J., Dredze , M., & Pereira, F. (2007). Biographies, Bollywood, boom -boxes: Domain \nadaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the \nAssociation for Computational Linguistics  (pp. 440 –447). Association for Computational \nLinguistics.  \nBoot, P., Zijlstra, H., & Geenen, R. (2017). The Dutch translation of the Linguistic Inquiry and \nWord Count (LIWC) 2007 dictionary. Dutch Journal of Applied Linguistics, 6 (1), 65 –76. \nBordia, S., & Bowman, S. R. (2019). Identifying and reducing gender bias in word -level \nlanguage models. In Proceedings of the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Student Research Workshop  (pp. 7 –15). Association \nfor Computational Linguistics. https://doi.org/10.18653/v1/N19 -3002  \nChun, J. (2021). SentimentArcs: A novel method for self -supervised sentiment analysis of time \nseries shows SOTA transformers can struggle finding narrative arcs. arXiv Preprint  \narXiv:2110.09454. https://arxiv.org/abs/2110.09454  \nDashtipour, K., Gogate, M., Adeel, A., Larijani, H., & Hussain, A. (2021). Sentiment analysis of \nPersian movie reviews using deep learning. Entropy, 23 , Article 596.  \nDe Smedt, T., & Daelemans, W. (2012). Pattern for Python. Journal of Machine Learning \nResearch, 13 , 2063 –2067.  \nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt, J. C., \nHecht, C., Jamieson, J., Johnson, M., Jones, M., Krettek -Cobb, D., Lai, L., Mitchell, N. J., Ong, \nD. C., Dweck, C. S., Gross, J. J., & Pennebaker, J. W. (2023 ). Using large language models in \npsychology. Nature Reviews Psychology, 2 (11), 688 –701. \nFiok, K., Karwowski, W., Gutierrez, E., & Wilamowski, M. (2021). Analysis of sentiment in \ntweets addressed to a single domain -specific Twitter account: Comparison of model performance \nand explainability of predictions. Expert Systems with Applications, 186 , 115771.  \n--- Page 13 ---\nFrisina, P. G., Borod, J. C., & Lepore, S. J. (2004). A meta -analysis of the effects of written \nemotional disclosure on the health outcomes of clinical populations. The Journal of Nervous and \nMental Disease, 192 (9), 629 –634. \nGatti, L., & van Stegeren, J. (2020). Improving Dutch sentiment analysis in Pattern. \nComputational Linguistics in the Netherlands Journal, 10 , 73–89. \nHutto, C. J., & Gilbert, E. (2014). V ADER: A parsimonious rule -based model for sentiment \nanalysis of social media text. In Proceedings of the International AAAI Conference on Web and \nSocial Media  (V ol. 8, pp. 216 –225). AAAI Press.  \nLi, Z., Shi, Y ., Liu, Z., Yang, F., Payani, A., Liu, N., & Du, M. (2024). Quantifying multilingual \nperformance of large language models across languages. arXiv Preprint  arXiv:2404.11553. \nhttps://arxiv.org/abs/2404.11553  \nLigthart, A., Catal, C., & Tekinerdogan, B. (2021). Systematic reviews in sentiment analysis: A \ntertiary study. Artificial Intelligence Review, 54 , 4997 –5053.  \nLiu, Z., Zhang, T., Yang, K., Thompson, P., Yu, Z., & Ananiadou, S. (2023). Emotion detection \nfor misinformation: A review. arXiv Preprint  arXiv:2311.00671. https://arxiv.org/abs/2311.00671  \nLiu, Z., Yang, K., Xie, Q., Zhang, T., & Ananiadou, S. (2024). EmoLLMs: A series of emotional \nlarge language models and annotation tools for comprehensive affective analysis. In Proceedings \nof the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining  (pp. 5487 –\n5496). ACM.  \nMartínez, G., Molero, J. D., González, S., Conde, J., Brysbaert, M., & Reviriego, P. (2025). \nUsing large language models to estimate features of multi -word expressions: Concreteness, \nvalence, arousal. Behavior Research Methods, 57 , Article 5. https://doi.org/10.3758/s13428 -024-\n02515 -z \nMeeus, M., Rathé, A., Rémy, F., Delobelle, P., Decorte, J. -J., & Demeester, T. (2024). \nChocoLlama: Lessons learned from teaching Llamas Dutch. arXiv Preprint  arXiv:2412.07633. \nhttps://arxiv.org/abs/2412.07633  \nMishra, K., Priya, P., Burja, M., & Ekbal, A. (2023). e -THERAPIST: I suggest you to cultivate a \nmindset of positivity and nurture uplifting thoughts. In Proceedings of the 2023 Conference on \nEmpirical Methods in Natural Language Processing  (pp. 13952 –13967). Association for \nComputational Linguistics.  \nMohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word –emotion association lexicon. \nComputational Intelligence, 29 (3), 436 –465. \nMuhammad, S., Abdulmumin, I., Ayele, A., Ousidhoum, N., Adelani, D., Yimam, S., Ahmad, I., \nBeloucif, M., Mohammad, S., Ruder, S., Hourrane, O., Jorge, A., Brazdil, P., Ali, F., David, D., \nOsei, S., Shehu -Bello, B., Lawan, F., Gwadabe, T., Rutunda, S., … Op oku, B. (2023). AfriSenti: \n--- Page 14 ---\nA Twitter sentiment analysis benchmark for African languages. In Proceedings of EMNLP 2023 . \nAssociation for Computational Linguistics.  \nObradovich, N., Khalsa, S. S., Khan, W. U., … (2024). Opportunities and risks of large language \nmodels in psychiatry. NPP —Digital Psychiatry Neuroscience, 2 , Article 8.  \nPataranutaporn, P., Liu, R., Finn, E., … (2023). Influencing human -AI interaction by priming \nbeliefs about AI can increase perceived trustworthiness, empathy and effectiveness. Nature \nMachine Intelligence, 5 , 1076 –1085.  \nPennebaker, J. W. (1997). Writing about emotional experiences as a therapeutic process. \nPsychological Science, 8 (3), 162 –166. \nPennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Linguistic Inquiry and Word Count: \nLIWC 2001 . Lawrence Erlbaum Associates.  \nPennebaker, J. W., Booth, R. J., & Francis, M. E. (2007). Linguistic Inquiry and Word Count \n(LIWC2007): Operator’ s manual . LIWC.net.  \nPennebaker, J. W., Booth, R. J., Boyd, R. L., & Francis, M. E. (2015). Linguistic Inquiry and \nWord Count: LIWC2015 . Pennebaker Conglomerates.  \nQiu, J., Lam, K., Li, G., … (2024). LLM -based agentic systems in medicine and healthcare. \nNature Machine Intelligence, 6 , 1418 –1420.  \nRadaideh, M. I., Hwang, O. H., & Radaideh, M. I. (2024). Fairness and social bias quantification \nin large language models for sentiment analysis. SSRN Preprint . \nhttps://doi.org/10.2139/ssrn.XXXXXX  \nReBatch. (2024). Reynaerde -7B-Chat  [Large language model]. Hugging Face. \nhttps://huggingface.co/ReBatch/Reynaerde -7B-Chat  \nReusens, M., Reusens, M., Callens, M., Vanden Broucke, S., & Baesens, B. (2022). Benchmark \nstudy for Flemish Twitter sentiment analysis. SSRN Preprint . https://ssrn.com/abstract=4096559  \nRoberts, M. E., Stewart, B. M., Tingley, D., Lucas, C., Leder -Luis, J., Gadarian, S. K., Albertson, \nB., & Rand, D. G. (2014). Structural topic models for open -ended survey responses. American \nJournal of Political Science, 58 (4), 1064 –1082. https://doi.org/10.1111/ajps.12103  \nShteynberg, G., Halpern, J., Sadovnik, A., … (2024). Does it matter if empathic AI has no \nempathy? Nature Machine Intelligence, 6 , 496 –497. \nTausczik, Y . R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and \ncomputerized text analysis methods. Journal of Language and Social Psychology, 29 (1), 24 –54. \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S., \nBhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton Ferrer, C., Chen, M., Cucurull, G., et al. \n--- Page 15 ---\n(2023). Llama 2: Open foundation and fine -tuned chat models. arXiv Preprint  arXiv:2307.09288. \nhttps://arxiv.org/abs/2307.09288  \nVanroy, B. (2024). GEITje 7B Ultra: A conversational model for Dutch. arXiv Preprint  \narXiv:2412.04092. https://arxiv.org/abs/2412.04092  \nVarghese, C., Harrison, E. M., O’Grady, G., … (2024). Artificial intelligence in surgery. Nature \nMedicine, 30 , 1257 –1268.  \nZhang, W., Deng, Y ., Liu, B., Pan, S., & Bing, L. (2024). Sentiment analysis in the era of large \nlanguage models: A reality check. In Findings of the Association for Computational Linguistics: \nNAACL 2024  (pp. 3881 –3906). Association for Computational Linguistics.  \n \n \n \n \n \n \n \n  \n--- Page 16 ---\nSupplementary Material  \nA. Data Preparation  \nParticipants were recruited from the community via flyers, online advertisements, and word -of-\nmouth referrals. Eligibility criteria included being native Dutch speakers, residing in Belgium, \nbeing at least 18 years of age, and owning a fully functional sma rtphone. Interested individuals \ncompleted an initial eligibility survey, with eligibility criteria verified during a subsequent online \nintroduction session.  \nParticipants received compensation of up to €250 for participating in a 70 -day (10 -week) \nexperience sampling protocol, accompanied by biweekly online surveys. The breakdown included \n€0.50 per completed experience sampling prompt (maximum of 280 prompts or €140 total), €10 \nfor each short survey at 2, 4, 6, and 8 weeks (maximum €40), and €15 for each long survey at \nbaseline and 10 weeks (maximum €30). An additional bonus of €40 was given to participants \ncompleting at least 60 days. Payment was issued in full upon study completion. Continued \nparticipation required completion of at least 75 \\% of prompts, with verbal responses having a \nminimum length of 25 words. Regular compliance checks and biweekly summary reports were \nprovided to participants, with reminders issued as necessary.  \nInitially, 115 participants enrolled (age range: 18 –65, M = 27.26, SD = 9.86; gender: 58 female, \n56 male, 1 other). Of these, 10 voluntarily withdrew, and 3 were dismissed due to poor compliance \n(response rates below 50 \\%). Thus, 102 participants completed  the study (age range: 18 –65, M = \n26.47, SD = 8.87; 52 women, 49 men, 1 other).  \nEthical approval was obtained from the KU Leuven Social and Societal Ethics Committee \n(SMEC), protocol G -2023 -6379 -R3(AMD). Data collection spanned from August 2023 to July \n2024. All study materials and instructions were administered in Dutch.  \nA.1 Procedure and Materials  \nParticipants engaged in a 70 -day experience sampling protocol via a dedicated mobile application \n(m-Path; Mestdagh et al., 2023), receiving four prompts daily at pseudorandom intervals between \n9 AM and 9 PM, spaced at least one hour apart. At each prompt, participants responded to \"What \nis going on now or since the last prompt, and how do you feel about it?\" Responses were typically \nrecorded as 1 -minute voice messages but could optionally be typed (3 -4 sentences). Participants \nthen rated their current emoti onal valence on a slider scale ranging from -50 (very unpleasant) to \n+50 (very pleasant). Simultaneously, m -Path recorded sensor data (GPS coordinates, ambient \nnoise, step counts, and app usage for Android devices).  \nAll assessment tools were validated previously in Dutch -speaking samples. Participants completed \nadditional online surveys after 2, 4, 6, and 8 weeks, and a comprehensive survey at 10 weeks, \nwhich included validation questions about recent visited location s. \n \n--- Page 17 ---\nParticipants received biweekly compliance summary reports via email, detailing prompt \ncompletion rates, description length adequacy, modality (voice vs. text), valence ratings, linguistic \ncontent trends (using LIWC 2015 Dutch translation), and accumulated compensation. These \nreports also indicated whether participants earned bonuses and their overall compensation.  \nV oice -recorded responses were automatically transcribed using a proprietary algorithm developed \nby the Department of Electrical Engineering (ESAT) at KU Leuven (Tamm et al., 2024). \nTranscripts and typed responses were integrated into a unified data file, a ligned manually with \nonline survey data into five study intervals: days 1 –14, 15 –28, 29 –42, 43 –56, and 57 –70. Missing \nsurvey data were noted where applicable.  \nA.1.2 Analytic Tools and Experimental Setup  \nTextual responses were analyzed using Linguistic Inquiry and Word Count (LIWC) with its Dutch \nlexicon, generating category -specific linguistic scores for each response. Additionally, the \nPattern.nl sentiment analysis algorithm provided continuous valence e stimates from -1 (very \nnegative) to +1 (very positive), accounting for word context and grammatical function. Model \nevaluations were performed using a NVIDIA RTX -5000 GPU and implemented in PyTorch, with \nmodels obtained via Hugging Face APIs.  \n \nA.2 Single Participant (Pilot Study)  \nWe conducted a preliminary pilot study involving one participant’s data from our dataset to identify \nthe best performing models. We also compared the performances of these models with English \nprompt and Dutch prompt.  \nEnglish Prompt:  \"You are a Dutch language expert analyzing the valence of Belgian Dutch texts. \nParticipants responded to: \"What is going on now or since the last prompt, and how do you feel \nabout it?\"   \nCarefully read the response of the participant: \"text\".  \nYour task is to rate its sentiment from 1 (very negative) to 7 (very positive).  \nReturn ONLY a single numerical rating enclosed in brackets, (e.g. [X]), with no additional text, \nexplanations, or formatting.    \nOutput Format: [number] .  \nReplace \"number\" with the integer score (1 -7).\" \nDutch  Prompt:  \"Je bent een Nederlandse taalexpert die de valentie van Belgisch Nederlandse \nteksten analyseert. Deelnemers reageerden op:  \n--- Page 18 ---\n“Wat speelt er nu of sinds de vorige beep en hoe voel je je daarover?”. Lees zorgvuldig het \nantwoord van de deelnemer: “text”.  \nHet is jouw taak om het sentiment te beoordelen van 1 (zeer slecht) tot 7 (zeer goed). Geef \nALLEEN een cijfer tussen haakjes (bijv. [X]), zonder extra tekst, uitleg of opmaak. Niet uitleggen. \nUitvoerformaat: [getal]. Vervang “getal” door de gehele score (1 -7).\" \nAs illustrated in Table 1, we selected ChocoLlama -8B-Instruct, GEITje -7B-ultra, Reynaerde -7B-\nChat, and Llama 2 -7B. We also ran the dataset with LIWC15 and Pattern. The models returned \nvalues for all the texts. Among these LLMs, when prompted in English, Ch ocoLlama -8B-Instruct \nachieved the highest polyserial correlation (r = 0.55) with the user’s ratings followed by GEITje -\n7B-ultra (r = 0.42) and Reynaerde -7B-Chat (r = 0.33). Surprisingly, Llama -2-7B returned no values \nfor all the texts when prompted in Engl ish but performed better than GEITje -7B-ultra (r = 0.0002) \nand Reynaerde -7B-Chat (r = 0.42). To maintain uniformity, we decided to proceed with only \nEnglish prompt for further experiments. Between LIWC15 and Pattern.nl, LIWC15 showed strong \nnegative (negem o: r = -0.54) and positive (posemo: r = 0.41) correlations, and Pattern.nl showed \npositive correlation with the user’s ratings (r = 0.44).   \nTable  1. Correlation coefficients across models and prompts  \nModel (variable)  Prompt  Pearson r Polyserial r \nLIWC15 (posemo)  - 0.41 - \nLIWC15 (negemo)  - -0.54 - \nPattern.nl  - 0.44 - \nChocoLlama -8B-Instruct  English  0.47 0.55 \nChocoLlama -8B-Instruct  Dutch  0.18 0.27 \nLlama -2-7B English  N/A N/A \nLlama -2-7B Dutch  0.37 0.46 \nGEITje -7B-ultra English  0.33 0.42 \nGEITje -7B-ultra Dutch  0.0001  0.0002  \nReynaerde -7B-Chat  English  -0.1 0.33 \nReynaerde -7B-Chat  Dutch  0.2 0.42 \n \nIn the following sections, we present the additional experiments conducted and the results \nobserved for the entire dataset using the English prompt:  \n \n \n \n \n \n--- Page 19 ---\nA.3 Distribution of Ratings (All Participants/Users, LIWC, Pattern.nl)  \n \nFig 1. Distribution of users’ self-reported valence ratings  \n  \n  \nFig 2. Distribution of LIWC’s scores  \n \n \nFig 3. Distribution of Pattern .nl’s polarity  \n  \n\n--- Page 20 ---\nA.4 Zero -shot setting results (All participants)  \nIn the zero -shot condition, as discussed previously in the Results section, the models did not return \nvalues for all the texts, unlike LIWC and Pattern. ChocoLlama -8B-Instruct returned values for \nonly 17378 texts, GEITje -7B-ultra for 9445 texts, and Reynae rde-7B-Chat for 446 estimates only. \nPattern.nl turned out to be performing better on most of the texts than these models.  \nPolyserial correlation analyses revealed significant relationships between user ratings and the \noutputs of all three language models tested, with all user -model comparisons yielding p -values < \n0.001. However, correlations between Reynaerde and traditional sentiment tools varied: LIWC15 \n(posemo) showed no significant association (p = 0.12), while LIWC15 (negemo) approached \nsignificance (p < 0.05) but did not meet the threshold for statistical reliability. In contrast, Pattern.nl \ndemonstrated a modest yet sta tistically significant correlation with Reynaerde (p = 0.022). GEITje \nshowed strong, statistically significant correlations with all lexicon -based models -LIWC15 \n(posemo and negemo) and Pattern.nl (all p < 0.001), indicating robust agreement. Similarly, \nChocoLlama's outputs correlated significantly with LIWC15 and Pattern.nl (all p < 0.001).  \n \nA.4.1 Distributional Analysis of the Models’ Ratings  \nChocoLlama -8B-Instruct showed a relatively balanced distribution compared to the other two \nLLMs (peaking around 3 and 5). GEITje -7B-ultra's ratings were skewed towards scores around 5. \nReynaerde -7B-Chat's predictions had the narrowest distribution amongst all these three models.  \n \nFig 4. Distribution of Chocollama -8B-instruct's predicted valence scores  \n \n\n--- Page 21 ---\n \nFig 5. Distribution of Reynaerde -7B's valence scores  \n \nFig 6. Distribution of GEITje -7B's valence scores  \n \n  \n\n--- Page 22 ---\nA.4.2 Box Plot Analysis of Users’ Ratings vs. Model’s Predictions  \n \nFig 7. Chocollama -8B-instruct  \n \nFig 8. Reynaerde -7B \n \nFig 9. GEITje -7B \n \n\n--- Page 23 ---\nA.4.3  Scatter  Plots \n \nFig 10. Chocollama -8B-instruct  \n \n \nFig 11. Reynaerde -7B \n \nFig 12. GEITje -7B \n\n--- Page 24 ---\nA.5 Few-shot setting results (All participants)  \nWe also attempted  a few-shot setting  by providing five examples from the text along with the \nuser ratings.  \nA.5.1 Prompt used  \nIn this prompt, “(Text)” has been replaced with the actual responses of the participants.  \n\"You are a Dutch language expert analyzing the valence of Belgian Dutch texts. Participants \nresponded to the question:  \n“What is going on now or since the last prompt, and how do you feel about it?”   \nThey also rated their own emotional valence on a continuous scale from -50 (very negative) to \n+50 (very positive). Your task is to read each response: {text} and rate its sentiment from 1 (very \nnegative) to 7 (very positive). Return ONLY a single numerical  rating enclosed in square \nbrackets, e.g. [X]. Provide no explanation or additional text.  \nOutput Format: [number].    \n \nBelow are a few examples of an input and the participant’s valence rating to guide your rating:   \n \nInput: (Text1)   \nOutput: [10]   \n \nInput: (Text2)  \nOutput: [ -10]   \n \nInput: (Text3)   \nOutput: [30]   \n \nInput: (Text4)   \nOutput: [45]   \n \nInput: (Text5)  \nOutput: [40]”   \n \nWe observed a similar trend in the LLMs chosen returned values for only a portion of the texts. \nPattern.nl proved to be more effective than LIWC.  \n \nTable 2. Few -shot coverage and correlation coefficients across models and benchmarks wrt users  \nModel  Coverage (N/%)  Pearson r Polyserial r \nLIWC15 (posemo)  24,848 / 99.9%  0.21 0.23 \nLIWC 15 (negemo)  24, 848 / 99.9%  -0.23 -0.26 \nPattern.nl  24,848 / 99.9%  0.31 0.33 \n--- Page 25 ---\nReynaerde -7B-Chat \n(English prompt)  7,219 / 29.0%  0.03 0.036  \nGEITje -7B-ultra (English \nPrompt)  11,323 / 45.6%  -0.08 -0.09 \nChocollama -8B-instruct \n(English prompt)  5,266 / 21.2 %  -0.03 -0.029  \n \nIn the few -shot setting, we provided five example texts with user ratings. Significance values (p -\nvalues) from polyserial correlation analyses were as follows:  \n \nA.5.2 Polyserial Correlation Between the models  \nTable 3. Polyserial correlation and significance for model -model comparison  \nComparison  r p Significance  \nLIWC15 ( posemo) vs Reynaerde -7B-Chat  0.11 < 0.001  significant  \nLIWC 15 (negemo) vs Reynaerde -7B-Chat  -0.004  0.916  non-significant  \nPattern vs Reynaerde -7B-Chat  0.044  0.001  significant  \n    \nLIWC15 (posemo) vs GEITje -7B-ultra 0.068  0.003  significant  \nLIWC 15 ( negemo) vs GEITje -7B-ultra 0.131  <0.001  significant  \nPattern vs GEITje -7B-ultra -0.05 0.7 non-significant  \n    \nLIWC15 (posemo) vs Chocollama -8B-instruct  -0.052  0.479  non-significant  \nLIWC 15 (negemo) vs Chocollama -8B-instruct  -0.004  0.001  significant  \nPattern vs Chocollama -8B-instruct  -0.046  0.940  non-significant  \n \nA.5.3 Distributional Analysis of the Models’ Ratings  \n \nFig 13. Distribution of Chocollama -8B-instruct's predicted valence scores  \n\n--- Page 26 ---\n \nFig 14. Distribution of Reynaerde -7B's valence scores  \n \n \nFig 15. Distribution of GEITje -7B's valence scores  \n \nA.5.4 Box Plots of Users’ Ratings vs. Model’s Predictions  \n \nFig 16. Chocollama -8B-instruct  \n\n--- Page 27 ---\n \nFig 17. Reynaerde -7B \n \n \nFig 18. GEITje -7B \nA.5.5 Scatter Plots  \n \nFig 19. Chocollama -8B-instruct  \n\n--- Page 28 ---\n \nFig 20. Reynaerde -7B \n \n \nFig 21. GEITje -7B \n \nA.6 References  \nMestdagh, M., Verdonck, S., Piot, M., Niemeijer, K., Kilani, G., Tuerlinckx, F., Kuppens, P., and \nDejonckheere , E. (2023). m -Path: an easy -to-use and highly tailorable platform for ecological \nmomentary assessment and intervention in behavioral research and clinical practice. Frontiers in \nDigital Health , 5. https://doi.org/10.3389/fdgth.2023.1182175  \nTamm, B., Poncelet, J., Barberis, M., Vandermosten, M., and Van hamme, H. (2024). Weakly \nsupervised training improves Flemish ASR of non -standard speech. Frontiers in Digital Health , 5. \nhttps://doi.org/10.3389/fdgth.2023.1182175",
  "text_length": 56738
}