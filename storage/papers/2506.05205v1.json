{
  "id": "http://arxiv.org/abs/2506.05205v1",
  "title": "RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition",
  "summary": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions.",
  "authors": [
    "Jackson Petty",
    "Michael Y. Hu",
    "Wentao Wang",
    "Shauli Ravfogel",
    "William Merrill",
    "Tal Linzen"
  ],
  "published": "2025-06-05T16:17:24Z",
  "updated": "2025-06-05T16:17:24Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05205v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05205v1  [cs.CL]  5 Jun 2025RELIC: Evaluating Compositional\nInstruction Following via Language Recognition\nJackson Pettyλ∗Michael Y. HuδWentao Wangδ\nShauli RavfogelδWilliam MerrillδTal Linzenλδ\nDepartment of LinguisticsλCenter for Data Scienceδ\nNew York University\nAbstract\nLarge language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of inputs\nand outputs; this ability is referred to as instruction following. We introduce the\nRecognition of Languages In-Context ( RELIC ) framework to evaluate instruction\nfollowing using language recognition: the task of determining if a string is generated\nby formal grammar. Unlike many standard evaluations of LLMs’ ability to use their\ncontext, this task requires composing together a large number of instructions (gram-\nmar productions) retrieved from the context. Because the languages are synthetic,\nthe task can be increased in complexity as LLMs’ skills improve, and new instances\ncan be automatically generated, mitigating data contamination. We evaluate state-of-\nthe-art LLMs on RELIC and find that their accuracy can be reliably predicted from\nthe complexity of the grammar and the individual example strings, and that even the\nmost advanced LLMs currently available show near-chance performance on more\ncomplex grammars and samples, in line with theoretical expectations. We also use\nRELIC to diagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases, models\nswitch to relying on shallow heuristics instead of following complex instructions.\nS -> A B\nA -> C D\nB -> E F\nC -> ‘t43’\nD -> ‘t51’\nE -> ‘t66’\nF -> ‘t72’Context-Free \nGrammars\nt43 t51 t66 t72\nt51 t43 t66 t72Positive\nExamples\nNegative\nExamplesIs the string “ t51 t43 t66 t72 ” \ngenerated by the the following \ngrammar: {S -> A B, …, F -> \n‘t72’} ?\nYes.\na b c\nFigure 1: RELIC is an evaluation framework for compositional instruction following , where we\n(a)stochastically generate context-free grammars (e.g., sets of instructions) of given complexities,\n(b)sample positive and negative strings (e.g., tasks) for each grammar, and (c)prompt models to\nclassify whether the strings are generated by those grammars.\n1 Introduction\nLarge language models (LLMs) are increasingly expected to solve tasks “zero-shot,” using only a\nspecification of the task provided in the LLM’s context window without examples of inputs and outputs\n∗Correspondence to research@jacksonpetty.org .\nPreprint. Under review.\n--- Page 2 ---\nor any additional training. These instructions are often compositional , involving the interaction of\nmultiple rules or constraints. Even a simple question such as “using my credit card statement, tell me if\nI spent more money in 2025 than I did on average in the past three years” (inspired by Zhou et al. 2024)\nrequires an LLM to retrieve a large number of items from its context and perform multiple operations\nin a particular sequence (in this example, summing and averaging them and comparing the outputs). As\nthe complexity of such instructions increases, the system needs to use its context in a sophisticated way\nto identify the relevant instructions and combine them appropriately. The importance of compositional\ninstruction following highlights the need for a reliable benchmark with controllable complexity that\ncan measure LLMs’ abilities in this area as they improve in the future.\nTo this end, we propose the Recognition of Languages In-Context ( RELIC ) framework, which\nevaluates if the model can determine whether a given string is derivable from a grammar describing\na formal language; to succeed, the model needs to apply multiple rules in an order that is not provided\nin advance, potentially applying each rule multiple times as a complex compositional tree, which we\ntake as a proxy for instruction following more generally. This task is akin to determining if a sentence\nis grammatical given a reference grammar for a natural language, or if a program is syntactically\ncorrect given a specification of the programming language. RELIC is a data generating process that\nstochastically samples new grammars of desired complexity and produces unlimited amounts of new\nevaluation examples for these grammars; this property of the benchmark addresses issues of data\ncontamination and benchmark saturation which plague static benchmarks.\nWe focus on grammars of context-free languages (CFLs), which mimic key aspects of the compositional\nstructure of both natural languages (Chomsky and Schützenberger, 1963) and programming languages.\nThe context-free grammars we generate consist of production rules of two kinds: A→B C, where A,\nB, and Care nonterminal symbols; and A→‘t’, where tis a vocabulary symbol. For each grammar,\nwe sample positive strings from the grammar and negative strings from the vocabulary, and prompt\nan LLM to determine whether the strings can be generated by the grammar. The difficulty of the\ntask can be modulated by varying the size of the grammar and the length of the strings that need to\nbe recognized. Because the asymptotic computational complexity of in-context CFL recognition is\nlikely beyond the capabilities of transformers (Greenlaw et al., 1991; Merrill and Sabharwal, 2023), we\nexpect this task to be challenging for transformer-based LLMs, which may motivate new architectures.\nUsing this framework, we generate and release a first static benchmark, RELIC -500, which consists of\n200 grammars which vary in complexity (up to 500 nonterminal production rules), and sets of positive\nand negative examples for each grammar (of lengths up to 50 symbols). We evaluate a range of LLMs\nonRELIC -500, including smaller open-weights models (Gemma 3 and a distilled version of DeepSeek-\nR1) and models from OpenAI ( o3,o4-mini and the gpt-4.1 family). Most of the models performed\nsubstantially above chance on simpler grammars, indicating an understanding of the general structure\nof the task; but all models, including OpenAI’s most advanced reasoning model o3, had near-chance\naccuracy on the larger grammars, which are still orders of magnitude smaller than the grammars needed\nto define commonly-used programming languages or approximate human languages. In a qualitative\nanalysis of the chain-of-thought tokens that LLMs produce before providing their response, we find\nthat in many cases models rely on incorrect heuristics; quantitative analysis of these tokens shows that\nthe amount of tokens does not grow as would be expected from the correct strategy to perform the task.\nThese results motivate using RELIC to assess LLMs’ compositional instruction following capabilities,\nand indicate substantial headroom on this task. More generally, our findings illustrate the potential\nfor using formal languages to produce evaluation datasets that are resistant to data contamination,\nand whose complexity can be precisely controlled and scaled as LLMs’ capabilities improve.\n2 Related Work\n2.1 Evaluating In-Context Instruction Following\nLLMs are typically fine-tuned to follow instructions (i.e. specifications of the task provided in context),\nand can generalize this skill to new instructions that were not included in fine-tuning (Sanh et al.,\n2022; Wei et al., 2022). It is often difficult, however, to automatically verify whether the model indeed\nfollowed the instructions; this concern is addressed in our formal language recognition setting. Like\nour work, the IFEval benchmark (Zhou et al., 2023) addresses the verification issue by placing formal\nconstraints on the output (e.g., avoiding commas), but it does not evaluate complex and long instructions\n2\n--- Page 3 ---\nas we do. Our focus on evaluating compositionality is also related to work on compositionality in\nquestion answering (Dziri et al., 2023; Press et al., 2023) and semantic parsing (Kim and Linzen, 2020).\nAs LLMs’ context windows have increased in size, there has been growing interests in methods to\nevaluate how well they in fact use their context. A simple method is the “needle-in-the-haystack”\nevaluation introduced by Kamradt (2023); in these evaluations, a specific piece of information is placed\nin context and a model is asked to retrieve it (see also Hsieh et al. 2024; Bohnet et al. 2024; Zhang\net al. 2024; Arora et al. 2023). While needle-in-the-haystack benchmarks evaluate LLMs’ ability to\nretrieve information from their context, they do not test for the model’s ability to compose different\nparts of the context or execute computations specified within it.\nRecent evaluations proposed to remedy this limitations include NeedleBench (Li et al., 2024) and\nMichelangelo (V odrahalli et al., 2024); in these tasks, the model is expected to reason over multiple\ndifferent pieces of information, each of which needs to be retrieved from context. Michelangelo is\nclosest to our work in that it is automatically generated, addressing complexity scaling and dataset\nleakage issues. But the tasks included in their benchmark are much simpler than CFL recognition; the\nmain challenge they present for the model is to ignore irrelevant information in the context. Finally, in\nthe realm of natural (non-formal) languages, Tanzer et al. (2023) provide an entire reference grammar\nand word-list for the Kalamang natural language in-context, and test if the LLM can follow the rules\nprovided in the grammar to translate sentences from English to Kalamang; this setting differs from ours\nin that the reference grammar they used includes not only grammar rules but also parallel examples in\nKalamang and English, and in the fact that the correctness of such LLM generations cannot be reliably\nassessed using non-heuristic measures.\n2.2 LLMs and Formal Language Recognition\nA number of studies have evaluated LLMs’ ability to recognize formal languages (Finlayson et al., 2022;\nBhattamishra et al., 2020; Delétang et al., 2023; Butoi et al., 2024). For example, Finlayson et al. (2022)\nevaluated the ability of T5 (Raffel et al., 2019), a transformer model, to recognize regular expressions,\nand found that the model’s performance degraded as languages became more complex. Our study\ndiffers from theirs both in that we study a richer class of languages, where recognition requires more\ncompositional computation, and in that we evaluate zero-shot instruction following, without fine-tuning\nthe model to recognize a particular grammar. In this sense, our work is more similar to that of Gupta et al.\n(2025), who evaluated LLMs on an in-context version of transduction and completion tasks for regular\nlanguages, finding that some regular languages can pose challenges for currently-available LLMs. Our\nwork is also distinct from studies where models are expected to induce the grammar of a language from\npositive examples (Akyürek et al., 2024; Hua et al., 2025; Qiu et al., 2024; Bhattamishra et al., 2023).\n3 RELIC: Recognizing Languages In-Context\nRELIC is a framework for generating synthetic benchmarks: it can be used to generate new evaluation\ninstances of increasing complexity that are extremely unlikely to have been observed by the model in\ntraining. In this paper, in addition to implementing this process in a codebase, we also generate a static\nbenchmark using this process, and evaluate contemporary language models on the benchmark.\nFollowing Clark (2017, 2018), we stochastically generate grammars which are parameterized by four\nvalues: the number of terminal symbols nterm, non-terminal symbols nnonterm , lexical production rules\nnlex, and non-lexical production rules nnonlex . We start by defining ntermsymbols Σ={t1,t2,...}; and\nnnonterm symbols V={NT1,NT2,...}. We then sample nlexlexical production rules NTa→‘tb’from the\nset of pairs V×Σ, and we sample nnonterm non-lexical production rules NTa→NTbNTcfrom the set of all\ntriples ({S}∪V)×V×V, where Sis a privileged start symbol. The grammar is then trimmed to remove\nany non-lexical rules which do not lead to a lexical rule, and any lexical rules which are inaccessible\nfrom non-lexical productions. The result is a coherent context-free grammar with at most as many\nterminals, nonterminals, lexical productions, and non-lexical productions as the generating parameters.\nUsing this framework, we generate a first static benchmark, which we refer to as RELIC -500. We\nsample 200 grammars where all parameters ( nterm,nnonterm ,nlexandnnonlex ) are less than 500. To\nreduce the correlation between the four parameters, we oversample many grammars and filter to obtain\nthe 200 with minimally-correlated parameters; we note that some parameter pairs, such as nlexand\n3\n--- Page 4 ---\nnterm, are inherently correlated and it is not possible to completely decorrelate them (see appendix B.2\nfor discussion and fig. 7 for the correlation coefficients).\nFor each grammar, we aim to sample 10 positive strings of each of the lengths 1≤ℓ≤50by treating\neach grammar as a probabilistic context-free grammar with uniform probability assigned to production\nrules sharing a left-hand symbol. We also generate 10 negative strings (i.e., strings which are\nnot generated by the grammar) of each length: we sample negative strings from a unigram model\nover the terminal symbols Σ+and reject any strings which are parseable by the grammar. Not all\nrandomly-generated grammars will be able to produce arbitrarily many strings of a given sequence\nlength. We refer to the size of the set of generated examples relative to a defined goal of 1000 examples\nas the grammar’s coverage , and report it as a summary statistic of each grammar. For the experiments\nconducted here, the majority of grammars have over 90% coverage; see appendix B for more details.\nNote that due to the sampling procedure, the distribution of lengths between positive and negative\nsamples is not matched; see fig. 8 for the ratios of positive to negative examples for each length\n1≤ℓ≤50. In our analyses (section 5.2), we stratify (rebalance) the classes.\nIntended use. RELIC is designed as a zero-shot evaluation of a model’s ability to use instructions\nprovided in-context without also using positive and negative exemplars , and without fine-tuning on\nthis task. Evaluating models in an in-context few-shot setting could lead to higher accuracy, but any\nincreases in accuracy could be due to heuristics that distinguish the two classes with some accuracy but\nfail to strictly test the model’s ability to follow in-context instructions——for example, ones that rely\non differences in the n-gram distributions in positive versus negative examples.\nWe release RELIC -500 and the codebase to generate new grammars and examples at the following\nGitHub repository: https://github.com/jopetty/relic/tree/neurips .\n4 Experimental Setup\nWe evaluate eight LLMs on RELIC -500. First, three models from OpenAI’s GPT line that were\nthe newest at the time of writing: gpt-4.1-nano ,gpt-4.1-mini , and gpt-4.1 (released on April\n14; OpenAI 2025a). Second, we evaluate two “reasoning” models from OpenAI, o4-mini ando3\n(OpenAI, 2025b); these models are trained to generate a variable number of tokens, referred to as\nreasoning tokens, between the prompt and the output, but these tokens are kept hidden from the user. All\nevaluations of OpenAI’s API-based models were carried out between 14 April 2025 and 1 May 2025.\nThird, we evaluate two models from Google’s Gemma 3 family of open-weights instruction-tuned\nlanguage models ( gemma-3-1b-it andgemma-3-4b-it ; Gemma Team et al. 2025). Finally, we\nevaluate an open-weights reasoning model, DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al.,\n2025). See appendix D for more details on API costs, compute, and evaluation hyperparameters.\nFor each grammar Gand each string s, we prompt the models with a description of the language\nrecognition task, the grammar G, and the string s, and ask the model to classify the string according\nto whether or not it is generated by the grammar (see appendix A for an example prompt). We use\na regular expression over the output to extract whether the model classified the example as positive\nor negative; we classify the response as “unknown” if the model fails to offer a prediction. We evaluate\ngpt-4.1-nano ,gpt-4.1-mini ,gpt-4.1 , and o4-mini on the full RELIC -500 dataset; for o3,\ngemma-3-1b-it ,gemma-3-4b-it , and DeepSeek-R1-Distill-Qwen-7B , we subsample each\ngrammar’s data to have at most two examples per length per type (instead of 10) due to the increased\ncost of running evaluations on these models.\n5 Results on RELIC-500\n5.1 Overall Performance\nIn Table 1, we report models’ mean class-balanced accuracy and their macro F1 score (mean of\nthe per-class F1 score); since models do not have access to the distribution from which positive and\nnegative examples are drawn, class-balancing penalizes models for incorrect biases towards predicting\none class (positive or negative) over another. Indeed, models’ accuracy often differs significantly\nbetween positive and negative examples, indicating such a bias: gpt-4.1-nano andgpt-4.1-mini\n4\n--- Page 5 ---\n1\nInstruction Set Complexity (# of Nonlexical Productions)  [log scale]0%100%Accuracy\n500gpt-4.1-nano\n1 500gpt-4.1-mini\n1 500gpt-4.1\n1 500o4-mini\n1 500o3\n1 500gemma-3-1b\n1 500gemma-3-4b\n1 500DSR1-7B\n1 50\nT ask Complexity (Example Length)0%100%Accuracy\n1 501 501 501 501 501 501 50Figure 2: Models’ accuracy on RELIC -500 reduces to near-chance (dashed lines) for all models as\ninstruction set complexity (number of nonlexical productions in the grammar, top row ) and task\ncomplexity (example length, bottom row ) increase.\n0%100%Accuracypositive\nnegativegpt-4.1-nano gpt-4.1-mini gpt-4.1 o4-mini o3 gemma-3-1b gemma-3-4b DSR1-7B\n1 50\nT ask Complexity (Example Length)0%100%Predicted Typepositive\nnegative\nunknown\n1 501 501 501 501 501 501 50\nFigure 3: Accuracy on positive and negative examples differs between models and across sequence\nlengths ( top row ); this is partially attributable to models’ biases towards predicting one class (positive\nor negative) over another ( bottom row ).\nhave a much higher accuracy on positive examples than they do on negative ones, while the reverse\nis true for gpt-4.1 ,o4-mini , and o3(fig. 3, top).\nModel Accuracy (%) Macro F1\ngpt-4.1-nano 54.1±3.0 44 .4±0.7\ngpt-4.1-mini 63.4±1.9 57 .5±1.3\ngpt-4.1 53.9±4.5 48 .5±0.9\no4-mini 59.2±3.5 58 .1±1.0\no3 70.4±1.9 70 .1±1.1\ngemma-3-1b 48.8±2.3 30 .9±0.3\ngemma-3-4b 48.7±2.1 34 .3±0.7\nDSR1-7B 47.9±3.3 29 .6±0.3\nTable 1: Overall performance (class-balanced accu-\nracy and macro F1 score) on RELIC -500. Chance\nperformance is 50%. Performance is calculated\nfirst by grouping observations by grammar, exam-\nple type, and example length, and then taking the\nmean. Errors reflect the standard error of the mean.This difference in accuracy is mostly attributable\nto the individual models’ bias towards one of\nthe two classes (fig. 3, bottom); while most\nmodels exhibit a strong tendency to classify\nshort examples as negative, gpt-4.1-nano and\ngpt-4.1-mini increasingly classify examples\nas positive as they get longer, to the point that\nnearly all its predictions for examples of length\nℓ= 50 are positive. gpt-4.1 , by contrast, ex-\nhibits nearly the opposite tendency, classifying\nmost examples as negative across all lengths.\n5.2 Performance Decreases as Grammar\nand Example Complexity Increases\nFor all models, performance on RELIC -500 de-\ncreases as a function of a grammar’s complexity,\n5\n--- Page 6 ---\n1.00.50.00.51.0\ngpt-4.1-nanogpt-4.1-minigpt-4.1o4-minio3\ngemma-3-1bgemma-3-4bDSR1-7Bgpt-4.1-nano\ngpt-4.1-mini\ngpt-4.1\no4-mini\no3\ngemma-3-1b\ngemma-3-4b\nDSR1-7B101 200 productions\ngpt-4.1-nanogpt-4.1-minigpt-4.1o4-minio3\ngemma-3-1bgemma-3-4bDSR1-7B201 300\ngpt-4.1-nanogpt-4.1-minigpt-4.1o4-minio3\ngemma-3-1bgemma-3-4bDSR1-7B1 10 symbols\ngpt-4.1-nanogpt-4.1-minigpt-4.1o4-minio3\ngemma-3-1bgemma-3-4bDSR1-7B11 20Figure 4: Spearman’s rank correlation coefficients for the mean accuracy per grammar ( left) and per\nexample ( right ) between different models, binned by complexity. Models generally agree on which\ngrammars and examples are hard, though gpt-4.1-nano/mini tend to disagree with other models.\nThe strengths of these correlations increases modestly as grammar and example complexity grow.\nas quantified by each of the four grammar parameters. Among these parameters, the number of\nnon-lexical productions nnonlex is most strongly anti-correlated with performance. On a per-model\nbasis, we observe a roughly log-linear relationship between the number of non-lexical productions and\nperformance: though some models have high accuracy on small grammars, as nnonlex approaches 500\nall models are at or below chance performance (fig. 2, top).\nModel performance is also affected by the complexity of individual examples. As example length ℓ\nincreases, models’ mean accuracy over both positive and negative examples decreases (fig. 2, bottom).\nThis drop-off happens quite rapidly, with an inflection point occurring at between ℓ= 5andℓ= 15\ndepending on the model. A regression shows that the effects of log(nnonlex)andlog(ℓ)are highly\nsignificant (see tables 2 and 3 in appendix B).\n5.3 Models Generally Agree on which Grammars and Examples are Hard\nAlongside the strong correlation between accuracy and grammar complexity, we found substantial\nvariability in accuracy among grammars of similar complexity; this is particularly the case for the\nmodels with higher overall accuracy, gpt-4.1-mini ,o4-mini ando3(fig. 2). Are the same grammars\nor examples challenging for all models? To test this, we rank the grammars and examples by their\naccuracy for each model and compute the Spearman’s rank correlation across models. To mitigate\nthe overall correlation with grammar complexity, we first divide the grammars into five bins of 100\nproduction rules each; we similarly divide examples into length bins, in increments of 10. We find that\ngrammars (fig. 4, left) and samples (fig. 4, right) have generally consistent difficulty across models.\nOf note is that fact that two models, gpt-4.1-nano andgpt-4.1-mini , are less correlated with\nthe remaining models but have positive correlation with one another; this behavior likely reflects the\nfact that these models have a bias in favor of predicting samples to be positive, opposite the pattern\nof all other models as shown in fig. 3. We also observe that these (anti-)correlations strengthen as\ngrammar and example complexity increase: correlations are closer to zero on grammars with 100–200\nproductions than they are for those with 200–300 productions, and similarly for examples of lengths\n1–10 and 11–20. For a full overview of these correlations, see figs. 10 and 11 in appendix C.\n6 How Do LLMs Perform the Task?\nTo understand the strategies that models are using to perform the CFL recognition task, we examine the\nintermediate tokens the models return before providing the answer (their “chains of thought”). We have\naccess to these tokens for all models except o3ando4-mini , whose chains of thought are obscured.\n6.1 Qualitative Analysis of Chains of Thought\nTo highlight the strategies that models employ, we first provide a qualitative evaluation of several\nchains of thought from DeepSeek-R1 (abbreviated as DSR1 below) and gpt-4.1-mini . Overall, we\nobserve that models can often identify a correct strategy to solve the CFL task—using the rules of the\ngrammar to construct a parse tree of the string, or showing (by exhaustion or proof) that one cannot\n6\n--- Page 7 ---\nexist. However, models do not always follow that strategy to completion, often getting distracted\nby irrelevant information, hallucinating success criteria, or second-guessing their (correct) choice of\nstrategy and attempting to solve the problem by other means. We also observe that DSR1 ’s tendency to\nbacktrack on strategy often leads it to reach the 2048 token limit we set for this model for computational\nreasons, though breaking down the model’s accuracy by completion status shows that this actually\nhelps its overall performance; see appendix H for additional discussion.\nSuccessful parses. In some cases, models succeed at parsing positive samples by identifying the\nrelevant production rules and deriving a parse tree. Appendix F.4 highlights such an instance in\nwhich gpt-4.1-mini is able to exhaustively search for derivations compatible with the provided\nexample; this search involved recursively applying the rule NT5→NT0 NT5 three times, resulting\nin a full derivation for the 5-symbol string t30 t24 t24 t23 t4 , as shown in fig. 5. The model’s\nchain-of-thought trace also demonstrates that it successfully ignores production rules irrelevant to the\nparticular example.\nt30S\nNT5\nNT5\nNT5\nNT0 NT0 NT0 NT5 NT2\nt24 t24 t23 t4S   -> NT5 NT2\nNT2 -> NT3 NT6\nNT5 -> NT0 NT5\n···\nNT2 -> ‘t4’\nNT0 -> ‘t30’\nNT0 -> ‘t24’\nNT4 -> ‘t18’\nNT5 -> ‘t23’\n···\nFigure 5: Successfully accepting a valid string by\nrecursively applying production rules to derive a\nparse tree for the example, as in the gpt-4.1-mini\nchain of thought shown in appendix F.4.Failed attempts at parsing. In the chain of\nthought shown in appendix F.1, DSR1 reports\nidentifying a strategy that, while vaguely de-\nscribed, is consistent with the appropriate way\nto determine if the example is generated by the\ngrammar ( “start from the string and see if I can\nbreak it down into the non-terminals according\nto the production rules” ; see paragraph F.1 of\nappendix F.1). It begins exploring this strategy,2\nbut quickly backtracks and veers off into a heuris-\ntic and irrelevant analysis based on string length\n(“Wait, let me count the tokens” ; par F.1). It then\nbacktracks again, and attempts to find a deriva-\ntion for the first symbol in the example (par F.1).\nIt correctly identifies two production rules that together can generate the first symbol in the example (the\nfirst symbol is t75, and the relevant rules are {S→NT97 NT1, NT97 →‘t75’} ). It then proceeds\nto determine how the second symbol in the example, t104 , could be derived. It notices that NT1—\nthe second nonterminal in the rule S→NT97 NT1 , which it used to derive the first symbol—does\nnot directly produce t104 , and considers this a potential reason for rejecting the string (par F.1). It\nthen considers the possibility that NT1produces t104 not directly but through intermediate produc-\ntions (par F.1); it then rejects this possibility based on the presence of a different lexical production\nrule involving NT1—although this is an incorrect reason to reject this possibility—and concludes,\nincorrectly, that the example is not generated by the grammar (par F.1).\nTrivial errors. Even though DSR1 is capable of implementing the correct strategy, it still makes\nerrors even on trivial samples. In appendix F.2, DSR1 is prompted by a (negative) example consisting\nofa single symbol t136 ; though it identifies that this token is derivable by some lexical production\nrule, it immediately begins investigating irrelevant derivations which contain additional terminal\nsymbols (par F.2). It continues along this line before arguing (incorrectly) that the example seems\nplausible since it seems like there exist derivations containing the provided terminal symbol (par F.2).\nIrrelevant heuristics. In some cases, models provide the correct answer for the wrong reasons\n(McCoy et al., 2019). In appendix F.3, DSR1 adopts a strategy that is entirely irrelevant: it decides to\ncheck if the string contains the symbol t27(par F.3), a symbol which not only does not appear in the\nexample it was given, but does not even appear in the grammar in question. It then proceeds to check if\nany symbol in the provided string is t27(par F.3) before rejecting the example because t27was not\npresent (par F.3). Because the example in this case happened to be a negative example, DSR1 happen to\nproduce the correct answer, but it reached this answer for irrelevant reasons.\n2To avoid cumbersome prose, we use language such as “it notices”, “it considers”, and so on, when describing\nthe LLMs’ chain of thought. This shorthand should not be taken to suggest that we see the LLM as having agency or\nawareness, or that we believe the LLM’s self-reported reasoning traces are faithful to its actual reasoning process.\n7\n--- Page 8 ---\n0%100%Relative TTCgpt-4.1-nano gpt-4.1-mini gpt-4.1 o4-mini o3\n14 50\nT ask Complexity (Example Length)0%100%Proportion\nof Strategiesrule-basedheuristic\n16 5018 5018 501 12 50Figure 6: As task complexity (example length) increases, the test-time compute (TTC) expended by\nmodels peaks early and then diminishes ( top row ; TTC is computed as the mean number of completion\ntokens produced for examples of a given length, relative to the length for which the mean number\nof tokens is highest; see 12 for absolute TTC). As relative TTC stops increasing, models shift from\npursuing rule-based approaches to relying on heuristic strategies ( bottom row ; this analysis is based\nono4-mini ’s classification, which may underestimate the number of rule-based strategies). o4-mini\nando3do not provide full CoTs, so we cannot classify these models’ strategies.\n6.2 Quantitative Analysis of Model Strategies\nThe qualitative analysis highlights some of the general strategies used by models in attempting to\nsolve specific instances of the RELIC task. We next perform a quantitative analysis categorizing the\ncompletions of the gpt-4.1 series of models according to the strategy they employ; we focus on\nthese models since they are are successful enough at solving the task for short examples (unlike the\nthree open-weights models studied; fig. 2, bottom) and also provide access to their chains of thought\n(unlike o3ando4-mini ). To do so, we employ the “LLM-as-a-judge” framework (Zheng et al., 2023),\nprompting o4-mini to classify model completions as either rule-based (requiring a full derivation\nof the string, or a proof that one cannot exist) or heuristic (appealing to properties of the example or\ngrammar to argue why a example seems likely or unlikely to be derivable). We validate o4-mini ’s\nclassifications by selecting a subset of model completions, having three of the authors categorize\ncompletions independently using the same instructions as given to o4-mini , and then comparing\nthe agreement between annotators and o4-mini ; see appendix G for details. o4-mini agrees with\nannotators 70% of the time; in all cases of disagreement, the human annotators classified the strategy\nas rule-based strategies and o4-mini classified it as heuristic. In other words, as a judge o4-mini\nhas high recall for identifying heuristic over rule-based reasoning chains. Figure 6 (bottom) shows\nthe results of this analysis. All three models switch from pursuing rule-based strategies to relying on\nshallower heuristics as examples get more complex; gpt-4.1 andgpt-4.1-mini are more likely than\ngpt-4.1-nano to use rules.\n6.3 Test-Time Compute as a Function of Length\nTransformer LLMs can solve more complex tasks by expending a larger number of intermediate\nchain-of-thought tokens; this is referred to as test-time compute. We next examine if the LLMs we\nstudy produce more intermediate tokens for longer examples, as we would expect if they were applying\nthe correct strategy; in fact, we expect the lengths of chains of thought implementing this strategy to\nscale cubically with the length of the string (§7.3). We find such a correlation for all OpenAI models (we\nhave access to the number of generated tokens from o4-mini ando3, though not to their identity), but\nthis correlation peaks relatively early: for example, it peaks for strings of length 6 for gpt-4.1-mini ,\nand for strings of length 12 for o3(fig. 6, top). For the gpt-4.1 models, for which we have access to\nthe contents of the chains of thought, the drop-off in test-time compute expenditure coincides with the\nmodels’ shift from rule-based to heuristic strategies, further supporting the hypothesis that the models\nstop attempting to apply the correct strategy once the string reaches a certain length.\n8\n--- Page 9 ---\n7 Discussion\n7.1 Strengths of the RELIC Framework\nRELIC is robust to contamination and saturation. The fact that RELIC is a generative framework\nfor producing novel grammars and examples increases its robustness to data contamination and\nbenchmark saturation. Alongside RELIC-500, the static dataset we release as part of this work, we\ncan easily generate new grammars and examples of matched distribution to verify that improved\nperformance does not result from the static benchmark being included in the training set (Elazar et al.,\n2024). Furthermore, because we can precisely control the size of the grammars and examples, and LLM\nperformance is predictable from these factors, we can generate new benchmarks with increasingly high\nlevels of complexity as LLMs’ capabilities improve, mitigating benchmark saturation concerns.\nRELIC evaluates sophisticated context use. To properly recognize RELIC examples, the LLM\nmust apply a large number of production rules that are densely distributed within its context window;\nthese rules are often recursive, applied multiple times for a single example, and applied in a non-linear\norder which is not constant between examples or grammars. These factors make RELIC a significantly\nstronger test of LLMs’ ability to reason over information provided in context than existing context-\nusage evaluations, in particular needle-in-a-haystack evaluations. Our results in §5.2 suggest that we\ncurrently overestimate the ability of language models to use their context in non-trivial ways. While\nmore sophisticated context-use evaluations such as Michelangelo (V odrahalli et al., 2024) partially\nmitigate this by using larger parts of the context, they still suffer from two problems: first, they only\nrequire modification or retrieval of a small proportion of the full context size, because most of the\ninformation in the context is irrelevant; second, they can be solved within a single traversal of context\nwindow, in a fixed order (e.g., the task is to modify a list by performing sequential actions).\n7.2 Diagnosing Reasoning Strategies\nAs a formal task, where the correct reasoning strategy is known, the chains of thought produced by\nLLMs as they attempt to solve RELIC provides a window into their reasoning capabilities. In particular,\nwe have found that models change strategies for solving the task as the difficulty of the task increases;\nthis was also the case for models that can identify and implement a length-generalizing solution for that\ntask. In §6.2, we show that as the length of the examples grows beyond a certain point (length 10 or\neven before), all models in the gpt-4.1 series switch to predominantly using heuristics to guess the\nanswer, rather than performing parsing. This is despite the fact that these models are familiar with the\ncorrect strategy, and in some cases apply it without mistakes for short inputs.\n7.3 Theoretical Difficulty of Solving RELIC with LLMs\nTheoretically, the RELIC task is an instance of in-context recognition for CFGs without ϵproductions,\nwhich is known to be in the complexity classes AC1andNC1-hard (Ruzzo, 1980; Venkateswaran,\n1991; Greenlaw et al., 1991). In contrast, transformers without CoT are in the complexity class TC0\n(Merrill and Sabharwal, 2023). Thus, under standard complexity conjectures, fixed-depth transformers\ncannot solve the in-context recognition task for general CFGs without using CoT. Moreover, solving\nRELIC by implementing parsing should require the CoT length to grow substantially for longer\ninputs. Standard context-free parsing algorithms run in time Θ(n3), and it is unlikely that this can\nbe significantly improved (Lee, 2002). Applying results relating CoT tokens to algorithmic runtime\n(Merrill and Sabharwal, 2024), a transformer can solve RELIC by implementing a standard parsing\nalgorithm using at least Ω(n1.7)and at most O(n6)CoT tokens: in either case, the CoT length must\ngrow substantially with the length of the input string n. In contrast, fig. 6 shows that the number\nof CoT tokens generated diminishes for longer input strings, suggesting models are backing off to\nheuristics rather than recognizing long strings by parsing them. These changes motivate LLM systems\nwhich are either not so limited in their inference compute or which can solve the problem in a more\ncompute-efficient manner.\n7.4 Limitations and Future Work\nTo produce the negative examples in RELIC -500, we sampled each of the tokens in the negative\nexamples independently and uniformly-at-random from the set of terminal symbols. As such there may\n9\n--- Page 10 ---\nbe statistical differences between the negative and positive examples such that a model that relies on\nheuristics could obtain higher-than-chance accuracy without fully parsing the strings. While in practice\nour negative samples remain challenging for current state-of-the-art models (fig. 3), in future work\nwe will explore more adversarial negative sampling methods, which maximize the similarity between\nnegative and positive examples. We will also explore how LLMs fare at extensions of the RELIC\ntask, like returning parses for strings instead of merely determining whether they are parseable by\nthe grammar, generating new samples from grammars, and transducing strings between synchronous\ncontext-free grammars, a formal analogue to the natural language translation task from an grammar\nprovided in context proposed by Tanzer et al. (2023). Other natural extensions include generalizing to\nprobabilistic CFGs or context-sensitive grammars, which are more expressive than CFGs; or including\nCFGs with ϵproductions, which increases the complexity of in-context CFG recognition from AC1to\nP-complete (Greenlaw et al., 1991).\nAcknowledgments. This work was supported in part through the NYU IT High Performance\nComputing resources, services, and staff expertise. This project is supported by the National Science\nFoundation (NSF) under grants IIS-2239862 and NRT-HDR: FUTURE. MYH is supported by the NSF\nGraduate Research Fellowship. WM is supported by the NSF Graduate Research Fellowship as well as\nthe Two Sigma PhD Fellowship. We also acknowledge financial support from Eric and Wendy Schmidt\n(made by recommendation of the Schmidt Futures program).\nReferences\nE. Akyürek, B. Wang, Y . Kim, and J. Andreas. In-context language learning: Architectures and\nalgorithms, 23 Jan. 2024. URL http://arxiv.org/abs/2401.12973 .\nS. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology:\nMeasuring and improving recall in efficient language models, 8 Dec. 2023. URL http://arxiv.\norg/abs/2312.04927 .\nS. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize\nformal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 7096–7116, Stroudsburg, PA, USA, Nov. 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.576. URL http://dx.doi.\norg/10.18653/v1/2020.emnlp-main.576 .\nS. Bhattamishra, A. Patel, P. Blunsom, and V . Kanade. Understanding In-Context Learning in\nTransformers and LLMs by Learning to Learn Discrete Functions. In The Twelfth International\nConference on Learning Representations , 13 Oct. 2023. URL https://openreview.net/forum?\nid=ekeyCgeRfC .\nB. Bohnet, K. Swersky, R. Liu, P. Awasthi, A. Nova, J. Snaider, H. Sedghi, A. T. Parisi, M. Collins,\nA. Lazaridou, O. Firat, and N. Fiedel. Long-span question-answering: Automatic question generation\nand QA-system ranking via side-by-side evaluation, 31 May 2024. URL http://arxiv.org/\nabs/2406.00179 .\nT. L. Booth and R. A. Thompson. Applying probability measures to abstract languages. IEEE\ntransactions on computers. Institute of Electrical and Electronics Engineers , C-22(5):442–450, May\n1973. ISSN 0018-9340. doi: 10.1109/t-c.1973.223746. URL https://ieeexplore.ieee.org/\ndocument/1672339 .\nA. Butoi, G. Khalighinejad, A. Svete, J. Valvoda, R. Cotterell, and B. DuSell. Training neural networks\nas recognizers of formal languages, 11 Nov. 2024. URL http://arxiv.org/abs/2411.07107 .\nN. Chomsky and M. P. Schützenberger. The Algebraic Theory of Context-Free Languages. In\nP. Braffort and D. Hirschberg, editors, Studies in Logic and the Foundations of Mathematics ,\nvolume 35 of Computer Programming and Formal Systems , pages 118–161. Elsevier, 1 Jan. 1963.\nURL https://www.sciencedirect.com/science/article/pii/S0049237X08720238 .\nA. Clark. Testing distributional properties of context-free grammars. In S. Verwer, M. van Zaanen, and\nR. Smetsers, editors, Proceedings of The 13th International Conference on Grammatical Inference ,\nvolume 57 of Proceedings of Machine Learning Research , pages 42–53, Delft, The Netherlands,\n2017. PMLR. URL https://proceedings.mlr.press/v57/clark16.pdf .\n10\n--- Page 11 ---\nA. Clark. syntheticpcfg: Code for generating synthetic PCFGs for testing grammatical inference\nalgorithms, 2018. URL https://github.com/alexc17/syntheticpcfg .\nDeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi,\nX. Zhang, X. Yu, Y . Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu,\nB. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai,\nF. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu,\nH. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong,\nK. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu,\nL. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang,\nQ. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou,\nS. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun,\nT. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu,\nX. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li,\nX. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y . K. Li, Y . Q.\nWang, Y . X. Wei, Y . Zhang, Y . Xu, Y . Li, Y . Zhao, Y . Sun, Y . Wang, Y . Yu, Y . Zhang, Y . Shi, Y . Xiong,\nY . He, Y . Piao, Y . Wang, Y . Tan, Y . Ma, Y . Liu, Y . Guo, Y . Ou, Y . Wang, Y . Gong, Y . Zou, Y . He,\nY . Xiong, Y . Luo, Y . You, Y . Liu, Y . Zhou, Y . X. Zhu, Y . Xu, Y . Huang, Y . Li, Y . Zheng, Y . Zhu, Y . Ma,\nY . Tang, Y . Zha, Y . Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma,\nZ. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and\nZ. Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.\nTechnical report, DeepSeek AI, 22 Jan. 2025.\nG. Delétang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter,\nS. Legg, J. Veness, and P. A. Ortega. Neural Networks and the Chomsky Hierarchy. In The Eleventh\nInternational Conference on Learning Representations , 2023. URL https://openreview.net/\nforum?id=WbxHAzkeQcn .\nN. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y . Lin, S. Welleck, P. West, C. Bhagavatula, R. L. Bras,\nJ. D. Hwang, S. Sanyal, X. Ren, A. Ettinger, Z. Harchaoui, and Y . Choi. Faith and fate: Limits of\ntransformers on compositionality. In Thirty-seventh Conference on Neural Information Processing\nSystems , 2023. URL https://openreview.net/forum?id=Fkckkr3ya8 .\nY . Elazar, A. Bhagia, I. Magnusson, A. Ravichander, D. Schwenk, A. Suhr, E. P. Walsh, D. Groeneveld,\nL. Soldaini, S. Singh, H. Hajishirzi, N. A. Smith, and J. Dodge. What’s in my big data? In The\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May\n7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=RvfPnOkPV4 .\nM. Finlayson, K. Richardson, A. Sabharwal, and P. Clark. What makes instruction learning hard?\nAn investigation and a new challenge in a synthetic environment, 19 Apr. 2022. URL http:\n//arxiv.org/abs/2204.09148 .\nGemma Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova,\nA. Ramé, M. Rivière, L. Rouillard, T. Mesnard, G. Cideron, J.-B. Grill, S. Ramos, E. Yvinec,\nM. Casbon, E. Pot, I. Penchev, G. Liu, F. Visin, K. Kenealy, L. Beyer, X. Zhai, A. Tsitsulin, R. Busa-\nFekete, A. Feng, N. Sachdeva, B. Coleman, Y . Gao, B. Mustafa, I. Barr, E. Parisotto, D. Tian,\nM. Eyal, C. Cherry, J.-T. Peter, D. Sinopalnikov, S. Bhupatiraju, R. Agarwal, M. Kazemi, D. Malkin,\nR. Kumar, D. Vilar, I. Brusilovsky, J. Luo, A. Steiner, A. Friesen, A. Sharma, A. Sharma, A. M.\nGilady, A. Goedeckemeyer, A. Saade, A. Feng, A. Kolesnikov, A. Bendebury, A. Abdagic, A. Vadi,\nA. György, A. S. Pinto, A. Das, A. Bapna, A. Miech, A. Yang, A. Paterson, A. Shenoy, A. Chakrabarti,\nB. Piot, B. Wu, B. Shahriari, B. Petrini, C. Chen, C. L. Lan, C. A. Choquette-Choo, C. J. Carey,\nC. Brick, D. Deutsch, D. Eisenbud, D. Cattle, D. Cheng, D. Paparas, D. S. Sreepathihalli, D. Reid,\nD. Tran, D. Zelle, E. Noland, E. Huizenga, E. Kharitonov, F. Liu, G. Amirkhanyan, G. Cameron,\nH. Hashemi, H. Klimczak-Pluci ´nska, H. Singh, H. Mehta, H. T. Lehri, H. Hazimeh, I. Ballantyne,\nI. Szpektor, I. Nardini, J. Pouget-Abadie, J. Chan, J. Stanton, J. Wieting, J. Lai, J. Orbay, J. Fernandez,\nJ. Newlan, J.-Y . Ji, J. Singh, K. Black, K. Yu, K. Hui, K. V odrahalli, K. Greff, L. Qiu, M. Valentine,\nM. Coelho, M. Ritter, M. Hoffman, M. Watson, M. Chaturvedi, M. Moynihan, M. Ma, N. Babar,\nN. Noy, N. Byrd, N. Roy, N. Momchev, N. Chauhan, N. Sachdeva, O. Bunyan, P. Botarda, P. Caron,\nP. K. Rubenstein, P. Culliton, P. Schmid, P. G. Sessa, P. Xu, P. Stanczyk, P. Tafti, R. Shivanna, R. Wu,\nR. Pan, R. Rokni, R. Willoughby, R. Vallu, R. Mullins, S. Jerome, S. Smoot, S. Girgin, S. Iqbal,\nS. Reddy, S. Sheth, S. Põder, S. Bhatnagar, S. R. Panyam, S. Eiger, S. Zhang, T. Liu, T. Yacovone,\n11\n--- Page 12 ---\nT. Liechty, U. Kalra, U. Evci, V . Misra, V . Roseberry, V . Feinberg, V . Kolesnikov, W. Han, W. Kwon,\nX. Chen, Y . Chow, Y . Zhu, Z. Wei, Z. Egyed, V . Cotruta, M. Giang, P. Kirk, A. Rao, K. Black,\nN. Babar, J. Lo, E. Moreira, L. G. Martins, O. Sanseviero, L. Gonzalez, Z. Gleicher, T. Warkentin,\nV . Mirrokni, E. Senter, E. Collins, J. Barral, Z. Ghahramani, R. Hadsell, Y . Matias, D. Sculley,\nS. Petrov, N. Fiedel, N. Shazeer, O. Vinyals, J. Dean, D. Hassabis, K. Kavukcuoglu, C. Farabet,\nE. Buchatskaya, J.-B. Alayrac, R. Anil, Dmitry, Lepikhin, S. Borgeaud, O. Bachem, A. Joulin,\nA. Andreev, C. Hardin, R. Dadashi, and L. Hussenot. Gemma 3 Technical Report. Technical report,\nGoogle DeepMind, 25 Mar. 2025.\nR. Greenlaw, W. L. Ruzzo, and J. Hoover. A compendium of problems complete for P (pre-\nliminary). Technical report, 1991. URL https://era.library.ualberta.ca/items/\n403292c5-460b-49e6-8b05-9a5a7b45b0d6 .\nK. Gupta, K. Sanders, and A. Solar-Lezama. Randomly sampled language reasoning problems reveal\nlimits of LLMs, 6 Jan. 2025. URL http://arxiv.org/abs/2501.02825 .\nC.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y . Zhang, and B. Ginsburg. RULER:\nWhat’s the real context size of your long-context language models?, 9 Apr. 2024. URL http:\n//arxiv.org/abs/2404.06654 .\nW. Hua, F. Sun, L. Pan, A. Jardine, and W. Y . Wang. InductionBench: LLMs Fail in the Simplest\nComplexity Class. In Workshop on Reasoning and Planning for Large Language Models , 2025.\nURL https://openreview.net/forum?id=brw11PScQM .\nG. Kamradt. gkamradt/LLMTest_NeedleInAHaystack. https://github.com/gkamradt/\nLLMTest_NeedleInAHaystack/blob/main/README.md , 2023. Accessed: 2025-4-10.\nN. Kim and T. Linzen. COGS: A compositional generalization challenge based on semantic interpre-\ntation. In B. Webber, T. Cohn, Y . He, and Y . Liu, editors, Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pages 9087–9105, Online, Nov.\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.731. URL\nhttps://aclanthology.org/2020.emnlp-main.731/ .\nL. Lee. Fast context-free grammar parsing requires fast boolean matrix multiplication. Journal of the\nACM , 49(1):1–15, 1 Jan. 2002. ISSN 0004-5411,1557-735X. doi: 10.1145/505241.505242. URL\nhttps://dl.acm.org/doi/10.1145/505241.505242 .\nM. Li, S. Zhang, Y . Liu, and K. Chen. NeedleBench: Can LLMs do retrieval and reasoning in 1 million\ncontext window?, 16 July 2024. URL http://arxiv.org/abs/2407.11963 .\nT. McCoy, E. Pavlick, and T. Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics\nin natural language inference. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448, Stroudsburg, PA, USA, 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/p19-1334. URL http://dx.doi.org/10.18653/\nv1/P19-1334 .\nW. Merrill and A. Sabharwal. The parallelism tradeoff: Limitations of log-precision transform-\ners.Transactions of the Association for Computational Linguistics , 11:531–545, 12 June 2023.\nISSN 2307-387X. doi: 10.1162/tacl\\_a\\_00562. URL https://dx.doi.org/10.1162/tacl_\na_00562 .\nW. Merrill and A. Sabharwal. The Expressive Power of Transformers with Chain of Thought. In The\nTwelfth International Conference on Learning Representations , 2024. URL https://openreview.\nnet/forum?id=NjNGlPh8Wh .\nOpenAI. Introducing GPT-4.1 in the API. https://openai.com/index/gpt-4-1/ , 15 Apr. 2025a.\nAccessed: 2025-5-12.\nOpenAI. OpenAI o3 and o4-mini System Card. https://openai.com/index/\no3-o4-mini-system-card/ , 16 May 2025b. Accessed: 2025-5-12.\n12\n--- Page 13 ---\nO. Press, M. Zhang, S. Min, L. Schmidt, N. Smith, and M. Lewis. Measuring and narrowing the\ncompositionality gap in language models. In Findings of the Association for Computational\nLinguistics: EMNLP 2023 , pages 5687–5711, Stroudsburg, PA, USA, Dec. 2023. Association\nfor Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.378. URL https://\naclanthology.org/2023.findings-emnlp.378/ .\nL. Qiu, L. Jiang, X. Lu, M. Sclar, V . Pyatkin, C. Bhagavatula, B. Wang, Y . Kim, Y . Choi, N. Dziri, and\nX. Ren. Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models\nwith Hypothesis Refinement. In The Twelfth International Conference on Learning Representations ,\n2024. URL https://openreview.net/forum?id=bNt7oajl2a .\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\nlearning research: JMLR , 21(2020):1–67, 23 Oct. 2019. ISSN 1532-4435,1533-7928. URL\nhttp://arxiv.org/abs/1910.10683 .\nW. L. Ruzzo. Tree-size bounded alternation. Journal of computer and system sciences , 21(2):218–\n235, 1 Oct. 1980. ISSN 0022-0000,1090-2724. doi: 10.1016/0022-0000(80)90036-7. URL\nhttp://dx.doi.org/10.1016/0022-0000(80)90036-7 .\nV . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja,\nM. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V .\nNayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey,\nR. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry, J. A. Fries, R. Teehan,\nT. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask prompted training enables\nzero-shot task generalization. In The Tenth International Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.\nnet/forum?id=9Vrb9D0WI4 .\nG. Tanzer, M. Suzgun, E. Visser, D. Jurafsky, and L. Melas-Kyriazi. A benchmark for learning to\ntranslate a new language from one grammar book, 28 Sept. 2023. URL http://arxiv.org/abs/\n2309.16575 .\nH. Venkateswaran. Properties that characterize LOGCFL. Journal of computer and system sciences ,\n43(2):380–404, 1 Oct. 1991. ISSN 0022-0000,1090-2724. doi: 10.1016/0022-0000(91)90020-6.\nURL http://dx.doi.org/10.1016/0022-0000(91)90020-6 .\nK. V odrahalli, S. Ontanon, N. Tripuraneni, K. Xu, S. Jain, R. Shivanna, J. Hui, N. Dikkala, M. Kazemi,\nB. Fatemi, R. Anil, E. Dyer, S. Shakeri, R. Vij, H. Mehta, V . Ramasesh, Q. Le, E. Chi, Y . Lu, O. Firat,\nA. Lazaridou, J.-B. Lespiau, N. Attaluri, and K. Olszewska. Michelangelo: Long context evaluations\nbeyond haystacks via Latent Structure Queries, 19 Sept. 2024. URL http://arxiv.org/abs/\n2409.12640 .\nJ. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le. Finetuned\nLanguage Models are Zero-Shot Learners. In International Conference on Learning Representations ,\n2022. URL https://openreview.net/forum?id=gEZrGCozdqR .\nT. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Fun-\ntowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. Le Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural lan-\nguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations , pages 38–45, Stroudsburg, PA, USA, Oct.\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttp://dx.doi.org/10.18653/v1/2020.emnlp-demos.6 .\nX. Zhang, Y . Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and\nM. Sun. ∞Bench: Extending long context evaluation beyond 100K tokens, 21 Feb. 2024. URL\nhttp://arxiv.org/abs/2402.13718 .\nL. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang,\nJ. E. Gonzalez, and I. Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In\nThirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack , 2023. URL https://openreview.net/forum?id=uccHPGDlao .\n13\n--- Page 14 ---\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y . Luan, D. Zhou, and L. Hou. Instruction-following\nevaluation for large language models. arXiv preprint arXiv:2311.07911 , 2023.\nS. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y . Bisk, D. Fried, U. Alon, and\nG. Neubig. WebArena: A Realistic Web Environment for Building Autonomous Agents. In The\nTwelfth International Conference on Learning Representations , 2024. URL https://openreview.\nnet/forum?id=oKn9c6ytLx .\nA Example Prompt\nIn this section, we show an abridged example of the default prompt we use for evaluating models on the\nRELIC task, taken from grammar_20250218222557 .\nYou will be presented with a context-free grammar in Chomsky normal\nform and a string which may or may not be in the language defined by\nthe given grammar. Your job is to determine whether or not the grammar\ngenerates the provided string. You can use any reasoning strategy you\nlike, but you must end your response with either ’Yes’ (if the string\nis generated by the grammar) or ’No’ (if it isn’t.)\nGrammar:\n“‘\nS -> NT97 NT1\nNT180 -> NT47 NT121\nNT120 -> NT73 NT121\nNT114 -> NT197 NT79\nNT191 -> NT76 NT49\nNT8 -> NT90 NT28\nNT192 -> NT140 NT152\n...\n...\nNT171 -> ’t59’\nNT31 -> ’t139’\nNT172 -> ’t28’\nNT100 -> ’t16’\nNT187 -> ’t158’\nNT100 -> ’t44’\n...\n...\n“‘\nHere is the string you need to evaluate:\nString: ‘t64‘.\nRemember, end your response with either ’Yes’ or ’No’.\nB Grammar and Example Information\nB.1 Distributional Statistics for Grammars and Examples\nTo construct the static evaluation set used in our experiments, we first over-generate grammars ( ∼1000\ntotal) where the four generating hyperparameters of each grammar (the numbers of terminal ntermand\nnonterminal nnonterm symbols and the numbers of lexical nlexand nonlexical nnonlex production rules)\nare bounded above by 500. We then subsample these grammars to include the 200 grammars whose\nhyperparameters are minimally correlated with one another. Figure 7 below reports the hyperparameter\ncorrelations in the released set. Note that the correlations between nterm∼nlexandnnonterm∼nnonlex are\nhigher than the others since the former term is bounded above by the latter in both cases.\nFrom each grammar we sample positive and negative strings. Positive strings are sampled by converting\neach grammar into a probabilistic context-free grammar (Booth and Thompson, 1973) with uniform\n14\n--- Page 15 ---\nnterm nnonterm nlexnnonterm nlex nnonlex0.01\n0.54 0.07\n-0.00 0.31 -0.01Figure 7: Correlations between generating hyperparameters for the released static set. Note that nlex\nandntermare inherently correlated (see appendix B.2 for discussion).\npositive negative\nSample Type01Proportion\n1 10 20 30 40 50\nSample Length0k5k10k\nCount\nFigure 8: Proportions of example types represented in the static dataset, in aggregate ( left) and broken\ndown by example length ( right ).\nprobability for each right-hand-side path among all productions which share a left-hand-side non-\nterminal and sampling a production stochastically. We over-sample positive strings and filter so that\nthey are of length at most 50, and such that we have no more than 10strings per length. Negative\nstrings are sampled by drawing strings over the set of terminal symbols Σ+of fixed length 1≤ℓ≤50\nuniformly-at-random and rejecting any strings which are parseable by the grammar. We repeat this\nprocess until we have 10strings per length.\nSince positive examples are not drawn with a pre-determined length and not all grammars can generate\n10strings for each length, the resulting set of sampled strings will in some cases be smaller than\nthat of the negative examples; fig. 8 shows the relative proportions of positive and negative samples\ndrawn from the released set of grammars. For our evaluations, we choose not to post-hoc rebalance\nexample types for each length since the distribution of positive examples by length is a property of\nthe grammar. Since not all grammars will generate strings of every length in equal proportions, the\nlength of an example contains relevant information about the example’s type, albeit information which\nis not provided to the model independently from the grammar itself. For a model to justifiably use the\nexample length to arrive at the correct answer, it must derive the relevant properties from the production\nrules itself.\nWe refer to the size of a grammar’s examples, relative to the theoretical maximum of 1000 strings (= 2\nexample types * 50 lengths * 10 examples per length per type) as the grammar’s coverage ; fig. 9 shows\nthe coverage of the grammars used in our experiments.\nB.2 Why are ntermandnlexPartially Correlated?\nIn any context-free grammar the number of terminal symbols ntermis bounded above by the number of\nlexical production rules nlex; to see why, consider the following minimal grammar with a single lexical\nruleA -> ‘a’ and a single terminal symbol ‘a’.\nS -> A\nA -> ‘a’\n15\n--- Page 16 ---\n0.00.3Proportion\n50% 60% 70% 80% 90% 100%\nCoverage01ProportionFigure 9: Distribution of grammars by coverage (i.e., the size of the language they generate, measured\nas the number of positive examples of lengths ℓ≤50, with a maximum of 10examples/length, out\nof a theoretical maximum of 500) shown as a histogram ( top) and a cumulative distribution function\n(bottom ).\ngpt-4.1-nano\ngpt-4.1-mini\ngpt-4.1\no4-mini\no3\ngemma-3-1b\ngemma-3-4b\nDSR1-7B1 10 symbols 11 20 21 30 31 40 41 50\nFigure 10: Spearman’s rank correlation coefficients for the per-example accuracies between different\nmodels, faceted by example length. On short examples of length ≤10, all models are moderately\ncorrelated with one another; on longer examples, gpt-4.1-nano andgpt-4.1.mini become less\ncorrelated with the other models.\nTo add a new terminal symbol ‘b’to the grammar, we must introduce a production rule which yields it:\nS -> A\nA -> ‘a’\nA -> ‘b’\nThus, nterm≤nlex. To see how it could be lower, consider a grammar like the following where multiple\ndistinct nonterminals yield the same terminal:\nS -> A B\nA -> ‘a’\nB -> ‘a’\nHere the grammar has two lexical production rules but only a single terminal symbol. A similar\nargument extends to nnonterm andnnonlex .\nC Detailed Results\nTable 2 reports the Pearson correlation coefficients for model accuracy and macro F1 score with the log\ntransforms of the four generating hyparparameters of each grammar (the numbers of terminal ntermand\nnonterminal nnonterm symbols and the numbers of lexical nlexand nonlexical nnonlex production rules).\nWe also report the coefficients of regression for accuracy (as a percentage) versus model type, nnonlex ,\nand example length ℓin table 3. Both nnonlex andℓwere log-transformed and centered before being\nentered into the regression.\nFigures 10 and 11 show the full Spearman’s rank correlation coefficients for the difficulties of individual\ngrammars and samples by model.\n16\n--- Page 17 ---\nlog(nterm) log( nlex) log( nnonterm ) log( nnonlex)\ngpt-4.1-nanorF1 0.26 0 .23 0 .14 −0.10\nrAcc. 0.15 0 .17 0 .09 −0.05\ngpt-4.1-minirF1 0.07 −0.02 −0.13 −0.57\nrAcc. 0.09 0 .02 −0.13 −0.45\ngpt-4.1rF1 −0.11 −0.37 −0.52 −0.59\nrAcc. −0.13 −0.28 −0.25 −0.27\no4-minirF1 −0.20 −0.39 −0.60 −0.79\nrAcc. −0.15 −0.28 −0.20 −0.36\no3rF1 −0.12 −0.22 −0.37 −0.75\nrAcc. −0.06 −0.12 −0.17 −0.40\ngemma-3-1brF1 −0.21 −0.24 −0.19 −0.20\nrAcc. −0.09 −0.13 −0.13 −0.11\ngemma-3-4brF1 −0.22 −0.36 −0.49 −0.61\nrAcc. −0.05 −0.09 −0.21 −0.19\nDSR1-7BrF1 −0.14 −0.24 −0.43 −0.38\nrAcc. −0.12 −0.20 −0.20 −0.20\nTable 2: Pearson correlation coefficients rbetween models’ accuracy/macro F1 scores and grammar\nhyperparameters, including the compression ratio CR of positive examples and the numbers of terminal\nsymbols nterm, non-terminal symbols nnonterm , lexical productions nlex, and non-lexical productions\nnnonlex . Correlation scores are taken over the mean F1 and accuracy values grouped by grammar.\ngpt-4.1-nano\ngpt-4.1-mini\ngpt-4.1\no4-mini\no3\ngemma-3-1b\ngemma-3-4b\nDSR1-7B1 100 productions 101 200 201 300 301 400 401 500\nFigure 11: Spearman’s rank correlation coefficients for the per-example accuracies between different\nmodels, faceted by example length. On short examples of length ≤10, all models are moderately\ncorrelated with one another; on longer examples, gpt-4.1-nano andgpt-4.1.mini become less\ncorrelated with the other models.\nD Inference Cost, Setup, and Hyperparameters\nEvaluations on OpenAI models used roughly $15k total of compute credits. Evaluations on the open-\nweights models were run on a GPU cluster using Nvidia A100s, H100s V100s, and RTX8000s; models\nwere loaded using the HuggingFace transformers library (Wolf et al., 2020).\nTable 4 reports the inference hyperparameters used in our experiments. For the open-weights models,\nwhich we run on local hardware, we restrict the number of new tokens ( max_completion_tokens ) to\n4096 to limit memory usage and inference time. For all models, we generate completions with sampling\nusing the default parameters (temperature τ, truncation constant k, nucleus constant p) specified by the\nmodel or API.\nE Test-time-compute\nIn addition to the graph of relative test-time-compute by example length discussed above in section 6.3,\nwe also provide a graph of the absolute test-time-compute (i.e., the total number of completion tokens\ngenerated by models) as a function of example length in fig. 12 below.\n17\n--- Page 18 ---\nTerm Coefficient p-value Sig.?\nIntercept 46.56 <0.001***\ngpt-4.1-mini 13.18 <0.001***\ngpt-4.1 19.43 <0.001***\no4-mini 21.69 <0.001***\no3 26.26 <0.001***\ngemma-3-1b 7.87 <0.001***\ngemma-3-4b 7.38 <0.001***\nDSR1-7B 9.66 <0.001***\nlog10(nnonlex)∗gpt-4.1-nano −2.28 <0.001***\nlog10(nnonlex)∗gpt-4.1-mini −24.17 <0.001***\nlog10(nnonlex)∗gpt-4.1 −10.28 <0.001***\nlog10(nnonlex)∗o4-mini −14.41 <0.001***\nlog10(nnonlex)∗o3 −23.54 <0.001***\nlog10(nnonlex)∗gemma-3-1b −4.44 <0.001***\nlog10(nnonlex)∗gemma-3-4b −9.93 <0.001***\nlog10(nnonlex)∗DSR1-7B −9.78 <0.001***\nlog10(ℓ)∗gpt-4.1-nano −26.79 <0.001***\nlog10(ℓ)∗gpt-4.1-mini 0.72 0 .385\nlog10(ℓ)∗gpt-4.1 15.93 <0.001***\nlog10(ℓ)∗o4-mini 8.73 <0.001***\nlog10(ℓ)∗o3 −1.36 0 .102\nlog10(ℓ)∗gemma-3-1b 28.37 <0.001\nlog10(ℓ)∗gemma-3-4b 41.20 <0.001***\nlog10(ℓ)∗DSR1-7B 28.03 <0.001***\nlog10(nnonlex)∗log10(ℓ) −10.28 <0.001***\nTable 3: Regression coefficients for accuracy by model, log10(nnonlex), and example length ℓ. The base\nlevel of the model factor is gpt-4.1-nano . Significance values are listed as ***forp<0.001,**for\np<0.01, and *forp<0.05.\nModel Attention New token limit τ k p\ngpt-4.1-nano — None 1.0 — 1.00\ngpt-4.1-mini — None 1.0 — 1.00\ngpt-4.1 — None 1.0 — 1.00\no4-mini — None 1.0 — 1.00\no3 — None 1.0 — 1.00\ngemma-3-1b-it sdpa 4096 1 .0 64 0 .95\ngemma-3-4b-it sdpa 4096 1 .0 64 0 .95\nDeepSeek-R1-Distill-Qwen-7B sdpa 2048 0 .6 — 0.95\nTable 4: Inference hyperparameters used in experiments ( sdpa : scaled dot product attention).\nF Chains-of-Thought\nThis appendix provides a few examples of the chains-of-thought (token generated between the prompt\nand the answer) produced by two of the models, DeepSeek-R1-Distill-Qwen-7B (which we abbre-\nviate here as DSR1) and gpt-4.1-mini .\nF.1 DSR1, Grammar 20250218222557, Example A\nExample t75 t104 t120 t27 t137 t27 t11 t11 t11 t173 t181 t27 t84 t11 t82 t104\nt120 t168 t69 t141 t158 t9 t12 t140 t102 t114 t190 t141 t147 t79 t141 t159\nt178 t183 t183 t83 t141 t158 t9 t55 t74 t170 t140 t102 t114 t190 (positive)\nOkay, so I’m trying to figure out whether the given\nstring is generated by the provided context-free\n18\n--- Page 19 ---\n1 50\nT ask Complexity (Example Length)025005000750010000TTCgpt-4.1-nano\n1 50gpt-4.1-mini\n1 50gpt-4.1\n1 50o4-mini\n1 50o3Figure 12: Absolute test-time-compute expended by models as a function of example length.\ngrammar. Let me start by understanding what each\npart of the problem entails.\nFirst, the grammar is in Chomsky Normal Form (CNF),\nwhich means each production rule has exactly two\nnon-terminal symbols on the left and two on the\nright, except for the start symbol which can go to\na single terminal. Wait, actually, in CNF, each\nproduction has exactly two non-terminals on the\nright, or if there’s a terminal, it’s the start\nsymbol. The grammar seems to be structured as S\n-> NT97 NT1 and the other NT rules produce pairs of\nother NTs or terminals.\nThe string to evaluate is a sequence of tokens starting\nwith ’t75’ and ending with ’t158’, etc. So, it’s a\nconcatenation of various terminals, some of which\nare prefixed with ’t’.\nNow, the task is to see if this string can be derived\nfrom the grammar. Since the grammar is in CNF, it’s\na binary tree structure, and the string’s length\nwill relate to how the tree is structured. The\nstring has n terminals, so the parse tree should have\nn−1production steps, and each production splits a\nsymbol into two.\nLet me look at the grammar rules to see what each NT\nsymbol produces. Each line is a non-terminal that\ncan be replaced by two others. For example, S\nbecomes NT97 NT1. Then NT97 becomes NT73 NT121,\nand so on.\nI think the approach is to start from the string and\nsee if I can break it down into the non-terminals\naccording to the production rules. Since the grammar\nis in leftmost form, maybe I’ll try to parse the string\nby trying to match the leftmost symbols.\nBut before diving into parsing, perhaps it’s useful to\nnote that in CNF, the number of symbols (non-terminals\nplus terminals) increases by one each time. Wait,\nno, each step of production replaces a non-terminal\n(NT) with two NTs, but if any of those are terminals,\nthe count changes. Since in the given grammar, the\nright-hand sides seem to often produce terminals,\nexcept for some cases. For example, NT114 -> NT197\nNT79, and NT79 -> ’t115’, so that’s a terminal.\nBut looking at the structure, probably it’s a binary tree,\nand since the string has 41 symbols, which is an odd\n19\n--- Page 20 ---\nnumber. Wait, let me count the tokens: the string\nis \"t75 t104 t120 t27 t137 t27 t11 t11 t11 t173 t181\nt27 t84 t11 t82 t104 t120 t168 t69 t141 t158 t9 t12\nt140 t102 t114 t190 t141 t147 t79 t141 t159 t178 t183\nt183 t83 t141 t158 t9 t55 t74 t170 t140 t102 t114 t190\".\nLet me count these: 41 tokens. Since it’s 41, the\nparse tree could be a full binary tree with 41 leaves.\nWait, a full binary tree with height hhas2h−1leaves.\n41 is not a power of two minus one, the closest is 32\n(26−1 = 31 ),64−1 = 63 . So 41 is between 31 and\n63, so it’s not a perfectly balanced tree, so it’s a\nleft-heavy or right-heavy tree.\nWait, no, the number of symbols for a leftmost derivation\nin CNF would be for a string of length n, it would have\nn−1productions, starting from a start symbol, so\nthe start symbol would perhaps be S. But let me see the\ngiven grammar: the start symbol is S, and S -> NT97\nNT1. So the first step would split S into NT97 and NT1.\nThen, each NT can be further split.\nBut perhaps a better approach is to see if each token in\nthe string corresponds to a path in the grammar tree.\nMaybe it’s easier to look for known derivation steps\nor see if the string can be broken down into parts that\nmatch grammar productions.\nAlternatively, perhaps it would help to see if the string\ncan be represented as a sequence of NT pairs that\nconform to the given productions.\nWait, let me think: Each production in the grammar is\nof the form NT -> a NT, where a is a terminal. So\neach symbol in the string has to come from a terminal\nproduction, and the parse would involve grouping\nthe string into pairs as per the grammar’s leftmost\nproductions.\nAlternatively, starting from the beginning, the first\nsymbol is ’t75’, which must come from some terminal in\nthe grammar. Looking at the grammar, let’s find which\nnon-terminals have a production ending with ’t75’.\nWait, in the grammar, NT44 -> ’t132’, and NT197 ->\n’t178’, and NT97 -> ’t75’ according to the end of the\ngrammar list: NT97 -> ’t75’. So, NT97 can directly\nproduce ’t75’.\nSo the first part of the string is ’t75 t104 t120...’,\nwhich would mean that in the parse tree, the first NT97\n(from the start symbol S) produces ’t75’ and another\nNT. Let me check: S splits into NT97 and NT1. The\nfirst NT97 is the leftmost, so it can produce ’t75’ and\nthe other part is whatever NT1 produces.\nNow, looking at NT1: according to the grammar, NT1 ->\n’t1’. So, that’s a terminal. So the first two symbols\nin the string must be ’t75’ followed by ’t1’. Let me\ncheck the given string: the first two tokens are ’t75\nt104’, but wait, that’s not ’t75’ followed by ’t1’.\nSo that’s a problem.\nBecause if S is split into NT97 NT1, and NT1 is a terminal\n’t1’, then the first two symbols should be ’t75’\nfollowed by ’t1’. But in the string, it’s ’t75 t104\n20\n--- Page 21 ---\nt120...’, meaning the second token is ’t104’, which\ndoesn’t match ’t1’. So that would mean that the\nleftmost derivation can’t proceed as expected because\nthe second symbol is not ’t1’.\nHmm, that suggests that the string as given doesn’t start\nwith the start symbol’s leftmost expansion.\nWait, alternatively, maybe the NT1 is not the immediate\nright child but another expansion. But according\nto the grammar, S splits into NT97 and NT1. NT1 is a\nterminal ’t1’, so the first token in the string must\nbe ’t75’ and the second must be ’t1’. But in the given\nstring, the first two are ’t75’ and ’t104’, which is a\nmismatch.\nTherefore, it’s not possible that the entire string is\ngenerated by this grammar because the second symbol\ndoesn’t match ’t1’.\nWait, but wait a minute: perhaps the grammar allows\nfor S to split into NT1 and NT97. Let’s check: the\noriginal grammar says S -> NT97 NT1, not S -> NT1 NT97.\nSo the order is fixed. The leftmost symbol is NT97,\nso it must produce ’t75’ followed by whatever NT1\nproduces.\nTherefore, the first two symbols must be ’t75’ and ’t1’,\nbut in the string, it’s ’t75’ followed by ’t104’,\nwhich doesn’t fit. Therefore, the string cannot be\ngenerated by this grammar.\nSo the answer would be ’No’.\n</think>\nNo\nF.2 DSR1, Grammar 20250402155408_676876, Example A\nExample t136 (negative)\nAlright, I’m trying to figure out if the string ‘t136‘ is\nin the given context. Let me go through the patterns\nstep by step.\nLooking at the patterns, I see that ‘t136‘ might\ncorrespond to multiple parts. I remember that ‘t136‘\ncould link to ‘t39‘ and ‘t6‘ based on some earlier\npatterns.\nI also notice that ‘t39‘ connects to ‘t8‘ and ‘t3‘. So,\nif ‘t136‘ connects to ‘t39‘ and ‘t6‘, that would\nmake sense. Additionally, ‘t6‘ is part of other\nconnections, like ‘t6‘ going to ‘t15‘ and ‘t6‘ itself.\nThere’s another part where ‘t136‘ connects to ‘t102‘ and\n‘t137‘. I’m not sure about ‘t102‘, but I think it’s a\nvalid link in the graph.\nPutting it all together, it seems like ‘t136‘ is a valid\nnode in the context. So, the answer should be ’Yes’.\n</think>\nYes\n21\n--- Page 22 ---\nF.3 DSR1, Grammar 20250402155408_676876, Example B\nExample t44 t249 t80 t333 t171 t194 t329 t224 t196 t64 t8 t43 t16 t15 t280 t220\nt107 t230 t334 t207 t19 t296 (negative)\nAlright, so I’ve got this problem where I need to evaluate\na string of tokens and determine if a specific\ncondition is met. Let me break it down step by step.\nFirst, I see that the task involves checking if the\ntoken ’t27’ appears in the output of a certain string\nevaluation. The string to evaluate is ‘t44 t249 t80\nt333 t171 t194 t329 t224 t196 t64 t8 t43 t16 t15 t280\nt220 t107 t230 t334 t207 t19 t296‘.\nI notice that the tokens are separated by spaces, so I can\nlist them out for clarity:\n- t44\n- t249\n- t80\n- t333\n- t171\n- t194\n- t329\n- t224\n- t196\n- t64\n- t8\n- t43\n- t16\n- t15\n- t280\n- t220\n- t107\n- t230\n- t334\n- t207\n- t19\n- t296\nMy goal is to see if any of these tokens is ’t27’. If it\nis, I should return ’Yes’; otherwise, ’No’.\nNow, I’ll go through each token one by one to check for\n’t27’:\n1. t44 →Not t27. 2. t249 →Not t27. 3. t80 →Not\nt27. 4. t333 →Not t27. 5. t171 →Not t27. 6.\nt194→Not t27. 7. t329 →Not t27. 8. t224 →Not\nt27. 9. t196 →Not t27. 10. t64 →Not t27. 11.\nt8→Not t27. 12. t43 →Not t27. 13. t16 →Not\nt27. 14. t15 →Not t27. 15. t280 →Not t27. 16.\nt220→Not t27. 17. t107 →Not t27. 18. t230 →\nNot t27. 19. t334 →Not t27. 20. t207 →Not t27.\n21. t19 →Not t27. 22. t296 →Not t27.\nAfter reviewing all the tokens, I don’t see ’t27’\nanywhere in the string. Each token is unique and\nnone of them is ’t27’.\nSo, based on my evaluation, the output of the evaluation\ndoesn’t contain ’t27’. Therefore, the condition\nisn’t met, and the answer should be ’No’.\n22\n--- Page 23 ---\n<think>\nNo\nF.4 gpt-4.1-mini, Grammar 20250415162507_949738, Example A\nExample t30 t24 t24 t23 t4 (positive). Here, gpt-4.1-mini correctly classifies the example by\nexhasutively searching for a parse tree that is compatible with the string.\nLet’s analyze the grammar carefully and the string to verify if the\nstring can be generated by the grammar.\n–-\n### Grammar summary\nNon-terminals: S, NT0, NT2, NT3, NT4, NT5, NT6\nTerminals: tokens like ‘’t1’‘, ‘’t4’‘, ‘’t5’‘, ..., ‘’t34’‘.\n–-\n### Start symbol:\n- **S -> NT5 NT2**\n–-\n### Non-terminals expansions:\n1. **NT2**\n- NT2 -> NT3 NT6\n- NT2 -> ’t9’\n- NT2 -> ’t23’\n- NT2 -> ’t27’\n- NT2 -> ’t4’\n- NT2 -> ’t30’\n2. **NT3** (various terminals)\n- ’t27’, ’t29’, ’t12’, ’t1’, ’t28’, ’t20’, ’t23’, ’t24’\n(In particular, ’t24’ is here.)\n3. **NT4**\n- NT4 -> NT2 NT5\n- NT4 -> many terminals: ’t18’, ’t34’, ’t23’, ’t5’, ’t29’, ’t11’,\n’t25’, ’t4’, ’t7’\n4. **NT5**\n- NT5 -> NT0 NT5\n- NT5 -> many terminals: ’t18’, ’t23’, ’t25’, ’t13’\nAlso:\n- NT0 has many terminals like ’t24’, ’t27’, ’t13’, etc., including\n’t24’\n5. **NT6**\n- NT6 -> NT4 NT6\n- NT6 -> many terminals: ’t8’, ’t25’, ’t29’, ’t22’, ’t12’, ’t9’, ’t18’,\n’t10’\n–-\n23\n--- Page 24 ---\n### String to parse:\n‘t30 t24 t24 t23 t4‘\n–-\n### Step 1: Since S -> NT5 NT2\nThe string breaks into two substrings:\n- substring generated by NT5\n- substring generated by NT2\nLet’s try possible splits:\n- split after 1 token: (t30) (t24 t24 t23 t4)\n- split after 2 tokens: (t30 t24) (t24 t23 t4)\n- split after 3 tokens: (t30 t24 t24) (t23 t4)\n- split after 4 tokens: (t30 t24 t24 t23) (t4)\nTry each possibility.\n–-\n### Step 2: Can NT5 generate the initial substring?\n**NT5 options:**\n- NT5 -> NT0 NT5\n- NT5 -> terminals like ’t18’, ’t23’, ’t25’, ’t13’ (no ’t30’ here)\n**NT0 terminals:**\n- ’t24’ (yes)\n- ’t27’\n- ’t13’\n- ’t6’\n- ’t30’ (yes)\n- ’t31’\n- ’t14’\n- ’t9’\n- ’t34’\nSo ’t30’ is in NT0 terminals.\n–-\nTry first split:\n- NT5 = ’t30’\n- NT2 = ’t24 t24 t23 t4’\nIs NT5 -> ’t30’ possible?\nNo, because NT5 terminals do not include ’t30’ directly. But NT5 ->\nNT0 NT5 or NT5 terminal.\nCould NT5 be a single terminal? The terminals allowed for NT5 are\n’t18’, ’t23’, ’t25’, ’t13’.\nNo ’t30’ here. So NT5 alone cannot be ’t30’.\n24\n--- Page 25 ---\nTry NT5 -> NT0 NT5:\nTry to parse first token ’t30’ as NT0 (allowed: yes), remaining NT5 ->\n?\nRemains 0 tokens for NT5. But NT5 cannot be empty.\nNT5 is not nullable (no empty production), so this is not possible.\n–-\nTry split after 2 tokens:\nNT5 -> ’t30 t24’\nCan NT5 generate ’t30 t24’?\nTry to parse ’t30’ as NT0 and ’t24’ as NT5 (or recursively):\nIs ’t24’ a terminal of NT5? ’t24’ is not a terminal of NT5 (only ’t18’,\n’t23’, ’t25’, ’t13’).\nSo ’t24’ terminal cannot be produced directly by NT5.\nBut NT5 -> NT0 NT5 children, so must break as:\nNT5 -> NT0 NT5\nNT0 -> ’t30’ or ’t24’ (both allowed) NT5 -> ? handling ’t24’\nTry NT5 -> NT0 NT5:\n- For tokens t30 t24,\nparse 1st ’t30’ as NT0 (allowed)\nthen t24 token for NT5 substring\nIs ’t24’ a terminal of NT5? No.\nTry again NT5 -> NT0 NT5 with NT0 producing ’t24’, then empty remaining\nNT5\nBut NT5 cannot be empty.\nSo no.\n–-\nTry split after 3 tokens:\nNT5 generates ’t30 t24 t24’\nTry to parse:\n- NT5 -> NT0 NT5\nBreak ’t30’ as NT0 (allowed)\nRemaining ’t24 t24’ as NT5\n25\n--- Page 26 ---\nTry again:\n- NT5 -> NT0 NT5\n’ t24’ as NT0 (allowed)\nRemaining ’t24’ as NT5\nNow check ’t24’ as NT5\nIs ’t24’ in NT5 terminals? No.\nNT5 terminals are ’t18’, ’t23’, ’t25’, ’t13’.\nTry NT5-> NT0 NT5 again\nParse ’t24’ as NT0 (allowed), NT5 remaining empty string? No empty\nstrings allowed.\nNo.\nTry NT5 as single terminal? ’t24’ is not in NT5 terminals.\nFail.\n–-\nTry split after 4 tokens:\nNT5 = ’t30 t24 t24 t23’\nTry parsing tokens stepwise:\n- t30 as NT0 (allowed)\nremaining t24 t24 t23 as NT5\nTry NT5->NT0 NT5 again:\n- t24 as NT0 (allowed)\nremaining t24 t23 as NT5\nAgain:\n- t24 as NT0 (allowed)\nremaining t23 as NT5\nCheck ’t23’ in NT5 terminals? Yes! NT5 -> ’t23’ allowed.\nSo:\nNT5 derivation:\nt30 (NT0)\nt24 (NT0)\nt24 (NT0)\nt23 (NT5 terminal)\nWait, but this would only work if NT5 allows chaining of:\nNT5 -> NT0 NT5 (>=1)\n26\n--- Page 27 ---\nEnding with NT5 terminal.\nSo parse:\n- NT5:\nNT5 -> NT0 NT5\nNT0 -> ’t30’\nNT5 -> NT0 NT5\nNT0 -> ’t24’\nNT5 -> NT0 NT5\nNT0 -> ’t24’\nNT5 -> ’t23’ (terminal)\nTokens: t30 t24 t24 t23\nSo yes, this derives the 4 tokens.\n–-\n### Step 3: Now check if NT2 can generate last token: ’t4’\nNT2 terminals: ’t9’, ’t23’, ’t27’, ’t4’, ’t30’\nYes, ’t4’ is a terminal allowed by NT2\nTherefore, NT2 can directly generate ’t4’\n–-\n### Step 4: Full derivation\nS -> NT5 NT2\nNT5 -> ’t30 t24 t24 t23’ (via recursive NT5 -> NT0 NT5)\nNT2 -> ’t4’ terminal\nThe full string ‘t30 t24 t24 t23 t4‘ is generated by S.\n–-\n### Final conclusion:\nThe string can be generated by the grammar.\n–-\n**Answer:**\nYes\nG Analysis of Strategies using the LLM-as-a-Judge Paradigm\nTo categorize the strategies used by the gpt-4.1- series of models, we instruct o4-mini to classify\nmodel completions into one of three categories (following the ‘LLM-as-a-judge’ framework of Zheng\net al. 2023): rule-based , where the model provides a complete derivation for the example or proves\nthat one cannot exist; heuristic , where the model appeals to distributional properties of the example or\npartial derivations to argue that a string is (un)likely to be generated by the grammar; or code , where\nthe model defines a program or algorithm to solve the task and then hallucinates ‘running’ the function\nto arrive at the result. We use the following prompt:\n27\n--- Page 28 ---\nYou will be presented with a completion from an LLM which was given a\ncontext-free grammar and a string of symbols drawn from that grammar’s\nset of terminal symbols and asked to determine whether the string is\ngenerated by the grammar or not. Your job is to classify how the LLM\nattempted to solve the task by binning the completion strategy into one\nof the following categories:\n- ‘heuristic‘: The LLM attempts to solve the task by using heuristics\nit surmises from the grammar, such as “if the string is long, it is\nlikely generated by the grammar” or “the string only contains terminal\nsymbols present in the grammar, so it’s likely a positive sample”.\nCount strategies as heuristic if they appeal to the existence of\ncertain production rules but do not rigorously determine that no such\nderivation exists.\n- ‘rule-based‘: The LLM attempts to solve the task by writing out\nthe FULL DERIVATION of the sample from the grammar, or rigorously\ndetermining that no such derivation exists. Only count strategies\nas rule-based if the LLM doesn’t use any guesswork to reach its final\nconclusion.\n- ‘code‘: The LLM attempts to solve the task by writing out a program\nor algorithm which it claims will solve the task. This includes\nwriting out a program in a programming language, or writing out\npseudocode.\nYou can write as much as you want in your answer, but please end\nyour response with the name of the classification you think is most\nappropriate.\nHere is the LLM’s completion:\n“‘\n{completion}\n“‘\nTo validate the accuracy of o4-mini ’s categorizations of model responses, we first prompt o4-mini\nto categorize the completions for a single grammar. We then sample 20 ( model response ,predicted\ncategory ) pairs and have three of the paper authors each label the model responses according to the\nsame rubric. We then compute the inter-annotator agreement, majority-annotator categories, and\no4-mini ’s agreement with the majority class. We attain an inter-annotator agreement of 93.3%, and\no4-mini attains a majority-class agreement of 70% and a weighted-class agreement of 73.3%, as\nshown in table 5. We qualitatively note that o4-mini is stricter than the three annotators at categorizing\nresponses as rule-based ; all instances of disagreement with the majority class are cases when annotators\nlabeled a response as rule-based while o4-mini labeled it as heuristic .\nH Completion Tokens Distribution\nThe three models ( gemma-3-1b ,gemma-3-4b , and DSR1-7b ) were hosted locally and, for computa-\ntional reasons, were limited in the number of tokens they were allowed to produce. Here we examine\nthe distribution of the lengths of the completions, with an eye towards determining whether the token\nlimit was responsible for the poor performance relative to other models. We find that the two Gemma 3\nvariants mostly generate short completions well below the prescribed token limit, but that DSR1\nnearly always reaches the limit (fig. 13, top). We also find that all models attain a higher accuracy on\nincomplete generations than on completed ones, suggesting that extending token the limits is unlikely to\nimprove performance. Part of this behavior may be due to the heuristic method we use to classify model\nresponses; since we categorize predictions by extracting keywords using a regular expression, models\nmay produce accidental predictions. For Gemma 3 models, these predictions are mostly symmetric\naround 50%, indicating that they produce false positives and negatives in roughly equal proportions.\nDSR1, by contrast, appears to have a rather skewed distribution for accuracy on incomplete generations,\nwhich is likely a result of its tendency to produce the word “ No” as a part of its chain of thought.\n28\n--- Page 29 ---\nA B C Majority o4-mini IAA MA WA\n1 heuristic rule-based rule-based rule-based heuristic 0.667 0 0 .333\n2 rule-based rule-based rule-based rule-based rule-based 1 1 1\n3 rule-based rule-based rule-based rule-based rule-based 1 1 1\n4 rule-based rule-based rule-based rule-based heuristic 1 0 0\n5 heuristic rule-based rule-based rule-based heuristic 0.667 0 0 .333\n6 heuristic heuristic heuristic heuristic heuristic 1 1 1\n7 rule-based rule-based rule-based rule-based rule-based 1 1 1\n8 heuristic rule-based rule-based rule-based heuristic 0.667 0 0 .333\n9 rule-based rule-based rule-based rule-based rule-based 1 1 1\n10 rule-based rule-based rule-based rule-based rule-based 1 1 1\n11 heuristic rule-based rule-based rule-based rule-based 0.667 1 0 .667\n12 heuristic heuristic heuristic heuristic heuristic 1 1 1\n13 rule-based rule-based rule-based rule-based heuristic 1 0 0\n14 rule-based rule-based rule-based rule-based rule-based 1 1 1\n15 rule-based rule-based rule-based rule-based heuristic 1 0 0\n16 rule-based rule-based rule-based rule-based rule-based 1 1 1\n17 heuristic heuristic heuristic heuristic heuristic 1 1 1\n18 rule-based rule-based rule-based rule-based rule-based 1 1 1\n19 rule-based rule-based rule-based rule-based rule-based 1 1 1\n20 heuristic heuristic heuristic heuristic heuristic 1 1 1\n0.933 0 .7 0 .733\nTable 5: Validation of o4-mini ’s categorizations for 20 model responses. We report inter-annotator-\nagreement (IAA) among the three annotators, the majority agreement (MA) between o4-mini and the\nannotator majority, and the weighted agreement (WA) between o4-mini and the three annotators.\n0 2000 4100\nCompletion length0%50%100%Completionsgemma-3-1b\n0 2000 4100gemma-3-4b\n0 1000 2000DSR1-7B\n0% 50% 100%\nMean accuracy, by grammar0%10%20%Completions\n0% 50% 100% 0% 50% 100%Incomplete Finished generating\nFigure 13: ( top row ) Among open-weights models, the Gemma 3 variants mostly generated comple-\ntions well below the new token limit, while DSR1 routinely failed to finish completions within bounds.\n(bottom row ) Within each grammar, models have a lower accuracy on finished completions than they\ndo on incomplete generations.\n29",
  "text_length": 85733
}