{
  "id": "http://arxiv.org/abs/2506.01197v1",
  "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures",
  "summary": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts\nto learn a set of human-understandable concepts that can explain variation on\nan abstract space. A basic limitation of this approach is that it neither\nexploits nor represents the semantic relationships between the learned\nconcepts. In this paper, we introduce a modified SAE architecture that\nexplicitly models a semantic hierarchy of concepts. Application of this\narchitecture to the internal representations of large language models shows\nboth that semantic hierarchy can be learned, and that doing so improves both\nreconstruction and interpretability. Additionally, the architecture leads to\nsignificant improvements in computational efficiency.",
  "authors": [
    "Mark Muchane",
    "Sean Richardson",
    "Kiho Park",
    "Victor Veitch"
  ],
  "published": "2025-06-01T22:20:07Z",
  "updated": "2025-06-01T22:20:07Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01197v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01197v1  [cs.CL]  1 Jun 2025Incorporating Hierarchical Semantics in Sparse\nAutoencoder Architectures\nMark Muchane, Sean Richardson, Kiho Park, and Victor Veitch\nUniversity of Chicago\nAbstract\nSparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn\na set of human-understandable concepts that can explain variation on an abstract space.\nA basic limitation of this approach is that it neither exploits nor represents the semantic\nrelationships between the learned concepts. In this paper, we introduce a modified\nSAE architecture that explicitly models a semantic hierarchy of concepts. Application of\nthis architecture to the internal representations of large language models shows both\nthat semantic hierarchy can be learned, and that doing so improves both reconstruction\nand interpretability. Additionally, the architecture leads to significant improvements\nin computational efficiency. Code is available at github.com /muchanem /hierarchical-\nsparse-autoencoders.\n1 Introduction\nDictionary learning—and, in particular, sparse autoencoders (SAEs)—have attracted signifi-\ncant attention as a tool for interpreting representations in large language models [Bri+23;\nCun+23; Gao +24; Lie +24; Lin +25]. The aim of these methods is to jointly learn some set\nof human-understandable concepts that can explain the model’s behavior, and a map from\nthe model’s internal representations to these concepts. Empirically, at least some of the\nfeatures learned by SAEs do seem clearly semantically interpretable—for example, “Golden\nGate Claude” used an SAE feature to coherently steer Claude [Tem+24]. However, despite\nconsiderable effort, these models have some strong limitations. In particular, the learned\nfeatures generally do not suffice to accurately reconstruct the input—limiting the value for\ninterpretability—and, as the model size increases, the features that are learned often seem\nsemantically unnatural. For example, Chanin et al. [Cha+24]find that increasing the model\nsize leads to a phenomenon they call “feature splitting”—where a single high-level concept\nis represented by multiple low-level features. The underlying tension here is that pushing\nfor accurate reconstruction leads us to increase the number of features, but increasing the\nnumber of features can lead to very specialized features that are not generally useful.\nThe goal of this paper is to improve this reconstruction-interpretability frontier by exploiting\nthe hierarchical structure of semantics. The motivating observation is that concepts that\nare meaningful to humans are often organized in a hierarchical fashion—for example,\ncorgi, greyhound, and shitzu are all particular instances of the general concept of dog.\nStandard dictionary learning will not make use of this hierarchical structure at all (instead,\nrepresenting each feature individually). Intuitively, we would like to modify the data\nstructure such that this hierarchy is explicitly represented. The hope is that this will allow us\nto capture the reconstruction power of large dictionaries while maintaining interpretability\nby appealing to the human-understandable structure of the hierarchy.\nThe main contribution of this paper is a new architecture for SAEs that explicitly, and highly\nefficiently, models hierarchical relationships between concepts. Concretely:\n1\n--- Page 2 ---\nExpert #858 \nSublatent 2 Sublatent 4 Sublatent 11 \nFigure 1: A Hierarchical Sparse Autoencoder architecture learns human interpretable hierarchy. Each\nbox shows 5 of the strongest activating contexts for the feature, all tokens underlined activate the\nfeature while the bold /text weight shows the relative strength of activation. For example, a \"marriage\"\nhigh-level feature and \"divorce\", \"engagement\", and \"marriage\" sublatents. Note that “48 sati svadba\" is\na Serbian wedding reality TV show and “Shaadi\" is the Hindi word for marriage.\n1.Park et al. [Par+24]give foundational results on how hierarchical structure is rep-\nresented in language models. We show how to translate this theory into a mixture-\nof-experts type architecture (see Figure 2) that captures hierarchical structure (see\nFigure 1).\n2.We then show that this architecture strongly improves the reconstruction performance\nof SAEs, while maintaining or improving interpretability.\nAs an additional benefit, the new architecture is highly computationally efficient. In principle,\nthis can allow scaling SAEs to much larger effective dictionary sizes, allowing fine-grained\nrepresentations of a vast number of concepts.\n2 Background and Related Work\nSparse Autoencoders SAEs are a particular approach to sparse dictionary learning, a\ngeneral class of unsupervised learning algorithms that aim to find a dictionary of human-\ninterpretable features (or atoms) that can be used to reconstruct the input data. The hope\nis that by enforcing sparsity we can recover some set of underlying latent factors. There are\na large number of methods based on this idea.\nWe’ll focus on the top-k SAE approach of Gao et al. [Gao+24], which has the advantages of\nbeing simple and scalable. The idea is to map each input vector xto a latent representation\nthat is sparsified by a TopKkoperation that preserves the klargest values and sets all others\nto zero. Then, the vector is reconstructed by decoding this sparse latent representation. In\ntotal, the SAE operation is given by:\nSAEk(x) =DTopKk\u0000\nLeakyReLUα(E(x−b))\u0001\n(2.1)\n2\n--- Page 3 ---\nwhere x∈Rdis the input, Eis the encoder matrix, Dis the decoder matrix, bis a bias\nvector (with the rows, columns, and dimension of E,D, and brespectively being equal to\nthe number of features) kis a hyperparameter, and α=1p\ndis a threshold for the LeakyReLU\nactivation (where LeakyReLU has some small negative slope below the threshold). The\nvectors in Dare referred to as “features” or “latent vectors.” The parameters are learned by\nminimizing the reconstruction loss:\nLrecon=∥x−SAEk(x)∥2\n2(2.2)\nRelated Work The use of sparse autoencoders in LLM interpretability has been a topic\nof considerable interest [SBb22; Bri +23; Cun +23; Gao +24; Tem +24; Lie +24]. These\nworks have shown that SAEs can be used to interpret internal representations, with down-\nstream applications such as circuit discovery [DCN24; Mar +25; Lin +25], identifying “multi-\ndimensional” representations [Eng+25], and steering model behavior [O’B+24]. A large\nbody of complementary work has introduced modifications to the architecture and /or\ntraining procedure for SAEs, primarily focusing on improving reconstruction loss at a given\nsparsity level, dictionary size, or compute budget [BLN24; Raj +24b; Raj +24a].\nThe most closely related is a line of work focused on ameliorating feature splitting. For\ninstance, Matroyshka SAEs [Bus+25]and EWG-SAEs [LR25]both operate by defining\ngroupings of dictionary atoms and then modifying the training objective of the SAE to\nencourage some of the groupings to contain coarser features, and other groupings to contain\nfiner features. These approaches are also motivated by the hierarchical nature of semantics.\nIn contrast to the approach we take here, they do not modify the flat set structure of\nthe architecture. Modifying the architecture has the advantages that it both makes the\nhierarchical structure explicit—improving interpretability—and also allows us to leverage\nthe hierarchical structure to greatly improve computational efficiency, as we will see.\nWe also highlight Switch SAEs [Mud+24], which introduce a mixture-of-experts type routing\nmechanism shares some structural similarities to our approach. However, this approach\nis entirely motivated by pushing the reconstruction quality vs sparsity frontier for a given\ncompute budget. Each expert has no high vs low-level distinction and isn’t designed to\ncontain a set of features that should be interpreted together. Indeed, they find that their\ndecoder features do not cluster in any particular way.\nOur focus on the need to align SAE architecture with the structure of the data is emphasized\nby Hindupur et al. [Hin+25], who establish a duality between SAE architectures and\nassumptions about concept geometry. They show that several standard SAE architectures\nare incompatible with plausible assumptions about the geometry of underlying concepts.\nWhile the SAE architecture presented here does not seek to satisfy all of these assumptions,\nit addresses hierarchical structure in particular.\nThe ideas here connect to the broader field of causal representation learning (CRL), which\naims to learn the causal factors underlying a data-generating process [Loc+19; Loc +20;\nSch+21; AHB22a; Bre +22; Lip +23; MA25 ]. Recent work has extended CRL to foundation\nmodel analysis, seeking identifiable and disentangled representations of concepts [Raj+24c;\nJos+25]. In this spirit, we also aim to learn disentangled concept representations.\n3 Hierarchical Sparse Autoencoder Architecture\nWe now turn to the development of an architecture that bakes in the hierarchical structure\nof concepts.\n3\n--- Page 4 ---\nx E z D ˆ xhigh\nx Πdown\nj Ej zj Dj Πup\nj ˆ xlow\nj+ ˆ x\nFor each j∈TopKIndicesk(z)Figure 2: The structure of the Hierarchical Sparse Autoencoder architecture. The design combines a\ntop-level encoder-decoder (upper path) that captures general concepts with expert-specific autoencoders\n(lower paths) that model refined features. For a given input, only a sparse subset of experts is activated,\nenhancing computational efficiency while maintaining expressive power.\n3.1 Architecture\nHierarchical Geometry Our main inspiration follows Park et al. [Par+24], who find that\nthe representations of categorical concepts in language models have a specific geometric\nstructure. In particular, they show that every categorical concept has tworepresentations:\na parent feature indicating whether the concept is active, and a low-rank subspace of the\nrepresentation space containing a polytope where each point is a child features corresponding\nto a different possible value of the concept. For example, the concept “dog” is represented\nby a parent feature indicating whether the concept is active (‘is dog’ vs ‘not dog’), and\na low-dimensional subspace containing a polytope where each point is a child feature\ncorresponding to a different breed of dog.\nThe idea is to create an architecture that matches this dual representation structure; see\nFigure 2. To achieve this, we propose an architecture with three parts:\n1.A top-level SAE that aims to capture the binary feature indicating whether a high-level\nconcept is active. This is a standard SAE with a relatively small number of features.\n2.For each atom in the high-level SAE, we have an associated down projector onto a low-\ndimensional subspace (and an up-projector mapping back to the original space). This\ncorresponds to the low-dimensional subspace associated to the categorical concept.\nSince each high-level concept spans a low-dimensional subspace, the only requirement\nfor the ‘projection’ is staying within the column space of the concept. Thus, we do not\nimpose idempotence or symmetry and simply use 2 trainable matrices.\n3.For each atom in the high-level SAE, we have a low-level SAE that operates on the\nlow-dimensional subspace associated with atom. The elements of each such SAE\ncorrespond to the different possible values of the categorical concept (e.g., the different\nbreeds of dog).\nThat is, the architecture is a high-level SAE where each atom also has a low-level SAE\nattached to it.\nNow, a critical observation here is that an atom in the low-level SAE can only be active if\nthe high-level atom is also active. That is, for the residual stream vector xto encode the\nconcept of “corgi” it must also encode the concept of “dog.” To respect this constraint, we\nstructure the model as a mixture-of-experts type architecture, where a low-level SAE is\nonly called if the associated high-level feature is active. This both respects the hierarchical\nstructure of the concepts and also allows for an enormous computational efficiency gain\n(see below).\nIn fact, the Park et al. [Par+24]results are more precise than the informal presentation\nabove. In particular, they show that a low level concept (“corgi\") is naturally represented\nas the sum of a vector for the high-level concept (“dog\") and a vector representing the\n4\n--- Page 5 ---\nAlgorithm 1 Hierarchical SAE Forward Pass and Loss Computation\nfunction ForwardPass( x)\nz←TopKk(LeakyReLUα(Ex)) ▷High-level encoding\nK← indices of nonzero elements in z\nˆxhigh←Dz ▷High-level reconstruction\nˆxlow←0 ▷Initialize low-level reconstruction\nforj∈K do ▷Only process activated experts\nxsub\nj←Πdown\njx ▷Project to expert subspace\nzj←LeakyReLUα(Ejxsub\nj) ▷Low-level encoding\nˆxsub\nj←Djzj ▷Reconstruct in subspace\nˆxlow←ˆxlow+Πup\njˆxsub\nj▷Project back and accumulate\nend for\nˆx←ˆxhigh+ˆxlow▷Combined reconstruction\nreturn ˆx,z,{zj}j∈K\nend function\nfunction ComputeLoss( x,ˆx,z,{zj}j∈K)\nLrecon←∥x−ˆx∥2\n2\nLtop_recon←∥x−Dz∥2\n2\nLrecon←Lrecon+βLtop_recon ▷Encourage meaningful top-level features\nLsparse←∥z∥1+P\nj∈K∥zj∥1\nLortho←∥DE−diag(DE)∥F\nn2−n\nL←Lrecon+λ1Lortho+λ2Lsparse\nreturnL\nend function\nlow-level concept in the context of the high level space (i.e., “corgi\" =“dog\" +“corgi|\ndog\"). It is these contextual representations that live in a low-dimensional space. This\noverall structure—a low level concept is represented as the sum of a high-level concept\nplus the low-level concept in the context of the high-level one—is exactly the structure\nimplemented by the H-SAE.\nForward Pass We can now make the architecture precise:\nH-SAE (x) =X\nj∈TopKIndicesk\u0000\nzjdj|{z}\nˆxhigh+Πup\njSAEj\n1(Πdown\njx)\n|{z}\nˆxlow\nj\u0001\n(3.1)\nwhere TopKIndiceskare the indices of the top kfeatures, djis the j-th high-level feature,\nzj= (LeakyReLU (Ex))jis the corresponding code, SAEj\n1is the expert-specific autoencoder\nfor high-level feature j, andΠj\nupandΠj\ndownare projection matrices (of dimension s) that\nmap the input to the expert subspace and back to the original space, respectively. Note that\nthe low-level SAE uses TopK1over its afeatures, retaining only a single low-level feature\nfor each expert. This corresponds to the idea that the representation of a subordinate\nconcept should be at a particular vertex in the polytope corresponding to the categorical\nconcept.\nWe depict this architecture visually in Figure 2 and algorithmically in Algorithm 1.\nComputational Efficiency Beyond explicitly enforcing the target semantic structure,\nactivating the low-level SAE only when the corresponding high-level feature is active\nprovides a significant computational advantage. The computational cost of a forward pass\n5\n--- Page 6 ---\ncan be expressed as follows:\nCostforward =O(mtopd)\n|{z}\nHigh-level encoding+ O(ksd)|{z}\nSubspace projection+O(kmlow)|{z}\nLow-level encoding\n+O(kmlows)|{z}\nLow-level decoding+ O(ksd)|{z}\nUpward projection+ O(kd)|{z}\nHigh-level decoding\nwhere mtopis the number of high-level features (experts), dis the dimensionality of the\ninput activation vector, sis the subspace dimension for each expert, mlowis the number of\nlow-level features per expert, and kis the sparsity level (number of activated experts). In\nthe typical case where k≪mtop(sparsity) and s≪d(low dimensional subspace) the cost\nis dominated by the top-level SAE. That is, equipping the top-level SAE with hierarchical\nstructure adds negligible computational overhead. Because the hierarchical SAE is much\nmore expressive, this significantly improves SAE scalability. We also note that because the\nmemory cost of a batch gradient step scales with the number of activated parameters, not\nthe total number of parameters, this efficiency also applies to (per-step) training. Indeed,\nin practice we find that the additional cost imposed by the hierarchical structure is very\nsmall.\n3.2 Training Objective\nWe also make some minor modifications to the training objective, using the following loss\nfunction:\nL=Lrecon+λ1Lortho+λ2Lsparse (3.2)\nReconstruction Loss Following standard practice, we measure reconstruction loss with\nEuclidean distance. Additionally, to encourage the top-level SAE features to be meaningful\nin their own right (rather than just as routers) we also penalize reconstruction error from\nthe top-level SAE only. The reconstruction loss is then:\nLrecon=∥x−H-SAE (x)∥2\n2+β∥x−ˆxhigh∥2\n2, (3.3)\nwhereβ=0.1was chosen because the top-level SAE alone has less capacity than the full\nH-SAE.\nAuxiliary Losses Park et al. [PCV24 ]show that the representations of “causally separable”\n(individually manipulable) features are bi-orthogonal in a certain sense.1This suggests\nthat we could discourage semantic redundancy in the learned features (e.g., having mul-\ntiple features for “dog”) by imposing a suitable orthogonality constraint on the learned\nfeatures. To operationalize this, we introduce a bi-orthogonality penalty on the top-level\nfeatures:\nLortho=∥ED−diag(ED)∥F\nm2\ntop−mtop, (3.4)\nwhere diag(ED)is the diagonal matrix formed by the diagonal elements of the top-level\ndecoder multiplied by the top-level encoder, and ∥·∥Fdenotes the Frobenius norm. This\npushes the encoder representation for feature iand decoder representation for feature\njto be orthogonal if i̸=j. Mechanically, this means that if we ran the encoder on the\nautoencoder’s own output, whether feature ziwas active in the first encoding would not\naffect feature zjin the second encoding.\nThe motivation here is to encourage compositionality in the learned features. However, it is\nnot clear how to empirically measure compositionality, and so we are unsure whether this\n1Namely, the dot product between primal and dual space representations is zero.\n6\n--- Page 7 ---\n8 16 323456·10−2\nModel Size (Thousands of Top-Level Latents) 1 - CE Loss ScoreStandard SAE\nH-SAE - 16 sublatents\nH-SAE - 64 sublatents\n(a)The H-SAE has better reconstruction performance than a standard SAE as measured\n1 - CE Loss score across different model sizes and with both 16 and 64 sublatents\nper expert. Lower values indicate better downstream model performance, using\nthe SAEBench CE loss score.8 16 320.250.30.350.4\nModel Size (Thousands of Top-Level Latents)1 - Explained VarianceStandard SAE\nH-SAE - 16 sublatents\nH-SAE - 64 sublatents\n(b)The H-SAE has better reconstruction performance than a standard SAE as mea-\nsured 1 - Explained Variance across different model sizes and with both 16 and 64\nsublatents per expert. Lower values indicate better capture of the data’s inherent\nstructure.\nFigure 3: H-SAEs have better reconstruction performance as measured by explained variance and the\ndownstream CE loss of a Gemma 2-2B using SAE-reconstructed activation vectors.\nterm actually achieves this goal. Nevertheless, we do observe empirically that adding this\nterm does an excellent job of mitigating the “dead atom” phenomena where many features\nare never used in reconstructions. We use this instead of the auxiliary dead latent loss\nproposed by Gao et al. [Gao+24], but we do not believe this is a crucial component. See\nAppendix B for ablations.\nWe also include a small ℓ1sparsity penalty on the latent values on both the top and low-level\nfeatures outside the top k to further encourage specialization; this is the Lsparse term. Again,\nit’s unclear how to measure the target compositionality effect, and we do not believe this is\na key component. See Appendix B for ablations.\n4 Experiments\nThe main motivation for this work is that by incorporating the hierarchical structure of\nsemantics we can improve the reconstruction-interpretability frontier. Accordingly, we want\nto answer three questions:\n1. Does the H-SAE improve reconstruction?\n2. Does the H-SAE maintain, or improve, interpretability?\n3. Does the model in fact learn hierarchical semantics?\nWe will see that the answer to all three questions is yes.\nExperimental Setup Our training data consists of 1 billion residual stream vectors ex-\ntracted from layer 20 of Gemma 2-2B. We collect this data by running Gemma on a large\ncorpus of Wikipedia articles, spanning multiple languages and a wide range of topics. For\neach article, we extract the residual stream vectors corresponding to the first 256 tokens.\nFollowing standard practice, we normalize the vectors to unit norm [Lie+24]. However, we\ndo not subtract any mean vector nor include a bias term as Gao et al. [Gao+24]do, as we\nfound this decreased training stability.\n7\n--- Page 8 ---\nAs a baseline, we use the TopK SAE architecture [Gao+24]. The H-SAE is trained using the\nobjective function described in (3.2). All models are trained on the same data for 4 epochs.\nThe baseline top-k autoencoders empirically perform equivalently on reconstruction loss to\nthe Gemma Scope JumpReLU autoencoders [Lie+24]. See Appendix A for more details on\ntraining.\nReconstruction Figure 3 shows the reconstruction performance of the H-SAE and the\nbaseline at a variety of dictionary sizes. We measure both 1 - explained variance (i.e.\nnormalizedℓ2reconstruction loss) and the LLM CrossEntropy loss induced by replacing the\noriginal activations with the reconstructions (normalized by SAEBench between 0 and 1). As\nexpected, adding hierarchical capacity to the model improves reconstruction performance.\nIndeed, this effect is dramatic. The H-SAE with 64 sublatents per expert is on par with the\nstandard SAE with 32 top-level features, despite having 1 /4 the compute cost.\n4.1 Interpretability\nBy itself, an improvement in reconstruction performance may not be meaningful. The reason\nis that there are many possible modifications that improve reconstruction performance by\nsacrificing interpretability. This concern is somewhat mitigated by the fact that the extra\ncapacity of the H-SAE is highly constrained; low-level features can only be activated when\ntheir corresponding high-level feature is active. Nevertheless, we would like to directly\nevaluate the interpretability of the H-SAE against a standard SAE.\nFigure 4 shows a comparison of the baseline and H-SAE decomposition of the same context.\nWe observe that the H-SAE has features that are of the same quality, if not better. This\nbehavior is typical. See the attached GitHub repo for visualizations of hundreds of features\nand a notebook to generate more.\nTo complement the visualizations, we perform some more systematic tests. Recent work\nhas shown that general-purpose automated interpretability evaluations are misleading–\nparticularly regarding more abstract features [Hea+25]. Accordingly, we focus in particular\non feature splitting and absorption. These are aspects that we would expect to see the\ngreatest effect on through learning more general top-level features, and are relatively\nmeasurable. To test feature absorption, we run the first letter classification benchmark from\nSAEBench [Kar+25]. This benchmark tests the improvement in probing performance for a\nfirst letter classification task as the number of features used increases above 1. This task\nshould only require a single feature if there is no undesirable splitting or absorption. As\nexpected, we find that the H-SAE architecture both improves performance on this task and\ndecreases the rate of increase in absorption as width increases. See Figure 5a, where we\nreport the fraction of the projection from SAE activations onto a first-letter classification\nprobe that is not explained by a single feature.\nWe can also test for the presence of duplicate features or ‘redundancy’. One particular\nexample of this that we and others observe is equivalent syntactic SAE features from\ndifferent languages (e.g. a French period and an English period), despite the underlying\nLLM sharing features across languages [Bri+25; Den +25]. To test for this phenomenon\nsystematically we sample 1,000 English sequences from the training data, consisting of\nbetween 16 and 48 tokens. We then use Gemini-2.0-Flash to translate these sentences into\nFrench, Spanish, and German (the most common non-English languages in the training\ndata). Then, we collect the top 8 most strongly activated features on the last token of the\ncontext for the standard and H-SAE. Ideally, the features activated by the same token in\ndifferent languages should be similar. We measure the size of the symmetric set difference\nbetween the top 8 features activated by the same token in different languages (a value\nbetween 0 and 16); see Figure 5b. As expected, the H-SAE uses more similar sets of features\nacross all language pairs than the baseline SAEs.\n8\n--- Page 9 ---\nby ▁the ▁Belgian ▁comics ▁artist ▁Peyo. ▁It ▁is ▁the ▁second ▁film ▁in \nExpert #7661, sublatent 3 Expert #10479 Expert #7661 \nH-SAE \nFeature #3445 \nFeature #4431 \nFeature #1725 \nSAE \n(a)The 16k x 16 H-SAE represents a token about the creator of the Smurfs cartoon using a high level \"Pay\" homophone feature, and a low-level\n\"Pey\" token feature. This feature is then composed with a comic books feature. Wheras, the 16k SAE has features for \"words that end with\n’ey’\" and \"Pe\" words, but none of the top-32 contain a high-level feature for the ’comic book’ context of this sentence, rather the SAE uses a\n’part of an artists’ name feature to reconstruct the embedding.\n. ▁Amongst ▁those ▁ranked ▁by ▁Nielsen , ▁the ▁Telegraph ▁website ▁is\nExpert #7491, sublatent 7 \nExpert #7491 \nExpert #4121 \nH-SAE \nFeature #6576 \nFeature #2804 \nFeature #1731 \nSAE \n(b)The 8k x 16 H-SAE uses Expert #7491 which not only has the meaningful sublatent visualized here but also additional sublatents for various\nparts of writing about rating like demographics, specific ratings agencies like Nielsen, and mediums like TV episodes. Expert #4121 is a\n\"magazines\" /“news website” feature, promoting the following tokens heavily if added to the residual stream: _Forbes, _Bloomberg, _CNN,\n_BBC, _Daily, _Newsweek, _Reuters, _Huffington, _magazine, _The. We see a similar decomposition on the 8k SAE, with a newspaper feature\nand \"media ratings\" feature. However, due to the lack of granular features, the specific ratings agency ’Nielsen’ is not represented (rather the\ntop-level ratings feature promotes the token ’Nielsen’ more heavily in the SAE). The SAE uses a more generic \"ratings\" /\"rankings\" feature\nthat shows signs of absorption with a \"University Rankings\" feature.\nFigure 4: A comparison decomposition of the same context /embedding in the H-SAE architecture and\nstandard SAE.\n9\n--- Page 10 ---\n8 16 320.20.30.40.50.6\nModel Size (Thousands of Top-Level Latents)Mean Absorption Fraction ScoreStandard SAE\nH-SAE - 16 sublatents\nH-SAE - 64 sublatents\n(a)The standard SAE shows higher absorption, indicating greater feature splitting,\nwhile our H-SAE architecture maintains more coherent representations as measured\nby the mean fraction of first-letter classifications that show signs of absorption.Language Pair H-SAE SAE\nEnglish vs French 8.448 9.772\nEnglish vs Spanish 8.190 9.598\nEnglish vs German 9.372 10.938\nFrench vs Spanish 5.748 7.170\nFrench vs German 7.306 9.038\nSpanish vs German 7.476 9.270\n(b)The H-SAE architecture has less divergence in features for the same sentence in\ndifferent languages. Higher values indicate more redundant features, as mea-\nsured by the mean set difference between the same token in different languages,\ndemonstrating our H-SAE architecture’s superior ability to learn highly composable\nfeatures.\nFigure 5: H-SAEs exhibit less feature absorption and fewer redundant features across languages.\nHierarchical Semantics Finally, we turn to the question of whether the H-SAE is indeed\nlearning hierarchical semantics. As a small-scale test, we start by running the H-SAE on\nthe unembeddings of the Gemma 2-2B model. Here we can easily select representative\nexamples for visualization and assess the quality of the learned hierarchy. Figure 6 shows\nthis clearly. Of particular note are the weights on the low-level topic. When the low-level\ntopic isn’t strongly relevant, the low-level features are not strongly activated. On the other\nhand when the low-level topic is relevant, the low-level features are strongly activated. On\nthe embeddings, we observe similar behavior. For clarity, we only visualize examples where\nlow-level features are strongly activated. Figures 1, 4 and 7 show examples from the 16k\nexperts x 16 sublatents H-SAE. We observe that hierarchical semantics are clearly emerging.\nSee the supplementary materials for an interactive notebook including hundreds of such\nvisualizations.\n5 Discussion\nThe main question in this work is whether the interpretability-reconstruction frontier can be\nimproved by exploiting the hierarchical structure of semantics. We find that the answer is\nyes. Incorporating hierarchy dramatically improves reconstruction and moderately improves\ninterpretability of the top-level features. Further, the two-level structure effectively com-\nmunicates semantic relationships between concepts—e.g., we observed high-level latents\ncapturing regional concepts like “Bay Area,” with corresponding expert-specific sub-latents\nrepresenting entities like “Stanford” that exist within that region. As a further benefit, the\nhierarchical structure allows for large increases in the computational efficiency relative to\nthe effective number of atoms.\nLimitations and Further Work In this work we primarily analyze the architectural changes\nin the context of a simple ‘standard’ SAE approach. Incorporating hierarchy leads to an\nimprovement, but the results are still imperfect—e.g., we still find some level hard-to-\ninterpret features, and we still far short of perfect reconstruction. It is unclear whether this\nreflects a fundamental limitation of dictionary learning, or whether there is some additional\nset of changes that would lead to dramatically improved performance. For example, our\n10\n--- Page 11 ---\nFigure 6: An H-SAE model learns hierarchical semantics on the unembeddings. Most of the time\nthe low-level feature isn’t highly activated. However, for ‘python’ Features 547 & 1867 and ‘Chicago’\nFeature 175 the low-level feature is highly activated and relevant (Feature 175 is US states and the\nsublatent is Midwestern states).\nablation studies on word unembeddings suggest that using a reconstruction objective\nother than Euclidean distance can massively improve interpretability; see Appendix B.3.\nSimilarly, work in causal representation learning has shown simple sparsity objectives have\nfundamental limitations [Loc+19], and developed a variety of more sophisticated objectives\nleading to much better performance [Loc+20; AHB22b; Lip +22]. It would be exciting to\nfind ways to incorporate such insights into LLM interpretability.\nAcknowledgements\nThis work is supported by ONR grant N00014-23-1-2591 and Open Philanthropy.\nReferences\n[AHB22a ]K. Ahuja, J. Hartford, and Y. Bengio. Weakly supervised representation learning\nwith sparse perturbations . 2022. arXiv: 2206.01101 [cs.LG] (cit. on p. 3).\n[AHB22b ]K. Ahuja, J. S. Hartford, and Y. Bengio. “Weakly Supervised Representation\nLearning with Sparse Perturbations”. en. Advances in Neural Information Pro-\ncessing Systems (2022) (cit. on p. 11).\n[Bra+18]J. Bradbury, R. Frostig, P . Hawkins, M. J. Johnson, C. Leary, D. Maclaurin,\nG. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. Jax:\ncomposable transformations of python +numpy programs . Computer software.\nVersion 0.3.13. 2018 (cit. on p. 15).\n[Bre+22]J. Brehmer, P . de Haan, P . Lippe, and T . Cohen. Weakly supervised causal repre-\nsentation learning . 2022. arXiv: 2203.16437 [stat.ML] (cit. on p. 3).\n[Bri+23] T . Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T . Conerly, N. Turner,\nC. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T .\nMaxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean, J. E.\nBurke, T . Hume, S. Carter, T . Henighan, and C. Olah. “Towards monoseman-\nticity: decomposing language models with dictionary learning”. Transformer\nCircuits Thread (2023). https: //transformer-circuits.pub /2023/monosemantic-\nfeatures /index.html (cit. on pp. 1, 3).\n11\n--- Page 12 ---\nExpert #3565 \nSublatent 7 Sublatent 8 Sublatent 12 \n(a)The 16k x 16 H-SAE has a \"airports\" high-level feature and \"US airport\", \"the token ’airport’\", and \"airport size\" sublatents.\nExpert #4073 \nSublatent 1 Sublatent 4 Sublatent 11 \n(b)The 8K x 16 sublatent H-SAE has a ‘question-word’ high level feature with “What\", “Where\" and “Who\" sublatents.\nFigure 7: Two more example H-SAE features\n12\n--- Page 13 ---\n[Bri+25] J. Brinkmann, C. Wendler, C. Bartelt, and A. Mueller. Large Language Mod-\nels Share Representations of Latent Grammatical Concepts Across Typologically\nDiverse Languages . arXiv:2501.06346 [cs]. 2025 (cit. on p. 8).\n[BLN24 ] B. Bussmann, P . Leask, and N. Nanda. Batchtopk sparse autoencoders . 2024.\narXiv: 2412.06410 [cs.LG] (cit. on p. 3).\n[Bus+25]B. Bussmann, N. Nabeshima, A. Karvonen, and N. Nanda. Learning multi-level\nfeatures with matryoshka sparse autoencoders . 2025. arXiv: 2503.17547 [cs.LG]\n(cit. on p. 3).\n[Cha+24]D. Chanin, J. Wilken-Smith, T . Dulka, H. Bhatnagar, and J. Bloom. A is for\nabsorption: studying feature splitting and absorption in sparse autoencoders .\n2024. arXiv: 2409.14507 [cs.LG] (cit. on p. 1).\n[Cun+23]H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse au-\ntoencoders find highly interpretable features in language models . 2023. arXiv:\n2309.08600 [cs.LG] (cit. on pp. 1, 3).\n[Den+25]B. Deng, Y. Wan, Y. Zhang, B. Yang, and F . Feng. Unveiling Language-Specific\nFeatures in Large Language Models via Sparse Autoencoders . arXiv:2505.05111\n[cs]. 2025 (cit. on p. 8).\n[DCN24 ] J. Dunefsky, P . Chlenski, and N. Nanda. Transcoders find interpretable llm feature\ncircuits . 2024. arXiv: 2406.11944 [cs.LG] (cit. on p. 3).\n[Eng+25]J. Engels, E. J. Michaud, I. Liao, W . Gurnee, and M. Tegmark. Not all language\nmodel features are one-dimensionally linear . 2025. arXiv: 2405.14860 [cs.LG]\n(cit. on p. 3).\n[Gao+24]L. Gao, T . D. la Tour, H. Tillman, G. Goh, R. Troll, A. Radford, I. Sutskever, J.\nLeike, and J. Wu. “Scaling and evaluating sparse autoencoders”. arXiv preprint\narXiv:2406.04093 (2024) (cit. on pp. 1–3, 7, 8).\n[Hea+25]T . Heap, T . Lawson, L. Farnik, and L. Aitchison. Sparse Autoencoders Can Inter-\npret Randomly Initialized Transformers . arXiv:2501.17727 [cs]. 2025 (cit. on\np. 8).\n[Hin+25]S. S. R. Hindupur, E. S. Lubana, T . Fel, and D. Ba. Projecting assumptions:\nthe duality between sparse autoencoders and concept geometry . 2025. arXiv:\n2503.01822 [cs.LG] (cit. on p. 3).\n[Joh24 ] D. D. Johnson. Penzai +treescope: a toolkit for interpreting, visualizing, and\nediting models as data . 2024 (cit. on p. 15).\n[Jos+25] S. Joshi, A. Dittadi, S. Lachapelle, and D. Sridhar. Identifiable steering via sparse\nautoencoding of multi-concept shifts . 2025. arXiv: 2502.12179 [cs.LG] (cit. on\np. 3).\n[Kar+25]A. Karvonen, C. Rager, J. Lin, C. Tigges, J. Bloom, D. Chanin, Y. -T . Lau, E.\nFarrell, C. McDougall, K. Ayonrinde, M. Wearden, A. Conmy, S. Marks, and\nN. Nanda. Saebench: a comprehensive benchmark for sparse autoencoders in\nlanguage model interpretability . 2025. arXiv: 2503.09532 [cs.LG] (cit. on p. 8).\n[KG21 ] P . Kidger and C. Garcia. Equinox: neural networks in jax via callable pytrees and\nfiltered transformations . 2021. arXiv: 2111.00254 [cs.LG] (cit. on p. 15).\n[LR25] E. Li and J. Ren. “UNLOCKING HIERARCHICAL CONCEPT DISCOVERY IN\nLANGUAGE MODELS THROUGH GEOMETRIC REGULARIZATION”. In: ICLR\n2025 Workshop on Building Trust in Language Models and Applications . 2025\n(cit. on p. 3).\n[Lie+24] T . Lieberum, S. Rajamanoharan, A. Conmy, L. Smith, N. Sonnerat, V . Varma,\nJ. Kramár, A. Dragan, R. Shah, and N. Nanda. Gemma scope: open sparse\nautoencoders everywhere all at once on gemma 2 . 2024. arXiv: 2408.05147\n[cs.LG] (cit. on pp. 1, 3, 7, 8).\n[Lin+25] J. Lindsey, W . Gurnee, E. Ameisen, B. Chen, A. Pearce, N. L. Turner, C. Citro, D.\nAbrahams, S. Carter, B. Hosmer, J. Marcus, M. Sklar, A. Templeton, T . Bricken,\nC. McDougall, H. Cunningham, T . Henighan, A. Jermyn, A. Jones, and J. Batson.\nOn the biology of a large language model . 2025 (cit. on pp. 1, 3).\n13\n--- Page 14 ---\n[Lip+23] P . Lippe, S. Magliacane, S. Löwe, Y. M. Asano, T . Cohen, and E. Gavves. Biscuit:\ncausal representation learning from binary interactions . 2023. arXiv: 2306.09643\n[cs.LG] (cit. on p. 3).\n[Lip+22] P . Lippe, S. Magliacane, S. Löwe, Y. M. Asano, T . Cohen, and S. Gavves. “CITRIS:\nCausal Identifiability from Temporal Intervened Sequences”. en. In: Proceedings\nof the 39th International Conference on Machine Learning . ISSN: 2640-3498.\n2022 (cit. on p. 11).\n[Loc+19]F . Locatello, S. Bauer, M. Lucic, G. Rätsch, S. Gelly, B. Schölkopf, and O. Bachem.\nChallenging common assumptions in the unsupervised learning of disentangled\nrepresentations . 2019. arXiv: 1811.12359 [cs.LG] (cit. on pp. 3, 11).\n[Loc+20]F . Locatello, B. Poole, G. Rätsch, B. Schölkopf, O. Bachem, and M. Tschannen.\nWeakly-supervised disentanglement without compromises . 2020. arXiv: 2002.\n02886 [cs.LG] (cit. on pp. 3, 11).\n[MSL22 ] J. Maitin-Shepard and L. Leavitt. Tensorstore for high-performance, scalable\narray storage . Computer software. 2022 (cit. on p. 15).\n[Mar+25]S. Marks, C. Rager, E. J. Michaud, Y. Belinkov, D. Bau, and A. Mueller. Sparse\nfeature circuits: discovering and editing interpretable causal graphs in language\nmodels . 2025. arXiv: 2403.19647 [cs.LG] (cit. on p. 3).\n[MA25 ] G. E. Moran and B. Aragam. Towards interpretable deep generative models via\ncausal representation learning . 2025. arXiv: 2504.11609 [stat.ML] (cit. on\np. 3).\n[Mud+24]A. Mudide, J. Engels, E. J. Michaud, M. Tegmark, and C. S. de Witt. Efficient\ndictionary learning with switch sparse autoencoders . 2024. arXiv: 2410.08201\n[cs.LG] (cit. on p. 3).\n[O’B+24]K. O’Brien, D. Majercak, X. Fernandes, R. Edgar, J. Chen, H. Nori, D. Carignan,\nE. Horvitz, and F . Poursabzi-Sangde. Steering language model refusal with sparse\nautoencoders . 2024. arXiv: 2411.11296 [cs.LG] (cit. on p. 3).\n[Par+24]K. Park, Y. J. Choe, Y. Jiang, and V . Veitch. “The geometry of categorical and hi-\nerarchical concepts in large language models”. arXiv preprint arXiv:2406.01506\n(2024) (cit. on pp. 2, 4).\n[PCV24 ] K. Park, Y. J. Choe, and V . Veitch. “The linear representation hypothesis and the\ngeometry of large language models”. In: Forty-first International Conference on\nMachine Learning . 2024 (cit. on pp. 6, 16).\n[Raj+24a]S. Rajamanoharan, A. Conmy, L. Smith, T . Lieberum, V . Varma, J. Kramár, R.\nShah, and N. Nanda. Improving dictionary learning with gated sparse autoen-\ncoders . 2024. arXiv: 2404.16014 [cs.LG] (cit. on p. 3).\n[Raj+24b]S. Rajamanoharan, T . Lieberum, N. Sonnerat, A. Conmy, V . Varma, J. Kramár,\nand N. Nanda. Jumping ahead: improving reconstruction fidelity with jumprelu\nsparse autoencoders . 2024. arXiv: 2407.14435 [cs.LG] (cit. on p. 3).\n[Raj+24c]G. Rajendran, S. Buchholz, B. Aragam, B. Schölkopf, and P . Ravikumar. Learning\ninterpretable concepts: unifying causal representation learning and foundation\nmodels . 2024. arXiv: 2402.09236 [cs.LG] (cit. on p. 3).\n[Sch+21]B. Schölkopf, F . Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and\nY. Bengio. Towards causal representation learning . 2021. arXiv: 2102.11107\n[cs.LG] (cit. on p. 3).\n[SBb22 ] L. Sharkey, D. Braun, and beren. [interim research report ]taking features out\nof superposition with sparse autoencoders . 2022 (cit. on p. 3).\n[Tem+24]A. Templeton, T . Conerly, J. Marcus, J. Lindsey, T . Bricken, B. Chen, A. Pearce,\nC. Citro, E. Ameisen, A. Jones, H. Cunningham, N. L. Turner, C. McDougall,\nM. MacDiarmid, C. D. Freeman, T . R. Sumers, E. Rees, J. Batson, A. Jermyn,\nS. Carter, C. Olah, and T . Henighan. “Scaling monosemanticity: extracting\ninterpretable features from claude 3 sonnet”. Transformer Circuits Thread\n(2024) (cit. on pp. 1, 3).\n14\n--- Page 15 ---\nA Training Details\nA.1 Input Data Construction\n•The 20231101 Wikipedia dump is used to construct the data. Approximately 500\nmillion English tokens are selected by taking the first 256 tokens among articles on\nEnglish and Simple English Wikipedia. The articles are selected by choosing the top\n2 million longest articles (using length as a proxy for quality to filter out low quality\nstub articles) and then randomly among those articles until 500 million tokens have\nbeen selected.\n•For non-English tokens, articles are chosen randomly from the largest 128 non-English\nWikipedias (as measured by active users). The mix of languages is also proportional\nto the number of active users. This is used as a rough proxy for Wikipedia quality.\n•Tokens are then packed into sequences to fill the Gemma 2-2B context window and\nactivations are collected using Penzai and stored on disk with Tensorstore [Joh24;\nMSL22 ].\n•BOS and EOS tokens are stripped from the data and shuffled on disk prior to training.\nSAE training requires data access well in excess of 1GB /s to saturate a 8xA100 node\nand so the data must be shuffled ahead of time.\nA.2 Model Implementation and Hyperparameters\n•The hierarchical sparse autoencoder is implemented in JAX and Equinox [Bra+18;\nKG21 ].\n•We train with a batch size of 32,512 and top-k of 32. When the number of sublatents\nper expert is 16, the subspace dimension is 4. Otherwise, it is 8.\n•We set the orthogonality penalty and top-level reconstruction coefficients to 0.1. The\nl1 coefficient is 0.001. For standard SAEs that require an auxiliary loss to prevent\ndead latents we use a coefficient of 1 /30.\n• We use the adam optimizer with global norm clipping of 0.75 and b1 of 0.9.\n•The learning rate is 5·10−4, initialized to 10−11with a 1,000 steps linear warmup\nand cosine decay over the remainder of the training run.\n• Additionally, the regularizers also warmup from 0 over the first 1,000 steps.\nA.3 Training Dynamics\n•We track an exponential moving average over 300 batches (i.e. roughly every million\ntokens) of the number of times latents activate. Both the auxillary loss and orthog-\nonality loss push the number of latents that don’t activate in 1 million tokens (i.e.\ndead) well under 1%. The dead atom auxillary loss coincidentally has exactly the\nsame compute cost as the addition of hierarchy with the hyperparameters used for\nexperiments. However, in terms of wall-time the H-SAE actually trains slightly faster\ndue to the auxillary loss being a much more memory-intensive operation using a naive\nimplementation. We do not attempt to use maximally efficient implementations so\ndetailed compute analysis is not conducted.\n15\n--- Page 16 ---\nB Ablations\nB.1 Pre-Processing\nWe conduct ablations by evaluating our H-SAE architecture applied to the token unembed-\nding matrix of the Gemma 2-2B model, rather than internal activations, as the unembedding\nmatrix provides a small data set that is more suitable for ablation studies. Gemma 2-2B has\n256,128 tokens in its vocabulary, however many of these are relatively rare and meaningless\ntokens. After discarding these, we are left with 186,032 tokens.\nPrevious work [PCV24 ]has shown that the semantic structure of the unembedding matrix\nis fundamentally non-Euclidean, meaning that the standard Euclidean inner product does\nnot align orthogonality with semantic independence. Following this line of work, we whiten\nthe unembedding matrix by multiplying it by the inverse square root of the covariance\nmatrix, which has been shown to align the inner product with the underlying geometry\nof the data. Empirically, we observe that this whitening step is necessary for the model to\nlearn meaningful features.\nB.2 Ablation Setup\nFor the ablations, we apply our H-SAE architecture to this whitened matrix, using a top-level\ndictionary with 2048 latent vectors (experts) and 32 sublatents per expert, k=5. We\nconsidered an ℓ1strength of 2.5×10−3and an orthogonality penalty of 2.5×10−2. As our\nfocus was on ablations, we were not concerned with the absolute performance of the model,\nbut rather with the relative performance of different configurations, so we did not tune\nhyperparameters beyond those being compared. We trained for 5000 steps with a batch\nsize of 8192 and a cosine-decay learning rate schedule starting at 8192×10−15and peak\nvalue of 8192×10−6.\nWe are interested in whether the orthogonality loss and ℓ1regularizer are necessary for\ninterpretability and hierarchy. Because this is fundamentally a qualitative question, we do not\nreport quantitative results. Instead, we qualitatively analyze the learned features and their\nhierarchical structure by examining the top activated features for a set of candidate tokens\n(e.g., “puppy”). We analyze these features in the same way as we do for the embeddings,\nexamining the max-activating examples for each feature.\nWe test the orthogonality loss with and without the ℓ1regularizer, and find that the orthog-\nonality loss does not seem to be necessary for interpretability and hierarchy. Nonetheless,\nwe chose to keep it in our final runs on the embeddings as it helps reduce the number of\ndead latents, which is a practical concern.\nWe also test the ℓ1regularizer with and without the orthogonality loss, and find that it\ntoo does not seem to be necessary for interpretability and hierarchy. Ultimately, we chose\nto keep it for our final runs on the embeddings to allow for some adaptivity beyond the\nfixed top-k selection, as we have observed in practice that very low activations are often\nnot meaningful, so we would like to encourage the model to use only the most informative\nfeatures.\nDisplayed below in Figures 8, 9, 10, 11, 12, 13, and 14 are the example results of our\nablations studies on the unembedding matrix. These were not chosen randomly but rather\nto show a variety of different types of features.\nB.3 Importance of The Causal Inner Product\nAs briefly mentioned in our pre-processing section, there are theoretical reasons to believe\nthat the Euclidean inner product does not align with the underlying geometry of the unem-\nbedding space. Our results below (in Figures 15, 16, 17, 18, 19, 20, and 21) validate this\n16\n--- Page 17 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 8: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\nhypothesis, as we find that the unwhitened unembedding matrix does not yield meaningful\nfeatures. The features are completely different from the baseline, and do not seem to have\nany coherent meaning, as illustrated below.\n17\n--- Page 18 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 9: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n18\n--- Page 19 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 10: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n19\n--- Page 20 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 11: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n20\n--- Page 21 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 12: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n21\n--- Page 22 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 13: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n22\n--- Page 23 ---\n(a)Baseline\n (b)No Orthogonality\n(c)No L1\n (d)No Orthogonality or L1\nFigure 14: Ablation studies on the unembedding matrix show no obvious advantage from the orthogo-\nnality orℓ1regularizers, though we chose to keep them for our final runs on the embeddings.\n(a)Baseline\n (b)No Whitening\nFigure 15: The causal inner product is necessary for the model to learn meaningful features.\n23\n--- Page 24 ---\n(a)Baseline\n (b)No Whitening\nFigure 16: The causal inner product is necessary for the model to learn meaningful features.\n(a)Baseline\n (b)No Whitening\nFigure 17: The causal inner product is necessary for the model to learn meaningful features.\n24\n--- Page 25 ---\n(a)Baseline\n (b)No Whitening\nFigure 18: The causal inner product is necessary for the model to learn meaningful features.\n(a)Baseline\n (b)No Whitening\nFigure 19: The causal inner product is necessary for the model to learn meaningful features.\n25\n--- Page 26 ---\n(a)Baseline\n (b)No Whitening\nFigure 20: The causal inner product is necessary for the model to learn meaningful features.\n(a)Baseline\n (b)No Whitening\nFigure 21: The causal inner product is necessary for the model to learn meaningful features.\n26",
  "text_length": 49082
}