{
  "id": "http://arxiv.org/abs/2506.01133v1",
  "title": "From Words to Waves: Analyzing Concept Formation in Speech and\n  Text-Based Foundation Models",
  "summary": "The emergence of large language models (LLMs) has demonstrated that systems\ntrained solely on text can acquire extensive world knowledge, develop reasoning\ncapabilities, and internalize abstract semantic concepts--showcasing properties\nthat can be associated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other modalities, such\nas speech? Furthermore, when models are trained jointly on multiple modalities:\nDo they develop a richer, more structured semantic understanding? To explore\nthis, we analyze the conceptual structures learned by speech and textual models\nboth individually and jointly. We employ Latent Concept Analysis, an\nunsupervised method for uncovering and interpreting latent representations in\nneural networks, to examine how semantic abstractions form across modalities.\nFor reproducibility we made scripts and other resources available to the\ncommunity.",
  "authors": [
    "Asım Ersoy",
    "Basel Mousi",
    "Shammur Chowdhury",
    "Firoj Alam",
    "Fahim Dalvi",
    "Nadir Durrani"
  ],
  "published": "2025-06-01T19:33:21Z",
  "updated": "2025-06-01T19:33:21Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01133v1",
  "full_text": "--- Page 1 ---\nFrom Words to Waves: Analyzing Concept Formation in Speech and\nText-Based Foundation Models\nAsım Ersoy, Basel Mousi, Shammur Chowdhury, Firoj Alam, Fahim Dalvi, Nadir Durrani\nQatar Computing Research Institute, HBKU, Qatar,\nshchowdhury@hbku.edu.qa, ndurrani@hbku.edu.qa\nAbstract\nThe emergence of large language models has demonstrated\nthat systems trained solely on text can acquire extensive world\nknowledge, develop reasoning capabilities, and internalize ab-\nstract semantic concepts–showcasing properties that can be as-\nsociated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other\nmodalities, such as speech? Furthermore, when models are\ntrained jointly on multiple modalities: Do they develop a richer,\nmore structured semantic understanding? To explore this, we\nanalyze the conceptual structures learned by speech and textual\nmodels both individually and jointly. We employ Latent Con-\ncept Analysis, an unsupervised method for uncovering and in-\nterpreting latent representations in neural networks, to examine\nhow semantic abstractions form across modalities. To support\nreproducibility, we have released our code1along with a curated\naudio version of the SST-2 dataset2for public access.\nIndex Terms : Multimodal Learning, Interpretability, Concep-\ntual Abstractions\n1. Introduction\nRecent advances in artificial intelligence have led to the devel-\nopment of large neural models capable of processing and gen-\nerating language, vision, and speech [1, 2, 3, 4, 5]. Among\nthese, large language models (LLMs) have demonstrated emer-\ngent capabilities once thought to require human intelligence.\nFrom commonsense reasoning to medical diagnosis and legal\nanalysis, these models have continuously pushed the boundaries\nof AI-driven understanding and decision-making [1, 6]. Their\nability to internalize abstract concepts, perform multi-step rea-\nsoning, and apply knowledge in novel contexts has fueled dis-\ncussions on their potential trajectory toward artificial general\nintelligence (AGI).\nHowever, a fundamental question remains: Are these emer-\ngent capabilities unique to text-based models, or do similar\nproperties arise in models trained on other modalities, such\nas speech? Furthermore, when models are trained on multiple\nmodalities, do they converge toward a shared semantic space\nthat facilitates conceptual abstraction , as proposed by the Se-\nmantic Hub Hypothesis [7]? To explore these questions, we\nanalyze the conceptual structures learned by speech models and\ncompare them to text-based models and jointly trained multi-\nmodal systems. We employ Latent Concept Analysis (LCA)\n[8], to uncover and compare the abstract concepts formed within\nthese models. We align the discovered concepts with predefined\n1https://github.com/shammur/MultimodalXplain\n2https://huggingface.co/collections/QCRI/multimodalxplain-\n6839bbe6fc98a0b221dc42bbtaxonomies, enabling a structured comparison of semantic rep-\nresentations across modalities.\nIn our study, we investigate unimodal models (HuBERT for\nspeech and BERT for text) and multimodal models (Seamless\nM4T and SpeechT5) to assess their ability to capture linguis-\ntic knowledge. We evaluate these models using core linguistic\ntasks like part-of-speech tagging, chunking, and semantic anal-\nysis, as well as sentiment analysis, to compare their ability to\nalign with human-defined concepts and task-specific represen-\ntations. Our study addresses the following research questions:\n•Question: How do conceptual structures in speech models\ndiffer from those in text and multimodal models?\nFinding: The alignment patterns reveal that text models di-\nrectly encode linguistic taxonomies from early layers, while\nspeech models gradually transition from acoustic to linguis-\ntic representations. Multimodal models like SpeechT5 show\nunique alignment due to cross-modal training.\n•Question: To what extent do different modalities yield\nshared or modality-specific semantic representations?\nFinding: Speech models allocate less capacity to linguistic\nand semantic taxonomies, focusing more on speech-specific\nfeatures like phonetics. In contrast, text models, which oper-\nate on tokenized inputs, develop more structured and deep\nlinguistic representations. This highlights the disparity in\nhow each modality internalizes semantic structures.\n2. Methodology\nOur methodology is designed to uncover and compare the latent\nconceptual structures emerging within speech, text, and multi-\nmodal foundation models. To achieve this, we employ Latent\nConcept Analysis (LCA) [8], an unsupervised approach that\nenables the discovery and interpretation of abstract represen-\ntations learned by neural networks. Our approach consists of\ntwo stages: concept discovery andconcept alignment .\n2.1. Concept Discovery\nContextualized representations learned in foundation models\ncapture latent conceptual structures that can be interpreted\nthrough clustering methods. Our investigation expands upon\nthe work done in discovering latent ontologies in contextualized\nrepresentations [9]. We extract contextualized representations\nfrom unimodal models and multimodal models, each with L\nlayers ( l1, l2, . . . , l L), and cluster them to obtain encoded con-\ncepts. In this context, a concept refers to a set of linguistic units\nsuch as words, phonemes, or acoustic patterns, grouped based\non lexical, semantic, syntactic, morphological, or phonetic rela-\ntionships. Figure 1 showcases concepts within the latent space\nof the different models, wherein word representations are ar-arXiv:2506.01133v1  [cs.CL]  1 Jun 2025\n--- Page 2 ---\n(a)Nationalities and Ethnicities\n (b)Names Starting with /dZ/\n (c)NEs ending in the /t@n/ sound\n (d)Positive Polarity Concept\nFigure 1: Sample latent concepts from different models. Figures 1a & 1d show BERT concepts; 1b & 1c show HuBERT concepts.\nranged based on distinct linguistic and task-specific concepts.\nTextual Input. Given a textual utterance U= [w1, . . . , w N],\nwe extract contextual embeddings at layer l:UMl\nt− − →Φl=\n[ϕl\n1, . . . , ϕl\nN]where ϕl\niis the embedding of wiat layer l.\nSpeech Input. For the corresponding speech utterance X=\n[x1, x2, . . . , x T], consisting of Tframes, we extract frame-level\nrepresentations Ψlat each layer. Since our analysis focuses\non word-based concepts, we derive word-level representations3\nΨwby averaging frame embeddings within the word boundary\n[tstart, tend][10]. Word boundaries are obtained using the Mon-\ntreal Forced Aligner.4\n2.2. Concept Alignment\nEncoded concepts capture latent relationships among words\nwithin a cluster, encompassing phonetic, lexical, syntactic, se-\nmantic, and task- or modality-specific patterns. To systemati-\ncally interpret these concepts, we employ an alignment metric\nproposed by [11], which maps the discovered concepts to struc-\ntured linguistic ontologies using predefined taxonomies.\nLetCL={Cl1, Cl2, . . . , C ln}be the set of linguis-\ntic concepts (e.g., parts-of-speech tags of words), and CE=\n{Ce1, Ce2, . . . , C em}be the set of encoded concepts dis-\ncovered within neural language models. We define their θ-\nalignment as follows:\nλθ(E,L) =1\n2\u0012P\nEαθ(Ce)\n|CE|+P\nHκθ(Cl)\n|CL|\u0013\n×100\nwhere alignment αθ(Ce)and coverage κθ(Cl)are defined as\nαθ(Ce) =(\n1,if∃Cl∈ CLsuch that|Ce∩Cl|\n|Ce|≥θ\n0,otherwise\nκθ(Cl) =(\n1,if∃Ce∈ CEsuch that|Ce∩Cl|\n|Ce|≥θ\n0,otherwise\nThe alignment term measures how many discovered con-\ncepts align with the categories of the underlying taxonomy,\nwhile the coverage term assesses how many linguistic concepts\nfrom a given taxonomy appear in the discovered clusters.\n3. Experimental Setup\nModels. We investigate both unimodal and multimodal models,\nfocusing on HuBERT, BERT, Seamless M4T, and SpeechT5.\nHuBERT [12] is a self-supervised speech model that excels\nat learning speech representations through masked prediction\n3Also known as the acoustic word embeddings [10]\n4https://github.com/MontrealCorpusTools/Montreal-Forced-\nAlignerof quantized acoustic features. BERT [13] is a widely used\npre-trained text model that captures rich contextual informa-\ntion in language through bidirectional transformers. We ex-\nplore two multimodal models: Seamless M4T [14], which com-\nbines speech and text in a shared decoder, and SpeechT5 [15],\nan extension of T5 that jointly learns speech and text for tasks\nlike speech-to-text and text-to-speech. These models represent\na spectrum of approaches to analyze representations learned\nacross different modalities.\nTasks. We conducted experiments using traditional taxonomies\ndesigned to capture core linguistic concepts. These include\nword morphology, represented by part-of-speech tagging [16];\nsyntax, explored through chunking tagging [17]; and semantics,\nexamined through Parallel Meaning Bank annotations [18]. We\ntrained sequence taggers for each of these tasks and annotated\nthe corresponding training data. Each core linguistic task rep-\nresents a human-defined concept, which we align with encoded\nrepresentations to assess how linguistic knowledge is structured\nin the model’s latent space. We also used the shallow lexical\nconcept such as suffixation.\nWe also compared encoded concepts with task-specific con-\ncepts in sentiment analysis [19] using the alignment function to\nmeasure affinity. For the positive sentiment concept , we define\nCsst(+ve)as the set of words appearing exclusively in posi-\ntively labeled sentences. An encoded concept Ceis considered\naligned with Csst(+ve)if a threshold θof its words also appear\nin positive sentences, with each word represented by its contex-\ntualized embedding. The same process applies to Csst(−ve)to\nidentify negative polarity concepts.\nData. We used two datasets for concept analysis: LibriSpeech\n[20], a large-scale read-speech corpus with diverse speakers,\nand Stanford Sentiment Treebankv2 (SST2) [19], a text-only\nsentiment classification dataset. We extended SST2 into the\nspeech domain (SST2-audio) using XTTSv2 ,5a controllable\nTTS model, to generate high-quality audio. SST2-audio pre-\nserves sentiment polarity while minimizing speaker and envi-\nronmental variability, enabling analysis of semantic concepts in\nspeech models.\nConcept Discovery and Annotation. We perform a forward\npass through the models to generate contextualized feature vec-\ntors using NeuroX toolkit [21]. Subsequently, we apply K-\nmeans clustering to the feature vectors, yielding K clusters (also\nreferred to as encoded concepts) for both base and fine-tuned\nmodels. We set K = 600 and filter out representations that ap-\npear at least 10 times, following the settings prescribed by [11].\nWe consider an encoded concept to be aligned with the linguis-\ntic concept, if it has at least 90% ( θ= 0.9) match in the number\nof words.\n5https://docs.coqui.ai/en/latest/models/xtts.html\n--- Page 3 ---\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n(a)HuBERT\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n (b)Seamless – Speech\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n (c)SpeechT5 – Speech\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n(d)BERT\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n (e)Seamless – Text\n0 2 4 6 8 10 12\nLayers010203040506070-Alignment\n (f)SpeechT5 – Text\nFigure 2: Layer-wise concept alignment. Y-axis shows % of aligned concepts (using θ=0.9) per encoder layer (X-axis).\n4. Findings and Analysis\n4.1. Comparing Modalities\nIn Figure 2, we illustrate how concepts learned by text, speech,\nand multimodal models align with the linguistic taxonomies\nstudied in this paper. The alignment patterns across layers re-\nveal distinct processing strategies: speech and text models han-\ndle linguistic information differently. Specifically, speech mod-\nels do not encode word-based linguistic taxonomies in early\nlayers; instead, linguistic structures emerge in the middle lay-\ners and peak in the upper layers. This trend is consistent with\nSpeechT5, which has been jointly trained with text.\nUnlike speech models, which gradually transition from\nacoustic to linguistic representations, text models operate di-\nrectly on tokenized inputs, allowing them to encode linguis-\ntic taxonomies from the initial layers. Text models capture\nsubword-level information such as suffixation early on, while\nmorphology (POS), syntax (chunking), and semantics (SEM)\ndevelop progressively, peaking in the middle layers. This pat-\ntern suggests that text models incrementally build linguistic un-\nderstanding, with deeper layers focusing more on integrating\nsyntax and semantics.\nBoth speech and text models exhibit a falling pattern in\nalignment in the upper most layers. This decline likely reflects\nthe increasing abstraction of learned representations. In speech\nmodels, this suggests a shift from phonetic and word-level pat-\nterns to more holistic cues such as paralingusitic representa-\ntions. In text models, the decrease in alignment indicates a tran-\nsition toward contextual abstraction and task-specific reasoning,\nwhere representations become more specialized for downstream\ntasks rather than directly reflecting linguistic taxonomies.\nWhile SpeechT5-Speech follows the observed trend in\nspeech models, SpeechT5-Text exhibits a different alignmentpattern. Unlike BERT and Seamless-text, which refine linguis-\ntic representations throughout the network, SpeechT5’s shared\nencoder allocates less capacity to explicit linguistic taxonomies\nin its deeper layers. This is due to its multimodal training ob-\njective, which aligns speech and text representations for speech-\nto-text and text-to-speech tasks. Unlike BERT, optimized for\nmasked language modeling and hierarchical linguistic abstrac-\ntion, and Seamless, where text and speech do not share the latent\nspace, SpeechT5’s encoder is optimized for cross-modal con-\nsistency, leading to representations that do not follow typical\npatterns observed in other models.\nOverall, our findings suggest that speech models allocate\nless capacity to learning linguistic and semantic taxonomies\nthan text models, likely because they must also account for\nspeech-specific features like phonetics, prosody, and speaker\nvariability, which consume much of the network’s representa-\ntional capacity. As a result, linguistic structures emerge later in\nspeech models and remain less prominent throughout their lay-\ners. We speculate that this constraint limits the ability of speech\nmodels to encode higher-level conceptual abstractions as effec-\ntively as text models, which operate directly on symbolic rep-\nresentations and can dedicate more capacity to linguistic and\nsemantic processing. This distinction could explain why speech\nmodels often struggle with tasks requiring deep linguistic rea-\nsoning or structured understanding compared to their text-based\ncounterparts [22, 23].\n4.2. Task-specific Concepts\nIn the previous section, we hypothesized that the observed de-\ncrease in alignment in the final layers reflects a shift toward\ntask-specific reasoning, where the model’s representations be-\ncome more specialized for downstream tasks. To test this hy-\npothesis, we compared the BERT and HuBERT large mod-\n--- Page 4 ---\n024681012141618202224\nLayer0100200300400500600Count(a)HuBERT–SST\n024681012141618202224\nLayer0100200300400500600Count (b)BERT–SST\nFigure 3: Alignment with task-specific polarity concepts\nels trained for sentiment classification. Using SST2-text and\nour SST2-audio, we extracted activation vectors and underly-\ning concepts from the models and aligned them with the output\nclasses: positive and negative. Our results, presented in Figure\n3, indicate that polarity concepts begin to emerge in the final\nlayers of both the fine-tuned text and speech models.\nA closer analysis reveals a notable asymmetry between\ntext and speech-based models in how they encode sentiment.\nSpecifically, the speech-based SST models predominantly rely\non signals from positive polarity concepts, whereas textual\nmodels exhibit a more balanced distribution of positive and neg-\native concepts. This suggests that speech models may struggle\nto capture negative sentiment as effectively as textual models,\npotentially due to the inherent differences in how sentiment is\nconveyed in speech versus text. In text, explicit negations and\nsentiment-laden words provide clear cues for polarity, whereas\nspeech-based sentiment relies more on prosodic features such as\nintonation, pitch, among others which may be harder to disen-\ntangle in the absence of large-scale speech-specific supervision.\nThis observation aligns with task performance: HuBERT\nunderperformed in predicting negative sentiment (accuracy of\n87.48% versus BERT’s 93.21%), while both models performed\ncomparably when classifying positive sentiment (93.31% ver-\nsus 94.98%). The disparity in negative sentiment prediction\nhighlights a potential limitation of current speech-based sen-\ntiment classifiers, which may require additional fine-tuning or\nexplicit modeling of prosodic features to achieve performance\nparity with text-based models. Overall, our results reinforce the\nidea that final layer representations are shaped by task-specific\nrequirements [24, 25], but they also highlight potential gaps in\nhow different modalities encode sentiment. Addressing these\ngaps could involve integrating additional linguistic or prosodic\ncues in speech-based models, leveraging multi-modal learning\nstrategies, or exploring alternative architectures that better cap-\nture the nuances of spoken sentiment.\n4.3. Qualitative Analysis\nIn our qualitative analysis, we explored how the models learn\nand represent concepts by leveraging GPT for annotation.\nWhile we identified concepts that align with established linguis-\ntic taxonomies, it’s important to note that these models may not\nalways strictly conform to human-defined concepts. To inter-\npret and analyze these concepts more effectively, we can an-\nnotate them for further exploration. Following [26], we em-\nploy ChatGPT in a zero-shot setting, prompting the model with\nstructured instructions, reported in Listing 1.\nOur findings suggest that concepts in the models are often or-\nganized in compositional hierarchies, where the model initially\ngroups concepts based on a primary objective and then refinesthem according to semantic relations. For example, the model\nmay first group all names starting with the sound dZbefore\ndistinguishing between other linguistic or semantic categories\n(see the concept in Figure 1b). This hierarchical structure high-\nlights the model’s ability to form complex, layered associations\namong concepts, which may offer insights into how it processes\nand organizes knowledge.\nAssistant is a large language model trained by\nOpenAI.\nInstructions:\nGive a short and concise label that best\ndescribes the following list of words:\n[\"w_1\", \"w_2\", ..., \"w_N\"]\nListing 1: Prompt for label assignment.\n5. Related Work\nThe discovery and interpretation of latent concepts in deep mod-\nels remain crucial challenges in NLP, particularly in speech\nprocessing. Recent studies have focused on understanding\nthe internal representations learned by these models, with an\nemphasis on layer-wise analysis and latent concept discov-\nery [27, 28, 9, 29, 30]. These foundational works have explored\nhow hierarchical linguistic features–such as surface properties,\nsyntax, and semantics are distributed across layers. The find-\nings suggest a progression from lexical and syntactic features\nin the lower and middle layers to more abstract semantic repre-\nsentations in the higher layers.\nThere are a handful of studies on how information is en-\ncoded in speech models [31, 32, 33, 34, 35]; however, latent\nconcept discovery and the evolution of representations across\nlayers remain largely underexplored compared to text-based\nmodels. Studies such as [36, 37, 38] indicate that phonetic and\nphonemic distinctions emerge in the early layers, often over-\nshadowing semantic information in speech models [39]. Previ-\nous analyses show a strong alignment between HuBERT’s la-\ntent units and linguistic structures, particularly phonetic cate-\ngories [38], and syllabic [37]. Additionally, studies like [40]\ndemonstrate that model size and training objectives significantly\ninfluence the distribution of linguistic information. Despite\nthese insights, in-depth research on speech, multimodal, and\nencoder-decoder models remains underexplored. To address\nthese gaps, our study focuses on understanding how semantic\nconcepts emerge in deep speech models.\n6. Conclusion\nIn this study, we compared speech, text, and multimodal mod-\nels to understand how they represent linguistic concepts. Our\nfindings suggest that text models, such as BERT, directly en-\ncode linguistic structures from early layers, while speech mod-\nels, like HuBERT, gradually develop linguistic representations\nfrom acoustic features. Multimodal models like SpeechT5 ex-\nhibit unique alignment patterns due to cross-modal training.\nWe observed that speech models allocate less capacity to lin-\nguistic taxonomies, focusing more on speech-specific features\nlike phonetics. In task-specific tests, such as sentiment anal-\nysis, speech models showed weaker performance in capturing\nnegative sentiment compared to text models. Our results em-\nphasize the different ways these models process and internalize\nlanguage, with text models offering richer, more structured lin-\nguistic representations.\n--- Page 5 ---\n7. References\n[1] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\nE. Kamar, P. Lee, Y . T. Lee, Y . Li, S. Lundberg, H. Nori,\nH. Palangi, M. T. Ribeiro, and Y . Zhang, “Sparks of artificial\ngeneral intelligence: Early experiments with GPT-4,” Tech. Rep.,\n2023.\n[2] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y . Zhang, D. Narayanan, Y . Wu, A. Kumar\net al. , “Holistic evaluation of language models,” arXiv preprint\narXiv:2211.09110 , 2022.\n[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. ,\n“LLaMA: Open and efficient foundation language models,”\narXiv:2302.13971 , 2023.\n[4] Y . Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y . Leng, Y . Lv,\nJ. He, J. Lin et al. , “Qwen2-audio technical report,” arXiv preprint\narXiv:2407.10759 , 2024.\n[5] J. Zhao, Q. Yang, Y . Peng, D. Bai, S. Yao, B. Sun, X. Chen,\nS. Fu, X. Wei, L. Bo et al. , “Humanomni: A large vision-speech\nlanguage model for human-centric video understanding,” arXiv\npreprint arXiv:2501.15111 , 2025.\n[6] K. Jeblick, B. Schachtner, J. Dexl, A. Mittermeier, A. T. St ¨uber,\nJ. Topalis, T. Weber, P. Wesp, B. Sabel, J. Ricke, and M. Ingrisch,\n“Chatgpt makes medicine easy to swallow: An exploratory case\nstudy on simplified radiology reports,” 2022.\n[7] K. E. Patterson, P. J. Nestor, and T. T. Rogers, “Where do you\nknow what you know? the representation of semantic knowledge\nin the human brain,” Nature Reviews Neuroscience , 2007.\n[8] F. Dalvi, A. R. Khan, F. Alam, N. Durrani, J. Xu, and H. Sajjad,\n“Discovering latent concepts learned in BERT,” in Proc. of ICLR ,\n2022.\n[9] J. Michael, J. A. Botha, and I. Tenney, “Asking without telling:\nExploring latent ontologies in contextual representations,” in\nProc. of EMNLP , 2020.\n[10] R. Sanabria, O. Klejch, H. Tang, and S. Goldwater, “Acoustic\nword embeddings for untranscribed target languages with con-\ntinued pretraining and learned pooling,” in Proc. of Interspeech ,\n2023.\n[11] M. Hawasly, F. Dalvi, and N. Durrani, “Scaling up discovery of\nlatent concepts in deep NLP models,” in Proc. of EACL , 2024.\n[12] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhutdi-\nnov, and A. Mohamed, “Hubert: Self-supervised speech represen-\ntation learning by masked prediction of hidden units,” 2021.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in Proc. of NAACL , 2019.\n[14] S. Communication, “Seamlessm4t: Massively multilingual &\nmultimodal machine translation,” 2023.\n[15] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y . Wu, S. Liu, T. Ko,\nQ. Li, Y . Zhang, Z. Wei, Y . Qian, J. Li, and F. Wei, “Speecht5:\nUnified-modal encoder-decoder pre-training for spoken language\nprocessing,” 2022.\n[16] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building\na large annotated corpus of English: The Penn Treebank,” Com-\nputational Linguistics , 1993.\n[17] E. F. Tjong Kim Sang and S. Buchholz, “Introduction to the\nCoNLL-2000 shared task chunking,” in Proc. of CoNLL and the\nSecond Learning Language in Logic Workshop , 2000.\n[18] L. Abzianidze, J. Bjerva, K. Evang, H. Haagsma, R. van Noord,\nP. Ludmann, D.-D. Nguyen, and J. Bos, “The parallel meaning\nbank: Towards a multilingual corpus of translations annotated\nwith compositional meaning representations,” in Proc. of EACL ,\n2017.\n[19] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\nand C. Potts, “Recursive deep models for semantic composition-\nality over a sentiment treebank,” in Proceedings of the 2013 Con-\nference on Empirical Methods in Natural Language Processing ,\n2013.[20] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\ninProc. of ICASSP , 2015.\n[21] F. Dalvi, N. Durrani, and H. Sajjad, “Neurox library for neuron\nanalysis of deep nlp models,” in Proc. of ACL , 2023.\n[22] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\n“GLUE: A multi-task benchmark and analysis platform for natu-\nral language understanding,” in Proc. of Blackbox NLP , 2018.\n[23] S. Shon, A. Pasad, F. Wu, P. Brusco, Y . Artzi, K. Livescu, and\nK. J. Han, “Slue: New benchmark tasks for spoken language un-\nderstanding evaluation on natural speech,” 2022.\n[24] A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney, “What\nhappens to BERT embeddings during fine-tuning?” in Proc of\nthe Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , Nov. 2020.\n[25] N. Durrani, H. Sajjad, and F. Dalvi, “How transfer learning im-\npacts linguistic knowledge in deep NLP models?” in Findings of\nthe ACL-IJCNLP 2021 .\n[26] B. Mousi, N. Durrani, and F. Dalvi, “Can llms facilitate interpreta-\ntion of pre-trained language models?” in Proc. of EMNLP , 2023.\n[27] I. Tenney, D. Das, and E. Pavlick, “BERT rediscovers the classical\nNLP pipeline,” in Proc. of ACL , 2019.\n[28] G. Jawahar, B. Sagot, and D. Seddah, “What does BERT learn\nabout the structure of language?” in Proc. of ACL , 2019.\n[29] H. Sajjad, N. Durrani, F. Dalvi, F. Alam, A. Khan, and J. Xu,\n“Analyzing encoded concepts in transformer language models,”\ninProc. of NAACL , 2022.\n[30] N. Durrani, H. Sajjad, F. Dalvi, and F. Alam, “On the transforma-\ntion of latent space in fine-tuned nlp models,” in Proc. of EMNLP ,\n2022.\n[31] S. A. Chowdhury, N. Durrani, and A. Ali, “What do end-to-\nend speech models learn about speaker, language and channel\ninformation? a layer-wise and neuron-level analysis,” Computer\nSpeech & Language , 2024.\n[32] A. Waheed, H. Atwany, B. Raj, and R. Singh, “What do speech\nfoundation models not learn about speech?” arXiv preprint\narXiv:2410.12948 , 2024.\n[33] Y . El Kheir, A. Ali, and S. A. Chowdhury, “Speech representation\nanalysis based on inter-and intra-model similarities,” in Proc. of\nICASSP Workshop , 2024.\n[34] H. Lee, D. Liu, S. Sinhamahapatra, and J. Niehues, “How do mul-\ntimodal foundation models encode text and speech? an analysis\nof cross-lingual and cross-modal representations,” arXiv preprint\narXiv:2411.17666 , 2024.\n[35] M. de Heer Kloots and W. Zuidema, “Human-like linguistic biases\nin neural speech models: Phonetic categorization and phonotactic\nconstraints in wav2vec2. 0,” in Proc. INTERSPEECH , 2024.\n[36] K. Martin, J. Gauthier, C. Breiss, and R. Levy, “Probing self-\nsupervised speech models for phonetic and phonemic informa-\ntion: A case study in aspiration,” in Proc. Interspeech , 2023.\n[37] C. J. Cho, A. Mohamed, S.-W. Li, A. W. Black, and G. K. Anu-\nmanchipalli, “SD-HuBERT: Sentence-level self-distillation in-\nduces syllabic organization in hubert,” in Proc. of ICASSP , 2024.\n[38] D. Wells, H. Tang, and K. Richmond, “Phonetic analysis of self-\nsupervised representations of english speech,” in Proc. of Inter-\nspeech , 2022.\n[39] K. Choi, A. Pasad, T. Nakamura, S. Fukayama, K. Livescu, and\nS. Watanabe, “Self-supervised speech representations are more\nphonetic than semantic,” arXiv preprint arXiv:2406.08619 , 2024.\n[40] A. Pasad, C.-M. Chien, S. Settle, and K. Livescu, “What do self-\nsupervised speech models know about words?” TACL , 2024.",
  "text_length": 28549
}