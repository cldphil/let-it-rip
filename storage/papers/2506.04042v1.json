{
  "id": "http://arxiv.org/abs/2506.04042v1",
  "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit\n  Knowledge Editing via Both Subject and Relation Awareness",
  "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.",
  "authors": [
    "Xiyu Liu",
    "Zhengxiao Liu",
    "Naibin Gu",
    "Zheng Lin",
    "Ji Xiang",
    "Weiping Wang"
  ],
  "published": "2025-06-04T15:06:46Z",
  "updated": "2025-06-04T15:06:46Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04042v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04042v1  [cs.CL]  4 Jun 2025Unveiling and Eliminating the Shortcut Learning for\nLocate-Then-Edit Knowledge Editing via Both Subject\nand Relation Awareness\nXiyu Liu1,2, Zhengxiao Liu1,2∗, Naibin Gu1,2, Zheng Lin1,2, Ji Xiang1, Weiping Wang1\n1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\n{liuxiyu, liuzhengxiao, gunaibin, linzheng, xiangji, wangweiping}@iie.ac.cn\nAbstract\nKnowledge editing aims to alternate the target knowledge predicted by large lan-\nguage models while ensuring the least side effects on unrelated knowledge. An\neffective way to achieve knowledge editing is to identify pivotal parameters for\npredicting factual associations and modify them with an optimization process to\nupdate the predictions. However, these locate-then-edit methods are uncontrollable\nsince they tend to modify most unrelated relations connected to the subject of target\nediting. We unveil that this failure of controllable editing is due to a shortcut learn-\ning issue during the optimization process. Specifically, we discover two crucial\nfeatures that are the subject feature and the relation feature for models to learn\nduring optimization, but the current optimization process tends to over-learning\nthe subject feature while neglecting the relation feature. To eliminate this shortcut\nlearning of the subject feature, we propose a novel two-stage optimization process\nthat balances the learning of the subject feature and the relation feature. Experimen-\ntal results demonstrate that our approach successfully prevents knowledge editing\nfrom shortcut learning and achieves the optimal overall performance, contributing\nto controllable knowledge editing.1\n1 Introduction\nLarge language models (LLMs) [ 1,2] have garnered a significant amount of attention nowadays,\nespecially known for their proficiency in natural language understanding and generation by utilizing\nknowledge encoded in parameters [ 3,4,5]. However, some factual knowledge may become outdated\novertime and there is also the possibility that some facts are learned incorrectly or overlooked during\npre-training of LLMs. Knowledge editing methods are proposed to alter the prediction of a small\namount of factual associations permanently while having the least impact on irrelevant facts.\nA prominent line of previous works achieve knowledge editing via localizing the decisive param-\neters during the recall of factual associations of auto-regressive transformer language models and\nthen modify the located parameters with regularization constrained updates, known as the locate-\nthen-edit methods [ 6,7,8,9]. However, these methods are uncontrollable, frequently resulting\nin unexpected alterations to most unrelated factual associations linked to the subject of the edited\nfactual association. [ 10,11]. Such uncontrollable knowledge editing process makes the edited model\nuntrustworthy.\nIn this work, we unveil that this failure of locate-then-edit methods stems from a shortcut learn-\ning [12] issue during the optimization process of editing, as exemplified by the investigation on the\n∗Zhengxiao Liu is the corresponding author.\n1Our code is released in https://github.com/sunshower-liu/TOP.\nPreprint. Under review.\n--- Page 2 ---\nUnrelated Factual Association Prompts\nUnrelated Subjects Unrelated Relations\nNeymar is a citizen of Lionel Messi is married to\nJoe Biden is a citizen of Lionel Messi plays for\n… …݄௦\n݄௥is\na\ncitizen\nofLionel\nMessis\nrMHSA\nMLP\nEdited\nMLP\n෩ܹ௦௟݇௦כݒ௦כ׷ܛܛܗܔܖܗܑܜ܉ܢܑܕܑܜܘܗ \nെ܏ܗܔ\n܏۾כ࢙࢜כ࢕(1) Over-learning the subject feature\n(2) Neglecting the relation featureEditing Target ( s, r, o)Æ(s, r, כ࢕)\nכ࢕ǣGermanyROME\nEditingROME-Edited Model\nInput OutputUnrelated Subjects\nUnrelated Relations\nis a citizen ofsubject relation object\nFactual Prompt: Lionel Messi is a citizen ofNeymar is a citizen of \nBrazil\nJoe Biden is a citizen of \nthe United States\n…\nLionel Messi is married to\nEva Braun \nLionel Messi plays for\nFC Bayern München\n…Figure 1: We unveil the shortcut learning issue of current locate-then-edit (i.e. ROME-like) knowledge\nediting methods that leads to unexpected alterations of facts with unrelated relations. The shortcut\nlearning refers to the over-learning of the subject feature while neglecting the relation feature during\nthe optimization process of editing.\nrepresentative ROME editing [ 7]. Specifically, through the analysis of the significance of gradients at\ndifferent token positions during the optimization of ROME, we discover that the hidden representa-\ntions traversing the layers at the last subject token position and the last relation token position are\ntwo crucial features that models learn during the optimization process. However, further exploration\ninto the contribution of these two features to the predictions of factual associations in edited models\nshows that ROME editing exhibits a bias towards over-learning the feature at the last subject\nposition while inadequately learning the feature at the last relation position . Since subject-related\nattributes are mainly recalled at the last subject token position [ 7] and relation-related attributes are\nmainly recalled at the last relation token position [ 10] (i.e. typically the last token position), the two\nfeatures can be regarded as the subject feature and the relation feature respectively. In conclusion,\nROME editing falls into the trap of shortcut learning of the subject feature while neglecting the\nrelation feature, resulting in uncontrollable editing (Fig. 1).\nTo eliminate the shortcut learning issue, we further point out that the direct cause of shortcut learning\nis the flawed design of the loss function during optimization. The objective of the loss function in\nthe current optimization process is simply the negative log-likelihood of the subject feature when\ngiven the target object token, which neglects the relation feature. In order to balance the learning of\nboth the relation feature, we amend the objective loss function and decompose it under approximate\nconditional independence of the subject feature and the relation feature. Accordingly, we design a\nTwo-stage Optimization Process for controllable knowledge editing ( TOP ). At the first stage, the\nrelation feature is optimized to obtain the new object token prediction. At the second stage, the subject\nfeature is optimized to obtain the updated relation feature. Thus the located MLP down-projection\nsublayer can be modified via both subject and relation awareness.\nEvaluation results on the widely used knowledge editing benchmark demonstrate that this simple\nchange of optimization strategy effectively alleviates the shortcut issue for locate-then-edit knowledge\nediting and exhibits the most balanced performance. This work contributes to controllable editing\nand we hope it can inspire more practical editing approaches with less side effects.\nTo sum up, our contributions are as follows:\n•This work conducts in-depth analyses on the cause of unexpected alterations of unrelated\nfacts by unveiling the shortcut learning of the subject feature while neglecting the relation\nfeature for locate-then-edit knowledge editing.\n•We propose a simple yet effective two-stage optimization process for locate-then-edit editing\nto balance the learning of the subject feature and the relation feature.\n•Experimental results demonstrate that our optimization strategy successfully alleviates\nshortcut learning and obtains the optimal overall performance, which makes a great step\ntowards controllable editing.\n2\n--- Page 3 ---\n2 Related Works\nKnowledge editing aims to alter the prediction of language models within the specific domain related\nto the edited fact while avoiding side effects on unrelated facts [ 13,14]. One line of works achieve\nknowledge editing through parameter-preserving strategies [ 15,16,17]. They utilize additional\nmemories or extra trainable parameters for learning new facts and freeze the original parameters of\nmodels. Another line of works adopt parameter-modifying strategy for knowledge editing, including\nmeta-learning [ 18,19,20] and locate-then-edit editing. We focus on Locate-then-edit editing which\ngarners significant interest for its outstanding performance.\nLocate-then-edit Knowledge Editing. Locate-then-edit works achieve knowledge editing through\nknowledge neurons localization and the modification of located parameters via regularization con-\nstrained updates. ROME [ 7] firstly identifies the decisive neuron activations for processing the subject\ntokens of factual associations in auto-regressive large language models through causal mediation\nanalysis. They reveal a distinct set of steps in middle-layer MLP modules at the last-subject token\nposition that mediate the prediction of factual prompts. Then, by modifying the most decisive sublayer\nof the located MLP modules with rank-one model editing, ROME alters the object prediction of each\nsingle fact with impressive generalizability. Some interpretation works [ 21,22] further highlight that\nthe enrichment of subject-related attributes of each factual association takes place at the last-subject\ntoken position. Thus, ROME can be understood as the modification of how subject-related attributes\n(i.e. subject knowledge) aggregate. To enable batch editing of thousands of facts, MEMIT [ 8] adjusts\nthe regularization constraints for multiple facts and spread the update evenly over several layers.\nOther works [ 9,23,24] extend the ability of locate-then-edit methods to life-long editing, which\nenables to modify the located parameters sequentially without suffering from catastrophic forgetting.\nThese works are all extensions based on the localization results of ROME.\nHowever, RETS [ 10] uncovers the over-generalizing problem of such subject-focused editing which\nchanges every fact linked to the subject of the edited factual association inadvertently. RETS\nunveils a similar recall mechanism for relational knowledge at the last-relation token position and\naddresses over-generalizing by editing from the relation-focused perspective with subject constraints.\nNevertheless, it suffers from a drastic degradation in the quality of text generation, limited by the\nexcessively stringent regularization constraints. In this work, we are dedicated to making in-depth\nexploration on why subject-focused editing tends to suffer from over-generalizing that makes editing\nuncontrollable, and to addressing this issue accordingly.\n3 Unveil Shortcut Learning\nTo uncover the causes of over-generalizing, we take an in-depth investigation on the regularization\nrestrained update process of ROME-like approaches.\n3.1 Background and Notations\nFirstly, we give the formulization of knowledge editing. Knowledge editing aims to alter the factual\nassociation < s, r, o > predicted by the model to < s, r, o∗>, where sis the subject, ris the relation,\nando, o∗denote the original object and the new target object respectively. Auto-regressive language\nmodels are normally triggered by the factual prompt < s, r> to give the object token prediction.\nROME-like editing approaches localize the mid-early MLP sublayer lat the last-subject token position\nas the most decisive neurons for object predictions and modify the down-projection matrix Wl\nsof the\nlocated MLP to overwrite the subject-related attributes recalled by the model. This down-projection\nmatrix can be treated as a set of associative key-value memories that store the map between the input\nvector kand the output vector v[7]. Unlike fine-tuning which directly optimizes the model parameters\nto generate target predictions, ROME firstly calculates the average input vector k∗\nsof prefixed target\nfactual prompts and optimizes the output vector vsofWl\ns, resulting in v∗\ns. Then the down-projection\nmatrix is modified by injecting (k∗\ns, v∗\ns)into the associative memories and optimizing the objective\nfunction with constraints as follows to obtain the updated ˜Wl\ns:\nminimize ||˜Wl\nsK−V||2\nFsuch that ˜Wl\nsk∗\ns=v∗\ns (1)\nminimize ||Wl\nsK−V||2\nF (2)\n3\n--- Page 4 ---\nThe objective function in Eqn. 2 ensures the least change on the original key-value pairs K=\n[k1|k2|k3|...]andV= [v1|v2|v3|...], where Wl\nsis the original associative memories\nthat store the mapping from KtoV, solving as Wl\nsKKT=V KT. By solving the minimal value\nsolution in Eqn. 1, ROME obtain the solution for ˜Wl\nsas the updated weight matrix:\n˜Wl\ns=Wl\ns+ Λs(C−1k∗\ns)T(3)\nwhere C=KKTis the constant estimated with the uncentered covariance of kon a slice of\nWikipedia corpus and Λsis solved as (v∗\ns−Wl\nsk∗\ns)/(C−1k∗\ns)Tk∗\nswhich is proportional to the gap\nbetween the initial output vector Wl\nsk∗\nsand the target output vector v∗\ns.\nSince the result of this regularization constrained update has been rigorously proven in [ 7] and extra\nstringent regularization constraints on the objective of optimization can lead to a drastic decline of\ntext generation fluency [ 10], we will not discuss the deficiencies of this updating formula in this work.\nInstead, we focus on the calculation of two crucial vectors k∗\nsandv∗\nsfor performing the parameter\nupdate. Vector k∗\nsis set to the average value of the input vectors over a small amount of texts ending\nwith the subject sto ensure the robustness of edited models on prefixed factual prompts. This value\nremains constant after the factual prompt for editing is provided. Therefore, the acquisition of v∗\ns\nis the determinative cause of the over-generalizing issue for ROME-like editing approaches. It is\nobtained via the optimization process of v∗\ns=argmin vL(v)in which the objective loss function is\nshown as follows:\nL(v) =−1\nNNX\nt=1logP[o∗|v, pt] +KL[v, vo\ns] (4)\nwhere vo\nsis the original output vector of Wl\ns,pt(t∈ {1,2, .., N}) are prefixed prompts ending\nwith the edited factual association < s, r>.KL[v, vo\ns]is the KL divergence DKL(P[x|v]||P[x|vo\ns)to\nconstrain the impact on other facts.\n3.2 Gradient Saliency Analysis\nTo take an in-depth investigation of the optimization process for v∗\ns, we analyze the saliency of\ngradients in this process. Since ROME edits at the mid-early layer l, we observe the gradients of\nhidden states after layer lat all token positions during the first few epochs, formalized as follows:\nG(hla\ni) =∂L(v)\n∂hla\ni(5)\nwhere la∈ {l, l+ 1, ..., L}denotes the layer after the editing layer lof the model with Llayers and\ni∈ {0,1,2, ..., n−1}denotes the token position in the tokens list of length n. The token positions\nare further categorized into relation prefix (\"rp\"), first subject (\"fs\"), middle subject (\"ms\"), last\nsubject (\"ls\"), first relation (\"fr\"), middle relation (\"mr\") and last relation (\"lr\") according to their\npositions in the tokenized subject or relation of a factual association. For example, token positions\nof \"The mother tongue language of \" are the relation prefix in the input prompt \" The mother tongue\nlanguage of Isabelle Breitman is \" and the token position of \" is\" is the last relation position.\nto a drastic decline of text generation fluency (Liu 250\net al., 2024), we will not discuss the deficiencies 251\nof this updating formula in this work. Instead, we 252\nfocus on the calculation of two crucial vectors k∗\ns253\nandv∗\nsfor performing the parameter update. Vector 254\nk∗\nsis set to the average value of the input vectors 255\nover a small amount of texts ending with the sub- 256\njectsto ensure the robustness of edited models on 257\nprefixed factual prompts. This value remains con- 258\nstant after the factual prompt for editing is provided. 259\nTherefore, the acquisition of v∗\nsis the determinative 260\ncause of the over-generalizing issue for ROME-like 261\nediting approaches. It is obtained via the optimiza- 262\ntion process of v∗\ns=argmin vL(v)in which the 263\nobjective loss function is shown as follows: 264\nL(v) =−1\nNN/summationdisplay\nt=1logP[o∗|v, pt] +KL[v, vo\ns](4) 265\nwhere vo\nsis the original output vector of Wl\ns,pt(t∈ 266\n{1,2, .., N}) are prefixed prompts ending with the 267\nedited factual association < s, r>.KL[v, vo\ns]is the 268\nKL divergence DKL(P[x|v]||P[x|vo\ns)to constrain 269\nthe impact on other facts. 270\n3.2 Gradient Saliency Analysis 271\nTo take an in-depth investigation of the optimiza- 272\ntion process for v∗\ns, we analyze the saliency of 273\ngradients in this process. Since ROME edits at the 274\nmid-early layer l, we observe the gradients of hid- 275\nden states after layer lat all token positions during 276\nthe first few epochs, formalized as follows: 277\nG(hla\ni) =∂L(v)\n∂hla\ni(5) 278\nwhere la∈ {l, l+ 1, ..., L}denotes the layer af- 279\nter the editing layer lof the model with Llayers 280\nandi∈ {0,1,2, ..., n−1}denotes the token po- 281\nsition in the tokens list of length n. The token 282\npositions are further categorized into relation prefix 283\n(\"rp\"), first subject (\"fs\"), middle subject (\"ms\"), 284\nlast subject (\"ls\"), first relation (\"fr\"), middle rela- 285\ntion (\"mr\") and last relation (\"lr\") according to their 286\npositions in the tokenized subject or relation of a 287\nfactual association. For example, token positions 288\nof \"The mother tongue language of \" are the rela- 289\ntion prefix in the input prompt \" The mother tongue 290\nlanguage of Isabelle Breitman is \" and the token 291\nposition of \" is\" is the last relation position. 292\nWe plot the average gradient saliency map over 293\n100 edits of factual associations on GPT2-XL 294\n17222732374247rp\nfs\nms\nls\nfr\nmr\nlr\n0.00000.00250.00500.00750.01000.01250.0150(a) GPT2-XL\n510152025rp\nfs\nms\nls\nfr\nmr\nlr\n0.000.010.020.030.04 (b) GPT-J 6B\nFigure 2: The average gradient saliency map of the first\nepoch of the optimization process for v∗\ns. The x-axis\nrepresents the layer numbers and the y-axis represents\nthe token positions. Each heat value represents the\ngradient magnitude at that position.\n1.5B (Radford et al., 2019) with 48 layers, GPT-J 295\n6B (Wang, 2021) with 28 layers by ROME. Here 296\nwe display the results of the first epoch in Fig. 2, 297\nwhile the results of initial several epochs are similar, 298\nas shown in Appendix A. We omit the gradients at 299\nhL\nn−1since this is the output hidden representation 300\nand it is supposed to have the most salient gradient. 301\nFig. 2 illustrates that the gradients are most 302\nsalient across layers at the last-subject position and 303\nthe last-relation position. Since the hidden rep- 304\nresentations at the last-subject position is found 305\nout to encode the subject-related attributes via pro- 306\njection onto the vocabulary space and are signif- 307\nicantly most decisive for processing the subject 308\ntokens (Geva et al., 2023; Meng et al., 2022a), 309\nthey can be viewed as the subject feature of fac- 310\ntual prompts that models learn. The phenomena 311\nare similar for the hidden representations at the 312\nlast-relation position for relation tokens (Liu et al., 313\n2024), which can be viewed as the relation feature 314\nof factual prompts. Therefore, we conclude that 315\nhidden representations at the last subject posi- 316\ntion and the last relation position are two crucial 317\nfeatures for auto-regressive language models to 318\nlearn during optimization, namely the subject 319\nfeature and the relation feature . 320\n3.3 The Shortcut Learning Issue 321\nThe subject feature and the relation feature during 322\noptimization are revealed in the previous section. 323\nWe further observe how well these two features are 324\nlearned through ROME editing. 325\nWe conduct causal tracing (Meng et al., 2022a) 326\non models before and after editing to investigate the 327\nchange in the contribution of MLP outputs to pre- 328\ndictions during inference. Causal tracing applies 329\nperturbation on each inner activation and records 330\nthe impact on the final prediction through three 331\n4\nFigure 2: The average gradient saliency map of the 1st epoch\nof the optimization process for v∗\ns. The x-axis: layer num-\nbers; the y-axis: token positions. Each heat value represents\nthe gradient magnitude. Left: GPT2-XL. Right: GPT-J.We plot the average gradient saliency\nmap over 100 edits of factual associa-\ntions on GPT2-XL 1.5B [ 25] with 48\nlayers, GPT-J 6B [ 26] with 28 layers\nby ROME. Here we display the re-\nsults of the first epoch in Fig. 2, while\nthe results of initial several epochs are\nsimilar, as shown in Appendix A. We\nomit the gradients at hL\nn−1since this is\nthe output hidden representation and\nit is supposed to have the most salient\ngradient.\nFig. 2 illustrates that the gradients are\nmost salient across layers at the last-\nsubject position and the last-relation position. Since hidden representations at the last-subject position\n4\n--- Page 5 ---\nare found to encode the subject-related attributes through projection onto the vocabulary space and are\nsignificantly more decisive for processing the subject tokens [ 22,7], they can be viewed as the subject\nfeature of factual prompts that models learn. The phenomena are similar for hidden representations\nat the last-relation position for relation tokens [ 10], which can be viewed as the relation feature of\nfactual prompts. Therefore, we conclude that hidden representations at the last subject position\nand the last relation position are two crucial features for auto-regressive language models to\nlearn during optimization, namely the subject feature and the relation feature .\n3.3 The Shortcut Learning Issue\nThe two crucial features during optimization are revealed in the previous section. We further observe\nhow well these two features are learned through ROME editing.\nWe conduct causal tracing [ 7] on models before and after editing to investigate the change in the\ncontribution of MLP outputs to predictions during inference. Causal tracing applies perturbation on\neach inner activation and records the impact on the final prediction through three runs. In the first\nrun, the factual association prompt pis given to the model to obtain the object owhen clean internal\nactivations are cached. Here we focus on the output of MLP sublayers (e.g. mlj\nifor the MLP output\nat token position iand layer lj) since MLP is the decisive module for knowledge recall [ 21,22]. Then,\nembeddings of certain tokens in the factual prompt are devastated with noises as p′and are input into\nthe model again to obtain the corrupted probability P(o|p′)for the second run. In the last run, the\ndevastated prompt p′is still sent to the model but each inner activation is restored with the cached\nclean activation from the first run respectively, resulting in the restored probability P(o|p′, mlj\ni).\nThus the contribution of mlj\nito the correct prediction of the model with parameters θis measured by\nthe indirect effect ( IE) as follows:\nIE(o, θ, mlj\ni) =Pθ(o|p′, mlj\ni)−Pθ(o|p′) (6)\nIn this paper, we further measure the logarithmic ratio of indirect effect ( RIE) of each activation mlj\ni\nfrom the edited model θedited to the original model θoas follows:\nRIE=logIE(o∗, θedited , mlj\ni)\nIE(o, θo, mlj\ni)(7)\nwhich measures the significance of each activation before and after editing and o∗is the target new\nobject. We plot the maximum RIE of MLP outputs across all layers at each token position over 100\nfactual prompts in Fig. 3. A case of the causal tracing result before and after editing is shown in\nAppendix B.\nFig. 3 illustrates that the contribution of MLP outputs at the last-subject position increases sharply\nafter editing, significantly more than the contribution increase at the last-relation position. It indicates\nthat models have over-learned the subject feature while neglecting the relation feature during\noptimization . However, both the subject feature and the relation feature are crucial for the prediction\nof factual associations, which means ROME-like editing falls into the trap of shortcut learning.\nIn conclusion, we unveil two decisive features which models learn during ROME-like editing that are\nthe subject feature and relation feature. However, the optimization process of ROME-like editing\nresults in the over-learning of the subject feature while ignoring the relation feature, falling into the\ntrap of shortcut learning. Therefore, our goal is to address this issue via improving the acquisition of\nv∗\nsin the optimization process.\n4 Eliminate Shortcut Learning\nTo start with, we take a further analysis on the loss objective in Eqn. 4. The negative log-likelihood\nfunction −logP[o∗|v]primarily serves the purpose of alternating the token predictions while the KL\ndivergence DKL(P[x|v]||P[x|vso)aims to constrain the impact on irrelevant factual associations.\nHowever, in order to change the prediction of models with modifications to only few parameters, the\noptimization process tends to prioritize reducing the loss of the negative log-likelihood function. As a\nresult, the constraint of the KL divergence becomes negligible. Thus, to address the shortcut issue,\nwe should refine the negative log-likelihood objective for the optimization of vswhich fails to take\nthe relation feature into account.\n5\n--- Page 6 ---\n0.01.02.03.04.05.0\nrp fs ms ls fr mr lrlog ratio of indirect effect \nGPT-J 10\n-1.00.01.02.03.04.05.0\nrp fs ms ls fr mr lrlog ratio of indirect effect Figure 3: Maximum RIE of MLP outputs.\nThe x-axis: token positions. Left: GPT2-XL.\nRight: GPT-J.\nls\n࢘ࢎ(b) Second Stage:\nOptimization of ݒ௦כ\n(a) First Stage: \nOptimization of ݄௥\nlr…\n… …ݒ௦כ\nכ࢕\n࢙࢙࢕࢒૚ǣെ܏ܗܔ\n܏ ۾࢘ࢎכ࢕࢙࢙࢕࢒૛ǣȁࡲ࢜ǡ࢖ െכ࢘ࢎ ȁࡲ Figure 4: The two-stage optimization process.\n4.1 Analysis of the Likelihood Function\nAs claimed in Sec. 3.2, the hidden representations at the last relation token position hrcan be viewed\nas the relation feature for models to learn. Since only the down-projection matrix of the mid-early\nMLP sublayer for processing the last subject token will be modified, the likelihood function should\nbe changed to P[o∗, hlar|v]to take the relation feature in to account during editing. hlarin layer la\n(la> l) is an intermediate hidden vector after the editing layer lwithout any supervision signal, which\nis obtained through the following iteration of forward propagation for auto-regressive transformer\nlanguage models:\nalj+1\nr=ATTN [hlj\n0, ..., hlj\ns, ..., hlj\nr−1] (8)\nmlj+1\nr=MLP [hlj\nr+alj+1\nrImodel ] (9)\nhlj+1\nr=hlj\nr+alj+1\nr+mlj+1\nr (10)\nwhere ris equal to n−1for the tokens list of length n,ATTN is the attention module and MLP is\nthe feed-forward network module. Imodel is a constant set to 0 or 1, which depends on the structure\nof models. For GPT2-XL with the sequential structure of MLP and the attention modules, Imodel\nis set to 1, while for GPT-J with the parallel structure, Imodel is set to 0. Layer normalizations are\nomitted for simplicity.\nDue to the lack of supervision signals in utilizing the new likelihood P[o∗, hlar|v]directly as the\nobjective for the optimization process, we need to approximate and decompose this likelihood function.\nAccording to the chain rule in probability theory, this likelihood function can be decomposed as\nfollows:\nP[o∗, hla\nr|v] =P[o∗|hla\nr, v]P[hla\nr|v] (11)\nwhere vis the variable of the MLP output at the last-subject position in layer l, which is propagated\ntohl\nsand further propagated to hlaras shown in Eqn. 8-10. hlarcontains the information of vfor\nthe prediction of o∗, implying that when hlaris given, vando∗can be considered approximately\nconditional independent. The likelihood in Eqn. 11 can be simplified as follows:\nP[o∗, hla\nr|v] =P[o∗|hla\nr]P[hla\nr|v] (12)\nwhich is feasible for optimization.\n4.2 The Two-Stage Optimization Process\nThe new objective of the optimization for v∗\nsis now decomposed as P[o∗|hlar]P[hlar|v]. Consequently,\nwe design a Two-stage Optimization Process (TOP) that is capable of sufficiently learning the relation\nfeature for controllable knowledge editing, as illustrated in Fig. 4. All hlarbelow are simplified as hr\nand the new target vector is denoted as h∗\nr.\nOptimization of hr.In the first stage, we optimize the hidden representation at the last-relation\nposition in layer lato switch the prediction of models from otoo∗and obtain the updated h∗\nr=\nargmin hL1(h). The objective function is shown as follows:\nL1(h) =−1\nNNX\nt=1logP[o∗|h, pt] +KL[h, ho\nr] (13)\nwhere ho\nris the original hidden representation at the last relation position in layer la.ptandKLare\nthe same as in Eqn. 4. The negative likelihood is in accordance with the left part of P[o∗|hlar]P[hlar|v]\nand we also use the KL divergence to constrain the impact on unrelated relations.\n6\n--- Page 7 ---\nEfficacy S-Spec. R-Spec. Gen. Fluency Consist.\nGPT2-XL 1.5B 22.4( ±0.0) 77.5( ±0.0) 100.0( ±0.0) 25.0( ±0.0) 626.7(±0.0) 31.6( ±0.1)\nFT-L 99.3( ±0.0) 69.7( ±0.0) 75.0( ±0.0) 48.6( ±0.0) 621.0(±0.2) 37.4( ±0.1)\nROME 99.8( ±0.0) 75.4( ±0.0) 45.7( ±0.2) 88.5( ±0.1) 623.4(±0.0) 41.1( ±0.0)\nRETS 100.0( ±0.0) 67.9( ±0.4) 78.7( ±0.5) 71.4( ±0.6) 585.1(±5.9) 32.9( ±0.5)\nMEMIT 94.0( ±0.1) 76.4( ±0.0) 61.5( ±0.5) 79.3( ±0.2) 627.4(±0.2) 39.6( ±0.1)\nTOP 98.7(±0.1) 76.5(±0.0) 72.2(±0.7) 78.3(±0.7) 619.7(±0.3) 36.5(±0.2)\nGPT-J 6B 16.3( ±0.0) 83.0( ±0.0) 100.0( ±0.0) 18.6( ±0.1) 622.1(±0.2) 29.9( ±0.0)\nFT-L 99.6( ±0.0) 78.6( ±0.0) 89.5( ±0.0) 47.9( ±0.0) 623.0(±0.1) 35.5( ±0.1)\nROME 100.0( ±0.0) 79.3( ±0.0) 52.8( ±0.4) 97.3( ±0.1) 621.0(±0.2) 42.8( ±0.0)\nRETS 100.0( ±0.0) 61.4( ±1.9) 81.5( ±0.7) 74.9( ±1.7) 530.6(±17.1) 29.3( ±0.8)\nMEMIT 100.0( ±0.0) 81.1( ±0.0) 66.8( ±0.3) 95.2( ±0.1) 621.5(±0.1) 41.9( ±0.0)\nTOP 99.6(±0.0) 82.0(±0.0) 83.5(±0.3) 82.4(±0.4) 618.6(±0.2) 35.9(±0.2)\nQwen2.5 14B 11.0( ±0.0) 86.1( ±0.0) 100.0( ±0.0) 12.0( ±0.1) 623.7(±0.1) 27.6( ±0.0)\nFT-L 49.0( ±0.1) 85.3( ±0.0) 100.0( ±0.0) 15.0( ±0.0) 624.1(±0.0) 27.4( ±0.1)\nROME 100.0( ±0.0) 61.3( ±0.0) 56.9( ±0.3) 97.9( ±0.1) 624.0(±0.1) 39.9( ±0.0)\nRETS 92.0( ±0.0) 73.6( ±1.1) 83.2( ±0.5) 73.3( ±0.9) 591.5(±6.3) 28.5( ±0.6)\nMEMIT 100.0( ±0.0) 79.8( ±0.0) 66.7( ±0.2) 95.5( ±0.1) 625.4(±0.1) 37.5( ±0.1)\nTOP 99.5(±0.1) 83.4(±0.5) 84.5(±0.4) 74.0(±0.3) 620.5(±0.2) 31.9(±0.1)\nTable 1: The evaluation results on COUNTERFACT_RS. Std deviations are shown in brackets.\n\"Gen.\", \"S-Spec.\", \"R-Spec.\" and \"Consist.\" stand for Generalization, S-Specificity, R-Specificity\nand Consistency respectively. \"Overall\" shows the average value on four basic criteria. R-Specificity\nvalues for raw models are 100.0%since the criterion is constructed according to the top token\npredictions of the raw models. Higher scores indicate better performance for all metrics. Significantly\nlower values are shown in red.\nOptimization of vs.In the second stage, we optimize the output vector of the down-projection\nmatrix of MLP in the mid-early layer lto alter the hidden representation at the last-relation position\nin layer lafrom the original ho\nrto the optimized h∗\nrin the first stage and obtain the updated v∗\ns=\nargmin vL2(v). The loss function is the Frobenius Norm shown as follows:\nL2(v) =||F[v, p]−h∗\nr||F (14)\nwhere pis the prompt of the edited factual association < s, r> andF[v, p]is the forward propagation\nresult in hrwhen vsis set to v. All weight decays are omitted for simplicity.\n5 Experiments\n5.1 Experimental Setup\nDatasets and evaluation. Here we present the single editing experiments on 2,000samples of\nCOUNTERFACT_RS [ 10] dataset which is supplemented from COUNTERFACT [ 7]. Experimental\nresults on more datasets and editing settings can be referred to the appendix (e.g. evaluation on\nZero-shot Relation Extraction [ 27,7] dataset). The COUNTERFACT_RS dataset is a challenging\nevaluation dataset for evaluating counterfactual edits in large language models, specifically designed\nfor knowledge editing. It contains diverse subjects and relations. The four basic metrics include\nthe basic Efficacy score to measure the success rate of target editing, the Generalization score for\ngeneralization on the paraphrased and prefixed statements, the Subject Specificity S-Specificity score\nfor evaluating the impact on neighborhood subjects and the Relation Specificity R-Specificity score\nfor evaluating the impact on unrelated relations. The advanced Fluency andConsistency scores\nmeasure the quality of generated full texts. Details for all metrics are displayed in Appendix C. The\nlarge language models that we choose to perform editing on are GPT2-XL 1.5B with 48 layers, the\nsequential structure of modules in each layer; GPT-J 6B with 28 layers, the parallel structure of\nmodules in each layer; Qwen2.5 14B with 48 layers. Each evaluation is conducted five times.\n7\n--- Page 8 ---\n556065707580859095100\n6 8 10 12 14 16 18 20 22 24 26Overall Efficacy Generalization\nS-Spec R-SpecFigure 5: The performance of TOP editing with\nthe relation representation hrof different layers\n(x-axis) respectively over 200 prompts for GPT-J\n6B. The editing layer lis the 6th layer.\n9.49.69.81010.210.410.6\n012345678\n1 2 3 4 5 6 7\n2nd stage loss1st stage loss\nepoch1st stage, la=8\n1st stage, la=16\n2nd stage, la=8\n2nd stage, la=16Figure 6: Average losses in the first 7 epochs\nof the two optimization stages for la= 8 and\nla= 16 .\nBaselines. To compare with, we choose the SOTA locate-then-edit methods as our baselines,\nincluding Constrained Fine-Tuning (FT+L) [ 28] , ROME [ 7] as introduced in Sec. 3.1, MEMIT [ 8]\nexpanded from ROME which spreads the updates evenly over several layers to enable batch editing\nand RETS [ 10] which edits at the last-relation position in mid-late MLP sublayers with subject\nconstraints to alleviate over-generalizing.\nImplementation. Since we primarily improve the optimization process of ROME, most hyper-\nparameters are in accordance with ROME. Since the objective in TOP is weaker to alter the prediction\nfor GPT2-XL with 48 layers, it is necessary to edit the 18th and 19th layers successively. For GPT-J,\nwe edit 6th layer following ROME. The layer for the relation feature lais the next layer of edited\nlayers (i.e. 19th and 20th layer) for GPT2-XL and 11th layer for GPT-J. The analysis of layer selection\ncan be referred to Sec. 5.3 and details for the implementation can be referred to Appendix C.\n5.2 Main Results\nTable 1 shows the evaluation results on the knowledge editing dataset COUNTERFACT_RS for\nGPT2-XL, GPT-J and Qwen2.5. It illustrates that TOP editing exhibits the most balanced performance\nover all baselines. It has the least side effects with no deficiencies. On the contrary, other editing\nbaselines either suffer from poor specificity (e.g. low R-Specificity score for ROME and MEMIT) or\ngeneralization (e.g. low Generalization score for FT-L), or disrupts the text generation fluency (e.g.\nlow 585.1 Fluency score for RETS). TOP more adequately satisfies the requirements for controllable\nediting that prioritizes minimizing the impact on unrelated facts while remaining competitive on other\ncriteria. Based on the ensurance of high specificity of editing (i.e. S-Specificity and R-Specificity), it\nalso shows better generalization performance compared to FT-L. In conclusion, TOP successfully\naddresses the shortcut learning issue for controllable knowledge editing with the least side effects\nwhile remaining a certain degree of generalization.\n5.3 Analysis\nWe further analyze how the layer selection of hraffect the performance of our approach and delve into\nthe underlying causes of the trade-offs between specificity and generalization in the layer selection.\nMoreover, we exhibit the Maximum RIE of MLP outputs through layers at each position as introduced\nin Section 3.3 for our TOP editing to directly show that TOP indeed balances the learning of the\nsubject feature and the relation feature (i.e. eliminating the shortcut learning issue).\nLayer Analysis of hr.Since we follow the parameter localization results of ROME for vs, the\nmost influential factor on the effectiveness of our approach is the layer selection of hr. Thus we\nfurther make in-depth analyses on how layer selection of hraffects the performance and discuss the\ntrade-off issue.\nFig. 5 illustrates the overall scores and the four basic scores of TOP with different layers laofhr.\nSince lais a layer later than the editing layer l, it ranges from l+1to the output layer. As laincreases,\nthe S-Specificity score exhibits little fluctuation, indicating that the different layer +choice of the\nrelation feature has little impact on the subject feature . Though the choice of lacan affect the\nEfficacy (i.e. the success rate of target editing), it remains stable at 100% in the mid-early layers after\n8\n--- Page 9 ---\nl. As long as the selected layers are neither near to lor the output layer, the layer selection of the\nrelation feature also has minor influence on the success rate of target editing.\nHowever, the R-Specificity score and Generalization score show significant changes with the\nincrease of laand it exhibits a trade-off between R-Specificity and Generalization. R-Specificity\ninitially declines and subsequently rises as laincreases, while Generalization shows the opposite\ntrend. It suggests that when the generalization ability of editing is preferred, hlarin middle layers\nafterlcan be selected as the relation feature, and when the editing is required to be precise and\ncontrollable, the mid-early layers after lcan be selected to meet the requirement.\nTrade-off Analysis of the Layer Selection. We here provide further analysis to understand the\ncause of trade-offs in the layer selection for TOP editing. The cause underlies in Eqn. 12 which\nmotivates the two-stage optimization process. While lais closer to the output layer (i.e. the layer\nofo∗), the convergence of the likelihood loss P[o∗|hlar](i.e. the optimization at the first stage)\nis easier and faster. This leads to overfitting of the input relation tokens with the same number of\nepochs at the first stage and failing to generalize to paraphrased relations, resulting in high relation\nspecificity and low generalization. It is similar while lais closer to the editing layer (i.e. the layer\nofv), which leads to overfitting of the input relation tokens at the second stage for P[hlar|v]. Fig. 6\nshow the average losses of first 7 epochs of the two optimization stages while lais set to 8 and 16\nfor GPT-J (the editing layer is the 6th layer). The results indicate that the convergence of the first\noptimization stage is faster and the convergence of the second optimization stage is slower while lais\nlarger, which verifies the above theory.\nConsequently, as laincreases, the overfitting to the relation at the second stage relieves before middle\nlayers, yet the overfitting to the relation at the first stage exacerbates after middle layers, resulting\nin the trend of trade-offs of R-Specificity and Generalization in Fig. 5. An possible way to further\nbalance these two metrics might involve setting dynamic learning epochs for different laat each\nstage, although it might introduce heavy hyperparameter searching overhead.\nGPT2 -XL\n-2-1012345\nrp fs ms ls fr mr lrlog ratio of indirect effect \nGPT-J\n-2-1012345\nrp fs ms ls fr mr lrlog ratio of indirect effect \nFigure 7: Maximum RIE of MLP outputs for TOP. The x-\naxis: token positions. Left: GPT2-XL. Right: GPT-J.Proof of the Elimination of Short-\ncut Learning. The balance of learn-\ning the subject feature and the relation\nfeature of TOP editing can be verified\nthrough the Maximum RIE of MLP\noutputs through layers at each position\nas introduced in Section 3.3. Fig. 7 in-\ndicates that the log ratio of indirect ef-\nfect (RIE) at the last relation position\nis much closer to that at the last sub-\nject position for TOP editing (1.7/2.0\non GPT-J and 1.3/1.9 on GPT2-XL)\ncompared to ROME editing (2.2/4.6\non GPT-J and 2.5/4.4 on GPT2-XL as\nshown in Fig. 3), indicating that TOP successfully balances the learning of the subject feature and the\nrelation feature during editing.\n6 Conclusion\nThis work provides a novel understanding of the failure of locate-then-edit knowledge editing on\nthe maintenance of unrelated facts. We uncover the shortcut learning issue during the optimization\nprocess of editing, which leads to unexpected alterations of unrelated facts in edited models. To\nbe specific, we discover two pivotal features during the optimization process of editing (i.e. the\nsubject feature and the relation feature). We further observe that the subject feature is over-learned\nwhile the relation feature is neglected in the optimization process. To eliminate shortcut learning, we\namend the objective loss function of the optimization process and propose a two-stage optimization\nprocess. Experimental results demonstrate the superiority of our strategy. It can inspire future works\nfor more practical editing and also serves as a paradigmatic example for eliminating shortcut learning\nby specifying the optimization trajectory.\n9\n--- Page 10 ---\nReferences\n[1]Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain,\nand Jianfeng Gao. Large language models: A survey, 2024.\n[2]Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of\nlarge language models, 2024.\n[3]Benjamin Heinzerling and Kentaro Inui. Language models as knowledge bases: On entity representations,\nstorage capacity, and paraphrased queries. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Pro-\nceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:\nMain Volume , pages 1772–1791, Online, April 2021. Association for Computational Linguistics.\n[4]Cunxiang Wang, Pai Liu, and Yue Zhang. Can generative pre-trained language models serve as knowledge\nbases for closed-book QA? In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 3241–\n3251, Online, August 2021. Association for Computational Linguistics.\n[5]Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters\nof a language model? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426,\nOnline, November 2020. Association for Computational Linguistics.\n[6]Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu\nMao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun\nXie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. A comprehensive\nstudy of knowledge editing for large language models, 2024.\n[7]Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin GPT. Advances in Neural Information Processing Systems , 35, 2022.\n[8]Kevin Meng, Arnab Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in\na transformer. ArXiv , abs/2210.07229, 2022.\n[9] Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, and Tat seng Chua.\nAlphaedit: Null-space constrained knowledge editing for language models, 2024.\n[10] Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, and Weiping Wang. Relation also\nknows: Rethinking the recall and editing of factual associations in auto-regressive transformer language\nmodels, 2024.\n[11] Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and Huajun Chen. Unveiling the pitfalls\nof knowledge editing for large language models, 2024.\n[12] Rui Song, Yingji Li, Lida Shi, Fausto Giunchiglia, and Hao Xu. Shortcut learning in in-context learning:\nA survey, 2024.\n[13] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and\nNingyu Zhang. Editing large language models: Problems, methods, and opportunities. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 10222–10240, Singapore, December 2023. Association for Computational\nLinguistics.\n[14] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for\nlarge language models: A survey, 2024.\n[15] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and\nHuajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language\nmodels, 2024.\n[16] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based\nmodel editing at scale. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu,\nand Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume\n162 of Proceedings of Machine Learning Research , pages 15817–15831. PMLR, 17–23 Jul 2022.\n10\n--- Page 11 ---\n[17] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi.\nAging with grace: lifelong model editing with discrete key-value adaptors. In Proceedings of the 37th\nInternational Conference on Neural Information Processing Systems , NIPS ’23, Red Hook, NY , USA,\n2023. Curran Associates Inc.\n[18] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Marie-\nFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing , pages 6491–6506, Online and Punta\nCana, Dominican Republic, November 2021. Association for Computational Linguistics.\n[19] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model\nediting at scale, 2022.\n[20] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning, 2024.\n[21] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predic-\ntions by promoting concepts in the vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\neditors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages\n30–45, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[22] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations\nin auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12216–12235,\nSingapore, December 2023. Association for Computational Linguistics.\n[23] Jun-Yu Ma, Hong Wang, Hao-Xiang Xu, Zhen-Hua Ling, and Jia-Chen Gu. Perturbation-restrained\nsequential model editing, 2024.\n[24] Yuchen Cai and Ding Cao. O-edit: Orthogonal subspace editing for language model sequential editing,\n2024.\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[26] Ben Wang. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with\nJAX. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\n[27] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading\ncomprehension. In Roger Levy and Lucia Specia, editors, Proceedings of the 21st Conference on Compu-\ntational Natural Language Learning (CoNLL 2017) , pages 333–342, Vancouver, Canada, August 2017.\nAssociation for Computational Linguistics.\n[28] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. Modifying memories in transformer models, 2020.\nA Gradient Saliency\nTo illustrate that the gradient saliency at these two positions is not only significant in the first epoch but also\nacross the initial several epochs of the optimization process, we further present the gradient saliency maps for\nthe first 5 epochs, the first 10 epochs, the first 15 epochs and the first 20 epochs, as shown in Fig. 8. There are\ntotal 20 epochs for ROME editing on GPT-J.\nAll figures show the same trend with that of the first epoch, demonstrating that the hidden representations across\nlayers at the last subject token position and the last relation token position are most significant during editing.\nB Case of Indirect Effect\nHere we show a case of the indirect effect (IE) obtained by causal tracing of MLP outputs before and after\nROME editing in Fig. 9 and Fig. 10. Fig. 9 illustrates that the contribution of subject tokens to the final prediction\nof \"The mother tongue of Danielle Darrieux \" increases 3 times after editing (i.e. maximum of approximately 0.2\nto maximum of approximately 0.8). Meanwhile, the contribution of relation tokens exhibits minor increase or\neven declines as shown in Fig. 10.\n11\n--- Page 12 ---\n5 10 15 20 25\nlayerrp\nsf\nsm\nsl\nrf\nrm\nrl\n0.0000.0050.0100.0150.0200.025grad(a) 5 epochs\n5 10 15 20 25\nlayerrp\nsf\nsm\nsl\nrf\nrm\nrl\n0.0000.0020.0040.0060.0080.0100.012grad (b) 10 epcohs\n5 10 15 20 25\nlayerrp\nsf\nsm\nsl\nrf\nrm\nrl\n0.0000.0020.0040.0060.008grad (c) 15 epcohs\n5 10 15 20 25\nlayerrp\nsf\nsm\nsl\nrf\nrm\nrl\n0.0000.0010.0020.0030.0040.0050.0060.007grad (d) 20 epcohs\nFigure 8: The gradient saliency maps for the first 5 epochs, the first 10 epochs, the first 15 epochs\nand the first 20 epochs. There are total 20 epochs of optimization during editing of GPT-J 6B. The\nx-axis represents layer numbers and the y-axis represents token positions.\n0510152025303540\ncenter of interval of 10 restored MLP layersThe\n mother\n tongue\n of\n Danielle*\n Dar*\nrie*\nux*\n isImpact of restoring MLP after corrupted input\n0.1000.1250.1500.1750.200\np(French)\n(a) before ROME editing\n0510152025303540\ncenter of interval of 10 restored MLP layersThe\n mother\n tongue\n of\n Danielle*\n Dar*\nrie*\nux*\n isImpact of restoring MLP after corrupted input\n0.20.40.60.8\np(English) (b) after ROME editing\nFigure 9: The impact of restoring MLP after input with corrupted subject tokens in causal mediation\nof GPT2-XL before and after editing.\nC Evaluation Details\nC.1 Metrics\nThe existing metrics we use in our experiments are in accordance with liu et al. [10]. The original basic metrics\ninclude the basic Efficacy accuracy Scores to measure the success rate of target editing with P[o∗\n1]>P[o1]where\no1is the original object of < s1, r1>, the Generalization accuracy score for generalization on the paraphrased\nstatement < s1, r′\n1> with P[o∗\n1]>P[o1], the Subject Specificity S-Specificity accuracy score for specificity\nwithin neighborhood subjects < s2, r1> with the original object o2thatP[o∗\n1]<P[o2]and the Relation Specific\nR-Specificity accuracy score for specificity within unrelated factual associations linked to the subject of target\nediting < s1, r3> with P[o∗\n1]<P[o3].\nC.2 Experimental Setup\nWe improve the optimization process of ROME and propose the two-stage optimization process to eliminate\nshortcut learning during editing.\nFor GPT2-XL, we edit 18th layer and 19th layer successively for each factual association. The hidden represen-\ntation at 19th layer and 20th layer are selected as the relation feature respectively. It takes 25 epochs for the both\ntwo stages. The loss threshold is 2e−2for the first stage and 5e−2for the second stage. The learning rate of\nthe two stages is 5e−1and the weight decay is set to 0.5. For the first stage, the weight for kl divergence loss is\n0.0625 and the weight for the negative likelihood loss is 1. For the second stage, there is no KL divergence and\nthe weight for the Frobenius Norm loss is 0.1.\nFor GPT-J 6B, we edit 6th layer. The hidden representation at 11th layer is selected as the relation feature. It\ntakes 20 epochs for the both two stages. The loss threshold is also 2e−2for the first stage and 5e−2for the\nsecond stage. The learning rate of the two stages is 5e−1and the weight decay is set to 0.5. For the first stage,\nthe weight for kl divergence loss is 0.0625 and the weight for the negative likelihood loss is 1. For the second\nstage, there is no KL divergence and the weight for the Frobenius Norm loss is 0.06.\nFor Qwen2.5 14B, the hyperparameter selections are similar with that for GPT2-XL which also consists of 48\nlayers. We edit 18th layer and 19th layer successively for each factual association. The hidden representation at\n28th layer is selected as the relation feature. It takes 20 epochs for the both two stages. The loss threshold is also\n2e−2for the first stage and 5e−2for the second stage. The learning rate of the two stages is 5e−1and the\n12\n--- Page 13 ---\n0510152025303540\ncenter of interval of 10 restored MLP layersThe*\n mother*\n tongue*\n of*\n Danielle\n Dar\nrie\nux\n is*Impact of restoring MLP after corrupted input\n0.050.10\np(French)(a) before ROME editing\n0510152025303540\ncenter of interval of 10 restored MLP layersThe*\n mother*\n tongue*\n of*\n Danielle\n Dar\nrie\nux\n is*Impact of restoring MLP after corrupted input\n0.0250.0500.0750.100\np(English) (b) after ROME editing\nFigure 10: The impact of restoring MLP after input with corrupted relation tokens in causal mediation\nof GPT2-XL before and after editing.\nweight decay is set to 0.5. For the first stage, the weight for kl divergence loss is 0.0625 and the weight for the\nnegative likelihood loss is 1. For the second stage, there is no KL divergence and the weight for the Frobenius\nNorm loss is 0.1.\nAll our experiments can be done on one NVIDIA A800 80GB GPU.\nThe code for baseline models are released under an open-source license to promote reproducibility and community\nuse, while the datasets are distributed under specific terms to ensure compliance with privacy and ethical\nguidelines.\nC.3 Evaluation on zsRE.\nZero-shot Relation Extraction (zsRE) is a question-answering dataset using rephrased questions generated by\nback-translation. For zsRE, we evaluate the performance on the basic metrics of Efficacy (rewriting accuracy\nfor target editing), Generalization (rewriting accuracy on paraphrased prompts of target editing) and Specificity\n(maintenance accuracy on unrelated facts). Table 2 displays the evaluation results on 1,500samples of zsRE for\nGPT-J.\nEditor Efficacy Generalization Specificity\nraw 26.0 25.6 26.8\nFT-L 68.7 35.0 26.9\nROME 99.9 94.2 26.9\nRETS 93.9 96.2 26.8\nMEMIT 99.8 87.5 26.9\nTOP 87.1 71.0 26.9\nTable 2: Results of editing zsRE on GPT-J.\nThe metric design for Specificity is extremely poor in zsRE. Thus the superiority of TOP can not be well\npresented via this dataset. We analyze that the inferiority of TOP on zsRE dataset compared with other baselines\nis due to the type of prompts. For \"what\" questions (e.g. \" What was the name of the architect who worked on\nLahti Town Hall? \") , the last token position may not provide enough information to reliably serve as the relation\nfeature. The Generalization score is definitely lower for TOP while the Efficacy score is already lower. The\nrobustness of knowledge editing for different forms of factual prompts is worth exploring in future works .\nD Ethics Statement\nOur research focuses on addressing the challenge of outdated or erroneous knowledge encoded in large language\nmodels by developing methods to detect and correct such inaccuracies. At the same time, we are acutely aware\nof the ethical risks this technology entails, particularly the possibility of misuse for disseminating harmful or\nmisleading content. To mitigate these risks, we underscore the critical importance of two key principles: first,\nlanguage models must be obtained from rigorously vetted and reliable sources; second, users must exercise\nutmost caution and critical judgment when utilizing outputs generated by these models.\n13\n--- Page 14 ---\nE Comparison with AlphaEdit\nAlphaEdit [9] is the latest locate-then-edit method that solves the catastrophic forgetting problem for sequential\nediting by projecting perturbation onto the null space of the preserved knowledge. The optimization strategy\nof AlphaEdit is in accordance with ROME and MEMIT. Here we evaluate the performance of AlphaEdit on\n2,000 samples of COUNTERFACT_RS and compare it with the performance of ROME, MEMIT and our TOP\nfor single editing on GPT2-XL 1.5B, GPT-J 6B and Qwen2.5 7B2. The layer selections for these methods on\nQwen2.5 7B can be referred to Table 3. laof TOP is the 11th layer.\nAlphaEdit ROME MEMIT TOP\nediting layer(s) [5,6,7,8,9] [6] [4,5,6,7,8] [6]\nTable 3: Layer selections on Qwen2.5 7B.\nThe performance of AlphaEdit compared with ROME, MEMIT and our TOP is shown in Table 4. All methods\nthat share the original optimization strategy (i.e. ROME, MEMIT and AlphaEdit) fail in Relation Specificity.\nEven the latest AlphaEdit editing can’t avoid this problem. Instead, TOP editing with our two stage optimization\nmaintains desirable Specificity of both Subject and Relation, though there are some trade-offs in Generalization,\nas discussed in the main text.\nEfficacy S-Specificity R-Specificity Generalization Fluency Consistency\nGPT2-XL 1.5B\nROME 99.8 75.4 45.7 88.5 623.4 41.1\nMEMIT 94.0 76.4 61.5 79.3 627.4 39.6\nAlphaEdit 99.8 76.2 41.2 96.3 623.4 42.6\nTOP 98.7 76.5 72.2 78.3 619.7 36.5\nGPT-J 6B\nROME 100.0 79.3 52.8 97.3 621.0 42.8\nMEMIT 100.0 81.1 66.8 95.2 621.5 41.9\nAlphaEdit 99.8 81.9 60.4 96.7 621.4 42.3\nTOP 99.6 82.0 83.5 82.4 618.6 35.9\nQwen2.5 7B\nROME 99.8 82.2 70.7 98.2 625.5 35.4\nMEMIT 100.0 67.3 64.7 90.6 624.4 34.3\nAlphaEdit 100.0 83.6 65.0 99.1 626.3 36.3\nTOP 96.2 84.0 92.2 72.0 623.9 30.0\nTable 4: The evaluation results for AlphaEdit compared with ROME, MEMIT and TOP on COUN-\nTERFACT_RS. Significantly lower values are shown in red.\nF Mass Editing Evaluation\nThe main experimental results exhibits the editing performance under the single editing setting. Here we provide\nthe experimental results under the mass editing setting on 2,000 samples of MCF dataset [8]. There are totally\n20 batches and each batch contains 100 samples which are edited simultaneously. We evaluate the mass editing\nperformance on GPT2-XL and GPT-J for FT-L, ROME, MEMIT and TOP. Hyperparameters are in accordance\nwith Section C.2. The results are shown in Table 5.\nWhile scaling to massive editing, MEMIT shows the best Efficacy performance as it is designed for mass editing.\nHowever, both MEMIT and ROME still suffer from low R-Specificity. For TOP and ROME, mass editing\nleads to some degradation over all metrics. Despite this, TOP exhibits the optimal overall performance with no\napparent deficiencies. The results verify the effectiveness of our two-stage optimization strategy in the mass\n2Due to the limited computational resource, here we evaluate AlphaEdit and other methods on Qwen2.5 7B\ninstead of Qwen2.5 14B\n14\n--- Page 15 ---\nediting setting. However, applying the two-stage optimization for stable and robust massive editing withour\ndegradation remains exploring in future works.\nEfficacy S-Specificity R-Specificity Generalization Fluency Consistency\nGPT2-XL 1.5B\nFT-L 30.6 76.7 91.6 26.2 625.0 32.0\nROME 89.4 63.9 36.9 86.8 560.1 26.1\nMEMIT 99.8 72.7 28.7 95.4 621.8 42.9\nTOP 83.9 62.9 61.5 73.9 600.0 23.0\nGPT-J 6B\nFT-L 24.9 83.0 99.8 19.9 621.4 29.5\nROME 91.1 67.9 42.8 89.3 575.6 33.6\nMEMIT 100.0 75.1 44.7 98.1 609.8 42.2\nTOP 89.1 73.3 75.7 78.5 614.0 30.6\nTable 5: The mass editing results on MCF. Each batch contains 100 samples which are edited\nsimultaneously. Significantly lower values are shown in red.\n15",
  "text_length": 59464
}