{
  "id": "http://arxiv.org/abs/2506.05331v1",
  "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
  "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
  "authors": [
    "Xinyan Chen",
    "Renrui Zhang",
    "Dongzhi Jiang",
    "Aojun Zhou",
    "Shilin Yan",
    "Weifeng Lin",
    "Hongsheng Li"
  ],
  "published": "2025-06-05T17:59:02Z",
  "updated": "2025-06-05T17:59:02Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05331v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05331v1  [cs.CV]  5 Jun 2025MINT-CoT: Enabling Interleaved Visual Tokens in\nMathematical Chain-of-Thought Reasoning\nXinyan Chen∗1, Renrui Zhang∗†1, Dongzhi Jiang1, Aojun Zhou1\nShilin Yan, Weifeng Lin1, Hongsheng Li1\n1CUHK MMLab\nchenxyxy06@gmail.com renruizhang@link.cuhk.edu.hk\nhsli@ee.cuhk.edu.hk\n∗Equal Contribution†Project Leader\nAbstract\nChain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT. However,\nthey face three key limitations for math problem-solving: reliance on coarse-\ngrained box-shaped image regions, limited perception of vision encoders on math\ncontent, and dependence on external capabilities for visual modification . In this\npaper, we propose MINT-CoT , introducing Mathematical INterleaved Tokens\nforChain- of-Thought visual reasoning. MINT-CoT adaptively interleaves rel-\nevant visual tokens into textual reasoning steps via an Interleave Token, which\ndynamically selects visual regions of any shapes within math figures. To empower\nthis capability, we construct the MINT-CoT dataset, containing 54K mathemati-\ncal problems aligning each reasoning step with visual regions at the token level,\naccompanied by a rigorous data generation pipeline. We further present a three-\nstage MINT-CoT training strategy, progressively combining text-only CoT SFT,\ninterleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B\nmodel. Extensive experiments demonstrate the effectiveness of our method for\neffective visual interleaved reasoning in mathematical domains, where MINT-\nCoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on\nGeoQA, and +23.2% on MMStar, respectively. Our code and data are available at\nhttps://github.com/xinyan-cxy/MINT-CoT .\n1 Introduction\nChain-of-Thought (CoT) [ 66,32] has emerged as an effective strategy for enhancing the reasoning\ncapabilities of Large Language Models (LLMs) [ 49,51,62,71,79,39] by generating sequential\nrationales in their responses. In Multimodal Large Language Models (MLLMs) [ 50,33,86,18,20],\nCoT also plays a significant role [ 82] across various tasks involving image [ 41,84,40,22,17,25],\nvideo [ 38,4,70,14], and 3D [ 69,24,58,21]. It enables MLLMs to reason over both textual and\nvisual inputs, serving as a bridge that connects visual perception with abstract reasoning tasks.\nHowever, despite these advances, applying CoT in mathematical reasoning with visual contexts\nremains challenging. Existing MLLMs mainly generate text-only reasoning steps for multimodal math\nproblems [ 82,83,60,77], simply adopting similar textual reasoning for image input. Nevertheless,\nPreprint. Under review.\n--- Page 2 ---\nIn the given diagram, circle O has line segment AB as its diameter and CD as a chord. A \ntangent passing through point C intersects the extension of AB at point E, and angle E \nmeasures 42°. What is the measure of angle CDB?  Choices:  A: 22°  B: 24°  C: 28°  D: 48°Question:\nText-only CoT Reasoning:\nBox-shaped Visual CoT Reasoning:\nVisual Interleaved CoT Reasoning (Ours):\nStep 1:             Connect OC, OC⊥CE.\nStep 2:                           AngleCOE = 180° - 90° - 42° = 48°.\nStep 3:             Since OC = OD, angleCDB = angleODC = 1/2 angleBOC = 24Step 1: Since angle E = 42°, therefore angle A = 42°. \nStep 2: Since AB is the diameter of circle O, angle ACB = 90°. Therefore, angle B = 180° - 42° - 90° = 48°.\nStep 3: Since AB is the diameter of circle O, angle CDB = angle B = 48°.\nAnswer: D  \nStep 1:            Since angle E = 42°, angle CEB = 42°\nStep 2:                            Since AB is the diameter of circle O, angle ACB = 90°.\nStep 3:              Therefore, angle CDB = angle CEB = 42°. \n ×\n×\nAttention-based Token Selection:\nMLLM\nAttn.\nMLLM×\nInterleaved Token Selection:\nInterleave \nToken\n✓×\n✓\n✓\n✓✓\nAnswer: BAnswer: D×✓\n✓×\nBox-level cues are too coarse for \nmath concepts.\nLack of visual information leads to \nperception error.\nToken-level interleaved CoT is \nfine-grained and effective.\nFigure 1: Comparison of three CoT reasoning methods: text-only CoT reasoning, box-shaped visual\nCoT reasoning and our visual interleaved CoT reasoning methods. (1) Text-only CoT lacks visual\ninformation, causing perception errors in mathematical reasoning. (2) Box-level cues are too coarse to\ncapture complex visual structures in mathematical images. (3) Token-level interleaved CoT accurately\nidentifies fine-grained visual regions to support reasoning.\ndue to the limited capability in perceiving math images, this strategy often fails to accurately interpret\nvisual information within the CoT process, leading to reasoning errors.\nRecent approaches have attempted to interleave visual content within reasoning steps through mecha-\nnisms such as bounding box selection and image cropping [ 55,26,74]. While effective in general\nvisual scenarios, these methods still face three key limitations when extended to multimodal mathe-\nmatical reasoning:\n1.Reliance on coarse-grained box-shaped image regions: Recent advances introduce visual\ninformation into the CoT process by selecting image regions through bounding box-based\nmethods. Visual-CoT [ 55], Visual SKETCHPAD [ 26], and VPT [ 74] all operate on box-\nshaped image regions, employing strategies such as bounding box generation, iterative\nmasking, cropping, or re-encoding. However, as shown in Figure 1, these approaches all rely\non bounding box-based cropping. While such box-level cues are effective in domains like\nobject detection, where objects are typically isolated, they are too coarse-grained to capture\nthe complex structures in mathematical images, where visual information is not discrete\nbut highly interconnected. As a result, box-shaped selection tends to interleave too many\nirrelevant or misleading visual tokens, impairing the accuracy of mathematical reasoning.\n2.Limited perception of vision encoders on math content: Some methods, like ICoT [ 16],\nadopt attention-based token selection to identify relevant visual tokens during reasoning\nwithout requiring additional training. These approaches rely heavily on visual features\nextracted by the vanilla vision encoders without specific tuning. However, as noted in\nMA VIS [ 81], mainstream vision encoders, which are primarily based on CLIP [ 54] or\nSigLIP [ 76], are pre-trained on natural images with general scenes, making mathematical\nimages out-of-distribution. As a result, such methods often struggle to accurately locate\nrelevant visual regions in complex mathematical tasks.\n2\n--- Page 3 ---\n3.Dependence on external capabilities for visual modification: Other approaches attempt\nto enhance visual reasoning by dynamically generating new visual content or modifying\nexisting images. MV oT [ 36] is built upon a unified autoregressive MLLM [ 59] to generate\nimages as part of the CoT process, but it is only applicable to spatial planning tasks.\nMeanwhile, Visual SKETCHPAD requires external tools to draw on the original image in\ngeometry-related tasks. These approaches depend on external capabilities, either requiring\nlarge-scale data to train the understanding model for generation, or relying on external tools\nwith additional inference over the modified images, which leads to numerous extra costs.\nTherefore, to address these challenges, we aim to propose a fine-grained, efficient visual interleaved\nCoT method to enhance the mathematical reasoning capabilities of MLLMs. In this paper, we\nintroduce MINT-CoT , an approach of Mathematical INterleaved Token selection for Chain- of-\nThought reasoning, which facilitates multimodal reasoning by interleaving relevant visual regions\nwithin reasoning steps. At the core of the MINT-CoT is the Interleave Token, a special token generated\nthrough the next-token prediction process. During reasoning, MINT-CoT automatically identifies and\nincorporates the most relevant visual tokens from the original image at each reasoning step. This is\nachieved by computing similarity scores between the output hidden states of the Interleave Token\nand all visual tokens, in order to identify the tokens most relevant to the mathematical concept at the\ncurrent step. These selected visual tokens are then dynamically integrated into the textual reasoning\nsteps, enabling the flexible selection of visual regions throughout the CoT process. In this way, the\ninterleaved regions of mathematical images are not restricted to box-shaped areas but can flexibly\ninclude geometric shapes, line segments, coordinates, and other elements.\nTo enable effective training of MINT-CoT, we construct the MINT-CoT dataset, a 54K visual\ninterleaved reasoning dataset. Each data point contains reasoning steps paired with the indices\nof selected tokens corresponding to the mathematical concepts involved in each step. We source\nmathematical problems from the Mulberry-260K dataset [ 73] to construct text-only CoT reasoning\nformat, then annotate the reasoning steps with corresponding image regions through a four-step\npipeline: (1) dividing images into grid-indexed regions, (2) mapping recognized text elements to\ngrid indices via OCR-based text localization, (3) extracting key words, and (4) assigning visual\nregions to these key words using an advanced MLLM. This process creates a visual interleaved CoT\nreasoning dataset providing token-level supervision for training models to interleave visual content\ninto reasoning steps.\nBuilding on the MINT-CoT framework and MINT-CoT dataset, we design a progressive training\nstrategy, the MINT-CoT training strategy, that incrementally improves MLLMs’ ability with three\ntraining stages: (1) Text-only CoT Training, (2) Interleaved CoT SFT, and (3) Interleaved CoT RL.\nThrough this training strategy, we train a MINT-CoT-7B model with the capability of mathematical\nvisual interleaved CoT reasoning. Extensive experiments demonstrate the superiority of our proposed\napproach. Specifically, our method achieves absolute improvement of +32.59% on MathVista [ 43],\n+26.92% on GeoQA [5], and +23.2% on MMStar [7] benchmark compared to the baseline model.\nOur main contributions are as follows:\n•We propose MINT-CoT, which uses the Interleave Token to interleave fine-grained visual\ntokens within reasoning steps, enhancing multimodal mathematical reasoning.\n•We construct the MINT-CoT dataset, a 54K dataset for multimodal mathematical reasoning,\noffering fine-grained alignment between textual rationales and visual inputs. We develop an\nautomated pipeline to generate visual interleaved CoT data annotated with token indices.\n•We develop a progressive three-stage MINT-CoT training strategy, to improve interleaved\nmathematical reasoning. Extensive experiments validate the efficiency of our method.\n2 Related work\nMLLMs for Mathematics. Recent advancements in MLLMs [ 50,41,2,31] have shown impressive\ncapabilities in various vision-language tasks. However, even powerful models like GPT-4V [ 50]\nand Qwen2-VL [ 63] fail to demonstrate satisfying performance on existing visual mathematical\nbenchmarks [ 5,44,43], as highlighted by MathVerse [ 80]. Various specialized approaches [ 15,81,\n28,9,45,57,53] have emerged to enhance visual mathematical reasoning. Current approaches\nmostly focus on enriching the multimodal math data. G-LLaV A [ 15] extends the LLaV A architecture\n3\n--- Page 4 ---\n×\nLarge Language Model\nVision \nEncoder... ...\nQuestion and \nInstructions...\nStep1: ... \nsegment AE ...\nPost Visual \nProjector\n... ......\nRationales\nInput Image\nPost Interleave Projector      \nSelected \nVisual TokensSelected \nVisual Tokens\nStep2: ...∠DBO ...\nInterleave \nTokenInterleave \nToken\n×Cosine Similarity \nScoresCosine Similarity \nScores\n......\n...\nyyy\nyyy...Figure 2: Overview of the MINT-CoT framework. During CoT reasoning, MINT-CoT generates an\nInterleave Token before each reasoning step and computes the similarity scores between embeddings\nprojected by the decoder-side visual projector and the interleave projector. Based on these similarity\nscores, relevant visual tokens are selected, and the model inferences with these selected visual tokens.\nwith geometric reasoning capabilities by augmenting the current dataset. Math-LLaV A [ 57] enlarges\nthe data scope with the introduced MathV360K dataset. MA VIS [ 81] first identifies the critical issue\nof the vision encoder and empowers it with the mathematical capability. Then it further develops an\nautomated system for generating mathematical visual datasets at scale. Reverse Chain-of-Thought\n(R-CoT) [ 9] introduces the Geometry Generation Chain for creating geometric images with more\naccurate descriptions.\nVisual Chain of Thought. With advancements of various visual reasoning tasks [ 43,75,30],\nvisual chain of thought has been emerging as an effective method for both image generation [ 23,29,\n61,85] and understanding [ 52,73,60] tasks. Our work focuses on leveraging it for reasoning on\nimages, where two distinct methods have emerged. One line of the method relies on textual CoT to\nconduct multimodal analysis [ 11,46,6,77,10,72]. For example, R1-V [ 6] extends the paradigm of\nDeepSeek R1 [ 19] to generate a comprehensive text CoT to analyze the visual information before\nproviding the final answer. Another line of method explicitly incorporates multimodal elements\nin the rational [ 55,47,67,26,35]. Visual CoT [ 55] and Chain-of-Spot [ 42] propose to crop the\nregion of high interest on the image and integrate it into the CoT process. Chain-of-Image [ 47] and\nVisual SKETCHPAD [ 26] introduce auxiliary tools to generate helpful diagrams for mathematical or\ngeometric problem-solving. Although these methods demonstrate competitive performance, they are\nlimited to rigid image cropping or dependence on external tools. Recently, ICoT [ 16] leverages the\nattention map of the MLLM to select the relevant visual tokens to compose the multimodal rational.\nHowever, this approach relies solely on attention scores on the image feature maps, which have been\nshown to be insufficiently informative for mathematical scenarios [81].\n3 Method\nTo address the challenges of multimodal CoT in mathematical reasoning, we propose MINT-CoT. In\nthis section, we first introduce the framework of MINT-CoT in Section 3.1. Then we introduce the\nMINT-CoT dataset and provide a detailed discussion of the dataset generation method in Section 3.2.\nFinally, we present the progressive MINT-CoT training strategy in Section 3.3.\n3.1 MINT-CoT\nPrevious CoT approaches in MLLMs mainly generate text-based reasoning steps, which are not\nexplicitly grounded in visual features and therefore struggle with mathematical reasoning that involves\n4\n--- Page 5 ---\nvisual details. We formulate this CoT reasoning process as:\n{s(1), s(2), . . . , s(k)}, answer =LLM(V,TextEncoder (T)), (1)\nwhere V=VisionEncoder (I) ={vτ}N\nτ=1denotes the visual feature extracted from the input image\nI, and each vτrepresents the τ-th visual token generated by the vision encoder. Tdenotes the input\nmathematical question and instructions, {s(i)}is the sequence of textual reasoning steps generated by\nthe model, and answer is the final answer. Recent advancements attempt to incorporate multimodal\nreasoning steps in the CoT process. However, current coarse-grained methods only focus on selecting\nbox-shaped visual regions; how to adaptively select the visual content in alignment with each textual\nreasoning step remains an open question. We thus propose the MINT-CoT framework and introduce\nan Interleave Token to help MLLMs select visual tokens from the visual feature V. The overview of\nthe MINT-CoT framework is illustrated in Figure 2.\nInterleave Token. An Interleave Token is a special token generated prior to each reasoning step. It\nis used to select visual tokens that are relevant to the mathematical concepts involved in that step (e.g.,\n“line segment AB”, “angle DOC”), thereby facilitating the reasoning process. When an Interleave\nToken is output in step i, its output hidden state h(i)\npost_intlv is projected via a post interleave projector\nPpost_intlv , while all the output hidden states of the visual tokens hpost_vis are projected via a post visual\nprojector Ppost_vis . The cosine similarity between the two projected embeddings is first computed and\nthen scaled by a learnable parameter γ:\nα(i)=γ·cos\u0010\nPpost_intlv (h(i)\npost_intlv ), P post_vis (hpost_vis )\u0011\n. (2)\nEach tokens’ similarity score α(i)\nτis then compared against a predefined threshold θ, and visual\ntokens with scores above this threshold are selected:\n{v(i)}={v(i)\nτ|α(i)\nτ> θ}. (3)\nThe selected tokens {v(i)}are interleaved into the reasoning process at step i. In this way, the\nimportant visual regions are interleaved into the model, prior to each textual step, enhancing visual\nperception and improving reasoning accuracy.\nInference with Interleaved Visual Tokens. With the selected visual tokens {v(i)}obtained at each\nreasoning step, MINT-CoT interleaves both visual content and text-based reasoning steps throughout\nthe inference process, ultimately producing the final answer. Formally, this process extends the\nstandard CoT formulation in Eq. 1 as:\n{v(1), s(1), v(2), s(2), . . . , v(k), s(k)},answer =LLM(V,TextEncoder (T)). (4)\nThis interleaved token selection mechanism enables the model to explicitly ground visual evidence\nthroughout the reasoning chain, thereby facilitating visual interleaved CoT reasoning for solving\nmultimodal mathematical problems.\n3.2 Dataset Curation\nTo empower MINT-CoT capabilities for MLLMs, we develop a data generation pipeline that au-\ntomatically generates mathematical visual interleaved data annotated with selected token indices,\nand obtain 54K samples for model training. To construct the text-only cot format of our dataset,\nwe begin by selecting mathematical problems from the Mulberry-260K dataset [ 73], which was\ncreated using Collective Monte Carlo Tree Search and demonstrates strong performance on reasoning\ntasks. Specifically, we extract the “### Rationale” and“### Steps” sections from the dataset as the\nreference reasoning steps for our task. Using these sections alongside the corresponding images, we\nfollow a four-step data construction process, as shown in Figure 3:\n1.Grid Images. To obtain the indices of visual tokens for subsequent token index annotation\nin textual reasoning steps, we divide the original images into grid cells. Following the\npatch-splitting strategy used in vision encoders such as Vision Transformer [ 12], each image\nis partitioned into a grid, and a unique index is assigned to each cell. These grid cells and\ntheir indices are subsequently overlaid onto the original images to produce grid-indexed\nimages .\n5\n--- Page 6 ---\nGPT-4o\nOriginal Images Grid-indexed Images\nOCR Text–index Pairs\nGPT-4o\nText-only CoT ReasoningKey WordsStep 1: Grid Images Step 2: Apply OCR\nStep 3: Extract Key Words\nStep 4: Align and \nAnnotate Key Words\nOutputs: \nWhat is the length of BC in triangle ABC, \nif DE is parallel to BC, and the ratio of \nAD to AB is 1.0:3.0, with DE equal to 4.0?\nChoices:\\nA: 16\\nB: 15\\nC: 12\\nD: 8MINT-CoT Dataset Example:\nQuestion:\nQuestion: ...\nSolution:\n### Rationales: ...\n### Step 1:\\nSince ...\n### Step 2:\\nThe ratio of similar \ntriangles ADE to ABC is the same as the \nratio of AD to AB. Given AD:AB = 1.0:3.0, \nthe ratio of sides is 1/3.\n### Step 3:\\nGiven DE = 4.0, which \ncorresponds to 1/3 of BC in the similar \ntriangles, set up the proportion using DE to \nBC.\n### Step 4:\\nLet x be the length of BC. \nThen, (1/3)x = 4.0.\n...\n### The final answer is: ...•“A”: 4; “B”: 24; “C”: 29; “D”: 8; \n“E”: 10,11\n•“9:00”: \n10,11,12,13,14,18,19,20,21,22\n•“1 cm”: 4,5; “2 cm”: 24,25\n•“A”: 8; “B”: 28; “C”: 34; “D”: 5; \n“E”: 3, “F”: 20\n\"ratio of AD to AB\": \n3,4,8,9,14,19,24,25\n\"DE\": 8,9,10,11,14,15,16\n\"BC\": 24,25,26,27,28,29\"Step 2\": [\"ratio of AD \nto AB\"],\n\"Step 3\": [\"DE\", \"BC\"],\n\"Step 4\": [\"BC\"]\nVisual Interleaved CoT Reasoning### Rationales:The problem involves a circle with a chord AB \nwhere a radius OC is perpendicular to the chord. This situation \ncreates a right-angled triangle ODB, which can be analyzed \nusing the Pythagorean theorem. The goal is to find the length \nof OB (denoted as u). Given data includes the length of the \nchord AB and the segment OD.\n### Let's think step by step.\n### Step 1:Since DE is parallel to BC, triangles ADE and ABC \nare similar by the Basic Proportionality Theorem (or Thales' \ntheorem).\n### Step 2:<interleave>3,4,8,9,14,19,24,25</interleave> \nThe ratio of similar triangles ADE to ABC is the same as the \nratio of AD to AB. Given AD:AB = 1.0:3.0, the ratio of sides is \n1/3.\n### Step 3:<interleave>8,9,10,11,14,15,16,24,25,26,27, \n28,29</interleave>Given DE = 4.0, which corresponds to 1/3 \nof BC in the similar triangles, set up the proportion using DE to \nBC.\n### Step 4:<interleave>24,25,26,27,28,29</interleave> \nLet x be the length of BC. Then, (1/3)x = 4.0.\n### Step 5:Solve for x by multiplying both sides by 3, giving \nx = 4.0 * 3 = 12.\n### The final answer is: CFigure 3: Data generation pipline. Step 1: Grid Images. We divide each image into grid cells\nand assign index values to each cell. Step 2: Apply OCR. We use PaddleOCR to recognize textual\nelements and associate them with corresponding grid indices. Step 3: Extract Key Words. We employ\nGPT-4o to extract key words from each reasoning step. Step 4: Align and Annotate Key Words. We\nuse GPT-4o to annotate each key word with the grid indices, and get the final visual interleaved CoT\nreasoning steps.\n2.Apply OCR. Then, to more accurately map token indices onto textual reasoning steps,\nwe apply PaddleOCR [ 37] to recognize textual elements in the original images. And we\nalign the bounding boxes of the detected text with their corresponding grid indices, thereby\nconstructing “OCR text–index” pairs .\n3.Extract Key Words. Certain mathematical concepts often play a significant role in each\nreasoning step. Selecting visual tokens closely related to these concepts can improve\nreasoning accuracy. Therefore, we employ GPT-4o [ 12] to extract key words from each\nreasoning step. Since the extracted key words are used in the subsequent annotation with\nvisual indices, they are extracted only when a reasoning step contains links to visual tokens.\n4.Align and Annotate Key Words. Finally, given the grid-indexed images, the “### Rationale”\nand “### Steps” sections, the “OCR text–index” pairs, and the extracted key words, we\nprompt GPT-4o to annotate each key word with the corresponding grid indices. These\nannotated indices are subsequently inserted into the reasoning steps associated with their\ncorresponding key words, resulting in a visual-interleaved CoT reasoning dataset.\nThrough this process, we construct a dataset of 54K samples, where the reasoning steps are annotated\nwith corresponding grid indices. As shown in the right column of Figure 3, each data point consists\nof a mathematical problem and an image as input, with the corresponding visual interleaved CoT\nresponse as output. This dataset serves as the foundation for training the MINT-CoT models. Further\ndetails are provided in Appendix A.2.\n3.3 Training strategy\nBuilding on the previously introduced MINT-CoT framework and dataset, we now describe the\ncorresponding MINT-CoT training strategy, which consists of three stages: (1) Text-only CoT\nTraining, (2) Interleaved CoT SFT, and (3) Interleaved CoT RL.\nStage 1: Text-only CoT SFT. To enable the MLLM to adopt a general reasoning format, we first\ntrain the base model using the text-only CoT reasoning data in MINT-CoT dataset, without visual\ninterleaving. This stage serves as a foundation for subsequent interleaved training.\nStage 2: Interleaved CoT SFT. In the second stage, we aim to train the model to select visual\ntokens using the Interleave Token and adapt to reasoning with interleaved visual content. The model\n6\n--- Page 7 ---\nis fine-tuned with a loss that jointly optimizes both textual reasoning and visual alignment. As\nintroduced in Eq. 4, the output sequence of MINT-CoT alternates between sets of selected visual\ntokens v(i)and textual reasoning steps s(i), followed by the final answer:\n{v(1), s(1), v(2), s(2), . . . , v(k), s(k)},answer ∼Pθ(· |I, T), (5)\nWe first apply a cross-entropy loss to textual tokens at positions T⊂ {1,2, . . . , T }covering all\nsegments {s(i)}and the answer, while conditioning on the full preceding sequence. Let Y=\n{y1, y2, . . . , y T}denotes the full sequence of output tokens. Specifically, the loss for predicting the\nnext textual token is defined as:\nLCE=−X\nt∈TlogPθ\u0000\nyt|y<t, I, T\u0001\n(6)\nWe do not supervise the cross-entropy loss for predicting the Interleave token. Instead, we manually\nconcatenate it at each step, and during inference, we concatenate the Interleave Token whenever\nthe “ ### Step ” marker is generated. To supervise the interleaved visual tokens, we apply a binary\ncross-entropy loss on the scaled cosine similarity scores αintroduced in Eq. 2 with ground-truth\nlabels X∈ {0,1}:\nLBCE=−NX\ni=1LX\nj=1\u0010\nXijlogσ(αij) + (1 −Xij) log(1 −σ(αij))\u0011\n, (7)\nwhere Nis the number of Interleaved Tokens in a batch, Lis the length of input visual tokens, and\nσ(·)denotes the sigmoid function. The final training objective is defined as the sum of both losses:\nL=LCE+LBCE. (8)\nThis combined loss guides the model to jointly align visual tokens and perform interleaved reasoning.\nStage 3: Interleaved CoT RL. To move beyond supervised annotations, we aim to enable the\nmodel to autonomously explore more flexible and effective selection of visual tokens guided by\nreasoning objectives, and enhance its ability to perform interleaving CoT reasoning. Reinforcement\nlearning provides a natural framework for this goal. To this end, we extend the Group Relative Policy\nOptimization (GRPO) [ 56] framework to our MINT-CoT training strategy. For a group of reasoning\nchains with group size G, we compute answer correctness as the reward r∈ {0,1}and define the\nadvantage via group-wise comparison as ˆAj=rj−mean (r)\nstd(r),where rjindicates if the j-th chain of\nsteps in a group yields the correct answer. The policy loss for the generated tokens is then formulated\nas:\nLGRPO =−E{Yj}G\nj=1\n1\nGGX\nj=1\u0012Pθ(Yj)\nPθold(Yj)ˆAj−βD KL[Pθ∥Pref]\u0013\n, (9)\nwhere Prefis a reference policy that serves as a regularization target. This stage further strengthens\nthe model’s reasoning ability with visual interleaved content, ultimately resulting in MINT-CoT-7B.\nAdditional theoretical details of this training stage are provided in Appendix A.3.\n4 Experiments\nIn this section, we first introduce the experimental settings in Section 4.1. Then, we discuss the\nquantitative results and ablation study in Section 4.2 and Section 4.3 respectively. Finally, we present\nthe qualitative results in Section 4.4.\n4.1 Experimental Settings\nImplementation Details. We build on Qwen2-VL-7B [ 64] and train our model in three stages with\na combination of SFT and RL on the MINT-CoT dataset. All model parameters except the vision\nencoder are updated. Full implementation details are provided in Appendix A.4.\nTest Benchmark. We evaluate MINT-CoT on three mathematical benchmarks: GeoQA [ 5], Math-\nVista [ 43] and MMStar [ 7]. GeoQA is a benchmark of geometric problems with annotated solution\nprograms. To evaluate on GeoQA, we follow R1-V [ 6] and Hint-GRPO [ 27] using the Geo170K\ntest set [ 15], the English version of the GeoQA benchmark. MathVista is a benchmark designed to\n7\n--- Page 8 ---\nTable 1: Combined quantitative results on MathVista. We evaluate MINT-CoT-7B, the baseline\nmodel, and state-of-the-art general and reasoning MLLMs on the mathematical subset of MathVista.\nMINT-CoT significantly outperforms the baseline model and achieves superior performance compared\nto open-source reasoning models. Bold andunderlined results indicate the best and second-best\namong open-source models, respectively.\nModel #ParamsMathVista-Math\nAll GEO ALG GPS TQA\nClosed-Source Model\nGPT-4o [48] – 66.67 63.68 67.04 63.46 77.42\nClaude-3.5 Sonnet [1] – 67.41 65.09 67.79 65.38 74.19\nOpen-Source General Model\nLLaV A-OneVision-Qwen2-7b-ov [34] 7B 67.04 69.34 67.04 69.71 58.06\nInternVL2-8B [8] 8B 62.59 62.26 62.92 62.50 62.90\nInternVL2-8B-MPO [65] 8B 68.52 68.87 68.91 69.71 64.52\nDeepSeek-VL2 [68] 4.5B 65.56 63.68 65.54 63.94 70.97\nQwen2.5-VL-7B-Instruct [3] 7B 66.66 65.56 66.29 65.87 69.35\nOpen-Source Reasoning Model\nOpen-R1-Multimodal [13] 7B 54.81 52.36 54.68 53.37 59.68\nR1-VL-7B [78] 7B 69.63 68.87 69.66 69.71 69.35\nMulberry [73] 7B 68.52 67.92 68.54 68.75 67.74\nMM-Eureka [46] 7B 72.59 71.22 72.66 72.60 72.58\nQwen2-VL-7B-Instruct [64] (Baseline) 7B 41.11 35.85 41.57 36.54 56.45\nMINT-CoT-7B 7B 73.70 74.53 73.78 75.00 69.35\n∆over the Baseline Model +32.59 +38.63 +32.21 +38.46 +12.9\nTable 2: Combined quantitative results of on\nGeoQA. We evaluate MINT-CoT-7B, the baseline\nmodel and the state-of-the-arts.\nModel GeoQA\nQwen2.5-VL-7B-Instruct [3] 43.50\nR1-V [6] 59.00\nOpen-R1-Multimodal [13] 48.67\nHint-GRPO [27] 55.31\nQwen2-VL-7B-Instruct [64] (Baseline) 37.80\nMINT-CoT-7B 64.72\n∆over the Baseline Model +26.92Table 3: Combined results on the mathematical\nsubset of MMStar. We evaluate MINT-CoT-7B,\nthe baseline model and the state-of-the-arts.\nModel MMStar-Math\nQwen2.5-VL-7B-Instruct [3] 66.8\nInternVL2-8B [8] 66.8\nR1-VL-7B [77] 68.4\nMulberry [73] 66.8\nOpen-R1-Multimodal [13] 59.2\nQwen2-VL-7B-Instruct [64] (Baseline) 46.4\nMINT-CoT-7B 69.6\n∆over the Baseline Model +23.2\nintegrate challenges from diverse mathematical and visual tasks. As our paper targets specifically\nmathematical problems, we extract the mathematical subsets (FunctionQA, Geometry3K, GeoQA+,\nGEOS, and UniGeo), i.e., ‘MathVista-Math’ in Table 1, and report accuracy scores across four\nprimary tasks: geometry reasoning (GEO), algebraic reasoning (ALG), geometry problem solving\n(GPS), and textbook question answering (TQA). MMStar is a multi-modal benchmark covering dif-\nferent core capabilities and detailed axes. For evaluation, we also extract the mathematical capability\ndimension, referred to as “MMStar-Math”.\n4.2 Quantitative Results\nComparison with the Baseline. As shown in Table 1 for the results of mathematical subsets of\nMathVista, our MINT-CoT-7B achieves an improvement of up to +32.59% over the baseline, and\nimproves a lot on all four primary tasks. This strongly demonstrates the effectiveness of our MINT-\nCoT framework and training strategy. Table 2 presents the results on the GeoQA benchmark, where\nour MINT-CoT-7B outperforms the baseline model by +26.92%. Similarly, in Table 3, MINT-CoT-7B\noutperforms the baseline model by +23.2% on MMStar-Math, validating the efficiency of MINT-CoT\non geometry problems.\n8\n--- Page 9 ---\nTable 4: Ablation study on different training stages. We evaluate the three progressive training\nstages on different benchmarks.\nModel MMStar-Math GeoQAMathVista-Math\nAll GEO ALG GPS TQA\nBaseline [64] 46.4 37.80 41.11 35.85 41.57 36.54 56.45\n+ Text-only CoT SFT 67.6 59.02 64.07 64.15 64.04 64.42 62.90\n+ Interleaved CoT SFT 68.0 62.07 67.78 66.51 67.79 67.31 69.35\n+ Interleaved CoT RL 69.6 64.72 73.70 74.53 73.78 75.00 69.35\nTable 5: Ablation study of different interleaving methods on GeoQA\nand MathVista-Math. Our Interleaved CoT SFT achieves the highest\nimprovement on both benchmarks, demonstrating the effectiveness of\nour interleaved token selection method.\nModel GeoQAMathVista-Math\nAll GEO ALG GPS TQA\nOriginal 37.80 41.11 35.85 41.57 36.54 56.45\nText-only CoT SFT 59.02 64.07 64.15 64.04 64.42 62.90\nOriginal Image CoT SFT 61.41 40.37 38.68 40.82 39.42 43.54\nBounding Box CoT SFT 61.80 65.56 63.21 65.54 63.94 70.97\nInterleaved CoT SFT (Ours) 62.07 67.78 66.51 67.79 67.31 69.35Figure 4: F1 score plot of\nvisual token selection dur-\ning Interleaved CoT SFT.\nQwen2vl-7B-Instruct Given triangle ABC where AB = AC and angle BAC measures 100.0 degrees, and the \nperpendicular bisector of AB intersects AB and BC at points D and E, what is the measure of angle \nBAE? Represent the measure of angle BAE as θ.  Choices:  A: 80°  B: 60°  C: 50°  D: 40°Question:/data4tb/xinyan/data/mulberry_interleave/mulberry_images/geoqa_plus/213.png<image>Provide a rationale to analyze the question.\\nNext, generate a step-by-step \nreasoning process to solve the problem. Ensure the steps are logical and \nconcise.\\nFinally, provide a concise summary of the final answer in the following format: \n'The final answer is: xxx.\\n\\nFormat your response with the following sections, \nseparated by ###:\\n### Rationales:\\n### Let's think step by step.\\n### Step 1:\\n### \nStep 2:\\n...\\n### The final answer is: \\n\\nQuestion: Given triangle ABC where AB = AC \nand angle BAC measures 100.0 degrees, and the perpendicular bisector of AB intersects \nAB and BC at points D and E, what is the measure of angle BAE? Represent the measure \nof angle BAE as θ.\\nChoices:\\nA: 80°\\nB: 60°\\nC: 50°\\nD: 40°\nMINT-CoT-7B\n### Rationales: The problem involves a geometry question about angles in an isosceles triangle with certain line \nintersections. To find angle BAE, consider properties of isosceles triangles and angle bisectors.\n### Let's think step by step.\n### Step 1:                        Since triangle ABC is isosceles with AB = AC, angles ABC and ACB are equal.\n### Step 2: \n### Step 3:\n### Step 4: \n### The final answer is: D\nCalculate angle ABC and ACB. The sum of angles in triangle ABC is 180 degrees. Therefore, \nangle ABC = angle ACB = (180 - angle BAC) / 2 = (180 - 100) / 2 = 40 degrees. \nThe perpendicular bisector of AB implies that point E is equidistant from A and B, making BE = \nAE. The bisector makes angle BAE an angle in triangle BAE.\nApply angular considerations: Consider that both AE and BE are equal, and with angle BAC = 100 \ndegrees and angle ABC = 40 degrees, angle BAE will share properties of an isosceles triangle and \nangle relationships.\nCosine Similarity \nScoresTo solve this problem, we first need to visualize the triangle and the given information:\\n\\n1. Triangle ABC is isosceles \nwith AB = AC and angle BAC = 100.0 degrees.\\n2. The perpendicular bisector of AB intersects AB at point D and intersects \nBC at point E.\\n\\nSince AB = AC, triangle ABC is isosceles as well. The perpendicular bisector of AB implies that AE is the \nmedian to the hypotenuse of triangle ABC (because D is the midpoint of AB). \\n\\nAlso, in an isosceles triangle, if a median \nis drawn to the hypotenuse, it creates two right-angled triangles. Let's denote the measure of angle BAE as θ. Due to the \nproperties of the isosceles triangle and the perpendicular bisector, angle BAE (θ) should be half of the apex angle \nBAC.\\n\\nSince ∠BAC = 100.0 degrees, θ = 100.0 / 2 = 50.0 degrees.\\n\\nTherefore, the correct answer is C.×\n✓\nComparison with State-of-the-arts. We also compare our model with state-of-the-art MLLMs,\nincluding closed-source model, open-source models, and open-source reasoning models. Specifically,\nfor open-source reasoning models, we choose recent works like R1-VL-7B [ 77], MM-Eureka [ 46]\nand Open-R1-Multimodal [ 13]. As shown in Table 1, our model achieves the highest overall accuracy\non the MathVista mathematical subsets, outperforming both open-source reasoning models and\ngeneral models, and surpassing the best-performing open-source MLLM by +1.11% as well as\nclosed-source models, demonstrating strong capabilities in mathematical reasoning. On geometry\nreasoning, geometry problem solving and algebraic reasoning, MINT-CoT-7B outperforms state-\nof-the-art models by +3.31%, +1.12%, and +2.4%, respectively. However, for textbook question\nanswering, our performance is slightly below MM-Eureka. On the GeoQA benchmark, as shown in\nTable 2, our model outperforms the state-of-the-art models by +5.72%. In Table 3, MINT-CoT-7B\nalso outperforms the state-of-the-art by +1.2% on MMStar-Math, further demonstrating its capability\nin geometry reasoning.\n4.3 Ablation Study\nTraining Stage Ablation. We conduct an ablation study on the different training stages of MINT-\nCoT, as described in Section 3.3. The results on different benchmarks are presented in Table 4. The\nText-only CoT SFT stage improves performance by +21.2% on MMStar-Math, +21.22% on GeoQA,\nand +22.96% on MathVista-Math, as it helps the model learn the general reasoning format illustrated\nin the left column of Figure 3. The Interleaved CoT SFT stage further boosts performance by +0.4%\non MMStar-Math, +3.05% on GeoQA, and +3.71% on MathVista-Math across all primary tasks by\nenabling the model to interleave visual tokens into textual reasoning steps. Finally, the Interleaved\nCoT RL stage enhances performance by an additional +1.6% on MMStar-Math, +2.65% on GeoQA,\nand +5.92% on MathVista-Math through reinforcement learning, which enables the model to reason\nmore effectively with interleaved tokens.\nInterleaving Method Ablation. We conduct an ablation study on the interleaving method used in\nthe Interleaved CoT SFT stage, with the results presented in Table 5. Starting with the model trained in\nthe Text-only CoT SFT stage, we simply interleave the original image into each reasoning step without\nthe use of projectors or the Interleave token structure, which we refer to as “Original Image CoT\nSFT”. We find that, on MathVista-Math, the performance of Original Image CoT SFT significantly\ndecreases compared to Text-only CoT SFT. On the GeoQA benchmark, it also underperforms our\nInterleaved CoT SFT. This decline is likely due to the interleaving of excessive unrelated visual\ntokens during reasoning. Furthermore, we train a model that uses the Interleave token to select a\n9\n--- Page 10 ---\nQwen2vl-7B-Instruct Given triangle ABC where AB = AC and angle BAC measures 100.0 degrees, and the \nperpendicular bisector of AB intersects AB and BC at points D and E, what is the measure of angle \nBAE? Represent the measure of angle BAE as θ.  Choices:  A: 80°  B: 60°  C: 50°  D: 40°Question:/data4tb/xinyan/data/mulberry_interleave/mulberry_images/geoqa_plus/213.png<image>Provide a rationale to analyze the question.\\nNext, generate a step-by-step \nreasoning process to solve the problem. Ensure the steps are logical and \nconcise.\\nFinally, provide a concise summary of the final answer in the following format: \n'The final answer is: xxx.\\n\\nFormat your response with the following sections, \nseparated by ###:\\n### Rationales:\\n### Let's think step by step.\\n### Step 1:\\n### \nStep 2:\\n...\\n### The final answer is: \\n\\nQuestion: Given triangle ABC where AB = AC \nand angle BAC measures 100.0 degrees, and the perpendicular bisector of AB intersects \nAB and BC at points D and E, what is the measure of angle BAE? Represent the measure \nof angle BAE as θ.\\nChoices:\\nA: 80°\\nB: 60°\\nC: 50°\\nD: 40°\nMINT-CoT-7B\n### Rationales: The problem involves a geometry question about angles in an isosceles triangle with certain line \nintersections. To find angle BAE, consider properties of isosceles triangles and angle bisectors.\n### Let's think step by step.\n### Step 1:                        Since triangle ABC is isosceles with AB = AC, angles ABC and ACB are equal.\n### Step 2: \n### Step 3:\n### Step 4: \n### The final answer is: D\nCalculate angle ABC and ACB. The sum of angles in triangle ABC is 180 degrees. Therefore, \nangle ABC = angle ACB = (180 - angle BAC) / 2 = (180 - 100) / 2 = 40 degrees. \nThe perpendicular bisector of AB implies that point E is equidistant from A and B, making BE = \nAE. The bisector makes angle BAE an angle in triangle BAE.\nApply angular considerations: Consider that both AE and BE are equal, and with angle BAC = 100 \ndegrees and angle ABC = 40 degrees, angle BAE will share properties of an isosceles triangle and \nangle relationships.\nCosine Similarity \nScoresTo solve this problem, we first need to visualize the triangle and the given information:\\n\\n1. Triangle ABC is isosceles \nwith AB = AC and angle BAC = 100.0 degrees.\\n2. The perpendicular bisector of AB intersects AB at point D and intersects \nBC at point E.\\n\\nSince AB = AC, triangle ABC is isosceles as well. The perpendicular bisector of AB implies that AE is the \nmedian to the hypotenuse of triangle ABC (because D is the midpoint of AB). \\n\\nAlso, in an isosceles triangle, if a median \nis drawn to the hypotenuse, it creates two right-angled triangles. Let's denote the measure of angle BAE as θ. Due to the \nproperties of the isosceles triangle and the perpendicular bisector, angle BAE (θ) should be half of the apex angle \nBAC.\\n\\nSince ∠BAC = 100.0 degrees, θ = 100.0 / 2 = 50.0 degrees.\\n\\nTherefore, the correct answer is C.×\n✓\nFigure 5: Qualitative results of Qwen2-VL-7B-Instruct and MINT-CoT-7B. MINT-CoT-7B demon-\nstrates improved CoT reasoning capability by interleaving fine-grained visual tokens. There is also a\nvisualization of the similarity scores for the Interleaved Token generated during Step 4.\nrectangular region of visual tokens at each reasoning step, referred to as “Bounding Box CoT SFT”.\nAs shown in the table, this approach underperforms our Interleaved CoT SFT on both benchmarks,\nexcept for the TQA task, and even underperforms the Text-only CoT SFT on GEO and GPS tasks\nin MathVista-Math. These results demonstrate the effectiveness of our token selection method for\nmathematical reasoning tasks.\n4.4 Qualitative Results\nWe present the qualitative results of the baseline model Qwen2-VL-7B-Instruct and our proposed\nmodel MINT-CoT-7B, as shown in Figure 5. Compared to the baseline, MINT-CoT-7B demonstrates\na more coherent reasoning format and is capable of selecting and interleaving relevant visual tokens\nduring training. More qualitative results of our model are shown in Appendix A.6. Moreover, we\nprovide a plot of the average F1 score between the selected visual tokens and ground truth visual\ntokens in each reasoning step during the Interleaved CoT SFT stage, as shown in Figure 4. For the\nInterleaved CoT RL stage, we do not report an F1 score plot due to the absence of ground truth visual\ntoken indices for online inference. As shown in the plot, the F1 score exhibits a fluctuating upward\ntrend during training, demonstrating that the accuracy of visual token selection is increasing during\nthe Interleaved CoT SFT training strategy.\n5 Conclusion\nIn this paper, we first propose MINT-CoT, a method for enhancing multimodal mathematical reasoning\nby interleaving fine-grained visual tokens into CoT. We use the novel Interleave Token to automatically\nselect visual tokens for each reasoning step. Then, we introduce the MINT-CoT dataset and a four-step\ndataset generation pipeline. Finally, we present the MINT-CoT training strategy, which includes\nText-only CoT Training, Interleaved CoT SFT and Interleaved CoT RL, enhancing the MLLMs’\nability to reason over interleaved visual tokens. Our experiments with the obtained MINT-CoT-7B\nmodel demonstrate significant improvements across various benchmarks.\n10\n--- Page 11 ---\nReferences\n[1] Sonnet Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet.\n[2]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile\nabilities. ArXiv , abs/2308.12966, 2023.\n[3]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang\nWan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen\nCheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report,\n2025.\n[4] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali\nWang, Yu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models.\narXiv preprint arXiv:2305.13292 , 2023.\n[5]Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin.\nGeoqa: A geometric question answering benchmark towards multimodal numerical reasoning.\nArXiv , abs/2105.14517, 2021.\n[6]Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generaliza-\ntion ability in vision-language models with less than $3. https://github.com/Deep-Agent/\nR1-V , 2025. Accessed: 2025-02-02.\n[7]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi\nWang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language\nmodels? arXiv preprint arXiv:2403.20330 , 2024.\n[8]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 24185–24198, 2024.\n[9]Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan\nLyu, Ziyang Zhang, Gang Zhang, Errui Ding, et al. R-cot: Reverse chain-of-thought\nproblem generation for geometric reasoning in large multimodal models. arXiv preprint\narXiv:2410.17885 , 2024.\n[10] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Open-\nvlthinker: An early exploration to complex vision-language reasoning via iterative self-\nimprovement, 2025.\n[11] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and\nZiwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language\nmodels. arXiv preprint arXiv:2411.14432 , 2024.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale, 2021.\n[13] EvolvingLMMs-Lab. open-r1-multimodal: A fork to add multimodal model training to open-\nr1.https://github.com/EvolvingLMMs-Lab/open-r1-multimodal , 2025. Accessed:\n2025-05-13.\n[14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive\nevaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075 ,\n2024.\n11\n--- Page 12 ---\n[15] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\nlarge language model. arXiv preprint arXiv:2312.11370 , 2023.\n[16] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. Interleaved-modal chain-of-thought, 2025.\n[17] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient\nvisual instruction model. arXiv preprint arXiv:2304.15010 , 2023.\n[18] Google Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 , 2023.\n[19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n[20] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\narXiv:2505.07062 , 2025.\n[21] Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui\nZhang, Chun-Mei Feng, and Zhen Li. Pisa: A self-augmented data engine and training strategy\nfor 3d understanding with large models. arXiv preprint arXiv:2503.10529 , 2025.\n[22] Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, and Pheng-Ann\nHeng. Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on\nmulti-modal scientific problems. arXiv preprint arXiv:2503.10627 , 2025.\n[23] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and\nPheng-Ann Heng. Can we generate images with cot? let’s verify and reinforce image generation\nstep by step. arXiv preprint arXiv:2501.13926 , 2025.\n[24] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin\nChen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud\nwith multi-modality for 3d understanding, generation, and instruction following. arXiv preprint\narXiv:2309.00615 , 2023.\n[25] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluat-\ning real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326 ,\n2025.\n[26] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith,\nand Ranjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal\nlanguage models. arXiv preprint arXiv:2406.09403 , 2024.\n[27] Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen,\nChang Yao, and Jie Song. Boosting mllm reasoning with text-debiased hint-grpo, 2025.\n[28] Zihan Huang, Tao Wu, Wang Lin, Shengyu Zhang, Jingyuan Chen, and Fei Wu. Autogeo:\nAutomating geometric image dataset creation for enhanced geometry understanding. arXiv\npreprint arXiv:2409.09039 , 2024.\n[29] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-\nAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative\nsemantic-level and token-level cot. arXiv preprint arXiv:2505.00703 , 2025.\n[30] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan\nJin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot:\nBenchmarking chain-of-thought in large multimodal models for reasoning quality, robustness,\nand efficiency, 2025.\n[31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui\nChen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large\nmodels as multi-modal search engines. arXiv preprint arXiv:2409.12959 , 2024.\n12\n--- Page 13 ---\n[32] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems ,\n35:22199–22213, 2022.\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\nLi, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326 , 2024.\n[34] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326 , 2024.\n[35] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli ´c,\nand Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv\npreprint arXiv:2501.07542 , 2025.\n[36] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli ´c, and\nFuru Wei. Imagine while reasoning in space: Multimodal visualization-of-thought, 2025.\n[37] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du,\nLingfeng Zhu, Baohua Lai, Xiaoguang Hu, Dianhai Yu, and Yanjun Ma. Pp-ocrv3: More\nattempts for the improvement of ultra lightweight ocr system, 2022.\n[38] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,\nand Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 ,\n2023.\n[39] Pengxiang Li, Shilin Yan, Joey Tsai, Renrui Zhang, Ruichuan An, Ziyu Guo, and Xiaowei\nGao. Adaptive classifier-free guidance via dynamic low-confidence masking. arXiv preprint\narXiv:2505.20199 , 2025.\n[40] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin,\nWenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual\nembeddings for multi-modal large language models. ECCV 2024 , 2023.\n[41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In\nNeurIPS , 2023.\n[42] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive\nreasoning improves large vision-language models. arXiv preprint arXiv:2403.12966 , 2024.\n[43] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao\nCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical\nreasoning of foundation models in visual contexts. In International Conference on Learning\nRepresentations (ICLR) , 2024.\n[44] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-chun\nZhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic\nreasoning. In Annual Meeting of the Association for Computational Linguistics , pages 6774–\n6786, 2021.\n[45] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and\nYujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal\nmathematics. arXiv preprint arXiv:2501.04686 , 2025.\n[46] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian\nShi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment\nwith rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365 , 2025.\n[47] Fanxu Meng, Haotong Yang, Yiding Wang, and Muhan Zhang. Chain of images for intuitively\nreasoning. arXiv preprint arXiv:2311.09241 , 2023.\n13\n--- Page 14 ---\n[48] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh,\nAidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛ adry, Alex\nBaker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex\nNichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis,\nAlexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew\nBraunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tul-\nloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz\nGhorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth\nHoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap,\nBrandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman,\nCamillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson,\nChak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea V oss, Chen Ding, Cheng\nLu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina\nKim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel\nKappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson,\nDavid Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen,\nDuncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang,\nEric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan\nMays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann,\nFreddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi\nSalman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather\nWhitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Hui-\nwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian\nSilber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitschei-\nder, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker,\nJames Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang,\nJason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee,\nJessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero\nCandela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan\nLachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga,\nJordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao,\nJoyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi,\nKavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg,\nKevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry\nKai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus,\nLiang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang,\nLouis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric\nDoshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan,\nMark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin,\nMatthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz,\nMeng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe,\nMichael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro,\nMiguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan,\nMira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie\nCone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder,\nNick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah\nDeutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk,\nOliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul\nMcMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter\nHoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming\nYuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes,\nRaul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob\nDonnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory\nCarmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi\nJain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara\nCulver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu\n14\n--- Page 15 ---\nJain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene,\nSpencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal\nBroda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan,\nThomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell,\nTianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi,\nTomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit\nMoeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda\nZhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim,\nYoulong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury\nMalkov. Gpt-4o system card, 2024.\n[49] OpenAI. Chatgpt. https://chat.openai.com , 2023.\n[50] OpenAI. GPT-4V(ision) system card, 2023.\n[51] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/ , 2024.\n[52] OpenAI. Introducing openai o1, 2024., 2024.\n[53] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multi-\nmath: Bridging visual and mathematical reasoning for large language models. arXiv preprint\narXiv:2409.00147 , 2024.\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning ,\npages 8748–8763. PmLR, 2021.\n[55] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu,\nand Hongsheng Li. Visual cot: Advancing multi-modal language models with a comprehen-\nsive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information\nProcessing Systems , 37:8612–8642, 2024.\n[56] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y . K. Li, Y . Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models, 2024.\n[57] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and\nRoy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large\nlanguage models. arXiv preprint arXiv:2406.17294 , 2024.\n[58] Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang\nWang, Dong Wang, Xuelong Li, et al. Exploring the potential of encoder-free architectures in\n3d lmms. arXiv preprint arXiv:2502.09620 , 2025.\n[59] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint\narXiv:2405.09818 , 2024.\n[60] Qwen Team. Qvq-72b-preview. https://huggingface.co/Qwen/QVQ-72B-Preview ,\n2025. Accessed: 2025-05-13.\n[61] Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hong-\nsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on dpo\nvs. grpo. arXiv preprint arXiv:2505.17017 , 2025.\n[62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[63] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing\nLiu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception\nof the world at any resolution. arXiv preprint arXiv:2409.12191 , 2024.\n15\n--- Page 16 ---\n[64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing\nLiu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men,\nDayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-\nlanguage model’s perception of the world at any resolution, 2024.\n[65] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu,\nXizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large\nlanguage models via mixed preference optimization. arXiv preprint arXiv:2411.10442 , 2024.\n[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems , 35:24824–24837, 2022.\n[67] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei.\nMind’s eye of llms: Visualization-of-thought elicits spatial reasoning in large language models.\nInThe Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024.\n[68] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao,\nYiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng\nSun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu,\nHaowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts\nvision-language models for advanced multimodal understanding, 2024.\n[69] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin.\nPointllm: Empowering large language models to understand point clouds. arXiv preprint\narXiv:2308.16911 , 2023.\n[70] Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, and\nRay Zhang. Crosslmm: Decoupling long video sequences from lmms via dual cross-attention\nmechanisms. arXiv preprint arXiv:2505.17020 , 2025.\n[71] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong\nTang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou,\nJinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li,\nMingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong\nDeng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan,\nYang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and\nZhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n[72] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu,\nDacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized\nmultimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615 ,\n2025.\n[73] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie\nWang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like\nreasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319 ,\n2024.\n[74] Runpeng Yu, Xinyin Ma, and Xinchao Wang. Introducing visual perception token into multi-\nmodal large language model. arXiv preprint arXiv:2502.17425 , 2025.\n[75] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 9556–9567, 2024.\n[76] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for\nlanguage image pre-training, 2023.\n16\n--- Page 17 ---\n[77] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng\nTao. R1-vl: Learning to reason with multimodal large language models via step-wise group\nrelative policy optimization. arXiv preprint arXiv:2503.12937 , 2025.\n[78] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng\nTao. R1-vl: Learning to reason with multimodal large language models via step-wise group\nrelative policy optimization, 2025.\n[79] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and\nPeng Gao. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized\nattention. In ICLR 2024 , 2024.\n[80] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun\nZhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see\nthe diagrams in visual math problems? In European Conference on Computer Vision , pages\n169–186. Springer, 2024.\n[81] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo\nTong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Chunyuan Li, and\nHongsheng Li. Mavis: Mathematical visual instruction tuning with an automatic data engine,\n2024.\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul-\ntimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 ,\n2023.\n[83] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct\nchain-of-thought prompting for multimodal reasoning in language models. Advances in Neural\nInformation Processing Systems , 36:5168–5191, 2023.\n[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 , 2023.\n[85] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed\nElhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization\nfor text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080 ,\n2025.\n[86] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng\nLi, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint\narXiv:2404.13046 , 2024.\n17\n--- Page 18 ---\nA Appendix\nA.1 Overview\nWe organize our supplementary material as follows.\n• Dataset Details\n–Dataset Example\n–Dataset Statistic\n• Theoretical Details of Interleaved CoT RL\n• Additional Implementation Details\n• Additional Ablation Study\n–Projector Ablation\n• Additional Qualitative Results\nA.2 Dataset Details\nDataset Example We present examples from our MINT-CoT Dataset in Figures 6 to 8, where the\nyellow highlights indicate the interleaved grid indices, and the blue highlights denote the key words\nin each reasoning step.\nDataset Statistic We provide the key statistics of MINT-CoT Dataset in Table 6. This dataset\ncomprises 54,031 data points derived from the mathematical portion of the Mulberry-260k dataset.\nTable 6: Key statistics of the MINT-CoT dataset.\nStatistic Value\nTotal data points 54,031\nData points containing Interleave Tokens (interleaved data points) 52,142\nAverage number of Interleave Tokens per interleaved data point 2.80\nMaximum number of Interleave Tokens in a single interleaved data point 12\nAverage number of selected indices per interleaved data point 19.91\nAverage number of selected indices per Interleave Token 7.10\nMinimum number of selected indices in a single Interleave Token 1\nMaximum number of selected indices in a single Interleave Token 140\nA.3 Theoretical Details of Interleaved CoT RL\nFollowing the standard GRPO framework [ 56], we integrate GRPO into our approach. Specifically,\nsimilar to LCEin Stage 2, we apply a policy loss LGRPO_text to textual tokens:\nLGRPO_text =−E{Yj}G\nj=1∼Pθold(·|I,T)\"\n1\nGPG\nj=11\n|Tj|P\nt∈Tj(\nPθ(yj,t|yj,<t,I,T )\nPθold(yj,t|yj,<t,I,T )·ˆAj,t−βD KL[Pθ∥Pref])#\n,(10)\nwhere ˆAj,tis the advantage detailed in Section 2.3, Prefis a reference policy that serves as a\nregularization target, and DKL[Pθ∥Pref]penalizes deviation from this reference distribution to\nencourage stable updates. The min and clip operations are omitted for brevity.\nTo enable more flexible and effective selection of visual tokens, we further apply a LGRPO_vis to\nthe scaled similarity scores α(i)\nj,τ, which are derived from the interactions between Interleave tokens\nand input visual tokens in the the j-th chain of reasoning steps. Let Njdenote the the number of\nreasoning steps in j-th chain, and M(i)\njdenote the number of visual tokens interleaved in the i-th\nreasoning step in the j-th chain. Formally, the loss is defined as:\nLGRPO_vis =−E{Yj}G\nj=1∼Pθold(·|I,T)\u0014\n1\nGPG\nj=11\nNjPNj\ni=11\nM(i)\njPM(i)\nj\nτ=1\u001a\nPθ(α(i)\nj,τ|yj,<τ,I,T )\nPθold(α(i)\nj,τ|yj,<τ,I,T )·ˆAj−βD KL[Pθ∥Pref]\u001b\u0015\n.(11)\n18\n--- Page 19 ---\nTable 7: Ablation study on the post interleave projector and the post visual projector. We\ncompare three configurations: without projectors, with single-layer linear projections, and with\ntwo-layer MLPs.\nConfiguration Layer Number All GEO ALG GPS TQA\nw.o. projectors – 64.44 63.68 64.42 63.94 66.13\nw. projectors1 67.78 66.51 67.79 67.31 69.35\n2 65.18 63.21 65.54 63.94 69.35\nThe final policy loss is defined as the sum of both losses, with the LGRPO_vis rescaled by a weighting\nfactor λ:\nLGRPO =LGRPO_text +λ· LGRPO_vis . (12)\nBy computing this combined loss, we enhance both token selection and inference capabilities using\nInterleave tokens.\nA.4 Additional Implementation Details\nWe use Qwen2-VL-7B [ 64] as the base MLLM model in our experiments. Each of the two projectors,\nPinterleave andPvis, is implemented as a single linear layer. We uniformly set the threshold θ= 0.7to\nfilter the similarity scores. The hyper-parameter γto scale the similarity is set to 1/0.07following\nCLIP [ 54]. The training procedure consists of three stages: (1) Text-only CoT Training , where we train\nfor 2 epochs on the MINT-CoT dataset without applying the interleaving strategy, using a learning\nrate of 5.0e-6 and a batch size of 64, following the configuration of Mulberry [ 73];(2) Interleaved\nCoT SFT , where we train for 3 epochs on the MINT-CoT dataset with a learning rate of 1e-6 and a\nbatch size of 64; and (3) Interleaved CoT RL , where we train for 700 steps on the MINT-CoT dataset,\nusing a group size G= 4, a weighting factor λ= 0.02, a learning rate of 1e-6 and a batch size of\n16. During training, all model parameters, including the Interleave Token and projector layers, are\nunfrozen, except for the vision encoder, which remains fixed. Finally, the resulting model is named\nMINT-CoT-7B.\nFor Bounding Box CoT SFT, we use the MINT-COT dataset and extract the minimal enclosing\nrectangle that covers the index positions of all labels as the ground truth bounding box to train the\nmodel. We train 2 epochs with a learning rate of 1e-6 and a batch size of 64. And during inference, it\ninterleave the minimal enclosing rectangle that covers all the seleted tokens. For Original Image CoT\nSFT, however, we enforce the concatenation of the entire image at the beginning of each step during\nboth training and inference. We train only 1 epoch with a learning rate of 1e-6 and a batch size of 64,\nA.5 Additional Ablation Study\nProjector Ablation We conduct an ablation study on the post interleave projector Ppost_intlv and\nthe post visual projector Ppost_vis on the Interleaved CoT SFT stage. Both projectors were initially\nimplemented as single-layer linear layers. We first remove both projectors entirely, and then replace\nthem with two-layer MLPs using GELU activation. Both configurations are trained for three epochs.\nThe results on the mathematical subset of MathVista are shown in Table 7, in which we find that the\ninitial configuration as single-layer linear layers performs the best over all primary tasks.\nA.6 Additional Qualitative Results\nIn addition to Section 3.4, we provide more qualitative results of the baseline model Qwen2-VL-7B-\nInstruct and our proposed model MINT-CoT-7B in Figures 9 to 11.\n19\n--- Page 20 ---\n### Rationales: The problem involves calculating the hypotenuse (slope AC) of a right triangle where one side (the \nheight AB) is given, and the ratio of the two legs (AB:BC) is provided. Using these details, the Pythagorean theorem \nwill help find the length of the hypotenuse.\n### Let's think step by step.\n### Step 1: <interleave>9,16,23,30<interleave>Identify the values and ratios given. The height of the bridge (AB) is 6 \nunits, with a slope ratio of 3:4, indicating AB:BC.\n### Step 2:<interleave>2,9,16,23,26,30,33<interleave>Assign the given ratio to the triangle‘s sides: AB corresponds to \n3 parts, and BC corresponds to 4 parts.\n### Step 3:<interleave>23,26,30,33<interleave>Calculate the length of BC. Since AB is 6 units (which corresponds to \n3 parts), then each part is 2 units. BC, having 4 parts, will be 4 * 2 = 8 units.\n### Step 4:Use the Pythagorean theorem, \\\\( AC = \\\\sqrt{AB^2 + BC^2} \\\\).\n### Step 5:Substitute the values into the equation: \\\\( AC = \\\\sqrt{6^2 + 8^2} = \\\\sqrt{36 + 64} = \\\\sqrt{100} = 10 \n\\\\).\n### The final answer is: AOutputs:The Municipal Planning Bureau is designing a cross-street bridge with a height of \n'z' units, where z = 6.0. If the slope of the overpass AC follows a 3:4 ratio, what is \nthe length of the slope AC?\\nChoices:\\nA: 10m\\nB: 8m\\nC: 6m\\nD: 6√3mQuestion:\nInput Image: Grid Image:\n### Rationales: The question asks for the measure of the altitude drawn to the hypotenuse of a right triangle. Given \nthe lengths of segments \\\\( MP \\\\) and \\\\( PO \\\\), we can use properties of right triangles involving altitudes.\n### Let's think step by step.\n### Step 1: <interleave>22,23,25,29,30,32<interleave>Identify the known segments: \\\\( MP = 11 \\\\), \\\\( PO = 27 \n\\\\).\n### Step 2:<interleave>21,22,27,28,29,34<interleave>The hypotenuse \\\\( MO = MP + PO = 11 + 27 = 38 \\\\).\n### Step 3:Use the geometric mean theorem (altitude-on-hypotenuse theorem), which states that the altitude \nsquared \\\\( NP^2 \\\\) is equal to the product of the segments it divides the hypotenuse into: \\\\( NP^2 = MP \\\\times \nPO \\\\).\n### Step 4:Calculate the product: \\\\( NP^2 = 11 \\\\times 27 = 297 \\\\).\n### Step 5:Find \\\\( NP \\\\) by taking the square root: \\\\( NP = \\\\sqrt{297} \\\\).\n### The final answer is: BOutputs:Question: Find the measure of the altitude drawn to the hypotenuse. \nChoices:  A: 11  B: \\\\sqrt{297}  C: 27  D: 297Question:Input Image: Grid Image:\nFigure 6: An example from MINT-CoT dataset.\n### Rationales: The problem involves calculating the hypotenuse (slope AC) of a right triangle where one side (the \nheight AB) is given, and the ratio of the two legs (AB:BC) is provided. Using these details, the Pythagorean theorem \nwill help find the length of the hypotenuse.\n### Let's think step by step.\n### Step 1: <interleave>9,16,23,30<interleave>Identify the values and ratios given. The height of the bridge (AB) is 6 \nunits, with a slope ratio of 3:4, indicating AB:BC.\n### Step 2:<interleave>2,9,16,23,26,30,33<interleave>Assign the given ratio to the triangle‘s sides: AB corresponds to \n3 parts, and BC corresponds to 4 parts.\n### Step 3:<interleave>23,26,30,33<interleave>Calculate the length of BC. Since AB is 6 units (which corresponds to \n3 parts), then each part is 2 units. BC, having 4 parts, will be 4 * 2 = 8 units.\n### Step 4:Use the Pythagorean theorem, \\\\( AC = \\\\sqrt{AB^2 + BC^2} \\\\).\n### Step 5:Substitute the values into the equation: \\\\( AC = \\\\sqrt{6^2 + 8^2} = \\\\sqrt{36 + 64} = \\\\sqrt{100} = 10 \n\\\\).\n### The final answer is: AOutputs:The Municipal Planning Bureau is designing a cross-street bridge with a height of \n'z' units, where z = 6.0. If the slope of the overpass AC follows a 3:4 ratio, what is \nthe length of the slope AC?\\nChoices:\\nA: 10m\\nB: 8m\\nC: 6m\\nD: 6√3mQuestion:\nInput Image: Grid Image:\n### Rationales: The question asks for the measure of the altitude drawn to the hypotenuse of a right triangle. Given \nthe lengths of segments \\\\( MP \\\\) and \\\\( PO \\\\), we can use properties of right triangles involving altitudes.\n### Let's think step by step.\n### Step 1: <interleave>22,23,25,29,30,32<interleave>Identify the known segments: \\\\( MP = 11 \\\\), \\\\( PO = 27 \n\\\\).\n### Step 2:<interleave>21,22,27,28,29,34<interleave>The hypotenuse \\\\( MO = MP + PO = 11 + 27 = 38 \\\\).\n### Step 3:Use the geometric mean theorem (altitude-on-hypotenuse theorem), which states that the altitude \nsquared \\\\( NP^2 \\\\) is equal to the product of the segments it divides the hypotenuse into: \\\\( NP^2 = MP \\\\times \nPO \\\\).\n### Step 4:Calculate the product: \\\\( NP^2 = 11 \\\\times 27 = 297 \\\\).\n### Step 5:Find \\\\( NP \\\\) by taking the square root: \\\\( NP = \\\\sqrt{297} \\\\).\n### The final answer is: BOutputs:Find the measure of the altitude drawn to the hypotenuse. \nChoices:  A: 11  B: \\\\sqrt{297}  C: 27  D: 297Question:Input Image: Grid Image:\nFigure 7: An example from MINT-CoT dataset.\n### Rationales: To solve this problem, we need to determine angle A in the right triangle ABC with a known 90-degree \nangle at C and a 30-degree angle at B.\n### Let's think step by step.\n### Step 1:<interleave>0,1,30,31,34<interleave>Recognize that triangle ABC is a right triangle with angle ACB \nmeasuring 90 degrees.\n### Step 2:<interleave>31,34<interleave>Identify that angle CBO, as part of triangle OBC, measures 30 degrees.\n### Step 3:Use the fact that the sum of angles in triangle ABC is 180 degrees.\n### Step 4:<interleave>0,1,34<interleave>Calculate angle ABC. Since angle CBO is 30 degrees and angle ACB is 90 \ndegrees, angle ABC = 90 - 30 = 60 degrees.\n### Step 5:<interleave>0,1,30,31,34<interleave>Find angle A using the angle sum property of a triangle:\\nAngle A + \nAngle ABC + Angle ACB = 180 degrees\\nAngle A + 60 + 90 = 180\\nAngle A = 180 - 150 = 30 degrees\n### The final answer is: BOutputs:Determine the degree measure of angle A in the figure, where circle O is inscribed in right \ntriangle ABC. Angle ACB measures 90 degrees and angle CBO measures 30 degrees.\nChoices:  A: 15°  B: 30°  C: 45°  D: 60°Question:Input Image:Grid Image:\nFigure 8: An example from MINT-CoT dataset.\n20\n--- Page 21 ---\nQwen2vl-7B-Instruct As shown in the figure, in diamond ABCD, the diagonals AC and BD intersect at O. It is known \nthat BD = 6 and AC = 8. The perimeter of diamond ABCD is ().\nChoices: A: 40  B: 20  C: 10  D: 5Question:\nMINT-CoT-7B\n### Rationales:\nTo find the perimeter of the rhombus, the length of all sides needs to be determined. Since the diagonals bisect each \nother at right angles in a rhombus, the sides can be found using the Pythagorean theorem.\\n\\n### Let's think step by \nstep.\n### Step 1:         \n### Step 2:\n### Step 3: Calculate the perimeter of the rhombus. Since all sides are equal in a rhombus, the perimeter is 4 times the \nside length.\\n\\nPerimeter \\\\( = 4 \\\\times 5 = 20 \\\\)\n### The final answer is: BCosine Similarity \nScoresThe quadrilateral ABCD is a rhombus since its diagonals are perpendicular. \\n\\n[asy]\\npair A,B,C,D,O;\\n\\nA = (0,4);\\nB = \n(6,0);\\nC = (-3*sqrt(3), 3);\\nD = (0, 4);\\nO = (0, 0);\\ndraw(A--B--C--D--cycle, black+linewidth(1));\\ndraw(A--C, \nblack+linewidth(1));\\ndraw(B--D, black+linewidth(1));\\ndraw(O--A, black+linewidth(1));\\ndraw(O--B, \nblack+linewidth(1));\\ndraw(O--C, black+linewidth(1));\\ndraw(O--D, black+linewidth(1));\\nlabel(\"$A$\", A, N);\\nlabel(\"$B$\", \nB, E);\\nlabel(\"$C$\", C, dir(315));\\nlabel(\"$D$\", D, N);\\nlabel(\"$O$\", O, S);\\n[/asy]\\n\\nLet\\'s label the intersection point \nof the diagonals O. \\n\\nSince OA = OC = 4 and OB = OD = 3, we have OB = 3 and BD = 6.\\n\\nSo, the perimeter of the \nrhombus ABCD is 4OA + 4OB = 4 * 4 + 4 * 3 = 16 + 12 = 28.\\n\\nTherefore, the answer is 28. \\n\\nSince the question asks \nfor the perimeter of the diamond ABCD, the answer is 20. \\n\\nTherefore, the answer is B. ×\n✓\nIdentify the properties of the rhombus. In a rhombus, the diagonals bisect \neach other at right angles. Therefore, AO = OC = AC/2 = 8/2 = 4 and BO = OD \n= BD/2 = 6/2 = 3.\nUse the Pythagorean theorem to calculate the side length (AD) of the rhombus. \nThe hypotenuse is AD, with AO = 4 and BO = 3.\\n\\n\\\\[\\nAD = \\\\sqrt{AO^2 + BO^2} \n= \\\\sqrt{4^2 + 3^2} = \\\\sqrt{16 + 9} = \\\\sqrt{25} = 5\\n\\\\]\nCosine Similarity \nScoresFigure 9: Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.\nQwen2vl-7B-Instruct As shown in the figure, the radius of circle O is 4.0, BC is the diameter, AC is the tangent of circle \nO, and AC = 6.0, then AB = () \nChoices:  A: 4  B: 6  C: 10  D: 12Question:\nMINT-CoT-7B\n### Rationales: To determine AB, understand the properties of the circle and the right-angled triangle formed by the \ntangent AC and the diameter BC. Use the Pythagorean theorem in triangle ABC.\n### Let's think step by step.\n### Step 1:                                          \n### Step 2: \n### Step 3: \n### Step 4: Apply the Pythagorean theorem in triangle ABC: \\\\( AB^2 = AC^2 + BC^2 \\\\).\n### Step 5: Substitute the known values into the Pythagorean theorem: \\n\\\\( AB^2 = 6^2 + 8^2 \\\\). \n### Step 6: Calculate the squares: \\n\\\\( 6^2 = 36 \\\\) and \\\\( 8^2 = 64 \\\\).\n### Step 7: Sum the squares: \\n\\\\( AB^2 = 36 + 64 = 100 \\\\). \n### Step 8: Take the square root to find AB: \\n\\\\( AB = \\\\sqrt{100} = 10 \\\\).\n### The final answer is: CBecause AC is tangent to circle O, it is perpendicular to OC. By defining the length of AC as x, we can solve for segment \nAB using the Pythagorean theorem in a right triangle:\\n\\n1. Start by understanding the Pythagorean theorem.\\n2. Apply \nthe Pythagorean theorem to a right triangle with segments AC and AB as the legs and segment BC as the \nhypotenuse.\\n\\nGiven that AO is the radius (4.0), we use the Pythagorean theorem in triangle OAB:\\n\\nAB = √(OA² + \nOB²) = √(4.0² + 4.0²) \\n\\nAB = √(16 + 16) \\nAB = √32 \\nAB ≈ 6.4\\n\\nSo the answer is approximate 6.4. It seems there is \nno exact option with this value, but the closest one would be C) 6. However, given the calculation, the length AB should be \na bit more than the radius of circle O.\\n\\nTherefore, the answer is C:6.×\n✓\nCosine Similarity \nScores\nIdentify the known lengths: radius of the circle (OC) is 4, and AC, the \ntangent, is 6.\nRecognize that BC is a diameter of the circle. Since the radius is 4, BC \n(being the diameter) is twice the radius: BC = 8.\nSince AC is a tangent and BC is a diameter, consider triangle ABC. Triangle \nABC is a right triangle with AC perpendicular to BC.\nCosine Similarity \nScoresCosine Similarity \nScores\nFigure 10: Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.\n21\n--- Page 22 ---\nQwen2vl-7B-Instruct In ABCD, given that AB < BC, the perpendicular bisector of diagonal AC intersects AD at point E. \nConnect CE. If the perimeter of ABCD is 200cm, what is the perimeter of triangle CDE? \nChoices:  A: 200.0  B: 400.0  C: 150.0  D: 100.0Question:\nMINT-CoT-7B\n### Rationales: The problem involves understanding the geometric placement of point E and the relations between various \nsides and diagonals in the quadrilateral. It requires determining the perimeter of the triangle CDE given the perimeter of \nABCD.\n### Let's think step by step.\n### Step 1: Given the perimeter of ABCD is 200 cm, we need to explore the relationships among the sides of the \nquadrilateral and the triangle formed by extending certain segments.\n### Step 2: \n### Step 3: Since AB < BC, consider potential equalities or relationships with the line segment created by the \nintersection of AD and the perpendicular bisector at E. \n### Step 4: Express the perimeter of triangle CDE: CD + DE + EC. Given EC = EA due to E being on the perpendicular \nbisector of AC, explore possible equalities or simplifications. \n### Step 5: Using the perimeter constraint and the properties outlined, determine the lengths of the sides of triangle \nCDE and thus its total perimeter.\n### The final answer is: DThe correct answer is D. \nBy symmetry, BC = 50 cm, AD = 50 cm, so AE = 25 cm. From the right triangle AEC, AC^2 - AE^2 = CE^2, so AC^2 - AE^2 \n= CE^2 = AD^2 - AE^2. Therefore, CE = 25 cm. Therefore, the perimeter of triangle CDE = 75 cm.×\n✓\nCosine Similarity \nScores\nThe perpendicular bisector of AC indicates a symmetrical property: \nEA = EC. The intersection at E means that E is equidistant from A \nand C.\nFigure 11: Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.\n22",
  "text_length": 86889
}