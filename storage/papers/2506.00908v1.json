{
  "id": "http://arxiv.org/abs/2506.00908v1",
  "title": "DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale\n  Generation",
  "summary": "Despite recent progress, most existing virtual try-on methods still struggle\nto simultaneously address two core challenges: accurately aligning the garment\nimage with the target human body, and preserving fine-grained garment textures\nand patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on\nframework that explicitly disentangles these objectives for more effective\nmodeling. DS-VTON consists of two stages: the first stage generates a\nlow-resolution try-on result to capture the semantic correspondence between\ngarment and body, where reduced detail facilitates robust structural alignment.\nThe second stage introduces a residual-guided diffusion process that\nreconstructs high-resolution outputs by refining the residual between the two\nscales, focusing on texture fidelity. In addition, our method adopts a fully\nmask-free generation paradigm, eliminating reliance on human parsing maps or\nsegmentation masks. By leveraging the semantic priors embedded in pretrained\ndiffusion models, this design more effectively preserves the person's\nappearance and geometric consistency. Extensive experiments demonstrate that\nDS-VTON achieves state-of-the-art performance in both structural alignment and\ntexture preservation across multiple standard virtual try-on benchmarks.",
  "authors": [
    "Xianbing Sun",
    "Yan Hong",
    "Jiahui Zhan",
    "Jun Lan",
    "Huijia Zhu",
    "Weiqiang Wang",
    "Liqing Zhang",
    "Jianfu Zhang"
  ],
  "published": "2025-06-01T08:52:57Z",
  "updated": "2025-06-01T08:52:57Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00908v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00908v1  [cs.CV]  1 Jun 2025DS-VTON: High-Quality Virtual Try-on via\nDisentangled Dual-Scale Generation\nXianbing Sun1, Yan Hong2, Jiahui Zhan1, Jun Lan2, Huijia Zhu2, Weiqiang Wang2\nLiqing Zhang1,Jianfu Zhang1‚àó\n1Shanghai Jiao Tong University\n2Ant Group\n{fufengsjtu, c.sis}@sjtu.edu.cn\nAbstract\nDespite recent progress, most existing virtual try-on methods still struggle to\nsimultaneously address two core challenges: accurately aligning the garment im-\nage with the target human body, and preserving fine-grained garment textures\nand patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on\nframework that explicitly disentangles these objectives for more effective model-\ning. DS-VTON consists of two stages: the first stage generates a low-resolution\ntry-on result to capture the semantic correspondence between garment and body,\nwhere reduced detail facilitates robust structural alignment. The second stage\nintroduces a residual-guided diffusion process that reconstructs high-resolution\noutputs by refining the residual between the two scales, focusing on texture fi-\ndelity. In addition, our method adopts a fully mask-free generation paradigm,\neliminating reliance on human parsing maps or segmentation masks. By leveraging\nthe semantic priors embedded in pretrained diffusion models, this design more\neffectively preserves the person‚Äôs appearance and geometric consistency. Extensive\nexperiments demonstrate that DS-VTON achieves state-of-the-art performance in\nboth structural alignment and texture preservation across multiple standard virtual\ntry-on benchmarks.\n1 Introduction\nGiven a garment image and a person image, the goal of virtual try-on is to synthesize a photorealistic\nimage of the person wearing the specified garment [Han et al., 2018]. As a key enabling technol-\nogy for online fashion and e-commerce, virtual try-on has attracted increasing attention in recent\nyears [Choi et al., 2021, Ge et al., 2021b, Gou et al., 2023, Lee et al., 2022, Morelli et al., 2022,\n2023, Choi et al., 2024, Chong et al., 2025, Zhou et al., 2025]. This task involves two fundamental\nchallenges: (1) accurately fitting the garment onto the human body, and (2) preserving fine-grained\ngarment textures. Existing methods fall into two main categories: Generative Adversarial Networks\n(GANs) [Goodfellow et al., 2020] and Diffusion Models [Ho et al., 2020, Rombach et al., 2022].\nEarly GAN-based approaches [Choi et al., 2021, Ge et al., 2021a, Lee et al., 2022, Xie et al., 2023]\ntypically follow a two-stage pipeline: a warping module first aligns the garment with the target pose,\nfollowed by a generation module to synthesize the final image. While warping helps preserve garment\nappearance, the subsequent generation stage often leads to detail loss due to imperfect feature fusion.\nRecent diffusion-based methods [Kim et al., 2024, Zhu et al., 2023, Xu et al., 2025, Choi et al.,\n2024, Zhou et al., 2025] address both structural alignment and texture preservation within a unified\ndenoising process. Early diffusion steps tend to capture global structure, while later steps refine\n‚àóCorresponding author.\nPreprint. Under review.\n--- Page 2 ---\n(b) Ours (a) E xisting Methods\nLow \nScaleHigh \nScale\nInputs w/o maskResult of \nLow-scale StructureInputsResult with\n High -Quality  Details\nSingle \nScale\nInputs w/ maskResult\nSingle \nScale\nInputs w/ maskResult\nInputs Results of\nExisting Methods\nResults of\nLow -scale StructureResults of\nHigh -Quality  DetailsFigure 1: Comparison between existing methods [Kim et al., 2024, Xu et al., 2025, Choi et al.,\n2024, Zhou et al., 2025] and our proposed DS-VTON. (a) Existing approaches typically adopt a\nsingle-scale pipeline with masked inputs, limiting their ability to capture full-body semantics and\ngarment structure. (b) In contrast, DS-VTON employs a fully mask-free, dual-scale framework: the\nlow-resolution stage focuses on garment-body alignment, while the high-resolution stage refines\ntexture details via residual-guided denoising.\ntexture details. This progressive generation order aligns naturally with the requirements of virtual\ntry-on. However, relying on a single-stage diffusion process remains inherently limited. In practice,\nmany existing approaches still struggle to simultaneously ensure accurate garment-body alignment\nand high-fidelity texture reconstruction. The unified framework may fail to fully disentangle structure\nfrom detail, leading to compromised visual quality in the final output.\nTo address these challenges, we propose a dual-scale framework that explicitly decouples structural\nalignment and texture refinement. Motivated by the natural progression from structure to detail in\ndiffusion processes, our pipeline first performs coarse alignment and subsequently enhances garment\ndetails in a second refinement stage. Specifically, our framework consists of two sequential stages:\n‚Ä¢Low-resolution stage : the model generates a coarse try-on result by suppressing high-frequency\ncontent and focusing on accurate structural alignment.\n‚Ä¢High-resolution stage : we extend a residual-guided denoising strategy that predicts the residual\nbetween the high-resolution image and its low-resolution counterpart. This enables the model to\nfocus specifically on texture restoration, building upon the structurally aligned base from the first\nstage.\nBeside, traditional virtual try-on methods rely on human parsing masks for spatial guidance, our\napproach adopts a mask-free strategy that eliminates this dependency by leveraging the strong\nsemantic priors embedded in pretrained diffusion models. This design avoids errors caused by\ninaccurate masks and preserves the full-body appearance and geometry, allowing the model to\nadaptively infer garment regions. As a result, our method improves the accuracy of body structure\nmodeling and better retains the individuality of the person‚Äôs appearance, leading to more realistic\nand personalized try-on results. Extensive experiments on VITON-HD [Choi et al., 2021] and\nDressCode [Morelli et al., 2022] validate the effectiveness of our method, demonstrating state-of-the-\nart performance both qualitatively and quantitatively.\n2\n--- Page 3 ---\nRes Block Self-Attention BlockResize\nùë•ùëù \nùë•ùëü \nùë•ùëî \nùúñ \n√óùõº \n√óùõΩ \nùë• ùëù \nùë• ùëü Low -Resolution Stage\nùúñ  \nDenoising U -Net\nHigh -Resolution Stage\nDenoising U -NetK,V concatK,V concat\nùë• ùëî \nùë• ùëü Reference U -NetReference U -Net\nratio:\nùúé Figure 2: Overview of our proposed method. The overall pipeline consists of a two-scale generation\nprocess: the low-resolution stage produces a coarse try-on result, which is then refined by the high-\nresolution stage. Both stages share the same network architecture. See Section 3 for further details.\n2 Related works\nGAN-based virtual try-on. Earlier methods [Choi et al., 2021, Ge et al., 2021a, Lee et al., 2022,\nXie et al., 2023], which are based on Generative Adversarial Networks (GANs) [Goodfellow et al.,\n2020], typically decompose the virtual try-on task into two stages: (1) warping the garment to\nalign with the human body shape, and (2) integrating the warped garment with the human image\nto generate the final result. For instance, ACGPN [Yang et al., 2020] employs a warping module\nbased on Thin-Plate Spline (TPS) [Duchon, 1977] to deform the garment. PFAFN [Ge et al., 2021b]\nproposes a parser-free method that guides the garment warping process using learned appearance\nflows. VITON-HD [Choi et al., 2021] introduces a specialized normalization layer and generator\ndesign to better handle garment-body misalignment during synthesis. However, a key limitation of\nGAN-based approaches is their constrained capacity in capturing both precise spatial alignment and\nfine-grained garment details. As a result, these methods often rely heavily on the warping stage to\nencode garment appearance early in the pipeline. Yet, the subsequent integration stage frequently\nintroduces artifacts or detail loss, degrading the overall realism of the try-on result.\nDiffusion-based virtual try-on. With the rapid advancement of diffusion models [Ho et al., 2020,\nRombach et al., 2022], a series of powerful virtual try-on methods have been proposed [Kim et al.,\n2024, Zhu et al., 2023, Choi et al., 2024, Zhou et al., 2025, Sun et al., 2024]. Early approaches like\nDCI-VTON [Gou et al., 2023] follow a traditional two-stage pipeline: first warping the garment to\nmatch the body, then applying a diffusion model to blend it with the person image. More recent\nmethods [Morelli et al., 2023, Kim et al., 2024, Zhu et al., 2023, Choi et al., 2024] adopt a single\ndiffusion process that directly synthesizes the try-on result, shifting the focus to more effective\nconditioning strategies. For example, LaDI-VTON [Morelli et al., 2023] employs textual inversion\nto encode garment identity, while IDM-VTON [Choi et al., 2024] introduces a GarmentNet module\nfor structural and appearance guidance. Leffa [Zhou et al., 2025] further proposes a Leffa loss to\nguide attention weights during the final 500 denoising steps, enhancing fine-grained texture recovery.\nComplementary to these methods, FitDiT [Jiang et al., 2024] explores an alternative direction. Built\non SD3 [Esser et al., 2024], which adopts a DiT-based architecture [Peebles and Xie, 2023], it uses\nan aggressive rectangular mask to impose stronger spatial constraints, effectively mitigating the\nalignment issues faced by earlier mask-based pipelines. Despite notable progress, current diffusion-\nbased methods still suffer from visible shortcomings. These include garment fragmentation artifacts\ndue to imprecise human segmentation, as well as inaccurate rendering of fine patterns such as flowers\nor text. Addressing these open challenges remains a key direction.\n3\n--- Page 4 ---\n3 Methodologies\nNotations. Given a person image xp‚ààRH√óW√ó3and a garment image xg‚ààRH√óW√ó3, the virtual\ntry-on task aims to generate a realistic output xr‚ààRH√óW√ó3, where the person appears to be wearing\nthe given garment. Our pipeline requires no auxiliary conditions such as segmentation masks or\nhuman parsing maps. The only additional variables appear in the low-resolution stage, where the\ninputs xpandxgare downsampled to Àúxp,Àúxg‚ààRh√ów√ó3, with h=H/œÉ ,w=W/œÉ , andœÉdenoting\nthe downsampling ratio. These are used to generate a low-resolution try-on result Àúxr‚ààRh√ów√ó3. For\nnotational simplicity, we assume that Àúxrhas been upsampled to the original resolution when used in\nthe high-resolution stage, and we do not introduce a separate symbol for it. Further details on the\nlow-resolution process and the choice of œÉare provided in Subsection 3.3.\nMask-free strategy. Previous virtual try-on methods [Kim et al., 2024, Xu et al., 2025, Choi\net al., 2024, Zhou et al., 2025, Jiang et al., 2024] commonly rely on external human parsers to\nobtain body segmentation masks. These masks are then used to explicitly isolate the garment region\nin the person image, serving as spatial guidance during the try-on process. While this strategy is\nreasonable in conventional pipelines, it becomes less compelling in the context of diffusion-based\nmodels like Stable Diffusion [Rombach et al., 2022], which are pretrained on large-scale data and\ninherently encode strong semantic priors related to human structure and clothing layout. Modern\npretrained diffusion models already demonstrate robust understanding of body geometry and clothing\nregions without requiring explicit mask supervision. Motivated by this observation, we adopt a fully\nmask-free design. Unlike prior work [Ge et al., 2021b], we do not distill pseudo-masks from human\nparsers. Instead, during both training and inference, the model directly takes a garment image and a\nperson image as input, without relying on any parsing or segmentation guidance. This design offers\ntwo major advantages. First, segmentation masks may contain errors, either omitting parts of the\ngarment or incorrectly covering body regions, leading to unrealistic or inconsistent outputs. Second,\nto avoid garment omission, large masks are sometimes used, which can obscure body regions and\nimpair the model‚Äôs ability to reconstruct accurate body shape. In contrast, our mask-free strategy\nallows the model to adaptively infer the true garment region while preserving full-body geometry.\nThis leads to more faithful body reconstruction and benefits downstream stages, ultimately improving\nthe visual quality of the final try-on result. Please refer to Subsection 4.2 for qualitative comparisons.\n3.1 Overview\nOur method is built upon Stable Diffusion [Rombach et al., 2022]. The backbone architecture follows\nthe dual U-Net framework [Zhang, 2023, Hu, 2024, Xu et al., 2024], which has also demonstrated\nstrong performance in virtual try-on tasks [Choi et al., 2024, Xu et al., 2025]. Further architectural\ndetails are provided in Subsection 3.2. As shown in Figure 2, both the low-resolution and high-\nresolution stages share this network architecture.\nIn the low-resolution stage, we first use a V AE [Kingma, 2013] to encode the downsampled garment\nimage Àúxg, which is then passed to the reference U-Net. The downsampled person image Àúxpis also\nV AE-encoded and concatenated along the feature channel with a Gaussian noise tensor of the same\nshape. This combined latent is fed into the denoising U-Net to generate a low-resolution result\nÀúxr‚ààRh√ów√ó3.\nIn the high-resolution stage, xgandxpare used in the same manner as in the low-resolution stage:\nxgis passed to the reference U-Net, while xpis encoded and concatenated with the latent, which is\nthen input to the denoising U-Net. The main difference is the initialization of the latent. Specifically,\ninstead of initializing xTas standard Gaussian noise, we define the noisy latent input as:\nxT=Œ±¬∑œµ+Œ≤¬∑Àúxr, (1)\nwhere œµ‚àº N(0,I), and Œ±, Œ≤ are balancing coefficients. The model is trained to gradually denoise\nthis latent, transitioning xTtoxrby converting the noise component Œ±¬∑œµinto the residual term\nxr‚àíŒ≤¬∑Àúxr, such that the final result satisfies x0=xr. We detail the full formulation, as well as the\ndesign choices for Œ±andŒ≤, in Subsection 3.4.\n4\n--- Page 5 ---\n3.2 Network architecture\nWe adopt a dual U-Net architecture, where the reference U-Net encodes garment features and\nintegrates them into the main denoising U-Net through self-attention layers. This structure has proven\neffective for maintaining garment fidelity and enhancing visual quality [Choi et al., 2024], and is\nretained in our architecture. Following the design philosophy of [Chong et al., 2025], we remove all\ncross-attention layers in the U-Net, relying solely on self-attention mechanisms. This simplification\nimproves performance and efficiency, as confirmed by our ablation studies (refer to Appendix B.1).\nAll garment-related information is delivered through the reference U-Net, eliminating the need for\nadditional conditioning pathways. To further improve computational efficiency, we execute the\nreference U-Net only once per sample, treating it purely as a conditioning module, in line with [Li\net al., 2024].\nWe initialize our U-Net weights using those of Stable Diffusion 1.5 [Rombach et al., 2022]. While\nSDXL [Podell et al., 2025] offers more powerful generative capabilities, the goal of our work is\nto propose a lightweight yet effective framework. Hence, we retain SD1.5 as our backbone. We\nalso conducted preliminary experiments based on transformer architectures such as SD3 and SD3.5,\nwith additional discussion provided in Appendix C. In conclusion, SD1.5 offers an optimal trade-off\nbetween model simplicity and try-on performance, and forms the foundation of our pipeline.\n3.3 Low-resolution stage\nLow-resolution images inherently emphasize structural information due to the suppression of fine-\ngrained textures. Thus, we first introduce a low-resolution stage to guide the overall generation\nprocess. This stage serves two main purposes. First, it enables the model to capture the human\nbody shape and garment category, ensuring accurate structural alignment between the person and\nthe clothing. Second, it provides coarse but semantically reliable garment structure, such as the\nplacement of stripes or patterns, offering guidance for fine-detail reconstruction in the high-resolution\nstage.\nIn this stage, we first downsample the garment image xgand the person image xpby the downsam-\npling ratio œÉ, and then perform denoising from pure Gaussian noise œµto generate the low-resolution\nresult Àúxr. Both training and inference follow the standard diffusion process used in Stable Diffu-\nsion [Rombach et al., 2022, Ho et al., 2020, Song et al., 2020].\nThe only hyperparameter in this stage is the downsampling ratio œÉ. We consider three values:\nœÉ= 1,2,and4. When œÉ= 1, the low-resolution stage operates at the same resolution as the\nhigh-resolution stage; œÉ= 2 corresponds to downsampling from 768√ó1024 to384√ó512, and\nso on. Based on our observations, downsampling ratios of œÉ= 2andœÉ= 4both enhance human\nbody structure understanding, while œÉ= 2offers the most reliable structural guidance for accurate\ngarment reconstruction. Quantitative and qualitative comparisons of different œÉvalues are presented\nin Subsection 4.3, and œÉ= 2is used in all experiments throughout this work.\n3.4 High-resolution stage\nWhile the low-resolution stage focuses on body structure by ignoring fine details, the high-resolution\nstage is designed to refine garment textures. Instead of directly reconstructing the full high-resolution\nimage, this stage aims to reconstruct the residual between the high- and low-resolution outputs. This\nstrategy effectively disentangles structural alignment from detailed appearance modeling.\n3.4.1 Reformulating the denoising process with residual learning\nDDPM. Denoising Diffusion Probabilistic Mode (DDPM) [Ho et al., 2020] aims to approximate\nthe true data distribution by leveraging the diffusion probabilistic model framework [Sohl-Dickstein\net al., 2015], which defines a Markov chain of length Tthat gradually transforms pure Gaussian noise\ninto a sample from the data distribution. Compared with earlier diffusion models, DDPM incorporates\nideas from score matching [Song and Ermon, 2019], simplifying the objective by training the model\nto predict only the noise component œµ, which approximates the score function ( i.e., the gradient of\n5\n--- Page 6 ---\nthe log-density). The forward and reverse diffusion processes are defined as:\nxt=‚àöŒ±txt‚àí1+‚àö\n1‚àíŒ±tœµ\n=‚àö¬ØŒ±tx0+‚àö\n1‚àí¬ØŒ±tœµ, (2)\nxt‚àí1=1‚àöŒ±t\u0012\nxt‚àí1‚àíŒ±t‚àö1‚àí¬ØŒ±tœµŒ∏(xt, t)\u0013\n+œÉtz. (3)\nEquations (2)and(3)define the forward and reverse diffusion processes, respectively. In these\nequations, œµ,z‚àº N(0,I), and œµŒ∏(xt, t)denotes the noise predicted by the model. The parameters Œ±t\nand¬ØŒ±tare predefined noise scheduling terms. In this framework, x0‚àºpdata(x0)denotes a sample\nfrom the true data distribution, while xT‚àº N(0,I). All formulations above hold for t= 1,2, . . . , T .\nResidual-guided denoising reformulation. Inspired by [Yue et al., 2023], we reformulate the\nforward and reverse processes to explicitly incorporate the low-resolution generation result Àúxr\nas a structural prior for the high-resolution generation. Instead of constructing a Markov chain\nsolely between Gaussian noise and pdata(x0), we aim to build a transition path from Àúxrto the\nhigh-resolution result xr. A naive formulation such as xT=œµ+Àúxr,x0=xrlacks flexibility in\ncontrolling the influence of Àúxrandœµ. To address this, we adopt a more flexible initialization as\ndefined in Equation (1), with x0=xr. Under this formulation, the residual-guided forward and\nreverse diffusion processes become:\nxt=‚àöŒ±txt‚àí1+‚àö\n1‚àíŒ±t(Œ±¬∑œµ+Œ≤¬∑Àúxr)\n=‚àö¬ØŒ±tx0+‚àö\n1‚àí¬ØŒ±t(Œ±¬∑œµ+Œ≤¬∑Àúxr), (4)\nxt‚àí1=1‚àöŒ±t\u0012\nxt‚àí1‚àíŒ±t‚àö1‚àí¬ØŒ±tÀúœµŒ∏(xt, t)\u0013\n+œÉtz. (5)\nEquations (4)and(5)define the forward and reverse processes, respectively. The model is trained to\npredict ÀúœµŒ∏(xt, t)‚âàŒ±¬∑œµ+Œ≤¬∑Àúxr, such that the noise component in xtis replaced by a composition\nofœµand the low-resolution result Àúxr. Except for this reformulated initialization and noise target, the\nrest of the denoising process remains consistent with the original DDPM. All formulations above\nhold for t= 1,2, . . . , T .\n3.4.2 Controlling noise-structure balance\nIn the high-resolution stage, generation begins from the latent xT=Œ±¬∑œµ+Œ≤¬∑Àúxr, which combines a\nstochastic noise term and a structural prior. Importantly, although the noise term has been modified,\nthe actual source of randomness remains from œµ. During the reverse denoising process, this component\nŒ±¬∑œµis gradually transformed into the residual xr‚àíŒ≤¬∑Àúxr, ensuring that the final output satisfies\nx0=xr. This formulation offers explicit control over the trade-off between randomness and\nstructural guidance. A larger Œ≤may cause the generation to overly rely on the low-resolution input,\nwhich is intended to provide only structural guidance, thereby potentially suppressing the recovery of\nfine details. While a larger Œ±introduces stronger randomness that may override structural consistency.\nThrough empirical studies, we find that setting Œ±=Œ≤= 0.5offers a good trade-off between fidelity\nand flexibility. More discussions can be found in Section 4.3.\n4 Experiments\n4.1 Experimental setup\nDatasets. In this paper, we conduct experiments on the VITON-HD [Choi et al., 2021] and\nDressCode [Morelli et al., 2022] datasets. All ablation studies are carried out on VITON-HD. While\nVITON-HD contains only upper-body garments, DressCode includes three garment categories: upper-\nbody, lower-body, and dresses. Both datasets consist of paired images, each containing a person\nimage and a corresponding garment image. However, as our method is mask-free, the original paired\ndata alone is insufficient for training. To address this, we use IDM-VTON [Choi et al., 2024] to\nsynthesize additional training data. Further details are provided in Appendix A.1.\n6\n--- Page 7 ---\nGarment Person CatVTON IDM-VTON FitDiT Leffa DS-VTON(HR) DS-VTON(LR)\n OOTDiffusion\nFigure 3: Qualitative comparison on the VITON-HD dataset. DS-VTON(LR) denotes the low-\nresolution result, and DS-VTON(HR) represents the final high-resolution result.\nImplementation details. For network initialization, both the reference U-Net and the denoising\nU-Net are initialized with pretrained weights from Stable Diffusion 1.5 [Rombach et al., 2022]. As\ndetailed in Subsections 3.3 and 3.4, we set the downsampling ratio œÉ= 2 and use Œ±=Œ≤= 0.5\nto initialize xTin the high-resolution stage. Consequently, the low-resolution stage operates at a\nresolution of 384√ó512, while the high-resolution stage produces outputs at 768√ó1024 . During\ninference, the two stages are executed sequentially with 20 sampling steps each, using the DDIM\nsampler [Song et al., 2020]. Additional training details are provided in Appendix A.2.\nBaselines. We compare our method with several recent state-of-the-art approaches, including\nCatVTON [Chong et al., 2025], IDM-VTON [Choi et al., 2024], Leffa [Zhou et al., 2025], OOT-\nDiffusion [Xu et al., 2025], and FitDiT [Jiang et al., 2024], using their official model weights and\ninference code. For fairness, we standardize the number of inference steps to 30 across all methods.\nAll methods, except FitDiT, are trained separately on each dataset and evaluated accordingly. FitDiT\nprovides only a single set of pretrained weights, jointly trained on VITON-HD, DressCode, and\nCVDD [Jiang et al., 2024], and is included in our evaluation for completeness.\nEvaluation metrics. Previous virtual try-on methods typically evaluate performance under both\npaired and unpaired settings. The paired setting involves reconstructing the original person image\nwith the same garment, while the unpaired setting replaces it with a different one [Choi et al., 2021].\nAs most prior approaches rely on masking the garment region, they support both settings. In contrast,\nour method is mask-free and is therefore evaluated only under the unpaired setting, which better\nreflects real-world scenarios. Following prior works [Chong et al., 2025, Jiang et al., 2024, Zhou\net al., 2025], we adopt Fr√©chet Inception Distance (FID) [Parmar et al., 2022] and Kernel Inception\nDistance (KID) [Bi ¬¥nkowski et al., 2018] as quantitative metrics. We also conduct a user study to assess\n7\n--- Page 8 ---\nTable 1: Quantitative comparisons on the VITON-HD [Choi et al., 2021] and DressCode [Morelli\net al., 2022] datasets. FitDiT [Jiang et al., 2024], trained jointly on VITON-HD, DressCode, and\nCVDD [Jiang et al., 2024], is included for completeness. In contrast, all other methods are trained\nindividually on each dataset.\nDataset VITON-HD DressCode\nMethod FID‚Üì KID ‚Üì User Study ‚Üë FID‚Üì KID ‚Üì User Study ‚Üë\nOOTDiffusion [Xu et al., 2025] 9.02 0.63 4.1 7.10 2.28 7.2\nIDM-VTON [Choi et al., 2024] 9.10 1.06 11.6 5.51 1.42 9.1\nCatVTON [Chong et al., 2025] 9.40 1.27 3.4 5.24 1.21 5.2\nLeffa [Zhou et al., 2025] 9.38 0.92 4.7 6.17 1.90 7.5\nFitDiT [Jiang et al., 2024] 9.33 0.89 19.7 4.47 0.41 34.3\nDS-VTON (ours) 8.24 0.31 56.5 4.21 0.34 36.7\nTable 2: Ablation on dual-scale design and down-\nsampling ratio œÉ.\nVersion FID ‚Üì KID ‚Üì\nœÉ= 1 8.97 1.01\nœÉ= 1, Œ±=Œ≤=1\n28.77 0.61\nœÉ= 4, Œ±=Œ≤=1\n28.41 0.57\nœÉ= 2, Œ±=Œ≤=1\n28.24 0.31Table 3: Ablation on coefficients Œ±, Œ≤ under\nfixed œÉ= 2.\nVersion FID ‚Üì KID ‚Üì\nœÉ= 2, Œ±=Œ≤=1\n28.24 0.31\nœÉ= 2, Œ±=2\n3, Œ≤=1\n38.46 0.55\nœÉ= 2, Œ±=1\n3, Œ≤=2\n38.26 0.35\nœÉ= 2, Œ±=Œ≤= 1 8.75 0.94\nperceptual quality: for VITON-HD, we randomly sample 100 results per method; for DressCode, we\nsample 33, 33, and 34 results from the dresses, lower-body, and upper-body categories, respectively.\nParticipants are asked to select the result they think performs better in the try-on task. All evaluations\nare conducted at a resolution of 768√ó1024 .\n4.2 Quantitative and qualitative results\nQualitative comparison. Figure 3 presents a qualitative comparison between DS-VTON and recent\nbaseline methods on the VITON-HD [Choi et al., 2021] dataset. Row numbers referenced below\ncorrespond to Figure 3. We evaluate each method in terms of two key aspects: structural alignment\nand detail preservation. In terms of structural alignment, most mask-based methods fail to accurately\ncapture body pose and garment shape, as illustrated in Row 1. FitDiT [Jiang et al., 2024], which uses\na larger rectangular mask, performs relatively better but introduces noticeable artifacts: it fails to\nreconstruct the hands and exhibits visible artifacts at the junction of the upper and lower garments in\nRow 1. OOTDiffusion [Xu et al., 2025] alters the original skin tone in Row 2. CatVTON [Chong\net al., 2025], IDM-VTON [Choi et al., 2024], and Leffa [Zhou et al., 2025] also show varying degrees\nof misalignment. In contrast, DS-VTON consistently achieves accurate alignment across a wide range\nof poses and garment types. Regarding detail preservation, CatVTON and IDM-VTON fail to retain\nfine-grained textures on complex garments (Row 3), with IDM-VTON generating oversimplified or\nmissing patterns. While Leffa, OOTDiffusion, and FitDiT better preserve textures, they each show\nlimitations: FitDiT achieves texture preservation at the cost of distorting the person‚Äôs actual body\nshape and dressing structure (Rows 2 and 4), OOTDiffusion introduces artifact patterns (Row 5),\nand Leffa exhibits reduced pattern clarity in complex regions (Rows 3). Furthermore, both FitDiT\nand Leffa suffer from tonal inconsistencies‚ÄîFitDiT produces noticeably brighter garments (Row 6),\nwhile Leffa tends to generate darker outputs. In contrast, DS-VTON preserves fine textures while\nmaintaining tonal fidelity throughout.\nQuantitative comparison. We conduct experiments on both the VITON-HD [Choi et al., 2021]\nand DressCode [Morelli et al., 2022] datasets. As shown in Table 1, DS-VTON achieves substan-\ntial improvements across both benchmarks. CatVTON [Chong et al., 2025] generates images at\n384√ó512, which we upsample to 768√ó1024 for comparison. To confirm that this does not bias\nresults, we also evaluate it at native resolution, obtaining FID and KID scores of 9.36 and 1.19,\nrespectively‚Äîindicating minimal degradation due to upsampling. Unlike prior methods [Chong\net al., 2025, Zhou et al., 2025, Choi et al., 2024] that rely on explicit mask generation and inpainting,\n8\n--- Page 9 ---\nGarment Person ùúé=4 (LR) ùúé=4 (HR) ùúé=2 (LR) ùúé=2 (HR) ùúé=1 (LR) ùúé=1 (HR)Figure 4: Visualized results under varying downsampling ratios œÉ. For each œÉ, the LR column shows\nthe output from the low-resolution stage, and the corresponding HR column shows the high-resolution\nresult refined from that output.\nGarment Person ùõº=2\n3ùõΩ=1\n3(HR) ùõº=1\n2ùõΩ=1\n2(HR) ùõº=1ùõΩ=1(HR) ùõº=1\n3ùõΩ=2\n3(HR) ùúé=2 (LR)\nFigure 5: Visualized results under different xTinitialization settings ( xT=Œ±¬∑œµ+Œ≤¬∑Àúxr).\nDS-VTON is entirely mask-free. This enables accurate, high-quality outputs that are robust and\nreproducible without dependence on mask quality.\n4.3 Ablation study\nAblation on dual-scale design. To validate the effectiveness of our dual-scale design, we train four\nvariants on the VITON-HD dataset. As shown in Table 2, Row 1 ( œÉ= 1) corresponds to training\nthe model directly at high resolution ( 768√ó1024 ) without a refinement stage, aligning with the\nœÉ= 1 (LR) column in Figure 4. Applying the mask-free strategy at this resolution leads to poor\nstructural results, highlighting the need for coarse-level guidance. Row 2 ( œÉ= 1, Œ±=Œ≤=1\n2), shown\nin the œÉ= 1 (HR) column, adds a refinement stage. While some structural errors are alleviated,\nrelying solely on the second stage to recover both structure and detail still results in failures, due to\nthe lack of reliable low-resolution guidance. This also violates our design principle of decoupling\nstructure modeling (stage one) from detail refinement (stage two). Rows 3 and 4 evaluate the full\ndual-scale pipeline with œÉ= 4 andœÉ= 2, respectively. When garments are structurally simple,\nboth perform reasonably well. However, as shown in Row 1 of Figure 4, œÉ= 4introduces visible\ninformation loss in complex cases. These qualitative results align with the quantitative trends: among\nthe four variants‚Äîsingle-stage ( œÉ= 1) and dual-stage ( œÉ= 1,œÉ= 2,œÉ= 4)‚Äîthe single-stage\nmodel performs worst, œÉ= 4shows moderate improvement, and œÉ= 2achieves the best overall\nperformance.\nAblation on coefficient-based initialization of xT.When fixing the downsampling ratio to œÉ= 2,\nthe coefficients Œ±andŒ≤control how much the low-resolution output influences the initialization\nof the high-resolution stage. As shown in Figure 5, setting Œ±too high‚Äîsuch as Œ±= 1, Œ≤= 1 or\nŒ±=2\n3, Œ≤=1\n3‚Äîleads to structural distortions in the final result, despite the low-resolution output\nalready providing accurate guidance. In both cases, the red stripe on the garment appears distorted,\nand in the Œ±=2\n3, Œ≤=1\n3configuration, the \"GANT\" text also becomes visibly warped. Among the\nremaining configurations, Œ±=1\n3, Œ≤=2\n3andŒ±=1\n2, Œ≤=1\n2yield generally good results. However,\nthe lower- Œ±version ( Œ±=1\n3) still shows minor distortion in the red stripe, indicating insufficient\n9\n--- Page 10 ---\ncontrol over fine structure. Balancing these observations, we adopt Œ±=Œ≤=1\n2as our default setting.\nThe quantitative results in Table 3 are consistent with this choice.\n5 Conclusions\nWe propose DS-VTON, a dual-scale virtual try-on framework that introduces residual-guided de-\nnoising to bridge low- and high-resolution generation. This design enables a principled decoupling\nof the two core challenges in virtual try-on: structural alignment and detail refinement. Combined\nwith a mask-free strategy, our approach achieves significant improvements over existing methods\nin both visual quality and robustness. Moreover, the proposed low-to-high resolution paradigm\nis inherently scalable and generalizable, offering potential for extension to higher resolutions and\nbroader generation tasks beyond virtual try-on, such as personalized image synthesis.\n10\n--- Page 11 ---\nReferences\nMiko≈Çaj Bi ¬¥nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd\ngans. arXiv preprint arXiv:1801.01401 , 2018.\nSeunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual\ntry-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 14131‚Äì14140, 2021.\nYoungjin Choi, Seunghyun Kwak, Kyungjune Lee, Hyojin Choi, and Jinwoo Shin. Improving\ndiffusion models for authentic virtual try-on in the wild. In European Conference on Computer\nVision (ECCV) , pages 206‚Äì235, Cham, 2024. Springer Nature Switzerland.\nZheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing\nZhao, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion\nmodels. In Proceedings of the International Conference on Learning Representations (ICLR) ,\n2025.\nJean Duchon. Splines minimizing rotation-invariant semi-norms in sobolev spaces. In Constructive\nTheory of Functions of Several Variables: Proceedings of a Conference held at Oberwolfach April\n25‚ÄìMay 1, 1976 , pages 85‚Äì100. Springer Berlin Heidelberg, 1977.\nPatrick Esser, Shubham Kulal, Andreas Blattmann, Reza Entezari, Jonas M√ºller, Harsh Saini, and\nRobin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In\nProceedings of the 41st International Conference on Machine Learning (ICML) , 2024.\nChongjian Ge, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo. Disentangled cycle\nconsistency for highly-realistic virtual try-on. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 16928‚Äì16937, 2021a.\nYuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual\ntry-on via distilling appearance flows. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 8485‚Äì8493, 2021b.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the\nACM , 63(11):139‚Äì144, 2020.\nJunhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power\nof diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the\n31st ACM International Conference on Multimedia , pages 7599‚Äì7607, 2023.\nXintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S. Davis. Viton: An image-based virtual try-\non network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , pages 7543‚Äì7552, 2018.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems , volume 33, pages 6840‚Äì6851, 2020.\nLi Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n8153‚Äì8163, 2024.\nBin Jiang, Xiaoxiao Hu, Dongdong Luo, Qian He, Chen Xu, Jing Peng, and Yanwei Fu. Fit-\ndit: Advancing the authentic garment details for high-fidelity virtual try-on. arXiv preprint\narXiv:2411.10499 , 2024.\nJeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning\nsemantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8176‚Äì8185, 2024.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.\n11\n--- Page 12 ---\nSangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. High-resolution\nvirtual try-on with misalignment and occlusion-handled conditions. In European Conference on\nComputer Vision , pages 204‚Äì219. Springer, 2022.\nYexin Li, Haoyu Zhou, Weichen Shang, Runyu Lin, Xinyu Chen, and Bingbing Ni. Anyfit: Con-\ntrollable virtual try-on for any combination of attire across any scenario. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2024.\nIlya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint\narXiv:1711.05101 , 5, 2017.\nDavide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara.\nDress code: High-resolution multi-category virtual try-on. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 2231‚Äì2235, 2022.\nDavide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita\nCucchiara. Ladi-vton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings\nof the 31st ACM International Conference on Multimedia , pages 8580‚Äì8589, 2023.\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in\ngan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 11410‚Äì11420, 2022.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) , pages 4195‚Äì4205, 2023.\nDaniel Podell, Zana English, Kenneth Lacey, Andreas Blattmann, Tobias Dockhorn, Jonas M√ºller,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis.\nInProceedings of the International Conference on Learning Representations (ICLR) , 2025.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning , pages\n8748‚Äì8763. PMLR, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages 10684‚Äì10695, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learning ,\npages 2256‚Äì2265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502 , 2020.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nInAdvances in Neural Information Processing Systems (NeurIPS) , volume 32, 2019.\nK. Sun, J. Cao, Q. Wang, L. Tian, X. Zhang, L. Zhuo, and D. Gao. Outfitanyone: Ultra-high quality\nvirtual try-on for any clothing and any person. arXiv preprint arXiv:2407.16224 , 2024.\nZhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and\nXiaodan Liang. Gp-vton: Towards general purpose virtual try-on via collaborative local-flow\nglobal-parsing learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 23550‚Äì23559, 2023.\nYuhao Xu, Tao Gu, Weifeng Chen, and Aoxue Chen. Ootdiffusion: Outfitting fusion based latent\ndiffusion for controllable virtual try-on. In Proceedings of the AAAI Conference on Artificial\nIntelligence , volume 39, pages 8996‚Äì9004, 2025.\nZhen Xu, Jing Zhang, Jun Hao Liew, Hongdong Yan, Jianwen Liu, Chunyan Zhang, Jiashi Feng, and\nMike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion\nmodel. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2024.\n12\n--- Page 13 ---\nHongwen Yang, Rongyao Zhang, Xiaonan Guo, Wei Liu, Wangmeng Zuo, and Ping Luo. Towards\nphoto-realistic virtual try-on by adaptively generating-preserving image content. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7850‚Äì7859,\n2020.\nHailin Ye, Jing Zhang, Shichao Liu, Xiangyu Han, and Wei Yang. Ip-adapter: Text compatible image\nprompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 , 2023.\nZongyu Yue, Jingyun Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image\nsuper-resolution by residual shifting. In Advances in Neural Information Processing Systems\n(NeurIPS) , volume 36, pages 13294‚Äì13307, 2023.\nL. Zhang. Reference-only controlnet. https://github.com/Mikubill/\nsd-webui-controlnet/discussions/1236 , 2023. Accessed: 2023-04.\nZiqian Zhou, Shichao Liu, Xiangyu Han, Hao Liu, Kwan-Yee Ng, Ting Xie, and Shimin He. Learning\nflow fields in attention for controllable person image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 2025.\nLuyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad\nNorouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: A tale of two unets. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4606‚Äì4615, 2023.\n13\n--- Page 14 ---\nAppendix\nA Experimental details\nA.1 Additional training data generation and associated challenges\nOur method adopts a mask-free paradigm, in contrast to prior approaches [Choi et al., 2024, Xu\net al., 2025, Li et al., 2024], which rely on paired person-garment images by masking out the garment\nregion in the person image and reconstructing it using the standalone garment image. In our case, the\nperson image remains unaltered throughout the training process. To train the low-resolution stage,\neach training sample requires three images: (1) a garment image, (2) a person image wearing that\ngarment, and (3) another person image of the same identity but wearing a different garment. The third\nimage is constructed by randomly sampling another garment of the same category from the dataset\nand synthesizing a new person image using IDM-VTON [Choi et al., 2024]. For the high-resolution\nstage, we additionally require the low-resolution output corresponding to the original person-garment\npair. To obtain this, we use our trained low-resolution model to generate a coarse try-on result by\ninputting the target garment and the synthesized person image (i.e., with a different garment). This\noutput is then used as the low-resolution input for the high-resolution stage.\nIn summary, each high-resolution training sample consists of: (1) the target garment image, (2) a\nperson image wearing a different garment, (3) the corresponding low-resolution try-on result for\nthe target garment, and (4) the ground-truth high-resolution image of the person wearing the target\ngarment.\nWhile the above construction enables mask-free supervision, it inevitably introduces certain artifacts.\nSpecifically, in the low-resolution stage, the reference U-Net encodes the garment image, while the\ndenoising U-Net operates on the concatenation of the noisy latent and the person image‚Äîhere, a\nsynthesized image of the same identity wearing a different garment.\nSince the synthesized person image is generated by a model such as IDM-VTON, it may exhibit\nvariations beyond the garment itself, including changes in hairstyle, background, or the presence of\naccessories. As a result, the model may inadvertently learn to alter these unrelated regions during\ntraining. Ideally, such issues would not occur if fully disentangled and clean ground-truth data were\navailable. Fortunately, we find this side effect to be limited in practice, as most synthesized person\nimages remain visually consistent with the original identity.\nA.2 More implementation details\nAll experiments are conducted on 8 NVIDIA A6000 GPUs. For both VITON-HD [Choi et al., 2021]\nand DressCode [Morelli et al., 2022], the low-resolution stage is trained with a batch size of 8,\nand the high-resolution stage with a batch size of 2. Both stages are optimized using the AdamW\noptimizer [Loshchilov et al., 2017] with a learning rate of 1e‚àí6. For VITON-HD, the low-resolution\nand high-resolution stages are trained for 15,000 and 30,000 steps, respectively (approximately 5 and\n24 hours). For DressCode, we jointly train all three garment categories, with the two stages trained\nfor 20,000 and 30,000 steps (approximately 7 and 24 hours). During inference, both stages use 20\nDDIM sampling steps. On a single A6000 GPU, the low-resolution stage takes about 1 second per\nsample, and the high-resolution stage about 4 seconds.\nB More ablation studies\nB.1 Ablation on removing cross-attention layers in U-Net\nSeveral previous try-on methods [Choi et al., 2024, Xu et al., 2025, Li et al., 2024] based on\ndual U-Net architectures incorporate additional conditional encoders, such as CLIP [Radford et al.,\n2021] or IP-Adapter [Ye et al., 2023], to inject garment information via cross-attention. However,\nthese approaches introduce extra complexity, and it remains unclear whether they actually improve\nperformance. To investigate this, we conduct an ablation study on the VITON-HD [Choi et al., 2021]\ndataset by comparing two versions of our low-resolution stage ( 384√ó512): (1) our baseline model,\nwhich does not include any cross-attention layers, and (2) a variant built on the baseline by adding\ncross-attention layers to both the reference U-Net and the denoising U-Net. In the latter, garment\n14\n--- Page 15 ---\nfeatures are encoded using CLIP and injected via cross-attention. As shown in Figure 6 and Table 4,\nadding cross-attention does not help preserve garment details; on the contrary, it introduces noticeable\ndistortions. This is also reflected in the performance metrics.\nGarment Person With cross -attention DS-VTON  (low-resolution stage)\nFigure 6: Comparison between our baseline architecture and the variant with cross-attention. In\nthe variant, garment features encoded by CLIP are injected into both the reference U-Net and the\ndenoising U-Net via cross-attention layers.\nTable 4: Ablation study on the effect of cross-attention layers in U-Net.\nVersion FID ‚ÜìKID‚Üì\nWith cross-attention 9.07 0.94\nDS-VTON (low-resolution stage) 8.88 0.72\nC Discussions\nHow about utilizing the DiT architecture for mask-free try-on? We also explored using the\nDiT [Peebles and Xie, 2023] architecture for mask-free try-on. Specifically, we implemented two\nvariants based on SD3 and SD3.5 [Esser et al., 2024], constructing a dual-DiT structure analogous to\nthe dual U-Net design. The reference DiT encodes the garment image, while the person image is\nconcatenated with the latent tensor along the sequence dimension and passed to the denoising DiT. To\nintegrate the two branches, we follow a structure-aligned fusion strategy: since the reference DiT and\ndenoising DiT share the same architecture, we concatenate the latent features from the corresponding\ntransformer block of the reference DiT into the denoising DiT block, specifically on the key and value\n(K, V) inputs of the attention layer, before computing self-attention. We also remove the text encoder\ninput entirely, so the joint attention layers in DiT degenerate into pure self-attention. However, both\nversions failed to converge during training. We speculate that directly applying DiT to the mask-free\ntry-on task may be suboptimal, and therefore did not pursue further investigation.\nHow about adopting a refinement mechanism similar to that of SDXL? In addition to our\nproposed refinement approach, we also explored an alternative refinement strategy inspired by\nSDXL [Podell et al., 2025]. In SDXL, the refinement process involves training a separate model that\nfocuses specifically on denoising the final 200 steps of the diffusion process. During inference, noise\ncorresponding to timestep 200 is added to the output of the first-stage model, and the refinement model\nis then used to denoise and generate the final result. We experimented with two variations of this\napproach. In the first variant, the first-stage and refinement-stage outputs share the same resolution.\nHowever, as shown in Subsection 4.3, directly applying a mask-free strategy at high resolution\ninevitably leads to structural distortions, which cannot be effectively corrected by the refinement stage.\nThis led us to the second variant, which we refer to as the noising-denoising strategy. Here, a low-\nresolution result is first generated and then upsampled to high resolution. Noise is added at this stage\n(corresponding to a specific timestep), and the refinement model performs denoising from that point\nonward. While this method can sometimes produce visually plausible results when the target patterns\n15\n--- Page 16 ---\nare simple, as shown in Table 5 and Figure 7, its overall performance remains suboptimal. The reason,\nwe believe, lies in the difference between data distributions involved. Although our residual-guided\ndenoising strategy also begins with adding noise to the low-resolution output, it differs fundamentally\nin how it relates the two stages. In the noising-denoising approach, the refinement model learns to\ndenoise samples drawn solely from the high-resolution distribution, and it lacks an explicit mechanism\nto relate this with the low-resolution inputs. In contrast, our residual-guided denoising bridges the\ngap between the low-resolution and high-resolution distributions. This connection is key: it allows\nthe model to better capture the transformation between the two distributions involved. Importantly,\nwhile the original diffusion process constructs a mapping from a simple, tractable distribution (such\nas Gaussian noise) to a complex data distribution, the residual-guided refinement builds a direct\nbridge between two complex distributions.\nGarment Person Low -Resolution Result Noising -Denoising Result DS-VTON\nFigure 7: Comparison between our baseline and the noising-denoising variant.\nTable 5: Comparison of different refinement strategies on the VITON-HD dataset.\nVersion FID ‚ÜìKID‚Üì\nNoising-denoising way 8.98 0.65\nDS-VTON 8.24 0.31\nBroader impacts. The ability of DS-VTON to generate realistic virtual try-on results at multiple\nresolutions makes it well-suited for practical deployment in e-commerce scenarios, where different\nresolutions are often required across platforms and devices. At the same time, as with other generative\ntechnologies, DS-VTON may raise concerns related to intellectual property and personal privacy. We\nencourage its responsible and ethical use.\nLimitation and Future Work. As discussed in Subsection A.1, one key limitation of our method\nlies in the data generation process. Due to the reliance on synthesized person images, the model may\ninadvertently learn to alter regions unrelated to the garment (e.g., hair, accessories, or background).\nWhile this issue is not severe in most cases, we acknowledge it as a limitation and consider improving\ndata disentanglement and identity preservation an important direction for future work. Another\nlimitation stems from the use of fixed coefficients Œ±andŒ≤to initialize the high-resolution refinement\nstage. Although this static strategy proves effective, it may be overly rigid. In future work, we plan to\ninvestigate adaptive or learnable coefficient scheduling mechanisms, which could offer more flexible\nand content-aware refinement during generation.\nD More experiment results\nIn this section, we present additional qualitative comparison results on the DressCode [Morelli et al.,\n2022] dataset and more comparison results on the VITON-HD [Choi et al., 2021] dataset.\n16\n--- Page 17 ---\nGarment Person CatVTON IDM-VTON FitDiT Leffa DS-VTON(HR) DS-VTON(LR) OOTDiffusion\nFigure 8: More qualitative comparison on the VITON-HD dataset. DS-VTON (LR) denotes the\nlow-resolution output, and DS-VTON (HR) represents the final high-resolution result.\n17\n--- Page 18 ---\nGarment Person CatVTON IDM-VTON FitDiT Leffa DS-VTON(HR) DS-VTON(LR) OOTDiffusion\nFigure 9: Qualitative comparison on the DressCode dataset (Dresses category). DS-VTON (LR)\ndenotes the low-resolution output, and DS-VTON (HR) represents the final high-resolution result.\n18\n--- Page 19 ---\nGarment Person CatVTON IDM-VTON FitDiT Leffa DS-VTON(HR) DS-VTON(LR) OOTDiffusion\nFigure 10: Qualitative comparison on the DressCode dataset (Lower category). DS-VTON (LR)\ndenotes the low-resolution output, and DS-VTON (HR) represents the final high-resolution result.\n19\n--- Page 20 ---\nGarment Person CatVTON IDM-VTON FitDiT Leffa DS-VTON(HR) DS-VTON(LR) OOTDiffusion\nFigure 11: Qualitative comparison on the DressCode dataset (Upper category). DS-VTON (LR)\ndenotes the low-resolution output, and DS-VTON (HR) represents the final high-resolution result.\n20",
  "text_length": 52274
}