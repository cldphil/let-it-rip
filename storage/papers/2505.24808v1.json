{
  "id": "http://arxiv.org/abs/2505.24808v1",
  "title": "RealDrive: Retrieval-Augmented Driving with Diffusion Models",
  "summary": "Learning-based planners generate natural human-like driving behaviors by\nlearning to reason about nuanced interactions from data, overcoming the rigid\nbehaviors that arise from rule-based planners. Nonetheless, data-driven\napproaches often struggle with rare, safety-critical scenarios and offer\nlimited controllability over the generated trajectories. To address these\nchallenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)\nframework that initializes a diffusion-based planning policy by retrieving the\nmost relevant expert demonstrations from the training dataset. By interpolating\nbetween current observations and retrieved examples through a denoising\nprocess, our approach enables fine-grained control and safe behavior across\ndiverse scenarios, leveraging the strong prior provided by the retrieved\nscenario. Another key insight we produce is that a task-relevant retrieval\nmodel trained with planning-based objectives results in superior planning\nperformance in our framework compared to a task-agnostic retriever.\nExperimental results demonstrate improved generalization to long-tail events\nand enhanced trajectory diversity compared to standard learning-based planners\n-- we observe a 40% reduction in collision rate on the Waymo Open Motion\ndataset with RAG.",
  "authors": [
    "Wenhao Ding",
    "Sushant Veer",
    "Yuxiao Chen",
    "Yulong Cao",
    "Chaowei Xiao",
    "Marco Pavone"
  ],
  "published": "2025-05-30T17:15:03Z",
  "updated": "2025-05-30T17:15:03Z",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24808v1",
  "full_text": "arXiv:2505.24808v1 [cs.RO] 30 May 2025RealDrive: Retrieval-Augmented Driving with Diffusion Models Wenhao Ding1Sushant Veer1Yuxiao Chen1Yulong Cao1 Chaowei Xiao1,2Marco Pavone1,3 1NVIDIA Research2University of Wisconsin – Madison3Stanford University wenhaod@nvidia.com Abstract Learning-based planners generate natural human-like driving behaviors by learning to reason about nuanced interactions from data, overcoming the rigid behaviors that arise from rule-based planners. Nonetheless, data-driven approaches often struggle with rare, safety-critical scenarios and offer limited controllability over the generated trajectories. To address these challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG) framework that initializes a diffusion- based planning policy by retrieving the most relevant expert demonstrations from the training dataset. By interpolating between current observations and retrieved examples through a denoising process, our approach enables fine-grained control and safe behavior across diverse scenarios, leveraging the strong prior provided by the retrieved scenario. Another key insight we produce is that a task-relevant retrieval model trained with planning-based objectives results in superior planning performance in our framework compared to a task-agnostic retriever. Experimental results demonstrate improved generalization to long-tail events and enhanced trajectory diversity compared to standard learning-based planners – we observe a 40%reduction in collision rate on the Waymo Open Motion dataset with RAG. 1 Introduction RetrievescenariosAugmentdrivingpolicy Figure 1: The ego vehicle (pink) first re- trieves similar scenarios in the database, then uses the retrieved behaviors to aug- ment the driving policy.Recent advancements in autonomous driving have increas- ingly shifted motion planners towards learning-based [ 1,2] and end-to-end [ 3,4] driving policies that predict planning waypoints directly from vectorized or visual inputs. This trend has largely been fueled by the ease of developing nuanced driving behaviors using data, replacing the need for painstaking manual parameter tuning driven by engi- neering intuition in rule-based planners and the removal of information bottlenecks that arise from hand-designed module interfaces. Despite their promise, learning-based planners suffer from two key limitations: (i) they struggle with generalizing to under-represented (long-tail) scenar- ios in the training dataset [ 5], which in turn affects the overall safety and performance of the autonomous vehicle; (ii) learning-based planners offer limited controllability of the planned trajectories which is usually restricted to route information [ 2] (e.g., lane centerlines, goal positions) or coarse high-level commands [ 6,7] (e.g., go straight slowly, turn left). This work aims to mitigate these issues by incorporating knowledge through a diffusion- based Retrieval-Augmented Generation (RAG) framework, which is illustrated in Figure 1. Preprint. Under review. Intuitively, RAG addresses under-represented scenarios by providing the policy with expert demon- strations from similar situations in the retrieval database. This grounds the policy in real-world expert behavior and offers a strong prior for decision-making. Furthermore, this prior – supplied in the form of retrievals – can be leveraged to exert greater control over the planned trajectories by selecting the retrieved samples for the desired characteristics; for example, selecting samples from a defensive expert can yield more defensive driving behaviors. RAG also improves the diversity of generated trajectories, effectively improving coverage of the feasible trajectory space. In autonomous driving, the integration of RAG has largely been explored with large language models (LLMs) [ 9,10]. Although general-purpose language descriptions and video embeddings that encode scene-level information are effective tools for retrieving similar scenarios, they do so at the expense of overlooking critical task-specific information (e.g., focusing on visual similarity of scenes instead of planning-relevant aspects, such as agent behaviors). Additionally, the mechanism by which LLMs incorporate retrieved examples is typically confined to textual reasoning, whereas trajectory planning requires alignment between current and retrieved scenarios in a shared representation space. To address these challenges, instead of using RAG in conjunction with an LLM, we develop a task-specific embedding model trained with trajectory planning objectives, enabling the extraction of driving-relevant features. Retrieved trajectories are used as initial conditions for a diffusion model [ 11], which refines them into the final trajectory through a denoising process. Although other classes of planners can also be enhanced with retrieval, we choose diffusion-based models as they provide an avenue for persistent injection of the observations and actions from the retrieved scenario during the denoising steps. In particular, to adapt the retrieved trajectory to the current scene and context and generate a reliable output trajectory, we introduce a retrieval interpolation module (RIM). RIM interpolates between the retrieved and current observations and actions by a coefficient that is adapted throughout the diffusion steps according to a sigmoidal scheduler. During inference, multiple retrieved examples can be incorporated to enhance trajectory diversity and robustness. In summary, this work makes the following key contributions: •We introduce a task-specific embedding model optimized for retrieval in planning, capturing crucial information, such as multi-agent interactions and critical objects, for driving policies. •We propose a novel RAG framework integrated with a diffusion model to improve learning-based planning performance and safety. •We demonstrate the benefits of our method on real-world driving datasets, analyze key factors that influence its effectiveness, and offer insights for future research. 2 Related Works 2.1 Learning-based Driving Planner Learning-based planning methods [ 12] are attracting increasing attention due to their adaptability and broader coverage compared to traditional rule-based approaches [ 13]. In the early stages of deep learning, initial research [ 14,15] investigated the use of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for training end-to-end driving planners. More recently, transformer-based policies have gained prominence in this field, driven by their success in sequence modeling within natural language processing [ 16]. Transfuser [ 3] integrates RGB image and LiDAR data to train an effective planning model in the Carla simulator [ 17]. UniAD [ 4] introduces a modular transformer-based architecture using image-only inputs, achieving significant improvements across multiple tasks on the nuScenes dataset [ 18]. V AD [ 19] and its successor V AD v2 [ 20] streamline the model design by decoupling the map and motion networks for parallel processing. PARA-Drive [ 21] advances this further by fully parallelizing the entire architecture, enhancing BEV feature extraction, and setting a new benchmark in performance. Although transformer-based policies provide an avenue for injecting vision-language and other high-level features, their ability to control the generation of the low-level trajectories is limited. Diffusion models – originally invented for image generation [ 11] – provide a way to exert greater control over the generated trajectories while still benefiting from transformer-based encoders. Their ability to capture multi-modality and to follow instructions [ 22] have led to their rapid adoption for autonomous driving. For instance, MotionDiffusion [ 23] and CTG [ 24] employ diffusion models to enhance the diversity of traffic scenario generation. DiffusionDrive [ 25] introduces the Trun- cated Diffusion Model, which integrates multi-modal anchors to learn a diverse action distribution. 2 Diffusion-ES [ 26] demonstrates the model’s capacity for zero-shot instruction following in planning tasks. Likewise, Diffusion Planner [ 2] achieves safe and adaptable planning by jointly modeling prediction. Our interest in diffusion policies in this paper arises from their ability to inject prior knowledge in the denoising process, which also makes them suitable for RAG. 2.2 Retrieval Augmented Generation (RAG) RAG is a widely adopted approach in LLMs, leveraging external knowledge sources to improve the quality of generated responses. Owing to its effectiveness in utilizing information from training data, RAG has been extended to various domains, including reinforcement learning [ 27], molecular generation [ 28,29], and robotic manipulation [ 30,31,32]. In particular, tasks involving physical interaction with the environment require not only static information retrieval but also the integration of sequential behaviors  and affordance. In the context of autonomous driving, RAG has been explored extensively, particularly in models em- ploying multi-modal LLMs. RAG-Driver [ 9] employs in-context learning to enhance interpretability by mapping video and control signal embeddings into a unified retrieval space. RAC3 [ 33] addresses hallucinations and weak real-world grounding by retrieving corner cases. Driving-RAG [ 10] focuses on improving the embedding, retrieval, and application processes for driving scenarios. Another approach [ 34] retrieves context-specific traffic rules and guidelines to guide vehicle behavior. Beyond traditional text similarity methods, recent studies have introduced advanced retrieval metrics such as Optimal Transport. For example, RALAD [ 35] reduces the domain gap between real and simulated data using Optimal Transport [ 36], while RealGen [ 37] applies the Wasserstein distance to train an embedding model for generating controllable scenarios. Unlike the methods mentioned above, in this paper, we will use RAG with diffusion for planning in autonomous driving. 2.3 Diffusion with Retrieval Augmented Generation While RAG is commonly employed alongside MLLMs, a growing body of research explores its integration with diffusion models across various domains. Blattmann et al. [ 38] enhance image generation by enabling the model to condition on similar examples retrieved from a database, thereby improving both quality and diversity. RAPID [ 39] leverages RAG to improve generation performance while reducing memory requirements, inference costs, and preserving privacy. Tan et al. [ 40] increases the fidelity of clothing image generation by incorporating external knowledge to mitigate structural distortions and hallucinations. ReMoDiffuse [ 41] improves text-driven 3D human motion synthesis by incorporating semantically and kinematically relevant motion samples during the denoising process. Liu et al. [ 42] introduces a reference-guided diffusion model that utilizes retrieved examples to enhance the accuracy of time series forecasting. The most closely related studies to our work use RAG within policy models for real-world interaction tasks. R2-Diff [ 43] employs SDEdit [ 44] to introduce noise into retrieved motion sequences derived from encoded images, followed by a denoising process to generate trajectories for manipulation tasks. A key limitation of SDEdit lies in the challenge of appropriately selecting the noise magnitude. To address this, READ [ 45] performs direct diffusion between retrieved and target motions, enabling a more efficient diffusion pathway and avoiding the need for long forward-reverse processes. Similarly, RAGDP [ 46] introduces two variants of Diffusion Policy [ 47] that utilize retrieved expert demon- strations. However, neither READ [ 45] nor RAGDP [ 46] incorporates the observations of retrieved samples during the generation, leading to a limitation where the models replicate expert behavior without accounting for the relationship between the retrieved and current observations; RealDrive addresses this limitation in the prior literature. 3 Preliminary In this section, we begin by formulating the planning problem addressed in this study, followed by an introduction to a basic diffusion-based planning model, which serves as the baseline. 3.1 Problem Formulation A motion planning system utilizes the historical trajectories of surrounding agents, including the ego vehicle, in conjunction with the map context to generate multiple future trajectories for the ego vehicle. The historical trajectory for an arbitrary timestep tis denoted as x[t−TH:t] n, where TH 3 TemporalMHADiffusionPlannerAgentMHAMapMHAActionDecoderHisttrajMapActionMapHisttrajAction ActionDenoiser Retrieval Interpolation ModuleRetrieveddataOriginaldataNoiseSchedulerHist EncoderMapEncoder DenoisingprocessEncodersPlannerEmbeddingDB KNNEmbeddingModel DataFlowTop-KFigure 2: Training and inference pipeline of RealDrive. After sampling the original data, the embedding model retrieves top-K similar scenarios from the database and uses them as the retrieved data. The historical trajectories and the map go through the encoders and are interpolated in the MHA. After going through the noise scheduler, the actions are interpolated in the action denoiser. represents the number of past time steps and n∈ {1,···, N}indexes the agents. The last point in history is used as the origin of the coordinate system. The predicted trajectory for the ego vehicle is represented as x[t:t+TF], where TFis the prediction horizon. Each time step along the trajectory includes features such as position coordinates, heading, and velocity, which determine the state of an agent. The map context comprises any features that describe the map and its associated aspects, such as lane topology, wait lines, traffic lights, etc. However, for the experiments in this paper we will limit the map context to nearby lane centerlines, denoted by mwith dimensions [S, P, 2], where Sis the number of lane segments, Pis the number of points per segment, and 2corresponds to the xandycoordinates. The observation at time t, denoted as st:= ({x[t−TH:t] n }N n=1, m), includes the historical trajectories up to time tand the map context m. To ensure the smoothness of the planning trajectory, we assume the agents to follow a kinematic bicycle model and use an inverse dynamics model to compute the acceleration and steering rate from the ground truth future trajectory and denote them as action at. During the training, the model outputs a sequence of actions {at}, which is then rolled out to trajectories using a bicycle model F(defined in the Appendix C.2) to calculate the loss function. 3.2 Diffusion Driving Planner Diffusion models [ 11] represent a class of likelihood-based generative models that learn to reverse a predefined Markovian noise process, which incrementally perturbs data with Gaussian noise. In the forward process, an initial action sample a0is transformed over Htimesteps according to: q(ah|ah−1):=N(ah;p 1−βhah−1, βhI), q(ah|a0):=N\u0000 ah;√¯αha0,(1−¯αh)I\u0001,(1) where βh∈[0,1]forms a predefined noise schedule {βh}H h=1, and ¯αh=Qh i=1(1−βi). The reverse (generative) process is parameterized by a neural network fθ, which estimates the original data ˆa0=fθ(ah, h)from its corrupted counterpart. Training involves minimizing the expected mean squared error between the true and predicted clean trajectory from the forward dynamics model F: Lθ:= E (h,ϵ)∼ U({1,···,H})×N(0,I)∥F(a0)−fθ\u0002√¯αhF(a0) +√ 1−¯αhϵ, h\u0003 ∥2. (2) During inference, the process begins with a sample aH∼ N(0,I), and the denoising network fθis applied iteratively to obtain a sequence {ah−1}that converges to a high-quality data sample: pθ(ah−1|ah):=N(ah−1;µh, σ2 hI), (3) where the mean µhand variance σ2 hare given by: µh=√¯αh−1·βh 1−¯αh·fθ(ah, h) +√1−βh·(1−¯αh−1) 1−¯αh·ah, σ2 h=1−¯αh−1 1−¯αh·βh. (4) In this study, the diffusion model serves as the planning model, as illustrated in Figure 2. We adopt the Diffusion Transformer architecture [ 48] as the backbone, utilizing two transformer-based encoders to extract history and map features. For sampling, we employ the DDPM method [ 11] with a cosine noise schedule, following common practices in prior works [49, 47]. 4 Algorithm 1: RealDrive Training Input: Training dataset D, Retrieval dataset Dr, Embedding Model E Output: The denoise model fθ for(s, a)∈ D do Retrieve (sr, ar)← E(s,Dr); Sample h∼ U({1,···, H}); Get noisy action ˜aand˜arusing (1); ˆa←fθ(s,˜a, sr,˜ar, h)with (6) and (7); Roll out ˆxt:t+TFfrom ˆawith dynamics; Update θwithLθin (2);Algorithm 2: RealDrive Inference Input: Retrieval dataset Dr, Embedding Model E, The denoise model fθ Output: Generated action sequence ˆx0:TF Retrieve (sr, ar)← E(s,Dr); Initialize ˜a∼ N(0,I); forh=Hto1do Get noisy action ˜arusing (1); ˆa←fθ(s,˜a, sr,˜ar, h)with (6) and (7); Update ˜awithˆausing (3); Roll out ˆxt:t+TFfrom ˆawith dynamics; 4 Retrieval-Augmented Driving Policy The proposed RealDrive comprises two key components: an embedding model for scenario retrieval and an augmentation model that utilizes retrieved examples to guide planning. The following sections detail the design and functionality of both components. 4.1 Embedding Model for Retrieval While prior work [ 37] has investigated embedding models to assess scenario similarity, it primarily focuses on the interaction between agents rather than decision-relevant information specific to the ego vehicle’s driving task. To address this limitation, we propose leveraging embeddings derived from a planning model to more effectively capture task-specific scenario similarities. Specifically, we adopt the same architecture as the denoising model fθfrom Section 3.2, but modify it to serve as the embedding model E. The input timestep his removed, and the original noisy data input is replaced with a learnable query embedding. The final transformer module produces an output of shape [TF, he], where hedenotes the embedding dimension. We compute the mean across all temporal dimensions, resulting in a fixed-size vector of shape [he]used for similarity measurement. Scenario retrieval is performed using the Euclidean distance between embeddings to identify the top-K most similar cases. For efficient nearest neighbor search, we employ the FAISS library [ 50], which supports multiple fast and approximate retrieval algorithms. 4.2 Augmentation via Retrieval Interpolation Module The central idea of RealDrive is to leverage retrieved scenarios as informative priors and progressively adapt their knowledge to the current driving context. Prior methods [ 45,46] typically transfer only the action from retrieved examples, resulting in models that naively replicate retrieved behaviors without accounting for differences in contextual observations. This approach overlooks the rela- tionship between retrieved and current scenarios, potentially impairing generalization. To address this limitation, we introduce the Retrieval Interpolation Module (RIM), which integrates an implicit reasoning mechanism into the diffusion denoising process by jointly interpolating both observations and actions between the retrieved and current scenarios. We define an interpolation coefficient λ∈[0,1]that governs the degree of transfer from the retrieved scenario. Since the rate of knowledge transfer should align with the denoising progression, we construct a flexible interpolation scheduler correlated with the denoising timestep. Specifically, we define λusing a sigmoid-based heuristic: λ(ˆh) =S(ˆh;n, m) =ˆhn ˆhn+ (1−ˆh)m,ˆh=h−1 H−1∈[0,1], (5) where nandmare hyperparameters that control the curvature near ˆh= 0andˆh= 1, respectively. For observation interpolation, we operate on the output embeddings of the multi-head attention (MHA) layers in the transformer blocks, following prior work demonstrating this strategy’s controllability benefits [ 51]. Given a learnable query q, a key embedding k, and its retrieved counterpart kr, the interpolated output of the original MHA is replaced by the following: RIM (q, k, k r) = (1 −λ)·MHA (q, k, k ) +λ·MHA (q, kr, kr). (6) 5 LanechangeYieldtopedestrianCutinYieldtoright-turnagentYieldtoleft-turnagentFigure 3: Examples of retrieved scenarios. The top row shows the query scenarios, and the bottom row shows the top-1 retrieved scenarios using our task-specific embedding model. This latent-space interpolation supports flexible integration of diverse contextual features without explicit alignment of vehicle counts or lane structures across scenarios. For action interpolation, we operate in the latent space after the denoising step, where both the current and retrieved noisy actions, denoted ˜aand˜ar, are processed by a shared multi-layer perceptron (MLP). The interpolated result that will be used as the query embedding in MHA is computed as: q=RIM (˜a,˜ar) = (1 −λ)·MLP(˜a) +λ·MLP(˜ar), (7) where ˜aand˜arare sampled from the forward process q(ah|a0)as described in Equation (1). Per- forming interpolation in the latent space ensures smooth integration without disrupting the underlying dynamics or spatial semantics. 5 Experiments In this section, we first describe the experimental setup, followed by a presentation of results addressing the following research questions: Q1: How effective is the retrieval model in identifying relevant scenarios? Q2: How does RAG improve planning performance? Q3: What factors influence the effectiveness of the RAG framework? Q4: How does RAG impact inference efficiency? 5.1 Experimental Setup Dataset. We conduct experiments on the nuScenes [ 18] and Waymo Open Motion Dataset [ 52]. The nuScenes dataset includes 700 training scenes and 150 validation scenes, while Waymo comprises 487,004 training scenes and 44,097 validation scenes. Each scene is segmented into clips with a 2-second observation history and a 4-second prediction horizon at a frequency of 10 Hz. The retrieval database is constructed from the training data, with segments from the same scene excluded during retrieval to prevent information leakage. Baselines. To evaluate the impact of RAG, we compare it against a diffusion-based model that omits retrieval augmentation, denoted as w/o RAG. Additionally, we include two alternative inference settings: w/ RAG (Random), where randomly selected samples from the retrieval database are used for augmentation; and w/ RAG (Oracle), where segments augmented with ground-truth future trajectories are retrieved, providing an upper performance bound. To streamline the exposition, we assign the experimental setups a setting index as denoted in the second row of Tables 1 and 2. Metrics. We adopt open-loop planning metrics for quantitative evaluation. Specifically, minADE 6 and minFDE 6represent the minimum average and final displacement errors, respectively, between the ground-truth trajectory and the 6predicted trajectories. The Time to Closest Encounter (TTCE) [ 53] measures the time until the minimum distance between the ego vehicle and surrounding agents, irrespective of collision. The Collision Rate (CR) quantifies the average number of collisions per segment across the 6generated trajectories. 6 Table 1: Open-loop planning evaluation results (mean ±std) on nuScenes dataset Inference w/o RAG w/ RAG w/ RAG (Random) w/ RAG (Oracle) Training w/o RAG w/ RAG w/o RAG w/ RAG†w/o RAG w/ RAG w/o RAG w/ RAG Setting Index 1 2 3 4 5 6 7 8 minADE 6(↓)0.527 ±.0050.542 ±.004 0.570 ±.0030.497 ±.002 0.574 ±.0060.536 ±.004 0.553 ±.0050.487 ±.003 minFDE 6(↓)1.583 ±.0121.582 ±.015 1.733 ±.0091.476 ±.009 1.811 ±.0101.669 ±.008 1.686 ±.0131.460 ±.011 minCR ( ↓) 0.012 ±.0000.019 ±.001 0.014 ±.0000.011 ±.000 0.012 ±.0000.019 ±.001 0.013 ±.0000.012 ±.000 avgCR ( ↓) 0.031 ±.0020.045 ±.002 0.026 ±.0010.020 ±.002 0.049 ±.0010.064 ±.003 0.027 ±.0010.031 ±.002 minTTCE ( ↑)0.119 ±.0030.119 ±.002 0.124 ±.0030.131 ±.002 0.114 ±.0040.109 ±.004 0.130 ±.0020.134 ±.001 avgTTCE ( ↑)0.155 ±.0030.156 ±.004 0.158 ±.0020.161 ±.004 0.157 ±.0040.161 ±.004 0.161 ±.0010.165 ±.002 *Underline means the best result. Bold font means the best result, excluding settings 7 and 8. Numbers are averaged over 5 runs. †The setting of RealDrive. Table 2: Open-loop planning evaluation results (mean ±std) on Waymo dataset Inference w/o RAG w/ RAG w/ RAG (Random) w/ RAG (Oracle) Training w/o RAG w/ RAG w/o RAG w/ RAG†w/o RAG w/ RAG w/o RAG w/ RAG Setting Index 1 2 3 4 5 6 7 8 minADE 6(↓)0.164 ±.0020.163 ±.001 0.168 ±.0010.152 ±.001 0.498 ±.0040.473 ±.003 0.097 ±.0000.069 ±.000 minFDE 6(↓)0.597 ±.0030.593 ±.004 0.626 ±.0040.544 ±.005 1.703 ±.0121.640 ±.015 0.347 ±.0030.230 ±.002 minCR ( ↓) 0.0016 ±.000.0014 ±.00 0.0016 ±.000.0012 ±.00 0.0016 ±.000.0015 ±.00 0.0014 ±.000.0011 ±.00 avgCR ( ↓) 0.0043 ±.000.0046 ±.00 0.0032 ±.000.0026 ±.00 0.0421 ±.000.0401 ±.00 0.0029 ±.000.0021 ±.00 minTTCE ( ↑)0.083 ±.0010.088 ±.000 0.091 ±.0010.095 ±.001 0.065 ±.0010.062 ±.001 0.095 ±.0010.096 ±.000 avgTTCE ( ↑)0.107 ±.0010.108 ±.001 0.108 ±.0000.112 ±.001 0.103 ±.0020.102 ±.002 0.110 ±.0010.114 ±.001 *Underline means the best result. Bold font means the best result, excluding settings 7 and 8. Numbers are averaged over 5 runs. †The setting of RealDrive. 5.2 Effectiveness of the Retrieval Embedding Model (Q1) As the foundation of RealDrive, the quality of the retrieval embedding model plays a critical role in the overall system performance. To assess its effectiveness, we conduct a qualitative analysis by visualizing both the query scenario and its top-1 retrieved counterpart, as shown in Figure 3. The retrieved examples consistently demonstrate strong semantic alignment with the query, capturing not only straightforward behaviors but also complex traffic dynamics such as single-agent maneuvers (e.g., cut-ins) and multi-agent interactions (e.g., yielding to a left-turning vehicle). These observations indicate that the embedding model successfully encodes task-relevant scenario features. 5.3 Impact of RAG on Planning Performance (Q2) Figure 4: Using RAG reduces the num- ber of samples with high collision rate and minade 6. Using RAG with oracle samples can further reduce the number.We evaluate the influence of retrieval augmentation on planning performance using the nuScenes and Waymo datasets, with results summarized in Table 1 and Table 2, respectively. From these results, several consistent trends emerge: RAG Enhances Performance: Comparing Set- ting 1 and Setting 4, we observe that retrieval augmen- tation leads to improvements in both trajectory accuracy and safety metrics. Importance of Joint Training: Set- ting 3 performs worse than Setting 4, underscoring the necessity of training the model jointly with retrieval aug- mentation to effectively utilize the retrieved information. Non-Disruptive Training: Setting 2 performs compa- rably to the baseline, indicating that incorporating RAG during training does not degrade performance in inference w/o RAG. Benefit of High-Quality Retrieval: Setting 8, which uses oracle retrieval with ground-truth future in- formation, achieves the best results across most metrics, demonstrating the upper bound potential of retrieval-based methods. Sensitivity to Retrieval Quality: Settings 5 and 6 highlight that poor-quality retrievals can significantly degrade performance, emphasizing the importance of a well-trained embedding model. Performance in Long-Tail Scenarios. While RAG provides an overall performance gain, its effectiveness truly shines in handling under-represented scenarios in the training data. Figure 4 7 w/RAGw/oRAG Example1Example2Example1Example2Egoplantrajectoryw/RAGw/oRAGFigure 5: The comparison between the planners w/ RAG and w/o RAG. It is shown that using RAG avoids potential collision (left example), reduces out-of-road rate (right example), and increases planning trajectory diversity (both examples). presents the distributions of minADE 6and Collision Rate for the three inference settings described in Table 2. The results reveal that RAG primarily improves performance in the tail of the distribution, significantly reducing both collision frequency and trajectory error in rare, safety-critical cases. These findings support our initial hypothesis in Section 1, which posits that RAG is particularly beneficial in handling under-represented scenarios in the training data. Qualitative Examples. To gain further insight into how RAG influences planning behavior, we present two illustrative examples in Figure 5. In the left scenario, the planner without RAG generates a trajectory that risks collision with a nearby cyclist (orange) and fails to capture the multi-modal nature of the ground-truth trajectory. Augmentation with retrieved examples enables the planner to produce safer and more diverse trajectories. In the right scenario, the planner without RAG generates trajectories that deviate from the drivable area. With RAG, the generated plans are not only more accurate but also remain within valid driving boundaries, highlighting the effectiveness of retrieval-based augmentation in correcting critical planning errors. 5.4 Factors Influencing RealDrive Performance (Q3) Given the numerous design components within RealDrive, we perform ablation studies to identify key factors that significantly affect its performance. Our analysis highlights four major considerations. Quality and Quantity of the Retrieval Database. A comparison between results on Tables 1 and 2 reveals that RAG yields greater improvements on Waymo. This discrepancy is largely attributable to the scale of the training sets: the nuScenes dataset contains only 700 training scenes, which limits the diversity of scenarios and reduces the diffusion model’s capacity to extract informative features from retrievals. In contrast, the more extensive Waymo dataset enables better generalization and more effective retrieval. Furthermore, the use of oracle retrieval, i.e., segments from the exact same scene with access to ground-truth future trajectories, yields the highest performance gains. This observation underscores the potential for further improvements through the construction of a higher-quality and more comprehensive retrieval database. Table 3: Ablation study on Waymo dataset Ablation w/o Obs. w/o Act. w/o TSE Full minADE 6(↓) 0.1604 0.1661 0.1674 0.1524 minFDE 6(↓) 0.5753 0.6117 0.6232 0.5436 minCR ( ↓) 0.0015 0.0016 0.0017 0.0012 avgCR ( ↓) 0.0034 0.0033 0.0035 0.0026 minTTCE ( ↑) 41.162 41.632 41.616 42.029 avgTTCE ( ↑) 43.549 43.598 42.461 44.632Embedding Model. To evaluate the impact of the embedding model on retrieval quality and overall performance, we compare RealDrive against a variant that omits the task-specific embedding model (denoted as w/o TSE). This baseline adopts an encoder-decoder structure for learning scenario embeddings, following the approach in [ 37]. As shown in Table 3, our task- specific embedding model, trained with a planning objective, outperforms the baseline, highlighting the importance of using downstream task representations for effective scenario retrieval. Interpolated Information. RealDrive performs interpolation on both observations and actions during the diffusion denoising process. To assess the contribution of each component, we conduct an ablation study comparing the effects of interpolating observation only, action only, and both. The results in Table 3 indicate that action interpolation is critical to the success of RAG in driving tasks. Nonetheless, combining observation and action interpolation yields the best performance, supporting our hypothesis that observation-level adaptation enhances the planner’s ability to effectively leverage the retrieved action. Design of the Interpolation Scheduler. An important design factor in RealDrive is the interpolation scheduler, which is defined by the sigmoid function in (5). This function governs the evolution 8 of the interpolation coefficient λacross the denoising steps h, with its shape controlled by two parameters. To evaluate its influence, we perform a parameter sweep and present the corresponding minADE 6values in Figure 6, where different colors denote performance across various scheduler configurations. Our analysis yields two key insights. First, scheduler curves that remain flatter near h= 0are associated with improved planning performance. This suggests that interpolation should begin early, before significant denoising, so that the denoiser has sufficient time to adjust and refine the transferred action. Second, steeper scheduler curves tend to degrade performance, likely due to abrupt interpolation transitions that introduce significant deviations between consecutive states. These findings emphasize the importance of a gradual, well-paced interpolation schedule in enhancing retrieval-augmented planning. 5.5 Impact of RAG on Inference Time (Q4) Bestresult(n,m)=(2,1) Figure 6: minADE 6of different in- terpolation schedulers on Waymo. When the curve is flatter close to h= 0, the performance is better.A critical requirement for driving planners is low-latency in- ference. However, incorporating retrieval during inference introduces additional computational overhead. To quantify this impact, we perform a detailed analysis of retrieval latency under various configurations, including embedding dimensionality, the size of the retrieval database, and the number of query samples. Without retrieval, the base diffusion planner requires 0.0154 seconds per inference. When retrieval is incorporated, the total inference time increases to 0.0246 seconds. As illus- trated in Figure 7, under the configuration used in our setting on the Waymo dataset (embedding dimension of 128, 107re- trieval entries, and 6 query samples), the retrieval step alone takes approximately 0.018 seconds per query. This latency is comparable to that of the planner itself, indicating that RAG can be feasibly integrated into real-time planning pipelines without introducing prohibitive delays. 1076128 Figure 7: Time statistics of retrieving under different configurations. The setting we used for the Waymo dataset takes 0.0246 seconds. 6 Conclusion and Limitation In this work, we present RealDrive, a novel Retrieval-Augmented Generation (RAG) framework for trajectory planning that combines task-specific retrieval with a diffusion model. Our approach addresses two key limitations of learning-based planners: limited generalization to rare, safety-critical scenarios and insufficient control over trajectory generation. By retrieving expert demonstrations relevant to the current driving context and interpolating them with the agent’s observations during the denoising process, RealDrive facilitates more reliable and controllable planning outcomes. Experi- ments conducted on two open-sourced datasets demonstrate that RealDrive substantially enhances trajectory diversity and generalization, reducing collision rates by up to 40 %in long-tail scenarios. Despite its demonstrated effectiveness, RealDrive exhibits two primary limitations. First, maintaining a large retrieval dataset imposes significant demands on on-device memory and can hinder real-time inference performance. Addressing this issue requires either selecting representative samples or compressing raw scenarios into structured memory. Second, the current implementation of RealDrive operates on vectorized inputs. Future research will focus on extending the framework to accommodate video input, enabling support for fully end-to-end autonomous driving systems. 9 References Jie Cheng, Yingbing Chen, and Qifeng Chen. Pluto: Pushing the limit of imitation learning- based planning for autonomous driving. arXiv preprint arXiv:2404.14327, 2024. Yinan Zheng, Ruiming Liang, Kexin Zheng, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, et al. Diffusion-based planning for autonomous driving with flexible guidance. arXiv preprint arXiv:2501.15564, 2025. Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE transactions on pattern analysis and machine intelligence, 45(11):12878–12895, 2022. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17853–17862, 2023. Wenhao Ding, Chejian Xu, Mansur Arief, Haohong Lin, Bo Li, and Ding Zhao. A survey on safety-critical driving scenario generation—a methodological perspective. IEEE Transactions on Intelligent Transportation Systems, 24(7):6971–6988, 2023. Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L Waslander, Yu Liu, and Hong- sheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15120–15130, 2024. Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2, 2023. Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. arXiv preprint arXiv:2402.10828, 2024. Cheng Chang, Jingwei Ge, Jiazhe Guo, Zelin Guo, Binghong Jiang, and Li Li. Driving-rag: Driving scenarios embedding, search, and rag applications. arXiv preprint arXiv:2504.04419, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. Ardi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro Fishman, and Naveed Muhammad. A survey of end-to-end driving: Architectures and training methods. IEEE Transactions on Neural Networks and Learning Systems, 33(4):1364–1384, 2020. Frédéric Bouchard, Sean Sedwards, and Krzysztof Czarnecki. A rule-based behaviour planner for autonomous driving. In International Joint Conference on Rules and Reasoning, pages 263–279. Springer, 2022. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 10  Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 1–16. PMLR, 2017. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020. Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8350, 2023. Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Par- allelized architecture for real-time autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15449–15458, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9644–9653, 2023. Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In 2023 IEEE international conference on robotics and automation (ICRA), pages 3560–3566. IEEE, 2023. Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, et al. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. arXiv preprint arXiv:2411.15139, 2024. Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-Wei Ke, Ayush Jain, Jeff Schneider, and Katerina Fragkiadaki. Diffusion-es: Gradient-free planning with diffusion for autonomous driving and zero-shot instruction following. arXiv preprint arXiv:2402.06559, 2024. Anirudh Goyal, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, et al. Retrieval-augmented reinforcement learning. In International Confer- ence on Machine Learning, pages 7740–7765. PMLR, 2022. Seul Lee, Karsten Kreis, Srimukh Veccham, Meng Liu, Danny Reidenbach, Saee Paliwal, Arash Vahdat, and Weili Nie. Molecule generation with fragment retrieval augmentation. Advances in Neural Information Processing Systems, 37:132463–132490, 2024. Zichen Wang, Yaokun Ji, Jianing Tian, and Shuangjia Zheng. Retrieval augmented dif- fusion model for structure-informed antibody design and optimization. arXiv preprint arXiv:2410.15040, 2024. Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, and Yue Wang. Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. arXiv preprint arXiv:2407.04689, 2024. Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, and Jonathan Francis. Strap: Robot sub-trajectory retrieval for augmented policy learning. arXiv preprint arXiv:2412.15182, 2024. 11  Yijie Guo, Bingjie Tang, Iretiayo Akinola, Dieter Fox, Abhishek Gupta, and Yashraj Narang. Srsa: Skill retrieval and adaptation for robotic assembly tasks. arXiv preprint arXiv:2503.04538, 2025. Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, and Hong Chen. Rac3: Retrieval-augmented corner case comprehension for autonomous driving with vision-language models. arXiv preprint arXiv:2412.11050, 2024. Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z Zhao, Zhiwen Wu, and Jiaqi Ma. Driving with regulation: Interpretable decision-making for autonomous vehicles with retrieval- augmented reasoning via llm. arXiv preprint arXiv:2410.04759, 2024. Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, and Chun Jason Xue. Ralad: Bridging the real-to-sim domain gap in autonomous driving with retrieval-augmented learning. arXiv preprint arXiv:2501.12296, 2025. Filippo Santambrogio. Optimal transport for applied mathematicians, volume 87. Springer, 2015. Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao, and Marco Pavone. Realgen: Retrieval augmented generation for controllable traffic scenarios. In European Conference on Computer Vision, pages 93–110. Springer, 2024. Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval- augmented diffusion models. Advances in Neural Information Processing Systems, 35:15309– 15324, 2022. Tanqiu Jiang, Changjiang Li, Fenglong Ma, and Ting Wang. Rapid: Retrieval augmented training of differentially private diffusion models. arXiv preprint arXiv:2502.12794, 2025. Xianfeng Tan, Yuhan Li, Wenxiang Shang, Yubo Wu, Jian Wang, Xuanhong Chen, Yi Zhang, Ran Lin, and Bingbing Ni. Ragdiffusion: Faithful cloth generation via external knowledge assimilation. arXiv preprint arXiv:2411.19528, 2024. Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364–373, 2023. Jingwei Liu, Ling Yang, Hongyan Li, and Shenda Hong. Retrieval-augmented diffusion models for time series forecasting. Advances in Neural Information Processing Systems, 37:2766–2786, 2024. Takeru Oba and Norimichi Ukita. R2-diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction. arXiv preprint arXiv:2306.09483, 2023. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Takeru Oba, Matthew Walter, and Norimichi Ukita. Read: Retrieval-enhanced asymmetric diffusion for motion planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17974–17984, 2024. Sodtavilan Odonchimed, Tatsuya Matsushima, Simon Holk, Yusuke Iwasawa, and Yutaka Matsuo. Ragdp: Retrieve-augmented generative diffusion policy. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4195–4205, 2023. 12  Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547, 2019. Jiaojiao Fan, Haotian Xue, Qinsheng Zhang, and Yongxin Chen. Refdrop: Controllable consistency in image or video generation via reference feature guidance. arXiv preprint arXiv:2405.17661, 2024. Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9710–9719, 2021. Julian Eggert. Predictive risk estimation for intelligent adas functions. In 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), pages 711–718. IEEE, 2014. 13 A Potential Societal Impacts While RealDrive improves planning performance and safety by leveraging retrieved expert demon- strations, it also introduces risks from reliance on retrieval-based priors. In particular, if the retrieval database contains examples that encode undesirable or unsafe behaviors – either unintentionally or through adversarial poisoning – the planner may generate risky trajectories that mirror those examples. As a mitigation strategy, it is essential to implement rigorous filtering and validation procedures to ensure that retrieved trajectories align with safety constraints and traffic rules. Furthermore, inter- pretability tools and uncertainty estimation should be employed to monitor and audit the influence of retrieved samples during deployment. B More Experiment Results B.1 Qualitative examples of retrieval model We provide more examples to demonstrate the quality of our retrieval embedding model in Figure 8. QueryRetrievalQueryRetrieval Figure 8: More examples of the retrieval model. B.2 Comparison between w/ RAG and w/o RAG models on long-tail scenarios. We find the top 50 samples according to the minADE of the w/o RAG model and plot them in Figure 9. We also plot the minADE results for the same samples using the w/ RAG model. We find that using RAG significantly reduces the minADE. 14 Figure 9: Top 50 samples with largest minADE of w/o RAG model. B.3 Qualitative examples of model outputs We provide more examples that compare the trajectory outputs from models without RAG and with RAG in Figure 10. The model using RAG generates more diverse and safe planning trajectories. Example1Example2Example1Example2w/RAGw/oRAGEgoplantrajectoryw/RAGw/oRAG Example1Example2Example1Example2Example1Example2Example1Example2 Figure 10: More examples of the comparison between different model outputs. C Experiment Details C.1 Details of Metric The minADE kand minFDE kmetrics are defined by the following equation: minADE k=1 NTFmin kNX n=1TFX t=1∥ˆxn,t−xk n,t∥2, (8) minFDE k=1 Nmin kNX n=1∥ˆxn,TF−xk n,TF∥2, (9) where xk n,tis the k-th sample for the agent nat timestep t. The minimal collision rate (CR) and average collision rate metrics are defined by: minCR = min k( 1,ifPN n=2PTF t=1Collision (x1,t, xn,t)>0, 0,otherwise,(10) avgCR =KX k=1( 1,ifPN n=2PTF t=1Collision (x1,t, xn,t)>0, 0,otherwise,(11) 15 where the function Collision (x1,t, xn,t)outputs 1 if the distance between two vehicles are smaller than a threshold. The Time to closest encounter (TTCE) metric we used in the experiments is calculated by: TTCE = min\u0012 max\u0012(p·v) ∥v∥2+δ,0\u0013, τmax\u0013, (12) where p=xagent−xegois the relative position vector, v=vagent−vegois the relative velocity vector, δis a small positive constant 10−8) to ensure numerical stability, and τmaxis the maximum allowed TTCE, which is set to 4 in our experiments. C.2 Forward Dyanmics Model We consider a bicycle model as our forward dynamics, which has the following transition function: F(at) =F(ut, δt) =  xt+1=xt+htcos(θt)·∆t yt+1=yt+htsin(θt)·∆t ht+1=ht+ut·∆t θt+1=θt+ht Ltan(δt)·∆t(13) where Lis the wheelbase of the vehicle in meter, δis the steering angle of the front wheels, and uis the longitudinal acceleration. C.3 Compute Resources Experiments are conducted on a desktop with AMD Ryzen 9 7950X 16-Core Processor, NVIDIA RTX 6000 Ada Generation GPU with 48GB memory, and 128GB memory. The training on the Waymo dataset takes about 72 hours. C.4 Hyperparameter We summarize all hyperparameters used in the experiments in Table 4. Note that we use identical hyperparameters for the nuScenes and Waymo datasets, except for the learning rate scheduler and batch size. For the RAG part, we only introduce two additional hyperparameters mandn. Table 4: Hyperparameters of experiments Notation Description Value (nuScenes) Value (Waymo) ∆t time interval between two waypoints in the trajectory 0.1 0.1 step size of segment window 0.1 0.1 Th time length of history (second) 2 2 Tf time length of future (second) 4 4 A max number of surrounding agents in the scene 20 20 S max number of lane in the map 100 100 P number of points per lane 50 50 Tmax total training steps 2×1042×105 B batch size 64 128 lr initial learning rate 5×e−45×e−4 multi-step learning rate scheduler [3,6,9,12,15]×103[2,4,6,8,10]×104 learning rate scheduler decay discount 0.5 0.5 weight decay of AdamW optimizer 0.01 0.01 max gradient clip value 5 5 discount of state loss over time steps 0.95 0.95 H diffusion step 10 10 he dimension of hidden embedding 128 128 hff dimension of the feedforward network in attention 512 512 hn number of head in attention 8 8 number of encoder layer 1 1 number of decoder layer 2 2 dropout rate 0.1 0.1 (n, m) parameter of the interpolation scheduler (2,1) (2,1) 16",
  "text_length": 52832
}