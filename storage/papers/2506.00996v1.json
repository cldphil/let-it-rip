{
  "id": "http://arxiv.org/abs/2506.00996v1",
  "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
  "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
  "authors": [
    "Kinam Kim",
    "Junha Hyung",
    "Jaegul Choo"
  ],
  "published": "2025-06-01T12:57:43Z",
  "updated": "2025-06-01T12:57:43Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00996v1",
  "full_text": "--- Page 1 ---\nTemporal In-Context Fine-Tuning for Versatile\nControl of Video Diffusion Models\nKinam Kim∗Junha Hyung∗Jaegul Choo\nKAIST AI\n{kinamplify, sharpeeee, jchoo}@kaist.ac.kr\nAbstract\nRecent advances in text-to-video diffusion models have enabled high-quality video\nsynthesis, but controllable generation remains challenging—particularly under\nlimited data and compute. Existing fine-tuning methods for conditional generation\noften rely on external encoders or architectural modifications, which demand large\ndatasets and are typically restricted to spatially aligned conditioning, limiting flexi-\nbility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning\n(TIC-FT), an efficient and versatile approach for adapting pretrained video diffu-\nsion models to diverse conditional generation tasks. Our key idea is to concatenate\ncondition and target frames along the temporal axis and insert intermediate buffer\nframes with progressively increasing noise levels. These buffer frames enable\nsmooth transitions, aligning the fine-tuning process with the pretrained model’s\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10–30 training samples. We validate our method across\na range of tasks—including image-to-video and video-to-video generation—using\nlarge-scale base models such as CogVideoX-5B and Wan-14B. Extensive experi-\nments show that TIC-FT outperforms existing baselines in both condition fidelity\nand visual quality, while remaining highly efficient in both training and inference.\nFor additional results, visit https://kinam0252.github.io/TIC-FT/ .\n1 Introduction\nText-to-video generation models have advanced rapidly, reaching quality levels suitable for profes-\nsional applications [ 1,2,3,4,5,6]. Beyond basic generation, recent research has increasingly\nfocused on leveraging pretrained models to enable more precise control and conditional guid-\nance, addressing the growing demand for finer adjustments and more nuanced generation capa-\nbilities [7, 8, 9, 10, 11, 12, 13, 14, 15, 16].\nDespite this progress, current fine-tuning approaches for conditioning video diffusion models face\nnotable limitations. Many methods require large training datasets and introduce additional archi-\ntectural components, such as ControlNet [ 7] or other external modules, which impose substantial\nmemory overhead. Moreover, the reliance on external encoders for conditioning often leads to the\nloss of fine-grained details during the encoding process. ControlNet-style methods [ 16,17,14], in\nparticular, operate within rigid conditioning frameworks: they are primarily designed for spatially\naligned conditions and require conditioning signals to match the target video length. For example,\nwhen conditioning on a single image, common workarounds include replicating the image across\nthe temporal dimension to align with the video frames or embedding it as a global feature. These\napproaches typically necessitate task-specific adaptations of the conditioning pipeline. Alternative\n* indicates equal contribution.\nPreprint. Under review.arXiv:2506.00996v1  [cs.CV]  1 Jun 2025\n--- Page 2 ---\nPr ompt: [CHARA CTER]: A jo yful y oung girl wit h long br o wn hair and r osy cheeks, w earing a pale gr een blouse and a t eal skir t decorat ed wit h grass patt erns, smiling ...\nPr ompt: [CHARA CTER]: A car t oon giraff e wit h orange spot s, lar ge r ound glasses, and a shin y brass jetpack on it s back, ho v ering midair wit h flames bursting beneat h...\n \nPr ompt: [IMA GE]: A w oman wit h a shor t bob hairstyle smiles gent ly . Then ...Pr ompt: [IMA GE]: A smiling w oman w ears a whit e cr op t op. Then transition t o ...\nPr ompt: [VIDEO1]: An elderly chef in a whit e unif orm and hat r olls out pizza dough ...\nPr ompt: [F A CE]: A man in his lat e 20s wit h smoot h, clear skin, neat ly par t ed black hair , and a soft...\nPr ompt: [STYLE]: A painting in t he style of v an gogh. Then ...Pr ompt: [CO N DITIO N ]: A gr oup of packaged f ood pr oduct s....Figure 1: Demonstration of our method across diverse tasks, including character-to-video, virtual\ntry-on, ad-video generation, object-to-motion, toonification, and video style transfer.\nfine-tuning strategies, such as IP-Adapter [ 18] and latent concatenation [ 10], encounter similar chal-\nlenges regarding flexibility and computational cost, as they modify or expand the pretrained model\narchitectures.\nIn contrast, in-context learning (ICL) [ 19] offers a more efficient and versatile paradigm. ICL is\ntraining-free and can be flexibly applied to user-defined tasks by providing examples directly within\nthe input context, eliminating the need for additional parameter updates. While ICL has shown strong\nsuccess in large language models [ 19,20], its application to image and video generation has primarily\nbeen explored in autoregressive models [21, 22], with limited adaptation to diffusion models.\nEfforts to implement ICL in diffusion models [ 23,15] have often relied on ControlNet-style training\napproaches, which contradict the core advantage of ICL: leveraging pretrained distributions without\nadditional training. Departing slightly from the pure ICL paradigm, recent work has introduced\nin-context LoRA [ 8], a related technique that enables consistent image generation by producing\nmultiple images in a single forward pass arranged in grids, thereby facilitating information sharing\nacross images. With minimal fine-tuning, this method achieves high-quality and highly consistent\nresults, benefiting from the inherent in-context generation capabilities of pretrained text-to-image\nmodels, which are naturally suited for grid-based generation.\nIn contrast, video generation models possess far less of this capability. Although concurrent research\nhas explored extending in-context LoRA to video generation [ 24], these models are poorly suited\nto producing grid-like outputs, making the approach significantly more training-intensive and less\neffective. Furthermore, these methods are not inherently designed for conditional generation and\noften depend on training-free inpainting strategies [ 25,26], which tend to degrade performance. They\nalso lack flexibility in handling mismatches between the condition length and the number of target\nframes, as there are no straightforward solutions for general cases. In the simple case of conditioning\non a single image, the image must be redundantly replicated across all frames, resulting in substantial\nincreases in memory usage and computational overhead.\n2\n--- Page 3 ---\nIn this paper, we propose a highly effective and versatile fine-tuning method for conditional video\ndiffusion models: temporal in-context fine-tuning. Instead of spatially concatenating condition\nand target inputs, our approach aligns them temporally—concatenating condition frame(s) and\ntarget frame(s) along the time axis—and fine-tunes the model using only a minimal number of\nsamples. This design leverages the inherent capability of pretrained video diffusion models to\nprocess temporally ordered inputs, enabling effective generation when condition and target frames\nare arranged sequentially.\nTo ensure a smooth transition between the condition and target frames, we introduce buffer\nframes—intermediate frames with monotonically increasing noise levels that bridge the gap be-\ntween the clean condition frames and the fully noised target frames. These buffer frames facilitate\nsmooth, natural fade-out transitions from condition to generated frames, preventing abrupt scene\ntransitions and preserving consistency with the pretrained model’s distribution. Combined with this\ndesign, our method enables fine-tuning with as few as 10–30 training samples. Additionally, our\nmethod preserves the original model architecture without introducing additional modules, thereby\nreducing VRAM requirements.\nThe proposed approach also allows the model to leverage condition frames directly through unified 3D\nattention, avoiding the detail loss typically introduced by external encoders. Furthermore, it enables\nversatile conditional generation by eliminating the need for spatial alignment and accommodating a\nwide range of condition lengths—from single images to full video sequences—thereby supporting\ndiverse video-to-video translations and image-to-video generation tasks.\nIn summary, our main contributions are as follows:\n•We propose temporal in-context fine-tuning , a simple yet highly effective method for con-\nditional video diffusion that minimizes the distribution mismatch between pretraining and\nfine-tuning, without requiring architectural modifications.\n•We demonstrate strong performance with minimal training data (10–30 samples), offering a\nhighly efficient fine-tuning strategy.\n•Our method enables versatile conditioning, supporting variable-length inputs and unifying\ndiverse image- and video-conditioned generation tasks within a single framework.\n•We validate our method across a wide range of tasks, including reference-to-video generation,\nmotion transfer, keyframe interpolation, and style transfer with varying condition content\nand lengths.\n2 Related work\nConditional Video Diffusion Models. Many conditional video generation methods [7, 8, 9, 10, 11,\n12,13,14,15,16] rely on auxiliary encoders (e.g., ControlNet [ 7]) or architectural modifications\n(e.g., IP-Adapter [ 18]), which prevent full exploitation of the pretrained model’s capabilities. These\napproaches typically require larger datasets, longer training, and incur significant memory overhead.\nMoreover, they are often limited to spatially aligned conditioning, making them less suitable for\nvariable-length or misaligned condition–target pairs.\nIn-Context Learning for Diffusion Models. Inspired by its success in language models [ 19,20],\nin-context finetuning (IC-FT) has been explored in visual domains via grid-based generation [ 8,24],\nbut its extension to video is limited. Videos rarely follow grid layouts, and inference methods like\nSDEdit [ 25] degrade output quality. Moreover, these approaches assume strict condition–output\nalignment, making them unsuitable for flexible conditional video generation.\nDiffusion with Heterogeneous Noise Levels. Recent works such as FIFO-Diffusion [27] and\nDiffusion Forcing [28] demonstrate that diffusion models can effectively operate on sequences with\nvarying noise levels across frames or tokens—challenging the conventional assumption of uniform\nnoise and motivating our use of buffer frames with progressively increasing noise.\nBuilding on these ideas, we propose Temporal In-Context Fine-Tuning (TIC-FT) —a simple yet\neffective method that temporally concatenates condition and target frames, inserting buffer frames\nwith increasing noise levels to smooth abrupt transitions in both scene content and noise levels.\n3\n--- Page 4 ---\nUnlike ControlNet-style methods, TIC-FT requires no architectural changes and naturally supports\nvariable-length, spatially misaligned condition–target pairs.\n3 Method\n3.1 Preliminaries\nWe briefly review diffusion-based text-to-video generation. A video with FinRGB frames is x1:Fin∈\nRFin×3×Hpix×Wpix. A spatio-temporal encoder ϕmaps it to latents z(0)=z(0)\n1:F=ϕ(x1:Fin)∈\nRF×C×H×WwithF≤Fin,H≤Hpix, and W≤Wpix, and a decoder ψapproximately inverts ϕ.\nLatent frames are diffused by q\u0000\nz(0), t\u0001\n:=z(t)=αtz(0)+σtεfort∈ {0, . . . , T }andε∼ N(0,I),\nwith a predefined schedule (αt, σt). A DiT[29] backbone ϵθpredicts the noise and is trained with\nLdiff=Ez(0),c,ε,th\r\rε−ϵθ(z(t), t,c)\r\r2\n2i\n, (1)\nwhere latent frames and paired text condition cis sampled from the dataset. Generation starts\nfromz(T)∼ N(0,I)and iteratively applies a sampler z(t−1)=S\u0000\nz(t), t,c;ϵθ\u0001\nuntilz(0), which ψ\ndecodes to video.\n3.2 Temporal concatenation\nOverview We introduce overall pipeline of the proposed temporal in-context fine-tuning (TIC-FT)\nin this section. We first detail the temporal concatenation of condition and target latents- with buffer\nframes that ease the abrupt scene and noise-level transition—followed by the inference andtraining\nprocedures formalized in Algorithms 1–2.\nSetup. The task is to generate a sequence of target frames of length K, denoted as ˆz(0)=\n[ˆz(0)\nL+1, . . .ˆz(0)\nL+K], conditioned on a set of input frame(s): ¯z(0)= [¯z(0)\n1, . . .¯z(0)\nL]. Our approach\nconcatenates the condtion and target frames along the temporal axis. A naïve formulation simply\nplaces the clean condition frames directly before the noisy target frames:\nz(t)=¯z(0)\n1:L|{z}\ncondition∥ˆz(t)\nL+1:L+K|{z}\ntarget∈R(L+K)×C×H×W. (2)\nHere, ˆz(t)\nL+1:L+Krepresents the target latent frames at denoising timestep t.\nAt inference time, we initialize with ¯z(0)∥ˆz(T)and iteratively denoise the concatenated frames with\n¯z(0)∥ˆz(t−1)=S\u0000¯z(0)∥ˆz(t), t,c;ϵθ\u0001\n(3)\nuntil reaching z(0)=¯z(0)∥ˆz(0). At each denoising step, only the Ktarget frames are denoised, while\nthe condition frames are fixed to enforce consistency. The final output video corresponds to the target\nslicez(0)\nL+1:L+K.\nThe flexibility of varying Lallows this formulation to generalize across a wide range of conditional\nvideo generation tasks. When L= 1, the problem becomes an image-to-video generation task:\nproducing a full video sequence from a single reference image together with a text description of the\nsequence.\n3.3 Buffer frames\nUnlike conventional image-to-video (I2V) approaches, where the condition acts as the first frame of\nthe output, our setup also allows for discontinuous conditioning, broadening its applicability. For\nL > 1, the method naturally extends to video-to-video generation. A reference clip can perform\nvideo style transfer by transferring its appearance onto a new motion sequence. Likewise, providing\nan action snippet along with a query frame enables in-context action transfer , where the observed\nmotion is adapted to a novel scene. Supplying sparsely sampled frames supports keyframe inter-\npolation , allowing the model to smoothly generate intermediate transitions between distant frames.\n4\n--- Page 5 ---\nAlgorithm 1: TIC-FT inference\nInput: Clean condition latents ¯z(0); buffer noise levels ˜τ1:B; text prompt c; denoiser ϵθ\nOutput: Denoised target latents ˆz(0)=z(0)\nL+B+1:L+B+K\nGenerate buffer latents ˜z(˜τ1:B)=q\u0000¯z(0),˜τ1:B\u0001\n; // add noise\nSample target latents ˆz(T)∼ N(0,I);\nConcatenate z(T)←¯z(0)∥˜z(˜τ1:B)∥ˆz(T);\nfort=Tto1do // global time descending\nt← T\u0000\nz(t)\u0001\n; // noise-level vector\nA ← { i|ti=t};\nz(t−1)\nA← S\u0000\nz(t), t,c;ϵθ\u0001\nA;\nreturn z(0)\nL+B+1:L+B+K\nThus, simple temporal concatenation serves as a unified and highly versatile framework for diverse\nconditional video generation tasks.\nHowever, this naïve approach is suboptimal for fully leveraging the capabilities of the pretrained\nvideo diffusion model. Aligning the finetuning task as closely as possible with the pretrained model’s\ndistribution is essential to achieve high efficiency—enabling strong performance with minimal data\nand computational resources. Thus, it is desirable to design the finetuning process around tasks the\nmodel is already proficient at.\nDirect concatenation violates this principle in two key ways. First, in scenarios where the target frames\ndo not naturally continue from the condition frames—i.e., when there is an abrupt scene transition\nbetween the last condition frame and the first target frame—the model is forced to synthesize highly\ndiscontinuous content. Pretrained video diffusion models are typically trained on smoothly evolving\nsequences and lack the inherent capability to handle such abrupt transitions, as datasets with sudden\nscene changes are commonly filtered out during data curation. Second, diffusion models are not\ndesigned to denoise sequences containing frames with heterogeneous noise levels, as would occur\nwhen combining clean condition frames with noisy target frames during the sampling process.\nWe therefore introduce Bintermediate frames whose noise levels ˜τblinearly bridge 0andT:\n˜z(˜τ1:B)=\u0002˜z(˜τ1)\n1, . . . , ˜z(˜τB)\nB\u0003\n, ˜τb=b\nB+ 1T. (4)\nThere can be different design choices for the buffer frames, and we empirically find that using the\nnoised condition frames, ˜z(t)=¯z(t), yields a good performance. Then the full initial latent sequence\nbecomes\nz(T)=¯z(0)\n1:L|{z}\ncondition∥˜z(˜τ1:B)\nL+1:L+B|{z}\nbuffer∥ˆz(T)\nL+B+1:L+B+K|{z }\ntarget. (5)\n3.4 Inference\nLetT(z(t))be a noise level list corresponding to the latent sequence z(t):T:RF×C×H×W−→\n{0, . . . , T }F. The initial noise level list at t=Tis\nT\u0000\nz(T)\u0001\n=\u0002\n0,˜τ1, . . . , ˜τB, T, . . . , T\u0003\n∈ {0, . . . , T }L+B+K. (6)\nAt any global timestep t, we define the noise levels as:\nT\u0000\nz(t)\u0001\n=\u0002\n0, τ1(t), . . . , τ B(t), t, . . . , t\u0003\n, (7)\nwhere τb(t) = ˜τbif˜τb< t, and τb(t) =totherwise.\nOur inference algorithm proceeds by iteratively identifying the frames currently at the maximal noise\nleveltand applying the video diffusion sampler exclusively to those frames. This process continues\nfrom t=Tdown to t= 0. The full inference procedure is detailed in Algorithm 1.\n5\n--- Page 6 ---\nAlgorithm 2: TIC-FT training\nInput: Dataset Dwith tuples (¯z(0),ˆz(0),c); buffer levels ˜τ1:B; noise schedule (αt, σt)\nOutput: Fine-tuned parameters θ\nforeach minibatch (¯z(0),ˆz(0),c)∼ D do\nforeach sample in minibatch do\nSample t∼ U{ 1, . . . , T }andε∼ N(0,I);\n˜z(τ1:B(t))←q\u0000¯z(0), τ1:B(t)\u0001\n;\nˆz(t)←αtˆz(0)+σtε;\nz(t)←¯z(0)∥˜z(τ1:B(t))∥ˆz(t);\nˆε←ϵθ(z(t), t,c);\nL ←1\nKPL+B+K\ni=L+B+1∥εi−ˆεi∥2\n2;\nUpdate θusing gradients of L;\n3.5 Training\nFor each video–text pair\u0000¯z(0),ˆz(0),c\u0001\n∼ D, the training proceeds as follows. First, we randomly\nsample a global timestep t∼ U{ 1, . . . , T }and Gaussian noise ε∼N(0,I). Next, we construct the\nnoised model input sequence z(t)with the noise level defined in Eq. 7.\nThe model then predicts the noise ˆε=ϵθ(z(t), t,c)for all frames. However, the loss is computed\nonly over the target frames to avoid enforcing supervision for the buffer frames. Specifically, we\nminimize the mean squared error between the true noise and the predicted noise over the target\nframe indices, defined as L=1\nKPL+B+K\ni=L+B+1\r\rεi−ˆεi\r\r2\n2. The model parameters θare updated via\na gradient step computed from this loss. By excluding the buffer frames from the loss calculation,\nthe network is free to predict whatever is most natural for these frames, thereby preventing spurious\ngradients that could shift the model away from the pretraining distribution. In practice, we observe\nthat the buffer frames often evolve into a smooth fade-out and fade-in transition between the condition\nand target frames. The full training procedure is summarized in Algorithm 2.\n4 Experiments\n4.1 Overview\nWe evaluate our proposed method on two recent large-scale text-to-video generation models:\nCogVideoX-5B and Wan-14B. Our experiments span a diverse range of conditional generation\ntasks, including:\n•Image-to-Video (I2V) : e.g., character-to-video generation, object-to-motion, virtual try-on,\nad-video generation.\n•Video-to-Video (V2V) : e.g., video style transfer, action transfer, toonification.\nA key strength of TIC-FT is its ability to operate in the few-shot regime . We fine-tune models with as\nfew as 10–30 training samples and fewer than 1,000 training steps—requiring less than one hour of\ntraining time for CogVideoX-5B on a single A100 GPU.\nWe use both real and synthetic datasets for evaluation and demonstrations. Real datasets include\nSSv2 [ 30] and manually curated paired videos, while synthetic datasets are created using models such\nas GPT-4o image generation [ 31] and Sora [ 32] (e.g., translating real images into stylized videos).\nEach task is provided with 20 condition–target pairs. Additional details are provided in the Appendix.\nWe compare TIC-FT with three representative fine-tuning methods for conditional video generation.\nWhile CogVideoX-5B and Wan-14B are among the most recent and powerful text-to-video diffusion\nmodels, most existing editing or fine-tuning approaches have not been evaluated on such large-\nscale backbones. To ensure meaningful comparisons, we reimplement the following representative\nbaselines.\n6\n--- Page 7 ---\nReplaceNo BufferW/ Buffer\nNo Image Cond\nCondBufferTarget frames(a) Zero-shot\nReplaceNo BufferW/ Buffer\nNo Image Cond\nCondBufferTarget frames(b) Trained\nFigure 2: (a) Zero-shot comparison of our method (last row) with variants: without buffer frames and\nwith an SDEdit-style inpainting strategy (“Replace”, second row). Buffer frames enable smoother\ntransitions and better condition preservation. (b) Corresponding results after fine-tuning.\nControlNet [7,9]. We include ControlNet as a baseline because a large number of recent methods\nare built upon it or extend its core architecture [ 16,17,14]. It is a widely adopted framework that\nintroduces an external reference network and zero-convolution layers to inject conditioning signals,\nenabling the model to preserve fine-grained visual details while integrating external guidance.\nFun-pose [10]. A simple yet widely adopted strategy is to concatenate the condition and target\nlatents, as seen in many recent methods[ 33,34,35]. However, this approach requires architectural\nmodifications and extensive retraining, which is infeasible in low-data regimes (e.g., 20 samples).\nSince training such a model from scratch yields extremely poor results, a direct comparison would\nbe uninformative. Instead, we adopt Fun-pose —a variant of CogVideoX and Wan that has already\nbeen finetuned to accept reference videos—effectively giving it a significant advantage.\nSIC-FT-Replace [8,24]. This method performs spatial in-context fine-tuning by training the model\nto predict videos arranged as spatial grids. At inference time, the ground-truth condition is noised\nand repeatedly injected into the condition grid slot at each denoising step, following an SDEdit-\nstyle replacement strategy[ 25], while the remaining grid elements are progressively denoised. This\napproach represents a recent trend in applying in-context fine-tuning techniques to diffusion models.\n4.2 Results\nWe conduct quantitative evaluations using CogVideoX-5B as the base model, focusing on two\nI2V tasks—object-to-motion and character-to-video—as shown in Table 1. For V2V , we evaluate\nperformance on a style transfer task (real videos to animation), summarized in Table 2. All models\nare fine-tuned using LoRA (rank 128) with 20 training samples over 6,000 steps, a batch size of 2,\nand a single NVIDIA H100 80GB GPU. Inference is conducted with 50 denoising steps.\nTo assess video quality comprehensively, we use three categories of evaluation metrics: VBench [ 36],\nGPT-4o [ 31], and Perceptual similarity scores. VBench provides human-aligned assessments of\ntemporal and spatial coherence, including subject consistency, background stability, and motion\nsmoothness. GPT-4o leverages a multimodal large language model to rate aesthetic quality, structural\nfidelity, and semantic alignment with the prompt. Perceptual metrics quantify low- and high-level\nvisual similarity between condition and target frames, including CLIP-I and CLIP-T (for image/text\nalignment), LPIPS and SSIM (for perceptual similarity), and DINO (for structural consistency).\nHowever, we omit Perceptual metrics when evaluating tasks like object-to-motion, where different\nviewpoints may reduce similarity scores despite correct semantics.\nOur model achieves strong performance even with limited training, showing competitive results after\nonly 2,000 training steps—unlike other baselines that require significantly more optimization to reach\n7\n--- Page 8 ---\nTable 1: Comparison on VBench, GPT-4o, and perceptual similarity metrics for I2V tasks.\nMethodVBench GPT-4o Perceptual similarity\nsubject background motion aesthetic structural semanticCLIP-I CLIP-T LPIPS ↓SSIM DINOconsistency consistency smoothness quality similarity similarity\nControlNet [7, 9] 0.9658 0.9600 0.9926 3.87 2.69 2.69 0.7349 0.2903 0.6535 0.3477 0.3427\nFun-pose [10] 0.9508 0.9598 0.9910 4.09 2.87 3.21 0.7714 0.3099 0.6339 0.3575 0.3866\nSIC-FT-Replace [8, 24] 0.9513 0.9676 0.9921 4.10 2.42 2.95 0.7993 0.3064 0.6190 0.4455 0.4246\nTIC-FT-Replace 0.9580 0.9702 0.9926 4.08 2.00 2.48 0.7925 0.3127 0.6165 0.4123 0.4221\nTIC-FT (w/o Buffer) 0.9474 0.9686 0.9892 4.05 3.05 3.53 0.7573 0.2986 0.6242 0.4058 0.4160\nTIC-FT (2K) 0.9505 0.9696 0.9920 4.03 3.08 3.54 0.8066 0.3135 0.6162 0.4203 0.4240\nTIC-FT (6K) 0.9672 0.9729 0.9930 4.13 3.14 3.63 0.8329 0.3143 0.4332 0.5917 0.5530\nTable 2: Comparison on VBench, GPT-4o, and perceptual similarity metrics for V2V tasks.\nMethodVBench GPT-4o Perceptual similarity\nsubject background motion aesthetic structural semanticCLIP-I CLIP-T LPIPS ↓SSIM DINOconsistency consistency smoothness quality similarity similarity\nControlNet [7, 9] 0.9553 0.9545 0.9854 3.44 2.23 2.41 0.6221 0.2727 0.5434 0.3494 0.2839\nFun-pose [10] 0.9679 0.9675 0.9902 4.24 2.68 3.23 0.7260 0.3018 0.5179 0.3328 0.4369\nSIC-FT-Replace [8, 24] 0.9609 0.9655 0.9853 3.99 2.44 2.94 0.7368 0.3198 0.5998 0.2192 0.4025\nTIC-FT-Replace 0.9584 0.9696 0.9802 3.93 2.33 2.92 0.7305 0.3015 0.6373 0.2526 0.3673\nTIC-FT (w/o Buffer) 0.9479 0.9571 0.9744 3.81 2.66 3.20 0.7471 0.3020 0.4687 0.3800 0.4429\nTIC-FT (2K) 0.9439 0.9600 0.9865 3.85 3.67 4.37 0.8174 0.3132 0.2970 0.5546 0.6089\nTIC-FT (6k) 0.9736 0.9743 0.9935 3.99 3.90 4.41 0.8794 0.3118 0.2251 0.6541 0.6745\nsimilar quality. Additional comparisons under this low-data, low-compute regime are presented in\nthe Appendix. Despite being conditioned on reference frames, Fun-pose and ControlNet exhibit poor\ncondition fidelity. While their outputs appear visually plausible—as indicated by favorable VBench\nand GPT-4o scores—they consistently underperform in Perceptual similarity metrics, highlighting a\nlack of alignment with the conditioning input. This is especially problematic for ControlNet, which\nrelies on strict spatial alignment and thus struggles in tasks such as character-to-video and object-to-\nmotion, where viewpoint shifts are common. SIC-FT-Replace[ 8,24] also performs suboptimally in\nI2V settings, as it requires replicating a single frame across a spatial grid—leading to high memory\nusage and inefficient training. Furthermore, its reliance on SDEdit [ 25]-style sampling during\ninference degrades generation quality and weakens condition adherence.\nWe supplement quantitative results with qualitative comparisons across I2V and V2V tasks in\nFigure 3. We also present additional scenarios—including virtual try-on, ad-video generation, and\naction transfer—are illustrated in Figures 1 and 4.\nOverall, our proposed TIC-FT consistently outperforms prior methods across diverse tasks, with\nboth quantitative metrics and qualitative examples supporting its superior condition alignment and\ngeneration quality. More results and task-specific details are provided in the Appendix.\n4.3 Ablation study\nWe validate the effectiveness of our temporal concatenation design with buffer frames by assessing\nits zero-shot performance. If the model successfully leverages the pretrained capabilities of video\ndiffusion models, it should generate plausible outputs even without any additional training.\nAs shown in Figure 2(a), our method with buffer frames (last row) generates target frames that align\nwell with the given condition—demonstrating strong zero-shot performance. In contrast, removing\nthe buffer frames leads to abrupt noise-level discontinuities between condition and target regions,\ncausing the target frames to degrade and the condition information to be poorly preserved. We also\ncompare with zero-shot inpainting methods similar to SDEdit, denoted as “Replace” (second row),\nwhich similarly fails to propagate condition signals into the generated frames.\nFurthermore, in Figure 2(b), we observe that strong zero-shot performance correlates with better\nresults after fine-tuning. Our method with buffer frames consistently outperforms other variants:\nmodels trained without buffer frames begin with blurry target frames, and the “Replace” strategy fails\nto apply condition information effectively even after training.\n8\n--- Page 9 ---\nFigure 3: Qualitative comparison between our method and baseline approaches.\nFigure 4: Demonstration of our method on character-to-video, action transfer, and virtual try-on.\n9\n--- Page 10 ---\n5 Conclusion and Limitation\nTemporal In-Context Fine-Tuning (TIC-FT) temporally concatenates condition and target frames\nwith intermediate buffer frames to better align with the pretrained model distribution. TIC-FT enables\na unified framework for diverse conditional video generation tasks and consistently outperforms\nexisting methods in our experiments. While efficient, the method is currently limited to condition\ninputs shorter than 10 seconds due to memory constraints—an important direction for future work.\nReferences\n[1]Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,\nYam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video\ndiffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023.\n[2]Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson,\nEran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki\nBitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint\narXiv:2501.00103 , 2024.\n[3] Genmo Team. Mochi 1. https://github.com/genmoai/models , 2024.\n[4]Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi\nHong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert\ntransformer. arXiv preprint arXiv:2408.06072 , 2024.\n[5]Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen\nYang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated\nparameters by tencent. arXiv preprint arXiv:2411.02265 , 2024.\n[6]Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming\nZhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint\narXiv:2503.20314 , 2025.\n[7]Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF international conference on computer vision , pages 3836–3847,\n2023.\n[8]Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu,\nand Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775 , 2024.\n[9]Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by\nusing diffusion model with controlnet. arXiv preprint arXiv:2307.14073 , 2023.\n[10] aigc apps. VideoX-Fun: A unified pipeline for video generation and editing. https://github.com/\naigc-apps/VideoX-Fun , apr 2025. GitHub repository, commit <hash> .\n[11] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao,\nand Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv\npreprint arXiv:2311.04145 , 2023.\n[12] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo.\nMoonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint\narXiv:2401.01827 , 2024.\n[13] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli\nZhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability.\nAdvances in Neural Information Processing Systems , 36:7594–7611, 2023.\n[14] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework\nfor adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967 , 2024.\n[15] Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan\nZhou, et al. In-context learning unlocked for diffusion models. Advances in Neural Information Processing\nSystems , 36:8542–8562, 2023.\n[16] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo:\nRemake a video with motion and content control. Advances in Neural Information Processing Systems , 37:\n18481–18505, 2024.\n10\n--- Page 11 ---\n[17] Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang.\nEasycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv\npreprint arXiv:2408.13005 , 2024.\n[18] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter\nfor text-to-image diffusion models. arXiv preprint arXiv:2308.06721 , 2023.\n[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems , 33:1877–1901, 2020.\n[20] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong\nWu, Tianyu Liu, et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234 , 2022.\n[21] Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning:\nAutoregressive transformers are zero-shot video imitators. In The Thirteenth International Conference on\nLearning Representations , 2025.\n[22] Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning.\narXiv preprint arXiv:2407.07356 , 2024.\n[23] Jianzhi Liu, Junchen Zhu, Lianli Gao, Heng Tao Shen, and Jingkuan Song. Aicl: Action in-context learning\nfor video diffusion model. arXiv preprint arXiv:2403.11535 , 2024.\n[24] Zhengcong Fei, Di Qiu, Debang Li, Changqian Yu, and Mingyuan Fan. Video diffusion transformers are\nin-context learners. arXiv preprint arXiv:2412.10783 , 2024.\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\nGuided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 ,\n2021.\n[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 11461–11471, 2022.\n[27] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos\nfrom text without training. arXiv preprint arXiv:2405.11473 , 2024.\n[28] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann.\nDiffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information\nProcessing Systems , 37:24081–24125, 2024.\n[29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision , pages 4195–4205, 2023.\n[30] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something\nsomething\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision , pages 5842–5850, 2017.\n[31] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,\nHanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities\nof large vision models. arXiv preprint arXiv:2402.17177 , 2024.\n[33] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels\ndance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 8850–8860, 2024.\n[34] Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Yuchi Huo, Rui Wang, Chi Zhang, and Xuelong\nLi. Omnivdiff: Omni controllable video diffusion for generation and understanding. arXiv preprint\narXiv:2504.10825 , 2025.\n[35] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. Concat-id: Towards universal\nidentity-preserving video synthesis. arXiv preprint arXiv:2503.14151 , 2025.\n11\n--- Page 12 ---\n[36] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu,\nQingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n21807–21818, 2024.\n[37] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view\nstereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 406–413, 2014.\n[38] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n12\n--- Page 13 ---\nA Technical Appendices and Supplementary Material\nA.1 Training Details\nAll models are fine-tuned using an NVIDIA H100 GPU. Our method builds on the CogVideoX-\n5B backbone and is fine-tuned with LoRA (rank 128), resulting in approximately 130M trainable\nparameters. Training with 49 frames requires roughly 30GB of GPU memory. For ControlNet,\nwe apply LoRA with the same rank, yielding a comparable parameter count of around 150M, and\nrequiring approximately 60GB of GPU memory. For Fun-pose, we use the official full fine-tuning\nsetup, which consumes around 75GB of GPU memory.\nA.2 Training Amount vs. Performance\nThis section demonstrates the training efficiency of our method compared to ControlNet. Figure 5\npresents performance curves for various metrics—including CLIP-T, CLIP-I, SSIM, DINO, and\nLPIPS—plotted against training time. Our method consistently outperforms ControlNet across all\nmetrics at equivalent training durations. Moreover, with the exception of CLIP-T, all metrics show a\nclear upward trend, indicating continued improvement with more training. In contrast, ControlNet\nexhibits no such trend, suggesting that its training style tends to overfit and struggles to generalize\nunder limited data regimes.\nA.3 Ablation Study\nWe conduct ablation study on various buffer frame designs. Specifically, we compare our default\nsetting—using a uniformly increasing noise schedule—with alternative strategies: (1) a constant\nnoise level t for all buffer frames (denoted as Constant- t, where T= 100), and (2) linear-quadratic\nschedules with concave or convex profiles. Figure 6 presents both zero-shot and fine-tuned results for\nthese configurations. While all variants produce reasonable target frames, we observe that the convex\nschedule and the constant-25 baseline exhibit poor condition alignment and noticeable artifacts in\nthe zero-shot setting. After fine-tuning, all methods perform comparably, though our default setting\nwith uniformly increasing noise remains preferred. Quantitative results after training are presented in\nTable 3 and Table 4 for the I2V and V2V tasks, respectively.\nWe also evaluate the effect of varying the number of buffer frames, ranging from 1 to 5, denoted as\nBuffer- nin Figure 7. In the zero-shot setting, we observe that all configurations perform comparably\noverall; however, shorter buffers tend to produce noisier transitions, likely due to abrupt scene\nchanges. Conversely, longer buffers show a tendency to weaken the influence of the condition. After\nfine-tuning, all variants produce similarly high-quality results.\nA.4 Dataset\nFor the object-to-motion task, we use the DTU dataset [ 37]. For character-to-video, keyframe\ninterpolation, and ad video generation tasks, we manually collected condition–video pairs tailored\nto each task. For action transfer, we curate videos from SSv2 [ 30]. In the video style transfer task,\nwe first synthesize starting frames using FLUX.1-dev [ 38], and then generate paired videos using\nSoRA [ 32] and Wan2.1 [ 6]. Each task is trained on 30 samples. All videos contain 49 frames at 10\nframes per second (fps), resized to either 480 ×480 or 848 ×480 while preserving the original aspect\nratio.\nFor evaluation and demonstration, we use image and video conditions that are not part of the training\nset. These include both manually collected images and synthesized ones generated using GPT-\n4o, FLUX, and Sora. For the action transfer task, we use unseen video samples from SSv2 [ 30].\nQuantitative evaluations are conducted on 100 samples. For image-based metrics such as CLIP and\nLPIPS, scores are computed on a per-frame basis and then averaged to obtain the final results.\nTraining and evaluation prompts are generated using GPT-4o. Each prompt is structured to encompass\nthe condition, buffer, and target frames, with condition and buffer frames denoted as [CONDITION]\nand target frames as [VIDEO] . Below is the full prompt used for the sample in the ablation study:\n13\n--- Page 14 ---\nTIC-FT prompt\nThis animated clip demonstrates the transformation of a static character illustration into\na lively and expressive animated figure; [CONDITION] the condition image showcases a\ncheerful cartoon-style young buffalo with thick brown fur, curved yellow horns, and a big,\nfriendly smile. The character’s wide eyes and upright posture are set against a warm orange\nbackground, giving it a lively and playful presence. [VIDEO] the video animates the buffalo\ninside a grand museum, where it wears a red t-shirt and points excitedly at a large dinosaur\nskeleton behind glass. Its eyes are wide with curiosity and its mouth open in awe, while\nelegant stone columns and soft lighting emphasize the sense of wonder and fascination with\nhistory.\nA.5 Task Descriptions\nWe detail the construction of data and latent sequences for each conditional video generation task used\nin our experiments. All tasks are configured with a total of 13 latent frames, corresponding to 49 video\nframes. While this number can be adjusted based on application needs, we adopt the 13-frame setting\nthroughout for implementation simplicity and consistency. The initial latent sequence comprises\ncondition frames, intermediate buffer frames, and noised target frames. An exception is the action\ntransfer task, where buffer frames are omitted, as the last condition frame serves as the starting frame\nof the target sequence. The specific configurations for each task are described below.\nImage-to-Video This task aims to generate a full video conditioned on a single image. The video\nneed not begin directly from the image’s visual content; instead, the image may represent a high-level\nconcept such as a character profile or an object viewed from the top, with the video depicting novel\ndynamics (e.g., a rotating 360° view).\nA single reference image is replicated to occupy the first 4 latent frames, followed by 9 target frames.\n• Clean condition: 1 frame (from the image)\n• Buffer: 3 frames (noised condition)\n• Target: 9 frames (pure noise)\nWe visualize the initial latent frames and their denoising process in Figure 8.\n2h3h4h5h10h15h0.270.2750.280.2850.290.2950.30.3050.310.315ControlnetOurs\nTraining TimeCLIP-T (↑)\nLoading [MathJax]/extensions/MathMenu.js2h3h4h5h10h15h0.350.40.450.50.550.60.65ControlnetOurs\nTraining TimeSSIM (↑)\nLoading [MathJax]/extensions/MathMenu.js2h3h4h5h10h15h0.650.70.750.80.85ControlnetOurs\nTraining TimeCLIP-I (↑)\nLoading [MathJax]/extensions/MathMenu.js\n2h3h4h5h10h15h0.30.350.40.450.50.550.60.650.7ControlnetOurs\nTraining TimeDINO (↑)\nLoading [MathJax]/extensions/MathMenu.js2h3h4h5h10h15h0.20.250.30.350.40.450.50.55ControlnetOurs\nTraining TimeLPIPS (↓)\nLoading [MathJax]/extensions/MathMenu.js2h3h4h5h10h15h3.33.43.53.63.73.83.94ControlnetOurs\nTraining TimeAesthetic Quality (↑)\nFigure 5: Performance curves for CLIP-T, CLIP-I, SSIM, DINO, and LPIPS metrics plotted against\ntraining time. Our method consistently outperforms ControlNet across all metrics at equivalent\ntraining durations.\n14\n--- Page 15 ---\nTable 3: Ablation study of constant noise scheduling for buffer frames, evaluated on I2V tasks using\nVBench, GPT-4o, and perceptual/similarity metrics.\nMethodVBench GPT-4o Perceptual similarity\nsubject background motion aesthetic structural semanticCLIP-I CLIP-T LPIPS ↓SSIM DINOconsistency consistency smoothness quality similarity similarity\nOurs 0.9672 0.9729 0.9930 4.13 3.14 3.63 0.8329 0.3143 0.4332 0.5917 0.5530\nConstant-25 0.9516 0.9724 0.9920 4.09 2.81 3.45 0.7734 0.3062 0.6088 0.4240 0.4202\nConstant-50 0.9509 0.9740 0.9915 4.05 3.01 3.51 0.7760 0.3010 0.6157 0.4188 0.4228\nConstant-75 0.9511 0.9722 0.9917 4.02 3.07 3.68 0.7725 0.3003 0.6148 0.4250 0.4259\nTable 4: Ablation study of constant noise scheduling for buffer frames, evaluated on V2V tasks using\nVBench, GPT-4o, and perceptual/similarity metrics.\nMethodVBench GPT-4o Perceptual similarity\nsubject background motion aesthetic structural semanticCLIP-I CLIP-T LPIPS ↓SSIM DINOconsistency consistency smoothness quality similarity similarity\nOurs 0.9736 0.9743 0.9935 3.99 3.90 4.41 0.8794 0.3080 0.2298 0.6541 0.6596\nConstant-25 0.9539 0.9652 0.9873 3.90 3.55 4.20 0.8037 0.3103 0.2744 0.5785 0.6083\nConstant-50 0.9524 0.9652 0.9886 3.88 3.86 4.31 0.8460 0.3153 0.2364 0.6039 0.6528\nConstant-75 0.9327 0.9552 0.9821 3.69 3.60 4.25 0.8330 0.3142 0.2797 0.5707 0.6368\nVideo Style Transfer This video-to-video task transforms the visual style of a source video into\nthat of a target domain (e.g., converting a realistic video into an animated version) while preserving\nmotion and structure.\nThe first 7 latent frames are taken from a source video and the remaining 6 from a style-transferred\nversion.\n• Clean condition: 4 frames (from the source video)\n• Buffer: 3 frames (noised condition)\n• Target: 6 frames (pure noise)\nWe visualize the initial latent frames and their denoising process in Figure 9.\nIn-Context Action Transfer This task generates a video that continues a novel scene using motion\ninferred from a source video. Given a reference action and the first frame of a new environment, the\nmodel synthesizes future frames that imitate the observed motion within the new context.\nThe first 6 latent frames are from a reference action video, the 7th is the first frame of a novel scene,\nand the rest are the continuation.\n• Clean condition: 6 frames (from the reference action video)\n• Query frame: 1 clean frame (from the novel scene)\n• Target: 6 frames (pure noise)\nNo buffer frames are used in this task, as the first frame of the target video is explicitly provided as\npart of the condition. We visualize the initial latent frames and their denoising process in Figure 10.\nKeyframe Interpolation This task fills in intermediate frames between sparse keyframes to produce\na temporally coherent video. The goal is to ensure smooth transitions between given keyframes.\nFour keyframes are replicated to fill the first 7 latent frames, and the remaining 6 are interpolated.\n• Clean condition: 4 frames (replicated keyframes)\n• Buffer: 3 frames (noised condition)\n• Target: 6 frames (pure noise)\nWe visualize the initial latent frames and their denoising process in Figure 11.\nMultiple Image Conditions This task takes two distinct types of image conditions—such as a person\nand clothing, or a person and an object—and generates a target video that reflects the combination of\n15\n--- Page 16 ---\nboth. This setup is useful for applications like virtual try-on (VITON) or ad video synthesis, where\ntwo semantic entities must be jointly represented in motion.\nThe first 3 latent frames are derived from the first condition image, and the next 4 from the second\ncondition image.\n• Clean condition: 4 frames (3 from the first image, 1 from the second)\n• Buffer: 3 frames (noised condition)\n• Target: 6 frames (pure noise)\nNote that the number of condition sources is not limited to two; the framework supports arbitrary\nmulti-condition setups. We visualize the initial latent frames and their denoising process in Figure 12.\nA.6 Broader Impacts and Misuse Discussion\nOur TIC-FT method enables efficient adaptation of video diffusion models with minimal data.\nHowever, this ease of fine-tuning also introduces risks, particularly the potential misuse for creating\ndeepfakes or misleading synthetic media. Clear usage policies and responsible deployment practices\nare essential to mitigate societal risks.\n16\n--- Page 17 ---\nFigure 6: Qualitative comparison of buffer frame designs in zero-shot and fine-tuned settings.\n17\n--- Page 18 ---\nFigure 7: Qualitative comparison of buffer frame designs in zero-shot and fine-tuned settings.\n18\n--- Page 19 ---\nFigure 8: Visual results for initial frames and their denoising process on image-to-video generation.\nPrompt: [Character] A clear, high-resolution front-facing close-up of a cheerful cartoon-style wolf\ncharacter, centered against ...\nFigure 9: Visual results for initial frames and their denoising process on video style transfer task.\nPrompt: [VIDEO1] A woman in a tan cloak walks gracefully along a forest path. Her hair flows\ngently with her movement, and the ...\n19\n--- Page 20 ---\nFigure 10: Visual results for initial frames and their denoising process on in-context action transfer\ntask. Prompt: [REFERENCE VIDEO] A white paper is folded in half by a person wearing black\nsleeves in a dark indoor environment. ...\nFigure 11: Visual results for initial frames and their denoising process on keyframe interpolation task.\nPrompt: [VIDEO1] A cartoon woman with red hair and a jeweled headpiece slowly tilts her head\nand changes facial expressions ...\n20\n--- Page 21 ---\nFigure 12: Visual results for initial frames and their denoising process on virtual try-on task. Prompt:\n[IMAGE] A young woman with long black hair, wearing a cream blouse, blue jeans, and black\nsandals, smiles with both ...\n21",
  "text_length": 50085
}