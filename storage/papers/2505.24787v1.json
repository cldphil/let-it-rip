{
  "id": "http://arxiv.org/abs/2505.24787v1",
  "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for\n  Complex Instruction-based Image Generation",
  "summary": "Recent advancements in text-to-image (T2I) generation have enabled models to\nproduce high-quality images from textual descriptions. However, these models\noften struggle with complex instructions involving multiple objects,\nattributes, and spatial relationships. Existing benchmarks for evaluating T2I\nmodels primarily focus on general text-image alignment and fail to capture the\nnuanced requirements of complex, multi-faceted prompts. Given this gap, we\nintroduce LongBench-T2I, a comprehensive benchmark specifically designed to\nevaluate T2I models under complex instructions. LongBench-T2I consists of 500\nintricately designed prompts spanning nine diverse visual evaluation\ndimensions, enabling a thorough assessment of a model's ability to follow\ncomplex instructions. Beyond benchmarking, we propose an agent framework\n(Plan2Gen) that facilitates complex instruction-driven image generation without\nrequiring additional model training. This framework integrates seamlessly with\nexisting T2I models, using large language models to interpret and decompose\ncomplex prompts, thereby guiding the generation process more effectively. As\nexisting evaluation metrics, such as CLIPScore, fail to adequately capture the\nnuances of complex instructions, we introduce an evaluation toolkit that\nautomates the quality assessment of generated images using a set of\nmulti-dimensional metrics. The data and code are released at\nhttps://github.com/yczhou001/LongBench-T2I.",
  "authors": [
    "Yucheng Zhou",
    "Jiahao Yuan",
    "Qianning Wang"
  ],
  "published": "2025-05-30T16:48:14Z",
  "updated": "2025-05-30T16:48:14Z",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24787v1",
  "full_text": "arXiv:2505.24787v1 [cs.CV] 30 May 2025Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation Yucheng Zhou1, Jiahao Yuan2, Qianning Wang3 1University of Macau, China,2East China Normal University, China, 3Auckland University of Technology, New Zealand yucheng.zhou@connect.um.edu.mo Insideavast,domedlibrary-observatory,lightfiltersthroughamassiverosewindowmadeoffragmentedstainedglass,depictinganabstractrepresentationoftheBigBang.Atthecenteroftheroom,insteadofatable,floatsagiganticorrery,wovenfrombrass,silver,andlivingvines.Its“planets”arenotmerespheres,butminiature,intricatelydetailedecosystemsencasedinglowingglassorbs—onedisplayingatinydesertscatteredwithweatheredbones,anotheraminiatureoceanwhereluminousfishglide,andathirdamicroforestovergrownwithstrangepurplefungi.…Onthepolishedobsidianfloor,reflectionsoftheorreryanddriftinglightsshiftconstantlyassmallmechanicalinsects—madeofgearsandfeathers—gatherglowingrune-stonesintoabasketwovenfromhumanhair.Fromtheorrery'sforestorb,aglowingfungaltendrildescendsslowlytowardthefloor,almosttouchingthechalk-drawnprotectivecircleofarobedfigurewhosefaceishiddeninshadowortranslucency.Throughoutthespace,soft,multicoloredorbsoflightfloatfreely,castingever-changingpatternsofglowandshadow.Theairsmellsfaintlyofozoneandancientpaper,andsubtleambientnoises—whispersofgears,softelectricalhums—contributetoanatmospheresuspendedbetweenarcaneknowledgeandscientificwonder.Complex Instruction~ 600 Tokens ElementsInformationGenerateImage Object:Thescenefeaturesastar-furredcatwithadeep-blue,constellation-speckledcoat,carefullynavigatingarainbow-crystalbeamwhileholdingabrasskey... BackgroundandEnvironment:Thesettingisavast,domedlibrary-observatory,aspacewherearcaneknowledgemeetsscientificwonder... ColorandTone:Thecolorpaletteisrichandvaried,featuringdeepbluesandvioletsfromthestar-furredcatandtheminiaturenebula,contrastedbythewarmbrass... TextureandMaterial:Thesceneisrichintactiledetails,fromtheiridescentlizardskinandroughbarkofthebookbindingstothepolishedobsidianfloorreflecting...LightingandShadow:Lightfiltersthroughamassiverosewindowmadeoffragmentedstainedglass,castingabstractpatternsacrosstheroom...TextandSymbol:Thesceneincludesseveralelementsofsymbolicsignificance.TherosewindowdepictsanabstractrepresentationoftheBigBang,symbolizing...SpecialEffects:Thesceneincorporatesseveralspecialeffectstoenhanceitsvisualimpact.Thebioluminescentpatternsonthebookshelvescreateamagicalglow...CompositionandFraming:Thecompositioniscenteredaroundthegiganticorrery,drawingtheeyetoitsintricatedetailsandminiatureecosystems...PoseandExpression:Thestar-furredcat’scarefulwalkalongtherainbow-crystalbeamsuggestsagilityandcuriosity... Evaluation Figure 1: LongBench-T2I benchmark evaluates text-to-image models on their ability to follow complex instructions. The framework extracts nine key elements from each instruction to systematically assess the detail of the generated image. Abstract Recent advancements in text-to-image (T2I) generation have en- abled models to produce high-quality images from textual descrip- tions. However, these models often struggle with complex instruc- tions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY ©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXXrequirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifi- cally designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts span- ning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model’s ability to follow complex instructions. Be- yond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation with- out requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation met- rics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that au- tomates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at https://github.com/yczhou001/LongBench-T2I. 1 Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Zhou et al. CCS Concepts •Computing methodologies →Computer vision;Language resources. Keywords Text-to-Image Generation, Benchmark, Complex Instruction, Agent ACM Reference Format: Yucheng Zhou1, Jiahao Yuan2, Qianning Wang3. 2018. Draw ALL Your Imag- ine: A Holistic Benchmark and Agent Framework for Complex Instruction- based Image Generation. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym ’XX). ACM, New York, NY, USA, 7 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Recent advancements in text-to-image (T2I) generation have led to the development of models capable of producing high-quality images from textual descriptions. Many T2I models, e.g., DALL ·E, Stable Diffusion [ 19], and Imagen [ 20], have demonstrated impressive abilities in generating diverse and visually appealing images. However, their performance often diminishes when tasked with generating images from complex, multi-faceted instructions that involve intricate object compositions, specific attributes, and detailed spatial relationships. Existing benchmarks, such as DrawBench [ 17], DPG-Bench [ 10], and T2I-CompBench [ 11], have made significant strides in eval- uating compositional capabilities by introducing categories like attribute binding and object relationships. While these benchmarks provide valuable insights, they primarily focus on individual as- pects of compositionality and often lack comprehensive coverage of the multifaceted nature of complex instructions. For instance, T2I- CompBench [ 11] categorizes prompts into attribute binding and object relationships but does not extensively address the nuanced interplay between these elements in complex scenes. Additionally, the evaluation metrics employed, such as CLIPScore [ 9], may not fully capture the intricacies of compositional understanding, as they often rely on image-text similarity measures that can be ambiguous in the context of complex instructions. Furthermore, while some benchmarks have introduced evalu- ation metrics tailored to specific compositional challenges, there is a lack of standardized, multi-dimensional assessment tools that can holistically evaluate a model’s ability to follow complex in- structions [10, 11, 23]. This gap in evaluation frameworks hinders the development of models that are not only capable of generating high-quality images but also adept at adhering to intricate and detailed instructions. To address these limitations, we propose LongBench-T2I, a com- prehensive benchmark designed to evaluate T2I models under com- plex instructions. LongBench-T2I encompasses 500 meticulously curated prompts that span nine diverse visual evaluation dimen- sions, providing a robust framework to assess a model’s instruction- following capabilities comprehensively. In addition to benchmark- ing, we introduce an agent framework, Plan2Gen, that facilitates complex instruction-based image generation without necessitating additional model training. This framework seamlessly integrates with existing T2I models, leveraging large language models (LLMs)Table 1: Comparison of other T2I benchmarks. Benchmark Prompts Source Component Long DrawBench  200 Human Object Level 15 T2I-CompBench  5,000 Template 3 Categories 10 DPG-Bench  1,065 Template Object Level 84 LongBench-T2I (Ours) 500 LLM 9 Elements 683 to parse and decompose complex instructions, thereby guiding the image generation process more effectively. By providing a holistic benchmark and a novel agent baseline, LongBench-T2I aims to bridge the existing gaps in the evaluation of complex instruction-following in T2I models. It can catalyze further research into developing more controllable and user-aligned genera- tive models, advancing the field of complex instruction-based image generation. The main contributions are summarized as follows: •We present LongBench-T2I, the first comprehensive benchmark for image generation under complex instructions, consisting of 500 intricate prompts covering 9 diverse visual evaluation dimen- sions, which enables a series of analyses on instruction-following capabilities of image generation models. •We conduct extensive pilot studies on various categories of text- to-image generation models using LongBench-T2I, providing comprehensive analyses across Closed-Source versus Open-Source models and Diffusion versus Autoregressive architectures. •We also propose Plan2Gen for complex instruction-based image generation, which requires no model training and can seamlessly integrate with existing image generation models under the direc- tion of an LLM. •To facilitate future research, we release an evaluation package that automates result evaluation and supports multi-dimensional assessment, hoping LongBench-T2I will promote comprehensive studies on image generation from complex instructions and drive the development of more controllable and user-aligned models. 2 Related Work Text-to-Image Generation. Recent T2I models, including diffusion- based methods (e.g., Imagen [ 20], Stable Diffusion [ 19], FLUX [ 14]), autoregressive approaches (e.g., DALL ·E, Infinity [ 8]), and hy- brid variants, have made notable progress in visual fidelity and semantic alignment [ 4,15,24,28,30,34]. However, they often fail under complex prompts involving multiple entities, fine-grained attributes, and spatial relations. To improve controllability, recent works have introduced agent-based frameworks [ 5,7,18,26] and chain-of-thought [ 6,13] prompting to guide generation via LLM reasoning [ 25,32,33,36] with self-correction [ 5,27], yet these meth- ods remain limited in handling long-form compositionality due to the lack of structured generation and iterative validation. Image Generation Benchmark. Existing benchmarks have been introduced to assess compositional text-to-image generation, yet each remains limited in scope. DrawBench [ 17], with 200 short prompts, targets basic skills like counting and attribute binding, but relies on single-sentence inputs with limited semantic depth. DAA- 200 [ 23] offers 200 adversarial prompts to test attribute binding precision, though its scope remains confined to simple two-object scenes. T2I-CompBench [ 11] focuses on open-world composition- ality with 5K prompts, but most are short and cover only isolated 2 A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Object ListSelectionSketch DescriptionComplex Scene Instruction GenerationInteractionCinematicVisual Entanglement IfPassedHumanReviewVisual Element Richness Structural and Compositional ComplexityInteraction and Semantic DiversityCreative Highlights and Special EffectsVisualSketch Description GenerationVisual Realism Object Relations Passed Complex InstructionElements Information Extraction Review Failed Element Extraction &Automated Review Figure 2: The multi-stage pipeline for generating the LongBench-T2I benchmark. skills per instance. DPG-Bench [ 10] expands compositional den- sity through template-based multi-object prompts, yet its auto- generated language lacks the descriptive nuance and narrative structure required for evaluating long-form instruction following. However, these are limited by brevity, narrow diversity, or synthetic prompts. In contrast, as summarized in Table 1, LongBench-T2I pro- vides 500 multi-sentence prompts generated by LLMs and filtered by humans, each covering nine visual aspects for evaluating models under complex instructions. 3 LongBench-T2I Benchmark The LongBench-T2I benchmark is systematically developed through a multi-stage pipeline aimed at producing complex and high-quality text-to-image instructions. As illustrated in Figure 2, the process utilizes LLMs for text generation and refinement, supported by meticulous human oversight to ensure the benchmark’s quality. 3.1 Sketch Description Generation The process commences with the generation of a foundational visual sketch. We begin by randomly sampling a single object from a predefined list, derived from the Object365 dataset [ 21]. This seed object is then fed into an LLM. The LLM is tasked with creating a descriptive visual sketch that incorporates this primary object. To foster diversity and contextual richness from the outset, the LLM is specifically prompted to consider Object Relations (how the seed object might interact with or be situated relative to other potential objects) and Visual Realism. The aim is to produce a varied set of initial descriptions that include multiple objects and lay the groundwork for more complex scenes, moving beyond simplistic single-object depictions. 3.2 Complex Scene Instruction Generation Building upon the visual sketch obtained in the previous stage, we elaborate this sketch into a comprehensive, intricate, and detailedtextual instruction suitable for advanced text-to-image models. This employs an LLM to enhance the description by incorporating con- siderations of visual entanglement (e.g., objects occluding or inter- twining with each other), complex interactions between multiple entities, and cinematic qualities (such as specific camera angles, depth of field, or dramatic lighting). The objective of this stage is to substantially increase the complexity and narrative depth of the textual prompt, pushing the boundaries of what current T2I models can interpret and render. 3.3 Element Extraction and Automated Review To ensure the structural integrity and semantic coherence of the generated complex scene instructions, a subsequent stage imple- ments element extraction and automated review. An LLM is utilized to parse each complex instruction and extract information on nine predefined key visual elements: ❶Object(s): Main subjects and supporting items. ❷Background and Environment: Setting and sur- rounding context. ❸Color and Tone: Dominant hues, mood, and atmosphere. ❹Texture and Material: Surface properties of objects. ❺Lighting and Shadow: Illumination sources, direction, and result- ing shadows. ❻Text and Symbol: Any textual or symbolic elements present. ❼Composition and Framing: Arrangement of elements and camera perspective. ❽Pose and Expression: Postures and facial ex- pressions of animate subjects. ❾Special Effects: Any unique visual augmentations (e.g., motion blur, lens flare). Following extraction, the LLM performs an automated validation. It cross-references the extracted elements against the original instruction to check for consistency and completeness. 3.4 Human Review The concluding stage involves a rigorous human review process for all instructions that have successfully passed the LLM-based automated validation. Two graduate students meticulously evaluate each candidate’s instruction based on four crucial dimensions: ❶ 3 Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Zhou et al. Instruction:Intheflickeringamberglowofagaslampthathoversmidairjustabovetheclutteredwoodenkitchentable,aweatheredleathersuitcaseremainsopen,itsbrasslatchesfrozenintheactofbeingunlatched,asiftimehadhesitatedinthemomentbeforeajourney.Inside,atangledballofearbudsisslowlyunravelingitself,eachwiretwistingthroughaconstellationoffoldedmaps,someofwhichareshiftingsubtlyasifthegeographytheydepictisaliveandrestless.Ahalf-eatenchocolatebarliesnestledamongtheserelicsoftravel,itsmeltingsidesdrippingnotintothegrainofthewoodbutupward,asifgravityhasmomentarilylostinterestinthisparticularcorneroftheroom.Aredscarf—threadedwiththefaintshimmerofliquidsilver...516Tokens Scene Decomposition Planning Step2: MidgroundPlacethemainobjectsontothetablesurface:anopenweatheredleathersuitcase,acoffeecup,astackofnotebooks,ahalf-eatenchocolatebar,andaredscarf.Positionahoveringgaslampjustabovethetable,castinglong,waveringshadowsacrosstheseobjectsandtheroom.Step3: ForegroundAddfinedetailsanddynamicsurrealelements:fillthesuitcasewithatangledballofearbudsandshiftingmaps;showthechocolatemeltingupward,notebookpagesturningandwritingthemselves,thescarfliftingwithasilvershimmer,thecoffeerippling,thesuspendedrainreflectingthescene,themountainsoutsidechangingpositionandhue,atickingsoundnearthewindow,andthesuitcasepulsinginrhythmwiththeupward-drippingchocolate,includingaloosepageliftingintothelamp'sglowandreappearingcrumpledinthesuitcase. Step1: BackgroundEstablishtheoverallsceneenvironment:aroomwithaclutteredwoodentable,awindowshowingsuspendedrainreflectingtheinterioranddistantmountains,andaceilingreplacedbyaswirlingexpanseofskywithblinkingconstellations,allilluminatedbyaflickeringambergloworiginatingfromabovethetable. Validation Validation: EndBackground + Refine 1 Validation Background + Refine 1 +Refine 2 Refine1:Ensuretheamberglowappearstobeflickeringslightly.Refine2:Makethewoodentableappearmoreclutteredwithvarioussmallobjects. Validation Validation: EndMidground+ Refine 1 Validation Midground +Refine 1 +Refine 2 Refine1:Thehalf-eatenchocolatebarshouldhaveapiecemeltinganddrippingupwards.Theredscarfshouldappeartobepartiallyemergingfromthestackofnotebooksandslightlyspiralingupwards.Refine2:Makethewoodentableappearmoreclutteredwithvarioussmallobjects. Validation Validation: EndBackground + Refine 1 Refine1:Makethetangledballofearbudsandshiftingmapsclearlyvisibleinsidetheopensuitcase. Progressive Layered Image Generation Figure 3: The Plan2Gen Agent generates images from complex long instructions by first decomposing the scene into background, midground, and foreground prompts, then progressively generating and refining each layer with validation to ensure alignment with the original description. Visual Element Richness: The diversity, detail, and specificity of the described visual components. ❷Structural and Compositional Complexity: The intricacy of object arrangements, scene layout, and spatial relationships. ❸Interaction and Semantic Diversity: The presence of meaningful interactions between elements and the overall semantic coherence and novelty of the scene. ❹Creative Highlights and Special Effects: The inclusion of imaginative concepts, unique artistic styles, or challenging visual effects that test advanced generation capabilities. Only instructions meeting these criteria are included in LongBench-T2I, ensuring the benchmark’s quality and relevance for evaluating long-context text-to-image generation. 4 Plan2Gen Agent The Plan2Gen agent tackles complex, long-form image generation through a structured, multi-stage approach (Figure 3), with scene planning, iterative generation, and validation. 4.1 Scene Decomposition Planning Given a complex and lengthy textual instruction, an LLM is first tasked with a strategic planning role. The LLM analyzes the en- tirety of the input text to identify and segregate the described visual elements into distinct compositional layers. Specifically, it deconstructs the overall scene into three core components: ❶Back- ground: Elements constituting the furthest plane of the scene,establishing the overall environment and atmosphere (e.g., sky, dis- tant mountains, room walls). ❷Midground: Objects and entities situated between the background and foreground, often serving as primary or secondary subjects (e.g., a table, a suitcase, a cof- fee cup upon the table). ❸Foreground: Elements closest to the viewer, or those requiring fine-grained detail and prominence, po- tentially interacting with or overlaying midground elements (e.g., specific items within an open suitcase, detailed textures, immediate atmospheric effects). This decomposition yields three layer-specific sub-prompts, enabling the model to focus on manageable details at each stage and progressively build scene complexity. 4.2 Progressive Layered Image Generation This process constructs the final image by iteratively generating and refining each compositional layer, background, midground, and foreground, in sequence. For each layer, a consistent generation and refinement pipeline is applied: ❶Initial Generation: The layer-specific sub-prompt (derived from the decomposition phase) is input to an LLM with image generation capabilities (or a special- ized text-to-image model). For midground and foreground layers, the generation is also conditioned on the successfully generated preceding layer(s) to ensure contextual coherence. ❷Validation: The generated image for the current layer is assessed by an LLM against the requirements outlined in its corresponding sub-prompt. ❸Iterative Refinement: If the validation indicates a mismatch 4 A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation Conference acronym ’XX, June 03–05, 2018, Woodstock, NY with the prompt (e.g., the “amber glow” not “flickering slightly” for the background, or issues with the “half-eaten chocolate bar” for the midground, as depicted in Figure 3), the original sub-prompt for that layer is augmented with specific refinement instructions. This revised prompt is then used to re-generate the image for the current layer. This generation-validation-refinement cycle can be repeated. ❹Progression Criterion: The refinement loop for the current layer terminates when either the generated image successfully passes validation or a predefined maximum number of refinement attempts is exhausted. This pipeline is applied sequentially to the background, midground, and foreground layers, with each stage conditioned on the approved outputs of previous layers. Layer-by-layer generation, validation, and refinement help Plan2Gen manage complexity and improve image fidelity to long-form instructions. The maximum refinement threshold ensures efficient termination. 5 Experiments 5.1 Experimental Setup Models To comprehensively assess the capabilities of current text- to-image generation systems on long-context instructions, we eval- uate a diverse range of models using our LongBench-T2I benchmark. The evaluated models span leading closed-source systems, promi- nent open-source alternatives, and architectures including both autoregressive and diffusion-based approaches. This broad selec- tion allows for a thorough comparison and highlights the challenges posed by our benchmark across different model types, including the performance of our proposed Plan2Gen agent. Evaluation Metrics To quantitatively and qualitatively assess image fidelity to the LongBench-T2I instructions, we evaluate gen- erated images across the nine key visual elements defined in Sec- tion 3.3: Object(s) ( Obj.), Background and Environment ( Backg. ), Color and Tone ( Color ), Texture and Material ( Texture ), Light- ing and Shadow ( Light ), Text and Symbol ( Text ), Composition and Framing ( Comp. ), Pose and Expression ( Pose ), and Special Effects ( FX). Evaluation utilizes MLLMs: Google’s Gemini-2.0-Flash (closed-source) and InternVL3-78B (open-source). These MLLMs assess generated images against nine visual elements specified in input instructions. The open-source InternVL3-78B is employed to ensure long-term reproducibility of the evaluation results. 5.2 Performance Comparison We evaluated Plan2Gen against various text-to-image models on LongBench-T2I, using Gemini-2.0-Flash (Table 2) and InternVL3- 78B (Table 3) as evaluators. (1) Overall Performance: Plan2Gen achieves the highest average scores (3.73 on Gemini-2.0-Flash, 3.41 on InternVL3-78B), slightly outperforming the strong proprietary model GPT-4o. This demonstrates Plan2Gen’s effectiveness on com- plex instructions. (2) Closed- and Open-Source Alternatives: Plan2Gen, leveraging Gemini-2.0, achieves state-of-the-art results, rivaling or surpassing GPT-4o. Among open-source models, the Infinity series (AR-based) and Omnigen (diffusion-based) also per- form well, but Plan2Gen’s planning mechanism offers an advantage on long, complex prompts. (3) Diffusion vs. AR Architectures: The results, grouped by “Diffusion-based” and “AR-based” methods, show that AR-based models (Plan2Gen, GPT-4o) achieve top scores,Table 2: Dimension-wise average scores for all models across differ- ent evaluation dimensions, evaluated by Gemini-2.0-Flash. Method Obj. Backg. Color Texture Light Text Comp. Pose FX Avg. Diffusion-based Methods Nexus-Gen  1.71 3.19 3.27 3.34 2.27 1.35 2.44 1.81 1.38 2.31 GoT  2.50 3.10 3.84 3.50 2.66 1.80 2.97 2.10 1.99 2.72 FLUX.1-dev  3.24 3.46 4.04 3.91 3.29 2.11 3.78 2.71 1.72 3.14 Omnigen  3.14 3.70 4.18 3.71 3.02 2.42 3.81 2.69 2.55 3.25 AR-based Methods LlamaGen-3B  1.01 1.34 2.35 2.05 1.66 1.35 1.03 1.12 1.85 1.53 Janus-pro-1B  2.23 2.60 2.94 2.92 2.09 1.58 2.36 1.84 1.60 2.24 Janus-pro-7B  2.67 2.95 3.54 3.34 2.61 1.74 2.83 2.01 1.73 2.60 Infinity-2B  2.99 3.49 4.09 3.65 2.96 2.14 3.54 2.59 2.69 3.13 Infinity-2B-reg  3.05 3.62 4.13 3.85 3.21 2.14 3.54 2.44 2.13 3.12 Infinity-8B  3.36 3.96 4.34 4.12 3.54 2.48 4.08 2.72 2.35 3.44 Lumina-mGPT  3.20 3.38 4.10 4.02 3.24 2.14 3.47 2.56 1.89 3.11 Gemini-2.0  3.60 3.75 4.34 3.91 3.39 2.64 4.21 3.02 2.56 3.49 Gemini-2.0 w/ CoT 2.79 3.20 3.91 3.37 3.20 2.16 3.26 2.47 2.34 2.96 GPT-4o  3.94 4.03 4.56 4.17 3.39 2.86 4.22 3.36 2.78 3.70 Plan2Gen (Ours) 3.76 4.22 4.43 3.80 3.72 2.86 4.43 3.29 3.06 3.73 Table 3: Dimension-wise average scores for all models, evaluated by InternVL3-78B. Method Obj. Backg. Color Texture Light Text Comp. Pose FX Avg. Diffusion-based Methods Nexus-Gen  1.78 2.88 2.84 2.90 2.42 1.31 2.35 1.64 1.44 2.17 GoT  2.28 2.91 3.33 3.07 2.63 1.67 2.84 1.91 1.99 2.51 FLUX.1-dev  2.86 3.04 3.52 3.39 2.99 1.92 3.47 2.26 1.55 2.78 Omnigen  2.79 3.25 3.67 3.37 2.84 2.29 3.48 2.41 2.56 2.96 AR-based Methods LlamaGen-3B  1.00 1.08 1.57 1.35 1.14 1.01 1.01 1.02 1.56 1.19 Janus-pro-1B  2.16 2.61 2.70 2.59 2.27 1.56 2.37 1.88 1.81 2.21 Janus-pro-7B  2.47 2.91 3.15 3.01 2.66 1.69 2.83 1.97 1.85 2.50 Infinity-2B  2.72 3.38 3.82 3.52 3.03 2.10 3.44 2.24 2.49 2.97 Infinity-2B-reg  1.78 2.88 2.84 2.90 2.42 1.31 2.35 1.64 1.44 2.17 Infinity-8B  2.98 3.53 3.97 3.73 3.30 2.30 3.79 2.36 2.21 3.13 Lumina-mGPT  2.80 3.10 3.57 3.40 3.00 1.96 3.18 2.23 1.79 2.78 Gemini-2.0  3.10 3.29 3.96 3.48 3.15 2.34 3.88 2.66 2.51 3.15 Gemini-2.0 w/ CoT 2.47 2.94 3.46 3.03 2.90 2.00 2.97 2.12 2.43 2.70 GPT-4o  3.33 3.53 4.27 4.13 3.47 2.30 3.93 2.93 2.60 3.39 Plan2Gen (Ours) 3.28 3.76 4.09 3.53 3.51 2.56 4.18 2.92 2.89 3.41 especially with advanced planning. While some AR models lag be- hind leading diffusion models, Plan2Gen’s layered approach excels in maintaining coherence on lengthy, multi-faceted prompts. (4) Di- mensional Strengths: Plan2Gen leads in “Background”, “Lighting”, “Text”, “Composition”, and “Special Effects (FX)”. GPT-4o performs best on “Object(s)”, “Color”, “Texture”, and “Pose”, though Plan2Gen remains highly competitive. Notably, both Plan2Gen and GPT-4o achieve the best results on the challenging “Text” dimension. (5) Evaluator Consistency: Model rankings and performance trends are consistent across both evaluators, with Plan2Gen and GPT-4o as top performers. This cross-evaluator agreement further validates our findings. 5.3 Human Evaluation To complement our automated metrics, we conducted pairwise human evaluations comparing Plan2Gen against Gemini-2.0 and GPT-4o on a subset of benchmark prompts. Evaluators chose the image that better represented the prompt across nine visual dimen- sions, with results shown in Figure 4. Against Gemini-2.0 (Figure 4, left), Plan2Gen demonstrated a significant advantage, achieving 5 Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Zhou et al. 0 20 40 60 80 100 Percentage (%)43.8% 42.7% 56.7% 60.0% 54.0% 63.3% 75.2% 64.7% 61.3%25.0% 16.7% 20.0% 8.0% 10.0% 13.3% 10.5% 10.0% 20.0%31.2% 40.7% 23.3% 32.0% 36.0% 23.3% 14.4% 25.3% 18.7%Object Background Color T exture Lighting T ext Composition Pose EffectsAgainst Gemini-2.0 0 20 40 60 80 100 Percentage (%)40.0% 46.7% 46.0% 43.3% 45.3% 29.3% 28.0% 37.3% 55.6%13.3% 19.3% 20.7% 31.3% 14.7% 44.0% 45.3% 29.3% 10.5%46.7% 34.0% 33.3% 25.3% 40.0% 26.7% 26.7% 33.3% 34.0%Against GPT-4o Win Tie Lose Figure 4: Comparative performance evaluation across nine visual dimensions when our model is compared against Gemini-2.0 (left) and GPT-4o (right). Each horizontal bar shows the proportion of images where our model was rated as better (Win), equivalent (Tie), or worse (Lose). 102103 Perplexity (log scale)1234Avg. Evaluation ScoreJanus-Pro-1B 102103 Perplexity (log scale)Janus-Pro-7B /uni0000002f/uni00000052/uni0000005a /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni0000002b/uni0000004c/uni0000004a/uni0000004b /uni00000033/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000053/uni00000016/uni00000011/uni00000019/uni0000001b/uni00000016/uni00000011/uni0000001a/uni00000013/uni00000016/uni00000011/uni0000001a/uni00000015/uni00000016/uni00000011/uni0000001a/uni00000017/uni00000016/uni00000011/uni0000001a/uni00000019/uni00000016/uni00000011/uni0000001a/uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 Figure 5: (Left) Instruction perplexity vs. average evaluation score for Janus-Pro models. Each point represents a sample. The red line indicates the linear fit in log10perplexity space. (Right) Relationship Between Plan Consistency and Generation Quality. higher win rates in all nine dimensions. The comparison with GPT- 4o (Figure 4, right) was more competitive. Plan2Gen achieved a higher win rate than loss rate in eight of the nine dimensions. GPT-4o held a slight edge only in “Object” fidelity. 5.4 Empowering Visual Generation with Language Understanding? We investigated the relationship between a model’s language un- derstanding, gauged by instruction perplexity (PPL), and the quality of its visual outputs. Figure 5 (left) reveals a notable trend for the Janus-Pro-1B model: a counter-intuitive positive correlation where prompts with higher PPL (indicating poorer textual comprehension by the model) sometimes yielded images with higher evaluation scores. This suggests a discernible gap between the current lan- guage understanding capabilities of such multi-modal models and their ability to translate that understanding into high-fidelity visual generation; simply achieving low PPL on a prompt does not guar- antee superior image output. Interestingly, this counter-intuitive trend is notably attenuated in the larger Janus-Pro-7B model. The general improvement in scores and a less pronounced positive PPL-score correlation for the 7B model suggest that increasing model scale can, to some extent, help bridge this gap, leading to a more consistent (though still imperfect) alignment where better text understanding more reliably yields higher-quality images. This highlights that while language understanding is a target, current MLLMs, especially smaller ones, find it particularly challenging to effectively translate their language understanding into visual synthesis, a difficulty that larger ones appear to partially alleviate.Table 4: Dimension-wise scores across different refinement steps. Steps Obj. Backg. Color Texture Light Text Comp. Pose FX Avg. 1 3.76 4.17 4.40 3.77 3.67 2.79 4.38 3.21 2.95 3.68 3 3.76 4.22 4.43 3.80 3.72 2.86 4.43 3.29 3.06 3.73 5 3.76 4.11 4.47 3.77 3.71 2.79 4.45 3.30 3.05 3.71 7 3.70 4.12 4.42 3.74 3.65 2.75 4.29 3.18 3.02 3.65 Table 5: Comparison of different planning models. Planner Obj. Backg. Color Texture Light Text Comp. Pose FX Avg. Plan2Gen (Gemini 2.0 Flash) 3.76 4.22 4.43 3.80 3.72 2.86 4.43 3.29 3.06 3.73 Plan2Gen (Gemini 1.5 Flash-8b) 3.41 3.63 4.26 3.54 3.55 2.50 3.89 3.25 2.76 3.42 5.5 Analysis of Plan2Gen Impact of Refine Iteration We investigated the effect of varying the maximum number of refinement iterations per layer within the Plan2Gen agent, with results shown in Table 4. The findings demonstrate that iterative refinement is beneficial, as performance improves when increasing from a single refinement step. The over- all average score peaks when allowing up to 3 refinement steps. Beyond this point, allowing more iterations (e.g., 5 or 7 steps) does not consistently improve results and can lead to a slight degra- dation in overall average scores. This suggests that while a few refinement cycles are effective for enhancing alignment with sub- prompts, excessive iterations may risk over-correction or semantic drift. Therefore, a maximum of 3 refinement steps per layer offers an optimal balance, and this configuration was used for Plan2Gen in our main experiments. Planning Analysis The initial scene decomposition plan signifi- cantly impacts Plan2Gen’s performance. Table 5 shows that using a more capable LLM (Gemini 2.0 Flash) as the planner leads to markedly better final image quality compared to a less capable one (Gemini 1.5 Flash-8b), underscoring the importance of high-quality planning. Furthermore, we assessed plan quality by evaluating the consistency of decomposed sub-prompts against the original in- struction using Qwen3-32B, categorizing plans into Low, Medium, and High consistency groups. Figure 5 (right) clearly demonstrates a positive correlation: higher plan consistency scores directly cor- respond to higher average evaluation scores for the final generated images. These findings validate that a well-structured and faith- ful plan is a key determinant for successfully generating complex scenes from long-form descriptions. 6 Conclusion In this work, we introduced LongBench-T2I, a novel benchmark with complex, high-quality instructions for evaluating long-context text-to-image generation, and Plan2Gen, an agent framework that tackles these instructions via strategic scene decomposition and progressive layered refinement. Our experiments demonstrate that Plan2Gen achieves state-of-the-art performance, outperforming diverse models on LongBench-T2I, with human evaluations corrob- orating these results. LongBench-T2I and Plan2Gen serve as useful resources for the community in the ongoing development of image generation models for complex and detailed instructions. 6 A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation Conference acronym ’XX, June 03–05, 2018, Woodstock, NY References James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.2023. Improving im- age generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf 2, 3 (2023), 8. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811 (2025). Google DeepMind. 2025. Gemini 2.0 Flash Experimental API (Image Generation). https://aistudio.google.com/prompts/new_chat Experimental model accessed via Gemini API, May 2025. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767 (2023). Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al.2025. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639 (2025). Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. 2025. Can We Generate Images with CoT? Let’s Verify and Reinforce Image Generation Step by Step. arXiv preprint arXiv:2501.13926 (2025). Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been Kim, and Zi Wang. 2024. Proactive Agents for Multi-Turn Text-to-Image Genera- tion Under Uncertainty. arXiv preprint arXiv:2412.06771 (2024). Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. 2024. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431 (2024). Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In EMNLP. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. 2024. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135 (2024). Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023. T2i- compbench: A comprehensive benchmark for open-world compositional text-to- image generation. Advances in Neural Information Processing Systems 36 (2023), 78723–78747. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. 2025. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703 (2025). Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Xinyao Liao, Wei Wei, Xiaoye Qu, and Yu Cheng. 2025. Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning. arXiv preprint arXiv:2505.19196 (2025). Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. 2024. Lumina-mgpt: Illuminate flexible photorealistic text- to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657 (2024). Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, and Shin’ichi Satoh. 2023. Toward verifiable and repro- ducible human evaluation for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14277–14286. Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. 2024. DiffusionGPT: LLM-driven text-to-image generation system. arXiv preprint arXiv:2401.10061 (2024). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684–10695. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems 35 (2022), 36479–36494. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. Objects365: A large-scale, high-quality dataset for ob- ject detection. In Proceedings of the IEEE/CVF international conference on computer vision. 8430–8439. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. 2024. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525 (2024). Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, and Marie-Francine Moens. 2024. Object-attribute binding in text-to-image generation: Evaluation and control. arXiv preprint arXiv:2404.13766 (2024). Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8228–8238. Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guang- hao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al.2025. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv preprint arXiv:2503.01298 (2025). Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. 2024. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems 37 (2024), 128374–128395. Tsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li, and Trevor Darrell. 2024. Self-correcting llm-controlled diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6327–6336. Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. 2024. Deep reward supervisions for tuning text-to-image diffusion models. In European Conference on Computer Vision. Springer, 108–124. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. 2024. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340 (2024). Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. Imagereward: Learning and evaluating human prefer- ences for text-to-image generation. Advances in Neural Information Processing Systems 36 (2023), 15903–15935. Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, and Yu Zhang. 2025. Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing. arXiv preprint arXiv:2504.21356 (2025). Yucheng Zhou, Xiang Li, Qianning Wang, and Jianbing Shen. 2024. Visual in- context learning for large vision-language models. arXiv preprint arXiv:2402.11574 (2024). Yucheng Zhou, Jianbing Shen, and Yu Cheng. 2025. Weak to strong generalization for large language models with multi-capabilities. In The Thirteenth International Conference on Learning Representations. Yucheng Zhou, Jihai Zhang, Guanjie Chen, Jianbing Shen, and Yu Cheng. 2024. Less is more: Vision representation compression for efficient video generation with large language models. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al.2025. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models. arXiv preprint arXiv:2504.10479 (2025). Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. 2025. VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model. arXiv preprint arXiv:2501.12327 (2025). 7",
  "text_length": 44496
}