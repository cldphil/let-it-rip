{
  "id": "http://arxiv.org/abs/2505.24846v1",
  "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
  "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
  "authors": [
    "Jingyan Shen",
    "Jiarui Yao",
    "Rui Yang",
    "Yifan Sun",
    "Feng Luo",
    "Rui Pan",
    "Tong Zhang",
    "Han Zhao"
  ],
  "published": "2025-05-30T17:44:28Z",
  "updated": "2025-05-30T17:44:28Z",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24846v1",
  "full_text": "arXiv:2505.24846v1 [cs.AI] 30 May 2025MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning Jingyan Shen2*, Jiarui Yao1*, Rui Yang1*, Yifan Sun1, Feng Luo3, Rui Pan1, Tong Zhang1,Han Zhao1 1University of Illinois at Urbana-Champaign, 2Columbia University,3Rice University Abstract Reward modeling is a key step in building safe foundation models when applying reinforce- ment learning from human feedback (RLHF) to align Large Language Models (LLMs). How- ever, reward modeling based on the Bradley- Terry (BT) model assumes a global reward function, failing to capture the inherently di- verse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribu- tion of diverse subgroups, a single BT model has an irreducible error. While existing solu- tions, such as multi-objective learning with fine- grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, Mi- CRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experi- ments on multiple preference datasets demon- strate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization. 1 Introduction Reinforcement Learning from Human Feedback (RLHF) unlocks a promising pathway to improve the performance, reliability, and adaptability of AI *Equal contribution. Emails: {jiarui14, ry21, yifan50, ruip4, tozhang, hanzhao}@illinois.edu, js5544@columbia.edu,fl38@rice.edusystem deployment (Bai et al., 2022; Dong et al., 2023a; Achiam et al., 2023; Dong et al., 2024). Rather than relying on handcrafted reward models, the prevailing approach in RLHF employs pref- erence learning (Christiano et al., 2017) to infer reward scores from human feedback, particularly for tasks involving subjective evaluation and open- ended responses without unanimous ground truths (Ziegler et al., 2019). However, most existing meth- ods rely on binary-labeled pairwise datasets build- ing upon the assumption that there exists a global reward function that can model human preferences. This fails to capture the diverse and often contra- dictory nature of human preferences, ultimately limiting their effectiveness for personalized and pluralistic alignment (Chakraborty et al., 2024a; Yang et al., 2024b; Mukherjee et al., 2024; Luo et al., 2025). Advancing preference learning to better accom- modate heterogeneous human preferences remains an open challenge. Some recent studies seek to capture the diversity by collecting multifaceted an- notations that distinguish between different eval- uation attributes, e.g., helpfulness, harmlessness, coherence, instruction-following, etc (Wang et al., 2024b; Bai et al., 2022; Wang et al., 2024c). Al- though fine-grained labels provide deeper insights into individual preferences, collecting and curat- ing them significantly increases data acquisition costs. Consequently, existing datasets limit their scope to a handful of pre-defined attributes and of- ten rely on LLM-as-a-Judge (Zheng et al., 2023) for labeling response pairs. This raises concerns about their fidelity in representing the nuanced and ever-evolving landscape of human values. In addition, while users may share common in- terests, such as preferring a helpful and harmless assistant, their expectations are ultimately individ- ualized and depend on use cases, i.e., contextual factors, as illustrated in Fig 1. To better capture such personalization, some approaches construct 1 0.72 0.030.06 0.12 √ó √ó √ó √ó. + ‚âª ùëù1 ùëù2 ùëù3 ùëùùêæ +Figure 1: Illustration of the two-stage pipeline of MiCRo for capturing personalized preferences. A mixture of reward models (Stage 1) is trained on binary-labeled data, while the context-aware router (Stage 2) dynamically adjusts preference distributions based on user-provided context. The final preference distribution is obtained through a convex combination of different preference distributions. datasets with elaborate and pluralistic contexts into user prompts (Pitis et al., 2024; Yang et al., 2024b) or system instructions (Lee et al., 2024a). While reward models trained on such enriched datasets have shown improved generalization to personal- ized preferences, designing these criteria manually is still labor-intensive. In this work, we introduce MiCRo, a two-stage, context-aware mixture modeling framework that leverages large-scale binary preference datasets to improve personalized preference learning. We first provide a theoretical result showing that when the underlying preference distribution follows a mix- ture of subpopulations, preference learning based on a single Bradley-Terry (BT) loss incurs an irre- ducible error. To address this, we propose a context- aware mixture modeling approach to decompose aggregate preferences into latent subpopulations, each with a distinct reward function. To further adapt to personalized preferences, we propose an online routing strategy with additional contextual information. In summary, our method offers two key advantages: ‚Ä¢MiCRo extracts multifaceted human prefer- ences from widely available pairwise comparison datasets without requiring explicit fine-grained annotations or predefined attributes. ‚Ä¢MiCRo adapts the mixture heads to personalized preference learning with contextual information with only a limited number of samples. Our extensive experiments across multiple pref- erence datasets empirically demonstrate that Mi- CRo ‚Äôs mixture heads effectively capture diversepreferences and achieve superior performance com- pared to baselines on multidimensional bench- marks. With the addition of context-aware rout- ing, the full MiCRo framework matches the perfor- mance of fully supervised and test-time adaptation methods, underscoring its effectiveness in enhanc- ing downstream personalization. 2 Related Work Reward modeling aims to learn a function that as- signs scalar scores to input‚Äìoutput pairs based on human preferences, playing a central role in RLHF by steering LLM behavior toward human-aligned outputs and mitigating harmful responses (He et al., 2024a; Sun et al., 2024). The typical approach adopts the BT model (Bradley and Terry, 1952; Christiano et al., 2017; Stiennon et al., 2020) to learn from pairwise comparisons. To further ad- dress the diversity of human preferences, person- alized preference learning seeks to align LLMs with user values in underspecified settings with ambiguous or heterogeneous intent (Fleisig et al., 2023; Baumler et al., 2023; Chakraborty et al., 2024b). One major approach focuses on multi- objective alignment through ensembles of reward models. Techniques such as Mixture-of-Experts and model merging are used to decompose reward functions into task-specific or capability-based components (Quan, 2024; Wang et al., 2024b; Rame et al., 2023; Wang et al., 2024a). However, training multiple reward models typically requires manually defined preference dimensions and dense supervision. To mitigate this, HyRe (Lee et al., 2024b) trains an ensemble offline and adapts it to in- dividual users at test time by dynamically reweight- 2 ing the components using a small number of user- specific examples. Recent work, DRMs (Luo et al., 2025), decomposes human preferences into a linear space using PCA, offering a promising training- free solution; however, its effectiveness depends on the choice of embedding model. A complementary line of work uses probabilistic approaches to model subgroup or latent preferences without explicit su- pervision (Siththaranjan et al., 2023; Poddar et al., 2024; Chen et al., 2024; Chakraborty et al., 2024a), but their potential for personalization remains un- derexplored. 3Limitation of a Single Reward Function 3.1 Problem Setup Notation and Preliminary LetXdenote the space of prompts, and Ydenote the space of re- sponses. Denote ‚àÜK={x‚ààRK|PK i=1xi= 1, xi‚â•0, i= 1,..., K }as the (K‚àí1)- dimensional probability simplex. In standard pref- erence learning, human preferences are modeled based on the classic BT model. Specifically, for a given prompt x‚àà X and an LLM œÄ, two candidate responses aw, al‚àà Y are sampled independently from œÄ(¬∑ |x). The probability that a human anno- tator prefers awoveralis given by: P(aw‚âªal|x) =œÉ(r‚àó(x, aw)‚àír‚àó(x, al)), where œÉdenotes the logistic function and r‚àó: X √óY ‚Üí Ris a latent reward function. For brevity, we assume aw‚âªal(i.e.,awis always preferred overal). In practice, a static, finite-sample prefer- ence dataset is collected, and r‚àóis estimated via maximum likelihood. Mixture Reward Distribution However, in prac- tice, reward data are collected from a population of annotators with inherently diverse preferences. Prior work has demonstrated that modeling all hu- man preferences with a single parametric reward function leads to systematic underfitting and can- not capture such heterogeneity (Chakraborty et al., 2024a; Siththaranjan et al., 2023). To better reflect this diversity, we assume that each observed anno- tation is generated from one of the Klatent subpop- ulations, where Kis treated as a hyperparameter. A latent categorical variable z‚àà {1,..., K }is in- troduced as an indicator of the subpopulation from which a preference pair originates. We introduce the overall probability of a preference observation as a context-aware mixture of KBradley-Terrymodels: P(aw‚âªal|x) =KX k=1P(z=k|x) ¬∑P(aw‚âªal|x, z=k),(1) where the weights of each mixture component de- pend on the prompt xand the probability of prefer- ence within a specific subpopulation is given by P(aw‚âªal|x, z=k) =œÉ(r‚àó k(x, aw)‚àír‚àó k(x, al)), andr‚àó kis assumed to be a latent reward function for subpopulation k. 3.2 Irreducible error of single BT model In this section, we provably show that, when the underlying preference distribution is a mixture of BT models, no matter how rich the model class is for reward functions, preference learning based on a single BT model has an irreducible error. Before we present our result, we first assume the diversity of the underlying population: Assumption 3.1 (Diversity).There exists a con- stantœÅ >0, such that for every prompt x‚àà X and every subpopulation group k‚àà[K],P(z=k| x)‚â•œÅ. For every tuple (x, aw, al), define the score func- tions‚àó kfor group kass‚àó k(x, aw, al):=r‚àó k(x, aw)‚àí r‚àó k(x, al). LetLCE(r)be the cross-entropy loss of a BT preference model P(aw‚âªal|x) = œÉ(r(x, aw)‚àír(x, al))according to the reward function r, Theorem 3.2 (Error lower bound).For an arbitrary reward function r, if the predicted preference is based on a single BT model, thenLCE(r)‚â•2œÅKExVarz[{Es‚àó k‚ôØœÄ(aw, al| x)}K k=1] +H(x, œÄ,P(z|x)). We defer the detailed proof of Theorem 3.2 to Appendix A. In the lower bound, s‚àó k‚ôØœÄ(aw, al|x) is the induced distribution of s‚àó kacting on the pair of responses (aw, al)‚àºœÄfrom the LLM given the prompt x, and the variance operator Varzis applied with respect to the Kgroups. H(x, œÄ,P(z|x))is the Shannon entropy of the joint distribution over preference data given by the prompt, subpopula- tions, as well as the LLM œÄ(c.f. Appendix A for its definition). At a colloquial level, the lower bound says that, the more diverse the ground-truth scores s‚àó kfrom each subpopulation (hence a larger vari- ance), or the subpopulation distribution P(z|x) (hence a larger œÅand entropy), then the larger the cross-entropy loss of using a single BT model. 3 Table 1: Comparison of different methods and their key characteristics. MiCRo optimizes a mixture of BT loss using binary labels and enables context-aware routing, setting it apart from prior methods in terms of context conditioning and weight learning. Method Binary Labels Context Conditioning Reward Objective Weight Learning Special Characteristic ARMO (Wang et al., 2024b) √ó √ó Mean Square Error End-to-end BT Fixed Num of attributes MaxMin (Chakraborty et al., 2024a) ‚úì √ó Mixture of BT Hard clustering Minority preference optimization HyRe (Lee et al., 2024b) ‚úì √ó BT Accuracy maximization Test-time adaptation DRMs (Luo et al., 2025) ‚úì √ó ‚Äì Accuracy maximization Training-free reward decomposition via PCA MiCRo (Ours) ‚úì ‚úì Mixture of BT Hedge Algorithm Context-aware Routing 4 Method The inherent limitation of a single BT model mo- tivates the need for richer preference modeling. However, two key challenges remain: (C1) How to extract a mixture of reward functions from binary- labeled datasets without incurring additional an- notation costs? (C2) Given limited access to user- specific intent, how can we efficiently adapt to per- sonalized preferences at deployment time? To this end, we propose a two-stage algorithm that first uncovers latent heterogeneity in human preferences through mixture modeling, and then adapts to individual users via a lightweight, context- aware online routing strategy. 4.1 Mixture Modeling for Diverse Preferences We begin by fundamentally comparing our mixture modeling objective with previous methods and then introduce the detailed design of our approach. Comparison with Prior Mixture Modeling Ap- proaches Unlike static and unconditional mix- ture approach used in previous work (Chakraborty et al., 2024a), our formulation from Equation (1) introduces a dynamic, context-aware weighting mechanism for mixture models by conditioning the subpopulation weights P(z=k|x)on the given prompt x. We emphasize that this is a cru- cial design that allows for contextual specialization, where prompts automatically activate the most rel- evant subpopulation‚Äôs reward model. By mimick- ing real-world expertise allocation, our approach avoids the diluted performance of static averag- ing. We provide a more detailed comparison of our method with existing works in Table 1. Mixture Modeling Designs In practice, we pa- rameterize the reward function for each subpop- ulation as rœïk:X √ó Y ‚Üí Rfork= 1,..., K and model the mixture weights with a network fœà:X ‚Üí ‚àÜK. Given a training dataset D= {(x, aw, al)i}n i=1, we minimize the negative log-likelihood defined as: Lmle=‚àí1 nX (x,aw,al)‚ààDlogKX k=1\u0010 fœà,k(x) ¬∑œÉ(rœïk(x, aw)‚àírœïk(x, al))\u0011.(2) To prevent any single model from dominating, we add a regularization term by imposing a uniform prior to the weight distribution: Lreg=1 nX (x,aw,al)‚ààDKX k=1fœà,k(x) logfœà,k(x).(3) The final loss function becomes L(œï,œà) =Lmle+Œ±Lreg, (4) where the coefficient Œ±is set to 0.5in our imple- mentation. Overall, this mixture training phase on large-scale datasets learns a diverse set of re- ward functions, establishing a robust foundation for adaptation to nuanced preferences. 4.2 Context-aware Routing for Personalized Preference Learning While the pre-trained mixture model has the poten- tial to capture latent reward functions, assigning meaningful weights to these reward heads upon a given prompt is difficult without a clear signal of user intent. Technically, during the training of the mixture model, since we do not have labeled data to train the router separately, we will need one strategy to learn the correspondence between the mixture reward heads and the underlying user intent for better routing assignments. Recent work on contextual alignment (Pitis et al., 2024; Lee et al., 2024a; Poddar et al., 2024) high- lights that incorporating context can reduce ambi- guity and improve estimation accuracy. Motivated by this, we introduce a second stage that incorpo- rates more concrete contextual information‚Äîsuch as user instructions or metadata (e.g., demograph- ics or interaction history)‚Äîto guide the routing 4 strategy by learning the correspondence between user intent and mixture reward heads. Unlike prior methods that require training re- ward models on large-scale contextual datasets (Pitis et al., 2024; Lee et al., 2024a), our approach avoids costly data collection and full model retrain- ing. Instead, we leverage the unsupervised mixture heads pre-trained in the first stage to enable sample- efficient online adaptation. This allows us to refine the mixture weights and generate personalized pre- dictions using only a small number of samples. To this end, we propose to fine-tune the routing network fœàusing the Hedge algorithm (Arora et al., 2012), where each input is a pair (xi, ci)‚àº D c, with cidenoting additional context information. Intuitively, Hedge maintains a set of experts (i.e., reward heads) and adaptively reweights them based on their performance‚Äîassigning higher weights to those that better align with observed prefer- ences. In our framework, the user preferences can be modeled as a convex combination of the Klatent subpopulation preferences. For an ex- ample (xi, ci, ai,w, ai,l), denote the output prob- ability from k-th head as pk(ai,w‚âªai,l|xi):= œÉ(rœïk(xi, ai,w)‚àírœïk(x, ai,l))and define Li,k:=‚àílogpk(ai,w‚âªai,l|xi, ci). We consider an online learning setting where contextual data is collected within a budget of B. We acquire a batch of preference pairs DA= {(x, c, a w, al)i}B i=1with additional contexts. Moti- vated by the multi-task learning literature (He et al., 2024b; Liu et al., 2024), we propose a training ob- jective for the router based on the framework of online mirror descent with KL divergence regular- ization (Hazan et al., 2016): min œà1 BBX i=1Li,s.t.fœà(xi, ci)‚àà‚àÜK, where Li:=PK k=1fœà(xi, ci)kLi,k+ œÑKL(fœà(xi, ci)‚à•œâi)andœâiis a weight vec- tor that comes from a previous iteration or pre-trained weights, and œÑ‚â•0is a temperature hyperparameter. Note that the first term is an upper bound of the negative log-likelihood function of mixture distribution based on Jensen‚Äôs inequality. Routing with Hedge Algorithm With œÑ > 0, the optimal solution for each batch will be: fi,k=œâi,kexp (‚àíLik/œÑ)PK j=1œâi,jexp (‚àíLij/œÑ), (5)which yields Algorithm 1 to learn the router it- eratively with contextual information. For each iteration, we determine optimal weights target as soft labels using Equation (5). Then, we fine-tune the router by minimizing the cross-entropy loss between the soft labels and the router network‚Äôs predictions. Algorithm 1 Context-aware Router Learning Input: Mini-batch {(xi, ci, ai w, ai l, yi)}Bt i=1, tem- perature œÑ, pre-trained router fœà, itera- tions T, reward heads from the first stage rœïk,‚àÄk= 1,..., K. Initialize œà(1)=œà,œâi,k=fœà,k(xi) fort= 1toTdo // weight update fori= 1toBtdo fork= 1toKdo Lik‚Üê ‚àí logpk(ai w‚âªai l|xi) œâi,k‚Üêfœà(t)(xi, ci)k¬∑exp\u0000‚àíLik œÑ\u0001 end Z‚ÜêPK j=1œâi,j œâi,k‚Üêœâi,k Zfork= 1,..., K end // router update L(t) weight(œà)‚Üê1 BtBtX i=1LCE\u0000 œâi, fœà(t)(xi, ci)\u0001 Backprop L(t) weight(œà)to transition from œà(t) toœà(t+1) end Our routing learning approach offers two clear advantages in deployment: 1) efficiency: By leveraging the expert heads trained on large-scale datasets in the first stage, the second stage does not require retraining the reward model or relying on extensive labeled data; instead, a lightweight, on- line router continuously adapts during deployment. 2)generalizability: Our learning-based router har- nesses contextual information to adjust weights with a learning-based algorithm. Unlike test-time adaptation methods (Lee et al., 2024b) that rely on a set of test data for re-weighting, our router is trained online, allowing generalizing to new con- texts without access to specific test data. 5 Experiments In our experiments, we aim to answer two research questions: (Q1) Can our context-aware mixture modeling framework extract diverse reward func- 5 tions from binary-labeled preference data? (Q2) Can the fine-tuned routing network enable effective personalization by adapting to contextual signals? 5.1 Experimental Setup Training datasets We train the mixture reward models on binary-labeled preference datasets: HelpSteer2 (Wang et al., 2024c), RPR (Pitis et al., 2024), and preference-700K (Dong et al., 2024). HelpSteer2 and RPR datasets contain human- labeled response pairs evaluated across multiple assessment dimensions. We construct the binary- labeled sets with the following process. For each di- mension, we extract binary preference pairs based on absolute ratings, treating responses with higher ratings as ‚Äúchosen‚Äù and those with lower ratings as ‚Äúrejected.‚Äù Pairs with identical ratings are ex- cluded from both training and test sets. To ensure diversity in preferences, we exclude pairs where all attributes are unanimously agreed upon from the training set. Ultimately, all pairs from all dimen- sions are mixed, resulting in 23.5K samples and 5.8K training samples from HelpSteer2 and RPR, respectively. We also apply the mixture model training on preference-700K dataset, a large-scale pairwise dataset created by aggregating instructions and response pairs from multiple data sources. Fur- ther details on these datasets are provided in Ap- pendix D.2. Models We use the best 3B open-source reward model GRM-Llama3.2-3B (Yang et al., 2024a) as the backbone, keeping it frozen while training K linear probing heads on top. The router network is implemented as a one-layer MLP containing 128 units and a softmax activation. We provide full implementation details in Appendix D.1. Baselines We evaluate the following baselines: 1) Single Reward: A single-head model trained using the standard BT loss. 2) Static Mixture: A simpli- fied variant of our method, corresponding to the ap- proach used in MaxMin-RLHF (Chakraborty et al., 2024a), where the mixture model is trained with fixed, input-independent weights, without leverag- ing contextual information. 3) Shared-Base Ensem- ble Model: Lee et al. (2024b) introduces HyRe, a test-time adaptation approach based on ensemble networks. We adopt the multi-head architecture with a frozen prior network and multiple trainable heads, optimizing a uniformly weighted sum of BT losses. 4) Fully Supervised Model: We include ARMO (Wang et al., 2024b), an 8B model trainedon more than 500K fine-grained labels, as a base- line with full supervision. 5.2 Stage-1 Evaluation: Can MiCRo Disentangle Diverse Human Preferences? Evaluation Setting In this experiment, we train MiCRo and baseline models (except ARMO) on HelpSteer2 and RPR training sets. We then evalu- ate the learned heads on the HelpSteer2 and RPR test sets, which cover 14 distinct preference dimen- sions. Each head is evaluated individually on every dimension. To ensure fair comparisons, we use the same number of heads Kfor MiCRo and other multi-head baselines. The full evaluation results of MiCRo mixture heads are provided in Appendix E. Results Fig 2 compares the best-performing head for each test dimension. The results demon- strate that different heads from MiCRo specialize in distinct evaluation dimensions and consistently surpass the performance of all baeslines across all dimensions. On average, MiCRo consistently achieves the highest average scores across both RPR (0.921) and HelpSteer2 (0.811) benchmarks, with substantial gains over the single-head baseline (+40.0% on RPR, +6.8% on HelpSteer2), the Share- base ensemble baseline (+20.7% and +9.1% re- spectively), and the mixture model without context routing (+5.5% and +1.1% respectively), demon- strating the robust benefits of context-aware mix- ture modeling approach. These results suggest that mixture modeling more effectively captures latent diverse preferences compared to single reward or ensemble models, and that context-aware weight- ing further improves over static mixtures. Fig 3 presents a qualitative example of mixture weights from the Stage 1 router, showing how different prompts from the RPR test set activate different heads. This highlights the effectiveness of our con- textual router compared to the unconditional router used in prior work. 5.3 Stage-2 Evaluation: Can MiCRo Adapt to Personalized Preference? User Context Datasets The RPR training dataset includes user-specific criteria for each preference pair, explicitly specifying the user‚Äôs intent and eval- uation dimension, and thus provides a well-defined source of user context. For HelpSteer2, we follow the approach of Pitis et al. (2024) and augment generic prompts with attribute-specific modifica- tions based on the original assessment dimensions 6 Figure 2: Comparison of accuracy scores between the best heads of MiCRo and other baselines on multiple test dimensions. The mixture heads can disentangle diverse human preferences, with different heads excelling on different attributes. They consistently outperform the single reward model across all attributes. Overlaps where the same head dominates multiple attributes may reflect inherent attribute correlations. Table 2: Accuracy scores on HelpSteer2 test set. On average, MiCRo outperforms baselines across various attributes and overall results. Scores in green indicate absolute improvement over the Single Reward baseline. All baselines except ARMO (8B) use the same 3B base model. Method Supervision Helpfulness Correctness Coherence Complexity Verbosity Average Single Reward Binary 0.7838 0.6686 0.6914 0.7907 0.8816 0.7632 Shared-Base Best Head Binary 0.7838 0.6628 0.7037 0.7519 0.8158 0.7436 Static Mixture Binary 0.7243 0.6570 0.6790 0.8372 0.9013 0.7598 ARMO (8B) Fine-grained 0.6919 0.6395 0.7593 0.7132 0.7500 0.7108 HyRe Binary + Test Labels 0.7692 0.6987 0.6781 0.7168 0.8015 0.7329 MiCRo-HyRe Binary + Test Labels 0.8270 0.7035 0.7407 0.8217 0.8487 0.7883 MiCRo (Ours) Binary + Context 0.8324 +0.05 0.7140 +0.04 0.7543 +0.06 0.7628 0.8513 0.7830 +0.02 Figure 3: Heatmaps of router weights for different prompts in MiCRo Stage 1. The router assigns varying weights to different heads depending on the prompt. in the annotation process (Wang et al., 2024c). Ex- amples of contexts are provided in Appendix D.2. For training and test datasets, we prepend the con- textual information to the user prompt and fine-tune the router accordingly. Evaluation Setting We assess personalized adap- tation under two scenarios: (1) In-distribution eval- uation. All models are trained on the HelpSteer2 and RPR training splits and evaluated on their re- spective test splits. (2) Cross-distribution gener- alization. Models are trained on HelpSteer2 and the large-scale preference-700K datasets, then eval- uated on the RPR test set to measure transfer to previously unseen user preferences.Implementation Details For MiCRo, we train the router using 50 context-annotated examples per attribute drawn from the training data. For the static mixture baseline, we keep the Stage-1 mix- ture weights fixed. For HyRe adaptation, we reuse the Stage-1 reward heads and derive adaptation weights from 16 labeled test samples per attribute. Results As shown in Tab.2 and Tab.3, MiCRo achieves average test accuracies of 0.7830 on Help- Steer2 and 0.8218 on RPR under the within-dataset evaluation setting, outperforming all three base- lines trained with binary labels. This highlights the effectiveness of the router in adapting to specific user preferences. The relatively lower performance in certain attributes can be attributed to noisy or inconsistent contextual signals. Compared to methods requiring stronger supervi- sion, MiCRo performs competitively with ARMO on RPR and outperforms it on HelpSteer2. Further- more, we find that applying test-time adaptation to MiCRo ‚Äôs mixture heads outperforms the orig- inal HyRe, indicating that our first-stage training provides a stronger base without requiring explicit supervision. While HyRe benefits from test-time 7 Table 3: Accuracy scores on the RPR test set. Scores in green indicate absolute improvement over the Single Reward baseline. All baselines except ARMO (8B) use the same 3B base model. Method Supervision Clarity CreativityScientific RigorUser- FriendlinessStorytelling PedagogicalLinguistic CreativityFactual AccuracyHumor Average Single Reward Binary 0.4717 0.6806 0.3333 0.7978 0.8375 0.6452 0.8654 0.4225 0.8810 0.6594 Shared-Base Best Head Binary 0.6226 0.7361 0.8095 0.6966 0.8000 0.6774 0.8558 0.7042 0.9643 0.7629 Static Mixture Binary 0.9057 0.6389 0.9048 0.6854 0.6250 0.7903 0.7404 0.8451 0.9167 0.7836 ARMO (8B) Fine-grained 0.9057 0.6806 0.9405 0.6966 0.7875 0.7903 0.9135 0.9014 0.9463 0.8403 HyRe Binary + Test Labels 0.7027 0.5893 0.6618 0.8493 0.6563 0.7826 0.7045 0.7091 0.4853 0.6823 MiCRo-HyRe Binary + Test Labels 0.9556 0.8125 0.9605 0.9012 0.8333 0.7963 0.9063 0.9524 0.9605 0.8974 MiCRo (Ours) Binary + Context 0.9170 +0.45 0.6289 0.8119 +0.48 0.8696 +0.07 0.7525 0.7935 +0.15 0.8558 0.8563 +0.43 0.9109 +0.03 0.8218 +0.16 Table 4: Performance on the RPR test set with models trained on HelpSteer2 and preference-700K dataset. MiCRo outperforms other baselines trained with binary labels on average scores. Training DatasetMethodClarity and ConcisenessCreativity and OriginalityScientific RigorUser- FriendlinessNarrative and Storytelling QualityPedagogical EffectivenessLinguistic CreativityFactual AccuracyHumor and EntertainmentAverage preferenceSingle Head 0.8679 0.6806 0.9286 0.6067 0.6500 0.8065 0.8077 0.9155 0.9405 0.8004 Shared-base Best Head 0.8302 0.8056 0.8095 0.8089 0.6500 0.7903 0.8654 0.8169 0.9167 0.8009 -700KStatic Mixture 0.8679 0.6250 0.9048 0.6292 0.6500 0.8065 0.7981 0.9014 0.9286 0.7902 MiCRo (Ours) 0.9358 0.6833 0.9190 0.6764 0.6700 0.7484 0.7827 0.9380 0.9405 0.8105 HelpSteer2Single Head 0.8491 0.6667 0.9048 0.6180 0.6500 0.8065 0.7404 0.8732 0.8690 0.7753 Shared-base Best Head 0.8491 0.6667 0.9048 0.6629 0.7000 0.8065 0.7404 0.8310 0.9048 0.7851 Static Mixture 0.9057 0.6389 0.9048 0.6854 0.6250 0.7903 0.7404 0.8451 0.9167 0.7838 MiCRo (Ours) 0.9245 0.6667 0.8690 0.7079 0.6875 0.8226 0.7692 0.8592 0.9286 0.8133 labels, it assumes access to labeled examples for each user at inference time. Context-aware routing offers a more practical alternative by generalizing to unseen users, but its performance depends on the quality of the context. In practice, we believe a richer and more informative context could further boost its effectiveness. Our ablation in Section 5.4 further shows that performance improves with ac- cess to more context samples. In general, these results highlight MiCRo as a practical and label- efficient solution to learn personalized preferences. Tab. 4 presents results under the unseen user set- ting, showing that MiCRo consistently outperforms other baselines trained with binary labels. This fur- ther demonstrates the router can generalize across user distributions with contextual information. Ad- ditional results on the RewardBench benchmark are provided in Appendix E. 5.4 Ablation Study We conduct an ablation study on two critical hyper- parameters in our method: the number of subpopu- lations Kand the router learning budget B. We in- clude a detailed study of Kin Appendix C.1. While too few subpopulations may limit the model‚Äôs abil- ity to capture diverse preferences, performance re- mains relatively stable as Kincreases. Fig 4 shows the convergence of context-aware routing in Stage 2 on the RPR and HelpSteer2 test sets as the number of context-labeled samples per attribute increases. At budget 50 (i.e., 450 and 250 examples in total for each dataset, respectively), the average accuracy across 9 attributes on RPR (a) RPR test set. (b) HelpSteer test set. Figure 4: Average accuracy across different context- labeling budgets per attribute. Accuracy is averaged over all dimensions in each test dataset. Shaded regions indicate the standard deviation across 5 independent runs. The curves show that performance tends to con- verge around budget 50. test set increases sharply from around 0.705 to over 0.841, while the accuracy on HelpSteer2 plateaus around 0.785. In both cases, performance improves steadily with larger budgets, with most gains oc- curring early, demonstrating that the router can efficiently adapt using only a small number of con- textual examples. 6 Conclusion In this work, we address the challenge of personal- ized preference learning by leveraging a large num- ber of binary-labeled datasets alongside a small set of fine-grained context-aware data. Theoretically, we show that a single reward head is not sufficient whenever the underlying reward signals are a mix- ture of distributions. Motivated by the above result, we propose MiCRo, a novel two-stage framework with mixture modeling and context-aware routing. 8 Through extensive experiments, we demonstrate that MiCRo effectively disentangles complex hu- man preferences and enhances downstream plural- istic alignment tasks. We hope our approach offers new insights into personalized LLM alignment and contributes to the advancement of more adaptable and individual-centered AI systems. 7 Limitations Although our formulation is general, there is a lim- ited availability of public datasets that provide rich and consistent user context information, making it difficult to comprehensively evaluate personaliza- tion capabilities. Our current implementation relies on access to explicitly defined context criteria and partially synthetic settings to simulate user-specific signals. However, in many real-world scenarios, user intent is often implicit‚Äîe.g., reflected in multi- turn dialogue, demographic metadata, or behavioral patterns. Incorporating such implicit user contexts into the routing process remains an important di- rection for future work. 8 Ethics Statement The paper introduces a two-stage, context-aware mixture modeling framework that uses large-scale binary preference datasets to improve personalized preference learning. All experiments are conducted using publicly available models and datasets. The licenses for all datasets are listed in Appendix D.2, and we ensure complete compliance with all the li- cense terms. While MiCRo improves scalability in personalized preference learning, the mixture heads are trained without explicit supervision. As a result, the learned mixture heads may encode preferences that are either beneficial or harmful, with a risk of inadvertently modeling undesirable or malicious intent. We therefore emphasize the importance of thorough evaluation and safety auditing to mitigate potential misuse. Acknowledgments This work is supported by an NSF IIS grant No. 2416897. HZ would like to thank the support from a Google Research Scholar Award. The views and conclusions expressed in this paper are solely those of the authors and do not necessarily reflect the official policies or positions of the supporting com- panies and government agencies. Additionally, we thank Hanyang Chen for his assistance and helpful input.References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Sanjeev Arora, Elad Hazan, and Satyen Kale. 2012. The multiplicative weights update method: a meta- algorithm and applications. Theory of computing, 8(1):121‚Äì164. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Connor Baumler, Anna Sotnikova, and Hal Daum√© III. 2023. Which examples should be multiply anno- tated? active learning when annotators may disagree. InFindings of the Association for Computational Linguistics: ACL 2023, pages 10352‚Äì10371. Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324‚Äì 345. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Kop- pel, Furong Huang, Dinesh Manocha, Amrit Bedi, and Mengdi Wang. 2024a. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Kop- pel, Furong Huang, Dinesh Manocha, Amrit Bedi, and Mengdi Wang. 2024b. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment. Daiwei Chen, Yi Chen, Aniket Rege, and Ramya Ko- rlakai Vinayak. 2024. Pal: Pluralistic alignment framework for learning from heterogeneous prefer- ences. arXiv preprint arXiv:2406.08469. Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar- tic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Ad- vances in neural information processing systems, 30. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting lan- guage models with high-quality feedback. Preprint, arXiv:2310.01377. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023a. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. 9 Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. Rlhf work- flow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863. Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. 2023b. Steerlm: Attribute conditioned sft as an (user-steerable) alter- native to rlhf. Preprint, arXiv:2310.05344. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with V-usable information. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988‚Äì6008. PMLR. Eve Fleisig, Rediet Abebe, and Dan Klein. 2023. When the majority is wrong: Modeling annotator disagreement for subjective tasks. arXiv preprint arXiv:2305.06626. Elad Hazan et al. 2016. Introduction to online convex optimization. Foundations and Trends ¬Æin Optimiza- tion, 2(3-4):157‚Äì325. Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, and Han Zhao. 2024a. Semi-supervised reward modeling via iterative self-training. arXiv preprint arXiv:2409.06903. Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng, Trishul Chilimbi, and Han Zhao. 2024b. Robust multi-task learning with ex- cess risks. arXiv preprint arXiv:2402.02009. Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and Yaodong Yang. 2024a. Pku-saferlhf: Towards multi-level safety alignment for llms with human preference. arXiv preprint arXiv:2406.15513. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024b. Beavertails: To- wards improved safety alignment of llm via a human- preference dataset. Advances in Neural Information Processing Systems, 36. Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. 2024a. Aligning to thousands of prefer- ences via system message generalization. Advances in Neural Information Processing Systems, 37:73783‚Äì 73829. Yoonho Lee, Jonathan Williams, Henrik Marklund, Archit Sharma, Eric Mitchell, Anikait Singh, and Chelsea Finn. 2024b. Test-time align- ment via hypothesis reweighting. arXiv preprint arXiv:2412.08812. Meitong Liu, Xiaoyuan Zhang, Chulin Xie, Kate Don- ahue, and Han Zhao. 2024. Online mirror descent for tchebycheff scalarization in multi-objective optimiza- tion. arXiv preprint arXiv:2410.21764.Feng Luo, Rui Yang, Hao Sun, Chunyuan Deng, Jiarui Yao, Jingyan Shen, Huan Zhang, and Hanjie Chen. 2025. Rethinking diverse human preference learning through principal component analysis. arXiv preprint arXiv:2502.13131. Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sen- gupta, Aniket Deshmukh, and Branislav Kveton. 2024. Multi-objective alignment of large language models through hypervolume maximization. arXiv preprint arXiv:2412.05469. Silviu Pitis, Ziang Xiao, Nicolas Le Roux, and Alessan- dro Sordoni. 2024. Improving context-aware pref- erence modeling for language models. Advances in Neural Information Processing Systems, 37:70793‚Äì 70827. Sriyash Poddar, Yanming Wan, Hamish Ivison, Ab- hishek Gupta, and Natasha Jaques. 2024. Person- alizing reinforcement learning from human feedback with variational preference learning. arXiv preprint arXiv:2408.10075. Shanghaoran Quan. 2024. Dmoerm: Recipes of mixture-of-experts for effective reward modeling. arXiv preprint arXiv:2403.01197. Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. 2023. Rewarded soups: towards pareto-optimal alignment by inter- polating weights fine-tuned on diverse rewards. Ad- vances in Neural Information Processing Systems, 36:71095‚Äì71134. Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. 2023. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural In- formation Processing Systems, NIPS ‚Äô20, Red Hook, NY, USA. Curran Associates Inc. Hao Sun, Yunyi Shen, and Jean-Francois Ton. 2024. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alterna- tives. arXiv preprint arXiv:2411.04991. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. 2024a. Arithmetic control of llms for di- verse user preferences: Directional preference align- ment with multi-objective rewards. arXiv preprint arXiv:2402.18571. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024b. Interpretable preferences via multi-objective reward modeling and mixture-of- experts. arXiv preprint arXiv:2406.12845. 10 Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024c. Helpsteer2: Open-source dataset for train- ing top-performing reward models. arXiv preprint arXiv:2406.08673. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. 2023. Help- steer: Multi-attribute helpfulness dataset for steerlm. Preprint, arXiv:2311.09528. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024a. Regularizing hidden states en- ables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216. Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. 2024b. Rewards- in-context: Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024. Advancing llm reasoning generalists with pref- erence trees. Preprint, arXiv:2404.02078. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595‚Äì46623. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Chris- tiano, and Geoffrey Irving. 2019. Fine-tuning lan- guage models from human preferences. arXiv preprint arXiv:1909.08593. 11 A Proof of Theorem 3.2 To ease the reading, we first restate Theorem 3.2 and provide its proof. Theorem 3.2 (Error lower bound).For an arbitrary reward function r, if the predicted preference is based on a single BT model, then LCE(r)‚â•2œÅKExVarz[{Es‚àó k‚ôØœÄ(aw, al|x)}K k=1] +H(x, œÄ,P(z|x)). Proof. For uncluttered notation, we use Œ≥k(x)to denote the mixture weight P(z=k|x)when the prompt is x. Based on Assumption 3.1, ‚àÄx‚àà X, k‚àà[K],Œ≥k(x)‚â•œÅ. For a tuple (x, aw, al), define the score s(x, aw, al):=r(x, aw)‚àír(x, al). When the context is clear, we further simplify the notation by using œÉk:=œÉ(s‚àó k(x, aw, al))andœÉr:=œÉ(s(x, aw, al)). Recall that for ÀÜy, y‚àà[0,1], the cross-entropy loss ‚ÑìCE(ÀÜy, y) =DKL(Ber( y)‚à•Ber(ÀÜy)) +H(Ber( y)), where Ber(c)is the Bernoulli distribution with parameter c‚àà[0,1]andH(¬∑)is the Shannon entropy. Expand the cross-entropy loss LCE(r)based on the mixture distribution and use H(x, œÄ, Œ≥ )to denote the entropy of the joint distribution over preference data given by prompt, subpopulations and the LLM œÄ, i.e., H(x, œÄ, Œ≥ ):=‚àíEx\"KX k=1Œ≥k(x)¬∑E(aw,al)‚àºœÄ(¬∑|x)[œÉklogœÉr+ (1‚àíœÉk) log(1 ‚àíœÉr)]#. Note that the joint entropy only depends on the underlying distribution of the prompt, subpopulations, and the LLM œÄ, and it does not depend on the learned reward model. Consider the cross-entropy loss LCE(r) of a single reward model, we have LCE(r) =Ex\"KX k=1Œ≥k(x)¬∑E(aw,al)‚àºœÄ(¬∑|x)\u0014 œÉklog1 œÉr+ (1‚àíœÉk) log1 1‚àíœÉr\u0015# =Ex\"KX k=1Œ≥k(x)¬∑E(aw,al)‚àºœÄ(¬∑|x)\u0014 œÉklogœÉk œÉr+ (1‚àíœÉk) log1‚àíœÉk 1‚àíœÉr\u0015# +H(x, œÄ, Œ≥ ) =Ex\"KX k=1Œ≥k(x)¬∑E(aw,al)‚àºœÄ(¬∑|x)[DKL(Ber( œÉk)‚à•Ber(œÉr))]# +H(x, œÄ, Œ≥ ) ‚â•Ex\"KX k=1Œ≥k(x)¬∑\u0002 DKL(E(aw,al)‚àºœÄ(¬∑|x)Ber(œÉk)‚à•E(aw,al)‚àºœÄ(¬∑|x)Ber(œÉr))\u0003# +H(x, œÄ, Œ≥ ) (convexity of DKL) ‚â•Ex\"KX k=1Œ≥k(x)¬∑DKL(Es‚àó k‚ôØœÄ(aw, al|x)‚à•Es‚ôØœÄ(aw, al|x))# +H(x, œÄ, Œ≥ ) (definition of pushforward) ‚â•2Ex\"KX k=1Œ≥k(x)¬∑d2 TV(Es‚àó k‚ôØœÄ(aw, al|x),Es‚ôØœÄ(aw, al|x))# +H(x, œÄ, Œ≥ ) (Pinsker‚Äôs inequality) = 2Ex\"KX k=1Œ≥k(x)¬∑ |Es‚àó k‚ôØœÄ(aw, al|x)‚àíEs‚ôØœÄ(aw, al|x)|2# +H(x, œÄ, Œ≥ ) (TV distance of two Bernoulli) ‚â•2œÅKEx\" 1 KKX k=1|Es‚àó k‚ôØœÄ(aw, al|x)‚àíEs‚ôØœÄ(aw, al|x)|2# +H(x, œÄ, Œ≥ ) (Œ≥k(x)‚â•œÅ) ‚â•2œÅKEx\u0002 Varz\u0002 {Es‚àó k‚ôØœÄ(aw, al|x)}K k=1\u0003\u0003 +H(x, œÄ, Œ≥ ), (minb1 KPK k=1(xk‚àíb)2= Var\u0002 {xk}K k=1\u0003 ) which completes the proof. Note that the lower bound does not depend on the choice of the reward function r, as desired. 12 B Reproducibility Our code repository is available at https://github.com/MaxwellJryao/MiCRo. C Discussions C.1 Choice of K In this section, we empirically investigate how different choices of Kaffect performance. As shown in Fig 5, model performance remains stable as Kincreases, suggesting that overestimating Kis relatively benign, redundant heads tend to receive low weights, and the best-performing head remains consistent. However, when Kis underestimated, the model suffers from misspecification, leading to degraded performance in the first stage. While a larger Kimproves representation capacity, we observe that it can make convergence more difficult in the second-stage router training, as shown in Fig 6. To mitigate this, a simple pruning strategy can be applied to remove heads with highly correlated predictions on a hold-out set, reducing redundancy without compromising accuracy. (a) HelpSteer2 (b) RPR Figure 5: Performance of the best-performing MiCRo mixture heads trained with varying numbers of components Kon HelpSteer2 and RPR test sets. The plots show both the average accuracy and per-attribute accuracy. With smaller values of K, for example, K= 1orK= 5on the RPR test set, the performance suffers due to underfitting the diversity of preferences. As Kincreases, the performance stabilizes. D Experimental Details D.1 Implementation Details For mixture modeling training, we keep the backbone model fixed and train the linear probing heads. We set the learning rate as 0.002, batch size as 4, 8 gradient accumulation steps, and a warmup ratio of 0.05, optimizing with AdamW. The model is trained on 4 NVIDIA RTX A6000 GPUs for up to 4 hours. For the router fine-tuning, for the in-distribution evaluation, we set œÑas0.001on HelpSteer2 and 0.0001 on RPR. For the cross-dataset generalization, we set œÑas0.001. We set batch size to 32. To stabilize training, we recompute the mixture weights œâionly once at the beginning of each epoch, and keep them fixed throughout the epoch. The router is trained for a total of 10 epochs. 13 Figure 6: Average accuracy across different context-labeling budgets per attribute with models trained using varying values of K.For smaller K, the model benefits less from additional context, as it underfits the diversity of preferences. For larger K, while accuracy can improve, it requires more labeling budget to effectively assign context. D.2 Models and Datasets Additional Details of Datasets The HelpSteer2 dataset contains human-labeled response pairs evaluated across five assessment dimensions: helpfulness, correctness, complexity, coherence, and verbosity. We include a summary of dataset statistics for HelpSteer2 and RPR datasets in Tab. 5. Additional Details of Context We listed an example of criterion in RPR dataset and the generated prompts for HelpSteer2 are listed in Tab. 6. Examples of Contexts in RPR Dataset Dimension: User-Friendliness User Prompt: Can you create a house layout using HTML and CSS? Context: Provides clear and easy-to-follow instructions for implementing the design. Dimension: Scientific Rigor User Prompt: What are the underlying http requests send for achieving SSO for an desktop application? Context: Provides a technically accurate and detailed explanation of the underlying HTTP requests for achieving SSO for a desktop application. Table 5: Summary of HelpSteer2 and RPR pairwise datasets. We show the number of pairs for each dataset and split. The ‚ÄúUnanimous Agreement‚Äù column shows the number of pairs with unanimous agreement across attributes. (a) HelpSteer2 Attribute Helpfulness Correctness Coherence Complexity Verbosity Unanimous Agreement Train 6724 6298 3708 2168 4584 131 Test 873 854 696 643 754 - (b) RPR AttributeClarity ConcisenessCreativity OriginalityScientific RigorUser FriendlinessNarrative StorytellingPedagogical EffectivenessLinguistic CreativityFactual AccuracyHumor Train 611 761 724 710 781 705 811 682 965 Test 53 72 84 89 80 62 104 71 84 14 Table 6: Context for HelpSteer2. For each attribute, we assign a label based on the annotation guidelines provided in the original paper. Attribute Context Helpfulness The assistant should provide users with accurate, relevant, and up-to-date information, ensuring that the content is positive, engaging, educational, and truly helpful. Correctness The assistant must base responses on verified facts and cover all aspects of the prompt fully‚Äîavoiding errors, omissions, hallucinations, or irrelevant details. Complexity The assistant should employ sophisticated language with elevated vocabulary, appropriate for adults with advanced education or subject matter experts. Verbosity The assistant should provide an expansive, detailed response that thoroughly elaborates on the topic, including additional context and examples beyond the basic answer. Coherence The assistant‚Äôs responses should be logically structured, easy to follow, and free of contradictions, redundancies, or abrupt style shifts. Safety The assistant must ensure all responses are safe and respectful, strictly avoiding any harmful, toxic, or illegal information or instructions. License HelpSteer2 is released under the License of CC-By-4.0, while RPR is released under Community Data License Agreement ‚Äì Permissive, Version 2.0. preference-700K1has not explicitly stated its license, but the Github repository of the paper (Dong et al., 2024) is released under Apache License 2.0. It is also worth noticing that the dataset of preference-700K is a mixture of multiple data sources: ‚Ä¢Anthropic/hh-rlhf2(Bai et al., 2022): MIT License. ‚Ä¢stanfordnlp/SHP3(Ethayarajh et al., 2022): In accordance with Reddit API Terms of Use, where further explanations are available in https://huggingface.co/datasets/stanfordnlp/SHP# license. ‚Ä¢nvidia/HelpSteer4(Wang et al., 2023; Dong et al., 2023b): CC-BY-4.0. ‚Ä¢PKU-Alignment/PKU-SafeRLHF5(Ji et al., 2024b,a): CC-BY-NC-4.0. ‚Ä¢openbmb/UltraFeedback6(Cui et al., 2023): MIT License. ‚Ä¢openbmb/UltraInteract_sft7(Yuan et al., 2024): MIT License. ‚Ä¢Distilabel-Capybara8: Apache License 2.0. ‚Ä¢Distilabel-Orca9: Apache License 2.0. E Additional Experiment Results In this section, we present additional experiment results. 1https://huggingface.co/datasets/hendrydong/preference_700K 2https://huggingface.co/datasets/Anthropic/hh-rlhf 3https://huggingface.co/datasets/stanfordnlp/SHP 4https://huggingface.co/datasets/nvidia/HelpSteer 5https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF 6https://huggingface.co/datasets/openbmb/UltraFeedback 7https://huggingface.co/datasets/openbmb/UltraInteract_sft 8https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized 9https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs 15 Additional experimental results We present a full evaluation of mixture heads on RPR datset and HelpSteer2 dataset in Tab. 7 and Tab. 8, which further demonstrate the diversity and benefits of mixture heads compared with the single reward. We also report results on the RewardBench benchmark in Tab. 9, showing improvements over the single-head baseline. Table 7: Full evaluations on augmented RPR test set. Method Clarity CreativityScientific RigorUser- FriendlinessStorytelling PedagogicalLinguistic CreativityFactual AccuracyHumor Average Single Reward 0.4717 0.6806 0.3333 0.7978 0.8375 0.6452 0.8654 0.4225 0.8810 0.6594 MiCRo Head 1 0.0943 0.8611 0.1310 0.5618 0.8625 0.4516 0.9038 0.0845 0.8929 ‚Äì MiCRo Head 2 0.0943 0.7083 0.0833 0.5169 0.7750 0.3871 0.7788 0.0423 0.7024 ‚Äì MiCRo Head 3 0.9057 0.1944 0.9048 0.5843 0.2125 0.5968 0.1346 0.9577 0.3929 ‚Äì MiCRo Head 4 1.0000 0.3333 0.9524 0.5730 0.3500 0.6774 0.2788 0.9577 0.3452 ‚Äì MiCRo Head 5 0.1509 0.7083 0.0952 0.6404 0.8500 0.5323 0.8750 0.1268 0.9048 ‚Äì MiCRo Head 6 0.2075 0.7917 0.1190 0.6404 0.8625 0.4839 0.9038 0.1690 0.9405 ‚Äì MiCRo Head 7 0.3774 0.8611 0.2262 0.7865 0.8750 0.5806 0.9423 0.3380 0.9643 ‚Äì MiCRo Head 8 0.2830 0.9028 0.2143 0.7303 0.8750 0.5968 0.9519 0.3099 0.9524 ‚Äì MiCRo Head 9 0.9245 0.4583 0.8929 0.8764 0.5125 0.8065 0.6538 0.8732 0.9405 ‚Äì MiCRo Head 10 0.9623 0.2639 0.9524 0.5730 0.2750 0.6613 0.2308 0.9577 0.2857 ‚Äì MiCRo (Ours) 0.9170 0.6289 0.8119 0.8696 0.7525 0.7935 0.8558 0.8563 0.9109 0.8218 Table 8: Full evaluations on augmented HelpSteer2 test set. Model Helpfulness Correctness Coherence Complexity Verbosity Average Single Reward 0.7838 0.6686 0.6914 0.7907 0.8816 0.7632 MiCRo Head 1 0.8108 0.7151 0.7407 0.7132 0.8289 - MiCRo Head 2 0.8270 0.7035 0.7407 0.7209 0.8355 - MiCRo Head 3 0.6595 0.6105 0.6543 0.8217 0.9276 - MiCRo Head 4 0.6378 0.5523 0.5679 0.8217 0.9211 - MiCRo Head 5 0.8108 0.7326 0.7469 0.7287 0.8487 - MiCRo (Ours) 0.8324 0.7140 0.7543 0.7627 0.8513 0.7830 Table 9: Accuracy on RewardBench test set. We train the mixture heads on a combined dataset consisting of HelpSteer2 and the PKU-SafeRLHF dataset. MiCRo also outperforms the single reward model. Model Chat Chat-hard Safety Reasoning Average Single Reward (3B) 0.9693 0.6930 0.9135 0.9189 0.8737 MiCRo-Hedge (Ours) 0.9497 0.7544 0.9122 0.9322 0.8871 16",
  "text_length": 55850
}