{
  "id": "http://arxiv.org/abs/2506.05310v1",
  "title": "Learning normalized image densities via dual score matching",
  "summary": "Learning probability models from data is at the heart of many machine\nlearning endeavors, but is notoriously difficult due to the curse of\ndimensionality. We introduce a new framework for learning \\emph{normalized}\nenergy (log probability) models that is inspired from diffusion generative\nmodels, which rely on networks optimized to estimate the score. We modify a\nscore network architecture to compute an energy while preserving its inductive\nbiases. The gradient of this energy network with respect to its input image is\nthe score of the learned density, which can be optimized using a denoising\nobjective. Importantly, the gradient with respect to the noise level provides\nan additional score that can be optimized with a novel secondary objective,\nensuring consistent and normalized energies across noise levels. We train an\nenergy network with this \\emph{dual} score matching objective on the ImageNet64\ndataset, and obtain a cross-entropy (negative log likelihood) value comparable\nto the state of the art. We further validate our approach by showing that our\nenergy model \\emph{strongly generalizes}: estimated log probabilities are\nnearly independent of the specific images in the training set. Finally, we\ndemonstrate that both image probability and dimensionality of local\nneighborhoods vary significantly with image content, in contrast with\ntraditional assumptions such as concentration of measure or support on a\nlow-dimensional manifold.",
  "authors": [
    "Florentin Guth",
    "Zahra Kadkhodaie",
    "Eero P Simoncelli"
  ],
  "published": "2025-06-05T17:53:57Z",
  "updated": "2025-06-05T17:53:57Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05310v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05310v1  [cs.LG]  5 Jun 2025Learning normalized image densities\nvia dual score matching\nFlorentin Guth\nCenter for Data Science, New York University\nFlatiron Institute, Simons Foundation\nflorentin.guth@nyu.eduZahra Kadkhodaie\nFlatiron Institute, Simons Foundation\nzk388@nyu.edu\nEero P. Simoncelli\nNew York University\nFlatiron Institute, Simons Foundation\neero.simoncelli@nyu.edu\nAbstract\nLearning probability models from data is at the heart of many machine learning\nendeavors, but is notoriously difficult due to the curse of dimensionality. We\nintroduce a new framework for learning normalized energy (log probability) models\nthat is inspired from diffusion generative models, which rely on networks optimized\nto estimate the score. We modify a score network architecture to compute an energy\nwhile preserving its inductive biases. The gradient of this energy network with\nrespect to its input image is the score of the learned density, which can be optimized\nusing a denoising objective. Importantly, the gradient with respect to the noise level\nprovides an additional score that can be optimized with a novel secondary objective,\nensuring consistent and normalized energies across noise levels. We train an energy\nnetwork with this dual score matching objective on the ImageNet64 dataset, and\nobtain a cross-entropy (negative log likelihood) value comparable to the state of the\nart. We further validate our approach by showing that our energy model strongly\ngeneralizes : estimated log probabilities are nearly independent of the specific\nimages in the training set. Finally, we demonstrate that both image probability and\ndimensionality of local neighborhoods vary significantly with image content, in\ncontrast with traditional assumptions such as concentration of measure or support\non a low-dimensional manifold.\n1 Introduction\nMany problems in image processing and computer vision rely, explicitly or implicitly, on prior\nprobability models. However, learning such models by maximizing the likelihood of a set of training\nimages is difficult. The dimensionality of the space (i.e., number of image pixels) is large, and\nworst-case data requirements for estimation grow exponentially (the “curse of dimensionality”). The\nmachine learning community has developed a variety of methods to train a parametric network to\nestimate logp(x), known as an “energy model” (Hinton et al., 1986; LeCun et al., 2006), relying\non the inductive biases of the network to alleviate the data requirements. For all but the simplest\nof models, this approach is frustrated by the intractability of estimating the normalization constant\n(Hinton, 2002; LeCun and Huang, 2005; Yedidia et al., 2005; Gutmann and Hyvärinen, 2010; Dinh\net al., 2014; Rezende and Mohamed, 2015; Dinh et al., 2017; Song and Kingma, 2021).\nA clever means of escaping this conundrum is to estimate the gradient of the energy with respect to\nthe image (known as the “score”), which eliminates the normalization constant, and can be learned\nPreprint. Under review.\n--- Page 2 ---\nfrom data with a “score-matching” objective (Hyvärinen and Dayan, 2005). The recent development\nof “diffusion” generative models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al.,\n2020; Kadkhodaie and Simoncelli, 2020) builds on this concept, by estimating a family of score\nfunctions for images corrupted by Gaussian white noise at different amplitudes. The scores may\nthen be used to sample from the corresponding estimated density, using an iterative reverse diffusion\nprocedure. These methods have enabled dramatic improvements in both the quality and diversity\nof generated image samples, but the learned density is implicit. An explicit and normalized energy\nmodel (and density) can be obtained through integration of the divergence of the score vector field\nalong a trajectory, at tractable but large computational cost.\nHere, we leverage the power of diffusion models to develop a robust and efficient framework for\ndirectly learning a normalized energy model from image data. We approximate the energy with a\ndeep neural network that takes as input both a noisy image and the corresponding noise variance,\nand derive two separate objectives by differentiating with respect to these two inputs. The first is\na denoising objective, as used in diffusion models. The second is novel and ensures consistency\nof the energy estimates across noise levels, which we show to be critical for obtaining accurate\nand normalized energies. We optimize the sum of the two, in a procedure that we refer to as “ dual\nscore matching”. We also propose a novel architecture for energy estimation by computing the inner\nproduct of the output of a score network with its input image. This preserves the inductive biases of\nthe base score network, leading to equal or superior denoising performance (and thus sample quality).\nWe train our energy model on ImageNet64 (Russakovsky et al., 2015; Chrabaszcz et al., 2017), and\nshow that the estimated energies lead to negative log likelihood (or cross-entropy) values comparable\nto the state-of-the-art. We further demonstrate that the energy model strongly generalizes in the sense\nof Kadkhodaie et al. (2024): two separate models trained on non-overlapping subsets of the training\ndata assign the same probabilities to a given image. This convergence is observed at moderate\ntraining set sizes, and is far faster than the worst case prediction of the curse of dimensionality.\nWe find that the distribution of log probabilities over ImageNet images covers a broad range, with\ndensely textured images at the lower end, and sparse images at the upper end. The probability is\nstable with respect to image luminance, but decreases with dynamic range. Finally, we highlight\ntwo geometrical properties of the learned image distribution. The first is an extremely tight inverse\nrelationship between volume and density that leads to an absence of concentration of energy values.\nThey furthermore follow a Gumbel distribution, indicating a surprising statistical regularity. The\nsecond is that the local dimensionality of the energy landscape in the neighborhood of an image\nvaries greatly depending on image content and neighborhood size. We find both images with full-\ndimensional neighborhoods of non-negligible size and images with lower-dimensional neighborhoods\neven at sub-quantization scales. These results challenge several traditional presuppositions regarding\nhigh-dimensional distributions, such as the concentration of measure phenomenon (Vershynin,\n2018; Wainwright, 2019) or the manifold hypothesis (Tenenbaum et al., 2000; Bengio et al., 2013).\nWe release code to reproduce all experiments and pre-trained models at https://github.com/\nFlorentinGuth/DualScoreMatching .\n2 Learning normalized energy models with dual score matching\nEstimating a high-dimensional probability density from samples faces two significant challenges\nas a result of the curse of dimensionality. The first is statistical : reasonably-sized datasets do not\ncontain enough information about the unknown distribution. One then needs powerful inductive\nbiases, typically in the form of a network architecture, to hope to recover the data distribution. The\nsecond challenge is computational : the traditional objective is to maximize the likelihood of the\nmodel over the data, which is intractable due to the need to compute the normalization constant.\nNevertheless, recently developed generative models implicitly solve this problem. Our approach\ndraws inspiration from diffusion models (Section 2.1), and derives a novel objective (Section 2.2)\nand architecture (Section 2.3) to learn normalized log probabilities (energies) from data, addressing\nboth challenges. It is validated numerically in Section 2.4.\n2.1 Motivation\nTraditionally, energy models are defined in terms of a parametric function Uθ(x)that approximates\nthe unnormalized log density over x∈Rd:pθ(x) =1\nZθe−Uθ(x), with a normalizing constant\n2\n--- Page 3 ---\n0 1 2 3 4 5\n/bardblx/bardbl/√\nd0\n−5\n−10\n−15Log probability (dB/dim)\n0 1 2 3 4 5\n/bardblx/bardbl/√\nd−8−6−4−20Score (dB/dim)Data\nGround truth\nSingle SM\nSingle SM (adjusted)\nDual SM\n<latexit sha1_base64=\"jT+DlSuGF7+GgK9daqcBGf5ULVI=\">AAAB73icbVA9TwJBEN3zE/ELtbTZCCZW5I4CLYk2lpjwlcCF7C0DbNi9O3fnTMiFP2FjoTG2/h07/40LXKHgSyZ5eW8mM/OCWAqDrvvtbGxube/s5vby+weHR8eFk9OWiRLNockjGelOwAxIEUITBUroxBqYCiS0g8nd3G8/gTYiChs4jcFXbBSKoeAMrdRpCAW0hKV+oeiW3QXoOvEyUiQZ6v3CV28Q8URBiFwyY7qeG6OfMo2CS5jle4mBmPEJG0HX0pApMH66uHdGL60yoMNI2wqRLtTfEylTxkxVYDsVw7FZ9ebif143weGNn4owThBCvlw0TCTFiM6fpwOhgaOcWsK4FvZWysdMM442orwNwVt9eZ20KmWvWq4+VIq12yyOHDknF+SKeOSa1Mg9qZMm4USSZ/JK3pxH58V5dz6WrRtONnNG/sD5/AHprI8+</latexit>Timet\n<latexit sha1_base64=\"UhnwNePmJdouQkF06vLB+l03QfQ=\">AAAB8HicbVBNTwIxEO3iF+IX6tFLI5h4Irsc0CPRi0eMghjYkG6ZhYa2u2m7JpsNv8KLB43x6s/x5r+xwB4UfEmTl/dmpjMviDnTxnW/ncLa+sbmVnG7tLO7t39QPjzq6ChRFNo04pHqBkQDZxLahhkO3VgBEQGHh2ByPfMfnkBpFsl7k8bgCzKSLGSUGCs93sWEAq6m1UG54tbcOfAq8XJSQTlag/JXfxjRRIA0lBOte54bGz8jyjDKYVrqJxrs8AkZQc9SSQRoP5svPMVnVhniMFL2SYPn6u+OjAitUxHYSkHMWC97M/E/r5eY8NLPmIwTA5IuPgoTjk2EZ9fjIVNADU8tIVQxuyumY6IINTajkg3BWz55lXTqNa9Ra9zWK82rPI4iOkGn6Bx56AI10Q1qoTaiSKBn9IreHOW8OO/Ox6K04OQ9x+gPnM8fpjSPqg==</latexit>Spacey<latexit sha1_base64=\"F9bPgG897UXRzP6H0vZjOAbhljw=\">AAAB8XicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsKtCQaE0tMPCTChewtA2zY27vs7pkQwr+wsdAYW/+Nnf/GBa5Q8CWTvLw3k5l5YSK4Nq777eTW1jc2t/LbhZ3dvf2D4uFRU8epYuizWMSqFVKNgkv0DTcCW4lCGoUCH8LR9cx/eEKleSzvzTjBIKIDyfucUWOlxxuJajAmZb/cLZbcijsHWSVeRkqQodEtfnV6MUsjlIYJqnXbcxMTTKgynAmcFjqpxoSyER1g21JJI9TBZH7xlJxZpUf6sbIlDZmrvycmNNJ6HIW2M6JmqJe9mfif105N/zKYcJmkBiVbLOqngpiYzN4nPa6QGTG2hDLF7a2EDamizNiQCjYEb/nlVdKsVrxapXZXLdWvsjjycAKncA4eXEAdbqEBPjCQ8Ayv8OZo58V5dz4WrTknmzmGP3A+fwBXPZAO</latexit>EnergyUFigure 1: Comparison of single and dual score matching on recovering the energy of a scale mixture\nof two Gaussians in d= 1000 dimensions. Left: Radial slices of the log probability. The single score\nmatching estimate (green dashed curve) fails to recover the true energy (blue solid curve), even after\nglobal normalization (green dotted curve), while dual score matching (red dashed curve) succeeds.\nMiddle: Radial components of the scores. Single score matching learns an accurate score over the\nsupport of the data (blue bar plot) but not outside of it. Right: Energy landscape across space and\ntime (noise level) for a mixture of two Gaussians in one dimension. The direct path between the\nmodes at t= 0crosses a large energy barrier (green curve), which is alleviated on a path that is not\ntime-restricted (red curve).\nZθ=R\ne−Uθ(x)dx. The parameters θare estimated by minimizing the expected negative log\nlikelihood (NLL), Ex[−logpθ(x)] =Ex[Uθ(x)] + log Zθ, which is equivalent to minimizing the KL\ndivergence between pθ(x)and the data distribution. The normalizing constant Zθplays a critical role\nin learning, representing the total energy over Rdwhich trades off against the energy of the data. But\ndirect estimation (i.e., computing the integral) is typically intractable.\nThe normalization constant can be eliminated from the NLL by differentiating w.r.t. x, yielding\na quantity known as the score: −∇xlogpθ(x) =∇xUθ(x). As a result, a score model can be\nefficiently fitted to data via “score matching” (Hyvärinen and Dayan, 2005), which minimizes the\nFisher (rather than KL) divergence between the model and the data. In the case of data corrupted\nby Gaussian white noise, this amounts to solving a denoising problem (Vincent, 2011; Raphan and\nSimoncelli, 2011). But this computational advantage comes at a statistical cost: a good approximation\nof the score does not always lead to a good model of the energy (Koehler et al., 2023), as we now\nillustrate. Consider the case of an equal mixture of two multivariate Gaussian distributions with zero\nmean and different variances σ2\n1andσ2\n2. In high dimensions, this distribution concentrates near the\ntwo spheres of radius σ1√\ndandσ2√\nd, leading to data scarcity in the rest of the space. We show in\nFigure 1 energies estimated from samples by optimizing their gradient via (single) score matching.\nThis fails to recover the true energy, even after adjusting a normalization constant. Indeed, estimating\nthe energy difference between the two Gaussians requires a good estimation of the score along an\nintegration path between them. If there is no data in-between modes to constrain the score, due to an\nenergy barrier or concentration phenomena, this leads to inconsistent energy values.\nDiffusion models expand on score matching by learning scores of data corrupted by white Gaussian\nnoise for a range of different noise amplitudes, ∇yU(y, t), with y=x+N(0, tId). The evolution of\nthe density p(y|t)astincreases is a diffusion process, and thus the corresponding energies increase\nin smoothness with t. This multiscale family of scores can be used to draw high-quality samples\nfrom p(x)using a reverse diffusion algorithm, which follows a trajectory of partial denoising steps\n(Song and Ermon, 2019; Song et al., 2021a; Ho et al., 2020; Kadkhodaie and Simoncelli, 2020). In\nfact, the diffusion scores implicitly capture a density model of the data (Song et al., 2021b; Kingma\nand Welling, 2013): the relative energy levels between modes are encoded in the score at the time\ntwhen they merge (Raya and Ambrogioni, 2023; Biroli et al., 2024). This density model can be\nevaluated in various ways, which all involve a tractable but costly integration of score divergences (or\ndenoising errors) along time (Song et al., 2021a; Kong et al., 2023; Skreta et al., 2024; Karczewski\net al., 2025). Intuitively, diffusion provides a high-probability path between modes through space and\ntime, as visualized in Figure 1. Energy differences along time can then be retrieved by integrating the\n“time score” ∂tUθ(y, t)(Choi et al., 2022), which is implicitly determined from the diffusion equation\n(Song et al., 2021a; Lai et al., 2023). See Appendix A.2 for a more in-depth exposition. Here, we\npropose to learn the “space-time” energy landscape directly by matching both space and time scores.\n3\n--- Page 4 ---\n2.2 Dual score matching: Objective function\nSpace and time score matching. We want to learn a time-dependent energy function, Uθ(y, t), to\napproximate the NLL of the noisy image distribution at all noise levels t:\nU(y, t) =−log\u0012Z\np(x)e−1\n2t∥x−y∥2−d\n2log(2πt)dx\u0013\n. (1)\nDifferentiating this NLL with respect to ygives the “space score”, which can be expressed using the\nMiyasawa-Tweedie identity (Robbins, 1956; Miyasawa, 1961) as\n∇yU(y, t) =Ex\u0014y−x\nt\f\f\f\fy\u0015\n. (2)\nIt leads to the denoising score matching objective (Vincent, 2011; Raphan and Simoncelli, 2011)\nused to train diffusion models:\nℓDSM(θ, t) =Ex,y\"\r\r\r\r∇yUθ(y, t)−y−x\nt\r\r\r\r2#\n. (3)\nWe also can differentiate the energy with respect to t, producing a “time score”, for which we can\nderive a similar score-matching identity (see Appendix B.1):\n∂tU(y, t) =Ex\"\nd\n2t−∥y−x∥2\n2t2\f\f\f\f\fy#\n. (4)\nFrom this identity, we define an analogous “time score matching” objective:\nℓTSM(θ, t) =Ex,y\n \n∂tUθ(y, t)−d\n2t+∥y−x∥2\n2t2!2\n. (5)\nMatching both scores enforces that the learned energy Uθ(y, t)is a solution of the diffusion equation,\nwhich is critical for combining information across noise levels into accurate energy values at t= 0.\nCombining objectives across noise levels. The two objectives in eqs. (3) and (5) are defined for\na fixed noise level t. We can form an overall objective by integrating over t, with an appropriate\nweighting. For the denoising score matching objective, a natural choice is the so-called maximum-\nlikelihood weighting (Song et al., 2021b; Kingma et al., 2021), which provides a bound on the KL\ndivergence with the data distribution:\nKL(p∥˜pθ)≤1\n2Z∞\n0ℓDSM(θ, t) dt, (6)\nwhere ˜pθis the implicit density of the generative diffusion model. This integral can be approximated\nby Monte-Carlo sampling from a distribution of noise levels. In practice, we have found it best\nto sample logtuniformly over a finite interval (specifically, p(t)∝1/t, t∈[tmin, tmax]), which\ncorresponds to the following implementation of the integral:\nZtmax\ntminℓDSM(θ, t) dt=Et[t ℓDSM(θ, t)]. (7)\nAppreciably, the resulting term in the expected value is unitless: it is invariant to a simultaneous\nrescaling of the data and noise level. We thus choose to weight the time score matching objective\nin eq. (5) by t2, so that it is also unitless, and to evaluate over the same distribution p(t). Finally,\nafter normalization by the dimensionality d, we simply add both objectives (we found no significant\nimprovement from tuning a tradeoff hyperparameter). Our dual score matching objective is then\nℓ(θ) =Et\"\nt\ndℓDSM(θ, t) +\u0012t\nd\u00132\nℓTSM(θ, t)#\n. (8)\n4\n--- Page 5 ---\nTable 1: Denoising MSE at several noise levels for corresponding score and energy networks,\naveraged over images in ImageNet64. All quantities are expressed as peak signal-to-noise ratio\n(PSNR) in dB: PSNR = −10 log10(MSE) .\nNoise variance 90 75 60 45 30 15 0 −15 −30\nScore network 90 .20 75 .47 60.43 47 .19 35 .58 25 .92 18 .84 13 .31−0.11\nEnergy network 90.09 75 .17 60 .45 47 .25 35 .67 26 .01 18 .88 13 .48 2 .53\nNormalization. The learned energy Uθ(y, t)is only estimated up to an additive constant, since the\ntwo objective functions only constrain its partial derivatives. An important aspect of our framework\nis that it enables estimation of this normalizing constant. Indeed, the time score objective ensures that\nthis normalizing constant does not depend on time, as mass is conserved through the diffusion . This\nallows normalizing the model at the largest noise level tmax, where the true distribution is almost\nGaussian: p(y|tmax)≈ N (0, tmaxId). We thus set the average energy value at t=tmaxto the\nentropy of this Gaussian distribution:\nUθ(y, t)−→Uθ(y, t)−Ey[Uθ(y, t)|t=tmax] +d\n2log(2 πetmax). (9)\nFigure 1 verifies that dual score matching indeed provides both consistent and normalized energy\nvalues (in particular, for t= 0), in contrast to single score matching. We note however that eq. (9) is\nanapproximate normalization procedure, which we discuss at more length in Appendix A.3.\n2.3 Dual score matching: Architecture\nHow should one choose an architecture to compute the energy Uθ(y, t)? Rather than designing\none from scratch (Salimans and Ho, 2021; Cohen et al., 2021; Thiry and Guth, 2024), we instead\nconstruct one by modifying a score-based denoising architecture that is known to have appropriate\ninductive biases. Let sθ:Rd×R→Rdbe such an architecture (e.g., a UNet). We wish to define a\nnew architecture Uθ:Rd×R→Rsuch that ∇yUθ≈sθ, preserving the inductive biases of sθ. We\npropose to set\nUθ(y, t) =1\n2⟨y, sθ(y, t)⟩. (10)\nWe show in Appendix B.2 that ∇yUθ(y, t) =sθ(y, t)if the score network sθis conservative and\nhomogeneous (Romano et al., 2017; Reehorst and Schniter, 2018). Homogeneity has been shown to\nhold approximately in the related setting of blind denoisers (Mohan* et al., 2020; Herbreteau et al.,\n2023). We note that the seemingly similar choice of a squared norm such as Uθ(y, t) =1\n2∥sθ(y, t)∥2\nused in some previous energy models (Salimans and Ho, 2021; Hurault et al., 2021; Du et al., 2023;\nYadin et al., 2024; Thiry and Guth, 2024) leads to the same desirable homogeneity properties in y\nbut not in θ, and thus fails to preserve the optimization properties of the original score network. We\ndescribe further architectural details in Appendix C.1.\n2.4 Performance evaluation\nDenoising performance. We first verify that the gradient of the energy network Uθprovides as\ngood a denoiser as the score network on which it is based, sθ. Both networks are separately trained;\nUθwith the dual score matching objective (eq. (8), using double back-propagation) and sθwith\nonly the standard (denoising) score matching objective (eq. (3)). Further details are provided in\nAppendix C.2. We compare their denoising performance across noise levels in Table 1. For all but\nthe smallest noise levels, the energy-based model achieves (slightly) better denoising performance\nthan the score model. These results show that there is no penalty in modeling the energy rather than\nthe score. This is contrary to the results of Salimans and Ho (2021), and is likely due to our use\nof an architecture that is homogeneous, and for which non-conservativeness of the score is not a\nlarge source of denoising error (Mohan* et al., 2020; Chao et al., 2023). Finally, these results also\ndemonstrate that the two components of our dual score matching objective are not trading off against\neach other; rather, they complement and even reinforce each other.\n5\n--- Page 6 ---\nTable 2: Negative log likelihood (in bits/dimension) on ImageNet64 test set. See Appendix A.1 for\nmore details.\nMethod Anti-aliasing Augmentation Discreteness Type Single NFE NLL\nGlow (Kingma and Dhariwal, 2018) ✗ None Continuous Normalized ✓ 3.81\nPixelCNN (Van den Oord et al., 2016) ✗ None Discrete Normalized ✗ 3.57\nI-DDPM (Nichol and Dhariwal, 2021) ✗ None Continuous Upper bound ✗ 3.54\nVDM (Kingma et al., 2021) ✗ None Discrete Upper bound ✗ 3.40\nFM (Lipman et al., 2023) ✓ None Uniform* Normalized ✗ 3.31\nNFDM (Bartosh et al., 2024) ✗ Horizontal flips Uniform* Normalized ✗ 3.20\nTarFlow (Zhai et al., 2024) ✗ Horizontal flips Uniform Normalized ✓ 2.99\nOurs ✓ Horizontal flips Continuous Estimate ✓ 3.36\n0 10 20 30 40\nlogpA(x)010203040logpB(x)N= 10\nDataset A\nDataset B\n0 10 20 30 40\nlogpA(x)010203040N= 100\n0 10 20 30 40\nlogpA(x)010203040N= 1,000\n0 10 20 30 40\nlogpA(x)010203040N= 10,000\n0 10 20 30 40\nlogpA(x)010203040N= 100,000\nFigure 2: Convergence of energy estimates. The data set is split into two halves (denoted A and\nB), and separate energy models are trained on Nsamples drawn from each half. Each scatterplot\ncompares the logp(x)estimated by the two models at t= 0 over all 2Ntraining images. As N\nincreases, the energy estimates of the two models converges for all images.\nNegative log likelihood. How accurate are our energy estimates? A standard evaluation of proba-\nbilistic models consists of calculating the cross-entropy Ex[−logpθ(x)]between the data distribution\nand the model, also known as negative log likelihood (NLL). In practice, NLL computes how much\nprobability the model assigns to a held-out test set. There are however several subtleties that make\ndirect comparisons challenging. Specifically, there are 3 factors that can cause variations in NLL\nup to±0.5bits/dimension or more: details of data pre-processing (e.g., downsampling or data\naugmentation), conversion method from continuous to discrete probabilities, and the estimator type\n(exactly normalized, variational bound, or approximately normalized). We expand on these issues in\nAppendix A.1.\nWe compare NLLs of our method and a variety of recent energy models in Table 2. This evaluation\ndemonstrates that our model is comparable to the best-performing models in the literature, within the\nvariability arising from the three factors of variation mentioned in the previous paragraph. Two unique\nadvantages of our method are that it provides (1) direct (one-shot) estimates of energy (as opposed\nto other density estimation approaches in diffusion models, which we review in Appendix A.2),\ngreatly facilitating analysis, and (2) access to energy across all noise levels, providing a window into\nlarger-scale features of the energy landscape.\n3 Analysis of the learned energy-based model\n3.1 Generalization\nThe previous section demonstrated that our energy-based model achieves near state-of-the-art NLL\non ImageNet64. That is, the model on average assigns high probability to a set of held-out images.\nNext, we establish that the energies of the individual images are reliable. In particular, we require\nthat the model’s energy assignment is stable under change of the training data.\nTo this end, we borrow the strong generalization test developed in Kadkhodaie et al. (2024). We\npartition the training data into two non-overlapping sets, train a separate energy-based model on each\nset, and then compare the energies computed by these two models on their own and each other’s\ntraining examples. We gradually increase the size of each training set until the two models assign\napproximately equal log probability across all images. Figure 2 shows the results of this experiment.\nThe two models assign very different probabilities to the same image when the training set size,\n6\n--- Page 7 ---\n0 10 20 30 40 50\nlogpθ(x) (dB/dim)10−410−310−210−1FrequencyData\nGumbel ﬁt\nFigure 3: Histogram of log probabilities of images in the ImageNet dataset. Color-coded arrows\nindicate values for the example images on the right, and the leftmost (brown) and rightmost (green)\narrows indicate values for a uniform noise image in [0,1]and a constant image of intensity 0.5,\nrespectively. The distribution is well-fit by a Gumbel distribution (red line). Additional examples of\nimages organized by probability are shown in Figures 6 to 8 (Appendix D).\nN, is small. But they converge gradually and compute almost the same values at N= 105. Note\nthat the rate of convergence depends on the probability: more data is needed before the two models\nagree on the high-probability images. It is also worth noting that the transition from memorization to\ngeneralization of energy models is marked by a large increase of variance in energy over the training\nset (starting at N= 100 ).\nThis result establishes that the model variance vanishes with a feasible training set size, demonstrating\nlack of memorization. However, it does not guarantee that the values are accurate, as the models\ncould be biased. Calculating model bias is not possible without access to the true density, hence it\ncan only be obtained for simple analytical models (as in Figure 1). However, smaller NLL values\nover test data (Table 2) are indicative of a smaller model bias.\n3.2 Distribution of energies and relationship to image content\nWe now study properties of the learned energy model. What is the entropy (average energy) of the\nimage dataset? Do all images have the same probability? If not, what determines the probability\nof an image? To investigate these questions, we compute the log probability of all 50,000images\nin the ImageNet test set in Figure 3. We express log probability in units of decibels per dimension\n(dB/dim), computed as1\nd10 log10pθ(x). In this scale, an additive change of 10dB/dim corresponds\nto a multiplicative change in probability density of 10d.\nEntropy. The average of −logpθ(x)provides an estimate of the differential entropy of ImageNet,\nwhich is here equal to −11.4dB/dim. Note that the uniform distribution over the hypercube [0,1]d\nhas an entropy of 0. This indicates that natural images occupy a fractional volume of about 10−1.14d.\nThis can be converted to an estimate of discrete entropy by assuming that log probability is constant\nwithin quantization bins. For 256possible intensity values ( 8bits), this gives an entropy of 4.20\nbits/dimension (the difference with Table 2 comes from the use of grayscale images here). In\nother words, there are ∼105,180quantized ImageNet images out of 109,860possible images at this\nresolution.\nLack of concentration. Many high-dimensional probability distributions exhibit a concentration\nphenomenon, where typical realizations have nearly equal energy. For instance, in simple image\nprobability models such as a Gaussian model or a sparse wavelet model (where wavelet coefficients\nare independent), the energy of independent components add, and the law of large numbers imply\na concentration of the energy around its mean. In contrast, our energy network reveals enormous\ndiversity in the log probabilities of individual ImageNet images (Figure 3). Over the entire test\nset, they span a range of 34.4dB/dim, corresponding to a probability ratio of ≈1014,000. To make\nsense of this value, it is important to realize that the frequency of images with a given value of\nlogpin a dataset is equal to this probability value multiplied by the volume of the corresponding\nprobability level set . Observing a range of probability values of 1014,000thus reveals that these\nenormous variations in probability density are nearly compensated by inverse variations in the volume\nof their corresponding level sets.\n7\n--- Page 8 ---\n0.00.20.40.60.81.0\na1.0\n0.8\n0.6\n0.4\n0.2\n0.0b\n010203040\nlogpθ(ax+b)\n0.0 0.1 0.2 0.3 0.4 0.5\na01020304050logpθ(ax+ 1/2)\n0.0 0.2 0.4 0.6\nParticipation ratio051015202530Log probability (dB/dim)Figure 4: Influence of image statistics on probability. Left. logpθ(ax+b)as a function of aandb.\nMiddle. Horizontal slice ( b=1\n2) of the left plot for the example images of Figure 3. Right. Log\nprobability as a function of sparsity, measured as the participation ratio of wavelet coefficients.\nShape of the distribution of log probabilities. The distribution of log probabilities values is\nhighly skewed: there is a heavy tail of high-probability images and a much lighter tail of low-\nprobability images. Surprisingly, this distribution is well approximated by a Gumbel distribution\n(we report parameter values in Appendix C.3). It arises as a limit distribution for the maximum\nof many i.i.d. random variables. This surprising observation is, to the best of our knowledge, new,\nand calls for an explanation. In independent component models, the log probabilities are sums of\ni.i.d. random variables, and are therefore Gaussian distributed (with a vanishing variance compared\nto their mean). A simple model which reproduces this Gumbel distribution is a high-dimensional\nspherically-symmetric distribution with an exponentially-distributed radial marginal.\nImage content. We also show in Figure 3 examples of images at various log probabilities. High-\nprobability images invariably contain small (often high-contrast) objects, on a blank (often white)\nbackground. Conversely, low-probability images are generally filled with dense detailed texture.\nThis agrees with the behavior of compression engines. Indeed, the energy of an image, when\nexpressed in bits, corresponds to the size of its optimally compressed representation. This implies\nthat low-probability images should intuitively have more “content” than high-probability images.\nIntensity range and sparsity. Following these visual observations, we further examine the influence\nof intensity range and sparsity on image probability (Figure 4). First, we evaluate the log probability\nof reference images as we manipulate their brightness or contrast through an affine operation. We\nfind that the brightness has minimal effect on the probability, while higher-contrast images have\nlower probability. This reveals that the distribution is star-shaped : any pair of images are connected\nby a high-probability path passing through a constant image. Next, we verify that log probability\nis correlated with a simple measure of sparsity of images. We compute a multi-scale wavelet\ndecomposition of the images, and measure the ℓ1-norm divided by the ℓ2-norm of the coefficients.\nThe square of this quantity, ∥x∥2\n1/d∥x∥2\n2∈(0,1]is known as participation ratio, with smaller values\nindicating higher sparsity. The right panel of Figure 4 shows that this simple measure is indeed\npredictive of logpθ(x)(albeit not very precisely).\n3.3 Effective dimensionality of the energy landscape\nBeyond estimating log probability for a given image, we aim to characterize the local behavior of the\ndensity in the vicinity of that image. For instance, we would like to assess whether the probability is\nlocally concentrated near a low-dimensional “tangent” subspace, and if so, estimate its dimensionality.\nIn this section, we explain how these quantities may be computed from our energy model. We\nthen show that the local dimensionality around an image is often much lower than the ambient\ndimensionality of the space, but that this dimensionality varies with the particular image and the\nscale that is used to define the local neighborhood.\nMulti-scale dimensionality. Consider the distribution supported on the blue regions in the left two\npanels of Figure 5. For the leftmost region, the effective dimensionality of a local neighborhood\ndecreases with the size of that neighborhood. The opposite behavior is also possible, as shown in\nthe rightmost region. These examples show that dimensionality measures which aim to describe\n8\n--- Page 9 ---\n<latexit sha1_base64=\"5ntlU9Hwmj2tjEnSTq2Wft/1R/o=\">AAAB+HicdVDLSsNAFJ34qLVaG+3SzWARXEhICm11IRTcuKxgH9CEMJlM2qGTBzMTIYZ+iRsXirj1U9z5MYKTtoKKHrhwOOde7r3HSxgV0jTftbX1jc3SVnm7srNb3avp+wcDEacckz6OWcxHHhKE0Yj0JZWMjBJOUOgxMvRml4U/vCVc0Di6kVlCnBBNIhpQjKSSXL3mu7nNQ0iCYA4vYNPVG6ZxbpotqwlNw1ygIK1Wx2pDa6U0uppd/6iWsp6rv9l+jNOQRBIzJMTYMhPp5IhLihmZV+xUkAThGZqQsaIRColw8sXhc3isFB8GMVcVSbhQv0/kKBQiCz3VGSI5Fb+9QvzLG6cyOHNyGiWpJBFeLgpSBmUMixSgTznBkmWKIMypuhXiKeIIS5VVRYXw9Sn8nwyahtU22tcqjVOwRBkcgiNwAizQAV1wBXqgDzBIwT14BE/anfagPWsvy9Y1bTVTBz+gvX4CjeeUig==</latexit>de!=2<latexit sha1_base64=\"0hdAF1JcPCdOS43yJV7YmkVuSZM=\">AAAB+HicdVDLSsNAFJ34qLVaG+3SzWARXEhICm11IRTcuKxgH9CEMJlM2qGTBzMTIYZ+iRsXirj1U9z5MYKTtoKKHrhwOOde7r3HSxgV0jTftbX1jc3SVnm7srNb3avp+wcDEacckz6OWcxHHhKE0Yj0JZWMjBJOUOgxMvRml4U/vCVc0Di6kVlCnBBNIhpQjKSSXL3mu7nNQ0iCYA4voOXqDdM4N82W1YSmYS5QkFarY7WhtVIaXc2uf1RLWc/V32w/xmlIIokZEmJsmYl0csQlxYzMK3YqSILwDE3IWNEIhUQ4+eLwOTxWig+DmKuKJFyo3ydyFAqRhZ7qDJGcit9eIf7ljVMZnDk5jZJUkggvFwUpgzKGRQrQp5xgyTJFEOZU3QrxFHGEpcqqokL4+hT+TwZNw2ob7WuVxilYogwOwRE4ARbogC64Aj3QBxik4B48giftTnvQnrWXZeuatpqpgx/QXj8BjGOUiQ==</latexit>de!=1<latexit sha1_base64=\"dDy6PMBzIdUinbSnRlgflm4vrG4=\">AAAB+HicdVBNSwMxEM36UWu1drVHL8EieJAlW2irB6HgxWMF+wFtKdk024Ymu0uSFdalv8SLB0W8+lO8+WMEs20FFX0w8Hhvhpl5XsSZ0gi9W2vrG5u5rfx2YWe3uFey9w86KowloW0S8lD2PKwoZwFta6Y57UWSYuFx2vVml5nfvaVSsTC40UlEhwJPAuYzgrWRRnZpPEoHUkDq+3N4AdHIriDnHKGaW4XIQQtkpFZruHXorpRK0xqUP4q5pDWy3wbjkMSCBppwrFTfRZEeplhqRjidFwaxohEmMzyhfUMDLKgapovD5/DYKGPoh9JUoOFC/T6RYqFUIjzTKbCeqt9eJv7l9WPtnw1TFkSxpgFZLvJjDnUIsxTgmElKNE8MwUQycyskUywx0Sarggnh61P4P+lUHbfu1K9NGqdgiTw4BEfgBLigAZrgCrRAGxAQg3vwCJ6sO+vBerZelq1r1mqmDH7Aev0Eit+UiA==</latexit>de!=0<latexit sha1_base64=\"5ntlU9Hwmj2tjEnSTq2Wft/1R/o=\">AAAB+HicdVDLSsNAFJ34qLVaG+3SzWARXEhICm11IRTcuKxgH9CEMJlM2qGTBzMTIYZ+iRsXirj1U9z5MYKTtoKKHrhwOOde7r3HSxgV0jTftbX1jc3SVnm7srNb3avp+wcDEacckz6OWcxHHhKE0Yj0JZWMjBJOUOgxMvRml4U/vCVc0Di6kVlCnBBNIhpQjKSSXL3mu7nNQ0iCYA4vYNPVG6ZxbpotqwlNw1ygIK1Wx2pDa6U0uppd/6iWsp6rv9l+jNOQRBIzJMTYMhPp5IhLihmZV+xUkAThGZqQsaIRColw8sXhc3isFB8GMVcVSbhQv0/kKBQiCz3VGSI5Fb+9QvzLG6cyOHNyGiWpJBFeLgpSBmUMixSgTznBkmWKIMypuhXiKeIIS5VVRYXw9Sn8nwyahtU22tcqjVOwRBkcgiNwAizQAV1wBXqgDzBIwT14BE/anfagPWsvy9Y1bTVTBz+gvX4CjeeUig==</latexit>de!=2\n<latexit sha1_base64=\"0hdAF1JcPCdOS43yJV7YmkVuSZM=\">AAAB+HicdVDLSsNAFJ34qLVaG+3SzWARXEhICm11IRTcuKxgH9CEMJlM2qGTBzMTIYZ+iRsXirj1U9z5MYKTtoKKHrhwOOde7r3HSxgV0jTftbX1jc3SVnm7srNb3avp+wcDEacckz6OWcxHHhKE0Yj0JZWMjBJOUOgxMvRml4U/vCVc0Di6kVlCnBBNIhpQjKSSXL3mu7nNQ0iCYA4voOXqDdM4N82W1YSmYS5QkFarY7WhtVIaXc2uf1RLWc/V32w/xmlIIokZEmJsmYl0csQlxYzMK3YqSILwDE3IWNEIhUQ4+eLwOTxWig+DmKuKJFyo3ydyFAqRhZ7qDJGcit9eIf7ljVMZnDk5jZJUkggvFwUpgzKGRQrQp5xgyTJFEOZU3QrxFHGEpcqqokL4+hT+TwZNw2ob7WuVxilYogwOwRE4ARbogC64Aj3QBxik4B48giftTnvQnrWXZeuatpqpgx/QXj8BjGOUiQ==</latexit>de!=1\n<latexit sha1_base64=\"dDy6PMBzIdUinbSnRlgflm4vrG4=\">AAAB+HicdVBNSwMxEM36UWu1drVHL8EieJAlW2irB6HgxWMF+wFtKdk024Ymu0uSFdalv8SLB0W8+lO8+WMEs20FFX0w8Hhvhpl5XsSZ0gi9W2vrG5u5rfx2YWe3uFey9w86KowloW0S8lD2PKwoZwFta6Y57UWSYuFx2vVml5nfvaVSsTC40UlEhwJPAuYzgrWRRnZpPEoHUkDq+3N4AdHIriDnHKGaW4XIQQtkpFZruHXorpRK0xqUP4q5pDWy3wbjkMSCBppwrFTfRZEeplhqRjidFwaxohEmMzyhfUMDLKgapovD5/DYKGPoh9JUoOFC/T6RYqFUIjzTKbCeqt9eJv7l9WPtnw1TFkSxpgFZLvJjDnUIsxTgmElKNE8MwUQycyskUywx0Sarggnh61P4P+lUHbfu1K9NGqdgiTw4BEfgBLigAZrgCrRAGxAQg3vwCJ6sO+vBerZelq1r1mqmDH7Aev0Eit+UiA==</latexit>de!=0\n10−1210−910−610−3100103\nNoise variance t−20−10010203040Log probability (dB/dim)E[logpθ(y,t)|x]\n10−1210−910−610−3100103\nNoise variance t01000200030004000Eﬀective dimensionalitydeﬀ(x,t)Figure 5: Left: Two hypothetical examples illustrating how local effective dimensionality depends on\nthe scale of the neighborhood. For both examples, support of density corresponds to the blue regions.\nIn the left example, the dimensionality around the red point decreases with scale (from 2 down to 0),\nwhile the opposite is true for the right example. Right: Log probability and effective dimensionality\nas a function of noise level. Colored lines correspond to different example images x(shown in right\npanel of Figure 3), while the dashed black line shows the average over the ImageNet test set. The\nvertical gray line indicates the minimum noise level presented during training ( t= 10−9), and the\nhorizontal gray line the ambient dimensionality of the dataset ( d= 4096 ).\nthese geometrical structures need to depend on both the location and the scale of the neighborhood\n(Tempczyk et al., 2022).\nEffective dimensionality. We now introduce an effective dimensionality measure which can be\nequivalently defined from the optimal denoiser or the evolution of the probability landscape as it\ndiffuses. Given a noisy observation yof a clean image xwith noise variance t, the optimal denoiser\nestimates xwith the conditional expectation E[x|y]. Its average deviation from xcapture the local\nsupport of the data distribution around xat scale t. Intuitively, from observing y, the optimal denoiser\nidentifies that xis located on a deff-dimensional “tangent” space and projects yonto it. This preserves\nthe components of the noise that lie along the subspace, incurring a denoising error tdeff. Thus, we\ndefine the local effective dimensionality around xat scale tas\ndeff(x, t) =1\ntEyh\n∥x−Ex[x|y]∥2\f\f\fxi\n. (11)\nEffective dimensionality can be equivalently defined directly from the energy. Consider the forward\ndiffusion which progressively adds more noise to an image x, blurring the probability landscape.\nAt time t, we can define an effective energy Ey[U(y, t)|x]. Its rate of change with tcaptures\nthe effective dimensionality around x. Intuitively, if this landscape is locally a deff-dimensional\nsubspace, then adding more noise causes probability to diffuse in the d−deffnormal “off-manifold”\ndirections, spreading over a volume ∼t(d−deff)/2. Taking the logarithm, we obtain Ey[U(y, t)|x]∼\n(d−deff)1\n2logt. Thus, we can equivalently define\ndeff(x, t) =d−∂1\n2logtEy[U(y, t)|x]. (12)\nWe show in Appendix B.3 that the definitions in eqs. (11) and (12) are equivalent, unifying two\nseemingly distinct points of views adopted in related dimensionality measures (Wu and Verdú, 2011;\nMohan* et al., 2020; Tempczyk et al., 2022; Stanczuk et al., 2022; Kamkari et al., 2024; Horvat and\nPfister, 2024).\nNumerical results. We show in Figure 5 the behavior of the effective energy and dimensionality as\na function of the noise level. As the noise level increases, the log probability initially remains constant,\nindicating that probability is uniformly spread in a ball around x. The probability eventually decreases\nas the diffusion radius becomes larger than the local support size, and all lines converge to the mean\n(the negative entropy), consistent with the asymptotic Gaussian behavior of the energy. The effective\ndimensionality vanishes at large noise levels (images have a compact support, which eventually looks\nlike a point at large enough scales) but increases as the noise level is reduced, eventually reaching the\nambient dimensionality d(our model has a continuous non-zero density everywhere by construction).\nIn-between these two extremes, there exists a sizable range of scales t∈[10−9,10−2]where almost\nthe entire range of dimensionalities coexist. In particular, higher-probability images have lower-\ndimensional neighborhoods, even at relatively small scales, while lower-probability images have\n9\n--- Page 10 ---\nnearly full-dimensional neighborhoods, even at relatively large scales. We note two potential limits to\nthese estimates. First, the quantization of pixel values into bins of size 1/256limits the resolution\ntot∼10−5, possibly explaining the momentary decrease in dimensionality as tdecreases. Second,\nour energy model was only trained on noise levels down to tmin= 10−9. Thus, energies at smaller\nvalues of tare solely determined by model inductive biases, which favor a constant energy.\n4 Discussion\nWe have developed a novel framework for estimating the log probability (energy) of a distribution of\nimages from observed samples. The estimation method is simple and robust, and converges to a stable\nsolution with relatively small amounts of data (in our examples, 100,000images of size 64×64\nsuffices). The framework leverages the tremendous power of generative diffusion models, relying on\nnetworks trained to estimate the score of the data distribution by minimizing a denoising objective.\nWe augment this with a secondary objective that ensures consistency of the model across noise levels,\nand an architecture constructed from an existing denoiser so as to preserve its inductive biases. We\nvalidate our model by verifying that it achieves denoising performance equal to or surpassing the\noriginal denoiser, and NLL values comparable to the state of the art.\nWe have investigated the geometrical properties of the learned energy model. Notably, we observe\na lack of concentration of measure, with log probability values varying over a wide range and\nfollowing a Gumbel distribution. As a limit distribution for the maximum, rather than the sum, of i.i.d.\nrandom variables, we expect that this might arise in a broad class of high-dimensional probability\ndistributions. We also introduced a novel image- and scale-dependent effective dimensionality\nmeasure which unifies two separate points of view adopted in the literature. We demonstrate that\ndifferent image neighborhoods display both high and low dimensionality over a wide range of scales.\nThese results challenge simple interpretations of the manifold hypothesis. Furthermore, the estimated\ndimensionalities remain too high to explain how the curse of dimensionality may be lifted. This\nindicates that the energy landscape of natural images must have additional geometrical regularity.\nLimitations\nWe mention two main limitations of our work. First, our energy model has a roughly doubled\ntraining time compared to the base score network, due to the use of double back-propagation. We\nbelieve this can be improved through architecture improvements or specialized auto-differentiation\nfunctions. Another limitation of our approach is that we cannot provide any theoretical guarantees\nthat minimizing our objective leads to a quantitative approximation of the energy (see Appendix A.3).\nAcknowledgments\nFG thanks Louis Thiry for starting this research direction (Thiry and Guth, 2024). We also thank Joan\nBruna and Pierre-Étienne Fiquet for inspiring discussions. We gratefully acknowledge the support\nand computing resources of the Flatiron Institute (a research division of the Simons Foundation).\nReferences\nGrigory Bartosh, Dmitry P Vetrov, and Christian Andersson Naesseth. Neural flow diffusion models:\nLearnable forward process for improved diffusion modelling. Advances in Neural Information\nProcessing Systems , 37:73952–73985, 2024.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828,\n2013.\nSagnik Bhattacharya, Abhiram R Gorle, Ahmed Mohsin, Ahsan Bilal, Connor Ding, Amit Ku-\nmar Singh Yadav, and Tsachy Weissman. ItDPDM: Information-theoretic discrete poisson diffusion\nmodel. arXiv preprint arXiv:2505.05082 , 2025.\nGiulio Biroli, Tony Bonnaire, Valentin De Bortoli, and Marc Mézard. Dynamical regimes of diffusion\nmodels. Nature Communications , 15(1):9957, 2024.\n10\n--- Page 11 ---\nJoan Bruna and Jiequn Han. Provable posterior sampling with denoising oracles via tilted transport.\nAdvances in Neural Information Processing Systems , 37:82863–82894, 2024.\nChen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, and Chun-Yi Lee. On investigating the conservative\nproperty of score-based generative models. International Conference on Machine Learning , 2023.\nKristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimal\nclassification. In International Conference on Artificial Intelligence and Statistics , pages 2552–\n2573. PMLR, 2022.\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an\nalternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819 , 2017.\nR Cohen, Y Blau, D Freedman, and E Rivlin. It has potential: Gradient-driven denoisers for\nconvergent solutions to inverse problems. Adv Neural Information Processing Systems (NeurIPS) ,\n34, 2021.\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516 , 2014.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In\nInternational Conference on Learning Representations , 2017.\nYilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha\nSohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Composi-\ntional generation with energy-based diffusion models and mcmc. In International conference on\nmachine learning , pages 8489–8510. PMLR, 2023.\nLawrence C Evans. Partial differential equations , volume 19. American Mathematical Society, 1993.\nDongning Guo, Shlomo Shamai, Sergio Verdú, et al. The interplay between information and\nestimation measures. Foundations and Trends® in Signal Processing , 6(4):243–429, 2013.\nMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings of the thirteenth international conference on\nartificial intelligence and statistics , pages 297–304. JMLR Workshop and Conference Proceedings,\n2010.\nSébastien Herbreteau, Emmanuel Moebel, and Charles Kervrann. Normalization-equivariant neural\nnetworks with application to image denoising. Advances in Neural Information Processing Systems ,\n36:5706–5728, 2023.\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\ncomputation , 14(8):1771–1800, 2002.\nGeoffrey E Hinton, Terrence J Sejnowski, et al. Learning and relearning in Boltzmann machines.\nParallel distributed processing: Explorations in the microstructure of cognition , 1(282-317):2,\n1986.\nJ Ho, A Jain, and P Abbeel. Denoising diffusion probabilistic models. Adv Neural Information\nProcessing Systems (NeurIPS) , 33, 2020.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-\nbased generative models with variational dequantization and architecture design. In International\nconference on machine learning , pages 2722–2730. PMLR, 2019.\nChristian Horvat and Jean-Pascal Pfister. On gauge freedom, conservativity and intrinsic dimen-\nsionality estimation in diffusion models. In The Twelfth International Conference on Learning\nRepresentations , 2024.\nSamuel Hurault, Arthur Leclaire, and Nicolas Papadakis. Gradient step denoiser for convergent\nplug-and-play. arXiv preprint arXiv:2110.03220 , 2021.\n11\n--- Page 12 ---\nMichael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian\nsmoothing splines. Communications in Statistics-Simulation and Computation , 18(3):1059–1076,\n1989.\nAapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.\nJournal of Machine Learning Research , 6(4), 2005.\nZ Kadkhodaie and E P Simoncelli. Solving linear inverse problems using the prior implicit in a\ndenoiser. arXiv preprint arXiv:2007.13640 , Jul 2020.\nZahra Kadkhodaie, Florentin Guth, Eero P Simoncelli, and Stéphane Mallat. Generalization in diffu-\nsion models arises from geometry-adaptive harmonic representations. In The Twelfth International\nConference on Learning Representations , 2024.\nHamid Kamkari, Brendan Ross, Rasa Hosseinzadeh, Jesse Cresswell, and Gabriel Loaiza-Ganem. A\ngeometric view of data complexity: Efficient local intrinsic dimension estimation with diffusion\nmodels. Advances in Neural Information Processing Systems , 37:38307–38354, 2024.\nRafał Karczewski, Markus Heinonen, and Vikas Garg. Diffusion models as cartoonists! The\ncurious case of high density regions. In The Thirteenth International Conference on Learning\nRepresentations , 2025.\nD P Kingma and M Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,\n2013.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\nin neural information processing systems , 34:21696–21707, 2021.\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\nAdvances in neural information processing systems , 31, 2018.\nFrederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical efficiency of score matching:\nThe view from isoperimetry. ICLR , 2023.\nXianghao Kong, Rob Brekelmans, and Greg Ver Steeg. Information-theoretic diffusion. In The\nEleventh International Conference on Learning Representations , 2023.\nChieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano\nErmon. FP-diffusion: Improving score-based diffusion models by enforcing the underlying score\nfokker-planck equation. In International Conference on Machine Learning , pages 18365–18398.\nPMLR, 2023.\nYann LeCun and Fu Jie Huang. Loss functions for discriminative training of energy-based models.\nInInternational workshop on artificial intelligence and statistics , pages 206–213. PMLR, 2005.\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, Fujie Huang, et al. A tutorial on energy-based\nlearning. Predicting structured data , 1(0), 2006.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching\nfor generative modeling. In The Eleventh International Conference on Learning Representations ,\n2023.\nK Miyasawa. An empirical Bayes estimator of the mean of a normal population. Bull. Inst. Internat.\nStatist. , 38:181–188, 1961.\nS Mohan*, Z Kadkhodaie*, E P Simoncelli, and C Fernandez-Granda. Robust and interpretable\nblind image denoising via bias-free convolutional neural networks. In Int’l Conf on Learning\nRepresentations (ICLR) , Addis Ababa, Ethiopia, Apr 2020.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nInInternational conference on machine learning , pages 8162–8171. PMLR, 2021.\nM Raphan and E P Simoncelli. Least squares estimation without priors or supervision. Neural\nComputation , 23(2):374–420, Feb 2011. doi: 10.1162/NECO_a_00076.\n12\n--- Page 13 ---\nGabriel Raya and Luca Ambrogioni. Spontaneous symmetry breaking in generative diffusion models.\nAdvances in Neural Information Processing Systems , 36:66377–66389, 2023.\nEdward T Reehorst and Philip Schniter. Regularization by denoising: Clarifications and new\ninterpretations. IEEE transactions on computational imaging , 5(1):52–67, 2018.\nD Rezende and S Mohamed. Variational inference with normalizing flows. In Int’l Conf on Machine\nLearning (ICML) , pages 1530–1538. PMLR, 2015.\nH Robbins. An empirical bayes approach to statistics. In Proc Third Berkeley Symposium on\nMathematical Statistics and Probability , volume I, pages 157–163. University of CA Press, 1956.\nYaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by\ndenoising (red). SIAM Journal on Imaging Sciences , 10(4):1804–1844, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision , 115:211–252, 2015.\nTim Salimans and Jonathan Ho. Should EBMs model the energy or the score? In Energy Based\nModels Workshop-ICLR 2021 , 2021.\nMarta Skreta, Lazar Atanackovic, Joey Bose, Alexander Tong, and Kirill Neklyudov. The superposi-\ntion of diffusion models using the Itô density estimator. In The Thirteenth International Conference\non Learning Representations , 2024.\nJ Sohl-Dickstein, E Weiss, N Maheswaranathan, and S Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proc 32nd Int’l Conf\non Machine Learning (ICML) , volume 37 of Proceedings of Machine Learning Research , pages\n2256–2265, Lille, France, 07–09 Jul 2015. PMLR.\nY Song and S Ermon. Generative modeling by estimating gradients of the data distribution. Adv\nNeural Information Processing Systems (NeurIPS) , 32, 2019.\nY Song, J Sohl-Dickstein, D P Kingma, A Kumar, S Ermon, and B Poole. Score-based generative\nmodeling through stochastic differential equations. In Int’l Conf on Learning Representations\n(ICLR) , 2021a.\nYang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint\narXiv:2101.03288 , 2021.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. Advances in Neural Information Processing Systems , 34:1415–1428,\n2021b.\nJan Stanczuk, Georgios Batzolis, Teo Deveney, and Carola-Bibiane Schönlieb. Your diffusion model\nsecretly knows the dimension of the data manifold. arXiv preprint arXiv:2212.12611 , 2022.\nPiotr Tempczyk, Rafał Michaluk, Lukasz Garncarek, Przemysław Spurek, Jacek Tabor, and Adam\nGolinski. LIDL: Local intrinsic dimension estimation using approximate likelihood. In Interna-\ntional Conference on Machine Learning , pages 21205–21231. PMLR, 2022.\nJoshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for\nnonlinear dimensionality reduction. science , 290(5500):2319–2323, 2000.\nL Theis, A van den Oord, and M Bethge. A note on the evaluation of generative models. In\nInternational Conference on Learning Representations (ICLR 2016) , pages 1–10, 2016.\nLouis Thiry and Florentin Guth. Classification-denoising networks. arXiv preprint arXiv:2410.03505 ,\n2024.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing\ningredient for fast stylization. arXiv preprint arXiv:1607.08022 , 2016.\n13\n--- Page 14 ---\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with PixelCNN decoders. Advances in neural information processing systems ,\n29, 2016.\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science ,\nvolume 47. Cambridge university press, 2018.\nP Vincent. A connection between score matching and denoising autoencoders. Neural Computation ,\n23(7):1661–1674, 2011.\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48. Cam-\nbridge university press, 2019.\nYihong Wu and Sergio Verdú. MMSE dimension. IEEE Transactions on Information Theory , 57(8):\n4857–4879, 2011.\nShahar Yadin, Noam Elata, and Tomer Michaeli. Classification diffusion models. arXiv preprint\narXiv:2402.10095 , 2024.\nJonathan S Yedidia, William T Freeman, and Yair Weiss. Constructing free-energy approximations\nand generalized belief propagation algorithms. IEEE Transactions on information theory , 51(7):\n2282–2312, 2005.\nShuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng,\nTianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are\ncapable generative models. arXiv preprint arXiv:2412.06329 , 2024.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood\nestimation for diffusion ODEs. In International Conference on Machine Learning , pages 42363–\n42389. PMLR, 2023.\nA Discussions on density estimation\nA.1 On comparison of NLL values\nWe identify three issues that arise in estimation of NLL values, each of which limits the precision\nwith which they can be compared.\nA first issue concerns the selection and pre-processing of the training data. There are two different\nversions of ImageNet at 64×64resolution: one introduced in Van den Oord et al. (2016), no longer\navailable, and a second which uses anti-aliasing during downsampling (reducing entropy and thus\nNLL), introduced in Chrabaszcz et al. (2017). This change in the dataset accounts for variations\nof about 0.3 bits/dimension (Zheng et al., 2023). Similarly, the data augmentations used can both\nincrease entropy (e.g., random flips) or reduce it (e.g., center crops, or resizing operations with\nanti-aliasing).\nA second point is that NLL estimates must be computed on a discrete probability model, and NLL\nestimates for continuous models are dependent on the method used to discretize the distribution.\nThe simplest option is to assume that the probability is uniform within quantization bins. Discrete\nprobabilities are then computed as a product of the continuous probability with the volume of the\nquantization bin, which corresponds to an additive shift of 8 bits in the NLL for image data. This\ncan be enforced by adding uniform noise to the data, a technique known as “uniform dequantization”\n(Ho et al., 2019), which leads to an upper bound on the NLL of the corresponding discrete model\n(Theis et al., 2016). These differences can account for variations of 0.1 to 0.5 bits/dimension (Kong\net al., 2023). A more sophisticated variational dequantization procedure can improve results by an\nadditional 0.1 bits/dimension (Ho et al., 2019; Song et al., 2021b). In contrast, directly modeling the\ndiscrete data with appropriate methods can lead to reductions in NLL of more than 2 bits/dimension\n(Bhattacharya et al., 2025).\nFinally, a third point is that depending on the method, the NLL should be interpreted differently.\nClassically, pθis a normalized probability distribution, typically obtained through a change-of-\nvariable formula (as in flow-based models), so that up to an unknown additive constant (the entropy\n14\n--- Page 15 ---\nof the data), the NLL directly evaluates the KL divergence KL(p∥pθ). For some other models (such\nas V AEs), the NLL is not directly tied to a probabilistic model and is instead evaluated through a\nvariational lower bound, leading to upper bounds on −logp(x)for each x. The NLL however remains\nan upper bound on the entropy of the data. The MSE-based formulas of Kong et al. (2023) also\nfall in this category when using a (necessarily suboptimal) network denoiser. Lastly, approximately\nnormalized energy-based models such as ours or CDM (Yadin et al., 2024) compute estimates of\n−logp(x)that are neither lower nor upper bounds, so that NLL can only be interpreted as an estimate\nof the entropy of the data.\nAs a result of these differences, NLL values of different methods are often not directly comparable.\nHere, we aim to learn an accurate continuous probability model of image distributions rather than\nobtaining the best upper bound on the discrete entropy of the data.\nWhen evaluating previously published NLL results, we found that these details were often not\nprovided, and we thus had to make assumptions in compiling Table 2. The “anti-aliasing” column\nrefers to the version of ImageNet64 used, we assumed that articles that cite Van den Oord et al.\n(2016) for the dataset made use of the aliased version. We assume no data augmentation if none is\nmentioned (for TarFlow (Zhai et al., 2024), the authors mention performing center crops of the data,\nbut an inspection of their code indicates that it has been disabled for ImageNet64). The “discreteness”\ncolumn refers to the nature of the probability model and the potential conversion from continuous\nto discrete probabilities (“discrete” for discrete probability models, “continuous” for an additive\nshift of 8bits, and “uniform” for uniform dequantization by adding uniform noise). We assume a\ncontinuous model if no dequantization is mentioned (FM (Lipman et al., 2023) and NFDM (Bartosh\net al., 2024) use a slightly different notion of uniform dequantization which improves NLL by 0.04\nbits/dimension). The “type” column refers to the nature of the LLM estimate (“normalized” for an\nexactly normalized probability model coming from a flow-based model, “upper bound” for variational\nlower bounds on log probability, and “estimate” for other approaches). Finally, we indicate those\nmethods that use a single neural function evaluation (NFE) to compute log probability in the “single\nNFE” column.\nA.2 Background on density computation in diffusion models\nWe review several methods of estimating log probabilities from diffusion models that have appeared\nin the literature.\nIn early publications on diffusion methods, probabilities are computed with the so-called probability\nflow ODE (Song et al., 2021a). This corresponds to the distribution of samples generated by the\nbackward ODE. Given a large noise level tmax, the backward ODE solves the equation\n−dxt\ndt=−1\n2∇yUθ(xt, t), (13)\nbackwards in time from xtmax∼ N(0, tmaxId)and produces an approximate sample x=x0∼pODE.\nThe log probability of this sample can be calculated as (Song et al., 2021a)\n−logpODE(x) =∥xtmax∥2\n2tmax+d\n2log(2 πtmax)−1\n2Ztmax\n0∆yUθ(xt, t)dt. (14)\nNote that eq. (14) is also valid for arbitrary test points x, in which case the ODE (13) needs to be\nsolved forward in time from x0=xatt= 0 tot=tmax. Equation (14) requires estimating the\ndivergence of the score (the Laplacian of the energy), which is typically approximated with the\nHutchinson trace estimator (Hutchinson, 1989).\nAnother approach is to use a variational bound as in Song et al. (2021b); Kingma et al. (2021); Kong\net al. (2023). This variational bound arises from the exact identity (Kong et al., 2023)\n−logp(x) =Ey[U(y, tmax)|x]−Ztmax\n0\u0010\ntd−Eyh\n∥x−Ex[x|y]∥2\f\f\fxi\u0011dt\n2t2. (15)\nThe first term is the effective energy at t=tmax, which is equivalent tod\n2log(2 πetmax)astmax→ ∞ .\nThe second term features the optimal mean squared error over noise levels t∈[0, tmax]. Note that\nthe integrand can be rewritten as (d−deff(x, t))d\u00001\n2logt\u0001\n, and eq. (15) can thus be derived by\nintegrating the two equivalent definitions of effective dimensionality (11) and (12) (see Appendix B.3).\n15\n--- Page 16 ---\nEquation (15) naturally leads to an upper-bound on the negative log probability of the data when\nreplacing the optimal denoiser Ex[x|y, t]with the denoiser derived from the Miyasawa-Tweedie\nexpression, y−t∇yUθ(y, t):\n−logpMSE(x) =d\n2log(2 πetmax)−Ztmax\n0\u0010\ntd−Eyh\n∥x−y+t∇yUθ(y, t)∥2\f\f\fxi\u0011dt\n2t2.(16)\nThis framework can be generalized to other noise distributions (Guo et al., 2013) such as Poisson\nnoise, which is more adapted to discrete distributions (Bhattacharya et al., 2025).\nFinally, it was recently observed in Skreta et al. (2024); Karczewski et al. (2025) that a cheaper\nunbiased stochastic estimator of this bound can be obtained with the Itô formula. Given a realization\n(xt)t∈R+of the forward SDE (Brownian motion) dxt= dwtstarted at x0=x∼p(x), then\n−logp(x) =U(xtmax, tmax)−Ztmax\n0\u0012\n⟨∇yU(xt, t),dxt⟩ −1\n2∥∇yU(xt, t)∥2dt\u0013\n. (17)\nWhen tmax→ ∞ , one can again use U(xtmax, tmax)∼∥xtmax∥2\n2tmax+d\n2log(2 πtmax). Replacing the\nunknown true energy Uin the integrand with a model Uθleads to a biased stochastic estimate of the\nlog probability:\n−logpSDE(x) =∥xtmax∥2\n2tmax+d\n2log(2 πtmax)−Ztmax\n0\u0012\n⟨∇yUθ(xt, t),dxt⟩ −1\n2∥∇yUθ(xt, t)∥2dt\u0013\n.\n(18)\nKarczewski et al. (2025) show that averaging over SDE trajectories started at the same x0=xleads\nto an upper bound on the NLL, in fact recovering the one in eq. (16).\nIn summary, the ODE gives deterministic probabilities exactly corresponding to the corresponding\ngenerative model (eq. (14)), while for the SDE one can choose between an denoising-based upper\nbound (eq. (16)) or a cheap stochastic estimator of it (eq. (18)). These evidently similar approaches\nare equivalent if and only if the model is consistent across noise levels (i.e., satisfies the diffusion equa-\ntion). In practice, the choice of estimation method can lead to variations of up to 0.5 bits/dimension\n(Kong et al., 2023), see in particular Karczewski et al. (2025) for a careful study). These approaches\nalso apply to our model, in addition to the direct evaluation of Uθ(x, t= 0) . While this latter estimate\ndoes not correspond to a generative model (like the ODE) or a variational bound (like the SDE), its\nmain advantage is its computational efficiency, as it does not require integrating over noise levels and\ngives deterministic values without averaging over noise realizations (for a divergence term or mean\nsquared error). We note the related work of Yadin et al. (2024) which also trains an energy model\nwith score matching and a second objective. The main conceptual difference with our approach is that\nwe use a continuous (as opposed to discrete) time variable t, which offerws robustness to noise level\nscheduling issues and to relies on the inductive biases of existing diffusion architectures (Section 2.3).\nA.3 On the theoretical justification of dual score matching\nA limitation of our approach is that we offer no theoretical guarantees that minimizing our objective\nleads to a good approximation of the energy. It would be desirable to show that our training objective\n(assuming infinite training data) quantitatively controls the distance between the learned energy and\nthe data energy. This could be achieved by showing that the joint distribution p(y, t)(with logt\nuniformly distributed in [logtmin,logtmax]) satisfies a Poincaré inequality, i.e.,\nVar[Uθ(y, t)−U(y, t)]≤CEh\n∥∇yUθ(y, t)− ∇ yU(y, t)∥2+ (∂tUθ(y, t)−∂tU(y, t))2i\n,(19)\nfor some constant Cthat is not too large. While we conjecture that such a result holds (or\na variant, such as when considering the distribution of (y/√\nt,logt), to match our noise level\nweighting), note that this is weaker than a control in the Kullback-Leibler divergence. Replacing\nVar[Uθ(y, t)−U(y, t)]withKL(p∥pθ)in eq. (19) would require that the model distribution pθ(y, t)\nsatisfies a log-Sobolev inequality, which could only be enforced with specific architectures. As\na result, there is no control of the learned energy outside the support of the data distribution, so\nthat out-of-distribution detection may be unreliable. It also implies our model may not be exactly\nnormalized. Our NLL calculations are thus only approximate (although computationally cheap).\nThese limitations are common to all current score-based and unnormalized energy-based approaches,\nincluding the related work of Yadin et al. (2024).\n16\n--- Page 17 ---\nA.4 On frequency estimation with density models\nHere, we describe a counter-intuitive property of density models in high dimensions which poses a\nchallenge for estimating frequencies. We also refer the interested reader to Theis et al. (2016) which\noffers related observations.\nConsider a mixture of two uniform distributions on two compact sets (classes) C1andC2with\nrespective frequencies f1andf2. The probability density is then constant on each class, with value\npi=fi\nViwhere Viis the volume of Ci. The volume Vitypically scales exponentially with d,Vi∼rd\ni\nwhere riis the characteristic size of Ci. The energy values on each class are then\nUi=−logpi=dlogri−logfi. (20)\nIn high dimensions d≫1, the energy values are dominated by the volume and only weakly depend\non the frequencies of each class.\nA concrete example of this phenomenon can be seen in the mixture of two Gaussian distributions\nconsidered in Figure 1, where the two classes correspond to the two spheres of radius√\ndσifor\ni∈ {1,2}. The typical values of the energy Ui=d\n2log\u0000\n2πeσ2\ni\u0001\n−logfiare dominated by the\nentropy of the corresponding Gaussian. The energy is then determined by the value of the standard\ndeviation σimuch more than the frequency fi.\nThe implications of this observation for high-dimensional density models are twofold. First, estimated\ndensities are dominated by volumes of typical sets (entropies of the mixture components), more\nthan frequencies of different categories (one-dimensional marginals). The latter is easily learned\nfrom data, while estimating the former is a much more challenging task. Second, estimating energy\nup to a small relative error is not sufficient to capture observed frequencies. For our ImageNet64\nmodel ( d= 4096 ), a relative error of ∼0.1dB/dimension (as estimated from the generalization\nexperiment in Figure 2) is negligible compared to typical energy differences of ∼10dB/dimension,\nbut the required precision to estimate frequencies with a relative accuracy of 10% is1/d= 2×10−4\ndB/dimension.\nB Proofs and derivations\nB.1 Time score matching (proof of eq. (4))\nWe start from the expression of the energy (negative log probability) of noisy data:\nU(y, t) =−logp(y|t), (21)\np(y|t) =Z\np(x)p(y|x, t)dx. (22)\nDifferentiating the energy with respect to tyields\n∂tU(y, t) =−∂tp(y|t)\np(y|t)(23)\n=−R\np(x)p(y|x, t)∂tlogp(y|x, t)dx\np(y|t)(24)\n=Ex[−∂tlogp(y|x, t)|y, t]. (25)\nNote that the derivation exactly matches that of the Miyasawa-Tweedie identity by replacing ∂twith\n∇y, and does not make any assumptions about the form of p(y|x, t). Restricting to additive Gaussian\nnoise, −logp(y|x, t) =1\n2t∥y−x∥2+d\n2log(2 πt), and substituting into eq. (25) gives the “time\nscore-matching identity of eq. (4):\n∂tU(y, t) =Ex\"\nd\n2t−∥y−x∥2\n2t2\f\f\f\f\fy#\n. (26)\nB.2 Energy architecture\nSuppose that the energy network is defined in terms of a score network sθ(y, t)asUθ(y, t) =\n1\n2⟨y, sθ(y, t)⟩, where the score network is assumed conservative and homogeneous. Conservativity\n17\n--- Page 18 ---\nmeans that there exists a scalar function ϕsuch that sθ(y, t) =∇yϕ(y, t), which implies that the\nJacobian ∇ysθ(y, t) =∇2\nyϕ(y, t)is symmetric. The homogeneity property requires that for all\nλ≥0,sθ(λy, t) =λsθ(y, t). Differentiating with respect to λand setting λ= 1yields\n∇ysθ(y, t)y=sθ(y, t). (27)\nWe now calculate the gradient of the energy network:\n∇yUθ(y, t) =1\n2\u0000\nsθ(y, t) +∇ysθ(y, t)Ty\u0001\n=sθ(y, t), (28)\nusing the conservative and homogeneity properties to derive that ∇ysθ(y, t)y=∇ysθ(y, t)Ty=\nsθ(y, t). Note that even if sθis not conservative, then it still holds that ∇yUθ(y, t) =\n1\n2\u0000\n∇ysθ(y, t) +∇ysθ(y, t)T\u0001\ny, which can be interpreted as a symmetrization of the Jacobian of sθ.\nWe remark that if sθis homogeneous, then Uθis quadratically homogeneous ( Uθ(λy, t) =\nλ2Uθ(y, t)). Note that this does not correspond to enforcing (asymmetric) Gaussian one-dimensional\nmarginals ⟨y, u⟩. Rather, this enforces that the distribution of ⟨y, u⟩conditioned on the orthogonal\nprojection y− ⟨y, u⟩u/∥u∥2is (asymmetric) Gaussian.\nB.3 Effective dimensionality (equivalence between eqs. (11) and (12))\nWe start by calculating the time derivative of the effective energy Ey[U(y, t)|x]. We have\n∂tEy[U(y, t)|x] =∂tZ\np(y|x, t)U(y, t)dy (29)\n=Z\n(∂tp(y|x, t)U(y, t) +p(y|x, t)∂tU(y, t))dy. (30)\nThe first term is the derivative with respect to variance of a Gaussian distribution N(x, tId). It\nsatisfies the diffusion equation:\n∂tp(y|x, t) =1\n2∆yp(y|x, t). (31)\nSimilarly, for the second term, we use the fact that U(y, t) =−logp(y|t)where p(y|t)also satisfies\nthe diffusion equation (as can be seen from the Fokker-Planck equation in the variance exploding\ncase (Song et al., 2021a)):\n∂tU(y, t) =−∂tp(y|t)\np(y|t)(32)\n=−∆yp(y|t)\n2p(y|t)(33)\n=1\n2p(y|t)∇y·(p(y|t)∇yU(y|t)) (34)\n=1\n2∆yU(y|t)−1\n2∥∇yU(y|t)∥2. (35)\nThis equation appeared in Lai et al. (2023); Bruna and Han (2024) and is a special case of a Hamilton-\nJacobi equation (Evans, 1993).\nCombining eqs. (31) and (35) into eq. (30) and integrating by parts twice, we have\n∂tEy[U(y, t)|x] =Z\np(y|x, t)\u0012\n∆yU(y, t)−1\n2∥∇yU(y|t)∥2\u0013\ndy. (36)\nWe recognize the expression of the mean squared error as given by combining Miyasawa-Tweedie\nwith Stein’s unbiased risk estimate (SURE). Indeed,\nEyh\n∥x−Ex[x|y]∥2\f\f\fxi\n=Eyh\n∥x−y+t∇yU(y, t)∥2\f\f\fxi\n(37)\n=Eyh\n∥x−y∥2+ 2t⟨x−y,∇yU(y, t)⟩+t2∥∇yU(y, t)∥2\f\f\fxi\n(38)\n=td+t2Eh\n−2∆yU(y, t) +∥∇yU(y, t)∥2\f\f\fxi\n, (39)\n18\n--- Page 19 ---\nwhere we have used Stein’s lemma in the last step. We finally combine eqs. (36) and (39) to obtain\n1\ntEyh\n∥x−Ex[x|y]∥2\f\f\fxi\n=d−2t∂tEy[U(y, t)|x]. (40)\nIt is convenient to rewrite 2t∂t=∂1\n2logtasd\u00001\n2logt\u0001\n=dt\n2t.\nWe define the common value in eq. (40) to be deff(x, t). It is related (but not equal) to dimensionality\nmeasures estimated from the singular values of the Jacobian of a denoiser (Mohan* et al., 2020;\nHorvat and Pfister, 2024). The limit of deff(x, t)when t→0has appeared in the literature under\nthe name of local intrinsic dimensionality using (approximate) likelihood (LIDL) (Tempczyk et al.,\n2022; Stanczuk et al., 2022; Kamkari et al., 2024). Its average over images xis equal to the MMSE\ndimension of Wu and Verdú (2011).\nC Experimental details\nC.1 Energy network architecture\nUNet architecture. Our UNet architecture is composed of 3encoder blocks, a middle block, and 3\ndecoder blocks. Each block is itself composed of 3layers, each a sequence of bias-free convolution,\nnormalization, and non-linearity, for a total of 21layers. The first convolutional layer of each encoder\nblock but the first and the middle block has a stride of 2(downsampling, in both vertical and horizontal\ndirections), and the last convolutional layer of each decoder block (except for the last one and the\nmiddle block) is transposed with a stride of 2(upsampling). The output of each encoder block is\nconcatenated to the input of the corresponding decoder block (which comes from the output of the\ncorresponding encoder block, via a “skip connection”). The number of channels is doubled in each\nblock, starting from a base value of 64at the coarsest scale. We replace ReLUs with GeLUs to ensure\nthat∇yUθis differentiable. Thus, Uθis only approximately quadratically homogeneous.\nNormalization. We also replace batch normalizations, whose behavior during the backward pass is\nincompatible with a second back-propagation, with a homogeneous version of instance normalization\n(Ulyanov et al., 2016). If the input xconsists of Cchannels (xc)1≤c≤C, and its spatial mean is\nµ(x)∈RC, each channel is normalized according to\nxc7→s\n∥x−µ(x)∥2+ε\nC∥xc−µ(xc)∥2+ε(xc−µ(xc)). (41)\nUp to a small ε > 0parameter for numerical stability, this ensures that after normalization all\nchannels xchave equal norms while preserving the global norm ∥x∥. This normalization layer is also\nhomogeneous when ε= 0, as the spatial mean is estimated from the input x. This normalization is\nfollowed by a learned rescaling of each channel, xc7→γcxc, where γ∈RCis learnable.\nNoise level conditioning. The noise variance tis also an input of sθ. As is standard in diffusion\nmodels (Nichol and Dhariwal, 2021), a time embedding e(t)∈R256is computed with Fourier\nfeatures cos(ωkt),sin(ωkt)(we use 32frequencies (ωk)kthat are linearly spaced in the log domain\nand ranging from 1/tmaxto1/tmin) followed by a shallow MLP. This time embedding e(t)is then\nused to condition the output of each normalization layer via gain control: xc7→((1 +⟨wc, e(t)⟩)xc)\nwhere wcis a learned layer- and channel-dependent vector ∈R256.\nC.2 Training hyper-parameters\nTo summarize Section 2.2, the dual score matching training objective is\nℓ(θ) =Ex,z,t\n\r\r\r\r\rr\nt\nd∇yUθ(y, t)−z√\nd\r\r\r\r\r2\n+ \nt\nd∂tUθ(y, t)−1\n2 \n1−∥z∥2\nd!!2\n, (42)\nwhere x∼p(x)is the data distribution, z∼ N (0,Id)is the noise, y=x+√\ntzis the noisy\nmeasurement, and logt∼ U(logtmin,logtmax). In our experiments we use tmin= 10−9and\ntmax= 103, and the training image intensities are rescaled to have values in [0,1].\n19\n--- Page 20 ---\nWe use the ImageNet64 dataset (Chrabaszcz et al., 2017), with (only) horizontal flips as data\naugmentation. The models used in Tables 1 and 2 and for the generalization experiment in Figure 2\nare trained on color images, while the model used in Figures 3 to 5 is trained on grayscale images.\nPixel values are rescaled to [0,1]by dividing by 255. All models are trained for 1Msteps, with a\nbatch size of 128. We use the Adam optimizer with default parameters and an initial learning rate\nof0.0005 (except for the generalization experiments which used a learning rate of 0.0002 ) that is\nhalved every 100,000steps. All models are trained on a single NVIDIA H100 GPU, which takes\nabout 4days for ImageNet64.\nC.3 Additional details\nGaussian scale mixture example (Figure 1). We generate n= 100 ,000samples from a mixture\nof two Gaussian distributions,1\n2N(0, σ2\n1Id) +1\n2N(0, σ2\n2Id), with σ1= 1andσ2= 4, in dimension\nd= 1,000. The true (normalized) energy is given by\nU(y, t) =−log 2X\ni=1e−∥y∥2\n2(σ2\ni+t)−d\n2log(2π(σ2\ni+t))−log 2!\n. (43)\nWe parameterize the energy as a mixture of quadratics:\nUθ(y, t) =−log 2X\ni=1e−ai(t)∥y∥2−bi(t)!\n, (44)\nwhere the functions ai, biare computed by a 5-layer MLP with a hidden dimension of 256that takes\nlog(t+tmin)as input. This network is trained either with single (space) score matching or with\ndual score matching, both across noise levels t∈[tmin, tmax]. Training is otherwise similar to the\nImageNet64 experiments (see Appendix C.2), for 20,000training steps with a batch size of 512and\nan initial learning rate of 0.0001 , over noise levels from tmin= 10−2totmax= 102. We note that\nthe energy learned after single score matching training is stochastic: as the relative energy between\nthe two mixture components is not constrained by the data, its value is determined by the random\ninitialization, so that rerunning the experiment will lead to a different value.\nHistogram of log probabilities (Figure 3). The Gumbel fit is calculated by maximizing likelihood.\nThe obtained parameters (in decibels/dimension) are 9.57for the location and 3.17for the scale.\nEquivalently, p(x)follows a Fréchet distribution, with a scale parameter equal to 9.05dand shape\nparameter equal to 1.37/d, where d= 64×64 = 4096 is the dimension of ImageNet64 grayscale\nimages.\nComputing dimensionality (Figure 5). Equations (11) and (12) provide two ways to estimate\neffective dimensionality from a learned energy model. If the model is exact, Uθ(y, t) =U(y, t),\nthen they coincide (more generally, they coincide when the model satisfies the diffusion equation),\nbut in general they only approximately coincide. For numerical stability, we found it preferable to\nuse the version defined from the mean squared error of the underlying denoiser (eq. (11)). This\nguarantees non-negative dimensionalities, and also has the advantage of being an upper bound on the\ntrue dimensionality (as any denoiser yields an upper bound on the minimum MSE).\nD ImageNet images according to their probability\nFigure 6: Highest probability images in ImageNet64 (test set).\n20\n--- Page 21 ---\nFigure 7: Lowest probability images in ImageNet64 (test set).\nFigure 8: Images in ImageNet64 (test set) with linearly-spaced log probabilities.\n21",
  "text_length": 78517
}