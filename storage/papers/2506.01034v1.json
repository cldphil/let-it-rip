{
  "id": "http://arxiv.org/abs/2506.01034v1",
  "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models",
  "summary": "Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.",
  "authors": [
    "Benjamin Matthias Ruppik",
    "Julius von Rohrscheidt",
    "Carel van Niekerk",
    "Michael Heck",
    "Renato Vukovic",
    "Shutong Feng",
    "Hsien-chin Lin",
    "Nurul Lubis",
    "Bastian Rieck",
    "Marcus Zibrowius",
    "Milica Gašić"
  ],
  "published": "2025-06-01T14:30:46Z",
  "updated": "2025-06-01T14:30:46Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01034v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01034v1  [cs.CL]  1 Jun 2025Less is More: Local Intrinsic Dimensions of\nContextual Language Models\nBenjamin Matthias Ruppik1Julius von Rohrscheidt2,3Carel van Niekerk1Michael Heck1\nRenato Vukovic1Shutong Feng1Hsien-chin Lin1Nurul Lubis1\nBastian Rieck2,3,4Marcus Zibrowius1Milica Gaši ´c1\n1Faculty of Mathematics and Natural Sciences, Heinrich Heine University Düsseldorf, Germany\n2Institute of AI for Health, Helmholtz Munich, Germany\n3Technical University of Munich, Germany\n4University of Fribourg, Switzerland\n{ruppik,niekerk,heckmi,revuk100,fengs,\nlinh,lubis,marcus.zibrowius,gasic}@hhu.de\njulius.rohrscheidt@helmholtz-munich.de\nbastian.grossenbacher@unifr.ch\nAbstract\nUnderstanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical evaluation. In\nthis paper, we introduce a novel perspective based on the geometric properties of\ncontextual latent embeddings to study the effects of training and fine-tuning. To that\nend, we measure the local dimensions of a contextual language model’s latent space\nand analyze their shifts during training and fine-tuning. We show that the local\ndimensions provide insights into the model’s training dynamics and generalization\nability. Specifically, the mean of the local dimensions predicts when the model’s\ntraining capabilities are exhausted, as exemplified in a dialogue state tracking\ntask, overfitting, as demonstrated in an emotion recognition task, and grokking,\nas illustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of fine-tuning\non embedding spaces, facilitating informed decisions when configuring models for\nspecific applications. The results of this work contribute to the ongoing discourse\non the interpretability, adaptability, and generalizability of LLMs by bridging the\ngap between intrinsic model mechanisms and geometric properties in the respective\nembeddings.\n1 Introduction\nLarge language models (LLMs) have transformed natural language processing in recent years,\nachieving impressive performance across a variety of tasks (Brown et al., 2020; Devlin et al., 2019;\nJiang et al., 2023; Radford et al., 2018; Touvron et al., 2023). These models learn contextual\ntoken embeddings in high-dimensional latent spaces, whose structure governs how information is\nrepresented and processed. However, most performance diagnostics in LLMs rely on supervised\nvalidation or task-specific probes, with few attempting to understand the geometry of the LLM\nembedding spaces. Our paper is motivated by a central question: Can structural changes in the\nembedding space yield unsupervised insights into model behavior across language modeling tasks?\nWe address this question by applying a localized version of the TwoNN estimator (Facco et al.,\nPreprint. Under review.\n--- Page 2 ---\n2017) to quantify the local intrinsic dimension of contextual token embeddings. While the ambient\ndimension of these embeddings is typically large (ranging from hundreds to thousands of dimensions),\nour observations show that the local intrinsic dimension reflects a lower-dimensional manifold\nstructure that varies across data space regions. This heterogeneity allows us to derive model- and data-\nspecific geometric signatures without relying on labeled validation data or task-specific supervision.\nContributions. We present a unified framework for analyzing LLM training dynamics through\ndimensionality. Based on this framework, we (i) demonstrate that fine-tuning reshapes the local\nintrinsic dimension in a dataset-specific manner, allowing us to infer overlap between training data\nused for fine-tuning with validation (or test) data from geometric shifts alone, without requiring\nlabels; (ii) show that dimensionality anticipates the onset of grokking in synthetic arithmetic tasks,\nidentifying generalization beyond training data from the dimensionality of the model’s embeddings\nof the training data alone; (iii) demonstrate that local intrinsic dimensions can predict training\nconvergence , as evidenced in a sequence-tagging-based dialogue state tracking task where stabilizing\ndimension correlates with model stabilization; and (iv) show that local intrinsic dimensions detect\noverfitting in a sequence classification setting, where an initial drop followed by a rise in dimension\nreflects the model’s tradeoff between generalization and memorization. In total, our results suggest\nthat local intrinsic dimensions serve as a valuable unsupervised signal for practitioners seeking to\ninterpret and monitor LLM behavior. Beyond offering empirical insights, our work thus highlights\nthe potential of geometric descriptors to complement traditional evaluation methods and potentially\nserve to inform future model design.\n2 Related Work\nOur work lies at the intersection of geometry-aware language model analysis and dataset-level\nrepresentation diagnostics. While prior efforts focus on the intrinsic dimensions of single sequences\nor model parameters, our contribution is quantifying local intrinsic dimensions across datasets and\ntraining stages, enabling unsupervised inspection of generalization dynamics and fine-tuning effects.\nNotions of intrinsic dimensions of language models In a first qualitative analysis of the internal\nrepresentations of transformer models, Ethayarajh (2019) and Cai et al. (2021) identify clusters\nand low-dimensional manifold structures in contextual embedding spaces. Recent work has started\nstudying relationships between geometric properties of LLMs and corresponding semantics. In\nTulchinskii et al. (2024), the authors discover that the average intrinsic global dimension of artificially\ngenerated texts is lower than that of human-written texts. In that work, the points in the latent space\nare taken from a single text paragraph, so their work applies to single data input sequences, not to\nentire datasets like our approach. Valeriani et al. (2024) investigate how the global intrinsic dimension\nestimated is altered as data is passed through an LLM. Aghajanyan et al. (2021) define a notion of\nintrinsic dimension based on restricting the model’s parameter space and its effect on the objective\nfunction, and show that larger LLMs tend to have smaller intrinsic dimensionality. That work studies\nthe dimensionality of the model’s parameter space, and not the latent space created from specific\ndatasets. In Viswanathan et al. (2025), the authors analyze token-level intrinsic dimensions, linking\ntoken geometry to model next-token-prediction loss. The difference between their method and ours is\nthat they always consider the contextual token embeddings of a single, sufficiently long prompt as\nmaking up the embedding space. In contrast, we sub-sample from an entire dataset split.\nTopology-based analysis of models The topological descriptors in Kushnareva et al. (2021); Perez\nand Reinauer (2022); Tulchinskii et al. (2024) correspond to entire sequences of input data, while our\nproposed estimates are defined on the token level. In Durrani et al. (2022), pre-trained and fine-tuned\nmodels are compared using hierarchical clustering and alignment functions. The results indicate\nthat the latent space in the higher layers adapts to task-specific concepts, while the lower layers\npreserve the general concepts learned in the pre-trained model. Ed-dib et al. (2024) introduces a\nmethod for fine-tuning large language models by dynamically adjusting LoRA ranks based on the\nintrinsic dimensionality of hidden state representations. Here, the intrinsic dimension is measured\nas the rank of an information matrix, and not from the data space as in our work. In Chang et al.\n(2022), the representational geometry of multilingual language models is explored, focusing on how\nthey balance encoding language-specific and language-agnostic features within a shared multilingual\nspace. Ruppik et al. (2024) utilize topological features derived from neighborhoods in a contextual\nembedding space to improve performance in a sequence tagging task. The definition of their features\n2\n--- Page 3 ---\nrequires a fixed ambient data corpus, whereas our methods give an intrinsic measure of the dataset\nunder consideration; thus, our measures are comparable between different model checkpoints of the\nfeature generation model. In contrast to mechanistic interpretability (see, for example, the survey\n(Ferrando et al., 2024)), in which one tries to explain how a language model’s internal representations\nfit together into higher-level abstractions, our approach is more atomic. We study the smallest\nnon-divisible representation in the model (the contextual embedding of single tokens) and the space\ncreated by collections of these.\n3 Methods\n3.1 Large Language Models\nModern contextual language models are typically either masked language models (MLMs) (Devlin\net al., 2019; Liu et al., 2019) or autoregressive language models (ALMs) (Radford et al., 2019). MLMs\npredict masked tokens based on bidirectional context, while ALMs generate tokens sequentially,\nconditioned on preceding tokens. In both cases, the representation of a token in context is given by a\nvector at each model layer, enabling geometric analysis of a data corpus on a layer-wise basis. We\nnow give an overview of this construction; the whole procedure is summarized in algorithm 1.\n3.2 Latent Space Modeling\nLetD= (s0, s1, . . . , s D)be a text corpus and Ma language model (MLM or ALM) of depth lwith\ntokenizer T. Each sequence smyields tokenized input:\nT(sm) =\u0000\ntm\n0, . . . , tm\nnm\u0001\n, (1)\nwith corresponding contextualized token embeddings at layer i:\nMi(sm) =\u0000\nMi(tm\n0), . . . ,Mi(tm\nnm)\u0001\n. (2)\nWe distinguish between regular embeddings (tokens embedded as-is during a forward-pass) and\nmasked embeddings (specific tokens replaced by [MASK] , requiring the model to infer their representa-\ntions from the surrounding context). Despite the simplified notation, embeddings are context-sensitive.\nThe token embedding point cloud at layer iis:\nTi={Mi(tm\nj)}m=0,...,D ;j∈Im, (3)\nwithImdenoting token indices in the m-th sequence. Distances between points are measured in the\nambient Euclidean space and are assumed to be Euclidean unless stated otherwise. In practice, Ti\ncan contain millions of vectors, making subsequent neighborhood computation on the full dataset\ninfeasible.\nIn order to obtain a representative subset of tokens T, we take a two-step sampling approach: We first\nsample Msequences from D, and after de-duplication, we sample Nvectors from the resulting token\nembedding space. Finally, we compute neighborhoods NL(tj;T)for each token using a locality\nparameter L. We assume the dataset is drawn from an underlying text-generating distribution, and\nour sampling approximates its geometric structure. We confirm this assumption in our sensitivity\nanalysis in Appendix A.\n3.3 Comparing Latent Spaces Across Models\nWe compare a base model Mwith its fine-tuned variant M∆, trained on a corpus ∆. Since both\nmodels share the same architecture and tokenizer, there exists a canonical bijection between Mi(D)\nandM∆\ni(D), mapping each token’s representation in the base model to its counterpart in the fine-\ntuned model. More generally, such a bijection arises whenever the underlying model architectures\nand tokenizers are identical. After identical sub-sampling, this allows for point-wise comparison of\nthe resulting latent spaces to evaluate geometric changes. Next, we define the local estimates used for\nthis comparison.\n3.4 Local Dimension Estimates\nThe TwoNN estimator (Facco et al., 2017) approximates the intrinsic dimension of a point cloud\nas a positive real number using only the distances r1andr2of each point to its nearest and second-\nnearest neighbors. It turns out that under weak assumptions, the ratior2\nr1of these distances is\n3\n--- Page 4 ---\nAlgorithm 1: COMPUTE LOCAL DIMENSION ESTIMATES\nInput : Text corpus D;\nEmbedding model M(with corresponding tokenizer T);\nDimension estimator dim (default dim = TwoNN );\nThree parameters:\n– Size of text corpus sequence sub-sample M∈N;\n– Size of token sub-sample N∈N;\n– Local neighborhood size L∈N.\nOutput : Token-level local estimates given as a vector in RN\n≥0, i.e., one non-negative number for\neach of the sub-sampled tokens.\n1Take a random sub-sample D∼Mof size Mof the dataset sequences ;\n2Compute embeddings for each token in each sequence of D∼M;\n3De-duplicate the embedding vectors ;\n4Take token vector sub-sample of size N, call the resulting set of token vectors T;\n5foralltj∈Tdo\n6 Compute the Lnearest neighbors of tjinT, resulting in the neighborhood NL(tj;T);\n7 Compute the local estimate dim(NL(tj;T))∈R≥0of this neighborhood ;\n8return The vector of token-level dimension estimates.\nPareto-distributed (see Denti et al. (2021) for details). Then, the intrinsic dimension can be estimated\nfrom parameters describing this distribution, as long as the density around the points is locally\napproximately constant on the scale defined by the distance of the second nearest neighbor.\nWe turn this into a local measure as follows: Let Xbe a point cloud and v∈X. ForL∈N>0, we\ndefine the L-local TwoNN estimate of XatvasTwoNN( NL(v;X)), i.e. as the TwoNN estimate\nof the local neighborhood NL(v;X)ofvof size LinX. In this way, the parameter Lcontrols\nthe locality scale of the dimension estimation. In the context of our model Mand corpus D, we\ntakeX=Tand compute a vector of positive real-valued numbers TwoNN( NL(v;X))v∈T∈RN\n≥0,\none for each token v∈T. We subsequently aggregate this to a mean estimate per corpus/model\nsetup. While sensitive to hyperparameters such as Land sample sizes, we show in Appendix A that\nestimates remain stable under reasonable settings, enabling consistent comparisons across datasets\nand models.\n4 Experiments\nWe now apply our method to analyze how embedding spaces evolve during LLM-related learning\ntasks. Unless stated otherwise, all results pertain to the model’s final hidden layer, i.e., T−1as\ndefined in Section 3. We refer the reader to Appendix C.2 for results on layers other than the last.\nSubsequently, we will focus on four central questions:\n(Q1) How does fine-tuning on different datasets alter latent space geometry?\n(Q2) How can local dimension estimates predict grokking ?\n(Q3) How can local dimension estimates detect the limit of training capabilities ?\n(Q4) How can local dimension estimates detect overfitting ?\n4.1 Fine-Tuning Induces Dataset-Specific Shifts in Heterogeneous Local Dimensions\nTo understand how fine-tuning alters model representations, we investigate the distribution of local\nintrinsic dimensions across token embeddings. These dimensions, estimated using the method\nintroduced in Section 3, reveal nuanced changes in the geometry of the embedding space.\nSetup We evaluate models on datasets with varying domain and familiarity to the base and\nfine-tuned models: MultiWOZ2.1 (Eric et al., 2020): Human-human multi-domain dialogues,\nSchema-Guided Dialogue (SGD) (Rastogi et al., 2020): Human–virtual assistant dialogues,\nReddit : 2022 Reddit comments mentioning Tesla, Inc., and Wikipedia : The Hugging Face\nwikitext-103-v1 corpus. As can be seen from the pre-training datasets of the publications that\n4\n--- Page 5 ---\nintroduced the respective models (Liu et al., 2019; Radford et al., 2019), this selection allows us to\ncompare: (i) distributions not seen during pre-training or fine-tuning (e.g., Reddit ), (ii) distributions\nseen during pre-training ( Wikipedia ), and (iii) distributions used for fine-tuning (e.g., MultiWOZ ,\nSGD). We split Reddit andWikipedia into training (80%), validation (10%), and test (10%) subsets.\nChronologically-ordered datasets (e.g., Reddit ) are shuffled before splitting to avoid temporal bias.\nWikipedia was pre-processed by removing headings, empty lines, and stripping leading and trailing\nwhitespace. Further dataset statistics are available in Table 1.\nFine-tuning of the RoBERTa-base models (Liu et al., 2019) is performed using masked language\nmodeling with a masking probability of 0.15. Each model is trained for 5 epochs on 10 000 training\nexamples using a batch size of 8, a learning rate peaking at 5·10−5with 500 warmup steps, and\nlinear decay thereafter. Weight decay of 0.01is applied throughout. For evaluation, we embed a\nsubset of the validation split of each dataset using both the base and fine-tuned model, ensuring the\nembeddings are out-of-sample relative to the fine-tuning set. Results for fine-tuning an autoregressive\nmodel can be found in Appendix C.1.\nRoBERT a RoBERT a fine-tuned on MultiWOZ05101520T woNNMean=9.09; Median=9.37; Std=3.01\nMean=6.67; Median=6.96; Std=2.79\n(a) TwoNN estimates on MultiWOZ validation.\nRoBERT a RoBERT a fine-tuned on MultiWOZ10203040T woNNMean=17.56; Median=17.37; Std=4.67\nMean=18.09; Median=17.85; Std=5.12\n(b) TwoNN estimates on Wikipedia validation.\nRoBERT a RoBERT a fine-tuned on MultiWOZ0102030T woNNMean=13.27; Median=13.08; Std=5.24\nMean=12.80; Median=12.58; Std=5.03\n(c) TwoNN estimates on Reddit validation.\nFigure 1: Comparison of the intrinsic dimension\n(ID) across three data modalities for both mod-\nels. The distribution of the local estimates over\ntokens is shown in the violin plot, together with\ntheir means and quartiles. The ID of embed-\ndings originating from the fine -tuning distribu-\ntion ( MultiWOZ ) differs markedly between mod-\nels, whereas the IDs for the out -of-distribution\ncorpora ( Wikipedia ,Reddit ) are almost indis-\ntinguishable.Results We find that local intrinsic dimensions\nvary significantly across tokens, as illustrated by\nthe wide spread of the TwoNN distributions in\nFigure 1. Here, we choose the parameters M=\n7000 ,N= 60 000 ,L= 128 . This heterogeneity\nis consistent across different models and datasets.\nIt aligns with prior observations that numerical\ndata is often not confined to a single manifold of\nuniform dimensionality, but is instead composed\nof multiple regions with varying local geometry\n(Brown et al., 2023). These findings reinforce our\ndecision to favor local over global estimates of\nembedding dimensionality.\nOur central observation is that fine-tuning sys-\ntematically lowers the local intrinsic dimension\nonly on the dataset used during the fine-tuning\nprocess. As shown in Figure 1a, the TwoNN\nestimates for the MultiWOZ -validation embed-\ndings are markedly lower for the fine-tuned\nRoBERTa model than for the base model. By con-\ntrast, embeddings of unrelated datasets—such as\nWikipedia (Figure 1b) or Reddit comments (Fig-\nure 1c)—show no or only minimal changes in their\nlocal dimension distributions. Quantitatively, this\nobservation can be validated by the standardized\nmean difference between the respective cohorts,\nwith a value of 1.19forMultiWOZ , and0.08(resp.\n0.1) for Reddit (resp. Wikipedia ).\nWe find this shift to be stable across training, vali-\ndation, and test splits of the same dataset, suggest-\ning that it reflects robust structural changes in the\nlatent space rather than artifacts (see Appendix A\nfor a thorough sensitivity analysis showing the\nmethod’s robustness in our setting). This behavior\nhighlights strong dataset-specificity in how local\ngeometry adapts under fine-tuning. The effect is absent when evaluating on out-of-distribution data,\nsupporting the view that local dimension reductions correspond to improved fit and specialization on\nthe fine-tuned task, while leaving unrelated regions of the embedding space unchanged.\n4.2 Local Dimensions Predict Grokking\nGrokking is the phenomenon that a machine learning model acquires specific skills only after extended\ntraining far beyond overfitting on the training set, which was first discovered by Power et al. (2022).\n5\n--- Page 6 ---\nIn such cases, from the model’s performance on the training set alone, it is a complex problem to\npredict under which choices of hyperparameters grokking will occur (Junior et al., 2024).\nSetup We here consider the task of learning an arithmetic operation on a small group (addition\nmodulo p). For example, for p= 197 such an input sequence would be [155, ‘o’, 88, ‘=’] ,\nwhere the model should predict the result 46. The tokenization is constructed so that every operand in\nthe group is encoded as a single token, with the addition of the operation token ‘o’and the equality\ntoken ‘=’. A certain fraction of all valid expressions in the group, ranging from 10 percent to 50\npercent, is taken as the training set, and the remainder will be taken as the validation set.\nWe use a tiny decoder-only transformer model with two layers, 128 hidden dimensions, and four\nattention heads trained from scratch. Optimization is performed by AdamW (Loshchilov and Hutter,\n2017) with learning rate 0.001(with linear schedule over 400k steps, warmup of 10 steps), batch\nsize 512, weight decay 0.01. Models are trained for many steps beyond the point where training\nperformance saturates, keeping the same setup as in the original paper (Power et al., 2022).\nFor the local estimates, we set the number of sampled tokens Nas the minimum of 3000 and the\nmaximum number of tokens available, and a neighborhood size Lof 64. Since the number of input\nsequences is comparatively small, we choose Mto encompass everything, and only subsample in\nthe token selection step. Still, we observe a similar qualitative behavior for other reasonable choices\nof these hyperparameters. The hidden states at the last layer are sampled from all available tokens,\nwhich could come from the operands and the operation or equality symbol.\n0 10000 20000 30000 40000 50000 60000\nStep0.00.20.40.60.81.0Training AccuracyTraining fraction\n0.10\n0.15\n0.20\n0.25\n0.300.35\n0.40\n0.45\n0.50\n(a)Training accuracy\n0 10000 20000 30000 40000 50000 60000\nStep0.00.20.40.60.81.0Validation Accuracy (b) Validation accuracy\n0 10000 20000 30000 40000 50000 60000\nStep5.07.510.012.515.017.520.0Mean local dimension (c)Training mean local estimates\nFigure 2: Training a model on addition mod p= 197 with different training data fraction selected\nfrom{0.1; 0.15; 0.2; 0.25; 0.3; 0.4; 0.5}. The plots show the development for 60 000 batches, with\nmean and 95% confidence interval over 5training seeds per configuration (plots per seed are in\nAppendix C.3). The mean local estimates are computed on the training split for the parameters\nN= 3000 ;L= 64 . Dashed lines highlight the runs where grokking did notoccur.\nResults Performance measures versus local estimates are shown in Figure 2 for the first 60 000\ntraining steps averaged over seeds (we present plots for individual seeds in Figure 15 in the appendix).\nThe training accuracy quickly reaches an almost perfect score in all cases, with the validation accuracy\nlagging behind. In the time frame under consideration, the model can generalize and reach almost\nperfect validation accuracy only in those runs for which the training data portion exceeds 20%.\nBased on the loss and accuracy of the training data alone, one would not be able to predict which\nconfigurations can break out and generalize to the validation data.\nA typical pattern for the mean local dimension computed on the training data in all these runs is that\nit increases in the first few thousand global steps (see Figure 2c). But subsequently, the training mean\nlocal dimension starts dropping significantly for those runs that exhibit grokking. Observe that the\ntiming of this drop coincides with the start of the increasing validation accuracy in Figure 2b. We can\nconclude that in this setting, a drop in the mean local dimension on the training set strongly indicates\na successful generalization to unseen validation data.\nFor those two runs in which the validation accuracy does not increase beyond the fraction of training\ndata during the selected training time (with 10% and 15% of training data, highlighted via dashed\nlines), the local dimension increases and then stays mostly flat. This behavior might hint that the\nmodel only tries to learn the training examples by heart and fails to generalize to unseen data.\n4.3 Local Dimensions Detect Exhaustion of Training Capabilities\nFinding relevant text segments in response to a query, also called span prediction orsequence tagging ,\nis a highly relevant problem in various natural language processing settings (Jurafsky and Martin,\n6\n--- Page 7 ---\n2025). One such application is retrieval augmented generation (RAG), where, during the information\nretrieval step, relevant passages from a large corpus need to be selected (Fan et al., 2024) before\ngenerating an answer. Here we study it in the context of dialogue state tracking—a critical task of\ndialogue modeling.\nSetup To demonstrate the application of our local dimension estimates in this setting, we compute\nit for the sequence-tagging-based dialogue state tracking model TripPy-R (Heck et al., 2022) trained\non the MultiWOZ2.1 dataset (Eric et al., 2020). In dialogue state tracking, the task is to predict\nthe user’s intent from the natural language input utterances, and keep track of the user’s goal\ndescribed throughout the conversation by updating the dialogue state (Young et al., 2010). The\ncontextualized hidden states for computing our measure are taken from the last layer of the unified\nencoder component, which has the RoBERTa architecture. The actual state tracking is performed\nbased on classifier outputs derived from this encoder’s output. The encoder is fine-tuned during the\nTripPy-R training via the loss signal derived from predicting the current dialogue turn’s state update.\nWe train the models for 20 epochs with Adam, with a linear learning rate schedule up to 5·10−5that\nstarts with one warm-up epoch.\nThe input data for the local dimension estimates is formatted in the same way as it is used during\ntraining and inference. A single input sequence consists of a dialogue turn, followed by the dialogue\nhistory, where the different sequence components are separated via special tokens. For the local\ndimension estimates, we sample M= 7000 sequences from the training, validation, and test split,\nwith a token subsample size N= 60 000 , and neighborhood size L= 128 .\n5000 15000 25000 35000\nSteps46810Mean local dimension\n0.10.30.50.7\nEvaluation measures\n(a)MultiWOZ Training split\n5000 15000 25000 35000\nSteps46810Mean local dimension\n00.20.40.6\nEvaluation measures\n (b)MultiWOZ Validation split\n5000 15000 25000 35000\nSteps46810Mean local dimension\nMean local dim.\nJoint Goal Accuracy\nLoss\n00.20.40.6\nEvaluation measures\n (c)MultiWOZ Test split\nFigure 3: Development of Trippy-R performance measures (model loss in green; joint goal accuracy\nin orange) compared with mean local dimension estimates (dark blue) evaluated on the training,\nvalidation, and test split of the MultiWOZ dataset. We show the mean and standard deviation of the\nmeasures evaluated at the end of each epoch over six different model training seeds.\nResults Figure 3 shows the development of two model performance measures: One is the differen-\ntiable model loss, which is used for backpropagation. The other is the non-differentiable joint-goal\naccuracy (JGA), which is used to judge the downstream performance of the state tracking model.\nNote that the mean local dimension estimates behave similarly when comparing the training, valida-\ntion, and test splits. This is in contrast to the loss and JGA: On the training set, the loss (green) is\nmonotonically decreasing, while the JGA (orange) is increasing. On the other hand, the loss reaches\na minimum on the validation set after 7500 batches, long before the JGA converges. In this case, the\ndifferentiable model validation loss would give the wrong impression that the model has finished\nlearning generalizable representations. However, from the dimension estimate, we conclude that the\nmodel has not converged at that point. The mean local dimension is still decreasing, and one should\ncontinue training. Notably, we can make this observation on the training data alone, and would not\nneed a separate labeled validation set when relying upon our estimates.\nIn the other direction, we can see that later in the training, after roughly 25 000 batches, the stabi-\nlization of the local dimension estimates coincides with the convergence of the model performance\nregarding JGA on the validation and test sets. This effect of stabilization of the local estimates\npredicting that the training capabilities have been exhausted can be seen even more clearly for longer\ntraining runs (50 epochs) in Figure 16 in Appendix C.4.\n4.4 Local Dimensions Detect Overfitting\nWe now examine the mean local dimension as an indicator of overfitting in a challenging sequence\nclassification setting.\n7\n--- Page 8 ---\nSetup We use the EmoWOZ dataset (Feng et al., 2022), with the task of classifying dialogue utterances\ninto one of seven emotion classes. Our experiments employ the ERToD model (Feng et al., 2023)\nin its baseline configuration, a linear classifier built upon a bert-base-uncased (Devlin et al.,\n2019) model feature extractor. We intentionally decide on the setup without dialogue history, data\naugmentation, or auxiliary predictions, which are modifications proposed by the authors to solve the\nproblem of classifying rare emotions. The model is trained for 8 epochs using a linear learning rate\nschedule with warmup for AdamW and peak learning rate of 2·10−5. At the end of each training\nepoch, local dimension estimates are computed from hidden states extracted at the last BERT-encoder\nlayer. Thus, this is the latent space that the model has tuned to aid its subsequent classification. We\nselect representative subsets of M= 7000 sequences from the training, validation, and test split,\nwith a token subsample size N= 60 000 , and neighborhood size L= 128 . Model performance is\nevaluated using cross-entropy loss and weighted F1 and macro F1 classification scores. To better\ncapture effects on minority classes, the majority “neutral” class is excluded from the F1-scores.\n1 3 5 7\nEpoch0246810Mean local dimension\n0.20.40.60.8\nEvaluation measures\nMean local dim.\nWeighted F1\nLoss\nMacro F1\n(a)EmoWOZ Training split\n1 3 5 7\nEpoch0246810Mean local dimension\n0.30.40.50.60.70.8\nEvaluation measures\n (b)EmoWOZ Validation split\n1 3 5 7\nEpoch0246810Mean local dimension\n0.30.40.50.60.70.8\nEvaluation measures\n (c)EmoWOZ Test split\nFigure 4: Development of emotion recognition model performance measures (loss in green; weighted\nF1 in orange; macro F1 in red) compared with mean local dimension estimates (dark blue) evaluated\non the training, validation, and test split of the EmoWOZ dataset. We show the mean and standard\ndeviation of the measures evaluated at the end of each epoch over four different training seeds.\nResults Figure 4 depicts the model’s performance measures plotted against the mean local estimates.\nAfter the first training epoch, the model’s mean local dimension drops notably to ≈7.25from the\nbase model’s initial dimension of ≈9.94. Over the subsequent eight training epochs, this dimension\nincreases to a value of about 8. Note that this behavior of the mean local estimates can be observed\non all three data splits. As expected, the training loss monotonically decreases, while both weighted\nand macro F1-scores increase on the train set.\nHowever, the minimum of the local dimension, followed by an increase, suggests that the model has\nalready found an efficient representation after a single training epoch. We previously observed a\nconnection between rising dimension and memorization in the grokking experiments in Section 4.2.\nThis suspicion is confirmed when considering the performance measures on the validation set in\nFigure 4b. After the first epoch, the validation loss increases strongly over the following epochs; thus,\nthe model is clearly overfitting on the training examples.\nIn terms of how this influences the classification accuracy, the effect is more nuanced. While the\nmacro F1-score increases and plateaus after about three epochs, the weighted F1-score declines\nfrom the end of epoch two onward. This combined behavior implies that the model improves on\nminority classes at the expense of performance on the majority classes, potentially memorizing rare\ninstances and diminishing its generalization capability. This interplay between macro and weighted\nF1 suggests that the model is learning a decision boundary that trades off the performance of majority\nand minority classes, a phenomenon that may be overlooked when looking at the loss curve alone.\nPractitioners monitoring the shifts of local dimensionality on the training set could, therefore, identify\nsubtler model behavior without requiring labeled validation or test data.\n4.5 Computational Resources\nFine-tuning of the RoBERTa-base and GPT-2-medium models (in Appendix C.1) on one of our\nselected datasets can be performed efficiently on a single NVIDIA V100 or GTX1080TI-12GB GPU\nwithin a few hours. Additionally, computing the embeddings for a single layer of these models\nrequires only forward passes, which takes approximately 10 minutes on the same hardware.\nFor the TwoNN computations, which are CPU-intensive, the computational requirements depend\non the size of the dataset and the dimensionality of the embeddings. The computation is feasible in\n20 minutes on an E5-2640v4 (Broadwell) 2.40GHz dual-core machine with 32GB of RAM using\n8\n--- Page 9 ---\nthescikit-dimension package (Bac et al., 2021) for a typical dataset with tens of thousands of\npoints in high-dimensional space (ambient dimension in the hundreds). For the grokking experiments,\nwhere we evaluate the local dimension frequently, this amounts to a total of 48 CPU core hours\nper run in the depicted range. However, precise timing may vary depending on implementation\noptimizations and the chosen neighborhood sizes. For a constant and small neighborhood size L\nand ambient embedding dimension d, the computation of the neighborhoods for a subsample of N\nusing exact search (Johnson et al., 2019) takes O(dN2). Then, since we already have the ratios\nof closest distances available for each neighborhood, the TwoNN local dimension estimation is a\nconstant overhead of performing a linear fit on values of a transformed cumulative distribution.\n4.6 Limitations\nWhile versatile, our framework also has several limitations. One drawback is its comparatively high\ncomputational complexity. Hence, to get a reliable dimension estimate, we need to build a sufficiently\nlarge sub-sample of the latent space by performing forward passes on the input data and subsequently\ncomputing the neighborhoods and estimates. Being independent of the model training procedure,\nthese computations can be performed in parallel to any potential fine-tuning, thus not slowing down\nthe training process. Further work might involve making the local estimate computation more efficient\nand developing local descriptors that share the stability properties but are computationally faster to\nevaluate. An intrinsic limitation arises from the assumptions of our dimension estimates. The TwoNN\nestimate, which we locally apply in the neighborhoods, is known to accurately reflect the true local\ndimension only under strong assumptions on the local point sampling distribution. (The point sample\nshould come from a Poisson point process on a subspace of uniform dimension.) However, this is\nonly of secondary importance for our method, as we are more interested in changes of dimension\nthan in absolute values. In addition, our framework is general and permits the use of other estimation\nmethods. Finally, while the TwoNN method is non-parametric, the precise absolute value of the mean\nTwoNN estimate depends on the hyperparameter choices in selecting the latent space sub-sample\nand neighborhood size. This implies that our mean of local dimensions is of a “relative” nature,\nbecause we cannot directly compare the dimensions’ values between different model architectures.\nFor fine-tuning runs, one should instead look at the relative change compared to the starting value.\n5 Conclusion\nWe introduce a novel geometric perspective on LLM training dynamics by measuring the local\nintrinsic dimension of contextual token embeddings. While the latent spaces of contextual language\nmodels exhibit regional variation in dimensionality, the mean local dimension provides a stable,\ninterpretable summary across dataset splits. Across diverse tasks, a sustained drop in mean local\ndimension reliably predicts improved generalization, offering a practical, unsupervised diagnostic\nsignal that, among other things, enables the prediction of grokking. Such a marked reduction in\nintrinsic dimension in supervised downstream tasks highlights their strong compressive effect on the\nlatent space. On the other hand, rising dimensions can point to a tradeoff between generalization\nand specialization, typically observed during overfitting. Our approach enables monitoring without\nsupervision by labels, which is particularly valuable in low-resource settings where validation labels\nare limited or unavailable. Thus, our findings highlight the utility of geometric descriptors for\nmonitoring and interpreting LLM behavior beyond traditional label-based metrics.\nImpact and Future Work To the best of our knowledge, we are the first to investigate local\ndimensional shifts and their implications in training and fine-tuning contextual language models for\ndifferent downstream tasks. The findings of this work enhance the ongoing discussion regarding\nthe interpretability, adaptability, and generalizability of LLMs. Our work provides a foundation for\ndesigning better model architectures and developing interventions that utilize the insight that lower\nintrinsic dimensions benefit machine learning problems.\nThe locality of the dimension estimates promises further applications beyond the scope of the current\nwork. One immediate avenue for investigation is to see to what extent the dimension can be used\nto detect on which data the model has been trained. Another important direction would be to find\nconnections between the underlying meaning that the tokens carry and their corresponding dimension\nestimates.\n9\n--- Page 10 ---\nAcknowledgments and Disclosure of Funding\nThis work was made possible through the support of the European Research Council (ERC) under\nthe Horizon 2020 research and innovation program (Grant No. STG2018 804636), the Ministry\nof Culture and Science of North Rhine-Westphalia within the Lamarr Fellow Network, and the\nSwiss State Secretariat for Education, Research, and Innovation (SERI). Computational resources\nwere provided by the Centre for Information and Media Technology at Heinrich Heine University\nDüsseldorf, and Google Cloud.\nReferences\nAghajanyan, A., Gupta, S., and Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the\nEffectiveness of Language Model Fine-Tuning. In Zong, C., Xia, F., Li, W., and Navigli, R.,\neditors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers) , pages 7319–7328, Online. Association for Computational Linguistics.\nBac, J., Mirkes, E. M., Gorban, A. N., Tyukin, I., and Zinovyev, A. (2021). Scikit-Dimension: A\nPython Package for Intrinsic Dimension Estimation. Entropy , 23(10).\nBrown, B. C., Caterini, A. L., Ross, B. L., Cresswell, J. C., and Loaiza-Ganem, G. (2023). Verifying\nthe Union of Manifolds Hypothesis for Image Data. In The Eleventh International Conference on\nLearning Representations (ICLR) .\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural\ninformation processing systems , 33:1877–1901.\nCai, X., Huang, J., Bian, Y ., and Church, K. (2021). Isotropy in the Contextual Embedding Space:\nClusters and Manifolds. In International Conference on Learning Representations (ICLR) .\nChang, T., Tu, Z., and Bergen, B. (2022). The Geometry of Multilingual Language Model Represen-\ntations. In Goldberg, Y ., Kozareva, Z., and Zhang, Y ., editors, Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing , pages 119–136, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nDenti, F., Doimo, D., Laio, A., and Mira, A. (2021). Distributional results for model-based intrinsic\ndimension estimators. arXiv preprint arXiv:2104.13832 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding. In Burstein, J., Doran, C., and Solorio, T.,\neditors, Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nDurrani, N., Sajjad, H., Dalvi, F., and Alam, F. (2022). On the Transformation of Latent Space in\nFine-Tuned NLP Models. In Goldberg, Y ., Kozareva, Z., and Zhang, Y ., editors, Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing , pages 1495–1516,\nAbu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nEd-dib, A., Datbayev, Z., and Aboussalah, A. M. (2024). GeLoRA: Geometric Adaptive Ranks For\nEfficient LoRA Fine-tuning. arXiv preprint arXiv:2412.09250 .\nEric, M., Goel, R., Paul, S., Sethi, A., Agarwal, S., Gao, S., Kumar, A., Goyal, A., Ku, P., and\nHakkani-Tur, D. (2020). MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with\nState Corrections and State Tracking Baselines. In Calzolari, N., Béchet, F., Blache, P., Choukri,\nK., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A.,\nOdijk, J., and Piperidis, S., editors, Proceedings of the Twelfth Language Resources and Evaluation\nConference , pages 422–428, Marseille, France. European Language Resources Association.\n10\n--- Page 11 ---\nEthayarajh, K. (2019). How Contextual are Contextualized Word Representations? Comparing the\nGeometry of BERT, ELMo, and GPT-2 Embeddings. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , pages 55–65, Hong Kong, China. Association\nfor Computational Linguistics.\nFacco, E., d’Errico, M., Rodriguez, A., and Laio, A. (2017). Estimating the intrinsic dimension of\ndatasets by a minimal neighborhood information. Scientific reports , 7(1):12140.\nFan, W., Ding, Y ., Ning, L., Wang, S., Li, H., Yin, D., Chua, T.-S., and Li, Q. (2024). A Survey on\nRAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. In Proceedings of\nthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , KDD ’24, page\n6491–6501, New York, NY , USA. Association for Computing Machinery.\nFeng, S., Lubis, N., Geishauser, C., Lin, H.-c., Heck, M., van Niekerk, C., and Gasic, M. (2022).\nEmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented\nDialogue Systems. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T.,\nGoggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors,\nProceedings of the Thirteenth Language Resources and Evaluation Conference , pages 4096–4113,\nMarseille, France. European Language Resources Association.\nFeng, S., Lubis, N., Ruppik, B., Geishauser, C., Heck, M., Lin, H.-c., van Niekerk, C., Vukovic, R.,\nand Gasic, M. (2023). From Chatter to Matter: Addressing Critical Steps of Emotion Recognition\nLearning in Task-oriented Dialogue. In Stoyanchev, S., Joty, S., Schlangen, D., Dusek, O.,\nKennington, C., and Alikhani, M., editors, Proceedings of the 24th Annual Meeting of the Special\nInterest Group on Discourse and Dialogue , pages 85–103, Prague, Czechia. Association for\nComputational Linguistics.\nFerrando, J., Sarti, G., Bisazza, A., and Costa-jussà, M. R. (2024). A Primer on the Inner Workings\nof Transformer-based Language Models. arXiv preprint arXiv:2405.00208 .\nHeck, M., Lubis, N., van Niekerk, C., Feng, S., Geishauser, C., Lin, H.-C., and Gaši ´c, M. (2022).\nRobust Dialogue State Tracking with Weak Supervision and Sparse Data. Transactions of the\nAssociation for Computational Linguistics , 10:1175–1192.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F.,\nLengyel, G., Lample, G., Saulnier, L., et al. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825 .\nJohnson, J., Douze, M., and Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE\nTransactions on Big Data , 7(3):535–547.\nJunior, T. N. P., Zhou, H., Pezeshki, M., Rish, I., and Dumas, G. (2024). Predicting Grokking Long\nBefore it Happens: A look into the loss landscape of models which grok. In ICLR 2024 Workshop\non Mathematical and Empirical Understanding of Foundation Models .\nJurafsky, D. and Martin, J. H. (2025). Speech and Language Processing: An Introduction to Natural\nLanguage Processing, Computational Linguistics, and Speech Recognition with Language Models .\nOnline manuscript, 3rd edition. Online manuscript released January 12, 2025.\nKushnareva, L., Cherniavskii, D., Mikhailov, V ., Artemova, E., Barannikov, S., Bernstein, A.,\nPiontkovskaya, I., Piontkovski, D., and Burnaev, E. (2021). Artificial Text Detection via Examining\nthe Topology of Attention Maps. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages 635–649, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and\nStoyanov, V . (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv\npreprint arXiv:1907.11692 .\nLoshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 .\nPerez, I. and Reinauer, R. (2022). The topological BERT: Transforming attention into topology for\nnatural language processing. arXiv preprint arXiv:2206.15195 .\n11\n--- Page 12 ---\nPower, A., Burda, Y ., Edwards, H., Babuschkin, I., and Misra, V . (2022). Grokking: Generalization\nbeyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 .\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving Language Under-\nstanding by Generative Pre-Training. OpenAI blog .\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models\nare unsupervised multitask learners. OpenAI blog .\nRastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khaitan, P. (2020). Towards scalable multi-domain\nconversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference\non artificial intelligence , volume 34.05, pages 8689–8696.\nRuppik, B. M., Heck, M., van Niekerk, C., Vukovic, R., Lin, H.-c., Feng, S., Zibrowius, M., and\nGasic, M. (2024). Local Topology Measures of Contextual Language Model Latent Spaces with\nApplications to Dialogue Term Extraction. In Kawahara, T., Demberg, V ., Ultes, S., Inoue, K.,\nMehri, S., Howcroft, D., and Komatani, K., editors, Proceedings of the 25th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue , pages 344–356, Kyoto, Japan. Association for\nComputational Linguistics.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,\nN., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971 .\nTulchinskii, E., Kuznetsov, K., Kushnareva, L., Cherniavskii, D., Nikolenko, S., Burnaev, E., Baran-\nnikov, S., and Piontkovskaya, I. (2024). Intrinsic Dimension Estimation for Robust Detection of\nAI-Generated Texts. Advances in Neural Information Processing Systems , 36.\nValeriani, L., Doimo, D., Cuturello, F., Laio, A., Ansuini, A., and Cazzaniga, A. (2024). The geometry\nof hidden representations of large transformer models. Advances in Neural Information Processing\nSystems , 36.\nViswanathan, K., Gardinazzi, Y ., Panerai, G., Cazzaniga, A., and Biagetti, M. (2025). The Geometry\nof Tokens in Internal Representations of Large Language Models. arXiv preprint arXiv:2501.10573 .\nYoung, S., Gaši ´c, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., and Yu, K. (2010).\nThe hidden information state model: A practical framework for POMDP-based spoken dialogue\nmanagement. Computer Speech & Language , 24(2):150–174.\nA Sensitivity Analysis\nThe following analysis gives insights into the stability of the sampling process in our pipeline of\nalgorithm 1. In particular, we discuss the influence of the following variations on our final mean local\nestimate:\n•In Appendix A.1, we show that with increasing sample and neighborhood sizes, the TwoNN\nestimates tend to converge in our setting, and already a small portion of samples suffices for\nreasonable reliability.\n•In Appendix A.2 we show that the mean local estimates are stable under applying noise to\nthe embedding vectors.\n•In Appendix A.3, we show that for masked language models, both masked and regular token\nembeddings lead to similar mean estimates.\nAll embeddings in the following analyses are computed using the regular token embeddings from the\nlast layer of the RoBERTa base model.\n12\n--- Page 13 ---\nA.1 Dependence on hyperparameter choices\nDataset sequence sub-sampling (dependence on M)Text corpora data splits usually contain\ndifferent numbers of text sequences, since, for example, the training dataset is larger than the\nvalidation or test dataset. Here, we present experiments to show that our method can be made stable in\nthe size of text corpus sequence sub-sample M∈N, and how it depends on the seed of the sequence\nsub-sample.\nNote that in our sub-sampling, we shuffle the dataset split using a given seed, and then truncate\nto a beginning segment of the specified size. Thus, when comparing different data sequence sub-\nsampling sizes for a fixed seed, the smaller sequence sub-samples are subsets of the larger ones. In\nall the experiments in this section, the size of token sub-sample is set to N= 60 000 , and the local\nneighborhood size parameter L= 128 . Note that for the smallest sequence sub-sample size M=\n2000 , not enough non-padding tokens are necessarily available to reach a token sample size of N=\n60 000 , but with more sequences, we get this common token space size.\nWe take sequence sub-samples of each given size with 5 different seeds and select a random token\nsub-sample for each of these, on which the local TwoNN estimates are based. For the results, see\nFigure 5 and Figure 6.\nFor a given sequence sample size, the average of the mean local estimates, calculated across different\nsampling seeds, remains consistent between the various dataset splits of the same dataset. This\nobservation indicates that the sampling process is robust to differences in how the data is divided.\nFurthermore, the estimates stabilize as the sequence sample size increases, suggesting convergence\nof the local mean estimates with larger samples. Based on these observations, we set Mto the values\n7000 or10 000 for the natural language-based tasks in Sections 4.1, 4.3 and 4.4, because it represents\na practical compromise between computation effort and stability of the value at that scale. This value\nprovides sufficient sampling density to capture the underlying data distribution comparably across\ndatasets while maintaining computational efficiency.\nToken sub-sampling (dependence on N)Here, we want to investigate the influence of the token\nsub-sample size on the local TwoNN estimate. We conduct our analysis on the MultiWOZ dataset.\nSince here we are interested in the effect of the second sub-sampling process, we want to fix the first\nsampling step for the sequences: We take the sub-sample of the first M= 10 000 sequences from the\nMultiWOZ training, validation, and test split. From the resulting token embedding spaces, we then\nsample 100 000 points (ignoring padding and end-of-sequence (EOS) tokens). Finally, we compute\nthe local estimates based on the first N= 10 000 ,20 000 , . . . , 90 000 ,100 000 points in the given\nsub-sample. Note that we are left with slightly fewer points in the local estimation step for the largest\nsub-sample size because of our de-duplication step in algorithm 1. This means that in a given token\nsub-sample, the beginning segment of the first 10 000 points is the same even when increasing the\nsample size, and we compare the mean of the local estimates of this beginning segment.\nThe analysis of the datasets reveals a clear trend in the behavior of the truncated mean values and their\nassociated standard deviations as the sample size increases, see Figure 7. Here, the mean stabilizes\nquickly, and the standard deviation is comparatively small. This justifies our choice of N= 60 000\nin the natural language dataset settings.\nNeighborhood size for local estimates (dependence on L)Having fixed sample sizes Mand\nN, Figure 8 shows the distribution of TwoNN dimension estimates across varying locality scale\nparameters, represented by the number of neighbors L. A key observation is the relative stability of the\nestimates as the number of neighbors increases, with smaller locality scales (fewer neighbors) yielding\nmore diverse estimate distributions. This indicates that local properties of the data significantly\ninfluence dimension estimates.\nThe increasing stability with larger neighborhoods suggests that the embedding space exhibits more\nhomogeneity at broader locality scales. However, the variability at smaller scales aligns with the\nobservation in Brown et al. (2023) that in many cases, numerical data is not confined to a single\nmanifold of uniform dimensionality but rather consists of multiple manifolds or regions with varying\nlocal dimensions.\nOur findings provide evidence that token embedding spaces of this nature do not conform to a single,\nunique dimension. Instead, they exhibit a complex structure, where local estimates diverge, reflecting\n13\n--- Page 14 ---\n2000 4000 6000 8000 10000 12000 14000 16000\nSequence Sample Size8.008.258.508.759.009.259.509.7510.00Mean T woNN Estimate\n(a)MultiWOZ Training split\n2000 4000 6000 8000 10000 12000 14000 16000\nSequence Sample Size8.008.258.508.759.009.259.509.7510.00Mean T woNN Estimate\n(b)MultiWOZ Validation split\n2000 4000 6000 8000 10000 12000 14000 16000\nSequence Sample Size8.008.258.508.759.009.259.509.7510.00Mean T woNN Estimate\n(c)MultiWOZ Test split\nFigure 5: Boxplots of the mean local TwoNN estimates for different sequence sample sizes M\nranging from 2000 to16 000 . We compute the mean estimates for 5 different sequence sub-sampling\nseeds for the three splits of the MultiWOZ dataset. Here and in all subsequent boxplots, the average of\nthe mean estimates is depicted as green triangles, their median is the orange line; outliers are shown\nas circles.\nthe diverse geometric and topological properties within the space. This emphasizes the importance of\nconsidering locality when evaluating dimension estimates in such embedding spaces.\nA.2 Noise Analysis\nThe following experiments investigate the stability of the TwoNN dimension estimates under small\nperturbations of point clouds in a realistic setting of high-dimensional language model embedding\nspaces. The original work in Facco et al. (2017) studies the influence of artificial noise on the\nglobal TwoNN estimator for specific toy datasets (points sampled from a 2-dimensional plane in\n20-dimensional space, and points sampled from a 2-dimensional Gaussian distribution wrapped\n14\n--- Page 15 ---\n2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 22000\nSequence Sample Size101112131415Mean T woNN Estimate(a)Reddit Training split\n2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 22000\nSequence Sample Size101112131415Mean T woNN Estimate\n(b)Reddit Validation split\n2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 22000\nSequence Sample Size101112131415Mean T woNN Estimate\n(c)Reddit Test split\nFigure 6: Mean local estimates for different sequence sample sizes Mfor splits of the Reddit dataset,\nshown for 5 sampling seeds.\naround a Swiss roll). Here, we confirm the stability under noise for our mean local estimates and in\nthe realistic setting of high-dimensional contextual language model embeddings.\nWe analyze the effect of Gaussian noise on the embeddings from the MultiWOZ test set using a fixed\nset of parameters and varying noise distortions. Specifically, we use M= 10 000 random sequences\nfrom the MultiWOZ test set and extract the masked token embeddings from the last layer of the\nRoBERTa base model, which produces 768-dimensional vectors. To compute local estimates, we\nsub-sample N= 60 000 points from the embeddings and set the neighborhood size to L= 128 .\nGaussian noise is applied to all dimensions of the embeddings with varying distortion parameters,\nσ∈ {0.001; 0 .002; 0 .003; 0 .004; 0 .01}, and multiple random seeds to introduce variability. For each\nnoise level, we compute the approximate Hausdorff distance between the point cloud of the 60 000\nclean embedding vectors and the 60 000 noisy embedding vectors, using the Euclidean distance as\nthe distance metric.\n15\n--- Page 16 ---\n10000 20000 30000 40000 50000 60000 70000 80000 90000 100000\nSample Size9.159.209.259.309.359.409.459.50Mean T woNN Estimate(a)MultiWOZ training data.\n10000 20000 30000 40000 50000 60000 70000 80000 90000 100000\nSample Size9.159.209.259.309.359.409.459.50Mean T woNN Estimate\n(b)MultiWOZ validation data.\n10000 20000 30000 40000 50000 60000 70000 80000 90000 100000\nSample Size9.159.209.259.309.359.409.459.50Mean T woNN Estimate\n(c)MultiWOZ test data.\nFigure 7: Distribution of the mean local TwoNN estimates for different cardinalities of token sub-\nsamples. We compute the mean estimates for 5 different token sub-sampling seeds and show their\ndistribution for each fixed subsampling cardinality.\nThe results are visualized in Figure 9, where we plot the Hausdorff distance against three measures:\nthe global TwoNN-estimate of the noisy point cloud, the mean of the local TwoNN estimates, and the\nstandard deviation of the local TwoNN estimates.\nFrom the plots, we observe the following trends. The Hausdorff distances between the clean and noisy\nembeddings and the global and the mean local TwoNN estimates increase with larger noise levels,\nand there is a clear correlation between the two. Notably, for a distortion parameter of σ= 0.01,\nthe difference in the dimension estimates exceeds one. As expected, the standard deviation of the\nlocal TwoNN estimates also increases with higher noise levels, reflecting the increased variability\nintroduced by the noise. Nevertheless, under small noise levels, the mean local estimate remains\nrelatively stable and has lower variability than the global TwoNN estimate.\n16\n--- Page 17 ---\n16 32 64 128 256 512 1024\nNumber of Neighbors0102030T woNN Estimates\n(a)MultiWOZ validation data.\n16 32 64 128 256 512 1024\nNumber of Neighbors0204060T woNN Estimates\n (b)Reddit validation data.\nFigure 8: Distribution of the local token-wise TwoNN estimates over different locality scales L\n(number of neighbors). Here shown are the mean and quartiles of the resulting measure varying over\nthe different tokens in a fixed subsample. The more global the estimates, the smaller the standard\ndeviation in the resulting distribution.\n0.1 0.2 0.3\nHausdorff Distance9.29.39.49.59.69.7Global Estimate\n0.0020.0040.0060.0080.010\nNoise Distortion\n(a) Global TwoNN estimate.\n0.1 0.2 0.3\nHausdorff Distance9.49.69.810.010.210.410.6Mean Pointwise\n0.0020.0040.0060.0080.010\nNoise Distortion (b) Mean of local TwoNN estimates.\n0.1 0.2 0.3\nHausdorff Distance2.53.03.54.04.5Std Pointwise\n0.0020.0040.0060.0080.010\nNoise Distortion\n(c) Standard deviation of local TwoNN esti-\nmates.\nFigure 9: Effect of artificial Gaussian noise on masked token embeddings. On the x-axis we mark\nthe Hausdorff distance of the noisy embeddings to the clean embeddings, with color denoting the\ndistortion σin the artificial Gaussian noise.\nA.3 Masked Token Embeddings versus Regular Token Embeddings\nThe plots in Figure 10 compare the TwoNN estimates for regular tokens (left) and masked tokens\n(right) in the MultiWOZ dataset at the last layer. Each box-plot represents the distribution of token-\nwise TwoNN estimates.\nThe estimates for regular tokens have an interquartile range from approximately 5.5to8, with a\nmedian near 7. This distribution closely resembles that of masked tokens, which exhibit a similar\nrange and median. However, the broader spread for regular tokens suggests distinct token-level\ngeometric behaviors influenced by their role in the learned representation space. In any case, this\njustifies the computation of the mean local estimates for the regular token embeddings, which are\n17\n--- Page 18 ---\ncheaper to obtain: Every sequence token must be masked individually to retrieve masked embeddings\nwith a corresponding model forward pass. In contrast, for the regular embeddings, a single forward\npass of the entire sequence automatically produces embeddings for every input token.\nFigure 10: TwoNN estimates for regular and masked tokens in the MultiWOZ dataset at the last layer.\nBoth token types show similar distributions.\nB Additional details on datasets used\nSee Table 1 for details on the dataset cards and corresponding split sizes.\nTable 1: Dataset sizes for various datasets used in the experiments. Some datasets use pre-determined\nsplits, while others involve random splits.\nDataset Training Size Validation Size Test Size\nMultiWOZ2.1\nhttps://huggingface.co/datasets/ConvLab/multiwoz21113 556 14 748 14 744\nEmoWOZ\nhttps://paperswithcode.com/dataset/emowoz-166 474 8509 8634\nSGD\nhttps://huggingface.co/datasets/google-research-datasets/\nschema_guided_dstc8329 964 48 726 84 594\nWikipedia\n(random 80%-10%-10% split)\nhttps://huggingface.co/datasets/Salesforce/wikitext/\nviewer/wikitext-103-v11 441 080 180 135 180 135\nReddit\n(random 80%-10%-10% split)\nhttps://huggingface.co/datasets/SocialGrep/\none-year-of-tsla-on-reddit174 996 21 874 21 874\nC Additional Experimental Results\nC.1 Additional fine-tuning results\nFine-tuning on unseen data Continuing the discussion in Section 4.1, in Figures 11 to 13, we\nprovide additional distribution plots of the local estimates. Figure 11 is interesting because here the\nRoBERTa model is fine-tuned on data that it has not seen during pre-training.\nFine-tuning an autoregressive language model Moreover, in Figures 12 and 13 we show the\ndistribution of local estimates for the auto-regressive GPT-2-medium model in a natural language\nmodeling task, and their shift under fine-tuning. The GPT-2-medium fine-tuning here is performed\nwith standard parameters on the first 10 000 sequences of the given dataset’s train portion, and we\nshow the local estimates at the checkpoint after 1200 batches.\n18\n--- Page 19 ---\nRoBERT a RoBERT a fine-tuned on Reddit05101520T woNNMean=9.09; Median=9.37; Std=3.01\nMean=8.52; Median=8.64; Std=2.61(a) TwoNN estimates on MultiWOZ validation embeddings.\nRoBERT a RoBERT a fine-tuned on Reddit05101520T woNNMean=7.16; Median=7.14; Std=2.41\nMean=6.91; Median=6.74; Std=2.18\n(b) TwoNN estimates on SGD validation embeddings.\nRoBERT a RoBERT a fine-tuned on Reddit010203040T woNNMean=13.27; Median=13.08; Std=5.24\nMean=11.69; Median=11.37; Std=4.67\n(c) TwoNN estimates on Reddit validation embeddings.\nFigure 11: Comparison of the intrinsic dimension (ID) across three data modalities for both models.\nThe ID of embeddings originating from the fine -tuning distribution ( Reddit ) differs markedly\nbetween models, whereas the IDs for the out -of-distribution corpora ( SGD ,MultiWOZ ) are almost\nindistinguishable.\nC.2 Layerwise computation of local estimates\nOur setup from Section 3 and algorithm 1 naturally applies to embeddings derived from arbitrary\nlayers of the language model. In particular, the mean local estimates can be compared layer-wise\nbetween different checkpoints of a model, as we show in Figure 14. The main observations here are\nthat the drop in mean local dimension on the dataset used for fine-tuning is visible over all the model\nlayers. It appears to be most pronounced in the intermediate and last layers of the model.\nC.3 Additional Grokking results\nThis section supplements the grokking experiments in Section 4.2. See Figure 15 for a plot comparing\nmodel performance and mean local estimates on the training set, shown separately for the training\nrun seeds. This representation is useful, as the timing of the onset of grokking can be very dependent\non the model and dataset shuffling seed. It remains true that for individual seeds, the drop in mean\nlocal dimension on the training set coincides with the increase in validation accuracy.\nC.4 Additional Trippy-R dialogue state tracking results\nWe here show additional results for Section 4.3, where we discuss exhausting model training capa-\nbilities. See Figure 16c for longer TripPy-R dialog state tracking runs over 50 epochs. After about\n20training epochs, the mean local estimates stabilize and predict the convergence of the model\nperformance on the downstream task.\n19\n--- Page 20 ---\nGPT-2 GPT-2 fine-tuned on MultiWOZ51015T woNNMean=8.23; Median=7.94; Std=1.92\nMean=6.22; Median=5.99; Std=1.54(a) TwoNN estimates on MultiWOZ validation embeddings.\nGPT-2 GPT-2 fine-tuned on MultiWOZ10203040T woNNMean=18.59; Median=18.52; Std=3.93\nMean=18.51; Median=18.43; Std=3.95\n(b) TwoNN estimates on Wikipedia validation embeddings.\nGPT-2 GPT-2 fine-tuned on MultiWOZ051015202530T woNNMean=13.59; Median=13.91; Std=5.38\nMean=13.18; Median=13.36; Std=5.07\n(c) TwoNN estimates on Reddit validation embeddings.\nFigure 12: Comparison of the intrinsic dimension (ID) across three data modalities for both models.\nThe ID of embeddings originating from the fine -tuning distribution ( MultiWOZ ) differs markedly\nbetween models, whereas the IDs for the out -of-distribution corpora ( Wikipedia ,Reddit ) are\nalmost indistinguishable.\n20\n--- Page 21 ---\nGPT-2 GPT-2 fine-tuned on Wikitext5101520T woNNMean=8.23; Median=7.94; Std=1.92\nMean=8.30; Median=7.98; Std=2.04(a) TwoNN estimates on MultiWOZ validation embeddings.\nGPT-2 GPT-2 fine-tuned on Wikitext10203040T woNNMean=18.59; Median=18.52; Std=3.93\nMean=17.33; Median=17.31; Std=3.89\n(b) TwoNN estimates on Wikipedia validation embeddings.\nGPT-2 GPT-2 fine-tuned on Wikitext0102030T woNNMean=13.59; Median=13.91; Std=5.38\nMean=13.88; Median=14.37; Std=5.82\n(c) TwoNN estimates on Reddit validation embeddings.\nFigure 13: Comparison of the intrinsic dimension (ID) across three data modalities for both models.\nThe ID of embeddings originating from the fine -tuning distribution ( Wikipedia ) is markedly lower,\nwhereas the IDs for the out -of-distribution corpora ( MultiWOZ ,Reddit ) are even higher for the\nfine-tuned variants.\n21\n--- Page 22 ---\n−12−11−10−9−8−7−6−5−4−3−2−15101520\nLayerMeanTwoNN dimension estimates for MultiWOZ -validation\nRoBERTa\nRoBERTa fine-tuned on MultiWOZ -train\n−12−11−10−9−8−7−6−5−4−3−2−15101520\nLayerMeanTwoNN dimension estimates for Wikipedia -validation\nRoBERTa\nRoBERTa fine-tuned on MultiWOZ -train\nFigure 14: Comparison of the development of the ID through the different layers of the two models.\nThe ID of embeddings from the distribution that has been used for fine-tuning differs significantly\nbetween the two models, whereas the ID of other data distributions are indistinguishable.\n0 2000 4000 6000 8000 10000 12000 14000\nStep0.00.20.40.60.81.0Training AccuracyTraining fraction\n0.10\n0.15\n0.20\n0.25\n0.300.35\n0.40\n0.45\n0.50\n(a)Training accuracy\n0 2000 4000 6000 8000 10000 12000 14000\nStep0.00.20.40.60.81.0Validation Accuracy (b) Validation accuracy\n0 2000 4000 6000 8000 10000 12000 14000\nStep4681012141618Mean local dimension (c)Training mean local estimates\nFigure 15: Training a model on addition mod p= 197 with different training data fraction selected\nfrom{0.1; 0.15; 0.2; 0.25; 0.3; 0.4; 0.5}. Five seeds per configuration are shown separately, demon-\nstrating that the onset of grokking and the drop in mean local dimension vary greatly depending\non the specific training run. The plots show the development over 15 000 batches. The mean local\nestimates are computed on the training split forN= 3000 ;L= 64 .\n20000 60000\nSteps57911Mean local dimension\nEvaluation measures\n(a)MultiWOZ Training split\n20000 60000\nSteps57911Mean local dimension\n00.20.40.6\nEvaluation measures\n (b)MultiWOZ Validation split\n20000 60000\nSteps57911Mean local dimension\nMean local dim.\nJoint Goal Accuracy\nLoss\n00.20.40.6\nEvaluation measures\n (c)MultiWOZ Test split\nFigure 16: Longer Trippy-R training runs over 50 epochs with a linear learning rate schedule. Shown\nare the mean and standard deviation of the measures over 4 seeds of the training runs.\n22",
  "text_length": 69271
}