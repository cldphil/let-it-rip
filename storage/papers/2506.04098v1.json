{
  "id": "http://arxiv.org/abs/2506.04098v1",
  "title": "TextAtari: 100K Frames Game Playing with Language Agents",
  "summary": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.",
  "authors": [
    "Wenhao Li",
    "Wenwu Li",
    "Chuyun Shen",
    "Junjie Sheng",
    "Zixiao Huang",
    "Di Wu",
    "Yun Hua",
    "Wei Yin",
    "Xiangfeng Wang",
    "Hongyuan Zha",
    "Bo Jin"
  ],
  "published": "2025-06-04T15:55:27Z",
  "updated": "2025-06-04T15:55:27Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04098v1",
  "full_text": "--- Page 1 ---\nTextAtari: 100K Frames Game Playing with Language\nAgents\nWenhao Li1Wenwu Li1Chuyun Shen2Junjie Sheng3Zixiao Huang2Di Wu1\nYun Hua4Wei Yin5Xiangfeng Wang2Hongyuan Zha6Bo Jin1\n1Tongji University, Shanghai, China2East China Normal University, Shanghai, China\n3Huawei Cloud Huawei Technologies Co., Ltd., Shanghai, China\n4Shanghai Jiao Tong University, Shanghai, China5Bank of Communications, Shanghai, China\n6The Chinese University of Hong Kong, Shenzhen, China\n{whli, wenwu, wu2002, bjin}@tongji.edu.cn ,zhahy@cuhk.edu.cn\n{zxhuang@stu, cyshen@stu, xfwang@cs}.ecnu.edu.cn\nshengjunjie6@huawei.com ,yinw_8@bankcomm.com ,hyyh28@sjtu.edu.cn\nFigure 1: Statistics of tasks and horizons.\nAbstract\nWe present TextAtari, a comprehensive benchmark for evaluating language agents\non very long-horizon decision-making tasks spanning up to 100,000steps. By\ntranslating the visual state representations of classic Atari games into rich tex-\ntual descriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. Our benchmark encompasses\nnearly 100 distinct tasks with varying complexity, action spaces, and planning\nPreprint.arXiv:2506.04098v1  [cs.CL]  4 Jun 2025\n--- Page 2 ---\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-\nshot, few-shot chain-of-thought, and reflection reasoning) to systematically assess\nhow different forms of prior knowledge affect performance on these unprece-\ndented long-horizon challenges. The four distinct scenarios—Basic, Obscured,\nManual Augmentation, and Reference-based—investigate the impact of semantic\nunderstanding, instruction comprehension, and expert demonstrations on agent\ndecision-making. Our results reveal significant performance gaps between lan-\nguage agents and human players in these extensive planning tasks, highlighting\nchallenges in sequential reasoning, state tracking, and strategic planning across\ntens of thousands of steps. TextAtari provides standardized evaluation protocols,\nbaseline implementations, and a comprehensive framework for advancing research\nat the intersection of language models and planning.\n1 Introduction\nSequential decision-making over extended time horizons represents one of the most fundamental\nchallenges in artificial intelligence (Aghzal et al., 2025; Kang et al., 2024; Pignatelli et al., 2023).\nWhile humans naturally navigate complex, long-term planning scenarios—from playing strategic\ngames to coordinating daily activities—AI systems have traditionally struggled with tasks requiring\nthousands of interdependent decisions. Recent analyses reveal a striking trend: the length of tasks that\ngeneralist autonomous AI agents can complete with 50% reliability has been doubling approximately\nevery 7months for the past 6years (Kwa et al., 2025), as shown in Figure 2. This exponential\ngrowth suggests that within a decade, AI agents may independently complete tasks that currently\ntake humans days or weeks—yet a critical gap remains in our ability to evaluate these systems on\ntruly long-horizon challenges.\nFigure 2: Screenshot from Kwa et al. (2025).The AI community has developed numerous\nbenchmarks for evaluating sequential decision-\nmaking, spanning web interfaces, desktop soft-\nware, games, and embodied environments (Tan\net al., 2024). However, our comprehensive anal-\nysis of 163existing sequential decision bench-\nmarks (see Appendix for the full list) reveals a\ncritical limitation: most operate on remarkably\nshort horizons. While web and software manipu-\nlation tasks typically involve fewer than 50steps,\ntext and video games represent the longest hori-\nzon challenges ( 75% approach 500steps)—yet\neven these remain dramatically underexplored,\nwith only 4out of 163benchmarks reaching the\n100,000-step threshold *needed to evaluate truly extended reasoning†.\nThis horizon gap is particularly concerning as digital games emerge as the most challenging sequential\ndecision domains due to their unique combination of environmental complexity, non-linear decision\npaths, and partial observability—requiring agents to store and reason upon past experiences for\neffective decision-making (Ecoffet et al., 2019; Fan et al., 2022; Ma et al., 2024). As language models\nincreasingly serve as the cognitive engine for autonomous systems, their capacity for sustained\nreasoning and decision-making across very long horizons becomes a critical frontier for research.\n*According to estimates from previous work (Kwa et al., 2025), current large language model agents can\ncomplete tasks that take humans less than 4minutes with a success rate approaching 100%. Based on the\nstatistics compiled in this paper (as shown in Figure 3), the median number of decision steps in most benchmarks\nis around 10steps. Therefore, 100 ,000steps would correspond to tasks that would take humans about a week\nto complete, this would require approximately 2-4years of development time (Kwa et al., 2025), making the\n100 ,000-step challenge an appropriately difficult goal that is unlikely to be achieved in the near future.\n†These benchmarks with ultra-long decision-making sequences have their own limitations, which will be\ndiscussed shortly.\n2\n--- Page 3 ---\nRecent advances in large language models (LLMs) (Anthropic, 2025; Guo et al., 2025; Jaech et al.,\n2024) have demonstrated remarkable capabilities in reasoning, planning, and decision-making within\nshort contexts. These models can generate coherent multi-step plans and engage in complex reasoning\ntasks (Chen et al., 2025; Li et al., 2025). However, their ability to maintain consistent decision-making\nover very long horizons—spanning tens of thousands of steps—remains largely unexplored. This\nrepresents not merely a quantitative challenge but a qualitative shift in the nature of reasoning required:\nfrom short-term tactical decisions to long-term strategic planning with compounding consequences.\nConsider the challenge of playing a video game. Human players naturally track game state, form\nstrategic plans, adapt to changing conditions, and execute thousands of actions over extended\ngameplay sessions. This requires maintaining contextual awareness, making predictions about\nfuture states, and continuously adjusting strategies based on feedback—all capabilities essential for\ngeneral-purpose AI systems (Lake et al., 2015, 2017; Rips & Hespos, 2015). Yet current language\nagents struggle with such long-horizon tasks, particularly when visual information must be processed\nthrough language descriptions rather than direct observation.\nFigure 3: Horizon statistics.In this work, we introduce TextAtari, a comprehensive\nbenchmark for evaluating language agents on very long-\nhorizon decision-making tasks spanning up to 100,000\nsteps. TextAtari transforms the visual states of clas-\nsic Atari games into rich textual descriptions using an\nunsupervised representation learning framework (Atari-\nARI) (Anand et al., 2019), creating a challenging testbed\nthat bridges sequential decision-making with natural lan-\nguage processing. Our decision to transform visual game\nenvironments into textual representations is methodolog-\nically motivated. While visual-language models have\nadvanced significantly, they remain limited in reasoning\ndepth, context length handling, and decision coherence\ncompared to text-only language models (Yang et al., 2022,\n2024). This approach enables evaluation of pure reasoning\ncapabilities by eliminating confounding variables associated with visual processing, allows precise\ncontrol over information representation, provides standardized inputs for enhanced experimental\nreproducibility, and aligns with the theoretical framework of language models as general reasoning\nengines interacting with environments through symbolic descriptions. TextAtari thus establishes a\ncontrolled experimental environment for evaluating language agents’ capacity to maintain coherent\nreasoning across extended horizons.\nOur benchmark encompasses nearly 100distinct tasks with varying complexity, action spaces, and\nplanning horizons. TextAtari offers several key advantages: First, Atari games provide well-defined\nenvironments with clear objectives and measurable performance metrics. Second, by rendering these\ntraditionally visual environments as text, we can directly evaluate language agents’ ability to process,\nreason about, and act upon textual information over extremely long horizons. Third, our four distinct\nscenario designs—Basic, Obscured, Manual Augmentation, and Reference-based—enable systematic\ninvestigation of how different forms of prior knowledge affect agent performance.\nWe hope TextAtari will serve as a valuable resource for the research community, providing standard-\nized evaluation protocols, baseline implementations, and a comprehensive framework for advancing\nresearch at the intersection of language models and planning. Progress on this benchmark could lead\nto language agents capable of coherent decision-making across very long time horizons, bringing us\ncloser to AI systems that can maintain consistent reasoning and planning at human-like scales.\n2 Benchmark Design and Construction\nTextAtari addresses the critical gap in existing sequential decision-making evaluations by transform-\ning visual Atari environments into rich textual descriptions for language model processing. Our\nbenchmark targets horizons of up to 100,000steps—a threshold reached by fewer than 2.5% of\nexisting benchmarks. We employed AtariARI, an unsupervised representation learning framework, to\nconvert visual states into detailed textual descriptions that preserve essential gameplay features while\ncreating a challenging testbed for language-based reasoning. The benchmark construction process\n3\n--- Page 4 ---\nDetailed Description of Atari Games\nGame Detailed Description\nVenture An exploration game where players control Winky, an adventurer navigating\nthrough a multi-room dungeon to collect treasures. Each room contains\ndifferent monsters guarding treasure, requiring specific strategies to overcome.\nPlayers view the dungeon layout from an overhead perspective but transition\nto a zoomed-in view when entering a room. If players take too long in a room,\nthe invincible \"Hallmonster\" appears, forcing swift action. The game features\nfour different dungeons with increasing difficulty and unique monsters in\neach room, from snakes and trolls to giant spiders and the Grim Reaper.\nVideoPinball A digital recreation of pinball that simulates the physics and features of a\ntraditional pinball machine. Players control left and right flippers to keep the\nball in play, aiming to hit various targets to score points. The table includes\nbumpers, spinners, rollover targets, and bonus areas. Players can tilt the\ntable (with limits) to influence ball direction. The game features realistic ball\nphysics including momentum, ricochet angles, and speed changes. Special\nfeatures include multiball play and bonus rounds that can be activated through\nspecific target combinations. Scoring emphasizes both quick reflexes and\nstrategic target selection.\nTable 1: This table provides detailed descriptions of selected atari games., explaining their gameplay\nmechanics, objectives, and distinctive features.\ninvolved selecting Atari games that represent diverse challenges in sequential decision-making. We\nprioritized games with varying complexity levels, action spaces, and planning horizons to evaluate\ndifferent aspects of language agents’ long-horizon reasoning capabilities.\n2.1 Task Suite\nOur task suite encompasses 23classic Atari games spanning four major categories: Action Games,\nPuzzle and Strategy Games, Sports Games, and Arcade Classics. Each category presents distinct\nchallenges for language models, testing different aspects of reasoning, planning, and decision-making\nover extended time horizons.\nAction Games like Asteroids and Berzerk challenge models with spatial reasoning requirements and\nstrategic target prioritization. Puzzle and Strategy Games such as Breakout and MontezumaRevenge\nemphasize planning and trajectory prediction, with MontezumaRevenge featuring extremely sparse\nrewards that require extensive planning. Sports Games including Boxing and Tennis present chal-\nlenges in adversarial reasoning and anticipating opponent behaviors. Arcade Classics like MsPacman\nand Seaquest demand dynamic path planning and resource management in partially observable\nenvironments. Each game presents unique combinations of challenges across four key dimensions:\nspatial reasoning, planning and strategy, partial observability, and temporal reasoning. For instance,\nSeaquest requires sophisticated resource management (oxygen) while MsPacman demands strategic\npower pellet usage and ghost behavior modeling.\n2.2 Environment Generation Pipeline\nOur framework transforms standard Atari games from the Arcade Learning Environment (ALE) into\na purely language-based interface for agents, as shown in Figure 4. Instead of pixel observations, the\nenvironment exposes a symbolic game state description at each timestep, allowing a large language\nmodel (LLM) to perceive the game through text alone. This is accomplished by extracting high-level\nstate variables from the emulator’s RAM ( 128bytes) and converting them into natural language. In\nparticular, we leverage the Atari Annotated RAM Interface (AtariARI) wrapper, which provides\nstructured labels for key game entities and variables (e.g. player and object coordinates, scores,\nlives) by mapping RAM bytes to human-interpretable state information. These raw values are then\nannotated and linearized into textual observations using template-based descriptions.\nEach game is supported by a dedicated translator module following a common design: a\nGameDescriber component provides static context (a brief overview of the game mechanics, objec-\ntives, and the action space in words), an ObsTranslator maps each current state vector to a sentence\n4\n--- Page 5 ---\nAtari Games Classification and LLM Gaming Challenges (Summary)\nGame Category Challenges for LLM\nAction Games\nAsteroids Space ShooterSpatial reasoning for circular movement\nReaction-based gameplay timing\nStrategic target prioritization\nBerzerk Maze ShooterNavigating complex maze layouts\nDynamic obstacle avoidance\nMultitasking (walls, enemies, bullets)\nPuzzle and Strategy Games\nBreakout Brick-breakerGeometry understanding\nTrajectory prediction\nTiming-sensitive paddle control\nMontezumaRevenge Puzzle PlatformerExtremely sparse rewards\nComplex dependency hierarchies\nPrecise timing for trap avoidance\nSports Games\nBoxing SportsAdversarial reasoning\nTactical positioning\nTiming attack and defense moves\nTennis SportsCourt positioning strategy\nOpponent behavior anticipation\nShot selection planning\nArcade Classics\nMsPacman MazeDynamic path planning\nGhost behavior modeling\nStrategic power pellet usage\nSeaquest Underwater ShooterResource management (oxygen)\nMulti-objective balancing\nBidirectional threat assessment\nTable 2: Selected Atari Games and Their LLM Challenges (Summary). This table presents\nkey examples from each game category with their primary challenges for LLMs. Color cod-\ning indicates challenge types: spatial reasoning ,planning & strategy ,partial observability , and\ntemporal reasoning .\nRaw Game Frame\n Translator\nObsT ranslatorGameDescriber\nTransition\nTranslator\n(optional)Symbolic Game State\nLive: 4 \nScore: 500 \nMs.Pacman: \n(72, 72) \nGhosts: 4\n…AtariARIMs. Pac-Man is at position (72, 72) ,\nfacing RIGHT  with 2 lives left. 72 dots\nhave been eaten so far , and the\ncurrent score is 500. \nThe game has 4 ghosts . A fruit is\npresent at position (72, 1 12).\nSue the ghost is at position (72, 108),\n... Textual Obsevation\nFigure 4: Environment verbalization with AtariARI.\n(or set of sentences) describing the agent’s situation (for example, reporting positions of relevant\nobjects and the current score), and a TransitionTranslator extends this by narrating state transi-\ntions (integrating the last state, the agent’s chosen action with a textual label, any reward obtained,\nand the resulting next state). This modular template structure generalizes across games – new Atari\ngames can be integrated by implementing a similar translator with game-specific vocabulary and\ntemplates, while reusing the same interface methods for observations and transitions. For instance,\nthe Bowling translator reports the ball and pin positions along with frame scores, whereas the Boxing\n5\n--- Page 6 ---\ntranslator details both fighters’ locations and points; both adhere to the same class structure and\noutput format.\nTo interface smoothly with an LLM-based agent, we throttle the observation generation to a fixed\nfrequency (approximately 5 Hz), ensuring the agent has time to process each description and choose\nan action. We also enforce a limit on the description length (in tokens) so that each observation\ncomfortably fits within the context window of the LLM. The result is a pixel-free, symbol-grounded\ninteraction loop: the agent perceives an Atari game only through textual narratives of the game\nstate and responds with actions accordingly, enabling direct application of language reasoning and\nprompting techniques to real-time game control.\nTo disentangle the contribution of different knowledge sources we define four textual conditions that\nvary only in the auxiliary information supplied to the language model, while holding the evaluation\nbudget, sampling frequency, and backbone checkpoint fixed. Each condition introduces distinct\ntypes of prior knowledge into the prompt, allowing us to isolate their individual effects on agent\nperformance. Below, we describe each setting in detail and provide the exact prompting format used\nduring interaction.\nBasic Scenario. At every step the agent receives only the live textual observation generated by\nthe TextAtari interface. Apart from this stream and the immutable header (game synopsis, goals,\nlegal actions), no external material is introduced, making Basic the reference against which all other\nconditions are measured. The prompt includes a static instruction and the latest observation with\naction choices.\nObscured Scenario. This condition tests reliance on lexical priors by replacing each domain-specific\nnoun in the observation sentence–such as ghost ,paddle , orasteroid –with the neutral token item.\nThe transformation is executed dynamically at runtime using a fixed dictionary (e.g., “ghost” →\n“item” ,“ball” →“item” ). This forces the agent to interpret environment dynamics based on\nstructure and positioning rather than familiar words. Numbers, coordinates, colours, scores, and the\ninitial header remain untouched.\nManual Augmentation Scenario. To supply explicit rule knowledge, we prepend a concise manual\nexcerpt to the prompt at the start of every episode. Manuals are harvested once per game from online\nrepositories such as AtariAge. If scanned, the pages are passed through an OCR pipeline, and then\nsummarised by a large language model (e.g., GPT-4) into ≤300tokens capturing key information\nabout controls, scoring mechanics, and game-end conditions. This summary is inserted after the\ngame header and serves as grounding for the LLM during gameplay.\nReference-based Scenario. Here the agent is primed with an expert demonstration prior to gameplay.\nFor each title, we train a Proximal Policy Optimisation (PPO) controller using Stable-Baselines3\nuntil it reaches at least average human performance. A single full evaluation episode is then recorded\nand subsampled by extracting every 10th state–action pair. The state is converted into text via the\nText-Atari encoder, and the action is rendered using a task-specific verb template. These state–action\nentries are then concatenated chronologically into a 400-token trajectory block injected once before\nlive inference begins. This method offers the model an in-context exemplar of competent play without\ndirectly encoding future knowledge.\n2.3 TextAtari Statistics\nOur experiments revealed significant computational demands across the TextAtari benchmark, as\nshown in Figure 5. The left figure illustrates the considerable variation in action space complexity\nacross the 23Atari games, with games like Hero, Tennis, and BattleZone exhibiting action spaces\napproximately three times larger than simpler games such as Skiing and Freeway. This action space\ndiversity creates varying degrees of decision complexity that challenge language models differently.\nThe computational resources required for these experiments were substantial, as shown in the upper\nright figure. Runtime performance varied dramatically across models and games, with LLaMA3.1-\n8B consistently requiring the most computation time—exceeding 18,000minutes ( 300hours) for\nmultiple games. Qwen2.5-7B and Gemma-7B demonstrated more efficient processing, typically\nrequiring 60-70% of LLaMA3.1’s runtime. The most computationally intensive games (Zaxxon,\nSeaquest, and Tennis) required over 15,000minutes of processing time per model, highlighting the\nextraordinary computational demands of evaluating long-horizon reasoning.\n6\n--- Page 7 ---\nFigure 5: Computational costs.\nToken consumption (lower right figure) further emphasizes the scale of these experiments. The\nmost token-intensive games consumed between 40k-50k tokens per decision step across all models,\nwith LLaMA3.1-8B consistently using more tokens than its counterparts. For context-heavy games\nlike Boxing and BattleZone, this translated to billions of tokens processed throughout the full\nevaluation. This extreme token consumption approaches the practical limits of current context\nwindows, particularly when agents must maintain coherent reasoning across millions steps.\nIn total, our comprehensive evaluation consumed approximately 820,000GPU-minutes on A100\nhardware, representing one of the most computationally intensive benchmarks for language agents to\ndate. This substantial resource commitment underscores the challenge and importance of evaluating\nlong-horizon reasoning capabilities.\n3 Experiments\n3.1 Evaluation Protocol\nAll scenarios employ the identical inference agent, language model, and roll-out protocol: 23Atari\n2600 games, a horizon of 1000 interaction steps (for cost saving), and five random seeds. Because\naugmentation increases prompt length, a sliding-window policy discards the oldest system-level\nmessages whenever the projected token count approaches the model’s context limit, ensuring that\nevery query remains admissible. Consequently, observed performance differences can be attributed\nto the injected knowledge rather than disparities in prompt size or compute budget.\n3.2 Baselines\nWe examine three dialogue policies–Basic, Chain-of-Thought (CoT), and CoT with Reflection–\nthat differ only in the structure and feedback they impose on the language model; all external\naugmentations (manual excerpts, expert trajectories, noun masking) are injected before prompt\nassembly and therefore affect the three agents identically. The discussion below focuses exclusively\non each agent’s internal procedure for constructing, delivering, and updating prompts.\nBasic. At every decision step the Basic agent issues the leanest prompt possible. It consists only of\n(i) a static system header that presents a one-sentence synopsis of the game, the win or termination\nclause, and a numbered list of legal actions, and (ii) a single user message that embeds the live\nText-Atari observation. The agent neither requests a reasoning trace nor retains any form of memory;\nno few-shot examples or auxiliary knowledge are provided. The user instruction is fixed across all\ngames and episodes, enforcing a strict zero-shot setting. The language model must respond with\na single valid action identifier–in JSON form when required–without any additional commentary,\nmaking this template the shortest baseline against which richer prompting schemes are evaluated.\n7\n--- Page 8 ---\nFigure 6: Relative perfor-\nmances.Chain-of-Thought. The CoT agent extends the Basic template by\ndemanding explicit step-by-step reasoning. A leading system in-\nstruction frames the model as an expert Atari player and mandates\na JSON reply with two keys– \"thought process\" and\"action\" .\nImmediately thereafter the agent appends the game synopsis, the win\nor termination clause, and the latest observation, followed by a fixed\nuser instruction template that drives the reasoning routine. This\nprompt asks the model to pause, articulate its internal deliberation,\nand emit a machine-readable decision. To stabilise style, a hand-\nful of demonstration dialogues illustrating ideal chain-of-thought\nreasoning are prefixed; these exemplars remain fixed across games.\nOptional long-term memory (archived reasoning traces and episode\nrewards) and short-term memory (the most recent state–action pairs)\nare inserted when token budget permits. A running counter tracks\nprompt length and discards the oldest background messages–first\nlong-term memory, then exemplars–once the predicted total nears\nthe context window, ensuring a deliverable query at every step.\nCoT with Reflection. The Reflection agent carries out step-wise\nreasoning exactly as in CoT but inserts a self-critique phase at the\nend of every episode. When the terminal state is reached, the agent\nsupplies the model with its complete thought trace, the realised\nreward trajectory, and the following system instruction. The model’s\nJSON reply is stored as a reflection block . At the start of the next\nepisode up to three of the most recent reflection blocks are inserted\nas additional system messages labelled Recent Plans , giving the\nmodel a concise self-generated briefing on past mistakes, lessons, and intended adjustments. Older\nreflections are discarded whenever their inclusion would push the prompt beyond the context window,\nensuring consistent token budgets. During gameplay the per-step prompt and action-parsing logic\nremain identical to CoT, but every episode begins with a short, data-driven feedback loop that\nencourages iterative policy refinement without parameter updates.\n3.3 Results\nWe evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B)\nacross three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) on\nTextAtari. Our findings reveal that language models struggle significantly with very long-horizon\ntasks, with performance across over 90% of scenarios falling below 10% of human capability. Only in\ntwo specific tasks did the best agent configurations approach or marginally exceed human performance.\nPrior knowledge integration—specifically game manuals and expert demonstrations—emerged as the\nmost consistent performance driver, yielding average improvements exceeding 100% across models\nand tasks. Surprisingly, reasoning-enhancement techniques such as chain-of-thought prompting and\nreflection mechanisms showed inconsistent benefits, with effectiveness varying dramatically across\nmodel architectures and game environments. This variability underscores the fundamental challenges\nin maintaining coherent state tracking, strategic planning, and decision consistency across extended\ntime horizons. The substantial performance gap between even the best language agents and human\nplayers highlights the difficulty of maintaining coherent decision-making over tens of thousands of\nsteps, suggesting that current language models, regardless of architecture or prompting technique,\nlack the cognitive mechanisms necessary for truly extended reasoning.\n4 Conclusion\nWe introduced TextAtari, a comprehensive benchmark for evaluating language agents on very long-\nhorizon decision-making tasks spanning up to 100,000steps. By transforming visual Atari games\ninto textual descriptions, we created a challenging testbed bridging sequential decision-making with\nnatural language processing. Our experimental results revealed significant challenges in long-horizon\nplanning for current language models. Even the best agent configurations achieved less than 10%\nof human performance across over 90% of tasks, with only two specific scenarios approaching\nhuman-level competence. Prior knowledge integration (game manuals and expert demonstrations)\n8\n--- Page 9 ---\nFigure 7: Selected performance comparison. See Appendix for more details.\nyielded the most consistent improvements, averaging over 100% performance gains, while reasoning\nenhancement techniques showed inconsistent benefits across model architectures and environments.\nTextAtari addresses a critical gap in existing benchmarks by specifically targeting language-based\nreasoning over extended temporal scales. Progress on this benchmark could advance autonomous\nagent development toward systems capable of maintaining consistent reasoning and planning at\nhuman-like scales.\nLimitations and future work TextAtari has several limitations suggesting directions for future re-\nsearch. The benchmark’s substantial computational demands—approximately 820,000GPU-minutes\nwith some games requiring over 300hours per model and up to 50k tokens per decision step—may\nlimit accessibility. Future work should explore more efficient evaluation protocols without sacrificing\nbenchmark validity. The transformation of visual environments into text, while methodologically\nsound, introduces potential information loss. Future iterations could explore multimodal extensions\nand evaluate how different textual representation granularities affect performance. Additionally,\nas new architectures emerge specifically targeting sequential reasoning and memory management,\nTextAtari should evolve accordingly. Finally, while our benchmark demonstrates the significant\ngap between current AI systems and human capabilities, it does not fully diagnose the specific\ncognitive mechanisms responsible for this discrepancy. Future work could incorporate more detailed\nerror analysis and causal interventions to identify specific reasoning bottlenecks, guiding targeted\narchitectural improvements for long-horizon reasoning in language models.\n9\n--- Page 10 ---\nReferences\nAghzal, M., Plaku, E., Stein, G. J., and Yao, Z. A survey on large language models for automated\nplanning. arXiv preprint arXiv:2502.12435 , 2025.\nAnand, A., Racah, E., Ozair, S., Bengio, Y ., Côté, M.-A., and Hjelm, R. D. Unsupervised state\nrepresentation learning in atari. In NeurIPS , 2019.\nAnthropic. Claude 3.7 sonnet, 2025. URL https://www.anthropic.com/claude/sonnet .\nChen, Q., Qin, L., Liu, J., Peng, D., Guan, J., Wang, P., Hu, M., Zhou, Y ., Gao, T., and Che, W.\nTowards reasoning era: A survey of long chain-of-thought for reasoning large language models.\narXiv preprint arXiv:2503.09567 , 2025.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune, J. Go-explore: a new approach for\nhard-exploration problems. arXiv preprint arXiv:1901.10995 , 2019.\nFan, L., Wang, G., Jiang, Y ., Mandlekar, A., Yang, Y ., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y ., and\nAnandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge.\nInNeurIPS , 2022.\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948 , 2025.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel,\nA., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720 , 2024.\nKang, L., Zhao, Z., Hsu, D., and Lee, W. S. On the empirical complexity of reasoning and planning\nin llms. arXiv preprint arXiv:2404.11041 , 2024.\nKwa, T., West, B., Becker, J., Deng, A., Garcia, K., Hasin, M., Jawhar, S., Kinniment, M., Rush, N.,\nV on Arx, S., et al. Measuring ai ability to complete long tasks. arXiv preprint arXiv:2503.14499 ,\n2025.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through\nprobabilistic program induction. Science , 350(6266):1332–1338, 2015.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and\nthink like people. Behavioral and brain sciences , 40:e253, 2017.\nLi, Z.-Z., Zhang, D., Zhang, M.-L., Zhang, J., Liu, Z., Yao, Y ., Xu, H., Zheng, J., Wang, P.-J., Chen,\nX., et al. From system 1 to system 2: A survey of reasoning large language models. arXiv preprint\narXiv:2502.17419 , 2025.\nMa, W., Mi, Q., Zeng, Y ., Yan, X., Lin, R., Wu, Y ., Wang, J., and Zhang, H. Large language models\nplay starcraft ii: Benchmarks and a chain of summarization approach. In NeurIPS , 2024.\nPignatelli, E., Ferret, J., Geist, M., Mesnard, T., van Hasselt, H., Pietquin, O., and Toni, L. A survey\nof temporal credit assignment in deep reinforcement learning. arXiv preprint arXiv:2312.01072 ,\n2023.\nRips, L. J. and Hespos, S. J. Divisions of the physical world: Concepts of objects and substances.\nPsychological bulletin , 141(4):786, 2015.\nTan, W., Zhang, W., Xu, X., Xia, H., Ding, Z., Li, B., Zhou, B., Yue, J., Jiang, J., Li, Y ., et al.\nCradle: Empowering foundation agents towards general computer control. arXiv preprint\narXiv:2403.03186 , 2024.\nYang, M., Schuurmans, D., Abbeel, P., and Nachum, O. Dichotomy of control: Separating what you\ncan control from what you cannot. arXiv preprint arXiv:2210.13435 , 2022.\nYang, S., Walker, J., Parker-Holder, J., Du, Y ., Bruce, J., Barreto, A., Abbeel, P., and Schuurmans,\nD. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139 ,\n2024.\n10\n--- Page 11 ---\nA TextAtari Supplementary Material\n• Section B: Related Work\n• Section C: Task Details\n• Section D: Prompt Engineering\n• Section E: Missing Results\n• Section F: Border Impact\nB Related Work\nThis section provides a comprehensive analysis of existing sequential decision-making bench-\nmarks to contextualize TextAtari’s contribution. We surveyed 163 benchmarks across multiple\ndomains—Video Games, Web Interactions, Software Operations, Text Games, Card Games, and\nEmbodied Tasks—to evaluate the current landscape of agent evaluation frameworks.\nOur analysis reveals a critical limitation in existing benchmarks: most operate on remarkably short\nhorizons, with the median decision step count remaining below 50 for web and software manipulation\ntasks. Even among text and video games, which represent the longest horizon challenges, 75%\napproach only 500 steps. As illustrated in Figure 3 of the main paper, fewer than 2.5% of existing\nbenchmarks reach the 100,000-step threshold that TextAtari addresses.\nTable 3-5 catalog these benchmarks, highlighting key characteristics including domain category,\ndecision horizon (number of steps required for task completion), number of distinct tasks, and agent\ntype (single-agent or multi-agent). This collection provides empirical evidence for the “horizon\ngap” discussed in the introduction—the absence of standardized evaluations for truly long-horizon\nsequential decision-making tasks that would take humans days or weeks to complete.\nTextAtari addresses this gap by providing a standardized framework for evaluating language agents’\ncapacity to maintain coherent reasoning and decision-making across extended horizons of up to\n100,000 steps. Unlike most existing benchmarks that focus on short-term tactical decisions, TextAtari\nchallenges agents with long-term strategic planning where consequences compound over tens of\nthousands of steps—a qualitative shift in reasoning requirements that better approximates real-world\nextended planning challenges.\nB.1 Compare to Existing 100K-Step Benchmarks\nNetHack Learning Environment (Küttler et al., 2020; Paglieri et al., 2024), while technically support-\ning long gameplay sessions of tens of thousands of steps like TextAtari, suffers from several critical\nlimitations. The complexity of NetHack creates an extremely sparse reward landscape that makes\nsystematic evaluation challenging—agents often fail catastrophically before meaningful learning can\noccur. Additionally, NetHack’s representation combines ASCII symbols with complex game mechan-\nics that require substantial domain expertise to interpret properly. This creates an artificial hurdle of\ngame-specific knowledge rather than testing general reasoning capabilities. Unlike TextAtari, which\nprovides rich natural language descriptions accessible to any language model, NetHack’s symbolic\nrepresentation obscures the underlying state, making it difficult to disentangle reasoning failures from\nrepresentation understanding issues.\nSmartPlay (Wu et al., 2024b), while built on Minecraft and theoretically supporting extended game-\nplay, fundamentally compromises on measuring true long-horizon planning by relying heavily on\npredefined macro actions. These high-level abstractions dramatically reduce the actual decision\nspace—agents make far fewer genuine decisions than the step count suggests. This abstraction masks\nthe fundamental challenges of maintaining coherent reasoning across extended sequences. In contrast,\nTextAtari preserves the raw granularity of decision-making, requiring agents to make individual\nprimitive actions that compound over tens of thousands of steps, thus providing a more genuine\nmeasure of extended reasoning capabilities without artificial simplifications.\nB.2 Compare to Existing 1K-Step Benchmarks\nMany existing benchmarks that approach the thousand-step range suffer from various simplifications\nthat reduce their effectiveness for evaluating true long-horizon reasoning. Game-based benchmarks\n11\n--- Page 12 ---\nTable 3: Comprehensive language agent benchmark collection for sequential decision making (Part\nI). This table catalogs benchmarks for evaluating language agents across diverse domains. Each\nbenchmark is characterized by its domain category (e.g., Video Game, Web, Software, Text Game,\nCard Game, Embodied), decision horizon (number of steps required for task completion), number of\ndistinct tasks, and agent type (single-agent or multi-agent). This collection contextualizes TextAtari’s\ncontribution to the benchmark landscape, particularly in addressing the challenge of very long-horizon\ndecision-making tasks (up to 100,000 steps) for language agents.\nID Name Category Horizon Tasks Agent Type\n1 StarCraft II (Ma et al., 2024) Video Game 1000 10 multi-agent\n2 Red Dead Redemption II (Tan et al., 2024) Video Game 1000 5 single-agent\n3 V-MAGE (Zheng et al., 2025) Video Game 100, 1000, 5000 50 single-agent\n4 ONUW (Jin et al., 2024) Card Game 1 1 multi-agent\n5 WebGames (Thomas et al., 2025) Web 50 50 single-agent\n6 BEARCUBS (Song et al., 2025) Web 10 100 single-agent\n7 BattleAgentBench (Wang et al., 2024c) Video Game 100 10 multi-agent\n8 Collab-Overcooked (Sun et al., 2025) Video Game 10, 100 50 multi-agent\n9 SwarmBench (Ruan et al., 2025) Video Game 100 5 multi-agent\n10 DSGBench (Tang et al., 2025) Card Game, Video Game 10, 100, 1000 5 multi-agent\n11 PokéChamp (Karten et al., 2025) Video Game 10, 100 1 multi-agent\n12 Minedojo (Fan et al., 2022) Video Game 1000 5000 single-agent\n13 TextWorld (Côté et al., 2019) Text Game 1000 50 single-agent\n14 AlfWorld (Shridhar et al., 2020b) Embodied 50 5000 single-agent\n15 BabyAI-Text (Carta et al., 2023) Video Game 100, 1000 50 single-agent\n16 AgentBench (Liu et al., 2024a) Card Game, Embodied, Software, Web 10, 50 10 single-agent\n17 GameTraversalBenchmark (Nasir et al., 2024) Video Game 10, 100 100 single-agent\n18 WorkArena++(Boisvert et al., 2024) Web 1, 10 500 single-agent\n19 EMBODIED AGENT INTERFACE(Li et al., 2024b) Embodied 50 100 single-agent\n20 VLABench (Zhang et al., 2024c) Embodied 500 100 single-agent\n21 MindCraft (White et al., 2025) Video Game 10, 100 5000 multi-agent\n22 Multi-Mission Tool Bench (Yu et al., 2025) Software, Web 50 1000 single-agent\n23 SPREADSHEETBENCH (Ma et al., 2024c) Software 10 1000 single-agent\n24 OSWorld (Xie et al., 2024b) Software 10 500 single-agent\n25 WebCanvas (Pan et al., 2024) Web 10 1000 single-agent\n26 TUR[K]INGBENCH (Xu et al., 2024b) Web 50 100 single-agent\n27 Windows Agent Arena (Bonatti et al., 2024) Software, Web 10 100 single-agent\n28 VisualAgentBench (Liu et al., 2024b) Embodied, Software, Web 10 1000 single-agent\n29 OFFICEBENCH (Wang et al., 2024f) Software 50 500 single-agent\n30 WONDERBREAD (Wornow et al., 2024) Web 10 500 single-agent\n31 EscapeBench (Qian et al., 2024) Text Game 100, 1000, 500 500 single-agent\n32 VisEscape (Lim et al., 2025) Video Game 100, 500 1000 single-agent\n33 B-MoCA (Lee et al., 2024) Software 10 100 single-agent\n34 Spider2-V (Cao et al., 2024) Software 10, 50 500 single-agent\n35 ELT-Bench (Jin et al., 2025) Software 10, 100, 50 100 single-agent\n36 VideoGUI (Lin et al., 2024b) Software 10, 50 500 single-agent\n37 TaskBench (Shen et al., 2024b) Software, Web 10 20000 single-agent\n38 AQA-Bench (Yang et al., 2024) Text Game 10, 50 10 single-agent\n39 FamilyTool (Wang et al., 2025b) Software, Web 1, 5 500 single-agent\n40 MirrorAPI (Guo et al., 2025) Software, Web 1, 5 5000 single-agent\n41 WhodunitBench (Xie et al., 2024a) Card Game 10, 50 50 multi-agent\n42 GTA (Wang et al., 2024a) Software, Web 1, 5 500 single-agent\n43 m&m’s (Ma et al., 2024b) Software, Web 1, 5 5000 single-agent\n44 DISCOVERYWORLD (Jansen et al., 2024) Video Game 1000 50 single-agent\n45 debug-gym (Yuan et al., 2025) Software 50 2000 single-agent\n46 HCAST (Rein et al., 2025) Software 5, 50 200 single-agent\n47 AdaSociety (Huang et al., 2024c) Video Game 50 5 multi-agent\n48 Mars (Tang et al., 2024) Video Game 1000, 50 10 single-agent\n49 AndroidControl (Li et al., 2024c) Software 5 15000 single-agent\n50 A3 (Chai et al., 2025) Software 10, 5, 50 200 single-agent\n51 ROBOTOUILLE (Gonzalez-Pumariega et al., 2025) Video Game 10, 50 50 multi-agent\n52 MindAgent (Gong et al., 2024) Video Game 10, 5 10 multi-agent\n53 PlanBench (Valmeekam et al., 2023) Text Game 10, 50 1000 single-agent\n54 τ-bench (Yao et al., 2024) Software, Web 5, 50 200 single-agent\n55 TheAgentCompany (Xu et al., 2024a) Software, Web 10, 50 200 single-agent\nlike StarCraft II (Ma et al., 2024), Red Dead Redemption II (Tan et al., 2024), and Minecraft variants\n(MineDojo (Fan et al., 2022), Mars (Tang et al., 2024), MineLand (Yu et al., 2024), Crafter (Hafner,\n2022)) all rely on predefined macro-actions or action abstractions that dramatically simplify the actual\nplanning required. These abstract actions encapsulate complex sequences of decisions into single\nsteps, creating an illusion of long-horizon planning while actually testing much shorter decision\nsequences.\nV-MAGE (Zheng et al., 2025) remains limited to just two specific environments (SuperMario and\nFlappyBird), providing insufficient diversity to evaluate general reasoning capabilities across different\ndomains. Its narrow focus on platforming mechanics fails to test the breadth of reasoning types\n12\n--- Page 13 ---\nTable 4: Comprehensive language agent benchmark collection for sequential decision making (Part\nII). This table catalogs benchmarks for evaluating language agents across diverse domains. Each\nbenchmark is characterized by its domain category (e.g., Video Game, Web, Software, Text Game,\nCard Game, Embodied), decision horizon (number of steps required for task completion), number of\ndistinct tasks, and agent type (single-agent or multi-agent). This collection contextualizes TextAtari’s\ncontribution to the benchmark landscape, particularly in addressing the challenge of very long-horizon\ndecision-making tasks (up to 100,000 steps) for language agents.\nID Name Category Horizon Tasks Agent Type\n56 WebArena (Zhou et al., 2024a) Web 10, 50 1000 single-agent\n57 WebShop (Yao et al., 2022) Web 5, 50 10000 single-agent\n58 SHORTCUTSBENCH (Shen et al., 2024a) Software, Web 10, 50 10000 single-agent\n59 BALROG (Paglieri et al., 2024) Text Game, Video Game 10, 100000 10 single-agent\n60 MLGym (Nathani et al., 2025) Software, Web 50 10 single-agent\n61 VGRP-Bench (Ren et al., 2025) Text Game 10, 50 20 single-agent\n62 INVESTORBENCH (Li et al., 2024a) Software 250, 50 5 single-agent\n63 AndroidWorld (Rawles et al., 2024) Software 5, 50 100 single-agent\n64 MobileAgentBench (Wang et al., 2024b) Software 10, 5 100 single-agent\n65 SPA-Bench (Chen et al., 2024b) Software 5, 50 500 single-agent\n66 PARTNR (Chang et al., 2024) Embodied 10, 100 100000 multi-agent\n67 LVLM-Playground (Wang et al., 2025a) Video Game 100, 5 20 multi-agent\n68 GameArena (Hu et al., 2024) Text Game 10, 5 20 single-agent\n69 TEXTARENA (Guertler et al., 2025) Text Game 10, 500 100 multi-agent\n70 MinePlanner (Hill et al., 2023) Video Game 100, 5 50 single-agent\n71 GAMEBENCH (Costarelli et al., 2024) Text Game 250 10 single-agent\n72 GTBENCH (Duan et al., 2024) Text Game 50 10 multi-agent\n73 ING-VP (Zhang et al., 2024a) Text Game, Video Game 50 300 single-agent\n74 RoCoBench (Mandi et al., 2024) Embodied 20, 5 10 multi-agent\n75 VillagerAgent (Dong et al., 2024) Video Game 500 200 multi-agent\n76 LLMARENA (Chen et al., 2024a) Text Game 10, 200 10 multi-agent\n77 CivRealm (Qi et al., 2024) Video Game 1000 10 multi-agent\n78 SmartPlay (Wu et al., 2024b) Video Game 100, 100000, 200, 5 10 single-agent\n79 MAgIC (Xu et al., 2024c) Card Game 20, 5 5 multi-agent\n80 AgentBoard (Ma et al., 2024a) Embodied, Software, Text Game, Web 10, 20, 50 10 single-agent\n81 AGENTGYM (Xi et al., 2024) Embodied, Software, Text Game, Web 20 100 single-agent\n82 Welfare Diplomacy (Mukobi et al., 2022) Card Game 1, 10 1 multi-agent\n83 Werewolf Arena (Bailis et al., 2024) Card Game 10 5 multi-agent\n84 MiniWoB (Shi et al., 2017) Web 1, 10 100 single-agent\n85 MiniWoB++(Liu et al., 2018) Web 1, 10 100 single-agent\n86 WorkArena(Drouin et al., 2024) Web 10, 20 100 single-agent\n87 ManiSkill (Mu et al., 2021) Embodied 50 20 single-agent\n88 LIBERO (Liu et al., 2023) Embodied 100 100 single-agent\n89 RoboCasa (Nasiriany et al., 2024) Embodied 500 100 single-agent\n90 ARNOLD (Gong et al., 2023) Embodied 100 10 single-agent\n91 Rlbench (James et al., 2020) Embodied 200 100 single-agent\n92 Overcooked-AI (Carroll et al., 2019) Video Game 500 5 multi-agent\n93 CerealBar (Pérez-Rodríguez et al., 2023) Video Game 500 1 multi-agent\n94 LLM-Coordination (Agashe et al., 2023) Video Game 50, 500 5 multi-agent\n95 MineLand (Yu et al., 2024) Video Game 2000 5000 multi-agent\n96 APIBench (Peng et al., 2022) Software, Web 1 20000 single-agent\n97 ToolBench (Xu et al., 2023b) Software, Web 5 100000 single-agent\n98 API-Bank (Li et al., 2023b) Software, Web 2 200 single-agent\n99 ToolAlpaca (Tang et al., 2023) Software, Web 1 5000 single-agent\n100 T-EV AL (Chen et al., 2024e) Software, Web 5 20000 single-agent\n101 UltraTool (Huang et al., 2024b) Software, Web 20 5000 single-agent\n102 SheetCopilotBench (Li et al., 2023a) Software 10 200 single-agent\n103 InstructExcel (Payan et al., 2023) Software 10 5000 single-agent\n104 SheetRM (Chen et al., 2024c) Software 10 500 single-agent\n105 GAIA (Mialon et al., 2024) Software, Web 50 500 single-agent\n106 Mind2Web (Deng et al., 2023) Web 10 2000 single-agent\n107 WEBLINX (Lu et al., 2024) Web 50 2000 single-agent\n108 METAGUI (Sun et al., 2022) Software 5 1000 single-agent\n109 AITW (Rawles et al., 2023) Software 5 30000 single-agent\n110 PIXELHELP (Li et al., 2020) Software 5 200 single-agent\n(strategic planning, resource management, spatial understanding) that TextAtari’s diverse game\nselection enables.\nText-based environments like TextWorld (Côté et al., 2019) and EscapeBench (Qian et al., 2024)\noffer simplified worlds with limited state spaces and highly constrained action possibilities. Their\ndeliberately simplified environments lack the complexity, state space size, and causal depth of Atari\ngames. BabyAI-Text (Carta et al., 2023) similarly restricts itself to basic grid-world scenarios with\nlimited objects and interactions, creating artificially simplified planning problems.\n13\n--- Page 14 ---\nBenchmarks like MLE-Bench (Chan et al., 2024), RE-Bench (Wijk et al., 2024), and DISCOVERY-\nWORLD (Jansen et al., 2024) limit themselves to specialized domains (machine learning experiments\nand scientific discovery) that contain substantial human annotation and guidance. These embed-\nded hints and structured exploration spaces implicitly simplify the planning challenge compared\nto TextAtari’s more general game environments where agents must discover effective strategies\nindependently.\nCivRealm (Qi et al., 2024), while offering complex strategic gameplay, suffers from an extremely\nrestricted action space at each decision point combined with extensive domain-specific knowledge\nrequirements. The game’s built-in advisors and infrastructure for managing civilization development\nsignificantly reduce the actual reasoning burden on agents. Moreover, its specialized domain of civi-\nlization building lacks the diversity of reasoning types required by TextAtari’s varied environments.\nTextAtari overcomes these limitations by providing: (1) diverse environments spanning multiple\nreasoning types without domain specialization; (2) primitive action spaces that require genuine\nsequential decision-making without macro-action shortcuts; (3) natural language descriptions that\neliminate the need for specialized visual processing while preserving state information; (4) stan-\ndardized evaluation protocols across games with varying difficulty levels; and (5) true long-horizon\nchallenges where decisions have compounding consequences over tens of thousands of steps. These\nadvantages make TextAtari uniquely positioned to evaluate language agents’ capacity for extended\nreasoning in a way that existing benchmarks cannot match.\nB.3 Future Direction\nDespite TextAtari’s strengths in evaluating long-horizon reasoning, we acknowledge that existing\nbenchmarks offer valuable perspectives our work can benefit from. Complex video games like\nStarCraft II and Red Dead Redemption II present rich, visually grounded environments with emer-\ngent dynamics that more closely mirror real-world complexity. While their macro-action approach\nsimplifies decision horizons, these games capture strategic depth and environmental richness that\ncomplement TextAtari’s focus on extended reasoning. Similarly, realistic task environments like DIS-\nCOVERYWORLD and MLE-Bench provide domain-specific challenges that better represent special-\nized professional reasoning in scientific discovery and machine learning experimentation—contexts\nwhere language agents may ultimately provide significant value.\nWe view TextAtari not as a replacement for these specialized benchmarks but as addressing a specific\ngap in evaluating pure long-horizon reasoning capacity . Our future work aims to develop a\nmore comprehensive meta-benchmark that integrates these complementary strengths—combining\nTextAtari’s extended planning horizons with the visual grounding of complex video games, the\nspecialized reasoning of professional domains, and the embodied interaction of Minecraft-style\nenvironments. This integrated approach would enable more holistic evaluation of language agents\nacross multiple dimensions of intelligence: sustained reasoning over time, multimodal understanding,\nspecialized domain knowledge application, and adaptive real-time control—potentially revealing\ncapabilities and limitations that no single benchmark could identify in isolation.\nC More Task Details\nThis appendix provides comprehensive information about the Atari games used in our TextAtari\nbenchmark. The following tables present detailed classifications and descriptions of all 23 Atari\ngames included in our evaluation, organized by game category with specific challenges they pose for\nlanguage models.\nTables 6 and 7 classify the games into four major categories (Action Games, Puzzle and Strategy\nGames, Sports Games, and Arcade Classics) and highlight the specific cognitive challenges each\ngame presents for language models. These challenges are color-coded to indicate their primary\nnature: spatial reasoning, planning and strategy, partial observability, and temporal reasoning. This\nclassification helps illuminate why certain games prove more difficult for language agents than others,\nand which cognitive capabilities are most critical for success across different game environments.\nTables 8-11 provide comprehensive descriptions of each game’s mechanics, objectives, and distinc-\ntive features. These detailed descriptions offer context for understanding the complexity of each\nenvironment and the specific demands they place on decision-making agents. The descriptions cover\n14\n--- Page 15 ---\naspects such as control mechanisms, scoring systems, hazards, progression structures, and unique\ngameplay elements that distinguish each title.\nTogether, these tables provide a thorough understanding of the task space encompassed by TextAtari,\nhighlighting the diverse challenges that make this benchmark particularly valuable for evaluating\nlong-horizon reasoning in language agents. The wide variety of game mechanics, objectives, and\ndifficulty levels ensures that TextAtari tests a broad spectrum of reasoning capabilities essential for\nadvanced sequential decision-making.\nD Prompt Engineering\nThis section provides detailed information about the prompt templates used across our TextAtari\nexperimental framework. These prompts determine how information is presented to the language\nmodels and how reasoning is structured during gameplay.\nD.1 Scenario-Based Prompts\nWe developed four distinct scenario-based prompts to systematically investigate how different forms\nof prior knowledge affect language agent performance in long-horizon game environments.\nBasic\nYou are an expert-level game player. Your whole response should be in\nJSON format.\nYou are in a game. {game_description} {goal_description}\nCurrently, {state_description}. {action_description}\nPlease suggest one valid action identifier. Your Suggested Action is:\nThe Basic scenario provides minimal information, representing our control condition. The prompt\nincludes only essential game elements: a brief game description, goal statement, current state\ndescription, and available actions. This lean template forces the language model to rely entirely on its\nintrinsic knowledge and reasoning capabilities without external guidance. By intentionally omitting\nadditional context, demonstrations, or strategic hints, we establish a baseline measurement of the\nmodel’s inherent game-playing abilities.\nObscured\nYou are an expert-level game player. Your whole response should be in\nJSON format.\nYou are in a game. {game_description} {goal_description}\nCurrently, {masked_state_description}. {action_description}\nPlease suggest one valid action identifier. Your Suggested Action is:\nThe Obscured scenario tests the model’s reliance on domain-specific terminology and semantic priors.\nThis prompt systematically replaces all game-specific nouns (e.g., “ghost,” “paddle,” “asteroid”)\nwith the generic token “item” while preserving structural information like coordinates, colors, and\nscores. This transformation forces the model to reason about game dynamics based purely on spatial\nrelationships and numeric patterns rather than leveraging pre-trained associations with familiar game\nelements. The contrast between Basic and Obscured performance reveals how much models depend\non semantic understanding versus structural reasoning.\nThe Manual Augmentation scenario supplements the basic prompt with explicit game knowledge\nthrough official game manual excerpts. We curate these manual snippets from authentic Atari\ndocumentation (primarily sourced from AtariAge archives), limiting them to approximately 300\n15\n--- Page 16 ---\nManual Augmentation\nYou are in a game. {game_description} {goal_description}\nThis is the game manual for this game. You need to read it carefully and\nunderstand the content and play strategies of the game:\n{manual_excerpt}\ntokens to ensure consistency across games. These excerpts provide formal explanations of game\nmechanics, scoring systems, strategic considerations, and win conditions. This condition tests whether\nexplicit domain knowledge injection enhances performance and to what degree different language\nmodels can operationalize written instructions into effective gameplay strategies.\nReference-based\nYou are in a game. {game_description} {goal_description}\nThis is the trajectory of playing this game using the RL algorithm.\nPlease read these trajectories carefully and refer to them to make\ndecisions during gameplay:\n[{state_1} -> {action_1}; {state_2} -> {action_2}; ...]\nThe Reference-based scenario provides the language model with exemplar gameplay through expert\ndemonstrations. For each game, we include trajectory samples from a trained Proximal Policy\nOptimization (PPO) agent that achieves at least average human-level performance. These trajectories\nare sampled at regular intervals (every 10th state-action pair) and formatted as state-action sequences,\ngiving the language model concrete examples of successful gameplay without directly encoding future\nknowledge. This condition examines how effectively language models can learn from demonstrations\nand adapt observed strategies to novel game states.\nD.2 Agent Reasoning Frameworks\nBeyond the knowledge variation in scenarios, we implemented three distinct reasoning frameworks\nthat structure how language models approach decision-making.\nBasic Agent Prompt\n{state_description}.{action_description}\nPlease suggest an action based on the current game state and the\ninformation you get.\nYou must select the appropriate action from the given action descriptions\nand cannot refrain from taking action or perform any prohibited\nactions.\nYour Suggested Action is:\nThe Basic Agent employs the simplest decision-making process, requesting a direct action selection\nwithout intermediate reasoning steps. The prompt instructs the model to suggest a valid action\nbased on the current game state, emphasizing the need to select from available actions without\nabstention. This framework tests the model’s ability to make intuitive decisions without explicit\nreasoning prompts, relying on the model’s internal processing to map observations directly to actions.\n16\n--- Page 17 ---\nChain-of-Thought Agent Prompt\nCurrently, {state_description}. {action_description}\nNow select your action. You should first take a deep breath. Then you\nshould think step by step about the action selection and lay out your\nthought process explicitly. After that you should decide an action\nbased on the thought. For the whole response, you should use JSON\nformat with two keys \"thought process\" and \"action\".\nThe Chain-of-Thought (CoT) Agent introduces explicit reasoning into the decision process by\nrequiring the model to articulate its thought process before selecting an action. The prompt instructs\nthe model to “take a deep breath” and “think step by step,” encouraging deliberate consideration of\nthe current state, available actions, and potential outcomes. The response must follow a structured\nJSON format with separate fields for the thought process and final action selection. This framework\ntests whether explicit reasoning improves decision quality in long-horizon gaming environments.\nReflection Agent Prompt\nYou are an analytic and game coach. You need to analyse the game and\nsummarize the current strategy step by step.\nYou will be given the history of a past experience in which you were\nplaced in an environment and given a task to complete. You were\nunsuccessful in completing the task.\nDo not summarize your environment, but rather think about the strategy\nand path you took to attempt to complete the task. Think step by step\nwhat mistakes you made leading the failure.\nThen devise a concise, new plan of action that accounts for your mistake\nwith reference to specific actions that you should have taken.\nFor example, if you tried A and B but forgot C, then you should reason\nthat forgetting C is the key mistake.\nAfter that, devise a plan to achieve C with environment-specific actions.\nRemind yourself of the plan you will take in the next trial and give your\nplan after \"Plan\".\nRespond in JSON with four keys: \"Strategy\", \"Knowledge\", \"Reflexion\", and\n\"New Plan\".\nThe Reflection Agent extends the CoT framework by incorporating episodic self-critique and strate-\ngic adaptation. After each game episode concludes, this agent prompts the model to analyze its\nperformance, identify mistakes, extract lessons, and formulate an improved strategy for subsequent\nattempts. The reflection must follow a structured JSON format covering strategy analysis, knowledge\nupdates, specific reflection on failures, and a concrete new plan. These reflections are preserved and\npresented to the model at the beginning of subsequent episodes, creating a feedback loop that enables\niterative improvement without parameter updates. This framework tests whether meta-cognitive\nprocesses enhance performance over extended interactions.\nThe combination of these scenario-based knowledge variations and reasoning frameworks creates\na comprehensive experimental matrix for evaluating language models’ capabilities in long-horizon\ndecision-making tasks. By systematically controlling these variables while maintaining consistent\nevaluation protocols, we can isolate the specific contributions of different knowledge sources and\nreasoning strategies to overall performance.\nE Missing Results\nIn this section, we provide a comprehensive set of additional results and visualizations that extend the\nanalysis presented in the main paper. These figures offer a detailed performance comparison across\n17\n--- Page 18 ---\nthe three dimensions of our experimental framework: model architectures (Qwen2.5-7B, Llama3.1-\n8B, and Gemma-7B), scenario types (Basic, Obscured, Game Manual, and Reference-based), and\nagent frameworks (Naive, Chain-of-Thought, Reflexion_last, and Reflexion_max).\nNomenclature Note. For clarity, we should note that throughout this appendix, the visualization labels\nuse “RL Trajectory” to refer to what we call the “Reference-based” scenario in the main text. This\nnaming discrepancy reflects implementation details, as the Reference-based scenario was implemented\nusing trajectories generated by reinforcement learning (PPO) agents. The underlying experimental\ncondition remains consistent with the Reference-based scenario described in the methodology section.\nE.1 Experimental Framework and Visualization Structure\nOur experimental framework explores three critical dimensions. First, we evaluate three state-of-\nthe-art open-source small-scale LLMs - Qwen2.5-7B, Llama3.1-8B, and Gemma-7B. These models\nrepresent different architectural approaches, training methodologies, and capabilities, allowing us to\nexamine how fundamental model design affects performance on long-horizon sequential decision-\nmaking tasks.\nSecond, we investigate four distinct knowledge conditions. The Basic scenario provides only the\nessential game description and current observation. The Obscured scenario replaces domain-specific\nnouns with neutral tokens to test reliance on lexical priors. The Game Manual scenario supplements\nthe Basic scenario with concise game manual excerpts. The Reference-based scenario (labeled as\n“RL Trajectory” in figures) primes the agent with expert demonstrations generated by reinforcement\nlearning algorithms.\nThird, we implement four prompting strategies. The Naive agent employs a zero-shot approach with\nminimal prompting. The Chain-of-Thought (CoT) agent encourages step-by-step reasoning before\nmaking decisions. The Reflexion_last agent incorporates the most recent episode reflection to guide\ncurrent gameplay. The Reflexion_max agent utilizes the best-performing reflection from previous\nepisodes to optimize decision-making.\nEach figure in this appendix presents a systematic comparison while holding two dimensions fixed\nand varying the third. The visualizations use a consistent format wherein bar charts represent the\nrelative performance differences (percentage change) across all 23 Atari environments, while vertical\nlines indicate the normalized average performance scores for each configuration. Each row contains\nmultiple pairwise comparisons, allowing for direct assessment of performance trends across different\nexperimental conditions.\nThe figures are organized into three major categories. The first 12 figures examine how different\nknowledge conditions affect performance while keeping the model architecture and agent framework\nconstant. The second 12 figures explore how different prompting strategies affect performance while\nkeeping the model architecture and scenario type constant. The left figures investigate how different\nmodel architectures perform while keeping the scenario type and agent framework constant.\nE.2 Key Observations\nThe visualization structure allows us to highlight several important experimental design elements.\nThe comparative analysis between Basic, Obscured, Game Manual, and Reference-based scenarios\nreveals how different forms of prior knowledge impact performance. By comparing these scenarios\nacross models and agent types, we can isolate the relative importance of domain-specific vocabulary,\nexplicit rule knowledge, and expert demonstrations in guiding language model decision-making.\nThe comparison between Naive, CoT, Reflexion_last, and Reflexion_max agents helps determine\nwhether explicit reasoning strategies and reflection mechanisms provide consistent benefits across\ndifferent games and knowledge conditions. This analysis is particularly important for understanding\nhow different forms of prompted reasoning affect long-horizon decision-making capabilities.\nBy comparing Qwen2.5-7B, Llama3.1-8B, and Gemma-7B under identical conditions, we can\nexamine whether architectural differences lead to systematic performance variations in long-horizon\nsequential decision-making. These comparisons allow us to assess whether certain model architectures\nare inherently better suited to particular types of reasoning required by different Atari games.\n18\n--- Page 19 ---\nFigure 8: Performance comparison of Qwen2.5-7B with Naive agent across different scenarios. Each\nplot shows relative performance differences (bars) and normalized average scores (vertical lines)\nacross all 23 Atari environments. Top row: Comparison between (left) Basic vs. RL Trajectory,\n(middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios. Bottom row: Comparison\nbetween (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL Trajectory, and (right)\nObscured vs. Game Manual scenarios.\nFigure 9: Performance comparison of Qwen2.5-7B with Chain-of-Thought (CoT) agent across\ndifferent scenarios. Each plot shows relative performance differences (bars) and normalized average\nscores (vertical lines) across all 23 Atari environments. Top row: Comparison between (left) Basic\nvs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios.\nBottom row: Comparison between (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL\nTrajectory, and (right) Obscured vs. Game Manual scenarios.\nThe visualization of performance across all 23 Atari environments highlights game-specific challenges\nand reveals which combinations of models, scenarios, and agent frameworks are most effective for\nparticular types of games. This fine-grained analysis helps identify specific strengths and weaknesses\nacross different experimental configurations.\n19\n--- Page 20 ---\nFigure 10: Performance comparison of Qwen2.5-7B with Reflexion_last agent (using the most recent\nreflection) across different scenarios. Each plot shows relative performance differences (bars) and\nnormalized average scores (vertical lines) across all 23 Atari environments. Top row: Comparison\nbetween (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs.\nObscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL Trajectory,\n(middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nFigure 11: Performance comparison of Qwen2.5-7B with Reflexion_max agent (using the best-\nperforming reflection) across different scenarios. Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right)\nBasic vs. Obscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL\nTrajectory, (middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nE.3 Methodological Considerations\nThese extended results should be interpreted with several methodological considerations in mind.\nThe relative performance differences across individual games demonstrate the inherent variability in\nsequential decision-making tasks, suggesting that no single approach is universally optimal across\nall game environments. This variability underscores the importance of evaluating language agent\nperformance across diverse task types.\n20\n--- Page 21 ---\nFigure 12: Performance comparison of Llama3.1-8B with Naive agent across different scenarios.\nEach plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) Basic vs. RL Trajectory,\n(middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios. Bottom row: Comparison\nbetween (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL Trajectory, and (right)\nObscured vs. Game Manual scenarios.\nFigure 13: Performance comparison of Llama3.1-8B with Chain-of-Thought (CoT) agent across\ndifferent scenarios. Each plot shows relative performance differences (bars) and normalized average\nscores (vertical lines) across all 23 Atari environments. Top row: Comparison between (left) Basic\nvs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios.\nBottom row: Comparison between (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL\nTrajectory, and (right) Obscured vs. Game Manual scenarios.\nAll comparative analyses use one configuration as a baseline, allowing for direct assessment of\nrelative improvements or degradations. This approach enables meaningful comparisons across\ndifferent experimental conditions while controlling for game-specific factors that might otherwise\nconfound interpretation.\nAs noted in the main text, the substantial computational demands of these experiments necessitated\ncertain practical limitations, including a reduced evaluation horizon of 1,000 steps rather than the\n21\n--- Page 22 ---\nFigure 14: Performance comparison of Llama3.1-8B with Reflexion_last agent (using the most recent\nreflection) across different scenarios. Each plot shows relative performance differences (bars) and\nnormalized average scores (vertical lines) across all 23 Atari environments. Top row: Comparison\nbetween (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs.\nObscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL Trajectory,\n(middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nFigure 15: Performance comparison of Llama3.1-8B with Reflexion_max agent (using the best-\nperforming reflection) across different scenarios. Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right)\nBasic vs. Obscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL\nTrajectory, (middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nfull 100,000 steps. While this reduction allows for broader experimental coverage, it may not fully\ncapture the challenges of extremely long-horizon reasoning.\nPerformance on Atari games should be considered in the context of the specific challenges they\npresent (spatial reasoning, planning, partial observability, and temporal reasoning) rather than as a\ngeneral measure of language model capability. These games were selected specifically because they\nexercise different aspects of decision-making that are relevant to long-horizon planning.\n22\n--- Page 23 ---\nFigure 16: Performance comparison of Gemma-7B with Naive agent across different scenarios. Each\nplot shows relative performance differences (bars) and normalized average scores (vertical lines)\nacross all 23 Atari environments. Top row: Comparison between (left) Basic vs. RL Trajectory,\n(middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios. Bottom row: Comparison\nbetween (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL Trajectory, and (right)\nObscured vs. Game Manual scenarios.\nFigure 17: Performance comparison of Gemma-7B with Chain-of-Thought (CoT) agent across\ndifferent scenarios. Each plot shows relative performance differences (bars) and normalized average\nscores (vertical lines) across all 23 Atari environments. Top row: Comparison between (left) Basic\nvs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs. Obscured scenarios.\nBottom row: Comparison between (left) Game Manual vs. RL Trajectory, (middle) Obscured vs. RL\nTrajectory, and (right) Obscured vs. Game Manual scenarios.\nThe comprehensive nature of these visualizations allows for nuanced analysis of the interplay between\nmodel architectures, knowledge conditions, and prompting strategies in long-horizon sequential\ndecision-making tasks. These visualizations provide valuable qualitative insights into the relative\nstrengths and weaknesses of different approaches across the TextAtari benchmark. Together, they\noffer a foundation for understanding how different factors contribute to language agent performance\non extended planning horizons.\n23\n--- Page 24 ---\nFigure 18: Performance comparison of Gemma-7B with Reflexion_last agent (using the most recent\nreflection) across different scenarios. Each plot shows relative performance differences (bars) and\nnormalized average scores (vertical lines) across all 23 Atari environments. Top row: Comparison\nbetween (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right) Basic vs.\nObscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL Trajectory,\n(middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nFigure 19: Performance comparison of Gemma-7B with Reflexion_max agent (using the best-\nperforming reflection) across different scenarios. Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Basic vs. RL Trajectory, (middle) Basic vs. Game Manual, and (right)\nBasic vs. Obscured scenarios. Bottom row: Comparison between (left) Game Manual vs. RL\nTrajectory, (middle) Obscured vs. RL Trajectory, and (right) Obscured vs. Game Manual scenarios.\nF Border Impact\nTextAtari introduces a benchmark for evaluating language agents on extremely long-horizon decision-\nmaking tasks, carrying various societal implications that warrant careful consideration. By estab-\nlishing a rigorous evaluation standard for long-horizon reasoning, this benchmark may accelerate\nprogress in temporal reasoning and strategic planning while highlighting the substantial gap between\n24\n--- Page 25 ---\nFigure 20: Performance comparison of Qwen2.5-7B in the Basic scenario across different agent types.\nEach plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\nFigure 21: Performance comparison of Qwen2.5-7B in the Obscured scenario across different agent\ntypes. Each plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\ncurrent AI systems and human capabilities. However, the framework’s substantial computational\ndemands raise important questions about research accessibility, environmental impact, and potential\nexacerbation of existing disparities in AI research.\nAdvances in long-horizon reasoning capabilities could transfer to beneficial applications across\ndomains requiring extended planning, such as logistics optimization and healthcare coordination.\nSimultaneously, these same capabilities might enable more sophisticated autonomous systems for\n25\n--- Page 26 ---\nFigure 22: Performance comparison of Qwen2.5-7B in the Game Manual scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\nFigure 23: Performance comparison of Qwen2.5-7B in the RL Trajectory scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\npotentially harmful applications, including surveillance, automated disinformation campaigns, or\nautonomous weapons systems. There’s also risk that optimizing for performance on game-based\nbenchmarks could prioritize capabilities that don’t transfer well to more nuanced real-world contexts\ninvolving ethical considerations, cultural sensitivity, or human welfare concerns.\nTextAtari’s findings regarding the performance gap between language agents and humans in extended\nreasoning tasks could inform more effective human-AI collaboration frameworks, potentially leading\n26\n--- Page 27 ---\nFigure 24: Performance comparison of Llama3.1-8B in the Basic scenario across different agent types.\nEach plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\nFigure 25: Performance comparison of Llama3.1-8B in the Obscured scenario across different agent\ntypes. Each plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\nto more productive partnerships rather than full automation approaches in complex domains. The\nbenchmark’s design, which requires explicit reasoning traces in some agent configurations, encourages\nmore interpretable AI systems and enables analysis of how language models construct and maintain\ninternal representations over time.\nWhile TextAtari itself represents a controlled research environment with minimal direct risk, the capa-\nbilities it aims to measure and advance have significant implications for AI development trajectories.\n27\n--- Page 28 ---\nFigure 26: Performance comparison of Llama3.1-8B in the Game Manual scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\nFigure 27: Performance comparison of Llama3.1-8B in the RL Trajectory scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\nThe techniques developed to improve performance could be applied in both beneficial and harmful\ncontexts—enhancing assistive technologies for individuals with cognitive impairments, but also\npotentially enabling more sophisticated autonomous systems for cyber attacks or manipulation. We\nencourage ongoing ethical reflection and governance discussions regarding long-horizon reasoning in\nautonomous systems as this research area progresses.\n28\n--- Page 29 ---\nFigure 28: Performance comparison of Gemma-7B in the Basic scenario across different agent types.\nEach plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\nFigure 29: Performance comparison of Gemma-7B in the Obscured scenario across different agent\ntypes. Each plot shows relative performance differences (bars) and normalized average scores (vertical\nlines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs. Reflexion_last,\n(middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row: Comparison\nbetween (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right) Reflexion_last\nvs. Reflexion_max agents.\n29\n--- Page 30 ---\nTable 5: Comprehensive language agent benchmark collection for sequential decision making (Part\nIII). This table catalogs benchmarks for evaluating language agents across diverse domains. Each\nbenchmark is characterized by its domain category (e.g., Video Game, Web, Software, Text Game,\nCard Game, Embodied), decision horizon (number of steps required for task completion), number of\ndistinct tasks, and agent type (single-agent or multi-agent). This collection contextualizes TextAtari’s\ncontribution to the benchmark landscape, particularly in addressing the challenge of very long-horizon\ndecision-making tasks (up to 100,000 steps) for language agents.\nID Name Category Horizon Tasks Agent Type\n111 OmniAct (Kapoor et al., 2024) Software, Web 5 10000 single-agent\n112 InterCode (Yang et al., 2023) Software 10 1000 single-agent\n113 VisualWebArena (Koh et al., 2024) Web 50 1000 single-agent\n114 Mobile-Env (Zhang et al., 2023) Software, Web 10 200 single-agent\n115 AssistGUI (Gao et al., 2023) Software 5 100 single-agent\n116 ScienceWorld (Wang et al., 2022) Video Game 100 50 single-agent\n117 MoTIF (Klissarov et al., 2023) Software 20 5000 single-agent\n118 MLAgentBench (Huang et al., 2024a) Software 20 10 single-agent\n119 Spider 2.0 (Lei et al., 2025) Software 50 500 single-agent\n120 MetaTool (Wang et al., 2024d) Software, Web 5 20000 single-agent\n121 DDD (Wu et al., 2024a) Card Game 10 5 multi-agent\n122 AvalonBench (Light et al., 2023) Card Game 20 1 multi-agent\n123 SOTOPIA (Zhou et al., 2024b) Text Game 20 500 multi-agent\n124 ToolEmu (Ruan et al., 2024) Software 10 100 single-agent\n125 MiniGrid (Chevalier-Boisvert et al., 2023) Video Game 100 20 single-agent\n126 Alfred (Shridhar et al., 2020a) Embodied 50 5 single-agent\n127 NetHack LE (Küttler et al., 2020) Text Game 100000 10 single-agent\n128 Alchemy (Chen et al., 2019) Video Game 200 1 single-agent\n129 IVRE (Xu et al., 2023a) Video Game 10 1 single-agent\n130 UGIF (Venkatesh et al., 2022) Software 5 500 single-agent\n131 WebV oyager (He et al., 2024) Web 500 20 single-agent\n132 AMEX (Chai et al., 2024) Software 10 3000 single-agent\n133 AndroidArena (Xing et al., 2024) Software 10 200 single-agent\n134 AndroidLab (Xu et al., 2024d) Software 30 100 single-agent\n135 ARA (Alghamdi et al., 2024) Software 5 10 single-agent\n136 AsyncHow (Lin et al., 2024a) Text Game 10 2000 single-agent\n137 VirtualHome (Puig et al., 2018) Embodied 100 3000 single-agent\n138 WAH (Puig et al., 2021) Embodied 250 1000 multi-agent\n139 VideoWebArena (Jang et al., 2024) Web 20 2000 single-agent\n140 HandMeThat (Wan et al., 2022) Embodied 50 30000 multi-agent\n141 DialFRED (Gao et al., 2022) Embodied 50 30000 multi-agent\n142 TEACh (Padmakumar et al., 2022) Embodied 100 5000 multi-agent\n143 LIGHT (Urbanek et al., 2019) Text Game 200 10000 multi-agent\n144 Diplomacy (Bakhtin et al., 2021) Card Game 100 1 multi-agent\n145 AppWorld (Trivedi et al., 2024) Software 20 1000 single-agent\n146 ToolLLM (Qin et al., 2024) Software, Web 5 10000 single-agent\n148 ToolQA (Zhuang et al., 2023) Software, Web 5 2000 single-agent\n149 ToolLens (Qu et al., 2024) Software, Web 5 20000 single-agent\n150 Crafter (Hafner, 2022) Video Game 1000 20 single-agent\n151 Baba is AI (Cloos et al., 2024) Video Game 100 5 single-agent\n152 MiniHack (Samvelyan et al., 2021) Text Game 100 100 single-agent\n153 MLE-Bench (Chan et al., 2024) Software 2000 100 single-agent\n154 RE-Bench (Wijk et al., 2024) Software 5000 10 single-agent\n155 ScienceAgentBench (Chen et al., 2024d) Software 10 100 single-agent\n156 LlamaTouch (Zhang et al., 2024b) Software 50 500 single-agent\n157 AgentStudio (Zheng et al., 2024) Software 10 200 single-agent\n158 RoboGen (Wang et al., 2024e) Embodied 10 100 single-agent\n159 Clembench (Chalamalasetti et al., 2023) Text Game 10 200 multi-agent\n160 LMRL-Gym (Abdulhai et al., 2023) Text Game 100 10 multi-agent\n161 Game-theoretic LLM (Hua et al., 2024) Text Game 20 10 multi-agent\n162 LAMEN (Davidson et al., 2024) Text Game 10 5 multi-agent\n163 SPIN-Bench (Yao et al., 2025) Text Game 50 5 multi-agent\n30\n--- Page 31 ---\nAtari Games Classification and LLM Gaming Challenges (Part 1)\nGame Category Challenges for LLM\nAction Games\nAsteroids Space ShooterSpatial reasoning for circular topology\nReaction-based gameplay timing\nContinuous state space navigation\nStrategic target prioritization\nBattleZone First-Person Tank3D spatial reasoning\nFirst-person partially observed environment\nStrategic target selection\nSituational awareness maintenance\nBerzerk Maze ShooterNavigating complex maze layouts\nDynamic obstacle avoidance\nTime pressure (Evil Otto pursuit)\nMultitasking (walls, enemies, bullets)\nDemonAttack Space ShooterPattern recognition in enemy waves\nPrediction of enemy movement patterns\nProgressive difficulty adaptation\nTiming of defensive movements\nHero ExplorationResource management (dynamite)\nComplex navigation in caverns\nLong-term planning for rescue\nNon-linear exploration paths\nPitfall PlatformerPrecise timing for obstacles\nComplex movement patterns\nLong horizon optimization (20-minute game-\nplay)\nRisk/reward assessment for treasure collection\nPuzzle and Strategy Games\nBreakout Brick-breakerGeometry understanding\nBall trajectory prediction\nStrategic brick targeting\nTiming-sensitive paddle control\nFrostbite BuildingPlanning sequences under time constraints\nRisk assessment with moving platforms\nMulti-objective optimization (ice blocks vs.\nsafety)\nTiming jumps between ice floes\nMontezumaRevenge Puzzle PlatformerExtremely sparse rewards\nLong causal chains\nComplex dependency hierarchies\nPrecise timing for trap avoidance\nPrivateEye DetectiveSparse reward structure\nLong-term memory requirements\nNon-linear gameplay\nLarge state space navigation\nQbert PuzzleIsometric spatial reasoning\nPlanning efficient color-changing pathways\nTrap and enemy avoidance\nProgressive difficulty adaptation\nTable 6: Detailed Analysis of Atari Games and Their LLM Challenges (Part 1). This table presents\naction and puzzle games with their specific challenges for LLMs. Color coding indicates challenge\ntypes: spatial reasoning , planning & strategy , partial observability , and temporal reasoning .\n31\n--- Page 32 ---\nAtari Games Classification and LLM Gaming Challenges (Part 2)\nGame Category Challenges for LLM\nSports Games\nBowling SportsPhysics understanding\nPrecise parameter control\nAdapting to pin configurations\nOptimizing for strike probability\nBoxing SportsAdversarial reasoning\nPredicting opponent movements\nTactical positioning\nTiming attack and defense moves\nPong SportsContinuous control optimization\nAnticipating ball trajectory\nTiming paddle movements\nOptimizing paddle positioning\nSkiing SportsPrecise timing for gates\nPath planning with fixed obstacles\nSpeed/accuracy trade-offs\nLong-term performance optimization\nTennis SportsPositioning for shots and court coverage\nAnticipating opponent strategy\nShot selection and placement\nBalancing offensive and defensive play\nArcade Classics\nFreeway Obstacle AvoidanceTiming road crossings\nPattern recognition in traffic flow\nBinary sparse reward navigation\nRisk assessment under time pressure\nMsPacman MazeDynamic path planning\nGhost behavior modeling (different behaviors)\nStrategic power pellet usage\nAdapting to maze layout variations\nRiverraid Scrolling ShooterResource management (fuel)\nPrioritizing various hazard types\nAdapting to increasing difficulty\nNavigating narrow passages\nSeaquest Underwater ShooterResource management (oxygen)\nMulti-objective balancing (rescue, combat,\nsurvival)\nBidirectional threat assessment\nRisk/reward decisions for surfacing\nSpaceInvaders Space ShooterStrategy shifts based on remaining enemies\nManaging defensive shelters\nAdapting to increasing enemy speed\nSpatial awareness for shelter usage\nVenture Dungeon CrawlerRoom-to-room navigation strategy\nPartially observed state\nRisk/reward assessment for treasures\nEnemy pattern recognition\nVideoPinball SimulationPhysics prediction\nTiming-based flipper control\nUnderstanding complex scoring mechanisms\nLong-term strategy for high scores\nTable 7: Detailed Analysis of Atari Games and Their LLM Challenges (Part 2). This table\npresents sports games and arcade classics with their specific challenges for LLMs. Color cod-\ning indicates challenge types: spatial reasoning ,planning & strategy ,partial observability , and\ntemporal reasoning .\n32\n--- Page 33 ---\nDetailed Description of Atari Games (Part 1)\nGame Detailed Description\nAsteroids A space-themed shooter in vector graphics where the player controls a trian-\ngular ship in an asteroid field. The player must shoot and destroy asteroids\nwhile avoiding collisions. As asteroids are destroyed, they break into smaller\npieces, creating more hazards. Occasionally, flying saucers appear and shoot\nat the player. The ship has momentum in a zero-gravity environment, re-\nquiring strategic thruster control and rotation. Players can hyperspace to a\nrandom location when in danger, though this carries risk.\nBattleZone One of the first 3D tank combat games using vector graphics to create a first-\nperson perspective. Players control a tank on a flat plain with mountains in\nthe background and various obstacles like blocks and pyramids. Enemy tanks,\nmissiles, and flying saucers attack the player, requiring strategic positioning\nand aiming. A radar display helps locate enemies outside the visible field.\nThe realistic control scheme requires separate controls for driving and turret\nrotation, demanding sophisticated spatial coordination.\nBerzerk A multi-directional shooter set in a maze of interconnected rooms. The player\nnavigates through rooms filled with robots that shoot at the player. Touching\nrobots, their bullets, or the electrified walls results in death. After a short time\nin any room, \"Evil Otto\" - an indestructible bouncing smiley face - appears\nand pursues the player, forcing quick movement to the next room. The robots’\nspeech synthesis (\"The humanoid must not escape!\") was groundbreaking for\nits time. Each maze is procedurally generated, creating effectively endless\ngameplay.\nBowling A simulation of ten-pin bowling where players control the position and\ncurvature of the ball’s path. The player can position their bowler horizontally,\nset the ball’s curve, and time the release for optimal accuracy. The game\naccurately models pin physics, with realistic pin scatter and knockdown\npatterns. Players compete across 10 frames, aiming for strikes and spares to\nmaximize their score. Different pin configurations after the first throw require\nadaptive targeting strategies for picking up spares.\nBoxing A top-down boxing simulation where two boxers face off in a ring. The player\ncontrols a boxer trying to land punches on the opponent while avoiding being\nhit. Players need to strategically position themselves, time their punches, and\nguard against counterattacks. The game has a three-minute time limit, and the\nwinner is determined by either knockout or points scored through successful\nhits. Different punching angles and positions result in varying effectiveness,\nrequiring strategic positioning and timing.\nBreakout A brick-breaking game where the player controls a paddle at the bottom of\nthe screen to bounce a ball upward into layers of bricks. When hit, bricks\ndisappear, and the ball bounces back. The goal is to eliminate all bricks\nwithout letting the ball pass the paddle. As more bricks are destroyed, the\nball moves faster. Some versions feature multi-colored brick layers with\nhigher-value bricks at the top. Breaking through to the top allows the ball to\nbounce around the top edge, potentially clearing many bricks rapidly.\nDemonAttack A fixed-shooter game where players control a cannon at the bottom of the\nscreen defending against waves of demons descending from the top. Each\nwave features demons with different movement patterns, attack strategies, and\npoint values. Some demons split into two when hit, while others drop bombs\nor dive-bomb the player. The player’s cannon can move horizontally and fire\nupward. As levels progress, demons become faster and more aggressive, with\nmore complex attack patterns. The game features distinctive sound effects\nand colorful graphics.\nTable 8: Detailed Descriptions of Selected Atari Games (Part 1). This table provides comprehensive\ndescriptions of seven classic Atari games, explaining their gameplay mechanics, objectives, and\ndistinctive features.\n33\n--- Page 34 ---\nDetailed Description of Atari Games (Part 2)\nGame Detailed Description\nFreeway A simple but challenging game where the player controls a chicken trying\nto cross a ten-lane freeway filled with traffic. The goal is to reach the other\nside as many times as possible within the time limit. Each lane has vehicles\nmoving at different speeds and directions. If the chicken collides with a\nvehicle, it is pushed backward. Players can only move up or down, requiring\nprecise timing to navigate through gaps in traffic. The game supports two-\nplayer competitive mode where players race to get their chickens across more\ntimes than their opponent.\nFrostbite An arctic-themed game where players control Frostbite Bailey, who must\nbuild an igloo by jumping on floating ice floes in a frigid river. Each time\nthe player lands on an ice floe, it changes color and adds a block to the igloo.\nOnce the igloo is complete, the player must reach it to advance to the next\nlevel. Hazards include bears, geese, and crabs that patrol the ice floes. The\ntemperature gradually drops, adding time pressure. Fish and clams appear as\nbonus items that can be collected for extra points.\nHero A complex adventure game where players control Roderick Hero, a miner\nequipped with a helicopter backpack, dynamite, and a beam weapon. The\nobjective is to navigate through multi-screen mine shafts to rescue trapped\nminers. Players must blast through walls with dynamite, defeat creatures\nwith the beam weapon, and avoid hazards like magma and falling rocks.\nThe helicopter backpack allows limited flight but consumes power. Lanterns\nthroughout the mine provide light in dark areas and extra power when col-\nlected. The game features complex, non-linear level designs.\nMontezumaRevenge A notoriously challenging platformer set in an Aztec temple. Players control\nan explorer named Panama Joe navigating through multiple screens of the\ntemple to collect treasures. The game features locked doors requiring keys,\ndeadly traps including fire pits and rolling skulls, and enemies like snakes\nand spiders. Players must use ladders, ropes, and disappearing floors to\nnavigate between rooms. The game is known for its punishing difficulty,\nsparse rewards, and the need for precise timing and planning. Death results\nin returning to the starting room, making progress particularly demanding.\nMsPacman An enhanced version of Pac-Man featuring a female protagonist. Players\nnavigate through four different maze designs consuming dots while avoiding\nghosts. Power pellets allow temporary ghost consumption. Ms. Pac-Man im-\nproved upon the original with more varied mazes, ghosts with less predictable\nmovement patterns, and bonus fruits that move around the maze rather than\nstaying stationary. Between levels, brief cutscenes tell the love story between\nMs. Pac-Man and Pac-Man. The game’s less deterministic ghost behavior\nmakes it more challenging and less susceptible to pattern-based strategies\nthan the original.\nPitfall A groundbreaking side-scrolling platformer where players control Pitfall\nHarry through a jungle collecting treasures within a 20-minute time limit. The\njungle consists of 255interconnected screens with various hazards including\nrolling logs, crocodiles, scorpions, quicksand, tar pits, and fires. Players\nnavigate by running, jumping, and swinging on vines. Underground passages\nallow for faster travel between areas. The game was revolutionary for its era\ndue to its large game world and fluid character animation. Points are scored\nby collecting treasures, with time penalties for falling into hazards.\nPong One of the earliest and most iconic video games, simulating table tennis.\nPlayers control vertical paddles on opposite sides of the screen and must hit\na ball back and forth. The ball bounces off the top and bottom edges, and\npoints are scored when one player fails to return the ball. The ball’s speed\nincreases after several successful returns, increasing difficulty. The angle of\nthe ball’s rebound depends on which part of the paddle it hits, allowing for\nstrategic aiming. Despite its simplicity, Pong established many foundational\nelements of video games and interactive entertainment.\nTable 9: Detailed Descriptions of Selected Atari Games (Part 2). This table provides comprehensive\ndescriptions of seven classic Atari games, explaining their gameplay mechanics, objectives, and\ndistinctive features.\n34\n--- Page 35 ---\nDetailed Description of Atari Games (Part 3)\nGame Detailed Description\nPrivateEye A detective adventure game where players control Private Eye Pierre Touché\nsolving cases by navigating a scrolling city environment. The main case\ninvolves recovering stolen items from the Goldfish Diamond case. Players\ndrive a car through the city, entering buildings, and collecting evidence while\navoiding gangsters and other hazards. The game features a complex scoring\nsystem based on catching criminals, recovering stolen items, and completing\nthe case within the time limit. Incorrect accusations result in penalties. The\ngame’s non-linear design with multiple locations to explore was innovative\nfor its time.\nQbert A puzzle-platformer where players control Q*bert, an orange creature with\na tubular nose, who hops around a pyramid of cubes. The objective is to\nchange the color of each cube’s top surface by hopping on it. Once all cubes\nare changed to the target color, the player advances to the next level. Enemies\ninclude Coily the snake, Wrong-Way and Ugg who travel along the sides of\nthe pyramid, and red balls that fall from the top. Q*bert can use floating discs\nto escape to the top of the pyramid or to lure Coily off the edge. Later levels\nrequire changing each cube’s color multiple times or in specific sequences.\nRiverraid A vertically scrolling shooter where players pilot a fighter jet over a river,\ndestroying enemy helicopters, ships, fuel depots, and bridges while avoiding\ncollisions with the shoreline. The plane consumes fuel continuously, requiring\nplayers to fly over fuel depots to refill. The river varies in width, creating\nnarrow passages that demand precise navigation. Destroying bridges marks\nprogression to new sections with increased difficulty. The game was notable\nfor its use of a pseudo-random algorithm to generate the river layout, creating\na different experience each play while using minimal memory.\nSeaquest An underwater shooter where players control a submarine that must rescue\ndivers while defending against sharks, enemy submarines, and other sea\ncreatures. The submarine can move in all directions and fire torpedoes hori-\nzontally. Players must manage their oxygen supply, surfacing periodically to\nreplenish it and deliver rescued divers. Each surfacing with a full complement\nof divers awards bonus points. If oxygen depletes completely, the submarine\nis lost. As levels progress, enemies become more numerous and aggressive.\nBalancing rescue operations with defense and oxygen management creates a\nmulti-objective challenge.\nSkiing A downhill skiing simulation where players navigate through a series of gates\non a continuously scrolling course. The objective is to complete the course in\nthe shortest time possible without missing gates, which incur time penalties.\nPlayers control their skier’s horizontal position as they automatically move\ndownward. Obstacles include trees and moguls that must be avoided. The\ngame offers two modes: the slalom, with wider gate spacing, and the more\nchallenging giant slalom with tighter gates. Precise control and planning the\noptimal line through gates are essential for achieving the best times.\nSpaceInvaders A fixed shooter game where players control a laser cannon moving horizon-\ntally at the bottom of the screen, defending against rows of descending aliens.\nThe aliens move side to side, dropping bombs as they advance toward the\nbottom. Players must eliminate all aliens before they reach the ground. Pro-\ntective bunkers provide temporary cover but degrade when hit by either player\nshots or alien bombs. As aliens are destroyed, the remaining invaders move\nfaster. Strategic gameplay involves targeting specific aliens to manipulate\ntheir movement patterns and using the bunkers effectively for protection.\nTennis A sports simulation of tennis from a side view of the court. Players control\ntennis players who can move around their side of the court and swing rackets\nto hit the ball over the net. The game implements basic tennis rules including\nserves, volleys, and scoring ( 15-30-40-game). Ball physics include appropri-\nate bouncing and speed changes based on the type of hit. Strategic gameplay\ninvolves positioning for shots, timing racket swings correctly, and anticipat-\ning opponent movements. The game can be played against the computer or\nanother human player, with varying difficulty levels for the AI opponent.\nTable 10: Detailed Descriptions of Selected Atari Games (Part 3). This table provides comprehen-\nsive descriptions of seven classic Atari games, explaining their gameplay mechanics, objectives, and\ndistinctive features.35\n--- Page 36 ---\nDetailed Description of Atari Games (Part 4)\nGame Detailed Description\nVenture An exploration game where players control Winky, an adventurer navigating\nthrough a multi-room dungeon to collect treasures. Each room contains\ndifferent monsters guarding treasure, requiring specific strategies to overcome.\nPlayers view the dungeon layout from an overhead perspective but transition\nto a zoomed-in view when entering a room. If players take too long in a room,\nthe invincible \"Hallmonster\" appears, forcing swift action. The game features\nfour different dungeons with increasing difficulty and unique monsters in\neach room, from snakes and trolls to giant spiders and the Grim Reaper.\nVideoPinball A digital recreation of pinball that simulates the physics and features of a\ntraditional pinball machine. Players control left and right flippers to keep the\nball in play, aiming to hit various targets to score points. The table includes\nbumpers, spinners, rollover targets, and bonus areas. Players can tilt the\ntable (with limits) to influence ball direction. The game features realistic ball\nphysics including momentum, ricochet angles, and speed changes. Special\nfeatures include multiball play and bonus rounds that can be activated through\nspecific target combinations. Scoring emphasizes both quick reflexes and\nstrategic target selection.\nTable 11: Detailed Descriptions of Selected Atari Games (Part 4). This table provides compre-\nhensive descriptions of the remaining classic Atari games, explaining their gameplay mechanics,\nobjectives, and distinctive features.\nFigure 30: Performance comparison of Gemma-7B in the Game Manual scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\n36\n--- Page 37 ---\nFigure 31: Performance comparison of Gemma-7B in the RL Trajectory scenario across different\nagent types. Each plot shows relative performance differences (bars) and normalized average scores\n(vertical lines) across all 23 Atari environments. Top row: Comparison between (left) CoT vs.\nReflexion_last, (middle) CoT vs. Reflexion_max, and (right) Naive vs. CoT agents. Bottom row:\nComparison between (left) Naive vs. Reflexion_last, (middle) Naive vs. Reflexion_max, and (right)\nReflexion_last vs. Reflexion_max agents.\nFigure 32: Cross-model performance comparison in the Basic scenario using Naive agent (top row)\nand Chain-of-Thought (CoT) agent (bottom row). Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-7B, and\n(right) Qwen2.5-7B vs. Llama3.1-8B using the Naive agent. Bottom row: Same model comparisons\nusing the CoT agent.\n37\n--- Page 38 ---\nFigure 33: Cross-model performance comparison in the Basic scenario using Reflexion_last agent\n(top row) and Reflexion_max agent (bottom row). Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-7B,\nand (right) Qwen2.5-7B vs. Llama3.1-8B using the Reflexion_last agent. Bottom row: Same model\ncomparisons using the Reflexion_max agent.\nFigure 34: Cross-model performance comparison in the Obscured scenario using Naive agent (top\nrow) and Chain-of-Thought (CoT) agent (bottom row). Each plot shows relative performance\ndifferences (bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top\nrow: Comparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-\n7B, and (right) Qwen2.5-7B vs. Llama3.1-8B using the Naive agent. Bottom row: Same model\ncomparisons using the CoT agent.\n38\n--- Page 39 ---\nFigure 35: Cross-model performance comparison in the Obscured scenario using Reflexion_last agent\n(top row) and Reflexion_max agent (bottom row). Each plot shows relative performance differences\n(bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top row:\nComparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-7B,\nand (right) Qwen2.5-7B vs. Llama3.1-8B using the Reflexion_last agent. Bottom row: Same model\ncomparisons using the Reflexion_max agent.\nFigure 36: Cross-model performance comparison in the Game Manual scenario using Naive agent\n(top row) and Chain-of-Thought (CoT) agent (bottom row). Each plot shows relative performance\ndifferences (bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top\nrow: Comparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-\n7B, and (right) Qwen2.5-7B vs. Llama3.1-8B using the Naive agent. Bottom row: Same model\ncomparisons using the CoT agent.\n39\n--- Page 40 ---\nFigure 37: Cross-model performance comparison in the Game Manual scenario using Reflexion_last\nagent (top row) and Reflexion_max agent (bottom row). Each plot shows relative performance\ndifferences (bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top\nrow: Comparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-7B,\nand (right) Qwen2.5-7B vs. Llama3.1-8B using the Reflexion_last agent. Bottom row: Same model\ncomparisons using the Reflexion_max agent.\nFigure 38: Cross-model performance comparison in the RL Trajectory scenario using Naive agent\n(top row) and Chain-of-Thought (CoT) agent (bottom row). Each plot shows relative performance\ndifferences (bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top\nrow: Comparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-\n7B, and (right) Qwen2.5-7B vs. Llama3.1-8B using the Naive agent. Bottom row: Same model\ncomparisons using the CoT agent.\n40\n--- Page 41 ---\nFigure 39: Cross-model performance comparison in the RL Trajectory scenario using Reflexion_last\nagent (top row) and Reflexion_max agent (bottom row). Each plot shows relative performance\ndifferences (bars) and normalized average scores (vertical lines) across all 23 Atari environments. Top\nrow: Comparison between (left) Llama3.1-8B vs. Gemma-7B, (middle) Qwen2.5-7B vs. Gemma-7B,\nand (right) Qwen2.5-7B vs. Llama3.1-8B using the Reflexion_last agent. Bottom row: Same model\ncomparisons using the Reflexion_max agent.\n41\n--- Page 42 ---\nAppendix References\nAbdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y ., Xu, K., and Levine, S. Lmrl\ngym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint\narXiv:2311.18232 , 2023.\nAgashe, S., Fan, Y ., Reyna, A., and Wang, X. E. Llm-coordination: evaluating and analyzing\nmulti-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903 ,\n2023.\nAlghamdi, E. A., Masoud, R. I., Alnuhait, D., Alomairi, A. Y ., Ashraf, A., and Zaytoon, M. Aratrust:\nAn evaluation of trustworthiness for llms in arabic. CoRR , 2024.\nBailis, S., Friedhoff, J., and Chen, F. Werewolf arena: A case study in llm evaluation via social\ndeduction. arXiv preprint arXiv:2407.13943 , 2024.\nBakhtin, A., Wu, D., Lerer, A., and Brown, N. No-press diplomacy from scratch. Advances in Neural\nInformation Processing Systems , 34:18063–18074, 2021.\nBoisvert, L., Thakkar, M., Gasse, M., Caccia, M., de Chezelles, T., Cappart, Q., Chapados, N.,\nLacoste, A., and Drouin, A. Workarena++: Towards compositional planning and reasoning-\nbased common knowledge work tasks. Advances in Neural Information Processing Systems , 37:\n5996–6051, 2024.\nBonatti, R., Zhao, D., Dupont, D., Abdali, S., Li, Y ., Lu, Y ., Wagle, J., Koishida, K., Bucker, A., Jang,\nL. K., et al. Windows agent arena: Evaluating multi-modal os agents at scale. In NeurIPS 2024\nWorkshop on Open-World Agents , 2024.\nCao, R., Lei, F., Wu, H., Chen, J., Fu, Y ., Gao, H., Xiong, X., Zhang, H., Hu, W., Mao, Y .,\net al. Spider2-v: How far are multimodal agents from automating data science and engineering\nworkflows? Advances in Neural Information Processing Systems , 37:107703–107744, 2024.\nCarroll, M., Shah, R., Ho, M. K., Griffiths, T., Seshia, S., Abbeel, P., and Dragan, A. On the utility\nof learning about humans for human-ai coordination. Advances in neural information processing\nsystems , 32, 2019.\nCarta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O., and Oudeyer, P.-Y . Grounding large\nlanguage models in interactive environments with online reinforcement learning. In International\nConference on Machine Learning , pp. 3676–3713. PMLR, 2023.\nChai, Y ., Huang, S., Niu, Y ., Xiao, H., Liu, L., Zhang, D., Gao, P., Ren, S., and Li, H. Amex: Android\nmulti-annotation expo dataset for mobile gui agents. CoRR , 2024.\nChai, Y ., Li, H., Zhang, J., Liu, L., Liu, G., Wang, G., Ren, S., Huang, S., and Li, H. A3: Android\nagent arena for mobile gui agents. arXiv preprint arXiv:2501.01149 , 2025.\nChalamalasetti, K., Götze, J., Hakimov, S., Madureira, B., Sadler, P., and Schlangen, D. clembench:\nUsing game play to evaluate chat-optimized language models as conversational agents. In Pro-\nceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pp.\n11174–11219, 2023.\nChan, J. S., Chowdhury, N., Jaffe, O., Aung, J., Sherburn, D., Mays, E., Starace, G., Liu, K., Maksin,\nL., Patwardhan, T., et al. Mle-bench: Evaluating machine learning agents on machine learning\nengineering. arXiv preprint arXiv:2410.07095 , 2024.\nChang, M., Chhablani, G., Clegg, A., Cote, M. D., Desai, R., Hlavac, M., Karashchuk, V ., Krantz, J.,\nMottaghi, R., Parashar, P., et al. Partnr: A benchmark for planning and reasoning in embodied\nmulti-agent tasks. arXiv preprint arXiv:2411.00081 , 2024.\nChen, G., Chen, P., Hsieh, C.-Y ., Lee, C.-K., Liao, B., Liao, R., Liu, W., Qiu, J., Sun, Q., Tang,\nJ., et al. Alchemy: A quantum chemistry dataset for benchmarking ai models. arXiv preprint\narXiv:1906.09427 , 2019.\n42\n--- Page 43 ---\nChen, J., Hu, X., Liu, S., Huang, S., Tu, W.-W., He, Z., and Wen, L. Llmarena: Assessing capabilities\nof large language models in dynamic multi-agent environments. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.\n13055–13077, 2024a.\nChen, J., Yuen, D., Xie, B., Yang, Y ., Chen, G., Wu, Z., Yixing, L., Zhou, X., Liu, W., Wang, S.,\net al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024\nWorkshop on Open-World Agents , 2024b.\nChen, Y ., Yuan, Y ., Zhang, Z., Zheng, Y ., Liu, J., Ni, F., and Hao, J. Sheetagent: A generalist agent\nfor spreadsheet reasoning and manipulation via large language models. In ICML 2024 Workshop\non LLMs and Cognition , 2024c.\nChen, Z., Chen, S., Ning, Y ., Zhang, Q., Wang, B., Yu, B., Li, Y ., Liao, Z., Wei, C., Lu, Z., et al.\nScienceagentbench: Toward rigorous assessment of language agents for data-driven scientific\ndiscovery. arXiv preprint arXiv:2410.05080 , 2024d.\nChen, Z., Du, W., Zhang, W., Liu, K., Liu, J., Zheng, M., Zhuo, J., Zhang, S., Lin, D., Chen, K.,\net al. T-eval: Evaluating the tool utilization capability of large language models step by step. In\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pp. 9510–9529, 2024e.\nChevalier-Boisvert, M., Dai, B., Towers, M., Perez-Vicente, R., Willems, L., Lahlou, S., Pal, S., and\nCastro, P. S. Minigrid & miniworld: Modular & customizable reinforcement learning environments\nfor goal-oriented tasks. Advances in Neural Information Processing Systems , 36:73383–73394,\n2023.\nCloos, N., Jens, M., Naim, M., Kuo, Y .-L., Cases, I., Barbu, A., and Cueva, C. J. Baba is ai: Break\nthe rules to beat the benchmark. In ICML 2024 Workshop on LLMs and Cognition , 2024.\nCostarelli, A., Allen, M., Hauksson, R., Sodunke, G., Hariharan, S., Cheng, C., Li, W., Clymer, J. M.,\nand Yadav, A. Gamebench: Evaluating strategic reasoning abilities of llm agents. In Language\nGamification-NeurIPS 2024 Workshop , 2024.\nCôté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M.,\nEl Asri, L., Adada, M., et al. Textworld: A learning environment for text-based games. In\nComputer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International\nConference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised\nSelected Papers 7 , pp. 41–75. Springer, 2019.\nDavidson, T. R., Veselovsky, V ., Josifoski, M., Peyrard, M., Bosselut, A., Kosinski, M., and West, R.\nEvaluating language model agency through negotiations. In ICLR 2024 , 2024.\nDeng, X., Gu, Y ., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y . Mind2web:\nTowards a generalist agent for the web. Advances in Neural Information Processing Systems , 36:\n28091–28114, 2023.\nDong, Y ., Zhu, X., Pan, Z., Zhu, L., and Yang, Y . Villageragent: A graph-based multi-agent framework\nfor coordinating complex task dependencies in minecraft. In Findings of the Association for\nComputational Linguistics ACL 2024 , pp. 16290–16314, 2024.\nDrouin, A., Gasse, M., Caccia, M., Laradji, I. H., Del Verme, M., Marty, T., Vazquez, D., Chapados,\nN., and Lacoste, A. Workarena: How capable are web agents at solving common knowledge work\ntasks? In International Conference on Machine Learning , pp. 11642–11662. PMLR, 2024.\nDuan, J., Zhang, R., Diffenderfer, J., Kailkhura, B., Sun, L., Stengel-Eskin, E., Bansal, M., Chen, T.,\nand Xu, K. Gtbench: Uncovering the strategic reasoning capabilities of llms via game-theoretic\nevaluations. In The Thirty-eighth Annual Conference on Neural Information Processing Systems ,\n2024.\nGao, D., Ji, L., Bai, Z., Ouyang, M., Li, P., Mao, D., Wu, Q., Zhang, W., Wang, P., Guo, X.,\net al. Assistgui: Task-oriented desktop graphical user interface automation. arXiv preprint\narXiv:2312.13108 , 2023.\n43\n--- Page 44 ---\nGao, X., Gao, Q., Gong, R., Lin, K., Thattai, G., and Sukhatme, G. S. Dialfred: Dialogue-enabled\nagents for embodied instruction following. IEEE Robotics and Automation Letters , 7(4):10049–\n10056, 2022.\nGong, R., Huang, J., Zhao, Y ., Geng, H., Gao, X., Wu, Q., Ai, W., Zhou, Z., Terzopoulos, D., Zhu,\nS.-C., et al. Arnold: A benchmark for language-grounded task learning with continuous states\nin realistic 3d scenes. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pp. 20483–20495, 2023.\nGong, R., Huang, Q., Ma, X., Noda, Y ., Durante, Z., Zheng, Z., Terzopoulos, D., Fei-Fei, L., Gao,\nJ., and V o, H. Mindagent: Emergent gaming interaction. In Findings of the Association for\nComputational Linguistics: NAACL 2024 , pp. 3154–3183, 2024.\nGonzalez-Pumariega, G., Yean, L. S., Sunkara, N., and Choudhury, S. Robotouille: An asynchronous\nplanning benchmark for llm agents. arXiv preprint arXiv:2502.05227 , 2025.\nGuertler, L., Cheng, B., Yu, S., Liu, B., Choshen, L., and Tan, C. Textarena. arXiv preprint\narXiv:2504.11442 , 2025.\nGuo, Z., Cheng, S., Niu, Y ., Wang, H., Zhou, S., Huang, W., and Liu, Y . Stabletoolbench-mirrorapi:\nModeling tool environments as mirrors of 7,000+ real-world apis. arXiv preprint arXiv:2503.20527 ,\n2025.\nHafner, D. Benchmarking the spectrum of agent capabilities. In International Conference on Learning\nRepresentations , 2022.\nHe, H., Yao, W., Ma, K., Yu, W., Dai, Y ., Zhang, H., Lan, Z., and Yu, D. Webvoyager: Building an\nend-to-end web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 6864–6890, 2024.\nHill, W., Liu, I., Koch, A. D. M., Harvey, D., Kumar, N., Konidaris, G., and James, S. Mineplanner: A\nbenchmark for long-horizon planning in large minecraft worlds. arXiv preprint arXiv:2312.12891 ,\n2023.\nHu, L., Li, Q., Xie, A., Jiang, N., Stoica, I., Jin, H., and Zhang, H. Gamearena: Evaluating llm\nreasoning through live computer games. arXiv preprint arXiv:2412.06394 , 2024.\nHua, W., Liu, O., Li, L., Amayuelas, A., Chen, J., Jiang, L., Jin, M., Fan, L., Sun, F., Wang, W., et al.\nGame-theoretic llm: Agent workflow for negotiation games. arXiv preprint arXiv:2411.05990 ,\n2024.\nHuang, Q., V ora, J., Liang, P., and Leskovec, J. Mlagentbench: Evaluating language agents on\nmachine learning experimentation. In International Conference on Machine Learning , pp. 20271–\n20309. PMLR, 2024a.\nHuang, S., Zhong, W., Lu, J., Zhu, Q., Gao, J., Liu, W., Hou, Y ., Zeng, X., Wang, Y ., Shang, L., et al.\nPlanning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world\ncomplex scenarios. In Findings of the Association for Computational Linguistics ACL 2024 , pp.\n4363–4400, 2024b.\nHuang, Y ., Wang, X., Liu, H., Kong, F., Qin, A., Tang, M., Wang, X., Zhu, S.-C., Bi, M., Qi, S., et al.\nAdasociety: An adaptive environment with social structures for multi-agent decision-making. In\nThe Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack , 2024c.\nJames, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench: The robot learning benchmark &\nlearning environment. IEEE Robotics and Automation Letters , 5(2):3019–3026, 2020.\nJang, L. K., Li, Y ., Ding, C., Lin, J., Liang, P. P., Zhao, D., Bonatti, R., and Koishida, K. Vide-\nowebarena: Evaluating long context multimodal agents with video understanding web tasks. In\nNeurIPS 2024 Workshop on Open-World Agents , 2024.\n44\n--- Page 45 ---\nJansen, P., Côté, M.-A., Khot, T., Bransom, E., Dalvi Mishra, B., Majumder, B. P., Tafjord, O.,\nand Clark, P. Discoveryworld: A virtual environment for developing and evaluating automated\nscientific discovery agents. Advances in Neural Information Processing Systems , 37:10088–10116,\n2024.\nJin, T., Zhu, Y ., and Kang, D. Elt-bench: An end-to-end benchmark for evaluating ai agents on elt\npipelines. arXiv preprint arXiv:2504.04808 , 2025.\nJin, X., Wang, Z., Du, Y ., Fang, M., Zhang, H., and Wang, J. Learning to discuss strategically: A\ncase study on one night ultimate werewolf. Advances in Neural Information Processing Systems ,\n37:77060–77097, 2024.\nKapoor, R., Butala, Y . P., Russak, M., Koh, J. Y ., Kamble, K., AlShikh, W., and Salakhutdinov, R.\nOmniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for\ndesktop and web. In European Conference on Computer Vision , pp. 161–178. Springer, 2024.\nKarten, S., Nguyen, A. L., and Jin, C. Pok \\’echamp: an expert-level minimax language agent. arXiv\npreprint arXiv:2503.04094 , 2025.\nKlissarov, M., D’Oro, P., Sodhani, S., Raileanu, R., Bacon, P.-L., Vincent, P., Zhang, A., and Henaff,\nM. Motif: Intrinsic motivation from artificial intelligence feedback. In Second Agent Learning in\nOpen-Endedness Workshop , 2023.\nKoh, J. Y ., Lo, R., Jang, L., Duvvur, V ., Lim, M., Huang, P.-Y ., Neubig, G., Zhou, S., Salakhutdinov,\nR., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pp. 881–905, 2024.\nKüttler, H., Nardelli, N., Miller, A., Raileanu, R., Selvatici, M., Grefenstette, E., and Rocktäschel,\nT. The nethack learning environment. Advances in Neural Information Processing Systems , 33:\n7671–7684, 2020.\nLee, J., Min, T., An, M., Hahm, D., Lee, H., Kim, C., and Lee, K. Benchmarking mobile device\ncontrol agents across diverse configurations. arXiv preprint arXiv:2404.16660 , 2024.\nLei, F., Chen, J., Ye, Y ., Cao, R., Shin, D., Hongjin, S., SUO, Z., Gao, H., Hu, W., Yin, P., et al.\nSpider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. In The\nThirteenth International Conference on Learning Representations , 2025.\nLi, H., Su, J., Chen, Y ., Li, Q., and ZHANG, Z.-X. Sheetcopilot: Bringing software productivity to\nthe next level through large language models. Advances in Neural Information Processing Systems ,\n36:4952–4984, 2023a.\nLi, H., Cao, Y ., Yu, Y ., Javaji, S. R., Deng, Z., He, Y ., Jiang, Y ., Zhu, Z., Subbalakshmi, K., Xiong,\nG., et al. Investorbench: A benchmark for financial decision-making tasks with llm-based agent.\narXiv preprint arXiv:2412.18174 , 2024a.\nLi, M., Zhao, Y ., Yu, B., Song, F., Li, H., Yu, H., Li, Z., Huang, F., and Li, Y . Api-bank: A\ncomprehensive benchmark for tool-augmented llms. In The 2023 Conference on Empirical\nMethods in Natural Language Processing , 2023b.\nLi, M., Zhao, S., Wang, Q., Wang, K., Zhou, Y ., Srivastava, S., Gokmen, C., Lee, T., Li, E. L., Zhang,\nR., et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances\nin Neural Information Processing Systems , 37:100428–100534, 2024b.\nLi, W., Bishop, W. E., Li, A., Rawles, C., Campbell-Ajala, F., Tyamagundlu, D., and Riva, O. On the\neffects of data scale on ui control agents. Advances in Neural Information Processing Systems , 37:\n92130–92154, 2024c.\nLi, Y ., He, J., Zhou, X., Zhang, Y ., and Baldridge, J. Mapping natural language instructions to\nmobile ui action sequences. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 8198–8210, 2020.\n45\n--- Page 46 ---\nLight, J., Cai, M., Shen, S., and Hu, Z. Avalonbench: Evaluating llms playing the game of avalon. In\nNeurIPS 2023 Foundation Models for Decision Making Workshop , 2023.\nLim, S., Kim, S., Yu, J., Lee, S., Chung, J., and Yu, Y . Visescape: A benchmark for evaluating\nexploration-driven decision-making in virtual escape rooms. arXiv preprint arXiv:2503.14427 ,\n2025.\nLin, F., Malfa, E. L., Hofmann, V ., Yang, E. M., Cohn, A. G., and Pierrehumbert, J. B. Graph-\nenhanced large language models in asynchronous plan reasoning. In Proceedings of the 41st\nInternational Conference on Machine Learning , pp. 30108–30134, 2024a.\nLin, K. Q., Li, L., Gao, D., Wu, Q., Yan, M., Yang, Z., Wang, L., and Shou, M. Z. Videogui: A\nbenchmark for gui automation from instructional videos. CoRR , 2024b.\nLiu, B., Zhu, Y ., Gao, C., Feng, Y ., Liu, Q., Zhu, Y ., and Stone, P. Libero: Benchmarking knowledge\ntransfer for lifelong robot learning. Advances in Neural Information Processing Systems , 36:\n44776–44791, 2023.\nLiu, E. Z., Guu, K., Pasupat, P., Shi, T., and Liang, P. Reinforcement learning on web interfaces using\nworkflow-guided exploration. In International Conference on Learning Representations , 2018.\nLiu, X., Yu, H., Zhang, H., Xu, Y ., Lei, X., Lai, H., Gu, Y ., Ding, H., Men, K., Yang, K., et al.\nAgentbench: Evaluating llms as agents. In ICLR , 2024a.\nLiu, X., Zhang, T., Gu, Y ., Iong, I. L., Xu, Y ., Song, X., Zhang, S., Lai, H., Liu, X., Zhao, H., et al.\nVisualagentbench: Towards large multimodal models as visual foundation agents. CoRR , 2024b.\nLu, X. H., Kasner, Z., and Reddy, S. Weblinx: Real-world website navigation with multi-turn\ndialogue. In International Conference on Machine Learning , pp. 33007–33056. PMLR, 2024.\nMa, C., Zhang, J., Zhu, Z., Yang, C., Yang, Y ., Jin, Y ., Lan, Z., Kong, L., and He, J. Agentboard: An\nanalytical evaluation board of multi-turn llm agents. In The Thirty-eight Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track , 2024a.\nMa, Z., Huang, W., Zhang, J., Gupta, T., and Krishna, R. m & m’s: A benchmark to evaluate\ntool-use for m ulti-step m ulti-modal tasks. In European Conference on Computer Vision , pp.\n18–34. Springer, 2024b.\nMa, Z., Zhang, B., Zhang, J., Yu, J., Zhang, X., Zhang, X., Luo, S., Wang, X., and Tang, J.\nSpreadsheetbench: Towards challenging real world spreadsheet manipulation. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track , 2024c.\nMandi, Z., Jain, S., and Song, S. Roco: Dialectic multi-robot collaboration with large language\nmodels. In 2024 IEEE International Conference on Robotics and Automation (ICRA) , pp. 286–299.\nIEEE, 2024.\nMialon, G., Fourrier, C., Wolf, T., LeCun, Y ., and Scialom, T. Gaia: a benchmark for general ai\nassistants. In The Twelfth International Conference on Learning Representations , 2024.\nMu, T., Ling, Z., Xiang, F., Yang, D., Li, X., Tao, S., Huang, Z., Jia, Z., and Su, H. Maniskill:\nGeneralizable manipulation skill benchmark with large-scale demonstrations. In 35th Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\nMukobi, G., Erlebach, H., Lauffer, N., Hammond, L., Chan, A., and Clifton, J. Welfare diplomacy:\nBenchmarking language model cooperation. In Socially Responsible Language Modelling Research ,\n2022.\nNasir, M. U., James, S., and Togelius, J. Gametraversalbenchmark: Evaluating planning abilities\nof large language models through traversing 2d game maps. In The Thirty-eight Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track , 2024.\nNasiriany, S., Maddukuri, A., Zhang, L., Parikh, A., Lo, A., Joshi, A., Mandlekar, A., and Zhu, Y .\nRobocasa: Large-scale simulation of everyday tasks for generalist robots. In RSS 2024 Workshop:\nData Generation for Robotics , 2024.\n46\n--- Page 47 ---\nNathani, D., Madaan, L., Roberts, N., Bashlykov, N., Menon, A., Moens, V ., Budhiraja, A., Magka,\nD., V orotilov, V ., Chaurasia, G., et al. Mlgym: A new framework and benchmark for advancing ai\nresearch agents. arXiv preprint arXiv:2502.14499 , 2025.\nPadmakumar, A., Thomason, J., Shrivastava, A., Lange, P., Narayan-Chen, A., Gella, S., Piramuthu,\nR., Tur, G., and Hakkani-Tur, D. Teach: Task-driven embodied agents that chat. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , 2022.\nPaglieri, D., Cupiał, B., Coward, S., Piterbarg, U., Wolczyk, M., Khan, A., Pignatelli, E., Kuci ´nski,\nŁ., Pinto, L., Fergus, R., et al. Balrog: Benchmarking agentic llm and vlm reasoning on games.\narXiv preprint arXiv:2411.13543 , 2024.\nPan, Y ., Kong, D., Zhou, S., Cui, C., Leng, Y ., Jiang, B., Liu, H., Shang, Y ., Zhou, S., Wu, T., et al.\nWebcanvas: Benchmarking web agents in online environments. In Agentic Markets Workshop at\nICML 2024 , 2024.\nPayan, J., Mishra, S., Singh, M., Negreanu, C. S., Poelitz, C., Baral, C., Roy, S., Chakravarthy, R.,\nVan Durme, B., and Nouri, E. Instructexcel: A benchmark for natural language instruction in excel.\nInThe 2023 Conference on Empirical Methods in Natural Language Processing , 2023.\nPeng, Y ., Li, S., Gu, W., Li, Y ., Wang, W., Gao, C., and Lyu, M. R. Revisiting, benchmarking and\nexploring api recommendation: How far are we? IEEE Transactions on Software Engineering , 49\n(4):1876–1897, 2022.\nPérez-Rodríguez, M., Hidalgo, M. J., Mendoza, A., González, L. T., Rodríguez, F. L., Goicoechea,\nH. C., and Pellerano, R. G. Measuring trace element fingerprinting for cereal bar authentication\nbased on type and principal ingredient. Food Chemistry: X , 18:100744, 2023.\nPuig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., and Torralba, A. Virtualhome: Simulating\nhousehold activities via programs. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pp. 8494–8502, 2018.\nPuig, X., Shu, T., Li, S., Wang, Z., Liao, Y .-H., Tenenbaum, J. B., Fidler, S., and Torralba, A.\nWatch-and-help: A challenge for social perception and human-ai collaboration. In ICLR , 2021.\nQi, S., Chen, S., Li, Y ., Kong, X., Wang, J., Yang, B., Wong, P., Zhong, Y ., Zhang, X., Zhang, Z.,\net al. Civrealm: A learning and reasoning odyssey in civilization for decision-making agents. In\nThe Twelfth International Conference on Learning Representations , 2024.\nQian, C., Han, P., Luo, Q., He, B., Chen, X., Zhang, Y ., Du, H., Yao, J., Yang, X., Zhang, D., et al.\nEscapebench: Pushing language models to think outside the box. arXiv preprint arXiv:2412.13549 ,\n2024.\nQin, Y ., Liang, S., Ye, Y ., Zhu, K., Yan, L., Lu, Y ., Lin, Y ., Cong, X., Tang, X., Qian, B., et al.\nToolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth\nInternational Conference on Learning Representations , 2024.\nQu, C., Dai, S., Wei, X., Cai, H., Wang, S., Yin, D., Xu, J., and Wen, J.-R. Towards completeness-\noriented tool retrieval for large language models. In Proceedings of the 33rd ACM International\nConference on Information and Knowledge Management , pp. 1930–1940, 2024.\nRawles, C., Li, A., Rodriguez, D., Riva, O., and Lillicrap, T. Android in the wild: A large-scale\ndataset for android device control, 2023. URL https://arxiv. org/abs/2307.10088 , 2023.\nRawles, C., Clinckemaillie, S., Chang, Y ., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W., Li, W.,\nCampbell-Ajala, F., et al. Androidworld: A dynamic benchmarking environment for autonomous\nagents. arXiv preprint arXiv:2405.14573 , 2024.\nRein, D., Becker, J., Deng, A., Nix, S., Canal, C., O’Connel, D., Arnott, P., Bloom, R., Broadley,\nT., Garcia, K., et al. Hcast: Human-calibrated autonomy software tasks. arXiv preprint\narXiv:2503.17354 , 2025.\nRen, Y ., Tertikas, K., Maiti, S., Han, J., Zhang, T., Süsstrunk, S., and Kokkinos, F. Vgrp-\nbench: Visual grid reasoning puzzle benchmark for large vision-language models. arXiv preprint\narXiv:2503.23064 , 2025.\n47\n--- Page 48 ---\nRuan, K., Huang, M., Wen, J.-R., and Sun, H. Benchmarking llms’ swarm intelligence. arXiv\npreprint arXiv:2505.04364 , 2025.\nRuan, Y ., Dong, H., Wang, A., Pitis, S., Zhou, Y ., Ba, J., Dubois, Y ., Maddison, C. J., and Hashimoto,\nT. Identifying the risks of lm agents with an lm-emulated sandbox. In The Twelfth International\nConference on Learning Representations , 2024.\nSamvelyan, M., Kirk, R., Kurin, V ., Parker-Holder, J., Jiang, M., Hambro, E., Petroni, F., Kuttler, H.,\nGrefenstette, E., and Rocktäschel, T. Minihack the planet: A sandbox for open-ended reinforcement\nlearning research. In Thirty-fifth Conference on Neural Information Processing Systems Datasets\nand Benchmarks Track (Round 1) , 2021.\nShen, H., Li, Y ., Meng, D., Cai, D., Qi, S., Zhang, L., Xu, M., and Ma, Y . Shortcutsbench: A\nlarge-scale real-world benchmark for api-based agents. CoRR , 2024a.\nShen, Y ., Song, K., Tan, X., Zhang, W., Ren, K., Yuan, S., Lu, W., Li, D., and Zhuang, Y . Taskbench:\nBenchmarking large language models for task automation. Advances in Neural Information\nProcessing Systems , 37:4540–4574, 2024b.\nShi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. World of bits: An open-domain platform\nfor web-based agents. In International Conference on Machine Learning , pp. 3135–3144. PMLR,\n2017.\nShridhar, M., Thomason, J., Gordon, D., Bisk, Y ., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D.\nAlfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition , pp. 10740–10749, 2020a.\nShridhar, M., Yuan, X., Cote, M.-A., Bisk, Y ., Trischler, A., and Hausknecht, M. Alfworld: Aligning\ntext and embodied environments for interactive learning. In International Conference on Learning\nRepresentations , 2020b.\nSong, Y ., Thai, K., Pham, C. M., Chang, Y ., Nadaf, M., and Iyyer, M. Bearcubs: A benchmark for\ncomputer-using web agents. arXiv preprint arXiv:2503.07919 , 2025.\nSun, H., Zhang, S., Ren, L., Xu, H., Fu, H., Yuan, C., and Wang, X. Collab-overcooked: Benchmark-\ning and evaluating large language models as collaborative agents. arXiv preprint arXiv:2502.20073 ,\n2025.\nSun, L., Chen, X., Chen, L., Dai, T., Zhu, Z., and Yu, K. Meta-gui: Towards multi-modal conver-\nsational agents on mobile gui. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pp. 6699–6712, 2022.\nTan, W., Ding, Z., Zhang, W., Li, B., Zhou, B., Yue, J., Xia, H., Jiang, J., Zheng, L., Xu, X., et al.\nTowards general computer control: A multimodal agent for red dead redemption ii as a case study.\nInICLR 2024 Workshop on Large Language Model (LLM) Agents , 2024.\nTang, Q., Deng, Z., Lin, H., Han, X., Liang, Q., Cao, B., and Sun, L. Toolalpaca: Generalized tool\nlearning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 , 2023.\nTang, W., Zhou, Y ., Xu, E., Cheng, K., Li, M., and Xiao, L. Dsgbench: A diverse strategic game\nbenchmark for evaluating llm-based agents in complex decision-making environments. arXiv\npreprint arXiv:2503.06047 , 2025.\nTang, X., Li, J., Liang, Y ., Zhu, S.-C., Zhang, M., and Zheng, Z. Mars: Situated inductive reasoning in\nan open-world environment. Advances in Neural Information Processing Systems , 37:17830–17869,\n2024.\nThomas, G., Chan, A. J., Kang, J., Wu, W., Christianos, F., Greenlee, F., Toulis, A., and Pur-\ntorab, M. Webgames: Challenging general-purpose web-browsing ai agents. arXiv preprint\narXiv:2502.18356 , 2025.\nTrivedi, H., Khot, T., Hartmann, M., Manku, R., Dong, V ., Li, E., Gupta, S., Sabharwal, A., and\nBalasubramanian, N. Appworld: A controllable world of apps and people for benchmarking\ninteractive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp. 16022–16076, 2024.\n48\n--- Page 49 ---\nUrbanek, J., Fan, A., Karamcheti, S., Jain, S., Humeau, S., Dinan, E., Rocktäschel, T., Kiela, D.,\nSzlam, A., and Weston, J. Learning to speak and act in a fantasy text adventure game. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp.\n673–683, 2019.\nValmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati, S. Planbench: An\nextensible benchmark for evaluating large language models on planning and reasoning about\nchange. Advances in Neural Information Processing Systems , 36:38975–38987, 2023.\nVenkatesh, S. G., Talukdar, P., and Narayanan, S. Ugif: Ui grounded instruction following. arXiv\npreprint arXiv:2211.07615 , 2022.\nWan, Y ., Mao, J., and Tenenbaum, J. Handmethat: Human-robot communication in physical and\nsocial environments. Advances in Neural Information Processing Systems , 35:12014–12026, 2022.\nWang, J., Zerun, M., Li, Y ., Zhang, S., Chen, C., Chen, K., and Le, X. Gta: a benchmark for general\ntool agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets\nand Benchmarks Track , 2024a.\nWang, L., Deng, Y ., Zha, Y ., Mao, G., Wang, Q., Min, T., Chen, W., and Chen, S. Mobileagentbench:\nAn efficient and user-friendly benchmark for mobile llm agents. arXiv preprint arXiv:2406.08184 ,\n2024b.\nWang, R., Jansen, P., Côté, M.-A., and Ammanabrolu, P. Scienceworld: Is your agent smarter than a\n5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , pp. 11279–11298, 2022.\nWang, W., Zhang, D., Feng, T., Wang, B., and Tang, J. Battleagentbench: A benchmark for evaluating\ncooperation and competition capabilities of language models in multi-agent systems. arXiv preprint\narXiv:2408.15971 , 2024c.\nWang, X., Li, D., Zhao, Y ., Wang, H., et al. Metatool: Facilitating large language models to master\ntools with meta-task augmentation. CoRR , 2024d.\nWang, X., Zhuang, B., and Wu, Q. Are large vision language models good game players? arXiv\npreprint arXiv:2503.02358 , 2025a.\nWang, Y ., Xian, Z., Chen, F., Wang, T.-H., Wang, Y ., Fragkiadaki, K., Erickson, Z., Held, D., and\nGan, C. Robogen: Towards unleashing infinite data for automated robot learning via generative\nsimulation. In International Conference on Machine Learning , pp. 51936–51983. PMLR, 2024e.\nWang, Y ., Guo, Y ., Zheng, Y ., Yin, Z., Chen, S., Yang, J., Chen, J., Huang, X., and Qiu, X. Familytool:\nA multi-hop personalized tool use benchmark. arXiv preprint arXiv:2504.06766 , 2025b.\nWang, Z., Cui, Y ., Zhong, L., Zhang, Z., Yin, D., Lin, B. Y ., and Shang, J. Officebench: Benchmarking\nlanguage agents across multiple applications for office automation. CoRR , 2024f.\nWhite, I., Nottingham, K., Maniar, A., Robinson, M., Lillemark, H., Maheshwari, M., Qin, L., and\nAmmanabrolu, P. Collaborating action by action: A multi-agent llm framework for embodied\nreasoning. arXiv preprint arXiv:2504.17950 , 2025.\nWijk, H., Lin, T., Becker, J., Jawhar, S., Parikh, N., Broadley, T., Chan, L., Chen, M., Clymer, J.,\nDhyani, J., et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents\nagainst human experts. arXiv preprint arXiv:2411.15114 , 2024.\nWornow, M., Narayan, A., Viggiano, B., Khare, I., Verma, T., Thompson, T., Hernandez, M.,\nSundar, S., Trujillo, C., Chawla, K., et al. Wonderbread: A benchmark for evaluating multimodal\nfoundation models on business process management tasks. Advances in Neural Information\nProcessing Systems , 37:115963–116021, 2024.\nWu, D., Shi, H., Sun, Z., and Liu, B. Deciphering digital detectives: Understanding llm behaviors\nand capabilities in multi-agent mystery games. In Findings of the Association for Computational\nLinguistics ACL 2024 , pp. 8225–8291, 2024a.\n49\n--- Page 50 ---\nWu, Y ., Tang, X., Mitchell, T., and Li, Y . Smartplay: A benchmark for llms as intelligent agents. In\nThe Twelfth International Conference on Learning Representations , 2024b.\nXi, Z., Ding, Y ., Chen, W., Hong, B., Guo, H., Wang, J., Yang, D., Liao, C., Guo, X., He, W., et al.\nAgentgym: Evolving large language model-based agents across diverse environments. CoRR ,\n2024.\nXie, J., Zhang, R., Chen, Z., Wan, X., and Li, G. Whodunitbench: Evaluating large multimodal agents\nvia murder mystery games. Advances in Neural Information Processing Systems , 37:86655–86687,\n2024a.\nXie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., et al.\nOsworld: Benchmarking multimodal agents for open-ended tasks in real computer environments.\nAdvances in Neural Information Processing Systems , 37:52040–52094, 2024b.\nXing, M., Zhang, R., Xue, H., Chen, Q., Yang, F., and Xiao, Z. Understanding the weakness of large\nlanguage model agents within a complex android environment. In Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining , pp. 6061–6072, 2024.\nXu, F. F., Song, Y ., Li, B., Tang, Y ., Jain, K., Bao, M., Wang, Z. Z., Zhou, X., Guo, Z., Cao, M., et al.\nTheagentcompany: benchmarking llm agents on consequential real world tasks. arXiv preprint\narXiv:2412.14161 , 2024a.\nXu, K., Kordi, Y ., Nayak, T., Asija, A., Wang, Y ., Sanders, K., Byerly, A., Zhang, J., Van Durme,\nB., and Khashabi, D. Tur [k] ingbench: A challenge benchmark for web agents. arXiv preprint\narXiv:2403.11905 , 2024b.\nXu, L., Hu, Z., Zhou, D., Ren, H., Dong, Z., Keutzer, K., Ng, S. K., and Feng, J. Magic: Inves-\ntigation of large language model powered multi-agent in cognition, adaptability, rationality and\ncollaboration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing , pp. 7315–7332, 2024c.\nXu, M., Jiang, G., Liang, W., Zhang, C., and Zhu, Y . Interactive visual reasoning under uncertainty.\nAdvances in Neural Information Processing Systems , 36:42409–42432, 2023a.\nXu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang, J. On the tool manipulation capability of\nopen-sourced large language models. In NeurIPS 2023 Foundation Models for Decision Making\nWorkshop , 2023b.\nXu, Y ., Liu, X., Sun, X., Cheng, S., Yu, H., Lai, H., Zhang, S., Zhang, D., Tang, J., and Dong, Y .\nAndroidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint\narXiv:2410.24024 , 2024d.\nYang, J., Prabhakar, A., Narasimhan, K., and Yao, S. Intercode: Standardizing and benchmarking\ninteractive coding with execution feedback. Advances in Neural Information Processing Systems ,\n36:23826–23854, 2023.\nYang, S., Zhao, B., and Xie, C. Aqa-bench: An interactive benchmark for evaluating llms’ sequential\nreasoning ability. CoRR , 2024.\nYao, J., Wang, K., Hsieh, R., Zhou, H., Zou, T., Cheng, Z., Wang, Z., and Viswanath, P. Spin-bench:\nHow well do llms plan strategically and reason socially? arXiv preprint arXiv:2503.12349 , 2025.\nYao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web\ninteraction with grounded language agents. Advances in Neural Information Processing Systems ,\n35:20744–20757, 2022.\nYao, S., Shinn, N., Razavi, P., and Narasimhan, K. τ-bench: A benchmark for tool-agent-user\ninteraction in real-world domains. arXiv preprint arXiv:2406.12045 , 2024.\nYu, P., Yang, Y ., Li, J., Zhang, Z., Wang, H., Feng, X., and Zhang, F. Multi-mission tool bench:\nAssessing the robustness of llm based agents through related and dynamic missions. arXiv preprint\narXiv:2504.02623 , 2025.\n50\n--- Page 51 ---\nYu, X., Fu, J., Deng, R., and Han, W. Mineland: Simulating large-scale multi-agent interactions with\nlimited multimodal senses and physical needs. CoRR , 2024.\nYuan, X., Moss, M. M., Feghali, C. E., Singh, C., Moldavskaya, D., MacPhee, D., Caccia, L., Pereira,\nM., Kim, M., Sordoni, A., et al. debug-gym: A text-based environment for interactive debugging.\narXiv preprint arXiv:2503.21557 , 2025.\nZhang, D., Shen, Z., Xie, R., Zhang, S., Xie, T., Zhao, Z., Chen, S., Chen, L., Xu, H., Cao, R., et al.\nMobile-env: Building qualified evaluation benchmarks for llm-gui interaction. arXiv preprint\narXiv:2305.08144 , 2023.\nZhang, H., Guo, H., Guo, S., Cao, M., Huang, W., Liu, J., and Zhang, G. Ing-vp: Mllms cannot play\neasy vision-based games yet. arXiv preprint arXiv:2410.06555 , 2024a.\nZhang, L., Wang, S., Jia, X., Zheng, Z., Yan, Y ., Gao, L., Li, Y ., and Xu, M. Llamatouch: A faithful\nand scalable testbed for mobile ui task automation. In Proceedings of the 37th Annual ACM\nSymposium on User Interface Software and Technology , pp. 1–13, 2024b.\nZhang, S., Xu, Z., Liu, P., Yu, X., Li, Y ., Gao, Q., Fei, Z., Yin, Z., Wu, Z., Jiang, Y .-G., et al.\nVlabench: A large-scale benchmark for language-conditioned robotics manipulation with long-\nhorizon reasoning tasks. arXiv preprint arXiv:2412.18194 , 2024c.\nZheng, L., Huang, Z., Xue, Z., Wang, X., An, B., and Shuicheng, Y . Agentstudio: A toolkit for\nbuilding general virtual agents. In NeurIPS 2024 Workshop on Open-World Agents , 2024.\nZheng, X., Li, L., Yang, Z., Yu, P., Wang, A. J., Yan, R., Yao, Y ., and Wang, L. V-mage: A game\nevaluation framework for assessing visual-centric capabilities in multimodal large language models.\narXiv preprint arXiv:2504.06148 , 2025.\nZhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y ., Fried, D.,\net al. Webarena: A realistic web environment for building autonomous agents. In The Twelfth\nInternational Conference on Learning Representations , 2024a.\nZhou, X., Zhu, H., Mathur, L., Zhang, R., Yu, H., Qi, Z., Morency, L.-P., Bisk, Y ., Fried, D., Neubig,\nG., et al. Sotopia: Interactive evaluation for social intelligence in language agents. In The Twelfth\nInternational Conference on Learning Representations , 2024b.\nZhuang, Y ., Yu, Y ., Wang, K., Sun, H., and Zhang, C. Toolqa: A dataset for llm question answering\nwith external tools. Advances in Neural Information Processing Systems , 36:50117–50143, 2023.\n51",
  "text_length": 147703
}