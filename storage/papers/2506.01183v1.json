{
  "id": "http://arxiv.org/abs/2506.01183v1",
  "title": "Doubly Robust Alignment for Large Language Models",
  "summary": "This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM",
  "authors": [
    "Erhan Xu",
    "Kai Ye",
    "Hongyi Zhou",
    "Luhan Zhu",
    "Francesco Quinzan",
    "Chengchun Shi"
  ],
  "published": "2025-06-01T21:34:37Z",
  "updated": "2025-06-01T21:34:37Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01183v1",
  "full_text": "--- Page 1 ---\nDoubly Robust Alignment for Large Language Models\nErhan Xu†1, Kai Ye†1, Hongyi Zhou†2, Luhan Zhu3,\nFrancesco Quinzan‡4, Chengchun Shi‡1\n1Department of Statistics, LSE\n2Department of Mathematics, Tsinghua University\n3School of Design, LCC, UAL\n4Department of Engineering Science, University of Oxford\n†equal contribution‡joint senior contributors\nAbstract\nThis paper studies reinforcement learning from human feedback (RLHF) for aligning large\nlanguagemodelswithhumanpreferences. WhileRLHFhasdemonstratedpromisingresults,\nmany algorithms are highly sensitive to misspecifications in the underlying preference\nmodel (e.g., the Bradley-Terry model), the reference policy, or the reward function,\nresulting in undesirable fine-tuning. To address model misspecification, we propose a\ndoubly robust preference optimization algorithm that remains consistent when either\nthe preference model or the reference policy is correctly specified (without requiring\nboth). Our proposal demonstrates superior and more robust performance than state-\nof-the-art algorithms, both in theory and in practice. The code is available at https:\n//github.com/DRPO4LLM/DRPO4LLM\n1 Introduction\nRecent advances in large language models (LLMs) have revolutionized various natural language\nprocessing tasks, ranging from text generation to human-AI conversation and more complex\nreasoning tasks [Brown et al., 2020, Guo et al., 2025, Touvron et al., 2023]. These models are\ntypically trained in two stages. In the pre-training stage, LLMs learn general linguistic patterns\nand commonsense knowledge from vast, unlabeled text data through autoregressive next-token\nprediction. However, pretrained models face a critical objective mismatch: while they are\noptimized for token prediction, real-world deployment requires alignment with complex human\nvalues such as helpfulness, honesty and harmlessness [Askell et al., 2021]. This mismatch calls\nfor an additional post-training stage, aiming at better aligning these pre-trained models with\nhuman preference.\nThe paper studies reinforcement learning from human feedback (RLHF), a post-training\nparadigm that adapts pre-trained models through reinforcement learning [RL, Sutton et al.,\n2018]. The RLHF literature has rapidly expanded in recent years, where existing algorithms\ncan be broadly categorized as reward-based or preference-based (Section 2 for a review). While\ndemonstrating remarkable success in domains including robotics control, video games, and\nLLMs fine-tuning [see e.g., Bai et al., 2022, Bakker et al., 2022, Christiano et al., 2017, Ouyang\net al., 2022, Ziegler et al., 2019], they often suffer from various model misspecifications (see\nalso Table 1 for a summary):\n1.Preference model misspecification . Most reward-based algorithms rely on the Bradley-\nTerry [BT, Bradley and Terry, 1952] preference model (see Equation 1). However, this\nmodel entails various unrealistic assumptions on human preference, including transitivity,\ncontext-independence and perfect relationality, which are likely violated based on empirical\nevidence [Agranov and Ortoleva, 2015, Gardner, 1970, Lindner and El-Assady, 2022, May,\n1arXiv:2506.01183v1  [cs.LG]  1 Jun 2025\n--- Page 2 ---\nFigure 1: A visualization of our proposed preference optimization algorithm. bπrefdenotes the\nspecified reference policy whereas bgdenotes the specified preference model. Our proposal is\ndoubly robust in that it requires correct specification of either the reference policy, or the\npreference model.\n1954, Michaud et al., 2020, Milano et al., 2021, Tversky, 1969]. While some preference-based\nalgorithms impose more general preference model (GPM) assumptions [see e.g., Zhang\net al., 2024d], their effectiveness still depends on correct model specification.\n2.Reward model misspecification . Under the BT model assumption, classical reward-\nbased algorithms first estimate the reward function from human preference data and then\napply RL algorithms such as the proximal policy optimization [PPO, Schulman et al., 2017]\nto derive the optimal policy. However, policy learning through RL is highly sensitive to\nthe estimated reward. Misspecifying the reward can lead to reward hacking [Laidlaw et al.,\n2024, Skalse et al., 2022] and misguide policy learning [Chen et al., 2024, Kaufmann et al.,\n2023, Zheng et al., 2023].\n3.Reference policy misspecification . To alleviate misspecification of the reward, recent\nalgorithms based on direct preference optimization [DPO, Rafailov et al., 2023] propose to\nexpress the reward in closed form using the reference policy for policy learning. However,\nthese algorithms are sensitive to the specification of reference policy [Gorbatovski et al.,\n2024, Liu et al., 2024b, Xu et al., 2024].\nDrawing from doubly robust estimation methods in econometrics and RL (see Section 2\nfor a literature review), we introduce a novel RLHF algorithm that is robust to model\nmisspecification and statistically efficient; see Figure 1 for a visualization of our algorithm.\nOur major contributions are summarized as follows:\n•We propose a robust and efficient estimator for preference evaluation, i.e., evaluating the\nprobability of a target policy being preferred over the reference policy. The proposed\npreference estimator achieves two desirable properties: (i) doublerobustness(Corollary 5.6)\n– it converges to the true preference probability when either the preference model or the\nreference policy is correctly specified, and (ii) semi-parametricefficiency(Corollary 5.7) – it\nattains the smallest mean squared error (MSE) among all regular and asymptotically linear\nestimators [Newey, 1990, Tsiatis, 2006].\n•Leveraging this preference estimator, we further develop a preference optimization algorithm\nfor LLM fine-tuning. The proposed algorithm maintains doublerobustness(Corollary (5.9))\nand remains consistent even when the BT model assumption is violated (Theorem 5.8).\nMeanwhile, when the BT model assumption holds, its suboptimality gap is lesssensitiveto\nthe reward model and reference policy compared to PPO- or DPO-based algorithms, and is\nlikely smaller than that of both types of algorithms (Theorem 5.10).\n2\n--- Page 3 ---\n2 Related Works\nOur work is closely related to reward- and preference-based RLHF algorithms, as well as\ndoubly robust (DR) methods. We discuss these related works below.\nReward-based RLHF. Reward-based algorithms assume the existence of a latent utility\nor reward function that determines human preferences, estimate the reward function from the\ndata and apply RL for policy learning. Recent research has focused on addressing practical\nchallenges such as reward hacking and model-collapse. These issues arise due to PPO’s\nsensitivity to reward specification, gradient clipping thresholds, and the tuning parameter\ncontrolling Kullback–Leibler (KL)-divergence regularization [Engstrom et al., 2019, Xiao et al.,\n2024, Zheng et al., 2023]. Existing approaches to these challenges fall into three categories: (i)\nThe first category focuses on improving the reward learning algorithm to obtain more accurate\nreward functions [Chan et al., 2024, Fu et al., 2025, Gao et al., 2024, Li et al., 2023b, Liu\net al., 2024a, Xiao et al., 2025, Ye et al., 2025]. (ii) The second category develops better policy\nlearning algorithms using the estimated reward function [Hu, 2025, Liu et al., 2025b, Shao\net al., 2024, Wu et al., 2024a, Yu et al., 2025, Yuan et al., 2025, Zhang et al., 2024a]. (iii) The\nthird category is DPO-based, which bypasses reward learning entirely and directly optimizes\npolicies under the BT model assumption [Ethayarajh et al., 2024, Song et al., 2024, Tang\net al., 2024, Wang et al., 2023a, Zhao et al., 2022].\nUnlike many of these algorithms, our proposal does not rely on the BT model assumption,\nand is more robust to the misspecification of reward or reference policy when the BT model\nholds.\nPreference-based RLHF. Preference-based algorithms do not assume the existence of\na latent reward function at all; instead, they search the optimal policy that maximizes the\nalignment with human preferences [see e.g., Azar et al., 2024]. In particular, there is a growing\nline of research that adopts the Nash learning from human feedback [NLHF, Munos et al., 2023]\nframework, which formulates the alignment problem as a two-player constant-sum game and\nsolves for policies that achieve the Nash equilibrium [Calandriello et al., 2024, Liu et al., 2025a,\nRosset et al., 2024, Swamy et al., 2024, Wu et al., 2024b, Ye et al., 2024, Zhang et al., 2024c].\nBeyond NLHF, Wang et al. [2023b] develops a Bayesian approach for alignment, whereas Hong\net al. [2024] and Zhang et al. [2024d] propose energy-based and general preference models to\nrelax the BT model assumption.\nOur proposal belongs to this class of preference-based methods. In particular, the proposed\nalgorithm is most closely related to the identity preference optimization (IPO) algorithm\nproposedbyAzaretal.[2024], asbothmaximizingthesameobjectivefunctioninthepopulation\nlevel (see Section 4 for the objective). However, unlike IPO, our proposed method is robust\nto misspecifications of the reference policy. Similarly, compared to Zhang et al. [2024d], the\nproposed algorithm is more robust to the misspecification of the preference model. Finally,\nour work differs from NLHF in its primary focus: we study robust and statistically efficient\npreference estimation from data, rather than developing computationally efficient algorithms\nto solve the Nash equilibrium.\nDoubly robust methods. DR has been extensively studied in statistics, econometrics\nand machine learning. These methods originate from the missing data and causal inference\nliterature [see e.g., Robins et al., 1994, Scharfstein et al., 1999]. To illustrate these methods,\nconsider the fundamental causal inference problem of estimating the average treatment effect\n(ATE) – the difference in the mean outcome between a newly-developed treatment policy\nand a baseline policy for a given patient population. DR first estimates two models from the\ndata: (i) a propensity score model (similar to the reference policy in LLMs) that characterizes\nthe treatment assignment mechanism and (ii) an outcome regression model (similar to the\n3\n--- Page 4 ---\nTable 1: Robustness of different algorithms to model misspecification. Our algorithm is\ndenoted by DRPO, short for doubly robust preference optimization.\nRobust to misspecified: preference model reward model reference policy\nRLHFReward-basedPPO-type ✗ ✗ ✓\nDPO-type ✗ ✓ ✗\nPreference-basedIPO ✓ - ✗\nGPM ✗ - ✓\nDRPO ✓ ✓ ✓\nreward function) that specifies the conditional mean function of a patient’s outcome. It\nthen employs both models to construct the ATE estimator, whose consistency requires only\none of the models to be correct. Furthermore, when both models are correct, the resulting\nestimator is semiparametrically efficient [Bang and Robins, 2005]. The favorable statistical\nproperties of these methods have led to extensive follow-up research [see e.g., Cui et al., 2023,\n2024, Farrell et al., 2021, Fulcher et al., 2020, Han et al., 2023, Imai and Ratkovic, 2014,\nKennedy, 2023, Kennedy et al., 2017, Künzel et al., 2019, Li et al., 2022, 2023a, Nie and\nWager, 2021, Oprescu et al., 2019, Robins et al., 2017, Shi et al., 2019, Tan, 2010, Tsiatis\net al., 2011, Vermeulen and Vansteelandt, 2015, Wager and Athey, 2018, Wang and Han, 2024,\nWang and Tchetgen Tchetgen, 2018, Wang and Shah, 2024]. A seminal extension appears in\nChernozhukov et al. [2018], which proposes to learn both the propensity score and outcome\nregression models using machine learning methods to deal with complex data structures with\nhigh-dimensional covariates, texts or images.\nBeyond treatment effect estimation in causal inference, doubly robust methods have been\nwidely applied to a broad range of other problems, including the estimation and evaluation\nof optimal (dynamic) treatment regimes [Fan et al., 2017, Jiang et al., 2017, Luedtke and\nVan Der Laan, 2016, Robins, 2004, Schulte et al., 2015, Shi et al., 2018, Song et al., 2017,\nZhang et al., 2012, 2013], conditional independence testing [Quinzan et al., 2023, Shah and\nPeters, 2020, Shi et al., 2021, Zhang et al., 2019, 2024b], offline policy learning [Dudík et al.,\n2014, Kallus and Uehara, 2020b, Liao et al., 2022, Shi et al., 2024a, Uehara et al., 2020b] and\noff-policy evaluation [OPE, Bibaut et al., 2019, Cao and Zhou, 2024, Farajtabar et al., 2018,\nJiang and Li, 2016, Kallus and Uehara, 2020a, Kallus and Zhou, 2018, Shi et al., 2022, 2024b,\nSu et al., 2020, Tang et al., 2020, Thomas and Brunskill, 2016, Uehara et al., 2020a, Xie et al.,\n2023, Xu et al., 2023].\nHowever, none of the aforementioned works considers the application of fine-tuning LLMs – a\ngap we aim to bridge by connecting these two vibrant research areas.\n3 RLHF Preliminaries\nData generating process. Assume we are given a dataset D, consisting of ni.i.d. tuples\nof the form (X, Y(1), Y(2), Z). Each of these tuples is generated as follows: Given a prompt X,\ntwo independent responses (Y(1), Y(2))are generated under a reference policy πrefsuch that\nY(1), Y(2)∼πref(·|X). These data (X, Y(1), Y(2))are then shown to a human expert, who\nprovides a binary preference Z=I(Y(1)≻Y(2))where Y(1)≻Y(2)indicates that the first\nresponse is preferred, and I(•)denotes the indicator function. Additionally, let g∗denote the\npreference function such that g∗(X, Y(1), Y(2)) =P(Y(1)≻Y(2)|X)determines the probability\nofY(1)being favored over Y(2)conditional on X.\nWe remark that the reference policy πrefis not always known. For instance, the responses\nmight be generated by an LLM different from the target model that we wish to fine-tune [Bai\net al., 2022]. Furthermore, the responses might be produced by a heterogeneous set of models\nrather than a single model [Stiennon et al., 2020].\n4\n--- Page 5 ---\nBT model. As commented in Section 2, most existing reward-based RLHF algorithms\nimpose the BT model assumption, which requires the preference function g∗to take the\nfollowing form,\ng∗(x, y(1), y(2)) =σ(r∗(y(1), x)−r∗(y(2), x)), (1)\nwhere r∗denotes some underlying reward function that measures how well a response answers\na given prompt, and σdenotes the sigmoid function. As commented in the introduction, this\nassumption is likely violated due to the inherent intransitivity, inconsistency and stochasticity\nin human preference.\nAssuming (1)holds, the goal is to learn an optimal policy π∗that maximizes the expected\nreward\nJ(π) =E[Ey∼π(·|X)r∗(y, X)], (2)\namong all policies π. Here, the outer expectation is taken with respect to the prompt\ndistribution, whereas the inner expectation is taken with respect to the response generated by\na given policy π.\nWe next introduce two types of baseline algorithms – PPO-based and DPO-based – for learning\nπ∗. Both approaches operate under Assumption (1).\nPPO-based approaches. PPO-based algorithms proceed in two steps. In the first step, they\ncompute an estimated reward function brusing maximum likelihood estimation or empirical\nrisk minimization. In the second step, they learn π∗by maximizing\nEX∼D, y∼π(·|X)[br(y, X)]−β DKL[π(y|X)∥πref(y|X)], (3)\noverπ∈Π(e.g., a transformer-based policy class), where the expectation is taken over prompts\nXfrom the empirical data distribution and responses yfrom a target policy π,DKLdenotes\nthe KL divergence measure between the target and reference policies, and the tuning parameter\nβ >0controls the degree to which πis allowed to deviate from πref. The KL regularization\nterm in(3)encourages the learned policy to stay close to πref, in order to mitigate over-fitting\nand prevent the learned policy from collapsing to a narrow set of high-reward responses [Zheng\net al., 2023].\nDPO-based approaches. DPO-based algorithms are motivated by the fact that the argmax\nto(3)(denoted by bπ) can be represented in closed-form using the estimated reward br. This in\nturn yields the following closed-form expression for br,\nbr(y, x) =βlog\u0012bπ(y|x)\nπref(y|x)\u0013\n−C(x), (4)\nfor some response-independent function C(x)that will cancel out in pairwise comparisons.\nAs such, instead of solving bπin two steps, DPO-based approaches directly parameterize the\nreward via Equation (4)and compute bπin a single step – for example, by maximizing the\nlikelihood of the human preference data under the BT model.\nTo conclude this section, we note that, as shown in Equation (3), the optimal policy computed\nby PPO can be highly sensitive to the estimated reward function br. While DPO-based\napproaches eliminate this dependence, Equation (4)reveals that their optimization relies on\nthe specification of the reference policy πref. Due to these sensitivities, even under the idealized\nsetting where the BT model holds, both PPO- and DPO-based algorithms can underperform\nour proposed algorithm, which is inherently more robust to misspecification in both brandπref.\nWe provide theoretical justification in Section 5 and empirical validation in Section 6.\n5\n--- Page 6 ---\n4Double Robust Preference Evaluation and Optimization\nThis section introduces the proposed doubly robust approach; see Figure 1 for a visualization.\nDifferent from these reward-based algorithms discussed in Section 3, we adopt a preference-\nbased approach that searches the optimal policy by maximizing its total preference. Specifically,\ngiven a target policy π, itstotal preference over the reference policy [Azar et al., 2024] is\ndefined by\np∗(π) :=P(π≻πref) =E[Ey∼π(·|X),y′∼πref(·|X)g∗(X, y, y′)],\nwhere we recall that g∗denotes the preference function P(y > y′|X), and the outer expectation\nis taken with respect to the prompt distribution. As both Y(1)andY(2)are generated under\nπref, we have\np∗(π) =1\n22X\na=1E[Ey∼π(·|X)g∗(X, y, Y(a))]. (5)\nFor preference evaluation, our goal is to accurately estimate p∗(π)for a given target policy\nπfrom the dataset D. In the following, we first introduce two baseline estimators: a direct\nmethod (DM) estimator and an importance sampling (IS) estimator, where the names are\nborrowed from the OPE literature [see e.g., Uehara et al., 2022]. We next introduce our\nproposed DR estimator, which combines both DM and IS for efficient and robust preference\nevaluation.\nDM estimator. The direct method estimator is motivated by (5). It proceeds by first\nestimating g∗and then plugging the estimated g∗(denoted by bg) into(5)to construct the\nestimator,\nbpDM(π) =1\n2EX∼D,y∼π(·|X)[bg(X, y, Y(1)) +bg(X, y, Y(2))], (6)\nwhere Xis drawn from the empirical data distribution, yis drawn from πand the expectation\ncan be approximated using Monte Carlo sampling.\nWhen an external preference model is available, it can be used directly as bg, as in [Munos\net al., 2023]. Otherwise, g∗can be estimated from the data D. For instance, under the BT\nmodel assumption, one can estimate the reward function r∗and plug the estimator into (1)to\nderive bg. Alternatively, one can employ more general preference models that do not rely on\nthe BT model.\nIS estimator. The second baseline estimator is the IS estimator, which is motivated by the\nfollowing lemma that expresses p∗(π)using the IS ratio w(y, x) =π(y|x)/πref(y|x).\nLemma 4.1. Assume w(y, x)<∞for any x,y. Then\np∗(π) =1\n2E[w(Y(1), X)Z+w(Y(2), X)(1−Z)].\nThe proof of Lemma 4.1 is straightforward. It follows directly from the symmetry of pairwise\ncomparisons where the preference can be equivalently expressed using either g∗(X, y, y′)or\n1−g∗(X, y′, y), and an application of the change-of-measure theorem (see Appendix A.1).\nBased on this identity, we define the following IS estimator:\nbpIS(π) =1\n2E(X,Y(1),Y(2),Z)∼Dhπ(Y(1)|X)\nbπref(Y(1)|X)Z+π(Y(2)|X)\nbπref(Y(2)|X)(1−Z)i\n, (7)\nwherebπrefdenotes an estimated reference policy. If πrefis known, we can directly use the oracle\nreference policy. Otherwise, for some external datasets [e.g., Bai et al., 2022], well-trained\n6\n--- Page 7 ---\nreference models are available and can be used as bπref. Finally, when no such external model\nis available and πrefis unknown, we estimate it from the observed data tuples (X, Y(1), Y(2))\nusing supervised fine-tuning (SFT).\nDR estimator. A closer look at Equations (6)and(7)reveals that the DM and IS esti-\nmators’ consistencies depend crucially on the correct specification of the preference function\nand the reference policy. We next introduce our proposed DR estimator, which is more\nrobust to misspecifications in these models. It relies on the following estimating function\nψ(X, Y(1), Y(2), Z;π,bπref,bg), defined as\n1\n22X\na=1Ey∼π(·|X)[bg(X, y, Y(a))] +1\n22X\na=1(−1)a−1π(Y(a)|X)\nbπref(Y(a)|X)[Z−bg(X, Y(1), Y(2))].(8)\nBy definition, this estimating function contains two terms: (i) the first term is essentially the\nestimating function of the DM estimator in (6), and (ii) the second term is an augmentation\nterm, which is similar to IS in (7), but with the observed preference Zreplaced by its residual\nZ−bg(X, Y(1), Y(2)). The purpose of introducing the additional augmentation term is to\ncorrect for the bias introduced by misspecification of the preference model in the DM estimator.\nThis leads to our DR estimator,\nbpDR(π) =E(X,Y(1),Y(2),Z)∼Dψ(X, Y(1), Y(2), Z;π,bπref,bg). (9)\nSimilar to the DR estimator in the bandit setting [Dudík et al., 2014], (9)is reduced to the\nDM estimator when setting bgto zero, and the IS estimator when setting the IS ratio π/bπref\nto zero. However, as shown in (8), a key different from those bandit estimators is that in\nour pairwise comparison setting, each data tuple is used twice – as (X, Y(1), Y(2), Z)and\n(X, Y(2), Y(1),1−Z)– in constructing the estimating function. This effectively reduces the\nvariance of the resulting estimator. As a result, we will formally show in Section 5 that our\nDR estimator is semi-parametrically efficient. Additionally, we will establish the consistency\nof (9) when either bgorbπrefis correctly specified.\nPreference optimization. For preference optimization, our goal is to identify the optimal\npolicy that maximizes the average total preference p∗(π). Under the BT model assumption, it\nis immediate to see that the argmax is equivalent to π∗defined in (2). Given the proposed\nDR estimator, we estimate the optimal policy by solving\nbπ= arg max\nπ∈Πn\nbpDR(π)−βEX∼DDKL[π(· |X)∥bπref(· |X)]o\n. (10)\nWe refer to (10)as DRPO, short for doubly robust preference optimization. Theoretically,\nwe will show in Section 5 that our estimated policy bπachieves a smaller regret bound than\nPPO- and DPO-type algorithms when the BT assumption holds. Practically, we implement\nthree refinements to stabilize the training: (i) clipping the IS ratio to avoid extremely large\nIS ratio; (ii) designing a pseudo objective function to enable Monte Carlo sampling from the\ntarget policy during optimization; (iii) adopting the KL divergence measure from the group\nrelative policy optimization [Shao et al., 2024] for variance reduction. Details are relegated to\nAppendix B to save space.\n5 Theoretical Analysis\nWe begin with a summary of our theories; Figure 2 outlines the roadmap. Our theories are\nconcerned with (i) the MSE of our preference evaluation estimator bpDR(π)(see(9)), and (ii)\nthe regret bounds of bπ(see(10)) computed by the proposed preference optimization algorithm.\nSpecifically, Theorem 5.5 provides a finite sample upper bound for the MSE of bpDR(π), which\n7\n--- Page 8 ---\nFigure 2: A visualization of our theoretical findings.\nin turn yields its double robustness (Corollary 5.6) and semi-parametric efficiency (Corollary\n5.7). Meanwhile, Theorem 5.8 upper bounds the difference in total preference between the\noptimal in-class policy and bπ, without assuming the BT model holds. It reveals the double\nrobustness property of our preference optimization algorithm (Corollary 5.9). When the BT\nmodel holds, Theorem 5.10 further upper bounds the suboptimal gap of bπ, demonstrating that\nit general achieves smaller gaps than PPO- and DPO-based algorithms.\nWe next introduce some technical conditions.\nAssumption 5.1 (Coverage) .Both π/πrefandπ/bπrefare upper bounded by ϵ−1for some\nconstant ϵ >0.\nAssumption 5.2 (Boundedness) .When the BT model holds, both the oracle reward function\nr∗and its estimator are bounded functions.\nAssumption 5.3 (Realizability) .When the BT model holds, π∗that maximizes the expected\nreward (see (2)) belongs to the parameterized policy class Πin (10).\nAssumption 5.4 (Model complexity) .Πbelongs to the Vapnik–Chervonenkis (VC) type\nclass [Chernozhukov et al., 2014, Definition 2.1] with a finite VC index v >0.\nWe remark that similar coverage, boundedness and realizability assumptions are commonly\nimposed in the OPE and RL literature [see e.g., Chen and Jiang, 2019, Fan et al., 2020, Uehara\net al., 2022]. The VC-class condition is also frequently assumed in statistics and machine\nlearning [see e.g., Shalev-Shwartz and Ben-David, 2014, Van Der Vaart et al., 1996].\nMSE of bpDR(π).We next study the statistical properties of the proposed preference estima-\ntorbpDR(π). Without loss of generality, we also assume both bπrefandbg(orbr, in the case where\nthe BT model holds) are obtained from external models independent of D. This condition is\nmild. Even when such external models are not available and bπrefandbgare learned internally\nfromD, independence can be preserved using sample-splitting and cross-fitting [Chernozhukov\net al., 2018].\nTheorem 5.5 (MSE).Under Assumption 5.1, with ndata tuples, the semi-parametric\nefficiency bound (SEB) for estimating p∗(π)is given by n−1Var(ψ(X, Y(1), Y(2), Z;π, πref, g∗)).\nAdditionally, the MSE of our bpDR(π)equals\nSEB +O\u00121\nn∥bg−g∗∥\u0013\n+O\u00121\nn∥bπref\nπref−1∥\u0013\n+O\u0012\n∥bπref\nπref−1∥2· ∥bg−g∗∥2\u0013\n,(11)\nwhere ∥bπref/πref−1∥and∥bg−g∗∥denote the root mean squared errors of bπref/πrefandbg; see\nAppendix A.3 for their definitions.\nThe first part of Theorem 5.5 establishes the SEB – the smallest-possible MSE that one can\nhope for estimating p∗(π). The second part upper bounds the excess MSE of our estimator\nover SEB. Specifically, this excess MSE consists of three parts: the first two are excess variance\n8\n--- Page 9 ---\nterms arising from estimation errors in the reference policy and the preference model, while\nthe third is a bias term introduced by these estimation errors. Notably, both variance terms\nand SEB scale as O(n−1). Consequently, when either bπreforbgis correctly specified, the MSE\nofbpDR(π)coverages to zero as the sample napproaches to infinity. This establishes the double\nrobustness property of our estimator, which we state below.\nCorollary 5.6 (Doubly robust evaluation) .Under Assumption 5.1, when either bπreforbgis\ncorrectly specified, the MSE of bpDR(π)decays to zero as napproaches to infinity.\nWe next consider the case where both bπrefandbgare “approximately” correct in that both\nroot MSEs ∥bπref/πref−1∥and∥bg−g∗∥decay to zero as n→ ∞. Since SEB is of the order\nO(n−1), the first two variance terms in (11)decay to zero at a much faster rate than SEB.\nMeanwhile, when the product ∥bπref/πref−1∥∥bg−g∗∥=o(n−1/2), the last bias term in (11)\nbecomes negligible compared to SEB as well. Together, these conditions imply that the MSE\nofbpDR(π)asymptotically matches the SEB, which establishes the semi-parametric efficiency\nof our estimator. We also remark that conditions similar to ∥bπref/πref−1∥∥bg−g∗∥=o(n−1/2)\nare widely assumed in the literature [see e.g., Chernozhukov et al., 2018, Farrell et al., 2021,\nKallus and Uehara, 2022].\nCorollary 5.7 (Semi-parametric efficiency) .Under Assumption 5.1, when both ∥bπref\nπref−1∥and\n∥bg−g∗∥decay to zero as n→ ∞, and their product is o(n−1/2), then MSE( bpDR(π))/SEB →1\nasn→ ∞.\nRegret of bπ.Next, we derive the statistical properties of the proposed policy bπ. When\nthe BT model assumption is violated, we measure the regret of a given policy πusing the\ngap between the total preference of the best in-class policy and that of π, i.e.,Reg(π) =\nsupπ′∈Πp∗(π′)−p∗(π). By definition, a smaller regret indicates a better policy.\nTheorem 5.8 (Regret).Under Assumptions 5.1 (assuming it holds for any π∈Π) and 5.4,\nthen\nReg(bπ) =O\u0010\nβ+rv\nn+v\nn+∥bπref/πref−1∥∥bg−g∗∥\u0011\n. (12)\nIt can be seen from (12)that the regret bound depends on several factors: (i) it decays\nwith the sample size n; (ii) it increases with the regularization parameter βin the KL\ndivergence penalty; (iii) it increases with v, which measures the complexity of the policy\nclass; (iv) it decreases with the estimating error of the reference policy and the preference\nmodel. Crucially, the last dependence appears as the product ∥bπref/πref−1∥∥bg−g∗∥, which\nenables us to establish the double robustness property in the context of preference optimization.\nCorollary 5.9 (Doubly robust optimization) .Suppose β→0asn→0. Under the conditions\nin Theorem 5.8, when either bπreforbgis correctly specified, the regret of bπdecays to zero as\nn→ ∞.\nFinally, we restrict our attention to the ideal setting where the BT model holds and upper\nbound the suboptimality gap, defined as the difference in the expected reward between the\noptimal policy π∗and our bπ, i.e., J(π∗)−J(bπ).\nTheorem 5.10 (Suboptimality gap) .Suppose the BT model assumption in (1)holds. Under\nAssumptions 5.2, 5.3, and the conditions in Theorem 5.8, the suboptimality gap of bπis upper\n9\n--- Page 10 ---\nbounded by\nO\u0010\nβ+rv\nn+v\nn+∥bπref/πref−1∥∥br−r∗∥\u0011\n. (13)\nMeanwhile, for PPO-based algorithms, their suboptimality gaps are bounded by\nO\u0010\nβ+rv\nn+v\nn+∥br−r∗∥\u0011\n. (14)\nFinally, for DPO-based algorithms, their suboptimality gaps are bounded by\nO\u0010\nexp(−¯cβ−1) +1\nβrv\nn+∥bπref/πref−1∥\u0011\n, (15)\nfor some constant ¯c >0, under conditions specified in Appendix A.7.\nAccording to (12)and(14)that, by using a sufficiently small β, the suboptimality gaps\nof PPO-based and our algorithms are of the order O(n−1/2+∥br−r∗∥)andO(n−1/2+\n∥bπref/πref−1∥∥br−r∗∥), respectively. As for DPO-based algorithms, For DPO-based algorithms,\nsetting β=¯c−1Clognfor some constant C > 0makes the first term in (15)of order\nO(n−C), which can be made arbitrarily small with a sufficiently large C. The second term\nremains of order O(n−1/2)up to a logarithmic factor, yielding an overall suboptimality gap\nofO(n−1/2logn+∥bπref/πref−1∥). Consequently, our algorithm’s suboptimality gap is more\nrobust to estimation errors in the reference policy and preference model, as these errors\ninfluence our bound only through their product. To the contrary, for PPO- and DPO-based\nalgorithms, these errors affect their suboptimality bounds in the first order. In particular, when\nthese errors converge to zero at a rate of O(n−c)for some 0< c < 1/2, our algorithm achieves\nstrictly smaller suboptimality bounds than both DPO- and PPO-based algorithms.\n6 Experiments\nIn this section, we first use the IMDb dataset [Maas et al., 2011] to empirically validate\nthe double robustness property of our preference estimator bpDR(Equation 9) established in\nCorollary 5.6. We next compare the proposed preference optimization algorithm (Equation\n10) against baseline approaches on the Too Long; Didn’t Read [TL;DR, Völske et al., 2017]\nandAnthropic Helpful and Harmless [HH, Bai et al., 2022] datasets.\n6.1 Preference Evaluation\nTask and objective. We consider the controlled sentiment generation task which aims\nto produce positive movie reviews using the IMDb dataset. We first apply SFT to the\nEleutherAI/gpt-neo-125m base model [Black et al., 2021], which serves as the reference\npolicy for response generation. The generated responses are then annotated using a pre-trained\nsentiment classifier to produce preference labels. Using these synthetic data, we train an\noptimal policy via DPO. Our objective in this section is to evaluate the total preference of\nthis DPO-trained policy over the SFT-based reference policy. Its oracle value, computed via\nMonte Carlo, is 0.681. Additional details on data generation and model training are provided\nin Appendix C.1.\nEvaluation and result. To empirically assess the double robustness property, we evaluate\nfour variants of our preference estimator, each with either the preference model and/or the\nreference policy correctly specified or misspecified. To misspecify the preference model, we set\nbgto a uniformly random value in [0,1]. To misspecify the reference policy, we use the unfine-\ntunedEleutherAI/gpt-neo-125m base model. Figure 3 displays the MSEs (solid lines on left\npanel) and their associated 95% confidence intervals (shaded areas) of the four estimators\nacross different sample sizes, averaged over 500 simulations. It can be seen that the estimator\n10\n--- Page 11 ---\nFigure 4: Pairwise win rate matrices between different methods across two datasets. Left:\nTL;DR dataset. Right: HH dataset. Each entry indicates how often the row method\noutperforms the column method; higher values denote better performance.\nwith both models misspecified (red line) exhibits a significantly larger MSE than the other\nthree and shows minimal improvement beyond 800 samples. To the contrary, when either the\npreference model or the reference policy is correctly specified (yellow and green lines), the\nMSE is substantially reduced with a moderately large sample size. This aligns with the double\nrobustness property. Meanwhile, the estimator with both correctly specified models (blue\nline) achieves the lowest MSE (being very close to zero with 1500 data tuples), supporting its\nsemiparametric efficiency.\n6.2 Preference Optimization\nFigure 3: MSEs of different preference eval-\nuation estimators on the IMDb dataset.\nShaded areas visualize the 95% confidence\nbands.Tasks and objective. This section considers\ntwo tasks: summarization andhuman dialogue .\nFirst, for summarization, we use the TL;DR\ndataset with preference annotations from [Stien-\nnon et al., 2020] (lengthy Reddit posts as prompts,\nbehavioral SFT models’ summaries as responses)\nto fine-tune LLMs for concise, informative sum-\nmaries. The SFT and reward models for this\ntask are from cleanrl [Huang et al., 2024], with\nthe SFT model trained on a similar but differ-\nent dataset. Second, for human dialogue, the HH\ndataset (human queries as prompts) is used to\nalign LLMs for helpful responses. Its SFT and\nreward models are trained using TRL[von Werra\net al., 2020] framework due to the unavailability of\npre-existing ones. For each task, a reward-based\nBT preference model (using the same reward model for PPO training) and a general pref-\nerence model [Zhang et al., 2024d] are adopted to serve as bg(donating as DRPO-BT and\nDRPO-GPM). Refer to more details of the implementation and baseline model training in\nAppendix C.2.\nEvaluation and result. Given the absence of ground-truth preference or reward models, we\nfollow prior work [Rafailov et al., 2023, Wu et al., 2024a, Ye et al., 2025] and use GPT-4o-mini\nto assess the quality of generated responses (details in Appendix C.2). Win rates at default\ntemperature 1.0 are presented in Figure 4, with results at other temperatures presented in\nAppendix D. In summarization , both DRPO-BT and DRPO-GPM largely outperform DPO\nand PPO. For this task, we use an external reference policy model for bπref, which is likely\n11\n--- Page 12 ---\nmisspecified due to being trained on a different dataset. This likely contributes to DPO’s poor\nperformance. However, despite relying on the same reference policy, the superior performance\nof our algorithms highlights their robustness to reference policy misspecification. In human\ndialogue, DRPO-GPM demonstrates the best performance, whereas DRPO-BT outperforms\nPPO and achieves comparable performance to DPO. The relatively weaker performance of\nPPO suggests potential misspecification in the reward model for this task. Despite using the\nsame reward model, DRPO-BT effectively mitigates this challenge and achieves a win rate\nof 57% against PPO, demonstrating its robustness. Finally, it is worth highlighting that the\nhyperparameters for our method are not extensively tuned, unlike the baselines – particularly\nPPO, for which the best-performing version is reported – further demonstrating the robustness\nof our proposal.\n7 Discussion\nThis work introduces a novel doubly robust preference optimization (DRPO) for LLM fine-\ntuning. Ourapproachenablesaccuratepreferenceevaluationandpolicyoptimization, providing\nrobustness against misspecifications in both the reference policy and the preference model. We\nformally establish that our preference evaluation estimator is both doubly robust (Corollary\n5.6) and semiparametrically efficient (Corollary 5.7) and demonstrate that our optimization\nprocedure yields policies with a small regret (Theorem 5.8), and a lower suboptimality bound\nthan DPO and PPO (Theorem 5.10). Our empirical results verify and reinforce the theoretical\nadvantages, showing that DRPO achieves more robust and often superior performance to\nthe baselines in preference optimization (Section 6.2). effectively address the high variance\npotentially arising from importance sampling ratios. An interesting direction for future work\nis to explore extensions of DRPO within the NLHF framework, by generalizing the estimator\nto handle arbitrary pairwise comparisons beyond the reference policy.\nA potential limitation of our methodology is its reliance on IS ratios for preference evaluation,\nwhich can result in high variance when the target and behavior policies differ substantially.\nWhile we apply clipping to the IS ratios to partially mitigate this issue, the issue may still\nremain a concern particularly when the reference policy differs substantially from the target\npolicy. Additionally, although our experiments on training large language models with real-\nworld datasets demonstrate the effectiveness of our approach, we did not evaluate it on\nsubstantially larger-scale models due to hardware constraints. This is a potential limitation of\nour experimental validation.\nOur work contributes to the development of a doubly robust approach to preference evaluation\nand optimization, which aims to improve the alignment of large language models (LLMs)\nwith human preferences. This may improve models’ ability, contributing to safer and more\ncontrollable LLM behavior. However, improved alignment methods may be misused, such as\naligning models with the preferences of a specific group will disadvantage others. Furthermore,\nifthetrainingdatacontainspreferencesforharmfulcontent, themodelmaylearnandreproduce\nsuch harmful behaviors. The alignment algorithm itself does not produce harmful content;\nsuch outcomes arise only when the model is optimized to align with harmful preferences.\nTherefore, it is important to carefully manage the dataset to prevent large language models\nfrom giving harmful responses.\nReferences\nAgranov, M. and Ortoleva, P. (2015). Stochastic choice and preferences for randomization.\nJournal of Political Economy , 125:40 – 68.\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N.,\n12\n--- Page 13 ---\nMann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for\nalignment. arXiv preprint arXiv:2112.00861 .\nAzar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D.\n(2024). A general theoretical paradigm to understand learning from human preferences. In\nInternational Conference on Artificial Intelligence and Statistics , pages 4447–4455. PMLR.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\nGanguli, D., Henighan, T., et al. (2022). Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\nBakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J.,\nMcAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al. (2022). Fine-tuning language\nmodels to find agreement among humans with diverse preferences. Advances in Neural\nInformation Processing Systems , 35:38176–38189.\nBang, H. and Robins, J. M. (2005). Doubly robust estimation in missing data and causal\ninference models. Biometrics , 61(4):962–973.\nBibaut, A., Malenica, I., Vlassis, N., and Van Der Laan, M. (2019). More efficient off-policy\nevaluation through regularized targeted learning. In International Conference on Machine\nLearning , pages 654–663. PMLR.\nBickel, P. J., Klaassen, C. A., Ritov, Y., and Wellner, J. A. (1998). Efficient and Adaptive\nEstimation for Semiparametric Models . Springer, New York.\nBlack, S., Leo, G., Wang, P., Leahy, C., and Biderman, S. (2021). GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please\ncite it using these metadata.\nBradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika , 39(3/4):324–345.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems , 33:1877–1901.\nCalandriello, D., Guo, D., Munos, R., Rowland, M., Tang, Y., Pires, B. A., Richemond, P. H.,\nLan, C. L., Valko, M., Liu, T., et al. (2024). Human alignment of large language models\nthrough online preference optimisation. arXiv preprint arXiv:2403.08635 .\nCao, D. and Zhou, A. (2024). Orthogonalized estimation of difference of q-functions. arXiv\npreprint arXiv:2406.08697 .\nChan, A. J., Sun, H., Holt, S., and Van Der Schaar, M. (2024). Dense reward for free in\nreinforcement learning from human feedback. arXiv preprint arXiv:2402.00782 .\nChen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement\nlearning. In International Conference on Machine Learning , pages 1042–1051. PMLR.\nChen, Y., Zhu, D., Sun, Y., Chen, X., Zhang, W., and Shen, X. (2024). The accuracy paradox\nin rlhf: When better reward models don’t yield better language models. arXiv preprint\narXiv:2410.06554 .\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., andRobins,\nJ. (2018). Double/debiased machine learning for treatment and structural parameters. The\nEconometrics Journal , 21(1):C1–C68.\n13\n--- Page 14 ---\nChernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema\nof empirical processes. The Annals of Statistics , pages 1564–1597.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30.\nCui, Y., Kosorok, M.R., Sverdrup, E., Wager, S., andZhu, R.(2023). Estimatingheterogeneous\ntreatment effects with right-censored data via causal survival forests. Journal of the Royal\nStatistical Society Series B: Statistical Methodology , 85(2):179–211.\nCui, Y., Pu, H., Shi, X., Miao, W., and Tchetgen Tchetgen, E. (2024). Semiparametric proximal\ncausal inference. Journal of the American Statistical Association , 119(546):1348–1359.\nDudík, M., Erhan, D., Langford, J., and Li, L. (2014). Doubly Robust Policy Evaluation and\nOptimization. Statistical Science , 29(4):485 – 511.\nEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., and Madry, A.\n(2019). Implementation matters in deep rl: A case study on ppo and trpo. In International\nconference on learning representations .\nEthayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. (2024). Kto: Model\nalignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 .\nFan, C., Lu, W., Song, R., and Zhou, Y. (2017). Concordance-assisted learning for estimating\noptimal individualized treatment regimes. Journal of the Royal Statistical Society Series B:\nStatistical Methodology , 79(5):1565–1582.\nFan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep q-learning. In\nLearning for dynamics and control , pages 486–489. PMLR.\nFarajtabar, M., Chow, Y., and Ghavamzadeh, M. (2018). More robust doubly robust off-policy\nevaluation. International Conference on Machine Learning , pages 1447–1456.\nFarrell, M. H., Liang, T., and Misra, S. (2021). Deep neural networks for estimation and\ninference. Econometrica , 89(1):181–213.\nFu, J., Zhao, X., Yao, C., Wang, H., Han, Q., and Xiao, Y. (2025). Reward shaping to mitigate\nreward hacking in rlhf. arXiv preprint arXiv:2502.18770 .\nFulcher, I. R., Shpitser, I., Marealle, S., and Tchetgen Tchetgen, E. J. (2020). Robust inference\non population indirect causal effects: the generalized front door criterion. Journal of the\nRoyal Statistical Society Series B: Statistical Methodology , 82(1):199–214.\nGao, J., Xu, S., Ye, W., Liu, W., He, C., Fu, W., Mei, Z., Wang, G., and Wu, Y. (2024). On de-\nsigning effective rl reward at training time for llm reasoning. arXiv preprint arXiv:2410.15115 .\nGardner, M. (1970). Mathematical games, the paradox of the nontransitive dice and the elusive\nprinciple of indifference .\nGorbatovski, A., Shaposhnikov, B., Malakhov, A., Surnachev, N., Aksenov, Y., Maksimov, I.,\nBalagansky, N., and Gavrilov, D. (2024). Learn your reference model for real good alignment.\narXiv preprint arXiv:2404.09656 .\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi,\nX., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 .\n14\n--- Page 15 ---\nHan, L., Shen, Z., and Zubizarreta, J. (2023). Multiply robust federated estimation of\ntargeted average treatment effects. Advances in Neural Information Processing Systems ,\n36:70453–70482.\nHartmann, J., Heitmann, M., Siebert, C., and Schamp, C. (2023). More than a feeling:\nAccuracy and application of sentiment analysis. International Journal of Research in\nMarketing , 40(1):75–87.\nHong, Y., Zhang, H., Bao, J., Jiang, H., and Song, Y. (2024). Energy-based preference model\noffers better offline alignment than the bradley-terry preference model. arXiv preprint\narXiv:2412.13862 .\nHu, J. (2025). Reinforce++: A simple and efficient approach for aligning large language\nmodels.arXiv preprint arXiv:2501.03262 .\nHuang, S., Noukhovitch, M., Hosseini, A., Rasul, K., Wang, W., and Tunstall, L. (2024). The\nn+ implementation details of rlhf with ppo: A case study on tl; dr summarization. arXiv\npreprint arXiv:2403.17031 .\nImai, K. and Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal\nStatistical Society Series B: Statistical Methodology , 76(1):243–263.\nJiang, N. and Li, L. (2016). Doubly robust off-policy evaluation for reinforcement learning. In\nInternational Conference on Machine Learning , pages 652–661. PMLR.\nJiang, R., Lu, W., Song, R., and Davidian, M. (2017). On estimation of optimal treatment\nregimes for maximizing t-year survival probability. Journal of the Royal Statistical Society\nSeries B: Statistical Methodology , 79(4):1165–1185.\nKallus, N. and Uehara, M. (2020a). Double reinforcement learning for efficient off-policy\nevaluation in markov decision processes. Journal of Machine Learning Research , 21:1–63.\nKallus, N. and Uehara, M. (2020b). Statistically efficient off-policy policy gradients. In\nInternational Conference on Machine Learning , pages 5089–5100. PMLR.\nKallus, N. and Uehara, M. (2022). Efficiently breaking the curse of horizon in off-policy\nevaluation with double reinforcement learning. Oper. Res. , 70(6):3282–3302.\nKallus, N. and Zhou, A. (2018). Policy evaluation and optimization with continuous treatments.\nInInternational conference on artificial intelligence and statistics , pages 1243–1251. PMLR.\nKaufmann, T., Weng, P., Bengs, V., and Hüllermeier, E. (2023). A survey of reinforcement\nlearning from human feedback. arXiv preprint arXiv:2312.14925 , 10.\nKennedy, E. H. (2023). Towards optimal doubly robust estimation of heterogeneous causal\neffects.Electronic Journal of Statistics , 17(2):3008–3049.\nKennedy, E. H., Ma, Z., McHugh, M. D., and Small, D. S. (2017). Non-parametric methods\nfor doubly robust estimation of continuous treatment effects. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology , 79(4):1229–1245.\nKünzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating\nheterogeneous treatment effects using machine learning. Proceedings of the national academy\nof sciences , 116(10):4156–4165.\nLaidlaw, C., Singhal, S., and Dragan, A. (2024). Correlated proxies: A new definition and\nimproved mitigation for reward hacking. arXiv preprint arXiv:2403.03185 .\nLi, H., Zheng, C., and Wu, P. (2022). Stabledr: Stabilized doubly robust learning for\nrecommendation on data missing not at random. arXiv preprint arXiv:2205.04701 .\n15\n--- Page 16 ---\nLi, X., Miao, W., Lu, F., and Zhou, X.-H. (2023a). Improving efficiency of inference in clinical\ntrials with external control data. Biometrics , 79(1):394–403.\nLi, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. (2023b). Remax: A simple,\neffective, and efficient reinforcement learning method for aligning large language models.\narXiv preprint arXiv:2310.10505 .\nLiao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S. A. (2022). Batch policy learning in\naverage reward markov decision processes. Annals of statistics , 50(6):3364.\nLindner, D. and El-Assady, M. (2022). Humans are not boltzmann distributions: Challenges\nand opportunities for modelling human feedback and interaction in reinforcement learning.\narXiv preprint arXiv:2206.13316 .\nLiu, K., Long, Q., Shi, Z., Su, W. J., and Xiao, J. (2025a). Statistical impossibility and\npossibility of aligning llms with human preferences: From condorcet paradox to nash\nequilibrium. arXiv preprint arXiv:2503.10990 .\nLiu, P., Shi, C., and Sun, W. W. (2024a). Dual active learning for reinforcement learning from\nhuman feedback. arXiv preprint arXiv:2410.02504 .\nLiu, Y., Liu, P., and Cohan, A. (2024b). Understanding reference policies in direct preference\noptimization. arXiv preprint arXiv:2407.13709 .\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. (2025b).\nUnderstanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783 .\nLoshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization.\nLuedtke, A. R. and Van Der Laan, M. J. (2016). Statistical inference for the mean outcome\nunder a possibly non-unique optimal treatment strategy. Annals of statistics , 44(2):713.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning\nword vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies , pages 142–150,\nPortland, Oregon, USA. Association for Computational Linguistics.\nMay, K. O. (1954). Intransitivity, utility, and the aggregation of preference patterns. Econo-\nmetrica, 22:1.\nMichaud, E. J., Gleave, A., and Russell, S. (2020). Understanding learned reward functions.\narXiv preprint arXiv:2012.05862 .\nMilano, S., Taddeo, M., and Floridi, L. (2021). Ethical aspects of multi-stakeholder recom-\nmendation systems. The information society , 37(1):35–45.\nMunos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y.,\nGeist, M., Mesnard, T., Michi, A., et al. (2023). Nash learning from human feedback. arXiv\npreprint arXiv:2312.00886 , 18.\nNewey, W. K. (1990). Semiparametric efficiency bounds. Journal of applied econometrics ,\n5(2):99–135.\nNie, X. and Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects.\nBiometrika , 108(2):299–319.\nOprescu, M., Syrgkanis, V., and Wu, Z. S. (2019). Orthogonal random forest for causal\ninference. In International Conference on Machine Learning , pages 4932–4941. PMLR.\n16\n--- Page 17 ---\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with\nhuman feedback. Advances in neural information processing systems , 35:27730–27744.\nQuinzan, F., Soleymani, A., Jaillet, P., Rojas, C. R., and Bauer, S. (2023). Drcfs: Doubly\nrobust causal feature selection. In International Conference on Machine Learning , pages\n28468–28491. PMLR.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. (2023). Direct\npreference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems , 36:53728–53741.\nRobins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In\nProceedings of the Second Seattle Symposium in Biostatistics: analysis of correlated data ,\npages 189–326. Springer.\nRobins, J. M., Li, L., Mukherjee, R., Tchetgen, E. T., and van der Vaart, A. (2017). Minimax\nestimation of a functional on a structured high-dimensional model. THE ANNALS of\nSTATISTICS , pages 1951–1987.\nRobins, J. M., Rotnitzky, A., and Zhao, L. P. (1994). Estimation of regression coefficients when\nsome regressors are not always observed. Journal of the American Statistical Association ,\n89(427):846–866.\nRosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. (2024). Direct\nnash optimization: Teaching language models to self-improve with general preferences. arXiv\npreprint arXiv:2404.03715 .\nScharfstein, D. O., Rotnitzky, A., and Robins, J. M. (1999). Adjusting for nonignorable\ndrop-out using semiparametric nonresponse models. Journal of the American Statistical\nAssociation , 94(448):1096–1120.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 .\nSchulte, P. J., Tsiatis, A. A., Laber, E. B., and Davidian, M. (2015). Q-and a-learning methods\nfor estimating optimal dynamic treatment regimes. Statistical science: a review journal of\nthe Institute of Mathematical Statistics , 29(4):640.\nShah, R. D. and Peters, J. (2020). The hardness of conditional independence testing and the\ngeneralised covariance measure. The Annals of Statistics , 48(3):1514–1538.\nShalev-Shwartz, S. and Ben-David, S. (2014). Understanding machine learning: From theory\nto algorithms . Cambridge university press.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y.,\net al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language\nmodels.arXiv preprint arXiv:2402.03300 .\nShi, C., Blei, D., and Veitch, V. (2019). Adapting neural networks for the estimation of\ntreatment effects. Advances in neural information processing systems , 32.\nShi, C., Fan, A., Song, R., and Lu, W. (2018). High-dimensional a-learning for optimal\ndynamic treatment regimes. Annals of statistics , 46(3):925.\nShi, C., Luo, S., Le, Y., Zhu, H., and Song, R. (2024a). Statistically efficient advantage\nlearning for offline reinforcement learning in infinite horizons. Journal of the American\nStatistical Association , 119(545):232–245.\n17\n--- Page 18 ---\nShi, C., Uehara, M., Huang, J., and Jiang, N. (2022). A minimax learning approach to off-policy\nevaluation in confounded partially observable markov decision processes. In International\nConference on Machine Learning , pages 20057–20094. PMLR.\nShi, C., Xu, T., Bergsma, W., and Li, L. (2021). Double generative adversarial networks for\nconditional independence testing. Journal of Machine Learning Research , 22(285):1–32.\nShi, C., Zhu, J., Shen, Y., Luo, S., Zhu, H., and Song, R. (2024b). Off-policy confidence\ninterval estimation with confounded markov decision process. Journal of the American\nStatistical Association , 119(545):273–284.\nSkalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. (2022). Defining and characterizing\nreward gaming. Advances in Neural Information Processing Systems , 35:9460–9471.\nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H. (2024). Preference ranking\noptimization for human alignment. In Proceedings of the AAAI Conference on Artificial\nIntelligence , volume 38, pages 18990–18998.\nSong, R., Luo, S., Zeng, D., Zhang, H. H., Lu, W., and Li, Z. (2017). Semiparametric\nsingle-index model for estimating optimal individualized treatment strategy. Electronic\njournal of statistics , 11(1):364.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei,\nD., and Christiano, P. (2020). Learning to summarize with human feedback. In Advances in\nNeural Information Processing Systems (NeurIPS) , volume 33, pages 3008–3021.\nSu, Y., Dimakopoulou, M., Krishnamurthy, A., and Dudík, M. (2020). Doubly robust off-\npolicy evaluation with shrinkage. In International Conference on Machine Learning , pages\n9167–9176. PMLR.\nSutton, R. S., Barto, A. G., et al. (2018). Reinforcement learning: An introduction . MIT press\nCambridge.\nSwamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. (2024). A minimaximalist\napproach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 .\nTan, Z. (2010). Bounded, efficient and doubly robust estimation with inverse weighting.\nBiometrika , 97(3):661–682.\nTang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H.,\nValko, M., Pires, B. Á., and Piot, B. (2024). Generalized preference optimization: A unified\napproach to offline alignment. arXiv preprint arXiv:2402.05749 .\nTang, Z., Feng, Y., Li, L., Zhou, D., and Liu, Q. (2020). Doubly robust bias reduction in infinite\nhorizon off-policy estimation. In International Conference on Learning Representations .\nTeam, Q. (2024). Qwen2.5: A party of foundation models.\nThomas, P. S. and Brunskill, E. (2016). Data-efficient off-policy policy evaluation for rein-\nforcement learning. In International Conference on Machine Learning , pages 2139–2148.\nPMLR.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,\nGoyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 .\nTsiatis, A. A. (2006). Semiparametric Theory and Missing Data . Springer.\n18\n--- Page 19 ---\nTsiatis, A. A., Davidian, M., and Cao, W. (2011). Improved doubly robust estimation when\ndata are monotonely coarsened, with application to longitudinal studies with dropout.\nBiometrics , 67(2):536–545.\nTversky, A. (1969). Intransitivity of preferences. Psychological Review , 76:31–48.\nUehara, M., Huang, J., and Jiang, N. (2020a). Minimax weight and q-function learning\nfor off-policy evaluation. In Proceedings of the 37th International Conference on Machine\nLearning , volume 119 of Proceedings of Machine Learning Research , pages 9659–9668. PMLR.\nUehara, M., Kato, M., and Yasui, S. (2020b). Off-policy evaluation and learning for external\nvalidity under a covariate shift. Advances in Neural Information Processing Systems , 33:49–\n61.\nUehara, M., Shi, C., and Kallus, N. (2022). A review of off-policy evaluation in reinforcement\nlearning. arXiv preprint arXiv:2212.06355 .\nVan Der Vaart, A. W., Wellner, J. A., van der Vaart, A. W., and Wellner, J. A. (1996). Weak\nconvergence . Springer.\nVermeulen, K. and Vansteelandt, S. (2015). Bias-reduced doubly robust estimation. Journal\nof the American Statistical Association , 110(511):1024–1036.\nVölske, M., Peyrard, M., Bevendorff, J., Potthast, M., and Stein, B. (2017). TL;DR: Mining\nreddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers\nin Summarization (EMNLP) , pages 59–63. Association for Computational Linguistics.\nvon Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S.,\nRasul, K., and Gallouédec, Q. (2020). Trl: Transformer reinforcement learning. https:\n//github.com/huggingface/trl .\nWager, S. and Athey, S. (2018). Estimation and inference of heterogeneous treatment effects\nusing random forests. Journal of the American Statistical Association , 113(523):1228–1242.\nWang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y. (2023a). Beyond reverse kl: Gener-\nalizing direct preference optimization with diverse divergence constraints. arXiv preprint\narXiv:2309.16240 .\nWang, J., Wang, H., Sun, S., and Li, W. (2023b). Aligning language models with human\npreferences via a bayesian approach. Advances in Neural Information Processing Systems ,\n36:49113–49132.\nWang, L. and Han, P. (2024). Multiply robust estimation for average treatment effect among\ntreated. Statistical Theory and Related Fields , 8(1):29–39.\nWang, L. and Tchetgen Tchetgen, E. (2018). Bounded, efficient and multiply robust estimation\nof average treatment effects using instrumental variables. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology , 80(3):531–550.\nWang, Y. and Shah, R. D. (2024). Debiased inverse propensity score weighting for estimation\nof average treatment effects with high-dimensional confounders. The Annals of Statistics ,\n52(5):1978–2003.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T.,\nLouf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y.,\nPlu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020).\nTransformers: State-of-the-art natural language processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing: System Demonstrations ,\npages 38–45, Online. Association for Computational Linguistics.\n19\n--- Page 20 ---\nWu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., and Jiao, J. (2024a). Pairwise\nproximal policy optimization: Language model alignment with comparative rl. In First\nConference on Language Modeling .\nWu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., and Gu, Q. (2024b). Self-play preference\noptimization for language model alignment. arXiv preprint arXiv:2405.00675 .\nXiao, J., Li, Z., Xie, X., Getzen, E., Fang, C., Long, Q., and Su, W. J. (2024). On the\nalgorithmic bias of aligning large language models with rlhf: Preference collapse and\nmatching regularization. arXiv preprint arXiv:2405.16455 .\nXiao, T., Yuan, Y., Li, M., Chen, Z., and Honavar, V. G. (2025). On a connection between\nimitation learning and rlhf. arXiv preprint arXiv:2503.05079 .\nXie, C., Yang, W., and Zhang, Z. (2023). Semiparametrically efficient off-policy evaluation in\nlinear markov decision processes. In International Conference on Machine Learning , pages\n38227–38257. PMLR.\nXu, W., Li, J., Wang, W. Y., and Li, L. (2024). Bpo: Staying close to the behavior llm creates\nbetter online llm alignment. arXiv preprint arXiv:2406.12168 .\nXu, Y., Zhu, J., Shi, C., Luo, S., and Song, R. (2023). An instrumental variable approach to\nconfounded off-policy evaluation. In International Conference on Machine Learning , pages\n38848–38880. PMLR.\nYe, C., Xiong, W., Zhang, Y., Dong, H., Jiang, N., and Zhang, T. (2024). Online iterative\nreinforcement learning from human feedback with general preference model. Advances in\nNeural Information Processing Systems , 37:81773–81807.\nYe, K., Zhou, H., Zhu, J., Quinzan, F., and Shi, C. (2025). Robust reinforcement learning from\nhuman feedback for large language models fine-tuning. arXiv preprint arXiv:2504.03784 .\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al.\n(2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint\narXiv:2503.14476 .\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X.,\net al. (2025). Vapo: Efficient and reliable reinforcement learning for advanced reasoning\ntasks.arXiv preprint arXiv:2504.05118 .\nZhang, B., Tsiatis, A. A., Laber, E. B., and Davidian, M. (2012). A robust method for\nestimating optimal treatment regimes. Biometrics , 68(4):1010–1018.\nZhang, B., Tsiatis, A. A., Laber, E. B., and Davidian, M. (2013). Robust estimation of optimal\ndynamic treatment regimes for sequential treatment decisions. Biometrika , 100(3):681–694.\nZhang, H., Lei, Y., Gui, L., Yang, M., He, Y., Wang, H., and Xu, R. (2024a). Cppo: Continual\nlearning for reinforcement learning with human feedback. In The Twelfth International\nConference on Learning Representations .\nZhang, H., Zhou, S., Guan, J., and Huan, J. (2019). Measuring conditional independence by\nindependent residuals for causal discovery. ACM Transactions on Intelligent Systems and\nTechnology (TIST) , 10(5):1–19.\nZhang, Y., Huang, L., Yang, Y., and Shao, X. (2024b). Doubly robust conditional independence\ntesting with generative neural networks. arXiv preprint arXiv:2407.17694 .\nZhang, Y., Yu, D., Peng, B., Song, L., Tian, Y., Huo, M., Jiang, N., Mi, H., and Yu, D. (2024c).\nIterative nash policy optimization: Aligning llms with general preferences via no-regret\nlearning. arXiv preprint arXiv:2407.00617 .\n20\n--- Page 21 ---\nZhang, Y., Zhang, G., Wu, Y., Xu, K., and Gu, Q. (2024d). General preference modeling with\npreference representations for aligning language models. arXiv preprint arXiv:2410.02197 .\nZhao, Y., Khalman, M., Joshi, R., Narayan, S., Saleh, M., and Liu, P. J. (2022). Cal-\nibrating sequence likelihood improves conditional language generation. arXiv preprint\narXiv:2210.00045 .\nZheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou,\nY., et al. (2023). Secrets of rlhf in large language models part i: Ppo. arXiv preprint\narXiv:2307.04964 .\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P.,\nand Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint\narXiv:1909.08593 .\n21\n--- Page 22 ---\nAppendix\nA Technical Proof\nIn this section, we present the regularity conditions and proofs for all the lemmas and theorems.\nBy nature, the vocabulary size is finite; as such, all random variables – including the prompts\nXand the responses Y– are discrete. We assume that ϵin the coverage assumption is a\nbounded constant , which is why it does not explicitly appear in the error bound. However, in\nthe proof of Theorems, for completeness, we will explicitly highlight how the leading terms of\nthe error bounds depend on ϵ.\nA.1 Proof of Lemma 4.1\nBy direct calculation, it follows that\nEn\nw(Y(1), X)Zo\n=E(\nE\"\nπ(Y(1)|X)\nπref(Y(1)|X)I{Y(1)≻Y(2)}\f\f\f\f\fX, Y(1), Y(2)#)\n=E(\nπ(Y(1)|X)\nπref(Y(1)|X)g∗\u0010\nY(1), Y(2), X\u0011)\n=E(X\nyπ(y|X)g∗\u0010\ny, Y(2), X\u0011)\n=En\nEy∼π(•|X)g∗\u0010\ny, Y(2), X\u0011o\n,\nwhere the first equality is derived by the law of total expectation, the second equality follows\nfrom the definition of the preference function g∗, and the third equality follows from the\nchange-of-measure theorem (e.g., Radon–Nikodym theorem).\nFollowing a similar argument and using the fact that 1−Z=I(Y(2)≻Y(1)), we obtain\nEn\nw(Y(2), X)(1−Z)o\n=En\nEy∼π(•|X)g∗\u0010\ny, Y(1), X\u0011o\n.\nConsequently, p∗(π) =1\n2E[w(Y(1), X)Z+w(Y(2), X)(1−Z)], which finishes the proof of the\nlemma.\nA.2 Auxiliary lemma for proving Theorem 5.5\nBefore proceeding to the proof of Theorem 5.5, we first introduce an auxiliary lemma.\nLemmaA.1. Under Assumption 5.1, with nindependent data tuple Wi= (Xi, Y(1)\ni, Y(2)\ni, Zi), i=\n1, . . . n, the efficient influence function [see e.g., Tsiatis, 2006, for the detailed definition] for\np∗(π)is given by1\nnPn\ni=1ψ(Xi, Y(1)\ni, Y(2)\ni, Zi;π, πref, g∗)−p∗(π), with ψdefined in equation\n(8).\nProof of Lemma A.1. To simplify notation, we denote ψ(W) =ψ(X, Y(1), Y(2), Z;π, πref, g∗).\nLetMdenote the model that generates these data triplets, which are i.i.d. copies of W=\n(Z, Y(1), Y(2), X). This model involves three types of parameters: (i) those to model the\nprobability mass function fX(•)of the prompt X(denoted by γ); (ii) those to model the\nreference policy which generates response Y(1), Y(2)independently conditional on the prompt\nX(denoted by b) and (iii) those to model the preference probability g∗which characterize\nthe probability of Y(1)is preferred than Y(2)given X(denoted by η). Then the likelihood\nfunction for a data tuple Wis given by\nl(W;γ, b, η ) =fγ(X)πb(Y(1)|X)πb(Y(2)|X)gη(Y(1), Y(2), X)Z(1−gη(Y(1), Y(2), X))1−Z.(16)\n22\n--- Page 23 ---\nAdditionally, let (γ0, b0, η0)denotethetrueparametersinthemodelsothat fγ0=fX, πb0=πref\nandgη0=g∗.\nThe proof follows from standard techniques in semi-parametric statistic; see e.g., Chapters 2\n& 3 in Bickel et al. [1998] and Theorem 3.5 in Tsiatis [2006]. See also the proof of Theorem 1\nin Kallus and Uehara [2020a]. Specifically:\n1.For any given policy π, we first prove that E[{ψ(W)−p∗(π)}∇logl(W;γ0, b0, η0)]is a valid\nderivative of p∗(π)with respect to the parameters (γ0, b0, η0), where ∇denotes the gradient\noperator.\n2.We next prove that ψ(W)−p∗(π)lies in the tangent space of the data generating process\nmodel M(denoted by TM), that is, ψ(W)−p∗(π)∈ TM.\nStep 1: E[{ψ(W)−p∗(π)}∇logl(W;γ0, b0, η0)]is a valid derivative of p∗(π)with respect\nto(γ0, b0, η0).\nNoted that the log-likelihood has zero mean. Therefore, in order to prove step 1, we only need\nto verify the following three equations hold.\n(i)En\nψ(W)∂\n∂γlogl(W;γ0, b0, η0)o\n=∂\n∂γp∗(π)|γ=γ0,\n(ii)E\b\nψ(W)∂\n∂blogl(W;γ0, b0, η0)\t\n=∂\n∂bp∗(π)|b=b0,\n(iii)En\nψ(W)∂\n∂ηlogl(W;γ0, b0, η0)o\n=∂\n∂ηp∗(π)η=η0.\nBy definition, p∗(π)can be represented as\np∗(π) = E[Ey1∼πθ,y2∼πrefP(y1≻y2|X)]\n=X\nx,y1,y2g∗(y1, y2, x)π(y1|x)πref(y2|x)fX(x).\nLetw= (x, y1, y2, z)denote the realization of W= (X, Y(1), Y(2), Z). It follows from equation\n(16) that\nlogl(w;γ, b, η ) = log fγ(x) + log πb(y1|x) + log πb(y2|x)\n+zloggη(y1, y2, x) + (1 −z) log(1 −gη(y1, y2, x)).(17)\nWith some calculations, we obtain\n∂\n∂γlogl(w;γ0, b0, η0) =1\nfX(x)∂\n∂γfγ(x)\f\f\f\nγ=γ0,\n∂\n∂blogl(w;γ0, b0, η0) =1\nπref(y1|x)∂\n∂bπb(y1|x)\f\f\f\nb=b0+1\nπref(y2|x)∂\n∂bπb(y2|x)\f\f\f\nb=b0,\n∂\n∂ηlogl(w;γ0, b0, η0) =\u0012z\ng∗(y1, y2, x)−1−z\n1−g∗(y1, y2, x)\u0013∂\n∂ηgη(y1, y2, x)\f\f\f\nη=η0.\nIn the following proof, we omit |γ=γ0,|b=b0and|η=η0to ease notation.\nFor equation (i) : LetBer(p)denote the Bernoulli distribution with success probability p.\nThe left-hand-side (LHS) of equation (i) can be represented by\nE\u001a\nψ(W)∂\n∂γlogl(W;γ0, b0, η0)\u001b\n=1\n2X\nx,y1,y2Ez∼Ber(g∗(y1,y2,x))\u001a\u0012π(y1|x)\nπref(y1|x)−π(y2|x)\nπref(y2|x)\u0013\n(z−g∗(y1, y2, x))\n×πref(y1|x)πref(y2|x)∂\n∂γfγ(x)\u001b\n+1\n2X\nx,y1,y2,y∗(g∗(y∗, y1, x) +g∗(y∗, y2, x))π(y∗|x)πref(y1|x)πref(y2|x)∂\n∂γfγ(x)\n23\n--- Page 24 ---\nUsing the fact that Ez∼Ber(g∗(y1,y2,x)){z−g∗(y1, y2, x)}= 0, the first term on the right-hand-\nside (RHS) of the above equation vanishes. Therefore,\nE\u001a\nψ(W)∂\n∂γlogl(W;γ0, b0, η0)\u001b\n=1\n2X\nx,y1,y∗g∗(y∗, y1, x)π(y∗|x)πref(y1|x)∂\n∂γfγ0(x)\n+1\n2X\nx,y2,y∗g∗(y∗, y2, x)π(y∗|x)πref(y2|x)∂\n∂γfγ0(x)\n=X\nx,y,y∗g∗(y∗, y, x)π(y∗|x)πref(y|x)∂\n∂γfγ0(x)\n=∂\n∂γp∗(π).\nFor equation (ii) : Notice that the LHS of equation (ii) can be represented as\nE\u001a\nψ(W)∂\n∂blogl(W;γ0, b0, η0)\u001b\n=1\n2X\nx,y1,y2Ez∼Ber(g∗(y1,y2,x))(\u0012π(y1|x)\nπref(y1|x)−π(y2|x)\nπref(y2|x)\u0013\u00121\nπref(y1|x)∂\n∂bπb(y1|x) +\n1\nπref(y2|x)∂\n∂bπb(y2|x)\u0013\n×(z−g∗(y1, y2, x))πref(y1|x)πref(y2|x)fX(x))\n+1\n2X\nx,y1,y2,y∗(g∗(y∗, y1, x) +g∗(y∗, y2, x))π(y∗|x)∂\n∂b[πb0(y1|x)πb0(y2|x)]fX(x).\nFollows a similar argument in proving equation (i), the first term on the RHS equals zero. The\nsecond term can be further represented by\n1\n2∂\n∂bX\nx,y1,y2,y∗(g∗(y∗, y1, x) +g∗(y∗, y2, x))π(y∗|x)πb0(y1|x)πb0(y2|x)fX(x)\n=1\n2∂\n∂bX\nx,y1,y∗g∗(y∗, y1, x)π(y∗|x)πb0(y1|x)fX(x)\n+1\n2∂\n∂bX\nx,y∗,y2g∗(y∗, y2, x)π(y∗|x)πb0(y2|x)fX(x)\n=X\nx,y,y∗g∗(y∗, y, x)π(y∗|x)∂\n∂bπb0(y|x)fX(x)\n=∂\n∂bp∗(π).\nThis finishes the proof of equation (ii).\n24\n--- Page 25 ---\nFor equation (iii) : Its LHS can be represented as\nE\u001a\nψ(w)∂\n∂ηlogl(w;γ0, b0, η0)\u001b\n=1\n2X\nx,y1,y2Ez∼Ber(g∗(y1,y2,x))(\u0012π(y1|x)\nπref(y1|x)−π(y2|x)\nπref(y2|x)\u0013\n(z−g∗(y1, y2, x))\n×πref(y1|x)πref(y2|x)\u0012z\ng∗(y1, y2, x)−1−z\n1−g∗(y1, y2, x)\u0013∂\n∂ηgη(y1, y2, x)fX(x))\n+1\n2X\nx,y1,y2,y∗Ez∼Ber(g∗(y1,y2,x))(\n(g∗(y∗, y1, x) +g∗(y∗, y2, x))π(y∗|x)πref(y1|x)\n×πref(y2|x)fX(x)\u0012z\ng∗(y1, y2, x)−1−z\n1−g∗(y1, y2, x)\u0013∂\n∂ηgη(y1, y2, x).\nThe second term is equal to zero due to the fact that\nEz∼Ber(g∗(y1,y2,x))\u001az\ng∗(y1, y2, x)−1−z\n1−g∗(y1, y2, x)\u001b\n= 0.\nOn the other hand, since\nEz∼Ber(g∗(y1,y2,x))\u001a\n(z−g∗(y1, y2, x))\u0012z\ng∗(y1, y2, x)−1−z\n1−g∗(y1, y2, x)\u0013\u001b\n=g∗(y1, y2, x)×(1−g∗(y1, y2, x))1\ng∗(y1, y2, x)\n+(1−g∗(y1, y2, x))×(−g∗(y1, y2, x))−1\n1−g∗(y1, y2, x)\n= 1 ,\nthe LHS in equation (iii) can be further represented by\n1\n2X\nx,y1,y2\u0012π(y1|x)\nπref(y1|x)−π(y2|x)\nπref(y2|x)\u0013\nπref(y1|x)πref(y2|x)∂\n∂ηgη(y1, y2, x)fX(x)\n=1\n2X\nx,y1,y2(π(y1|x)πref(y2|x)−π(y2|x)πref(y1|x))∂\n∂ηgη(y1, y2, x)fX(x)\n=X\nx,y1,y2π(y1|x)πref(y2|x)∂\n∂ηgη(y1, y2, x)fX(x)\n=∂\n∂ηp∗(π) (18)\nwhere the second-to-last equality follows from the fact∂\n∂ηgη(y1, y2, x) =−∂\n∂ηgη(y2, y1, x). This\nfinishes the proof of equation (iii).\nThus, with equation (i) - (iii) verified, Step 1 is proven.\nStep 2: ψ(W)−p∗(π)lies in the tangent space TM.\nBy definition, the tangent space TMis the linear closure of the set of score functions of the all\none-dimensional submodels regarding Mthat pass through true parameter; see Definition 2 in\nKallus and Uehara [2020a]. Based on the likelihood function in equation (17), we can explicitly\ncalculate the tangent space of the data generating process model M. In fact, the tangent\nspaceTMis a product space, which can be represented as TfLTπLTg, with Tf,Tπ,Tgbeing\nthe sets of score functions of all one-dimensional submodels passing through the marginal\n25\n--- Page 26 ---\ndistribution fX(x), conditional distribution πrefand preference probability g∗. Take the\ncalculation of Tfas an example. Consider a one-dimensional submodel {fε(x)}, defined as\nfε(x) =fX(x)(1 + εq(x)),\nwhere q(x)satisfiesP\nxf(x)q2(x)<∞. Since we require fεto be a valid probability mass\nfunction, it must satisfyP\nxfε(x) = 1, which indicates Eq(X) = 0. Then the score function\nwith respect to εis given by\nd\ndεlogfε(x) =q(x).\nTherefore, the tangent space for the marginal distribution function f(x)can be represented as\nTf=(\nq(x) :E[q(X)] = 0 ,X\nxf(x)q2(x)<∞)\n.\nFollowing similar arguments, we can obtain\nTπ=(\nq(y1, x) +q(y2, x) :Ey∼πref[q(y, x)|X=x] = 0,X\nyπref(y|x)q2(y, x)<∞)\n,\nTg=(\nz−g∗(y1, y2, x)\ng∗(1−g∗)q(y1, y2, x) :X\nx,y1,y2q2(x, y1, y2)f(x)πref(y1|x)πref(y2|x)<∞)\n.\nTo verify ψ(W)−p∗(π)lies in the tangent space, consider the following three functions:\nψ1(w) :=\u0012π(y1|x)\nπref(y1|x)−π(y2|x)\nπref(y2|x)\u0013\n(z−g∗(y1, y2, x))πref(y1|x)πref(y2|x)fX(x)\n=z−g∗(y1, y2, x)\ng∗(1−g∗)g∗(1−g∗) (π(y1|x)πref(y2|x)−π(y2|x)πref(y1|x))fX(x),\nψ2(y1, y2, x) := Ey∗∼π{g(y∗, y1, x) +g(y∗, y2, x)} −2Ey∼πref(•|x)\ny∗∼π(•|x){g(y∗, y, x)},\nψ3(x) := 2 Ey∼πref(•|x)\ny∗∼π(•|x){g(y∗, y, x)} −2p∗(π).\nIt is easy to verify that ψ1(W)∈ Tg,ψ2(Y(1), Y(2), X)∈ Tπandψ3(X)∈ Tf. Therefore,\nψ(W)−p∗(π) =1\n2\u0010\nψ1(W) +ψ2(Y(1), Y(2), X) +ψ3(X)\u0011\n∈ TM.\nThis finishes the proof of Step 2.\nWith Step 1 and Step 2 verified, together with the fact that Eψ(W) =p∗(π), we obtain that\nψ(W)is an efficient influence function.\nA.3 Proof of Theorem 5.5\nLetEndenote the empirical average over the ntuples (X, Y(1), Y(2), Z)in the dataset D.\nAccordingly, our estimator for p∗(π)can be represented by En[ψ(w;π,bπref,bg)].\nWe further define the following norms:\n∥bg−g∗∥=\u0012\nEh\nbg(Y(1), Y(2), X)−g∗(Y(1), Y(2), X)i2\u00131/2\n∥bπref\nπref−1∥=\nE\"\nbπref(Y(1)|X)\nπref(Y(1)|X)−1#2\n1/2\n.\n26\n--- Page 27 ---\nIn the proof of this theorem, we assume these norms are bounded. Such a boundedness\nassumption is automatically satisfied for ∥bg−g∗∥, since both g∗andbgare probabilities. These\nassumptions are to simplify our finite-sample error bound by omitting some higher-order\nremainder terms, which can be more heavily dependent on the aforementioned norms.\nWith some calculations, we can show that\nEnψ(w;π,bπref,bg) =Enψ(w;π, πref, g∗) + I + II + III ,\nwhere\nI =1\n2En(2X\na=1(−1)a(Z−g∗(X, Y(1), Y(2)))\"\nπ(Y(a)|X)\nbπref(Y(a)|X)−π(Y(a)|X)\nπref(Y(a)|X)#)\n,\nII =1\n2En(2X\na=1Ey∼π(•|x)h\nbg(X, y, Y(a))−g∗(X, y, Y(a))i)\n+1\n2En(2X\na=1(−1)aπ(Y(a)|X)\nπref(Y(a)|X)[bg(X, Y(1), Y(2))−g∗(X, Y(1), Y(2))])\n,\nIII =1\n2En(2X\na=1(−1)a[bg(X, Y(1), Y(2))−g∗(X, Y(1), Y(2))]\"\nπ(Y(a)|X)\nbπref(Y(a)|X)−π(Y(a)|X)\nπref(Y(a)|X)#)\n.\nFrom Lemma A.1, we know that Enψ(w;π, πref, g∗)is an unbiased estimator for p∗(π)with\nvariance equal to SEB. Since both bπrefandbgare obtained from external models independent\nofD, analogous to the proof of Lemma 4.1, we know that the first term Iand the second term\nIIhave zero means. The third term IIIis the bias term. Therefore, we obtain the following\nbias-variance decomposition for MSE (bpDR):\nMSE (bpDR(π)) =Var(Enψ(w;π, πref, g∗) + I + II + III) + ( E[III])2(19)\nSince g∗is bounded by 1, under the coverage assumption (Assumption 5.1), we obtain\nthat\nVar(Enψ(w;π, πref, g∗) =1\nnVar(ψ(w;π, πref, g∗)) =O\u00101\nnEπ2(Y|X)\nπ2\nref(Y|X)\u0011\n=O\u00101\nnX\nyEπ2(y|X)\nπref(y|X)\u0011\n=O\u00121\nnϵ\u0013\n.(20)\nSimilarly, we have\nEI2≤1\nnE(\u0014π(Y|X)\nbπref(Y|X)−π(Y|X)\nπref(Y|X)\u00152)\n≤1\nnE(\nπ2(Y|X)\nbπ2\nref(Y|X)\u0014bπref(Y|X)\nπref(Y|X)−1\u00152)\n=O \n1\nnϵ2\r\r\r\rbπref\nπref−1\r\r\r\r2!\n,(21)\nand\nEII2=O\u00121\nnϵ2∥bg−g∗∥2\u0013\n, EIII2=O \n1\nnϵ2\r\r\r\rbπref\nπref−1\r\r\r\r2!\n. (22)\n27\n--- Page 28 ---\nBy Cauchy-Schwarz inequality, we have for any random variables UandVthat|Cov(U, V)| ≤p\nVar(U)Var(V). It follows that\nCov(Enψ(w;π, πref, g∗),I + III) = O\u00121\nnϵ3/2\r\r\r\rbπref\nπref−1\r\r\r\r\u0013\n,\nCov(Enψ(w;π, πref, g∗),II) = O\u00121\nnϵ3/2∥bg−g∗∥\u0013\n,\nCov(I + III ,II) = O\u00121\nnϵ2∥bg−g∗∥ ·\r\r\r\rbπref\nπref−1\r\r\r\r\u0013\n.(23)\nSince ϵis a constant, the high-order terms Var(I),Var(I)andVar(III)are dominated by the\nfirst two terms in (23). Combining equations (20), (21),(22) with (23) yields\nVar(Enψ(w;π, πref, g∗) + I + II + III) = SEB +O\u00121\nnϵ3/2∥bg−g∗∥\u0013\n+O\u00121\nnϵ3/2\r\r\r\rbπref\nπref−1\r\r\r\r\u0013\n.(24)\nFinally, using Cauchy-Schwarz inequality again, we obtain that\nE\f\fIII\f\f=O\nEn\n(bg−g∗)2(X, Y(1), Y(2))o1/2\nE(\u0014πref(Y|X)\nbπ2\nref(Y|X)−1\u00152π2(Y|X)\nπ2\nref(Y|X))1/2\n\n=O\u00121\nϵ∥bg−g∗∥ · ∥bπref/πref−1∥\u0013\n.\nCombining (19) with (25), we obtain that\nMSE (bpDR(π)) = E{Enψ(w;π,bπref,bg)−p∗(π)}2\n=SEB +O\u00121\nnϵ3/2∥bg−g∗∥\u0013\n+O\u00121\nnϵ3/2∥bπref/πref−1∥\u0013\n+O\u00121\nϵ2∥bπref/πref−1∥2· ∥bg−g∗∥2\u0013\n.\nThis finishes the proof of Theorem 5.5.\nA.4 Proofs of Corollaries 5.6 and 5.7\nThe proofs of Corollaries 5.6 and 5.7 follow directly from the assertion of Theorem 5.5.\nA.5 Proof of Theorem 5.8\nLetπ∗denote the maximizer of p∗(π)in the policy class Π. Throughout the proof, for\nany policies π1andπ2, we use a shorthand and write EX∼DDKL[π1(• |X)∥π2(• |X)]as\nKL(π1∥π2). Since bπis a maximizer of bpDR(π)−βKL(π∥bπref), we have\nbpDR(bπ)−βKL(bπ∥bπref)≥bpDR(π∗)−βKL(π∗∥bπref).\nIt directly follows that\np∗(π∗)−p∗(bπ)\n≤p∗(π∗)−bpDR(π∗) +bpDR(bπ)−p∗(bπ) +β(KL(π∗∥bπref)−KL(bπ∥bπref))\n≤E|p∗(π∗)−bpDR(π∗)|+E|bpDR(bπ)−p∗(bπ)|+O\u0000\nβlog−1ϵ\u0001\n≤2Esup\nπ∈Π|p∗(π)−bpDR(π)|+O\u0000\nβlog−1ϵ\u0001\n, (25)\nwhere the second inequality follows from the coverage assumption, which entails that\nKL(π∥bπref) =EX∼DEy∼π(•|X)logπ(y|X)\nbπref(y|X)=O(log−1ϵ).\n28\n--- Page 29 ---\nAdditionally, following the proof of Theorem 5.5, the bias of the proposed preference evaluation\nestimator can be upper bounded by\nsup\nπ∈Π|E[p∗(π)−bpDR(π)]|=E|Enψ(w;π, πref, g∗)−p∗(π)|+O\u00121\nϵ∥bg−g∗∥ · ∥bπref\nπref−1∥\u0013\n.(26)\nIt remains to upper bound the empirical process term Esupπ∈Π|bpDR(π)−EbpDR(π)|. Toward\nthat end, we employ Corollary 5.1 in Chernozhukov et al. [2014]. To invoke this corollary,\nnotice that\n1.According to Assumption 5.4, Πis a policy class with VC dimension v. Under Assumption\n5.1, it follows from Lemma A.6 in Chernozhukov et al. [2014] that the function class\nF={ψ(•, π,bπref,bg)|π∈Π}also has a VC dimension of v.\n2.Using the coverage assumption again, the function class Fis uniformly bounded by O(1/ϵ).\n3. The variance supf∈FVar(f(W))is uniformly bounded by O(1/ϵ2).\nConsequently, an application of Corollary 5.1 in Chernozhukov et al. [2014] yields that\nEsup\nπ∈Π|bpDR(π)−E[bpDR(π)]|=O\u00121√nrv\nϵ2log−1ϵ2+v\nnlog−1ϵ2\u0013\n=O\n1\nϵs\nvlog−1ϵ\nn+vlog−1ϵ\nnϵ\n.\nCombining equations (25), (26) and (27), we obtain for any π∈Πthat\np∗(π∗)−p∗(bπ) =O\nβlog−1ϵ+1\nϵs\nvlog−1ϵ\nn+vlog−1ϵ\nnϵ+1\nϵ∥bg−g∗∥ · ∥bπref\nπref−1∥\n.\nThis completes the proof of Theorem 5.8.\nA.6 Proof of Corollary 5.9\nThe proof of Corollary 5.9 follows directly from the assertion of Theorem 5.8.\nA.7 Proof of Theorem 5.10\nSuboptimality gap for DRPO: If the BT assumption holds, we have g∗(y1, y2, x) =\nσ(r∗(y1, x)−r∗(y2, x))where σ(x) = 1 /(1 +e−x)is the sigmoid function. Since the sigmoid\nfunction is monotonically increasing, under the realizability assumption, π∗which maximizes\nJ(π)also maximizes p∗(π). This follows from the classical results on the maximum rank\ncorrelation estimator that has been widely studied in the econometrics literature [see e.g., ??].\nTherefore,\np∗(π∗)−p∗(bπ) = Ey∗∼π∗,ey∼bπ,y∼πref{g∗(y∗, y, x)−g∗(ey, y, x )}\n=Ey∗∼π∗,ey∼bπ,y∼πref\b\nσ′(ξ) [(r∗(y∗, x)−r∗(y, x))−(r∗(ey, x)−r∗(y, x))]\t2\n=Ey∗∼π∗,ey∼bπ\b\nσ′(ξ)(r∗(y∗, x)−r∗(ey, x))\t\n≥C0(J(π∗)−J(bπ)),\nwhere C0is some positive constant and ξis some real number between r∗(y∗, x)−r∗(y, x)\nandr∗(ey, x)−r∗(y, x). Here, the second equality follows from mean value theorem. The last\nequality follows from the identity that σ′(x) =σ(x)(1−σ(x)), which is bounded away from\nzeroo under Assumption 5.2 that the reward is bounded by some constant. Thus, we obtain\n29\n--- Page 30 ---\nJ(π∗)−J(bπ) =O(Reg(bπ))and the suboptimality gap for DRPO follows directly from the\nassertion in Theorem 5.8.\nSuboptimality gap for PPO-based algorithm: We begin with some notations. For a\ngiven estimated reward br, define\n•l(π) =E[Ey∼πbr(y, X)]−βKL(π∥πref),\n•ln(π) =EnEy∼πbr(y, X)−βKL(π∥πref),\n•eπ= arg max π∈Πl(π),\n•bπ= arg max π∈Πln(π).\nUsing the fact that l(eπ)≥l(π∗)andln(bπ)≥ln(eπ), we obtain the following upper bound:\nJ(π∗)−J(bπ)≤E{[J(π∗)−l(π∗)] + [l(eπ)−ln(eπ)] + [ln(bπ)−l(bπ)] + [l(bπ)−J(bπ)]}\n≤E{[J(π∗)−l(π∗)]}+E{[l(bπ)−J(bπ)]}+ 2Esup\nπ∈Π{|l(π)−ln(π)|}.(27)\nFor the first term, we have\nE{|J(π∗)−l(π∗)|}=Ey∼π∗|br(y, X)−r∗(y, X)|+βKL(π∗∥πref)\n=Ey∼πref\u0014π∗(y|X)\nπref(y|X)|br(y, X)−r∗(y, X)|\u0015\n+O(βlog−1ϵ)\n=O\u00121√ϵ∥br−r∗∥\u0013\n+O(βlog−1ϵ), (28)\nwhere the last equation follows from Cauchy-Schwarz inequality.\nUsingasimilarargument,weobtainthat E{|l(bπ)−J(bπ)|}=O\u0010\n1√ϵ∥br−r∗∥+βlog−1ϵ\u0011\n.\nFinally, under assumption 5.2, the function class F=nP\nybr(y, X)π(y|X)\f\fπ∈Πo\nis bounded\nby a constant. Using similar arguments to the proof of Theorem 5, we can employ Corollary\n5.1 in Chernozhukov et al. [2014] to show that\nEsup\nπ∈Π{|l(π)−ln(π)|}=O\u0012v\nn+rv\nn\u0013\n+O(βlog−1ϵ). (29)\nCombining equations (27), (28) and (29), we obtain that\nJ(π∗)−J(bπ) =O\u0012\nβlog−1ϵ+v\nn+rv\nn+1√ϵ∥br−r∗∥\u0013\n.\nSuboptimality gap for DPO-based algorithm: We need some additional technical\nconditions to prove the suboptimality gap for DPO-based algorithms. Recall that when\nBT-model holds, there exists a one-on-one correspondence between the policy and reward\nmodel [Rafailov et al., 2023]. We further assume\nAssumption A.2 (Realizability) .The oracle reward r∗lies in the bounded reward function\nclassR={βlog(π(y|x)/πref(y|x)) +βZ(x) :π∈Π}induced by the policy class Π.\nAssumption A.3 (Coverage) .Both πrefandbπrefare lower bounded by some constant ϵ >0.\nAssumption A.4 (Suboptimality gap for oracle reward) .Lety∗\nx=arg max yr∗(y|x)and\n¯yx= arg max y̸=y∗r∗(y|x). There exists a positive constant ¯csuch that for any x,\nr∗(y∗\nx, x)−r∗(¯yx, x)≥¯c.\n30\n--- Page 31 ---\nNotice that both the realizability and the coverage in Assumptions A.2 and A.3 differ from\nthose in the main text. Specifically, Assumption A.2 imposes the realizability assumption on\nthe oracle reward rather than the optimal policy whereas Assumption A.3 is stronger than that\nin the main text by requiring the denominators of the IS ratios to be strictly positive.\nWe also redefine the norm ∥bπref/πref−1∥as\nEh\nmax\u0010bπref(Y(1)|X)\nπref(Y(1)|X),πref(Y(1)|X)\nbπref(Y(1)|X)\u0001\n−1i2\n.\nNotice that this norm is no smaller than the one used in the proposed algorithm.\nWe next introduce some notations. For a given estimated reference policy bπref, any policy π\ninduce a reward function\nrπ(y, x) =βlog\u0012π(y|x)\nbπref(y|x)\u0013\n+βZ(x) (30)\nLetl(π)be the log-likelihood function induced by reward rπandl∗(π)be its variant with bπref\nin the denominator of (30)replaced by the ground truth πref. Denote eπ=arg max πEnl(π)\nandbπ= arg max πEl(π). It follows that\nEnl(eπ)−Enl(bπ)−El(eπ) +El(bπ)\n≤El(bπ)−El(eπ)\n≤El(bπ)−El∗(eπ) +El∗(eπ)−El(eπ)\n≤ − C1E∥br(y1, x)−br(y2, x)−r∗(y1, x) +r∗(y2, x)∥2\n2+β2C2E \nlogbπref(Y(1)|X)\nπref(Y(1)|X)!2\n≤ − C1σ2+β2C2∥bπref/πref−1∥2, (31)\nwhere σ2=E∥br(y1, x)−br(y2, x)−r∗(y1, x) +r∗(y2, x)∥2\n2, and both C1andC2are positive\nconstants because the Hessian matrix is bounded away from zero and infinity, which follows\nfrom the boundedness assumption on the reward. Additionally, the last inequality is due to\nthatx≤exp(x)−1for any x≥0, which entails\nE \nlogbπref(Y(1)|X)\nπref(Y(1)|X)!2\n≤E\"\nlog max\u0010bπref(Y(1)|X)\nπref(Y(1)|X),πref(Y(1)|X)\nbπref(Y(1)|X)\u0011#2\n≤Eh\nmax\u0010bπref(Y(1)|X)\nπref(Y(1)|X),πref(Y(1)|X)\nbπref(Y(1)|X)\u0001\n−1i2\nMoreover, according to Corollary 5.1 in Chernozhukov et al. [2014], using similar arguments\nto the proof of Theorem 5 and PPO-based algorithms, we have\nEnl(eπ)−Enl(bπ)−El(eπ) +El(bπ)≤2Esup\nπ∈Π|l(π)−Eln(π)|\n≤O(σrv\nn+v\nn). (32)\nThis together with equation (31)yields that C1(σ−¯cp\nv/n)2≤¯cv/n +β2C2∥bπref/πref−1∥2\nfor some constant ¯c >0, and hence\nσ=O\u0012rv\nn+β∥bπref/πref−1∥2\u0013\n. (33)\n31\n--- Page 32 ---\nRecall that π∗is the true optimal policy, and bπin this part of the proof denotes DPO’s\nestimated optimal policy. We further define bπ∗as a softmax optimal policy based on the oracle\nreward function r∗\nbπ∗(y|x) =bπ∗(y|x) exp(1\nβr∗(y, x))\nP\ny′bπ∗(y′|x) exp(1\nβr∗(y′, x)).\nWith some calculations, it follows that\nJ(π∗)−J(bπ)\n=E[Ey∼π∗r∗(y, X)−Ey∼bπr∗(y, X)]\n=E(Ey∼π∗r∗(y, X)−Ey∼bπ∗r∗(y, X)) +E(Ey∼bπ∗r∗(y, X)−Ey∼bπr∗(y, X)),(34)\nwhere the outer expectations are taken with respect to the prompt distribution.\nRecall that y∗\nxdenotes the optimal response to the prompt x. The first term E[Ey∼π∗r∗(y, X)−\nEy∼bπ∗r∗(y, X)]can be upper bounded by\nEr∗(y∗\nX, X)−E[Ey∼bπ∗r∗(y, X)] = Er∗(y∗\nX, X)−E\n\nP\nyr∗(y, X)bπref(y|X) exp\u0010\n1\nβr∗(y, X)\u0011\nP\nybπref(y|X) exp\u0010\n1\nβr∗(y, X)\u0011\n\n\n≤Er∗(y∗\nX, X)−E\n\nr∗(y∗\nX, X)bπref(y∗\nX|X) exp\u0010\n1\nβr∗(y∗\nX, X)\u0011\nP\nybπref(y|X) exp\u0010\n1\nβr∗(y, X)\u0011\n\n\n=O\u00121\nϵexp\u0012\n−¯c\nβ\u0013\u0013\n,\nwhere the last equality is due to that under Assumptions A.3 and A.4, the difference between\n1and the ratiobπref(y∗\nX|X) exp\u0010\n1\nβr∗(y∗\nX,X)\u0011\nP\nybπref(y|X) exp\u0010\n1\nβr∗(y,X)\u0011is of the order O\u0010\n1\nϵexp\u0010\n−¯c\nβ\u0011\u0011\n, almost surely.\nUsing mean value theorem, the second term can be bounded by\nEX\ny|bπ(y|X)−bπ∗(y|X)| ≤1\nβEmax\ny|br(y, X)−r∗(y, X)| ≤1\nβ√ϵ∥br−r∗∥2,(35)\nwhere the last inequality follows from the fact that\n∥br−r∗∥2=E{(br−r∗)2}1/2\n=E(X\nyπref(y|X)(br(y|X)−r∗(y|X))2)1/2\n≥√ϵE(X\ny(br(y|X)−r∗(y|X))2)1/2\n≥√ϵmax\ny|br(y, X)−r∗(y|X)|. (36)\nTo complete the proof, it remains to upper bound ∥br−r∗∥2using σ2. Recall that σ2=\nE\r\rbr(Y(1), X)−br(Y(2), X)−r∗(Y(1), X) +r∗(Y(2), X)\r\r2\n2. Since Y(2)is independent of Y(1)\ngiven Xand that πrefis lower bounded by ϵ >0, it follows that\nσ2≥ϵE\r\r\rbr(Y(1), X)−br(y0, X)−r∗(Y(1), X) +r∗(y0, X)\r\r\r2\n2,\nfor a fixed y0. Notice that the RHS corresponds to the mean squared error between brandr∗,\nup to a baseline term that is independent of Y(1). Without loss of generality, we can assume\n32\n--- Page 33 ---\nthis baseline term r∗(y0, X)−br(y0, X)this equal to zero without affecting the validity of the\nproof. This is because the true reward can be redefined as r∗(•, X)−r∗(y0, X), since it is\nequivalent up to a function independent of the response. Similarly, the estimated optimal\npolicybπ(•|x)computed by DPO can be represented using the difference br(•, x)−br(y0, x), and\nwe can replace brin(35)using this difference. Consequently, we obtain that σ2≥ϵ∥br−r∗∥2\nand hence\n∥br−r∗∥=O\u0012\nϵ−1/2rv\nn+βϵ−1/2∥bπref/πref−1∥2\u0013\n.\nCombining this together with equations (33)and(34), we obtain that the regret is upper\nbounded by\nO\u0012exp(−¯cβ−1)\nϵ+1\nβϵrv\nn+1\nϵ∥bπref/πref−1∥\u0013\n.\nThe proof is hence completed.\nBDRPO Algorithm Details and Practical Implementation\nThis section details our proposed algorithm. Notably, the reference model bπrefand the\npreference model bgare pre-trained independently prior to policy optimization. The proposed\nobjective function is defined as\nJ(πθ;bπref,bgη,D) =bpDR(π)−βEX∼D[DKL[π(· |X)∥bπref(· |X)]]. (37)\nThe gradient of J(πθ)is given by:\n∇θJ(πθ) =1\n2EX,Y(1),Y(2)∼D\"2X\na=1Ey∼πθ(·|X)h\nbg(X, y, Y(a))∇θlogπθ(y|X)i\n+2X\na=1(−1)a−1∇θπθ(Y(a)|X)\nbπref(Y(a)|X)\u0000\nZ−bg(X, Y(1), Y(2))\u0001#\n−β∇θDKL[πθ(· |X)∥bπref(· |X)] (38)\nIntuitively, the gradient operates as follows: The first term guides the policy to favor responses\npreferred by the preference model bg. When Y(1)≻Y(2),which means Z= 1, the second term\nenhances the likelihood of Y(1)while diminishing the likelihood of Y(2), and vice versa.\nThe empirical loss function is constructed such that its negative gradient corresponds to\n∇θJ(πθ)in Equation 38. The direct-method term is approximated using Monte Carlo\nsampling by drawing several new responses D∗\nX:={Y∗|Y∗∼πθ(· |X)}from the current\npolicy πθfor a given prompt X at each policy update. A k3-type empirical KL divergence is\nutilized, following [Shao et al., 2024].\nLDRPO =−1\n2EX,Y(1),Y(2)∼D(\nEY∗∼D∗\nX\"2X\na=1bg(Y∗, Y(a), X) logπθ(Y∗|X)#\n+2X\na=1(−1)a−1πθ(Y(a)|X)\nπref(Y(a)|X)\u0000\nZ−bg(X, Y(1), Y(2))\u0001)\n+βEY∗∼D∗\nX,X∼D\u0014bπref(Y∗|X)\nπθ(Y∗|X)−1−logbπref(Y∗|X)\nπθ(Y∗|X)\u0015\n(39)\nMaximization of J(πθ)is achieved by minimizing the loss function. In practice, the original\noffline dataset is augmented to DtoeDby including swapped pairs (i.e. for (X, Y(1), Y(2), Z),\nwe add (X, Y(2), Y(1),1−Z)toeD, simplifying the empirical loss function (39). Furthermore,\n33\n--- Page 34 ---\nthe importance sampling ratio is clipped, and its calculation is decoupled from the gradient\ncomputation. This is achieved by stopping auto-differentiation for the ratio and multiplying\nthe importance sampling term by logπθ, which shrinks (rather than eliminates) gradients in\nsmallbπrefregions while maintaining approximate arithmetic equivalence. Consequently, the\nloss function is reformulated as:\nLDRPO =−1\n2EX,Y(1),Y(2)∼eD\"\nEY∗∼D∗\nXh\nbg(Y∗, Y(2), X) logπθ(Y∗|X)i\n| {z }\nterm I\n+sg\u0012\nclip\u0010πθ(Y(1)|X)\nπref(Y(1)|X),1−ϵ1,1 +ϵ2\u0011\u0000\nZ−bg(Y(1), Y(2), X)\u0001\n| {z }\nterm II\u0013\nlogπθ(Y(1)|X)#\n+βEY∗∼D∗\nX,X∼eD\u0014bπref(Y∗|X)\nπθ(Y∗|X)−1−logbπref(Y∗|X)\nπθ(Y∗|X)\u0015\n(40)\nwhere sg(·)denotes stop-gradient operation, clip(·, a, b)signifies clipping values to the inter-\nval[a, b], and ϵ1, ϵ2are hyperparameters defining the clipping range. See Algorithm 1 for\ndetails.\nAlgorithm 1 Double Robust Preference Optimization\nRequire: reference policy bπref, preference model bg, offline dataset eD={Xi, Y(1)\ni, Y(2)\ni, Zi},\nclipping range [ϵ1, ϵ2], regularization parameter β, and other hyperparameters, effective\nbatch size |B|, learning rate αand the optimizer, number of Monte Carlo samples |D∗|.\nEnsure: trained policy πθ\n1:Initialize policy π(0)\nθ, total train steps T=|eD|\n|B|. For brevity let number of training epochs\nN= 1.\n2:fort= 1, . . . , Tdo\n3:foriinBt:={(t−1)|B|, . . . , t|B|}do\n4:Sample D∗\nXi={Y∗\nj|Y∗\nj∼π(t−1)\nθ(· |Xi)}j∈[|D∗|].\n5:Esitmate term I: bIi=1\n|D∗\nXi|P\nY∗∈D∗\nXibg(Y∗, Y(2)\ni, Xi) logπ(t−1)\nθ(Y∗|Xi);\n6:Estimate term II:\nbIIi= clip \nπ(t−1)\nθ(Y(1)\ni|Xi)\nπref(Y(1)\ni|Xi),1−ϵ1,1 +ϵ2!\n\u0000\nZ−bg(Y(1)\ni, Y(2)\ni, Xi)\u0001\n;\n7:Estimate KL divergence:\nbDKLi=1\n|D∗\nXi|X\nY∗∈D∗\nXi\u0012bπref(Y∗|X)\nπθ(Y∗|X)−1−logbπref(Y∗|X)\nπθ(Y∗|X)\u0013\n;\n8:Compute the empirical loss function on the batch:\nL=1\n|Bt|X\ni∈Bt\u001a\n−1\n2\u0014\nbIi+ sg\u0000bIIi\u0001\nlogπ(t−1)\nθ(Y(1)\ni|Xi)\u0015\n+βbDKLi)\n9:end for\n10:update θ(t)with gradient descent and get π(t)\nθ:θ(t)=θ(t−1)−α∇θL;\n11:end for\n34\n--- Page 35 ---\nC Experiments Implementation details\nFor the baseline models training, we follow the framework of TRL:Transformer Reinforcement\nLearning [von Werra et al., 2020] and Transformers :State-of-the-Art Natural Language\nProcessing [Wolf et al., 2020]. For the general preference model, we follow the framework of\ngeneral-preference/general-preference-model proposedbyZhangetal.[2024d]. Allmod-\nels were trained with default hyperparameter configurations unless otherwise specified.\nThe Preference Evaluation experiments are conducted on a machine equipped with an NVIDIA\nRTX 6000 Ada GPU and an AMD Ryzen Threadripper PRO 7945WX 12-core CPU. The\nPreference Optimization experiments are performed on a system with an H20 NVLink GPU\nand a 20 vCPU Intel(R) Xeon(R) Platinum 8457C processor. AdamW [Loshchilov and Hutter,\n2019] are used as default optimizer.\nC.1 Preference Evaluation Experiment on IMDb\nOracle Preference Model . Since the IMDb dataset does not contain human preference\nlabels, we adopt the known sentiment classifier siebert/sentiment-roberta-large-english\n[Hartmann et al., 2023], as a ground-truth reward-based labeler. This classifier will give a\nscore s(X, Y) =p(positive |X, Y), which we convert into a reward signal using the log-odds\ntransformation:\nr∗(X, Y) = log\u0012s(X, Y)\n1−s(X, Y)\u0013\n.\nUsing the Bradley–Terry (BT) model, we then compute the ground-truth preference probability\nbetween two completions as:\nP∗(Y(1)≻Y(2)|X) =σ(r∗(X, Y(1))−r∗(X, Y(2))),\nwhere σ(·)is the sigmoid function.\nData Generation and Policy Training Process. We begin by fine-tuning a supervised\nfine-tuning (SFT) model based on the EleutherAI/gpt-neo-125m base model [Black et al.,\n2021] for 3 epochs using the 25,000 training samples from the IMDb dataset. Prompts are\nconstructed by extracting 5-word prefixes from movie reviews. Using the fine-tuned SFT\nmodel as the reference policy, we generate pairs of completions for each prompt. Next, we\nuse the oracle preference model to estimate the preference probabilities between each pair of\ncompletions. Based on these probabilities, we sample binary preference labels indicating which\nresponse is preferred. This synthetic preference dataset is then used to train a target policy\nusing the Direct Preference Optimization (DPO) algorithm over an additional 3 epochs. To\nquantify the relative preference for the target policy over the reference policy, we adopt a Monte\nCarlo estimation approach. Specifically, for each of the 25,000 prefixes in the IMDb test set,\nboth the target and reference policies generate a single completion. The oracle preference model\nis then used to compute the preference probability between the two completions. Aggregating\nthese results, we estimate the overall probability, which is 0.681, that the target policy’s\noutputs are preferred over those of the reference policy.\nPreference Evaluation Process. We consider two versions of the reference policy estimator\nbπref: a correctly specified version, where bπrefcorresponds to the SFT model, and a misspecified\nversion, where bπrefcorresponds to the untrained base model. Similarly, we consider two\nversions of the preference estimator bg: a correctly specified version, which uses the oracle\npreference model, and a misspecified version, where bgis drawn uniformly at random from\n[0,1]. By taking all pairwise combinations of bπrefandbg, we construct four distinct variants of\nthe preference evaluation framework. For the Direct Method (DM) estimator in Equation 6,\nwe apply a Monte Carlo approach by sampling 8 responses from the target policy for each\nprompt. For the Importance Sampling (IS) estimator in Equation 7, we use a clipping ratio of\n35\n--- Page 36 ---\n100 when bπrefis correctly specified and 40 when it is misspecified. In contrast to the clipping\nratio used during preference optimization, a larger ratio is adopted here to better demonstrate\nthe double robustness property of our preference evaluation framework.\nC.2 Preference Optimization Experimen on Real Data\nBaseline models training. For thesummarization task, we adopt models from a group of\nHugging Face, cleanrl, known for their validated and quality-assured implementations [Huang\net al., 2024]. Specifically, we use cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr as\nboth the reference and initial policy model. This SFT policy is trained via token-level super-\nvised fine-tuning on human-written summaries from a filtered TL;DR Reddit dataset [Huang\net al., 2024]. The associated reward model is\ncleanrl/EleutherAI_pythia-1b-deduped__reward__tldr . For Proximal Policy Optimiza-\ntion (PPO) training, we search the hyperparameter over the KL coefficient β∈ {0.05,0.1,0.2}\nand select β= 0.05based on empirical performance. Notably,we observe that PPO training can\nexperience policy collapse under low-precision, as the value function fails to fit accurately; thus,\nPPO models are trained under full precision (FP32). In contrast, all our models are trained\nusing bfloat16 (BF16) for improved computational efficiency. To ensure a fair comparison,\nwe set the maximum response length to 128 for all models, providing a consistent basis for\nassessing summarization quality.\nForhuman dialogue , the SFT model is trained from the base model Qwen/Qwen2.5-1.5B\n[Team, 2024] to better align with the Helpfulness and Harmlessness (HH) dataset. Unlike the\nsummarization SFT model, this version leverages both the preferred (chosen) and non-preferred\n(rejected) responses from the HH preference dataset. It is trained for 3 epochs. We also train\nthree versions of the reward model, all from the same base model ( Qwen/Qwen2.5-1.5B ) to\navoid additional information, corresponding to epochs 1, 2, and 3, as we observe that PPO\ntraining in this setting is highly sensitive to the reward model. When the reward model overfits\nor becomes overly confident, the KL penalty becomes ineffective, and PPO tends to suffer\nfrom policy collapse, hacking the reward model by repeating high-reward tokens. To mitigate\nthis issue, we select the reward model from epoch 1, which achieves an evaluation accuracy of\n72.1%. We further conduct a hyperparameter search over KL coefficients β∈ {0.05,0.1,0.2}\nand learning rates in {1e-7,1e-6,3e-6}. We select a KL coefficient of 0.05 combined with a\nlearning rate of 1e-7as it yields the most stable and effective PPO training performance.\nDRPO Implementation. DRPO implementation inherits transformers.Trainer class.\nFor DRPO-BT, we compute the rewards for two candidate responses and output the preference\nprobability under the BT framework as bg. For DRPO-GPM, we directly compute the preference\nprobability using the corresponding general preference model [Zhang et al., 2024d]. Although\nour proposed algorithm allows the use of a more powerful general preference model for\nestimating bg, as in [Munos et al., 2023], we ensure fairness by training all preference models\nusing the same base model and dataset. This avoids introducing any additional information\nthat could bias the comparison. For both tasks, we set the clipping range to [0.04,2.5], a fairly\ncasual (and wide) specification only to force the IS ratio to not deviate far from 1 and thus\nnot inject too much variance into our estimation. The regularization parameter βis set to\n0.04, the same as that in the default trlimplementation for GRPO [Shao et al., 2024], which\nalso uses k3-type empirical KL divergence. The number of Monte Carlo samples |D∗|is set to\n3 (TL;DR) or 2 (HH). Although more samples may mitigate bias, the effect of adding samples\nis marginally decreasing (since the convergence rate is O((n∗)−1\n2)). As such, it is proper to\nchoose a parsimonious volume of samples and thus incurring little extra computational cost\ncompared to PPO. Other not-mentioned hyperparameters are simply set to default values. For\nfurther details, please refer to the examples in the codebase.\nEvaluation. We compare DRPO with DPO and PPO using GPT-4o-mini to evaluate the\nquality of generated response of each task. Specifically, for the language model fine-tuned\n36\n--- Page 37 ---\nby either baseline or our method, we can sample a response at a certain temperature after it\nreceives a prompt. With the responses of two methods (say A and B), we feed them with a\nquery asking GPT to judge which is more aligned with certain demands. The query template\nused for TL;DR is shown in Table 2, which tries to avoid GPT’s favor of lengthy responses\nfollowing [Ye et al., 2025]. The query template used for HH is shown in Table 3, a standard\ntemplate that is widely adopted by e.g. [Rafailov et al., 2023, Wu et al., 2024a, Ye et al.,\n2025]. It is noteworthy that we randomly shuffle the order of the responses for each query to\neliminate the potential bias from the order of the responses.\nHere, temperature is the scaler of logits before softmax, which can be used to adjust the output\ndistribution of a certain policy. In general, a temperature less than 1 tends to make kurtosis\nof the distribution larger (thus more greedy when generating responses), and a temperature\nlarger than 1 generate even more random responses. The win rate of A over B is equal to the\nproportion of GPT-4o-mini that prefers the responses returned by method A.\nTable 2: Query template for the summarization task.\nWhich of the following summaries does a better job of summarizing the\npost? Strictly follow two criteria when selecting the best summary:\n1. Prioritize the summary which eliminates unnecessary details and\nkeeps the author’s main concern or question.\n2. Prioritize the shorter summary as long as it remains clear and\npreserves the main idea.\nPost: <post>\nResponse A: <response_a>\nResponse B: <response_b>\nFIRST provide a one-sentence comparison of the two summaries, explaining\nwhich\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to\nindicate your\nchoice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: <“A” or “B”>\nD Additional Empirical Results\nIn this section, we first provide pairwise win rates on the TL;DR dataset with other sampling\ntemperatures (see Figure 5). Our method consistently dominates across all temperatures.\nDPO’s performance improves when temperature gets lower, which is in line with results in\n[Rafailov et al., 2023]. PPO’s performance deteriorates in decreasing temperature, likely due\nto PPO is trained with default temperature 1.0. Next, we present pairwise win rates on HH\ndataset with other sampling temperatures (see Figure 6). The results are consistent with that\nof temperature 1.0. In general, DRPO-GPM ≻DRPO-BT ≈DPO≻PPO, showcasing the\nrobustness of our algorithm.\nAdditionally, we present some of the sampled responses of our method and baselines and how\ngpt-4o-mini judges the quality of the completions. See Table 4, 5, 6, 7 for TL;DR examples\nand Table 8, 9, 10, 11 for HH examples.\n37\n--- Page 38 ---\nTable 3: Query template for the human dialogue task.\nFor the following query to a chatbot, which response is more helpful?\nQuery: <user_query>\nResponse A: <response_a>\nResponse B: <response_b>\nFIRST provide a one-sentence comparison of the two responses and\nexplain which you feel is more helpful. SECOND, on a new line, state\nonly\n“A” or “B” to indicate which response is more helpful.\nYour response should use the format:\nComparison: <one-sentence comparison and explanation>\nMore helpful: <“A” or “B”>\nFigure 5: Pairwise Win Rates on TL;DR Dataset under different sampling temperatures (left:\n0.75; right: 0.25)\nFigure 6: Pairwise Win Rates on HH Dataset under different sampling temperatures (left:\n0.75; right: 0.25)\n38\n--- Page 39 ---\nTable 4: Sample Responses of a TL;DR Prompt and Judgement\nPrompt SUBREDDIT: r/relationships\nTITLE: [18/F] My friend [18/F] has naked picture of my boyfriend [17/M]?\nPOST: This is the weirdest situation ever so please bear with me. My friend\nwas over earlier tonight and she was using my cell phone while I was making\nfood. My cell phone has nudes of my boyfriend on it, I know it’s stupid to\ndo that so I don’t need a lesson about it please. We’ve been together for over\ntwo years and we fully trust eachother and I’m never doing anything like it\nagain. I had gave her permission to use my phone, she didn’t just take it.\nBut I hadn’t thought about the pictures, she was just using it to go online\non though so I didn’t think much of it. I came back in the room and got\nmy phone back and we just went on with our evening. But about an hour\nafter she leaves I got some email notifications on my phone, that was weird\nbecause I don’t have my email set up on my phone in the email app. So I\ncheck it and she had left her email signed in, I saw messages from herself to\nherself, of pictures that was sent around the time she wouldve been using my\nphone. I know it’s kind of snooping but I clicked on it, and she literally sent\npictures of my boyfriend on my phone, to her own email. So she probably\nhas them saved on her computer or something now. I have no idea how to\nconfront her about it because it’s the most awkward situation ever, but I\nhave to say something. And I want to make sure she deletes those pictures.\nI don’t even want to continue our friendship anymore if she doesn’t. I feel\ncompletely betrayed and I think that was disrespectful to my boyfriend, me,\nour relationship, and our friendship. I haven’t told my boyfriend yet as I’m\nsure he’ll be mad at me, but I’ll tell him after I resolve this. What can I do?!\n:(\nTL;DR:\nDPO My friend made sure to send naked pictures to me around the time she was\nusing my phone just to \"go online\" apparently, there’s obviously something\nsuspicious since my boyfriend will be angry at me and how can I prove she\ndoesn’t actually have those pictures on her computer now?\nDRPO-BT my friend has nude pictures of my boyfriend on her cell phone. I was over\nearlier tonight and she was using my phone to go online. We didn’t think\nmuch of it because we trust eachother. I got email notifications on my phone\nlater that night that were of my boyfriend sending pictures of her to her own\nemail. I need advice on how to deal with this situation! I don’t even want to\ncontinue our friendship if it is that bad.\nJudgment I prefer Response A [ DRPO-BT ] because it concisely captures the main\nissue of the friend’s betrayal regarding the nude pictures, while eliminating\nunnecessary details and maintaining clarity about the author’s concerns.\n39\n--- Page 40 ---\nTable 5: Sample Responses of a TL;DR Prompt and Judgement\nPrompt SUBREDDIT: r/tifu\nTITLE: TIFU by terrifying a freshie for weeks\nPOST: So, this is about 4 years ago. I was a sophomore in highschool. It\nwas in the first week of school that me and my friend group heard the\nname of this freshman.. He had an awesome name. Idk how i can make\nsomething comparable.. We’ll call him Barry Powers. It wad like that, a\nsuperhero type name. So anyway, we heard the name Barry Powers during\nfreshman roll call and we were like \"damn! What a sick name!\" And made\nit our ultimate mission to find this guy. And so we searched. It took\nweeks. Whenever we met other freshmen, we asked, \"do you know Barry\nPowers?\" And other questions like \"what does he look like?\", \"where did\nyou last see him?\", and \"where can we find him?\". We really wanted to\nfind this kid and congratulate him on his name. So, after two weeks of\nthis walking around and searching and asking, we ask someone about Mr.\nPowers and their answer is \"oh, he’s right there.\" we get freakin hyped\nand beeline to the dude, who looks at us with fear in his eyes. Thinking\nback on it, we had the poor kid, who was eating lunch on his own, in\na corner surround by 4 of us. I asked, \"hey man. You Barry Powers?\"\n\"y-yeah..\" He managed to respond. \"oh, well, you have a sweet name!\" I\nsaid. \"ok..\" Mumbled Powers. Now, the next moment was like the end of\nan always sunny episode; my friends and I looked around and each other\nand decided, \"oh cool we did it, moving on now.\" It was only a little bit\nlater that we heard that this poor guy thought this was some massive\nhazing ritual the whole time. And since we had asked so many people,\nmost of the freshman class thought here was some crazy hazing where\nwe chose one random dude, hunted him down, and did something to him.\nOops!\nTL;DR:\nDPO Had the creepy freshies name Barry Powers searched for about four years\nand finally found an extremely freshie who thought they had some hazing\ndone to it’s name.\nDRPO-GPM TIFU by terrifying a freshie for 4 weeks for the name Barry Powers. The\ndude who was eating lunch by himself thought it was some huge hazing\nritual and was devastated.\nJudgment I prefer Response A [ DRPO-GPM ] because it clearly conveys the main\nconcern of the post while eliminating unnecessary details, and it does so\nin a concise manner.\n40\n--- Page 41 ---\nTable 6: Sample Responses of a TL;DR Prompt and Judgement\nPrompt SUBREDDIT: r/relationships\nTITLE: me (23f) lying to my boyfriend (27) about smoking\nPOST: So, I’ve been a smoker since I was 14 years old, I realize at this point\nhow horrible it is for my health and after many failed attempts to quit I seem\nto be having an extremely hard time. I can’t take chantix as I have epilepsy\nand anything with nicotine in it usually leads me back to smoking again. I’ll\nget it in my head that im going to stop and then the addict in my brain finds\nsome excuse to make it seem okay. My boyfriend of 3 years also used to be\na smoker but quit a few months back, he’s been trying to make an honest\nattempt to get healthier and create more positive habits for himself and the\nlast thing I want to do is pull him away from that. He’s caught me lying a\nfew times the past couple weeks. I feel guilty every time I pick one up, not\nonly because I’m lying but because it’s just so bad for me and I don’t want\nto continue such a nasty habit. I realize that he probably doesn’t feel he can\ntrust me, that if it’s so easy for me to justify smoking and lying about it,\nwhat else might there be? He says he’d prefer for me to just be honest with\nhim and DGAF it but it would absolutely kill me if he picked up smoking\nagain because of my lack of impulse control which I have always struggled\nwith when it comes to doing the difficult thing. I’m afraid of failing and I\nknow that’s exactly what im doing by not trying. I need to quit smoking, I\nneed to work on healthier habits and I’m having an extremely difficult time\nworking through the mental gymnastics that is impulse control and gaining\nmotivation. Any advice on working towards becoming healthier and dealing\nwith this situation and furthermore quitting would be awesome.\nTL;DR:\nPPO I’ve been a smoker since I was 14 years old, I realize at this point how horrible\nit is for my health and after many failed attempts to quit I seem to be having\nan extremely hard time. I can’t take chantix as I have epilepsy and anything\nwith nicotine in it usually leads me back to smoking again. I need to quit\nsmoking, I need to work on healthier habits and I’m having an extremely\ndifficult time working through the mental gymnastics that is impulse control\nand gaining motivation.\nDRPO-BT I’m a smoker and I’m lying to my boyfriend about smoking and it’s causing\nso much stress in our relationship, I need help quitting and working through\nimpulse control.\nJudgment I prefer Response B DRPO-BT because it succinctly captures the main\nissues of lying about smoking and the associated stress in the relationship,\nwhile still conveying the need for help with quitting and impulse control,\nmaking it a clearer and more concise summary of the author’s main concern.\n41\n--- Page 42 ---\nTable 7: Sample Responses of a TL;DR Prompt and Judgement\nPrompt SUBREDDIT: r/relationships\nTITLE: Found out my (28f) ex (32m) is having a kid, and now all I want\nis to sleep around and I’m very depressed.\nPOST: I dated this guy last year. He impregnated a 20f pretty quickly\nafter we broke up and when I found out, I got extremely sad and jealous,\nbecause I wish I was on my way to having a family. We broke up because\nof infidelity. Now though, all that I want to do is go out and get plowed\nby pretty much any guy. I don’t want a relationship, I just want to go\nout and have revenge sex. Why is that? I think I’m just so upset, and\nangry. I don’t know what to do, or really how to handle this.\nTL;DR:\nPPO Found out my (28f) ex (32m) is having a kid, and now all I want is to\nsleep around and I’m very depressed. Why is that? I think I’m just so\nupset, and angry. I don’t know what to do, or really how to handle this.\nDRPO-GPM Found out ex is having a kid, all I want to do is have revenge sex, I’m\nvery upset and angry. How do I handle this?\nJudgment I prefer Response B [ DRPO-GPM ] because it eliminates unnecessary\ndetails while clearly conveying the author’s main emotional state and\nquestion, leading to a more concise summary.\nTable 8: Sample Responses of a HH Prompt and Judgement\nPrompt Human: I woke up with a really stiff neck that hurts when I try to turn my\nhead. Can you help me manage the pain?\nAssistant : You might have “stinger” neck, which is a common injury caused\nby sleeping in an awkward position.\nHuman: I think that’s it. What can I do for the pain?\nAssistant :\nDPO For minor neck pain, over-the-counter pain relievers like ibuprofen or ac-\netaminophen can help reduce the pain and swelling. You can apply a warm\ntowel to the area to promote relaxation of the muscles, and if the pain\npersists you can see a doctor. Are you experiencing any other symptoms?\nFor example, does your neck hurt when you move your head to the side?\nDRPO-BT You could use a heat or cold pack on the neck to reduce the pain and swelling.\nA heat pack is better for improving blood flow to the area, and a cold pack\nwill help with reducing swelling and pain. You might feel better in 10-20\nminutes. If the pain is severe, you could also take over-the-counter pain\nrelievers like acetaminophen or ibuprofen. You could also warm up your neck\nwith some gentle movements, but be careful not to force any movements that\ncause pain.\nJudgment I prefer Response B [ DRPO-BT ] because it provides a clearer, step-by-step\napproach to managing neck pain with both heat and cold options, as well as\ngentle movement suggestions, making it more comprehensive.\n42\n--- Page 43 ---\nTable 9: Sample Responses of a HH Prompt and Judgement\nPrompt Human: What are some ways that I can get smoother skin?\nAssistant : Here are some links about a few of them:\n- Try using rosewater to clean your face\n- Try a face mask on occasion\n- Try drinking a lot of water\n- Try putting a mild facial peel on your face (A glycolic peel, or a salicylic\npeel, or a fruit acid peel)\nHuman: What is rosewater?\nAssistant : Rosewater is a special distilled form of rose petals. It’s used\nfor beauty products. Some people use it for watering the garden or for\nwashing dishes, or to soothe the skin.\nHuman: How much water should I try drinking for smoother skin?\nAssistant :\nDPO It depends on you, but generally you want to drink as much water as you\ncan.\nDrinking enough water helps your body flush out toxins.\nYou can drink water throughout the day, but it’s probably best to drink\nat least eight 8-ounce glasses a day.\nDRPO-GPM Drinking lots of water has benefits for the health and appearance of your\nskin, as well as other bodily functions. The US Centers for Disease Control\nrecommends drinking 8-12 cups a day, and most people’s body needs vary\nbased on their size, age, gender, and other factors.\nJudgment I prefer Response B [ DRPO-GPM ] because it provides a clearer rec-\nommendation based on guidelines from a reputable source (CDC) and\nacknowledges individual variability in hydration needs, making it more\ninformative and supportive.\n43\n--- Page 44 ---\nTable 10: Sample Responses of a HH Prompt and Judgement\nPrompt Human: How do I groom a cat?\nAssistant : Grooming a cat involves combing their fur with a brush. It keeps\nthe fur healthy, and also strengthens the bond between you and your cat.\nHuman: That sounds good. Are there special brushes for cats?\nAssistant :\nPPO Yes, there are many types of brushes for cats, including ones that have soft\nbristles for easy grooming and ones that have a firmer, more efficient design\nfor cleaning and brushing. Many cat owners like to use a finger brush, which\ncan be especially helpful for removing tangles and mats.\nDRPO-BT Yes, there are different kinds of cat brushes, or grooming tools, as they’re\noften called. They come in a variety of materials, like plastic or nylon, and\nthey also come in different sizes. A cat brush for a small cat will be very\ndifferent than a brush for a large cat. You might also want to consider a\n“comb” or “slicker”, which works on a cat’s fur from behind, and is more\neffective for removing a lot of loose hair. A slicker is specifically designed to\nremove hair, while a comb is used to detangle or brush. Some of the brushes\nused for grooming have a long, flexible brush handle with a tiny comb or\nslicker, which you could use to groom your cat from the front or the back.\nJudgment I prefer Response A [ DRPO-BT ] because it provides a more detailed\nexplanation of different types of brushes, their materials, and their specific\nuses, offering the user a broader understanding of grooming tools for their\ncat.\nTable 11: Sample Responses of a HH Prompt and Judgement\nPrompt Human: Is snowboarding hard to learn?\nAssistant :\nPPO Snowboarding is a challenging and demanding sport, and it takes practice\nand dedication to master the skills required for it. However, with proper\ninstruction, training, and practice, it is possible for anyone to learn how\nto snowboard.\nDRPO-GPM It can be! You need good flexibility, balance, strength, and must be\nconfident in your abilities. That doesn’t mean you can’t learn though.\nMany people can definitely learn from coaching and some even try to\npractice on their own, often in combination with lessons. Your best bet is\nto start somewhere, find a class or someone to go with, and then figure it\nout as you go. You should be able to give snowboarding a shot in a few\nweeks.\nJudgment I prefer Summary A [ DRPO-GPM ] because it provides a more detailed\nand practical approach to learning snowboarding, highlighting the impor-\ntance of flexibility, balance, and the value of coaching.\n44",
  "text_length": 117192
}