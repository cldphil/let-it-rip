{
  "id": "http://arxiv.org/abs/2506.04051v1",
  "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through\n  Capability-Aligned Finetuning",
  "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.",
  "authors": [
    "Tim Franzmeyer",
    "Archie Sravankumar",
    "Lijuan Liu",
    "Yuning Mao",
    "Rui Hou",
    "Sinong Wang",
    "Jakob N. Foerster",
    "Luke Zettlemoyer",
    "Madian Khabsa"
  ],
  "published": "2025-06-04T15:16:21Z",
  "updated": "2025-06-04T15:16:21Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04051v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04051v1  [cs.CL]  4 Jun 2025\nHigh Accuracy, Less Talk ( HALT ): Reliable LLMs\nthrough Capability-Aligned Finetuning\nTim Franzmeyer1,∗,Archie Sravankumar2,Lijuan Liu2,Yuning Mao2,Rui Hou2,Sinong Wang2,Jakob\nFoerster1,3,Luke Zettlemoyer3,Madian Khabsa2\n1University of Oxford,2Meta GenAI,3Meta FAIR\n∗Work done at Meta\nLarge Language Models (LLMs) currently respond to every prompt. However, they can produce\nincorrect answers when they lack knowledge or capability – a problem known as hallucination. We\ninstead propose post-training an LLM to generate content only when confident in its correctness\nand to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned\npost-training data that encodes what the model can and cannot reliably generate. We generate\nthis data by splitting responses of the pretrained LLM into factual fragments (atomic statements\nor reasoning steps), and use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments or replacing them with\n“Unsure from Here” – according to a tunable threshold that allows practitioners to trade off response\ncompleteness and mean correctness of the response’s fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALTfor three different trade-off\nthresholds. HALTeffectively trades off response completeness for correctness, increasing the mean\ncorrectness of response fragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the relevant baselines. By\ntuning HALTfor highest correctness, we train a single reliable Llama3-70B model with correctness\nincreased from 51% to 87% across all four domains while maintaining 53% of the response completeness\nachieved with standard finetuning.\nDate:June 3rd, 2025\nCorrespondence: Tim Franzmeyer frtim@robots.ox.ac.uk , Madian Khabsa mkhabsa@meta.com\n1 Introduction\nMost current language models attempt to respond to the majority of prompts, regardless of how complex they\nare or how much domain knowledge is required to answer them. While this behavior is desirable for creative\ntasks (e.g., poem writing), it is undesired when factual correctness is crucial. This phenomenon, often referred\nto as hallucination, poses risks in high-stakes applications such as medicine or law. We propose HALT, which\nfinetunes an LLM to only respond with information it is confident about, and to (partially) abstain otherwise.\nFor example, when asked to write a person’s biography, the LLM would list only facts it is confident in and\nwould omit others. Similarly, when presented with a complex math problem, it would show only the reasoning\nsteps in which it has high confidence, terminating its response with “Unsure from here” if it cannot proceed –\npotentially forgoing a final answer. Naturally, restricting an LLM to provide only high-confidence information\nreduces the total number of correct statements, as it withholds any information it deems uncertain – even if\nsome of that information might ultimately be correct. To balance this trade-off, HALTallows practitioners to\nadjust a confidence threshold. This threshold controls whether the LLM should respond more eagerly (and\nrisk occasional errors) or more conservatively (and risk omitting correct information).\nHALT’s key insight is to finetune the pretrained LLM only on correct content it is capable of generating , i.e.,\ncontent within the bounds of the knowledge and reasoning capabilities obtained during pretraining, as we\nillustrate in Figure 1. Further, HALTfinetunes the LLM to output “Unsure from here” when uncertain about\nthe remainder of the generation.\nHALTis motivated by the observation that LLMs possess internal calibration (Tian et al., 2023b; Kadavath\n1\n--- Page 2 ---\n    HALT Finetuning    Standard Finetuning\n   Example Responses after HAL T Finetuning   Example Responses after Standard Finetuning   Example Responses used for Standard FinetuningFormat  ax2+bx+c=0 [...]Compute x=2a−b±b2−4ac  Solutions: x=1 and x = -4The email is support@meta.com and the phone is 07747124321.   Example Responses used for HAL T FinetuningFormat  ax2+bx+c=0 [...]Compute x=2a−b±b2−4ac  .Unsure from here.The email is support@meta.com.   Training PromptsSolve 4x^2+3x-7=0How can I contact Meta?Use the divisibility rule for 11.Separate and sum digits in odd numbers.Result: 22Their dog was called Bo and was a German Shepherd.\nUse the divisibility rule for 11.Unsure from here.Their dog was called Bo.   Test-Time User PromptsRemainder 123456 divided by 11?Tell me about Obama’s dog.\nBeyond Capabilities of  Pretrained LLMIncorrect Responses from Finetuned LLM\nAdjusted to Capabilities of Pretrained LLM Responses from HALT Finetuned LLM  Figure 1 Comparison between Standard and HALT Finetuning for LLMs: Standard finetuning trains the LLM on\nresponses that potentially exceed the pretrained LLM’s capabilities, which results in incorrect outputs at test time.\nHALT finetuning trains the model only on content within the pretrained LLM’s capability limits, omitting unknowns\nor replacing them with “Unsure from here”. HALT finetuning improves response correctness as the LLM is trained to\ngenerate responses according to its capabilities, which may result in partially incomplete responses when the pretrained\nLLM’s capabilities are insufficient.\net al., 2022), suggesting they can produce responses aligned with their internal confidence in the relevant facts\nor reasoning steps. HALT’s approach of generating capability-aligned finetuning samples according to the\npretrained LLM’s capabilities is based on recent work (Lin et al., 2023; Zhou et al., 2024) which shows that\nLLMs do not acquire novel capabilities during finetuning but only learn to effectively utilize knowledge and\nreasoning capabilities obtained during pretraining. Moreover, HALT’s approach of finetuning only on content\nthat the LLM is capable of generating is supported by recent work (Gekhman et al., 2024; Kang et al., 2024)\nshowing that finetuning LLMs on concepts unknown to them increases hallucination. While HALTrequires\nadditional model generations and post-processing for each pretrained LLM it is applied to, we believe this is\nreasonable, as recent work (Zhou et al., 2024) found that merely 1000 finetuning examples can be sufficient\nto finetune generally capable models. Unlike previous methods (Yadkori et al., 2024; Farquhar et al., 2024;\nCheng et al., 2024; Brahman et al., 2024; Feng et al., 2024; Zhang et al., 2024a; Kang et al., 2024), HALT\nrequires no additional computations at test time, as it does not rely on post-processing or sampling. Instead,\nit produces a finetuned model that attempts the best possible answer within its capability. Also, unlike earlier\nwork, HALTextends to prompts involving not only knowledge retrieval but also complex reasoning.\nGiven a pretrained LLM and a finetuning dataset, for each prompt, HALT generates a response aligned to the\ncapabilities of the LLM. For a given prompt, the HALTpipeline (1) generates an initial response via few-shot\nprompting of the pretrained LLM, (2) splits it into factual fragments , (3) assesses correctness of individual\nfragments via an Evaluator LLM with access to ground truth information (e.g. the ground truth response),\nand (4) post-processes the fragments to arrive at the final HALTfinetuning response – which only contains\nfragments that the pretrained LLM is capable of generating. Figure 2 provides an example of the pipeline.\nFor the second step of decomposing the response into fragments, we make the assumption that responses\neither consist of independent fragments orcausally dependent fragments , i.e., fragments that build on top of\neach other in a logical sequence. This distinction is made for each prompt based on its domain, e.g., math\nsamples are assumed to consist of causally dependent fragments, while biographies are assumed to consist of\nindependent ones.\nFor responses composed of independent fragments, we rely on prior work (Song et al., 2024) and use a\nfinetuned LLM to extract atomic statements, i.e., independently verifiable statements. For example, “Barack\nObama was born in Hawaii” is an atomic statement that could be extracted from his biography. In step 3, the\ncorrectness of each fragment is assessed independently using the ground-truth-conditioned evaluator LLM\n(e.g. conditioned on the respective Wikipedia article). The HALTfinetuning response (4) is then composed of\n2\n--- Page 3 ---\nall correct fragments, or set to “Unsure from here” if no fragments were correct.\nFor responses composed of causally dependent fragments, such as responses to math questions, HALTinstead\nsplits responses at natural boundaries like new lines or equation statements. HALTthen determines the\nfragment at which the first error occurs using an evaluator LLM with access to the ground truth response.\nTheHALTresponse is then composed by replacing the incorrect fragment and all subsequent fragments with\na single “Unsure from here” statement, arriving at a capability-aligned finetuning response.\nImportantly, HALTenables practitioners to choose the desired trade-off between response completeness\n(analogous to Recall) and response correctness (analogous to Precision ) according to the deployment scenario.\nThis is effected by estimating model capabilities based on multiple responses of the pretrained LLM. For\nexample, composing the HALTresponse from a response with less-than-average correct fragments yields a\nconservative estimate of the model’s capability, resulting in an LLM that responds more carefully.\nWe evaluate HALTon a wide variety of language models, including LLama3-8B, LLama3-70B, Gemma2-9B,\nand Mistral-7B, and on a series of tasks, including writing Wikipedia-style biographies, solving competition\nmathematics problems, answering medical questions, and solving coding problems posed in natural language.\nWe compare HALTto standard finetuning, prior work on factuality improvement (FactTune, (Tian et al.,\n2023a)), prior work on training models to abstain (IDK, (Cheng et al., 2024)), and a heuristic baseline that\ntrims responses to match the response length of HALT. We split generated responses from prompts in the\neval dataset into factual fragments and assess the completeness and average correctness of fragments for each\nresponse. Tuning HALTbased on our threshold allows to increase the correctness by 17% for Llama3-70B\non average. Across all settings, HALTimproves the F1 score, i.e. the harmonic mean of completeness and\ncorrectness, on average by 4%, as compared to the baselines. Tuning HALTfor correctness, we combine data\nfrom all domains and finetune a single reliableLLama3-70B model that achieves an accuracy of 87% across the\nfour domains – which is in contrast to 51% accuracy achieved with standard finetuning – while maintaining\n51% completeness of responses. Last, we demonstrate that slightly modifying HALTto annotate fragments as\n“Unsure” – instead of omitting them – informs users about uncertainty while retaining complete responses.\n2 Methods\nOverview. A pretrained LLM Mmaps from input sequences to output sequences, M:X → Y. Additionally,\nwe have a finetuning dataset D=(xj, yj)m\nj=1, which consists of mpairs of prompts xjand their corresponding\nground truth responses yj. The goal of our method, which we refer to as HALT, is to create a modified\nfinetuning dataset DH=(xj, yH\nj)m\nj=1, where each target response yH\njis aligned with the capabilities of the\npretrained model M. We assume access to an evaluator Ethat takes as input a statement fand information\nJand determines whether fis correct or incorrect according to the given information, i.e., E: (f, J)→ {0,1}.\nThis evaluation could be performed by a human annotator, an additional LLM, or any other means.\nFigure 2 provides an overview of how HALTprocesses each prompt xj: First, a preliminary response is\ngenerated using the pretrained model M, then split into factual fragments. Next, the evaluator Eassesses the\ncorrectness of individual fragments, using the given ground truth response yj, potentially supplemented by\nother relevant context. Finally, incorrect fragments are removed or replaced by “Unsure from here” to arrive\nat the target response yH\nj.\n2.1 Creating a capability-aligned finetuning dataset with HALT\nGenerating the Preliminary Response. For each prompt xj, we first generate a response that aligns with the\nformat of the ground truth responses in the finetuning dataset Dthrough few-shot prompting of the pretrained\nmodel. We sample a set of four prompt-response pairs {(xk, yk)}4\nk=1uniformly at random from D\\ {(xj, yj)}.\nThe context Cj={(xk, yk)}4\nk=1is concatenated with the target prompt xjusing a question-answer format.\nThe pretrained model Mis then prompted to create a preliminary response ypt\njfor the prompt xj, i.e.\nypt\nj=M(concat (Cj, xj)), whereconcatdenotes the concatenation of the context Cjand the target prompt\nxjinto a single input sequence.\n3\n--- Page 4 ---\nResponse split into factual FragmentsFragment Correctness Assessment with Evaluator EHALT Finetuning ResponseTrain PromptFew-Shot prompted response of Pre-Trained LLM                                                                                                                                                               Causally Dependent Response Fragments                                                                                                                                                                         Independent Response FragmentsFormat  ax2+bx+c=0, thencompute x=2a−b±b2−2ac  and find the solutions as x=3 and x = -1005Evaluation Overview\nSolve 4x^2+3x-7=0(F1) Format  ax2+bx+c=0(F2) Compute x=2a−b±b2−2ac  (F3) Sol: x=3 and x=-10(F1) Format  ax2+bx+c=0(F2) Compute x=2a−b±b2−2ac  (F3) Sol: x=3 and x=-10Format  as ax2+bx+c=0.Unsure from here.Tell me about Barack Obama’s dog!Barack Obama, the 45th President of the USA, had a deceased German Shepherd called Sunny.(F1) Barack Obama is the 45th president of the USA.(F2) Barack Obama’s dog is called Sunny.(F3) Barack Obama’s dog Sunny is deceased. (F4) Barack Obama’s  dog Sunny was a German Shepherd.(F1) Barack Obama is the 45th president of the USA.(F2) Barack Obama’s dog is called Sunny.(F3) Barack Obama’s dog Sunny is deceased. (F4) Barack Obama’s  dog Sunny was a German Shepherd.Barack Obama had a dog called Suny.Figure 2 TheHALTpipeline includes generating a preliminary response via few-shot prompting of the pretrained LLM,\nsplitting it into factual fragments, and assessing each fragment’s correctness before compiling the HALT finetuning\nresponse. For a causally dependent response, as displayed at the top, an error results in an “Unsure from here”\nmarker to indicate uncertainty. For a response consisting of independent fragments, as shown at the bottom, incorrect\nfragments are removed.\nSplitting the Preliminary Response into Factual Fragments. Next, we split the preliminary responses ypt\nj\ninto individual factual fragments, denoted as ypt\nj= (f1, . . . , f k), where each fragment represents a verifiable\nunit. The method used for fragmentation generally depends on the type of response. Simple, inherently\nstructured responses may allow fragmentation along natural boundaries, such as equal signs in math equations,\nbullet points or newline characters. Long and complex sentences may require more sophisticated methods for\nfragmentation, such as machine learning methods specifically trained to extract verifiable statements from\ncomplex sentences (Song et al., 2024; Min et al., 2023; Wei et al., 2024). We empirically find that grouping\nprompts by type of question (e.g., math, coding, knowledge) allows us to determine the required fragmentation\nmethod. We describe fragmentation in Section 3.1.\nAssessing Correctness of Fragments. We distinguish between two types of response structures when determin-\ning the correctness of the individual response fragments f1, . . . , f k: Responses consisting of either independent\nfragments , or of causally dependent fragments that build on each other in a logical sequence. Figure 2 shows\nthe HALT processing pipeline for example prompts of both categories. We note that not every response will\nperfectly fit one of the two categories but leave more complex dependency structures, such as dependency\ngraphs, for future work.\nFor responses consisting of independent fragments, the correctness of each fragment is independent of the\ncorrectness of other fragments. Examples are Wikipedia-style information overviews, lists of independent\nrecommendations, and results for independent tasks. For responses consisting of causally dependent fragments,\nthe correctness of a fragment depends on the correctness of prior fragments. Examples are step-by-step\nsolutions, code implementations, or sequential reasoning, where each part builds upon previous ones.\nFor responses consisting of independent fragments, we assess the correctness of each fragment individually\nusing the evaluator E, with information Jgiven by the ground truth response yjand potentially additional\ninformation. Forresponsesconsistingofcausallydependentfragments, weprompttheevaluator E–conditioned\non the ground truth response – to identify the first incorrect fragment of the response, and consider all\nsubsequent fragments as incorrect.\nCreating the HALT Finetuning Response. For responses with independent fragments, we merely remove any\nincorrect fragments. For responses with causal dependency, we remove all statements starting from the first\nincorrect statement, and, in case any incorrect statements were present, place “Unsure from here” at the end\nof the response, as illustrated in Figure 2.\n4\n--- Page 5 ---\n(Fragment 1) Format  ax2+bx+c=0 [...](Fragment 2) Compute x=2a−b±b2−4ac  (Fragment 3) Solutions: x=1 and x = -4Completeness: 100%Correctness: 100%F1: 1(Fragment 1) Format  ax2+bx+c=0 [...](Fragment 2) Compute x=2a−b±b2−4ac  (Fragment 3) Solutions: x=1 and x = -4Completeness: 30%Correctness: 30%F1: 0.30(Fragment 1) Format  ax2+bx+c=0 [...] (Fragment 2) Unsure from here.Completeness: 30%Correctness: 100%F1: 0.46(Fragment 1) Unsure from here.Completeness: 0%Correctness: 0%F1: 0Figure 3 Example responses (ground truth response in\nthe top left corner), with completeness, correctness, and\nF1 scores annotated for each. The number of required\nfragments here is three. We observe that the highest F1\nscore is achieved when the LLM answers with the correct\nfirst fragments and abstains after.Table 1Avg. number of fragments per response and avg.\nnumber of fragments incorrectly labelled by the Llama3-\n405B evaluator. Analysis run on 300 manually-labelled\nLlama3-70B responses per dataset.\nWikipedia MATH Medical Q. Coding\nAvg # fragm. 9.58 5.54 5.26 7.85\nAvg # incorrect 0.27 0.63 0.41 1.14\nTable2Finetuningmodelsonresponsesgeneratedviabest-\nof-5 few-shot prompting results in only marginally lower\nperformance (average correctness of statements) than\nfinetuning on the Unchanged (ground truth) responses.\nPretrained Model Responses MATH Medical Q. Coding\nLlama-3-8BUnchanged 0.30 0.69 0.30\nFew-Shot 0.25 0.68 0.27\nLlama-3-70BUnchanged 0.81 0.80 0.38\nFew-Shot 0.76 0.78 0.35\nMistral-7BUnchanged 0.30 0.69 0.30\nFew-Shot 0.25 0.69 0.24\nGemma-2-9BUnchanged 0.39 0.69 0.27\nFew-Shot 0.33 0.71 0.26\n2.2 Trading off between Response Correctness and Completeness\nThe trade-off between average correctness of fragments and response completeness can be tuned by choosing\nthe preliminary response from a set of sampled responses of the pretrained LLM. Specifically, instead of relying\non a single greedily decoded response to estimate the pretrained model’s capabilities, we sample multiple\npreliminary responses. Selecting the “worst” response among the sampled preliminary responses, i.e., the\nresponse in which the lowest number of fragments is correct, provides a conservative estimate of the model’s\ncapability. Composing the HALTresponse based on the worst response results in a finetuned model with\nhigher correctness but lower completeness. The opposite holds true when selecting the best response instead.\nSpecifically, for each prompt xj, we sample Npreliminary responses {ypt,n\nj}N\nn=1with different random seeds\nand compute the average fragment correctness for each response as outlined in Section 3.1. We sort the\npreliminary responses in ascending order of average correctness and select the α-percentile response as\nn∗=⌈αN⌉andypt\nj=ypt,n∗\nj. Here, αdetermines the trade-off, where a lower αfavors correctness over\ncompleteness. The selected response ypt\njis then processed with the HALTpipeline shown in Figure 2.\n3 Experimental Validation\nOverview. As HALT responses are derived from few-shot prompted responses of the pre-trained model, we\nfirst validate that finetuning on these yields performance similar to finetuning on the ground truth responses.\nWe then assess the accuracy of our evaluator E, which we implement using Llama3-405B. We observe in\nTable 1 that the evaluator has an average per-response absolute error ranging from 0.27 to 1.14 fragments.\nWe then evaluate HALTon four different datasets and for four open-source LLMs, namely Llama3-8B (Dubey\net al., 2024), Llama3-70B (Dubey et al., 2024), Gemma2-9B (Team et al., 2024), and Mistral-7B (Jiang et al.,\n2023). We demonstrate that HALTallows to trade off between response completeness and correctness while\nsimultaneously achieving the best arithmetic mean of completeness and correctness. Utilizing HALT, we train\na single reliable model on all four datasets that achieves 87% correctness, an increase of 36% compared to the\ncorrectness achieved by standard finetuning. Last, we demonstrate that HALTcan be modified to annotate\nresponse fragments that the model is unsure about as “Unsure” – instead of omitting them. This allows to\nprovide users with complete responses, while simultaneously informing users about likely incorrect parts of\nthe response.\n5\n--- Page 6 ---\nFigure 4 Response correctness (y-axis) and completeness (x-axis) for LLama3-70B (top) and Llama3-8B (bottom)\nwhen finetuned with different methods (desired number of fragments is nall). F1 Score is constant along curved grey\nlines, and highest in the top right corner. For HALT and Randomtrim, results are shown with different trade-offs\nbetween completeness and correctness, where lighter colors indicate tuning for higher correctness. We omit results\nwith less than 5% completeness, e.g., RandomTrim results for Wikipedia Biographies and Math. We observe that\nHALTallows for strongly influencing the trade-off between correctness and completeness across all four datasets while\nachieving higher F1 scores (closer to the top right corner) than baseline methods. We show results for Gemma2-9B\nand Mistral7B in Figure 7 in the Appendix.\n3.1 Implementation and Validation of Relevant Components\nPrior Work and Baselines. We implement FactTune (Tian et al., 2023a), IDK (Cheng et al., 2024), and\nsupervised finetuning, which we refer to as “Unchanged”. We remark that none of these baselines allow for\nadjusting the trade-off between completeness and correctness. FactTune, applicable to knowledge tasks, i.e.\nthe Wikipedia dataset, first finetunes the pretrained model, e.g. on Wikipedia biographies. It then generates\nresponses from the finetuned model, evaluates atomic statements using FactScore (Min et al., 2023), and\ncreates a preference dataset to finetune the model again using Direct Preference Optimisation (Rafailov et al.,\n2024). IDK evaluates which prompts a model can answer, then finetunes the model to either entirely abstain\nor completely answer to given prompts. We additionally implement a RandomTrim baseline, which, instead\nof removing fragments according to the capability of the pretrained model as done with HALT, removes the n\nlast fragments of a given response. nis sampled from a Poisson distribution, which is chosen such that the\naverage response length matches that of HALTresponses.\nDatasets. Wikibios (Lebret et al., 2016) contains the summary paragraph of individuals listed on Wikipedia,\nsampled uniformly randomly. Hence, this dataset contains many individuals who are only little known. Since\nthe original dataset was assembled 8 years ago, we updated all information using the official Wikipedia API,\nand excluded all individuals not uniquely identified when searching for their name on the Wikipedia API.\nWe added each person’s full Wikipedia article to the dataset, which is used as additional information for\nthe evaluator E. In the reasoning domain, we only consider datasets that provide step-by-step solutions,\nas we found that these increase the accuracy of the evaluator Esignificantly. MATH (Hendrycks et al.,\n2021b) contains mathematical problems across various fields. MedExQA (Kim et al., 2024) is a medical\nquestion-answering dataset covering five distinct specialties. APPS (Hendrycks et al., 2021a) evaluates code\ngeneration, containing a wide range of programming challenges; we consider those marked as “introductory”.\n6\n--- Page 7 ---\nSplitting Responses into Fragments. We decompose a response into factual fragments as follows. Wikipedia\nanswers are often complex and nested, which prohibits a heuristic-based fragmentation. We instead use\na state-of-the-art fact-extraction LLM, made available by Song et al. (2024). Although responses are well\nstructured for MATH, we found that a heuristic-based fragmentation yields unsatisfactory results and instead\nprompted LLama3-405B to fragment responses at natural boundaries. For MedExQA we found it satisfactory\nto split answers at full stop signs. Similarly, code responses in APPS are split line-for-line.\nEvaluator Implementation. For Wikibios, we prompt Llama3-405B with the fragment to assess, as well as\nwith the entire Wikipedia article of the relevant person, and ask it to assess whether the given fragment is\ncorrect or incorrect in the given context (see example prompt in App. B.1). Hence, the evaluator is called\nmultiple times to assess a single response. For MATH, MedExQA, and Wikibios, we prompt Llama3-405B\nwith the numbered fragments that make up the response and the ground-truth step-by-step solution, and\nprompt it to identify the first incorrect fragment.\nF1 Score to Measure the Mean of Completeness and Correctness. We follow Wei et al. (2024) and use the F1\nscore as a combined measure of response correctness and completeness, analogous to binary classification.\nHere, response completeness (Recall) measures the ratio of correct fragments relative to the desired number\nof fragments, i.e.,ncorrect\nndesired. Response correctness (Precision) is the relative correctness of all given response\nfragments, i.e.,ncorrect\nngiven. We consider two definitions for the desired number of fragments. The first defines\nthe desired number of fragments as the number required to answer the question fully (see App. A.1 for\ndetails), which we refer to as nall(which is independent of the model’s capability). Hence, any method can\nonly achieve full recall under this definition if the pretrained LLM’s capabilities are sufficient to answer all\nquestions completely and correctly. The second option defines the number of desired fragments according\nto the pretrained LLM’s capability, which we refer to as ncapable. Specifically, ncapableis the number of\ncorrectfragments in the few-shot prompted response of the pretrained LLM. We remark that under the latter\ndefinition, a method could attain full recall (and an F1 score of 1) if the finetuned LLM answers exactly\nwith those response fragments that are within its capability limits. Figure 3 shows response completeness\nand correctness for different example responses, with the number of desired fragments defined as the number\nrequired to answer the question fully ( nall).\nFigure 5 We show results for training a single reliableLLama3-70B model with HALTtuned for increased correctness,\ntrained on equal shares of all four datasets, referred to HALT (Combined Model). We additionally plot results for\nHALT trained on each dataset individually, referred to as HALT, and results when finetuning on the Unchanged\ndataset. We observe that HALTallows for increasing average accuracy across all datasets by 36% to 87% while\nmaintaining an average completeness of 25%.\nInformation-Conditioned Llama-3-405B Yields a Strong Correctness Evaluator. We evaluate the performance\nof our evaluator in assessing the correctness of individual fragments of responses of all four datasets. For\neach dataset, we randomly sampled 300 responses generated by Llama3-70B and manually annotated the\ncorrectness of all fragments. We observe in Table 1 that the Llama3-405B evaluator’s error ranges from\n0.27 incorrectly assessed atomic statements per Wikipedia response, composed of 9.58 atomic statements on\naverage, to a discrepancy of 1.14 lines on Coding, with an average number of 7.85 lines of code per response.\nIn MATH, the evaluator misjudges the location of the first incorrect fragment by only 0.6 reasoning steps, at\nan average response length of 5.54. We conclude that Llama3-405B – prompted with relevant information – is\nwell suited to assess the correctness of individual logical fragments.\n7\n--- Page 8 ---\nFinetuning on Few-Shot Prompted Responses is comparable to Finetuning on Ground Truth Responses.\nHALT relies on responses derived from few-shot prompting of the pretrained LLM. For the MATH, Medical\nQA, and Coding datasets, we compare the performance of the finetuned model when either finetuned on the\ndataset’s unchanged ground truth response or the best-of-5 few-shot prompted response of the pretrained LLM.\nTable 2 shows that the average gap ranges from 1.7% to 3.7% only across the models, and could be further\nclosed by sampling more responses. As a result, the expected correctness for finetuning methods that rely on\nthe ground truth response is 1.7% to 3.7% higher than that of HALT responses; however, we empirically find\nthat HALT can still outperform such methods. We remark that for Mistral-7B, the gap is at 5% and 6% in\nMath and Coding, respectively, suggesting that this model has worse in-context learning capabilities than\nthe other examined models. As discussed later, we found that this performance gap influences the finetuned\nMistral-7B HALT models. These findings support the assumption that LLMs do not acquire novel knowledge\nor capabilities during finetuning (Lin et al., 2023; Zhou et al., 2024).\n3.2 HALT Enables Trading off Completeness and Correctness\nNext, we finetune all four models on all four datasets, comparing HALTto the baselines. For HALT, we sample\nfive few-shot prompted preliminary responses, sort them by relative correctness, choose a response according\ntoα∈[40%,60%,80%]. That is, we either choose the most, second most, or third most accurate response, and\nthen process it with the HALTpipeline. We provide examples of finetuning responses for different tradeoff\nparameters αin App. B.2. We discard the response with the lowest and second lowest relative correctness, as\nwe empirically found that finetuning on these can result in close to zero completeness and correctness. Figure 4\nshows that choosing different αforHALTallows to trade off correctness and completeness effectively, altering\ncorrectness by ±17% and ±12% for Llama3-70B and Llama3-8B, respectively. Table 3 in the Appendix\nshows that HALTfurther achieves the highest harmonic mean of completeness and correctness in most cases,\nas compared to baseline methods, both for ndesiredset to nalland to ncapable.\n3.3 Training a Multi-Domain LLM Achieving 87% Correctness\nWe now evaluate the real-world use case of training a single model that achieves high correctness across\nmultiple domains. Specifically, we train a single reliableLlama3-70B model on an equal-parts mix of data\nfrom all four domains with HALTtuned for increased accuracy, i.e., with α= 40%. Figure 5 shows that this\nmodel achieves an average accuracy of 87% across all four domains, as compared to 51% achieved by training\non a mix of the ground truth datasets. Meanwhile, the response completeness remains at 25% on average.\nThis demonstrates that HALT allows training a generally capable model that users can trust significantly\nmore than models trained with standard finetuning.\n3.4 Add-On HALT: Marking Fragments as “Uncertain” instead of omitting them.\nInstead of only being provided with response fragments that the model is certain about, users might desire to\nobserve the model’s full response to a prompt while being informed which parts of the response might be\nincorrect. This is in contrast to responses achieved with standard finetuning, where the user is not informed\nwhich parts (fragments) the model is uncertain about. For example, this would enable users to edit generated\nCode which the LLM is uncertain about or to verify correctness of specific statements using external sources.\nWe evaluate this use case by modifying HALTto annotate uncertain response fragments with “Uncertain”,\ninstead of omitting them. Specifically, we modify the finetuning dataset for Math, Medical Q., and Coding\nby marking the first incorrect response fragment and all subsequent fragments as “Uncertain”. Note that\nthese fragments are replaced by “Unsure from here” in the default HALTresponse. We do not investigate the\nopen-ended Wikipedia biography writing task as it allows for generating an unbounded number of “Uncertain”\nstatements. Figure 6 in the Appendix shows that training Llama3-70B at α= 60%with Add-On HALT\nresults in significant correctness improvements when fragments marked as “Uncertain” are excluded from the\ncorrectness evaluation, achieving improvements similar to those of HALT’s default implementation. When the\n“Unsure” markers are ignored Add-On HALT achieves response completeness similar to that achieved when\nfinetuning on the Unchanged responses. In conclusion, a slight modification of HALT enables the retention of\nresponse completeness while reliably informing the user about likely incorrect fragments.\n8\n--- Page 9 ---\n4 Related Work\nA large body of prior work studied detecting LLM generations that likely contain hallucinations, i.e., likely\ncontain incorrect statements. Approaches include training probes (Su et al., 2024), directly inspecting hidden\nstates (Chen et al., 2024), or evaluating semantic entropy of generations (Farquhar et al., 2024). These works\nare complemented by approaches to mitigate hallucinations via weight-space editing (Zhang et al., 2024b),\nmodified decoding techniques (Chuang et al., 2023), preference training on samples that contain fewer or\nmore hallucinations (Tian et al., 2023a), or post-processing of generated responses (Mohri and Hashimoto,\n2024; Gui et al., 2024; Wang et al., 2024). In contrast to these works, HALTaddresses the trade off between\nresponse completeness and correctness, does not require any post-processing of generations at test-time, and\napplies to reasoning problems.\nRecent works have discovered that finetuning LLMs on unknown examples increases hallucinations (Kang\net al., 2024; Gekhman et al., 2024; Tian et al., 2023b), which motivates HALT’s approach of only finetuning\non samples that are within the LLM’s capabilities.\nAnother line of prior work has found that LLMs are well-calibrated, and found that confidence scores can,\nfor example, be obtained via inspection of logits (Kadavath et al., 2022) or via prompting LLMs to state\ntheir confidence (Lin et al., 2022). This provides a basis for recent works that train models to abstain when\nuncertain (Zhang et al., 2024a; Chen et al., 2023; Yadkori et al., 2024; Wen et al., 2024; Brahman et al.,\n2024; Feng et al., 2024; Cheng et al., 2024; Tuan et al., 2024). Unlike these works, which train models to\neither fully abstain or fully answer, which is unsuitable for prompts that require long responses, HALTtrains\nLLMs to selectively compose responses according to the given capabilities. In further contrast, HALTenables\nmodification of the desired trade off between completeness and correctness of responses.\n5 Conclusion\nWe present HALT, a novel finetuning paradigm that trains models to generate responses according to their\ninternal confidence. HALT relies on evaluating the knowledge and capabilities of a given pretrained model\nwith respect to specific prompts and trains the pretrained model to omit response fragments it is uncertain\nabout. HALT training yields models that are up to 37% more accurate than those trained using unchanged\nground-truth answers while maintaining high response completeness.\nLimitations and Future Work. Future work could address the following limitations. First, our method\nassumes either independent factual fragments or causal dependence. This could be addressed by instead\nestablishing dependency graphs; for instance, coding responses might be decomposed into smaller blocks whose\ncorrectness depends on other blocks. Second, HALTrelies on a binary assessment of whether a fragment is\nwithin the model’s capability, but a more fine-grained scale and thresholding could improve performance.\nThird, we train separate models for different correctness–completeness trade-offs. Training a single model\nthat adapts its responses based on a user-selected parameter would improve flexibility. Fourth, our approach\nassumes no access to external sources. Instead, LLMs should call tools or acquire external information when\nuncertain about the remaining response. Fifth, we treat all factual fragments as equally valuable, though\nweighting them by their impact on the final answer could improve utility. Finally, we focus on text-based\nmodels and datasets; extending HALTto multi-modal domains is a promising direction.\n9\n--- Page 10 ---\nReferences\nFaeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander,\nSarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et al. The art of saying no: Contextual noncompliance\nin language models. arXiv preprint arXiv:2407.12043 , 2024.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms’ internal\nstates retain the power of hallucination detection. arXiv preprint arXiv:2402.03744 , 2024.\nJiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, and Somesh Jha. Adaptation with\nself-evaluation to improve selective prediction in llms. arXiv preprint arXiv:2310.11689 , 2023.\nQinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Kai Chen, and\nXipeng Qiu. Can ai assistants know what they don’t know? arXiv preprint arXiv:2401.13275 , 2024.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by\ncontrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883 , 2023.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024.\nSebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models\nusing semantic entropy. Nature, 630(8017):625–630, 2024.\nShangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. Don’t hallucinate,\nabstain: Identifying llm knowledge gaps via multi-llm collaboration. arXiv preprint arXiv:2402.00367 , 2024.\nZorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does\nfine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904 , 2024.\nYu Gui, Ying Jin, and Zhimei Ren. Conformal alignment: Knowing when to trust foundation models with guarantees.\narXiv preprint arXiv:2405.10301 , 2024.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Jackson Jiang, Andy Zou, Dawn Song, and\nJacob Steinhardt. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938 , 2021a.\nhttps://arxiv.org/abs/2105.09938 .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021b. https:\n//arxiv.org/abs/2103.03874 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint\narXiv:2310.06825 , 2023.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac\nHatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv\npreprint arXiv:2207.05221 , 2022.\nKatie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control\nhow language models hallucinate. arXiv preprint arXiv:2403.05612 , 2024.\nYunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan Wu. Medexqa: Medical question answering benchmark with\nmultiple explanations. arXiv preprint arXiv:2406.06331 , 2024.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.\nRémi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with the Wikipedia\nInfoboxes. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\npages 1061–1071. Association for Computational Linguistics, 2016. https://aclanthology.org/D16-1112/ .\nBill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhaga-\nvatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. In The\nTwelfth International Conference on Learning Representations , 2023.\n10\n--- Page 11 ---\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint\narXiv:2205.14334 , 2022.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer,\nand Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.\narXiv preprint arXiv:2305.14251 , 2023.\nChristopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees. arXiv preprint\narXiv:2402.10978 , 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\nSystems, 36, 2024.\nYixiao Song, Yekyung Kim, and Mohit Iyyer. Veriscore: Evaluating the factuality of verifiable claims in long-form text\ngeneration. arXiv preprint arXiv:2406.19276 , 2024.\nWeihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Unsupervised real-time\nhallucination detection based on the internal states of large language models. arXiv preprint arXiv:2403.06448 , 2024.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models\nat a practical size. arXiv preprint arXiv:2408.00118 , 2024.\nKatherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Fine-tuning language models\nfor factuality. arXiv preprint arXiv:2311.08401 , 2023a.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christo-\npher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models\nfine-tuned with human feedback. arXiv preprint arXiv:2305.14975 , 2023b.\nYi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz, William Yang Wang,\nand Daniel M Bikel. Towards safety and helpfulness balanced responses via controllable large language models.\narXiv preprint arXiv:2404.01295 , 2024.\nZhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Xiaoshuang Shi, Kaidi Xu, Hengtao Shen, and\nXiaofeng Zhu. Conu: Conformal uncertainty in large language models with correctness coverage guarantees. arXiv\npreprint arXiv:2407.00499 , 2024.\nJerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang,\nCosmo Du, et al. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802 , 2024.\nBingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe, and Lucy Lu Wang. The art of\nrefusal: A survey of abstention in large language models. arXiv e-prints , pages arXiv–2407, 2024.\nYasin Abbasi Yadkori, Ilja Kuzborskij, David Stutz, András György, Adam Fisch, Arnaud Doucet, Iuliya Beloshapka,\nWei-Hung Weng, Yao-Yuan Yang, Csaba Szepesvári, et al. Mitigating llm hallucinations via conformal abstention.\narXiv preprint arXiv:2405.01563 , 2024.\nHanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang.\nR-tuning: Instructing large language models to say ‘i don’t know’. In Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume\n1: Long Papers) , pages 7106–7132, 2024a.\nShaolei Zhang, Tian Yu, and Yang Feng. Truthx: Alleviating hallucinations by editing large language models in\ntruthful space. arXiv preprint arXiv:2402.17811 , 2024b.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\nYu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems , 36, 2024.\n11\n--- Page 12 ---\nA Supplementary Experimental Details and Results\nA.1 Defining the Number of Fragments Required to Fully Answer a Prompt.\nWe remark that the number of fragments necessary to answer a question cannot be unambiguously determined\nand, in practice, slightly varies for different pre-trained LLMs. However, when, e.g., an LLM responds with two\ncorrect fragments, we must define nallto be able to compute response completeness. As different pretrained\nLLMs will answer the same few-shot prompt with slightly different numbers of fragments, as shown in Table 4,\nwe choose nallaccording to the pre-trained model’s response statistics in the MATH and Medical Question\ndatasets. For biography writing and coding, we found that the number of fragments of the ground truth\nresponse defines nallwell.\nA.2 Additional Implementation Details\nLLM implementations. We implement all LLMs using their open-source implementations given at https:\n//github.com/huggingface/transformers .\nFinetuning Details. We use supervised finetuning with LoRA (Hu et al., 2021) for compute-efficient training,\nwith a rank of 256 and alpha of 512. Training is performed using the AdamW (Kingma, 2014) optimizer with\nbetas set to (0.9,0.95), weight decay of 0.01, and an initial learning rate of 1e−5. The learning rate schedule\nfollows a warmup fraction of 5%, with a decay to 1e−6over 5 epochs. The effective batch size is set to 128.\nComputational Resources. Finetuning of the 7-9B LLMs took around 5-6 hours on a single Nvidia A100\nGPU. Finetuning of the Llama3-70B model took around 3-4 hours on a node of 8 Nvidia A100 (80GB) GPUs.\nDatasets. We downloaded the datasets from the following domains. The Wikibios dataset was downloaded\nfrom https://github.com/DavidGrangier/wikipedia-biography-dataset ; the MATH dataset from https://github.\ncom/hendrycks/math , the MedexQA dataset from https://huggingface.co/datasets/bluesky333/MedExQA , and\nthe APPS dataset from https://github.com/hendrycks/apps .\nBaseline Implementations. We implemented FactTune (Tian et al., 2023a) using the authors’ implementation\ngiven at https://github.com/kttian/llm_factuality_tuning . For the IDK baseline (Cheng et al., 2024) we follow\nthe authors’ implementation given at https://github.com/OpenMOSS/Say-I-Dont-Know .\nFigure 6 We evaluate an Add-On version of HALT for LLama3-70B. Add-On HALT annotates response fragments as\n’Unsure’ instead of omitting them. Removing fragments marked by Add-On HALT as ’Unsure’ results in a significant\nincrease in correctness. At the same time, evaluating ’Unsure’ fragments yields results similar to training on the\nUnchanged (Few-Shot generated) responses. This demonstrates that Add-On HALT allows for increased correctness\nwhile preserving the completeness of unchanged finetuning.\n12\n--- Page 13 ---\nTable 3F1 score (arithmetic mean of response correctness and completeness) for all models and finetuning methods.\nWe show results when setting the desired number of fragments as those required to fully answer the question (left\nside of Table) and when setting the desired number as those that the pre-trained model is capable of generating\n(right side). Note that a perfect F1 score is only attainable when setting ndesired=ncapable. We find that HALT\nlargely outperforms prior work and baselines, by larger margins for ncapable. For Gemma2-7B, we hypothesize that the\nrelatively lower performance of HALTin Math and coding is likely due to the worse in-context learning capabilities, as\noutlined in Section 3.1.\nndesired =nall ndesired =ncapable\nLLM Finetuning Wikipedia Math Medical Q. Coding Wikipedia Math Medical Q. CodingLlama-3-8BUnchanged 0.23 ±0.01 0.30 ±0.05 0.70±0.02 0.33 ±0.01 0.26 ±0.04 0.49 ±0.07 0.81 ±0.02 0.58 ±0.05\nRandomTrim 0.05 ±0.01 0.08 ±0.00 0.70±0.02 0.30 ±0.02 0.21 ±0.03 0.26 ±0.00 0.82 ±0.02 0.61 ±0.06\nAbstain (IDK) (Cheng et al., 2024) 0.14 ±0.00 0.12 ±0.02 0.58 ±0.16 0.05 ±0.03 0.25 ±0.04 0.38 ±0.10 0.69 ±0.18 0.18 ±0.09\nFactTune (Tian et al., 2023a) 0.24 ±0.01 - - - 0.38 ±0.02 - - -\nHALT (ours) 0.28±0.00 0.35±0.07 0.68 ±0.03 0.35±0.04 0.63±0.01 0.66±0.12 0.81 ±0.04 0.77±0.05Llama-3-70BUnchanged 0.26 ±0.01 0.68 ±0.01 0.80 ±0.03 0.44±0.03 0.35 ±0.02 0.75 ±0.01 0.87 ±0.03 0.71 ±0.09\nRandomTrim 0.12 ±0.01 0.64 ±0.01 0.82 ±0.00 0.38 ±0.03 0.26 ±0.01 0.75 ±0.01 0.88 ±0.00 0.75 ±0.05\nAbstain (IDK) (Cheng et al., 2024) 0.28 ±0.01 0.53 ±0.27 0.75 ±0.07 0.19 ±0.13 0.44 ±0.03 0.68 ±0.38 0.82 ±0.07 0.48 ±0.32\nFactTune (Tian et al., 2023a) - - - - - - - -\nHALT (ours) 0.33±0.03 0.68±0.04 0.81±0.06 0.41 ±0.01 0.68±0.02 0.87±0.02 0.88±0.06 0.83±0.04Mistral-7BUnchanged 0.14 ±0.00 0.31±0.02 0.69±0.06 0.33 ±0.02 0.20 ±0.04 0.50 ±0.06 0.81 ±0.06 0.57 ±0.05\nRandomTrim 0.05 ±0.00 0.12 ±0.06 0.64 ±0.04 0.25 ±0.06 0.34 ±0.04 0.43 ±0.25 0.80 ±0.05 0.52 ±0.37\nAbstain (IDK) (Cheng et al., 2024) 0.11 ±0.00 0.09 ±0.02 0.56 ±0.17 0.01 ±0.00 0.18 ±0.03 0.43 ±0.07 0.73 ±0.20 0.04 ±0.02\nFactTune (Tian et al., 2023a) 0.20±0.01 - - - 0.44 ±0.03 - - -\nHALT (ours) 0.13 ±0.01 0.28 ±0.03 0.66 ±0.02 0.34±0.03 0.68±0.05 0.69±0.41 0.81 ±0.08 0.82±0.06Gemma-2-9BUnchanged 0.15±0.01 0.42 ±0.08 0.71 ±0.01 0.34 ±0.02 0.21 ±0.08 0.64 ±0.15 0.82 ±0.01 0.62 ±0.05\nRandomTrim 0.02 ±0.00 0.28 ±0.06 0.72 ±0.04 0.34 ±0.03 0.19 ±0.05 0.59 ±0.14 0.82 ±0.04 0.65 ±0.04\nAbstain (IDK) (Cheng et al., 2024) 0.13 ±0.00 0.13 ±0.04 0.67 ±0.04 0.02 ±0.01 0.19 ±0.02 0.38 ±0.11 0.79 ±0.04 0.08 ±0.03\nFactTune (Tian et al., 2023a) 0.08 ±0.00 - - - 0.31 ±0.05 - - -\nHALT (ours) 0.09 ±0.00 0.41±0.27 0.74±0.02 0.44±0.06 0.57±0.03 0.79±0.60 0.86±0.03 0.85±0.08\nTable 4The table shows the average number of fragments per response for answers generated by each pretrained model\nusing in-context learning. The ‘Ground Truth‘ row represents the ground truth answer, with each category showing\n100% correctness. Among the models, Llama-3-70B has the highest relative correctness, followed by Llama-3-8B and\nGemma-2-9B, with Mistral-7B-v0.3 performing the lowest.\nPretrained ModelWikipedia Math Medical Q. Coding% corr. avg# given % corr. # given % corr. # given % corr. # given % corr.\nGround Truth 9.8 ±8.0 100.0 4.5 ±3.4 100.0 5.1 ±1.2 100.0 15.0 ±12.7 100.0 100.0\nLlama-3-8B 11.9 ±11.2 12.9 3.3 ±3.2 30.1 5.4 ±1.5 66.9 9.3 ±11.7 42.2 38.0\nLlama-3-70B 9.6 ±10.3 30.9 5.5 ±3.0 54.1 5.2 ±1.3 83.8 7.8 ±9.0 58.8 56.9\nGemma-2-9B 9.6 ±8.0 6.4 4.1 ±3.3 28.6 5.3 ±1.9 71.8 12.4 ±10.2 30.2 34.3\nMistral-7B-v0.3 9.6 ±7.9 6.3 3.7 ±3.1 17.1 5.2 ±1.8 60.8 11.7 ±9.1 27.9 28.0\nB Prompt Templates and examples of HALT responses\nB.1 Example Evaluator Prompts\nWe show example prompts in Figures 8 and 9.\nB.2 Example HALT responses for different trade offs between response correctness and\nresponse completeness\nB.2.1 Example HALT finetuning responses for the Wikibios and MATH datasetsand for different thresholds\nWe show examples of HALTresponses for different trade-offs between response completenes and response\ncorrectness in Figures 10 and 11.\nWe note that responses for the Wikipedia biography dataset are segmented into individual atomic statements.\nThis segmentation results fromt he application of the fact-extraction LLM (Song et al., 2024), but can reverted\nby prompting the LLM to recompose the response into a single text at the end of the generation. We omitted\nthis recomposition as it complicates the assessment of individual statements.\n13\n--- Page 14 ---\nFigure 7 Response correctness (y-axis) and completeness (x-axis) for Gemma2-9B (top) and Mistral-7B (bottom) when\nfinetuned with different methods (desired number of fragments is nall). F1 Score is constant along curved grey lines,\nand highest in the top right corner. For HALT and Randomtrim, results are shown with different trade-offs between\ncompleteness and correctness, where lighter colors indicate tuning for higher correctness. We omit results with less\nthan 5% completeness.\nExample prompt to Llama3-405B evaluator to determine correctness of a fragment that is part of a response composed of\nindependent fragments.\n****Context:****\n{{FULL WIKIPEDIA ARTICLE OF BARACK OBAMA}}\n****Statement:****\nBarack Obama was born on May 12, 1960.\n****Instruction:****\nIs the above Statement True or False according to the given Context? Reply with either ’True’ or ’False.’\nFigure 8 Example prompt to Evaluator for assessing a fragment from a Wikipedia-style response.\n14\n--- Page 15 ---\nExample prompt to Llama3-405B evaluator to determine correctness of a response composed of causally dependent fragments.\n****Math Question:****\nA triangle has a base of 8 units and a height of 5 units. If the triangle is part of a rectangle with the same base and\nheight, what fraction of the rectangle’s area does the triangle occupy?\n****Ground truth Response for Math Question:****\nThe area of a triangle is calculated using the formula: Area =1\n2×base×height.\nSubstituting the given values: Area =1\n2×8×5 = 20square units.\nThe area of the rectangle is calculated using the formula: Area =base×height. Substituting the given values:\nArea = 8×5 = 40square units.\nThe triangle occupies a fraction of the rectangle’s area given by:Triangle’s area\nRectangle’s area=20\n40=1\n2.\nThus, the triangle occupies1\n2of the rectangle’s area.\n****Step-wise Response given by student:****\nLine 1: The area of a triangle is calculated using the formula: Area =1\n2×base×height.\nLine 2: Substituting the given values: Area =1\n2×8×5 = 20square units.\nLine 3: The area of the rectangle is 8×5 = 40square units.\nLine 4: The fraction of the rectangle’s area that the triangle occupies is 20square units.\n****Instruction:****\nYou are given a Math question above, as well as a Ground truth Response to it. Determine until which line the step-wise\nresponse given by the student is correct. Reply with the number of the last correct line. If the first step is already\nwrong, reply with zero. Otherwise, reply with the number of the last correct line, i.e., with an integer between 0 and the\nnumber of lines in the response. Reply with ’Line X’ only, where X is the number of the last correct line.\nFigure 9 Example prompt to Evaluator for assessing a MATH response.\nExample for α= 0.4\n**** Question:****\nTell me about Daniel the\nStylite\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nSaint Daniel the Stylite died\non 11 December 493.Example for α= 0.6\n**** Question:****\nTell me about Daniel the\nStylite\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nSaint Daniel the Stylite was\nborn around 409.\nSaint Daniel the Stylite\ndied on 11 December 493.\nSaint Daniel the Stylite\nwas a Christian saint.Example for α= 0.8\n**** Question:****\nTell me about Daniel the\nStylite\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nSaint Daniel the Stylite was\nborn around 409.\nSaint Daniel the Stylite\ndied on 11 December 493.\nSaint Daniel the Stylite\nwas a Christian saint.\nSaint Daniel the Stylite\nis commemorated on 11\nDecember in the Eastern\nOrthodox Church.\nFigure 10 Example HALTresponses for different treshold parameters αfor the Wikipedia dataset. Finetuning on the\nleft response (lowest α) results in a conservative LLM providing responses of higher correctness and lower completeness,\nwhile finetuning on right response results in a eagerly responding LLM.\n15\n--- Page 16 ---\nExample for α= 0.4\n**** Question:****\nA triangle has a base of\n8 units and a height of 5 units.\nIf the triangle is part of a\nrectangle with the same base\nand height, what fraction of\nthe rectangle’s area does the\ntriangle occupy?\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nThe area of a triangle is\ncalculated using the formula:\nArea =1\n2×base×height.\nSubstituting the given val-\nues:Area =1\n2×8×5 =\n20square units.\nUnsure from here.Example for α= 0.6\n**** Question:****\nA triangle has a base of\n8 units and a height of 5 units.\nIf the triangle is part of a\nrectangle with the same base\nand height, what fraction of\nthe rectangle’s area does the\ntriangle occupy?\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nThe area of a triangle is\ncalculated using the formula:\nArea =1\n2×base×height.\nSubstituting the given val-\nues:Area =1\n2×8×5 =\n20square units.\nThe area of the rectangle\nis calculated using the for-\nmula:Area =base×height.\nSubstituting the given val-\nues:Area = 8 ×5 =\n40square units.\nUnsure from here.Example for α= 0.8\n**** Question:****\nA triangle has a base of\n8 units and a height of 5 units.\nIf the triangle is part of a\nrectangle with the same base\nand height, what fraction of\nthe rectangle’s area does the\ntriangle occupy?\nRespond with no more than\n500 tokens. Only list facts\nabout the person that you are\ncertain about. Separate all\nfacts by ’\\n’. If unsure how to\ncontinue at any point during\nthe response, write ’Unsure\nfrom here.’ and terminate\nyour response.\n**** Response:****\nThe area of a triangle is\ncalculated using the formula:\nArea =1\n2×base×height.\nSubstituting the given val-\nues:Area =1\n2×8×5 =\n20square units.\nThe area of the rectangle\nis calculated using the for-\nmula:Area =base×height.\nSubstituting the given val-\nues:Area = 8 ×5 =\n40square units.\nThe triangle occupies a frac-\ntion of the rectangle’s area\ngiven by:Triangle’s area\nRectangle’s area=\n20\n40=1\n2.\nThus, the triangle occupies1\n2\nof the rectangle’s area.\nFigure 11 Example HALTresponses for different treshold parameters αfor the MATH dataset. The left response\n(lowest α) results in a conservative LLM providing responses of higher correctness and lower completeness, while\nfinetuning on the right response results in a eagerly responding LLM.\n16",
  "text_length": 60645
}