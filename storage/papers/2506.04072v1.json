{
  "id": "http://arxiv.org/abs/2506.04072v1",
  "title": "Controlling Difficulty of Generated Text for AI-Assisted Language\n  Learning",
  "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.",
  "authors": [
    "Meiqing Jin",
    "Liam Dugan",
    "Chris Callison-Burch"
  ],
  "published": "2025-06-04T15:38:21Z",
  "updated": "2025-06-04T15:38:21Z",
  "categories": [
    "cs.CL",
    "cs.HC",
    "I.2.7"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04072v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04072v1  [cs.CL]  4 Jun 2025Controlling Difficulty of Generated Text for AI-Assisted Language\nLearning\nMeiqing Jin∗, Liam Dugan∗, Chris Callison-Burch\nUniversity of Pennsylvania\n{mqjin, ldugan, ccb}@seas.upenn.edu\nAbstract\nPracticing conversations with large language\nmodels (LLMs) presents a promising alterna-\ntive to traditional in-person language learning.\nHowever, most LLMs generate text at a near-\nnative level of complexity, making them ill-\nsuited for beginner learners (CEFR: A1–A2).\nIn this paper, we investigate whether control-\nlable generation techniques—specifically, mod-\nular methods that do not require model fine-\ntuning—can adapt LLM outputs to better sup-\nport absolute beginners. We evaluate these\nmethods through both automatic metrics and\na user study with university-level learners of\nJapanese. Our findings show that while prompt-\ning alone fails to control output difficulty, the\nuse of future discriminators (Yang and Klein,\n2021) significantly improves output compre-\nhensibility (from 40.4% to 84.3%). We further\nintroduce a novel token-level evaluation met-\nric, Token Miss Rate (TMR), that quantifies\nthe proportion of incomprehensible tokens per\nutterance and correlates strongly with human\njudgments. To support future research in AI-\nassisted language learning, we release our code,\nmodels, annotation tools, and dataset.1\n1 Introduction\nMany learners struggle to acquire a second lan-\nguage (L2), often citing low motivation, anxiety,\nand a lack of conversational practice opportunities\n(Alzaanin, 2023; Papi and Khajavy, 2023). In paral-\nlel, large language models (LLMs) have emerged as\nfluent and engaging conversational agents (Tseng\net al., 2024), prompting interest in their use for lan-\nguage learning. However, these models typically\nproduce output at a near-native level, which can\noverwhelm beginners and impede learning.\nThus, a critical component of a second language\nconversation partner is difficulty control . The dif-\nficulty of utterances should fall within the zone\n*Equal contribution\n1https://github.com/EmmaJin0210/ChatLingual\nLLMこんにちは、先生 \n(“Hello Sensei”) \nL ogits X\nW eights \nDiﬃculty \nController \n=\nOutput Figure 1: We control the difficulty of language model\noutputs to assist in language learning using future dis-\ncriminators Yang and Klein (2021). We show that this\nworks better than prompting in both automatic and hu-\nman evaluations\nof proximal development —not too easy but not\ntoo hard (Krashen, 1985). However, studies have\nshown that LLMs are unable to tailor their outputs\nto a specified difficulty level using simple prompt-\ning or in-context learning approaches (Imperial\net al., 2023; Almasi et al., 2025; Ramadhani et al.,\n2023; Uchida, 2025; Benedetto et al., 2025; Zhang\nand Huang, 2024). Fine-tuning approaches offer\nmore control but are often impractical due to cost,\naccessibility, or compatibility with closed-source\nmodels (Malik et al., 2024; Stowe et al., 2022).\nIn this work, we conduct the first study to evalu-\nate LLM-based conversation partners with absolute\nbeginner learners (CEFR: A1–A2). We focus on\nmodular controllable generation techniques—that\nis, methods that operate externally to the model\nand do not require access to weights or training\ndata. Specifically, we examine future discrimina-\ntors (Yang and Klein, 2021), which bias generation\ntoward desired characteristics during decoding.\nThrough automatic simulations and a user study\nwith beginner-level Japanese learners, we find that\nfuture discriminators significantly enhance output\ncomprehensibility while preserving fluency and\n--- Page 2 ---\nnaturalness. Compared to prompting alone, this\napproach more than doubles the rate of comprehen-\nsible utterances (from 40.4% to 84.3%).\nIn addition to these findings, we introduce the\nToken Miss Rate (TMR), a new metric that mea-\nsures the percentage of tokens per utterance that\nare likely to be incomprehensible to the learner.\nTMR provides a finer-grained lens on model output\nand correlates strongly with human comprehensi-\nbility ratings, enabling scalable evaluation without\nrelying solely on user studies.\nTogether, our results suggest that modular con-\ntrol techniques offer a practical path toward adapt-\ning LLMs for beginner language learners. By ad-\ndressing difficulty control without retraining, and\nby rigorously evaluating output both holistically\nand at the token level, our work lays the founda-\ntion for more accessible, personalized AI-driven\nlanguage tutoring systems.\n2 Related Work\nLanguage Learning with AI Chatbots Previ-\nous work on adapting AI for language learning\nhave primarily focused on non-conversational set-\ntings. There have been studies using AI to generate\nexample short stories (Malik et al., 2024), exam-\nple sentences (Stowe et al., 2022; Glandorf and\nMeurers, 2024), recommendations for material at a\nlearner’s level (Jamet et al., 2024), and automated\nassessments (Caines et al., 2023). However, the\npotential for AI-based L2 conversation partners has\nbeen relatively under-explored.\nOf the few studies that do evaluate AI for\nconversation-based language learning, most simply\nevaluate the vanilla model’s ability with no explicit\ndifficulty control applied (Lee et al., 2023; Wang,\n2025; Hayashi and Sato, 2024; Almasi et al., 2025).\nThe only previous work that evaluates the effect\nof difficulty control for AI conversation partners\nis Tyen et al. (2024) who evaluate a BlenderBot-\n2.7B model with and without a reranking-based\ndifficulty control on a population of 160 L2 En-\nglish speakers recruited through Prolific. While\nthey found difficulty control largely ineffective, ab-\nsolute beginners are very under-represented in their\nwork with 0% of participants self-reporting as A1\nand less than 12% reporting as A2. This is likely\ndue to the recruitment material being in the target\nlanguage. Our study is the first to specifically target\nthese beginner speakers.Controlling Difficulty of LLMs Previous work\non using controllable generation techniques in\nL2 learning has primarily focused on re-ranking\ncandidate generations at inference time. This\ninvolves training a classifier for the target diffi-\nculty and using it to rank a set of kcandidate re-\nsponses—outputting the one that maximizes the\ndesired metric (Tyen et al., 2022; Jamet et al., 2024;\nGlandorf and Meurers, 2024; Malik et al., 2024).\nWhile this does improve LLMs’ ability to adhere\nto a particular difficulty level, it is expensive and\nprone to failure in cases where the LLM produces\nno good candidates.\nStowe et al. (2022) attempt to fix this problem\nby fine-tuning on example utterances labeled with\ncontrol tokens and Malik et al. (2024) additionally\nuse reinforcement learning to encourage models\nto generate outputs at the correct difficulty level.\nWhile these techniques are effective at controlling\ndifficulty, they are expensive and require all diffi-\nculty levels to be specified at training time. We are\nthe first study to investigate the feasibility of using\nmore modular controllable generation approaches\non this task that do not require fine-tuning the full\nmodel.\n3 Methods\nIn this section we give a brief introduction to con-\ntrollable generation and explain the future discrimi-\nnators method introduced by Yang and Klein (2021)\nas it is relevant for our comparison.\n3.1 Controllable Text Generation\nControllable text generation is the task of generat-\ning text X=x1. . . x nconditioned on a specific\ndesired attribute a(e.g. the desired difficulty of\nan utterance). Let Gbe an LLM which models\nP(xi|x1:i−1)for tokens x1. . . x n. The likelihood\nof the full sequence P(X)can be factorized as\nP(X) =nY\ni=1P(xi|x1:i−1)\nTo condition on a particular attribute, we must in-\nstead model P(X|a), which modifies the previous\nfactorization:\nP(X|a) =nY\ni=1P(xi|x1:i−1, a)\nThus the controlled generation task can be simpli-\nfied to finding a method for modeling the attribute-\nconditional distribution P(xi|x1:i−1, a).\n--- Page 3 ---\n3.2 Future Discriminators (FUDGE)\nIn this work we employ Future Discriminators for\nGeneration (FUDGE) (Yang and Klein, 2021) to ac-\ncomplish the controllable generation task. FUDGE\noperates by approximating P(xi|x1:i−1, a)based\non a Bayesian factorization:\nP(xi|x1:i−1, a)∝P(a|x1:i)·P(xi|x1:i−1)\nWhere P(a|x1:i)denotes the likelihood of the at-\ntribute agiven the prefix x1:i. Intuitively this can\nbe seen as a re-ranking of the candidate tokens\nfromGby some attribute predictor model M.\nImportantly, this technique requires the predictor\nmodel Mto model how likely it is that a particular\nsequence will have the attribute in the future . This\nis a very different task from the standard sequence\nclassification task, thus preventing us from using\noff-the-shelf classifiers as predictors.\n3.3 Training Predictors for Difficulty\nGiven a dataset Dof text-attribute tuples (x1:n, a′)\nthe predictor model Mis fine-tuned using all\nprefix-attribute pairs (x1:i, a′)to learn the future\nlikelihood of the attribute given the prefix. For our\nuse-case we modify the predictor to minimize a\nmulti-class cross entropy loss instead of a binary\nloss.\nLM=−X\nxlogP(a′|x)\nThis encourages the model to output a distribution\nof likelihoods over ndifficulty levels.\n3.4 Inference\nTo re-rank candidate tokens for a particular prefix\nx1:i−1the predictor model is run on each potential\nnext token sequence x1:i. Doing this for the full\ndistribution would be prohibitively expensive, we\nfollow Yang and Klein (2021) and truncate the next\ntoken distribution to top- k=50 sampling.\nWe also employ a control parameter λwhich\nallows us to modify the magnitude of the difficulty\npredictor. Thus the logit vector ˆyfor the full system\nis calculated by computing:\nˆy=λa+ (1−λ)x\nwhere xandaare the truncated logit vectors from\nthe LLM and the predictor respectively. After this\nwe normalize the vector ˆyand sample the next\ntoken normally.JLPT CEFR Example Conversation Topic\nN5 A1 introduce yourself (such as your name,\njob/school, where you’re from, etc.)\nN4 A2 describe your favorite hobby and how\noften you do it\nN3 A2-B1 talk about planning a birthday party: lo-\ncation, food, and guests\nN2 B1-B2 describe a news story you found interest-\ning, and why it caught your attention\nN1 B2-C1 discuss recent advancements in regener-\native medicine and their ethical implica-\ntions in Japan\nTable 1: JLPT levels and their corresponding CEFR\nlevels (Japan Foundation, 2025) along with example\nconversation topics from the self-chat pipeline (§5.1).\n4 Experimental Setup\nIn this section we discuss our evaluation metrics\n(§4.1), the data we use for training and evaluation\n(§4.2), and the models we use for comparisons\n(§4.3). In Section 5 we report the results for the\nautomatic evaluation and in Section 6 we report the\nresults of the human study. Additional detail on all\naspects of our setup can be found in Appendix B.\n4.1 Evaluation Metrics\nToken Miss Rate (TMR) To evaluate the diffi-\nculty of the tutor’s responses, we introduce a new\nmetric we call Token Miss Rate (TMR) . This mea-\nsures the percentage of tokens in a particular utter-\nance that are above a user’s specified level. We\ncalculate this metric as follows:\nTMR =cnt_above\ntotal_tokens\nWhere cnt_above represents the number of tokens\nabove a user’s level and total_tokens represents\nthe total number of tokens in an utterance. This in-\ntuitively measures the percentage of the output that\nis comprehensible and can serve as an automatic\nproxy for human judgements of understandability.\nIn order to map each token to its appropri-\nate difficulty level we need to use whole word\ntokenization—not sub-word tokenization. To do\nthis we use the Sudachi tokenizer (Takaoka et al.,\n2018) on segmentation mode C (coarse-grained)\nand set it to produce the base form (i.e. dictionary\nform) of each token (Table 2). This allows us to\nlater classify each token to a JLPT level by exact\nmatch string searching over a list of vocabulary\n(see Appendix D).\nControlError In addition to our TMR metric we\nalso calculate ControlError (Malik et al., 2024).\n--- Page 4 ---\n天気がいいから散歩しましょう\n天気,が,いい ,から ,散歩,する ,ます\nTable 2: Example tokenization using Sudachi (Takaoka\net al., 2018) on segmentation level C (coarse-grained).\nVerbs and grammatical structures are lemmatized for\ntoken matching and punctuation is removed.\nThis metric measures the squared distance between\nthe predicted difficulty level of some input text x\naccording to some classifier sand the target diffi-\nculty level t.\nControlError (x, t) = (sjlpt(x)−t)2\nFor our classifier model we use the fine-tuned\nTokohu-BERT model2from Benedetti et al. (2024).\nJReadability As a final metric for difficulty we\ncalculate the JReadability score (Fujitani et al.,\n2012)—a Japanese-language analogue to the pop-\nular Flesch-Kincaid Grade Level metric (Kincaid\net al., 1975). JReadability includes features such\nas sentence length, word difficulty, and syntactic\ncomplexity and was derived from statistical models\ntrained on grade-level Japanese corpora.\nFluency Metrics We also report fluency met-\nrics such as perplexity (PPL), average utterance\nlength in tokens (Length), and average trigram di-\nversity scores (div@3) for each output model. We\ncalculate perplexity using the Aya-Expanse 8B\nmodel3from Dang et al. (2024) as it scored well on\nthe Swallow open LLM leaderboard for Japanese\n(Okazaki et al., 2024).\n4.2 Data\nFor our project we used two datasets. One for train-\ning the predictor model (§3.3) and one to provide\nJLPT vocabulary lists for evaluating Token Miss\nRate (TMR) (§4.1).\nTraining Data For training the predictor model\nwe use the jpWaC-L 1.0 corpus (Erjavec et al.,\n2008), a 300-million-token web corpus of sen-\ntences annotated with corresponding JLPT levels.\nThe labels were derived using heuristics based on\nJLPT content specs (Japan Foundation, 2024).\n2https://huggingface.co/bennexx/\ncl-tohoku-bert-base-japanese-v3-jlpt-classifier\n3https://huggingface.co/CohereLabs/\naya-expanse-8bEvaluation Data For evaluation we use the\njlpt-anki-decks dataset developed by chyyran,\nan open-source collection of Japanese vocabu-\nlary decks for the Anki spaced repetition plat-\nform (Chyyran, 2015). The vocabulary lists are\nbased on the widely recognized compilations from\ntanos.co.uk , a foundational resource in JLPT\npreparation that provides curated vocabulary lists,\ngrammar guides, and example sentences.\n4.3 Comparisons\nWe implement the following four controllable gen-\neration methods as comparisons:\n1.Baseline Prompt : A baseline prompt that in-\nstructs the tutor to only output at the target dif-\nficulty level (Almasi et al., 2025, inter alia ).\n2.Detailed Prompt : A more involved prompt\nthat instructs the model using few-shot exam-\nple conversations and vocab lists as well as\nmore detailed descriptions of the target level.\n3.Overgenerate : A re-ranking model following\nprevious work (Tyen et al., 2022, inter alia )\nthat re-ranks candidates based on estimated\nToken Miss Rate (TMR) calculated using vo-\ncabulary bins heuristically derived from the\ntraining data, as detailed in Section B.1.\n4.FUDGE : A future discriminators-based\nmethod using the technique detailed in Sec-\ntion 3 from Yang and Klein (2021).\nThe prompts for the Overgenerate andFUDGE\nmethods are identical to the baseline prompting\nmethod. Prompt templates for Baseline andDe-\ntailed can be found in Appendix B along with level\nand topic descriptions.\nFor all methods except FUDGE we evaluate with\nboth Qwen2.5-72B-Instruct (Qwen et al., 2025)\nandgpt-4-turbo (OpenAI et al., 2024). Qwen2.5\nwas picked as the open-source model as it per-\nformed the best on the Swallow open LLM leader-\nboard for Japanese (Okazaki et al., 2024).\nFor the predictor model in FUDGE we used\nModernBERT (Warner et al., 2024). We fine-tuned\nthe model using a multi-class classification head\nthrough the Huggingface Trainer API for one epoch\nwith a learning rate of 5e−5. More details on the\ntraining process can be found in Appendix A.\n--- Page 5 ---\nお気に入りは、AIと協力して書いた短編小説だよ。SFの世界観で、人間とAIの関係性について書いてみたんだ。読んでくれる？(A favorite of mine is a short story written in collaboration with AI. It's set in a science ﬁction world and is about the relationship between humans and AI. Would you like to read it?)もちろん、聞かせてください！どんな物語なのかとても気になります。お願いします。(Of course, please tell me! I’m very interested to know what type of story that ends up being. Go ahead.)Level: N2Level: N5Student ModelTutor Model\n(Business Fluent)(Absolute Beginner)N5 Tokens: 7  N4 Tokens: 2  N3 Tokens: 1\nExplain a cultural difference[…],Describe a recent news story[…],Discuss a workplace challenge[…]Topic Pool (N2 Level)Topic Pool: N2Token Miss Rate (TMR): (1 + 2) / (1 + 2 + 7) = 30%Figure 2: In the “self-chat” evaluation pipeline (§5.1) we evaluate our controlled generation methods by simulating\nconversations between a student LLM and difficulty-controlled tutor LLM. Tutor outputs are evaluated using Token\nMiss Rate (TMR) (§4.1) which quantifies the percentage of tokens in an utterance above the target level.\nAutomatic Evaluation Results\nModel Length Avg. PPL ↓div@3 ↑JReadability ↑TMR ↓ControlError ↓\nBaseline (Qwen) 104.33 4.62 0.501 3.68 15.7 2.19\nBaseline (GPT-4) 74.90 5.73 0.613 3.33 15.0 2.22\nDetailed (Qwen) 127.90 4.50 0.555 3.59 14.4 1.89\nDetailed (GPT-4) 73.45 5.67 0.620 3.39 13.7 1.92\nOvergenerate (Qwen) 107.48 4.62 0.475 3.67 14.7 2.19\nOvergenerate (GPT-4) 70.19 5.65 0.611 3.48 13.1 2.30\nFUDGE ( λ= 0.25) 77.81 4.87 0.580 3.74 14.2 2.15\nFUDGE ( λ= 0.5) 74.37 5.16 0.583 3.73 14.3 2.09\nFUDGE ( λ= 0.8) 75.80 5.13 0.599 3.78 13.3 1.89\nFUDGE ( λ= 0.9) 74.27 5.23 0.599 3.82 11.9 1.78\nTable 3: Automatic evaluation of controlled generation approaches using the “self-chat” pipeline (§5.1). We see that\nFUDGE performs the best on difficulty control metrics such as Token Miss Rate (TMR) and ControlError while\nperforming comparatively well on fluency metrics such as perplexity and diversity.\n5 Automatic Evaluation\n5.1 “Self-Chat” Pipeline\nFollowing Almasi et al. (2025) we employ a self-\nchat approach to automatically evaluate our models.\nWe simulate dialogues between a tutor agent and a\nstudent agent. The tutor is the subject of the eval-\nuation, and uses controlled generation techniques\n(§4.3), while the student acts as the conversation\npartner. The Student’s role is to simulate a lan-\nguage learner at a specific proficiency level and is\nintentionally set up without using any explicit diffi-\nculty control mechanisms. Both Student and Tutor\nhave their own prompts that explain the scenario\nand instruct them to stay on topic.\nTo evaluate a given Tutor model’s ability we\nsimulate 15 dialogues per level—3 for each of the\n5 levels of the Japanese Language Proficiency Test\n(JLPT) N1 through N5 (see Table 1). For each level,\na set of three conversation topics are defined based\non the topics of official JLPT sample exams. Wedo this to test the tutor’s ability to control difficulty\nregardless of the student level or complexity of\nconversation topic.\n5.2 Results\nDifficulty Control In Table 3 we report the re-\nsults of our automatic evaluation. Echoing previous\nwork (Imperial et al., 2023), we see that prompt-\ning results in only modest improvements in Token\nMiss Rate (roughly −1.5%). We also see incon-\nsistent gains from the Overgenerate approach. In\nboth GPT-4 and Qwen models, overgenerate im-\nproves TMR over the baseline prompt but does not\nimprove ControlError. This underscores the im-\nportance of ensuring high quality candidates for\nre-ranking. Finally, we see that FUDGE consis-\ntently outperforms all other methods achieving an\n11.9% TMR with λ= 0.9which is equivalent to a\nroughly 88% token comprehension rate.\nFluency Across all metrics we see that increas-\ning the FUDGE control parameter λincreases the\n--- Page 6 ---\nN3 (A2 B1)\n16.7%\nN4 (A2)\n16.7%\nN5 (A1)\n66.7%Student Level Distribution\n0 hours\n37.5%\n0-1 hours\n25.0%1-3 hours\n37.5%Hours Speaking per WeekFigure 3: Distribution of JLPT / CEFR level of our\nstudy participants along with their self-reported average\nnumber of hours spoken per week.\ndifficulty control while keeping fluency metrics rel-\natively stable. We see that on both trigram diversity\n(div@3) as well as perplexity (Avg. PPL) FUDGE\nperforms comparatively well to other methods at\nhighλvalues. In our own testing we found that\nabove λ= 0.9the fluency of FUDGE begins to\ndegrade. For our human study we selected λ= 0.8\nas our comparison due to its perceived tradeoff\nbetween fluency and controllability.\n6 Human Evaluation\nIn this section we discuss the results of our human\nevaluation. We conducted an in-lab user study to\ncollect human judgments of the controllable gener-\nation methods we discuss in our work.\n6.1 Participants\nWe recruited 6 participants from Japanese language\ncourses at an R1 university in the United States.\nSince previous studies have struggled to include\nstudents of CEFR level A2 or lower (Tyen et al.,\n2024), we focus on lower levels of Japanese profi-\nciency. Instructors of beginner- and intermediate-\nlevel courses shared our study materials via course\nplatforms (e.g., Canvas) and email. To validate\nparticipants’ language proficiency, we asked them\nto report their current Japanese class. We mapped\ntheir course enrollment to a JLPT level based on\nthe syllabus and primary text, resulting in most par-\nticipant proficiency levels between JLPT N5 and\nN4 (CEFR A1-A2) (see Figure 3).\n6.2 Task Design\nEach session lasted about 45 minutes and took\nplace in a conference room on campus. After re-\nviewing the consent form, we informed the partici-\npant that they would be interacting with four lan-\nguage learning chatbots through a voice-based web\nFigure 4: The voice-based interface used for the human\nevaluation. Users clicked the microphone icon when\nthey wanted to speak and clicked again when finished.\ninterface. The participant then went through a three-\nturn tutorial conversation to become familiar with\nthe interface. Their spoken input was transcribed\nwith the API-based whisper-1 model (Radford\net al., 2022) and given as input to the backend\nmodel. The model’s response was read aloud with\naudio generated by the OpenAI TTS-1 model4. For\nconvenience, the text of the participant and model’s\nresponses were displayed on the screen.\nParticipants completed four 6-turn conversa-\ntions5, one for each of the controlled generation\nmethods ( Baseline ,Detailed ,Overgenerate ,\nand FUDGE ). For all four methods we used the\nQwen2.5-72B-Instruct model as the chatbot.\nThe order of the models was shuffled and hidden\nfrom participants, but the participants chose a new\nconversation topic each time from a list based on\ntheir JLPT level (see Appendix F.4).\n6.3 Data Collection\nPer-Turn Evaluation After each turn, partici-\npants were shown a transcript of the model’s previ-\nous response and asked to review it word by word.\nThey were instructed to highlight any spans of text\nthat they did not understand, which we later used\nto compute TMR. Participants were also asked to\n4http://platform.openai.com/docs/models/tts-1\n5In the case of major transcription errors affecting fluency\nor comprehension, participants were allowed to continue for\none additional turn.\n--- Page 7 ---\nbaseline detailed overgenfudge0%20%40%60%\n1234560%20%40%60%80%\nbaseline detailed overgenfudge0%5%10%15%\n1234565%10%15%20%25%\nPercentage of Rounds Not Understood Token Miss Rate (TMR)Figure 5: Results from the human evaluation for Baseline Prompt, Detailed Prompt, Overgenerate, and FUDGE. We\nsee that FUDGE consistently has the lowest TMR (bar) and stays consistently low across multiple rounds (line).\nHuman Evaluation Results\nModel %Rounds Not Understood ↓TMR ↓Understandable? ↑Effortful? ↓Natural? ↑\nBaseline (Qwen) 60.6 17.2 5.17 5.17 5.67\nDetailed (Qwen) 41.7 11.0 5.50 5.00 6.50\nOvergenerate (Qwen) 41.7 12.1 6.33 5.83 7.50\nFUDGE ( λ= 0.8) 16.7 8.0 7.67 4.67 7.33\nTable 4: Comparison of controlled generation approaches from the human evaluation using Qwen2.5-72B. We\nreport Token Miss Rate (TMR) as well as average 1-10 Likert scores from post-session evaluation. We also report\nthe percentage of rounds labelled as not understandable by the users. We see that high TMR is strongly correlated\nwith a round being not comprehensible to human subjects ( ρ= 0.78)\nreport whether they felt that they could understand\nthe overall meaning of the response.\nPer-Conversation Evaluation In addition to an-\nnotating each response, participants completed a\nbrief questionnaire after each conversation, con-\nsisting of the following 10-point Likert-scale ques-\ntions:\n1. How much of the bot could you understand?\n2. How effortful was it to talk to the bot?\n3.How comfortable did you feel talking to the bot?\n4.Did you feel like the bot’s responses were natu-\nral given the context of the conversation?\n5.Would you want to chat with this version of the\nbot again in the future?\nThe full text of the evaluation form can be found in\nAppendix F.\n6.4 Results\nIn Table 4 we report the aggregated results of our\nhuman evaluation. Interestingly we see that the\ndifference between FUDGE and other methods is\nmuch more pronounced in this setting. FUDGE\nachieves roughly half the TMR of the other meth-\nods and is the only method to come close to the oft-\ncited 95% comprehension threshold for full com-\nprehension (Laufer, 1992).\nWe see this difference reflected most stronglyin the Likert scale metrics (Figure 6). FUDGE\nwas rated as the easiest to understand (7.67) and as\ntaking the least amount of cognitive effort to speak\nwith (4.67). We also see that FUDGE maintains\na high degree of fluency, being rated the second\nmost natural sounding (7.33) after the overgenerate\nmethod (7.50).\nIn addition to these aggregate results, in Figure\n5 we show the plot of the TMR across subsequent\nround of conversation. Like Almasi et al. (2025)\nwe also find that uncontrolled models suffer from\n“alignment drift” —gradually straying more and\nmore from the target difficulty and getting higher\nTMR as a result. We find that FUDGE does a good\njob at maintaining a low TMR throughout multiple\nrounds of conversation and does not suffer from\nthis alignment drift issue to the extent that other\nmethods do. Future work should seek to further\nunderstand how these metrics change over time and\nhow to ensure consistency in difficulty control over\nlong conversations.\n7 Conclusion\nLearning a second language can open up significant\nopportunities. However, many beginner students\nfind it difficult to practice conversational speech\ndue to a lack of available conversation partners. In\nthis paper we demonstrate the feasibility of using\n--- Page 8 ---\nbaseline detailed overgen fudge2468How much of the bot could you\nunderstand(1-10)?\nbaseline detailed overgen fudge2468How effortful was it to talk to\nthe bot(1-10)?\nbaseline detailed overgen fudge2468Did you feel the bot's responses\nwere natural(1-10)?Figure 6: Our approach allows us to control the dif-\nficulty of the output of a language model to assist in\nlanguage learning using future discriminators Yang and\nKlein (2021). We show that this works better than\nprompting in both automatic and human evaluations\ncontrollable generation techniques along with state-\nof-the-art large language models as AI conversation\npartners for learning Japanese.\nThrough both human and automatic evaluations\nwe show that using controllable generation tech-\nniques from Yang and Klein (2021) allows us to\nsubstantially constrain the difficulty of chatbot re-\nsponses with over 83% of utterances rated as un-\nderstandable by early and late beginner speakers\n(JLPT N4-N5; CEFR A1-A2). We also show that\nthis result stays consistent across multiple rounds\nand that the outputs of the bot are natural.\nUnlike prior work, our proposed technique does\nnot require any fine-tuning of the base model—\nallowing for more personalized difficulty control.\nWe imagine a scenario where students each havetheir own personal on-device predictor models\ntrained to predict exactly their proficiency level.\nDuring conversation practice, these on-device pre-\ndictors can combine with logits sent from an API-\nbased chatbot for client-side difficulty control and\npersonalization. We believe that our results con-\nstitute a meaningful step towards democratizing\nlanguage learning and making conversation prac-\ntice more accessible to speakers of all levels.\nLimitations\nWe acknowledge that several sources of potential\nbias exist within this evaluation framework.\nDefinition of Difficulty It is hard to define an\none-off approach to measure the level of difficulty\nof an utterance. Difficulty can be based on V ocab-\nulary Level, Sentence Length, Grammatical Com-\nplexity, or Readability Scores, etc. Our criteria cho-\nsen or way of implementation may not perfectly\nalign with a target learner’s actual proficiency or\ncapture the full nuance of language difficulty. How-\never, despite this inherent subjectivity and potential\nmisalignment, defining measurable difficulty cri-\nteria is a necessary first step to enable any form\nof automated difficulty control or evaluation, even\nif the criteria themselves are imperfect representa-\ntions of human difficulty perception.\nUndetected Tokens The Token Miss Rate is com-\nputed as the ratio of tokens detected in \"above\"\nlevels to the total count of all tokens in the utter-\nance. This calculation implicitly classifies tokens\nthat were unable to be binned to a difficulty level\nas being understood by the learner. This can po-\ntentially skew the score for utterances with a high\nproportion of undetected tokens.\nAcknowledgments\nThis research is supported in part by the Office\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity (IARPA), via the HIATUS Program contract\n#2022-22072200005. The views and conclusions\ncontained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofficial policies, either expressed or implied, of\nODNI, IARPA, or the U.S. Government. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for governmental purposes notwith-\nstanding any copyright annotation therein.\n--- Page 9 ---\nReferences\nMina Almasi, Ross Deans Kristensen-McLachlan, and\net al. 2025. Alignment drift in cefr-prompted\nllms for interactive spanish tutoring. Preprint ,\narXiv:2505.08351.\nEman Alzaanin. 2023. Efl learners’ versus instructors’\nattributions of success and failure factors: A complex-\nity theory perspective. Language Teaching Research ,\n0(0):13621688231189777.\nMarco Baroni, Silvia Bernardini, Adriano Ferraresi, and\nEros Zanchetta. 2009. The wacky wide web: A col-\nlection of very large linguistically processed web-\ncrawled corpora. Language Resources and Evalua-\ntion, 43(3):209–226.\nEnrico Benedetti, Akiko Aizawa, and Florian Boudin.\n2024. Automatically suggesting diverse example\nsentences for L2 Japanese learners using pre-trained\nlanguage models. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 4: Student Research Workshop) ,\npages 114–131, Bangkok, Thailand. Association for\nComputational Linguistics.\nLuca Benedetto, Gabrielle Gaudeau, Andrew Caines,\nand Paula Buttery. 2025. Assessing how accurately\nlarge language models encode and apply the com-\nmon european framework of reference for languages.\nComputers and Education: Artificial Intelligence ,\n8:100353.\nAndrew Caines, Luca Benedetto, Shiva Taslimipoor,\nChristopher Davis, Yuan Gao, Øistein E. Andersen,\nZheng Yuan, Mark Elliott, Russell Moore, Christo-\npher Bryant, Marek Rei, Helen Yannakoudakis, An-\ndrew Mullooly, Diane Nicholls, and Paula Buttery.\n2023. On the application of large language mod-\nels for language teaching and assessment technology.\nInLLM@AIED , volume 3487 of CEUR Workshop\nProceedings , pages 173–197. CEUR-WS.org.\nChyyran. 2015. Jlpt anki decks. Accessed: 2025-05-17.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-\nBerliac, and 26 others. 2024. Aya expanse: Combin-\ning research breakthroughs for a new multilingual\nfrontier. Preprint , arXiv:2412.04261.\nTomaž Erjavec, Kristina Hmeljak Sangawa, and\nYoshiko Kawamura. 2008. Japanese web corpus with\ndifficulty levels jpWaC-l 1.0. Slovenian language\nresource repository CLARIN.SI.\nNaoaki Fujitani, Jiyo Tsubaki, Kazuhide Yamamoto,\nand Manabu Okumura. 2012. Jreadability: A tool for\nassessing japanese text readability. In Proceedings of\nthe 24th International Conference on Computational\nLinguistics (COLING 2012): Demonstration Papers ,\npages 103–110.Dominik Glandorf and Detmar Meurers. 2024. Towards\nfine-grained pedagogical control over English gram-\nmar complexity in educational text generation. In\nProceedings of the 19th Workshop on Innovative Use\nof NLP for Building Educational Applications (BEA\n2024) , pages 299–308, Mexico City, Mexico. Associ-\nation for Computational Linguistics.\nKotaro Hayashi and Takeshi Sato. 2024. The effec-\ntiveness of chatgpt in enhancing english language\nproficiency and reducing second language anxiety\n(l2). In WorldCALL Official Conference Proceedings ,\npages 201–208.\nJoseph Marvin Imperial, Harish Tayyar Madabushi, and\net al. 2023. Flesch or fumble? evaluating readabil-\nity standard alignment of instruction-tuned language\nmodels. In Proceedings of the Third Workshop on\nNatural Language Generation, Evaluation, and Met-\nrics (GEM) , pages 205–223, Singapore. Association\nfor Computational Linguistics.\nHenri Jamet, Maxime Manderlier, Yash Raj Shrestha,\nand Michalis Vlachos. 2024. Evaluation and simpli-\nfication of text difficulty using llms in the context of\nrecommending texts in french to facilitate language\nlearning. In Proceedings of the 18th ACM Confer-\nence on Recommender Systems , RecSys ’24, page\n987–992, New York, NY , USA. Association for Com-\nputing Machinery.\nJapan Foundation. 2024. Japanese-Language Profi-\nciency Test: Test Guide. Accessed: 2025-05-17.\nJapan Foundation. 2025. Indication of the CEFR Level\nfor Reference. Accessed: 2025-05-17.\nJ. P. Kincaid, R. P. Fishburne, R. L. Rogers, and B. S.\nChissom. 1975. Derivation of new readability for-\nmulas (automated readability index, fog count and\nflesch reading ease formula) for navy enlisted per-\nsonnel. Technical Report Research Branch Report\n8-75, Naval Technical Training Command, Naval Air\nStation Memphis, Millington, TN.\nStephen D. Krashen. 1985. The Input Hypothesis: Is-\nsues and Implications . Longman, New York.\nBatia Laufer. 1992. How Much Lexis is Necessary for\nReading Comprehension? , pages 126–132. Palgrave\nMacmillan UK, London.\nSeungjun Lee, Yoonna Jang, Chanjun Park, Jungseob\nLee, Jaehyung Seo, Hyeonseok Moon, Sugyeong\nEo, Seounghoon Lee, Bernardo Yahya, and Heuiseok\nLim. 2023. PEEP-talk: A situational dialogue-based\nchatbot for English education. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 3: System Demonstra-\ntions) , pages 190–207, Toronto, Canada. Association\nfor Computational Linguistics.\nAli Malik, Stephen Mayhew, Christopher Piech, and\nKlinton Bicknell. 2024. From tarzan to Tolkien:\nControlling the language proficiency level of LLMs\n--- Page 10 ---\nfor content generation. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2024 , pages\n15670–15693, Bangkok, Thailand. Association for\nComputational Linguistics.\nYuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,\nYoshitaka Hirano, Hideki Matsuda, Kazutoshi\nTakaoka, and Masayuki Asahara. 2000. Japanese\nmorphological analysis system chasen version 2.0\nmanual. In Proceedings of the Workshop on Usage\nof Language Resources at the Third International\nConference on Language Resources and Evaluation\n(LREC) , Athens, Greece. European Language Re-\nsources Association (ELRA).\nNaoaki Okazaki, Sakae Mizuki, Youmi Ma, Koki\nMaeda, Kakeru Hattori, Masanari Ohi, Hi-\nnari Shimada, Taihei Shiotani, Koshiro Saito,\nRio Yokota, Kazuki Fujii, Taishi Nakamura,\nTakumi Okamoto, Ishida Shigeki, Yukito\nTajima, Masaki Kawamura, and Hiroya Taka-\nmura. 2024. Swallow project leaderboard.\nhttps://swallow-llm.github.io/evaluation/\nindex.en.html?index=%22__ALL__%22 .\nGitHub repository available at https:\n//github.com/swallow-llm/leaderboard .\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, and\n262 others. 2024. Gpt-4 technical report. Preprint ,\narXiv:2303.08774.\nMostafa Papi and Hassan Khajavy. 2023. Second lan-\nguage anxiety: Construct, effects, and sources. An-\nnual Review of Applied Linguistics , 43:127–139.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan\nLin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, and 25 oth-\ners. 2025. Qwen2.5 technical report. Preprint ,\narXiv:2412.15115.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision. arXiv preprint .\nReski Ramadhani, Hilmi Aulawi, and Risma Liyana\nUlfa. 2023. Readability of reading texts as authentic\nmaterials issued by chatgpt: A systemic functional\nperspective. Indonesian Journal of English Lan-\nguage Teaching and Applied Linguistics , 8(2):149–\n168.\nKevin Stowe, Debanjan Ghosh, and Mengxuan Zhao.\n2022. Controlled language generation for language\nlearning items. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing: Industry Track , pages 294–305, Abu Dhabi,\nUAE. Association for Computational Linguistics.Kazuma Takaoka, Sorami Hisamoto, Noriko Kawahara,\nMiho Sakamoto, Yoshitaka Uchida, and Yuji Mat-\nsumoto. 2018. Sudachi: a japanese tokenizer for busi-\nness. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018) , Paris, France. European Language Re-\nsources Association (ELRA).\nYu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-\nLin Chen, Chao-Wei Huang, Yu Meng, and Yun-\nNung Chen. 2024. Two tales of persona in LLMs: A\nsurvey of role-playing and personalization. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2024 , pages 16612–16631, Miami, Florida,\nUSA. Association for Computational Linguistics.\nGladys Tyen, Mark Brenchley, Andrew Caines, and\nPaula Buttery. 2022. Towards an open-domain chat-\nbot for language practice. In Proceedings of the 17th\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications (BEA 2022) , pages 234–249,\nSeattle, Washington. Association for Computational\nLinguistics.\nGladys Tyen, Andrew Caines, and Paula Buttery. 2024.\nLLM chatbots as a language practice tool: a user\nstudy. In Proceedings of the 13th Workshop on Nat-\nural Language Processing for Computer Assisted\nLanguage Learning , pages 235–247, Rennes, France.\nLiU Electronic Press.\nSatoru Uchida. 2025. Generative ai and cefr lev-\nels: Evaluating the accuracy of text generation with\nchatgpt-4o through textual features. Vocabulary\nLearning and Instruction , 14(1):2078.\nYing Wang. 2025. A study on the efficacy of chatgpt-4\nin enhancing students’ english communication skills.\nSAGE Open , 15(1):21582440241310644.\nBenjamin Warner, Antoine Chaffin, Benjamin Clavié,\nOrion Weller, Oskar Hallström, Said Taghadouini,\nAlexis Gallagher, Raja Biswas, Faisal Ladhak, Tom\nAarsen, Nathan Cooper, Griffin Adams, Jeremy\nHoward, and Iacopo Poli. 2024. Smarter, better,\nfaster, longer: A modern bidirectional encoder for\nfast, memory efficient, and long context finetuning\nand inference. Preprint , arXiv:2412.13663.\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3511–3535, Online. Association for Computational\nLinguistics.\nZhihui Zhang and Xiaomeng Huang. 2024. The im-\npact of chatbots based on large language models on\nsecond language vocabulary acquisition. Heliyon ,\n10(3):e25370.\nAndrew Zhu, Liam Dugan, Alyssa Hwang, and Chris\nCallison-Burch. 2023. Kani: A lightweight and\nhighly hackable framework for building language\n--- Page 11 ---\nHyper-parameter Value\nper_device_train_batch_size 16\nper_device_eval_batch_size 16\nnum_train_epochs 3\nlearning_rate 5×10−5\nevaluation_strategy epoch\nlogging_steps 10\ndataloader_num_workers 4\nTable 5: Hyper -parameters used to train the FUDGE\npredictor model.\nmodel applications. In Proceedings of the 3rd Work-\nshop for Natural Language Processing Open Source\nSoftware (NLP-OSS 2023) , pages 65–77, Singapore.\nAssociation for Computational Linguistics.\nA Hyperparameters & Configurations\nThis appendix outlines the training configuration\nused for our FUDGE predictor model. We fine-\ntune the answerdotai/ModernBERT-base trans-\nformer on sentence-level JLPT annotations, using\na weighted sampling strategy to address class im-\nbalance. All relevant hyperparameters and settings\nare summarized in Figure 5.\nB Implementation Details\nIn this section we will go over more details about\nhow our multi-modal AI tutor was constructed.\nKani To build our chatbot we use the Kani6\nframework, which is a flexible abstraction for build-\ning multi-turn conversational agents in Python (Zhu\net al., 2023). It provides lightweight abstractions\naround LLM engines ( BaseEngine ), message for-\nmats ( ChatMessage ,ChatRole ), and generation\nmanagement ( Completion ). This allows us to eas-\nily swap between local and API based chat models\nwithout changing our interface code. In addition,\nwe implement the controllable generation using the\nHuggingFace LogitsProcessor interface which\nallows us to use our predictor model with any valid\nHuggingface decoder-only language model.\nHardware For inference we used six NVIDIA\nRTX A6000 GPUs queried via local server through\na web socket. We allocated one machine to the\npredictor model and the other five machines to host\nthe 72 billion parameter Qwen model. For cases\nduring “self-chat” where a student Qwen agent was\n6https://github.com/zhudotexe/kanichatting with a tutor Qwen agent, we loaded one\ninstance of the model and swapped out the chat\ncontext as necessary.\nB.1 Heuristic Re–Ranking for the\nOvergenerate Method\nThe Overgenerate method follows a gener-\nate–then–rank paradigm. Given a user prompt,\nthe underlying HF model (GPT-4 / Qwen) first pro-\nduces N=5candidate continuations.\nB.1.1 Vocabulary binning for Overgenerate.\nStep 1: Tokenization. For each collection of sen-\ntences belonging to a certain JLPT level in the\njpWaC dataset, we tokenize every line by using\nSudachiPy in coarse-grained mode C. Tokenization\nincludes lemmatization and punctuation removal,\nand we additionally filter for strings consisting en-\ntirely of Japanese script (kanji, hiragana, katakana)\nusing a Unicode whitelist.\nStep 2: Frequency Aggregation. We keep track\nof two frequencies globally:\n•how often each token appears in that level’s\nsubcorpus\n• per-level token occurrences across all levels\nStep 3: Rare Word Filtering. To eliminate spuri-\nous and rare tokens, we apply two threshold filters:\n•Global frequency filter: words must appear\nmore than 10−6of the total tokens across all\nlevels;\n•Level-specific filter: words must appear more\nthan10−6of the tokens within their level.\nThese thresholds eliminate noise and ensure that\nbin assignments are based on consistent patterns.\nStep 4: Best-Level Assignment. We assign each\ntoken wto the easiest level in which it reaches\na sufficient relative frequency. That is, for each\ntoken, we scan levels N5 →N1 in order and assign\nit to the first level where its frequency is non-trivial\nrelative to that level’s total token count:\nscore w,ℓ=count w,ℓ\ntotal tokens ℓ\nThe first level ℓfor which score w,ℓexceeds a small\nthreshold is selected as the word’s bin.\nStep 5: Extracting Unique Bag-of-Words. To\nreduce overlap between levels, we construct dis-\njoint vocabulary bins by progressively subtracting\nalready-assigned tokens from lower levels. This\nresults in a set of per-level vocabulary lists where\neach word appears only in one level.\n--- Page 12 ---\nJLPT N5 36563\nJLPT N4 103298\nJLPT N3 372421\nJLPT N2 137312\nJLPT N1 2627335\nTotal 3276929\nTable 6: Total number of sentences per JLPT level in\nthe jpWaC corpus.\nOutputs. Each final vocabulary bin is written as\na plain-text file named {level}_bagofwords.txt ,\nwhich contains one token per line. These lists are\nused in the Overgenerate engine for detecting level-\nappropriate vs. above-level vocabulary in gener-\nated responses.\nRe-ranking. We evaluate every candidate in par-\nallel threads. Candidates are first sorted by their\nTMR (ascending) and then, to break ties, by total\ntoken count (shorter is preferred). The top-ranked\ncontinuation is returned to the user and logged to-\ngether with its diagnostic statistics.\nC Extra Dataset Details\nIn this section we provide additional details about\nthe datasets used in the project for training and\nevaluating the system.\nC.1 jpWaC\nThe corpus was collected using WaCkY tools (Ba-\nroni et al., 2009), lemmatized and POS-tagged via\nChasen (Matsumoto et al., 2000), with tags mapped\nto both English and Japanese tagsets. Each sen-\ntence includes a @level attribute indicating overall\ndifficulty, enabling fine-grained control over in-\nput complexity for language modeling tasks. This\ndataset is used as the sentence and vocabulary pool\nof the Japanese language for our system, used for\ntraining our FUDGE predictor, as well as for the\nprompting and overgeneration baseline methods\nmentioned in Section 4.3. The dataset’s distribu-\ntion of sentences across different JLPT levels is\nshown in Table 6.\nD JLPT Vocabulary Bins\nD.1 Anki Deck Parsing\nTo construct vocabulary bins grouped by JLPT\nlevel, we extract and normalize vocabulary en-\ntries from (Chyyran, 2015), originally providedin the form of ‘.apkg’ files. These decks are orga-\nnized by JLPT level (N5–N1) and contain Japanese\nexpressions, readings, and glosses. Our parsing\nprocess extracts useful vocabulary while handling\nvariations in formatting and ambiguity in expres-\nsion/reading pairs.\nInput Format. Each JLPT level deck is imported\ninto Anki and stored as a local SQLite file under\n‘collection.anki2’. Each vocabulary entry is stored\nas a row in the ‘notes’ table, where the ‘flds’ field\ncontains tab-separated content fields, including: -\nExpression (kanji, kana, or mixed), - Reading (op-\ntional, typically kana), - Gloss/Meaning (in En-\nglish).\nParsing Overview. We use a Python script to:\n1. Traverse the anki deck for each JLPT level,\n2. Parse entries from the ‘notes’ table,\n3.Clean and normalize parentheses and tilde\nvariations in expressions/readings,\n4.Expand expressions and readings that include\nalternations or parenthetical annotations,\n5.Perform heuristic-based filtering to exclude\nduplicates or overly ambiguous entries,\n6.Generate one flat JSON vocabulary file per\nJLPT level, saved to disk for later use.\nNormalization Heuristics. We apply several nor-\nmalization steps:\n•Japanese parentheses are first converted to\nstandard parentheses ( and ).\n•Tilde-like characters (e.g., 〜) are stripped\nentirely, as they denote alternation or ellipsis.\n•Parenthetical expressions (e.g., (する ),\n（を）) are expanded using two strategies:\n–The outside form (e.g., 話す(こと ) be-\ncomes話す)\n–The full form with parenthetical inserted\n(e.g.,話すこと )\n•Readings are similarly expanded, and\nmatched against expressions using filters to\navoid over-generation.\n--- Page 13 ---\nintroduce yourself (such as your name, job/school, where you’re from, etc.)\nJLPT N5 describe what you usually do in the morning and evening\ntalk about your favorite food and where you usually eat it\nexplain what you will do this weekend and with whom\nJLPT N4 describe your favorite hobby and how often you do it\ntalk about a typical day at school or work, including schedule and people you meet\ndescribe a travel experience: where you went, what you saw, and who you went with\nJLPT N3 talk about planning a birthday party: location, food, and guests\ndescribe your favorite movie: the story, characters, and why you like it\ndescribe a recent news story you found interesting, and why it caught your attention\nJLPT N2 explain one cultural difference between Japan and your country, and how it affects communication\ndiscuss a challenge people face when communicating in a Japanese workplace\ndiscuss recent advancements in regenerative medicine and their ethical implications in Japan\nJLPT N1 explain the role of quantum computing in future communication technologies and how Japan is preparing for it\nanalyze the impact of declining biodiversity on Japan’s agricultural sustainability and food security\nTable 7: The full list of conversation topics used in our self-chat evaluation pipeline (see Section 5.1)\nYou are roleplaying as a student learning {language} at the {level_word} level .\nYou are having a conversation with your language partner (i.e. the user ) to practice\n{language}.\nThe topic of this conversation is: {topic}.\nAs a {level_word} student , you are : {desc}.\nYou must speak using only the vocabulary and grammar allowed at this level .\nYou are not in a formal class - this is casual language practice with someone your\nage .\nYou should ALWAYS follow the rules below :\n1. You should stick to using only the vocabulary and grammar allowed at your level\nmentioned above .\n2. Do not ask the user to teach you things . Just bring up the topic naturally and\ncontinue the conversation .\n3. Your conversation should revolve around the topic of: {topic}. Respond one idea at\na time .\n4. You must keep the conversation going . Do not assume the conversation is over just\nbecause a few turns have passed .\n5. DO NOT say anything like 'goodbye ','see you next time ', or anything else that\nsignals the end of this conversation . You MUST keep the conversation going .\n6. You should speak in {language} and {language} only .\nFigure 7: System prompt defining the Student model.\nFiltering Strategy. The script includes several\nregular expression-based heuristics to exclude non-\ninformative entries:\n•Skip readings that are only one hiragana char-\nacter (e.g.,の), or extremely short forms un-\nlikely to be useful alone.\n•Avoid adding a reading as a standalone entry\nif it overlaps directly with another alternative\nexpression.\n•Skip duplicates, malformed entries, or entries\nmissing either expression or meaning.\nOutput Format. The result is a set of per-level\nJSON files, e.g., n5.json , each containing normal-\nized entries in the form:{\n\"会う\": {\n\" meaning \": \"to meet , to see \"\n},\n\"あなた \": {\n\" meaning \": \" you \"\n},\n...\n}\nThese files are used downstream for populating\nvocabulary bins, level-appropriate conversation\nprompts, and difficulty evaluation.\nIn Table 8, we show a representative sample of\nvocabulary items used at each JLPT level from N5\n(easiest) to N1 (hardest), along with their English\nglosses.\n--- Page 14 ---\nYou are a {language} language tutor .\nYour goal is to help the user improve their {language} conversation skills through a\nnatural , back -and - forth dialogue .\nYou are a native {language} speaker , around the same age as the user , and you 're\nacting as their language partner .\nThe user you are speaking with is at the {level_word} level .\nPlease be aware of the user 's level at all times and ensure that all of your\nresponses stay within a level that is understandable to a user at this\nproficiency .\nStick to the topic the user brings up. Do not suggest topics or introduce new topics\non your own .\nStay on the user 's topic and follow their lead throughout the conversation .\nDon't pick on small mistakes the user makes . If the user makes a really big grammar\nmistake , remind the user by saying the corrected version of the sentence . DO NOT\ntry to explain their mistake .\nYou should keep the conversation going back and forth .\nYou must never say things like 'goodbye ','see you tomorrow ', or anything else that\nsignals the end of the conversation unless the user initiates it.\nYou should speak in {language} and {language} only .\nFigure 8: System prompt defining the Tutor model.\nJLPT N5 明日(tomorrow),あなた (you),魚(fish),いいえ (no, not at all),少し(little, few), 有名 (famous)\n先生(teacher, professor; master; doctor), 有る(to be, to have),会う(to meet, to see),行く(to go)\nJLPT N4生きる (to live),心配(worry, concern),始める (to start, to begin), 止める (to end, to stop)\n中学校(junior high school), 会話(conversation),そろそろ (gradually, soon),専門(major; speciality)\nJLPT N3 様々(varied, various),全て(all, the whole, entirely), 集まり (gathering, meeting, collection)\n印象(impression),作品(work, opus, production), わざと (on purpose), 変化(change, variation, shift)\nJLPT N2重力(gravity),純粋 (pure, genuine, unmixed), 先祖 (ancestor), 課税(taxation),清い(clear, pure, noble)\n強化(strengthen, intensify, reinforce), 論ずる (to argue, to discuss), 必需品(necessities, essential)\nJLPT N1 賢明(wisdom, intelligence, prudence), 倹約(thrift, economy, frugality), 鉱業(mining industry)\n護衛(guard, convoy, escort), 戸籍(census, family register), 臆病(cowardice, timidity), 放射能(radioactivity)\nTable 8: Example vocabulary at each JLPT level.\nE Web Interface\nParticipants engaged with a custom-built web inter-\nface for the human evaluation. Figure 10 presents\nthe stages of preparation for each round of user\nstudy, including navigating from the homepage (a),\nselecting the corresponding JLPT level of the user\n(b), navigating to the chat interface (c), and select-\ning which method to run on (d). We complete all\nsetup before each round before we hand the laptop\nto the participant to begin their conversation with\nthe bot.\nF Human Study Details\nF.1 Intake Form\nAfter signing up and prior to their user study ses-\nsion, each participant completed an intake form.\nThis form collected language background, class en-\nrollment, and JLPT experience, along with confirm-\ning informed consent for participation and record-\ning. The full content of the intake and consent formis shown in Figure 11.\nF.2 Mapping of Textbook to JLPT level\nTo estimate a participant’s JLPT level, we mapped\ntheir current university Japanese course (as self-\nreported) to an approximate JLPT level based on\ntextbook progression and kanji coverage. Table 9\nsummarizes this mapping, which uses both Genki\nand Tobira textbooks as primary anchors for N5\nthrough N3 levels, and more advanced materials\nsuch as news articles to the N2 level.\nF.3 Annotation Interface\nWithin a user study session, the participant has a\ntotal of four full conversations of six rounds back-\nand-forth. After each conversation, participants\nwere asked to review the system’s output and anno-\ntate any words or phrases they could not understand.\nThey were also asked to indicate whether they un-\nderstood the overall meaning of the round. At each\nround, the system’s generation is sent to an iPad\n--- Page 15 ---\nYou are a {language} language tutor .\nYour goal is to help the user improve their {language} conversation skills through a\nnatural , back -and - forth dialogue .\nYou are a native {language} speaker , around the same age as the user , and you 're\nacting as their language partner .\nThe user you are speaking with is at the {level_word} level .\nThis means that they : {level_description}.\nAn example of a short dialogue at the user 's comprehension level is:\n{level_conv_example}\nPlease be aware of the user 's level at all times and ensure that all of your\nresponses stay within a level that is understandable to a user at this\nproficiency .\nYou should ALWAYS follow the rules below :\n1. {level_guidelines}\n2. Remember , the user is a language learner , not a native speaker . You should make\nsure that you are speaking in a way that the user could understand with their\ncurrent {language} level .\n3. You should try to match the user 's abilities of understanding and speaking : if\nthe user only uses simple expressions , you should only use simple expressions as\nwell .\n4. During the conversation , don 't pick on small mistakes the user makes . If the user\nmakes a really big grammar mistake , remind the user by saying the corrected\nversion of the sentence . DO NOT try to explain their mistake .\n5. Stick to the topic the user brings up. Do not suggest topics or introduce new\ntopics on your own . Stay on the user 's topic and follow their lead throughout\nthe conversation .\n6. You should keep the conversation going back and forth .\n7. You must never say things like 'goodbye ','see you tomorrow ', or anything else\nthat signals the end of the conversation unless the user initiates it.\n8. You should speak in {language} and {language} only .\n9. Here are some expressions the user knows : {known_expressions}. Restrict your\nspeaking to use these words and other words of similar or lower difficulty .\nFigure 9: System prompt defining the Tutor model (detailed prompt).\nGenki I ,Genki II (Lesson 13-Lesson 14) JLPT N5\nGenki II ,Tobira: Gateway to Advanced Japanese (Unit 1–Unit 2) JLPT N4\nTobira: Gateway to Advanced Japanese (Unit 9-Unit 14) JLPT N3\nAdvanced materials selected from the internet, newspapers, and books JLPT N2\nTable 9: Mapping from textbooks used in university Japanese courses to their corresponding JLPT levels.\nby email for prompt highlighting after each and\nevery round. As shown in Figure 12, red highlights\ndenote individual words and phrases marked as in-\ncomprehensible, while green or red dots indicate\nbinary comprehension of the round as a whole.\nF.4 Conversation Topics\nBased on each participant’s most recent Japanese\ncourse and their estimated JLPT level, we compile\na list of level-appropriate conversation topics uti-\nlizing the corresponding textbooks of each level\n(see Figure 13). Before each round, the system ran-\ndomly selects three topics from the corresponding\nlist and presents them for the participant to choose\nfrom.G Use of AI Assistants\nAI assistants were used in writing this paper to en-\nhance clarity of wording, format tables and figures,\nand help fix compilation errors. The models used\nwere ChatGPT and GPT-o3.\n--- Page 16 ---\n(a) The homepage of the interface used for the human evalua-\ntion. For the purpose of this study, we only use the Conversa-\ntion Mode.\n(b) We select the user’s corresponding level through a drop-\ndown.\n(c) The user interacts with the main chat interface , which is\ninitially blank with a recording button.\n(d) We select the method through a dropdown before each\nround; we also set show scripts to true so that the user can\nread the text output of the bot.\nFigure 10: Preparation on interface for each user study conversation round: Homepage (a), Level-Selection (b),\nChat Interface (c), and Method-Selection (d).\n--- Page 17 ---\nIntro:\nThank you again for signing up to participate in our study !\nThis is a small - scale user study , so your participation truly means a lot to us.\nThe information you provide here will help us schedule your session , accommodate\nyour preferences , and better understand your language background .\nAll responses will be kept confidential and used solely for research purposes .\nWe are collecting your email only to coordinate your session and follow up if needed\n. It will not be shared or used for any other purpose .\nInformed Consent:\n1. Informed Consent Statement :\nThis study involves interacting with an AI - powered chatbot in Japanese . While we 've\ndesigned it to be level - appropriate ,\nthe chatbot may occasionally provide inaccurate or confusing responses .\nYour interactions will be recorded and anonymized for research purposes .\nParticipation is voluntary , and you may withdraw at any time without penalty .\nAll data collected will be kept confidential and used solely for research purposes .\nNo personally identifiable information will be shared outside the research team .\nAudio from your session will be used to analyze the interaction and will be deleted\nafter it is transcribed and anonymized .\nConsent Confirmation :\nBy continuing , you confirm that :\n- You are at least 18 years old\n- You have read and understood the information above\n- You voluntarily consent to participate in this study\n[ ] I consent to participate in this study under the terms described above .\nParticipant Background:\n1. Did you grow up speaking or hearing Japanese at home as a child ?\n[ Yes / No ]\n2. Approximately how many hours per week do you spend speaking Japanese\nconversationally outside of class ?\n[ Text response ]\n3. Approximately how many hours per week do you spend listening to conversational\nJapanese ( podcasts , etc .) outside of class ?\n[ Text response ]\n4. If you 're currently taking a Japanese class at the university , which class are\nyou enrolled in?\n[ Text response ]\n5. How many semesters of Japanese study have you completed ?\n[ Text response ]\n6. Have you taken the Japanese Language Proficiency Test ( JLPT )?\n[ Yes / No ]\nIf yes , what level did you achieve ?\n[ N5 / N4 / N3 / N2 / N1 ]\nIf not , what level do you estimate you 're at ( based on textbook , course , or self -\nassessment )?\n[ N5 / N4 / N3 / N2 / N1 ]\nFigure 11: Full content of the form used to collect intake and consent data in the study.\n--- Page 18 ---\n    Words / Phrases participant couldn’t understand Could Understand Round: True \nCould Understand Round: False Figure 12: The annotation interface used during human evaluation. After each round, participants were asked\nto highlight specific words or phrases they did not understand (shown in red). They also indicated whether they\nunderstood the overall meaning of the round: a green dot denotes comprehension, and a red dot denotes lack of\nunderstanding.\n--- Page 19 ---\nJLPT N5 Topics List:\n- Summer vacation plans\n- Weekend hobbies or routines\n- Favorite movie or TV show\n- Favorite book , story , or folklore\n- Favorite sport or physical activity\n- A memorable trip or vacation\n- A time you got sick\n- A favorite holiday or festival\nJLPT N4 / Early N3 Topics List:\n- A time something was stolen\n- A time you were hurt or injured\n- Doing house chores\n- A habit that annoys you\n- A time you reported a crime or accident\n- A favor you asked from someone\n- A time you had to say goodbye\n- A promise or decision you made\n- A future goal that you have\n- A region in Japan you want to visit\n- A famous place you 've been to\n- A local food or specialty you like\n- A festival you 've attended or want to see\n- Your hometown and what it 's known for\n- A memorable travel story\n- A seasonal event you enjoy\n- A travel recommendation for a friend\nJLPT N2 Topics List:\n- Describe a recent news story you found interesting , and why it caught your\nattention\n- Explain one cultural difference between Japan and your country , and how it affects\ncommunication\n- Discuss a challenge people face when communicating in a Japanese workplace\n- Talk about a social issue you care about and why it 's important to you\n- Describe a time you had to be polite in a difficult situation\n- Compare education systems in Japan and your home country\n- Share your opinion on using AI or technology in daily life\n- Describe a tradition or custom from your country and how it 's changing\n- Talk about how your communication style changes depending on the situation\n- Discuss the pros and cons of working remotely or studying online\n- Talk about a piece of Japanese literature you like\n- Discuss how Japanese society is addressing the social issue of aging population\nFigure 13: Conversation topics participants chose from, grouped by estimated corresponding JLPT level.",
  "text_length": 63747
}