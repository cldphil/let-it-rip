{
  "id": "http://arxiv.org/abs/2506.04220v1",
  "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large\n  Multimodal Models",
  "summary": "Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts\noften rely on explicit 3D inputs or specialized model architectures, we ask:\ncan LMMs reason about 3D space using only structured 2D representations derived\nfrom perception? We introduce Struct2D, a perception-guided prompting framework\nthat combines bird's-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs\n(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning\nabilities when provided with structured 2D inputs, effectively handling tasks\nsuch as relative direction estimation and route planning. Building on these\ninsights, we construct Struct2D-Set, a large-scale instruction tuning dataset\nwith 200K fine-grained QA pairs across eight spatial reasoning categories,\ngenerated automatically from 3D indoor scenes. We fine-tune an open-source LMM\n(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple\nbenchmarks, including 3D question answering, dense captioning, and object\ngrounding. Our approach demonstrates that structured 2D inputs can effectively\nbridge perception and language reasoning in LMMs-without requiring explicit 3D\nrepresentations as input. We will release both our code and dataset to support\nfuture research.",
  "authors": [
    "Fangrui Zhu",
    "Hanhui Wang",
    "Yiming Xie",
    "Jing Gu",
    "Tianye Ding",
    "Jianwei Yang",
    "Huaizu Jiang"
  ],
  "published": "2025-06-04T17:58:04Z",
  "updated": "2025-06-04T17:58:04Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04220v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04220v1  [cs.CV]  4 Jun 2025Struct2D: A Perception-Guided Framework for\nSpatial Reasoning in Large Multimodal Models\nFangrui Zhu1∗, Hanhui Wang3∗, Yiming Xie1, Jing Gu4, Tianye Ding1\nJianwei Yang2, Huaizu Jiang1\n1Northeastern University2Microsoft Research\n3University of Southern California4University of California, Santa Cruz\n1{zhu.fang, xie.yim, ding.tian, h.jiang}@northeastern.edu ,\n2jw2.yang@gmail.com ,3hanhuiwa@usc.edu ,4jgu110@ucsc.edu\nhttps://github.com/neu-vi/struct2d\nLMM<image>  Object marks are shown in bird’s eye view image. Metadata: 1: sofa: [-2.4, 3.0, -0.5]… <image> < image> <image> Selected keyframes are given. QuesMon: If I am standing by the stove and facing the tv, is the sofa to my front-leO, front-right, back-leO, or back-right? <think> … step-by-step <answer> front right<answer>\nRelaMve  DirecMonRoute  PlanningSize  EsMmaMonObject CounMng3D Scene QA 3D Dense CapMoning 3D Object GroundingSpaMal Reasoning, Numerical PredicMon\n0%12.5%25%37.5%50%\nRel. DirecMonRoute PlanObj. CountAbs. Distance\n72B3BEnables spa.al reasoning across mul.ple tasks\nAccuracy on VSI-Bench Tasks\n…\nRGB-D Video InputMetadata (3D prior)  1: sofa: [-2.4, 3.0, -0.5] 2: tv: [-3.5, 0.5, 0.2] 3: stove: [-0.7, -0.9, -0.1]\nBEV Image w/ MarksKeyframes (opMonal)Obtain structured 2D inputs via percep.on module\nStruct2D….\nFigure 1: Overview of our Struct2D framework for enabling spatial reasoning in large multi-\nmodal models (LMMs). From an RGB-D video, we generate structured 2D inputs—BEV images\nwith filtered object marks, object-centric metadata, and optional keyframes—via a 3D perception\nmodule. These inputs prompt an LMM with spatial priors and visual context, enabling diverse spatial\nreasoning tasks without explicit 3D input at inference.\nAbstract\nUnlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts often\nrely on explicit 3D inputs or specialized model architectures, we ask: can LMMs\nreason about 3D space using only structured 2D representations derived from\nperception? We introduce Struct2D , a perception-guided prompting framework\nthat combines bird’s-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs ( e.g.,\nGPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities\nwhen provided with structured 2D inputs, effectively handling tasks such as relative\ndirection estimation and route planning. Building on these insights, we construct\n∗Equal contribution.\nPreprint. Under review.\n--- Page 2 ---\nStruct2D-Set , a large-scale instruction tuning dataset with 200K fine-grained QA\npairs across eight spatial reasoning categories, generated automatically from 3D\nindoor scenes. We fine-tune an open-source LMM (Qwen2.5VL) on Struct2D-Set,\nachieving competitive performance on multiple benchmarks, including 3D question\nanswering, dense captioning, and object grounding. Our approach demonstrates\nthat structured 2D inputs can effectively bridge perception and language reasoning\nin LMMs-without requiring explicit 3D representations as input. We will release\nboth our code and dataset to support future research.\n1 Introduction\nUnderstanding objects and their spatial relationships in 3D space is a cornerstone of intelligent inter-\naction in complex physical environments. Tasks such as robotic manipulation [ 37,66], autonomous\nnavigation [ 25,55], and visual reasoning [ 3,10,14,52,81,100] all depend on accurate spatial under-\nstanding of scenes. At the core of these tasks lies the ability to localize objects precisely and reason\nabout their configurations in 3D space. Moreover, grounding such spatial understanding in natural\nlanguage enhances an AI system’s ability to interpret, explain, and act upon spatial information in\nhuman-centric contexts.\nTraditional task-specific models rely on explicit 3D representations as input, such as point clouds\nor reconstructed environments [ 3,33,52,101], providing detailed geometric information. However,\nthese models, often trained on limited data sources, making them less adaptable and struggle to\ngeneralize to diverse and complex textual queries. As a result, they fail to effectively bridge spatial\nreasoning with language comprehension, limiting their applicability for embodied AI.\nIn recent years, Large Multimodal Models (LMMs) [ 24,39,59,95] developed with Large Language\nModels (LLMs) have achieved significant advances in perception and reasoning tasks for images and\nvideos. To extend LMMs’ capabilities to 3D understanding, point cloud-based LLMs [ 13,23,26,27,\n30,53,60,61,72,78] have emerged, incorporating 3D spatial features by aligning point cloud data\nwith LLMs. This integration enhances spatial reasoning and provides a richer understanding of the\n3D physical world. However, they often rely on well-annotated datasets for instruction tuning and\nrequire point-cloud features as input, which limits their flexibility.\nUnlike models that take explicit 3D representations as input, humans perceive the world as a\ncontinuous stream of 2D visual inputs akin to a video , and naturally infer spatial relationships\nand object configurations by building mental representations subconsciously [ 57,69]. Naturally,\nwe ask “ Can LMMs perform spatial reasoning without using explicit 3D features as direct inputs? ”\nRecent work has begun to explore this direction by leveraging cognitive maps [ 81] and Bird’s Eye\nView (BEV) images [ 62] generated from video as 2D spatial cues, enabling LMMs to perform spatial\nreasoning [ 80,84]. While promising, these approaches often omit object appearance and detailed\npriors ( e.g., coordinates, categories), which are critical for comprehensive 3D understanding.\nWe conduct an in-depth analysis of LMMs’ spatial reasoning abilities using a perception-guided 2D\nframework called Struct2D Prompting. This strategy transforms 3D perception outputs—obtained\nfrom off-the-shelf detectors—into structured 2D inputs, consisting of (1) a rendered bird’s-eye-view\n(BEV) image with projected object marks2and (2) object-centric metadata such as category labels and\n3D coordinates. When appearance cues are needed, we optionally incorporate egocentric keyframes\nselected based on object visibility. This design enables LMMs to reason about complex 3D scenes\nusing only structured 2D visual and textual cues, eliminating the need for explicit 3D inputs. We\nbegin by evaluating this approach on GPT-o3 [ 59], a representative closed-source LMM, to assess its\nzero-shot spatial reasoning capabilities.\nTo better understand the spatial reasoning capabilities of existing LMMs, we begin with a zero-\nshot analysis using our proposed Struct2D Prompting strategy. The goal is to evaluate whether a\npretrained, closed-source model such as GPT-o3 can accurately infer 3D spatial relationships when\ngiven only structured 2D visual and textual inputs. We use rendered bird’s-eye-view (BEV) images\nwith projected object marks and object-centric metadata, allowing the model to reason about 3D\nscenes without access to explicit 3D features. This analysis yields several key insights: (1) A single\ninformative BEV image, combined with metadata, is often sufficient for accurate zero-shot 3D scene\n2We follow the term “mark” as used in [80].\n2\n--- Page 3 ---\nunderstanding; (2) Prompt composition is critical—different spatial reasoning tasks benefit from\ntailored input formats; (3) For challenging tasks in VSI-Bench [ 81], such as egocentric-to-allocentric\ntransformations, LMMs can perform robustly when provided with well-structured 2D projections of\nthe 3D scene.\nGuided by the findings from our zero-shot analysis, we construct a large-scale instructional tuning\ndataset, named Struct2D-Set , using an automated pipeline. The dataset consists of 200K QA pairs\ngenerated from 6K 3D indoor scenes, leveraging ground-truth object annotations provided by the\noriginal 3D datasets. It spans eight categories of spatial reasoning tasks relevant to embodied AI.\nTo ensure data quality, we use ChatGPT to both enrich the QA pairs with step-by-step reasoning\ntraces and identify potentially low-quality samples. Additionally, we incorporate a human-in-the-loop\nreview process to further refine and validate the dataset. We then fine-tune an open-source LMM\n(Qwen2.5VL [ 71]) using Struct2D-Set . Although the fine-tuned model is evaluated under noisy 3D\nperception conditions, it achieves strong performance across multiple spatial reasoning benchmarks,\nincluding 3D question answering [ 3,52,81], spatial captioning [ 14], and object grounding [ 10,94],\ndemonstrating the practicality and robustness of our approach.\nOur main contributions are as follows:\n•We propose a perception-guided 2D prompting strategy, Struct2D Prompting, and conduct a\ndetailed zero-shot analysis that reveals LMMs’ ability to perform 3D spatial reasoning from\nstructured 2D inputs alone.\n•We introduce Struct2D-Set , a large-scale instructional tuning dataset with automatically gener-\nated, fine-grained QA pairs covering eight spatial reasoning categories grounded in 3D scenes.\n•We fine-tune an open-source LMM to achieve competitive performance across several spatial\nreasoning benchmarks, validating the real-world applicability of our framework.\n2 Related Work\n3D Spatial Reasoning with MLLMs. Developing real-world embodied agents requires equipping\nLarge Multimodal Models (LMMs) with robust 3D spatial reasoning abilities [ 8,9,16,41,47,82,\n98]. Recent efforts have explored spatial understanding through language [ 56,64,76], static 2D\nimages [ 51,54,63,68,80], or videos [ 44,62,81]. Our work builds upon the video-input setting,\nbut diverges by enabling spatial reasoning in LMMs using only structured 2D inputs—BEV images,\nobject marks, and metadata—without relying on explicit 3D encoder / representations at inference.\nInstruction Tuning for 3D Spatial Reasoning. Recent work [ 13,34,42,43,44] has explored\ninstruction tuning to enhance LMMs’ capabilities for 3D spatial reasoning, targeting tasks such\nas 3D visual grounding [ 2,10,94], 3D dense captioning [ 14], and 3D question answering [ 3,\n52]. M3DBench [ 42] provides region- and scene-level instruction-response pairs for general 3D\nunderstanding, while 3DMIT [ 43] focuses on scene-centric instructions. LL3DA [ 13] supports\ninteractive planning and reasoning across omni-3D inputs. Robin3D [ 34] introduces a 3D LLM trained\non diverse instruction-following examples. R1-Zero-VSI [ 44] proposes a video-based instruction\ntuning dataset and a GRPO-based training method to enhance spatial reasoning in Qwen2-VL,\nyet its QA pairs involve limited reasoning complexity, cover fewer task types, and yield marginal\nperformance gains. In contrast, we propose Struct2D-Set , a large-scale dataset that enables open-\nsource LMMs to acquire rich 3D spatial reasoning skills through instruction tuning—using only\nstructured 2D representations, without requiring direct access to 3D point clouds.\n3D Point Cloud LLMs. Recent advances in 3D point cloud LLMs enable natural language generation\nand interaction grounded in 3D geometry by directly processing point clouds as input. These models\nbenefit from the geometric precision and texture richness of point clouds, offering stronger spatial\nunderstanding than raw image or video inputs. Prior work has focused on object-level [ 26,60,61,78]\nand scene-level [ 13,23,27,30,53,72] spatial reasoning. However, directly using point cloud\nfeatures requires additional training and infrastructure, limiting flexibility and scalability in real-\nworld applications.\nPrompting LLMs. Despite the rapid scaling of large language models (LLMs)[ 1,5,17,22,32,70,\n92], their reasoning capabilities remain heavily dependent on effective prompt design. In-context\nlearning[ 5,21], which conditions models on a few representative examples, has become a widely\nadopted technique for improving instruction-following behavior. To further enhance reasoning,\n3\n--- Page 4 ---\n…\nObject-centric Metadata 11: sofa: [-2.4,3.0,-0.5]  4: stove: [-0.7, 0.9, -0.1]  18: tv: [-3.8, 0.8, 0.5]\n(4) Prompt Construc0onFollow the steps below to complete the task accurately: 1. Draw Line A from the point where I am standing to the object I am facing. 2. Draw Line B, which is perpendicular to Line A and intersects it at my standing posiPon. 3. To determine the object's relaPve posiPon in the quesPon: Check which side of Line B the object is on. If the object is on the same side as the object I am facing, it is in the front; otherwise, it is in the back. 4. Next, determine the object's posiPon relaPve to Line A: If it is to the leX of Line A, it is on the leX. If it is to the right of Line A, it is on the right. LeX and right should always be determined based on the direcPon in which I am facing.Guide Prompt(1) Input: egocentric video(2) Percep0on module Point cloud reconstrucPon 3D object detecPon(3) BEV Genera0onRotated BEV Image\nLine ALine BQues%on: If I am standing by the sofa and facing the stove, is the tv to my le9, right, or back?Figure 2: Illustration of Struct2D prompting. Given an egocentric video, we first reconstruct\na point cloud and detect 3D objects. A bird’s-eye-view (BEV) image is rendered and drawn with\nobject marks related with the question. To facilitate reasoning about relative directions, the BEV is\nrotated to align with the agent’s facing direction. We further construct object-centric metadata and a\nstructured guide prompt to support the model in understanding spatial relationships between objects.\nstrategies such as chain-of-thought [ 73] and tree-of-thought [ 85] prompting have been proposed.\nMore recently, large multimodal models (LMMs)[ 15,20,24,39,40,45,49,59,79,90,91,95,99]\nhave gained prominence for their ability to reason over multiple input modalities. This has led to\na surge of research into prompting techniques tailored for LMMs[ 7,16,28,29,38,46,48,58,67,\n74,75,77,80,83,84,88,93,97]. Building on this direction, we propose Struct2D , a structured\n2D prompting strategy that enables LMMs to perform 3D spatial reasoning effectively—without\nrequiring explicit 3D input representations.\n3 Analysis on Struct2D prompting with GPT-o3\n3.1 Struct2D Prompting\nGiven a video Vas input, an LMM Fprocesses a set of Nsampled video frames, denoted as\nI={I1, I2, . . . , I N}, where each frame Inhas dimensions RH×W×3forn∈ {1, ..., N}. Alongside\nvisual input, the LMM receives a text query of length li, represented as Tin= [ti\n1, . . . , ti\nli]. The\nmodel then generates a textual response of length lo, denoted as Tout= [to\n1, . . . , to\nlo], formulated as:\nTout=F(I,Tin). (1)\nHowever, directly using video frames for spatial reasoning introduces two major limitations: (1)\nIncomplete perception — Video frames are typically sampled sparsely and from limited viewpoints,\nwhich can result in missing critical visual evidence required for spatial understanding. For instance,\nconsider a scene where a chair is tucked partially under a table. If most sampled frames are taken from\nfrontal views or from a standing height, the chair’s presence might be obscured or entirely invisible,\nleading the model to incorrectly assume there is empty space beneath the table. This limitation\nbecomes more severe in cluttered or occluded environments, where small objects or those blocked\nby other furniture. (2) Lack of global context — Video frames offer fragmented, egocentric views\nthat often fail to capture the overall structure of the scene. For example, determining whether a lamp\nis closer to the couch or the bookshelf may be impossible if the two objects never co-occur in the\nsame frame. Without a consistent top-down or holistic representation, the model must rely on spatial\nmemory or reasoning across disjoint perspectives—an ability that remains weak in most LMMs. This\nfragmentation also impedes the understanding of traversability ( e.g., identifying a clear path from the\ndoor to the kitchen) or relational queries ( e.g., which chair is directly behind the dining table).\nTo address these issues, Struct2D incorporates a perception module ϕpercept that extracts point clouds\nPand object detections Ofrom the input video V. We then generate a top-down bird’s-eye-view\nimage with filtered object marks—only including objects relevant to the question, as illustrated in\nFigure 2. Additionally, we construct object-centric metadata Tmeta(e.g., categories, coordinates) as\n4\n--- Page 5 ---\nTable 1: Zero-shot evaluation of GPT-o3 on the VSI-Bench subset. The first row simply uses 16\nframes from the input video, proposed in VSI-Bench [ 81]. For our prompting, we only input a BEV\nimage with object marks on it along with object-centric meta information.\nSettings # images Cost ($) Avg.Numerical Answer Multiple-Choice Answer\nObj. Count Abs. Dist. Room Size Rel. Dist. Rel. Dir. Route Plan\nVSI-Bench [81] 16 105.07 48.6 44.3 34.1 50.9 51.0 49.4 61.9\nGPT4Scene [62] 9 78.67 50.3 51.5 35.3 58.0 50.5 47.9 58.8\nOurs (Noisy Objects) 1 27.25 56.1 52.8 38.4 48.9 60.0 60.1 76.2\nOurs (GT Objects) 1 27.25 83.8 93.8 90.6 47.4 96.5 94.4 80.1\ntextual input to guide spatial reasoning. Formally, we redefine Eq. 1 as:\nTout=F(Struct2D (ϕpercept(V),Tmeta),Tin). (2)\nFor questions requiring appearance or depth cues ( e.g., object color or size), we supplement the BEV\nview with selected egocentric keyframes Ikeyframe that capture clear views of the relevant objects.\nInstead of uniformly sampling keyframes, we use 3D projections to select frames that better capture\nthe spatial coverage of the scene. The full formulation becomes:\nTout=F(Struct2D (ϕpercept(V),Tmeta,Ikeyframe ),Tin). (3)\nThis formulation illustrates how Struct2D leverages 3D perception as an intermediate step to\ngenerate informative 2D inputs that preserve spatial structure. Although 3D point clouds are used\nduring preprocessing, they are not directly provided to the LMM. Instead, they are transformed into\nBEV images and metadata used for prompting. As a result, the model performs spatial reasoning\neffectively without requiring explicit 3D representations as input.\nFigure 3: Distribution of question types in\nthe selected VSI-Bench subset. This follows\nthe distribution of the full set.Evaluation Setup. We sample questions from VSI-\nBench [ 81], which is designed to evaluate complex\nspatial reasoning skills. Compared to traditional\n3D QA datasets [ 3,52], VSI-Bench covers more\nfine-grained object perception requirements, intri-\ncate global spatial relationships, and egocentric-to-\nallocentric transformations. It also features diverse\nindoor scene sources and robust evaluation metrics\nthat go beyond rule-based NLP scoring.\nComparison to GPT4Scene Prompting [ 62].While\nGPT4Scene pioneered 2D spatial prompting using\nBEV images, our Struct2D strategy introduces sev-\neral key improvements: (1) Filtered object marks tai-\nlored to the query improve visual relevance and re-\nduce distraction; (2) Guided metadata prompts pro-\nvide additional spatial priors; (3) Keyframe selection\nis optimized using depth-aware 3D projection instead of uniform sampling, making them both fewer\nand more informative.\n3.2 Zero-shot Analysis of Struct2D Prompting\nWe construct a subset of 422 QA pairs for evaluation, selected due to API call budgets. As shown\nin Figure 3, the distribution of question types is consistent with the full benchmark. For our\nanalysis, we generate object marks using both ground-truth 3D annotations and noisy detections\n(following [ 30,62]), ensuring comprehensive object coverage while eliminating perception errors.\nThis also enables a fair comparison with prior work, particularly [62].\nZero-shot Prompting Results. Table 1 shows that GPT-o3 exhibits strong spatial reasoning capabili-\nties when prompted with structured 2D inputs. Specifically, providing both object-centric metadata\nand filtered object marks significantly boosts performance, achieving 96.5 on relative distance, 94.4\non relative direction, and 80.1 on route planning. This highlights that explicit 3D representations\nare not strictly necessary—LMMs can reason effectively with carefully structured 2D projections.\nThe ablation further reveals that rotation alignment and a structured guide prompt each contribute\n5\n--- Page 6 ---\nto improved accuracy on relative direction tasks, with the combination of both yielding the best\nperformance (94.4). These results underscore the importance of aligning spatial context and guiding\nthe model through geometric reasoning steps. Notably, our method requires only a single BEV image\nand lightweight metadata, making it a low-cost and robust alternative to multi-frame prompting\nstrategies [62, 81].\nTable 2: Ablation on different prompting strategies.\nMetadataFiltered\nMarksRel. Dist. Rel. Dir. Route Plan\n– – 67.5 82.1 74.3\n– ✓ 72.1 88.3 78.3\n✓ – 75.3 89.5 50.6\n✓ ✓ 96.5 94.4 80.1\n(a)Effects of metadata and filtered marks.Guide\nPromptRotation Rel. Dir.\n– – 75.3\n– ✓ 89.2\n✓ – 80.2\n✓ ✓ 94.4\n(b)Effects of rotation and guide prompt.\nWhat makes a good prompt for spatial reasoning? Table 2 highlights the impact of key components\nin our prompting strategy. Incorporating object-centric metadata consistently improves performance\nacross tasks—raising relative distance accuracy from 67.5 to 96.5 and route planning from 74.3 to\n80.1—highlighting its importance for grounding spatial context. Filtering object marks based on\nquestion relevance further reduces ambiguity, yielding substantial gains in route planning (from 50.6\nto 80.1). For relative direction, both the use of a structured guide prompt and rotation alignment prove\nessential. While each individually improves accuracy (89.2 and 80.2 respectively), their combination\nleads to the best performance (94.4). We focus on these question types in ablation because they\nrepresent core challenges in spatial understanding.\n4 Large-Scale Instruction Tuning with Struct2D-Set\nBuilding on the insights from our zero-shot analysis (Sec. 3), we construct a large-scale instruction\ntuning dataset, Struct2D-Set , tailored to support diverse spatial reasoning tasks grounded in\nrealistic 3D indoor scenes. Notably, the dataset is designed to require only 2D projected inputs during\ntraining, enabling efficient supervision without reliance on full 3D data.\nIn this section, we first describe the design and construction of Struct2D-Set , highlighting its\ncoverage, annotation pipeline, and task diversity. We then present the supervised fine-tuning (SFT)\nsetup using open-source LMMs, detailing the model configurations and training procedures. Fi-\nnally, we evaluate the effectiveness of our instruction-tuned model across multiple spatial reasoning\nbenchmarks, assessing its generalization and reasoning capabilities.\n4.1 Struct2D-Set\nFigure 4: Distribution of QA types in Struct2D-Set .The\ndataset covers a diverse range of spatial reasoning skills, with\na focus on spatial relationships and localization tasks that\nrequire strong geometric understanding.Overview. Struct2D-Set consists\nof 200K QA pairs generated from over\n6K richly annotated indoor scenes,\nsourced from large-scale 3D recon-\nstruction datasets—ARKitScenes[ 4],\nScanNet [ 19], and ScanNet++[ 87].\nThese datasets capture diverse real-\nworld environments, including homes,\noffices, and industrial settings. The\nQA pairs cover eight categories of\nspatial reasoning tasks. Figure 4\nshows the distribution of question\ntypes across the dataset.\nConstruction pipeline. We generate\ntwo types of QA pairs to support both\n6\n--- Page 7 ---\nObject A )ribute \nIden 0ﬁca0on\n1\nQ: What is the color of the table  in front \nof the couch?\nA: The table in front of the couch is blue.\nSpa0al Rela 0on \n(Rela 0ve)\nQ: If I am standing next to the oven  and \nfacing the sofa, is the chair  in front-left, \nfront-right, back-left, or back-right?\nAugmented answer: The chair is located \nin the back-right position relative to the \noven ( ID 6) and the sofa ( ID 11 ), as it is \nsituated towards the lower right \nquadrant when facing north along the \npositive y-axis.Short answer: back-right.\nObject Coun 0ng\n1\nQ: How many green pillows  are there \non the bed?\nA: There are 3 green pillows placed \non the head of the bed.\nQ: Which of the following objects \n(toilet, bathtub, sink ) is closest to the \nwasher  when measuring from their \nnearest points?\nAugmented answer: The closest object \nto the washer is the toilet, with \ncoordinates [0.25, 1.27, -0.09] , which \nis approximately 0.54 units away from \nthe washer at [0.79, 1.35, -0.07] , while \nthe other objects are farther away.Compara 0ve \nReasoning\nShort answer: Toilet.\nQ: You are a robot beginning at the white \ntoilet  and facing the black  toilet paper \nholder . You want to navigate to the brown  \ndoor. You will perform the following actions \n(Note: for each [please ﬁll in], choose either \n'turn back,' 'turn left,' 'turn right,' or 'pass \nby.'): 1. [please ﬁll in] 2. Go forward until the \nwall 3. [please ﬁll in] 4. Go forward until the \nwall 5. [please ﬁll in] 6. Go forward until the \nwhite shower curtain  7. [please ﬁll in] 8. Go \nforward until the brown door . You have \nreached the ﬁnal destination.\nAugmented answer: The robot is now at the \nwhite toilet, facing the black toilet paper \nholder. The next object, the wall, is to its \nright, so it should turn right. Now, the robot \nis facing the wall. The next object, another \nwall, is directly ahead, so it should turn right \nagain. The robot is now facing the white \nshower curtain. The next object, the shower \ncurtain, is directly ahead, so it should pass \nby. Continuing forward, the robot now faces \nthe brown door. The next object, the door, is \ndirectly ahead, so it should turn right. The \nrobot is now at the brown door, having \nreached its ﬁnal destination.Short answer: \"turn right”, \"turn right”, \"pass \nby”, \"turn right”. \nEgocentric Naviga 0on / \nOrienta 0on\nFigure 5: QA examples of Struct2D-Set .Examples cover diverse spatial reasoning tasks,\nincluding object attributes, counting, relative positioning, navigation, and comparative reasoning.\nEach QA pair includes a short answer from 3D geometry and an augmented answer with detailed\nreasoning generated by ChatGPT.\nspatial reasoning and scene understanding tasks. Each type encompasses multiple subtypes targeting\ndistinct reasoning skills. Representative examples from both types are shown in Figure 5.\nThe first type, inspired by VSI-Bench [ 81], involves tasks that require understanding global spatial\nrelationships in 3D, such as spatial relation identification, egocentric navigation, and comparative\nreasoning. These questions cannot be answered from a single keyframe alone. We begin by extracting\nground-truth object annotations from the training sets of the 3D datasets, including object boxes,\ndepth maps, and camera poses. Using structured templates, we generate initial QA pairs based on this\nmeta information, and then enrich them using ChatGPT to produce step-by-step reasoning traces and\nmore natural language formulations. Each QA pair includes a short answer derived from geometry\ntemplates and a long-form answer elaborating on the reasoning process.\nThe second type of QA pairs is adapted from existing 3D scene understanding benchmarks, including\nScanQA [ 3], SQA3D [ 52], Scan2Cap [ 14], ScanRefer [ 10], and Multi3DRefer [ 94]. These examples\ncover tasks such as object attribute identification, counting, and binary verification. We augment\nthe original training set questions and descriptions using ChatGPT to improve clarity and reasoning\ndepth. These tasks typically benefit from selecting keyframes where relevant objects are clearly\nvisible, allowing the model to ground spatial reasoning in egocentric frames.\n4.2 Experiment Setup\nWe fine-tune the open-source LMM Qwen2.5VL [ 71] using our proposed dataset, Struct2D-Set .\nFor evaluation, we primarily focus on VSI-Bench [ 81], which includes complex spatial reasoning tasks\nsuch as relative direction and route planning. Additionally, we assess model performance on three\nstandard 3D scene understanding tasks built on ScanNet [ 19]: 3D question answering (ScanQA [ 3],\n7\n--- Page 8 ---\nTable 3: Performance comparison of various models on VSI-Bench [ 81].The model fine-tuned\nwith Struct2D-Set surpasses both the Struct2D prompting and the video-based tuning baseline.\nMethods Avg.Numerical Answer Multiple-Choice Answer\nObj. Count Abs. Dist. Room Size Obj. Size Rel. Dist. Rel. Dir. Route Plan\nOpen-source Models\nInternVL2-2B [15] 30.3 21.8 24.9 35.0 22.0 33.8 44.2 30.5\nInternVL2-8B [15] 33.9 23.1 28.7 39.8 48.2 36.7 30.7 29.9\nLongVILA-8B [79] 21.1 29.1 9.1 0.0 16.7 29.6 30.7 32.5\nVILA-1.5-8B [45] 29.5 17.4 21.8 18.8 50.3 32.1 34.8 31.0\nLongV A-7B [90] 31.1 38.0 16.6 22.2 38.9 33.1 43.3 25.4\nLLaV A-NeXT-Video-7B [95] 36.3 48.5 14.0 24.2 47.8 43.5 42.4 34.0\nLLaV A-OneVision-0.5B [39] 31.2 46.1 28.4 28.3 15.4 28.9 36.9 34.5\nLLaV A-OneVision-7B [39] 33.5 47.7 20.2 12.3 47.4 42.5 35.2 29.4\nR1-Zero-VSI [44] (Qwen2-VL-7B) 32.1 39.4 25.0 43.2 25.8 32.6 30.9 27.8\nR1-Zero-VSI [44] (Qwen2-VL-7B) + SFT 38.8 44.7 27.6 50.4 46.1 34.0 35.7 33.0\nOurs\nQwen2.5-VL-3B 25.6 27.0 22.0 25.6 32.5 17.5 28.9 25.6\nQwen2.5-VL-3B ( Struct2D Prompting ) 29.4 46.6 24.6 22.3 33.6 21.2 30.5 27.2\nQwen2.5-VL-3B (Baseline) 33.9 24.6 34.0 46.4 53.5 21.2 30.5 27.2\nQwen2.5-VL-3B (SFT) 41.9 46.0 34.7 42.6 56.4 35.1 44.9 33.5\nSQA3D [ 52]), 3D dense captioning (Scan2Cap [ 14]), and 3D visual grounding (ScanRefer [ 10],\nMulti3DRef [ 94]). For VSI-Bench, we input only BEV images with filtered object marks and\nmetadata, as the tasks focus purely on spatial relationships. For the other benchmarks, which often\ninvolve object attributes or visual details, we additionally provide selected egocentric keyframes to\nsupport reasoning.\n4.3 Implementation Details\nWe adopt Qwen2.5VL [ 71] as our base LMM for instruction tuning. During training, the model\nreceives BEV images with filtered object marks and object-centric metadata. For tasks that require\nappearance or attribute information (e.g., object color or count), we additionally provide egocentric\nkeyframes. All visual inputs are resized to 480×480, and object marks are adaptively scaled based\non their original resolution.\nFor questions involving complex spatial reasoning, such as relative direction or route planning, we\ninsert special tokens <think> and</think> to guide the model to generate a step-by-step reasoning\nprocess, followed by the final answer enclosed within <answer> and</answer> . For simpler\nquestions involving object appearance or quantitative estimation, the model is trained to directly\nproduce short answers without reasoning traces. We train the model for one epoch using a base\nlearning rate of 2e-6 with cosine annealing. Training takes approximately 8 hours on 4 ×H200 GPUs.\nFor evaluation, we follow [ 30,62] by reconstructing point clouds offline using BundleFusion [ 18],\ndetecting 3D object boxes with Mask3D and UniDet, and projecting them into BEV images and 2D\nobject marks.\n4.4 Main results\nWe present quantitative results on VSI-Bench[ 81] in Table 3 and on ScanQA[ 3] and SQA3 [ 52] in\nTable 4. Additional benchmark results are provided in the Appendix due to space limitations.\nAs shown in Table 3, our model fine-tuned with the Struct2D-Set dataset achieves the highest\naverage score (41.9) among all open-source models evaluated on VSI-Bench. Notably, it surpasses\nboth the Struct2D prompting variant (29.4) and the standard baseline trained with uniformly sampled\n16 video frames (33.9), confirming the effectiveness of our full instruction tuning approach. The\nperformance gains are especially prominent on spatial reasoning tasks like relative direction (44.9) and\nroute planning (33.5), where the model must integrate both geometric understanding and egocentric\ncontext. Compared with R1-Zero-VSI [ 44] (38.8), a recent method that trains Qwen2-VL-7B using\nvideo-based supervision, our tuned model not only achieves stronger average performance but also\nuses fewer visual frames and does not rely on dense temporal input. These results highlight the\nscalability and efficiency of Struct2D-Set for training capable spatial reasoners without explicit\n3D features at inference.\nTable 4 shows results on two traditional 3D question answering benchmarks, ScanQA and SQA3D.\nOur model outperforms most existing methods, including several that rely on explicit 3D point cloud\n8\n--- Page 9 ---\nTable 4: 3D Question Answering Evaluation on ScanQA [3] and SQA3D [52] datasets.\nMethodsScanQA(val) SQA3D(val)\nBLEU-1 BLEU-4 METEOR ROUGE CIDEr EM-1 EM-R1\nTask-Specific Model\nScanQA [3] 30.2 10.1 13.1 33.3 64.9 – –\nSQ3D [52] – – – – – 46.6 –\n3D-VLP [33] 30.5 11.2 13.5 34.5 – – –\n3D-Vista [101] – – 13.9 35.7 – 48.5 –\n3D LLM Based Model\nChat-3D [72] 29.1 6.4 11.9 28.5 53.2 – –\nChat-3D v2 [30] 38.4 7.3 16.1 40.1 77.1 – –\n3D-LLM [27] 39.3 12.0 14.5 37.3 69.4 – –\nLL3DA [13] – 13.5 15.9 37.3 76.8 – –\nPQ3D [102] – – – – – 47.1 –\nLEO [31] – 11.5 16.2 39.3 80.0 50.0 50.0\nChat-Scene [30] 43.2 14.3 18.0 41.6 87.7 54.6 57.5\nVision LLM Based Model\nInternVL-2-8B [15] 23.9 3.3 14.5 34.3 62.5 33.0 45.3\nMiniCPM-V-2.6 [86] 25.1 8.4 11.8 31.5 60.1 42.6 46.6\nQwen2-VL-7B (GPT4Scene) 43.4 14.6 17.7 43.6 90.9 57.4 60.7\nQwen2.5-VL-7B (Ours) 45.2 15.8 17.4 44.1 92.1 58.5 61.3\nTable 5: Ablation on different variants. To save computational resource, models are trained with\nQwen2.5VL-3B model by default.\nSettings Avg.Numerical Answer Multiple-Choice Answer\nObj. Count Abs. Dist. Room Size Rel. Dist. Rel. Dir. Route Plan\nTuning Data Format\nwo/ augmented QA 31.5 43.7 33.1 34.1 32.1 14.7 31.5\nw/ augmented QA 38.0 44.4 33.6 41.5 33.3 42.2 33.0\nEvaluation Strategy\nwo/</think> 36.2 44.1 33.6 41.5 33.3 38.6 26.3\nw/</think> 36.1 44.4 30.0 35.6 31.5 42.2 33.0\ninputs. Compared with GPT4Scene [62], our model performs on par across most metrics. However,\nthese benchmarks primarily require identifying relevant keyframes and generating free-form textual\nanswers. As a result, models can often rely on memorizing object-level attributes, and the rule-based\nevaluation metrics ( e.g., BLEU, CIDEr) may not fully reflect the correctness or reasoning quality of\nthe generated answers. Please refer to Appendix for qualitative results.\nAblation Study To better understand the impact of individual components in our framework, we\nconduct a series of ablation studies using the Qwen2.5VL-3B model for efficiency, as shown in\nTable 5. First, we evaluate the effect of QA augmentation. Incorporating enriched QA pairs generated\nwith ChatGPT leads to a substantial improvement in overall performance (Avg: 38.0 vs. 31.5),\nespecially on reasoning-heavy tasks such as relative direction (42.2 vs. 14.7). This supports our\nearlier claim that step-by-step reasoning traces help guide the model’s attention and inference. We\nfurther assess the role of explicit reasoning supervision using the <think> and<answer> tokens.\nWhile the average scores are comparable, including <think> tokens improves performance on\nreasoning-intensive tasks like relative direction and route planning (42.2 vs. 38.6 and 33.0 vs. 26.3,\nrespectively), indicating that instructing the model to explicitly reason can enhance output quality on\ncomplex spatial questions. Figure 6 illustrates step-by-step reasoning generated using our <think>\nand<answer> prompting format in egocentric navigation tasks. In each case, the model is prompted\nto analyze object positions and spatial transitions from a top-down BEV image with object markers.\nThese examples highlight the model’s ability to decompose spatial reasoning tasks into interpretable\nsteps and generate structured answers grounded in visual context.\n5 Limitations and future work\nWhile Struct2D demonstrates strong spatial reasoning capabilities with structured 2D inputs, there\nremain areas where future work could extend its applicability:\n9\n--- Page 10 ---\n6\n29\n15Here are object labels and ids: table:2, sofa:6, tv:9, table:15  in the bird's eye \nview image. If I am standing by the tv and facing the sofa, is the table to my \nfront-le B, front-right, back-le B, or back-right? Here are the op Fons: ['A. front-\nright', 'B. front-le B', 'C. back-le B', 'D. back-right’]. Please think step by step and \nenclose your reasoning in <think> </think>  tags and then provide the short \nanswer with only the op Fon le Ser in  <answer> </answer> . \n<think> The table, iden Fﬁed as marker ID 2, is located in the front-right \nquadrant rela Fve to my posi Fon by the TV (marker ID 9) while facing the \nsofa (marker ID 6). </think>   \n111126\n16\n28\n89Here are object labels and ids: window :28, monitor:111, door:89, monitor:126, \nmonitor:16  in the bird's eye view image. You are a robot beginning at the door \nand facing the window. You want to navigate to the monitor. You will perform \nthe following ac Fons (Note: for each [please ﬁll in], choose either 'turn back,' \n'turn le B,' or 'turn right.'): 1. Go forward un Fl the heater 2. [please ﬁll in] 3. Go \nforward un Fl the monitor. You have reached the ﬁnal des FnaFon. Here are the \nopFons: ['A. Turn Le B', 'B. Turn Back', 'C. Turn Right’]. Please think step by step \nand enclose your reasoning in <think> </think>  tags and then provide the short \nanswer with only the op Fon le Ser in <answer> </answer>.\n<think> The robot is now at the door[89], facing the window [28]. The next object, the heater, is straight \nahead, so it should go forward. Upon reaching the heater, the robot is s Fll facing the window. The next \nobject, the monitor, is to its right, so it should turn right. Now, the robot is facing the monitor. It goes \nforward un Fl it reaches the monitor, which is the ﬁnal des FnaFon. </think>  \n                   \n<answer>  A </answer>  Model’s reasoning steps:\nFinal predic Fon:\nModel’s reasoning steps:\nFinal predic Fon: <answer>  A </answer> Figure 6: Example reasoning traces using <think> and<answer> prompting. The top example\nshows relative direction reasoning, where the model infers the spatial relation between objects from a\nfixed viewpoint in the BEV image. The bottom example shows step-by-step planning, requiring the\nmodel to simulate orientation changes across a sequence of waypoints.\n•3D preprocessing requirements. Although Struct2D does not use 3D features during infer-\nence, it currently relies on 3D perception modules to generate BEV images and object-centric\nmetadata. This may pose a challenge in latency-sensitive or resource-constrained environments.\nHowever, since Struct2D is agnostic to the specific perception backbone, it can readily integrate\nwith ongoing advances in real-time and lightweight 3D reconstruction systems.\n•Indoor scene focus. The current version of Struct2D-Set is constructed from over 6K richly\nannotated indoor scenes, including homes, offices, and classrooms. While this enables detailed\nreasoning in structured environments, generalization to outdoor or open-world scenes remains\nless explored. Incorporating diverse spatial layouts and object categories from outdoor domains\nis a promising direction for future dataset expansion.\n6 Conclusion\nWe present Struct2D , a perception-guided framework that enables LMMs to perform 3D spatial\nreasoning using structured 2D inputs. Through zero-shot analysis and instruction tuning, we show that\nBEV images, object-centric metadata, and keyframes are sufficient to unlock strong spatial reasoning\ncapabilities—without requiring explicit 3D inputs. Our curated dataset, Struct2D-Set , supports\nscalable instruction tuning with fine-grained QA pairs grounded in real 3D scenes. Fine-tuning with\nStruct2D-Set yields significant gains across spatial reasoning benchmarks, outperforming prior\nopen-source methods even under noisy perception. These findings demonstrate that structured 2D\nprojections are a practical and effective alternative to direct 3D representations, offering a scalable\npath toward robust multimodal spatial understanding in LMMs.\n10\n--- Page 11 ---\nReferences\n[1]Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\n[2]Achlioptas, P., Abdelreheem, A., Xia, F., Elhoseiny, M., and Guibas, L. (2020). Referit3d: Neural\nlisteners for fine-grained 3d object identification in real-world scenes. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16 ,\npages 422–440. Springer.\n[3]Azuma, D., Miyanishi, T., Kurita, S., and Kawanabe, M. (2022). Scanqa: 3d question answering\nfor spatial scene understanding. In CVPR .\n[4]Baruch, G., Chen, Z., Dehghan, A., Dimry, T., Feigin, Y ., Fu, P., Gebauer, T., Joffe, B., Kurz,\nD., Schwartz, A., et al. (2021). Arkitscenes: A diverse real-world dataset for 3d indoor scene\nunderstanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897 .\n[5]Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances\nin neural information processing systems ,33, 1877–1901.\n[6]Cai, D., Zhao, L., Zhang, J., Sheng, L., and Xu, D. (2022). 3djcg: A unified framework for joint\ndense captioning and visual grounding on 3d point clouds. In CVPR .\n[7]Cai, M., Liu, H., Mustikovela, S. K., Meyer, G. P., Chai, Y ., Park, D., and Lee, Y . J. (2024a).\nVip-llava: Making large multimodal models understand arbitrary visual prompts. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12914–12923.\n[8]Cai, W., Ponomarenko, I., Yuan, J., Li, X., Yang, W., Dong, H., and Zhao, B. (2024b). Spatialbot:\nPrecise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642 .\n[9]Chen, B., Xu, Z., Kirmani, S., Ichter, B., Sadigh, D., Guibas, L., and Xia, F. (2024a). Spatialvlm:\nEndowing vision-language models with spatial reasoning capabilities. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14455–14465.\n[10] Chen, D. Z., Chang, A. X., and Nießner, M. (2020). Scanrefer: 3d object localization in\nrgb-d scans using natural language. In European conference on computer vision , pages 202–221.\nSpringer.\n[11] Chen, D. Z., Wu, Q., Nießner, M., and Chang, A. X. (2021a). D3net: A speaker-listener\narchitecture for semi-supervised dense captioning and visual grounding in rgb-d scans.\n[12] Chen, S., Zhu, H., Chen, X., Lei, Y ., Yu, G., and Chen, T. (2023). End-to-end 3d dense\ncaptioning with vote2cap-detr. In CVPR .\n[13] Chen, S., Chen, X., Zhang, C., Li, M., Yu, G., Fei, H., Zhu, H., Fan, J., and Chen, T. (2024b).\nLl3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n26428–26438.\n[14] Chen, Z., Gholami, A., Nießner, M., and Chang, A. X. (2021b). Scan2cap: Context-aware\ndense captioning in rgb-d scans. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 3193–3203.\n[15] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X.,\nLu, L., et al. (2024c). Internvl: Scaling up vision foundation models and aligning for generic\nvisual-linguistic tasks. In CVPR .\n[16] Cheng, A.-C., Yin, H., Fu, Y ., Guo, Q., Yang, R., Kautz, J., Wang, X., and Liu, S. (2024). Spa-\ntialrgpt: Grounded spatial reasoning in vision language models. arXiv preprint arXiv:2406.01584 .\n[17] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\nH. W., Sutton, C., Gehrmann, S., et al. (2023). Palm: Scaling language modeling with pathways.\nJournal of Machine Learning Research ,24(240), 1–113.\n11\n--- Page 12 ---\n[18] Dai, A., Nießner, M., Zollhöfer, M., Izadi, S., and Theobalt, C. (2017a). Bundlefusion: Real-time\nglobally consistent 3d reconstruction using on-the-fly surface reintegration. ACM Transactions on\nGraphics (ToG) ,36(4), 1.\n[19] Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Nießner, M. (2017b). Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In CVPR .\n[20] Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. (2023).\nInstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-\nseventh Conference on Neural Information Processing Systems .\n[21] Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Liu, T., et al. (2022).\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 .\n[22] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten,\nA., Yang, A., Fan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783 .\n[23] Fu, R., Liu, J., Chen, X., Nie, Y ., and Xiong, W. (2024). Scene-llm: Extending language model\nfor 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401 .\n[24] Gemini (2024). Gemini: A family of highly capable multimodal models. https://gemini.\ngoogle.com/app .\n[25] Gu, J., Stefani, E., Wu, Q., Thomason, J., and Wang, X. (2022). Vision-and-language navigation:\nA survey of tasks, methods, and future directions. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 7606–7623.\n[26] Guo, Z., Zhang, R., Zhu, X., Tang, Y ., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al.\n(2023). Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding,\ngeneration, and instruction following. arXiv preprint arXiv:2309.00615 .\n[27] Hong, Y ., Zhen, H., Chen, P., Zheng, S., Du, Y ., Chen, Z., and Gan, C. (2023). 3d-llm: Injecting\nthe 3d world into large language models. NeurIPS .\n[28] Hu, J., Lin, J., Yan, J., and Gong, S. (2025a). Leveraging hallucinations to reduce manual\nprompt dependency in promptable segmentation. Advances in Neural Information Processing\nSystems ,37, 107171–107197.\n[29] Hu, Y ., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna,\nR. (2025b). Visual sketchpad: Sketching as a visual chain of thought for multimodal language\nmodels. Advances in Neural Information Processing Systems ,37, 139348–139379.\n[30] Huang, H., Chen, Y ., Wang, Z., Huang, R., Xu, R., Wang, T., Liu, L., Cheng, X., Zhao, Y ., Pang,\nJ.,et al. (2023a). Chat-scene: Bridging 3d scene and large language models with object identifiers.\narXiv preprint arXiv:2312.08168 .\n[31] Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y ., Li, Q., Zhu, S.-C., Jia, B., and Huang,\nS. (2023b). An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871 .\n[32] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda,\nA., Hayes, A., Radford, A., et al. (2024). Gpt-4o system card. arXiv preprint arXiv:2410.21276 .\n[33] Jin, Z., Hayat, M., Yang, Y ., Guo, Y ., and Lei, Y . (2023). Context-aware alignment and mutual\nmasking for 3d-language pre-training. In CVPR .\n[34] Kang, W., Huang, H., Shang, Y ., Shah, M., and Yan, Y . (2024). Robin3d: Improving 3d large\nlanguage model via robust instruction tuning. arXiv:2410.00255 .\n[35] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead,\nS., Berg, A. C., Lo, W.-Y ., et al. (2023). Segment anything. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 4015–4026.\n[36] Kolodiazhnyi, M., V orontsova, A., Skripkin, M., Rukhovich, D., and Konushin, A. (2024).\nUnidet3d: Multi-dataset indoor 3d object detection. arXiv preprint arXiv:2409.04234 .\n12\n--- Page 13 ---\n[37] Landsiedel, C., Rieser, V ., Walter, M., and Wollherr, D. (2017). A review of spatial reasoning\nand interaction for real-world robotics. Advanced Robotics ,31(5), 222–242.\n[38] Lee, O. Y ., Xie, A., Fang, K., Pertsch, K., and Finn, C. (2024). Affordance-guided reinforcement\nlearning via visual prompting. arXiv preprint arXiv:2407.10341 .\n[39] Li, B., Zhang, Y ., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y ., Liu, Z.,\net al. (2024a). Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 .\n[40] Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J., et al. (2024b). Multimodal\nfoundation models: From specialists to general-purpose assistants. Foundations and Trends ®in\nComputer Graphics and Vision ,16(1-2), 1–214.\n[41] Li, C., Zhang, C., Zhou, H., Collier, N., Korhonen, A., and Vuli ´c, I. (2024c). Topviewrs:\nVision-language models as top-view spatial reasoners. arXiv preprint arXiv:2406.02537 .\n[42] Li, M., Chen, X., Zhang, C., Chen, S., Zhu, H., Yin, F., Yu, G., and Chen, T. (2023). M3dbench:\nLet’s instruct large models with multi-modal 3d prompts. arXiv preprint arXiv:2312.10763 .\n[43] Li, Z., Zhang, C., Wang, X., Ren, R., Xu, Y ., Ma, R., Liu, X., and Wei, R. (2024d). 3dmit: 3d\nmulti-modal instruction tuning for scene understanding. In 2024 IEEE International Conference\non Multimedia and Expo Workshops (ICMEW) , pages 1–5. IEEE.\n[44] Liao, Z., Xie, Q., Zhang, Y ., Kong, Z., Lu, H., Yang, Z., and Deng, Z. (2025). Improved\nvisual-spatial reasoning via r1-zero-like training. arXiv preprint arXiv:2504.00883 .\n[45] Lin, J., Yin, H., Ping, W., Lu, Y ., Molchanov, P., Tao, A., Mao, H., Kautz, J., Shoeybi, M., and\nHan, S. (2023). Vila: On pre-training for visual language models.\n[46] Lin, W., Wei, X., An, R., Gao, P., Zou, B., Luo, Y ., Huang, S., Zhang, S., and Li, H. (2024).\nDraw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want.\narXiv preprint arXiv:2403.20271 .\n[47] Liu, B., Dong, Y ., Wang, Y ., Rao, Y ., Tang, Y ., Ma, W.-C., and Krishna, R. (2024a). Coarse\ncorrespondence elicit 3d spacetime understanding in multimodal language model. arXiv preprint\narXiv:2408.00754 .\n[48] Liu, D., Dong, X., Zhang, R., Luo, X., Gao, P., Huang, X., Gong, Y ., and Wang, Z. (2023a). 3dax-\niesprompts: Unleashing the 3d spatial task capabilities of gpt-4v. arXiv preprint arXiv:2312.09738 .\n[49] Liu, H., Li, C., Wu, Q., and Lee, Y . J. (2023b). Visual instruction tuning. Advances in neural\ninformation processing systems ,36, 34892–34916.\n[50] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al.\n(2024b). Grounding dino: Marrying dino with grounded pre-training for open-set object detection.\nInEuropean Conference on Computer Vision , pages 38–55. Springer.\n[51] Ma, W., Chou, Y .-C., Liu, Q., Wang, X., de Melo, C., Chen, J., Xie, J., and Yuille, A.\n(2025). Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv preprint\narXiv:2504.20024 .\n[52] Ma, X., Yong, S., Zheng, Z., Li, Q., Liang, Y ., Zhu, S.-C., and Huang, S. (2022). Sqa3d:\nSituated question answering in 3d scenes. In ICLR .\n[53] Man, Y ., Zheng, S., Bao, Z., Hebert, M., Gui, L.-Y ., and Wang, Y .-X. (2024). Lexi-\ncon3d: Probing visual foundation models for complex 3d scene understanding. arXiv preprint\narXiv:2409.03757 .\n[54] Marsili, D., Agrawal, R., Yue, Y ., and Gkioxari, G. (2025). Visual agentic ai for spatial reasoning\nwith a dynamic api. arXiv preprint arXiv:2502.06787 .\n[55] Marza, P., Matignon, L., Simonin, O., and Wolf, C. (2022). Teaching agents how to map:\nSpatial reasoning for multi-object navigation. In 2022 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS) , pages 1725–1732. IEEE.\n13\n--- Page 14 ---\n[56] Momennejad, I., Hasanbeig, H., Vieira Frujeri, F., Sharma, H., Jojic, N., Palangi, H., Ness, R.,\nand Larson, J. (2023). Evaluating cognitive maps and planning in large language models with\ncogeval. Advances in Neural Information Processing Systems ,36, 69736–69751.\n[57] Nadel, L. (2008). The Hippocampus and Context Revisited . Oxford University Press.\n[58] Nasiriany, S., Xia, F., Yu, W., Xiao, T., Liang, J., Dasgupta, I., Xie, A., Driess, D., Wahid, A.,\nXu, Z., et al. (2024). Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv\npreprint arXiv:2402.07872 .\n[59] OpenAI (2024). Gpt-o3: A large language model by openai. https://openai.com/index/\nhello-gpt-4o/ .\n[60] Qi, Z., Fang, Y ., Sun, Z., Wu, X., Wu, T., Wang, J., Lin, D., and Zhao, H. (2024a). Gpt4point:\nA unified framework for point-language understanding and generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 26417–26427.\n[61] Qi, Z., Dong, R., Zhang, S., Geng, H., Han, C., Ge, Z., Yi, L., and Ma, K. (2024b). Shapellm:\nUniversal 3d object understanding for embodied interaction. In European Conference on Computer\nVision , pages 214–238. Springer.\n[62] Qi, Z., Zhang, Z., Fang, Y ., Wang, J., and Zhao, H. (2025). Gpt4scene: Understand 3d scenes\nfrom videos with vision-language models. arXiv preprint arXiv:2501.01428 .\n[63] Ramakrishnan, S. K., Wijmans, E., Kraehenbuehl, P., and Koltun, V . (2024). Does spatial\ncognition emerge in frontier models? arXiv preprint arXiv:2410.06468 .\n[64] Rozanova, J., Ferreira, D., Dubba, K., Cheng, W., Zhang, D., and Freitas, A. (2021). Grounding\nnatural language instructions: Can large language models capture spatial information? arXiv\npreprint arXiv:2109.08634 .\n[65] Schult, J., Engelmann, F., Hermans, A., Litany, O., Tang, S., and Leibe, B. (2023). Mask3d:\nMask transformer for 3d semantic instance segmentation. In 2023 IEEE International Conference\non Robotics and Automation (ICRA) . IEEE.\n[66] Sisbot, E. A., Marin, L. F., and Alami, R. (2007). Spatial reasoning for human robot interaction.\nIn2007 IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 2281–2287.\nIEEE.\n[67] Sun, F.-Y ., Liu, W., Gu, S., Lim, D., Bhat, G., Tombari, F., Li, M., Haber, N., and Wu, J. (2024).\nLayoutvlm: Differentiable optimization of 3d layout via vision-language models. arXiv preprint\narXiv:2412.02193 .\n[68] Tang, Y ., Qu, A., Wang, Z., Zhuang, D., Wu, Z., Ma, W., Wang, S., Zheng, Y ., Zhao, Z., and\nZhao, J. (2024). Sparkle: Mastering basic spatial capabilities in vision language models elicits\ngeneralization to composite spatial reasoning. arXiv preprint arXiv:2410.16162 .\n[69] Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological Review ,55(4), 189–208.\n[70] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,\nGoyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971 .\n[71] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al.\n(2024). Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.\narXiv preprint arXiv:2409.12191 .\n[72] Wang, Z., Huang, H., Zhao, Y ., Zhang, Z., and Zhao, Z. (2023). Chat-3d: Data-efficiently tuning\nlarge language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769 .\n[73] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al.\n(2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems ,35, 24824–24837.\n14\n--- Page 15 ---\n[74] Wu, J., Zhang, Z., Xia, Y ., Li, X., Xia, Z., Chang, A., Yu, T., Kim, S., Rossi, R. A., Zhang, R.,\net al. (2024a). Visual prompting in multimodal large language models: A survey. arXiv preprint\narXiv:2409.15310 .\n[75] Wu, M., Cai, X., Ji, J., Li, J., Huang, O., Luo, G., Fei, H., Jiang, G., Sun, X., and Ji, R.\n(2025). Controlmllm: Training-free visual prompt learning for multimodal large language models.\nAdvances in Neural Information Processing Systems ,37, 45206–45234.\n[76] Wu, W., Mao, S., Zhang, Y ., Xia, Y ., Dong, L., Cui, L., and Wei, F. (2024b). Visualization-of-\nthought elicits spatial reasoning in large language models. arXiv e-prints , pages arXiv–2404.\n[77] Wu, Y ., Wang, Y ., Tang, S., Wu, W., He, T., Ouyang, W., Torr, P., and Wu, J. (2024c).\nDettoolchain: A new prompting paradigm to unleash detection ability of mllm. In European\nConference on Computer Vision , pages 164–182. Springer.\n[78] Xu, R., Wang, X., Wang, T., Chen, Y ., Pang, J., and Lin, D. (2024). Pointllm: Empowering\nlarge language models to understand point clouds. In European Conference on Computer Vision ,\npages 131–147. Springer.\n[79] Xue, F., Chen, Y ., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y ., Tang, H., Yang, S., Liu, Z., et al.\n(2024). Longvila: Scaling long-context visual language models for long videos. arXiv preprint\narXiv:2408.10188 .\n[80] Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. (2023a). Set-of-mark prompting\nunleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 .\n[81] Yang, J., Yang, S., Gupta, A. W., Han, R., Fei-Fei, L., and Xie, S. (2024a). Thinking in\nspace: How multimodal large language models see, remember, and recall spaces. arXiv preprint\narXiv:2412.14171 .\n[82] Yang, J., Ding, R., Brown, E., Qi, X., and Xie, S. (2024b). V-irl: Grounding virtual intelligence\nin real life. In European Conference on Computer Vision , pages 36–55. Springer.\n[83] Yang, J., Tan, R., Wu, Q., Zheng, R., Peng, B., Liang, Y ., Gu, Y ., Cai, M., Ye, S., Jang, J., et al.\n(2025). Magma: A foundation model for multimodal ai agents. arXiv preprint arXiv:2502.13130 .\n[84] Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. (2023b). The dawn of\nlmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 ,9(1), 1.\n[85] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y ., and Narasimhan, K. (2023). Tree of\nthoughts: Deliberate problem solving with large language models. Advances in neural information\nprocessing systems ,36, 11809–11822.\n[86] Yao, Y ., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al.\n(2024). Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800 .\n[87] Yeshwanth, C., Liu, Y .-C., Nießner, M., and Dai, A. (2023). Scannet++: A high-fidelity dataset\nof 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 12–22.\n[88] Yu, R., Yu, W., and Wang, X. (2024). Attention prompting on image for large vision-language\nmodels. In European Conference on Computer Vision , pages 251–268. Springer.\n[89] Yuan, Z., Yan, X., Liao, Y ., Guo, Y ., Li, G., Cui, S., and Li, Z. (2022). X-trans2cap: Cross-modal\nknowledge transfer using transformer for 3d dense captioning. In CVPR .\n[90] Zhang, P., Zhang, K., Li, B., Zeng, G., Yang, J., Zhang, Y ., Wang, Z., Tan, H., Li, C., and Liu, Z.\n(2024a). Long context transfer from language to vision. arXiv preprint arXiv:2406.16852 .\n[91] Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., and Qiao, Y .\n(2023a). Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv\npreprint arXiv:2303.16199 .\n15\n--- Page 16 ---\n[92] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X.,\nLin, X. V ., et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068 .\n[93] Zhang, W., Cai, M., Zhang, T., Zhuang, Y ., Li, J., and Mao, X. (2024b). Earthmarker: A\nvisual prompting multi-modal large language model for remote sensing. IEEE Transactions on\nGeoscience and Remote Sensing .\n[94] Zhang, Y ., Gong, Z., and Chang, A. X. (2023b). Multi3drefer: Grounding text description to\nmultiple 3d objects. In ICCV .\n[95] Zhang, Y ., Li, B., Liu, h., Lee, Y . j., Gui, L., Fu, D., Feng, J., Liu, Z., and Li, C. (2024c).\nLlava-next: A strong zero-shot video understanding model.\n[96] Zhao, L., Cai, D., Sheng, L., and Xu, D. (2021). 3dvg-transformer: Relation modeling for\nvisual grounding on point clouds. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 2928–2937.\n[97] Zheng, R., Liang, Y ., Huang, S., Gao, J., Daumé III, H., Kolobov, A., Huang, F., and Yang,\nJ. (2024). Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist\nrobotic policies. arXiv preprint arXiv:2412.10345 .\n[98] Zhu, C., Wang, T., Zhang, W., Pang, J., and Liu, X. (2024a). Llava-3d: A simple yet effective\npathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125 .\n[99] Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. (2023a). Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 .\n[100] Zhu, F., Yang, J., and Jiang, H. (2024b). Towards flexible visual relationship segmentation.\nAdvances in Neural Information Processing Systems ,37, 107633–107661.\n[101] Zhu, Z., Ma, X., Chen, Y ., Deng, Z., Huang, S., and Li, Q. (2023b). 3d-vista: Pre-trained\ntransformer for 3d vision and text alignment. In ICCV .\n[102] Zhu, Z., Zhang, Z., Ma, X., Niu, X., Chen, Y ., Jia, B., Deng, Z., Huang, S., and Li, Q. (2024c).\nUnifying 3d vision-language understanding via promptable queries. In ECCV .\n16\n--- Page 17 ---\nA Details of Struct2D Prompting Strategy.\nFigure 7 illustrates the overall Struct2D prompting framework, which transforms egocentric 3D\nscene input into structured 2D representations for spatial reasoning. Given an input video and a spatial\nquestion, we first reconstruct a 3D point cloud from RGB-D frames and remove the ceiling to obtain\na clear top-down view of the scene. Object detection is then performed in 3D space, and detected\nobjects are projected onto a bird’s-eye-view (BEV) image to produce a layout of the environment.\nThese object marks are filtered to retain only those relevant to the input question.\nWe optionally extract egocentric keyframes to capture detailed object appearances. Keyframes are\nselected by projecting 3D object bounding boxes onto sampled video frames and depth maps, and iden-\ntifying views where each object is both visible and unobstructed. Object-centric metadata—including\nobject categories and 3D coordinates—is encoded as text and used as part of the prompt input.\nAlgorithm 1 outlines the core procedure for constructing the Struct2D prompt. Given an input video\nV, depth frames D, a reconstructed 3D scene P, and a set of target objects O, we begin by rendering\na BEV image vand projecting each object oi∈ O into the view using the RGB camera parameters\nCrgb. The 2D projections are then drawn as object marks on the image.\nTo select keyframes, we sample NRGB-D frames and iteratively check for visibility of objects not\nyet covered in the BEV . For each candidate frame Ii, we project the remaining unseen objects onto\nboth the frame and its depth map. If a valid projection exists (i.e., the projected location lies within\nthe image and has valid depth), the object mark is rendered and the frame is added to the keyframe\nsetIkeys. This process continues until all relevant objects are covered. The final prompt consists of\n❶a BEV image with filtered marks, ❷optional keyframes containing visible objects, and ❸object\nmetadata text, all of which are passed to a large multimodal model for reasoning.\nThis framework allows the LMM to perform 3D spatial reasoning from 2D visual and textual inputs,\nwithout requiring direct access to raw 3D data at inference time. It enables scalable, flexible spatial\nunderstanding grounded in realistic perception outputs.\nAlgorithm 1 Struct2D Visual Prompting\nInput: Input video V, Depth frames D, Reconstructed 3D scene P, Objects of interest O, RGB\ncamera parameters Crgb, Depth camera parameters Cd\n1:Render a Bird’s Eye View image: v←BEV(P)\n2:foroi∈ O do\n3: Project oiontov:pi←Project (oi,v,Crgb)\n4: Update view: v←Add-Mark (v, pi)\n5:end for\n6:Sample Nframes: I,DI←Sample (V,D)\n7:Initialize key frame set: Ikeys← {}\n8:Initialize found objects set: OF← {}\n9:forIi∈IandDi∈DIdo\n10: bi←False\n11: foroj∈ O and/∈ O Fdo\n12: Project ojontoIiandDi:pI\nj←Project (oj,Ii,Crgb),pD\nj←Project (oj,Di,Cd)\n13: ifpI\nj∈IiandpD\nj∈DiandpD\nj≥0then\n14: bi←True\n15: Update frame: Ii←Add-Mark (Ii, pI\nj)\n16: Add object to set: OF← O F∪ {oi}\n17: end if\n18: end for\n19: ifbithen\n20: Add to key frame set: Ikeys←Ikeys∪ {Ii}\n21: end if\n22:end for\nReturn: Informative BEV view vand key frame set Ikeys\nQualitative comparison of our Struct2D prompting. To better understand the impact of prompt\ndesign on spatial reasoning, we conduct qualitative analyses highlighting two key aspects of our\n17\n--- Page 18 ---\n…\n             Question:\nIf I am standing  by the stove \nand facing the tv, is the sofa \nto my front-left,  front-right,  \nback-left,  or back-right?3D Object  Locations:\n1: sofa 2: tv 3: stove\nPerception  ModulePoint cloud\nreconstruction\nCeiling  removalObject  Detection\n1\n2\n3\nStruct2D  Prompting  Module\n Large Multimodal  Model\nInputBEV image with ﬁltered \nobject  marksEgocentric  Keyframes  (optional)\n<Question>\n1 2 3\n2\n 1\n 3\nReasoningObject-centric  Metadata\nHere are objects  with \ntheir mark ids and \ncoordinates  in 3D space.\n1: sofa: [-2.4, 3.0, -0.5]\n2: tv: [-3.5, 0.5, 0.2]\n3: stove: [-0.7, -0.9, -0.1]Figure 7: Overview of the Struct2D Prompting Framework. Given an egocentric video and a\nspatial question, we first reconstruct a 3D point cloud and remove the ceiling for a clear top-down\nview. Objects are detected in 3D space, and a bird’s-eye-view (BEV) image is rendered with object\nmarks projected onto the floor plane. These object marks are filtered based on the content of the\nquestion. We also extract egocentric keyframes by projecting 3D bounding box centers onto the\nvideo, when appearance cues are needed. Object-centric metadata—including object IDs and 3D\ncoordinates—is encoded as text. The structured 2D visual and textual inputs are then fed into a large\nmultimodal model for spatial reasoning.\nframework: reasoning guidance, object orientation, and structured metadata. As shown in Fig. 8, when\nthe model is prompted only with a BEV image and object marks, it struggles to accurately resolve\nrelative spatial relationships. Adding a structured guide prompt enables the model to decompose the\ntask into interpretable geometric steps, though it may still fail without an aligned reference frame.\nOnce the BEV is rotation-aligned with the agent’s viewpoint, the reasoning becomes more intuitive,\nleading to the correct answer. Similarly, in Fig. 9, we illustrate the benefit of object-centric metadata.\nWithout access to precise coordinates, the model must estimate distances visually, which can lead to\nerrors. When provided with 3D object positions, the model can directly compute spatial relations such\nas Euclidean distances, significantly improving its accuracy on localization tasks. These examples\nhighlight how prompt structure—through guided reasoning and geometric priors—plays a crucial\nrole in unlocking spatial understanding in LMMs.\nB Details of Struct2D-Set\nOverview. Struct2D-Set is a large-scale instruction tuning dataset aimed at enabling spatial\nreasoning and scene understanding in indoor 3D environments using only 2D projected inputs. It\ncontains over 200K question-answer (QA) pairs derived from 6K richly annotated indoor scenes\ndrawn from ScanNet [ 19], ScanNet++[ 87], and ARKitScenes[ 4]. Each QA instance is paired with\nstructured scene- and object-level metadata, allowing models to learn spatial concepts without relying\non explicit 3D feature representations during training.\nThe dataset spans eight categories of spatial reasoning tasks, such as object attribute identification,\nrelative localization, and egocentric navigation. Each QA pair follows an instruction-style format and\nincludes:\n• A natural language question,\n• A concise short-form answer,\n•A long-form answer, when applicable, containing step-by-step reasoning or contextual elabora-\ntion,\n18\n--- Page 19 ---\nQues %on: You are a 3D indoor scene assistant. We provide a Bird's Eye View(BEV) image with object marks and \nobject categories for analysis. Object marks are objects that are men %oned in the ques %on. The category names \nof object marks are: 1:tv, 2:sofa, 3:table.  \nIf I am standing by the tv and facing the table, is the sofa to my front-le O, front-right, back-le O, or back-right?  \nOp%ons: A. front-right B. back-le O C. back-right D. front-le O  \nPlease refer to the object loca %ons through their marks.  \nAnswer:  I’m sorry, I’m unable to con ﬁdently determine the sofa's rela %ve posi %on based on the current image. .\nGuide Prompt: Follow the steps below to complete the task accurately. \n1. Draw Line A from the point where I am standing to the object I am facing. \n2. Draw Line B, which is perpendicular to Line A and intersects it at my standing posi %on. \n3. To determine the object's rela %ve posi %on in the ques %on: Check which side of Line B the object is on. If the \nobject is on the same side as the object I am facing, it is in the front; otherwise, it is in the back. \n4. Next, determine the object's posi %on rela %ve to Line A: If it is to the le O of Line A, it is on the le O. If it is to the \nright of Line A, it is on the right. Le O and right should always be determined based on the direc %on in which I am \nfacing.  \nAnswer:  Let’s follow the steps: \n1.Line A goes from object 1 (tv) to object 3 (table), roughly from le O to right . \n2.Line B is perpendicular to that, so it goes top to bo ]om. \n3.Object 2 (sofa) appears to be behind  Line B (i.e., not on the same side as table), so it’s in the back . \n4.It is to the le O of Line A. \nSo, I conclude: B. back-le O \nAnswer:  Let’s follow the steps: \n1.Line A goes from object 1 (tv) to object 3 (table), which is exactly ver %cal and straight up. \n2.Line B is perpendicular Line A, so it goes le O to right through the tv. \n3.The sofa (object 2) is clearly above  Line B (same side as table) → it’s in the front . \n4.The sofa is on the le2 side of Line A. \nSo, I conclude: The sofa is at the front-le 2 rela %ve to my orienta %on. The correct answer should be D. front-le 2. Original\nw/ Guide Prompt\nw/ Rota %onRota %onFigure 8: Effects of guide prompting and rotation correction in relative spatial reasoning\nof the zero-shot prompting. We show different variations of our Struct2D . The original model\nstruggles to interpret spatial relationships using the raw BEV image and object marks alone, failing\nto answer correctly. With the addition of a structured guide prompt, the model is able to decompose\nthe reasoning process into geometric steps, though it still arrives at an incorrect answer. When\nobject orientation is further corrected via rotation alignment, the spatial reference frame becomes\nmore intuitive, and the model’s reasoning becomes clearer and more accurate. This demonstrates\nthe importance of both prompting design and canonical alignment in unlocking LMMs’ spatial\nunderstanding abilities. (Red texts are wrong answers; Green texts are correct ones.)\n•Accompanying metadata including relevant object marks, spatial coordinates, and references to\nvisual input modalities ( e.g., BEV image, selected keyframes).\nLong-form answers are provided selectively for tasks that benefit from explicit reasoning or contextual\nunderstanding. For categories requiring direct factual responses—such as object counting or binary\nverification—only short-form answers are used. This balanced design ensures effective supervision\nacross tasks of varying complexity, while maintaining interpretability and richness in reasoning. We\nnext describe the construction process for each task category in detail.\nObject counting. To construct object counting questions, we begin by sampling a scene from the\ntraining split of the source datasets and extracting its ground-truth object annotations. A target object\ncategory ( e.g.,chair ) is randomly selected from the annotated instances within the scene. A QA\npair is then generated using a templated prompt such as “How many class label(s) are there in this\nroom?” , paired with the correct numerical count as the answer. To improve linguistic diversity and\nfluency, we further augment these questions by prompting ChatGPT to generate alternative phrasings\nwith equivalent semantic meaning.\nSpatial Relationship. This category evaluates a model’s ability to reason about the directional\nrelationships between objects in a 3D scene from an egocentric perspective. Following the formulation\nin VSI-Bench [ 81], we focus on the subtask of relative direction , where the goal is to identify the\ndirectional location of a target object based on a specified standing point and facing direction.\nTo construct each QA pair, we begin by computing the 3D centers of all objects in the scene and\nprojecting them onto the 2D BEV image. We then sample a triplet of objects representing the\nroles of standing ,facing , and target , while filtering out ambiguous categories ( e.g., object clusters\nor large connected instances) and enforcing a minimum pixel-distance threshold to ensure spatial\ndistinguishability. The < standing ,facing > vector defines the forward direction of the agent, and the\n19\n--- Page 20 ---\nQues%on: You are a 3D indoor scene assistant. We provide a Bird's Eye View(BEV) image with object marks and object categories for analysis. Object marks are objects that are men%oned in the ques%on.  The category names of object marks are: 2: cup, 6: door, 13: toilet, 34: ceiling light, 36: heater. Measuring from the closest point of each object, which of these objects (heater, cup, door, ceiling light) is the closest to the toilet? Op%ons: A.Heater B. cup C. door D. ceiling light Please refer to the object loca%ons through their marks.  Answer: Let’s analyze the rela%ve distances in the BEV image: Toilet (13) is near the upper center. Ceiling light (34) is directly above or just slightly in front of the toilet. Cup (2) is placed under the sink, somewhat farther toward the boXom of the image. Heater (36) is on the far leY wall. Door (6) is on the top-leY, implying it's on the ﬂoor plane. Now compare the closest point of each object to the toilet: •The ceiling light (34) is almost touching or directly above the toilet — essen%ally zero horizontal distance. •The cup (2) is in front and below — s%ll clearly farther. •The heater (36) is across the room — farther. •The door (6) is on the top — not spa%ally adjacent. Correct Answer: D. ceiling light Answer: We are given the center coordinates of the objects and asked to ﬁnd which one is closest to the toilet (object 13). We approximate this by compu%ng the Euclidean distance between the toilet and each of the other objects: Coordinates: •Toilet (13): [1.68, 1.69, 0.49] •Cup (2): [1.45, 0.20, 0.98] •Door (6): [0.69, 1.88, 1.06] •Ceiling light (34): [1.69, 1.11, 2.44] •Heater (36): [0.15, 1.44, 0.95] Distances to the toilet (using 3D Euclidean distance): •Cup: √((1.68−1.45)² + (1.69−0.20)² + (0.49−0.98)²) ≈ 1.584 •Door: √((1.68−0.69)² + (1.69−1.88)² + (0.49−1.06)²) ≈ 1.158 •Ceiling light: √((1.68−1.69)² + (1.69−1.11)² + (0.49−2.44)²) ≈ 2.034 •Heater: √((1.68−0.15)² + (1.69−1.44)² + (0.49−0.95)²) ≈ 1.616 Conclusion: The door is the closest to the toilet. Answer: C. door Originalw/ Object-centric MetadataThe center coordinates of the objects in the 3D point cloud are 2: cup: [1.45, 0.20, 0.98], 6: door: [0.69, 1.88, 1.06], 13: toilet: [1.68, 1.69, 0.49], 34: ceiling light: [1.69, 1.11, 2.44], 36: heater: [0.15, 1.44, 0.95]. Figure 9: Effect of object-centric metadata for precise spatial reasoning. Originally, the model\nattempts to estimate distances based solely on the spatial layout in the BEV image but fails to identify\nthe correct object closest to the toilet. In contrast, with access to object-centric metadata—specifically,\n3D coordinates of each object—the model can compute accurate Euclidean distances and correctly\nidentify the nearest object. This example highlights how structured metadata enhances geometric\nreasoning and helps avoid ambiguity in visual interpretation. (Red text indicates incorrect reasoning;\nGreen text indicates the correct answer.)\n<standing ,target > vector is used to determine the relative orientation of the target object. The angular\noffset between these vectors is then discretized into directional bins such as front-left ,right , orback,\nproducing the correct label.\nWe format each QA pair using a natural language template ( e.g.,“If I am standing by the TV and\nfacing the refrigerator, is the sink to my left, right, or back?” ) and provide the short-form directional\nanswer. To enhance both linguistic variation and model supervision, we further augment each instance\nusing ChatGPT, which paraphrases the question and generates a long-form answer that walks through\nthe step-by-step reasoning process under the egocentric frame of reference.\nComparative Reasoning. This category involves tasks where the model must compare spatial\nattributes among multiple objects. We focus on relative distance comparison , where the objective is\nto identify which candidate object is closest or farthest from a given reference object.\nTo construct such questions, we first select a reference object whose identity is unambiguous based on\nits class label. Next, we sample a set of candidate objects, including multiple instances—potentially\nof the same class—to encourage instance-level discrimination. In contrast to reference selection, we\ndo not filter ambiguous or repeated categories among the candidates, as the goal is to challenge the\nmodel to reason over instance-specific spatial relations.\nWe compute the 3D centroid of each object using the center of its oriented bounding box and measure\npairwise Euclidean distances between the reference and each candidate. Based on the ranking of\nthese distances, we generate a templated question, such as “Measuring from the closest point of each\nobject, which of these objects (candidate labels) is closest to the reference object?” , along with the\ncorrect answer derived from the computed rankings.\n20\n--- Page 21 ---\nTo enhance linguistic variation and encourage deeper reasoning, we further augment each instance\nusing ChatGPT, which paraphrases the question and generates a long-form answer. These enriched\nresponses guide the model through comparative spatial reasoning before producing the final answer.\nQuantitative Spatial Measuring. This category targets tasks requiring the model to reason about\nmetric properties in 3D space, such as object size, spatial extent, and inter-object distance. We focus\non the object absolute distance subtask, where the model needs to estimate the physical distance\nbetween two specified objects within a scene.\nTo construct these questions, we begin by selecting two distinct objects with clearly identifiable class\nlabels to avoid semantic ambiguity. Using the oriented bounding box annotations, we extract all\neight corner points for each object and compute the minimum Euclidean distance across all point\npairs—this serves as the ground-truth physical distance between the two objects. Based on this\ncalculation, we generate templated questions such as: “Measuring from the closest point of each\nobject, what is the distance between the object1 and the object2 (in meters)?”\nTo enhance supervision and promote reasoning transparency, we further use ChatGPT to produce\nlong-form answers. These responses walk through the spatial computation process, prompting the\nmodel to conceptually simulate pairwise distance comparisons before arriving at the correct numerical\nanswer.\nEgocentric Navigation. This category focuses on tasks that require the model to plan navigation\nroutes from an egocentric perspective, reasoning about object references, turning actions, and scene\nlayout. The goal is to simulate how an embodied agent would traverse a 3D space by following\ninstructions grounded in object-level references.\nTo construct these tasks, we first sample up to 15 candidate objects per scene and project their\n3D centers onto the BEV image. Each object is visually marked in the BEV , and a mark-to-label\ndictionary string is generated to facilitate object identification. These scene representations are then\npassed to ChatGPT to generate plausible navigation routes in natural language.\nRoute generation is guided by several constraints: ❶Each route must consist of a sequence of\nconsecutive object marks (IDs) that an agent can follow. ❷At each step, the agent must perform\na local navigation action (e.g., turn left, turn right, pass by). ❸Routes must avoid collisions with\nirrelevant or obstructing objects. ❹Each path should span 3 to 5 objects to ensure sufficient reasoning\ncomplexity.\nAll generated routes undergo human review to ensure spatial plausibility. Invalid routes are discarded,\nand valid ones are further augmented via route reversal and sub-segmentation to increase diversity.\nTo determine the action sequence along the path, we randomly choose a facing object at the starting\npoint to establish the initial egocentric orientation. For each transition between objects, we compute\nthe vector from the current object to the next and compare it with the current facing direction to infer\nthe correct action (e.g., turn left, go forward). These navigation actions form the short-form answer.\nFor each object along the route, we apply our keyframe selection algorithm to extract egocentric\nviews from the original video. These keyframes, combined with the object labels, are used to prompt\nChatGPT to generate rich textual descriptions of each waypoint. Finally, we instruct ChatGPT to\nproduce long-form answers that walk through the full navigation route, step by step, reasoning over\norientation shifts and identifying the appropriate navigation action at each stage.\nOther Categories. The remaining task types—such as object attribute identification and binary\nattribute verification—are constructed by augmenting QA pairs from existing 3D vision-language\ndatasets, including ScanQA [ 3], SQA3D [ 52], Scan2Cap [ 14], ScanRefer [ 10], and Multi3DRefer [ 94].\nThese datasets provide scene-specific questions grounded in the ScanNet environment and collectively\ncover all eight spatial reasoning categories defined in Struct2D-Set .\nTo adapt these examples for instruction tuning, we first use ChatGPT to rephrase each question\ninto a more natural and instructional style. For tasks requiring reasoning, we also prompt ChatGPT\nto generate long-form answers that walk through the inference process. For visual grounding, we\nlocalize referenced objects using two approaches: when object IDs are available, we apply our\nkeyframe detection method to extract representative egocentric views. In datasets with descriptive\nreferences (e.g., SQA3D), we extract referring expressions with ChatGPT and apply Grounding\nDINO [50] and SAM [35] to identify and segment the mentioned objects.\n21\n--- Page 22 ---\nQ: What is placed on the bed of the room ?\nA: On the bed, there is a blue pillow with blue arms, \nwhich is visible in the image where the bed is \nprominently located towards the right side, \nsugges >ng a cozy setup. \nQ: How many black chairs  are located beside the table  ?\nA: There are 2 black chairs on the right side of the table. \nQ: What is to the right of the picture ?\nA: To the right side of the image, there is a \ndoorframe. This is indicated in the bird's-eye view \nand the keyframe image, where the doorframe is \nposi>oned adjacent to the sea >ng area, sugges >ng a \nclear exit point in the layout. \nQ: What color is the curtain  ?\nA: The curtain is white as seen in the image where it \nhangs beside the bed, contras >ng with the surrounding \ndécor. Spa>al Rela >on \n(Rela >ve)\nObject A Nribute \nIden >ﬁca>on\nObject Coun >ng\n Spa>al Loca >onFigure 10: Examples of QA pairs from Struct2D-Set used for instruction tuning. Each example\nillustrates a distinct category of spatial reasoning: spatial localization, object counting, spatial\nrelationship, and object attribute identification. For each question, the model is provided with a\nBEV image annotated with object marks, and optionally an egocentric keyframe to enhance visual\ngrounding. The answers include descriptive reasoning grounded in object positions and appearances,\nenabling the model to learn to associate structured 2D inputs with fine-grained spatial understanding.\nThe resulting keyframes are paired with each QA instance to serve as visual inputs during fine-tuning.\nThis pipeline enables instruction tuning on complex, object-centric spatial tasks while relying only\non 2D visual projections and avoiding the need for explicit 3D geometry at training time.\nC Implementation Details\nWe use Qwen2.5VL [ 71] as the base large multimodal model (LMM) for instruction tuning. During\ntraining, the model receives BEV images with filtered object marks and object-centric metadata as\ncore inputs. For tasks requiring visual cues such as object color or quantity, we additionally provide\negocentric keyframes. The BEV images are resized to 640×640, with object marks adaptively\nscaled based on their original image resolution. Keyframes are resized to 256×246and stitched into\ncompact 1 ×2 or 2 ×4 grids, enabling efficient batch loading and reducing GPU memory consumption.\nTo support reasoning supervision, we adopt a task-specific output format. For complex spatial\nreasoning tasks—such as relative direction estimation or route planning—we wrap the reasoning\nprocess between special tokens <think> and</think> , followed by the final answer enclosed\nwithin <answer> and</answer> . For tasks focused on appearance or simple measurements, the\nmodel is trained to generate direct short-form answers without explicit reasoning traces. The model is\ntrained for one epoch using a base learning rate of 2e-6 with cosine annealing, taking approximately\n8 hours on 8×H200 GPUs.\nAt evaluation time, we follow standard practices from prior work [ 30,62], reconstructing point\nclouds offline using BundleFusion [ 18], detecting 3D objects using Mask3D[ 65] and UniDet [ 36],\n22\n--- Page 23 ---\nQ: Is the bedside lamp  on or o ﬀ  ?\nA: The bedside lamp between the two beds is on. \nQ: What would be on the le ; side of the room if you were \nfacing the window  ?\nA: On the le ; side of the room, when facing the window, \nthere is a large whiteboard, which is visible in the image \nand occupies a signi ﬁcant part of that wall area . \nQ: What is the size of this room (in square meters)? If mul Hple \nrooms are shown, es Hmate the size of the combined space.\nShort answer: 21.2. \nAugmented answer: The calculated 2D distance between the \ntwo beds is approximately 2.0 meters. This distance was used as \na scale reference to es Hmate the width and length of the room. \nGiven the visible cues such as the walls and ﬂoor boundaries, I \nesHmate the room dimensions to be approximately 4.0 meters \nin width and 5.3 meters in length. Thus, the dimensions are \n~4.0m by ~5.3m, yielding a total area of approximately 21.2 \nsquare meters, which matches the original room size value of \n21.2. \nQ: What is the smaller of the two objects hanging on the wall?\nA: The smaller object hanging on the wall is the soap \ndispenser, which is typically more compact compared to \nother wall-mounted items, such as a towel holder or larger \nstorage units that may also be present in the kitchen area. \nThis can be inferred from the layout and features of the \nkitchen visible in the images  . \nBinary A Yribute \nVeriﬁcaHon \nQuan HtaHve Spa Hal \nMeasurement\nEgocentric Naviga Hon / \nOrienta Hon\nCompara Hve \nReasoning\nFigure 11: Additional QA examples from Struct2D-Set covering diverse spatial reasoning\ncategories. This figure showcases examples from the remaining categories in our dataset: binary\nattribute verification, egocentric navigation and orientation, quantitative spatial measurement, and\ncomparative reasoning. Each QA pair is grounded in structured 2D visual inputs (BEV views and\nkeyframes) and enriched with object marks and contextual metadata. These examples demonstrate\nthe model’s ability to reason about object states, egocentric spatial references, metric estimations,\nand relative comparisons—key competencies for embodied spatial understanding.\nand projecting the results to produce BEV images and 2D object marks. For object-level grounding,\nwe apply a rule-based method to identify the relevant objects mentioned in each question.\nD Results on 3D Grounding and 3D Dense Captioning\nQuantitative results. Tables 6 and 7 present our model’s performance on 3D grounding (ScanRefer,\nMulti3DRefer) and dense captioning (Scan2Cap) benchmarks. While our method does not achieve the\nhighest scores under rule-based metrics such as all F1@0.25/0.5 and BLEU/ROUGE, it consistently\ndelivers competitive results compared to existing vision-language baselines. Importantly, our approach\ndoes not rely on point cloud features during training or evaluation, in contrast to task-specific and 3D\nLLM models that depend heavily on explicit 3D representations. In addition, our approach requires\nsubstantially fewer egocentric keyframes on average (2 compared to 8 in GPT4Scene [ 62]), resulting\nin a more efficient and scalable training process. Compared to models designed for narrow tasks, our\nframework is more general and supports a wider range of spatial reasoning types, including relative\ndirection and route planning, which are not covered by these benchmarks. It is also worth noting\nthat the current evaluation metrics are rule-based and limited in expressiveness, which may not fully\nreflect a model’s capability in spatial understanding.\nQualitative results. Figure 12 illustrates qualitative examples of our fine-tuned Qwen2.5-VL-7B\nmodel across three major spatial reasoning tasks: 3D dense captioning, object grounding, and 3D\nquestion answering. In each case, the model receives a BEV image with object marks, optionally\nsupplemented with egocentric keyframes and metadata, and produces either a descriptive caption, an\n23\n--- Page 24 ---\nTable 6: 3D Grounding Evaluation on ScanRefer [10] and Multi3DRefer [94] datasets.\nMethodsScanRefer (val) Multi3DRefer (val)\nAcc@0.25 Acc@0.50 all F1@0.25 all F1@0.50\nTask-Specific Model\n3DVG-Transformer [96] 47.6 34.7 – 25.5\n3DJCG [6] 49.6 37.3 – 26.6\nD3Net [11] – 37.9 – 32.2\nM3DRef-CLIP [94] 51.9 44.7 42.8 38.4\n3D LLM Based Model\nChat-Scene [30] 55.5 50.2 57.1 52.4\nVision LLM Based Model\nQwen2-VL-7B [71] 5.4 5.1 21.1 19.9\nQwen2-VL-7B (GPT4Scene [62]) 40.5 36.7 45.4 42.1\nQwen2.5-VL-7B (Ours) 51.7 48.5 42.1 40.6\nTable 7: 3D Dense Captioning Evaluation on Scan2Cap [14] dataset.\nMethodsIoU@0.25 IoU@0.5\nBLEU-4 ROUGE BLEU-4 ROUGE\nTask-Specific Model\nScan2Cap [14] 34.2 55.3 23.3 44.5\n3DJCG [6] 40.2 59.2 31.0 50.8\nX-Trans2Cap [89] 35.7 54.7 25.1 45.3\n3D-VisTA [101] 36.5 57.6 34.0 54.3\nV ote2Cap-DETR [12] 39.3 59.3 34.5 54.4\n3D LLM Based Model\nLL3DA [13] 41.4 59.5 36.8 55.1\nLEO [31] – – 36.9 57.8\nChat-Scene [30] 38.2 60.6 36.3 58.1\nRobin3D [34] – – 38.4 –\nVision LLM Based Model\nQwen2-VL-7B [71] 3.8 24.7 3.8 24.6\nQwen2-VL-7B (GPT4Scene [62]) 36.3 57.6 34.2 55.2\nQwen2.5-VL-7B (Ours) 34.8 57.0 32.7 54.5\nobject ID, or a short-form answer. The examples demonstrate the model’s ability to reason about\nvisual attributes ( e.g., “a brown rectangle”), relative spatial positions ( e.g., “the table is to the right of\nthe couch”), and numerical or commonsense questions. We observe that the model often produces\nanswers consistent with the ground truth, and in some cases offers additional descriptive clarity\ngrounded in the visual context. These results highlight the effectiveness of our Struct2D prompting\nstrategy in enabling rich spatial understanding from structured 2D inputs.\nE Broader impacts\nOur work introduces Struct2D , a perception-guided prompting framework that enables Large\nMultimodal Models (LMMs) to perform robust spatial reasoning in 3D environments using only\nstructured 2D and text inputs. This direction offers several broader implications for research, society,\nand the reasonable development of AI systems.\nSocial Benefits. Struct2D lowers the barrier to 3D spatial reasoning by leveraging RGB-D percep-\ntion instead of requiring dense 3D annotations or point cloud inputs during inference. This makes\n24\n--- Page 25 ---\nQues %on: Paint a picture of the visual a 5ributes of \n<OBJ11>, then explore how it relates spa %ally to \nother elements in the scene.  \nGround Truth: This is a brown table. It is by the couch.\nOurs: The table is to the right of the couch. The \ntable is a brown rectangle.\nQues %on: Paint a picture of the visual a 5ributes of \n<OBJ002>, then explore how it relates spa %ally to \nother elements in the scene.  \nGround Truth: The chair is black and pushed into a \ndesk. It is the chair furthest to the le K. That is on the \nleK.\nOurs: This is a black chair. It is at a desk.\nQues %on: Iden %fy the ID of the object that closely \naligns with the descrip %on “a brown table, placed in \nthe middle of the room, on the le K is 4 brown chairs, \non the right are 4 brown chairs. the front is a brown \ndoor with light shining on”.  \nGround Truth: 23.\nOurs: 23.\nQues %on: I sat in the chair at the head of the table \nand my backpack was on my right. Can the amount \nof book in front of me divided by three?\nGround Truth: No.\nOurs: No.\nQues %on: Iden %fy the ID of the object that closely \naligns with the descrip %on “this is a plain o ﬃce \nchair. it is slightly pulled out from the table. it is \nnear the black doorway entrance”.  \nGround Truth: 12.\nOurs: 12.\nQues %on: I am walking up the stairs near the \nmiddle of the stairs going to the second ﬂoor. What \nis covering the window to my right?\nGround Truth: Curtains.\nOurs: Curtains.3D dense cap +oning 3D dense cap +oning 3D grounding\n3D Ques +on Answering3D Ques +on Answering 3D groundingFigure 12: Output examples from our fine-tuned Qwen2.5-VL-7B model across multiple 3D\nspatial reasoning tasks. The figure showcases model responses on 3D dense captioning, object\ngrounding, and 3D question answering tasks. Each example includes the question, BEV and keyframe\ninputs with object marks, the ground-truth answer, and our model’s prediction. These examples\nillustrate the model’s ability to localize, describe, and reason about spatial relations using structured\n2D prompts derived from 3D scenes. Across tasks, the model demonstrates strong alignment with\nground-truth answers, even when questions require appearance attributes, relative spatial context, or\nnumerical reasoning.\nspatial understanding more accessible to a wide range of applications, especially in settings where\nreal-time 3D sensing is noisy, sparse, or unavailable. Potential downstream applications include:\n•Assistive robotics , where spatial-language understanding is critical for navigation and object\nmanipulation in dynamic home environments;\n•Augmented reality interfaces , where natural-language spatial queries must be resolved in\npartially reconstructed environments;\n•Accessibility technologies , especially for users with visual impairments, by enabling robust,\nlanguage-driven scene understanding with minimal hardware.\nPotential Negative Impacts. The preprocessing pipeline relies on egocentric video and 3D recon-\nstruction, which may involve scenes from private homes or workplaces. If deployed in real-world\napplications, such systems may inadvertently capture sensitive spatial or personal data. Ensuring\nstrict anonymization, access control, and user consent mechanisms is essential.\nResearch Contributions. By decoupling LMM training from explicit 3D input requirements,\nStruct2D promotes research into modular, scalable instruction-tuning pipelines that can generalize\nacross environments with different sensor setups. Furthermore, our public release of Struct2D-Set —\na large-scale spatial instruction dataset built with a principled blend of structured prompts, egocentric\nframes, and metadata—contributes valuable benchmarks to the broader vision-language community.\n25",
  "text_length": 95874
}