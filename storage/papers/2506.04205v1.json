{
  "id": "http://arxiv.org/abs/2506.04205v1",
  "title": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation",
  "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation.",
  "authors": [
    "Jinghan Jia",
    "Hadi Reisizadeh",
    "Chongyu Fan",
    "Nathalie Baracaldo",
    "Mingyi Hong",
    "Sijia Liu"
  ],
  "published": "2025-06-04T17:49:10Z",
  "updated": "2025-06-04T17:49:10Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04205v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04205v1  [cs.LG]  4 Jun 2025EPIC: T OWARDS LOSSLESS SPEEDUP FOR REASONING\nTRAINING THROUGH EDGE-PRESERVING COT C ONDENSATION\nA P REPRINT\nJinghan Jia†Hadi Reisizadeh¶Chongyu Fan†Nathalie Baracaldo§Mingyi Hong¶,‡Sijia Liu†,§\n†Michigan State University\n‡Amazon\n¶University of Minnesota\n§IBM Research\nABSTRACT\nLarge language models (LLMs) have shown remarkable reasoning capabilities when trained with\nchain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those\ndistilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training\ncosts during the distillation process, where a non-reasoning base model is taught to replicate the\nreasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-\nefficient reasoning training, aimed at pruning intermediate reasoning steps ( i.e., thoughts) in CoT\ntraces, enabling supervised model training on length-reduced CoT data while preserving both answer\naccuracy and the model’s ability to generate coherent reasoning. Our rationale is that CoT traces\ntypically follow a three-stage structure: problem understanding, exploration, and solution convergence.\nThrough empirical analysis, we find that retaining the structure of the reasoning trace, especially\nthe early stage of problem understanding (rich in reflective cues) and the final stage of solution\nconvergence (which closely relates to the final answer), is sufficient to achieve lossless reasoning\nsupervision. To this end, we propose an Edge-Preserv ingCondensation method, EPiC , which\nselectively retains only the initial and final segments of each CoT trace while discarding the middle\nportion. This design draws an analogy to preserving the “edge” of a reasoning trajectory, capturing\nboth the initial problem framing and the final answer synthesis, to maintain logical continuity.\nExperiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC\nreduces training time by over 34% while achieving lossless reasoning accuracy on MATH500,\ncomparable to full CoT supervision. To the best of our knowledge, this is the first study to explore\nthought-level CoT condensation for efficient reasoning model distillation. Codes are available at\nhttps://github.com/OPTML-Group/EPiC .\n1 Introduction\nLarge language models (LLMs) have demonstrated strong performance on complex reasoning tasks, especially when\ntrained with chain-of-thought (CoT) supervision [ 1–3]. CoT training encourages models to generate step-by-step\nintermediate reasoning before producing a final answer, enhancing both interpretability and task performance in domains\nsuch as mathematics [ 4] and science [ 5]. More recently, large reasoning models (LRMs) such as DeepSeek-R1 [ 3],\nOpenAI-O1 [ 4], and Kimi [ 6] have pushed this paradigm further by generating rich CoT traces infused with self-\nreflection, verification, and backtracking, e.g., acquired via reinforcement learning. These LRMs have enabled a new\ntraining pipeline: Their reasoning ability can be distilled into smaller LLMs via supervised fine-tuning (SFT) on\nLRM-generated CoT data [ 3,7–10]. Throughout this paper, we refer to training (non-reasoning) LLMs with CoT\nsupervision (for reasoning enhancement) as reasoning training .\nHowever, despite their quality, LRM-generated CoT traces are often excessively verbose and suffer from overthinking , a\ntendency to include repetitive or speculative reasoning steps that inflate sequence length without improving final answer\naccuracy [ 11,12]. This verbosity leads to two key issues: (1) high computational cost during SFT, and (2) reduced\n--- Page 2 ---\nTherefore, \nthe \noriginal \ntime \nT \nis \nindeed \n180 \ndays. \nSo \nthe \nanswer \nis \n180. \n...\nWait, \nbut \nactually, \nmaybe \nit's \nbetter \nto \nthink \nin \nterms \nof \ntime.  \n...\nOkay, \nlet \nme \ntry \nto \nfigure \nout \nthis \nproblem. \nSo, \na \nconstruction \ncompany \n...\nWait, \nbut \nactually, \nmaybe \nit's \nbetter \nto \nthink \nin \nterms \nof \ntime.  \n...\nTherefore, \nthe \noriginal \ntime \nT \nis \nindeed \n180 \ndays. \nSo \nthe \nanswer \nis \n180. \n...\nHead\nMiddle\nTail\nFull \nReasoning\nTrace\nEPiC\nOkay, \nlet \nme \ntry \nto \nfigure \nout \nthis \nproblem. \nSo, \na \nconstruction \ncompany \n...\n52\n90.2\n90.2\n46.7\n40.0\n38.4\n41.9\n1.5x \nFaster\n34Figure 1: Overview of EPiC. Left: EPiC removes the middle portion of CoT while preserving the head (problem understanding) and\ntail (solution convergence). EPiC applies to training data in OpenR1Math. Right: Performance and training time comparison between\nEPiC and full CoT training based on QWEN 2.5-M ATH-7B-I NSTRUCT . At 50% condensation ratio, EPiC achieves competitive\naccuracy with 1.5× faster training.\nsupervision quality due to noise, particularly in the middle of the trace where speculative exploration dominates. These\nobservations raise a central question: Are all reasoning steps equally important for reasoning training?\nIn this work, we propose Edge-Preserv ingCondensation (EPiC), a simple yet effective thought-level pruning method\nthat retains the head and tail segments of each CoT trace, corresponding to problem understanding and solution\nconvergence, while removing only the middle portion of the reasoning trajectory. As illustrated in Figure 1 (left) , EPiC\ntargets the overgenerated middle stage, preserving the structural and semantic integrity of the reasoning process. As\nshown in Figure 1 (right) , EPiC enables models to achieve competitive reasoning accuracy while reducing training\ntime by 1.5× compared to full-trace fine-tuning.\nTo better understand which segments of a CoT trace are most critical for reasoning supervision, we conduct a mutual\ninformation (MI) analysis between individual segments and the full reasoning trace. We find that the portions selected\nby EPiC consistently exhibit the highest MI with the complete trace, supporting our empirical finding that the middle\nsegment is the least informative (and often the noisiest) part of the reasoning trajectory. These findings motivate EPiC as\na principled and efficient strategy for CoT data-based reasoning training. Our contributions are summarized as follows:\n•We introduce the first framework EPiC to perform thought-level condensation during training , enabling efficient\nreasoning distillation by pruning uninformative steps within CoT traces.\n•We provide a series of analyses, including thought landscape visualization, mutual information analysis, and CoT\nperturbation studies, to quantify informativeness across CoT segments and validate the effectiveness of EPiC.\n•We conduct extensive experiments across two CoT training datasets (OpenR1Math and GeneralThought), three\nreasoning benchmarks (MATH500, AIME24, and GPQA-Diamond), and multiple non-reasoning model initializations\n(QWEN 2.5-M ATH-7B-I NSTRUCT ,QWEN 2.5-7B-I NSTRUCT , and LLAMA3.1-8B-I NSTRUCT ). Our results show that\nEPiC consistently achieves final answer accuracy comparable or better than full CoT supervision, while significantly\nreducing training time.\n2 Related Work\nModel distillation for LRMs. CoT prompting has been shown to significantly improve the reasoning capabilities\nof large language models (LLMs) [ 1], motivating a line of work that seeks to build LRMs through CoT-style data\nconstruction [ 13–15]. With the emergence of strong LRMs such as OPENAI-O1 [4],DEEPSEEK-R1[3], and KIMI-1.5\n[6], which can autonomously generate long and structured CoT traces, including self-reflection, verification, and\nbacktracking, researchers have increasingly focused on distilling such behaviors into smaller models. Guo et al. [3]\nwas among the first to demonstrate that the reasoning capabilities of LRMs can be effectively transferred to smaller\nmodels through SFT. Building on this insight, numerous works have explored distillation from LRM-generated CoT\ndata to improve reasoning performance in smaller LLMs [ 7–10,16–22]. These works can be broadly grouped into two\ncategories: (1) distillation via high-quality, long-form CoT traces generated from LRMs [ 7–10,16–18,20,21]; and (2)\nalignment-based approaches that directly supervise logits [ 19]. A complementary direction, explored by Ji et al. [22],\nproposes a hybrid strategy that uses truncated CoT prefixes with a small portion of full traces for efficient distillation.\nWhile prior work has successfully leveraged LRM-generated traces for performance improvement, only a few efforts\n[9,10] have addressed the efficiency bottlenecks in CoT distillation. These works focus on reducing the number of\ntraining examples while ensuring trace diversity and quality. In contrast, our work preserves all examples but reducing\nthe length of each trace through structured thought-level pruning. Specifically, we study how to remove uninformative\n--- Page 3 ---\nreasoning steps within each sample to reduce training cost, without compromising the effectiveness of the distilled\nmodel.\nScaling test-time reasoning and the challenge of overlength generation. Increasing test-time computation has\nconsistently improved model performance on complex reasoning tasks such as mathematical problem solving and\ncode generation [ 23–26]. These gains often come from generating longer reasoning traces or sampling diverse\nreasoning paths [ 4,24]. Recent methods include parallel path sampling [ 27–29], tree-based search [ 30,31], and\niterative refinement [ 32–34]. Additionally, Muennighoff et al. [9]proposed enhancing the use of reflection tokens at\ninference time, and others [ 26,35] showed that scaling test-time computation can rival or exceed model size increases.\nHowever, these strategies often induce overthinking, verbose, repetitive outputs that slow inference and may reduce\nquality [ 11,12]. This is especially common in LRMs, which tend to generate redundant reasoning steps and excessive\nself-reflection. To mitigate this, several methods promote concise, efficient reasoning: Team et al. [6], Aggarwal\nand Welleck [36], and Luo et al. [37] introduce length-regularized RL; Xia et al. [38] apply SFT with truncated or\ntoken-pruned inputs; Wang et al. [12] penalize reflection token usage; and Zhang et al. [39] compress thoughts via token\nprojection for faster decoding. While these approaches primarily target inference-time efficiency, our work addresses\nthe complementary challenge of improving training-time efficiency. Specifically, we investigate how to condense\nreasoning trajectories during supervised fine-tuning, enabling small models to learn LRM-style reasoning with reduced\ncomputational cost.\nDataset pruning for efficient training. To reduce training costs, data pruning has been widely studied in dis-\ncriminative settings like image classification, where redundant samples are removed [ 40–43]. Importance score for\neach sample is estimated using geometry-based [ 44], uncertainty-based [ 45], margin-based [ 46], gradient-based [ 47],\nforgetting-based [ 48], and training-dynamics-based methods [ 49], with learned pruners also explored [ 50]. These\napproaches have recently been adapted for LLM instruction tuning [ 51,52], and Zhou et al. [53] showed that strong\nperformance can be achieved with just 1,000 high-quality examples. However, pruning for reasoning training remains\nunderexplored. Ye et al. [10] leveraged a small set of curated CoT traces, but no prior work investigates pruning at the\nlevel of individual reasoning steps. In contrast, we introduce thought-level condensation , a fine-grained strategy that\nprunes within examples rather than across them.\n3 Condensed CoT for Efficient Reasoning Training: Motivation and Problem\nIn this section, we first reviews CoT-based reasoning training and its standard setup, then highlight the trade-off between\nefficiency and accuracy identified in prior work. Motivated by this tension, we investigate the potential of thought\nselection and formally introduce the problem of CoT condensation , which seeks to accelerate reasoning training without\ncompromising reasoning performance.\nReasoning enhancement via supervised fine-tuning (SFT) on CoT data. Training LLMs to reason step by step,\nrather than directly predicting final answers, using CoT supervision has shown significant promise [ 3,21,54,55]. This\nreasoning training approach has emerged as an effective model distillation technique, enabling smaller, non-reasoning\nLLMs to acquire reasoning abilities by fine-tuning on long CoT traces generated by larger models.\nThis work focuses on improving the efficiency of reasoning training with CoT supervision, aiming to achieve faster\ntraining while maintaining or even enhancing reasoning capabilities, as measured by final answer accuracy on complex\nproblems ( e.g., mathematics) and the ability to generate coherent reasoning traces ( e.g., reflected in the length of the\ndistilled model outputs).\nTo be more concrete, let D={(x,r,y)}denote a CoT-style training dataset, where xis the input question, r=\n[r1, r2, . . . , r n]denotes the corresponding full reasoning trajectory consisting of nintermediate steps ( i.e., thoughts),\nandyis the final answer. Following [ 39], we use “\\n\\n” as a delimiter to simply segment the CoT trajectory rinto\ndifferent thoughts {ri}. In addition, let θdenote the parameters of an LLM, and let πθ(b|a)represent the model’s\npredicted probability of generating response bgiven input a. The reasoning training for θunderDcan be then cast as\nminimize\nθ−E(x,r,y)∈D[logπθ(r,y|x)], (1)\nwhere the training objective is defined as a cross-entropy-based sequence prediction loss, which maximizes the likelihood\nof generating the reasoning trace and final answer conditioned on the input.\nPrior work: Efficiency-accuracy trade-off through dataset size reduction. While SFT on long CoT significantly\nenhances the reasoning abilities of LLMs, it is highly resource-intensive, particularly when the traces are generated\n--- Page 4 ---\nby LRMs like DEEPSEEK-R1. This renders solving problem (1) computationally expensive, particularly in resource-\nconstrained settings such as academic labs.\nTo improve the efficiency of reasoning training, prior work has explored size-reduced, high-quality CoT datasets\nsuch as S1 [ 9] and LIMO [ 10], each containing around 1k carefully curated examples. However, we find that these\ndatasets are typically benchmarked on large models ( e.g., 32B), and their effectiveness does not consistently transfer\nto the training of smaller models. As shown in Figure 2 , training a 7B model on LIMO or S1 significantly speeds up\nreasoning training compared to conventional SFT using the larger CoT dataset OpenR1Math (93k examples). However,\nPerformance \nGap\nS1 \n(1k)\nLIMO(0.8k)\nOpenR1Math \n(93k)\nOpenR1Math \n(Random \nthought-level \ncondensation \n93k)\nFigure 2: Accuracy and training time for\nreasoning training on OpenR1Math (93k ex-\namples), LIMO (0.8k examples), and S1\n(1k examples), using QWEN 2.5-M ATH-7B-\nINSTRUCT as the base non-reasoning LLM.\nAccuracy is evaluated on the MATH500 bench-\nmark. In addition to standard CoT datasets, we\nalso include a thought-level condensed version\nof OpenR1Math, where 50% of the intermedi-\nate thoughts in each CoT trace are randomly\nretained and the remainder pruned.this speedup comes at the cost of reduced accuracy on MATH500, 80.0%\nand 83.6% for LIMO and S1, respectively, compared to 90.2% achieved\nwhen training on OpenR1Math. This indicates that small-scale datasets like\nS1 and LIMO are insufficient to consistently support effective reasoning\nperformance.\nProblem statement. As motivated by Figure 2, curating smaller CoT\ndatasets does not appear to be an effective solution for improving the ef-\nficiency of reasoning training while preserving reasoning performance.\nTherefore, we propose shifting the focus from reducing the number of train-\ning examples tocondensing the reasoning trajectory within each example .\nThat is, we ask whether thought-level condensation of CoT data, rather than\nexample-level dataset reduction, can enable more efficient and effective\nreasoning training. Therefore, we define the CoT condensation operation\nas the selection (or pruning) of intermediate thoughts within a reasoning\ntrajectory. Given a CoT trace r= [r1, r2, . . . , r n], the condensed version\nis denoted as rcond= [ri]i∈Ω, where Ω⊆ {1, . . . , n }is the index set of se-\nlected thoughts and the remaining thoughts are discarded. The potential of\nthought-level CoT condensation is evident with random thought selection.\nAs shown in Figure 2 , randomly selecting 50% of the reasoning steps in\neach example from OpenR1Math yields 88.0% accuracy, outperforming\nLIMO and S1, while cutting training time by approximately 40% compared\nto training on the full OpenR1Math.\nFigure 2 motivates the central research question of our work: Can we design an effective CoT condensation method\nto address the supervised fine-tuning problem in (1), one that substantially reduces training cost while preserving\nreasoning performance comparable to full-length CoT supervision?\n4 Edge-Preserving CoT Condensation: Method and Rationale\nCoT \nTrace:\n \n... \nThe \nproblem \nmentions \nthree\n \ndifferent \njourneys\n: \n1. \n24 \nkm \nupstream\n \nand \n28 \nkm \ndownstream\n, \ntaking \nhalf\n \nan \nhour\n \nless\n \nthan\n \nthe \nsecond \njourney\n. \n2. \n30 \nkm \nupstream \nand \n21 \nkm \ndownstream\n, \nwhich \ntakes \nhalf\n \nan \nhour\n \nmore \nthan \nthe\n \nfirst \njourney\n. \n3.\n \n...\nWait, \nactually, \nthe \nproblem \nstates: \n\"For \nthis \njourney, \nit \ntook \nhalf\n \nan \nhour\n \nless \nthan\n \nfor \ntraveling \n30 \nkm \nupstream\n \nand \n21 \nkm \ndownstream\n, \nor \nhalf\n \nan \nhour\n \nmore \nthan\n \nfor \ntraveling \n15 \nkm \nupstream \nand \n42 \nkm\n \ndownstream. \n...\nFigure 3: Performance of TokenSkip-based token-level con-\ndensation for reasoning training. (Left) Visualization of a CoT\ntrace pruned by TokenSkip [ 38] with a 50% pruning ratio. To-\nkens highlighted in red are retained, while the rest are removed.\n(Right) Final answer accuracy of models trained on three datasets:\nTokenSkip-pruned (50%), random thought-level condensation\n(50%), and the original full dataset, evaluated on AIME and\nMATH500. All models are fine-tuned from QWEN 2.5-M ATH-\n7B-I NSTRUCT on the OpenR1Math dataset.In this section, we begin with a warm-up study to motivate\nwhy individual thoughts ( i.e., reasoning steps) serve as a\nproper unit for condensing CoT traces. We then present\nour method, EPiC, which performs CoT condensation by\nleveraging the underlying structure of reasoning trajec-\ntories. Finally, we justify the design of EPiC from two\ncomplementary perspectives: (1) the mutual information\nbetween individual reasoning steps and the final answer,\nand (2) a sensitivity analysis that contrasts the importance\nof reasoning structure versus content.\nCoT condensation unit: Thoughts or tokens? As an al-\nternative to thought-level condensation, one may consider\npruning a CoT trace at the token level. A representative\napproach is TokenSkip [38] to assign importance scores\nto individual tokens and prune those deemed less critical\n[56]. We can apply this method to compress CoT traces.\nHowever, when training models on these token-pruned\nCoT datasets, we observe a significant drop in performance compared to training on the original, unpruned data, as\nshown in Figure 3 . This may be because token-level pruning disrupts the thought-level reasoning pattern. For example,\nas shown in Figure 3(Left), TokenSkip overlooks transitional markers and reflective words ( e.g., “wait”) that are essential\n--- Page 5 ---\nfor connecting thoughts and preserving logical flow. Therefore, token-level condensation becomes ineffective for\nreasoning training, performing even worse than random thought-level condensation in Figure 3(Right).\nsolution \nconvergence\nOkay, \nlet \nme \ntry \nto \nfigure \nout \nthis \nproblem. \nSo, \na \nconstruction \ncompany \n...\nWait, \nbut \nactually, \nmaybe \nit's \nbetter \nto \nthink \nin \nterms \nof \ntime.  \n...\nWait, \nbut \nmaybe \nwe \nneed \nto \nrelate \nthe \noriginal \ntime \nto \nthe \nactual \ntime \n...\nTherefore, \nthe \noriginal \ntime \nT \nis \nindeed \n180 \ndays. \nSo \nthe \nanswer \nis \n180. \n...\nExploration\nUnderstanding\nSolution\nConvergence\nProblem\n...\n...\n...\nFigure 4: Illustration of three-stage struc-\nture of long CoT reasoning: problem under-\nstanding (head), exploration (middle), and\nsolution convergence (tail).Edge-preserving condensation (EPiC). First, we segment each CoT trace\ninto three distinct stages based on their functions: (1) Understand : parsing\nand interpreting the problem; (2) Explore : inferring and iterating through\nreasoning paths; And (3) Converge : synthesizing information and finalizing\nthe solution. See Figure 4 for an illustration. These stages correspond to the\nhead, middle, and tail segments of the CoT trace, respectively. Based on this\nstructured progression, we can achieve reasoning condensation by selectively\nremoving one of these stages from the full CoT trace.\nBased on the above CoT segmentation, we next develop EPiC, a method\nthat preserves only the head and tail portions of the CoT trace, effectively\nconnecting the initial and final stages while discarding the exploration stage.\nThis design mirrors the idea of retaining the “edges” of a reasoning trajectory.\nRecall that r= [r1, r2, . . . , r n]denotes the full reasoning trajectory consisting\nofnthoughts. We define the condensation ratio (CR)τ∈[0,1]as the fraction of thoughts retained after pruning ( i.e.,\nthe length of the condensed trajectory). EPiC compresses the full reasoning trajectory rinto a condensed version rcond\nby pruning the middle portion of the CoT trace while retaining the proper head and tail segments:\nrcond= [ri]i∈Ω,Ω =\b\n1, . . . ,\u0004τn\n2\u0005\t\n∪\b\nn−\u0004τn\n2\u0005\n+ 1, . . . , n\t\n. (EPiC)\nHere,⌊·⌋denotes the floor function. The total number of retained thoughts, ⌊τn⌋, is equally divided between the head\nand tail segments, each of length ⌊τn\n2⌋. Please refer to Appendix A for visualizations of example reasoning traces after\ncondensation.\nCorrect \nAnwser\nWrong\nAnswer\nMiddle \nthoughts\nHead \nthoughts\nTail \nthoughts\nStart\nEnd\nFigure 5: Visualization of a reasoning trajectory generated\nbyDEEPSEEK-R1-D ISTILL -QWEN -7Bon the AQuA [ 57]\ndataset. The plot is produced using the trajectory landscape\ntool from [ 58], where each node represents the model’s rea-\nsoning state in a latent space after kthought steps. And\nxandyaxes correspond to two t-SNE-projected dimen-\nsions. The trajectory is segmented into three parts: the\nfirst 25% of steps (blue, “head thoughts\"), the middle 50%\n(orange,“middle thoughts\"), and the final 25% (violet,“tail\nthoughts\"). The correct answer is shown as a green star,\nwhile red cross denotes incorrect (distractor) answers.Although EPiC appears simple, its effectiveness can be demon-\nstrated through extensive empirical results in Sec. 5: It enables\nefficient reasoning training on condensed CoT datasets while\npreserving performance comparable to full-length supervision.\nIn what follows, we validate the soundness of EPiC using a CoT\nlandscape visualization tool [ 58], offering an interpretable view\nof how individual reasoning steps contribute to final answer\ngeneration. This stems from our observation that the explo-\nration stage ( i.e., the middle portion discarded by EPiC), which\nis often the longest part of the reasoning trace, can be redundant\nor even distracting sometimes.\nLeveraging the visualization tool from [ 58],Figure 5 illustrates\nthe landscape of a reasoning trajectory projected into a latent\nsemantic space, where darker regions represent intermediate\nreasoning states that are semantically closer to the correct an-\nswer. The xandyaxes correspond to two t-SNE-projected\ndimensions. The visualization shows that the initial and final\nreasoning steps tend to contribute more directly to the final an-\nswer, as reflected by their proximity to the correct solution. In\ncontrast, a large portion of intermediate steps often drift away\nfrom this path, potentially steering the model toward suboptimal conclusions. This supports our hypothesis that the\nmiddle portion of a CoT trace may be less informative, or even detrimental, to accurate reasoning.\nUnderstanding EPiC via mutual information (MI). To further understand which parts of the reasoning trajectory\nare most important for improving reasoning ability, we analyze EPiC using MI. Our goal is to quantify how much\ninformation different portions of the reasoning trace retain compared with the full reasoning trace. For a condensed\ntracercond= [ri]i∈Ω, we obtain a matrix representation EΩ= [eΩ\n1, . . . ,eΩ\nm]⊤∈Rm×dby feeding each trace through\na pretrained LLM and applying mean pooling over the final hidden states across the token dimension. Here, mdenotes\nthe number of samples used for MI evaluation and dis the hidden dimension of the model. We compute the mutual\ninformation between EΩandEFull, denoted as I(EΩ;EFull), using the Kraskov estimator [ 59], which approximates\nMI based on distances between nearest neighbors in the sample space. See Appendix B for more details. The MI score\nserves as a proxy for how informative the selected reasoning steps are compared with the full reasoning trace.\n--- Page 6 ---\nTable 1: Comparison of MI, computed using (A1),\nbetween the full reasoning trajectory and selected por-\ntions of the reasoning trajectory under various con-\ndensation methods and condensation ratios ( τ). The\nevaluation is performed on 2500 examples sampled\nfrom the OpenR1Math dataset using the QWEN 2.5-\n1.5B-I NSTRUCT model.\nMethod τ= 0.01τ= 0.05τ= 0.1τ= 0.5\nFull (τ= 1) 8.77\nRandom 0.56 1.90 2.64 4.57\nHoC 0.93 1.77 2.27 4.85\nMoC 0.64 1.39 1.84 3.81\nToC 0.43 1.06 1.46 3.05\nEPiC 3.07 3.57 4.06 8.70A higher I(EΩ;EFull)indicates that the condensed subset Ωpre-\nserves more of the information in the full reasoning trace, and thus\ncorresponds to a more effective condensation strategy. We compute\nMI between the full reasoning trace and different portions of the\nreasoning trajectory to assess the informativeness of each segment.\nSpecifically, given a CR (condensation ratio) τ, we define: Head-only\nCondensation (HoC ) asΩH={1, . . . ,⌊τn⌋},Tail-only Condensa-\ntion(ToC) asΩT={n− ⌊τn⌋+ 1, . . . , n }, and Middle-only Con-\ndensation (MoC ) asΩM=nj\n(1−τ)n\n2k\n+ 1, . . . , n −j\n(1−τ)n\n2ko\n.\nAs shown in Table 1 , EPiC consistently achieves the highest MI\nacross all condensation ratios τ∈ {0.01,0.05,0.1,0.5}, closely\nmatching the MI of the full reasoning trace. This indicates that\nEPiC effectively preserves the most informative parts of the reason-\ning trace. Notably, at τ= 0.5, EPiC attains an MI of 8.70, nearly matching the full trace MI of 8.77. This indicates\nthat EPiC preserves nearly all the semantic content of the full reasoning trajectory while using only half the tokens,\nproviding strong evidence that its structural selection strategy captures the most informative parts of the trace. We\nobserve similar results using Q WEN 2.5-M ATH-7B-I NSTRUCT , as shown in Table A2.\nAIME MATH500020406080100Accuracy (%)46.790.2\n6.758.6\n26.786.0\n26.788.6\n33.389.4\nPerturb-None\nPerturb-All\nPerturb-Head\nPerturb-Tail\nPerturb-Middle\nFigure 6: Final answer accuracy comparison\nfor reasoning training using QWEN 2.5-M ATH-7B-\nINSTRUCT on various perturbed CoT training sets,\nevaluated on AIME and MATH500 at test time. Per-\nturbations are applied to specific regions of the CoT\ntrace–head, tail, middle, entire trace-or not applied\nat all (no perturbation).Reasoning structure may matter more than content. To assess\nthe importance of different CoT stages, we conduct a perturbation\nanalysis inspired by EPiC. Rather than removing the reasoning steps\nnot in the selected index set Ω, we replace their content with randomly\nsampled text while preserving the structural layout of the full trace.\nWhen perturbing the reasoning content, we preserve reflection tokens,\noften realized as discourse markers such as wait andhmm[9], as they\nserve as important transitional and reflective cues that help maintain\ncoherence between thoughts.\nBuilding on the above, we investigate the impact of fixing the reason-\ning condensation pattern Ωwhile perturbing the unselected thoughts\n{ri}i/∈Ωby replacing their content (between reflection tokens) with\nrandomly sampled sentences from WikiText [ 60].Figure 6 shows\nhow reasoning training on the perturbed CoT dataset impacts model\nperformance. We evaluate five settings: perturbing (1) all reasoning\nsteps, or 50% of the trace located in the (2) head, (3) middle, or (4) tail\nsegments. Perturbing the middle region results in the smallest performance degradation, achieving 89.4% accuracy\non MATH500, compared to 90.2% with the original dataset. In contrast, perturbing the head or tail leads to more\nsignificant accuracy drops, while perturbing the entire trace severely degrades performance. These results support the\ncore idea of EPiC: The middle stage of a CoT trace is less critical than the head and tail, and much of its content can be\npruned or abstracted without substantially compromising reasoning ability.\n5 Experiments\n5.1 Experiment Setups\nTraining datasets. To demonstrate the effectiveness of EPiC in facilitating CoT training for enhanced reasoning\ncapabilities, we train models on two long-form CoT datasets distilled from DEEPSEEK-R1:(1) OpenR1Math [7]:\nThis dataset comprises 220k math problems, each paired with reasoning traces generated by DEEPSEEK-R1. Answers\nare verified using either a math verifier [ 61] orLLAMA-3.3-70B-I NSTRUCT to ensure correctness. In our experiments,\nwe use the default main subset, which includes 93k verified examples. (2) GeneralThoughts [16]: This dataset offers\na diverse reasoning traces beyond mathematics and coding, spanning natural sciences, humanities, social sciences,\nand general conversational reasoning. The traces are generated by a diverse set of strong LLMs, including O3-M INI,\nGEMINI -2-F LASH -THINKING , CLAUDE -3.7-S ONNET , and D EEPSEEK-R1.\nModel setups. In our experiments, we primarily use the non-reasoning LLM QWEN 2.5-M ATH-7B-I NSTRUCT\n[62] as the base model for SFT-based reasoning training, due to its strong mathematical capabilities. To evaluate the\nrobustness and generalizability of EPiC across different model initializations, we additionally conduct experiments with\ntwo alternative models: QWEN 2.5-7B-I NSTRUCT [63], which shares the same architecture but lacks math-specific\n--- Page 7 ---\ninstruction tuning, and LLAMA3.1-8B-I NSTRUCT [64], which differs in both architecture and pretraining corpus.\nThese variants assess EPiC’s effectiveness when initialized from a weaker math model or a different architecture.\nEvaluation benchmarks. To assess the acquired reasoning capabilities, we primarily evaluate models on three\nbenchmarks: (1) MATH500 [2]: A curated set of 500 multi-step problems from the OpenAI MATH benchmark,\ndesigned to measure mathematical reasoning ability. (2) AIME24 [65]: A set of 30 high school competition-level\nmath problems from the 2024 American Invitational Mathematics Examination (AIME). (3) GPQA-Diamond [5]:\nA graduate-level STEM benchmark consisting of multiple-choice questions in biology, physics, and chemistry. All\nproblems are written and verified by domain experts (PhD-level), providing a challenging testbed for evaluating general\nscientific reasoning beyond mathematics. For evaluation, we set a maximum generation length of 9000 tokens for both\nMATH500 andAIME24 , and 4000 tokens for GPQA-D IAMOND . Decoding is performed using nucleus sampling\nwith a temperature of 0.6 and top- pof 0.95, following [ 3]. In addition to final answer accuracy, we also assess reasoning\ngeneration quality using two auxiliary metrics: (1) the length of the generated reasoning traces, and (2) the number\nof reflection tokens, which serve as strong indicators of reasoning ability, e.g., the “Aha Moment” emphasized in\nDEEPSEEK-R1 [3].\nBaselines. To evaluate the effectiveness of EPiC, we compare it against several baseline condensation strategies: (1)\nRandom: Randomly selects a subset of reasoning steps based on the same condensation ratio τ, without considering\ntheir position within the trajectory. (2) HoC (Head-only Condensation): Retains only the first ⌊τn⌋steps of the\nreasoning trajectory, where nis the total number of steps. (3) ToC (Tail-only Condensation): Retains only the last\n⌊τn⌋steps. (4) TokenSkip [38]: A recent token-level condensation that scores and selects important tokens across\nthe trace for retention. Unless otherwise specified, the condensation ratio τis set to 50% throughout our experiments.\nPlease refer to the training and implementation details in Appendix C.\n5.2 Experiment Results\nTable 2: Performance comparison of EPiC against full dataset training and baseline condensation methods across three reasoning\nbenchmarks: Math500, AIME24, and GPQA-Diamond. Each benchmark reports both accuracy (%) , the average number of\ngenerated tokens (#Toks) and the average number of reflection tokens (#Rtoks). All models are trained via SFT from Qwen2.5-Math-\n7B-Instruct, using a fixed condensation ratio of 50%. The reasoning training is conducted on two datasets, OPENR1M ATH and\nGENERAL THOUGHT 195 K, respectively. The final column reports the total training time in hours.\nMethods Math500 AIME24 GPQA -Diamond Time\nAcc #Toks #RToks Acc #Toks #RToks Acc #Toks #RToks (Hours)\nw/o SFT 82.6 696.6 0.0 3.3 1624.2 0.1 34.9 1331.5 0.0 -\nOpenR1Math [7]\nFull dataset 90.2 3213.5 17.6 46.7 7365.2 43.7 38.4 3817.1 37.0 51.9\nTokenskip 81.0 4861.5 28.0 23.3 8499.4 48.3 31.3 3896.7 30.4 30.0\nRandom 88.0 3221.9 17.2 33.3 7382.8 45.8 36.4 3802.7 37.8 32.4\nHoC 89.6 3178.3 17.3 33.3 7549.0 45.9 40.4 3753.5 39.5 34.2\nToC 84.6 3088.6 16.4 33.3 7141.4 41.8 43.9 3472.8 28.7 32.0\nEPiC 90.2 3109.1 17.5 40.0 7330.8 45.4 41.9 3725.7 37.8 34.0\nGeneralThought195k [16]\nFull dataset 87.0 3072.7 23.3 26.7 7613.6 50.2 40.4 3494.9 46.2 48.8\nTokenskip 58.4 4281.8 0.0 0.0 9000.0 0.0 29.3 3653.5 0.0 32.0\nRandom 58.2 3621.8 27.0 0.0 7591.9 52.0 35.0 3329.4 41.4 32.0\nHoC 85.8 3252.7 18.3 26.7 8004.9 43.8 37.4 3490.1 40.1 33.5\nToC 75.4 2963.8 19.1 13.3 6991.3 46.6 41.4 3182.5 31.5 31.6\nEPiC 86.0 2874.2 18.5 20.0 7967.2 46.9 42.4 3388.3 40.8 32.3\nPerformance overview of EPiC vs. full-data training and condensation baselines. InTable,2 , we evaluate the\nperformance of EPiC under a 50% condensation ratio across three reasoning benchmarks: MATH500,AIME24 , and\nGPQA-D IAMOND . We compare against baselines trained on two datasets: OPENR1M ATH, which focuses exclusively\non mathematical problems, and GENERAL THOUGHT 195 K, which contains reasoning traces spanning diverse domains\nsuch as science, humanities, and general knowledge. All models are fine-tuned from QWEN 2.5-M ATH-7B-I NSTRUCT .\nFirst, EPiC matches the performance of full-data training while significantly reducing training time up to 34%. For\nexample, when trained on OPENR1M ATH, EPiC achieves 90.2% accuracy on MATH500, identical to the full model,\n--- Page 8 ---\nbut requires only 34.0 hours of training compared to 51.9 hours for the full dataset. On GENERAL THOUGHT 195 K,\nEPiC also maintains comparable performance (86.0% vs. 87.0%) while reducing training time from 48.8 to 32.3 hours,\ndemonstrating substantial efficiency gains without performance loss.\nIn-domain evaluation on GPQA-D IAMOND further validates the strength of EPiC. Since GENERAL THOUGHT 195 K\nincludes STEM-related reasoning ( e.g., physics and biology), GPQA-D IAMOND serves as a natural in-domain test.\nAs shown, EPiC achieves 42.4% accuracy, outperforming the full-data baseline of 40.4%. Even when treated as an\nout-of-domain task–training only on math-focused OPENR1M ATH–EPiC generalizes better than full-data training\n(41.9% vs. 38.4%), suggesting that pruning the middle portion of reasoning traces may help improve generalization.\nCompared to other structural condensation baselines, EPiC also exhibits clear advantages. Both HoC and ToC, which\npreserve only the head or tail of the reasoning trace, perform noticeably worse than EPiC across all tasks and datasets.\nThese results confirm that preserving both the beginning and end of the reasoning trace, while discarding the middle, is\na highly effective and efficient strategy.\nLast but not least, we analyze reasoning behavior through the lens of generation length. Across all benchmarks, EPiC\nproduces responses of comparable length to those generated by models trained on the full dataset, indicating that it\neffectively preserves reasoning complexity. For example, on MATH500, EPiC achieves the same 90.2% accuracy as the\nfull model while generating, on average, only 100 fewer tokens (3109.1 vs. 3213.5). In addition, we observe that EPiC\nmaintains a similar number of reflection tokens, such as \"wait\", \"hmm\", and other metacognitive markers, compared to\nthe full model, further confirming that its condensed traces still elicit rich and deliberate reasoning behavior during\ninference. This result is particularly noteworthy given that a 50% condensation ratio was applied to the CoT training\ndata. Despite this reduction, the model’s ability to generate complete and coherent reasoning traces at test time remains\nessentially lossless compared to full-data training. Appendix D also includes qualitative generation examples illustrate\nthat EPiC achieves comparable reasoning quality to the full-data model.\n1% 10% 50% 75% 100%\nCondensation Ratio10203040Accuracy (%)\n1% 10% 50% 75% 100%\nCondensation Ratio657075808590Accuracy (%)\nEPiC\nHoC\nToC\nw/o SFT\n1% 10% 50% 75% 100%\nCondensation Ratio343638404244Accuracy (%)\n1% 10% 50% 75%\nCondensation Ratio0.00.51.01.52.02.53.03.54.0Speedup over Full Trace3.5×\n2.7×\n1.5×\n1.2×3.7×\n2.7×\n1.5×\n1.2×3.7×\n3.1×\n1.6×\n1.3×EPiC\nHoC\nToC\n(a) AIME24 (b) MATH500 (c)GPQA-Diamond (d)Speedup\nFigure 7: Reasoning accuracy of CoT training at different condensation ratios using EPiC, HoC, and ToC, on three benchmarks: (a)\nAIME24, (b)MATH500, and (c)GPQA-Diamond. All models are fine-tuned using Qwen2.5-Math-7B-Instruct on OpenR1Math.\nThe dashed line indicates performance without SFT, and the 100% condensation ratio refers to the full training dataset. (d)shows the\ntraining speedup relative to full-trace fine-tuning across condensation ratios for each method.\nPerformance against CoT condensation ratios. InFigure 7-(a,b,c) , we present the performance of reasoning\ntraining under varying CoT condensation ratios using different condensation methods (EPiC, HoC, ToC), evaluated on\nthe reasoning benchmarks AIME24, MATH500, and GPQA-Diamond. As expected, reasoning performance generally\nimproves as the condensation ratio increases ( i.e., more thoughts are retained), where 100% corresponds to the full-data\ntraining scenario. Notably, GPQA-Diamond exhibits a sweet spot at 50% condensation, where the accuracy even\nsurpasses that of the full-dataset baseline. This suggests that moderate pruning may help eliminate noisy or redundant\nreasoning, thereby improving generalization. We also observe that the use of CoT training data plays a key role in\ndriving reasoning accuracy, compared to the initial model without SFT. Across all benchmarks and condensation levels,\nEPiC consistently outperforms both HoC and ToC, reinforcing the results shown in Table 2. Furthermore, Figure 7-(d)\nreports the corresponding training speedups relative to full CoT fine-tuning. As expected, lower condensation ratios\nlead to faster training. At 50% condensation, EPiC delivers a substantial 1.5 ×improvement in training efficiency over\nfull-data training.\nPerformance against problem difficulty levels. InFigure 8 , we present a fine-grained breakdown of model perfor-\nmance across five difficulty levels on the Math500 benchmark. The top plot shows accuracy comparisons, while the\nbottom plot reports the corresponding average number of generated tokens. All models are trained using QWEN 2.5-\nMATH-7B-I NSTRUCT on the OpenR1Math dataset. As expected, accuracy generally decreases and generation length\nincreases as problem difficulty rises. This trend holds consistently across all condensation strategies, reflecting the\nintrinsic complexity of harder problems. However, EPiC performs better than the full-data baseline on the most difficult\nLevel 5 problems. This indicates that CoT condensation via EPiC did notdisproportionately disadvantage on the harder\nlevels. In addition, as evidenced by the bottom plots, all condensation methods applied to the CoT training datasets\ndo not hinder the reasoning generation capability of the resulting models after training, across all problem difficulty\n--- Page 9 ---\nFigure 8: Accuracy and generation length across difficulty levels on the Math500 benchmark. (Top) Accuracy comparison of\ncondensation methods (50% condensation ratio) and full-data baseline across five difficulty levels. (Bottom) Average number of\ngenerated tokens per method and difficulty level. All models are fine-tuned from QWEN 2.5-M ATH-7B-I NSTRUCT on OpenR1Math.\nlevels. This also echoes the finding in Table 2 that reasoning ability can be effectively acquired using shorter CoT traces\nwithout compromising generation quality.\nEPiC is resilient to non-reasoning base model choice. To evaluate the robustness of EPiC under different ini-\ntialization conditions, we assess its performance when fine-tuning two distinct pretrained backbones: QWEN 2.5-\n7B-I NSTRUCT [63] and LLAMA3.1-8B-I NSTRUCT [64]. As shown in Table 3 , EPiC consistently achieves strong\nperformance despite using only 50% of the original reasoning traces. Compared to full-data fine-tuning, it attains\ncomparable or even superior accuracy while significantly reducing training time, saving 19.1 hours on LLaMA3 and 19\nhours on Qwen. These results indicate that EPiC generalizes well across base models.\nTable 3: Performance comparison between full-dataset training and EPiC on the OpenR1Math dataset across different backbone\nmodels, with similar format as Table 2.\nMethods Math500 AIME24GPQA\nDiamondTime\nAcc #Toks Acc #Toks Acc #Toks (Hours)\nQWEN 2.5-7B-I NSTRUCT\nw/o SFT 76.4 583.0 10.0 1061.8 30.8 577.8 -\nFull dataset 84.4 3499.4 26.7 7590.1 35.9 3798.7 52.1\nEPiC 84.2 3378.4 26.7 7839.1 35.9 3760.1 33.2\nLLAMA3.1-8B-I NSTRUCT\nw/o SFT 47.4 1311.7 3.3 3180.6 29.8 1060.4 -\nFull dataset 78.2 9000.0 13.3 9000.0 27.2 4000.0 56.6\nEPiC 75.0 9000.0 16.7 9000.0 29.8 4000.0 37.5\n6 Conclusion\nIn this work, we investigate a novel perspective on reasoning supervision by proposing EPiC , a simple yet effective\nthought-level condensation strategy that preserves only the head andtailportions of long CoT traces. Motivated by\nthe observation that intermediate reasoning steps often contain redundant or exploratory content, we systematically\nevaluate the informativeness of different reasoning segments using mutual information analysis. Our findings reveal that\nremoving the middle portion of CoT traces leads to significant efficiency gains without sacrificing model performance.\nExtensive experiments across multiple reasoning benchmarks demonstrate that EPiC achieves comparable or even\nsuperior performance to full-dataset training, while reducing training time. The method proves robust across datasets,\ndifficulty levels, and model architectures, offering a practical and interpretable solution for improving the efficiency of\nreasoning fine-tuning. Efficient reasoning supervision is increasingly important as the cost of full CoT training becomes\nprohibitive. Limitations and broader impacts are further discussed in Appendix E andAppendix F .\n--- Page 10 ---\nAcknowledgments\nThe work of J. Jia, C. Fan, and S. Liu is supported by the National Science Foundation (NSF) CISE Core Program\nAward IIS-2207052, the NSF CAREER Award IIS-2338068, the Cisco Faculty Research Award, the ARO Award\nW911NF2310343, and the Amazon Research Award for AI in Information Security.\nBibliography\n[1]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\nsystems , 35:24824–24837, 2022.\n[2]Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference on\nLearning Representations , 2023.\n[3]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948 , 2025.\n[4]OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander\nNeitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea\nVallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin\nNair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys\nMinaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi,\nCary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea V oss, Chen Shen, Chong Zhang, Chris\nKoch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel\nLevy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li,\nDragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell,\nEric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos\nTsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas\nChabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin,\nHessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian\nOsband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki,\nJames Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu,\nJoaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan\nGordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal,\nKarina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin\nLu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian\nWeng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz,\nMadelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt\nKaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y . Guan, Mengyuan Xu, Mengyuan Yan, Mia\nGlaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail\nPavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil\nChowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk,\nOlivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall\nLin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown,\nRoshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel\nMiserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao,\nShengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin,\nSuchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders,\nTejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi,\nTrapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr\nPong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu,\nYining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li.\nOpenai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720 .\n[5]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\nMichael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on\nLanguage Modeling , 2024.\n--- Page 11 ---\n[6]Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint\narXiv:2501.12599 , 2025.\n[7]Hugging Face. Open r1: A fully open reproduction of deepseek-r1, January 2025. URL https://github.com/\nhuggingface/open-r1 .\n[8] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025.\n[9]Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer,\nPercy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint\narXiv:2501.19393 , 2025.\n[10] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning.\narXiv preprint arXiv:2502.03387 , 2025.\n[11] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei\nZhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv\npreprint arXiv:2412.21187 , 2024.\n[12] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li,\nZhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint\narXiv:2501.18585 , 2025.\n[13] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching\nalgorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066 , 2022.\n[14] Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller language\nmodels. Findings of the Association for Computational Linguistics: ACL 2023 , pages 7059–7073, 2023.\n[15] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards\nmulti-step reasoning. In International Conference on Machine Learning , pages 10421–10430. PMLR, 2023.\n[16] GeneralReasoning. Generalthought-195k. https://huggingface.co/datasets/GeneralReasoning/\nGeneralThought-195K , 2024. Accessed: 2025-05-06.\n[17] NovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1,\n2025. Accessed: 2025-01-09.\n[18] Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation.\nwww.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation, 2025.\nAccessed: 2025-01-22.\n[19] Appu Shaji Hicham Badri. Re-distilling smaller deepseek r1 models for better performance, January 2025. URL\nhttps://mobiusml.github.io/r1_redistill_blogpost/ .\n[20] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian,\nand Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143 ,\n2025.\n[21] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji,\nYingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint\narXiv:2501.11284 , 2025.\n[22] Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen,\nBenyou Wang, et al. The first few tokens are all you need: An efficient and effective unsupervised prefix fine-tuning\nmethod for reasoning models. arXiv preprint arXiv:2503.02875 , 2025.\n[23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https:\n//arxiv.org/abs/2201.11903 .\n[24] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical\nanalysis of compute-optimal inference for problem-solving with language models, 2024. URL https://arxiv.\norg/abs/2408.00724 .\n[25] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao,\nZhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin,\nFucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\n--- Page 12 ---\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang,\nJingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai\nHu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue\nZhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang,\nRuizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,\nShengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang,\nWenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen,\nXiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin,\nX. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou,\nXianzu Wang, Xinxia Shan, Y . K. Li, Y . Q. Wang, Y . X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao,\nYaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia\nHe, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong Xu, Yanping\nHuang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui\nRen, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan,\nZhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng\nXu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025. URL https://arxiv.org/abs/2501.12948 .\n[26] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more\neffective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314 .\n[27] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https:\n//arxiv.org/abs/2203.11171 .\n[28] Pranjal Aggarwal, Aman Madaan, Yiming Yang, et al. Let’s sample step by step: Adaptive-consistency for\nefficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860 , 2023.\n[29] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia Mirhoseini.\nLarge language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787 ,\n2024.\n[30] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/\nabs/2305.10601 .\n[31] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang,\nXuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan.\nDeepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree\nsearch, 2024. URL https://arxiv.org/abs/2408.08152 .\n[32] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generat-\ning sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations ,\n2023. URL https://openreview.net/forum?id=hH36JeQZDaO .\n[33] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.\nURLhttps://arxiv.org/abs/2303.17651 .\n[34] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov,\nand Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models,\n2024. URL https://arxiv.org/abs/2406.16838 .\n[35] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm\nsurpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703 , 2025.\n[36] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with reinforcement\nlearning. arXiv preprint arXiv:2503.04697 , 2025.\n[37] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng\nTao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570 ,\n2025.\n--- Page 13 ---\n[38] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought\ncompression in llms. arXiv preprint arXiv:2502.12067 , 2025.\n[39] Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu\nZhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589 , 2025.\n[40] Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: Submodular information\nmeasures based active learning in realistic scenarios. Advances in Neural Information Processing Systems , 34,\n2021.\n[41] Krishnateja Killamsetty, Durga Sivasubramanian, Baharan Mirzasoleiman, Ganesh Ramakrishnan, Abir De, and\nRishabh Iyer. Grad-match: A gradient matching based data subset selection for efficient learning. arXiv preprint\narXiv:2103.00123 , 2021.\n[42] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and\nNicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 ,\n2021.\n[43] Abdul Hameed Azeemi, Ihsan Ayyub Qazi, and Agha Ali Raza. Dataset pruning for resource-constrained spoofed\naudio detection. Proc. Interspeech 2022 , pages 416–420, 2022.\n[44] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. In\nECCV , pages 137–153. Springer, 2020.\n[45] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure\nLeskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. arXiv preprint\narXiv:1906.11829 , 2019.\n[46] Dongmin Park, Dimitris Papailiopoulos, and Kangwook Lee. Active learning is a strong baseline for data subset\nselection. In Has it Trained Yet? NeurIPS 2022 Workshop , 2022.\n[47] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning\nmodels. In ICML . PMLR, 2020.\n[48] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint\narXiv:1812.05159 , 2018.\n[49] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important\nexamples early in training. Advances in Neural Information Processing Systems , 34:20596–20607, 2021.\n[50] Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, and Mao Yang. Fewer is more: Boosting llm\nreasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901 , 2023.\n[51] Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Tianlin Li, Weipeng Jiang, and Yang Liu. Staff: Specu-\nlative coreset selection for task-specific fine-tuning. In The Thirteenth International Conference on Learning\nRepresentations .\n[52] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential\ndata for targeted instruction tuning. arXiv preprint arXiv:2402.04333 , 2024.\n[53] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\nYu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems , 36:\n55006–55021, 2023.\n[54] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720 , 2024.\n[55] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue\nCheng, Huatong Song, et al. Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning\nsystems. arXiv preprint arXiv:2412.09413 , 2024.\n[56] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle,\nYuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt\ncompression. arXiv preprint arXiv:2403.12968 , 2024.\n[57] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning\nto solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 , 2017.\n[58] Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, Jian Tang, and Bo Han. Land-\nscape of thoughts: Visualizing the reasoning process of large language models. arXiv preprint arXiv:2503.22165 ,\n2025.\n--- Page 14 ---\n[59] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical Review\nE—Statistical, Nonlinear, and Soft Matter Physics , 69(6):066138, 2004.\n[60] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n[61] Hynek Kydlí ˇcek. Math-verify: Math verification library, 2024. URL https://github.com/huggingface/\nmath-verify . Apache-2.0 License.\n[62] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,\nJingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via\nself-improvement. arXiv preprint arXiv:2409.12122 , 2024.\n[63] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\nHuang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024.\n[64] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.\n[65] MAA Committees . Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/\nAIME_Problems_and_Solutions .\n--- Page 15 ---\nAppendix\nA Visualization of Condensed Reasoning Examples\nTo provide qualitative insight into how EPiC condense long reasoning traces, Table A1 presents an example from the\ncondensed training dataset based on OPENR1M ATH. This example highlights the head and tail portions retained by\nEPiC, with the pruned middle segment shown in red.\nTable A1: Visualization of a training example with EPiC. xis the input question, ris the full CoT reasoning trace, and yis the final\nanswer. The red-highlighted reasoning steps in rcorrespond to the middle portion that is pruned, while the head and tail segments\nare retained for training.\nxA ship traveling along a river has covered $24 km$ upstream and\n$28 km$ downstream, ... Determine the speed of the ship in still\nwater and the speed of the river.\nr<think>\\nOkay, so I need to find the speed of the ship in still\nwater and the speed of the river. Let me start by recalling that\nwhen a ship is moving upstream, its effective speed is the speed\nof the ship minus the speed of the river. Conversely, when\nmoving downstream, its effective speed is the ship’s speed plus\nthe river’s speed. \\n\\nLet me denote the speed of the ship in\nstill water as \\( v \\) (in km/h) and the speed of the river as \\(\nr \\) (also in km/h). Then, the upstream speed would be \\( v - r\n\\), and the downstream speed would be \\( v + r \\).\\n\\nThe problem\nmentions three different journeys:\\n\\n1. 24 km upstream and 28\nkm downstream, taking half an hour less than the second\njourney.\\n2. ... Let me first work on the first equation:\\n\\n\\(\n\\frac{24}{v - r} + \\frac{28}{v + r} = \\frac{30}{v - r} +\n\\frac{21}{v + r} - 0.5 \\)\\n\\nLet me subtract \\( \\frac{24}{v - r}\n+ \\frac{28}{v + r} \\) from both sides, but maybe it’s better to\nbring all terms to the left side:\\n\\n\\( \\frac{24}{v - r} +\n\\frac{28}{v + r} - \\frac{30}{v - r} - \\frac{21}{v + r} + 0.5 = 0\n\\)\\n\\nSimplify the terms:\\n\\nFor the upstream terms: \\( 24 - 30\n= -6 \\), so \\( \\frac{-6}{v - r} \\)\\n\\nFor the downstream terms:\n\\( 28 - 21 = 7 \\), so \\( \\frac{7}{v + r} \\)\\n\\nSo, the equation\nbecomes:\\n\\n\\( \\frac{-6}{v - r} + \\frac{7}{v + r} + 0.5 = 0\n\\)\\n\\nSimilarly, for the second equation:\\n\\n\\( \\frac{24}{v - r}\n+ \\frac{28}{v + r} - \\frac{15}{v - r} - \\frac{42}{v + r} - 0.5 =\n0 \\)\\n\\nSimplify:\\n\\nUpstream terms: \\( 24 - 15 = 9 \\), so \\(\n\\frac{9}{v - r} \\)\\n\\nDownstream terms: \\( 28 - 42 = -14 \\), so\n\\( \\frac{-14}{v + r} \\)\\n\\nThus:\\n\\n\\( \\frac{9}{v - r} -\n\\frac{14}{v + r} - 0.5 = 0 \\)\\n\\nNow, we have two\nequations:\\n\\n1. \\( -\\frac{6}{v - r} + \\frac{7}{v + r} + 0.5 = 0\n\\)\\n2. \\( \\frac{9}{v - r} - \\frac{14}{v + r} - 0.5 = 0 \\) ... ...\n\\( 2v = 20 \\) ˘21d2 \\( v = 10 \\)\\n\\nSubstituting back into \\( v +\nr = 14 \\):\\n\\n\\( 10 + r = 14 \\) ˘21d2 \\( r = 4 \\)\\n\\nSo, the speed\nof the ship in still water is 10 km/h, and the speed of the river\nis 4 km/h.\\n\\nLet me verify these results by plugging them back\ninto the original time equations.\\n\\nFirst journey: 24/(10-4) +\n28/(10+4) = 24/6 + 28/14 = 4 + 2 = 6 hours.\\n\\nSecond journey:\n30/(10-4) + 21/(10+4) = 30/6 + 21/14 = 5 + 1.5 = 6.5 hours. So,\n6.5 - 0.5 = 6 hours, which matches the first journey.\nGood.\\n\\nThird journey: 15/(10-4) + 42/(10+4) = 15/6 + 42/14 =\n2.5 + 3 = 5.5 hours. Then, 5.5 + 0.5 = 6 hours, which also\nmatches the first journey. Perfect.\\n\\nTherefore, the solution\nis correct.\\n\\n**Final Answer**\\nThe speed of the ship in still\nwater is \\boxed{10} km/h and the speed of the river is \\boxed{4}\nkm/h.\\n </think>\ny***Fianl Answer*** , ..., The speed of the ship in still water is\n\\(\\boxed{10}\\) km/h and the speed of the river is \\(\\boxed{4}\\)\nkm/h.\n--- Page 16 ---\nB Implementation Details of Mutual Information and Extended Analysis\nTable A2: Comparison of MI, computed using (A1),\nbetween the full reasoning trajectory and selected por-\ntions of the reasoning trajectory under various con-\ndensation methods and condensation ratios ( τ). The\nevaluation is performed on 2500 examples sampled\nfrom the OpenR1Math dataset using the QWEN 2.5-\n7B-I NSTRUCT model.\nMethod τ= 0.01τ= 0.05τ= 0.1τ= 0.5\nFull (τ= 1) 8.79\nRandom 0.41 1.79 2.48 4.27\nHoC 1.03 1.97 2.39 4.90\nMoC 0.41 1.25 1.71 3.65\nToC 0.46 1.19 1.50 3.07\nEPiC 3.11 3.58 4.07 8.67Implementation details of mutual information. To identify which\nparts of the reasoning trajectory contribute most to model learning,\nwe analyze the mutual information (MI) between different portions\nof the trace and the full CoT trajectory. For a condensed trace\nrcond= [ri]i∈Ω, we encode it using a pretrained LLM and apply\nmean pooling over the token dimension to obtain a representation\nmatrix EΩ∈Rm×d, where mis the number of samples and dis the\nhidden dimension. We then compute MI between EΩand the full\ntrace embedding EFullasI(EΩ;EFull)using the Kraskov k-nearest\nneighbor estimator [59], which approximates MI based on distances\nbetween nearest neighbors in the sample space. This non-parametric\nmethod is well-suited for high-dimensional representations and pro-\nvides a robust estimate of MI without requiring density assumptions.\nWe use k= 5in all experiments. For each i∈ {1, . . . , m }, we compute the radius ρiusing the ℓ∞norm as\nρi= min\nj∈Ei,kmaxn\n∥eΩ\ni−eΩ\nj∥∞,∥eFull\ni−eFull\nj∥∞o\n,\nwhere eΩ\niandeFull\niare the ith rows of EΩandEFull, respectively. Ei,k⊆ {1, . . . , m } \\ {i}denotes the indices of the\nk-nearest neighbors of the joint embedding (eFull\ni,eΩ\ni)in the joint space R2d. Using this radius ρi, we then count the\nnumber of neighbors of eΩ\niandeFull\nithat lie within ρiin their respective marginal spaces\nnΩ\ni=\f\f\fn\nj̸=i;∥eΩ\ni−eΩ\nj∥∞< ρio\f\f\f, nFull\ni=\f\f\fn\nj̸=i;∥eFull\ni−eFull\nj∥∞< ρio\f\f\f.\nFinally, we estimate the mutual information as\nI(EΩ;EFull) =ψ(k) +ψ(m)−1\nmmX\ni=1h\nψ(nΩ\ni+ 1) + ψ(nFull\ni+ 1)i\n, (A1)\nwhere ψ(·)is the digamma function. The MI score (A1) serves as a proxy for how informative the selected reasoning\nsteps are compared with the full reasoning trace.\nAdditional results for mutual information. To further validate the robustness of our mutual information analysis,\nwe perform an additional evaluation using the model, QWEN 2.5-7B-I NSTRUCT , to compute the latent representations\nEΩ. As shown in Table A2, EPiC consistently achieves the highest MI across all tested condensation ratios τ∈\n{0.01,0.05,0.1,0.5}, outperforming all other condensation baselines. Remarkably, at τ= 0.5, EPiC attains an MI of\n8.67, which is almost indistinguishable from the MI of the full reasoning trace (8.79). These findings are consistent\nwith the results reported in Table 1, and further corroborate that EPiC preserves the majority of semantic content in the\nreasoning trace while using only 50% of the tokens. This highlights the effectiveness of our method in maintaining\nreasoning fidelity under significant token budget constraints.\nC Additional Experimental Details\nC.1 Training setup\nSupervised fine-tuning setup. We adopt a unified training configuration across all base models and data condensation\nstrategies to ensure fair comparison. All models are fine-tuned for 3 epochs using the AdamW optimizer with a learning\nrate of 5×10−5, weight decay of 0.0001, and a linear learning rate scheduler with 10% warmup. Training is performed\non 8 NVIDIA A6000 GPUs with a global batch size of 16, achieved via a per-device batch size of 1 and gradient\naccumulation over 2 steps. We use bfloat16 precision and enable gradient checkpointing for memory efficiency. To\nimprove throughput, long sequences are packed into fixed-length inputs with a maximum context length of 32,768\ntokens.\nC.2 Inference setup\nFor evaluation, we set a maximum generation length of 9000 tokens for both MATH500 andAIME24 , and 4000\ntokens for GPQA-D IAMOND . Decoding is performed using nucleus sampling with a temperature of 0.6 and top- pof\n0.95, following [ 3]. For AIME24 , we sample 32 responses per query and report pass@1 . For all other benchmarks, we\nreport accuracy from a single sampled response.\n--- Page 17 ---\nD Visualizations of Model Responses after Training with EPiC\nTo qualitatively evaluate the effectiveness of EPiC, we present representative examples of model-generated responses\nfrom the AIME24 benchmark in Table A3. The example is generated by a model fine-tuned using the condensed dataset\nproduced by EPiC. In the visualization, the input question ( x), the reasoning trace ( r), and the final answer ( y) are shown.\nWe highlight reflection cues and structural tokens ( e.g.,Wait ,Therefore ,<think> ,</think> ) inredto indicate the\nmodel’s internal reasoning structure. The correct final answer is marked in green. The visualization demonstrates that\ndespite being trained on pruned traces that remove the middle portion of reasoning steps, the model retains coherent\nreasoning abilities and produces accurate answers, without hurting its capacity for reflection or disrupting its thinking\nstructure. This supports that EPiC enables reasoning enhancement without compromising the model’s reflective capacity\nor overall reasoning ability.\nTable A3: Visualization of a model-generated response from the AIME24 benchmark, produced by a model fine-tuned using EPiC.\nxdenotes the input question, rthe generated reasoning trace, and ythe final answer. Red highlights reflection cues ( e.g., Wait,\nTherefore) and structural tokens (<think>, </think>) marking the reasoning span. The correct final answer is shown in green.\nxLetA,B,C, and Dbe point on the hyperbolax2\n20−y2\n24= 1such\nthat ABCDis a rhombus whose diagonals intersect at the origin.\nFind the greatest real number that is less than BD2for all such\nrhombi.\nr<think>Okay, so I have this problem about a rhombus ABCD on the\nhyperbola x2/20 - y2/24 = 1. The diagonals intersect at the\norigin, and I need to find the greatest real number less than BD ²\nfor all such rhombi. Hmm, let me start by recalling some\nproperties of hyperbolas and rhombuses.\\n\\n First, ... But wait ,\nthe problem says the diagonals intersect at the origin. Since\nthe diagonals of a rhombus bisect each other, that means the\norigin is the midpoint of both diagonals. ... Wait ,BD is from B\nto D. Coordinates of B: ... Therefore , the greatest real number\nless than BD2for all such rhombi is 480. So, the final answer\nis \\boxed{480}.\\n\\n**Final Answer**\\n\\boxed{480} </think>\ny..., Thus, the greatest real number less than \\( BD2\\) for all\nsuch rhombi is \\(\\boxed{480}\\)\nE Limitations\nWhile EPiC presents a practical approach to reducing training costs through thought-level condensation, several\nlimitations remain. First, it relies on a heuristics-based segmentation of CoT traces into head, middle, and tail, which\nmay not align with the true semantic structure of reasoning. Second, the condensation ratio is globally defined and does\nnot adapt to the difficulty of individual examples, more challenging problems may benefit from retaining additional\nreasoning steps. Third, EPiC is implemented via supervised fine-tuning and does not explore reinforcement learning\n(RL)-based training, which could enable more dynamic, reward-driven condensation. Finally, our evaluation is limited\nto structured mathematical reasoning; extending EPiC to domains such as open-ended QA or legal reasoning requires\nfurther validation.\nF Broader Impact\nThis work aims to improve the efficiency of reasoning supervision in large language models by condensing CoT-type\nreasoning traces. By reducing the length of training trajectories, our approach can lower computational costs and carbon\nfootprint, making reasoning-enhanced model training more accessible to researchers and practitioners with limited\nresources. However, efficiency gains through condensation may come at the expense of preserving subtle but important\nreasoning patterns, potentially affecting model robustness, interpretability, or fairness. These potential trade-offs\nhighlight the need for further investigation into how condensation impacts downstream performance across diverse\ntasks, domains, and user groups. We encourage future work to assess these dimensions and to develop techniques that\nbalance efficiency with reliability and inclusivity.",
  "text_length": 75790
}