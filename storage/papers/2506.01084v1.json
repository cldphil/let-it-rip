{
  "id": "http://arxiv.org/abs/2506.01084v1",
  "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
  "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
  "authors": [
    "Saibo Geng",
    "Nathan Ranchin",
    "Yunzhen yao",
    "Maxime Peyrard",
    "Chris Wendler",
    "Michael Gastpar",
    "Robert West"
  ],
  "published": "2025-06-01T17:03:02Z",
  "updated": "2025-06-01T17:03:02Z",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01084v1",
  "full_text": "--- Page 1 ---\nzip2zip : Inference-Time Adaptive Vocabularies for\nLanguage Models via Token Compression\nSaibo Geng1∗Nathan Ranchin1∗Yunzhen Yao1Maxime Peyrard4\nChris Wendler1,2Michael Gastpar1Robert West1,3\n1EPFL2Northeastern University3Microsoft\n4Université Grenoble Alpes, CNRS, Grenoble INP, LIG\n{saibo.geng, nathan.ranchin, yunzhen.yao, michael.gastpar, robert.west}@epfl.ch\nmaxime.peyrard@univ-grenoble-alpes.fr ch.wendler@northeastern.edu\nAbstract\nTokenization efficiency plays a critical role in the performance and cost of large\nlanguage models (LLMs), yet most models rely on static tokenizers optimized\non general-purpose corpora. These tokenizers’ fixed vocabularies often fail to\nadapt to domain- or language-specific inputs, leading to longer token sequences\nand higher computational costs. We introduce zip2zip , a framework that enables\nLLMs to dynamically adjust the token vocabulary at inference time, allowing for\nfewer generated tokens and thus faster inference. zip2zip consists of three key\ncomponents: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that\nincrementally merges co-occurring tokens into reusable hypertokens on the fly; (2)\nan embedding layer that computes embeddings for newly formed hypertokens at\nruntime; and (3) a causal language modeling variant that trains the model to operate\non hypertokenized, compressed sequences. We show that an existing LLM can\nbe zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting\nzip2zip LLMs effectively learn to use hypertokens at inference time, reducing input\nand output sequence length by 20–60%, with significant improvements in inference\nlatency. Code will be released at epfl-dlab/zip2zip.\n1 Introduction\nLarge language models (LLMs) have shown impressive versatility across a broad spectrum of tasks\nand domains [Brown et al., 2020, Bubeck et al., 2023], including biomedical tests [Nori et al., 2023],\nmathematical reasoning [Frieder et al., 2023], programming [Jiang et al., 2024], and multiple human\nlanguages. A critical underlying component of this flexibility is the tokenizer, which defines the\nmodel’s vocabulary and governs how raw text is converted into token sequence fed to the model.\nThe efficiency of the tokenization scheme—i.e., how compactly a text is represented as tokens—has\nsignificant impact on model performance. In particular, a more compact tokenization yields three key\nbenefits: (1) larger effective context window; (2) lower computational (and thus monetary) cost; and\n(3) shorter response times.\nDespite its importance, the tokenizer used in most LLMs produces a fixed, static vocabulary using\nalgorithms such as Byte Pair Encoding [Sennrich et al., 2016] over large-scale, general-purpose web\ncorpora. While this globally optimized vocabulary performs reasonably well on average, it often fails\nto adapt to domain-specific or language-specific distributions [Ahia et al., 2023, Petrov et al., 2023],\nwhere the text distribution diverges significantly from the pretraining data. The resulting mismatch\nleads to longer token sequences, increasing both memory and compute demands, as well as the end\n∗Equal contribution.\nPreprint. Under review.arXiv:2506.01084v1  [cs.CL]  1 Jun 2025\n--- Page 2 ---\nuser’s cost by a factor of 2-3x when processing domain-specific text [Ahia et al., 2023]. To mitigate\nthis issue, prior work has explored expanding the token vocabulary during domain or language\nadaptation to improve tokenization efficiency [Wang et al., 2019, Zhao et al., 2024, Kim et al., 2024,\nLiu et al., 2023, 2024]. While effective, this approach needs to be repeated for each target domain\nor language and requires maintaining separate tokenizers. Meanwhile, commercial LLM providers\ntrend toward increasing the size of token vocabularies—growing from 32K to 128K [Grattafiori\net al., 2024] and even up to 200K [Abdin et al., 2024] tokens—to improve overall tokenization\nefficiency. However, prior work [Dagan et al., 2024, Liang et al., 2023] shows that simply enlarging\nthe vocabulary yields diminishing returns in domain adaptation, and vocabularies past a certain size\ncan potentially degrade model performance [Liang et al., 2023].\nThese limitations point to a compelling need for an adaptive tokenization mechanism—one that\ncan dynamically tailor the vocabulary to the input text at inference time, without retraining the\nmodel or maintaining separate tokenizers. Such a mechanism would allow the model to construct\nnew domain-specific tokens on-the-fly, so to enhance tokenization efficiency. However, adaptive\ntokenization poses architectural challenges, as both the embedding layer and the language modeling\nhead in transformer models [Vaswani et al., 2017] are static matrices tied to a fixed vocabulary size.\nIn this paper, we propose zip2zip (with a hat-tip to seq2seq [Sutskever et al., 2014]), a method that\nequips LLMs with a dynamic token vocabulary, enabling inference-time token adaptation. zip2zip\nachieves adaptive tokenization through continuous vocabulary expansion at runtime, allowing the\nmodel to represent a repeated or domain-specific pattern with a single long token rather than inef-\nficient short tokens. This requires modest modifications to both the transformer architecture and\nthe language modeling objective. zip2zip comprises three key components: (1) Tokenizer: an\nintegration of Lempel-Ziv-Welch (LZW) compression2[Welch, 1984] into the tokenization process,\nwhich continuously merges frequently co-occurring token sequences into reusable longer tokens\n(hypertokens) at runtime; (2) Architecture: a lightweight encoder added to the transformer that\ncomputes embeddings for newly formed tokens on the fly; (3) Training: a compression-aware\ncausal language modeling variant that trains the model directly on LZW-compressed sequences,\naligning learning with the inference-time token distribution. The name zip2zip reflects its dual\nrole in achieving compression of both the input tokens (the first zip) and output tokens (the second\nzip), thereby jointly improving the efficiency of input encoding and output decoding. We finetune\nPhi-3-4B andPhi-3-14B to support zip2zip using as few as 100M tokens—requiring only 10 and\n40 H100 GPU hours, respectively—for effective adaptation. The resulting models demonstrate strong\ninference-time compression capabilities and achieve 20–60% reductions in both input and output\nsequence lengths, translating to up to 60% improvements in end-to-end latency.\nTo make it easy to upgrade existing LLMs to zip2zip , we release an efficient, open-source imple-\nmentation of the framework. It includes (1) a fast Rust-based LZW tokenizer, (2) a drop-in model\narchitecture compatible with Hugging Face Transformers and vLLM, (3) a training pipeline for LZW-\ncompression-based finetuning. Existing LLMs can be seamlessly extended with zip2zip , gaining\nadaptive tokenization capabilities through parameter-efficient finetuning—without any changes to the\nbase model or tokenizer.\n2zip2zip\n2.1 Dynamic Token Vocabulary\nTo enable dynamic tokenization at inference time, we associate each LLM with a hyper-vocabulary\nVhthat augments the model’s static token vocabulary. Tokens from the original vocabulary Vare\nreferred to as base tokens . Each entry in the hyper-vocabulary is a hypertoken , representing a merged\nsequence of base tokens. The total vocabulary for a zip2zip model is the union V ∪ V h. At the\nbeginning of each inference session, Vhis initialized as an empty set, and is incrementally populated\nduring decoding by identifying and merging recurring token subsequences in the context window, as\nillustrated in Figure 1.\nContinuous Vocabulary Expansion. As decoding proceeds, zip2zip continuously merge co-\noccurring tokens as new hypertokens to Vhand recurrently apply merging on newly generated tokens.\nThis continual expansion allows the model to represent longer, recurring sequences of base tokens\n2LZW is the algorithm used in zip compression tool, which inspired the name zip2zip .\n2\n--- Page 3 ---\nFigure 1: Overview of the zip2zip inference pipeline. At each decoding step, the model has a growing\ncontext composed of both base tokens (blue) and hypertokens (green). The static vocabulary of size 6 remains\nfixed, while the dynamic vocabulary is continuously expanded by merging co-occurring tokens using LZW\ncompression . The codebook (right) maps hypertoken IDs to their corresponding base tokens. As decoding\nprogresses, new hypertokens created at step t(e.g., “to be”, “or not”) become immediately available for reuse at\nstept+. Additionally, output tokens, once generated, instantly become eligible for compression. Hypertokens\nare also eligible for merging, enabling the formation of nested hypertokens . The final output sequence (bottom)\nis reconstructed via LZW decompression.\ncompactly. Hypertokens are treated as first-class tokens within the model, used interchangeably\nwith base tokens throughout the decoding process. Importantly, this process occurs entirely during\ninference, without modifying the underlying tokenizer or requiring model retraining.\nLZW Algorithm. We implement vocabulary expansion using the Lempel-Ziv-Welch (LZW) com-\npression algorithm—a dictionary-based, lossless compression method that incrementally builds a\ncodebook of variable-length sequences. In our setting, the codebook is initialized with the base\ntoken vocabulary Vand expands by adding new hypertokens on the fly as recurring token patterns\nare encountered. To control the growth of the dynamically expanding vocabulary, we impose a\nmaximum merge size Mthat restricts how many base tokens a single hypertoken can represent. LZW\nis particularly well-suited for zip2zip due to the following properties:\n(1) it is streaming —hypertokens created at step tcan be immediately reusable at step t+ 1; in\ncontrast, methods like BPE require access to the full sequence and operate offline;\n(2) it is self-contained —input base tokens can be perfectly reconstructed from the compressed\ntoken sequence alone3;\n(3) it is unambiguous —when both base tokens and hypertokens are available, which one to use\nis consistently determined by the LZW algorithm without ambiguity.\n2.2 Hyper-Embedding and Hyper-Projection\nHypertokens do not have fixed embedding vectors in the original model’s embedding layer (and\nprojection layer), as they are not part of the original vocabulary. To compute the embedding of a hyper-\ntoken, we learn a mapping from the base token embeddings to the hypertoken embedding. We achieve\nthis by introducing a hyper-encoder , which is a neural network that takes the embeddings of the\nconstituent base tokens as input and outputs the corresponding hypertoken embedding. Specifically,\nfor a sequence of Mbase tokens y1:M:=y1. . . y M, the hyper-encoder fϕ:VM→Rdproduces\nthe hypertoken embedding h=fϕ(y1:M)∈Rd, where Mis the maximum merge size and dis the\nembedding dimension. For hypertokens composed of fewer than Mbase tokens, we pad the input se-\nquence to length M. Since the embedding map for base tokens remains unchanged, the hyper-encoder\nfϕessentially maps the concatenated base token embeddings from a (M×d)-dimensional space to a\nd-dimensional hypertoken embedding vector, performing nonlinear dimensionality reduction.\nFor the output projection layer, if the underlying transformer ties the embedding and the projection\nmatrices, one can reuse the same hyper-encoder to compute the representation used for projection.\nOtherwise, a separate hyper-encoder is trained to produce the hypertoken projection vectors.\n3There is no need to persist or transmit the codebook across inference calls, preserving compatibility with\nexisting LLM libraries and interfaces.\n3\n--- Page 4 ---\n2.3 Architecture\nWe illustrate the architecture of zip2zip in Figure 2. The input text is first tokenized into base\ntokens ( STEP 1), which are then passed through an online LZW compressing module that compresses\nthe token sequence into a stream of hypertokens ( STEP 2). Since hypertokens are not part of\nthe model’s original embedding layer, their embedding vectors are computed on-the-fly using the\nhyper-encoder during inference ( STEP 3–4). Once embedded, both base token embeddings and\nhypertokens embeddings are passed through the standard transformer layers of the base model,\nproducing contextualized hidden states ( STEP 5–6). This step is identical to vanilla transformer, with\nhypertokens and base tokens treated equally. At the output projection layer, hypertoken projection\nvectors (same as the hypertoken embedding vectors in the tied case, and computed by a separate hyper-\nencoder otherwise) are appended to the original projection matrix in the language modeling head\n(STEP 7). This allows the model to compute a joint softmax over the union of the base vocabulary\nand the hyper vocabulary V ∪ V h(STEP 8). The resulting probability distribution is over V ∪ V h,\nand the sampled token may be either a base token or a hypertoken ( STEP 9). In the next cycle, the\nnewly generated token ( STEP 10) —whether base or hyper—is appended to the input sequence,\nand the process repeats (back to STEP 1). At the end of generation, the hypertoken sequence is\ndecompressed via the LZW decoding function into a sequence of base tokens ( STEP 11–12 ).The\nwhole process works in a fully autoregressive way, where newly generated tokens will also be merged\ninto hypertokens for future steps. Furthermore, we highlight two points:\nFigure 2: zip2zip architecture. At inference time, base tokens\nare compressed into hypertokens using LZW ( STEPS 1–2). A\nhyper-encoder computes embeddings for hypertokens ( STEP 3–\n4), which are processed by the base LLM ( STEPS 5–6). Output\nrepresentations are projected jointly on base and hyper-projection\nlayers ( STEP 7), producing joint logits and sampled tokens ( STEPS\n8–10 ), which can be decoded back to base tokens ( STEPS 11–12 ).Consistent Vocabulary Up-\ndates. The expanding vocabu-\nlary—comprising newly created\nhypertokens—must be updated in\naconsistent manner across both\nthe input embedding layer and the\noutput projection layer, maintaining\na consistent view of the hypertoken\nset. Failure to update both sides\nconsistently can result in two types of\nerrors: (1) hypertokens that cannot be\ndecoded, or (2) the model attempting\nto decode a non-existing hypertoken.\nHyper-Embedding Cache. Although\nhypertoken embeddings are com-\nputed on-the-fly, they are context-\nindependent and can thus be cached\nacross inference steps. Similar to\nthe transformer’s KV-cache, this en-\nables incremental updates: only newly\ncreated hypertokens need to be pro-\ncessed at each step. Since the code-\nbook grows linearly with the number\nof tokens in the context n, the total\ncache size grows also linearly in mem-\nory. Thus, the computational cost for\nhypertoken embeddings remains con-\nstant per step—i.e., one token embed-\nding is computed per step.\n2.4 Training zip2zip models\nObjective. LetDdenote the target\ntext distribution. Given a language\nmodel πθparameterized by θ, stan-\ndard pretraining seeks to minimize the causal language modeling (CLM) objective, which corresponds\nto the expected negative log-probability of data sequences under the model:\nmin\nθEy∼D[−logπθ(y)], (1)\n4\n--- Page 5 ---\nwhere πθ(y)denotes the probability of the token sequence yunder the model πθ.\nLetCbe an online compression algorithm (e.g., LZW), and ϕbe the parameters of the hyper-encoder.\nGiven a sequence y∼ D, letz=C(y)be its compressed form. In zip2zip , we aim to optimize the\nsame CLM loss, but over the compressed sequences z. The training objective becomes:\nmin\nθ,ϕEy∼D[−logπθ,ϕ(C(y))] = min\nθ,ϕEz∼C(D)[−logπθ,ϕ(z)]. (2)\nHere, we slightly abuse the notation to let πθ,ϕ(z)denote the probability assigned to the compressed\nsequence z, parameterized by the base model weights θand the hyper-encoder parameters ϕ.\nTo construct the compressed dataset C(D), we first tokenize the corpus using a standard tokenizer,\nand then apply the LZW compression algorithm. This preprocessing step is performed once prior to\ntraining and can be efficiently parallelized through batching.\nParallelizable Training via Causal Masking. Although hypertokens introduce additional vocabulary\ndynamics, training remains fully parallelizable. We leverage the standard causal masking mechanism\nused in language models, allowing the model to predict the next token—whether a base token or\na hypertoken—at each position in parallel. To eliminate the need for sequential codebook updates\nduring inference, we precompute a fixed codebook by applying LZW compression to the entire input\nsequence. This precomputed codebook is then used consistently throughout training to condition\ntoken predictions, ensuring efficiency and compatibility with standard training pipelines.\nAuxiliary Reconstruction Loss. We introduce an auxiliary reconstruction objective that encourages\na hypertoken embedding to retain sufficient information about its underlying base token sequence.\nSpecifically, the model is trained to reconstruct the original base token embeddings from the hyperto-\nken embedding. We jointly optimize the language model and the hyper-encoder using a combined\nloss that includes both the standard next-token prediction loss and the auxiliary reconstruction loss.\nFormally, we optimize:\nmin\nθ,ϕ,ψEy∼D[−logπθ,ϕ(C(y))] + λEy1:M[∆ (y1:M, fψ(fϕ(y1:M)))], (3)\nwhere fϕ:VM→Rdis the hyper-encoder, fψ:Rd→ VMis the decoder aiming to reconstruct the\ncorresponding base tokens from their hyper-embedding, and ∆ :VM×VM→Ris the reconstruction\nloss function, such as the cross-entropy loss, between the base tokens y1:Mand the reconstructed\nbase tokens fψ(fϕ(y1:M)). The hyperparameter λcontrols the trade-off between the prediction\nerror of the language model and the reconstruction error of the autoencoder. This joint optimization\nobjective encourages the hyper-encoder to learn a compact d-dimensional manifold embedded in the\nhigher-dimensional (M×d)space of base token embeddings, while the language model πθ,ϕlearns\nto predict the next (hyper)token given the preceding context. The reconstruction loss can be viewed\nas a form of auto-encoding, where the hypertoken acts as a compressed latent representation and\nreconstruction encourages the preservation of semantic content and the compression to be lossless.\nAdapting Pretrained Language Models. The proposed objectives (Equation 2, 3) integrate naturally\nwith pretrained language models. In this setting, the base model can be frozen while training only\nthe hyper-encoder to adapt to compressed token sequences. Parameter-efficient methods such as\nLoRA [Hu et al., 2022] may also be used to adapt select components of the base model, enabling\neffective adaption with minimal computes.\n2.5 Efficiency Advantage\nzip2zip improves efficiency by increasing the average token length, thereby reducing the number of\ntokens required to represent the same text. This compression applies to both inputs (e.g., prompts)\nand outputs (e.g., completions), leading to shorter effective context lengths. As a result, the model\nperforms fewer computations—both in the attention mechanism and the feedforward layers—and,\nmore importantly, requires fewer autoregressive decoding steps during inference. Since the latency\nof large language models is primarily driven by the cost of sequential decoding, reducing the\nnumber of output tokens by n% leads to an approximate n% speedup in decoding latency, which we\nwill demonstrate empirically in Section 3.6. A more detailed discussion of FLOPs is provided in\nAppendix B for completeness.\n5\n--- Page 6 ---\n3 Experiments\nTo evaluate the effectiveness of zip2zip , we adapt the Phi-3 models (3B and 14B) within the zip2zip\nframework. We evaluate our adapted models across four dimensions: (1) token efficiency, (2) language\nmodeling perplexity, (3) downstream task performance, (4) inference efficiency. For perplexity and\ndownstream benchmarks, we use the widely adopted lm-evaluation-harness framework [Gao et al.,\n2024].\n3.1 Training Setup\nRather than updating the full model weights, we adopt parameter-efficient finetuning using LoRA [Hu\net al., 2022]. In addition, we train the hyper-embedding andhyper-projection modules. We set\nthe maximum merge size to M= 3and use a two-layer transformer encoder as the hyper-encoder.\nAblation studies on Mand hyper-encoder architecture can be found in Appendix A. For compari-\nson, we also perform continual finetuning of the base model using LoRA under identical training\nconditions, serving as a baseline (denoted as Cont. Finetune in the Tables) The fine-tuning process\nis highly efficient, requiring approximately 10 H100-GPU hours for a 4B-parameter model and up\nto 40 H100-GPU hours for a 14B-parameter model, using only 0.1 billion training tokens. Interest-\ningly, the reconstruction loss converges to near zero during training, indicating that the model can\nalmost perfectly recover the original base token sequences from the hypertoken representations. This\nhighlights the learned compression is highly information-preserving. Details of the training setup,\ncompute infrastructure, and dataset curation are provided in Appendices D and E.\n3.2 Sample Outputs and Hypertoken Patterns\nWe present several examples to provide intuition into how the zip2zip model generates text.\nFigure 3: Zip2Zip output examples. Blue: base tokens; Yellow: hypertokens (composed of 2 base tokens);\nOrange: hypertokens (composed of 3+ base tokens).\nWe see that the model successfully generates a mixture of hypertokens and base tokens in the output\n(see Figure 3). The hypertoken ratio is as high as 40% in the Python code generation example, and\n20% in the biomedical text generation example. Many of the hypertokens correspond to semantically\nmeaningful units or domain-specific terms as shown in Table 1. For a more fine-grained visualization\nof hypertoken with zip2zip , we provide visualizations of token streams in Appendix 8.\nTable 1: Examples of hypertokens formed by zip2zip across three domains\nCode Generation Biomedical French\ntor +ch=torch m+R+NA=mRNA E+iff +el=Eiffel\nAtt +ention =Attention trans +cribed =transcribed de+la=de la\nMulti +Head =MultiHead synth +esis =synthesis Gust +ave =Gustave\nk+dim =kdim cell +ular =cellular comm +enc +é=commencé\n3.3 Token Efficiency\nGiven an input text xand a tokenizer, we define token efficiency η:=Bytes( x)\nTokens( x)as the average number\nof bytes represented by each token, where Bytes( x)refers to the number of bytes in the UTF-8\n6\n--- Page 7 ---\nTable 2: Token efficiency (bytes /token ) across domains for different tokenizers w/wo zip2zip .\nTokenizer Code Math Chat Multilingual Web\nLlama-3-128K [Grattafiori et al., 2024] 4.1 2.7 5.1 3.8 4.6\n+zip2zip 6.3 (+54%) 4.0 (+48%) 6.4 (+25%) 4.7 (+24%) 5.4 (+17%)\nQwen-2-150K [Yang et al., 2024] 4.0 2.3 5.1 3.7 4.4\n+zip2zip 6.2 (+55%) 3.7 (+61%) 6.4 (+25%) 4.6 (+24%) 5.2 (+18%)\nPhi-4-200K [Abdin et al., 2024] 4.1 2.7 5.4 4.6 4.7\n+zip2zip 6.3 (+54%) 4.1 (+52%) 6.7 (+24%) 5.5 (+20%) 5.4 (+15%)\nGemma-3-256K [Team et al., 2025] 3.3 2.3 5.0 4.4 4.5\n+zip2zip 5.6 (+70%) 3.7 (+61%) 6.4 (+28%) 5.4 (+23%) 5.4 (+20%)\nencoding of x. This measures how compactly a tokenizer encodes input text—higher values of η\nindicate more efficient tokenization.\nWe evaluate token efficiency using the tokenizers of four LLMs—Llama-3 [Grattafiori et al., 2024],\nQwen-2 [Yang et al., 2024], Phi-4 [Abdin et al., 2024], and Gemma-3 [Team et al., 2025]—each\nassociated with a different base vocabulary size ranging from 128K to 256K. Token efficiency is\nmeasured across five representative domains, sampled from publicly available datasets: code [Lozhkov\net al., 2024b], math [LI et al., 2024], chat [Ding et al., 2023], multilingual [Penedo et al., 2024], and\nweb [Lozhkov et al., 2024a]. Table 2 shows that applying LZW zip2zip consistently improves token\nefficiency across all tokenizer and domains. Gains are particularly strong in structured domains like\ncode and math—50% higher than the base tokenizer. Interestingly, models with larger vocabulary\nsizes do not always achieve better token efficiency, suggesting that simply enlarging the vocabulary\nsize is not sufficient to improve it.\n3.4 Perplexity\nTable 3: Byte-perplexity ( ↓) on four corpora\nusing a 1024-token context window.\nModel Method Wiki Pile mC4 dC4\nPhi-3.5-4B Base 1.58 1.79 1.88 1.74\nCont. finetune 1.59 1.81 1.88 1.74\nzip2zip 1.69 1.95 2.00 1.82\nPhi-3-14B Base 1.43 1.72 1.82 1.67\nCont. finetune 1.47 1.79 1.86 1.68\nzip2zip 1.56 1.90 1.96 1.75We evaluate the perplexity of zip2zip models on four\ncorpora: Wikitext [Merity et al., 2016], the Pile [Gao\net al., 2020], and two subsets of Paloma [Magnusson\net al., 2023]: mC4, a multilingual subset of C4, and\ndC4 (aka C4-100D), a subset of C4 spanning 100 do-\nmains. Given a token sequence x=x1, . . . , x N, and\na model q, perplexity and byte-level perplexity [Rad-\nford et al., 2019, Magnusson et al., 2023] are de-\nfined as: PPL:=\u0010QN\ni=1q(xi)\u0011−1/N\n,Byte-PPL :=\n\u0010QN\ni=1q(xi)\u0011−1/B\n=PPL1/η, where Bis the num-\nber of UTF-8 bytes of the text, and ηdenotes the\ntoken efficiency (i.e., bytes per token). Token-level\nperplexity depends on the tokenization scheme and is\nunsuitable for cross-tokenizer comparison. We instead report byte-level perplexity, a vocabulary-\nagnostic metric that normalizes for tokenization differences. Table 3 shows that zip2zip model has a\nmodest increase in Byte-perplexity, indicating a drop in language modeling performance.\n3.5 Evaluation on NLP Benchmarks\nWe next evaluate zip2zip ’s performance on real-world tasks. We evaluate on seven widely used NLP\nbenchmarks, including ARC-[Challenge, Easy] [Clark et al., 2018], HellaSwag [Zellers et al., 2019],\nLAMBADA [Paperno et al., 2016], OpenbookQA [Mihaylov et al., 2018], PIQA [Bisk et al., 2019],\nWinogrande [Sakaguchi et al., 2019] and GSM8K [Cobbe et al., 2021]. As shown in Table 4, the\nmodel finetuned with zip2zip performs similarly to the baseline on most tasks. However, on GSM8K,\nwhere the primary task involves numerical computation, the model exhibits significant degradation.\nDue to the sensitivity of such tasks to tokenization, it occasionally generates malformed or repeated\nnumbers. While token-level operations are already known to be challenging for LLMs [Singh and\nStrouse, 2024], adaptive tokenization appears to exacerbate this issue.\n7\n--- Page 8 ---\nTable 4: Two-shot accuracy (in %) across 7 NLP benchmarks, higher is better. Standard deviations (bootstrapped)\n≈0.02 across all tasks.\nModel Method ARC-c ARC-e HS OBQA PIQA WG GSM8K\nPhi-3.5-4B Base 0.60 0.83 0.66 0.46 0.79 0.75 0.82\nCont. finetune 0.60 0.82 0.63 0.47 0.82 0.75 0.40\nzip2zip 0.57 0.83 0.61 0.46 0.82 0.75 0.15\nPhi-3-14B Base 0.62 0.80 0.70 0.51 0.83 0.76 0.84\nCont. finetune 0.62 0.88 0.66 0.52 0.87 0.80 0.52\nzip2zip 0.62 0.86 0.68 0.51 0.85 0.79 0.25\nTo validate the effectiveness of zip2zip on non-English languages, we evaluate the model on machine\ntranslation tasks, including WMT14 [Machá ˇcek and Bojar, 2014], WMT16 [Bojar et al., 2016]. The\nresults, shown in Table 5, indicate a small performance degradation across BLEU, CHRF, and TER\nmetrics when using zip2zip . However, the drop is relatively minor, suggesting that the model retains\nstrong multilingual capabilities even in the compressed representation.\nTable 5: Machine translation performance on WMT benchmarks. Scores are averaged across both translation\ndirections. Standard deviations (approximately 1.0∼2.0) are reported in Table 10 in Appendix C.\nModel Method WMT14 En-Fr WMT16 En-De WMT16 En-Ro\nBLEU ↑CHRF ↑TER↓BLEU ↑CHRF ↑TER↓BLEU ↑CHRF ↑TER↓\nPhi-3.5-4B Base 33.6 58.3 53.0 39.2 63.2 47.9 17.7 45.5 73.4\nCont. finetune 36.5 61.0 51.5 42.3 65.4 44.9 16.7 45.8 79.7\nzip2zip 34.1 59.4 54.5 39.7 64.5 48.0 14.3 44.2 93.5\nPhi-3-14B Base 39.1 62.6 49.3 43.1 65.6 44.1 21.3 51.0 70.5\nCont. finetune 38.9 63.2 48.8 48.4 70.1 39.8 21.8 52.0 68.3\nzip2zip 36.4 62.8 51.2 44.8 68.1 42.9 19.5 50.1 72.9\n3.6 Inference Efficiency\nzip2zip reduces decoding time by lowering the number of tokens that need to be generated. However,\nit introduces additional FLOPs due to the on-the-fly computation of hyper-embeddings by the hyper-\nencoder. To address this overhead, we implement hyper-embedding caching and optimize the\ncomputation using a custom Triton kernel. We report separate timings for prefill anddecoding across\nmultiple models, with and without zip2zip , in Table 6.\nTable 6: Throughput (tokens/sec) comparison of the zip2zip framework against the baseline Hugging Face\nTransformers generate and MLX generate implementation. Performance is detailed for prefill and decode\nphases across various context lengths (first value in column headers) combined with a 256-token generation\nlength. zip2zip demonstrates notable throughput improvements, in both prefill and decoding phase.\nSetting Method256+256 512+256 1024+256 2048+256\nPrefill Decode Prefill Decode Prefill Decode Prefill Decode\nHardware: Apple M1 (16GB RAM)\nPhi-3-4BBase model 165.0 7.3 211.3 7.5 200.9 7.1 196.6 6.8\nzip2zip 145.5 7.9 231.4 10.1 189.6 7.4 233.8 7.3\nRelative % -11.8% +7.5% +9.5% +34.8% -6.6% +3.9% +18.9% +7.5%\nHardware: NVIDIA H100 80GB GPU\nPhi-3.5-4BBase model 700.9 56.2 1347.2 54.4 2689.4 52.8 4993.2 53.1\nzip2zip 936.6 61.4 2722.1 79.8 4326.7 61.5 9258.1 61.9\nRelative % +33.6% +9.3% +102.6% +46.6% +60.9% +16.6% +85.4% +16.5%\nPhi-3-14BBase model 724.4 44.6 1356.3 43.8 2328.6 45.1 3849.5 42.2\nzip2zip 1024.6 54.9 1973.0 61.1 3657.0 66.8 7239.1 46.3\nRelative % +41.5% +23.0% +45.5% +39.5% +57.0% +48.1% +88.1% +9.6%\n8\n--- Page 9 ---\nFigure 4: zip2zip tokenizer la-\ntency (ms) vs. HF tokenizer.As we show in Table 6, zip2zip achieves a significant speedup in\nall four settings. Both prefill and decoding times are significantly\nreduced, with the most substantial gains observed in the 512+256\nsetting with the Phi-3.5-4B model. Improvements are significantly\nstronger on datacenter-grade GPUs like the NVIDIA H100 and more\nmodest on consumer hardware (e.g., Apple M1).\nEfficient LZW Tokenization. zip2zip introduces an additional\nLZW compression step during inference and a decompression step at\nthe end of generation. As a result, the efficiency of LZW-integrated\ntokenization is important to overall performance. To minimize over-\nhead, we implemented a Rust-based zip2zip tokenizer that outper-\nforms the Python version (see Figure 4) and matches the latency of\nHuggingFace’s fast BPE tokenizer.\n4 Related Work\nVocabulary Expansion. Several works have explored expanding the tokenizer vocabulary to better\nsupport specific domains or languages. Zhao et al. [2024], Kim et al. [2024], Liu et al. [2023,\n2024] adapt LLaMA to Chinese, Korean, and specialized domains such as mental health and law by\nappending new tokens. Wang et al. [2025], Liu et al. [2024] conducted studies on how to effectively\nexpand the vocabulary by better selecting the subset of tokens to add. In contrast, zip2zip is the first\nto enable dynamic vocabulary expansion at inference time , constructing new tokens based on the\ninput context without requiring retraining or modifying the tokenizer ahead of time.\nPrompt Compression. Prompt compression methods include GistTokens [Mu et al., 2023], Selective\nContext [Li et al., 2023], LLMLingua [Jiang et al., 2023], Summary Vectors [Chevalier et al., 2023],\nIn-context Autoencoder [Ge et al., 2024], and others [Wingate et al., 2022] reduce the input token\nlength and but do not impact the number of output tokens, which often dominates overall generation\ntime. In contrast, zip2zip compresses both the input and output token sequences.\nLatent Tokens Representation. The concept of latent token representations, or patches , has been\nmostly explored in computer vision, with methods like Token Merging [Bolya et al., 2023] and\nToken Pooling [Marin et al., 2023] aiming to reduce sequence length while preserving semantic\ncontent. Recently, Byte Latent Transformer (BLT) [Pagnoni et al., 2024] extended this concept to\nlanguage modeling by discarding tokens entirely and operating directly at the byte level. Both BLT\nandzip2zip adopt a hierarchical modeling of input for LLMs, but they differ in three key ways:\n(1)Goal : BLT aims to replace the tokenizer, whereas zip2zip seeks to expand and improve it;\n(2)Algorithm : BLT uses entropy-based segmentation, while zip2zip applies LZW-based token\ncompression; (3) Training : BLT requires training from scratch, whereas zip2zip enables continued\nadaptation of pretrained models. Lester et al. [2024] propose improving language model efficiency by\ntraining LLMs directly on text compressed with arithmetic coding. While both approaches leverage\ncompression to enhance efficiency, zip2zip emphasizes dynamic vocabulary expansion to enable\nuptraining of existing models. In contrast, Lester et al. [2024] requires training from scratch.\n5 Discussion and Limitations\nBeyond LZW. While we adopt LZW for dynamic construction of hypertokens, zip2zip is broadly\ncompatible with any online compression algorithm. Future work may explore alternative schemes\nthat provide different trade-offs between compression efficiency and model performance.\nCodebook Management Strategy. The LZW algorithm grows the codebook linearly with the number\nof tokens in the context window. Empirical results show that only about 25% of hypertokens are\nreused during generation, leaving substantial room for optimization. Two potential improvements are:\n(1)pruning orselective retention strategies to reduce unused entries, and (2) codebook prefilling ,\nwhich could be beneficial if likely tokens can be anticipated before input processing.\nCompression–Quality Trade-off. There is an inherent trade-off between compression and modeling:\nas the token space is compressed more aggressively, redundancy is reduced—but so is predictabil-\nity—making it harder for the model to forecast the next (hyper)token. In the extreme, optimal\n9\n--- Page 10 ---\ncompression schemes such as arithmetic coding produce sequences that are statistically indistin-\nguishable from random noise, rendering them unlearnable by language models [Lester et al., 2024].\nEmpirically, we observe this effect as increased perplexity under higher compression levels (Table 3),\nwhich can undermine the benefits of compression by degrading generation quality (though minor\nin the tasks in Table 4 and Table 5). Striking the right balance between compression and model\nperformance remains an important direction for future research.\n6 Conclusion\nWe introduced zip2zip , a framework for inference-time vocabulary adaptation with LLMs. By\nintegrating LZW-based token compression with a dynamic hypertoken embedding mechanism,\nzip2zip enables substantial reductions in sequence length and decoding steps, leading to improved\ninference efficiency with minimal architectural modifications. Our experiments demonstrate that\nzip2zip maintains strong performance across a range of tasks while achieving significant gains in\ninference efficiency. These findings highlight the promise of integrating dynamic tokenization into\nLLMs, opening up new directions for research in LLM efficiency.\nReferences\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat\nLee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa,\nOlli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang,\nand Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905 .\nOrevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and\nYulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language\nmodels. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing , pages 9904–9923, Singapore,\nDecember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.\n614. URL https://aclanthology.org/2023.emnlp-main.614/ .\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. In AAAI Conference on Artificial Intelligence , 2019.\nURL https://api.semanticscholar.org/CorpusID:208290939 .\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias\nHuck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri,\nAurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia\nSpecia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. Findings of the 2016 conference\non machine translation. In Ond ˇrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann,\nLiane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana\nNeves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post,\nLucia Specia, Karin Verspoor, Jörg Tiedemann, and Marco Turchi, editors, Proceedings of the\nFirst Conference on Machine Translation: Volume 2, Shared Task Papers , pages 131–198, Berlin,\nGermany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301.\nURL https://aclanthology.org/W16-2301/ .\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy\nHoffman. Token merging: Your vit but faster, 2023. URL https://arxiv.org/abs/2210.09461 .\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,\nM.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,\npages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\npaper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n10\n--- Page 11 ---\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio\nRibeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4,\n2023. URL https://arxiv.org/abs/2303.12712 .\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models\nto compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing , pages 3829–3846,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nemnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/ .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,\n2018. URL https://arxiv.org/abs/1803.05457 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\nGautier Dagan, Gabriel Synnaeve, and Baptiste Rozière. Getting the most out of your tokenizer for\npre-training and domain adaptation, 2024. URL https://arxiv.org/abs/2402.01035 .\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations, 2023.\nSimon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas\nLukasiewicz, Philipp Christian Petersen, and Julius Berner. Mathematical capabilities of chatgpt,\n2023. URL https://arxiv.org/abs/2301.13867 .\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb\ndataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027 .\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff,\nChris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation\nharness, 07 2024. URL https://zenodo.org/records/12608602 .\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder\nfor context compression in a large language model. In The Twelfth International Conference on\nLearning Representations , 2024. URL https://openreview.net/forum?id=uREj4ZuGJE .\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\nAnirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru,\nBaptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak,\nChloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu,\nCorinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego\nGarcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel\nSynnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\nWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua\nSaxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak,\n11\n--- Page 12 ---\nKe Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley\nChiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas\nLandzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri,\nMarcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie\nKambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes\nTorabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne,\nOnur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal\nBhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRoberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie\nPolidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana\nChennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie,\nSharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon\nVandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan,\nSydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\nVibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish V ogeti,\nVítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier\nMartinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao\nJia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,\nYuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe\nPapakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya\nGangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu,\nAndres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit\nRamchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury,\nAshley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,\nBenjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu,\nBo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu\nKim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,\nCynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu,\nDavide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc\nLe, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers,\nFei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank\nKanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee,\nGil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,\nHamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,\nHelen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog,\nIgor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James\nKohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny\nZhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings,\nJon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai\nWu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish\nBhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim\nNaumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle\nRestrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang,\nMiquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam,\nNatascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier,\nNikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia\nHart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro\nRittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,\nRaghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\n12\n--- Page 13 ---\nBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\nMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\nSheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang,\nShuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie\nMax, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman,\nTal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun\nZhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun V ontimitta, Victoria Ajayi, Victoria\nMontanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,\nVlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz,\nWill Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv\nKleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait,\nZachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The\nllama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference\non Learning Representations , 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9 .\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Com-\npressing prompts for accelerated inference of large language models. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pages 13358–13376, Singapore, December 2023. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https:\n//aclanthology.org/2023.emnlp-main.825 .\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language\nmodels for code generation, 2024. URL https://arxiv.org/abs/2406.00515 .\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\n2020. URL https://arxiv.org/abs/2001.08361 .\nSeungduk Kim, Seungtaek Choi, and Myeongho Jeong. Efficient and effective vocabulary expansion\ntowards multilingual large language models, 2024. URL https://arxiv.org/abs/2402.14714 .\nBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein,\nand Noah Constant. Training llms over neurally compressed text, 2024. URL https://arxiv.\norg/abs/2404.03626 .\nJia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa\nHuang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong,\nLi Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https:\n//huggingface.co/AI-MO/NuminaMath-1.5](https://github.com/project-numina/\naimo-progress-prize/blob/main/report/numina_dataset.pdf) , 2024.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference\nefficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages\n6342–6353, Singapore, December 2023. Association for Computational Linguistics. doi: 10.\n18653/v1/2023.emnlp-main.391. URL https://aclanthology.org/2023.emnlp-main.391/ .\nDavis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke\nZettlemoyer, and Madian Khabsa. XLM-V: Overcoming the vocabulary bottleneck in multilingual\nmasked language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing , pages 13142–13152,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nemnlp-main.813. URL https://aclanthology.org/2023.emnlp-main.813/ .\n13\n--- Page 14 ---\nChengyuan Liu, Shihang Wang, Lizhi Qing, Kun Kuang, Yangyang Kang, Changlong Sun, and Fei\nWu. Gold panning in vocabulary: An adaptive method for vocabulary expansion of domain-specific\nLLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing , pages 7442–7459, Miami,\nFlorida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n2024.emnlp-main.424. URL https://aclanthology.org/2024.emnlp-main.424/ .\nSiyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, and Rada Mihalcea. Task-\nadaptive tokenization: Enhancing long-form text generation efficacy in mental health and beyond.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , pages 15264–15281, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.944. URL\nhttps://aclanthology.org/2023.emnlp-main.944/ .\nAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the\nfinest collection of educational content, 2024a. URL https://huggingface.co/datasets/\nHuggingFaceFW/fineweb-edu .\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane\nTazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov,\nArthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul,\nZhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii,\nNii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan\nDey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov,\nChristopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri\nDao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten\nScholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa\nPatwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2:\nThe next generation, 2024b.\nMatouš Machá ˇcek and Ond ˇrej Bojar. Results of the WMT14 metrics shared task. In Ond ˇrej Bojar,\nChristian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post,\nand Lucia Specia, editors, Proceedings of the Ninth Workshop on Statistical Machine Translation ,\npages 293–301, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.\ndoi: 10.3115/v1/W14-3336. URL https://aclanthology.org/W14-3336/ .\nIan Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A. Jha, Oyvind Tafjord, Dustin\nSchwenk, Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hanna Hajishirzi,\nNoah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark for evaluating language\nmodel fit. ArXiv , abs/2312.10523, 2023. URL https://api.semanticscholar.org/CorpusID:\n266348815 .\nDmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and\nOncel Tuzel. Token pooling in vision transformers for image classification. In 2023 IEEE/CVF\nWinter Conference on Applications of Computer Vision (WACV) , pages 12–21, 2023. doi: 10.1109/\nWACV56688.2023.00010.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing , pages 2381–2391, Brussels, Belgium, October-\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL\nhttps://aclanthology.org/D18-1260/ .\nJesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. In\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances\nin Neural Information Processing Systems , volume 36, pages 19327–19352. Curran Asso-\nciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf .\n14\n--- Page 15 ---\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities\nof gpt-4 on medical challenge problems, 2023. URL https://arxiv.org/abs/2303.13375 .\nArtidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret\nLi, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari\nHoltzman, and Srinivasan Iyer. Byte latent transformer: Patches scale better than tokens, 2024.\nURL https://arxiv.org/abs/2412.09871 .\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Katrin Erk and Noah A. Smith, editors,\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pages 1525–1534, Berlin, Germany, August 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144/ .\nGuilherme Penedo, Hynek Kydlí ˇcek, Vinko Sabol ˇcec, Bettina Messmer, Negar Foroutan, Martin\nJaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: A sparkling update with 1000s of\nlanguages, 2024. URL https://huggingface.co/datasets/HuggingFaceFW/fineweb-2 .\nAleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. Language model tokenizers\nintroduce unfairness between languages. In Advances in Neural Information Processing Systems ,\n2023. URL https://arxiv.org/abs/2305.15425 .\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-\nguage models are unsupervised multitask learners. OpenAI blog , 2019. URL https://api.\nsemanticscholar.org/CorpusID:160025533 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande. Communi-\ncations of the ACM , 64:99 – 106, 2019. URL https://api.semanticscholar.org/CorpusID:\n198893658 .\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 1715–1725, Berlin,\nGermany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162.\nURL https://aclanthology.org/P16-1162/ .\nAaditya K. Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in\nfrontier llms, 2024. URL https://arxiv.org/abs/2402.14903 .\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural networks.\nInProceedings of the 28th International Conference on Neural Information Processing Systems -\nVolume 2 , NIPS’14, page 3104–3112, Cambridge, MA, USA, 2014. MIT Press.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas\nMesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon,\nEtienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai\nZhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman,\nYi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-\nThorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi,\nDan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe\nFriesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa\nSaade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András\nGyörgy, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia\nPaterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini,\nCharlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel\nDeutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar\nSreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene\nKharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-\nPluci ´nska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne,\nIdan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan\n15\n--- Page 16 ---\nLai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy\nYu, Kevin Hui, Kiran V odrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho,\nMarvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma,\nNabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen\nSachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton,\nPhilipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna,\nRenjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome,\nSara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar,\nSindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty,\nUday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov,\nWoohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed,\nVictor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo,\nErica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris\nWarkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia\nHadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff\nDean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste\nAlayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin,\nAlek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report,\n2025. URL https://arxiv.org/abs/2503.19786 .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\neditors, Advances in Neural Information Processing Systems , volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\nHai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong Yu. Improving pre-trained multilingual model\nwith vocabulary expansion. In Mohit Bansal and Aline Villavicencio, editors, Proceedings of the\n23rd Conference on Computational Natural Language Learning (CoNLL) , pages 316–327, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nK19-1030. URL https://aclanthology.org/K19-1030/ .\nShumin Wang, Yuexiang Xie, Bolin Ding, Jinyang Gao, and Yanyong Zhang. Language adaptation\nof large language models: An empirical study on LLaMA2. In Owen Rambow, Leo Wanner,\nMarianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors,\nProceedings of the 31st International Conference on Computational Linguistics , pages 7195–\n7208, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https:\n//aclanthology.org/2025.coling-main.480/ .\nWelch. A technique for high-performance data compression. Computer , 17(6):8–19, 1984. doi:\n10.1109/MC.1984.1659158.\nDavid Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction in language models. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Lin-\nguistics: EMNLP 2022 , pages 5621–5634, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.412. URL\nhttps://aclanthology.org/2022.findings-emnlp.412/ .\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai,\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng\nXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai\nBai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan\nZhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671 , 2024.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez,\n16\n--- Page 17 ---\neditors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npages 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/ .\nJun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond\nenglish: An empirical study on language capability transfer, 2024. URL https://arxiv.org/\nabs/2401.01055 .\nA Ablation Studies\nDefinition A.1 (Compression Rate) .We define the compression rate as the ratio between the number\nof tokens after compression ( Ncomp) and the number of tokens in the original uncompressed text\n(Norig), expressed as a percentage:\nCompression Rate =Ncomp\nNorig×100% .\nA lower compression rate indicates greater reduction in token count, and thus more effective com-\npression.\nA.1 LZW Maximum Merge Size\nThe last column of Table 7 shows how the maximum merge size Maffects compression rate when the\ncontext window length is 2048 . AsMincreases, compression rate improves significantly, especially\nfrom M= 1 toM= 3. Beyond that, gains diminish, suggesting M= 3 strikes a good balance\nbetween efficiency and compression rate.\nTable 7: Effect of maximum merge size ( M) on byte-\nlevel perplexity and compression rate. Perplexity is\nmeasured for Phi-3.5-4B across four corpora with a\n1024-token context window. Compression rate is evalu-\nated over the training corpus with a 2048-token context.\nM= 1corresponds to no compression.\nMWiki Pile mC4 dC4 Compression Rate(%)\n1 1.62 1.70 2.00 1.91 100.00\n2 1.96 2.21 2.55 2.22 75.30\n3 1.72 1.84 2.15 2.00 71.21\n4 1.71 1.84 2.14 1.99 68.93\n5 1.72 1.84 2.14 1.99 68.41Interestingly, the relationship between maxi-\nmum merge size and training loss in Figure 5 as\nwell as perplexity in Table 7 is non-monotonic.\nThe baseline case with M= 1(i.e., no zip2zip\ncompression) yields the lowest perplexity over-\nall, which is expected and consistent with prior\nfindings that compression typically incurs a\ntrade-off in model performance. Among the\ncompressed settings, the case M= 2performs\nthe worst, with noticeably slower convergence\nand higher final loss. In contrast, the case\nM= 3 achieves the best performance within\nthe compressed configurations, striking a favor-\nable balance between compression and predic-\ntion performance. While M= 4 andM= 5\nalso perform reasonably well, they exhibit slightly higher loss than M= 3, suggesting diminishing\nreturns or possible over-compression at larger maximum merge sizes (see Figure 5).\nTable 7 reports the byte-level perplexity across four corpora using a 1024-token context window. The\nresults align closely with the training loss trends observed earlier. Setting M= 1(i.e., no compres-\nsion) consistently achieves the lowest perplexity across all datasets, reaffirming that compression\nintroduces a performance trade-off. Notably, M= 2performs the worst across all corpora, exhibiting\nthe highest perplexity values. For merge sizes M= 3,M= 4, and M= 5, perplexity scores\nare nearly identical, suggesting that moderate compression can be achieved without significantly\nsacrificing language modeling quality—provided M= 2is avoided. This consistency across loss\nand perplexity metrics further supports the robustness of maximum merge size M= 3as the most\neffective trade-off point.\nA.2 Hyper-encoder architecture\nWe ablate the architecture of the hyper-encoder to evaluate its effect on language modeling perfor-\nmance, as shown in Table 8. We compare increasingly expressive architectures, starting with a simple\n17\n--- Page 18 ---\nFigure 5: Effect of maximum merge size Mon zip2zip training loss :M= 1(no compression) achieves\nthe lowest loss overall. Among compressed settings, M= 3 performs best, while M= 2 shows the worst\nconvergence. Larger M(4 and 5) yield slightly worse results than M= 3.\nTable 8: Ablation of hyper-encoder architecture on byte-perplexity ( ↓) across four corpora using a 1024-token\ncontext window. Performance improves with increasingly expressive architectures.\nModel Method Wiki Pile mC4 dC4\nPhi-3.5-4B averaging 1.81 1.97 2.29 2.08\n1-attention-layer 1.73 1.86 2.16 2.01\n1-transformer-layer 1.71 1.83 2.13 1.99\n2-transformer-layer 1.72 1.84 2.15 2.00\naveraging method that introduces no additional parameters. This baseline yields the highest perplexity,\nhighlighting its limited capacity. Adding a single attention layer significantly improves performance,\nand further gains are observed with a 1-layer transformer encoder. The 2-layer transformer offers\nmarginal additional benefit, suggesting that a lightweight transformer (1–2 layers) is sufficient for\neffective hypertoken modeling.\nFigure 6 illustrates the effect of hyper-encoder architecture on zip2zip training loss. We observe that\nthe simple averaging method converges the fastest but plateaus at a relatively high loss, reflecting\nits limited capacity. As model complexity increases—with attention and transformer layers—the\nconvergence becomes slower, yet the final loss is significantly lower. Notably, the 1-layer and 2-layer\ntransformer encoders yield the best performance, demonstrating that additional parameters enable the\nmodel to better capture structure, albeit at the cost of slower training dynamics.\nFigure 6: Effect of hyper-encoder architecture on zip2zip training loss. Averaging (no additional parameters)\nconverges quickly but to a higher loss. As architectural complexity increases—from attention to transformer\nlayers—convergence becomes slower, but the final loss is lower. This highlights a trade-off between training\nspeed and modeling capacity.\nB FLOPs Estimation for zip2zip\nFollowing the assumptions of Kaplan et al. [2020], we estimate training FLOPs ( Γ) as:\n18\n--- Page 19 ---\nΓ≈6·Ntokens·Nparams,\nwhere Ntokens is the total number of processed tokens and Nparams is the number of trainable parameters.\nThis estimate ignores the quadratic attention cost, assuming:\n12·dmodel≪sequence length .\nForzip2zip , this becomes:\nΓz2z≈6·Ntokens·ρ·Nparams(1 +α),\nwhere ρis the compression ratio, and αaccounts for the overhead of the hyper-encoder applied at the\nembedding and LM head. The relative FLOPs ratio is then:\nΓz2z\nΓ=ρ·(1 +α).\nAssuming the hyper-encoder mirrors the base model’s configuration, we estimate:\nα≈lM\nL,\nwhere lis the number of hyper-encoder layers, Mis the maximum merge size, and Lis the number\nof base model layers. We illustrate this estimate across several model scales in Table 9, showing that\nthe relative FLOPs overhead from the hyper-module remains modest (typically under 15%).\nModel L M l α=lM\nL\nLLM-4B 14 2 1 0.14\nLLM-7B 32 2 2 0.13\nLLM-70B 80 3 3 0.11\nLLM-400B 128 3 4 0.09\nTable 9: Relative FLOPs overhead from the hyper-module across different model sizes.\nC Additional Results\nMachine Translation\nWe report standard deviations for machine translation results across WMT benchmarks in Table 10,\ncomputed using the lm-evaluation-harness codebase.\nTable 10: Standard deviations (bootstrapped) for machine translation scores across WMT benchmarks.\nModel Method WMT14 En-Fr WMT16 En-De WMT16 En-Ro\nBLEU CHRF TER BLEU CHRF TER BLEU CHRF TER\nPhi-3-4B +Raw ±2.1 ±1.4 ±1.7 ±1.9 ±1.6 ±1.8 ±1.5 ±1.3 ±2.4\n+Finetune ±2.2 ±1.6 ±1.8 ±1.8 ±1.4 ±1.7 ±1.4 ±1.5 ±2.3\n+zip2zip ±1.9 ±1.5 ±2.0 ±1.7 ±1.6 ±1.9 ±1.6 ±1.4 ±2.5\nPhi-3-14B +Raw ±2.0 ±1.4 ±1.9 ±2.0 ±1.5 ±1.7 ±1.5 ±1.4 ±2.2\n+Finetune ±2.2 ±1.4 ±1.9 ±2.0 ±1.3 ±1.9 ±1.4 ±1.3 ±2.9\n+zip2zip ±2.1 ±1.5 ±1.8 ±2.1 ±1.6 ±1.8 ±1.5 ±1.3 ±2.6\n19\n--- Page 20 ---\nD Technical Details\nModel and Training Configuration\n•Pretrained Model: microsoft/Phi-3-medium-4k-instruct\n•Sequence Length: 1024\n•Total Batch Size: 32,768 tokens\n•Learning Rate Schedule: Cosine decay\n•Learning Rate Range: Max = 3e-4, Min = 1e-5\n•LoRA rank and alpha value: Both are 32\n•Training Steps: 10,000\n•Validation Interval: Every 100 steps\n•Checkpoint Interval: Every 500 steps\n•Pytorch Model Compilation: Enabled\nLoRA Configuration\n•Rank: 16\n•Alpha: 16\n•Target Modules: qkv_proj, o_proj, gate_proj, down_proj, up_proj\nSystem and Libraries\n•Hardware: 4 × NVIDIA A100-SXM4-80GB GPUs, 64-core CPU (128 threads)\n•Key Libraries:\n–PyTorch >= 2.5.0\n–Transformers >= 4.47.0\n–Datasets <= 3.1.0\n–Accelerate >= 0.26.0\nCompute Resources\nWe report the compute resources used for training our models in Table 11. All training was conducted\non internal servers equipped with NVIDIA H100 GPUs. We estimate GPU-hours by multiplying\nwall-clock training time by the number of GPUs used. No additional compute was used beyond the\nreported experiments; we did not perform parameter grid search, large-scale hyperparameter tuning,\nor exploratory runs that were excluded from the paper.\nTable 11: Training compute resources for zip2zip experiments.\nModel GPUs Time GPU Type GPU-Hours\nPhi-3.5-Medium (14B) 4 15h 46m NVIDIA H100 80GB 63.0\nPhi-3.5-Mini (4B) 2 7h 0m NVIDIA H100 80GB 14.0\nInference. All evaluations complete within 1 hour on a single A100 GPU, demonstrating the\nruntime efficiency of zip2zip .\nE Data Mixture\nTo support effective fine-tuning, we construct a curated dataset with balanced representation across\ndiverse domains, including code, mathematics, dialogue, general web content, and multilingual text.\nThe final dataset contains approximately 1 billion compressed tokens.\n20\n--- Page 21 ---\nDataset Domain Proportion (%)\nHuggingFaceFW/fineweb-edu [Lozhkov et al., 2024a] Web / Knowledge 20%\ndevngho/the-stack-llm-annotations-v2 [Lozhkov et al., 2024b] Code 25%\nAI-MO/NuminaMath-1.5 [LI et al., 2024] Math 20%\nHuggingFaceH4/ultrachat_200k [Ding et al., 2023] Chat / Dialogue 20%\nHuggingFaceFW/fineweb-2 [Penedo et al., 2024] Multilingual 15%\nTable 12: Training data composition across domains.\nTable 12 summarizes the constituent datasets and their respective proportions. A visualization of the\ndataset composition and sequence length characteristics is shown in Figure 7.\nThe multilingual subset in fineweb-2 includes the following languages: Mandarin Chinese ( cmn_-\nHani ), German ( deu_Latn ), Japanese ( jpn_Jpan ), Spanish ( spa_Latn ), French ( fra_Latn ), Italian\n(ita_Latn ), Portuguese ( por_Latn ), Dutch ( nld_Latn ), and Arabic ( arb_Arab ).\nFigure 7: Left: Proportional breakdown of the fine-tuning dataset across five domains. Right:\nCumulative distribution of input sequence lengths per domain (log scale). Code and multilingual data\nexhibit longer tail distributions, indicating greater variability in sequence lengths.\nF Token Stream Visualization\n21\n--- Page 22 ---\n(a) Default Tokenization of some Python code.\n (b) The same code with adaptive tokenization.\n(c) Default Tokenization of some biomedical text.\n (d) The same text with adaptive tokenization.\n(e) Default Tokenization of text in French.\n (f) The same text with adaptive tokenization.\nFigure 8: Examples comparing default and adaptive tokenization. Dotted-line frames highlight where\nthe differences are most noticeable.\n22\n--- Page 23 ---\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We have clearly stated the claims, assumption and contributions we have made\nin the abstract and introduction.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We have included a section called Discussion and Limitations in the main\ntext.\n3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: While we don’t theory results. We have tried to formalize our method with\nformulas but it’s not really theoretical contribution. And we have tried to make it clear with\nempirical results.\n4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We have described how our methods works in details and we also provide code\nand specify dataset and experimental details in the supplementary material as well as on this\nanonymous repository.\n5.Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide code and data, as well as detailed experimental settings, in the\nsupplementary material, as well as on this anonymous repository.\n6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: It can be found in the code included in the supplemental material; also\navailable on this anonymous repository. We also include the settings of key hyperparameters\nin Appendix D.\n7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: For brevity, in the main text we report experimental results in tables, and we\nleave the corresponding standard deviations in Table 10. For the two-shot accuracy, we\nreport the standard deviations in the title of Table 4, which are approximately always 0.02.\n23\n--- Page 24 ---\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n•The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n•The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n•It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n•It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n•For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n•If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provide detailed information on the computer resources in Appendix D.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n•The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n•The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?\nAnswer: [Yes]\nJustification: We have reviewed the NeurIPS Code of Ethics, and we confirm that the\nresearch conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.\nGuidelines:\n•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n•If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n•The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\n24\n--- Page 25 ---\nAnswer: [NA]\nJustification: This work does not involve any societal impacts.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n•If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n•Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n•The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n•The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n•If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper does not involve any high-risk components such as pretrained\ngenerative models or web-scraped data.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n•Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n•Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n•We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We have cited the original paper that produced the code package, dataset, and\nmodels, and included the url links when applicable.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n25\n--- Page 26 ---\n• The authors should cite the original paper that produced the code package or dataset.\n•The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n•For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n•If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n•For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n•If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: We have provided our code package and detailed settings, as well as license,\nin this anonymous link.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n•Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n•The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n•At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n•The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n•Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15.Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\n26\n--- Page 27 ---\nGuidelines:\n•The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n•Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n•We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n•For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16.Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this paper does not involve any LLM as any\nimportant, original, or non-standard components.\nGuidelines:\n•The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n•Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM ) for\nwhat should or should not be described.\n27",
  "text_length": 90869
}