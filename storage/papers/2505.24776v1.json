{
  "id": "http://arxiv.org/abs/2505.24776v1",
  "title": "Diffusion-Based Symbolic Regression",
  "summary": "Diffusion has emerged as a powerful framework for generative modeling,\nachieving remarkable success in applications such as image and audio synthesis.\nEnlightened by this progress, we propose a novel diffusion-based approach for\nsymbolic regression. We construct a random mask-based diffusion and denoising\nprocess to generate diverse and high-quality equations. We integrate this\ngenerative processes with a token-wise Group Relative Policy Optimization\n(GRPO) method to conduct efficient reinforcement learning on the given\nmeasurement dataset. In addition, we introduce a long short-term risk-seeking\npolicy to expand the pool of top-performing candidates, further enhancing\nperformance. Extensive experiments and ablation studies have demonstrated the\neffectiveness of our approach.",
  "authors": [
    "Zachary Bastiani",
    "Robert M. Kirby",
    "Jacob Hochhalter",
    "Shandian Zhe"
  ],
  "published": "2025-05-30T16:39:29Z",
  "updated": "2025-05-30T16:39:29Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24776v1",
  "full_text": "arXiv:2505.24776v1 [cs.LG] 30 May 2025Diffusion-Based Symbolic Regression Zachary Bastiani1,2Robert M. Kirby1,2Jacob Hochhalter3Shandian Zhe1 1Kahlert School of Computing2Scientific Computing and Imaging Institute 3Department of Mechanical Engineering University of Utah {u0450013, jacob.hochhalter} @utah.edu {kirby, zhe} @cs.utah.edu Abstract Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlight- ened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative pro- cesses with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach. 1 Introduction Given a dataset of measurements D={(xi, yi)}N i=1, symbolic regression aims to discover a simple mathematical expression that captures the relationship between the input and output variables, such asy= 3 sin( x1) +x2 2. Unlike traditional machine learning, where the model architecture is fixed, symbolic regression explores an open-ended space, dynamically adjusting the number, order, and type of parameters and operations. While machine learning models can also be written as mathematical expressions, they are often too complicated or opaque in form for humans to understand. Symbolic regression prioritizes simplicity and interpretability, making it especially popular among scientists and engineers who seek for not only accurate predictions but also a deeper understanding of the underlying data relationships. Interpretable models also earn greater trust, as they avoid unexplained behaviors and require less extensive testing for validation. In contrast, large, complex models often behave unpredictably, especially in regions with sparse training data. Since publication in 1994, genetic programming (GP) (Koza, 1994; Randall et al., 2022; Burlacu et al., 2020) has been the dominant approach to symbolic regression. It begins with a population of randomly generated seed expressions and iteratively evolves the population through genetic operations such as selection, crossover, and mutation, until a set of optimal equations is found. Despite its strong performance, GP is known to be computationally expensive due to the need for many generations and extensive genetic operations. To address this, Petersen et al. (2019) proposed Deep Symbolic Regression (DSR), which significantly accelerates expression discovery. DSR introduces a recurrent neural network (RNN) to sample expressions and employs a reinforcement learning framework, using a risk-seeking policy gradient, to train the RNN on the measurement dataset. DSR has since become a major baseline in symbolic regression research and development. More recent efforts have explored using pretrained foundation models (Kamienny et al., 2022; Valipour et al., 2021) to map datasets directly to candidate expressions, followed by GP and/or Monte Carlo Tree Search (MCTS) (Browne et al., 2012) to further optimize the expression(s) for a given dataset. Preprint. Under review. Most of recent symbolic regression (SR) approaches rely on a generative model for expression sampling, trained by maximizing the likelihood of the correct next token. However, diffusion methods, as another powerful generative modeling framework (Ho et al., 2020), have been relatively overlooked. Diffusion models apply a forward process that gradually adds noise to the training instances and then learn a reverse process to reconstruct the original data, effectively performing denoising. New samples are generated by starting from random noise and applying a sequence of denoising steps. Through this approach, diffusion models can produce diverse and high-quality samples. They have achieved remarkable success across various domains, including image generation (Rombach et al., 2022), audio synthesis (Huang et al., 2023), and more recently, large language model training (Nie et al., 2025). Motivated by this progress, we propose a diffusion-based deep symbolic regression method (DDSR) to generate expressions for a given measurement dataset. Our major contributions are summarized as follows. •Random Masked-Based Discrete Diffusion. We propose a discrete diffusion model for expression generation, where noise is represented by token masking. The forward process randomly masks out one token at a time. Generation starts with a fully masked (empty) sequence and progressively reconstructs the tokens step by step. This approach not only enables the generation of diverse expressions but also significantly reduces the number of denoising steps and the overall computational cost. •Token-Wise GRPO. We integrated our diffusion model into a Group Relative Policy Opti- mization (GPRO) (Shao et al., 2024b) framework for efficient reinforcement learning. At each step, we employ a risk-seeking strategy by selecting the top-performing expressions generated by our model. We maximize the per-token denoising likelihood for each expres- sion, scaled by its corresponding reward. The GRPO framework enforces updates within a trust region, thereby improving both the stability and efficiency of the learning process. •Long Short-Term Risk-Seeking. We extend the risk-seeking policy used in DSR, which selects top-performing expressions solely from the current model. While effective locally, this strategy may focus too much on short-term improvements and overlook longer-term trends. To address this, we expand the candidate pool to include top-performing expressions sampled from all model versions seen so far. This combined strategy resolves both long-term and short-term risks, aiming to build a more robust and effective model. •Experiments. We evaluated DDSR on the SRBench benchmark, comparing it against eigh- teen baseline methods. Our results show that DDSR significantly improves both solution accuracy and symbolic recovery rate on datasets with known ground-truth expressions, as compared to DSR. Moreover, DDSR achieves a higher symbolic solution rate than most genetic programming (GP) methods, while generating considerably simpler and more in- terpretable expressions. On the black-box problems, DDSR lies on the Pareto frontier, demonstrating a favorable trade-off between expression complexity and predictive perfor- mance. Ablation studies further validate the contribution of each individual component in our framework, confirming their collective importance to overall performance. 2 Background Diffusion Models. The development of the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) has fundamentally shifted the paradigm of generative modeling and inspired numerous follow-up works. For this section, we abuse notation by using xto refer to the target sample, following standard practice in diffusion literature. Given a data sample x0, DDPM defines a forward process as a Gauss-Markov chain that gradually adds Gaussian noise, producing increasingly blurred samples xt. In the limit, xtapproaches Gaussian white noise. DDPM then trains a machine learning model, typically a deep neural network, to predict the noise that was added to x0in order to obtain xt, given xtand the timestep t. The predicted noise is used to compute the reverse process, allowing the model to reconstruct xt−1fromxt. To generate a new sample, one starts by sampling Gaussian white noise xT, and then repeatedly predicts the noise to sequentially sample xT−1,xT−2,..., until recovering x0as the final result. However, DDPM is inherently designed for continuous data. For categorical data such as tokens in symbolic expressions, adding continuous Gaussian noise is neither feasible nor meaningful. To enable the feasiability of diffusion for categorical data, Austin et al. (2021) proposed the Dis- crete Denoising Diffusion Probabilistic Model (D3PM). Each data instance X0∈RM×drepresents 2 a collection of Mtokens, where each row is the one-hot encoding of a token (assuming ddif- ferent categories for each token). D3PM defines a forward process that gradually transforms the deterministic one-hot encoding X0into a uniform distribution, effectively modeling discrete white noise. Specifically, at each step t >0, the token distribution is updated via Xt=Xt−1Qt, where Qt=βtI+ (1−βt)11⊤/d,1is a vector of ones, and βt∈(0,1). It can be shown that each row ofXtremains a valid probability distribution, and as t→ ∞, each row converges to the uniform distribution. Given this forward process, one can derive the closed-form conditional distribution for sampling: q(Xt|X0) =X0Qt,Qt=Q1Q2···Qt, (1) q(Xt−1|Xt,X0) =XtQ⊤ t⊙X0Qt−1 X0QtX⊤ t, (2) where ⊙denotes element-wise multiplication. During training, a random timestep tis selected, and tokens are sampled from q(Xt|X0). These sampled tokens, along with t, are fed into a neural network tasked with predicting the initial token distribution q(X0). The model is trained by minimizing a cross-entropy loss between the predicted distribution and the ground-truth tokens. During generation, the process begins with randomly sampled tokens from the uniform distribution. At each step t, the conditional distribution q(Xt−1|Xt)is computed by marginalizing out X0in(2) with the distribution q(X0)predicted by the neural network. A sample Xt−1is drawn accordingly. This process repeats until t= 0, at which X0is obtained as the final generated sample. Deep Symbolic Regression (DSR). Given a measurement dataset, DSR trains a recurrent neural network (RNN) to generate expressions that describe the underlying data. The RNN predicts each token in the preorder traversal of the expression tree in an autoregressive manner. Training is performed via reinforcement learning, where the reward is based on the normalized root mean squared error (NRMSE) of the data fit: NRMSE =1 σyq 1 nPn i=1(yi−τ(xi))2, where τdenotes the generated expression and σyis the standard deviation of the outputs in the dataset. Since the goal is to prioritize only the best expressions generated by the model, DSR employs a risk-seeking policy, where only the top α%of expressions are used to update the model at each iteration. The risk-seeking policy gradient is defined as: R(τ) =1 1 +NRMSE (τ,x, y), (3) ∇Jrisk(θ;α) =1 Bα/100XB i=1[R(τ(i))−Rα]·1\u0010 R(τ(i))≥Rα\u0011 ∇θlog(p(τ(i)|θ)),(4) where Bis the size of expression batch sampled at each epoch, Rαis the minimum reward among the top α%expressions, R(τ)is the reward for any expression τ,1(·)is an indicator function, and θ denotes the parameters of the RNN. 3 Method 3.1 Random Masked-Based Discrete Diffusion We represent a symbolic expression as a token matrix X0∈RM×d, where each row is the one-hot encoding of a token and Mdenotes the maximum number of tokens. If the actual number of tokens is fewer than M, we pad the matrix with zero rows. While one could directly apply the D3PM method for expression generation (see Section 2), we empirically found its performance to be unsatisfactory. In D3PM, at each diffusion and denoising step, the distribution of every token is perturbed, which can severely disrupt the structure of the expression. This disruption leads to unstable and inefficient training, particularly when combined with reinforcement learning, resulting in degraded performance. Recent work of Nie et al. (2025) on large language models proposed randomly masking a portion of sequence elements at each step and training the model to reconstruct the masked elements conditioned on the remaining ones. Inspired by their success, we adopt a similar idea but with a key difference: we mask out only one token at each step. This approach gradually and smoothly blurs the expression structure, avoiding abrupt distortions and preserving most structural information, thereby promoting learning stability and efficiency. 3 Forward Pass Backward PassFigure 1: Illustration of the forward and backward process of the random masked-based diffusion. White entries represent the masked tokens. Green entries represent tokens in the original expression (left), and the generated tokens (right). Specifically, let qtdenote the token index to be masked at time step t, and let qt={q1,..., q t} represent the set of all masked indices up to step t. Given X0, we sample qtandXtas follows: qt∼Uniform( {1,..., M }\\qt−1), Qt=I−diag(eqt), Xt=QtX0, Qt=QtQt−1...Q1, (5) where eqtis a one-hot vector with one at position qtand zeros elsewhere. We design a Transformer network ϕθthat takes Xtas input and predicts q(X0)— the token distribution matrix corresponding toX0. The architectural details of ϕθare provided in Appendix B. Training of ϕθis integrated into a reinforcement learning framework, whose details are described later. To generate an expression, we begin with a full zero matrix XM, representing that all tokens are masked. At each backward step t=M, M −1,..., we input Xtintoϕθto predict the distribution q(X0). We use q(X0)to sample the masked tokens in Xt, combining them with the unmasked tokens to form an intermediate instance of X0. We then apply (5)to obtain the sample of Xt−1. This iterative process continues, reconstructing one token at a time, until we recover a complete sample of X0(i.e.,t= 0). At each step, we preserve the tokens that have already been reconstructed. Generation terminates early if the current token matrix forms a valid expression. If the final X0 does not represent a valid expression, we randomly replace invalid tokens until a valid expression is obtained. The full generation process is summarized in Algorithm 2 of the Appendix. 3.2 Reinforcement Learning with Token-Wise GPRO To train the diffusion model ϕθusing a given measurement dataset D, we adopt a reinforcement learning framework. Specifically, we employ a risk-seeking strategy similar to that used in DSR. At each training step, we select the top α%of expressions generated by our model based on their NRMSE of the data fit. For each expression τ(i), we assign a reward R(τ(i))as defined in (3). We then update ϕθto encourage the generation of expressions with similarly high rewards. More concretely, let X(i) 0denote the token matrix representation of expression τ(i). We randomly select a diffusion step tand generate a noisy version X(i) tusing the forward process as described in(5). Feeding X(i) tintoϕθ, we obtain a prediction of q(X(i) 0). The model is trained to maximize the log-likelihood of X(i) 0scaled by the relative reward Ai=R(τ(i))−Rα, under the predicted q(X(i) 0). This leads to the following optimization objective: maximize θEthX τ(i)∈SαAi·logp\u0010 X(i) 0|ϕθ(X(i) t)\u0011i, (6) where Sαdenotes the set of top α%expressions, and the likelihood p(X(i) 0|ϕθ(X(i) t)) =Q kp(x(i) k|η(i) k,t,θ), each x(i) kis the k-th row of X(i)corresponding to an non-empty token, and η(i) k,t,θis the predicted probability distribution for that token, i.e.,thek-th row of ϕθ(X(i) t). 4 To further enhance training stability and efficiency, we adapt the Group Relative Policy Optimization (GRPO) framework (Shao et al., 2024a) to optimize (6). Instead of using all token likelihoods equally, we selectively use them to update model parameters based on a trust region criterion: we only use token likelihoods whose variation remains within a bounded range relative to the earlier model. Specifically, the model update is given by: θ←θ+γ∇JGRPO(θ;α), JGRPO(θ;α) =1 Bα/100X τ(i)∈SαX kmin{hθkt,clip(hθkt,1−ϵ,1 +ϵ)} ·Ai−β·gθkt,(7) where γ >0is the learning rate, hθkt=p(x(i) k|η(i) k,t,θ) p(x(i) k|η(i) k,t,θold)is the likelihood ratio of k-th token between the current model and the model at the beginning of the current epoch (with parameters θold), gθkt=KL[p(x(i) k|η(i) k,t,θ)∥p(x(i) k|η(i) k,t,θ ref)is a regularization term that regularizes the current model not to deviate too much from a reference model with parameters θref. Here β >0controls the strength of regularization, and KL(·∥·)denotes the Kullback–Leibler divergence. By restricting updates to tokens whose likelihood ratios hθktlie within the trust region [1−ϵ,1 +ϵ], we prevent unstable and potentially harmful updates caused by outlier token samples. As with DSR, we additionally append an entropy gradient for the generated expression distributions to encourage exploration and mitigate the risk of model collapse. 3.3 Long Short-Term Risk-Seeking The risk-seeking policy used in DSR leverages only the top-performing expressions generated by the current model. However, this local policy can limit the exploitation capability of reinforcement learning. To encourage broader exploitation and to drive the model toward generating diverse yet high-quality expressions, we expand the candidate set by incorporating the top α%expressions not only from the current epoch but also from all previous epochs. Specifically, at each epoch k, we update the candidate pool as follows: Sα← S α∪ Sk α, (8) where Sk αdenotes the top α%expressions sampled at epoch k. We then set Rαto the minimum reward among all expressions in Sα, and update the model accordingly following (7). To prevent the buffer from growing indefinitely, after each epoch we remove the bottom α%expressions with the lowest rewards from Sα. This policy can be viewed as a hybrid of the risk-seeking strategy in DSR and the priority training queue proposed in (Mundhenk et al., 2021). By jointly considering both short-term and long-term risks, our approach prevents the model from drifting away from well-performing but hard-to-sample expressions. By continuously updating from such expressions, the model progressively improves its ability to generate high-quality outputs. A full summary of our training procedure is provided in Algorithm 1. 4 Related Work The Deep Symbolic Regression (DSR) framework (Petersen et al., 2019) pioneered the use of rein- forcement learning to train RNN-based expression generators from the measurement data. Building on this framework, recent extensions (Tenachi et al., 2023; Jiang et al., 2024) have enforced physics- unit constraints as domain knowledge to enhance the quality of expression generation or confined the search space through vertical discovery strategies for vector symbolic regression. Another line of work has shifted toward building foundation models that map numerical measurements outright to symbolic expressions (Biggio et al., 2021; Kamienny et al., 2022; Valipour et al., 2021; Vastl et al., 2022). These models typically adopt an encoder-decoder transformer architecture, where encoder layers extract structural patterns from the input data, and decoder layers synthesize symbolic outputs. Although these approaches are promising, the training is costly and acutely sensitive to data preparation pipelines, often requiring massive synthetic datasets. More critically, without a data-specific search mechanism, such models often struggle to generalize — especially when faced with out-of-distribution measurement datasets (Kamienny et al., 2023). To overcome these limitations, a new wave of research couples pretrained foundation models with explicit search or planning mechanisms tailored to the target dataset, for instance, TPSR (Shojaee et al., 5 Algorithm 1 Diffusion based Deep Symbolic Regression (DDSR) input Learning rate γ; risk factor α; expression batch size B; steps per epoch C; epochs per reference G; number of epochs N; entropy scalar λ output The best equation τ∗ 1:Initialize transformer ϕwith parameters θ 2:Sα← {} 3:fori= 0toN−1do 4: ifimod(G) = 0 then 5: θref←θ 6: end if 7: θold←θ 8: Sample Bexpression with the current model, and obtain the top α%expressions Si α 9:Sα← S α∪ Si α 10: SetRαto the minimum award among the expressions in Sα. 11: forj= 1toCdo 12: Randomly sample diffusion time step t 13: Compute JGRPO from (7) 14: θ←θ+γ(∇θJGRPO +λ·Entropy-Gradient ) 15: end for 16: Remove the bottom α%expressions from Sαaccording to the rewards 17:end for 18:return the best expression from Sα 2023), GPSR (Holt et al., 2023), and GPSR-MCTS (Kamienny et al., 2023). In TPSR, the pretrained model is integrated into a modified Monte Carlo Tree Search (MCTS) method (Browne et al., 2012), using model-guided token selection and tree expansion driven by an upper confidence bound (UCB) heuristic. GPSR combines a pretrained encoder-decoder model with genetic programming (GP) at inference time: decoder-generated expressions seed the initial GP population, and top candidates are used to iteratively fine-tune the decoder. GPSR-MCTS uses a flexible MCTS search model including a mutation policy network, augmented with critic layers; The mutation policy network is pretrained on external datasets and then fine-tuned on the task-specific data while the critic layers are trained from scratch on the task-specific data. Beyond these model-based approaches, ensemble frameworks such as uDSR (Landajuela et al., 2022) combine multiple symbolic regression strategies ( e.g., GP and DSR) to boost robustness and accuracy. Meanwhile, model-free methods (Sun et al., 2023; Xu et al., 2024) explore purely search-based techniques, using MCTS or ensemble strategies combining MCTS with GP, to uncover symbolic expressions without relying on heavy pretraining. In the domain of discrete diffusion, beyond D3PM (Austin et al., 2021), several works have explored graph (Vignac et al., 2023) and tree (Li et al., 2024) generation. Both graph diffusion models employ a modified transition matrix of the form Qt= (1−βt)I+βt1m⊤, where βtis a scalar determined by the diffusion schedule, and mrepresents the marginal distribution over nodes (or edges) in the training data. More recently, Nie et al. (2025) proposed a masked discrete distribution framework, training models in a supervised learning setting to improve sampling quality in large language models. Our approach adopts a similar masking idea but differs in two key aspects: (1) at each step, we mask out only a single token to preserve structural information more effectively, and (2) we integrate the diffusion process into a reinforcement learning framework to guide training toward high-reward expressions. 5 Experiment 5.1 Performance on SRBench We first evaluated DDSR on the well-known and comprehensive SRBench dataset (La Cava et al., 2021), which is divided into two groups: 133 problems with known ground-truth solutions and 120 black-box problems without known solutions. For the first group, four noise levels are considered: 6 0 20 40 60 80DDSR DSR-W/C DSR-W/OC TPSR AFP AFP_FE Bingo EPLEX FEAT FFX GP-GOMEA ITEA MRGP Operon SBP-GP gplearn AIFeynman BSR uDSR Symbolic Solution Rate (%) 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy Rate 101102103104 Simplified Complexity Target Noise 10% 1% 0.1% 0%Figure 2: Performance on SRBench problems with ground-truth solutions. Error bars denote a 95% confidence interval. uDSR’s high performance is attributed to it being an ensemble method that combines several SR methodologies. 0%, 0.1%, 1%, and 10%. All experiments were conducted on A40s from the NCSA Delta cluster1. Each A40 ran 8 trials in parallel. For each problem at each noise level, we ran DDSR eight times, with each run capped at a four-hour time limit. The hyperparameter settings used by DDSR are detailed in Appendix Table 2. We compared DDSR against eighteen existing symbolic regression (SR) methods, spanning GP-based, MCTS-based, deep learning-based, and ensemble approaches. Notably, we include two versions of DSR in the comparison: the original version without constant tokens and an extended version that incorporates and optimizes constant tokens during training. We denote these as DSR-W/OC and DSR-W/C, respectively. DSR is the most comparable method to DDSR, as it also uses a reinforcement learning framework and learns directly from data, without relying on pretrained models. AIFeynman is a method that searches for hyperplanes in the dataset and fits each with a polynomial (Udrescu and Tegmark, 2020). Gplearn, Bingo (Randall et al., 2022), GP-GOMEA (Virgolin et al., 2021), SBP-GP (Virgolin et al., 2019), Operon (Burlacu et al., 2020), and MRGP (Arnaldo et al., 2014) all incorporate GP as a primary or supporting component. TPSR combines a pretrained foundation model with a Monte Carlo Tree Search to find optimal expressions, while uDSR ensembles a pretrained model with DSR, GP, AIFeynman, and linear models. For problems with known solutions, we evaluated each method on symbolic solution rate, accuracy rate, and simplified complexity. The symbolic solution rate is computed using two criteria: symbolic equivalence, as determined by the SymPy library2, or an R2score of exactly 1.0. Accuracy rate is a binary metric indicating whether the method finds an expression with R2>0.999. The simplified complexity is defined as the number of tokens in the expression tree after simplification by SymPy. We present results for the most comparable and several representative methods in Table 1. Full comparison results are provided in Figure 2 and Appendix Table 3. Table 1: Performance on SRBench problems with ground truth solutions. Symbolic Solution Rate (%) Accuracy Rate (%) Algorithm 0.0 0.001 0.01 0.1 0.0 0.001 0.01 0.1 DDSR 46.54 27.02 20.33 10.69 60 60 59 56 DSR-W/C 24.81 24.42 17.53 10.48 38 41 40 39 DSR-W/OC 19.71 19.23 18.92 16.61 24 25 25 25 GP-GOMEA 43.08 10.62 4.69 1.46 71 70 73 68 TPSR 36.09 0.00 0.00 0.00 68 68 66 38 1https://www.ncsa.illinois.edu/research/project-highlights/delta/ 2https://www.sympy.org/ 7 We observe that DDSR substantially outperforms DSR in nearly all settings, with the exception of the 10% noise level, where DSR-W/OC achieves a higher solution rate. This may be attributed to the larger token space employed by DDSR. Since DSR-W/OC excludes constant tokens, its reduced token space makes the search process more conservative, potentially offering greater robustness in the presence of high noise. However, even at the 10% noise level, DDSR still achieves substantially higher solution accuracy than DSR-W/OC. When DSR uses the same token space as DDSR— namely, in the DSR-W/C variant — both its solution rate and accuracy consistently lag behind DDSR, demonstrating the advantage of our diffusion-based framework. DDSR also outperforms TPSR, GP-GOMEA, SBP-GP, and many other GP-based methods in terms of solution rate, particularly under non-zero noise levels. Although these methods often achieve higher solution accuracy, the expressions they generate tend to be much longer and more complex. For instance, TPSR and GP-GOMEA yield average simplified complexities of 61.4 and 35.4, respectively, while DDSR maintains a significantly lower average of 17.7. These results underscore the robustness of DDSR and its ability to generate more interpretable symbolic expressions. AIFeynman outperforms DDSR in symbolic solution rate in the noiseless setting, which aligns with its algorithmic design. AIFeynman fits augmented polynomials and excels when the target expression is a clean polynomial — common among the noiseless symbolic problems in SRBench. However, its performance degrades sharply in black-box settings; its average R2score falls below zero, leading to its exclusion from Figure 3. Lastly, while uDSR — an ensemble method — still achieves higher overall performance than DDSR, it is important to note that DDSR can be seamlessly incorporated into the uDSR pipeline. It could either replace DSR or be added as a complementary component, further enhancing the ensemble’s effectiveness in symbolic expression discovery. 0.0 0.2 0.4 0.6 0.8 1.0 R²020406080100Model Size AFP*AFP_FE* BSR* BingoDDSR DSR-W/C DSR-W/OCEPLEX*FEAT* GP-GOMEA* LinearOperon*TPSR* gplearn* Figure 3: Mean R2score vs.model size on black- box problems. The numerical values are reported in Appendix Table 5.For the black-box problems — where ground- truth solutions are unavailable — we evaluated model performance using the trade-off between average R2score and expression size, as shown in Figure 3. DDSR lies at the frontier of the Pareto curve, indicating that it achieves one of the best balances between accuracy and inter- pretability. In other words, DDSR offers high data-fitting performance while maintaining rel- atively concise expressions. As an example for comparison, TPSR achieves slightly higher R2 scores, but the resulting expressions are sub- stantially more complex, averaging around 70 tokens. Running Time. Average running time of each method is reported in Appendix Table 4. When using the same token space, DDSR requires only about 1/2 of the runtime of DSR (specifically, DSR-W/C), highlighting the training efficiency enabled by our diffusion-based framework. Note that DDSR-W/OC took less running time due to its reduced token space — no constant tokens — and the thereof lack of constant optimization step. 5.2 Ablation Studies Next, we conducted ablation studies to assess the effectiveness of individual components in our method. Specifically, we evaluated three variants of DDSR by: (1) replacing our random mask-based discrete diffusion with the standard Discrete Denoising Diffusion Probabilistic Model (D3PM) (see Section 2); (2) replacing Grouped Relative Policy Optimization (GRPO) with the standard risk- seeking policy gradient (RSPG); (3) substituting our Long-Short-Term (LST) policy with a short-term (ST) policy that selects the top α%expressions only from the current model. We tested these variants on the Feynman and Strogatz datasets in SRBench. The solution accuracy results are shown in Figure 4. 8 DDSR DDSR-RSPGDDSR-STD3PM Algorithm52.553.053.554.054.555.055.556.056.5Accuracy Rate (%)55.2%55.6% 53.9% 53.0%(a) Feynman dataset DDSR DDSR-RSPGDDSR-STD3PM Algorithm556065707580859095100Accuracy Rate (%)96.4% 92.9% 92.9% 60.7% (b) Strogatz dataset Figure 4: DDSR ablations accuracy rate ( %) for the Feynman and Strogatz datasets. 0 100 200 300 400 500 600 Epoch0.60.70.80.91.0Reward DDSR-GRPO DDSR-RSPG (a)feyman_ll_6_11 0 100 200 300 400 500 600 Epoch0.60.70.80.91.0Reward DDSR-GRPO DDSR-RSPG (b)strogatz_predprey2 0 100 200 300 400 500 600 Epoch0.60.70.80.9Reward DDSR-GRPO DDSR-RSPG (c)feyman_test_3 Figure 5: Learning curves examples for DDSR with GPRO and with RSPG. Error bars denote one standard deviation. First, our random mask-based diffusion improves solution accuracy rate — by 2.2% on the Feynman dataset and 35.7% on the Strogatz dataset — compared to the standard D3PM. This improvement may stem from the fact that D3PM perturbs all tokens at each diffusion step, which can introduce instability during training. In contrast, our method perturbs only one token at each step, providing more stable and effective learning dynamics. Second, while RSPG leads to a modest 0.4% improvement on the Feynman dataset, it results in a 3.5% performance drop on the Strogatz dataset relative to GRPO. This highlights GRPO’s robustness. Moreover, GRPO accelerates training: on average, it converges 30 epochs faster than RSPG. Representative learning curves for randomly selected problems are shown in Figure 5 and Appendix Figure 8. Third, the LST policy outperforms the ST policy by 1.3% and 3.5% on the Feynman and Stro- gatz datasets, respectively. This demonstrates the benefit of leveraging historically top-performing expressions to enhance exploitation. 6 Conclusion We have introduced DDSR— a random masked discrete diffusion model for symbolic regression. Our experimental results on the SRBench benchmark demonstrate that DDSR outperforms deep reinforcement learning-based approaches and achieves performance comparable to state-of-the-art genetic programming (GP) methods. However, DDSR has some limitations. In particular, it tends to require longer runtimes to discover solutions for complex problems and exhibits reduced robustness to high levels of noise in the data. Future work may explore extending the discrete diffusion framework to supervised learning of foundational models, improving the efficiency of the diffusion transformation process, and refining the reward function to enhance exploitation and increase robustness to noisy data. 9 References Arnaldo, I., Krawiec, K., and O’Reilly, U.-M. (2014). Multiple regression genetic programming. In Proceedings ofthe2014 Annual Conference onGenetic andEvolutionary Computation, GECCO ’14, page 879–886, New York, NY, USA. Association for Computing Machinery. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. (2021). Structured denoising diffusion models in discrete state-spaces. Advances inneural information processing systems, 34:17981–17993. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Parascandolo, G. (2021). Neural symbolic regression that scales. In International Conference onMachine Learning (ICML), pages 936–945. Pmlr. Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S. (2012). A survey of monte carlo tree search methods. IEEE Transactions onComputational Intelligence andAIingames, 4(1):1–43. Burlacu, B., Kronberger, G., and Kommenda, M. (2020). Operon C++: an efficient genetic program- ming framework for symbolic regression. In Proceedings ofthe2020 Genetic andEvolutionary Computation Conference Companion, GECCO ’20, pages 1562–1570, New York, NY, USA. Association for Computing Machinery. Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances inneural information processing systems, 33:6840–6851. Holt, S., Qian, Z., and van der Schaar, M. (2023). Deep generative symbolic regression. In The Eleventh International Conference onLearning Representations. Huang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A., Chen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., Engel, J., Le, Q. V., Chan, W., Chen, Z., and Han, W. (2023). Noise2music: Text-conditioned music generation with diffusion models. Jiang, N., Nasim, M., and Xue, Y. (2024). Vertical symbolic regression via deep policy gradient. In Proceedings oftheThirty-Third International Joint Conference onArtificial Intelligence (IJCAI), pages 5891–5899. Kamienny, P.-A., d’Ascoli, S., Lample, G., and Charton, F. (2022). End-to-end Symbolic Regression with Transformers. Kamienny, P.-A., Lample, G., Lamprier, S., and Virgolin, M. (2023). Deep generative symbolic regression with monte-carlo-tree-search. Koza, J. R. (1994). Genetic programming as a means for programming computers by natural selection. Statistics andComputing, 4(2):87–112. La Cava, W., Orzechowski, P., Burlacu, B., de Fran cca, F. O., Virgolin, M., Jin, Y., Kommenda, M., and Moore, J. H. (2021). Contemporary Symbolic Regression Methods and their Relative Performance. arXiv:2107.14351 [cs]. Landajuela, M., Lee, C., Yang, J., Glatt, R., Santiago, C. P., Aravena, I., Mundhenk, T. N., Mulcahy, G., and Petersen, B. K. (2022). A unified framework for deep symbolic regression. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, Advances inNeural Information Processing Systems. Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares. Quarterly ofApplied Mathematics, 2(2):164–168. Li, M., Shitole, V., Chien, E., Man, C., Wang, Z., Srinivas, Zhang, Y., Krishna, T., and Li, P. (2024). LayerDAG: A layerwise autoregressive diffusion model of directed acyclic graphs for system. In Machine Learning forComputer Architecture andSystems 2024. 10 Mundhenk, T., Landajuela, M., Glatt, R., Santiago, C. P., faissol, D., and Petersen, B. K. (2021). Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 24912–24923. Curran Associates, Inc. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. (2025). Large language diffusion models. Petersen, B. K., Landajuela, M., Mundhenk, T. N., Santiago, C. P., Kim, S. K., and Kim, J. T. (2019). Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. Randall, D. L., Townsend, T. S., Hochhalter, J. D., and Bomarito, G. F. (2022). Bingo: a customizable framework for symbolic regression with genetic programming. In Proceedings oftheGenetic and Evolutionary Computation Conference Companion, GECCO ’22, pages 2282–2288, New York, NY, USA. Association for Computing Machinery. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024a). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. (2024b). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Shojaee, P., Meidani, K., Farimani, A. B., and Reddy, C. K. (2023). Transformer-based Planning for Symbolic Regression. Sun, F., Liu, Y., Wang, J.-X., and Sun, H. (2023). Symbolic physics learner: Discovering governing equations via monte carlo tree search. In The Eleventh International Conference onLearning Representations. Tenachi, W., Ibata, R., and Diakogiannis, F. I. (2023). Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws. TheAstrophysical Journal, 959(2):99. Udrescu, S.-M. and Tegmark, M. (2020). AI Feynman: a Physics-Inspired Method for Symbolic Regression. arXiv:1905.11481 [hep-th, physics:physics]. Valipour, M., You, B., Panju, M., and Ghodsi, A. (2021). SymbolicGPT: A Generative Transformer Model for Symbolic Regression. arXiv:2106.14131 [cs]. Vastl, M., Kulhánek, J., Kubalík, J., Derner, E., and Babu vska, R. (2022). SymFormer: End-to-end symbolic regression using transformer-based architecture. arXiv:2205.15764 [cs]. Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., and Frossard, P. (2023). Digress: Discrete denoising diffusion for graph generation. Virgolin, M., Alderliesten, T., and Bosman, P. A. N. (2019). Linear scaling with and within semantic backpropagation-based genetic programming for symbolic regression. In Proceedings ofthe Genetic andEvolutionary Computation Conference, GECCO ’19, page 1084–1092, New York, NY, USA. Association for Computing Machinery. Virgolin, M., Alderliesten, T., Witteveen, C., and Bosman, P. A. N. (2021). Improving Model-based Genetic Programming for Symbolic Regression of Small Expressions. Evolutionary Computation, 29(2):211–237. arXiv:1904.02050 [cs]. Xu, Y., Liu, Y., and Sun, H. (2024). Reinforcement symbolic regression machine. In TheTwelfth International Conference onLearning Representations. 11 Appendix A Algorithms A.1 Sampling Expressions We sample an expression from a distribution matrix of size M×d, where Mdenotes the maximum length of the expression, and ddenotes the number of tokens in the library. At each step in the process, we get a vector of valid tokens for the current node based on the current incomplete version ofτ. This vector enforces rules to avoid the expression from being invalid. DDSR further restricts that the sampled expression trees can include up to 10 constants, and trigonometric functions such assinandcoscan not be nested. The rule for setting a maximum number of constant tokens can help reduce optimization runtime and mitigate overfitting. Constant tokens are optimized with the Levenberg-Marquardt algorithm (Levenberg, 1944) for each discovered equation. The prevention of nested trigonometric functions is inherited from DSR, as the authors of DSR claimed nested trigonometric functions do not occur in physics. Algorithm 2 Sampling expressions for a given sequence of categorical distributions input probability distribution p, max depth of the tree M output An expression τ 1:fori= 0toMdo 2: r←Get Valid Tokens (τ, i) 3: pi←pi·r 4: ifPd j=1pi,j= 0then 5: pi←1∗r 6: end if 7: pi←pi/Pd j=1pi,j 8: τi∼pi 9:end for 10:return τ Ordering: Converting expression trees into a vector of tokens can be done in various ways. We used the tree’s breadth-first search ordering (BFS) to order the nodes/tokens. This ordering keeps siblings of parents nodes close to each other in the ordering. In comparison, the preorder traversal ordering (POT) can place siblings far apart. Figure 6 shows the conversion of an expression tree into a breadth-first search ordering and the comparison to a preorder traversal ordering. B Model Architecture The positional encoding in the input layer enables the attention layers to capture positional-based relationships of the tokens. In addition to generating tokens from the BFS search order, our diffusion model also involves a time step variable tinforming the progress of the diffusion and denoising. To integrate the information from both from token positions land time steps t, we introduce a two-dimensional encoding as defined below. The full architecture of our model is shown in Figure 7. The hyperparameter settings are listed in Table 2. PE(l, t)2i= sin(l 10000(4i/D)),PE(l, t)2i+1= cos(l 10000(4i/D)), PE(l, t)D+2j= sin(t 10000(4j/D )),PE(l, t)D+2j+1= cos(t 10000(4j/D )), (9) C Additional Results Table 3 provides the numerical values of the symbolic solution rate, accuracy rate and simplified complexity of all the methods offered by SRBench for problems with known solutions. Table 5 reports the R2scores, model size, and training time of every method on the black-box problems of 12 BFS: POT:Figure 6: Breadth first search ordering of an expression compared to the pre-order traversal Table 2: Hyperparameter settings of DDSR. Hyperparameter DDSR Variables {1,c(Constant Token), xi} Unary Functions {sin, cos, log,p (·), exp} Binary Functions {+, -, *, /, ˆ} Batch Size 1000 Risk Seeking Percent ( α) 5% Optimizer ADAM Learning Rate 1E-4 Max Depth 32 Oversampling 3 Number of Epochs 600 Entropy Coefficient λ 0.0005 Encoder Number 1 Decoder Number 1 Number of Heads 1 Feed Forward Layers Size 2048 β 0.01 ϵ 0.2 Embedding Dim 15 C 5 G 5 SRBench. DDSR has similar performance to Bingo and GP-GOMEA on the black box problems, performing as an improvement in reducing the complexity of GP-GOMEA and increasing the R2 score of Bingo. Significantly, DDSR outperforms four of the five machine learning methods from SRBench (linear fit, random forests, AdaBoost, and MLPs) in R2while offering an interpretable model. XGBoost is the only machine learning method that outperforms DDSR in R2score by 0.046 while increasing the model size by 713x and losing any natural interpretability. 13 BFS Expression Tree Output Categorical DistributionAttention Block Normalization Layer Attention Block Normalization Layer Feed Forward Layer Linear Layer SoftmaxAttention Block Normalization Layer Feed Forward LayerPositional Encoding One Hot Encoding Linear Layer Relu Linear LayerNormalization Layer+ ++ + + Feed Forward LayerEncoding LayerDecoding Layer+Positions Timestep Linear LayerFigure 7: The architecture of the diffusion model in DDSR. 14 0 200 400 600 Epoch0.50.60.70.80.91.0Rewardfeynman_II_15_5 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.910.920.930.940.950.960.97Rewardfeynman_I_15_3t DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.5250.5500.5750.6000.6250.6500.6750.700Rewardfeynman_I_30_3 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.50.60.70.80.91.0Rewardstrogatz_vdp1 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_II_10_9 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.750.800.850.900.951.00Rewardfeynman_I_34_14 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.750.800.850.900.951.00Rewardfeynman_II_8_31 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.600.650.700.750.800.850.90Rewardfeynman_test_8 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_II_6_11 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_III_13_18 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.700.750.800.850.900.951.001.05Rewardfeynman_I_12_5 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardstrogatz_lv2 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.50.60.70.80.91.0Rewardfeynman_III_15_12 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.500.550.600.650.700.75Rewardfeynman_test_20 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_II_34_2a DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.650.700.750.800.850.900.95Rewardfeynman_test_5 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_I_18_14 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.600.650.700.750.800.850.900.951.00Rewardfeynman_II_11_27 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.50.60.70.80.9Rewardfeynman_I_44_4 DDSR-GRPO DDSR-RSPG 0 200 400 600 Epoch0.60.70.80.91.0Rewardfeynman_I_32_5 DDSR-GRPO DDSR-RSPGFigure 8: Learning curves of 20 problems for DDSR with GPRO and with RSPG. These problems are randomly selected from the Feynman and Strogatz dataset. Colored regions denote one standard deviation. Table 3: Performance on SRBench problems with known solutions. Symbolic Solution Rate (%) Accuracy Rate (%) Simplified Complexity Algorithm 0.0 0.001 0.01 0.1 0.0 0.001 0.01 0.1 0.0 0.001 0.01 0.1 AFP 22.31 19.92 16.85 12.85 43 42 40 41 29.35 28.43 29.01 30.82 AFP_FE 28.08 22.69 20.31 12.85 56 50 50 50 34.58 35.66 34.45 36.77 AIFeynman 61.84 31.89 12.61 0.86 74 74 68 10 83.29 88.66 99.27 110.54 BSR 2.50 0.61 0.08 0.00 12 11 12 7 34.29 35.51 36.80 38.38 Bingo 48.77 14.62 4.77 0.77 64 60 62 59 15.56 19.29 21.32 22.54 DDSR 46.54 27.02 20.33 10.69 60 60 59 56 17.33 17.13 17.87 18.48 DSR-W/C 24.81 24.42 17.53 10.48 38 41 40 39 16.57 16.10 16.96 18.45 DSR-W/OC 19.71 19.23 18.92 16.61 24 25 25 25 13.14 14.36 14.61 14.40 EPLEX 12.50 9.92 8.77 9.54 44 45 52 47 53.24 51.74 49.91 40.04 FEAT 0.10 0.00 0.00 0.00 40 43 41 14 88.01 77.32 72.61 50.40 FFX 0.00 0.00 0.00 0.08 0 0 3 18 274.88 273.29 286.03 341.38 GP-GOMEA 43.08 10.62 4.69 1.46 71 70 73 68 25.73 32.75 37.59 45.41 ITEA 20.77 13.77 7.69 1.46 27 27 27 26 14.46 14.96 15.35 16.00 MRGP 0.00 0.00 0.00 0.00 93 92 89 2 109.95 106.50 83.06 0.00 Operon 16.00 12.31 1.92 0.08 87 86 86 73 40.80 40.13 60.40 70.78 SBP-GP 22.69 0.69 0.00 0.00 74 74 75 54 109.94 102.09 112.93 116.30 TPSR 36.09 0.00 0.00 0.00 68 68 66 38 57.11 59.32 63.42 65.83 gplearn 16.15 16.86 16.59 16.00 30 29 27 22 45.80 37.76 36.42 33.84 15 Table 4: Average run time for each method on SRBench symbolic dataset. Algorithm Run Time (s) AFP 3488.63 AFP_FE 28830.37 AIFeynman 29590.66 Bingo 22542.44 BSR 28800.16 DDSR 14441.12 DSR-W/C 27131.73 DSR-W/OC 630.83 EPLEX 10865.90 FEAT 1079.46 FFX 17.34 GP-GOMEA 2072.25 ITEA 1411.92 MRGP 18527.59 Operon 2483.09 SBP-GP 28968.48 TPSR 172.87 gplearn 1396.98 Table 5: Performance on black-box problems of SRBench. Algorithm R² Test AFP 0.657613 34.5 AFP_FE 0.664599 35.6 AIFeynman -3.745132 2500 AdaBoost 0.704752 10000 BSR 0.257598 19.8 Bingo 0.711951 22.2 DDSR 0.730218 23.8 DSR-W/OC 0.571669 8.89 DSR-W/C 0.642417 14.8 EPLEX 0.760414 55.8 FEAT 0.784662 74.2 FFX -0.667716 1570 GP-GOMEA 0.746634 27.3 ITEA 0.640731 112 KernelRidge 0.615147 1820 LGBM 0.637670 5500 Linear 0.454174 17.4 MLP 0.531249 3880 MRGP 0.417864 12100 Operon 0.794831 65.0 RandomForest 0.698541 1.54e+06 SBP-GP 0.798932 639 TPSR 0.792001 95.7 XGB 0.775793 16400 gplearn 0.541264 16.3 16",
  "text_length": 47974
}