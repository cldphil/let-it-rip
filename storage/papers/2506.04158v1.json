{
  "id": "http://arxiv.org/abs/2506.04158v1",
  "title": "Image Editing As Programs with Diffusion Models",
  "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
  "authors": [
    "Yujia Hu",
    "Songhua Liu",
    "Zhenxiong Tan",
    "Xingyi Yang",
    "Xinchao Wang"
  ],
  "published": "2025-06-04T16:57:24Z",
  "updated": "2025-06-04T16:57:24Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04158v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04158v1  [cs.CV]  4 Jun 2025Image Editing As Programs with Diffusion Models\nYujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, and Xinchao Wang∗\nNational University of Singapore\n{yujia.hu,songhua.liu,zhenxiong,xyang}@u.nus.edu,xinchao@nus.edu.sg\nOriginal Instruction: What would it be like if we placed the lady in a forest during autumn, and she was wearing a white dress, with a fox beside her? \nSimple Instruction Editing\nReplace coffee with a cat.Change expression to not smiling. Minify the plane.\nAdd a dog on the grass and \nremove the flowers.Change the expression to \nsmiling and make it a sketch.Put the cat on the beach, \nmake it decorated in a \nChristmas style and be in \norange color.Change the material of the \nsign to wood and turn the red \ncar into a tiger. Change the action of the \nbaby to crawling and make it \na stormy day.\nComplex Instruction Editing-Results Only\nComplex Instruction Editing-Step Display\nInput Change the background to the forest. Make the lady wear a white dress. Add a fox beside the lady. Change the time to autumn.\nEnlarge the unicorn on the \nleft and move it on the lake.\nMove the plant on the cupboard.\nFigure 1: Visual results of our IEAP. Rows 1 and 3 showcase complex multi-step edits (Row 1 is\nfurther decomposed into individual instructions), while Row 2 shows single-instruction edits. Single\ninstructions are underlined if needing to be reduced to atomic operations.\nAbstract\nWhile diffusion models have achieved remarkable success in text-to-image genera-\ntion, they encounter significant challenges with instruction-driven image editing.\nOur research highlights a key challenge: these models particularly struggle with\nstructurally inconsistent edits that involve substantial layout changes. To mitigate\nthis gap, we introduce ImageEditing AsPrograms (IEAP), a unified image editing\nframework built upon the Diffusion Transformer (DiT) architecture. At its core,\nIEAP approaches instructional editing through a reductionist lens , decomposing\ncomplex editing instructions into sequences of atomic operations. Each operation\nis implemented via a lightweight adapter sharing the same DiT backbone and is\nspecialized for a specific type of edit. Programmed by a vision-language model\n(VLM)-based agent, these operations collaboratively support arbitrary and struc-\nturally inconsistent transformations. By modularizing and sequencing edits in this\n∗Corresponding Author\nPreprint. Under review.\n--- Page 2 ---\nway, IEAP generalizes robustly across a wide range of editing tasks, from simple ad-\njustments to substantial structural changes. Extensive experiments demonstrate that\nIEAP significantly outperforms state-of-the-art methods on standard benchmarks\nacross various editing scenarios. In these evaluations, our framework delivers supe-\nrior accuracy and semantic fidelity, particularly for complex, multi-step instructions.\nCodes are available here.\n1 Introduction\nImage editing lies at the heart of a wide range of applications from photo retouching and content\ncreation to visual storytelling and scientific visualization [ 42,5,62]. With the advent of diffusion\nmodels [ 23,53,47], the field has shifted towards highly precise and controllable manipulations\n[45,12,61]. The inherently progressive denoising process enables multi-stage pipelines [ 24,4,2]\nand localized editing methods [ 10,74,57], and its native support for multi-modal inputs has inspired\nunified frameworks that integrate heterogeneous signals within a single model [33, 15, 68, 18].\nMore recently, text-to-image pipelines based on Diffusion Transformers (DiTs) [ 46,13,31] have set\nnew standards in generative fidelity. However, their capacity for instruction-driven editing [ 41,27]\nremains under-explored. Notably, although there are a few existing methods [ 77,37] that have\nextended DiTs to instruction-driven editing, they are always restricted to a narrow set of common\nediting operations and lack evaluation on comprehensive editing tasks.\nTo address this limitation, we initiate a taxonomy study of image editing instructions to system-\natically assess the editing capabilities of current DiT-based conditional generation methods. Our\nempirical analysis reveals an interesting performance dichotomy: While current methods demonstrate\nproficiency in structurally-consistent edits where the layouts of the input and output images remain\naligned, they exhibit significant degradation when handling structurally-inconsistent operations that\nrequire layout modifications.\nTo overcome this issue, we introduce ImageEditing AsPrograms (IEAP), a unified framework atop\nthe DiT architecture which is capable of handling diverse types of editing operations efficiently and\nrobustly in this paper. Notably, we show that structurally-inconsistent instructions can in fact be\nreduced to a small set of simple operations, which are called as atomic operations in our paper. Thus,\ninstead of treating each edit as a monolithic, end -to-end task, IEAP levarages the Chain-of-Thought\n(CoT) reasoning [ 63] to break the original editing command into a sequence of atomic operations,\nwhich are namely Region of Interest (RoI) localization, RoI inpainting, RoI editing, RoI compositing\nand global transformation, and then executes them in a sequential manner via a neural program\ninterpreter [49].\nThe five atomic operations serve as the fundamental building blocks for complex editing tasks. As\nsuch, through the sequential combination of atomic operations, IEAP can robustly handle complex,\nmulti-step instructions that are typically confound in conventional end-to-end approaches.\nExtensive experiments show that our framework demonstrates state-of-the-art performance across\nstandard benchmarks, excelling in both structural preservation and alteration tasks through atomic-\nlevel operation decomposition compared to other approaches. Simultaneously, the CoT reasoning and\nprogramming pipeline of IEAP enable significantly more accurate and semantically more coherent\nedits under complex, multi-step instructions even compared to the leading proprietary models.\nOur main contributions can be summarized as follows:\n•We present a comprehensive taxonomy and empirical analysis of instruction-driven editing in\nDiT-based conditional generation, revealing a performance dichotomy between structurally-\nconsistent and -inconsistent edits.\n•We introduce ImageEditing AsPrograms (IEAP), a unified framework on the DiT back-\nbone that leverages CoT reasoning to parse free-form instructions into sequential atomic\noperations and then executes them sequentially by a neural program interpreter, thereby\nenabling robust handling of layout-altering and complex edits.\n•Extensive experiments demonstrate that IEAP achieves state-of-the-art performance in both\nstructure-preserving and -altering scenarios, delivering notably higher accuracy and semantic\nfidelity especially on complex, multi-step instructions compared to existing methods.\n2\n--- Page 3 ---\n2 Related Work\nConditional image generation. Early conditional image generation approaches like ControlNet [ 74]\ntypically adopt plug-in control adapters to incorporate single condition [ 3,16,35] like segmentation\nmask or diverse conditional inputs [ 79,48,26,40,67] to guide the generation of images. Recently, the\nfield of conditional image generation has witnessed remarkable breakthroughs through the integration\nof DiTs [ 13,46,31], with continuous innovations improving output quality and edit precision [ 45].\nSome methods [ 66,32,65,9] aim to create a unified DiT foundation for versatile conditional image\ngeneration and editing by integrating diverse inputs within a single framework. while approaches\nlike OminiControl [ 59] and so on [ 60,76,38,77,64] leverage LoRA-based fine-tuning [ 25] for\nlightweight and effective control.\nInstructional image editing. Instruction-based image editing [ 41,27] enables intuitive, language-\ndriven modifications of existing images. Early works like InstructPix2Pix [ 6] establishes paired\ninstruction–image datasets for supervised fine-tuning of diffusion models. For subsequent works,\nsome of them focus on architectural refinement [ 38,37,78,34,20], which introduce specialized\nconditioning units and multi-stage training to improve control granularity and consistency, others\nconcentrate on data-centric enhancements [ 73,17,55,8], that expand instruction coverage and\ndiversify edit examples. Moreover, some approaches [ 72,28,33,15] has unified LLM-based [ 1]\nlanguage reasoning with diffusion-based synthesis in a single framework, and some [ 69,75] leverage\nCoT [ 63] and in-context learning [ 21] to enhance the reasoning ability of models for more complex\nediting tasks. More recently, some works [ 14,77,37] have advanced image editing with DiTs. For\ninstance, ICEdit [ 77] leverages the in-context generation capabilities of large-scale DiTs to achieve\nflexible few-shot instruction editing, while Step1X-Edit [ 37] focuses on large-scale data construction\nand multi-modal integration to enable general-purpose image editing with performance approaching\nproprietary models.\n3 Motivation\n3.1 Preliminaries\nDiffusion Transformer Fundamentals. The image generation process of text-guided DiTs [ 46,13,\n31] is accomplished by successively denoising input tokens in multiple steps. At step t, the model\nprocesses:\nSt= [Xt,CT] (1)\nwhereXt∈RN×drepresents noisy image tokens and CT∈RM×ddenotes text tokens, they share\nthe embedding dimension d. Image tokens use Rotary Position Embedding (RoPE) [ 58] with spatial\ncoordinates (i, j), while text tokens fix positions at (0,0), enabling Multi-Modal Attention (MMA)\n[44] mechanisms to model cross-modal interactions.\nUnified Conditioning Framework. To integrate visual control signals, the prior work [59] extends\nthe baseline formulation by incorporating encoded condition images:\nSt= [Xt,CT,CI] (2)\nwhereCI∈RN×ddenotes latent tokens from condition images via the pretrained V AE encoder\n[30,52]. This unified sequence enables tri-modal fusion within transformer architectures, eliminating\nspatial misalignment inherent in feature concatenation baselines.\nMoreover, an auxiliary adaptive positional encoding mechanism further preserves spatial consistency\nacross these modalities by assigning coordinates to each token type with minimal overhead.\nGap in Instruction-Driven DiT Editing. Despite the rapid advances in DiT-based conditional image\ngeneration [ 59,76,38,64], research on instruction-driven editing [ 41,27] remains scarce. The few\nexisting methods [ 77,37] that do support instructional edits are typically confined to a small set of\nroutine operations, and lack a comprehensive evaluation across diverse editing scenarios, leaving\nDiT’s true editing potential unclear. This gap motivates us to conduct a taxonomy study of DiT’s\nability in instructional image editing, which is detailed in Sec. 3.2.\n3\n--- Page 4 ---\n[Add] “Add a person on the boat.”\n[Action change] “Change the action of the girl to jumping.”\n[Remove] “Remove the bananas.”\n [Replace] “Replace the dog with a backpack.”\n[Resize] “Enlarge the bird.”\n [Move] “Move the couch to the left.”\n(a) (b)Semantic ConsistencyInstruction FailthfulnessScores\nEditing TypesLocal Attribute Editing\nLocal Semantic Editing\nOverall Content EditingFigure 2: Results of our preliminary experiments. Figure (a) shows the GPT-4o scores for three\nediting types across instruction faithfulness and semantic consistency, ranging from 1 to 5. Figure (b)\nshows the representative failure cases from local semantic editing.\n3.2 Preliminary Experiments and Observations\nTo this end, we conduct a comprehensive evaluation of diffusion models for instruction-driven\nediting, uncovering an interesting performance dichotomy: While these methods excel at structurally-\nconsistent edits, they falter dramatically on structurally-inconsistent operations that demand explicit\nlayout modifications.\nTaxonomy and Experimental Setup. To enable systematic analysis [ 27,70,69], we first categorize\ninstruction-based image editing into three main types: local semantic editing, which modifies the\nidentity, position or size, e.g., add, remove, replace, action change, move and resize; local attribute\nediting, which adjusts certain properties of objects, e.g., color change, texture change, appearance\nchange, expression change, and background change; and overall content editing, which alters the\nwhole image consistently, e.g., tone transfer and style change.\nThen we use AnyEdit dataset [ 70] and OminiControl [ 59] to train models on the above editing types,\naccompanied by GPT-4o [29] to rate each edit on instruction faithfulness and semantic consistency.\nResults and Analysis. As shown in Fig. 2(a), both local attribute editing and overall content editing\nattain relatively high GPT-4o scores, whereas local semantic editing exhibits a notable performance\ndrop. As illustrated in Fig. 2(b), the cases of “add” and “action change” alter unrelated areas like the\nbackground, and the remaining four cases demonstrate a complete failure.\nWe attribute this discrepancy to the fact that, unlike local attribute and overall content edits, local se-\nmantic edits require explicit spatial-layout modifications. For instance, “add” and “delete” operations\nnecessitate instance-level scene recomposition, while “move” and “resize” further demand precise\ncoordinate system recalibration.\nKey Insight. Based on the above analysis, spatial-layout modification remains a critical challenge\nfor diffusion-based editing models; conversely, edits that preserve the original layout demonstrate\nsubstantially better performance. We speculate that, with limited training data, it is difficult for the\nmodel to learn the complex patterns underlying layout-changing tasks. Although DiT architectures\n[46,13,31] employ powerful full-attention mechanisms to capture long-range dependencies, they\nstill struggle with editing operations that require nontrivial scene reconfiguration.\nDue to the combinatorial complexity of spatial-layout modifications and the empirical limitations of\nDiT architectures, we propose to simplify the layout-editing paradigm through decomposition, which\nis detailed in Sec. 4.\n4 Methods\n4.1 Program with Atomic Operations\nThe insight in Sec. 3.2 motivates us to decouple semantic and spatial reasoning. Building on this foun-\ndation, we propose a programmatic reduction framework that systematically decomposes complex\nediting instructions into modular atomic operations. Specifically, we first formulate instruction-driven\nimage editing as an executable program via Chain-of-Thought (CoT) reasoning [ 63], and then use\na neural program interpreter [ 49] to transcode the reasoning graph into a dynamic execution plan,\nsequentially invoking relevant atomic modules.\n4\n--- Page 5 ---\nInstruction: “Make the cat have a \nfloral pattern, put a vase on the \nchair. Then, erase the pink \ndecoration on the wall and alter the \ncolor of the spoon to yellow. Replace \nthe biggest blue coffee cup with a \ncake and zoom in the pink cup next \nto the cat. Finally, change the time \nto the evening.”\nIEAP\nColor Change\nTone Change\n Style Transfer\nBackground \nChange\nAdd\n Remove\n Replace\nAction \nChange\nMove\n Resize\nColor Change\n Texture Change\nExpression \nChange\nAppearance \nChange\nRemove Replace Resize Tone Change Add Appearance Change\nRoI Localization\n RoI Inpainting RoI Editing RoI Compositioning Global Transformation\nLocal Semantic Editing\nLocal Attribute Editing\nOverall Content Editing\nFigure 3: Our pipeline. The original instruction is first parsed by a VLM into atomic operations,\nwhich are then sequentially executed via a neural program interpreter.\n4.2 General Pipeline\nWe abstract all editing instructions into five atomic primitives: (1) RoI Localization: Identify and iso-\nlate the relevant region in the image that the instruction refers to, serving as the spatial grounding step\nfor subsequent localized edits; (2) RoI Inpainting: Introduce new visual content or remove existing\nelements within the localized region, enabling semantic-level additions, substitutions, or deletions;\n(3) RoI Editing: Modify visual attributes within the region, such as color, texture, or appearance, to\nreflect fine-grained property changes specified by the instruction; (4) RoI Compositing: Reintegrate\nthe edited region into the full image while preserving spatial coherence and visual continuity; (5)\nGlobal Transformation: Adjust the overall content for coherent full-image modifications, such as\nchanging the illumination, weather, or style of the whole image.\nThe overall pipeline is shown as Fig. 3. We reduce any editing instruction into an arbitrary combina-\ntion of the five atomic operations described above, which can be formulated as:\nT≡KM\nk=1Ak,Ak∈ {A loc,Ainp,Aedit,Acomp,Aglobal} (3)\nwhere Tdenotes the free-form editing instruction,Lrepresents the sequential program combination,\nKis the number of atomic operations, Aloc,Ainp,Aedit,Acomp, andAglobal represent the five atomic\nprimitives respectively.\nRoI Localization. All problematic local semantic edits share a common first step: localizing a\nRegion of Interest (RoI) in the image for editing. Given an image Iand an editing instruction T, we\nfirst employ a Large Language Model (LLM) [1] to locate the text RoI:\nρ=MLLM(T), (4)\nwhere ρrepresents the text RoI extracted by the LLM MLLM. Subsequently, we achieve accurate\nlocalization of image RoI by:\nR=Mseg(I, ρ), (5)\nwhere Rdenotes the image RoI segmented by the segmentation model Mseg[71].\nFor add operation, the instruction may not specify a text RoI, or the specification may be ambiguous.\nIn such cases, we first derive the overall layout of all candidate objects using the capability of\nsegmentation models [ 50,71], and then prompt the LLM to determine the appropriate image RoI\nbased on T.\nRegarding move and resize, once the image RoI is obtained, we update the spatial layout of the image\nusing an LLM [ 1]. Specifically, we provide the LLM with a set of in-context examples that define our\nlayout representation and demonstrate representative editing patterns [ 36]. Given the current layout L\nand the instruction T, the LLM is prompted to produce a modified layout Ledit, as formulated below:\nTags =MLLM(I), L =Mseg(Tags), L edit=MLLM(L, T). (6)\n5\n--- Page 6 ---\nInput\nChange the action of the \nwoman to dancing.\n(a) Example Procedure of Action Change Operation.RoI Localication RoI Inpainting RoI Editing RoI Compositioning\nLayout ReconfigurationRoI Localication RoI Inpainting RoI Compositioning\nInput\nMove the apple with the cat \ntogether so that the cat can \nintersect the computer.Annular mask\n(b) Example Procedure of Move Operation.\nFigure 4: Example procedure. Figure (a) and Figure (b) illustrate the procedures of action change\nand movement respectively.\nWe then derive the geometric differences between LandLeditand convert them into the corresponding\naffine transformations, consisting of translation, scaling, and reshaping, and apply it to Rto update\nthe spatial configuration, yielding the transformed mask R′.\nRoI Inpainting. Once the image RoI has been localized, we apply inpainting to seamlessly fill and\ncomplete the region. For additive and substitutive operations, which aim to introduce new objects, we\nemploy a prompt-conditioned inpainting process to guide the generation of new content. Specifically,\nwe first extract the semantic entity Efrom the instruction Tvia an LLM [1]:\nE=MLLM(T), (7)\nand then construct a composite prompt Pin the form: “add Eon the black region” . For removal\noperations, which aim to eliminate existing content without introducing new semantics, we adopt a\nbackground-oriented infilling strategy, setting Pas“fill in the hole of the image” . The edited image\nIeditis then generated by:\nIedit=Minpaint (I⊙(1−R), P), (8)\nwhere Minpaint denotes the inpainting model trained by us.\nRoI Editing. When operations pertain to property change are performed, we use the trained attribute\nediting model Mattrto perform edits in this stage to obtain Iedit:\nIedit=Mattr(I, T). (9)\nRoI Compositing. To ensure seamless integration of the edited RoI with its surrounding context, we\nfirst construct an annular mask Mannby applying morphological dilation and erosion [ 51,54] to the\ntransformed RoI mask R′:\nMann= Dilate( R′, k1)\\Erode( R′, k2). (10)\nThen, we employ a fusion network Mfusion , trained on ring-masked object boundaries, to refine the\npre-composited image Iprepusing the generated annular mask. The final edited image is obtained as:\nIedit=Mfusion (Iprep⊙(1−Mann), P), (11)\nwhere Pis set as “inpaint the black-bordered region so that the object’s edges blend smoothly with\nthe background” to guide seamless boundary blending.\nGlobal Transformation. Like RoI editing, in the scenarios involving global transformation, we use\nthe trained global transformation model Mglobal to perform edits in this final stage to obtain Iedit.\n5 Experiments\n5.1 Experimental Settings\nTraining Settings. We train four specialized models for RoI inpainting, RoI editing, RoI compositing,\nand global transformation respectively. All models are fine-tuned on FLUX.1-dev [ 31] using LoRA\n6\n--- Page 7 ---\nInput InstructP2P MagicBrush UltraEdit ICEdit IEAP(Ours)\n[Add] Add a bird on the yellow stool.\n[Remove] Remove the bananas.\n[Replace] Replace the woman with a mirror.\n[Action Change] Change the action of the woman to running.\n[Color Change] Change the color of the boy’s hair to red.\n[Style Change] Change the style of the image to bubbles.Figure 5: Comparison results of ours with baseline methods on representative editing cases. Others\nexhibit poor performance even on some common editing operations, while our approach demonstrates\nsuperior effectiveness across all operations.\n[25], with default settings for rank 128 and alpha 128. Training is conducted with a batch size of 1\nand runs for 50,000 iterations each. We use the Prodigy optimizer [ 39], enabling safeguard warmup\nand bias correction, with a weight decay of 0.01. The experiments are conducted on single NVIDIA\nH100 GPU (80GB).\nDataset Setup. For both the RoI editing and global transformation models, we sample from the\nrelevant subsets of the AnyEdit [ 70] dataset and apply GPT-4o [ 29] to filter the data of some types\nthat have numerous noisy examples. To cover facial expression edits absent in AnyEdit, we integrate\nthe CelebHQ-FM dataset [11], which offers consistent identities and annotated expressions suitable\nfor our instruction schema. The RoI inpainting and RoI compositing models are trained on samples\nfrom the “add”, “remove” and “replace” splits of AnyEdit. For each sample, we first obtain the\nimage RoI according to the editing instruction. In the RoI Inpainting training setup, we set the pixels\nwithin image RoI to black as input to train. For RoI Compositing, we set k1andk2as 3 in default to\nblackout the annular mask region of image RoI as input for training.\n7\n--- Page 8 ---\nMethodMagicBrush test AnyEdit test\nCLIP im↑CLIP out↑ L1↓ DINO ↑ CLIP im↑ L1↓ DINO ↑GPT ↑\nInstructPix2Pix 0.838 0.229 0.112 0.758 0.801 0.110 0.765 3.83\nMagicBrush 0.886 0.241 0.074 0.859 0.824 0.128 0.742 3.90\nUltraEdit 0.911 0.227 0.061 0.883 0.833 0.114 0.772 3.93\nICEdit 0.913 0.236 0.058 0.885 0.847 0.110 0.765 4.13\nOurs 0.922 0.247 0.060 0.897 0.882 0.096 0.825 4.41\nTable 1: Quantitative results on MagicBrush and AnyEdit test set.\nMethodLocal Semantic Editing Local Attribute Editing Overall Content Editing\nCLIP im↑L1↓DINO ↑GPT ↑CLIP im↑L1↓DINO ↑GPT ↑CLIP im↑L1↓DINO ↑GPT ↑\nInstructP2P 0.826 0.132 0.738 3.74 0.790 0.135 0.737 3.92 0.766 0.156 0.642 3.91\nMagicBrush 0.860 0.106 0.796 3.90 0.809 0.117 0.762 4.21 0.763 0.187 0.616 3.99\nUltraEdit 0.867 0.095 0.812 3.86 0.801 0.092 0.793 3.94 0.754 0.201 0.611 4.41\nICEdit 0.881 0.088 0.810 4.08 0.825 0.095 0.795 4.06 0.759 0.188 0.603 4.45\nOurs 0.907 0.081 0.854 4.42 0.861 0.083 0.821 4.54 0.895 0.107 0.879 4.51\nTable 2: Quantitative results on different types of editing operations.\nEvaluation Settings. We evaluate our method on two benchmarks: MagicBrush test set [ 73], a\nwidely used dataset spanning diverse editing types, and AnyEdit test set [ 70], from which we select 16\ninstruction-based editing categories. For MagicBrush, we follow previous works [ 73,78,15,55] and\nreport CLIPimg, CLIPout [ 22],L1, and DINO [ 7,43] scores to measure the similarity between the\ngenerated results and ground-truth images. While for AnyEdit, where some categories lack reference\ncaptions required for calculating CLIPout, we instead leverage GPT-4o [ 29] to assign ratings on a\nscale from 1 to 5 across three aspects: instruction faithfulness, semantic consistency, and aesthetic\nquality. The final quality score is computed as the average of these three dimensions.\nWe first compare our method with existing state-of-the-art open-source baselines, including In-\nstructPix2Pix [ 6], MagicBrush [ 73], UltraEdit [ 78], and ICEdit [ 77]. In addition, to demonstrate\nthe competitiveness of our approach against powerful proprietary multimodal foundation models\nin complex image editing scenarios, we further make comparisons with SeedEdit (Doubao) [ 56],\nGemini 2.0 Flash [19], and GPT-4o [29].\n5.2 Comparisons with State of the Art.\nQualitative Comparisons. Fig. 5 shows the results of ours against other four methods [ 6,73,78,77]\non some representative editing cases, where our method demonstrates comprehensive superiority\nover others in accurate instruction execution, structural consistency, and instance-level fidelity.\nQuantitative Comparisons. Table 1 exhibits the quantitative comparison results of our method and\nother approaches [ 6,73,78,77] on MagicBrush test set [ 73] and AnyEdit test set [ 70]. The results\nshow that our method demonstrates state-of-the-art performance on both datasets. On MagicBrush,\nour method achieves the best performance in terms of caption alignment, semantic consistency, and\npreservation of fine-grained structural details. Although it incurs a marginal increase in pixel-level\ndeviation compared to the best [ 77], this is far outweighed by the substantial gains in perceptual\nquality and semantic fidelity. Furthermore, on AnyEdit, our approach yields significant and compre-\nhensive improvements across all evaluation metrics, further highlighting its superiority over existing\ntechniques.\nTo provide a more fine-grained analysis of editing performance, we group a subset of the instruction-\nbased categories from the AnyEdit test set [ 70] into three macro-tasks: local semantic editing, local\nattribute editing and overall semantic editing. For local attribute editing, we augment with some\nCelebHQ-FM [ 11] test images to evaluate facial expression changes. The quantitave comparison\nresults are shown in Tab. 2, where our method consistently outperforms other candidates across all\nthree task categories and evaluation metrics.\nComparisons with Cutting-Edge Multimodal Models. To demonstrate the superiority of our\nreduction strategy on complex editing tasks, we also conduct comparative experiments against\nprominent closed-source multimodal models [ 56,19,29]. As illustrated in Fig. 6, our method rivals,\nand in most cases surpasses the performance of these leading models on intricate scenarios requiring\n8\n--- Page 9 ---\nMove the biggest orange butterfly to the center of the image and change the time to evening.\nRemove the necklace, add a pair of sunglasses on the dog and put on a clothes with text ‘IEAP’. \nChange the material of the car to corduroy and minify the car. Put the car on the street scene.\nSeedEdit(Doubao)Gemini  2.0 Flash IEAP(Ours) GPT-4o\n Input SeedEdit(Doubao)Gemini  2.0 Flash IEAP(Ours) GPT-4o Input\nAdd a cake on the table, make the table to be pink and be in a floral pattern, remove the left chair.\nChange the action of the cat to jumping and replace the lamp and cupboard with a plant.\nRemove the monkeys and change the style to painting.Figure 6: Comparisons on Complex Instructions with Leading Multimodal Models. Our method\nachieves comparable or even better edit completeness and pre-post consistency.\nmultiple sequential edits. Unlike competing approaches, which frequently omit specified instructions\nor introduce extraneous alterations unrelated to the editing directives, our framework faithfully\nexecutes each instruction while maintaining superior image consistency and instance preservation.\n5.3 Ablation Studies\nSettings CLIP im↑CLIP out↑L1↓DINO ↑GPT ↑\nw/o CoT & Reduction 0.873 0.241 0.117 0.795 4.10\nw/o RoI Inpainting 0.861 0.218 0.124 0.775 3.65\nw/o RoI Editing 0.900 0.244 0.088 0.843 4.23\nw/o Layout Reconfiguration 0.900 0.245 0.088 0.848 4.31\nw/o Annular Mask Integration 0.906 0.252 0.083 0.854 4.39\nFull 0.907 0.252 0.081 0.854 4.42\nTable 3: Ablation results on AnyEdit local semantic editing\ntest set.\nInstruction: \nChange the action of the \ndog to jumping.w/o CoT & Reduction Input w/o RoI Inpainting\nFull w/o RoI Editing w/o Annular MaskBackground change Strange filling\nInconsistent instance Unnatural edgesFigure 7: Qualitative ablation of\naction change operation.\nModule-wise Ablation Studies. To quantify the impact of each key component in our framework,\nwe perform a series of ablation studies on the AnyEdit local semantic editing test set as we split in\nSec. 5.2. As shown in Tab. 3, we first substitute our CoT reasoning and reduction pipeline with\nend-to-end editing pipeline, resulting in a marked performance deterioration across all metrics. Next,\nwe replace our specialized RoI inpainting and RoI editing models respectively with the generic\ninpainting model from [ 59], which induces performance declines of varying degrees. We then remove\nthe LLM-guided layout reconfiguration and instead employing random layout modifications for\nrelevant operations, which incurs a noticeable performance decline. Finally, omitting the annular\nmask integration produces a modest drop, underscoring its role in precise boundary delineation. Fig.\n7 exhibits the ablation results on an example of “action change”, visually showcasing each module’s\nnecessity. Collectively, these ablation results confirm that each component in our pipeline contributes\nsignificantly in handling robust local semantic editing tasks requiring layout changes.\n6 Conclusions, Limitations and Future Work\nIn this paper, we propose Image Editing As Programs (IEAP), a unified DiT-based framework for\ninstruction-driven image editing. By defining five atomic operations and using CoT reasoning to\nconvert instructions into sequential programs, IEAP processes the ability to handle both simple and\ncomplex edits. Experiments demonstrate that IEAP outperforms state-of-the-art methods in both\nstructure-preserving and structure-altering tasks, especially for complex edits.\nDespite its strong overall performance, there are also some limitations. First, for complex shadow\nchanges, our method sometimes leaves shadows inconsistent after compositing operations. Second,\nmultiple editing iterations may induce progressive image quality decay. Future work could focus on\naddressing these issues via physics-aware shadow modeling and diffusion-based quality restoration.\n9\n--- Page 10 ---\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, et al. Gpt-4 technical report, 2024.\n[2]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics\n(TOG) , 42(4):1–11, 2023.\n[3]Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski,\nOhad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18370–\n18380, 2023.\n[4]Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural\nimages. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n18208–18218, 2022.\n[5]Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized\ncorrespondence algorithm for structural image editing. ACM Trans. Graph. , 28(3):24, 2009.\n[6]Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\npages 18392–18402, 2023.\n[7]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 9650–9660, 2021.\n[8]Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, and Smaranda Muresan. Learning to follow object-\ncentric image editing instructions faithfully. arXiv preprint arXiv:2310.19145 , 2023.\n[9]Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang,\nNanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world\ndynamics. arXiv preprint arXiv:2412.07774 , 2024.\n[10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based\nsemantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 , 2022.\n[11] Brian DeCann and Kirill Trapeznikov. Comprehensive dataset of face manipulations for development and\nevaluation of forensic tools, 2022.\n[12] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for\ncontrollable image generation. Advances in Neural Information Processing Systems , 36:16222–16239,\n2023.\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution\nimage synthesis. In Forty-first international conference on machine learning , 2024.\n[14] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang.\nDit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial\nIntelligence , volume 39, pages 2969–2977, 2025.\n[15] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-\nbased image editing via multimodal large language models. arXiv preprint arXiv:2309.17102 , 2023.\n[16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision ,\npages 89–106. Springer, 2022.\n[17] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng\nZhang, Houqiang Li, Han Hu, et al. Instructdiffusion: A generalist modeling interface for vision tasks. In\nProceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pages 12709–12720,\n2024.\n[18] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu Sebe, Trevor Darrell, Zhangyang\nWang, and Humphrey Shi. Pair diffusion: A comprehensive multimodal object-level image editor. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8609–8618,\n2024.\n10\n--- Page 11 ---\n[19] Google. Experiment with gemini 2.0 flash native image generation. Technical report, Google AI Studio,\n2025.\n[20] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing\nby attention modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 6986–6996, 2024.\n[21] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without\ntraining, 2022.\n[22] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. arXiv preprint arXiv:2104.08718 , 2021.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\n[24] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research ,\n23(47):1–33, 2022.\n[25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n[26] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and\ncontrollable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778 , 2023.\n[27] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang,\nLiangliang Cao, and Shifeng Chen. Diffusion model-based image editing: A survey. IEEE Transactions\non Pattern Analysis and Machine Intelligence , page 1–27, 2025.\n[28] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao\nDong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing\nwith multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 8362–8371, 2024.\n[29] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, et al. Gpt-4o system card, 2024.\n[30] Diederik P Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013.\n[31] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n[32] Duong H Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay\nKrishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318 , 2024.\n[33] Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal\ninstruction following. arXiv preprint arXiv:2312.06738 , 2023.\n[34] Sijia Li, Chen Chen, and Haonan Lu. Moecontroller: Instruction-based arbitrary image manipulation with\nmixture-of-expert controllers. arXiv preprint arXiv:2309.04372 , 2023.\n[35] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 22511–22521, 2023.\n[36] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt\nunderstanding of text-to-image diffusion models with large language models, 2024.\n[37] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang,\nHonghao Fu, Chunrui Han, et al. Step1x-edit: A practical framework for general image editing. arXiv\npreprint arXiv:2504.17761 , 2025.\n[38] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++:\nInstruction-based image creation and editing via context-aware content filling, 2025.\n[39] Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner.\narXiv preprint arXiv:2306.06101 , 2023.\n[40] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In\nProceedings of the AAAI conference on artificial intelligence , volume 38, pages 4296–4304, 2024.\n11\n--- Page 12 ---\n[41] Thanh Tam Nguyen, Zhao Ren, Trinh Pham, Thanh Trung Huynh, Phi Le Nguyen, Hongzhi Yin, and Quoc\nViet Hung Nguyen. Instruction-guided editing controls for images and multimedia: A survey in llm era.\narXiv preprint arXiv:2411.09955 , 2024.\n[42] Byong Mok Oh, Max Chen, Julie Dorsey, and Frédo Durand. Image-based modeling and photo editing.\nInProceedings of the 28th annual conference on Computer graphics and interactive techniques , pages\n433–442, 2001.\n[43] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual\nfeatures without supervision. arXiv preprint arXiv:2304.07193 , 2023.\n[44] Zexu Pan, Zhaojie Luo, Jichen Yang, and Haizhou Li. Multi-modal attention for speech emotion recognition.\narXiv preprint arXiv:2009.04107 , 2020.\n[45] Rishubh Parihar, VS Sachidanand, Sabariswaran Mani, Tejan Karmali, and R Venkatesh Babu. Precisecon-\ntrol: Enhancing text-to-image diffusion models with fine-grained attribute control. In European Conference\non Computer Vision , pages 469–487. Springer, 2024.\n[46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision , pages 4195–4205, 2023.\n[47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952 , 2023.\n[48] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,\nCaiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for controllable visual\ngeneration in the wild. arXiv preprint arXiv:2305.11147 , 2023.\n[49] Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279 ,\n2015.\n[50] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang\nChen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang.\nGrounded sam: Assembling open-world models for diverse visual tasks, 2024.\n[51] Jean-Francois Rivest, Pierre Soille, and Serge Beucher. Morphological gradients. Journal of Electronic\nImaging , 2(4):326–336, 1993.\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 10684–10695, 2022.\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models, 2022.\n[54] Khairul Anuar Mat Said and Asral Bahari Jambek. Analysis of image processing using morphological\nerosion and dilation. In Journal of Physics: Conference Series , volume 2071, page 012033. IOP Publishing,\n2021.\n[55] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and\nYaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8871–8879, 2024.\n[56] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv\npreprint arXiv:2411.06686 , 2024.\n[57] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan,\nand Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8839–8849,\n2024.\n[58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing , 568:127063, 2024.\n[59] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal\nand universal control for diffusion transformer. arXiv preprint arXiv:2411.15098 , 2024.\n12\n--- Page 13 ---\n[60] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient\nconditioning for diffusion transformers. arXiv preprint arXiv:2503.08280 , 2025.\n[61] Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, and Chris Xiaoxuan\nLu. Click to grasp: Zero-shot precise manipulation via visual diffusion descriptors. In 2024 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) , pages 11610–11617. IEEE, 2024.\n[62] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-\nresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , June 2018.\n[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems , 35:24824–24837, 2022.\n[64] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more gener-\nalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160 ,\n2025.\n[65] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia.\nDreamomni: Unified image generation and editing. arXiv preprint arXiv:2412.17098 , 2024.\n[66] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shut-\ning Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint\narXiv:2409.11340 , 2024.\n[67] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free\ndiffusion: Taking\" text\" out of text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 8682–8692, 2024.\n[68] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint: A unified framework for multimodal image\ninpainting with pretrained diffusion model. In Proceedings of the 31st ACM International Conference on\nMultimedia , pages 3190–3199, 2023.\n[69] Siwei Yang, Mude Hui, Bingchen Zhao, Yuyin Zhou, Nataniel Ruiz, and Cihang Xie. Complex-Edit :\nCot-like instruction generation for complexity-controllable image editing benchmark, 2025.\n[70] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang,\nHanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea.\narXiv preprint arXiv:2411.15738 , 2024.\n[71] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi\nFeng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of\nimages and videos, 2025.\n[72] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, and Yu Zhang. Nexus-gen: A\nunified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356 , 2025.\n[73] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for\ninstruction-guided image editing. Advances in Neural Information Processing Systems , 36:31428–31449,\n2023.\n[74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF international conference on computer vision , pages 3836–3847,\n2023.\n[75] Xinyu Zhang, Mengxue Kang, Fei Wei, Shuang Xu, Yuhe Liu, and Lin Ma. Tie: Revolutionizing text-based\nimage editing for complex-prompt following and high-fidelity editing, 2024.\n[76] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient\nand flexible control for diffusion transformer, 2025.\n[77] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image\nediting with in-context generation in large scale diffusion transformer, 2025.\n[78] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia\nZhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale.\nAdvances in Neural Information Processing Systems , 37:3058–3093, 2024.\n[79] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-\nYee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural\nInformation Processing Systems , 36:11127–11150, 2023.\n13\n--- Page 14 ---\nTechnical Appendices and Supplementary Material\nIn this part, we provide additional algorithm illustration, implementation details, more comparison\nresults, more visualization results, and more analysis and discussions of the proposed approach.\nA Algorithm Illustration\nTo better elaborate the details of the proposed IEAP, we provide an algorithmic illustration for the\nwhole pipeline in Alg. 1.\nAlgorithm 1 IEAP: Image Editing As Programs\nInput:\n•I: input image path\n•T: original instruction\n•{RoI_Localization ,RoI_Inpainting , . . . , Global_Transformation }: editing primitives\n•cot_with_gpt (·): CoT prompt to GPT–4o\n•extract_instructions (·): parse CoT output\n•infer_with_DiT (op,·): invoke DiT for primitive op\n•roi_localization (I, instr ): returns mask for region of interest\n•fusion (I1, I2): blends two intermediate outputs\n•layout_change (I, instr ): compute geometric transform\nOutput: final edited image I∗\n1:uri←encode_image_to_datauri (I)\n2:(C,T)←cot_with_gpt (uri, T ) ▷Categories and instructions\n3:I(0)←I\n4:fori= 1to|C|do\n5: cat← C[i],instr← T [i]\n6: ifcat∈ {Add,Remove ,Replace }then\n7: M←roi_localization (I(i−1), instr )\n8: I′←infer_with_DiT (RoI Inpainting , M, instr )\n9: I(i)←I′\n10: else if cat=Action Change then\n11: M←roi_localization (I(i−1), instr )\n12: Ibg←infer_with_DiT (RoI Inpainting , M, instr )\n13: Iact←infer_with_DiT (RoI Editing , I(i−1), instr )\n14: I(i)←infer_with_DiT (RoI Compositing ,fusion (Ibg, Iact), instr )\n15: else if cat∈ {Move,Resize }then\n16: M←roi_localization (I(i−1), instr )\n17: Ibg←infer_with_DiT (RoI Inpainting , M, instr )\n18: Ilc←layout_change (I(i−1), instr )\n19: I(i)←infer_with_DiT (RoI Compositing ,fusion (Ibg, Ilc), instr )\n20: else if cat∈ {Appearance Change ,Background Change ,\n21: Color Change ,Material Change ,Expression Change }then\n22: I(i)←infer_with_DiT (RoI Editing , I(i−1), instr )\n23: else if cat∈ {Tone Transfer ,Style Change }then\n24: I(i)←infer_with_DiT (Global Transformation , I(i−1), instr )\n25: else\n26: raise ValueError(“Invalid category: ” cat”)\n27: end if\n28:end for\n29:return I(|C|)\n14\n--- Page 15 ---\nB Implementation Details\nIn this section, we present the prompts employed to leverage a VLM for CoT reasoning over complex\ninstructions, providing further details on the layout-adjustment prompts.\nBelow are the detailed prompts used to invoke the VLM for the CoT process on complex instructions:\nNow you are an expert in image editing. Based on the given single image, what atomic image editing\ninstructions should be if the user wants to {instruction}? Let’s think step by step.\nAtomic instructions include 13 categories as follows:\n- Add: Introduce a new object, person, or element into the image, e.g.: add a car on the road\n- Remove: Eliminate an existing object or element from the image, e.g.: remove the sofa in the image\n- Color Change: Modify the color of a specific object, e.g.: change the color of the shoes to blue\n- Material Change: Alter the surface material or texture of an object, e.g.: change the material of the\nsign like stone\n- Action Change: Modify the pose or action of an instance, e.g.: change the action of the boy to\nraising hands\n- Expression Change: Adjust the facial expression, e.g.: change the expression to smiling\n- Replace: Substitute one object in the image with a different object, e.g.: replace the coffee with an\napple\n- Background Change: Change the background scene to another, e.g.: change the background into\nforest\n- Appearance Change: Modify visual attributes such as patterns or accessories, e.g.: make the cup\nhave a floral pattern\n- Move: Change the spatial position of an object within the image, e.g.: move the plane to the left\n- Resize: Adjust the scale or size of an object, e.g.: enlarge the clock\n- Tone Transfer: Change the global atmosphere or lighting conditions, e.g.: change the weather to\nfoggy, change the time to spring\n- Style Change: Modify the entire image to adopt a different visual style, e.g.: make the style of the\nimage to cartoon\nRespond *only* with a numbered list. Each line must begin with the category in square brackets,\nthen the instruction. Please strictly follow the atomic categories. The operation (what) and the target\n(to what) are crystal clear. Do not split replace to add and remove. Always place [Tone Transfer] and\n[Style Change] instructions at the end of the list.\nFor example:\n1. [Add] add a car on the road\n2. [Color Change] change the color of the shoes to blue\n3. [Move] move the lamp to the left\nDo not include any extra text, explanations, JSON or markdown, just the list.\nBelow are the detailed prompts used to adjust the layout of move and resize operations:\nYou are an intelligent bounding box editor. I will provide you with the current bounding boxes and\nthe editing instruction. Your task is to generate the new bounding boxes after editing. Let’s think step\nby step.\nThe images are of size 512x512. The top-left corner has coordinate [0, 0]. The bottom-right\ncorner has coordinnate [512, 512]. The bounding boxes should not overlap or go beyond the image\nboundaries. Each bounding box should be in the format of (object name, [top-left x coordinate,\ntop-left y coordinate, bottom-right x coordinate, bottom-right y coordinate]).\nDo not add new objects or delete any object provided in the bounding boxes. Do not change the size\nor the shape of any object unless the instruction requires so.\nPlease consider the semantic information of the layout. When resizing, keep the bottom-left corner\nfixed by default. When swaping locations, change according to the center point.\nIf needed, you can make reasonable guesses. Please refer to the examples below:\nInput bounding boxes: [(\"bed\", [50, 300, 450, 450]), (\"pillow\", [200, 200, 300, 230])]\nEditing instruction: Move the pillow to the left side of the bed.\nOutput bounding boxes: [(\"bed\", [50, 300, 450, 450]), (\"pillow\", [70, 270, 170, 300])]\n15\n--- Page 16 ---\nEditing instruction: Input bounding boxes: [(’a car’, [21, 281, 232, 440])]\nEditing instruction: Move the car to the right.\nOutput bounding boxes: [(’a car’, [121, 281, 332, 440])]\nInput bounding boxes: [(\"dog\", [150, 250, 250, 300])]\nEditing instruction: Enlarge the dog.\nOutput bounding boxes: [(\"dog\", [150, 225, 300, 300])]\nInput bounding boxes: [(\"chair\", [100, 350, 200, 450]), (\"lamp\", [300, 200, 360, 300])]\nEditing instruction: Swap the location of the chair and the lamp.\nOutput bounding boxes: [(\"chair\", [280, 200, 380, 300]), (\"lamp\", [120, 350, 180, 450])]\nNow, the current bounding boxes is {bbox}, the instruction is {instruction}.\nBelow are the detailed prompts used to adjust the layout of add operations:\nYou are an intelligent bounding box editor. I will provide you with the current bounding boxes and an\nadd editing instruction. Your task is to determine the new bounding box of the added object. Let’s\nthink step by step.\nThe images are of size 512x512. The top-left corner has coordinate [0, 0]. The bottom-right corner\nhas coordinnate [512, 512].\nThe bounding boxes should not go beyond the image boundaries. The new box must be at least as\nlarge as needed to encompass the object. Each bounding box should be in the format of (object name,\n[top-left x coordinate, top-left y coordinate, bottom-right x coordinate, bottom-right y coordinate]).\nDo not delete any object provided in the bounding boxes. Please consider the semantic information\nof the layout, preserve semantic relations.\nIf needed, you can make reasonable guesses. Please refer to the examples below:\nInput bounding boxes: [(’a green car’, [21, 281, 232, 440])]\nEditing instruction: Add a bird on the green car.\nOutput bounding boxes: [(’a bird’, [80, 150, 180, 281])]\nInput bounding boxes: [(’stool’, [300, 350, 380, 450])]\nEditing instruction: Add a cat to the left of the stool.\nOutput bounding boxes: [(’a cat’, [180, 250, 300, 450])]\nHere are some examples to illustrate appropriate overlapping for better visual effects:\nInput bounding boxes: [(’the white cat’, [200, 300, 320, 420])]\nEditing instruction: Add a hat on the white cat.\nOutput bounding boxes: [(’a hat’, [200, 150, 320, 330])]\nNow, the current bounding boxes is {bbox}, the instruction is {instruction}.\nC More Quantitative Results\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.847 0.264 0.092 0.829 4.50 4.40 4.26 4.39\nMagicBrush 0.889 0.277 0.068 0.892 4.66 4.76 4.62 4.68\nUltraEdit 0.897 0.274 0.056 0.909 3.36 4.24 4.22 3.94\nICEdit 0.925 0.277 0.057 0.915 4.60 4.80 4.76 4.72\nIEAP(Ours) 0.928 0.278 0.056 0.917 4.68 4.84 4.60 4.71\nTable 4: Quantitative comparison results on AnyEdit Add test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.800 0.202 0.108 0.721 2.74 3.42 3.20 3.12\nMagicBrush 0.853 0.211 0.083 0.800 3.08 3.60 3.18 3.29\nUltraEdit 0.846 0.211 0.066 0.802 2.50 3.54 3.44 3.16\nICEdit 0.895 0.212 0.054 0.875 4.06 4.48 4.32 4.29\nIEAP(Ours) 0.916 0.230 0.057 0.886 4.18 3.88 3.66 3.91\nTable 5: Quantitative comparison results on AnyEdit Remove test set.\n16\n--- Page 17 ---\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.766 0.234 0.179 0.588 3.72 3.68 3.80 3.73\nMagicBrush 0.806 0.248 0.148 0.671 4.52 4.48 4.38 4.46\nUltraEdit 0.779 0.242 0.142 0.621 3.80 4.40 4.40 4.20\nICEdit 0.797 0.228 0.128 0.614 3.68 4.02 4.04 3.91\nIEAP(Ours) 0.866 0.252 0.099 0.701 4.68 4.68 4.48 4.61\nTable 6: Quantitative comparison results on AnyEdit Replace test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.829 0.254 0.164 0.774 3.46 3.84 3.58 3.63\nMagicBrush 0.831 0.266 0.156 0.784 2.96 4.28 4.28 3.84\nUltraEdit 0.847 0.259 0.157 0.781 2.92 4.22 4.24 3.79\nICEdit 0.827 0.255 0.152 0.745 2.68 4.04 4.04 3.59\nIEAP(Ours) 0.848 0.267 0.154 0.798 4.66 4.86 4.68 4.73\nTable 7: Quantitative comparison results on AnyEdit Action Change test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.881 0.219 0.127 0.771 3.82 4.44 4.36 4.21\nMagicBrush 0.902 0.219 0.088 0.828 2.94 3.94 3.90 3.59\nUltraEdit 0.923 0.211 0.074 0.867 3.48 4.40 4.40 4.09\nICEdit 0.944 0.213 0.063 0.868 3.28 4.64 4.30 4.07\nIEAP(Ours) 0.963 0.223 0.058 0.903 3.88 4.44 4.38 4.23\nTable 8: Quantitative comparison results on AnyEdit Relation test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.831 0.241 0.124 0.746 2.94 3.56 3.62 3.37\nMagicBrush 0.875 0.258 0.094 0.802 2.80 3.88 4.00 3.56\nUltraEdit 0.908 0.262 0.073 0.889 3.22 4.38 4.38 4.00\nICEdit 0.895 0.253 0.074 0.841 3.14 4.28 4.26 3.89\nIEAP(Ours) 0.923 0.263 0.066 0.921 4.38 4.32 4.28 4.32\nTable 9: Quantitative comparison results on AnyEdit Resize test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.815 0.280 0.139 0.744 3.60 4.08 3.92 3.87\nMagicBrush 0.852 0.294 0.094 0.815 3.96 4.32 3.98 4.09\nUltraEdit 0.857 0.277 0.068 0.845 4.04 4.62 4.42 4.36\nICEdit 0.847 0.273 0.085 0.808 4.04 4.42 4.16 4.21\nIEAP(Ours) 0.886 0.285 0.082 0.833 4.06 4.72 4.80 4.53\nTable 10: Quantitative comparison results on AnyEdit Appearance test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.725 0.224 0.216 0.582 3.40 3.60 3.44 3.48\nMagicBrush 0.746 0.230 0.228 0.567 4.58 4.38 4.46 4.47\nUltraEdit 0.796 0.257 0.169 0.747 3.48 4.36 3.14 3.66\nICEdit 0.799 0.241 0.166 0.757 3.04 4.16 3.88 3.69\nIEAP(Ours) 0.801 0.243 0.165 0.759 4.74 4.68 4.70 4.71\nTable 11: Quantitative comparison results on AnyEdit Background Change test set.\n17\n--- Page 18 ---\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.886 0.279 0.120 0.876 3.60 4.40 4.00 4.00\nMagicBrush 0.898 0.282 0.087 0.869 4.20 4.82 4.62 4.55\nUltraEdit 0.890 0.280 0.065 0.87 3.80 4.40 4.20 4.13\nICEdit 0.896 0.278 0.073 0.849 4.72 4.80 4.64 4.72\nIEAP(Ours) 0.911 0.276 0.059 0.876 4.62 4.72 4.78 4.71\nTable 12: Quantitative comparison results on AnyEdit Color Change test set.\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.776 0.068 0.936 3.74 4.60 4.30 4.21\nMagicBrush 0.770 0.064 0.940 3.86 4.48 4.18 4.17\nUltraEdit 0.699 0.073 0.907 3.14 4.10 3.80 3.68\nICEdit 0.796 0.065 0.943 3.16 4.60 4.30 4.02\nIEAP(Ours) 0.882 0.052 0.945 4.34 4.72 4.50 4.52\nTable 13: Quantitative comparison results on Expression test set.\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.746 0.130 0.549 4.00 4.18 4.04 4.07\nMagicBrush 0.778 0.110 0.621 3.36 4.06 3.84 3.75\nUltraEdit 0.765 0.086 0.598 3.34 4.28 4.04 3.89\nICEdit 0.787 0.086 0.616 3.48 3.92 3.58 3.66\nIEAP(Ours) 0.826 0.055 0.696 4.08 4.48 4.18 4.25\nTable 14: Quantitative comparison results on Material Change test set.\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.710 0.212 0.463 3.56 4.32 3.94 3.94\nMagicBrush 0.692 0.214 0.440 3.12 4.64 4.00 3.92\nUltraEdit 0.703 0.201 0.467 4.02 4.8 4.62 4.48\nICEdit 0.706 0.219 0.458 4.04 4.82 4.36 4.41\nIEAP(Ours) 0.922 0.097 0.915 4.44 4.64 4.44 4.51\nTable 15: Quantitative comparison results on AnyEdit Style Change test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.822 0.260 0.100 0.821 3.72 4.48 3.92 4.04\nMagicBrush 0.834 0.266 0.159 0.791 3.56 4.64 3.98 4.06\nUltraEdit 0.804 0.268 0.201 0.767 4.12 4.62 4.26 4.33\nICEdit 0.812 0.260 0.157 0.748 4.06 4.88 4.56 4.50\nIEAP(Ours) 0.868 0.268 0.116 0.843 4.44 4.64 4.44 4.51\nTable 16: Quantitative comparison results on AnyEdit Tone Transfer test set.\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.815 0.134 0.647 3.40 4.04 4.80 4.08\nMagicBrush 0.835 0.081 0.697 1.82 3.56 3.50 2.96\nUltraEdit 0.833 0.066 0.756 2.58 4.02 4.02 3.54\nICEdit 0.906 0.042 0.842 2.98 4.40 3.40 3.59\nIEAP(Ours) 0.908 0.056 0.794 3.42 4.48 4.46 4.12\nTable 17: Quantitative comparison results on AnyEdit Counting test set.\n18\n--- Page 19 ---\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.773 0.208 0.581 3.46 4.18 4.08 3.91\nMagicBrush 0.806 0.174 0.631 2.98 3.88 4.04 3.63\nUltraEdit 0.825 0.167 0.669 2.82 4.38 4.38 3.86\nICEdit 0.806 0.171 0.629 3.56 4.16 4.06 3.93\nIEAP(Ours) 0.833 0.169 0.662 3.88 4.44 4.52 4.28\nTable 18: Quantitative comparison results on AnyEdit Implicit Change test set.\nMethod CLIP im↑ L1↓ DINO ↑ GPTIF↑ GPTFC↑ GPTAQ↑ GPTavg↑\nInstructPix2Pix 0.887 0.111 0.858 4.30 4.50 4.30 4.37\nMagicBrush 0.900 0.100 0.874 4.12 4.36 4.54 4.34\nUltraEdit 0.922 0.077 0.911 3.24 4.4 4.36 4.00\nICEdit 0.898 0.079 0.864 4.16 4.46 4.20 4.27\nIEAP(Ours) 0.938 0.084 0.925 4.18 4.56 4.38 4.37\nTable 19: Quantitative comparison results on AnyEdit Move test set.\nMethod CLIP im↑CLIP out↑L1↓DINO ↑GPTIF↑GPTFC↑GPTAQ↑GPTavg↑\nInstructPix2Pix 0.688 0.243 0.189 0.742 1.04 4.38 3.92 3.11\nMagicBrush 0.680 0.255 0.156 0.786 1.02 4.48 4.10 3.20\nUltraEdit 0.732 0.279 0.147 0.843 1.96 4.46 3.98 3.47\nICEdit 0.810 0.289 0.155 0.811 4.18 4.42 4.68 4.43\nIEAP(Ours) 0.788 0.285 0.162 0.786 3.96 4.58 4.06 4.20\nTable 20: Quantitative comparison results on AnyEdit Textual Change test set.\nD More Visualization Results\nIn this section, we provide more visualization results, as shown below:\nInput\nAdd a cat on the grass. Add a dog on the grass.\nEdited Results\nAdd\nAdd a laptop on the grass. Add a fish on the grass.\nRemove the hat. Remove the necklace. Remove the cat. Remove the clothes.\nReplace the dog with a backpack. Replace the dog with a cat. Replace the dog with a cake. Replace the dog with flowers.Remove\nReplace\nFigure 8: More Visualization Results.\n19\n--- Page 20 ---\nInput\nChange the action of the woman \nto dancing. Change the action of the woman \nto squatting. Edited Results\nAction Change\nChange the action of the woman \nto standing.Change the action of the woman \nto running.\nMove the bird to the left. Move the bird to the right. Move the bird upward. Move the bird downward.\nZoom in the tiger. Minify the tiger. Enlarge the tiger to twice its \noriginal sizeMinify the tiger to half its \noriginal size.Move\nResize\nMake the cat wear a scarf  filled \nwith butterfliesMake the cat wear a bow tie. Make the cat wear a hat and \nnecklace.Make the cat decorated in \nChristmas style.\nChange the background to the \nforest.Change the background to the \nbeach.Change the background to the \nmountain.Change the background to the \ncity street.\nChange the color of the shirt to \nwhite.Change the color of the hat to \npurple.Change the color of the hair to \nred.Change the color of the jeans to \nblack.Appearance \nChange\nBackground \nChange\nColor ChangeFigure 9: More Visualization Results.\n20\n--- Page 21 ---\nInput\nChange the material of the car \nlike stone. Change the material of the car \nlike styrofoam.Edited Results\nMaterial\nChange the material of the car \nlike leather.Change the material of the car \nlike corduroy.\nChange the expression to not \nsmiling.Change the expression to smiling. Change the expression to \nsurprised.Change the appearance to old.\nChange the weather to foggy. Change the weather to raining. Change the time to winter. Change the time to the evening.Expression\nTone Transfer\nChange the style to 8-bit. Change the style to clean line. Change the style to cartoon. Change the style to ink painting.\nStyle ChangeFigure 10: More Visualization Results.\nInstruction: “Make the cat have a \nfloral pattern, put a vase on the chair. \nThen, alter the color of the spoon to \nyellow and erase the pink decoration \non the wall. Replace the biggest blue \ncoffee cup with a cake and zoom in \nthe pink cup next to the cat. Finally, \nchange the time to the evening.”1. [Appearance Change] make the cat have a floral pattern \n2. [Add] add a vase on the chair \n3. [Color Change] change the color of the spoon to yellow\n4. [Remove] erase the pink decoration on the wall\n5. [Replace] replace the biggest blue coffee cup with a cake\n6. [Resize] zoom in the pink cup next to the cat\n7. [Tone Transfer] change the time to the evening\nRoI Localization\n RoI Inpainting\n RoI Editing\n RoI Compositioning\n Global TransformationCoT\nNeural \nProgram \nInterpreter\n1. [Background Change] \n    Change the background to the forest.\n2. [Appearance Change] \n    Make the lady wear a white dress.\n3. [Add] Add a fox beside the lady\n4. [Tone Transfer] \n    Change the time to autumn \nNeural \nProgram \nInterpreter\nInstruction: “What \nwould it be like if we \nplaced the lady in a \nforest during autumn, \nand she was wearing \na white dress, with a \nfox beside her?”CoT\nFigure 11: More Detailed Visualization Processes of the pipeline.\n21\n--- Page 22 ---\nE Analysis and Discussions\nE.1 Runtime Performance Analysis\nWe evaluate the time required for each atomic operation of IEAP on a single NVIDIA H100 GPU.\nEmpirical measurements indicate that the RoI Localization stage requires approximately 3 sto5 sper\noperation. Other editing primitives, including RoI Inpainting, RoI Editing, RoI Compositing, and\nGlobal Transformation, each consumes roughly 7 sto9 sper operation.\nConsequently, a complete multi-step edit involving katomic operations exhibits a total latency of\nTtotal=kX\ni=1Tiwith Ti=\u001a3 sto5 s,if operationi=RoI Localization ,\n7 sto9 s,otherwise .\nWhile this per-operation cost precludes real-time interactivity, it remains acceptable for batch-oriented\nworkflows in digital content creation, scientific visualization, and other offline editing scenarios.\nE.2 Limitations and Future Work\nLimitations. Despite its strengths, IEAP exhibits several limitations in handling dynamic scenes and\ncomplex physical interactions. First, the RoI compositing may introduce geometric distortions or\ntexture discontinuities when editing highly dynamic or non-rigid content, such as motion-blurred\ninstances, and fluid or smoke effects. For example, in the task of “changing the cat’s action to\njumping,” in Fig. 6, the rapid motion of fur can produce blurred regions that fail to blend naturally\nwith the background. Second, RoI compositing struggles to simulate physically consistent lighting\neffects in scenes with reflective or refractive surfaces, sometimes resulting in mismatched shadow\ndirections and illumination conflicts between edited objects and their environments. For example,\nin the task of “change the action of the woman to dancing,” in Fig. 4, the shadows before and after\nediting remain the same, but the action of the woman has changed, so it is unnatural. Third, the\nDiT-based architecture and multi-stage atomic operations incur substantial inference latency for 5 s\nto9 sper operation on a single H100 GPU, precluding real-time interactivity in applications such\nas AR/VR. Finally, the requirement for high-memory GPUs like NVIDIA H100 (80 GB) limits\nreproducibility for resource-constrained researchers, and multi-iteration editing can exacerbate image\nquality degradation over successive operations.\nFuture Work. As for future work, several avenues may be pursued to overcome the identified limi-\ntations. To begin with, physics-aware compositing techniques and motion-compensated inpainting\ncould be explored to better accommodate dynamic blur and fluid effects, thereby ensuring seamless\nintegration of non-rigid edits. Meanwhile, differentiable lighting models or neural rendering modules\nmay be incorporated to enforce global illumination consistency, particularly in reflective and refrac-\ntive contexts. On the performance front, model distillation, operation fusion, and sparse attention\nstrategies could be investigated to reduce per-operation latency and facilitate interactive editing. To\nenhance accessibility, memory optimization and support for smaller-footprint architectures amenable\nto commodity GPUs may be implemented. Moreover, iterative refinement and error-correction\nmechanisms may be developed to mitigate quality degradation over successive editing steps. Fur-\nthermore, beyond still-image editing, an extension to video-based complex instruction editing could\nbe considered, where temporal coherence and motion consistency present additional challenges and\nopportunities for dynamic, multi-step visual manipulation.\nE.3 Societal Impacts and Ethical Safeguards\nPositive Societal Impacts. The proposed IEAP framework introduces a modular and interpretable\napproach to complex image editing, which holds significant potential to benefit a range of creative\nand technical domains. By decomposing high-level visual instructions into atomic operations, IEAP\nenables users to perform multi-step edits with enhanced precision and control. This capability\nis particularly valuable in digital content creation, advertising, and education, where fine-grained\nmanipulation of visual content is often required. For example, IEAP’s ability to support structurally\ninconsistent modifications can streamline visual storytelling workflows or facilitate the generation of\naccurate scientific visualizations for publications and teaching materials. Furthermore, its potential\nextensions to fields such as medical imaging by enabling localized enhancement of diagnostic visuals,\n22\n--- Page 23 ---\nand accessibility technology by generating descriptive visual representations for users with visual\nimpairments, demonstrate the framework’s broader societal utility and interdisciplinary relevance.\nNegative Societal Impacts and Ethical Safeguards. Despite its benefits, IEAP’s high-fidelity\nediting capabilities also introduce ethical risks, particularly in the domains of misinformation and\nprivacy. The framework’s precision in altering visual content could be misused for the creation of\ndeepfakes or manipulated images intended for disinformation, identity falsification, or reputational\nharm. Operations such as “Remove” or “Replace” could be exploited to tamper with sensitive or\nprivate imagery, potentially infringing on individual rights.\nTo address these concerns, the development and deployment of IEAP adhere to strict ethical standards.\nSpecifically, safeguards include the implementation of data filtering pipelines, such as the use of\nGPT-4o-filtered subsets of AnyEdit and the compliance-oriented CelebHQ-FM dataset, to reduce\nharmful biases and content. Additionally, the modular nature of IEAP facilitates transparency and\ntraceability in the editing process, supporting future content provenance systems designed to detect\nand flag manipulated media. All these safeguards jointly contribute to ongoing efforts in AI safety\nand accountability.\n23",
  "text_length": 72391
}