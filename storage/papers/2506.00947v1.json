{
  "id": "http://arxiv.org/abs/2506.00947v1",
  "title": "Deformable registration and generative modelling of aortic anatomies by\n  auto-decoders and neural ODEs",
  "summary": "This work introduces AD-SVFD, a deep learning model for the deformable\nregistration of vascular shapes to a pre-defined reference and for the\ngeneration of synthetic anatomies. AD-SVFD operates by representing each\ngeometry as a weighted point cloud and models ambient space deformations as\nsolutions at unit time of ODEs, whose time-independent right-hand sides are\nexpressed through artificial neural networks. The model parameters are\noptimized by minimizing the Chamfer Distance between the deformed and reference\npoint clouds, while backward integration of the ODE defines the inverse\ntransformation. A distinctive feature of AD-SVFD is its auto-decoder structure,\nthat enables generalization across shape cohorts and favors efficient weight\nsharing. In particular, each anatomy is associated with a low-dimensional code\nthat acts as a self-conditioning field and that is jointly optimized with the\nnetwork parameters during training. At inference, only the latent codes are\nfine-tuned, substantially reducing computational overheads. Furthermore, the\nuse of implicit shape representations enables generative applications: new\nanatomies can be synthesized by suitably sampling from the latent space and\napplying the corresponding inverse transformations to the reference geometry.\nNumerical experiments, conducted on healthy aortic anatomies, showcase the\nhigh-quality results of AD-SVFD, which yields extremely accurate approximations\nat competitive computational costs.",
  "authors": [
    "Riccardo Tenderini",
    "Luca Pegolotti",
    "Fanwei Kong",
    "Stefano Pagani",
    "Francesco Regazzoni",
    "Alison L. Marsden",
    "Simone Deparis"
  ],
  "published": "2025-06-01T10:30:58Z",
  "updated": "2025-06-01T10:30:58Z",
  "categories": [
    "cs.CV",
    "cs.NA",
    "math.NA",
    "68T07, 68U05,",
    "J.3; I.2.m; I.4.m"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00947v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00947v1  [cs.CV]  1 Jun 2025DEFORMABLE REGISTRATION AND GENERATIVE\nMODELLING OF AORTIC ANATOMIES BY\nAUTO –DECODERS AND NEURAL ODE S\nRiccardo Tenderini1,* Luca Pegolotti2,3,4Fanwei Kong3,4,5Stefano Pagani6\nFrancesco Regazzoni6Alison L. Marsden2,3,4,7Simone Deparis1\n1Institute of Mathematics, EPFL, Lausanne, Switzerland\n2Department of Bioengineering, Stanford University, CA, USA\n3Department of Pediatrics, Stanford University, CA, USA\n4Institute for Computational and Mathematical Engineering, Stanford University, CA, USA\n5Department of Mechanical Engineering and Materials Science, Washington University, St. Louis, MO, USA\n6MOX – Department of Mathematics, Politecnico di Milano, Milano, Italy\n7Cardiovascular Institute, Stanford University, CA, USA\nABSTRACT\nThis work introduces AD–SVFD, a deep learning model for the deformable registration of vascular\nshapes to a pre–defined reference and for the generation of synthetic anatomies. AD–SVFD operates\nby representing each geometry as a weighted point cloud and models ambient space deformations as\nsolutions at unit time of ODEs, whose time–independent right–hand sides are expressed through ar-\ntificial neural networks. The model parameters are optimized by minimizing the Chamfer Distance\nbetween the deformed and reference point clouds, while backward integration of the ODE defines\nthe inverse transformation. A distinctive feature of AD–SVFD is its auto–decoder structure, that\nenables generalization across shape cohorts and favors efficient weight sharing. In particular, each\nanatomy is associated with a low–dimensional code that acts as a self–conditioning field and that is\njointly optimized with the network parameters during training. At inference, only the latent codes\nare fine–tuned, substantially reducing computational overheads. Furthermore, the use of implicit\nshape representations enables generative applications: new anatomies can be synthesized by suit-\nably sampling from the latent space and applying the corresponding inverse transformations to the\nreference geometry. Numerical experiments, conducted on healthy aortic anatomies, showcase the\nhigh–quality results of AD–SVFD, which yields extremely accurate approximations at competitive\ncomputational costs.\nKeywords Diffeomorphic Surface Registration, Implicit Neural Representations, Generative Shape Modelling,\nNeural Ordinary Differential Equations, Computational Vascular Anatomy\nIntroduction\nOver the last two decades, the deformable registration of three–dimensional images has become increasingly\nimportant in a wide number of computer graphics and computer vision applications. In broad terms, the deformable\n— or non–rigid — registration problem consists in aligning and locating different shapes within a shared coordinate\nsystem, to enable meaningful comparisons and analyses [1, 2, 3]. Besides industrial and engineering applications,\ndeformable registration nowadays plays a crucial role in several medical imaging tasks, such as multimodal image\nfusion, organ atlas creation, and monitoring of disease progression [4, 5]. Unlike rigid registration, which involves\n*Corresponding author. Email: riccardo.tenderini@outlook.com\n--- Page 2 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nonly global scaling, rotations, and translations, deformable registration must estimate complex, localized deformation\nfields that account for natural anatomical variability. This challenge is enhanced by the presence of noise, outliers,\nand partial overlaps, which are very common in clinical data. Furthermore, exact point–to–point correspondences\nbetween different anatomies are rarely available in practice, which requires the adoption of alternative metrics to\nevaluate data adherence.\nThe challenge of developing efficient, reliable, and computationally tractable registration methods is of paramount\nimportance for improving medical imaging workflows, healthcare technologies, and patient care. Manual alignment\nof images in subject–specific clinical contexts is often infeasible or impractical, due to the complexity and variability\nof biological structures, as well as to the differences in imaging modalities and acquisition times. To address this\nlimitation, several automatic registration approaches have been developed. Among the most widely employed ones,\nwe can mention DARTEL [3], Diffeomorphic Demons [6], and LDDMM [7, 8, 9]. Notably, all these methods\nshare remarkable robustness characteristics, since they are based on a deformation of the ambient space, which is\nguaranteed to be smooth, differentiable, invertible, and topology preserving.\nWhile traditional image and shape registration approaches can yield extremely accurate results, they nonetheless\nentail non–negligible computational costs, that may hinder their use in real–time clinical practice. To mitigate\nthis issue and improve the overall performance, deep learning (DL) techniques have been exploited in various\nways. A non–exhaustive list of the most popular state–of–the–art DL–based registration methods includes the\nprobabilistic models developed in [10, 11], Voxelmorph [12], Smooth Shells [13], Neuromorph [14], Cyclemorph [15],\nDiffusemorph [16] and Transmorph [17]. We refer to [5, 18, 19, 20] for comprehensive literature reviews on the topic.\nIn our study, we are specifically interested in the registration of vascular surfaces. The latter can be seamlessly\nextracted from volumetric data, acquired through traditional imaging modalities, such as CT–scans or MRI. Further-\nmore, novel techniques like photoacoustic scanning [21, 22, 23] are rapidly gaining traction in clinical practice, since\nthey provide a low–cost radiation–free alternative, particularly well–suited for superficial vascular anatomies, located\nup to 15 mm beneath the skin. A review of the classical techniques for surface registration can be found in [24].\nIn this scenario, DL–based approaches can be subdivided into two major groups, depending on how surfaces are\nrepresented. On the one hand, we have methods that treat shapes as 3D point clouds [25], such as the ones introduced\nin [26, 27, 28]. On the other hand, instead, there exist several methods that represent 3D geometries by means of Deep\nImplicit Functions — namely continuous signed distance functions, expressed through neural networks [29, 30, 31]\n— such as the ones presented in [32, 33]. Notably, the models described in [27, 32, 33] encapsulate learnable latent\nshape representations, which enable the simultaneous registration of multiple geometries to a common reference, as\nwell as their use as generative AI tools.\nIn this work, we present a DL–based model for the deformable registration and synthetic generation of vascular\nanatomies, named AD–SVFD ( Auto–Decoder Stationary Vector Field Diffeomorphism ). The general structure of\nAD–SVFD, reported in Figure 1, is inspired to the models introduced by Amor et al. in [28] ( ResNet–LDDMM ), by\nKong et al. in [33] ( SDF4CHD ), and by Croquet et al. in [27]. Analogously to [28, 27], AD–SVFD treats geometries\nas three–dimensional point clouds and employs ad hoc data attachment measures to compensate for the absence of\nground–truth point–to–point correspondences. The vascular shapes registration is achieved by deforming the ambient\nspace according to an optimizable diffeomorphic map. The latter is approximated as the solution at unit time of\nan ordinary differential equation (ODE), whose time–independent right–hand side, representing a velocity field, is\nexpressed through a fully–connected artificial neural network (ANN) ( Neural ODE paradigm [34]). Another major\nfeature of AD–SVFD is its auto–decoder (AD) structure, introduced in a similar context in [31] ( DeepSDF ) and then\nfurther exploited e.g. in [33]. In fact, AD–SVFD enables the simultaneous registration of a cohort of source shapes\nto a pre–defined common reference by introducing low–dimensional learnable latent codes, that are provided as input\nto the model and that condition its weights. As such, AD–SVFD configures as a self–conditional neural field [35],\nsince the conditioning variable is part of the model trainables. Compared to the more widely employed auto–encoders\n(AEs) [36, 37, 38], that obtain latent input representations through a trainable encoding network, ADs entail faster\nand lighter optimization processes. Indeed, they roughly halve model complexity, at the cost of a cheap latent code\ninference procedure to be performed at the testing stage. Other than featuring improved generalization capabilities\nand favoring efficient weight sharing, implicit neural representations through latent codes also enable generative\nAI applications [39]. Indeed, synthetic anatomies can be crafted by drawing samples from empirical distributions,\ndefined over the latent space, and by applying the associated inverse transforms to the reference geometry.\n--- Page 3 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 1: General structure of the AD–SVFD model. The proposed approach leverages deep learning techniques\nto perform the diffeomorphic registration of vascular anatomies to a reference. Invertible ambient space deforma-\ntions are modeled as solutions at unit time of ODEs, whose right–hand sides are parametrized by neural networks.\nThe source and template geometries, represented as point clouds, are provided as input to AD–SVFD. The direct (top\npart of the image) and inverse (bottom part of the image) transforms are obtained by integrating the flow equations\nforward and backward in time, respectively. Geodesic paths can be visualized by morphing the input shapes at in-\ntermediate stages during the ODE integration. Generalization capabilities are enabled by associating each source\nshape with a trainable latent code (in green). The baseline model is optimized by minimizing the Chamfer distance\n(CD) between the mapped and the target geometries. Pointwise errors are quantified through the forward local dis-\ntance (FLD), expressed in cm, namely the distance of each point in the mapped geometry from the closest one in the\ntarget.\nLetTdenote the template (or reference) geometry and let {Si}Ns\ni=1denote the cohort of available patient–specific\nvascular anatomies; the latter will be referred to as the source cohort in the following. In particular, TandSiidentify\nthree–dimensional closed surfaces, that are represented as weighted point clouds of the form:\nT:=\b\u0000\nxt\nj, wt\nj\u0001\tMt\nj=1; Si:=\b\u0000\nxs\ni,j, ws\ni,j\u0001\tMs\ni\nj=1fori= 1,···, Ns. (1)\nHere,xt\nj,xs\ni,j∈R3are, respectively, the template and source points, and wt\nj, ws\ni,j∈R+are the associated weights,\nwhich add up to one. In general, the weights associated with isolated points in the cloud should be large, while those\nin regions of high local density should be lower. In this work, we construct the point clouds from available triangular\nsurface meshes by selecting the cell centers as points and computing the weights as the corresponding (normalized)\ncell areas. To facilitate training, we perform a preliminary rigid registration of the source shapes to the template,\nbased on the Coherent Point Drift algorithm [40], and we apply an anisotropic rescaling. In this way, we embed\nevery geometry in the unit cube Ω := [0 ,1]3and we can assume that xt\nj,xs\ni,j∈Ωwithout loss of generality. It is\nworth remarking that a tailored template shape can be estimated from the set of available anatomies, as done e.g. in\n[41, 42, 43, 33]. However, for simplicity, in this work we simply select one patient–specific anatomy to serve as a\nreference.\n--- Page 4 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nIn mathematical terms, our goal is to find a set of diffeomorphisms {⃗ φi}Ns\ni=1that solve the following minimization\nproblem:\n(⃗ φ∗\n1,···, ⃗ φ∗\nNs) = arg min\n(⃗ φ1,···,⃗ φNs)1\nNsNsX\ni=1\u0000\nD(⃗ φi(Si),T) +D\u0000\nSi,(⃗ φi)−1(T)\u0001\u0001\n, (2)\nwhere D:RM1×RM2→R+is some discrepancy measure between two point clouds, of cardinalities M1, M2∈N.\nHence, we want to learn a family of invertible ambient space deformations, whose elements allow to optimally (i)map\nthe source shapes to the template via the direct transforms {⃗ φ∗\ni}Ns\ni=1and(ii)map the template shape to the sources via\nthe inverse transforms {(⃗ φ∗\ni)−1}Ns\ni=1. As discussed before, the AD–SVFD model features an auto–decoder structure,\nthrough the use of low–dimensional latent codes {zi}Ns\ni=1,zi∈RNz, associated to the source shapes. In this way, the\nambient space deformation map associated to Sican be expressed as ⃗ φi(x) =⃗ φ(x;Θ,zi), so to entirely encapsulate\nthe input dependency into the shape code. Hence, the optimization problem in Eq.(2) can be conveniently rewritten as\nfollows: find Θ∗∈RNΘ,z∗\ni∈RNzfori= 1,···, Ns, such that\nΘ∗,(z∗\n1,···,z∗\nNs) = arg min\nΘ,(z1,···,zNs)1\nNsNsX\ni=1E\u0010\nSi,T, ⃗ φ(·;Θ,zi)\u0011\n,\nwhere E(S,T,⃗ϕ) :=D(⃗ φ(S),T) +D(⃗ φ−1(T),S)denotes the bidirectional mapping error between two point clouds\nSandT, through the diffeomorphism ⃗ φ.\nAdopting the stationary vector field (SVF) parametrization of diffeomorphisms [3, 44] (as done in [27, 33]), we exploit\nthe Neural ODE paradigm [34] to express the map φias the solution at unit time to the following learnable ODE:\n∂⃗ φi(x;t)\n∂t=⃗ v(⃗ φi(x;t),Θ,zi)such that ⃗ φi(x; 0) = x, (3)\nwhere the vector Θ∈RNΘcollects the trainable parameters of an ANN. As demonstrated in [45], if the ANN that\nexpresses the velocity field ⃗ vis fully–connected and features ReLU orLeaky–ReLU activation functions, then ⃗ vis\nLipschitz continuous and Eq.(3) admits a unique solution. Consequently, the inverse transform (⃗ φi)−1, that deforms\nthe ambient space so as to overlap the template point cloud Tto the source one Si, can be found by integrating Eq.(3)\nbackward in time. In this work, we employed the first–order forward Euler and modified Euler schemes to numerically\nintegrate the diffeomorphic flow equations forward and backward in time, respectively, considering K= 10 discrete\ntime steps, as in [28].\nOur approach is developed under the assumption that all shapes share the same topology. Conversely, it is not possible\nto guarantee the existence (and uniqueness) of a diffeomorphic flow field that exactly deforms one into the other. In\nfact, non–rigid registration under topological variability remains an open challenge [46].\nTo train the AD–SVFD model, we employ the following loss function:\nL(Θ,Z) :=1\nNsNsX\ni=1\u0010\nE\u0000\nSi,T;⃗ φ(·;Θ,zi)\u0001\u0011\n+wz∥Z∥2\n2+wΘ∥Θ∥2\n2+wvLreg(Θ), (4)\nwhere wz, wΘ, wv∈R+are scalar weight factors, Z∈RNz×Nsis a matrix collecting the shape codes associated\nto the Nstraining shapes, and Lregis a regularization term that constrains the velocity field learned by the ANN. In\nthe numerical experiments, we explore multiple alternatives for the data attachment measure Dthat appears in the\ndefinition of the bidirectional mapping error E. Specifically, we consider the Chamfer Distance (CD) [47], the point–\nto–plane Chamfer Distance (PCD) [48], the Chamfer Distance endowed with a penalization on the normals’ orientation\nscaled by the factor wn∈R+(denoted as NCD), and the debiased Sinkhorn divergence (SD) [49]. Furthermore, we\nexploit the availability of weights (see Eq.(1)) to derive data adherence measures, that should be better able to deal with\nunevenly distributed point clouds. The training procedure is carried out with the Adam optimizer [50], considering\nE= 500 epochs, a batch size B= 8, and setting the same learning rate λ∈R+to update the ANN parameters and\nthe shape codes. At each epoch, sub–clouds made of M= 2,000points are adaptively sampled to limit computational\nefforts and memory requirements. More details on both the data attachment measures and the training pipeline are\nprovided in Methods . During testing, we can combine Adam with higher–order memory–intensive methods, such as\nL–BFGS [51], since only the latent code entries have to be optimized. Specifically, we first run 100epochs using\nAdam and then we fine–tune the predictions using L–BFGS for 10epochs. Additionally, preliminary numerical results\nsuggested using a learning rate 50times larger than the one employed for training with Adam , as this facilitates and\nspeeds–up convergence.\n--- Page 5 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nResults\nWe present the numerical experiments conducted on the AD–SVFD model and briefly discuss the obtained results.\nAll tests have been performed starting from a dataset containing 20healthy aortic anatomies, that have been\nsegmented from medical images (CT–scans and MRIs) using SimVascular [52] (see Figure 2 (a)) and are publicly\navailable in the Vascular Model Repository [53]. As depicted in Figure 2 (b), we underline that all the geometries\nshare the same topology, that comprises the aortic vessel (ascending chunk (AA) and descending chunk (DA)),\nthe brachiocefalic artery (BA), the left and right subclavian arteries (LSA, RSA), and the left and right common\ncarotid arteries (LCCA, RCCA). To generate weighted point cloud representations of the shapes, we created\nvolumetric tetrahedral computational meshes and extracted triangulations of the external surfaces. This allowed\nus to choose the surface cell centers as the cloud points, and the surface cell areas as the associated weights (see Eq.(1)).\nThe number of available anatomies is evidently too low to train a DL–based model, whose performances drastically\ndepend on the amount of data at disposal. Therefore, we implemented an ad hoc data augmentation pipeline, based\non Coherent Point Drift (CPD) rigid registration [40] and thin–plate spline (TPS) interpolation [54]. We refer to\nAppendix A for a detailed description. This allowed us to assemble a dataset made of 902anatomies, out of which\n882have been artificially generated. A few synthetic anatomies are reported in Figure 2 (c). We perform a train–test\nsplitting, reserving 38shapes solely for testing. In particular, 2of the test geometries belong to the original dataset,\nand their corresponding augmented versions are not taken into account for training; the remaining 36test geometries\nare instead augmented versions of the 18original anatomies included in the training dataset ( 2augmented shapes\nper patient). Except for the cross–validation procedure, all the numerical tests are carried out considering the same\ntraining and testing datasets. They have been obtained by reserving patients P#093 andP#278 for testing, which\nresults in employing 780geometries for training. We remark that patient P#091 serves as a reference in all test cases.\nMost hyperparameters of the ANN model have been calibrated in a simplified single shape–to–shape registration\nscenario, using the Tree–structured Parzen Estimator (TPE) Bayesian algorithm [55, 56]. We refer to Appendix B.1\nfor a complete list of the hyperparameters and for a detailed description of the tuning procedure. Besides dic-\ntating the specifics of the ANN model architecture, the calibration results suggested to set the learning rate\nλΘ=λz=λ= 10−3, and the loss weights wv= 10−4andwz= 10−3(see Eq.(4)). Unless differently specified, the\nloss is computed considering the standard (i.e. not weighted) CD as a data attachment measure. The model accuracy is\nquantified through the forward and backward local distances (FLD and BLD), expressed in cm. The former identifies\nthe distance of each point in the mapped geometry from the closest one in the target, while the latter is the distance of\neach point in the target from the closest one in the mapped geometry.\nAll computations were performed on the Sherlock cluster at Stanford University , employing an AMD 7502P processor\n(32 cores), 256 GB RAM, HDR InfiniBand interconnect, and a single NVIDIA GeForce RTX 2080 Ti GPU. We note\nthat the exact reproducibility of the results cannot be guaranteed, owing to the use of non–deterministic algorithms\nprovided by the PyTorch library to enhance efficiency.\nTest 1: Latent shape codes\nWe investigate the effect of shape codes on the AD–SVFD model results, focusing in particular on the latent space\ndimension Nz. We point out that the training errors are computed only on the 18original shapes.\nFigure 3 reports the average (a) and maximal (b) FLD and BLD for both the direct and the inverse deformation,\nconsidering different values of Nz. On the one hand, the results demonstrate that the latent space dimension should\nbe taken sufficiently large, in order to effectively condition the model weights towards accurate approximations of\nthe diffeomorphic maps. On the other hand, we notice that model accuracy stalls for large values of Nz, suggesting\nredundant information in the shape codes. Ultimately, we select Nz= 256 as the latent dimension, since it appears to\noptimally balance accuracy and efficiency. In terms of generalization power, we note that training and testing errors\nare comparable for Nz≥256, thus indicating that no overfitting phenomenon occurs. Incidentally, we remark that no\nmajor discrepancy between the registration errors on original and augmented testing geometries can be observed. For\ninstance, only marginally lower maximal FLD are obtained on the augmented geometries, both considering the direct\nand the inverse map (direct map errors: 0.2774 cm vs.0.2822 cm ; inverse map errors: 0.2562 cm vs.0.2613 cm ). In\nFigure 3 (c), we appreciate how AD–SVFD smoothly and gradually warps the source shapes into the reference one,\nthrough the forward–in–time numerical integration of the learnable diffeomorphic flow equations (see Eq.(3)) by the\nexplicit Euler method.\n--- Page 6 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 2: Healthy aortic shapes dataset overview. In particular: (a) original dataset of patient–specific anatomies;\n(b) topology of the considered geometries, with nomenclature of the different branches; (c) original shape and three\nsynthetic samples, generated by deforming four anatomies with the implemented data augmentation pipeline.\nTest 2: Data attachment measures\nWe analyse the AD–SVFD model performances considering the different data attachment measures mentioned in the\nIntroduction . We refer to Methods for a detailed description of the different options and of their specifics. Table 1\nreports the maximal pointwise errors on both training and testing datapoints. To quantitatively compare the results,\nwe evaluate the maximal pointwise FLD and BLD, even when metrics different from the (unweighted) CD are used in\nthe loss. Since this approach may introduce a bias in the analysis, we also provide a qualitative accuracy assessment\nthrough Figure 4.\n--- Page 7 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 3: Deformable mapping results of the baseline AD–SVFD model. In particular, we report the average (a) and\nmaximal (b) pointwise errors — quantified through the forward and backward local distances FLD and BLD, in cm\n— on training and testing datapoints, obtained for different shape code dimensions Nz; in (c), we show the geodesic\npaths between two source shapes (P#090 for training, P#093 for testing) and the reference shape (P#091), generated\nby numerical integration of the diffeomorphic flow equations by the forward Euler method at K= 10 intermediate\nsteps.\nFrom both a quantitative and a qualitative standpoint, the best results are obtained considering the baseline model,\nwhich employs unweighted CD as a data attachment measure. Indeed, this model yields precise geometry reconstruc-\ntions on both training and testing shapes, and it is associated with the lowest training time (equal to 7h40m) and\nwith an average testing time of just 1m28sper shape†. Incorporating a penalization of the normals’ orientation in\nthe loss (with wn= 10−2) allows for marginal accuracy improvements, but at the cost of a much larger training time\n(equal to 16h07m), due to the increased number of model evaluations. Incidentally, the larger memory requirements\ninduced by the normals’ calculation prevent the use of L–BFGS at inference. To mitigate this issue, we replace NCD\nwith unweighted CD at testing; this allows to retain acceptable accuracy levels, even though worse than the training\nones, at equivalent inference times. Neither leveraging the weights associated with the point clouds nor adopting PCD\nimproves the mapping quality; in fact, both approaches are substantially outperformed by the baseline model. With a\nspecific focus on PCD, from Figure 4 we can observe marked discrepancies at the upper branches, which in the case\nof patient P#093 tend to squeeze into unrealistic flat morphologies. Lastly, we remark that the registration quality\ngets considerably worse when using debiased SD. Indeed, the deformed geometries take unlikely convoluted shapes,\nwhich become twisted and almost flat in the upper branches region. From a quantitative point of view, this translates\ninto errors that roughly double the ones obtained with CD. Furthermore, compared to the baseline model, the heavier\ncosts associated with the calculation of SD entail drastic increases in the durations of both training (from 7h40mto\n27h15m) and testing (from 1m28sto3m08sper shape on average).\n†Average testing times have been computed on the Kuma cluster at EPFL , considering a single NVIDIA H100 SXM5 GPUs,\n94 GB RAM (HBM2e), memory bandwidth of 2.4 TB/s, Interconnected with NVLink, 900 GB/s bandwidth.\n--- Page 8 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 1: Registration results of AD–SVFD considering different data attachment measures. In particular, we report\nthe maximal pointwise errors on training and testing datapoints, obtained for six different data adherence metrics.\nThe errors are quantified through the forward and backward local distances (FLD and BLD), expressed in cm. The\nbest value for each performance metric is marked in green. For reference, the template shape inlet diameter is 1.31\ncm, while the average inlet diameter in the dataset is 1.45 cm . Acronyms. CD: Chamfer distance; PCD: point–to–\nsurface Chamfer distance [48]; NCD: Chamfer distance with normals penalization; SD: debiased Sinkhorn diver-\ngence [49]. Notation: the Wsuperscript denotes the use of a weighted measure.\nTrain Errors (in cm) Test Errors (in cm)\nDirect Inverse Direct Inverse\nLoss FLD BLD FLD BLD FLD BLD FLD BLD\nDCD 0.2162 0.2175 0.2686 0.2297 0.2777 0.2253 0.2562 0.2642\nDW\nCD 0.2412 0.2497 0.2869 0.2564 0.4088 0.2479 0.2952 0.4283\nDPCD 0.3195 0.2165 0.3225 0.2611 0.3138 0.2516 0.2749 0.3540\nDW\nPCD 0.2515 0.2510 0.2958 0.2579 0.3489 0.2439 0.3043 0.3686\nDNCD 0.2033 0.2166 0.2628 0.2396 0.2965 0.2260 0.2497 0.3090\nDSD 0.4887 0.3795 0.5355 0.3923 0.4519 0.4695 1.1094 0.4384\nDW\nSD 0.5866 0.3861 0.5155 0.3917 0.4156 0.4155 0.8275 0.4420\nFigure 4: Registration results of AD–SVFD considering different data attachment measures. In particular, we show\nthe direct and inverse mapping pointwise errors, obtained on a training ( P#090 ) and a testing ( P#093 ) datapoint, for\nfour different data adherence metrics. The errors are quantified through the forward local distance (FLD), expressed\nincm, namely the distance of each point in the mapped geometry from the closest one in the target. For reference,\nthe inlet diameters are 1.31 cm for the template shape, 1.21 cm forP#090 , and 1.32 cm forP#093 . Acronyms. CD:\nChamfer distance; PCD: point–to–surface Chamfer distance [48]; NCD: Chamfer distance with normals penaliza-\ntion; SD: debiased Sinkhorn divergence [49].\nTest 3: Comparison with state–of–the–art methods\nTo fairly assess the capabilities of AD–SVFD, we run a comparison test with five alternative shape registration models.\nSpecifically, we evaluate the mapping quality considering two different source shapes: P#090 (training) and P#278\n(testing). We investigate the following models: Coherent Point Drift (CPD) [40] (rigid registration model, serving as\n--- Page 9 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 2: Comparison test of AD–SVFD with six alternative registration methods. In particular, we report the av-\nerage and maximal pointwise errors on patients P#090 andP#278 , obtained considering CPD [40], TPS [54],\nLDDMM [9], ResNet–LDDMM [28] (optionally endowed with a penalty of the inverse deformation, I–ResNet–\nLDDMM), SDF4CHD [33] and AD–SVFD. The errors are quantified through the forward and backward local dis-\ntances (FLD and BLD), expressed in cm. For reference, the inlet diameters are: 1.31 cm forP#091 (template); 1.22\ncmforP#090 ;1.52 cm forP#278 .\nMax Errors (in cm) Avg Errors (in cm)\nDirect Inverse Direct Inverse\nMethod FLD BLD FLD BLD FLD BLD FLD BLDP#090CPD 1.4321 2.0983 1.3985 2.9714 0.2709 0.3351 0.3768 0.4669\nTPS 0.2918 0.2615 0.4305 0.3674 0.0777 0.0770 0.1050 0.1029\nLDDMM 0.1813 0.1227 0.2625 0.3332 0.0315 0.0290 0.0438 0.0412\nResNet 0.2470 0.2806 0.4393 0.6520 0.0530 0.0554 0.0948 0.0983\nI–ResNet 0.1948 0.2269 0.2139 0.2466 0.0454 0.0468 0.0484 0.0469\nSDF4CHD 0.3861 1.7831 0.6250 0.4207 0.0399 0.0684 0.0502 0.0405\nAD–SVFD 0.1693 0.1923 0.2071 0.1719 0.0387 0.0399 0.0430 0.0411P#278CPD 1.5265 1.1772 1.3802 2.0471 0.2997 0.2962 0.3481 0.3648\nTPS 0.5092 0.3521 0.7974 0.7104 0.0649 0.0644 0.0987 0.0931\nLDDMM 0.1281 0.1681 0.5160 0.5441 0.0294 0.0285 0.0552 0.0692\nResNet 0.3085 0.2720 0.3546 0.3594 0.0551 0.0555 0.0651 0.0615\nI–ResNet 0.2805 0.2498 0.2986 0.3367 0.0486 0.0485 0.0547 0.0525\nSDF4CHD 0.2598 1.2918 1.0277 0.2754 0.0421 0.0604 0.0592 0.0464\nAD–SVFD 0.2166 0.1807 0.2817 0.2933 0.0379 0.0370 0.0420 0.0419\na baseline), thin–plate spline (TPS) interpolation [54] (see Appendix A for details), LDDMM [9], SDF4CHD [33] and\nResNet–LDDMM [28]. A few aspects deserve attention.\n• Except from SDF4CHD, all the other approaches perform single shape–to–shape registrations, without lever-\naging any form of implicit geometry representation. Hence, for these models there is no distinction between\ntraining and testing shapes.\n• Using CPD, TPS and LDDMM, we can only estimate a one–directional map, warping the source shape\ninto the reference one or viceversa. Therefore, the direct and inverse maps are retrieved by running two\nindependent optimization processes. While this approach may improve registration accuracy, it comes at the\ncost of increased computational efforts and does not guarantee that the two maps compose to the identity.\n• As reported in [28], despite learning a diffeomorphism between two shapes, the ResNet–LDDMM model\nis solely optimized considering the source–to–template map result. To enhance inverse mapping quality,\nwe introduce the I–ResNet–LDDMM model. Compared to the baseline, the latter solves a multi–objective\noptimization problem, including both direct and inverse mapping results within the loss. To provide a fair\ncomparison with AD–SVFD, we employ CD as a data attachment measure, rely on the modified Euler scheme\nto integrate the diffeomorphic flow ODE backward in time, and equally weigh direct and inverse errors.\nRegarding the models’ specifics, for ResNet–LDDMM and SDF4CHD we employ the “optimal” model structures\nand hyperparameter sets, as identified in [28] and [33], respectively. Furthermore, with SDF4CHD we do not exploit\nthe DeepSDF model [31] to learn the SDF representation of an Atlas shape; instead, we use the pre–computed SDF\nof patient P#091 to serve as reference. For LDDMM, we rely on the Deformetrica [9] software, and perform single\nshape–to–shape registration employing the varifold distance, with a Gaussian kernel of width 0.8as data attachment\nmeasure. This last value, which leads to the optimization of roughly 1,000control points and momenta vectors, has\nbeen manually calibrated to balance efficiency and accuracy.\n--- Page 10 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 5: Registration results obtained with AD–SVFD and three alternative approaches. In particular, we show the\ndirect and inverse mapping pointwise errors, obtained with LDDMM [9], SDF4CHD [33], ResNet–LDDMM [28]\nand AD–SVFD on a training ( P#090 ) and a testing ( P#278 ) datapoint. The errors are quantified through the forward\nlocal distance (FLD), expressed in cm, namely the distance of each point in the mapped geometry from the closest\none in the target. For reference, the inlet diameters are 1.31 cm for the template shape, 1.21 cm forP#090 , and 1.52\ncmforP#278 .\nTable 2 reports the maximal and average pointwise errors of the direct and inverse mappings obtained on the two\nconsidered source shapes, for the different registration models. Figure 5 displays the results for four of the models. In\nsummary, we can claim that AD–SVFD and LDDMM significantly outperform all the other approaches. In particular,\nLDDMM yields the most precise approximations of the direct map; however, its performances deteriorate and fall\nbehind the ones of AD–SVFD on the inverse map, particularly because of discrepancies at the inlet/outlet faces. A\nsimilar consideration holds for the SDF4CHD model, that is capable of producing anatomies that closely match the\ntarget ones, but that often feature artifacts and/or completely miss the final portion of the smallest branches. In contrast\nwith the results reported in [28], the residual neural network structure of ResNet–LDDMM does allow it to outperform\nthe canonical LDDMM method. Nonetheless, we acknowledge that fine–tuning the main model hyperparameters to\nthe present test case could sensibly improve the results. Additionally, we underline that the introduction of a penalty\non the inverse mapping in ResNet–LDDMM determines minor but tangible improvements on all metrics.\nTest 4: Robustness assessment\nTo save computational resources, all numerical experiments described so far were conducted in a “fixed” scenario,\nnamely for the same random initialization of the trainable parameters and reserving the same patients ( P#093 ,P#278 )\nto testing. This way of proceeding prevents from thoroughly assessing robustness, which is instead of paramount\nimportance in DL applications. To this aim, we perform a 10–fold cross–validation, designed with respect to the\n“original” geometries in the dataset. This means that, if the anatomy of a given patient is reserved for testing, then\nall the augmented versions of such anatomy are not considered for training. For each fold, we run three independent\ntraining processes, considering different random seeds. For this test, both training and testing errors are computed\nsolely accounting for original anatomies.\nTable 3 reports the obtained results, in terms of training and testing FLD and BLD, for both the inverse and the\ndirect mapping. We observe that all models yield precise approximations of the diffeomorphic maps on the training\ndatapoints, with maximal pointwise errors that always lie below the 0.30 cm threshold. However, markedly larger\nerrors are produced at testing, in particular for folds #1, #2, #8, #10. This phenomenon can be explained by considering\nthat these folds respectively reserve for testing patients P#275 ,P#207 ,P#188 ,P#205 , whose geometries present\nfeatures that are uniquely represented within the dataset. For instance (see Figure 2): patient P#207 is characterized\n--- Page 11 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 3: Cross–validation procedure results. In particular, we report the maximal pointwise errors on training and\ntesting datapoints, solely considering original anatomies, obtained with the AD–SVFD model for the ten different\nfolds during cross–validation. The errors are quantified through the forward and backward local distances (FLD\nand BLD), expressed in cm. The reported results are averages, that stem from three independent training procedures,\nconducted by setting different random seeds. For reference, the template shape inlet diameter is 1.31 cm , while the\naverage inlet diameter in the dataset is 1.45 cm .\nTrain Errors (in cm) Test Errors (in cm)\nDirect Inverse Direct Inverse\nFold # Test P# FLD BLD FLD BLD FLD BLD FLD BLD\n1 090,275 0.2439 0.2367 0.2775 0.2401 0.5000 0.3153 0.3708 0.5747\n2 091,207 0.2406 0.2174 0.2692 0.2397 0.6429 0.2690 0.2512 0.6902\n3 139,143 0.2232 0.2171 0.2704 0.2259 0.3091 0.2687 0.2710 0.2638\n4 093,187 0.1984 0.2125 0.2641 0.2252 0.3485 0.2458 0.2400 0.4382\n5 272,277 0.2390 0.2418 0.2702 0.2312 0.3119 0.2281 0.2856 0.3770\n6 092,201 0.2351 0.2210 0.2748 0.2291 0.2236 0.2350 0.2429 0.2745\n7 144,278 0.2307 0.2166 0.2675 0.2360 0.3484 0.2328 0.2950 0.3608\n8 094,188 0.2252 0.2228 0.2639 0.2267 0.6567 0.3454 0.3602 0.4967\n9 142,145 0.2120 0.2046 0.2525 0.2225 0.2901 0.2867 0.3414 0.2676\n10 141,205 0.2205 0.2170 0.2628 0.2260 0.5114 0.4143 0.3124 0.3639\nAvg 0.2269 0.2208 0.2673 0.2302 0.4143 0.2841 0.2971 0.4107\nStd 0.0134 0.0104 0.0067 0.0060 0.1448 0.0564 0.0455 0.1340\nby the only anatomy whose RSA bends towards (and not away from) RCCA; patient P#205 is the only one whose\nhorizontal LSA chunk could not be segmented. Hence, the drop in precision can be ascribed to data paucity.\nTest 5: Latent space analysis and generative modelling\nThe use of low–dimensional latent codes, belonging to the learnable space Z, makes AD–SVFD suited for\ngenerative modelling. Indeed, once the model is trained, new anatomies can be generated by sampling shape\ncode instances from Zand applying the corresponding inverse maps to the template geometry. Incidentally, we\nhighlight that the robustness of the generative process is intimately related to the latent space regularity. For this rea-\nson, we include a penalization of the shape code entries in the loss, weighted by the positive constant ωz(see Eq. (11)).\nFigure 6 reports a sketch of the latent space learned by the AD–SVFD model. We show the projections of the shape\ncodes onto a two–dimensional subspace, obtained through Principal Component Analysis (PCA). Furthermore, we\ndisplay 10entries that are randomly sampled from N(0,Σz)— where Σzis an unbiased estimate of the covariance\nmatrix computed from the training shape codes (red&black circles) — and 2entries sampled by linear interpolation in\nthe latent space (red&black squares). From a qualitative standpoint, the learned space seems rather smooth. Indeed,\ngeometries whose codes are close in Zalso look similar in the physical space, whereas shapes whose codes lie far\napart in the latent space exhibit evident discrepancies. The linear interpolation results (see bottom–left corner in\nFigure 6) support this observation, since the two sampled geometries feature intermediate traits between those of the\ntwo patients.\nDiscussion\nWe introduced AD–SVFD, a deep learning model for the non–rigid registration and synthetic generation of three–\ndimensional surfaces, tailored to vascular anatomies and, in particular, to healthy aortas.\nAnalogously to [27, 28], the AD–SVFD model performs 3D point cloud registration, leveraging shape representations\nin the form of weighted point clouds, whose weights are proportional to the nearest neighbours distance (see Eq.(1)).\nIn this regard, AD–SVFD differs from deformable registration models based on continuous signed distance functions\n(SDFs), such as the ones presented in [32, 33]. As empirically demonstrated in Test 3 through a comparison of\n--- Page 12 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 6: Representation of the latent space learned by the AD–SVFD model. In particular, we show the projection\nof the shape codes onto the two–dimensional subspace obtained through PCA on the whole set of training codes. We\nreport the latent codes of the original patients (stars), and, for each of those, the latent codes of 10associated aug-\nmented geometries (circles). Furthermore, we display 10entries sampled from N(0,Σz), where Σzis an unbiased\nestimate of the covariance matrix computed from the training codes (red&black circles), and 2entries sampled by\nlinear interpolation between two patients in the latent space (red&black squares). The black arrows map shape code\ninstances to their counterparts in the physical space.\nthe performances of AD–SVFD and SDF4CHD, this approach enables more precise reconstructions, at least for\nvascular anatomies. Indeed, as shown in Figure 5, AD–SVFD clearly outperforms SDF4CHD [33], whose deformed\nanatomies either omit or severely distort most of the smallest branches. This phenomenon can plausibly be attributed\nto the use of SDFs, whose resolution must remain limited for computational efficiency reasons, thereby hindering the\naccurate capture of the finest details. Notably, the outcomes of Test 3 also reveal two additional key aspects. On the\none hand, AD–SVFD demonstrates superior performance, in both accuracy and efficiency, compared to alternative\n3D point cloud registration methods such as ResNet–LDDMM [28]. On the other hand, traditional approaches\nlike LDDMM [9], not rooted in DL techniques, exhibit comparable accuracy metrics, but are significantly more\ncomputationally demanding at inference.\nDealing with point clouds in the absence of ground–truth point–to–point correspondences required the consideration\nof alternative data attachment measures, both to construct an effective loss function and to design informative error\nindicators. This aspect was analysed in Test 2 , where multiple data adherence metrics were investigated. Although\nrepresenting the baseline alternative, the canonical (i.e. unweighted) Chamfer Distance outperforms all other options,\ndelivering the most precise geometry reconstructions at the lowest computational costs and memory requirements. In\nthe test case at hand, neither incorporating the normals’ orientation nor exploiting the point cloud weights resulted in\nimproved precision, while instead inducing moderate to substantial increases in complexity. Notably, the performance\nachieved using the debiased Sinkhorn divergence in the loss proved unsatisfactory in terms of both accuracy and\nefficiency, as also observed in [28] on non–elementary geometries.\n--- Page 13 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nAs in the model proposed in [27], AD–SVFD expresses diffeomorphic maps through the stationary vector field\nparametrization. Specifically, the ambient space deformation is defined as the solution at unit time of a system\nof ODEs, whose learnable right–hand side does not explicitly depend on time (see Eq.(3)). In particular, the\nright–hand side is modeled by a fully–connected and Leaky–ReLU activated ANN, so to ensure well–posedness. By\nnumerically integrating the diffeomorphic flow equations over time, it becomes possible to reconstruct the geodesic\npaths connecting source anatomies to the template. As illustrated in Figure 3 (bottom), this procedure gives rise to\na collection of synthetic shapes, exhibiting a smooth and gradual transition from the source characteristics to the\nreference ones.\nExtracting the intermediate stages of numerical integration is not the only means of generating artificial geometries\nwith AD–SVFD. Indeed, a crucial feature of the proposed model, distinguishing it for instance from ResNet–\nLDDMM [28], is the internalization of latent embeddings for the source anatomies. Similarly to the models presented\nin [32, 33], this is accomplished by introducing low–dimensional shape codes, which serve as trainable input variables\nwithin an auto–decoder architecture. Consequently, the available geometries are somehow non–linearly projected\nonto a low–dimensional latent space, where convenient random sampling routines can be implemented for generative\npurposes. Further details regarding the definition and treatment of shape codes are provided in Methods . In Test\n1, it was demonstrated that the dimension of the latent space, denoted as Nz, should be carefully calibrated to\noptimally balance accuracy and efficiency. As illustrated in Figure 3 (top), accuracy is significantly compromised\nwhen excessively small shape codes are employed, while it plateaus for large values of Nz, where model complexity\nand memory demands become instead prohibitive.\nIn addition to controlling the latent space dimension, monitoring its regularity is of paramount importance to ensure\nthe robustness and reliability of downstream generative AI applications. To this end, a suitable penalization term was\nincluded in the loss function (see Eq.(4)), with its weight wz= 10−3carefully fine–tuned. As briefly discussed in\nTest 5 , and illustrated in Figure 6, this strategy ultimately enables the construction of a smooth latent space that can\nbe robustly queried to generate customizable, realistic, synthetic anatomies. It is worth noting that a well–established\nand widely adopted technique to enforce latent space regularity consists of variational training. Accordingly, several\nexploratory experiments were conducted in this direction, updating both the model structure and the loss function to\nimplement a variational auto–decoder formulation for AD–SVFD [57]. However, no substantial improvements in\nregularity or robustness were observed, while approximation quality was markedly degraded.\nDespite exhibiting highly promising results, the current work nonetheless presents certain limitations. First and\nforemost, as with many DL–based models, data availability imposes non–negligible performance constraints, which\ncould only be partially mitigated through data augmentation. This issue becomes particularly evident from the\ncross–validation results reported in Test 4 . Specifically, AD–SVFD accuracy on unseen anatomies declines for folds\ncontaining testing shapes featuring unique traits within the dataset, such as P#205 andP#207 (see Figure 2). It\nis noteworthy that additional aortic anatomies from the Vascular Model Repository were also considered during\npreliminary stages. However, these were subsequently discarded due to incompatibility with the adopted data\naugmentation pipeline, which produced undesired non–physiological artifacts. Incidentally, although the conducted\ntests were limited to healthy aortas, it is important to emphasize that the proposed registration approach is general and\ncan be seamlessly extended to a wide range of challenging applications. Secondly, to reduce computational effort,\nmost hyperparameters were fine–tuned within a simplified single shape–to–shape registration setting, as detailed in\nAppendix B.1. In practice, only the hyperparameters associated with the shape codes ( Nz,wz, andλz) were calibrated\nusing the full AD–SVFD model. Consequently, at least marginal performance improvements may be achievable\nthrough hyperparameters configurations specifically tailored to a multi–shape context. Lastly, the present analysis\nfocused on the size and regularity of the latent space, but it did not address its interpretability. Investigating this aspect\nmay substantially enhance the generative pipeline, and will therefore be the subject of future developments.\nIn conclusion, AD–SVFD may serve as a valuable tool for engineering applications involving physical problems in\ncomplex geometries. The proposed approach introduces potentially distinctive elements for facilitating geometry\nmanipulation, notably by simultaneously providing compact and portable representations and by learning accurate,\nsmooth, invertible, and topology–preserving mappings to a pre–defined reference. Notwithstanding improvements\nof its generalization capabilities, AD–SVFD is envisioned as a pre–trained module within physics–aware machine\nlearning frameworks, enabling the incorporation of realistic geometrical variability into physical processes simulations\n[58, 59, 60, 61, 62].\n--- Page 14 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nMethods\nWe provide a more detailed analysis of the AD–SVFD model, specifically focusing on the shape codes, the ANN\narchitecture, the numerical integration of the flow equations, the data attachment measures and the optimization pro-\ncedure.\nLatent shape codes\nAD–SVFD provides a unified framework for the simultaneous registration of the source anatomies to a pre–defined\ntemplate, leveraging implicit neural representations. Indeed, every source shape Siis associated to a shape code\nzi∈RNz, so that the diffeomorphism ⃗ φimapping SitoTconfigures as the specialized version of a “generic”\ndiffeomorphism ⃗ φ, i.e.⃗ φi(x) :=⃗ φ(x,zi), withx∈R3.\nInstead of directly providing the latent codes in input to the model, we borrow from [33] the use of a position–\naware shape encoding strategy [63]. Given a shape code zi, we define the associated shape code grid Zi∈\nRgz×gz×gz×(Nz/g3\nz)asZi=Rz(zi), where Rz:RNz→Rgz×gz×gz×(Nz/g3\nz)is a suitable reshaping function. Here\nwe suppose that the shape code dimension Nzis a multiple of g3\nz; in this work, we always select gz= 2. Then, for a\ngiven point x∈Ω, the position–aware shape code ¯zi(x)∈RNz/g3\nz, associated to the source shape Si, is obtained by\nevaluating the trilinear interpolation of Ziatx, i.e. ¯zi(x) := Lerp( x,Zi), being Lerp(·,·)the trilinear interpolation\nfunction. This approach comes with two major advantages. On the one hand, the positional–awareness of the latent\ncodes helps the model in better differentiating the deformation flow field, depending on the location within the domain.\nOn the other hand, even if the total number of trainable parameters is unchanged, only (Nz/g3\nz)–dimensional vectors\nare provided as input to the ANN. Hence, model complexity is (slightly) reduced compared to the naive approach,\nideally at no loss in representation power.\nArtificial neural network architecture\nTo learn the diffeomorphisms between the source anatomies and the template, we exploit the Neural ODE ap-\nproach [34], employing an ANN to approximate the right–hand side of Eq.(3). More specifically, we consider a\nDL–based structure comprising three modules:\n•Feature Augmentation network (FA–NN) : the first part of the model performs a data–driven feature augmen-\ntation of the input locations. It consists of a fully–connected ANN that takes as input a spatial location x∈Ω\nand the associated position–aware shape code ¯zi(x)and yields a set of latent features xi,FA∈RNFAas\noutput. Since FA–NN solely acts as a feature augmentation compartment, we do not want it to weigh down\nmodel complexity. So, we consider shallow networks with few neurons per layer. We highlight that the\nlearned features are anatomy–dependent, thanks to the conditioning effect of the shape code on the model\nweights. We can summarize the FA–NN action via the function FFA:R3×RNz/g3\nz→RNFA, such that\nxi,FA=FFA(x,¯zi(x);Θ).\n•Fourier Positional Encoder (FPE) : in the second module, the learned latent features xi,FA undergo a fur-\nther augmentation step, via a deterministic Fourier positional encoding [64], adopting a base–2 logarith-\nmic sampling strategy in the frequency domain. This step is crucial for mitigating the spectral bias of\nANNs [65]. We can summarize the FPE action via the function FFPE :RNFA→RNNPE, such that\nxi,FPE =FFPE(xi,FA), where NNPE := (2 Ne+ 1)NFA.\n•Diffeomorphic Flow network (DF–NN) : the last chunk of the ANN model is responsible for the approximation\nof the stationary velocity field at the spatial location x— the right–hand side of Eq.(3) — given the augmented\nfeatures xi,FPE and the position–aware shape code ¯zi(x). As for FA–NN, we use a fully–connected ANN.\nHowever, since DF–NN is the core part of the model, we consider deeper architectures with a larger number\nof neurons in each layer. We underline that the output of DF–NN depends on the source anatomy, thanks to\nthe external conditioning effect of the shape codes. We can summarize the DF–NN action via the function\nFDF:RNFPE×RNz/g3\nz→R3, such that vi=FDF(xi,FPE ,¯zi(x);Θ).\nUltimately, we can express the action of the entire ANN model by the function F:R3×RNz/g3\nz→R3, defined as\nF:=FFA◦ FFPE◦ FDF. As resulting from the hyperparameters tuning procedure (see Appendix B), we consider:\n(i)Leaky–ReLU activation functions, with negative slope of 0.2, for both FA–NN and DF–NN; (ii)FA–NN with 3\nlayers of dimension 64;(iii)DF–NN with 5layers of dimension 256;(iv)Ne= 3for the FPE. Neglecting the latent\ncodes’ contributions, the ANN model counts approximately 278ktrainable parameters. To ease the notation, in the\nfollowing we omit the explicit spatial dependency of the shape codes.\n--- Page 15 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nNumerical integration of the flow equations\nFor the numerical integration of the diffeomorphic flow equations (see Eq.(3)), we rely on first–order methods. Specif-\nically, for the forward–in–time direct mapping, we employ the explicit forward Euler method. Let xs,(0)\ni,j=xs\ni,j∈Ω\nbe a point of the source point cloud Si, where the superscript (0)denotes the initial iteration count. Also, let K∈N\nbe the number of time steps; in all tests, we set K= 10 . Then, for k < K , the time marching scheme proceeds as\nfollows:\nxs,(k+1)\ni,j =xs,(k)\ni,j+1\nKF\u0010\nxs,(k)\ni,j,¯zi;Θ\u0011\n=xs,(k)\ni,j+1\nKvs,(k)\ni,j. (5)\nThe point corresponding to xs\ni,jin the template space is then the result of Eq.(5) at k=K−1, i.exs,(K)\ni,j .\nTo compute the inverse map, which deforms the ambient space so as to overlap the template anatomy to the source,\nwe integrate the flow equations backward–in–time, given a final condition. In particular, we want the discrete inverse\nmap to be the “true” inverse of the discrete direct map, defined in Eq.(5). So, let xt,(K)\ni,j =xt\nj∈Ωbe a point of the\ntemplate point cloud T. The time marching scheme at step k >0proceeds as follows:\nxt,(k−1)\ni,j =xt,(k)\ni,j−1\nKF\u0010\nxt,(k−1)\ni,j ,¯zi;Θ\u0011\n. (6)\nEven though Eq.(6) allows to invert Eq.(5) exactly, its use may be difficult in practice, being an implicit scheme.\nIndeed, the nonlinearity of Fentails the use of ad hoc numerical techniques, such as Newton iterations, to compute\na solution. Despite the Jacobian of Fcan be efficiently computed by automatic differentiation, the whole procedure\nis likely to slow down both the forward and the backward pass. For this reason, we rely on a first–order explicit\napproximation of Eq.(6) — known as the modified Euler scheme — that writes as follows:\nxt,(k−1)\ni,j =xt,(k)\ni,j−1\nKF\u0012\nxt,(k)\ni,j−1\nKF\u0010\nxt,(k)\ni,j,¯zi;Θ\u0011\n,¯zi;Θ\u0013\n=xt,(k)\ni,j−1\nKvt,(k)\ni,j. (7)\nThe point corresponding to xt\njin the source space is then the result of Eq.(7) at k= 1, i.ext,(0)\ni,j.\nData attachment measures\nAs reported in Eq.(1), we represent three–dimensional surfaces as (weighted) point clouds and we assume to not\nknow exact point–to–point correspondences. Therefore, suitable data attachment measures to quantify the discrepancy\nbetween point clouds have to be considered. The simplest alternative is offered by the Chamfer Distance (CD) DCD:\nRM×3×RM′×3→R+, that is defined as\nDCD(Y, Y′) :=1\nMMX\ni=1min\nc′∈Y′∥Yi−c′∥2\n2+1\nM′M′X\ni′=1min\nc∈Y∥c−Y′\ni′∥2\n2. (8)\nIn particular, the CD comprises the sum of two terms: the forward CD (FCD), which compares the points in Ywith the\nclosest ones in Y′, and the backward CD (BCD), which compares the points Y′with the closest ones in Y. Considering\nboth components is crucial to obtain a meaningful goodness–of–fit measure. CD has proven to be an effective metric\nfor diffeomorphic registration, particularly in the computational anatomy framework, as shown e.g. in [28]. However,\nin [47] it has been demonstrated that using CD is also likely to yield low–quality gradients. To mitigate this issue, we\nconsider the Earth’s Mover Distance (EMD) DEMD :RM×3×RM′×3→R+[66, 67]:\nLEMD (Y, Y′) = min\nξ∈M(Y,Y′)X\ny∈Y||y−ξ(y)||2\n2,\nwhere M(Y, Y′)denotes the set of 1–to–1 (bipartite) mappings from Y to Y’. In a nutshell, EMD is a Wasserstein\ndistance, that seeks for the optimal transport plan that orders the points in Y′to match the ones in Y. In practice, we\napproximate EMD with the debiased Sinkhorn Divergence (SD) DSD[49]. The latter is the solution to an Optimal\nTransport problem with entropic constraints, and it can be estimated using the iterative Sinkhorn’s algorithm [68].\nWe refer the reader to [47] for the precise definition of DSD; further details and a state–of–the–art literature review\non diffeomorphic registration using SD can be found in [69]. In all numerical tests conducted using SD, we consider\na quadratic ground cost point function, a temperature scalar ε= 10−4, and a linear ε–scaling with factor 0.9. This\ncombination of hyperparameters should be sensible for input measures that lie in the unit cube, providing a good\ntrade–off between accuracy and efficiency [70].\n--- Page 16 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nAccording to [47], SD is a good overlapping metric only if the points are roughly equispaced. However, SD can be\neffectively extended to unevenly distributed point clouds if the latter are weighted, i.e. if each point is associated to a\nquantity proportional to its distances from the closest neighbours. In fact, such weights appear in the entropic regular-\nization term and in the entropic constraints of the associated optimal transport problem, awarding more “importance”\nto the most isolated points in the cloud. As reported in Introduction , in this work we extract the cloud points as the cell\ncenters of available surface triangulations and we compute the weights as the corresponding cell areas, normalized to\nadd up to one. In the following, we denote by DSDthe standard SD, where all the weights are assumed to be equal,\nand by DW\nSDthe weighted SD. A similar reasoning can also be extended to the CD, even if the latter is not related to\nany optimal transport problem [71]. In this work, we define a weighted CD DW\nCD:RM×(3+1)×RM′×(3+1)→R+\nas follows:\nDW\nCD((Y, w),(Y′, w′)) :=1\nNMX\ni=1wimin\nc′∈Y′∥Yi−c′∥2\n2+1\nN′M′X\ni′=1w′\nimin\nc∈Y∥c−Y′\ni′∥2\n2. (9)\nAlternatively to the use of SD, we also try to mitigate the low–quality gradient issue by developing variants of CD\nthat exploit information coming from the source and template surface normals. In fact, CD is agnostic of the closed\nsurface structure of the manifold from which the points are sampled, as it solely relies on point–to–point distances. In\nparticular, we consider two surface–aware corrections of CD. The first one — denoted as DNCD — simply consists\nof adding a regularization term that penalizes the discrepancy between the normals, i.e.\nDNCD(Y, Y′) :=DCD(Y, Y′) +wn\n2MMX\ni=1\u0010\n1−ni·nc′\ni\u00112\n+wn\n2M′M′X\ni′=1\u0010\n1−ni′·nci′\u00112\n, (10)\nwhere c′\ni:= min c′∈Y′∥Yi−c′∥2\n2,ci′:= min c∈Y∥c−Y′\ni′∥2\n2, andwn∈R+is a scale factor. Here ni,ni′,nc′\ni,nci′∈\nR3denote the (supposed known) outward unit normal vectors to the target surface, evaluated at Yi, Y′\ni,c′\ni,ci′, respec-\ntively. Also, a·b:=P\njajbjis the standard inner product. Preliminary numerical tests suggested to set wn= 10−2,\nwhich results in the normals’ penalization term to account for roughly 10% of the loss value. The second variant of\nCD, instead, is offered by the point–to–plane CD (PCD) [48], denoted as DPCD and defined as\nDPCD(Y, Y′) :=1\nNMX\ni=1min\nc′∈Y′((Yi−c′)·ni)2+1\nN′M′X\ni′=1min\nc∈Y((c−Y′\ni)·ni′)2,\nwhere ni,n′\niare as in Eq.(10). The PCD computes the error projections along the normal directions, thus solely\npenalizing points that “move away” from the target local plane surface. For point clouds that are sampled from\nsurfaces, this distance is better aligned with the perceived overlapping quality than the canonical CD. Analogously to\nDW\nCDdefined in Eq.(9), weighted versions of DNCD andDPCD , respectively denoted as DW\nNCD andDW\nPCD , can be\nconstructed.\nTraining procedure\nThe training pipeline of AD–SVFD is reported in detail in Algorithm 1. Hereafter, we only discuss a few relevant\naspects. Algorithm 1 features a two–stage sampling procedure over the training epochs. Firstly, since we employ\na batched stochastic optimization algorithm, we sample uniformly at random (without replacement) B–dimensional\nbatches of source shapes with the associated shape codes (line 12). Then, for each of the selected point clouds, we\nsample M–dimensional sub–clouds (line 15); we also sample a M–dimensional sub–cloud for the template anatomy\n(line 13). In all tests, we set B= 8andM= 2′000. The motivation behind point clouds resampling is two fold. On\nthe one hand, it makes the training algorithm complexity independent of the level of refinement in the data, which is\nof paramount importance if the cardinality of the original clouds is large. Indeed, the complexity of all considered\ndata attachment measures is quadratic in the number of points. On the other hand, resampling can be interpreted as\na form of data augmentation and as such it allows improving robustness. Furthermore, we remark that the trilinear\ninterpolation to compute the position–aware shape codes is repeated at every epoch (line 16), and it is also recurrently\nperformed during time integration of the diffeomorphic flow ODE.\nTo further improve model performance, when using data attachment measures that allow for a pointwise evaluation\n(such as the ones based on CD), we implement a simple adaptive sampling procedure. This explains the presence of\nthe template and source pointwise loss functions as input arguments to PointSample in lines 13 and 15, respectively.\nSpecifically, at each training epoch we sample ⌈(1−a)M⌉points uniformly at random, whereas the remaining ⌊aM⌋\npoints are retained from the previous epoch, being the ones associated to the highest loss values. In this way, we\n--- Page 17 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nAlgorithm 1 AD–SVFD model training pipeline\n1:procedure TRAIN AD SVFD( S1,···,SNs,T, E, B, M )\n▷Si:i–th source point cloud; T: template point cloud; E: # epochs; B: batch size; M: # sampled points\n2: Initialize ANN parameters Θ\n3: for all i∈ {i1,···, iNs}do\n4: Ls\ni,Lt\ni←0M,0M ▷Initialize pointwise loss functions\n5: Sample zs\ni∼ N\u0010\n0,2\nNzI\u0011\n▷Initialize shape code\n6: Lt←0M\n7: e←0\n8: while e < E do ▷Loop over epochs\n9: b,B ← 0,[ ]\n10: while b <\u0006Ns\nB\u0007\ndo ▷Loop over batches\n11: ¯B←B+ 1 ifb <(Ns%B)elseB ▷ Define batch size\n12: Sample i1,···, i¯B∼ U({1,···, Ns} \\ B)\n13: Tb←PointSample( T,Lt, M) ▷Sample template points\n14: for all i∈ {i1,···, i¯B}do\n15: Sb\ni←PointSample( Si,Ls\ni, M) ▷Sample source points\n16: ¯zi←CodeSample( zi,Sb\ni) ▷Sample shape code\n17: Sb,(K)\ni← D SV F(Sb\ni,¯zi,Θ) ▷Direct mapping\n18: Tb,(0)\ni← I SV F(Tb,¯zi,Θ) ▷Inverse mapping\n19: Ls\ni← L(Sb,(K)\ni,T) ▷Direct mapping loss\n20: Lt\ni← L(Tb,(0)\ni,Si) ▷Inverse mapping loss\n21: Ltot←1\n¯BMP¯B,M\ni,j=1(Ls\ni,j+Lt\ni,j) +Lreg ▷Total loss\n22: Θ←Update ANN( Θ,Ltot) ▷Update ANN parameters\n23: for all i∈ {i1,···, i¯B}do\n24: zi←Update Codes( zi,Ls\ni,Lt\ni) ▷Update shape codes\n25: b,B ← b+ 1,[B, i1,···, i¯B]\n26: Lt←1\nNsPNs\ni=1Lt\ni ▷Average template loss\n27: e←e+ 1\noversample regions featuring larger mapping errors, tentatively driving the model towards homogeneously accurate\npredictions in space. In all tests, we consider a= 0.15, as resulting from the calibration procedure reported in\nAppendix B.1.\nRemark 1. If the chosen data attachment measure does not allow for a pointwise evaluation, because it yields a\ncumulative discrepancy value, adaptive sampling cannot be performed. For instance, this is the case with SD. In\nAlgorithm 1, the point sampling routines no longer depend on the loss at the previous epoch (lines 13,15), and no\naveraging over the points in the clouds is necessary to compute the total loss (line 21).\nThe joint optimization of the ANN parameters Θ(line 22) and of the latent shape codes {zi}Ns\ni=1(line 24) is achieved\nby minimizing the loss function reported in Eq.(4). Notably, to limit the kinetic energy of the system that connects\nthe source to the target, thus encouraging minimal deformations and reducing the risk of overfitting, we introduce the\nregularization term Lreg:\nLreg(Θ) :=NsX\ni=1MX\nj=1K−1X\nk=0\u0010\n∥vs,(k)\ni,j(Θ)∥2\n2+∥vt,(k+1)\ni,j (Θ)∥2\n2\u0011\n, (11)\nwhere vs,(k)\ni,j,vt,(k)\ni,j are defined as in Eqs.(5),(7), respectively. For both Θand the latent codes, we perform\nrandom initialization, drawing values from a Kaiming normal distribution [72] and we rely on the first–order Adam\n--- Page 18 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\noptimizer [50] for the update step. Hyperparameter calibration tests suggested adopting the same learning rate\nλ= 10−3for all the trainable parameters. Finally, we run E= 500 training epochs, which guarantees convergence\nof the optimization procedure.\nRemark 2. During inference, a pipeline similar to Algorithm 1, but much cheaper, is performed. Indeed, the opti-\nmization problem to be solved is much smaller, since just the Nzlatent code entries associated with a single unseen\nshape have to be optimized. Remarkably, the low memory requirements enable the use of more advanced and memory–\nintensive optimizers, such as L–BFGS [51], to fine–tune Adam predictions, attaining superlinear convergence rates.\nAcknowledgements\nRT and SD were supported by the Swiss National Science Foundation , grant No 200021 197021 – “Data–driven\napproximation of hemodynamics by combined reduced order modeling and deep neural networks”.\nLP, FK, and AM were supported by the U.S. National Science Foundation , grant No. 2310909 – “Collaborative\nResearch: Frameworks: A multi-fidelity computational framework for vascular mechanobiology in SimVascular”,\nand grant No. 1663671 – “SI2-SSI Collaborative Research: The SimCardio Open Source Multi-Physics Cardiac\nModeling Package”. FK was also supported by the National Institutes of Health (R01EB029362, R01LM013120,\nR38HL143615).\nFR and SP were supported by the grant Dipartimento di Eccellenza 2023-2027 of Dipartimento di Matematica, Po-\nlitecnico di Milano , and by the project PRIN2022, MUR, Italy, 2023-2025, P2022N5ZNP , “SIDDMs: shape–informed\ndata–driven models for parametrized PDEs, with application to computational cardiology”, funded by the European\nUnion (Next Generation EU, Mission 4 Component 2). FR and SP are members of GNCS, “Gruppo Nazionale per il\nCalcolo Scientifico” (National Group for Scientific Computing) of INdAM (Istituto Nazionale di Alta Matematica).\nData availability\nThe dataset employed for the current study is publicly available at https://doi.org/10.5281/zenodo.\n15494901 . All patient–specific anatomies are publicly available on the Vascular Model Repository ( https://www.\nvascularmodel.com/dataset.html ).\nCode availability\nThe underlying code for this study is currently not available, but may be made provided to qualified researchers upon\nrequest to the corresponding author. Future publications of the software are being considered to support transparency\nand reproducibility.\nReferences\n[1] Jan Modersitzki. Numerical methods for image registration . OUP Oxford, 2003.\n[2] Jan Modersitzki. FAIR: flexible algorithms for image registration . SIAM, 2009.\n[3] John Ashburner. A fast diffeomorphic image registration algorithm. Neuroimage , 38(1):95–113, 2007.\n[4] Aristeidis Sotiras, Christos Davatzikos, and Nikos Paragios. Deformable medical image registration: a survey.\nIEEE transactions on medical imaging , 32(7):1153–1190, 2013.\n[5] Grant Haskins, Uwe Kruger, and Pingkun Yan. Deep learning in medical image registration: a survey. Machine\nVision and Applications , 31(1):8, 2020.\n[6] Tom Vercauteren, Xavier Pennec, Aymeric Perchant, and Nicholas Ayache. Diffeomorphic demons: Efficient\nnon–parametric image registration. NeuroImage , 45(1):S61–S72, 2009.\n[7] M Faisal Beg, Michael I Miller, Alain Trouv ´e, and Laurent Younes. Computing large deformation metric map-\npings via geodesic flows of diffeomorphisms. International journal of computer vision , 61:139–157, 2005.\n[8] Marc Vaillant and Joan Glaunes. Surface matching via currents. In Biennial international conference on infor-\nmation processing in medical imaging , pages 381–392. Springer, 2005.\n[9] Alexandre B ˆone, Maxime Louis, Beno ˆıt Martin, and Stanley Durrleman. Deformetrica 4: an open-source soft-\nware for statistical shape analysis. In Shape in Medical Imaging: International Workshop, ShapeMI 2018, Held\n--- Page 19 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nin Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings , pages 3–13. Springer,\n2018.\n[10] Julian Krebs, Tommaso Mansi, Boris Mailh ´e, Nicholas Ayache, and Herv ´e Delingette. Unsupervised probabilis-\ntic deformation modeling for robust diffeomorphic registration. In Deep Learning in Medical Image Analysis\nand Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th\nInternational Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September\n20, 2018, Proceedings 4 , pages 101–109. Springer, 2018.\n[11] Adrian V Dalca, Guha Balakrishnan, John Guttag, and Mert R Sabuncu. Unsupervised learning of probabilistic\ndiffeomorphic registration for images and surfaces. Medical image analysis , 57:226–236, 2019.\n[12] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. V oxelmorph: a learning\nframework for deformable medical image registration. IEEE transactions on medical imaging , 38(8):1788–1800,\n2019.\n[13] Marvin Eisenberger, Zorah Lahner, and Daniel Cremers. Smooth shells: Multi–scale shape registration with\nfunctional maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 12265–12274, 2020.\n[14] Marvin Eisenberger, David Novotny, Gael Kerchenbaum, Patrick Labatut, Natalia Neverova, Daniel Cremers,\nand Andrea Vedaldi. Neuromorph: Unsupervised shape interpolation and correspondence in one go. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7473–7483, 2021.\n[15] Boah Kim, Dong Hwan Kim, Seong Ho Park, Jieun Kim, June-Goo Lee, and Jong Chul Ye. CycleMorph: cycle\nconsistent unsupervised deformable image registration. Medical image analysis , 71:102036, 2021.\n[16] Boah Kim, Inhwa Han, and Jong Chul Ye. Diffusemorph: Unsupervised deformable image registration using\ndiffusion model. In European conference on computer vision , pages 347–364. Springer, 2022.\n[17] Junyu Chen, Eric C Frey, Yufan He, William P Segars, Ye Li, and Yong Du. Transmorph: Transformer for\nunsupervised medical image registration. Medical image analysis , 82:102615, 2022.\n[18] Jing Zou, Bingchen Gao, Youyi Song, and Jing Qin. A review of deep learning–based deformable medical image\nregistration. Frontiers in Oncology , 12:1047215, 2022.\n[19] Bailin Deng, Yuxin Yao, Roberto M Dyke, and Juyong Zhang. A survey of non–rigid 3D registration. Computer\nGraphics Forum , 41(2):559–589, 2022.\n[20] Hiba Ramadan, Dounia El Bourakadi, Ali Yahyaouy, and Hamid Tairi. Medical image registration in the era of\nTransformers: a recent review. Informatics in Medicine Unlocked , page 101540, 2024.\n[21] Jan Laufer, Peter Johnson, Edward Zhang, Bradley Treeby, Ben Cox, Barbara Pedley, and Paul Beard. In vivo\npreclinical photoacoustic imaging of tumor vasculature development and therapy. Journal of biomedical optics ,\n17(5):056016–056016, 2012.\n[22] Chuqin Huang, Yanda Cheng, Wenhan Zheng, Robert W Bing, Huijuan Zhang, Isabel Komornicki, Linda M\nHarris, Praveen R Arany, Saptarshi Chakraborty, Qifa Zhou, et al. Dual–scan photoacoustic tomography for the\nimaging of vascular structure on foot. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control ,\n70(12):1703–1713, 2023.\n[23] NT Huynh, E Zhang, O Francies, F Kuklis, T Allen, J Zhu, O Abeyakoon, F Lucka, M Betcke, J Jaros, et al. A\nfast all–optical 3D photoacoustic scanner for clinical vascular imaging. Nature Biomedical Engineering , pages\n1–18, 2024.\n[24] Gary KL Tam, Zhi-Quan Cheng, Yu-Kun Lai, Frank C Langbein, Yonghuai Liu, David Marshall, Ralph R Martin,\nXian-Fang Sun, and Paul L Rosin. Registration of 3D point clouds and meshes: A survey from rigid to nonrigid.\nIEEE transactions on visualization and computer graphics , 19(7):1199–1217, 2012.\n[25] Sara Monji-Azad, J ¨urgen Hesser, and Nikolas L ¨ow. A review of non–rigid transformations and learning–based\n3D point cloud registration methods. ISPRS journal of photogrammetry and remote sensing , 196:58–72, 2023.\n[26] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3D: Learning scene flow in 3D point clouds. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 529–537, 2019.\n[27] Balder Croquet, Daan Christiaens, Seth M Weinberg, Michael Bronstein, Dirk Vandermeulen, and Peter Claes.\nUnsupervised diffeomorphic surface registration and nonlinear modelling. In Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September\n27–October 1, 2021, Proceedings, Part IV 24 , pages 118–128. Springer, 2021.\n--- Page 20 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\n[28] Boulbaba Ben Amor, Sylvain Arguill `ere, and Ling Shao. ResNet–LDDMM: advancing the LDDMM framework\nusing deep residual networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 45(3):3707–\n3720, 2022.\n[29] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , pages 5939–5948, 2019.\n[30] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. DISN: Deep implicit surface\nnetwork for high–quality single–view 3D reconstruction. Advances in neural information processing systems ,\n32, 2019.\n[31] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: learning\ncontinuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 165–174, 2019.\n[32] Shanlin Sun, Kun Han, Deying Kong, Hao Tang, Xiangyi Yan, and Xiaohui Xie. Topology–preserving shape\nreconstruction and registration via neural diffeomorphic flow. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 20845–20855, 2022.\n[33] Fanwei Kong, Sascha Stocker, Perry S Choi, Michael Ma, Daniel B Ennis, and Alison L Marsden. SDF4CHD:\nGenerative modeling of cardiac anatomies with congenital heart defects. Medical Image Analysis , 97:103293,\n2024.\n[34] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equa-\ntions. Advances in neural information processing systems , 31, 2018.\n[35] Jian Peng, Liefeng Bo, and Jinbo Xu. Conditional neural fields. Advances in neural information processing\nsystems , 22, 2009.\n[36] Diederik P Kingma and Max Welling. Auto–encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.\n[37] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-\nhamed, and Alexander Lerchner. β–V AE: learning basic visual concepts with a constrained variational frame-\nwork. In International conference on learning representations , 2016.\n[38] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and Trends in\nMachine Learning , 12(4):307–392, 2019.\n[39] Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks. Deep generative modelling: a comparative\nreview of V AEs, GANs, normalizing flows, energy–based and autoregressive models. IEEE transactions on\npattern analysis and machine intelligence , 2021.\n[40] Andriy Myronenko and Xubo Song. Point set registration: coherent point drift. IEEE transactions on pattern\nanalysis and machine intelligence , 32(12):2262–2275, 2010.\n[41] Stanley Durrleman, Marcel Prastawa, Nicolas Charon, Julie R Korenberg, Sarang Joshi, Guido Gerig, and Alain\nTrouv ´e. Morphometry of anatomical shape complexes with dense deformations and sparse parameters. Neu-\nroImage , 101:35–49, 2014.\n[42] Pietro Gori, Olivier Colliot, Linda Marrakchi-Kacem, Yulia Worbe, Cyril Poupon, Andreas Hartmann, Nicholas\nAyache, and Stanley Durrleman. A Bayesian framework for joint morphometry of surface and curve meshes in\nmulti–object complexes. Medical image analysis , 35:458–474, 2017.\n[43] Jiancheng Yang, Udaranga Wickramasinghe, Bingbing Ni, and Pascal Fua. ImplicitAtlas: learning deformable\nshape templates in medical imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 15861–15871, 2022.\n[44] Monica Hernandez, Matias N Bossa, and Salvador Olmos. Registration of anatomical images using paths of\ndiffeomorphisms parameterized with stationary vector field flows. International Journal of Computer Vision , 85:\n291–306, 2009.\n[45] Qiang Ma, Liu Li, Emma C Robinson, Bernhard Kainz, Daniel Rueckert, and Amir Alansary. CortexODE:\nLearning cortical surface reconstruction by neural ODEs. IEEE Transactions on Medical Imaging , 42(2):430–\n443, 2022.\n[46] Yusuf Sahillio ˘glu. Recent advances in shape correspondence. The Visual Computer , 36(8):1705–1721, 2020.\n[47] Jean Feydy, Benjamin Charlier, Franc ¸ois-Xavier Vialard, and Gabriel Peyr ´e. Optimal transport for diffeomorphic\nregistration. In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th Interna-\ntional Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20 , pages 291–299.\nSpringer, 2017.\n--- Page 21 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\n[48] Dong Tian, Hideaki Ochimizu, Chen Feng, Robert Cohen, and Anthony Vetro. Geometric distortion metrics for\npoint cloud compression. In 2017 IEEE International Conference on Image Processing (ICIP) , pages 3460–3464.\nIEEE, 2017.\n[49] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information\nprocessing systems , 26, 2013.\n[50] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[51] L ´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large–scale machine learning. SIAM\nreview , 60(2):223–311, 2018.\n[52] Adam Updegrove, Nathan M Wilson, Jameson Merkow, Hongzhi Lan, Alison L Marsden, and Shawn C Shadden.\nSimVascular: an open source pipeline for cardiovascular simulation. Annals of biomedical engineering , 45:525–\n541, 2017.\n[53] Nathan M Wilson, Ana K Ortiz, and Allison B Johnson. The vascular model repository: a public resource of\nmedical imaging data and blood flow simulation results. Journal of medical devices , 7(4), 2013.\n[54] Jean Duchon. Splines minimizing rotation–invariant semi–norms in Sobolev spaces. In Constructive Theory of\nFunctions of Several Variables: Proceedings of a Conference Held at Oberwolfach April 25–May 1, 1976 , pages\n85–100. Springer, 1977.\n[55] James Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal ´azs K ´egl. Algorithms for hyper–parameter optimization.\nAdvances in neural information processing systems , 24, 2011.\n[56] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization\nin hundreds of dimensions for vision architectures. In International conference on machine learning , pages 115–\n123. PMLR, 2013.\n[57] Amir Zadeh, Yao-Chong Lim, Paul Pu Liang, and Louis-Philippe Morency. Variational auto–decoder: A method\nfor neural generative modeling from incomplete data. arXiv preprint arXiv:1903.00840 , 2019.\n[58] Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. Surrogate modeling for fluid flows based on physics–\nconstrained deep learning without simulation data. Computer Methods in Applied Mechanics and Engineering ,\n361:112732, 2020.\n[59] Jan Oldenburg, Finja Borowski, Alper ¨Oner, Klaus-Peter Schmitz, and Michael Stiehm. Geometry–aware\nphysics–informed neural network surrogate for solving Navier–Stokes equation (GAPINN). Advanced Mod-\neling and Simulation in Engineering Sciences , 9(1):8, 2022.\n[60] Francesco Regazzoni, Stefano Pagani, and Alfio Quarteroni. Universal Solution Manifold Networks (USM–\nNets): non–intrusive mesh–free surrogate models for problems in variable domains. Journal of Biomechanical\nEngineering , 144(12):121004, 2022.\n[61] Francisco Sahli Costabal, Simone Pezzuto, and Paris Perdikaris. ∆–PINNs: Physics–informed neural networks\non complex geometries. Engineering Applications of Artificial Intelligence , 127:107324, 2024.\n[62] Simone Brivio, Stefania Fresca, and Andrea Manzoni. Handling geometrical variability in nonlinear reduced\norder modeling through continuous geometry–aware DL-ROMs. Computer Methods in Applied Mechanics and\nEngineering , 442:117989, 2025.\n[63] Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, and Hao Zhang. UNIST:\nunpaired neural implicit shape translation network. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 18614–18622, 2022.\n[64] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,\nRavi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions\nin low–dimensional domains. Advances in Neural Information Processing Systems , 33:7537–7547, 2020.\n[65] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and\nAaron Courville. On the spectral bias of neural networks. In International conference on machine learning ,\npages 5301–5310. PMLR, 2019.\n[66] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for image retrieval.\nInternational journal of computer vision , 40:99–121, 2000.\n[67] Atul Kumar Sinha and Franc ¸ois Fleuret. DeepEMD: A transformer–based fast estimation of the earth mover’s\ndistance. In International Conference on Pattern Recognition , pages 1–15. Springer, 2024.\n--- Page 22 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\n[68] Lenaic Chizat, Pierre Roussillon, Flavien L ´eger, Franc ¸ois-Xavier Vialard, and Gabriel Peyr ´e. Faster Wasserstein\ndistance estimation with the Sinkhorn divergence. Advances in Neural Information Processing Systems , 33:\n2257–2269, 2020.\n[69] Lucas De Lara, Alberto Gonz ´alez-Sanz, and Jean-Michel Loubes. Diffeomorphic registration using Sinkhorn\ndivergences. SIAM Journal on Imaging Sciences , 16(1):250–279, 2023.\n[70] Jean Feydy, Thibault S ´ejourn ´e, Franc ¸ois-Xavier Vialard, Shunichi Amari, Alain Trouv ´e, and Gabriel Peyr ´e. Inter-\npolating between optimal transport and mmd using Sinkhorn divergences. In The 22nd international conference\non artificial intelligence and statistics , pages 2681–2690. PMLR, 2019.\n[71] Tong Wu, Liang Pan, Junzhe Zhang, Tai Wang, Ziwei Liu, and Dahua Lin. Density–aware chamfer distance as a\ncomprehensive metric for point cloud completion. arXiv preprint arXiv:2111.12702 , 2021.\n[72] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human–\nlevel performance on ImageNet classification. In Proceedings of the IEEE international conference on computer\nvision , pages 1026–1034, 2015.\n[73] Richard L Bishop. There is more than one way to frame a curve. The American Mathematical Monthly , 82(3):\n246–251, 1975.\n[74] Mehran Ebrahimi, Adrian Butscher, and Hyunmin Cheong. A low order, torsion deformable spatial beam element\nbased on the absolute nodal coordinate formulation and Bishop frame. Multibody System Dynamics , 51(3):247–\n278, 2021.\n[75] Paul J Besl and Neil D McKay. Method for registration of 3D shapes. In Sensor fusion IV: control paradigms\nand data structures , volume 1611, pages 586–606. Spie, 1992.\n[76] Andrew W Fitzgibbon. Robust registration of 2D and 3D point sets. Image and vision computing , 21(13-14):\n1145–1153, 2003.\n[77] Steven Gold, Anand Rangarajan, Chien-Ping Lu, Suguna Pappu, and Eric Mjolsness. New algorithms for 2D\nand 3D point matching: pose estimation and correspondence. Pattern recognition , 31(8):1019–1031, 1998.\n[78] Haili Chui and Anand Rangarajan. A new algorithm for non–rigid point matching. In Proceedings IEEE Con-\nference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662) , volume 2, pages 44–51.\nIEEE, 2000.\n--- Page 23 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nA Data augmentation procedure\nSince the available dataset of healthy aortic anatomies is too restrained for seep learning (DL) applications, we imple-\nmented a data augmentation algorithm based on radial basis functions interpolation. More specifically, we considered\nthin–plate splines (TPS), a special case of polyharmonic splines introduced in [54], that admits a natural radial basis\nfunction representation via the infinite–support kernel function κ(x) =x2logx.\nA.1 Deformable registration by TPS interpolation\nLet us consider a pair of geometries (Gα, Gβ). Let us suppose to know M∈Nexact point–to–point correspondences\n{(xj\nα,xj\nβ)}M\nj=1. Then, TPS interpolation finds a diffeomorphism that deforms GαintoGβby solving the following\nenergy minimization problem:\n⃗ g⋆= arg min\n⃗ g∈GMX\nj=1∥xj\nβ−⃗ g(xj\nα)∥2\n2+wH∥Hg(xj\nα)∥2\nF,\nwhere G:=n\n⃗ g:R3→R3:⃗ g(⃗ x) =MX\nj=1gjκ\u0000\n∥⃗ x−xj\nα∥2\u0001o\n.(12)\nHere, Hg:R3→R3×3denotes the Hessian of g, and∥·∥F:R3→R+is the Frobenius norm operator. The\nsmoothing parameter wH∈R+allows to balance the goodness of fit with the regularity of the deformation. The most\nrelevant limiting factor to the use of TPS interpolation is the availability of reliable point–to–point correspondences.\nIn this work, we identify corresponding points by exploiting the peculiar structure of the geometries at hand. Indeed,\nvascular anatomies consist of the intersection of several vessels, each one featuring a tube–like shape. In SimVascu-\nlar[52], vessels are conveniently modelled by their centerline, which is approximated by a trivariate cubic spline, and\nby a number of surface contours, planar closed lines that define the cross–sectional vessel lumen boundary at selected\nlocations along the centerline. Even if SimVascular allows to accurately describe surface contours using B–splines,\nwe relied on a much simpler approximation, supposing the cross–sectional areas to be circular and centered at the cen-\nterline points. Furthermore, to derive more precise point–to–point correspondences, we partitioned some of the vessel\ninto chunks, which are defined depending on the location of eventual branches. Indeed, as displayed in Figure 2 (b) in\nthe manuscript, each anatomy in the dataset features Nv= 5vessels (aorta, LSA, RSA, RCCA, LCCA), but Np= 7\nvessel portions (AA, DA, BA, LSA, RSA, LCCA, RCCA). Now, let Mp∈Nbe the total number of points sampled\nin each vessel portion, and let Mc∈Nbe the number of points sampled at each contour. In this work, we consider\nMp= 250 andMc= 4. For a given vessel portion pofGα, the sampled points {xj\nα,p}are structured as follows:\n•Mp/(Mc+ 1) points are uniformly distributed along the centerline;\n•(McMp)/(Mc+ 1) points are uniformly distributed along the (approximated) circular contours, correspond-\ning to each sampled centerline point.\nThe final set of sampled points is then given by Xα=SNp\nℓ=1{xj\nα,pℓ}Mp\nj=1. The same sampling strategy is used to define\nthe set of sampled points XβforGβ.\nRemark. For every point in a child branch, we compute the convex hull generated by its 1,000nearest neighbours\nin the parent vessel. If the point belongs to the convex hull, it means that it lies inside the parent vessel and so it is\nremoved from the dataset. Additionally, also the points that lie outside of the convex hull by a distance smaller than\nτDpare discarded, where Dp∈R+is the maximal distance between two points in the child branch and τ∈R+is\na prescribed threshold. This helps in guaranteeing the well–posedness of the TPS interpolation problem. Ultimately,\nthe total number of points sampled at a vessel portion is∼Mp≤Mp. If a point is removed from Xα, the corresponding\none is removed from Xβ, and viceversa.\nWhile the correspondences quality for the centerline points is often remarkable, the same consideration does not\nhold for the ones sampled on the lateral surface. Indeed, since the centerline is an open curve, the knowledge of\nthe curvilinear coordinates alone is sufficient to derive solid correspondences. However, the surface contours are\nplanar closed curves; this entails that reliably corresponding samples can be selected only upon convenient choices\nof two–dimensional reference frames. In fact, the selection of matching surface samples closely depends on the\nidentification of topologically equivalent zero–degree angles in the cross–sectional planes. To this aim, we employ the\nBishop frame of reference [73, 74], a coordinates system for curves, which is defined by transporting a given reference\n--- Page 24 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 4: TPS deformable registration errors. In particular, we report the average and maximal pointwise errors for\nthe registration of two of the patients in the dataset to the template and the average errors over all patients. Patient\nP#091 serves as reference and it is not considered in the average errors calculation. In all cases, we compare the\nresults obtained without and with a preliminary rigid registration of the geometries to the template by the Coherent\nPoint Drift algorithm. The errors are quantified through the forward and backward local distances (FLD and BLD),\nexpressed in cm. For reference, the template inlet diameter is 1.31 cm forP#091 .\nMax Errors Avg Errors\nFCD BCD FCD BCD\nWithout\nRigid\nAlignmentP#090 0.2918 0.0777 0.2615 0.0770\nP#272 0.5760 0.0670 0.3428 0.0641\nAverage 0.7304 0.0861 0.4480 0.0759\nWith\nRigid\nAlignmentP#090 0.2786 0.0736 0.2349 0.0739\nP#272 0.4667 0.0759 0.3002 0.0744\nAverage 0.6211 0.0825 0.3908 0.0756\nframe (forward and/or backward) along the curve itself. Two peculiarities of the Bishop frame are noteworthy. Firstly,\none of its vectors always coincides with the curve tangent. Secondly, the coordinates system exhibits a uniform zero\ntwist along the curve. Therefore, if we are able to define equivalent reference frames for the cross–sectional planes\nlocated at two corresponding centerline points, then such frames can be robustly “extended” to the whole vessel. In\nthis work, the equivalent reference frames have been derived using ad hoc techniques, based on the relative positions\nof inlets and outlets. For instance, the vector that defines the zero–degree angle at the aorta’s inlet contour is computed\nas the orthogonal projection of the vector that connects the aorta’s centerline endpoints.\nIn order to guarantee the quality of point–to–point correspondences, an initial rigid alignment of the geometries is\ncrucial. To this aim, we employ the Coherent Point Drift (CPD) algorithm, a point set registration method based\non Gaussian Mixture Models [40]. Compared to the most popular Iterative Closest Point (ICP) algorithm [75] and\nto its most widely employed variants and alternatives (such as Levenberg–Marquardt ICP [76] or Robust Point\nMatching [77, 78]), CPD proved to be more accurate and robust in presence of noise, outliers and missing points.\nNonetheless, CPD is an iterative algorithm, and hence its accuracy is strictly linked to the choice of a good initial\nguess. For this reason, prior to the execution of CPD, we perform the following three–steps ad hoc rigid registration\nand rescaling procedure, as reported at line 6 in Algorithm 2.\nLetSα,Sβbe two point clouds, computed from the surface meshes Mα,Mβ. Furthermore, let us suppose to know the\nposition of the aorta’s inlet center and the normal vector to the aorta’s outlet, for both geometries. Then, we proceed\nas follows:\n1.Rescaling : translate Sα, so that its barycenter coincides with the one of Sβ. Perform an isotropic rescaling of\nSα, so that the maximal distance between points in Sαequals the one in Sβ. We call the output S(1)\nα.\n2.Translation : translate S(1)\nα, so that its aorta’s inlet center coincides with the one of Sβ. We call the output\nS(2)\nα.\n3.Rotation : rotate S(2)\nαat the aorta’s inlet center around the aorta’s outlet normal vector by the angle ϑthat\nminimizes the Chamfer Distance (CD) between SβandS(2)\nα. We call the output ¯Sα, which serves as the\ninitial guess for the CPD iterations.\nTable 4 reports the results of the TPS interpolation algorithm, with and without prior rigid registration, obtained on\ntwo of the patients in the dataset ( P#090 ,P#272 ) and averaged over all the shapes in the dataset (see Figure 2 in\nthe manuscript), except from P#091 , that serves as reference. The pointwise registration errors, computed at all the\ncell centers of the available surface triangulations, are quantifies through the forward and backward local distances\n(FLD and BLD), expressed in cm. The former identifies the distance of each point in the mapped geometry from the\n--- Page 25 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nFigure 7: Visualization of the TPS interpolation results. In particular, for two patients in the dataset (P#090 and\nP#272) we show the locations of the interpolation points in the target and template geometries — color–coded so\nthat corresponding points share the same value — and the pointwise mapping errors, quantified through the forward\nlocal distance (FLD), expressed in cm. For the mapping results, we compare the errors obtained without and with a\npreliminary rigid registration of the geometries to the template by the Coherent Point Drift algorithm. For reference,\nthe template inlet diameter is 1.31 cm forP#091 .\nclosest one in the target, while the latter is the distance of each point in the target from the closest one in the mapped\ngeometry. Figure 7 offers a visualization of the results, showing the positions of the interpolation points and the\npointwise FLD values. A few considerations deserve attention. Firstly, TPS interpolation attains a notable degree of\naccuracy, with FLDs that are always well below the 1 cm threshold. Secondly, preliminary rigid registration is crucial\nwhen the original orientation of the target geometry differs from the reference one. This is showcased by patient\nP#272 ; indeed, the geometry obtained upon TPS interpolation without rigid registration is extremely irregular and\nconvoluted, particularly in the aortic arch. Finally, we underline that TPS interpolation is rather sensitive to the values\nof (i) the smoothing parameter wHin Eq.(12), and ( ii) the tolerance τ. The calibration of the latter is particularly\nimportant to obtain good quality results. In the reported tests, we select τ= 5·10−3forP#090 , and τ= 2.5·10−3\nforP#272 . However, to compute the aggregate metrics in Table 4, we set τ= 5·10−3for all the geometries; this\njustifies why average errors are larger than patient–specific ones.\nA.2 Data augmentation pipeline\nThe proposed TPS interpolation algorithm can provide good quality mapping results at a contained computational\ncost. However, robustness is a major drawback. Indeed, undesired artifacts are often introduced for too small values\nofwH(and for inadequate choices for τ), while large values of wHnegatively impact the overall goodness of fit. For\nthis reason, we do not use TPS interpolation to solve the deformable registration problem on the vascular anatomies\nat hand, but we nonetheless exploit it for data augmentation.\n--- Page 26 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nAlgorithm 2 TPS–based data augmentation\n1:procedure AUGMENT DATASET (N,M1, . . . ,MG,X1, . . . , X G)\n▷N: number of geometries; Mk: k–th mesh;\nXk: k–th set of sampling points\n2: n←0\n3: D ← [M1, . . . ,MG] ▷Initialize the dataset\n4: while n < N do\n5: Sample α, β∼ U({1, . . . , G }), α̸=β ▷ Select two shapes\n6: Ad hoc rigid registration of MαtoMβ\n7: CPD–based rigid registration of MαtoMβ\n8: Sample L∼ U({1,2}) ▷Select number of vessel portions\n9: Sample Lvessel portions p1, . . . , p L ▷Select vessel portions\n10: Sample Cℓ∼ U([0.5,1]),ℓ∈ {1, . . . , L } ▷Select matching factors\n11: ˜Xα← {Xα,pℓ}L\nℓ=1 ▷Interpolation points\n12: ˜Xβ← {{ (1−Cℓ)Xα,pℓ+CℓXβ,pℓ}}L\nℓ=1 ▷Interpolation field\n13: I ← TPS interpolator( ˜Xα,˜Xβ)\n14: M′← I(Mα) ▷Query the interpolator\n15: ifquality( M′)is good then ▷Check mesh quality\n16: D ← [D,M′] ▷Update dataset\n17: n←n+ 1\nreturn D\nOur TPS–based data augmentation pipeline is reported in Algorithm 2. The procedure involves the evaluation of\n“partial” TPS interpolators, where the word “partial” refers to the fact that only points from a subset of randomly\nselected vessel portions are considered. More specifically, at each iteration, we choose a random pair of geometries\nfrom the source cohort (line 5), whose corresponding sampled point sets are Xα,Xβ, and a random number of vessel\nportions L∈ {1,2}(line 8). Firstly, we rigidly deform the points in Xαthat belong to the selected vessel portions;\nthis leads to the definition of ¯Xα=SL\nℓ=1{¯Xα,pℓ}(lines 6,7). Then, the interpolation values are computed as follows\n(line 12):\n˜Xβ:=L[\nℓ=1\b\n(1−Cℓ)¯Xα,pℓ+CℓXβ,pℓ\t\n,with Cℓ∼ U([0.5,1]).\nHence, the points selected from Xαare not mapped to the corresponding ones in Xβ, but to some intermediate\nlocations along the connecting segments, whose precise position depends on the random matching factors Cℓ. The\nderived TPS interpolator is used to deform the surface mesh Mαof the first shape, so that a new triangulation M′\nis generated (lines 13,14). Finally, the resulting geometry is added to the dataset if the quality of the associated\nsurface mesh is sufficiently high (line 15). Specifically, we require the scaled Jacobian — the determinant of the\nJacobian divided by the product of the two longest edges — to be strictly positive for all cells and to have a bottom\ndecile average value greater than 0.1. From a qualitative point of view, this choice allows to obtain “trustworthy”\ngeometries that do not feature undesired artifacts and irregularities. Incidentally, we remark that the obtained\nsurface meshes are not used to perform numerical simulations, but only serve as a tool for shape discretization.\nTherefore, it is not necessary to require a high level of regularity, and we can accept the presence of a few bad elements.\nFigure 2 (c) in the manuscript displays some of the shapes obtained by deforming the anatomies of four different\npatients with the proposed data augmentation pipeline. Despite being relatively simple, we remark the ability of\nthe method to generate rather diverse shapes. Using Algorithm 2, we created 50new geometries from each of the\n“original” anatomies, hence assembling a dataset comprising 1,020shapes. However, the final dataset used to train\nand test the AD–SVFD model only counts 902geometries ( 88.4%). The remaining 118ones have been manually\nremoved, since they were showing artifacts that could not be captured with the implemented mesh quality check.\n--- Page 27 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nB Hyperparameters tuning\nIn this section, we focus on the calibration of the most relevant hyperparameters of the AD–SVFD model.\nB.1 ANN hyperparameters tuning\nAt first, we fine–tune the hyperparameters that are not related to the implicit neural representation of the source\nshapes. Since the latent codes are not involved in the calibration procedure, we can consider the case of a single\nshape–to–shape registration. This choice allows to dramatically lighten and speedup the training (from ≈8 hto\n≈5 min ), hence enabling an exhaustive exploration of the hyperparameters’ space at affordable computational costs.\nWe consider ten hyperparameters, namely: the activation function, width and depth of FA–NN ( ϕFA,WFA,LFA)\nand DF–NN ( ϕDF,WDF,LDF), the refinement level of the FPE ( Ne), the penalty term wv, the learning rate\nλ=λΘ, and the adaptive sampling factor a. To limit the number of trainings and yet retain an extensive coverage\nof the hyperparameters’ space, we run the Tree–structured Parzen Estimator (TPE) Bayesian algorithm [55] for five\ndifferent shapes ( P#090 ,P#144 ,P#188 ,P#207 ,P#272 ) Considering quantized values, the total number of possible\nhyperparameters combinations is 8.64M. However, adopting TPE, we only perform 500trainings for each target\nshape; hence the overall duration of the fine–tuning procedure sets to ≈30 h per patient.\nIn order to identify a common (sub–)optimal set of hyperparameters, we marginalize the results of the five TPE\nruns with respect to the hyperparameter values. Firstly, for every patient, we associate every model with a score\ns∈R+, computed by averaging the mean forward and backward local distances associated with the direct and\ninverse mapping. To balance the contributions of the five patients, we normalize the model score sby the best (i.e.\nthe lowest) score s∗; this defines the normalized score ˜s. Then, for each patient, every hyperparameter value is\nassociated with the bottom decile average with respect to ˜s, computed considering all the trained models that feature\nsuch value. Finally, for each hyperparameter value, we compute an aggregate performance score S∈R+by av-\neraging the bottom decile averages obtained on the five patients. Table 5 reports the results of the calibration procedure.\nEven though the set of hyperparameters reported in Table 5 features (sub–)optimal properties for single shape–to–\nshape registration, those are not guaranteed to automatically transfer to the “complete” AD–SVFD model. In fact,\nwith this configuration, the training of AD–SVFD fails, since all shape codes converge to the zero vector, leading to\nlarge errors. Empirically, we found that the problem is related to vanishing gradient issues in the trainable feature\naugmentation model compartment FA–NN. To circumvent this pitfall, we changed the FA–NN activation function\nϕFAfrom ReLU toleaky–ReLU (with negative slope equal to 0.2); this allowed to retain remarkable accuracy levels\neven in the multiple–shape scenario.\nB.2 Shape code hyperparameters tuning\nWe focus on the calibration of two hyperparameters related to the shape codes, namely the regularization factor wz(see\nEq.(4)) and the learning rate λz. Table 6 reports the maximal pointwise errors — quantified through the forward and\nbackward local distances FLD and BLD, in cm— corresponding to different choices of wz(forλz= 10−3) and λz\n(forwz= 10−3). Concerning the regularization parameter, the results show little sensitivity, provided that sufficiently\nsmall values are considered. Indeed, all the models featuring wz≤10−3yield similar results, but accuracy deteriorates\nfor larger values. Conversely, the quality of the results heavily depends on the choice of the learning rate λz. Indeed,\nsensibly larger errors are obtained when either too small (e.g. λz= 10−4) or too large (e.g. λz= 10−2) values are\nselected. Ultimately, based on the obtained results, we set wz= 10−3andλz= 10−3. Notably, we choose wzas\nlarge as possible, in order to maximally regularize the latent space without compromising the registration accuracy.\n--- Page 28 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 5: Results of the AD–SVFD hyperparameters calibration procedure. To save computational resources, we\nworked in a single shape–to–shape registration scenario and employed the Tree–structured Parzen Estimator al-\ngorithm, considering five different shapes. We refer to the text for a detailed definition of each hyperparameter. Every\nhyperparameter value is associated with the aggregate performance score S, computed from the average pointwise\nforward and backward local distances related to the direct and inverse mappings. Low values of Scorrespond to\naccurate models. The optimal hyperparameter choices are marked in green. The yellow cells denote the hyperpa-\nrameter values that were changed when incorporating the shape codes, for the simultaneous registration of multiple\nshapes. Notation: l–ReLU stands for leaky–ReLU , with a negative slope equal to 0.2.\nParameter\nϕFA ReLU l−ReLU ELU SELU\n1.0425 1.0655 1.0586 1.0573\nϕDF ReLU l−ReLU ELU SELU\n1.0632 1.0488 1.0578 1.0522\nWFA 2324252627\n1.0502 1.0548 1.0531 1.0481 1.0758\nWDF 26272829\n1.0677 1.0489 1.0413 1.0715\nLFA 0 1 2 3 4\n1.0632 1.0627 1.0557 1.0474 1.0629\nLDF 4 5 6 7 8\n1.0489 1.0444 1.0504 1.0709 1.0717\nNe 0 1 2 3 4 5\n1.0532 1.0639 1.0515 1.0336 1.0393 1.0604\nwv 10−610−510−410−310−2\n1.0579 1.0371 1.0554 1.0654 1.0815\nλΘ 10−410−3.510−310−2.510−2\n1.1698 1.0671 1.0272 1.0565 1.1083\na 0.00 0 .05 0 .10 0.15 0 .20 0.25\n1.0830 1.0535 1.0542 1.0404 1.0737 1.0738\n--- Page 29 ---\nR.Tenderini et al. Deformable registration and generative modelling of aortic anatomies by ADs and NODEs\nTable 6: Registration results of AD–SVFD considering different values of the regularization parameter wzand of\nthe shape codes learning rate λz. In particular, we report the maximal pointwise errors on training and testing dat-\napoints, obtained for six different values of wzand for five different values of λz. The errors are quantified through\nthe forward and backward local distances (FLD and BLD), expressed in cm. The best value for each performance\nmetric is marked in green. For reference, the template shape inlet diameter is 1.31 cm , while the average inlet diam-\neter in the dataset is 1.45 cm .\nTrain errors (in cm) Test errors (in cm)\nDirect Inverse Direct Inverse\nwz FLD BLD FLD BLD FLD BLD FLD BLD\n0.0 0.2095 0.2201 0.2583 0.2257 0.2725 0.1934 0.2422 0.2860\n10−50.2333 0.2308 0.2834 0.2398 0.2714 0.2092 0.2564 0.2905\n10−40.2166 0.2207 0.2719 0.2259 0.2853 0.2238 0.2815 0.3187\n10−30.2162 0.2175 0.2686 0.2297 0.2777 0.2253 0.2562 0.2642\n10−20.2413 0.2353 0.3078 0.2408 0.3149 0.2751 0.3099 0.3713\n10−10.3019 0.2794 0.3855 0.3020 0.4587 0.3641 0.4294 0.6546\nλz FLD BLD FLD BLD FLD BLD FLD BLD\n10−40.2557 0.2404 0.3107 0.2680 0.4529 0.2679 0.3105 0.4771\n5·10−40.2072 0.2176 0.2665 0.2176 0.2783 0.2144 0.2671 0.3117\n10−30.2162 0.2175 0.2686 0.2297 0.2777 0.2253 0.2562 0.2642\n5·10−30.2574 0.2444 0.2938 0.2572 0.3556 0.2264 0.2893 0.3202\n10−21.1374 2.1948 2.2775 1.1860 1.5228 1.8003 1.8297 1.5442",
  "text_length": 106980
}