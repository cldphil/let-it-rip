{
  "id": "http://arxiv.org/abs/2506.05231v1",
  "title": "Progressive Tempering Sampler with Diffusion",
  "summary": "Recent research has focused on designing neural samplers that amortize the\nprocess of sampling from unnormalized densities. However, despite significant\nadvancements, they still fall short of the state-of-the-art MCMC approach,\nParallel Tempering (PT), when it comes to the efficiency of target evaluations.\nOn the other hand, unlike a well-trained neural sampler, PT yields only\ndependent samples and needs to be rerun -- at considerable computational cost\n-- whenever new samples are required. To address these weaknesses, we propose\nthe Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion\nmodels sequentially across temperatures, leveraging the advantages of PT to\nimprove the training of neural samplers. We also introduce a novel method to\ncombine high-temperature diffusion models to generate approximate\nlower-temperature samples, which are minimally refined using MCMC and used to\ntrain the next diffusion model. PTSD enables efficient reuse of sample\ninformation across temperature levels while generating well-mixed, uncorrelated\nsamples. Our method significantly improves target evaluation efficiency,\noutperforming diffusion-based neural samplers.",
  "authors": [
    "Severi Rissanen",
    "RuiKang OuYang",
    "Jiajun He",
    "Wenlin Chen",
    "Markus Heinonen",
    "Arno Solin",
    "José Miguel Hernández-Lobato"
  ],
  "published": "2025-06-05T16:46:04Z",
  "updated": "2025-06-05T16:46:04Z",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05231v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05231v1  [cs.LG]  5 Jun 2025Progressive Tempering Sampler with Diffusion\nSeveri Rissanen* 1RuiKang OuYang* 2Jiajun He2Wenlin Chen2 3Markus Heinonen1Arno Solin1\nJos´e Miguel Hern ´andez-Lobato2\nAbstract\nRecent research has focused on designing neu-\nral samplers that amortize the process of sam-\npling from unnormalized densities. However, de-\nspite significant advancements, they still fall short\nof the state-of-the-art MCMC approach, Parallel\nTempering (PT), when it comes to the efficiency\nof target evaluations. On the other hand, unlike a\nwell-trained neural sampler, PT yields only depen-\ndent samples and needs to be rerun—at consider-\nable computational cost—whenever new samples\nare required. To address these weaknesses, we\npropose the Progressive Tempering Sampler with\nDiffusion (PTSD), which trains diffusion models\nsequentially across temperatures, leveraging the\nadvantages of PT to improve the training of neu-\nral samplers. We also introduce a novel method\nto combine high-temperature diffusion models to\ngenerate approximate lower-temperature samples,\nwhich are minimally refined using MCMC and\nused to train the next diffusion model. PTSD en-\nables efficient reuse of sample information across\ntemperature levels while generating well-mixed,\nuncorrelated samples. Our method significantly\nimproves target evaluation efficiency, outperform-\ning diffusion-based neural samplers.\n1. Introduction\nSampling from probability distributions is a fundamental\nproblem in many fields of science, including Bayesian infer-\nence (Gelman et al., 2013; Welling & Teh, 2011), statistical\nphysics (V on Toussaint, 2011) and molecular simulations\n*Equal contribution1Department of Computer Science,\nAalto University, Finland2Department of Engineering, Univer-\nsity of Cambridge, United Kingdom3Department of Empir-\nical Inference, Max Planck Institute for Intelligent Systems,\nT¨ubingen, Germany. Correspondence to: Severi Rissanen <sev-\neri.rissanen@aalto.fi >, RuiKang OuYang <ro352@cam.ac.uk >,\nJiajun He <jh2383@cam.ac.uk >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).10610710810910101011456789\niDEMBNEM\nDDS\nCMCD\nPT+DM\n(baseline)\nPTSD\n(ours)Improved error and eval. count\n←Target evaluation count←Sample error ( W2)\nFigure 1. Sample error ( W2distance) and target evaluation times\nfor several diffusion (and control)-based neural samplers on Many-\nWell-32 target, including DDS, iDEM, BNEM, and CMCD, with\nour proposed approach. We include the results obtained by first\nrunning PT and fit a diffusion model post hoc for comparison.\n(No´e et al., 2019). We aim to draw independent samples\nfrom a probability distribution with a density:\np(x) =˜p(x)\nZs.t.Z=Z\n˜p(x) dx, (1)\nwhere we only assume access to the unnormalized density\nfunction ˜p(x)without any ground-truth observations.\nThe classical way to sample from the target density pis\nMarkov chain Monte Carlo (MCMC), where we design a\nMarkov chain whose invariant density is p, with the state-\nof-the-art method being Parallel Tempering (PT, Swend-\nsen & Wang, 1986; Geyer, 1991; Hukushima & Nemoto,\n1996), also known as replica exchange. The key to PT’s\nsuccess lies in running parallel MCMC chains at Ktem-\nperatures T1, . . . , T K, each corresponding to a tempering\nversion pTk∝p1/TKof the original target. The chains at\nhigher temperatures are easier to traverse across the entire\nsupport, and samples in those chains are swapped, facili-\ntating better mixing in low temperatures (Woodard et al.,\n2009). We also note that other choices of path exist, for\nexample, geometric interpolant path pk∝pβkp1−βk\n0 with a\ntractable reference p0andβkranging from 0 to 1, or other\n1\n--- Page 2 ---\nProgressive Tempering Sampler with Diffusion\nmore flexible designs (Syed et al., 2021; Surjanovic et al.,\n2022). In this paper, we follow the convention in most\npractical applications to use the annealing path.\nHowever, despite significant advancements in PT, obtaining\na new, independent sample requires an independent sam-\nple from the highest temperature to be propagated to the\nlowest temperature (Surjanovic et al., 2024). Consequently,\nwhenever new samples are needed, we need to run PT until\nuncorrelated samples have been obtained at the highest tem-\nperature and transferred to the target, which might require\nconsiderable cost. Therefore, a growing trend in research\nhas focused on learned neural samplers, which aim to amor-\ntizethe sampling process and generate uncorrelated samples\ndirectly. Early approaches involved fitting normalizing flows\nto data generated by MCMC (No ´e et al., 2019), while more\nrecent efforts typically focus on training generative models\ndirectly using only the target unnormalized density. Of par-\nticular recent interest are diffusion-based neural samplers,\ndriven by the remarkable success of diffusion models (Ho\net al., 2020; Song et al., 2021) in generation tasks. How-\never, a key factor behind the success of diffusion models in\nthese fields lies in their simple and stable denoising score\nmatching (DSM) objective (Vincent, 2011), which relies\non access to ground truth data from the target distribution.\nAs a result, while diffusion models demonstrate impressive\nperformance in generation tasks, translating this success\nto the problem of learning neural samplers directly from\nunnormalized densities remains challenging.\nTo illustrate this challenge, in Fig. 1, we compare several\nrecent diffusion and control-based neural samplers, includ-\ning DDS (Vargas et al., 2023), iDEM (Akhound-Sadegh\net al., 2024), BNEM (OuYang et al., 2024), and CMCD\n(Vargas et al., 2024), on the Many-Well-32 target (Midg-\nley et al., 2023), alongside the results by running PT and\nfitting a diffusion model post hoc to PT-generated data.\nSadly, while showing promising sample qualities, these ap-\nproaches present significantly lower efficiency compared to\ndirectly fitting the diffusion to PT results. This inefficiency\narises either from running importance sampling (IS) or an-\nnealed importance sampling (AIS) to estimate the objectives\n(Akhound-Sadegh et al., 2024; OuYang et al., 2024), or from\nthe extensive number of target evaluations in simulating the\ntrajectory (Vargas et al., 2023; 2024) due to the prevalent\nLangevin preconditioning in the network parameterization\n(He et al., 2025).\nDoes this imply that using PT followed by fitting a diffusion\nmodel is the ultimate solution for neural samplers? We posit\nthat this is not the case. In fact, the methods shown in Fig. 1\nrepresent two ends of the methodological spectrum. On one\nend, approaches such as DDS, iDEM, BNEM, CMCD, aim\nto train neural samplers without utilizing any data, while\non the other, methods depend exclusively on data generatedthrough PT and defer model fitting to the final stage.\nTherefore, a natural question is to integrate PT and neural\nsamplers—positioned in the middle of the methodological\nspectrum—leveraging the strengths of both to build more\nefficient sampling approaches. In this paper, we formalize\none attempt towards this direction. Concretely, we propose\nProgressive Tempering Sampler with Diffusion (PTSD)1.\nOur contributions are as follows:\n1.We introduce a novel guidance mechanism that enables\ndiffusion models, trained on higher-temperature data,\nto generate samples that approximate those from a\nlower-temperature distribution.\n2.Using this guidance term, we propose Progressive Tem-\npering Sampler with Diffusion (PTSD). It fits diffusion\nmodels sequentially on higher temperatures and gener-\nates samples for lower temperatures using our proposed\nguidance, allowing the information at higher tempera-\ntures to transfer to lower ones efficiently.\n3.We evaluate PTSD on a variety of targets, demonstrat-\ning its effectiveness. Our approach achieves orders-of-\nmagnitude improvement in target density evaluation\nefficiency compared to other diffusion-based neural\nsamplers, demonstrating a great potential in integrating\nstandard sampling algorithms and neural samplers.\n2. Background\nMarkov chain Monte Carlo and Parallel Tempering\nMarkov Chain Monte Carlo (MCMC) is a family of al-\ngorithms designed to sample from an unnormalized density\nfunction by constructing a Markov chain whose station-\nary distribution matches the target. Representative MCMC\nmethods include Gibbs Sampling (Geman & Geman, 1984),\nthe Metropolis-Hastings (MH) algorithm (Metropolis et al.,\n1953; Hastings, 1970), the Metropolis-Adjusted Langevin\nAlgorithm (MALA, Grenander & Miller, 1994), and Hamil-\ntonian Monte Carlo (HMC, Duane et al., 1987).\nHowever, for multi-modal distributions, MCMC can easily\nget stuck in a single mode, failing to explore the entire sup-\nport effectively in practice (Neal, 1993). Parallel Tempering\n(PT, Swendsen & Wang, 1986; Geyer, 1991; Hukushima\n& Nemoto, 1996) addresses this issue by introducing a se-\nquence of temperatures, TK> TK−1>···> T 1, where\nT1corresponds to the original target distribution. PT runs\nmultiple Markov chains in parallel, each sampling from\na tempered version of the target distribution, defined as\npTk∝p1/Tk, as illustrated in Fig. 2. MCMC at high temper-\natures can traverse between modes more efficiently, while\n1The code for the paper will be available at\nhttps://github.com/cambridge-mlg/Progressive-Tempering-\nSampler-with-Diffusion.\n2\n--- Page 3 ---\nProgressive Tempering Sampler with Diffusion\nFigure 2. Illustration of parallel tempering with three temperatures.\nMCMC at lower temperatures explores local modes and\nprovides unbiased samples from the target distribution. In-\nformation between different temperatures is shared by swap-\nping samples. Specifically, for samples xiandxjat adjacent\ntemperatures TiandTj, PT swaps them with probability\np= min\u0012\n1,˜p(xj)1/Ti˜p(xi)1/Tj\n˜p(xi)1/Ti˜p(xj)1/Tj\u0013\n. (2)\nDiffusion models DMs (Sohl-Dickstein et al., 2015; Ho\net al., 2020; Song et al., 2021) define a forward process cor-\nrupting the original data distribution with a Gaussian noise,\nwhich corresponds to a diffusion SDE. We can generate\nsamples by denoising progressively, corresponding to the\ntime-reversal of the forward SDE.\nAssume a target density p0(x0). We define a noising pro-\ncess2\ndxt=p\n2 ˙σ(t)σ(t) dwt, x 0∼p0 (3)\nwhere wtis a Wiener process. Its time reversal is given by\ndxt=−2 ˙σ(t)σ(t)∇xtlogpt(xt) dt+p\n2 ˙σ(t)σ(t) d ¯wt,(4)\nwhere xtmax∼ptmaxand∇xtlogpt(xt)is the score function\nat diffusion time t, and ¯wtis a reverse time Wiener process.\nThept(xt)is the target density convolved with a Gaussian\nkernel pt(xt) =R\nN(xt|x0, σ(t)2I)p0(x0) dx0.\nFor a sufficiently large σ(tmax),ptmaxis well approximated\nby a Gaussian N(xt|0, σ(tmax)2I)and hence is tractable.\nDuring training, we approximate the score ∇xtlogpt(xt)\nwith a time-dependent neural network by denoising score\n2For simplicity, we only discuss the variance-exploding process\nintroduced by Song et al. (2021); Karras et al. (2022) in this paper.\nHowever, we note that our proposed approach is also compatible\nwith the variance-preserving process.matching (Vincent, 2011). During sampling, we start with\nsamples from N(xtmax|0, σ(tmax)2I)and follow Eq. (4).\nAdditionally, the score function is related to the denoising\nmean through Tweedie’s formula (Efron, 2011; Roberts &\nTweedie, 1996):\n∇xtlogpt(xt) = (E[x0|xt]−xt)/σ(t)2. (5)\nTherefore, for numerical stability, rather than directly ap-\nproximating the score function with a neural network, a\ncommon choice is to regress the denoising mean using a\ndenoiser network Dθ(xt, t), as done by Karras et al. (2022).\nPF-ODE and Hutchinson’s trace estimator Besides the\nreverse SDE defined in Eq. (4), diffusion models (DMs) can\nalso generate samples by the probabilistic flow (PF) ODE:\ndxt=−˙σ(t)σ(t)∇xtlogpt(xt) dt, x tmax∼ptmax.(6)\nThis formulation not only provides an alternative method\nfor sample generation but also offers a principled approach\nto estimating the log density of the generated samples. Con-\ncretely, by employing the instantaneous change-of-variables\nformula (Chen et al., 2018), we obtain\nd logpt(xt)\ndt=−˙σ(t)σ(t) tr(∇2\nxtlogpt(xt)) (7)\nand the Jacobian can be approximated by Hutchinson’s trace\nestimator (Hutchinson, 1989; Grathwohl et al., 2018)\ntr(∇2\nxtlogpt(xt)) =Eϵ[ϵ⊤∇2\nxtlogpt(xt)ϵ], (8)\nwhere ϵfollows a Rademacher distribution (Hutchinson,\n1989). We can approximate the expectation using Monte\nCarlo integration, with vector-Jacobian products (VJP) en-\nabling efficient computation.\n3. Methods\nIn this section, we describe our approach, Progressive Tem-\npering Sampler with Diffusion (PTSD). Unlike other meth-\nods shown in Fig. 1, it integrates Parallel Tempering (PT)\nand neural samplers to achieve more efficient utilization of\ntarget energy evaluations.\nIn spite of the advances by PT, it can only generate inde-\npendent samples when (1) uncorrected samples are drawn\nat the highest temperature, and (2) the uncorrected samples\nare propagated to the lowest temperature. Therefore, the\nefficiency of PT will highly rely on the quality of local ex-\nploration and the swapping. On the other hand, although\nthe MCMC chain can mix faster at higher temperatures, ob-\ntaining an uncorrected sample still requires a considerable\nnumber of steps. Also, while the (unnormalized) densi-\nties at different temperatures differ only in their exponents\n3\n--- Page 4 ---\nProgressive Tempering Sampler with Diffusion\n𝜽\nsample train\nMCMC𝜽𝐾−1\n 𝜽𝐾\nMCMC\nPTinit\nMCMC𝜽𝑘−1 𝜽𝑘 𝜽𝑘−2\nMCMCguide\nStage 1: initialize buffer\n and model at highest \ntwo temperaturesStage 𝐾−𝑘+1: sample with \nguidance and train model\n at temperature 𝑇𝑘−2𝑇𝐾 𝑇𝐾−1 𝑇𝑘 𝑇𝑘−1 𝑇𝑘−2\nDSM DSM DSM DSM\nPT\nTemperature decreasing…\ninit\nMCMC𝜽2 𝜽3 𝜽1\nMCMCguide\nFinal Stage: sample with \nguidance and train model\n at temperature 𝑇1𝑇𝑘 𝑇𝑘−1 𝑇𝑘−2\nDSM DSM\nPT\nFigure 3. The training process of PTSD. We unroll the training\nprocess into a sequence of target temperatures. We first initialize\nbuffers and models at the highest two temperatures and generate\nsamples from lower temperatures sequentially using the tempera-\nture guidance to extrapolate to lower temperatures.\nand hence share a global similarity, the swapping is only\nperformed by “copying and pasting” the sample between\ntemperatures, failing to leverage such prior knowledge.\nTherefore, a natural question is: can we find an algorithm\nto produce uncorrelated samples for higher temperatures,\nand more efficiently propagate the samples for higher tem-\nperatures to lower ones ?\nWe answer this question affirmatively with a learned neural\nsampler. A well-trained neural sampler at one tempera-\nture can produce independent samples at a cheaper cost.\nMoreover, the neural sampler can be viewed as a “func-\ntional representation” of the target density, and sharing the\nweights of the neural sampler across temperatures offers\na more efficient mechanism for transferring information\ncompared to traditional sample swapping.\nWith these motivations, we now turn to the details of our\napproach. We begin by introducing temperature guidance in\nSec. 3.1, which is a more efficient way to share information\nbetween temperatures by enabling the neural sampler trained\nat higher temperatures to generate approximate samples for\na lower temperature. Then, we integrate this guidance term\ninto the full pipeline of PTSD in Sec. 3.2.\n3.1. Temperature guidance\nTo avoid overloading notation, we use uppercase Tto de-\nnote temperatures while reserving lowercase tfor diffusion\ntime steps. Given the target unnormalized density ˜p, the un-\nnormalized density function at temperature Tis defined by\n˜p1/T. For simplicity, we will use p0(x0, T)to represent the\ntarget distribution at temperature T,i.e.p0(x0, T)∝˜p1/T,\nandpt(xt, T) =R\nN(xt|x0, σ(t)2I)p0(x0, T) dx0.\nAssume we have a trained diffusion model that approximatesthe score function ∇xtlogpt(xt, T)fort∈[0, tmax]and\nT∈[T1, T2]. Now, we want to generate samples at a lower\ntemperature, T0< T 1, which requires knowing the score\nfunction ∇xtlogpt(xt, T0). At t= 0, this is simple be-\ncause, by definition, the scores at different temperatures are\nrelated by ∇x0logp0(x0, T0)·T0=∇x0logp0(x0, T1)·T1.\nHowever, for t >0, this relationship no longer holds in gen-\neral due to the complex nature of Gaussian convolution.\nInstead, we apply Taylor expansion on score around T1:\n∇xtlogpt(xt, T)≈ ∇ xtlogpt(xt, T1)\n+ (T−T1)∂\n∂T∇xtlogpt(xt, T)\f\f\f\nT=T1.(9)\nIf the trained model is conditioned on a continuum of tem-\nperatures T, we could calculate the partial derivative by au-\ntomatic differentiation. However, this requires training the\ntemperature-conditioned diffusion model on a sufficiently\ndiverse set of temperatures to ensure a robust estimation of\nthe partial derivative.\nFortunately, we can approximate the derivative with finite\ndifferences:\n∂\n∂T∇xtlogpt(xt, T)\f\f\f\nT=T1\n≈∇xtlogpt(xt, T2)− ∇ xtlogpt(xt, T1)\nT2−T1.(10)\nPlugging Eq. (10) into Eq. (9), we have\n∇xtlogpt(xt, T)≈T2−T\nT2−T1∇xtlogpt(xt, T1)\n−T1−T\nT2−T1∇xtlogpt(xt, T2)(11)\n= (1 + w)∇xtlogpt(xt, T1)−w∇xtlogpt(xt, T2),\nwhere w= (T1−T)/(T2−T1). We highlight the similarity\nbetween Eq. (11) and the guidance in diffusion models\n(Ho & Salimans, 2021; Karras et al., 2024), offering an\nintuitive perspective on this estimator: contrasting a “better”,\nlower-temperature model by guiding the model at current\ntemperature T1with its “worse”, higher-temperature version.\nFor this reason, we term Eq. (11) as temperature guidance .\nOne concern on this guidance is that when diffusion\ntimet= 0, the known relation ∇x0logp0(x0, T)·T=\n∇x0logp0(x0, T1)·T1=∇x0logp0(x0, T2)·T2does not\nhold, indicating this approximate guidance is inaccurate\nwhen t→0. Fortunately, in diffusion models, the accuracy\nof the score at small time steps typically has a minimal\nimpact on the quality of the generation.\nFinally, similar to standard guidance methods, we note that\ntemperature guidance is independent of the specific parame-\nterization of diffusion models. In our experiments, instead\nof estimating the score function, we follow Karras et al.\n(2022) to learn the denoising mean using a denoiser.\n4\n--- Page 5 ---\nProgressive Tempering Sampler with Diffusion\n3.2. Progressive Tempering Sampler with Diffusion\nWe now explore the use of temperature guidance to con-\nstruct a pipeline that integrates neural samplers with parallel\ntempering (PT). Analogous to PT, we define a decreasing\nsequence of temperatures [TK, TK−1,···, T1], where T1\ncorresponds to the temperature of our target distribution.\nWith the proposed temperature guidance formulation, we\nstructure our algorithm as follows:\n1.Run PT at the highest two temperatures: To ini-\ntialize, we run MCMC (specifically, PT with two tem-\nperatures) at the two highest temperatures, TKand\nTK−1, and collect samples into buffers BKandBK−1.\nThe Markov chain at these high temperatures is gener-\nally easier to explore the support and less likely to get\ntrapped in a local mode (Earl & Deem, 2005).\n2.Fit initial diffusion model: After obtaining sufficient\nsamples at the two highest temperatures, we fit two\ndiffusion models θKandθK−1to each temperature3.\n3.Draw lower-temperature samples by temperature\nguidance: We then draw samples at TK−2using the\ntemperature guidance method proposed in Sec. 3.1 and\nstore these samples in the buffer BK−2.\n4.Fine-tune diffusion model for lower temperature:\nWe initialize θK−2←θK−1, and fine-tune θK−1and\nθK−2using samples in buffer BK−1andBK−2.\nWe repeat steps 3 and 4 until we obtain diffusion models θ1.\n3.3. Improving Techniques for Training PTSD\nWhile the temperature guidance provides an efficient way to\nguide higher temperature models to sample from lower tem-\nperature distributions, we should note that it only generates\napproximate samples. This approximation error accumu-\nlates when running PTSD with multiple temperature levels,\nultimately leading to highly biased samples and models.\nLuckily, we have access to the marginal (unnormalized)\ndensity of each temperature at intermediate steps. This en-\nables us to incorporate importance resampling or MCMC\nsteps to refine the quality of the buffer. We now introduce\ntwo techniques in detail.\nTruncated importance resampling We consider apply-\ning importance resampling to intermediate steps at tempera-\ntureTk. Specifically, we replace the score function in Eq. (6)\nwith the temperature guidance in Eq. (11) and generate sam-\nples by following the PF ODE. This allows us to obtain\nsamples x1, . . . , x Balong with their corresponding densi-\ntiesq(x1), . . . , q (xB)(in log space). We then compute the\n3From now on, we denote θkas the parameter of the diffusion\nmodel trained at temperature Tk.Algorithm 1 Training for PTSD\nInput: Target density ˜p, Temperatures {Tk}K\nk=1, Empty\nBuffers B={Bk}K\nk=1, Initial parallel tempering (PT)\nstepsL, Refinement PT steps l, Truncate quantile τ, Train-\ning iterations M, Buffer size B.\nOutput: Model θ1.\n# Initialize at two highest temperatures:\nInitialize buffers BK−1,BKwithLsteps PT;\nTrain models θK−1,θKforMiterations;\n# Progressively decrease the temperature:\nforkfrom Kto3do\n# Sample with temperature-guidance:\nDraw Bsamples {xn}B\nn=1forTk−2by PF ODE with\ntemperature-guidance, using models θk−1,θk;\n# Calculate Truncated IS Weights:\nCalculate the IS weights {wn}B\nn=1by Eq. (12);\nwmax←τ-quantile\u0000\n{wn}B\nn=1\u0001\n;\nForn= 1,···, B, setwn←min(wn, wmax);\nRenormalize {wn}B\nn=1;\n# Importance Resample:\nforifrom 1toBdo\nn←Category( {wn}B\nn=1);\nAppend xntoBk−2;\nend for\n# Local PT Refinement:\nRefine samples by l-step PT in Bk−2andBk−1;\n# Fine-tune models:\nInitialize θk−2←θk−1;\nTrain θk−2onBk−2forMiterations;\nTrain θk−1onBk−1forMiterations;\nend for\nself-normalized importance weights as\nwn=˜p(xn)1/Tk/q(xn)PB\nn′=1˜p(xn′)1/Tk/q(xn′). (12)\nFinally, we resample Binstances from these Bsamples\nwith replacement, where the selection probabilities are pro-\nportional to wn.\nThis method is generally guaranteed to be unbiased as\nB→ ∞ and is commonly adopted in Sequential Monte\nCarlo (SMC, Liu & Chen, 1998). When applying the\nmethod with diffusion model proposals, we must be careful,\nhowever: the discretization of the PF ODE and the approxi-\nmation error in Hutchinson’s trace estimator (Hutchinson,\n1989; Grathwohl et al., 2018) introduce unwanted variance\nin the normalized importance weights. To prevent the sam-\npling process from becoming unstable, we employ truncated\nimportance sampling (Ionides, 2008), where the unnormal-\nized importance weights are clipped to a maximum value to\n5\n--- Page 6 ---\nProgressive Tempering Sampler with Diffusion\nreduce variance. In practice, we set the truncation threshold\nat a predefined quantile of the importance weights.\nLocal parallel tempering refinement While the trun-\ncated IS significantly improves sample quality, biases re-\nmain due to the approximation error in Hutchinson’s trace\nestimator, the self-normalized importance weights, and the\ntruncation of weights. To address these biases, in addition to\nIS resampling, we refine the samples by performing several\nMCMC steps after collecting a buffer. Similar strategies\nwere employed by Sendera et al. (2024); Chen et al. (2024a),\nwhere a few steps of MCMC were applied to improve sam-\nple quality in buffers.\nAdditionally, to improve mixing, rather than running\nMCMC within a single temperature, we can apply PT be-\ntween pairs of adjacent temperatures. Specifically, assume\na buffer size of B, after collecting buffer Bk−2at temper-\nature Tk−2, we randomly pair samples in Bk−2andBk−1\nto form Bpairs. Then, we run BPT processes in parallel,\neach initialized with a pair of samples containing two chains\nat temperatures Tk−2andTk−1, improving sample qual-\nity in both buffers. This approach integrates well with our\nframework, as we will use both Bk−2andBk−1to further\nfine-tune the diffusion models θk−2andθk−1, by which we\nextrapolate further to Tk−3.\nOptionally, we can optimize energy evaluation usage by\nonly running the PT chains from a subset of IS samples.\nThe results from the PT chains are then added to the original\nIS results instead of entirely replacing them. Effectively we\naugment the IS results with the MCMC samples, which can\nbe enough to get around the issue that IS by itself may result\nin a low effective sample size.\nWe illustrate the training pipeline in Fig. 3, and detail the\npseudo-code in Alg. 1. After training, we sample using the\nmodel at the lowest temperature θ1by either the reverse-\nSDE in Eq. (4) or the PF ODE in Eq. (4).\n4. Connection with Related Works\nTraining neural samplers for unnormalized density\nThere are many approaches aiming to learn a network to\nsample from the target density without getting access to\ndata. Flow Annealed Importance Sampling Bootstrap (FAB,\nMidgley et al., 2023) trains a normalizing flow using the\nα-2 divergence, estimated by Annealed Importance Sam-\npling (Neal, 2001), and incorporates a replay buffer (Mnih\net al., 2015; Schaul et al., 2016) to reduce computational\ncost and mitigate forgetting. Recently, due to the success of\ndiffusion models, studies have focused on SDE and control-\nbased neural samplers, such as Path Integral Sampler (PIS,\nZhang & Chen, 2022), Denoising Diffusion Samplers (DDS,\nVargas et al., 2023), and Controlled Monte Carlo Diffusion\n(CMCD, Vargas et al., 2024), which match the path mea-sure between the sampling process and a target process.\nOn the other hand, Iterated Denoising Energy Matching\n(iDEM, Akhound-Sadegh et al., 2024) estimates the score\nfunction directly using the target score identity (De Bor-\ntoli et al., 2024) combined with self-normalized importance\nsampling. Bootstrapped Noised Energy Matching (BNEM,\nOuYang et al., 2024) generalizes this estimator to energy-\nparameterized diffusion models, while Diffusive KL (DiKL,\nHe et al., 2024) integrates this estimator with variational\nscore distillation techniques (Poole et al., 2023; Luo et al.,\n2024) to train a one-step generator as the neural sampler.\n𝜽\nsample train\nMCMC𝜽𝐾−1\n 𝜽𝐾\nMCMC\nPT\nMCMC𝜽𝑘−1 𝜽𝑘 𝜽𝑘−2\nMCMCinit\nguide\nStage 1: initialize buffer\n and model at highest \ntwo temperaturesStage 𝐾−𝑘+1: sample with \nguidance and train model\n at temperature 𝑇𝑘−2𝑇𝐾 𝑇𝐾−1 𝑇𝑘 𝑇𝑘−1 𝑇𝑘−2\nMCMC𝜽2 𝜽3\nMCMC\nFinal Stage: sample with\n guidance at target\n temperature 𝑇1𝑇3 𝑇2 𝑇1\nDSM DSM\n DSM DSM\nPT PTguide\nFigure 4. “Self-bootstrapping”\ntraining of neural samplers.A key property of neu-\nral samplers is self-\nbootstrapping behavior,\nas illustrated in Fig. 4.\nAt each training step,\nthe sampler generates\nsamples from itself. Some\napproaches, such as FAB,\niDEM, and BNEM, collect\nthese samples in a buffer.\nWe then use these samples\nto compute the loss and\nimprove the neural sampler.\nAs the sampler improves, it generates samples that are\ncloser to the target distribution. In turn, these higher-quality\nsamples can provide a stronger training signal, further\nenhancing the neural sampler in a self-reinforcing cycle.\nHowever, this self-bootstrapping behavior can also intro-\nduce inefficiencies. The improvement occurs only through\nthe training objective. If the current samples fail to provide\na strong enough signal for the objective to refine the neu-\nral sampler, the sampler will repeatedly generate similar\nsamples, leading to wasted computation.\nIn contrast, our approach can be seen as bootstrapping\nacross temperatures . We first train models at higher tem-\nperatures, where sampling is easier. Then, we use the high-\ntemperature model to generate samples at lower temper-\natures and leverage these samples to fine-tune the model\ntoward lower temperatures. Additionally, we incorporate\nimportance sampling and PT to refine buffers at each in-\ntermediate step. As a result, improvement occurs not only\nthrough the training objective but also through the proposed\ntemperature guidance and refinement strategies.\nIntegrating MCMC algorithms with neural samplers\nRecent line of research has explored integrating MCMC\nalgorithms to improve the performance of neural samplers.\nThey either introduce Sequential Monte Carlo (SMC) to\ncorrect bias (Arbel et al., 2021; Albergo & Vanden-Eijnden,\n2024; Phillips et al., 2024; Chen et al., 2024a) or employ\nMCMC to improve the buffer quality (Sendera et al., 2024).\nHowever, while these works primarily focus on ensuring\n6\n--- Page 7 ---\nProgressive Tempering Sampler with Diffusion\nasymptotic correctness or improving the sample quality of\nneural samplers, they still incur a substantial overhead in\nevaluating the target energy compared to vanilla PT. In a\nconcurrent work, Zhang et al. (2025) proposed generalized\nparallel tempering , employing a neural network to transport\nsamples between adjacent temperatures and thus boost the\nswap rate of vanilla PT. In contrast, our work takes a dual\nperspective: rather than embedding neural samplers into PT\nto improve PT, we incorporate PT’s temperature-exchange\nmechanism into neural samplers to enhance both sampling\nefficiency and sample quality of the neural sampler.\nGuidance and inference-time control in diffusion models\nMany tools exist for modifying and aggregating pretrained\ndiffusion models at inference time. For instance, it is possi-\nble to combine multiple diffusion models to get their inter-\nsection (Liu et al., 2022; Du et al., 2023) and add additional\nconstraints and controls to the final distribution ( e.g., for\ninverse problem solving) (Chung et al., 2023; Song et al.,\n2023; Rissanen et al., 2024). Of particular interest to us are\nso-called classifier-free guidance (Ho & Salimans, 2021)\nand its recent development (Karras et al., 2024), where two\ndiffusion model denoisers, one closely fit to the data, and\none less closely fit, are contrasted to form diffusion models\nwith even better fits, and hence, better sample quality:\nDguided ,w(xt) = (1 + w)Dθ1(xt)−wDθ2(xt),(13)\nDθ1andDθ2represent the well-fitted and poorly-fitted mod-\nels respectively (Karras et al., 2024). Guidance strength\nw > 1improves sample quality. When applied to condi-\ntional and unconditional models as the ’better’ and ’worse’\nmodels, the method is called classifier-free guidance (Ho\n& Salimans, 2021), which has been crucial to the success\nof text-to-image diffusion models (Saharia et al., 2022;\nRamesh et al., 2022; Rombach et al., 2022). This ap-\nproach closely parallels our temperature guidance method\nin Eq. (11) and Eq. (13).\n5. Experiments and Results\nIn this section, we evaluate our proposed approach and\ncompare with other baselines. In Sec. 5.1, we first test\nour temperature guidance method on the Lennard-Jones\npotential with 55 particles (LJ-55), as introduced in (K ¨ohler\net al., 2020; Klein et al., 2024). As we will show, this\nguidance enables effective extrapolation.\nIn Sec. 5.2, we compare PTSD on two distinct multi-modal\ndistributions, Mixture of 40 Gaussians (MoG-40) and Many-\nWell-32 (MW-32), with other neural samplers, including\nFAB (Midgley et al., 2023), iDEM (Akhound-Sadegh et al.,\n2024), BNEM (OuYang et al., 2024), DiKL (He et al., 2024),\nDDS (Vargas et al., 2023), and CMCD (Vargas et al., 2024).\nWe also evaluate the performance of a diffusion model\ntrained directly on PT-generated data (PT+DM).\nFigure 5. First two panels: Marginal density of the interatomic dis-\ntance on buffers at two temperatures, along with the marginal den-\nsity of a diffusion model fit to the buffer. Right: The temperature-\nguided extrapolation based on the higher-temperature models. AD:\nauto-diff extrapolation; ME: model extrapolation; RS: score rescal-\ning heuristic; GT: ground truth; PTSD : our temperature guidance.\n5.1. Extrapolation with Temperature guidance\nTo showcase the capability of proposed guidance, we con-\nduct experiment on the LJ-55 potential, where we train DMs\nat temperature 2.0and1.5and aim to extrapolate to a target\ntemperature 1.0. Fig. 5 visualizes the histograms of inter-\natomic distance of samples generated by the trained models\nat high temperatures, as well as the one of extrapolated\nsamples at the target temperature.\nThe extrapolation is closer to the target temperature than\neither model at temperature 2.0or1.5, showing a large\noverlap with the ground truth. We also compare our ex-\nploration approach with several other strategies. (1) model\nextrapolation (ME): as we train a single model on both\ntemperatures 2.0 and 1.5, and by conditioning on the tem-\nperature label, we can directly input temperature 1.0 and\nrely on the generalisation in deep neural networks, an ap-\nproach that was demonstrated useful for interpolation in\nMoqvist et al. (2025). (2) auto-diff extrapolation (AD):\nwe directly use Equation (9) instead of finite difference\napproximation for the time-derivative; (3) score rescaling\nheuristic (RS): we simply anneal the score with the tem-\nperature ∇xtlogpt(xt,1.0)≈1.5· ∇xtlogpt(xt,1.5), an\napproach considered by Skreta et al. (2025). Our proposed\ntemperature guidance provides a better extrapolation than\nthe other choices. Even though the extrapolation is not per-\nfect, it serves as an initialization for the local PT refinement,\nas proposed in Sec. 3.3.\n5.2. Comparison with Baselines\nWe now measure the sample quality of PTSD and compare\nit with other baselines. We report Wasserstein-2 ( W2) dis-\ntance, Total Variation distance (TVD), and Maximum Mean\nDiscrepancy (MMD). For all tasks, TVD and MMD are mea-\nsured over the energy histograms of samples, while W2is\nmeasured in the samples. The calculation W2in LJ-55 task\ntakes the SE(3)-equivariance into account as (OuYang et al.,\n7\n--- Page 8 ---\nProgressive Tempering Sampler with Diffusion\nTable 1. Comparing PTSD with other neural sampler baselines. We measure ( best,second best ) the TVD and MMD between Energy\nhistograms, and W2distance between data samples. We note that as the energy histograms project the entire data space into one dimension,\nit can be sensitive to outliers but insensitive to mode coverage. ‘-’ indicates that the method diverges or is significantly worse than others.\nGMM ( d= 2) MW32 ( d= 32 ) LJ55 ( d= 165 )\nTVD↓ MMD↓ W2↓ TVD↓ MMD↓ W2↓ TVD↓ MMD↓ W2↓\nFAB 0.23±0.01 0.30±0.02 2.50±0.19 0.32±0.01 0.16±0.02 5.70±0.01 - - -\nCMCD 0.14±0.01 0.08±0.01 3.36±0.22 0.69±0.01 0.62±0.01 7.44±0.03 - - -\nDDS 0.22±0.01 0.10±0.03 4.52±0.30 0.77±0.00 0.70±0.02 7.60±0.02 - - -\niDEM 0.09±0.01 0.01±0.01 3.26±0.42 0.87±0.01 0.87±0.01 8.30±0.02 - 0.49±0.01 1.95±0.00\nBNEM 0.12±0.00 0.07±0.00 2.16±0.16 0.31±0.01 0.05±0.01 8.41±0.09 0.18±0.01 0.01±0.00 1.76±0.00\nDiKL 0.10±0.01 0.03±0.01 3.23±0.24 0.34±0.00 0.16±0.02 6.59±0.03 - - -\nPT+DM 0.13±0.05 0.06±0.06 3.01±0.66 0.13±0.03 0.03±0.02 5.04±0.02 0.64±0.02 0.22±0.01 1.82±0.00\nPTSD 0.08±0.02 0.02±0.01 1.93±0.41 0.14±0.05 0.04±0.03 4.99±0.07 0.79±0.01 0.19±0.00 1.81±0.00\nTable 2. Number of target density calls for different approaches to\nachieve the performance reported in Table 1.\nAlgorithm GMM ( d= 2) MW32 ( d= 32 ) LJ55 ( d= 165 )\nFAB 6.6×1067.2×109–\nCMCD 4.4×1091.6×109–\nDDS 2.6×1091.1×109–\niDEM 5.0×10101.8×10101.3×1010\nBNEM 7.5×1091.8×10106.4×109\nDiKL 1.2×10108.0×109–\nPT+DM 1.2×1068.0×1061.65×105\nPTSD (ours) 1.0×1065.3×1061.5×105\n(a) MoG-40\n (b) MW-32\nFigure 6. Comparing PTSD with the results obtained by fitting a\ndiffusion model post hoc to PT-generated data. For MoG-40, we\ncompare two different settings, using 4 or 10 temperature levels.\n2024). For LJ-55, we optimize the energy evaluation usage\nby running PT from a subset of IS samples, as explained\nin Sec. 3. We note that the energy histogram is sensitive to\noutliers while generally being robust to mode collapse. In\nHe et al. (2024, Fig. 5), the authors showed that the energy\nhistogram can closely approximate the ground truth, even\nwhen significant mode collapse occurs. On the other hand,\nW2distance may provide a more comprehensive assessment\nof sample quality. We also report the computational cost (in\nterms of target evaluations) in Table 2.\nOn both MoG-40 and MW-32, our approach achieves\nstate-of-the-art sample quality and demonstrates orders-\nof-magnitude improvement in efficiency over other neural\nsamplers. Our algorithm falls slightly behind on BNEM on\nsample quality. However, it requires a significantly smallernumber of energy evaluations. In fact, LJ-55 is a relatively\nsimple task for MCMC: a standard MALA requires only\n4000 steps to mix. The challenge of LJ-55 for neural sam-\nplers arises because the target contains inhibitory regions\nand has large gradients, leading to instabilities. BNEM\naddresses this by first smoothing the target density and ex-\nplicitly regressing the target energy. On the other hand, our\napproach relies only on samples, making it stable and effi-\ncient but less sensitive to inhibitory regions. This explains\nwhy we fall slightly behind.\nAdditionally, to provide a more detailed comparison with\nPT+DM, we plot the sample quality of PT+DM using sam-\nples obtained by PT with different numbers of target evalua-\ntions in Fig. 6. As we can see, on these targets, PTSD out-\nperforms PT+DM using the same setting. This gain might\ncome from the fact that PTSD can provide uncorrelated\nsamples for high temperatures easily after being trained on\nthese temperatures, and the temperature guidance serves as\nan approximate yet more informative “swap” mechanism.\n5.3. Ablation Study\nWe conduct ablation studies to verify the effectiveness of\ntemperature guidance and the improvement techniques pro-\nposed in Sec. 3.3 in Table 4 on MW-32. As we can see, both\nthe guidance and the truncated IS enhance the performance\nof our algorithm. Additionally, we note that even without\nIS, PTSD remains competitive among all neural samplers\nin Table 1. This further demonstrates the effectiveness of\ntemperature guidance on its own.\n5.4. Scaling PTSD on Alanine Dipeptide\nWe also apply the method to the alanine dipeptide molecule\nin Cartesian coordinates. To maximise the benefits of PTSD,\nwe run local PT refinement only for a subset of IS outputs,\nas detailed in Sec. 3.3. We evaluate both PTSD and PTDM\nwith approximately the same amount of energy function\nevaluations ( 2.6×107), and show the resulting Ramachan-\ndran plots in Fig. 7. This example demonstrates the poten-\n8\n--- Page 9 ---\nProgressive Tempering Sampler with Diffusion\nTable 3. Mean log-likelihood of real data under our PTDM and\nPTSD models for the alanine dipeptide molecule, and the KL\ndivergence between the ground-truth Ramachandran histogram\nand the model-generated one from 106samples.\nPTSD PT+DM\nEp[logpθ(x)]213.32 ±0.06 212.38±0.05\nKLD 6.9e-2 3.2e-02\nFigure 7. Ramachandran plots of ground truth, PT+DM, and PTSD.\nPT+DM and PTSD were trained with 2.6e7 energy evaluations.\ntial scalability and efficiency of PTSD in realistic sampling\nproblems. We also evaluate the log-likelihood of ground-\ntruth data under our PTDM and PTSD models, and the KL\ndivergence between the ground-truth Ramachandran his-\ntogram and the model-generated one in Table 3. We show\nthe projection of Ramachandran plots along two axes ( ϕ\nandψ) in App. D.4. As we can see, while PT+DM appears\nslightly better performance on generate the large metastable\nsates, resulting to be more plausible than PTSD along the ϕ\naxis, PTSD achieves better log-likelihood than PT+DM and\nalso captures the small metastable state more effectively,\nas shown in Fig. 7. This showcases the great potential of\nPTSD when applied to more complex systems.\n6. Conclusions and Limitations\nIn this paper, we proposed a heuristic temperature guidance\nthat allows us to generate samples at lower temperatures\nwith pretrained diffusion models at two higher temperatures.\nBased on this, we formulated the Progressive Tempering\nSampler with Diffusion (PTSD). PTSD achieved competi-\ntive sample quality, demonstrated orders-of-magnitude im-\nprovement in efficiency over other neural samplers, and\nshowed promising direction in combining parallel temper-\ning to enhance neural samplers.\nWhile training without data is appealing, this pursuit may be\ninefficient with current approaches. Instead, designing meth-\nods that effectively integrate neural samplers with available\ndata could offer greater practical benefits. While our work\nmay not represent the optimal solution for this direction,\nit opens a promising avenue for future works, potentially\nadvancing the practicality of neural samplers.\nHowever, several key limitations still remain:Table 4. Ablations on the temperature guidance in Sec. 3.1 and the\ntruncated IS in Sec. 3.3. We do not provide ablation on the local\nPT refinement as running MCMC is clearly helpful.\nTVD↓ W2↓\nPTSD w/o temp-guide 0.34 24.59\nPTSD w/o IS 0.23 5.84\nPTSD 0.14 4.99\n1.Network training cost : while our approach improves\nefficiency in terms of target evaluations, it remains\nslower than parallel tempering in terms of wall-clock\ntime in our experiments. This is because we need\nto fine-tune the diffusion model for each temperature\nlevel. For more complex target distributions, we expect\nthat training the diffusion model will be more compu-\ntationally efficient than directly evaluating the target\ndensity and its gradient. Therefore, an important direc-\ntion for future work is extending our pipeline to more\ndifficult target distributions.\n2.Non-parallelizable execution : our approach relies\non decreasing the temperature progressively, which\nis different from PT, where different chains can be\ndistributed on multiple devices and run in parallel.\n3.Sensitivity to temperature schedule and network\nlearning quality : our approach’s performance can\nbecome fragile when temperature levels differ signifi-\ncantly or when the target distribution is too complex for\nthe network to learn accurately. In contrast, vanilla PT\nis more robust to a suboptimal temperature schedule\nand also offers additional flexibility in path selection.\nThe sensitivity also appears in hyperparameters: the\nnetwork choice, learning rate, and the truncation thresh-\nold for truncated IS can also impact the final sample.\nAnother line of future work is to test the amortizability of our\nproposed approach. In a recent study, Havens et al. (2025)\nintroduced a conformer-generation dataset comprising many\ntargets with similar properties. Using PT to obtain samples\nfor each target separately and then fitting a diffusion model\nwould be expensive. In contrast, PTSD framework may\nhave the potential to substantially reduce this cost.\nSpecifically, we could first collect data for each target at the\ntwo highest temperatures and fit a diffusion model condi-\ntioned on the target label. When performing temperature\nguidance, we could randomly sample targets to build a buffer\nwith mixed targets. This buffer could be used (with its asso-\nciated target labels) to train the conditional diffusion model\nat lower temperatures. Thus, PTSD only requires running\nseparate MCMC during initialization, and it can share infor-\nmation across targets as temperature decreases, resulting in\na potential efficiency gain.\n9\n--- Page 10 ---\nProgressive Tempering Sampler with Diffusion\nAcknowledgments\nWe acknowledge Saifuddin Syed for discussions on par-\nallel tempering, and Yuanqi Du, Mingtian Zhang, Louis\nGrenioux, Laurence Midgley and Javier Antor ´an for dis-\ncussions on neural samplers. JH acknowledges support\nfrom the University of Cambridge Harding Distinguished\nPostgraduate Scholars Programme. JMHL and RKOY ac-\nknowledge the support of a Turing AI Fellowship under\ngrant EP/V023756/1. SR, MH, and AS acknowledge fund-\ning from the Research Council of Finland (grants 339730,\n362408, 334600). This project acknowledges the resources\nprovided by the Cambridge Service for Data-Driven Discov-\nery (CSD3) operated by the University of Cambridge Re-\nsearch Computing Service (www.csd3.cam.ac.uk), provided\nby Dell EMC and Intel using Tier-2 funding from the En-\ngineering and Physical Sciences Research Council (capital\ngrant EP/T022159/1), and DiRAC funding from the Science\nand Technology Facilities Council (www.dirac.ac.uk). We\nacknowledge CSC – IT Center for Science, Finland, for\nawarding this project access to the LUMI supercomputer,\nowned by the EuroHPC Joint Undertaking, hosted by CSC\n(Finland) and the LUMI consortium through CSC. We ac-\nknowledge the computational resources provided by the\nAalto Science-IT project.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAkhound-Sadegh, T., Rector-Brooks, J., Bose, J., Mittal,\nS., Lemos, P., Liu, C.-H., Sendera, M., Ravanbakhsh, S.,\nGidel, G., Bengio, Y ., et al. Iterated denoising energy\nmatching for sampling from boltzmann densities. In\nProceedings of the Forty-first International Conference\non Machine Learning (ICML) , 2024.\nAlbergo, M. S. and Vanden-Eijnden, E. Nets: A\nnon-equilibrium transport sampler. arXiv preprint\narXiv:2410.02711 , 2024.\nArbel, M., Matthews, A., and Doucet, A. Annealed flow\ntransport monte carlo. In International Conference on\nMachine Learning , pp. 318–330. PMLR, 2021.\nBlessing, D., Jia, X., Esslinger, J., Vargas, F., and Neu-\nmann, G. Beyond ELBOs: A large-scale evaluation of\nvariational methods for sampling. In Proceedings of the\nForty-first International Conference on Machine Learning\n(ICML) , 2024.Chen, J., Richter, L., Berner, J., Blessing, D., Neumann,\nG., and Anandkumar, A. Sequential controlled Langevin\ndiffusions. arXiv preprint arXiv:2412.07081 , 2024a.\nChen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud,\nD. K. Neural ordinary differential equations. In Advances\nin Neural Information Processing Systems (NeurIPS) ,\nvolume 31. Curran Associates, Inc., 2018.\nChen, W., Zhang, M., Paige, B., Hern ´andez-Lobato, J. M.,\nand Barber, D. Diffusive Gibbs sampling. In Proceedings\nof the International Conference on Machine Learning\n(ICML) , pp. 7731–7747. PMLR, 2024b.\nChung, H., Kim, J., Mccann, M. T., Klasky, M. L., and\nYe, J. C. Diffusion posterior sampling for general noisy\ninverse problems. In The Eleventh International Confer-\nence on Learning Representations (ICLR) , 2023.\nDe Bortoli, V ., Hutchinson, M., Wirnsberger, P., and\nDoucet, A. Target score matching. arXiv preprint\narXiv:2402.08667 , 2024.\nDu, Y ., Durkan, C., Strudel, R., Tenenbaum, J. B., Diele-\nman, S., Fergus, R., Sohl-Dickstein, J., Doucet, A., and\nGrathwohl, W. S. Reduce, reuse, recycle: Composi-\ntional generation with energy-based diffusion models and\nmcmc. In Proceedings of the International Conference\non Machine Learning (ICML) , pp. 8489–8510. PMLR,\n2023.\nDuane, S., Kennedy, A., Pendleton, B. J., and Roweth, D.\nHybrid Monte Carlo. Physics Letters B , 195(2):216–222,\n1987.\nEarl, D. J. and Deem, M. W. Parallel tempering: Theory,\napplications, and new perspectives. Physical Chemistry\nChemical Physics , 7(23):3910–3916, 2005.\nEfron, B. Tweedie’s formula and selection bias. Journal\nof the American Statistical Association , 106(496):1602–\n1614, 2011.\nFlamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Bois-\nbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras,\nK., Fournier, N., Gautheron, L., Gayraud, N. T., Janati,\nH., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz,\nA., Seguy, V ., Sutherland, D. J., Tavenard, R., Tong, A.,\nand Vayer, T. Pot: Python optimal transport. Journal of\nMachine Learning Research , 22(78):1–8, 2021.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B.,\nVehtari, A., and Rubin, D. B. Bayesian Data Analysis .\nCRC Press, 2013.\nGeman, S. and Geman, D. Stochastic relaxation, Gibbs dis-\ntributions, and the Bayesian restoration of images. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , PAMI-6(6):721–741, 1984.\n10\n--- Page 11 ---\nProgressive Tempering Sampler with Diffusion\nGeyer, C. J. Markov chain Monte Carlo maximum likeli-\nhood. In Computing Science and Statistics: Proceedings\nof the 23rd Symposium on the Interface , 1991.\nGrathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I.,\nand Duvenaud, D. FFJORD: Free-form continuous dy-\nnamics for scalable reversible generative models. In Inter-\nnational Conference on Learning Representations (ICLR) ,\n2018.\nGrenander, U. and Miller, M. I. Representations of knowl-\nedge in complex systems. Journal of the Royal Statistical\nSociety: Series B (Methodological) , 56:549–581, 1994.\nHastings, W. K. Monte Carlo sampling methods using\nMarkov chains and their applications. Biometrika , 57(1):\n97–109, 1970.\nHavens, A., Miller, B. K., Yan, B., Domingo-Enrich, C.,\nSriram, A., Wood, B., Levine, D., Hu, B., Amos, B.,\nKarrer, B., et al. Adjoint sampling: Highly scalable\ndiffusion samplers via adjoint matching. arXiv preprint\narXiv:2504.11713 , 2025.\nHe, J., Chen, W., Zhang, M., Barber, D., and Hern ´andez-\nLobato, J. M. Training neural samplers with reverse dif-\nfusive KL divergence. arXiv preprint arXiv:2410.12456 ,\n2024.\nHe, J., Du, Y ., Vargas, F., Zhang, D., Padhy, S., OuYang,\nR., Gomes, C., and Hern ´andez-Lobato, J. M. No trick,\nno treat: Pursuits and challenges towards simulation-\nfree training of neural samplers. arXiv preprint\narXiv:2502.06685 , 2025.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\nInNeurIPS 2021 Workshop on Deep Generative Models\nand Downstream Applications , 2021.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS) , volume 33, pp. 6840–6851.\nCurran Associates, Inc., 2020.\nHukushima, K. and Nemoto, K. Exchange Monte Carlo\nmethod and application to spin glass simulations. Journal\nof the Physical Society of Japan , 65(6):1604–1608, 1996.\nHutchinson, M. F. A stochastic estimator of the trace of the\ninfluence matrix for Laplacian smoothing splines. Com-\nmunications in Statistics-Simulation and Computation ,\n18(3):1059–1076, 1989.\nIonides, E. L. Truncated importance sampling. Journal of\nComputational and Graphical Statistics , 17(2):295–311,\n2008.Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nInAdvances in Neural Information Processing Systems\n35 (NeurIPS) , pp. 26565–26577. Curran Associates, Inc.,\n2022.\nKarras, T., Aittala, M., Kynk ¨a¨anniemi, T., Lehtinen, J., Aila,\nT., and Laine, S. Guiding a diffusion model with a bad\nversion of itself. In Advances in Neural Information\nProcessing Systems (NeurIPS) . Curran Associates, Inc.,\n2024.\nKlein, L., Kr ¨amer, A., and No ´e, F. Equivariant flow match-\ning. In Advances in Neural Information Processing\nSystems (NeurIPS) , volume 36. Curran Associates, Inc.,\n2024.\nK¨ohler, J., Klein, L., and No ´e, F. Equivariant flows: exact\nlikelihood generative learning for symmetric densities. In\nProceedings of the International Conference on Machine\nLearning (ICML) , pp. 5361–5370. PMLR, 2020.\nLiu, J. S. and Chen, R. Sequential Monte Carlo methods\nfor dynamic systems. Journal of the American Statistical\nAssociation , 93(443):1032–1044, 1998.\nLiu, N., Li, S., Du, Y ., Torralba, A., and Tenenbaum, J. B.\nCompositional visual generation with composable dif-\nfusion models. In European Conference on Computer\nVision , pp. 423–439. Springer, 2022.\nLuo, W., Hu, T., Zhang, S., Sun, J., Li, Z., and Zhang,\nZ. Diff-instruct: A universal approach for transfer-\nring knowledge from pre-trained diffusion models. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) , volume 36. Curran Associates, Inc., 2024.\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,\nTeller, A. H., and Teller, E. Equation of state calculations\nby fast computing machines. The Journal of Chemical\nPhysics , 21(6):1087–1092, 06 1953.\nMidgley, L. I., Stimper, V ., Simm, G. N., Sch ¨olkopf, B.,\nand Hern ´andez-Lobato, J. M. Flow annealed importance\nsampling bootstrap. In The Eleventh International Con-\nference on Learning Representations (ICLR) , 2023.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature , 518(7540):\n529–533, 2015.\nMoore, J. H., Cole, D. J., and Csanyi, G. Computing hydra-\ntion free energies of small molecules with first principles\naccuracy. arXiv preprint arxiv:2405.18171 , 2024.\n11\n--- Page 12 ---\nProgressive Tempering Sampler with Diffusion\nMoqvist, S., Chen, W., Schreiner, M., N ¨uske, F., and Olsson,\nS. Thermodynamic interpolation: A generative approach\nto molecular thermodynamics and kinetics. Journal of\nChemical Theory and Computation , 2025.\nNeal, R. M. Probabilistic inference using Markov chain\nMonte Carlo methods. Technical report, Department of\nComputer Science, University of Toronto, 1993.\nNeal, R. M. Annealed importance sampling. Statistics and\nComputing , 11:125–139, 2001.\nNo´e, F., Olsson, S., K ¨ohler, J., and Wu, H. Boltzmann\ngenerators: Sampling equilibrium states of many-body\nsystems with deep learning. Science , 365(6457), 2019.\nOuYang, R., Qiang, B., and Hern ´andez-Lobato, J. M. Bnem:\nA boltzmann sampler based on bootstrapped noised en-\nergy matching. arXiv preprint arXiv:2409.09787 , 2024.\nPhillips, A., Dau, H.-D., Hutchinson, M. J., De Bortoli,\nV ., Deligiannidis, G., and Doucet, A. Particle denois-\ning diffusion sampler. In Proceedings of the 41st Inter-\nnational Conference on Machine Learning , pp. 40688–\n40724, 2024.\nPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dream-\nfusion: Text-to-3d using 2d diffusion. In The Eleventh\nInternational Conference on Learning Representations\n(ICLR) , 2023.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125 , 2022.\nRissanen, S., Heinonen, M., and Solin, A. Free hunch: De-\nnoiser covariance estimation for diffusion models without\nextra costs. arXiv preprint arXiv:2410.11149 , 2024.\nRoberts, G. O. and Tweedie, R. L. Exponential convergence\nof langevin distributions and their discrete approxima-\ntions. Bernoulli , pp. 341–363, 1996.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pp. 10684–10695, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J.,\nDenton, E. L., Ghasemipour, K., Gontijo Lopes, R.,\nKaragol Ayan, B., Salimans, T., et al. Photorealistic\ntext-to-image diffusion models with deep language under-\nstanding. In Advances in Neural Information Processing\nSystems (NeurIPS) , volume 35, pp. 36479–36494. Curran\nAssociates, Inc., 2022.Satorras, V . G., Hoogeboom, E., and Welling, M. E(n)\nequivariant graph neural networks. arXiv preprint\narxiv:2102.09844 , 2021.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized experience replay. arXiv preprint arxiv:1511.05952 ,\n2016.\nSendera, M., Kim, M., Mittal, S., Lemos, P., Scimeca, L.,\nRector-Brooks, J., Adam, A., Bengio, Y ., and Malkin,\nN. Improved off-policy training of diffusion samplers.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) . Curran Associates, Inc., 2024.\nSkreta, M., Akhound-Sadegh, T., Ohanesian, V ., Bondesan,\nR., Aspuru-Guzik, A., Doucet, A., Brekelmans, R., Tong,\nA., and Neklyudov, K. Feynman-Kac correctors in diffu-\nsion: Annealing, guidance, and product of experts. arXiv\npreprint arXiv:2503.02819 , 2025.\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequilib-\nrium thermodynamics. arXiv preprint arxiv:1503.03585 ,\n2015.\nSong, J., Vahdat, A., Mardani, M., and Kautz, J.\nPseudoinverse-guided diffusion models for inverse prob-\nlems. In International Conference on Learning Represen-\ntations (ICLR) , 2023.\nSong, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. In International\nConference on Learning Representations (ICLR) , 2021.\nSurjanovic, N., Syed, S., Bouchard-C ˆot´e, A., and Camp-\nbell, T. Parallel tempering with a variational reference.\nAdvances in Neural Information Processing Systems , 35:\n565–577, 2022.\nSurjanovic, N., Syed, S., Bouchard-C ˆot´e, A., and Campbell,\nT. Uniform ergodicity of parallel tempering with effi-\ncient local exploration. arXiv preprint arXiv:2405.11384 ,\n2024.\nSwendsen, R. H. and Wang, J.-S. Replica Monte Carlo\nsimulation of spin-glasses. Physical Review Letters , 57\n(21):2607, 1986.\nSyed, S., Romaniello, V ., Campbell, T., and Bouchard-C ˆot´e,\nA. Parallel tempering on optimized paths. In Inter-\nnational Conference on Machine Learning , pp. 10033–\n10042. PMLR, 2021.\nVargas, F., Grathwohl, W., and Doucet, A. Denoising diffu-\nsion samplers. arXiv preprint arXiv:2302.13834 , 2023.\n12\n--- Page 13 ---\nProgressive Tempering Sampler with Diffusion\nVargas, F., Padhy, S., Blessing, D., and N ¨usken, N. Trans-\nport meets variational inference: Controlled Monte Carlo\ndiffusions. In The Twelfth International Conference on\nLearning Representations (ICLR) , 2024.\nVincent, P. A connection between score matching and de-\nnoising autoencoders. Neural Computation , 23(7):1661–\n1674, 2011.\nV on Toussaint, U. Bayesian inference in physics. Reviews\nof Modern Physics , 83(3):943–999, 2011.\nWelling, M. and Teh, Y . W. Bayesian learning via stochastic\ngradient Langevin dynamics. In Proceedings of the 28th\nInternational Conference on Machine Learning (ICML) ,\npp. 681–688, 2011.\nWoodard, D. B., Schmidler, S. C., and Huber, M. Conditions\nfor rapid mixing of parallel and simulated tempering on\nmultimodal distributions. 2009.\nWu, H., K ¨ohler, J., and Noe, F. Stochastic normalizing\nflows. In Advances in Neural Information Processing\nSystems (NeurIPS) , volume 33, pp. 5933–5944. Curran\nAssociates, Inc., 2020.\nZhang, L., Potaptchik, P., Doucet, A., Dau, H.-D., and\nSyed, S. Generalised parallel tempering: Flexible\nreplica exchange via flows and diffusions. arXiv preprint\narXiv:2502.10328 , 2025.\nZhang, Q. and Chen, Y . Path integral sampler: A stochastic\ncontrol approach for sampling. In International Confer-\nence on Learning Representations (ICLR) , 2022.\n13\n--- Page 14 ---\nProgressive Tempering Sampler with Diffusion\nAppendices\nA. Target Distributions\nMixture of 40 Gaussians (MoG-40) is a mixture of Gaussians with 40 components in 2-dimensional space, proposed by\nMidgley et al. (2023). Each component of the MoG-40 has a mean within [−40,40]×[−40,40]and an diagonal covariance\nofsoftplus (1).\nMany-Well-32 (MW-32) is a multi-modal distribution in 32-dimensional space with 232modes, proposed by Midgley\net al. (2023). It is constructed by concatenating 16 independent samples from the Double-Well distribution (DW-2, No ´e\net al., 2019; Wu et al., 2020) in 2-dimensional space. While sampling 16 samples independently from DW-2 and stacking\nthem to construct a MW-32 sample is easy, directly drawing samples from the MW-32 distribution is challenging as a\nconsequence of its multi-modal nature.\nLennard-Jones- n(LJ-n)describes a n-particle system, where the energy between two particles is described by the\nLennard-Jones potential and the system energy is given by the sum of pairwise 2-particle energy, i.e.\nV(r) = 4 ϵ\u0014\u0010σ\nr\u001112\n−\u0010σ\nr\u00116\u0015\nand U(X) =X\ni∈[n]X\ni>iV(rij), (14)\nwhere X= (x1, ..., x n),rijis the distance between xiandxj, and ϵandσare physical constants. While LJ- ncan be\nchallenging in terms of numerical stability since the energy of system can be problematically large when particles are\ntoo close to each others, it remains a relatively simple target for MCMC with proper initialization or numerically stable\nimplementation. Therefore, we only employ LJ-55 for showcasing the capability of our proposed extrapolation. To ensure\nnumerical stability, we employ cubic spline interpolation introduced by Moore et al. (2024) to smooth the extreme energy\nvalues when two particles are close to each other. In our main experiment, we consider LJ-55. While for the ablation study,\nwe conduct experiments on both LJ-13 and LJ-55.\nAlanine Dipeptide (ALDP) is a 22-particle system formed by two Alanine amino acids. We consider an implicit solvent\nwith a temperature of 300K, which is used by Midgley et al. (2023). We use the implementation in Midgley et al. (2023) to\ncalculate the energy. In contrast to FAB, which generates samples in the internal coordinate system, we consider generating\nsamples in the Cartesian coordinate system.\nB. Evaluation Metrics\nWasserstein-2 distance ( W2)measures the difference between two probability distributions in an optimal transport\nframework. Given empirical samples µfrom the sampler and ground truth samples ν, theW2distance is defined as:\nW2(µ, ν) =\u0012\ninf\nγ∈Π(µ,ν)Eγ\u0002\nd2(X, Y)\u0003\u00131/2\n, (15)\nwhere Πis the transport plan with marginals distributions µandνrespectively, and dis a distance measure. To calculate the\nW2in practice, we use the Hungarian algorithm implemented in the Python optimal transport package (POT, Flamary et al.,\n2021), where we employ the Euclidean distance as our distance measure. We measure the W2over samples for all tasks.\nFor LJ-55, we calculate the W2by taking SE(3), i.e.rotation and translation equivariance, into account. In particular, we\ncalculate the distance between two set of samples by dKabsch (X, Y) = min R,t∈SE(3)∥X−(Y R⊤+t)∥2, where Kabsch\nalgorithm is applied to find the optimal rotation and translation.\nTotal Variation distance (TVD) measures the dissimilarity between two probability distributions, which quantifies the\nabsolute differences between two densities over the entire sample space. Given two distribution PandQdefined on a space\nΩ, with density functions pandq, TVD is defined as\nTVD( P, Q) =1\n2Z\nΩ|p(x)−q(x)|dx. (16)\nWe measure the TVD over energy histograms of samples for all tasks.\n14\n--- Page 15 ---\nProgressive Tempering Sampler with Diffusion\nMaximum Mean Discrepancy (MMD) measures the difference between two distributions using functions in a Reproduc-\ning Kernel Hilbert Space (RKHS). Given two distributions PandQ, the MMD is defined as\nMMD( P, Q) = \nsup\nf∈H,∥f∥H≤1EP[f(X)]−EQ[f(Y)]!1/2\n, (17)\nwhere fis a function in the RKHS associated with a kernel k(x, y)and∥ · ∥Ha norm defined in this RKHS. The MMD can\nbe computed by using the kernel trick as follows.\nMMD( P, Q) = (EX,X′∼P[k(X, X′)] +EY,Y′∼Q[k(Y, Y′)]−2EX∼P,Y∼Q[k(X, Y)])1/2. (18)\nWe measure the MMD over energy histograms of samples for all tasks.\nThe error intervals in Table 1 for the GMM and MW32 tasks for our method and PTDM were obtained by running the\nmethod with multiple seeds, and estimating the standard deviation. Any runs where the metrics calculation resulted in\nundefined values due to generated outliers in MW32 were discarded. For GMM and MW32 with the baselines and LJ55 in\ngeneral, we estimated the error by sampling multiple times from a trained model. The error intervals for the log-likelihood\nevaluations were obtained by estimating the standard deviation of logpθ(x)evaluations and calculating the standard error\nby dividing by the square root of the amount of samples taken.\nC. Experimental Settings\nC.1. Diffusion Models for PTSD and PT+DM\nNetwork Architecture. For non-particle system, i.e.GMM-40 and MW-32, we employ a 5-layer MLP. For particle\nsystems, i.e.LJ-55 and ALDP, we employ the EGNN implemented by Satorras et al. (2021) with 3 layers.\nParameterization. We parameterize the DMs as a denoiser network to approximate the denoising mean following Karras\net al. (2022), where the σdatais approximated by the standard deviation of the data used for training, i.e.the buffer in PTSD\nand the drawn samples from PT in PT+DM.\nNoise Schedule. Our noise schedule follows Karras et al. (2022), with the maximum time tmax= 40 .\nSampling Process. We employ the Euler solver for sampling, where we discretize the time following Karras et al. (2022).\nParallel Tempering. Our implementation of parallel tempering is based on the codebase of DiGS (Chen et al., 2024b).\nEnergy evaluation optimization. On both LJ55 and ALDP, we use the method of running PT at the temperature\nextrapolation steps only from a subset of IS filtered samples, and concatenate the PT samples to the original IS samples. On\nthe GMM and MW32 tasks, we instead run a chain from each diffusion model generated (and resampled) sample, and take\nonly the final output of the chain.\nTemperature levels. For all experiments, we use a range of geometrically spaced temperatures from a maximum\ntemperature to the desired minimum temperature.\nC.2. Hyperparameters\nPTSD. Hyperparameter settings for PTSD on each task is illustrated in 5. The key hyperparameters are the temperature\nrange ,number of temperatures ,buffer size (amount of samples to train the diffusion models on), the number of initial PT\nsteps at the highest two temperatures ,the number of steps in PT while following the extrapolation steps , and the number of\nPT chains to use at the extrapolation step. For MW32, we did not perform the importance resampling at the last extrapolation\nstep.\nPT+DM. Hyperparameter settings for running PT on different targets are illustrated in Table 6. The batch size for DM\ntraining is the same as PTSD, and we train the DM until converged.\n15\n--- Page 16 ---\nProgressive Tempering Sampler with Diffusion\nTable 5. Hyperparameter settings for PTSD on different targets.\nHyperparameters ↓Target → MoG-40 MW-32 LJ-55 LJ-55 (illustrative)\nTemperature range [1,100] [1 ,10] [1 ,3] [1 ,2]\nNumber of temperatures 10 10 3 3\nTemperature schedule geom geom geom linear\nBuffer Size 10000 12000 20000 20000\nBatch size 1000 1000 1000 1000\nNumber of initial PT chains 100 20 1 1\nNumber of initial PT steps 1000 20000 40000 40000\nPT swap interval 5 5 5 5\nBurn-in at the initial PT 100 10000 5000 5000\nInterval for subsampling the initial PT chain 9 10 1 1\nNumber of generated samples at extrapolation 10000 12000 20000 20000\nNumber of PT chains at extrapolation 10000 12000 10 10\nNumber of PT steps after extrapolation 5 25 2500 2500\nNumber of training iterations 10000 10000 120000 120000\nImportance resampling at last step Yes No Yes Yes\nTable 6. Hyperparameter settings for PT on different targets.\nhyperparams ↓target→ MoG-40 MW-32 LJ-55 LJ-55 (illustrative)\nTemperature range [1,200] [1 ,10] [1 ,3] [1 ,2]\nNum. of temperatures 4 10 3 3\nTemperature schedule geom geom geom linear\nFAB. For both MoG-40 and WM-32, we run FAB following exactly the setting by Midgley et al. (2023), with the\ncode at https://github.com/lollcat/fab-torch/ . As they use buffer which can influence the target density\nevaluation time, we directly count the time when running the code.\nDDS. We evaluate DDS using the implementation by Blessing et al. (2024) with KL divergence following Vargas et al.\n(2023). For MoG-40, we train DDS for 10000 iterations with a batch size of 2000, using Euler-Maruyama discretization\nwith 128 steps. Note that DDS’s network takes a score term as the input, and hence, the total number of target evaluations\nis10000 ×2000×128. For WM-32, we apply early stopping at 3200 iterations, and hence, the total number of target\nevaluations is 3200×2000×128.\nCMCD. We evaluate CMCD using the implementation by Blessing et al. (2024) with KL divergence following Vargas et al.\n(2024). For MoG-40, we train CMCD with a batch size of 2000, using Euler-Maruyama discretization with 128 steps. It\ntakes 17002 iterations to achieve a good performance, and hence the total number of target evaluations is 17002×2000×128.\nFor WM-32, we train CMCD with a batch size of 2000, using Euler-Maruyama discretization with 256 steps. We found\ntraining for more steps may lead to mode collapsing, and hence we early stop at 3200 iterations. The total number of target\nevaluations is 3200×2000×256.\niDEM. We evaluate iDEM using the implementation by Akhound-Sadegh et al. (2024). For all tasks, we use Eyler-\nMaruyama discretization with 1000 steps and 100inner-loops. For MoG-40, we train iDEM for 1000 outer-loops to ensure\nconvergence, with a batch size of 1000 . The marginal scores are estimated by 500MC samples, which is clipped to a\nmaximum norm of 70. The total number of target evaluations is 1000×100×1000×5000 . For MW-32, we train iDEM\nfor180outer-loops, increase the number of MC samples to 1000 , clip the score to a maximum norm of 1000 , and keep the\nother hyperparameter the same. The total number of target evaluations is 180×100×1000×1000 . For LJ-55, we further\nclip the score to a maximum norm of 20, decrease the batch size to 128, and train iDEM for 1000 outer-loops. The total\nnumber of target evaluations is 1000×100×128×1000 .\n16\n--- Page 17 ---\nProgressive Tempering Sampler with Diffusion\nBNEM. We evaluate BNEM using the implementation by OuYang et al. (2024). For all tasks, we use Eyler-Maruyama\ndiscretization with 1000 steps and 100inner-loops, we also clip the score to a maximum norm of 1000 during sampling.\nFor MoG-40, we train BNEM for 150outer-loops to ensure convergence, with a batch size of 1000 . The noised energies\nare estimated by 500MC samples. The total number of target evaluations is 150×100×1000×5000 . For MW-32,\nwe train BNEM with 180outer-loops and change the number of MC samples to 1000 , resulting in the total number of\ntarget evaluations to be 180×100×1000×1000 . For LJ-55, we use the almost the same hyperparameters as MW-32,\nexcept for decreasing the batch size to 128. We train BNEM for 500outer-loops. The total number of target evaluations is\n500×100×128×1000 .\nDiKL. We evaluate DiKL with the implementation by He et al. (2024). For Mog-40, we train for 75000 iterations using a\nbatch size of 1024, and we take 15 AIS steps with 10 samples, with an extra MALA of 5 steps. Therefore, the total number\nof target evaluations is 75000 ×1025×(15×10 + 5) . Similarly, for MW-32, we train it for 50000 iterations, leading to a\ntotal number of target evaluation of 50000 ×1025×(15×10 + 5) .\nC.3. Training DM for ALDP in Cartesian Space\nALDP are chiral molecules, which exist in two indistinguishable forms (L-form and D-form) that are mirror images of each\nother. While in nature, the ALDPs are found in L-form only, therefore we are interested in generating L-form samples.\nHowever, the EGNN implemented by Satorras et al. (2021) cannot distinguish these two forms, we reflect the D-form\nsamples generated by PT+DM to make them into L-form. For PTSD, since the difference in forms doesn’t influence model\ntraining, we leave the D-form samples in all buffers while reflecting them at the last stage only for evaluation.\nD. Supplementary Experiments and Results\nD.1. Ablation Study for Extrapolation\nIn this section, we ablate different methods for extrapolating from higher-temperature DMs to lower-temperature one. In\nparticular, we train two DMs on two temperatures T2< T3, then extrapolate to a lower temperature T1< T2. We consider\nthe following ways:\n1.NN Generalization : train a temperature-conditioned DM Dθ(xt, t, T)onT2andT3, then directly generalize to T1by\nsampling from Dθ(xt, t, T 1).\n2.Auto-diff TE : train the same temperature-conditioned DM, but extrapolate it through the first-order Taylor expansion by\nauto-differentiation, i.e.Eq. (9).\n3.Finite-difference TE : train two distinct standard DMs, then extrapolate by Eq. (11).\nWe conduct experiments on GMM-40, DW-44, LJ-13, and LJ-55 benchmarks. For all experiments, we choose T1= 0.1,\nT2= 1, and T3= 1.3, where T2= 1 is the same as the target distributions introduced in App. A. We use the same\narchitecture introduced in App. C.1, and the training of each model is long enough to ensure convergence. The ground truth\nof each benchmark at T1= 0.1is obtained by running MCMC on top of the T2= 1samples for a long enough time to\nensure mixing.\nFig. 8 showcases the advantage of Finite-difference TE, where the extrapolated samples can be much closer to the ground-\ntruth ones, compared to the other methods.\nD.2. Measuring Performance through Observables\nSamples from the equilibrium ( i.e.target) distribution are used for estimating the observables in many physical problems,\nwhich are mathematically the expectation of an observable function over the target distribution, i.e.⟨O⟩X:=Ep(x)[O(x)].\nTo measure the quality of generated samples by different models, we design a toy observable function, which is a quadratic\n4DW-4 is a 4-particle system in 2-dimensional space introduced by K ¨ohler et al. (2020). To clarify, we follow the notation used in\n(K¨ohler et al., 2020; Akhound-Sadegh et al., 2024) where the number 4indicates the number of particles . While the number 2in DW-2\nindicates the potential is in a 2-dimensional space.\n17\n--- Page 18 ---\nProgressive Tempering Sampler with Diffusion\n(a) NN Generalization\n (b) Auto-diff TE\n (c) Finite-difference TE\n (d) Ground Truth\nFigure 8. Different ways for extrapolation to temperature 0.1. In (d), oranges are Finite-Difference TE; blues are ground-truth, which\ncan also be obtained by running 2 steps, 10 steps, 200 steps, and 200 steps of Langevin MCMC on top of Finite-Difference TE samples,\nrespectively.\n18\n--- Page 19 ---\nProgressive Tempering Sampler with Diffusion\nTable 7. Comparing PTSD with other neural sampler baselines. We measure a toy observable O(X) =Ep(x)[λxTx]. We report\nthe Mean Absolute Error (MAE) corresponding to the ground truth observable, which is estimated by 10000 samples from the target\ndistribution. For each model, we use 10000 samples for estimating O(X). ‘-’ indicates that the method diverges or is significantly worse\nthan others.\nGMM ( d= 2) MW32 ( d= 32 ) LJ55 ( d= 165 )\nFAB 1.91±0.98 4.09±0.14 -\nCMCD 7.03±0.83 3.76±0.12 -\nDDS 6.85±1.36 9.89±0.13 -\niDEM 6.5±0.75 9.28±0.06 28.72±0.02\nBNEM 7.42±0.63 11.09±0.18 9.48±0.01\nDiKL 7.59±0.76 3.24±0.17 -\nPT+DM 12.09±0.81 4.31±0.10 0.34±0.03\nPTSD (ours) 1.35±0.69 1.49±0.16 1.86±0.04\n1061071081091010\nT arget Density Evaluations0.00.10.20.3GMM (d=2)\n1071081091010\nT arget Density Evaluations300\n250\n200\n150\n100\n50\nMW32 (d=32)\n1061081010\nT arget Density Evaluations250\n200\n150\n100\nLJ55 (d=165)\n1081091010\nT arget Density Evaluations65.50\n65.25\n65.00\n64.75\n64.50\n64.25\nLJ55 with 1000 Langevin \nsteps (d=165)\nPareto Frontier\nFAB\nCMCD\nDDS\niDEM\nBNEM\nDiKL\nPT+DM\nPTSD (ours)\nFigure 9. Log-likelihood Ep(x)[logqmodel (x)]and energy evaluations for all the models. To ensure consistency, the log-likelihood is\ncalculated by fitting a diffusion model to samples generated from a given model.\nfunction O(x) = (x−a)TC(x−a). To make the observable function be SE(3)-invariant for LJ system, we remove the\nmean of samples and ensure rotation-invariant for O(x),i.e.O(Rx) =O(x), by taking a= 0andC=I.\nTable 7 reports the MAE of the observable estimated by 10000 samples from different models, where the ground truth is\nobtained by Monte Carlo estimation through 10000 samples from the target distribution.\nD.3. Measuring Performance through Log-Likelihood\nNegative Log-Likelihood (NLL) is a statistical metric that measures the distance between the target distribution pand the\nprobabilistic model pθ, which is computed as follows\nNLL(pθ;p) =−Eplogpθ(x). (19)\nNotice that computing NLL requires access to model density, which is intractable in both our method and most of baselines.\nOnly FAB implemented with a Normalizing Flow has tractable model density. To remedy this, we train an additional\ndiffusion model to 10000 samples from each model and evaluate the log model density of a generated sample x0through\ntheprobability-flow ODE (PF-ODE) as follows\nlogpθ(x0) = log p1(x1) +Z1\n0∇ ·˜f(xt, t)dt, withxt=x1−Zt\n1˜f(xu, u)du. (20)\nNote that, while the above model density is exact and guaranteed by the instantaneous-change-of-variable, bias can be\nintroduced in two ways: (1) the discretization; and (2) the variance of the estimation for the divergence term ∇ ·˜fusing\nHutchinson’s estimator.\nThe results are shown in Fig. 9. PTSD is consistently in the Pareto Frontier with respect to energy function evaluations and\nlog-likelihood on all of the data sets.\n19\n--- Page 20 ---\nProgressive Tempering Sampler with Diffusion\nD.4. Alanine Dipeptide Experiment Setup and Results\nWe used 5 temperature geometrically spaced temperature levels for both the PTDM and PTSD models, spaced geometrically\nfrom 300K to 1500K. We initialised sampling by running a single PT chain on the top two temperatures for 1×107steps\n(total 2×107energy evaluations), and obtained a buffer of 200000 samples by subsampling. We trained the model for\n100000 epochs on those 200000 samples with a learning rate of 2×10−3. At each step, we then generate 100000 samples\non the next level and do IS resampling with the truncation quantile set to 0.8. We pick a random subset of 1000 samples,\nand run PT for 1000 chains for each, and pick every 10th sample from these chains, resulting in 100000 new samples\nthat we mix with the original IS resampled results. This requires 2×1000×1000 = 2 ×106energy evaluations at each\nextrapolation step. We have 3 such steps, and the IS requires 100000 ×3 = 3×105energy evaluations, resulting in a total\nof2×107+ 2×106×3 + 3×105= 2.63×107energy evaluations. We optimised the learning rate and truncation quantile\nhyperparameters to get to our final results. We chose the number 1000 for the amount of PT chains at each extrapolation\nsuch that we get a small enough energy evaluation budget, and early experiments showing that this was enough not to\ndestabilize the training process.\nFor PTDM, we ran parallel tempering for 5.2e6 steps on the same 5 temperature levels, resulting in 5.2×106×5 = 2 .6×107\nenergy evaluations. We trained the model for twice as long as we use to train each individual diffusion model in our PTSD\nimplementation with a learning rate of 4×10−3. We optimized over the learning rate to get to the final results. The setup\nof the 5 temperature levels was chosen after noticing in initial experiments that it works well for parallel tempering in\nparticular, and we did not tune it for PTSD.\nFig. 10 shows the marginals of Fig. 7. Although PTDM does have less bias in the high-probability regions, the relatively\nshort running time for PT has left it unable to effectively model the small mode on the right side of the ϕplot. Table 3 shows\nthe log-likelihoods as estimated with the probability flow ODE. We also evaluated the effective samples sizes (ESS), but\nthe values for both diffusion models were very low (less than 1%) and showed high variance. We hypothesize that this is\ndue to noise in the probability flow ODE causing some outliers with unusually low log-likelihood values, which causes\nthese samples to dominate the importance weight distribution. In the experiments, we use truncated importance sampling\nto counter this. Fig. 11 shows the energy histogram of samples generated by the model, compared to the ground-truth\ndistribution. The energies are in general higher for PTSD, indicating that the distribution is more spread out than the\nground-truh distribution.\nThe reason we run MCMC from a subset of samples is that running it from all of the samples entangles the amount of\nsamples we use for importance sampling with the amount of energy evaluations needed for MCMC. IS and MCMC have\ncomplimentary effects in the model: The benefit of importance sampling is that it is highly energy function evaluation\nefficient, using only one energy function per generated sample. The natural tradeoff is that it requires more compute for\ndiffusion sampling and log-likelihood estimation. On the other hand, a benefit of MCMC sampling is that it can be used to\nincrease the diversity of the IS outputs and obtain better coverage of the distribution for training the next diffusion model.\nAnother advantage is that using correlated samples from the MCMC chains is also energy evaluation efficient. Accordingly,\nwe noticed in our experiments that running a relatively small amount of longer MCMC chains from a large amount of IS\ngenerated samples results in a set of samples that has good distribution coverage while keeping the energy evaluation count\nlow.\nD.5. Additional Visualizations for Generated Samples\nPlots Fig. 12 and Fig. 13 visualise the GMM and MW32 samples from the different baseline models.\nD.6. Results on LJ55 with additional Langevin dynamics\nThe original evaluation scheme for the LJ55 task in (Akhound-Sadegh et al., 2024) involved running additional steps of\nLangevin dynamics on top of the pure neural sampler outputs. In our main table, we chose to follow (OuYang et al., 2024)\nand show the results for the pure neural sampler outputs, to more directly compare the performance of the base generative\nmodels. Here we provide results for the MCMC refined evaluation scheme, where we ran 1000 Langevin dynamics steps for\neach model generated sample. The sample evaluation results are shown in Table 8. The overall pattern is similar to the one\nin Table 1, where BNEM is the best, followed by PTDM, PTSD and iDEM. Fig. 9 shows the log-likelihood results with\nrespect to energy function evaluations, placing PTSD and PTDM at the Pareto frontier.\n20\n--- Page 21 ---\nProgressive Tempering Sampler with Diffusion\n-\n -/2\n 0 /2\n0.00.20.40.60.81.0Marginal density\n-\n -/2\n 0 /2\nGround Truth\nPT+DM\nPTSD\nFigure 10. Marginal distributions of the ϕandψangles in the alanine dipeptide molecule. While PTDM captures the lower part of the ϕ\nvalues more accurately, the small mode on the right side of the ϕplot is better represented by PTSD.\n50\n 0 50 100\nEnergy  / kBT0.000.020.040.06DensityGround truth\nPTSD\nFigure 11. The energy histogram of samples generated by our model on the alanine dipeptide molecule, vs. the energy histogram of\nground-truth samples. The PTSD samples tend have slightly higher energies, indicating that the distribution is slightly more spread out\nthan the ground-truth distribution.\n21\n--- Page 22 ---\nProgressive Tempering Sampler with Diffusion\nFigure 12. Visualising the samples generated by PTSD and other baselines on MoG-40.\nTable 8. LJ55 results for neural sampler methods using the evaluation scheme where we run additional MCMC steps on top of the\ngenerated samples. We measure ( best,second best ) the TVD and MMD between Energy histograms, and W2distance between data\nsamples.\nLJ55 ( d= 165 )\nTVD↓ MMD↓ W2↓\niDEM 0.42±0.006 0.10±0.001 1.83±0.001\nBNEM 0.09±0.004 0.00±0.000 1.81±0.000\nPT+DM 0.15±0.004 0.01±0.001 1.82±0.001\nPTSD 0.29±0.001 0.05±0.001 1.86±0.001\n22\n--- Page 23 ---\nProgressive Tempering Sampler with Diffusion\nFigure 13. Visualising 2-dimensional slices of samples generated by PTSD and other baselines on MW-32.\n23",
  "text_length": 82274
}