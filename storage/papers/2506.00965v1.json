{
  "id": "http://arxiv.org/abs/2506.00965v1",
  "title": "Unlocking Personalized Knowledge in Federated Large Language Model: The\n  Power of Mixture of Experts",
  "summary": "The Mixture of Experts (MoE) architecture has emerged as a prominent strategy\nfor scaling large language models (LLMs), effectively leveraging sparse\nactivation and facilitating task-specific personalization. However, current\nfederated learning (FL) approaches are primarily designed for dense models,\nmaking them unable to directly exploit the sparsity inherent in MoE\narchitectures. Treating MoE models as dense networks in federated scenarios\nresults in excessive communication overhead and computational costs,\nundermining the potential for personalized knowledge sharing. To address these\nchallenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel\nfederated learning framework explicitly tailored for MoE-based LLMs. FLEx\nefficiently personalizes by pruning the global MoE model to keep only one\nexpert per client, and employs an adaptive gating mechanism to reintegrate\nthese personalized experts into the pre-trained MoE layers, ensuring the\noriginal backbone architecture remains unchanged. These personalized experts\nare trained with local data and stored locally on each client, while the shared\nmodules are aggregated globally. Extensive evaluations on diverse\ninstruction-based datasets under non-IID conditions consistently demonstrate\nthat FLEx outperforms existing federated baselines. Our code is available at\nhttps://anonymous.4open.science/r/FLEx-8F12.",
  "authors": [
    "Fan Liu",
    "Bikang Pan",
    "Zhongyi Wang",
    "Xi Yao",
    "Xiaoying Tang",
    "Jingya Wang",
    "Ye Shi"
  ],
  "published": "2025-06-01T11:24:43Z",
  "updated": "2025-06-01T11:24:43Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00965v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00965v1  [cs.AI]  1 Jun 2025Unlocking Personalized Knowledge in Federated Large\nLanguage Model: The Power of Mixture of Experts\nFan Liu1Bikang Pan1Zhongyi Wang1Xi Yao2\nXiaoying Tang3Jingya Wang1Ye Shi1∗\n1ShanghaiTech University\n2China Mobile Communications Company Limited Research Institute\n3The Chinese University of Hong Kong, Shenzhen\n{v-liufan, panbk2023, wangjingya, shiye}@shanghaitech.edu.cn\nzywang2003@163.com\nyaoxi@cmsr.chinamobile.com\ntangxiaoying@cuhk.edu.cn\nAbstract\nThe Mixture of Experts (MoE) architecture has emerged as a prominent strategy\nfor scaling large language models (LLMs), effectively leveraging sparse activation\nand facilitating task-specific personalization. However, current federated learning\n(FL) approaches are primarily designed for dense models, making them unable\nto directly exploit the sparsity inherent in MoE architectures. Treating MoE mod-\nels as dense networks in federated scenarios results in excessive communication\noverhead and computational costs, undermining the potential for personalized\nknowledge sharing. To address these challenges, we propose FLEx (Federated\nLLMs with Personalized Experts), a novel federated learning framework explicitly\ntailored for MoE-based LLMs. FLEx efficiently personalizes by pruning the global\nMoE model to keep only one expert per client, and employs an adaptive gating\nmechanism to reintegrate these personalized experts into the pre-trained MoE\nlayers, ensuring the original backbone architecture remains unchanged. These\npersonalized experts are trained with local data and stored locally on each client,\nwhile the shared modules are aggregated globally. Extensive evaluations on di-\nverse instruction-based datasets under non-IID conditions consistently demonstrate\nthat FLEx outperforms existing federated baselines. Our code is available at\nhttps://anonymous.4open.science/r/FLEx-8F12 .\n1 Introduction\nLarge language models (LLMs) have seen a remarkable increase in both scale and capability in recent\nyears, largely due to the rise of the Mixture-of-Experts (MoE) paradigm [ 7,3,16]. By activating\nonly a small subset of expert sub-networks per token, MoE architectures separate the number\nof parameters from the computational cost, enabling state-of-the-art performance at significantly\nlower inference overhead compared to dense Transformers of similar size. Recent advances [ 2,19]\nhighlight that MoEs are not just an efficient compression strategy, but a powerful means of task-\nspecific personalization, where experts naturally specialize in different linguistic styles, domains, or\nuser preferences.\n∗Corresponding author.\nPreprint. Under review.\n--- Page 2 ---\nMeanwhile, federated learning (FL) has become a compelling framework for training and adapting\nLLMs on privacy-sensitive data that remains on users’ devices [ 28,26,11,23]. However, nearly\nall existing FL algorithms for LLMs are designed for dense model architectures. Applying them\ndirectly to MoE models requires each client to transmit all experts, even those unrelated to its local\ndata, resulting in high communication costs and diminished personalization. Early attempts to bridge\nMoE and FL [29, 15, 13] were developed for traditional models rather than trillion token pretrained\nLLMs. While they prove that federated MoE training is feasible, they still require each client to\nmanage and transmit multiple experts, including those that are not activated by local data, resulting in\nunnecessary communication overhead, which makes these methods economically impractical and\nleaves a significant efficiency gap in federated training of large-scale pretrained LLMs.\nDeploying MoE-based LLMs in a federated setting introduces two main challenges. First, the\ncommunication and computation costs that arise from the sparse structure of MoE models are\nhigh, especially under limited bandwidth and constrained local device resources. Second, existing\nmethods primarily focus on personalized experts trained from local data while neglecting global\ninformation, failing to achieve an effective balance between personalization and global knowledge\nwithin MoE-based LLMs.\nTo address these challenges, we propose FLEx ( Federated LLMs with Personalized Experts), a\nsimple framework that combines the sparsity of Mixture-of-Experts models with the benefits of\nfederated personalization. In FLEx, each client selects and fine-tunes only one expert from the global\nMoE model, tailored to its local data. During fine-tuning, a lightweight gating module integrates\nthe personalized expert’s output into the original MoE layer, allowing the model to leverage expert\nspecialization without altering the backbone architecture. In FLEx, clients locally update their chosen\nexpert and specific parameters in the shared module. Only the shared module updates are then\ncommunicated for aggregation into the global model, ensuring shared improvements, while the\nexpert’s modifications remain local to the client for personalization. Our main contributions are as\nfollows:\n•We propose FLEx , a novel federated learning framework explicitly tailored for MoE-based\nlarge language models. FLEx addresses the lack of federated learning approaches specifically\ndesigned for sparse MoE architectures, effectively balancing global knowledge sharing and\nlocal personalization, thereby significantly reducing communication and computational\ncosts compared to traditional FL methods.\n•FLEx efficiently personalizes by pruning the MoE model to retain a single expert per client\nand using an adaptive gating mechanism to reintegrate personalized experts into the pre-\ntrained MoE layers without altering the original backbone architecture. These personalized\nexperts are trained using local data and stored locally on each client, while the shared\nmodules are aggregated globally. This design maximally preserves pre-trained knowledge,\nleverages personalized experts to enhance client-specific capabilities, benefits from shared\nglobal knowledge aggregation, and significantly reduces communication overhead.\n•Extensive evaluations conducted on multiple challenging instruction-based datasets, includ-\ning Databricks-dolly-15k, Alpaca-GPT4, Finance-Alpaca, and MedAlpaca, under non-IID\nscenarios demonstrate that FLEx consistently outperforms existing federated learning base-\nlines across diverse tasks.\n2 Related Work\n2.1 Federated Learning on Large Language Models\nFederated Learning (FL) has gained significant attention as a promising method for training Large\nLanguage Models (LLMs), addressing challenges such as decentralized data management and privacy\nconcerns [ 28,26,23]. Recent surveys [ 25,26] have identified important intersections between FL\nand LLMs, particularly in areas such as efficient foundation model training, federated fine-tuning\ntechniques, and the collaborative potential of FL in advancing LLM development. Notable contribu-\ntions in this domain include FedIT [ 28], which demonstrated the utility of FL in instruction tuning\ntasks for LLMs. Further innovations involve personalized LLM training methods like DualLoRA\n[11] and FDLoRA [ 18], which combine federated learning with personalized adapter methods. Other\napproaches such as FRLoRA [ 24], FLoRA [ 23], and FlexLoRA [ 1] introduce new aggregation strate-\n2\n--- Page 3 ---\nFigure 1: Overview of FLEx framework. The FLEx framework begins by pruning personalized\nexperts for each client using local data. The next step involves injecting personalized knowledge into\nthe MoE layer via a gating mechanism, striking a balance between global knowledge sharing and\nlocal adaptation.\ngies designed to optimize low-rank fine-tuning and allow more flexible model updates in federated\nsettings. Additionally, selective aggregation methods, like those presented in [ 5], further improve\nfederated LLM training by refining the aggregation process.\n2.2 Mixture of Experts\nMixture-of-Experts (MoE) [ 7,3] models activate only a subset of expert layers during inference or\ntraining, which significantly reduces computational costs while maintaining high performance. In\nrecent, MoE architectures have gained attention for their scalability and efficiency in large language\nmodels [16, 2, 19].\nHowever, existing federated MoE approaches [ 27,13] are primarily designed for dense model\narchitectures. Applying these methods directly to MoE models can result in substantial computational\nand communication overhead. To address this limitation, we propose FLEx , a novel framework\nspecifically designed for federated training of MoE-based LLMs.\n2.3 Expert Pruning for LLMs\nModel pruning is a widely used technique to improve communication and computation efficiency in\ndeep learning. In the context of MoE-based LLMs, recent pruning methods, such as Wanda [ 21] and\nSparseGPT [ 4], offer effective strategies to reduce the number of model parameters. Additionally,\nrecent studies [ 12,10] have demonstrated that many experts are either unimportant or redundant\nduring inference on specific benchmarks. However, in federated learning with MoE-based LLMs,\ndirectly pruning certain experts may result in the loss of essential foundational knowledge. To address\nthis, we propose pruning personalized side experts on local data. Furthermore, we integrate the\npruned experts back into the original pretrained model to achieve an optimal balance between local\nadaptation and global generalization.\n3 Preliminary\nFederated Learning Setup We consider a federated learning (FL) setup with Kclients, each\nholding a private dataset Dk. In this setup, each client independently trains a model on its local data,\nwhile the central server periodically aggregates the updates to improve the global model. A widely\nadopted approach, the Federated Averaging (FedAvg) algorithm, aggregates the global parameters by\ncomputing a weighted average of the local parameters:\nWt+1=NX\ni=1ni\nnWt+1\ni, (1)\n3\n--- Page 4 ---\nwhere n=PN\ni=1niis the total number of training samples across all clients, and Wt+1\niandWt+1\nare weight of i-th client and server in t+ 1-th round. This approach enables the global model to\nleverage distributed data while maintaining privacy.\nMixture of Experts in Large Language Models The Mixture of Experts (MoE) architecture\nintroduces a sparse activation mechanism, where only a subset of experts is activated for each input.\nThis design enables large-scale models to scale efficiently while reducing computational costs. In\nmodern LLMs, the backbone consists of transformer layers, each comprising a self-attention layer\nand a feedforward network (FFN) layer:\nul\n1:T=Self-Att\u0000\nhl−1\n1:T\u0001\n+hl−1\n1:T, (2)\nhl\nt=FFN\u0000\nul\nt\u0001\n+ul\nt, (3)\nwhere ul\n1:Tis hidden states of all tokens after the l-th attention module and hl\ntis the output hidden\nstate of the t-th token after the l-th Transformer block. In MoE-based models, the FFN layer is\nreplaced by a MoE layer, which consists of multiple experts. This substitution yields:\nhl\nt=NX\ni=1\u0000\ngi,tFFN i\u0000\nul\nt\u0001\u0001\n+ul\nt, (4)\nwhere Ndenotes the total number of experts, and gi,tis the gating function that selects which experts\nare activated for each token t. The gating function gi,tis defined as:\ngi,t=\u001asi,t,ifsi,t∈Topk ({sj,t|1≤j≤N}, K)\n0, otherwise ,(5)\nwhere si,tis the relevance score for expert ifor token t, computed using the softmax function:\nsi,t=Softmax i(router (ut)). (6)\nHere, the router function router (ut)computes the raw scores for each expert based on the token’s\nrepresentation ut, and the softmax function Softmax inormalizes these scores to determine the\nrelevance of each expert. The gating mechanism activates only the top Kexperts for each token t,\nwhich significantly reduces computational costs. However, if we treat MoE-based LLMs as dense\nmodels and aggregate the entire model, the communication cost would grow significantly, making it\nimpractical. To address this, we propose FLEx and unlock the power for experts in LLMs.\n4 Unlock the Power of Experts: FLEx\nFederated learning (FL) enables distributed model training across multiple clients, each holding\na private dataset. Mixture of Experts (MoE) models employ sparse activation, activating only a\nsmall subset of experts for each input. This sparsity makes it impractical to aggregate the entire\nmodel across all clients due to significant communication overhead. In this paper, we propose FLEx,\na federated learning approach that optimizes MoE models by pruning the MoE layer to retain a\npersonalized expert for each client. FLEx reduces communication costs, maintains high performance,\nand preserves knowledge from the original pre-trained MoE model. By integrating personalized\nexperts into each client’s model, FLEx strikes a balance between global model benefits and local data\nneeds. The framework of FLEx is shown in Figure 1.\n4.1 Selective Aggregation in FLEx\nIn MoE-based LLMs, communication efficiency is crucial. FedAvg, the standard federated training\nalgorithm, faces challenges when aggregating both dense attention layers and sparse MoE layers.\nWhile MoE layers activate only a subset of experts for each input, the attention layers in LLMs are\ndense and require substantial resources for aggregation in a federated setup. To address this, we\npropose a hybrid approach. The attention layers remain shared and aggregated globally, while the\nMoE layers are pruned locally. Instead of aggregating all MoE parameters, we fix the MoE layer’s\nweights and prune it to retain a personalized expert for each client.\n4\n--- Page 5 ---\n4.2 Personalized Experts via MoE Pruning\nTo further enhance personalization and computational efficiency in federated learning, inspired by\n[12], we introduce a pruning strategy based on MoE layers. This approach allows each client to select\nthe most suitable subset of experts for their local data, maximizing personalization while avoiding\nthe redundancy and resource consumption that may arise during global aggregation. Next, we will\ndetail the specific steps and optimization strategies involved in this process.\nEach client icomputes the output of all experts for a given input x∈ Di. Let F(l)(x)denote the\noutput of the MoE layer, and let S⊂ {1, . . . , K }represent the subset of experts retained after\npruning. The pruned output at layer lis given by F′(l)(x, S).\nFor personalization, each client selects the subset of experts that best approximates the original MoE\noutput for their local dataset at each layer by minimizing the reconstruction loss, as shown in Step 1\nof Figure 1. This optimization problem for a given layer lis formulated as:\nS(l)\ni= arg min\nS⊂{1,...,K},|S|=n \n1\n|Di|X\nx∈D i∥F(l)\nS(x)−F(l)(x)∥F!\n.\nTo simplify the problem, we reduce the search space by setting n= 1, choosing only a single\nexpert per client. This reduces the computational complexity while ensuring effective personalization.\nThrough this optimization problem, we greedyly select the personalized side experts at each layer.\n4.3 Personalized Layer Integration\nOnce the personalized expert is selected, it is integrated into the global model by combining it with\nthe global MoE layer, as shown in Step 2 of Figure 1. During the federated aggregation process,\nthe LoRA adapter for the attention layer is aggregated globally, while the LoRA adapter for the\npersonalized expert remains on the client side. This approach ensures that each client benefits from\nboth the global model’s knowledge and the personalized expert’s adaptation to local data. Then the\nforward equation of MoE layer (4)\nhl\nt=NX\ni=1\u0000\ngi,tFFN i\u0000\nul\nt\u0001\u0001\n+gt\neFFN e(ul\nt) +ul\nt, (7)\ngt\ne=sigmoid (router e(ul\nt)), (8)\nwhere router eandFFN edenote the personalized router and personalized side expert generated by\npruned experts, respectively. Here, we keep the original FFN layers fixed to ensure that our method\npreserves as much knowledge as possible for trillion-token pretrained MoE-based LLMs.\nRemark. Our method retains the pre-trained weights in the MoE layer while adding a personalized\nside expert. This design maximally preserves pre-trained knowledge, leverages personalized experts\nto enhance client-specific capabilities, benefits from shared global knowledge aggregation, and\nsignificantly reduces communication overhead.\n5 Experiments\nIn this section, we will use substantial experiments to demonstrate the effectiveness of FLEx.\n5.1 Experimental Setup\nDatasets and Data Partition. To simulate real-world scenarios, we utilize the Databricks-dolly-\n15k2dataset, an open-source collection of instruction follow-up records generated by numerous\nDatabricks employees. This dataset covers a variety of behavioral categories as described in the\nInstructGPT paper [ 17] and includes eight subtasks. We select four subtasks—classification, closed\nQA, information extraction, and summarization—to create a non-IID scenario. These tasks have fixed\n2https://huggingface.co/datasets/databricks/databricks-dolly-15k\n5\n--- Page 6 ---\nresponses, unlike open QA or creative writing tasks. Therefore, ROUGE-L serves as an effective\nevaluation metric to assess the instruction followability capability of large language models.\nImplementation. We utilize the OpenFedLLM framework [ 26] to evaluate federated learning\nmethods, including FedAvg [ 14], FedProx [ 9], SCAFFOLD [ 8], FedAvgM [ 6], FedAdagrad [ 20],\nFedYogi [ 20] and FedAdam [ 20]. Each client trains on 20 samples per round for 100 rounds. We\nselect the same round checkpoint for each client to compare ROUGE-L scores. The Qwen1.5-MoE-\nA2.7B model3, configured with bfloat16, serves as the base model and is trained on NVIDIA H20\nGPUs with 96GB of memory. We apply LoRA to the attention layer with a rank of 32 and an alpha of\n64. The Adam optimizer is used to train the LoRA adapter with a batch size of 4, using a consistent\nlearning rate of 4e-5. The maximum input sequence length is set to 2048, and we provide the Alpaca\ntemplate as a prompt.\nTable 1: Rouge-L performance comparison of different federated learning strategies, including our\nproposed method, on the Databricks-dolly-15k dataset under a non-IID scenario (one subtask per\nclient). The results are reported for four distinct instruction-based tasks: classification, closed QA,\ninformation extraction, and summarization, using the Rouge-L metric to evaluate instruction followa-\nbility. The best result over federated method is bolded and the second-best result is underlined . CLF:\nClassification, CQA: Closed Question Answering, IE: Information Extraction, Summ: Summarization\nDatabricks-dolly-15k MMLU\nMethod CLF CQA IE Summ Avg Avg\nLocal Training 51.90 34.34 40.93 40.37 41.88 44.32\nMoE + FedAvg 51.39 34.52 37.23 41.54 41.17 43.91\nMoE + FedAvgM 51.69 36.72 38.54 42.54 42.37 40.13\nMoE + FedAdam 49.77 35.88 37.45 41.11 41.05 42.57\nMoE + FedAdagrad 50.09 36.38 38.71 42.24 41.85 47.06\nMoE + SCAFFOLD 51.20 34.45 37.93 41.75 41.33 45.01\nMoE + FedProx 51.55 34.71 35.07 41.71 40.76 44.79\nMoE + FedYogi 49.73 36.67 36.27 41.36 41.00 44.05\nFLEx (Ours) 52.10 38.23 40.13 42.09 43.13 49.74\n5.2 Performance under Pathological Non-IID Distribution\nWe first consider pathological data heterogeneity, where we assign one subtask per client, with a total\nof four clients. As shown in Table 1, we compared our methods with baseline methods. From this\ntable, we can observe that FLEx (Ours) has the highest average performance (43.13), outperforming\nall other methods. From an averaging perspective, MoE+FedAvgM (42.37) is the next closest\ncompetitor. Also, performance seems to vary by task, except summarization task, FLEx achieves the\nhighest score. From this table, we can observe that the proposed method consistently leads in average\nperformance, indicating its effectiveness across various tasks. Besides, it can be observed that due to\nthe powerful pretrained model, simply apply averaging as traditional federated paradigm will easily\ncannot outperform simple local update and only our method and MoE+FedAvgM can ourperform\nlocal update.\nWe also evaluated the performance of each client’s checkpoint on the MMLU-STEM benchmark\n[22] to assess the extent of knowledge forgetting after personalization. Specifically, each client’s\nmodel was tested individually, and the average performance was calculated. The results indicate\nthat FLEx (Ours) preserves the most general knowledge while maintaining strong performance\nin personalization. In contrast, MoE+FedAvgM, which showed competitive results in terms of\npersonalization, demonstrated the greatest loss of general knowledge, highlighting its tendency to\nforget important shared knowledge during the personalization process. This further demonstrates\nthe effectiveness of our FLEx method, which strikes a better balance between general knowledge\nretention and personalization.\nTable 2 presents the ROUGE-L performance comparison of different federated learning strategies,\nincluding our proposed method, FLEx, on datasets that simulate real-world user scenarios: Alpaca-\n3https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B\n6\n--- Page 7 ---\nTable 2: Rouge-L performance comparison of different federated learning strategies, including our\nproposed method, on the Alpaca-gpt4, Finance-Alpaca and MedAlpaca dataset under a non-IID\nscenario (one subtask per client). The results are using the Rouge-L metric to evaluate instruction\nfollowability. The best result over federated method is bolded and the second-best result is underlined .\nMethod Alpaca-gpt4 Finance-Alpaca MedAlpaca Avg\nLocal Training 31.31 28.64 30.93 30.29\nMoE + FedAvg 30.11 28.17 30.06 29.44\nMoE + FedAvgM 28.90 27.18 28.78 28.28\nMoE + FedAdam 29.23 28.70 31.18 27.70\nMoE + FedAdagrad 30.24 28.79 30.60 29.87\nMoE + SCAFFOLD 29.96 28.38 28.92 29.08\nMoE + FedProx 30.05 28.60 30.01 29.55\nMoE + FedYogi 29.73 28.42 30.51 29.55\nFLEx (Ours) 31.54 29.89 31.31 30.91\ngpt4, Finance-Alpaca, and MedAlpaca. The experiment was conducted in a highly non-IID setting,\nwhere each client was assigned a domain-specific dataset.\nFLEx outperforms all other methods across all datasets, achieving the highest ROUGE-L scores:\n31.54 for Alpaca-gpt4, 29.89 for Finance-Alpaca, and 31.31 for MedAlpaca, with an average score of\n30.91. This demonstrates its robustness in handling diverse and specialized data. The second-best\nperforming method, MoE + FedAdagrad, shows an average score of 29.87, but its performance is\ninconsistent. While it ranks second on MedAlpaca (31.18), its performance on Alpaca-gpt4 and\nFinance-Alpaca is lower, highlighting its instability. Interestingly, federated averaging methods\n(e.g., MoE + FedAvg) and other federated optimizers (MoE + FedAvgM, MoE + FedAdam) fail\nto outperform the local update method (Local), which achieves an average score of 30.29. This\nemphasizes that traditional federated learning approaches struggle to effectively aggregate models in\nhighly non-IID scenarios, sometimes even underperforming compared to isolated local training.\nThese results reinforce the rationale behind our choice of domain-specific datasets like Finance-\nAlpaca and MedAlpaca, which simulate the specialized needs of real-world users. FLEx excels in\nsuch complex, heterogeneous data environments by capturing personalized knowledge and effectively\nleveraging domain-specific features, outperforming both baseline methods and local training.\n5.3 Performance under Dirichlet Non-IID Distribution\nCompared to previous experiments on pathological non-IID scenerios, we introduce dirichlet non-IID\ndistributions into our experiments as shown in Table 3. In federated learning (FL), the Dirichlet\nnon-IID distribution refers to the scenario where data is non-identically and independently distributed\nacross clients, with each client’s data distribution being generated from a Dirichlet distribution,\ntypically used to model the unevenness of data distribution in real-world scenarios. Here, we choose\nα= 1.0as our dirichlet distribution parameter.\nWe introduced the Dirichlet non-IID distribution to further evaluate the robustness of our proposed\nFLEx method in a more realistic and challenging scenario. The Dirichlet distribution, with its ability\nto model the unevenness of data distribution across clients, is commonly used in federated learning to\nsimulate real-world data heterogeneity. By increasing the number of clients to 10 in this experiment,\nwe aimed to simulate a more complex and distributed environment, which is closer to practical\ndeployment scenarios in federated learning.\nTo ensure fairness and consistency, we selected a fixed communication round and evaluated the\nperformance using the aggregated checkpoints of four clients per round, chosen based on their data\ndistributions. For each task, the client whose data distribution most closely matches the task’s focus\nwas selected. For example, if a client’s dataset contained the majority of samples for a specific\ntask, the evaluation for that task was conducted based on the corresponding client’s model. This\napproach ensures that the task-specific performance is reflective of the data the clients are primarily\nworking with, making the evaluation more aligned with the real-world distribution of data. When\nanalyzing the results for α= 0.1, FLEx (37.30) ranks closely behind the top method, MoE +\n7\n--- Page 8 ---\nTable 3: ROUGE-L performance comparison of different federated learning strategies, including\nour proposed method, on the Databricks-dolly-15k dataset under a Dirichlet distribution with 10\nclients, aggregating 4 clients per communication round. The results are reported for four distinct\ninstruction-based tasks: classification, closed QA, information extraction, and summarization, using\nthe ROUGE-L metric to evaluate instruction following ability.\nDatabricks-dolly-15k\nMethod alpha=0.1 alpha=1.0\nLocal Training 35.18 36.88\nMoE + FedAvg 35.29 37.51\nMoE + FedAvgM 37.41 35.94\nMoE + FedAdam 34.69 35.79\nMoE + FedAdagrad 36.84 36.84\nMoE + SCAFFOLD 34.08 36.91\nMoE + FedProx 35.05 36.68\nMoE + FedYogi 35.26 36.41\nFLEx (Ours) 37.30 37.54\nFedAvgM (37.41). It is important to note that as a personalized federated learning approach, FLEx is\nevaluated on each client’s personalized checkpoint, rather than a single global model. This distinction\ninfluences its performance relative to methods optimized solely for global averages. The relatively\nclose performance values observed for α= 1.0in this experiment are characteristic of the Dirichlet\ndistribution; higher αvalues in this distribution lead to more balanced data heterogeneity across\nclients, which tends to reduce the performance gap between different FL methods compared to\npathological non-IID scenarios. The key takeaway is that FLEx remains competitive even in scenarios\nwith less extreme data heterogeneity. Its ability to effectively balance personalization with adaptation\nto a broader data distribution underscores its robustness and adaptability to diverse client needs,\nstanding out from methods focused solely on global performance.\n5.4 Performance under LLM-as-a-Judge Evaluation\nIn this section, we evaluate the performance of FLEx on an open-ended benchmark Vicuna from\nhelpfulness and harmlessness in two aspects. This experiment was conducted on a set of 20 clients\nunder an IID (Independent and Identically Distributed) scenario to assess the performance of FLEx in\nopen-ended generation tasks, specifically focusing on the aspects of Helpfulness and Harmlessness.\nThe goal of this experiment was to evaluate the model’s ability to maintain high-quality, useful, and\nsafe responses in a more balanced and homogeneous data setting. This IID setting allows for a more\nstraightforward comparison of the different methods under conditions where the data distribution is\nuniform across clients, ensuring that the performance differences observed are primarily due to the\nmethods themselves rather than data heterogeneity.\nThe left side of Figure 2 visualizes the distribution of Helpfulness and Harmlessness scores for\nOriginal Model, FLEx and Local. The scatter points represent individual evaluations, while the kernel\ndensity contours show the overall distribution of scores. The larger markers indicate the average\nscores for each method. We can see that FLEx not only achieves higher average scores but also\nexhibits a more concentrated and consistent performance, especially in the Harmlessness dimension.\nFrom the table on the right, we observe that FLEx outperforms all other methods in both Helpfulness\n(6.360) and Harmlessness (7.993), achieving the highest scores in both categories. This is particularly\nnotable in the Harmlessness aspect, where FLEx significantly exceeds all other methods, including\nMoE + FedAdagrad (the second-best method).\nThese results demonstrate the advantages of FLEx in open-ended generation tasks, particularly in\ncapturing the subtle distinctions of helpfulness and harmlessness. Compared to other methods,\nFLEx, through its personalized client models, is able to more finely capture the nuanced features of\nhelpful and harmless content. In terms of helpfulness, FLEx performs exceptionally well, generating\ncontent that is not only valuable to users but also accurate and effective. In terms of harmlessness,\nFLEx stands out especially, as it effectively avoids generating potentially harmful or unsafe outputs,\nsignificantly surpassing other methods.\n8\n--- Page 9 ---\nVicuna\nMethod Helpfulness Harmlessness\nBase Model 5.145 6.319\nLocal Training 6.026 7.096\nMoE + Fedavg 6.159 7.661\nMoE + FedAvgM 6.043 7.527\nMoE + FedAdam 6.267 7.577\nMoE + FedAdagrad 6.314 7.780\nMoE + SCAFFOLD 5.908 7.248\nMoE + FedProx 5.949 7.475\nMoE + FedYogi 5.964 7.368\nFLEx (Ours) 6.360 7.993\nFigure 2: The figure on the left visualizes the distribution and central tendency of Helpfulness and\nHarmlessness scores for different methods (Original Model, FLEx, Local). Scatter points and kernel\ndensity contours represent the data distribution, with larger markers indicating the average scores.\nThe table on the right presents the average Helpfulness and Harmlessness scores across evaluated\nmethods, demonstrating performance in these two key aspects of open-ended generation. FLEx\n(Ours) achieves the highest average scores in both Helpfulness (6.360) and Harmlessness (7.993),\nshowing significant improvement, particularly in harmlessness.\n5.5 Communication Cost Analysis\nWe compare the communication cost of FLEx on MoE-based LLMs. Specifically, we examine\nthe communication cost when applying the LoRA adapter to all MoE experts and transmitting\nall the experts across clients on Qwen1.5-MoE-A2.7B. The traditional FL cost is approximately\n3.41% of the total parameters (i.e., 889M), while the FLEx cost is nearly 0.0659% (i.e., 17.2M)\nper communication round. It is evident that FLEx uses significantly lower communication costs to\nachieve high performance.\n5.6 Ablation Study\nTable 4: Ablation Study Results. ✓indicates the component\nis included, and ✗indicates it is excluded. Here, \"No Person-\nalized\" denotes that each client prunes the expert using the\nC4 dataset, a pre-training dataset that does not contain per-\nsonalized data. Conversely, \"Personalized\" refers to pruning\nthe expert using the client’s own data.\nDatabricks-dolly-15k\nPruning Personalized Adaptive Gate Rouge-L\n✗ ✗ ✗ 41.17\n✓ ✗ ✗ 5.16\n✓ ✓ ✗ 7.06\n✓ ✗ ✓ 42.67\n✓ ✓ ✓ 43.14In this ablation study, we evaluate the\ncontributions of the three core compo-\nnents of the FLEx framework: Prun-\ning, Personalized Pruning, and Adap-\ntive Gate. Experiments were con-\nducted using the Databricks-dolly-15k\ndataset with FedAvg as the aggrega-\ntion method and performance evalu-\nated using the ROUGE-L metric. Re-\nsults are summarized in Table 4. The\nbaseline model, without any compo-\nnents enabled, achieved a ROUGE-\nL score of 41.17. Notably, enabling\nonly Pruning drastically reduced the\nperformance to 5.16, underscoring the\ncritical necessity of personalization.\nIncorporating Personalized Pruning\nalongside Pruning slightly improved\nthe score to 7.06, yet the performance remained limited, demonstrating that personalization alone\nis insufficient without effectively leveraging client-specific expertise. Enabling the Adaptive Gate\nraised performance substantially to 42.67, surpassing the baseline. Finally, integrating all three\n9\n--- Page 10 ---\ncomponents—Pruning, Personalized Pruning, and Adaptive Gate yielded the highest performance of\n43.14. This result underscores the critical role personalization plays within the FLEx framework, high-\nlighting that its full potential is best realized when combined with the dynamic selection capability of\nadaptive gating to optimally accommodate diverse client datasets.\n6 Conclusion\nIn this paper, we introduced FLEx, a federated learning framework tailored for MoE-based large\nlanguage models. FLEx reduces communication and computation costs by allowing each client to\nfine-tune and retain a single, local expert while aggregating only the necessary updates from multi-\nhead attention layers. This design effectively balances local personalization with global knowledge\nsharing, achieving up to 50 ×reduction in communication overhead compared to traditional methods.\nExtensive evaluations show that FLEx outperforms existing federated learning baselines on non-IID\ntasks, demonstrating its potential for efficient and scalable deployment of MoE-based LLMs in\nprivacy-sensitive settings. Future work will focus on improving scalability and robustness across\ndiverse applications.\nAcknowledgement\nThis work was supported by National Natural Science Foundation of China (62303319, 62406195),\nShanghai Local College Capacity Building Program (23010503100), ShanghaiTech AI4S Initiative\nSHTAI4S202404, HPC Platform of ShanghaiTech University, Core Facility Platform of Computer\nScience and Communication of ShanghaiTech University, and MoE Key Laboratory of Intelligent\nPerception and Human-Machine Collaboration (ShanghaiTech University), and Shanghai Engineering\nResearch Center of Intelligent Vision and Imaging.\nReferences\n[1]Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. Federated Fine-tuning of\nLarge Language Models under Heterogeneous Tasks and Client Resources. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems .\n[2] DeepSeek-AI. DeepSeek-V3 Technical Report, February 2025.\n[3]William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity. Journal of Machine Learning Research ,\n23(120):1–39, 2022.\n[4]Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned\nin one-shot. In International Conference on Machine Learning , pages 10323–10337. PMLR,\n2023.\n[5]Pengxin Guo, Shuang Zeng, Yanran Wang, Huijie Fan, Feifei Wang, and Liangqiong Qu.\nSelective Aggregation for Low-Rank Adaptation in Federated Learning, March 2025.\n[6]Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the Effects of Non-Identical\nData Distribution for Federated Visual Classification, September 2019.\n[7]Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive\nmixtures of local experts. Neural Computation , 3(1):79–87, 1991.\n[8]Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and\nAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In\nInternational Conference on Machine Learning , pages 5132–5143. PMLR, 2020.\n[9]Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia\nSmith. Federated optimization in heterogeneous networks. Proceedings of Machine learning\nand systems , 2:429–450, 2020.\n10\n--- Page 11 ---\n[10] Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B. Blaschko, Shengen Yan, Guohao\nDai, Huazhong Yang, and Yu Wang. Efficient Expert Pruning for Sparse Mixture-of-Experts\nLanguage Models: Enhancing Performance and Reducing Inference Costs, July 2024.\n[11] Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. Dual-personalizing adapter for\nfederated foundation models. Advances in Neural Information Processing Systems , 37:39409–\n39433, 2024.\n[12] Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and\nHongsheng Li. Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-\nof-Experts Large Language Models, May 2024.\n[13] Jun Luo, Chen Chen, and Shandong Wu. Mixture of Experts Made Personalized: Federated\nPrompt Learning for Vision-Language Models, April 2025.\n[14] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial\nIntelligence and Statistics , pages 1273–1282. PMLR, 2017.\n[15] Hanzi Mei, Dongqi Cai, Ao Zhou, Shangguang Wang, and Mengwei Xu. FedMoE: Personalized\nFederated Learning via Heterogeneous Mixture of Experts. Corr , 2024.\n[16] Mixtral.AI. Mixtral of Experts, January 2024.\n[17] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems ,\n35:27730–27744, 2022.\n[18] Jiaxing QI, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, and Depei Qian.\nFDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning,\nJune 2024.\n[19] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\nKeqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang\nSu, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5\ntechnical report, 2025.\n[20] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcný,\nSanjiv Kumar, and H. Brendan McMahan. Adaptive Federated Optimization, September 2021.\n[21] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A Simple and Effective Pruning\nApproach for Large Language Models, May 2024.\n[22] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex\nZhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging\nmulti-task language understanding benchmark. In A. Globerson, L. Mackey, D. Belgrave, A. Fan,\nU. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing\nSystems , volume 37, pages 95266–95290. Curran Associates, Inc., 2024.\n[23] Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, and Ang\nLi. FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank\nAdaptations, September 2024.\n[24] Yunlu Yan, Chun-Mei Feng, Wangmeng Zuo, Rick Siow Mong Goh, Yong Liu, and Lei Zhu.\nFederated Residual Low-Rank Adaptation of Large Language Models. In The Thirteenth\nInternational Conference on Learning Representations .\n11\n--- Page 12 ---\n[25] Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang,\nSungchul Kim, Ryan Rossi, Ang Li, Lina Yao, Julian McAuley, Yiran Chen, and Carlee Joe-\nWong. Federated Large Language Models: Current Progress and Future Directions, September\n2024.\n[26] Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, and\nSiheng Chen. OpenFedLLM: Training Large Language Models on Decentralized Private Data\nvia Federated Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining , pages 6137–6147, Barcelona Spain, August 2024. ACM.\n[27] Ziwei Zhan, Wenkuan Zhao, Yuanqing Li, Weijie Liu, Xiaoxi Zhang, Chee Wei Tan, Chuan\nWu, Deke Guo, and Xu Chen. FedMoE-DA: Federated Mixture of Experts via Domain Aware\nFine-grained Aggregation. Corr , 2024.\n[28] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin\nWang, and Yiran Chen. Towards building the federatedgpt: Federated instruction tuning. In\nICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 6915–6919. IEEE, 2024.\n[29] Yifei Zou, Senmao Qi, Yuan Yuan, Dawei Wang, Shikun Shen, Libing Wu, Shaoyong Guo,\nand Dongxiao Yu. Fed-MoE: Efficient Federated Learning for Mixture-of-Experts Models\nvia Empirical Pruning. In Yupeng Li, Yong Zhang, and Jianliang Xu, editors, Parallel and\nDistributed Computing, Applications and Technologies , volume 15502, pages 128–139. Springer\nNature Singapore, Singapore, 2025.\n12\n--- Page 13 ---\nSupplementary organization:\nA Algorithm Framework 14\nB Discussion 14\nC Further Experiments 15\nC.1 Evaluation Across Different Model Architectures . . . . . . . . . . . . . . . . . . 15\nC.2 Experiments on Trainable Modules . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nC.3 Comparison with Traditional Federated Learning . . . . . . . . . . . . . . . . . . 16\nC.4 Experiments on Expert Aggregation Strategies . . . . . . . . . . . . . . . . . . . . 17\nC.5 Experiments on Expert Load Balancing . . . . . . . . . . . . . . . . . . . . . . . 17\nC.6 Data Distribution for Non-IID . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.7 Overview of Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.8 Dataset Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nD Prompt Template 19\nD.1 Alpaca Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD.2 Vicuna Helpful Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD.3 Vicuna Harmless Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n13\n--- Page 14 ---\nA Algorithm Framework\nWe now present a detailed overview of our FLEx ( Federated LLMs with Personalized Experts)\nalgorithm framework. As illustrated in Algorithm 1, we incorporate personalized expert selection,\nexecute local model updates for each client, and aggregate the attention-based updates at the server\nlevel. This process effectively balances global aggregation and local personalization. Algorithm 1\noutlines the FLEx framework, beginning with personalized expert selection for each client. Each\nclient utilizes their respective local dataset to perform expert selection, subsequently updating their\nlocal model using a personalized MoE layer. During global aggregation phases, clients transmit only\ntheir attention layer LoRA adapters to the central server, retaining personalized MoE layers locally.\nThese adapters are aggregated across all clients, and the resulting global attention layer is integrated\nlocally within each personalized MoE model.\nAlgorithm 1 FLEx: Federated LLMs with Personalized Experts\n1:Input: Pre-trained MoE model, client set C\n2:foreach client i∈ Cdo\n3: Perform personalized expert selection (Algorithm 2)\n4: Update local model with personalized MoE layer\n5:end for\n6:foreach global iteration do\n7: foreach client i∈ Cdo\n8: Send attention layer LoRA adapters to server\n9: Keep personalized MoE layer locally\n10: end for\n11: Aggregate global attention LoRA adapters\n12: Send global attention to clients and integrate into personalized models locally\n13:end for\n14:Output: Federated model with personalized experts on each client\nAlgorithm 2 provides the detailed procedure for personalized expert selection. Specifically, we\nemploy a reconstruction loss strategy to prune the original MoE layers down to a tailored subset of\nexperts uniquely optimized for each client. The selected subset effectively minimizes the discrepancy\nbetween the outputs of the full expert set and those produced by the personalized subset, thus ensuring\noptimal local performance.\nAlgorithm 2 Personalized Expert Selection\n1:Input: Local dataset Difor client i, pre-trained MoE weights, number of experts K, size of\npruned subset n\n2:Initialize local model with pre-trained MoE weights\n3:foreach MoE layer independently do\n4: foreach input x∈ Dido\n5: Compute the outputs of all experts: F(x)\n6: end for\n7: Select personalized expert subset Siby solving:\nSi= arg min\nS⊂{1,...,K},|S|=n \n1\n|Di|X\nx∈D i∥FS(x)−F(x)∥F!\n8: Prune the MoE layer by retaining only experts in Si\n9:end for\n10:Output: Personalized expert subsets and updated MoE layers\nB Discussion\nOur method retains the pre-trained weights in the MoE layer while adding a personalized side expert.\nThis design maximally preserves pre-trained knowledge, leverages personalized experts to enhance\n14\n--- Page 15 ---\nclient-specific capabilities, benefits from shared global knowledge aggregation, and significantly\nreduces communication overhead.\nLimitation. Our method adopts a layer-wise greedy pruning strategy, which ensures minimal\nreconstruction loss at each individual layer. However, this localized optimization does not necessarily\nguarantee a globally optimal pruning configuration across the entire model. As a result, the pruning\ndecisions made at early layers may limit the potential of subsequent layers, leading to suboptimal\nglobal performance. Exploring more globally-aware pruning strategies across layers that better\nbalance efficiency and model fidelity remains an important and underexplored direction for future\nresearch.\nFurther Comparsion with Previous Works. It is worth mentioning that [ 15] proposed a federated\nlearning approach specifically targeting frameworks utilizing Switch Transformers to address data\nheterogeneity. However, their approach involves fine-tuning all MoE layers, inevitably leading to\nsignificant forgetting from the pretrained models. This forgetting issue is particularly concerning\ngiven that current decoder-only large language models are pretrained on billions of tokens, making\nthe preservation of pretrained knowledge an important task in federated learning. In contrast, our\nproposed method maintains the integrity of the original pretrained model parameters by introducing a\nspecialized side-expert specifically designed to handle data heterogeneity. Our approach effectively\nmitigates the forgetting issue inherent in methods that fine-tune all MoE layers. Furthermore,\nwe validate the applicability and efficacy of our federated learning approach through extensive\nexperiments on billion-token pretrained, instruction-tuned large language models, including Qwen\nand DeepSeek.\nC Further Experiments\nC.1 Evaluation Across Different Model Architectures\nIn this section, we validate the robustness of FLEx across different model architectures. Specifically,\nwe conduct experiments using the Databricks-dolly-15k dataset in combination with an alternative\nMoE-based model, DeepSeek-MoE-16B-Base4. We assess performance under a pathological non-IID\nscenario, with the results summarized in Table 5. From the table, we observe significant improvements\nacross most evaluated tasks, with only a slight reduction in classification performance. When\nconsidering the average performance gains, our proposed method demonstrates a relatively substantial\nimprovement (32.66) compared to the closest competitor, MoE+FedYogi (30.85). These results\nconfirm that our approach maintains effectiveness and consistently delivers enhanced performance\nacross varying model architectures.\nTable 5: Experiments under another MoE-based LLM, DeepSeek-MoE-16B-Base. The following\ntable shown Rouge-L performance comparison of different federated learning strategies on the\nDatabricks-dolly-15k dataset under a pathological non-IID scenario. The best result over federated\nmethod is bolded and the second-best result is underlined . CLF: Classification, CQA: Closed\nQuestion Answering, IE: Information Extraction, Summ: Summarization\nDatabricks-dolly-15k\nMethod CLF CQA IE Summ Avg\nLocal Training 51.71 14.17 17.70 37.15 30.18\nMoE + FedAvg 48.33 14.74 17.89 31.92 28.97\nMoE + FedAvgM 48.25 14.21 17.99 32.55 28.25\nMoE + FedAdam 49.96 19.15 20.30 33.83 30.81\nMoE + FedAdagrad 50.10 18.75 20.83 32.91 30.64\nMoE + SCAFFOLD 49.35 14.34 17.41 31.30 28.10\nMoE + FedProx 49.35 14.55 17.50 32.17 28.39\nMoE + FedYogi 50.63 18.42 20.70 33.66 30.85\nFLEx (Ours) 50.33 20.11 21.29 38.93 32.66\n4https://huggingface.co/deepseek-ai/deepseek-moe-16b-base\n15\n--- Page 16 ---\nC.2 Experiments on Trainable Modules\nTo further validate the effectiveness of FLEx’s training and sharing strategies, we conducted additional\nexperiments investigating various combinations of trainable modules. Specifically, we explored the\nfollowing approaches:\n1.We select a single expert randomly at each layer for keeping same trainable parameters with\nFLEx.\n2.We select all experts at each layer instead of choosing a single pruned expert, which\nconsequently leads to higher communication costs and an increased number of trainable\nparameters.\nThe experimental results, summarized in Table 6, demonstrate that our proposed method achieves\ncomparable performance while significantly reducing both communication and computational costs.\nTable 6: Rouge-L performance comparison of different federated learning strategies, including\nour proposed method, on the Databricks-dolly-15k dataset with Qwen1.5-MoE-A2.7B under a\npathological non-IID scenario. The best result over federated method is bolded and the second-best\nresult is underlined . CP: Communication Parameter, TP: Trainable Parameter.△denote that further\nshared and averaging all LoRA layers on MoE layers, which lead to more communication costs\nduring training.\nDatabricks-dolly-15k\nMethod Modules CLF CQA IE Summ Avg CP(%) ↓TP(%)↓\nMoE + FedAvg Single Expert△50.83 34.34 36.26 41.59 40.75 0.1327 0.1327\nMoE + FedAvg All Experts△51.80 35.91 38.91 42.12 42.18 3.4172 3.4172\nMoE + FedAvg Single Expert 51.48 34.77 37.23 41.74 41.30 0.0672 0.1327\nMoE + FedAvg All Experts 51.10 32.51 41.39 40.60 41.40 0.0672 3.4172\nFLEx (Ours) Single Experts 52.10 38.23 40.13 42.09 43.13 0.0672 0.1307\nC.3 Comparison with Traditional Federated Learning\nAdditionally, we compare our method with traditional federated learning algorithms that treat MoE\nas dense models from two perspectives: performance and communication cost. The experiment\nis conducted using the Qwen1.5-MoE-A2.7B model on the Databricks-dolly-15k dataset under\na challenging pathological non-IID scenario. As illustrated in Figure 3, our approach achieves\ncomparable performance with significantly lower communication overhead.\nFigure 3: Our method significantly reduces communication overhead while simultaneously enhancing\nperformance, a stark contrast to traditional federated learning approaches that treat MoE as dense\nmodels, leading to prohibitive communication costs.\n16\n--- Page 17 ---\nC.4 Experiments on Expert Aggregation Strategies\nIn this section, we evaluate several expert aggregation strategies within FLEx. In this paper, FLEx\nemploys a sigmoid function as presented in formula (8). We have experimented with various\naggregation strategies for side experts, comparing the sigmoid function against alternative approaches,\nsuch as ReLU and Tanh functions. Specifically, the ReLU-based weighting for side expert gt\necan be\nexpressed as:\ngt\ne=ReLU (router e(ul\nt)). (9)\nSimilarly, for the Tanh-based aggregation, the weight gt\neis calculated as:\ngt\ne=Tanh(router e(ul\nt)). (10)\nAs shown in Table 7, our sigmoid-based aggregation method consistently demonstrates superior\neffectiveness. In contrast, the use of ReLU and Tanh functions results in suboptimal performance\nacross aggregation strategies. From this result, we discover that the aggregation strategy of side\nexpert requires carefully chosen.\nTable 7: Experiments under different side expert aggregation strategies. Rouge-L performance on the\nDatabricks-dolly-15k dataset with Qwen1.5-MoE-A2.7B under pathological non-IID scenario.\nDatabricks-dolly-15k\nMethod CLF CQA IE Summ Avg\nReLU 51.16 33.02 37.51 41.78 40.86\nTanh 41.40 20.40 31.34 37.88 32.75\nSigmoid 52.10 38.23 40.13 42.09 43.13\nFigure 4: Activation counts of experts in the Qwen1.5-MoE-A2.7B model evaluated on the C4\ndataset.\nC.5 Experiments on Expert Load Balancing\nWe conducted experiments to evaluate the activation frequency of experts within the\nQwen1.5-MoE-A2.7B model using the C4 dataset5across different methods. Figure 4 illustrates\nthe activation count for individual experts. This analysis demonstrates that our method achieves\na more balanced workload distribution across different experts. Specifically, the original model\nand the FedAvg method tend to produce uneven workloads, causing certain experts, such as expert\n5https://huggingface.co/datasets/allenai/c4\n17\n--- Page 18 ---\n31 and expert 43, to become significantly overloaded. Specifically, the original model exhibits\na standard deviation of 1859.80, while FedAvg demonstrates a slightly higher imbalance with a\nstandard deviation of 1906.85. In contrast, our proposed FLEx method significantly improves load\nbalancing among experts, achieving a notably lower standard deviation of 1259.88.\nC.6 Data Distribution for Non-IID\nIn this paper, we explore different scenarios for data distribution, specifically focusing on pathological\nnon-IID and Dirichlet non-IID settings. As illustrated in Figure 5, the pathological non-IID scenario\nassigns four distinct tasks to four separate clients, resulting in an extreme non-IID environment. This\nscenario significantly complicates the aggregation of models across clients. The Dirichlet non-IID\ndistribution offers a compromise between the pathological non-IID and IID distributions. Specifically,\na lower value of the hyperparameter αindicates a higher degree of non-IID distribution among clients.\nUnless explicitly stated otherwise, this paper defaults to the pathological non-IID distribution.\nFigure 5: Distribution of instruction types across different clients in federated learning under various\nsettings. The first panel shows the pathological distribution, where each client has a highly imbalanced\ndata distribution. The second panel illustrates the Dirichlet distribution with alpha=0.1, resulting in\na more skewed distribution across clients. The third panel presents the Dirichlet distribution with\nalpha=1.0, demonstrating a more uniform distribution of categories (classification, closed question\nanswering, information extraction, and summarization) across clients.\nC.7 Overview of Hyperparameters\nIn this section, we present the hyperparameters used for the models Qwen1.5-MoE-A2.7B and\nDeepSeek-MoE-16B-Base . Detailed hyperparameter configurations are summarized in Table 8.\nModel Name Qwen1.5-MoE-A2.7B DeepSeek-MoE-16B-Base\n# Params 14.3B 16.4B\n# Layers 24 28\nHidden Size 2048 2048\n# Attn Heads 16 16\n# Shared Experts 1 2\n# Routed Experts 60 (4 activated) 64 (6 activated)\nRelative Expert Size 0.25 0.25\nSequence Length 2048 2048\nLearning Rate 4e-5 4e-5\nTable 8: Model hyperparameter configurations.\nC.8 Dataset Information\nDatabricks-dolly-15k. TheDatabricks-dolly-15k dataset is an open-source collection consist-\ning of 15k instruction-following examples generated collaboratively by Databricks employees. In our\nanalysis, we specifically utilize data from the classification, closed QA, information extraction, and\nsummarization categories.\n18\n--- Page 19 ---\nAlpaca-gpt4. TheAlpaca-gpt4 dataset comprises 52k English-language instruction-following\ninstances generated by GPT-4, based on prompts originally crafted for the Alpaca dataset. This\ndataset mirrors the structure of the Alpaca data but leverages GPT-4’s advanced generative capabilities\nfor creating more nuanced outputs suitable for fine-tuning large language models.\nFinance-Alpaca. TheFinance-Alpaca dataset merges Stanford’s Alpaca dataset and the FiQA fi-\nnancial question-answering dataset, supplemented with an additional 1,300 instruction-response pairs\nuniquely generated using GPT-3.5. In total, it encompasses 68,912 QA pairs designed specifically for\napplications in finance-related natural language processing tasks.\nMedAlpaca. TheMedAlpaca dataset provides comprehensive coverage of medical domains, incor-\nporating fundamental medical sciences, clinical knowledge, and clinical reasoning skills. It contains\na total of 33,955 QA pairs designed to support the training and evaluation of medical-domain-specific\nlanguage models and AI applications.\nVicuna Benchmark. TheVicuna benchmark employs an innovative ’LLM-as-a-judge’ method-\nology, presenting 80 carefully curated questions spanning multiple categories, including general\nknowledge, coding challenges, role-playing, creative writing, common sense reasoning, and complex\nproblem-solving. This approach enables a multidimensional evaluation of large language models,\nhighlighting their capabilities across diverse cognitive tasks.\nD Prompt Template\nD.1 Alpaca Template\nWe utilize the Alpaca template as the standardized format for our instruction tuning, which helps\ndefine the structure of prompts and responses for more efficient model training.\nBelow is an instruction that describes a task . Write a\nresponse that appropriately completes the request .\n### Instruction :\n{}\n### Response :\n{}\n{}\nD.2 Vicuna Helpful Template\nTo effectively evaluate helpfulness in open-ended tests, we use this template to configure powerful\nmodels as judges.\n[ Instruction ]\nPlease act as an impartial judge and evaluate the quality of\nthe response provided by an AI assistant to the user question\ndisplayed below . Your evaluation should consider factors such\nas the helpfulness , relevance , accuracy , depth , creativity ,\nand level of detail of the response . Begin your evaluation by\nproviding a short explanation . Be as objective as possible .\nAfter providing your explanation , you must rate the response\non a scale of 1 to 10 (You score can be a float with two\ndecimal place ) by strictly following this format :\n\"[[ rating ]]\" , for example : \" Rating : [[5.62]]\".\n[ Question ]\n{ question }\n19\n--- Page 20 ---\n[The Start of Assistant ’s Answer ]\n{ answer }\n[The End of Assistant ’s Answer ]\nD.3 Vicuna Harmless Template\nFor open-ended testing, this template guides powerful models acting as judges in assessing harmless-\nness.\n[ Instruction ]\nYou are an impartial judge evaluating whether an AI assistant\n’s response contains harmful content . Be extremely strict even\nminor issues should lower the score . Your evaluation should\nconsider factors such as inaccuracy , impoliteness ,\nsubjectivity , irrelevance , offensiveness , sensitivity ,\nharmfulness , uselessness , or misleading nature . Begin your\nevaluation by providing a short explanation . Be as strict as\npossible . After providing your explanation , you must rate the\nresponse on a scale of 1 ( highly dangerous ) to 10 ( harmless ).\n( Your score can be a float with two decimal places ) by\nstrictly following this format : \"[[ rating ]]\" , for example :\n\" Rating : [[5.62]]\".\n[ Question ]\n{ question }\n[The Start of Assistant ’s Answer ]\n{ answer }\n[The End of Assistant ’s Answer ]\n20",
  "text_length": 58971
}