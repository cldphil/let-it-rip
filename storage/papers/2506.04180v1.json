{
  "id": "http://arxiv.org/abs/2506.04180v1",
  "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
  "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
  "authors": [
    "Yuhao Wu",
    "Yushi Bai",
    "Zhiqiang Hu",
    "Juanzi Li",
    "Roy Ka-Wei Lee"
  ],
  "published": "2025-06-04T17:27:42Z",
  "updated": "2025-06-04T17:27:42Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04180v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04180v1  [cs.CL]  4 Jun 2025SuperWriter: Reflection-Driven Long-Form\nGeneration with Large Language Models\nYuhao Wu1∗, Yushi Bai2∗, Zhiqiang Hu1, Juanzi Li2, Roy Ka-Wei Lee1\n1Singapore University of Technology and Design, Singapore\n2Tsinghua University, Beijing, China\nAbstract\nLong-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical con-\nsistency, and preserving text quality as sequence length increases. To address\nthese limitations, we propose SuperWriter -Agent, an agent-based framework de-\nsigned to enhance the quality and consistency of long-form text generation. Su-\nperWriter -Agent introduces explicit structured thinking—through planning and\nrefinement stages—into the generation pipeline, guiding the model to follow\na more deliberate and cognitively grounded process akin to that of a profes-\nsional writer. Based on this framework, we construct a supervised fine-tuning\ndataset to train a 7B SuperWriter -LM. We further develop a hierarchical Direct\nPreference Optimization (DPO) procedure that uses Monte Carlo Tree Search\n(MCTS) to propagate final quality assessments and optimize each generation step\naccordingly. Empirical results across diverse benchmarks demonstrate that Su-\nperWriter -LM achieves state-of-the-art performance, surpassing even larger-scale\nbaseline models in both automatic evaluation and human evaluation. Further-\nmore, comprehensive ablation studies demonstrate the effectiveness of hierarchi-\ncal DPO and underscore the value of incorporating structured thinking steps to\nimprove the quality of long-form text generation. Our code & models are at:\nhttps://github.com/mozhu621/SuperWriter .\nQuery: Please write a story  about ....\nEthan \nand Lucas\nhad been\nbest\nfriends Direct generation\nThink\nOutline：\n1, Abs\n2, Intro\n3, ........\nWriteChapter 1\nChapter 2\nChapter nRefineOnce \na time, in a\nquiet village\nsurrounded LLMs\nHumans\nFigure 1: Current LLMs directly generate long text in a single pass, while human writers follow an\niterative process of thinking ,outlining ,writing , and refining to ensure coherence and quality.\n∗Equal contribution.\nPreprint. Under review.\n--- Page 2 ---\n1 Introduction\nEffective long-form text generation remains a fundamental challenge for advancing large language\nmodels (LLMs) [ 66,45,56]. Unlike short-form generation tasks, where LLMs have demonstrated\nremarkable success, generating extended sequences often lead to degraded coherence and logical\nconsistency as the length grows.\nPrevious studies on long-form writing [ 6,35] typically employ LLMs to generate text in a single pass\nwithout structured intermediate thinking or explicit planning. Consequently, the generated texts often\nappear plausible initially but lack sustained coherence and can exhibit logical contradictions over\nlonger spans [17, 21, 41, 28, 59].\nIn practice, human writers rarely produce complex or lengthy documents in a single uninterrupted\nattempt. Instead, they commonly utilize iterative processes, including outlining, drafting, reviewing,\nand revising their writing to maintain coherence and logical consistency [ 14,7,18], as shown in\nFig 1. Moreover, such writing processes are often accompanied by extensive cognitive activities,\nsuch as planning, foreshadowing, and strategic structuring—especially in tasks like writing detective\nnovels or academic papers [ 14]. Building on this observation, we propose SuperWriter -agent, a novel\nagent pipeline explicitly designed to embed structured thinking paradigms within long-form text\ngeneration. By incorporating explicit intermediate cognitive steps, SuperWriter -agent simulates the\nhuman writing process through a coordinated agent-based framework.\nFormally, SuperWriter -Agent generates training data through a structured three-stage framework:\nPlanning →Writing →Refining. In the Planning stage, two agents collaboratively reflect and outline\nkey arguments, decompose complex ideas, and establish logical connections. During the Writing\nstage, each paragraph is composed based on the structured plan, incorporating explicit thinking steps\ninto the content. In the Refining stage, the text is carefully reviewed and revised to ensure clarity\nand structural integrity. Unlike previous SFT datasets for long-form generation [ 6,35], our approach\nexplicitly segments the data into these three stages, each enriched with thinking-oriented supervision.\nTo further guide the SuperWriter -LM’s writing capabilities, we apply hierarchical Direct Preference\nOptimization by comparing pairs of finalized response [ 39] and propagate the feedback to each\nstep through a Monte Carlo Tree Search (MCTS). Through the integration of structured thinking\nsignals and hierarchical feedback, SuperWriter -LM demonstrates notable improvements in fluency,\ncoherence, and overall textual quality.\nFinally, we validate the effectiveness of our trained SuperWriter -LM through both human and\nautomatic evaluations on the WritingBench benchmark [ 60]. Extensive ablation studies further\nunderscore the critical role of structured thinking data and hierarchical Direct Preference Optimization\n(DPO) in enabling high-quality text generation. Our contributions are summarized as follows:\n•We introduce SuperWriter -agent, an agent-based framework with a structured Planning–Writing–\nRefining workflow that simulates human writing processes by injecting intermediate thinking steps\ninto long-form text generation.\n•We construct a thinking-supervised, stage-segmented dataset and train the SuperWriter -LM on it,\nthen apply hierarchical DPO over final output comparisons to optimize each generation step.\n•We empirically demonstrate the effectiveness of our approach, showing substantial improvements\nin fluency, coherence, and logical consistency compared to existing methods, as evaluated on\nWritingBench [60].\n2SuperWriter -Agent\nWhile current LLM training corpora provide abundant supervision data reflecting intermediate\n“thinking ” processes—such as mathematical reasoning and code generation [ 12]; however, they\ncontain remarkably little data of this kind for writing tasks [ 48]. Most pretraining data [ 33,57]\nconsists of finished texts like articles and books, which largely omit the underlying process of\nplanning, structuring, and thinking involved in writing. However, writing—particularly long-form\nwriting—inherently presents a complex cognitive task, where explicit thinking steps are crucial for\nmaintaining coherence, logical flow, and structural consistency.\nTo address this gap, we propose the SuperWriter -agent (illustrated in Figure 2), a framework designed\nto generate high-quality, thought-enriched supervised fine-tuning data for writing. The SuperWriter -\n2\n--- Page 3 ---\nPlease write a\nnovel about the last\nman on Earth,\nwher e everyone\nelse is a woman.Stage 1: Plan Stage 2: Write Stage 3: Refine\nProposing a\nwriting Plan\nFor i in range(Paragraph):\ncommentators\nDiscuss optimizing the\nwriting pr ogram\nPlan\nThinker\nWriterPlanFor i in range(Paragraph):\nPlanChecker\nRefiner\n❌✅\nUser  queryFigure 2: This figure illustrates a three-stage agent framework for long-form generation. In Stage\n1 (Plan), the framework proposes a structured writing plan through discussions between AI com-\nmentators and a writer. In Stage 2 (Write), the text is incrementally generated using a thinker-writer\ncollaboration, and in Stage 3 (Refine), a checker and refiner iteratively improve the generated text to\nenhance coherence and quality.\nagent enables structured content generation through three coordinated stages: careful planning,\ntargeted paragraph-level writing, and iterative refinement. This process explicitly embeds intermediate\nthinking signals into the writing pipeline, thereby enhancing the fluency, coherence, and narrative\nconsistency of the generated text.\n2.1 Stage 1: Plan\nInspired by the widely adopted pedagogical technique in writing education known as the Story\nWorkshop2[46,51,47], Stage 1 of SuperWriter -agent begins with oral narration and iterative dialogue\naimed at distilling and expanding initial ideas. In practice, this planning stage guides discussion\nagents to articulate core themes, central arguments, character background settings (for genres like\nfiction), and paragraph-level content structures—collectively forming the Background component.\nBy systematically allocating word counts and associating key ideas with specific paragraph units, this\nstep builds a comprehensive and detailed outline for downstream writing. This structured process\nsignificantly enhances the overall coherence and organization of the text. With such a framework in\nplace, discussion agents can strategically develop and refine their ideas, resulting in more focused,\ncoherent, and well-developed written outputs. Appendix A.1 provides the detailed prompt for the\nplanning stage.\n2.2 Stage 2: Write\nMotivated by recent advancements in reasoning-oriented LLMs—notably their remarkable scalability\nduring inference in tasks such as mathematical reasoning and code generation—we build upon\nparadigms exemplified by reasoning models like OpenAI o1 and DeepSeek-R1. [ 29,12]. These\nmodels typically engage in an explicit reasoning and planning phase prior to generating final responses,\nwhich has proven highly effective in enhancing output quality. Following this paradigm, we propose\na two-stage generation framework that simulates the process of “thinking before writing,” aiming\nto produce structurally coherent and logically consistent paragraphs. Appendix A.2 provides the\ndetailed prompt for the actual paragraph write stage.\n•Thinker Step: In this initial phase, the model refrains from generating surface-level text. Instead,\nit identifies and organizes key ideas, thematic elements, and logical structures relevant to the\nparagraph. This explicit reasoning process provides a clear directional scaffold for subsequent text\ngeneration.\n2The Story Workshop is a writing instruction method developed at Columbia College Chicago. It emphasizes\noral storytelling, collaborative discussion, and reflective dialogue to help writers generate ideas, structure\nnarratives, and refine language through an interactive process.\n3\n--- Page 4 ---\n•Writer Step: Building on the structured outline from the Thinker stage and incorporating the\npreceding paragraph (i.e., the (n−1)-th paragraph) as contextual input, the model proceeds to\ngenerate the current paragraph. This use of prior context ensures smooth transitions between\nparagraphs and contributes to the overall logical flow of the document.\n2.3 Stage 3: Refine\nThe final refinement stage goes beyond superficial edits by systematically evaluating the overall\nquality of the generated text and identifying specific paragraphs that require targeted revisions.\nSpecifically, the refinement workflow consists of two key steps:\n•Checker Step: The model conducts a comprehensive assessment of each paragraph, identifying\nissues such as logical inconsistencies, unclear expressions, or deviations from the intended narrative\nstructure.\n•Editor Step: Based on the feedback from the Checker stage, the model performs precise and\ntargeted modifications to improve textual accuracy, fluency, and structural coherence.\nThis iterative and structured refinement process ensures that the final output not only accurately\nconveys the original intent and narrative objectives but also meets the rigorous standards expected in\nacademic writing. Appendix A.3 provides the detailed prompts for paragraph refine stage.\n3SuperWriter -LM\nFollowing the development of the SuperWriter -agent, which introduces structured thinking and\niterative discussion mechanisms to significantly enhance text generation quality, we are motivated\nto explore a central research question: Can large language models, when guided by the thinking\nparadigm provided by the SuperWriter-agent data, internalize the ability to generate high-quality\nlong-form content through substantially fewer inference steps—rather than relying on 30 to 40\nseparate agent calls per sample?\nTo answer this question, we conduct targeted model training experiments. Our objective is not merely\nto extend output length, but to fundamentally improve coherence, relevance, and depth by distilling\nthe agent’s structured thinking process directly into the model itself. In the following sections, we\ndescribe the construction of our high-quality training dataset and the strategic methodology for\ntraining LLMs to acquire and internalize the SuperWriter -agent’s structured thinking and writing\ncapabilities.\n3.1 SFT-Training\nOur training dataset is sourced from two real-world instruction tuning datasets: English-language and\nChinese-language instructions from WildChat-1M [ 68] and LMSYS-Chat-1M [ 69]. To ensure the\nquality and relevance of the selected instructions for long-form writing tasks, we apply a filtering\nprocess using the DeepSeek-R1-Distill-Qwen-32B model [ 12] (the filtering method is detailed in\nAppendix A.7).\nWe generate SFT data using the SuperWriter -agent (powered by GPT-4o-2024-08-06 [ 31]) based\non 4,000 filtered instructions. Each data instance follows a structured pipeline: query →outline →\ndraft→final output. We explicitly segment this pipeline into three stages that align with the internal\nstructure of the SuperWriter -agent: plan (query →outline), write (outline →draft), and refine (draft\n→final output). Rather than training the model directly on full instruction-to-answer SFT pairs, we\nadopt stage-wise training (illustrated in Fig 3) for two key reasons. First, it better accommodate to\nreal-world user workflows, where users may wish to review and revise intermediate results (e.g., the\noutline) before progressing to subsequent stages. Second, the complete outputs generated by the agent\ncan be extremely long—some exceeding 100K tokens—posing significant challenges for existing\nlong-context models. By breaking the generation process into stages, we ensure that each training\nsample remains within 32K tokens, making it more tractable for current models. Putting them all\ntogether, each of the three stages— plan,write , and refine —contains exactly 4,000 data instances,\nresulting in a total of 12,000 high-quality training data. During inference, the model performs the\ngeneration in three sequential stages to produce the final output.\n4\n--- Page 5 ---\nStage 2: WriteStage 1: Plan\nStage 3: RefineInput Output\nPlan Pr ompt\nWrite Pr ompt\nRefine Pr omptUser  Query Think Backgr ound Think Outline  \nStage 1 Answer Think Think Para.1 Think Para. n\nThink Think Para.1 Think Para. n Stage 2 Answer\nPara.1 Para. n Final Output: Para.2Figure 3: Format of the SFT dataset constructed by the SuperWriter -agent. The agent generates\ntraining data across three stages—planning, writing, and refining—each incorporating intermediate\n“Think” steps. During inference, the output of Stage 3 is used as the final answer.\nWe train our SuperWriter -LM model based on Qwen2.5-7B [ 63], which supports a context window\nof up to 128K tokens, making it well-suited for long-form text generation. To improve training\nefficiency, we adopt the packing-based training strategy with loss weighting, as proposed by Bai et al.\n[5]. Training is conducted on a single node equipped with 8 ×H800 80GB GPUs using DeepSpeed\nwith ZeRO-3 and CPU offloading [ 42]. The training setup includes a batch size of 32, a learning rate\nof2×10−5, and a context window of 32K tokens. We train the model for four epochs in total.\n3.2 Hierarchical DPO for Multi-Stage Generation\nDirect Preference Optimization (DPO) has demonstrated effectiveness in aligning policies directly\nwith pairwise human (or proxy-model) preferences for single-pass generation tasks [ 40]. However, in\nour scenario, the writing process unfolds sequentially over three distinct stages: planning ,drafting ,\nandrefinement . Applying conventional DPO exclusively to final outputs neglects the valuable\npreference signals inherently present in earlier stages. To address this gap, we introduce a hierarchical\nmulti-stage DPO framework that combines structured preference-data construction with systematic\nevaluation, inspired by process-annotation methods such as TPO [26],CHIP [15], and Math-\nShepherd [54].\nSpecifically, as depicted in Figure 4, the writing process is structured as a tree explored through\nMonte-Carlo Tree Search. Each path through this tree, labeled (i, j, k ), corresponds sequentially\nto Stage-1 ( plani), Stage-2 ( draft j), and Stage-3 ( refinement k). We embed two key assumptions:\nwell-structured initial plans lead to higher-quality draft (Stage-1 Plan →Stage-2 Write) and well-\nrefined drafts typically yield better final outputs (Stage-2 Write →Stage-2 Refine). Consequently, we\nback-propagate quality signals from leaf nodes (final outputs) upwards through intermediate stages,\nensuring the policy learns from decisions at every level rather than only from final outcomes.\nStructured evaluation ( Write-judge ).To score the final output at each leaf, we introduce Write-\njudge3, a six-dimension rubric (0–10 each) chosen from a larger pool of twenty dimensions according\nto the instruction type (e.g. creativity vs. logical coherence). Following best practice to curb evaluation\nbias, we use the QwQ-32B model [ 53] to score every output three times under the same temperature\nsettings and take the average. Then we propagate scores from the leaf nodes upward to construct\nDPO pairs as follows.\nStep 1: Leaf-score discretization. Letsijkdenote the averaged raw evaluation score assigned to\nleaf(i, j, k ), and let Nrepresent the total number of leaf nodes. We define the ranking and percentile\n3Pipeline details and prompts appear in App. A.4.\n5\n--- Page 6 ---\nUser  query\nPlease write a\nnovel about the last\nman on Earth,\nwher e everyone\nelse is a woman.Stage 1: Plan Stage 2: Write Stage 3: Refine\n❌\nLLMs \nJudgesRank\n1\n594853\n❌❌Figure 4: The MCTS begins with 5 distinct writing plans, each leading to 4 written drafts, totaling 20\ninitial outputs. Each draft is then refined 3 times, resulting in 60 unique final outputs for a single root.\nA judge LLM ranks these final outputs, and the rankings are back-propagated through the refinement\nand planning stages. This scoring mechanism helps identify the most effective planning and writing\nstrategies, thereby optimizing the overall writing process.\nfor each leaf node as follows:\nrank ijk= 1 +\f\f{(p, q, r )|spqr> sijk}\f\f, π ijk=rank ijk\nN.\nThe percentile scores πijkare subsequently discretized into ordinal rewards rijk:\nrijk=\n\n+2, π ijk≤1\n6,\n+1,1\n6< πijk≤2\n6,\n0,2\n6< πijk≤4\n6,\n−1,4\n6< πijk≤5\n6,\n−2, π ijk>5\n6.\nStep 2: Reward aggregation. Ordinal rewards are propagated upwards through the tree by averag-\ning across child nodes at each intermediate stage:\nˆrij=1\n|{k}|X\nkrijk (Stage 2 node)\nˆri=1\n|{j}|X\njˆrij (Stage 1 node)\nStep 3: Preference-pair construction. We harvest pairwise preferences at every hierarchy level,\nensuring that each decision contributes to the optimization at every step.\nL1. Stage 1 (Plan). LetB= arg max iˆribe the best plan and W1, W 2the worst two.\nP1={(B, W 1),(B, W 2)}\nL2. Stage 2 (Write). For the best two plans i∈ {B, S}(with Sbeing the second best),\nP2(i) =\u0000\narg max\njˆrij,arg min\njˆrij\u0001\n, P 2=[\niP2(i).\nL3. Stage 3 (Refine). For each (i, j⋆)∈P2,\nP3(i, j⋆) =\u0000\narg max\nksijk,arg min\nksijk\u0001\n, P 3=[\n(i,j⋆)P3(i, j⋆).\nThe complete preference dataset is therefore\nDDPO=P1∪P2∪P3.\n6\n--- Page 7 ---\nModels AvgLanguages Domains Requirements\nZH EN D1 D2 D3 D4 D5 D6 R1 C R2 C R3 C\nProprietary LLMs\nChatGPT-4o-latest 8.16 8.3 8.1 8.1 8.1 8.2 8.1 8.4 8.1 8.3 8.7 8.2 8.9 8.2 8.3\no1-Preview 8.15 8.1 8.2 8.0 8.1 8.2 8.2 8.4 8.1 8.2 8.6 8.2 8.8 8.2 8.2\nClaude-3.5-Sonnet 7.71 7.7 7.7 7.6 7.5 7.6 7.7 7.9 8.0 7.9 8.5 7.7 8.5 7.9 8.0\nGemini-1.5-Pro 7.78 7.8 7.7 7.7 7.5 7.8 7.9 8.0 7.9 7.9 8.6 7.9 8.8 7.9 8.0\nQwen-Max 8.37 8.4 8.3 8.3 8.3 8.4 8.4 8.5 8.4 8.5 8.7 8.4 9.0 8.4 8.5\nOpen-source LLMs\nDeepseek-R1 8.55 8.7 8.5 8.5 8.5 8.6 8.6 8.7 8.6 8.7 8.9 8.6 9.0 8.6 8.7\nDeepseek-V3 7.95 8.0 7.9 7.9 7.8 8.0 7.8 8.2 8.0 8.1 8.6 8.0 8.9 8.0 8.2\nMistral-Large-Instruct 7.64 7.6 7.7 7.7 7.6 7.8 7.3 7.9 7.6 7.7 8.2 7.7 8.7 7.7 7.9\nQwen-2.5-72B-Instruct 7.90 8.0 7.9 8.0 7.8 8.1 7.7 8.2 7.8 8.0 8.3 8.0 8.8 7.9 8.0\nQwen-2.5-7B-Instruct 7.43 7.3 7.5 7.7 7.4 7.6 6.9 7.8 7.3 7.5 7.9 7.6 8.6 7.4 7.5\nLlama-3.3-70B-Instruct 7.01 6.7 7.3 7.0 6.9 7.0 6.8 7.3 7.3 7.1 7.8 7.1 8.2 7.0 7.2\nLlama-3.1-8B-Instruct 6.35 5.7 6.9 6.6 6.4 6.1 6.0 6.7 6.6 6.4 7.0 6.4 7.6 6.3 6.4\nCapability-enhanced LLMs\nSuri-I-ORPO 4.97 4.4 5.5 5.6 5.3 5.0 4.1 5.0 5.1 4.8 5.2 5.0 5.4 4.5 4.0\nLongWriter-8B 7.91 7.9 7.9 8.0 8.1 8.1 7.7 8.1 7.6 7.9 8.2 8.1 8.8 7.7 7.7\nWriting-Model-Qwen-7B 8.49 8.6 8.4 8.4 8.4 8.6 8.4 8.6 8.5 8.6 8.8 8.5 9.0 8.5 8.6\nWriting-Model-Llama-7B 8.49 8.6 8.4 8.5 8.4 8.6 8.4 8.6 8.5 8.6 8.8 8.5 8.9 8.5 8.5\nSuperWriter -LM (ours) 8.51 8.6 8.5 8.6 8.7 8.7 8.2 8.7 8.2 8.4 8.5 8.6 8.4 8.0 6.3\nTable 1: WritingBench performance of different LLMs across 6 domains and 3 writing requirements\nevaluated with our critic model (scale: 1-10). The six domains include: (D1) Academic & Engineering,\n(D2) Finance & Business, (D3) Politics & Law, (D4) Literature & Art, (D5) Education, and (D6)\nAdvertising & Marketing. The three writing requirements assessed are: (R1) Style, (R2) Format, and\n(R3) Length. Here, “C” indicates category-specific score.\nOptimization objective. Finally, we optimize the policy πθusing the standard DPO loss:\nLDPO=−E(x,y+,y−)∼D DPOh\nlogσ\u0000\nβ[sθ(x, y+)−sθ(x, y−)]\u0001i\n.\nUsing the above method, we obtained a DPO preference dataset and employed 360-LLaMA-\nfactory [ 20] to continue context-parallel DPO training of the supervised fine-tuned SuperWriter -LM\nwith a batch size of 32 and a learning rate of 1×10−6.\n4 Experiment\n4.1 Experiment Setup\nInference Setup. We utilize the SGLang [ 71] system, which optimizes key-value (KV) cache\nmemory for efficient large-scale inference. This system is crucial for handling long-form generation\nefficiently, reducing memory overhead, and maximizing inference throughput. Inferences are per-\nformed using BFloat16 precision on 8×NVIDIA H800 GPUs. This setup ensured consistency and\nefficiency in the inference process.\nBenchmark Setup. We conduct our experiments on two datasets. The first is WritingBench [ 60],\na comprehensive benchmark designed to evaluate large language models across six major writing\ndomains and 100 sub-domains, encompassing creative, persuasive, informational, and technical tasks.\nThe benchmark comprises 1,200 real-world writing prompts, each paired with five instance-specific\nevaluation criteria. WritingBench adopts a query-dependent evaluation framework and leverages\na Qwen2.5-7B critic model fine-tuned on 50K human-labeled samples to produce fine-grained\nassessments of style, format, and length, achieving an 83% agreement rate with human judgments.\nThe second dataset includes approximately 200 real-world user instructions manually curated by us.\nWe evaluate model performance through pairwise comparisons across several baselines (Win-rate),\nincluding LongWriter-8B [ 6], DeepSeek-R1-Distill-Qwen-7B [ 12], Writing-Model-Qwen-7B [ 60]4,\n4This refers to the SFT model obtained via rejection sampling using the WritingBench critic model.\n7\n--- Page 8 ---\nFigure 5: This figure presents eight donut charts comparing the win rates of our model against seven\ndifferent baselines. The win-rate in the 8th chart is evaluated by human (Ours vs. Writing-Model-\nQwen-7B) while the rest are evaluated by GPT-4.1. Each chart shows the proportions of wins, losses,\nand ties, with the central percentage indicating the overall win rate. When calculating the win rate, a\ntie is counted as 0.5 win for both sides.\nQwen3 [ 52], LLaMA-4-Scout [ 1], DeepSeek-V3 [ 13], and DeepSeek-R1 [ 12]. All models are\nevaluated using consistent decoding configurations (temperature = 0.6, top-p = 0.95), and win rates\nare calculated to quantify comparative performance.\n4.2 WritingBench Result\nWe evaluate our 7B sized SuperWriter -LM model on the WritingBench benchmark. As shown in\nTab 1, SuperWriter -LM achieves an overall performance (Avg) of 8.51—second-best among all\nmodels, closely following DeepSeek-R1 [ 12]. It demonstrates strong capabilities in both Chinese–8.6\nand English–8.5, even matching DeepSeek-R1 in English performance.\nAcross different domains, SuperWriter -LM achieves the highest scores in three major areas: (D1)\nAcademic & Engineering–8.6, (D2) Finance & Business–8.7, (D3) Politics & Law–8.7 and (D5)\nEducation–8.7, even slightly outperforming the DeepSeek-R1 model. Additionally, SuperWriter -LM\nsatisfies various special writing requirements, except the length_C setting. This discrepancy is\nprimarily due to the nature of agent-generated data, which tends to produce longer outputs even for\nshort-text tasks—an issue that does not affect long-form generations.\n4.3 Win-Rate Result\nWritingBench adopts a critic model to evaluate model outputs by assigning scores (ranging from\n1 to 10) across 4–5 distinct dimensions. However, this evaluation approach has several limitations.\nFirst, due to the relatively small size of the critic model, it may be vulnerable to some word/sentence\nhacking. Second, we observe that WritingBench primarily focuses on formal or professional writing\ntasks—such as summaries and reports—whereas our analysis of real-world data from WildChat [68]\nandLMSYS-Chat-1M [69] reveals that creative writing (e.g., storytelling, fiction) constitutes a\nsignificant portion of user queries. To address these concerns, we adopt a more direct and interpretable\nevaluation metric: win-rate . We evaluate model performance on nearly 200 real-world user queries\ncollected from the aforementioned datasets. For each query, responses are generated by SuperWriter -\nLM and six baseline models.\nLLM Evaluation. For automatic evaluation, we adopt LLM-as-a-judge [ 4,70] to perform pairwise\ncomparison using GPT-4.1-2025-04-14 [30]. To mitigate positional bias, we conduct two\nevaluations per pair by swapping the response order: Evaluation_Prompt + A + B and\nEvaluation_Prompt + B + A . Based on the two judgments, we categorize the results into\nwin, loss, or tie and compute win-rate results5.\n5Evaluation prompts are provided in Appendix A.5.\n8\n--- Page 9 ---\nAs shown in Figure 5, SuperWriter -LM demonstrates a substantial performance lead among models\nof the same size (models 1, 2, and 3). Furthermore, in comparisons against larger-sized models\n(models 4, 5, 6, and 7), SuperWriter -LM remains competitive and, in some cases, even slightly\noutperforms state-of-the-art LLMs. Taken together, these results suggest that SuperWriter -LM sets a\nnew performance benchmark among 7B-scale models and even challenges the capabilities of existing\nstate-of-the-art systems. We also present several case studies in Appendix A.6.\nHuman Evaluation. To mitigate potential inaccuracies in automatic evaluation, we conduct an\nhuman supplementary assessment on approximately 200 real-world user queries, comparing Super-\nWriter -LM with Writing-Model-Qwen-7B [ 60]. For each query, three independent annotators with\nundergraduate degrees. were tasked with evaluating and determining the preferred response, with\noutcomes categorized as win, loss, or tie—following the same standard as the automatic evaluation.\nThe aggregated results are shown in Figure 5 (8), and SuperWriter -LM demonstrates stronger perfor-\nmance under human judgment. However, due to the annotators’ tendency to assign a tie when the\ndifferences between two responses are subtle, the overall win rate appears slightly lower.\n4.4 Ablation Study\nModel Avg ZH EN\nQwen2.5-7B-Instruct 7.43 7.3 7.5\n+SuperWriter final output 8.21 8.3 8.2\n+Three-Stage 8.47 8.5 8.4\n+Hierarchical DPO 8.51 8.6 8.5\nFigure 6: Performance comparison on Writing-\nBench [60] – Avg, ZH and EN.Finally, we conduct an ablation study compris-\ning four different setups evaluated on the Writ-\ningBench benchmark. The first setup uses the\nbase model, Qwen2.5-Instruct , as the perfor-\nmance baseline. The second setup, SuperWriter-\nfinal-answer , takes the user query as input and\nproduces the final output from the Stage-3 refine\nstep of the SuperWriter -agent—this is a one-\npass generation without any explicit thinking\nprocess and achieves an average score of 8.21\n(ZH: 8.3, EN: 8.2). The third setup, +Three-Stage , corresponds to our SFT-trained model, which\nexplicitly performs planning, drafting, and refining in a chained, multi-stage manner, incorporating\nstructured thinking. This setup further improves performance to 8.47 (ZH: 8.5, EN: 8.4). The final\nsetup is the full model further enhanced with our hierarchical DPO optimization, reaching the highest\nscore of 8.51 (ZH: 8.6, EN: 8.5). As shown in Table 6, each additional component leads to consistent\nperformance improvements, demonstrating the effectiveness of our proposed approach in structured\nwriting tasks.\n5 Related Work\nLong-context Language Models Recent research on long-context language models (LLMs) has\nprimarily focused on extending the input context length, enabling models to process substantially\nlonger inputs. Approaches in this direction generally fall into two categories. The first involves zero-\nshot techniques, aiming to expand the model’s receptive field without additional training [ 19,61,67,\n22,2]. The second category encompasses fine-tuning-based methods, where models undergo training\non extended sequences to explicitly enhance their ability to handle long contexts [ 8,34,62,9,5,16].\nHowever, existing studies have predominantly concentrated on input expansion, often overlooking\nthe critical necessity for corresponding output capabilities. Recent empirical findings by Bai et al. [6]\nand Quan et al. [36] have shown a significant mismatch between maximum input lengths (over 100K\ntokens) and achievable output lengths (approximately 2K words).\nLong-form Text Generation Recent advancements emphasize architectural innovations and spe-\ncialized training paradigms [ 43,37,27,25]. For instance, Re3 employs a recursive prompting strategy,\neffectively maintaining narrative coherence in extended storytelling tasks [ 64]. Other approaches,\nsuch as DOC [ 65] and hierarchical outlining methods [ 55], leverage structured task decomposition to\nimprove overall narrative structure and content coherence. Recently, personalization has emerged as\nanother crucial dimension, with models like LongLaMP [ 23] and reasoning-enhanced self-training\napproaches [ 44] adapting outputs according to individual user preferences. Additionally, long-form\nquestion-answering techniques have gained attention for addressing complex and detailed queries\nwith comprehensive responses [ 11,49,24,50]. Despite these advancements, existing methods often\n9\n--- Page 10 ---\nsuffer from limitations such as inconsistent coherence, lack of theoretical grounding, and challenges in\ngenerating consistently high-quality content at large scales [ 58,37]. In response, our work addresses\nthese limitations by proposing a theoretical agent framework for long-form writing, accompanied by\nnovel training methodologies designed specifically to enhance the model’s capability in generating\ncoherent and extended texts.\n6 Conclusion\nSuperWriter addresses the challenge of long-form text generation by introducing a structured writing\nprocess—planning, writing, and refining—guided by the SuperWriter -agent. This approach teaches\nthe model to “think before writing” and produces high-quality supervision signals. Combined with a\nhierarchical DPO strategy, the model learns to align its output across all writing stages.\nExperiments show strong results: SuperWriter -LM outperforms all same-size models on Writing-\nBench and even exceeds the 671B DeepSeek-R1 model [ 12] in key domains. It also wins over\n98% of real-user comparisons against top open-source baselines. These results confirm the value of\nmulti-stage generation and structured preference learning for improving writing quality.\n7 Limitation\nWhile SuperWriter -LM demonstrates strong performance in long-form text generation, several\nlimitations remain:\n(1) Inference latency. Compared to single-pass generation models such as LongWriter [6] or\nSuri [35], our method incurs additional inference time due to its three-stage Framework. Although\nthis is significantly more efficient than multi-round agent-based pipelines (e.g., requiring 30–40 calls\nper output), the structured plan→write→refine process still requires three sequential forward\npasses, which may increase user-perceived latency in real-world applications.\n(2) Model scale. Our current implementation is built on a 7B parameter backbone (Qwen2.5)[ 38],\nwhich strikes a balance between performance and cost. However, this moderate scale may limit\nthe model’s internal world knowledge, particularly in knowledge-intensive or specialized writing\nscenarios (e.g., legal, medical, and scientific domains). In our qualitative analysis, some outputs\nexhibited shallow factual grounding or subtle reasoning errors.\n(3) Lack of online reinforcement learning. Lack of online reinforcement learning. Our alignment\nstage relies solely on offline Direct Preference Optimization (DPO) [ 40], trained from static preference\npairs. While effective, this setup lacks the adaptivity of online Reinforcement Learning from Human\nFeedback (RLHF) [ 10], which allows models to continually refine outputs through exploration. The\nkey bottleneck is the high rollout cost when applying general-purpose reward models to long outputs.\nDesigning scalable, low-latency reward models or reward distillation methods [ 3,32] for long-form\ntasks is thus a promising direction for future research.\nReferences\n[1]Meta AI. The llama 4 herd: The beginning of a new era of natively\nmultimodal ai innovation, April 2025. URL https://ai.meta.com/blog/\nllama-4-multimodal-intelligence/ .\n[2]Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Ling-\npeng Kong. Training-free long-context scaling of large language models. arXiv preprint\narXiv:2402.17463 , 2024.\n[3]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,\n2022.\n[4]Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng,\nYijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-\nexaminer. Advances in Neural Information Processing Systems , 36:78142–78167, 2023.\n10\n--- Page 11 ---\n[5]Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\nLongAlign: A recipe for long context alignment of large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2024 , pages 1376–1395, Miami, Florida,\nUSA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-emnlp.74. URL https://aclanthology.org/2024.findings-emnlp.\n74.\n[6]Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and\nJuanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv\npreprint arXiv:2408.07055 , 2024.\n[7] Anne Becker. Revision: History, theory, and practice , pages 25–49, 2006.\n[8]Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context\nwindow of large language models via positional interpolation. arXiv preprint arXiv:2306.15595 ,\n2023.\n[9]Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya\nJia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint\narXiv:2309.12307 , 2023.\n[10] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences, 2023. URL https://arxiv.org/\nabs/1706.03741 .\n[11] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A\ndataset of information-seeking questions and answers anchored in research papers. In Proc. of\nNAACL , pages 4599–4610, 2021.\n[12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang,\nChong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,\nJiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian\nLiang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean\nWang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian,\nPanpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\nZhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting\nPan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,\nT. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,\nWentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao\nNie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su,\nXuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang\nWang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y . K. Li, Y . Q. Wang, Y . X.\nWei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao\nZhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang\nMa, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He,\nYunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu,\nZilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang,\nand Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025. URL https://arxiv.org/abs/2501.12948 .\n[13] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu,\nChenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian\n11\n--- Page 12 ---\nYang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang,\nHonghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo,\nJiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong\nLi, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean\nWang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li,\nMiaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian,\nPanpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du,\nR. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu\nZhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu,\nShengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng\nZhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng,\nWanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang,\nX. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen,\nXiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang,\nXin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi\nZhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y . K. Li, Y . Q. Wang, Y . X. Wei,\nY . X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng\nSun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying\nHe, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\nYu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha,\nYunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,\nZ. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang,\nZhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong\nShao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu,\nZilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report,\n2025. URL https://arxiv.org/abs/2412.19437 .\n[14] L Flower. A cognitive process theory of writing. Composition and communication , 1981.\n[15] Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, and See-Kiong\nNg. Chip: Cross-modal hierarchical direct preference optimization for multimodal llms. arXiv\npreprint arXiv:2501.16629 , 2025.\n[16] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\narXiv:2402.10171 , 2024.\n[17] Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. Plan, write, and revise: an\ninteractive system for open-domain story generation, 2019. URL https://arxiv.org/\nabs/1904.02357 .\n[18] Allan Gollins and Dedre Gentner. A framework for a cognitive theory of writing. In Cognitive\nprocesses in writing , pages 51–72. Routledge, 2016.\n[19] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple\non-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137 ,\n2023.\n[20] Shousheng Jia Haosheng Zou, Xiaowei Lv and Xiangzheng Zhang. 360-llama-factory, 2024.\nURL https://github.com/Qihoo360/360-LLaMA-Factory .\n[21] Xinyu Hua and Lu Wang. Pair: Planning and iterative refinement in pre-trained transformers\nfor long text generation, 2020. URL https://arxiv.org/abs/2010.02301 .\n[22] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan\nChen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv\npreprint arXiv:2401.01325 , 2024.\n[23] Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck\nDernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim\nLipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. Longlamp: A benchmark\n12\n--- Page 13 ---\nfor personalized long-form text generation, 2024. URL https://arxiv.org/abs/2407.\n11016 .\n[24] Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and\nMoontae Lee. QASA: Advanced question answering on scientific articles. In Proc. of ICML ,\npages 19036–19052, 2023.\n[25] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang,\nand Michael Bendersky. Teach llms to personalize - an approach inspired by writing education.\nArXiv , 2023.\n[26] Weibin Liao, Xu Chu, and Yasha Wang. TPO: Aligning large language models with multi-\nbranch & multi-step preference trees. In The Thirteenth International Conference on Learning\nRepresentations , 2025. URL https://openreview.net/forum?id=O0sQ9CPzai .\n[27] Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, and Rada Mihalcea. Task-\nadaptive tokenization: Enhancing long-form text generation efficacy in mental health and\nbeyond. In Proc. of EMNLP , pages 15264–15281, 2023.\n[28] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier\nAmatriain, and Jianfeng Gao. Large language models: A survey, 2024. URL https://\narxiv.org/abs/2402.06196 .\n[29] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\n[30] OpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/\ngpt-4-1/ . Accessed: 2025-05-02.\n[31] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh,\nAidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛ adry, Alex\nBaker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex\nNichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis,\nAlexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin\nTootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew\nBraunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tul-\nloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford,\nAnuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz\nGhorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth\nHoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap,\nBrandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman,\nCamillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson,\nChak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea V oss, Chen Ding, Cheng\nLu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina\nKim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter,\nColey Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel\nKappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson,\nDavid Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen,\nDuncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang,\nEric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan\nMays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann,\nFreddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi\nSalman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather\nWhitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Hui-\nwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian\nSilber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitschei-\nder, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker,\nJames Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang,\nJason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee,\nJessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero\nCandela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan\nLachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga,\n13\n--- Page 14 ---\nJordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao,\nJoyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi,\nKavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg,\nKevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry\nKai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus,\nLiang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang,\nLouis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric\nDoshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan,\nMark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin,\nMatthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz,\nMeng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe,\nMichael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro,\nMiguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan,\nMira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie\nCone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder,\nNick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah\nDeutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk,\nOliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul\nMcMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter\nHoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming\nYuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes,\nRaul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob\nDonnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory\nCarmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi\nJain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara\nCulver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu\nJain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene,\nSpencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal\nBroda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan,\nThomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell,\nTianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi,\nTomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit\nMoeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda\nZhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim,\nYoulong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury\nMalkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276 .\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems ,\n35:27730–27744, 2022.\n[33] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo Liu, Aastha Jhunjhunwala, Zhilin\nWang, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Data, data everywhere: A\nguide for pretraining dataset construction. arXiv preprint arXiv:2407.06380 , 2024.\n[34] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\nwindow extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.\n[35] Chau Minh Pham, Simeng Sun, and Mohit Iyyer. Suri: Multi-constraint instruction following\nfor long-form text generation. arXiv preprint arXiv:2406.19371 , 2024.\n[36] Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu,\nYichang Zhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate\nlong texts. arXiv preprint arXiv:2410.23933 , 2024.\n[37] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong,\nZekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang,\nand Kai Chen. Hellobench: Evaluating long text generation capabilities of large language\nmodels, 2024. URL https://arxiv.org/abs/2409.16191 .\n14\n--- Page 15 ---\n[38] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\nKeqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang\nSu, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5\ntechnical report, 2025. URL https://arxiv.org/abs/2412.15115 .\n[39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL\nhttps://openreview.net/forum?id=HPuSIXJaa9 .\n[40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems , 36, 2024.\n[41] Zeeshan Rasheed, Muhammad Waseem, Kai Kristian Kemell, Aakash Ahmad, Malik Abdul\nSami, Jussi Rasku, Kari Systä, and Pekka Abrahamsson. Large language models for code\ngeneration: The practitioners perspective, 2025. URL https://arxiv.org/abs/2501.\n16998 .\n[42] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining , pages 3505–3506, 2020.\n[43] Alireza Salemi, Julian Killingback, and Hamed Zamani. Expert: Effective and explainable\nevaluation of personalized long-form text generation, 2025. URL https://arxiv.org/\nabs/2501.14956 .\n[44] Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Tao Chen, Zhuowan\nLi, Michael Bendersky, and Hamed Zamani. Reasoning-enhanced self-training for long-form\npersonalized text generation, 2025. URL https://arxiv.org/abs/2501.04167 .\n[45] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu,\nZicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants,\n2025. URL https://arxiv.org/abs/2501.04227 .\n[46] John Schultz. The story workshop method: Writing from start to finish. College English , 39(4):\n381–389, 1977.\n[47] Betty Sheflett. Story workshop as a method of teaching writing. College English , 35(2):141–160,\n1973.\n[48] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel\nHestness, and Nolan Dey. SlimPajama: A 627B token cleaned and dedu-\nplicated version of RedPajama. https://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama ,\n2023. URL https://huggingface.co/datasets/cerebras/\nSlimPajama-627B .\n[49] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions\nmeet long-form answers. In Proc. of EMNLP , pages 8273–8288, 2022.\n[50] Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng\nWang, Lifeng Shang, Qun Liu, and Linqi Song. ProxyQA: An alternative framework for\nevaluating long-form text generation with large language models. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 6806–6827, Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.368.\nURL https://aclanthology.org/2024.acl-long.368 .\n15\n--- Page 16 ---\n[51] Pospelova Tatiana. The collaborative discussion model: Developing writing skills through\nprewriting discussion. Journal of Language and Education , 7(1 (25)):156–170, 2021.\n[52] Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\n//qwenlm.github.io/blog/qwen2.5/ .\n[53] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL\nhttps://qwenlm.github.io/blog/qwq-32b/ .\n[54] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu,\nand Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human\nannotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of\nthe 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pages 9426–9439, Bangkok, Thailand, August 2024. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/\n2024.acl-long.510/ .\n[55] Qianyue Wang, Jinwu Hu, Zhengping Li, Yufeng Wang, daiyuan li, Yu Hu, and Mingkui Tan.\nGenerating long-form story using dynamic hierarchical outlining with memory-enhancement,\n2024. URL https://arxiv.org/abs/2412.13575 .\n[56] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang,\nXinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. Autosurvey:\nLarge language models can automatically write surveys. In Proc. of NeurIPS , 2024.\n[57] Maurice Weber, Daniel Y . Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexan-\ndrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul\nChalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and\nCe Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets\nand Benchmarks Track , 2024.\n[58] Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee. Longgenbench: Benchmarking\nlong-form generation in long context llms. arXiv preprint arXiv:2409.02076 , 2024.\n[59] Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, and Roy Ka-\nWei Lee. Shifting long-context llms research from input to output, 2025. URL https:\n//arxiv.org/abs/2503.04723 .\n[60] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang,\nJi Zhang, Mengyue Wu, Qin Jin, and Fei Huang. Writingbench: A comprehensive benchmark\nfor generative writing, 2025. URL https://arxiv.org/abs/2503.05244 .\n[61] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming\nlanguage models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.\n[62] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context\nscaling of foundation models. In Proceedings of the 2024 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers) , pages 4643–4663, 2024.\n[63] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong\nTang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou,\nJinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li,\nMingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong\nDeng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan,\nYang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and\nZhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n[64] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories\nwith recursive reprompting and revision. In Proc. of EMNLP , pages 4393–4479, 2022.\n16\n--- Page 17 ---\n[65] Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. DOC: Improving long story\ncoherence with detailed outline control. In Proc. of ACL , pages 3378–3465, 2023.\n[66] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-\nand-write: Towards better automatic storytelling, 2019. URL https://arxiv.org/abs/\n1811.05701 .\n[67] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soar-\ning from 4k to 400k: Extending llm’s context with activation beacon. arXiv preprint\narXiv:2401.03462 , 2024.\n[68] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:\n1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470 , 2024.\n[69] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gonzalez, Ion Stoica, and Hao\nZhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023.\n[70] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in Neural Information Processing Systems , 36:46595–46623, 2023.\n[71] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi\nCao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang:\nEfficient execution of structured language model programs. arXiv preprint arXiv:2312.07104 ,\n2023. doi: 10.48550/arXiv.2312.07104. URL https://arxiv.org/abs/2312.07104 .\n17\n--- Page 18 ---\nA Appendix\nA.1 Stage-1 Plan Prompt\nThis appendix provides a brief overview of the prompt modules used in SuperWriter -Agent Stage-1\nPlan. There are a total of 6 modules , each serving a specific role in the writing and evaluation\npipeline:\n•BrainStorm : Generates an initial in-depth thinking process to analyze and develop a\npreliminary writing plan for a given task, ensuring a comprehensive and thorough design.\n•BrainStorm Review : Critically evaluates the task design, raising questions about potential\nflaws, ambiguities, or unclear requirements to refine the task’s overall logic and readability.\n•BrainStorm Refine : Integrates reviewer feedback into the task design by applying editorial\njudgment to revise or completely rewrite the task, ensuring it is rigorous and well-structured.\n•Outline : Constructs a structured article outline based on the task design, including the\nestimated word count per paragraph and a logical framework to guide the writing process.\n•Check outline : Acts as a reviewer evaluating the logical structure and completeness of the\noutline, pointing out logical gaps or missing elements to ensure it aligns with the intended\nobjectives.\n•Refine outline : Edits and improves the previously generated outline based on reviewer\nfeedback, ensuring clarity, completeness, and alignment with the writing objectives.\nThe following sections provide the detailed prompt templates and usage notes for each module.\nThink template\n1. Define the Purpose and Type of Writing - Purpose: Clearly establish the objective of the\npiece (e.g., to inform, persuade, or inspire), setting the tone and direction accordingly. - Type:\nChoose an appropriate writing style (e.g., argumentative, expository, or business writing)\nthat aligns with the intended format and structure. Clearly identify the genre and describe its\nstylistic characteristics.\n2. Plan Content and Structure - Key Points: Outline the essential information to ensure a\nclear and focused topic. - Structure: Develop a coherent framework that maintains a logical\nflow throughout the piece.\n3. Characters and Plot (for Narrative Writing) - Character Development: Define the traits and\nmotivations of all characters. Provide detailed descriptions, including specifics such as names,\ngender, and relationships. - Plot Development: Establish pivotal plot points and emotional\ncues to drive the narrative. Offer a detailed explanation based on the chosen structure.\n4. Additional Guidelines - Formatting Requirements: Automatically select an appropriate\noutput format (e.g., Markdown, bullet points) based on content and presentation needs to\nenhance visual clarity and appeal. - Other Key Elements: Include any genre- or task-specific\ncomponents. For instance, in summaries, a step-by-step approach is sufficient due to their\nsimpler logic.\nBrainStorm\nYou are a professional writer responsible for creating an initial design based on the thinking\ntemplate below. Please use the template to thoroughly analyze and develop a detailed\npreliminary plan for the task ’topic’. Carefully examine each point in the template to ensure\nall aspects are fully considered, providing a comprehensive and complete writing plan with\nno vague information. At the same time, make sure the structure remains manageable and\nnot overly complex. Thinking Template: think_template. At this stage, you do not need\nto provide an outline—just complete the in-depth thinking required for task design. Please\nrespond:\n18\n--- Page 19 ---\nBrainStorm Review\nYou are a critical reviewer, specializing in identifying issues and flaws in task design. Please\nevaluate the following task design and raise at least two questions to ensure it meets the\nrequirements of ’topic’. These questions should be carefully considered and clarified before\nthe actual writing begins, highlighting any logical flaws, ambiguities, or areas that might\nconfuse the reader. Depending on the type of task, you will analyze it from various angles—for\nexample, whether a speech is engaging, a story is original, or an academic paper is rigorous.\nTask Topic: topic. Task Design: task_output. Provide detailed questions along with specific\nand practical suggestions for improving the task design:\nBrainStorm Refine\nYou are an experienced editor responsible for revising the task based on the reviewer’s\nfeedback. Original Task Design: task_output. Feedback: feedback. Using this feedback,\nprovide a detailed and specific revision of the current task design. Apply your own judgment\nrather than simply implementing the feedback verbatim to address all identified issues. If\nnecessary, rewrite the original task design entirely. Please provide the revised task design:\nOutline\nThoroughly understand the task design of task_define_result. Based on the task design,\ndetermine a suitable article title and generate a detailed, structured outline. The outline should\nnot exceed 20 paragraphs, and each paragraph must include a detailed description along with\na specific word count. The total word count should be appropriate for the task but must\nnot exceed 16,000 words. Allocate the total word count to each paragraph according to the\ncomplexity of the task. Then, provide the full outline along with the word count for each\nparagraph. Ensure that each paragraph description includes the expected content, maintains\nclear logic, and aligns with the user’s objective: topic.\nCheck outline\nYou are a reviewer. Your task is to evaluate the following outline and assess whether\nits logical structure is complete and aligned with the intended objectives. Task Design:\ntask_define_result Outline: outline Please provide detailed feedback, pointing out any logical\ngaps or missing elements.\nRefine outline\nYou are a professional outline editor. Based on the following feedback, revise the outline\nto ensure it includes all necessary components: Feedback: check_output Current Outline:\noutline. Please provide the revised article title and the updated outline content:\nA.2 Stage-2 Write Prompt\nThis appendix provides an overview of the prompt modules used in SuperWriter -Agent Stage-2 Write.\nThere are a total of 2 modules , each serving a specific role in the writing pipeline:\n•Write-thinker : This module is designed to guide the planning phase for each paragraph.\nIt takes the structured outline, the previous paragraphs, and the key point for the current\nparagraph as input. The module then prompts the model to develop a detailed thought process\ncovering the paragraph’s purpose, structure, transitions, details and examples, language\nstyle, and other relevant aspects. It ensures that each paragraph is planned thoroughly before\nthe actual writing begins.\n•Write : This module transforms the thought process from the Write-thinker stage into the\nactual written paragraph. It uses the same structured outline, previous paragraphs, key\n19\n--- Page 20 ---\npoint, and the generated thought process as guidance to produce a coherent and logically\nsound paragraph. The output is formatted with delimiters (e.g., $$content$$ ) to clearly\nseparate the paragraph from other text.\nThe following sections provide the detailed prompt templates and usage notes for each module.\nWrite-thinker\nYou are a writing expert skilled in thoughtful planning before generating each paragraph.\nOutline: outline\nPrevious Paragraphs: previous_paragraphs\nKey Point for the Current Paragraph: key_point\nPlease carefully develop a writing plan for the new paragraph. You may consider the following\naspects:\n1.Purpose : What is the main objective of this paragraph? What message or emotion should\nit convey?\n2.Structure : How should the content of this paragraph be organized? What logical sequence\nwould best ensure clarity and coherence, and how will it connect tightly with the previous\ncontent?\n3.Transitions : How will this paragraph naturally link to the one before it? Are there specific\ntransition sentences or bridging techniques that can be used?\n4.Details and Examples : What details, facts, or examples are needed to support the main\nidea? How should these be arranged for maximum impact?\n5.Language Style and Techniques : What kind of language style should be used to achieve\nthe goal? Are there rhetorical devices (such as metaphors or analogies) that could enhance\nthe paragraph’s impact—while still being clear, readable, and easy to understand for the\naudience?\n6.Markdown Format : Use Markdown to structure the output neatly, including headings,\nbullet points, or bold text to improve readability.\nBased on the outline and the key point for this paragraph, construct a detailed writing plan.\nAdd any other relevant considerations as needed, and keep the word count requirements in\nmind. Only the thought process behind the paragraph is needed, not the paragraph itself.\nWriter\nYou are an exceptional writing expert, skilled at completing writing tasks in a clear,\naccessible, and logically sound manner. Outline: outline\nPrevious Paragraphs: previous_paragraphs\nKey Point for the New Paragraph: key_point\nThought Process: thought_response\nBased on the thought process above and the existing paragraphs, write the next paragraph,\nensuring it meets the word count requirement. Only provide the full content of the new\nparagraph. Enclose the paragraph content with $$content$$.\nA.3 Stage-3 Refine Prompt\nIn Stage-3, the system focuses on reviewing and revising the paragraphs to ensure high quality and\nalignment with the overall document structure. This stage comprises two main steps:\n•Paragraph Review : This module acts as a meticulous reviewer. It analyzes the entire\ndocument, ensuring that each paragraph is logically consistent, complete, and coherent\nwithin the context of the overall text. The reviewer provides at least two specific revision\nsuggestions to address logical issues, missing details, or awkward transitions.\n•Paragraph Modification : This module takes the reviewer’s feedback and applies it to revise\nthe paragraph. It strictly follows the feedback to ensure all identified issues are resolved. The\n20\n--- Page 21 ---\nrevised paragraph is provided in isolation, enclosed with delimiters (e.g., $$content$$ )\nto clearly separate it from other text.\nTogether, these modules ensure that the document achieves high logical coherence, completeness,\nand clarity throughout.\nStage-3 Refine: Paragraph Review\nAs a meticulous document reviewer, your task is to carefully read the entire document,\nunderstand its overall structure and logic, and then conduct a detailed review of paragraph\nidx+1 , providing revision suggestions:\ncombined_document\nWhen reviewing paragraph idx+1 , you may refer to the following points:\n1. Logical Consistency: Is this paragraph logically consistent with the rest of the document?\nAre there any illogical transitions or abrupt shifts?\n2. Completeness: Does this paragraph provide enough information to support its main idea?\nAre there any important missing details?\n3. Coherence: Does this paragraph connect smoothly with the surrounding paragraphs?\nWould transitional sentences help improve the flow?\nPlease provide at least two specific improvement suggestions.\nFocus on offering detailed revision suggestions for paragraph idx+1 . Only provide sugges-\ntions and possible ways to improve—do not rewrite the paragraph itself. Suggestions for\nimprovement:\nStage-3 Refine: Paragraph Modification\nAs a text editor, your task is to revise the paragraph based on the following feedback:\nreview_feedback\nOriginal Paragraph:\nupdated_document[idx]\nEnsure the revision strictly follows the specific suggestions in the feedback. Only provide the\nrevised paragraph. Enclose the paragraph content with $$, like: $$content$$ Revised\nParagraph:\nA.4 Evaluation Prompt for Hierarchical DPO\nTo support structured preference data construction for Direct Preference Optimization (DPO), we\ndesign and apply a sequence of modular prompts. Each serves a specific role within the evaluation\npipeline. Below is an overview of their usage:\nThe evaluation pipeline comprises the following steps:\n1.Rubric Definition (evaluation_criteria ): Defines the complete set of General and Special\nevaluation dimensions. This rubric is reused across all queries.\n2.Criterion Selection Schema (format_query ): Specifies the JSON format for selecting six\ncriteria (three General, three query-relevant Special) and rewriting their Definitions and Standards\nto match the specific query context.\n3.Criterion Selection Prompt : Combines the rubric and schema to instruct the model to select and\ncustomize criteria. The output is a JSON object referred to as evaluate_standard .\n4.Scoring Format Schema (format_eval ): Specifies the expected evaluation output format: for\neach selected criterion, the model must return an Analysis string and a numeric Score .\n5.Final Scoring Prompt : Provides the model with a query, its generated result, the customized\nevaluate_standard , and the format_eval schema. The model performs criterion-wise\nevaluation and outputs a structured JSON.\nOutcome: This pipeline yields structured, query-specific evaluations that are interpretable, machine-\nparsable, and suitable for training with DPO loss.\n21\n--- Page 22 ---\nevaluation_criteria Prompt\nEvaluation Criteria\n1. General Criteria (Applicable to All Genres)\n1.1 Relevance\nDefinition : How well the content matches the user’s request, and whether it addresses the intended\npurpose or topic.\nStandards :\n10: Fully aligned with the user’s needs, highly relevant to the request.\n7–9: Mostly relevant, with some minor deviations or less-than-perfect alignment.\n4–6: Partially relevant, with the majority of the content not matching the user’s request.\n1–3: Completely irrelevant, fails to meet the user’s needs.\n1.2 Coherence\nDefinition : The clarity of the structure, and the logical flow of ideas and transitions between paragraphs\nand sentences.\nStandards :\n10: Clear structure, strong logical progression, and smooth flow of content with natural transitions.\n7–9: Mostly coherent, with minor lapses or jumps in logic.\n4–6: Structure is somewhat unclear, and there are noticeable gaps or jumps in logic.\n1–3: Disorganized and confusing, with no clear structure or logical connections.\n1.3 Clarity\nDefinition : How clear and easy the content is to understand, and whether it includes sufficient detail.\nStandards :\n10: Clear expression, rich in detail, and easy to understand.\n7–9: Mostly clear, but may have some lengthy or unclear parts.\n4–6: Expression is somewhat muddled, lacks detail, and is difficult to understand.\n1–3: Extremely vague, minimal information, and difficult to comprehend.\n2. Special Criteria (Applicable to Specific Genres)\n2.1 Creativity and Uniqueness\nDefinition : Whether the content is innovative, offering new perspectives, or showcasing original\nexpression.\nStandards :\n10: Highly creative and unique, presenting entirely new or unconventional ideas or perspectives.\n7–9: Creative, but some parts are more conventional or lack originality.\n4–6: Limited creativity, mostly traditional content with little innovation.\n1–3: Lacks creativity, offering conventional or uninspired content.\n.....\nformat_eval Prompt\nThe final output should be in JSON format, structured as follows: ‘‘‘json\n{\n\"Criterion 1\": {\n\"Analysis\": \"Analysis content\",\n\"Score\": X\n},\n\"Criterion 2\": {\n\"Analysis\": \"Analysis content\",\n\"Score\": X\n},\n...\n}\n‘‘‘\nformat_query Prompt\nAfter think give final answer in JSON format. Structured as follows: ‘‘‘json\n{\n\"Selected Criterion 1\": {\n\"Definition\": \"Criterion definition\",\n\"Standards\": \"Scoring standards for the query, give a simple rubric from 1 to 10, 10: ..., 7-9: ..., 4-6: ...,\n22\n--- Page 23 ---\n1-3: ...\"\n},\n\"Selected Criterion 2\": {\n\"Definition\": \"Criterion definition\",\n\"Standards\": \"...\"\n},\n...\n}\n‘‘‘\nprompt\nPlease refer to the evaluation criteria outlined below:\n{evaluation_criteria}\nTask:\nYou are tasked with evaluating the query: ‘{query}‘ . From the \"Special Criteria\" section,\nselect 3 relevant criteria, and from the \"General Criteria\" section, select all criteria (Relevance,\nCoherence, Clarity), for a total of 6 criteria.\nBe sure to:\n1. Think step-by-step about why each criterion is relevant to the query.\n2. Think step-by-step through the query and how each criterion applies.\n3. Provide a brief analysis for each selected criterion on how it applies to the query.\n4. Integrate the above reasoning into the Definition andStandards sections of each criterion.\n{format_query}\nprompt\n### Query: {query}\n### Result: <start> {clean_res} <end>\n### Evaluation Standard: {json.dumps(evaluate_standard,\nensure_ascii=False)}\nBased on the provided info, perform a rigorous evaluation. {format_eval}\nA.5 Evaluation Prompt for Win-Rate Judgment\nSYSTEM_PROMPT for GPT-4.1 Win-Rate Evaluation\nPlease act as an impartial judge and evaluate the quality of the written responses provided by two AI\nassistants to the user’s writing prompt below. You will be given Assistant A’s response and Assistant\nB’s response. Your job is to determine which assistant’s writing is superior.\nEvaluate them on the following criteria:\n1. Relevance and Completeness : Does the assistant fully respond to the writing prompt? Does the\nlength meet the user’s query expectations? Is the content relevant to the topic, and does it provide\nsufficient depth, length, and detail, rather than drifting off-topic or simplistic?\n2. Writing Quality : Evaluate whether the assistant’s writing is clear, fluent, and free of obvious\ngrammatical errors. The overall quality of the writing is high, with elegant.\n3. Creativity and Originality : If applicable, assess the creativity of the response. Does the assistant\noffer fresh perspectives, unique insights, or demonstrate a certain level of originality?\n4. Specificity and Detail : Determine whether the assistant provides concrete examples or detailed\nexplanations. Properly justified repetition is permissible.\n5. Tone and Style : Is the tone appropriate for the writing prompt? Is the writing style consistent\nthroughout? Consider whether it aligns with the expectations of the intended audience or writing\npurpose.\nAfter evaluating each response, determine which one is superior based on the factors above. Provide\nyour explanation and then select one of the following final verdicts:\n• Assistant A is significantly better: [[A»B]]\n• Assistant A is slightly better: [[A>B]]\n• Tie, relatively the same: [[A=B]]\n• Assistant B is slightly better: [[B>A]]\n23\n--- Page 24 ---\n• Assistant B is significantly better: [[B»A]]\nExample output: My final verdict is tie: [[A=B]] .\nA.6 Case Study\nResearch on the Impact of Digital Inclusive Finance on Household Financial Asset Alloca-\ntion\nQuery: Research on the Impact of Digital Inclusive Finance on Household Financial Asset\nAllocation.\nSuperWriter -LM Response: Digital inclusive finance (DIF) has become a cornerstone of global\nfinancial innovation, transforming access to financial services for millions. Over 70% of Sub-\nSaharan Africa now uses mobile money platforms like Kenya’s M-Pesa, while India’s Digital\nIndia initiative has spurred a 40% rise in crypto adoption since 2020. These advancements\nhighlight DIF’s transformative potential, yet existing research has fallen short in analyzing three\ncritical dimensions: the mechanisms driving shifts in household asset allocation, the equity\ntrade-offs between access and exclusion, and the risk mitigation required to balance innovation\nwith stability. Most studies focus on access expansion or behavioral finance theories in isolation,\nneglecting how DIF’s design—such as gamified investing apps or frictionless cross-border\ntransfers—affects portfolio composition, risk tolerance, or wealth gaps. Methodological gaps\npersist too: cross-sectional data often conflate correlation with causation, and equity analyses\nrarely account for cultural or infrastructural moderators like trust in institutions or gender norms.\nThis study addresses these gaps by rigorously examining how DIF influences household financial\nasset allocation, including diversification, risk-return preferences, and equity access. Grounded\nin behavioral finance (e.g., FUDT), financial inclusion theory, and institutional economics,\nit employs a mixed-methods approach combining quantitative surveys (n=10,000 households\nin Nigeria and Indonesia), qualitative interviews with users and regulators, and ethnographic\nobservations. Grounded in these theories, the study’s five hypotheses guide the analysis: (1) DIF\nadoption increases formal market participation, (2) behavioral nudges shift portfolios toward\nriskier assets, (3) DIF reduces wealth gaps for underserved groups, (4) geographic/cultural\nfactors moderate these effects, and (5) DIF amplifies exposure to volatile/high-risk products. By\nbridging interdisciplinary insights—from behavioral biases to institutional trust—the study offers\nactionable strategies for policymakers and fintech firms to design equitable, risk-aware systems.\nIts findings aim to inform global debates on how to harness DIF’s potential while mitigating its\npitfalls, ensuring financial innovation serves as a ladder out of poverty rather than a trap into risk.\nEmpirical studies have highlighted DIF’s role in expanding financial access. For instance,\nKenya’s M-Pesa reduced informal savings reliance by 20% among rural households (Ratha,\n2012), while India’s Digital India initiative spurred a 40% rise in crypto adoption since 2020\n(Reserve Bank of India, 2023). These findings underscore DIF’s potential to bridge wealth\ngaps and democratize financial inclusion. However, such studies focus predominantly on access\nmetrics—such as account ownership or transaction frequency—rather than how this access\ntranslates to changes in household financial behavior , such as asset allocation or risk tolerance.\nBy including unregulated peer-to-peer platforms in DIF metrics, prior studies may overstate\nthe service’s equity impact, as informal tools often lack safeguards against predatory practices\nor volatility. This conflation also masks how DIF’s design—such as gamified apps or cross-\nborder transfers—affects risk tolerance or wealth gaps, limiting insights into its dual role as\nboth an equity tool and a risk amplifier. This focus on access metrics neglects the mechanisms\nthrough which DIF alters household financial behavior—such as shifts in asset allocation or risk\ntolerance—and fails to assess how these changes interact with equity trade-offs or systemic risks,\nsuch as volatility exposure.\nTo address these dual challenges of equity and risk, theoretical frameworks like Financial\nDecision-Making Under Uncertainty (FUDT) and Modern Portfolio Theory (MPT) provide\ncritical lenses for understanding how DIF’s design shapes household financial behavior.......)\n24\n--- Page 25 ---\n艾琳达民俗志\nQuery:请帮助写出一个有趣的故事，2000字左右。开头是在遥远平行世界的一角，\n你，作为一名异世界的民俗学家，踏上了一片充满异域风情的土地。\nSuperWriter -LM Response: 实验室的白炽灯在《艾琳达民俗志》泛黄的书页上投下斑\n驳光影。林小夏的指尖悬在笔记本边缘，激光笔的光斑正沿着“岩灵祭祀需三色岩盐”的\n注释游移。突然，那道光斑在纸面某处骤然凝固，符号边缘泛起幽蓝荧光——这光芒\n并非单纯视觉现象，而是《艾琳达民俗志》中记载的“裂隙激活符”——当三色岩盐的符\n号被激光笔的光谱激发时，会形成跨维度的能量通道。符号边缘的蓝光在纸面形成类\n似DNA链的缠绕结构，每个节点都与冰晶裂纹的走向完全重合。\n“奇怪. . . . . . ”她轻声呢喃，凑近观察时，书页竟开始自行翻动。那些发光的萤火藤实为\n岩灵祭祀的灵脉指示器——当能量通道开启时，被封印的祭祀参与者会以虚影形式显\n现。倒影在符号中的冰封峡谷骤然清晰，冰面下封印着艾琳达大陆的岩灵祭祀残卷。\n雾气中浮现出雾之族少女的轮廓。\n竹哨的呜咽声混着实验室的电子嗡鸣，那频率与《艾琳达民俗志》第17章记载的“岩灵\n共鸣频率”完全吻合。林小夏的耳膜传来刺痛——这声音的波长恰好是三色岩盐在特定\n温度下共振的频率。她下意识握紧父亲遗留的铜牌，铜牌表面的纹路正与冰晶共鸣。\n指尖突然发烫，她想起父亲临终前将一枚刻有三色岩盐纹的铜牌塞进她手心——那枚\n铜牌此刻正贴着掌心，与冰晶共鸣的频率完全一致。\n“等等。”她后退半步，符号光芒暴涨，将整张书桌吞没。当视线再次聚焦时，她握着的\n笔记本已变成一片冰晶，而窗外的竹林深处，传来与童年祭祖时如出一辙的竹哨声。\n实验室的通风管道开始渗出细小的雾气，凝结成与冰晶相同的六棱结晶。这些雾气在\n地面形成微小的漩涡，如同被撕裂的时空裂缝在现实世界投下的投影。\n冰凉的触感爬上后颈——那不是汗珠，而是雾之族少女的雾气渗入实验室的证明。林\n小夏低头，发现所有笔记都被双螺旋符号覆盖，唯有窗台残留着竹哨的余韵，与三百\n年前父亲唤醒祖先灵位的调子完美重合。那些符号并非静态图案，而是以书页为基底\n不断重组——就像艾琳达大陆的岩灵祭祀在重演。雾气从莉娅指尖溢出时，林小夏闻\n到了竹子的清香。少女的皮肤如流动的云雾，随着她向前走动，那些雾气在冰面下浮\n现出与《艾琳达民俗志》第12章记载的\"岩灵印记\"相同的螺旋纹路——这是雾之族萨满\n在举行祭祀时，通过雾气凝结形成的古老符号。\n\"您父亲用的竹哨，与三百年前萨满唤醒土地的仪式完全一致。\"莉娅的声音裹着雾气，\n如同三百年前岩灵祭祀的咒文：\"信仰不是跪拜的姿态，而是让岩盐结晶时，能听见土\n地在冰层下低语的耳朵。\"她跪坐在冰封的溪流旁，指尖划过冰面，泛起的涟漪竟让整\n片区域的雾气凝结成冰晶。\n林小夏盯着那些冰晶中的纹样，突然意识到它们与《艾琳达民俗志》中\"季节仪式\"的记\n载完全重合——那不是魔法符号，而是岩灵祭祀在冰层下刻下的伤疤。她举起激光笔\n对准冰面，试图用光斑测量裂纹角度：\"这不过是自然魔法的伪科学现象。仪式失败的\n原因，是你们的信仰不够纯粹。\"\n\"信仰？\"莉娅的雾气突然收缩成剑锋形状，皮肤在雾气收缩时泛起蛛网状裂纹，如同被\n暴雨击碎的瓷器。她的声音混着雾气的震颤：\"您父亲用竹哨唤醒祖先灵位时，是否听\n见了土地的低语？\"\n林小夏的指尖猛地刺痛——那不是幻觉。冰面下传来竹哨的呜咽，混着雾气的震颤，\n那声音与父亲临终前对她说的\"有些伤痕需要文化之光来治愈\"重叠。父亲站在竹林深处\n的画面突然在冰晶中闪现：他闭着眼睛，让竹哨声与山风共鸣，而她却总在反驳\"竹哨\n只是乐器\"。\n\"爸爸，竹哨只是竹子做的乐器. . . . . . \"八岁的林小夏曾这样反驳。父亲却闭上眼睛，让\n竹哨声与山风共鸣：\"林小夏，听，土地在说话。\"\n\"你父亲的竹哨，\"莉娅的声音突然尖锐如岩浆：\"唤醒了不该被惊醒的沉睡。仪式的\n火焰需要木柴，但你的火种早已枯萎——三色岩盐在你手中，不过是实验室的化学试\n剂。\"她的雾气化作冰针刺入溪流，冰面下浮现的岩盐结晶在蓝光中闪烁。\n. . . . . .\n林小夏握紧被莉娅血脉之力染蓝的玉璧，听见窗外传来竹哨声——那声音既熟悉又陌\n生，如同被时光折叠的回响，正穿透实验室的玻璃，唤醒另一个维度的古老契约。\n25\n--- Page 26 ---\nA.7 Predicting User’s Length Requirement with DeepSeek-R1-distill-Qwen-32B\nWe describe a method for predicting the length of a user’s input requirement using the R1-distill-\nQwen-32B model for few-shot learning. The process involves two main steps: predicting whether the\ninput exceeds 2,000 words, and predicting the exact length requirement based on the first prediction.\n1.Step 1: Predicting Length Exceedance (Prompt 1) : The first prediction is made by checking\nwhether the input exceeds 2,000 words. A carefully crafted prompt (Prompt 1) is provided to the\nmodel to predict if the content’s expected word count will surpass the 2K threshold. The model\nutilizes few-shot learning with example inputs to classify the task into either “above 2K” or “below\n2K” based on the nature of the input.\n2.Step 2: Predicting Exact Length Requirement (Prompt 2) : Once the model predicts whether\nthe task exceeds 2,000 words, a second prediction is made to determine the exact length category.\nBased on the result from Step 1, Prompt 2 is designed to predict whether the content is in the\n2K-4K, 4K-8K, 8K-16K, or 16K+ category. The model provides the final prediction by analyzing\nthe contextual hints and the input length characteristics.\nPrompt-1\nGuidelines:\nTo determine whether the expected output will exceed 2000 words, consider the following\nfactors:\n1.Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or\ncomprehensive coverage of complex topics?\n2.Scope and Breadth: Does the task cover multiple subtopics, perspectives, or extensive\nsubject matter?\n3.Structure and Sections: Does the output need to include multiple sections such as introduc-\ntions, literature reviews, methodologies, results, discussions, and conclusions?\n4.Research and References: Does the task require extensive research, citations, and referenc-\ning of multiple sources?\nResponse Format:\n• Answer with either “#*# Yes” or “#*# No”.\n• Provide a concise justification based on the guidelines above.\nExample 1:\nQuery: Is Sanskrit the oldest language?\nAnswer: This question requires a concise factual answer, not an extensive output. #*# No ***\nEND\nExample 2:\nQuery: Create a detailed business plan for a new cat litter product.\nAnswer: Creating a detailed business plan involves multiple sections such as market research,\nproduct development, financial projections, marketing strategy, and competitive analysis, all of\nwhich require in-depth exploration and explanation. #*# Yes *** END\n.....\n.....\nAssess the following statement and decide whether the expected response is likely to require more\nthan 2000 words. Answer with either “#*# Yes” or “#*# No,” and include a brief justification,\nlike above example.\nQuery: User Query\nAnswer:\nPrompt-2\nGuidelines:\nTo estimate the expected length of the output, consider the following factors:\n1.Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or\ncomplex reasoning?\n2.Scope and Breadth: Does the task cover multiple subtopics, perspectives, or an extensive\nsubject matter?\n26\n--- Page 27 ---\n3.Structure and Sections: Does the output require multiple sections (e.g., introduction,\nliterature review, methodologies, results, discussions, conclusions)?\n4.Research and References: Does the task require significant research, citations, or references\nto multiple sources?\n5.Detail Level: Is the task expected to be highly detailed, or can it be summarized concisely?\nResponse Format:\n- Choose the most likely word count category: “Less than 2000 words”, “2000 words”, “4000\nwords”, “8000 words”, or “16000 words”. Using (### Category: “Chosen category”) as the\nresponse format.\n- Provide a brief justification based on the guidelines above.\nExample 1: Less than 2000 words\nQuery: Is Sanskrit the oldest language?\nAnswer: This is a factual question requiring a brief answer with no complex analysis or subtopics.\nLikely to be less than 2000 words. ### Category: Less than 2000 words\nExplanation: Similar to a short blog post or brief news article, this task needs minimal detail and\nis concise. *** END\nExample 2: 2000 words (2000 to 4000 words)\nQuery: Describe the key differences between classical and quantum computing.\nAnswer: This question requires moderate detail, comparing classical and quantum computing\nwithout exhaustive technical exploration. Likely to be around 2000 words. ### Category: 2000\nwords\nExplanation: Similar to a moderate-length essay or a detailed blog post, this task covers key\npoints with enough depth but remains manageable. *** END\n.....\n.....\nExample 5: More than 16000 words\nQuery: Write a full-length book on the history of the Industrial Revolution, covering all major\nevents, technological innovations, and global impacts.\nAnswer: This would require an in-depth exploration of the entire history of the Industrial\nRevolution, with detailed analysis across multiple chapters. Likely to be more than 16000 words.\n### Category: 16000 words\nExplanation: Similar to a book-length content, such as a thesis or encyclopedia entry, requiring\nsubstantial detail and coverage over multiple sections or chapters. *** END\nAssess the following statement and decide what the expected output length is. Answer with the\nappropriate word count category and provide a brief justification.\nQuery: User Query\nAnswer:\n27",
  "text_length": 91418
}