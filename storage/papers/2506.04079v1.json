{
  "id": "http://arxiv.org/abs/2506.04079v1",
  "title": "EuroLLM-9B: Technical Report",
  "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.",
  "authors": [
    "Pedro Henrique Martins",
    "João Alves",
    "Patrick Fernandes",
    "Nuno M. Guerreiro",
    "Ricardo Rei",
    "Amin Farajian",
    "Mateusz Klimaszewski",
    "Duarte M. Alves",
    "José Pombal",
    "Manuel Faysse",
    "Pierre Colombo",
    "François Yvon",
    "Barry Haddow",
    "José G. C. de Souza",
    "Alexandra Birch",
    "André F. T. Martins"
  ],
  "published": "2025-06-04T15:43:31Z",
  "updated": "2025-06-04T15:43:31Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04079v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04079v1  [cs.CL]  4 Jun 2025EUROLLM-9B: T ECHNICAL REPORT\nPedro Henrique Martins*ÈJoão Alves*1Patrick Fernandes*2,3Nuno M. Guerreiro*1,2,4\nRicardo Rei*1Amin Farajian*1Mateusz Klimaszewski*6Duarte M. Alves2\nJosé Pombal1,2Manuel Faysse4,5Pierre Colombo4,7François Yvon9Barry Haddow6,8\nJosé G. C. de Souza1Alexandra Birch ⋄6,8André F. T. Martins ⋄1,2\n1Unbabel2Instituto de Telecomunicações & Instituto Superior Técnico, Universidade de Lisboa\n3Carnegie Mellon University4MICS, CentraleSupélec, Université Paris-Saclay\n5Illuin Technology6University of Edinburgh7Equall8Aveni\n9Sorbonne Université, CNRS, ISIR\nABSTRACT\nThis report presents EuroLLM-9B , a large language model trained from scratch to\nsupport the needs of European citizens by covering all 24 official European Union\nlanguages and 11 additional languages. EuroLLM addresses the issue of Euro-\npean languages being underrepresented and underserved in existing open large\nlanguage models. We provide a comprehensive overview of EuroLLM-9B’s de-\nvelopment, including tokenizer design, architectural specifications, data filtering,\nand training procedures. We describe the pre-training data collection and filter-\ning pipeline, including the creation of EuroFilter , an AI-based multilingual filter,\nas well as the design of EuroBlocks-Synthetic , a novel synthetic dataset for post-\ntraining that enhances language coverage for European languages.\nEvaluation results demonstrate EuroLLM-9B’s competitive performance on mul-\ntilingual benchmarks and machine translation tasks, establishing it as the leading\nopen European-made LLM of its size. To support open research and adoption,\nwe release all major components of this work, including the base and instruction-\ntuned models, the EuroFilter classifier, and the synthetic post-training dataset.1\n1 I NTRODUCTION\nLarge language models (LLMs) have emerged as key drivers of progress in natural language pro-\ncessing (NLP) and artificial intelligence (AI), with notable examples including OpenAI’s GPT se-\nries (OpenAI et al., 2024), Anthropic’s Claude (Anthropic, 2023) or Google’s Gemini (Google et al.,\n2025). LLMs are first pre-trained on vast amounts of unlabelled data relying on a self-supervised\ntask ( e.g., next word prediction or missing word prediction). This process enables the model to ac-\nquire knowledge, to develop strong language understanding and generation skills, and to perform\nvarious downstream tasks, often leveraging in-context learning techniques. Following pre-training,\nLLMs are further refined through post-training techniques that enhance their ability to follow natu-\nral language instructions, improve task-specific performance, and enhance the adherence to safety\nprotocols.\nDespite the growing availability of open-weight LLMs such as LLaMA, Mistral, Gemma, DeepSeek,\nand Qwen (Llama Team et al., 2024; Jiang et al., 2023; Gemma Team et al., 2024; DeepSeek-AI\net al., 2025; Qwen-Team et al., 2025), most advanced models are closed, owned by major corpora-\ntions with limited commitment to open science. Moreover, existing open models primarily focus on\nEnglish and a few high-resource languages, leaving many European languages underserved.\nTo address this gap, we launched the EuroLLM project with the aim of developing a suite of open\nLLMs capable of understanding and generating text in all 24 official European Union languages\n(Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek,\nHungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak,\nSlovenian, Spanish, and Swedish). As the aim of EuroLLM is to provide EU citizens with pow-\nerful and useful AI tools, it is critical that the model can also translate and answer questions in other\n* Core contributors, ⋄equal contributors,Èwork done while working at Unbabel\n1Resources available in HuggingFace as part of the EuroLLM Collection.\n1\n--- Page 2 ---\nEuropean and non-European languages. With this in mind, we added support for 11 additional lan-\nguages (Arabic, Catalan, Chinese, Galician, Hindi, Japanese, Korean, Norwegian, Russian, Turkish,\nand Ukrainian). Our journey began with the release of EuroLLM-1.7B (Martins et al., 2024), a com-\npact yet powerful model that excels in machine translation and performs competitively on general\nbenchmarks. Building on this foundation, we now introduce the technical report on EuroLLM-9B, a\nmodel that, at the date of its release (December 9, 2024), was the most capable open European-made\nLLM of its size.\nThis technical report provides a comprehensive overview of the development and evaluation of\nEuroLLM-9B:\n• We begin by examining the pre-training phase of EuroLLM in §2, where we introduce and\ndescribe EuroFilter —a multilingual AI-based filter used to curate the pre-training data.\nWe also release EuroFilter alongside this report to support future work on high-quality\nmultilingual data filtering.\n• In §3, we describe the post-training process of EuroLLM-9B and introduce EuroBlocks-\nSynthetic —a post-training dataset that extends existing synthethic data with multilingual\ninstructions covering many European languages.\n• Finally, we present EuroLLM-9B’s performance on multilingual benchmarks in §4, com-\nparing it with leading open LLMs.\nTo support further research and development, we openly release all major components introduced in\nthis report: the base model (EuroLLM-9B), the instruction-tuned variant (EuroLLM-9B-Instruct),\nthe multilingual data filter (EuroFilter), and the synthetic post-training dataset (EuroBlocks-\nSynthetic).\n2 P RE-TRAINING\nIn this section, we describe the pre-training process of EuroLLM-9B, covering the tokenizer mod-\neling (§2.1), architectural decisions (§2.2), the pre-training phases (§2.3), and the pre-training data\n(§2.4). We carried out the pre-training using the Megatron-LM codebase (Shoeybi et al., 2020).\n2.1 T OKENIZER\nThe starting point for developing EuroLLM is designing the tokenizer that best fits our language\ncoverage. To train the tokenizer, we adopt the BPE (Sennrich et al., 2016) with byte-fallback algo-\nrithm, following the approach used by the LLaMa-2 and Mistral-7B models (Touvron et al., 2023;\nJiang et al., 2023). To do so, we use the SentencePiece framework (Kudo & Richardson, 2018).\nFor multilingual language models, the vocabulary size of the tokenizer presents a crucial trade-off:\nwhile larger vocabularies enable efficient processing across multiple languages, they also increase\nthe model’s embedding parameter count. Through experimentation, we determined that a vocabulary\nsize of 128,000 pieces offers a good balance between these competing factors.\nTo evaluate our tokenizer’s performance, we conducted a comparison with the tokenizers of several\nopen-weight LLMs: Mistral-7B, LLaMa-3, Gemma-2, Teuken, and Salamandra (Jiang et al., 2023;\nAI@Meta, 2024; Gemma Team et al., 2024; Ali et al., 2024; Gonzalez-Agirre et al., 2025). The\ncomparison focuses on tokenizer fertility—the average number of tokens per word. The compared\nmodels feature varying vocabulary sizes: Mistral-7B with 32,000 pieces, LLaMa-3 with 128,256\npieces, Teuken with 250,680 pieces, and both Gemma-2 and Salamandra with 256,000 pieces. For\nthis analysis, we used a concatenation of the F LORES -200 (Team et al., 2022) and Universal De-\npendencies datasets (Nivre et al., 2020) for each language.\nFigure 1 presents the fertility rates for a subset of the languages included in EuroLLM. Our analy-\nsis reveals that, compared to the Mistral-7B tokenizer, the larger vocabulary of EuroLLM leads to\nsignificantly lower fertility rates. While LLaMa-3, with its similar vocabulary size, demonstrates\nsuperior (lower) fertility for English, it shows higher fertility rates for most other languages in our\nevaluation. Notably, despite having a smaller vocabulary than Gemma-2, Teuken, and Salamandra,\nEuroLLM achieves comparable fertility levels across the evaluated languages.\n2.2 M ODELING\n2\n--- Page 3 ---\nFigure 1: Fertility (tokens per word) obtained with the Mistral-7B, LLaMa-3, Gemma-2, Salaman-\ndra, Teuken, and EuroLLM tokenizers for a subset of the EuroLLM languages. With 128k tokens,\nEuroLLM achieves comparable fertilities to 256k token models (Gemma, Teuken, Salamandra) for\nmost languages, saving 50% of the embedding parameters.\n9B\nSequence Length 4,096\nNumber of Layers 42\nEmbedding Size 4,096\nFFN Hidden Size 12,288\nNumber of Heads 32\nNumber of KV Heads (GQA) 8\nActivation Function SwiGLU\nPosition Encodings RoPE ( Θ=10,000)\nLayer Norm RMSNorm\nTied Embeddings No\nMax Learning Rate 3×10−4\nEmbedding Parameters 0.524B\nLM Head Parameters 0.524B\nNon-embedding Parameters 8.105B\nTotal Parameters 9.153B\nTable 1: EuroLLM-9B hyperparameters.EuroLLM-9B uses a standard, dense Trans-\nformer architecture (Vaswani et al., 2017)\nwith the same design choices as EuroLLM-\n1.7B (Martins et al., 2024).\n• We use grouped query attention (GQA;\nAinslie et al. (2023)) with 8 key-value\nheads, which has been demonstrated to\nenhance inference speed while preserving\ndownstream performance (Gemma 2 Team\net al., 2024).\n• For improved training stability, we use pre-\nlayer normalization (Xiong et al., 2020)\nand RMSNorm (Zhang & Sennrich, 2019),\nwhich offers faster computation compared\nto LayerNorm (Ba et al., 2016).\n• We use the SwiGLU activation function\n(Shazeer, 2020) since it has been shown to\nlead to good results on downstream tasks\n(Shazeer, 2020; Le Scao et al., 2022).\n• We use rotary positional embeddings\n(RoPE; Su et al. (2024)) in every layer\nsince these have been shown to lead to good\nperformances while allowing the extension\nof the context length (Chen et al., 2023).\n2.3 P RE-TRAINING PHASES\nWe pre-train EuroLLM-9B on approximately 4 trillion tokens using a trapezoid learning rate sched-\nuler (Xing et al., 2018) (also named Warmup-Stable-Decay (Hu et al., 2024)). We conduct training\n3\n--- Page 4 ---\nFigure 2: Scheme of the learning rate scheduler.\non 400 Nvidia H100 GPUs from the MareNostrum 5 supercomputer, maintaining a constant batch\nsize of 2,800 sequences (approximately 12 million tokens), and employing the Adam optimizer\n(Kingma & Ba, 2014), with bfloat16 mixed precision.\nThe training process consists of three distinct phases, as shown in Figure 2. As we progress through\ntraining, we expose the model to higher quality and specialised data (AI@Meta, 2024).\n•1stPhase: Learning Rate Warm-up and Plateau . The initial phase involves linearly\nincreasing the learning rate during the first 10% of training steps followed by maintaining\nit at a constant level for the subsequent 80%. This phase comprises approximately 3.6\ntrillion tokens.\n•2ndPhase: Learning Rate Annealing . During this phase, the learning rate decays linearly\nfrom its maximum value ( 3×10−4) to 10% of its peak ( 3×10−5). This annealing period\nencompasses roughly 10% of training steps, processing about 400 billion tokens.\n•3rdPhase: Annealing to Zero . During the final phase, the learning rate decays linearly to\nzero over a brief period, corresponding to about 40 billion tokens. This phase is a novelty\nof EuroLLM-9B compared to EuroLLM-1.7B (Martins et al., 2024).\nWe will describe next the data used in these three phases.\n2.4 D ATA\nTo train EuroLLM-9B, we collect and filter data from various sources for all supported languages.\nThe data that composes the final corpus can be categorized into four main types: web data, parallel\ndata, code / math data, and higher-quality data. The data distribution for the first two phases is kept\nthe same as that of the 1.7B model. Figure 3 illustrates the distribution of these categories across the\nthree pre-training phases.\n2.4.1 D ATA COLLECTION AND FILTERING (EUROFILTER )\nWeb Data For web data collection, we employ different strategies based on language resources.\nFor English, we use the FineWeb-edu dataset (Lozhkov et al., 2024), selecting documents with edu-\ncational scores above 2 according to their model-based classifier. The dataset underwent individual\ndump deduplication and heuristic filtering.\n4\n--- Page 5 ---\nFigure 3: Percentage attributed to each data category in the 3 pre-training phases.\nFor other high-resource languages (German, Spanish, French, and Italian), we collect data from\nRedPajama-Data-v2 (Computer, 2023), which has been pre-deduplicated. We further apply perplex-\nity filtering using KenLM (Heafield, 2011), along with various heuristic filters. Specifically, we\nremove documents with fewer than 200 characters (Xue et al., 2021a) and any page containing the\nphrase “lorem ipsum,” the word “javascript,” or curly brackets (Raffel et al., 2023). Additionally, we\nexclude paragraphs where the fraction of uppercase letters exceeds 40%, the symbol-to-word ratio\nis greater than 0.1, or the ratio of words without alphabetic letters exceeds 0.2 (Rae et al., 2022).\nFor the remaining languages, we aggregate data from several datasets: HPLT (de Gibert et al.,\n2024), MADLAD-400 (Kudugunta et al., 2023), CulturaX (Nguyen et al., 2023), and mC4 (Xue\net al., 2021b). After concatenation, we perform deduplication, language identification, perplexity\nfiltering, and the same set of heuristic filters that we used for the high-resource languages, using a\nCCNet-based preprocessing pipeline (Wenzek et al., 2019).\nTo further improve the quality of the web data used in the 2ndand 3rdtraining phases, we raise\nthe FineWeb-edu score threshold to 3 for English. For the other languages, we reuse the FineWeb-\nEdu (Lozhkov et al., 2024) annotation; however, we translate all data using T OWER V 2-supported\nlanguages (Rei et al., 2024). This is done to create multilingual texts paired with educational scores.\nThen we train a multilingual classifier on top of mDeBERTa (He et al., 2023) which we use to\nannotate the rest of the languages. Our filter is publicly available at utter-project/EuroFilter-v1.\nParallel Data Regarding parallel data, we collect sentence-level to-English (xx →en) and from-\nEnglish (en →xx) parallel data from various public sources listed in Table 2.\nWe use Bifixer (Ramírez-Sánchez et al., 2020) to remove duplicates and ensure translation quality\nby removing sentence pairs below quality thresholds for Bicleaner (Sánchez-Cartagena et al., 2018;\nRamírez-Sánchez et al., 2020) and C OMET KIWI-22 (Rei et al., 2022b). For Bicleaner, we use a\nthreshold of 0.6 for Portuguese and of 0.5 for all the other languages. For C OMET KIWI-22 we use\na threshold of 0.7.\nFor the 2ndand 3rdtraining phases we also collect document-level data from Europarl (Koehn, 2005)\nand ParaDocs (Wicks et al., 2024), applying the same filtering criteria.\nCode / Math Data We collect code and mathematical data from The Stack (Kocetkov et al., 2022),\nthe Algebraic-stack (Azerbayev et al., 2023), and Open-web-math (Paster et al., 2023).\nFor the 2ndand 3rdtraining phases we also incorporate the python-edu dataset (Ben Allal et al., 2024)\nand the training sets of GSM8k (Cobbe et al., 2021) and of Mathematics Aptitude Test of Heuristics\n(Hendrycks et al., 2021b).\n5\n--- Page 6 ---\nDataset Version\nEuroparl (Koehn, 2005) v8\nParaCrawl (Esplà et al., 2019) v9\nMultiParaCrawl (Esplà et al., 2019) v7.1\nCCMatrix (Schwenk et al., 2020) v1\nCCAligned (El-Kishky et al., 2020) v1\nMultiCCAligned (El-Kishky et al., 2020) v1\nWikiTitles (Tiedemann, 2012) v2014\nWikiMatrix (Schwenk et al., 2019) v1\nNews-Commentary (Tiedemann, 2012) v16\nOPUS100 (Zhang et al., 2020) v1\nTildeModel (Rozis & Skadin ,š, 2017) v2018\nBible (Mayer & Cysouw, 2014) v1\nUbuntu (Tiedemann, 2012) v14.10\nTatoeba (Tiedemann, 2012) v2\nGNOME (Tiedemann, 2012) v1\nGlobalV oices (Tiedemann, 2012) v2018q4\nKDE4 (Tiedemann, 2012) v2\nKDE-Doc (Tiedemann, 2012) v1\nPHP (Tiedemann, 2012) v1\nWikipedia (Wołk & Marasek, 2014) v1.0\nWikimedia (Tiedemann, 2012) v20210402\nJRC (Tiedemann, 2012) v3.0\nDGT (Tiedemann, 2012) v2019\nEuroPat (Europat) v3\nEUbookshop (Tiedemann, 2012) v2\nEMEA (Tiedemann, 2012) v3\nEUConst (Tiedemann, 2012) v1\ntico-19 (Anastasopoulos et al., 2020) v20201028\nECB (Tiedemann, 2012) v1\nElitr-ECA (Williams & Haddow, 2021) v1\nMultiUN (Eisele & Chen, 2010) v1\nOpenOffice (Tiedemann, 2012) v3\nAda83 (Tiedemann, 2012) v1\ninfopankki (Tiedemann, 2012) v1\nScielo (Soares et al., 2018) v1\ngiga-fren (Tiedemann, 2012) v2\nUNPC (Ziemski et al., 2016) v1.0\nTable 2: Data sources from which we collect parallel data along with the datasets’ version.\nFor the 3rdphase, we also include synthetic data (around 1.7 million samples) generated using\nthe Qwen-2.5 models (Qwen-Team et al., 2025; Yang et al., 2024), which were used not only to\nrewrite questions, but also to generate answers to original data from MathInstruct (Toshniwal et al.,\n2024c;b) and MetaMathQA (Yu et al., 2024). These answers were then evaluated using techniques\nsuch as LLM-as-a-Judge (Zheng et al., 2023b). Specifically, this involved generating answers us-\ning Qwen2.5-Math-7B, and subsequently evaluating them with Qwen2.5-Instruct as an LLM-as-a-\nJudge (filtering at a score of 9/10). Additionally, we also drawn samples from those datasets, and\ngenerated multiple-choice questions based on the original data, employing gemma-2-9b-it . The\ndataset was further augmented with samples from SlimOrca , which included original prompts and\ngenerations from gemma-2-9b-it ,gemma-2-27b-it ,Llama-3.1-70B-Instruct , and\nQwen2.5-32B-Instruct ; for these, Qwen2.5-Instruct provided judgements to ascertain\nthe ’best-of-N’ answer, with random sampling applied in cases of tied maximum ratings.\nHigher-quality Data Regarding high-quality data, we use the Wikipedia (Foundation) for all lan-\nguages and the Arxiv (Clement et al., 2019), Books (Zhu et al., 2015), and Apollo (Wang et al.,\n2024b) for English.\nFor the 2ndand 3rdtraining phases we add the Cosmopedia dataset (2ndversion; Ben Allal et al.\n(2024)). For the 3rdwe further include documents of Cosmopedia translated using Tower (Alves\net al., 2024) to German, Spanish, French, Italian, Portuguese, Dutch, Chinese, and Russian.\n6\n--- Page 7 ---\nFigure 4: Percentage of the training corpus attributed to each language, excluding English and code\n/ math data. English accounts to 50% in the 1stphase and 32.5% during the 2ndphase and 3rdphases.\n5% of the corpus is left for datasets composed of code and math in the first phase, 7% during the 2nd\nphase and 23% during the 3rdphase.\n2.5 D IVISION ACROSS LANGUAGES\nThe training corpus distribution evolves across phases to optimize multilingual capabilities and rea-\nsoning skills:\n•1stPhase : During this phase, we designate 50% for English, as both high-quality data and\nweb data are predominantly in English, and include 5% of code / math data. The remaining\n45% of the tokens are distributed among the other languages based on the amount of data\nobtained after the collection and filtering processes as shown by the blue bars on Fig. 4.\n•2ndPhase : In order to increase EuroLLM’s multilinguality, we decrease the English allo-\ncation to 32.5% and distribute the surplus across the other languages. We also increase the\ncode / math allocation to 7%. See analysis in §5.1.1.\n•3rdPhase : Finally, to improve the model’s reasoning abilities, in this last phase, we in-\ncrease the code / math allocation to 23%, by maintaining the English allocation but revert-\ning the multilingual increase done for the 2ndphase. See analysis in §5.1.2. This is the\nmain difference in terms of data and training to that of the EuroLLM-1.7B model (Martins\net al., 2024).\nFigure 4 shows the exact percentage attributed to each language in the three pre-training phases.\nThese design decisions for data mixes were done with careful scaling laws that are thoroughly de-\nscribed in Martins et al. (2024).\n2.5.1 P RE-TRAINING PROGRESS\nWe track the performance of EuroLLM-9B throughout the pre-training process along its three dif-\nferent phases.\nThe results, shown in Figure 5, demonstrate consistent improvement across all benchmarks during\npre-training, with particularly notable gains in the second phase. During the third phase, there is an\neven steeper improvement on Arc Challenge and MMLU, but a slight decline on Hellaswag. We\nattribute this decrease to the increased proportion of code and math data in the corpus used for the\nthird pre-training phase.\n7\n--- Page 8 ---\nFigure 5: Results on Arc Challenge (left), Hellaswag (middle), and MMLU (right) throughout the\npre-training process averaged across 11 EU languages.\n3 P OST TRAINING\nIn this section, we describe the post-training process of EuroLLM-9B-Instruct, covering the post-\ntraining data—released as EuroBlocks (§3.1)—and the modeling decisions (§3.2). We carried out\nthe post-training using the Axolotl codebase2.\n3.1 D ATA\nTo enable EuroLLM-9B to follow natural language instructions, we constructed EuroBlocks, a mul-\ntilingual dataset that combines both human-written and synthetic instruction-following conversa-\ntions. The human-written portion draws from several publicly available sources, including Magpie\n(Xu et al., 2024)3, Aya (Singh et al., 2024), lmsys-chat-1m (Zheng et al., 2023a), OpenMath-2\n(Toshniwal et al., 2024a), and smol-magpie-ultra (Allal et al., 2024).\nTo ensure data quality, we applied filtering based on complexity and readability scores. Prompts\nfrom OpenMath-2 and smol-magpie-ultra falling below a score of 4 on either dimension were re-\nmoved, while low-scoring prompts from Magpie, Aya, and lmsys-chat-1m were downsampled rather\nthan discarded entirely. We further filtered conversations using ArmoRM-v0.1 (Wang et al., 2024a),\nremoving responses with scores below 0.08—except in cases where low readability in the prompt\nskewed the score.4\nTo broaden language coverage and support less-represented languages, we generated synthetic\ndata. This involved prompting an LLM—either Llama 3 (AI@Meta, 2024) or an earlier EuroLLM\ncheckpoint—with a monolingual document, a target language, and a category, asking it to create\nan instruction relevant to the document (see prompt in Fig. 8). The same model then produced an\nanswer in a RAG-style setup, using both the document and the generated instruction (see prompt\nin Fig. 9). Additionally, we synthesized further supervised fine-tuning (SFT) data by translat-\ning prompt–answer pairs,5and incorporating high-quality examples from multilingual translation\nbenchmarks such as NTREX-128 (Federmann et al., 2022), F LORES -200- DEV (Team et al., 2022),\nWMT-21 (Farhad et al., 2021), and WMT-22 (Kocmi et al., 2022), leaving WMT-23 and later edi-\ntions for evaluation purposes.\nAltogether, we collected approximately 4.5 million instructions. After applying filtering and dedu-\nplication, the final EuroBlocks dataset contains 1.95 million high-quality examples. To support fur-\nther research and development of European-centric LLMs, we publicly release the synthetic portion\nof the dataset: utter-project/EuroBlocks-SFT-Synthetic-1124.\n2https://docs.axolotl.ai/\n3Magpie datasets are generated with several models with different licenses. We used only the data from\nQwen 2, Llama 3 and Phi 3 models which have commercially permissive licenses.\n4ArmoRM-v0.1 was found to be robust on multilingual data, with a Pearson correlation above 0.7 between\nEnglish and translated examples. While reward models yield uncalibrated scores, the 0.08 threshold provided\na good balance between quality and data retention.\n5Translations were produced using Tower v2 (Rei et al., 2024) or earlier EuroLLM-9B models.\n8\n--- Page 9 ---\n3.2 M ODELING\nWe fine-tune EuroLLM-9B on EuroBlocks to create an instruction-following conversational model,\nEuroLLM-9B-Instruct. We use the standard cross-entropy loss, enabling bfloat16 mixed preci-\nsion and packing. The loss is calculated only on target tokens, with the loss on prompt tokens being\nmasked. The model is trained for three epochs using a learning rate of 7×10−6.\n4 E VALUATION\nWe evaluate EuroLLM-9B’s performance by comparing it against publicly available models (listed\nin §4.2) and analyzing its progression throughout pre-training (§5.1). Our evaluation encompasses\naverage performance across EU official languages (§4.3), per-language results for EU languages\n(§A.1), and results for additional supported languages by EuroLLM-9B (§A.2).\n4.1 E VALUATION SETTINGS\nOur assessment framework encompasses two main categories: general benchmarks to evaluate world\nknowledge acquisition, and machine translation to assess multilingual understanding and generation.\nRegarding English general benchmarks, we evaluate the pre-trained and post-trained versions of\neach LLM (when available) on:\n• Arc-Easy and Arc-Challenge (Clark et al., 2018): multiple-choice question-answering\ndataset, containing questions from science exams from grade 3 to grade 9. The Challenge\npartition contains the most difficult questions.\n• Hellaswag (Zellers et al., 2019): multiple-choice commonsense inference test which re-\nquires the understanding not just of the words in the sentence, but also of the underlying\nmeaning and context.\n• MMLU (Hendrycks et al., 2021a): multiple-choice questions from various branches of\nknowledge: humanities, social sciences, hard sciences, and other areas\n• MMLU-PRO (Wang et al., 2024c): refined and more challenging version of the MMLU\ndataset.\n• MUSR (Sprague et al.): multiple-choice complex problems with around 1,000 words in\nlength generated algorithmically. These problems, which include murder mysteries, object\nplacement questions, and team allocation problems require models to reason with long-\nrange context.\n• TruthfulQA (Lin et al., 2022): multiple-choice questions designed to evaluate the model’s\nability to identify true statements. It contains 817 questions from 38 different categories,\nincluding health, law, finance and politics.\n• IFEval (Kovalevskyi, 2024): set of prompts that test a model’s ability to follow explicit\ninstructions, such as “include keyword x” or “use format y”.\nTo ensure comprehensive multilingual evaluation, in the final evaluations we employed translated\nversions of six benchmarks: Arc (both Easy and Challenge partitions), Hellaswag, MMLU, Truth-\nfulQA, MMLU-Pro, and MUSR. For European languages, we utilized the EU20-Benchmarks6\n(Thellmann et al., 2024), which contains translations of the first four benchmarks across all tar-\nget European languages except Irish, Maltese, and Croatian. For non-European languages, we used\nthe translated versions of Arc-challenge, Hellaswag, and MMLU from the Okapi benchmark col-\nlection (Lai et al., 2023). For MMLU-Pro and MUSR benchmarks, no existing translations were\navailable, so we created our own translations using Tower (Alves et al., 2024), covering a subset of\nthe languages supported in EuroLLM. And finally, for English we also used IF-Eval in addition to\nthe other test sets.7\nRegarding evaluation, we follow the Open LLM Leaderboard methodology for MMLU-PRO,\nMUSR, and IFEval, normalizing scores between random baseline and maximum possible score (see\nthe leaderboard blog post for details). For the remaining benchmarks, we adhere to the Open LLM\nLeaderboard v1 specifications.\n6https://huggingface.co/collections/openGPT-X/eu20-benchmarks-67093b13db8ff192dc39312d\n7For reproducibility, we release our evaluation code, detailled parameter settings and result files:\nhttps://github.com/utter-project/eurollm-evaluation.\n9\n--- Page 10 ---\nPre-trained Post-trained Technical Report European EU Lang. Supp.\nGemma-2-9B Gemma-2-9B-IT Gemma 2 Team et al. (2024) No —–\nLLaMa-3.1-8B LLaMa-3.1-8B-IT Llama Team et al. (2024) No —–\nGranite-3-8B Granite-3-8B-IT Granite Team (2024) No No\nQwen-2.5-7B Qwen-2.5-7B-IT Qwen-Team et al. (2025) No No\nOLMo-2-7B OLMo-2-7B-IT OLMo et al. (2024) No No\nAya-23-8B Aya-Expanse-8B Singh et al. (2024); Dang et al. (2024) No No\nMistral-7B Mistral-7B-IT Jiang et al. (2023) Yes No\nNot available Ministral-8B-IT —- Yes No\nOcciglot-7B-eu5 Occiglot-7B-eu5-IT —- Yes No\nSalamandra-7B Salamandra-7B-IT Gonzalez-Agirre et al. (2025) Yes Yes\nNot available Pharia-1-LLM-7B-C —- Yes No\nNot available Teuken-7B-IT-R-v0.4 Ali et al. (2024) Yes Yes\nNot available Teuken-7B-IT-C-v0.4 Ali et al. (2024) Yes No\nTable 3: List of pre-trained and post-trained LLMs which we compare with EuroLLM-9B.\nFor machine translation evaluation, we use the WMT24++ dataset (Deutsch et al., 2025) and report\nresults using the COMET-22 metric (Rei et al., 2022a) in both directions: from and into English.\nWMT24++ extends the official WMT24 dataset by providing new post-edited references for 8 out of\n9 original language pairs, as well as new human-written references and post-edits for 46 additional\nlanguages and dialects, covering a total of 55 languages.\nWe also provide the Borda count (Irurozki et al., 2022), which corresponds to the average ranking\nof the models.\n4.2 B ASELINES\nOur evaluation includes a comprehensive comparison with publicly available LLMs of the same\nsize range, categorized into European-made and non-European-made models, considering both pre-\ntrained and post-trained versions. The full list of models can be found in Table 3.\n4.3 R ESULTS\nThe analyses of pre-trained (Table 4) and post-trained models (Table 5) across EU languages\ndemonstrate EuroLLM-9B’s strong performance. Both the base model and its post-trained variant\n(EuroLLM-9B-IT) emerge as the top performers among European-made models, achieving supe-\nrior results across most benchmarks as reflected in their lowest Borda count scores. Furthermore,\nEuroLLM-9B shows performance comparable to Gemma-2-9B while outperforming the remaining\nnon-European-made LLMs on the majority of the evaluated tasks. Comparing model performance\non the machine translation task reveals that EuroLLM-9B-IT achieves the best results among all Eu-\nropean and non-European models, in both translation directions (from and into English), with more\nthan three points of difference with the second best model Gemma-2-9B-IT, which is a very strong\nresult.\nThe results of the pre-trained and post-trained models for each language are presented in Appendix\nA. As we can see in Tables 6–47, in 13 languages, our pre-trained model outperforms the other\nEuropean open-weight models in all the tasks. In the case of post-trained models, in 14 languages,\nEuroLLM-9B-IT outperforms all the other open-weight models in all the tasks, except TruthfulQA.\nFor this specific task, it ranks fourth on average. Our initial investigation suggests that it is mainly\ndue to the challenging characteristics of this benchmark. It contains questions that some humans\nwould answer falsely due to a false belief or misconception (Lin et al., 2022). Finally, in the machine\ntranslation task, EuroLLM-9B-IT outperforms all other European models across all language pairs\nand translation directions, with the sole exception of Greek →English.\n10\n--- Page 11 ---\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 67.89 67.73 66.19 50.63 29.75 9.70 1.3\nLLaMa-3.1-8B 55.46 58.86 55.54 49.49 19.94 5.44 3.0\nGranite-3-8B 47.42 51.73 47.10 49.34 20.38 7.07 4.0\nQwen-2.5-7B 50.68 52.17 62.44 54.06 31.63 8.04 2.2\nOLMo-2-7B 38.25 42.23 41.32 45.24 13.91 4.53 5.8\nAya-23-8B 47.53 53.48 45.44 47.64 14.04 3.64 4.7\nEuropean\nMistral-7B 51.31 53.38 50.09 47.15 17.36 8.69 2.3\nOcciglot-7B-eu5 46.90 51.12 42.52 44.81 11.98 3.83 3.7\nSalamandra-7B 61.15 64.73 42.75 46.06 5.25 2.63 3.0\nEuroLLM-9B 66.48 67.00 55.68 51.84 17.60 10.97 1.0\nTable 4: Comparison of the pre-trained versions of open-weight LLMs on multilingual benchmarks,\naveraged across EU official languages. For Arc, Hellaswag, MMLU, and TruthfulQA we use EU20\nbenchmark (Thellmann et al., 2024). For MMLU-Pro and MUSR we translate the English version\nwith Tower (Alves et al., 2024) to 7 EU languages (German, French, Spanish, Portuguese, Italian,\nDutch, and Czech). Scores of MMLU-PRO, and MUSR are normalized between random baseline\nand maximum possible score, following the methodology used in Open LLM Leaderboard .\nPost-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 64.60 64.28 65.13 60.65 27.42 8.38 80.47 80.39 1.1\nLLaMa-3.1-8B-IT 51.39 56.92 57.07 55.05 24.22 4.01 77.43 77.70 3.0\nGranite-3-8B-IT 46.98 52.86 49.36 56.04 20.10 7.90 66.59 67.24 4.1\nQwen-2.5-7B-IT 47.91 51.61 62.27 57.88 29.68 7.62 69.32 69.54 3.0\nOLMo-2-7B-IT 39.00 43.30 41.86 48.57 12.38 4.02 62.43 63.20 5.9\nAya-Expanse-8B 47.78 54.94 51.33 53.57 19.77 5.52 72.02 73.90 3.9\nEuropean\nMistral-7B-IT 52.63 53.40 48.29 58.01 18.19 6.94 70.08 70.01 4.0\nMinistral-8B-IT 51.71 55.36 51.22 52.53 17.41 6.17 73.52 73.77 4.1\nOcciglot-7B-eu5-IT 42.39 49.52 39.75 48.10 11.77 4.17 61.14 59.59 6.4\nSalamandra-7B-IT 55.16 63.46 47.30 51.15 7.01 7.17 81.38 78.21 3.8\nPharia-1-LLM-7B-C 38.58 43.13 34.68 45.80 10.10 9.83 51.71 49.44 6.8\nTeuken-7B-IT-R-v0.4 55.42 60.96 37.65 54.75 9.29 2.25 75.19 70.50 4.8\nTeuken-7B-IT-C-v0.4 53.77 60.25 40.05 52.68 9.79 2.94 76.44 71.65 4.6\nEuroLLM-9B-IT 60.67 64.94 55.37 53.99 17.04 9.02 84.19 83.94 1.6\nTable 5: Comparison of the post-trained versions of open-weight LLMs on multilingual benchmarks,\naveraged across EU official languages. For WMT24++ we average the Comet-22 scores on all 21\nlanguage pairs which include English as source (for en →xx) or as the target language (for xx →en).\nScores of MMLU-PRO, and MUSR are normalized between random baseline and maximum possi-\nble score, following the methodology used in Open LLM Leaderboard .\n11\n--- Page 12 ---\n5 A NALYSIS AND DISCUSSION\n5.1 P RE-TRAINING ANALYSIS\nIn this section, we analyse the pre-training process averaging scores across 11 EU languages from\nthe Okapi benchmark: English, German, Spanish, French, Italian, Portuguese, Dutch, Swedish,\nHungarian, Romanian, and Danish.\n5.1.1 S ECOND PHASE DATA MIXTURES\nTo determine the optimal data mixture for the second training phase, we perform several experiments\nusing EuroLLM-1.7B (Martins et al., 2024) with reduced datasets of 80 billion tokens instead of the\nfull 400 billion. We evaluate three distinct data mixtures, each maintaining an increased percentage\nof higher-quality data at 34% while varying other components:\n1. English content reduced to 48% with code/math data increased to 7%.\n2. English content reduced to 40% with code/math data increased to 15%.\n3. English content reduced to 32.5% with code/math data at 7%, redistributing the remaining\npercentage across the other languages.\nFigure 6: Results on Arc Challenge (left), Hellaswag (middle), and MMLU (right) with different\n2ndphase data mixes, averaged across 11 EU languages.\nThe experimental results, shown in Figure 6, demonstrate that the third data mixture, while showing\nslightly lower performance on Arc-Challenge, achieved superior results on Hellaswag and more\npredominantly on MMLU. Based on this performance profile, we select this data mixture for the\nsecond training phase.\n5.1.2 T HIRD PHASE DATA MIXTURES\nThen, to decide what data mixture to use in the third pre-training phase, we experiment performing\nit on EuroLLM-9B with three data mixtures:\n1. Code/math data increased to 9.5% with English content reduced to 30%.\n2. Code/math data increased to 23% with proportional reductions across non-English lan-\nguages.\n3. Code/math data increased to 23%, reduced proportions for non-English languages, and\nparallel data decreased to 2% with corresponding increases in other data sources.\nThe experimental results, presented in Figure 7, demonstrate that the third configuration achieved\nequal or slightly superior performance on the three benchmarks. Based on these results, we select\nthis mixture for the 3rdpre-training phase.\n6 C ONCLUSIONS\nWe have presented EuroLLM-9B, detailing the comprehensive development process from data col-\nlection and filtering to the creation of our multilingual tokenizer and the pre-training and post-\ntraining processes. The release of both EuroLLM-9B and its instruction-tuned variant, EuroLLM-\n9B-Instruct, is accompanied by extensive performance evaluations on multilingual general bench-\n12\n--- Page 13 ---\nFigure 7: Results on Arc Challenge (left), Hellaswag (middle), and MMLU (right) with different 3rd\nphase data mixes, averaged across 11 EU languages.\nmarks and machine translation tasks. In addition to the models, we also release EuroFilter, our mul-\ntilingual data filtering model, and EuroBlocks-Synthetic, a synthetic instruction dataset designed to\nimprove post-training coverage across European languages.\nLooking ahead, we will continue to focus on developing larger-scale multilingual language models\nspecifically designed for European languages and use cases.\nACKNOWLEDGMENTS\nPart of this work was supported by the EU’s Horizon Europe Research and Innovation Actions\n(UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), and by\nthe Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for\nResponsible AI). We thank EuroHPC for the HPC resources used to support this work through grant\nEHPC-EXT-2023E01-042 and grant EHPC-AI-2024A01-085.\n13\n--- Page 14 ---\nREFERENCES\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/\nllama3/blob/main/MODEL_CARD.md .\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit\nSanghai. Gqa: Training generalized multi-query transformer models from multi-head check-\npoints. In Conference on Empirical Methods in Natural Language Processing , 2023.\nMehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rut-\nmann, Charvi Jain, Max Lübbering, Daniel Steinigen, Johannes Leveling, et al. Teuken-7b-base\n& teuken-7b-instruct: Towards european llms. arXiv preprint arXiv:2410.03730 , 2024.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín\nPiqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with\ngreat data, comes great performance, 2024.\nDuarte M Alves, José Pombal, Nuno M Guerreiro, Pedro H Martins, João Alves, Amin Farajian,\nBen Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. Tower: An open multilingual\nlarge language model for translation-related tasks. COLM , 2024.\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Feder-\nmann, Dmitriy Genzel, Franscisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie\nLazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and\nSylwia Tur. TICO-19: the translation initiative for COvid-19. In Proceedings of the 1st Workshop\non NLP for COVID-19 (Part 2) at EMNLP 2020 , Online, December 2020. Association for Com-\nputational Linguistics. URL https://aclanthology.org/2020.nlpcovid19-2.5 .\nAnthropic. The claude 3 model family: Opus, sonnet, haiku, 2023. URL https://www-cdn.\nanthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_\nCard_Claude_3.pdf .\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Al-\nbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model\nfor mathematics, 2023.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nLoubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von\nWerra. Smollm-corpus, 2024. URL https://huggingface.co/datasets/\nHuggingFaceTB/smollm-corpus .\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\nof large language models via positional interpolation. arXiv preprint arXiv:2306.15595 , 2023.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning chal-\nlenge. arXiv:1803.05457v1 , 2018.\nColin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe, and Alexander A. Alemi. On the use of\nArXiv as a dataset, 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\nTogether Computer. RedPajama: an open dataset for training large language models, 2023. URL\nhttps://github.com/togethercomputer/RedPajama-Data .\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Made-\nline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, et al. Aya\nexpanse: Combining research breakthroughs for a new multilingual frontier. arXiv preprint\narXiv:2412.04261 , 2024.\n14\n--- Page 15 ---\nOna de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji,\nJaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-Sánchez, Andrey Kutuzov, Sampo\nPyysalo, Stephan Oepen, and Jörg Tiedemann. A new massive multilingual dataset for high-\nperformance language technologies, 2024. URL https://arxiv.org/abs/2403.14009 .\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-\ngang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting\nChen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui\nDing, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi\nNi, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li,\nJunxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang,\nLecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun\nWang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan\nHuang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J.\nChen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang,\nRuyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng\nYe, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shut-\ning Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao,\nWei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue\nJin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xi-\naokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin\nLiu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang,\nXinyuan Li, Xuecheng Su, Xuheng Lin, Y . K. Li, Y . Q. Wang, Y . X. Wei, Y . X. Zhu, Yang\nZhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui\nLi, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying\nTang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu,\nYuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan\nXiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F.\nWu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda\nXie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao,\nZhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL\nhttps://arxiv.org/abs/2412.19437 .\nDaniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska,\nGeza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth\nSalesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. WMT24++: Ex-\npanding the Language Coverage of WMT24 to 55 Languages & Dialects, 2025. URL https:\n//arxiv.org/abs/2502.12404 .\nAndreas Eisele and Yu Chen. MultiUN: A multilingual corpus from united nation documents.\nInProceedings of the Seventh International Conference on Language Resources and Eval-\nuation (LREC’10) , Valletta, Malta, May 2010. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_\nPaper.pdf .\nAhmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. CCAligned: A\nmassive collection of cross-lingual web-document pairs. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP) , Online, November 2020.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2020.\nemnlp-main.480 .\nMiquel Esplà, Mikel Forcada, Gema Ramírez-Sánchez, and Hieu Hoang. ParaCrawl: Web-scale\nparallel corpora for the languages of the EU. In Proceedings of Machine Translation Summit\nXVII: Translator, Project and User Tracks , Dublin, Ireland, August 2019. European Association\nfor Machine Translation. URL https://aclanthology.org/W19-6721 .\nEuropat. Europat. europat.net/ .\nAkhbardeh Farhad, Arkhangorodsky Arkady, Biesialska Magdalena, Bojar Ond ˇrej, Chatterjee Ra-\njen, Chaudhary Vishrav, Marta R Costa-jussa, España-Bonet Cristina, Fan Angela, Federmann\nChristian, et al. Findings of the 2021 conference on machine translation (WMT21). In Proceed-\nings of the Sixth Conference on Machine Translation , 2021.\n15\n--- Page 16 ---\nChristian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 – news test references for MT\nevaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilin-\ngual Evaluation , pp. 21–24, Online, nov 2022. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2022.sumeval-1.4 .\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org .\nGoogle Gemma 2 Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 ,\n2024.\nGoogle Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma:\nOpen models based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\nAitor Gonzalez-Agirre, Marc Pàmies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo,\nJosé Javier Saiz, Ferran Espuña, Jaume Prats, Javier Aula-Blasco, Mario Mina, Adrián Rubio,\nAlexander Shvets, Anna Sallés, Iñaki Lacunza, Iñigo Pikabea, Jorge Palomar, Júlia Falcão, Lucía\nTormo, Luis Vasquez-Reina, Montserrat Marimon, Valle Ruíz-Fernández, and Marta Villegas.\nSalamandra technical report, 2025. URL https://arxiv.org/abs/2502.08489 .\nGoogle, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioan-\nnis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap,\nAngeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Henni-\ngan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins,\nClemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk,\nCosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal\nFaruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis\nMahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah\nLiu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapa-\nthy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal,\nJarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Mar-\ntin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker,\nEnrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs,\nAnaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lu-\ncas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura\nCulp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban\nRrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi\nHoward, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober,\nDan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William\nWong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan\nSenter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li,\nSarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hart-\nman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego\nde Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Re-\nitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane\nLabanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi\nAddanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Bala-\nguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Gana-\npathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting\nSun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy\nWang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault\nSellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli,\nSébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin,\nRichard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan\nFerret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander\nNeitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipan-\njan Das, Dominika Rogozi ´nska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka,\nFlavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei\nJia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim,\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan\n16\n--- Page 17 ---\nOzturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo,\nCraig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Lan-\ndon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai\nGiménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal,\nRachel Saputro, Kiran V odrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fer-\nnando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex\nCastro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek,\nRoss McIlroy, Mario Lu ˇci´c, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul\nMichel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin\nChung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc,\nTimothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua\nMaynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash\nKatariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose\nSlone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth,\nLisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay\nGhemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina\nZablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si,\nJeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexi-\nang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Toma-\nsev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada\nMa, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Chang-\npinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan,\nKrishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu\nWang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe\nSjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan,\nVittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate\nBaumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio\nPardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kass-\nner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El\nBadawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao\nGong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec,\nCe Zheng, Phoebe Thacker, Ça ˘glar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson,\nMax Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco\nSelvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan\nDafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pel-\nlat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi,\nRichard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang,\nThibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Has-\nsan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal,\nMatthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki ´ce-\nvi´c, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine\nHuot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin\nBrooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus\nWang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian\nAlbert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz\nK˛ epa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang,\nSanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker,\nNorbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xi-\nang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragag-\nnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin\nLing, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mer-\ncado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido,\nClemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak,\nYadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg,\nYifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu,\nAlex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Ka-\nreem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upad-\nhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch,\nPavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Hi-\nmanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Aker-\nlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze,\n17\n--- Page 18 ---\nFrancesco Bertolini, Liana-Eleonora Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta,\nTejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen\nThiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin,\nJasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin,\nJon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene,\nDaniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick\nSiegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Marco Selvatici,\nPedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessan-\ndro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso,\nAdam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane\nJang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan\nPham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja,\nPranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O’Neill, Anand Gokulchandran,\nRyan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence\nSafranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gor-\ngolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi\nLahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar,\nPetru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Nic-\ncolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV ,\nSarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas,\nMinjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo\nKwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho\nPark, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles\nSutton, Wojciech Rzadkowski, Fiona Macintosh, Roopali Vij, Konstantin Shagin, Paul Medina,\nChen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine\nLehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai\nYang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández\nAlcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kär-\nrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica\nMallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal\nVerma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian\nTenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu,\nNan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan,\nXuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal\nBen-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang,\nPiotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson,\nAdam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad,\nJin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon,\nAbhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William\nIsaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran,\nIvan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski,\nYu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer,\nPaul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybi ´nski, Ash-\nwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen\nJafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana\nKulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez,\nAndrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek\nChakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kush-\nman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline\nKaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pa-\nsumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cos-\nmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen,\nPaula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao\nZhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Or-\ngad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins,\nRobert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung,\nHarry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti,\nChu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark\nOmernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Pic-\ncinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna,\nSasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom\n18\n--- Page 19 ---\nNatan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong,\nJames Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek\nSharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie,\nEmily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic,\nWeize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason\nSanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Lint-\ning Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp,\nSushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie,\nCosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John\nWieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu,\nShane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez,\nSonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin\nLi, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy\nKoh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba\nSeyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe\nStanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah,\nPrakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume\nDesjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy,\nFedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clé-\nment Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srini-\nvasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna\nKlimczak-Pluci ´nska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio,\nLexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia\nLoher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig,\nAntonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong\nLi, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal,\nAnuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang\nWei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak,\nDoug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Ev-\ngenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong,\nKai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver\nWang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang,\nRiham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen,\nXiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha,\nWeiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Guru-\nmurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jen-\nnimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe\nRahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee,\nDenny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran\nMilan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahen-\ndru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh\nGhiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve\nLi, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun\nChen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya,\nSarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar\nDrath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng\nFan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-\nSung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de\nKerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly\nGatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca\nStefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar,\nDivya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter\nDanenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava\nUrala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak,\nIanna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael\nKucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh\nKumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben\nBariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar\nSubramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le,\nTrevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of\n19\n--- Page 20 ---\nhighly capable multimodal models, 2025. URL https://arxiv.org/abs/2312.11805 .\nIBM Granite Team. Granite 3.0 language models, 2024.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. DeBERTav3: Improving deBERTa using\nELECTRA-style pre-training with gradient-disentangled embedding sharing. In The Eleventh\nInternational Conference on Learning Representations , 2023. URL https://openreview.\nnet/forum?id=sE7-XhLxHA .\nKenneth Heafield. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation , Edinburgh, Scotland, July 2011. Association for\nComputational Linguistics. URL https://www.aclweb.org/anthology/W11-2123 .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR) , 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXiv\npreprint arXiv:2103.03874 , 2021b.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models\nwith scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024.\nEkhine Irurozki, Pierre Colombo, Nathan Noiry, and Stéphan Clémençon. What are the best sys-\ntems? new perspectives on nlp benchmarking. In Conference on Neural Information Processing\nSystems , 2022.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code. Preprint ,\n2022.\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, et al. Findings of the\n2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on\nMachine Translation (WMT) , 2022.\nPhilipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceed-\nings of Machine Translation Summit X: Papers , Phuket, Thailand, 2005. URL https://\naclanthology.org/2005.mtsummit-papers.11 .\nBohdan Kovalevskyi. Ifeval-extended: Enhancing instruction-following evaluation in large language\nmodels through dynamic prompt generation. Journal of Artificial Intelligence General science\n(JAIGS) ISSN: 3006-4023 , 5(1):513–524, 2024.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Processing. EMNLP 2018 , pp. 66, 2018.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo,\nKatherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat.\nMADLAD-400: A multilingual and document-level large audited dataset, 2023. URL https:\n//arxiv.org/abs/2309.04662 .\nViet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien\nNguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforce-\nment learning from human feedback. In Proceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing: System Demonstrations , pp. 318–327, 2023.\n20\n--- Page 21 ---\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman, Hady\nElsahar, Niklas Muennighoff, Jason Phang, Ofir Press, et al. What language model to train if you\nhave one million GPU hours? In Findings of the Association for Computational Linguistics:\nEMNLP 2022 , pp. 765–782, 2022.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.\nacl-long.229/ .\nMeta Llama Team, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama\n3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\nAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu, 2024.\nURL https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu .\nPedro Henrique Martins, Patrick Fernandes, João Alves, Nuno M Guerreiro, Ricardo Rei, Duarte M\nAlves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, et al. Eurollm:\nMultilingual language models for europe. arXiv preprint arXiv:2409.16235 , 2024.\nThomas Mayer and Michael Cysouw. Creating a massively parallel Bible corpus. In Proceed-\nings of the Ninth International Conference on Language Resources and Evaluation (LREC’14) ,\nReykjavik, Iceland, 2014. European Language Resources Association (ELRA). URL http:\n//www.lrec-conf.org/proceedings/lrec2014/pdf/220_Paper.pdf .\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt,\nRyan A. Rossi, and Thien Huu Nguyen. CulturaX: A cleaned, enormous, and multilingual dataset\nfor large language models in 167 languages, 2023. URL https://arxiv.org/abs/2309.\n09400 .\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal dependencies v2: An\nevergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources\nand Evaluation Conference , pp. 4034–4043, 2020.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita\nBhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint\narXiv:2501.00656 , 2024.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann,\nBrittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis,\nDerek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey\nChu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,\nThomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila\nDunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gib-\nson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hal-\nlacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan\nHickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu,\nJoost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel\nKokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen\nKrueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel\n21\n--- Page 22 ---\nLevy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,\nRyan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,\nChristine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick,\nLuke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Ra-\njeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe,\nJakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel\nParish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe\nde Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,\nMichelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra\nRimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,\nShibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Sel-\nsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor,\nEric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,\nNikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vi-\njayvergiya, Chelsea V oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan\nWard, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,\nMatt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Work-\nman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming\nYuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL\nhttps://arxiv.org/abs/2303.08774 .\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An open\ndataset of high-quality mathematical web text, 2023.\nQwen-Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei\nZhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao,\nKexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,\nTianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.\nURL https://arxiv.org/abs/2412.15115 .\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\npher, 2022. URL https://arxiv.org/abs/2112.11446 .\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer, 2023. URL https://arxiv.org/abs/1910.10683 .\nGema Ramírez-Sánchez, Jaume Zaragoza-Bernabeu, Marta Bañón, and Sergio Ortiz-Rojas. Bifixer\nand Bicleaner: two open-source tools to clean your parallel data. In Proceedings of the 22nd\nAnnual Conference of the European Association for Machine Translation , pp. 291–298, Lisboa,\nPortugal, November 2020. European Association for Machine Translation. ISBN 978-989-33-\n0589-8.\n22\n--- Page 23 ---\nRicardo Rei, José GC De Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André FT Martins. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Seventh Conference on Machine Translation , 2022a.\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine\nMaroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie,\nand André F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality estima-\ntion shared task. In Philipp Koehn, Loïc Barrault, Ond ˇrej Bojar, Fethi Bougares, Rajen Chatter-\njee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag,\nYvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio\nJimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata,\nToshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Marco Turchi,\nand Marcos Zampieri (eds.), Proceedings of the Seventh Conference on Machine Translation\n(WMT) , pp. 634–645, Abu Dhabi, United Arab Emirates (Hybrid), December 2022b. Association\nfor Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.60 .\nRicardo Rei, Jose Pombal, Nuno M. Guerreiro, João Alves, Pedro Henrique Martins, Patrick Fer-\nnandes, Helena Wu, Tania Vaz, Duarte Alves, Amin Farajian, Sweta Agrawal, Antonio Farinhas,\nJosé G. C. De Souza, and André Martins. Tower v2: Unbabel-IST 2024 submission for the general\nMT shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), Pro-\nceedings of the Ninth Conference on Machine Translation , pp. 185–204, Miami, Florida, USA,\nNovember 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wmt-1.12.\nURL https://aclanthology.org/2024.wmt-1.12/ .\nRoberts Rozis and Raivis Skadin ,š. Tilde MODEL - multilingual open data for EU languages. In\nProceedings of the 21st Nordic Conference on Computational Linguistics , Gothenburg, Sweden,\n2017. Association for Computational Linguistics. URL https://aclanthology.org/\nW17-0235 .\nVíctor M. Sánchez-Cartagena, Marta Bañón, Sergio Ortiz-Rojas, and Gema Ramírez-Sánchez.\nPrompsit’s submission to WMT 2018 parallel corpus filtering shared task. In Proceedings of\nthe Third Conference on Machine Translation , 2018.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. Wiki-\nmatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. arXiv preprint\narXiv:1907.05791 , 2019. URL https://arxiv.org/abs/1907.05791 .\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin.\nCcmatrix: Mining billions of high-quality parallel sentences on the web. arXiv preprint\narXiv:1911.04944 , 2020. URL https://arxiv.org/abs/1911.04944 .\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1715–1725, Berlin,\nGermany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162.\nURL https://aclanthology.org/P16-1162/ .\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism, 2020. URL https://arxiv.org/abs/1909.08053 .\nShivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin\nKo, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith\nHettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemi ´nski,\nHakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai,\nVu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann,\nNiklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara\nHooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024.\nFelipe Soares, Viviane Moreira, and Karin Becker. A large parallel corpus of full-text scientific\narticles. In Proceedings of the Eleventh International Conference on Language Resources and\nEvaluation (LREC 2018) , Miyazaki, Japan, 2018. European Language Resources Association\n(ELRA). URL https://aclanthology.org/L18-1546 .\n23\n--- Page 24 ---\nZayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the\nlimits of chain-of-thought with multistep soft reasoning. In The Twelfth International Conference\non Learning Representations .\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-\nhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-\nnon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela\nFan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind:\nScaling human-centered machine translation. 2022.\nKlaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude,\nFabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, and\nMehdi Ali. Towards multilingual llm evaluation for european languages, 2024. URL\narXivpreprintarXiv:2410.08928 .\nJörg Tiedemann. Parallel data, tools and interfaces in opus. In Proceedings of the eighth inter-\nnational conference on language resources and evaluation (LREC’12) , Istanbul, Turkey, 2012.\nEuropean Language Resources Association (ELRA). URL http://www.lrec-conf.org/\nproceedings/lrec2012/pdf/463_Paper.pdf .\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction\ndata. arXiv preprint arXiv:2410.01560 , 2024a.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction\ndata, 2024b. URL https://arxiv.org/abs/2410.01560 .\nShubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.\nOpenmathinstruct-1: A 1.8 million math instruction tuning dataset, 2024c. URL https://\narxiv.org/abs/2402.10176 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems , 30, 2017.\nHaoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences\nvia multi-objective reward modeling and mixture-of-experts. In Yaser Al-Onaizan, Mohit Bansal,\nand Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP\n2024 , pp. 10582–10592, Miami, Florida, USA, November 2024a. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.findings-emnlp.620. URL https://aclanthology.\norg/2024.findings-emnlp.620/ .\nXidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao,\nXiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight Multilingual Medical LLMs\ntowards Democratizing Medical AI to 6B People, 2024b.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging\nmulti-task language understanding benchmark. arXiv preprint arXiv:2406.01574 , 2024c.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán,\nArmand Joulin, and Edouard Grave. CCNet: Extracting High-Quality Monolingual Datasets from\nWeb Crawl Data, 2019. URL https://arxiv.org/abs/1911.00359 .\n24\n--- Page 25 ---\nRachel Wicks, Matt Post, and Philipp Koehn. Recovering document annotations for sentence-level\nbitext, 2024. URL https://arxiv.org/abs/2406.03869 .\nPhilip Williams and Barry Haddow. The elitr eca corpus. arXiv preprint arXiv:2109.07351 , 2021.\nURL https://arxiv.org/abs/2109.07351 .\nKrzysztof Wołk and Krzysztof Marasek. Building subject-aligned comparable corpora and mining\nit for truly parallel sentence pairs. Procedia Technology , 2014. URL http://dx.doi.org/\n10.1016/j.protcy.2014.11.024 .\nChen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint\narXiv:1802.08770 , 2018.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\nInInternational Conference on Machine Learning , pp. 10524–10533. PMLR, 2020.\nZhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and\nBill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with\nnothing. ArXiv , abs/2406.08464, 2024. URL https://api.semanticscholar.org/\nCorpusID:270391432 .\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer,\n2021a. URL https://arxiv.org/abs/2010.11934 .\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer,\n2021b. URL https://arxiv.org/abs/2010.11934 .\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,\nJianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical ex-\npert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122 .\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhen-\nguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions\nfor large language models, 2024. URL https://arxiv.org/abs/2309.12284 .\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , 2019.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Infor-\nmation Processing Systems , 32, 2019.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual\nneural machine translation and zero-shot translation. arXiv preprint arXiv:2004.11867 , 2020.\nURL https://arxiv.org/abs/2004.11867 .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang.\nLmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023b. URL https://arxiv.org/\nabs/2306.05685 .\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In International Conference on Computer Vision , 2015.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations parallel cor-\npus v1.0. In Proceedings of the Tenth International Conference on Language Resources and\nEvaluation (LREC’16) , Portorož, Slovenia, 2016. European Language Resources Association\n(ELRA). URL https://aclanthology.org/L16-1561 .\n25\n--- Page 26 ---\nA A PPENDIX\nHere we present the results of the pre-trained and post-trained models for each language separately.\nA.1 E UROPEAN LANGUAGES\nA.1.1 B ULGARIAN (BG)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 67.87 68.32 64.34 54.37 1.2\nLLaMa-3.1-8B 54.12 55.71 51.46 52.01 2.8\nGranite-3-8B 42.17 44.64 33.08 52.45 4.0\nQwen-2.5-7B 47.95 48.95 61.22 55.84 2.2\nOLMo-2-7B 33.67 37.79 29.75 51.53 5.8\nAya-23-8B 39.29 42.06 40.51 47.76 5.0\nEuropean\nMistral-7B 57.05 56.89 48.82 53.08 2.5\nOcciglot-7B-eu5 45.93 47.26 34.61 51.09 3.8\nSalamandra-7B 61.30 64.90 36.72 48.13 2.8\nEuroLLM-9B 65.18 66.63 53.90 56.11 1.0\nTable 6: Comparison of the pre-trained versions of open-weight LLMs on Bulgarian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 64.51 63.42 64.96 61.71 80.74 82.99 1.0\nLLaMa-3.1-8B-IT 51.47 54.24 51.27 54.54 75.99 81.19 2.7\nGranite-3-8B-IT 43.11 46.51 42.11 57.79 61.20 79.87 4.0\nQwen-2.5-7B-IT 47.70 49.97 61.18 58.56 62.96 81.50 2.5\nOLMo-2-7B-IT 34.96 38.84 37.25 52.24 53.98 76.78 6.0\nAya-Expanse-8B 42.26 45.23 48.43 52.99 60.44 79.29 4.8\nEuropean\nMistral-7B-IT 58.78 57.14 48.23 61.68 74.39 80.76 3.2\nMinistral-8B-IT 52.44 53.35 48.78 54.17 69.91 79.96 4.7\nOcciglot-7B-eu5-IT 39.94 46.11 35.82 53.14 57.66 70.93 6.7\nSalamandra-7B-IT 55.12 63.56 46.23 53.35 83.19 83.56 3.0\nPharia-1-LLM-7B-C 25.72 31.02 24.76 50.80 34.45 57.66 8.0\nTeuken-7B-IT-R-v0.4 54.23 61.10 30.74 55.41 71.63 76.26 4.7\nTeuken-7B-IT-C-v0.4 51.46 59.68 36.80 53.03 74.41 81.27 4.7\nEuroLLM-9B-IT 59.92 63.69 53.92 57.75 85.65 84.06 1.2\nTable 7: Comparison of the post-trained versions of open-weight LLMs on Bulgarian benchmarks.\n26\n--- Page 27 ---\nA.1.2 C ZECH (CS)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 67.56 66.34 66.14 50.73 1.5\nLLaMa-3.1-8B 56.81 58.31 55.51 50.70 3.5\nGranite-3-8B 56.94 60.95 52.02 52.40 2.8\nQwen-2.5-7B 51.95 51.92 63.58 54.81 3.2\nOLMo-2-7B 33.42 35.44 38.99 43.51 6.0\nAya-23-8B 56.58 61.10 49.93 46.70 4.0\nEuropean\nMistral-7B 55.93 53.88 51.86 48.71 2.5\nOcciglot-7B-eu5 46.34 47.64 43.31 44.90 4.0\nSalamandra-7B 61.41 63.83 43.59 47.39 2.5\nEuroLLM-9B 66.51 65.88 55.82 51.08 1.0\nTable 8: Comparison of the pre-trained versions of open-weight LLMs on Czech benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 63.98 62.87 65.76 61.10 81.27 82.80 1.7\nLLaMa-3.1-8B-IT 52.48 56.23 58.12 54.70 77.68 80.90 4.2\nGranite-3-8B-IT 56.67 62.46 54.76 58.76 79.33 82.84 2.8\nQwen-2.5-7B-IT 48.32 51.87 63.75 57.24 71.05 81.99 4.0\nOLMo-2-7B-IT 33.31 36.16 39.48 42.75 49.59 75.48 6.0\nAya-Expanse-8B 55.63 62.89 55.39 54.94 84.23 83.43 2.3\nEuropean\nMistral-7B-IT 56.86 53.90 50.48 60.48 71.90 80.20 3.5\nMinistral-8B-IT 50.68 51.76 52.15 53.07 70.29 80.31 4.8\nOcciglot-7B-eu5-IT 40.43 46.31 40.59 47.55 59.05 71.08 6.7\nSalamandra-7B-IT 54.82 62.11 47.76 51.50 81.64 83.33 3.2\nPharia-1-LLM-7B-C 27.62 30.22 29.29 45.25 35.10 59.20 8.0\nTeuken-7B-IT-R-v0.4 54.56 60.57 36.89 54.23 77.79 76.02 4.2\nTeuken-7B-IT-C-v0.4 52.55 58.91 38.06 53.14 75.90 81.38 4.3\nEuroLLM-9B-IT 60.58 64.40 56.08 53.35 84.91 83.69 1.3\nTable 9: Comparison of the post-trained versions of open-weight LLMs on Czech benchmarks.\n27\n--- Page 28 ---\nA.1.3 D ANISH (DA)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 69.10 69.78 68.05 50.60 1.2\nLLaMa-3.1-8B 55.90 61.49 56.92 50.53 2.5\nGranite-3-8B 44.30 49.73 49.11 50.49 4.0\nQwen-2.5-7B 50.90 52.38 65.41 53.10 2.2\nOLMo-2-7B 35.93 42.31 44.92 42.78 5.8\nAya-23-8B 39.40 47.67 43.89 44.45 5.2\nEuropean\nMistral-7B 55.19 57.98 54.25 45.60 2.5\nOcciglot-7B-eu5 47.48 52.81 44.69 42.89 3.8\nSalamandra-7B 61.32 66.75 43.37 43.97 2.8\nEuroLLM-9B 66.17 68.69 56.74 50.89 1.0\nTable 10: Comparison of the pre-trained versions of open-weight LLMs on Danish benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 65.09 66.25 67.07 60.44 82.54 84.74 1.0\nLLaMa-3.1-8B-IT 50.65 59.55 58.95 56.01 79.93 82.80 2.5\nGranite-3-8B-IT 44.56 51.63 50.67 52.81 63.92 81.67 4.3\nQwen-2.5-7B-IT 48.06 52.17 64.92 57.16 68.41 83.24 2.5\nOLMo-2-7B-IT 36.41 43.28 44.32 45.28 59.15 80.12 6.0\nAya-Expanse-8B 40.58 50.59 50.55 49.61 66.16 82.37 4.7\nEuropean\nMistral-7B-IT 57.25 58.52 52.13 58.26 75.24 82.50 3.8\nMinistral-8B-IT 51.11 56.97 53.41 51.36 76.06 82.88 4.5\nOcciglot-7B-eu5-IT 43.18 50.36 41.01 47.25 61.03 74.35 6.8\nSalamandra-7B-IT 56.49 65.53 47.80 50.31 82.79 85.21 3.3\nPharia-1-LLM-7B-C 31.23 35.79 32.94 40.05 47.15 71.98 8.0\nTeuken-7B-IT-R-v0.4 57.28 63.62 40.49 52.24 76.83 79.73 4.2\nTeuken-7B-IT-C-v0.4 53.99 62.21 41.52 51.33 78.95 83.26 4.2\nEuroLLM-9B-IT 60.81 66.80 55.61 55.05 85.14 85.61 1.2\nTable 11: Comparison of the post-trained versions of open-weight LLMs on Danish benchmarks.\n28\n--- Page 29 ---\nA.1.4 D UTCH (NL)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 69.74 68.86 67.63 53.24 29.01 10.25 1.5\nLLaMa-3.1-8B 58.10 63.10 58.60 52.11 19.50 4.10 3.6\nGranite-3-8B 60.74 66.95 56.30 54.95 19.04 5.39 2.8\nQwen-2.5-7B 56.51 58.41 67.62 57.67 29.97 5.69 2.7\nOLMo-2-7B 38.39 43.31 45.41 45.35 11.97 3.67 6.0\nAya-23-8B 57.36 63.95 50.93 51.33 12.64 4.10 4.4\nEuropean\nMistral-7B 57.02 58.34 54.61 51.23 16.38 12.63 2.2\nOcciglot-7B-eu5 51.01 54.93 46.56 46.56 10.38 2.81 3.5\nSalamandra-7B 61.77 65.20 44.87 48.19 5.34 2.68 3.2\nEuroLLM-9B 67.19 67.69 56.57 56.37 17.18 9.63 1.2\nTable 12: Comparison of the pre-trained versions of open-weight LLMs on Dutch benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 65.65 66.92 67.52 60.47 27.68 9.89 81.79 84.17 1.9\nLLaMa-3.1-8B-IT 52.79 61.91 60.67 56.20 22.66 3.57 80.65 82.90 4.1\nGranite-3-8B-IT 59.18 68.89 58.31 62.89 18.61 9.26 81.21 84.18 2.4\nQwen-2.5-7B-IT 51.90 58.08 66.74 61.42 28.23 7.67 77.05 83.51 3.4\nOLMo-2-7B-IT 39.97 44.90 45.08 48.22 9.93 3.08 66.60 81.00 6.0\nAya-Expanse-8B 56.14 65.80 56.36 58.25 19.54 6.65 83.02 84.42 2.8\nEuropean\nMistral-7B-IT 58.68 59.07 52.77 61.56 17.51 6.58 76.26 81.98 4.1\nMinistral-8B-IT 52.85 56.50 53.77 49.54 16.34 4.23 76.80 82.66 4.9\nOcciglot-7B-eu5-IT 43.16 52.83 44.31 49.58 10.31 2.94 65.82 73.52 6.9\nSalamandra-7B-IT 55.26 64.52 48.44 53.42 5.76 7.27 81.31 84.27 3.9\nPharia-1-LLM-7B-C 55.02 64.04 44.33 39.35 10.06 9.09 77.99 82.40 4.6\nTeuken-7B-IT-R-v0.4 57.28 62.24 41.44 55.51 9.12 1.29 73.25 79.52 6.0\nTeuken-7B-IT-C-v0.4 55.65 62.19 42.38 56.45 9.22 3.44 79.57 82.67 4.6\nEuroLLM-9B-IT 61.17 67.05 56.17 57.10 15.48 8.90 84.13 84.86 1.5\nTable 13: Comparison of the post-trained versions of open-weight LLMs on Dutch benchmarks.\n29\n--- Page 30 ---\nA.1.5 E NGLISH (EN)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 78.66 82.39 70.30 45.44 34.87 14.48 1.8\nLLaMa-3.1-8B 71.12 82.08 65.10 45.21 25.17 9.13 4.3\nGranite-3-8B 75.45 83.51 63.40 51.31 25.82 9.36 2.8\nQwen-2.5-7B 75.47 80.01 74.26 56.31 37.33 13.06 2.0\nOLMo-2-7B 75.27 82.23 62.95 43.31 22.74 10.12 4.3\nAya-23-8B 66.84 77.83 54.89 45.35 16.68 5.85 5.7\nEuropean\nMistral-7B 72.76 83.03 62.33 42.59 21.78 8.50 1.5\nOcciglot-7B-eu5 67.38 78.96 52.81 40.32 13.87 2.68 3.2\nSalamandra-7B 70.92 77.85 46.48 41.49 5.52 2.58 3.7\nEuroLLM-9B 72.88 78.63 57.41 48.50 17.68 12.47 1.7\nTable 14: Comparison of the pre-trained versions of open-weight LLMs on English benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR IFEval Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 78.23 78.87 71.87 60.49 31.77 9.16 75.00 2.3\nLLaMa-3.1-8B-IT 66.32 77.39 68.25 52.99 30.52 7.21 77.83 3.6\nGranite-3-8B-IT 71.72 84.40 65.46 66.35 27.30 4.63 54.38 3.4\nQwen-2.5-7B-IT 67.96 73.46 73.41 62.45 36.91 9.00 76.16 2.6\nOLMo-2-7B-IT 70.13 80.32 61.00 55.58 20.17 6.08 76.07 4.0\nAya-Expanse-8B 66.11 78.37 62.01 52.28 22.27 4.86 63.95 4.6\nEuropean\nMistral-7B-IT 75.79 82.00 60.67 66.01 23.21 3.54 53.43 2.9\nMinistral-8B-IT 68.86 78.53 62.93 54.02 13.12 7.84 58.57 2.4\nOcciglot-7B-eu5-IT 62.07 76.58 50.65 47.95 13.89 7.21 40.09 5.0\nSalamandra-7B-IT 66.91 76.40 52.54 50.87 9.08 8.17 24.09 5.6\nPharia-1-LLM-7B-C 68.41 75.15 49.32 45.88 11.86 9.75 36.95 5.0\nTeuken-7B-IT-R-v0.4 67.47 73.32 45.53 51.36 10.82 4.07 35.26 6.0\nTeuken-7B-IT-C-v0.4 67.61 73.51 45.33 50.93 10.78 1.72 35.05 6.6\nEuroLLM-9B-IT 68.68 76.38 58.36 49.77 19.58 8.44 65.69 3.1\nTable 15: Comparison of the post-trained versions of open-weight LLMs on English benchmarks.\n30\n--- Page 31 ---\nA.1.6 E STONIAN (ET)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 59.88 59.22 62.50 49.50 1.0\nLLaMa-3.1-8B 44.35 46.33 48.58 46.38 2.5\nGranite-3-8B 28.91 32.24 35.24 44.11 4.2\nQwen-2.5-7B 34.29 37.08 51.68 48.89 2.5\nOLMo-2-7B 26.78 31.14 32.02 43.40 6.0\nAya-23-8B 28.07 32.76 33.38 43.43 4.8\nEuropean\nMistral-7B 29.64 34.15 38.85 43.15 3.2\nOcciglot-7B-eu5 29.07 33.37 32.97 44.19 3.8\nSalamandra-7B 56.08 58.92 41.66 45.42 2.0\nEuroLLM-9 B 62.37 62.28 54.59 47.15 1.0\nTable 16: Comparison of the pre-trained versions of open-weight LLMs on Estonian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 55.71 55.04 61.44 57.02 79.42 83.75 1.0\nLLaMa-3.1-8B-IT 41.21 45.35 51.10 50.13 72.95 79.33 2.2\nGranite-3-8B-IT 29.11 31.89 36.60 46.62 42.63 67.92 5.0\nQwen-2.5-7B-IT 33.05 37.40 51.75 49.33 53.59 76.27 3.0\nOLMo-2-7B-IT 27.84 30.87 32.61 41.92 43.09 64.51 5.8\nAya-Expanse-8B 28.04 32.04 38.83 49.94 45.59 70.87 4.0\nEuropean\nMistral-7B-IT 30.82 33.72 36.12 46.72 47.27 70.13 6.0\nMinistral-8B-IT 40.43 43.81 42.45 51.58 66.52 78.32 4.0\nOcciglot-7B-eu5-IT 28.18 32.51 28.44 46.53 44.04 63.49 7.0\nSalamandra-7B-IT 49.21 57.87 45.53 49.22 83.26 84.94 2.5\nPharia-1-LLM-7B-C 25.53 29.85 28.32 44.77 36.46 51.55 8.0\nTeuken-7B-IT-R-v0.4 47.15 53.39 37.94 54.29 77.81 71.30 3.3\nTeuken-7B-IT-C-v0.4 44.68 52.45 39.72 50.53 72.39 81.90 3.8\nEuroLLM-9B-IT 55.48 58.96 54.09 51.55 86.49 85.81 1.3\nTable 17: Comparison of the post-trained versions of open-weight LLMs on Estonian benchmarks.\n31\n--- Page 32 ---\nA.1.7 G ERMAN (DE)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 69.71 69.15 67.83 48.71 28.98 10.65 2.0\nLLaMa-3.1-8B 58.69 63.36 59.09 49.57 19.57 8.04 3.8\nGranite-3-8B 63.10 66.77 57.21 50.90 20.58 5.99 3.0\nQwen-2.5-7B 60.65 59.78 68.15 54.86 30.27 10.25 2.2\nOLMo-2-7B 47.16 48.13 47.81 42.56 12.76 7.21 5.7\nAya-23-8B 59.99 64.48 51.88 50.08 14.26 4.10 4.3\nEuropean\nMistral-7B 59.88 59.90 55.24 46.44 17.30 10.48 2.7\nOcciglot-7B-eu5 61.44 67.87 51.13 43.94 12.70 4.99 2.8\nSalamandra-7B 62.18 65.73 43.82 44.11 4.50 3.97 3.3\nEuroLLM-9B 66.74 67.28 56.53 51.80 17.62 12.17 1.2\nTable 18: Comparison of the pre-trained versions of open-weight LLMs on German benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 67.42 66.33 67.08 60.01 27.91 10.68 78.10 83.94 2.1\nLLaMa-3.1-8B-IT 53.38 60.99 61.18 56.70 23.14 4.57 78.38 82.63 4.4\nGranite-3-8B-IT 61.81 68.77 59.23 62.06 18.64 10.02 79.54 83.66 2.5\nQwen-2.5-7B-IT 57.52 58.83 67.27 60.76 29.17 10.32 76.27 83.52 2.9\nOLMo-2-7B-IT 47.31 51.31 47.37 50.43 11.36 5.09 73.66 82.74 5.8\nAya-Expanse-8B 57.47 66.28 56.93 57.60 19.03 8.47 81.61 84.14 2.9\nEuropean\nMistral-7B-IT 61.03 60.43 53.11 61.77 17.76 8.99 74.58 81.86 4.6\nMinistral-8B-IT 58.08 63.56 57.61 50.77 17.53 9.10 78.88 83.47 3.6\nOcciglot-7B-eu5-IT 54.96 65.85 49.09 47.64 12.38 3.94 70.59 74.87 5.9\nSalamandra-7B-IT 56.74 65.01 48.83 49.27 5.81 9.69 79.58 83.89 4.5\nPharia-1-LLM-7B-C 60.56 65.50 45.06 46.38 10.41 9.42 78.33 82.22 4.8\nTeuken-7B-IT-R-v0.4 61.51 63.43 41.00 53.31 9.04 2.61 77.31 79.21 5.5\nTeuken-7B-IT-C-v0.4 59.52 62.34 42.63 52.23 9.70 1.95 77.93 82.32 5.6\nEuroLLM-9B-IT 60.92 66.27 56.37 52.02 17.30 11.97 82.25 84.33 2.0\nTable 19: Comparison of the post-trained versions of open-weight LLMs on German benchmarks.\n32\n--- Page 33 ---\nA.1.8 G REEK (EL)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 64.67 65.87 63.71 53.06 1.0\nLLaMa-3.1-8B 50.45 55.16 51.29 50.40 3.0\nGranite-3-8B 32.50 39.72 34.39 45.96 5.2\nQwen-2.5-7B 39.17 41.54 51.21 50.50 3.5\nOLMo-2-7B 27.09 33.94 29.68 47.46 5.8\nAya-23-8B 55.94 62.21 47.91 50.63 2.5\nEuropean\nMistral-7B 31.69 38.32 36.20 43.94 3.2\nOcciglot-7B-eu5 28.83 35.47 31.42 44.01 3.8\nSalamandra-7B 58.80 64.82 39.57 48.54 2.0\nEuroLLM-9B 64.80 64.96 52.26 52.20 1.0\nTable 20: Comparison of the pre-trained versions of open-weight LLMs on Greek benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 60.48 62.60 62.24 61.46 80.95 82.35 1.5\nLLaMa-3.1-8B-IT 48.41 53.54 44.05 57.00 77.33 80.73 3.2\nGranite-3-8B-IT 33.35 40.22 35.51 50.69 57.75 78.16 5.0\nQwen-2.5-7B-IT 38.27 42.19 52.77 56.85 61.07 79.60 3.8\nOLMo-2-7B-IT 30.49 34.07 32.75 48.64 48.93 74.71 6.0\nAya-Expanse-8B 54.55 64.09 54.35 57.82 85.27 84.76 1.5\nEuropean\nMistral-7B-IT 31.76 37.76 37.03 49.80 54.12 74.68 5.5\nMinistral-8B-IT 46.89 49.89 40.14 51.24 71.82 80.53 4.3\nOcciglot-7B-eu5-IT 26.80 34.12 31.15 44.43 46.98 63.49 7.2\nSalamandra-7B-IT 52.92 63.27 45.07 52.14 83.12 84.52 2.3\nPharia-1-LLM-7B-C 25.76 29.09 24.15 47.17 37.38 59.40 7.8\nTeuken-7B-IT-R-v0.4 54.19 60.91 34.22 57.41 77.32 73.55 3.3\nTeuken-7B-IT-C-v0.4 52.05 59.15 32.81 52.93 74.49 79.71 4.2\nEuroLLM-9B-IT 58.23 63.34 51.67 54.64 85.88 84.20 1.3\nTable 21: Comparison of the post-trained versions of open-weight LLMs on Greek benchmarks.\n33\n--- Page 34 ---\nA.1.9 F INNISH (FI)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 63.16 64.08 63.79 47.78 1.2\nLLaMa-3.1-8B 48.55 52.29 51.32 45.33 2.5\nGranite-3-8B 29.59 35.85 39.46 40.96 5.2\nQwen-2.5-7B 38.79 40.86 54.51 49.69 2.2\nOLMo-2-7B 32.51 37.76 39.12 43.20 4.2\nAya-23-8B 29.92 34.95 34.96 42.86 5.5\nEuropean\nMistral-7B 35.25 40.56 42.81 42.37 3.0\nOcciglot-7B-eu5 31.37 36.98 35.37 43.08 3.8\nSalamandra-7B 56.10 61.39 41.72 43.41 2.2\nEuroLLM-9B 63.93 65.46 54.79 48.88 1.0\nTable 22: Comparison of the pre-trained versions of open-weight LLMs on Finnish benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 60.16 60.35 63.59 56.81 83.41 85.27 1.0\nLLaMa-3.1-8B-IT 45.43 51.26 53.14 49.99 79.86 82.69 2.3\nGranite-3-8B-IT 30.16 35.86 39.61 46.98 52.79 76.61 5.5\nQwen-2.5-7B-IT 37.22 40.98 54.82 51.10 63.73 80.76 2.8\nOLMo-2-7B-IT 33.74 38.41 39.15 47.76 69.62 79.86 4.2\nAya-Expanse-8B 31.84 35.09 41.22 41.70 54.27 77.54 5.2\nEuropean\nMistral-7B-IT 35.79 40.11 40.68 47.79 59.82 78.86 5.7\nMinistral-8B-IT 44.57 47.04 45.78 47.86 72.52 81.36 4.3\nOcciglot-7B-eu5-IT 29.31 35.26 31.31 43.20 53.00 69.61 7.0\nSalamandra-7B-IT 49.80 59.82 45.19 47.92 85.54 85.87 2.8\nPharia-1-LLM-7B-C 27.64 30.16 28.88 42.81 37.96 52.34 8.0\nTeuken-7B-IT-R-v0.4 51.14 57.60 38.66 50.80 78.41 78.34 3.7\nTeuken-7B-IT-C-v0.4 49.92 56.83 41.20 50.36 78.33 82.62 3.5\nEuroLLM-9B-IT 58.34 61.70 52.91 52.57 87.74 86.23 1.0\nTable 23: Comparison of the post-trained versions of open-weight LLMs on Finnish benchmarks.\n34\n--- Page 35 ---\nA.1.10 F RENCH (FR)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 72.64 73.08 68.93 42.72 29.41 10.15 2.2\nLLaMa-3.1-8B 62.20 67.45 59.67 47.79 19.23 2.94 4.0\nGranite-3-8B 65.71 70.90 57.30 45.49 19.59 6.94 2.8\nQwen-2.5-7B 65.01 67.46 69.34 55.97 32.44 10.45 1.8\nOLMo-2-7B 53.51 56.57 50.69 40.45 14.66 5.42 5.5\nAya-23-8B 61.40 68.78 51.97 44.15 13.82 3.47 4.7\nEuropean\nMistral-7B 63.00 65.34 56.30 45.18 17.32 7.97 2.7\nOcciglot-7B-eu5 64.67 71.39 50.36 39.35 12.70 5.19 2.7\nSalamandra-7B 64.63 68.96 44.29 40.33 5.38 3.27 3.5\nEuroLLM-9B 68.95 70.18 56.44 46.56 18.36 11.08 1.2\nTable 24: Comparison of the pre-trained versions of open-weight LLMs on French benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 68.71 69.91 67.81 60.81 28.76 9.66 77.99 83.21 2.4\nLLaMa-3.1-8B-IT 56.94 65.01 61.95 56.55 23.92 4.30 78.00 82.35 4.4\nGranite-3-8B-IT 63.64 72.01 60.26 63.95 20.03 11.28 78.88 83.21 2.4\nQwen-2.5-7B-IT 58.91 65.60 68.64 64.45 30.74 11.71 77.02 83.11 2.6\nOLMo-2-7B-IT 53.14 59.07 50.56 50.81 13.20 3.64 76.69 82.67 5.9\nAya-Expanse-8B 60.10 69.59 57.51 57.57 20.24 5.36 80.83 83.72 2.8\nEuropean\nMistral-7B-IT 63.96 65.90 54.35 62.00 18.32 9.16 75.62 81.62 3.9\nMinistral-8B-IT 59.92 67.75 57.08 55.17 18.82 8.01 78.53 82.83 3.2\nOcciglot-7B-eu5-IT 58.18 69.00 48.86 46.99 12.04 4.80 70.77 74.87 6.0\nSalamandra-7B-IT 58.64 67.98 48.69 47.92 7.07 5.92 79.14 83.57 4.9\nPharia-1-LLM-7B-C 61.65 68.36 44.99 47.13 10.41 11.38 77.04 81.82 4.2\nTeuken-7B-IT-R-v0.4 60.06 65.94 41.47 56.16 9.73 2.78 76.19 77.36 5.9\nTeuken-7B-IT-C-v0.4 59.42 65.72 43.57 53.63 10.93 1.46 76.97 80.51 6.1\nEuroLLM-9B-IT 62.47 69.31 57.03 50.90 18.41 8.04 81.18 84.20 2.2\nTable 25: Comparison of the post-trained versions of open-weight LLMs on French benchmarks.\n35\n--- Page 36 ---\nA.1.11 H UNGARIAN (HU)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 63.07 63.55 64.10 53.46 1.0\nLLaMa-3.1-8B 51.98 55.83 54.76 50.47 2.2\nGranite-3-8B 29.96 33.48 37.84 50.17 4.2\nQwen-2.5-7B 36.23 38.99 53.59 51.64 2.8\nOLMo-2-7B 28.44 31.78 33.81 47.96 6.0\nAya-23-8B 29.88 34.68 37.16 49.23 4.8\nEuropean\nMistral-7B 49.42 49.50 50.07 47.92 2.8\nOcciglot-7B-eu5 40.83 41.74 40.50 47.02 4.0\nSalamandra-7B 56.43 60.15 40.77 49.52 2.2\nEuroLLM-9B 61.86 62.44 54.30 55.69 1.0\nTable 26: Comparison of the pre-trained versions of open-weight LLMs on Hungarian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 60.16 59.86 57.56 63.86 80.19 82.74 1.0\nLLaMa-3.1-8B-IT 47.59 54.54 56.10 54.93 79.13 80.99 2.0\nGranite-3-8B-IT 30.22 33.73 39.45 50.70 46.45 72.23 4.8\nQwen-2.5-7B-IT 35.78 39.75 54.30 54.29 56.33 77.74 3.0\nOLMo-2-7B-IT 27.91 31.96 34.24 45.72 44.61 69.22 6.0\nAya-Expanse-8B 30.42 35.16 42.39 46.32 50.42 75.84 4.2\nEuropean\nMistral-7B-IT 51.52 50.35 47.71 59.74 71.31 79.08 3.5\nMinistral-8B-IT 45.24 47.53 46.63 51.42 68.08 79.13 5.2\nOcciglot-7B-eu5-IT 36.34 40.63 35.91 48.66 53.34 69.76 6.8\nSalamandra-7B-IT 50.73 58.58 45.29 52.33 80.88 83.11 3.0\nPharia-1-LLM-7B-C 26.41 29.31 28.47 50.66 34.04 51.72 7.8\nTeuken-7B-IT-R-v0.4 51.84 57.09 29.15 56.41 73.60 74.88 4.2\nTeuken-7B-IT-C-v0.4 50.62 55.79 34.09 52.21 73.78 79.73 4.3\nEuroLLM-9B-IT 55.42 58.78 54.07 57.82 83.75 83.51 1.2\nTable 27: Comparison of the post-trained versions of open-weight LLMs on Hungarian bench-\nmarks.\n36\n--- Page 37 ---\nA.1.12 I TALIAN (IT)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 71.42 70.69 68.87 48.96 29.17 9.39 1.8\nLLaMa-3.1-8B 62.63 65.53 59.40 48.39 18.57 6.09 4.2\nGranite-3-8B 64.63 68.65 57.31 51.15 19.76 10.02 2.3\nQwen-2.5-7B 62.52 62.57 69.24 56.60 31.39 8.07 2.5\nOLMo-2-7B 45.07 48.50 46.92 44.68 13.67 7.54 5.7\nAya-23-8B 59.20 66.31 51.41 48.70 13.90 6.68 4.5\nEuropean\nMistral-7B 59.72 61.94 55.75 47.02 16.40 9.23 2.7\nOcciglot-7B-eu5 63.12 70.18 51.65 45.11 12.68 6.25 2.8\nSalamandra-7B 63.95 66.95 43.40 45.91 4.99 4.36 3.3\nEuroLLM-9B 69.25 68.79 56.98 50.92 17.64 14.05 1.2\nTable 28: Comparison of the pre-trained versions of open-weight LLMs on Italian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 68.72 68.23 68.25 59.48 28.44 9.82 81.18 84.37 2.2\nLLaMa-3.1-8B-IT 58.37 63.26 62.54 58.57 23.58 4.89 81.56 83.04 3.8\nGranite-3-8B-IT 62.91 70.70 60.16 62.50 18.86 10.52 81.02 84.15 2.5\nQwen-2.5-7B-IT 57.52 61.37 68.97 61.72 29.63 6.81 79.31 83.96 3.4\nOLMo-2-7B-IT 47.33 52.13 47.12 53.26 11.36 4.43 75.29 82.46 6.0\nAya-Expanse-8B 59.66 68.28 57.59 57.38 19.26 8.99 83.46 84.58 2.6\nEuropean\nMistral-7B-IT 61.83 62.13 53.99 62.38 17.32 7.01 78.26 82.31 4.5\nMinistral-8B-IT 59.71 65.99 58.44 54.47 18.10 6.15 81.36 83.64 3.1\nOcciglot-7B-eu5-IT 56.38 67.71 48.63 49.07 12.98 6.51 72.09 75.04 5.6\nSalamandra-7B-IT 59.12 65.78 48.10 49.11 7.36 7.97 81.94 84.49 4.5\nPharia-1-LLM-7B-C 59.67 67.03 44.97 47.40 11.12 17.53 79.39 82.37 4.8\nTeuken-7B-IT-R-v0.4 57.98 63.18 37.06 57.11 9.17 2.71 79.89 79.19 6.1\nTeuken-7B-IT-C-v0.4 57.92 63.32 42.65 52.54 10.03 5.39 79.70 82.55 5.9\nEuroLLM-9B-IT 63.72 67.45 57.35 54.22 15.98 8.83 84.43 84.98 2.0\nTable 29: Comparison of the post-trained versions of open-weight LLMs on Italian benchmarks.\n37\n--- Page 38 ---\nA.1.13 L ATVIAN (LV)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 60.40 60.88 62.29 54.38 1.2\nLLaMa-3.1-8B 42.97 44.18 48.20 51.44 2.5\nGranite-3-8B 29.47 32.51 35.67 49.50 4.0\nQwen-2.5-7B 35.18 38.14 53.44 54.58 2.2\nOLMo-2-7B 27.31 30.77 32.91 48.79 5.8\nAya-23-8B 28.67 32.39 33.70 48.60 5.2\nEuropean\nMistral-7B 29.40 32.94 37.10 50.16 3.0\nOcciglot-7B-eu5 28.70 32.20 32.65 50.69 3.5\nSalamandra-7B 55.52 58.34 40.89 49.99 2.5\nEuroLLM-9B 62.88 63.48 54.24 56.47 1.0\nTable 30: Comparison of the pre-trained versions of open-weight LLMs on Latvian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 58.76 56.38 60.67 58.81 76.84 81.35 1.0\nLLaMa-3.1-8B-IT 41.52 43.88 49.63 52.03 65.03 76.17 2.7\nGranite-3-8B-IT 29.19 32.36 36.52 50.40 41.67 66.96 5.0\nQwen-2.5-7B-IT 36.06 38.39 52.91 52.99 52.44 76.59 2.7\nOLMo-2-7B-IT 28.87 30.63 32.94 49.54 40.01 62.05 6.0\nAya-Expanse-8B 29.78 32.43 38.77 53.92 42.57 69.19 3.7\nEuropean\nMistral-7B-IT 29.40 32.84 34.27 51.66 42.41 66.68 6.3\nMinistral-8B-IT 40.13 42.27 43.52 51.89 60.25 76.26 4.7\nOcciglot-7B-eu5-IT 28.31 31.90 29.83 52.64 44.11 60.09 6.5\nSalamandra-7B-IT 48.97 56.83 43.83 54.43 78.90 82.20 2.3\nPharia-1-LLM-7B-C 26.72 29.52 28.67 47.73 33.38 52.12 8.0\nTeuken-7B-IT-R-v0.4 47.69 53.81 35.94 57.18 74.60 73.02 3.5\nTeuken-7B-IT-C-v0.4 45.77 54.07 40.54 54.65 72.97 79.59 3.5\nEuroLLM-9B-IT 57.57 60.78 52.79 55.64 84.40 83.02 1.2\nTable 31: Comparison of the post-trained versions of open-weight LLMs on Latvian benchmarks.\n38\n--- Page 39 ---\nA.1.14 L ITHUANIAN (LT)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 62.86 62.47 63.15 53.48 1.0\nLLaMa-3.1-8B 44.32 44.99 48.23 48.61 2.8\nGranite-3-8B 30.06 32.91 35.88 47.67 5.0\nQwen-2.5-7B 35.33 37.47 52.04 52.08 2.8\nOLMo-2-7B 26.74 30.75 32.59 46.43 6.0\nAya-23-8B 32.23 37.64 39.00 49.85 3.5\nEuropean\nMistral-7B 28.91 34.10 37.48 46.38 3.2\nOcciglot-7B-eu5 27.66 32.62 32.64 46.61 3.8\nSalamandra-7B 56.69 59.16 41.45 48.63 2.0\nEuroLLM-9B 64.29 63.66 54.47 53.37 1.0\nTable 32: Comparison of the pre-trained versions of open-weight LLMs on Lithuanian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 58.67 58.75 61.90 59.11 79.11 80.52 1.0\nLLaMa-3.1-8B-IT 42.06 44.09 49.53 52.72 67.34 75.12 2.3\nGranite-3-8B-IT 29.81 33.06 36.07 48.78 43.03 66.22 5.0\nQwen-2.5-7B-IT 35.14 37.83 52.41 53.84 53.16 73.57 3.0\nOLMo-2-7B-IT 28.84 30.88 34.02 45.70 42.89 62.53 6.0\nAya-Expanse-8B 33.24 36.99 42.88 52.05 54.90 74.27 3.7\nEuropean\nMistral-7B-IT 29.15 33.43 35.44 49.71 45.24 67.26 6.0\nMinistral-8B-IT 41.39 43.33 42.32 51.43 60.62 74.37 4.5\nOcciglot-7B-eu5-IT 27.69 32.57 29.74 48.49 44.77 61.51 7.0\nSalamandra-7B-IT 50.07 58.00 44.54 55.94 80.45 81.12 2.0\nPharia-1-LLM-7B-C 25.39 29.66 28.82 45.61 34.43 51.19 8.0\nTeuken-7B-IT-R-v0.4 49.31 54.69 36.46 57.01 71.26 68.98 3.5\nTeuken-7B-IT-C-v0.4 48.62 55.16 40.41 52.64 68.82 78.27 3.7\nEuroLLM-9B-IT 57.95 61.37 53.86 54.23 84.43 81.81 1.3\nTable 33: Comparison of the post-trained versions of open-weight LLMs on Lithuanian bench-\nmarks.\n39\n--- Page 40 ---\nA.1.15 P OLISH (PL)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 69.13 67.22 66.32 53.38 1.2\nLLaMa-3.1-8B 57.07 58.48 55.28 51.69 3.0\nGranite-3-8B 41.93 44.10 45.03 49.87 5.0\nQwen-2.5-7B 53.37 54.31 64.14 53.96 2.8\nOLMo-2-7B 34.82 38.41 39.57 47.51 6.0\nAya-23-8B 56.11 61.96 49.48 51.91 3.0\nEuropean\nMistral-7B 56.44 55.39 51.73 50.54 2.5\nOcciglot-7B-eu5 48.03 48.92 43.05 43.03 4.0\nSalamandra-7B 62.24 64.34 43.60 48.89 2.5\nEuroLLM-9B 66.71 66.27 55.86 53.20 1.0\nTable 34: Comparison of the pre-trained versions of open-weight LLMs on Polish benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 64.32 64.18 65.11 62.42 80.42 81.55 1.3\nLLaMa-3.1-8B-IT 50.58 56.19 57.67 58.13 77.69 79.41 3.2\nGranite-3-8B-IT 41.60 45.90 46.67 53.31 59.04 77.56 5.2\nQwen-2.5-7B-IT 48.51 54.68 63.91 59.39 70.65 80.64 3.2\nOLMo-2-7B-IT 35.27 39.62 40.36 51.75 59.94 75.89 5.8\nAya-Expanse-8B 54.63 63.88 54.69 57.43 83.01 82.28 2.3\nEuropean\nMistral-7B-IT 58.31 55.89 50.06 61.72 73.22 78.92 3.0\nMinistral-8B-IT 50.89 53.40 47.75 55.00 70.88 79.31 4.7\nOcciglot-7B-eu5-IT 42.86 46.65 40.22 47.47 58.80 69.97 6.7\nSalamandra-7B-IT 55.52 63.40 47.24 52.89 80.37 81.74 3.3\nPharia-1-LLM-7B-C 27.29 31.56 29.57 44.02 37.21 60.59 8.0\nTeuken-7B-IT-R-v0.4 55.54 60.84 37.64 54.80 71.83 74.49 4.5\nTeuken-7B-IT-C-v0.4 53.24 59.14 38.01 54.19 71.13 79.65 4.7\nEuroLLM-9B-IT 60.10 64.13 55.54 55.34 84.24 81.93 1.2\nTable 35: Comparison of the post-trained versions of open-weight LLMs on Polish benchmarks.\n40\n--- Page 41 ---\nA.1.16 P ORTUGUESE (PT)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 71.55 71.14 68.09 47.78 29.31 11.58 1.8\nLLaMa-3.1-8B 61.94 66.16 59.61 46.67 19.98 8.00 4.0\nGranite-3-8B 63.66 69.14 57.77 48.27 20.44 9.16 2.7\nQwen-2.5-7B 64.29 65.50 69.73 56.52 31.57 9.09 2.2\nOLMo-2-7B 48.82 50.38 48.58 44.26 13.23 2.45 6.0\nAya-23-8B 60.74 67.54 51.67 47.84 14.08 4.14 4.3\nEuropean\nMistral-7B 61.35 62.38 55.97 44.91 17.54 11.77 2.2\nOcciglot-7B-eu5 57.82 61.47 48.97 42.05 11.54 4.96 3.5\nSalamandra-7B 64.62 67.63 43.88 42.97 5.43 2.94 3.2\nEuroLLM-9B 70.01 68.67 56.90 49.43 17.28 15.08 1.2\nTable 36: Comparison of the pre-trained versions of open-weight LLMs on Portuguese benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 69.88 67.89 63.63 63.61 18.52 8.87 79.11 83.56 2.4\nLLaMa-3.1-8B-IT 57.34 62.40 62.03 57.40 23.73 6.22 79.27 82.40 4.0\nGranite-3-8B-IT 62.28 70.64 60.63 60.08 20.01 12.50 79.86 83.35 2.4\nQwen-2.5-7B-IT 59.42 63.61 68.88 62.05 28.26 6.78 78.62 83.32 3.1\nOLMo-2-7B-IT 49.86 52.96 48.59 49.50 11.69 5.09 75.52 82.18 6.0\nAya-Expanse-8B 59.44 68.68 58.10 56.20 18.99 7.87 81.44 84.13 2.8\nEuropean\nMistral-7B-IT 63.01 62.22 53.78 61.38 17.62 7.05 76.61 81.71 4.2\nMinistral-8B-IT 60.28 65.69 57.53 55.18 20.08 9.56 79.94 83.18 2.4\nOcciglot-7B-eu5-IT 54.00 59.54 45.09 48.46 10.83 4.20 68.93 73.78 6.9\nSalamandra-7B-IT 58.27 66.96 47.38 51.12 7.10 9.33 80.34 83.99 4.0\nPharia-1-LLM-7B-C 58.02 64.62 44.37 47.81 10.37 15.11 76.27 81.25 5.4\nTeuken-7B-IT-R-v0.4 58.64 63.95 38.68 49.28 8.87 2.78 77.02 77.38 6.2\nTeuken-7B-IT-C-v0.4 57.53 63.34 41.24 52.14 9.20 5.39 78.52 82.04 5.5\nEuroLLM-9B-IT 64.23 68.37 56.63 54.65 15.23 12.60 82.03 84.17 1.8\nTable 37: Comparison of the post-trained versions of open-weight LLMs on Portuguese bench-\nmarks.\n41\n--- Page 42 ---\nA.1.17 R OMANIAN (RO)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 69.45 67.81 67.77 51.81 1.8\nLLaMa-3.1-8B 57.97 59.78 57.94 51.82 2.8\nGranite-3-8B 45.11 45.59 46.90 52.48 4.2\nQwen-2.5-7B 51.27 49.04 64.81 57.05 2.8\nOLMo-2-7B 40.04 42.69 44.09 48.99 6.0\nAya-23-8B 57.87 63.83 50.48 51.60 3.5\nEuropean\nMistral-7B 55.79 54.87 53.68 53.89 2.2\nOcciglot-7B-eu5 49.32 48.81 43.42 47.58 3.8\nSalamandra-7B 62.24 64.55 43.23 48.87 2.8\nEuroLLM-9B 65.60 66.50 56.89 52.82 1.2\nTable 38: Comparison of the pre-trained versions of open-weight LLMs on Romanian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 65.42 63.59 66.72 61.29 81.76 83.74 1.5\nLLaMa-3.1-8B-IT 53.76 57.59 59.39 57.91 80.29 82.18 3.0\nGranite-3-8B-IT 46.02 46.58 50.33 57.79 61.77 79.80 5.2\nQwen-2.5-7B-IT 48.93 49.28 64.33 58.90 68.53 81.65 3.5\nOLMo-2-7B-IT 41.09 44.16 45.47 50.55 70.18 80.31 5.5\nAya-Expanse-8B 57.32 64.68 56.48 55.96 84.85 84.67 2.3\nEuropean\nMistral-7B-IT 57.82 54.45 51.66 62.54 73.64 80.60 3.5\nMinistral-8B-IT 51.98 52.57 53.46 54.02 71.53 81.03 4.7\nOcciglot-7B-eu5-IT 44.64 47.54 40.61 52.06 62.82 71.63 6.5\nSalamandra-7B-IT 56.30 62.97 48.37 49.92 82.43 84.25 3.3\nPharia-1-LLM-7B-C 31.18 33.12 33.90 48.16 38.69 67.62 8.0\nTeuken-7B-IT-R-v0.4 55.66 60.22 36.44 57.03 79.51 76.29 4.2\nTeuken-7B-IT-C-v0.4 53.99 59.57 40.39 53.06 79.09 82.17 4.5\nEuroLLM-9B-IT 61.30 64.68 57.27 54.89 85.67 84.96 1.3\nTable 39: Comparison of the post-trained versions of open-weight LLMs on Romanian benchmarks.\n42\n--- Page 43 ---\nA.1.18 S LOVAK (SK)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 66.69 65.21 65.61 50.14 1.2\nLLaMa-3.1-8B 52.09 53.29 54.31 49.70 2.5\nGranite-3-8B 45.11 48.51 47.25 47.25 4.0\nQwen-2.5-7B 45.88 47.12 62.57 50.98 2.8\nOLMo-2-7B 29.65 34.13 37.53 42.64 6.0\nAya-23-8B 43.50 48.95 44.96 45.35 4.5\nEuropean\nMistral-7B 45.24 46.42 48.32 43.26 2.8\nOcciglot-7B-eu5 40.23 43.10 41.29 42.40 4.0\nSalamandra-7B 60.43 62.28 43.24 44.88 2.2\nEuroLLM-9B 66.28 66.03 55.96 52.67 1.0\nTable 40: Comparison of the pre-trained versions of open-weight LLMs on Slovak benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 62.59 62.24 65.45 61.59 78.56 82.19 1.0\nLLaMa-3.1-8B-IT 49.59 51.77 56.02 55.75 69.79 79.39 3.2\nGranite-3-8B-IT 45.09 50.33 49.26 53.75 62.44 80.12 4.3\nQwen-2.5-7B-IT 44.83 47.59 62.23 57.38 58.02 80.19 3.7\nOLMo-2-7B-IT 31.36 34.96 37.15 41.71 45.76 71.67 6.0\nAya-Expanse-8B 46.99 53.83 51.53 53.78 72.69 81.16 2.8\nEuropean\nMistral-7B-IT 45.99 46.48 47.21 52.47 58.22 77.81 5.0\nMinistral-8B-IT 48.34 50.29 49.81 50.50 64.34 79.01 4.3\nOcciglot-7B-eu5-IT 35.52 42.39 38.24 44.19 50.60 69.04 7.0\nSalamandra-7B-IT 54.14 60.81 47.95 50.84 79.94 82.78 2.5\nPharia-1-LLM-7B-C 27.06 30.10 29.59 45.06 33.17 56.93 7.8\nTeuken-7B-IT-R-v0.4 51.69 58.39 37.88 53.73 72.32 75.15 4.0\nTeuken-7B-IT-C-v0.4 49.60 57.64 38.96 50.00 75.38 80.51 4.2\nEuroLLM-9B-IT 61.14 63.08 54.87 52.49 83.86 83.10 1.2\nTable 41: Comparison of the post-trained versions of open-weight LLMs on Slovak benchmarks.\n43\n--- Page 44 ---\nA.1.19 S LOVENIAN (SL)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 64.85 62.96 64.67 49.55 1.2\nLLaMa-3.1-8B 50.95 50.20 52.65 47.54 2.8\nGranite-3-8B 35.77 39.31 41.54 48.56 3.8\nQwen-2.5-7B 41.30 42.89 58.86 51.96 2.2\nOLMo-2-7B 28.90 33.84 36.23 46.02 5.8\nAya-23-8B 33.25 37.90 39.37 45.67 5.2\nEuropean\nMistral-7B 52.94 50.51 50.00 47.64 2.5\nOcciglot-7B-eu5 41.83 43.53 39.41 43.69 4.0\nSalamandra-7B 58.61 61.01 42.33 45.96 2.5\nEuroLLM-9B 65.73 64.53 55.57 52.87 1.0\nTable 42: Comparison of the pre-trained versions of open-weight LLMs on Slovenian benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 60.96 59.36 63.98 59.17 77.43 82.09 1.0\nLLaMa-3.1-8B-IT 46.46 48.91 53.50 52.77 71.80 78.72 2.5\nGranite-3-8B-IT 36.25 39.81 43.69 51.56 50.94 76.36 4.5\nQwen-2.5-7B-IT 40.92 43.13 59.41 56.29 56.38 78.93 2.5\nOLMo-2-7B-IT 30.38 34.10 36.51 45.44 48.01 72.79 6.0\nAya-Expanse-8B 36.18 40.50 44.97 49.13 56.00 76.20 4.5\nEuropean\nMistral-7B-IT 52.72 50.55 48.92 55.82 69.32 79.31 3.3\nMinistral-8B-IT 46.67 48.29 50.07 51.18 65.82 78.58 5.0\nOcciglot-7B-eu5-IT 35.74 41.71 35.77 46.08 56.03 69.09 6.8\nSalamandra-7B-IT 51.75 59.87 46.81 50.76 80.94 82.97 3.3\nPharia-1-LLM-7B-C 25.28 29.94 28.60 41.34 35.31 55.67 8.0\nTeuken-7B-IT-R-v0.4 51.99 56.82 34.19 55.29 72.78 75.77 4.3\nTeuken-7B-IT-C-v0.4 49.27 56.89 38.56 53.67 73.21 80.45 3.7\nEuroLLM-9B-IT 59.46 62.12 55.73 53.32 83.95 83.64 1.5\nTable 43: Comparison of the post-trained versions of open-weight LLMs on Slovenian benchmarks.\n44\n--- Page 45 ---\nA.1.20 S PANISH (ES)\nPre-trained Arc Hellaswag MMLU TruthfulQA MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 73.11 72.85 68.27 48.88 29.92 10.65 2.0\nLLaMa-3.1-8B 63.52 67.88 60.45 47.03 19.70 4.10 3.7\nGranite-3-8B 65.07 71.35 57.92 49.71 20.67 11.41 2.7\nQwen-2.5-7B 66.34 67.11 69.65 55.80 32.63 11.90 1.8\nOLMo-2-7B 51.33 53.90 49.39 42.89 13.81 2.12 6.0\nAya-23-8B 60.56 67.71 52.29 45.64 14.06 2.78 4.8\nEuropean\nMistral-7B 62.99 65.16 56.25 45.56 17.36 10.09 2.3\nOcciglot-7B-eu5 63.97 71.24 51.69 43.86 12.52 4.99 2.7\nSalamandra-7B 65.95 69.73 45.07 41.98 5.96 2.45 3.5\nEuroLLM-9B 70.51 70.02 56.51 45.36 17.18 14.72 1.5\nTable 44: Comparison of the pre-trained versions of open-weight LLMs on Spanish benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 70.36 69.63 68.01 58.96 29.37 8.53 78.80 84.23 2.6\nLLaMa-3.1-8B-IT 59.42 65.15 63.33 53.04 24.31 2.88 80.66 83.55 4.2\nGranite-3-8B-IT 64.23 72.43 60.51 61.32 20.58 10.95 81.30 84.30 2.2\nQwen-2.5-7B-IT 60.34 64.12 69.36 59.95 30.69 11.01 79.11 84.08 2.6\nOLMo-2-7B-IT 51.95 56.06 48.90 52.71 12.31 3.54 78.35 83.10 5.8\nAya-Expanse-8B 59.56 68.92 57.92 53.42 20.41 3.08 82.54 84.48 3.0\nEuropean\nMistral-7B-IT 65.09 64.58 54.05 61.45 17.70 11.28 78.13 82.63 4.2\nMinistral-8B-IT 62.29 67.08 58.11 51.90 19.68 7.64 80.54 83.64 3.4\nOcciglot-7B-eu5-IT 57.86 69.06 47.85 46.01 12.28 4.13 71.82 75.66 6.1\nSalamandra-7B-IT 60.13 67.88 49.42 46.06 7.64 11.51 81.58 84.85 4.4\nPharia-1-LLM-7B-C 62.96 67.25 45.09 47.99 10.53 14.32 78.53 82.74 4.8\nTeuken-7B-IT-R-v0.4 60.58 65.28 39.91 53.06 9.56 3.24 78.64 78.99 6.0\nTeuken-7B-IT-C-v0.4 59.60 64.46 43.87 52.64 10.63 4.46 79.38 82.82 5.5\nEuroLLM-9B-IT 64.18 68.31 56.93 48.05 18.32 14.78 83.19 85.12 2.1\nTable 45: Comparison of the post-trained versions of open-weight LLMs on Spanish benchmarks.\n45\n--- Page 46 ---\nA.1.21 S WEDISH (SV)\nPre-trained Arc Hellaswag MMLU TruthfulQA Borda C ↓\n(25-shot) (10-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 70.21 70.38 67.65 55.16 1.5\nLLaMa-3.1-8B 59.00 64.36 57.97 55.83 2.2\nGranite-3-8B 45.63 49.42 48.47 52.47 4.0\nQwen-2.5-7B 51.94 53.94 66.10 56.37 2.2\nOLMo-2-7B 38.45 43.04 44.74 46.33 5.8\nAya-23-8B 41.37 48.31 44.40 49.32 5.2\nEuropean\nMistral-7B 57.83 59.47 54.34 50.68 2.5\nOcciglot-7B-eu5 49.79 53.05 44.46 48.56 3.8\nSalamandra-7B 62.95 66.80 43.82 48.77 2.8\nEuroLLM-9B 68.25 68.87 56.50 56.25 1.0\nTable 46: Comparison of the pre-trained versions of open-weight LLMs on Swedish benchmarks.\nPost-trained Arc Hellaswag MMLU TruthfulQA WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 66.75 67.24 67.11 64.92 83.27 85.24 1.0\nLLaMa-3.1-8B-IT 53.32 61.95 60.11 58.05 82.42 83.68 2.5\nGranite-3-8B-IT 45.69 51.87 50.65 57.74 68.75 82.53 4.5\nQwen-2.5-7B-IT 49.79 53.59 65.64 59.28 72.40 83.81 2.5\nOLMo-2-7B-IT 38.84 44.52 44.14 50.53 65.51 81.64 6.0\nAya-Expanse-8B 43.38 50.38 51.00 56.69 69.78 82.99 4.5\nEuropean\nMistral-7B-IT 59.69 59.93 51.53 63.31 77.81 82.99 3.5\nMinistral-8B-IT 53.08 56.86 53.85 57.33 78.14 83.07 4.3\nOcciglot-7B-eu5-IT 44.66 51.36 41.63 52.81 66.11 74.16 6.7\nSalamandra-7B-IT 57.44 65.46 48.22 54.85 84.06 85.63 3.2\nPharia-1-LLM-7B-C 31.10 34.50 34.20 46.43 47.83 73.03 8.0\nTeuken-7B-IT-R-v0.4 58.00 63.83 38.93 58.09 76.37 78.74 4.5\nTeuken-7B-IT-C-v0.4 56.14 62.79 38.40 54.00 80.55 83.55 4.7\nEuroLLM-9B-IT 62.50 66.82 55.51 58.41 86.26 86.18 1.2\nTable 47: Comparison of the post-trained versions of open-weight LLMs on Swedish benchmarks.\n46\n--- Page 47 ---\nA.2 A DDITIONAL LANGUAGES\nResults for pre-trained and post-trained models across eight languages which are not in the 24 offi-\ncial European languages (Catalan, Ukrainian, Arabic, Chinese, Hindi, Russian, Japanese, and Ko-\nrean) are presented in the following tables. Among EU-made models, EuroLLM-9B demonstrates\nconsistently superior performance across all these languages. In comparison with non-EU models,\nEuroLLM-9B has a comparable performance to Gemma-2-9B while surpassing the capabilities of\nother LLMs in the evaluation.\nA.2.1 C ATALAN (CA)\nPre-trained Arc-C Hellaswag MMLU Borda C ↓\n(25-shot) (10-shot) (5-shot)\nNon-European\nGemma-2-9B 55.49 66.07 64.56 1.3\nLLaMa-3.1-8B 45.03 60.81 55.12 3.3\nGranite-3-8B 47.68 63.21 54.58 2.7\nQwen-2.5-7B 45.63 53.77 65.30 2.7\nOLMo-2-7B 33.10 41.72 42.62 6.0\nAya-23-8B 35.42 48.70 45.06 5.0\nEuropean\nMistral-7B 47.17 59.59 51.56 2.7\nOcciglot-7B-eu5 26.52 59.49 45.07 3.7\nSalamandra-7B 48.37 63.24 39.56 2.7\nEuroLLM-9B 51.54 64.15 51.95 1.0\nTable 48: Comparison of the pre-trained versions of open-weight LLMs on Catalan benchmarks.\nPost-trained Arc-C Hellaswag MMLU WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 0.5326 0.6195 0.6402 78.84 82.67 1.0\nLLaMa-3.1-8B-IT 0.4991 0.5883 0.5591 77.82 80.76 2.8\nGranite-3-8B-IT 0.4074 0.4849 0.4832 77.34 82.21 4.0\nQwen-2.5-7B-IT 0.4734 0.5312 0.6293 67.38 81.03 3.6\nOLMo-2-7B-IT 0.3791 0.4524 0.4302 63.49 78.12 6.0\nAya-Expanse-8B 0.4185 0.5359 0.5256 74.28 81.64 3.6\nEuropean\nMistral-7B-IT 0.4983 0.5893 0.5071 75.46 80.23 2.8\nMinistral-8B-IT 0.4700 0.5873 0.5015 75.38 80.53 3.4\nOcciglot-7B-eu5-IT 0.4125 0.5715 0.4089 67.22 72.37 5.4\nSalamandra-7B-IT 0.4383 0.6070 0.4553 80.5 83.6 2.8\nPharia-1-LLM-7B-C 0.3722 0.4789 0.3801 62.05 76.92 7.4\nTeuken-7B-IT-R-v0.4 0.4065 0.4947 0.3714 65.3 78.16 6.2\nTeuken-7B-IT-C-v0.4 0.3825 0.4918 0.3924 62.63 72.26 7.0\nEuroLLM-9B-IT 0.5043 0.6194 0.5420 81.21 83.62 1.0\nTable 49: Comparison of the post-trained versions of open-weight LLMs on Catalan benchmarks.\n47\n--- Page 48 ---\nA.2.2 U KRAINIAN (UK)\nPre-trained Arc-C Hellaswag MMLU Borda C ↓\n(25-shot) (10-shot) (5-shot)\nNon-European\nGemma-2-9B 55.86 62.89 61.24 1.0\nLLaMa-3.1-8B 44.82 54.72 49.99 2.7\nGranite-3-8B 31.65 42.74 39.60 5.0\nQwen-2.5-7B 42.00 49.96 60.98 3.0\nOLMo-2-7B 25.15 35.00 34.22 6.0\nAya-23-8B 42.00 56.86 46.65 3.0\nEuropean\nMistral-7B 44.48 54.67 48.42 2.7\nOcciglot-7B-eu5 36.18 45.18 39.35 3.7\nSalamandra-7B 46.02 57.75 36.83 2.7\nEuroLLM-9B 50.21 59.95 50.94 1.0\nTable 50: Comparison of the pre-trained versions of open-weight LLMs on Ukrainian benchmarks.\nPost-trained Arc-C Hellaswag MMLU MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) 90-shot) (5-shot) (0-shot) (0-shot)\nNone-European\nGemma-2-9B-IT 49.87 59.42 60.56 10.32 26.76 81.99 81.92 1.3\nLLaMa-3.1-8B-IT 45.00 53.39 50.45 4.07 18.43 77.94 79.84 3.7\nGranite-3-8B-IT 32.76 41.12 38.94 5.29 13.40 56.76 76.88 4.7\nQwen-2.5-7B-IT 40.89 50.05 58.38 8.96 25.17 66.71 80.2 3.0\nOLMo-2-7B-IT 30.71 37.64 35.98 2.32 7.59 54.98 74.17 6.0\nAya-Expanse-8B 45.77 59.38 52.99 5.03 18.63 83.87 82.39 2.3\nEuropean\nMistral-7B-IT 46.19 54.95 47.78 9.56 15.74 75.25 79.93 2.3\nMinistral-8B-IT 44.48 54.26 45.09 5.65 15.60 72.91 79.78 3.4\nOcciglot-7B-eu5-IT 32.68 43.42 34.89 2.94 8.12 53.69 69.73 5.1\nSalamandra-7B-IT 41.49 56.00 42.60 4.23 5.72 81.24 82.21 3.7\nPharia-1-LLM-7B-C 23.52 29.31 26.76 9.89 2.31 34.62 56.71 7.0\nTeuken-7B-IT-R-v0.4 31.22 38.84 30.88 2.61 6.24 45.13 73.59 6.3\nTeuken-7B-IT-C-v0.4 31.48 38.72 32.15 2.28 7.28 41.35 64.73 6.6\nEuroLLM-9B-IT 48.16 58.61 51.34 8.33 15.26 84.01 82.36 1.6\nTable 51: Comparison of the post-trained versions of open-weight LLMs on Ukrainian benchmarks.\n48\n--- Page 49 ---\nA.2.3 A RABIC (AR)\nPre-trained Arc-C Hellaswag MMLU Borda C ↓\n(25-shot) (10-shot) (5-shot)\nNon-European\nGemma-2-9B 50.38 58.34 55.65 1.3\nLLaMa-3.1-8B 38.92 50.17 46.92 3.7\nGranite-3-8B 34.90 44.63 38.03 5.0\nQwen-2.5-7B 42.00 52.47 58.11 2.3\nOLMo-2-7B 27.72 35.59 33.66 6.0\nAya-23-8B 42.09 56.62 45.09 2.7\nEuropean\nMistral-7B 29.68 40.58 35.79 2.0\nOcciglot-7B-eu5 26.52 35.97 31.03 3.0\nSalamandra-7B 24.81 34.51 28.12 4.0\nEuroLLM-9B 47.82 57.31 45.84 1.0\nTable 52: Comparison of the pre-trained versions of open-weight LLMs on Arabic benchmarks.\nPost-trained Arc-C Hellaswag MMLU WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 50.64 55.37 55.54 68.75 77.59 2.2\nLLaMa-3.1-8B-IT 41.75 49.43 44.65 72.08 74.67 3.8\nGranite-3-8B-IT 33.70 46.55 38.61 71.07 74.87 4.4\nQwen-2.5-7B-IT 42.43 50.13 56.22 65.89 76.55 3.0\nOLMo-2-7B-IT 31.57 38.86 34.96 64.29 72.75 6.0\nAya-Expanse-8B 46.79 58.30 49.59 77.50 79.07 1.6\nEuropean\nMistral-7B-IT 29.51 39.97 33.76 59.42 71.55 3.0\nMinistral-8B-IT 38.67 46.74 35.77 62.85 74.47 2.0\nOcciglot-7B-eu5-IT 25.58 34.46 28.34 48.50 61.91 4.8\nSalamandra-7B-IT 23.1 31.81 26.21 50.11 68.01 5.8\nPharia-1-LLM-7B-C 22.41 28.96 25.56 37.03 52.43 8.0\nTeuken-7B-IT-R-v0.4 24.47 33.04 28.00 48.31 63.88 5.4\nTeuken-7B-IT-C-v0.4 24.29 32.81 28.89 46.75 58.39 6.0\nEuroLLM-9B-IT 47.39 55.81 49.2 77.18 78.56 1.0\nTable 53: Comparison of the post-trained versions of open-weight LLMs on Arabic benchmarks.\n49\n--- Page 50 ---\nA.2.4 C HINESE (ZH)\nPre-trained Arc-C MMLU MMLU-pro MUSR Borda C ↓\n(25-shot) (5-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 57.78 61.05 26.81 13.26 1.5\nLLaMa-3.1-8B 45.64 52.15 18.03 7.57 4.3\nGranite-3-8B 46.32 47.97 17.34 11.08 4.0\nQwen-2.5-7B 56.24 67.00 31.37 13.03 1.5\nOLMo-2-7B 35.30 40.20 10.61 10.19 5.8\nAya-23-8B 46.58 46.54 13.09 12.80 4.0\nEuropean\nMistral-7B 45.73 45.58 14.61 8.30 2.0\nOcciglot-7B-eu5 40.60 38.28 9.39 6.58 3.3\nSalamandra-7B 35.73 31.24 2.89 7.41 3.8\nEuroLLM-9B 54.87 49.29 15.78 10.02 1.0\nTable 54: Comparison of the pre-trained versions of open-weight LLMs on Arabic benchmarks.\nPost-trained Arc-C MMLU MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (5-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 54.79 62.03 15.11 25.88 79.77 81.95 2.0\nLLaMa-3.1-8B-IT 50.00 54.88 8.80 20.27 80.07 80.56 3.3\nGranite-3-8B-IT 42.82 48.93 8.10 19.10 79.46 81.29 4.5\nQwen-2.5-7B-IT 50.94 65.92 12.27 29.92 81.52 81.77 1.8\nOLMo-2-7B-IT 38.12 40.91 7.90 10.58 76.97 80.50 5.8\nAya-Expanse-8B 45.38 53.43 7.74 18.61 82.37 82.48 3.5\nEuropean\nMistral-7B-IT 48.29 45.26 17.20 14.98 74.55 79.77 2.7\nMinistral-8B-IT 48.55 49.77 9.19 16.86 78.15 80.54 2.3\nOcciglot-7B-eu5-IT 37.35 36.71 9.26 9.88 58.92 73.21 5.0\nSalamandra-7B-IT 34.02 34.09 7.28 4.24 68.20 79.77 6.0\nPharia-1-LLM-7B-C 28.21 31.04 10.22 3.89 49.94 73.81 6.8\nTeuken-7B-IT-R-v0.4 35.04 33.79 6.12 6.52 69.86 78.02 6.2\nTeuken-7B-IT-C-v0.4 35.98 35.86 7.87 7.04 69.90 72.38 5.5\nEuroLLM-9B-IT 52.31 51.20 12.63 15.38 83.61 82.37 1.3\nTable 55: Comparison of the post-trained versions of open-weight LLMs on Arabic benchmarks.\n50\n--- Page 51 ---\nA.2.5 H INDI (HI)\nPre-trained Arc-C Hellaswag MMLU MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 41.18 51.53 53.43 24.63 6.42 1.4\nLLaMa-3.1-8B 35.27 45.84 43.98 14.98 1.59 3.8\nGranite-3-8B 26.71 35.07 33.80 7.91 8.80 4.4\nQwen-2.5-7B 32.11 38.67 49.72 18.67 12.37 2.6\nOLMo-2-7B 24.40 31.70 32.81 6.01 2.12 5.6\nAya-23-8B 35.19 46.58 39.65 10.69 1.95 3.6\nEuropean\nMistral-7B 27.31 34.05 32.85 10.62 3.48 1.8\nOcciglot-7B-eu5 24.49 30.58 29.73 4.57 2.98 3.0\nSalamandra-7B 24.14 29.80 24.86 1.56 1.95 4.0\nEuroLLM-9B 39.04 48.64 45.08 13.86 3.27 1.2\nTable 56: Comparison of the pre-trained versions of open-weight LLMs on Hindi benchmarks.\nPost-trained Arc-C Hellaswag MMLU MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 55.18 47.77 61.87 14.19 27.47 69.09 83.02 1.6\nLLaMa-3.1-8B-IT 49.19 44.82 54.93 5.79 20.33 67.19 81.83 3.3\nGranite-3-8B-IT 38.32 34.17 42.97 8.6 16.38 57.93 80.56 4.6\nQwen-2.5-7B-IT 49.44 38.97 63.39 12.01 29.22 55.52 81.11 2.7\nOLMo-2-7B-IT 37.21 32.33 40.49 3.94 9.80 54.00 78.94 6.0\nAya-Expanse-8B 46.79 47.50 53.10 5.89 18.81 71.42 83.88 2.9\nEuropean\nMistral-7B-IT 48.50 41.36 48.27 12.8 15.38 47.07 77.6 2.4\nMinistral-8B-IT 47.65 33.98 48.19 9.86 15.71 58.19 81.85 2.7\nOcciglot-7B-eu5-IT 35.07 29.73 38.06 4.04 9.77 34.49 66.18 5.0\nSalamandra-7B-IT 43.37 28.84 43.08 6.85 6.23 39.32 73.05 4.7\nPharia-1-LLM-7B-C 24.55 27.51 27.26 11.81 2.26 31.47 58.07 6.9\nTeuken-7B-IT-R-v0.4 35.16 28.36 32.65 2.48 6.69 33.85 56.98 6.4\nTeuken-7B-IT-C-v0.4 34.99 28.83 35.86 1.95 8.03 31.38 51.13 6.9\nEuroLLM-9B-IT 50.30 47.96 52.45 12.96 16.19 70.08 83.78 1.0\nTable 57: Comparison of the post-trained versions of open-weight LLMs on Hindi benchmarks.\n51\n--- Page 52 ---\nA.2.6 R USSIAN (RU)\nPre-trained Arc-C Hellaswag MMLU MMLU-pro MUSR Borda C ↓\n(25-shot) (10-shot) (5-shot) (5-shot) (0-shot)\nNon-European\nGemma-2-9B 57.40 66.77 61.98 27.47 12.44 1.4\nLLaMa-3.1-8B 46.45 60.40 52.83 17.94 7.21 3.6\nGranite-3-8B 37.81 51.33 44.48 15.16 7.41 4.4\nQwen-2.5-7B 51.15 61.67 65.44 31.12 8.87 1.6\nOLMo-2-7B 33.88 45.43 39.64 10.27 7.41 5.4\nAya-23-8B 43.97 60.01 47.23 12.56 7.38 4.4\nEuropean\nMistral-7B 46.02 58.13 48.94 15.29 6.42 2.4\nOcciglot-7B-eu5 39.44 50.80 40.35 10.04 3.90 3.6\nSalamandra-7B 46.54 61.05 38.90 5.09 5.19 3.0\nEuroLLM-9B 52.10 63.04 50.44 16.80 12.80 1.0\nTable 58: Comparison of the pre-trained versions of open-weight LLMs on Russian benchmarks.\nPost-trained Arc-C Hellaswag MMLU MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(25-shot) (10-shot) (5-shot) (0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 55.18 63.28 61.87 14.19 27.47 79.59 79.49 1.6\nLLaMa-3.1-8B-IT 49.19 58.68 54.93 5.79 20.33 77.22 77.96 3.4\nGranite-3-8B-IT 38.32 49.16 42.97 8.6 16.38 70.59 77.19 5.1\nQwen-2.5-7B-IT 49.44 57.71 63.39 12.01 29.22 72.77 78.96 2.6\nOLMo-2-7B-IT 37.21 49.77 40.49 3.94 9.80 73.94 77.32 5.4\nAya-Expanse-8B 46.79 61.82 53.10 5.89 18.81 80.98 79.73 2.9\nEuropean\nMistral-7B-IT 48.50 57.58 48.27 12.8 15.38 74.51 77.44 3.0\nMinistral-8B-IT 47.65 59.56 48.19 9.86 15.71 77.27 78.36 3.0\nOcciglot-7B-eu5-IT 35.07 49.42 38.06 4.04 9.77 58.89 69.61 5.4\nSalamandra-7B-IT 43.37 59.57 43.08 6.85 6.23 78.48 78.98 3.7\nPharia-1-LLM-7B-C 24.55 31.00 27.26 11.81 2.26 40.38 64.24 7.3\nTeuken-7B-IT-R-v0.4 35.16 47.24 32.65 2.48 6.69 58.94 75.81 5.9\nTeuken-7B-IT-C-v0.4 34.99 46.78 35.86 1.95 8.03 51.41 68.83 6.7\nEuroLLM-9B-IT 50.30 61.99 52.45 12.96 16.19 81.76 79.64 1.0\nTable 59: Comparison of the post-trained versions of open-weight LLMs on Russian benchmarks.\n52\n--- Page 53 ---\nA.2.7 J APANESE (JA)\nPre-trained MMLU-pro MUSR Borda C ↓\n(5-shot) (0-shot)\nNon-European\nGemma-2-9B 25.32 4.96 3.0\nLLaMa-3.1-8B 15.98 5.06 3.0\nGranite-3-8B 15.20 4.13 4.5\nQwen-2.5-7B 27.69 7.08 1.5\nOLMo-2-7B 9.96 3.44 6.0\nAya-23-8B 12.84 8.66 3.0\nEuropean\nMistral-7B 13.47 3.77 2.5\nOcciglot-7B-eu5 8.44 3.57 3.5\nSalamandra-7B 2.40 3.80 3.0\nEuroLLM-9B 14.88 8.30 1.0\nTable 60: Comparison of the pre-trained versions of open-weight LLMs on Japanese benchmarks.\nPost-trained MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 11.15 24.67 82.08 81.35 2.3\nLLaMa-3.1-8B-IT 12.44 18.06 81.19 79.45 3.0\nGranite-3-8B-IT 3.54 16.97 80.98 80.26 4.5\nQwen-2.5-7B-IT 12.07 25.86 78.55 81.26 2.8\nOLMo-2-7B-IT 3.27 8.96 76.21 78.43 6.0\nAya-Expanse-8B 5.06 17.92 86.24 82.69 2.5\nEuropean\nMistral-7B-IT 13.36 13.90 74.76 78.30 2.5\nMinistral-8B-IT 9.73 15.06 79.65 78.95 2.0\nOcciglot-7B-eu5-IT 2.94 8.17 54.95 69.71 5.8\nSalamandra-7B-IT 2.09 2.48 62.91 77.41 6.0\nPharia-1-LLM-7B-C 4.17 2.96 43.11 65.59 7.0\nTeuken-7B-IT-R-v0.4 2.61 5.69 57.88 74.18 6.0\nTeuken-7B-IT-C-v0.4 6.18 5.71 60.02 65.88 5.0\nEuroLLM-9B-IT 4.57 15.64 86.05 82.66 1.8\nTable 61: Comparison of the post-trained versions of open-weight LLMs on Japanese benchmarks.\n53\n--- Page 54 ---\nA.2.8 K OREAN (KO)\nPre-trained MMLU-pro MUSR Borda C ↓\n(5-shot) (0-shot)\nNon-European\nGemma-2-9B 24.66 7.64 2.0\nLLaMa-3.1-8B 16.03 5.03 4.0\nGranite-3-8B 13.96 5.62 4.0\nQwen-2.5-7B 26.89 9.96 1.0\nOLMo-2-7B 9.34 3.31 6.0\nAya-23-8B 12.21 5.95 4.0\nEuropean\nMistral-7B 13.14 3.34 2.5\nOcciglot-7B-eu5 8.50 3.27 3.5\nSalamandra-7B 2.36 12.17 2.5\nEuroLLM-9B 15.16 7.84 1.5\nTable 62: Comparison of the pre-trained versions of open-weight LLMs on Korean benchmarks.\nPost-trained MUSR MMLU-pro WMT24++ WMT24++ Borda C ↓\nen→xx xx →en\n(0-shot) (5-shot) (0-shot) (0-shot)\nNon-European\nGemma-2-9B-IT 11.61 23.20 80.59 82.27 2.3\nLLaMa-3.1-8B-IT 8.4 14.97 80.57 80.39 4.0\nGranite-3-8B-IT 2.88 16.11 77.85 80.85 4.5\nQwen-2.5-7B-IT 13.3 24.77 72.78 82.51 2.3\nOLMo-2-7B-IT 3.67 8.63 68.55 77.64 5.8\nAya-Expanse-8B 5.1 17.31 85.48 83.59 2.3\nEuropean\nMistral-7B-IT 14.72 13.16 72.96 79.59 2.3\nMinistral-8B-IT 6.38 12.38 76.8 80.52 2.8\nOcciglot-7B-eu5-IT 6.02 7.91 54.97 71.4 5.0\nSalamandra-7B-IT 7.47 2.90 60.73 77.25 4.8\nPharia-1-LLM-7B-C 3.83 3.01 38.17 60.21 7.3\nTeuken-7B-IT-R-v0.4 2.28 4.83 53.8 72.73 6.0\nTeuken-7B-IT-C-v0.4 1.62 4.93 53.36 63.63 6.8\nEuroLLM-9B-IT 7.51 14.78 85.66 83.93 1.3\nTable 63: Comparison of the post-trained versions of open-weight LLMs on Korean benchmarks.\nB P ROMPT USED TO GENERATE SYNTHETIC INSTRUCTIONS\nC P ROMPT USED TO GENERATE SYNTHETIC ANSWERS\n54\n--- Page 55 ---\nPrompt used to generate synthetic instructions\n## Definition of an “Instruction”:\nIn the context of training a large language model (LLM) for an AI assistant, an \"instruction\" can be\ncharacterized as a specific directive or command given to the model to perform a particular task or\nbehave in a certain way. An instruction for training an AI assistant is a clear, concise statement or\nquestion that:\n1. Directs the model to perform a specific action or task\n2. Provides context for the desired behavior or output\n3. May include constraints or specific requirements\n4. Is typically phrased in natural language\n5. Can be general or domain-specific\nInstructions are used to fine-tune the model’s behavior, helping it understand and respond appropri-\nately to various user inputs and scenarios. They are crucial in shaping the AI assistant’s capabilities\nand ensuring it aligns with the intended use case. Instructions typically fall into one of the categories\nbelow:\n1. Problem Solving: Coding, Mathematical reasoning, Knowledge and reasoning\n2. Creative Tasks: Creative writing, Brainstorming\n3. Information Processing: Summarization, Extraction, Classification, Translation\n4. Question Answering: Open-ended, Closed-ended, Multiple Choice\n5. Text Transformation: Rewriting\n6. Roleplay and Simulation: Inhabiting a character/persona\n7. Advisory: Asking for advice\n8. Domain-Specific Knowledge: Humanity, history, and social studies, Other (specific domains could\nbe added here as needed)\n9. General / Miscellaneous\n## Task Description:\nConsidering the definition of an Instruction, analyze the following {language} web document and\nperform a three-step analysis:\n<document >{text} <document / >\nFirstly, try to give a short summary in {language} of the document. This should help you better\nunderstand the document.\nSecondly, using the knowledge in the document, create an instruction suitable for training an AI\nsystem, where the document provides the topic and, if possible, the answer. Prioritize complex in-\nstructions that require knowledge and reasoning to solve. Aim to create an instruction that falls into\nthe{category} category. Note that the instruction should be easy to understand **without requiring\naccess to the document**. It should be clear and detailed enough for the model to comprehend the\ntask without ambiguity, even without seeing the document.\nThirdly, categorize the instruction into one of the categories defined above.\nProvide your final response in the following format:\nSummary: <summary of the document in {language} >\nInstruction: <{language} instruction for which the answer is in the document above >\nCategory: <one of the categories above >\nNOTE: “Summary:”, “Instruction:” and “Category:” keywords must be kept in english without any\nchange while the generated summary and instruction should be in {language} .\nDO NOT provide the actual response to the instruction and make sure the instruction is in {language}\neven if the document is not in the same language/variant!\nFigure 8: Prompt used to generate synthetic instructions from monolingual web data.\n55\n--- Page 56 ---\nPrompt used to generate synthetic answers\nConsider the following {language} document and the language user’s instruction below:\n<document >{document} </document >\n<instruction >{instruction} < /instruction >\nGiven the above user’s instruction, and the context provided in the document, provide a response to the\ninstruction in language . Your response should include, as much as possible, the information presented\nin the document while making the flow more clear, useful, relevant and providing a direct response\nto the instruction. It should be written to be impeccably tailored to the user’s instruction as if written\nby an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating\na high-quality, engaging, and insightful answer. Try not to add new facts that are not already in the\ndocument.\nSince the user does not have access to the document, your answer cannot directly reference the docu-\nment. The document serves only as supporting context for the instruction.\nNOTE: Since the instruction is in language , your response should also be in language . Provide ONLY\nthe response to the instruction.\nFigure 9: Prompt used to generate answers for the synthethic instructions created. The prompt uses\nthe web document used to generate the instruction as context to better contextualize the provided\nanswer.\n56",
  "text_length": 148474
}