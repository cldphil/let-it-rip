{
  "id": "http://arxiv.org/abs/2506.01091v1",
  "title": "PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation",
  "summary": "Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.\nCreating 3D effects requires specialized expertise and training in 3D animation\nsoftware and can be time consuming. Generative solutions typically rely on\ncomputationally intense methods such as diffusion models which can be slow at\n4D inference. We reformulate 3D animation as a field prediction task and\nintroduce a text-driven framework that infers a time-varying 4D flow field\nacting on 3D Gaussians. By leveraging large language models (LLMs) and\nvision-language models (VLMs) for function generation, our approach interprets\narbitrary prompts (e.g., \"make the vase glow orange, then explode\") and\ninstantly updates color, opacity, and positions of 3D Gaussians in real time.\nThis design avoids overheads such as mesh extraction, manual or physics-based\nsimulations and allows both novice and expert users to animate volumetric\nscenes with minimal effort on a consumer device even in a web browser.\nExperimental results show that simple textual instructions suffice to generate\ncompelling time-varying VFX, reducing the manual effort typically required for\nrigging or advanced modeling. We thus present a fast and accessible pathway to\nlanguage-driven 3D content creation that can pave the way to democratize VFX\nfurther.",
  "authors": [
    "Mert Kiray",
    "Paul Uhlenbruck",
    "Nassir Navab",
    "Benjamin Busam"
  ],
  "published": "2025-06-01T17:22:59Z",
  "updated": "2025-06-01T17:22:59Z",
  "categories": [
    "cs.GR",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01091v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01091v1  [cs.GR]  1 Jun 2025PromptVFX: Text-Driven Fields\nfor Open-World 3D Gaussian Animation\nMert Kiray1,2∗Paul Uhlenbruck1∗Nassir Navab1Benjamin Busam1,2\n1Technical University of Munich\n23Dwe.ai\nPromptVFX\nPromptVFXAutoVFX\nGaussian s2Life\n“Turn the vase into lava.“\n“The bulldozer accelerates forward .“\n<1𝑚𝑖𝑛\n<1𝑚𝑖𝑛10−30𝑚𝑖𝑛\n10−30𝑚𝑖𝑛\nFigure 1: Comparison of PromptVFX with existing text-driven 3D animation methods. Our\napproach generates high-quality animations in seconds, whereas others require 30 ×more time due to\ndiffusion or physics-based simulation. Exact prompts are provided in the supplementary material.\nAbstract\nVisual effects (VFX) are key to immersion in modern films, games, and AR/VR.\nCreating 3D effects requires specialized expertise and training in 3D animation\nsoftware and can be time consuming. Generative solutions typically rely on\ncomputationally intense methods such as diffusion models which can be slow at\n4D inference. We reformulate 3D animation as a field prediction task and introduce\na text-driven framework that infers a time-varying 4D flow field acting on 3D\nGaussians. By leveraging large language models (LLMs) and vision-language\nmodels (VLMs) for function generation, our approach interprets arbitrary prompts\n(e.g., “make the vase glow orange, then explode”) and instantly updates color,\nopacity, and positions of 3D Gaussians in real time. This design avoids overheads\nsuch as mesh extraction, manual or physics-based simulations and allows both\nnovice and expert users to animate volumetric scenes with minimal effort on a\nconsumer device even in a web browser2. Experimental results show that simple\ntextual instructions suffice to generate compelling time-varying VFX, reducing\nthe manual effort typically required for rigging or advanced modeling. We thus\npresent a fast and accessible pathway to language-driven 3D content creation\nthat can pave the way to democratize VFX further. Code available at 3dwe-\nai.github.io/promptvfx.\n∗Authors contributed equally to this work.\n2Web Demo: http://promptvfx.duckdns.org\nPreprint. Under review.\n--- Page 2 ---\n1 Introduction\n\"Today, with computer-generated visual effects, everything is possible. So we’ve seen everything. If\nit can be imagined, it can be put on screen.\"3The creation of 3D animation is omnipresent, yet it\nrequires specialized software and expert knowledge like the ones from visual effect studios Campisi\ntypically works with for his Hollywood productions.\nVFX artists rely on offline physical simulation software for precise motion and collision dynamics\nwhich are computationally expensive and prohibitively slow on consumer hardware [ 1]. Recent trends\nin generative AI [ 2,3] make the creative process more accessible to a wider audience. The pipelines\ntypically employ large diffusion models to generate or edit 3D scenes with text prompts [ 4,5].\nComputation cost and multi-view inconsistencies are obstacles on the way to realistic real-time\ninteraction [6, 7].\nTo truly democratize 3D animation, a fast and intuitive system that permits open-ended text instruc-\ntions with instant feedback is required. To this end, promising frameworks [ 8,9] have been created\nthat integrate large (vision) language models (L(V)LMs) to parse user prompts. However, they\nequally depend on either physics-based simulations or diffusion training loops, preventing real-time\nediting. So far, there is no solution established that accepts any textual command (e.g., “rotate the\nstatue, then change its color to gold in three seconds”) that immediately updates a 3D scene without\noffline simulation or optimization.\nPromptVFX . In this paper, we propose PromptVFX, a text-driven framework for 3D animation that\nprovides these features. Centered on Gaussian Splatting [ 10], our system translates high-level textual\ninstructions into time-varying transformations on a set of Gaussians without generating new geometry\nor launching physics simulators. A time-varying field is applied to a Gaussian splatting scene\nultimately changing centre positions, colour and opacity of individual Gaussians. When instructed\nto “raise the vase by two meters,” the system directly modifies the Gaussian centre positions over\ntime. This simple yet generic idea naturally supports real-time rendering such that the effect of a user\nprompt can be visualized within seconds rather than minutes or hours.\nA key enabler of our approach is its open-text interface, powered by an LLM and optionally a LVLM\nfor appearance-grounding. In contrast to methods that restrict users to a narrow command set or\ndomain-specific scripts, we allow any natural language request (“make the tree shake, then fade to\ntransparent”). The LLM parses these instructions into text-driven fields that change the position, color\nand opacitiy of scene parts. By pairing VFX-transformations with a robust foundation model that\nhas been trained on a large body of data, we can capitalize on the fact that \"we’ve seen everything\"\n(Campisi) and create generic cinematic or stylized effects within a single pipeline without training\nnew models or worrying about 3D consistency of hallucinated content. Our contributions can be\nsummarized as follows:\n•Open world VFX with text driven fields. We recast 3D animation as time varying transfor-\nmations on Gaussian splats to enable volumetric effects without retraining or simulation.\n•Training-free 4D animations. Our zero-shot workflow breaks natural language instructions\ninto animation phases, generates transformation functions via LLM and refines them using\nvisual language model feedback with no per scene training.\n•Real-time interactive editing. Updating Gaussian parameters directly removes mesh\nextraction, diffusion loops or simulators. Animations are delivered in under one minute on a\nsingle GPU or in a browser allowing for instant user feedback.\nWe demonstrate our method on various scenes and user prompts, showcasing dynamic, visually\nconsistent animations generated within seconds of receiving text instructions in Figure 1. Beyond\naccelerating the workflow for experienced artists, this real-time, open-text interface makes 3D\nanimation accessible to a significantly broader audience. We strongly believe that our work can help\nunify intuitive language interfaces with efficient 3D animation.\n3Gabriel Campisi. Producer, Screenwriter, Director, and Author.\n2\n--- Page 3 ---\nTable 1: Comparison of visual editing methods. The table shows existing methods sorted by\nfeatures regarding input/output, interfaces, effect capacities, and system properties. PromptVFX\n(Ours) is the only method that simultaneously fullfills all features while allowing for interactive user\nrefinement.\nInput & Output Editing Capacities Properties\nMethodReal World\nVideo EditingFree-Viewpoint\nRenderingEditing\nInterfaceViewpoint\nConsistentContinuous\nTime Rep.Open-world\nQueryInteractive\nRefinementAppearance\nChangeObject\nAnimationsParticle\nEffectsZero-\nShotNo Complex\nPreprocessingDiffusion\nModel FreeAnimation\nSoftware Free\nVisual Programming [11] ✓ Language ✓ ✓ ✓\nFRESCO [12] ✓ Language ✓ ✓ ✓ ✓ ✓\nClimateNeRF [13] ✓ ✓ Scripts ✓ ✓ ✓ ✓\nGaussianEditor [14] ✓ ✓ GUI ✓ ✓ ✓ ✓ ✓\nGaussian Grouping [15] ✓ ✓ GUI ✓ ✓ ✓ ✓\nPhysGaussian [16] ✓ ✓ GUI ✓ ✓ ✓ ✓\nVR-GS [17] ✓ ✓ GUI ✓ ✓ ✓\nGaussian Splashing [18] ✓ ✓ GUI ✓ ✓ ✓\nDMRF [19] ✓ ✓ GUI ✓ ✓ ✓ ✓ ✓ ✓\nInstruct-N2N [9] ✓ ✓ Language ✓ ✓ ✓ ✓\nDGE [20] ✓ ✓ Language ✓ ✓ ✓ ✓\nChat-Edit-3D [21] ✓ ✓ Language ✓ ✓ ✓ ✓ ✓ ✓\nGaussians2Life [5] ✓ ✓ Language ✓ ✓ ✓ ✓ ✓ ✓\nDreamGaussians4D [22] ✓ ✓ Language ✓ ✓ ✓ ✓ ✓ ✓ ✓\nAutoVFX [8] ✓ ✓ Language ✓ ✓ ✓ ✓ ✓ ✓ ✓\nPromptVFX (Ours) ✓ ✓ Language ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\n2 Related Work\nPromptVFX builds upon recent advancements in generative models for text-driven content creation,\n4D content generation, 3D editing, physics-based VFX, and interactive interfaces for scene manipula-\ntion. We review key contributions in these areas and position our work in relation to them. Table 1\nsummarizes key features of related editing tools.\nGenerative text-to-pixel models. Early generative models such as Denoising Diffusion Probabilis-\ntic Models (DDPM) [ 23] and latent diffusion models [ 2] have demonstrated remarkable success\nin text-to-image generation. These approaches have been extended to video generation through\ntext-to-video diffusion models [ 24–26], which generate temporally coherent image sequences but\nremain computationally expensive. Diffusion models have also been adapted to synthesize images\nfrom different viewpoints of a scene [ 27,28]. However, enforcing multi-view consistency is not\nstraightforward and generated results can change appearance under varied 3D viewpoint. Several\nworks attempt to enforce view consistency through explicit constraints [29, 30, 4].\nDynamic 3D generation. The representation of 3D Gaussian splatting [ 10] has emerged as an efficient\nalternative to implicit neural representations like NeRF [ 31]. Recent works extend Gaussian Splatting\nto dynamic 4D content [ 32–35], enabling animated reconstructions with controlled deformations [ 36,\n37]. However, dynamic 3D content generation still lags behind video-based approaches in realism,\ntypically focusing on isolated foreground or single objects [ 38–40]. Current approaches can require\nmultiple hours of optimization for a few seconds of scene content [41].\nDiffusion-based 3D editing. Editing static 3D scenes has been explored through diffusion-based\nrefinement, such as InstructNeRF2NeRF [ 9], which adapts the Pix2Pix paradigm [ 42] for NeRF-based\nrepresentations. Similar approaches exist for Gaussian splatting [ 43], but these methods remain\ncomputationally expensive and require retraining for each edit with many 2D diffusion steps from\nvarious perspectives.\nVisual effects with simulations. Conventional VFX systems rely on physics engines for scene\ninteractions, but their applications to generative 3D content remain limited. Existing approaches\nincorporate rigid-body physics [ 44,45], particle dynamics [ 18,13], and elastic deformations [ 46],\nall constraint to specific physical interactions. Spring-Gaus [ 47], for instance, applies spring-mass\ndynamics to Gaussian splats in scenarios of objects falling onto a plane. More flexible approaches\nsuch as PhysGaussian [ 16] extend capabilities to a few simulation types. An indirect way to facilitate\nobject interactions is to first extract explicit meshes from the scene representation [ 48,17,49–51].\nThis allows to use existing mesh-based physics simulators at the cost of error propagation and\ncompute. Unlike these works, our method enables open-ended, user-driven animations without\nrequiring physical constraints, explicit simulations or the preliminary extraction of object meshes.\nInterfaces: from code to language. Traditional 3D editing interfaces rely on GUI-based or script-\ndriven tools [ 14,17,13,52,15], which often require expert knowledge. This prevents the models\nto be accessible to a wider user group. Recent approaches integrate LLMs for structured editing by\nsplitting a complex task into smaller sub-elements [ 53,21] at the cost of computational overhead. In\ncontrast, our system offers an intuitive direct natural language interface allowing for democratization\nof 3D content editing while being fast enough to iterate during the creative process.\n3\n--- Page 4 ---\n“The bulldozer \naccelerates forward.“\nAnimation \nBreakdown\nParameter 𝐴 \nBehavior\n𝑓𝑖(𝐴,𝑡)\n𝑓3(𝐴,𝑡)\n𝑓2(𝐴,𝑡)\n𝑓1(𝐴,𝑡)\nHypotheses\n𝑓𝑚(𝐴,𝑡)Hypothesis Generation\n Initial PromptInitial Prompt\nHypothesis Selection\n𝑚\nSelected \nHypothesis\n“Don’t drive off the table.”Initial Prompt\nFollow -Up Prompt\nRefined \nHypothesis\nInitial Prompt\nRefinement \nPromptRefined \nHypothesisManual\nAutomaticRefinement Hypotheses Generation & Selection Input Animation\noptional\nrequiredFigure 2: Overview of the PromptVFX pipeline. Given a user-provided textual prompt, the system\nfirst decomposes it into structured animation phases. A large language model (LLM) then generates\nparametric functions that define the motion, color, and opacity changes of 3D Gaussians over time. To\nhandle ambiguity, multiple animation hypotheses are generated and evaluated using a vision-language\nmodel (VLM) or user feedback. The selected animation is further refined through automatic and\ninteractive text-based corrections, ensuring high-quality, real-time results. More details on prompt\nformulation and scoring are provided in the supplementary material.\nTowards open-world 3D scene editing. Recent works have explored ways towards more flexible\nscene editing of Gaussian Splatting scenes. Animate3D [ 54] allows the animation of single assets but\nis constrained by dataset limitation and training. Gaussians2Life [ 5] extends DreamGaussian4D [ 22]\nby applying 2D diffusion models to generate motion sequences, which are then lifted to 3D, making\nthem view-consistent at the cost of computational complexity. Our approach bypasses diffusion-based\ninference by directly modifying Gaussian parameters using field-based transformations which enables\ninteractive updates. Similarly, AutoVFX [ 8] integrates LLM-generated scripts into Blender-based\nsimulations, producing highly realistic effects at the cost of compute. While producing highly realistic\nscenes for supported effects, this approach is limited to pre-defined VFX modules. In contrast, we\napply transformations directly on Gaussian splats, providing a more flexible and efficient framework\nfor text-driven animation.\nWhile previous works have demonstrated powerful generative capabilities in 3D content creation, they\noften suffer from slow inference times, limited editability, or reliance on predefined physical models.\nOur method introduces a novel approach by reformulating 3D animation as a field prediction task,\nallowing interactive, open-world text-driven modifications without additional training or per-scene\noptimization.\n3 Methodology\nOur goal is to enable real-time, text-driven animation of 3D scenes without relying on diffusion-based\noptimization or physics simulators. In this section, we describe our approach in four parts: (1) repre-\nsentation of 3D scenes as Gaussian splats, (2) formulation of continuous time-dependent animation\nfields, (3) LLM-based translation of open-domain text into parametric fields, and (4) pipeline details\nensuring efficient, user-friendly performance. Figure 2 provides an overview of our zero-shot pipeline.\n3.1 3D Representation via Gaussian Splatting\nWe adopt a Gaussian splatting (3DGS) representation for each object or region in our scene. Con-\ncretely, an object is approximated by a set of elliptical Gaussians {Gi}n\ni=1, where each Giis defined\nby:\n•µi∈R3: the center position in 3D,\n•Σi∈R3×3: a covariance (or scale/rotation) tensor,\n•ci∈R3: an RGB color vector (or spherical harmonic coefficients for view dependence),\n•αi∈R: an opacity or density parameter.\nThis explicit point-based representation can be rendered in real time via fast rasterization [ 10],\nand it enables direct manipulation of positions, colors, and opacities without requiring expensive\nmesh extraction. Thus, any user-specified animation can dynamically update the attributes Awith\nAi= (µi,ci, αi)on the fly.\n4\n--- Page 5 ---\n“Melting like lava.” \n“Shift through the colors of the rainbow.” \n“An explosion like a powder keg.” \nFigure 3: Qualitative results showcasing the diversity and fidelity of animations generated by\nPromptVFX across different scenes and user prompts. Exact prompts used to generate these anima-\ntions are provided in the supplementary material.\nTo animate a specific object within the scene, we first identify its corresponding Gaussians. Our\nframework remains agnostic to the selection method, enabling seamless integration with auto-\nmated 3D segmentation techniques. Methods such as LangSplat [ 55], Gaussian Grouping [ 56], or\nGARField [ 57] can be employed to localize objects within the scene and associate them with their\nrespective Gaussians. In this work, we assume that the segmentation of the target object is provided,\nwith its corresponding Gaussians pre-selected for animation.\n3.2 Time-Dependent Fields for Animation\nRather than generating new geometry or using a physics simulator, we define animation through a\ncontinuous time-varying fieldfoperating on each Gaussian attribute:\nf:R3×R3×R×[0, T]7→R3×R3×R\nf(Ai, t) = (fµ(µi, t), fc(ci, t), fα(αi, t)) (1)\n= (µi(t),ci(t), αi(t)) =: Ai(t)\nwhere iindicates a particular Gaussian, and t∈[0, T], T∈Rspans the animation’s duration. These\nfields allow structured transformations such as translation, rotation, color transitions, or opacity\nchanges over time.\n3.3 LLM-Based Translation of Text Instructions\nWe now describe how open-domain text instructions are converted into parametric fields that update\neach Gaussian’s center, color, and opacity. Our pipeline comprises multiple stages, each realized by a\nspecialized LLM prompt. If additional renderings of the object are available, we can also involve a\nVLM for more precise attribute grounding. The detailed text instructions to the L(V)LMs that are\nmentioned here schematically can be found in the supplementary material.\n3.3.1 Design Phase\nA user provides an animation description (e.g., “move the vase up for two seconds, then dissolve it\nover one second”). We first request that the LLM break down this abstract instruction into animation\nphases, specifying approximate timings and actions. For instance, it might produce:\n•Phase 1 (0-2s) : “translate vase upward,”\n•Phase 2 (2-3s) : “fade from opaque to transparent.”\nIn doing so, the LLM interprets the user’s high-level goals into a structured set of phases. If images\nof the object are provided, it can tailor the breakdown accordingly (e.g., factoring in bounding-box\ndimensions or shape cues). Simple static text instructions guarantee a structured output.\n5\n--- Page 6 ---\n“Turn the vase into lava.” Ours AutoVFX Gaussians2Life \n“The bear expands and contracts like a lung.” Ours AutoVFX Gaussians2Life \nFigure 4: Qualitative comparison with baselines on different scenes and user prompts. Our\nmethod achieves high-fidelity visual transformations and realistic motion, outperforming AutoVFX\nand Gaussians2Life. Exact prompts and additional qualitative comparisons are provided in the\nsupplementary material.\n3.3.2 Field Generation Phase\nThe output from the desing phase is now translated by LLM calls into µi(t),ci(t), αi(t)(cf. Eq. 1),\nstill considering user-specified durations and transformations. For example, the geometry function\nmight linearly interpolate the vase’s z-coordinate from z0toz0+ 2overt∈[0,2], while the opacity\nfunction steadily decreases αfrom 1 to 0 during t∈[2,3].\n3.3.3 Animation Hypotheses Generation & Scoring\nSome textual instructions might be ambiguous (e.g., “make the vase shake randomly”). For those, we\ncan generate mvariations f1, f2, . . . , f mof these parametric functions\nfµ\nj(µi, t), fc\nj(ci, t), fα\nj(αi, t), j∈ {1,2, . . . , m }. (2)\nEach variation applies slightly different motion curves or color transitions. We then render each\ncandidate animation (producing, for instance, a short sequence of frames) and feed these frames,\nalong with the original text prompt, to a VLM that scores how well each animation matches the\nintended description on a 0–100scale. We select the highest-scoring candidate as our base animation.\nWe also include two optional refinement processes to improve the output.\n3.3.4 Automatic Refinement\nTo further refine the chosen base animation, we prompt a specialized VLM that compares the\nuser’s description and the rendered frames. It identifies discrepancies (e.g., “the vase does not rise\nhigh enough”) and adjusts our field functions µi(t),ci(t), αi(t)to more closely match the user’s\ninstructions.\n6\n--- Page 7 ---\n3.3.5 Conversational Refinement\nIf a user observes misinterpretations of the intended animation, open-text feedback can be provided\n(e.g., “spin faster in the first second, fade more quickly in the final half-second”). We combine this\nfeedback with the existing field Aiand animation frames, asking the VLM to produce another iteration\nof refined functions. This conversational loop can continue until the user is satisfied. This enables\nrapid iteration cycles with a natural text-driven interface due to iteration without computational\noverhead from diffusion models or physics-based simulations.\nTable 2: CLIP and VQAScore scores for each method on different prompts.\nOurs Gaussians2Life [5] AutoVFX [8]\nPrompt CLIP [58] VQAScore [59] CLIP [58] VQAScore [59] CLIP [58] VQAScore [59]\nTurn the vase into lava. 0.1712 0.7150 0.2058 0.1962 0.1587 0.4437\nThe bear expands and contracts like a lung. 0.2044 0.2833 0.2381 0.2583 0.2393 0.2832\nThe bulldozer accelerates forward. 0.2272 0.6845 0.3084 0.6813 0.1717 0.4512\n4 Experiments\n4.1 Experimental Details\nDataset & Preprocessing. We evaluate our method on real-world scenes from Mip-NeRF360 [ 60]\n(thegarden vase andbulldozer ),bear scene from Instruct-NeRF2NeRF [ 9] and an additional horse\nscene from Tanks and Temples [ 61]. Each scene is reconstructed via a Gaussian splatting pipeline,\nproviding a set of Gaussians encoding geometry and color. When animating, user instructions (e.g.,\n“the vase”) are mapped to corresponding Gaussians.\nBaselines. We compare against Gaussians2Life [ 5], which uses video diffusion to synthesize 2D\nmotion lifted to 3D Gaussians via optimization, and AutoVFX [ 8], which generates physically\nsimulated animations through Blender scripts, requiring mesh extraction and offline rendering. Both\npipelines depend on heavy preprocessing and lack interactivity. In contrast, our method applies\nfunctional transformations directly to Gaussians, enabling real-time animation without diffusion or\nphysics simulation.\nImplementation Notes. Our method is implemented in Python and leverages GPT-4 [ 62] to parse\ntext prompts and generate animation functions. All experiments are conducted on a single NVIDIA\nRTX 4090 GPU.\n4.2 Qualitative Evaluation\nWe demonstrate the capabilities of PromptVFX across a variety of scenes and user instructions.\nFigure 3 shows that our system can generate animations involving motion, color changes, and opacity\nvariations in response to natural language prompts. Each animation is produced by updating Gaussian\nparameters such as position and appearance in real time, enabling interactive feedback and rapid\niteration during the creative process.\nComparison with Baselines. Figure 4 provides a visual comparison between PromptVFX and two\nrecent methods, Gaussians2Life [ 5] and AutoVFX [ 8]. Unlike these approaches, which rely on\ndiffusion models or physics simulations and require offline processing, our framework applies direct\nfunctional transformations to the Gaussians. This design results in more responsive editing, better\ntemporal coherence, and consistent appearance transformations across views. PromptVFX delivers\nhigh-quality animations that align with user prompts while maintaining the speed and flexibility\nrequired for interactive content creation.\n4.3 Quantitative Evaluation\nMetrics. Evaluating 3D animation quality is difficult due to the absence of ground-truth sequences.\nFollowing prior work [ 20,8,5], we report CLIP similarity [ 58] as a frame-level proxy for text-to-\nimage alignment. However, CLIP does not capture motion or temporal coherence. We therefore also\nreport VQAScore, a video-level metric shown to better align with human judgment [ 59]. It estimates\nthe probability that a model answers “Yes” to the prompt: Does this video align with the described\nanimation: “{prompt}”? , denoted as P(“Yes”|video, prompt ).\n7\n--- Page 8 ---\n“The bulldozer accelerates forward.” Vision ✖ Vision ✔Figure 5: Impact of VLM feedback on animation accuracy. Without visual feedback, the bulldozer\nincorrectly accelerates off the table, failing to account for scene constraints. With VLM refinement,\nthe motion is corrected to remain contextually appropriate.\n“A ‘breathing animation’, where the object expands and contracts like a lung.”\n“The bear should expand significantly further.”\nFigure 6: Animation refinement is demonstrated through iterative user interaction. After generating\nan initial animation, the system enables users to provide follow-up prompts for further adjustments,\nenhancing control and precision in the final animation.\nResults. Table 2 shows that PromptVFX achieves the highest VQAScore [ 59] across all evaluated\nprompts, demonstrating better text-video alignment from a temporal and semantic perspective. While\nGaussians2Life [ 5] ranks higher in CLIP [ 58] for some cases due to preserving frame-level appearance,\nit does not always reflect coherent or realistic motion.\n4.4 Ablation Studies\nWe conduct several ablation studies to evaluate the impact of different components in our pipeline.\n4.4.1 Effect of Hypothesis Sampling\nWe analyze how the number of generated animation hypotheses influences performance. Figure 7\nshows that increasing the number of candidate animations improves VLM scores. The VLM score\nis obtained by querying a vision-language model to evaluate how well snapshots of the animation\nfrom different viewpoints match the original text prompt, providing a more holistic assessment\nof animation coherence. However, we observe diminishing returns beyond a certain number of\nhypotheses, indicating that an optimal balance exists between diversity and computational efficiency.\n4.4.2 Impact of Vision-Language Model Feedback\nTo assess the role of VLM-based feedback, we compare animations generated with and without visual\nrefinement. As shown in Figure 5, without visual feedback, the bulldozer accelerates off the table,\nfailing to respect scene constraints. When rendered frames are provided to the VLM for refinement,\nthe animation is corrected to maintain a physically plausible motion. This highlights the importance\nof incorporating visual guidance in aligning animations with scene context.\n4.4.3 User-Guided Animation Refinement\nWe examine the benefits of allowing iterative user feedback in refining animations. In Figure 6 an\ninitial animation is generated based on the text prompt, after which the user provides a follow-up\n8\n--- Page 9 ---\n1 2 3 4 5 6 7 810 12 15 20 24 300.60.70.8VLMAverage Maximum Score\nGroup SizeFigure 7: Best Scores for various amount of hypotheses. Impact of the number of hypotheses on\nVLM scores. As the group size increases, VLM scores improve.\n“The bulldozer accelerates forward.” LLaV A 4o mini \nFigure 8: VLM comparison for animation quality effect with different vision-language models\n(VLMs) that interpret the prompt. Given the same input text, GPT-4o-mini produces a coherent\nbulldozer acceleration, while LLaV A struggles to maintain object consistency and motion realism.\nadjustment. The refined animation better captures the intended motion, demonstrating how interactive\nrefinements improve animation precision. This experiment underscores the importance of keeping\nthe user in the loop for fine-grained control over the animation generation process.\n4.4.4 Effect of VLM Choice on Animation Quality\nWe compare different vision-language models used in our pipeline. Figure 8 illustrates that GPT-4o-\nmini produces a fluent bulldozer acceleration, while LLaV A [ 63,64] struggles with maintaining object\nconsistency and motion realism. This suggests that model selection significantly affects animation\nquality and highlights the need for well-designed system prompts tailored to the VLM’s capabilities.\n5 Limitations\nWhile PromptVFX delivers fast, interactive text-driven volumetric animations, it has several con-\nstraints. It cannot support collisions or contact interactions. It can only animate existing Gaussians,\nso particle effects such as smoke or fire cannot be generated. Without semantic scene understanding,\nrelational prompts such as “place the vase under the table” may not be interpreted correctly. In future\nwork, we plan to integrate physics priors, enable dynamic splat insertion for particle effects, and\nincorporate semantic object labels to address these limitations.\n6 Conclusion\nThis paper introduces PromptVFX, a framework that reformulates 3D animation as a field prediction\ntask by applying time-varying transformations directly to Gaussian splats. Unlike diffusion-based or\nphysics-driven pipelines, our method enables real-time, text-driven animation through parametric\nupdates generated by large language and vision-language models. The approach reduces manual\neffort, supports fast iteration, and produces high-fidelity, temporally consistent results. By lowering\nthe barriers to 3D animation, this work takes a step toward democratizing open-ended visual effects\ncreation.\n9\n--- Page 10 ---\nReferences\n[1]Blender Online Community. Blender - a 3D modelling and rendering package . Blender Foundation,\nStichting Blender Foundation, Amsterdam, 2019. URL http://www.blender.org .\n[2]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 10684–10695, 2022.\n[3]Zifan Shi, Sida Peng, Yinghao Xu, Andreas Geiger, Yiyi Liao, and Yujun Shen. Deep generative models\non 3d representations: A survey. arXiv preprint arXiv:2210.15663 , 2022.\n[4]Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988 , 2022.\n[5]Thomas Wimmer, Michael Oechsle, Michael Niemeyer, and Federico Tombari. Gaussians-to-life: Text-\ndriven animation of 3d gaussian splatting scenes. In 2025 International Conference on 3D Vision (3DV) ,\n2025.\n[6]Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong\nZeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model. arXiv\npreprint arXiv:2310.15110 , 2023.\n[7]Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency\nfor multi-view images diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 7079–7088, 2024.\n[8]Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, and Shenlong Wang. Autovfx: Physically realistic\nvideo editing from natural language instructions. arXiv preprint arXiv:2411.02394 , 2024.\n[9]Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-\nnerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2023.\n[10] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for\nreal-time radiance field rendering. ACM Trans. Graph. , 42(4):139–1, 2023.\n[11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without\ntraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n14953–14962, 2023.\n[12] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Fresco: Spatial-temporal correspondence for\nzero-shot video translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8703–8712, 2024.\n[13] Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, and Shenlong Wang. Climatenerf: Extreme weather\nsynthesis in neural radiance field. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 3227–3238, 2023.\n[14] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei\nYang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian\nsplatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n21476–21485, 2024.\n[15] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in\n3d scenes. In European Conference on Computer Vision , pages 162–179. Springer, 2024.\n[16] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian:\nPhysics-integrated 3d gaussians for generative dynamics. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 4389–4398, 2024.\n[17] Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng\nGao, Yin Yang, et al. Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual\nreality. In ACM SIGGRAPH 2024 Conference Papers , pages 1–1, 2024.\n[18] Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi\nWu, Kun Zhou, Chenfanfu Jiang, and Yin Yang. Gaussian splashing: Unified particles for versatile motion\nsynthesis and rendering. arXiv preprint arXiv:2401.15318 , 2024.\n10\n--- Page 11 ---\n[19] Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, and Ming C Lin. Dynamic mesh-aware\nradiance fields. In Proceedings of the IEEE/CVF international conference on computer vision , pages\n385–396, 2023.\n[20] Minghao Chen, Iro Laina, and Andrea Vedaldi. Dge: Direct gaussian 3d editing by consistent multi-view\nediting. In European Conference on Computer Vision , pages 74–92. Springer, 2024.\n[21] Shuangkang Fang, Yufeng Wang, Yi-Hsuan Tsai, Yi Yang, Wenrui Ding, Shuchang Zhou, and Ming-Hsuan\nYang. Chat-edit-3d: Interactive 3d scene editing via text prompts. In European Conference on Computer\nVision , pages 199–216. Springer, 2024.\n[22] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d:\nGenerative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 , 2023.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems , 33:6840–6851, 2020.\n[24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The\nEleventh International Conference on Learning Representations , 2023.\n[25] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,\nYam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video\ndiffusion models to large datasets. CoRR , 2023.\n[26] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 22563–22575,\n2023.\n[27] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael\nVasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large\nvideo diffusion transformers for 3d camera control. CoRR , 2024.\n[28] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl:\nEnabling camera control for text-to-video generation. CoRR , 2024.\n[29] Yiming Xie, Chun-Han Yao, Vikram V oleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content\ngeneration with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470 , 2024.\n[30] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas J Guibas, and Gordon\nWetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. In\nA. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Ad-\nvances in Neural Information Processing Systems , volume 37, pages 16240–16271. Curran Associates,\nInc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\n1d49235669869ab737c1da9d64b7c769-Paper-Conference.pdf .\n[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM , 65(1):99–106,\nDecember 2021. ISSN 0001-0782. doi: 10.1145/3503250. URL https://doi.org/10.1145/\n3503250 .\n[32] Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, and Jan Eric Lenssen. Neural parametric\ngaussians for monocular non-rigid object reconstruction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages 10715–10725, June 2024.\n[33] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic\n3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 21136–21145, June 2024.\n[34] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking\nby persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV) , pages\n800–809, 2024. doi: 10.1109/3DV62453.2024.00044.\n[35] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and\nXinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 20310–20320, June\n2024.\n11\n--- Page 12 ---\n[36] HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab,\nand Benjamin Busam. Deformable 3d gaussian splatting for animatable human avatars, 2023. URL\nhttps://arxiv.org/abs/2312.15059 .\n[37] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable\navatars via deformable 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 5020–5030, June 2024.\n[38] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka,\nSergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy: Text-to-4d generation\nusing hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 7996–8006, 2024.\n[39] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv\npreprint arXiv:2301.11280 , 2023.\n[40] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians:\nText-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 8576–8588, June 2024.\n[41] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, László Jeni,\nSergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion\nmodels. Advances in Neural Information Processing Systems , 37:45256–45280, 2025.\n[42] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\npages 18392–18402, 2023.\n[43] Cyrus Vachha and Ayaan Haque. Instruct-gs2gs: Editing 3d gaussian splats with instructions, 2024. URL\nhttps://instruct-gs2gs.github.io/ .\n[44] Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, and Yanfeng Wang.\nEditable scene simulation for autonomous driving via collaborative llm-agents. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15077–15087, 2024.\n[45] Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, and Shenlong Wang. Video2game: Real-time interactive realistic\nand browser-compatible environment from a single video. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages 4578–4588, June 2024.\n[46] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y . Feng, Changxi Zheng, Noah Snavely, Jiajun Wu,\nand William T. Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation.\nIn Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors,\nComputer Vision – ECCV 2024 , pages 388–406, Cham, 2025. Springer Nature Switzerland. ISBN\n978-3-031-72627-9.\n[47] Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yunzhu Li. Reconstruction and simulation of elastic\nobjects with spring-mass 3d gaussians. European Conference on Computer Vision (ECCV) , 2024.\n[48] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda Zhang, Zhaopeng Cui, and Guofeng Zhang.\nNeumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In Shai\nAvidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, Computer\nVision – ECCV 2022 , pages 597–614, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19787-1.\n[49] Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng, Shanyan Guan, Bowen Pan, Guangtao Zhai,\nand Xiaokang Yang. Cagenerf: Cage-based neural radiance field for generalized 3d deformation and\nanimation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad-\nvances in Neural Information Processing Systems , volume 35, pages 31402–31415. Curran Associates,\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\ncb78e6b5246b03e0b82b4acc8b11cc21-Paper-Conference.pdf .\n[50] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In Shai Avidan, Gabriel Brostow,\nMoustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, Computer Vision – ECCV 2022 ,\npages 159–175, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19827-4.\n[51] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: Geometry\nediting of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 18353–18364, June 2022.\n12\n--- Page 13 ---\n[52] Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Language-driven physics-based scene synthesis\nand editing via feature splatting. In European Conference on Computer Vision , pages 368–383. Springer,\n2024.\n[53] Junjie Wang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. Gaussianeditor: Editing 3d gaussians\ndelicately with text instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 20902–20911, June 2024.\n[54] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating\nany 3d model with multi-view video diffusion. Advances in Neural Information Processing Systems , 37:\n125879–125906, 2025.\n[55] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaus-\nsian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 20051–20060, 2024.\n[56] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in\n3d scenes. In ECCV , 2024.\n[57] Chung Min* Kim, Mingxuan* Wu, Justin* Kerr, Matthew Tancik, Ken Goldberg, and Angjoo Kanazawa.\nGarfield: Group anything with radiance fields. In Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2024.\n[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning , pages 8748–8763. PmLR,\n2021.\n[59] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva\nRamanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on\nComputer Vision , pages 366–384. Springer, 2024.\n[60] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360:\nUnbounded anti-aliased neural radiance fields. CVPR , 2022.\n[61] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking\nlarge-scale scene reconstruction. ACM Transactions on Graphics , 36(4), 2017.\n[62] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 , 2023.\n[63] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[64] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next:\nImproved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.\nio/blog/2024-01-30-llava-next/ .\n13\n--- Page 14 ---\nA Ethical Considerations: Deep Fakes\nAs with many generative AI methods, PromptVFX introduces ethical concerns regarding the potential\nmisuse of synthetic media for deceptive purposes. While our approach is intended to empower creativ-\nity and streamline 3D animation workflows, we acknowledge the broader implications associated with\nAI-generated content, especially where distinguishing between real and synthetic media is crucial.\nWe emphasize the importance of responsible usage and advocate for generative animation as a means\nto support artistic expression and accessibility rather than manipulation or misinformation.\nB Implementation Details\nIn this section, we provide detailed descriptions of the prompt templates, specific animation prompts\nused in our experiments to support the reproducibility and transparency of our results.\nB.1 Prompt design\nWe present the system prompts used in our pipeline for interacting with the L(V)LM to generate\nanimations. Each prompt is designed to guide different stages of the animation process, from parsing\nabstract user instructions to generating and refining animation functions. Figure 9 to Figure 22\nillustrate the various system prompts employed across different modules of our framework.\nFigure 9: System Message for breaking down an abstract animation description into concrete\nanimation phases.\nB.2 Prompts Used in the Figures\nBelow, we list the exact textual prompts used to generate animations presented in our experiments.\nThese inputs directly guide animation generation for our method and the baselines.\n\"Turn the vase into lava.\", \"Melting like lava.\"\n•Ours: The animation can be broadly described as \"Melting like lava\". More detailed the\ncenters slowly slide downwards like a thick liquid, but starting from the outer shell of the\n14\n--- Page 15 ---\nFigure 10: System Message for deriving the behavior of the Gaussian’s center positions during the\nanimation phases.\n15\n--- Page 16 ---\nFigure 11: System Message for deriving the behavior of the Gaussian’s colors during the animation\nphases.\n16\n--- Page 17 ---\nFigure 12: System Message for deriving the behavior of the Gaussian’s opacity values during the\nanimation phases.\n17\n--- Page 18 ---\nFigure 13: System message for generating Gaussian center position transformations.\nobject towards also the core liquidating. Meanwhile, the colors change slowly to lava colors\nand opacities can stay unchanged. IMPORTANT: The Z coordinate for all center does not\nend up lower than the lowest Z of the original centers. This will make it seem as if the object\nis standing on the ground and the lava just lays on the ground at the end.\n•Baselines (Gaussians2Life [ 5] & AutoVFX [ 8]):The animation can be broadly described\nas melting the vase with flowers like lava. More detailed the centers slowly slide downwards\nlike a thick liquid, but starting from the outer shell of the object towards also the core\nliquidating. Meanwhile, the colors change slowly to lava colors and opacities can stay\nunchanged. IMPORTANT: The Z coordinate for all center does not end up lower than the\nlowest Z of the original centers. This will make it seem as if the object is standing on the\nground and the lava just lays on the ground at the end.\n\"The bulldozer accelerates forward.\"\n•Ours & Gaussians2Life [5]: The bulldozer accelerates forward.\n•AutoVFX [ 8]:The baseline failed to generate a valid function with the initial prompt. To\nwork around this limitation, we attempted two alternative prompt variations:\n–The bulldozer accelerates forward. Do not put numpy arrays into animation requests.\n–The bulldozer accelerates forward to (0, 1, 0).\nHowever, neither of these resulted in a successful animation.\n\"Shift through the colors of the rainbow\"\n18\n--- Page 19 ---\nFigure 14: System message for generating Gaussian color transformations.\n•All Methods: The object stays stationary, but the colors shift through all colors of the\nrainbow. At the end, the object should transition back to its original colors to create a loop\nanimation.\n\"An explosion like a powder keg\"\n• textbfAll Methods: An explosion like a powder keg.\n\"The bear expands and contracts like a lung.\"\n•Ours: A breathing animation, where the object expands and contracts like a lung.\n•Baselines (Gaussians2Life [ 5] & AutoVFX [ 8]):A breathing animation where the bear\nexpands and contracts like a lung.\nC Additional Qualitative Results\nWe provide additional visual comparisons against baseline methods in Figure 25. Specifically, we\ndemonstrate our method’s superior alignment to the animation prompts, particularly highlighting its\ncapability in accurate object displacement, compared to AutoVFX [8] and Gaussians2Life [5].\n19\n--- Page 20 ---\nFigure 15: System message for generating Gaussian opacity transformations.\n20\n--- Page 21 ---\nFigure 16: System message for automatically improving Gaussian center position transformations.\n21\n--- Page 22 ---\nFigure 17: System message for automatically improving Gaussian color transformations.\n22\n--- Page 23 ---\nFigure 18: System message for automatically improving Gaussian opacity transformations.\n23\n--- Page 24 ---\nFigure 19: System message for refining Gaussian center transformations based on user feedback.\n24\n--- Page 25 ---\nFigure 20: System message for refining Gaussian color transformations based on user feedback.\n25\n--- Page 26 ---\nFigure 21: System message for refining Gaussian opacity transformations based on user feedback.\n26\n--- Page 27 ---\nFigure 22: System message for evaluating the alignment of animations to their textual descriptions.\n27\n--- Page 28 ---\nFigure 23: User message template for automatically improving Gaussian transformations.\nFigure 24: User message template for refining Gaussian transformations based on user feedback.\n“The bulldozer accelerates forward. ” Ours AutoVFX Gaussians2Life \nFigure 25: Qualitative comparison with baselines on the lego bulldozer scene. Solely our method is\nable to arrive at an animation that fits the animation description. Furthermore it showcases impressive\ncapabilities in object displacement tasks.\n28",
  "text_length": 50838
}