{
  "id": "http://arxiv.org/abs/2506.01077v1",
  "title": "TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal\n  Interaction in Digital Humans",
  "summary": "Large Language Model (LLM)-driven digital humans have sparked a series of\nrecent studies on co-speech gesture generation systems. However, existing\napproaches struggle with real-time synthesis and long-text comprehension. This\npaper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel\nmulti-modal framework for real-time 3D gesture generation. Our method\nincorporates three modules: 1) a cross-modal attention mechanism to achieve\nprecise temporal alignment between speech and gestures; 2) a long-context\nautoregressive model with a sliding window mechanism for effective sequence\nmodeling; 3) a large-scale gesture matching system that constructs an atomic\naction library and enables real-time retrieval. Additionally, we develop a\nlightweight pipeline implemented in the Unreal Engine for experimentation. Our\napproach achieves real-time inference at 120 fps and maintains a per-sentence\nlatency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive\nsubjective and objective evaluations on the ZEGGS, and BEAT datasets\ndemonstrate that our model outperforms current state-of-the-art methods. TRiMM\nenhances the speed of co-speech gesture generation while ensuring gesture\nquality, enabling LLM-driven digital humans to respond to speech in real time\nand synthesize corresponding gestures. Our code is available at\nhttps://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching",
  "authors": [
    "Yueqian Guo",
    "Tianzhao Li",
    "Xin Lyu",
    "Jiehaolin Chen",
    "Zhaohan Wang",
    "Sirui Xiao",
    "Yurun Chen",
    "Yezi He",
    "Helin Li",
    "Fan Zhang"
  ],
  "published": "2025-06-01T16:27:24Z",
  "updated": "2025-06-01T16:27:24Z",
  "categories": [
    "cs.GR",
    "cs.HC",
    "68U05(Primary), 62M45(Secondary)"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01077v1",
  "full_text": "--- Page 1 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time\nmulti-modal Interaction in Digital Humans\nYUEQIAN GUO, Jiangxi University of Finance and Economics, China\nTIANZHAO LI, Communication University of China, China\nXIN LYU, Communication University of China, China\nJIEHAOLIN CHEN, Communication University of China, China\nZHAOHAN WANG, Communication University of China, China\nSIRUI XIAO, Communication University of China, China\nYURUN CHEN, Communication University of China, China\nYEZI HE, Communication University of China, China\nHELIN LI, Communication University of China, China\nFAN ZHANGâˆ—,Communication University of Zhejiang, China\nLarge Language Model (LLM)-driven digital humans have sparked a series of recent studies on co-speech gesture generation\nsystems. However, existing approaches struggle with real-time synthesis and long-text comprehension. This paper introduces\nTransformer-Based Rich Motion Matching (TRiMM), a novel multi-modal framework for real-time 3D gesture generation. Our\nmethod incorporates three modules: 1) a cross-modal attention mechanism to achieve precise temporal alignment between\nspeech and gestures; 2) a long-context autoregressive model with a sliding window mechanism for effective sequence modeling;\n3) a large-scale gesture matching system that constructs an atomic action library and enables real-time retrieval. Additionally,\nwe develop a lightweight pipeline implemented in the Unreal Engine for experimentation. Our approach achieves real-time\ninference at 120 fps and maintains a per-sentence latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060).\nExtensive subjective and objective evaluations on the ZEGGS, and BEAT datasets demonstrate that our model outperforms\ncurrent state-of-the-art methods. TRiMM enhances the speed of co-speech gesture generation while ensuring gesture quality,\nenabling LLM-driven digital humans to respond to speech in real time and synthesize corresponding gestures. Our code is\navailable at https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching\nAdditional Key Words and Phrases: Transformer-Based Models, Multi-modal Fusion, Large motion graph, Real-Time Motion\nGeneration, Digital Humans\nâˆ—corresponding author\nAuthorsâ€™ addresses: Yueqian Guo, xiaoheguo@outlook.com, Jiangxi University of Finance and Economics, China, Nanchang; Tianzhao Li,\nCUCLelivre@gmail.com, Communication University of China, China, Beijing; Xin Lyu, lvxinlx@cuc.edu.cn, Communication University\nof China, China, Beijing; Jiehaolin Chen, 1202390090@jxufe.edu.cn, Communication University of China, China, Beijing; Zhaohan Wang,\n1202390090@jxufe.edu.cn, Communication University of China, China, Beijing; Sirui Xiao, nightcruising79@gmail.com, Communication\nUniversity of China, China, Beijing; Yurun Chen, 2575738708@qq.com, Communication University of China, China, Beijing; Yezi He,\nyates00619@gmail.com, Communication University of China, China, Beijing; Helin Li, kingselyee67@sina.cn, Communication University of\nChina, China, Beijing; Fan Zhang, Fanzhang@cuz.edu.cn, Communication University of Zhejiang, China, Beijing.\n. XXXX-XXXX//6-ART $15.00\nhttps://doi.org/\n, Vol. 1, No. 1, Article . Publication date: June .arXiv:2506.01077v1  [cs.GR]  1 Jun 2025\n--- Page 2 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\n1 INTRODUCTION\nFig. 1. The system trains a Transformer model using text, speech, and motion feature data, performs real-time multi-modal\ninference with text and speech to predict motion features, synthesizes actions in real-time from a large motion library via\nMotion Matching, and renders the results through Unreal Engine using LiveLink.\nRecent advances in Human-computer interaction have demonstrated the potential of large language models\n(LLMs) to drive digital human behaviors[Wan et al .2024][Sonlu et al .2024][Wu et al .2025]. Therefore, many\nmethods for generating digital human actions have emerged recently. While current methods excel at text-based\ndialogue generation, the integration of real-time motion synthesis remains a critical challenge for immersive\napplications like virtual live-streaming and interactive gaming[Chen et al .2021][Tang et al .2022][Xiao et al .\n2025] Traditional approaches for digital human animation rely on two distinct paradigms: motion capture-based\nsystems and deep generative models. Early motion matching techniques [Clavet 2016] leveraged pre-recorded\nmotion libraries to ensure natural movements, but suffered from limited action diversity due to combinatorial\nconstraints. Recent generative approaches using VAEs[Petrovich et al .2021][Yao et al .2022][Petrovich et al .\n2022]and diffusion models [Zhang et al .2024a][Zhang et al .2023][Karunratanakul et al .2023] have improved\nmotion variety, yet their prohibitive computational demands hinder real-time deployment MDM [Tevet et al .\n2022]requires 12s to produce a motion sequence given a textual description, making it impractical for interactive\napplications.\nThis paper introduces Transformer-Based Rich Motion Matching (TRiMM) that addresses three fundamental\nlimitations in existing methods: (1) Temporal misalignment between speech prosody and gesture dynamics\nin cross-modal synthesis; (2) Restricted motion diversity caused by limited action libraries and rigid retrieval\nmechanisms; (3) Computational overhead from iterative denoising processes in diffusion-based approaches.\nOur key innovations include:\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 3 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nâ€¢A multi-modal attention mechanism that dynamically aligns speech and text features with gesture kine-\nmatics. it is implemented through space-time attention transformer, achieving precise synchronization\nbetween speech prosody and gesture dynamics.\nâ€¢A sliding-window autoregressive model that maintains 8 sentencesâ€™ contextual memory. it also achieves\n0.159s inference latency, addressing the computational overhead problem in diffusion-based approaches.\nOur method enables real-time motion synthesis at 120 fps on RTX3060.\nâ€¢A hierarchical motion graph containing 9,143 atomic actions with multi-criteria similarity search (semantic,\nkinematic, temporal). it overcomes the limited action diversity in traditional motion capture systems,and\nraises motion diversity by 2 times.\n2 RELATED WORK\n2.1 Generative Methods\nThe main methods, contributions, and limitations of the current diffusion model in the field of action generation\nare as follows: motiondiffusion, as a text-driven action generation framework based on the diffusion model,\nrealizes diversified fine-grained action synthesis through probabilistic mapping and hierarchical operation. Its\ncore contribution is to introduce the de-noising process instead of deterministic mapping, which breaks through\nthe limitation of action diversity of traditional retrieval methods. Experiments show that its generation quality\nexceeds the same period method; However, its multi-step iterative generation mechanism leads to significant\nreasoning delay, which can not meet the needs of real-time interaction. MDM [Tevet et al .2022] constructs motion\nlatent space through a two-stage training strategy, and has made progress in improving the generation efficiency.\nHowever, the non-end-to-end training mode leads to incomplete motion distribution modeling, and it is difficult\nto accurately capture complex motion patterns. MLD [Chen et al .2023]uses the latent space diffusion strategy\nto reduce the computational complex motion sequence compression through hierarchical feature extraction.\nHowever, its two-stage training architecture has the problem of insufficient feature decoupling, resulting in the\nloss of motion details. [Zhang et al .2023]innovatively integrates the retrieval strategy and diffusion model to\nenhance the semantic consistency of generated actions through semantic similarity matching. However, the\nintroduction of retrieval mechanism increases the computational overhead and fails to fundamentally solve the\nreal-time bottleneck. [Zhou et al .2025] accelerates the generation process by improving the sampling algorithm,\nbut its optimization direction focuses on the algorithm level, and lacks the explicit modeling of physical constraints\n(such as foot contact dynamics), which may lead to the generation action violating the biomechanical laws.\nIt is worth noting that although transformer architecture shows potential in time series modeling, the existing\nmethods have not fully exploited the advantages of the autoregressive mechanism, which enhances the modelâ€™s\ncomprehension of long-text dialogues in LLM-driven conversations, enabling coherent gesture generation across\nextended interactions. Most generative models suffer from high computational complexity, making it challenging\nto achieve real-time inference above 30 fps. Our TRiMM framework addresses this limitation by combining sliding\nwindow autoregressive generation with a lightweight deployment pipeline, enabling real-time motion synthesis\nat 120 fps with a per-sentence latency of only 0.13 seconds on consumer-grade GPUs.\n2.2 Retrieval-based Methods\nThe retrieval-based motion generation method continues to evolve in the field of animation and gesture synthesis.\nMotion Matching [Clavet 2016] aims to retrieve the optimal next frame animation from the pre-recorded animation\nlibrary. This technology replaces the manual state setting of traditional state machines in the motion capture\ndatabase animation. However, it has two major limitations: memory usage increases linearly with the amount of\nanimation data, and the fixed feature weights lead to the need for manual adjustment of the matching strategy\nin complex interaction scenarios. [Holden et al .2020] divides the traditional motion-match algorithm into\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 4 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nthree stages: projection, stepping, and decompression, which are replaced by neural networks, respectively,\nsignificantly reducing the memory dependence and improving the scalability. However, the physical rationality\nof the generated action still depends on the quality of the original action in the database, which is difficult to deal\nwith the no motion mode.[Habibie et al .2022] introduces audio pose joint similarity retrieval in speech-driven\ngesture synthesis, and optimizes the retrieval results in combination with confrontation generation network.\nIts contribution is to improve the synchronization and naturalness of motion and voice rhythm; However, this\nmethod is still based on keyword-level semantic matching and lacks a deep understanding of context semantics,\nresulting in a rough association between gesture and semantics in complex conversation scenes. [Yang et al .\n2023a] quantized the discrete gesture units through the gesture vq-vae module, aligned the speech and gesture\nsequences using Levenshtein Distance, and introduced phase guidance to optimize the motion matching, which\neffectively alleviated the problem of random motion jitter and phoneme asynchrony; However, its two-stage\nquantization matching process introduces additional computational delay, and relies on a fixed size of pre training\ncodebook, which is difficult to dynamically expand diversified actions. Although the current retrieval methods\nhave advantages in real-time and physical rationality, they are still limited by a single-level index structure (such\nas pure motion features or phoneme matching), resulting in coarse semantic granularity and insufficient accuracy\nof cross-modal alignment.\nIn this regard, our proposed hierarchical action retrieval engine uses a semantic action phoneme three-level\nindex architecture to simultaneously model language context, action coherence and phoneme timing constraints\nin real-time retrieval, support the dynamic combination of 9143 atomic actions, and break through the bottleneck\nof the capacity and alignment accuracy of the traditional retrieval library.\n2.3 multi-modal Fusion Approaches\nIn the latest progress in the field of multi-modal driven action generation,[Wang et al .2024] fused multi-modal\nsignals such as voice, emotion, and identity through a diffusion model, proposed a progressive intermediate\nfusion strategy and a mask style matrix to achieve fine-grained voice action alignment. Its core contribution\nis to use the geometric loss function to constrain the continuity of joint velocity and acceleration to generate\na high smooth motion sequence; However, its multi-modal interaction relies on the static intermediate fusion\nmechanism, and the dynamic context (such as the semantic evolution in the dialogue History) is not fully modeled,\nresulting in the rigid switching of action style in the long-term generation. [Zhang et al .2024b] Integrates the\nbody part perception modeling into the diffusion transformer backbone network through the unified dataset\nmotionverse and artattention attention mechanism, supports the joint control of multi-modal inputs such as\ntext and music, and breaks through the generalization limitation of single task model; However, its pre training\nstrategy relies on fixed frame rate and mask mode, which is difficult to adapt to the asynchronous timing changes\nof multi-modal signals in real-time interactive scenes. Based on the coarse-to-fine training strategy and MC Attn\nparallel human body topology modeling, [Bian et al .2025] realizes the zoning control of whole body movements\n(such as the decoupling of fine hand movements and trunk posture). Its contribution is to enhance the generation\ncontrollability through text semantic pre-training and low-level control adaptation; However, its phased training\nframework leads to the response delay of dynamic control signals (such as real-time user instructions), and\nmulti-position independent control may destroy the physical consistency of the kinematic chain. Although the\nexisting methods have made progress in the breadth of multi-modal fusion, the temporal modeling of dynamic\ncontext (such as real-time bullet screen semantics in virtual live broadcast) is still limited to window-level splicing,\nlacking the dynamic weight allocation mechanism of cross-modal features.\nIn this regard, we propose a multi-modal context fusion transformer, which dynamically aligns language\nsemantics (such as real-time dialog text) and motion features through cross modal attention, and realizes the\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 5 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\njoint reasoning of voice rhythm, text emotion and user intention at the transformer level, effectively solving the\nproblems of semantic gesture dislocation and style discontinuity in the generation of virtual humanoid actions.\n3 METHOD\nThe Transformer-Based Rich Motion Matching (TRiMM) system is composed of five core modules. 1) The\nmulti-modal feature extractor leverages WAVE2vec2 and Bert to extract audio and text features. 2) The feature\nencoder-decoder fuses these features via a gated mechanism and enables autoregressive prediction with a sliding\nwindow. 3) The Time-Space attention transformer captures space-time relationships using positional encoding and\nattention mechanisms. 4) The Real-Time Motion Matching Engine constructs a K-NN graph for action retrieval.\n5) the motion hybrid system uses quaternion-based interpolation for smooth motion transitions. Together, these\ncomponents enable TRiMM to process multi-modal data effectively and generate accurate motion predictions.\nFig. 2. This system processes text and audio inputs through dedicated embedding layers, combines them via a fusion\ngate, and adds positional encoding to capture timing. The fused features then pass through a divided space-time attention\nmechanism and a 6-layer Transformer encoder stack (with layer normalization, self-attention, and feed-forward networks) to\nmodel space-time relationships. The final time step is selected and mapped by a fully connected output layer to predict the\nnext-frame action feature.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 6 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\n3.1 Multi-modal information feature extractor\nIn view of the multi-modal characteristics and autoregressive properties[Katharopoulos et al .2020] of this study,\nthis study uses [Schneider et al .2019] and [Devlin et al .2019] models to extract the feature vectors of audio and\ntext respectively, and uses the sliding window mechanism to input serialized data to the transformer model, so as\nto learn the context information existing in the text and audio sequence.\nIn this study, WAVE2 vec2 base 960h model is used to extract audio features. It can extract a two-dimensional\nmatrix with timing spectrum characteristics from the original audio. To unify the feature dimensions, we use\nprincipal component analysis (PCA)[Greenacre et al .2022] to reduce the dimension of the feature matrix to a\nfixed 2048 dimensional vector to form a standardized audio feature representation. Because wav2vec can learn the\ncontext information between different time steps in audio, it can be combined with the powerful long sequence\nmodeling ability and multi head attention mechanism of transform model to significantly improve the speech\ncomprehension ability of the model.\nText feature extraction is based on Bert model. By extracting the final hidden state (768 dimensions) of the\nspecial tag [cls] as the text representation, the vector contains the deep semantic information of the sentence.\n3.2 Encoder and Decoder of feature vector\n3.2.1 Feature encoder and multi-modal Embedding Fusion. The input text features TâˆˆRğ‘‘ğ‘¡(ğ‘‘ğ‘¡=768) and audio\nfeatures AâˆˆRğ‘‘ğ‘(ğ‘‘ğ‘=512) undergo a three-stage gated fusion process:\nUnified Representation Learning: Both modalities are first aligned into a shared metric space using dedicated\nprojection layers. The text features are transformed to a 1024-dimensional subspace through a learnable linear\nlayer, while the audio features undergo similar dimensionality expansion. This resolves distribution discrepancies\nbetween modalities.\nContext-Aware Gating: A gating mechanism dynamically adjusts modality contributions by learning adaptive\nweights. The concatenated projected features pass through a sigmoid-activated layer that generates two gating\ncoefficients ( ğ‘”ğ‘¡,ğ‘”ğ‘), reflecting the relative importance of each modality for the current input.\nAdaptive Fusion: The final representation combines the projected features through gated summation: Hfusion=\nğ‘”ğ‘¡Â·Tâ€²+ğ‘”ğ‘Â·Aâ€²âˆˆR1024followed by duplication to form the 2048D output Hfinal. This architecture enables automatic\ncalibration of modality weights while preserving information density.\nThe function of embedding and fusion gate mechanism is to fuse text and speech in two modalities, and the\ndifference in distribution between modalities can be solved by linear projection of embedding layers into a unified\nmetric space. Through the gating mechanism, the modal weights in different scenarios can be automatically\nadjusted.\n3.2.2 Action decoder and autoregressive prediction . In this study, a sliding window mechanism was used to\nread multiple previous text and speech feature frames. Predicts the action frame at the next time, enabling\nautoregressive prediction. This mechanism enhances the modelâ€™s contextual understanding of text and speech.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 7 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nFig. 3. This figure illustrates the autoregressive prediction process. And the connection between autoregressive prediction,\nmotion match engine, and motion hybrid system.\nWe use the [Lee et al. 2001] strategy to process the text and audio feature sequences separately and generate\nsequential feature vector information according to the first several frames of the current time point. By inputting\nthe feature vector in the sliding window into the transformer, the model can learn the time or order relationship\nbetween features, so that it can predict the next element in the next window or sequence according to the features\nin the current window and the information of the previous window, so as to realize autoregressive prediction. Let\nğ‘be the window size. At timestep ğ‘¡, the input matrix contains ğ‘historical multi-modal features:\nXğ‘¡=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Tğ‘¡âˆ’ğ‘+1Ağ‘¡âˆ’ğ‘+1\n......\nTğ‘¡ Ağ‘¡ï£¹ï£ºï£ºï£ºï£ºï£ºï£»âˆˆRğ‘Ã—(ğ‘‘ğ‘¡+ğ‘‘ğ‘)(1)\nwhere Tğ‘˜âˆˆRğ‘‘ğ‘¡andAğ‘˜âˆˆRğ‘‘ğ‘represent text and audio features at timestep ğ‘˜.\nIn the autoregressive process, the transformer uses the previous prediction results as the conditions for\nsubsequent prediction, which can make the generated output more coherent and logical. Moreover, the sliding\nwindow can divide the feature sequences with different lengths into fixed length windows, which gives the\ntransformer model the ability to adapt to different length sequences. The encoder of the action feature selects the\nlast time step of the Transformer Encoder stack and outputs the 750-dimensional action feature value through a\nfully connected layer.\nTransformer Output: Câˆ—=TransformerBlockğ¿(C)\nAction Prediction: Ë†Yğ‘¡+1=Wğ‘œCâˆ—[âˆ’1,:]\nWindow Update: Xğ‘¡+1=\u0014Xğ‘¡[1 :,:]\n(Tğ‘¡+1,Ağ‘¡+1)\u0015(2)\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 8 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\n3.3 Time-Space attention transformer\n3.3.1 Positional Encoding . To preserve the temporal sequence, we augment each patch vector with positional\nembeddings, which encode the spatial and temporal location of the patch within the audio-text sequence.\nPositional embeddings follow standard sinusoidal encoding. The encoded features become:\nËœğ‘‹=ğ‘‹+PE(1 :ğ‘‡)âˆˆRğ‘‡Ã—ğ‘‘ğ‘’(3)\nThis allows the model to learn relative positions via trigonometric identities:\nPE(ğ‘ğ‘œğ‘ +Î”)=PE(ğ‘ğ‘œğ‘ )Â·PE(Î”)+cross-terms (4)\n3.3.2 Temporal Spacial Attention Mechanism. In this study, we employ Space-Time Attention within a Transformer-\nbased architecture to facilitate action inference from integrated textual and vocal data. This approach allows\nour model to capture the intricate relationships between multi-modal inputs across both spatial and temporal\ndimensions, enabling the prediction of subsequent action states.\nThe input sequence first calculates self attention on the time dimension:\nğ‘‹time=ğ‘‹âˆˆRğµÃ—ğ‘†Ã—ğ·\n{ğ‘„,ğ¾,ğ‘‰}time=Permute(ğ‘‹time)âˆˆRğ‘†Ã—ğµÃ—ğ·\nOutputtime=ğ‘‹+Permuteâˆ’1(Attention(ğ‘„,ğ¾,ğ‘‰))(5)\nWhere attention weights are computed via scaled dot-product similarity.\nAfter feature transformation, attention is paid on the transformed spatial feature dimension:\nğ‘‹space=ğ‘Šprojğ‘‹âˆˆRğµÃ—ğ‘†Ã—ğ·\n{ğ‘„,ğ¾,ğ‘‰}space=Permute(ğ‘‹space)\nOutputfinal=LayerNorm(ğ‘‹space+Permuteâˆ’1(Attention(ğ‘„,ğ¾,ğ‘‰)))(6)\nThe model computes attention weights for each patch by measuring the dot-product similarity between query,\nkey, and value representations. This process allows the model to selectively focus on the most relevant patches\nwhen predicting the next state in the action sequence.\n3.4 Real-Time Motion Matching Engine\n3.4.1 K-NN Graph Construction . The construction process of the action graph can be mathematically formalized\nas follows: Given a set of ğ‘›actions, each action ğ‘ğ‘–(ğ‘–âˆˆ{1,2,...,ğ‘›}) is characterized by two attributes: feature\nvectorğ‘¥ğ‘–âˆˆRğ‘‘(ğ‘‘=750) and duration scalar ğ‘¡ğ‘–âˆˆR+. We define a metric space (Rğ‘‘,ğ‘‘)using the Euclidean\ndistance. For each ğ‘¥ğ‘–, compute its ğ‘˜-nearest neighbors (k-NN) set:\nNğ‘˜(ğ‘¥ğ‘–)=\b\nğ‘¥ğ‘—âˆˆğ‘‹\f\fğ‘¥ğ‘—is one of the ğ‘˜closest points to ğ‘¥ğ‘–under metric ğ‘‘\t\n. (7)\nConstruct an undirected weighted graph ğº=(ğ‘‰,ğ¸,ğ‘Š): Vertex set: ğ‘‰={ğ‘£1,ğ‘£2,...,ğ‘£ğ‘›}, whereğ‘£ğ‘–â†”ğ‘ğ‘–.\nEdge set:\nğ¸=ğ‘›Ã˜\nğ‘–=1\b\n(ğ‘£ğ‘–,ğ‘£ğ‘—)\f\fğ‘¥ğ‘—âˆˆNğ‘˜(ğ‘¥ğ‘–)âˆ§ğ‘–â‰ ğ‘—\t\n. (8)\nEdge weight function:\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 9 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nğ‘¤ğ‘–ğ‘—=âˆ¥ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—âˆ¥2=vutğ‘‘âˆ‘ï¸\nğ‘˜=1(ğ‘¥ğ‘–,ğ‘˜âˆ’ğ‘¥ğ‘—,ğ‘˜)2,âˆ€(ğ‘£ğ‘–,ğ‘£ğ‘—)âˆˆğ¸. (9)\nFig. 4. K-NN Graph Visualization after t-SNE Dimensionality Reduction. The graph consists of 9,143 action nodes, where\neach node contains an action feature vector and its corresponding duration, with every node connected to its K=10 nearest\nneighboring nodes to form a K-Nearest Neighbor (K-NN) graph structure.\nThis graph structure establishes a topological foundation for efficient graph-traversal-based action sequence\nretrieval and optimization.\n3.4.2 Retrieval Pipeline. The algorithm begins by identifying the nearest node in the preconstructed KNN graph\n[Zhao et al .2021]to the previous action feature vector, which serves as the starting point for constrained search.\nIt then retrieves the top-k neighbors of this node, sorted by their Euclidean distance to the previous vector, to\nprioritize locally similar actions. A breadth-first search (BFS)[Kurant et al .2010] is initiated from these neighbors,\nexpanding iteratively to their connected nodes while avoiding revisiting explored nodes. During traversal, the\nalgorithm enforces a duration constraint by only considering nodes whose precomputed action durations exceed\na threshold t For each valid node, the Euclidean distance between its feature vector and the current target vector\nis calculated and stored.\nThe search terminates once the queue is exhausted, and the node with the smallest distance to the current\nvector is selected as the optimal match. If no nodes satisfy the duration constraint during the entire traversal, the\nalgorithm returns an empty result. This approach balances motion continuity (via graph-based proximity to the\nprevious action) and temporal constraints while efficiently exploring the graph structure to align with evolving\naction requirements.\n3.5 Motion Hybrid system\nWe propose a real-time gradual change switching method of motion signal based on quaternion, We realize the\nseamless transition of BVH animation sequence through the dual mechanism of quaternion rotation representation\nand cube interpolation. In the attitude interpolation phase, the quaternion spherical linear interpolation algorithm\nis adopted:\nq(ğ‘¡)=sin((1âˆ’ğ‘¡)ğœƒ)\nsinğœƒq1+sin(ğ‘¡ğœƒ)\nsinğœƒq2 (10)\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 10 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nAlgorithm 1: Constrained KNN Search with Duration Filter\nInput: prev vector, current vector, graph ğº, vectorsğ‘‰, action durations ğ·, action duration ğœ, top k\nOutput: best action index or âˆ…\nğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘¡ğ‘–ğ‘‘ğ‘¥â†argminğ‘–âˆ¥ğ‘‰[ğ‘–]âˆ’ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿâˆ¥2;\nğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ â†Sortğº[ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘¡ğ‘–ğ‘‘ğ‘¥]byâˆ¥ğ‘‰[ğ‘¥]âˆ’ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿâˆ¥2;\nğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ â†ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ [1..ğ‘¡ğ‘œğ‘ğ‘˜];\nInitialize visited set: Vâ†âˆ… ;\nInitialize queueQwithğ‘ğ‘Ÿğ‘’ğ‘£ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘  ;\nInitialize valid nodes: Nâ†[] ,Dâ†[] ;\nwhileQâ‰ âˆ…do\nğ‘¢â†Q .pop(0);\nifğ‘¢âˆˆV then\ncontinue;\nVâ†Vâˆª{ ğ‘¢};\nifğ·[ğ‘¢]>ğœthen\nğ‘‘â†âˆ¥ğ‘‰[ğ‘¢]âˆ’ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿâˆ¥2;\nN.append(u),D.append(d);\n// Add neighbors of ğ‘¢to queue\nQâ†Qâˆªğº[ğ‘¢];\nifNâ‰ âˆ…then\nğ‘ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥â†argminğ‘‘D;\nreturnN[ğ‘ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥];\nelse\nreturnâˆ…;\nWhereğœƒis the angle between quaternion q1andq2,ğ‘¡is the interpolation parameter. The formula ensures the\nshortest path interpolation of the rotation axis on the spherical surface, and effectively avoids the problem of\nuniversal joint deadlock caused by Euler angle interpolation. The position gradient uses cube interpolation\nfunction to realize smooth transition of continuous acceleration:\nğ¿(ğ‘¡)=(2ğ‘¡3âˆ’3ğ‘¡2+1)ğ¿1+(âˆ’2ğ‘¡3+3ğ‘¡2)ğ¿2 (11)\nWhereğ‘™1andğ‘™2respectively represent the position coordinates of adjacent keyframes. Its speed curve meets:\nğ‘£(ğ‘¡)=ğ‘‘ğ¿\nğ‘‘ğ‘¡=(6ğ‘¡2âˆ’6ğ‘¡)ğ¿1+(âˆ’6ğ‘¡2+6ğ‘¡)ğ¿2 (12)\nThe interpolation function satisfies the zero boundary condition (V (0)=v (1)=0) at the endpoint to ensure\ncontinuous speed at the beginning and end of the switching action, which effectively inhibits the sudden change\nof mechanical motion.\n3.6 Training and inference\nThe training process involves the following steps: Formally, for a window size W and input sequences:\nText:ğ‘‹text=[ğ‘¥ğ‘¡,ğ‘¥ğ‘¡+1,...,ğ‘¥ğ‘¡+ğ‘Šâˆ’1], extracted from textual data using Bert.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 11 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nAudio:ğ‘‹audio=[ğ‘ğ‘¡,ğ‘ğ‘¡+1,...,ğ‘ğ‘¡+ğ‘Šâˆ’1], extracted from audio data using WAVE2vec2.\nTrue Action: ğ‘¦=[ğ‘¦ğ‘¡,ğ‘¦ğ‘¡+1,...,ğ‘¦ğ‘¡+ğ‘Šâˆ’1], extracted from Bvh files. We convert Bvh files into numpy arrays and\nthen use PCA to reduce the dimensionality.\nThe model predicts: Ë†ğ‘¦=ğ‘“(ğ‘‹text,ğ‘‹audio)â†’ğ‘¦ğ‘¡+ğ‘Šâˆ’1, representing the predicted action feature.\nFor loss function, we use Mean Squared Error (MSE) loss between predicted and true actions:\nğ¿(ğœƒ)=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1âˆ¥Ë†ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–âˆ¥2\n2 (13)\nwhereğœƒrepresents all trainable parameters\nFor optimizer, we use Adam optimizer to update parameters via:\nğœƒğ‘¡=ğœƒğ‘¡âˆ’1âˆ’ğœ‚ğ‘¡Ë†ğ‘šğ‘¡/(âˆšï¸\nË†ğ‘£ğ‘¡+ğœ–) (14)\nwhere Ë†ğ‘šğ‘¡andË†ğ‘£ğ‘¡are bias-corrected momentum estimates, ğœ‚ğ‘¡is the decaying learning rate\nThe sliding window creates the training pairs (ğ‘‹ğ‘¡:ğ‘¡+ğ‘Š\ntext,ğ‘‹ğ‘¡:ğ‘¡+ğ‘Š\naudio)â†’ğ‘¦ğ‘¡+ğ‘Š, allowing the model to learn temporal\ndynamics while maintaining fixed-length input dimensionality. The attention mechanism then discovers relevant\ncross-modal patterns within these windows, while the optimization process adjusts parameters to minimize\nprediction error across the dataset.\nIn inference processing, audio and text inputs are processed through a multi-modal transformer to generate\nmotion features, which are then synthesized into real-time animations via a motion matching system. These\nanimations are converted into JSON signals and streamed through the LiveLink plugin to be rendered with\nMetaHuman characters in Unreal Engine 5. In the mean time motion signal is recorded in the format BVH.\nFig. 5. The real-time motion is inferred from JSON signals transmitted through the LiveLink plugin, then rendered in Unreal\nEngine 5\n4 EXPERIMENTAL DESIGN AND RESULTS\nThe primary purpose of this experiment is to comprehensively evaluate the performance of the proposed model\nin generating digital human movements. Specifically, it aims to assess how well the model can generate natural,\nfluent, and semantically-appropriate movements in different conversation scenarios. By comparing the model\nwith baseline models and conducting ablation experiments, we can identify the modelâ€™s strengths and weaknesses,\nand gain insights into the contribution of different components, This will provide a basis for further improving\nthe model and enhancing its practical application value in fields like human-computer interaction and virtual\nreality.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 12 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\n4.1 Dataset and Preprocessing\nWe utilized two datasets for their rich content and wide coverage in the field of gesture-related research. The\nBEAT dataset[Liu et al .2022] is a comprehensive resource comprising 76 hours of high-quality, multi-modal\ndata captured from 30 speakers. the ZEGGS dataset[Ghorbani et al .2023]contains 67 monologues performed in\nEnglish by a female actor, showcasing 19 different emotions. The total duration is approximately 135 minutes.\nwe segmented the BVH (Biovision Hierarchy) motion files, text files, and audio recordings into short clips\nranging from 0.8 to 20 seconds in duration. Each sample is a triplet consisting of a mono-channel WAV audio file,\na TXT text file, and a BVH motion file, ensuring a comprehensive representation of the speech, linguistic content,\nand kinematic data, respectively.\nTo create a temporally aligned dataset suitable for subsequent analysis, we implemented a sliding window\napproach. Each window contains a contiguous segment of feature vectors from all three modalities, arranged in\nchronological order. This approach ensures that our dataset maintains the temporal coherence and inter-modal\nsynchrony inherent in the original BEAT dataset, paving the way for robust and insightful multi-modal analysis.\n4.2 System setup\nWe conducted the training process on a system equipped with an Intel Core i9-13900K CPU and an NVIDIA\nGeForce RTX 4090 GPU. The segmented BEAT dataset was utilized for training, which is expected to take\napproximately 10 hours.\nWe set the batch size to 256, the learning rate to 1e-4, and the number of epochs to 10000. The Adam optimizer\nwas used for training, with a learning rate of 1e-4 and a decay rate of 0.999. The model was trained using the\nMSE loss function.\nConsidering the need for real-time inference on consumer-grade devices, we use an ordinary computer equipped\nwith an AMD 5800H CPU and an NVIDIA 3060M graphics card in the inference process. In practice, we record\nBVH files and simultaneously drive a digital human in the UE5 engine in real-time.\n4.3 Visualization Results\nWe systematically generated BVH animations for all models using identical input data from two standardized\ncorpora: 67 clean speech samples in ZEGGS dataset and 130 Wayne-subset utterances from BEAT dataset. Each\nmodel processed identical text-audio pairs through their native pipelines, with outputs uniformly converted to\nBVH format for fair comparison. The rendered BVH Animation is shown in Figure 6.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 13 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nFig. 6. Visual comparisons of motion generation results, GT(ground truth) represents the original motion data, and the other\nmodels represent the generated motion data.\nIn the process of generating the animation, we conducted a experiment of The Average Inference Time per\nSentence(AITS) details. The inference time of each module of TRiMM was recorded. Figure 7. shows the average\ninference time of each module, and the overall latency from loading data to rendering.\nFig. 7. The Average Inference Time per Sentence of each module in TRiMM.\n4.4 Subjective and Objective Evaluation\n4.4.1 Subjective Evaluation Criteria. This study draws on the papers of [Zhang et al .2024c]. In order to conduct a\ncomprehensive subjective assessment, this study uses the same three indicators: human likeness, appropriateness\nand style Appropriateness. Human like nature is used to measure whether the generated action is close to human\naction. This index does not consider the matching degree between the generated action and the original text and\nthe original audio, but only considers whether the generated action is true and natural; Matching focuses on the\ntime alignment of semantic, emotional (intonation), and rhythm between the generated action and the original\ntext and the original audio, and the matching degree of generated action and the expression of semantic and\nemotional expression; Style matching evaluates the matching degree of the style between the generated action\nand the original action corresponding to the original text and the original audio.\nWe refer to [Zhang et al .2024c] To carry out a user study based on paired comparison. In a single experiment,\nthe research participants will see the video clips generated by two different models (including the original motion\ndata ground truth, GT). The research participants are required to watch the video clips for at least 10 seconds and\ncompare them side by side. According to [Zhang et al .2024c] In order to reduce the tendency of neutral selection\nand increase the discrimination and measurement accuracy, this study will increase the guidelines for evaluating\nthe preference dimensions of research participants from five to seven according to Millerâ€™s law of psychology\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 14 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nand cognitive research. Research participants choose the corresponding dimension guidance according to their\npreferences and degree of preference. The degree of preference is quantified to 7 scales. According to the selected\ndimension guidance, research participantsâ€™ preference video clips get corresponding scores, and nonpreference\nvideo clips get negative scores (for example, research participantsâ€™ preference video clips get 2 points, and\nnonpreference video clips get -2 points). If there is no preference, both video clips get 0 points. To ensure the\nparticipation of research participants, The attention test was set to ensure the attention of people tested.\nThis study uses the BEAT dataset and the zeggs dataset. Since the BEAT dataset and the ZEGGS dataset cover\na large amount of data. It is not feasible and necessary to include all the data in the user research experiment.\nThis study adopts the method of random sampling, and randomly extracts 20 data of a role in the BEAT dataset\nand 20 data of a style in the zeggs dataset for each participant. Neither the training set nor the validation set used\nin this study contained the selected data.\nA total of 24 volunteers were recruited to participate in the user research experiment. There are 7 males and 17\nfemales, aged from 17 to 30. All research participants have a high level of English.\n4.4.2 Objective Evaluation Criteria. We use the following metrics for evaluation.\nAverage Inference Time per Sentence (AITS) [Dai et al .2024] tested the generation speed and the ability of\nreal-time motion synthesis, For each sentence (text description) in the dataset, the time taken from inputting the\ntext to generating the corresponding movement output by the model was recorded. This time encompassed all\nprocesses, including text processing and movement sequence generation. The process was repeated multiple\ntimes (30 times in this study). The total time of all repetitions was divided by the total number of sentences to\nobtain AITS, expressed as AITS=Total Inference Time/Total Number of Sentences.\nFrechet Gesture Distance (FGD) [Yang et al .2023a], inspired by the Frechet Inception Distance (FID)[Heusel\net al.2017], evaluates the quality of generated movements. It measures the similarity between generated and\nreal-world gestures. A lower FGD value indicates a closer match between their feature distributions. Feature\nvectors were extracted from both generated and real-gesture data to represent key gesture characteristics. Then,\nthe mean and covariance matrices of these feature vectors were computed separately. The FGD value quantifies\nthe difference between the two sets of feature vectors, with the distance between mean vectors and the difference\nin covariance matrices as key factors.\nBeat Align assessed the synchronization between musical and motion BEATs [Bian et al .2025]. First, it extracts\nmusical BEATs from audio using libraries like librosa, identifying BEAT positions via onset strength and BEAT\ndetection. Second, it derives motion BEATs from 3D joint data by calculating velocity, and finding local minima\nin the velocity envelope. Finally, it computes an alignment score by matching each motion BEAT to the nearest\nmusical BEAT and averaging the resulting scores, providing a measure of synchronization quality.\nDiversity [Lee et al .2019], was used to measure movement diversity. We randomly selected a set of text\ndescriptions. For each description, ten subsets of the same size were randomly extracted from all corresponding\ngenerated movements. Feature vectors were then extracted from these subsets, and the variance of their differences\nwas calculated to quantify diversity.\nFor the selection of baseline model, DiffuseGesture[Yang et al .2023b] from IJCAI2023 is selected as the scheme\nof diffuse method and multi-modal model. ZeroEGGS[Ghorbani et al .2023] from UBIsoft is selected as the example\nwith controllable styles and diversity, and Diffsheg[Chen et al .2024] from CVPR2024 is selected as the reference\nfor real-time action generation. QPGesture[Yang et al .2023a] from CVPR2023 is selected as the reference of\nmotion-match based method. We use the same data set of BEAT and zeggs, including text and audio, and generate\nBVH files for evaluation.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 15 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\n4.5 Ablation Experiments\nTo evaluate the contribution of key components in the TRiMM framework, we designed systematic ablation\nexperiments. All experiments were conducted on both BEAT and ZEGGS datasets using identical evaluation\nprotocols.\nmulti-modal Fusion Ablation (TRiMM MFA) : We removed the gated fusion mechanism (Section 3.2) that\ndynamically combines text and audio features through subspace projection and attention routing. This forced the\nmodel to process modalities independently, simulating traditional unimodal approaches. This experiment aims to\nassess the importance of cross-modal feature interaction for natural gesture synthesis.\nTime-Space Attention Ablation (TRiMM TSAA) : We replaced the divided space-time attention mechanism\n(Section 3.3) with standard self-attention. The modified version lost the explicit temporal locality constraints\nprovided by our sliding window auto-regressor. This experiment quantifies the impact of our space-time modeling\nstrategy.\nMotion Graph Ablation (TRiMM MGA) : We disabled the K-NN motion graph (Section 3.4), eliminating\nthe hierarchical action retrieval capability and forcing pure generative synthesis. This experiment evaluates the\ncontribution of the hybrid approach that combines retrieval learning with generative modeling.\nAll ablation models used the same training protocol and hyperparameters as the complete TRiMM model to\nensure fair and comparable results. We employed the same evaluation metrics as the complete model, including\nsubjective evaluation (human likeness, appropriateness, style appropriateness) and objective metrics (FGD,\ndiversity, Beat alignment, AITS).\n4.6 Evaluation Results\nWe evaluated our TRiMM model on both objective and subjective metrics. We compared TRiMM with several\nstate-of-the-art baselines on two datasets, BEAT [Liu et al .2022] and ZEGGS [Ghorbani et al .2023], and conducted\nablation experiments to assess the impact of different components.\nFig. 8. The mean rating of each metric for each approach across the two datasets in comparative experiments.\n4.6.1 Subjective Evaluation Results. For subjective evaluation, TRiMM demonstrates superior performance\nacross all metrics. On the ZEGGS dataset, TRiMM(ours) achieves 0.85 Â±1.00 for Human likeness, 1.11 Â±0.66 for\nAppropriateness, and 0.67 Â±0.60 for Style appropriateness, outperforming all baselines. This trend continues on\nthe BEAT dataset with scores of 1.08 Â±0.94, 1.280.81, and 1.11 Â±0.60 respectively. These results confirm TRiMMâ€™s\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 16 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nTable 1. Evaluation Results shown in one table, The best-performing values from the different models are shown in bold\nMethods Subject Evaluation Metric Objective Evaluation Metric\nDataset Model Human Appropr- Style FGD â†“ Diversity â†‘Beat align AITS â†“\nlikeness â†‘iateness â†‘appropriateness â†‘(Raw space)\nzeggsTRiMM(ours) 0.85Â±1.00 1.11Â±0.66 0.67Â±0.60 59011.57 6575.11 0.67 0.14\nDiffsheg -1.65 Â±1.00 -1.01Â±1.30 -1.21Â±1.06 10675.88 2193.84 0.67 0.32\nZeroEGGS 0.24 Â±0.85 -0.29Â±0.77 0.13Â±0.87 10202.83 2152.02 0.67 2.45\nDiffuseGesture -0.98 Â±1.01 -0.93Â±0.88 -0.24Â±1.13 103129.68 2930.34 0.62 19.06\nQPGesture 0.67 Â±0.69 0.57Â±0.75 0.61Â±0.94 86621.01 4778.74 0.67 193.07\nTRiMM MFA 0.11 Â±1.02 -0.01Â±0.81 0.09Â±0.79 58901.43 4840.56 0.66 0.14\nTRiMM TSAA 0.07 Â±0.67 0.17Â±0.79 -0.13Â±0.87 60714.39 5047.49 0.66 0.14\nTRiMM MGA 0.32 Â±1.07 0.11Â±0.85 -0.04Â±0.82 59191.43 5749.69 0.67 0.14\nBEATTRiMM(ours) 1.08Â±0.94 1.28Â±0.81 1.11Â±0.60 2826268.23 8862.52 0.64 0.19\nDiffsheg -1.05 Â±1.04 -0.28Â±1.32 -0.44Â±0.81 3023024.60 3847.35 0.64 0.36\nZeroEGGS 0.54 Â±0.85 0.01Â±0.57 0.26Â±0.79 3023715.75 1940.04 0.62 6.37\nDiffuseGesture 0.06 Â±0.96 0.09Â±1.20 0.03Â±0.96 2708706.48 4419.51 0.61 23.58\nQPGesture 0.38 Â±1.19 0.62Â±0.92 0.70Â±0.97 2764739.94 5636.87 0.64 419.96\nTRiMM TSAA -0.45 Â±0.79 -0.48Â±0.72 -0.32Â±1.15 3029053.96 6349.18 0.63 0.19\nTRiMM MGA -0.40 Â±1.04 -0.61Â±0.92 -0.64Â±0.65 3030758.72 6377.01 0.62 0.19\nTRiMM MFA -0.31 Â±0.58 -0.65Â±0.73 -0.75Â±0.91 3028293.14 6362.48 0.63 0.19\nFig. 9. The mean rating of each metric for each approach across the two datasets in ablation experiments.\nability to generate natural, contextually appropriate motions that maintain high fidelity to the original data in\nreal-world applications.\nAcross both the BEAT and ZEGGS datasets, TRiMM consistently outperformed all baseline models in terms of\nHuman-likeness, Appropriateness, and Style-appropriateness, as measured by user ratings. In addition, ablation\nstudies demonstrated that removing any major component from TRiMM led to a noticeable decline in perceptual\nquality, confirming the necessity of each architectural module.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 17 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\n4.6.2 Objective Evaluation Results. For quantitative results, as shown in Tab. 1, TRiMM demonstrates superior\nperformance across multiple metrics. On the ZEGGS dataset, TRiMM(ours) achieves an outstanding AITS of 0.14,\nsignificantly outperforming other baselines such as Diffsheg (0.22), ZeroEGGS (2.45), DiffuseGesture (19.06), and\nQPGesture (193.07). This remarkable inference speed is maintained on the BEAT dataset, where TRiMM(ours)\nachieves an AITS of 0.19, compared to 0.36 for Diffsheg, 6.37 for ZeroEGGS, 23.58 for DiffuseGesture, and 419.96\nfor QPGesture. This exceptional AITS performance establishes TRiMM as a leading solution for real-time digital\nhuman motion synthesis.\nIn terms of motion diversity, TRiMM(ours) achieves a score of 6575.11 on the ZEGGS dataset, significantly\nsurpassing all baselines including QPGesture (4778.74), Diffsheg (2193.84), ZeroEGGS (2152.02), and DiffuseGesture\n(2930.34). This trend continues on the BEAT dataset, where TRiMM(ours) achieves an even more impressive\ndiversity score of 8862.52, outperforming QPGesture (5636.87), Diffsheg (3847.35), ZeroEGGS (1940.04), and\nDiffuseGesture (4419.51). The high diversity metric ensures TRiMM can generate a wide variety of natural and\nengaging motions.\nTRiMM also excels in motion-audio synchronization, achieving a BEAT Alignment score of 0.67 on both datasets.\nThis performance matches the best baselines (Diffsheg and QPGesture) while significantly outperforming others\nlike DiffuseGesture (0.62) and ZeroEGGS (0.62 on BEAT). This synchronization capability enhances the overall\nrealism and naturalness of the generated motions.\n4.6.3 Ablation Experiments Results. Our systematic ablation studies validate the core architectural innovations\nin TRiMM through three key component removals (see Tab. 1):\nmulti-modal Fusion Removal (MFA) caused 87% human likeness reduction (0.85 â†’0.11) on ZEGGS and\n168% style appropriateness decline (1.11 â†’-0.75) on BEAT, while marginally increasing FGD1, confirming the\ngated fusionâ€™s critical role in cross-modal alignment.\nTime-Space Attention Removal (TSAA) reduced motion diversity by 23% (6575 â†’5047) and increased FGD\nby 2.9% (59011â†’60714) on ZEGGS, demonstrating the necessity of explicit space-time modeling for coherent\ngesture synthesis.\nMotion Graph Removal (MGA) caused the sharpest style degradation (1.11 â†’-0.64 on BEAT) while main-\ntaining real-time performance (AITS=0.19s), proving the graphâ€™s essential role in stylistic consistency.\nNotably, all ablation models maintained equivalent inference speeds (0.14-0.19s AITS), confirming performance\ngains stem from architectural improvements rather than computational tradeoffs. The experiments establish that\ncross-modal fusion enables 87-168% improvements in perceptual metrics, space-time attention provides 23-28%\ndiversity gains, and hierarchical motion graph ensures 115% better style preservation.\nThese findings collectively demonstrate the complementary strengths of TRiMMâ€™s three pillars in balancing\nnaturalness, diversity, and efficiency.\n4.6.4 Scalability and Generalization. Scalability : The hierarchical motion graph supports dynamic expansion\nof the library, enabling the model to adapt to new datasets without retraining. This scalability is particularly\nbeneficial in cross-dataset evaluation, where the model can be fine-tuned on one dataset and then applied to\nanother.\nGeneralization : Cross-dataset evaluation reveals strong transfer learning capabilities-when trained on BEAT\nand tested on ZEGGS, TRiMM maintains 84% of original performance (0.85 â†’0.71 human likeness) versus 52-67%\nfor diffusion baselines. The modular architecture enables component upgrades without full retraining, like\nwav2vec and bert modules.\n1Though absolute FGD appears lower in ZEGGS (59011 â†’58901), the relative degradation pattern holds across datasets\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 18 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\n5 DISSCUSSION\nIn this paper, we present a novel framework for real-time motion synthesis, TRiMM, that combines multi-modal\nfusion, space-time attention, and hierarchical motion graph for seamless and contextually appropriate motion\ngeneration. The framework is designed to enable real-time motion synthesis from text and audio inputs, offering\na versatile and efficient solution for applications such as virtual avatars, virtual characters, and virtual human\ninterfaces.\nThrough the space-time attention mechanism, TRiMM achieves high-precision alignment of speech, text,\nand motion features, achieving a BEAT alignment score of 0.67, which is significantly better than traditional\nmethods. This precise cross-modal synchronization capability ensures the naturalness and semantic consistency\nof generated actions.\nThe sliding window-based autoregressive model ensures coherent gesture generation in long-text dialogues,\nand achieves an inference latency of 0.14 seconds and a real-time rendering capability of 120FPS on RTX3060\ngraphics cards. Compared to diffusion-based methods such as DiffuseGestureâ€™s 19.06 seconds, TRiMM delivers\n135 times faster inference and provides reliable technical support for real-time digital human interaction.\nA hierarchical action graph structure of 9,143 atomic actions, combined with a multi-criteria similarity search,\nresults in a motion diversity of 6575.11, which is twice that of traditional motion capture systems. This high\nversatility ensures that the system is able to generate rich and natural action sequences to meet the needs of\ndifferent scenarios.\nThe modular architecture supports component upgrades and dataset expansion, maintaining 84% of the original\nperformance in cross-dataset evaluation, demonstrating good generalization capabilities.\nAlthough TRiMM has made significant progress in real-time digital human action generation, there are still\nthree major limitations of the current framework: first, although the action transition method based on cubic\ninterpolation is efficient in real-time applications, it may lead to subtle discontinuities when dealing with complex\ngesture sequences, especially when switching between actions with large semantic differences; Secondly, the\nsystem is limited by a predefined action library of 9,143 actions, and lacks the ability to generate new action\npatterns beyond the scope of the existing action library, which may limit the adaptability of the system in different\ncultural backgrounds and personalized gesture styles. Finally, the model fails to make full use of the emotional\ncues in audio prosody and text semantics, which may lead to inconsistencies between the generated actions and\nthe emotional intensity implied by the input modalities.\nIn response to these challenges, we propose three future research directions: 1) expanding the action library by\ncollecting region-specific motion capture data, developing adaptive motion synthesis techniques, and implement-\ning user-customizable gesture preferences; 2) replacing the heuristic mixing rules with a lightweight diffusion\nmodel that is trained on action transition segments and can perform data-driven interpolation based on previous\nactions and cross-modal contexts, achieving smoother gesture evolution while maintaining a delay of <30ms; 3)\nImplement a multi-stage sentiment alignment pipeline, including extracting sentiment descriptors through a\npre-trained model, injecting sentiment embeddings through an auxiliary adapter in the Transformer architecture,\nand introducing sentiment consistency loss terms to penalize the mismatch between the generated action and\nthe input sentiment features.\nACKNOWLEDGMENTS\nWe acknowledge the language polishing services provided by DeepSeek for their contribution to this paper.The\nauthors bear full responsibility for content.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 19 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nREFERENCES\nYuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. 2025. Motioncraft: Crafting whole-body motion with\nplug-and-play multimodal controls. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 39. 1880â€“1888.\nJunming Chen, Yunfei Liu, Jianan Wang, Ailing Zeng, Yu Li, and Qifeng Chen. 2024. DiffSHEG: A Diffusion-Based Approach for Real-Time\nSpeech-Driven Holistic 3D Expression and Gesture Generation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) . IEEE, Seattle, WA, USA, 7352â€“7361. doi:10.1109/CVPR52733.2024.00702\nKang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang, and Shi-Min Hu. 2021. ChoreoMaster: choreography-\noriented music-driven dance synthesis. ACM Transactions on Graphics 40, 4 (Aug. 2021), 1â€“13. doi:10.1145/3450626.3459932\nXin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. 2023. Executing your commands via motion diffusion in latent\nspace. 18000â€“18010 pages.\nSimon Clavet. 2016. Motion Matching and The Road to Next-Gen Animation. GDC 2016 Presentation (2016). https://archive.org/details/\nGDC2016Clavet\nWenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. 2024. Motionlcm: Real-time controllable motion\ngeneration via latent consistency model. In European Conference on Computer Vision . Springer, 390â€“408.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human\nlanguage technologies, volume 1 (long and short papers) . 4171â€“4186.\nSaeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F Troje, and Marc-AndrÃ© Carbonneau. 2023. ZeroEGGS: Zero-shot Example-based\nGesture Generation from Speech. In Computer Graphics Forum , Vol. 42. Wiley Online Library, 206â€“216.\nMichael Greenacre, Patrick JF Groenen, Trevor Hastie, Alfonso Iodice dâ€™Enza, Angelos Markos, and Elena Tuzhilina. 2022. Principal component\nanalysis. Nature Reviews Methods Primers 2, 1 (2022), 100.\nIkhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar, Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, and Christian Theobalt.\n2022. A Motion Matching-based Framework for Controllable Gesture Synthesis from Speech. In Special Interest Group on Computer\nGraphics and Interactive Techniques Conference Proceedings . ACM, Vancouver BC Canada, 1â€“9. doi:10.1145/3528233.3530750\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale\nUpdate Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems , Vol. 30. Curran Associates, Inc.\nhttps://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned motion matching. ACM Trans. Graph. 39, 4 (Aug.\n2020), 53:53:1â€“53:53:12. doi:10.1145/3386569.3392440\nKorrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. 2023. Guided Motion Diffusion for Controllable\nHuman Motion Synthesis. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV) . IEEE, Paris, France, 2151â€“2162. doi:10.\n1109/ICCV51070.2023.00205\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers\nwith Linear Attention. In Proceedings of the 37th International Conference on Machine Learning . PMLR, 5156â€“5165. https://proceedings.mlr.\npress/v119/katharopoulos20a.html ISSN: 2640-3498.\nMaciej Kurant, Athina Markopoulou, and Patrick Thiran. 2010. On the bias of BFS (breadth first search). In 2010 22nd International Teletraffic\nCongress (LTC 22) . IEEE, 1â€“8.\nChang-Hung Lee, Cheng-Ru Lin, and Ming-Syan Chen. 2001. Sliding-window filtering: an efficient algorithm for incremental mining. In\nProceedings of the tenth international conference on Information and knowledge management . 263â€“270.\nHsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz. 2019. Dancing to Music.\nInAdvances in Neural Information Processing Systems , Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/hash/\n7ca57a9f85a19a6e4b9a248c1daca185-Abstract.html\nHaiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. 2022. Beat: A large-scale\nsemantic and emotional multi-modal dataset for conversational gestures synthesis. In European conference on computer vision . Springer,\n612â€“630.\nMathis Petrovich, Michael J. Black, and Gul Varol. 2021. Action-Conditioned 3D Human Motion Synthesis with Transformer VAE. In 2021\nIEEE/CVF International Conference on Computer Vision (ICCV) . IEEE, Montreal, QC, Canada, 10965â€“10975. doi:10.1109/ICCV48922.2021.01080\nMathis Petrovich, Michael J Black, and GÃ¼l Varol. 2022. Temos: Generating diverse human motions from textual descriptions. In European\nConference on Computer Vision . Springer, 480â€“497.\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019. wav2vec: Unsupervised Pre-Training for Speech Recognition. In\nInterspeech 2019 . 3465â€“3469. doi:10.21437/Interspeech.2019-1873\nSinan Sonlu, Bennie Bendiksen, Funda Durupinar, and UÄŸur GÃ¼dÃ¼kbay. 2024. The Effects of Embodiment and Personality Expression on\nLearning in LLM-based Educational Agents. http://arxiv.org/abs/2407.10993 arXiv:2407.10993 [cs].\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 20 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nXiangjun Tang, He Wang, Bo Hu, Xu Gong, Ruifan Yi, Qilong Kou, and Xiaogang Jin. 2022. Real-time controllable motion transition for\ncharacters. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1â€“10.\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano. 2022. Human Motion Diffusion Model.\ndoi:10.48550/arXiv.2209.14916 arXiv:2209.14916 [cs].\nHongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao, Dakuo Wang, Yvonne Coady, and Mirjana Prpa. 2024. Building LLM-based\nAI Agents in Social Virtual Reality. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI,\nUSA) (CHI EA â€™24) . Association for Computing Machinery, New York, NY, USA, Article 65, 7 pages. doi:10.1145/3613905.3651026\nSen Wang, Jiangning Zhang, Weijian Cao, Xiaobin Hu, Moran Li, Xiaozhong Ji, Xin Tan, Mengtian Li, Zhifeng Xie, Chengjie Wang, and\nLizhuang Ma. 2024. MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model. CoRR abs/2403.02905 (2024).\ndoi:10.48550/ARXIV.2403.02905 arXiv:2403.02905\nQi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. 2025. Motion-Agent: A Conversational Framework for\nHuman Motion Generation with LLMs. In The Thirteenth International Conference on Learning Representations . https://openreview.net/\nforum?id=AvOhBgsE5R\nLixing Xiao, Shunlin Lu, Huaijin Pi, Ke Fan, Liang Pan, Yueer Zhou, Ziyong Feng, Xiaowei Zhou, Sida Peng, and Jingbo Wang. 2025.\nMotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space. doi:10.48550/arXiv.2503.\n15451 arXiv:2503.15451 [cs] version: 1.\nSicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Ming Cheng, and Long Xiao. 2023b. DiffuseStyleGesture:\nStylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models. In Proceedings of the Thirty-Second International Joint\nConference on Artificial Intelligence (IJCAI-23) . IJCAI. https://www.ijcai.org/proceedings/2023/0650.pdf\nSicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, and Haolin Zhuang. 2023a. QPGesture: Quantization-Based\nand Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation. In 2023 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) . IEEE, Vancouver, BC, Canada, 2321â€“2330. doi:10.1109/CVPR52729.2023.00230\nHeyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. 2022. ControlVAE: Model-Based Learning of Generative Controllers for Physics-\nBased Characters. ACM Transactions on Graphics 41, 6 (Dec. 2022), 1â€“16. doi:10.1145/3550454.3555434 arXiv:2210.06063 [cs].\nFan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, and Shunman Li.\n2024c. Speech-Driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference. IEEE Transactions on Visualization\nand Computer Graphics 30, 10 (Oct. 2024), 6984â€“6996. doi:10.1109/TVCG.2024.3393236\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. 2024a. Motiondiffuse: Text-driven\nhuman motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence 46, 6 (2024), 4115â€“4128.\nMingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. 2023. ReMoDiffuse:\nRetrieval-Augmented Motion Diffusion Model. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV) . IEEE, Paris, France,\n364â€“373. doi:10.1109/ICCV51070.2023.00040\nMingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang,\nYing He, et al .2024b. Large motion model for unified multi-modal motion generation. In European Conference on Computer Vision . Springer,\n397â€“421.\nWan-Lei Zhao, Hui Wang, and Chong-Wah Ngo. 2021. Approximate k-NN graph construction: a generic online approach. IEEE Transactions\non Multimedia 24 (2021), 1909â€“1921.\nWenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and\nLingjie Liu. 2025. EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation. In Computer Vision â€“ ECCV 2024 ,\nAleÅ¡ Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and GÃ¼l Varol (Eds.). Vol. 15060. Springer Nature Switzerland,\nCham, 18â€“38. doi:10.1007/978-3-031-72627-9_2 Series Title: Lecture Notes in Computer Science.\n6 APPENDIX\nDetails of Datasets\nWe employed the BEAT dataset as the training corpus, while both the BEAT and ZEGGS datasets were utilized\nfor evaluation and user study. The BEAT dataset includes multiple speakers, spans a long period, and has a rich\nvariety of emotions and action types, we selected 29 speakers from it as the training set, and used data from one\nof the speakers, Wayne, as the test set. Additionally, ZEGGS contains various emotional types that can be used to\ntest the robustness and generalization of this model, so it was also included in the test set. Table 2 summarizes\nthe detailed statistics of the datasets used in our experiments.\nUser study details\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 21 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nTable 2. Dataset statistics used in training, evaluation, and user studies.\nDataset Training\nTimeEval/User\nTimeTotal Dura-\ntionFrame Rate Audio Rate Speakers\nBEAT 34 h 1 h 35 h 120 fps 48 kHz 30 (multi-\nspeaker)\nZEGGS 0 h 135 min 135 min 60 fps 48 kHz 1 (female)\nIn this study, we designed three user evaluation tasks targeting Human-likeness, appropriateness, and style-\nappropriateness. For the Human-likeness and appropriateness tasks, two videos were presented side by side (left\nand right). For the style-appropriateness task, three videos were displayed simultaneously (left, center, right),\nwith the center video always showing the ground-truth human performance, enabling more intuitive visual\ncomparison for the participants. Due to the technical challenges of losslessly retargeting skeletal motion data\nto virtual avatarsâ€”â€”especially in preserving fine-grained movementsâ€”â€”and the potential perceptual biases\nintroduced by different avatar appearances, we chose to use a unified skeletal rendering pipeline for all models.\nThe animations were rendered directly as skeleton-only sequences without any character meshes. An example of\nthe rendering setup is provided in Figure 9.\nBelow the video playback area, participants were presented with discrete options indicating their comparative\npreferences: \"Left video is much better,\" \"Left video is moderately better,\" \"Left video is slightly better,\" \"Videos\nare equally good,\" \"Right video is slightly better,\" \"Right video is moderately better,\" and \"Right video is much\nbetter.\" Based on these responses, the evaluation platform assigned scores to the left and right videos on a 0â€“3\nscale, where 0 indicates no preference. The unselected video automatically received the inverse score (e.g., if\n\"Left video is moderately better\" is selected, the left video is assigned +2 and the right video -2).\nFig. 10. userstudy program UI\nIn our user study, we adopted a 7-point Likert scale to measure participantsâ€™ subjective preferences. According\nto Nunnallyâ€™s seminal work, scale reliability tends to plateau around seven response categories, with marginal\ngains observed beyond eleven points. Similarly, Symondsâ€™ early research also identified the 7-point Likert scale\nas yielding optimal reliability. Although studies by Jenkins, Wakita, and others have suggested that 5-point scales\nmay produce higher data quality in certain contexts, the present study necessitates capturing subtle perceptual\ndifferences in motion quality generated by various models. Therefore, the 7-point scale is considered more\nappropriate for eliciting fine-grained user judgments in our experimental setting.\nFor all three user evaluation tasksâ€”human-likeness, appropriateness, and style-appropriatenessâ€”we provided\nparticipants with detailed and clearly defined instructions and assessment criteria:\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 22 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nâ€¢Human-likeness: Participants were shown two silent gesture videos. They were asked to judge which of\nthe two animations appeared more natural and more closely resembled human-like motion in terms of\nbody movement.\nâ€¢Appropriateness: Participants were presented with two gesture videos accompanied by speech audio. They\nwere asked to evaluate which animation better aligned with the speech in terms of rhythm, intonation, and\nsemantic correspondence, and which appeared more natural overall.\nâ€¢Style-Appropriateness: Participants were shown three gesture videos with audio, where the center video\nrepresented the ground-truth motion rendered from real human performance. Participants were instructed\nto compare the two generated gesture videos (left and right) and determine which one more closely\nresembled the motion style of the ground-truth video.\nTo complement the user evaluation tasks, we conducted a post-study questionnaire to collect participantsâ€™\nsubjective feedback. By aggregating and analyzing the responses, we obtained an overall view of participantsâ€™\nimpressions and preferences regarding the generated gestures.A total of 24 participants took part in the study,\nincluding 7 males and 17 females, with ages ranging from 17 to 30 years.\nAt the beginning of the questionnaire phase, participants were provided with a detailed explanation of\nthe evaluation procedure and scoring methodology. To help them familiarize themselves with the task, we\npresented example videos that were not part of the actual evaluation set. Subsequently, participants were\ninstructed to complete the questionnaire in a quiet and distraction-free environment. During the entire evaluation\nprocess, participants were not informed of the underlying gesture generation method associated with each video.\nAdditionally, the playback order of the videos was randomized to minimize order effects and potential bias.\nTo ensure the quality and reliability of the collected responses, attention check mechanisms were embedded\nthroughout the questionnaire.\nGiven the large scale of the selected datasetsâ€”â€”BEAT, which contains 30 distinct styles with 118 short clips\nand 12 long clips per style, and ZEGGS, which comprises 19 styles with 1 to 4 clips eachâ€”â€”we adopted a random\nsampling strategy to construct the evaluation set. Participants were divided into multiple subgroups, and for each\nsubgroup. We assigned a unique subset of evaluation stimuli. Specifically, each subgroup received 5 randomly\nsampled clips from the BEAT dataset and 5 randomly selected styles from the ZEGGS dataset, with one clip\nrandomly chosen per style. To ensure coverage and reduce participant fatigue, each participant was only exposed\nto a manageable number of comparisons. Questionnaire content varied across different participant groups but\nremained consistent within each group.\nFor the ZEGGS dataset, each participant was randomly assigned 5 distinct styles, ensuring diversity in style\ncoverage. In the human-likeness and appropriateness evaluations, a total of 11 models, including the ground\ntruth (GT), were assessed, resulting in 55 rendered videos per participant.\nFor the BEAT dataset, 10 generative models along with the ground truth were evaluated, producing 66 videos\nper participant. In the human-likeness and appropriateness tasks, participants performed.\nStatistical Analysis and Visualization for User study results\nWe conducted an experiment to compare the performance differences among various models across multiple\nmetrics, such as Human-likenss, Appropriateness, and Style-appropriateness.\nFigure 8 presents the subjective evaluation results for all models across three metricsâ€”Human-likeness,\nAppropriateness, and Style-appropriatenessâ€”on both the BEAT and ZEGGS datasets. Each box plot illustrates\nthe distribution and central tendency of user ratings.\nOur model, TRiMM, consistently achieves the highest median scores across all dimensions and datasets, with\ntighter interquartile ranges, indicating both superior perceptual quality and stronger inter-rater agreement. The\nadvantage is particularly notable in the ZEGGS dataset for the Style-appropriateness metric.\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 23 ---\nTRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans â€¢\nFigure 9 presents the results of an ablation study comparing the full TRiMM model with three ablated\nvariantsâ€”TRiMM TSAA, MGA, and MFAâ€”on the BEAT and ZEGGS datasets. User ratings were collected for\nHuman-likeness, Appropriateness, and Style-appropriateness.\nThe full model (yellow) consistently achieves the highest median scores across all metrics and datasets,\nindicating the effectiveness of the full architecture. In contrast, all ablated variants show degraded performance,\nparticularly in Appropriateness and Style-appropriateness, suggesting that the removed modules play essential\nroles in maintaining semantic and stylistic coherence.\nBased on the action BVH data of different models on the BEAT dataset and the ZEGGS dataset, we extracted\ntheir motion node matrices and then constructed a score-difference matrix. For each pair of models (row ğ‘–and\ncolumnğ‘—), we calculated the difference in average scores between the row model and the column model as:\nscore_diff =average(sampleğ‘–)âˆ’average(sampleğ‘—)\nThe diagonal elements ( ğ‘–=ğ‘—) were set to 0, indicating no difference for the same model. Then we performed a\npaired t-test on the rating samples of each pair of models and calculated the ğ‘-value to test the significance of the\ndifference.\nThe numerical values reflect the magnitude of differences between models, where positive values indicate\nsuperior performance compared to other models, while negative values indicate inferior performance. The\nsignificance symbols (e.g., *) reflect the reliability of experimental results, where * indicates a significant difference\n(p < 0.05), and ns (no symbol) indicates an insignificant difference.\nWe merged multiple metrics and multiple datasets. Each dataset contains multiple metrics (such as ratings in\ndifferent dimensions), and each metric generates a sub-heatmap.\nThe heatmaps use a symmetric color mapping, with 0 at the center. Positive and negative values correspond to\ndifferent colors to highlight the direction of differences. The color range is adjusted globally to ensure that the\nscales of heatmaps from different datasets are consistent, facilitating horizontal comparison.\nFig. 11. Pairwise significance heatmaps of gesture generation methods on BEAT dataset\n, Vol. 1, No. 1, Article . Publication date: June .\n--- Page 24 ---\nâ€¢Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, and Fan Zhang\nFig. 12. Pairwise significance heatmaps of gesture generation methods on ZEGGS dataset\nTRiMM demonstrates significantly positive score differences across multiple dimensions (Human-likeness,\nAppropriateness, and Style-appropriateness) compared to various baseline models, indicating its superior per-\nformance in these metrics. Moreover, TRiMM maintains consistent performance across both ZEGGS and BEAT\ndatasets, showing its strong generalization capability and robustness.\n, Vol. 1, No. 1, Article . Publication date: June .",
  "text_length": 74823
}