{
  "id": "http://arxiv.org/abs/2506.05332v1",
  "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
  "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
  "authors": [
    "Jingyang Lin",
    "Jialian Wu",
    "Ximeng Sun",
    "Ze Wang",
    "Jiang Liu",
    "Yusheng Su",
    "Xiaodong Yu",
    "Hao Chen",
    "Jiebo Luo",
    "Zicheng Liu",
    "Emad Barsoum"
  ],
  "published": "2025-06-05T17:59:04Z",
  "updated": "2025-06-05T17:59:04Z",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05332v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05332v1  [cs.CV]  5 Jun 2025Unleashing Hour-Scale Video Training for\nLong Video-Language Understanding\nJingyang Lin1,2∗Jialian Wu1Ximeng Sun1Ze Wang1Jiang Liu1Yusheng Su1\nXiaodong Yu1Hao Chen1Jiebo Luo2Zicheng Liu1Emad Barsoum1\n1AMD2University of Rochester\nhttps://videomarathon.github.io/\nAbstract\nRecent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of well-\nannotated long videos has left the training of hour-long Video-LLMs underexplored.\nTo close this gap, we present VideoMarathon , a large-scale hour-long video\ninstruction-following dataset. This dataset includes around 9,700 hours of long\nvideos sourced from diverse domains, ranging from 3 to 60 minutes per video.\nSpecifically, it contains 3.3M high-quality QA pairs, spanning six fundamental\ntopics: temporality, spatiality, object, action, scene, and event. Compared to\nexisting video instruction datasets, VideoMarathon significantly extends training\nvideo durations up to 1 hour, and supports 22 diverse tasks requiring both short-\nand long-term video comprehension. Building on VideoMarathon, we propose\nHour-LLaV A , a powerful and efficient Video-LMM for hour-scale video-language\nmodeling. It enables hour-long video training and inference at 1-FPS sampling\nby leveraging a memory augmentation module, which adaptively integrates user\nquestion-relevant and spatiotemporal-informative semantics from a cached full\nvideo context. In our experiments, Hour-LLaV A achieves the best performance on\nmultiple long video-language benchmarks, demonstrating the high quality of the\nVideoMarathon dataset and the superiority of the Hour-LLaV A model.\n1 Introduction\nLeveraging strong foundation models [ 41,63] and high-quality video instruction-following data [ 6,\n23,66], recent Video-LMMs [ 6,25,48,66] have achieved promising performance on basic video-\nlanguage tasks, such as video question-answering (QA) [ 56,57,62] and video summarization [ 17,\n19,28,36,45]. As attention shifts toward longer video modeling, these models face new challenges\nin capturing long-term dependencies. To measure progress in this direction, several benchmarks have\nbeen introduced for hour-scale video understanding, where the goal is to assess model performance\non tasks involving long-form video content over one hour [ 4,13,52,54]. Early attempts on long-form\nvideo-language modeling [42, 43, 47, 64, 68] have shown promising results on these benchmarks.\nHowever, a significant gap remains between the length of videos used during training and those\nencountered at inference time. While testing videos often exceed one hour [ 4,13,52], most existing\ntraining datasets [ 6,25,66] consist of videos shorter than ten minutes, as summarized in Table 1.\nThis mismatch limits the models’ ability to explicitly learn long-term dependencies, highlighting the\nnecessity for long-form video instruction-following datasets to bridge this gap.\nTo this end, we present VideoMarathon, a large-scale video instruction-following dataset specifically\ndesigned for long-form video-language modeling. VideoMarathon contains around 9,700 hours of\n∗Work was done during the internship at AMD.\nPreprint. Under review.\n--- Page 2 ---\nTable 1: Comparison between VideoMarathon and other existing video instruction-following datasets .\nOE and MC denote open-ended and multiple-choice, respectively.\nDataset Captioner SummarizerTotal Video Average Video Duration #OE #MC\nTime Length Range QA QA\nLLaV A-Hound [65] GPT-4V GPT-4 3K hrs 0.2 mins 0-8 mins 900K 0\nShareGPT4Video [6] GPT-4V GPT-4 0.2K hrs 0.3 mins <2 mins 0 0\nLLaV A-Video-178K [66] GPT-4o GPT-4o 2K hrs 0.6 mins <3mins 960K 196K\nVideoMarathon ( ours) Qwen2VL-7B DeepSeek-V3 9.7K hrs 20.9 mins 3-60 mins 1.73M 1.57M\nlong videos (3-60 min per video) sourced from diverse domains, such as activities [ 3], egocentric [ 15],\nmovie [ 44], cooking [ 67], and open-world [ 7] scenarios. Inspired by the recent efforts [ 4,13,25,\n40], we carefully define a comprehensive task taxonomy covering six essential topics–temporality,\nspatiality, object, action, scene, and event–across 22 diverse tasks requiring both short- and long-term\ncomprehension. To construct high-quality video instruction-following QA samples, we first develop a\nhierarchical video captioning pipeline: clip-level descriptions are generated using Qwen2VL-7B [ 51]\nacross the above six topics, then summarized at the event and global levels via DeepSeek-V3 [ 30].\nBased on these hierarchical video captions, we synthesize 3.3M high-quality QA pairs by DeepSeek-\nV3, guided by task-specific prompts that integrate detailed task definitions and QA exemplars from\nprior benchmarks [ 4,13,25,40], covering both open-ended (OE) and multiple-choice (MC) formats.\nAs shown in Table 1, the comparison between our VideoMarathon and other existing video instruction-\nfollowing datasets shows that VideoMarathon features a significantly longer average video length,\nbroader duration range, and a larger number of QA pairs.\nMost existing Video-LMMs [ 6,26,32,66] are trained on short videos, typically less than one\nminute on average, due to the limited availability of long video data in prior work. Uniform temporal\nsampling [ 24,66], which selects a fixed number of frames (8-256 frames per video), is commonly used\nfor short videos. However, when applied to hour-long videos, such sparse sampling leads to substantial\ninformation loss, leading to performance degradation [ 4,13,52]. To overcome this limitation, we\npropose Hour-LLaV A, a Video-LMM specifically designed for long-form video-language modeling\nand capable of directly ingesting hour-long and densely sampled videos. Hour-LLaV A stores the full\nvideo context sampled at 1 FPS in a dedicated memory repository. To accommodate GPU memory\nconstraints, a subset of the full video context (which we refer to as decayed video tokens ) is used\nas input to the language model. The decayed video tokens are augmented with the information\ncollected from the memory repository, enabling long-term dependency modeling. This memory\naugmentation (MemAug) mechanism effectively mitigates the information loss caused by reduced\ntokens of sparse sampling, particularly in long videos. Moreover, in contrast to explicit token\ncompression methods [ 11,42,43] that rely on hand-crafted heuristics, our method achieves a\nlearnable compression, directly supervised by the final next-token prediction loss.\nOur main contributions are three-fold: (1) we introduce VideoMarathon, the first large-scale hour-\nlong video instruction-following dataset that contains videos ranging from 3 minutes to 1 hour\nwith a total duration of around 9,700 hours and 3.3M diverse QA instructions across 22 tasks;\n(2) We propose Hour-LLaV A, a Video-LMM capable of processing hour-long videos at 1 FPS in\nboth training and inference, supported by the proposed MemAug mechanism; (3) Training Hour-\nLLaV A on VideoMarathon, we achieve the best performance among open-source Video-LMMs on\nfour well-known video-language benchmarks, including LVBench [ 52] (average duration: 4037s),\nVideo-MME [13] (1021s), LongVideoBench [54] (459s), and TempCompass [33] (11s).\n2 Related Work\nVideo Instruction-following Data. Human annotation is costly and even limits the scale of video\ninstruction-following datasets. Early attempts [ 22,37] address the high cost of human annotation by\nusing ASR-generated subtitles, while later methods [ 16,24] employ template-based QA generation\non existing captioning datasets. However, these approaches often suffer from noisy, low-quality\nlabels. Recent pipelines [ 6,65,66] leverage powerful LMMs ( e.g., GPT-4V [ 38]/4o [ 39]) to produce\nhigh-quality video captions and diverse QA pairs. Despite these advancements, most training videos\nremain short, limiting the models’ ability to learn long-term dependencies explicitly. To this end,\nthis work introduces a high-quality, synthetic video instruction-following dataset built on long-form\nvideos, enabling Video-LMMs to capture the effective patterns in long videos.\n2\n--- Page 3 ---\nVideoMarathon\nTemporalityTemporal PerceptionDuration PerceptionTemporal ReasoningTemporal Gap EstimationTemporal Order\nSpatialitySpatial PerceptionSpatial Reasoning\nObjectObject RecognitionObject AttributeObject InteractionObject ReasoningObject ExistenceObject Permanence\nActionAction RecognitionAction LocalizationAction SequenceAction Reasoning\nSceneScene RecognitionScene Transition\nEventEvent SequenceKey Event ExtractionOverall Summarization\n(a) Task Taxonomy of VideoMarathon\nPanda-70M\n62.3%Ego4D\n21.3%YouCook2\n5.5%MovieChat-1K\n2.7%\nActivity/glyph1197et\n8.2%\n(b) Data Source\nScene\n6.7%\nTemporality\n25.3%\nSpatiality\n10.1%Object\n30.2%Action\n20.1% Event\n7.6% (c) Question Types\n0 10 20 30 40 50 60\nDuration (min)010002000300040005000#Videos (d) Video Duration\n0 20 40 60\nEvent Counts0200040006000800010000#Video (e) Event Counting\nFigure 1: VideoMarathon: A diverse long video instruction-following dataset . (a) The dataset contains 22\ndiverse tasks, covering both short-form (yellow tag) and long-form (redtag) comprehension. (b) The dataset\nspans diverse video source domains. (c) The dataset features a wide range of question types for long-form\nvideo-language modeling. (d) The dataset consists of long videos ranging from three minutes to one hour. (e)\nThe dataset includes complex video content reflected by the number of events per video.\nLMMs for Long Video-Language Modeling . Recent advanced LMMs have shown significant\npromise in long video-language modeling. Two main approaches facilitate long-form video under-\nstanding: Compression-based and extension-based methods. Compression-based methods compress\ninput video tokens across temporal, spatial, and user question-guided dimensions, allowing Video-\nLMMs to process fewer but more informative video tokens. Temporal compression techniques include\nkeyframe selection [ 42,47] and slow-fast sampling [ 12]. Spatial compression involves average pool-\ning [ 66] and token merging [ 21]. Other methods [ 43,53] apply joint temporal-spatial compression.\nUser question-guided methods [ 11,42] retain tokens based on question relevance. Inspired by the\nsuccess of long-context LLMs [ 14,29,31,46], extension-based methods aim to expand the context\nwindow for long video processing. LongV A [ 64] extrapolates the language model’s context to handle\nover 200K visual tokens. LongVILA [ 59] further scales to 2M context length using a multi-modal\nsequence parallelism system with large computational resources.\n3 VideoMarathon: A Synthetic Long Video Instruction-following Dataset\nThis section elaborates on details of constructing VideoMarathon, a long video instruction-following\ndataset with a total duration of around 9,700 hours , consisting of 3.3M QA pairs across 22 tasks .\nTask Taxonomy . A comprehensive long video instruction-following dataset requires a well-defined\nand comprehensive task taxonomy. Inspired by existing video-language benchmarks [ 4,13,25,40],\nwe introduce a comprehensive task taxonomy designed for long video-language understanding. As\nshown in Figure 1a, the task taxonomy consists of 22 QA tasks across six fundamental topics,\nincluding temporality, spatiality, object, action, scene, and event. In particular, the temporal topic\ncaptures long-term dependencies, while the spatial topic improves localization. Objects and actions\nare essential for fundamental video perception, and the scene offers contextual grounding and\ntransition cues. Event topic supports long-range understanding of key video content. Furthermore, the\ntask taxonomy requires Video-LMMs to achieve both long-form andshort-form video comprehension.\nPlease refer to Figure 11 for detailed descriptions of the above 22 QA tasks in Appendix A.\nVideo Sources . High-quality video content is essential for effective video-language modeling. To en-\nsure a sufficient amount of long videos while maintaining video quality and diversity, VideoMarathon\nintegrates five representative public video datasets: Panda-70M [ 7], Ego4D [ 15], ActivityNet [ 3],\nYouCook2 [67], and MovieChat-1K [44]. Figure 1b presents the proportions of these video sources.\nPanda-70M forms the majority of VideoMarathon, covering diverse topics such as daily activities,\nsports, science, arts, transportation, travel, education, wildlife, entertainment, and industry. In addi-\ntion, domain-specific datasets further enrich VideoMarathon: Ego4D captures real-world activities\nfrom a first-person view; ActivityNet covers a broad range of human actions; YouCook2 focuses on\n3\n--- Page 4 ---\ncooking scenarios; and MovieChat-1K contributes content from movies and TV series. To enhance\ncontent complexity within videos, we select videos with at least three events. Figure 1e shows the\ndistribution of event counts per video in the VideoMarathon dataset. Finally, we collect 28K videos\nfrom diverse domains, supporting robust long-form video-language modeling.\nHierarchical Video Captioning . In contrast to short-video QA, which typically relies on concise\ncaptions, long-video QA demands hierarchical contextual understanding, including detailed clip-wise\ncontent, event-level structures, and global narratives. To address this need, we introduce a hierarchical\nvideo captioning pipeline for capturing both fine-grained and high-level semantics over extended\ntemporal ranges. These captions later serve as the essential inputs for generating reliable, diverse,\nand context-rich QA training samples tailored to long video-language understanding.\n•Clip-Level Video Captioning . Instead of generating a single brief summary for each video\nclip [ 6,59,66], VideoMarathon provides detailed descriptions (as shown in Figure 12a) for each\nvideo clip from six perspectives: temporality, spatiality, object, action, scene, and overall summary.\nThese detailed, chronologically ordered descriptions serve as reliable contexts for the subsequent\ndiverse QA generation. Technically, we leverage Qwen2VL-7B [ 51], a powerful yet lightweight\nLMM as the video clip captioner, following the existing practice [68].\n•Event-Level Video Captioning . In this step, we first identify event boundaries ( i.e., start and end\ntimestamps) based on the chronologically ordered clip-level captions. Once the event boundaries\nare determined, we aggregate all detailed captions within each event to generate a comprehensive\nevent-level summary. Specifically, we use DeepSeek-V3 [ 30] to summarize the six-aspect clip-\nlevel captions along with the corresponding adjacent event descriptions into a cohesive event-level\nsummary. As shown in Figure 12c, this process results in detailed event-level video captions, each\nassociated with its corresponding event boundaries.\n•Global-Level Video Captioning . This step focuses on generating a natural-toned and global-\nlevel description for the entire video. Specifically, we feed the event-level captions along with\ntheir boundaries into DeepSeek-V3, obtaining a cohesive and comprehensive video summary.\nFigure 12d shows that these captions provide global context for the subsequent QA generation.\nIn addition, Appendix A presents the detailed prompts used for the hierarchical video captioning,\nincluding Figure 5 for clip-level video captioning, Figure 6 for event splitting, Figure 7 for event-level\nvideo captioning, and Figure 8 for global-level video captioning.\nDiverse QA Generation for Long Videos . Leveraging the multi-level video captions, we generate\nhigh-quality and diverse QA pairs across 22 tasks, spanning six major topics in both open-ended\nand multiple-choice formats. To achieve this, we design topic-specific prompts that incorporate (1)\ndetailed instructions for QA generation, (2) topic-specific task descriptions ( e.g., scene recognition\nand transitions within the scene topic), (3) clip-level captions of the corresponding topic following\nthe chronological order, (4) the comprehensive global-level captions, and (5) high-quality QA demo\nexamples sourced from established video benchmarks [ 4,13,25,40]. These structured prompts\nfacilitate the creation of diverse, contextually grounded QA pairs tailored to the unique challenges of\nlong-video understanding. Please refer to Appendix A for the detailed prompts of open-ended QA\n(Figure 9) and multiple-choice QA (Figure 10) generation.\nComparison . Table 1 presents a comparison between our proposed VideoMarathon dataset and exist-\ning video instruction-following datasets [ 6,66]. The most significant distinction is VideoMarathon’s\nsubstantially longer total video time and average video length. Moreover, VideoMarathon includes\nvideos ranging from 3 to 60 minutes in duration, filling the gap in long-form video instruction training\ndata. Also, the diverse QA tasks and the balanced distribution of open-ended and multiple-choice QA\npairs enable Video-LMMs to better handle a wide range of challenging real-world questions.\n4 Hour-LLaV A: An Efficient Hour-scale Video-LMM\nProblem Formulation . Standard Video QA tasks require video-language models to generate an\nanswer Xa, based on a given input video Xvand a user question Xqby modeling the conditional\nprobability of pθ(Xa|Xv,Xq), where θis the parameter of video-language models. More specifically,\nthe input video is first fed into a vision encoder fθV(·)parameterized by θVto produce the visual\nfeature Zv=fθV(Xv). Then, a projection matrix Wconvert the visual feature Zvto a visual\nembedding Hv=W·Zvin the language embedding space. Meanwhile, the input user question Xq\n4\n--- Page 5 ---\nUser QuestionMemAug ModuleInput Video Features at 1-FPS Sampling\nxNTime\nMemory RepositoryForgetting Mechanism\nFull Video TokenDecayed Video TokenQuestion TokenMemory-Augmented Video TokenFinal ResponseHqHqHvHv^Hv~Xa\nSelfAttentionCrossAttentionFFNLLM DecoderFigure 2: Overview of the Hour-LLaV A Framework. Input video features Hvencoded from 1-FPS sampled\nframes are selectively decayed spatially and temporally through a forgetting mechanism, producing decayed\nvideo tokens ˜Hvfor efficient video modeling. Meanwhile, full video features Hvare stored in a memory\nrepository. Given the decayed tokens ˜Hvand a user question tokens Hq, the MemAug module enhances them\nwith full video context and user question-relevant details from the memory repository, obtaining memory-\naugmented video tokens ˆHv. These augmented tokens are then passed with the original user question tokens Hq\ninto the LLM decoder to generate the final response Xa.\nis projected to language embeddings Hqby a word embedding model. Finally, the language model\nfθL(·)predicts the answer Xa=fθL(Hv,Hq), where θLis the parameter of the language model.\nMemory Augmentation for Long Video Modeling . For modeling long videos spanning hours, it is\nimpractical to process all frames densely by LLMs due to the GPU memory constraints. To address\nthis challenge, we draw inspiration from the human memory system, which selectively retains and\nrecalls essential past experiences while systematically discarding irrelevant or redundant information,\nstriking a balance between efficiency and comprehensiveness [ 2,10]. Motivated by this efficient\nmemory system, we design a memory augmentation mechanism to enable hour-scale video-language\nunderstanding. This mechanism consists of three main components: a memory repository, a forgetting\nmechanism, and a MemAug module. Together, they allow the model to operate on compressed\nrepresentations while maintaining access to the full video context.\n•Memory Repository . We store the high-fidelity video features Hv, extracted at 1 FPS, into a\nmemory repository that serves as long-term memory. It enables the model to retain the full video\ncontext without requiring the LLM decoder to consume every frame.\n•Forgetting Mechanism . Due to the GPU memory constraints, we employ a forgetting mechanism\nMforget to compresses the full video tokens Hvinto a reduced set of decayed video tokens\n˜Hv=Mforget(Hv)by discarding the tokens in both spatial and temporal spaces. Technically, the\nforgetting mechanism can be implemented by various token compression strategies [ 20,42,43,66].\nIn Section 5.4, we present a comprehensive comparison among different forgetting mechanisms.\n•MemAug Module . The MemAug module is designed to recover the information discarded\nduring the forgetting stage from the memory repository, as well as gather content relevant to\nthe user’s question. It is implemented using standard transformer blocks [ 50]. Specifically, in\ncross-attention, the decayed video tokens ˜Hvand user question tokens Hq(as queries) attend\nand collect information from the memory repository Hv(as keys and values). In self-attention,\nquestion-relevant video information flows from the user question tokens Hqto the decayed video\ntokens ˜Hv. Through this process, the decayed video tokens ˜Hvare enriched with both full video\ncontext and question-specific details, resulting in memory-augmented video tokens ˆHvthat support\nlong-dependence modeling for video understanding:\nˆHv=fθM(˜Hv,Hq|Hv), (1)\nwhere θMdenotes the parameters of the MemAug module fθM(·).\nHour-LLaV A . Powered by memory augmentation, we propose Hour-LLaV A, an efficient video-\nlanguage model capable of modeling hour-long videos at 1 FPS. It comprises three key modules:\na video encoder, a memory augmentation module ( i.e., MemAug), and an LLM decoder. Figure 2\nshows the Hour-LLaV A framework, with the video encoder omitted for simplicity.\n5\n--- Page 6 ---\n•Video Encoding . Following existing Video-LMMs [ 66,68], we adopt SigLIP [ 63] followed by a\nvision-language projector for video encoding. Specifically, we sample video frames at 1 FPS and\nextract visual features Zv, which are then average-pooled to a fixed spatial resolution of 8×8per\nframe. We then project the average-pooled visual features into the language embedding space via\na two-layer MLP with GELU [18] activation, obtaining the full video tokens Hv.\n•MemAug . The decayed video tokens ˜Hvare obtained by “forgetting” ∼94%full video tokens\nwith a compression ratio of1/16through the forgetting mechanism. It is achieved by discarding\ntokens with a compression ratio of1/4in spatial and temporal dimensions, respectively. These\ndecayed tokens are then enriched with full video tokens by the MemAug module, which is\nimplemented with N= 4transformer blocks.\n•LLM Decoder . Given the memory-augmented video tokens ˆHvand the user question tokens Hq,\nLLM decoder generates final response Xa. We employ Qwen2.5-3B-Instruct [ 61] and Qwen2-7B-\nInstruct [60] as the LLM decoders for Hour-LLaV A-3B and Hour-LLaV A-7B, respectively.\nTraining Schedules . The training schedules of Hour-LLaV A follow three stages: image-language\npretraining, video-language adaptation, and video instruction tuning. Table 6 presents more details of\nthe training schedules.\n•Image-Language Pretraining . We perform image-language pretraining with 3B image-text pairs\nfrom the single-image subset of LLaV A-OV [ 23]. Full-resolution image tokens are stored in the\nmemory repository, from which1/4tokens are retained as decayed visual tokens via the spatial\nforgetting mechanism. Only the Transformer blocks of MemAug are trained for one epoch.\n•Video-Language Adaptation . Following the data composition that adheres to established prac-\ntices [ 66,68], the model is adapted to video-language inputs using a small amount of mixture of\n0.12M single-image, 0.05M multi-image, and 0.09M text data from LLaV A-OV , plus 0.3M short\nvideo samples from LLaV A-Video-178K [ 66]. A1/4token compression is applied in both spatial\nand temporal dimensions via the forgetting mechanism. All model parameters are trained for one\nepoch. This stage intends to slightly adapt all model parameters to the video domain, thereby\nconstructing a strong baseline for the subsequent long-video training stage.\n•Video Instruction Tuning . This stage is supervised by instruction-following data involving long\nvideo content. For training efficiency, up to 5 QA pairs from the same VideoMarathon video are\ngrouped as a multi-turn conversation per training sample. The training corpus includes 1.14M\nsingle-image, 0.5M multi-image, and 0.81M text samples from LLaV A-OV , 1.3M short video\nsamples from LLaV A-Video-178K, and 0.7M long video samples from VideoMarathon. We apply\nthe same forgetting mechanism as the previous stage. The vision encoder is frozen, while the rest\nof the model is fine-tuned.\n5 Experiments\n5.1 Experimental Setting\nEvaluation Benchmarks . We evaluate our models on four mainstream video-language benchmarks:\nTempCompass [ 33], LongVideoBench [ 54], Video-MME [ 13], and LVBench [ 52].TempCompass fo-\ncuses on assessing the temporal reasoning ability of Video-LMMs for short videos. LongVideoBench\nconsists of varying-length web-collected videos up to one hour with their subtitles across diverse\nthemes, evaluating the abilities in retrieving and reasoning over detailed information from long videos.\nVideo-MME is a comprehensive multimodal benchmark designed to evaluate long video understand-\ning across diverse video types and temporal spans. LVBench challenges models to demonstrate\nlong-term memory and extended comprehension across multimodal inputs.\nImplementation Details . Following existing practices [ 42,64,66], we initialize our Hour-LLaV A\nmodels with pretrained Image-LMMs. Specifically, the vision encoder and the LLM decoder of\nHour-LLaV A-7B are initialized from LLaV A-OV-SI-7B [ 23], which is trained solely on image data.\nDue to the absence of LLaV A-OV-SI-3B, we pretrain a 3B version of LLaV A-OV-SI using Qwen2.5-\n3B-Instruct model, and then initialize Hour-LLaV A-3B from it. For video-language training, we set\nthe global batch sizes to 128 and 256 for the 3B and 7B models, respectively. A learning rate of\n2e-5 is used with a 0.03 warmup ratio under a cosine annealing schedule. The models are optimized\nusing the AdamW [ 34] optimizer with a cross-entropy loss. We train Hour-LLaV A-3B with 64 AMD\nMI250 GPUs and Hour-LLaV A-7B with 64 AMD MI300X GPUs, respectively. We conduct all the\nablation studies using Hour-LLaV A-3B in Section 5.3, 5.5, and 5.4. Please refer to Appendix A.2 for\nmore implementation details.\n6\n--- Page 7 ---\nTable 2: Performance comparison of existing LMMs on TempCompass, LongVideoBench, VideoMME, and\nLVBench datasets. M-Avg denotes the average performance of multiple-choice tasks. Blue boxes denote the\naverage durations of benchmarks. Red highlights that LVBench’s average video length exceeds the maximum\nlength in the training stage. The symbol†marks our reimplemented results. Bold font denotes the best\nperformance among models at the same scale, while underline indicates the second-best.\nMethodLLM InputTempCompass LongVideoBench VideoMME (w/o & w/ subtitles) LVBench\nParams VideoM-Avg M-Avg Overall Medium Long Avg\n11s 459s 1021s 516s 2466s 4037s\nProprietary LMM\nGPT-4V [38] - 10 frames - 61.3 59.9/63.3 55.8/59.7 53.5/56.9 -\nGPT-4o [39] - 384 frames 70.9 66.7 71.9/77.2 70.3/76.6 65.3/72.1 48.9\nGemini 1.5 Flash [49] - 0.5/1 fps - 61.6 70.3/75.0 68.8/74.7 61.1/68.8 -\nGemini 1.5 Pro [49] - 0.5/1 fps 69.3 64.0 75.0/81.3 74.3/81.0 67.4/77.4 33.1\nOpen-source LMM ( <7B)\nVILA1.5-3B [27] 3B 8 frames 56.1 42.9 42.2/44.2 - - -\nPhi-3.5-Vision-4.2B [1] 4.2B 16 frames - - 50.8/ - - - -\nLongVU-3.2B [42] 3.2B 1 fps - - - /51.5 - - /47.2 -\nInternVL2.5-2B [8] 2B 64 frames 53.4 46.0 51.9/54.1 - - -\nApollo-1.5B [68] 1.5B 2 fps 60.8 54.1 53.0/54.6 - - -\nApollo-3B [68] 3B 2 fps 62.5 55.1 58.4/60.6 - - -\nLLaV A-Video-3B†[66] 3B 64 frames 63.4 55.2 58.7/60.7 55.2/57.3 47.0/49.9 41.7\nHour-LLaV A-3B ( ours)3B 1 fps 63.6 57.8 60.6/66.7 59.0/65.4 52.1/60.4 44.7\nOpen-source LMM (7-8B)\nVideo-LLaV A [26] 7B 8 frames 37.9 39.1 39.9/41.6 38.0/40.7 36.2/38.1 -\nVideoChat2 [25] 7B 16 frames 51.1 39.3 39.5/43.8 37.0/39.4 33.2/39.2 -\nShareGPT4Video [6] 8B 16 frames 59.4 41.8 39.9/43.6 36.3/39.3 35.0/37.9 -\nVideoLLaMA2 [9] 7B 16 frames - 51.4 47.9/50.3 37.0/39.4 33.2/39.2 -\nVideo-XL [43] 7B 1 fps - 50.7 55.5/61.0 - - -\nKangaroo [32] 8B 64 frames 62.5 54.8 56.0/57.6 55.3/55.4 46.7/49.3 39.4\nLongV A [64] 7B 128 frames - - 52.6/54.3 50.4/53.6 46.2/47.6 -\nLongVILA [59] 7B 256 frames - - 60.1/65.1 58.3/64.9 53.0/57.4 -\nLongVU [42] 7B 1 fps - - - /60.9 - - /59.5 -\nApollo-7B [68] 7B 2 fps 64.9 58.5 61.3/63.3 - - -\nLLaV A-Video-7B [66] 7B 64 frames 64.3†58.2 63.3 /69.7 58.9/62.9†53.0/55.0†42.2†\nHour-LLaV A-7B ( ours)7B 1 fps 68.1 60.4 63.6/70.2 63.8/70.0 55.0/65.1 45.6\n5.2 Main Results\nOverview . As shown in Table 2, Hour-LLaV A consistently achieves the best performance on\nfour well-established video benchmarks in both the 3B and 7-8B model size categories. Notably,\nHour-LLaV A-3B even surpasses more than half of the current 7-8B Video-LMMs.\nTempCompass . The results on TempCompass show that Hour-LLaV A maintains strong performance\non short-form video-language tasks, even after introducing long video-language training samples.\nLongVideoBench . Hour-LLaV A demonstrates state-of-the-art performance among open-source\nmodels across both the 3B and 7B parameter scales on LongVideoBench. Specifically, Hour-LLaV A-\n3B and Hour-LLaV A-7B outperform the second-best model by 2.6 and 1.9 points, respectively.\nVideoMME . Hour-LLaV A consistently outperforms other open-source models at both the 3B and 7B\nscales on VideoMME. Remarkably, Hour-LLaV A achieves significantly higher performance in both\nmedium- and long-length video settings, highlighting its strong capability in long video understanding\nLVBench . The maximum video length of LVBench is more than two hours, and its average video\nlength (67 minutes) already exceeds the maximum video length in our training (60 minutes). As such,\nthis benchmark serves as a pressure test for long video understanding and highlights the model’s\nability to generalize beyond its training distribution. As shown in Table 2, Hour-LLaV A achieves\nleading performance, outperforming the second-best models by 3.0 and 3.4 points under the 3B and\n7B settings, respectively.\n5.3 Ablation Study for VideoMarathon\nWe compare our proposed VideoMarathon dataset with LLaV A-Video-178K [ 66], the largest publicly\navailable video instruction tuning dataset to date. For a fair comparison, we construct two subsets\nfrom VideoMarathon (V .M.) and LLaV A-Video-178K (L.V .), matched in both the number of videos\nand the number of video-language instruction training samples. Specifically, we randomly select\n70K video-language instruction samples over 10K videos from each video dataset. The training\n7\n--- Page 8 ---\n58.058.559.059.560.060.5\n(a) TempCompass (Short)\n51.552.052.553.053.554.0\n (b) LongVideoBench (Long)\n38.038.539.039.540.040.5\nMethods\nHour-LLaV A-3B\nLLaV A-Video-3B\nData Mix Ratios\n    0% V .M. + 100% L.V .\n  25% V .M. +   75% L.V .\n  50% V .M. +   50% L.V .\n  75% V .M. +   25% L.V .\n100% V .M. +     0% L.V . (c) LVBench (Long)\nFigure 3: Dataset ablation and methodology comparison . The analysis is evaluated across three benchmarks:\n(a) TempCompass, (b) LongVideoBench, and (c) LVBench. It presents the performance of Hour-LLaV A-3B and\nLLaV A-Video-3B models. The x-axis represents different training data mixture configurations, with exact ratios\nindicated in the legend. V .M. and L.V . refer to the VideoMarathon and LLaV A-Video-178K datasets.\ndata also includes the same 30K single-image, 10K multi-image, and 20K text-only samples from\nLLaV A-OV [23] to support multimodal learning.\nFigure 3 shows how performance changes with different mixtures of V .M. (long-video) and L.V .\n(short-video) training data. On the far left of the x-axis, the model is trained with 70K short-video\nsamples from L.V . only. Moving right, these short-video samples are gradually replaced by long-video\nsamples from V .M., until reaching 100% V .M. on the far right. The pie charts visualize the data\nmix ratios at each point. The results of Hour-LLaV A ( ) on long-video benchmarks (Figure 3b\nand 3c) show a clear improvement as the proportion of V .M. data increases, peaking when the ratio\nof long- to short-video samples is 3:1, and then declining when L.V . data is entirely absent. These\nresults highlight the crucial role of VideoMarathon in improving long video understanding. They\nalso suggest that mixing different datasets can be beneficial even when the training and testing video\nlengths differ, possibly due to the increased diversity introduced by heterogeneous data sources. In\naddition, Figure 3a shows that incorporating long videos from VideoMarathon in training does not\nconsiderably affect performance on short video understanding. In the following ablation experiments,\nwe use the same data recipe, along with a mixture of long- and short-video at a 3:1 ratio.\nUsing our VideoMarathon dataset, we also train LLaV A-Video [ 66], a representative Video-LMM\nthat employs sparse temporal compression by uniformly sampling 64-frame video features as input\nto the LLM decoder. As shown in Figure 3, LLaV A-Video ( ) does not benefit from an increased\nproportion of long-video training samples; its performance even declines after training with long\nvideos. It highlights that sparse sampling fails to learn effective patterns from long videos . In contrast,\nHour-LLaV A ( ) effectively models long videos by leveraging access to a memory repository that\nstores full dense video content at 1 FPS, showcasing the distinct capability of our Hour-LLaV A in\nmodeling long videos effectively.\n5.4 Ablation Study for Hour-LLaV A\nForgetting Mechanisms . The forgetting mechanism is proposed to compress full video tokens into\ndecayed video tokens to reduce the number of tokens processed by the LLM decoder. The decayed\ntokens will then be augmented by the MemAug module to retain or recall the informative semantics.\nThe forgetting mechanism can be implemented with different token compression strategies [ 20,42,\n43,66] in both spatial and temporal domains. We ablate different forgetting mechanisms guided by\nthe MemAug module as follows:\n•Spatial Forgetting (SF). We compare two straightforward methods: uniform SF (discarding\ntokens at regular intervals over the 2D image) and random SF (discarding tokens randomly)\nwith a compression ratio of1/4. We conduct this ablation during the image-language pretraining\nstage and evaluate on three popular image-language benchmarks: MMStar [ 5], ScienceQA [ 35]\n(Sci.QA), and RealWorldQA [ 55] (R.W.QA). Notably, despite using only1/4tokens, both SF\nstrategies achieve comparable performance to the base model without token compression as shown\nin Table 3. It highlights the success of our MemAug module for token compression in the image\ndomain, suggesting that MemAug can also be applied to Image-LMMs for token compression. For\nHour-LLaV A, we adopt random SF with1/4compression ratio for its simplicity and effectiveness.\n•Temporal Forgetting (TF). We evaluate four temporal compression approaches: random, uni-\nform [ 20,32,58,66], keyframe-based [ 42,47], and user question-guided temporal compres-\nsion [ 42,43]. All strategies are evaluated using a compression ratio of1/4. As shown in Table 4,\n8\n--- Page 9 ---\nTable 3: Comparison of spatial forgetting strategies.\nSpatial Forgetting Tokens/img MMStar Sci.QA R.W.QA\nLLaV A-OV-SI-3B (Base) 729 52.8 84.7 58.8\n+ Random 196 51.9 84.5 59.6\n+ Uniform 196 51.5 83.7 59.5Table 4: Comparison of temporal forgetting strategies.\nTemporal Forgetting TempCompass LongVideoBench LVBench\nUniform 59.7 54.0 40.6\nRandom 58.5 53.2 38.8\nKeyframe 59.1 53.5 40.3\nUser question-guided 56.3 53.4 39.7\n1 1/2 1/4 1/8 1/16\nTemporal Compression Ratio363840Score on LVBench\n100% 50% 25% <10%\nMemory Repository Scale373941Score on LVBench\n0 1000 2000 3000\nVideo Duration (s)02550#Tokens (K)Hour-LLaV A\nLLaV A-Video\n1 FPS Sampling\nFigure 4: Impact of compression ratio for temporal forgetting ( left), and memory repository scale ( middle ).\nComparison of the number of visual tokens input to the LLM decoder ( right ).\nuniform TF yields the best overall performance, suggesting that the MemAug module is effective\nwithout relying on manually designed compression strategies. Furthermore, we analyze the effect\nof different compression ratios for temporal forgetting on LVBench, ranging from 1 down to\n1/16as shown in Figure 4 ( left). We adopt the uniform TF with a ratio of1/4for Hour-LLaV A,\nbalancing the trade-off between performance and efficiency introduced by temporal compression.\n•Token Control for Videos with Extreme Lengths . Although forgetting mechanisms significantly\nreduce the tokens processed by the LLM decoder, they remain insufficient for extremely long\nvideos ( e.g., 24-hour videos). We set the maximum number of retained frames to 512 so as to\nconstrain the computational cost. Conversely, for extremely short videos, we enforce a minimum\nof 32 retained frames to preserve sufficient contextual information. Note that these thresholds\napply to decayed tokens only, which means Hour-LLaV A still accesses the full video context at\n1-FPS sampling via memory repository. Figure 4 ( right ) compares the number of visual tokens fed\ninto the LLM decoder across Hour-LLaV A, LLaV A-Video, and vanilla 1-FPS sampling methods.\nIt highlights that Hour-LLaV A consistently uses fewer tokens than others.\nMemory Repository . We investigate the impact of the memory repository scale on performance on\nLVBench. Figure 4 ( middle ) shows that the performance of Hour-LLaV A decreases as the memory\nrepository scale becomes smaller. This trend indicates that information loss in the memory repository\nlimits the capacity of long video understanding . Therefore, to preserve as much temporal information\nas possible, we retain video tokens sampled at 1 FPS in the memory repository.\nPlease refer to the Appendix A.2.2 for more details of the above experiments.\n5.5 Comparison with Other Video Token Compression Techniques\nTable 5: Comparison of video token compression methods .\nMethod TempCompass LongVideoBench LVBench\nHour-LLaV A 59.7 54.0 40.6\nRandom 59.6 51.7 38.1\nUniform [20, 58] 59.3 52.1 38.3\nKeyframe [42, 47] 59.1 52.0 38.9\nUser question-guided [11, 42] 56.0 51.0 37.5Early attempts on video token com-\npression [ 11,42,47] have explored\napproaches to retain or recall informa-\ntive tokens from the full video. How-\never, these approaches typically in-\nvolve hand-crafted algorithms in the\ncompression mechanism ( e.g., prede-\nfined thresholds). By contrast, Hour-\nLLaV A employs the MemAug module to compress the full video context through a learnable process\nsupervised by the video instruction data, rather than relying on manually designed algorithms. To\nvalidate the superiority of MemAug module, we compare Hour-LLaV A with several representa-\ntive video token compression methods [ 11,42,47] under the same experimental setting, including\nuniform [ 20,32,58,66], keyframe-based [ 42,47], and user question-guided [ 42,43] temporal com-\npression. We also implement random temporal compression as a baseline. For a fair comparison,\neach approach passes the same total number of compressed tokens to the LLM decoder. Specifically,\nwe apply random spatial compression with a ratio of1/4and conduct different temporal compression\ntechniques with a ratio of1/4on 1 FPS input videos. The implementation details of the methods\nused in the comparison are described in the Appendix A.2.2. Table 5 shows that Hour-LLaV A\noutperforms all other common video token compression techniques, demonstrating the effectiveness\nof our learnable compression method ( i.e., the MemAug module).\n9\n--- Page 10 ---\n6 Conclusion\nIn this study, we introduce VideoMarathon, a large-scale video instruction-following dataset. Com-\nprising long videos with a total duration of around 9,700 hours and 3.3M QA pairs, VideoMarathon\ncovers 22 challenging tasks that require both short- and long-term video understanding. Building\non VideoMarathon, we propose Hour-LLaV A, an efficient and powerful Video-LMM optimized for\nhour-scale video modeling. By leveraging a memory augmentation (MemAug) mechanism, Hour-\nLLaV A effectively integrates information from the full video context while maintaining efficiency\nthrough 1-FPS sampling. Extensive evaluations on several video-language benchmarks validate the\nhigh quality of the VideoMarathon dataset and the superiority of the Hour-LLaV A model.\nReferences\n[1]Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit\nBahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: A highly capable\nlanguage model locally on your phone, 2024. arXiv:2404.14219 , 2024.\n[2] Alan Baddeley. Working memory. Memory , 2020.\n[3]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity understanding. In CVPR , 2015.\n[4]Keshigeyan Chandrasegaran, Agrim Gupta, Lea M Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre,\nZane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour video-language understanding.\nInNeurIPS , 2024.\n[5]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,\nYu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In\nNeurIPS , 2024.\n[6]Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan,\nBin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better\ncaptions. In NeurIPS , 2024.\n[7]Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun\nJeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos\nwith multiple cross-modality teachers. In CVPR , 2024.\n[8]Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\nHao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models\nwith model, data, and test-time scaling. arXiv:2412.05271 , 2024.\n[9]Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi\nZhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio\nunderstanding in video-llms. arXiv:2406.07476 , 2024.\n[10] Nelson Cowan. What are the differences between long-term, short-term, and working memory? Progress\nin brain research , 2008.\n[11] Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, TaoZhong, Hao Cheng, Bolin Li, Wanggui\nHe, Fangxun Shu, and Hao Jiang. Streaming video question-answering with in-context video KV-cache\nretrieval. In ICLR , 2025.\n[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video\nrecognition. In CVPR , 2019.\n[13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\nYunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of\nmulti-modal llms in video analysis. In CVPR , 2025.\n[14] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data\nengineering for scaling language models to 128k context. arXiv:2402.10171 , 2024.\n[15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar,\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In CVPR , 2022.\n10\n--- Page 11 ---\n[16] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark for\ncompositional spatio-temporal reasoning. In CVPR , 2021.\n[17] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries from\nuser videos. In ECCV , 2014.\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415 , 2016.\n[19] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization\nwith temporal prompt instruction tuning. In AAAI , 2025.\n[20] Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding\nYu, Kurt Keutzer, et al. Token-efficient long video understanding for multimodal llms. arXiv:2503.04130 ,\n2025.\n[21] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual\nrepresentation empowers large language models with image and video understanding. In CVPR , 2024.\n[22] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song.\nAcav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In\nICCV , 2021.\n[23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 ,\n2024.\n[24] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023.\n[25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\nLuo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In CVPR , 2024.\n[26] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaV A: Learning united\nvisual representation by alignment before projection. In EMNLP , 2024.\n[27] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training\nfor visual language models. In CVPR , 2024.\n[28] Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Jiebo Luo. Videoxum:\nCross-modal visual and textural summarization of videos. IEEE Transactions on Multimedia , 2023.\n[29] Jingyang Lin, Andy Wong, Tian Xia, Shenghua He, Hui Wei, Mei Han, and Jiebo Luo. Facilitating long\ncontext understanding via supervised chain-of-thought reasoning. arXiv:2502.13127 , 2025.\n[30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv:2412.19437 , 2024.\n[31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite\ncontext. arXiv:2310.01889 , 2023.\n[32] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua\nWu, and Jie Hu. Kangaroo: A powerful video-language model supporting long-context video input.\narXiv:2408.15542 , 2024.\n[33] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, Lu Hou,\nAndre Martins, and Vivek Srikumar. TempCompass: Do video LLMs really understand videos? In ACL\nFindings , 2024.\n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR , 2019.\n[35] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. In NeurIPS , 2022.\n[36] Alexander Martin, Reno Kriz, William Gantt Walden, Kate Sanders, Hannah Recknor, Eugene Yang,\nFrancis Ferraro, and Benjamin Van Durme. Wikivideo: Article generation from multiple videos.\narXiv:2504.00939 , 2025.\n11\n--- Page 12 ---\n[37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In\nCVPR , 2019.\n[38] OpenAI. Gpt-4v. https://openai.com/index/gpt-4v-system-card/ , 2023.\n[39] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/ , 2024.\n[40] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse,\nSkanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic\nbenchmark for multimodal video models. In NeurIPS , 2023.\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML , 2021.\n[42] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun\nLiu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive\ncompression for long video-language understanding. arXiv:2410.17434 , 2024.\n[43] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl:\nExtra-long vision language model for hour-scale video understanding. In CVPR , 2025.\n[44] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi,\nXun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video\nunderstanding. In CVPR , 2024.\n[45] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos\nusing titles. In CVPR , 2015.\n[46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing , 2024.\n[47] Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A Plummer, Bryan\nRussell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In CVPR , 2024.\n[48] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang\nLin, Rongyi Zhu, et al. Video understanding with large language models: A survey. IEEE Transactions on\nCircuits and Systems for Video Technology , 2025.\n[49] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable\nmultimodal models. arXiv:2312.11805 , 2023.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017.\n[51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\nresolution. arXiv:2409.12191 , 2024.\n[52] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin\nXu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv:2406.08035 ,\n2024.\n[53] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long\nvideo understanding via large language models. In ECCV , 2024.\n[54] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context\ninterleaved video-language understanding. In booktitle , 2025.\n[55] xAI. Realworldqa. https://huggingface.co/datasets/xai-org/RealworldQA/ , 2024.\n[56] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to\nexplaining temporal actions. In CVPR , 2021.\n[57] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. In ACM Multimedia , 2017.\n12\n--- Page 13 ---\n[58] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and\nAfshin Dehghan. Slowfast-llava: A strong training-free baseline for video large language models.\narXiv:2407.15841 , 2024.\n[59] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang,\nShang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos.\narXiv:2408.10188 , 2024.\n[60] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671 , 2024.\n[61] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv:2412.15115 , 2024.\n[62] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A\ndataset for understanding complex web videos via question answering. In AAAI , 2019.\n[63] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image\npre-training. In ICCV , 2023.\n[64] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang,\nHaoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv:2406.16852 ,\n2024.\n[65] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan\nLi, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal\nmodels from language model reward. arXiv:2404.01258 , 2024.\n[66] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction\ntuning with synthetic data. arXiv:2410.02713 , 2024.\n[67] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web\ninstructional videos. In AAAI , 2018.\n[68] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu,\nXiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in\nlarge multimodal models. In CVPR , 2025.\n13\n--- Page 14 ---\nA Appendix\nThis document provides more implementation details of the VideoMarathon dataset construction,\nadditional experimental details, limitations, and broader impacts, organized as follows:\n•Details of VideoMarathon Construction (Section A.1). We provide the exact prompts and\nexamples for hierarchical video captioning and topic-specific QA generation.\n•More Experimental Details (Section A.2). We present additional experimental details, including\ndetailed training schedules for Hour-LLaV A and experimental settings of ablation studies in\nSections 5.3, 5.5, and 5.4.\n•Limitations (Section A.3). We discuss several limitations of this study.\n•Broader Impacts (Section A.4). We analyze both the potential positive societal impacts and the\nnegative societal impacts of this work.\nA.1 Details of VideoMarathon Construction\nA.1.1 Details of Hierarchical Video Captioning\nPrompts. We provide the exact prompts used for hierarchical video captioning, including clip-level\ncaptioning (Figure 5), event-level captioning (Figure 7), and global-level captioning (Figure 8).\nAdditionally, the prompt used for event splitting is presented in Figure 6.\nExamples . Figure 12 presents several examples of hierarchical video captions, including clip-level,\nevent-level, global-level video captions, and event splits. In addition, Figure 13 presents the word\ncloud of all the global-level video descriptions in the VideoMarathon dataset.\nA.1.2 Details of QA Generation\nPrompts . We present the exact prompts used for topic-specific QA generation, including the open-\nended (OE) QA prompt in Figure 9 and the multiple-choice (MC) QA prompt in Figure 10. Also, the\ndetailed descriptions of the 22 sub-tasks across six core topics are provided in Figure 11. Please refer\nto the data file for exact QA examples from the VideoMarathon dataset.\nA.2 Additional Experimental Details\nA.2.1 Detailed Training Schedules for Hour-LLaV A\nIn Section 4, we briefly introduce the training schedules of Hour-LLaV A. Furthermore, Table 6\npresents the detailed training schedules for each training stage of Hour-LLaV A, containing compres-\nsion details, data usage, and training hyperparameters.\nA.2.2 Experimental Settings of Ablation Studies.\nDue to page limitations, we are unable to include all experimental settings for the ablation studies in\nSection 5. Below, we provide the experimental settings for each ablation study. We conduct all the\nablation studies using the Hour-LLaV A-3B model.\nAblation Study for VideoMarathon (Section 5.3). In this section, we investigate how performance\nchanges with different mixtures of VideoMarathon (long-video) and LLaV A-Video-178K (short-\nvideo) training data. We construct two subsets from VideoMarathon and LLaV A-Video-178K,\nmatched in both the number of videos and the number of video-language instruction training samples.\nEach sub-dataset consists of 70K video-language instruction samples over 10K videos. We then train\nthe Hour-LLaV A-3B model on different mixtures of these two subsets (as shown in Figure 3), while\nalso incorporating the same 60K multimodal samples from LLaV A-OV [ 23] (i.e., 20K text-only,\n30K single-image, and 10K multi-image samples) to support multimodal learning. In addition, the\nHour-LLaV A-3B model is initialized from the checkpoint obtained after image-language pretraining\n(Stage 1 in Table 6). All training hyperparameters follow the settings used for video instruction\ntuning of the 3B model, as detailed in Table 6.\nImplementation Details of Temporal Compression Techniques (Section 5.5). We compare the\nproposed MemAug with several existing video token compression techniques, including uniform [ 20,\n14\n--- Page 15 ---\nImage-Language Pretraining Video-Language Adaptation Video Instruction Tuning\n3B 7B 3B 7B 3B 7BCompressionFPS 1 1 1 1 1 1\nSpatial Forgetting (SF) 1/4 1/4 1/4 1/4 1/4 1/4\nSF Mechanism Random Random Random Random Random Random\nTemporal Forgetting (TF) - - 1/4 1/4 1/4 1/4\nTF Mechanism - - Uniform Uniform Uniform UniformDataData Type SI SI T + SI + MI + SV T + SI + MI + SV T + SI + MI + SV + LV T + SI + MI + SV + LV\nData Sources OV-SI OV-SI OV-SI + L.V . OV-SI + L.V . OV-SI + L.V . + V .M. OV-SI + L.V . + V .M.\n#Samples 3B 3B 0.6M 0.6M 4.4M 4.4MTrainingBatch Size 128 128 128 128 128 256\nLR of Vision Encoder 0 0 5×10−65×10−60 0\nLR of Projector 0 0 1×10−41×10−41×10−41×10−4\nLR of MemAug 1×10−41×10−41×10−41×10−41×10−41×10−4\nLR of LLM 0 0 2×10−51×10−52×10−52×10−5\nEpoch 1 1 1 1 1 1\nTable 6: Detailed training schedule for each training stage of Hour-LLaV A , including compression\ndetails, data usage, and training hyperparameters. In Data part, we use the following abbreviations: T\nfor text, SI for single-image, MI for multi-image, SV for short-video, LV for long-video, OV-SI for\nLLaV A-OV-SI [23], L.V . for LLaV A-Video-178K [66], and V .M. for VideoMarathon.\n58], keyframe [ 42,47], and user question-guided [ 11,42] temporal compression. Also, we implement\nrandom temporal compression as a reference baseline. In particular, the implementation details of\ndifferent temporal compression methods are shown below:\n•Random . We randomly select1/4frames from a video sampled at 1 FPS.\n•Uniform . We uniformly sample1/4frames across the temporal dimension from a 1 FPS video.\n•Keyframe . Following [ 42], for each frame, we compute the average cosine similarity with its K\nnearest temporal neighbors (with K= 8). We then retain the1/4frames that are least similar to\ntheir neighbors, removing the3/4most redundant frames. In addition, video frame features are\nextracted by SigLIP vision encoder [63].\n•User question-guided . Following [ 11], we calculate the cosine similarity between the frame\nembedding vand question embedding q. Next, we select1/4frames with the highest similarity\nscores. In particular, the frame embedding vis computed as v=1\nNvPNv\ni=1vi, where Nvis the\nnumber of tokens per frames and virefers to the i-th visual token vector. The question embedding\nis calculated as q=1\nNqPNq\ni=1qi, where Nqis the number of tokens in the given question and qi\ndenotes the i-th query token vector. Additionally, viis obtained using the SigLIP vision encoder\nfollowed by the projector, while qiis derived from the Qwen2.5-3B word embedding model.\nFor a fair comparison, all the above temporal compression techniques apply different temporal\ncompression strategies with the same compression ratio of1/4on 1-FPS input videos, along with a\nrandom spatial compression with a compression ratio of1/4.\nFollowing a similar training configuration to Section 5.3, we use a training set composed of 70K video-\nlanguage instruction samples with a 75%/25% mixture of VideoMarathon and LLaV A-Video-178K,\nrespectively, and involving 60K multimodal samples from LLaV A-OV . All training hyperparameters\nare aligned with the 3B model’s video instruction tuning setup in Table 6.\nAblation Study for Hour-LLaV A (Section 5.4). In this section, we primarily conduct analysis on\ntwo key components in Hour-LLaV A: forgetting mechanisms and memory repository.\nWe ablate forgetting mechanisms from both spatial and temporal perspectives.\n•Spatial Forgetting . In this setting, we adopt 3B LLaV A-OV-SI data for training. The vision\nencoder, projector, and LLM decoder of the Hour-LLaV A-3B model are initialized by a pretrained\nLLaV A-OV-SI-3B model as mentioned in Section 5.1. During training, only the parameters of the\nMemAug module are tuned, while the rest of the model remains frozen. Training hyperparameters\nfollow the settings used for image-language pretraining of the 3B model in Table 6.\n•Temporal Forgetting . We follow the same temporal compression implementations as in Sec-\ntion 5.5. Note that for training, we also adopt the same as in Section 5.5, using a dataset composed\nof 70K video-language instruction samples with a 75%/25% mixture of VideoMarathon and\n15\n--- Page 16 ---\nLLaV A-Video-178K, along with 60K multimodal samples from LLaV A-OV . The Hour-LLaV A-\n3B model is initialized from the checkpoint obtained after image-language pretraining. All training\nhyperparameters follow the video instruction tuning setup for the 3B model.\nFormemory repository , we mainly analyze the impact of the memory repository scale in Figure 4\n(middle ). The 100% scale refers to storing full video tokens sampled at 1 FPS. For the 50% and 25%\nscales, we uniformly retain 50% and 25% of the frame-level features along the temporal dimension,\nrespectively. The <10% scale denotes a lightweight configuration in which only decayed video\ntokens are replaced in the memory repository, resulting in fewer than 10% of the full video token\ncount. For the training, we follow the same setup as in Section 5.5. All training hyperparameters are\nconsistent with the video instruction tuning configuration of the 3B model in Table 6.\nA.3 Limitations\nDespite the promising results of Hour-LLaV A, several limitations remain. First, due to the lack\nof comprehensive evaluation metrics for hour-long video-language understanding, multiple-choice\nQA remains the most practical task for evaluating long video-language models. However, this\nevaluation format is limited in scope and fails to assess the broader capabilities of Video-LMMs. The\ndevelopment of more diverse and holistic benchmarks is therefore essential for advancing this field.\nSecond, Hour-LLaV A is trained on large-scale synthetic instruction-following data, which inevitably\ncontains noise. The current training pipeline does not explicitly consider this issue, and future work\ncan further explore noise-robust training strategies. Third, the current framework is limited to video\nand language modalities, neglecting audio, a crucial component in many long-form videos such as\nlectures, interviews, and documentaries. Incorporating audio or additional modalities could further\nenhance the model’s capacity for comprehensive multimodal understanding.\nA.4 Broader Impacts\nThis study presents significant advancements in long-form video-language modeling through the in-\ntroduction of the VideoMarathon dataset and the Hour-LLaV A model. Positively, the VideoMarathon\ndataset paves the way for more sophisticated AI systems capable of handling realistic, long-duration\nscenarios, which are essential for practical applications in education, security, autonomous driving,\nand augmented or virtual reality. However, potential negative impacts include the risk of misuse\nin surveillance, such as continuous monitoring and profiling of individuals in public or private set-\ntings without consent, and the possibility of misinterpreting nuanced or sensitive content in long\nvideos, which might lead to harmful decisions in critical domains like healthcare or security. These\nimplications highlight the importance of responsible development practices, including robust privacy\nsafeguards and clear ethical guidelines for the deployment of long-form video-language models.\n16\n--- Page 17 ---\nClip-level Video CaptioningYou are given a 30-second video clip. Your task is to generate a detailed, structured description of the video based on six specific perspectives. ### Instructions:Please ensure your descriptions are vivid, precise, and rich in detail. For each of the six topics below, please provide a comprehensive caption. Your goal is to clearly convey the video's visual and contextual content from multiple dimensions:•Temporality: Describe how the scene evolves over time. Highlight key transitions, unfolding actions, or changes from the beginning to the end of the video.•Spatiality: Describe the spatial layout of the scene. Explain how key elements are positioned and oriented, and how they relate to one another within the frame.•Object: Identify key objects in the video, including inanimate items and humans. Describe their appearance, clothing, physical traits, materials, and possible roles.•Action: Describe the main actions taking place. Specify what actions occur, who or what performs them, and the manner or sequence in which they unfold.•Scene: Provide a high-level overview of the setting. Describe the environment, background, and general activity or context presented in the video. •Summary: Offer a brief yet comprehensive summary that contains the core event or purpose of the given video clip.### Output Format (JSON):Your response should be formatted as a JSON object, where each key corresponds to a topic and each value is the description associated with that topic.#### Example Output:```json {\"Temporality\": \"The video begins with the group standing idle under the tree and progresses to active conversation and movement, suggesting preparation for an activity.\", \"Spatiality\": \"The individuals are grouped under a large tree, spaced out in a semicircle. The surrounding area is an open, grassy park with scattered trees in the background.\",\"Object\": \"The scene includes several casually dressed men wearing t-shirts, shorts, hats, and sunglasses. One man carries a medium-sized cardboard box. The background features natural objects like trees, grass, and park benches.\", \"Action\": \"The men engage in conversation, gesturing with their hands, and preparing for an activity. One man walks away, possibly to retrieve additional items.\", \"Scene\": \"The video captures a casual outdoor setting in a park. A small group of men gather under a shady tree, seemingly preparing for a social or recreational activity in a relaxed, natural environment.\", \"Summary\": \"A group of men gather under a tree in a public park, casually conversing and preparing for an activity. The video captures a moment of calm interaction and coordination in a relaxed outdoor setting.\"}```Figure 5: The prompt for clip-level video captioning.\n17\n--- Page 18 ---\nEvent SplittingYou will be given a list of video clips, where each clip is defined by a start time, end time, and its video content. Your task is to merge related clips into a small number of coherent events, each represented by a merged time span and an event title.### Instructions:Consider the context for each video clip and ignore some outliers or unreasonable video content. If the given video clips are already an event, do not merge.•Merge Related Clips: Combine consecutive clips that logically belong to the same event based on content continuity.•Preserve Context: Each merged event should represent a self-contained and contextually consistent unit of action or activity.•Filter Outliers: Ignore clips that are irrelevant, inconsistent with the surrounding context, or clearly out of place.•Balance Granularity: Avoid over-segmenting the video. Aim to minimize the number of events while ensuring that each one captures a distinct scene, set of actions, or objects.•Respect Standalone Clips: If a single clip already represents a complete, meaningful event, retain it without merging.### Output Format (JSON):Return a JSON object where:•Each key is a string denoting the merged time span of an event (e.g., \"0–40s\").•Each value is a concise event title that summarizes the main content or activity.#### Example Output:```json {\"0-40s\": \"Introduction to Holiday Desserts\",\"60-110s\": \"Preparing Ingredients and Tools\",\"120-190s\": \"Mixing Ingredients\",\"200-290s\": \"Placing Ingredients into Tools\",\"300-350s\": \"Baking of Desserts\",\"360-410s\": \"Presentation and Enjoyment of Desserts\",}```Figure 6: The prompt for event splitting.\n18\n--- Page 19 ---\nEvent-level Video CaptioningYou will be provided with a sequence of video clips depicting a specific event. Each clip includes metadata (start time, end time) and structured descriptions from six perspectives. Your task is to generate a cohesive, natural-language narrative that summarizes the entire event in chronological order. To support your understanding of the broader context, you will also receive brief descriptions of the events immediately before and after the current one. Use this surrounding context to enhance the flow and interpretation, but do not include those descriptions in your output.### Instructions:#### 1. Maintain Chronological Flow•The clip-level video captions are already chronologically ordered. Your summary must preserve this timeline and describe the event as a continuous progression.•Avoid explicitly referencing individual clips (e.g., \"The first clip shows...\" or \"As the clip progresses...\"). Instead, describe the event as a seamless and continuous narrative. •Do not assume the first or last frame of any clip represents the beginning or end of the entire event. Instead, focus on how the event unfolds as a whole.#### 2. Use Adjacent Event Context Thoughtfully•To improve coherence and contextual understanding, you will receive brief descriptions of the events that occur immediately before and after the current event.•Use this information to better understand the current event.•IMPORTANT: Do not include these descriptions in your summary. They are provided solely to help you maintain context and continuity. #### 3. Preserve Key Details, Filter Outliers•Retain all relevant details from the clip descriptions to ensure accuracy and completeness. •Based on the brief description of the current event, ignore outliers or clips with inconsistent, irrelevant, or contradictory content that do not contribute meaningfully to the event-level narrative. #### 4. Write in a Natural, Engaging Tone•Your summary should read like a natural video description, as if you are directly describing the event rather than summarizing segmented clips. •Avoid mechanical phrases often found in clip descriptions, such as \"The clip begins...\", \"As the clip progresses...\", \"The clip concludes...\", \"The first/last frame of this clip...\", \"The second clip shows...”.•Instead, focus on crafting a flowing narrative. The goal is to help a reader visualize the full event as if watching it unfold. ### Output Format (JSON):The output should be structured as a JSON object.#### Example Output:```json {\"Event-Level Description\": \"YOUR DESCRIPTION HERE.”}```### Input:•Brief description of current event: <EVENT TITLE>•The event before the current event: <PREV EVENT DESCRIPTION>•The event after the current event: <NEXT EVENT DESCRIPTION>•Detailed descriptions of all clips in the current event:•<CLIP-LEVEL DESCRIPTION 1>•<CLIP-LEVEL DESCRIPTION 2>•...•<CLIP-LEVEL DESCRIPTION N>Figure 7: The prompt for event-level video captioning.\n19\n--- Page 20 ---\nGlobal-level Video CaptioningYou will be provided with a chronologically ordered sequence of video events, each accompanied by both a brief and a detailed description. Your task is to synthesize these descriptions into a single, cohesive, and vivid narrative that summarizes the entire sequence of events as a unified whole, without breaking the flow into separate segments or referencing individual clips mechanically.### Instructions:#### 1. Event-level Video DescriptionsEach event is described with two levels of granularity:•Brief Description: A concise overview of the event.•Detailed Description: A more in-depth description, including finer details.#### 2. Generate a Unified Narrative•Write a single, flowing narrative that describes the full sequence of events from beginning to end.•Maintain chronological order without explicitly mentioning timestamps or referencing individual events (e.g., \"In the second event...\").•Ensure the narrative reads as if you are describing a continuous experience, not a series of separate parts. #### 3. Focus on Relevance and Consistency•Incorporate all meaningful and consistent details across the event descriptions.•If a detail appears inconsistent, irrelevant, or clearly unrelated, omit it based on your understanding of the overall narrative. #### 4. Use a Natural and Engaging Tone•Write in a fluent, descriptive style, suitable for someone reading or hearing a natural summary of the video.•Avoid mechanical phrases like: \"The event begins...\", \"As the event progresses...\", \"The first/last event shows...”.•Instead, immerse the reader in the experience, emphasizing continuity, clarity, and engagement.### Output Format (JSON):The output should be structured as a JSON object.#### Example Output:```json {\"Global-Level Description\": \"YOUR DESCRIPTION HERE.”}```### Input:Event from <START TIME 1> - <END TIME 1>:•Brief description: <EVENT TITLE 1>•Detailed description: <EVENT-LEVEL DESCRIPTION 1>Event from <START TIME 2> - <END TIME 2>:•Brief description: <EVENT TITLE 2>•Detailed description: <EVENT-LEVEL DESCRIPTION 2>...Event from <START TIME N> - <END TIME N>:•Brief description: <EVENT TITLE N>•Detailed description: <EVENT-LEVEL DESCRIPTION N>Figure 8: The prompt for global-level video captioning.\n20\n--- Page 21 ---\nOpen-Ended QA GenerationYou are an intelligent assistant specializing in open-ended question-answer generation for video understanding. Please follow the instructions precisely and use only the information provided in the input. Do not introduce any content unrelated to the described video clips.You will be provided with:•A chronologically ordered sequence of <TOPIC>-based video clip descriptions (each 30 seconds long, with start and end timestamps).•A global-level description summarizing the overall content of the video.Your task is to generate open-ended Question-Answer (QA) pairs from the perspective of <TOPIC>. These QA pairs should promote deep understanding and must be strictly grounded in the given descriptions. No hallucination or fabrication is allowed.### Instructions:#### 1. <TOPIC>-Based Sub-TasksThe <TOPIC>-based sub-tasks can be categorized into the following sub-tasks:• <SUB-TASK 1>: <TASK DESCRIPTION 1>• <SUB-TASK 2>: <TASK DESCRIPTION 2>•(…additional sub-tasks as needed)#### 2. QA Examples for Each Sub-TaskTo guide your generation, here are example QA pairs for each sub-task:Examples of <SUB-TASK 1>:[{ \"question\": <DEMO Q1-1>, \"answer\": <DEMO A1-1>},{ \"question\": <DEMO Q1-2>, \"answer\": <DEMO A1-2>},…]Examples of <SUB-TASK 2>:[{ \"question\": <DEMO Q2-1>, \"answer\": <DEMO A2-1>},{ \"question\": <DEMO Q2-2>, \"answer\": <DEMO A2-2>},…]…#### 3. Guidelines for Question-Answer Generation:•Focus on <TOPIC>-Relevant Information: Carefully analyze the descriptions to identify patterns relevant to the <TOPIC>.•Relevance and Context: The questions and answers must align with the content and context of the video clips. The generated question-answer pairs should not introduce information that is not present in the description.•Balance Diversity and Clarity: Create a variety of questions that collectively capture a full understanding of the topic.•Quantity: Generate exactly three QA pairs for each sub-task.### Output Format (JSON):The output should be structured as a JSON object.#### Example Output:```json {\"<Sub-Task 1>\": [{\"question\": \"<Question 1-1>\", \"answer\": \"<Answer 1-1>\"}, ...],\"<Sub-Task 2>\": [{\"question\": \"<Question 2-1>\", \"answer\": \"<Answer 2-1>\"}, ...],...}```### Input:• <TOPIC>-based Descriptions:•Clip from <START TIME 1> - <END TIME 1>: <TOPIC-SPECIFIC CLIP DESCRIPTION 1>•Clip from <START TIME 2> - <END TIME 2>: <TOPIC-SPECIFIC CLIP DESCRIPTION 2>•…•Overall Description: <GLOBAL-LEVEL DESCRIPTION>Figure 9: The prompt for open-ended (OE) question-answer generation.\n21\n--- Page 22 ---\nMultiple-Choice QA GenerationYou are an intelligent assistant specializing in multiple-choice question-answer generation for video understanding. Please follow the instructions precisely and use only the information provided in the input. Do not introduce any content unrelated to the described video clips.You will be provided with:•A chronologically ordered sequence of <TOPIC>-based video clip descriptions (each 30 seconds long, with start and end timestamps).•A global-level description summarizing the overall content of the video.Your task is to generate multiple-choice Question-Option-Answer triplets from the perspective of <TOPIC>. These triplets should promote deep understanding and must be strictly grounded in the given descriptions. No hallucination or fabrication is allowed.### Instructions:#### 1. <TOPIC>-Based Sub-TasksThe <TOPIC>-based sub-tasks can be categorized into the following sub-tasks:• <SUB-TASK 1>: <TASK DESCRIPTION 1>• <SUB-TASK 2>: <TASK DESCRIPTION 2>•(…additional sub-tasks as needed)#### 2. QA Examples for Each Sub-TaskTo guide your generation, here are example QA pairs for each sub-task:Examples of <SUB-TASK 1>:[{ \"question\": <DEMO Q1-1>, \"options\": <DEMO OP1-1>, \"answer\": <DEMO A1-1>},{ \"question\": <DEMO Q1-2>, \"options\": <DEMO OP1-2>, \"answer\": <DEMO A1-2>},…]Examples of <SUB-TASK 2>:[{ \"question\": <DEMO Q2-1>, \"options\": <DEMO OP2-1>, \"answer\": <DEMO A2-1>},{ \"question\": <DEMO Q2-2>, \"options\": <DEMO OP2-2>, \"answer\": <DEMO A2-2>},…]…#### 3. Guidelines for Question-Answer Generation:•Focus on <TOPIC>-Relevant Information: Carefully analyze the descriptions to identify patterns relevant to the <TOPIC>.•Relevance and Context: The questions, options, and answers must align with the content and context of the video clips. The generated question-option-answer triplets should not introduce information that is not present in the description.•Balance Diversity and Clarity: Create a variety of questions that collectively capture a full understanding of the topic.•Quantity: Generate exactly three QA pairs for each sub-task.### Output Format (JSON):The output should be structured as a JSON object.#### Example Output:```json {\"<Sub-Task 1>\": [{\"question\": \"<Question 1-1>\", \"options\": [...], \"answer\": \"<Answer 1-1>\"}, ...],\"<Sub-Task 2>\": [{\"question\": \"<Question 2-1>\", \"options\": [...], \"answer\": \"<Answer 2-1>\"}, ...],...}```### Input:• <TOPIC>-based Descriptions:•Clip from <START TIME 1> - <END TIME 1>: <TOPIC-SPECIFIC CLIP DESCRIPTION 1>•Clip from <START TIME 2> - <END TIME 2>: <TOPIC-SPECIFIC CLIP DESCRIPTION 2>•…•Overall Description: <GLOBAL-LEVEL DESCRIPTION>Figure 10: The prompt for multiple-choice (MC) question-answer generation.\n22\n--- Page 23 ---\nTemporality•Temporal Perception: Focuses on recognizing and interpreting the temporal flow of events within a video, including their sequencing, relative timing, and transitions between scenes. The task emphasizes understanding the local context of temporal configurations.•Duration Perception: Involves estimating the length of individual events and comparing the durations of multiple actions. The objective is to determine which activities are longer, shorter, or equal in duration. The task emphasizes understanding the global context of temporal configurations.•Temporal Reasoning: Requires logical inference based on temporal structure. This includes understanding causal relationships, predicting upcoming events, and reasoning about dependencies between temporally related actions. The task emphasizes understanding the global context of temporal configurations.•Temporal Gap Estimation: Entails estimating the elapsed time between two events. The goal is to infer the most accurate time gap based on contextual clues within the video. The task emphasizes understanding the global context of temporal configurations.•Temporal Order: Focuses on identifying the correct chronological sequence of events. This task assesses the ability to determine the order in which actions or visual elements appear throughout the video. The task emphasizes understanding the global context of temporal configurations.Spatiality•Spatial Perception: Focuses on recognizing and interpreting the spatial relationships among objects, people, and movements within a scene. This includes identifying directions, positions, orientations, and local arrangements. The task emphasizes understanding the local context of spatial configurations.•Spatial Reasoning: Involves higher-level inference about spatial structures, such as deducing the relative positions, movements, and locations of entities within broader environments. This task requires interpreting interactions across scenes and relies on understanding the global context of spatial relationships.Object•Object Recognition: Involves identifying and naming specific objects, products, or items that appear in a video. The task also includes providing contextual information about the recognized objects. This task primarily relies on understanding the local context.•Object Interaction: Assesses the ability to describe interactions between objects within a scene—how they are used, manipulated, or influence each other. This task emphasizes the local context of object dynamics.•Object Attribute: Focuses on describing the visual or functional characteristics of objects, such as appearance, material, color, or size. This task also depends on local context understanding.•Object Reasoning: Involves higher-level inference about objects in the scene, such as their roles, functions, or contextual significance. This requires a global context understanding across the video.•Object Existence: Evaluates the ability to determine the presence or absence of specific objects at various points in the video. This task requires reasoning over global context.•Object Permanence: Assesses the understanding of an object's continuity over time, including its movement, transformation, or disappearance. This task tests the model’s ability to track and reason about objects in a global context.Action•Action Recognition: Focuses on identifying and describing specific actions performed by subjects at given moments in the video. This task requires detailed interpretation of movements and activities within the local context.•Action Localization: Aims to determine the temporal and spatial boundaries of actions. It involves identifying when and where actions occur, including their start and end times, and comparing durations. This task relies on understanding the global context.•Action Sequence: Involves analyzing the chronological order of actions. The task requires recognizing the sequence of events, including what occurs before or after a given action, and understanding the progression over time. This task requires global context reasoning.•Action Reasoning: Requires inferring the motivations or causes behind observed actions. This includes identifying cause-and-effect relationships and interpreting contextual cues to explain why an action was taken. This task depends on global context comprehension.Scene•Scene Recognition: Assesses the ability to identify and describe the primary setting or environment in which the video takes place (e.g., indoor vs. outdoor, kitchen vs. park). This task involves understanding where the events occur and the general atmosphere surrounding the characters. It relies on global context comprehension.•Scene Transition: Involves detecting and analyzing changes in scene or setting throughout the video. The task requires recognizing shifts in environment, lighting, background, or thematic focus, and describing these transitions accurately. This task also depends on global context understanding.Event•Event Sequencing: Involves identifying the correct chronological order of events. This task requires recognizing the sequence in which actions unfold and determining prerequisite steps that lead to specific outcomes. It depends on a coherent understanding of the global context.•Key Events Extraction: Focuses on summarizing the most important actions and interactions within a video. The task entails identifying pivotal moments, evaluating their significance, and extracting overarching themes that define the event flow. This also requires comprehensive global context comprehension.Figure 11: Detailed descriptions of 22 sub-tasks over six fundamental topics (except “overall\nsummarization” task). For the “overall summarization” task, we directly use global-level video\ndescriptions as the corresponding answers, so there is no need to design an additional prompt.\n23\n--- Page 24 ---\n\"20-50s\": {\"Temporality\": \"The video progresses from the woman introducing the recipe to showing the ingredients and possibly demonstrating the steps involved.\",\"Spatiality\": \"The woman is seated in front of the table, which is positioned against a plain wall. The ingredients are arranged neatly on the table.\",\"Object\": \"The woman wears a black top and a choker necklace with a pendant. On the table, there are three bowls containing sugar, flour, and butter, an egg, and a glass bowl.\",\"Action\": \"The woman speaks to the camera, gestures with her hands, and points to the ingredients on the table. She appears to be explaining the recipe.\",\"Scene\": \"The video features a woman sitting in a chair, discussing a recipe while showing various ingredients laid out on a table.\",\"Summary\": \"This video showcases a woman presenting a homemade chocolate salted caramel tart recipe. She explains the ingredients and their arrangement on the table, providing a step-by-step guide for viewers to follow along.”}\n(a) Examples of clip-level video captioning.\n\"0-70s\": \"Introduction and Overview of Chocolate Salted Caramel Tart Recipe\",\"60-130s\": \"Preparing and Kneading the Dough\", \"120-190s\": \"Rolling and Shaping the Tart Crust\", \"180-270s\": \"Creating the Caramel Sauce and Assembling the Tart\",\"280-350s\": \"Finalizing and Presenting the Chocolate Salted Caramel Tart\"\n(b) Examples of event splitting\n\"120-190s\": {\"Brief Description\": \"Rolling and Shaping the Tart Crust\",\"Detailed Description\": \"In a cozy kitchen setting, a person meticulously prepares a tart crust. The process begins with a ball of dough on a wooden surface, surrounded by a generous sprinkling of flour. The individual, dressed in a dark-colored shirt, kneads the dough by hand, ensuring it is well-mixed and pliable. Once the dough is ready, they use a traditional rolling pin to flatten it, moving the pin back and forth to achieve an even thickness. The rolling pin's rhythmic motion is a testament to the baker's skill and experience. As the dough is rolled out, it is carefully transferred to a round tart pan, which is lined with parchment paper to prevent sticking. The person then shapes the dough into the pan, pressing it gently into the sides and bottom to create a smooth and even crust. Excess dough is trimmed away, and the final touches are added to ensure the tart base is perfect. The entire process is carried out with a focused and meticulous approach, highlighting the importance of precision in baking. The scene is a blend of traditional techniques and modern kitchen tools, creating a harmonious and engaging visual of the tart crust preparation.\"}\n(c) Examples of event-level video captioning.\nThe video begins with a warm and inviting introduction to a homemade chocolate salted caramel tart recipe. A woman, dressed in a black top and wearing a choker necklace with a pendant, sits in front of a table adorned with colorful candies spelling out 'THE SWEETEST THINGS.' She speaks directly to the camera, gesturing with her hands in a friendly and engaging manner. The scene transitions to her discussing the recipe, pointing out the ingredients laid out on the table: sugar, flour, butter, an egg, and a glass bowl. The setting appears to be a kitchen or dining area, bathed in natural daylight, creating a cozy atmosphere. The woman stands in front of a wooden table, where she begins to mix the ingredients for the tart. Using a fork, she combines the flour, butter, sugar, and egg methodically, demonstrating each step clearly and encouraging viewers to follow along. As the video progresses, the focus shifts to the preparation and kneading of the dough. The woman, now in a slightly messy kitchen environment, works on a wooden table set with all the necessary ingredients. She starts by mixing the flour and butter with a fork, ensuring a thorough blend. After adding an egg and a small amount of water, she continues to mix the dough until it reaches the right consistency. Her hands, now covered in flour, demonstrate a practiced technique as she kneads the dough on the wooden surface, shaping it into a ball. The scene highlights her attention to cleanliness and precision, setting the stage for the next step in the recipe. The process of rolling and shaping the tart crust follows. The woman, dressed in a dark-colored shirt, works with the dough on a floured wooden surface. She uses a traditional rolling pin to flatten the dough, moving it back and forth to achieve an even thickness. The dough is then carefully transferred to a round tart pan lined with parchment paper. She presses the dough gently into the sides and bottom of the pan, ensuring a smooth and even crust. Excess dough is trimmed away, and the final touches are added to perfect the tart base. The scene showcases a blend of traditional techniques and modern kitchen tools, emphasizing the importance of precision in baking.Next, the video transitions to the creation of the caramel sauce and the assembly of the tart. The woman places parchment paper into the tart pan and adds rice to weigh it down, ensuring an even distribution of weight. After removing the parchment paper, she applies an egg wash to the pre-baked tart shell. She then prepares the caramel sauce by heating sugar and water in a pot, stirring the mixture with a wooden spoon. Cream and butter are added to create a rich, smooth caramel sauce, which is spread evenly over the tart shell. Dark chocolate chunks are added for flavor and texture, setting the stage for the final steps in the recipe.The video concludes with the finalization and presentation of the chocolate salted caramel tart. A close-up shows a bowl of melted chocolate being poured into the tart shell, which is placed on a black slate board. The chocolate is spread evenly using a spoon, and the tart is then moved to a wooden cutting board for presentation. Sea salt is sprinkled on top, adding a touch of contrast and enhancing the tart's flavor. The tart is cut into triangular slices with a steady and deliberate technique, ready to be served. The entire process highlights the simplicity and elegance of the chocolate salted caramel tart, from its preparation to its final presentation, leaving viewers inspired to try the recipe themselves.\n(d) Examples of global-level video captioning.\nFigure 12: Examples of hierarchical video captioning.\n24\n--- Page 25 ---\nFigure 13: Word cloud of global-level video descriptions in VideoMarathon dataset.\n25",
  "text_length": 91659
}