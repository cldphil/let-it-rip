{
  "id": "http://arxiv.org/abs/2506.04118v1",
  "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs",
  "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .",
  "authors": [
    "Jonathan Geuter",
    "Youssef Mroueh",
    "David Alvarez-Melis"
  ],
  "published": "2025-06-04T16:12:26Z",
  "updated": "2025-06-04T16:12:26Z",
  "categories": [
    "cs.LG",
    "stat.ML",
    "I.2.7"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04118v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04118v1  [cs.LG]  4 Jun 2025GUIDED SPECULATIVE INFERENCE\nFOR EFFICIENT TEST-TIME ALIGNMENT OF LLMS\nJONATHAN GEUTER1,2,∗YOUSSEF MROUEH3DAVID ALVAREZ-MELIS1,2\n1Harvard John A. Paulson School of Engineering and Applied Sciences\n2Kempner Institute\n3IBM Research\nAbstract. We propose Guided Speculative Inference (GSI), a novel algorithm for efficient reward-\nguided decoding in large language models. GSI combines soft best-of- ntest-time scaling with a\nreward model r(x, y) and speculative samples from a small auxiliary model πS(y|x). We provably\napproximate the optimal tilted policy πβ,B(y|x)∝πB(y|x)exp(β r(x, y)) of soft best-of- n\nunder the primary model πB. We derive a theoretical bound on the KL divergence between our\ninduced distribution and the optimal policy. In experiments on reasoning benchmarks (MATH500,\nOlympiadBench, Minerva Math), our method achieves higher accuracy than standard soft best-of- n\nwith πSand reward-guided speculative decoding (Liao et al., 2025), and in certain settings even\noutperforms soft best-of- nwith πB. The code is available at: https://github.com/j-geuter/GSI .\n1.Introduction\nLarge language models (LLMs) have demonstrated remarkable performance across diverse genera-\ntion tasks, with scaling model and data size emerging as a reliable and efficient way to enhance their\ncapabilities (Kaplan et al., 2020; Team, 2024; OpenAI, 2024). However, this scaling has resulted in\nsignificant computational and economic costs, prompting the need for more efficient alternatives.\nOne such approach is test-time scaling (Snell et al., 2024; Muennighoff et al., 2025; Zhang et al.,\n2025), which focuses on scaling inference-time rather than training time compute. An orthogonal\ndirection in LLM post-training is model alignment (Ouyang et al., 2022; Gao et al., 2022; Touvron\net al., 2023), where models are optimized to maximize a given reward model r(x, y) that quantifies\nthe quality of a response ygiven a prompt x. Several techniques have been proposed for aligning\nLLMs with reward models (Yang & Klein, 2021; Mudgal et al., 2024; Huang et al., 2025). Recent\nwork on reward-guided speculative decoding (RSD) (Liao et al., 2025) introduces a single-step\nspeculative check to filter samples by a reward threshold, though it lacks theoretical guarantees on\ndistributional fidelity. Alternatively, best-of- nsampling (Gao et al., 2022; Mroueh, 2024; Beirami\net al., 2025) with temperature (“soft BoN”) (Verdun et al., 2025) can interpolate between the base\ndistribution πBand reward maximization, helping to mitigate reward hacking (Skalse et al., 2025).\nContributions. In this paper, we introduce a novel algorithm, Guided Speculative Inference (GSI),\nwhich combines speculative decoding, soft best-of- nsampling, and rejection sampling. Importantly,\nbytilting (i.e., adjusting) the rewards according to the loglikelihoods under both πBandπS, GSI\nprovably approximates the optimal reward-regularized policy under πB(Section 3), namely\nπβ,B(y|x) =πB(y|x) exp( β r(x, y))\nZβ,B(x).\n∗Correspondence to jonathan.geuter@gmx.de .\n1\n--- Page 2 ---\nWe evaluate GSI on several reasoning benchmarks and show that it outperforms both reward-guided\nspeculative decoding (Liao et al., 2025) and soft best-of- nsampling (Section 4).\n2.Background\nLetVdenote a (finite) vocabulary. Let X={x= (x1, ..., x n) :n∈N, xi∈ V} be the (countable)\nspace of inputs (in practice, consisting of the prompt and already generated reasoning steps), and\nY={y= (y1, ..., y n) :n∈N, yi∈ V} the (countable) space of reasoning steps (e.g. token\nsequences). Note that mathematically, these two spaces are identical, but we define both XandY\nfor notational convenience. By ∆( Y), we denote the set of probability measures over Y. For x∈ X,\nletπB(y|x)∈∆(Y) and πS(y|x)∈∆(Y) be the base andsmall language model distributions\nover y∈ Ygiven x. Note that we define the distributions over reasoning steps instead of single\ntokens . When we write πB(· |x, y), it denotes the distribution of πBoverYgiven a prompt xand a\n(partial) response y. We further assume we are given a process reward model (PRM)(Lightman\net al., 2023) r:X × Y → [0, R] for some R <∞, which assigns a reward r(x, y) to a reasoning\nstep y∈ Ygiven an input x∈ X. We assume that rapproximates a golden reward (Gao et al.,\n2022) r∗:X × Y → R, which can be thought of as the “true” reward function. Recall that the\nKullback–Leibler divergence between two distributions P, Q∈∆(Y) with P≪Qis defined as\nKL(P∥Q) =Ey∼P\u0002\nlogP(y)\nQ(y)\u0003\n,\nand the chi-square divergence as\nχ2(P||Q) =Z\u0012dP\ndQ−1\u00132\ndQ=ZdP2\ndQ−1.\nKL Regularized Reward Alignment. A standard formulation for maximizing the reward r(x, y)\ngiven x∈ X, while constraining how far the policy can move from the base policy πB(· |x), is to\nadd a KL regularizer, and find π∗\nBmaximizing\nmax\nπ∈∆(Y)Ey∼π[r(x, y) ]−1\nβKL\u0000\nπ(· |x)∥πB(· |x)\u0001\n,\nwhere β >0 trades off maximizing the reward versus fidelity to πB. It is well known (e.g. (Korbak\net al., 2022)) that the optimal policy has the closed form\nπβ,B(y|x) =πB(y|x) exp\u0000\nβ r(x, y)\u0001\nZβ,B(x), Z β,B(x) =Ey′∼πB(·|x)\u0002\neβ r(x,y′)\u0003\n. (1)\nBest-of- nSampling. Best-of- n(BoN) (Beirami et al., 2025) is a common inference-time method\nfor scaling LLMs. Hard best-of- ndraws y1, . . . , y n∼πB(· |x), and selects\ny∗= arg max\ni∈{1,...,n}r(x, yi).\nSoft best-of- n(S-BoN) (Verdun et al., 2025) weighs each draw by wi∝exp\u0000\nβ r(x, yi)\u0001\n, then\nsample a response yiwith probability wi/P\njwj. We denote the soft best-of- ndistribution over y\nbyπr,n\nβ,B(· |x). Note that both soft and hard BoN can be applied both to one-shot generation (where\nthe complete response it generated in one step) or reasoning tasks, where the yicorrespond to\nreasoning steps, and the BoN procedure is repeatedly applied. In this work, we focus on reasoning\ntasks. By moving from hard to soft best-of- n, the distribution πr,n\nβ,B(· |x) enjoys a KL bound to the\ntilted distribution πβ,B(Verdun et al., 2025):\nKL(πβ,B∥πr,n\nβ,B)≤log \n1 +Vary∼πB[eβr(x,y)]\nn(Ey∼πB[eβr(x,y)])2!\n. (2)\n2\n--- Page 3 ---\nxSmall Model πS\nSample yi∼πS(· |x)\nCompute log πS(yi|x)\ni= 1. . . n\nReward Model r\nri=r(x, yi)\nBig Model πB\nCompute log πB(yi|x)\nSample i∗via\nSoftmax\u0010\nβri+ logπB\nπS(yi|x)\u0011\nReturn yi∗Condition\nIfβr(x, yi∗) + logπB\nπS(yi∗|x)>u\nAccept yi∗\nElse Sample using SBoN from πB\nFigure 1. Guided Speculative Inference workflow. All computations involving the\nsmall, big, and reward models can be efficiently performed using vLLM.\nSpeculative Decoding. Speculative decoding (SD) (Leviathan et al., 2023) accelerates sampling\nfrom πBby first drawing proposals from πSand then accepting or rejecting them based on a\ncriterion derived from the ratio πB/πS. On rejection, one falls back to direct sampling from πB.\nSD provably samples from the distributions of πB. The core idea is that ktokens can be sampled\nfrom πSautoregressively, but verified by πBin parallel, thus generating up to k+ 1 tokens from\nπBwith a single forward pass of πB. Variants of SD include block verification (Sun et al., 2025)\nwhere sequences of draft tokens are verified jointly instead of token-by-token, and SpecTr (Sun\net al., 2024) which allows for verification of multiple draft sequences in parallel by framing SD as an\noptimal transport problem. SD has also been combined with early-exiting (Liu et al., 2024), and\n(Bhendawade et al., 2024) propose using n-gram predictions of πBas drafts, which alleviates the\nneed for an auxiliary model.\nA recent work proposes RSD (reward-guided speculative decoding) (Liao et al., 2025), an algorithm\nin which samples are generated from πS, and a threshold on the reward of the samples from πS\ndetermines whether one should resample from πB, or accept the sample from πS. While this\napproach shares similarities with GSI, it only provides a guarantee on the expected reward: under\nthe assumption that EπB[r(y|x)]≥EπS[r(y|x)], RSD satisfies EπRSD[r(y|x)]≥EπS[r(y|x)],\nwhich in the worst case does not yield any improvement over the small model πS, and also does\nnot guarantee anything about the policy πRSDitself. As we will see in Section 3, GSI provides\nguarantees on the induced policy directly.\n3.Guided Speculative Inference\nNote that we can write the tilted distribution (1) as\nπβ,B(y|x) =πS(y|x) exp\u0010\nβr(x, y) + log\u0010\nπB(y|x)\nπS(y|x)\u0011\u0011\nZβ,B(x),\ni.e. we can rewrite it as a distribution over πS(exponentially) tilted by the rewards\n˜r(x, y) =r(x, y) +1\nβlog\u0012πB(y|x)\nπS(y|x)\u0013\nThus, one can sample from πSand re-weight candidates to approximate πβ,B:\nReward-Likelihood Tilted S-BoN. Forx∈ X, the reward-tilted S-BoN is defined as follows:\n(1) sample y1, ..., y n∼πS(· |x)\n(2) compute ˜ ri=r(x, yi) +1\nβlog\u0010\nπB(yi|x)\nπS(yi|x)\u0011\n(3) sample yi∝exp(β˜ri)\nWe will denote the distribution generated by this sampling algorithm by π˜r,n\nβ,S(· |x). Of course,\nwe can only hope that π˜r,n\nβ,S(· |x) is close to πβ,B(· |x) if the support of πBis sufficiently covered by πS.\n3\n--- Page 4 ---\nAlgorithm 1 Guided Speculative Inference\n1:Input: base model πB, small model πS, PRM r,β >0, threshold u∈R,n∈N, prompt x∈ X\n2:y←() ▷empty response\n3:fort= 0,1, . . .until EOSdo\n4: Sample {yi\nt}n\ni=1∼πS(· |x, y)\n5: ˜ri←r(x, yi\nt) +1\nβ\u0010\nlogπB(yi\nt|x)−logπS(yi\nt|x)\u0011\n6: Sample index c∼softmax\u0000\nβ˜r1, . . . , β ˜rn\u0001\n7: if˜rc≥uthen\n8: y←(y, yc\nt) ▷append yc\nt\n9: else\n10: Sample {yj\nt}n\nj=1∼πB(· |x, y)\n11: rj←r(x, yj\nt)\n12: Sample index c∼softmax\u0000\nβr1, . . . , βrn\u0001\n13: y←(y, yc\nt)\n14: end if\n15:end for\nCoverage Assumption. Throughout, we will assume that\nC∞(x) := sup\ny∈Y:πB(y|x)>0πB(y|x)\nπS(y|x)<∞. (3)\nUnder this assumption, Reward-Likelihood Tilted S-BoN with πSindeed approximates the tilted\ndistribution πβ,Bin the sense of the following theorem.\nTheorem 1. Letx∈ X. Assume that the coverage assumption (3)holds. Let ϵ >0be arbitrary,\nand\nn≥\u0000\nχ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\u0001\ne2β∥r∥∞−1\neϵ−1.\nThen,\nKL\u0000\nπβ,B(· |x)∥π˜r,n\nβ,S(· |x)\u0001\n≤ϵ.\nThe proof can be found in Appendix B.\nThis lies at the core of our proposed algorithm. In addition to sampling from the Reward-\nLikelihood Tilted S-BoN, we also add a rejection sampling-like threshold on the tilted reward, which\ntriggers resampling from the base model πBin case the tilted reward falls below it. This improves\nperformance empirically. The complete GSI method can be seen in Algorithm 1. Note that in\nprinciple, it is possible to choose different nfor the small and large model in the algorithm. We\nleave exploring this for future research. Note that while GSI is, in theory, applicable to one-shot\ngeneration tasks, we consider ytin Algorithm 1 to be a reasoning step (i.e., a subsequence of the\nfull response), and ris a process reward model (PRM). The algorithm generates reasoning steps\nuntil an end-of-sequence (EOS) token is created.\nWe denote the distribution generated by Algorithm 1 as πGSI. While Theorem 1 does not apply\ntoπGSI(only to π˜r,n\nβ,S), we can also guarantee that the expected difference in (golden) reward goes\nto 0 as nincreases (see Theorem 2 in Appendix B):\nEπβ,B[r∗]−EπGSI[r∗]n→∞− − − → 0.\n4\n--- Page 5 ---\nFigure 2. Accuracy on reasoning datasets vs. n. We plot GSI without rejection step,\nGSI with rejection step, and S-BoN with πB(with β= 20 for all). GSI (with and\nwithout rejection step) converges to S-BoN on πBasnincreases, which empirically\nconfirms Theorem 1.\n4.Experiments\nModels. We use Qwen2.5-Math-1.5B-Instruct as πS, Qwen2.5-Math-7B-Instruct as πB, and\nQwen2.5-Math-PRM-7B as the PRM rthroughout. The rewards lie in [0 ,1].\nImplementation. We implement all models with vLLM (Kwon et al., 2023). The log-likelihoods\nforπSare computed without any additional computational overhead within the forward pass of πS.\nThe log-likelihoods for πBcan be computed with minimal computational overhead, as they only\nrequire a single forward pass through πB. We host each of the three models on its own NVIDIA\nH100 GPU.\nDatasets. We evaluate on three mathematical reasoning benchmarks: MATH500 (Lightman et al.,\n2023), OlympiadBench (He et al., 2024) (the OE TOmaths enCOMP split which is text-only\nmath problems in English), and Minerva Math (Lewkowycz et al., 2022). We decode stepwise with\nchain-of-thought; rewards are computed on each reasoning step. For each method and dataset, we\nreport the average accuracies over two different random seeds.\nMethods. We compare GSI against RSD (Liao et al., 2025), S-BoN with πS, and S-BoN with πB.\nHyperparameters. We use β= 20, u= 0.5 (selected empirically amongst a range of values\nbased on acceptance rate vs. accuracy trade-off), temperature = 0.7, and top_p = 1.0. We set the\nthreshold in RSD to 0 .7, which is the same as in the RSD paper.\n4.1.Performance on Reasoning Benchmarks. In Table 1, we compare the average accuracies\nof GSI, RSD, and S-BoN on the small and big model. We see that GSI clearly outperforms RSD\nand S-BoN on the small model across the datasets. While RSD is slightly better on OlympiadBench,\nthis might be the case because the small model is better on this dataset than the big model, and\nRSD is closer to the small model due to its high overall acceptance rate, cmp. Table 2.\nFigure 2 compares GSI without the rejection sampling step (i.e., without lines 6 to 11 in Algorithm\n1) to regular GSI and S-BoN from πB. We see that GSI clearly outperforms GSI without rejection\nstep; however, this difference becomes less significant as nincreases, hinting at the fact that with\nlarger n, the samples from the small model reach better coverage of the support of πB. In future\nresearch, we plan to investigate this behavior as we scale nbeyond 256.\nIn Table 2, we report the inference time per sample (in seconds) across methods (averaged over\ndatasets), as well as the average percentage of samples accepted by GSI and RSD. We see that RSD\ngenerally tends to accept almost all samples, which explains why its performance is comparable\nto S-BoN with the small model, compare Table 1, while being slightly worse in terms of inference\n5\n--- Page 6 ---\nTable 1. Accuracies on reasoning benchmarks for n= 16 and n= 64. GSI performs\nbetter than RSD (Liao et al., 2025) and S-BoN with the small model. S-BoN with\nthe big model is the target distribution.\nMethod MATH500 OlympiadBench Minerva Math Mean Acc.\nGSI, n=16 (ours) 82.2 41.4 29.6 51.1\nRSD, n=16 79.5 41.7 24.3 48.5\nS-BoN (small), n=16 80.3 41.2 25.5 49.0\nS-BoN (big), n=16 82.5 39.3 36.0 52.6\nGSI, n=64 (ours) 83.3 41.2 29.6 51.3\nRSD, n=64 79.9 41.1 25.4 48.8\nS-BoN (small), n=64 80.3 42.3 24.3 49.0\nS-BoN (big), n=64 83.0 42.5 36.4 54.0\nTable 2. Avg. inference time (in seconds) per reasoning step, avg. number of\nreasoning steps per sample, and avg. percentage of samples accepted (averaged\nacross all datasets). GSI is significantly faster than S-BoN on the big model, but has\nslightly higher inference time per step than RSD (Liao et al., 2025) as the acceptance\nrate is lower.\nMethod s / step # steps % accept\nGSI, n=16 (ours) 0.88 13.9 91\nRSD, n=16 0.77 11.6 97\nS-BoN (small), n=16 0.79 11.0 –\nS-BoN (big), n=16 1.14 11.8 –\nGSI, n=64 (ours) 2.05 15.3 93\nRSD, n=64 1.88 12.7 98\nS-BoN (small), n=64 1.99 10.9 –\nS-BoN (big), n=64 2.76 11.7 –\nspeed. GSI accepts less samples, thus is slower than RSD, while still outperforming S-BoN on the\nlarge model in terms of inference speed.\n5.Discussion\nWe introduced Guided Speculative Inference (GSI), a novel inference-time algorithm for efficient\nreward-guided decoding from language models. GSI leverages speculative samples from a small\nauxiliary model to approximate the optimal tilted policy of a base model with respect to a given\nreward function. We showed that unlike previous approaches, GSI provably approaches the optimal\npolicy as the number of samples generated at each step nincreases. Empirical results on various\nreasoning datasets show that GSI significantly outperforms reward-guided speculative decoding (Liao\net al., 2025) and soft best-of- nusing the small model—and, perhaps surprisingly, even surpasses\nsoft best-of- nwith the base model in some cases. Future work will explore extending GSI beyond\nreasoning tasks (e.g., alignment to safety rewards), studying its scaling behavior with respect to n,\nand analyzing its sensitivity to different values of βandnacross both models. Deriving tighter\ntheoretical bounds is also an important aspect in better understanding the behavior of GSI.\n6\n--- Page 7 ---\nAcknowledgements\nWe would like to thank Nick Hill and Guangxuan (GX) Xu from red hat for their help with\nvLLM. JG and DAM acknowledge support from the Kempner Institute, the Aramont Fellowship\nFund, and the FAS Dean’s Competitive Fund for Promising Scholarship.\nReferences\nBeirami, A., Agarwal, A., Berant, J., D’Amour, A., Eisenstein, J., Nagpal, C., and Suresh, A. T.\nTheoretical guarantees on the best-of-n alignment policy, 2025. URL https://arxiv.org/abs/\n2401.01879 .\nBhendawade, N., Belousova, I., Fu, Q., Mason, H., Rastegari, M., and Najibi, M. Speculative\nStreaming: Fast LLM Inference without Auxiliary Models, 2024. URL https://arxiv.org/abs/\n2402.11131 .\nGao, L., Schulman, J., and Hilton, J. Scaling Laws for Reward Model Overoptimization, 2022. URL\nhttps://arxiv.org/abs/2210.10760 .\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang,\nY., Liu, J., Qi, L., Liu, Z., and Sun, M. OlympiadBench: A Challenging Benchmark for\nPromoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems, 2024. URL\nhttps://arxiv.org/abs/2402.14008 .\nHuang, A., Block, A., Liu, Q., Jiang, N., Krishnamurthy, A., and Foster, D. J. Is Best-of-N the\nBest of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment, 2025. URL\nhttps://arxiv.org/abs/2503.21878 .\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,\nA., Wu, J., and Amodei, D. Scaling Laws for Neural Language Models, 2020. URL https:\n//arxiv.org/abs/2001.08361 .\nKorbak, T., Perez, E., and Buckley, C. L. RL with KL penalties is better viewed as Bayesian\ninference, 2022. URL https://arxiv.org/abs/2205.11275 .\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and\nStoica, I. Efficient Memory Management for Large Language Model Serving with PagedAttention,\n2023. URL https://arxiv.org/abs/2309.06180 .\nLeviathan, Y., Kalman, M., and Matias, Y. Fast Inference from Transformers via Speculative\nDecoding, 2023. URL https://arxiv.org/abs/2211.17192 .\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A.,\nAnil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving\nQuantitative Reasoning Problems with Language Models, 2022. URL https://arxiv.org/abs/\n2206.14858 .\nLiao, B., Xu, Y., Dong, H., Li, J., Monz, C., Savarese, S., Sahoo, D., and Xiong, C. Reward-Guided\nSpeculative Decoding for Efficient LLM Reasoning, 2025. URL https://arxiv.org/abs/2501.\n19324 .\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J.,\nSutskever, I., and Cobbe, K. Let’s Verify Step by Step. arXiv preprint arXiv:2305.20050 , 2023.\nLiu, J., Wang, Q., Wang, J., and Cai, X. Speculative decoding via early-exiting for faster llm inference\nwith thompson sampling control mechanism, 2024. URL https://arxiv.org/abs/2406.03853 .\nMroueh, Y. Information theoretic guarantees for policy alignment in large language models, 2024.\nURL https://arxiv.org/abs/2406.05883 .\nMudgal, S., Lee, J., Ganapathy, H., Li, Y., Wang, T., Huang, Y., Chen, Z., Cheng, H.-T., Collins,\nM., Strohman, T., Chen, J., Beutel, A., and Beirami, A. Controlled Decoding from Language\nModels, 2024. URL https://arxiv.org/abs/2310.17022 .\n7\n--- Page 8 ---\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P.,\nCand` es, E., and Hashimoto, T. s1: Simple test-time scaling, 2025. URL https://arxiv.org/\nabs/2501.19393 .\nOpenAI. GPT-4 Technical Report, 2024. URL https://arxiv.org/abs/2303.08774 .\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell,\nA., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow\ninstructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155 .\nSkalse, J., Howe, N. H. R., Krasheninnikov, D., and Krueger, D. Defining and Characterizing\nReward Hacking, 2025. URL https://arxiv.org/abs/2209.13085 .\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM Test-Time Compute Optimally can be More\nEffective than Scaling Model Parameters, 2024. URL https://arxiv.org/abs/2408.03314 .\nSun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and Yu, F. SpecTr: Fast Speculative\nDecoding via Optimal Transport, 2024. URL https://arxiv.org/abs/2310.15141 .\nSun, Z., Mendlovic, U., Leviathan, Y., Aharoni, A., Ro, J. H., Beirami, A., and Suresh, A. T. Block\nVerification Accelerates Speculative Decoding, 2025. URL https://arxiv.org/abs/2403.10444 .\nTeam, G. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\n2024. URL https://arxiv.org/abs/2403.05530 .\nTouvron, H., Martin, L., Stone, K., Albert, P., and et al. Llama 2: Open Foundation and Fine-Tuned\nChat Models, 2023. URL https://arxiv.org/abs/2307.09288 .\nVerdun, C. M., Oesterling, A., Lakkaraju, H., and Calmon, F. P. Soft Best-of-n Sampling for Model\nAlignment, 2025. URL https://arxiv.org/abs/2505.03156 .\nYang, K. and Klein, D. FUDGE: Controlled Text Generation With Future Discriminators. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies . Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.naacl-main.276. URL http://dx.doi.org/10.18653/v1/2021.\nnaacl-main.276 .\nZhang, Q., Lyu, F., Sun, Z., Wang, L., Zhang, W., Hua, W., Wu, H., Guo, Z., Wang, Y., Muennighoff,\nN., King, I., Liu, X., and Ma, C. A survey on test-time scaling in large language models: What,\nhow, where, and how well?, 2025. URL https://arxiv.org/abs/2503.24235 .\n8\n--- Page 9 ---\nAppendix A.Code\nThe code is available at: https://github.com/j-geuter/GSI .\nAppendix B.Proofs\nTheorem 1. Letx∈ X. Assume that the coverage assumption (3)holds. Let ϵ >0be arbitrary,\nand\nn≥\u0000\nχ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\u0001\ne2β∥r∥∞−1\neϵ−1.\nThen,\nKL\u0000\nπβ,B(· |x)∥π˜r,n\nβ,S(· |x)\u0001\n≤ϵ.\nProof. By Lemma 1 in (Verdun et al., 2025) (which equally holds for countable instead of finite\nspaces), we have\nπ˜r,n\nβ,S(y|x)≥πS(y|x) exph\nβ r(x, y) + logπB(y|x)\nπS(y|x)i\n1\nnexph\nβ r(x, y) + logπB(y|x)\nπS(y|x)i\n+n−1\nnEy′∼πS(·|x)\u0002πB(y′|x)\nπS(y′|x)eβ r(x,y′)\u0003\n=πB(y|x)eβ r(x,y)\n1\nnπB(y|x)\nπS(y|x)eβ r(x,y)+n−1\nnEy′∼πB(·|x)\u0002\neβ r(x,y′)\u0003.\nHence\nKL\u0000\nπβ,B∥πn,t\nS\u0001\n=X\nyπβ,B(y|x) logπβ,B(y|x)\nπn,t\nS(y|x)\n≤X\nyπB(y|x)eβr(x,y)\nEy′∼πB(·|x)[eβr(x,y′)]log\nπB(y|x)eβr(x,y)\u0014\n1\nnπB(y|x)\nπS(y|x)eβr(x,y)+n−1\nnEy′∼πB(·|x)[eβr(x,y′)]\u0015\nEy′∼πB(·|x)[eβr(x,y′)]πB(y|x)eβr(x,y)\n\n=X\nyπB(y|x)eβr(x,y)\nEy′∼πB(·|x)[eβr(x,y′)]log \n1\nnπB(y|x)\nπS(y|x)eβ r(x,y)\nEy′∼πB(·|x)[eβ r(x,y′)]+n−1\nn!\n≤log \n1\nn X\nyπB(y|x)2\nπS(y|x)e2β r(x,y)\n\u0000\nEy′∼πB(·|x)[eβ r(x,y′)]\u00012!\n+n−1\nn!\n(Jensen’s inequality)\n≤log \n1\nne2β∥r∥∞χ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\n\u0000\nEy′∼πB(·|x)[eβ r(x,y′)]\u00012+n−1\nn!\n≤log \u0010\nχ2\u0000\nπB(·|x)∥πS(·|x)\u0001\n+1\u0011\ne2β∥r∥∞\nn+n−1\nn!\n,\nusing the fact that\nEy′∼πB(·|x)[eβ r(x,y′)]≥1\n9\n--- Page 10 ---\nsince r(x, y′)≥0. Now for ϵ >0, we have\nlog \u0010\nχ2\u0000\nπB(·|x)∥πS(·|x)\u0001\n+1\u0011\ne2β∥r∥∞\nn+n−1\nn!\n≤ϵ\n⇔\u0010\nχ2\u0000\nπB(·|x)∥πS(·|x)\u0001\n+1\u0011\ne2β∥r∥∞\nn+ 1−1\nn≤eϵ,\n⇔ 1 +\u0010\nχ2\u0000\nπB(·|x)∥πS(·|x)\u0001\n+1\u0011\ne2β∥r∥∞−1\nn≤eϵ,\n⇔\u0010\nχ2\u0000\nπB(·|x)∥πS(·|x)\u0001\n+1\u0011\ne2β∥r∥∞−1\nn≤eϵ−1,\n⇔\u0000\nχ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\u0001\ne2β∥r∥∞−1\neϵ−1≤n,\nwhich finishes the proof. □\nTheorem 2. Letx∈ X. Assume that EπGSI[r∗]<∞andEπβ,B[r∗]<∞(here we implicitly\nassume that distributions and rewards are conditioned on x, which we omit for ease of notation).\nFurthermore, assume the coverage assumption (3)holds. Denote by p(u)the acceptance probability\nof GSI. Then\nEπβ,B[r∗]−EπGSI[r∗]≤∥r∗∥∞√n\u0014\np(u)1\n2eβ∥r∥∞\u0000\nχ2(πB∥πS) + 1\u00011\n2+ (1−p(u))\u0010\nCV(eβr)2+ 1\u00111\n2\u0015\n,\nwhere CV(eβr) =s\nVary′∼πB(·|x)[eβr(x,y′)]\n\u0010\nEy′∼πB(·|x)[eβr(x,y′)]\u00112. In particular, we have EπGSI[r∗]−Eπβ,B[r∗]n→∞− − − → 0.\nProof. Denote by Y≥⊂Ythe set where ˜rt≥u, i.e. where πGSI=π˜r,n\nβ,S, and let Y<=Y\\Y≥, hence\nπGSI=πr,n\nβ,BonY<. We write\nEπβ,B[r∗]−EπGSI[r∗] =Eπβ,B[ 1Y≥r∗]−EπGSI[ 1Y≥r∗]\n| {z }\n(a)+Eπβ,B[ 1Y<r∗]−EπGSI[ 1Y<r∗]\n| {z }\n(b).\nStep 1: Bounding (a). We have by Cauchy-Schwarz:\n(a) = Ey∼πβ,B(·|x)\u0002\n1Y≥(y)r∗(x, y)\u0003\n−Ey∼π˜r,n\nβ,S(·|x)\u0002\n1Y≥(y)r∗(x, y)\u0003\n≤ ∥r∗∥∞\u0012Z\n1Y≥(y) dπ˜r,n\nβ,S(y|x)\u00131\n2\nZ \nπβ,B(y|x)−π˜r,n\nβ,S(y|x)\nπ˜r,n\nβ,S(y|x)!2\nπ˜r,n\nβ,S(dy|x)\n1\n2\n=∥r∗∥∞\u0010\nπ˜r,n\nβ,S(Y≥|x)\u00111\n2\u0010\nχ2\u0000\nπβ,B(· |x)\r\rπ˜r,n\nβ,S(· |x)\u0001\u00111/2\n. (4)\n10\n--- Page 11 ---\nBy Lemma 1 from (Verdun et al., 2025) we have\nχ2\u0000\nπβ,B(· |x)\r\rπ˜r,n\nβ,S(· |x)\u0001\n(5)\n=Zπβ,B(y|x)2\nπ˜r,n\nβ,S(y|x)dy−1\n=Z \u0000\nπB(y|x)eβ r(x,y)\u00012\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012π˜r,n\nβ,S(y|x)dy−1\nLemma 1\n≤Z\u0000\nπB(y|x)eβr(x,y)\u00012\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u000121\nnπB(y|x)\nπS(y|x)eβr(x,y)+n−1\nnEy′∼πB(·|x)[eβr(x,y′)]\nπB(y|x)eβr(x,y)dy−1\n=1\nn\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012ZπB(y|x)2\nπS(y|x)e2βrdy+n−1\nnEy′∼πB(·|x)[eβr(x,y′)]\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012−1\n≤e2β∥r∥∞\nn\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012\u0010\nχ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\u0011\n−1\nn\n≤1\nne2β∥r∥∞\u0010\nχ2\u0000\nπB(· |x)∥πS(· |x)\u0001\n+ 1\u0011\n. (6)\nPlugging (6) into (4) yields\n(a)≤ ∥r∗∥∞\u0010\nπ˜r,n\nβ,S(Y≥|x)\u00111\n2\u00121\nne2β∥r∥∞\u0000\nχ2(πB∥πS) + 1\u0001\u00131\n2\n=∥r∗∥∞√np(u)1\n2eβ∥r∥∞\u0000\nχ2(πB∥πS) + 1\u00011\n2. (7)\nStep 2: Bounding (b). Similar to the bound for (a), we get\n(b) = π˜r,n\nβ,S( 1Y <) Z\nr∗(x, y)πβ,B(y|x)−πr,n\nβ,B(y|x)\nπr,n\nβ,B(y|x)πr,n\nβ,B(dy|x)!\n≤π˜r,n\nβ,S( 1Y <)\u0012Z\nr∗(x, y)2πr,n\nβ,B(dy|x)\u00131\n2\nZ \nπβ,B(y|x)−πr,n\nβ,B(y|x)\nπr,n\nβ,B(y|x)!2\nπr,n\nβ,B(dy|x)\n1\n2\n≤π˜r,n\nβ,S( 1Y <)∥r∗∥∞\nZ \nπβ,B(y|x)−πr,n\nβ,B(y|x)\nπr,n\nβ,B(y|x)!2\nπr,n\nβ,B(dy|x)\n1\n2\n= (1−p(u))∥r∗∥∞\u0010\nχ2(πβ,B||πr,n\nβ,B)\u00111\n2(8)\nby independence of the event Y<andπn\nBresp. πβ,B, and applying Cauchy-Schwarz.\n11\n--- Page 12 ---\nAgain, using Lemma 1 from (Verdun et al., 2025) we get\nχ2(πβ,B||πr,n\nβ,B) =Zπβ,B(y|x)2\nπr,n\nβ,B(y|x)dy−1\nLemma 1\n≤Z\u0000\nπB(y|x)eβr(x,y)\u00012\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u000121\nneβr(x,y)+n−1\nnEy′∼πB(·|x)[eβr(x,y′)]\nπB(y|x)eβr(x,y)dy−1\n=1\nnEy′∼πB(·|x)[e2βr(x,y′)]\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012+n−1\nn−1\n≤1\nnEy′∼πB(·|x)[e2βr(x,y′)]\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012\n=1\nn \nVary′∼πB(·|x)[eβr(x,y′)]\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012+ 1!\n. (9)\nPlugging equation (9) into (8) yields\n(b)≤∥r∗∥∞√n(1−p(u)) \nVary′∼πB(·|x)[eβr(x,y′)]\n\u0000\nEy′∼πB(·|x)[eβr(x,y′)]\u00012+ 1!1\n2\n(10)\nCombining equations (7) and (10) gives\nEπβ,B[r∗]−EπGSI[r∗]≤∥r∗∥∞√n\u0014\np(u)1\n2eβ∥r∥∞\u0000\nχ2(πB∥πS) + 1\u00011\n2+ (1−p(u))\u0010\nCV(eβr)2+ 1\u00111\n2\u0015\nas desired.\n□\n12",
  "text_length": 26498
}