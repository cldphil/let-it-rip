{
  "id": "http://arxiv.org/abs/2506.05176v1",
  "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
  "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
  "authors": [
    "Yanzhao Zhang",
    "Mingxin Li",
    "Dingkun Long",
    "Xin Zhang",
    "Huan Lin",
    "Baosong Yang",
    "Pengjun Xie",
    "An Yang",
    "Dayiheng Liu",
    "Junyang Lin",
    "Fei Huang",
    "Jingren Zhou"
  ],
  "published": "2025-06-05T15:49:48Z",
  "updated": "2025-06-05T15:49:48Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05176v1",
  "full_text": "--- Page 1 ---\nTechnical Report\nQwen3 Embedding: Advancing Text Embedding and\nReranking Through Foundation Models\nYanzhao Zhang* Mingxin Li* Dingkun Long* Xin Zhang*\nHuan Lin Baosong Yang Pengjun Xie An Yang\nDayiheng Liu Junyang Lin Fei Huang Jingren Zhou\nTongyi Lab Alibaba Group\nhttps://huggingface.co/Qwen\nhttps://modelscope.cn/organization/qwen\nhttps://github.com/QwenLM/Qwen3-Embedding\nAbstract\nIn this work, we introduce the Qwen3 Embedding series, a significant advancement\nover its predecessor, the GTE-Qwen series, in text embedding and reranking capabili-\nties, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs’ robust\ncapabilities in multilingual text understanding and generation, our innovative multi-\nstage training pipeline combines large-scale unsupervised pre-training with supervised\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure\nthe robustness and adaptability of the Qwen3 Embedding series. During the training\nprocess, the Qwen3 LLMs serve not only as backbone models but also play a crucial role\nin synthesizing high-quality, rich, and diverse training data across multiple domains\nand languages, thus enhancing the training pipeline. The Qwen3 Embedding series\noffers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks,\naddressing diverse deployment scenarios where users can optimize for either efficiency\nor effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series\nachieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various\nretrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval.\nTo facilitate reproducibility and promote community-driven research and development,\nthe Qwen3 Embedding models are publicly available under the Apache 2.0 license.\n1 Introduction\nText embedding and reranking are fundamental components in numerous natural language pro-\ncessing and information retrieval applications, including web search, question answering, recom-\nmendation systems, and beyond (Karpukhin et al., 2020; Huang et al., 2020; Zhao et al., 2023; 2024).\nHigh-quality embeddings enable models to capture semantic relationships between texts, while\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,\nemerging application paradigms such as Retrieval-Augmented Generation (RAG) and agent sys-\ntems, driven by the advancement of large language models (e.g., Qwen3 (Yang et al., 2025), GPT-4o\n(Hurst et al., 2024)), have introduced new requirements and challenges for text embedding and\nreranking, both in terms of model training paradigms and application scenarios. Despite significant\nadvancements, training embedding and reranking models that perform well in scalability, contextual\nunderstanding, and alignment with specific downstream tasks remains challenging.\nThe emergence of large language models (LLMs) has significantly advanced the development of text\nembedding and reranking models. Prior to the introduction of LLMs, the predominant approach\n∗Equal contribution\n1arXiv:2506.05176v1  [cs.CL]  5 Jun 2025\n--- Page 2 ---\nTechnical Report\ninvolved using encoder-only pretrained language models like BERT as the foundational model\nfor training (Reimers & Gurevych, 2019). The richer world knowledge, text understanding, and\nreasoning abilities inherent in LLMs have led to further enhancements in models trained on these\narchitectures. Additionally, there has been considerable research facilitating the integration of LLMs\ninto processes such as training data synthesis and quality data filtering (Wang et al., 2024; Lee et al.,\n2024; 2025b). The fundamental characteristics of LLMs have also inspired the introduction of new\ntraining paradigms. For instance, during the embedding model training process, incorporating\ndifferentiated tasks across aspects such as instruction type, domain, and language has yielded\nimproved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\nadvancements have been realized through both zero-shot methods based on user prompts and\napproaches combining supervised fine-tuning (Ma et al., 2023; Pradeep et al., 2023; Zhang et al.,\n2024a; Zhuang et al., 2024).\nIn this work, we introduce the Qwen3 Embedding series models, which are constructed on top\nof the Qwen3 foundation models. The Qwen3 foundation has simultaneously released base and\ninstruct model versions, and we exploit the robust multilingual text understanding and generation\ncapabilities of these models to fully realize their potential in training embedding and reranking\nmodels. To train the embedding models, we implement a multi-stage training pipeline that involves\nlarge-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\nWe also employ model merging with various model checkpoints to enhance robustness and general-\nization. The Qwen3 instruct model allows for efficient synthesis of a vast, high-quality, multilingual,\nand multi-task text relevance dataset. This synthetic data is utilized in the initial unsupervised\ntraining stage, while a subset of high-quality, small-scale data is selected for the second stage of\nsupervised training. For the reranking models, we adopt a two-stage training scheme in a similar\nmanner, consisting of high-quality supervised fine tuning and a model merging stage. Based on\ndifferent sizes of the Qwen3 backbone models (including 0.6B, 4B, and 8B), we ultimately trained\nthree text embedding models and three text reranking models. To facilitate their application in\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible\ndimension representation for embedding models and customizable instructions for both embedding\nand reranking models.\nWe evaluate the Qwen3 Embedding series across a comprehensive set of benchmarks spanning\nmultiple tasks and domains. Experimental results demonstrate that our embedding and reranking\nmodels achieve state-of-the-art performance, performing competitively against leading proprietary\nmodels in several retrieval tasks. For example, the flagship model Qwen3-8B-Embedding attains a\nscore of 70.58 on the MTEB Multilingual benchmark (Enevoldsen et al., 2025) and 80.68 on the MTEB\nCode benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers\ncompetitive results across a range of retrieval tasks. The Qwen3-Reranker-0.6B model exceeds\npreviously top-performing models in numerous retrieval tasks, while the larger Qwen3-Reranker-8B\nmodel demonstrates even superior performance, improving ranking results by 3.0 points over the\n0.6B model across multiple tasks. Furthermore, we include a constructive ablation study to elucidate\nthe key factors contributing to the superior performance of the Qwen3 Embedding series, providing\ninsights into its effectiveness.\nIn the following sections, we describe the design of the model architecture, detail the training\nprocedures, present the experimental results for both the embedding and reranking models of the\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and\noutlining potential directions for future research.\n2 Model Architecture\nThe core idea behind embedding and reranking models is to evaluate relevance in a task-aware\nmanner. Given a query qand a document d, embedding and reranking models assess their relevance\nbased on a similarity criterion defined by instruction I. To enable the models for task-aware rele-\nvance estimation, training data is often organized as {Ii,qi,d+\ni,d−\ni,1,· · ·,d−\ni,n}, where d+\nirepresents a\n2\n--- Page 3 ---\nTechnical Report\nFigure 1: Model architecture of Qwen3-Embedding (left) and Qwen3-Reranker (right).\npositive (relevant) document for query qi, and d−\ni,jare negative (irrelevant) documents. Training the\nmodel on diverse text pairs broadens its applicability to a range of downstream tasks, including\nretrieval, semantic textual similarity, classification, and clustering.\nArchitecture The Qwen3 embedding and reranking models are built on the dense version of\nQwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters. We initialize\nthese models using the Qwen3 foundation models to leverage their capabilities in text modeling\nand instruction following. The model layers, hidden size, and context length for each model\nconfiguration are detailed in Table 1.\nEmbedding Models For text embeddings, we utilize LLMs with causal attention, appending an\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state\nof the last layer corresponding to this [EOS] token.\nTo ensure embeddings follow instructions during downstream tasks, we concatenate the instruction\nand the query into a single input context, while leaving the document unchanged before processing\nwith LLMs. The input format for queries is as follows:\n{Instruction} {Query}<|endoftext|>\nReranking Models To more accurately evaluate text similarity, we employ LLMs for point-wise\nreranking within a single context. Similar to the embedding model, to enable instruction-following\ncapability, we include the instruction in the input context. We use the LLM chat template and frame\nthe similarity assessment task as a binary classification problem. The input to LLMs adheres to the\ntemplate shown below:\n<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the\nInstruct provided. Note that the answer can only be \"yes\" or\n\"no\".<|im_end|>,→\n,→\n<|im_start|>user\n<Instruct>: {Instruction}\n<Query>: {Query}\n<Document>: {Document}<|im_end|>\n<|im_start|>assistant\n<think>\\n\\n</think>\\n\\n\n3\n--- Page 4 ---\nTechnical Report\nModel Type Models Size LayersSequence\nLengthEmbedding\nDimensionMRL\nSupportInstruction\nAware\nText EmbeddingQwen3-Embedding-0.6B 0.6B 28 32K 1024 Yes Yes\nQwen3-Embedding-4B 4B 36 32K 2560 Yes Yes\nQwen3-Embedding-8B 8B 36 32K 4096 Yes Yes\nText RerankingQwen3-Reranker-0.6B 0.6B 28 32K - - Yes\nQwen3-Reranker-4B 4B 36 32K - - Yes\nQwen3-Reranker-8B 8B 36 32K - - Yes\nTable 1: Model architecture of Qwen3 Embedding models. “MRL Support” indicates whether the\nembedding model supports custom dimensions for the final embedding. “Instruction Aware” notes\nwhether the embedding or reranker model supports customizing the input instruction according to\ndifferent tasks.\nFigure 2: Training pipeline of Qwen3 Embedding and Reranking models.\nTo calculate the relevance score based on the given input, we assess the likelihood of the next token\nbeing ”yes” or ”no.” This is expressed mathematically as follows:\nscore (q,d) =eP(yes|I,q,d)\neP(yes|I,q,d)+eP(no|I,q,d)\n3 Models Training\nIn this section, we describe the multi-stage training pipeline adopted and present the key elements of\nthis training recipe, including training objective, training data synthesis, and filtering of high-quality\ntraining data.\n3.1 Training Objective\nBefore introducing our training pipeline, we first outline the optimized loss functions used for the\nembedding and reranking models during the training process. For the embedding model, we utilize\nan improved contrastive loss based on the InfoNCE framework (Oord et al., 2018). Given a batch of\nNtraining instances, the loss is defined as:\nLembedding =−1\nNN\n∑\niloge(s(qi,d+\ni)/τ)\nZi, (1)\nwhere s(·,·)is a similarity function (we use cosine similarity), τis a temperature parameter, and Zi\nis the normalization factor that aggregates the similarity scores of the positive pair against various\nnegative pairs:\nZi=e(s(qi,d+\ni)/τ)+K\n∑\nkmike(s(qi,d−\ni,k)/τ)+∑\nj̸=imije(s(qi,qj)/τ)+∑\nj̸=imije(s(d+\ni,dj)/τ),\n4\n--- Page 5 ---\nTechnical Report\nwhere these terms represent similarities with: (1) the positive document d+\ni, (2)Khard negatives d−\ni,k,\n(3) other in-batch queries qj, (4) other in-batch positive and negative documents dj. The mask factor\nmijis designed to mitigate the impact of false negatives and is defined as:\nmij=(\n0 if sij>s(qi,d+\ni) +0.1 or dj==d+\ni,\n1 otherwise,\namong which sijis the corresponding score of qi,djorqi,qj.\nFor the reranking model, we optimize the Supervised Fine-Tuning (SFT) loss defined as:\nLreranking =−logp(l|P(q,d)), (2)\nwhere p(·|∗)denotes the probability assigned by LLM. The label lis “yes” for positive documents\nand “no” for negatives. This loss function encourages the model to assign higher probabilities to\ncorrect labels, thereby improving the ranking performance.\n3.2 Multi-stage Training\nThe multi-stage training approach is a common practice for training text embedding models (Li et al.,\n2023; Wang et al., 2022; Chen et al., 2024). This strategy typically begins with initial training on large-\nscale, semi-supervised data that includes noise, followed by fine-tuning using smaller, high-quality\nsupervised datasets. This two-step process enhances the performance and generalization capabilities\nof embedding models. Large-scale weakly supervised training data contribute significantly to\nthe model’s generalization, while fine-tuning with high-quality data in subsequent stages further\nimproves model performance. Both stages of training for embedding models utilize the optimization\nobjective defined in Equation 1, whereas the reranking model training employs the loss function\ndefined in Equation 2 as the optimization target.\nBuilding upon the existing multi-stage training framework, the Qwen3 Embedding series introduces\nthe following key innovations:\n•Large-Scale Synthetic Data-Driven Weak Supervision Training: Unlike previous works (e.g.,\nGTE, E5, BGE models), where weakly supervised training data are primarily collected from\nopen-source communities such as Q&A forums or academic papers, we propose leveraging\nthe text understanding and generation capabilities of foundation models to synthesize pair\ndata directly. This approach allows for arbitrary definition of various dimensions of the\ndesired pair data, such as task, language, length, and difficulty within the synthesis prompts.\nCompared to data collection from open-domain sources, foundation model-driven data\nsynthesis offers greater controllability, enabling precise management of the quality and\ndiversity of the generated data, particularly in low-resource scenarios and languages.\n•High-Quality Synthetic Data Utilization in Supervised Fine Tuning: Due to the exceptional\nperformance of the Qwen3 Foundation model, the synthesized data is of notably high quality.\nTherefore, in the second stage of supervised training, selective incorporation of this high-\nquality synthetic data further enhances the overall model performance and generalization\ncapabilities.\n•Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\nfine-tuning, we applied a model merging technique based on spherical linear interpolation\n(slerp). This technique involves merging multiple model checkpoints saved during the\nfine-tuning process. This step aims to boost the model’s robustness and generalization\nperformance across various data distributions.\nIt is important to note that the reranking model’s training process does not include a first-stage\nweakly supervised training phase.\n5\n--- Page 6 ---\nTechnical Report\n3.3 Synthetic Dataset\nTo create a robust synthetic dataset for training models on various similarity tasks, we generate\ndiverse text pairs spanning categories such as retrieval, bitext mining, classification, and semantic\ntextual similarity (STS). The quality of these synthetic data pairs is ensured by utilizing the Qwen3-\n32B model as the foundational model for data synthesis. We have designed a diverse prompting\nstrategy to improve the variety and authenticity of the generated data. For instance, in the text\nretrieval task, we synthesize data using the multilingual pre-training corpus from Qwen3. During\nthe data synthesis process, specific roles are assigned to each document to simulate potential\nusers querying that document. This injection of user perspectives enhances the diversity and\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five\nrole candidates for each document from a role library and present these documents along with\ntheir role candidates to the prompt. This guides the model in outputting the most suitable role\nconfiguration for query generation. Moreover, the prompt incorporates various dimensions such as\nquery type (e.g., keyword, factual, summary, judgment), query length, difficulty, and language. This\nmultidimensional approach ensures the quality and diversity of the synthetic data.\nFinally, we create a total of approximately 150 million pairs of multi-task weak supervision training\ndata. Our experiments reveal that the embedding model trained with these synthetic data performs\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised\nmodels in the MTEB Multilingual benchmarks. This motivates us to filter the synthetic data to\nidentify high-quality pairs for inclusion in a second stage of supervised training. We employ a\nsimple cosine similarity calculation to select data pairs, retaining those with a cosine similarity\ngreater than 0.7 from randomly sampled data. Ultimately, approximately 12 million high-quality\nsupervised training data pairs are selected for further training.\nModel SizeMean\n(Task)Mean\n(Type)Bitext\nMiningClass-\nificationClus-\nteringInst.\nRetrievalMultilabel\nClass.Pair\nClass.Rerank Retrieval STS\nSelected Open-Source Models\nNV-Embed-v2 7B 56.29 49.58 57.84 57.29 40.80 1.04 18.63 78.94 63.82 56.72 71.10\nGritLM-7B 7B 60.92 53.74 70.53 61.83 49.75 3.45 22.77 79.94 63.78 58.31 73.33\nBGE-M3 0.6B 59.56 52.18 79.11 60.35 40.88 -3.11 20.1 80.76 62.79 54.60 74.12\nmultilingual-e5-large-instruct 0.6B 63.22 55.08 80.13 64.94 50.75 -0.40 22.91 80.86 62.61 57.12 76.81\ngte-Qwen2-1.5B-instruct 1.5B 59.45 52.69 62.51 58.32 52.05 0.74 24.02 81.58 62.58 60.78 71.61\ngte-Qwen2-7b-Instruct 7B 62.51 55.93 73.92 61.55 52.77 4.94 25.48 85.13 65.55 60.08 73.98\nCommercial APIs\ntext-embedding-3-large - 58.93 51.41 62.17 60.27 46.89 -2.68 22.03 79.17 63.89 59.27 71.68\nCohere-embed-multilingual-v3.0 - 61.12 53.23 70.50 62.95 46.89 -1.89 22.74 79.88 64.07 59.16 74.80\nGemini Embedding - 68.37 59.59 79.28 71.82 54.59 5.18 29.16 83.63 65.58 67.71 79.40\nQwen3 Embedding Models\nQwen3-Embedding-0.6B 0.6B 64.33 56.00 72.22 66.83 52.33 5.09 24.59 80.83 61.41 64.64 76.17\nQwen3-Embedding-4B 4B 69.45 60.86 79.36 72.33 57.15 11.56 26.77 85.05 65.08 69.60 80.86\nQwen3-Embedding-8B 8B70.58 61.69 80.89 74.00 57.65 10.06 28.66 86.40 65.63 70.88 81.08\nTable 2: Performance on MTEB Multilingual (Enevoldsen et al., 2025). For compared models, the\nscores are retrieved from MTEB online leaderboard on June 4th, 2025.\n4 Evaluation\nWe conduct comprehensive and fair evaluations across multiple benchmarks to assess the capabilities\nof Qwen3 Embedding models.\n4.1 Settings\nFor the text embedding models, we utilize the Massive Multilingual Text Embedding Benchmark\n(MMTEB) (Enevoldsen et al., 2025) for evaluation. MMTEB is a large-scale, community-driven\nexpansion of MTEB (Muennighoff et al., 2023), covering over 500 quality-controlled evaluation tasks\n6\n--- Page 7 ---\nTechnical Report\nModel Size Dim MTEB (Eng, v2) CMTEB MTEB (Code)\nMean (Task) Mean (Type) Mean (Task) Mean (Type)\nSelected Open-Source Models\nNV-Embed-v2 7B 4096 69.81 65.00 63.0 62.0 -\nGritLM-7B 7B 4096 67.07 63.22 - - 73.6α\nmultilingual-e5-large-instruct 0.6B 1024 65.53 61.21 - - 65.0α\ngte-Qwen2-1.5b-instruct 1.5B 1536 67.20 63.26 67.12 67.79 -\ngte-Qwen2-7b-instruct 7B 3584 70.72 65.77 71.62 72.19 56.41γ\nCommercial APIs\ntext-embedding-3-large - 3072 66.43 62.15 - - 58.95γ\ncohere-embed-multilingual-v3.0 - 1024 66.01 61.43 - - 51.94γ\nGemini Embedding - 3072 73.30 67.67 - - 74.66γ\nQwen3 Embedding Models\nQwen3-Embedding-0.6B 0.6B 1024 70.70 64.88 66.33 67.44 75.41\nQwen3-Embedding-4B 4B2560 74.60 68.09 72.26 73.50 80.06\nQwen3-Embedding-8B 8B4096 75.22 68.70 73.83 75.00 80.68\nTable 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code.αTaken from (Enevoldsen\net al., 2025).γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved\nfrom MTEB online leaderboard on June 4th, 2025.\nacross more than 250 languages. In addition to classic text tasks such as as a variety of retrieval,\nclassification, and semantic textual similarity, MMTEB includes a diverse set of challenging and\nnovel tasks, such as instruction following, long-document retrieval, and code retrieval, representing\nthe largest multilingual collection of evaluation tasks for embedding models to date. Our MMTEB\nevaluations encompass 216 individual evaluation tasks, consisting of 131 tasks for MTEB (Multilin-\ngual) (Enevoldsen et al., 2025), 41 tasks for MTEB (English, v2) (Muennighoff et al., 2023), 32 tasks\nfor CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).\nMoreover, we select a series of text retrieval tasks to assess the text reranking capabilities of our\nmodels. We explore three types of retrieval tasks: (1) Basic Relevance Retrieval, categorized into\nEnglish, Chinese, and Multilingual, evaluated on MTEB (Muennighoff et al., 2023), CMTEB (Xiao\net al., 2024), MMTEB (Enevoldsen et al., 2025), and MLDR (Chen et al., 2024), respectively; (2) Code\nRetrieval, evaluated on MTEB-Code (Enevoldsen et al., 2025), which comprises only code-related\nretrieval data.; and (3) Complex Instruction Retrieval, evaluated on FollowIR (Weller et al., 2024).\nCompared Methods We compare our models with the most prominent open-source text embed-\nding models and commercial API services. The open-source models include the GTE (Li et al.,\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-\nEmbed-v2 (Lee et al., 2025a), GritLM-7B Muennighoff et al. (2025). The commercial APIs evaluated\nare text-embedding-3-large from OpenAI, Gemini-embedding from Google, and Cohere-embed-\nmultilingual-v3.0. For reranking, we compare with the rerankers of jina1, mGTE (Zhang et al., 2024b)\nand BGE-m3 (Chen et al., 2024).\n4.2 Main Results\nEmbedding In Table 2, we present the evaluation results on MMTEB (Enevoldsen et al., 2025),\nwhich comprehensively covers a wide range of embedding tasks across multiple languages. Our\nQwen3-Embedding-4B/8B models achieve the best performance, and our smallest model, Qwen3-\nEmbedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,\nv2) (Muennighoff et al., 2023), CMTEB (Xiao et al., 2024), and MTEB (Code) (Enevoldsen et al.,\n2025). The scores reflect similar trends as MMTEB, with our Qwen3-Embedding-4B/8B models\n1https://hf.co/jinaai/jina-reranker-v2-base-multilingual\n7\n--- Page 8 ---\nTechnical Report\nBasic Relevance Retrieval\nModel Param MTEB-R CMTEB-R MMTEB-R MLDR MTEB-Code FollowIR\nQwen3-Embedding-0.6B 0.6B 61.82 71.02 64.64 50.26 75.41 5.09\nJina-multilingual-reranker-v2-base 0.3B 58.22 63.37 63.73 39.66 58.98 -0.68\ngte-multilingual-reranker-base 0.3B 59.51 74.08 59.44 66.33 54.18 -1.64\nBGE-reranker-v2-m3 0.6B 57.03 72.16 58.36 59.51 41.38 -0.01\nQwen3-Reranker-0.6B 0.6B 65.80 71.31 66.36 67.28 73.42 5.41\nQwen3-Reranker-4B 4B 69.76 75.94 72.74 69.97 81.20 14.84\nQwen3-Reranker-8B 8B 69.02 77.45 72.94 70.19 81.22 8.05\nTable 4: Evaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2),\nMTEB(cmn, v1) and MMTEB, which are MTEB-R, CMTEB-R and MMTEM-R. The rest are all retrieval\ntasks. All scores are our runs based on the retrieval top-100 results from the first row.\nModel MMTEB MTEB (Eng, v2) CMTEB MTEB (Code, v1)\nQwen3-Embedding-0.6B w/ only synthetic data 58.49 60.63 59.78 66.79\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58\nQwen3-Embedding-0.6B w/o model merge 62.56 68.18 64.76 74.89\nQwen3-Embedding-0.6B 64.33 70.70 66.33 75.41\nTable 5: Performance (mean task) on MMTEB, MTEB(eng, v2), CMTEB and MTEB(code, v1) for\nQwen3-Embedding-0.6B model with different training setting.\nconsistently outperforming others. Notably, the Qwen3-Embedding-0.6B model ranks just behind\nthe Gemini-Embedding, while being competitive with the gte-Qwen2-7B-instruct.\nReranking In Table 4, we present the evaluation results on various reranking tasks ( §4.1). We\nutilize the Qwen3-Embedding-0.6B model to retrieve the top-100 candidates and then apply different\nreranking models for further refinement. This approach ensures a fair evaluation of the reranking\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared\nto the embedding model and surpass all baseline reranking methods, with Qwen3-Reranker-8B\nachieving the highest performance across most tasks.\n4.3 Analysis\nTo further analyze and explore the key elements of the Qwen3 Embedding model training framework,\nwe conduct an analysis from the following dimensions:\nEffectiveness of Large-Scale Weakly Supervised Pre-Training We first analyze the effectiveness\nof the large-scale weak supervised training stage for the embedding models. As shown in Table 5,\nthe Qwen3-Embedding-0.6B model trained solely on synthetic data (without subsequent training\nstages, as indicated in the first row) achieves reasonable and strong performance compared to the\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak\nsupervised training stage (i.e., without synthetic data training, as seen in the second row), the final\nperformance shows a clear decline. This indicates that the large-scale weak supervised training\nstage is crucial for achieving superior performance.\nEffectiveness of Model Merging Next, we compare the performance differences arising from the\nmodel merging stage. As shown in Table 5, the model trained without model merging techniques\n(the third row, which uses data sampling to balance various tasks) performs considerably worse\nthan the final Qwen3-Embedding-0.6B model (which employs model merging, as shown in the last\nrow). This indicates that the model merging stage is also critical for developing strong models.\n8\n--- Page 9 ---\nTechnical Report\n5 Conclusion\nIn this technical report, we present the Qwen3-Embedding series, a comprehensive suite of text\nembedding and reranking models based on the Qwen3 foundation models. These models are\ndesigned to excel in a wide range of text embedding and reranking tasks, including multilingual\nretrieval, code retrieval, and complex instruction following. The Qwen3-Embedding models are\nbuilt upon a robust multi-stage training pipeline that combines large-scale weakly supervised\npre-training on synthetic data with supervised fine-tuning and model merging on high-quality\ndatasets. The Qwen3 LLMs play a crucial role in synthesizing diverse training data across multiple\nlanguages and tasks, thereby enhancing the models’ capabilities. Our comprehensive evaluations\ndemonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased\nto open-source the Qwen3-Embedding and Qwen3-Reranker models (0.6B, 4B, and 8B), making\nthem available for the community to use and build upon.\nReferences\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding:\nMulti-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge\ndistillation. In Findings of the Association for Computational Linguistics: ACL 2024 , pp. 2318–2335,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://\naclanthology.org/2024.findings-acl.137/ .\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua, M ´arton Kardos, Ashwin Mathur, David Stap,\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive\nmultilingual text embedding benchmark. In The Thirteenth International Conference on Learning\nRepresentations , 2025. URL https://openreview.net/forum?id=zl3pfz4VCV .\nTao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation\nwith 1,000,000,000 personas. arXiv preprint arXiv:2406.20094 , 2024.\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padman-\nabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,\npp. 2553–2561, 2020.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\narXiv:2410.21276 , 2024.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\n(1), pp. 6769–6781, 2020.\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\nand Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models.\narXiv preprint arXiv:2405.17428 , 2024.\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\nand Wei Ping. NV-embed: Improved techniques for training LLMs as generalist embedding\nmodels. In The Thirteenth International Conference on Learning Representations , 2025a. URL https:\n//openreview.net/forum?id=lgsyLSsDRe .\nJinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gus-\ntavo Hern ´andez ´Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding:\nGeneralizable embeddings from gemini. arXiv preprint arXiv:2503.07891 , 2025b.\n9\n--- Page 10 ---\nTechnical Report\nMingxin Li, Zhijie Nie, Yanzhao Zhang, Dingkun Long, Richong Zhang, and Pengjun Xie. Improving\ngeneral text embedding model: Tackling task conflict and data imbalance through model merging.\narXiv preprint arXiv:2410.15035 , 2024.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards\ngeneral text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv.org/\nabs/2308.03281 .\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. Zero-shot listwise document reranking\nwith a large language model. arXiv preprint arXiv:2305.02156 , 2023.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embed-\nding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics , pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/ .\nNiklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and\nDouwe Kiela. Generative representational instruction tuning. In The Thirteenth International Con-\nference on Learning Representations , 2025. URL https://openreview.net/forum?id=BC4lIvfSzv .\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 , 2018.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankvicuna: Zero-shot listwise docu-\nment reranking with open-source large language models. arXiv preprint arXiv:2309.15088 , 2023.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp.\n3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. URL\nhttps://aclanthology.org/D19-1410/ .\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih,\nNoah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text\nembeddings. In Findings of the Association for Computational Linguistics: ACL 2023 , pp. 1102–1121,\n2023.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\nhttps://arxiv.org/abs/2212.03533 .\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Im-\nproving text embeddings with large language models. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 11897–11916,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\n//aclanthology.org/2024.acl-long.642/ .\nOrion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme,\nDawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models\nto follow instructions. arXiv preprint arXiv:2403.15246 , 2024.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval , SIGIR ’24, pp. 641–649, New\nYork, NY, USA, 2024. Association for Computing Machinery. URL https://doi.org/10.1145/\n3626772.3657878 .\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025.\n10\n--- Page 11 ---\nTechnical Report\nLonghui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. A\ntwo-stage adaptation of large language models for text ranking. In Findings of the Association for\nComputational Linguistics ACL 2024 , pp. 11880–11891, 2024a.\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong\nYang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mGTE: Generalized\nlong-context text representation and reranking models for multilingual text retrieval. In Franck\nDernoncourt, Daniel Preo t ¸iuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing: Industry Track , pp. 1393–1412,\nMiami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/ .\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained\nlanguage models: A survey. ACM Transactions on Information Systems , 42(4):1–60, 2024.\nXiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei Yin, Qing Li,\nJiliang Tang, and Ruocheng Guo. Embedding in recommender systems: A survey. arXiv preprint\narXiv:2310.18608 , 2023.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for\neffective and highly efficient zero-shot ranking with large language models. In Proceedings of the\n47th International ACM SIGIR Conference on Research and Development in Information Retrieval , pp.\n38–47, 2024.\n11\n--- Page 12 ---\nTechnical Report\nA Appendix\nA.1 Synthetic Data\nWe construct four types of synthetic data—retrieval, bitext mining, semantic textual similarity, and\nclassification to enable the model to adapt to various similarity tasks during pre-training. To ensure\nboth multilingual and cross-lingual diversity, the data is generated using Qwen3 32B. Below is an\nexample of a synthetic retrieval text pair. The retrieval data is synthesized using a document-to-\nquery approach. We collect a multilingual corpus from the pre-training corpus of the Qwen3 base\nmodel to serve as the document source. A two-stage generation pipeline is then applied, consisting\nof: (1) configuration and (2) query generation. In the configuration stage, we use large language\nmodels (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top\nfive most relevant to the given document. This step aims to enhance the diversity of the generated\nqueries. The template used is as follows:\nGiven a **Passage** and **Character**, select the appropriate option from\nthree fields: Character, Question_Type, Difficulty, and return the output\nin JSON format.,→\n,→\nFirst, select the Character who are likely to be interested in the Passage\nfrom the candidates. Then select the Question_Type that the Character\nmight ask about the Passage; Finally, choose the Difficulty of the\npossible question based on the Passage, the Character, and the\nQuestion_Type.,→\n,→\n,→\n,→\nCharacter: Given by input **Character**\nQuestion_Type:\n- keywords: ...\n- acquire_knowledge: ...\n- summary: ...\n- yes_or_no: ...\n- background: ...\nDifficulty:\n- high_school: ...\n- university: ...\n- phd: ...\nHere are some examples\n<Example1> <Example2> <Example3>\nNow, generate the **output** based on the **Passage** and **Character** from\nuser, the **Passage** will be in {language} language and the **Character**\nwill be in English.,→\n,→\nEnsure to generate only the JSON output with content in English.\n**Passage**:\n{passage}\n**Character**:\n{character}\nIn the query generation stage, we use the configuration selected in the first stage to guide the\ngeneration of queries. Additionally, we explicitly specify the desired length and language of the\ngenerated query. The template used is as follows:\n12\n--- Page 13 ---\nTechnical Report\nGiven a **Character**, **Passage**, and **Requirement**, generate a query from\nthe **Character**'s perspective that satisfies the **Requirement** and can\nbe used to retrieve the **Passage**. Please return the result in JSON\nformat.,→\n,→\n,→\nHere is an example:\n<example>\nNow, generate the **output** based on the **Character**, **Passage** and\n**Requirement** from user, the **Passage** will be in {corpus_language}\nlanguage, the **Character** and **Requirement** will be in English.,→\n,→\nEnsure to generate only the JSON output, with the key in English and the value\nin {queries_language} language. ,→\n**Character**\n{character}\n**Passage**\n{passage}\n**Requirment**\n- Type: {type};\n- Difficulty: {difficulty};\n- Length: the length of the generated sentences should be {length} words;\n- Languange: the language in which the results are generated should be\n{language} language; ,→\nStage Dataset Size\nWeakly Supervised Pre-Training Synthetic Data ∼150M\nSupervised Fine TuningMS MARCO, NQ, HotpotQA, NLI,\nDureader, T2-Ranking, SimCLUE,\nMIRACL, MLDR, Mr.TyDi,\nMulti-CPR, CodeSearchNet .etc\n+ High-quality Synthetic DataLabeled Data: ∼7M\nSynthetic Data: ∼12M\nTable 6: Statistics of training data utilized at each stage.\nA.2 Detail Results\nMTEB(eng, v2) ParamMean\n(Task)Mean\n(Type)Class-\nificationClus-\nteringPair\nClass.Rerank Retrieval STS Summ.\nmultilingual-e5-large-instruct 0.6B 65.53 61.21 75.54 49.89 86.24 48.74 53.47 84.72 29.89\nNV-Embed-v2 7.8B 69.81 65.00 87.19 47.66 88.69 49.61 62.84 83.82 35.21\nGritLM-7B 7.2B 67.07 63.22 81.25 50.82 87.29 49.59 54.95 83.03 35.65\ngte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\nstella en1.5B v5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91\ngte-Qwen2-7B-instruct 7.6B 70.72 65.77 88.52 58.97 85.9 50.47 58.09 82.69 35.74\ngemini-embedding-exp-03-07 - 73.3 67.67 90.05 59.39 87.7 48.59 64.35 85.29 38.28\nQwen3-Embedding-0.6B 0.6B 70.70 64.88 85.76 54.05 84.37 48.18 61.83 86.57 33.43\nQwen3-Embedding-4B 4B 74.60 68.09 89.84 57.51 87.01 50.76 68.46 88.72 34.39\nQwen3-Embedding-8B 8B 75.22 68.70 90.43 58.57 87.52 51.56 69.44 88.58 34.83\nTable 7: Results on MTEB(eng, v2) (Muennighoff et al., 2023). We compare models from the online\nleaderboard.\n13\n--- Page 14 ---\nTechnical Report\nMTEB(cmn, v1) ParamMean\n(Task)Mean\n(Type)Class-\nificationClus-\nteringPair\nClass.Rerank Retrieval STS\nmultilingual-e5-large-instruct 0.6B 58.08 58.24 69.80 48.23 64.52 57.45 63.65 45.81\ngte-Qwen2-7B-instruct 7.6B 71.62 72.19 75.77 66.06 81.16 69.24 75.70 65.20\ngte-Qwen2-1.5B-instruct 1.5B 67.12 67.79 72.53 54.61 79.5 68.21 71.86 60.05\nQwen3-Embedding-0.6B 0.6B 66.33 67.44 71.40 68.74 76.42 62.58 71.03 54.52\nQwen3-Embedding-4B 4B 72.26 73.50 75.46 77.89 83.34 66.05 77.03 61.26\nQwen3-Embedding-8B 8B 73.84 75.00 76.97 80.08 84.23 66.99 78.21 63.53\nTable 8: Results on C-MTEB (Xiao et al., 2024) (MTEB(cmn, v1).\nMTEB(Code, v1) Avg. AppsCOIR-\nCodeSearch-\nNetCode-\nEdit-\nSearchCode-\nFeedback-\nMTCode-\nFeedback-\nSTCode-\nSearchNet-\nCCRCode-\nSearchNetCode-\nTrans-\nOcean-\nContestCode-\nTrans-\nOcean-DLCosQAStack-\nOverflow-\nQASynthetic-\nText2SQL\nBGE multilingual 62.04 22.93 68.14 60.48 60.52 76.70 73.23 83.43 86.84 32.64 27.93 92.93 58.67\nNV-Embed-v2 63.74 29.72 61.85 73.96 60.27 81.72 68.82 86.61 89.14 33.40 34.82 92.36 60.90\ngte-Qwen2-7B-instruct 62.17 28.39 71.79 67.06 57.66 85.15 66.24 86.96 81.83 32.17 31.26 84.34 53.22\ngte-Qwen2-1.5B-instruct 61.98 28.91 71.56 59.60 49.92 81.92 72.08 91.08 79.02 32.73 32.23 90.27 54.49\nBGE-M3 (Dense) 58.22 14.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65\nJina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49\nQwen3-Embedding-0.6B 75.41 75.34 84.69 64.42 90.82 86.39 91.72 91.01 86.05 31.36 36.48 89.99 76.74\nQwen3-Embedding-4B 80.06 89.18 87.93 76.49 93.21 89.51 95.59 92.34 90.99 35.04 37.98 94.32 78.21\nQwen3-Embedding-8B 80.68 91.07 89.51 76.97 93.70 89.93 96.35 92.66 93.73 32.81 38.04 94.75 78.75\nQwen3-Reranker-0.6B 73.42 69.43 85.09 72.37 83.83 78.05 94.76 88.8 84.69 33.94 36.83 93.24 62.48\nQwen3-Reranker-4B 81.20 94.25 90.91 82.53 95.25 88.54 97.58 92.48 93.66 36.78 35.14 97.11 75.06\nQwen3-Reranker-8B 81.22 94.55 91.88 84.58 95.64 88.43 95.67 92.78 90.83 34.89 37.43 97.3 73.4\nTable 9: Performance on MTEB(Code, v1) (Enevoldsen et al., 2025). We report nDCG@10 scores.\n14",
  "text_length": 42144
}