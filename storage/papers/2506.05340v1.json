{
  "id": "http://arxiv.org/abs/2506.05340v1",
  "title": "Exploring Diffusion Transformer Designs via Grafting",
  "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu",
  "authors": [
    "Keshigeyan Chandrasegaran",
    "Michael Poli",
    "Daniel Y. Fu",
    "Dongjun Kim",
    "Lea M. Hadzic",
    "Manling Li",
    "Agrim Gupta",
    "Stefano Massaroli",
    "Azalia Mirhoseini",
    "Juan Carlos Niebles",
    "Stefano Ermon",
    "Li Fei-Fei"
  ],
  "published": "2025-06-05T17:59:40Z",
  "updated": "2025-06-05T17:59:40Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05340v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05340v1  [cs.LG]  5 Jun 2025Exploring Diffusion Transformer Designs via Grafting\nKeshigeyan Chandrasegaran∗‡1,2Michael Poli∗1,2Daniel Y. Fu3,4Dongjun Kim1\nLea M. Hadzic1Manling Li1,5Agrim Gupta6Stefano Massaroli2\nAzalia Mirhoseini1Juan Carlos Niebles1,7†Stefano Ermon1†Li Fei-Fei1†\n1Stanford University2Liquid AI3Together AI4UC San Diego\n5Northwestern University6Google DeepMind7Salesforce Research\ngrafting.stanford.edu\nAbstract\nDesigning model architectures requires decisions such as selecting operators (e.g.,\nattention, convolution) and configurations (e.g., depth, width). However, evaluating\nthe impact of these decisions on model quality requires costly pretraining, limiting\narchitectural investigation. Inspired by how new software is built on existing code,\nwe ask: can new architecture designs be studied using pretrained models? To\nthis end, we present grafting , a simple approach for editing pretrained diffusion\ntransformers (DiTs) to materialize new architectures under small compute budgets.\nInformed by our analysis of activation behavior and attention locality, we construct\na testbed based on the DiT-XL/2 design to study the impact of grafting on model\nquality. Using this testbed, we develop a family of hybrid designs via grafting:\nreplacing softmax attention with gated convolution, local attention, and linear\nattention, and replacing MLPs with variable expansion ratio and convolutional\nvariants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs.\n2.27 for DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image\nmodel (PixArt- Σ), achieving a 1.43 ×speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by converting\nevery pair of sequential transformer blocks into parallel blocks via grafting. This\nreduces model depth by 2 ×and yields better quality (FID: 2.77) than other models\nof comparable depth. Together, we show that new diffusion model designs can be\nexplored by grafting pretrained DiTs, with edits ranging from operator replacement\nto architecture restructuring. Code and grafted models: grafting.stanford.edu.\n1 Introduction\nModel architecture design plays a central role in machine learning, alongside data, algorithms,\ncompute, and benchmarks. It defines a learnable function and entails key decisions, including\nthe choice of operators (e.g., attention, convolution) and configurations (e.g., model depth, width).\nDespite this, insight into architectures—what works and what doesn’t—is difficult to obtain due to\nthe prohibitive costs of training models from scratch, especially in today’s foundation model era. As a\nresult, studying new architectures remains a challenge, particularly for generative models. Much like\nhow new software is built on existing code rather than written from scratch, can pretrained models\nserve as scaffolds for studying new architectures? In this work, we investigate architectural editing of\npretrained models to study new architecture designs. We focus on diffusion transformers (DiTs), a\nclass of generative transformers widely used for image and video generation [1, 2, 3].\n∗Equal contribution.†Equal senior authorship.\n‡Part of this work was done at Liquid AI.\nCorrespondence to {keshik,poli}@stanford.edu\nPreprint. Under review.\n--- Page 2 ---\n(a) Model Architecture Design via Grafting\nRandom Init.\n High Quality Model\nPretrained Model High Quality Model\nPretrained Model(b) Grafting Procedure\nRegression\nLimited data\nEnd-to-end  \nFinetuning(i) Stage 1: Activation Distillation\n(ii) Stage 2: Lightweight FinetuningIdea: Architecture editing\nLimited  \ndataHigh Quality Model(e) Depth       Width \nGrafting(c) Image Generation with Hybrid Architectures obtained via Grafting\nA fantasy illustration of stoic \nrobot named Rob, a …\nMamba-2  \nFID=2.55\nSWA  \nFID=2.62\nHyena-Y  \nFID=2.61\nHyena-X  \nFID=2.64\nr=6 \nFID=2.38\nr=3 \nFID=2.53\nA.R=164.6  \nFID=2.77\nFull show of a Cairn Terrier \nwearing a Yellow Plaid …  Ancient misty moor, forest, \novergrown, wide, highly ...A young rabbit in adventure \nclothes sets off for a new …\n(d) Text-to-Image Generation with Hybrid Architectures obtained via GraftingTrain from  \nscratch\nGrafting\nFigure 1: Grafting overview. (a,b) Model architecture design via grafting. Studying new model\narchitecture designs requires costly pretraining. Grafting materializes new architectures by editing\npretrained models under small compute budgets (Sec. 3). (c) Class-conditional image generation.\nSamples generated by hybrid architectures obtained via grafting (Sec. 4). (d) High-resolution text-to-\nimage generation. 2048×2048 samples generated using a grafted model (Sec. 5). (e) Depth →width\ncase study. Samples generated using a model restructured via grafting (depth: 28 →14) (Sec. 6).\nA pretrained model implements a computational graph to perform tasks such as image or video\ngeneration. Given a new architectural idea and a pretrained model, we investigate whether the idea\ncan be materialized by modifying its computational graph under small compute budgets. For example,\none might hypothesize that a convolutional design could replace Multi-Head Attention (MHA) or\nMulti-Layer Perceptron (MLP) in a DiT. A simple way to materialize this idea is to replace MHA or\nMLP operators with a convolutional operator, while preserving model quality. This raises two key\nquestions: (Q1) operator initialization : How to initialize a new operator before integrating it into\nthe computational graph? (Q2) error accumulation : How to mitigate error propagation as multiple\noperators are integrated into the computational graph?\nTo address these questions, we present grafting1, a simple two-stage approach to architecture editing\n(Fig. 1). Grafting proceeds as follows: (i) activation distillation : This stage transfers the functionality\nof the original operator to the new one by distilling its activations using a regression objective.\n(ii)lightweight finetuning : This stage mitigates error propagation caused by integrating multiple new\noperators by finetuning using limited data. Architectural editing spans multiple strategies—adding,\nremoving, and replacing [ 5,6,7] operators. We focus on operator replacement as the core strategy:\nswapping one operator for another. Other strategies can be viewed as special cases of replacement.\nThe space of architecture editing is vast, raising a practical question: what types of replacements\nshould we study? To ground this investigation, we first establish a self-grafting baseline that replaces\nexisting operators with randomly initialized weights. We show that our two-stage grafting procedure\nrecovers near-baseline model quality, validating the approach. Building on this, we focus on replacing\nexisting operators with efficient alternatives, aiming to reduce model FLOPs while preserving quality.\nIn some cases, we explore replacements that increase model FLOPs to examine broader design\nchoices. To study this systematically, we construct a testbed based on DiT-XL/2. We define a set\nof architectural edits that enable us to evaluate how different grafting schemes affect model quality.\nWe highlight our design axes: (1) which operator to replace (e.g., MHA, MLP); (2) what to replace\nit with (e.g., convolutions); (3) how to select layers to edit (e.g., all layers); (4) replacement ratio\n(full vs. partial). The motivation for replacing MHA and MLP operators stems from both empirical\nevidence and architectural considerations: for MHA, our attention-locality analysis motivates the use\nof local operators, while for MLP, we adopt prior architecture ideas [8, 9, 10].\n1Grafting draws inspiration from horticultural grafting, where efficient components (scions) are integrated\ninto established systems (rootstock) to enhance functionality, such as yield and disease resistance [4].\n2\n--- Page 3 ---\nWe validate our grafting approach in increasingly challenging generative modeling setups:\nResult I: Grafting yields hybrid architecture designs with good quality for class-conditional\nimage generation (Sec. 4.2). We validate grafting using our testbed. For MHA (softmax attention),\nwe explore several alternatives: local gated convolution (Hyena-SE, and our proposed Hyena-X/\nHyena-Y), local attention (sliding window), and linear attention (Mamba-2). For MLPs, alternatives\ninclude MLPs with variable expansion ratio (ratios=3, 6), and a convolutional variant (Hyena-\nX). Interestingly, several interleaved hybrid architecture designs achieve FID scores between 2.38\nand 2.64 (DiT-XL/2 256x256 baseline: 2.27), showing that grafting can construct good quality\nhybrids (Tab. 4)2. Grafting is simple and lightweight: each experiment completes in under 24 hours\non 8×H100 GPUs, using less than 2% of pretraining compute.\nResult II: We construct efficient hybrid architectures for high-resolution text -to-image (T2I) gen-\neration via grafting (Sec. 5). We validate grafting in a challenging, real-world setting: 2048 ×2048\nresolution T2I generation using PixArt- Σ(DiT) [ 11]. This setting reflects key challenges: it operates\non long sequences (16,384 tokens), involves a multimodal setup with text conditioning, and lacks\ntraining data. We target MHA operators for grafting, as they account for over 62% of generation\nlatency. Using 12k synthetic data, our grafted model achieves a 1.43 ×speedup with <2% drop in\nGenEval score (47.78 vs. 49.75), showing that grafting scales to high-resolution, T2I generation.\nCase Study: Converting model depth to width via grafting (Sec. 6). Motivated by our MLP\ngrafting results, we rewire DiT-XL/2 by parallelizing every pair of transformer blocks, as modern\nGPUs favor parallel over sequential computation. This reduces model depth by 2 ×(28→14).\nThe grafted model achieves FID=2.77, outperforming other models of comparable depth. To our\nknowledge, this is the first attempt to convert sequential transformer blocks into parallel in pretrained\nDiTs, enabling architectures to be restructured.\n2 Prerequisites\nDiffusion models (DMs). DMs generate data samples by iteratively denoising random noise. This\nsampling process inversely mirrors the forward data corruption mechanism: zt=αtz+σtϵwhere\nz=E(x)∼q(z)withErepresenting a pretrained encoder and xthe data variable. The noise\ntermϵfollows the prior distribution N(0, I). The transition kernel from time 0totis given by\nqt(zt|z) =N(zt;αtz, σ2\ntI). The choice of αtandσtdefines the diffusion variant, such as variance-\npreserving [12], or flow matching [13]. The training objective [12] is as follows:\nLDM(ϕ) =Eq(t)q(z,c)N(ϵ;0,I)[∥ϵ−ϵϕ(zt, t,c)∥2\n2], (1)\nwhere q(t): time sampling distribution, and q(z,c): joint distribution of latent zand condition c.\nDiffusion transformers (DiTs). DiTs model the diffusion process by patchifying the input—noised\nimages or latent—into a sequence of 1D tokens with positional embeddings. These tokens are pro-\ncessed through transformer blocks comprising self-attention, feedforward layers, residual connections,\nand normalization layers. DiTs also incorporate conditioning signals, such as noise timestep ( t), class\nlabels ( c), or natural language prompts, enabling controllable generation [1, 14].\nDatasets and evaluation metrics. For class-conditional image generation, we use ImageNet-1K [ 15].\nWe follow [ 1] and report Inception Score (IS), FID, sFID, Precision, and Recall using 50k generated\nsamples (250 steps DDPM, cfg=1.5). For text-to-image generation, we report GenEval score [16].\n3 Grafting Diffusion Transformers\n3.1 Two-Stage Grafting Approach\nGrafting aims to materialize new architectures by editing a pretrained model’s computational graph.\nGiven that we focus on replacing existing operators with alternatives, this raises two questions:\n(Q1) How should a new operator be initialized before being integrated into the computational graph?\nStage 1: Activation distillation. We cast initialization as a regression task. Operators in a DiT block\nprocess [B, N, D ]inputs (batch, sequence, hidden) and output tensors of the same shape. Given a\n2Strictly speaking, variable expansion ratio MLPs constitute a heterogeneous design rather than a hybrid\n(i.e. they do not introduce a new operator class); we use ‘hybrid’ throughout the paper for simplicity.\n3\n--- Page 4 ---\npretrained operator fl\nϕat layer l, we learn a new operator gl\nθthat approximates fl\nϕ[17]. Since DiT\nactivations are continuous and smooth, this can be posed as a regression problem:\nL(θ) =Eq(t)q(z,c)qt(zt|z)\u0002\nLreg(gl\nθ(zt, t,c), fl\nϕ(zt, t,c))\u0003\n(2)\nwhere q(z,c)is the joint distribution of latent representation zand condition c,q(t)is the time\nsampling distribution, and qt(zt|z)is the transition kernel from time 0tot.Lregis a regression\nobjective such as L2. In practice, a good initialization requires as few as 8k samples.\n(Q2) How can we mitigate error propagation as multiple operators are integrated into the compu-\ntational graph? Stage 2: Lightweight finetuning. As more operators are replaced, initialization\nerrors propagate, leading to deviations from the pretrained model’s behavior. We apply end-to-end\nfinetuning with limited data to mitigate cumulative errors from stage 1. The fine-tuning objective is\ngiven in Equation 1. In practice, we find that competitive performance can be recovered using only\n10% of the training data, even when replacing all MHA or MLP layers in DiT-XL/2.\n3.2 Self-grafting Baseline\nPrior to studying new architectural designs, we introduce self-grafting , a simple control setup where\nexisting operators (e.g., MHA, MLP) are replaced with identical operators whose weights are\nrandomly initialized. This preserves the computational graph’s structure—operator types, receptive\nfields, and parameter count—while altering the computation performed. Self-grafting serves three\npurposes: (1) to assess the grafting procedure without architectural changes, (2) to provide a baseline\nfor comparing replacements, and (3) to study factors affecting performance, such as data scale,\nregression objectives, and hyperparameters.\n3.3 Activation Behavior Analysis and Self-grafting Results\nWe begin by analyzing the activation behavior of MHA and MLP operators across all layers in\nDiT-XL/2. In both cases, we observe large variance in activation values, particularly in deeper layers\n(Tab. 1 (i, ii)). When using regression-based distillation for Stage 1, these outliers affect optimization,\nparticularly under the commonly used L2objective which penalizes all errors quadratically. This\nmotivates a closer look at regression objectives. We study three regression objectives with different\nlevel of sensitivity to outliers— L2,L1, and Huber [ 18]—using a self-grafting setup. We select five\nrepresentative layers ( l= 1,8,17,27,28) for both MHA and MLP, spanning a range of activation\n(i) Range of MHA and MLP Activation Values for all layers (DiT-XL/2) (ii) Activation Distribution for Selected Layers\n2728 1 8 17\nfMLP,l\nϕx\nRange of activation values\n7 14 21 28\n0 5000 10000 15000 200002728 1 8 17\n DiT-XL/2 Layer\nActivationslog(Counts)fMHA,l\nϕ\nActivationslog(Counts)\nDiT-XL/2 LayerfMLP,l\nϕ\nfMHA,l\nϕx\nx\n(iii) MHA initialization\nIS↑ FID↓sFID↓Prec.↑Rec.↑\nBaseline 278.20 2.27 4.60 0.83 0.57\nRandom Init. 40.86 76.27 10.33 0.33 0.52\nL2 269.31 2.58 5.75 0.82 0.58\nHuber ( δ=1.0) 271.30 2.55 5.44 0.82 0.57\nL1 273.03 2.51 5.48 0.83 0.58(iv) MLP initialization\nIS↑ FID↓ sFID↓Prec.↑Rec.↑\nBaseline 278.20 2.27 4.60 0.83 0.57\nRandom Init. 2.18 297.34 161.76 0.01 0.00\nL2 265.34 2.33 4.38 0.81 0.59\nHuber ( δ=1.0) 262.93 2.38 4.49 0.81 0.59\nL1 235.54 2.83 4.69 0.77 0.61\nTable 1: Activation statistics and self-grafting (Stage 1) results (DiT-XL/2). (i) Activation ranges\n(max–min) across all 28 MHA and MLP operators, computed using 1,000 samples. Deeper layers\nexhibit higher variance in activation values. (ii)Activation distributions (log-scale histograms) for\nfive selected layers (1, 8, 17, 27, 28), used in our initialization study. MLP layers show higher\nvariance in activations than MHA, especially in deeper layers. (iii, iv) Stage 1 results for these layers\nusing L2, Huber, and L1 regression. L1 yields the best FID for MHA (2.51), while L2 performs best\nfor MLP (2.33), which contains 2 ×more parameters than MHA (10.6M vs. 5.3M). This study shows\nthat high-quality initialization can be achieved by choosing operator-specific regression objectives.\n4\n--- Page 5 ---\nvalues. Each operator is trained with 8K ImageNet-1K [ 15] samples, for 200 epochs with batch size\n64 and learning rate 1e−4. We use δ= 1.0for Huber objective. We then integrate the initialized\noperators into the pretrained DiT-XL/2 and evaluate quality without any finetuning.\nHigh-quality initialization can be achieved by choosing operator-specific regression objectives.\nStage 1 Stage 2 IS ↑ FID↓ sFID↓Prec.↑Rec.↑\nBaseline 278.20 2.27 4.60 0.83 0.57\nMHA (Full Self-grafting)\nRandom Init. 1.66 289.23 154.00 0.00 0.00\n0.63% – 117.68 16.78 13.69 0.60 0.61\n0.63% 0.63% 148.56 11.26 11.10 0.66 0.60\n0.63% 5.0% 270.39 2.70 5.46 0.81 0.57\n0.63% 10.0% 287.81 2.49 4.71 0.83 0.56\nMLP (Full Self-grafting)\nRandom Init. 1.27 314.72 204.99 0.00 0.00\n0.63% 10.0% 277.72 2.54 4.52 0.83 0.57\nTable 2: Full self-grafting (Stage 2) results (DiT-XL/2).\nWe report results after replacing all28 MHA and MLP\noperators using different amounts of training data. As we\nincrease the training data from 0.63% (8k) to 10.0% (128k),\nFID improves consistently. Using only 10% of the training\ndata, near-baseline performance is achieved: FID 2.49 for\nMHA and 2.54 for MLP.As shown in Tab. 1 (iii,iv), the choice\nof the regression objective affects per-\nformance. For MHA, L1achieves the\nbest FID (2.51), followed by Huber\n(2.55) and L2(2.58). For MLPs, L2\nperforms best (2.33), while L1under-\nperforms (2.83); notably, MLPs have\n2×more parameters than MHA which\nexplains its robustness to outliers [ 19].\nThis shows that high-quality initializa-\ntion requires tailored, activation-aware\nstrategies. Further, we evaluate vali-\ndation loss on held-out samples. For\nMHA, L1achieves the lowest loss; for\nMLPs, L2achieves the lowest loss for\nall blocks (See Sec. B.3).\nFull self-grafting with 10% data\nachieves near-baseline performance.\nWe extend our study to replace allMHA\nand MLP operators in DiT-XL/2 under\nthe self-grafting setup and evaluate the effect of data on recovery (Tab. 2). For MHA, replacing\nall 28 layers without adaptation results in a noticeable performance drop, but Stage 2 (lightweight\nfine-tuning) is highly effective: using just 10% of the training data (128k samples), we achieve an\nFID of 2.53 vs. 2.27 for the baseline. Similarly, full MLP self-grafting with 10% data yields an FID\nof 2.54. We use batch size 256, learning rate 1e−4, and 30k iterations. In both cases, the quality\nis within 0.3 FID of the baseline, showing that full self-grafting is feasible under modest data and\ncompute budgets.\n3.4 Locality Analysis of Self-attention\nFigure 2: Locality of self-attention in DiT-\nXL/2. We plot Lkvalues for all 28 MHA\noperators, averaged over timesteps and sam-\nples. At k=32, 15 out of 28 layers exhibit\nvalues exceeding 0.5, indicating that several\nMHA operators model local interactions.MHA scales quadratically with sequence length, mak-\ning it a computational bottleneck. A natural idea is\nto replace it with local operators, such as convolu-\ntion or local attention. However, this will fail if the\nmodel relies on long-range dependencies: for exam-\nple, replacing all MHA operators in DiT-XL/2 with\na sliding window attention degrades FID from 2.27\nto 53.9. To guide grafting, we quantify MHA local-\nity using a simple band- kmetric. Given an attention\nmatrix A∈RN×Nand a band size of k, we define a\nbi-directional band indicator matrix Bk∈RN×Nas:\n(Bk)i,j=\u001a1,if|i−j| ≤k\n0,otherwise\nThen, locality within a band of size kis computed as:\nLk=1\nNX\ni,jAi,j(Bk)i,j (3)\nWe compute Lkfor all 28 MHA operators in DiT-XL/2 using 50-step DDIM sampling (250 ImageNet\nsamples, sequence length 256, cfg scale 1.5), averaging across timesteps and samples. As shown in\nFig. 2, MHA is largely local: on average, for k=32, 15 out of 28 layers attend to more than 50%\nattention mass within the band. The first few layers ( l=1,2) display non-local attention patterns. Our\nanalysis provides guidance for replacing MHA operators with efficient local operators.\n5\n--- Page 6 ---\n4 Experiments I: Hybrid Architectures via Grafting\n4.1 Testbed and Experiment setup\nBuilding on our self-grafting results, we now ask: can we maintain model quality when existing\noperators are replaced with efficient alternatives? To investigate this, we study the grafting procedure\nalong four design axes:\n1. operator type to replace – MHA or MLP\n2. replacement operator type – such as convolutions\n3. layer selection strategy – replace operators in all layers or use heuristic-based selection\n4. replacement ratio – full or partial\nWe construct a testbed to systematically evaluate how design decisions affect generative quality\nunder grafting. We focus on efficient replacements that reduce FLOPs, but also include higher-FLOP\nvariants to explore a broader range of architectural edits. We target MHA and MLP operators, which\naccount for a significant portion of FLOPs in DiTs compared to other operators (e.g., normalization,\nactivation, residuals). The rationale for replacing MHA or MLP operators is grounded in both\nempirical and architectural considerations: for MHA, our attention locality analysis motivates the\nuse of local operators; for MLP, we leverage prior architecture ideas [ 8,20,21,22,9,10]. Given a\nreplacement operator, the decision to graft it to a model with Ltransformer layers spans a space of\n2Lconfigurations. To make this tractable, we study two layer selection strategies: full (replace all\noperators) and interleaved (replace operators in a repeating pattern) strategies. The latter is inspired\nby striped transformer designs [23, 24, 25]. Our testbed is detailed in Tab. 3.\nx\nvkq\nHyena-X\nx\nvkq\nHyena-Y\nShort Explicit ConvolutionDense\nHadamard Gating\nFigure 3: Our proposed Hyena-X and Hyena-Y ,\nefficient local gated convolution operators used\nas drop-in replacements for MHA.We introduce Hyena-X and Hyena-Y—two new\nefficient gated convolution operators designed as\ndrop-in replacements for MHA. While our study\nincludes several off-the-shelf efficient alternatives,\nwe also contribute new operator designs motivated\nby our MHA locality analysis. This allows us to test\nnovel architectural ideas via grafting, broadening\nour study. Both Hyena-X and Hyena-Y are local\ngated convolutions composed of dense, short causal\ndepth-wise 1D convolutions. Fig. 3 (left) illustrates\ntheir structure. We also adapt Hyena-X as an MLP\nalternative by applying it along the channel dimen-\nsion. Hyena-X and Hyena-Y scale linearly with\nsequence length, compared to the quadratic scaling\nof MHA. Operator details are provided in Sec. E.\nWe provide FLOP calculation for both operators in\nSec. F.1.\nExperiment setup. For our hybrid experiments, we mostly use the hyperparameters determined from\nour self-grafting studies.\nStage 1: Operator initialization. For each new operator, we perform activation distillation using\n8K ImageNet-1K samples. Each operator is trained for 200 epochs with a batch size of 64 and an\ninitial learning rate of 1e−4. We pre-extract and store all regression features. All operators can be\ninitialized in parallel. Each operator’s training completes in under 30 minutes on a single H100 GPU.\nExperiment details for stage 1 are included in Sec. B.1.\nStage 2: Lightweight finetuning. For all experiments in Table 3, we use 10% of the ImageNet-1K\ntraining data and train for 50K steps. We use a batch size of 256, linearly warming up the learning\nrate to 1e−4over 1000 steps. Experiments typically complete in under 10 hours on 8 ×H100 GPUs.\nFor specific ablations on increasing data, such as those involving 20% data or 100K steps, runtimes\nextend up to 24 hours ( <2% pretraining compute). We provide experiment details in Sec. B.1.\n6\n--- Page 7 ---\nOperator Type: Which operator types are we replacing?\nMHA, MLP\nEfficient Alternative: What do we replace it with?\nMHAConvolutions: Hyena-SE [ 24], Hyena-X/ Hyena-Y (Ours) K=4, causal\nLocal Attention: Sliding Window Attention (SWA) [ 26,27]w=4, bidirectional\nLinear Attention: Mamba-2 [28] ds=64,E=2\nMLPVariable expansion ratio r=3,6\nHyena-X (Ours) r=2,K=4, causal, mix channels\nLayer Selection: In which layers is the operator replaced?\nFull Replace the operator in all layers\nInterleaved Replace the operator in a repeating pattern (e.g., every 2 or 3 out of 4)\nReplacement Ratio: What percentage of operators are replaced?\n50%, 75%, 100%\nTable 3: Grafting testbed with configurations. This table defines the core design axes used in our\nstudy: operator type, efficient alternatives, layer selection strategy, and replacement ratio. For each\nalternative, we report configurations, including kernel size K, window size w, state size ds, expand\nfactor E, and MLP expansion ratio r. The baseline DiT-XL/2 operator uses H=16 attention heads\nand MLP expansion ratio r=4. Operator variants marked with (Ours) are proposed in this work.\n4.2 Results and Insights(a) Ablation 1: Data scaling (MHA)\nFull self-graftingHyena-SEHyena-XHyena-YSWAMamba-2\n2.55\n2.62\n2.61\n2.61\n2.61\n2.46\n2.65\n2.67\n2.72\n2.74\n2.73\n2.4910% Data20% Data\nFID\n(b) Ablation 2: Layer selection heuristics\n(MHA / Hyena-X)\n4.00\n3.18\n3.02\n2.74FID\nInterleaved (50%)Top-local (50%)Low-local (50%)Deep (50%)Full (100%)FID>140\nFigure 4: Ablation studies. (a) Data scale:\nIncreasing fine-tuning data from 10% to 20%\nimproves FID. (b) Layer selection strategies:\nInterleaved replacement outperforms other\nheuristics.MHA results. Replacing MHA operators in DiT-XL/2\nvia grafting yields strong quality-efficiency tradeoffs.\nWe discuss our key insights below:\n•Surprising effectiveness of operators with smaller\nreceptive fields under interleaved grafting . Our find-\nings highlight that at 50% interleaved replacement,\nseveral alternatives—including SWA, Hyena-X/Y ,\nand Mamba-2—consistently achieve FID scores\nwithin 0.5 of the baseline (2.27). The minimal FID\ndrop observed especially with the SWA and Hyena\nvariants, despite their limited receptive field ( K=4,\nw=4), aligns with our locality analysis (Section 3.4).\n•Replacement strategy: Interleaved vs. Full. Per-\nformance generally declines when increasing inter-\nleaved replacement from 50% to 75%. However,\nSWA remains effective at 75% interleaved replace-\nment (FID=3.09). At 100% replacement, perfor-\nmance sharply degrades (all FIDs > 75). This trend\naligns with our locality analysis, indicating that only\na subset of layers are local and amenable to grafting.\n•Ablations on data scale and layer selection. We\nstudy two factors under 50% MHA replacement.\n(i) Increasing fine-tuning data from 10% to 20%\nimproves FID across all variants (e.g., Hyena-\nX: 2.74 →2.61; SWA: 2.67 →2.62, Mamba-\n2: 2.65 →2.55) (Fig. 4 (a)). (ii) Under 50% replacement, we compare Hyena-X (interleaved)\nto three targeted heuristics: top-local (layers with highest band- kvalues), low-local (layers with\nlowest band- kvalues), and deep (last 14 layers). Interleaved yields the best FID (2.74), followed\nby top-local (3.02), low-local (3.18), and deep (4.00). These results confirm that interleaving is\neffective, and our band- kmetric identifies layers that are more amenable to grafting (Fig. 4 (b)).\n7\n--- Page 8 ---\nMLP results. Replacing MLP operators via grafting is effective. We discuss our key insights below:\n•Variable expansion ratio MLPs are effective under full replacement. MLP alternatives with\nexpansion ratio r=3 and r=6 demonstrate good quality under all replacement ratios. Even under\nfull (100%) replacement, both variants maintain good performance, with r=3 achieving FID=2.66.\nThis highlights that MLP width is a robust dimension for grafting.\n•Convolutional alternatives. Hyena-X which combines dense and local channel mixing, performs\ncompetitively at 50% replacement (FID=2.63) but degrades at higher ratios, suggesting that such\noperators are only effective at moderate ratios.\nTakeaway 1: Grafting is effective for constructing efficient hybrid architectures with good\ngenerative quality under small compute budgets. Interleaved designs are particularly effective.\nRatio IS ↑ FID↓sFID↓Prec.↑Rec.↑∆FLOPs op.↓∆FLOPs ft.↓∆Param ↓\nBaseline – 278.20 2.27 4.60 0.83 0.57 — — —\nMHA Grafting\nRandom Init. 100% 1.66 289.23 154.00 0.00 0.00 — — —\nSelf-grafting 100% 287.81 2.49 4.71 0.83 0.56 — — —\nHyena-SE\n(K=4)50% 274.73 2.73 5.05 0.82 0.56 -49.52% +0.13% +0.22%\n75% 231.15 3.62 6.04 0.81 0.54 -74.27% +0.20% +0.33%\n100% ✗ ✗ ✗ ✗ ✗ -99.03% +0.26% +0.43%\nHyena-X\n(K=4)50% 273.30 2.74 5.03 0.83 0.56 -49.90% +0.13% +0.16%\n75% 229.11 3.69 6.10 0.81 0.53 -74.85% +0.20% +0.24%\n100% ✗ ✗ ✗ ✗ ✗ -99.81% +0.26% +0.33%\nHyena-Y\n(K=4)50% 273.37 2.72 5.02 0.83 0.55 -49.52% 0.00% +0.05%\n75% 228.99 3.66 5.95 0.81 0.53 -74.27% 0.00% +0.08%\n100% ✗ ✗ ✗ ✗ ✗ -99.03% 0.00% +0.11%\nSWA\n(w=4)50% 280.62 2.67 4.90 0.83 0.56 -48.24% 0.00% 0.00%\n75% 249.99 3.09 5.54 0.82 0.55 -72.36% 0.00% 0.00%\n100% ✗ ✗ ✗ ✗ ✗ -96.48% 0.00% 0.00%\nMamba-2\n(ds=64,E=2)50% 285.08 2.65 4.84 0.83 0.55 -37.59% +77.89% +28.02%\n75% 257.66 3.02 5.48 0.82 0.53 -56.38% +116.83% +42.04%\n100% ✗ ✗ ✗ ✗ ✗ -75.17% +155.77% +56.05%\nMLP Grafting\nRandom Init. 100% 1.27 314.72 204.99 0.00 0.00 — — —\nSelf-grafting 100% 277.72 2.54 4.52 0.83 0.57 — — —\nExp. ratio ↓\n(r=3)50% 272.14 2.53 4.51 0.83 0.57 -12.50% 0.00% -12.50%\n75% 279.72 2.61 4.61 0.83 0.56 -18.75% 0.00% -18.75%\n100% 252.11 2.66 4.57 0.81 0.57 -25.00% 0.00% -25.00%\nExp. ratio ↑\n(r=6)50% 278.00 2.38 4.50 0.83 0.58 +25.00% 0.00% +25.00%\n75% 277.94 2.37 4.48 0.82 0.58 +37.50% 0.00% +37.50%\n100% 276.86 2.42 4.50 0.82 0.58 +50.00% 0.00% +50.00%\nHyena-X\n(r=2,K=4)50% 265.60 2.64 4.66 0.83 0.56 +0.01% 0.00% +0.02%\n75% 226.13 3.26 4.79 0.81 0.55 +0.02% 0.00% +0.03%\n100% ✗ ✗ ✗ ✗ ✗ +0.02% 0.00% +0.03%\nTable 4: Generation quality and efficiency of grafted hybrid architectures in DiT-XL/2. We\nreport quality (IS, FID, sFID, Precision, Recall) and efficiency ( ∆FLOPs and ∆Param) results for\nMHA and MLP grafting experiments. For each alternative, setups that maintain FID within 0.5 of\nthe baseline and offer the largest FLOPs reduction (or smallest FLOPs increase) are highlighted .\n✗denotes setups with poor generation (FID > 50). ∆FLOPs and∆Param denote the percentage\nchange in FLOPs and parameters, respectively (for MHA and MLP operators). For MHA, total\ncost is split into ∆FLOPs op.(softmax attention, gating) and ∆FLOPs ft.(QKV/output projections,\nfeaturizers). We do not use this decomposition for MLP variants. Mamba-2 incurs higher ∆FLOPs ft.\ndue to additional projections. Exact FLOP calculations are provided in Sec. F.1. Key result: Many\ninterleaved designs achieve good quality generation (FID within 0.5 of baseline).\n8\n--- Page 9 ---\n5 Experiments II: Grafting Text-to-Image Diffusion Transformers\nWe apply grafting to a more challenging setting: high-resolution text-to-image generation with\nPixArt- Σ[11]. This presents three challenges: (1) long sequences (16,384 tokens for 2048 ×2048\nresolution), (2) a multimodal setup with text conditioning, and (3) lack of publicly available training\ndata. These factors make PixArt- Σa representative setting for evaluating grafting under real-world\nconstraints. PixArt- Σcontains 28 transformer layers similar to DiT-XL/2.\nExperiment setup. We replace MHA operators in PixArt- Σwith Hyena-X via grafting, as MHA\naccounts for over 62% of generation latency. Hyena-X was chosen based on its good quality-efficiency\ntradeoff in the ImageNet-1K setup, achieving FID 2.61 with 20% data (see Fig. 4 (b)). Interleaved\ngrafting is applied for layers 8, 10, 12, 14, 16, 18, and 20–27; empirically, we found that layers 20–27\ncan be replaced without significant quality drop. For grafting, we created a small, uncurated synthetic\ndataset of 12k image-text pairs. The text prompts for this dataset were sampled from the 30k publicly\nreleased evaluation set. Stage 1 (activation distillation): 8k uncurated synthetic image-text pairs are\nused to initialize Hyena-X blocks. We use the L1 regression objective, as we observe similar MHA\nactivation behavior in PixArt- Σ(Fig. D.1). Stage 2 (finetuning): We use LoRA (rank=64) [ 29] for\nfinetuning. LoRA enables efficient finetuning by managing the high memory demands associated\nwith long sequences (16,384 tokens). The full 12k synthetic dataset is used in this stage. We use 20\nstep DPM Solver [30] for generation. Experiment details are provided in Sec. D.2.\nResults. The grafted model achieves a 1.43 ×speedup in wall-clock time, with a small drop in GenEval\nscore (47.78 vs. 49.75). Attribute-specific metrics remain comparable, and qualitative samples show\ngood alignment and quality. Some localized artifacts are observed in textured regions likely due to\nLoRA’s adaptation capacity and low-quality synthetic data (see failure cases in Fig. D.3, D.4).\nTakeaway 2: We graft high-resolution text-to-image DiTs, constructing hybrid architectures with\nmeaningful speedups and minimal quality drop.\nModel Ratio Obj(1) Obj(2) Count Colors Pos Color Attr. Overall ↑Latency (ms) ↓\nBaseline - 81.45 61.62 46.25 77.13 10.75 21.50 49.75 235.46\nHyena-X 29% 80.31 59.34 49.69 68.62 11.50 18.75 48.04 194.95 (1.21 ×)\nHyena-X 50% 80.00 57.07 48.13 70.74 11.25 19.50 47.78 164.58 (1.43×)\nTable 5: GenEval results and latency for PixArt- Σand the grafted variants. The 50% grafted\nmodel achieves a 1.43 ×speedup while retaining strong text-image alignment (GenEval overall score:\n47.78 vs. 49.75). Attribute-specific scores remain comparable across models. Latency is measured\nfor a single forward pass on an Nvidia H100 (batch size=2).\n6 Case Study: Converting Model Depth to Width via Grafting\nTransformer Layer NTransformer Layer N+1\nTransformer Layer NTransformer Layer N+1\n(a) Sequential Transformer Blocks(b) Rewiring Transformer Blocks in Parallel  via Grafting \nInputOutputOutputInput\nLinear\nFigure 5: Convert model depth →width via\ngrafting: (a) Two sequential transformer lay-\ners. (b) Rewiring in parallel via grafting\n(includes skip connections).Can we rewire two sequential transformer blocks\nto run in parallel? Our MLP grafting results showed\nthat MLPs are amenable to grafting, even at 100%\nreplacement with an expansion ratio of r= 6, demon-\nstrating that wider computation within an operator is\nfeasible. This success, combined with the fact that\nmodern GPUs favor parallel over sequential compu-\ntation, motivates a broader question: can we convert\ndeeper, sequential DiT computations into wider, par-\nallel ones via grafting while maintaining quality? To\nexplore this, we rewire DiT-XL/2 by parallelizing ev-\nery pair of sequential transformer blocks—each pair\nreceives the same input, and their outputs are merged\nvia a linear projection. This reduces model depth by\n2×(28→14) with a 6% increase in parameters.\n9\n--- Page 10 ---\nExperiment Setup. The rewiring schematic is shown in Fig. 5. Stage 1: Activation distillation. Each\nparallel pair was initialized via activation distillation using L1 regression. The weights for each block\nin the parallel pair were initialized from their corresponding pre-trained weights, rather than random\ninitialization. Similar to our previous experiments, 8k ImageNet-1K samples were used for this stage.\nStage 2: Lightweight finetuning. Given the architectural restructuring, finetuning was performed\nusing 25% of the training data. The learning rate was linearly warmed up to 1e-4 and halved at 75k\nand 150k iterations. Experiment details can be found in Sec. C.\nResults. The goal of this study is to evaluate generative quality (FID) vs. model depth. We report\nresults in Tab. 6. To contextualize our findings, we compare against two categories: (i) DiTs trained\nfrom scratch at lower depth, and (ii) pruning methods [ 31,32]. Our 14-layer grafted model achieves\nan FID of 2.77—surpassing DiT variants trained from scratch with similar or increased depth,\nincluding DiT-L/2 (depth 24, FID 3.73) and U-ViT-L (depth 21, FID 3.44). It also outperforms\npruning baselines such as TinyDiT-D14 with masked knowledge distillation (depth 14, FID 2.86)\nand BK-SDM (depth 14, FID 7.43), though these baselines have fewer parameters (340M) compared\nto the grafted variants (712M). To our knowledge, this is the first attempt to restructure DiTs by\nconverting sequential computation into parallel at the transformer block level.\nTakeaway 3: Grafting enables architectural restructuring at the transformer block level, allowing\ndepth to be traded for width.\nMethod Depth A.R Iters IS ↑FID↓sFID↓Prec.↑Recall ↑Speedup ↑Params ↓\nDiT-L/2 [1] 24 42.7 1,000K 196.26 3.73 4.62 0.82 0.54 — 458M\nU-ViT-L [33] 21 48.8 300K 221.29 3.44 6.58 0.83 0.52 — 287M\nDiT-B/2 [1] 12 64.0 1000K 119.63 10.12 5.39 0.73 0.55 — 130M\nBK-SDM [31] 14 82.3 100K 141.18 7.43 6.09 0.75 0.55 2 × 340M\nTinyDiT-D14 [32] 14 82.3 500K 198.85 3.92 5.69 0.78 0.58 2 × 340M\nTinyDiT-D14 w/ MKD [32] 14 82.3 500K 234.50 2.86 4.75 0.82 0.55 2 × 340M\nDiT-XL/2 [1] 28 41.4 7,000K 278.20 2.27 4.60 0.83 0.57 1 × 675M\nGrafting (Ours) 14 164.6 100K 231.91 3.12 4.71 0.82 0.55 2×¶712M\nGrafting (Ours) 14 164.6 230K 251.77 2.77 4.87 0.82 0.56 2×¶712M\nTable 6: Generative quality vs. model depth. Generative quality metrics are reported: FID, IS, sFID,\nPrecision, and Recall. A.R. (Aspect Ratio) is defined as model width divided by depth (e.g., 1152/14\n= 82.3). Parameters (Params) are reported in millions. For pruning and grafting setups, we report\nspeedup with respect to DiT-XL/2 (depth=28). Off-the-shelf DiT-L/2, U-ViT-L, and DiT-B/2 scores,\nalong with pruning baselines (BK-SDM, TinyDiT-D14, and TinyDiT-D14 w/ MKD), are sourced\nfrom [ 32]. MKD refers to Masked Knowledge Distillation, a recovery method used in TinyDiT [ 32].\nOur grafted models achieve better generative quality at depth=14, surpassing baselines in FID, IS,\nPrecision, and Recall.¶Speedup is measured for a single forward pass on an Nvidia H100 (batch\nsize=2). More implementation details are provided in Sec. C.\n7 Related Work\nDiffusion model architectures. Recently, many architectural innovations have been proposed for\ndiffusion models for image and video generation [ 34,35,36,37,38,39,40,41,42,43]. Many recent\nworks focus on improving the attention mechanism in diffusion models to enhance efficiency and\nscalability. One major direction is the use of modern linear attention variants, such as DiffuSSM [ 35],\nDiS [ 36], Zigma [ 37], DiM [ 38], and DIG [ 39]. Recently, text-to-image diffusion models such as\nSANA [ 34] have also adapted linear attention variants to support high-resolution generation. Another\nrecent direction explores the mixture-of-experts (MoE) idea. DiT-MoE [ 44] introduces sparse\ndiffusion transformers with shared expert routing and expert-level balance loss, enabling efficient\nscaling to 16.5B parameters while achieving competitive performance. We note that methods like\nSTAR [ 45] have also successfully discovered architectures via evolutionary methods for autoregressive\nlanguage modeling. While effective, these approaches require training from scratch, making such\nstudies expensive and inaccessible to practitioners. In contrast, grafting focuses on architecture\nediting of pretrained models to materialize new architectures under small compute budgets.\n10\n--- Page 11 ---\nArchitectural editing of pretrained models. Another line of work focuses on linearizing large\nlanguage models by replacing softmax attention with efficient operators, such as linear attention [ 5,6,\n7]. Similar ideas have also been adopted for diffusion models in [ 46,47,48], though these works\nfocus only on ultra-high-resolution settings. These prior efforts typically focus on replacing a single\noperator type (primarily attention) or are limited to specific application domains. Grafting presents a\nmore general and comprehensive approach for architectural editing. It extends beyond single-operator\nreplacement to enable modifying multiple operator types, exploring diverse architectural alternatives\n(e.g., both MHA and MLP replacements), and restructuring entire architectures (e.g., converting\ndepth to width). Recently, FFN Fusion [ 49] explored parallelizing transformer blocks in LLMs,\naiming to reduce sequential computation.\n8 Conclusion and Discussion\nIn this work, we introduced grafting , a simple approach to architecture editing. We constructed\nhybrid models by replacing self-attention and MLPs with efficient alternatives, achieving competitive\nquality (FID 2.38–2.64 vs. 2.27 baseline). We then applied grafting to a high-resolution text-to-image\nmodel (PixArt- Σ), yielding a 43% speedup with less than a 2% drop in GenEval score. We then used\ngrafting to restructure DiT-XL/2, converting every pair of sequential transformer blocks into parallel,\nreducing model depth by half and yielding better quality (FID 2.77) among 14-layer DiTs. These\nresults demonstrate grafting’s utility in both short- and long-context settings (e.g., ImageNet-1K and\nPixArt- Σ, respectively), and for architecture restructuring. Overall, grafting proves to be a lightweight\napproach for exploring diffusion transformer designs under small compute budgets.\nLimitations. This work primarily focuses on architectural editing of pretrained Diffusion Trans-\nformers (DiTs), specifically targeting Multi-Head Attention (MHA) and Multi-Layer Perceptron\n(MLP) components. Other architectural components, such as normalization layers and activation\nfunctions, will be explored in future work. We note that these are latent diffusion models, and grafting\ncomponents in their corresponding V AEs remains an area for future study. Our experiments primarily\nfocus on DiTs, and generalizing grafting to other model families, such as autoregressive models,\nis a direction for future research. The PixArt- Σsetup used synthetic data for grafting, which may\npropagate artifacts and biases into the grafted models. While this work focuses on architectural\nediting, it remains an open question whether architectures that perform well under grafting also\nperform well when trained from scratch. Finally, grafting requires access to a pretrained model.\nApplications and future work. Grafting holds promise for diverse applications where efficiency\nis important. This includes adapting models from low-resolution to high-resolution generation,\nextending capabilities from short-form video understanding/generation to long-form [ 50,51], or\nimproving user experience in interactive applications like image editing where even modest speedups\n(e.g., 10%) are highly valued. We hope that our testbed, insights, and results will encourage the\ncommunity to actively explore new architecture designs.\nAcknowledgments\nWe thank Liquid AI for sponsoring compute for this project. We also thank Armin W. Thomas, Garyk\nBrixi, Kyle Sargent, Karthik Dharmarajan, Stephen Tian, Cristobal Eyzaguirre, and Aryaman Arora\nfor their feedback on the manuscript.\n11\n--- Page 12 ---\nReferences\n[1]William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 4195–4205, 2023. 1, 3, 10\n[2]Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor,\nTroy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as\nworld simulators. 2024. 1\n[3] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José\nLezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662 , 2023. 1\n[4]Eliezer Goldschmidt. Plant grafting: new mechanisms, evolutionary implications. Frontiers in Plant\nScience , 5:727, 12 2014. 2\n[5]Michael Zhang, Simran Arora, Rahul Chalamala, Benjamin Frederick Spector, Alan Wu, Krithik Ramesh,\nAaryan Singhal, and Christopher Re. Lolcats: On low-rank linearizing of large language models. In The\nThirteenth International Conference on Learning Representations , 2025. 2, 11\n[6] Junxiong Wang, Daniele Paliotta, Avner May, Alexander M Rush, and Tri Dao. The mamba in the llama:\nDistilling and accelerating hybrid models. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems , 2024. 2, 11\n[7] Aviv Bick, Kevin Li, Eric Xing, J Zico Kolter, and Albert Gu. Transformers to ssms: Distilling quadratic\nknowledge to subquadratic models. Advances in Neural Information Processing Systems , 37:31788–31812,\n2024. 2, 11\n[8]Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin\nSpector, Michael Poli, Atri Rudra, and Christopher Ré. Monarch mixer: A simple sub-quadratic gemm-\nbased architecture. Advances in Neural Information Processing Systems , 36:77546–77603, 2023. 2,\n6\n[9]Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from\ndense checkpoints. In The Eleventh International Conference on Learning Representations , 2023. 2, 6\n[10] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361 , 2020. 2, 6\n[11] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping\nLuo, Huchuan Lu, and Zhenguo Li. Pixart-P: Weak-to-strong training of diffusion transformer for 4k\ntext-to-image generation, 2024. 3, 9\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems , 33:6840–6851, 2020. 3\n[13] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\ngenerative modeling. arXiv preprint arXiv:2210.02747 , 2022. 3\n[14] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, and Zhenguo Li. Pixart- α: Fast training of diffusion transformer for photorealistic\ntext-to-image synthesis. In The Twelfth International Conference on Learning Representations , 2023. 3\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255,\n2009. 3, 5\n[16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for\nevaluating text-to-image alignment. Advances in Neural Information Processing Systems , 36:52132–52152,\n2023. 3\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In\nNeurIPS Deep Learning and Representation Learning Workshop , 2015. 4\n[18] Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):73\n– 101, 1964. 4\n[19] Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear\nregression. Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020. 5\n12\n--- Page 13 ---\n[20] Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim,\nYungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet\neffective depth up-scaling. In Proceedings of the 2024 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track) ,\npages 23–35, 2024. 6\n[21] Shikai Qiu, Andres Potapczynski, Marc Anton Finzi, Micah Goldblum, and Andrew Gordon Wilson.\nCompute better spent: Replacing dense layers with structured matrices. In International Conference on\nMachine Learning , pages 41698–41716. PMLR, 2024. 6\n[22] Jackson Petty, Sjoerd Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. The impact\nof depth on compositional generalization in transformer language models. In Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers) , pages 7232–7245, 2024. 6\n[23] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting,\nTaiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and scaling of hybrid\narchitectures. arXiv preprint arXiv:2403.17844 , 2024. 6\n[24] Jerome Ku, Eric Nguyen, David W Romero, Garyk Brixi, Brandon Yang, Anton V orontsov, Ali\nTaghibakhshi, Amy X Lu, Dave P Burke, Greg Brockman, et al. Systems and algorithms for convo-\nlutional multi-hybrid language models at scale. arXiv preprint arXiv:2503.01868 , 2025. 6, 7\n[25] Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A.\nGonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara\nRicci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton V orontsov, Brandon Yang, Myra\nDeng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann,\nStefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K.\nMofrad, Madelena Y . Ng, Jaspreet Pannu, Christopher Ré, Jonathan C. Schmok, John St. John, Jeremy\nSullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-\nBoussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi,\nPatrick D. Hsu, and Brian L. Hie. Genome modeling and design across all domains of life with evo 2.\nbioRxiv , 2025. 6\n[26] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150 , 2020. 7\n[27] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509 , 2019. 7\n[28] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through\nstructured state space duality. In Forty-first International Conference on Machine Learning , 2024. 7\n[29] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on Learning\nRepresentations , 2022. 9\n[30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\nsolver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information\nProcessing Systems , 35:5775–5787, 2022. 9\n[31] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: A lightweight,\nfast, and cheap version of stable diffusion. In European Conference on Computer Vision , pages 381–399.\nSpringer, 2024. 10\n[32] Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang. Tinyfusion: Diffusion transformers learned\nshallow. arXiv preprint arXiv:2412.01199 , 2024. 10\n[33] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A\nvit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 22669–22679, 2023. 10\n[34] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang\nLi, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion\ntransformers. arXiv preprint arXiv:2410.10629 , 2024. 10\n[35] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8239–8249, 2024. 10\n13\n--- Page 14 ---\n[36] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state\nspace backbone. arXiv preprint arXiv:2402.05608 , 2024. 10\n[37] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer,\nand Bjorn Ommer. Zigma: Zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802 , 2024. 10\n[38] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim:\nDiffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224 , 2024. 10\n[39] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang.\nDig: Scalable and efficient diffusion models with gated linear attention. arXiv preprint arXiv:2405.18428 ,\n2024. 10\n[40] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao\nChen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu\nQing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui,\nSheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang,\nHoumin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei\nSun, Xiao Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai\nZhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, and Lu Jiang. Seaweed-7b:\nCost-effective training of video generation foundation model. 2025. 10\n[41] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. A\nsurvey on video diffusion models. ACM Comput. Surv. , 57(2), November 2024. 10\n[42] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong, and Lin Ma. Matten: Video generation\nwith mamba-attention. arXiv preprint arXiv:2405.03025 , 2024. 10\n[43] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao\nLuo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video\ngeneration with linear computational complexity. arXiv preprint arXiv:2412.09856 , 2024. 10\n[44] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transform-\ners to 16 billion parameters. arXiv preprint arXiv:2407.11633 , 2024. 10\n[45] Armin W Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, and Michael Poli. Star: Syn-\nthesis of tailored architectures. In The Thirteenth International Conference on Learning Representations ,\n2025. 10\n[46] Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion\ntransformers up. arXiv preprint arXiv:2412.16112 , 2024. 11\n[47] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image.\narXiv preprint arXiv:2409.02097 , 2024. 11\n[48] Philipp Becker, Abhinav Mehrotra, Ruchika Chavhan, Malcolm Chadwick, Luca Morreale, Mehdi Noroozi,\nAlberto Gil Ramos, and Sourav Bhattacharya. Edit: Efficient diffusion transformers with linear compressed\nattention. arXiv preprint arXiv:2503.16726 , 2025. 11\n[49] Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak\nGolan, Ehud Karpas, Itay Levy, Zach Moshe, et al. Ffn fusion: Rethinking sequential computation in large\nlanguage models. arXiv preprint arXiv:2503.18908 , 2025. 11\n[50] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre,\nZane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding.\nInAdvances in Neural Information Processing Systems , volume 37, 2024. 11\n[51] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin\nByeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting long-context post-training for frontier\nvision-language models. arXiv preprint arXiv:2504.15271 , 2025. 11\n[52] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\nStefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In\nInternational Conference on Machine Learning , pages 28043–28078. PMLR, 2023. 19\n14\n--- Page 15 ---\nSupplementary Material\n• Section A : Standard Deviation of Experiments\n• Section B : Hybrid Architecture Experiments: Additional details\n–Section B.1 : Experiment details and additional samples\n–Section B.2 : Modulated Regression Targets for MHA\n–Section B.3 : Validation Loss Curves for Self-grafting Experiments\n• Section C : Depth to Width Grafting Experiments: Additional details\n• Section D : Text-to-Image Generation Experiments: Additional details\n–Section D.1 : MHA activation plots\n–Section D.2 : Experiment Details\n–Section D.3 : Generated samples and failure cases\n• Section E : Hyena-X and Hyena-Y operators: Additional details\n• Section F : FLOP calculation\nA Standard Deviation of Experiments\nTo compute variance associated with our reported results, we repeat two representative experi-\nments—MHA (Hyena-Y) and MLP (width=6)—using three different random seeds ( seed = 0,\n200, 300 ). We follow the exact grafting setup used in the main paper for these experiments. We\nreport the mean and standard deviation of IS, FID, sFID, Precision and Recall in Tab. A.1. We\nobserve that the standard deviations are within an acceptable range.\nSetup IS FID sFID Precision Recall\nMHA/ Hyena-Y 273.19 ±0.46 2.73 ±0.01 5.06 ±0.04 0.83 ±0.00 0.55 ±0.00\nMLP/ higher width ( r= 6)277.91 ±0.95 2.41 ±0.01 4.48 ±0.02 0.82 ±0.00 0.58 ±0.00\nTable A.1: Mean and standard deviation of IS, FID, sFID, Precision and Recall calculated for three\nruns with different random seeds ( 0,200,300 ).\nB Hybrid Architecture Experiments: Additional details\nB.1 Experiment details and additional samples\nWe provide all hyperparameters used for the experiments in Tab. B.1. To ensure a fair comparison,\nwe used identical hyperparameters across every hybrid experiment. We include additional qualitative\nsamples generated using our hybrid architectures obtained via grafting in Fig. B.1.\nB.2 Modulated Regression Targets for MHA\nFor Stage 1, we explored a modulation-aware regression variant for MHA experiments that incor-\nporates the learned scalar ( gate_msa ) applied to the attention output. In the standard setup, we\nregress from input xto the raw output of the attention block y=MHA(·). In the modulation-aware\nformulation, the target becomes y=gate_msa ⊙MHA(·). Tab. B.2 compares these two variants with\nL1 and L2 loss. Modulation-aware regression increases target scale, which adversely affects L2 loss\nperformance due to its sensitivity to large values. L1 performs similarly in both settings. We adopted\nthe standard (modulation-agnostic) formulation for all experiments for simplicity.\nB.3 Validation Loss Curves for Self-grafting Experiments\nTo support the trends reported in the main paper (Sec.3.2, Table 2), we include validation loss curves\nin Fig.B.2 for five representative layers. Loss is computed using L2on a held-out set of 8k samples.\nFor MHA layers (top row), L1consistently achieves lower validation loss in deeper layers, reflecting\nrobustness to high activation variance. For MLP layers (bottom row), L2generalizes best across all\nlayers. This contrast may be explained by parameter count: MLPs have roughly 2 ×more parameters\nthan MHA layers, making them less sensitive to outliers and better suited to L2regression.\n--- Page 16 ---\nStage 1: Activation Distillation\nInitial Learning Rate 1×10−3\nWeight Decay 0\nEpochs 200\nBatch Size 64\nClip Norm 10.0\nOptimizer AdamW ( betas = (0.9,0.999) )\nLoss Function L1 (MHA), L2 (MLP)\nStage 2: Lightweight Finetuning\nInitial Learning Rate 1×10−4\nWeight Decay 5×10−5\nIterations 50,000 (100 epochs)\nBatch Size 256\nOptimizer AdamW ( betas = (0.9,0.999) )\nScheduler Linear Warmup over 1K steps, then constant lr\nTraining Data 10% of ImageNet-1K (128k samples)\nTable B.1: Experiment details for MHA/MLP grafting experiments using DiT-XL/2 (ImageNet-1K).\nModulation-aware IS FID sFID Precision Recall\nBaseline 278.20 2.27 4.60 0.83 0.57\nL2✓ 246.17 3.00 7.11 0.79 0.58\n✗ 269.31 2.58 5.75 0.82 0.58\nL1✓ 272.86 2.51 5.29 0.82 0.57\n✗ 273.03 2.51 5.48 0.83 0.58\nTable B.2: Comparison of modulation-aware and standard regression targets for Stage 1. The\nmodulation-aware setup includes the learned scalar ( gate_msa ) as a multiplicative factor in the\nregression target. L2 loss is sensitive to the amplified target scale and performs worse, while L1 loss\nremains robust and performs similarly in both cases. We adopt the standard formulation by default.\nC Depth to Width Grafting Experiments: Additional details\nWe provide all hyperparameters used for these experiments in Tab. C.1. We show additional samples\nin Fig. C.1.\nImplementation details. The generative quality metrics (FID, IS, sFID, Precision, Recall) reported\nin Tab. 6 correspond to the exact implementation of rewiring presented in Fig. 5. For speedup\nmeasurements, a simple fused version of our rewired version was used. It is important to note that\nthe reported speedup values are expected to decrease with large batch sizes, primarily due to the\nmodel parameter count (712M). Future work will focus on exploring hardware-aware / optimized\nimplementations to achieve consistent speedup across a wider range of batch sizes.\n16\n--- Page 17 ---\nFigure B.1: Additional samples generated by grafted DiT-XL/2 variants. Each row corresponds\nto a different hybrid. We report FID scores ( lower is better , ImageNet-1K 256 ×256) for each\nhybrid. MHA variants (top 4 rows) : Hyena-X (2.61), Hyena-Y (2.61), SWA (2.62), Mamba-2 (2.55).\nMLP variants (bottom 3 rows) : Lower width (2.53), Higher width (2.38), Hyena-X MLP (2.64).\nThese results highlight the flexibility of grafting in constructing high-quality hybrid architectures by\nreplacing MHA or MLP operators.\nStage 1: Activation Distillation\nInitial Learning Rate 1×10−4\nRegression Objective L1\nEpochs 200\nBatch Size 64\nOptimizer AdamW ( betas = (0.9,0.999) )\nStage 2: Lightweight Finetuning\nLearning Rate 1×10−4\nWeight Decay 0\nIterations 230k\nBatch Size 256\nOptimizer AdamW ( betas = (0.9,0.95))\nScheduler Warmup over 1K steps, half every 75k steps\nTraining Data 25% of ImageNet-1K (320k)\nTable C.1: Experiment details for depth-to-width grafting experiments using DiT-XL/2 (ImageNet-\n1K).\n17\n--- Page 18 ---\nL1LHuber,δ=1.0L201002000.0112\n010020011020\n010020011040\n01002001020100\n0100200510100val.lossOperator Initialization Results (Self-grafting/ MHA)val.loss\nval.loss\nval.loss\nval.lossLayer 1Layer 17Layer 8Layer 27Layer 28\nL1LHuber,δ=1.0L201002000.00112\n010020011020\n01002000.11020\n01002000.11020\n01002000.110100val.lossOperator Initialization Results (Self-grafting/ MLP)val.loss\nval.loss\nval.loss\nval.lossLayer 1Layer 17Layer 8Layer 27Layer 28Figure B.2: Validation loss curves for MHA (top) and MLP (bottom) operator distillation showing the\ntraining dynamics for three regression objectives. As one can observe, L1 shows better generalization\nfor MHA and L2 shows better generalization for MLP.\nFigure C.1: Depth-to-width grafting samples . Samples from a DiT-XL/2 model in which every pair\nof transformer block is converted into a parallel block, effectively reducing depth by 2×(FID=2.77).\nD Text-to-Image Generation Experiments: Additional details\nD.1 MHA activation plots\nWe show activation distribution for five representative layers ( l= 15,17,19,21,23) in Fig. D.1.\nD.2 Experiment details\nWe provide all hyperparameters used in our PixArt- Σgrafting experiments in Tab. D.1.\nD.3 Generated samples and failure cases\nWe show additional high-resolution samples generated by the grafted PixArt- Σmodel in Fig. D.2,\nillustrating the model’s ability to preserve generative quality across diverse prompts despite substantial\narchitectural edits. Figure D.3 illustrates two types of failure modes observed in grafted PixArt- Σ\noutputs. Each column pair shows the output of PixArt- Σ(left) and the grafted model (right) for the\nsame text prompt. In the top row, the original model generates images that are reasonably aligned\nwith the prompts, while the grafted model fails to preserve this alignment—indicating limitations\nduring the LoRA-based finetuning stage. In the bottom row, the synthetic supervision itself is of low\nquality, resulting in poor outputs from both the original and grafted models. To better understand this\nissue, Figure D.4 presents additional examples of low-quality synthetic data produced by PixArt- Σ\nand used for grafting. These samples often exhibit artifacts and unrealistic physics. While synthetic\ndata enables low-cost adaptation, these results highlight the importance of improved data curation\nand filtering to avoid propagating errors during the grafting process.\n18\n--- Page 19 ---\nActivation Values DiT-XL/2 Layerlog(Counts)x(PixArt−Σ)  Activation Distribution for Selected Layers\nfMHA\nl(x)\nActivation Values log(Counts)\nDiT-XL/2 LayerFigure D.1: Similar to DiT-XL/2, we observe high activation variance in PixArt- ΣMHA operators.\nWe show input (left) and output (right) activation values corresponding to five representative layers\n(15, 17, 19, 21, 23) in PixArt- Σ.\nThe image captures a scene of stark \nbeauty in the ...\nOne watercolor illustration of a \nman on a white back …  The image captures a closeup view \nof a vibrant red …A very large cartoon panda wear a \nbaseball cap, in the style of …\nIn the professional oil painting \nstudio there is an …\nHandmade vintage sock monkey \ndoll, attention to ...\nToday marks the beginning of \nautumn, summer has not …  Fresh coriander leaves plant on \nwhite background, …The image captures a moment of \ntranquility in the deep blue …\nPaint a captivating watercolor \nclipart of a Lovebird, …\nFigure D.2: Additional 2048 ×2048 samples generated using our grafted PixArt- Σmodel.\nE Hyena-X and Hyena-Y operators: Additional details\nInformed by our band-k analysis of MHA operators, we introduce a collection of efficient operators\ndesigned to exploit the locality in attention matrices. Given an input x∈Rℓ×d, a generic Hyena\noperator performs the following transformation:\nqc\ns=X\ns′Tc\nss′X\nc′xc′\ns′Wc′c\nkc\ns=X\ns′Hc\nss′X\nc′xc′\ns′Uc′c\nvc\ns=X\ns′Kc\nss′X\nc′xc′\ns′Pc′c\nyc\ns=X\nc′X\ns′(qc′\nsGc′\nss′kc′\ns′vc′\ns′)Mc′c\nwhere W, U, P, M ∈Rd×dare parametrized as dense or low-rank matrices, and T, H, K, G ∈Rℓ×ℓ\nare Toeplitz matrices corresponding to convolutions with the filters hT, hH, hK, hG, respectively. In\nthe original formulation [ 52], the filters hT, hH, hKare short and explicitly parametrized, whereas\nhGis implicitly parametrized.\n19\n--- Page 20 ---\nThe image presents a delightful assortment of four donuts, each with its \nown unique topping, set …\nPlaying classical guitar, Aesthetic flower, Full body shot, anime style, \nvector …\nA dog chasing its tail looks cute and funny, abstract photography, \nstippling, UHD, high ...\nThis image captures a moment on a city street, frozen in black and white. \nDominating the left …Figure D.3: Text-to-image generation failure cases. Each pair shows outputs from PixArt- Σ(left)\nand the grafted model (right) for the same prompt. In the top row, the prompt specifies four donuts\nwith unique toppings and a full-body anime-style character playing classical guitar. The grafted\noutputs deviate from these prompts—showing incorrect object counts (e.g., five donuts) and degraded\nstructure (e.g., distorted hands), reflecting text-image misalignment and visual artifacts introduced\nduring grlafting. In the bottom row, the supervision itself is poor: prompts such as a ‘dog chasing its\ntail in UHD stippling style’ and ‘a black-and-white street photo’ are not faithfully captured by either\nmodel. These examples highlight challenges arising both from LORA finetuning and low-quality\nsynthetic data.\nFigure D.4: Examples of low-quality samples generated by PixArt- Σused for grafting . These\nimages contain unrealistic features, inconsistent physics, and visual artifacts. Their presence in the\ngrafting dataset can degrade generation quality of grafted models, highlighting the importance of\ndata curation when using synthetic data.\n20\n--- Page 21 ---\nStage 1: Activation Distillation\nInitial Learning Rate 1×10−4\nWeight Decay 1×10−5\nEpochs 100\nClip Norm Value 0.1 (Layers 20-27), 0.01 (Other layers)\nBatch Size 16\nOptimizer AdamW\nScheduler Half lr at epochs = 50\nStage 2: Lightweight Finetuning\nInitial Learning Rate 1×10−5\nWeight Decay 0\nIterations 18k\nBatch Size 64 (with gradient accumulation)\nOptimizer AdamW\nScheduler linear warmup (500 steps), then constant lr\nLoRA rank 64\nTable D.1: Experiment details for PixArt- Σgrafting experiments.\nWe build on this formulation and propose Hyena-X andHyena-Y , two Hyena operators designed for\ngrafting. Hyena-X removes the implicit convolution entirely by setting G=I. In contrast, Hyena-Y\nintroduces two changes: (i) it removes all three featurizer convolutions ( T,H,K), and (ii) replaces\nthe implicit long convolution in Gwith a short, explicit convolution. This modified structure preserves\nlocal inductive bias while significantly reducing computational cost. An illustration is provided in\nmain paper. These operators allows us to realize speedups across a range of inputs resolutions: both\nHyena-X andHyena-Y are faster than Mamba-2 operators on all input sequence lengths, including\nlower resolution regimes.\nF FLOP calculation\nNotations are provided in Tab. F.1.\nF.1 MHA\n•Input projections (Q, K, V) :6LD2\n•Softmax attention computation :4L2D+ 2HL2\n•Output projection :2LD2\nF.2 SWA\n•Input projections (Q, K, V) :6LD2\n•Sliding window attention (Bidirectional) :4L(2w+ 1)D+ 2HL(2w+ 1)\n•Output projection :2LD2\nF.3 Hyena-SE\n•Input projections :6LD2\n•Featurizer :3LDK ×2\n•Inner filter convolution :LDK ×2\n21\n--- Page 22 ---\n•gates :LD×2\n•Output projection :2LD2\nF.4 Hyena-X\n•Input projections :6LD2\n•Featurizer :3LDK ×2\n•gates :LD×2\n•Output projection :2LD2\nF.5 Hyena-Y\n•Input projections :6LD2\n•Inner filter convolution :LDK ×2\n•gates :LD×2\n•Output projection :2LD2\nF.6 Hyena-X (MLP)\n•Dense input projections :6LD2r\n•Featurizer :3LDK ×2\n•Gates :LD×2\n•Dense output projections :2LD2r\nF.7 Mamba-2\n•Projections :8LD2E\n•Short convolution :6LDE\n•Featurization :2LDE (1 + 2 dstate) + 2LDE\n•Associative scan :2LDEd state\n•Output layer :2LD2E\nSymbol Description\nL Sequence length\nD Hidden dimension\nH Number of attention heads\nK Kernel size for convolutions\nw Window size for sliding window attention\nr MLP expansion ratio\nE Expansion factor in Mamba-2\ndstate State size in Mamba-2\nTable F.1: Notation for FLOP calculation.\n22",
  "text_length": 70392
}