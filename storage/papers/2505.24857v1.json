{
  "id": "http://arxiv.org/abs/2505.24857v1",
  "title": "Accelerated Sampling from Masked Diffusion Models via Entropy Bounded\n  Unmasking",
  "summary": "Recent masked diffusion models (MDMs) have shown competitive performance\ncompared to autoregressive models (ARMs) for language modeling. While most\nliterature has focused on performance enhancing sampling procedures, efficient\nsampling from MDMs has been scarcely explored. We make the observation that\noften a given sequence of partially masked tokens determines the values of\nmultiple unknown tokens deterministically, meaning that a single prediction of\na masked model holds additional information unused by standard sampling\nprocedures. Based on this observation, we introduce EB-Sampler, a simple\ndrop-in replacement for existing samplers, utilizing an Entropy Bounded\nunmasking procedure that dynamically unmasks multiple tokens in one function\nevaluation with predefined approximate error tolerance. We formulate the\nEB-Sampler as part of a broad family of adaptive samplers for which we provide\nan error analysis that motivates our algorithmic choices. EB-Sampler\naccelerates sampling from current state of the art MDMs by roughly 2-3x on\nstandard coding and math reasoning benchmarks without loss in performance. We\nalso validate the same procedure works well on smaller reasoning tasks\nincluding maze navigation and Sudoku, tasks ARMs often struggle with.",
  "authors": [
    "Heli Ben-Hamu",
    "Itai Gat",
    "Daniel Severo",
    "Niklas Nolte",
    "Brian Karrer"
  ],
  "published": "2025-05-30T17:52:55Z",
  "updated": "2025-05-30T17:52:55Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24857v1"
}