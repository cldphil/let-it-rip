{
  "id": "http://arxiv.org/abs/2505.24857v1",
  "title": "Accelerated Sampling from Masked Diffusion Models via Entropy Bounded\n  Unmasking",
  "summary": "Recent masked diffusion models (MDMs) have shown competitive performance\ncompared to autoregressive models (ARMs) for language modeling. While most\nliterature has focused on performance enhancing sampling procedures, efficient\nsampling from MDMs has been scarcely explored. We make the observation that\noften a given sequence of partially masked tokens determines the values of\nmultiple unknown tokens deterministically, meaning that a single prediction of\na masked model holds additional information unused by standard sampling\nprocedures. Based on this observation, we introduce EB-Sampler, a simple\ndrop-in replacement for existing samplers, utilizing an Entropy Bounded\nunmasking procedure that dynamically unmasks multiple tokens in one function\nevaluation with predefined approximate error tolerance. We formulate the\nEB-Sampler as part of a broad family of adaptive samplers for which we provide\nan error analysis that motivates our algorithmic choices. EB-Sampler\naccelerates sampling from current state of the art MDMs by roughly 2-3x on\nstandard coding and math reasoning benchmarks without loss in performance. We\nalso validate the same procedure works well on smaller reasoning tasks\nincluding maze navigation and Sudoku, tasks ARMs often struggle with.",
  "authors": [
    "Heli Ben-Hamu",
    "Itai Gat",
    "Daniel Severo",
    "Niklas Nolte",
    "Brian Karrer"
  ],
  "published": "2025-05-30T17:52:55Z",
  "updated": "2025-05-30T17:52:55Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24857v1",
  "full_text": "arXiv:2505.24857v1 [cs.LG] 30 May 2025 Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking Heli Ben-Hamu,Itai Gat,Daniel Severo,Niklas Nolte,Brian Karrer FAIR at Meta Recent masked diffusion models (MDMs) have shown competitive performance compared to autore- gressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with. Date:June 2, 2025 Correspondence: Heli Ben-Hamu at helib@meta.com 1 Introduction Motivated by the success of diffusion and flow (Esser et al., 2024; Polyak et al., 2024) models in continuous domains ( e.g., images, video), research efforts have focused on adapting these frameworks to discrete state spaces. Many of these approaches (Austin et al., 2021a; Lou et al., 2023; Sahoo et al., 2024; Campbell et al., 2024; Gat et al., 2024; Kitouni et al., 2024) utilize the masked modeling paradigm, thus we generally refer to these approaches as Masked Diffusion Models (MDMs). Recent large MDMs such as LLaDa (Nie et al., 2025b) and Dream (Ye et al., 2025) have shown competitive performance compared to similarly sized autoregressive models (ARMs) in the 7billion parameter range on traditional language tasks including code, text, and mathematical reasoning benchmarks. These results open the possibility of scaled non-autoregressive generation for language using MDMs, a possible competitor to large language models (LLMs). Abstractly, MDMs sample fixed-length sequences as discrete tokens from a vocabulary. They begin from a sequence of mask tokens and iteratively update tokens until all tokens are unmasked. Replacing a masked token with a token from the vocabulary is unmasking. In contrast to standard ARMs, the order in which tokens are unmasked becomes a design choice. Recent models often leverage samplers that surpass the performance of random order unmasking, deviating from the masked diffusion process used at training (Austin et al., 2021a). In particular, LLaDa and Dream achieve strong performance by unmasking tokens in a much more favorable order dictated by model predictions (Chang et al., 2022; Kim et al., 2025; Zheng et al., 2023). Performance is not the only criteria by which large MDMs have become popular. Another additional important aspect is efficient computation. While MDMs cannot reuse past computation via key-value caching like ARMs due to full attention, MDMs offer an exciting alternative route to efficiency via the opportunity to sample multiple tokens simultaneously. MDM efficiency is captured by the number of function evaluations (NFE) which is directly controlled via the sampler. The default for more efficient sampling of MDMs like LLaDa and Dream is to unmask a fixed number of tokens each step. Unfortunately, generation quality degrades quickly as more unmasked tokens are sampled independently and hence this opportunity has been thus far unrealized. 1 Developing an MDM sampler for language that simultaneously achieves good performance and efficiency is the focus of this work. Our approach is motivated by two empirical observations. The first is the strong performance of unmasking orders determined by the model, which indicates the associated predictions aligned with that order may have low model error, i.e. match the desired optimal distribution. The second is that multiple masked tokens are often predicted with high certainty, or, equivalently, low entropy. Intuitively, masked tokens that have both low model error and low entropy can be unmasked in parallel, which we propose to exploit with our approach. By strategically unmasking such tokens, a sampler can follow high performance unmasking orders while avoiding error due to independently unmasking tokens. We realize these intuitions and make them concrete and precise via our main contributions: •We propose an adaptive sampler for masked diffusion models called EB-Sampler (short for Entropy Bounded Sampler), that decides both which tokens to unmask and how many tokens to unmask at each step using an entropy bound that approximately limits the dependence of unmasked tokens. •EB-Sampler is a simple drop-in replacement to existing samplers and can be directly applied to sample from existing masked diffusion models without further training. •EB-Sampler accelerates sampling from the best performing masked diffusion models (LLaDa and Dream) by2-3x on standard coding and math reasoning benchmarks without loss in performance. We provide Pareto fronts between efficiency and performance and conclude EB-Sampler is superior across most settings. We also validate EB-Sampler works well on smaller reasoning tasks, including maze navigation and Sudoku. •We show EB-Sampler is a member of a broad family of adaptive multi-token samplers for masked diffusion models. Our theory explains why this family samples correctly, why it can leverage a pre-trained masked diffusion model, and provides an intuitive error decomposition into model error and joint dependence error that motivates the design of EB-Sampler. 2 Preliminaries 2.1 Notations Consider a discrete state space denoted S=Td, of sequences of length dover a finite sized vocabulary T= [K] ={1,2,..., K }. A state, x∈ S, is a sequence of elements, often called tokens, from the vocabulary T, where x= (x1, x2,..., xd)with each element xl∈ T. We denote the set of indices of a sequence in SwithI= [d] ={1,2,..., d }. For masked modeling, we extend our vocabulary to include a mask token, m. If index lor token xlis described as masked, then xl=mand if unmasked, xl̸=m. Finally, for a random variable X= (X1, X2,..., Xd)and a set A⊆ I, we define the notation for conditional probabilities p(xl|xA) =P(Xl=xl|{Xi=xi|i∈A}). 2.2 Masked diffusion models Given a finite set of samples D ⊆ Sdrawn from an unknown target data distribution X∼q, most MDMs, with few exceptions, have been proven to learn to model clean data conditionals q(xl|x¯M)(Ou et al., 2025; Zheng et al., 2024) where ¯M⊆ Iis a set, possibly empty, of unmasked token indices. The main difference from BERT-like masked language modeling (Devlin et al., 2019) is that BERT-like models are trained on a fixed masking ratio, as opposed to all possible masks. In particular, the MDMs we consider learn factorized conditional predictions of pθ(xl|x¯M), using full attention, for all lmasked tokens, that ideally match q(xl|x¯M), for every possible ¯M. Sampling with trained factorized conditionals can be done sequentially, unmasking one token at a time. This baseline sampler can be defined as follows: At each step (i) randomly pick an index to unmask, e.g.,l; (ii) sample from learned factorized conditional distribution Xl∼pθ(xl|x¯M), where ¯Mis the set of currently masked tokens. We refer to this sampling procedure as random unmasking. 2 forminm:\\nformmmm12345 z=({1},{3,5},{2,4},∅,∅)forstringinstrings:\\nxz<2xz<1xz<3pθ(xl|xz<1)EB step2345=err()=err()=err()=err()for12345Order of unmasking Sort by error proxyChoose how many to unmask Bounded entropy2345cumsum_entropy−cummax_entropy≤γFigure 1 Illustration of an unmasking step with EB-Sampler. At each step EB sampler determines which tokens to unmask by ordering according to an error proxy and then chooses how many tokens to independently unmask by bounding their joint dependence via model prediction entropies. 3 Known challenges of MDM sampling We now discuss two challenges in sampling from MDMs. The connections we draw between these challenges will support the intuition for the construction of the EB-Sampler in the next section. 3.1 Order of unmasking matters Accuracy (%)020406080 MBPPGSM8K RandomConfidenceEntropyMargin Figure 2 Performance of greedy sam- pling with various unmasking criteria from LLaDa 8B Base model.For the optimal factorized conditionals predictor, the order of sequential unmasking will not change the underlying model distribution, pθ(x). That is, by the chain rule of probability, for any two permutations (orders of unmasking) σ, σ′: pθ σ(x) =pθ σ′(x) where pθ σ=Qd l=1pθ(xσ(l)|x∪j<lσ(j)). Nevertheless, training an optimal pθis intractable due to both learning on extremely high dimensional state spaces with limited model capacity and finite data and no hard con- straints on the modeling itself such that the chain rule holds. Formally, it means that there exists local model error DKL(q(xl|x¯M), pθ(xl|x¯M))>0. A key consequence is that the total model error,i.e., the discrepancy between the data distribution, q(x), and the model distribution for a certain order of unmasking, pθ σ(x), depends on the order of unmasking. A question that arises then is, do MDMs manage to learn such there are unmasking orders with low total model error? If so, can one find local model error proxies such that the chosen unmasking order will result in low total model error? Recent works (Nie et al., 2025a,b; Kim et al., 2025; Ye et al., 2025) proposed sequential greedy sampling with unmasking orders dictated by one of the following criteria (Chang et al., 2022; Kim et al., 2025; Zheng et al., 2023) for choosing the next coordinate lto unmask: Confidence: l= argmax l′∈M[max xl′pθ(xl′|x¯M)] Entropy: l= argmin l′∈M[H(Xl′|X¯M=x¯M)] (1) Margin: l= argmax l′∈M[pθ(Xl′=y1|x¯M)−pθ(Xl′=y2|x¯M)] where (y1, y2) =arg Top 2 xl′pθ(xl′|x¯M)andH(p) =−P xp(x)logp(x)is the entropy of p. Greedy samplers show superior performance compared to random unmasking samplers, as shown in Figure 2, closing the gap 3 in performance between MDMs and ARMs (Nie et al., 2025b; Ye et al., 2025). This answers the first part of the question above, indicating the existence of orders with lower total model error. As for the second part, on how to find those unmasking orders, Figure 2 shows that the criteria in Equation (1) can serve as local model error proxies, determining an unmasking order with low(er) total model error. 3.2 Sampling efficiency 100 200 300 400 500 NFE0510152025303540Accuracy (%) LLaDa 8B Top-k, confidence Top-k, entropy Top-k, margin Figure 3 Efficiency-accuracy tradeoff of Top- k(NFE) sampling on MBPP.The best performing sampling procedures for language MDMs described above, similarly to ARMs, predict one token per function evaluation, hence the efficiency of MDMs remains a disadvantage compared to ARMs due to costly computations of full attention that does not allow KV-caching. An avenue for improving sampling efficiency of MDMs is to make use of the model predictions on all masked tokens to unmask multiple tokens per function evaluation. Common multi-token unmask- ing procedures unmask a fixed number of tokens, k, at each step by sampling independently from the predicted factorized conditionals. We refer to these approaches as Top- ksampling, and they can all be cast in terms of choosing the Top- klowest model error proxy tokens to be unmasked at each step. The parameter ktunes an efficiency-accuracy tradeoff. The larger kis, the larger the joint dependence error will be, as it wrongly assumes independence of a fixed number of tokens at every step. Figure 3 empirically shows the degradation in performance in Top- k sampling for k∈ {1,2,4,8,16}with various error proxies. 4 Entropy Bounded (EB) Sampler The previous section described how challenges in MDM sampling come from two distinct sources of error: local model error and joint dependence error. Controlling these errors motivates our Entropy Bounded (EB) Sampler, a direct replacement for Top- ksamplers. Section 3.1 found that not only do masked tokens with low model error likely exist, but that past research has already identified proxies computable via model predictions that identify these tokens in Equation (1). A step in EB-Sampler begins by sorting unmasked tokens in ascending order on this error proxy, exactly as in Top- k samplers. Then Section 3.2 showed Top- ksampling accumulates substantial joint dependence error that harms performancebysamplingtokensindependently. WenowmaketheadditionalobservationthatMDMpredictions are often highly confident about multiple masked tokens simultaneously. These tokens are predicted to have low dependence in the data distribution q, because these tokens are all predicted to have low entropy. EB-Sampler thereforedefinesathreshold γ≥0anddecidestounmaskthelargestsubset Uofsortedmaskedtokenssuchthat X l∈UH(pθ(xl|x¯M))−max l∈UH(pθ(xl|x¯M))≤γ. (2) When Uis comprised of low model error tokens, this expression approximately bounds a rigorous joint dependence error introduced later in Section 5. Figure 1 visually illustrates a step of EB-Sampler. Python code showing Top- ksampling and EB-Sampler is provided in Figure 4, where EB-Sampler is shown to be a minimal change in PyTorch. Like Top- ksampling, EB-Sampler is easily compatible with ad hoc adjustments like temperature and unsupervised classifier-free guidance that alter pθ. Consider this code with the same inputs. Different outputs are only from EB-Sampler determining the value ofkusing the entropy bound of Equation (2). When unmasked tokens have low dependence, EB-Sampler will unmask more tokens, and conversely, when there is (potentially) high dependence, EB-Sampler will unmask less tokens. The amount of tokens unmasked per step in EB-Sampler is therefore not fixed. More function evaluations are used when the sample is predicted complex and less when the sample is predicted simple. The threshold γinfluences the number of steps where γ= 0will unmask one token each step and γ=∞will unmask all tokens at once. 4 1 deftop_k_step(x, model, sample_fn, error_proxy_fn, k): 2 p = model(x) 3 err = error_proxy_fn(p) 4 err = torch.where(x == model.mask_id, err, np.inf) 5 _, ids = torch.sort(err, dim=-1) 6 7 8 9 10 k = torch.minimum(k, (x == model.mask_id).sum()) 11 ids_to_unmask = ids[:k] 12 x[ids_to_unmask] = sample_fn(p, ids_to_unmask) 13 return xdefEB_step(x, model, sample_fn, error_proxy_fn, gamma): p = model(x) err = error_proxy_fn(p) err = torch.where(x == model.mask_id, err, np.inf) _, ids = torch.sort(err, dim=-1) + entropy = torch.distributions.Categorical(probs=p).entropy()[ids] + acc_entropy = torch.cumsum(entropy) + cummax_entropy = torch.cummax(entropy, dim=0).values + k = (acc_entropy - cummax_entropy <= gamma).sum() k = torch.minimum(k, (x == model.mask_id).sum()) ids_to_unmask = ids[:k] x[ids_to_unmask] = sample_fn(p, ids_to_unmask) return x Figure 4 Python code implementation of a single sampling step for common Top- kapproaches and for EB-Sampler. 5 Adaptive unmasking samplers In this section, we formulate the EB-Sampler as a member of a more general family of adaptive multi-token samplers. The object we use to mathematically describe varying length unmasking steps is an ordered partition I, denoted by z= (z1, z2,..., z d), where zsatisfies: d[ i=1zi=I, z i∩zj=∅ (3) and either zi⊆ Iorzi=∅. Notation z<idenotes ordered sub-partitions up to index i, that is z<i= (zj|j∈ [i−1]). For a random variable X= (X1, X2,..., Xd)overSand ordered sub-partitions s, s′, we extend the notation from Section 2.1 for conditional probabilities: p(xs|xs′) =P\u0010\b Xl=xl,∀l∈ Is \b Xj=xj,∀j∈ Is′ \u0011 (4) where Is=S|s| i=1siandIs′=S|s′| i=1s′ i. Depending on context, we will also be using the notation in Equation (4) with sbeing a set of indices, e.g.,s=zior a single index, e.g.,s=l. We consider a broad family of sampling procedures defined by ϕthat leverage approximate clean data conditionals provided by pθ, and produce a joint distribution pϕ(x, z)over state xand partition z: pϕ(x, z) =dY i=1pϕ(zi, xzi|xz<i, z<i) =dY i=1 Y l∈zipθ(xl|xz<i)! ϕ(zi|xz<i, z<i). (5) The distributions ϕenforce zis a valid partition, and sampling zimeans token indices ziare unmasked at stepi. Without loss of generality, ϕalways unmasks at least one token if possible, i.e. only samples zi=∅ when z<i=I. Similarly define qϕ(x, z) =dY i=1q(zi, xzi|xz<i, z<i) =dY i=1q(xzi|xz<i)ϕ(zi|xz<i, z<i). (6) Importantly, qϕ(x, z) =q(x)Qd i=1ϕ(zi|xz<i, z<i), because the product of the clean data conditionals does not depend on the order of unmasking, z, so thatP zqϕ(x, z) =q(x)for any ϕ. Expressiveness of ϕ.This family encompasses existing common samplers that always unmask the same number of tokens on each step, such as deterministic samplers of Top- ksmallest margin, Top- ksmallest entropy, and Top- kconfidence, or simple random unmasking, but also contains additional options, including dynamically determining the number of tokens to unmask at each step. 5 Error decomposition. We quantify the error from sampling pϕ(x) =P zpϕ(x, z)instead of q(x)via KL divergence DKL(q(x), pϕ(x))≤DKL(qϕ(x, z), pϕ(x, z)) =dX i=1Eqϕ[lnq(xzi|xz<i)−X l∈zilnpθ(xl|xz<i)]. Appendix A.1 discusses when this is an equality, and proves this can be rewritten into two terms dX i=1Eqϕ[X l∈ziDKL(q(xl|xz<i), pθ(xl|xz<i)) | {z } model error+DKL(q(xzi|xz<i),Y l∈ziq(xl|xz<i)) | {z } joint dependence error]. (7) Model error comes from sampling incorrect conditionals pθthat do not match the data distribution. Joint dependence error comes from sampling tokens independently that are not actually independent in q. This second KL divergence is precisely joint mutual information, upper-bounded by DKL(q(xzi|xz<i),Y l∈ziq(xl|xz<i))≤X l∈ziH(q(xl|xz<i))−max l∈ziH(q(xl|xz<i)). (8) Choosing ϕgiven a pre-trained model. EB-Sampler is directly motivated by this error decomposition. The pθthat achieves zero model error is the same for any ϕ, justifying using any pre-trained model learned to match clean data conditionals. Assume we can identify low model error tokens and we design ϕto only select zifrom such tokens, where for all l∈zi,pθ(xl|xz<i)≈q(xl|xz<i). Then model error is negligible and joint dependence error is approximately upper-bounded by X l∈ziH(pθ(xl|xz<i))−max l∈ziH(pθ(xl|xz<i)), (9) our criteria in Equation (2). So after adaptively identifying low model error tokens, ϕcan control overall error by selecting subsets of such tokens to unmask with bounded joint dependence. EB-Sampler applies this approach with the model error proxies from Section 3.1. 6 Experiments We evaluate the performance of the EB-Sampler on standard code and math reasoning generation tasks and on logic puzzles solving. The empirical findings in this section support our theoretical derivations and demonstrate the proposed sampler’s capabilities. Baselines. We compare the EB-Sampler to Top- ksamplers with the three error proxy functionals described in Equation (1): (i) confidence; (ii) entropy; and (iii) margin. Experimental setting. On all tasks and models we follow the same general experimental setting. We test the EB-Sampler with a range of thresholds, γ, depending on the task. For the Top- ksamplers we test a range of k values. Each point in the plots ( e.g., Figure 5) corresponds to the resulting model performance when sampling with parameter γorkfor EB or Top- ksampler respectively. After finding temperature was unhelpful for LLaDa, we used zero temperature sampling for all experiments. Full details on threshold and kvalues can be found in Appendix C. 6.1 Code and math reasoning We evaluate the performance of the EB-Sampler variants on text generation tasks in which (i) success can be measured quantitatively; and (ii) require an answer that is longer than a single token. These two properties facilitate quantitative assessment of the efficiency-accuracy tradeoff a sampler exhibits. Models.We report results on two recent open source state-of-the-art language MDMs: LLaDa 8B Base (Nie et al., 2025b) and Dream 7B Base (Ye et al., 2025). 6 0 20 40 60 80 100 120 NFE5101520253035Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence 20 40 60 80 100 120 NFE01020304050Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence(a)HumanEval 50 100 150 200 NFE0510152025303540Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence 25 50 75 100 125 150 175 200 NFE0102030405060Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence (b)MBPP 20 40 60 80 NFE203040506070Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence 20 40 60 80 100 NFE203040506070Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence (c)GSM8K 25 50 75 100 125 150 175 200 NFE051015202530Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence 50 100 150 200 250 NFE0510152025303540Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence (d)MATH Figure 5 pass@1 accuracy vs. NFE with generate_until logic on code and math reasoning tasks. Benchmarks. We use 4 widely used benchmarks. HumanEval (0 shot) (Chen et al., 2021) and MBPP (4 shot) (Austin et al., 2021b) code generation benchmarks; and GSM8K (8 shot) (Cobbe et al., 2021) and Math (4 shot) (Hendrycks et al., 2021) math reasoning benchmarks. We note that we use different variants of GSM8K and Math in our evaluation compared to reported results for Dream 7B, hence the minor gaps in performance for standard Top- 1sampling. Setup.For each task, we follow common practice and set a maximal number of generated tokens, max_gen_len. Then, the prompt is padded to the right with mask tokens until the maximal sequence length of the model. Duringsampling,onlymaskedtokensintherangeof [len(prompt),len(prompt) +max_gen_len ]areallowedto beunmasked. Generationstopsoncealltokensinthedesignatedrangehavebeenunmasked. Weshowresultsfor the two best performing error proxy functions entropy and confidence. Results with margin are in Appendix D. 6.1.1 Measuring efficiency gains of MDM samplers In this part, we describe how we measure the gains obtained by our proposed sampler. We explain why current generation practices for MDMs require rethinking and propose to add a generate_until logic to save function evaluations. We then further observe that unlike ARMs that generate in a left-to-right order, a generate_until logic may still be suboptimal for MDM efficient generation. generate_until logic.Current generation practices for MDMs fix an apriori amount of tokens to be generated, i.e.,max_gen_len, and generate until all tokens are unmasked. This is rather wasteful as typically producing a response to a given prompt will require less tokens than the full length, that is answer_len <max_gen_len. We therefore propose to incorporate a generate_until logic in MDM samplers similar to ARM generation practices. For example, for the MBPP benchmark, the few shot samples given as context to the model include a concluding phrase “[DONE]”. In ARM evaluation procedures, this phrase is used as a stopping criterion. For MDMs, however, due to the non left-to-right order of unmasking, we extend the generate_until logic to include an additional condition that all tokens in indices preceding the concluding phrase are unmasked. All experiments were run with the generate_until logic applied as a post-process in order to measure gains both against full max_gen_len and effective generation length to stopping criterion. The NFE reported in Figure 5 measures average number of function evaluations until a task specific generate_until logic is satisfied. EB-Sampler consistently improves upon accuracy-NFE Pareto frontier across all datasets and error proxies, gaining speed-ups of 2-4x compared to Top- 1sampler at the same accuracy. In Figures 9 and 10 we show the same plots against full max_gen_len NFE. 7 Table 1NFE and Speed-Ups for Dream 7B on the MBPP for various evaluation schemes at roughly same best pass@1. For all configurations in the table the mean answer length is ∼50tokens. Full max_gen_len = 512 generate_until logicgenerate_until logic + semi-AR (block_len=64) pass@1 NFE Speed-Up NFE Speed-Up pass@1 NFE Speed-Up Top-1 58.8% 512 x 1 101.71 x 1 58.8% 64.59 x 1 EB, entropy, γ= 0.001 59.2% 174.57 x 2.93 49.39 x 2.05 59% 38.90 x 1.66 EB, entropy, γ= 0.1 58% 85.33 x 6.00 25.49 x 3.99 58.6% 21.19 x 3.05 Bias in function evaluation count. Surprisingly, under the generate_until post-process we still observed high NFE counts at γ= 0compared to expected answer lengths. This is most pronounced on the MBPP benchmark, where the model finds it “easy” to repeat the instructions given to the model after the concluding phrase although it did not finish unmasking all the masks before that. To get a better estimate of the actual speed-up gains achieved by the EB-Sampler we take MBPP as a test case and adopt the semi-AR block generation scheme from (Arriola et al., 2025; Nie et al., 2025b) to restrict the model from generating tokens that are far from the context. In Table 1 we compare the average NFE at roughly the same performance for the various strategies of controlling the generation length. We note that for this dataset and the sampling strategies evaluated in the table the mean answer length is around 50tokens, while the semi-AR block generation requires 64.59NFE, thus not fully resolving the bias. On the contrary, EB-Sampler requires 21.19 function evaluations to get the same performance with same average answer length, generating tokens at a rate of 2.4tokens per step. We therefore refrain from claiming a 6x speed up as it compares to a loose generation procedure and believe a better estimate of the EB-Sampler’s gains is around 2-3x. Table 1 provides three key insights on MDM efficiency evaluation. First, measuring the efficiency gains of samplers for MDMs is non-trivial and depends on the generation configuration, second, MDMs invest computation on generating tokens that are not used and this opens up an important practical question for the future use of MDMs, and lastly, EB-Sampler shows strong performance in efficiency gains across all settings. 6.2 Logic puzzles Discrete diffusion models have been shown to excel on logic puzzles such as maze navigation, Sudoku and more (Nolte et al., 2024; Ye et al., 2024). We investigate whether these strong performances can be retained while sampling more efficiently than one token at a time. Specifically, we train small scale discrete diffusion models on Sudoku and Maze navigation problems and evaluate their performance on held-out data when varying the sampling strategies. 6.2.1 Maze navigation 0 5 10 15 20 25 NFE20406080100Accuracy (%) Maze navigation EB, entropy Top-k, entropy EB, confidence Top-k, confidence EB, margin Top-k, margin Figure 6 10x10 mazes - accuracy vs average NFE.We use the maze generation methods for “DFS mazes” in (Nolte et al., 2024), see their Section 4.1. We generate 48Kmazes for training and 2Kfor validation. All mazes are defined on a grid of size 10x10. They are serialized into tokens by enumerating all the connections between cells in the grid, i.e. the edges in the graph defined by the maze. To aid learning the invariance with respect to edge reordering, the edges are shuffled before tokenization. A 6 million parameter discrete DiT model, using code adapted from (Lou et al., 2023), is trained to optimize the masked diffusion objective, without explicit time dependence (Kitouni et al., 2024; Ou et al., 2025). The metric used for performance comparison is the accuracy, defined as the fraction of validation mazes fully solved. Figure 6 shows the accuracy as a function of average NFE over the validation set for the different ordering metrics. All metrics perform extremely similarly, thus partially obscured in the plot. All strategies work best in the single token unmasking regime. EB-Sampler preserves most of the accuracy before experiencing a sharp drop-off at less than 5 NFEs. In contrast, the Top- kbaselines exhibit a steeper decline in performance, already at around 10 NFEs. 8 6.2.2 Sudoku 0 20 40 60 80 NFE020406080100Accuracy (%) Sudoku solving EB, entropy Top-k, entropy EB, confidence Top-k, confidence EB, margin Top-k, margin Figure 7 Sudoku - accuracy vs aver- age NFE.To further assess sampling strategies in structured logic problems, we evaluate performance on the task of Sudoku completion. Unlike maze navigation, which emphasizes path finding, Sudoku requires reasoning over dense, globally constrained grids. This setting provides a com- plementary benchmark to test sampling strategies under those global constraints. We adopt the standard 9 ×9 Sudoku setting and adapt the code from (Alp, 2024) to generate 48Ktraining puzzles and 2Kheld-out puzzles with, all with unique solutions. Each puzzle is serialized into a sequence of 89 tokens corresponding to the cell values and end-of-line tokens, with zeros indicating blank cells. The discrete DiT model architecture used for maze navigation is reused here. We again show accuracy as a function of average NFE. The results are depicted in Figure 7. The trend is similar to the trend in the maze navigation setup. EB-Sampler retains most of its performance for a longer time than the Top- ksamplers, and then drops off sharply. Notably, almost full performance is retained even when averaging around 10-15 NFEs. 7 Related Work Performant sampling for discrete diffusion. Procedures that improve sampling from MDMs are often focused on improving performance for a given pre-trained model. Recent approaches consider planning (Kim et al., 2025), deciding which masked tokens should be unmasked next, as well as remasking (Wang et al., 2025), deciding which unmasked tokens should be masked again, related to predictor-corrector iterations (Gat et al., 2024; Lezama et al., 2022) and forward-backwards sampling (Campbell et al., 2024), or consider both planning and remasking in (Zheng et al., 2023; Peng et al., 2025; Liu et al., 2024). Like EB-Sampler, this research often considers KL (equivalently ELBO) bounds. Unlike past research though, we focus on multi-token adaptive planning with a variable-sized set of tokens to unmask, crucial for efficiency. While sampling for LLaDa (Nie et al., 2025b) was described as remasking, it can be viewed as determining what to unmask first and then their token values within semi-autoregressive blocks, a member of our ϕfamily. Because we aim for efficient and justified planning for scaled MDMs, we do not consider revisiting unmasked tokens here, enabling a simpler, time-independent analysis with a KL bound that is minimized via adaptive sampling. Future research might devise an efficient multi-token sampler that both unmasks and revisits past tokens upon EB-Sampler. Efficient sampling for discrete diffusion. Efficiency has received relatively less attention than performance. (Ren et al., 2025) proposed higher-order numerical solvers for discrete diffusion, not specific to MDMs. (Park et al., 2024) introduced a method to avoid joint dependence error from parallel sampling, by performing a global optimization for a non-adaptive sampling schedule (i.e. the number of tokens per step). EB-Sampler determines this adaptively per sampling step. (Besnier et al., 2025) introduced an unmasking sampler for MaskGIT (Chang et al., 2022) that controls joint dependence error via quasi-random, low-discrepancy unmasking of an image, outperforming a confidence-based sampler in that domain. Finally, recent research (Zhu et al., 2025) has proposed a distillation procedure for MDMs, and trained one-step image generators from multi-step MaskGIT (Chang et al., 2022) and Messionic (Bai et al., 2024). Speculative decoding. A prominent method to accelerate LLMs is speculative decoding, with 2-2.5x speedup in (Chen et al., 2023). Instead of sampling from a large targetlanguage model, speculative decoding generates a candidate sequence from a smaller draftlanguage model and accepts some portion of that candidate utilizing sequence probabilities. Modified rejection sampling guarantees the sequence is extended and this extension is sampled from the target model. Because evaluating models with causal attention can be done cheaply compared to sampling, efficiency gains occur when the draft model quickly samples reasonable sequences. This procedure has also been adapted to any-order masked models (Uria et al., 2014; Hoogeboom et al., 2022) with causal attention (Pannatier et al., 2024; Guo and Ermon, 2025). For MDMs with full attention, we cannot directly apply speculative decoding because it is generally expensive to compute the sequence probability in a 9 full attention target model. (De Bortoli et al., 2025) has applied speculative sampling to continuous Gaussian diffusion, but relies upon querying the target model in parallel. Speculative decoding has been combined with discrete diffusion in (Christopher et al., 2024), where the draft is an MDM and the target is a language model. Our EB-Sampler approach is complementary, and could be simply applied to speed up the draft model. 8 Conclusions and Future Work In this paper we propose EB-Sampler, a theoretically grounded adaptive sampler for masked discrete diffusion and flow models. This algorithm controls both which and how many tokens to sample using an interpretable entropy bound and serves as a drop-in replacement for existing samplers. We evaluate our approach on math, code and reasoning benchmarks with contemporary diffusion models of different scales, against the standard samplers used by the authors of those models. We find that EB-Sampler significantly outperforms existing samplers on the compute-vs-performance Pareto frontier and even yields 2-3x speed-ups without any loss of performance. Future research might consider learning a parameterized adaptive sampler from data, perhaps optimizing our KL bound with respect to ϕ, or expand upon EB-Sampler to incorporate revisiting past unmasked tokens. Such research could further advance efficient and performant sampling for masked diffusion models. 10 References Ali Alp. Sudoku. https://github.com/alicommit-malp/sudoku/tree/main, 2024. Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin T Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id= tyEyYT267x. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:17981–17993, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In The Thirteenth International Conference on Learning Representations, 2024. Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord. Halton scheduler for masked generative image transformer, 2025. https://arxiv.org/abs/2503.17076. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11315–11325, 2022. CharlieChen, SebastianBorgeaud, GeoffreyIrving, Jean-BaptisteLespiau, LaurentSifre, andJohnJumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Jacob K Christopher, Brian R Bartoldson, Tal Ben-Nun, Michael Cardei, Bhavya Kailkhura, and Ferdinando Fioretto. Speculative diffusion decoding: Accelerating language generation through diffusion. arXiv preprint arXiv:2408.05636, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Valentin De Bortoli, Alexandre Galashov, Arthur Gretton, and Arnaud Doucet. Accelerated diffusion models via speculative sampling. arXiv preprint arXiv:2501.05370, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171–4186, 2019. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. https://arxiv.org/abs/2403.03206. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. arXiv preprint arXiv:2407.15595, 2024. Gabe Guo and Stefano Ermon. Reviving any-subset autoregressive models with principled parallel sampling and speculative decoding. arXiv preprint arXiv:2504.20456, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 11 Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Sal- imans. Autoregressive diffusion models. In International Conference on Learning Representations, 2022. https://openreview.net/forum?id=Lm8T39vLDTE. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. CoRR, abs/1906.02691, 2019. http://arxiv.org/abs/1906.02691. Ouail Kitouni, Niklas Nolte, James Hensman, and Bhaskar Mitra. Disk: A diffusion model for structured knowledge, 2024. https://arxiv.org/abs/2312.05253. Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Discrete predictor-corrector diffusion models for image synthesis. In The Eleventh International Conference on Learning Representations, 2022. Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, and Rafael Gómez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising. arXiv preprint arXiv:2410.06264, 2024. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. In The Thirteenth International Conference on Learning Representations, 2025a. https://openreview.net/forum?id=WNvvwK0tut. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025b. https://arxiv.org/abs/2502.09992. Niklas Nolte, Ouail Kitouni, Adina Williams, Mike Rabbat, and Mark Ibrahim. Transformers can navigate mazes with multi-step prediction, 2024. https://arxiv.org/abs/2412.05117. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data, 2025. https://arxiv.org/abs/2406.03736. Arnaud Pannatier, Evann Courdier, and François Fleuret. σ-gpts: A new approach to autoregressive models. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 143–159. Springer, 2024. Yong-Hyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, and Yuki Mitsufuji.: Optimizing sampling schedule of discrete diffusion models. CoRR, 2024. Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Alexander Tong, and Pranam Chatterjee. Path planning for masked diffusion model sampling. arXiv preprint arXiv:2502.03540, 2025. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: A cast of media foundation models, 2024. https://arxiv.org/abs/2410.13720. Yinuo Ren, Haoxuan Chen, Yuchen Zhu, Wei Guo, Yongxin Chen, Grant M Rotskoff, Molei Tao, and Lexing Ying. Fast solvers for discrete diffusion models: Theory and applications of high-order algorithms. arXiv preprint arXiv:2502.00234, 2025. Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander M Rush, Yair Schiff, Justin T Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. https://openreview.net/forum? id=L4uaAR4ArM. 12 Benigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 467–475, Bejing, China, 22–24 Jun 2014. PMLR. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. https://hkunlp.github.io/blog/2025/dream. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023. Yuanzhi Zhu, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. Di [M]o: Distilling masked diffusion models into one-step generator. arXiv preprint arXiv:2503.15457, 2025. 13 Appendix A Theorems and proofs A.1 KL divergence error decomposition We revisit the KL divergence introduced in Section 5. Recall we quantify the error from sampling pϕ(x) =P zpϕ(x, z)instead of q(x)via KL divergence DKL(q(x), pϕ(x))≤DKL(qϕ(x, z), pϕ(x, z)) =dX i=1Eqϕ\" lnq(xzi|xz<i)−X l∈zilnpθ(xl|xz<i)#,(10) where the inequality can be derived from the Evidence Lower BOund (ELBO) (Kingma and Welling, 2019). In the equality we plug in the definitions of the joint distributions pϕ(x, z), qϕ(x, z)from Equation (5) and Equation (6) respectively, where the ϕterm cancels out, and the sum and expectation can be interchanged since the sum does not We add and subtract ln\u0010Qd i=1Q l∈ziq(xl|xz<i)\u0011 from Equation (10): dX i=1Eqϕ\" lnq(xzi|xz<i)−X l∈zilnpθ(xl|xz<i)# = =dX i=1Eqϕ\"X l∈zilnq(xl|xz<i) pθ(xl|xz<i)+ lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)#,(11) and will now separately simplify the two terms. Joint dependence error. We begin with the right term of the expectation in last equality of Equation (11), which will turn out to be the joint dependence error term in Equation (7). Eqϕ\u0014 lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)\u0015 = =X x,zqϕ(x, z)\u0012 lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)\u0013 = (∗)=X z≤i,xz≤iqϕ(z≤i, xz≤i)\u0012 lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)\u0013 = =X z≤i,xz<iqϕ(z≤i, xz<i)X xziqϕ(xzi|xz<i, z≤i)\u0012 lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)\u0013 = (∗∗)=Eqϕ(z≤i,xz<i)\"X xziq(xzi|xz<i) lnq(xzi|xz<i)Q l∈ziq(xl|xz<i)# = =Eqϕ(z≤i,xz<i)\" DKL q(xzi|xz<i),Y l∈ziq(xl|xz<i))!#. (12) where in (∗)we marginalize over xz>iandz>isince the function the expectation is taken over does not depend on them. In (∗∗)we use the fact that for the sampling procedure defined by ϕ,qϕ(xzi|xz<i,z≤i) =q(xzi|xz<i). 14 At last, we arrive at an expectation of the KL-divergence between the joint and the factorized product distributions of xziconditioned on all the unmasked tokens before, measuring the joint dependence withoun the subset of indices zi. Model error. As for the left term in the last equality of Equation (11), it will turn out to be the model error term in Equation (7). Recalling the left term: Eqϕ\"X l∈zilnq(xl|xz<i) pθ(xl|xz<i)# = =X x,zqϕ(x, z) X l∈zilnq(xl|xz<i) pθ(xl|xz<i)! = (∗)=X z≤i,xz≤iqϕ(z≤i, xz≤i) X l∈zilnq(xl|xz<i) pθ(xl|xz<i)! = =X z≤i,xz<iqϕ(z≤i, xz<i)X xziqϕ(xzi|xz<i, z≤i) X l∈zilnq(xl|xz<i) pθ(xl|xz<i)! = (∗∗)=Eqϕ(z≤i,xz<i)\"X l∈ziX xziq(xzi|xz<i) lnq(xl|xz<i) pθ(xl|xz<i)# = (∗∗∗)=Eqϕ(z≤i,xz<i)\"X l∈ziX xlq(xl|xz<i) lnq(xl|xz<i) pθ(xl|xz<i)# = =Eqϕ(z≤i,xz<i)\"X l∈ziDKL(q(xl|xz<i), pθ(xl|xz<i))#. (13) where (∗)and(∗∗)are the same steps as in the joint dependence error derivation above. In (∗ ∗ ∗), we again marginalize over variable that do not appear in the expectation. At last, we arrive at a sum of of the KL-divergences between the factorized conditionals, which is exactly what pθis trained to learn, and this we call this term model error. KL divergence equality: The two KL divergences, DKL(q(x), pϕ(x))andDKL(qϕ(x, z), pϕ(x, z)), are equal when qϕ(z|x) =pϕ(z|x). This only occurs under special ϕ. For any ϕ, qϕ(z|x) =qϕ(x, z) q(x)=q(x)Qd i=1ϕ(zi|xz<i, z<i) q(x)=dY i=1ϕ(zi|xz<i, z<i). (14) Then pϕ(z|x) =pϕ(x, z) pϕ(x) =Qd i=1\u0000Q l∈zipθ(xl|xz<i)\u0001 ϕ(zi|xz<i, z<i) pϕ(x) =Qd i=1\u0000Q l∈zipθ(xl|xz<i)\u0001 pϕ(x)qϕ(z|x). (15) Thus pϕ(z|x)is almost certainly not equal to qϕ(z|x)even if we unmask one token sequentially, because pθ is learned. The product of the learned conditionals almost certainly results in a different joint distribution depending on the order, unlike for the data distribution where the product of the true conditionals is q(x)for every order. 15 However, when every ϕis deterministic we do have equality. This special case is relevant because many schemes introduced in the main text are deterministic, or nearly so. Then the partition zis entirely determined byxand can be written zϕ(x), and qϕ(z|x) =pϕ(z|x)are a point mass on zϕ(x). For a deterministic ϕ optimizing the KL divergence hence directly optimizes the likelihood Eq[lnpϕ(x)], and not a lower-bound on that likelihood, because Eq[lnpϕ(x)] =Eqϕ[lnpϕ(x, zϕ(x))]when zis deterministically generated. B Algorithm Algorithm 1 EB-Sampler with generate_until logic Require: Factorized conditionals predictions pθ,l(·|x), threshold γ≥0, prompt y0, sequence length d, error proxy functional E, entropy functional H, stopping criteria C:S → { True,False}, mask token m n=d−len(y0) x←[y0,m∗n] ▷Set initial condition Im={l|xl=m} whileIm̸=∅and not C(x)do ▷Stop if all tokens unmasked ˆpl=pθ,l(·|x), forl∈ I m ˆe=E(ˆp) ˆh=H(ˆp) Isort=argsort (ˆeIm) ▷Sort masked tokens by error U← {} ▷Initialize subpartition forainIsortdo ▷Iterate over sorted masked tokens U←U∪a ifsum(ˆhU)−max(ˆhU)≤γthen ▷Compute entropy bound equation 2 xa=sample (ˆpa) ▷Sample unmasked value from posterior if below threshold else break ▷Halt unmasking for loop if entropy bound is exceeded Im={l|xl=m} return x C Experimental details C.1 Code and math reasoning C.1.1 Datasets For code generation tasks we evaluate EB-Sampler on 0-shot HumanEval Chen et al. (2021), 4-shot MBPP Austin et al. (2021b), and for math reasoning we test 8-shot GSM8K Cobbe et al. (2021) without Chain Of Thought (COT) variant, and 4-shot Math Hendrycks et al. (2021) corresponding to the hendrycks_math variant. C.1.2 Setup We evaluate the efficiency gains of EB-Sampler on two recent state of the art MDMs: LLaDa 8B Nie et al. (2025b) and Dream 7B Ye et al. (2025). For LLaDa 8B the maximal sequence length is 4096and for Dream 7B,2048. That is, the input to the models in the beginning of generation, is a padded sequence, starting with the prompt given in each task and then padded with the mask token, m, to the maximal sequence length, denoted max_seq_len. For each dataset a predetermined generation length is set, denoted max_gen_len. We enforce unmasking tokens that are in the range [len(prompt),len(prompt) +max_gen_len ]. In the rare case when len(prompt) +max_gen_len >max_seq_len the prompt is truncated from the left. 16 promptmask paddinganswermax_seq_lenlen(prompt)max_gen_lenFigure 8 Input sequence visualization. Table 2Evaluation parameters for code and math reasoning. Dataset size #-shots max_gen_lengenerate_until phrase LLaDa 8B Dream 7B HumanEval 164 0 512 [”<|endoftext|>”,”\\n\\n\\n”] [”<|endoftext|>”, ”“‘\\n”] MBPP 500 4 512 ”[DONE]” GSM8K 1320 8 256 ”The answer is %d.” MATH 5000 4 512 ”I hope it is correct.” All benchmarks were run in the same computational setting, on 8×H100. We report runtimes for confidence and entropy error proxies in Table 3. Runtimes with margin error proxy are longer due to the need to sort over the vocabulary size to compute the top-2 tokens. Runtimes for LLaDa 8B are longer than Dream 7B due to having twice the maximal sequence length of Dream. Table 3Average runtimes on 8×H100 of code and math benchmarks evaluation for 1 token per step sampling ( Top 1) with entropy and confidence error proxies. Relative standard deviation of measurements is 1%. Runtimes (hrs.) LLaDa 8B Dream 7B HumanEval 0.58 0.26 MBPP 1.75 0.79 GSM8K 2.29 1.03 MATH 17.30 7.84 C.1.3 Post-process Accuracy evaluation. Model outputs for all datasets evaluated with both LLaDa 8B and Dream 7B had been directly fed into the standard evaluation scripts, except for HumanEval with Dream 7B. Evaluating the raw output of Dream 7B on the HumanEval benchmark results in around 8%drop in performance compared to reported results by the authors. Investigating the cause for the drop in performance led to the observation that Dream 7B sometimes produces answers with a template that places the generated code inside code blocks which get ignored when compiled and are therefore considered as failure at the task. We thus post-process Dream’s raw outputs on HumanEval to extract the function implementation, closing the gap to reported results to <2%. Importantly, we emphasize that all comparisons in our paper are between sampling procedures from the same model and same evaluation. Efficiency measurement. As noted in Section 6.1.1, standard sampling procedures from MDM generate a sequence with predetermined length, max_gen_len. In most cases, the answer to the given prompt will be shorter, denoted answer_len, and there are (max_gen_len −answer_len )generated tokens that are being truncated, hence unused, during evaluation. To isolate the efficiency gain EB-Sampler provides in generating the answer tokens from the gain in generating the rest of the tokens that are later not used, we incorporate thegenerate_until logic as a post-process. We note that this logic can also be integrated in the sampling procedure itself, saving up computation, without changing the performance of the model, as the logic ensures termination of generation only once the stopping criterion has been met. The phrases used as markers for the generate_until post-process logic for each dataset and each model are listed in Table 2. 17 D Additional experiments - code and math reasoning 100 200 300 400 500 NFE5101520253035Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100 200 300 400 500 NFE01020304050Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (a)HumanEval 100 200 300 400 500 NFE0510152025303540Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100 200 300 400 500 NFE0102030405060Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (b)MBPP Figure 9 pass@1 accuracy vs. full max_gen_len NFE on code reasoning tasks. 50 100 150 200 250 NFE203040506070Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 50 100 150 200 250 NFE203040506070Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (a)GSM8K 100 200 300 400 500 NFE051015202530Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100 200 300 400 500 NFE0510152025303540Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (b)MATH Figure 10 pass@1 accuracy vs. full max_gen_len NFE on math reasoning tasks. D.1 Results with margin error proxy In Figure 11 we show the results with the margin error proxy along with the results with confidence and entropy error proxies presented in the main body of the paper in Figure 5. We observed that confidence error proxy was typically the best for the LLaDa 8B model and entropy error proxy worked best in most cases for Dream 7B. The margin error proxy mostly yielded inferior accuracy in full NFE, thus to maintain readability of the plots we did not include it in the main body of the paper. D.2 Measuring efficiency of MDMs In Section 6.1.1 we outline the different ways to measure sampling efficiency and discuss the problems with some of the approaches. We explain why measuring efficiency against full sequence length generation results in apparent high gains (like in Figures 9 and 10), and then propose two ways to better approximate the gains provided by different samplers: •Unmask with generate_until logic •Unmask semi-auto-regressively with generate_until logic Effective tokens/step with generate_until logic.In Figure 12 we show accuracy against the effective generation speed of the model quantified via: Effective Tokens/Step =mean_answer_len mean_NFE_to_condition, (16) where mean_answer_len is the average number of tokens from left to right until the generate_until answer markers, and mean_NFE_to_condition is the number of function evaluations required by the model to generate 18 0 50 100 150 200 NFE5101520253035Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 25 50 75 100 125 150 175 NFE01020304050Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin(a)HumanEval 50 100 150 200 250 300 350 NFE0510152025303540Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 0 50 100 150 200 250 300 NFE0102030405060Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (b)MBPP 20 40 60 80 100 NFE203040506070Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 20 40 60 80 100 120 140 NFE203040506070Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (c)GSM8K 50 100 150 200 250 NFE051015202530Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 50 100 150 200 250 300 NFE0510152025303540Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (d)MATH Figure 11 pass@1 accuracy vs. NFE with generate_until logic on code and math reasoning tasks. the answer until both generate_until answer markers apear in answer and all tokens before the marker are unmasked. Figure 12 shows that in most cases the effective speed at γ= 0orTop 1, is actually less than 1, meaning that the model unmasks tokens that are not used in the final answer, or equivalently unmasks tokens that come after the stopping phrase in the sequence. This observation led us to explore an approach to mitigates this inefficiency via semi-autoregressive generation (Arriola et al., 2025; Nie et al., 2025b). Semi-autoregressive generation. Figure 12 supports our claim that mostly in the MBPP benchmark many tokens are generated after the stopping phrase although not all tokens have been unmasked before. In the main body of the paper, in Table 1, we showed an ablation with semi-autoregressive generation for that benchmark with the Dream 7B model. In Table 4 we also add the same ablation with the LLaDa 8B model. We report the same ablation for the GSM8K benchmark in Tables 5 and 6, which show that the gap in efficiency between with and without semi-autoregressive generation is small to non existing aligning with Figure 12 that shows effective token/step of around 1on this benchmark. That is, on GSM8K it is less likely that the model generates tokens that come after the stopping phrase. Table 4NFE and Speed-Ups for LLaDa 8B on the MBPP for various evaluation schemes at roughly same best pass@1. For all configurations in the table the mean answer length is ∼60tokens. Full max_gen_len = 512 generate_until logicgenerate_until logic + semi-AR (block_len=64) pass@1 NFE Speed-Up NFE Speed-Up pass@1 NFE Speed-Up Top-1 39.6% 512 x 1 224.93 x 1 39.4% 73.31 x 1 EB, confidence, γ= 0.001 39.8% 347.28 x 1.47 155.31 x 1.44 39.4% 60.83 x 1.21 EB, confidence, γ= 0.1 39.2% 138.41 x 3.67 64.01 x 3.51 38.8% 33.20 x 2.21 19 100 Effective Tokens/Step5101520253035Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100101 Effective Tokens/Step01020304050Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin(a)HumanEval 100101 Effective Tokens/Step0510152025303540Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100101 Effective Tokens/Step0102030405060Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (b)MBPP 100101 Effective Tokens/Step203040506070Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100101 Effective Tokens/Step203040506070Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (c)GSM8K 100101 Effective Tokens/Step051015202530Accuracy (%) LLaDa 8B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin 100101 Effective Tokens/Step0510152025303540Accuracy (%) Dream 7B EB, entropy Top-k, entropy EB, confidence Top-k, confidence Top-k, margin Top-k, margin (d)MATH Figure 12 pass@1 accuracy vs. tokens/step on code and math reasoning tasks. Table 5NFE and Speed-Ups for Dream 7B on the GSM8K for various evaluation schemes at roughly same best pass@1. For all configurations in the table the mean answer length is ∼93tokens. Full max_gen_len generate_until logicgenerate_until logic + semi-AR (block_len=64) pass@1 NFE Speed-Up NFE Speed-Up pass@1 NFE Speed-Up Top-1 74.30% 256 x 1 97.44 x 1 74.90% 95.23 x 1 EB, entropy, γ= 0.0174.37% 156.29 x 1.64 49.60 x 1.96 75.36% 48.29 x 1.97 EB, entropy, γ= 0.174.83% 129.27 x 1.98 35.94 x 2.71 75.36% 35.60 x 2.66 Table 6NFE and Speed-Ups for LLaDa 8B on the GSM8K for various evaluation schemes at roughly same best pass@1. For all configurations in the table the mean answer length is ∼93tokens. Full max_gen_len generate_until logicgenerate_until logic + semi-AR (block_len=64) pass@1 NFE Speed-Up NFE Speed-Up pass@1 NFE Speed-Up Top-1 71.79% 256 x 1 95.19 x 1 71.95% 93.62 x 1 EB, confidence, γ= 0.0171.64% 185.78 x 1.36 66.57 x 1.43 72.33% 65.42 x 1.43 EB, confidence, γ= 0.172.17% 147.48 x 1.74 45.83 x 2.08 72.71% 45.30 x 2.07 20",
  "text_length": 62582
}