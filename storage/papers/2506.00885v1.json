{
  "id": "http://arxiv.org/abs/2506.00885v1",
  "title": "CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully\n  Non-Autoregressive Flow Matching",
  "summary": "Generating natural-sounding, multi-speaker dialogue is crucial for\napplications such as podcast creation, virtual agents, and multimedia content\ngeneration. However, existing systems struggle to maintain speaker consistency,\nmodel overlapping speech, and synthesize coherent conversations efficiently. In\nthis paper, we introduce CoVoMix2, a fully non-autoregressive framework for\nzero-shot multi-talker dialogue generation. CoVoMix2 directly predicts\nmel-spectrograms from multi-stream transcriptions using a flow-matching-based\ngenerative model, eliminating the reliance on intermediate token\nrepresentations. To better capture realistic conversational dynamics, we\npropose transcription-level speaker disentanglement, sentence-level alignment,\nand prompt-level random masking strategies. Our approach achieves\nstate-of-the-art performance, outperforming strong baselines like MoonCast and\nSesame in speech quality, speaker consistency, and inference speed. Notably,\nCoVoMix2 operates without requiring transcriptions for the prompt and supports\ncontrollable dialogue generation, including overlapping speech and precise\ntiming control, demonstrating strong generalizability to real-world speech\ngeneration scenarios.",
  "authors": [
    "Leying Zhang",
    "Yao Qian",
    "Xiaofei Wang",
    "Manthan Thakker",
    "Dongmei Wang",
    "Jianwei Yu",
    "Haibin Wu",
    "Yuxuan Hu",
    "Jinyu Li",
    "Yanmin Qian",
    "Sheng Zhao"
  ],
  "published": "2025-06-01T07:51:45Z",
  "updated": "2025-06-01T07:51:45Z",
  "categories": [
    "cs.SD",
    "cs.AI",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00885v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00885v1  [cs.SD]  1 Jun 2025CoVoMix2: Advancing Zero-Shot Dialogue Generation\nwith Fully Non-Autoregressive Flow Matching\nLeying Zhang1,2∗Yao Qian2†Xiaofei Wang2Manthan Thakker2Dongmei Wang2\nJianwei Yu2Haibin Wu2Yuxuan Hu2Jinyu Li2Yanmin Qian1Sheng Zhao2\n1Shanghai Jiao Tong University, China2Microsoft, USA\nAbstract\nGenerating natural-sounding, multi-speaker dialogue is crucial for applications\nsuch as podcast creation, virtual agents, and multimedia content generation. How-\never, existing systems struggle to maintain speaker consistency, model overlapping\nspeech, and synthesize coherent conversations efficiently. In this paper, we intro-\nduce CoV oMix2, a fully non-autoregressive framework for zero-shot multi-talker\ndialogue generation. CoV oMix2 directly predicts mel-spectrograms from multi-\nstream transcriptions using a flow-matching-based generative model, eliminating\nthe reliance on intermediate token representations. To better capture realistic\nconversational dynamics, we propose transcription-level speaker disentanglement,\nsentence-level alignment, and prompt-level random masking strategies. Our ap-\nproach achieves state-of-the-art performance, outperforming strong baselines like\nMoonCast and Sesame in speech quality, speaker consistency, and inference speed.\nNotably, CoV oMix2 operates without requiring transcriptions for the prompt and\nsupports controllable dialogue generation, including overlapping speech and pre-\ncise timing control, demonstrating strong generalizability to real-world speech\ngeneration scenarios. Audio samples are available3.\n1 Introduction\nThe field of speech synthesis has witnessed remarkable progress in recent years, particularly in zero-\nshot text-to-speech (TTS) systems that can generate high-quality, natural-sounding speech in voices\nnot seen during training [ 1–6]. These advancements have enabled applications such as personalized\nvirtual assistants, audiobook narration, and interactive voice response systems. However, extending\nthese capabilities to multi-talker dialogue generation, where multiple speakers engage in natural,\ndynamic, multi-turn conversations, remains a significant challenge.\nConventional approach to synthesize a dialogue is by generating multiple monologues sequentially and\nconcatenating them, which often results in unnatural interactions and poor speaker coordinations [ 7–\n10]. Recent efforts have shifted towards generating the entire dialogues in a more integrated manner.\nCoV oMix [ 9] was the first attempt to employ a multi-stream autoregressive (AR) text-to-semantic\nmodel and a non-autoregressive (NAR) acoustic model to synthesize mixed mel-spectrograms\nfor zero-shot dialogue generation. NotebookLM [ 11,12] leverages hierarchical transformer to\nproduce a stream of audio tokens autoregressively, which are decoded back to a dialogue waveform.\n∗Work done during an internship at Microsoft Azure AI. zhangleying@sjtu.edu.cn\n†Correspondence: yaoqian@microsoft.com\n3https://www.microsoft.com/en-us/research/project/covomix/covomix2/\nPreprint. Under review.\n--- Page 2 ---\nMasked\nHi! Nice to meet you!\nGood Morning! \n𝑡2𝑠𝑡𝑎𝑟𝑡𝑡2𝑒𝑛𝑑𝑡1𝑠𝑡𝑎𝑟𝑡𝑡1𝑒𝑛𝑑𝑡3𝑠𝑡𝑎𝑟𝑡𝑡3𝑒𝑛𝑑Spk1’s sequence\nSpk2’s sequenceNoisy \nSpeech\nPrompt \nSpeech\n[Spk1] [Spk2]\n[Spk1] [Spk2]Predicted \nTargetNo\n LossNo \nLossNo \nLoss\nStep 2: Randomly choose 2 prompt segments \nSpk1’s prompt Spk2’s prompt\nMasked\nHello! I’m fine, thank you!\nHow are you?\n𝑡2𝑠𝑡𝑎𝑟𝑡𝑡2𝑒𝑛𝑑𝑡1𝑠𝑡𝑎𝑟𝑡𝑡1𝑒𝑛𝑑𝑡3𝑠𝑡𝑎𝑟𝑡𝑡3𝑒𝑛𝑑Spk1’s sequence\nSpk2’s sequenceNoise\nPrompt \nSpeech\n[Spk1] [Spk2]\n[Spk1] [Spk2]Output Discarded\nStep 1: Randomly choose 1 dialogue for trainingStep 3: Train the flow -matching backbone\nSpk1’s prompt Spk2’s prompt\nSpk1 | Hi! | 𝑡1𝑠𝑡𝑎𝑟𝑡| 𝑡1𝑒𝑛𝑑\nSpk2 | Good Morning! | 𝑡2𝑠𝑡𝑎𝑟𝑡 | 𝑡2𝑒𝑛𝑑\nSpk1 | Nice to meet you! | 𝑡3𝑠𝑡𝑎𝑟𝑡 | 𝑡3𝑒𝑛𝑑\nVocoder\nStep 1: Provide 2 speaker’s prompts\nStep 2: Choose desired speed/duration/silence \nof each utteranceSpk1’s prompt Spk2’s promptStep 3: Generate wavs through ODE and vocoder\n1. Spk1 speaks fast\n2.Second sentence’s \nduration is 3s\n3. Have a 5s silence𝑡1𝑠𝑡𝑎𝑟𝑡,𝑡1𝑒𝑛𝑑\n𝑡2𝑠𝑡𝑎𝑟𝑡, 𝑡2𝑒𝑛𝑑\n· · ·  · · ·\n𝑡𝑛𝑠𝑡𝑎𝑟𝑡, 𝑡𝑛𝑒𝑛𝑑DefineCoVoMix2 ?\nWhat’s that ? \nHi! Do you know \nCoVoMix2 ?It is a new dialogue\ngeneration model!\nInference Stage\nTraining Stage\nt=0t=1\nt=0t=1\n𝑑𝑤𝑡(𝑚)=𝑣𝑡𝑤𝑡(𝑚)𝑚𝑐𝑡𝑥, 𝑧𝑑𝑡Flow -matching Backbone𝑑𝑤𝑡(𝑚)=𝑣𝑡𝑤𝑡(𝑚)𝑚𝑐𝑡𝑥, 𝑧𝑑𝑡Flow -matching BackboneFigure 1: The overview of the proposed CoV oMix2 framework\nMoonCast [ 10] utilizes long-context text-to-semantic model to support coherent dialogue generation.\nThese systems typically adopt a two-stage AR+NAR pipeline involving intermediate representations,\nsuch as semantic or audio tokens.\nDespite these advances, current systems face several key limitations. Firstly, dependence on AR\ncomponents and intermediate representations introduces considerable complexity and slow inference\nspeed. Secondly, the lack of fine-grained control over prosodic features, such as speaking rate,\nutterance duration, overlap, and pauses, often results in unnatural and less coherent dialogue flow.\nThirdly, speaker identity inconsistencies, where the wrong voice is occasionally assigned to an\nutterance, undermine perceived authenticity. Lastly, current methods [ 7,9,13,14] are hindered\nby their reliance on stereo audio for training and paired audio-text prompts for inference, often\nnecessitating external ASR models and limiting scalability.\nTo address these challenges, we propose CoV oMix2 ( Conversational VoiceMixture Generation), a\nnovel framework for zero-shot multi-talker dialogue generation based on a fully NAR flow-matching\napproach. Our main contributions are summarized as follows:\n1.We exploit a fully non-autoregressive framework for zero-shot multi-talker dialogue gen-\neration. It is an end-to-end modeling from the interleaved character sequence of two\nspeakers to their mixed mel-spectrograms, with intrinsic alignment between characters and\nmel-spectrogram frames.\n2.We propose a transcription-level disentanglement strategy in which each speaker’s tran-\nscriptions are provided as separate streams, enabling potential control over the timing of\ngenerated overlapping speech.\n2\n--- Page 3 ---\n3.Additionally, we introduce sentence-level alignment and prompt-level masking strategies\nthat facilitate accurate speaker identity assignment and eliminate the need for intermediate\nrepresentations or transcriptions of speaker prompts during inference.\n4.Extensive experiments show that CoV oMix2 outperforms strong baselines, achieving state-of-\nthe-art (SOTA) performance among open-source checkpoints, while requiring significantly\nless training data and delivering faster inference speed.\n2 Related Work\n2.1 Flow-matching based Speech Synthesis\nFlow-matching-based speech synthesis has emerged as a promising approach in the field of text-to-\nspeech (TTS) systems [ 15,16]. It models speech generation as a continuous transformation from\nsimple distributions to complex speech representations, enabling fast, parallel inference and improved\ncontrollability compared to traditional AR models [6, 17].\nSeveral recent models [ 18–23] have demonstrated the effectiveness of flow matching across tasks such\nas zero-shot TTS, speech inpainting, and voice conversion. Notably, E2-TTS [ 24] and F5-TTS [ 25]\nremove the need for phoneme alignment or explicit duration modeling, resulting in simpler pipelines\nwith high-quality output. These advancements establish flow-matching as a promising direction for\nbuilding scalable and efficient TTS systems.\n2.2 Multi-talker Dialogue Generation\nMulti-talker dialogue generation aims to synthesize entire multi-turn conversations between multiple\nspeakers, where each utterance must be coherent, contextually relevant, and speaker-specific. This task\nis significantly more complex than single-speaker synthesis due to the need for speaker turn-taking,\nidentity preservation, and interaction dynamics.\nEarly approaches, such as CoV oMix [ 9], introduced a multi-stream AR text-to-semantic model\ncombined with a NAR acoustic decoder to generate dialogues in a zero-shot manner. Other systems\nlike Soundstorm [ 12] and NotebookLM [ 11] incorporate both AR and NAR components to model\nhierarchical audio tokens or contextual embeddings. Further, works such as Chats[ 14] and SLIDE[ 13],\nbuilt upon the dGSLM framework [ 7], leverage dual-tower transformer architectures to capture\ninterleaved speaker information. Encoder-decoder models like Dia 1.6B[ 26] and Parakeet[ 27] directly\npredict audio codec tokens, which are then decoded into waveforms. MoonCast [ 10] addresses\nlong-context dialogue generation using a transformer-based language model, concatenating acoustic\nprompts at each turn to manage speaker changes.\nWhile these methods have advanced the field by improving naturalness and speaker consistency, most\nrely on AR decoding or hybrid AR/NAR architectures. These designs introduce several drawbacks:\nslow inference due to step-by-step generation, reliance on complex intermediate representations, lim-\nited controllability over conversational dynamics such as overlap, pauses, or timing, and dependence\non the paired text-audio speaker prompts.\n2.3 Conversational Speech Synthesis\nConversational Speech Synthesis (CSS) focuses on generating individual utterances that are contextu-\nally appropriate within a dialogue, typically from a single speaker. Unlike full dialogue generation,\nCSS produces each utterance one-by-one, conditioned on previous dialogue history, rather than\nsynthesizing an entire conversation simultaneously [28–30].\nFor instance, GPTTalker [ 31] models multimodal dialogue context by converting history into discrete\ntokens processed by a language model to generate expressive, context-aware responses. Sesame [ 8]\nadopts two AR transformers: a multimodal transformer backbone processes text and audio tokens,\nfollowed by a audio decoder transformer that reconstructs high-quality speech. Although Sesame\nsupports dialogue-style synthesis, it fundamentally generates each speaker’s utterance sequentially\ngiven the previous dialogue context, rather than modeling simultaneous multi-speaker interaction.\nWhile CSS methods achieve high expressiveness and coherence for individual utterances, they fall\nshort in handling the broader structure and dynamics of real-time, overlapping multi-speaker dialogue.\n3\n--- Page 4 ---\nOur proposed CoV oMix2 differs fundamentally from both traditional multi-talker dialogue generation\nsystems and CSS approaches. Unlike prior work that relies on AR or hybrid pipelines, or CSS\nsystems that synthesize one speaker’s utterance at a time, CoV oMix2 enables fully NAR, simultaneous\ngeneration of multi-speaker dialogues. It directly predicts mel-spectrograms from disentangled multi-\nstream transcriptions, allowing efficient, accurate, and controllable synthesis with better support for\nreal conversational dynamics such as overlapping speech and natural pauses.\n3 CoVoMix2\nZero-shot dialogue generation aims to synthesize multi-speaker conversations in voices not encoun-\ntered during training. We propose CoV oMix2, a fully NAR zero-shot dialogue generation model\nthat directly generates mel-spectrograms from raw dialogue transcriptions. As shown in Figure 1,\nCoV oMix2 operates without intermediate representations such as phonemes or audio tokens, enabling\nefficient and scalable zero-shot dialogue generation.\nLet the training dataset be denoted as D={x, y}, where xis a dialogue waveform containing utter-\nances from two speakers, and y= [y1, ..., y n]is the corresponding transcription. Each transcription\nsegment yiis annotated with the content Ti, speaker label si, the start and end time tstart\ni, tend\ni. The\ncorresponding mel-spectrogram of xis noted as m. To support simultaneous, zero-shot, multi-speaker\ndialogue generation, we introduce three key design strategies in the following sections, yielding a pair\nof input text sequences z= [z1, z2], each representing a separate speaker stream, and the acoustic\nprompts mctxof the target speakers.\n3.1 Transcription-Level Speaker Disentanglement\nNatural speaker switching and overlapping speech are essential features in realistic dialogue genera-\ntion. Prior work often inserts speaker-change tokens (e.g., [spkchange]) to indicate a switch between\nspeakers [ 9–12]. However, single-stream representations entangle multiple speakers’ utterances into\na flat sequence, making it difficult for the model to explicitly capture speaker-specific timing, and\noverlap. Moreover, this design creates ambiguity in conditioning, particularly when aligning acoustic\nprompts to textual content. In a shared text stream, the model must disentangle which portions belong\nto which speaker and match them to the correct prompt, increasing the risk of identity confusion.\nInstead, as shown in Figure 2, we propose a more structured approach by disentangling the transcript\ninto multiple parallel streams, one per speaker. Each text stream zicontains two types of content:\nActive speech segments, defined by their respective time intervals [tstart\ni, tend\ni]. and Silence intervals,\ndenoted using a special token [S]. These streams enable precise temporal control over individual\nspeaker utterances, including overlapping and silence. This design grants fine-grained control over\nthe interaction structure, improving naturalness and flexibility.\nHow is the weather today?[P] [P] [P] [P] [P] [P]Hello! \nHow is the weather today?I think it’s sunny? Silence token [S]\n[S] [S]\nContinuation token [P] Character sequence\n𝑡4𝑠𝑡𝑎𝑟𝑡𝑡4𝑒𝑛𝑑𝑡2𝑠𝑡𝑎𝑟𝑡𝑡3𝑒𝑛𝑑𝑡1𝑠𝑡𝑎𝑟𝑡𝑡1𝑒𝑛𝑑𝑚\n𝑧1\n𝑧2Spk1 Spk1 Spk2\nHello! [S]𝑡2𝑒𝑛𝑑𝑡3𝑠𝑡𝑎𝑟𝑡Spk2Prompt\nCandidates\nFigure 2: Example of the input data organization\n3.2 Sentence-Level Alignment\nHigh-quality alignment at the phoneme level is often unavailable in real-world data [ 9]. Likewise,\nmodeling intermediate representations (e.g., codec tokens) is resource-intensive and requires large-\n4\n--- Page 5 ---\nscale training data. As an alternative, inspired by recent models [ 24,25], we design the sentence-level\nalignment that is suitable for both monologue and dialogue scenarios.\nAs indicated in Figure 2, each active speech segment in the speaker stream is converted into a sequence\nof characters (including upper and lower cases letters and punctuation). For the rest of the active\nspeech segment, we add a special continuation token [P]for padding. These sequences, temporally\nanchored by the corresponding start and end times, serve as the text input for the mel-spectrogram\nprediction. The model learns the intrinsic alignment between characters and mel-spectrogram frames\nand synthesizes each utterance within its designated time range without explicit duration prediction.\n3.3 Prompt-Level Random Masking\nAccurate voice conditioning is critical to avoid speaker confusion. Previous approaches either main-\ntained parallel conditioning streams [ 9,13], used concatenated speaker prompt for continuation [ 12]\nor appended speaker prompts at each dialogue turn to guide speech synthesis [ 10]. We introduce a\nprompt-level random masking strategy to ensure robust and diverse speaker conditioning.\nAs shown in Figure 1 and 2, for each training sample, we first find all the available monologue\nsegments for each speaker as the prompt candidates. We then randomly select a prompt segment\nfrom the candidates. The chosen prompts are concatenated at the very beginning with the masked\ntraining sample using a separator token to construct the prompt sequence mctx. Moreover, in the text\nstream z, we use special tokens [Spk1]and[Spk2]to indicate the range of each prompt, instead of\nproviding prompt’s text transcription.\nFurthermore, to prevent prompt leakage, where the model copies the prompt directly into the output,\nwe exclude the prompt region from the loss computation. This encourages the model to generalize\nvoice characteristics rather than memorize the prompt audio.\n3.4 Flow-Matching-Based Mel Spectrogram Generation\nOur model employs Flow Matching (FM), a simulation-free training method for Continuous Nor-\nmalizing Flows (CNFs) [ 16,32]. It is a class of generative models that learn to transform a simple\ndistribution (e.g., Gaussian noise) into a complex data distribution (e.g., mel-spectrograms) through a\ncontinuous mapping. This mapping is defined by solving an Ordinary Differential Equation (ODE).\nSpecifically, the model learns the distribution q(m|z, m ctx)where mis the target mel-spectrogram,\nzis the speaker-aware text streams and mctxis the acousic prompts. At training time, a noise sample\nm0is drawn from a standard Gaussian distribution. The training objective is to minimize the L2\ndistance between the predicted and true flow, given by Eq.1, where vt(·)is the model’s predicted\nvector field at time t∈[0,1],w= (1−(1−σmin)t)m0+tm, and σminis a hyper-parameter to\ncontrol the deviation of flow-matching. Notably, the loss is not computed on segments that originate\nfrom the prompt region, which is excluded using a masking function M(·).\nL=E∥M((m−(1−σmin)m0)−vt(w, m ctx, z;θ))∥2(1)\nWe also apply Classifier-Free Guidance (CFG) [ 33,19] to improve sample quality by interpolating\nbetween conditioned and unconditioned flows. During training, the acoustic prompt mctxand text\nsequences zare dropped with puncond .\nDuring inference, the CFG vector field becomes Eq.2, with αcontrolling the strength of guidance.\nDurations for each utterance are computed based on syllable counts and a predefined speaking\nrate, allowing the construction of the input text streams z. A mel-spectrogram is then generated by\nsampling noise m0and solving the ODE defined by the flow field.\n˜vt(w, m ctx, z;θ) = (1 + α)vt(w, m ctx, z;θ)−α˜vt(w;θ) (2)\n3.5 Training Strategy: Curriculum Learning and Data Mixing\nTo enable the model with dialogue generation capability efficiently, we adopt a two-stage curriculum\nlearning strategy during training. First, the model is pretrained on high-quality monologue datasets,\nwhich helps it learn accurate pronunciation and acoustic modeling. In contrast, directly training on\nmulti-speaker data causes degraded output quality, including mispronunciations and unintelligible\n5\n--- Page 6 ---\nspeech. Then, in the second stage, we train the model on multi-speaker dialogue datasets to enhance\nthe dialogue generation capability.\nTo improve robustness and generalization, built on the large scale of monologue dataset, we design the\ndata mixing strategy, using various sources of data during the second stage training. Specifically, we\nmix data from several sources, including ASR-transcribed podcast dialogues, audiobook-style single-\nspeaker datasets and simulated overlapped dialogues to support overlapping capability. Benefited\nfrom this data diversity, CoV oMix2 does not require any human-annotated dialogues to get satisfied\nand natural results. For further enhancement, we demonstrate in Appendix C.2 that fine-tuning the\nmodel with just 20 minutes of clean, human-annotated dialogue can significantly boost performance.\n4 Experimental Setup\n4.1 Training and Inference Data Preparation\nDenoising\nASRDiarization\nSpeaker Aware \nWord -level TranscriptionIn the wild \nPodcast dataSpk1 | Okay | 3501ms | 3799ms\nSpk1 | Right | 3802ms | 3920ms\nSpk2 | Yeah | 3920ms | 4000ms\nSpk3 | Bye | 10580ms | 11200msSpeaker Number\nMaximum Duration\nMinimum DurationSegmentation\nPodcast dialogue\nPodcast monologue\nSimulated dialogue\nFigure 3: Data processing pipeline\nTo train CoV oMix2, we curated a diverse dataset comprising both in-the-wild dialogues and clean\nmonologue data. The core training corpus consists of 3,000 hours of English podcast data, which we\nprocessed with 4 steps, as shown in Figure 3.\nFirst, we applied a market leading speech enhancement API to remove background noise and music.\nSecond, we implemented automatic speech recognition and speaker diarization. The diarization\nresults were used to assign speaker identities to each utterance with timestamps. Specifically, we used\nthe Deepgram API [ 34]4to obtain word-level transcriptions and speaker labels. Third, following [ 9]5,\nlong dialogues were segmented into shorter clips, each involving two or one speakers, to improve\ndata quality and training efficiency. Finally, we simulate dialogue segments using the monologue\ndatasets. By pairing utterances from different speakers and introducing controlled overlap or silence\nratios, we generate synthetic two-speaker dialogues.\nFor the first training stage, we used the LibriHeavy dataset [ 35], comprising 60k hours of high-quality\nsingle-speaker audiobook-style recordings. In the second stage, in addition to the podcast dataset,\nwe simulated dialogue-style data by concatenating utterances from different speakers using both\nLibriHeavy and LibriTTS [ 36]. To further enhance the model’s ability to handle overlapping speech,\nwe generated highly overlapped data from LibriHeavy, varying the overlap ratio from 0% to 100%.\nIn order to evaluate the model performance, we design a dialogue test set6, containing 1000 dialogue\ntranscriptions from Dailydialog [ 37] and the acoustic prompts are from Librispeech-test-clean [ 38].\nWe also use samples from this dialogue dataset for subjective evaluation. To ensure that the monologue\ngeneration capability is retained, we also designed a monologue test set, with results presented in\nAppendix B.\n4.2 Model Configuration\nIn our experiments, the backbone architecture closely followed the configurations in [ 24]. Specifically,\nwe used Transformer with 24 layers, 16 attention heads, and an embedding dimension of 1024 with\nU-Net [ 39] style skip connections. The σminis set to 0.1. We modeled the 100-dimensional log\nmel-filter bank features, extracted every 10.7 milliseconds from audio samples with a 24kHz sampling\nrate. A BigVGAN-based [ 40] vocoder was employed to convert the log mel-filter bank features\n4https://deepgram.com/\n5https://github.com/vivian556123/NeurIPS2024-CoVoMix/tree/main\n6https://github.com/vivian556123/covomix2-dialogue-testset.git\n6\n--- Page 7 ---\ninto waveforms. In addition, we implemented Classifier-Free Guidance (CFG) [ 33] with a dropout\nprobability puncond = 20% , randomly removing conditioning during training.\nIn the first training stage, we train the model on 60k hours LibriHeavy [ 35] dataset for 200k steps\nwith peak learning rate(lr) of 7.5e-5. In the second training stage, we train it for another 200k steps\non the combined podcast, audiobook, and simulated dialogue datasets with a peak lr of 5e-5. The\nmodel was optimized using the Adam optimizer. A linear-decay learning rate schedule was used in\nboth stages. Each training batch contained two samples, each less than 30 seconds in duration.\nTraining was conducted on 32 NVIDIA Tesla V100 GPUs (32GB) with gradient accumulation set to\n4. During inference, we used a guidance strength αof 1.0 and performed sampling with 32 function\nevaluations (NFE) using an ODE solver.\n4.3 Baseline and Evaluation Metrics\nWe adopt MoonCast [ 10], a latest state-of-the-art dialogue generation model employing a hybrid\nAR+NAR architecture, as a representative baseline. While simple audio concatenation based on\nsingle-speaker models has been proved to be ineffective in capturing speaker interactions [ 9,10], we\nadditionally include Sesame [ 8] as a strong baseline. As a representative model for the CSS task,\nSesame generates each utterance sequentially, leveraging previously generated audio as contextual\ninput to maintain coherence across dialogue turns. Detailed baseline configuration comparison is\nprovided in Appendix A.\nAlthough other models are capable of dialogue generation, we exclude Dia[ 26], CoV oMix[ 9], and\nNotebookLM [ 11] from our comparisons. Dia tends to produce unnaturally rapid and truncated\ndialogues, and CoV oMix is trained exclusively on 8kHz audio, leading to low-fidelity outputs that are\nnot directly comparable to our high-quality generation setting. NotebookLM is not open-sourced,\npreventing us from making a reasonable comparison.\nTo comprehensively assess the recognition accuracy and speaker consistency, we adopt the following\nobjective evaluation metrics: Real-Time Factor (RTF), Word Error Rate (WER), Speaker-Aware\nWord Error Rate (SA-WER) [ 41], Speaker-Aware Speaker Similarity (SA-SIM) and UTMOS [ 42].\nSpecifically, We measure the RTF on a single NVIDIA A100 machine. We utilize market leading\nautomatic speech recognition and diarization tool to transcribe the generated speech, and we calculate\nthe SA-WER for each word and their corresponding speaker identity. The SA-SIM enhances SIM\nby ensuring that the similarity is computed between embeddings attributed to the speaker identity\ndetected by the diarization model. We utilize WavLM-TDNN [ 43] to extract the speaker embeddings.\nWe perform a human evaluation on the generated dialogue examples. We conducted a Comparable\nMean Opinion Score (CMOS) experiment to assess user preference in terms of speaker turn handling,\ninteractivity, fluency, and coherence. 15 professional linguistic experts provide judges for all subjec-\ntive evaluations. They provide a rating to the second audio, which is randomly selected from a pair of\naudios, in the (-3 to +3) range. Detailed instructions are in Appendix D.\n5 Result and Analysis\n5.1 Objective and Subjective Evaluation Results\nTable 1: Model performance comparison on dialogue data\nModel RTF↓ WER↓ SA-WER ↓ SA-SIM ↑UTMOS ↑ CMOS ↑\nMoonCast 1.37 7.08±46.23 20.40 ±51.09 0.40 ±0.20 2.65 ±0.42 -0.25±0.63\nSesame 2.08 5.62±5.61 9.65±13.20 0.49 ±0.17 2.70 ±0.44 -0.39±0.40\nCoV oMix2 0.30 5.73±6.68 6.31±9.24 0.56 ±0.14 3.10 ±0.35 0.00±0.00\nTable 1 presents evaluation results comparing CoV oMix2 against the baseline models MoonCast\nand Sesame on dialogue test sets. Standard deviations (1-sigma) are reported assuming approximate\nnormality. While WER is positive, some large standard deviations result in negative lower bounds,\nwhich are not meaningful in practice but reflect high sample variability.\n7\n--- Page 8 ---\nAcross nearly all metrics, CoV oMix2 demonstrates clear superiority in speech quality, speaker\nconsistency, and inference speed. CoV oMix2 achieves a RTF of 0.30, significantly faster than both\nMoonCast and Sesame, demonstrating its efficiency as a fully NAR model.\nIn terms of content accuracy and speaker consistency, CoV oMix2 attains the best SA-WER and\nSA-SIM, highlighting its ability to maintain both linguistic accuracy and consistent speaker identity\nacross multi-turn interactions.\nIn contrast, MoonCast, being language-model-based, often suffers from issues such as speaker\nconfusion, hallucinated content, repetitive outputs, and improper termination. These artifacts lead to\nunstable generation, reflected in its relatively high WER and SA-WER scores.\nSesame, despite its strong performance in standard WER, occasionally exhibits speaker confusion.\nWe observe cases where monologue-style outputs include voice characteristics from the other speaker,\neven though speaker identities are clearly specified. This suggests that the shared contextual input,\ncontaining prompts and prior audio from both speakers, may introduce ambiguity, making it difficult\nfor the model to consistently distinguish between speakers in extended dialogue scenarios.\nFurthermore, the lower standard deviations observed across all metrics for CoV oMix2 indicate more\nstable and reliable output quality, further validating the effectiveness of our flow-matching-based\narchitecture for multi-speaker dialogue synthesis.\nFinally, the subjective CMOS comparison between CoV oMix2 and other models demonstrates that\nour model performs better in terms of speaker turn handling, interactivity, fluency, and coherence with\nthe transcription. We also ask judges to give detailed comments, where better pitch and intonation,\nbetter rhythm and less distortion are three main advantages of our proposed CoV oMix2.\n5.2 Speaker Consistency\nSpeech Turns Speech Turns Speech Turns Speech TurnsSpeech TurnsAverage Speaker Similarity\n(a) (b)Mooncast Sesame Ours\nFigure 4: Speaker consistency analysis across dialogue turns. (a) Average speaker similarity between\neach generated turn and its corresponding prompt. (b) Pairwise speaker similarity between turns from\nthe same speaker within a dialogue. Consistent color indicates stable speaker timbre across turns.\nTo evaluate speaker consistency across turns, we selected a long dialogue sequence containing 12\nspeech turns, with each speaker contributing 6 utterances.\nFigure 4(a) shows the average speaker similarity between each generated turn and its corresponding\nprompt. While Sesame achieves relatively high similarity in some turns, its performance is incon-\nsistent. This may be due to its design, which generates each utterance independently, relying on a\nprompt placed only at the beginning. As the dialogue progresses, the prompt information may be\nforgotten or get confused, especially in longer contexts. MoonCast, by contrast, demonstrates consis-\ntently low similarity. Although it incorporates prompts at the beginning of each turn, this additional\nconditioning does not improve speaker accuracy, indicating limited benefit despite the increased\ncomputational overhead. In comparison, CoV oMix2 achieves the highest average similarity with the\nlowest variance, even though the prompt is provided only once at the beginning of the dialogue. This\nresult demonstrates the robustness in maintaining speaker identity over long conversations.\nFigure 4(b) further investigates intra-dialogue speaker consistency by measuring pairwise speaker\nsimilarity across all turns from the same speaker. A more uniform and consistent color distribution\nalong the rows and columns reflects stronger identity preservation. Both MoonCast and Sesame\ndisplay noticeable variability, indicating timbre drift or identity shift during generation. In contrast,\n8\n--- Page 9 ---\nour model demonstrates consistently high pairwise speaker similarity across all turns, highlighting its\neffectiveness in preserving speaker timbre throughout multi-turn dialogues.\n5.3 Overlapping Analysis\nAchieving overlap in dialogue generation is a challenge for current models. Most models rely on data\nto achieve overlaps [ 9] and are constrained by the overlap reconstruction capabilities of codecs [ 11].\nFigure 5 shows a visual comparison of mel-spectrograms, extracted from overlapping segments\ngenerated by NotebookLM, CoV oMix, CoV oMix2, and a real overlapping sample, where the first two\nsamples are extracted from the official demo page.\nOverlapping speech is characterized by the simultaneous presence of multiple harmonic structures\nin the mel-spectrogram, resulting in smooth, continuous spectral patterns. These patterns blend\nnaturally over time, leading to dense, interwoven energy distributions without abrupt boundaries or\nartificial transitions [ 44–46]. We observe that CoV oMix2 generates overlapping speech with higher\nspectral fidelity, smoother harmonic structure, and more natural timing alignment, closely matching\nreal overlapping dialogue. In contrast, NotebookLM’s output resembles a concatenation of sound\nsegments rather than a genuine learned overlapping process, and CoV oMix’s speech has low fidelity\nbecause of the lower data quality.\n(a) NotebookLM (c) CoVoMix2 (d) Real overlapping sample (b) CoVoMix\nFigure 5: Mel-spectrogram comparison between overlapping samples generated by NotebookLM,\nCoV oMix, CoV oMix2 and the real sample.\n6 Ablation Studies and Extension\nWe conducted extensive ablation studies in Appendix C to validate our design choices across data\ncomposition, training stages, and special token representation. Results show that pretraining on\nmonologue data is essential for learning stable pronunciation, while data mixing strategy improves\ngeneralization to diverse dialogue patterns. Simulated dialogue data may introduce noise. Addition-\nally, we demonstrate that a brief fine-tuning stage using just 20 minutes of human-annotated data can\nsignificantly boost transcription accuracy and speaker consistency.\nOur framework enables a wide range of practical applications beyond standard dialogue synthesis.\nSince CoV oMix2 does not require transcriptions for speaker prompts, it supports cross-lingual voice\ncloning, allowing voices to be transferred across languages. Its temporal control features—including\nfine-grained manipulation of speech overlap, pauses, and duration—make it especially suited for\napplications like podcast creation and video dubbing, where precise alignment with visual content or\nconversational pacing is essential. The ability to generate overlapping speech also enhances realism\nin multi-speaker scenes, such as dramatic dialogues or animated character interactions.\n7 Conclusion, Limitation, Future Work and Broader Impacts\nIn this work, we introduced CoV oMix2, a fully NAR framework for zero-shot multi-talker dialogue\ngeneration. By directly predicting mel-spectrograms from disentangled multi-stream transcriptions\nand leveraging a flow-matching-based method, CoV oMix2 enables efficient, high-quality synthesis of\nnatural, speaker-consistent dialogues, including controlled overlapping speech and fine-grained timing\ncontrol. Through extensive experiments, we demonstrated that CoV oMix2 outperforms existing\nmodels in both speech quality and speaker accuracy while achieving significantly faster inference.\nFuture work In future work, we plan to extend our framework to support conversations involving\nmore than two speakers, with improved modeling of naturalistic speech overlap and richer conver-\nsational dynamics. We also aim to scale up the training to even larger and more diverse datasets,\nenabling broader generalization across domains and languages.\n9\n--- Page 10 ---\nLimitation While our training data covers a wide range of conversational scenarios, it is primarily\nautomatically transcribed, which may introduce minor inaccuracies, particularly in handling disflu-\nencies such as repetitions or backchannel words. Additionally, since ASR tools lack support for\nword-level timestamps in overlapping speech, we rely on simulated dialogue data to train overlap\nscenarios, which can introduce slight deviations in naturalness and degraded audio quality.\nBroader Impacts CoV oMix2 offers a versatile and scalable solution for high-quality, human-like\nspeech generation, with potential applications in assistive technology, media production, language\nlearning, and virtual agents. However, since CoV oMix2 could synthesize speech that maintains\nspeaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identifica-\ntion or impersonating a specific speaker. To mitigate such risks, it is possible to build a detection\nmodel to discriminate whether an audio clip was synthesized by CoV oMix2.\n10\n--- Page 11 ---\nReferences\n[1]Z. Ju, Y . Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y . Liu, Y . Leng, K. Song, S. Tang et al. ,\n“Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models,” arXiv\npreprint arXiv:2403.03100 , 2024.\n[2]Y . Leng, Z. Guo, K. Shen, Z. Ju, X. Tan, E. Liu, Y . Liu, D. Yang, leying zhang, K. Song, L. He,\nX. Li, sheng zhao, T. Qin, and J. Bian, “PromptTTS 2: Describing and generating voices with\ntext prompt,” in The Twelfth International Conference on Learning Representations , 2024.\n[3]M. Łajszczak, G. Cámbara, Y . Li, F. Beyhan, A. Van Korlaar, F. Yang, A. Joly, Á. Martín-\nCortinas, A. Abbas, A. Michalski et al. , “Base tts: Lessons from building a billion-parameter\ntext-to-speech model on 100k hours of data,” arXiv preprint arXiv:2402.08093 , 2024.\n[4]Z. Du, Y . Wang, Q. Chen, X. Shi, X. Lv, T. Zhao, Z. Gao, Y . Yang, C. Gao, H. Wang et al. ,\n“Cosyvoice 2: Scalable streaming speech synthesis with large language models,” arXiv preprint\narXiv:2412.10117 , 2024.\n[5]P. Anastassiou, J. Chen, J. Chen, Y . Chen, Z. Chen, Z. Chen, J. Cong, L. Deng, C. Ding, L. Gao\net al. , “Seed-tts: A family of high-quality versatile speech generation models,” arXiv preprint\narXiv:2406.02430 , 2024.\n[6]S. Chen, C. Wang, Y . Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y . Liu, H. Wang, J. Li, L. He,\nS. Zhao, and F. Wei, “Neural codec language models are zero-shot text to speech synthesizers,”\nIEEE Transactions on Audio, Speech and Language Processing , vol. 33, pp. 705–718, 2025.\n[7]T. A. Nguyen, E. Kharitonov, J. Copet, Y . Adi, W.-N. Hsu, A. Elkahky, P. Tomasello, R. Algayres,\nB. Sagot, A. Mohamed et al. , “Generative spoken dialogue language modeling,” Transactions\nof the Association for Computational Linguistics , vol. 11, pp. 250–266, 2023.\n[8]J. Schalkwyk, A. Kumar, D. Lyth, S. Eskimez, Z. Hodari, C. Resnick, R. Sanabria, and R. Jiang,\n“Crossing the uncanny valley of conversational voice — sesame.com,” https://www.sesame.com/\nresearch/crossing_the_uncanny_valley_of_voice, [Accessed 17-04-2025].\n[9]L. Zhang, Y . Qian, L. Zhou, S. Liu, D. Wang, X. Wang, M. Yousefi, Y . Qian, J. Li, L. He et al. ,\n“CoV oMix: Advancing zero-shot speech generation for human-like multi-talker conversations,”\nAdvances in Neural Information Processing Systems , vol. 37, pp. 100 291–100 317, 2024.\n[10] Z. Ju, D. Yang, J. Yu, K. Shen, Y . Leng, Z. Wang, X. Tan, X. Zhou, T. Qin, and X. Li, “MoonCast:\nHigh-quality zero-shot podcast generation,” arXiv preprint arXiv:2503.14345 , 2025.\n[11] “Pushing the frontiers of audio generation — deepmind.google,” https://deepmind.google/\ndiscover/blog/pushing-the-frontiers-of-audio-generation/, [Accessed 27-04-2025].\n[12] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi, “Sound-\nstorm: Efficient parallel audio generation,” arXiv preprint arXiv:2305.09636 , 2023.\n[13] H. Lu, G. Cheng, L. Luo, L. Zhang, Y . Qian, and P. Zhang, “SLIDE: Integrating speech language\nmodel with llm for spontaneous spoken dialogue generation,” in ICASSP 2025 - 2025 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2025, pp. 1–5.\n[14] K. Mitsui, Y . Hono, and K. Sawada, “Towards human-like spoken dialogue generation between\nai agents from written dialogue,” arXiv preprint arXiv:2310.01088 , 2023.\n[15] R. T. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural ordinary differential\nequations,” Advances in neural information processing systems , vol. 31, 2018.\n[16] Y . Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative\nmodeling,” in The Eleventh International Conference on Learning Representations , 2023.\n[17] B. Han, L. Zhou, S. Liu, S. Chen, L. Meng, Y . Qian, Y . Liu, S. Zhao, J. Li, and F. Wei, “Vall-e\nr: Robust and efficient zero-shot text-to-speech synthesis via monotonic alignment,” arXiv\npreprint arXiv:2406.07855 , 2024.\n11\n--- Page 12 ---\n[18] S. Mehta, R. Tu, J. Beskow, É. Székely, and G. E. Henter, “Matcha-tts: A fast tts architecture\nwith conditional flow matching,” in ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp. 11 341–11 345.\n[19] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V . Manohar, Y . Adi,\nJ. Mahadeokar et al. , “V oicebox: Text-guided multilingual universal speech generation at scale,”\nAdvances in neural information processing systems , vol. 36, pp. 14 005–14 034, 2023.\n[20] A. Vyas, B. Shi, M. Le, A. Tjandra, Y .-C. Wu, B. Guo, J. Zhang, X. Zhang, R. Adkins, W. Ngan\net al. , “Audiobox: Unified audio generation with natural language prompts,” arXiv preprint\narXiv:2312.15821 , 2023.\n[21] L. Zhang, W. Zhang, Z. Chen, and Y . Qian, “Advanced zero-shot text-to-speech for background\nremoval and preservation with controllable masked speech prediction,” in ICASSP 2025 - 2025\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2025.\n[22] Y . Guo, C. Du, Z. Ma, X. Chen, and K. Yu, “V oiceflow: Efficient text-to-speech with rectified\nflow matching,” in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2024, pp. 11 121–11 125.\n[23] Z. Chen, S. Wang, M. Zhang, X. Liu, J. Yamagishi, and Y . Qian, “Disentangling the prosody\nand semantic information with pre-trained model for in-context learning based zero-shot voice\nconversion,” in 2024 IEEE Spoken Language Technology Workshop (SLT) , 2024.\n[24] S. E. Eskimez, X. Wang, M. Thakker, C. Li, C.-H. Tsai, Z. Xiao, H. Yang, Z. Zhu, M. Tang,\nX. Tan et al. , “E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts,” in 2024 IEEE\nSpoken Language Technology Workshop (SLT) . IEEE, 2024, pp. 682–689.\n[25] Y . Chen, Z. Niu, Z. Ma, K. Deng, C. Wang, J. Zhao, K. Yu, and X. Chen, “F5-tts: A fairytaler\nthat fakes fluent and faithful speech with flow matching,” arXiv preprint arXiv:2410.06885 ,\n2024.\n[26] “GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.\n— github.com,” https://github.com/nari-labs/dia, [Accessed 27-04-2025].\n[27] J. Darefsky, G. Zhu, and Z. Duan, “Parakeet,” 2024. [Online]. Available: https:\n//jordandarefsky.com/blog/2024/parakeet/\n[28] Y . Hu, R. Liu, G. Gao, and H. Li, “Fctalker: Fine and coarse grained context modeling for\nexpressive conversational speech synthesis,” in 2024 IEEE 14th International Symposium on\nChinese Spoken Language Processing (ISCSLP) . IEEE, 2024, pp. 299–303.\n[29] J. Xue, Y . Deng, F. Wang, Y . Li, Y . Gao, J. Tao, J. Sun, and J. Liang, “M2-ctts: End-to-end\nmulti-scale multi-modal conversational text-to-speech synthesis,” in ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2023,\npp. 1–5.\n[30] H. Guo, S. Zhang, F. K. Soong, L. He, and L. Xie, “Conversational end-to-end tts for voice\nagents,” in 2021 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2021, pp.\n403–409.\n[31] R. Liu, Y . Hu, Y . Ren, X. Yin, and H. Li, “Generative expressive conversational speech synthesis,”\ninProceedings of the 32nd ACM International Conference on Multimedia , 2024, pp. 4187–4196.\n[32] R. T. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural ordinary differential\nequations,” Advances in Neural Information Processing Systems , vol. 31, 2018.\n[33] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” arXiv preprint arXiv:2207.12598 ,\n2022.\n[34] “Enterprise Voice AI: STT, TTS & Agent APIs | Deepgram — deepgram.com,” https://deepgram.\ncom/, [Accessed 17-04-2025].\n12\n--- Page 13 ---\n[35] W. Kang, X. Yang, Z. Yao, F. Kuang, Y . Yang, L. Guo, L. Lin, and D. Povey, “Libriheavy: A\n50,000 hours asr corpus with punctuation casing and context,” in ICASSP 2024-2024 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024,\npp. 10 991–10 995.\n[36] H. Zen, V . Dang, R. Clark, Y . Zhang, R. J. Weiss, Y . Jia, Z. Chen, and Y . Wu, “LibriTTS: A\ncorpus derived from librispeech for text-to-speech,” in Interspeech 2019 , 2019, pp. 1526–1530.\n[37] Y . Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu, “Dailydialog: A manually labelled multi-turn\ndialogue dataset,” arXiv preprint arXiv:1710.03957 , 2017.\n[38] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on\npublic domain audio books,” in 2015 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP) . IEEE, 2015, pp. 5206–5210.\n[39] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image\nsegmentation,” in Medical image computing and computer-assisted intervention–MICCAI 2015:\n18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 .\nSpringer, 2015, pp. 234–241.\n[40] S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, “BigVGAN: A universal neural\nvocoder with large-scale training,” in The Eleventh International Conference on Learning\nRepresentations .\n[41] N. Kanda, Y . Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, and T. Yoshioka, “Joint speaker\ncounting, speech recognition, and speaker identification for overlapped speech of any number\nof speakers,” arXiv preprint arXiv:2006.10930 , 2020.\n[42] K. Baba, W. Nakata, Y . Saito, and H. Saruwatari, “The t05 system for the VoiceMOS Challenge\n2024: Transfer learning from deep image classifier to naturalness MOS prediction of high-\nquality synthetic speech,” in IEEE Spoken Language Technology Workshop (SLT) , 2024.\n[43] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao\net al. , “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE\nJournal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505–1518, 2022.\n[44] D. Wang and J. Chen, “Supervised speech separation based on deep learning: An overview,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 26, no. 10, pp.\n1702–1726, 2018.\n[45] W. Chen, E. S. C. Van Tung Pham, E. S. Chng, and X. Zhong, “Overlapped speech detection\nbased on spectral and spatial feature fusion.” in Interspeech , 2021, pp. 4189–4193.\n[46] T. Kristjansson and J. Hershey, “High resolution signal reconstruction,” in 2003 IEEE Workshop\non Automatic Speech Recognition and Understanding (IEEE Cat. No. 03EX721) . IEEE, 2003,\npp. 291–296.\n[47] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech\nrecognition via large-scale weak supervision,” in International conference on machine learning .\nPMLR, 2023, pp. 28 492–28 518.\n13\n--- Page 14 ---\nA Model Comparison\nWe compare our proposed CoV oMix2 with two baseline models: MoonCast [ 10]7and Sesame [ 8]8.\nThe detailed model comparisons are shown in Table 2, where N/A means not available.\nTable 2: Model detail comparison\nModel Params Training Data (hour) Training Resource\nModel Type Backbone Decoder V ocoder Dialogue Monologue GPU\nMoonCast AR+NAR 2.5B 0.8B 0.25B 215k 300k 64 A100 80GB\nSesame AR 1B 0.1B / 1000k N/A\nCoV oMix2 NAR 0.3B / 0.01B 3k 65k 32 V100 32GB\nB Monologue Performance Comparison\nTo ensure that the dialogue generation models retain strong performance in monologue settings, we\nevaluate all systems on an audiobook-style monologue test set. The transcription of the audio prompts\nis obtained using Whisper-large-v3 [ 47]9. As shown in Table 3, our CoV oMix2 trained solely on\nmonologue data, outperforms all baseline models. Notably, after the second-stage training on dialogue\ndata, CoV oMix2 maintains its monologue generation capabilities without much degradation. In\ncontrast, Sesame and MoonCast exhibit degraded performance due to hallucination issues, frequently\nproducing repetitive or semantically irrelevant content.\nIn extreme cases, MoonCast fails to terminate the speech generation, resulting in infinite loops. Note\nthat the transcription recognized by Whisper may contain errors, which might be one of the reasons\nwhy these language model’s hallucination worsens. Our CoV oMix2, however, does not require the\ntranscription corresponding to the prompt, thus effectively avoiding such issues.\nTable 3: Model performance comparison on monologue data.\nModel WER↓ SIM↑ UTMOS ↑\nMoonCast 15.92±28.50 0.63 ±0.16 2.82 ±0.48\nSesame 7.58±12.39 0.72 ±0.17 2.81 ±0.52\nCoV oMix2 † 4.30±4.75 0.78 ±0.09 2.97±0.35\nCoV oMix2 4.45±4.19 0.66 ±0.17 3.19±0.37\n†CoV oMix2 †is only pre-trained on the monologue data.\nC Ablation Studies\nTo better understand the design choices and training strategies that contribute to the performance of\nCoV oMix2, we conducted a series of ablation studies covering data mixing, training stages, and the\nhandling of silence tokens.\nC.1 Data Mixing Strategy\nWe examine how various combinations of training data affect performance. Table 4 presents results\non both monologue and dialogue test sets, comparing models trained on different subsets of the\ndata. We observe that in addition to dialogue podcast data, incorporating monologue podcast data\nsignificantly improves performance. Adding LibriTTS and LibriHeavy audiobook datasets provides\nfurther gains, with LibriTTS showing slightly better results due to its cleaner and more accurately\naligned transcriptions. Interestingly, simulated dialogue data (created by combining monologues\n7https://github.com/jzq2000/MoonCast\n8https://huggingface.co/spaces/sesame/csm-1b\n9https://huggingface.co/openai/whisper-large-v3\n14\n--- Page 15 ---\nwith a certail ratio of overlap or silence) does not consistently improve performance and may even\nslightly degrade it. This is likely due to distribution mismatch or artifacts introduced by the simulation\nprocess. However, simulated dialogues are still necessary to enable overlapping speech generation\ncapability, as ASR-transcribed real-world data rarely provides accurate overlapping timestamps.\nTable 4: Ablation Study of data\nTraining Data Monologue Dialogue\nIDDia Mono Simu LH LT WER↓SIM↑UTMOS ↑WER↓SA-WER ↓SA-SIM ↑UTMOS ↑\n1✓ ✗ ✗ ✗ ✗ 8.58 0.65 3.12 8.11 8.63 0.50 2.97\n2✓ ✓ ✗ ✗ ✗ 6.30 0.61 3.18 7.89 8.51 0.50 3.00\n3✓ ✓ ✓ ✗ ✗ 7.69 0.64 3.10 8.72 9.31 0.51 2.99\n4✓ ✓ ✓ ✓ ✗ 6.18 0.66 3.24 7.08 7.43 0.55 3.06\n5✓ ✓ ✓ ✗ ✓ 5.37 0.65 3.21 6.00 6.58 0.57 3.10\n6✓ ✓ ✗ ✗ ✓ 5.42 0.65 3.27 5.81 6.27 0.56 3.13\nC.2 Training Stages\nCoV oMix2 is trained using a two-stage training pipeline, with an optional third fine-tuning stage that\ncan further improve performance. To assess the benefit of the first and the third stage, we conduct\nan ablation study by eliminating the first stage and fine-tuning the model for 2,000 steps on just 20\nminutes of human-annotated dialogue data. Although this fine-tuning data is small in scale, it is\nhighly accurate and clean, in contrast to the ASR-transcribed training data.\nTable 5 shows results on a test set featuring the same two speakers as in the fine-tuning data. Models\ntrained without the pretraining perform very poorly, with WER and SA-WER exceeding 80%,\nhighlighting the critical importance of the first stage. Fine-tuning with just 2,000 steps on a small\namount of human-annotated dialogue data significantly improves accuracy, especially in WER and\nSA-WER. This suggests that even limited high-quality data can help correct transcription noise\nlearned from ASR-transcribed training sets.\nTable 5: Impact of training stages on a two-speaker test set\nPre-train Fine-tune WER↓SA-WER ↓SA-SIM ↑UTMOS ↑\n✗ ✗ 82.66 92.40 0.29 3.00\n✓ ✗ 5.67 6.21 0.53 3.48\n✓ ✓ 3.66 3.81 0.56 3.35\nC.3 Silence Token Representation\nOur input text streams include not only characters but also two types of special tokens: a continuation\ntoken [P]and a silence token [S]. We explore three options. 1) using [P]for both continuation\nand silence. 2) using a generic [S]for silence. 3) using speaker-aware silence tokens [S1]and[S2].\nTable 6 summarizes the results. We find that using a separate silence token [S]improves dialogue\naccuracy and controllability over using [P]alone. However, using speaker-aware silence tokens does\nnot provide significant benefits, and in some cases slightly degrades dialogue performance, possibly\ndue to over-specification.\nTable 6: Impact of different silence token representation\nMonologue Dialogue\nSilence Token WER↓SIM↑UTMOS ↑WER↓SA-WER ↓SA-SIM ↑UTMOS ↑\n[P] 5.26 0.64 3.15 6.45 7.09 0.55 3.08\n[S] 6.83 0.65 3.14 5.22 5.59 0.56 3.02\n[S1, S2] 5.37 0.65 3.21 6.00 6.58 0.57 3.10\n15\n--- Page 16 ---\nD Subjective Evaluation\nTable 7 shows the Comparative Mean Opinion Score (CMOS) Evaluation instruction. 15 professional\nlinguistic experts provide judges for this CMOS evaluation. They provide a rating of preference in\nthe (-3 to +3) range, given two audios with the same transcription and the speaker prompts.\nTable 7: Comparative Subjective Mean Opinion Score (CMOS) Evaluation Instructions\nInstruction\nThis is to compare two AI podcast audio. Listen to both audios as you are listening to a real\nhuman podcast and give your preference considering the following aspects.\n1. Speaker Attribution Accuracy : Is each utterance spoken by the correct speaker as indicated\nin the transcription? Does the voice match the intended identity?\n2. Speaker Turn Handling: Are speaker changes handled correctly and clearly? Does the\ntransition between speakers align with the dialogue flow?\n3. Speaker Consistency: Does each speaker maintain a consistent voice, tone, and speaking\nstyle throughout the clip?\n4. Interactivity and Fluency: Does the conversation sound natural and interactive? Are the\nresponses well-timed and appropriate, without awkward pauses or overlaps?\n5. Coherence with the Transcription: Does the spoken content accurately follow the given\ntranscription, including emotion, prosody, and speaking style?\nWhich one is better?\n-3: Dialogue 1 is much better\n-2: Dialogue 1 is better\n-1: Dialogue 1 is slightly better\n0: Can’t tell which is better\n1: Dialogue 2 is slightly better\n2: Dialogue 2 is better\n3: Dialogue 2 is much better\n16",
  "text_length": 52727
}