{
  "id": "http://arxiv.org/abs/2506.00868v1",
  "title": "Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\n  Person-Centric Visual and Conceptual Manipulations",
  "summary": "The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.",
  "authors": [
    "Parul Gupta",
    "Shreya Ghosh",
    "Tom Gedeon",
    "Thanh-Toan Do",
    "Abhinav Dhall"
  ],
  "published": "2025-06-01T07:17:16Z",
  "updated": "2025-06-01T07:17:16Z",
  "categories": [
    "cs.MM",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00868v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00868v1  [cs.MM]  1 Jun 2025Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\nPerson-Centric Visual and Conceptual Manipulations\nParul Gupta\nparul@monash.edu\nMonash University\nMelbourne, AustraliaShreya Ghosh\nshreya.ghosh@curtin.edu.au\nCurtin University\nPerth, AustraliaTom Gedeon\ntom.gedeon@curtin.edu.au\nCurtin University\nPerth, Australia\nThanh-Toan Do\ntoan.do@monash.edu\nMonash University\nMelbourne, AustraliaAbhinav Dhall\nabhinav.dhall@monash.edu\nMonash University\nMelbourne, Australia\nFigure 1: MultiFakeVerse. A brief overview of the proposed dataset. Here, we introduce subtle and profound person-centric deepfakes\ncovering person-level ,object-level ,scene-level ,(person+object)-level ,(person+scene)-level manipulations. Image best viewed in color.\nAbstract\nThe rapid advancement of GenAI technology over the past few years\nhas significantly contributed towards highly realistic deepfake con-\ntent generation. Despite ongoing efforts, the research community\nstill lacks a large-scale and reasoning capability driven deepfake\nbenchmark dataset specifically tailored for person-centric object,\ncontext and scene manipulations. In this paper, we address this\ngap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through ma-\nnipulation suggestions and image manipulations both derived from\nvision-language models (VLM). The VLM instructions were specif-\nically targeted towards modifications to individuals or contextual\nelements of a scene that influence human perception of importance,\nintent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and\nhuman-object interactions rather than synthetic or low-level identity\nswaps and region-specific edits that are common in existing datasets.\nOur experiments reveal that current state-of-the-art deepfake detec-\ntion models and human observers struggle to detect these subtle yet\nmeaningful manipulations. The code and dataset are available on\nGitHub.\nKeywords\nDatasets, Deepfake, Person-Centric, Detection1 Introduction\nThe last decade has witnessed an unprecedented surge in the capabil-\nities of content generation technologies powered by deep learning.\nModels trained on massive and diverse datasets now possess the abil-\nity to generate content that is often indistinguishable from authentic\ndata. This advancement spans across multiple modalities, includ-\ning text [ 4,33], images [ 15], videos [ 12], and audio [ 30]. Such\ngenerative models ranging from large language models (LLMs)\nto generative adversarial networks (GANs) and diffusion models\nhave transformed the creative and information landscapes. However,\nwhile these models present numerous opportunities in entertainment,\neducation, accessibility and design, they also present a potential\nnegative societal impact.\nIn recent years, numerous data sets have been proposed to fa-\ncilitate the training and evaluation of deepfake detectors [ 5‚Äì7,37].\nThese datasets often span different modalities and types of manipu-\nlations. For instance, visual-only datasets focus on face-swapping,\nfacial reenactment, or expression synthesis [ 22]; while audio-only\ndatasets target voice cloning or speech synthesis [ 8]; and audiovisual\ndatasets integrate both, enabling the study of multimodal manipula-\ntions [ 5]. These resources have proven invaluable for benchmarking\ndetection algorithms, enabling the deepfake research community\nto systematically compare methods and track progress. However,\ndespite these efforts, one of the most significant limitations is in\nterms of human-centric factors present in the image, specially from\nhuman-object and human scene interaction reasoning perspective\n(as shown in Fig. 1).\n--- Page 2 ---\nConference, June 2025, Washington, DC, USA Gupta et al.\nTable 1: Details for publicly available deepfake datasets in a\nchronologically ascending order.\nDataset Year Manipulation #Total\nMethod Content\nDFFD [31] 2019 GAN Face 299,039\nFakeSpotter [36] 2020 GAN Face 11,000\nFFHQ [16] 2020 GAN Face 70,000\nCNNSpot [37] 2020 GAN Face & Objects 724,000\nIMD2020 [24] 2020GAN Face, Objects105,000Inpainting & Scene\nOHImg [20] 2023 Diffusion Overhead 13,150\nArtiFact [28] 2023 Diffusion & GAN Objects 2,496,738\nAIGCD [45] 2023 GAN Objects 1,440,000\nGenImage [46] 2023 Diffusion & GAN Objects 2,681,167\nCiFAKE [2] 2024 CIFAR Objects 120,000\nM3Dsynth [47] 2024 Diffusion Medical 8,577\nSemiTruths [26] 2024 Diffusion Face, Objects 1,500,300\nSIDA [15] 2024 Diffusion Objects 300,000\nMultiFakeVerse 2025Vision-Language Face, Persons,845,286Models Objects, Scene, Text\nTo this end, we propose MultiFakeVerse dataset, a collection\nof synthetically manipulated images created using large Vision-\nlanguage models (VLMs), designed to explore perceptual modifi-\ncations within visual content. Unlike traditional deepfake datasets\nthat primarily focus on altering facial identities or facial expressions,\nMultiFakeVerse emphasizes targeted modifications to individuals or\ncontextual elements within a scene to influence human perception.\nThese alterations are carefully crafted to change the narrative, im-\nportance, or intent perceived by viewers, often without modifying\nthe core identity of subjects. This approach bridges the gap between\nface-based deepfakes and broader image semantics by focusing on\nsemantic and contextual manipulations that can impact how an im-\nage‚Äôs story or message is interpreted. Such modifications can include\nchanging background elements, repositioning objects, or altering\nactions to shift the scene‚Äôs overall meaning or emphasis. The purpose\nofMultiFakeVerse is to challenge detection methods and deepen un-\nderstanding of perceptual manipulation, as these subtle yet impactful\nchanges can be more challenging to identify than overt facial swaps.\nBy concentrating on perceptual cues rather than identity alterations,\nthe dataset provides a valuable resource for advancing research in\nrobust detection algorithms against manipulative visual content. The\nmain contributions of the paper are:\n‚Ä¢We propose MultiFakeVerse , a large-scale reasoning-driven human-\ncentric deepfake dataset for the task of deepfake detection. The\nproposed dataset mainly explore novel image level perceptual\nmodifications within visual content in terms of human gestures ,\nhuman-object interaction andhuman-scene interaction .\n‚Ä¢We propose a novel data generation pipeline employing novel\nLLM-based image perception manipulation strategies which in-\nclude human gestures ,human-object interaction andhuman-scene\ninteraction and incorporating the state-of-the-art (SOTA) in image\ngeneration.\n‚Ä¢We perform comprehensive analysis and benchmark the proposed\ndataset with SOTA deepfake detection methods.\n2 Related Work\nDeepfake Datasets. The current landscape of image-based deep-\nfake datasets is shown in Table 1. Most existing deepfake datasetsfocus primarily on person-level changes, where manipulations are\nconfined to a person‚Äôs face or facial expressions. A prominent exam-\nple is DFFD [ 31], which contains 300K samples, which are gener-\nated and/or edited facial images with GAN methods. Other promi-\nnent datasets, which have been commonly used are FFHQ [ 16]\nand FakeSpotter [ 36]. Moving beyond faces, OHImg [ 20] intro-\nduced a deepfake dataset focused on overhead aerial imagery, while\nM3Dsynth [ 47] addressed medical image based manipulations. More\nrecently, Huang et al. proposed the SIDA dataset [ 15], which lever-\nages vision-language models to identify objects in images and cap-\ntions, replacing them via inpainting with Stable Diffusion (e.g. cat\n‚Üídog). However, these datasets primarily focus on object- and\nscene-level manipulations and do not address person-level edits that\nalter the perceived meaning of an image.\nAnother interesting recent work, SemiTruths [ 26], proposed a\nlarge-scale dataset comprising varying levels of manipulation, focus-\ning on objects and scenes. Both inpainting (Stable Diffusion) and\nprompt-based (LLAMA-7B [ 32]) methods are used for introducing\nimage manipulations. The focus is mainly on objects and scenes\nwith the change criterion being change in perceptual meaning of the\nimage at varying levels. In contrast, our work focuses exclusively on\npeople in images. This focus is critical: studies show that deepfakes\ntargeting real individuals are shared and viewed up to six times more\noften than generic content [ 34], and that over 96% of deepfakes\nonline are non-consensual pornography [ 40]. This underscores the\nurgent societal impact of person-centered manipulations, an area\nwhere current datasets are lacking.\nExisting datasets predominantly focus on within-face manipula-\ntions or object-scene perturbations, which do not capture the com-\nplexity of real-world deepfakes. However, person-centered deep-\nfakes often involve diverse manipulations such as full-body ed-\nits, co-occurring people, context shifts, object edits and scene re-\ncompositions, which pose unique challenges for detection methods.\nTherefore, a dataset focusing on these person-centric manipulations\nis essential for advancing deepfakes detection systems.\nDeepfake Detection. Deepfake detection techniques are commonly\nframed as classification problems within a data-centric framework [ 29].\nThese approaches often rely on a range of neural network architec-\ntures, notably Convolutional Neural Networks (CNNs) [ 23] and\nTransformers [ 39], to identify unique artifacts introduced during\nimage based manipulation. A number of deepfake detection meth-\nods [ 15,21] have expanded beyond simple binary classification by\ndeveloping datasets with pixel-level annotations of tampered regions,\nthereby supporting both detection and localization tasks. However,\nsuch resources are predominantly centered on facial forgeries [ 21],\nwith limited availability of datasets targeting non-facial manipula-\ntions, and even fewer large-scale, publicly accessible datasets that\nreflect the diversity of scene content. To bridge these gaps, our work\nintroduces a comprehensive dataset featuring a wide range of manip-\nulations beyond facial edits, with a particular emphasis on realistic\nimage-based manipulation and localization.\nLLM Powered Reasoning for Generation. The emergence of dif-\nfusion models has significantly advanced the field of image edit-\ning/manipulations [ 35]. Recent progress in image inpainting, under\nboth text-guided and unguided settings, has enabled more precise and\nhigh-quality manipulations [ 35]. Text-conditioned inpainting [ 41]\nallows for detailed control over edits using natural language, while\n--- Page 3 ---\nMultiFakeVerse Dataset Conference, June 2025, Washington, DC, USA\nPIC\nPIPA\n PISC\nEMOTICReal ImagesManipulation\nsuggestions\nEdit\nInstructionsImage Editing\nVLM\n MultiFakeV erse Dataset\nManipulation\nmaskVLM - GPT/Gemini\nPrompt: Suggest\nmanipulations\nPrompt for\nPerceptual\nComparison\n1. Level of Changes\n2. Ethical Implication\n3. Perceptual Impact\nImage Caption\nShareGPT ChatGPT\nGemini ICEdit\n(a) Dataset generation pipeline.\n34.57 17.178.16\n21.463.261.992.19Person\nObject\nSceneNormalized Distribution of\ndifferent levels of changes in MultiFakeVerse (b) Different Manipulation levels.\nFigure 2: (a) Depicts the MultiFakeVerse dataset generation pipeline. For details please see Section 3. (b) Venn Diagram depicting the\nsemantic categorical-levels of manipulations in the MultiFakeVerse dataset.\nunconditioned methods offer strong baseline capabilities without ex-\nplicit guidance. Traditionally, inpainting techniques [ 15] rely on the\nuse of masks to specify regions for modification. In contrast, prompt-\nbased image editing [ 32] allows for targeted alterations driven solely\nby text inputs, eliminating the need for explicit masks. Frameworks\nsuch as LANCE [ 27] and InstructPix2Pix [ 3] harness this approach\nto automate image perturbation pipelines. LANCE [ 27], in particu-\nlar, combines large language models (LLMs) with image captioning\ntechniques to facilitate diverse, human-free image editing work-\nflows. Expanding on this line of work, our method uses GPT-Image-\n1 [25], Gemini-2.0-Flash-Image-Generation [ 1] and ICEdit [ 44] to\nsupport image manipulation with a wider spectrum of perturbation\nintensities, guided by definitions of semantic change. To achieve\nthis, we integrate models like Gemini-2.0-Flash [ 1], ChatGPT-4o-\nlatest [ 25] using prompt-based editing to generate precise, seman-\ntically meaningful image modifications. Multimodal models like\nGemini, ChatGPT-4o, and Icedit leverage both text and image inputs\nto enable precise, context-aware edits and have a natural advan-\ntage over text-driven models like LANCE and InstructPix2Pix in\nfine-grained and semantically consistent image manipulation tasks.\nWhile text-driven models are effective for stylistic transformations,\nmultimodal approaches offer broader capabilities and greater control\nover localized edits.\n3 MultiFakeVerse Dataset\nTo create the MultiFakeVerse dataset for person-centric manipula-\ntions, we use publicly available real image datasets involving humans\nin diverse environments. EMOTIC [ 17], PISC [ 18], PIPA [ 43] and\nPIC 2.0 [ 19]. Altogether, we use 86,952images from these datasets\nto create a total of 758,041fake images. This huge scale positions\nour proposed dataset as the most comprehensive reasoning-guided\ndeepfake benchmark. The dataset generation details are below:\n3.1 Data Generation Pipeline\n3.1.1 Curation of Person-centric edit instructions. As the first\nstep in our approach, we leverage the extensive visual understand-\ning capability of foundational VLMs (particularly Gemini-2.0-Flash\nand ChatGPT-4o-latest) to obtain six suitable edits for each image,\naimed towards altering the viewer‚Äôs perception of the most important\nperson in the image. To this end, we ask the VLM for minimal editsthat would make the person appear too naive ,proud ,remorseful ,\ninexperienced ,nonchalant , or some factual information depicted\nin the image would change. To ensure that the edits can be used\naccurately in the further stages, the output includes the referring\nexpression of the target of each modification along with the edit in-\nstruction. Note that referring expression is a widely explored domain\nin the community, which means a phrase which can disambiguate\nthe target in an image, e.g. for an image having two men sitting\non a desk, one talking on the phone and the other looking through\ndocuments, a suitable referring expression of the later would be the\nman on the left holding a piece of paper .\n3.1.2 VLM based Image perception Manipulation. In this step,\nfor each of the <referring expression, edit instruction >pair, we\nprompt a VLM to perform the edit on the image, while ensur-\ning that nothing else changes. We experiment with three different\nVLMs here - GPT-Image-1, Gemini-2.0-Flash-Image-Generation\nand ICEdit [44]. After observing 22K generated images, Gemini\nTable 2: Number of images in MultiFakeVerse.\nSubset Source #Real Images #Fake Images #Images\nTrainEmotic 15976 176052\n592,349PIC 2.0 8565 54320\nPIPA 20163 115730\nPISC 16677 184866\nValidationEmotic 2226 25132\n84,309PIC 2.0 1203 7450\nPIPA 2864 16564\nPISC 2327 26543\nTestEmotic 4453 50315\n168,628PIC 2.0 2406 15383\nPIPA 5730 32961\nPISC 4655 52725\nOverallEmotic 22655 251499\n845,286PIC 2.0 12174 77153\nPIPA 28757 165255\nPISC 23659 264134\n--- Page 4 ---\nConference, June 2025, Washington, DC, USA Gupta et al.\nPrompt 3.1: VLM based Image Perception Manipulation\nHuman: {INPUT image to VLM} Given the attached image, identify the most\nimportant person in this image, and suggest minimal modifications to the image to\nobtain each of the following effects:\n(1) the person you identified appears naive\n(2) the person you identified appears nonchalant\n(3) the person you identified appears proud\n(4) the person you identified appears remorseful\n(5) the person you identified appears inexperienced\n(6) some factual information depicted in the image changes\nThe possible change targets for the modifications are: the objects or text or humans\nin the image. When suggesting changes to text in the image, be specific about what\ntext is to be replaced and what text should be added instead. Give output as a valid\nJSON string in the following format:\n{‚ÄòMost Important Person‚Äô:<referring expression for most important person>\n‚ÄòEffects‚Äô:[{‚ÄòEffect‚Äô:<effect>, ‚ÄòChange Target‚Äô: <change target>, ‚ÄòExplanation‚Äô: [<re-\nferring expression for the change target>, <edit_instruction>]}]}.\nDo not include any other information in the response.\nHuman: {INPUT image to VLM} In this image, change {target} {edit_instruction}.\nDo not change anything else in the image.\nemerged as the VLM for highest quality manipulations, as its edits\nare the most coherent with the rest of the scene with no changes to\nthe untargeted regions of the input image.The ICEdit model often\nresults in easily identifiable fakes with significant artifacts, whereas\nGPT-image-1 tends to edit in a few cases, untargeted regions due to\nconstraints in the output aspect ratios (current options are: 1024√ó\n1024,1536√ó1024,1024√ó1536).\n3.2 Analyzing the Manipulated images\nTo further enrich our dataset, we perform the following analyses on\nthe generated images:\nRatio of edited area To obtain a measure of the spatial degree of\nmodification, we calculate the mean squared difference between the\npixel values of the original and altered images, threshold the differ-\nence to remove noise, and perform connected component analysis\nto obtain the masks of the edited regions. The ratio of edited area\nto the total image area is then plotted over the entire dataset, shown\nin Figure 3a, which displays a distribution spread out from 0 to 0.8,\nthus signifying the wide variety of edits present in our dataset.\nImage Captioning We use ShareGPT4V [ 10] VLM model to obtain\nrich captions of both the original as well as tampered images. ThesePrompt 3.2: VLM based Image Perception Analysis\nHuman: {EXAMPLE INPUT image and Manipulated image} Compare these two\nimages: the first is the original (Image A) and the second is its manipulated version\n(Image B). For each of the following categories, describe the changes observed, if\nany, and their potential perceptual impact on a human viewer. Finally, classify the\nchanges in Image B into person-level, person-object level or person-scene level,\ndepending upon whether a person has been changed, an object connected to the\nperson has been changed or a component of the scene away from the person has\nbeen changed respectively. Note that a manipulated image can have multiple types\nof changes, so output multiple labels for such image. Strictly use the following\nstructure in your response, and do not add anything else:\n1. Emotion/Affect: - Image A: [Describe emotion, mood, facial expression, atmo-\nsphere] - Image B: [Describe changes] - Perceptual Impact: [How might this affect\nviewer emotion or interpretation?]\n2. Identity & Character Perception: - Image A: [Describe apparent age, gender,\ntrustworthiness, etc.] - Image B: [Describe changes] - Perceptual Impact: [Does this\nshift how the person is perceived?]\n3. Social Signals & Status: - Image A: [Describe clothing, posture, social roles,\nproximity] - Image B: [Describe changes] - Perceptual Impact: [Do power dynamics\nor relationships change?]\n4. Scene Context & Narrative: - Image A: [Describe implied story or setting] -\nImage B: [Describe changes] - Perceptual Impact: [Does the story or situation\nchange?]\n5. Manipulation Intent: - Description: [What might be the intent behind the manipu-\nlation?] - Perceptual Impact: [Does the edit appear deceptive, persuasive, aesthetic,\netc.?]\n6. Ethical Implications: - Description: [Could the manipulation mislead or cause\nharm?] - Assessment: [Mild / Moderate / Severe ethical concern]\n7. Type of changes: - [Person level / Person-Object level / Person-Scene level]\nare then used to calculate the directional similarity [ 11] between the\nreal versus edited images and their corresponding captions as follows:\n(ùê∂ùêøùêºùëÉùëÇùëüùëñùëîùëñùëõùëéùëôùêºùëöùëéùëîùëí -ùê∂ùêøùêºùëÉùëáùëéùëöùëùùëíùëüùëíùëëùêºùëöùëéùëîùëí )¬∑(ùê∂ùêøùêºùëÉùëÇùëüùëñùëîùëñùëõùëéùëôùêºùëöùëéùëîùëíùê∂ùëéùëùùë°ùëñùëúùëõ\n-ùê∂ùêøùêºùëÉùëáùëéùëöùëùùëíùëüùëíùëëùêºùëöùëéùëîùëíùê∂ùëéùëùùë°ùëñùëúùëõ ). Particularly, we use Long-CLIP [ 42]\nto calculate the image and text embeddings. From this analysis, we\nfind that the maximum semantic change is observed in the object\ncategory, where the modification is around or on the person.\nThe next set of analyses are performed through the Gemini-2.0-\nFlash VLM to compare the real and manipulated images and measure\nthe perceptual changes for different concepts.\nLevel of manipulation performed We categorize the manipula-\ntions into Person-level, Object-level and Scene-level depending upon\nwhether the manipulation target is a person (facial expression, iden-\ntity, gaze, pose, or clothing), an object connected to a person (i.e. an\nobject in the foreground - appearance change, addition, removal etc.)\nor some component in the overall scene/background of the image\nrespectively. Since an image can have multiple levels of changes\ndepending upon the edit instruction, we display the distribution of\nthe these alteration levels in our dataset through a Venn diagram in\nFigure 2b. The distribution shows that the different levels of manipu-\nlations are widely distributed in our dataset with around one-third of\nthe edits being person-only, around one-fifth being scene-only and\none-sixth being object-only.\nPerceptual Impact of the edit on the viewer To obtain a measure of\nthe change in the viewer perception as a result of the image tamper-\nings, we consider the impact on the following six perceptual factors:\n(A) the viewer‚Äôs emotion or interpretation (B) the perceived identity\nor character of the person in the image (C) the perceived power\ndynamics/relationships (D) the perceived scene narrative (E) the\npossible intent of manipulation , and (F) the ethical implications of\nthe tamperings. We prompt the Gemini-2.0-Flash VLM to describe\nthese impacts, whose instruction details are shown in Prompt 3.2.\nThe word clouds of the changes based on responses obtained from\n--- Page 5 ---\nMultiFakeVerse Dataset Conference, June 2025, Washington, DC, USA\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000052/uni00000049/uni00000003/uni00000028/uni00000047/uni0000004c/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000055/uni00000048/uni00000044/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056\n(a) Ratio of Edited Regions.\n (b) Emotion Perception.\n(c) Scene Context and Narrative\nshift.\n(d) Perceived identity of depicted\nindividual.\n(e) Intent of Manipulations.\n(f) Power dynamics and relation-\nships.\nFigure 3: The visualizations illustrate the characteristics and\nimpact of fake images in the MultiFakeVerse dataset. (a) shows\nthe distribution of the ratio of pixels edited in fake images. (b‚Äìf)\ndisplay word maps highlighting the perceptual changes observed\nby viewers, derived using VLM analysis. These word maps cap-\nture how image manipulations affect the following attributes:\nperceived emotion, scene context and narrative shift, perceived\nidentity, intent of manipulation, and impact on power dynamics\nand relationships.\nthe VLM are shown in Figure 3. The first word map, corresponding\nto the changes in emotion perception (Figure 3b) features words\nsuch as significant, joyful, engaging, approachable, facial suggest-\ning that our semantic modifications subtly influence the emotional\nperception. In case of scene context and narrative shifts (Fig. 3c), the\nword cloud contains terms such as focused, professional, potential,\ndifferent , indicating plausible alterations to the conveyed story or\nsetting due to the corresponding edits. Fig. 3d, which illustrates the\nword map for shift in the perceived identity of depicted persons\nincludes terms such as playful, younger, vulnerable implying that\nour manipulations are able to influence the individual‚Äôs identity. The\nword map representing the intent behind the manipulations in Fig.\n3e shows deceptive ,persuasive andaesthetic as the most prominent\nwords, reflecting a tendency for the manipulations to be viewed as\nintentional acts of persuasion or deception. According to the word\nmap in Fig. 3f, though the power dynamics and relationships remain\nstable for many cases, the presence of words like altered, dynamic,\nformal, competitive highlights that such dynamics do shift notice-\nably in a subset of the images. In Figure 4, we show some sample\nresponses obtained for two such edits, with the different perceptual\nimpacts highlighted. The VLM responses show that around 81% of\nthe edits have mild ethical implications, while 14.2% have moderate\nand 0.3% of the edits have severe ethical implications in our dataset.Table 3: Visual quality of MultiFakeVerse. Quality of the gener-\nated images in terms of PSNR, SSIM and FID.\nDataset PSNR(‚Üë)SSIM(‚Üë)FID(‚Üì)\nMultiFakeVerse 66.30 0.5774 3.30\nQuality of generations. To evaluate the visual quality of MultiFake-\nVerse, we used the peak signal-to-noise ratio (PSNR), structural sim-\nilarity index (SSIM) [ 38] and Fr√©chet inception distance (FID) [ 14]\nmetrics as shown in Table 3. SSIM value 0 indicates no similarity\nbetween the compared images, while 1 indicates perfect similarity,\ntherefore, our value of 0.5774 is indicative of the targeted edits which\nensure that the untargeted regions remain as close as possible to the\noriginal image. Similarly, a low FID value of 3.30 indicates that\nthe quality and diversity of the generated fakes is excellent. A high\nPSNR value of 66.30 dB is indicative of good image quality.\n3.2.1 User Survey. To investigate how well humans can identify\ndeepfakes in MultiFakeVerse, we conducted a user study with 18\nparticipants.1We selected 50 random samples (25 real and 25 fake),\nrepresenting a range of modification levels. Each participant was\nasked to classify the images as real or fake and for the images\nthey identified as fake, to specify the manipulation level(s). The\nhuman accuracy of classifying an image as real or fake came out\nas 61.67%, i.e. more than one-thirds of the times, the users tend to\nmisclassify the images. The class-wise average F1 score was 61.14%.\nAnalyzing the human predictions of manipulation levels for the fake\nimages, the average intersection over union between the predicted\nand actual manipulation levels was found to be 24.96%. This shows\nthat it is non-trivial for human observers to identify the regions of\nmanipulations in our dataset.\n3.2.2 Computational Cost. A total of 845,286 API calls were\nmade to both Gemini and GPT, respectively, for edit suggestions,\nresulting in an overall cost of ‚àº1000 USD. For image generation\nthrough Gemini API, the total cost was 2867 USD. The cost for the\nGPT-Image-1 model based images was 200 USD and ICEdit images\nwere generated on one Nvidia A6000 GPU in 24 hours.\n4 Benchmarks and Experiments\nThis section outlines the benchmark protocol for MultiFakeVerse\nalong with the used evaluation metrics. The goal is to detect and\nlocalize image based manipulations.\nData Partitioning and Evaluation Metrics: We split the dataset\ninto train ,validation , and testsets as follows: first we randomly\nselect 70% of the real images as train set, 10% real images as vali-\ndation set and remaining 20% as test set. Next we add the images\ngenerated corresponding to each of the real images in the same set\nas the real image. We evaluate detection using image-level accu-\nracy and F1 scores. For forgery localization, we use Area Under the\nCurve (AUC), F1 scores and Intersection over Union (IoU).\nDetection Evaluation. We compare MultiFakeVerse against sev-\neral SOTA deepfake detection methods on the test set, including\nCnnSpot [ 37], AntifakePrompt [ 9], TruFor [ 13] and VLM based\nSIDA [ 15]. To ensure a fair comparison, we first evaluate these mod-\nels on our dataset using their original pre-trained weights (zero-shot\n1All procedures in this study were conducted in accordance with Monash University\nHuman Research Ethics Committee approval for Project ID 47707.\n--- Page 6 ---\nConference, June 2025, Washington, DC, USA Gupta et al.\nFigure 4: Analyzing the perceptual impact of manipulations in images. The edited regions are highlighted by yellow boxes. The analysis\ncovers changes in attributes such as perceived emotion, identity, and ethical implications.\nTable 4: Benchmarking of MultiFakeVerse dataset on Deepfake detection in zero-shot and supervised finetuning setting. The numbers\nin bracket indicate the changes after supervised finetuning.\nMethod Year Real Fake Overall\nAcc (‚Üë) F1 (‚Üë) Acc (‚Üë) F1 (‚Üë) Acc (‚Üë) F1 (‚Üë)\nCnnSpot [37] 2021 100.00 (6.16‚Üì) 19.00 (69.88‚Üë)0.00 (97.97‚Üë) 0.00 (98.62‚Üë)50.02 (45.88‚Üë) 9.54 (84.21‚Üë)\nTrufor [13] 2023 84.00 18.52 15.27 26.07 49.64 22.30\nAntiFakePrompt [9] 2024 63.30 30.47 70.45 80.63 66.87 55.55\nSIDA-13B [15] 2024 67.40(23.07‚Üë) 21.30(64.09‚Üë)44.55(52.94‚Üë) 60.08(38.09‚Üë)55.97(38.01‚Üë) 40.69(51.09‚Üë)\nsetting), and then retrain two of them (CNNSpot and SIDA) with our\ntrain dataset to assess performance improvements. Table 4 demon-\nstrates that these models trained on earlier inpainting-based fakes\nstruggle to identify our VLM-Editing based forgeries, particularly,\nCNNSpot tends to classify almost all the images as real. AntiFake-\nPrompt has the best zero-shot performance with 66.87% average\nclass-wise accuracy and 55.55% F1 score. After finetuning on our\ntrain set, we observe a performance improvement in both CNNSpot\nand SIDA-13B, with CNNSpot surpassing SIDA-13B in terms of\nboth average class-wise accuracy (by 1.92%) as well as F1-Score\n(by 1.97%).\nLocalization Evaluation. We evaluate the localization performance\nof SIDA-13B [ 15] model on our test set, using the perturbation\nmasks obtained in Section 3.2, both using their pre-trained weights\nas well as after finetuning on our dataset. In zero-shot setting, the\nmodel has an IoU of 13.10, F1-Score 19.92 and AUC 14.06. After\nfinetuning on our train dataset, the localization performance of the\nSIDA-13B model remains at an IoU of 24.74, F1-Score 39.40 and\nAUC 37.53, signifying that the model struggles to accurately identify\nthe manipulated image regions. Thus, there is room for further im-\nprovement in the forgery localization methods, with MultiFakeVerse\nas an ideal benchmark.5 Conclusion\nThis paper introduces MultiFakeVerse, the largest image-based dataset\nfor spatial deepfake localization. A thorough benchmarking using\nSOTA detection and localization methods reveals that VLM-based\nmanipulations are predominantly unidentifiable for both humans as\nwell as detection models solely trained on the existing inpainting-\nbased datasets; underscoring the increased complexity and realism\nof MultiFakeVerse. These results demonstrate that the proposed\ndataset is a valuable resource for advancing the development of\nnext-generation deepfake localization techniques.\nLimitation. Like other deepfake datasets, MultiFakeVerse presents\nan imbalance between the number of real and fake images, which\nmay affect training dynamics and evaluation outcomes.\nBroader Impact With its diverse and content-rich manipulations,\nMultiFakeVerse is expected to foster the development of more robust\nand generalizable deepfake detection and localization models beyond\nfacial and object manipulation, especially in image-based settings\nthat reflect real-world usage scenarios.\nEthics statement We acknowledge potential ethical concerns related\nto the misuse of manipulated images, such as unauthorized use of\nidentities or image-based misinformation. To mitigate these risks,\nMultiFakeVerse is distributed under a strict research-only license.\nAccess to the dataset requires agreement to an end-user license\nagreement (EULA) that limits usage to non-commercial, academic\nresearch.\n--- Page 7 ---\nMultiFakeVerse Dataset Conference, June 2025, Washington, DC, USA\nReferences\n[1]Rohan Anil and et al. 2023. Gemini: A Family of Highly Capable Multimodal\nModels. arXiv preprint arXiv:2312.11805 (2023). https://arxiv.org/abs/2312.\n11805\n[2]Jordan J Bird and Ahmad Lotfi. 2024. Cifake: Image classification and explainable\nidentification of ai-generated synthetic images. IEEE Access 12 (2024), 15642‚Äì\n15650.\n[3]Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. InstructPix2Pix:\nLearning to Follow Image Editing Instructions. arXiv preprint arXiv:2211.09800\n(2022).\n[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in\nNeural Information Processing Systems , V ol. 33. Curran Associates, Inc., 1877‚Äì\n1901.\n[5]Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav\nDhall, Tom Gedeon, and Kalin Stefanov. 2024. A V-Deepfake1M: A Large-\nScale LLM-Driven Audio-Visual Deepfake Dataset. In Proceedings of the 32nd\nACM International Conference on Multimedia (Melbourne VIC, Australia) (MM\n‚Äô24). Association for Computing Machinery, New York, NY , USA, 7414‚Äì7423.\ndoi:10.1145/3664647.3680795\n[6]Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, and\nMunawar Hayat. 2023. Glitch in the matrix: A large scale benchmark for content\ndriven audio‚Äìvisual forgery detection and localization. Computer Vision and\nImage Understanding 236 (2023), 103818.\n[7]Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. 2022. Do You\nReally Mean That? Content Driven Audio-Visual Deepfake Dataset and Multi-\nmodal Method for Temporal Forgery Localization. In 2022 International Confer-\nence on Digital Image Computing: Techniques and Applications (DICTA) . 1‚Äì10.\ndoi:10.1109/DICTA56598.2022.10034605\n[8]Edresson Casanova, Christopher Shulby, Eren G√∂lge, Nicolas Michael M√ºller,\nFrederico Santos De Oliveira, Arnaldo Candido Jr., Anderson Da Silva Soares,\nSandra Maria Aluisio, and Moacir Antonelli Ponti. 2021. SC-GlowTTS: An\nEfficient Zero-Shot Multi-Speaker Text-To-Speech Model. In Interspeech 2021 .\nISCA, 3645‚Äì3649.\n[9]You-Ming Chang, Chen Yeh, Wei-Chen Chiu, and Ning Yu. 2024. Antifake-\nPrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors.\narXiv:2310.17419 [cs.CV] https://arxiv.org/abs/2310.17419\n[10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng\nZhao, and Dahua Lin. 2024. Sharegpt4v: Improving large multi-modal models\nwith better captions. In European Conference on Computer Vision . Springer,\n370‚Äì387.\n[11] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and\nDaniel Cohen-Or. 2022. StyleGAN-NADA: CLIP-guided domain adaptation of\nimage generators. ACM Trans. Graph. 41, 4, Article 141 (July 2022), 13 pages.\ndoi:10.1145/3528223.3530164\n[12] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs,\nJia-Bin Huang, and Devi Parikh. 2022. Long video generation with time-agnostic\nvqgan and time-sensitive transformer. In European Conference on Computer\nVision . Springer, 102‚Äì118.\n[13] Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, and Luisa\nVerdoliva. 2023. TruFor: Leveraging All-Round Clues for Trustworthy Image\nForgery Detection and Localization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) . 20606‚Äì20615.\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule\nConverge to a Local Nash Equilibrium. In Advances in Neural Informa-\ntion Processing Systems , V ol. 30. https://papers.nips.cc/paper/2017/hash/\n8a1d694707eb0fefe65871369074926d-Abstract.html\n[15] Zhenglin Huang, Jinwei Hu, Xiangtai Li, Yiwei He, Xingyu Zhao, Bei Peng,\nBaoyuan Wu, Xiaowei Huang, and Guangliang Cheng. 2024. SIDA: Social Media\nImage Deepfake Detection, Localization and Explanation with Large Multimodal\nModel. arXiv preprint arXiv:2412.04292 (2024).\n[16] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator ar-\nchitecture for generative adversarial networks. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition . 4401‚Äì4410.\n[17] Ronak Kosti, Jose M Alvarez, Adria Recasens, and Agata Lapedriza. 2019. Con-\ntext based emotion recognition using emotic dataset. IEEE transactions on pattern\nanalysis and machine intelligence 42, 11 (2019), 2755‚Äì2766.\n[18] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. 2017. Dual-\nglance model for deciphering social relationships. In Proceedings of the IEEE\ninternational conference on computer vision . 2650‚Äì2659.[19] Si Liu, Zitian Wang, Yulu Gao, Lejian Ren, Yue Liao, Guanghui Ren, Bo Li,\nand Shuicheng Yan. 2022. Human-Centric Relation Segmentation: Dataset and\nSolution. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 9\n(2022), 4987‚Äì5001. doi:10.1109/TPAMI.2021.3075846\n[20] Brandon B May, Kirill Trapeznikov, Shengbang Fang, and Matthew Stamm. 2023.\nComprehensive dataset of synthetic and manipulated overhead imagery for de-\nvelopment and evaluation of forensic tools. In Proceedings of the 2023 ACM\nWorkshop on Information Hiding and Multimedia Security . 145‚Äì150.\n[21] Changtao Miao, Qi Chu, Zhentao Tan, Zhenchao Jin, Tao Gong, Wanyi Zhuang,\nYue Wu, Bin Liu, Honggang Hu, and Nenghai Yu. 2023. Multi-spectral Class\nCenter Network for Face Manipulation Detection and Localization. arXiv preprint\narXiv:2305.10794 (2023). https://arxiv.org/abs/2305.10794\n[22] Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa,\nand Richa Singh. 2023. DF-Platter: Multi-Face Heterogeneous Deepfake Dataset.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) . 9739‚Äì9748.\n[23] Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh\nNguyen, Thien Huynh-The, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet\nPham, and Cuong M. Nguyen. 2019. Deep Learning for Deepfakes Creation and\nDetection: A Survey. arXiv preprint arXiv:1909.11573 (2019). https://arxiv.org/\nabs/1909.11573\n[24] Adam Novozamsky, Babak Mahdian, and Stanislav Saic. 2020. IMD2020: A large-\nscale annotated dataset tailored for detecting manipulated images. In Proceedings\nof the IEEE/CVF winter conference on applications of computer vision workshops .\n71‚Äì80.\n[25] OpenAI. 2024. GPT-4o System Card. https://arxiv.org/abs/2410.21276. Accessed:\n2025-05-31.\n[26] Anisha Pal, Julia Kruk, Mansi Phute, Manognya Bhattaram, Diyi Yang,\nDuen Horng Chau, and Judy Hoffman. 2024. Semi-Truths: A Large-Scale Dataset\nof AI-Augmented Images for Evaluating Robustness of AI-Generated Image\ndetectors. Advances in Neural Information Processing Systems 37 (2024), 118025‚Äì\n118051.\n[27] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman.\n2023. LANCE: Stress-testing Visual Models by Generating Language-guided\nCounterfactual Images. In Neural Information Processing Systems (NeurIPS) .\n[28] Md Awsafur Rahman, Bishmoy Paul, Najibul Haque Sarker, Zaber Ibn Abdul\nHakim, and Shaikh Anowarul Fattah. 2023. Artifact: A large-scale dataset with\nartificial and factual images for generalizable and robust synthetic image detection.\nIn2023 IEEE International Conference on Image Processing (ICIP) . IEEE, 2200‚Äì\n2204.\n[29] Vishal Kumar Sharma, Rakesh Garg, and Quentin Caudron. 2024. A system-\natic literature review on deepfake detection techniques. Multimedia Tools and\nApplications (2024). doi:10.1007/s11042-024-19906-1\n[30] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng\nZhao, and Jiang Bian. 2023. Naturalspeech 2: Latent diffusion models are natural\nand zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116\n(2023).\n[31] Joel Stehouwer, Hao Dang, Feng Liu, Xiaoming Liu, and Anil Jain. 2019. On the\ndetection of digital face manipulation. arXiv (2019), arXiv‚Äì1910.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al .2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs].\n[34] Soroush V osoughi, Deb Roy, and Sinan Aral. 2018. The Spread of True and False\nNews Online. Science 359, 6380 (2018), 1146‚Äì1151. https://news.mit.edu/2018/\nstudy-twitter-false-news-travels-faster-true-stories-0308 Accessed: 2024-05-30.\n[35] Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Xiaoming Wei, and Enhua Wu. 2025.\nImage Editing with Diffusion Models: A Survey. arXiv preprint arXiv:2504.13226\n(2025). https://arxiv.org/abs/2504.13226\n[36] Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian Wang,\nand Yang Liu. 2019. Fakespotter: A simple yet robust baseline for spotting\nai-synthesized fake faces. arXiv preprint arXiv:1909.06122 (2019).\n--- Page 8 ---\nConference, June 2025, Washington, DC, USA Gupta et al.\n[37] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A\nEfros. 2020. CNN-generated images are surprisingly easy to spot... for now.\nInProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition . 8695‚Äì8704.\n[38] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. 2004. Image quality\nassessment: from error visibility to structural similarity. IEEE Transactions on\nImage Processing 13, 4 (April 2004), 600‚Äì612.\n[39] Zhikan Wang, Zhongyao Cheng, Jiajie Xiong, Xun Xu, Tianrui Li, Bharadwaj\nVeeravalli, and Xulei Yang. 2024. A Timely Survey on Vision Transformer for\nDeepfake Detection. arXiv preprint arXiv:2405.08463 (2024). https://arxiv.org/\nabs/2405.08463\n[40] Claire Wardle. 2019. The Disturbing World of Deepfake Pornography. WIRED\n(October 2019). https://www.wired.com/story/deepfakes-pornography Accessed:\n2024-05-30.\n[41] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. 2023. Smart-\nBrush: Text and Shape Guided Object Inpainting with Diffusion Model. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) . 22428‚Äì22437. doi:10.1109/CVPR52729.2023.02148\n[42] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 2024.\nLong-CLIP: Unlocking the Long-Text Capability of CLIP. arXiv preprint\narXiv:2403.15378 (2024).[43] Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, and Lubomir Bourdev.\n2015. Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues.\narXiv:1501.05703 [cs.CV] https://arxiv.org/abs/1501.05703\n[44] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. 2025. In-Context\nEdit: Enabling Instructional Image Editing with In-Context Generation in Large\nScale Diffusion Transformer. arXiv:2504.20690 [cs.CV] https://arxiv.org/abs/\n2504.20690\n[45] Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang. 2023. Rich and poor\ntexture contrast: A simple yet effective approach for ai-generated image detection.\nCoRR (2023).\n[46] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li,\nZhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. 2023. Genimage: A million-scale\nbenchmark for detecting ai-generated image. Advances in Neural Information\nProcessing Systems 36 (2023), 77771‚Äì77782.\n[47] Giada Zingarini, Davide Cozzolino, Riccardo Corvi, Giovanni Poggi, and Luisa\nVerdoliva. 2024. M3Dsynth: A dataset of medical 3D images with AI-generated\nlocal manipulations. In ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 13176‚Äì13180.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "text_length": 45335
}