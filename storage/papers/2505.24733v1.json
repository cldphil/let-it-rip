{
  "id": "http://arxiv.org/abs/2505.24733v1",
  "title": "DreamDance: Animating Character Art via Inpainting Stable Gaussian\n  Worlds",
  "summary": "This paper presents DreamDance, a novel character art animation framework\ncapable of producing stable, consistent character and scene motion conditioned\non precise camera trajectories. To achieve this, we re-formulate the animation\ntask as two inpainting-based steps: Camera-aware Scene Inpainting and\nPose-aware Video Inpainting. The first step leverages a pre-trained image\ninpainting model to generate multi-view scene images from the reference art and\noptimizes a stable large-scale Gaussian field, which enables coarse background\nvideo rendering with camera trajectories. However, the rendered video is rough\nand only conveys scene motion. To resolve this, the second step trains a\npose-aware video inpainting model that injects the dynamic character into the\nscene video while enhancing background quality. Specifically, this model is a\nDiT-based video generation model with a gating strategy that adaptively\nintegrates the character's appearance and pose information into the base\nbackground video. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of DreamDance, producing high-quality and\nconsistent character animations with remarkable camera dynamics.",
  "authors": [
    "Jiaxu Zhang",
    "Xianfang Zeng",
    "Xin Chen",
    "Wei Zuo",
    "Gang Yu",
    "Guosheng Lin",
    "Zhigang Tu"
  ],
  "published": "2025-05-30T15:54:34Z",
  "updated": "2025-05-30T15:54:34Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24733v1",
  "full_text": "arXiv:2505.24733v1 [cs.CV] 30 May 2025 DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds Jiaxu Zhang1,2,3Xianfang Zeng3‚Ä†Xin Chen4 Wei Zuo3Gang Yu3‚Ä°Guosheng Lin2Zhigang Tu1‚Ä° 1Wuhan University2Nanyang Technological University3StepFun4ByteDance Project page: https://kebii.github.io/DreamDance Abstract This paper presents DreamDance, a novel character art an- imation framework capable of producing stable, consistent character and scene motion conditioned on precise camera trajectories. To achieve this, we re-formulate the animation task as two inpainting-based steps: Camera-aware Scene Inpainting and Pose-aware Video Inpainting. The first step leverages a pre-trained image inpainting model to gener- ate multi-view scene images from the reference art and op- timizes a stable large-scale Gaussian field, which enables coarse background video rendering with camera trajecto- ries. However, the rendered video is rough and only con- veys scene motion. To resolve this, the second step trains a pose-aware video inpainting model that injects the dy- namic character into the scene video while enhancing back- ground quality. Specifically, this model is a DiT-based video generation model with a gating strategy that adaptively in- tegrates the character‚Äôs appearance and pose information into the base background video. Through extensive experi- ments, we demonstrate the effectiveness and generalizabil- ity of DreamDance, producing high-quality and consistent character animations with remarkable camera dynamics. 1. Introduction Animating character art is a fundamental challenge in the 2D animation industry, with a wide range of applications in film, game, and digital design. However, traditional 2D ani- mation is a labor-intensive and time-consuming process that requires expertise in professional software such as MMD  and Live2D. Recently, human video generation methods, particularly MikuDance, have revolutionized this challenging task, making it accessible to non-experts. Derived from previous methods [2, 13, 59], MikuDance per- forms two strategies to animate character art using driving videos, as illustrated in Figure 1. The first strategy is motion modeling, which uses pose ‚Ä†Xianfang Zeng is the project leader. ‚Ä°Corresponding authors: skicy@outlook.com, tuzhigang@whu.edu.cn 3D Gaussian Field Stable Scene Reconstruction Dynamic Character AnimationPoses Backgrounds ReferenceSplattingCamera- aware Scene Inpainting Pose -aware Video Inpainting+Previous Methods DreamDance (Ours) 3D-agnostic Motion Modeling UNet- based Image Animation Stable 3D Scene vs3D-agnostic Guidance Video-based Inpainting vsImage -based Animation Figure 1. We propose DreamDance, a novel paradigm that re- formulates the character art animation task into two inpainting- based steps: Camera-aware Scene Inpainting for stable scene re- construction and Pose-aware Video Inpainting for dynamic char- acter animation. image sequences to drive characters and 2D scene flow to guide backgrounds. Similar to MikuDance, existing meth- ods incorporate other motion guidance, such as optical flow [24, 42] and camera parameters [31, 44], to represent global camera movements. However, this motion guidance is en- tirely 3D-agnostic and struggles to provide consistent scene context, leading to scene distortion during large-scale cam- era movements. This inconsistency arises from the implicit inpainting process, where the scene dynamics exceed the area covered by the reference, requiring the model to hal- lucinate missing regions. Therefore, it is crucial to explore 3D-aware scene modeling for consistent camera control. The second strategy is image animation, which utilizes the UNet-based Stable Diffusion model [29, 30] to ani- mate the reference character art using mixed motion guid- ance. Additionally, some recent methods have developed incremental pose encoders [55, 59] and reference adapters [5, 40], using SVD  as a base model to achieve hu- man image animation. However, due to the limitations of base model capacity and the ambiguity of mixed guidance, the animation results from these methods exhibit significant temporal jitters in both characters and backgrounds. There- 1 fore, it is crucial to introduce a more powerful video founda- tion model and redefine the animation process with explicit contextual scene guidance. Unlike MikuDance and other relevant methods, we pro- pose DreamDance, a new paradigm for animating in-the- wild character art. As illustrated in Figure 1, DreamDance reformulates the motion modeling and image animation processes into two inpainting-based steps: Camera-aware Scene Inpainting and Pose-aware Video Inpainting. These two components work synergistically to generate consis- tent, high-quality animation sequences from the reference character art and driving videos. Camera-aware Scene Inpainting is presented for stable scene reconstruction. Inspired by existing 3D Gaussian methods [6, 39, 48], we leverage a pre-trained image in- painting model  to generate multi-view images and con- struct a large-scale 3D Gaussian field from the reference character art. This process utilizes both a pre-defined spi- ral camera trajectory and an extracted custom camera tra- jectory. A coarse background video is then rendered along the custom trajectory by splatting the stable Gaussian field. This background video contains consistent scene motion in- formation and serves as a rough yet foundational video for the later character animation stage. Pose-aware Video Inpainting is proposed for injecting dynamic character animation into the coarse scene video. Based on a video generation model, i.e., CogVideoX, we train a gated video inpainting model to refine the coarse backgrounds and inject the reference character according to the pose guidance. The gating strategy is designed to adaptively incorporate both the character‚Äôs appearance and poses based on the denoising time step, ensuring character and background consistency throughout the animation. Ad- ditionally, we exploit a 3D Gaussian-free training approach to train the dynamic video inpainting model directly using background-degraded video datasets. By leveraging this new paradigm, DreamDance animates diverse character art with stable scenes and precise camera movements, generating spatio-temporally consistent anima- tions. We evaluate DreamDance using a wide range of ref- erence character art and driving videos. Both qualitative and quantitative results demonstrate that DreamDance can generate high-quality animations, particularly with flexible and coherent scene dynamics. Contributions of DreamDance are listed in three folds: ‚Ä¢ Camera-aware Scene Inpainting and Pose-aware Video Inpainting are proposed to re-formulate the charac- ter art animation task, enabling explicit and consistent scene context modeling. ‚Ä¢ A gating strategy is introduced into a fundamental video generation model to achieve adaptive video in- painting, enabling high-dynamic animation of charac- ter art within a stable Gaussian scene.‚Ä¢ Extensive experiments demonstrate the effectiveness and generalizability of DreamDance, achieving supe- rior animation quality over state-of-the-art methods. 2. Related Work 2D character animation provides a vibrant platform for storytelling but has long been a challenge in the animation industry. Some previous methods construct animated 3D characters from reference images and re-project them into 2D videos [27, 33, 50, 58]. These methods require precise geometry, rigging, and motion editing, making them hard to automate, and often resulting in a loss of the original 2D style. Recent approaches like Textoon  and AniClipart  aim to generate animatable 2D characters using im- age and video generation models. However, they still re- quire significant manual work, and the character‚Äôs motion freedom is limited. In contrast, MikuDance  directly generates 2D animation through an image animation model, offering a promising solution, but it faces issues with scene distortion and artifacts due to its 3D-agnostic approach. De- rived from MikuDance, we propose DreamDance, which introduces two inpainting steps for 3D context-aware and consistent character art animation in stable Gaussian scenes. Human image animation has gained popularity in recent years [2, 20], with many methods building on pre-trained image and video generation models [3, 29]. For exam- ple, Animate Anyone  uses a reference-denoising UNet structure and a temporal module from AnimateDiff  to improve video consistency. DisCo  separates hu- man subjects from backgrounds, allowing for more flexi- ble combinations. MimicMotion  introduces a regional loss based on pose confidence to enhance human fidelity. Animate-X  extends the pipeline to anthropomorphic characters. However, these methods mainly focus on char- acter actions and overlook scene dynamics. Moving forward, Human4DiT  and HumanVid  address camera movements by incorporating a camera en- coder. However, camera guidance alone cannot handle large-scale scene dynamics, as it is difficult for the model to consistently fill in missing areas. MIMO  and Animate Anyone 2  capture environmental representations from the driving video and restore the backgrounds in the ani- mation, but they focus on different goals than 2D character animation, where the scene comes from the reference char- acter art. In this work, we reconstruct 3D Gaussian scenes by inpainting reference character art to support precise cam- era dynamics, and use an MM-DiT-based video foundation model to generate high-quality character animations. Video inpainting typically focuses on two main tasks: ob- ject removal and text-guided inpainting. Traditional meth- ods, like E2FGVI  and FGT, use optical flow- guided feature propagation to reconstruct missing areas 2 Image Inpainting Face Pose Hand Pose Inpainting Mask 3D V AE Decoder 3D V AE Encoder MM- DiTNoiseC Project InProject InProject IntStable Scene Reconstruction Dynamic Character Animation 3D Gaussian Gates Background Video Inpainting Guidance (Reference & Poses) ùú∑ùú∑ùú∏ùú∏Latent PoseLatent BackgroundsLatent Referenceùíôùíôùë°ùë° Body Pose ùùìùùìFigure 2. Illustration of our DreamDance. The reference character art is decomposed into foreground and background layers. The background image is used to reconstruct a stable 3D Gaussian scene through a wrap-and-inpaint scheme, enabling coarse background video rendering based on custom camera trajectories. The gated MM-DiT model then inpaints the background video based on the foreground character and the driving poses, generating dynamic character animations. with coherent content. More recently, models like A VID  and CoCoCo  have integrated pre-trained genera- tive inpainting models  with motion modules for text- guided video inpainting. Unlike these approaches, our video inpainting step focuses on filling in the coarse back- ground with animated characters guided by pose videos. To achieve this, we propose a gating strategy within the DiT [26, 47] model to adaptively integrate the character‚Äôs ap- pearance and poses, enabling dynamic character animation. 3D Gaussian Splatting  utilizes the concept of Gaus- sian splats combined with spherical harmonics and opacity to represent 3D scenes. Later work incorporates image in- painting to generate multi-view images and reconstruct 3D Gaussian fields from a single image [6, 19, 39, 48, 61]. In- spired by these approaches, we reconstruct stable Gaussian scenes from the reference character art and render coarse background videos to improve scene consistency. 3. Method As illustrated in Figure 2, given a character art Iand a driv- ing video V, the goal of DreamDance is to animate the im- ageIbased on the human and camera motion in the video V. Specifically, we utilize Xpose  to separately extract the pose sequences of the human body, face, and hand, and employ DPVO  to extract the camera poses {pc l}L l=1, pc‚ààRL√ó7fromV.Lindicates the sequence length. The character and the background are segmented from Iusing BiRefNet. Next, we reconstruct a 3D Gaussian field from the reference background through multi-view image inpainting, using both a pre-defined spiral camera trajectory and the extracted camera trajectory. Then, a coarse back- ground video is rendered by splitting the 3D Gaussian field according to the extracted camera poses, and an inpainting mask is generated from the driving pose sequence. Finally, all the references and guidance are processed using the pre- trained V AE and input into the gated video inpainting model for dynamic character animation. 3.1. Preliminaries Diffusion Denoising Probabilistic Models. Diffusion- based generative models [11, 34] represent the data distri-bution by constructing a Markov chain. Given an input data distribution x0, the forward process applies a Markov nois- ing process of Tsteps on x0to obtain {xt}T t=0: q(xt|xt‚àí1) =N(‚àöŒ±txt‚àí1,(1‚àíŒ±t)I), (1) where Œ±t‚àà(0,1)are constant hyper-parameters. When Œ±tis small enough, xT‚àº N (0,I). The reverse process takes a noisier data distribution xtand generates a less noisy distribution xt‚àí1using a noise predictor, which is trained with the simple loss function: Lsimple:=Eœµ,t,c\u0002 ‚à•œµ‚àíœµŒ∏(xt, t, c)‚à•2 2\u0003, (2) where œµis the Gaussian noise. cis the text condition. œµŒ∏(¬∑) is the trainable noise predictor. In this work, we utilize the pre-trained CogVideoX, an MM-DiT-based video diffusion model, as the base model to achieve pose-aware video in- painting in DreamDance. 3D Gaussain Splatting (3DGS). Prior works [15, 62] pro- pose to represent a 3D scene as a set of scaled 3D Gaus- sian primitives {Gk|k= 1,..., K}and render scene images using volume splitting. The geometry of each scaled 3D Gaussian Gkis parameterized by an opacity Œ±k‚àà[0,1], center ok‚ààR3√ó1, and covariance matrix Œ£k‚ààR3√ó3. To render an image for a given camera defined by rota- tionR‚ààR3√ó3and translation t‚ààR3, the 3D Gaussians are first transformed into camera coordinates: o‚Ä≤ k=Rpk+t,Œ£‚Ä≤ k=RŒ£kRT. (3) Then, they are projected to ray space via a local affine trans- formation. Finally, 3DGS utilizes spherical harmonics to model view-dependent color ckand renders images via al- pha blending according to the primitive‚Äôs depth order: c(x) =KX k=1ckŒ±kG2D k(x)k‚àí1Y j=1\u0000 1‚àíŒ±jG2D j(x)\u0001, (4) where G2Dis the scaled 2D Gaussian, obtained by remov- ing the third row and column of the ray space covariance matrix. In this work, we reconstruct a 3D Gaussian field by inpainting the reference image and then rendering the coarse background video using volume splitting. 3 3D Gaussian ReconstructionCustom Camera Trajectory Scene Image Inpainting Splatting Coarse Background VideoSpiral TrajectoryCusto m Traject ory + Figure 3. Camera-aware Scene Inpainting for stable scene re- construction. We use both the pre-defined spiral camera trajectory and the custom camera trajectory to reconstruct a 3D Gaussian field via the warp-and-inpaint scheme. 3.2. Stable Scene Reconstruction Existing 3D-agnostic motion guidance makes consistent background generation during large-scale camera move- ments an ill-posed problem. Therefore, we reconstruct stable Gaussian scenes to facilitate character art animation. Camera-aware Scene Inpainting. As illustrated in Fig- ure 3, inspired by the warp-and-inpaint scheme [32, 39], we use a pre-defined spiral camera trajectory to reconstruct the 3D scene. Firstly, at the starting point of the camera trajectory, we use LLaV A  to generate detailed descrip- tions of the reference background, and use Fooocus  to inpaint the empty regions left by the removed character. Afterward, we estimate the global depth map on this com- plete background using DepthPro. Next, as the camera moves along the spiral trajectory, we warp the background image to each new viewpoint using its depth map, and then fill the empty regions through image inpainting. After this warp-and-inpaint process, we obtain a set of RGBD images, which are then used to train a stable 3D Gaussian field. The spiral trajectory can be formulated as: P(t) =Ô£Æ Ô£∞r¬∑sin(2œÄt)¬∑cos(2œÄt) r¬∑sin(2œÄt)¬∑sin(2œÄt) ‚àísin(2œÄt)Ô£π Ô£ª, R(t) =Ô£Æ Ô£∞norm (o‚àípt)√ó U U norm (o‚àípt)Ô£π Ô£ª‚àí1,(5) where Pis the position and Ris the rotation of the cam- era,t‚àà[0,1]is the camera time step, and ris the radius of the field. Uis the up vector [0,1,0]andois the cam- era looking point. This well-defined spiral trajectory ef- fectively covers most of the missing regions and generates comprehensive multi-view images. However, the custom camera movements, provided by the user or extracted from the driving video, may differ significantly from the spiral trajectory. Therefore, we expand the 3D Gaussian field ac- cording to the custom trajectory using the warp-and-inpaint strategy again. Before this process, the camera rotations (a) HunyuanVideo (b) OmniHuman -1 C Patchify ùú∏ùú∏ùíôùíôùíïùíïùùìùùì Patchify C ùú∏ùú∏ùíôùíôùíïùíïùùìùùì (c) DreamDanceùëîùëîùëùùëù(ùë°ùë°) ùëîùëîc(ùë°ùë°)ùëìùëìùëùùëù(¬∑) ùëìùëìùëèùëè(¬∑) ùëìùëìùëêùëê(¬∑) Patchify C‚Ä¶ C ùùìùùì ùíôùíôùíïùíï ùú∏ùú∏ ùú∑ùú∑Figure 4. The gating strategy in our MM-DiT model and its com- parison with the mainstream condition incorporation methods. are standardized based on the first camera frame to ensure consistency with the spiral trajectory. Finally, based on the custom camera trajectory, a background video is rendered through volume splatting at each camera step. The reason we do not directly use the custom camera tra- jectory to reconstruct the 3D scene is that it may be exces- sively dynamic, potentially resulting in a discontinuous and unstable 3D scene. Additionally, since the reconstructed 3D scene often suffers from fidelity issues, the rendered back- ground video may contain blurring, distortions, and black voids. To address these challenges, we introduce a pose- aware video inpainting strategy in the next step, which not only integrates the animated character but also refines the coarse background video for improved visual quality. 3.3. Dynamic Character Animation Based on the coarse background video generated in the first stage, we implement pose-aware video inpainting to achieve dynamic character animation. Previous UNet-based reference-denoising architectures lack the ability to model video coherence effectively. Therefore, we introduce an MM-DiT-based video foundation model along with a gating strategy to enable pose-guided character integration and ensure temporal consistency. Pose-aware Video Inpainting. As illustrated in Figure 2, we divide the input references and guidance into three sets. The first is the background set, which includes the coarse background video, and an inpainting mask video generated based on the region of the driving pose. The second is the pose set, consisting of the driving face, hand, and body pose videos. The final set is the reference character. All elements in these three sets are encoded by the pre-trained 3D V AE, and then stacked along the channel dimension to obtain the latent background Œ≤, pose Œ≥, and character œï. Next, the latent background is concatenated with the base latent noise xtalong the channel dimension. Then, three convolutional layers are applied to project each of the three latent features to the same channels, respectively. Existing DiT-based models for character image anima- tion typically use simple feature concatenation or addition to inject the latent reference and guidance [16, 20]. How- 4 ever, unlike these methods, the goal of our model is to inpaint the base background video using the posed char- acter. Obviously, character appearance and pose informa- tion should be prioritized during the initial denoising steps, while at later steps, the model should focus more on overall video refinement. To achieve this, we propose two denois- ing step-based gates that adaptively inject the latent char- acter and pose into the base latent noise according to the denoising step t. Each gate consists of a Linear layer fol- lowed by a tanh activation function. This gating strategy can be formulated as: x‚Ä≤ t=fb([Œ≤,xt]) +tanh (gp(t))¬∑fp(Œ≥) +tanh (gc(t))¬∑fc(œï),(6) where f(¬∑)denotes the convolutional Project-In layers, and g(¬∑)represents the Linear layers. A detailed comparison of the structural differences between existing methods and our gating strategy is shown in Figure 4. Finally, we use MM- DiT from CogVideoX  to perform the diffusion denois- ing steps. Additionally, the reference character is embed- ded using the CLIP image encoder  and serves as the text hidden states in the cross-attention operations of MM- DiT. This process is commonly used in existing work and is therefore omitted from Figure 2. The resulting latent out- put is decoded through the 3D V AE Decoder to generate the character art animation. 3D Gaussian-free Training Approach. We perform super- vised fine-tuning to train the gated video inpainting model in DreamDance, starting from the image-to-video model CogVideoX-5B. Given that constructing 3D Gaussian fields is time-consuming, we inpaint the character region and apply random down-sampling to the original video back- ground to simulate the coarse video rendered from the 3D Gaussian scene. The down-sampling approach includes adding black blocks, introducing noise, blurring, and apply- ing random perspective transformations. Additionally, fol- lowing, we randomly generate stylized pair-wise im- ages by concatenating the initial frames along the spatial dimension and use the depth and edge-controlled SDXL- Neta model  to transfer the art style. Then, the stylized frames are repeated along the temporal dimension to con- struct a fake video for training. To simulate the inference process, in which the reference character art is irrelevant to the driving pose, we randomly select reference frames that are not involved in the target video clips. During the training, we found that the supervision of the background region in the video was too strong, caus- ing the bodies of the inpainted characters to be incom- plete. To address this issue, we use inpainting masks to re- weight the loss and fill the character bounding boxes of the background videos with black in the early training stages, thereby enhancing dynamic character learning and elimi- nating the model from overfitting to the backgrounds.4. Experiments Datasets. To train DreamDance, we collected an MMD video dataset comprising 4,800 animations created by artists, which is comparable to that of MikuDance. We split these videos into approximately 150,000 clips, which to- gether include over 14.8 million frames. For the quanti- tative evaluation, we used 100 MMD videos that were not included in the training set, with their first frames serving as reference images. For the qualitative evaluation, all charac- ter art was randomly generated using SDXL-Neta, and the driving videos were not seen during training. Implementation details. We implement DreamDance us- ing the code base of VistaDream  and Finetrainers. Experiments are conducted on 32 NVIDIA A800 GPUs. During training, the videos are center-cropped and resized to a resolution of 768√ó768, and the length is sampled to 48 frames. Training is conducted for 60,000 steps with a batch size of 32. The learning rates are set to 1e-4, and the dropout ratio for the character and pose guidance is set to 0.1. During inference, we use a DDIM sampler for 50 de- noising steps. We adopt the temporal aggregation method described in  to generate long videos. The code will be released in the final version. Evaluation metrics. Following MikuDance, we evalu- ate the results from two aspects: image and video. To assess image quality, we report frame-wise FID, SSIM, LISPIS, PSNR, and and L1. For video quality, we concatenate every consecutive 16 frames to form a sample, from which we report FID-VID  and FVD. 4.1. Qualitative Results Comparison with image animation baselines, including MikuDance, as well as recent human animation meth- ods, such as Animate Anyone (AniAny), UniAnimate, MimicMotion, and DisCo. The results in Figure 5 demonstrate that AniAny, Uni- Animate, MimicMotion, and DisCo struggle with a strong shape prior on the human body, which leads to substan- tial character distortion and fidelity issues in their outputs. Moreover, the backgrounds in their generated videos remain nearly static or excessively blurry, resulting in monotonous and visually flat effects. While DisCo employs an inde- pendent ControlNet to process the backgrounds, it suffers from scene collapse when animating character art. Miku- Dance shows significant improvements in animating char- acter art, but the backgrounds still exhibit inconsistencies when confronted with large-scale camera movements. No- tably, thanks to the explicit reconstruction of the 3D scene and the MM-DiT-based inpainting paradigm, our Dream- Dance achieves precise camera control, coherent scene dy- namics, and consistent animation generation, producing high-quality and vivid 2D animation results. 5 Reference DreamDance (Ours) MimicMotion (ArXiv24) AniAny (CVPR24) MikuDance (ArXiv24) DisCo (CVPR24) UniAnimate (ArXiv24) Figure 5. Comparison with image animation baselines. The red arrows represent the approximate direction of camera movement, while the circles highlight significant correspondences. The black boxes contain the reconstructed 3D Gaussian scenes of our DreamDance. Reference DreamDance (Ours) MotionShop -2 MIMO (ArXiv24) Driving Video Figure 6. Comparison with character replacement baselines. MIMO and MotionShop-2 only support full-body reference images. Comparison with character replacement baselines. One valuable application of DreamDance is its ability to directly replace humans in driving videos with reference charac- ters. We compare it with MotionShop-2  and MIMO, which are 3D and 2D-based methods, respectively. As shown in Figure 6, MotionShop-2 exhibits noticeable character distortion due to the unresolved challenges of 3D character reconstruction, while MIMO fails to effectively preserve the attributes of the reference characters. Addi- tionally, both MotionShop-2 and MIMO only support full- body character images. In contrast, our DreamDance seam- lessly integrates the 2D character into the driving video without disrupting the harmony of the scene. This applica- tion opens up broad prospects for DreamDance in creating flexible video content. High-dynamic and precise camera control. A key high- light of DreamDance is its ability to animate characterswith high-dynamic camera movements while maintaining scene coherence through precise camera control. Distinct from MikuDance, which relies on 2D flow for scene mo- tion guidance, and AniAny, which always outputs static backgrounds, DreamDance explicitly reconstructs stable 3D scenes. This approach avoids the context ambiguity that typically arises in the scene inpainting process, ensuring more consistent and immersive character animation with high-dynamic motion, as demonstrated in Figure 7. Ablation study. In Figure 8 and Figure 10, we conduct ab- lation experiments to verify the key designs of our Dream- Dance, which include the camera-aware scene inpainting method, the MM-DiT-based model architecture, and the gating strategy for video inpainting. To demonstrate the necessity of the camera-aware scene inpainting in DreamDance, we implemented a baseline (w/o 3DGS) that animates the character art directly using the 6 AniAny (CVPR24) MikuDance (ArXiv24) Reference Camera Trajectory Time Camera Inconsistency Character Inconsistency Static backgroundsCharacter Inconsistency Zoom In Zoom Out Figure 7. High-dynamic and precise camera control of our DreamDance. MikuDance exhibits inconsistencies due to its 3D-agnostic motion guidance, while AniAny produces static backgrounds. In contrast, DreamDance generates coherent and vivid animations. w/o 3DGS Dream Dance w/ Unet w/o ùí¢ùí¢ Figure 8. Ablation experiments. ‚Äúw/o 3DGS‚Äù, ‚Äúw/ UNet‚Äù, and ‚Äúw/o G‚Äù are defined in Section 4.1. Figure 9. Generalizability on various scenes (left) and characters (right). Please see our demo video for a clearer understanding. MM-DiT model, bypassing the 3D Gaussian reconstruction process. The results in Figure 8 indicate that this model fails to generate consistent backgrounds due to the implicit in- painting of unknown regions, leading to flickering and blur- ring in the generated video. Moreover, as shown in Figure 10, reconstructing the 3D scene without the spiral camera trajectory may result in discontinuous Gaussian fields. To evaluate the MM-DiT-based model architecture, in Figure 8, we implemented a UNet-based video inpainting model. Since this model has limited capabilities in spatio- temporal modeling and a smaller pre-training scale com- pared to MM-DiT, its results are inferior to those of Dream- Dance. To assess the effectiveness of the gating strategy in our pose-aware video inpainting step, we conducted an experiment in which the latents were directly added to the noise without re-weighting them using the gates (w/o G). This ablation model was trained on the same dataset with the same training settings as DreamDance. However, its re- sults exhibited an underfitting phenomenon, with the videos appearing blurry during high-dynamic motion. Moreover,as shown in Figure 10, our gated video inpainting model also enhances the quality of the backgrounds provided by the rendered 3D scene for character art animation. We visualize the values of the adaptive gates across the denoising time steps in the right part of Figure 10. As the denoising steps progress from 0 to 50, both the character gate and the pose gate decrease. This supports our con- jecture that the model requires more information about the character‚Äôs appearance and pose during the early denoising steps, whereas in the later stages, it prioritizes optimizing the quality of the existing latent videos. Generalizability on various scenes and characters. Be- yond reconstructing the scene from the reference character art, DreamDance supports scene reconstruction from cus- tom images and the animation of the reference character across various scenes. As shown in the left part of Figure 9, DreamDance effectively integrates animated characters into diverse 3D Gaussian scenes. On the other hand, Dream- Dance is also capable of handling multiple characters in a wide range of art styles, including but not limited to cellu- 7 Table 1. Quantitative comparisons with baselines and ablative experiments. ‚ÄòUNet‚Äô and ‚Äòw/o G‚Äô are defined in Section 4.1. ‚ÄòForeground-only‚Äô refers to replacing the reference backgrounds with white images and evaluating only the character animations. The best results are highlighted in bold, and the second-best are underlined. DreamDance achieves superior results across most metrics. Methods SSIM ‚ÜëPSNR ‚ÜëLISPIS ‚ÜìL1E‚àí05 ‚Üì FID‚Üì FID-VID ‚Üì FVD‚Üì Foreground-onlyFID‚Üì FVD‚ÜìSVDUniAnimate  0.417 12.074 0.571 7.930 47.328 40.924 882.245 29.818 381.485 MimicMotion  0.325 12.264 0.600 9.313 60.210 44.517 903.674 30.125 407.856SDDisCo  0.313 10.732 0.615 9.248 59.221 46.852 923.921 31.221 564.892 AniAny  0.488 12.530 0.548 7.307 43.945 38.179 846.414 27.927 326.842 MikuDance  0.576 14.592 0.493 5.726 24.597 22.868 502.380 14.835 194.124 DreamDance UNet 0.612 16.721 0.383 4.622 32.923 19.387 477.235 15.227 221.126DiTDreamDance w/o G 0.626 17.135 0.378 4.601 30.794 17.198 441.057 16.831 217.946 DreamDance (Ours) 0.699 17.964 0.355 4.109 29.659 16.411 430.136 16.102 188.852 1 11 21 31 41 w/o SpiralOurs Gate Value 0.00.20.40.6 Denoising Step ùë°ùë° Character Gate Pose Gate0.659 0.359 0.116 0.049Before Video Inpainting After Video Inpainting Figure 10. Ablations on spiral trajectory (left), Scene enhance- ment (middle), and visualization of the gate values (right). loid, antiquity, and line sketch, as demonstrated in the right part of Figure 9. This high level of flexibility opens up vast possibilities for 2D animation applications. 4.2. Quantitative Results Table 1 presents quantitative comparisons and the results demonstrate that DreamDance achieves state-of-the-art per- formance across most image and video metrics. Addition- ally, the ablation results confirm the effectiveness of the key design elements in the dynamic character animation stage of DreamDance. To isolate character quality from background effects, we conducted evaluations on foreground-only re- sults, as shown in the right part of Table 1. In this setup, we replaced the backgrounds of reference character art with white images for baseline evaluation. For DreamDance, we provided white background videos for character animation inpainting. Under these conditions, DreamDance consis- tently achieved the best video temporal quality. User study. We invited 50 volunteers to evaluate Dream- Dance against baseline methods on two tasks: image anima- tion and character replacement. Each participant reviewed 15 videos, each containing one pose guidance and three or four anonymous animation results. They ranked the results based on character quality, background quality, and tempo- ral consistency. After filtering out abnormal responses, the average rankings are summarized in Figure 11. For image animation, DreamDance significantly outperforms baseline methods in background and temporal quality, with over 1.502.001.862.001.88 1.892.50 2.122.25 1.79 1.431.62 1.572.07 2.053.313.173.39 3.31 3.33 2.93 DreamDance MikuDance AniAny UniAnimate MINO MotionShop2Character Quality Background Quality Temporal Quality Character Quality Background Quality Temporal Quality01234 0123Average Rank Average RankFigure 11. User Study for image animation (top) and character replacement (bottom). The smaller value means the better quality. 77.27% of users preferring its animations. For character re- placement, DreamDance achieves the highest character and temporal quality, favored by more than 59.09% of users. 5. Conclusions In this work, we propose DreamDance, a new inpainting- based pipeline for animating character art. DreamDance in- tegrates two key techniques: Camera-aware Scene Inpaint- ing and Pose-aware Video Inpainting. Camera-aware Scene Inpainting reconstructs stable Gaussian scenes, allowing for the rendering of coarse yet context-coherent background videos. Pose-aware Video Inpainting then adaptively incor- porates pose-guided characters into the background, refin- ing the video quality and ensuring consistent animation for stylized character art. Extensive experiments demonstrate that DreamDance outperforms baseline methods, achieving state-of-the-art results in character art animation. Limitations. We acknowledge that some generated anima- tions exhibit artifacts, particularly in character details such as the hands and clothing. This issue arises from the limita- tions of the datasets and the base model. Additionally, ex- tracting precise camera parameters from real-world videos remains a challenge, often requiring manual adjustments. 8 References  Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chel- lappa, and Hans Peter Graf. Conditional gan with discrimi- native filter generation for text-to-video synthesis. In IJCAI, page 2, 2019. 5  Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, and Fahad Shahbaz Khan. Person image synthesis via de- noising diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5968‚Äì5976, 2023. 1, 2  Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2  Aleksei Bochkovskii, Ama ¬®el Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than a second. arXiv preprint arXiv:2410.02073, 2024. 4  Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, et al. X-dyna: Expressive dynamic hu- man image animation. arXiv preprint arXiv:2501.10021, 2025. 1  Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free gen- eration of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 2, 3  Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text- to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representa- tions, 2024. 2  Chao He, Jianqiang Ren, and Liefeng Bo. Textoon: Generat- ing vivid 2d cartoon characters from text descriptions. arXiv preprint arXiv:2501.10020, 2025. 2  Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. Advances in neural information processing systems, 30, 2017. 5  Yu Higuchi. Mikumikudance. https://sites. google.com/view/evpvp. 1, 4, 5  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. Advances in neural information processing systems, 33:6840‚Äì6851, 2020. 3  Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 2366‚Äì2369. IEEE, 2010. 5  Li Hu. Animate anyone: Consistent and controllable image- to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8153‚Äì8163, 2024. 1, 2, 5, 8 Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image ani- mation with environment affordance, 2025. 2  Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¬®uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139‚Äì1, 2023. 3  Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 4  Neta.art Lab. neta-art-xl-1.0. https://huggingface. co/neta-art/neta-art-xl-1.0. 5  Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. Towards an end-to-end framework for flow-guided video inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17562‚Äì17571, 2022. 2  Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N Pla- taniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Nav- igating 3d scenes from a single image. arXiv preprint arXiv:2412.12091, 2024. 3  Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 2, 4  Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 4  Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. 2, 6  Tetsuya Nakajo. Live2d. https://www.live2d.com/. 1  Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In European Con- ference on Computer Vision, pages 111‚Äì128. Springer, 2024. 1  Sayak Paul. Finetrainers. https://github.com/a- r-r-o-w/finetrainers. 5  William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 4195‚Äì4205, 2023. 3  Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, et al. Anigs: Animatable gaussian avatar from a single image with inconsistent gaussian reconstruction. arXiv preprint arXiv:2412.02684, 2024. 2  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 9 Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748‚Äì8763. PMLR, 2021. 5  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¬®orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022. 1, 2, 3  Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical image computing and computer-assisted intervention‚ÄìMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234‚Äì241. Springer, 2015. 1  Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: Free-view human video generation with 4d diffusion transformer. arXiv preprint arXiv:2405.17405, 2024. 1, 2  Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ra- mamoorthi. Realmdreamer: Text-driven 3d scene gener- ation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 4  Harrison Jesse Smith, Qingyuan Zheng, Yifei Li, Somya Jain, and Jessica K Hodgins. A method for animating chil- dren‚Äôs drawings of the human figure. ACM Transactions on Graphics, 42(3):1‚Äì15, 2023. 2  Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International confer- ence on machine learning, pages 2256‚Äì2265. PMLR, 2015. 3  Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image ani- mation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. 2  Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch vi- sual odometry. Advances in Neural Information Processing Systems, 36, 2024. 3  Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448‚Äì458, 2023. 5  Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To- wards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5  Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: Sampling multi- view consistent images for single-view scene reconstruction. arXiv preprint arXiv:2410.16892, 2024. 2, 3, 4, 5  Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung- Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9326‚Äì9336, 2024. 1, 2, 5, 8 Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion mod- els for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. 5, 8  Yaohui Wang, Xin Ma, Xinyuan Chen, Cunjian Chen, An- titza Dantcheva, Bo Dai, and Yu Qiao. Leo: Generative latent image animator for human video synthesis. Interna- tional Journal of Computer Vision, pages 1‚Äì13, 2024. 1  Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600‚Äì612, 2004. 5  Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, et al. Humanvid: Demystifying training data for camera-controllable human image animation. arXiv preprint arXiv:2407.17438, 2024. 1, 2  Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Ani- clipart: Clipart animation with text-to-video priors. Interna- tional Journal of Computer Vision, pages 1‚Äì17, 2024. 2  Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Xpose: Detecting any keypoints. arXiv preprint arXiv:2310.08530, 2023. 3  Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao- han Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5  Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from a single image. arXiv preprint arXiv:2406.09394, 2024. 2, 3  Junfei Zhang, Xiaodan Ye, Chao Xu, Feng Wang, Qing Ran, Kejie Qiu, Guangyuan Wang, Jianfeng Luo, Junyao Wu, Gang Cheng, Zilong Dong, and Liefeng Bo. Motionshop- 2.https://aigc3d.github.io/motionshop/. 6  Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiao- hang Zhan, YU Gang, and Ying Shan. Tapmo: Shape- aware motion generation of skeleton-free characters. In The Twelfth International Conference on Learning Representa- tions, 2024. 2  Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, and Zhigang Tu. Mikudance: Animating character art with mixed motion dynamics. arXiv preprint arXiv:2411.08656, 2024. 1, 2, 8  Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided transformer for video inpainting. In European Conference on Computer Vision, pages 74‚Äì90. Springer, 2022. 2  Lvming Zhang. Fooocus. https://github.com/ lllyasviel/Fooocus. 2, 4  Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 586‚Äì595, 2018. 5 10  Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmo- tion: High-quality human motion video generation with confidence-aware pose guidance, 2024. 1, 2, 5, 8  Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with dif- fusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7162‚Äì 7172, 2024. 3  Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral refer- ence for high-resolution dichotomous image segmentation. arXiv preprint arXiv:2401.03407, 2024. 3  Jie Zhou, Chufeng Xiao, Miu-Ling Lam, and Hongbo Fu. Drawingspinup: 3d animation from single character draw- ings. In SIGGRAPH Asia 2024 Conference Papers, pages 1‚Äì10, 2024. 2  Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 1  Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. arXiv preprint arXiv:2403.12035, 2024. 3  Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10324‚Äì10335, 2024. 3  Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visu- alization, 2001. VIS‚Äô01., pages 29‚Äì538. IEEE, 2001. 3 11",
  "text_length": 48576
}