{
  "id": "http://arxiv.org/abs/2506.00895v1",
  "title": "State-Covering Trajectory Stitching for Diffusion Planners",
  "summary": "Diffusion-based generative models are emerging as powerful tools for\nlong-horizon planning in reinforcement learning (RL), particularly with offline\ndatasets. However, their performance is fundamentally limited by the quality\nand diversity of training data. This often restricts their generalization to\ntasks outside their training distribution or longer planning horizons. To\novercome this challenge, we propose State-Covering Trajectory Stitching\n(SCoTS), a novel reward-free trajectory augmentation method that incrementally\nstitches together short trajectory segments, systematically generating diverse\nand extended trajectories. SCoTS first learns a temporal distance-preserving\nlatent representation that captures the underlying temporal structure of the\nenvironment, then iteratively stitches trajectory segments guided by\ndirectional exploration and novelty to effectively cover and expand this latent\nspace. We demonstrate that SCoTS significantly improves the performance and\ngeneralization capabilities of diffusion planners on offline goal-conditioned\nbenchmarks requiring stitching and long-horizon reasoning. Furthermore,\naugmented trajectories generated by SCoTS significantly improve the performance\nof widely used offline goal-conditioned RL algorithms across diverse\nenvironments.",
  "authors": [
    "Kyowoon Lee",
    "Jaesik Choi"
  ],
  "published": "2025-06-01T08:32:22Z",
  "updated": "2025-06-01T08:32:22Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00895v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00895v1  [cs.LG]  1 Jun 2025State-Covering Trajectory Stitching\nfor Diffusion Planners\nKyowoon Lee\nKAIST\nleekwoon@kaist.ac.krJaesik Choi\nKAIST, INEEJI\njaesik.choi@kaist.ac.kr\nAbstract\nDiffusion-based generative models are emerging as powerful tools for long-horizon\nplanning in reinforcement learning (RL), particularly with offline datasets. How-\never, their performance is fundamentally limited by the quality and diversity of\ntraining data. This often restricts their generalization to tasks outside their training\ndistribution or longer planning horizons. To overcome this challenge, we propose\nState-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory aug-\nmentation method that incrementally stitches together short trajectory segments,\nsystematically generating diverse and extended trajectories. SCoTS first learns\na temporal distance-preserving latent representation that captures the underlying\ntemporal structure of the environment, then iteratively stitches trajectory segments\nguided by directional exploration and novelty to effectively cover and expand this\nlatent space. We demonstrate that SCoTS significantly improves the performance\nand generalization capabilities of diffusion planners on offline goal-conditioned\nbenchmarks requiring stitching and long-horizon reasoning. Furthermore, aug-\nmented trajectories generated by SCoTS significantly improve the performance of\nwidely used offline goal-conditioned RL algorithms across diverse environments.\n1 Introduction\nIn many real-world applications, agents must plan over hundreds of steps, often receiving sparse\nor delayed feedback until they reach a distant goal. Perfect knowledge of the environment allows\npowerful planners like MPC (Tassa et al., 2012) and MCTS (Silver et al., 2016, 2017) to excel.\nHowever, most real-world tasks instead require learning environment dynamics from data. Model-\nbased reinforcement learning (MBRL) (Sutton, 2018) constructs such world models, offering sample-\nefficient learning and improved generalization (Ha & Schmidhuber, 2018; Hafner et al., 2019; Kaiser\net al., 2020). However, autoregressive predictions from learned models accumulate small errors into\na cascade of inaccuracies. This compounding error can cause planners to exploit model inaccuracies\nand generate trajectories that are suboptimal or even physically infeasible, especially in long-horizon\ntasks (Talvitie, 2014; Asadi et al., 2018; Janner et al., 2019; V oelcker et al., 2022; Chen et al., 2024a).\nTo address these limitations, diffusion planners (Janner et al., 2022; Ajay et al., 2023; Liang et al.,\n2023; Chen et al., 2024c) have recently emerged as a promising alternative for trajectory generation\nin sequential decision-making. Instead of rolling out one step at a time, diffusion planners treat each\ntrajectory as a single high-dimensional sample, learning a denoising process that transforms noise\ndrawn from a simple prior into trajectories that match the target distribution (Ho et al., 2020; Song\net al., 2021). By operating on entire trajectories simultaneously, these methods inherently prevent\nthe compounding of prediction errors that undermine autoregressive dynamics models. Moreover,\nthe generative nature of diffusion models allows for flexible conditioning and guidance mechanisms,\nenabling the synthesis of plans with properties like reaching specific goals or maximizing expected\nreturns (Dhariwal & Nichol, 2021).\nPreprint. Under review.\n--- Page 2 ---\n(a) Offline Data (b) HD (c) Ours\nFigure 1: Improved generalization with SCoTS.\n(a) Examples from the training dataset, illustrating\nlimited coverage. (b) Plans generated by Hierar-\nchical Diffuser (HD) (Chen et al., 2024c), which\nfail to generalize well to these out-of-distribution\ntasks due to insufficient coverage of the training\ndata. (c) Plans generated by HD trained on SCoTS-\naugmented data, demonstrating significantly im-\nproved trajectory stitching capability and general-\nization to unseen tasks. Each color corresponds to\none of 10 plans generated by the planner.Despite these advantages, the effectiveness of\ndiffusion planners remains fundamentally lim-\nited by the quality, diversity, and coverage of the\noffline training data. First, their effective plan-\nning horizon is inherently coupled to the maxi-\nmum trajectory length observed during training,\nmaking it challenging to generate coherent plans\nthat significantly exceed this length. Second,\ntheir generalization capability is often confined\nto the specific types of trajectories and transi-\ntions represented in the training data. For in-\nstance, if the dataset predominantly features cer-\ntain movement patterns, the planner may strug-\ngle to synthesize solutions for novel tasks re-\nquiring different compositions of behaviors (as\nillustrated in Figure 1). While exhaustively col-\nlecting data for all conceivable scenarios could\nmitigate this, such an approach is prohibitively\nexpensive. Trajectory stitching (Ziebart et al.,\n2008) offers a promising alternative by com-\nposing novel, longer sequences from existing\nshort segments. However, existing methods rely\nheavily on extrinsic rewards for segment selec-\ntion, and maintaining the dynamic consistency\nand feasibility of stitched trajectories remains\nchallenging.\nIn this paper, we propose State-Covering Trajectory Stitching ( SCoTS ), a reward-free trajectory\naugmentation framework that systematically extends trajectories to cover diverse, unexplored regions\nof the state space. Specifically, SCoTS employs a three-stage approach: First, we learn a temporal\ndistance-preserving latent representation by training a model to encode states based on learned\noptimal temporal distances, facilitating efficient identification of viable trajectory segments. Second,\nwe introduce a novel iterative stitching strategy that balances directed exploration with state-space\ncoverage. In this process, trajectory segments are selected based on their progress along a learned\ndirection in the latent space and their novelty relative to previously explored regions within the rollout.\nFinally, we refine the resulting stitched trajectories using a diffusion-based refinement procedure.\nConsequently, the resulting trajectories exhibit broader state-space coverage while preserving dynamic\nfeasibility.\nTo summarize, our contribution in this paper is the introduction of SCoTS, a reward-free trajectory\naugmentation approach designed to generate diverse, long-horizon trajectories that enhance diffusion\nplanners. Extensive experiments across diverse and challenging benchmark tasks show that SCoTS\nsignificantly enhances the stitching capabilities and long-horizon generalization of diffusion planners.\nFurthermore, augmented trajectories generated by SCoTS notably boost the performance of widely\nused offline goal-conditioned reinforcement learning (GCRL) algorithms in across multiple trajectory\nstitching benchmarks.\n2 Planning with Diffusion Models\nDiffusion-based planners (Janner et al., 2022; Liang et al., 2023; Chen et al., 2024c) provide a\npromising framework for long-horizon decision-making by modeling entire trajectories as joint\ndistributions. A trajectory τis typically represented as a sequence of states stand actions atover a\nplanning horizon T:\nτ=\u0014\ns1s2. . .sT\na1a2 aT\u0015\n, (1)\nwhere standatdenote the state and action at time step t, respectively. Diffusion planners utilize\ndiffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) to learn a trajectory\nCodes are available at https://github.com/leekwoon/scots .\n2\n--- Page 3 ---\n(c) Stitch\n (b) Select (a) Searchdenoising steps\nFigure 2: Overview of the SCoTS stitching process. (a)Temporal Distance-Preserving Search:\nGiven the currently composed trajectory (red), we identify candidate segments (gray) by searching in\na latent space learned to preserve temporal distances. Candidates are selected based on proximity to\nthe endpoint of the current trajectory in latent space. (b) Exploratory Segment Selection: Among\nthe retrieved candidate segments, we select the segment (blue) that best balances directional progress\ntoward a randomly sampled latent direction and novelty relative to previously visited states in latent\nspace. (c) Diffusion-based Stitching Refinement: To ensure smooth transitions, a diffusion model\nrefines the stitching point between segments, generating dynamically consistent trajectories.\ndistribution pθ(τ0)over noise-free trajectories τ0. This involves a predefined forward noising process\nand a learned reverse denoising process. The forward process incrementally adds Gaussian noise to\nthe trajectories through Mdiscrete diffusion timesteps with a variance schedule {βi}M\ni=1:\nq(τi|τi−1):=N(τi;p\n1−βiτi−1, βiI). (2)\nA key property is the direct sampling of intermediate trajectories:\nq(τi|τ0) =N(τi;√αiτ0,(1−αi)I), (3)\nwhere αi:=Qi\ns=1(1−βs). The schedule ensures that τMapproximates a standard Gaussian\ndistribution N(0,I). The reverse process learns to invert this noising process and define following\ngenerative process with a standard Gaussian prior p(τM):\npθ(τ0) =Z\np(τM)MY\ni=1pθ(τi−1|τi) dτ1:M(4)\nwith a learnable Gaussian transition: pθ(τi−1|τi) =N(τi−1|µθ(τi, i),Σi).\nGiven an offline dataset D, diffusion models in practice simplify training by parameterizing a noise-\nprediction network ϵθ, trained to predict the noise ϵadded during the forward process (Ho et al.,\n2020):\nL(θ) :=Ei,ϵ,τ0[∥ϵ−ϵθ(τi, i)∥2], (5)\nwhere i∈ {0,1, ..., M }is the diffusion timestep, ϵ∼ N(0,I)is target noise that was used to corrupt\nclean trajectory τ0intoτi=√αiτ0+√1−αiϵ.\nRemark. Previous works generally assume the offline dataset Dsufficiently covers diverse trajectories\nwith substantial length. Consequently, these studies have primarily focused on improving network\narchitectures, action generation methods, and planning strategies. In contrast, we explicitly aim to\ngenerate an augmented dataset Daugthat extends trajectory coverage, enabling diffusion planners to\ngeneralize effectively beyond their training distribution.\n3 State-Covering Trajectory Stitching\nWe introduce State-Covering Trajectory Stitching ( SCoTS ), a novel reward-free trajectory augmenta-\ntion framework designed to synthesize an augmented dataset Daugfrom an offline dataset D. The core\nidea of SCoTS is to iteratively construct long and diverse trajectories by repeatedly stitching short\nsegments guided by latent directional exploration, resulting in significantly improved generalization\nand extended planning horizons for diffusion planners. SCoTS consists of three stages: (1) learning a\ntemporal distance-preserving embedding for efficient segment retrieval (Section 3.1); (2) iterative\ntrajectory stitching driven by latent directional exploration and novelty-based selection (Section 3.2);\nand (3) diffusion-based refinement to ensure dynamically consistent transitions (Section 3.3). The\noverall procedure of SCoTS, including segment search, exploratory selection, and diffusion-based\nrefinement, is illustrated in Figure 2. The detailed algorithm is summarized in Algorithm 1.\n3\n--- Page 4 ---\nAlgorithm 1 Overview of the SCoTS Framework\n1:Input: Offline dataset D, Temporal distance-preserving embedding ϕ, Diffusion stitcher pstitcher\nθ\n2:Initialize: Augmented dataset Daug=∅\n3:forn= 1, . . . , N trajdo\n4: // Sample initial segment from offline data\n5:τcomp∼ D\n6: // Sample a random latent exploration direction\n7:z∼ N(0,I);z←z/∥z∥\n8: fort= 1, . . . , N stitchdo\n9: // Retrieve nearest segments using temporal embedding\n10: {τj}k\nj=1←TopKNeighbors (ϕ(end(τcomp)), ϕ(D), k)\n11: // Compute directional progress and novelty scores\n12: Compute scores Sj=Pj+βNj (Eq. (11))\n13: // Select best candidate segment\n14: τbest←arg max jSj\n15: // Diffusion-based stitching refinement\n16: τ′∼pstitcher\nθ\u0000\n· |s1=end(τcomp),sH=end(τbest)\u0001\n17: // Concatenate refined segment to trajectory\n18: τcomp←[τcomp,τ′]\n19: end for\n20: Daug← D aug∪ {τcomp}\n21:end for\n22:Train diffusion planner on Daug\n3.1 Temporal Distance-Preserving Embedding\nIdentifying trajectory segments that are suitable for stitching requires accurately measuring their\ntemporal closeness. However, simply using raw state-space distances can yield temporally incoherent\nresults due to potential dynamic inconsistencies arising from ignoring state reachability. To address\nthis, we employ a temporal distance-preserving embedding ϕ:S → Z , which maps raw states\nto a latent space Zdesigned such that the Euclidean distance ∥ϕ(s)−ϕ(g)∥2approximates the\noptimal temporal distance d∗(s,g), defined as the minimum number of environment steps required\nto transition from state sto state g. Formally, we parameterize a goal-conditioned value function\nV(s,g)following (Park et al., 2024b):\nV(s,g):=−||ϕ(s)−ϕ(g)||2, (6)\nwhich is trained on the offline dataset Dusing a temporal difference objective inspired by implicit\nQ-learning (Kostrikov et al., 2022):\nLϕ:=E(s,a,s′,g)∼D\u0002\nℓ2\nξ(− 1(s̸=g)−γ||¯ϕ(s′)−¯ϕ(g)||2+||ϕ(s)−ϕ(g)||2)\u0003\n, (7)\nwhere ¯ϕis a target network (Mnih, 2013), γis a discount factor, and ℓ2\nξdenotes the expectile\nloss (Kostrikov et al., 2022; Newey & Powell, 1987).\n3.2 Directional and Exploratory Trajectory Stitching\nGiven the learned temporal distance-preserving embedding ϕ, we iteratively construct extended\ntrajectories via stitching. We start each new trajectory by randomly sampling an initial segment τinit\nfrom the offline dataset D. To encourage diverse state coverage, we randomly sample a fixed latent\nexploration direction zas a unit vector, i.e., z∼ N(0,I),z←z/∥z∥, for each trajectory rollout.\nAt each stitching iteration, let τcomp denote the currently composed trajectory. We define end(τ)\nas a function returning the final state of trajectory τ. We then identify a set of candidate segments\n{τj}k\nj=1whose initial states are nearest neighbors to end (τcomp)in the latent space:\n{τj}k\nj=1=TopKNeighbors (ϕ(end(τcomp)), ϕ(D), k), (8)\n4\n--- Page 5 ---\nwhere the distance metric is ∥ϕ(end(τcomp))−ϕ(s1,j)∥2.\nTo select the best candidate for stitching, we evaluate each candidate segment τj= (s1,j, . . . ,sH,j)\nbased on a composite score balancing directional progress and novelty. The progress score quantifies\nthe alignment in the latent space between the segment direction and the exploration direction z:\nPj=⟨ϕ(end(τj))−ϕ(s1,j),z⟩. (9)\nThenovelty score promotes exploration and coverage of novel latent states by estimating the entropy\nof the endpoint of each candidate segment τjrelative to previously visited latent states. Here, Vrollout\ndenotes the collection of latent representations of every state along previously stitched segments.\nLeveraging a non-parametric particle-based estimator (Liu & Abbeel, 2021) on our temporal distance-\npreserving embeddings, we compute the novelty score as:\nNj=1\nkdensityX\nϕv∈k-NN\u0000\nϕ(end(τj)),Vrollout , kdensity\u0001\r\rϕ(end(τj))−ϕv\r\r\n2. (10)\nA higher Njindicates greater novelty, signaling that the candidate segment expands coverage by\nmoving towards less-explored regions of the latent space. We combine these two metrics to form the\noverall selection criterion:\nSj=Pj+βNj, (11)\nwhere βbalances progress and novelty. We then stitch the candidate τbestwith the highest score to\nτcomp.\n3.3 Diffusion-based Stitching Refinement\nAlthough the exploratory selection step identifies segments with desirable progress and novelty, the\nstitching points, i.e., the connecting states between consecutive trajectory segments, may still exhibit\nminor dynamic inconsistencies or sub-optimal transitions. To mitigate these issues, we introduce a\ndiffusion-based refinement step. Specifically, we train a diffusion model, termed the stitcher pstitcher\nθ ,\nwhich generates intermediate states conditioned on the boundary states of adjacent segments. Given\na selected segment τbest, the stitcher produces a refined trajectory τ′by sampling from:\nτ′∼pstitcher\nθ\u0000\n· |s1=end(τcomp),sH=end(τbest)\u0001\n, (12)\nwhere end(τcomp)denotes the end state of the current composite trajectory τcomp, and end(τbest)\ndenotes the end state of the newly selected segment τbest. This diffusion-based refinement effectively\nsmooths out transitions, ensuring dynamic coherence and feasibility of the stitched trajectories.\nBy iteratively repeating segment search, exploratory selection, and this refinement process, we\nconstruct a diverse set of augmented trajectories. To generate corresponding action sequences for these\ntrajectories, we train an inverse dynamics model at=fψ(st,st+1)on the offline dataset D, which\ninfers the actions that transition between consecutive states. The resulting state-action trajectories are\naggregated into the augmented dataset Daug. This systematic and iterative augmentation approach\ngenerates an augmented dataset that broadly covers the state space. Crucially, diffusion planners\ntrained on this augmented data exhibit significantly enhanced trajectory stitching capabilities and\nimproved long-horizon generalization, particularly for tasks requiring extensive trajectory stitching\nand long-horizon reasoning (Section 4.3).\n4 Experiments\nIn this section, we empirically validate the effectiveness of our proposed SCoTS framework. Specif-\nically, we aim to investigate (1)whether SCoTS can generate diverse trajectories that extend sig-\nnificantly beyond the planning horizons present in the original offline dataset, (2)whether training\ndiffusion planners on these augmented trajectories enhances their capability to produce feasible\nlong-horizon plans in unseen scenarios, and (3)whether the augmented dataset generated by SCoTS\nprovides significant performance improvements for existing offline goal-conditioned reinforcement\nlearning (GCRL) algorithms. Additional results can be found in Appendix C.\n5\n--- Page 6 ---\nTable 1: Quantitative results on locomotion tasks in OGBench. Results are averaged over 5\nrandom seeds, each with 50 episodes per task. Standard deviations are reported after the ±sign.\nEnv Type Size GCIQL QRL CRL HIQL GSC CD HD SCoTS\nPointMaze StitchMedium 21±9 80±120±1 74±6100±0100±024±3100±0\nLarge 31±2 84±150±0 13±6100±0100±017±2100±0\nGiant 0±0 50±80±0 0±0 29±3 68±3 0±0 100±0\nAntMazeStitchMedium 29±6 59±753±694±197±2 96±2 71±197±1\nLarge 7±2 18±211±267±566±2 86±2 36±293±1\nGiant 0±0 0±0 0±0 2±2 20±1 65±3 0±0 87±2\nExploreMedium 13±2 1±1 3±2 37±1090±2 81±242±399±1\nLarge 0±0 0±0 0±0 4±5 21±3 27±1 13±298±1\nAverage 12.6 36 .5 8 .4 36 .4 65 .3 77 .9 25 .4 96.8\nFigure 3: SCoTS enables long-horizon planning. We visualize trajectories generated by a diffusion\nplanner trained on SCoTS-augmented data, evaluated on two challenging AntMaze datasets: Explore\n(top) and Stitch (bottom). The original Stitch dataset contains trajectories limited to four maze\ncells per segment, necessitating extensive stitching, whereas the Explore dataset comprises low-\nquality trajectories with large action noise. Despite these constraints, SCoTS augmentation allows\nthe planner to synthesize trajectories that substantially surpass the horizon and quality of the original\ndata, connecting specified start\n and goal\n .\n4.1 Datasets and Environments\nWe evaluate SCoTS on OGBench benchmark (Park et al., 2024a), spanning diverse difficulties,\nenvironment sizes, agent state dimensions, and training data qualities. Specifically, the benchmark\nincludes three locomotion environments: PointMaze (controlling a 2D point mass) and AntMaze\n(controlling an 8-DoF quadrupedal Ant). We consider two distinct dataset types, each designed\nto evaluate specific challenges. The Stitch dataset comprises short, goal-reaching trajectories\nlimited to four cell units, thus requiring the agent to stitch multiple segments (up to 8) for successful\ninference. In contrast, the Explore dataset assesses learning navigation behaviors from extensive\nyet low-quality exploratory trajectories, collected by frequently resampling random directions and\ninjecting significant action noise. For each environment, we report the success rate averaged over all\nevaluation episodes, where an episode is considered successful if the agent reaches sufficiently close\nto the goal state within a predefined distance threshold. See Appendix A for dataset details.\n4.2 Diversity and State Coverage Analysis\nTo investigate whether SCoTS effectively promotes diverse state-space coverage through trajectory\nstitching, we evaluate its performance in the PointMaze-Giant-Stitch environment. As illustrated\nin Figure 4, we visualize the incremental stitching process for different values of the novelty weighting\nparameter β∈ {0,2,20}. We observe that when β= 0, trajectory stitching predominantly follows\nlatent directional guidance, resulting in trajectories with limited coverage but clear directional\ndistinctions. With a moderate setting β= 2, trajectories exhibit a balanced trade-off, achieving\nsubstantial state-space coverage with notable diversity. Conversely, at a higher novelty weight β= 10 ,\ntrajectories broadly cover the state space but lose their distinctiveness, leading to overlapping paths\n6\n--- Page 7 ---\nacross different latent exploration directions. Based on these results, we use β= 2.0across all\nenvironments in our experiments.\n4.3 Diffusion Planning with SCoTS-Augmented Data\nFigure 4: Effect of novelty score on Trajectory\nStitching. Trajectory stitching examples in the\nPointMaze-Giant-Stitch environment. The\noriginal dataset ( Stitch ) consists of short seg-\nments limited to at most four maze cells. Different\ncolors represent trajectories generated from dis-\ntinct latent exploration directions z.We next demonstrate how SCoTS-generated tra-\njectories enhance the ability of diffusion plan-\nners to generate feasible, long-horizon plans\nbeyond their training distribution. We com-\npare our approach with offline goal-conditioned\nreinforcement learning (GCRL) methods in-\ncluding goal-conditioned implicit Q-learning\n(GCIQL) (Kostrikov et al., 2022), Quasimet-\nric RL (QRL) (Wang et al., 2023), Contrastive\nRL (CRL) (Eysenbach et al., 2022), and Hierar-\nchical implicit Q-learning (HIQL) (Park et al.,\n2023a). We also include diffusion-based gener-\native planning baselines explicitly designed for\nlong-horizon generalization, such as Generative\nSkill Chaining (GSC) (Mishra et al., 2023) and\nCompositional Diffuser (CD) (Luo et al., 2025).\nFor our experiments, we adopt a hierarchical\ndiffusion planner (HD) (Chen et al., 2024c) that\ngenerates plans through a two-level planning\nprocess. Specifically, the high-level diffusion\nmodel first generates sparse, temporally coarse waypoints, after which a low-level diffusion model\nfills in the intermediate states between these waypoints, producing a temporally dense trajectory.\nInitially constrained by limited and short-horizon training data, we augment the original dataset with\nSCoTS-generated trajectories. After dataset augmentation, we train diffusion planner and employ a\nvalue-based low-level controller for action execution, following recent approaches (Yoon et al., 2025;\nLu et al., 2025). The plans generated by the difussion planner serve as sequences of subgoals for the\nlow-level controller. At each step, the low-level controller executes actions toward a subgoal selected\nfrom the generated plan; after a fixed horizon or once the subgoal is reached, it dynamically updates\nthe subgoal by selecting the next state at a specified horizon further along in the plan generated by\nthe diffusion planner. For each dataset, we upsample the original data to 5M samples. Additional\nimplementation details, including hyperparameters and specifics of the low-level controller, are\nprovided in Appendix B.\nAs shown in Table 1, integrating SCoTS consistently enhances the performance of the hierarchical\ndiffusion planner across all tasks, achieving near-optimal success rates. Notably, the advantage\nof SCoTS becomes especially pronounced as the complexity and scale of the mazes increase,\nwith the gap between SCoTS and other baselines maximized in the largest ( Giant ) environments.\nFurthermore, in the challenging Explore dataset of the AntMaze environment consisting of noisy\nand short-range exploratory trajectories, augmentation via SCoTS significantly improves the planner\nability to generate coherent, long-range, goal-directed plans, clearly highlighting the effectiveness of\nSCoTS.\n4.4 Offline GCRL with SCoTS-Augmented Data\nAlthough SCoTS is primarily designed for diffusion planners, we additionally evaluate whether\ntrajectories augmented by SCoTS can enhance the performance of existing offline goal-conditioned\nRL (GCRL) algorithms. Specifically, we retrain widely used offline GCRL algorithms, including\nGCIQL (Kostrikov et al., 2022), CRL (Eysenbach et al., 2022), and HIQL (Park et al., 2023a), on\nthe SCoTS-augmented dataset. All hyperparameters remain identical to their original implementa-\ntions. Additionally, we compare our approach with SynthER (Lu et al., 2023), which employs an\nunconditional diffusion model for transition-level data augmentation. Results summarized in Table 2\nclearly demonstrate that SCoTS-generated trajectories consistently outperform SynthER and methods\ntrained solely on the original offline datasets, significantly boosting performance across all tested\nalgorithms. This indicates that augmenting data at the trajectory-level with SCoTS, which explicitly\n7\n--- Page 8 ---\nTable 2: Performance enhancement of offline GCRL algorithms with SCoTS-augmented dataset.\nResults are averaged over 5 seeds, each with 50 episodes per task. Standard deviations are indicated\nby±sign.\nEnv Type SizeGCIQL CRL HIQL\nOriginal SynthER SCoTS Original SynthER SCoTS Original SynthER SCoTS\nPointMaze StitchMedium 21±9 30±3 79±1 0±1 0±0 46±2 74±6 77±4 82±4\nLarge 31±2 35±4 26±2 0±0 0±0 39±2 13±6 16±3 67±1\nGiant 0±0 0±0 0±0 0±0 0±0 18±2 0±0 0±0 27±2\nAntMazeStitchMedium 29±6 31±3 35±2 53±6 48±3 65±3 94±1 91±2 94±1\nLarge 7±2 3±4 7±1 11±2 12±2 19±1 67±5 65±3 91±2\nGiant 0±0 0±0 0±0 0±0 0±0 2±1 2±2 0±0 55±5\nExploreMedium 13±2 12±3 18±3 3±2 3±1 15±3 37±10 45±8 94±1\nLarge 0±0 0±0 0±0 0±0 2±1 19±1 4±5 12±3 77±2\nAverage 12.6 13 .9 20.7 8.4 8 .1 27.9 36.4 38 .3 73.4\nconsiders long-term dynamics and diversity, provides more effective supervision for learning robust\ntrajectory stitching and long-horizon planning capabilities.\n4.5 Ablation Studies\n5 10 15 20 25\nLow-level Controller Horizon556065707580859095100Success Rate (%)\nAntMaze-Giant-Stitch\nSCoTS\nCD\nFigure 5: Ablation study on low-level\ncontroller horizon. Success rates in the\nAntMaze-Giant-Stitch environment compar-\ning SCoTS against Compositional Diffuser (CD)\n(Luo et al., 2025), across various low-level con-\ntroller horizon lengths.\n10−310−210−1100101\nDynamic MSE02004006008001000Number of SamplesWithout Reﬁnement\nWith ReﬁnementFigure 6: Dynamic MSE comparison at stitch-\ning points. Histograms showing the distribu-\ntions of Dynamic MSE at trajectory stitching\npoints in the AntMaze-Giant-Stitch environ-\nment, comparing results with and without the\ndiffusion-based stitching refinement step.\nAblation study on low-level controller horizon. We investigate how the performance of our\napproach (SCoTS) is influenced by varying the horizon length of the low-level controller in the\nAntMaze-Giant-Stitch environment. As shown in Figure 5, SCoTS achieves consistently strong\nperformance across different horizon lengths H∈ {5,10,15,20,25}, outperforming the Composi-\ntional Diffuser (CD) (Luo et al., 2025). These results demonstrate that the diffusion planner trained\nwith SCoTS generates highly feasible subgoals, maintaining robustness and effectiveness regardless\nof the chosen low-level execution horizon.\nEffectiveness of diffusion-based stitching refinement. To further illustrate the effectiveness of\nthe diffusion-based stitching refinement step in our SCoTS framework, we quantitatively evaluate\nits impact on dynamic consistency at stitching points. Specifically, we compute the Dynamic Mean\nSquared Error (Dynamic MSE) (Lu et al., 2023), defined as:\nDynamic MSE =∥f∗(s,a)−s′∥2\n2,\nwhich measures how closely the generated transitions adhere to the true environment dynamics f∗.\nFigure 6 compares the distribution of Dynamic MSE at stitching points before and after applying\nrefinement on a logarithmic scale. Results clearly show that diffusion-based refinement substantially\nreduces dynamic inconsistencies, highlighting its critical role in generating dynamically feasible and\ncoherent trajectories.\n8\n--- Page 9 ---\nTable 3: Impact of Replanning. Success rates on\nOGBench PointMaze andAntMaze Stitch datasets,\ncomparing SCoTS and CD (Luo et al., 2025). ✓indi-\ncates with replanning; ✗indicates without replanning.\nEnv SizeCD SCoTS\n✗ ✓ ✗ ✓\nPointMazeMedium 100±0100±0100±0100±0\nLarge 100±0100±0100±0100±0\nGiant 53±668±389±2100±0\nAntMazeMedium 92±296±297±197±1\nLarge 76±286±292±293±1\nGiant 27±465±384±287±2\nAverage 74.7 85 .9 93 .7 96.2Ablation Study on Replanning. We em-\nploys replanning during a rollout, enabling\nthe agent to recover from failures, such as\nwhen the diffusion planner generates un-\nreachable subgoals for the low-level con-\ntroller. In practice, we set a replanning in-\nterval (e.g., every 200 steps); further im-\nplementation details are provided in Ap-\npendix B. In Table 3, we present an abla-\ntion study comparing performance with and\nwithout replanning on the PointMaze and\nAntMaze Stitch datasets from OGBench.\nSCoTS consistently outperforms Composi-\ntional Diffuser (CD) (Luo et al., 2025), the\nbest-performing baseline, even without re-\nplanning. Additionally, the performance with and without replanning is similar, highlighting the\nreliability and efficacy of the SCoTS-augmented diffusion planner.\n5 Related Work\nPlanning with Diffusion Models. Diffusion probabilistic models (Sohl-Dickstein et al., 2015;\nHo et al., 2020) have emerged as powerful tools for reinforcement learning, especially in offline\nsettings. These models iteratively denoise sampled data from noise, effectively learning gradients\nof the data distribution (Song & Ermon, 2019) and demonstrating strong capabilities in modeling\ncomplex trajectories. Early work such as Diffuser (Janner et al., 2022) employed unconditional\ndiffusion models guided by learned value estimators (Dhariwal & Nichol, 2021). Subsequent methods\nlike Decision Diffuser (Ajay et al., 2023) and AdaptDiffuser (Liang et al., 2023) introduced classifier-\nfree guidance and progressive fine-tuning. Recent advancements further leveraged hierarchical\nstructures (Chen et al., 2024c; Li et al., 2023), multi-task conditioning (He et al., 2023; Ni et al.,\n2023), and multi-agent setups (Zhu et al., 2023). Additionally, diffusion planners have explored\nintegration with tree search methods (Yoon et al., 2025), refined trajectory sampling techniques (Dong\net al., 2024), and investigated critical design choices to improve robustness (Lu et al., 2025). Despite\nthese advances, diffusion planners still fundamentally depend on the quality and diversity of the offline\ntraining datasets, limiting their ability to generate coherent and feasible long-horizon plans beyond\ntheir training distribution. Recent approaches such as Generative Skill Chaining (GSC) (Mishra et al.,\n2023) and Compositional Diffuser (Luo et al., 2025) address this by composing short segments at test\ntime into long-horizon trajectories. Our work presents an orthogonal solution by directly augmenting\nthe offline dataset itself, significantly enhancing the capability of diffusion planners to generalize to\ndiverse and substantially longer trajectories.\nData Augmentation for RL. Data augmentation is a recognized strategy for improving sample\nefficiency and generalization in reinforcement learning (RL). In pixel-based RL, techniques like\nrandom image transformations (e.g., cropping, translation) have proven effective in works such\nas CURL (Laskin et al., 2020b), RAD (Laskin et al., 2020a), and DrQ (Yarats et al., 2021). For\nstate-based observations, methods like S4RL (Sinha et al., 2022) and AWM (Ball et al., 2021) often\nintroduce perturbations to states or learned dynamics models to enhance robustness. Recent advances\nin generative models have enabled trajectory-level augmentation methods, either at the transition\nlevel (Lu et al., 2023; Wang et al., 2024) or the full trajectory level (He et al., 2023; Jackson et al.,\n2024; Lee et al., 2024). For instance, MTDiff-S (He et al., 2023) generates synthetic trajectories for\nmulti-task scenarios, while Policy-Guided Diffusion (PGD) (Jackson et al., 2024) and GTA (Lee et al.,\n2024) employ generative models to produce high-reward trajectories guided by policies or returns.\nDiffStitch (Li et al., 2024) further systematically connects trajectories based on extrinsic rewards, yet\nthese methods typically require explicit reward signals and are limited to generating short-horizon\ntrajectories. In contrast, our proposed SCoTS method operates in a reward-free manner, systematically\nsynthesizing long-horizon, diverse, and dynamically consistent trajectories to significantly enhance\noffline datasets, thereby facilitating the generation of feasible plans in downstream tasks requiring\nextended horizon reasoning.\n9\n--- Page 10 ---\nTemporal Distance in RL. Temporal distance has been widely adopted as a structural inductive\nbias in various reinforcement learning (RL) paradigms, including imitation learning (Sermanet\net al., 2018), unsupervised skill discovery (Hartikainen et al., 2019; Park et al., 2023b, 2024b),\ngoal-conditioned RL (Durugkar et al., 2021; Eysenbach et al., 2022; Wang et al., 2023; Bae et al.,\n2024), and curriculum learning (Zhang et al., 2020; Kim et al., 2023). Recent methods such as\nMETRA (Park et al., 2023b), QRL (Wang et al., 2023), HILP (Park et al., 2024b), and TLDR (Bae\net al., 2024) particularly focus on learning temporal distance-preserving representations to facilitate\ndiverse skill discovery or efficient goal-reaching behaviors. Distinct from prior methods, our SCoTS\nframework explicitly leverages temporal distance-preserving representations to identify temporally\nviable trajectory segments for stitching. This allows systematic synthesis of extended, diverse,\nand dynamically consistent trajectories, significantly augmenting offline datasets and improving\nlong-horizon generalization for diffusion-based planners.\n6 Conclusion\nIn this work, we introduced State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajec-\ntory augmentation approach designed to enhance the performance and generalization capabilities\nof diffusion planners. By leveraging temporal distance-preserving embeddings, SCoTS iteratively\nstitches together short trajectory segments, systematically extending the diversity and horizon of\noffline data. Empirical results across challenging benchmarks demonstrated that SCoTS-generated tra-\njectories significantly improve the ability of diffusion planners to perform long-horizon planning and\ngeneralize to novel tasks. Furthermore, we showed that our augmented dataset notably enhances the\nperformance of widely used offline goal-conditioned reinforcement learning algorithms, highlighting\nthe broad utility of our approach.\nLimitations. While SCoTS achieves strong empirical performance, it exhibits certain limita-\ntions. First, generating augmented trajectories through iterative stitching and diffusion-based re-\nfinement introduces significant computational overhead, especially due to the additional training of\nthe diffusion-based stitcher model and the trajectory augmentation process. Second, our temporal\ndistance-preserving embeddings do not capture the asymmetric temporal distances between states,\npotentially limiting their effectiveness in highly asymmetric or disconnected Markov Decision Pro-\ncesses (MDPs), such as object manipulation tasks involving irreversible actions or environments\ncontaining isolated regions with sparse connectivity.\nReferences\nAjay, A., Du, Y ., Gupta, A., Tenenbaum, J. B., Jaakkola, T. S., and Agrawal, P. Is conditional\ngenerative modeling all you need for decision making? In International Conference on Learning\nRepresentations (ICLR) , 2023.\nAsadi, K., Misra, D., and Littman, M. Lipschitz continuity in model-based reinforcement learning.\nInInternational Conference on Machine Learning , pp. 264–273. PMLR, 2018.\nBae, J., Park, K., and Lee, Y . Tldr: Unsupervised goal-conditioned rl via temporal distance-aware\nrepresentations. arXiv preprint arXiv:2407.08464 , 2024.\nBall, P. J., Lu, C., Parker-Holder, J., and Roberts, S. Augmented world models facilitate zero-\nshot dynamics generalization from a single offline environment. In International Conference on\nMachine Learning , pp. 619–629. PMLR, 2021.\nChen, B., Martí Monsó, D., Du, Y ., Simchowitz, M., Tedrake, R., and Sitzmann, V . Diffusion forcing:\nNext-token prediction meets full-sequence diffusion. Advances in Neural Information Processing\nSystems , 37:24081–24125, 2024a.\nChen, C., Baek, J., Deng, F., Kawaguchi, K., Gulcehre, C., and Ahn, S. Plandq: hierarchical plan\norchestration via d-conductor and q-performer. arXiv preprint arXiv:2406.06793 , 2024b.\nChen, C., Deng, F., Kawaguchi, K., Gulcehre, C., and Ahn, S. Simple hierarchical planning with\ndiffusion. arXiv preprint arXiv:2401.02644 , 2024c.\n10\n--- Page 11 ---\nDhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2021.\nDong, Z., Yuan, Y ., Hao, J., Ni, F., Mu, Y ., Zheng, Y ., Hu, Y ., Lv, T., Fan, C., and Hu, Z. Aligndiff:\nAligning diverse human preferences via behavior-customisable diffusion model. arXiv preprint\narXiv:2310.02054 , 2023.\nDong, Z., Hao, J., Yuan, Y ., Ni, F., Wang, Y ., Li, P., and Zheng, Y . Diffuserlite: Towards real-time\ndiffusion planning. Advances in Neural Information Processing Systems , 37:122556–122583,\n2024.\nDouze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.-E., Lomeli, M., Hosseini, L.,\nand Jégou, H. The faiss library. arXiv preprint arXiv:2401.08281 , 2024.\nDurugkar, I., Tec, M., Niekum, S., and Stone, P. Adversarial intrinsic motivation for reinforcement\nlearning. Advances in Neural Information Processing Systems , 34:8622–8636, 2021.\nEysenbach, B., Zhang, T., Levine, S., and Salakhutdinov, R. R. Contrastive learning as goal-\nconditioned reinforcement learning. Advances in Neural Information Processing Systems , 35:\n35603–35620, 2022.\nHa, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122 , 2018.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent\ndynamics for planning from pixels. In International Conference on Machine Learning (ICML) ,\n2019.\nHartikainen, K., Geng, X., Haarnoja, T., and Levine, S. Dynamical distance learning for semi-\nsupervised and unsupervised skill discovery. arXiv preprint arXiv:1907.08225 , 2019.\nHe, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D., Zhao, B., and Li, X. Diffusion model is an\neffective planner and data synthesizer for multi-task reinforcement learning. Advances in neural\ninformation processing systems , 36:64896–64917, 2023.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2020.\nJackson, M. T., Matthews, M. T., Lu, C., Ellis, B., Whiteson, S., and Foerster, J. Policy-guided\ndiffusion. arXiv preprint arXiv:2404.06356 , 2024.\nJanner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy\noptimization. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.\nJanner, M., Du, Y ., Tenenbaum, J. B., and Levine, S. Planning with diffusion for flexible behavior\nsynthesis. In International Conference on Machine Learning (ICML) , 2022.\nKaiser, Ł., Babaeizadeh, M., Miłos, P., Osi ´nski, B., Campbell, R. H., Czechowski, K., Erhan, D.,\nFinn, C., Kozakowski, P., Levine, S., et al. Model-based reinforcement learning for atari. In\nInternational Conference on Learning Representations (ICLR) , 2020.\nKim, S., Lee, K., and Choi, J. Variational curriculum reinforcement learning for unsupervised\ndiscovery of skills. In International Conference on Machine Learning (ICML) , 2023.\nKostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. In\nInternational Conference on Learning Representations (ICLR) , 2022.\nLaskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with\naugmented data. Advances in neural information processing systems , 33:19884–19895, 2020a.\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for rein-\nforcement learning. In International conference on machine learning , pp. 5639–5650. PMLR,\n2020b.\nLee, J., Yun, S., Yun, T., and Park, J. Gta: Generative trajectory augmentation with guidance for\noffline reinforcement learning. arXiv preprint arXiv:2405.16907 , 2024.\n11\n--- Page 12 ---\nLi, G., Shan, Y ., Zhu, Z., Long, T., and Zhang, W. Diffstitch: Boosting offline reinforcement learning\nwith diffusion-based trajectory stitching. arXiv preprint arXiv:2402.02439 , 2024.\nLi, W., Wang, X., Jin, B., and Zha, H. Hierarchical diffusion for offline decision making. In\nInternational Conference on Machine Learning , pp. 20035–20064. PMLR, 2023.\nLiang, Z., Mu, Y ., Ding, M., Ni, F., Tomizuka, M., and Luo, P. Adaptdiffuser: Diffusion models as\nadaptive self-evolving planners. In International Conference on Machine Learning (ICML) , 2023.\nLiu, H. and Abbeel, P. Behavior from the void: Unsupervised active pre-training. Advances in Neural\nInformation Processing Systems , 34:18459–18473, 2021.\nLu, C., Ball, P., Teh, Y . W., and Parker-Holder, J. Synthetic experience replay. Advances in Neural\nInformation Processing Systems , 2023.\nLu, H., Han, D., Shen, Y ., and Li, D. What makes a good diffusion planner for decision making? In\nThe Thirteenth International Conference on Learning Representations , 2025.\nLuo, Y ., Mishra, U. A., Du, Y ., and Xu, D. Generative trajectory stitching through diffusion\ncomposition. arXiv preprint arXiv:2503.05153 , 2025.\nMishra, U. A., Xue, S., Chen, Y ., and Xu, D. Generative skill chaining: Long-horizon skill planning\nwith diffusion models. In Conference on Robot Learning , pp. 2905–2925. PMLR, 2023.\nMnih, V . Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.\nNewey, W. K. and Powell, J. L. Asymmetric least squares estimation and testing. Econometrica:\nJournal of the Econometric Society , pp. 819–847, 1987.\nNi, F., Hao, J., Mu, Y ., Yuan, Y ., Zheng, Y ., Wang, B., and Liang, Z. Metadiffuser: Diffusion model\nas conditional planner for offline meta-rl. In International Conference on Machine Learning , pp.\n26087–26105. PMLR, 2023.\nPark, S., Ghosh, D., Eysenbach, B., and Levine, S. Hiql: Offline goal-conditioned rl with latent states\nas actions. Advances in Neural Information Processing Systems , 36:34866–34891, 2023a.\nPark, S., Rybkin, O., and Levine, S. Metra: Scalable unsupervised rl with metric-aware abstraction.\narXiv preprint arXiv:2310.08887 , 2023b.\nPark, S., Frans, K., Eysenbach, B., and Levine, S. Ogbench: Benchmarking offline goal-conditioned\nrl.arXiv preprint arXiv:2410.20092 , 2024a.\nPark, S., Kreiman, T., and Levine, S. Foundation policies with hilbert representations. In International\nConference on Machine Learning (ICML) , 2024b.\nPeebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pp. 4195–4205, 2023.\nSermanet, P., Lynch, C., Chebotar, Y ., Hsu, J., Jang, E., Schaal, S., Levine, S., and Brain, G. Time-\ncontrastive networks: Self-supervised learning from video. In 2018 IEEE international conference\non robotics and automation (ICRA) , pp. 1134–1141. IEEE, 2018.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,\nAntonoglou, I., Panneershelvam, V ., Lanctot, M., et al. Mastering the game of go with deep neural\nnetworks and tree search. nature , 529(7587):484–489, 2016.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature , 550\n(7676):354–359, 2017.\nSinha, S., Mandlekar, A., and Garg, A. S4rl: Surprisingly simple self-supervision for offline\nreinforcement learning in robotics. In Conference on Robot Learning , pp. 907–917. PMLR, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML) ,\n2015.\n12\n--- Page 13 ---\nSong, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 , 2020.\nSong, Y . and Ermon, S. Generative modeling by estimating gradients of the data distribution. In\nAdvances in Neural Information Processing Systems (NeurIPS) , 2019.\nSong, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based\ngenerative modeling through stochastic differential equations. In International Conference on\nLearning Representations (ICLR) , 2021.\nSutton, R. S. Reinforcement learning: An introduction. A Bradford Book , 2018.\nTalvitie, E. Model regularization for stable sample rollouts. In Proceedings of the Conference on\nUncertainty in Artificial Intelligence (UAI) , 2014.\nTassa, Y ., Erez, T., and Todorov, E. Synthesis and stabilization of complex behaviors through online\ntrajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and\nSystems , pp. 4906–4913, 2012.\nV oelcker, C., Liao, V ., Garg, A., and Farahmand, A.-m. Value gradient weighted model-based\nreinforcement learning. In International Conference on Learning Representations (ICLR) , 2022.\nWang, R., Frans, K., Abbeel, P., Levine, S., and Efros, A. A. Prioritized generative replay. arXiv\npreprint arXiv:2410.18082 , 2024.\nWang, T., Torralba, A., Isola, P., and Zhang, A. Optimal goal-reaching reinforcement learning via\nquasimetric learning. In International Conference on Machine Learning , pp. 36411–36430. PMLR,\n2023.\nYarats, D., Kostrikov, I., and Fergus, R. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. In International conference on learning representations , 2021.\nYoon, J., Cho, H., Baek, D., Bengio, Y ., and Ahn, S. Monte carlo tree diffusion for system 2 planning.\narXiv preprint arXiv:2502.07202 , 2025.\nZhang, Y ., Abbeel, P., and Pinto, L. Automatic curriculum learning through value disagreement.\nAdvances in Neural Information Processing Systems , 33:7648–7659, 2020.\nZhu, Z., Liu, M., Mao, L., Kang, B., Xu, M., Yu, Y ., Ermon, S., and Zhang, W. Madiff: Offline\nmulti-agent learning with diffusion models. arXiv preprint arXiv:2305.17330 , 2023.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al. Maximum entropy inverse reinforcement\nlearning. In Aaai , volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n13\n--- Page 14 ---\nA Details of Datasets\nTable 4: Dataset specifications.\nEnv Type Size # Transitions # EpisodesData Episode\nLength\nPointMaze StitchMedium 1M 5,000 200\nLarge 1M 5,000 200\nGiant 1M 5,000 200\nAntMazeStitchMedium 1M 5,000 200\nLarge 1M 5,000 200\nGiant 1M 5,000 200\nExploreMedium 5M 10,000 500\nLarge 5M 10,000 500We evaluate our method on the OGBench\nbenchmark (Park et al., 2024a). Since our\nprimary goal is to assess trajectory stitching\ncapability and long-horizon reasoning, we\nspecifically utilize the Stitch andExplore\ndatasets. As shown in Figure 7, the Stitch\ndataset is explicitly designed to challenge\ntrajectory stitching ability, comprising short,\ngoal-reaching trajectories limited to a maxi-\nmum length of four cell units. Consequently, agents must effectively stitch together multiple short\nsegments (up to eight) to successfully complete long-horizon tasks. In contrast, the Explore dataset\nis designed to test navigation skills learned from extensive yet low-quality trajectories. These trajec-\ntories are generated by commanding a low-level policy with random movement directions re-sampled\nevery ten steps, along with significant action noise. Each demonstration trajectory typically spans\nonly two to three blocks, resulting in noisy and clustered paths that pose additional challenges for\nevaluating the ability to learn effective policies from highly suboptimal data.\n(a)PointMaze-Medium-Stitch (b)PointMaze-Large-Stitch (c)PointMaze-Giant-Stitch\n(d)AntMaze-Medium-Stitch (e)AntMaze-Large-Stitch (f)AntMaze-Giant-Stitch\n(g)AntMaze-Medium-Explore (h)AntMaze-Large-Explore\nFigure 7: Visualization of trajectories from OGBench datasets. Each sub-figure illustrates example\ntrajectories from different combinations of environments and datasets used in our experiments.\nB Implementation Details\nNetwork architecture. We utilize DiT1D (Peebles & Xie, 2023) as the neural network backbone\nfor both the diffusion planner and the stitcher, due to its large receptive field and effectiveness in\nmodeling trajectory-level dependencies. Following prior studies (Dong et al., 2023; Lu et al., 2025),\nwe employ a DiT1D architecture with a hidden dimension of 256, a head dimension of 32, and a total\nof 8 DiT blocks consistently across all environments.\nhttps://github.com/seohongpark/ogbench\n14\n--- Page 15 ---\nDetails of the low-level controller. A key challenge in diffusion-based planning is balancing global\ntrajectory coherence with effective low-level control in high-dimensional state-action spaces (Chen\net al., 2024a,b; Yoon et al., 2025). Previous approaches, such as PlanDQ (Chen et al., 2024b)\nand MCTD (Yoon et al., 2025), address this issue by integrating high-level diffusion planners\nwith separately trained low-level controllers. Similarly, we adopt a hierarchical strategy, where the\ndiffusion planner generates plans based primarily on compact, lower-dimensional state representations\n(e.g., positions of the agent itself), delegating the fine-grained, low-level action execution to a\ndedicated low-level controller. In our experiments, we specifically employ GCIQL (Kostrikov\net al., 2022) as the learned low-level policy in the PointMaze environments and CRL (Eysenbach\net al., 2022) in the AntMaze environments. A detailed visualization of generated subgoals and their\ncorresponding execution rollouts can be seen in Figure 9. Furthermore, an ablation study examining\nthe impact of the horizon length of the low-level controller is presented in Figure 5.\nImplementation details for SCoTS and diffusion planning. In the temporal distance-preserving\nsearch stage of SCoTS, we retrieve the top k= 10 candidate segments based on their proximity\nin the learned latent embedding space during each stitching step. For computing the novelty score,\nwe utilize a density estimator parameter kdensity = 30 and set the novelty weighting factor β= 2.0\nconsistently across all tested environments. The horizon length for the diffusion-based stitcher is\nuniformly set to Hstitcher = 26 .\nTo generate the augmented dataset Daug, we perform the stitching procedure Nstitch iterations\nper trajectory, creating a total of Ntrajtrajectories, thus ensuring the augmented dataset comprises\napproximately 5 million transitions. Specifically, in the AntMaze-Large-Stitch environment, we\nsetNstitch= 40 andNtraj= 5000 .\nFor configuring the Hierarchical Diffusion (HD) planner (Chen et al., 2024c), parameters are adapted\naccording to the properties of the training data. When training on the original Stitch andExplore\ndatasets, which contain inherently shorter trajectories (as detailed in Table 4, column \"Data Episode\nLength\"), we set the high-level planning horizon to 101 steps for Stitch and 401 steps for Explore ,\nboth with temporal jumps of 26 steps between waypoints. However, when utilizing SCoTS-augmented\ndatasets that feature longer and more diverse trajectories, we extend this planning horizon to 501\nsteps for Medium andLarge environments, and to 1001 steps for Giant environments, maintaining\nthe temporal jump of 26 steps. Similarly, for SCoTS-augmented Explore datasets, we also use a\nplanning horizon of 1001 steps with 26-step jumps.\nWe apply jumpy denoising with DDIM sampling (Song et al., 2020) using 20 denoising steps\nacross all environments. Additionally, we tune the replanning interval from the set {50,100,200}\nsteps and tune the horizon for the low-level controller from {5,10,15,20,25}. A full list of the\nhyperparameters is reported in Table 5.\nPractical implementation of temporal distance-preserving search. Our SCoTS framework relies\non a learned latent space Zwhere the L2distance, ∥ϕ(s)−ϕ(g)∥2, approximates the optimal\ntemporal distance d∗(s,g)between states (as detailed in Section 3.1). A critical step in SCoTS is\nthe efficient identification of suitable candidate trajectory segments from a large offline dataset D.\nThis requires a fast nearest neighbor search mechanism within the learned latent space Z. To achieve\nthis, we employ an Inverted File (IVF) index from the Faiss library (Douze et al., 2024), which is\nspecifically designed for large-scale similarity searches.\nThe practical implementation of this search mechanism involves several stages. First, we prepare\nthe data for indexing. This consists of computing the latent embeddings ϕ(sinit)for the initial states\nsinitof all trajectories within the offline dataset D. Let ddenote the dimensionality of these latent\nembeddings. An IVF index is then constructed upon this collection of d-dimensional vectors. The\nconstruction process begins by partitioning the latent vectors into nlistclusters using the k-means\nalgorithm. Each cluster is represented by a centroid cj∈ {c1, . . . ,cnlist}. Subsequently, each latent\nvector ϕ(sinit)in our collection is assigned to its nearest centroid, and for each centroid, an inverted\nlist is maintained, storing references to the vectors belonging to its cluster.\nDuring the temporal distance-preserving search phase of SCoTS (detailed in Algorithm 1, line 10),\nthe latent embedding of the current composed trajectory endpoint, ϕ(end(τcomp)), serves as the query\nvector q. To find the knearest neighbors for q, the IVF index first identifies a limited set of clusters\nwhose centroids {cj}are closest to the query vector q. The search for neighbors is then confined to\n15\n--- Page 16 ---\nthe latent vectors stored within the inverted lists corresponding to these selected clusters. This targeted\napproach significantly prunes the search space compared to an exhaustive search. Furthermore, the\nFaiss library provides support for GPU acceleration, which can further expedite this search process\nand enable efficient candidate retrieval. Once the knearest latent embeddings corresponding to initial\nstates of segments are identified, we retrieve the full original trajectory segments from Dto form the\ncandidate set for the stitching process.\nTable 5: Hyperparameters for SCoTS.\nComponent Hyperparameter Value Tuning Choices\nSCoTS: Temporal Distance-Preserving Embedding ( ϕ)\nLearning Rate 3×10−4-\nLatent Dimension 32 -\nBatch Size 1024 -\nTraining Steps 1,000,000 -\nNetwork Backbone MLP -\nMLP Dimensions (512, 512, 512) -\nExpectile ( ξforℓ2\nξ) 0.95 -\nSCoTS: Inverse Dynamics Model (for actions in Daug)\nNetwork Backbone MLP -\nMLP Dimensions (256, 256, 256) -\nTraining Steps 200,000 -\nSCoTS: Stitching Process Parameters\nTop-kCandidates (Search) 10 -\nkdensity (Novelty Score) 30 -\nNovelty Weight ( β) 2.0 -\nAugmented Dataset Size ∼5M transitions -\nNstitch(Stitches per Traj.) Task-dependent (e.g., 40) -\nNtraj(Generated Traj.) Task-dependent (e.g., 5,000) -\nSCoTS: Diffusion-based Stitcher ( pstitcher\nθ )\nNetwork Backbone DiT1D -\nLearning Rate 2×10−4-\nWeight Decay 1×10−5-\nBatch Size 64 -\nTraining Steps 1,000,000 -\nSolver DDIM -\nSampling Steps (DDIM) 20 -\nHorizon ( Hstitcher ) 26 -\nHierarchical Diffusion Planner (HD)\nNetwork Backbone DiT1D -\nLearning Rate 2×10−4-\nWeight Decay 1×10−5-\nBatch Size 64 -\nTraining Steps 1,000,000 -\nSolver DDIM -\nSampling Steps (DDIM) 20 -\nPlan Horizon (on original data) 101 ( Stitch ), 401 ( Explore ) -\nPlan Horizon (on Daug) 501 (M/L), 1001 (G/Explore) -\nTemporal Jump 26 -\nExecution Parameters\nLow-level Controller Horizon Tuned {5,10,15,20,25}\nReplanning Interval Tuned {50,100,200}\nComputational resources and runtimes. All experiments were conducted using a single NVIDIA\nA10 GPU. The approximate execution times for each component of our method are as follows:\n• Temporal distance-preserving embedding training: 1.5 hours\n16\n--- Page 17 ---\n• Inverse dynamics model training: 0.25 hours\n• Low-level controller training: 2.5 hours\n• Diffusion-based stitcher training: 7 hours\n• Trajectory augmentation via SCoTS: 0.5 hours\n• Diffusion planner training: 18 hours\nThese times are per model training instance or data generation run and may vary slightly depending\non the specific environment and dataset characteristics.\nC Additional Results\nVisualization of temporal distance-preserving latent representations. We train temporal\ndistance-preserving latent representations with dimension 32 across all environments. To visu-\nalize these learned representations, we apply a t-distributed stochastic neighbor embedding (t-SNE)\nto project the 32-dimensional latent vectors onto a 2-dimensional plane, as shown in Figure 8. Recall\nfrom Equation 6 that we parameterize a goal-conditioned value function V(s,g)following (Park\net al., 2024b):\nV(s,g):=−||ϕ(s)−ϕ(g)||2, (13)\nwhich approximates the optimal goal-conditioned value function, defined as the maximum possible\nreturn (cumulative sum of rewards) for sparse-reward settings. Specifically, an agent receives a\nreward of 0if the l2distance between states sandgis within a small threshold δg, and−1otherwise.\nThe embedding function ϕis trained using a temporal-difference objective inspired by implicit\nQ-learning (Kostrikov et al., 2022) on the offline dataset D. As illustrated in Figure 8, the learned\nrepresentations effectively capture the temporal proximity between states, resulting in latent spaces\nwhere states that are temporally close in the environment are also clustered closely in the embedding\nspace.\nVisualization of rollout execution. We visualize a generated plan by the diffusion plan-\nner trained on SCoTS-augmented data, along with its corresponding rollout execution in the\nAntMaze-Giant-Stitch environment, as illustrated in Figure 9. The initial image (top-left) shows\nthe overall planned trajectory generated by the diffusion planner, with subgoals marked by green\nspheres. Subsequent images provide sequential snapshots from the rollout execution, demonstrating\nthe agent actively pursuing and reaching these subgoals. This visualization highlights how effectively\nthe generated high-level plan guides the low-level controller during task execution.\nVisualization of trajectories generated by SCoTS. In Figure 10, 11, and 12, we present rep-\nresentative examples of trajectories synthesized by our SCoTS framework across all considered\nenvironments and dataset types. Compared to the original trajectories provided in Figure 7, the\nSCoTS-generated trajectories clearly demonstrate extended coverage, illustrating the effectiveness of\nour method in augmenting the original offline datasets.\nD Baseline Performance Sources\nPerformance scores reported for offline goal-conditioned reinforcement learning (GCRL) methods,\nincluding Goal-Conditioned Implicit Q-Learning (GCIQL) (Kostrikov et al., 2022), Quasimetric RL\n(QRL) (Wang et al., 2023), Contrastive RL (CRL) (Eysenbach et al., 2022), and Hierarchical Implicit\nQ-Learning (HIQL) (Park et al., 2023a), are sourced from Table 2 in Park et al. (2024a). Scores for\ndiffusion-based generative planning methods explicitly designed for long-horizon generalization,\nincluding Generative Skill Chaining (GSC) (Mishra et al., 2023) and Compositional Diffuser (CD)\n(Luo et al., 2025), are sourced from Tables 1 and 2 in Luo et al. (2025).\n17\n--- Page 18 ---\n0 5 10 15 2005101520\n30\n 20\n 10\n 0 10 2020\n10\n01020\n6\n 4\n 2\n 0 2 4 6 86\n4\n2\n0246\n8\n 6\n 4\n 2\n 0 2 4 6 86\n4\n2\n0246(a) States ( Medium ) (b) PointMaze-Medium-Stitch (c)AntMaze-Medium-Stitch (d)AntMaze-Medium-Explore\n0 5 10 15 20 25 30 350510152025\n40\n 20\n 0 2030\n20\n10\n01020\n5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.06\n4\n2\n0246\n20\n 0 20 40 6040\n30\n20\n10\n0102030\n(e) States ( Large ) (f) PointMaze-Large-Stitch (g)AntMaze-Large-Stitch (h)AntMaze-Large-Explore\n0 10 20 30 40 5005101520253035\n15\n 10\n 5\n 0 5 10 1510\n5\n051015\n60\n 40\n 20\n 0 20 4040\n30\n20\n10\n010203040\n(i) States ( Giant ) (j) PointMaze-Giant-Stitch (k)AntMaze-Giant-Stitch\nFigure 8: Visualization of learned temporal distance-preserving latent representations. The left-\nmost column shows original states from maze environments of varying sizes ( Medium ,Large ,Giant ).\nSubsequent columns illustrate t-SNE projections of latent embeddings ϕ(s)for corresponding OG-\nBench datasets, maintaining the same color scheme for consistency. This visualization demonstrates\nhow spatial proximity and structure in the original state space are preserved and reflected in the\nlearned latent representations.\n18\n--- Page 19 ---\nFigure 9: Visualization of diffusion Planner rollout execution. The top left image shows the\nplanned trajectory generated by the diffusion planner, with subgoals marked by green spheres.\nSubsequent images sequentially illustrate the agent progressing toward these subgoals in the\nAntMaze-Giant-Stitch environment, demonstrating effective guidance provided by the generated\nplan.\n19\n--- Page 20 ---\nFigure 10: SCoTS-augmented trajectories for PointMaze Stitch datasets. For each PointMaze\nStitch dataset, the leftmost column shows trajectories from the original OGBench dataset. The\nsubsequent columns are examples of SCoTS-generated trajectories.\n(a)PointMaze-Medium-Stitch\n(b)PointMaze-Large-Stitch\n(c)PointMaze-Giant-Stitch\n20\n--- Page 21 ---\nFigure 11: SCoTS-augmented trajectories for AntMaze Stitch datasets. For each AntMaze Stitch\ndataset, the leftmost column shows trajectories from the original OGBench dataset. The subsequent\ncolumns are examples of SCoTS-generated trajectories.\n(d)AntMaze-Medium-Stitch\n(e)AntMaze-Large-Stitch\n(f)AntMaze-Giant-Stitch\nFigure 12: SCoTS-augmented trajectories for AntMaze Explore datasets. For each AntMaze\nExplore dataset, the leftmost column shows trajectories from the original OGBench dataset. The\nsubsequent columns are examples of SCoTS-generated trajectories.\n(g)AntMaze-Medium-Explore\n(h)AntMaze-Large-Explore\n21",
  "text_length": 62465
}