{
  "id": "http://arxiv.org/abs/2506.05321v1",
  "title": "LSM-2: Learning from Incomplete Wearable Sensor Data",
  "summary": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.",
  "authors": [
    "Maxwell A. Xu",
    "Girish Narayanswamy",
    "Kumar Ayush",
    "Dimitris Spathis",
    "Shun Liao",
    "Shyam A. Tailor",
    "Ahmed Metwally",
    "A. Ali Heydari",
    "Yuwei Zhang",
    "Jake Garrison",
    "Samy Abdel-Ghaffar",
    "Xuhai Xu",
    "Ken Gu",
    "Jacob Sunshine",
    "Ming-Zher Poh",
    "Yun Liu",
    "Tim Althoff",
    "Shrikanth Narayanan",
    "Pushmeet Kohli",
    "Mark Malhotra",
    "Shwetak Patel",
    "Yuzhe Yang",
    "James M. Rehg",
    "Xin Liu",
    "Daniel McDuff"
  ],
  "published": "2025-06-05T17:57:11Z",
  "updated": "2025-06-05T17:57:11Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05321v1",
  "full_text": "--- Page 1 ---\n2025-6-6\nLSM-2: Learning from Incomplete\nWearable Sensor Data\nMaxwell A. Xu1,3*, †, Girish Narayanswamy1,4*, †, Kumar Ayush1, Dimitris Spathis1, Shun Liao1, Shyam A.\nTailor1, Ahmed Metwally1, A. Ali Heydari1, Yuwei Zhang1, Jake Garrison1, Samy Abdel-Ghaffar1, Xuhai Xu1,\nKen Gu1, Jacob Sunshine1, Ming-Zher Poh1, Yun Liu1, Tim Althoff1, Shrikanth Narayanan2, Pushmeet Kohli2,\nMark Malhotra1, Shwetak Patel1, Yuzhe Yang1, James M. Rehg3, Xin Liu1,◦, Daniel McDuff1,◦\n1Google Research,2Google DeepMind,3University of Illinois Urbana-Champaign,4University of Washington\n†Co-first authors,◦Co-last authors,*Work done during an internship at Google\nFoundation models, a cornerstone of recent advancements in machine learning, have predominantly\nthrived on complete and well-structured data. Wearable sensor data frequently suffers from significant\nmissingness, posing a substantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of Large Sensor Model\n(LSM-2) with Adaptive and Inherited Masking ( AIM), a novel SSL approach that learns robust\nrepresentations directly from incomplete data without requiring explicit imputation. AIM’s\ncore novelty lies in its use of learnable mask tokens to model both existing (\"inherited\") and\nartificially introduced missingness, enabling it to robustly handle fragmented real-world data\nduring inference. Pre-trained on an extensive dataset of 40M hours of day-long multimodal\nsensor data, our LSM-2 with AIMachieves the best performance across a diverse range of tasks,\nincluding classification, regression and generative modeling. Furthermore, LSM-2 with AIM\nexhibits superior scaling performance, and critically, maintains high performance even under\ntargeted missingness scenarios, reflecting clinically coherent patterns, such as the diagnostics\nvalue of nighttime biosignals for hypertension prediction. This makes AIMmore reliable choice\nfor real-world wearable data applications.\n1. Introduction\nIn the real world, missing or incomplete data is a pervasive challenge across a variety of domains. In\nclinical settings for example, electronic health records frequently exhibit missingness due to factors\nsuch as loss to follow-up [ 27,75] or condition-specific diagnostic procedures [ 26,39]. Similarly,\nsensor systems grapple with incomplete data streams due to strategic intermittent deactivation for\nenergy conservation, environmental noise, sensor obstruction, or hardware malfunctions [ 22,7,18].\nMissing data for wearable mobile health sensors is especially prevalent and problematic. In addition\nto the aforementioned causes, user compliance issues (e.g. improper/insecure device attachment)\nor mobile-specific challenges (e.g. data transmission failures, battery charging periods) further\nexacerbate the problem [50, 68].\nSelf-Supervised Learning (SSL) has emerged as a powerful paradigm for learning transferable\nrepresentations by exploiting inherent structures within unlabeled data [ 24]. When scaled to large\npre-training datasets with sufficient compute, these approaches yield foundation models capable\nof strong generalization across diverse downstream tasks [ 45,60]. This is especially promising for\nwearable sensors, where physiological signals contain rich information predictive of diverse health\noutcomes, with several recent large-scale data collection initiatives, such as UK Biobank [ 35], All of Us\n[33], and the Apple Heart and Movement Study [ 62]. This has enabled the development of wearable\nCorresponding author(s): maxu@illinois.edu, girishvn@uw.edu, {xliucs,dmcduff}@google.com\n©2025 Google. All rights reservedarXiv:2506.05321v1  [cs.LG]  5 Jun 2025\n--- Page 2 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nsensor foundation models that generalize across multiple health prediction tasks [42, 70, 52, 1].\nFigure 1|LSM-2 Models Incomplete Data. Our\nmethod uses a learned mask token to represent\nexisting missingness during inference. Then, if\nsensorsaremissing,itcandirectlyreconstructthem\n[L] or classify directly on the incomplete data [R].Unfortunately, state-of-the-art time-series\nSSL approaches typically assume fully-observed\ndata inputs. As such, prior wearable sensor\nfoundation models have handled missingness\nby modeling short context windows (i.e. <60s\n[1], 2.56s [ 70], 10s [47]), where incomplete\ninstances can easily be filtered out. However,\nmany clinically relevant physiological patterns\n(e.g. circadian rhythms [ 76], heart rate variabil-\nity[15],anddailyactivityprofiles[ 32])require\nanalyzing day-long recordings. Unfortunately,\nday-long data inevitably contains missingness\ndue to wearable sensor limitations (e.g. battery\ndrainnecessitatingstrategicsensordeactivation,\nmotion artifacts corrupting signals). As detailed\nin Section3, our dataset exhibits pervasive miss-\ningness: 0% of records are complete. While\nprior work with similar data employed imputa-\ntion methods in order to train their SSL model\n[42], such approaches risk introducing biases\nthat can propagate to downstream models [ 34].\nIn this paper, we introduce the second gen-\neration of Large Sensor Model (LSM-2) based\nonAdaptive and Inherited Masking, AIM, a self-\nsupervised learning approach that learns a representation directly from incomplete data with diverse\nmissingness patterns. To the best of our knowledge, this is the first work to address representation\nlearning directly on incomplete wearable sensor data. Building on masked autoencoder (MAE) pre-\ntraining [ 29],AIMuses a shared learnable mask token to represent both inherited and artificial masks.\nInherited masks are derived from existing missingness in raw data, thereby masking incomplete data\nand avoiding the need for imputation. Artificial masks , are randomly applied on observed tokens,\nproviding a ground truth for the reconstruction pre-training objective. Via AIM’s introduction of\ninherited masks, mask tokens are learned to represent real-world missingness. During evaluation,\nmissingness still occurs in the raw data. Here, the inherited mask allows for missingness-aware\nembeddings. Like real missingness, the number of inherited mask tokens may vary, violating the naive\nMAE’s assumption of a fixed number of masked tokens [ 29]. As such, the adaptive component of\nAIMis able to suppress any additional missing tokens from contributing to the final encoder output,\nensuring that the encoding is a learned representation of the non-missing data solely. This encoding\ncan then be used in conjunction with a linear probe to predict a variety of downstream classification\nand regression tasks, as well as being fed back into the decoder for downstream generative tasks.\nThe key contributions of our work are:\n1.We introduce LSM-2 and propose a novel training strategy, Adaptive and Inherited Masking, AIM,\nthat uses adaptive masking to jointly model artificial and inherited missingness and learn a strong,\ngeneralizable representation, directly on incomplete data. By incorporating adaptive masking\nduring pre-training and inference, our method enables a single model to robustly support a variety\nof downstream tasks under real-world missingness conditions without requiring any explicit\nimputation.\n2.We demonstrate that our LSM-2 w/ AIMpre-trained foundation model achieves state-of-the-art\n2\n--- Page 3 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nperformance across diverse set of tasks (3x classification, 4x generative, 3x regression) that cover\na wide range of semantics (clinical, mental health, wearables, demographics) after large-scale\npre-training on 40 million hours of day-long multimodal sensor data. Our model also demonstrates\nsuperior scaling performance as compared to our prior LSM-1 model [42].\n3.We evaluate the robustness of our LSM-2 across a wide range of targeted missing scenarios , dropping\nout specific sensors or time windows, and we demonstrate much less performance degradation\ncompared to the baseline method that is pre-trained with imputed data. The missingness scenarios\nin which our model does express sensitivity is reflective of physiological domain knowledge,\nproviding interesting insights into the nature of a given prediction target.\n2. Related Work\nSelf-Supervised Learning for Time-Series Foundation Models. Our LSM-2 model utilizes AIM, an\nMAE [29] SSL framework that combines an artificial mask with an inherited mask from real-world\nsensor data. This differs from LSM-1 [ 42], the most closely-related work, which performs MAE\npre-training with just an artificial mask and uses naive imputation to fill in pre-existing missingness,\nboth of which negatively impacts downstream performance (see Section 6). Other MAE-style methods\nfor time series data are limited in that they either: (a) focus exclusively on complete univariate\nsignals [ 20,37,14], (b) work with highly correlated channels from a single modality [ 41], or (c)\nfocus on task-specific forecasting without learning general-purpose embeddings [ 5,44,17]. Notably,\nnone of these approaches handle the missing data patterns inherent in real-world multivariate sensor\ndata. Alternatively, contrastive SSL methods learn representations by attracting positives and repelling\nnegatives in embedding space. Positives are generated via augmentations [ 59] or sampling using\ntemporal proximity [ 61], subject labels [ 1], domain knowledge [ 47], or motif similarity [ 69,70].\nHowever, these require strong assumptions, either carefully designed augmentations or reliable\npositive selection strategies and are unable to do reconstruction out-of-box unlike the MAE methods.\nLearning from Incomplete Multimodal Data. Our model learns general-purpose embeddings\ndirectly from incomplete multimodal time-series data through self-supervised pre-training, enabling\neffective transfer to diverse downstream tasks via simple linear probes. Existing representation\nlearning works for incomplete data have focused primarily on either tabular data [ 12] or irregularly-\nsampled event time-series [ 8], both of which differ fundamentally from wearable sensors. Tabular\nmissingness consists of simple, scattered, point-wise missingness, unlike the complex structured\npatterns in wearables, in which sensor groups across a time window will be missing and not at\nrandom (Figure 2). While the irregularly-sampled domain shares some similarities, they have\nfundamentally different data characteristics. Irregularly sampled time-series such as ICU lab testing\n[56] are collected at arbitrary intervals with all other modalities typically missing, whereas wearables\nproduce regularly-sampled data where some modalities will drop out in structured groups (Figure 2).\nAlternatively, a separate body of incomplete multimodal data work has focused on learning\nimputation methods. The most relevant work is ReMasker [ 23], which combines inherited and\nartificial masking in an MAE framework. Our approach differs in three fundamental aspects: (1) we\noptimize for representation learning rather than imputation, (2) we handle the complex missingness\npatterns characteristic of multimodal time-series (see Fig. 2), as opposed to the simpler point-wise\nmissingnessintabulardata, and(3) wescaleefficientlytolongsequences(N=3744tokens)compared\nto their limited context (N<20 tokens), representing a 35000x increase in compute (see Section 4\nfor details). Another approach, [ 65], similarly uses both inherited and artificial masks but limits\nattention to handcrafted time points (N=206) and uses self-attention blocks. While numerous deep\nlearning methods exist for multivariate time-series imputation [ 72,10,49,16], these approaches\nfocus solely on reconstruction quality and fail to produce general-purpose embeddings necessary\n3\n--- Page 4 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nSensor Data Present Sensor Data Missing\nFigure 2|The Fragmented Nature of Sensor Data. Multimodal time-series sensor data frequently\ncontains missing observations. Missingness can take several modes. In wearable data, these modes\ntake the form of temporary periods in which a sensor(s) are off, periods in which the device is not\nwarn, and measurements that are filtered out because they are clearly spurious/out of range.\nfor foundation models. [ 34] investigate various imputation methods and train classifiers on the\nreconstructed data, but do not learn representations for multiple downstream tasks. In contrast, our\nwork handles real-world missingness patterns within a scalable representation learning framework.\n3. Large Scale Incomplete Wearable Data\n0 20 40 60 80\nMissingness (%)0246Amount (%)/uni00000079/uni000000d8/uni0000012e/uni00000002/uni00000080/uni000000b4/uni0000010b/uni0000012b/uni00000106/uni000000d8/uni00000002/uni00000059/uni000000f4/uni00000132/uni00000132/uni000000f4/uni0000010c/uni000000ec/uni0000010c/uni000000d8/uni00000132/uni00000132/uni00000002/uni00000311\nMean\nFigure 3|Distribution of Missing-\nness % Per Sample. Mean 49%, Me-\ndian 48%, Std Dev 15%, Minimum\n2%, Maximum 80%. Samples with\n>80%missingness are discarded.Data Summary. A primary contribution of our work is in\nmodeling incomplete data during pre-training, post-training,\nand inference. To validate our method we curate a large,\nunlabeled, pre-training dataset in addition to two labeled\ndatasets for downstream tasks. Each data sample contains\n26 minutely aggregated features from a set of 5 sensors\n(photoplethysmography, accelerometer, skin conductance,\naltimeter, and temperature) for a time span of 1440 minutes\n(1 day). A core property of these data is that they have\ncomplex, structured missingness patterns. A representative\nexample of sensor data with missingness can be seen in\nFig. 2, along with the missingness distribution and statistics\nshown in Fig. 3. Missing data is ubiquitous in long-duration\nwearable sensor recordings, with 0% of samples over our\nentire dataset of 1.6 million instances of 1 day data. All pre-\ntrained and downstream datasets utilize similar devices and\nthus are subject to similar missingness patterns. Please refer\nto the Appendix for further data descriptions and statistics.\nPre-training Data. For pre-training, we used a de-identified dataset collected between March\n1stand May 1st2024 inclusive. The dataset included 3,581,748 person-days (or 40 million hours)\nsampled at minutely resolution from 60,440 people (37,352 men, 23,041 women, 47 unspecified). A\nmean of 59 days (min: 1, max: 93) were contributed per person with standard deviation of 32 days.\nAll data used in this study were collected with the informed consent of research participants. This\nconsent permits the use of data to generate findings for publication in scientific journals and other\n4\n--- Page 5 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\noutlets, contributing to general knowledge about health and science. The mean reported participant\nage was 42.5 years (min: 18, max: 96 years, st.dev.: 12.6). The population reflects a wide range of\nbody-mass index (BMI) values with 37% healthy, 34% overweight and 25% obese in the training set\nand a similar cross-section in the validation set.\nDownstream Metabolic Study Data. These data come from an IRB approved observational\nstudy of adults in the United States. We enrolled 4,416 participants, of which 1,250 had wearable\ndata, labels and were included in our analysis. Demographics (age, BMI) and medical conditions\n(hypertension, anxiety) were collected via self-report.\nDownstream Activity Study Data. These data come from the same source as our pretraining\ndata. We randomly sampled approximately 5,000 examples for each of 20 activities for training\nand 1,000 examples of each activity for testing. The training and testing data were sampled in\nperson-independent manner. The activities were from the following classes: Walking\n ,Bike\n,Playing\nSports\n ,Running\n ,Aerobics\n ,Elliptical\n ,Spinning\n ,Weightlifting\n ,Swimming\n ,Hiking\n ,\nPlaying Tennis\n ,CrossFit\n ,Pilates\n ,Stairclimber\n ,Dancing\n ,Indoor climbing\n ,Golf\n,Skiing\n ,\nSnowboarding\n , andKayaking\n . In total, 104,086 activities were sampled from 46,199 people. The\nmean duration per activity was 66 minutes (min: 20 minutes, max: 360).\n4. Learning to AIMwith Adaptive Inherited Masking\nMotivation. As sensor data frequently exhibits inherent missingness, our key idea is to inherit these\nmissingness patterns to be used in conjunction with a masked pre-training framework [ 30]. These\nmethods introduce an artificial mask on the present data and learn to reconstruct them. Artificial\nmissingness sits in contrast to inherited missingness inherent to the data. Similar to the original MAE\nwork [29], our method implements an transformer-based encoder-decoder structure.\nOur method first takes an input matrix of sensor features, which are then tokenized to be\nX∈ℝ𝐵×𝑁×𝐸(𝐵is batch size, 𝑁is number of tokens, and 𝐸is embedding dimension). We then define\na binary vector mask, M∈{0,1}𝐵×𝑁(where 1 is masked and 0 is non-masked) equal in length to the\nnumber of tokenized sensor inputs, where masked tokens are ignored by the encoder. Our method\nsetsMas the union of the inherited and artificial masks such that:\nM=Minherited∨Martificial\nThe inherited mask, Minherited, is the original, existing missingness present in the dataset. The artificial\nmask,Martificial, is a simulated missingness on observed data. Critically, this inclusion of the inherited\nmask ensures that the encoder exclusively learns representations from reliable sensor data without\ncontamination from imputation artifacts.\nTable 1|Capabilities of Different Masking Im-\nplementations. We combine dropout removal’s\nefficiency [30] with attention masking’s flexibility\n[23] to allow us to process to long sequences with\ninherited masks that have varying mask %.\nMasking ComplexityFixed\nMask %Dynamic\nMask %Allows\nInherited\nMask\nDropout Removal [30] 𝑂((𝑁−𝐷)2)✓ ✗ ✗\nAttention Mask [23] 𝑂(𝑁2) ✓ ✓ ✓\nAIM(ours)𝑂((𝑁−𝐷)2)✓ ✓ ✓\n𝑁: Number of tokens 𝐷: Number droppedBackground. The original MAE work [ 30]\nimplements masking through dropout removal ,\nwhere masked tokens are not passed through\nthe encoder. Specifically it assumes that a fixed\nnumber of tokens 𝐷are dropped for every sam-\nple, such thatÍ𝑁\n𝑛=1M[𝑏,𝑛]=𝐷∀𝑏∈[1,𝐵]. The\ntransformer encoder input can then be formu-\nlated asX[M,:]∈ℝ𝐵×(𝑁−𝐷)×𝐸. This reduces the\ntransformer’s computational complexity from\n𝑂(𝑁2)→𝑂((𝑁−𝐷)2), which translates to 25x\nless computation when masking 80% of tokens\n5\n--- Page 6 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nDiscriminative Tasks\nHype�ension\nAnxiety\nActivities (Multi-Class)\nRegression Tasks\nAge\nBMI\nInsulin Resistance\nFigure 4|LSM-2 Pre-training with AIM[A-F] and Evaluation [G,H] . Our mask is a union of [A]\ninherited missingness from real-world noise and [B]artificial masking of observed data. Both are\nmodeled with identical, learnable tokens. Because the inherited mask introduces variable masking,\n[C]we first remove 𝐷(size of artificial mask) tokens and [D]then use an attention mask to remove the\nremaining. [E]Dropped tokens are reinserted before [F]the final reconstruction. [G]Reconstruction\nerror is computed only on artificial masks with known ground truth. [H]For predictive evaluations, a\nlinear probe is trained on a pooled representation of the non-missing data.\n(𝐷=0.8𝑁). While efficient, this approach re-\nquires fixed masking amount 𝐷, in order to construct batched encoder input X[M,:]with𝐵 >1. The\nmotivation of our AIMapproach is to include inherited masking in the MAE procedure in order to\nmodel real-world missingness. However, we are unable to do so with dropout removal because the\namount of pre-existing missingness will vary, and causing the 𝐷of the inherited mask also vary. Recent\nmethods have attempted to handle variable masking [ 23] by utilizing the transformer’s attention\nmasking mechanism [ 64]. While flexible, these methods fail to use dropout removal, making them\ncomputationally prohibitive for long sequences and large scale pre-training.\nAdaptive Attentive Masking Design. Our key insight is to combine both masking modes in a\nunified approach: we maintain dropout removal’s efficiency while incorporating the flexibility of\nattention masking. This hybrid strategy is visualized in Figure 4. Dropout removal limits the number\nof tokens that must be encoded to the lower bound of artificially masked tokens. This is because the\nset of dropped tokens 𝐷is static. In scenarios where a sample has no inherent missing data, these\ndropped tokens must be entirely defined by the artificial mask. In practice, dropped-out tokens can be\na mix of inherited and artificially masked tokens. Similarly, the remaining masked tokens, which are\ndisregarded using the transformer’s attention mask, can also be of either type. This fusion provides\nthe benefits of both paradigms while mitigating their individual limitations.\nUnified Framework for Pre-training and Evaluation. AIMprovides a unified framework for\nLSM-2 that consistently handles missing data during both pre-training and inference. The full pre-\ntraining procedure can be seen in Figure 4 [A-G]. During pre-training, the adaptive masking not\nonly enables the inclusion of varying inherited mask sizes, but also allows the artificial masking to\ninclude a mix of strategies with differing masking percentages. Our artificially masking mix seeks to\nmodel the real-world missingness patterns shown in Figure 2. The mix includes (1) 80% random\nimputation masking (to model noise), where a random patch is masked, (2) 50% temporal slice\nmasking (to model off body), where all sensors at a random time point are masked, and (3) 50%\nsignal slice masking (to model sensor off), where all time points for a random sensor are masked.\nEach instance uses a randomly selected masking strategy with equal probability. The specific masking\n6\n--- Page 7 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\npercentages were identified via an ablation study, reported within the Appendix. As such, we set\n𝐷=0.5𝑁, boosting our computational efficiency by 4x.\nCrucially, AIM’s adaptive masking is also used during evaluation, which can be seen in Figure 4\n[G,H]. The pre-trained model is then able to operate directly on incomplete multimodal sensor data by\ndynamically attending only to observed segments. This eliminates the need for external preprocessing,\nsuch as imputing or discarding missing values, and ensures generalization from pre-training to\ndownstream deployment in real-world settings.\n5. Experiments\nPre-training Set-up. We pre-train LSM-2 on minutely multimodal wearable data ( A∈ℝ𝑇×𝑆) where\n𝑆=26sensor features and 𝑇=1440minutes. Inputs are tokenized using a ViT-1D [ 21,1] encoder\nwith a 1D patch size of 10 minutes, resulting in 3744 tokens (144 tokens per signal). We apply a\nshared kernel across channels and use a 2D positional embedding to encode time and signal identity.\nThemodelhas25Mparameters, 384-dhiddensize, 12encoderlayers, and4decoderlayers. Following\nSection 4, we apply a composite mask (80% random, 50% temporal, 50% signal slices) and optimize\nmean squared error over masked patches on reconstruction. Notably, we do not back-propagate on\nmissing pixels for any of the SSL methods trained including baselines. Training is performed on 8x16\nGoogle v5e TPUs with a batch size of 512 for 100K steps. SSL baselines—LSM-1 [ 42], SimCLR [ 13],\nDINO [11], and MSN [ 6]—are trained from scratch using the same setup unless otherwise noted.\nLSM-1 uses a ViT-2D with a (10,2) patch size and 0.8 random masking, while the contrastive methods\nrely on jittering, scaling, and time-flipping augmentations [ 59,38,74,51]. All baselines use imputed\ndata to meet their full-input requirement. See Appendix for further implementation details.\nDownstream Evaluation. We evaluate LSM-2 across three downstream targets: generative,\nclassification, and regression. For generative , we assess reconstruction under structured missing-\nness patterns: (1) random imputation (30%, 50%, 80%), (2) temporal interpolation (contiguous\nmasked windows of 10, 30, or 60 minutes), (3) temporal extrapolation (masked window at the\nend of the sequence), and (4) signal imputation (masking 2/26, 6/26, or 12/26 channels). Since\ncontrastive baselines lack reconstruction objectives, we compare against LSM-1 [ 42] in addition to\nsimple imputation methods used in practice—Linear Interpolation, Nearest Neighbors, and Mean\nFilling—under the same union masking scheme. We omit MICE [ 63] due to its missingness at random\nassumptions not holding and its lower performance in prior work [ 42]. For classification , we\naverage embeddings over non-inherited-masked tokens and apply a trainable linear probe; LSM-1\npools across all tokens, and contrastive methods use the CLS token. We report F 1, Accuracy, Balanced\nAccuracy, and AUROC on targets including hypertension, anxiety (Metabolics dataset; see Section 3),\nand 20-class activity recognition (Activity dataset). For regression , we follow the same setup\nwith a linear regression probe and report MAE and Pearson correlation on BMI and age (Metabolics\ndataset). See Appendix for further details.\n6. Results and Discussion\nGeneralizability across classification, generative, and regression tasks. LSM-2 with AIMlearns a\nstrong generalizable representation, useful for classification, regression and generative tasks (Tables\n2, 3, 4 respectively). This research presents preliminary findings and should not be interpreted as\nproviding diagnostic tools or recommendations.\nDue to our improved pre-training reconstruction objective, LSM-2 obtains much stronger genera-\ntive results compared to the prior state-of-the-art work - LSM-1 [ 42] which was limited in its masking\n7\n--- Page 8 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nTable 2|Classification Task Results\nHypertension (2) Anxiety (2) Activity Recognition (20)\nMethod↑↑F1↑↑Acc↑↑BAcc↑↑AUC↑↑F1↑↑Acc↑↑BAcc↑↑AUC↑↑F1↑↑Acc↑↑BAcc↑↑AUCSTResNet 0.516 0.529 0.587 0.624 0.645 0.655 0.651 0.709 0.729 0.721 0.734 0.965\nViT1D 0.481 0.516 0.509 0.520 0.583 0.597 0.586 0.620 0.351 0.367 0.374 0.863LPSimCLR 0.501 0.524 0.548 0.568 0.594 0.603 0.601 0.636 0.098 0.109 0.124 0.603\nDINO 0.487 0.536 0.504 0.510 0.551 0.557 0.562 0.582 0.102 0.110 0.124 0.635\nMSN 0.512 0.553 0.538 0.552 0.579 0.585 0.588 0.622 0.108 0.118 0.125 0.662\nLSM-1 0.640 0.676 0.682 0.739 0.670 0.678 0.678 0.743 0.470 0.470 0.489 0.900\nLSM-2 0.651 0.687 0.693 0.754 0.683 0.690 0.692 0.758 0.474 0.472 0.493 0.899\nΔLSM-1+1.7% +1.6% +1.6% +2.0% +1.9% +1.8% +2.1% +2.0% +0.8% +0.4% +0.8% -0.1%\nMetrics: F 1Score, Accuracy, Balanced Accuracy, AUROC with Macro One-vs-Rest |Tasks: 20-class Activity Recognition, rest are binary |Methods: Supervised\nTraining (ST), Linear Probe (LP).\nTable 3|Generative Task Results\n↓↓Random Imp.↓↓Temporal Interp.↓↓Temporal Extrap.↓↓Signal Imp.\nMethod 30% 50% 80% 10m 30m 60m 10m 30m 60m 2 6 12\nLinear Int. 0.57 0.62 0.74 0.42 0.56 0.70 0.47 0.64 0.82 NA NA NA\nNN Fill 0.70 0.76 0.90 0.52 0.69 0.84 0.47 0.64 0.82 NA NA NA\nMean Fill 0.92 0.96 0.93 0.79 0.80 0.84 0.78 0.80 0.83 1.28 1.30 1.29\nLSM-1 0.21 0.24 0.30 0.49 0.55 0.60 0.45 0.52 0.56 0.73 0.58 0.45\nLSM-2 0.180.200.200.260.370.450.280.380.480.170.210.27\nΔLSM-1 +14%+17%+33% +47%+31%+25% +38%+27%+14% +77%+64%+40%\nMetrics: Mean Squared Error |Tasks: Random Imputation (30%, 50%, 80% missing), Temporal Interpolation/Extrapolation (10, 30, 60 missing minutes), Signal Imputation (2, 6, or 12\nout of 26 missing modalities) |Methods: Linear interpolation, Nearest neighbor fill, Mean filling\nstrategy (artificial random imputation masking). By introducing a mixture of artificial masking strate-\ngies with flexible missing ratios, as well as the inclusion of the inherited mask, not only do we achieve\na+33% performance increase on the 80% random imputation evaluation, but we also achieve\nstrong benefits across different generative tasks, with +77% improvement in 2 signal imputation\nand a+47% improvement in 10 minute temporal interpolation. This demonstrates that explicitly\nmodeling diverse missingness patterns during pre-training leads to more robust representations that\ngeneralize better to real-world scenarios with complex data gaps.\nTable 4|Regression Task Results\nAge BMI\nMethod↓↓MAE↑↑Corr↓↓MAE↑↑CorrSTResNet 7.43 0.618 5.07 0.515\nViT1D 9.65 0.132 6.06 0.047LPSimCLR 9.21 0.345 5.85 0.235\nDINO 9.69 0.112 5.97 0.122\nMSN 9.42 0.255 5.84 0.250\nLSM-1 6.41 0.728 4.39 0.667\nLSM-2 6.49 0.722 4.38 0.673\nΔLSM-1 -1.2% -0.8% +0.2% +1.0%\nMetrics: Mean Absolute Error, Pearson Correlation |Methods: Supervised Training\n(ST), Linear Probe (LP).Despite being pre-trained on with a reconstruc-\ntion objective, LSM-2 achieves SOTA performance\nacross classification tasks, beating all other self-\nsupervised learning baselines . Even with a sim-\nple linear probe and frozen features, our model sur-\npasses fully supervised baselines on hypertension\nand anxiety prediction — two challenging tasks that\npreviously required hand-crafted features or custom\narchitectures [ 55,2]. This suggests that pre-training\nhelps avoid overfitting and enables the model to cap-\nture subtle physiological cues that generalize across\nconditions. The strong results across both binary (hy-\npertension/anxiety) and multi-class (activity recogni-\ntion)tasksindicatethatthemodellearnshierarchical\n8\n--- Page 9 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nFigure 5|Scaling Performance of Our Model. LSM-2 model achieves better scaling than LSM-1\nacross all dimensions: subjects,data,compute , andmodel size . LSM-2 uses a mixed masking strategy\nduring pre-training, but here we report only random imputation loss to match LSM-1.\nfeatures suited to different levels of task complexity.\nIn regression tasks, LSM-2 improves correlation on BMI by +1.0%, while underperforming on\nage prediction by -0.8%. Since the absolute metric (e.g., mean absolute error) is affected by differing\ntarget scales (e.g., Age: 18–90 vs. BMI: 12–65), correlation offers a clearer view of model quality.\nStrongscalingperformanceon40millionhoursofincompletedata. Figure5showthatour AIM\nscales more effectively than the LSM-1 model across 4 different dimensions: subject, data, compute,\nand model. The LSM-1 model exhibits scaling saturation for the data and compute dimensions, but\nour model’s trend indicates a more aggressive downwards slope that has not yet saturated. These\nresults are promising as they suggest that continued investment in larger datasets and compute may\nyield further performance gains, indicating that our method has not yet reached its limits.\nStrongRobustnesstoTargetedMissingness. LSM-2with AIMdemonstratessubstantiallygreater\nresilience to targeted missingness compared to prior work, as seen in Figure 6. Across 11 out of 12\nmissingness scenarios, our model consistently maintains stronger performance. For example, when\naccelerometry is removed—a key sensor for activity recognition—our model’s F 1score drops from\n0.47 to 0.20 ( −57%), while LSM-1 degrades more severely from 0.47 to 0.14 ( −71%). Notably, even\nin this degraded setting, our model still outperforms LSM-1 by +47%in absolute terms. A similar\ntrend holds across other modalities: removing PPG during hypertension prediction leads to only a\n−6% drop for AIM(0.65 to 0.61), compared to −11% for LSM-1 (0.64 to 0.57).\nRobustness also generalizes across temporal ablations. While both models reach similar peak\nactivity recognition scores ( ∼0.47 F 1), our model maintains an average F 1of 0.43 across temporal\nablations—substantially higher than LSM-1’s 0.26 ( +65%relative gain). Overall, these results\nvalidate the effectiveness of our adaptive masking strategy in modeling missingness patterns. Our\nmodel experiences 73% smaller performance drops across all 12 ablation settings and retains +15%\nhigherabsolute performance in degraded states. This combination of robustness and accuracy makes\nAIMa more reliable choice for real-world deployment, where missing data is a reality.\nReflects Physiological Domain Knowledge and Other Real-world Implications. The targeted\nmissingness experiments in Figure 6 also reveal clinically coherent patterns with real-world implica-\ntions. LSM-2’shypertensionandanxietypredictionsshowtheexpectednocturnaladvantage, suchthat\nthe removal of nighttime signals has 5% degradation in F 1for both targets, compared to an average\n0.4% and 0.01% degradation for the daytime windows for each target. This finding strongly aligns\nwith clinical literature demonstrating the diagnostic value of nighttime biosignals for hypertension\n[71,28] and stress prediction [ 36,25], which are less affected by daily activity artifacts and better\ncapture underlying pathophysiology.\nInterestingly, LSM-2 also demonstrates a large 11% drop in performance for anxiety prediction\n9\n--- Page 10 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nHypertensionAnxietyActivity\nRemoved Sensor\nRemoved Temporal WindowF1 ScoreSensor RemovalF1 ScoreTemporal Window Removal\nFigure 6|Robustness to Targeted Missingness. In sensor removal, all signals derived from the\nspecificsensorareremoved. Intemporalwindowremoval, allsignalsareremovedatagiventimeframe\n(Morning [8am-12pm], Afternoon [12pm-4pm], Evening [4pm-8pm], Night [8pm-8am]). The dotted\nlinedenotesamodeltrainedonallmodalities. Whenevaluatingwithsimulatedsensor-ortime-specific\nmissingness, LSM-2 maintains consistent performance while LSM-1 degrades significantly. Where\nLSM-2 does show sensitivity, it aligns with domain knowledge. For example, nighttime BP’s stronger\npredictive power of hypertension over daytime [ 28], accelerometry’s role in distinguishing anxiety\nfrom physiological stress responses [54].\nafter removing the accelerometry sensor, whereas removing the other sensors only results in an\naverage 0.5% drop. This suggests accelerometry provides unique signals for anxiety detection that are\nnot captured by other modalities. There have been recent research works [ 54,67] that demonstrate\nthe importance of utilizing accelerometry sensors in stress prediction in order to distinguish anxiety\nand mental stress from physiological stress responses from physical activity.\nThese results demonstrate three key advantages of our AIMadaptive masking approach: (1)\nperformance degrades proportionally to a sensor’s clinical importance, (2) cross-modal relationships\nare maintained when inputs are missing, and (3) known temporal biases in physiological data are\npreserved. This robustness is crucial for real-world deployment where missing data is inevitable,\nmaking AIMsignificantly more reliable in field settings.\nTable 5|Ablation Study\nGenerative (↓↓MSE)Classification (↑↑F1)\n80%\nR.Imp.60m\nT.Interp.Anxiety Activity\nAIM 0.20 0.45 0.683 0.474\nw/o Inheritance 0.28 0.62 0.671 0.445\nw/o Mixing 0.19 0.58 0.637 0.460Importance of Inheritance and Mask Mixing.\nAIMis composed of two main components: (1) in-\nclusion of an Inherited Mask and (2) usage of a mix\nof artificial masking with randomly using either 80%\nrandom imputation, 50% temporal slices, or 50%\nsignals slices. In Table 5, we show how removing\ninheritance leads to performance degradation across\n10\n--- Page 11 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nall of the various tasks. Without mixing, only an 80% random imputation pre-training task is used,\nmatching prior work[ 42]. While the random imputation performance improves, all other tasks\ndegrade, including the other generative task, temporal interpolation.\nLimitations and Future Work. Our study has several important constraints. First, training and\nevaluation were limited to a specific private datasets, necessitating future work on exploring other\ndatasets with complex missingness patterns, such as All of Us [ 33], and understanding missingness\ndistribution shifts. Furthermore, we make use of minutely aggregated features, which is helpful\nfor helping us model our 1-day longer time-scale day data, but uncommon in the broader wearable\nsensing space, which focuses primarily on raw high frequency sensor signal. Unfortunately, this is a\npractical limitation, as data is not stored in its raw form at such scale. Finally, although the focus\nof our work is on multimodal sensor data, our technique is broadly applicable and domain-agnostic\nrequiring only that the data contains existing missingness. Therefore, future work can explore the\napplication of our AIMacross different missingness-afflicted domains.\n7. Conclusion\nIn this work, we introduced the second generation of Large Sensor Model (LSM-2) with Adaptive\nandInherited Masking, AIM, a novel self-supervised learning approach designed to learn robust\nrepresentations directly from incomplete wearable sensor data. By integrating both inherited (real-\nworld) and artificial masking strategies, AIM eliminates the need for explicit imputation while\neffectivelymodelingthepervasivemissingnessinreal-worldsensordata. Ourexperimentsdemonstrate\nthat our foundation model LSM-2, pre-trained with AIM, achieves state-of-the-art performance and\nscaling capability across a diverse range of tasks across differing semantics. Our targeted missingness\nexperiments reveal that LSM-2 maintains strong performance even when entire sensors are dropped,\nsuggesting broad applicability to scenarios with varying sensor availability. Our model’s strong\nperformance under real-world missingness conditions demonstrates its practical applicability, and we\nhope the insights in our work will guide future work in machine learning methodologies for wearable\nsensors and health time-series.\nReferences\n[1]S. Abbaspourazad, O. Elachqar, A. C. Miller, S. Emrani, U. Nallasamy, and I. Shapiro. Large-scale\ntraining of foundation models for wearable biosignals. arXiv preprint arXiv:2312.05409 , 2023.\n[2]A. Abd-Alrazaq, R. AlSaad, S. Aziz, A. Ahmed, K. Denecke, M. Househ, F. Farooq, and J. Sheikh.\nWearable artificial intelligence for anxiety and depression: scoping review. Journal of Medical\nInternet Research , 25:e42672, 2023.\n[3]A. Afdala, N. Nuryani, and A. S. Nugroho. Automatic detection of atrial fibrillation using basic\nshannon entropy of rr interval feature. In Journal of Physics: Conference Series , volume 795,\npage 012038. IOP Publishing, 2017.\n[4]M. Amiri and R. Jensen. Missing data imputation using fuzzy-rough methods. Neurocomputing ,\n205:152–164, 2016.\n[5]A. F. Ansari, L. Stella, C. Turkmen, X. Zhang, P. Mercado, H. Shen, O. Shchur, S. S. Rangapuram,\nS. P. Arango, S. Kapoor, et al. Chronos: Learning the language of time series. arXiv preprint\narXiv:2403.07815 , 2024.\n11\n--- Page 12 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n[6]M. Assran, M. Caron, I. Misra, P. Bojanowski, F. Bordes, P. Vincent, A. Joulin, M. Rabbat, and\nN. Ballas. Masked siamese networks for label-efficient learning. In European conference on\ncomputer vision , pages 456–473. Springer, 2022.\n[7]S. Bähr, G.-C. Haas, F. Keusch, F. Kreuter, and M. Trappmann. Missing data and other mea-\nsurement quality issues in mobile geolocation sensor data. Social Science Computer Review ,\n40(1):212–235, 2022.\n[8]N. Beebe-Wang, S. Ebrahimi, J. Yoon, S. O. Arik, and T. Pfister. Paits: pretraining and augmenta-\ntion for irregularly-sampled time series. arXiv preprint arXiv:2308.13703 , 2023.\n[9]G. Bleser, D. Steffen, A. Reiss, M. Weber, G. Hendeby, and L. Fradet. Personalized physical\nactivity monitoring using wearable sensors. Smart health: Open problems and future challenges ,\npages 99–124, 2015.\n[10]W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li. Brits: Bidirectional recurrent imputation for\ntime series. Advances in neural information processing systems , 31, 2018.\n[11]M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international\nconference on computer vision , pages 9650–9660, 2021.\n[12]L.-W. Chang, C.-T. Li, C.-P. Yang, and S.-d. Lin. Learning on missing tabular data: Attention\nwith self-supervision, not imputation, is all you need. ACM Transactions on Intelligent Systems\nand Technology .\n[13]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on machine learning , pages 1597–1607.\nPmLR, 2020.\n[14]H.-Y. S. Chien, H. Goh, C. M. Sandino, and J. Y. Cheng. Maeeg: Masked auto-encoder for eeg\nrepresentation learning. arXiv preprint arXiv:2211.02625 , 2022.\n[15]H. ChuDuc, K. NguyenPhan, and D. NguyenViet. A review of heart rate variability and its\napplications. APCBEE procedia , 7:80–85, 2013.\n[16]Z. Dai, E. Getzen, and Q. Long. Sadi: Similarity-aware diffusion model-based imputation for\nincomplete temporal ehr data. In International Conference on Artificial Intelligence and Statistics ,\npages 4195–4203. PMLR, 2024.\n[17]A. Das, W. Kong, R. Sen, and Y. Zhou. A decoder-only foundation model for time-series\nforecasting. In Forty-first International Conference on Machine Learning , 2024.\n[18]T. Decorte, S. Mortier, J. J. Lembrechts, F. J. Meysman, S. Latré, E. Mannens, and T. Verdonck.\nMissing value imputation of wireless sensor data for environmental monitoring. Sensors,\n24(8):2416, 2024.\n[19]C. M. DeGiorgio, P. Miller, S. Meymandi, A. Chin, J. Epps, S. Gordon, J. Gornbein, and R. M.\nHarper. Rmssd, a measure of vagus-mediated heart rate variability, is associated with risk factors\nfor sudep: the sudep-7 inventory. Epilepsy & behavior , 19(1):78–81, 2010.\n[20]J. Dong, H. Wu, H. Zhang, L. Zhang, J. Wang, and M. Long. Simmtm: A simple pre-training\nframework for masked time-series modeling. Advances in Neural Information Processing Systems ,\n36:29996–30025, 2023.\n12\n--- Page 13 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n[21]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n[22]J. Du, M. Hu, and W. Zhang. Missing data problem in the monitoring system: A review. IEEE\nSensors Journal , 20(23):13984–13998, 2020.\n[23]T. Du, L. Melis, and T. Wang. Remasker: Imputing tabular data with masked autoencoding.\narXiv preprint arXiv:2309.13793 , 2023.\n[24]L. Ericsson, H. Gouk, and T. M. Hospedales. How well do self-supervised models transfer?\nInProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n5414–5423, 2021.\n[25]J. Fan, J. Mei, Y. Yang, J. Lu, Q. Wang, X. Yang, G. Chen, R. Wang, Y. Han, R. Sheng, et al.\nSleep-phasic heart rate variability predicts stress severity: Building a machine learning-based\nstress prediction model. Stress and Health , 40(4):e3386, 2024.\n[26]E.Ford, P.Rooney, P.Hurley, S.Oliver, S.Bremner, andJ.Cassell. Cantheuseofbayesiananalysis\nmethods correct for incompleteness in electronic health records diagnosis data? development\nof a novel method using simulated and real-life clinical data. Frontiers in Public Health , 8:54,\n2020.\n[27] S. Haneuse, D. Arterburn, and M. J. Daniels. Assessing missing data assumptions in ehr-based\nstudies: a complex and underappreciated task. JAMA Network Open , 4(2):e210184–e210184,\n2021.\n[28]T. W. Hansen, Y. Li, J. Boggia, L. Thijs, T. Richart, and J. A. Staessen. Predictive role of the\nnighttime blood pressure. Hypertension , 57(1):3–10, 2011.\n[29]K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision\nlearners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 16000–16009, 2022.\n[30]K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision\nlearners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 16000–16009, 2022.\n[31]K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR,\n2016.\n[32]A. Hecht, S. Ma, J. Porszasz, R. Casaburi, C. C. R. Network, et al. Methodology for using\nlong-term accelerometry monitoring to describe daily activity patterns in copd. COPD: Journal\nof Chronic Obstructive Pulmonary Disease , 6(2):121–129, 2009.\n[33]H. Jeong, A. Roghanizad, H. Master, and et al. Data from the All of Us research program\nreinforces existence of activity inequality. npj Digital Medicine , 8(8), 2025.\n[34]J.Jungo,Y.Xiang,S.Gashi,andC.Holz. Representationlearningforwearable-basedapplications\nin the case of missing data. arXiv preprint arXiv:2401.05437 , 2024.\n[35]M. Katori, S. Shi, K. Ode, Y. Tomita, and H. Ueda. The 103,200-arm acceleration dataset in the\nuk biobank revealed a landscape of human sleep phenotypes. Proceedings National Academy of\nScience, U.S.A. , 119(12), 2022.\n13\n--- Page 14 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n[36]H. Kinnunen, A. Rantanen, T. Kenttä, and H. Koskimäki. Feasible assessment of recovery and\ncardiovascular health: accuracy of nocturnal hr and hrv assessed via ring ppg in comparison to\nmedical grade ecg. Physiological measurement , 41(4):04NT01, 2020.\n[37]Z. Li, Z. Rao, L. Pan, P. Wang, and Z. Xu. Ti-mae: Self-supervised masked time series autoen-\ncoders. arXiv preprint arXiv:2301.08871 , 2023.\n[38]Z. Liu, A. Alavi, M. Li, and X. Zhang. Guidelines for augmentation selection in contrastive\nlearning for time series classification. arXiv preprint arXiv:2407.09336 , 2024.\n[39]M. McDermott, B. Nestor, E. Kim, W. Zhang, A. Goldenberg, P. Szolovits, and M. Ghassemi.\nA comprehensive ehr timeseries pre-training benchmark. In Proceedings of the Conference on\nHealth, Inference, and Learning , pages 257–278, 2021.\n[40]S. Mekruksavanich, A. Jitpattanakul, K. Sitthithakerngkiet, P. Youplao, and P. Yupapin. Resnet-\nse: Channel attention-based deep residual network for complex activity recognition using\nwrist-worn wearable sensors. IEEE Access , 10:51142–51154, 2022.\n[41]Y. Na, M. Park, Y. Tae, and S. Joo. Guiding masked representation learning to capture spatio-\ntemporal relationship of electrocardiogram. arXiv preprint arXiv:2402.09450 , 2024.\n[42]G. Narayanswamy, X. Liu, K. Ayush, Y. Yang, X. Xu, S. Liao, J. Garrison, S. Tailor, J. Sunshine,\nY. Liu, et al. Scaling wearable foundation models. arXiv preprint arXiv:2410.13638 , 2024.\n[43]G.Narayanswamy,Y.Liu,Y.Yang,C.Ma,X.Liu,D.McDuff,andS.Patel. Bigsmall: Efficientmulti-\ntask learning for disparate spatial and temporal physiological measurements. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision , pages 7914–7924, 2024.\n[44]Y.Nie,N.H.Nguyen,P.Sinthong,andJ.Kalagnanam. Atimeseriesisworth64words: Long-term\nforecasting with transformers. arXiv preprint arXiv:2211.14730 , 2022.\n[45]M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,\nF. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv\npreprint arXiv:2304.07193 , 2023.\n[46]Y.-C. Pan, B. Goodwin, E. Sabelhaus, K. M. Peters, K. F. Bjornson, K. L. Pham, W. Walker, and\nK. M. Steele. Feasibility of using acceleration-derived jerk to quantify bimanual arm use. Journal\nof NeuroEngineering and Rehabilitation , 17:1–8, 2020.\n[47]A. Pillai, D. Spathis, F. Kawsar, and M. Malekzadeh. Papagei: Open foundation models for\noptical physiological signals. International Conference on Learning Representations (ICLR) , 2025.\n[48]I. M. Pires, F. Hussain, N. M. Garcia, and E. Zdravevski. Improving human activity monitoring\nby imputation of missing sensory data: Experimental study. Future Internet , 12(9):155, 2020.\n[49]R. Qin and Y. Wang. Imputegan: Generative adversarial network for multivariate time series\nimputation. Entropy, 25(1):137, 2023.\n[50]M. M. Rahman, N. Ali, R. Bari, N. Saleheen, M. al’Absi, E. Ertin, A. Kennedy, K. L. Preston, and\nS. Kumar. mDebugger: Assessing and diagnosing the fidelity and yield of mobile sensor data. In\nMobile Health: Sensors, Analytic Methods, and Applications , chapter 7, page 121–143. 2017.\n[51]C. Rommel, J. Paillard, T. Moreau, and A. Gramfort. Data augmentation for learning predictive\nmodels on eeg: a systematic comparison. Journal of Neural Engineering , 19(6):066020, 2022.\n14\n--- Page 15 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n[52]M. Saha, M. A. Xu, W. Mao, S. Neupane, J. M. Rehg, and S. Kumar. Pulse-ppg: An open-source\nfield-trained ppg foundation model for wearable applications across lab and field settings. arXiv\npreprint arXiv:2502.01108 , 2025.\n[53]P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laerhoven. Introducing wesad,\na multimodal dataset for wearable stress and affect detection. In Proceedings of the 20th ACM\ninternational conference on multimodal interaction , pages 400–408, 2018.\n[54]M. Sevil, M. Rashid, M. R. Askari, Z. Maloney, I. Hajizadeh, and A. Cinar. Detection and\ncharacterization of physical activity and psychological stress from wristband data. Signals,\n1(2):188–208, 2020.\n[55]G. F. Silva, T. P. Fagundes, B. C. Teixeira, and A. D. Chiavegatto Filho. Machine learning for\nhypertension prediction: a systematic review. Current hypertension reports , 24(11):523–533,\n2022.\n[56]I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality of\nicu patients: The physionet/computing in cardiology challenge 2012. In 2012 computing in\ncardiology , pages 245–248. IEEE, 2012.\n[57]D. Spathis, I. Perez-Pozuelo, S. Brage, N. J. Wareham, and C. Mascolo. Self-supervised transfer\nlearning of physiological representations from free-living wearable data. In Proceedings of the\nConference on Health, Inference, and Learning , pages 69–78, 2021.\n[58]B. Srimedha, R. N. Raj, and V. Mayya. A comprehensive machine learning based pipeline for an\naccurate early prediction of sepsis in icu. Ieee Access , 10:105120–105132, 2022.\n[59]C. I. Tang, I. Perez-Pozuelo, D. Spathis, and C. Mascolo. Exploring contrastive learning in human\nactivity recognition for healthcare. arXiv preprint arXiv:2011.11542 , 2020.\n[60]G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\nK. Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 , 2023.\n[61]S. Tonekaboni, D. Eytan, and A. Goldenberg. Unsupervised representation learning for time\nseries with temporal neighborhood coding. arXiv preprint arXiv:2106.00750 , 2021.\n[62]J. Truslow, A. Spillane, H. Lin, K. Cyr, A. Ullal, E. Arnold, R. Huang, L. Rhodes, J. Block, J. Stark,\net al. Understanding activity and physiology at scale: The apple heart & movement study. npj\nDigital Medicine , 7(1):242, 2024.\n[63]S.VanBuurenandK.Groothuis-Oudshoorn. mice: Multivariateimputationbychainedequations\nin r.Journal of statistical software , 45:1–67, 2011.\n[64]A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polosukhin.\nAttention is all you need. Advances in neural information processing systems , 30, 2017.\n[65]H. Wei, M. A. Xu, C. Samplawski, J. M. Rehg, S. Kumar, and B. M. Marlin. Temporally multi-scale\nsparse self-attention for physical activity data imputation. Proceedings of machine learning\nresearch , 248:137, 2024.\n[66]J. M.-T. Wu, M.-H. Tsai, S.-H. Xiao, and Y.-P. Liaw. A deep neural network electrocardiogram\nanalysis framework for left ventricular hypertrophy prediction. Journal of Ambient Intelligence\nand Humanized Computing , pages 1–17, 2020.\n15\n--- Page 16 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n[67]M. Wu, H. Cao, H.-L. Nguyen, K. Surmacz, and C. Hargrove. Modeling perceived stress via hrv\nand accelerometer sensor streams. In 2015 37th annual international conference of the IEEE\nengineering in medicine and biology society (EMBC) , pages 1625–1628. IEEE, 2015.\n[68]M. Xu, A. Moreno, S. Nagesh, V. Aydemir, D. Wetter, S. Kumar, and J. M. Rehg. Pulseimpute:\nA novel benchmark task for pulsative physiological signal imputation. Advances in Neural\nInformation Processing Systems , 35:26874–26888, 2022.\n[69]M.A.Xu, A.Moreno, H.Wei, B.M.Marlin, andJ.M.Rehg. Rebar: Retrieval-basedreconstruction\nfor time-series contrastive learning. arXiv preprint arXiv:2311.00519 , 2023.\n[70]M. A. Xu, J. Narain, G. Darnell, H. Hallgrimsson, H. Jeong, D. Forde, R. Fineman, K. J. Raghuram,\nJ. M. Rehg, and S. Ren. Relcon: Relative contrastive learning for a motion foundation model for\nwearable data. arXiv preprint arXiv:2411.18822 , 2024.\n[71]G. Yilmaz, X. Lyu, J. L. Ong, L. H. Ling, T. Penzel, B. T. Yeo, and M. W. Chee. Nocturnal blood\npressure estimation from sleep plethysmography using machine learning. Sensors, 23(18):7931,\n2023.\n[72]J. Yoon, J. Jordon, and M. Schaar. Gain: Missing data imputation using generative adversarial\nnets. In International conference on machine learning , pages 5689–5698. PMLR, 2018.\n[73]H.Yuan, S.Chan, A.P.Creagh, C.Tong, A.Acquah, D.A.Clifton, andA.Doherty. Self-supervised\nlearning for human activity recognition using 700,000 person-days of wearable data. NPJ digital\nmedicine , 7(1):91, 2024.\n[74]X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik. Self-supervised contrastive pre-training for\ntime series via time-frequency consistency. Advances in neural information processing systems ,\n35:3988–4003, 2022.\n[75]Y. Zhou, J. Shi, R. Stein, X. Liu, R. N. Baldassano, C. B. Forrest, Y. Chen, and J. Huang. Missing\ndata matter: an empirical evaluation of the impacts of missing ehr data in comparative effec-\ntiveness research. Journal of the American Medical Informatics Association , 30(7):1246–1256,\n2023.\n[76]T. Zielinski, A. M. Moore, E. Troup, K. J. Halliday, and A. J. Millar. Strengths and limitations of\nperiod estimation methods for circadian data. PloS one , 9(5):e96462, 2014.\n16\n--- Page 17 ---\nAppendix — LSM-2: Learning from Incomplete Sensor Data\nTable of Contents\nA.1 Data Details 18\nA.1.1 Imputing Missingness for Non AIMModels . . . . . . . . . . . . . . . . . . . . . . . 18\nA.1.2 Device Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.1.3 Sensor Derived Minutely Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.1.4 Demographic Breakdown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.1.5 Discriminative Task Label Breakdown . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.1.6 Acquisition and Approval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.2 Missingness Visualizations 22\nA.2.1 Additional Examples of Data with Existing Missingness . . . . . . . . . . . . . . . . 22\nA.2.2 Prevalence and Length of Missingness . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.3 Pre-training Masking % Ablation Experiment 22\nA.4 Model Hyperparameter and Implementation Details 26\nA.4.1 Pre-training Set-up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nA.4.2 Downstream Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nA.5 Additional Results 29\nA.5.1 Confusion Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nA.5.2 Reconstruction Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.6 Additional Discussions 30\nA.6.1 The Utility of Day-Level Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.6.2 Person-Level versus Event-Level Performance . . . . . . . . . . . . . . . . . . . . . . 30\nA.6.3 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA.6.4 Broader Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA.6.5 Ethics Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n17\n--- Page 18 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nA.1. Data Details\nA.1.1. Imputing Missingness for Non AIMModels\nAlthough AIMis able to organically handle existing missing values using clever masking, the same\ncannotbesaidforourbaselinemethods. Furthermore,manystandarddeeplearningframeworks(such\nas pytorch, jax, and tensorflow) are unable to handle nan values in model training and evaluation,\ncausing value errors or propogating nans throughout the network during forward and backward\npasses. For this reason we impute missing (nan) values in our data. We use linear interpolation\nbetween gaps and then back and forward fill for missingness at the start and end of the sequence.\nA.1.2. Device Details\nThere are many different types of smartwatches and fitness trackers. Fig. 7 shows the distribution of\ndifferent trackers and smartwatches present in our pretraining dataset. Given the scale of our dataset\nwe are able to train on examples of data from many different devices. Consequently, our model\ndemonstrates robustness across diverse device types, handling their varying sensor technologies and\ndiffering inherent missingness patterns.\nFigure 7|Device Distribution. The count of each fitness tracker present in our pre-training dataset.\nA.1.3. Sensor Derived Minutely Features\nOur wearable devices utilize 5 different sensors: Photoplethysmography, Accelerometer, Skin Conduc-\ntance (electrodermal activity or EDA), Temperature, and Altitude. Each of these sensors collects raw\nwaveform signals at 100 Hz, 25 Hz, 200 Hz, 6 Hz, amd 10 Hz respectively, but we do not use the\nsignals at this high resolution because (1) due to practical reasons (i.e. prohibitive storage costs and\nbattery drain), data is not stored in this raw form at our scale, and (2) it is computationally impracti-\ncal to learn models on raw waveforms across an entire day (i.e. 200 Hz for 1 day is 𝑇=17million\ntime-points, per instance). As such, various features are curated from the raw waveforms as minutely\naggregrated features and saved to be used as inputs into our model. Each of these features are\n18\n--- Page 19 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\ngrounded in the domain literature, based on prior work that has shown their clinical effectiveness.\nFor example, heart rate variability metrics like RMSSD [ 19] or Shannon Entropy of RR intervals [ 3]\nhave well-established prognostic value for cardiovascular health, while accelerometry features like\njerk ratio [46] effectively characterize movement quality.\nEach of the derived features, as well as their base sensor origin, can be found in Table 6 below. For\nthe targeted sensor removal experiments, as well as any other descriptions of the sensor as a whole,\nwe refer to the sensor as all features derived from the sensor . For example, when removing the PPG\nsensor in the targetted missingness experiment, we remove all PPG-derived features, from Heart Rate\nto Shannon Entropy RR Differences.\nA.1.4. Demographic Breakdown\nA statistical breakdown of our datasets, by demographic features can be found in Table 7. A subset of\nthese, age and BMI, represent two of the regression tasks used to validate our method.\nA.1.5. Discriminative Task Label Breakdown\nTable 8 shows label and data breakdown of the discriminative tasks used to validate our method.\nThese tasks include 20-class activity recognition (Table 8(a)) from the activity dataset, and binary\nanxiety and hypertension classification (Table 8(b.i)) from the metabolic dataset.\nA.1.6. Acquisition and Approval\nThe data used for training in our analysis was curated from a large corpus of historical wearable data\ncollected with consent from partcipants for these data to be used in research. Specifically, the consent\nlanguage described use of the data for developing new health features and algorithms and being\nincluded in publications:\nREDACTED will collect and use your data to research and develop new health and wellness products\nand services for you and others. This data includes your: Health and wellness data, such as steps, heart\nrate, andsleepdata. Yourdatamayalsobeusedtogeneratefindingsthatcouldbeincludedinpublications\n(such as scientific journals) to contribute to general knowledge about health and science. For example,\nactivity, heartrate, andsleepdatacontributedtopublishedfindingsthatFitbitdevicescouldhelpdetectflu\noutbreaks. None of the data used for these purposes will include your name, email, or other information\nthat directly identifies you.\nTheuseofdataforpretraininginthismannerwasapprovedasexemptunder45CFR§46.104(d)(4)\n\"because the research involves the use of identifiable private information/biospecimens; and information,\nwhich may include information about biospecimens, is recorded by the investigator in such a manner that\nthe identity of the human subjects cannot readily be ascertained directly or through identifiers linked\nto the subjects, the investigator does not contact the subjects, and the investigator will not re-identify\nsubjects.\"\nThe Metabolic downstream dataset for anxiety and hypertension prediction came from an IRB\napproved study (protocol number removed for anonymization). The core objective of this study as\ndescribed in the IRB protocol was to: \"Evaluate the feasibility of using the data provided by wrist-worn\nwearable devices to develop algorithms and scores to assess metabolic health.\"\nIn the consent for the observational study, participants were informed that data on up to 7,500\nparticipants in the United States would be collected. We used a mobile study platform that allows\nparticipantstoenroll, checkeligibilityandprovidefullinformedconsent. Thesamemobileapplication\n19\n--- Page 20 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nTable 6|Sensor Feature Definitions and the Sensor they are Derived From.\nFeature Unit Definition\nPhotoplethysmography\nHeart Rate Beats/Min Mean of instantaneous heart rate.\nHeart Rate at Rest Beats/Min Mean of heart rate at rest.\nRR Percent Valid % % of 5-minute window with valid RR intervals.\nRR 80𝑡ℎPercentile Msec 80𝑡ℎpercentile of 5-minute window of RR ints.\nRR 20𝑡ℎPercentile Msec 20𝑡ℎpercentile of RR ints.\nRR Median Msec Median RR interval.\nRMSSD Msec Root mean squared st. dev. of RR ints.\nSDNN Msec Standard deviation of RR intervals.\nShannon Ent. RR Nats Shannon entropy of the RR intervals.\nShannon Ent. RR Diffs Nats ShannonentropyoftheRRintervaldifferences.\nAccelerometer\nStep Count Steps Number of steps.\nJerk Autocorrelation Ratio a.u. Ratio of lag=1 autocorrelation to energy in 1st\n3-axis principal component.\nLog Energy a.u. Log of sum of 3-axis root mean squared magni-\ntude.\nCovariance Condition a.u. Estimate of condition number for the 3-axis\ncovariance.\nLog Energy Ratio a.u. Logofratioofsumofenergyin1st3-axisprinci-\npal component over energy of 3-axis root mean\nsquared magnitude.\nZero Crossing St.Dev. Seconds Standard deviation of time between zero cross-\ning of 1st 3-axis principal component.\nZero Crossing Average Seconds Mean of time between zero crossing of 1st 3-\naxis principal component.\nAxis Mean a.u. Mean of 3-axis\nKurtosis a.u. Kurtosis of 3-axis root mean squared magni-\ntude.\nSleep Coefficient a.u. Sum of 3-axis max-min range with 16 log-\nscaled bins.\nSkin Conductance\nSkin Conductance Value 𝜇Siemens Center of linear tonic SCL value fit.\nSkin Conductance Slope 𝜇S/Min Intraminute slope of SCL values.\nLead Contact Counts Counts Number of times sensor leads contacted the\nwrist in a minute.\nSkin Temperature\nSkin Temperature Value °C Mean value of skin temperature.\nSkin Temperature Slope °C/Min Slope of skin temperature.\nAltimeter\nAltitude St.Dev. Norm Hectopascals Standard deviation of altimeter readings.\n20\n--- Page 21 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nTable 7|Demographics of our Various Datasets.\nPre-training Downstream Activity Downstream Metabolic\nCategory Train (%) Val (%) Train (%) Val (%) Train (%) Val (%)\nSex\nMale 37,352 (68.1) 3,657 (63.8) 27,653 (73.1) 6,092 (73.0) 551 (44.1) 258 (35.4)\nFemale 23,041 (38.1) 2,065 (36.0) 10,145 (26.8) 2,248 (26.9) 670 (53.6) 455 (62.4)\nNot Specified 48 (0.1) 10 (0.2) 24 (0.1) 3 (0.1) 0 (0) 0 (0)\nAge\n18–39 28,519 (47.2) 2,583 (45.1) 19,340 (51.1) 4,492 (53.8) 415 (33.2) 223 (30.6)\n40–59 24,888 (41.2) 2,433 (42.4) 15,309 (40.5) 3,172 (38.0) 637 (51.0) 384 (52.7)\n60–79 6,473 (10.7) 664 (11.6) 2,875 (7.6) 618 (7.4) 198 (15.8) 121 (16.6)\n≥80 364 (0.6) 39 (0.7) 120 (0.3) 31 (0.4) 0 (0) 1 (0.1)\nNot Specified 197 (0.3) 178 (0.5) 30 (0.4) 0 (0) 0 (0) 0 (0)\nBMI\nHealthy (<25) 22,425 (37.1) 2,173 (37.9) 15,942 (42.2) 3,685 (44.2) 319 (25.5) 188 (25.8)\nOverweight (25–30) 20,242 (33.5) 1,952 (34.1) 14,154 (37.4) 3,017 (36.2) 343 (27.4) 206 (28.6)\nObese (≥30) 14,799 (24.5) 1,330 (23.2) 6,131 (16.2) 1,316 (15.8) 481 (38.5) 274 (37.6)\nNot Specified 230 (0.4) 14 (0.2) 81 (0.2) 18 (0.2) 49 (3.9) 28 (3.8)\nTotal 60,440 (100) 5,732 (100) 37,822 (100) 8,343 (100) 1,250 (100) 729 (100)\nTable 8|Discriminative Task Dataset Distribution\n(a)Activity Recognition Dataset\nTask / Label Train (%) Test (%)\nActivity\nWalk 4,434 (6.0) 874 (5.8)\nBike 4,363 (5.9) 858 (5.6)\nSport 4,433 (6.0) 902 (5.9)\nRun 4,023 (5.4) 790 (5.2)\nAerobics 4,417 (6.0) 906 (6.0)\nElliptical 4,402 (5.9) 879 (5.8)\nSpinning 4,402 (5.9) 858 (5.6)\nWeightlifting 4,335 (5.9) 841 (5.5)\nSwim 4,280 (5.7) 867 (5.8)\nHike 4,062 (5.5) 841 (5.5)\nTennis 4,138 (5.6) 815 (5.4)\nCrossFit 4,305 (5.8) 887 (5.8)\nPilates 4,365 (5.9) 846 (5.6)\nStairclimber 4,272 (5.8) 834 (5.5)\nDancing 4,288 (5.8) 826 (5.4)\nIndoor climbing 3,520 (4.8) 853 (5.6)\nGolf 3,003 (4.1) 710 (4.7)\nSkiing 1,594 (2.1) 420 (2.8)\nSnowboarding 662 (0.9) 167 (1.1)\nKayaking 732 (1.0) 212 (1.4)\nTotal 74,030 (100) 15,186 (100)(b.i)Metabolic Dataset Classification Tasks\nTask / Label Train (%) Test (%)\nAnxiety\nPositive 55,030 (36.4) 34,749 (38.5)\nNegative 96,316 (63.6) 55,437 (61.5)\nHypertension\nPositive 36,349 (24.0) 23,353 (25.9)\nNegative 114,997 (76.0) 66,833 (74.1)\nTotal 151,346 (100) 90,186 (100)\n21\n--- Page 22 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nenables the collection of Fitbit data using Fitbit devices or Pixel watches and allows participants\nto complete questionnaires. The participants reported their anxiety, depression and hypertension\ndiagnoses through this app. Data was de-identified and stored in accordance with the approved IRB\nprotocol. The participants were compensated with a free set of lab tests from Quest Diagnostics for\nparticipating in the study.\nA.2. Missingness Visualizations\nA core property of these data is that they are fragmented, and the missingness has several modal\ntypes. Three very common modes occur: 1) When the device is being charged or off all sensor stop\nrecording data (device off), 2) when the device is in certain operation modes (e.g., when in sleep\nmode) certain signals stop being recorded (sensor off) and 3) when there is noise in the sensor\ndata spurious values (e.g., values that are not physiologically possible - HR=0) are filtered out. The\nfollowing sections demonstrate additional visualizations of the missingness patterns present from\nthese mechanisms.\nA.2.1. Additional Examples of Data with Existing Missingness\nIn order to demonstrate the ubiquity and broad range of missingness patterns found within the\ndata, we randomly sample an additional 8 data examples, shown in Figure 8. These examples\nfurther demonstrate how some patterns are consistent across users, such as increased missingness\nduring early morning hours (12am-6am) (reflecting device removal during sleep) or correlated\nmissingness dropout across various sensor channels. However, it should be noted that all samples\nexhibit unique missingness signatures with no two patterns being identical with vastly differing\nmissingness percentages (27-63%) and demonstrating the ubiquity of real-world missingness. These\nfindings motivated our development of AIM’s flexible masking approach, which explicitly models such\nheterogeneous missingness patterns during pre-training.\nA.2.2. Prevalence and Length of Missingness\nIn Figure 9, we demonstrate the prevalence of missingness as well as the length of the missingness,\nbroken down across each sensor type across all 1.6 million instances of pre-training data. As we can\nsee, each sensor has very different patterns of missingness, and across all sensors, their missingness\npresentsaslongextendedgaps,makingthemnon-trivialtoreconstructover. Notably,theaccelerometry\nfeatures in particular, have missingness in the form of these extended gaps, whereas most of the\nmissingness for PPG sensors is of shorter length.\nA.3. Pre-training Masking % Ablation Experiment\nThe adaptive component of our AIMmethodologies allows for us to utilize a mix of artificial mask\npre-training masking strategies. Each of these artificial masks are applied ontop of the existing,\ninherited mask. In order to model both dimensionalities of our data, across time and sensors, and the\nreal-world missingness paradigms, we have a mix of 3 different artificial mask pre-training strategies:\n1.Random Imputation Pre-training: Here we drop out a % of total tokens. This is useful for\nmodeling sensor noise, in which random channels at random times will be missing.\n2.Temporal Slice Pre-training: Here we drop out a % of total temporal slices, across all sensor\nchannels. This is useful for modeling device off, in which, for a given period of time, all\n22\n--- Page 23 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nFigure 8|Gallery of Data Examples with Real-world Missingness. White designates missingness.\n23\n--- Page 24 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nFigure 9|Distribution of Prevalence and Length of Missingness.\nsensors are off because the wearable device is off body. Here, we do not model it like temporal\ninterpolation, in which the slices are necessarily contiguous. This is because, during pre-training,\nwe would like to learn to reconstruction across a variable number of contiguous slices.\n3.Sensor Slice Pre-training: Here we drop out a % of total sensor slices, across all time points. This\nis useful for modeling sensor off, in which a given sensor channel is off because of a non-random\nmissingness mechanism that tells the device to turn off the channel (i.e. to save battery life).\nBelow in Tables 9, 10, 11, we see that an 80% random imputation mask %, 50% temporal slice\n%, and a 50% sensor slice % produce a good mix of reconstruction results across small and large\namounts of evaluation masking, for each generative task. Note that when there is a tie, we would\nprefer higher masking %, in order to allow for a higher dropout removal ratio, and to produce a\nharder task for our model to pre-train with.\nTable 9|Effect of Differing Pre-training Random Imputation Mask % on Random Imputation.\nRandom Imp. Eval Ratio\nPT Random Imp. Mask % 30% 50% 80%\n90% 0.13 0.14 0.20\n80% 0.100.12 0.19\n70% 0.10 0.12 0.19\n60% 0.10 0.12 0.19\n50% 0.09 0.12 0.20\n24\n--- Page 25 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nTable 10|Effect of Differing Pre-training Temporal Slice Mask % on Temporal Interpolation.\nTemporal Interp. Eval Amount\nPT Temporal Slice % 10 min 30 min 60 min 180 min\n70% 0.23 0.34 0.41 0.56\n60% 0.26 0.36 0.42 0.57\n50% 0.23 0.33 0.40 0.55\n40% 0.22 0.33 0.40 0.56\n30% 0.22 0.33 0.40 0.57\nTable 11|Effect of Differing Pre-training Sensor Slice Mask % Ratios on Sensor Imputation.\nSensor Imp. Eval Amount\nPT Sensor Slice % 2/26 6/26 12/26 24/26\n70% 0.19 0.23 0.28 0.43\n60% 0.18 0.22 0.27 0.45\n50% 0.170.21 0.27 0.48\n40% 0.17 0.21 0.27 0.56\n30% 0.16 0.21 0.30 0.63\n25\n--- Page 26 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nA.4. Model Hyperparameter and Implementation Details\nA.4.1. Pre-training Set-up.\nWepre-trainourmodelsonalargesetofwearableminutelysensordatadescribed. Therawmultimodal\nsensor data input can be denoted by A∈ℝ𝑇×𝑆.𝑆=26, which is the full number of signals in our\nmultimodal data. These signals are derived from 4 different wearable sensors: Accelerometry, PPG,\nEDA, and Temperature. In our setting, we set 𝑇=1440, which is composed of all minutes from a\nfull 24 hour day, from midnight to midnight local time. We use this window size as days normally\nhave a consistent structure, allowing for a more meaningful absolute positional embedding than if an\narbitrary window size was set (e.g. 300 minutes [42]).\nOur model was pre-trained with a ViT-1D [ 21,1] encoder backbone by using a 1D patch size of\n10 time-steps (i.e. 10 minutes). This results in a total of 3744 tokens (the 1440 minutes are reduced\nto 144 tokens per signal. With 26 signals, 26*144=3744 is the final number of tokens). Similar to\nprior work [ 41], each signal channel is patched with a shared kernel, and we utilize a 2D positional\nembedding to encode information about the temporal position and signal channel. The ViT model\nhad 25 million parameters with an encoding dimensionality of 384, 12 encoder layers, and 4 decoder\nlayers. Our mask is a union of the inherited mask with an artificial masking mix of 80% random\nimputation, 50% temporal slices, and 50% signal slices. Our primary pre-training objective is to\noptimize the signal reconstruction loss (i.e. mean squared error), averaged over the artificially masked\npatches. The model was pre-trained on 8x16 Google v5e TPUs with a total batch size of 512 across\n100,000 training steps. The training process uses the AdamW optimizer with a base learning rate of\n5𝑒−3, weight decay set to 1𝑒−4, and betas set to 0.9and 0.95. Gradients were clipped at 1.0. A\nlinear warm-up schedule is applied for the first 5% of total steps, followed by a cosine learning rate\ndecay to zero.\nOur SSL baselines include LSM [ 42], SimCLR [ 13], DINO [ 11], and a Masked Siamese Network\n(MSN) [ 6]. LSM is an MAE [ 29] approach with 0.8 random masking ratio with no inherited masking.\nSimCLR, DINO, and MSN are augmentation-based contrastive approaches, and we utilize a set of\ncommon time-series augmentations [ 59,38,74,51]: jittering, scaling, and time flipping. Each\naugmentation has a 0.5 probability of being applied. Jittering was implemented as a random sample\nfrom a gaussian distribution with zero-mean and a uniformly randomly sampled standard deviation\nfrp, 0 to 0.5, per value in the time-series. Scaling was implemented by multiplying all of the data\ninput with a scale, uniformly sampled from 1.1 to 1.5. For DINO, we omit scaling as the model was\nunable to converge.\nEach of these baselines were all pre-trained from scratch, following the same previously stated\ntraining conditions, unless stated otherwise. All baselines expect full, complete data as input, and as\nsuch, they utilize the imputed version of our sensor dataset. LSM was trained with a ViT-2D with a\n2D patch size of (10,2), in order to match their image-based encoding approach, and all other ViT\nparameters remain constant.\nA.4.2. Downstream Evaluation\nWegroupourdownstreamevaluation into threesectionsbasedonthe target: generative, classification,\nand regression.\nIn ourGenerative Evaluation , we evaluate how well our model is able to reconstruct different\ntypes of structured missingness patterns that mimic real-world missingness patterns: (1) Random\nImputation, where a [30%, 50%, 80%] of tokens is masked out, (2) Temporal Interpolation, where\nall signals in a contiguous temporal window of length [10, 30, 60 minutes] is completely masked out,\n26\n--- Page 27 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\n(3) Temporal Extrapolation, which is similar to interpolation, but the window is necessary at the end\nof the time-series, and (4) Signal Imputation, where all time points for a random set of [2/26, 6/26,\n12/26] signal channels is masked. Reconstruction performance was calculated with mean squared\nerror (MSE) on the artificially masked tokens, averaging only over the data points that have a ground\ntruth.\nOur deep learning baselines include the LSM model [ 42], another MAE-based model, which can be\nused to evaluate these generative tasks out-of-box by setting the artificial masking procedure to match\nthe proposed tasks. Our AIMmodel is done in the same way, but the full encoder mask includes the\ninherited mask as well. Unfortunately, the contrastive SSL baselines are unable to provide generative\nperformance metrics because they do not utilize a reconstruction objective. Instead, we use alternative\nsimple generative baselines, which match practical applications. Many application-focused biosensor\nalgorithms will employ simple imputation methods [ 48,68,58,66,4] as quick data preprocessing\nmethods. Thus, we choose to include these additional methods as baselines: Linear Interpolation,\nK-Nearest Neigbhors, and Mean Filling. Similar to our method, we run these baselines with a union\nmask of the mask inherited from existing missingness and the artificial mask. MICE [ 63] is another\npopular, simple baseline designed for multivariate data, but we opted to not include it due to our\nexisting missingness patterns violating the Missingness At Random assumption, and prior work\ndemonstrate a relative poorer performance compared to nearest neighbor and linear interpolation\n[42].\nIn ourClassification Evaluation , we evaluate how well our model’s embedding representation is\nable to capture discriminative features. During evaluation, our model calculates the embedding on\nall non-inherited-masked tokens and uses an average pooling followed by a trainable linear probe\nto classify each of the prediction targets. For the LSM model, because it is unable to represent the\ninherited mask, the embedding for all tokens is pooled, such that tokens that were part of the existing\nmissingness but have been filled with imputation will be included. For the contrastive methods, the\nlearned CLS token is used as the pooled representation. We report performance with F1 score as it\nbalances precision and recall for class-imbalanced targets, Accuracy as a straightforward measure\nof overall correctness, Balanced Accuracy to account for potential class imbalance, and AUROC to\nevaluate the model’s ranking capability across all classification thresholds. The prediction targets are\nhypertension, anxiety, which originate from the Metabolics dataset and 20-class activity recognition,\nwhich originates from the Activity dataset.\nThe linear probe was trained by freezing the learned ViT backbone, averaging over the entire\nembedding and training a logistic regression head ontop of it. For our AIMmodel specifically, with\nthe inherited mask, the average was only done over the non-masked tokens. Training was done with\na batch size of 512, across 500 training steps with an AdamW optimizer with a base learning rate of\n5𝑒−3, weight decay set to 1𝑒−4, and betas set to 0.9and 0.95. Gradients were clipped at 1.0. For\nactivity specifically, training steps and learning rate were increased to 1000 and 1𝑒−1to achieve\nbetter convergence.\nAdditionally, we include two extra supervised baselines, ViT-1D [ 21] and a ResNet [ 31], that are\ntrained end-to-end for each of our tasks. ViT-1D is a transformer-based architecture that follows the\nsame architecture as our AIMwith 25 million parameters, but with randomly initialized weights,\ntrained end-to-end. ResNet is a strong CNN-based architecture that has seen broad success throughout\nthe health biosignal time-series domain [ 70,47,1,40]. This model was a ResNet-50 [ 31] with 25\nmillion parameters, in order to match the ViT model. Specifically, it contains 50 layers, with 64 filters\nthat double after each residual block, with a final average pooling and logistic regression head. Both\nmodels are trained with a batch size of 512, across 500 training steps with an AdamW optimizer with\na base learning rate of 5𝑒−3, weight decay set to 1𝑒−4, and betas set to 0.9and 0.95. Gradients\n27\n--- Page 28 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nwere clipped at 1.0. A linear warm-up schedule is applied for the first 5% of total steps, followed by\na cosine learning rate decay to zero. Because these models do not handle missingness, they were\ntrained directly on the imputed data.\nInourRegressionEvaluation , weutilizethesameevaluationproceduredescribedinclassification,\nonly instead the linear probe is specifically a linear regression. We report performance with MAE as it\nprovides an interpretable deviation from the correct value, as well as Pearson Correlation Coeffecient,\nas it is a common metric for evaluating how well a regressor is able to capture the trend of the target\n[70, 73]. The prediction targets are BMI and Age.\nThe linear probe was trained by freezing the learned ViT backbone, averaging over the entire\nembedding and fit a linear regression head ontop of it using Scikit-Learn’s LinearRegression imple-\nmentation out-of-box. The supervised baselines were trained in an identical way as done in the\nclassification evaluation, but using a linear regression head instead of logistic regression.\n28\n--- Page 29 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nA.5. Additional Results\nA.5.1. Confusion Matrices\nFigure 10 illustrates the utility of AIMlearned embeddings for downstream applications. Specifically,\nthis confusion matrix shows the performance of AIM, post-trained on the 20-class activity recognition\ntask using a linear probe. It is clear that the embedding are useful in discriminating between a\nlarge number of activities, even those which may be semantically clustered, such as skiing and\nsnowboarding. Future work may explore how to expand to even more activities and behavioral events,\nand investigate the utility of large-scale pre-training in address long-tail task labels.\nWalk (911)Bike (925)Sport (856)Run (1,017)Aerobics (599)Elliptical (601)Spinning (843)Weightlifting (896)Swim (792)Hike (866)Tennis (751)CrossFit (698)Pilates (1,064)Stairclimber (630)Dancing (888)Indoor climbing (1,003)Golf (576)Skiing (361)Snowboarding (325)Kayaking (568)\nPredictedWalk (873)\nBike (858)\nSport (900)\nRun (790)\nAerobics (905)\nElliptical (878)\nSpinning (857)\nWeightlifting (841)\nSwim (866)\nHike (840)\nTennis (814)\nCrossFit (884)\nPilates (845)\nStairclimber (834)\nDancing (825)\nIndoor climbing (853)\nGolf (708)\nSkiing (420)\nSnowboarding (167)\nKayaking (212)True Label52%\n(457)4%\n(33)4%\n(37)4%\n(35)2%\n(19)2%\n(21)2%\n(18)3%\n(24)2%\n(14)10%\n(87)1%\n(5)1%\n(10)4%\n(32)1%\n(12)2%\n(19)3%\n(22)1%\n(7)0%\n(2)0%\n(2)2%\n(17)\n3%\n(28)65%\n(559)2%\n(21)2%\n(14)2%\n(13)2%\n(15)2%\n(18)2%\n(18)2%\n(18)2%\n(20)1%\n(5)1%\n(11)2%\n(17)1%\n(10)1%\n(6)3%\n(22)1%\n(8)1%\n(11)1%\n(7)4%\n(37)\n4%\n(37)5%\n(41)43%\n(383)3%\n(26)8%\n(75)1%\n(5)1%\n(9)1%\n(10)2%\n(17)3%\n(24)10%\n(93)3%\n(26)1%\n(7)1%\n(9)2%\n(17)4%\n(35)6%\n(50)0%\n(4)2%\n(16)2%\n(16)\n3%\n(27)2%\n(16)2%\n(17)64%\n(507)2%\n(18)3%\n(25)1%\n(4)2%\n(15)2%\n(13)3%\n(21)1%\n(10)2%\n(19)1%\n(9)1%\n(10)4%\n(33)3%\n(22)1%\n(4)0%\n(2)0%\n(1)2%\n(17)\n4%\n(40)3%\n(30)15%\n(134)6%\n(56)28%\n(251)2%\n(17)1%\n(11)1%\n(11)3%\n(30)2%\n(19)5%\n(45)3%\n(30)4%\n(38)1%\n(8)15%\n(134)2%\n(21)1%\n(9)0%\n(4)0%\n(4)1%\n(13)\n6%\n(49)3%\n(27)2%\n(14)6%\n(49)3%\n(22)31%\n(274)10%\n(88)6%\n(52)4%\n(34)3%\n(25)1%\n(6)3%\n(22)5%\n(45)9%\n(81)4%\n(31)2%\n(17)0%\n(3)0%\n(3)1%\n(6)3%\n(30)\n3%\n(22)2%\n(15)1%\n(8)2%\n(20)0%\n(3)6%\n(48)49%\n(423)4%\n(38)2%\n(14)2%\n(15)1%\n(7)3%\n(26)5%\n(41)12%\n(104)2%\n(16)3%\n(29)1%\n(6).0%\n(3)2%\n(19)\n3%\n(24)1%\n(12)1%\n(5)3%\n(25)0%\n(4)1%\n(11)4%\n(31)39%\n(327)1%\n(12)2%\n(16)1%\n(10)10%\n(83)8%\n(70)8%\n(65)2%\n(17)12%\n(104)1%\n(6).1%\n(5)2%\n(14)\n3%\n(26)2%\n(20)1%\n(9)3%\n(26)2%\n(20)3%\n(26)1%\n(8)1%\n(11)59%\n(510)2%\n(18)1%\n(9)2%\n(17)3%\n(24)1%\n(7)3%\n(22)2%\n(21)0%\n(1)0%\n(3)0%\n(1)10%\n(87)\n7%\n(61)4%\n(35)1%\n(10)5%\n(44)1%\n(9)4%\n(30)1%\n(12)1%\n(9)1%\n(10)58%\n(487)1%\n(5)1%\n(8)2%\n(13)2%\n(16)3%\n(26)1%\n(10)2%\n(15)1%\n(6)1%\n(8)3%\n(26)\n2%\n(20)1%\n(10)8%\n(65)1%\n(11)4%\n(33)1%\n(5)1%\n(5)2%\n(13)2%\n(17)2%\n(14)55%\n(449)3%\n(28)1%\n(8)1%\n(8)6%\n(47)5%\n(37)2%\n(15)0%\n(2)0%\n(3)3%\n(24)\n1%\n(6)3%\n(23)3%\n(24)6%\n(53)1%\n(10)2%\n(16)2%\n(19)15%\n(130)2%\n(18)2%\n(14)2%\n(21)28%\n(248)8%\n(71)5%\n(42)4%\n(32)12%\n(107)1%\n(7).1%\n(13)3%\n(30)\n3%\n(22)1%\n(7)0%\n(3)1%\n(6)2%\n(18)1%\n(8)5%\n(46)3%\n(24)1%\n(11)3%\n(25)2%\n(14)4%\n(33)55%\n(466)3%\n(29)7%\n(61)4%\n(37)1%\n(7)0%\n(3)0%\n(4)2%\n(21)\n3%\n(23)2%\n(18)1%\n(8)5%\n(45)1%\n(11)8%\n(67)12%\n(102)12%\n(104)2%\n(14)2%\n(14)1%\n(5)6%\n(47)9%\n(77)21%\n(172)6%\n(47)5%\n(43)1%\n(6)0%\n(1)2%\n(15)2%\n(15)\n3%\n(26)1%\n(12)2%\n(18)7%\n(58)9%\n(77)2%\n(16)1%\n(12)2%\n(16)2%\n(16)1%\n(12)3%\n(28)3%\n(28)9%\n(78)2%\n(15)41%\n(342)6%\n(50)1%\n(7)0%\n(1)0%\n(1)1%\n(12)\n3%\n(22)5%\n(46)3%\n(25)2%\n(15)1%\n(10)1%\n(5)4%\n(31)9%\n(76)1%\n(12)2%\n(14)1%\n(10)5%\n(46)6%\n(48)4%\n(30)3%\n(26)43%\n(370)2%\n(14)1%\n(7)2%\n(14)4%\n(32)\n2%\n(14)1%\n(9)9%\n(62)2%\n(16)0%\n(1)1%\n(6)1%\n(4)2%\n(16)2%\n(15)5%\n(32)4%\n(28)2%\n(13)1%\n(6)1%\n(8)1%\n(5)6%\n(42)57%\n(406)1%\n(4)2%\n(11)1%\n(10)\n1%\n(3).1%\n(4)1%\n(3). .0%\n(1)0%\n(2)1%\n(4)1%\n(5). .1%\n(5)0%\n(1).1%\n(4)0%\n(2)68%\n(286)20%\n(83)4%\n(17)\n.1%\n(1)1%\n(1)1%\n(1). . . .3%\n(5).1%\n(1).1%\n(1)1%\n(1).2%\n(3)2%\n(3)13%\n(21)72%\n(120)5%\n(9)\n2%\n(4)5%\n(11)4%\n(8)3%\n(7)2%\n(5)3%\n(6)0%\n(1).4%\n(8)2%\n(4).1%\n(3)4%\n(8)1%\n(2)3%\n(7)3%\n(7).0%\n(1)4%\n(8)58%\n(122)\nFigure 10|Activity Recognition Confusion Matrix. The results of a linear probe applied to AIMfor\nthe 20-class activity recognition task. Rows add up to 100%.\n29\n--- Page 30 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nA.5.2. Reconstruction Examples\nFigure 11 shows various reconstruction examples for a specific sensor signal. Here we can clearly see\nOurAIMapproach leads to much stronger performance, across different generative tasks.\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n2\n4\n6\n8\n10\n12\n14eda_level_real\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n0\n2\n4\n6\n8\n10\n12\n14eda_level_real\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n0\n2\n4\n6\n8\n10\n12\n14eda_level_real\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n0\n2\n4\n6\n8\n10\n12\n14eda_level_real\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n22\n24\n26\n28\n30\n32\n34wrist_temperatures\n0\n200\n400\n600\n800\n1000\n1200\n1400\nTime (minutes from midnight)\n24\n26\n28\n30\n32\n34wrist_temperatures\nOurs LSM\nFigure 11|Reconstruction Examples for 2/26 Sensor Signal Imputation (Row 1), 3 Hour\nTemporal Interpolation (Row 2), 3 Hour Temporal Extrapolation (Row 3). Red highlighted\nregions demonstrate regions of artificial masking. Orange shows original data with imputation (i.e.\nthe first 400-500 steps of the each row were originally missing, then imputed, as demonstrated by\nthe straight line) and blue shows the reconstructed data.\nA.6. Additional Discussions\nA.6.1. The Utility of Day-Level Features\nTraditionally, generalist methods for time-series health signals have focused on small windowed\nsegments of data on the order of seconds or sub-seconds [ 1,70,43,73]. Such methods allow for\nfine-grain activity and physiological tracking. An adjacent body of work has explored the utility of\nlonger observations, on the order of hours [ 57,42], enabling more complex person-level insights. In\nthis work seek to expand the observation window to encode a high-level of context. Day level features\nallow models to learn relationships not possible from shorter spans, for example, how a person’s\nactivity during the day may affect their night-time resting heart rate. Looking forward, we intend to\ncontinue exploring how best to encode large context windows to include known week, seasonal, and\nyear level periodicities.\nA.6.2. Person-Level versus Event-Level Performance\nAnalysis of the discriminative results (classification and regression) presented in the main body of the\npaper, raise an interesting question: how do generative pre-training affect performance on person-\n30\n--- Page 31 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nlevel and event-level tasks. For person-level tasks (hypertension, anxiety, age, BMI) we find that AIM\nconsistently outperforms supervised baselines while only using a simple linear probe. In contrast, we\nfind for the event-level task (20-class activity recognition), ResNet50, a supervised baseline performs\nextremely well, and likely a fully-finetuned AIMmodel is needed to surpass it. This suggest that\nwhile supervised methods easily capture event-level features (e.g., sudden heart rate changes due\nto activity), they struggle to learn slow-changing, near-constant day-level features more-relevant to\nperson-level tasks. This highlights how method, like are own, learn a more complex representation of\nthe data via generative pre-training. We further concede that our contrastive SSL baselines fail to\nfully realize the gains of pre-training. We hypothesizes that more complex time-series augmentations\nare needed to leverage their effect.\nA.6.3. Limitations and Future Work\nHere we expand upon the limitations and future work introduced in the main body of the paper.\nGeneralizing to New Devices. Though many commodity wearables host a similar suite of sensors\nthere are inevitable differences between these software-hardware systems. We acknowledge that our\nmethods focuses on a small subset of such devices. Future work will explore the generalizability of\nour methods to additional devices and datasets, and investigate the extent to which device specific\nmissingness patterns result in a distribution shift.\nGeneralizingtoOpenData. Mostpubliclyavailablewearabledatasets(e.g. WESAD[ 53],PAMAP2\n[9]) are composed of high-frequency raw signals that are very limited in their temporal context with\nonly a subset of the sensors we have available. Thus, they are unable to shown to be used in our\nsetting of day-level context. All of Us [ 33] demonstrates an interesting avenue to apply our work.\nAlthough limited to only the Heart Rate and Step Count channels (compared to our 26 channels), the\ndataset contains with long context windows and minutely data, and presents an interesting direction\nin future work to apply our AIMmethod.\nData and Feature Scales. Time-series analysis often requires explicit assumptions regarding data\nscale. As such, our method focuses on day-long samples. We acknowledge that such data disregards\nknown periodicities (e.g., weekly, seasonal, etc.). Future work will explore combining our fine-grained\nbehavioral and physiological modeling with insights from longer windows. Furthermore, our method\nutilizes minutely aggregated features as opposed to the raw sensor feeds common in sensing research.\nThis is a practical limitation, as data is not stored in its raw form at this scale.\nHandling Sensor Feature. Our method utilizes 26 features derived from a set of 5 sensors, and\nregards each feature as independent in the modeling. In reality there are significant correlations\nbetween features from the same sensor (e.g., heart rate and heart rate variability). More work can be\ndone to explore how best to combine these multimodal features – potentially sensor-specific encoders,\ncross-attention, or special class tokens per-sensor feed.\nA.6.4. Broader Impact\nPersonalandubiquitoushealthtechnologies,includingsmartphonesandwearables,havethepotential\nto scale to billions of individuals. Such devices allow for significant self- and longitudinal tracking,\nand in so doing may augment the current paradigm of clinical healthcare. To-date, consumer health\ntechnologies focus on low-level insights, such as steps, resting heart-rate, and sleep staging, which\nallow users to reason on personal higher-level insights (e.g., \"my resting heart-rate has been elevated\never since I fell sick\").\nIn contrast, our method, trained on day-level samples, learns behavioral and physiological patterns\n31\n--- Page 32 ---\nLSM-2: Learning from Incomplete Wearable Sensor Data\nuseful in deriving more complex insights. For example, our method shows the potential to predict\nanxiety and hypertension, insights that humans and commercial algorithms would struggle to derive\ngiven only sensor data. We believe this line of work will one day enable people to make the most of\ntheir tracked wearable data, better understand their behavior and physiology, and in so doing receive\nmore proactive and better informed care.\nA.6.5. Ethics Statement\nWhile consumer health research holds potential for significant positive impact, with so many possible\nstake holders, such research must be performed intentionally to ensure that it is safe and fair.\nAdditionally, there exists the unfortunate possibility that bad-actor may attempt to leverage methods,\nsuch as our own, in negligent ways. As researchers in the field, the burden falls to us to consider the\nimplications of this research, and act to fulfill the positive impacts and mitigate the associated risks.\nBuilding upon this, we concede that training our methods on closed (non-public) data, prevents\nthe scientific community from fully replicating our work. We acknowledge this as a limitation and\nattest our support for open science and open data. However, due to the sensitive nature of health\ndata, these considerations must be balanced by with the privacy and protection of our participants.\n32",
  "text_length": 92542
}