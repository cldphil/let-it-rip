{
  "id": "http://arxiv.org/abs/2505.24858v1",
  "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
  "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
  "authors": [
    "Gabrielle Kaili-May Liu",
    "Gal Yona",
    "Avi Caciularu",
    "Idan Szpektor",
    "Tim G. J. Rudner",
    "Arman Cohan"
  ],
  "published": "2025-05-30T17:54:08Z",
  "updated": "2025-05-30T17:54:08Z",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24858v1"
}