{
  "id": "http://arxiv.org/abs/2506.05200v1",
  "title": "Transformers Meet In-Context Learning: A Universal Approximation Theory",
  "summary": "Modern large language models are capable of in-context learning, the ability\nto perform new tasks at inference time using only a handful of input-output\nexamples in the prompt, without any fine-tuning or parameter updates. We\ndevelop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a\ndistinct task), we demonstrate how to construct a transformer that, without any\nfurther weight updates, can perform reliable prediction given only a few\nin-context examples. In contrast to much of the recent literature that frames\ntransformers as algorithm approximators -- i.e., constructing transformers to\nemulate the iterations of optimization algorithms as a means to approximate\nsolutions of learning problems -- our work adopts a fundamentally different\napproach rooted in universal function approximation. This alternative approach\noffers approximation guarantees that are not constrained by the effectiveness\nof the optimization algorithms being approximated, thereby extending far beyond\nconvex problems and linear function classes. Our construction sheds light on\nhow transformers can simultaneously learn general-purpose representations and\nadapt dynamically to in-context examples.",
  "authors": [
    "Gen Li",
    "Yuchen Jiao",
    "Yu Huang",
    "Yuting Wei",
    "Yuxin Chen"
  ],
  "published": "2025-06-05T16:12:51Z",
  "updated": "2025-06-05T16:12:51Z",
  "categories": [
    "cs.LG",
    "math.ST",
    "stat.ML",
    "stat.TH"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05200v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05200v1  [cs.LG]  5 Jun 2025Transformers Meet In-Context Learning:\nA Universal Approximation Theory\nGen Li∗Yuchen Jiao∗Yu Huang†Yuting Wei†Yuxin Chen†\nJune 6, 2025\nAbstract\nModern large language models are capable of in-context learning, the ability to perform new tasks at\ninference time using only a handful of input-output examples in the prompt, without any fine-tuning or\nparameter updates. We develop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a distinct task), we demonstrate\nhow to construct a transformer that, without any further weight updates, can perform reliable prediction\ngiven only a few in-context examples. In contrast to much of the recent literature that frames transform-\nersasalgorithmapproximators—i.e., constructingtransformerstoemulatetheiterationsofoptimization\nalgorithms as a means to approximate solutions of learning problems — our work adopts a fundamen-\ntally different approach rooted in universal function approximation. This alternative approach offers\napproximation guarantees that are not constrained by the effectiveness of the optimization algorithms\nbeing approximated, thereby extending far beyond convex problems and linear function classes. Our\nconstruction sheds light on how transformers can simultaneously learn general-purpose representations\nand adapt dynamically to in-context examples.\nKeywords: in-context learning, universal approximation, transformers\nContents\n1 Introduction 2\n1.1 In-context learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Approximation theory for in-context learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.3 An overview of our main contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.4 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.5 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Problem formulation 5\n2.1 Setting: in-context learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Transformer architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3 Key quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3 Main results: transformers as universal in-context learners 7\n4 Analysis 9\n5 Discussion 14\nThe first two authors contributed equally.\n∗Department of Statistics, Chinese University of Hong Kong.\n†Department of Statistics and Data Science, the Wharton School, University of Pennsylvania.\n1\n--- Page 2 ---\nA Proof of key lemmas 14\nA.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.3 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA.4 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n1 Introduction\n1.1 In-context learning\nThe transformer architecture introduced by Vaswani et al. (2017), which leverages a multi-head attention\nmechanism to capture intricate dependencies between tokens in a sequence, has catalyzed remarkable break-\nthroughs in large language models and reshaped algorithm designs across diverse AI domains (Khan et al.,\n2022; Shamshad et al., 2023; Lin et al., 2022; Gillioz et al., 2020). Built upon and powered by the transformer\nstructure, recent pre-trained foundation models (e.g., the Generative Pre-trained Transformer (GPT) series)\nhave unlocked a host of emergent capabilities that were previously unattainable (Bommasani et al., 2021).\nOne striking example is the emergent capability of “in-context learning” (ICL) — a concept coined by\nBrown et al. (2020) with the release of GPT-3 — which has since become a cornerstone of modern foundation\nmodels (Dong et al., 2022). In a nutshell, in-context learning refers to the ability to perform new tasks at\ninference time, without any update of the learned model. A contemporary large language model, pretrained\nin a universal, task-agnostic fashion, can readily handle a new task on the fly when given just a handful of\ninput-outputdemonstrations. Asa concreteexample, anewtask mightbe describedusingsome function f(·)\n(which was unknown a prioriduring pretraining), and be presented in a prompt containing Ninput-output\nexamples:\nprompt: x1→f(x1),x2→f(x2),···,xN→f(xN),xN+1→?\nwith the model then asked to predict f(xN+1)for the input instance xN+1. Remarkably, a pretrained model\nwith ICL capabilities can accomplish so without any fine-tuning or retraining, relying solely on the context\nprovided in the few-shot demonstrations to make high-quality predictions.\n1.2 Approximation theory for in-context learning?\nThe intriguing, phenomenal capability of in-context learning has sparked substantial interest from the theo-\nretical community, motivating a flurry of recent activity to illuminate its fundamental principles and uncover\nnew insights. Such theoretical pursuits have attempted to tackle various facets of ICL, spanning approx-\nimation capability, training dynamics, generalization performance, to name just a few (Garg et al., 2022;\nVon Oswald et al., 2023; von Oswald et al., 2023; Ahn et al., 2023; Bai et al., 2023). In the current paper,\nwe contribute to this growing body of work by investigating the effectiveness of transformers as universal\napproximators that support in-context learning.\nPrior work: transformers as algorithm apprxoimators. Approximation theory has emerged as a\npowerful lens for demystifying the representation power of transformers for ICL. Towards this end, a pre-\ndominant approach adopted in recent work is to interpret transformers as algorithm approximators, whereby\ntransformers are constructed to emulate the iterative dynamics of classical optimization algorithms during\ntraining, such as gradient descent (Von Oswald et al., 2023), preconditioned gradient descent (Ahn et al.,\n2023), transfer learning (Hataya et al., 2024), and Newton’s method (Giannou et al., 2023; Fu et al., 2024).\nThe underlying rationale is that: if each iteration of these optimization algorithms can be realized via a few\nattention and feed-forward layers, then a multi-layer transformer could, in principle, be constructed to emu-\nlate the full iterative procedure of an optimization algorithm, as a means to approximate solutions returned\nby these optimization-based training algorithms. Beyond emulating fixed optimization procedures, trans-\nformers can also be constructed to support in-context algorithm selection with the aid of a few more carefully\nchosen layers, enabling automatic selection of appropriate algorithms based on in-context demonstrations\n(Bai et al., 2023).\n2\n--- Page 3 ---\nWhile this algorithm approximator perspective is versatile — owing to the broad applicability of op-\ntimization algorithms like gradient descent — its utility is fundamentally constrained by the convergence\nproperties of the algorithms being approximated. Noteworthily, except for Giannou et al. (2023); Hataya\net al. (2024), existing analyses from this perspective have been restricted to linear tasks. Indeed, optimiza-\ntion algorithms such as gradient and Newton’s methods enjoy global convergence guarantees primarily in\nthe context of convex loss minimization problems like linear regression, which explains why prior work along\nthis line focused predominantly on simple convex loss minimization settings or on learning linear functions.\nWhen this approach is extended to tackle more general problems, the resulting approximation guarantees\nmust account for the optimization error inherent to these algorithms being approximated, thereby limiting\nthe efficacy of this technical approach for addressing broader nonconvex learning problems.\nTransformers as universal function approximators? In this work, we follow a fundamentally different\nroute: rather than designing transformers to approximate optimization algorithms as an intermediate step\ntowards approaching desirable solutions, we seek to investigate transformers’ capabilities as direct function\napproximators within the framework of in-context learning. While function approximation theory has been\nwell established for neural network models (e.g., Barron (1993); Hornik et al. (1994); Bach (2017); Kurková\nand Sanguineti (2002)), little work has been done on understanding the universal function approximation\ncapabilities of transformers in the context of ICL. It was largely unclear how transformers can learn universal\nrepresentation of a general class of functions while being fully adaptive to in-context examples.\n1.3 An overview of our main contributions\nIn this paper, we make progress towards understanding the approximation capability of transformers for\nin-context learning, with the aim of accommodating general function classes. More concretely, consider a\ngeneral class Fof functions mapping RdtoR, with each function representing a distinct task. Assuming\nthat the gradient of each function from Fhas a certain bounded Fourier magnitude norm, we show how\nto construct a universal transformer, with Llayers and input dimension O(d+n)for some large enough\nparameter n, such that: for every function f∈ F, this transformer can make reliable prediction given N\nin-context examples generated based on f(in a way that works universally across all tasks in F.) More\nprecisely, the mean squared prediction error after observing Nin-context examples is bounded by (up to\nsome logarithmic factor):r\n1\nN+n\nL+\u0012log|Nε|\nn\u00132/3\n,\nwithNεdenoting an ε-cover of the function class and input domain of interest. Clearly, the prediction error\ncan be vanishingly small with judiciously chosen parameters of the transformer. Notably, the term “universal\ntransformer” here refers to a model whose parameters depend only on F, without prior knowledge about the\nspecific in-context tasks to be performed.\nOur universality approximation theory implies the plausibility of designing a transformer that can predict\nin-context any target function in Fat inference time, without any sort of additional retraining or fine-tuning.\nFromthetechnicalperspective, ouranalysisconsistsof(i)identifyingacollectionofuniversalgeneral-purpose\nfeatures to linearly represent any function from the target function class F, and (ii) constructing transformer\nlayers to perform in-context computation of the optimal linear coefficients for the task on the fly. These\ndesign ideas shed light on how transformers can learn general-purpose representations for a complicated\nfunction class (far beyond linear functions) while adapting dynamically to in-context examples.\n1.4 Related work\nIn-context learning. The remarkable ICL capability of large language models (LLMs) has inspired an\nexplosion of recent research towards better understanding its emergence and inner workings from multi-\nple different perspectives. Several recent studies attempted to interpret transformer-based ICL through a\nBayesian lens (Xie et al., 2022; Ahuja et al., 2023; Zhang et al., 2023; Hahn and Goyal, 2023), while another\nstrand of work (Li et al., 2023; Kwon et al., 2025; Cole et al., 2024) analyzed the generalization and stability\nproperties of transformers in the context of ICL. Particularly relevant to the current paper is a seminal line\n3\n--- Page 4 ---\nof recent work exploring the representation power of transformers. For instance, Akyürek et al. (2023); Bai\net al. (2023); Von Oswald et al. (2023) demonstrated that transformers can implement gradient descent (GD)\nto perform linear regression in-context, whereas Guo et al. (2024) extended this capability to more complex\nscenarios involving linear functions built atop learned representations. Note, however, that empirical anal-\nysis conducted by Shen et al. (2023) revealed significant functional differences between ICL and standard\nGD in practical settings. Further bridging these perspectives, Vladymyrov et al. (2024) showed that linear\ntransformers can implement complex variants of GD for linear regression, while von Oswald et al. (2023);\nDai et al. (2022) unveiled connections between ICL and meta-gradient-based optimization. Additionally, Fu\net al. (2024); Giannou et al. (2023) constructed transformers capable of executing higher-order algorithms\nsuch as the Newton method. Expanding this further, Giannou et al. (2024); Furuya et al. (2024); Wang et al.\n(2024) showed that transformers can perform general computational operations and learn diverse function\nclasses in-context. More recently, Cole et al. (2025) investigated the representational capabilities of multi-\nlayer linear transformers, uncovering their potential to approximate linear dynamical systems in-context.\nFrom a complementary perspective through the lens of loss landscapes, Ahn et al. (2023); Mahankali et al.\n(2024); Cheng et al. (2024) showed that transformers can implement variants of preconditioned or functional\nGD in-context. Another important line of research investigated the optimization dynamics underlying trans-\nformers trained to perform ICL. For instance, Zhang et al. (2024); Kim and Suzuki (2024) analyzed training\ndynamics for linear-attention transformers, while Huang et al. (2024); Li et al. (2024a); Nichani et al. (2024);\nYang et al. (2024) studied softmax-attention transformers across a variety of ICL tasks, including linear\nregression (Huang et al., 2024), binary classification (Li et al., 2024a), causal structure learning (Nichani\net al., 2024), representation-based learning (Yang et al., 2024), and chain-of-thought (CoT) reasoning (Huang\net al., 2025). Furthermore, Chen et al. (2024b) explored the optimization dynamics of multi-head attention\nmechanisms tailored to linear regression settings.\nRepresentation theory of transformers. Substantial theoretical efforts have been recently devoted to\ncharacterizing the representational power and capabilities of transformers and self-attention mechanisms\nacross a variety of computational settings and statistical tasks (Pérez et al., 2019; Elhage et al., 2021; Liu\net al., 2022; Likhosherstov et al., 2021; Wen et al., 2023; Yao et al., 2021; Chen and Li, 2024). A promi-\nnent strand of recent research (Sanford et al., 2023; Wen et al., 2024; Jelassi et al., 2024) revealed notable\nadvantages of transformers over alternative architectures such as RNNs. Despite these advances, several\nwork (Hahn, 2020; Sanford et al., 2024; Peng et al., 2024; Chen et al., 2024a) also identified inherent limita-\ntions of transformers, proving that transformers might fail at certain computational tasks (e.g., parity) and\nestablishing complexity-theoretic lower bounds concerning their representational capabilities. Recently, mo-\ntivated by the widespread success of the CoT techniques — which explicitly leverage intermediate reasoning\nsteps — several work (Li et al., 2024b; Merrill and Sabharwal, 2024; Feng et al., 2023) began investigating\nthe theoretical foundations and expressive power of the CoT paradigm.\n1.5 Notation\nThroughout this paper, bold uppercase letters represent matrices, while bold lowercase letters represent\ncolumn vectors. For any vector v, we use ∥v∥2to denote its ℓ2norm, and ∥v∥1itsℓ1norm. For any matrix\nA, we denote by [A]i,jits(i, j)-th entry. The indicator function 1(·)takes the value 1when the condition in\nthe parentheses is satisfied and zero otherwise. The sign function sign(x)returns 1ifx >0,−1ifx <0, and 0\nifx= 0. Foranyscalarfunction σ:R→R, thenotation σ(x)forx∈Rddenotestheelementwiseapplication\nofσto each entry of x. LetX={N, L, n, log|Nε|, CF, σ}(and sometimes with the additional inclusion of\nsome precision parameter εpred). The notation f(X) =O(g(X))orf(X)≲g(X)(resp. f(X)≳g(X)) means\nthat there exists a universal constant C0>0such that f(X)≤C0g(X)(resp. f(X)≥C0g(X)) for any\nchoice of X. The notation f(X)≍g(X)means f(X)≲g(X)andf(X)≳g(X)hold simultaneously. We\ndefine eO(·)in the same way as O(·)except that it hides logarithmic factors. We also use 0to denote the\nall-zero vector. For any positive integer m, we denote [m]:={1, . . . , m }.\n4\n--- Page 5 ---\n2 Problem formulation\n2.1 Setting: in-context learning\nTo set the stage, we formulate an in-context learning setting that comprises the following components:\n•Function class. We denote by Fa class of real-valued functions, mapping from RdtoR, that we aim\nto learn. Each function f∈ Frepresents a distinct prediction task (i.e., given an input x, predict the\noutput f(x)).\n•Input sequence. The input sequence, typically provided in the prompt, is composed of Ninput-output\npairs — namely, Nin-context examples — along with a new input vector for prediction. To be precise,\na prompt takes the form of\u0000\nx1, y1,x2, y2, . . . ,xN, yN,xN+1\u0001\n, (1a)\nwhere for every i,\nxii.i.d.∼ D X, z ii.i.d.∼ D Z, y i=f(xi) +zifor some function f∈ F. (1b)\nHere,{xi} ⊂Rd(resp. {zi} ⊂R)areinputvectors(resp.noise)sampledrandomlyfromthedistribution\nDX(resp. DZ), and the corresponding output vectors are produced by some function f∈ Fnot\nrevealed to the learner. Throughout this paper, the noise {zi}is assumed to be independent zero-mean\nsub-Gaussian with the sub-Gaussian norm upper bounded by σ(Vershynin, 2018), that is,\nE[zi] = 0 and E[etzi]≤exp\u0010σ2t2\n2\u0011\nfor every t∈R. (2)\nFor simplicity, we assume throughout that all input vectors lie within a unit Euclidean ball:\nx∈ B:={u| ∥u∥2≤1}for any input vector x, (3)\nbut this assumption can be easily relaxed and generalized.\nIt is worth emphasizing that {(xi, yi)}should be regarded notas training examples, but rather as\nin-context demonstrations, as they are typically provided within the prompt at inference time instead\nof being used during the training phase.\nGoal.The aim is to design a transformer that, given an input sequence as in (1) produced by any function\nf∈ F, outputs a prediction byN+1obeying\nbyN+1≈f(xN+1)\nin some average sense. Particular emphasis is placed on universal design , where the objective is to find a\nsingle transformer (to be described next) that performs well simultaneously for all f∈ F, without knowing\nwhich fto tackle in advance. This universal design requirement aligns closely with the concept of in-context\nlearning, as the goal is for the pre-trained transformer to make reliable predictions based on input-output\ndemonstrations at inference time, without performing any prompt-specific parameter updates.\n2.2 Transformer architecture\nNext, let us present a precise description of the transformer architecture to be used for in-context learning.\nThroughout this paper, we would like to use a matrix\nH=\u0002\nh1,···,hN+1\u0003\n∈RD×(N+1)(4)\nto encode the input sequence (1a) comprising Ninput-output examples along with an additional new input.\nHere, the input dimension Dis typically chosen to be larger than dto allow for incorporation of several useful\n5\n--- Page 6 ---\nauxiliary features. For instance, in our construction (to be detailed momentarily), Htakes the following\nform:\nH=\nx1··· xNxN+1\n1 ··· 1 1\ny1··· yN 0\n...auxiliary info...\nby1··· byNbyN+1\n, (5)\nwhere each column entails the original input-output pair (xi, yi)(except the last column where yN+1is\nreplaced with 0), a constant 1, a few dimension containing auxiliary information, and the prediction byi.\nBasic building blocks. To begin with, we single out two basic building blocks.\n•Attention layer. For any input matrix H, the (self)-attention operator is defined as\nattn(H;Q,K,V):=1\nNV Hσattn\u0000\n(QH)⊤KH\u0001\n, (6)\nwhere Q,K,V∈RD×Drepresent the parameter matrices, commonly referred to as the query, key,\nand value matrices, respectively, and the activation function σattn(·)is applied either columnwise or\nentrywise to the input. A multi-head (self)-attention layer, which we denote by AttnΘ(·)as parameter-\nized by Θ={Qm,Km,Vm}1≤m≤M⊂RD×D, computes a superposition of the input and the outputs\nfrom Mattention operators (or attention heads). Namely, given the input matrix H, the output of\nthe attention layer with Mattention heads is defined as\nAttnΘ(H):=H+MX\nm=1attn(H;Qm,Km,Vm). (7)\nThis attention mechanism plays a pivotal role in the transformer architecture (Vaswani et al., 2017),\nallowing one to dynamically attend to different parts of the input data.\n•Feed-forward layer (or multilayer perceptron (MLP) layer). Given an input matrix H, the feed-forward\nlayer produces an output as follows:\nFFΘ(H):=H+Uσff(WH ), (8)\nwhere Θ={U,W} ⊂RD×Dbundles the parameter matrices UandWtogether, and the activation\nfunction σff(·)is applied entrywise to the input.\nThroughout this paper, the two activation functions described above are chosen to be the sigmoid function\nand the ReLU function:\nσattn(x) =ex\nex+ 1, σ ff(x) =x1(x >0), (9)\neach of which is applied entrywise to its respective input.\nMulti-layer transformers. With the aforementioned building blocks in place, we can readily introduce\nthe multi-layer transformer architecture. Given an input H(0)=H∈RD×(N+1), a transformer comprising\nLattention layers — each coupled with a feed-forward layer — carries out the following computation:\nH(l)=FFΘ(l)\nff\u0010\nAttnΘ(l)\nattn\u0000\nH(l−1)\u0001\u0011\n, l = 1, . . . , L, (10a)\nwith the final output given by\nTFΘ(H):=H(L). (10b)\n6\n--- Page 7 ---\nHere, Θencapsulates all parameter matrices:\nΘ=\b\nΘ(l)\nattn,Θ(l)\nff\t\n1≤l≤Lwith Θ(l)\nattn=\b\nQ(l)\nm,K(l)\nm,V(l)\nm\t\n1≤m≤M,Θ(l)\nff=\b\nU(l),W(l)\t\n⊂RD×D.\nIn particular, the transformer’s prediction for the (N+ 1)-th input can be read out from the very last entry\nofTFΘ(H), i.e.,\nbyN+1=ReadOut\u0000\nTFΘ(H)\u0001:=\u0002\nH(L)\u0003\nD,N+1. (11)\n2.3 Key quantities\nBefore embarking on our main theory, let us take a moment to isolate a couple of key quantities that play a\ncrucial role in our theoretical development.\nFor any absolutely integrable function f:Rd→R, we denote by Ffits Fourier transform, which allows\none to express\nFf(ω) =1\n2πZ\nxe−jω⊤xf(x)dx (12a)\nf(x) =Z\nωejω⊤xFf(ω)dω (12b)\nwith j=√−1the imaginary unit. It is also helpful to define, for each ω, the maximum magnitude of the\nFourier transform over the function class Fas:\nFsup(ω):= sup\nf∈F\f\fFf(ω)\f\f. (13)\nInspired by the seminal work Barron (1993), we introduce the following key quantity:\nCF:= sup\nf∈F|f(0)|+Z\nω∥ω∥2Fsup(ω)dω<∞. (14)\nInformally, this quantity bounds the first moment of the Fourier magnitude distribution over this function\nclassF. Compared with the quantity Cf:=R\nω∥ω∥2|Ff(ω)|dωintroduced in Barron (1993) for each function\nf, the main difference lies in the fact that CFinvolves taking the supremum over the entire function class F.\nAdditionally, recognizing that jωFf(ω)is precisely the Fourier transform of ∇f(x), one can alternatively\nexpress CFas\nCF= sup\nf∈F|f(0)|+Z\nωsup\nf∈F\r\rF∇f(ω)\r\r\n2dω, (15)\nwhich tracks the ℓ1norm of the maximum Fourier magnitude of the function gradient.\nRecall that we have restricted our input space to be within the unit Euclidean ball B={x∈Rd:∥x∥2≤\n1}. A set, denoted by Nε, is said to be an ε-cover of F × Bif, for every (f,x)∈ F × B , there exists some\n(bf,bx)∈ Nεsuch that\n∥x−bx∥2≤εand\f\ff(x)−f(0)−bf(bx) +bf(0)\f\f≤CFε. (16)\n3 Main results: transformers as universal in-context learners\nEquipped with the preliminaries and key quantities in Section 2, we are now ready to present our main\ntheoretical findings, as summarized in the theorem below.\nTheorem 1. LetNεbe an ε-cover of F × B(see(16)). Then one can construct a transformer such that:\ni) it has Llayers, M=O(1)attention heads per layer, and input dimension D=d+ 2n+ 7for some\nn≳log|Nε|;\n7\n--- Page 8 ---\nii) for every f∈ Fand any input sequence {(xi, yi)}1≤i≤N∪ {xN+1}generated according to (1), with\nprobability at least 1−O(N−10), this transformer’s prediction byN+1(cf.(11)) satisfies\nE\u0002\u0000\nbyN+1−f(xN+1)\u00012\u0003\n≲ r\nlogN\nN+n\nL!\nCF(CF+σ) +C2\nF\u0012log|Nε|\nn\u00132\n3\n, (17)\nHere, the precision of the ε-cover is taken to satisfy ε≲q\nlogN\nN+n\nL; the expectation in (17)is taken over\nthe randomness of xN+1; and the probability that the event (17)occurs is governed by the randomness in the\ninput sequence {(xi, yi)}1≤i≤N.\nIt is worth taking a moment to reflect on the interpretation and implications of this theorem.\nUniversal in-context prediction. Theorem 1 establishes the existence of a pretrained, multi-layer multi-\nhead transformer — configured with judiciously chosen parameters, depth, number of heads, etc. — that\ncan make reliable predictions based on in-context demonstrations. This in-context prediction capability is\nuniversal, in the sense that a single transformer can simultaneously handle all functions fin the function\nclass of interest, without requiring any additional training or prompt-specific parameter updates.\nParameter choices. According to Theorem 1, the in-context prediction error depends on the number of\nlayers L, the number of input-output examples N, the transformer’s input dimension D, and the intrinsic\ndata dimension d. Specifically, when both the Fourier quantity CFand the noise level σare no larger than\nO(1), the mean squared prediction error can’t exceed the order eO(1/√\nN+ 1/L+ (log |Nε|/n)2/3), where\n2n≈D−d. For instance, in the case of the linear function class F={f:f(x) =a⊤x+b,∥a∥2≤1,|b| ≤1},\nthe Fourier quantity satisfies CF=O(1)(Barron, 1993). This result implies that properly increasing the\nmodel complexity — through the use of deeper architectures (i.e., the ones with larger L) and higher\ninput dimension D— and utilizing more in-context examples (i.e., larger N)—could enhance the in-context\nlearning capability of the transformer, leading to improved prediction accuracy. More concretely, to achieve\nanεpred-accurate prediction (with εpredthe target mean squared prediction error), it suffices to employ a\ntransformer with parameters satisfying (up to log factors)\nD−d≍C3\nFε−3/2\npredlog|Nε|, (18a)\nN≳C2\nF(CF+σ)2ε−2\npred, (18b)\nL≳(D−d)CF(CF+σ)ε−1\npred≍C4\nF(CF+σ)ε−5/2\npredlog|Nε|. (18c)\nLogarithmic scaling on the covering number of the function class. Our approximation theory\nallows the function class of interest to be fairly general. Notably, both the input dimension (including that\nof auxiliary features) and the depth of the transformer we construct only need to scale logarithmically with\nthe covering number of the target function class (see (18)). In other words, in order to achieve sufficient\nrepresentation power for ICL, the model complexity needs to grow with the complexity of the target function\nclass — but a logarithmic scaling with the covering number of Fsuffices.\nFunction approximators vs. algorithm approximators. Theorem 1 unveils that transformers can\nserve as universal function approximators for in-context learning. This perspective contrasts sharply with a\nsubstantial body of recent work — e.g., Von Oswald et al. (2023); von Oswald et al. (2023); Bai et al. (2023);\nAhn et al. (2023); Giannou et al. (2023); Xie et al. (2022); Cheng et al. (2024) — which has primarily focused\non interpreting transformers as algorithm approximators. As alluded to previously, the approximation theory\nderived from the algorithm approximation perspective is often constrained by the effectiveness of the specific\nalgorithms being approximated. For instance, algorithms like gradient descent and Newton’s method are\ntypically not guaranteed to perform well outside the realm of convex optimization. This limitation partly\nexplains why much of the prior literature has concentrated on relatively simple convex problems, such as\nlinear regression. By contrast, the universal function approximation framework we deliver is not tied to the\nperformance of such (mesa)-optimization algorithms, and as a result, can often deliver direct approximation\nguarantees for much broader in-context learning problems.\n8\n--- Page 9 ---\n4 Analysis\nIn this section, we present the key steps for establishing Theorem 1. Informally, our proof comprises the\nfollowing key ingredients:\n•Identify a collection of general-purpose features such that every function (or task) in Fcan be (ap-\nproximately) represented as a combination of these features.\n•For each function f∈ F, the corresponding linear coefficients can be found by means of a Lasso\nestimator, which is efficiently solvable via the proximal gradient method.\n•A transformer can then be designed to approximate the above proximal gradient iterations.\nThroughout the proof, we let ϕ(x)denote the following sigmoid function:\nϕ(z) =\u0012\nz+1\n2\u0013\n1\u001a\nz+1\n2>0\u001b\n−\u0012\nz−1\n2\u0013\n1\u001a\nz−1\n2>0\u001b\n, (19)\nwhich satisfies limz→−∞ ϕ(z) = 0,limz→∞ϕ(z) = 1, and ϕ(0) = 1 /2.\nStep 1: constructing universal features for the target function class. In this step, we construct\na finite collection of features to approximately represent f(x)−f(0), with the aid of the sigmoid function\ndefined in (19). Our construction is formally presented in the following lemma (and its analysis), whose\nproof can be found in Appendix A.1.\nLemma 1. Consider any τ >4and any n≥c0log|Nε|for some large enough constant c0>0. There exist\na collection of functions ϕfeature\ni :Rd→R(1≤i≤n) such that: for every f∈ Fandx∈ B, one has\n\f\f\f\ff(x)−f(0)−1\nnnX\ni=1ρ⋆\nf,iϕfeature\ni (x)\f\f\f\f≲CF\u00121\nτ+τε+\u0010log|Nε|\nn\u00111\n3\u0013\n(20)\nfor some f-dependent coefficients {ρ⋆\nf,i}1≤i≤n⊂Robeying\n|f(0)|+1\nnnX\ni=1|ρ⋆\nf,i|<4CF. (21)\nHere, the functions {ϕfeature\ni (·)}1≤i≤nare given by\nϕfeature\ni (x) =ϕ\u0012\nτ\u00101\n∥ωi∥2ω⊤\nix−ti\u0011\u0013\n(22)\nfor some {(ti,ωi)}1≤i≤n⊂R×Rdindependent of any specific f, where ϕ(·)is defined in (19).\nIn words, for any function f∈ Fand any x∈ B, the quantity f(x)−f(0)can be closely approximated by\nalinearcombinationofthefeatures {ϕfeature\ni (x)}, wherethe ℓ1normofthelinearcoefficientsiswell-controlled.\nThis reveals that {ϕfeature\ni (·)}can serve as general-purpose features capable of linearly representing arbitrary\nfunctions in the function class F. We remark here that the f-dependent coefficients {ρ⋆\nf,i}1≤i≤nare not\nrequired to be positive. In the rest of the proof, we shall take\nτ= 1/√εand εdis:=cdisCF\u0012√ε+\u0010log|Nε|\nn\u00111\n3\u0013\n(23)\nfor some large enough constant cdis>0, which allow one to obtain (see Lemma 1)\n\f\f\f\ff(x)−f(0)−1\nnnX\ni=1ρ⋆\nf,iϕfeature\ni (x)\f\f\f\f≤εdisfor every f∈ Fandx∈ B. (24)\n9\n--- Page 10 ---\nStep 2: learning linear coefficients in-context via Lasso. Armed with the general-purpose features\n{ϕfeature\ni (·)}, we now proceed to show how the linear coefficients {ρ⋆\nf,i}in (20) can be approximately located\nin-context.\nRecallfromLemma1that {ρ⋆\nf,i}satisfysome ℓ1-normconstraint(cf.(21)). Withthisinmind, weattempt\nestimating {ρ⋆\nf,i}from the in-context demonstration {(xi, yi)}1≤i≤Nby means of the following regularized\nproblem (a.k.a. the Lasso estimator):\nminimize\nρ∈Rn+1ℓ(ρ):=1\nNNX\ni=1(yi−ϕ⊤\niρ)2+λ∥ρ∥1, (25)\nwhere λdenotes the regularized parameter to be suitably chosen, and ϕi∈Rn+1is defined as\nϕi=\u0002\nϕfeature\n1 (xi), ϕfeature\n2 (xi),···, ϕfeature\nn (xi),1\u0003⊤. (26)\nIn general, it is difficult to obtain an exact solution of (25), which motivates us to analyze approximate\nsolutions instead. More specifically, we would like to analyze the prediction error of any bρobeying\nℓ(bρ)−ℓ(ρ⋆)≤εopt (27)\nfor some accuracy level εopt, where ρ⋆∈Rn+1collects the f-dependent coefficients ρ⋆\nf,iin Lemma 1 in the\nfollowing way:\nρ⋆=\u0014ρ⋆\nf,1\nn,···,ρ⋆\nf,n\nn, ρ⋆\nf,0\u0015⊤\n,where ρ⋆\nf,0=f(0). (28)\nNote that in (27) we are comparing ℓ(bρ)with ℓ(ρ⋆)rather than that of the minimizer of (25), as it facilitates\nour analysis. The following lemma quantifies the prediction error and the ℓ1norm of any bρobeying (27),\nwhose proof is postponed to Appendix A.2.\nLemma 2. Consider any given λ≥cλ\u0012q\nlogN\nNσ+C−1\nFε2\ndis\u0013\nfor some sufficiently large constant cλ>0,\nwhere εdisis defined in (23). For any bρthat is statistically independent from xN+1and satisfies (27)with\nεopt≥0, we have\nEh\u0000\nϕ⊤\nN+1bρ−f(xN+1)\u00012i\n≲r\nlogN\nN\u0000\nC2\nF+λ−2ε2\nopt+σεdis\u0001\n+ε2\ndis+λCF+εopt (29)\n∥bρ∥1≲CF+λ−1εopt (30)\nwith probability at least 1−O(N−10). Here, the expectation in (29)is taken over the randomness of xN+1.\nRemark 1.In the statement of Lemma 2, bρis allowed to be statistically dependent on {(xi, yi)}1≤i≤Nbut\nnot on xN+1.\nStep 3: solving the Lasso (25)via the inexact proximal gradient method. In light of Lemma 2, it\nis desirable to make the optimization error εoptas small as possible. Here, we propose to run the (inexact)\nproximal gradient method in an attempt to solve (25). More precisely, starting from the initialization\nρproximal\n0 =0, the update rule for each iteration t= 0, . . . , Tis given by\nρproximal\nt+1 =proxproximal\nηλ∥·∥1\u0010\nρproximal\nt +2η\nNNX\ni=1(yi−ϕ⊤\niρproximal\nt )ϕi\u0011\n+et+1 (31a)\n=STηλ\u0010\nρproximal\nt +2η\nNNX\ni=1(yi−ϕ⊤\niρproximal\nt )ϕi\u0011\n+et+1, (31b)\n10\n--- Page 11 ---\nwhere η >0stands for the stepsize, and we have included an additive term et+1that allows for inexact\nupdates. Here, the proximal operator proxηλ∥·∥1(·)and the soft thresholding operator STηλ(·)are given\nrespectively by\nproxproximal\nηλ∥·∥1(x):= arg min\nρ\u001a1\n2∥x−ρ∥2\n2+ηλ∥ρ∥1\u001b\n, (32a)\nSTηλ(z):=sign(z) max{|z| −ηλ,0}. (32b)\nNote that STηλ(·)is applied entrywise in (31b).\nWe now develop convergence guarantees for the above proximal gradient method. It can be shown that:\nafter T= (L−1)/2iterations, the (inexact) proximal gradient method (31b) produces an iterate ρproximal\nT\nenjoying the following performance guarantees; the proof is postponed to Appendix A.3.\nLemma 3. Take T= (L−1)/2. Assume that\nρproximal\n0 =0,∥ρ⋆∥1≲CF, λ≳r\nlogN\nN(CF+σ), η=1\n2n,and∥et∥1≤εapprox≲CFfor all t≤T.\n(33)\nThen with probability at least 1−O(N−10), the output of the algorithm (31b)at the T-th iteration satisfies\nℓ(ρproximal\nT )≤ℓ(ρ⋆) +c1nC2\nF\nL+c1(L+n)εapprox\u0012\nCF+σ+ max\n1≤k≤T∥ρproximal\nk∥1+λ\u0013\n(34)\nfor some universal constant c1>0, and for every t≤Twe have\n∥ρproximal\nt ∥1≲CF+nC2\nF\ntλ+√\nN(t+n) max\n1≤k≤t∥ek∥1+t+n\nλmax\n1≤k≤t\b\n∥ek∥1∥ρproximal\nk∥1\t\n.(35)\nRemark 2.The careful reader might remark that the upper bounds in Lemma 3 appear somewhat intricate,\nas they depend on the ℓ1norm of the previous iterates. Fortunately, once {∥ek∥1}are determined, we can\napply mathematical induction to derive more concise bounds — an approach to be carried out in Step 4.\nStep 4: constructing the transformer to emulate proximal gradient iterations. To build a trans-\nformer with favorable in-context learning capabilities, our design seeks to approximate the above proximal\ngradient iterations, which we elucidate in this step.\nLet us begin by describing the input structure for each layer of our constructed transformer. For the l-th\nhidden layer ( 0≤l≤L), the input matrix H(l)(see (10)) takes the following form:\nH(l)=\nx(l)\n1x(l)\n2. . .x(l)\nNx(l)\nN+1\ny(l)\n1 y(l)\n2. . . y(l)\nN 0\nw(l)\n1w(l)\n2. . . w(l)\nNw(l)\nN+1\nϕ(l)\n1ϕ(l)\n2. . .ϕ(l)\nNϕ(l)\nN+1\nρ(l)ρ(l). . .ρ(l)ρ(l)\nλ(l)λ(l). . . λ(l)λ(l)\nby(l)by(l). . .by(l)by(l)\n∈R(d+2n+7)×(N+1)(36)\nwhere\nx(l)\ni∈Rd+1,ϕ(l)\ni,ρ(l)∈Rn+1,and w(l)\ni, λ(l),by(l)∈Rfor all 1≤i≤N+ 1.\nNote that the last three row blocks in (36) contain N+ 1identical copies of ρ(l),λ(l)andby(l). In particular,\nH(0)admits a simpler form, for which we initialize as follows:\nx(0)\ni= [x⊤\ni,1]⊤,ϕ(0)\ni=0 for all 1≤i≤N+ 1,(37a)\n11\n--- Page 12 ---\ny(0)\ni=yi, w(0)\ni= 1 for all 1≤i≤N, (37b)\ny(0)\nN+1= 0, w(0)\nN+1= 0,ρ(0)=0, λ(0)=by(0)= 0. (37c)\nAs a result, the input matrix H(0)can be simply expressed as\nH(0)=\nx1x2. . .xNxN+1\n1 1 . . . 1 1\ny1y2. . . y N 0\n1 1 . . . 1 0\n0 0 . . . 0 0\n∈R(d+2n+7)×(N+1). (38)\nBased on the above input structure of H(l), we are positioned to present our construction, whose proof\nis postponed to Appendix A.4.\nLemma 4. One can construct a transformer such that:\ni) it has Llayers, M=O(1)attention heads per layer, and takes the matrix H(0)(cf.(38)) as input;\nii) the component ρ(L)in the final output matrix H(L)coincides with ρproximal\n(L−1)/2(cf.(31b)) for some et,\nwhere we choose\nλ≍\u0010logN\nN\u00111/6\nC−1/3\nFbε2/3+r\nlogN\nN\u0010\nCF+σ\u0011\n+C−1\nFε2\ndis, (39a)\nmax\n1≤t≤T∥et∥1≲CF\n(L+n)nN, (39b)\nwith\nbε:=r\nlogN\nNCF(σ+CF) +ε2\ndis+nC2\nF\nL; (39c)\niii) for every f∈ F, the components ρ(L)andby(L)in the final output matrix H(L)(cf.(36)) satisfy\nℓ(ρ(L))−ℓ(ρ⋆)≲r\nlogN\nNCF(σ+CF) +ε2\ndis+nC2\nF\nL(40a)\n\u0000\nϕ⊤\nN+1ρ(L)−by(L)\u00012≲r\nlogN\nNC2\nF (40b)\nwith probability at least 1−O(N−10).\nIn words, Lemma 4 demonstrates how a single multi-layer transformer can be constructed to emulate the\niterations of the proximal gradient method (cf. (31b)) and achieve high optimization accuracy (see (40a)),\nwhile in the meantime controlling the fitting error between ϕ⊤\nN+1ρ(L)andby(L)) (see (40b)). And all this\nholds simultaneously for all fin the function class of interest.\nStep 5: putting everything together. Equipped with the above results, we are ready to put all pieces\ntogether towards establishing our theory.\nIn view of (40a), the output ρ(L)of our constructed transformer in Lemma 4 satisfies\nℓ(ρ(L))−ℓ(ρ⋆)≤εopt,\nby taking εopt≍r\nlogN\nNCF(σ+CF) +ε2\ndis+nC2\nF\nL. (41)\nInvoking Lemma 2 with bρ=ρ(L)reveals the following bound concerning the estimate ρ(L):\nE\u0002\u0000\nϕ⊤\nN+1ρ(L)−f(xN+1)\u00012\u0003(a)\n≲r\nlogN\nN\u0000\nC2\nF+λ−2ε2\nopt+σCF\u0001\n+ε2\ndis+λCF+εopt\n12\n--- Page 13 ---\n(b)\n≲r\nlogN\nNCF(CF+σ) +C2\nFε+C2\nF\u0012log|Nε|\nn\u00132/3\n+εopt+λCF+r\nlogN\nNλ−2ε2\nopt\n(c)\n≲r\nlogN\nNCF(CF+σ) +C2\nFε+C2\nF\u0012log|Nε|\nn\u00132/3\n+nC2\nF\nL+λCF+r\nlogN\nNλ−2ε2\nopt (42)\nholds with probability at least 1−O(N−10), where in (a) we have used (29) and a basic bound εdis≲CF\nthat holds under our assumptions, (b) arises from (23), and (c) results from (41). Next, let us bound the\nlast two terms in (42).\n•To begin with, with λas specified in (39a), we obtain\nλCF≲\u0010logN\nN\u00111/6\nC2/3\nFbε2/3+r\nlogN\nNCF\u0010\nCF+σ\u0011\n+ε2\ndis\n(a)\n≲r\nlogN\nNCF\u0010\nCF+σ\u0011\n+ε2\ndis+bε (43)\n(b)\n≲r\nlogN\nNCF\u0010\nCF+σ\u0011\n+C2\nFε+C2\nF\u0012log|Nε|\nn\u00132/3\n+bε, (44)\nwhere (a) invokes Young’s inequality to derive\n\u0010logN\nN\u00111/6\nC2/3\nFbε2/3≤1\n3\u0012\u0010logN\nN\u00111/6\nC2/3\nF\u00133\n+2\n3(bε2/3)3/2≤1\n3r\nlogN\nNC2\nF+2\n3bε,(45)\nand (b) applies (23).\n•Moreover, the last term in (42) satisfies\nr\nlogN\nNλ−2ε2\nopt(a)\n≲\u0010logN\nN\u00111/2\u0010logN\nN\u0011−1/3\nC2/3\nFbε−4/3ε2\nopt(b)\n≲\u0010logN\nN\u00111/6\nC2/3\nFbε2/3\n(c)\n≲r\nlogN\nNC2\nF+bε. (46)\nHere, (a) results from the fact that λ2≳\u0010\nlogN\nN\u00111/3\nC−2/3\nFbε2/3, (b) is valid since bε≍εopt(see (39c) and\n(41)), whereas (c) makes use of (45).\nSubstituting (44) and (46) into (42), and recalling the definition of bεin (39c), we arrive at\nE\u0002\u0000\nϕ⊤\nN+1ρ(L)−f(xN+1)\u00012\u0003\n≲r\nlogN\nNCF(σ+CF) +C2\nFε+C2\nF\u0010log|Nε|\nn\u00112\n3+nC2\nF\nL(47)\nwith probability exceeding 1−O(N−10), where we have used the fact that (see (39c) and (23))\nbε≲r\nlogN\nNCF(σ+CF) +C2\nFε+C2\nF\u0012log|Nε|\nn\u00132\n3\n+nC2\nF\nL.\nCombining (47) with (40b) and recalling byN+1=by(L)(cf. (11)), we reach\nE\u0002\u0000\nbyN+1−f(xN+1)\u00012\u0003\n≤2E\u0002\u0000\nbyN+1−ϕ⊤\nN+1ρ(L)\u00012\u0003\n+ 2E\u0002\u0000\nϕ⊤\nN+1ρ(L)−f(xN+1)\u00012\u0003\n≲r\nlogN\nNC2\nF+r\nlogN\nNCF(σ+CF) +C2\nFε+C2\nF\u0010log|Nε|\nn\u00112\n3+nC2\nF\nL\n≍r\nlogN\nNCF(σ+CF) +C2\nFε+C2\nF\u0010log|Nε|\nn\u00112\n3+nC2\nF\nL.\nWe can therefore conclude the proof of Theorem 1 by taking ε≤p\nlogN/N +n/L.\n13\n--- Page 14 ---\n5 Discussion\nIn this work, we have investigated the in-context learning capabilities of transformers through the lens\nof universal function approximation, establishing approximation guarantees that extend far beyond the\npreviously studied convex settings or the problems of learning linear functions. We have demonstrated that:\nfor a fairly general function class Fsatisfying mild Fourier-type conditions, one can construct a universal\nmulti-layer transformer achieving the following intringuing property: for every task represented by some\nfunction f∈ F, the constructed transformer can readily utilize the Ninput-output examples to achieve the\nprediction error on the order of 1/√\nN+n/L+ (log |Nε|/n)2/3(up to some logarithmic factor), where Nε\ndenotes an ε-cover and we choose the auxiliary input dimension nto exceed the log of a certain covering\nnumber. Our analysis imposes only fairly mild assumptions on F, requiring neither linearity in the function\nclass nor convexity in the learning problem, thereby offering a comprehensive theoretical understanding for\nthe empirical success of transformer-based models in real-world tasks.\nLookingforward, wewouldliketoestablishtightperformanceboundsonthepredictionerror, particularly\nwith respect to its dependence on the number of examples Nand the number of layers L. In addition, while\nour current analysis has focused on the approximation ability of a universal transformer, understanding how\npretraining or finetuning affects generalization in in-context learning remains an open question, which we\nleave for future studies. Finally, the current paper is concerned with constructed approximation, and it\nwould be of great interest to understand end-to-end training dynamics for transformers, which are highly\nnonconvex and call for innovative technical ideas.\nAcknowledgments\nG. Li is supported in part by the Chinese University of Hong Kong Direct Grant for Research and the Hong\nKong Research Grants Council ECS 2191363. Y. Wei is supported in part by the NSF grants CCF-2106778,\nCCF-2418156 and CAREER award DMS-2143215. Y. Chen is supported in part by the Alfred P. Sloan\nResearch Fellowship, the ONR grants N00014-22-1-2354 and N00014-25-1-2344, the NSF grants 2221009 and\n2218773, and the Amazon Research Award.\nA Proof of key lemmas\nIn this section, we provide complete proofs of the key lemmas introduced in Section 4.\nA.1 Proof of Lemma 1\nFor notational convenience, we shall write the Fourier transform Ffas\nFf(ω) =|Ff(ω)|ejθf(ω), (48)\nwith θf(ω)representing the angle. By virtue of the Fourier transform of f(x), we have\nf(x)−f(0) =Z\nωejω⊤xFf(ω)dω−Z\nωFf(ω)dω\n=Z\nω\u0000\nejω⊤xejθf(ω)−ejθf(ω)\u0001\n|Ff(ω)|dω\n(a)=Z\nω̸=0\u0010\ncos\u0000\nω⊤x+θf(ω)\u0001\n−cos\u0000\nθf(ω)\u0001\u0011\n|Ff(ω)|dω, (49)\nwhere (a) follows since f(x)is real-valued. In view of (49), we would like to construct a finite collection of\nfeatures to approximately represent f(x)−f(0), using the sigmoid function defined in (19).\nTowards this end, we first introduce a quantity related to the difference between ϕ(τx)and the unit step\nfunction 1(x >0)as follows:\nδτ:= inf\n0<ε≤1/2n\n2ε+ sup\n|x|≥ε\f\fϕ(τx)−1(x >0)\f\fo\n. (50)\n14\n--- Page 15 ---\nThe proof of Lemma 1 relies heavily upon the following result, whose proof is postponed to Appendix A.1.1.\nLemma 5. For any f∈ F, any xobeying ∥x∥2≤1, any positive integer m, and any τ >2, we have\n\f\ff(x)−f(0)−fapprox(x)\f\f≤CF\u0010\n3δτ+π\nm\u0011\n, (51)\nwhere for any f∈ Fwe define\nfapprox(x):=mX\nk=1Z\nω̸=0Z\ntρf(k, t,ω)ϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\nΛ(k,dt,dω) (52)\nwith ϕ(·)defined in (19). Here, Λ(k,dt,dω)is a probability measure on [m]×R×Rdindependent from f,\nwhile ρf(k, t,ω)denotes some weight function depending on f, k, t, ωsuch that\n|ρf(k, t,ω)| ≤3m\u0010\nCF−sup\n˜f∈F|˜f(0)|\u0011\nand E(k,t,ω)∼Λ\u0002\n|ρf(k, t,ω)|\u0003\n≤3\u0010\nCF−sup\n˜f∈F|˜f(0)|\u0011\n.(53)\nNext, we would like to obtain a more succinct finite-sum approximation of the integration in (52) via\nrandom subsampling. Let us draw nindependent samples (ki, ti,ωi)(1≤i≤n) from the probability\nmeasure Λ(k,dt,dω). Applying the Bernstein inequality and the union bound over Nεreveals that: with\nprobability at least 3/4,\n\f\f\f\fbfapprox(bx)−1\nnnX\ni=1ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\f\f\f\f≲max\n\ns\nmeC2\nFlog|Nε|\nn,meCFlog|Nε|\nn\n\n\nholds simultaneously for all (bf,bx)∈ Nε, where bfapprox(bx)is defined in (52) with fandxtaken respectively\nto bebfandbx, and\neCF:=CF−sup\n˜f∈F|˜f(0)|. (54)\nNote that the above application of the Bernstein inequality has used the following bounds for any fixed bf\nandbx:\nE\u0002\nρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\u0003\n=bfapprox(bx),\n\f\fρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\f\f≤3meCF,\nVar\u0000\nρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\u0001\n≤E\u0002\u0000\nρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\u00012\u0003\n≤ sup\n1≤k≤m,t∈R,ω∈Rd\f\fρbf(k, t,ω)\f\fE\u0002\n|ρbf(ki, ti,ωi)|\u0003\n≤9meC2\nF,\nwhere the last relation arises from (53).\nSimilarly, applying the Bernstein inequality and the union bound once again yields that: with probability\nat least 3/4,\n\f\f\f\fE(k,t,ω)∼Λ\u0002\f\fρbf(k, t,ω)\f\f\u0003\n−1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\f\f\f\f\f≲max\n\ns\nmeC2\nFlog|Nε|\nn,meCFlog|Nε|\nn\n\n\nholds simultaneously for all (bf,bx)∈ Nε, where we have used the following bounds:\n\f\fρbf(ki, ti,ωi)\f\f≤3meCF;\nVar\u0000\f\fρbf(ki, ti,ωi)\f\f\u0001\n≤E\u0002\f\fρbf(ki, ti,ωi)\f\f2\u0003\n≤ sup\n1≤k≤m,t∈R,ω∈Rd\f\fρbf(k, t,ω)\f\fE\u0002\f\fρbf(ki, ti,ωi)\f\f\u0003\n≤9meC2\nF.\n15\n--- Page 16 ---\nIfn≳mlog|Nε|andτ >4, then we can see thatq\nmeC2\nFlog|Nε|\nn≳meCFlog|Nε|\nn. Thus with probability at\nleast 3/4, one has\n1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\f<E(k,t,ω)∼Λ\u0002\f\fρbf(k, t,ω)\f\f\u0003\n+s\nc1meC2\nFlog|Nε|\nn(a)\n≤3eCF+eCF= 4eCF\nsimultaneously for all (bf,bx)∈ N ε, where c1>0is some universal constant, and (a) is valid under the\ncondition that n≥c1mlog|Nε|. This further shows that, with probability at least 3/4,\n|f(0)|+1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\f<|f(0)|+ 4eCF≤4CF\nsimultaneously for all (bf,bx)∈ Nε.\nHaving established several key properties for the epsilon-cover Nε, we now extend these properties to all\nf∈ F. For any f∈ F, there exists some (bf,bx)∈ Nε, such that for all ∥x∥2≤1,\n\f\f\f\ff(x)−f(0)−1\nnnX\ni=1ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nix−ti)\u0001\f\f\f\f\n≤\f\ff(x)−f(0)−bf(bx) +bf(0)\f\f+\f\f\f\fbf(bx)−bf(0)−1\nnnX\ni=1ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\f\f\f\f\n+\f\f\f\f1\nnnX\ni=1\u0010\nρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\n−ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nix−ti)\u0001\u0011\f\f\f\f\n(a)\n≤CF\u0010\nε+ 3δτ+π\nm+r\nc1mlog|Nε|\nn+ 4τε\u0011\n,\nwhere c1is a universal constant, (a) uses the definition of the epsilon-cover (16), and takes advantage of the\nfact that\n\f\f\f\f1\nnnX\ni=1\u0010\nρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\n−ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nix−ti)\u0001\u0011\f\f\f\f\n=1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\f\f\f\fϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nibx−ti)\u0001\n−ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nix−ti)\u0001\f\f\f\n≤1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\f\f\fτ∥ωi∥−1\n2ω⊤\ni(bx−x)\f\f\n≤1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\fτ∥bx−x∥2≤1\nnnX\ni=1\f\fρbf(ki, ti,ωi)\f\fτε≤4CFτε.\nRecalling that τ >4, and taking m=\u0006\n(n\nlog|Nε|)1/3\u0007\n, we arrive at\n\f\f\f\ff(x)−f(0)−1\nnnX\ni=1ρbf(ki, ti,ωi)ϕ\u0000\nτ(∥ωi∥−1\n2ω⊤\nix−ti)\u0001\f\f\f\f≲CF\u0010\nδτ+\u0012log|Nε|\nn\u00131\n3\n+τε\u0011\n.(55)\nTo finish up, it suffices to prove that δτ≤1/τ. Recalling the definition of δτin (50) and the function\nϕ(τx), we have\n|ϕ(τx)−1(x >0)|= (1/2−τ|x|)1{τ|x| ≤1/2},\nand as a result,\nsup\n|x|≥ε′\f\fϕ(τx)−1(x >0)\f\f= sup\n|x|≥ε′(1/2−τ|x|)1(τ|x| ≤1/2) = max\u001a1\n2−τε′,0\u001b\n.\n16\n--- Page 17 ---\nThis reveals that\nδτ= inf\n0≤ε′≤1/2\u001a\n2ε+ max\u001a1\n2−τε′,0\u001b\u001b\n=1\nτ\nfor any τ≥2. Substituting it into (55) and recalling the definition of ϕfeature\ni (x)in (22), we establish the\nclaimed result (20) with ρ⋆\nf,i=ρbf(ki, ti,ωi).\nA.1.1 Proof of Lemma 5\nConsider any positive integer m. According to (49), in order to approximate f(x)−f(0), it is helpful to\napproximate cos\u0000\nω⊤x+θf(ω)\u0001\n−cos\u0000\nθf(ω)\u0001\n. Towards this end, we first make the following observation\n\f\f\f\f∂\n∂θ\u0012cos(ω⊤x+θ)−cos(θ)\n∥ω∥2\u0013\f\f\f\f=\f\f\f\fsin(ω⊤x+θ)−sin(θ)\n∥ω∥2\f\f\f\f≤|ω⊤x|\n∥ω∥2≤∥ω∥2∥x∥2\n∥ω∥2= 1 (56)\nfor any θ∈[0,2π). Then (56) implies that, for any θ∈[0,2π), there exists some integer k(θ)obeying\n1≤k(θ)≤msuch that\nsup\nx∈B\f\f\f\fcos(ω⊤x+θ)−cos(θ)\n∥ω∥2−cos\u0000\nω⊤x+ 2πk(θ)/m\u0001\n−cos\u0000\n2πk(θ)/m\u0001\n∥ω∥2\f\f\f\f≤\f\f\f\fθ−2πk(θ)\nm\f\f\f\f≤π\nm.(57)\nIt has been shown by Barron (1993, Lemma 5) that: for any ω∈Rd,θ∈[0,2π)and any τ≥2, there exists\nsome function γω,θ(·)such that\n\f\f\f\fcos(ω⊤x+θ)−cos(θ)\n∥ω∥2−Z\ntγω,θ(t)ϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\ndt\f\f\f\f≤3δτ, (58a)\nZ\nt|γω,θ(t)|dt≤3. (58b)\nTo make it self-contained, we will present the proof of (58) towards the end of this section.\nCombine (57) and (58a) and invoke the triangle inequality to reach\n\f\f\f\fcos(ω⊤x+θ)−cos(θ)\n∥ω∥2−Z\ntγω,2πk(θ)/m(t)ϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\ndt\f\f\f\f\n≤\f\f\f\fcos(ω⊤x+θ)−cos(θ)\n∥ω∥2−cos\u0000\nω⊤x+ 2πk(θ)/m\u0001\n−cos\u0000\n2πk(θ)/m\u0001\n∥ω∥2\f\f\f\f\n+\f\f\f\fcos\u0000\nω⊤x+ 2πk(θ)/m\u0001\n−cos\u0000\n2πk(θ)/m\u0001\n∥ω∥2−Z\ntγω,2πk(θ)/m(t)ϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\ndt\f\f\f\f\n≤3δτ+π\nm.\nSubstitution into (49) then gives\n\f\f\f\ff(x)−f(0)−fapprox(x)\f\f\f\f≤\u0010\n2δτ+π\nm\u0011Z\nω∥ω∥2|Ff(ω)|dω≤CF\u0010\n3δτ+π\nm\u0011\n, (59)\nwhere we define\nfapprox(x):=Z\nω̸=0\u0012Z\ntϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\nγω,2πk(θf(ω))/m(t)dt\u0013\n∥ω∥2|Ff(ω)|dω, (60)\nand recall the definition of CFin (14).\nNext, let us define the following key quantity independent of f:\nΓF:=mX\nk=1Z\nω\u0012Z\nt|γω,2πk/m(t)|dt\u0013\n∥ω∥2Fsup(ω)dω,\n17\n--- Page 18 ---\nwhich can be bounded by\nΓF≤3mX\nk=1Z\nω∥ω∥2Fsup(ω)dω≤3m\u0000\nCF−sup\nf′∈F|f′(0)|\u0001\n<∞ (61)\nfor any f∈ F. As a result,\nΛ(k,dt,dω):= Γ−1\nF|γω,2πk/m(t)|∥ω∥2Fsup(ω)dtdω (62)\nforms a valid probability measure on [m]×R×Rd, given that\nΛ(k,dt,dω)≥0andmX\nk=1Z\nωZ\ntΛ(k,dt,dω) = 1 .\nImportantly, this probability measure Λ(k,dt,dω)allows one to express the function fapprox(x)(cf. (60)) as\nfapprox(x) =mX\nk=1Z\nω̸=0Z\ntρf(k, t,ω)ϕ\u0000\nτ(∥ω∥−1\n2ω⊤x−t)\u0001\nΛ(k,dt,dω),\nwhere\nρf(k, t,ω):=γω,2πk/m(t)1\b\nk=k(θf(ω))\t\n∥ω∥2|Ff(ω)|dtdω\nΛ(k,dt,dω). (63)\nSubstitution into (59) immediately establishes the advertised result (51).\nTo finish up, it suffices to observe that (i)\n|ρf(k, t,ω)| ≤|γω,2πk/m(t)|1\b\nk=k(θf(ω))\t\n∥ω∥2|Ff(ω)|\nΓ−1\nF|γω,2πk/m(t)|∥ω∥2Fsup(ω)≤ΓF≤3m\u0010\nCF−sup\n˜f∈F|˜f(0)|\u0011\nas a consequence of (61), (62) and (63), and (ii)\nmX\nk=1Z\nω̸=0Z\nt|ρf(k, t,ω)|Λ(k,dt,dω) =Z\nω̸=0Z\nt\f\fγω,2πk(θf(ω))/m(t)\f\fdt∥ω∥2|Ff(ω)|dω\n≤3Z\nω̸=0∥ω∥2|Ff(ω)|dω≤3\u0010\nCF−sup\n˜f∈F|˜f(0)|\u0011\nas a result of (58b).\nProof of (58).This proof is similar to the proof of Lemma 5 in Barron (1993). We begin by claiming the\nfollowing result (to be proven shortly): for any function g: [−1,1]→Rsatisfying |g′(z)| ≤1,|g(−1)| ≤1,\nand for any τ >0, there exists a function γ(·)such that\n\f\f\f\fg(z)−Z\nγ(t)ϕ\u0000\nτ(z+t)\u0001\ndt\f\f\f\f≤3δτ, (64a)\nwhere\nZ\n|γ(t)|dt≤3. (64b)\nHere, δτandϕ(·)have been defined in (50) and (19), respectively. Suppose for the moment that this claim\nis valid. To establish (58), we find it convenient to introduce another auxiliary function\ngω,θ(z):=cos(∥ω∥2z+θ)−cos(θ)\n∥ω∥2\n18\n--- Page 19 ---\ndefined on [−1,1], which clearly satisfies\n\f\fg′\nω,θ(z)\f\f≤1for all z∈[−1,1],and |gω,θ(−1)|=|gω,θ(−1)−gω,θ(0)| ≤1.\nApplying inequality (64) to gω,θ(·)and recognizing that gω,θ\u0000ω⊤x\n∥ω∥2\u0001\n=cos(ω⊤x+θ)−cos(θ)\n∥ω∥2establish (58).\nTo finish up, it remains to establish (64). Define\nγ(t) =\n\ng′(t), ift∈[−1,1],\nsign\u0000\ng(−1)\u0001\n,if−1− |g(−1)| ≤t <−1,\n0, else.(65)\nwhich clearly satisfies\n|γ(t)| ≤1,Z\nt|γ(t)|dt=Z1\n−1−|g(−1)||γ(t)|dt≤2 +|g(−1)| ≤3.\nObserve that for any z∈[−1,1], it holds that\nZ∞\n−∞γ(t)1(z−t >0)dt=Zz\n−∞γ(t)d =Z−1\n−1−|g(−1)|sign\u0000\ng(−1)\u0001\ndt+Zz\n−1g′(t)dt=g(−1) +g(z)−g(−1) = g(z).\nRecalling the definition of δτin (50), we arrive at\n\f\f\f\fg(z)−Z∞\n−∞γ(t)ϕ\u0000\nτ(z−t)\u0001\ndt\f\f\f\f=\f\f\f\fZ∞\n−∞γ(t)\u0000\n1(z−t >0)−ϕ\u0000\nτ(z−t)\u0001\u0001\ndt\f\f\f\f\n≤inf\n0<ε≤1/2(Z\nt:|z−t|≤ε|γ(t)|dt+Z\nt:|z−t|≥ε|γ(t)|\f\f1(z−t >0)−ϕ\u0000\nτ(z−t)\u0001\f\fdt)\n≤inf\n0<ε≤1/2(\n2ε+ 3 sup\nx:|x|≥ε\f\fϕ(τx)−1(x >0)\f\f)\n≤3δτ\nas claimed.\nA.2 Proof of Lemma 2\nIn view of (21) in Lemma 1, we have\n∥ρ⋆∥1≲CF.\nA key step in this proof is to establish the following lemma, whose proof is postponed to Appendix A.2.1.\nLemma 6. With probability at least 1−O(N−11), for any ρ∈Rn+1(which can be statistically dependent\non{(xi, yi)}1≤i≤Nbut not on xN+1), we have\n\f\f\f\f1\nNNX\ni=1\u0002\n(yi−ϕ⊤\niρ)2−z2\ni\u0003\n−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ\u00012\u0003\f\f\f\f\n≲r\nlogN\nN\u0000\nε2\ndis+∥ρ−ρ⋆∥2\n1+σ(εdis+∥ρ−ρ⋆∥1)\u0001\n(66)\nand1\nNNX\ni=1\u0002\n(yi−ϕ⊤\niρ)2−(yi−ϕ⊤\niρ⋆)2\u0003\n≳−ε2\ndis−r\nlogN\nNσ∥ρ−ρ⋆∥1. (67)\nHere, the expectation in (66)is taken over the randomness of xN+1.\nArmed with this lemma, we can proceed to the proof of Lemma 2.\n19\n--- Page 20 ---\nProof of (30).Suppose for the moment that\n∥bρ∥1>4∥ρ⋆∥1+CF+ 4λ−1εopt, (68)\nthen it follows from Lemma 6 that\nℓ(bρ)−ℓ(ρ⋆) =1\nNNX\ni=1(yi−ϕ⊤\nibρ)2+λ∥bρ∥1−1\nNNX\ni=1(yi−ϕ⊤\niρ⋆)2−λ∥ρ⋆∥1\n(a)\n≥λ∥bρ∥1−λ∥ρ⋆∥1−Cε2\ndis−Cr\nlogN\nNσ∥bρ−ρ⋆∥1\n(b)\n> λ∥bρ∥1−λ∥bρ∥1\n4−Cε2\ndis−2Cr\nlogN\nNσ∥bρ∥1 (69)\nfor some universal constant C >0. Here, (a) results from (67), whereas (b) invokes the properties (see (68))\nthat∥bρ∥1>4∥ρ⋆∥1and∥bρ−ρ⋆∥1≤ ∥bρ∥1+∥ρ⋆∥1<2∥bρ∥1. In addition, under our assumption on λ, we\nhave\nλ≥4CC−1\nFε2\ndis+ 8Cp\nlogN/Nσ (70)\nas long as cλis large enough, which then gives\nCε2\ndis≤λCF\n4<λ∥bρ∥1\n4and 2Cr\nlogN\nNσ∥bρ∥1<λ∥bρ∥1\n4.\nThese combined with (69) result in\nℓ(bρ)−ℓ(ρ⋆)> λ∥bρ∥1−λ∥bρ∥1\n4−λ∥bρ∥1\n4−λ∥bρ∥1\n4=λ\n4∥bρ∥1> ε opt\nwith the last inequality due to (68). This, however, contradicts the εopt-optimality of bρ, which in turn\njustifies that the assumption (68) cannot possibly hold. As a result, we can conclude that\n∥bρ1∥ ≤4∥ρ⋆∥1+CF+ 4λ−1εopt≍CF+λ−1εopt, (71)\nwhere the last relation is valid since ∥ρ⋆∥1≲CF(see (21) and (28)).\nProof of (29).Applying (66) in Lemma 6 and making use of the fact that ∥bρ−ρ⋆∥1≤ ∥bρ∥1+∥ρ⋆∥1≲\nCF+λ−1εopt(see (71)), one can demonstrate that\n\f\f\f\fE\u0002\u0000\nϕ⊤\nN+1bρ−f(xN+1)\u00012\u0003\n−1\nNNX\ni=1\u0002\n(yi−ϕ⊤\nibρ)2−z2\ni\u0003\f\f\f\f\n≲r\nlogN\nN\u0000\nε2\ndis+C2\nF+λ−2ε2\nopt+σ(εdis+CF+λ−1εopt)\u0001\n. (72)\nMoreover, we make the observation that\n1\nNNX\ni=1(yi−ϕ⊤\nibρ)2≤ℓ(bρ)≤ℓ(ρ⋆) +εopt≤1\nNNX\ni=1(yi−ϕ⊤\niρ⋆)2+λ∥ρ⋆∥1+εopt\n=1\nNNX\ni=1z2\ni+1\nNNX\ni=1\u0000\nf(xi)−ϕ⊤\niρ⋆\u00012+2\nNNX\ni=1zi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\n+λ∥ρ⋆∥1+εopt\n≤1\nNNX\ni=1z2\ni+O\u0010\nε2\ndis+r\nlogN\nNσεdis+λCF\u0011\n+εopt, (73)\n20\n--- Page 21 ---\nwhere the last inequality follows from the properties (f(xi)−ϕ⊤\niρ⋆)2≲ε2\ndisand∥ρ⋆∥1≲CF(see (24) and\n(21)), as well as the concentration bound for1\nNPN\ni=1zi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\nto be derived in (79). Combine (72)\nand (73) to derive\nE\u0002\u0000\nϕ⊤\nN+1bρ−f(xN+1)\u00012]≲r\nlogN\nN\u0000\nε2\ndis+C2\nF+λ−2ε2\nopt+σ(εdis+CF+λ−1εopt)\u0001\n+ε2\ndis+λCF+εopt\n≍r\nlogN\nN\u0000\nC2\nF+λ−2ε2\nopt+σεdis\u0001\n+ε2\ndis+λCF+εopt,\nwhere the last line follows sinceq\nlogN\nNσCF≲λCFandq\nlogN\nNσλ−1εopt≲εopt(see (70)).\nA.2.1 Proof of Lemma 6\nWe begin by establishing (66). Given that yi=f(xi) +zi, we make note of the following decomposition:\n\f\f\f\f1\nNNX\ni=1\u0002\n(yi−ϕ⊤\niρ)2−z2\ni\u0003\n−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ\u00012\u0003\f\f\f\f\n=\f\f\f\f1\nNNX\ni=1n\u0000\nf(xi)−ϕ⊤\niρ\u00012−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ\u00012\u0003\n+ 2zi\u0000\nf(xi)−ϕ⊤\niρ\u0001o\f\f\f\f\n≤\f\f\f\f1\nNNX\ni=1n\u0000\nf(xi)−ϕ⊤\niρ\u00012−\u0000\nf(xi)−ϕ⊤\niρ⋆\u00012o\n+E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ⋆\u00012\u0003\n−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ\u00012\u0003\f\f\f\f\n+\f\f\f\f1\nNNX\ni=1\u0000\nf(xi)−ϕ⊤\niρ⋆\u00012−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ⋆\u00012\u0003\f\f\f\f+ 2\f\f\f\fNX\ni=1zi\u0000\nf(xi)−ϕ⊤\niρ\u0001\f\f\f\f=:E1+E2+E3,\n(74)\nleaving us with three terms to cope with.\n•Regarding the term E1, we can apply a little algebra to derive\nE1≤\f\f\f\f2\nNNX\ni=1(ρ⋆−ρ)⊤ϕi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\n−2E\u0002\n(ρ⋆−ρ)⊤ϕN+1\u0000\nf(xN+1)−ϕ⊤\nN+1ρ⋆\u0001\u0003\f\f\f\f\n+\f\f\f\f1\nNNX\ni=1\u0000\n(ρ⋆−ρ)⊤ϕi\u00012−E\u0002\u0000\n(ρ⋆−ρ)⊤ϕN+1\u00012\u0003\f\f\f\f\n≤2∥ρ⋆−ρ∥1\r\r\r\r1\nNNX\ni=1ϕi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\n−E\u0002\nϕN+1\u0000\nf(xN+1)−ϕ⊤\nN+1ρ⋆\u0001\u0003\r\r\r\r\n∞\n+∥ρ⋆−ρ∥2\n1\r\r\r\r1\nNNX\ni=1ϕiϕ⊤\ni−E\u0002\nϕN+1ϕ⊤\nN+1\u0003\r\r\r\r\n∞, (75)\nwhere the first line arises from the elementary inequality (a−b)2−(a−c)2=−(c−b)2+2(c−b)(a−b)\nin conjunction with the triangle inequality. Here, for any matrix Awe denote by ∥A∥∞= max i,j|Ai,j|\nthe entrywise ℓ∞norm. Recognizing that ∥ϕi∥∞≤1(see (26) and (22), as well as ϕ(z)∈[−1,1]as in\n(19)), we obtain\n\r\rϕi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\r\r\n∞≤ ∥ϕi∥∞sup\nx∈B\f\ff(x)−eϕ(x)⊤ρ⋆\f\f(a)\n≲εdis,\n\r\rϕiϕ⊤\ni\r\r\n∞≤ ∥ϕi∥2\n∞≤1\n21\n--- Page 22 ---\nwitheϕ(x) = [ϕfeature\n1 (x),···, ϕfeature\nn (x),1]⊤andεdisdefined in (23). Here, (a) follows since\nsup\nx∈B\f\ff(x)−eϕ(x)⊤ρ⋆\f\f≤εdis+\f\ff(0)−ρ⋆\nf,0\f\f=εdis, (76)\nwhich holds due to (24) as well as our choice ρ⋆\nf,0=f(0)(see (28)). Then applying Hoeffding’s\ninequality shows that, with probability at least 1−O(N−12),\n\r\r\r\r1\nNNX\ni=1ϕi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\n−E\u0002\nϕN+1\u0000\nf(xN+1)−ϕ⊤\nN+1ρ⋆\u0001\u0003\r\r\r\r\n∞≲r\nlogN\nNεdis,\n\r\r\r\r1\nNNX\ni=1ϕiϕ⊤\ni−E\u0002\nϕN+1ϕ⊤\nN+1\u0003\r\r\r\r\n∞≲r\nlogN\nN.\nSubstitution into (75) yields\nE1≲r\nlogN\nN∥ρ−ρ⋆∥1\u0000\n∥ρ−ρ⋆∥1+εdis\u0001\n(77)\nwith probability exceeding 1−O(N−12).\n•With regards to E2, taking the Hoeffding inequality in conjunction with (76) tell us that, with proba-\nbility exceeding 1−O(N−12),\nE2≲r\nlogN\nNε2\ndis. (78)\n•We now turn to E3. Recalling that ∥ϕi∥∞≤1and making use of (76), we apply the triangle inequality\nto obtain\nE3≤2\f\f\f\f1\nNNX\ni=1zi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\f\f\f\f+ 2\f\f\f\f1\nNNX\ni=1ziϕ⊤\ni(ρ−ρ⋆)\f\f\f\f\n≤2\f\f\f\f1\nNNX\ni=1zi\u0000\nf(xi)−ϕ⊤\niρ⋆\u0001\f\f\f\f+ 2\r\r\r\r1\nNNX\ni=1ziϕi\r\r\r\r\n∞∥ρ−ρ⋆∥1\n≲r\nlogN\nNσ\u0000\nεdis+∥ρ−ρ⋆∥1\u0001\n, (79)\nwhere we have used the sub-Gaussian assumption on {zi}.\nSubstituting (77), (78) and (79) into (74) reveals that, with probability at least 1−O(N−12),\n\f\f\f\f1\nNNX\ni=1\u0002\n(yi−ϕ⊤\niρ)2−z2\ni\u0003\n−E\u0002\u0000\nf(xN+1)−ϕ⊤\nN+1ρ\u00012\u0003\f\f\f\f\n≲r\nlogN\nN\u0000\n∥ρ−ρ⋆∥2\n1+∥ρ−ρ⋆∥1εdis+ε2\ndis+σ∥ρ−ρ⋆∥1+σεdis\u0001\n≍r\nlogN\nN\u0000\n∥ρ−ρ⋆∥2\n1+ε2\ndis+σ∥ρ−ρ⋆∥1+σεdis\u0001\n.\nNext, we turn to proving (67). In view of the Hoeffding inequality, with probability at least 1−O(N−12)\nwe have\n1\nNNX\ni=1\u0002\n(yi−ϕ⊤\niρ)2−(yi−ϕ⊤\niρ⋆)2\u0003\n=1\nNNX\ni=1\u0002\u0000\nf(xi)−ϕ⊤\niρ\u00012−\u0000\nf(xi)−ϕ⊤\niρ⋆\u00012\u0003\n+2\nNNX\ni=1ziϕ⊤\ni(ρ⋆−ρ)\n22\n--- Page 23 ---\n≥ −1\nNNX\ni=1\u0000\nf(xi)−ϕ⊤\niρ⋆\u00012−\r\r\r\r2\nNNX\ni=1ziϕi\r\r\r\r\n∞∥ρ⋆−ρ∥1\n≳−ε2\ndis−r\nlogN\nNσ∥ρ⋆−ρ∥1.\nwhere we have used the following facts (already proven previously):\n|f(xi)−ϕ⊤\niρ⋆|≲εdis,\r\r\r\r1\nNNX\ni=1ziϕi\r\r\r\r\n∞≲r\nlogN\nNσ.\nA.3 Proof of Lemma 3\nTo begin with, we find it convenient to introduce an auxiliary sequence ρ⋆\nt+1obeying ρ⋆\n0=0and\nρ⋆\nt+1=STηλ \nρproximal\nt +2η\nNNX\ni=1\u0000\nyi−ϕ⊤\niρproximal\nt\u0001\nϕi!\n,\nwhere ρ⋆\nt+1is obtained by running one exactproximal gradient iteration from ρproximal\nt. Standard convergence\nanalysis for the proximal gradient method (e.g., Beck (2017)) reveals that\nℓ(ρ⋆\nt+1)≤ℓ(ρproximal\nt ), (80a)\nℓ(ρ⋆\nt+1)−ℓ(ρ⋆)≤n\u0000\n∥ρproximal\nt −ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2\u0001\n, (80b)\nwhere we recall our choice that η= 1/(2n). For completeness, we shall provide the proof of (80) towards\nthe end of this subsection.\nRecognizing that ρproximal\nt =ρ⋆\nt+etfor some additive term et, we can invoke (80b) to show that\nℓ(ρ⋆\nt+1)−ℓ(ρ⋆)≤n\u0000\n∥ρproximal\nt −ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2\u0001\n=n\u0000\n∥ρ⋆\nt−ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2+∥et∥2\n2+ 2e⊤\nt(ρ⋆\nt−ρ⋆)\u0001\n≤n\u0000\n∥ρ⋆\nt−ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2+∥et∥2\n1+ 2∥et∥1(O(CF) +∥ρ⋆\nt∥1)\u0001\n≤n\u0000\n∥ρ⋆\nt−ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2+∥et∥2\n1+ 2∥et∥1(O(CF) +∥ρproximal\nt ∥1+∥et∥1)\u0001\n≤n\u0000\n∥ρ⋆\nt−ρ⋆∥2\n2− ∥ρ⋆\nt+1−ρ⋆∥2\n2+c1∥et∥1(CF+∥ρproximal\nt ∥1)\u0001\n(81)\nfor some universal constant c1>0, where we have used ∥ρ⋆∥1≲CF(cf. (21) and (28)) and the assumption\n∥et∥1≲CF. Define\nkt= arg min\n1≤k≤tℓ(ρ⋆\nk).\nSumming (81) over iterations 0tot, we obtain a telescoping sum and can then deduce that\nℓ(ρ⋆\nkt)−ℓ(ρ⋆) = min\n1≤k≤tℓ(ρ⋆\nk)−ℓ(ρ⋆)\n≤1\nttX\nk=1\u0000\nℓ(ρ⋆\nk)−ℓ(ρ⋆)\u0001\n≤n∥ρproximal\n0 −ρ⋆∥2\n2\nt+c1nmax\n1≤k<t\b\n∥ek∥1(CF+∥ρproximal\nk∥1)\t\n=n∥ρ⋆∥2\n1\nt+c1nmax\n1≤k<t\b\n∥ek∥1(CF+∥ρproximal\nk∥1)\t\n, (82)\nwhere the last line follows since ρproximal\n0 =0.\nIn addition, it is seen that\nℓ(ρproximal\nt+1 )−ℓ(ρ⋆\nt+1) =1\nNNX\ni=1\u0000\nyi−ϕ⊤\niρproximal\nt+1\u00012−1\nNNX\ni=1\u0000\nyi−ϕ⊤\niρ⋆\nt+1\u00012+λ\r\rρproximal\nt+1\r\r\n1−λ∥ρ⋆\nt+1∥1\n23\n--- Page 24 ---\n≤2\nNNX\ni=1ϕ⊤\ni\u0000\nρ⋆\nt+1−ρproximal\nt+1\u0001\u0000\nyi−ϕ⊤\niρproximal\nt+1\u0001\n+λ\r\rρproximal\nt+1−ρ⋆\nt+1\r\r\n1\n≤\r\rρ⋆\nt+1−ρproximal\nt+1\r\r\n1 \r\r\r\r\r2\nNNX\ni=1\u0000\nyi−ϕ⊤\niρproximal\nt+1\u0001\nϕi\r\r\r\r\r\n∞+λ!\n(a)\n≤c2∥et+1∥1\u0000\nCF+σ+∥ρproximal\nt+1∥1+λ\u0001\n(83)\nfor some universal constant c2>0, where the first inequality comes from the elementary inequality (a−\nb)2−(a−c)2=−(c−b)2+ (c−b)(2a−2b)≤2(c−b)(a−b)as well as the triangle inequality. Here, (a)\nfollows since\n1\nNNX\ni=1|yi| ≤1\nNNX\ni=1|f(xi)|+1\nNNX\ni=1|zi| ≤max\n1≤i≤N|ϕ⊤\niρ⋆|+εdis+\f\f\f\f\f1\nNNX\ni=1|zi| −E[|z|]\f\f\f\f\f+E[|z|]\n≲∥ρ⋆∥1max\n1≤i≤N∥ϕi∥∞+εdis+σ≲∥ρ⋆∥1+εdis+CF+σ≍CF+σ,\na consequence of the sub-Gaussian assumption on {zi}and the facts that ∥ϕi∥∞≤1,∥ρ⋆∥1≲CF, and\nεdis=CF\u0010√ε+ (log|Nε|\nn)1/3\u0011\n≲CFfor the assumption that ε≲p\nlogN/N +n/Landn≳log|Nε|.\nRecalling that ℓ(ρ⋆\nt+1)≤ℓ(ρproximal\nt )(see (80a)), we can invoke the bound (83) recursively to derive\nℓ(ρproximal\nt+1 )≤ℓ(ρ⋆\nt+1) +c2∥et+1∥1\u0000\nCF+σ+∥ρproximal\nt+1∥1+λ\u0001\n≤ℓ(ρproximal\nt ) +c2∥et+1∥1\u0000\nCF+σ+∥ρproximal\nt+1∥1+λ\u0001\n(84)\n≤ℓ(ρ⋆\nt) + 2c2max\nt≤k≤t+1\b\n∥ek∥1\u0000\nCF+σ+∥ρproximal\nk∥1+λ\u0001\t\n≤ℓ(ρ⋆\nkt) +c2(t+ 1−kt) max\n1≤k≤t+1\b\n∥ek∥1(CF+σ+∥ρproximal\nk∥1+λ)\t\n. (85)\nIt then follows from (82) that\nℓ(ρproximal\nt+1 )−ℓ(ρ⋆)≤n∥ρ⋆∥2\n1\nt+c1nmax\n1≤k≤t−1\b\n∥ek∥1(CF+∥ρproximal\nk∥1)\t\n+c2(t+ 1) max\n1≤k≤t+1\b\n∥ek∥1(CF+σ+∥ρproximal\nk∥1+λ)\t\n≤n∥ρ⋆∥2\n1\nt+c3(t+n+ 1) max\n1≤k≤t+1\b\n∥ek∥1(CF+σ+∥ρproximal\nk∥1+λ)\t\n,\nwhere c3= max {c1, c2}. Recalling that ∥ρ⋆∥1≲CF(cf. (21) and (28)) as well as our parameter choice\nT=L−1\n2, we have thus completed the proof of (34).\nIn addition, combining (30) in Lemma 2 with the above result, we know that for λ≳q\nlogN\nN(CF+σ),\n∥ρproximal\nt ∥1≲CF+λ−1\u0012n∥ρ⋆∥2\n1\nt−1+ (t+n) max\n1≤k≤t\b\n∥ek∥1(CF+σ+∥ρproximal\nk∥1+λ)\t\u0013\n≲CF+n∥ρ⋆∥2\n1\ntλ+√\nN(t+n) max\n1≤k≤t∥ek∥1+t+n\nλmax\n1≤k≤t\b\n∥ek∥1∥ρproximal\nk∥1\t\n, t≥2.\nFort= 0andt= 1, we have ∥ρproximal\n0 ∥1= 0, and with probability at least 1−O(N−20),\n∥ρproximal\n1 ∥1≤nX\nk=1max(\r\r\r\r\r2η\nNNX\ni=1yiϕi\r\r\r\r\r\n∞+|e1,k| −λη,0)\n≤nX\nk=1max(\nc\nn \nCF+r\nlogN\nNσ!\n+|e1,k| −λ\n2n,0)\n≲CF+∥e1∥1,\nprovided that λ≥2cq\nlogN\nNσ. Here, e1,kdenotes the k-th element of e1.\n24\n--- Page 25 ---\nProof of (80a)and(80b).Define\ng(ρ) =1\nNNX\ni=1(yi−ϕ⊤\niρ)2,\nψ(ρ) =g(ρproximal\nt ) + (ρ−ρproximal\nt )⊤∇g(ρproximal\nt ) +n∥ρ−ρproximal\nt ∥2\n2+λ∥ρ∥1.\nRecall that η= 1/(2n). It is self-evident that ρ⋆\nt+1= arg min ρψ(ρ)andψ(·)is(2n)-strongly convex. Thus,\nfor any bρ, we have\nψ(bρ)≥ψ(ρ⋆\nt+1) +n∥ρ⋆\nt+1−bρ∥2\n2. (86)\nIn addition, observe that\ng(ρproximal\nt ) + (ρ−ρproximal\nt )⊤∇g(ρproximal\nt ) +n∥ρ−ρproximal\nt ∥2\n2\n=g(ρproximal\nt )−2\nNNX\ni=1(ρ−ρproximal\nt )⊤(yi−ϕ⊤\niρ)ϕi+n∥ρ−ρproximal\nt ∥2\n2\n≥g(ρproximal\nt )−2\nNNX\ni=1(ρ−ρproximal\nt )⊤(yi−ϕ⊤\niρ)ϕi+1\nNNX\ni=1\u0000\nϕ⊤\ni(ρ−ρproximal\nt )\u00012=g(ρ),\nwhere the last inequality applies the fact thatPN\ni=1∥ϕi∥2\n2≤Nn. Thus, we can conclude that\nψ(ρ⋆\nt+1)≥g(ρ⋆\nt+1) +λ∥ρ⋆\nt+1∥1=ℓ(ρ⋆\nt+1). (87)\nSimilarly, we can also demonstrate that\ng(ρproximal\nt ) + (ρ−ρproximal\nt )⊤∇g(ρproximal\nt ) =g(ρproximal\nt )−2\nNNX\ni=1(ρ−ρproximal\nt )⊤(yi−ϕ⊤\niρ)ϕi\n=g(ρ)−1\nNNX\ni=1\u0000\nϕ⊤\ni(ρ−ρproximal\nt )\u00012≤g(ρ),\nand as a consequence,\nψ(bρ)≤g(bρ) +n∥bρ−ρproximal\nt ∥2\n2+λ∥bρ∥1=ℓ(bρ) +n∥bρ−ρproximal\nt ∥2\n2. (88)\nSubstituting (87) and (88) into (86), we see that for any bρ,\nℓ(bρ)≥ℓ(ρ⋆\nt+1) +n\u0000\n∥bρ−ρ⋆\nt+1∥2\n2− ∥bρ−ρproximal\nt ∥2\n2\u0001\n.\nTaking bρ=ρproximal\ntyields\nℓ(ρproximal\nt )≥ℓ(ρ⋆\nt+1) +n∥ρproximal\nt −ρ⋆\nt+1∥2\n2≥ℓ(ρ⋆\nt+1),\nwhich completes the proof of (80a). In addition, taking bρ=ρ⋆gives\nℓ(ρ⋆)≥ℓ(ρ⋆\nt+1) +n\u0000\n∥ρ⋆−ρ⋆\nt+1∥2\n2− ∥ρ⋆−ρproximal\nt ∥2\n2\u0001\n,\nthus concluding the proof of (80b).\nA.4 Proof of Lemma 4\nIn this proof, we first show how to construct the desirable transformer, followed by an analysis of this\nconstruction. In particular, we would like to show that the transformer as illustrated in Figure 1, with\nparameters specified below, satisfy properties i) and ii) in Lemma 4. For ease of presentation, we denote\nH(l)=FFΘ(l)\nff\u0000\nH(l−1/2)\u0001\n,H(l−1/2)=AttnΘ(l)\nattn\u0000\nH(l−1)\u0001\n, l = 1, . . . , L. (89)\nThroughout this subsection, all components in H(l−1/2)share the same superscript l−1/2(e.g.,ϕ(l−1/2)\nj,\nλ(l−1/2)\nj), which help distinguish between different layers.\n25\n--- Page 26 ---\nFF0 Attn1 FF1 Attn2 …Input\n𝑯(𝟎)Output\nො𝑦(𝐿)\n2nd and 3rd layersAttn0 FF2 Attn1 FF1 Attn2 FF2(𝐿−1)layers\n4th and 5th layersFigure 1: Structure of the desirable transformer.\nA.4.1 High-level structure\nThe overall structure of the transformer is depicted in Figure 1.\n•The architecture begins with an attention layer Attn0and a feedforward layer FF0, which serve to\ninitialize certain variables (particularly the features identified in Lemma 1) based on the in-context\ninputs {xi}.\n•The remaining (L−1)layers are divided into (L−1)/2blocks with identical structure and parameters.\nMore concretely, each block consists of two attention layers and two feed-forward layers, (Attn1 ,FF1)\nand(Attn2 ,FF2), which are designed to perform inexact proximal gradient iterations and update the\ncorresponding prediction.\nIn the sequel, we shall first describe what update rule each layer is designed to implement, followed by\ndetailed explanation about how they can be realized using the transformer architecture.\nA.4.2 Intended updates for each layer\nOur transformer construction comprises multiple layers (as illustrated in Figure 1) designed to emulate the\niterations of the inexact proximal gradient method (31b). Let us begin by describing the desired update to\nbe performed at each layer, abstracting away the specifics of the transformer implementation.\n•FF0: This feed-forward layer intends to update the components ϕjandλas follows:\nϕ(l)\nj=ϕ(l−1/2)\nj +\u0002\nϕfeature\n1\u0000\nx(l−1/2)\nj\u0001\n,···, ϕfeature\nn\u0000\nx(l−1/2)\nj\u0001\n,1\u0003⊤,1≤j≤N+ 1; (90a)\nλ(l)=c1\u0010logN\nN\u00111/6\nC−1/3\nFbε2/3+c1r\nlogN\nN\u0010\nCF+σ\u0011\n+c1C−1\nFε2\ndis=:λ, (90b)\nwhere ϕfeature\ni (x)is defined in (19), bεis defined in Lemma 4 (see (39c)), and c1>0is some large enough\nuniversal constant.\n•Attn1: In this attention layer, we attempt to implement the following updates:\nρ(l−1/2)=ρ(l−1)+2η\nNNX\ni=1ϕ(l−1)\nin\ny(l−1)\ni−\u0000\nϕ(l−1)\ni\u0001⊤ρ(l−1)o\n+e(l−1)(91)\nfor some residual (or error) term e(l−1), corresponding to an iteration of gradient descent in (31a)\nbefore the proximal operator is applied. This residual term e(l−1)shall be bounded shortly.\n•FF1: This feed-forward layer is designed to implement the following updates:\nρ(l)=STηλ(l−1/2)\u0000\nρ(l−1/2)\u0001\n, (92a)\nby(l)= 0, (92b)\nwhich applies the proximal operator (i.e., soft-thresholding) to the output in (91). As a result, this in\nconjunction with (91) in Attn1completes one (inexact) proximal gradient iteration (31a).\n26\n--- Page 27 ---\n•Attn2: This attention layer intends to update the prediction bybased on ρ(l), namely,\nby(l+1/2)=\u0000\nϕ(l)\nN+1\u0001⊤ρ(l)+ee(l), (93)\nwhereee(l)is some residual term that will be bounded momentarily.\n•Attn0, FF2 : These layers do not update the hidden representation H(l); instead, they are included\nto ensure consistency with the transformer architecture defined in (10).\nOnahighlevel, thistransformerimplementstheinexactproximalgradientmethodin(31b). Inparticular,\nafter passing the input through Attn0andFF0, we obtain\nϕ(1)\nj=ϕj, λ(1)=λ,1≤j≤N+ 1.\nThe remaining layers then proceed as follows:\n•All parameters except ρ(l)andby(l)will stay fixed throughout the remaining layers, i.e.,\nϕ(l)\nj=ϕj, λ(l)=λ(l+1/2)=λ, y(l)\nj=y(0)\nj=yj, w(l)\nj=w(0)\nj,∀1≤l≤L,1≤j≤N+ 1.(94)\nwhere w(0)\nj= 1for1≤j≤Nandw(0)\nN+1= 0, as previously defined in (37b) and (37c).\n•The components ρ(l)are updated in a way that resembles (31b), namely,\nρ(2t+1)=ρproximal\ntfort≥1and ρ(0)=ρproximal\n0 =0, λ =λ. (95)\n•The components by(l)are computed to approximate the prediction of f(xN+1), namely,\nby(2t)= 0,by(2t+1)≈ϕ⊤\nN+1ρproximal\ntfort≥1.\nA.4.3 Parameter design in our transformer construction\nNext, we explain how the transformer architecture can be designed to implement the updates described\nabove for each layer.\n•FF0: Note that the function ϕ(z)(cf. (19)) is intimately connected with the ReLU function σff(·)as:\nϕ(z) =σff(z+ 1/2)−σff(z−1/2). (96)\nThis allows us to decompose (19) as\nϕfeature\ni (x) =σff\u0012\nτff\u0012ω⊤\nix\n∥ωi∥2−ti\u0013\n+1\n2\u0013\n−σff\u0012\nτff\u0012ω⊤\nix\n∥ωi∥2−ti\u0013\n−1\n2\u0013\n,1≤i≤n,(97)\nwhere we take\nτff= 1/√ε. (98)\nMoreover, the last entry in ϕ(l)\njcan be expressed by 1 =σff(1)−σff(−1). In order for this layer to\ncarry out (90), we take the parameter matrices W(l)∈RD×DandU(l)∈RD×D(see (8)) to satisfy\nW(l)\n1:2n+2,1:d+1=\n∥ω1∥−1\n2τffω⊤\n1−t1τff+1\n2......\n∥ωn∥−1\n2τffω⊤\nn−tnτff+1\n2\n0⊤1\n∥ω1∥−1\n2τffω⊤\n1−t1τff−1\n2......\n∥ωn∥−1\n2τffω⊤\nn−tnτff−1\n2\n0⊤−1\n,W(l)\n2n+3,:=λu⊤\nd+1,(99a)\n27\n--- Page 28 ---\nU(l)\nd+4:d+n+4,1:2n+2= [In+1,−In+1],U(l)\nd+2n+6,:=u⊤\n2n+3, (99b)\nwith all remaining entries set to zero. Here, λhas been defined in (90b), and Wi:j, r:hdenotes a\nsubmatrix of Wconsisting of rows ithrough jand columns rthrough h,Wi,:represents the i-th row\nofW, whereas ui∈RDstands for the i-th standard basis vector.\n•Attn1: First, we discuss how the sigmoid function σattn(·)defined in (9) can help us implement (91).\nNote that σ′\nattn(0) = 1 /4and observe the Taylor expansion σattn(τ−1x) =σattn(0) +x\n4τ+O\u0000x2\nτ2\u0001\n,which\nallow us to express\nx≈4τ\u0000\nσattn(τ−1x)−σattn(0)\u0001\n. (100)\nIn light of this observation, we propose to carry out (91) via the following updates that exploit the\nsigmoid function:\nρ(l−1/2)=ρ(l−1)+2\nNN+1X\ni=14τ\u0010\nσattn\u0000\nτ−1ηy(l−1)\ni\u0001\n−σattn(0)\u0011\nϕ(l−1)\ni\n−2\nNN+1X\ni=14τ\u0010\nσattn\u0010\nτ−1η\u0000\nϕ(l−1)\ni\u0001⊤ρ(l−1)\u0011\n−σattn(0)\u0011\nϕ(l−1)\ni\n+2\nNN+1X\ni=14τ\u0010\nσattn\u0000\nτ−1η(1−w(l−1)\ni)by(l−1)\u0001\n−σattn(0)\u0011\nϕ(l−1)\ni, (101)\nwhere the error vector e(l−1)can be straightforwardly determined by checking the difference between\n(91) and (101). Next, to fully realize (101) via a attention layer, we use 4 attention heads in this layer\nand take the parameter matrices V(l)\nm,Q(l)\nm, andK(l)\nm∈RD×Dform= 1,2,3,4to be:\nV(l)\n1,(d+n+5:d+2n+5, d+4:d+n+4)= 8τIn+1,Q(l)\n1,(1,:)=τ−1ηu⊤\nd+2,K(l)\n1,(1,:)=u⊤\nd+1,\nV(l)\n2=−V(l)\n1,Q(l)\n2,(1:n+1, d+4:d+n+4)=τ−1ηIn+1,K(l)\n2,(1:n+1, d+n+5:d+2n+5)=In+1,\nV(l)\n3=V(l)\n1,Q(l)\n3,(1,:)=τ−1η(ud+1−ud+3)⊤,K3,(1,:)=u⊤\nd+2n+7,\nV(l)\n4=−V(l)\n1,Q(l)\n4=0,K(l)\n4=0,\nwith all remaining entries set to zero. Here, V(l)\nm,(i:j, r:h)denotes a submatrix of V(l)\nmcomprising rows i\nthrough jand columns rthrough h,V(l)\nm,(i,:)denotes the i-th row of V(l)\nm, andui∈RDrepresents the\ni-th standard basis vector.\nBefore proceeding, let us bound the residual vector e(l−1)in (91). Observing that σ′\nattn(0) = 1 /4and\n|σ′′\nattn(x)|=|ex−e−x|/(ex/2+ e−x/2)4<0.5for all x, we can show that\n\f\f\fx−4τ\u0002\nσattn(τ−1x)−σattn(0)\u0003\f\f\f≤0.5×4ττ−2|x|2= 2τ−1|x|2,∀x∈R. (102)\nTaking this collectively with (101) and the choice that 1−wi̸= 0only for i=N+ 1, we arrive at\n∥e(l)∥∞≤4η2\nNτNX\ni=1\u0000\ny(l)\ni\u00012+4η2\nNτN+1X\ni=1\u0010\u0000\nϕ(l)\ni\u0001⊤ρ(l)\u00112\n+4η2\u0000\nby(l)\u00012\nNτ+2η\nN\f\f\fby(l)−\u0000\nϕ(l)\nN+1\u0001⊤ρ(l)\f\f\f\n≲η2\nτ\u0010\nC2\nF+σ2+∥ρ(l)∥2\n1+\u0000\nby(l)\u00012\u0011\n+2η\nN\f\f\fby(l)−\u0000\nϕ(l)\nN+1\u0001⊤ρ(l)\f\f\f, (103)\nwhere we have used the facts that\n|y(l)\ni|=|yi| ≤ |f(xi)|+|zi| ≤ |ϕ⊤\niρ⋆|+εdis+|z1|≲∥ρ⋆∥1+CF+σ≍CF+σ,\n|\u0000\nϕ(l)\ni\u0001⊤ρ(l)|=|ϕ⊤\niρ(l)| ≤ ∥ϕi∥∞∥ρ(l)∥1≤ ∥ρ(l)∥1.\n28\n--- Page 29 ---\n•FF1: Inordertoimplementthesoft-thresholdingoperatorusingthefeed-forwardlayer, wefirstneedto\ninspect the connection between the soft-thresholding operator and the ReLU function σff(·). Towards\nthis end, observe that\nSTηλ(z) =z+ηλ−(z+ηλ)1(z+ηλ > 0) + ( z−ηλ)1(z−ηλ > 0)\n=z+σff(ηλ)−σff(z+ηλ) +σff(z−ηλ). (104)\nConsequently, we propose to design a feed-forward layer capable of implementing the following updates\n(in an attempt to carry out the proposed update (92)):\nρ(l)=ρ(l−1/2)+σff(ηλ(l−1/2))−σff(ρ(l−1/2)+ηλ(l−1/2)) +σff\u0000\nρ(l−1/2)−ηλ(l−1/2)\u0001\n=STηλ(l−1/2)\u0000\nρ(l−1/2)\u0001\n,\nby(l)=by(l−1/2)−σff\u0000\nby(l−1/2)\u0001\n+σff\u0000\n−by(l−1/2)\u0001\n= 0.\nTo do so, it suffices to set W(l)∈RD×D,U(l)∈RD×Dto be\nW(l)\n1:2n+5,:=\n0(n+1)×(d+n+4) In+1 η1 0\n0(n+1)×(d+n+4) In+1 −η1 0\nηu⊤\nd+2n+6\nu⊤\nd+2n+7\n−u⊤\nd+2n+7\n, (105a)\nU(l)\nd+n+5:d+2n+5,1:2n+5= [−In+1,In+1,1n+1,0,0],U(l)\nD,1:2n+5= [0⊤\n2n+3,−1,1],(105b)\nwith all remaining entries set to zero.\n•Attn2: Recall that (100) tells us that\n\u0000\nϕ(l)\ni\u0001⊤ρ(l)≈4τn\nσattn\u0010\nτ−1\u0000\nϕ(l)\ni\u0001⊤ρ(l)\u0011\n−σattn(0)o\n. (106)\nAs a result, we would like to design this attention layer to actually implement\nby(l+1/2)=by(l)+ 4τN+1X\ni=1\u0000\n1−w(l)\ni\u0001h\nσattn\u0010\nτ−1\u0000\nϕ(l)\ni\u0001⊤ρ(l)\u0011\n−σattn(0)i\n, (107)\nand hence the residual term ee(l)in (93) can be easily determined by comparing (93) with (107).\nTo realize (107), it suffices to use 2 attention heads, and set V(l+1)\n1 ,V(l+1)\n2∈RD×D,Q(l+1)\n1,Q(l+1)\n2∈\nRD×D, andK(l+1)\n1,K(l+1)\n2∈RD×Dto be:\nV(l+1)\n1,(D,:)= 4τ(ud+1−ud+3)⊤,Q(l+1)\n1,(1:n+1, d+4:d+n+4)=τ−1In+1,K(l+1)\n1,(1:n+1, d+n+5:d+2n+5)=In+1,\nV(l+1)\n2 =−V(l+1)\n1 ,Q(l+1)\n2 =0,K(l+1)\n2 =0,\nwith all remaining entries set to zero. Here, we recall that uidenotes the i-th standard basis vector.\nBefore moving on, let us single out some bound on ee(l). Recalling that 1−w(l)\ni= 0for all 1≤i≤N\nand making use of (106) and (102), we can demonstrate that\n|ee(l)| ≤2\u0000\n(ϕ(l)\nN+1)⊤ρ(l)\u00012\nτ. (108)\n•Attn0, FF2 : The parameter matrices in these layers are taken to be\nU(l+1)=W(l+1)=Q(1)=K(1)=V(1)=0, (109)\nso as to ensure that\nH(1/2)=H(0)and H(l+1)=H(l+1/2).\n29\n--- Page 30 ---\nA.4.4 Verifying (95)and controlling the size of et\nWe now verify (95) by induction, and establish upper bounds on etin (31b).\nFirst, it is self-evident that ρ(1)=ρproximal\n0 =0. Now, let us assume that ρ(2t−1)=ρproximal\nt−1, and proceed\nto prove ρ(2t+1)=ρproximal\nt. In the t-th block (which contains Attn1 ,FF1,Attn2 ,FF2), we have\nρ(2t+1)=ρ(2t)=STηλ\u0000\nρ(2t−1/2)\u0001\n=STηλ\u0012\nρ(2t−1)+2η\nNNX\ni=1ϕi\b\nyi−ϕ⊤\niρ(2t−1)\t\n+e(2t−1)\u0013\n=STηλ\u0012\nρproximal\nt−1+2η\nNNX\ni=1ϕi\b\nyi−ϕ⊤\niρproximal\nt−1\t\u0013\n+et=ρproximal\nt ,\nwhere\net:=STηλ\u0012\nρproximal\nt−1+2η\nNNX\ni=1ϕi\b\nyi−ϕ⊤\niρproximal\nt−1\t\n+e(2t−1)\u0013\n−STηλ\u0012\nρproximal\nt−1+2η\nNNX\ni=1ϕi\b\nyi−ϕ⊤\niρproximal\nt−1\t\u0013\n.\nRecalling that ST(·)is a contraction operator and using (103), we can show that\n∥et∥∞≤ ∥e(2t−1)∥∞≲η2\nτ\u0010\nC2\nF+σ2+∥ρ(2t−1)∥2\n1+\u0000\nby(2t−1)\u00012\u0011\n+η\nN\f\f\fby(2t−1)−ϕ⊤\nN+1ρ(2t−1)\f\f\f\n≲η2\nτ\u0010\nC2\nF+σ2+∥ρproximal\nt−1∥2\n1+\u0000\nby(2t−1)\u00012\u0011\n+η\nN\f\f\fby(2t−1)−ϕ⊤\nN+1ρproximal\nt−1\f\f\f, (110)\nwhich is valid since ρ(2t−1)=ρproximal\nt−1. Also, it follows from (93) that\nby(2t−1)=by(2t−3/2)=ϕ⊤\nN+1ρ(2t−2)+ee(2t−2)=ϕ⊤\nN+1ρproximal\nt−1+ee(2t−2),\nwhere we have used ρ(2t−2)=ρ(2t−1)=ρproximal\nt−1. Moreover, it is seen from (108) that\n|ee(2t−2)| ≤2\u0000\nϕ⊤\nN+1ρ(2t−2)\u00012\nτ=2\u0010\nϕ⊤\nN+1ρproximal\nt−1\u00112\nτ.\nSubstituting these into (110) and using ∥ϕN+1∥∞≤1then yield\n∥et∥∞≲η2\nτ\u0010\nC2\nF+σ2+∥ρproximal\nt−1∥2\n1+ (ϕ⊤\nN+1ρproximal\nt−1)2+ (ee(2t−2))2\u0011\n+η\nN|ee(2t−2)|\n≲η2\nτ \nC2\nF+σ2+∥ρproximal\nt ∥2\n1+ (ϕ⊤\nN+1ρproximal\nt−1)2+(ϕ⊤\nN+1ρproximal\nt−1)4\nτ2!\n+η(ϕ⊤\nN+1ρproximal\nt−1)2\nNτ\n≲η2\nτ \nC2\nF+σ2+∥ρproximal\nt−1∥2\n1+∥ρproximal\nt−1∥4\n1\nτ2!\n+η∥ρproximal\nt−1∥2\n1\nNτ. (111)\nA.4.5 Proof of property iii) in Lemma 4\nEquipped with the above transformer parameters, we are now positioned to establish property iii) in Lemma\n4. To this end, we first establish the following lemma, whose proof is postponed to Appendix A.4.6.\nLemma 7. Suppose that λ≥Cp\nlogN/N (CF+σ) +C−1\nFε2\ndis, and take τto be sufficiently large such that\nτ≥CNn2(L+n) (N+n)CF,\nwithC >0some large enough constant. Then for all t≥0, it holds that\n∥et∥1≲CF\n(L+n)nNand\r\rρproximal\nt\r\r\n1≲n√\nNCF.\n30\n--- Page 31 ---\nWhen L= 2T+ 1is an odd number, it holds that ρ(L)=ρproximal\nTandby(L)=by(L−1/2). Combining\nLemma 7 with (108) then reveals that: by taking\nτ≥CNn2(L+n) (N+n)CF≥Cn2N5/4CF\nfor some large enough constant C >0, we have\n\f\fϕ⊤\nN+1ρ(L)−by(L)\f\f=\f\fϕ⊤\nN+1ρ(L−1)−by(L−1/2)\f\f=|ee(L−1)| ≤2(ϕ⊤\nN+1ρproximal\nL−1\n2)2\nτ\n≤2∥ϕN+1∥2\n∞\r\rρproximal\nL−1\n2\r\r2\n1\nτ≤\u0012logN\nN\u00131/4\nCF.\nAdditionally, taking Lemma 7 together with (34) leads to\nℓ(ρproximal\nT )−ℓ(ρ⋆)≲nC2\nF\nL+ (L+n)CF\n(L+n)nN(CF+σ+n√\nNCF+λ)\n≲nC2\nF\nL+1√\nNCF(CF+σ) +λCF. (112)\nRecalling the bound on λCFin (43) and the definition of bεin (39c), we have\nλCF≲r\nlogN\nNCF\u0010\nCF+σ\u0011\n+ε2\ndis+bε≲r\nlogN\nNCF\u0010\nCF+σ\u0011\n+ε2\ndis+nC2\nF\nL,\nwhich combined with (112) completes the proof.\nA.4.6 Proof of Lemma 7\nWe intend to prove the following inequalities by induction:\n∥et∥1≤c1CF\n(L+n)Nn, (113)\n∥ρproximal\nt ∥1≤c2n√\nNCF, (114)\nfor some sufficiently small (resp. large) constant c1>0(resp. c2>0).\nFor the base case, we have\n∥e0∥1=∥ρproximal\n0 −ρ⋆\n0∥1= 0,\nand thus (113) holds for t= 0. Next, we intend to prove that (114) holds for t=kunder the assumption\nthat (113) holds for t≤kand (114) holds for t≤k−1. According to (35), it holds that\n∥ρproximal\nk∥1≤c5CF+c5nC2\nF\nkλ+c5√\nN(k+n) max\n1≤i≤k∥ei∥1+c5(k+n)\nλmax\n1≤i≤k\b\n∥ei∥1∥ρproximal\ni ∥1\t\n(a)\n≤c5CF+c5n√\nNCF\ncλ+c5c1CF+c5c1c2CF\ncλ+c5c1\ncλ√\nNn∥ρproximal\nk∥1,\nwhere (a) results from the fact that λ≥cλCF/√\nN. For some sufficiently small constant c1>0obeying\nc1c5/cλ≤1/4, we have\n∥ρproximal\nk∥1≤4c5CF\n3 \n1 +n√\nN\ncλ+c1!\n+c2CF\n3≤c2n√\nNCF,\nprovided that c2≥2c5\u0000\n1 +c−1\nλ+c1\u0001\n.\n31\n--- Page 32 ---\nNext, we intend to establish that (113) holds for t=k+ 1. By virtue of (111), one has\n∥ek+1∥1≤c3nη2\nτ \nC2\nF+σ2+∥ρproximal\nk∥2\n1+∥ρproximal\nk∥4\n1\nτ2!\n+c3nη∥ρproximal\nk∥2\n1\nNτ\n≤c3\n4nτ\u0000\n1 + 2 c2\n2n2N+ 2c2\n2n3\u0001\nC2\nF+c3σ2\n4nτ,\nforτ≥c2n√\nNCF. Without loss of generality, we assume thatp\nlogN/Nσ ≤√c4CF, since otherwise the\nupper bound in Theorem 1 is larger than C2\nFand holds trivially by outputting bρ=0. It is then seen that\n∥ek+1∥1≤c3\n4nτ\u0000\n1 + 2 c2\n2n2N+ 2c2\n2n3\u0001\nC2\nF+c3c4NC2\nF\n4nτ\n=C2\nF\nτ\u0010c3\n4n(1 + 2 c2\n2n2N+ 2c2\n2n3+c4N)\u0011\n≤c1CF\n(L+n)Nn,\nwith the proviso that\nτ≥Nn(L+n)CF\nc1\u0010c3\n4n(1 + 2 c2\n2n2N+ 2n3c2\n2+c4N)\u0011\n+c2n√\nNCF.\nThis concludes the proof.\nReferences\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S. (2023). Transformers learn to implement preconditioned\ngradient descent for in-context learning. Advances in Neural Information Processing Systems , 36:45614–\n45650.\nAhuja, K., Panwar, M., and Goyal, N. (2023). In-context learning through the bayesian prism. arXiv preprint\narXiv:2306.04891 .\nAkyürek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2023). What learning algorithm is\nin-context learning? investigations with linear models. In International Conference on Learning Repre-\nsentations .\nBach, F. (2017). Breaking the curse of dimensionality with convex neural networks. Journal of Machine\nLearning Research , 18(19):1–53.\nBai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. (2023). Transformers as statisticians: Provable in-\ncontext learning with in-context algorithm selection. Advances in neural information processing systems ,\n36:57125–57211.\nBarron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal function. IEEE\nTransactions on Information Theory , 39(3):930–945.\nBeck, A. (2017). First-order methods in optimization . SIAM.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J.,\nBosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258 .\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., etal.(2020). Languagemodelsarefew-shotlearners. Advances in neural information\nprocessing systems , 33:1877–1901.\n32\n--- Page 33 ---\nChen, L., Peng, B., and Wu, H. (2024a). Theoretical limitations of multi-layer transformer. arXiv preprint\narXiv:2412.02975 .\nChen, S. and Li, Y. (2024). Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084 .\nChen, S., Sheen, H., Wang, T., and Yang, Z. (2024b). Training dynamics of multi-head softmax attention\nfor in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442 .\nCheng, X., Chen, Y., and Sra, S. (2024). Transformers implement functional gradient descent to learn\nnon-linear functions in context. In International Conference on Machine Learning , pages 8002–8037.\nCole, F., Lu, Y., O’Neill, R., and Zhang, T. (2024). Provable in-context learning of linear systems and linear\nelliptic pdes with transformers. arXiv preprint arXiv:2409.12293 .\nCole, F., Lu, Y., Zhang, T., and Zhao, Y. (2025). In-context learning of linear dynamical systems with\ntransformers: Error bounds and depth-separation. arXiv preprint arXiv:2502.08136 .\nDai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. (2022). Why can gpt learn in-context?\nlanguage models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559 .\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Liu, T., et al. (2022). A survey\non in-context learning. arXiv preprint arXiv:2301.00234 .\nElhage,N.,Nanda,N.,Olsson,C.,Henighan,T.,Joseph,N.,Mann,B.,Askell,A.,Bai,Y.,Chen,A.,Conerly,\nT., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J.,\nLovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., andOlah, C.(2021).\nA mathematical framework for transformer circuits. Transformer Circuits Thread . https://transformer-\ncircuits.pub/2021/framework/index.html.\nFeng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. (2023). Towards revealing the mystery behind\nchainofthought: atheoreticalperspective. Advances in Neural Information Processing Systems , 36:70757–\n70798.\nFu, D., Chen, T.-q., Jia, R., and Sharan, V. (2024). Transformers learn to achieve second-order convergence\nratesforin-contextlinearregression. Advances in Neural Information Processing Systems , 37:98675–98716.\nFuruya, T., de Hoop, M. V., and Peyré, G. (2024). Transformers are universal in-context learners. arXiv\npreprint arXiv:2408.01367 .\nGarg, S., Tsipras, D., Liang, P. S., and Valiant, G. (2022). What can transformers learn in-context? a case\nstudy of simple function classes. Advances in Neural Information Processing Systems , 35:30583–30598.\nGiannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. (2023). Looped transform-\ners as programmable computers. In International Conference on Machine Learning , pages 11398–11442.\nPMLR.\nGiannou, A., Yang, L., Wang, T., Papailiopoulos, D., and Lee, J. D. (2024). How well can transformers\nemulate in-context newton’s method? arXiv preprint arXiv:2403.03183 .\nGillioz, A., Casas, J., Mugellini, E., and Abou Khaled, O. (2020). Overview of the transformer-based models\nfor nlp tasks. In 2020 15th Conference on computer science and information systems (FedCSIS) , pages\n179–183. IEEE.\nGuo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. (2024). How do transformers\nlearn in-context beyond simple functions? a case study on learning with representations. In International\nConference on Learning Representations .\nHahn, M. (2020). Theoretical limitations of self-attention in neural sequence models. Transactions of the\nAssociation for Computational Linguistics , 8:156–171.\n33\n--- Page 34 ---\nHahn, M. and Goyal, N. (2023). A theory of emergent in-context learning as implicit structure induction.\narXiv preprint arXiv:2303.07971 .\nHataya, R., Matsui, K., and Imaizumi, M. (2024). Automatic domain adaptation by transformers in in-\ncontext learning. arXiv preprint arXiv:2405.16819 .\nHornik,K.,Stinchcombe,M.,White,H.,andAuer,P.(1994). Degreeofapproximationresultsforfeedforward\nnetworks approximating unknown mappings and their derivatives. Neural computation , 6(6):1262–1275.\nHuang, Y., Cheng, Y., and Liang, Y. (2024). In-context convergence of transformers. In International\nConference on Machine Learning , pages 19660–19722.\nHuang, Y., Wen, Z., Singh, A., Chi, Y., and Chen, Y. (2025). Transformers provably learn chain-of-thought\nreasoning with length generalization.\nJelassi, S., Brandfonbrener, D., Kakade, S. M., and Malach, E. (2024). Repeat after me: Transformers are\nbetter than state space models at copying. arXiv preprint arXiv:2402.01032 .\nKhan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. (2022). Transformers in vision:\nA survey. ACM computing surveys (CSUR) , 54(10s):1–41.\nKim, J. and Suzuki, T. (2024). Transformers learn nonlinear features in context: Nonconvex mean-field\ndynamics on the attention landscape. In Forty-first International Conference on Machine Learning .\nKurková, V.andSanguineti, M.(2002). Boundsonratesofvariable-basisandneural-networkapproximation.\nIEEE Transactions on Information Theory , 47(6):2659–2665.\nKwon, S. M., Xu, A. S., Yaras, C., Balzano, L., and Qu, Q. (2025). Out-of-distribution generalization of\nin-context learning: A low-dimensional subspace perspective. arXiv preprint arXiv:2505.14808 .\nLi, H., Wang, M., Lu, S., Cui, X., and Chen, P.-Y. (2024a). Training nonlinear transformers for efficient\nin-context learning: A theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607 .\nLi, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. (2023). Transformers as algorithms: Generalization\nand stability in in-context learning. In International conference on machine learning , pages 19565–19594.\nPMLR.\nLi, Z., Liu, H., Zhou, D., and Ma, T. (2024b). Chain of thought empowers transformers to solve inherently\nserial problems. arXiv preprint arXiv:2402.12875 , 1.\nLikhosherstov, V., Choromanski, K., and Weller, A. (2021). On the expressive power of self-attention\nmatrices. arXiv preprint arXiv:2106.03764 .\nLin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of transformers. AI open, 3:111–132.\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. (2022). Transformers learn shortcuts to\nautomata. arXiv preprint arXiv:2210.10749 .\nMahankali, A. V., Hashimoto, T., and Ma, T. (2024). One step of gradient descent is provably the opti-\nmal in-context learner with one layer of linear self-attention. In International Conference on Learning\nRepresentations .\nMerrill, W. and Sabharwal, A. (2024). The expressive power of transformers with chain of thought. In\nInternational Conference on Learning Representations .\nNichani, E., Damian, A., and Lee, J. D. (2024). How transformers learn causal structure with gradient\ndescent. In International Conference on Machine Learning , pages 38018–38070.\nPeng, B., Narayanan, S., and Papadimitriou, C. (2024). On limitations of the transformer architecture. In\nFirst Conference on Language Modeling .\n34\n--- Page 35 ---\nPérez, J., Marinković, J., and Barceló, P. (2019). On the turing completeness of modern neural network\narchitectures. arXiv preprint arXiv:1901.03429 .\nSanford, C., Hsu, D., and Telgarsky, M. (2024). Transformers, parallel computation, and logarithmic depth.\narXiv preprint arXiv:2402.09268 .\nSanford, C., Hsu, D.J., andTelgarsky, M.(2023). Representationalstrengthsandlimitationsoftransformers.\nAdvances in Neural Information Processing Systems , 36:36677–36707.\nShamshad, F., Khan, S., Zamir, S.W., Khan, M.H., Hayat, M., Khan, F.S., andFu, H.(2023). Transformers\nin medical imaging: A survey. Medical image analysis , 88:102802.\nShen, L., Mishra, A., and Khashabi, D. (2023). Do pretrained transformers learn in-context by gradient\ndescent? arXiv preprint arXiv:2310.08540 .\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.\n(2017). Attention is all you need. Advances in neural information processing systems , 30.\nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science , vol-\nume 47. Cambridge university press.\nVladymyrov, M., VonOswald, J., Sandler, M., andGe, R.(2024). Lineartransformersareversatilein-context\nlearners. Advances in Neural Information Processing Systems , 37:48784–48809.\nVon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vla-\ndymyrov, M. (2023). Transformers learn in-context by gradient descent. In International Conference on\nMachine Learning , pages 35151–35174.\nvon Oswald, J., Schlegel, M., Meulemans, A., Kobayashi, S., Niklasson, E., Zucchet, N., Scherrer, N., Miller,\nN., Sandler, M., Vladymyrov, M., et al. (2023). Uncovering mesa-optimization algorithms in transformers.\narXiv preprint arXiv:2309.05858 .\nWang, Z., Jiang, B., and Li, S. (2024). In-context learning on function classes unveiled for transformers. In\nForty-first International Conference on Machine Learning .\nWen, K., Dang, X., and Lyu, K. (2024). Rnns are not transformers (yet): The key bottleneck on in-context\nretrieval. arXiv preprint arXiv:2402.18510 .\nWen, K., Li, Y., Liu, B., and Risteski, A. (2023). Transformers are uninterpretable with myopic methods: a\ncase study with bounded dyck grammars. Advances in Neural Information Processing Systems , 36:38723–\n38766.\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2022). An explanation of in-context learning as implicit\nbayesian inference. In International Conference on Learning Representations .\nYang, T., Huang, Y., Liang, Y., and Chi, Y. (2024). In-context learning with representations: Contextual\ngeneralization of trained transformers. arXiv preprint arXiv:2408.10147 .\nYao, S., Peng, B., Papadimitriou, C., and Narasimhan, K. (2021). Self-attention networks can process\nbounded hierarchical languages. arXiv preprint arXiv:2105.11115 .\nZhang, R., Frei, S., and Bartlett, P. L. (2024). Trained transformers learn linear models in-context. Journal\nof Machine Learning Research , 25(49):1–55.\nZhang, Y., Zhang, F., Yang, Z., andWang, Z.(2023). Whatandhowdoesin-contextlearninglearn? bayesian\nmodel averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420 .\n35",
  "text_length": 86404
}