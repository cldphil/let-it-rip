{
  "id": "http://arxiv.org/abs/2506.04211v1",
  "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector",
  "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher.",
  "authors": [
    "Boyong He",
    "Yuxiang Ji",
    "Zhuoyue Tan",
    "Liaoni Wu"
  ],
  "published": "2025-06-04T17:56:46Z",
  "updated": "2025-06-04T17:56:46Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04211v1",
  "full_text": "--- Page 1 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive\nObject Detector\nBoyong Heâˆ—\nXiamen University\nInstitute of Artifcial\nIntelligence\nXiamen, China\nboyonghe@stu.xmu.edu.cnYuxiang Jiâˆ—\nXiamen University\nInstitute of Artifcial\nIntelligence\nXiamen, China\nyuxiangji@stu.xmu.edu.cnZhuoyue Tan\nXiamen University\nInstitute of Artifcial\nIntelligence\nXiamen, China\ntanzhuoyue@stu.xmu.edu.cnLiaoni Wuâ€ \nXiamen University\nInstitute of Artifcial\nIntelligence\nSchool of Aerospace\nEngineering\nXiamen, China\nwuliaoni@xmu.edu.cn\nAbstract\nObject detectors often suffer a decrease in performance due to the\nlarge domain gap between the training data (source domain) and\nreal-world data (target domain). Diffusion-based generative models\nhave shown remarkable abilities in generating high-quality and\ndiverse images, suggesting their potential for extracting valuable\nfeature from various domains. To effectively leverage the cross-\ndomain feature representation of diffusion models, in this paper,\nwe train a detector with frozen-weight diffusion model on the\nsource domain, then employ it as a teacher model to generate\npseudo labels on the unlabeled target domain, which are used to\nguide the supervised learning of the student model on the target\ndomain. We refer to this approach as Diffusion Domain Teacher\n(DDT ). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection perfor-\nmance without compromising the inference speed. Our method\nachieves an average mAP improvement of 21.2% compared to the\nbaseline on 6 datasets from three common cross-domain detection\nbenchmarks ( Cross-Camera, Syn2Real, Real2Artistic) , surpassing the\ncurrent state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our\nmethod consistently brings improvements even in more powerful\nand complex models, highlighting broadly applicable and effective\ndomain adaptation capability of our DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher .\nKeywords\nUnsupervised domain adaptation; Object detection; Diffusion model\nâˆ—Contribute equally to the work.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nÂ©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0686-8/24/10\nhttps://doi.org/10.1145/3664647.3680962ACM Reference Format:\nBoyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu. 2024. Diffusion Do-\nmain Teacher: Diffusion Guided Domain Adaptive Object Detector. In Pro-\nceedings of the 32nd ACM International Conference on Multimedia (MM â€™24),\nOctober 28-November 1, 2024, Melbourne, VIC, Australia. ACM, New York,\nNY, USA, 22 pages. https://doi.org/10.1145/3664647.3680962\n1 Introduction\nObject detection is a fundamental task in computer vision, with its\napplications permeating an array of real-world scenarios. There\nhave been impressive strides and significant achievements in ob-\nject detection, leveraging both Convolutional Neural Networks\n(CNNs) [ 45,57,58,72] and transformer-based models [ 4,92]. Nonethe-\nless, these data-driven detection algorithms wrestle with the chal-\nlenging issue of domain shift: the large gap between the training\ndata (source domain) and the testing environments (target domain)\nfrequently results in a substantial decline in detection accuracy.\nThis obstacle is ubiquitous across various sectors, including robot-\nics, autonomous driving, and healthcare, and it poses a formidable\nbarrier to the widespread applications of object detection in practice.\nConsequently, the deployment of domain adaptation techniques,\naimed at minimizing domain disparities, has become essential to\nboost the robustness and generalizability of models across diverse\nenvironments.\nUnsupervised Domain Adaptation (UDA) methodologies have\nsurged to the forefront of research, taking advantage of the sparse\nlabeled data from the source domain in conjunction with copious\nunlabeled data from the target domain to significantly enhance\ncross-domain detection performance. Current UDA tactics have\nexplored a variety of strategies including domain classifiers [ 11,27,\n64,88,91], graph matching [ 39,41,42], domain randomization [ 34],\nimage-to-image translation [ 28], and self-training frameworks [ 6,\n15,44,61]. These techniques have been crucial in achieving notable\nadvancements in cross-domain object detection.\nMoreover, diffusion-based generative models [ 25,59,67] have\nshowcased remarkable capabilities in generating high-quality and\ndiverse images, signaling their vast potential for a spectrum of\ndownstream applications. Some works [ 1,73,78] have already har-\nnessed diffusion models for a breadth of tasks. This evidence sug-\ngests a promising avenue for employing these models to bolster\ncross-domain detection efficacy. Nevertheless, the step-by-step in-\nference process of these models is not fast enough to meet the\nimmediate processing needs of object detection. Although therearXiv:2506.04211v1  [cs.CV]  4 Jun 2025\n--- Page 2 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\n1k 2k 3k 4k 5k 6k 7k 8k 9k10k 11k 12k 13k 14k 15k 16k 17k 18k 19k 20k\nSteps15202530354045505560Result (mAP)\nTraining on source Self-training with target\n16.616.626.526.5\n22.422.427.927.927.527.5\n22.522.532.932.9\n28.128.130.430.431.931.9\n25.125.127.827.844.954.7\n50.254.6\n46.654.2\n46.854.7\n49.254.7\n47.854.2\n49.254.8\n50.255.6\n47.4Mean Teacher\nStudent\nDiffusion Teacher\nFigure 1: Evaluation results on Clipart [ 30] during training.\nIt is evident that the performance of the student significantly im-\nproves after entering self-training, even surpassing the diffusion\nteacher , and the mean teacher exhibits better performance com-\npared to the student.\nhas been some effort to adapt diffusion models for image generation\nand manipulation, as seen with tools like LoRA [ 29] and Control-\nNet [ 83], there is a lack of research on applying diffusion models to\ncross-domain detection.\nFortunately, current UDA methods provide us with valuable in-\nsights. Specifically, we draw inspiration from previous state-of-the-\nart (SOTA) approaches [ 3,15,44] and adopt the Mean Teacher [ 71]\nself-training framework, where the teacher model generates pseudo\nlabels for the supervised learning of student model on the target\ndomain. The weights of the teacher model are typically updated\nthrough Exponential Moving Average (EMA) by the student model.\nThis consistency-based self-training approach allows the student\nmodel to progressively learn from the target domain, thereby im-\nproving the performance of the detector in cross-domain detection.\nIn our approach, we freeze all parameters of the diffusion model\nand extract intermediate feature from the upsampling structure of\nthe U-Net [ 60] architecture during the inversion process. These\nfeatures are then passed through a bottle-neck structure to generate\nhierarchical features similar to a general backbone for downstream\ndetection tasks. This enables effective training and fine-tuning of\nthe diffusion model with a small number of parameters, and yields\ndiscriminative feature for classification and regression tasks, lead-\ning to improved performance in cross-domain detection. The mean\nteacher, updated through EMA from the student model, further\nenhances stability and generalization.\nThrough the detector with the diffusion backbone for feature\nextraction struggles to match or surpass the performance of general\nbackbones like ResNet [ 23] on intra-domain. However, in the target\ndomain, the performance of the diffusion detector surpasses them\ngreatly. This strongly confirms the diffusion model is an incredibly\npowerful and highly generalized feature extractor. Furthermore, it\nis even more remarkable that the diffusion teacher model continues\nto enhance the cross-domain performance of stronger backbones,\nall without any increase in additional inference speed.\nThe contributions of this paper can be summarized as follows:â€¢We introduce a frozen-weight diffusion model as backbone,\nwhich efficiently extracts highly generalized and discrimina-\ntive feature for cross-domain object detection. Notably, the\ndiffusion-based detector, trained exclusively on the source\ndomain, demonstrates exceptional performance when ap-\nplied to the target domain.\nâ€¢We incorporate the diffusion detector as a teacher model\nwithin the self-training framework, providing valuable guid-\nance supervised learning of the student model on the target\ndomain. This integration effectively enhances cross-domain\ndetection performance without any increasing of inference\ntime.\nâ€¢We achieve substantial improvements in cross-domain de-\ntection. Our method achieves an average mAP improve-\nment of 21.2% compared with the baseline, and surpassing\nthe current SOTA methods by 5.7% mAP. Further experi-\nments demonstrate that the diffusion domain teacher con-\nsistently enhances cross-domain performance for detectors\nwith stronger backbones, leading to superior results in the\ntarget domain.\n2 Related Work\n2.1 Object Detection\nObject detection aims to locate and classify objects in given images.\nDeep convolutional neural networks [ 23,66] have revolutionized\nthis teak and is widely applied in real-world applications. Faster\nR-CNN [ 58], a prominent two-stage detection method, employs a\nregion proposal network (RPN) to generate candidate regions, fol-\nlowed by region of interest (ROI) refinement to determine the final\nbounding boxes and classes. Some research is focused on improving\nthe precision and efficiency of two-stage methods [ 2,55]. In addi-\ntion, researchers are investigating single-stage detectors [ 45,57,74]\naimed at simplifying the detection process by integrating box re-\ngression and classification. Moreover, anchor-free detectors [ 72,84]\nthat eliminate the reliance on predefined anchors have garnered\nsignificant attention. Recent research trends involve the adoption\nof transformer-based end-to-end detectors [ 4,82,92], which recon-\nceptualize the detection task as a set prediction problem, thereby\nobviating the necessity for traditional handcrafted components\nsuch as anchor generation and non-maximum suppression (NMS).\n2.2 Domain adaptation Detection\nAlthough object detection has made significant advancements, per-\nformance can suffer greatly due to domain shifts between training\nand test data. To address this issue, UDA aims to mitigate the impact\nof domain shifts by leveraging labeled source data and unlabeled\ntarget data. Initially, studies inspired by GANs [ 20] introduced do-\nmain adversarial training [ 18], which minimized domain gaps by\nextracting invariant feature. This approach is later adapted to de-\ntection tasks [ 11,27,64,88,91], resulting in notable improvements\nin performance on the target domain. Some methods [ 28] focus\non reducing inter-domain differences at image level, by applying\nimage-to-image translate like CycleGAN [90].\nRecently, self-training domain adaptation methods [ 3,14,15,44]\nhave achieved better results in cross-domain object detection by\noptimizing the learning of pseudo labels in the target domain. For\n--- Page 3 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nDetectorPseudo labelDiffusion TeacherStudent Model\nMean TeacherFinal PredictionsRPNROI\nDetectorRPNROIDetectorRPNROI\nDiffusion Backbone\nBackbone\nBackboneEMA updateEMA updateStrong Aug.\nWeak Aug.Strong Aug.\nunsup flowsup flowBack  propagation\nTarget image\nSource image\nImage EncoderInput Image\nT time stepst = 0t = T\n......Frozen weightsDiffusion BackboneU-NetIntermediate features\nBottle-Neck\nTrainable structure\nOutputFeatures\nFigure 2: Overview of our proposed Diffusion Domain Teacher (DDT). Left: We employ a frozen-weight diffusion model with\nbottleneck as the diffusion backbone , which acquires and aggregates intermediate feature from the U-Net [ 60] during the inversion process\natğ‘‡time steps for detection. Right: We use the diffusion teacher , which is a detector with the diffusion backbone, and applied it in\nself-training to generate pseudo labels for unlabeled target, guiding the learning of the student. By EMA updated from the student model,\nmean teacher is refined and serves as the final model, resulting in improved cross-domain detection results.\nexample, AT [ 44] combines domain adversarial training and self-\ntraining to improve the quality of pseudo labels. UMT [ 14] utilizes\nconsistency learning for a teacher model to generate high-quality\npseudo labels to improve cross-domain detection. CMT [ 3] intro-\nduces contrastive learning to optimize the utilization and alignment\nof features from the source and target domain. HT [ 15] optimizes\nthe generation of pseudo labels by applying consistency measures\nin regression and classification. Overall, self-training methods in\ncross-domain object detection enhance the detection performance\nin the target domain by improving the quality of pseudo labels,\nwith the teacher model being updated from the student model.\n2.3 Diffusion Models\nDiffusion models [ 25,56,59,63,67] have achieved impressive results\nin image generation, surpassing previous models like GAN [ 20].\nWith their strong generative and generalization capabilities, some\nresearch has begun to explore the potential of diffusion models in\nfeature representation and their application to downstream tasks.\nFor instance, DDPMSeg [ 1] and ODISE [ 78] utilize feature extracted\nfrom diffusion models for semantic and panoptic segmentation\ntasks, respectively. DIFT [ 69] and HyperFeature [ 53] use the diffu-\nsion model to discover correspondences in images. This inspires\nus to consider the application of diffusion models for improving\ncross-domain detection tasks.\n3 Approach\nIn this section, we present our Diffusion Domain Teacher (DDT)\nframework in detail. First, in Sec. 3.1, we review the formulation\nof Unsupervised Domain Adaptation Detection (UDAD). Then, in\nSec. 3.2, we provide a detailed description of how the frozen-weight\ndiffusion model serves as a feature extractor, producing hierarchical\nfeatures, to adapt to the detection task. Furthermore, in Sec. 3.3,we explain the application of the diffusion teacher detector in thr\nself-training framework, where pseudo labels generated on the un-\nlabeled target domain guide the supervised learning of the student\nmodel. Finally, we summarize the total training objective.\n3.1 Formulation of Unsupervised Domain\nadaptation Detection\nTo be specific, we denote a given set of ğ‘ğ‘ samplesS=\b\nğ‘‹ğ‘–ğ‘ , ğ‘Œğ‘–ğ‘ \tğ‘ğ‘ \nğ‘–=1\nas source domian, where ğ‘‹ğ‘–ğ‘ represents an image and ğ‘Œğ‘–ğ‘ represents\nthe bounding box with category labels in the respective image. Sim-\nilarly, we denote the target domain data as T=\b\nğ‘‹ğ‘–\nğ‘¡\tğ‘ğ‘ \nğ‘–=1, which\nconsists of ğ‘ğ‘¡unlabeled samples. Exactly, the distribution of S\nandT, including the distributions of images from ğ‘ƒ(ğ‘‹ğ‘ )andğ‘ƒ(ğ‘‹ğ‘¡)\n(e.g., style, scene, weather), labels ğ‘ƒ(ğ‘Œğ‘ )andğ‘ƒ(ğ‘Œğ‘¡)(e.g., the shapes,\nsizes, and density of instance), and even the scales of ğ‘ğ‘ andğ‘ğ‘¡\nare different, denoted as ğ‘ƒ(S)â‰ ğ‘ƒ(T), is what we refer to as a\ncross-domain detection problem. Furthermore, relying solely on\nsupervised learning from the labeled source domain results in an\ninherent bias towards source domain in cross-domain detection. Do-\nmain adaptation for detection aims to improving the performance\non the target domain by reducing the dissimilarity between Sand\nT, seeking to a domain-invariant detector.\n3.2 Fozen-Diffusion Feature Extractor\nDiffusion generative models [ 25,59,67] aim to minimize the discrep-\nancy between the distribution of images generated by the model,\ndenoted as ğ‘ƒğœƒ(ğ‘¥), and the distribution of the training data, denoted\nasğ‘ƒdata(ğ‘¥). During training, gaussian noise of varying magnitudes\nis added to the clean training data, commonly referred to as dif-\nfusion process. The diffusion process starts with a clean image ğ‘¥0\nfrom the training data and generates a noisy image ğ‘¥ğ‘¡by mixing\n--- Page 4 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nğ‘¥0with noise of different magnitudes:\nğ‘¥ğ‘¡=âˆšÂ¯ğ›¼ğ‘¡ğ‘¥0+âˆš1âˆ’Â¯ğ›¼ğ‘¡ğœ– (1)\nwhere ğœ–âˆ¼N( 0,I)represents randomly sampled noise, and ğ‘¡âˆˆ\n[0,ğ‘‡]denotes the time step, where larger values correspond to\nadding more noise. The amount of noise added is determined by\nğ›¼ğ‘¡, which is a predefined noise schedule, and Â¯ğ›¼ğ‘¡=ğ›¼1ğ›¼2. . . ğ›¼ ğ‘¡. The\nmodel ğ‘“ğœƒis trained to predict the input noise ğœ–, given ğ‘¥ğ‘¡andğ‘¡,\ntypically using structures like U-Net [60].\nThe iterative process of the diffusion model poses challenges\nwhen directly applied to downstream supervised tasks. We extract\nintermediate feature at a specific time step ğ‘¡during the inversion\nprocess, and apply these features for regression and classification\ntasks in the detection task. Specifically, we append a input noise\ncorresponding the time step ğ‘¡to the input image, shift it to ğ‘¥ğ‘¡and\nthen input it along with ğ‘¡into ğ‘“ğœƒto extract activation layers as\nintermediate feature. More specifically, we apply the intermediate\nfeature from the four stages of the upsampling process in the de-\nnoise network U-Net [ 60]. For each input image, we concatenate\nmultiple time step feature together and employ a bottle-neck struc-\nture to project the feature into hierarchical layers with a channel\nsize of[256,512,1024 ,2048], similar to the output of ResNet [ 23],\nwhich is directly applied to the object detection task, as shown in\nthe left side of Fig. 2.\n3.3 Diffusion Teacher Guided Self-training\nFramework\nWe employ a detector that extracts feature using the diffusion model\nand trained on the source domain as the teacher model, denoted as\nFdiff. It is used to generate pseudo labels Â¯ğ‘Œğ‘¡on the target domain\nT, where Â¯ğ‘Œğ‘¡=Fğ‘‘ğ‘–ğ‘“ ğ‘“(ğ‘‹ğ‘¡). These pseudo labels are constructed to\nform a new dataset Â¯T=\b\nğ‘‹ğ‘–\nğ‘¡,Â¯ğ‘Œğ‘–\nğ‘¡\tğ‘ğ‘¡\nğ‘–=1. Subsequently, we optimize\nthe student model using the pseudo labels. We introduce a hyper-\nparameter ğœas a threshold for the confidence scores of the output\nfor the teacher model, enabling us to select more reliable pseudo\nlabels.\nWe define the supervised learning of the student model Fstuon\nthe source domain as follows:\nLsup(ğ‘‹ğ‘ , ğ‘Œğ‘ )=LRPN\ncls(ğ‘‹ğ‘ , ğ‘Œğ‘ )+LRPN\nreg(ğ‘‹ğ‘ , ğ‘Œğ‘ )\n+LROI\ncls(ğ‘‹ğ‘ , ğ‘Œğ‘ )+LROI\nreg(ğ‘‹ğ‘ , ğ‘Œğ‘ )(2)\nwhere RPN is used to generate potential candidate regions, and\nROI performs classification and bounding box regression on these\ncandidate regions to obtain more accurate class and bounding box\npredictions, denoted as clsandreg, respectively. Similarly, we define\nthe learning of the student model in the target domain as follows:\nLunsup\u0000ğ‘‹ğ‘¡,Â¯ğ‘Œğ‘¡\u0001=LRPN\ncls\u0000ğ‘‹ğ‘¡,Â¯ğ‘Œğ‘¡\u0001+LRPN\nreg\u0000ğ‘‹ğ‘¡,Â¯ğ‘Œğ‘¡\u0001\n+LROI\ncls\u0000ğ‘‹ğ‘¡,Â¯ğ‘Œğ‘¡\u0001+LROI\nreg\u0000ğ‘‹ğ‘¡,Â¯ğ‘Œğ‘¡\u0001 (3)\nThen, we employ EMA to update a mean teacher model Fğ‘šğ‘’ğ‘ğ‘›\nby copying the weights from the student model. We define this\nprocess as follows:\nğœƒğ‘¡â†ğ›¼ğœƒğ‘¡+(1âˆ’ğ›¼)ğœƒğ‘  (4)\nwhere ğ‘¡andğ‘ represent the parameters of Fmean andFstu, re-\nspectively. By employing EMA to update the mean teacher model,we aim to create a more stable and robust model by gradually in-\ncorporating the knowledge learned by the student model over time.\nWe select the output of Fmean as result for predicting.\nWe apply a hyper parameter ğœ†to adjust the weights between\nLunsup andLsup. The final formulation of our comprehensive loss\nfunction is summarized as follows:\nL=Lsup+ğœ†Â·Lunsup (5)\nIn our DDT framework, following [ 44], we employ Weak Aug-\nmentation to provide target domain images to the diffusion teacher\nmodel for generating reliable and accurate pseudo labels. Simulta-\nneously, we apply Strong Augmentation to the images as inputs to\nthe student model, as illustrated in Fig. 2. Specifically, Weak Aug-\nmentation includes random crop and random horizontal flip, while\nStrong Augmentation involves color transformations such as color\nspace conversion, contrast adjustment, equalization, sharpness en-\nhancement, and posterization, as well as spatial transformations\nsuch as rotation, shear, and translation of the position.\n4 Experiments\n4.1 Datasets\nCityscapes. Cityscapes [ 13] dataset provides a diverse of urban\nscenes from 50 cities. It includes 2,975 training images and 500\nvalidation images with detailed annotations. The dataset covers 8\ndetection categories, using bounding boxes sourced from instance\nsegmentation.\nBDD100K. BDD100K [ 80] dataset is a comprehensive collection\nof 100,000 images specifically designed for autonomous driving\napplications. The dataset offers detailed detection annotations with\n10 categories.\nSim10K. Sim10k [ 32] is a synthetic dataset comprising 10,000\nrendered images simulated within the Grand Theft Auto gaming\nengine, specifically designed to facilitate the training and evaluation\nof object detection algorithms in autonomous driving systems.\nVOC. VOC [ 17] is a general-purpose object detection dataset\nthat includes bounding box and class annotations for common\nobjects across 20 categories from the real world. Following [ 44], we\ncombined the PASCAL VOC 2007 and 2012 editions, resulting in a\ntotal of 16,551 images.\nClipart. Clipart [ 30] dataset comprises 1,000 clipart images\nacross the same 20 categories as the VOC dataset, exhibiting signif-\nicant differences from real-world images. Following [ 44], we utilize\n500 images each for training and testing purposes.\nComic. Comic [ 30] dataset consists of 2,000 comic-style images,\nfeaturing 6 categories shared with the VOC dataset. Following [ 31],\nwe allocate 1,000 images each for training and testing.\nWatercolor. Watercolor [ 30] dataset contains 2,000 images in a\nwatercolor painting style, with 6 categories shared with the VOC\ndataset. Following [ 44], we use 1,000 images for both training and\ntesting.\n4.2 Cross-domian Detection Settings\nCross-Camera. We train on Cityscapes [ 13] (source domain)\nand validate on BDD100K [ 80] (target domain) to evaluate the\ncross-camera detection performance in diverse weather and scene\nconditions. We focus on the 7 same categories as SWDA [64].\n--- Page 5 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nClipartBDD100kGround TruthVisualization of baselinePredictions of baselineVisualization of our DDTPredictions of our DDT\nFigure 3: Qualitative prediction results and feature visualization of baseline and our DDT. Compared to the baseline, our method\nfocuses more on specific classes of objects in the target domain images, effectively reducing the numbers of false negative on Clipart [ 13]\n(first row ) and BDD100K [80] ( second row ).\nSynthetic to Real (Syn2Real). We train on Sim10K (source\ndomain) and validate on Cityscapes [ 13] and BDD100K [ 80] (target\ndomian)to validate the performance of synthetic-to-real detection.\nFollowing SWDA [64], we focus on the shared category car.\nReal to Artistic. We train on the VOC [ 17] (source domain)\nand perform validation on the Clipart [ 30], Comic [ 30], and Wa-\ntercolor [ 30] (target domains) to assess cross-domain detection\nperformance from real-world images to artistic styles. Referring to\nAT [ 44] and D-ADAPT [ 31], we respectively apply the 20, 6, and\n6 shared categories between VOC and each of the Clipart, Comic,\nand Watercolor.\n4.3 Implementation Details\nFollowing [ 31,44,64], we use Faster R-CNN [ 58] as the default de-\ntector with a ResNet101 [ 23] backbone pretrained on ImageNet [ 62],\nimplemented with MMDetection [ 9]. The training and testing sizes\nof images are set to (1333, 800) for Cityscapes, BDD100K, and\nSim10K, and (1200, 600) for VOC, Clipart, Comic, and Watercolor.\nThe models are trained with 20,000 steps on two 3090 GPUs, with a\ntotal batch size of 16. We employ the SGD optimizer with an initial\nlearning rate of 0.02, following the default settings in MMDetection.\nIn self-training, we refer to the settings in [ 15,44] to apply both\nweak and strong augmentation on the unlabeled target domain.\nWe employ the EMA update parameter ğ›¼of 0.999 for the mean\nteacher model updates and simply set the loss weight ğœ†to 1. We\ntrain exclusively on the source domain for the first 12000 steps and\nthen perform joint training on both the source and target domain\nfor the remaining 8000 steps.\nFor evaluation, we report the Average Precision (AP) for each\nobject category and the mean Average Precision (mAP) across all\ncategories, with applying an Intersection over Union (IoU) threshold\nof 0.5.\n4.4 Results and Comparisons\nIn this section, we present the evaluation result of our DDT frame-\nwork along with other SOTA approaches. Current cross-domain\nobject detection methods employ different detectors [ 48,58,72,92],Table 1: Quantitative results on adaptation from Cityscapes to\nBDD100K (Csâ†’B). The bold indicates the best results.\nMethod Reference Detector bicycle bus car mcycle person rider truck mAP\nDA-Faster [11] CVPRâ€™18 FRCNN -V16 22.4 18.0 44.2 14.2 28.9 27.4 19.1 24.9\nSWDA [64] CVPRâ€™19 FRCNN -V16 23.1 20.7 44.8 15.2 29.5 29.9 20.2 26.2\nSCDA [91] CVPRâ€™19 FRCNN -V16 23.2 19.6 44.4 14.8 29.3 29.2 20.3 25.8\nCRDA [77] CVPRâ€™20 FRCNN -R101 25.5 20.6 45.8 14.9 32.8 29.3 22.7 27.4\nSED [43] AAAIâ€™21 FRCNN -V16 25.0 23.4 50.4 18.9 32.4 32.6 20.6 29.0\nTDD [24] CVPRâ€™22 FRCNN -V16 28.8 25.5 53.9 24.5 39.6 38.9 24.1 33.6\nPT[10] ICMLâ€™22 FRCNN -V16 28.8 33.8 52.7 23.0 40.5 39.9 25.8 34.9\nEPM [27] ECCVâ€™20 FCOS -R101 20.1 19.1 55.8 14.5 39.6 26.8 18.8 27.8\nSIGMA [41] CVPRâ€™22 FCOS -R50 26.3 23.6 64.1 17.9 46.9 29.6 20.2 32.7\nSIGMA++ [42] TPAMIâ€™23 FRCNN -V16 27.1 26.3 65.6 17.8 47.5 30.4 21.1 33.7\nNSA [89] ICCVâ€™23 FRCNN -V16 / / / / / / / 35.5\nHT[15] CVPRâ€™23 FCOS -V16 38.0 30.6 63.5 28.2 53.4 40.4 27.4 40.2\nBaseline/ FRCNN -R1823.8 13.0 51.8 17.0 42.5 27.4 15.7 27.3\nDDT(Ours) 36.8 27.0 64.9 25.8 55.3 39.2 27.3 39.5 +12.2\nBaseline/ FRCNN -R5024.8 16.5 53.9 15.4 45.3 27.6 18.2 28.8\nDDT(Ours) 39.0 31.6 65.9 30.2 57.7 39.8 28.6 41.8 +13.0\nBaseline/ FRCNN -R10125.9 18.4 48.8 17.2 41.1 29.8 21.7 29.0\nDDT(Ours) 40.3 32.3 66.7 31.8 59.1 41.6 31.8 43.4 +14.4\nwhich we refer to as FRCNN, FCOS, SSD, and DDETR in our table.\nFurthermore, the backbones with varying depths, including ResNet-\n18, ResNet-50, ResNet-101 [ 23], and VGG-16 [ 66], are denoted as\nR18, R50, R101, and V16, respectively. To provide a comprehensive\ncomparison, we report the results of our method with ResNet18,\nResNet50, and ResNet101. The baseline refers to the results that\nonly train on the source domain and test on the target domain.\nCross-camera adaptation. Tab. 1 presents the results of the\nCross-camera settings. Our DDT method achieved the best perfor-\nmance with mAP 43.4 on the target domain, surpassing the previ-\nous SOTA method HT [ 15] by 3.2 mAP and outperforming other\nmethods by a significant margin. Notably, AT [ 44] and HT [ 15]\nutilize self-training framework, have demonstrated substantial per-\nformance improvements by enhancing the quality of generated\npseudo labels. Leveraging the powerful feature representation ca-\npability of the diffusion model and its exceptional performance on\n--- Page 6 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nTable 2: Quantitative results on adaptation from Sim10K to\nBDD100K (Sâ†’B).The bold indicates the best results.\nMethod Reference Detector mAP(car)\nSWDA [64] CVPRâ€™19 FRCNN -V16 42.9\nCDN [68] ECCVâ€™20 FRCNN -V16 45.3\nBaseline/ FRCNN -R1830.9\nDDT(Ours) 57.2 +26.3\nBaseline/ FRCNN -R5034.4\nDDT(Ours) 57.6 +23.2\nBaseline/ FRCNN -R10134.2\nDDT(Ours) 58.3 +24.1\nTable 3: Quantitative results on adaptation from Sim10K to\nCityscapes (Sâ†’Cs). The bold indicates the best results.\nMethod Reference Detector mAP(car)\nSSAL [54] NeurIPSâ€™22 FCOS -R50 51.8\nO2NET [19] ACMMMâ€™22 DDETR -R50 54.1\nDDF [47] TMMâ€™22 FRCNN -R50 44.3\nD-ADAPT [31] ICLRâ€™22 FRCNN -R50 51.9\nSCAN [40] AAAIâ€™22 FCOS -V16 52.6\nMTTrans [81] ECCVâ€™22 DDETR -R50 57.9\nSIGMA [41] CVPRâ€™22 FCOS -R50 53.7\nTDD [42] CVPRâ€™22 FRCNN -V16 53.4\nMGA [88] CVPRâ€™22 FCOS -R101 54.1\nOADA [79] ECCVâ€™22 FCOS -V16 59.2\nSIGMA++ [42] TPAMIâ€™23 FCOS -V16 53.7\nCIGAR [50] CVPRâ€™23 FCOS -V16 58.5\nNSA [89] ICCVâ€™23 FRCNN -V16 56.3\nHT[15] CVPRâ€™23 FRCNN -V16 65.5\nBaseline/ FRCNN -R1842.9\nDDT(Ours) 62.3 +19.4\nBaseline/ FRCNN -R5043.0\nDDT(Ours) 62.7 +19.7\nBaseline/ FRCNN -R10143.4\nDDT(Ours) 64.0 +20.6\ndiverse images, our DDT method achieve a remarkable enhance-\nment in cross-camera detection.\nSynthetic to Real adaptation. In Tab. 2, our method achieves\nimprovements of 26.3, 23.3, and 24.2 mAP on BDD100K [ 80] com-\npared with baseline, respectively, surpassing the results of previous\nalgorithms SWDA [ 64] and CDN [ 68]. Similarly, our method ob-\ntains improvements of 19.4, 19.7, and 19.3 mAP on Cityscapes [ 13],\nrespectively, surpassing all methods except HT [ 15] in Table 3.\nIt can be observed that detectors for synthetic-to-real detection,\ndue to the significant differences between synthetic and real-world\nimages, does not perform well with source data only, while our\nmethod significantly improves the cross-domain performance from\nSim10K [32] to BDD100K [80] and Cityscapes [13].\nReal to Artistic adaptation. In Tab. 4, 5, and 6, we show the\nresults of real to artistic cross-domain object detection. Our results\naeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitorbackground\nPrediction Labelaeroplane\nbicycle\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ndiningtable\ndog\nhorse\nmotorbike\nperson\npottedplant\nsheep\nsofa\ntrain\ntvmonitor\nbackgroundGround Truth Label\n01020304050607080\naeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitorbackground\nPrediction Labelaeroplane\nbicycle\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ndiningtable\ndog\nhorse\nmotorbike\nperson\npottedplant\nsheep\nsofa\ntrain\ntvmonitor\nbackgroundGround Truth Label\n020406080(a) Confusion matrix of baseline (left) and DDT (right)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Precision[0.114]C75\n[0.283]C50\n[0.344]Loc\n[0.437]Sim\n[0.437]Oth\n[0.522]BG\n[1.000]FN\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Precision\n[0.277]C75\n[0.556]C50\n[0.623]Loc\n[0.689]Sim\n[0.689]Oth\n[0.817]BG\n[1.000]FN\n(b) COCO [ 46] style detection error analysis of the baseline (left) and DDT (right)\nFigure 4: Error analysis on Clipart. It is evident that our method\nsignificantly reduces false negatives, which correspond to missed\ndetections.\nof Resnet50 and ResNet101 significantly surpass the previous best\nmethod AT [ 44] by 3.9 and 4.8 mAP, respectively. On Comic [ 30],\nthe results of ResNet18, ResNet50, and ResNet101 [ 23] greatly sur-\npass the previous best result of AT [ 44], by 2.0, 6.5, and 9.7 mAP,\nrespectively. Similarly, our best result surpasses AT [ 44] by 3.8 mAP\non Watercolor [ 30] as shown in Tab. 6. In real to artistic benchmark,\noverall, we find that due to the significant differences between\nreal-world images and artistic-style images, the cross-domain per-\nformance is poor. Compared to the baseline, our method exhibits\nan average relative improvement of 95%, 163%, and 48% on Clipart,\nComic, and Watercolor, respectively. This indicates that real to\nartistic adaptation is a challenging task, and it also demonstrates\nthat our approach we have successfully improved the cross-domain\nperformance by reducing the gap between real and artistic domain.\n4.5 Ablation Studies\nWe conduct additional experiments to analyze the feature repre-\nsentation capabilities of different models. Specifically, we com-\npared our diffusion model with powerful backbones, including\nConvNext [ 52], Swin Transformer [ 51], VIT [ 16], as well as the\nself-supervised method MAE [ 22], pretrained on ImageNet [ 62].\nAdditionally, GLIP [ 36], which is pretrained on a larger dataset and\nhas shown promising performance on object detection benchmarks.\nOur objective is to investigate two questions:\n(1)Will the diffusion model offer better intra-domain and cross-\ndomain feature representation?\n(2) Will the diffusion model serve as a better teacher?\nAblation Study on the Intra-domain and Cross-domain\nRepresentation. To answer the first question, we evaluate the\nperformance of seven models across different data settings in Tab. 7.\n--- Page 7 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nTable 4: Quantitative results on adaptation from VOC to Clipart (V â†’Ca). The bold indicates the best results.\nMethod Reference Detector aero bcycle bird boat bottle bus car cat chair cow table dog horse bike psn plant sheep sofa train tv mAP\nAT[44] CVPRâ€™22 FRCNN -V16 33.8 60.9 38.6 49.4 52.4 53.9 56.7 7.5 52.8 63.5 34.0 25.0 62.2 72.1 77.2 57.7 27.2 52.0 55.7 54.1 49.3\nD-ADAPT [31] ICLRâ€™22 FRCNN -R50 56.4 63.2 42.3 40.9 45.3 77.0 48.7 25.4 44.3 58.4 31.4 24.5 47.1 75.3 69.3 43.5 27.9 34.1 60.7 64.0 49.0\nTIA [85] CVPRâ€™22 FRCNN -R101 42.2 66.0 36.9 37.3 43.7 71.8 49.7 18.2 44.9 58.9 18.2 29.1 40.7 87.8 67.4 49.7 27.4 27.8 57.1 50.6 46.3\nLODS [38] CVPRâ€™22 FRCNN -R101 43.1 61.4 40.1 36.8 48.2 45.8 48.3 20.4 44.8 53.3 32.5 26.1 40.6 86.3 68.5 48.9 25.4 33.2 44.0 56.5 45.2\nCIGAR [50] CVPRâ€™23 FCOS -R101 35.2 55.0 39.2 30.7 60.1 58.1 46.9 31.8 47.0 61.0 21.8 26.7 44.6 52.4 68.5 54.4 31.3 38.8 56.5 63.5 46.2\nCMT [3] CVPRâ€™23 FRCNN -V16 39.8 56.3 38.7 39.7 60.4 35.0 56.0 7.1 60.1 60.4 35.8 28.1 67.8 84.5 80.1 55.5 20.3 32.8 42.3 38.2 47.0\nBaseline/ FRCNN -R1825.0 36.4 16.2 19.6 34.3 50.7 30.3 0.2 33.6 5.5 22.1 6.5 23.3 47.9 36.1 26.8 3.2 18.5 31.7 25.1 24.6\nDDT(Ours) 55.8 64.8 37.0 37.3 46.5 61.7 52.4 3.3 50.8 39.9 36.6 25.1 46.9 87.4 77.6 52.7 25.2 40.1 45.0 44.9 46.5 +21.9\nBaseline/ FRCNN -R5029.5 36.1 22.8 25.6 34.2 41.3 27.6 3.4 39.2 8.3 28.0 5.2 18.8 47.9 34.1 37.6 4.0 21.8 31.8 24.0 26.0\nDDT(Ours) 48.9 64.9 41.1 48.1 60.5 76.6 60.2 7.9 57.4 51.5 40.8 33.0 55.4 98.0 82.8 60.1 35.2 36.0 50.8 55.8 53.2 +27.2\nBaseline/ FRCNN -R10136.7 27.2 22.6 22.8 38.5 46.4 32.2 10.7 40.7 7.2 27.2 7.6 28.2 56.1 38.2 38.2 9.2 29.3 25.9 21.6 28.3\nDDT(Ours) 56.1 66.6 39.4 55.2 51.3 79.9 62.1 8.3 57.9 46.3 40.5 39.3 51.4 96.3 84.5 60.9 28.0 42.7 56.7 59.2 55.6 +27.3\nTable 5: Quantitative results on adaptation from VOC to\nComic (Vâ†’Co). The bold indicates the best results.\nMethod Reference Detector bicycle bird car cat dog person mAP\nDA-Faster [11] CVPRâ€™18 FRCNN -V16 31.1 10.3 15.5 12.4 19.3 39.0 21.2\nSWDA [64] CVPRâ€™19 FRCNN -V16 36.4 21.8 29.8 15.1 23.5 49.6 29.4\nSTABR [33] CVPRâ€™19 SSD -V16 50.6 13.6 31.0 7.5 16.4 41.4 26.8\nMCRA [86] ECCVâ€™20 FRCNN -V16 47.9 20.5 37.4 20.6 24.5 50.2 33.5\nI3Net [8] CVPRâ€™21 SSD -V16 47.5 19.9 33.2 11.4 19.4 49.1 30.1\nDBGL [5] ICCVâ€™21 FRCNN -R101 35.6 20.3 33.9 16.4 26.6 45.3 29.7\nD-ADAPT [31] ICLRâ€™22 FRCNN -R101 52.4 25.4 42.3 43.7 25.7 53.5 40.5\nBaseline/ FRCNN -R1829.4 6.8 11.6 5.6 7.4 26.3 14.5\nDDT(Ours) 56.8 21.2 46.6 25.5 32.3 72.6 42.5 +28.0\nBaseline/ FRCNN -R5037.1 6.9 29.9 6.9 10.5 30.0 20.2\nDDT(Ours) 60.9 28.2 52.5 29.3 36.8 74.5 47.0 +26.8\nBaseline/ FRCNN -R10137.0 6.8 31.2 4.8 7.2 26.8 19.0\nDDT(Ours) 63.2 34.8 56.6 31.7 39.0 75.9 50.2 +31.2\nTable 6: Quantitative results on adaptation from VOC to Wa-\ntercolor (Vâ†’W). The bold indicates the best results.\nMethod Reference Detector bicycle bird car cat dog person mAP\nSWDA [11] CVPRâ€˜19 FRCNN -V16 82.3 55.9 46.5 32.7 35.5 66.7 53.3\nMCRA [87] ECCVâ€˜20 FRCNN -V16 87.9 52.1 51.8 41.6 33.8 68.8 56.0\nUMT [14] CVPRâ€™21 FRCNN -R101 88.2 55.3 51.7 39.8 43.6 69.9 58.1\nIIOD [75] TPAMIâ€˜21 FRCNN -V16 95.8 54.3 48.3 42.4 35.1 65.8 56.9\nI3Net [8] CVPRâ€™21 SSD -V16 81.1 49.3 46.2 35.0 31.9 65.7 51.5\nSADA [12] IJCVâ€˜21 FRCNN -R50 82.9 54.6 52.3 40.5 37.7 68.2 56.0\nCDG [37] AAAIâ€™21 FRCNN -V16 97.7 53.1 52.1 47.3 38.7 68.9 59.7\nVDD [76] ICCVâ€˜21 FRCNN -V16 90.0 56.6 49.2 39.5 38.8 65.3 56.6\nDBGL [5] ICCVâ€™21 FRCNN -R101 83.1 49.3 50.6 39.8 38.7 61.3 53.8\nAT[44] CVPRâ€™22 FRCNN -V16 93.6 56.1 58.9 37.3 39.6 73.8 59.9\nLODS [38] CVPRâ€™22 FRCNN -R101 95.2 53.1 46.9 37.2 47.6 69.3 58.2\nBaseline/ FRCNN -R1871.4 36.4 39.1 19.9 12.7 52.0 38.6\nDDT(Ours) 81.9 53.7 54.3 37.7 31.5 68.3 54.6 +16.0\nBaseline/ FRCNN -R5068.2 40.2 44.7 21.7 10.3 44.0 38.2\nDDT(Ours) 96.4 58.6 52.6 33.7 36.2 71.9 58.2 +20.0\nBaseline/ FRCNN -R10172.5 40.1 45.7 30.5 18.1 45.8 42.1\nDDT(Ours) 87.1 64.0 55.7 50.6 48.8 75.7 63.7 +21.6Table 7: Results of our diffusion feature extractor (Diff.) com-\npared to other backbones. The bold and underlined represent\nthe best and second performances, respectively\nBackbones Vâ†’V\nCaâ†’Ca\nVâ†’Ca\nRel.(%) Sâ†’S\nCSâ†’CS\nSâ†’CS\nRel.(%) Bâ†’B\nSâ†’B\nRel.(%)\nR101 [23] 84.0 40.0 28.3 70.8 83.0 72.6 43.4 59.8 74.2 34.2 46.1\nConvNext -Base [52] 91.5 62.8 44.1 70.2 87.9 78.2 61.8 79.0 80.5 44.8 55.6\nSwin -Base [51] 86.9 51.6 32.2 62.4 87.6 73.7 53.7 72.8 79.7 39.3 49.4\nVIT-Base [16] 84.9 31.5 28.6 90.8 77.6 72.1 48.4 67.1 73.4 40.9 55.7\nMAE (VIT-Base) [16, 22] 85.8 37.8 26.4 69.8 85.0 77.7 57.9 74.5 78.3 40.9 52.2\nGLIP (Swin-Tiny) [36, 51] 88.1 55.6 39.8 71.6 86.5 77.2 59.3 76.8 79.7 50.4 63.3\nDiff (Ours) 75.4 43.9 47.4 108.1 76.1 71.7 58.2 81.2 71.8 50.1 69.8\nTable 8: Results of using our diffusion backbone (Diff.) as the\nteacher model to train student models on different backbones.\nThe bold indicates the best results.\nDetector Setting Cross Domain Settings\nStudent Teacher Vâ†’Ca Sâ†’Cs Sâ†’B\nR101 [23] ConvNext -Base [52] 43.2 +19.9 60.2 +16.8 54.6 +20.4\nR101 [23] Swin -Base [51] 35.1 +6.8 57.5 +14.1 52.9 +18.7\nR101 [23] VIT -Base [16] 37.4 +9.1 54.8 +11.4 51.1 +16.9\nR101 [23] MAE (VIT-Base) [16, 22] 35.8 +7.5 59.0 +16.2 53.2 +19.0\nR101 [23] GLIP (Swin-Tiny) [36, 51] 39.6 +11.3 58.9 +15.5 54.0 +19.8\nR101 [23] Diff. (Ours) 55.6 +27.3 64.0 +20.6 58.3 +24.1\nConvNext -Base [52] Diff. (Ours) 59.5 +18.4 63.5 +1.7 59.6 +7.1\nSwin -Base [51] Diff. (Ours) 46.6 +14.4 63.6 +9.9 58.6 +13.8\nVIT -Base [16] Diff. (Ours) 41.7 +13.1 60.2 +11.8 54.9 +15.6\nMAE (VIT-Base) [16, 22] Diff. (Ours) 43.1 +16.7 64.6 +6.7 57.3 +16.4\nGLIP (Swin-Tiny) [36, 51] Diff. (Ours) 49.5 +9.7 64.0 +4.7 59.5 +9.4\nTable 9: Results of ablation experiments on Diffusion Teacher\nand Mean Teacher.\nSettings of Self-training Csâ†’B Sâ†’Cs Sâ†’B Vâ†’Ca Vâ†’Co Vâ†’W\nDDT (R101) 43.4 64.0 58.3 55.6 50.2 63.7\nw/o Mean Teacher 40.6 -2.862.5 -1.555.5 -2.849.1 -6.5 46.2 -4.061.0 -2.7\nw/o Diffusion Teacher 40.1 -3.357.4 -6.654.4 -3.946.9 -8.737.5 -12.7 57.4 -6.3\nw/o All Teacher 37.5 -5.956.1 -7.953.7 -4.641.9 -13.7 37.4 -12.8 57.1 -6.6\n--- Page 8 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nTable 10: Ablation results of the diffusion backbone under\ndifferent time steps and save steps.\nTime\nStepsSave\nStepsTrain Time\n(s/iter)Inf. Time\n(ms/image)Csâ†’B Sâ†’Cs Sâ†’B Vâ†’Ca Vâ†’Co Vâ†’W\n1 1 0.82 271.1 29.8 57.2 50.4 37.8 36.9 52.7\n2 2 1.14 402.5 31.0 57.5 50.1 39.3 36.3 51.8\n5 5 1.56 780.4 32.7 58.2 50.1 47.4 39.4 53.8\n10 5 2.84 1424.2 29.8 56.3 51.1 48.4 40.9 53.7\n20 10 5.48 2710.2 28.0 55.2 50.2 48.8 41.3 54.6\nSpecifically, we compared their intra-domain performance in train-\ning and testing within the source domain ( Vâ†’V,Sâ†’S) and tar-\nget domain ( Caâ†’Ca,Csâ†’Cs,Bâ†’B), and cross-domain testing\n(Vâ†’Ca,Sâ†’Cs,Sâ†’B) to assess their cross-domain feature repre-\nsentation capabilities. we find that our diffusion model performe\npoorly within the intra-domain, lagging behind the other six models.\nIn cross-domain testing, our diffusion model outperformed other\nmethods on Clipart [ 30] but remained inferior to ConvNext [ 52]\nand GLIP [ 36]. Additionally, we calculate the cross-domain metrics\nfor each model and the results obtained from training and testing in\nthe target domain to measure the relative cross-domain capabilities\nof each models, represented as \"Rel.\" in Tab. 7. We find that the dif-\nfusion model consistently achieves the best relative cross-domain\nperformance. Overall, the answer to the first question may be dis-\nappointing, as the diffusion detector showes some improvement in\ncross-domain performance but still fall behind the detectors with\nstronger and large-dataset pretrained backbones.\nAblation Study of Different Teachers. In Tab. 8, we present\nthe performance of different teacher and student settings for cross-\ndomain detection. First, we use ResNet101 [ 23] as the student and\nother models as teacher. We find that although the diffusion model\nperforms worse than ConvNext [ 52] and GLIP [ 36] in cross-domain\nperformance, it exhibits significantly better performance when used\nas a teacher model. Furthermore, when we use the diffusion model\nas the teacher and the other six models as students, it consistently\nbrings large improvements. This answers our second question, con-\nfirming that our diffusion model is indeed a better teacher, even\nwhen faced with highly competent students and consistently im-\nprove their performance.\nAblation Results on Diffusion and Mean Teacher. To better\nunderstand the significance of teachers in our DDT, we present the\nresults of different teacher model settings in Tab. 9. The findings\nreveal that excluding the Mean Teacher and Diffusion Teacher from\nour method leads to an average decrease of 3.1 and 6.7 mAP, respec-\ntively. When all teachers are removed, the self-training performance\nexperiences an average decline of 8.3 mAP. These results clearly\ndemonstrate that both the diffusion teacher and mean teacher play\ncrucial roles in our DDT and are indispensable for achieving better\nperformance. Fig. 1 provides an intuitive illustration of the impacts\nof the diffusion teacher and mean teacher in training process.\nAblation Results of Different Diffusion Settings. We report\nthe results of the diffusion models with different time steps and\nsave steps settings in Tab. 10. It is observed that in cross-domain\ndetection with a larger domain gap (real to artistic), longer timesteps and save steps show better results. We consider a trade-off\nbetween accuracy and efficiency and choose time steps 5 and save\nsteps 5 as our default settings.\n4.6 Analysis\nAnalysis of Feature Representation of Diffusion Model. The\nresults in Tab. 7 and 8 further deepen our understanding of the\nfeature representation of diffusion model and its advantages in\ncross-domain detection. In our view, the observed results can be\nattributed as: fully frozen weight and adaptation of the diffusion\nmodel with the light structure that aligns with the hierarchical\nfeature outputs, limit its performance within the intro-domain com-\npared to fully trainable models. However, when applied as a teacher,\nthe diffusion model guides the student to achieve superior perfor-\nmance in cross-domain, surpassing even the teacher itself. We think\nthat the improved cross-domain representation ability can be attrib-\nuted to the inherent characteristics of the diffusion model as well\nas the advantages gained from supervised learning on the source\ndomain. In contrast, other fully trainable teacher models often con-\ncentrate primarily on supervised learning on the source domain,\nresulting in homogeneous optimization and limited guidance for\nthe student. As a result, it becomes challenging to enhance the per-\nformance of the students to the level achieved by the homogeneous\nteacher. These results provide compelling evidence for the advan-\ntages of the diffusion model in addressing cross-domain detection\ntasks.\nError analysis. Error analysis on Clipart [ 30] reveals that false\nnegatives, i.e., missed detections, are the main factor impacting the\nperformance on the target domain as shown in Fig. 4. Our method\nsignificantly reduces the number of missed detections, thereby\ngreatly improving the performance of cross-domain detection. A\nrepresentation of prediction results and feature visualization further\ncorroborate this conclusion, as depicted in the Fig. 3.\n5 Conclusion\nIn this paper, we propose a domain adaptive method based on the\ndiffusion model to address the performance degradation caused by\nthe large gap between the source and target domain. We employ a\nfrozen-weight diffusion model as the backbone and extract interme-\ndiate feature in the inversion process for the detection task, which\nwe refer to as the diffusion teacher. Subsequently, we apply diffusion\nteacher in the self-training framework to generate pseudo labels\non the unlabeled target domain, guiding the learning of the student\nmodel. Our method significantly improves cross-domain detection\nperformance on six datasets, achieving an average improvement of\n21.2% mAP compared to the baseline, surpassing the current SOTA\nmethods by an average of 5.7% mAP, without compromising the\ninference speed. Furthermore, we validate the consistent perfor-\nmance improvement of our method in more extensive experiments\nfor detectors with more powerful backbones, demonstrating the\nstrong and universality domain adaptive capability of our approach.\n6 Acknowledgment\nThe authors would like to thank Xiamen University and Unmanned\nAerial Vehicle (UAV) Laboratory for the funding and providing with\nall the necessary technical support.\n--- Page 9 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nReferences\n[1]Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and\nArtem Babenko. 2022. Label-Efficient Semantic Segmentation with Diffusion\nModels. In International Conference on Learning Representations .\n[2]Zhaowei Cai and Nuno Vasconcelos. 2018. Cascade r-cnn: Delving into high\nquality object detection. In Proceedings of the IEEE conference on computer vision\nand pattern recognition . 6154â€“6162.\n[3]Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, and Yu-Xiong Wang. 2023. Con-\ntrastive mean teacher for domain adaptive object detectors. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 23839â€“23848.\n[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European conference on computer vision . Springer, 213â€“229.\n[5]Chaoqi Chen, Jiongcheng Li, Zebiao Zheng, Yue Huang, Xinghao Ding, and\nYizhou Yu. 2021. Dual bipartite graph learning: A general approach for domain\nadaptive object detection. In Proceedings of the IEEE/CVF International Conference\non Computer Vision . 2703â€“2712.\n[6]Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and Qi Dou. 2020. Har-\nmonizing transferability and discriminability for adapting object detectors. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n8869â€“8878.\n[7]Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and Qi Dou. 2020. Har-\nmonizing transferability and discriminability for adapting object detectors. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n8869â€“8878.\n[8]Chaoqi Chen, Zebiao Zheng, Yue Huang, Xinghao Ding, and Yizhou Yu. 2021.\nI3net: Implicit instance-invariant network for adapting one-stage object detec-\ntors. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition . 12576â€“12585.\n[9]Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li,\nShuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al .2019. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019).\n[10] Meilin Chen, Weijie Chen, Shicai Yang, Jie Song, Xinchao Wang, Lei Zhang,\nYunfeng Yan, Donglian Qi, Yueting Zhuang, Di Xie, et al .2022. Learning Domain\nAdaptive Object Detection with Probabilistic Teacher. In International Conference\non Machine Learning . PMLR, 3040â€“3055.\n[11] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2018.\nDomain adaptive faster r-cnn for object detection in the wild. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 3339â€“3348.\n[12] Yuhua Chen, Haoran Wang, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc\nVan Gool. 2021. Scale-aware domain adaptive faster r-cnn. International Journal\nof Computer Vision 129, 7 (2021), 2223â€“2243.\n[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En-\nzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016.\nThe cityscapes dataset for semantic urban scene understanding. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 3213â€“3223.\n[14] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. 2021. Unbiased mean teacher\nfor cross-domain object detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 4091â€“4101.\n[15] Jinhong Deng, Dongli Xu, Wen Li, and Lixin Duan. 2023. Harmonious teacher\nfor cross-domain object detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 23829â€“23838.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and\nAndrew Zisserman. 2010. The pascal visual object classes (voc) challenge. Inter-\nnational journal of computer vision 88 (2010), 303â€“338.\n[18] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, FranÃ§ois Laviolette, Mario March, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. Journal of machine learning\nresearch 17, 59 (2016), 1â€“35.\n[19] Kaixiong Gong, Shuang Li, Shugang Li, Rui Zhang, Chi Harold Liu, and Qiang\nChen. 2022. Improving transferability for domain adaptive detection transformers.\nInProceedings of the 30th ACM International Conference on Multimedia . 1543â€“\n1551.\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\nnetworks. Commun. ACM 63, 11 (2020), 139â€“144.\n[21] Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, and Yanpeng Cao. 2021.\nUncertainty-aware unsupervised domain adaptation in object detection. IEEE\nTransactions on Multimedia 24 (2021), 2502â€“2514.\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 16000â€“16009.[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[24] Mengzhe He, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan,\nWei Wu, and Yu Qiao. 2022. Cross domain object detection by target-perceived\ndual branch distillation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition . 9570â€“9580.\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in neural information processing systems 33 (2020), 6840â€“6851.\n[26] Lukas Hoyer, Dengxin Dai, Haoran Wang, and Luc Van Gool. 2023. MIC: Masked\nimage consistency for context-enhanced domain adaptation. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 11721â€“11732.\n[27] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. 2020. Ev-\nery pixel matters: Center-aware feature alignment for domain adaptive object\ndetector. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23â€“28, 2020, Proceedings, Part IX 16 . Springer, 733â€“748.\n[28] Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih Hung, Hung-Yu Tseng,\nManeesh Singh, and Ming-Hsuan Yang. 2020. Progressive domain adaptation for\nobject detection. In Proceedings of the IEEE/CVF winter conference on applications\nof computer vision . 749â€“757.\n[29] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu\nWang, Weizhu Chen, et al .2022. LoRA: Low-Rank Adaptation of Large Language\nModels. In International Conference on Learning Representations .\n[30] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. 2018.\nCross-domain weakly-supervised object detection through progressive domain\nadaptation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition . 5001â€“5009.\n[31] Junguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng Long. 2021. Decou-\npled Adaptation for Cross-Domain Object Detection. In International Conference\non Learning Representations .\n[32] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Srid-\nhar, Karl Rosaen, and Ram Vasudevan. 2017. Driving in the Matrix: Can virtual\nworlds replace human-generated annotations for real world tasks?. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE.\n[33] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Changick Kim. 2019. Self-\ntraining and adversarial background regularization for unsupervised domain\nadaptive one-stage object detection. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 6092â€“6101.\n[34] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim.\n2019. Diversify and match: A domain adaptive representation learning paradigm\nfor object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 12456â€“12465.\n[35] Congcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian Luo, Yanjun Wu,\nand Pengfei Zhu. 2020. Spatial Attention Pyramid Network for Unsupervised\nDomain Adaptation. In European Conference on Computer Vision . 481â€“497.\n[36] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan\nLi, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al .\n2022. Grounded language-image pre-training. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 10965â€“10975.\n[37] Shuai Li, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. 2021. Category dictio-\nnary guided unsupervised domain adaptation for object detection. In Proceedings\nof the AAAI conference on artificial intelligence , Vol. 35. 1949â€“1957.\n[38] Shuaifeng Li, Mao Ye, Xiatian Zhu, Lihua Zhou, and Lin Xiong. 2022. Source-\nfree object detection by learning to overlook domain style. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 8014â€“8023.\n[39] Wuyang Li, Xinyu Liu, Xiwen Yao, and Yixuan Yuan. 2022. Scan: Cross domain\nobject detection with semantic conditioned adaptation. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 36. 1421â€“1428.\n[40] Wuyang Li, Xinyu Liu, Xiwen Yao, and Yixuan Yuan. 2022. Scan: Cross domain\nobject detection with semantic conditioned adaptation. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 36. 1421â€“1428.\n[41] Wuyang Li, Xinyu Liu, and Yixuan Yuan. 2022. Sigma: Semantic-complete graph\nmatching for domain adaptive object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 5291â€“5300.\n[42] Wuyang Li, Xinyu Liu, and Yixuan Yuan. 2023. Sigma++: Improved semantic-\ncomplete graph matching for domain adaptive object detection. IEEE Transactions\non Pattern Analysis and Machine Intelligence (2023).\n[43] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and\nYueting Zhuang. 2021. A free lunch for unsupervised domain adaptive object\ndetection without source data. In Proceedings of the AAAI Conference on Artificial\nIntelligence , Vol. 35. 8474â€“8481.\n[44] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, Kan Chen, Bichen Wu,\nZijian He, Kris Kitani, and Peter Vajda. 2022. Cross-domain adaptive teacher for\nobject detection. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 7581â€“7590.\n[45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.\nFocal loss for dense object detection. In Proceedings of the IEEE international\nconference on computer vision . 2980â€“2988.\n--- Page 10 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In Computer Visionâ€“ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 . Springer, 740â€“\n755.\n[47] Dongnan Liu, Chaoyi Zhang, Yang Song, Heng Huang, Chenyu Wang, Michael\nBarnett, and Weidong Cai. 2022. Decompose to adapt: Cross-domain object\ndetection via feature disentanglement. IEEE Transactions on Multimedia 25 (2022),\n1333â€“1344.\n[48] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,\nCheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox detec-\ntor. In Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11â€“14, 2016, Proceedings, Part I 14 . Springer, 21â€“37.\n[49] Xinyu Liu, Wuyang Li, Qiushi Yang, Baopu Li, and Yixuan Yuan. 2022. Towards\nrobust adaptive object detection under noisy annotations. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 14207â€“14216.\n[50] Yabo Liu, Jinghua Wang, Chao Huang, Yaowei Wang, and Yong Xu. 2023. CIGAR:\nCross-Modality Graph Reasoning for Domain Adaptive Object Detection. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n23776â€“23786.\n[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer us-\ning shifted windows. In Proceedings of the IEEE/CVF international conference on\ncomputer vision . 10012â€“10022.\n[52] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,\nand Saining Xie. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition . 11976â€“11986.\n[53] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell.\n2024. Diffusion hyperfeatures: Searching through time and space for semantic\ncorrespondence. Advances in Neural Information Processing Systems 36 (2024).\n[54] Muhammad Akhtar Munir, Muhammad Haris Khan, M Sarfraz, and Mohsen Ali.\n2021. Ssal: Synergizing between self-training and adversarial learning for domain\nadaptive object detection. Advances in Neural Information Processing Systems 34\n(2021), 22770â€“22782.\n[55] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. 2021. Detectors: Detecting\nobjects with recursive feature pyramid and switchable atrous convolution. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n10213â€“10224.\n[56] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.\nInInternational conference on machine learning . Pmlr, 8821â€“8831.\n[57] Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental improvement.\narXiv preprint arXiv:1804.02767 (2018).\n[58] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster R-CNN: To-\nwards real-time object detection with region proposal networks. IEEE transactions\non pattern analysis and machine intelligence 39, 6 (2016), 1137â€“1149.\n[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n10684â€“10695.\n[60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-\ntional networks for biomedical image segmentation. In Medical image computing\nand computer-assisted interventionâ€“MICCAI 2015: 18th international conference,\nMunich, Germany, October 5-9, 2015, proceedings, part III 18 . Springer, 234â€“241.\n[61] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin,\nHuaizu Jiang, Liangliang Cao, and Erik Learned-Miller. 2019. Automatic adapta-\ntion of object detectors to new domains using self-training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 780â€“790.\n[62] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al .\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision 115 (2015), 211â€“252.\n[63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L\nDenton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim\nSalimans, et al .2022. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in neural information processing systems 35\n(2022), 36479â€“36494.\n[64] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. 2019. Strong-\nweak distribution alignment for adaptive object detection. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 6956â€“6965.\n[65] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2018. Semantic foggy scene\nunderstanding with synthetic data. International Journal of Computer Vision 126\n(2018), 973â€“992.\n[66] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks\nfor large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).\n[67] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 (2020).[68] Peng Su, Kun Wang, Xingyu Zeng, Shixiang Tang, Dapeng Chen, Di Qiu, and\nXiaogang Wang. 2020. Adapting object detectors with conditional domain nor-\nmalization. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23â€“28, 2020, Proceedings, Part XI 16 . Springer, 403â€“419.\n[69] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath\nHariharan. 2023. Emergent correspondence from image diffusion. Advances in\nNeural Information Processing Systems 36 (2023), 1363â€“1389.\n[70] Renshuai Tao, Hainan Li, Tianbo Wang, Yanlu Wei, Yifu Ding, Bowei Jin, Hong-\nping Zhi, Xianglong Liu, and Aishan Liu. 2022. Exploring endogenous shift for\ncross-domain detection: A large-scale benchmark and perturbation suppression\nnetwork. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) . IEEE, 21157â€“21167.\n[71] Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models:\nWeight-averaged consistency targets improve semi-supervised deep learning\nresults. Advances in neural information processing systems 30 (2017).\n[72] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. 2020. FCOS: A simple and\nstrong anchor-free object detector. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 44, 4 (2020), 1922â€“1933.\n[73] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. Plug-and-play\ndiffusion features for text-driven image-to-image translation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 1921â€“1930.\n[74] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2023. YOLOv7:\nTrainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\nInProceedings of the IEEE/CVF conference on computer vision and pattern recogni-\ntion. 7464â€“7475.\n[75] Aming Wu, Yahong Han, Linchao Zhu, and Yi Yang. 2021. Instance-invariant\ndomain adaptive object detection via progressive disentanglement. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 4178â€“4193.\n[76] Aming Wu, Rui Liu, Yahong Han, Linchao Zhu, and Yi Yang. 2021. Vector-\ndecomposed disentanglement for domain-invariant object detection. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision . 9342â€“9351.\n[77] Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, and Xiu-Shen Wei. 2020. Exploring\ncategorical regularization for domain adaptive object detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 11724â€“\n11733.\n[78] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini\nDe Mello. 2023. Open-vocabulary panoptic segmentation with text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 2955â€“2966.\n[79] Jayeon Yoo, Inseop Chung, and Nojun Kwak. 2022. Unsupervised domain adap-\ntation for one-stage object detector using offsets to bounding box. In European\nConference on Computer Vision . Springer, 691â€“708.\n[80] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu,\nVashisht Madhavan, and Trevor Darrell. 2020. Bdd100k: A diverse driving dataset\nfor heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition . 2636â€“2645.\n[81] Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi Zhou, Yohei Nakata, Denis Gudovskiy,\nTomoyuki Okuno, Jianxin Li, Kurt Keutzer, and Shanghang Zhang. 2022. MT-\nTrans: Cross-domain object detection with mean teacher transformer. In European\nConference on Computer Vision . Springer, 629â€“645.\n[82] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and\nHeung-Yeung Shum. 2022. DINO: DETR with Improved DeNoising Anchor Boxes\nfor End-to-End Object Detection. In The Eleventh International Conference on\nLearning Representations .\n[83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional con-\ntrol to text-to-image diffusion models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 3836â€“3847.\n[84] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. 2020. Bridging\nthe gap between anchor-based and anchor-free detection via adaptive training\nsample selection. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition . 9759â€“9768.\n[85] Liang Zhao and Limin Wang. 2022. Task-specific inconsistency alignment for\ndomain adaptive object detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 14217â€“14226.\n[86] Zhen Zhao, Yuhong Guo, Haifeng Shen, and Jieping Ye. 2020. Adaptive object\ndetection with dual multi-label prediction. In Computer Visionâ€“ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XXVIII\n16. Springer, 54â€“69.\n[87] Zhen Zhao, Yuhong Guo, Haifeng Shen, and Jieping Ye. 2020. Adaptive object\ndetection with dual multi-label prediction. In Computer Visionâ€“ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XXVIII\n16. Springer, 54â€“69.\n[88] Wenzhang Zhou, Dawei Du, Libo Zhang, Tiejian Luo, and Yanjun Wu. 2022. Multi-\ngranularity alignment domain adaptation for object detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9581â€“9590.\n[89] Wenzhang Zhou, Heng Fan, Tiejian Luo, and Libo Zhang. 2023. Unsupervised\nDomain Adaptive Detection with Network Stability Analysis. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision . 6986â€“6995.\n--- Page 11 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\n[90] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired\nimage-to-image translation using cycle-consistent adversarial networks. In Pro-\nceedings of the IEEE international conference on computer vision . 2223â€“2232.\n[91] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and Dahua Lin. 2019.\nAdapting object detectors via selective cross-domain alignment. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 687â€“696.\n[92] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2021.\nDeformable DETR: Deformable Transformers for End-to-End Object Detection.\nInInternational Conference on Learning Representations .\n--- Page 12 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nA Appendix\nA.1 Additional Ablation Studies\nIn this section, we present additional ablation studies and com-\nparisons to further investigate the effectiveness of our proposed\nDiffusion Domain Teacher (DDT).\nAblation Study on Unsup Loss Weight ğœ†.We investigate the\nimpact of different unsupervised loss weight ğœ†on cross-domain\ndetection in Tab.11, including Cityscapes [ 13] to BDD100K [ 80]\n(Csâ†’B), Sim10K [ 32] to Cityscapes [ 13] (Sâ†’Cs), and VOC [ 17] to\nClipart [ 30] (Vâ†’Ca). Excessively high or low weight ğœ†will degrade\nthe performance of cross-domain detection, so we simply set the\nparameter ğœ†to 1.\nAblation Study on the Threshold ğœof Pseudo Labels. In Tab.\n12, we show the impact of different pseudo label threshold ğœon\nthe final results. The threshold plays a crucial role in determining\nthe quality of the generated pseudo labels during self-training,\nand inappropriate threshold will affect cross-domain performance.\nUltimately, we choose 0.5 as the default threshold ğœsetting in all\nour experiments.\nAblation Study on Data Augmentation of Sup. and Unsup.\nBranches. We follow previous work [ 44] and utilize Strong Aug-\nmentation andWeak Augmentation during the self-training process.\nTab. 13 presents the impact of Strong Augmentation on both super-\nvised and unsupervised data, highlighting the importance of data\naugmentation in the self-training process.\nAblation Study on Different Versions of Stable Diffusion. In\nTab. 14, we present the results of using the popular Stable Diffusion\nV1.5 (SD-1.5) and the latest version (SD-2.1) respectively. Overall,\nthe results of SD-1.5 are superior to those using SD-2.1, except for\nthe cross-domain detection from Sim10K to BDD100K.\nComparison of Backbone Efficiencies. In Tab. 15, we present\na comparative analysis of these models with respect to their ar-\nchitectural sizes, training cost, and inference latency. Diffusion\nmodel, with its substantial parameter count and protracted infer-\nence time, emerges as an impractical choice for deployment in\nroutine detection tasks within operational settings. Nevertheless,\nthe employment of this model as a strong teacher in self-training\nframework offers a strategic avenue to leverage its exceptional\ncross-domain prowess, achieving an average absolute improvement\nof 21.2 mAP and a relative improvement of 39.7% compared to the\nbaseline, without introducing any additional inference overhead.\nTable 11: Ablation Result of Unsup. Loss Weight ğœ†.\nğœ† Csâ†’B Sâ†’Cs Vâ†’Ca\n0.33 42.7 63.5 54.7\n0.50 42.4 63.7 55.3\n1.0 43.4 64.0 55.6\n2.0 43.0 64.0 53.3\n3.0 42.9 63.9 50.0\nA.2 Additional Experiments\nIn this section, we showcase further experiments, including: (1)\nCityscapes [ 13] to FoggyCityscapes [ 65], aiming to validate theTable 12: Ablation Result of Threshold ğœfor Pseudo Labels.\nğœ Csâ†’B Sâ†’Cs Vâ†’Ca\n0.3 41.9 63.7 52.4\n0.4 42.3 64.2 53.9\n0.5 43.4 64.0 55.6\n0.6 43.2 62.1 56.1\n0.7 43.0 60.3 53.2\nTable 13: Ablation Study of Data Augmentation.\nSettings of data Aug. Csâ†’B Sâ†’Cs Vâ†’Ca\nDDT(Ours) 43.4 64.0 55.6\nw/o Strong Aug. on Sup. 42.0 -1.4 62.0 -2.0 54.3 -1.3\nw/o Strong Aug. on Unsup 42.5 -0.9 62.2 -1.8 52.3 -3.3\nw/o All Aug. 42.0 -1.4 61.8 -2.2 52.1 -3.5\nTable 14: Ablation Result of Different Stable Diffusion Ver-\nsions.\nDetector Version Csâ†’B Sâ†’Cs Sâ†’B Vâ†’Ca Vâ†’Co Vâ†’W\nDiffusion DetectorSD-1.5 32.7 58.2 50.1 47.4 44.4 58.7\nSD-2.1 34.6 58.9 56.4 45.4 42.8 55.2\nDDTSD-1.5 43.4 64.0 58.3 55.6 50.2 63.7\nSD-2.1 42.3 63.4 61.6 53.7 48.9 63.3\nresults of adverse weather adaptation, and (2) the outcomes of our\nDDT method employing the FCOS [ 72] detector, aiming to assess\nthe performance of our approach across different detectors.\nAdverse Weather Adaptation from Cityscapes to FoggyCi-\ntyscapes. We present the result of adverse weather adaptation in\nTab. 16. Compared to the baseline, our method with R18, R50, and\nR101 show improvements of 11.8, 18.0, and 21.6 mAP, respectively.\nHowever, our method (50.0 mAP) still falls short of surpassing CMT\n[3] (50.3 mAP) and HT [15] (50.4 mAP). FoggyCityscapes [65] is a\ndataset where foggy weather conditions are added to Cityscapes,\nwith similar images and identical labels from Cityscapes. We ob-\nserve that our DDT method does not demonstrate superior perfor-\nmance in inter-domain as train and test on Cityscapes, which might\nbe the reason for our weaker performance on the FoggyCityscapes\ncompared to the current state-of-the-art results.\nResults of Adaptation with FCOS Detector. Previous do-\nmain adaptation methods for detection primarily employ the Faster\nRCNN and FCOS detectors. In main text, we report the results of our\nmethod using the Faster RCNN [ 11] detector. To further validate\nthe effectiveness of our approach, we also present the performance\nusing the FCOS [ 72] detector in Tab. 17, 18, 19, 20, 21, 22. Overall,\nthe FCOS detector yielded results comparable to the Faster RCNN\nand outperforms the previous SOTA results in five out of the six\ndatasets, with the exception of Sim10K to Cityscapes where it falls\nshort of HT [15].\n--- Page 13 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nCross Camera\nSynthetic to Real\nReal to Artist\nCityscapes\n(Source)BDD100k\n(Target)\nSim10K\n(Source)Cityscapes\n(Target)\nBDD100k\n(Target)\nVOC\n(Source)Clipart\n(Target)\nComic\n(Target)\nWatercolor\n(Target)Cross-domain Detection Benchmarks\nAdd Noise Add Noise\nDiffusion \nBackbone\nNoise \nSchedule\nSource \nDistribution\nTarget\nDistribution\nDiffusion Process\nUNetFeature \nFusion \nDiffusion\nTeacher Unlabeled\nTarget Data\nPseudo Labels Predictions\nUnsup. Loss\nStudent\nDiffusion Teacher Guided Self-training\nFeature Distribution of DDTDiffusion Teacher and Self-training Adaptation Cross-domain Performance\nCross Camera\nSynthetic to Real\nReal to Artist\nFigure 5: Main content of our work. Left: We present three cross-domain detection benchmarks and visualize the image\ndistributions from source and target domains using the UMAP method. It is evident that there is a large gap between different\ndomains. Middle: We utilize a frozen-weight diffusion model as the backbone to extract features, and employ a detector with\nthe diffusion backbone as the teacher to guide the learning of the student model on the target domain with the self-training\nframework. Based on the visualization results of the feature distributions, our method significantly reduces the domain gap.\nRight: Our method substantially enhances the performance of cross-domain object detection without increasing the inference\ncost.\nA.3 Additional visualization results\nIn Fig. 6, 7, 8, 9, 10, 11, we present additional visualization results\nfor Cityscapes to BDD100K ( Csâ†’B), Sim10K to BDD100K ( Sâ†’B),\nSim10K to Cityscapes ( Sâ†’Cs), VOC to Clipart ( Vâ†’Ca), VOC to\nComic ( Vâ†’Co), and VOC to Watercolor ( Vâ†’W).\n--- Page 14 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nTable 15: Results of the efficiency comparison of detectors with different backbones.\nMethods Params (M) Flops (G)Train Time\n(s/iter)Inference Time\n(ms/image)Average\nGain (mAP)Average Rel.\nImprove (%)\nConvNext -Base [52] 105 401 0.672 73.2 / /\nSwin -Base [51] 104 413 0.693 79.4 / /\nVIT -Base [16] 107 605 0.634 116.5 / /\nMAE (VIT-Base) [16, 22] 107 605 0.634 116.5 / /\nGLIP (Swin-Tiny) [36, 51] 45 198 0.486 84.1 / /\nDiff. (Ours) 991 8,256 1.56 780.4 / /\nDDT (R18) 28 137 0.82 24.6 19.0 38.9\nDDT (R50) 41 184 0.90 33.6 21.7 40.1\nDDT (R101) 60 262 1.04 40.9 22.9 40.2\nTable 16: Quantitative results on adaptation from Cityscapes to FoggyCityscapes. The bold indicates the best results.\nMethod Reference Detector bus bicycle car mcycle person rider train truck mAP\nUMT [14] CVPRâ€™21 FRCNN- R101 56.5 37.3 48.6 30.4 33.0 46.7 46.8 34.1 41.7\nIIOD [75] TPAMIâ€™21 FRCNN- V16 46.1 35.3 49.6 29.9 32.8 44.4 38.0 33.0 38.6\nSADA [12] IJCVâ€™21 FRCNN- R50 50.3 45.4 62.1 32.4 48.5 52.6 31.5 29.5 44.0\nCDG [37] AAAIâ€™21 FRCNN- V16 47.5 38.9 53.1 38.3 38.0 47.4 41.1 34.2 42.3\nUaDAN [21] TMMâ€™21 FRCNN- R50 49.4 38.9 53.6 32.3 36.5 46.1 42.7 28.9 41.1\nVDD [76] ICCVâ€™21 FRCNN- V16 52.0 36.8 51.7 34.2 33.4 44.0 34.7 33.9 40.0\nO2Net [19] ACMMMâ€™22 DDETR- R50 47.6 45.9 63.6 38.0 48.7 51.5 47.8 31.1 46.8\nSSAL [54] NeurIPSâ€™22 FCOS 50.0 38.7 59.4 26.0 45.1 47.4 25.7 24.5 39.6\nDDF [47] TMMâ€™22 FRCNN- R50 50.4 39.8 56.1 31.1 37.6 45.5 47.0 30.7 42.3\nD-ADAPT [31] ICLRâ€™22 FRCNN- R50 36.3 46.1 61.7 37.3 44.9 54.2 24.7 25.6 42.2\nSCAN [40] AAAIâ€™22 FCOS- R50 48.6 37.3 57.3 31.0 41.7 43.9 48.7 28.7 42.1\nSIGMA [41] CVPRâ€™22 FCOS- R50 50.4 40.6 60.3 31.7 44.0 43.9 51.5 31.6 44.2\nTIA [85] CVPRâ€™22 FRCNN- V16 52.1 38.1 49.7 37.7 34.8 46.3 48.6 31.1 42.3\nTDD [24] CVPRâ€™22 FRCNN- V16 53.0 49.1 68.2 38.9 50.7 53.7 45.1 35.1 49.2\nNLTE [49] CVPRâ€™22 FRCNN- R50 56.7 43.3 58.7 33.7 43.1 50.7 42.7 33.6 45.4\nLODS [38] CVPRâ€™22 FRCNN- V16 39.7 37.8 48.8 33.2 34.0 45.7 19.6 27.3 35.8\nPSN [70] CVPRâ€™22 FRCNN- V16 48.7 39.2 53.0 33.1 37.4 45.2 38.8 31.1 40.9\nMGA [88] CVPRâ€™22 FCOS- R101 53.2 36.9 61.5 27.9 43.1 47.3 50.3 30.2 43.8\nMTTrans [81] ECCVâ€™22 DDETR- R50 45.9 46.5 65.2 32.6 47.7 49.9 33.8 25.8 43.4\nOADA [79] ECCVâ€™22 FCOS- V16 48.5 39.8 62.9 34.3 47.8 46.5 50.9 32.1 45.4\nSCAN++ [40] TMMâ€™22 FCOS- R101 48.1 39.5 57.9 30.1 44.2 43.9 51.2 28.2 42.8\nMIC [26] CVPRâ€™23 FRCNN- R101 52.4 47.5 67.0 40.6 50.9 55.3 33.7 33.9 47.6\nSIGMA++ [42] TPAMIâ€™23 FRCNN- V16 52.2 39.9 61.0 34.8 46.4 45.1 44.6 32.1 44.5\nCIGAR [50] CVPRâ€™23 FCOS- V16 56.6 41.3 62.1 33.7 46.1 47.3 44.3 27.8 44.9\nCMT [3] CVPRâ€™23 FRCNN- V16 66.0 51.2 63.7 41.4 45.9 55.7 38.8 39.6 50.3\nHT[15] CVPRâ€™23 FCOS- V16 55.9 50.3 67.5 40.1 52.1 55.8 49.1 32.7 50.4\nBaseline/ FRCNN- R1838.6 31.3 45.6 26.1 37.6 45.6 13.9 17.6 32.0\nDDT(Ours) 49.4 44.0 59.0 36.3 47.9 56.5 27.8 30.0 43.8 +11.8\nBaseline/ FRCNN- R5039.1 32.0 42.2 23.8 36.4 44.6 14.7 19.7 31.6\nDDT(Ours) 53.2 51.5 63.8 44.1 50.3 59.3 41.7 33.1 49.6 +18.0\nBaseline/ FRCNN- R10135.7 31.9 41.6 23.8 34.9 41.9 5.7 19.7 29.4\nDDT(Ours) 53.5 52.2 64.2 43.5 50.9 60.0 42.4 33.6 50.0 +21.6\n--- Page 15 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nTable 17: Quantitative results on adaptation from Cityscapes to BDD100K (Cs â†’B) with FCOS. The bold indicates the best\nresults.\nMethod Reference Detector bicycle bus car mcycle person rider truck mAP\nDA-Faster [11] CVPRâ€™18 FRCNN -V16 22.4 18.0 44.2 14.2 28.9 27.4 19.1 24.9\nSWDA [64] CVPRâ€™19 FRCNN -V16 23.1 20.7 44.8 15.2 29.5 29.9 20.2 26.2\nSCDA [91] CVPRâ€™19 FRCNN -V16 23.2 19.6 44.4 14.8 29.3 29.2 20.3 25.8\nCRDA [77] CVPRâ€™20 FRCNN -R101 25.5 20.6 45.8 14.9 32.8 29.3 22.7 27.4\nSED [43] AAAIâ€™21 FRCNN -V16 25.0 23.4 50.4 18.9 32.4 32.6 20.6 29.0\nTDD [24] CVPRâ€™22 FRCNN -V16 28.8 25.5 53.9 24.5 39.6 38.9 24.1 33.6\nPT[10] ICMLâ€™22 FRCNN -V16 28.8 33.8 52.7 23.0 40.5 39.9 25.8 34.9\nEPM [27] ECCVâ€™20 FCOS -R101 20.1 19.1 55.8 14.5 39.6 26.8 18.8 27.8\nSIGMA [41] CVPRâ€™22 FCOS -R50 26.3 23.6 64.1 17.9 46.9 29.6 20.2 32.7\nSIGMA++ [42] TPAMIâ€™23 FRCNN -V16 27.1 26.3 65.6 17.8 47.5 30.4 21.1 33.7\nNSA [89] ICCVâ€™23 FRCNN -V16 / / / / / / / 35.5\nHT[15] CVPRâ€™23 FCOS -V16 38.0 30.6 63.5 28.2 53.4 40.4 27.4 40.2\nBaseline/ FCOS- R1818.7 12.6 49.0 11.0 40.1 23.4 14.5 24.2\nDDT(Ours) 35.7 26.2 63.2 24.3 53.5 35.7 27.0 37.9 +13.7\nBaseline/ FCOS- R5021.7 15.9 49.1 13.7 40.4 26.6 14.6 26.0\nDDT(Ours) 38.0 32.0 64.0 25.9 55.9 36.8 29.3 40.3 +14.3\nBaseline/ FCOS- R10127.0 16.4 51.4 14.7 44.0 28.8 21.2 29.1\nDDT(Ours) 37.9 36.1 64.5 30.8 56.9 38.7 31.8 42.4 +13.3\n--- Page 16 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nTable 18: Quantitative results on adaptation from Sim10K\nto BDD100K (Sâ†’B) with FCOS. The bold indicates the best\nresults.\nMethod Reference Detector mAP(car)\nSWDA [64] CVPRâ€™19 FRCNN -V16 42.9\nCDN [68] ECCVâ€™20 FRCNN -V16 45.3\nBaseline/ FCOS -R1836.5\nDDT(Ours) 56.0 +19.5\nBaseline/ FCOS -R5038.7\nDDT(Ours) 56.2 +17.5\nBaseline/ FCOS -R10136.5\nDDT(Ours) 57.4 +20.9\nTable 19: Quantitative results on adaptation from Sim10K to\nCityscapes (Sâ†’Cs) with FCOS. The bold indicates the best\nresults.\nMethod Reference Detector mAP(car)\nDA-Faster [11] CVPRâ€™18 FRCNN -V16 39.0\nSWDA [64] CVPRâ€™19 FRCNN -V16 40.7\nHTCN [7] CVPRâ€™20 FRCNN -R101 42.5\nUMT [14] CVPRâ€™21 FRCNN -R101 43.1\nSSAL [54] NeurIPSâ€™22 FCOS -R50 51.8\nO2NET [19] ACMMMâ€™22 DDETR -R50 54.1\nDDF [47] TMMâ€™22 FRCNN -R50 44.3\nD-ADAPT [31] ICLRâ€™22 FRCNN -R50 51.9\nSCAN [40] AAAIâ€™22 FCOS -V16 52.6\nMTTrans [81] ECCVâ€™22 DDETR -R50 57.9\nSIGMA [41] CVPRâ€™22 FCOS -R50 53.7\nTDD [42] CVPRâ€™22 FRCNN -V16 53.4\nMGA [88] CVPRâ€™22 FCOS -R101 54.1\nOADA [79] ECCVâ€™22 FCOS -V16 59.2\nSIGMA++ [42] TPAMIâ€™23 FCOS -V16 53.7\nCIGAR [50] CVPRâ€™23 FCOS -V16 58.5\nNSA [89] ICCVâ€™23 FRCNN -V16 56.3\nHT[15] CVPRâ€™23 FRCNN -V16 65.5\nBaseline/ FCOS -R1847.0\nDDT(Ours) 61.4 +14.4\nBaseline/ FCOS -R5048.4\nDDT(Ours) 62.5 +14.1\nBaseline/ FCOS -R10151.5\nDDT(Ours) 63.5 +12.0Table 20: Quantitative results on adaptation from VOC to\nComic (Vâ†’Co) with FCOS. The bold indicates the best results.\nMethod Reference Detector bicycle bird car cat dog person mAP\nDA-Faster [11] CVPRâ€™18 FRCNN -V16 31.1 10.3 15.5 12.4 19.3 39.0 21.2\nSWDA [64] CVPRâ€™19 FRCNN -V16 36.4 21.8 29.8 15.1 23.5 49.6 29.4\nSTABR [33] CVPRâ€™19 SSD -V16 50.6 13.6 31.0 7.5 16.4 41.4 26.8\nMCRA [86] ECCVâ€™20 FRCNN -V16 47.9 20.5 37.4 20.6 24.5 50.2 33.5\nI3Net [8] CVPRâ€™21 SSD -V16 47.5 19.9 33.2 11.4 19.4 49.1 30.1\nDBGL [5] ICCVâ€™21 FRCNN -R101 35.6 20.3 33.9 16.4 26.6 45.3 29.7\nD-ADAPT [31] ICLRâ€™22 FRCNN -R101 52.4 25.4 42.3 43.7 25.7 53.5 40.5\nBaseline/ FCOS- R1817.7 7.1 8.3 2.4 5.7 25.3 11.1\nDDT(Ours) 54.2 26.6 45.5 29.8 34.4 72.2 43.8 +32.7\nBaseline/ FCOS- R5020.9 7.2 11.3 4.7 7.9 27.0 13.2\nDDT(Ours) 55.1 32.2 51.4 33.8 38.7 74.6 47.6 +34.4\nBaseline/ FCOS- R10126.0 9.5 15.4 7.3 8.0 29.3 15.9\nDDT(Ours) 55.6 38.2 55.6 36.6 48.1 75.9 51.6 +35.7\nTable 21: Quantitative results on adaptation from VOC to\nWatercolor (Vâ†’W) with FCOS. The bold indicates the best\nresults.\nMethod Reference Detector bicycle bird car cat dog person mAP\nSWDA [11] CVPRâ€˜19 FRCNN -V16 82.3 55.9 46.5 32.7 35.5 66.7 53.3\nMCRA [87] ECCVâ€˜20 FRCNN -V16 87.9 52.1 51.8 41.6 33.8 68.8 56.0\nUMT [14] CVPRâ€™21 FRCNN -R101 88.2 55.3 51.7 39.8 43.6 69.9 58.1\nIIOD [75] TPAMIâ€˜21 FRCNN -V16 95.8 54.3 48.3 42.4 35.1 65.8 56.9\nI3Net [8] CVPRâ€™21 SSD -V16 81.1 49.3 46.2 35.0 31.9 65.7 51.5\nSADA [12] IJCVâ€˜21 FRCNN -R50 82.9 54.6 52.3 40.5 37.7 68.2 56.0\nCDG [37] AAAIâ€™21 FRCNN -V16 97.7 53.1 52.1 47.3 38.7 68.9 59.7\nVDD [76] ICCVâ€˜21 FRCNN -V16 90.0 56.6 49.2 39.5 38.8 65.3 56.6\nDBGL [5] ICCVâ€™21 FRCNN -R101 83.1 49.3 50.6 39.8 38.7 61.3 53.8\nAT[44] CVPRâ€™22 FRCNN -V16 93.6 56.1 58.9 37.3 39.6 73.8 59.9\nLODS [38] CVPRâ€™22 FRCNN -R101 95.2 53.1 46.9 37.2 47.6 69.3 58.2\nBaseline/ FCOS- R1869.8 34.9 37.8 23.3 16.0 48.7 38.4\nDDT(Ours) 80.3 60.1 52.5 42.4 34.3 75.8 57.6 +19.2\nBaseline/ FCOS- R5066.9 42.4 44.6 21.5 13.7 48.3 39.6\nDDT(Ours) 94.4 63.1 51.8 40.8 34.3 75.9 60.1 +20.5\nBaseline/ FCOS- R10164.0 44.3 41.8 25.7 21.5 53.6 41.8\nDDT(Ours) 96.9 65.6 55.4 49.6 40.5 77.2 64.2 +22.4\n--- Page 17 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nTable 22: Quantitative results on adaptation from VOC to Clipart (V â†’Ca) with FCOS. The bold indicates the best results.\nMethod Reference Detector aero bcycle bird boat bottle bus car cat chair cow table dog horse bike psn plant sheep sofa train tv mAP\nSWDA [64] CVPRâ€™19 FRCNN -V16 26.2 48.5 32.6 33.7 38.5 54.3 37.1 18.6 34.8 58.3 17.0 12.5 33.8 65.5 61.6 52.0 9.3 24.9 54.1 49.1 38.1\nCRDA [77] CVPRâ€™20 FRCNN -R101 28.7 55.3 31.8 26.0 40.1 63.6 36.6 9.4 38.7 49.3 17.6 14.1 33.3 74.3 61.3 46.3 22.3 24.3 49.1 44.3 38.3\nHTCN [7] CVPRâ€™20 FRCNN -R101 33.6 58.9 34.0 23.4 45.6 57.0 39.8 12.0 39.7 51.3 21.1 20.1 39.1 72.8 63.0 43.1 19.3 30.1 50.2 51.8 40.3\nSAPNet [35] ECCVâ€™20 FRCNN -R101 27.4 70.8 32.0 27.9 42.4 63.5 47.5 14.3 48.2 46.1 31.8 17.9 43.8 68.0 68.1 49.0 18.7 20.4 55.8 51.3 42.2\nUMT [14] CVPRâ€™21 FRCNN -R101 39.6 59.1 32.4 35.0 45.1 61.9 48.4 7.5 46.0 67.6 21.4 29.5 48.2 75.9 70.5 56.7 25.9 28.9 39.4 43.6 44.1\nIIOD [75] TPAMIâ€™21 FRCNN -V16 41.5 52.7 34.5 28.1 43.7 58.5 41.8 15.3 40.1 54.4 26.7 28.5 37.7 75.4 63.7 48.7 16.5 30.8 54.5 48.7 42.1\nSADA [12] IJCVâ€™21 FRCNN -R50 29.4 56.8 30.6 34.0 49.5 50.5 47.7 18.7 48.5 64.4 20.3 29.0 42.3 84.1 73.4 37.4 20.5 39.8 41.2 48.0 43.3\nUaDAN [21] TMMâ€™21 FRCNN -R50 35.0 72.7 41.0 24.4 21.3 69.8 53.5 2.3 34.2 61.2 31.0 29.5 47.9 63.6 62.2 61.3 13.9 7.6 48.6 23.9 40.2\nDBGL [5] ICCVâ€™21 FRCNN -R50 28.5 52.3 34.3 32.8 38.6 66.4 38.2 25.3 39.9 47.4 23.9 17.9 38.9 78.3 61.2 51.7 26.2 28.9 56.8 44.5 41.6\nAT[44] CVPRâ€™22 FRCNN -V16 33.8 60.9 38.6 49.4 52.4 53.9 56.7 7.5 52.8 63.5 34.0 25.0 62.2 72.1 77.2 57.7 27.2 52.0 55.7 54.1 49.3\nD-ADAPT [31] ICLRâ€™22 FRCNN -R50 56.4 63.2 42.3 40.9 45.3 77.0 48.7 25.4 44.3 58.4 31.4 24.5 47.1 75.3 69.3 43.5 27.9 34.1 60.7 64.0 49.0\nTIA [85] CVPRâ€™22 FRCNN -R101 42.2 66.0 36.9 37.3 43.7 71.8 49.7 18.2 44.9 58.9 18.2 29.1 40.7 87.8 67.4 49.7 27.4 27.8 57.1 50.6 46.3\nLODS [38] CVPRâ€™22 FRCNN -R101 43.1 61.4 40.1 36.8 48.2 45.8 48.3 20.4 44.8 53.3 32.5 26.1 40.6 86.3 68.5 48.9 25.4 33.2 44.0 56.5 45.2\nCIGAR [50] CVPRâ€™23 FCOS -R101 35.2 55.0 39.2 30.7 60.1 58.1 46.9 31.8 47.0 61.0 21.8 26.7 44.6 52.4 68.5 54.4 31.3 38.8 56.5 63.5 46.2\nCMT [3] CVPRâ€™23 FRCNN -V16 39.8 56.3 38.7 39.7 60.4 35.0 56.0 7.1 60.1 60.4 35.8 28.1 67.8 84.5 80.1 55.5 20.3 32.8 42.3 38.2 47.0\nBaseline/ FCOS- R1818.7 26.0 15.0 10.1 19.5 65.6 30.6 1.8 24.3 4.2 24.1 7.9 24.9 42.1 33.5 26.1 0.2 17.2 23.0 11.2 21.3\nDDT(Ours) 48.9 58.9 32.3 30.0 42.4 72.4 54.5 11.2 48.6 38.9 30.2 27.5 40.1 87.7 76.0 53.2 33.5 38.8 49.5 47.1 46.1 +24.8\nBaseline/ FCOS- R5040.0 26.7 17.8 21.0 31.9 32.2 28.8 12.2 36.3 35.7 28.3 6.1 25.5 43.1 37.2 33.5 5.1 25.6 24.5 26.3 26.9\nDDT(Ours) 50.7 53.1 34.1 41.5 57.0 86.3 57.1 9.3 49.5 52.8 33.6 32.4 49.0 93.1 82.1 57.8 37.1 42.6 54.1 60.8 51.7 +24.8\nBaseline/ FCOS- R10133.6 42.3 21.2 20.1 32.9 62.0 30.0 14.5 41.1 17.9 33.0 9.1 30.4 46.5 39.1 37.4 8.8 22.6 27.3 16.1 29.3\nDDT(Ours) 58.6 73.2 42.0 48.0 54.7 84.7 65.2 17.0 55.9 49.4 35.5 40.5 58.6 84.8 82.9 58.0 39.1 41.7 54.7 61.3 55.3 +26.0\n--- Page 18 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 6: Qualitative prediction results and feature visualization of baseline and our DDT from Cityscapes to BDD100K.\n--- Page 19 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 7: Qualitative prediction results and feature visualization of baseline and our DDT from Sim10K to BDD100K.\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 8: Qualitative prediction results and feature visualization of baseline and our DDT from Sim10K to Cityscapes.\n--- Page 20 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 9: Qualitative prediction results and feature visualization of baseline and our DDT from VOC to Clipart.\n--- Page 21 ---\nDiffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 10: Qualitative prediction results and feature visualization of baseline and our DDT from VOC to Comic.\n--- Page 22 ---\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Boyong He, Yuxiang Ji, Zhuoyue Tan, and Liaoni Wu.\nGround Truth Visualization of baseline Predictions of baseline Visualization of our DDT Predictions of ours DDT\nFigure 11: Qualitative prediction results and feature visualization of baseline and our DDT from VOC to Watercolor.",
  "text_length": 88798
}