{
  "id": "http://arxiv.org/abs/2505.24844v1",
  "title": "Chameleon: A Flexible Data-mixing Framework for Language Model\n  Pretraining and Finetuning",
  "summary": "Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon.",
  "authors": [
    "Wanyun Xie",
    "Francesco Tonin",
    "Volkan Cevher"
  ],
  "published": "2025-05-30T17:43:10Z",
  "updated": "2025-05-30T17:43:10Z",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24844v1"
}