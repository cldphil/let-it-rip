{
  "id": "http://arxiv.org/abs/2505.24844v1",
  "title": "Chameleon: A Flexible Data-mixing Framework for Language Model\n  Pretraining and Finetuning",
  "summary": "Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon.",
  "authors": [
    "Wanyun Xie",
    "Francesco Tonin",
    "Volkan Cevher"
  ],
  "published": "2025-05-30T17:43:10Z",
  "updated": "2025-05-30T17:43:10Z",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24844v1",
  "full_text": "arXiv:2505.24844v1 [cs.LG] 30 May 2025CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Wanyun Xie1Francesco Tonin1Volkan Cevher1 Abstract Training data mixtures greatly impact the gen- eralization performance of large language mod- els. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, CHAMELEON, that employs leverage scores to quantify domain importance within a learned embedding space. We first con- struct a domain affinity matrix over domain em- beddings. The induced leverage scores determine a mixture that upweights domains sharing com- mon representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In ex- periments, we demonstrate improvements over three key scenarios: (i) our computed weights im- prove performance on pretraining domains with a fraction of the compute of existing methods; (ii)CHAMELEON can adapt to data changes with- out proxy retraining, boosting few-shot reason- ing accuracies when transferred to new data; (iii) our method enables efficient domain reweight- ing in finetuning, consistently improving test per- plexity on all finetuning domains over uniform mixture. Our code is available at https:// github.com/LIONS-EPFL/Chameleon. 1. Introduction Pretraining large language models (LLMs) relies heavily on vast and diverse datasets, encompassing sources such as academic papers, books, and code repositories (Brown et al., 2020; Dubey et al., 2024). The composition of these datasets 1Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. Corre- spondence to: Wanyun Xie <wanyun.xie@epfl.ch>. Proceedings of the 42ndInternational Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).significantly influences the generalization capabilities and downstream performance of LLMs. Domain reweighting, which involves adjusting the relative contributions of different domains in the training dataset, has emerged as a critical aspect of LLM training (Gao et al., 2020; Du et al., 2022). However, obtaining optimal domain weights is a challenging problem due to factors such as data quality, diversity, inter-domain overlap, and task-specific complexities (Shen et al., 2023; Longpre et al., 2024). Early domain reweighting methods relied on manual selec- tion, often favoring high-quality sources like Wikipedia and academic texts (Brown et al., 2020; Gao et al., 2020). While intuitive, these approaches are neither optimal nor scalable. Recent work explores computational strategies for optimiz- ing domain mixtures, such as DoReMi (Xie et al., 2023) and DoGE (Fan et al., 2024b), which use a small proxy model to derive domain weights for training a larger base model. Though effective, these methods are computationally expen- sive and have limited practical applicability. We contend that an ideal data-mixing method should ( i) improve universal generalization, the fundamental goal of domain reweighting; ( ii) adapt to domain modifications – data naturally evolves between preparation and LLM train- ing, making frequent recalibration impractical; ( iii) handle different training stages such as pertaining and fine-tuning. Most existing methods are limited to pretraining scenarios and do not consider either domain changes or the fine-tuning stage where domain specificity often plays a larger role. In this work, we introduce CHAMELEON, a novel and effi- cient framework for data mixing that addresses these chal- lenges. Our method computes domain weights directly from learned embeddings using kernel ridge leverage scores (KRLS), which quantify the importance of each domain based on its contribution to the overall embedding space. Unlike existing approaches, CHAMELEON reduces the data- mixing compute and eliminates the need for frequent re- training of proxy models when domains change. Instead, it constructs a domain affinity matrix to capture relationships between domains and computes leverage scores that guide domain reweighting. Our data-centric approach enables 1 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning seamless adaptation to new data and flexibility across both pretraining and fine-tuning stages. Specifically, our contributions are summarized as follows: •We propose CHAMELEON, an efficient data-mixing framework that leverages KRLS to quantify do- main representativeness from embedded data. In- verse KRLS-based domain weights emphasize general knowledge for pertaining. We empirically demonstrate thatCHAMELEON matches DoReMi and DoGE in pre- training performance at a fraction of their cost. •As a data-centric method, CHAMELEON can flexibly adapt to changes in domain composition without re- training proxy models, enhancing its practicality in real scenarios. It outperforms baselines at 1% of the retraining cost, even as domains double. •We extend domain reweighting to fine-tuning, where KRLS-based weights emphasize domain-specific uniqueness. Empirical results show consistent perplex- ity improvements across all domains on both natural language and code datasets. From a practical standpoint, CHAMELEON significantly lowers the computational burden associated with domain reweighting, making advanced LLM training pipelines more accessible to researchers with limited resources. Indeed, our KRLS-based scores are computationally inex- pensive, hyperparameter-robust, and converge quickly. This efficiency is particularly advantageous when incorporating new data, where our method can be applied directly without re-running the entire proxy pipeline. By bridging the gap between pretraining and fine-tuning, our method provides a unified and agile framework for efficient as well as effective data mixing across all stages of LM training. 2. Related Work Domain Reweighting. Domain reweighting improves LLM pretraining by balancing data contributions from dif- ferent sources. In this setting, online adaptation strategies require frequent recalibration and monitoring (Albalak et al., 2023; Jiang et al., 2024; Fan et al., 2024a). Two closely related approaches are DoReMi (Xie et al., 2023) and DoGE (Fan et al., 2024b). DoReMi trains both a reference and a proxy model using Group DRO (Sagawa et al., 2020) to mitigate excess domain loss, while DoGE tracks domain-specific gradients during proxy training. Both methods are inefficient: DoReMi depends on the refer- ence model’s quality and requires training two models, while DoGE incurs high gradient tracking costs. Their domain weights fluctuate significantly during training (Figure 3).Table 1: Data-mixing methods capabilities comparison. DoReMi DoGE CHAMELEON Generalization ✓ ✓ ✓ Downstream ✓ ✓ ✓ New data ✗ ✗ ✓ Finetuning ✗ ✗ ✓ In this setting, our work develops an offline method that achieves uniformly strong performance across domains with- out relying on downstream task knowledge. Other offline methods, such as Data Mixing Laws (Ye et al., 2024) and RegMix that–in contrast to ours–requires access to the down- stream tasks (Liu et al., 2024), use multiple proxy models to search for optimal data mixtures, revealing that domain weights transfer across model sizes. Additionally, studies on data and model scaling laws provide further insights into domain weighting strategies (Kang et al., 2024; Ye et al., 2024). However, they critically rely on the scaling strategy and do not have an easy way of adapting to domain expansion. Kernel Ridge Leverage Scores (KRLS). The notion of statistical leverage score (Gareth et al., 2013) is used in best- rank approximation to define an importance score for the rows in a matrix by their influence on the optimal low-rank approximation (Mahoney & Drineas, 2009), with approxi- mation error guarantees for sampling (Li et al., 2013). Ridge leverage scores are also proposed with additional regularization term (Cohen et al., 2015; 2017; Rudi et al., 2018). Leverage scores are extended to the kernel setting in (Bach, 2013), namely kernel ridge leverage scores. KRLS sampling is extensively used to accelerate kernel methods (Alaoui & Mahoney, 2015; Musco & Musco, 2017; Rudi et al., 2017; 2018). Inverse KRLS have been related to Christoffel functions (Pauwels et al., 2018), which in machine learning are used for, e.g., landmark sampling (Fanuel et al., 2022), density estimation (Pauwels et al., 2018), and outlier detection (Lasserre & Pauwels, 2019; Beckermann et al., 2021; Ducharlet et al., 2024). 3. Data Mixing via CHAMELEON Setup. We consider a dataset D={D1,..., D k}consist- ing of kdistinct domains, each represented by its metadata (e.g., source, topic). Our objective is to determine a domain weight vector α∈∆k, where ∆kis the probability simplex, enhancing the performance of LMs. Following prior work (Xie et al., 2023; Fan et al., 2024b), we adopt a two-stage strategy: (1) training a small proxy model to infer domain weights and (2) training a large base model using the com- 2 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning 010\u00005051042\u000017\u000015\u00001463\u00005\u000017271215\u000021\u0000161\u000015122419\u000020\u000013\u00006\u000014151922\u000022\u000017\u000026\u000021\u000020\u0000224631\u0000193\u000016\u000013\u0000173129\u000017\u000051\u00006\u00002\u000019\u00001749ArxivBookCCC4GithubStackWiki.00.03.05.08.10KRLSS\u0000PretrainingDomain Weights:↵PTi=softmax(S\u00001\u0000)Transfer to New DataDomain Weights:↵NDi=softmax(S\u00001\u0000)FinetuningDomain Weights:↵FTi=softmax(S\u0000)Learn Domain Embeddings1Compute Domain Afﬁnity2Compute KRLS Scores3Train base LMs4 Figure 1: Pipeline of domain reweighting via CHAMELEON. The given data is ﬁrst embedded through the proxy model, previously trained on a corpus Dwith uniform weights. Domain embeddings are then determined by averaging the embeddings for each domain. The domain afﬁnity matrix ⌦Dis computed as the pairwise inner products between domain embeddings. Finally, (KRLS )is applied to ⌦Dto obtain score S\u0000indicating the degree of inter-domain independency. A resampling non-uniform distribution ↵is obtained by softmax normalizing the scores. Finally, the target base language model is trained with the obtained data mixture, where the inverse KRLS S\u00001 \u0000is used during pretraining of initial or new data to promote general knowledge learning and the KRLS S\u0000is used during ﬁnetuning to emphasize task-speciﬁc knowledge. We approach the data mixture problem from a data-centric perspective. Unlike prior works such as DoReMi ( Xie et al., 2023 ) and DoGE ( Fan et al.,2024b ), which derive domain weights based on the optimization process of a proxy model, we focus instead on the intrinsic properties of the data itself. To characterize domain characteristics, we extract embed- dings from hidden layers of the proxy model. These embed- dings encapsulate rich semantic and structural information about the input data in a continuous, high-dimensional space. As a result, the embeddings not only represent the individual domains but also capture their inter-domain relationships. Figure 4presents a two-dimensional UMAP ( McInnes et al., 2018 ) projection of mid-layer embeddings derived from the SlimPajama dataset using a proxy language model. The visualization highlights the following key characteristics: (i) semantic distinctiveness: similar domains cluster closely, while unique domains stand apart; ( ii) centrality and cov- erage: broad domains like \"CC\" and \"C4\" create dominant regions, covering shared semantic space, while more spe- ciﬁc domains like \"Arxiv\" are more distinct. Our observations lead to two central questions: 1.How can we precisely quantify domain characteristics? 2.How can such properties inform domain reweighting? We tackle these questions in the sequel. 3.1. Quantifying domain characteristics We ﬁrst introduce domain embeddings capturing domain- level characteristics, representing each domain Dias theembedding vector xi2Rp, with pbeing the embedding dimension. This embedding is computed by averaging the LM embeddings of data points in the domain: xi=1 |Di|X a2Dih(L) ✓p(a), where h(L) ✓p(a)denotes the L-th layer embedding of the proxy model h✓p. In practice, using a sufﬁciently large, randomly sampled batch Bi✓Diprovides a robust approximation of the domain embedding and is used in experiments. The proxy is trained on Dwith uniform domain weights, i.e., ↵i=1 /k, following ( Xie et al., 2023;Fan et al.,2024b ). We deﬁne the resulting domain embedding matrix as X=[x1,...,x k]>2Rk⇥p. To quantify properties across domains, we exploit Kernel Ridge Leverage Scores (KRLS) ( Alaoui & Mahoney,2015 ). KRLS measure the contribution of each domain to the over- all embedding space. First, we deﬁne a kernel function (xi,xj)= x> ixj, which captures the similarity between domains DiandDj. Using this kernel, we construct the domain afﬁnity matrix: ⌦D=[(xi,xj)]k i,j=1=XX>. The domain afﬁnity matrix ⌦Dcaptures pairwise rela- tionships between domains, with higher values indicating a higher degree of semantic similarity. Note that we employ the linear kernel as the LM itself already introduces signiﬁcant non-linearity. An example domain afﬁnity matrix is visualized in Figure 5. While ⌦Dcaptures 3 Figure 1: Pipeline of domain reweighting via CHAMELEON. The given data is first embedded through the proxy model, previously trained on a corpus Dwith uniform weights. Domain embeddings are then determined by averaging the embeddings for each domain. The domain affinity matrix ΩDis computed as the pairwise inner products between domain embeddings. Finally, (KRLS) is applied to ΩDto obtain score Sλindicating the degree of inter-domain independency. A resampling non-uniform distribution αis obtained by softmax normalizing the scores. Finally, the target base language model is trained with the obtained data mixture, where the inverse KRLS S−1 λis used during pretraining of initial or new data to promote general knowledge learning and the KRLS Sλis used during finetuning to emphasize task-specific knowledge. puted weights. Note that many studies have empirically shown that domain weights transfer across different model sizes (Xie et al., 2023; Fan et al., 2024b; Liu et al., 2024). We approach the data mixture problem from a data-centric perspective. Unlike prior works such as DoReMi (Xie et al., 2023) and DoGE (Fan et al., 2024b), which derive domain weights based on the optimization process of a proxy model, we focus instead on the intrinsic properties of the data itself. To characterize domain characteristics, we extract embed- dings from hidden layers of the proxy model. These embed- dings encapsulate rich semantic and structural information about the input data in a continuous, high-dimensional space. As a result, the embeddings not only represent the individual domains but also capture their inter-domain relationships. Figure 1 ➀presents a two-dimensional UMAP (McInnes et al., 2018) projection of mid-layer embeddings derived from the SlimPajama dataset using a proxy language model. The visualization highlights the following key characteris- tics: ( i) semantic distinctiveness: similar domains cluster closely, while unique domains stand apart; ( ii) centrality and coverage: broad domains like \"CC\" and \"C4\" create dominant regions, covering shared semantic space, while more specific domains like \"Arxiv\" are more distinct. Our observations lead to two central questions: 1.How can we precisely quantify domain characteristics? 2.How can such properties inform domain reweighting? We tackle these questions in the sequel.3.1. Quantifying domain characteristics We first introduce domain embeddings capturing domain- level characteristics, representing each domain Dias the embedding vector xi∈Rp, with pbeing the embedding dimension. This embedding is computed by averaging the LM embeddings of data points in the domain: xi=1 |Di|X a∈Dih(L) θp(a), where h(L) θp(a)denotes the L-th layer embedding of the proxy model hθp. In practice, using a sufficiently large, randomly sampled batch Bi⊆Diprovides a robust approximation of the domain embedding and is used in experiments. The proxy is trained on Dwith uniform domain weights, i.e., αi= 1/k, following (Xie et al., 2023; Fan et al., 2024b). We define the resulting domain embedding matrix as X= [x1,..., x k]⊤∈Rk×p. To quantify properties across domains, we exploit Kernel Ridge Leverage Scores (KRLS). KRLS is a well-established tool in data analysis quantifying the influence or importance of data points (Alaoui & Mahoney, 2015). KRLS measure the contribution of each domain to the overall embedding space. First, we define a kernel function κ(xi, xj) =x⊤ ixj, which captures the similarity between domains DiandDj. Using this kernel, we construct the domain affinity matrix: ΩD= [κ(xi, xj)]k i,j=1=XX⊤. The domain affinity matrix ΩDcaptures pairwise rela- tionships between domains, with higher values indicating a higher degree of semantic similarity. Note that we 3 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning employ the linear kernel as the LM itself already introduces significant non-linearity. An example domain affinity matrix is visualized in Figure 1 ➁. While ΩDcaptures inter-domain relationships, it does not directly provide a measure of individual domain importance for data mixing. To address this, we propose to employ KRLS defined on ΩD to quantify the influence of each domain within the overall embedding space. We compute the scores as defined below. Definition 3.1 (Domain KRLS).For a given regularization parameter λ >0, the KRLS for domain Diis defined as: Sλ(Di) = [Ω D(ΩD+kλI)−1]ii, (KRLS) where Iis the k×kidentity matrix. The KRLS Sλ(Di)quantifies the importance of domain Di in the embedding space. Specifically, these scores corre- spond to the diagonal entries of the hatmatrix ΩD(ΩD+ kλI)−1of Kernel Ridge Regression (KRR) (Hastie et al., 2005). They are proportional to the weights assigned to each domain in the dual KRR estimator, and they depend only on the inputs xiand constant λindependently of specific target values (Calandriello et al., 2016; Chen & Yang, 2021). Inputs with higher KRLS indicate higher contribution to the KRR estimator, i.e., they are more unique in the kernel representation. More discussions on the KRLS and the role of regularization are provided in Appendix A.1 and A.2, respectively. In data mixing, a high KRLS value for domain Diindicates that its embedding xicannot be well-approximated as a combination of embeddings from other domains, implying thatDiis relatively distinct or unique in its characteristics. Conversely, a low KRLS value suggests that Diis highly well-represented, as it can be readily reconstructed from other domain embeddings, representing broader or more widely shared characteristics across domains. 3.2. Incorporating KRLS into LM training An essential question is thus: Which domains should be pri- oritized: the general ones with higher degree of dependency with others or independent ones with more unique charac- teristics? Prior work suggests that data mixing strategies should adapt to different training phases (Ma et al., 2023; Feng et al., 2024). Therefore, we address this by consider- ing pretraining and fine-tuning separately, as their objectives differ fundamentally (Parthasarathy et al., 2024). Remark 3.2 (Theoretical motivation).Before introducing our weights, we provide a theoretical motivation for our approach. It is well established that the inverse KRLS S−1 λ is proportional to the Christoffel function (Pauwels et al., 2018), which is a common measure of density of the data in embedding space. Christoffel functions precisely charac- terize the local density of the data distribution in the featureAlgorithm 1 Domain Weighting via C HAMELEON. 1:Input: Training data from kdomains D= {D1,..., D k}, regularization parameter λ, embedding layer index L. 2:ifPretraining then 3: Train proxy hθp(a)with uniform weights αi=1 k. 4:end if 5:Extract domain embeddings: xi=1 |Di|P a∈Dih(L) θp(a) for each domain Di. 6:Construct the feature matrix: X= [x⊤ 1,..., x⊤ k]. 7:Compute the domain affinity matrix: ΩD=XX⊤. 8:Compute KRLS Sλ(Di)for each domain Diusing ΩD. 9:ifPretraining then 10: Domain weights αPT i=exp(S−1 λ(Di))Pk j=1exp(S−1 λ(Dj)). 11:else if Fine-Tuning then 12: Domain weights αFT i=exp(Sλ(Di))Pk j=1exp(Sλ(Dj)). 13:end if 14:Output: Domain weights αPTorαFT. space, where higher values indicate denser regions. Detailed remarks are provided in Appendix A.2. Pretraining. During pretraining, assigning higher sam- pling probability to domains with high S−1 λupweights high- density data regions, which are most influential on base LMs’ performance (Mallen et al., 2023; Feng et al., 2024). To achieve this, we determine domain weights using the inverse of KRLS: αPT i=exp\u0000 S−1 λ(Di)\u0001 Pk j=1exp\u0000 S−1 λ(Dj)\u0001, where we convert the scores S−1 λinto the probability distri- bution αPTby appliying softmax normalizationexp(·)Pk j=1exp(·). Importantly, the domain weights obtained by CHAMELEON focus on the intrinsic properties of the data and our method does not affect the proxy model’s training process. This al- lows to compute importance weights αND iofnew domains directly without requiring retraining of the proxy model by applying it to the new data. The proxy is used to obtain the new domain embeddings, from which the new (KRLS) score is calculated. In contrast, existing data mixture methods couple domain reweighting with the proxy model’s optimization, necessitating costly retraining whenever domains are added. This dependency not only increases computational overhead but also contradicts the goal of im- proving efficiency in large-scale dynamic training pipelines. Finetuning. Finetuning aims to specialize on a novel spe- cific task (Yang et al., 2024), requiring the model to learn differential features not fully captured during pretraining, 4 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning so we instead prioritize the domains with high KRLS Sλ: αFT i=exp (Sλ(Di)) Pk j=1exp (Sλ(Dj)). Note that the key difference between finetuning and pretrain- ing in the data mixture problem is that, during finetuning, we do not need to train a separate proxy model. Instead, we directly use the pre-trained model for finetuning. This allows us to extract the embeddings from the pre-trained model for the relevant domains, which are then used to compute the domain weights. Algorithm and complexity analysis. The overall domain reweighting process is summarized in Algorithm 1. Our phase-specific strategy ensures that it well adapts to the differing demands of pretraining and fine-tuning. Obtaining embeddings xirequires a single forward pass for each sample a∈Dithrough the proxy; inference is fast as the proxy is a small model. Given the typically small number of domains k, the KRLS computation in Definition 3.1 is cheap in O(k3). We do not add any overhead in proxy training. Our approach is therefore efficient and contrasts with prior methods requiring domain-specific iterative optimization (Xie et al., 2023; Fan et al., 2024b). 4. Experiments We show CHAMELEON improves the model’s performance through data mixture with less computational costs (Sec- tion 4.1). In addition, CHAMELEON is scalable and can be applied to larger datasets without the need to retrain a proxy model (Section 4.2). Moreover, it can easily applied to fine-tuning tasks (Section 4.3). 4.1. Universal Generalization Considering universal generalization, the main goal is to im- prove the general performance of the model across domains in the training set and also in various downstream tasks. For the performance on the in-distribution data, we measure the perplexity across all domains. For downstream tasks, we follow RegMix (Liu et al., 2024) selecting 13 tasks that cover various realistic scenarios. Training setup. We experiment on the SlimPajama-627B dataset (Soboleva et al., 2023) consisting of 7 data domains. We choose Uniform with uniform domain weights, DoReMi, and DoGE as our baselines, which are downstream task agnostic offline methods same as CHAMELEON. We include RegMix as an additional reference, as it instead leverages prior knowledge of downstream tasks. Specifically, RegMix optimizes domain weights using the validation loss of the domain most correlated with downstream performance, identified as “CC” in their work (Liu et al., 2024). Arxiv BookCC C4 Github StackExchangeWikipedia.00.05.10.15.20.25Domain WeightDoReMi DoGE CHAMELEONFigure 2: Domain weights on SlimPajama. We compare weights computed by data-mixing methods on SlimPajama. Table 4: GPU hours for obtaining domain weight. DoReMi DoGE C HAMELEON 684M base model 7.4h 6.3h 0.8h 56h Following DoGE (Fan et al., 2024b), we use a small 82M decoder-only transformer (Vaswani et al., 2017) as the aux- iliary models for CHAMELEON, DoReMi, and DoGE. Aux- iliary models for both DoGE and DoReMi are trained for 10k iterations, and the proxy model of CHAMELEON is only trained for 2k steps and we use 4k samples for embedding computation per domain. Detailed training setup is demon- strated in Appendix B.2. Domain weight obtained through different methods is reported in Figure 2. Through train- ing small auxiliary models, domain weights are obtained to train larger base models with the size of 684M. Note that we employ the simple, linear kernel κ(xi, xj) =x⊤ ixj, which does not need further kernel hyperparameter tuning. Perplexity and computational cost. Table 2 shows per- domain perplexity on the held-out test dataset for 684M base models. CHAMELEON outperforms Uniform and DoReMi, achieving similar performance to DoGE but with significantly lower computational cost. Specifically, DoReMi trains two auxiliary models, while DoGE computes k= 7gradients per iteration with roughly 1.7×wall-clock time per iteration, both of which are costly. In contrast, CHAMELEON achieves competitive results with just 2k steps of training, and its inference cost is minimal compared to the training. In our setting, training an 82M proxy model requires 1017–1018FLOPs, and extracting embeddings re- quires only 1015FLOPs ( <1%of proxy training). As a re- sult, the total FLOPs of CHAMELEON, including both train- ing and inference for embeddings, is only 10% of DoReMi 5 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 2: Universal generalization - perplexity. Per-domain test perplexity for the universal generalization compared with the uniform baseline, DoReMi, DoGE, and RegMix with 684M parameter models. Note that, unlike other methods, RegMix knows the target downstream tasks for optimization. We report the compute (in FLOPs) required to arrive at the data mixture. CHAMELEON boosts generalization at a fraction of the computational cost. Domain Uniform DoReMi DoGE CHAMELEON RegMix Arxiv 8.16 9.16 9.07 8.31 11.35 Book 42.55 46.48 40.30 39.23 41.52 CC 45.26 40.62 38.99 40.11 37.32 C4 49.00 43.92 40.65 42.59 43.85 Github 3.99 4.10 4.09 4.20 4.99 Stackexchange 7.99 8.35 7.39 7.94 10.63 Wikipedia 12.42 10.78 15.74 13.90 20.88 Average PPL ( ↓) 24.20 23.34 22.32 22.31 24.36 # Domains Over Uniform - 3/7 4/7 4/7 3/7 FLOPs 0 1.34×10186.68×10171.36×10171.20×1018 (10×) (5×) (1×) (9×) Table 3: Universal generalization - reasoning. Accuracy of downstream tasks in the same settings as Table 2. Benchmark Unif. DoReMi DoGE CH.RegMix ARC-E 36.8 37.6 38.0 37.8 39.1 COPA 55.7 59.3 62.3 61.9 63.0 HellaSwag 26.5 27.0 27.2 27.1 27.0 Lambada 13.5 13.6 14.7 15.1 16.5 LogiQA 21.7 21.9 22.4 22.6 21.4 MultiRC 57.2 55.7 57.3 57.2 56.6 OpenBook 14.1 13.3 14.6 14.4 14.7 PiQA 59.2 59.5 60.0 60.5 57.6 QQP 36.8 36.8 36.8 39.2 37.1 RACE 26.1 25.3 26.4 26.5 27.3 SciQ 61.8 62.5 64.9 64.3 64.1 Social IQA 35.0 35.5 35.7 35.7 35.6 WinoGrande 50.5 51.3 52.0 52.1 50.9 Average ( ↑)37.9 38.4 39.439.639.3 and 20% of DoGE. In addition, we demonstrate GPU hours in Table 4 and discuss its details in Appendix B.4, high- lighting the efficiency and lower computational overhead of CHAMELEON. Evaluation on downstream tasks. We apply our method on realistic downstream tasks. We follow RegMix (Liu et al., 2024) selecting 13 tasks that cover various realistic tasks: ARC-E (Clark et al., 2018), COPA (Sarlin et al., 2020), HellaSwag (Zellers et al., 2019), Lambada (Paperno et al., 2016), LogiQA (Liu et al., 2020), MultiRC (Khashabi et al., 2018), OpenBookQA (Mihaylov et al., 2018), PiQA (Bisk et al., 2020), QQP (Wang, 2018), RACE (Lai et al.,Table 5: Universal generalization with 1.2B model - rea- soning. Large-scale pretraining experiments. Benchmark Unif. DoReMi DoGE CH.RegMix ARC-E 39.4 41.2 41.9 42.4 43.0 COPA 64.0 66.0 63.0 61.0 66.0 HellaSwag 27.5 27.7 28.2 28.4 27.6 Lambada 17.9 17.3 18.7 21.6 20.7 LogiQA 22.0 24.0 22.0 21.2 20.7 MultiRC 57.2 57.2 57.2 57.2 56.9 OpenBookQA 15.0 13.6 13.8 16.4 17.4 PIQA 61.5 61.9 61.8 63.8 58.7 QQP 36.8 36.8 36.9 36.9 36.8 RACE 26.0 26.7 27.8 29.1 28.4 SciQ 69.7 68.3 69.0 72.6 72.0 SocialIQA 36.2 36.5 35.9 37.2 36.1 WinoGrande 52.8 49.6 48.9 51.5 50.0 Average ( ↑) 40.5 40.5 40.4 41.5 41.1 2017), SciQ (Welbl et al., 2017), Social IQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021). The reported accuracy in Table 3 is the average from 0-shot to 5-shot evaluations following (Liu et al., 2024), scored using the lm-eval-harness evaluation framework (Gao et al., 2024). These benchmarks cover a diverse range of tasks, en- abling a comprehensive evaluation of the real-world im- pact of CHAMELEON. For each benchmark, we use nor- malized accuracy as the evaluation metric if provided by lm-eval-harness else we use regular accuracy. Notably, CHAMELEON also shows competitive performance across all downstream tasks even compared with RegMix, a task- aware method. 6 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 6: Universal generalization with 1.2B model - perplexity. Large-scale experiments in the same settings of Table 5. Domain Uniform DoReMi DoGE CHAMELEON RegMix Arxiv 6.30 7.09 7.07 6.33 10.61 Book 28.25 32.66 27.83 24.63 27.55 CC 31.19 29.96 28.11 26.95 24.70 C4 34.74 33.05 31.06 29.58 31.94 Github 2.91 3.03 3.07 2.94 4.08 Stackexchange 6.01 6.44 5.80 5.76 9.54 Wikipedia 8.65 7.93 10.88 9.03 20.08 Average PPL ( ↓) 16.86 17.17 16.26 15.03 18.36 # Domains Over Uniform - 3/7 4/7 4/7 3/7 Scale to 1.2B model. Prior works (Xie et al., 2023; Fan et al., 2024b; Liu et al., 2024) have shown that domain weights transfer effectively across different model scales. To validate this for CHAMELEON, we extended our experiments to 1.2B models. The detailed results for perplexity and downstream task performance are presented in Table 6 and Table 5, respectively. Our findings confirm such transferabil- ity: the weights derived from an 82M proxy model proved effective when applied to both the 684M and 1.2B models. Importantly, we observed that CHAMELEON yielded even more significant improvements on larger models. Stability and Practicality. We show that domain weights obtained by CHAMELEON remain stable across different training steps of the proxy model, whereas DoReMi and DoGE are sensitive or converge slowly as shown in Figure 3. Additionally, our method is robust to variations in proxy model size, the hyperparameter λ, and the number of sam- ples computing embeddings, as detailed in Appendix B.5. This stability significantly reduces the need for extensive hyperparameter tuning, making CHAMELEON more prac- tical and resource-efficient for real-world applications. In addition, we report GPU hours in Table 4 for domain weight computation and for training 684M base model, showing that the proxy training cost is non-negligible. Compared to DoReMi and DoGE, we reduce computational overhead to less than 2% of final training cost. This reduction is crucial for academic labs and smaller-scale training. More details are given in Appendix B.4. 4.2. Scalable to Pile It is common for new data to be introduced during the offi- cial training of large base models, particularly when training on diverse and evolving datasets. However, existing meth- ods including DoReMi, DoGE and RegMix require retrain- ing a new proxy model from scratch whenever domains are added or removed, making them both inefficient and incon- venient for dynamic data environments. This process notonly incurs significant computational costs but also delays the adaptation to changes in domain composition. How can we develop a scalable method to reliably compute domain weights when domain composition changes? CHAMELEON has such scalability. Unlike DoReMi and DoGE, CHAMELEON ’s algorithm does not alter the proxy model’s optimization, instead it focuses more on the intrin- sic data characteristics, where the trained proxy model can already capture domain features, even for new unseen data..0.1.2.3Domain WeightDoReMi 1k 2k 5k 10k.0.1.2.3Domain WeightDoGE 1k 2k 5k 10k Arxiv BookCC C4 Github StackexchangeWikipedia.0.1.2.3Domain WeightCHAMELEON 1k 2k 5k 10k Figure 3: KRLS scores converge quickly. Domain weights across methods during proxy training. DoReMi and DoGE require more iterations to stabilize, while CHAMELEON con- verges quickly after 1k iterations. The detailed discussion is in Appendix B.5. 7 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 7: Transfer to new domains - perplexity. Per-domain test perplexity when adapting to new data. The Human baseline is (Gao et al., 2020). When domain composition changes, the other methods need to retrain proxy models from scratch and re-run domain weight optimization, while we can directly compute the KRLS of the new domains by extending the proxy model trained on previous data. We report the extra compute (in FLOPs) required to adapt the data mixture to the Pile. Domain Human DoReMi DoGE CHAMELEON RegMix Arxiv 9.76 14.11 10.78 9.73 16.19 Dm_mathematics 5.52 6.27 4.52 5.31 19.26 Enron_emails 12.82 9.96 9.39 7.77 12.06 Europarl 34.69 24.77 11.62 28.18 131.80 Freelaw 14.12 16.20 17.99 15.04 18.66 Github 5.92 5.84 4.90 6.16 9.95 Gutenberg_pg_19 39.36 38.36 39.57 33.28 34.43 Hackernews 35.94 29.68 29.87 27.02 29.80 Nih_exporter 22.93 25.89 26.81 24.12 28.69 Philpapers 47.59 36.43 44.56 34.63 51.71 Pile_cc 43.19 32.85 58.17 34.90 32.30 Pubmed_abstracts 17.87 24.19 25.62 21.87 23.20 Pubmed_central 9.76 9.43 8.10 7.36 13.80 Stackexchange 10.41 11.48 11.79 11.25 18.96 Ubuntu_irc 36.12 32.10 23.20 29.34 20.71 Uspto_backgrounds 17.22 21.19 20.08 18.25 22.05 Wikipedia_en 28.70 25.95 40.42 24.68 29.32 Average PPL ( ↓) 23.05 21.45 22.79 19.94 30.17 # Domains Over Human - 9/17 10/17 11/17 4/17 Extra FLOPs 0 1.34×10186.68×10174.62×10153.5×1018 (290×) (145×) (1×) (758×) Table 8: Transfer to new domains - reasoning. Accuracy of downstream tasks in the same settings as Table 7. Benchmark Hum. DoReMi DoGE CH.RegMix ARC-E 37.5 39.3 35.3 39.239.5 COPA 56.8 61.5 54.7 60.961.2 HellaSwag 26.7 27.3 26.1 27.427.3 Lambada 12.5 15.8 9.3 15.915.4 LogiQA 22.5 21.2 22.1 23.821.9 MultiRC 56.7 56.5 57.2 57.356.2 OpenBook 13.3 13.1 13.1 14.214.5 PiQA 57.8 59.3 55.9 59.760.4 QQP 37.5 36.8 36.8 37.237.6 RACE 25.8 27.2 24.9 26.827.2 SciQ 64.1 65.7 58.1 66.067.1 Social IQA 35.0 36.0 34.2 36.636.3 WinoGrande 50.7 51.2 49.8 50.949.9 Average ( ↑)38.2 39.3 36.739.739.6 To test its scalability, we employ the proxy model trained on Slimpajama in Section 4.1 to Pile dataset (Gao et al., 2020) directly. Both SlimPajama and Pile are large-scale datasetsused for pretraining LMs, with overlapping data sources such as books, scientific texts, web content, and codebases. The Pile dataset includes more domains than Slimpajama and its data is more diverse. Note that the original Pile dataset includes 22 domains but only 17 are now available due to copyright issues. To obtain domain weights of Pile, we input 4k samples per domain of Pile to the proxy model trained on Slimpajama in Section 4.1 and infer embeddings for computing domain weights through (KRLS). The computed domain weights are reported in Table 19, where we use the domain weights reported in their respective papers for DoReMi and RegMix. We use Human as baseline that is selected manually as in the Pile paper (Gao et al., 2020). As in Section 4.1, we report per-domain perplexity in Table 7 and downstream accuracy in Table 8. CHAMELEON outperforms DoReMi, DoGE and even Reg- Mix in both cases. Importantly, FLOPs of C HAMELEON is marginal compared with DoReMi and DoGE (only 1% of training a new proxy model from scratch) since we can reuse the previous proxy model and our extra FLOPs only include inference cost for extracting embeddings, while DoReMi DoGE, and RegMix require retraining the proxy models. 8 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 9: Finetuning. Per-domain test perplexity compared with the uniform baseline for finetuning on the 7 languages in Wiki40b. CHAMELEON can flexibly be extended to the finetuning pipeline by computing the KRLS of the finetun- ing domains, improving performance across all domains. Domain Uniform CHAMELEON French 6.86 6.51 German 10.12 8.78 Italian 13.29 12.42 Spanish 8.41 8.04 Portuguese 8.00 7.78 Dutch 13.98 12.30 Polish 5.07 4.21 Average PPL ( ↓) 9.43 8.58 # Domains Over Uniform - 7/7 Note that as the number of domains increases, DoGE com- putes k= 17 gradients per iteration, resulting in approxi- mately 2.5×wall-clock time per iteration on this dataset. 4.3. Finetuning Besides pertaining, CHAMELEON interestingly allows data- mixture optimization during finetuning. We take advantage of the existing pretrained model and can extract embeddings on finetuning data directly for domain weight computation. As discussed in Section 3.2, the goals of pretraining and fine-tuning are distinct, with pretraining aiming for broad generalization and fine-tuning focusing on specialization. Therefore, we directly use leverage scores for computing domain weights, as described in Section 3.2. We fine-tune a pretrained model trained on the Pile (from Section 4.2) for 10k steps on two separate datasets: ( i) Wiki40b (Guo et al., 2020), which includes multiple lan- guages, for which we select 7 Latin languages, and ( ii) Stack-decup (Kocetkov et al., 2022), which covers various programming languages, from which we use 7. The results are shown in Table 9 and Table 10. CHAMELEON outper- forms the uniform weights baseline across all domains in both tasks, showing our data mixture can greatly benefit finetuning. Remarkably, our weight computation is compu- tationally cheap in finetuning, as we simply need forward passes through the pretrained model to compute domain embeddings and we can then directly apply (KRLS). Additionally, we present fine-tuning results using αPTin- stead of αFTin Appendix B.9 for reference. The results demonstrate that fine-tuning with KRLS-based domain weights outperforms using their inverse. This indicates that data-mixing strategies should be tailored to different training phases.Table 10: Finetuning. Per-domain test PPL vs. the uniform baseline for finetuning on the 7 programming languages in Stack dataset. C HAMELEON improves across all domains. Domain Uniform CHAMELEON Python 19.98 16.53 Java 19.27 15.53 C 28.24 22.58 C++ 25.16 21.09 Go 30.25 19.26 Ruby 21.78 17.83 PHP 9.45 7.43 Average PPL ( ↓) 22.02 17.18 # Domains Over Uniform - 7/7 5. Conclusion We introduce CHAMELEON, a novel and efficient framework for data mixing that leverages KRLS to quantify the repre- sentativeness of data domains. We demonstrate that inverse KRLS-based domain weights effectively identify highly im- portant domains for pretraining LMs. CHAMELEON can adapt to new domains without retraining proxy models, out- performing baselines in downstream tasks. Given that it is computationally inexpensive and stable, CHAMELEON lowers the overall cost of the expensive LLM pretraining pipeline, which can be useful both in industry and within academic budgets. We also extend domain reweighting to fine-tuning with KRLS-based weights, demonstrating con- sistent improvements. This work highlights the need to tailor data-mixing strate- gies to different training phases. In future work, we aim to extend our approach to online settings for dynamic optimiza- tion during training. Additionally, we will extend to target specific downstream tasks by modifying the identity matrix within the KRLS to emphasize relevant domains, enhancing our method’s flexibility for specific downstream tasks. Acknowledgements We thank the reviewers for their constructive feedback. Thanks to Simin Fan for the helpful discussion. This work was supported as part of the Swiss AI Initiative by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a06 on Alps. This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021_205011. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). Research was sponsored by the Army Re- search Office and was accomplished under Grant Number W911NF-24-1-0048. 9 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Impact Statement The approach presented in this paper aims at advancing the field of Machine Learning. No other potential societal con- sequences of our work are deemed necessary to specifically highlight here. References Alaoui, A. E. and Mahoney, M. W. Fast Random- ized Kernel Methods With Statistical Guarantees, 2015. arXiv:1411.0306. Albalak, A., Pan, L., Raffel, C., and Wang, W. Y. Efficient online data mixing for language model pre-training. In R0- FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models Workshop, 2023. Bach, F. Sharp analysis of low-rank kernel matrix approxi- mations. In Conference on Learning Theory, pp. 185–209. PMLR, 2013. Beckermann, B., Putinar, M., Saff, E. B., and Stylianopou- los, N. Perturbations of christoffel–darboux kernels: De- tection of outliers. Foundations of Computational Mathe- matics, 21(1):71–124, 2021. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. InAdvances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1877–1901, 2020. Calandriello, D., Lazaric, A., and Valko, M. Analysis of nyström method with sequential ridge leverage score sam- pling. In Uncertainty in Artificial Intelligence Conference, 2016. Chen, Y. and Yang, Y. Fast statistical leverage score ap- proximation in kernel ridge regression. In International Conference on Artificial Intelligence and Statistics (AIS- TATS), pp. 2935–2943. PMLR, 2021. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cohen, M. B., Musco, C., and Musco, C. Ridge lever- age scores for low-rank approximation. arXiv preprint arXiv:1511.07263, 6, 2015. Cohen, M. B., Musco, C., and Musco, C. Input sparsity time low-rank approximation via ridge leverage scoresampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1758– 1777. SIAM, 2017. De Marchi, S., Sommariva, A., and Vianello, M. Mul- tivariate christoffel functions and hyperinterpolation. Dolomites Research Notes on Approximation, 7(Special Issue), 2014. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of- experts. In International Conference on Machine Learn- ing (ICML), pp. 5547–5569. PMLR, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ducharlet, K., Travé-Massuyès, L., Lasserre, J.-B., Le Lann, M.-V., and Miloudi, Y. Leveraging the christoffel func- tion for outlier detection in data streams. International Journal of Data Science and Analytics, 2024. Dunkl, C. F. and Xu, Y. Orthogonal polynomials of sev- eral variables, volume 155. Cambridge University Press, 2014. Fan, S., Grangier, D., and Ablin, P. Dynamic gradi- ent alignment for online data mixing. arXiv preprint arXiv:2410.02498, 2024a. Fan, S., Pagliardini, M., and Jaggi, M. DOGE: Domain reweighting with generalization estimation. In Interna- tional Conference on Machine Learning (ICML), 2024b. Fanuel, M., Schreurs, J., and Suykens, J. A. Nyström land- mark sampling and regularized christoffel functions. Ma- chine Learning, 111(6):2213–2254, 2022. Feng, S., Prabhumoye, S., Kong, K., Su, D., Patwary, M., Shoeybi, M., and Catanzaro, B. Maximize your data’s potential: Enhancing llm accuracy with two-phase pre- training. arXiv preprint arXiv:2412.15285, 2024. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 2024. 10 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Gareth, J., Daniela, W., Trevor, H., and Robert, T. An introduction to statistical learning: with applications in R. Spinger, 2013. Guo, M., Dai, Z., Vrande ˇci´c, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In Proceedings of the Twelfth Language Resources and Evaluation Confer- ence, 2020. Hastie, T., Tibshirani, R., Friedman, J., and Franklin, J. The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer, 27(2): 83–85, 2005. Jiang, Y., Zhou, A., Feng, Z., Malladi, S., and Kolter, J. Z. Adaptive data optimization: Dynamic sample selection with scaling laws. arXiv preprint arXiv:2410.11820, 2024. Kang, F., Sun, Y., Wen, B., Chen, S., Song, D., Mah- mood, R., and Jia, R. Autoscale: Automatic prediction of compute-optimal data composition for training llms. arXiv preprint arXiv:2407.20177, 2024. Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., and Roth, D. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Pro- ceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018. Kocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis, C. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from exami- nations. arXiv preprint arXiv:1704.04683, 2017. Lasserre, J. B. and Pauwels, E. The empirical christoffel function with applications in data analysis. Advances in Computational Mathematics, 45(3):1439–1468, 2019. Li, M., Miller, G. L., and Peng, R. Iterative row sampling. In2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 127–136, 2013. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Liu, Q., Zheng, X., Muennighoff, N., Zeng, G., Dou, L., Pang, T., Jiang, J., and Lin, M. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024.Longpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A., Zoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D., and Ippolito, D. A pretrainer‘s guide to training data: Measuring the effects of data age, domain coverage, qual- ity, & toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, pp. 3245–3276. Association for Computational Linguis- tics, 2024. Ma, Y., Liu, Y., Yu, Y., Zhang, Y., Jiang, Y., Wang, C., and Li, S. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298, 2023. Mahoney, M. W. and Drineas, P. CUR matrix decompo- sitions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009. Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Inves- tigating effectiveness of parametric and non-parametric memories. In Annual Meeting of the Association for Com- putational Linguistics (ACL), Toronto, Canada, July 2023. Association for Computational Linguistics. McInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Musco, C. and Musco, C. Recursive sampling for the nys- trom method. In Advances in Neural Information Process- ing Systems (NeurIPS), volume 30. Curran Associates, Inc., 2017. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The LAMBADA dataset: Word predic- tion requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Parmar, J., Prabhumoye, S., Jennings, J., Liu, B., Jhunjhun- wala, A., Wang, Z., Patwary, M., Shoeybi, M., and Catan- zaro, B. Data, data everywhere: A guide for pretraining dataset construction. arXiv preprint arXiv:2407.06380, 2024. Parthasarathy, V. B., Zafar, A., Khan, A., and Shahid, A. The ultimate guide to fine-tuning llms from basics to breakthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities. arXiv preprint arXiv:2408.13296, 2024. 11 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Pauwels, E., Bach, F., and Vert, J.-P. Relating leverage scores and density using regularized christoffel functions. InAdvances in Neural Information Processing Systems (NeurIPS), volume 31, 2018. Rudi, A., Carratino, L., and Rosasco, L. Falkon: An opti- mal large scale kernel method. In Advances in Neural Information Processing Systems (NeurIPS), volume 30. Curran Associates, Inc., 2017. Rudi, A., Calandriello, D., Carratino, L., and Rosasco, L. On fast leverage score sampling and optimal learning. InAdvances in Neural Information Processing Systems (NeurIPS), volume 31. Curran Associates, Inc., 2018. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks. In International Conference on Learning Representations (ICLR), 2020. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64, 2021. Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interac- tions. arXiv preprint arXiv:1904.09728, 2019. Sarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich, A. Superglue: Learning feature matching with graph neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Schölkopf, B., Herbrich, R., and Smola, A. J. A generalized representer theorem. In Conference on Learning Theory, pp. 416–426. Springer, 2001. Shen, Z., Tao, T., Ma, L., Neiswanger, W., Liu, Z., Wang, H., Tan, B., Hestness, J., Vassilieva, N., Soboleva, D., et al. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: A 627B token cleaned and deduplicated version of RedPa- jama, 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Vapnik, V. N. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988–999, 1999. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten- tion is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wang, A. Glue: A multi-task benchmark and analysis plat- form for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.Welbl, J., Liu, N. F., and Gardner, M. Crowdsourc- ing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Xie, S. M., Pham, H., Dong, X., Du, N., Liu, H., Lu, Y., Liang, P., Le, Q. V., Ma, T., and Yu, A. W. DoReMi: Optimizing data mixtures speeds up language model pre- training. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Yang, Y., Mishra, S., Chiang, J. N., and Mirzasoleiman, B. SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Ye, J., Liu, P., Sun, T., Zhou, Y., Zhan, J., and Qiu, X. Data mixing laws: Optimizing data mixtures by pre- dicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a machine really finish your sentence? InAnnual Meeting of the Association for Computational Linguistics (ACL), pp. 4791–4800, 2019. 12 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Appendix Table of Contents A Additional discussions on the KRLS 14 A.1 Details of leverage scores........................................... 14 A.2 Discussions.................................................. 15 B Additional experiments 16 B.1 Additional figures for Figure 1........................................ 16 B.2 Experimental setup............................................... 16 B.3 Domain weights on Slimpajama....................................... 17 B.4 Temporal cost................................................. 17 B.5 Stable domain weights of C HAMELEON................................... 17 B.6 Additional baseline: Data Mixing Laws................................... 18 B.7 Domain weights on Pile............................................ 19 B.8 Domain weights for finetuning........................................ 19 B.9 PPL of finetuning................................................ 19 13 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning A. Additional discussions on the KRLS A.1. Details of leverage scores In this work, we employ KRLS to assign scores to data domains, and we focus on the leverage scores as a measure of domain importance, both in pretraining and finetuning language models. The KRLS (Alaoui & Mahoney, 2015) is a kernelized-version of the ridge leverage scores, which are used to quantify the importance of the rows in a matrix for the best-rank approximation with approximation error guarantees (Mahoney & Drineas, 2009; Li et al., 2013). The KRLS for thei-th domain is defined as Si=\u0000 ΩD(ΩD+kλI)−1\u0001 ii, (1) with kernel matrix ΩDij=κ(xi, xj)and regularization constant λ >0. In our data-centric approach, we rank domains based on the degree of inter-domain dependency. To better see this, we establish the equivalence of RLS computed in the feature space induced by the finite-dimensional feature map ϕand KRLS when employing the corresponding kernel κ(xi, xj) =ϕ(xi)⊤ϕ(xj), where we use ϕ(x) =xin Section 3.1. This result provides insights into the behavior of the employed KRLS for domain reweighting. Let the set of kdomain embeddings {xi}k i=1, where xi∈Rp. Letϕ:Rp→Rd be a finite-dimensional feature map with associated p.s.d. kernel κ(xi, xj) =ϕ(xi)Tϕ(xj)by kernel trick (Vapnik, 1999), andΦ(X)∈Rk×dbe the matrix whose rows are the feature mappings ϕ(xi)⊤. The ridge regression hat matrix in feature space is Hϕ λ= Φ(X)(Φ(X)TΦ(X) +λI)−1Φ(X)T, with ridge leverage scores diag(Hϕ λ). In kernel ridge regression, the kernel ridge hat matrix is Hκ= ΩD(ΩD+λI)−1and the kernel ridge leverage scores are given by diag (Hκ). Lemma A.1. With kernel κ(x, y) =ϕ(x)Tϕ(y), where ϕ:Rd→Rpis a finite-dimensional feature map, the RLS in feature space induced by ϕand the KRLS are s.t. diag (Hϕ λ) =diag(Hκ). Proof. With the kernel κ(x, y) =ϕ(x)Tϕ(y), we have ΩD= Φ(X)Φ(X)T. Substituting into Hκ: Hκ= (Φ( X)Φ(X)T)(Φ(X)Φ(X)T+λI)−1. We utilize the following matrix identity: for matrices A∈Rm×nandB∈Rn×mand a scalar λ̸= 0, A(BA+λI)−1= (AB+λI)−1A. This identity can be verified by multiplying both sides by (BA+λI)from the right, then by (AB+λI)from the left, which yields the same result on both sides. LetA= Φ(X)andB= Φ(X)T. Then: Φ(X)(Φ(X)TΦ(X) +λI)−1= (Φ( X)Φ(X)T+λI)−1Φ(X). Therefore, Hϕ λ= Φ(X)(Φ(X)TΦ(X) +λI)−1Φ(X)T= (Φ( X)Φ(X)T+λI)−1Φ(X)Φ(X)T=Hκ. Thus, the diagonal elements, and hence the leverage scores, are equivalent: diag (Hϕ λ) =diag(Hκ). The above lemma establishes the equivalence of the ridge leverage scores computed in the feature space induced by the finite-dimensional feature map ϕand the KRLS when employing the corresponding kernel κ(xi, xj) =ϕ(xi)⊤ϕ(xj). It is possible to relate the RLS to the ridgless solution, where the regularized solution converges to the least-norm solution as λ→0. Let the eigendecomposition of ΩD=UΣU⊤, then the KRLS of domain i, i= 1,..., k can be written as Sλ(Di) =kX j=1σj σj+λU2 ij, where σjis the j-th eigenvalue of ΩD. Therefore, the KRLS is a weighted version of the standard statistical leverage (Gareth et al., 2013), i.e.,Pk j=1U2 ij, with weights depending on the regularization and the eigenspectrum of ΩD. We now recall the relationship between the least-norm solution of a system of equations to the (ridgless) statistical leverage score ℓi=ϕ(xi)⊤(Φ(X)⊤Φ(X))+ϕ(xi)(Mahoney & Drineas, 2009). 14 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Lemma A.2. The least-norm solution characterizes ℓias follows: ℓi= min Φ(X)⊤y=ϕ(xi)∥y∥2 2. (2) The leverage score of domain imeasures how important it is in composing the row space of Φ(X). If a row (domain) has a component orthogonal to all other rows (domains), its leverage score is 1. In data mixture, (2)seeks a linear combination of features that best approximates the embedding xiof the i-th domain. Intuitively, ℓiis highest when xiis linearly independent from the other domain embeddings. In our approach, we use the KRLS to assign scores to data domains both in pretraining and finetuning language models. High scores indicate data domains that are difficult to approximate with a linear combination of other domains, and are thus more unique. On the other hand, low scores indicate data domains that show higher degree of dependency with other domains, identifying more common data characteristics. During pretraining, we rank domains with lower KRLS as more important, as they are more common and thus more useful for learning general-purpose language representations. During finetuning to a specific task, we upweight domains with higher KRLS, as they are more unique and therefore better suited for learning task-specific representations. A.2. Discussions We now discuss additional theoretical insights and properties of our methodology. •Role of regularization. Adding λItoΩDin(1)reduces the influence of the small principal components, resulting in proportionately lower sampling probability. A large λsoft-thresholds the low part of the spectrum of ΩDand amplifies the contribution of the top eigenvectors of the kernel matrix, focusing on the most dominant domains. In practice, due to the relatively small number of domains, we observe that our algorithm is robust to the choice of λ, where small regularization is sufficient and larger values do not significantly alter the computed domain weights. •Adaptability to larger models. Our approach can robustly transfer domain weights obtained from a small proxy model hθpto a larger target model hθtthanks to its use of domain affinities. Specifically, our method computes domain weights based on the kernel matrix ΩD, which captures pairwise inner products between domain embeddings. While the absolute embeddings ximay vary between the proxy and target model, their inner products show a higher degree of consistency across model sizes. For instance, if the proxy learns that the “github” and “stackexchange” domains are semantically close, a larger, more powerful model typically also maintains this proximity in embedding space. Consequently, the pairwise similarities encoded in ΩD, and therefore the resulting KRLS scores (1)and domain weights, are robust across different model scales. Empirical evidence is presented in Table 16. •Relation with Christoffel functions. In machine learning literature, the Christoffel function is a key concept that characterizes the local density of the data distribution in feature space (Pauwels et al., 2018). Christoffel functions are known in orthogonal polynomials (Dunkl & Xu, 2014) and approximation theory (De Marchi et al., 2014). They are extended to machine learning (Pauwels et al., 2018), where it makes the connection between inverse leverage scores and the kernelized Christoffel function. In machine learning, they are mainly used for landmark sampling (Fanuel et al., 2022), density estimation (Pauwels et al., 2018), and outlier detection (Lasserre & Pauwels, 2019; Beckermann et al., 2021; Ducharlet et al., 2024). Given samples {zi}n j=1, the kernelized Christoffel function is defined as the following regularized minimization over a reproducing kernel Hilbert space (RKHS) Hwith associated kernel κH: Cλ,η(z) = inf g∈HnX j=1ηj ng(zi)2+λ∥g∥2 Hs.t.g(z) = 1, (3) where λ >0is a regularization constant, ∥g∥Hdenotes the RKHS norm of g, and ηj>0. The Christoffel function is linked to the ridge leverage scores (RLS) (Alaoui & Mahoney, 2015; Cohen et al., 2017; Rudi et al., 2018), which quantify the influence of each sample on the learned model. Specifically, the Christoffel function at a point is proportional to the inverse of its RLS. High RLS values indicate data points that are difficult to represent as linear combinations of other points in the feature space. Conversely, a high Christoffel function value (and thus a low RLS) suggests a data point lies in a region of high data density and can be better expressed in terms of other points. In our work, focusing on domains with high degree of linear dependency, i.e., high Christoffel function, is shown to enable improved generalization and transfer learning capabilities. The following lemma details the closed-form expression for the regularized kernelized Christoffel function at each sample. 15 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Lemma A.3 ((Pauwels et al., 2018)).The regularized kernelized Christoffel function takes the following value at sample j, forj= 1,..., n: Cλ,η(zj) =ηj n\u0010 K\u0000 K+nλdiag(η)−1\u0001−1\u0011−1 jj, (4) where K∈Rn×nis the kernel matrix with entries Kij=κH(zi, zj), and diag(η) =diag(η1,..., η n)is a diagonal matrix with entries ηjon the diagonal. This is derived from the representer theorem applied to (3)(Schölkopf et al., 2001; Pauwels et al., 2018). Lemma A.3 reveals the connection between the Christoffel function and the KRLS (1). The Christoffel function Cλ,1(zj)is therefore inversely proportional to KRLS of sample zjwithη= 1. B. Additional experiments B.1. Additional figures for Figure 1 0 5 10 15−50510 UMAP1UMAP2 wikipedia book ccc4 arxiv github stackexchange Figure 4: Domains in embedding space. 2D UMAP vi- sualization of embeddings of SlimPajama learned by the proxy model. Semantically similar domains occupy sim- ilar regions in embedding space, creating high-density clusters. Arxiv BookCC C4 Github StackexchangeWikipediaArxiv Book CC C4 Github Stackexchange Wikipedia42.2−16.9−14.5−14.55.6 2.9−4.9 −16.926.7 12 14.6−21.2−16.10.9 −14.5 12 23.9 18.7−20.3−13.3−6.4 −14.514.6 18.7 22.1−21.8−16.7−2.3 5.6−21.2−20.3−21.845.7 31.1−19 2.9−16.1−13.3−16.731.1 29−16.9 −4.9 0.9−6.4−2.3−19−16.948.6 −2002040 Relationship StrengthFigure 5: Domain affinity matrix. The matrix shows the relationship strength between domains in SlimPajama. B.2. Experimental setup. We follow the experimental setup of DoGE (Fan et al., 2024b), using a small 82M decoder-only Transformer (Vaswani et al., 2017) as the auxiliary model for CHAMELEON, DoReMi, and DoGE. Additionally, we use a 684M model as the base model for pretraining. Moreover, we set the batch size 128, the cosine learning rate scheduler, weight decay 0.01, and gradient clipping 1.0 for all models. For the training on Slimpajama, Wiki40b, and Stack datasets, we set batch size 128. We increase the batch size to 512 on the Pile dataset since it is more noisy and has a larger number of domains. InCHAMELEON, a temperature factor τin the softmax normalization for domain weights is additionally applicable: αi=exp(zi/τ)/Pk j=1exp(zj/τ),where z=S−1 λ(D)for pretraining and z=Sλ(D)for fine-tuning. In our experiments, we typically set τPT∈[5,10]andτFT∈[0.2,0.5]. For the ablation study of CHAMELEON, we also evaluate proxy models with other sizes (60M, 124M, and 210M). The model architectures and their corresponding learning rates are reported in Table 11. For the setup of RegMix on the Slimpajama dataset, we follow its original setup (Liu et al., 2024) and train 200 1M proxy models with 1k steps to fit a regression model. 16 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 11: Architecture hyperparameters for various model scales. Layers Attention heads Embed dim Hidden dim Max. learning rate (min.) 60M 3 6 768 3072 5×10−4(1×10−4) 82M 6 12 768 3072 5×10−4(1×10−4) 124M 12 12 768 3072 5×10−4(1×10−4) 210M 24 16 768 3072 5×10−4(1×10−4) 684M 36 24 1200 4800 1.5×10−4(5×10−5) 1.2B 36 25 1600 6400 1.5×10−4(5×10−5) B.3. Domain weights on Slimpajama We report our final domain weights for base model training in Table 12. Specifically, DoReMi and DoGE use domain weights through training proxy models with 10k steps. CHAMELEON use the model with 2k steps. For RegMix, we follow its paper (Liu et al., 2024) using “CC” as the target domain and train 200 1M proxy models to get the domain weights. Table 12: Final domain weights. Domain DoReMi DoGE C HAMELEON RegMix Arxiv 0.057 0.041 0.083 0.001 Book 0.002 0.078 0.164 0.025 CC 0.237 0.268 0.202 0.924 C4 0.237 0.283 0.247 0.024 Github 0.130 0.059 0.082 0.019 Stackexchange 0.101 0.230 0.149 0.006 Wikipedia 0.236 0.041 0.073 0.001 B.4. Temporal cost In Table 13, we report the required GPU (H100) hours for obtaining domain weight regarding the experiments in Table 2. We also show GPU hours of training 684M base model for a reference. We claim that improving the efficiency of determining the domain mixture is essential because the associated computational cost of proxy training is non-negligible. Compared to DoReMi and DoGE, which add over 10% to base model training costs, we reduce computational overhead to less than 2% of final training cost. This reduction is crucial for academic labs and smaller-scale training. More importantly, the computational cost reported is often an optimistic lower bound for the baselines even for larger base models, since DoReMi and DoGE require extensive hyperparameter tuning. It has been shown that DoReMi’s weights are unstable or difficult to reproduce (Fan et al., 2024b; Parmar et al., 2024) and DoGE approximations make it more sensitive to learning rate (Kang et al., 2024). Such sensitivity necessitates repeated validation on base models. Nevertheless, CHAMELEON is insensitive to hyperparameters and a detailed discussion is in Appendix B.5. Table 13: GPU hours for obtaining domain weight. DoReMi DoGE C HAMELEON 684M base model 7.4h 6.3h 0.8h 56h B.5. Stable domain weights of CHAMELEON Table 14 corresponds to Figure 3 and reports the specific domain weights obtained by the proxy model with the different number of steps.CHAMELEON is more stable and converges faster than DoReMi and DoGE. In addition, we report perplexity using domain weights derived from a proxy model trained for 2k steps for DoReMi and DoGE in Table 15. This further demonstrates that DoReMi and DoGE converge slower than C HAMELEON. 17 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Furthermore, Table 16 demonstrates that CHAMELEON is also very stable across different model sizes,λvalues, and number of samples for embedding computations. Table 14: Domain weights at different steps. Domain DoReMi DOGE CHAMELEON 1k 2k 5k 10k 1k 2k 5k 10k 1k 2k 5k 10k Arxiv 0.251 0.172 0.095 0.057 0.116 0.092 0.060 0.041 0.088 0.083 0.072 0.096 Book 0.003 0.008 0.004 0.002 0.139 0.131 0.102 0.078 0.149 0.164 0.174 0.158 CC 0.080 0.114 0.153 0.237 0.177 0.209 0.247 0.268 0.174 0.202 0.188 0.195 C4 0.080 0.115 0.154 0.237 0.176 0.210 0.259 0.283 0.281 0.247 0.220 0.251 Github 0.215 0.138 0.241 0.130 0.121 0.101 0.074 0.059 0.096 0.082 0.094 0.083 Stackexchange 0.095 0.146 0.118 0.101 0.155 0.167 0.200 0.230 0.137 0.149 0.137 0.136 Wikipedia 0.276 0.308 0.235 0.236 0.116 0.090 0.058 0.041 0.074 0.073 0.082 0.080 Table 15: Perplexity using domain weights derived from a proxy model trained for 2k steps. Domain DoReMi DoGE C HAMELEON Arxiv 8.08 8.49 8.31 Book 52.07 40.38 39.23 CC 48.69 41.31 40.11 C4 52.98 44.54 42.59 Github 3.99 4.05 4.20 Stackexchange 7.98 7.81 7.94 Wikipedia 10.57 13.98 13.90 Average PPL ( ↓) 26.34 23.01 22.31 Table 16: Domain weights across different model sizes, λvalues, and number of samples. DomainModel Sizes λValues Number of Samples 60M 82M 124M 210M λ= 1 λ= 10 λ= 100 2k 4k 8k Arxiv 0.084 0.083 0.087 0.093 0.080 0.083 0.087 0.079 0.083 0.081 Book 0.158 0.164 0.157 0.165 0.159 0.164 0.173 0.165 0.164 0.170 CC 0.170 0.202 0.169 0.170 0.178 0.202 0.201 0.193 0.202 0.209 C4 0.271 0.247 0.288 0.259 0.243 0.247 0.258 0.253 0.247 0.241 Github 0.073 0.082 0.072 0.089 0.097 0.082 0.057 0.079 0.082 0.066 Stackexchange 0.152 0.149 0.153 0.145 0.152 0.149 0.128 0.138 0.149 0.143 Wikipedia 0.072 0.073 0.074 0.078 0.081 0.073 0.095 0.093 0.073 0.090 B.6. Additional baseline: Data Mixing Laws We add an additional baseline, Data Mixing Laws (Ye et al., 2024), for comparison. It derives domain weights by leveraging scaling laws of training steps, model sizes, and data mixtures to predict the performance of large models trained on diverse data from small-scale training. This requires training multiple small proxy models with varying domain weights, making it more computationally expensive than ours, which trains just one proxy model. We use their reported domain weights to train a 684M model on Slimpajama. Since their weights are optimized with the Pile as the target, they may be suboptimal for SlimPajama. However, given the alignment of their objectives and overlap in data sources, we consider the comparison meaningful. As shown in Table 17 and Table 18, CHAMELEON outperforms Data Mixing Laws in both perplexity and downstream tasks 18 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning at a fraction of the cost. Data Mixing Laws’ FLOPs is calculated for 4 different proxy sizes and 20 separate mixtures, where our cost is 2 orders of magnitude lower. Table 17: Data Mixing Laws - perplexity. Perplexity in the same settings as Table 2. Domain CHAMELEON Data Mixing Laws Arxiv 8.31 7.55 Book 39.23 45.06 CC 40.11 44.21 C4 42.59 45.79 Github 4.20 4.01 Stackexchange 7.94 7.96 Wikipedia 13.90 16.20 Average PPL ( ↓) 22.31 24.40 # Domains Over Uniform 4/7 4/7 FLOPs 1.36×10175.36×1019 (1×) (394×) Table 18: Data Mixing Laws - reasoning. Accuracy of downstream tasks in the same settings as Table 2. Benchmark CHAMELEON Data Mixing Laws ARC-E 37.8 34.5 COPA 61.9 59.0 HellaSwag 27.1 27.4 Lambada 15.1 14.7 LogiQA 22.6 26.0 MultiRC 57.2 57.2 OpenBook 14.4 25.2 PiQA 60.5 58.5 QQP 39.2 36.8 RACE 26.5 26.4 SciQ 64.3 57.2 Social IQA 35.7 36.1 WinoGrande 52.1 48.4 Average ( ↑) 39.6 39.0 B.7. Domain weights on Pile We report the domain weights we use on the Pile dataset in Table 19. Note that CHAMELEON and DoGE are from our own experiments, Human is suggested in (Gao et al., 2020), DoReMi uses the same as (Xie et al., 2023), and RegMix uses weights from (Liu et al., 2024). B.8. Domain weights for finetuning We report the αFTon Wiki40b and Stack datasets separately below, which corresponds to Section 4.3. B.9. PPL of finetuning. We report the results of fine-tuning with αPTin Table 22 and Table 23 for reference. It is clear to see that fine-tuning with KRLS-based domain weights is better than the one with the inverse of KRLS-based weights. 19 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 19: Domain weights on Pile. Domain Human DoReMi DoGE RegMix CHAMELEON Arxiv 0.134 0.004 0.0608 0.001 0.0386 Dm_mathematics 0.025 0.002 0.0280 0.000 0.0538 Enron_emails 0.004 0.009 0.0239 0.002 0.0085 Europarl 0.005 0.008 0.0407 0.000 0.0048 Freelaw 0.049 0.005 0.0293 0.001 0.0147 Github 0.054 0.022 0.0693 0.000 0.0099 Gutenberg_pg_19 0.025 0.009 0.0283 0.002 0.0115 Hackernews 0.010 0.016 0.3949 0.012 0.0637 Nih_exporter 0.007 0.008 0.180 0.001 0.0424 Philpapers 0.003 0.034 0.0266 0.000 0.0226 Pile_cc 0.142 0.743 0.0348 0.870 0.4519 Pubmed_abstracts 0.107 0.014 0.0398 0.024 0.0104 Pubmed_central 0.136 0.006 0.0251 0.003 0.1207 Stackexchange 0.118 0.019 0.0266 0.000 0.0226 Ubuntu_irc 0.009 0.011 0.0474 0.064 0.0123 Uspto_backgrounds 0.053 0.004 0.0366 0.002 0.0212 Wikipedia_en 0.117 0.086 0.0425 0.016 0.1075 Table 20: Wiki40b Domain Weights. Domain CHAMELEON French 0.115 German 0.163 Italian 0.127 Spanish 0.109 Portuguese 0.090 Dutch 0.140 Polish 0.257Table 21: Stack Dataset Training Weights. Domain CHAMELEON Python 0.125 Java 0.129 C 0.102 C++ 0.088 Go 0.241 Ruby 0.118 PHP 0.197 Table 22: Per-domain perplexity compared with the Uniform baseline for fine-tuning with 684M parameter models on the 7 languages in the Wiki40b dataset. Domain Uniform CHAMELEON (αFT) C HAMELEON (αPT) French 6.86 6.51 7.14 German 10.12 8.78 10.85 Italian 13.29 12.42 14.42 Spanish 8.41 8.04 8.70 Portuguese 8.00 7.78 8.14 Dutch 13.98 12.30 15.05 Polish 5.07 4.21 5.31 Average PPL ( ↓) 9.43 8.58 9.94 20 CHAMELEON: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning Table 23: Per-domain perplexity compared with the Uniform baseline for fine-tuning with 684M parameter models on the 7 languages in the Stack dataset. Domain Uniform CHAMELEON (αFT) C HAMELEON (αPT) Python 19.98 16.53 20.11 Java 19.27 15.53 19.32 C 28.24 22.58 25.02 C++ 25.16 21.09 23.83 Go 30.25 19.26 28.66 Ruby 21.78 17.83 21.75 PHP 9.45 7.43 9.47 Average PPL ( ↓) 22.02 17.18 21.17 21",
  "text_length": 77003
}