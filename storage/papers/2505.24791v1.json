{
  "id": "http://arxiv.org/abs/2505.24791v1",
  "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective\n  Jacobi Decoding",
  "summary": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity.",
  "authors": [
    "Jiaru Zhang",
    "Juanwu Lu",
    "Ziran Wang",
    "Ruqi Zhang"
  ],
  "published": "2025-05-30T16:53:15Z",
  "updated": "2025-05-30T16:53:15Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24791v1",
  "full_text": "arXiv:2505.24791v1 [cs.LG] 30 May 2025Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding Jiaru Zhang Juanwu Lu Ziran Wang∗Ruqi Zhang∗ Purdue University {jiaru,juanwu,ziran,ruqiz}@purdue.edu Abstract Normalizing flows are promising generative models with advantages such as theo- retical rigor, analytical log-likelihood computation, and end-to-end training. How- ever, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advance- ments utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential depen- dency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method’s superlinear conver- gence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity. 1 Introduction Normalizing flow models have emerged as promising generative models by modeling the transforma- tion between a data variable and a latent Gaussian variable through a series of mappings, where the mapping functions are constructed as invertible functions parameterized by neural networks. This approach allows for end-to-end training with a single loss function, ensuring consistency between encoding and decoding. Moreover, the model’s structure facilitates analytical log-likelihood compu- tation. These benefits have attracted significant interest from the research community, positioning normalizing flow models as a compelling tool for generative modeling [5, 6, 9, 10, 19]. Even though normalizing flow models are theoretically rigorous, they often exhibit limited generation capabilities in practice due to the unavoidable architectural constraints imposed by the requirement for invertible mappings and Jacobian matrix calculation. For example, a classical construction of the invertible function, i.e., the affine coupling layer, is to split the input into two parts, and the output is obtained by the concatenation of those processed separately by trainable networks [ 5,6,10]. This formulation ensures the analytical solution of both the invertible function and Jacobian matrix, but also leads to relatively worse expressive power and network architecture compatibility, which affects its generation quality and practical applications. To overcome this, the recently proposed TarFlow [ 31] *Equal advising. Preprint. Under review. model uses a block autoregressive architecture drawing inspiration from autoregressive normalizing flows [ 11,20]. It splits the input into a longer sequence instead of a couple, and constructs the invertible function by masked autoregressive transformations. This autoregressive modeling can be naturally incorporated with the causal vision transformer architecture, which provides powerful representative power, making TarFlow achieve state-of-the-art performance in both density estimation and image synthesis. However, while such autoregressive sequential modeling endows the model with powerful generative capabilities, it results in a high parallel computational complexity during inference at the same time. Concretely, the inverse function of the sequential construction is an autoregressive inference process, meaning that each new patch is generated sequentially, relying on all of the previously generated patches. It limits parallel computation, hence leading to slow generation speeds and restricting practical applications, as also noted in previous work. In this work, we first identify that the strict sequential conditioning contains significant and layer- varying redundant dependencies for inference. Specifically, we observe that subsequent patches can still be approximated without the nearest preceding patches. Moreover, we find that the quantity of this redundancy varies substantially across different layers, where the later layer exhibits much more redundancy compared with the first layer. Motivated by these observations, we propose a selective Jacobi decoding (SeJD) approach to accelerate autoregressive flow inference. Our approach leverages parallel iterative optimization to achieve fast convergence towards high-fidelity samples, all without requiring additional training or modifications to the original model architecture, backed by theoretical convergence guarantees. Experimental results on diverse datasets such as CIFAR-10, CIFAR-100, and AFHQ verify the generality and effectiveness of the proposed acceleration approach. In summary, our key contributions include: •We discover that strict dependency on the original sequential inference of autoregressive normal- izing flows contains redundancy, and the quantity of the redundant varies across different layers, based on our theoretical analysis and empirical observation. •We introduce SeJD, an approach to accelerate inference in autoregressive normalizing flows by selectively applying Jacobi decoding in inference. Theoretical analysis demonstrates the superlinear convergence rate and finite convergence guarantee of SeJD. •We conduct comprehensive experiments to validate the effectiveness of SeJD, showing up to 4.7 times speed improvements with little impact on generation quality across diverse datasets including CIFAR-10, CIFAR-100, and AFHQ. The codes will be released upon acceptance. 2 Related Work Inference Acceleration of Generative Models. Inference acceleration is critical for the practical application of generative models, and researchers have proposed various effective strategies for different generative models. For Diffusion Models (DMs), techniques such as improved numerical solvers [ 7,15,25], knowledge distillation [ 22], and consistency modeling [ 26] have significantly reduced sampling times. For Variational Autoencoders (V AEs) and Generative Adversarial Networks (GANs), methods like network pruning [ 14,24], quantization [ 2], and knowledge distillation [ 1,30] are widely adopted to enhance inference efficiency. These approaches, however, typically leverage specific architectural properties or training paradigms inherent to these models. Consequently, they are generally not directly transferable to inference acceleration of autoregressive normalizing flows. To the best of our knowledge, this work is the first to explore the inference acceleration of normalizing flow models. Jacobi Decoding. Inspired by the research on non-linear equation solutions [ 17], Jacobi decoding methods have emerged as a promising direction for the acceleration of neural network inference. These methods aim to break the sequential dependency by reformulating generation as an iterative process of solving a system of equations, often framed as finding a fixed point. [ 27] firstly built the theoretical framework of interpreting feedforward computation as solving nonlinear equations and revealed the huge potential of Jacobi decoding on network structures such as RNNs and DenseNets. further confirms the effectiveness of Jacobi decoding on the language generation task. It was further improved with additional fine-tuning to keep the consistency of decoded tokens [ 12]. In image generation, [ 28] explored the inference acceleration by the combination of Jacobi decoding 2 with a probabilistic criterion for token acceptance on autoregressive text-to-image generation models. Although previously applied in language and image generation, vanilla Jacobi decoding methods commonly encountered issues like quality degradation [ 28] and minimal acceleration in discrete token spaces. 3 Selective Jacobi Decoding In this section, we introduce SeJD, a method designed to accelerate autoregressive normalizing flows. It directly addresses their inherent efficiency bottleneck by breaking the dependency among image patches during inference. We first present the preliminaries of normalizing flows and our problem settings (Sec. 3.1). Motivated by the analysis and observations of sequential redundancy and its depthwise heterogeneity (Sec. 3.2), we propose to break the sequential dependency by parallel inference using Jacobi iteration (Sec. 3.3). We show that this method has superlinear and finite convergence (Sec. 3.4). Moreover, we propose a selective strategy that only applies parallel Jacobi decoding on layers with higher redundancy based on the depthwise heterogeneity and improve the generative efficiency (Sec. 3.5). 3.1 Problem Settings A normalizing flow is a generative model that explicitly models a probability distribution p(x)by leveraging invertible functions and the change of variables law [ 5,21]. Suppose we have zK, a random variable following a distribution pK(zK)that is easy to sample from. Normalizing flow learns a cascade of intermediate functions such that x=F(zK) = (f1◦f2◦...◦fK)(zK). (1) As a result, the log-likelihood of the random variable xis given by logp(x) = log pK(zK)−log det∂ ∂zKF−1(zK) = log pK(zK)−KX k=1log det∂ ∂zkf−1 k(zk), (2) where zk−1=fk(zk)is the intermediate random variable. Properly constructing invertible functions fkfor high-dimensional variables is tricky. A commonly adopted method, coupling-based normalizing flow [ 5], splits the high-dimensional zkinto a pair of sub-variables zk= [zk,1,zk,2]⊺, and has inspired consecutive works that combine neural networks for modeling by constructing building blocks such as the real-valued non-volume preserving (RealNVP) transformation [ 6], invertible 1×1 convolution [ 10], and self-attention layers [ 9]. Nevertheless, splitting random variables into a pair of sub-variables limits their performance for more complicated data. More recent studies propose the autoregressive paradigm for normalizing flow by extending the above formulation and arbitrarily splitting random variables into a sequence of Lsub-variables [ 11,20]. The invertible transformation is defined by zk−1:=fk(zk) =fk  zk,1 zk,2... zk,L  = zk,1 (zk,2−gk(zk,<2))⊙exp(sk(zk,<2)... (zk,L−gk(zk,<L))⊙exp(sk(zk,<L) , (3) where s·(·)andg·(·)can be arbitrary functions. This naturally yields an inverse transformation: zk:=f−1 k(zk−1) = zk−1,1 zk−1,2⊙exp(−sk(zk,<2)) +gk(zk,<2)... zk−1,L⊙exp(−sk(zk,<L)) +gk(zk,<L) . (4) Autoregressive normalizing flows have shown superiority in density estimation and image gener- ation [ 31] by integrating with architectures such as causal vision transformer. However, a critical issue arises during inference, where data generation requires computing the inverse transformation. As suggested by Eq. (4), generating zk,ldepends on all previous generations zk,<l, which prohibits parallel computation and leads to slow sampling that restricts practical use cases. In the next section, we will present two observations that motivate our method design. 3 (a) CIFAR-10, groundtruth (b) CIFAR-10, o= 1 (c) CIFAR-10, o= 2 (d) CIFAR-10, o= 5 (e) AFHQ, groundtruth (f) AFHQ, o= 1 (g) AFHQ, o= 2 (h) AFHQ, o= 5 Figure 2: Generations where dependency on the nearest opatches is masked. It can still generate meaningful images, indicating the potential feasibility of acceleration with parallel computing. 3.2 Sequential Redundancy and Its Depthwise Heterogeneity Sequential Redundancy. As shown in Eq. (4), the generation process defined by the inverse transfor- mation reveals a sequential dependency between each patch and all preceding patches. Consequently, the original inference procedure enforces a strictly sequential generation paradigm, significantly limiting generation speed. We hypothesize that this strict dependence contains redundancies, espe- cially for data like images possessing inherent spatial locality and continuity. Therefore, an element zk,lmight be reasonably inferred even without precise, up-to-the-moment information from all its predecessors. To validate our hypothesis, we conducted experiments using a straightforward transformation during inference, where the transformation step for element zk,lis modified to ignore information from the o-nearest preceding elements in the sequence: zk,l=\u001azk−1,1, l = 1; zk−1,l⊙exp(−sk(zk,<(l−o)) +gk(zk,<(l−o)), l= 2,..., L.(5) 6 5 4 3 2 1 Layer index t0.60.70.80.9 Cosine Similarity (a) CIFAR-10 8 7 6 5 4 3 2 1 Layer index t0.70.80.9 Cosine Similarity (b) AFHQ Figure 1: Cosine similarities between layer outputs from stan- dard inference and inference with o= 5 nearest preceding dependencies masked.In our implementation, zk,<(l−o) is obtained by masking out the o- nearest preceding patches in the attention operation. The experi- mental results shown in Fig 2 in- dicate that while the image quality diminishes as more of these near- est preceding patches are removed, the model is still capable of gen- erating meaningful images. It sup- ports the hypothesis that the strict sequential dependency contains po- tentially exploitable redundancy. Depthwise Heterogeneity. We fur- ther investigate whether the degree of sequential redundancy varies across different layers during the generation process ( zK→ ··· → x). Theoretically, we expect heterogeneity: random variables 4 zKfrom the first layer, which performs structure initiation from a Gaussian noise, tends to exhibit high dependency on preceding patches, as the pure noise input contains theoretically no information and hence rely more on context given by proceeding generations zK,<l. Conversely, subsequent transformations refine informative outputs from the previous transformation zk+1,l, leading to weak sequential dependency regarding preceding generations zk,<l. To verify this, we measure the cosine similarity deviation between the standard inference outputs zkand those generated while masking the o= 5 nearest dependencies as shown in Eq. (5). As shown in Fig 1, the deviation is significantly larger for the first layer compared to subsequent ones. This result empirically confirms the low redundancy in the first layer, which is consistent with our expectation. The minimal deviation in subsequent layers aligns with their refinement role, which leverages informative inputs and the existing contextual structure. This observation motivates us to explore layer-specific optimizations for the generation process. 3.3 Parallel Inference by Jacobi Iteration Algorithm 1: Jacobi decoding for fk Input: Sequence zk+1, functions sk(·)andgk(·), threshold τ Output: Sequence zk Initialize z0 k=0,t= 0 while true t←t+ 1 zt k,1←zk+1,1 forl= 2,... L do in parallel zt k,l←zk+1,l⊙exp(−sk(zt−1 k,<l)) +gk(zt−1 k,<l) end for if∥zt k−zt−1 k∥∞< τ break end if end while zk=zt kOur empirical observations of sequential redundancy revealed a key property of in- ference in autoregressive normalizing flow: the generation of subsequent element zk,l exhibits a degree of robustness to inaccura- cies in the preceding elements zk,<l. This observation suggests that the strict, fully converged sequential dependency enforced by the standard inference procedure might contain redundancies and is not strictly nec- essary at every step for generation quality. This finding motivates exploring parallel computation strategies that can exploit this robustness. To enable parallelization, we first re- examine the inference task. As established, generating the target sequence zkfrom the input zk+1via the inverse transformation defined in Eq. (4) fundamentally requires finding the unique solution zkthat satisfies the entire set of autoregressive conditional dependencies. This can be formally viewed as solving a system of Lnon-linear equations Fl, implicitly defined for l= 1,..., L as Fl(zk,l,zk,<l,zk+1,l) = 0, (6) where Fk= 0represents the condition imposed by the k-th step of the inverse transform, given the known input zt+1[k]. The standard sequential inference method implicitly solves this system using a Gauss-Seidel-like approach, where the computation of zk,lrelies on the immediately preceding, fully computed values zk,1,..., zk,l−1. Leveraging the potential for parallelism indicated by our observations in Sec. 3.2, we propose employing the Jacobi iterative method to solve the system defined by Eq. (6). Instead of sequential updates, the Jacobi method performs updates iteratively and in parallel. Starting from an initial estimate zk,0, each iteration t+ 1 computes a new estimate zt+1 kwhere every element zt+1 k,lis calculated based only on the elements from the previous iterate zt kand the output from previous layer zk+1: Forl= 1,..., L, solve for zt+1 k,lfrom:Fk(zt+1 k,l,zt k,<l,zk+1,l) = 0. (7) Because the calculation of each zt+1 k,lwithin an iteration only depends on values from the completed previous iteration zt k, allLupdates can be computed concurrently, breaking the sequential bottleneck. This iterative process continues until a suitable stopping criterion is met, such as the norm of the difference between consecutive iterates ∥zt k−zt−1 k∥being sufficiently small. The whole process of applying Jacobi decoding in autoregressive normalizing flow inference is summarized in Alg. 1. 5 Remark. Jacobi decoding techniques have been explored in other generative modeling contexts, such as language models and autoregressive image generation [ 23,27]. However, the straightforward application of Jacobi decoding has often shown limited success, e.g., marginal speedups for language models [ 12] or compromised sample quality in image synthesis [ 28]. Despite these challenging precedents, we hypothesize that autoregressive normalizing flows possess characteristics that might mitigate such issues and make Jacobi iteration a more viable strategy. The redundant dependencies empirically observed suggest the system might tolerate the use of slightly inaccurate information zt k,<l from the previous iteration, which is fundamental to enabling Jacobi’s parallel updates. Moreover, the deterministic nature of inversion avoids the compounding sampling errors possible in stochastic models, and operating in continuous spaces could potentially allow for smoother iterative convergence than in discrete settings. These points provide a rationale for incorporating Jacobi decoding for autoregressive normalizing flows. 3.4 Convergence In this section, we theoretically analyze the convergence of the proposed Jacobi iterative inference method and identify two crucial properties: •The iteration exhibits local superlinear convergence under certain conditions, implying rapid convergence in practice. •Due to the inherent triangular dependency structure of autoregressive normalizing flow, the iteration is guaranteed to converge to the exact solution with no more than the number of iterations of the original inference, providing a worst-case bound. Firstly, we analyze the local convergence behavior. Assuming an appropriate initialization close to the true solution, the Jacobi iteration converges superlinearly. Proposition 3.1 (Superlinear Convergence Speed).For the iterative sequence zt kdefined in Alg. 1, ∃δ >0, s.t.∀zk,0satisfies ||zk,0−zk||< δ, the iterative sequence {zt k}converges to zk, with superlinear convergence rate, i.e., ∥zt+1 k−zk∥=o(∥zt k−zk∥). (8) Appendix A provides the detailed proof. Beyond the convergence rate, the specific structure of autoregressive normalizing flow inference provides a strong global guarantee. The task is equivalent to solving a triangular system of equations, where the l-th unknown patch depends only on preceding unknowns. This structural property, also noted in [ 23,27], ensures that the Jacobi method converges to the exact solution in at most Literations for a sequence with length L. Proposition 3.2 (Finite Convergence Guarantee).For the iterative sequence zt kdefined in Alg. 1. Denoting the sequence length of zkasL, we have zL k=zK. This finite convergence occurs because the triangular structure allows information to propagate definitively through the sequence during the updates. Appendix A presents a formal proof. Prop. 3.2 thus provides a strict upper bound on the computational steps needed to reach the exact solution. These propositions jointly establish the inherent efficiency of Jacobi decoding. Prop. 3.1 reveals that the error reduces with a superlinear rate. This indicates the convergence can be fast, as each iteration t < L yields an increasingly substantial reduction in their respective errors [ 4]. On the other hand, Prop. 3.2 further presents that the convergence to the exact solution zkis guaranteed in at most Literations. These results confirm the potential of applying Jacobi decoding for fast inference in autoregressive normalizing flows. 3.5 Selective Layer Processing While the theoretical analysis shows promise for Jacobi iteration, its uniform application involves practical trade-offs. Concretely, in one iteration, we need to update all Lelements in the sequence at the same time, which trades off memory for time required by one iteration in the original sequential iterations, where only one sequence element is required to be updated. Moreover, the commonly used key-value (KV) cache for optimized attention operator [ 18] is not directly applicable to Jacobi decoding, as the decoded patches are approximated and must be updated in each iteration. These 6 limitations are particularly relevant where dependencies are strong, potentially making the uniform Jacobi decoding approach slower than optimized sequential decoding in such scenarios. Motivated by this trade-off and the presented depthwise heterogeneity of sequential redundancy in Sec. 3.2, we exploit the fact that the first network layer often exhibits stronger dependencies and propose our SeJD method. Specifically, it utilizes standard sequential decoding for the first layer, which is a dependency-heavy layer. Subsequently, it switches to the parallel Jacobi iteration only for the remaining layers where higher redundancy is expected. It allows for the benefits of parallelism while avoiding the high additional computational costs in the first layer with stronger dependencies. This selective application aims to achieve more effective overall inference acceleration by strategically utilizing the Jacobi decoding method on layers with more redundant dependencies. 4 Experiments 4.1 Experimental Settings We apply our proposed SeJD approach to the state-of-the-art TarFlow model [ 31]. Experiments were conducted on CIFAR-10 and CIFAR-100 [ 13] with models trained from scratch, and AFHQ [ 3] with the released TarFlow checkpoint. All results are reported relative to the standard sequential inference baseline of TarFlow, and the uniform Jacobi decoding (UJD) method, measured on two L40S GPUs. The default stopping threshold τfor Jacobi iterations is set as 0.5. More experimental details including network architectures and hyperparameters are available in Appendix B. Our evaluation covers both computational efficiency and the quality of the generated samples. Generation speed is quantified by the average inference time for one batches, and the overall speedup factor relative to the sequential baseline. For generative quality, we utilize the Fréchet Inception Distance (FID) [ 8], a widely adopted metric that measures the perceptual similarity between the distribution of generated images and the real data distribution. For a more comprehensive view of perceptual quality, we report another two no-reference metrics: CLIP-IQA [ 29], which assesses quality based on alignment with CLIP embeddings, and BRISQUE [ 16], a blind image quality assessor sensitive to common distortions. 4.2 Comparisons on Autoregressive Normalizing Flow Inference We evaluate our SeJD method against the standard sequential baseline and the UJD method. As illustrated in Fig 3 and Appendix Sec. E, visual inspection of samples confirms that our SeJD maintains high perceptual fidelity, producing outputs visually comparable to the original sequential generations. This qualitative observation is confirmed by quantitative analysis detailed in Tab. 1. Metrics such as FID, CLIP-IQA, and BRISQUE indicate that generative quality remains largely preserved with both UJD and SeJD methods across all scenarios, showing small degradation compared to the sequential baseline. However, while UJD demonstrates acceleration benefits on the smaller CIFAR-10 and CIFAR-100 datasets, it fails on the larger AFHQ dataset, resulting in inference times slower than the sequential baseline. This poorer performance on AFHQ is likely attributable to higher per-iteration computa- tional costs combined with potentially stronger dependencies negating parallel gains. Conversely, our SeJD strategy consistently achieves substantial speedups across all datasets by selectively applying Jacobi iterations. Notably, SeJD reaches up to 4.7 times acceleration compared to the sequential baseline, as shown in Tab. 1, highlighting its effectiveness. This confirms that the proposed selec- tive approach is crucial for achieving effective and generalizable acceleration without sacrificing generation quality. 4.3 Verification of Analysis To experimentally validate our theoretical analysis and empirical observations, we analyze the convergence dynamics of the Jacobi iterations. Fig 4 plots the error measured as the L2 norm of the difference between the iterate zt iand the groundtruth ztfrom sequential inference during the iteration process across different layers on AFHQ dataset. As a reference, we also present the error variation of the original sequential inference, where the uninference patches are regarded as the input patches zt+1according to the default implementation for calculation. The results clearly show a rapid 7 (a) Sequential (b) SeJD, 4.5 times acceleration Figure 3: Visualization comparison on AFHQ. SeJD accelerates generation by 4.5 times while keeping the generation quality and fidelity. Additional visual comparisons on CIFAR-10 and CIFAR-100 are available in Appendix Sec. E. Table 1: Comparison of sequential inference, uniform Jacobi decoding, and our SeJD approach. The subscript indicates the maximum deviation in three runs. Configuration Generation Speed Generation Quality Dataset Method Time (s) ↓Speed Up ↑ FID↓ CLIP-IQA ↑BRISQUE ↑ CIFAR-10Sequential 9.56±0.42 1.0× 9.71±0.03 0.35±0.00 56.35±0.21 UJD 3.92±0.09 2.4× 10.19±0.03 0.35±0.00 56.79±0.20 SeJD (Ours) 2.63±0.13 3.6× 10.20±0.03 0.35±0.00 56.78±0.19 CIFAR-100Sequential 9.57±0.27 1.0× 8.22±0.03 0.35±0.00 57.75±0.12 UJD 3.30±0.10 2.9× 8.26±0.10 0.35±0.00 57.76±0.06 SeJD (Ours) 2.04±0.03 4.7× 8.19±0.16 0.35±0.00 57.78±0.12 AFHQSequential 186.28 1.0× 15.42 0.63 15.55 UJD 219.24 0.8× 15.44 0.63 15.44 SeJD (Ours) 41.21 4.5× 15.44 0.63 15.56 decrease in error for the Jacobi process, often reaching nearly zero error substantially fewer than the theoretically worst case bound Kiterations, providing empirical support for the fast convergence properties discussed earlier. Furthermore, Fig 4 reveals distinct convergence behavior across layers. The error associated with the first layer decreases noticeably more slowly via Jacobi iterations compared to subsequent layers. This directly validates our observation in Sec. 3.2 of stronger dependencies in the initial layer and empirically confirms the rationale behind the selective strategy, which applies parallel iterations to the faster-converging later layers. These results confirm the layer-wise dependency differences, verifying the effectiveness of our SeJD method. 4.4 Ablation Study Influence of τ.To further understand the impact of the stopping threshold hyperparameter τ, we perform an ablation study. We vary the value of τand measure the resulting generative quality by FID 8 100101102103102103 Jacobi Sequential(a) Layer 1 101103102 100102 Jacobi Sequential (b) Layer 2 101103102 100102 Jacobi Sequential (c) Layer 3 101103102 100102 Jacobi Sequential (d) Layer 4 101103102 100102 Jacobi Sequential (e) Layer 5 101103102 100102 Jacobi Sequential (f) Layer 6 101103102 100102 Jacobi Sequential (g) Layer 7 101103102 100102 Jacobi Sequential (h) Layer 8 Figure 4: Convergence dynamics of Jacobi decoding across network layers. Plot shows error (measured by norm of difference between current iterate vs. sequential output) variation during iteration, demonstrating fast overall convergence and the notably slower convergence of the first layer. 0.2 0.5 1.0 2.0050100150200250FID FID Time 2.03.04.0 Time (s) (a) CIFAR-10 0.2 0.5 1.0 2.0255075100125FID FID Time 1.901.952.002.052.102.152.20 Time (s) (b) CIFAR-100 0.2 0.5 1.0 2.015.415.615.816.016.2FID FID Time 100 405060708090200 Time (s) (c) AFHQ Figure 5: Ablation study on the stopping threshold τ: FID scores and inference times for SeJD across different τvalues, illustrating the speed-quality trade-off. and inference time. The results, illustrating the trade-off between these two metrics, are presented in Fig 5. As expected, increasing the threshold τallows the parallel iterations to terminate earlier, which significantly reduces the overall inference time. However, allowing larger differences between consecutive iterates before stopping can lead to a less precise generation. This is reflected in the FID scores, which tend to increase as τbecomes larger. Notably, the results show that for values τ below 1.0, the increase in FID is relatively gradual, while the reduction in inference time remains substantial. This supports that, with an appropriately chosen τ, SeJD effectively increases generation speed with only a minor impact on generation quality. τ= 0.5consistently provides a favorable balance, achieving considerable acceleration while maintaining generative quality close to the baseline. Therefore, we adopt τ= 0.5as the default setting for all other experiments presented in this paper. 5 Conclusion In this paper, we first observed the dependency redundancy within the autoregressive normalizing flow model and its significant variation across different layers. Based on this observation, we proposed the selective Jacobi decoding (SeJD) approach, which uses a selectively applied parallel Jacobi decoding method to non-first layers with high dependency redundancy for inference acceleration. Theoretical analysis demonstrated the superlinear convergence speed of the proposed approach and provided the worst-case guarantee on the total required iterations. Comprehensive experiments verified the correctness of the theoretical analysis and demonstrated that SeJD achieves significant inference acceleration across multiple scenarios, enhancing the practical value of normalizing flow models. For discussions about current limitations and future work, please refer to Appendix C. 9 References A. Aguinaldo, P.-Y. Chiang, A. Gain, A. Patil, K. Pearson, and S. Feizi. Compressing GANs using Knowledge Distillation. arXiv preprint arXiv:1902.00159, 2019. P. Andreev and A. Fritzler. Quantization of Generative Adversarial Networks for Efficient Inference: A Methodological Study. In ICPR, 2022. Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha. StarGAN v2: Diverse Image Synthesis for Multiple Domains. In CVPR, 2020. J. E. Dennis and R. B. Schnabel. Numerical Methods for Unconstrained Optimization and Nonlinear Equations (Classics in Applied Mathematics, 16). SIAM, 1996. L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear Independent Components Estimation. InICLR Workshop, 2015. L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density Estimation using Real NVP. In ICLR, 2017. T. Dockhorn, A. Vahdat, and K. Kreis. GENIE: Higher-Order Denoising Diffusion Solvers. In NeurIPS, 2022. M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In NeurIPS, volume 30, 2017. J. Ho, X. Chen, A. Srinivas, Y. Duan, and P. Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In ICML, 2019. D. P. Kingma and P. Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In NeurIPS, 2018. D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved Variational Inference with Inverse Autoregressive Flow. In NeurIPS, 2016. S. Kou, L. Hu, Z. He, Z. Deng, and H. Zhang. CLLMs: Consistency Large Language Models. InICML, 2024. A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto, 2009. A. Kumar, K. Anand, S. Mandloi, A. Mishra, A. Thakur, N. Kasera, and A. Prathosh. CoroNet- GAN: Controlled Pruning of GANs via Hypernetworks. In ICCV Workshop, 2023. C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. NeurIPS, 2022. A. Mittal, A. K. Moorthy, and A. C. Bovik. No-Reference Image Quality Assessment in the Spatial Domain. IEEE Transactions on image processing, 21(12):4695–4708, 2012. J. M. Ortega and W. C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. SIAM, 2000. M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling. In NAACL, 2019. G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normal- izing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning Research, 22(57):1–64, 2021. G. Papamakarios, T. Pavlakou, and I. Murray. Masked Autoregressive Flow for Density Estimation. In NeurIPS, 2017. D. Rezende and S. Mohamed. Variational Inference with Normalizing Flows. In ICML, 2015. T. Salimans and J. Ho. Progressive Distillation for Fast Sampling of Diffusion Models. In ICLR, 2022. 10  A. Santilli, S. Severino, E. Postolache, V. Maiorca, M. Mancusi, R. Marin, and E. Rodola. Accelerating Transformer Inference for Translation via Parallel Decoding. In ACL, 2023. D. Saxena, J. Cao, J. Xu, and T. Kulshrestha. RG-GAN: Dynamic Regenerative Pruning for Data-Efficient Generative Adversarial Networks. In AAAI, 2024. J. Song, C. Meng, and S. Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021. Y. Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency Models. In ICLR, 2023. Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating Feedforward Computation via Parallel Nonlinear Equation Solving. In ICML, 2021. Y. Teng, H. Shi, X. Liu, X. Ning, G. Dai, Y. Wang, Z. Li, and X. Liu. Accelerating Auto- regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding. In ICLR, 2025. J. Wang, K. C. Chan, and C. C. Loy. Exploring CLIP for Assessing the Look and Feel of Images. InAAAI, 2023. S. Yeo, Y. Jang, and J. Yoo. Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation. In ECCV, 2024. S. Zhai, R. Zhang, P. Nakkiran, D. Berthelot, J. Gu, H. Zheng, T. Chen, M. A. Bautista, N. Jaitly, and J. Susskind. Normalizing Flows are Capable Generative Models. arXiv preprint arXiv:2412.06329, 2024. 11 A Theoretical Proofs We first formally redefine our Jacobi iteration map as the function F(·). Definition A.1 (Jacobi Iteration Map).Letzk+1be a given vector sequence of length L, and let sk, gkbe given functions. The iteration map Fis defined component-wise for zas: F(z)l=\u001azk+1,1 l= 1 zk+1,l⊙exp\u0000 −sk(z<l)\u0001 +gt(z<l)l= 2,..., L(9) where z<l:= [z1,..., z l−1]T. The iterative sequence is generated by zt+1 k=F(zt k)with initial z0 k. It is easy to observe that there exists a fixed point z∗ kfor this iteration. For example, the output zkfrom the sequential decoding approach in Eq. (4) is a fixed point. Moreover, as we use neural networks as parameterized functions skandgk, the map Fis continuously differentiable. Under these observations, we have the iterative sequence zt kconverges to z∗ kwith at least superlinear convergence rate starting from a close initial sequence, as shown in the following theorem. Proposition A.1 (Superlinear Convergence Rate).There exists δ >0such that if z0 k−z∗ k < δ, the iterative sequence {zt k}converges to z∗ kwith at least a superlinear convergence rate. This means that the error satisfies: ∥zt+1 k−z∗ k∥=o(∥zt k−z∗ k∥)aszt k→z∗ k. (10) Proof. Denote et=zt k−z∗ kthe approximation error at iteration t. The structure of the iteration map F(z)(as detailed in prior definitions: F(z)1=zk+1,1is constant, andF(z)l=F(z<k)forl= 2,..., L ) ensures that the Jacobian matrix JF(z)is strictly lower tri- angular for any z. This implies that JF(z∗ k)is strictly lower triangular. Consequently, all eigenvalues ofJF(z∗ k)are zero. Therefore, the spectral radius of the Jacobian at the fixed point is ρ(JF(z∗ k)) = 0. (11) Assuming FisC1-diffeomorphism in a neighborhood of z∗ k, the Taylor’s theorem allows us to expand F(zt k)around z∗ k. Forzt ksufficiently close to z∗ k: F(zt k) =F(z∗ k) +JF(zk∗)(zt k−z∗ k) +o( zt k−z∗ k ). (12) Using the iteration definition zt+1 k=F(zt k)and the property at fixed-point z∗ k=F(z∗ k), we have: zt+1 k−z∗ k=F(zt k)−F(z∗ k) =JF(z∗ k)(zt k−z∗ k) +o( zt k−z∗ k ). (13) This yields an error propagation dynamic: et+1=JF(z∗ k)et+o(∥et∥). (14) Standard theorems on iterative methods (e.g., results related to Q-order of convergence as found in previous work [ 17], see discussion around Theorem 10.1.4 for Q-superlinear) state that if an iteration converges. Its error satisfies the relationship (14), then the convergence is Q-superlinear if and only if ρ(JF(z∗ k)) = 0. Meanwhile, the condition ρ(JF(z∗ k))<1(which is satisfied here since ρ= 0) ensures that z∗ kis a point of attraction, so for z0 ksufficiently close to z∗ k, the sequence {zt k}is guaranteed to converge at z∗ k. Given that ρ(JF(z∗ k)) = 0, the cited convergence theory directly implies that the iteration is Q- superlinear. By definition, Q-superlinear convergence means that et+1 =o(∥et∥)aset→0. Therefore, we conclude that: ∥zt+1 k−z∗ k∥=o( zt k−z∗ k )aszt k→z∗ k. (15) This demonstrates at least a superlinear convergence rate. Proposition A.2 (Finite Convergence Guarantee).For iteration map Fand iterative sequence {zt k}t≥0defined in Definition A.1, the iteration converges to the fixed point in at most Ksteps: zt k=z∗ k∀t≥K. (16) 12 Proof. The core property is that the l-th component of the output, (F(z))l, depends only on the first l−1components of the input z, specifically z<k. We also know that (F(z0 k))1=zk+1,1=z∗ k,1. We will prove by induction on the iteration step t(from t= 1tot=K) that the first tcomponents of the iterate zt kmatch those of the fixed point z∗ k. LetP(t)be the statement: P(t): zt k,l=z∗ k,l∀1≤l≤t. (17) It is easy to check that the statement P(1)holds. By assuming that P(t)holds for 1≤t < K, that is,zt k,l=z∗ k,lfor all 1≤l≤t, we want to show that P(t+ 1) holds, meaning zt+1 k,l=z∗ k,lfor all 1≤l≤t+ 1. Since zt+1 k=F(zt k): •For1≤l≤t:The calculation of zt+1 k,l= (F(zt k))ldepends only on zt k,<l. Given j < l≤t, the inductive hypothesis P(t)implies zt k,j=z∗ k,jfor these components. Thus, zt k,<l=z∗ k,<l. Because (F(·))lonly depends on these first l−1components, we have (F(zt k))l= (F(z∗ k))l. (18) Since z∗ kis a fixed point hence F(z∗ k) =z∗ k), we have zt+1 k,l= (F(z∗ k))l=z∗ k,l. (19) •Forl=t+1:The calculation of zt+1 k,t+1= (F(zt k))t+1depends only on zt k,<t +1. The components in this sub-vector are zt k,jforj= 1,..., t. By the inductive hypothesis P(t), these are equal to the corresponding components of z∗ k. Therefore, zt k,<t +1=z∗ k,<t +1. Because (F(·))t+1only depends on the first tcomponents (F(zt k))t+1= (F(z∗ k))t+1 (20) Using the fixed-point property: zt+1 k,t+1= (F(z∗ k))t+1=z∗ k,t+1. (21) This indicates that the (i+ 1) -th component becomes correct at step i+ 1. Combining (19) and (21), we show that zt+1 k,l=z∗ k,l,∀1≤l≤t+ 1. Thus, P(t+ 1) holds. By mathematical induction, P(t)holds for all t= 1,..., L. In particular, P(L)holds: zK k,l=z∗ k,l∀1≤l≤L. (22) This implies the entire vector is guaranteed to match the fixed point after Lsteps: zL k=z∗ k. (23) Assume zt k=z∗ kfor some t≥L. Then in the next iteration: zt+1 k=F(zt k) =F(z∗ k) =z∗ k (24) For the same reason, if the sequence reaches z∗ kat step L, it remains at z∗ kfor all subsequent steps. Therefore, it is shown that zt k=z∗ k∀t≥L. (25) B Experimental Details Model Details. The network architectures for our baseline models are adopted from the publicly available implementation of TarFlow1. For experiments on the AFHQ dataset, we utilize the pre- trained checkpoint released by the TarFlow authors. Due to computational resource constraints, 1https://github.com/apple/ml-tarflow 13 training models on the ImageNet dataset according to the original TarFlow configurations was not feasible within the scope of this work. For experiments conducted on the CIFAR datasets, we largely follow the default settings provided by TarFlow, with specific adjustments. Concretely, the numbers of layers and blocks are 6. The number of channels is 256. The patch size is 2. These modifications are implemented to better suit our experimental objectives or resource availability. Evaluation Details. To estimate generation speed, we calculate the average time cost per batch over ten distinct runs. For Fréchet Inception Distance (FID) estimation, we compute the distance between the original dataset and a generated set of samples equal in size to the original dataset, adhering to the standard FID definition. To evaluate generation quality, we use metrics such as CLIP-IQA and BRISQUE, calculating the average score for each metric on the set of generated samples matching the original dataset size. These approaches involving averaging and the use of comprehensive sample sizes ensure stable representative results. The evaluation methods employed, which involve averaging results across multiple batches and utilizing extensive datasets, can ensure representative results. It is consistent with established practices in the literature [ 27,28]. For CIFAR datasets, we additionally report the maximum deviation in three runs. The magnitude of this deviation is considerably smaller than the performance differentials observed between methods, thereby affirming the statistical significance of our comparative results and the validity of the reported enhancements. C Limitations and Future Work While our proposed SeJD demonstrates promising improvements in inference acceleration for autore- gressive normalizing flow models, this work also highlights several interesting open problems. Firstly, although sequential and depthwise redundancy is commonly observed in trained models, it remains unknown whether this redundancy persists to the same extent in partially-trained or under-trained models. This uncertainty might affect SeJD’s effectiveness when models are under-fitting or in earlier stages of training. Secondly, this paper focuses exclusively on inference-time acceleration. The potential for leveraging SeJD’s principles to optimize the training process or guide neural architecture design has not been explored. Future work could further investigate the nature of these observed sequential and depthwise redundancy. Such insights might then be leveraged to guide model training strategies and inform architectural design, potentially leading to models that are inherently more efficient. D Broader Impact This paper focuses on accelerating the inference of autoregressive models by the proposed SeJD approach. It offers significant promise for advancing the utility and efficiency of normalizing flow models. Its broader impact will likely be positive, fostering innovation and new applications of autoregressive normalizing flows. E Additional Experimental Results We provide more visualized experimental results on CIFAR-10 and CIFAR-100 in Fig 6 and Fig 7. All results consistently confirm the little impact of SeJD on generation quality. 14 (a) Sequential (b) SeJD, 3.6 times acceleration Figure 6: Visualization comparison on CIFAR-10. (a) Sequential (b) SeJD, 4.7 times acceleration Figure 7: Visualization comparison on CIFAR-100. 15",
  "text_length": 45277
}