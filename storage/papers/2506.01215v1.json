{
  "id": "http://arxiv.org/abs/2506.01215v1",
  "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers",
  "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
  "authors": [
    "Woomin Song",
    "Sai Muralidhar Jayanthi",
    "Srikanth Ronanki",
    "Kanthashree Mysore Sathyendra",
    "Jinwoo Shin",
    "Aram Galstyan",
    "Shubham Katiyar",
    "Sravan Babu Bodapati"
  ],
  "published": "2025-06-01T23:49:14Z",
  "updated": "2025-06-01T23:49:14Z",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01215v1",
  "full_text": "--- Page 1 ---\nCompress, Gather, and Recompute:\nREFORMing Long-Context Processing in Transformers\nWoomin Song1 2†Sai Muralidhar Jayanthi1Srikanth Ronanki1Kanthashree Mysore Sathyendra1\nJinwoo Shin2Aram Galstyan1Shubham Katiyar1Sravan Babu Bodapati1\nAbstract\nAs large language models increasingly gain pop-\nularity in real-world applications, processing\nextremely long contexts, often exceeding the\nmodel’s pre-trained context limits, has emerged\nas a critical challenge. While existing approaches\nto efficient long-context processing show promise,\nrecurrent compression-based methods struggle\nwith information preservation, whereas random\naccess approaches require substantial memory\nresources. We introduce REFORM, a novel in-\nference framework that efficiently handles long\ncontexts through a two-phase approach. First,\nit incrementally processes input chunks while\nmaintaining a compressed KV cache, constructs\ncross-layer context embeddings, and utilizes early\nexit strategy for improved efficiency. Second, it\nidentifies and gathers essential tokens via simi-\nlarity matching and selectively recomputes the\nKV cache. Compared to baselines, REFORM\nachieves over 50% and 27% performance gains\non RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on\n∞-Bench and MM-NIAH, demonstrating flexibil-\nity across diverse tasks and domains. Addition-\nally, REFORM reduces inference time by 30%\nand peak memory usage by 5%, achieving both\nefficiency and superior performance.\n1. Introduction\nThe ability to handle extremely long contexts, often ex-\nceeding the original model’s pre-trained context limits, has\nemerged as a critical challenge for the advanced usage of\nlarge language models (LLMs) in real-world scenarios. This\ncapability is essential for various applications, such as pro-\ncessing life-long user interactions, understanding and debug-\n†Work done during an internship at Amazon.1Amazon\nAGI2KAIST. Correspondence to: Sravan Babu Bodapati <sra-\nvanb@amazon.com >.ging repository-level codebases, and handling multi-modal\ninputs (interleaved sequences of text and visual informa-\ntion can result in extremely long contexts). However, under\nexisting Transformer-based language model architectures\n(Dubey et al., 2024; Jiang et al., 2023), processing such\nlong sequences often causes significant computational chal-\nlenges, requiring substantial computation as well as memory\nresources. These demanding requirements often prove in-\nfeasible in practical deployment settings, necessitating new\ntechnologies that can handle extremely long sequences with\nreasonable computational resources.\nCurrent approaches to efficient infinite-context processing\nbroadly fall into two categories: recurrent context process-\ning and random access mechanisms. Recurrent methods\n(Xiao et al., 2023a; Zhang et al., 2023; Oren et al., 2024;\nKim et al., 2024) divide the input into manageable chunks\nand iteratively process them while maintaining a summa-\nrized representation of prior chunks, typically by compress-\ning or evicting parts of the Key-Value (KV) cache. While\nthese approaches reduce memory and computational costs,\nthey often suffer from ‘forgetting’ due to the loss of critical\ninformation during compression and/or eviction.\nIn contrast, another line of work aims to enable dynamic\nrandom-access to the previous inputs by preserving the full\nKV cache and retrieving relevant portions when processing\nnew chunks (Xiao et al., 2024a; Liu et al., 2024). These\nmethods provide more flexibility in accessing prior con-\ntext, as they allow selective re-attention to specific segments\nof the input. However, maintaining the full KV cache re-\nquires substantial memory resources, often leading to sig-\nnificant memory overhead and latency increases, especially\nin practical deployments where CPU memory offloading\nis necessary. Furthermore, the increased flexibility does\nnot necessarily lead to high retrieval performance. These\nlimitations highlight the need for a more balanced approach\nthat combines efficiency with precise long-context handling.\nTo address the above challenges, we propose REFORM\n(REcurrent chunked Forwarding with On-demand cache\nRecoMputation), a novel inference framework that com-\nbines the efficiency of recurrent approaches with the su-\nperior recall capabilities of random-access methods. Our\n1arXiv:2506.01215v1  [cs.CL]  1 Jun 2025\n--- Page 2 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nTransformer LayersContext Encoding with Recurrent Chunked Forwarding\nCurrent Chunk…Last KV CacheOn-Demand Cache Recomputation\nPrevious ChunksQueryChunk N1. Forward current chunk conditioned on previous KV cache3. Compress KV cacheusingtoken eviction2. Gather & store selected QKV statesNew KV CacheCross-Layer Context EmbeddingsRecomputed KV Cache\nGathered Inputs4. Gather input tokens usingcosine-similarity searchwith query embeddings5. Recompute KV cache with the gathered inputs\nFigure 1. An overview of the proposed framework. REFORM efficiently processes long inputs through two phases. In the recurrent\nchunked forwarding phase, it segments inputs into chunks and processes them iteratively. In each iteration, REFORM (1) forwards each\nchunk conditioned on the previous KV cache, (2) extracts key QKV states from selected layers and heads for constructing cross-layer\ncontext embeddings, and (3) compresses the cache via token eviction (Zhang et al., 2023). An early exit strategy skips upper layers\nbeyond those used for embedding collection, further improving efficiency. In the on-demand cache recomputation phase, REFORM\nselects important tokens via cosine similarity search with the query embeddings (last part of the input), gathers them, and recomputes the\nKV cache for further generation.\napproach implements a computationally efficient compress-\ngather-recompute pipeline that processes long contexts\nthrough two key phases, as illustrated in Figure 1.\nIn the encoding phase, we process input tokens in chunks\nthrough an adaptive caching mechanism called recurrent\nchunked forwarding: as each chunk is processed, tokens\nare added to the KV cache and sparsified by retaining only\nthe heavy hitters (most influential tokens). Using this pro-\ngressively sparsified KV cache, we compute representa-\ntions up to an intermediate transformer layer L, collecting\nQKV states from multiple layers and heads to generate and\nstore lightweight cross-layer context embeddings for all to-\nkens. This multi-faceted efficiency strategy—combining\nchunked processing, sparse KV cache updates, and early\nexit—significantly reduces both computation time and mem-\nory overhead, as we maintain only small representations for\nretrieval while dynamically managing KV cache sparsity.\nIn the recomputation phase, the query tokens (correspond-\ning to the recent context) identify relevant historical tokens\nthrough cosine similarity matching with the stored retrieval\nembeddings, and only these selected tokens undergo full\nKV cache recomputation across all layers. While this phase\nrequires full computation for selected tokens, this recompu-\ntation is crucial: it restores high-fidelity representations for\ncontextually important tokens, ensuring accurate processing\nof long-range dependencies. By selectively recomputing\nonly the most relevant tokens, we achieve an optimal balance\nbetween computational efficiency and model performance,\nallowing detailed historical context access while avoidingthe costs of maintaining full representations for all tokens.\nOur extensive evaluations demonstrate REFORM’s effec-\ntiveness across various long-context understanding tasks. In\nneedle-in-a-haystack tests, REFORM achieves perfect recall\nfor contexts up to 1 million tokens at various depths. On\nmore complex benchmarks, REFORM significantly outper-\nforms existing methods, achieving over 50% performance\ngain on RULER and 27% on BABILong at 1M context\nlengths with the Mistral-NeMo-Instruct-2407 (Jiang et al.,\n2023) model, compared to the best-performing baselines.\nOn more realistic tasks from ∞-Bench, REFORM achieves\n31.2% average performance, substantially exceeding the\nbaseline performance of 24.8%.\nOperating at the transformer architecture level, our method\nis modality-agnostic and applicable to any domain/modality\nthe base model supports, making it particularly valuable\nfor real-world applications requiring extensive context un-\nderstanding. To demonstrate the flexibility of our method,\nwe evaluate in on three multi-modal needle-in-a-haystack\ndatasets, achieving 57.51% performance with the Pixtral-\n12B-2409 (Agrawal et al., 2024) model, exceeding the base-\nline performance of 52.95%.\nREFORM also delivers substantial efficiency improvements\nover recent state-of-the-art long-context processing meth-\nods. Compared to InfLLM (Xiao et al., 2024a) and InfiniPot\n(Kim et al., 2024), REFORM reduces inference time by\n80% and 33%, respectively, in evaluations with 256k token\ninputs. Additionally, REFORM operates with lower mem-\nory requirements, reducing peak memory usage by 32% and\n2\n--- Page 3 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\n5%, compared to InfLLM and InfiniPot, respectively. These\nresults demonstrate that REFORM effectively combines the\nbenefits of both recurrent compression and random access\napproaches while mitigating their respective limitations.\n2. Related Works\nIn this section, we discuss the existing approaches to extend\nexisting models to efficiently handle extremely long inputs.\nWe categorize these methods into two groups: methods that\nuse recurrent context processing and methods that allow ran-\ndom access. We provide a more comprehensive discussion\non related works in Appendix A.\nRecurrent context processing. To address the computa-\ntional challenges of long-context processing, several studies\nexplore the use of recurrence for greater efficiency. A line\nof works (Dai et al., 2019; Bulatov et al., 2022; Munkhdalai\net al., 2024) introduce architectural changes to Transform-\ners, enabling chunk-level recurrence operations to process\nlong contexts in smaller, manageable units. However, these\napproaches typically necessitate extensive training of the\nmodel, and therefore is not directly applicable to existing\npre-trained large language models. More recent efforts lever-\nage KV cache eviction to iteratively encode input chunks\nand compress the KV cache, avoiding architectural modifi-\ncations or additional training. For instance, StreamingLLM\n(Xiao et al., 2023a) maintains fluent generation by preserv-\ning initial and most recent tokens while compressing inter-\nmediate ones. Later approaches (Zhang et al., 2023; Oren\net al., 2024; Kim et al., 2024) identify important tokens from\nthe prior context, enabling more informative cache compres-\nsion. Despite their efficiency, the process of compressing\nprior inputs often results in the loss of critical information,\nleading to ‘forgetting’ issues. Consequently, these methods\nmay struggle with tasks requiring precise retrieval of earlier\ninputs.\nRandom access approaches. An alternative direction in-\nvolves enabling random access to prior context, akin to\nfull attention, but in a more computationally efficient man-\nner. These methods typically store the full KV cache in\nmemory and dynamically retrieve relevant tokens as needed.\nSome approaches train the model (Wu et al., 2022) or an\nauxiliary side-network (Wang et al., 2023b) to utilize the\nretrieved tokens effectively. More recently, training-free\nstrategies have emerged, which store the full KV cache in\nmemory and retrieve it dynamically (Xiao et al., 2024a; Liu\net al., 2024). While these methods allow random access to\nany part of the input sequence, they introduce significant\nmemory overhead due to the need to maintain large caches.\nIn practice, this often necessitates CPU offloading, which\ncan further increase latency. Furthermore, the flexibility to\naccess previous context may not necessarily lead to high\nretrieval performance.Algorithm 1 Overview of REFORM\nprocedure FORWARD CHUNK (chunk ,cache,emb)\n/*Initialize hidden states */\nhs←input\n/*Forward with early exit */\nforlayer inmodel layers[:early exitlayer] do\nhs,cache,qkv←layer.Forward (hs,cache )\n/*Save selected embeddings */\nemb.SaveSelected (qkv)\nend for\n/*Evict less important tokens */\ncache←Compress (cache )\nreturn cache,emb\nend procedure\nprocedure REFORM(input)\n/*Initialize */\ncache,emb←EmptyInit()\n/*Prepare input chunks */\ncontext ,query←SplitQuery (input )\nchunks ←ChunkInputs(context) + [query]\n/*Recurrent chunked forwarding */\nforciinchunks do\ncache,emb←ForwardChunk (ci,cache,emb )\nend for\n/*Gather relevant inputs */\nrelevant inputs ←GatherRelevant(input, emb)\n/*On-demand recomputation */\ncache←model.Forward (relevant inputs )\nreturn cache\nend procedure\n3. Method\nIn this section, we present the details of our proposed\nmethod. In Section 3.1, we first describe REFORM’s recur-\nrent chunk forwarding phase in detail. This phase efficiently\nconstructs token-level, cross-layer context embeddings by\nsegmenting the long input into multiple chunks and repeat-\nedly processing them while conditioning on a compressed\nprevious KV cache. In Section 3.2, we further elaborate\non how we construct the cross-layer context embeddings.\nFinally, in Section 3.3, we describe how we use the con-\ntext embeddings to identify the relevant input segments and\nhighlight our on-demand cache recomputation framework\nthat enables random access to previous contexts while main-\ntaining the integrity of the KV cache. We outline the full\nprocedure in Figure 1 and Algorithm 1.\n3.1. Recurrent Chunked Forwarding with Early Exit\nEncoding long contexts with pre-trained Transformers is\noften infeasible due to the quadratic computational cost\nand the model’s limited context window. To overcome this\n3\n--- Page 4 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nproblem, we focus on recurrent KV cache compression ap-\nproaches that allow the processing of infinite context under\nlimited resources and context windows. Here, we describe\nthe encoding process in detail, discuss key efficiency bene-\nfits, and present our early exit strategy that provides further\nefficiency gains when using recurrent chunked forwarding\nto create context embeddings.\nRecurrent chunked forwarding. To process extremely\nlong inputs under limited computational budget and con-\ntext windows, several KV cache compression methods (Xiao\net al., 2023a; Zhang et al., 2023; Oren et al., 2024; Kim et al.,\n2024) suggest using an iterative approach. This approach\nrepeatedly compresses the previous input’s KV cache and\nforwards a small portion of the remaining input based on\nthe compressed cache. While many existing works demon-\nstrate their context extrapolation capabilities by applying\nKV cache eviction when processing every token (Xiao et al.,\n2023a; Zhang et al., 2023), this process can become ex-\ntremely slow for long inputs as it is not parallelizable. In-\nstead, we segment the long input into larger chunks (32k\ntokens for our experiments) and apply KV cache compres-\nsion after forwarding each chunk to better utilize parallel\ncomputation.\nFor KV cache compression, we employ attention-based to-\nken eviction following H2O (Zhang et al., 2023), which\nuses cumulative attention scores as a signal to determine the\nmost significant context. We retain the tokens that receive\nthe highest attention scores in the KV cache. After com-\npression, we reassign the position IDs so that the tokens in\nthe compressed cache have consecutive position IDs. This\nposition reassignment allows the model to handle longer\nsequences beyond its pre-trained context limit.\nThe recurrent chunked forwarding approach enables the en-\ncoding of very long sequences with limited computational\nresources because it encodes all inputs conditioned on a lim-\nited KV cache. This bypasses the need to store a large KV\ncache corresponding to the long input or compute expensive\nself-attention on the long input. Meanwhile, it allows the\nmodel to encode the inputs in a way that’s aware of the\nprevious context, thanks to the presence of such information\nin the compressed KV cache.\nEarly exit. Utilizing a compressive approach for creating\nembeddings introduces an additional benefit: efficiency can\nbe further improved by employing an early exit strategy. As\nobserved in Section 3.2, high-performing embeddings are\noften available in the lower Transformer layers. Therefore,\nforwarding the inputs through the remaining layers after the\ntopmost layer used for embedding extraction is unnecessary\nand can be skipped. The proposed early exit strategy reduces\nboth computation and memory requirements because we do\nnot need to keep the KV cache for the upper layers.3.2. Constructing Cross-Layer Context Embeddings\nPrior research has revealed the existence of specialized\nTransformer heads distributed across different layers that\ncan accurately retrieve relevant information from long con-\ntext input (Wu et al., 2024). To construct informative embed-\ndings, we thus analyze the retrieval performance of various\nheads and embeddings in Transformers to determine the\nmost suitable ones for our method. Specifically, we com-\npare the token-level retrieval performance of Transformer\nhidden states, attention scores (without positional encoding,\nto make it applicable to extremely long inputs), and the\nattention QKV states across Transformer layers.\n02468101214161820222426283032343638\nLayer0\n1\n2\n3\n4\n5\n6\n7Value Head\n51015202530\nFigure 2. MNR Scores for Value Heads. The distribution of the\nMNR scores (lower is better) across value states of different at-\ntention heads, measured by Mistral-Nemo-Instruct-2407 model\nover 500 synthetic multi-hop QA examples. Recurrent chunked\nforwarding with 256-token heavy hitter budget was employed for\ncomputing the embeddings.\nEmbedding head identification. We conducted a set\nof experiments on a synthetically constructed multi-hop\nquestion-answering dataset, and used Mean Normalized\nRank (MNR) scores to measure retrieval performance; see\nAppendix B.1 for details. We report the MNR scores for the\ntop-performing layers and heads in Table 1. Interestingly,\ndespite their smaller size, we observed that the retrieval\nperformance of attention QKV states was often better than\nthat of the commonly used LLM hidden states (Wang et al.,\n2023a). This finding suggests that, with careful selection of\nthe appropriate heads, directly using the QKV states from\nthe attention layer can achieve higher accuracy than the\nTable 1. Comparing different LLM embeddings. Best-3 MNR\nscores (lower is better) corresponding to the hidden states and the\nattention states, measured by Mistral-Nemo-Instruct-2407. Scores\nare averaged over 500 synthetic multi-hop QA examples.\nType Dim. Top-1 Top-2 Top-3 Avg.\nHidden States 5120 9.40 9.63 9.80 9.61\nAttention 160 6.91 7.70 7.81 7.47\nQuery 160 6.48 6.74 6.93 6.72\nKey 160 6.77 7.31 7.41 7.16\nValue 160 5.77 6.57 6.57 6.30\n4\n--- Page 5 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nhidden states, while being more computationally efficient.\nFigure 2 illustrates the distribution of the top-performing\nvalue heads. Notably, the best-performing heads are not nec-\nessarily from the final Transformer layers. This observation\nimplies that forwarding inputs through the upper layers may\nnot be required for constructing effective retrieval embed-\ndings, serving as a key motivation for our early exit strategy\ndescribed in the previous section.\nTo create a universal embedding that is useful for a wide\nrange of tasks, we do additional experiments to identify\nheads that can effectively represent complex input patterns.\nSpecifically, we create a synthetic key-value retrieval task,\nwhich involves embedding multiple sentences of the format\n“The value corresponding to the id {key}is{value}. ”within\nthe WikiText (Merity et al., 2016) corpus, where keys and\nvalues are random 10-character ASCII strings.\nWe selected the top-performing embeddings for each syn-\nthetic dataset after evaluating 500 samples each. It is worth\nnoting that although head selection is based on relatively\nshort synthetic data (8k tokens), the benefits extrapolate to\nlonger contexts involving millions of tokens.\nCombining multiple heads. After identifying the top-\nperforming heads, we combine their embeddings to create\na single, token-level embedding. In our preliminary experi-\nments, we observed that using an average of retrieval scores\nobtained from different heads often improves final retrieval\nperformance. Accordingly, we concatenate the gathered\nembeddings after normalizing them:\necomb =concat\u0012\u001aei\n||ei||,i∈selected heads\u001b\u0013\nThis approach ensures that performing a cosine similarity\nsearch using the resulting embedding is mathematically\nequivalent to independently computing cosine similarity\nscores for each head and then averaging them.\n3.3. On-Demand Cache Recomputation\nTo enable random access to the previous inputs, we utilize\nthe cross-layer context embeddings to identify the input\nsegments that are relevant to the last part of the input. Then,\nwe gather the corresponding input embeddings and forward\nthem through the model again, re-constructing the KV cache\nwith the most relevant inputs. We conduct the detailed\nprocess as follows.\nIdentification of significant inputs. After constructing the\ncross-layer context embeddings corresponding to the input,\nwe perform a token-level cosine similarity search between\nthe query (the last part of the input) and the remaining inputs\nusing the context embeddings. Then, we max-pool the\nsimilarity scores over the query to tokens to ensure that each\ntoken is assigned a single score. To preserve the continuityof the identified inputs, we further max-pool each token’s\nscore with the 128 adjacent tokens. After processing the\nsignificance scores, we identify the tokens with the highest\nscores. We always keep the initial and final 256 tokens to\nmaintain coherence.\nOn-Demand Cache Recomputation. Once we identify\nthe relevant segments, we gather the corresponding input\nembeddings and forward them through the model again,\nrecomputing the KV cache. The new KV cache is then\nused for the further decoding process. By introducing an\non-demand cache recomputation scheme, we avoid the need\nof storing the full KV cache while enabling random ac-\ncess to previous inputs, significantly reducing the memory\nrequirements.\n4. Experiments\nThis section demonstrates the performance of our method\nacross diverse tasks. In Section 4.1, we begin by showcasing\nthe precise retrieval ability of our approach using a needle-\nin-a-haystack benchmark. In Section 4.2, we evaluate our\nmethod on RULER (Hsieh et al., 2024), a comprehensive\nbenchmark comprising multiple synthetic datasets that in-\nclude more diverse and challenging needle-in-a-haystack\ntasks, aggregation tasks, and question answering tasks. Also,\nwe extend our evaluations to more a demanding BABILong\ndataset (Kuratov et al., 2024), which features complex long-\ncontext assessments, including multi-hop reasoning. In\nSection 4.3, we further test REFORM’s performance on\nmore realistic tasks from ∞-Bench (Zhang et al., 2024). In\nSection 4.4, we highlight the flexibility of our approach by\nevaluating it on multi-modal benchmarks. In Section 4.5,\nwe compare our approach to retrieval-augmented generation,\nan emerging direction for handling long inputs. Finally, we\nanalyze the efficiency of our approach in Section 4.6.\nCommon setup and baselines. Throughout the paper, we\nmainly compare our approach against training-free meth-\nods that utilize recurrence-based and random-access ap-\nproaches. Specifically, we compare our method against\nStreamingLLM (Xiao et al., 2023a), TOV A (Oren et al.,\n2024), H2O (Zhang et al., 2023), InfiniPot (Kim et al., 2024),\nand InfLLM (Xiao et al., 2024a). We also include a trun-\ncation baseline, which simply drops the middle part of the\ninput. To enhance efficiency, we implement recurrent chun-\nked forwarding for recurrence-based methods as described\nin Section 3.1 for all compression baselines, replacing token-\nlevel recurrence. For H2O, we restrict attention score com-\nputations to the last 128 tokens of each chunk for efficient\nimplementation.\nFor all text-based experiments, we use Mistral-NeMo-\nInstruct-2407 (Jiang et al., 2023) and Qwen2.5-7B-Instruct\n(Yang et al., 2024) models. For multi-modal experiments,\n5\n--- Page 6 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nwe use Pixtral-12B-2409 (Agrawal et al., 2024). All recur-\nrent baselines operate with a KV cache budget and chunk\nsize of 32k tokens, and InfLLM also uses 32k active KV\ncache budget. We always keep the initial and recent 256\ntokens in cache for all baselines and REFORM to maintain\nthe coherency of the text. For REFORM, we use a recompu-\ntation budget of 8k tokens for Mistral-Nemo-Instruct-2407\nand 16k tokens for all other models. We provide more\ndetails in Appendix B.1.\n4.1. Needle-In-A-Haystack Evaluation\nTo evaluate the precise retrieval performance of our ap-\nproach, we employ the Needle-in-a-Haystack (NIAH)\nbenchmark, following gkamradt (2023). In this task, a\nspecific ”needle” sentence ( “The best thing to do in San\nFrancisco is eat a sandwich and sit in Dolores Park on a\nsunny day” ) is embedded within various depths of irrelevant\ncontext consisting of diverse essays by Paul Graham. The\nmodel must correctly answer the question: “What is the\nbest thing to do in San Francisco?” This benchmark is\nmore challenging than the commonly used passkey retrieval\ntask (Mohtashami & Jaggi, 2023), where the haystack often\nconsists of repetitive sentences, as it requires identifying\nand recalling a needle sentence from a highly diverse and\nnon-repetitive context. For evaluation, we consider a re-\nsponse to be correct if it contains all three key phrases: “eat\na sandwich” ,“sit in Dolores Park” , and “a sunny day. ”\nIn Figure 3, we compare the performance of our method\nagainst H2O, InfiniPot (representing recurrence-based meth-\nods) and InfLLM (representing random access methods).\nOur method demonstrates stable performance across all\nneedle depths and context lengths up to 1M tokens, signifi-\ncantly outperforming both baselines. In contrast, H2O and\nInfiniPot exhibit forgetting issues, failing to retain critical\ninformation in longer inputs.\nInfLLM also struggle at long contexts despite having the\nflexibility to perform random access, suggesting that it of-\nten fails to retrieve relevant information from the full KV\ncache. This aligns with our findings Appendix C.1, where\ncertain heads indeed show poor retrieval performance. On\nthe contrary, REFORM collects only the top-performing\nheads across multiple layers to construct the context em-\nbeddings, ensuring that the relevant inputs are successfully\nidentified. These results highlight the limitations of exist-\ning approaches and underscore our method’s robustness in\nhandling extremely long contexts while maintaining precise\nretrieval performance.\n4.2. Performance on RULER and BABILong\nIn this section, we further demonstrate the performance\nof our approach in more diverse and challenging synthetic\nbenchmarks. Specifically, we evaluate different methods onTable 2. Evaluation on RULER. We measure the performance on\nan extended version of the RULER (Hsieh et al., 2024) benchmark,\na synthetic dataset consisting of multiple needle-in-a-haystack\ntasks along with some aggregation and question answering tasks.\nWe report the averaged performance of all tasks at different context\nlengths. The best values are highlighted in bold .\n64k 128k 200k 300k 400k 500k 1M\nMistral-NeMo-Instruct-2407\nTruncation 32.6 20.4 17.8 15.2 12.3 12.5 10.8\nStreamingLLM 27.6 13.8 11.6 9.3 7.2 7.1 4.7\nTOV A 21.6 15.3 14.0 11.8 7.9 8.7 4.6\nH2O 15.1 7.4 7.8 5.6 4.2 5.7 3.6\nInfiniPot 26.9 19.4 15.6 14.5 12.7 13.4 12.0\nInfLLM 52.7 39.7 28.5 24.9 20.9 22.0 23.3\nREFORM (Ours) 79.9 81.1 83.0 84.6 84.1 83.5 75.5\nQwen2.5-7B-Instruct\nTruncation 46.3 25.1 21.8 17.4 14.9 15.2 11.3\nStreamingLLM 43.5 25.3 18.7 17.3 11.8 11.8 9.1\nTOV A 66.2 27.7 25.7 25.8 21.9 20.4 17.0\nH2O 51.8 20.9 18.5 17.1 11.6 12.1 8.7\nInfiniPot 65.7 51.7 39.2 33.9 27.8 26.7 23.7\nInfLLM 47.1 34.2 29.2 24.0 22.0 23.2 23.8\nREFORM (Ours) 78.2 75.8 74.7 74.9 74.9 73.0 75.1\nan extended version of the RULER (Hsieh et al., 2024) and\nBABILong (Kuratov et al., 2024) benchmarks. RULER is a\nsynthetic long-context benchmark consisting of multiple di-\nverse and challenging needle-in-a-haystack tasks, as well as\nsome aggregation and question answering tasks. BABILong\nfurther challenges the model by introducing more difficult\ntasks, such as multi-hop reasoning. Although the original\nversion of RULER only supports up to 128k tokens, we\nfurther extend the dataset to 1M using the same recipe to\nevaluate the performance on longer inputs.\nWe highlight the evaluation results on RULER in Table 2,\nand BABILong in Table 3. In both benchmarks, REFORM\noutperforms the baselines by a large margin, indicating its\nsuperiority in tasks that require precise recall of essential\nparts of the context, benefiting both from the ability to lo-\ncate essential contexts from long inputs and the removal\nof distribution shifts in the KV cache that commonly come\nwith recurrence-based or random-access approaches.\n4.3. Performance on ∞-Bench\nIn this section, we compare the performance on more re-\nalistic long-context understanding tasks ∞-bench (Zhang\net al., 2024), derived from books and dialogues. The average\ncontext length exceeds 100k tokens for most tasks, making\nit one of the longest long-context benchmarks that include\nrealistic data. We outline the evaluation results in Table 4.\nREFORM shows superior performance over the baselines,\ndemonstrating that it is also capable of handling complex\nproblems on realistic data.\n6\n--- Page 7 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\n32k 64k 128k 256k 512k 1M\nContext Length10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%Needle Depth\n020406080100\n(a) H2O\n32k 64k 128k 256k 512k 1M\nContext Length10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%Needle Depth\n020406080100 (b) Infini-Pot\n32k 64k 128k 256k 512k 1M\nContext Length10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%Needle Depth\n020406080100 (c) InfLLM\n32k 64k 128k 256k 512k 1M\nContext Length10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%Needle Depth\n020406080100 (d) REFORM (Ours)\nFigure 3. Needle-In-A-Haystack Evaluation. We compare the precise retrieval ability of different long-context handling approaches by\nvisualizing the needle-in-a-haystack performance at different depth and context lengths. All experiments were done with Qwen2.5-7B-\nInstruct model, and the performance is averaged over 20 samples.\nTable 3. Evaluation on BABILong. We measure the performance\non BABILong (Kuratov et al., 2024), a synthetic benchmark con-\nsisting of more challenging tasks including multi-hop and multi-\nargument reasoning. We report the averaged performance of all\ntasks. The best values are highlighted in bold .\n64k 128k 256k 512k 1M Avg.\nMistral-NeMo-Instruct-2407\nTruncation 32.2 26.2 17.0 13.6 14.0 20.6\nStreamingLLM 38.8 23.4 15.4 11.0 6.2 19.0\nTOV A 37.8 23.6 14.4 9.6 3.4 17.8\nH2O 38.0 25.2 16.2 7.2 3.6 18.0\nInfiniPot 39.6 26.8 18.6 11.2 8.8 21.0\nInfLLM 40.6 34.0 23.6 13.0 9.6 24.2\nREFORM (Ours) 57.4 51.4 50.6 47.6 48.8 51.2\nQwen2.5-7B-Instruct\nTruncation 48.4 33.4 27.4 20.0 15.6 29.0\nStreamingLLM 53.4 40.6 33.2 23.8 19.6 34.1\nTOV A 56.0 46.6 40.6 29.4 21.8 38.9\nH2O 57.0 41.6 36.4 24.6 18.8 35.7\nInfiniPot 59.6 51.0 53.4 48.2 40.2 50.5\nInfLLM 43.0 29.2 20.4 15.4 11.4 23.9\nREFORM (Ours) 61.6 60.4 59.8 58.8 58.8 59.9\n4.4. Multi-Modal Evaluation\nThis section highlights the broad applicability of REFORM\nby demonstrating its performance on multi-modal bench-\nmarks. Operating at the transformer architecture level, RE-\nFORM can be seamlessly applied to any domain the base\nmodel supports. We evaluate Pixtral-12B-2409 (Agrawal\net al., 2024) on three tasks from the MM-NIAH benchmark\n(Wang et al., 2024a), which examine the model’s ability to\ncorrectly locate and reason on multiple text needles hidden\ninside a long multi-modal document. We provide detailed\nexperiment setup in Appendix B.2. The evaluation results in\nTable 5 demonstrate the superior performance of REFORM\nover the baseline approaches, highlighting the flexibility and\ngenerality of our approach.Table 4. Evaluation on ∞-Bench. We evaluate each method on\nmore realistic datasets from ∞-Bench (Zhang et al., 2024) ranging\nfrom long text summarization (En.Sum), question answering from\nlong documents (En.QA, En.MC) and long dialogue understanding\n(En.Dia). The best values are highlighted in bold .\nEn.Sum En.QA En.MC En.Dia Avg.\nMistral-Nemo-Instruct-2407\nTruncation 13.7 16.0 51.1 11.5 23.1\nStreamingLLM 12.5 12.6 45.9 6.5 19.3\nTOV A 12.3 13.8 47.2 8.0 20.3\nH2O 14.2 17.6 49.3 6.0 21.8\nInfiniPot 11.9 17.1 52.0 7.0 22.0\nInfLLM 16.9 17.4 58.1 7.0 24.8\nREFORM (Ours) 18.2 18.0 70.3 18.5 31.2\nQwen2.5-7B-Instruct\nTruncation 29.0 13.3 43.2 15.0 25.1\nStreamingLLM 29.2 8.6 52.4 14.5 26.2\nTOV A 29.4 8.6 56.8 15.0 27.4\nH2O 31.0 11.0 56.3 15.5 28.5\nInfiniPot 30.6 11.3 59.0 17.0 29.4\nInfLLM 27.6 9.6 38.0 12.0 21.8\nREFORM (Ours) 27.8 16.5 61.6 21.5 31.9\nTable 5. Multi-modal Evaluation. We measure the performance\non three datasets from MM-NIAH (Wang et al., 2024a), that as-\nsesses the model’s ability to handle text needles hidden in a long\nmulti-modal document. We report normalized performance across\ninput lengths to ensure equal contribution from each context length\nrange. The best results are highlighted in bold .\nRetrieval Counting Reasoning Avg.\nPixtral-12B-2409\nTruncate 72.2 18.7 51.2 47.4\nStreamingLLM 71.9 17.8 49.8 46.5\nTOV A 82.9 18.8 54.1 52.0\nH2O 83.3 18.9 53.5 51.9\nInfiniPot 85.4 18.8 54.7 53.0\nREFORM (Ours) 89.2 22.0 61.3 57.5\n7\n--- Page 8 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nTable 6. Comparison with RAG. We compare the performance\nof RAG methods and REFORM on four groups of needle-in-a-\nhaystack datasets from RULER at 300k contexts, using Mistral-\nNeMo-Instruct-2407 model.\nSingle Multikey Multivalue Multiquery\nSparse RAG 86.7 77.3 88.5 90.0\nDense RAG 87.3 57.3 82.5 78.0\nREFORM 99.3 93.3 98.5 100.0\nREFORM + RAG 99.3 94.7 99.0 100.0\n4.5. Comparison to Retrieval Augmented Generation\nWe now compare REFORM to Retrieval Augmented Gener-\nation (RAG), a popular method for processing long inputs\n(Li et al., 2024b; Yu et al., 2024). RAG frameworks seg-\nment inputs into smaller chunks, which are independently\nencoded, and use external retrieval models to identify rel-\nevant segments. While effective in some scenarios, RAG\nsuffers from key limitations that REFORM mitigates.\nFirst, REFORM avoids the context fragmentation inherent\nin RAG by conditioning retrieval embeddings on the en-\ntire input, ensuring global context continuity and allowing\nfor cohesive processing of long contexts. Second, while\nRAG frameworks are constrained by the training domain of\nthe retrieval model—requiring domain-specific retraining\nor advanced adaptations for different domains and modal-\nities—REFORM is inherently flexible and can seamlessly\nhandle diverse domains, including multi-modal applications,\nwithout requiring such modifications. Finally, REFORM\nintegrates retrieval functionality directly into the model,\neliminating the need for external retrieval models.\nIn Table 6, we compare the performance of REFORM\nagainst RAG approaches using sparse and dense retriev-\ners on the needle-in-a-haystack datasets from RULER at\n300k contexts. We provide a more detailed experiment\nsetup in Appendix B.3. REFORM consistently outperforms\nboth approaches in all evaluations, demonstrating its ro-\nbustness and efficiency. Furthermore, we explore a hybrid\napproach by combining REFORM with a dense retriever,\nblending REFORM ’s token-level significance scores with\nretrieval scores using a weighted sum (25% for the dense re-\ntriever, 75% for REFORM). This hybrid approach performs\neven better, highlighting the complementary strengths of\nREFORM and RAG.\n4.6. Efficiency Analysis\nIn this section, we highlight the efficiency benefits of our\napproach. To this end, we demonstrate the compute- and\nmemory-efficiency of our method by measuring the peak\nmemory usage and inference time required for processing a\nlong input. Specifically, we measure the peak memory andTable 7. Efficiency Analysis. We compare the peak memory us-\nage and inference time required for generating 10 tokens con-\nditioned on 256k inputs. All measurements are made with the\nMistral-NeMo-Instruct-2407 model on a single H100 GPU, and\nare averaged over 10 samples. The best values are highlighted in\nbold .\nInference Time (sec.) Peak Memory (GB)\nStreamingLLM 36.58 37.34\nH2O 41.33 37.85\nTOV A 39.46 37.06\nInfiniPot 40.90 37.06\nInfLLM 129.14 51.62\nREFORM (Ours) 27.24 35.00\ninference time that each method requires for generating 10\ntokens conditioned on 256k tokens.\nWe outline the results in Table 7. InfLLM suffers from high\ninference time due to frequent memory transfer between\nCPU and GPU despite requiring large memory to store the\ncache. Recurrent methods offer faster inference at lower\nmemory costs, enjoying the benefits of using a fixed-size KV\ncache. Interestingly, our approach shows lower latency and\nmemory requirements compared to the recurrent baselines,\ndespite having to forward the gathered inputs again and\nstoring token-level context embeddings in memory. This\nis mainly due to the savings from early exit, which saves\ncomputation as well as memory by removing the need to\nkeep the KV cache for the upper Transformer layers.\n5. Conclusion\nIn this work, we introduced REFORM, a novel inference\nframework for efficient long-context processing. REFORM\nincrementally processes input chunks while maintaining a\ncompressed KV cache, extracting key QKV states to con-\nstruct cross-layer context embeddings. An early-exit strat-\negy enhances efficiency, and a similarity-based selection\nmechanism identifies and gathers essential tokens for KV\ncache recomputation, ensuring accurate long-range infor-\nmation retention. REFORM outperforms existing methods\nacross long-context benchmarks while reducing inference\ntime and memory usage. Furthermore, its modality-agnostic\ndesign makes it applicable to a wide range of use cases\nincluding multi-modal applications.\nFuture work. In this paper, we directly adopt the token\neviction criteria proposed by H2O (Zhang et al., 2023) as\nthe compression component of our framework. As a future\ndirection, we intend to investigate more sophisticated com-\npression approaches that are better tailored for constructing\nthe context embeddings. Furthermore, applications to more\ndiverse data modalities such as audio and video is another\npromising direction to explore.\n8\n--- Page 9 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nImpact Statement\nAs the demand for processing vast amounts of informa-\ntion grows across various fields, the ability to handle ex-\ntremely long-context inputs efficiently becomes increas-\ningly vital. REFORM presents a significant advancement in\nlong-context processing, enabling large language models to\nreason over extremely long sequences without prohibitive\ncomputational costs.\nBy significantly improving efficiency, memory usage, and\nperformance, REFORM makes advanced AI more accessi-\nble and sustainable. Its ability to retain and prioritize critical\ninformation while reducing computational overhead may\nsupport real-world applications, where processing extensive\nrecords with accuracy is essential.\nFurthermore, REFORM contributes to AI democratization\nby lowering resource barriers, making high-performance\nlong-context processing available to a broader range of users,\nincluding researchers, educators, and smaller enterprises.\nIts impact extends to environmental sustainability, as re-\nducing computational demands translates to lower energy\nconsumption in large-scale AI deployments.\nBy bridging the gap between efficiency and performance,\nREFORM not only advances the technical landscape of AI\nbut also fosters a future where AI-driven insights are more\ninclusive, responsible, and impactful across diverse societal\ndomains.\nReferences\nAgrawal, P., Antoniak, S., Hanna, E. B., Bout, B., Chap-\nlot, D., Chudnovsky, J., Costa, D., De Monicault, B.,\nGarg, S., Gervet, T., et al. Pixtral 12b. arXiv preprint\narXiv:2410.07073 , 2024.\nbloc97. Ntk-aware scaled rope allows llama models\nto have extended (8k+) context size without any\nfine-tuning and minimal perplexity degradation.\nhttps://www.reddit.com/r/LocalLLaMA/\ncomments/14lz7j5/ntkaware_%20scaled_\nrope_allows_llama_models_to_have/ , 2023.\nBulatov, A., Kuratov, Y ., and Burtsev, M. Recurrent memory\ntransformer. Advances in Neural Information Processing\nSystems , 35:11079–11091, 2022.\nChen, S., Wong, S., Chen, L., and Tian, Y . Extending\ncontext window of large language models via positional\ninterpolation. arXiv preprint arXiv:2306.15595 , 2023a.\nChen, Y ., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and\nJia, J. Longlora: Efficient fine-tuning of long-context\nlarge language models. arXiv preprint arXiv:2309.12307 ,\n2023b.Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860 , 2019.\nDao, T. FlashAttention-2: Faster attention with better paral-\nlelism and work partitioning. In International Conference\non Learning Representations (ICLR) , 2024.\nDong, H., Yang, X., Zhang, Z., Wang, Z., Chi, Y ., and Chen,\nB. Get more with less: Synthesizing recurrence with\nkv cache compression for efficient llm inference. arXiv\npreprint arXiv:2402.09398 , 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.\ngkamradt. Needle in a haystack - pressure test-\ning llms. https://github.com/gkamradt/\nLLMTest_NeedleInAHaystack , 2023.\nHan, C., Wang, Q., Xiong, W., Chen, Y ., Ji, H., and Wang, S.\nLm-infinite: Simple on-the-fly length generalization for\nlarge language models. arXiv preprint arXiv:2308.16137 ,\n2023.\nHooper, C., Kim, S., Mohammadzadeh, H., Mahoney,\nM. W., Shao, Y . S., Keutzer, K., and Gholami, A.\nKvquant: Towards 10 million context length llm in-\nference with kv cache quantization. arXiv preprint\narXiv:2401.18079 , 2024.\nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\nJia, F., Zhang, Y ., and Ginsburg, B. Ruler: What’s the\nreal context size of your long-context language models?\narXiv preprint arXiv:2404.06654 , 2024.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825 , 2023.\nJin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y .,\nChen, H., and Hu, X. Llm maybe longlm: Self-extend\nllm context window without tuning, 2024.\nkaiokendev. Things i’m learning while training super-\nhot. https://kaiokendev.github.io/til#\nextending-context-to-8k./ , 2023.\nKang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna,\nT., and Zhao, T. Gear: An efficient kv cache compression\nrecipefor near-lossless generative inference of llm. arXiv\npreprint arXiv:2403.05527 , 2024.\n9\n--- Page 10 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nKim, M., Shim, K., Choi, J., and Chang, S. Infinipot: Infinite\ncontext processing on memory-constrained llms. arXiv\npreprint arXiv:2410.01518 , 2024.\nKuratov, Y ., Bulatov, A., Anokhin, P., Rodkin, I., Sorokin,\nD., Sorokin, A., and Burtsev, M. Babilong: Testing the\nlimits of llms with long context reasoning-in-a-haystack.\narXiv preprint arXiv:2406.10149 , 2024.\nLi, Y ., Huang, Y ., Yang, B., Venkitesh, B., Locatelli, A.,\nYe, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm\nknows what you are looking for before generation. arXiv\npreprint arXiv:2404.14469 , 2024a.\nLi, Z., Li, C., Zhang, M., Mei, Q., and Bendersky, M. Re-\ntrieval augmented generation or long-context llms? a\ncomprehensive study and hybrid approach. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing: Industry Track , pp. 881–\n893, 2024b.\nLiu, X., Li, R., Guo, Q., Liu, Z., Song, Y ., Lv, K., Yan, H.,\nLi, L., Liu, Q., and Qiu, X. Reattention: Training-free\ninfinite context with finite attention scope. arXiv preprint\narXiv:2407.15176 , 2024.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMohtashami, A. and Jaggi, M. Landmark attention:\nRandom-access infinite context length for transformers.\narXiv preprint arXiv:2305.16300 , 2023.\nMunkhdalai, T., Faruqui, M., and Gopal, S. Leave no con-\ntext behind: Efficient infinite context transformers with\ninfini-attention. arXiv preprint arXiv:2404.07143 , 2024.\nOren, M., Hassid, M., Yarden, N., Adi, Y ., and Schwartz,\nR. Transformers are multi-state rnns. arXiv preprint\narXiv:2401.06104 , 2024.\nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:\nEfficient context window extension of large language\nmodels. arXiv preprint arXiv:2309.00071 , 2023.\nRobertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu,\nM. M., Gatford, M., et al. Okapi at trec-3. Nist Special\nPublication Sp , 109:109, 1995.\nSinghania, P., Singh, S., He, S., Feizi, S., and Bhatele, A.\nLoki: Low-rank keys for efficient sparse attention. arXiv\npreprint arXiv:2406.02542 , 2024.\nSong, W., Oh, S., Mo, S., Kim, J., Yun, S., Ha, J.-W.,\nand Shin, J. Hierarchical context merging: Better long\ncontext understanding for pre-trained llms. arXiv preprint\narXiv:2404.10308 , 2024.Su, J. Rectified rotary position embeddings. https://\ngithub.com/bojone/rerope , 2023.\nSu, J., Ahmed, M., Lu, Y ., Pan, S., Bo, W., and Liu, Y .\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing , 568:127063, 2024.\nTang, H., Lin, Y ., Lin, J., Han, Q., Hong, S., Yao, Y ., and\nWang, G. Razorattention: Efficient kv cache compression\nthrough retrieval heads. arXiv preprint arXiv:2407.15891 ,\n2024.\nWang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and\nWei, F. Improving text embeddings with large language\nmodels. arXiv preprint arXiv:2401.00368 , 2023a.\nWang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J.,\nand Wei, F. Augmenting language models with long-term\nmemory. arXiv preprint arXiv:2306.07174 , 2023b.\nWang, W., Zhang, S., Ren, Y ., Duan, Y ., Li, T., Liu, S.,\nHu, M., Chen, Z., Zhang, K., Lu, L., et al. Needle in a\nmultimodal haystack. arXiv preprint arXiv:2406.07230 ,\n2024a.\nWang, Z., Jin, B., Yu, Z., and Zhang, M. Model tells you\nwhere to merge: Adaptive kv cache merging for llms\non long-context tasks. arXiv preprint arXiv:2407.08454 ,\n2024b.\nWu, W., Wang, Y ., Xiao, G., Peng, H., and Fu, Y . Re-\ntrieval head mechanistically explains long-context factual-\nity, 2024. URL https://arxiv.org/abs/2404.\n15574 .\nWu, Y ., Rabe, M. N., Hutchins, D., and Szegedy, C. Mem-\norizing transformers. arXiv preprint arXiv:2203.08913 ,\n2022.\nXiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y ., Zhang,\nZ., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling\nthe intrinsic capacity of llms for understanding extremely\nlong sequences with training-free memory. arXiv preprint\narXiv:2402.04617 , 2024a.\nXiao, G., Tian, Y ., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453 , 2023a.\nXiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H.,\nFu, Y ., and Han, S. Duoattention: Efficient long-context\nllm inference with retrieval and streaming heads. arXiv\npreprint arXiv:2410.10819 , 2024b.\nXiao, S., Liu, Z., Zhang, P., and Muennighoff, N. C-pack:\nPackaged resources to advance general chinese embed-\nding, 2023b.\n10\n--- Page 11 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115 , 2024.\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W.,\nSalakhutdinov, R., and Manning, C. D. Hotpotqa: A\ndataset for diverse, explainable multi-hop question an-\nswering. arXiv preprint arXiv:1809.09600 , 2018.\nYu, T., Xu, A., and Akkiraju, R. In defense of rag in the\nera of long-context language models. arXiv preprint\narXiv:2409.01666 , 2024.\nZhang, X., Chen, Y ., Hu, S., Xu, Z., Chen, J., Hao, M., Han,\nX., Thai, Z., Wang, S., Liu, Z., et al. ∞bench: Extending\nlong context evaluation beyond 100k tokens. In Proceed-\nings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp.\n15262–15277, 2024.\nZhang, Z., Sheng, Y ., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y ., R ´e, C., Barrett, C., et al. H2o:\nHeavy-hitter oracle for efficient generative inference of\nlarge language models. Advances in Neural Information\nProcessing Systems , 36:34661–34710, 2023.\nZhu, D., Yang, N., Wang, L., Song, Y ., Wu, W., Wei, F.,\nand Li, S. Pose: Efficient context window extension of\nllms via positional skip-wise training. arXiv preprint\narXiv:2309.10400 , 2023.\n11\n--- Page 12 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nA. Extended related works\nExtending LLMs to handle extremely long inputs. To extend the context windows of Large Language Modles (LLMs)\nefficiently, various approaches have been proposed. A significant body of work focuses on modifying positional embeddings.\nThese include scaling Rotary Positional Embeddings (RoPE) (Su et al., 2024) beyond the model’s context limit (Chen et al.,\n2023a; kaiokendev, 2023; bloc97, 2023; Peng et al., 2023), applying attention masks (Han et al., 2023), or adjusting the\nrelative distances between tokens to fall within a predefined range (Su, 2023; Jin et al., 2024). Another line of research\nexplores fine-tuning techniques to adapt models for longer contexts (Zhu et al., 2023; Chen et al., 2023b). While these\nmethods enable models to handle extended inputs, they do not address the significant computational and memory costs\nintroduced by the self-attention mechanism, limiting their practical utility for extremely long contexts.\nOther approaches for efficient long context processing. Together with the recurrent KV cache compression approaches, a\nlarge volume of recent works focus on reducing the size of the KV cache to enable more efficient inference at long contexts.\nFor example, SnapKV (Li et al., 2024a) proposes to forward the full input through the model, and then compress the cache\nby evicting tokens based on attention scores. While efficient at decoding-time, it requires the model to first process the\nfull input, and therefore is not applicable to extremely long inputs that exceed the model’s pre-trained context window.\nAlternatively, HOMER (Song et al., 2024) proposes to use a hierarchical divide-and-conquer approach to combine the\nencoding and eviction process. Some works propose to further enhance KV cache compression by merging tokens instead\nof evicting them (Wang et al., 2024b; Dong et al., 2024), but their experiments also only consider inputs within the model’s\ncontext limit, and their extrapolation capabilities remain unknown. Some recent works propose another direction to keep the\nfull cache only for some selected attention heads known as ‘retrieval heads’ (Tang et al., 2024; Xiao et al., 2024b), reducing\nthe memory burden of preserving the full KV cache. Other works investigate quantization (Hooper et al., 2024; Kang et al.,\n2024) and low-rank cache compression (Singhania et al., 2024) to further reduce the memory requirements of the KV cache.\nHowever, these methods also cannot extrapolate to longer sequences beyond the model’s pre-trained context limit.\nB. Experimental Details\nB.1. Evaluating Retrieval Heads and Embeddings\nDataset preparation. To evaluate the embeddings, we constructed a synthetic dataset based on multi-hop question answering.\nIn this setup, we embedded documents from the HotPotQA dataset (Yang et al., 2018) at random positions within a long text\ncorpus derived from the WikiText dataset (Merity et al., 2016). Each question was appended at the end of the context, and\ntoken-level labels were created, where tokens from the golden documents were marked as ground truth. All samples were\ndesigned to be 8k tokens long, which is within the context window of the Mistral-7B-Instruct-v0.2 model.\nEmbedding extraction. To simulate long-context scenarios where full attention computation is infeasible due to compu-\ntational or memory constraints, we employed a recurrent chunk forwarding method based on H2O (Zhang et al., 2023),\nelaborated in Section 3.1. For attention, we compute the retrieval scores using the dot product between query states (Q) and\nthe key states (K) without applying positional encoding. For all other embeddings, we compute the significance scores using\ncosine similarity between question embeddings and context embeddings, followed by max-pooling over question tokens.\nAdditionally, retrieval scores for each context token were smoothed by mean-pooling with 20 neighboring tokens.\nPerformance measurement. Retrieval performance was quantified using the Mean Normalized Rank (MNR), which is\ncalculated as the average normalized rank of the golden tokens. Lower scores correspond to higher performance, as the\ngolden tokens have a high rank.\nMNR =1\nlen(gold doc)X\nt∈gold docrank (t)\nnumtokens\nB.2. Multimodal Evaluations\nBaseline details. In the multi-modal experiments, we evaluate the model performance using recurrence-based methods only,\nas the codebase for InfLLM only supports text-based models. For InfiniPot, the NuC (novelty under compression) score\ncannot be utilized for cache compression for multi-modal models because the vision tokens do not output a logit. Therefore,\nwe only apply the CaP (catalyst prompt) score for the InfiniPot baseline in multi-modal experiments.\n12\n--- Page 13 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nB.3. Comparison against RAG\nExperiment setup. We follow the setup in OP-RAG (Yu et al., 2024) for the RAG experiments, segmenting the inputs\nto 128-token chunks and preserving the order of the chunks instead of rearranging them according to the retrieval scores.\nWe use Mistral-NeMo-Instruct-2407 as the base LLM. We use BM25 (Robertson et al., 1995) as the sparse retriever, and\nbge-large-en-v1.5 (Xiao et al., 2023b) as the dense retriever. For each sample, 8k tokens are retrieved in total, matching the\nKV size with our approach to ensure fair comparison.\nB.4. Efficiency Measurements\nExperiment setup. We measure the average inference time and peak memory usage for generating 10 tokens conditioned\non 256k tokens. All measurements are made on a single H100 GPU, and we apply Flash Attention 2 (Dao, 2024) for all\nmeasurements. We further elaborate the experiment setup for InfLLM, as the inference speed and memory consumption\ncan largely vary depending on the configuration. We use the default configuration provided in their GitHub repository,\nwhile modifying the number of retrieved blocks to keep 32k active tokens in the cache. The maximum number of blocks\ncached in GPU was set to be the twice as large as the number of retrieved blocks, following the convention in their official\nconfiguration file.\nB.5. Embedding Construction and Similarity Search for REFORM\nEmbedding head selection. We construct the context embeddings by combining four QKV embeddings, where two heads\nare identified using the pattern matching dataset and the other two are identified using the multi-hop QA dataset. To balance\nbetween performance and efficiency gains, we select the top-performing heads from layers with depth under 70% for pattern\nmatching heads. See Appendix C.2 for a more detailed discussion.\nFor Mistral-NeMo-Instruct-2407, the following heads are used:\n1.Pattern Matching : Value heads 0 and 7 at layer 27\n2.Multi-hop QA : Value head 5 at layer 19, query head 9 at layer 15\nFor Qwen2.5-7B-Instruct, the following heads are used:\n1.Pattern Matching : Value head 3 at layer 7, key head 0 at layer 14\n2.Multi-hop QA : Value head 3 at layer 14, value head 0 at layer 19\nFor Pixtral-12B-2409, the following heads are used:\n1.Pattern Matching : Value heads 0 and 7 at layer 27\n2.Multi-hop QA : Value head 3 at layer 10, value head 5 at layer 19\nSimilarity search. REFORM performs a cosine similarity search between each token in the query (the final part of the\ninput) and the remaining tokens. For better precision in identifying the relevant inputs, we remove the special tokens and the\ngeneration prefix (e.g. ‘the answer is’) when computing the similarity scores.\n13\n--- Page 14 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nC. Additional Results\nC.1. Ablation Studies\nTable 8. Ablation on different components. We report the average performance on RULER 300k and BABILong 512k datasets using\nMistral-NeMo-Instruct-2407 model. The best values are highlighted in bold .\nRULER BABILong\nREFORM (Ours) 84.6 47.6\nw/ StreamingLLM 82.7 44.6\nw/ TOV A 81.4 46.8\nw/ Random heads 80.3 43.0\nw/ Bad heads 44.7 22.8\nIn this section, we conduct an ablation study to evaluate the key components contributing to the effectiveness of our approach.\nSpecifically, we analyze the impact of (1) the choice of the recurrent compression method and (2) the selection of attention\nheads used for retrieval. We assess model performance on the BABILong and RULER benchmarks at context lengths of\n512k and 300k, respectively.\nChoice of recurrent compression method. To demonstrate the generality of our approach, we replace our recurrent\ncompression component with alternative methods, namely StreamingLLM and TOV A. While H2O yields the best results,\nTable 8 shows that other compression methods achieve comparable performance. This further highlights the flexibility of\nour framework and its potential for even higher performance with more advanced compression techniques.\nChoice of attention heads. To examine the importance of attention head selection for embedding construction, we\nreplace the selected heads with (1) four randomly chosen heads and (2) four suboptimal ”bad” heads, identified based\non poor performance on both synthetic datasets used for head selection. As shown in Table 8, the heads selected by\nour mechanism achieve the best performance, demonstrating its effectiveness. Random heads generally show lower but\nreasonable performance. In contrast, using bad heads results in a substantial performance drop on both benchmarks,\nunderscoring the importance of proper attention head selection to ensure effective embedding construction.\nC.2. Embedding Head Identification for Pattern Matching Task\nIn this section, we present the distribution of MNR scores measured with our pattern matching dataset, similarly to what\nwe presented in Table 1 and Figure 2. The corresponding results for pattern matching dataset is presented in Table 9 and\nFigure 4. The retrieval performance of QKV heads often outperform that of the hidden states, similarly to the case of\nmulti-hop QA datasets.\nInterestingly, the distribution of best-performing heads show a different pattern compared to the milti-hop QA dataset, and\nthe heads at lower layers and middle-to-upper layers show the highest performance. This suggests that different heads\nshow different characteristics depending on the task. It also motivates our approach of using the embeddings identified\nby the different tasks as it yields more general representations and makes similarity-based retrieval more accurate. It is\nalso important to note that while the upper layer has more good-performing heads, these heads can be also identified in\nthe mid-lower layers (e.g., Layer 16, Head 1). To balance the performance with the efficiency gains provided by early-exit\nTable 9. Comparing different LLM embeddings. Best-3 MNR scores (lower is better) corresponding to the hidden states and the\nattention states, measured by Mistral-Nemo-Instruct-2407. Scores are averaged over 500 synthetic pattern matching examples.\nType Dim. Top-1 Top-2 Top-3 Avg.\nHidden States 5120 1.72 1.88 2.10 1.90\nAttention 160 1.24 1.36 1.37 1.32\nQuery 160 1.51 1.56 1.57 1.55\nKey 160 1.53 1.65 1.72 1.63\nValue 160 0.93 0.95 1.13 1.00\n14\n--- Page 15 ---\nCompress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers\nstrategy, we select the best-performing pattern-matching heads from layers under 70% of depth. This strategy ensures that\nwe utilize the high-performing heads as well as enjoying the computation savings from early exit.\n02468101214161820222426283032343638\nLayer0\n1\n2\n3\n4\n5\n6\n7Value Head\n02468\nFigure 4. MNR Scores for Value Heads. The distribution of the MNR scores (lower is better) across value states of different attention\nheads, measured by Mistral-Nemo-Instruct-2407 model over 500 synthetic pattern matching examples. Recurrent chunked forwarding\nwith 256-token heavy hitter budget was employed for computing the embeddings.\n15",
  "text_length": 61314
}