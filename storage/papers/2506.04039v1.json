{
  "id": "http://arxiv.org/abs/2506.04039v1",
  "title": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization",
  "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.",
  "authors": [
    "Jiulong Wu",
    "Zhengliang Shi",
    "Shuaiqiang Wang",
    "Jizhou Huang",
    "Dawei Yin",
    "Lingyong Yan",
    "Min Cao",
    "Min Zhang"
  ],
  "published": "2025-06-04T15:03:50Z",
  "updated": "2025-06-04T15:03:50Z",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04039v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04039v1  [cs.CV]  4 Jun 2025Mitigating Hallucinations in Large Vision-Language Models via\nEntity-Centric Multimodal Preference Optimization\nJiulong Wu1, Zhengliang Shi3, Shuaiqiang Wang2, Jizhou Huang2\nDawei Yin2,Lingyong Yan2∗,Min Cao1∗,Min Zhang1\n1Soochow University, Suzhou, China2Baidu Inc., Beijing, China\n3Shandong University, Qingdao, China\nwjlwujiulong@gmail.com, zhengliang.shii@gmail.com ,\nlingyongy@gmail.com, mcao@suda.edu.cn\nAbstract\nLarge Visual Language Models (LVLMs) have\ndemonstrated impressive capabilities across\nmultiple tasks. However, their trustworthiness\nis often challenged by hallucinations, which\ncan be attributed to the modality misalignment\nand the inherent hallucinations of their under-\nlying Large Language Models (LLMs) back-\nbone. Existing preference alignment methods\nfocus on aligning model responses with human\npreferences while neglecting image-text modal-\nity alignment, resulting in over-reliance on\nLLMs and hallucinations. In this paper, we pro-\npose Entity-centric Multimodal Preference Op-\ntimization (EMPO), which achieves enhanced\nmodality alignment than existing human pref-\nerence alignment methods. Besides, to over-\ncome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruc-\ntion datasets to automatically construct high-\nquality preference data across three aspects:\nimage, instruction, and response. Experiments\non two human preference datasets and five\nmultimodal hallucination benchmarks demon-\nstrate the effectiveness of EMPO, e.g., reduc-\ning hallucination rates by 85.9% on Object-\nHalBench and 49.8% on MM-HalBench. The\ncode and dataset will be released at https:\n//github.com/RobitsG/EMPO .\n1 Introduction\nLarge Vision Language Models (LVLMs) have re-\ncently demonstrated impressive capabilities in mul-\ntimodal question answering (Chen et al., 2023; Liu\net al., 2023b; Bai et al., 2023; Lu et al., 2024),\nwhich typically consists of a visual encoder to ex-\ntract image features, and a Large Language Model\n(LLM) to answer the image-related textual instruc-\ntions based on the provided visual context. The\nLVLMs are usually learned in two steps (Li et al.,\n2023a; Du et al., 2022; Lin et al., 2024): (1) pre-\ntraining on large-scale image-text pairs to learn\n*Co-corresponding authors.\n(a) Modality Misalignment\nUser: Please describe this image.\nLLaVA : A no -parking sign is posted \non the farmland …\nGround Truth : A no motor vehicles  \nsign is posted on the farmland …\n(b) LLM Inherent Hallucination\nUser: Is there a car on the road?\nLLaVA : Yes, there is a car driving \non the road.\nGround Truth : No, there is not a \ncar driving on the road.\nFigure 1: Two types of hallucinations. a) Modality mis-\nalignment: LVLM recognizes the presence of entities\nbut confuses their semantics. b) LLM inherent halluci-\nnation: The LVLM’s response is entirely dependent on\ntextual context, disregarding the image content.\nmultimodal knowledge, and (2) fine-tuning on\nmultimodal instruction-following datasets to steer\ntheir responsiveness to user instructions (Liu et al.,\n2023b; Wang et al., 2024c; Bai et al., 2023).\nHowever, recent studies have identified that the\nLVLMs usually suffer from the hallucination prob-\nlem (Li et al., 2023d; Liu et al., 2024a; Gunjal et al.,\n2024; Guan et al., 2024; Jiang et al., 2024b,a), akin\nto the LLM hallucinations (Zhang et al., 2023; Li\net al., 2023b; Dhuliawala et al., 2023). Specifically,\nthere are usually two types of LVLM hallucina-\ntions (as shown in Figure 1) (Liu et al., 2024a; Lan\net al., 2024). The first type is the modality mis-\nalignment , which arises from the modality gap\nbetween the visual encoder and LLM, resulting in\nsemantic mismatches between image context and\ntextual instructions. For instance, in Figure 1(a),\nthe LVLM (i.e., LLaVa) (Liu et al., 2023b) correctly\nidentifies the sign on the farmland but misinterprets\nits meaning as \"No Parking\" instead of the correct\ninterpretation, \"No Motor Vehicles Allowed\" . The\nsecond type is the LLM inherent hallucination .\nWhen the LLM inherent knowledge is either incor-\nrect or conflicts with visual inputs, hallucinations\nmanifest as entity co-occurrence phenomena (Lan\net al., 2024). For example, as shown in Figure 1(b),\n1\n--- Page 2 ---\n\"car\" and\"road\" frequently co-occur in the LLM’s\npretraining corpus; the LVLM erroneously infers\nthat whenever a \"road\" is present, a \"car\" must\nalso be present, disregarding the image content.\nTo mitigate hallucinations in LVLMs, many re-\ncent studies (Zhou et al., 2024; Sun et al., 2023;\nYu et al., 2024c) adopt preference alignment al-\ngorithms such as Direct Preference Optimization\n(DPO) (Rafailov et al., 2024), to align the model’s\nmultimodal response capabilities with human pref-\nerences. However, existing multimodal prefer-\nence optimization methods extend DPO by simply\nadding images to preference conditions, without\npaying sufficient attention to entity-centric factual\nfragments, which are highly related to hallucina-\ntions. On the other hand, incorporating new modal-\nities into preference conditions increases the range\nof possible preference combinations, necessitating\na comprehensive exploration of preference dimen-\nsions to ensure LVLM responses effectively over-\ncome the inherent hallucinations of LLMs based\non images and user instructions.\nTo address these questions, we propose Entity-\nCentric Multimodal Preference Optimization\n(EMPO), which efficiently aligns images, user in-\nstructions, and model responses through preference\noptimization on comprehensive aspects. Specifi-\ncally, we first construct a multimodal preference\ndataset based on open-source image-text instruc-\ntion datasets (Zhou et al., 2024; Yu et al., 2024c)\nby automatically editing the entities, attributes, and\nrelationships within the image-instruction-response\ntriplets. Then, we apply the DPO loss across three\naspects—image instruction conditioning and model\nresponses, helping LVLMs align image entity fea-\ntures with the corresponding textual semantics in\nuser instructions and model responses. To vali-\ndate the effectiveness of EMPO, we evaluate it on\nfive benchmarks under the LLaV A-1.5 (Li et al.,\n2024) framework with two training datasets. Exper-\nimental results demonstrate that EMPO achieves\nlower hallucination rates than GPT-4V (Chen et al.,\n2024a) across three hallucination benchmarks. Fur-\nthermore, compared to the well-known DPO algo-\nrithm, EMPO reduces hallucination rates by 85.9%\non Object HalBench (Rohrbach et al., 2018) and\nby 49.8% on MM HalBench (Sun et al., 2023).\nWe summarize our contributions as: (1) We pro-\npose EMPO to effectively mitigate hallucinations\nby addressing the insufficient alignment of image\nand text modalities in existing multimodal DPO al-\ngorithms. Through preference optimization acrossthree aspects, EMPO helps LVLMs align entity fea-\ntures and semantic concepts better. (2) We propose\nautomatically constructing multimodal preference\ndata using open-source instruction datasets to ad-\ndress the lack of high-quality multimodal prefer-\nence data caused by the increasing complexity of\npreference combinations. Our data construction\nmethod can be applied to any existing instruction\ndataset without additional manual annotation. (3)\nExperimental results on two preference training\ndatasets across five widely-used benchmarks show\nthat EMPO enhances multimodal semantic align-\nment and effectively reduces hallucinations.\n2 Related Work\nLarge Vision Language Models. Recent re-\nsearch on LVLMs (Zhu et al., 2023; Liu et al.,\n2023b; Bai et al., 2023; Lu et al., 2024; Zhang\net al., 2024; Li et al., 2023a) constructs LVLMs\nby aligning LLMs with visual models, demon-\nstrating superior performance across various vi-\nsual–language tasks compared to earlier studies (Jia\net al., 2021; Radford et al., 2021; Ju et al., 2022;\nAlayrac et al., 2022). These LVLMs typically adopt\na two-stage training strategy: (1) Pretraining on\nlarge-scale image–text pairs to learn fundamental\nmultimodal knowledge (Li et al., 2023a; Du et al.,\n2022; Lin et al., 2024; Bai et al., 2023), and (2)\nInstruction fine-tuning using instruction datasets\nto improve instruction-following abilities (Chen\net al., 2024b; Wang et al., 2024c; Bai et al., 2023;\nWang et al., 2024a; Li et al., 2023a, 2024). For in-\nstance, LLaV A (Li et al., 2024) introduces synthetic\ninstructions to fine-tune an instruction-following\nLVLM, while MiniGPT-v2 (Chen et al., 2023) em-\nploys unique task identifiers during fine-tuning to\nreduce instruction ambiguity.\nHallucination in LVLMs. Despite their impres-\nsive performance, LVLMs suffer from hallucina-\ntions , where model responses conflict with the im-\nages, instructions, or context (Du et al., 2022; Sun\net al., 2023; Xiao et al., 2025). To mitigate the hal-\nlucination, some methods have been proposed to\nfilter out long-tail or entity co-occurrence data (Liu\net al., 2023c; Yu et al., 2024a; Hu et al., 2023; Liu\net al., 2023a), though this involves high annotation\ncosts. Others recognized modal misalignment as\na key factor (Li et al., 2023e; Tong et al., 2024;\nCao et al., 2024; Jiang et al., 2024d,b), yet over-\nlooked inherent LLM errors. Post-processing tech-\nniques—optimizing decoding strategies (Zhang\n2\n--- Page 3 ---\net al., 2025; Huang et al., 2024; Yang et al., 2024;\nGao et al., 2024; Leng et al., 2024) or applying post-\nhoc corrections (Lee et al., 2023; Zhou et al., 2023;\nYin et al., 2023)—reduce hallucinations but add in-\nference cost. Human preference alignment has also\nemerged as an effective approach to mitigate hal-\nlucinations (Lan et al., 2024). LLaV A-RLHF (Sun\net al., 2023) pioneered this exploration in LVLMs,\nwhile RLHF-V (Yu et al., 2024b), RLAIF-V (Yu\net al., 2024c), and POVID (Zhou et al., 2024)\nfurther refined the approach with improved vi-\nsual localization, text segment scoring, and pref-\nerence example generation. However, these meth-\nods primarily focus on response-level preferences\nwhile neglecting the multimodal task’s require-\nments to align human preferences with images and\ninstructions in multiple aspects. MDPO (Wang\net al., 2024b) proposed image-conditional prefer-\nence alignment but overlooked aligning instruc-\ntions with human preferences. In contrast, our\nEMPO incorporates preferences across comprehen-\nsive aspects for efficient alignment between visual\ncontent and semantic concepts.\n3 Method\nIn this section, we detail the data construction\nand training process in the proposed EMPO. Sec-\ntion 3.1 introduces the foundation of preference\noptimization. Section 3.2 details the procedure to\nconstruct multimodal preference data. Section 3.3\nintroduces how EMPO addresses the hallucination\nby an entity-centric alignment training framework.\n3.1 Preliminaries\nDirect Preference Optimization (DPO) (Rafailov\net al., 2024) is a primary method for human prefer-\nence alignment that implicitly models the reward\nfunction in Reinforcement Learning from Human\nFeedback (Yu et al., 2024b). Given an instruction\nq, chosen response yw, and rejected response yl,\nDPO formulates the reward function as\nr(q, y) =βlogπθ(y|q)\nπref(y|q)+Z(q), (1)\nwhere Z(q)is a partition function, πθrepresents\nthe policy model, πrefis the reference model, and\nβis a hyperparameter controlling deviation from\nthe reference model. DPO directly optimizes:\nLDPO=−logσ(r(q, yw)−r(q, yl))\n=−logσ(βlogπθ(yw|q)\nπref(yw|q)−βlogπθ(yl|q)\nπref(yl|q)).(2)3.2 Preference Dataset Construction\nAs aforementioned, most existing multimodal pref-\nerence datasets (Zhou et al., 2024; Yu et al., 2024c)\nmerely align human preferences with the overall re-\nsponse, lacking focus on entity-centered key facts\nin the images and instructions. Therefore, we per-\nform comprehensive multimodal alignment via con-\nstructing multimodal preference datasets across the\nfull aspects: image context, instruction condition,\nand model response. As shown in Figure 2, we\nkeep the original images, instructions, and response\ndata as chosen samples, and then edit the entities,\nattributes, and relationships in three aspects respec-\ntively to construct rejected preference samples.\nImage Preference Data To construct the rejected\nimage, we initially employ GPT4o-mini (Achiam\net al., 2023) to identify entities in both the\ninstruction and response, ensuring close align-\nment between the edited image and text. Subse-\nquently, we use an object detection model to locate\nthese entities. Next, we apply Stable-diffusion-\n2 (SD2) (Rombach et al., 2022) to either remove\n30% entities or substitute them with visually plau-\nsible alternatives, thereby generating an edited im-\nage as a rejected image sample vl. Finally, we\nuse CLIP (Jiang et al., 2023) to calculate the sim-\nilarity between the edited image regions and the\nentity labels to ensure the image has been correctly\nedited. Based on the rejected image samples, these\nselected entities will be weighted as described in\nSection 3.3. We introduce two strategies to con-\nstruct image preference rejected samples ql: (1)\nEntity Deleting : Use SD2 to delete the chosen en-\ntities, helping the LVLM reduce the occurrence of\nnon-existent entities generated by the LVLM. (2)\nEntity Replacement : Use SD2 to replace the chosen\nentities with incorrect but high-frequency entities,\nhelping the LVLM overcome entity co-occurrence\nhallucinations (Du et al., 2022).\nInstruction Preference Data We employ\nGPT4o-mini (Achiam et al., 2023) to adapt the\noriginal instructions in positions of the selected\nentities above, as well as their related attributes\nand relationships, thereby constructing rejected\ninstructions ql. Consistent with the findings of\nHA-DPO (Zhao et al., 2023), we observe that the\ndistribution of GPT-modified instructions differs\nfrom the vanilla instructions, resulting in a decline\nin performance. We analyze that rejected samples\nserve as hard negative samples (Kalantidis et al.,\n3\n--- Page 4 ---\nWhere are the man \nand his dog  located in \nthis image?Where are the man \nand the lion  located \nin this image?The man and the lion\nare located on the \nbeach, near the ocean.\nThe man and his dog\nare located on the \nbeach, near the ocean.\nEntity -centric Multimodal \nPreference Optimization (EMPO)\nFinal LVLM\n(1) Multimodal Preference Construction (2) Multimodal Preference AlignmentRejected Image Preference Rejected Instruction Preference Rejected Response Preference\nChosen PreferenceCollected Multimodal Preference\nFigure 2: Illustration of our framework. (1) At the data level, we construct a fine-grained preference alignment\ndataset across three aspects: image, instruction condition, and model response. (2) At the method level, we propose\nentity-centric multimodal preference optimization for aligning image contents with semantic concepts.\n2020) because they are sufficiently similar to\nchosen samples while highlighting hallucination-\nrelated factual errors, which enhances LVLM’s\nattention to entity-centric key facts. We use\nword2vec (Mikolov et al., 2013) and rule-based\nmatching to ensure that the rejected instruction ql\nand chosen instruction qwhave different semantic\nmeanings but the same syntactic structure.\nResponse Preference Data We propose a gen-\neral multimodal preference data construction\nmethod applicable to any instruction dataset,\ndemonstrating it by construct response prefer-\nences from two open-source datasets. The first\nis POVID (Zhou et al., 2024) containing 17,332\nentries, where we collect the rejected image prefer-\nence sample (vl)and the rejected instruction pref-\nerence sample (ql)from the previous paragraphs,\nusing them as LVLM input to generate incorrect re-\nsponses as the rejected response preference sample\n(yl). The second is RLAIF-V (Yu et al., 2024c),\ncontaining 81,342 entries, where we employed\nMiniCPM-V2.5 (Yao et al., 2024) to compare two\ncandidate answers generated by LLaV A-1.5 (Li\net al., 2024), establishing preference rankings be-\ntween responses in four iterations. Human evalu-\nation conducted by experts on randomly sampled\nentries confirms the quality of these datasets. Due\nto space constraints, quality control and cost analy-\nsis details are included in Appendix A.\n3.3 Entity-centric Multimodal Preference\nOptimization\nIn the context of LVLMs, aligning human pref-\nerence includes three aspects: image, instruction,\nand response. To enhance LVLM’s attention toimage features and mitigate hallucinations caused\nby over-reliance on text modality, we align im-\nage and text modalities through explicit preference\noptimization of image instruction conditions and\nmodel responses. We define three optimization ob-\njectives: LDPOv for improving visual entity recog-\nnition, LDPOq for enhancing instruction following,\nandLDPOr directly align with human preference:\nLDPOv =−logσ(βlogπθ(y|vw, q)\nπref(y|vw, q)\n−βlogπθ(y|vl, q)\nπref(y|vl, q)),(3)\nLDPOq =−logσ(βlogπθ(y|v, qw)\nπref(y|v, qw)\n−βlogπθ(y|v, ql)\nπref(y|v, ql)),(4)\nLDPOr =−logσ(βlogπθ(yw|v, q)\nπref(yw|v, q)\n−βlogπθ(yl|v, q)\nπref(yl|v, q)),(5)\nwhere wandlrepresent chosen and rejected pref-\nerences respectively, and v=vw,q=qw,y=yw.\nIntroducing image conditions brings about more\ncomplex preference combinations, making it chal-\nlenging to use vanilla DPO to assign credit to de-\nsirable key facts, leading to reward hacking (Pan\net al., 2024). We propose assigning token weights\nto key entities in three aspects to solve this prob-\nlem. Specifically, we allocate higher weights to\ncritical features in the image, instruction, and re-\nsponse, thereby enhancing the LVLM’s focus on\nentity features and enabling it to distinguish hallu-\ncinated tokens from non-hallucinated ones better.\n4\n--- Page 5 ---\nIt is noteworthy that assigning token weights does\nnot incur additional effort, as the positions of high-\nweight tokens are already determined during the\nconstruction of preference data. The formula for\nassigning weights to model outputs is as follows:\nlogπ(y|v, q) = (1 −α)X\nyi/∈yelogp(yi|v, q, y <i)\n+αX\nyi∈yelogp(yi|v, q, y <i),(6)\nwhere αis a weighting hyperparameter, yiis the\ni-th token of response y, with larger αindicating\ngreater token influence on preference. In this way,\nemphasizing hallucination-related entities strength-\nens human preference feedback to the LVLM,\nthereby enhancing its factual accuracy.\nThe overall multimodal preference optimization\nobjective combines all three aspects:\nLEMPO =LDPOv +LDPOq +LDPOr,(7)\nwhere the LVLMs are optimized to fully align\nhallucination-related key facts with human pref-\nerence, reducing its hallucinations.\n4 Experiments\n4.1 Experimental settings\nFollowing Yu et al., we evaluate the models from\ntwo aspects: trustworthiness for hallucination and\nhelpfulness for general capability.\nFor the trustworthiness, we use three bench-\nmarks: (1) CHAIR (Yu et al., 2024b), a widely\nused hallucination detection benchmark, which\nevaluates the multimodal object hallucinations by\ncomparing the generated entities with manually\nlabeled entities in the COCO (Lin et al., 2014a).\nWe both report the sentence-level and the entity-\nlevel hallucination rate (denoted as CHAIR sand\nCHAIR i), respectively. (2) MMHal-Bench (Sun\net al., 2023), which uses GPT-4 to evaluate model\noutputs with human responses from two aspects:\nhallucination rate (Hall.) and information rich-\nness (Score). (3) AMBER (Wang et al., 2023),\nwhich evaluates multimodal hallucinations based\non 15220 yes-or-no questions. We report accuracy\n(Acc.) and F1 score on discriminative tasks.\nFor the helpfulness, we use two benchmarks:\n(1)LLaV A Bench (Li et al., 2024) assesses mul-\ntimodal understanding and reasoning capabilities,\nwith overall accuracy reported across all tasks. (2)\nMME (Fu et al., 2023) evaluates LVLMs on ten\nperception and four cognition subtasks, with re-\nported scores for both categories (Per. and Cog.).Baselines. We compare our method with state-of-\nthe-art baselines of various types, including general\nbaselines with strong performance and baselines\ndesigned to mitigate hallucinations.\nFirst, Vanilla LVLM baselines. We use the open-\nsource LVLMs, i.e., LLaV A-1.5 (Li et al., 2024),\nQwen-VL (Bai et al., 2023), and LLaV A-Next (Liu\net al., 2024b) VCD (Leng et al., 2024) for compar-\nison. Besides, we also include GPT-4V (OpenAI,\n2023) as a closed-source baseline.\nSecond, Fine-tuned LVLMs baselines. We in-\nclude seven fine-tuned LVLMs aiming at mitigating\nhallucinations: (1) Silkie (Li et al., 2023c), which\nfine-tunes LVLMs using diverse instruction and\nfeedback from GPT-4V . (2) LLaV A-RLHF (Sun\net al., 2023), which extends human feedback align-\nment from text-only models to the multimodal do-\nmain. (3) HA-DPO (Zhao et al., 2023), which\nproposes the first multimodal DPO algorithm. (4)\nmDPO (Wang et al., 2024b), which optimizes im-\nage preferences rather than language preferences to\navoid over-optimization issues. (5) POVID (Zhou\net al., 2023), which fine-tunes VLLMs using model-\ngenerated preference data that targets differences\nbetween image and text. (6) RLHF-V (Yu et al.,\n2024b), which eliminates hallucination of VLLMs\nusing high-quality human feedback to improve pre-\ncise behavior boundaries. (7) RLAIF-V (Yu et al.,\n2024c), which automatically synthesizes prefer-\nence data and trains the model using iterative DPO.\nTraining Datasets. We use the following prefer-\nence datasets for training: (1) POVID (Zhou et al.,\n2024) incorporating 17,000 randomly sampled ex-\namples from the LLaV A-Instruct-150K dataset (Liu\net al., 2023b). Its hallucinated responses are pro-\nduced by using GPT-4V (OpenAI, 2023), which\nintroduces potential errors in areas like object co-\noccurrence. (2) RLAIF-V (Yu et al., 2024c) is an\nopen-source feedback dataset, including 4,000 in-\nstructions from 7 sources, such as MSCOCO (Lin\net al., 2014b), Google Landmark v2 (Weyand et al.,\n2020), and VQA-v2 (Goyal et al., 2017). Each\nRLAIF-V instruction pairs with multiple open-\nsource LVLM-generated responses, with more ca-\npable LVLMs determining response preferences.\nImplementation Details. We implement EMPO\nbased on the LLaV A-v1.5-7B (Li et al., 2024),\nwhich uses CLIP-ViT (Radford et al., 2021) as\nthe vision module and Vicuna (Zheng et al., 2023)\nas the LLM backbone. We train the EMPO for 4\nepochs using DeepSpeed (Lian et al., 2024), which\n5\n--- Page 6 ---\nModel Model SizeObject-HalBench MMHal-Bench AMBER\nCHAIR s↓ CHAIR i↓ Hall.↓ Score↑ Acc.↑ F1↑\nGPT-4V (OpenAI, 2023)▲GPT-4V - 13.6 7.3 28.1 3.42 83.4 87.4\nVanilla LVLMs\nQWEN-VL (Bai et al., 2023)▲Qwen-VL-Chat 10B 40.4 20.7 38.5 2.76 81.9 86.4\nLLaV A-NeXT (Liu et al., 2024b)▲LLaV A-NeXT 34B 12.6 6.4 34.4 3.14 81.4 85.4\nVCD (Leng et al., 2024)▲LLaV A-1.5 7B 48.8 24.3 54.2 2.12 71.8 74.9\nLLaV A-1.5 (Li et al., 2024) LLaV A-1.5 7B 52.3 25.5 52.7 2.36 73.5 77.7\nLLaV A-1.5 (Li et al., 2024) LLaV A-1.5 13B 50.7 24.8 51.4 2.39 81.8 87.3\nFine-tuned LVLMs\nLLaV A-RLHF (Sun et al., 2023)▲LLaV A-1.5 13B 38.1 18.9 62.5 2.02 79.7 83.9\nSilkie (Li et al., 2023c)▲Qwen-VL-Chat 10B 27.1 13.4 32.3 3.19 82.2 87.6\nHA-DPO (Zhao et al., 2023)▲LLaV A-1.5 7B 39.9 19.9 60.4 1.98 75.2 79.9\nPOVID (Zhou et al., 2024)▲LLaV A-1.5 7B 40.4 19.1 56.2 2.08 82.9 87.4\nRLHF-V (Yu et al., 2024b)▲Muffin 13B 12.2 7.5 51.0 2.45 72.6 75.0\nRLAIF-V (Yu et al., 2024c)▲LLaV A-1.5 7B 8.5 4.3 29.2 3.06 76.8 84.5\nMDPO (Wang et al., 2024b)♢LLaV A-1.5 7B 35.7 9.8 54.0 2.39 73.4 74.7\nDPO (POVID DataSet) LLaV A-1.5 7B 48.9 22.4 56.0 2.15 75.1 78.9\nDPO (RLAIF-V DataSet) LLaV A-1.5 7B 19.13 9.32 36.6 2.70 76.8 81.5\nEMPO (POVID DataSet) LLaV A-1.5 7B 38.1 19.3 49.1 2.58 82.7 87.1\nEMPO (RLAIF-V DataSet) LLaV A-1.5 7B 7.16 3.44 25.6 3.21 82.4 87.7\nTable 1: Main experimental results. The best and second-best results are highlighted in bold andunderlined ,\nrespectively.▲denotes the results are reported by RLAIF-V (Yu et al., 2024c), and♢denotes the results from the\noriginal papers.\nModelLLaV A-Bench MME\noverall ↑ Cog.↑ Per.↑\nLLaV A-1.5 60.6 297.5 1496.7\n+ DPO 66.4 +9.57% 299.3 +0.61% 1356.7 -9.35%\n+ EMPO 69.3 +14.36% 302.8 +1.78% 1389.8 -7.14%\nTable 2: General capability evaluation results. Red indi-\ncates improvements after using EMPO, green indicates\nperformance decline.\nis an open-source library by Microsoft for efficient\ndistributed training. We set a hyperparameter αof\n0.7 and βof 0.5, an image resolution of 336*336,\na learning rate of 5e-7, and a batch size of 8. The\ntraining is conducted on 8 A100 GPUs, taking 4\nhours on the POVID dataset and approximately 12\nhours on the RLAIF-V dataset.\n4.2 Main Results\nAs shown in Table 1: (1) EMPO can substantially\nreduce hallucinations in the baseline model LLaV A-\n1.5-7B. EMPO, trained on either POVID or RLAIF-\nV , reduces LLaV A-1.5-7B’s object hallucination\nrates on Object HalBench by 24.8% and 85.9%,\nrespectively. Additionally, EMPO increases the ac-\ncuracy (F1) on the AMBER benchmark by 12.5%\n(12.9%) relative percentage points and achieves\ncontinuous improvement in terms of overall Hallu-\ncination rate on the MMHal dataset. (2) EMPO con-\nsistently outperforms DPO across all three bench-\nmarks and two training datasets, with better hallu-\ncination reduction effects. This indicates that theproposed EMPO method can effectively improve\nmodal alignment. (3) EMPO achieves state-of-the-\nart performance in trustworthiness among open-\nsource models, even outperforming commercial\nmodels like GPT-4V .\n4.3 Ablation Studies\nOur framework consists of two key compo-\nnents: Multi-modal Preference Alignment and\nFine-grained Entity Weighting. As shown in Ta-\nble 4, to verify the contribution of each component\nin our framework, we conduct comprehensive abla-\ntion studies on the RLAIF-V dataset.\nAblation of Multi-modal Preference Alignment.\nWe first examine the necessity of aligning with\nhuman preferences across the image ,instruction ,\nandresponse modalities, respectively. Removing\nany modality preference markedly increases hallu-\ncination: +22.9%, +9.5%, and +11.9% for image,\ninstruction, and response, respectively, but still out-\nperforms vanilla DPO. The full three-modal align-\nment performs best, showing that jointly modeling\nvisual and textual signals captures human intent\nmore comprehensively.\nAblation of Fine-grained Entity Weighting. To\nevaluate the effect of explicitly emphasizing key\nentities, as shown in Table 4, removing entity\nweights in Formula 6 increases hallucination by\n2.6%. Moreover, we study the impact of weighting\n6\n--- Page 7 ---\nChosen (a) Delete (c) Replace\n(d) Crop (e) Noise (f) Random\nFigure 3: Examples of different rejected image construction strategies and the hallucination rates.\nTable 3: Ablation results on different rejected instruc-\ntion & response sampling strategies.\nStrategyObject-HalBench MMHal-Bench\nCHAIR s↓CHAIR i↓Hall.↓Res↑\nRandom Instruction 7.61 4.10 39.9 2.70\nEdit Instruction (EMPO) 7.16 3.44 25.6 3.21\nPOVID Response 38.10 19.30 49.1 2.58\nRLAIFV Response (EMPO) 7.16 3.44 25.6 3.21\nTable 4: Ablation results on different components. We\nindicates “w/o image / instruction / response” denotes\nremoving the corresponding modality preference; “w/o\nweighting” removes the weights on key entities.\nModelObject-HalBench MMHal-Bench\nCHAIR s↓CHAIR i↓Hall.↓Score↑\nDPO 19.13 9.32 36.6 2.70\nEMPO 7.16 3.44 25.6 3.21\nw/o image 10.8 5.4 34.4 2.78\nw/o instruction 8.5 4.1 30.9 2.96\nw/o response 11.2 6.3 32.1 2.87\nw/o weighting 7.45 3.88 28.1 3.12\ncoefficient αin Formula 6 that balances the entity-\ncentric term and generic preference-alignment loss.\nFigure 4 shows that α=0.7yields the lowest hallu-\ncination rate on both ObjectHalBench and MMHal-\nBench, indicating that assigning higher weights to\nentity-related tokens can effectively mitigate hallu-\ncination, while the overall semantic coherence of\nthe response cannot be ignored.\n4.4 Analysis\nGeneral Capability Analysis. Previous studies\nshow that preference learning may impair mod-\nels’ general understanding capabilities (Xiao et al.,\n2024; Lan et al., 2024). We evaluate LVLMs’ gen-\neral capabilities after EMPO training using rec-\nognized evaluation datasets: LLaV A-Bench and\nMME’s Perception and Cognition assessments. Ta-\nble 2 shows EMPO improves LVLM performance\non LLaV A-Bench and MME (cog.) by 14.36%\nand 1.78% respectively. On MME (Perception),\nEMPO shows a slight 7.14% decrease versus the\nbaseline model, though it still outperforms DPO.\nThese results indicate that while EMPO reduces\n1.0 0.7 0.5 0.2 0.0\nWeighting Coefficient \n678910CHAIRs \n7.94\n7.167.457.858.24Object-HalBench\nCHAIRs \nCHAIRi \n1.0 0.7 0.5 0.2 0.0\nWeighting Coefficient \n2025303540Hall. \n33.3\n25.628.130.031.2MMHal-Bench\nHall. \nScore \n345\nCHAIRi \n4.4\n3.443.883.924.19\n2.53.03.5\nScore \n2.993.21\n3.12\n3.033.0Figure 4: Hyper-parameter ablation on the α.\nhallucination in LVLMs, it maintains the model’s\ngeneral understanding capabilities.\nImpact of Rejection Construction Strategy.\nThe construction strategy of preference samples\nsignificantly impacts the performance of preference\nalignment algorithms (Jiang et al., 2024c). Below,\nwe compare different strategies with our vanilla\nmethod in three modalities.\nForrejected images , we test five strategies: (1)\nDelete: using Stable-Diffusion-2 to remove key\nentities; (2) Replace: using Stable-Diffusion-2 to\nreplace key entities with same-type entities; (3)\nCrop: randomly cropping the chosen image; (4)\nNoise: adding Gaussian noise; (5) Random: select-\ning a random training set image. Figure 3 shows\nDelete andReplace outperform others. These strate-\ngies maintain structural similarity with chosen im-\nages while introducing significant object represen-\ntation differences, serving as hard negative sam-\nples (Kalantidis et al., 2020) and enabling finer-\ngrained modal feature alignment. We finally adopt\na combination of the Delete and Replace as rejected\nimage construction strategies.\nForrejected instructions , we test two strategies:\n(1) Edit: randomly replacing/deleting entity-related\n7\n--- Page 8 ---\nQuestion: What is the gender of the person riding the motorcycle in the image?\nLLaVA -1.5: The person riding the motorcycle in the image is a woman .\nEMPO (ours): In the image, there is a motorcycle present, but there is no person  \nriding it, so it is not possible to determine the gender of the person who would be \nriding it. The focus of the image is on the silver and black motorcycle parked on the \nside of the road.\n(a) LLaV A -1.5\n (b) EMPO (Ours)Figure 5: Attention weight heatmaps of outputs to image and instruction tokens from LLaV A-1.5 and EMPO. The\nhallucinated tokens are highlighted in red. The image is tokenized into 4×4 patches (above the red dash line).\nnouns, attributes, and relationships. (2) Random:\nselecting a different random instruction from the\ntraining set. As shown in Table 3, Edit achieves a\nlower hallucination rate than Random, so we select\nEdit as our primary strategy.\nForrejected responses , we experiment with: (1)\nPOVID (Zhou et al., 2024): using LVLM-generated\nresponses from rejected images. (2) RLAIF-V (Yu\net al., 2024c): constructing preference pairs using\nan evaluation model. Table 3 shows RLAIF-V sig-\nnificantly outperforms POVID, thus we adopt it for\nour final EMPO implementation, yet to verify our\napproach’s robustness across different data config-\nurations, we report the results of both instruction\ndatasets compared against DPO in Section 4.2.\nModality Alignment Visualization. Figure 5\npresents attention heatmaps illustrating how model\noutput tokens (horizontal axis) attend to input im-\nage patches (above the red line) and instruction\ntokens (below the red line) during inference. In\nthe baseline LLaV A-1.5 (Figure 5(a)), the attention\nheatmap reveals that while generating the hallu-\ncinated entity \"woman\" , the model incorrectly fo-\ncuses on specific image patches (e.g., <PATCH_6>,\n<PATCH_11>) and the word \"gender\" in the ques-\ntion. This misaligned attention pattern exemplifies\nhow the model grounds erroneous assertions in vi-\nsual features, leading to hallucination. In contrast,our EMPO-enhanced model (Figure 5(b)) demon-\nstrates significantly improved modality alignment.\nWhen inferring \"no person\" and describing \"sil-\nver and black motor\" , the model’s attention cor-\nrectly concentrates on image patches showing only\nthe motorcycle (e.g., <PATCH_6>, <PATCH_7>,\n<PATCH_11>), effectively verifying the absence of\na rider. This precise attention allocation shows how\nEMPO helps LLaV A-1.5 correctly attend to key\nfacts in both the image and user instruction, foster-\ning stronger semantic consistency between visual\ninputs and generated text to avoid hallucination.\n5 Conclusion\nThis paper addresses the LVLM hallucination prob-\nlem from two perspectives: modality misalignment\nand LLM inherent hallucination. At the method\nlevel, we propose a comprehensive multimodal\npreference optimization method to help LVLM\nalign entity features with semantic concepts, en-\nhancing its trustworthiness. For data side, we in-\ntroduce a general method for constructing multi-\nmodal preference data. Experiments on multiple\nbenchmarks show our method significantly reduces\nhallucinations while maintaining LVLM capabil-\nities. For future work, we will explore common-\nsense knowledge in multimodal domains and inves-\ntigate hallucinations in long-term interactive envi-\nronments like multi-turn dialogue.\n8\n--- Page 9 ---\nLimitations\nA limitation of this paper is that the investigation\ninto hallucinations was restricted to entity-centric\nhallucinations. Although entity-centric halluci-\nnations constitute a major component of multi-\nmodal hallucinations, non-entity-related hallucina-\ntions such as common sense knowledge and long\ncontext memory loss are also important aspects for\noptimizing LVLM effectiveness. Due to space lim-\nitations and the complexity of defining common\nsense and long context, we did not explore these\nissues in this paper. We propose the following direc-\ntions for future research: (1) Exploring methods to\ndefine common-sense knowledge in multimodal do-\nmains and its relationship with hallucinations. (2)\nInvestigating hallucination issues in a long-term in-\nteractive environment, such as multi-turn dialogue.\nEthics Statement\nThis study focuses on mitigating hallucination phe-\nnomena in LVLMs to enhance their reliability and\ntrustworthiness. We have carefully considered the\nethical implications of the research and do not ex-\npect any major ethical issues to arise. This study is\nbased on publicly available and widely used data\nand models; therefore, our findings may inherit the\nbiases and limitations present in these resources.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, and 1 others. 2023. Gpt-4 techni-\ncal report. arXiv preprint arXiv:2303.08774 .\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, and 1 others. 2022. Flamingo: a visual\nlanguage model for few-shot learning. Advances in\nneural information processing systems , 35:23716–\n23736.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966 .\nYuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and\nJiaqi Wang. 2024. Dualfocus: Integrating macro and\nmicro perspectives in multi-modal large language\nmodels. arXiv preprint arXiv:2402.14767 .Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu, Pengchuan Zhang, Raghuraman Krishnamoor-\nthi, Vikas Chandra, Yunyang Xiong, and Mohamed\nElhoseiny. 2023. Minigpt-v2: large language model\nas a unified interface for vision-language multi-task\nlearning. arXiv preprint arXiv:2310.09478 .\nLin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Con-\nghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\n2024a. Sharegpt4v: Improving large multi-modal\nmodels with better captions. In European Confer-\nence on Computer Vision , pages 370–387. Springer.\nRuibo Chen, Yihan Wu, Lichang Chen, Guodong Liu,\nQi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, and\nHeng Huang. 2024b. Your vision-language model\nitself is a strong filter: Towards high-quality in-\nstruction tuning with data selection. arXiv preprint\narXiv:2402.12501 .\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja-\nson Weston. 2023. Chain-of-verification reduces hal-\nlucination in large language models. arXiv preprint\narXiv:2309.11495 .\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao.\n2022. A survey of vision-language pre-trained mod-\nels.arXiv preprint arXiv:2202.10936 .\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei\nQin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xi-\nawu Zheng, Ke Li, Xing Sun, and 1 others. 2023.\nMme: A comprehensive evaluation benchmark for\nmultimodal large language models. arXiv preprint\narXiv:2306.13394 .\nMinghe Gao, Shuang Chen, Liang Pang, Yuan Yao,\nJisheng Dang, Wenqiao Zhang, Juncheng Li, Siliang\nTang, Yueting Zhuang, and Tat-Seng Chua. 2024.\nFact: Teaching mllms with faithful, concise and trans-\nferable rationales. In Proceedings of the 32nd ACM\nInternational Conference on Multimedia , pages 846–\n855.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition , pages 6904–6913.\nTianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, and 1 others. 2024.\nHallusionbench: an advanced diagnostic suite for\nentangled language hallucination and visual illusion\nin large vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 14375–14385.\nAnisha Gunjal, Jihan Yin, and Erhan Bas. 2024. De-\ntecting and preventing hallucinations in large vision\nlanguage models. In Proceedings of the AAAI Con-\nference on Artificial Intelligence .\n9\n--- Page 10 ---\nHongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang\nSun. 2023. Ciem: Contrastive instruction evaluation\nmethod for better instruction tuning. arXiv preprint\narXiv:2309.02301 .\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,\nConghui He, Jiaqi Wang, Dahua Lin, Weiming\nZhang, and Nenghai Yu. 2024. Opera: Alleviating\nhallucination in multi-modal large language models\nvia over-trust penalty and retrospection-allocation. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 13418–\n13427.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International conference on ma-\nchine learning , pages 4904–4916. PMLR.\nChaoya Jiang, Hongrui Jia, Mengfan Dong, Wei Ye,\nHaiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang.\n2024a. Hal-eval: A universal and fine-grained hal-\nlucination evaluation framework for large vision lan-\nguage models. In Proceedings of the 32nd ACM\nInternational Conference on Multimedia , pages 525–\n534.\nChaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing\nChen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei\nHuang, and Shikun Zhang. 2024b. Hallucination\naugmented contrastive learning for multimodal large\nlanguage model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition , pages 27036–27046.\nDongsheng Jiang, Yuchen Liu, Songlin Liu, Jin’e Zhao,\nHao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and\nHongkai Xiong. 2023. From clip to dino: Visual\nencoders shout in multi-modal large language models.\narXiv preprint arXiv:2310.08825 .\nRuili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Jun-\ntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, and\nMin Zhang. 2024c. A survey on human preference\nlearning for large language models. arXiv preprint\narXiv:2406.11191 .\nSongtao Jiang, Yan Zhang, Chenyi Zhou, Yeying Jin,\nYang Feng, Jian Wu, and Zuozhu Liu. 2024d. Joint\nvisual and text prompting for improved object-centric\nperception with multimodal large language models.\narXiv preprint arXiv:2404.04514 .\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and\nWeidi Xie. 2022. Prompting visual-language mod-\nels for efficient video understanding. In European\nConference on Computer Vision , pages 105–124.\nSpringer.\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion,\nPhilippe Weinzaepfel, and Diane Larlus. 2020. Hard\nnegative mixing for contrastive learning. Advances\nin neural information processing systems , 33:21798–\n21809.Wei Lan, Wenyi Chen, Qingfeng Chen, Shirui Pan,\nHuiyu Zhou, and Yi Pan. 2024. A survey of hal-\nlucination in large visual language models. arXiv\npreprint arXiv:2410.15359 .\nSeongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-\njoon Seo. 2023. V olcano: mitigating multimodal\nhallucination through self-feedback guided revision.\narXiv preprint arXiv:2311.07362 .\nSicong Leng, Hang Zhang, Guanzheng Chen, Xin\nLi, Shijian Lu, Chunyan Miao, and Lidong Bing.\n2024. Mitigating object hallucinations in large vision-\nlanguage models through visual contrastive decoding.\nInProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages\n13872–13882.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2024. Llava-\nmed: Training a large language-and-vision assistant\nfor biomedicine in one day. Advances in Neural In-\nformation Processing Systems , 36.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023a. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In International conference on ma-\nchine learning , pages 19730–19742. PMLR.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023b. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models. arXiv preprint arXiv:2305.11747 .\nLei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi\nWang, Liang Chen, Yazheng Yang, Benyou Wang,\nand Lingpeng Kong. 2023c. Silkie: Preference dis-\ntillation for large visual language models. arXiv\npreprint arXiv:2312.10665 .\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\nWayne Xin Zhao, and Ji-Rong Wen. 2023d. Eval-\nuating object hallucination in large vision-language\nmodels. arXiv preprint arXiv:2305.10355 .\nZhuang Li, Yuyang Chai, Terry Yue Zhuo, Lizhen\nQu, Gholamreza Haffari, Fei Li, Donghong Ji, and\nQuan Hung Tran. 2023e. Factual: A benchmark for\nfaithful and consistent textual scene graph parsing.\narXiv preprint arXiv:2305.17497 .\nXinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro\nTanaka, Stas Bekman, Olatunji Ruwase, and Minjia\nZhang. 2024. Universal checkpointing: Efficient\nand flexible checkpointing for large scale distributed\ntraining. Preprint , arXiv:2406.18820.\nJi Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mo-\nhammad Shoeybi, and Song Han. 2024. Vila: On pre-\ntraining for visual language models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 26689–26699.\n10\n--- Page 11 ---\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014a. Microsoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740–755. Springer.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014b. Microsoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740–755. Springer.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser\nYacoob, and Lijuan Wang. 2023a. Mitigating hal-\nlucination in large multi-modal models via robust\ninstruction tuning. In The Twelfth International Con-\nference on Learning Representations .\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,\nXiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,\nand Wei Peng. 2024a. A survey on hallucination\nin large vision-language models. arXiv preprint\narXiv:2402.00253 .\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. 2024b. Llava-\nnext: Improved reasoning, ocr, and world knowledge.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning.\nShi Liu, Kecheng Zheng, and Wei Chen. 2024c. Pay-\ning more attention to image: A training-free method\nfor alleviating hallucination in lvlms. In European\nConference on Computer Vision , pages 125–140.\nSpringer.\nYanqing Liu, Kai Wang, Wenqi Shao, Ping Luo,\nYu Qiao, Mike Zheng Shou, Kaipeng Zhang,\nand Yang You. 2023c. Mllms-augmented visual-\nlanguage representation learning. arXiv preprint\narXiv:2311.18765 .\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-\noshu Li, Hao Yang, and 1 others. 2024. Deepseek-vl:\ntowards real-world vision-language understanding.\narXiv preprint arXiv:2403.05525 .\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781 .\nOpenAI. 2023. Gpt-4v system card.\nAlexander Pan, Erik Jones, Meena Jagadeesan, and\nJacob Steinhardt. 2024. Feedback loops with lan-\nguage models drive in-context reward hacking. arXiv\npreprint arXiv:2402.06627 .Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, and\n1 others. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nconference on machine learning , pages 8748–8763.\nPMLR.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2024. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in Neu-\nral Information Processing Systems , 36.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object\nhallucination in image captioning. arXiv preprint\narXiv:1809.02156 .\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) ,\npages 10684–10695.\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,\nChunyuan Li, Yikang Shen, Chuang Gan, Liang-\nYan Gui, Yu-Xiong Wang, Yiming Yang, and 1\nothers. 2023. Aligning large multimodal mod-\nels with factually augmented rlhf. arXiv preprint\narXiv:2309.14525 .\nShengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma,\nYann LeCun, and Saining Xie. 2024. Eyes wide\nshut? exploring the visual shortcomings of multi-\nmodal llms. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition ,\npages 9568–9578.\nBin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping\nZhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei\nLi, Jiaqi Wang, and 1 others. 2024a. Vigc: Visual\ninstruction generation and correction. In Proceedings\nof the AAAI Conference on Artificial Intelligence .\nFei Wang, Wenxuan Zhou, James Y Huang, Nan Xu,\nSheng Zhang, Hoifung Poon, and Muhao Chen.\n2024b. mdpo: Conditional preference optimiza-\ntion for multimodal large language models. arXiv\npreprint arXiv:2406.11839 .\nJunyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang,\nYukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and\nJitao Sang. 2023. An llm-free multi-dimensional\nbenchmark for mllms hallucination evaluation. arXiv\npreprint arXiv:2311.07397 .\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie\nZhou, Yu Qiao, and 1 others. 2024c. Visionllm:\nLarge language model is also an open-ended decoder\nfor vision-centric tasks. Advances in Neural Informa-\ntion Processing Systems , 36.\n11\n--- Page 12 ---\nTobias Weyand, Andre Araujo, Bingyi Cao, and Jack\nSim. 2020. Google landmarks dataset v2-a large-\nscale benchmark for instance-level recognition and\nretrieval. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition ,\npages 2575–2584.\nWenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He,\nHaoyuan Li, Zhelun Yu, Fangxun Shu, Hao Jiang,\nand Linchao Zhu. 2025. Detecting and mitigating\nhallucination in large vision language models via\nfine-grained ai feedback. In Proceedings of the AAAI\nConference on Artificial Intelligence .\nWenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao,\nWanggui He, Luu Anh Tuan, Long Chen, Hao\nJiang, Zhou Zhao, and Fei Wu. 2024. A com-\nprehensive survey of direct preference optimiza-\ntion: Datasets, theories, variants, and applications.\nPreprint , arXiv:2410.15595.\nDingchen Yang, Bowen Cao, Guang Chen, and\nChangjun Jiang. 2024. Pensieve: Retrospect-then-\ncompare mitigates visual hallucination. arXiv\npreprint arXiv:2403.14401 .\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin\nZhao, Zhihui He, and 1 others. 2024. Minicpm-v:\nA gpt-4v level mllm on your phone. arXiv preprint\narXiv:2408.01800 .\nShukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao\nWang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,\nand Enhong Chen. 2023. Woodpecker: Hallucina-\ntion correction for multimodal large language models.\narXiv preprint arXiv:2310.16045 .\nQifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wen-\ntao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and\nYueting Zhuang. 2024a. Hallucidoctor: Mitigating\nhallucinatory toxicity in visual instruction data. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 12944–\n12953.\nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng\nHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao\nZheng, Maosong Sun, and 1 others. 2024b. Rlhf-v:\nTowards trustworthy mllms via behavior alignment\nfrom fine-grained correctional human feedback. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 13807–\n13816.\nTianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang,\nDa Chen, Xiaoman Lu, Ganqu Cui, Taiwen He,\nZhiyuan Liu, Tat-Seng Chua, and 1 others. 2024c.\nRlaif-v: Aligning mllms through open-source ai feed-\nback for super gpt-4v trustworthiness. arXiv preprint\narXiv:2405.17220 .\nCe Zhang, Zifu Wan, Zhehan Kan, Martin Q Ma, Si-\nmon Stepputtis, Deva Ramanan, Russ Salakhutdi-\nnov, Louis-Philippe Morency, Katia Sycara, and Yaqi\nXie. 2025. Self-correcting decoding with generativefeedback for mitigating hallucinations in large vision-\nlanguage models. arXiv preprint arXiv:2502.06130 .\nPan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao,\nRui Qian, Lin Chen, Qipeng Guo, Haodong Duan,\nBin Wang, Linke Ouyang, and 1 others. 2024.\nInternlm-xcomposer-2.5: A versatile large vision lan-\nguage model supporting long-contextual input and\noutput. arXiv preprint arXiv:2407.03320 .\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, and 1 others. 2023. Siren’s song in the\nai ocean: a survey on hallucination in large language\nmodels. arXiv preprint arXiv:2309.01219 .\nZhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong,\nJiaqi Wang, and Conghui He. 2023. Beyond hallu-\ncinations: Enhancing lvlms through hallucination-\naware direct preference optimization. arXiv preprint\narXiv:2311.16839 .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in Neural Information Pro-\ncessing Systems , 36:46595–46623.\nYiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea\nFinn, and Huaxiu Yao. 2024. Aligning modalities\nin vision large language models via preference fine-\ntuning. arXiv preprint arXiv:2402.11411 .\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun\nZhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\nHuaxiu Yao. 2023. Analyzing and mitigating object\nhallucination in large vision-language models. arXiv\npreprint arXiv:2310.00754 .\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592 .\n12\n--- Page 13 ---\nA Quality control and Cost analysis\nA.1 Quality Control\nTo rigorously assess the effectiveness of our entity-\nlevel image editing strategy, we conducted a de-\ntailed analysis using CLIP-based semantic similar-\nity metrics. For each edit, we separately evaluated\nthe similarity between the image region and both\nthe original (source) entity label as well as the in-\ntended (target) entity label, before and after editing.\nThis dual-perspective evaluation enables objective\nmeasurement of whether the edit not only increases\nthe semantic presence of the desired entity, but also\nreduces residual evidence of the original entity. We\nemployed CLIP (Jiang et al., 2023) to compute\nthe semantic similarity between localized image re-\ngions and corresponding entity labels. Figure 6(a)\noverlays the distributions of CLIP similarity scores\nfor both the original and target entities before and\nafter editing. The results show a pronounced and\ndesirable shift: after editing, the similarity between\nthe image region and the target entity rises sharply,\nforming a high-density mode near 0.8, while the\nsimilarity to the original entity drops toward zero.\nBefore editing, the pattern is reversed, with image\nregions matching the original entity but not the\nintended target. This demonstrates our method’s\nability to both install semantic cues for the target\nentity and erase clues of the original entity. Entity-\nwise statistics (Figure 6(b)) show that this effect\nis robust across all five entity types. Regardless\nof the category, the target entity’s median similar-\nity rises after editing, while the original entity’s\nsimilarity markedly falls, confirming entity token\nmanipulation at the vision-language level. Cate-\ngories such as “person” and “car” benefit most, re-\nflecting their strong visual signatures in both CLIP\ntraining and entity editing efficacy. To quantify the\nimpact, Figure 6(c) presents the empirical distribu-\ntion of similarity changes. Target entity similarity\ntypically improves by about 0.6 on average post-\nediting, while the original entity’s similarity most\noften decreases, with minimal overlap in the two\ndistributions, indicating reliable entity substitution\nwhile minimizing unintended blending or halluci-\nnation. Finally, Figure 6(d) summarizes success\nrates across a range of similarity thresholds. For\nthresholds above 0.5, over 90% of image regions\nnow surpass the success criterion for the target en-\ntity, while the corresponding rate for the original\nentity drops near zero. This robust separation di-\nrectly validates that our method transforms imageevidence is both semantically meaningful and dis-\ncriminative.\nIn summary, our CLIP-based analysis verifies\nthat the proposed editing pipeline achieves two crit-\nical goals: (1) enhancing alignment between edited\nimage regions and their newly assigned (target)\nentities, and (2) effectively suppressing residual\nevidence of the original entities. This fine-grained\ncontrol supports our wider claim that both image\nand language data in our preference construction\nexhibit clear, contrastive, and controllable entity-\ncentric differences while maintaining structural\nconsistency.\nA.2 Cost analysis\nHaving established robust quality safeguards, we\nnow consider quantifying the computational re-\nquirements and monetary costs of each processing\nstep. Extracting image entities costs about $0.002\nper entry; generating all 17,332 image entries on 8\nA100 GPUs takes roughly 4–6 hours. Instruction-\nlevel entity extraction and rewriting add $0.002 and\n$0.003 per entry, respectively. In total, the label-\ning cost is $0.007 per entry plus 8.3 seconds of\ncompute time on a single A100. By contrast, man-\nual annotation would cost approximately $0.30 per\nentry, over 40× higher.\nB Hallucination Cause Analysis\nExperiment\nTo better understand the severity of hallucinations\nin LVLMs, we conducted a pilot experiment to eval-\nuate their inference performance. Specifically, we\nassessed the models on 200 preference examples se-\nlected from the POVID dataset (Zhou et al., 2024).\nThrough this analysis, we identify two prominent\ntypes of errors in LVLM responses, which high-\nlight critical limitations in their reasoning and mul-\ntimodal understanding capabilities.\n1.Concept Confusion: We observe that LVLMs\nstruggle to accurately interpret semantic rela-\ntionships between entities, leading to concept\nconfusion. The models frequently generate\nidentical or highly similar responses to user\ninstructions that were semantically conflict-\ning or conceptually distinct, which suggests\nthat LVLMs may fail to fully grasp the fine-\ngrained differences between related but dis-\ntinct concepts, resulting in responses that lack\nprecision and contextual appropriateness.\n13\n--- Page 14 ---\nFigure 6: CLIP-based similarity analysis for image editing evaluation: (a) Distribution of similarity scores before\nand after editing, (b) Box plots showing similarity changes by entity type, (c) Distribution of similarity improvement\nscores, and (d) Success rates at different similarity thresholds.\n2.Visual Neglect: When provided with only tex-\ntual context (i.e., without accompanying vi-\nsual input), the models tend to generate image-\nagnostic responses that disregard the potential\nrelevance of visual information. This behavior\nindicates an over-reliance on textual cues and\ninsufficient attention to visual content, which\nwe attribute to the influence of LLM-induced\nhallucinations. Such hallucinations appear to\nbias the models toward text-based reasoning,\neven in scenarios where visual understanding\nis critical. This is also in line with the previous\nwork PAI (Liu et al., 2024c)\nThese findings highlight the challenges LVLMs\nface in achieving robust multimodal understanding\nand highlight the need for improved mechanisms\nto mitigate hallucinations. Addressing these issues\nis essential for enhancing the reliability and appli-\ncability of LVLMs in real-world tasks that require\nboth textual and visual reasoning.C Prompt Appendix\nThe section describes the GPT4o-mini prompt for\nidentifying entities, as well as the prompts for\nrewriting chosen instructions and rejected instruc-\ntions.\nThe prompt for rewriting the chosen instruction:\n# prompt for rewriting chosen instruction\nprompt = ’’’Task : Rephrase the following question\nwhile maintaining its original meaning :\nOriginal question : { question }\nRequirements :\n1. If original question was a declarative sentence ,\nthen keep rewritten question as a declarative\nsentence .\n2. Ensure the rephrased question is clear , concise ,\nand maintains the original inquiry intent .\n3. You may adjust sentence structure or wording , but\ndo not change the essence of the question .\n4. If necessary , slightly expand the question to\nimprove clarity , but keep it concise .\n5. Use natural , fluent English in the rephrased\nversion .\nPlease only provide the rephrased question that\nmeets these criteria without any additional\nexplanation .\n’’’\n14\n--- Page 15 ---\nThe prompt for rewriting rejected instruction:\n# prompt for rewriting rejected instruction\n’’’You are an expert in creative writing and\nlinguistic transformation . Your task is to rewrite\nthe given question so that its meaning is\nsignificantly different from the original , while\nmaintaining the same general structure and format .\nFollow these guidelines :\n1. Analyze the original question ’s structure , tone ,\nand key elements .\n2. Identify a different perspective or context that\ncould radically change the question ’s meaning .\n3. Rewrite the question using the new perspective ,\nensuring it has a distinctly different meaning .\n4. Maintain the original question ’s format ,\nincluding any specific phrasing or sentence\nstructure .\n5. Ensure the rewritten question is coherent ,\ngrammatically correct , and makes sense on its own .\nOriginal question : { question }\nRewritten question :\nProvide only the rewritten question without any\nadditional explanation .’’’\n’’’\nThe prompt for identifying entities:\n# prompt for identifying entities\nprompt = ’’’\nYou are a selective entity replacement engine . You\nneed to perform entity replacement on the original\ntext .\nCore Instructions :\n1. Analyze the input text to identify replaceable\nentities .\n2. Randomly select approximately 50% of the\nidentified entities for substitution .\n3. Replace the chosen entities with contextually\nappropriate alternatives .\n4. Maintain grammatical correctness and readability .\n5. Output the modified version and a summary of\nchanges .\nWorkflow :\n1. Entity Identification\n- Named entities ( people , places , organizations )\n- Common nouns\n- Actions / verbs\n- Descriptors / adjectives\n2. Replacement Rules :\n- Maintain the original part of speech .\n- Preserve sentence structure .\n- Ensure semantic coherence .\n- Keep consistent tense and number .\n- Replace only approximately 50% of the\nidentified entities to retain the original\ncontext and flow .\n3. Input Original Text :\n{ original_text }\n4. Output Format :\n- Modified Text :\n[ text with approximately 30% replaced entities ]\n- Changes Summary :\n- [ Original Entity 1] -> [ Replacement Entity 1]\n- [ Original Entity 2] -> [ Replacement Entity 2]\n- ...\nAdditional Instructions :\n- Entity Selection :\n- After identifying all replaceable entities ,\ncalculate 30% of the total number .\n- Randomly select the calculated number of\nentities to replace .\n- Ensure that the selection is random to maintain\nvariability across different texts .- Replacement Constraints :\n- Do not replace entities that are crucial for the\nunderstanding of the text .\n- Avoid replacing more than 30% to prevent\naltering the original meaning significantly .\n- If the total number of replaceable entities is\nsmall , adjust the replacement percentage\nproportionally to avoid replacing too many .\n’’’\nD Example Appendix\nThis section presents additional examples compar-\ning EMPO and LLaV A-1.5, along with the corre-\nsponding heatmaps.\nQuestion: Who in the \npicture are wearing trousers?\nLLaVA -1.5: The man in the \npicture is wearing pants.\nEMPO (ours): A woman  in the \npicture is wearing pants.\n(a) LLaV A -1.5\n(b) EMPO (Ours)\nFigure 7: Who in the picture are wearing trousers?\nQuestion: Is the man in the picture indoors \nor outdoors?\nLLaV A -1.5: The man in the picture is \nindoors , specifically in a boat.\nEMPO (ours): It's not possible to \ndetermine with certainty from the image \nwhether the man is indoors or outdoors. \nHowever, what we can see is a smiling \nman in sunglasses and a swimsuit, which \nsuggests that he might be at a location \nwhere swimwear and outdoor activities \nare common .\nFigure 8: Is the man in the picture indoors or outdoors?\n15\n--- Page 16 ---\nQuestion: Describe the image in detail.\nLLaVA -1.5: The image features a small storefront \nwith a green sign that reads \"Hawkins Bazaar.\" The \nstore is located on a street corner, and there are several \npeople walking by, including a group of three people \nstanding close to the store. Inside the store, there are \na few more people browsing the items on display. The \nstore has a variety of items, including a clock on the \nwall and a couple of books placed on a surface. The \noverall atmosphere of the scene is lively and bustling, \nwith people engaging in various activities.EMPO (ours): The image features a \nstreet -front shop called Hawkins Bazaar, \nwhich has a prominent storefront to \nattract customers. The shop‘s name and \nsignage are prominently displayed, \nmaking it easily identifiable. Several \npeople can be seen around the shop, \npossibly browsing or passing by. The \noverall atmosphere suggests a lively and \nactive commercial area. The presence of \nthe shop and the people contributes to the \nurban setting depicted in the image.(a) LLaV A -1.5\n(b) EMPO (Ours)\nFigure 9: Describe the image in detail.\nQuestion: Describe the image in detail.\nLLaVA -1.5: The image features a lively city street \nwith a storefront for Hawkins Bazaar prominently \ndisplayed on the side of a building. The shopfront \nis decorated with Christmas ornaments, giving it a \nfestive appearance. There are a few people visible \non the sidewalk near the storefront, possibly \nwindow shopping or going about their daily \nactivities . The overall scene conveys a bustling \nurban atmosphere with businesses and pedestrians \ncontributing to the vibrancy of the area.EMPO (ours): In the image, a yellow school \nbus is parked on the side of a street, with a man \nin a blue outfit riding a bicycle right next to it. \nThe bus has a stop sign on its front, indicating \nits presence in the area. The bicycle rider is \nlikely commuting or traveling alongside the \nbus. The scene also features a tattoo shop in the \nvicinity, adding to the overall atmosphere of \nthe location. The combination of the school bus, \nbicycle rider, and tattoo shop creates an \ninteresting and diverse setting.\n(a) LLaV A -1.5\n(b) EMPO (Ours)\nFigure 10: Describe the image in detail.\n16",
  "text_length": 65206
}