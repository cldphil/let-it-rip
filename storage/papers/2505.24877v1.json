{
  "id": "http://arxiv.org/abs/2505.24877v1",
  "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional\n  Multiview Diffusion",
  "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.",
  "authors": [
    "Yangyi Huang",
    "Ye Yuan",
    "Xueting Li",
    "Jan Kautz",
    "Umar Iqbal"
  ],
  "published": "2025-05-30T17:59:54Z",
  "updated": "2025-05-30T17:59:54Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24877v1"
}