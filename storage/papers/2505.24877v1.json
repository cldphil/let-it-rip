{
  "id": "http://arxiv.org/abs/2505.24877v1",
  "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional\n  Multiview Diffusion",
  "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.",
  "authors": [
    "Yangyi Huang",
    "Ye Yuan",
    "Xueting Li",
    "Jan Kautz",
    "Umar Iqbal"
  ],
  "published": "2025-05-30T17:59:54Z",
  "updated": "2025-05-30T17:59:54Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24877v1",
  "full_text": "arXiv:2505.24877v1 [cs.CV] 30 May 2025AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion Yangyi Huang Ye Yuan Xueting Li Jan Kautz Umar Iqbal NVIDIA https://nvlabs.github.io/AdaHuman Animatable Avatars & Novel Pose AnimationPixel -Aligned Detailed 3DGS Avatar InputInput Figure 1. Given a single input image, AdaHuman reconstructs pixel-aligned a 3DGS avatar with detailed appearance. It can also generate the same avatar in novel poses, or in a standard animation-friendly A-pose to build an animatable avatar. Abstract Existing methods for image-to-3D avatar generation strug- gle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHu- man, a novel framework that generates high-fidelity an- imatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose- conditioned 3D joint diffusion model that synthesizes con- sistent multi-view images in arbitrary poses alongside cor- responding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refine- ment module that enhances the details of local body parts through image-to-image refinement and seamlessly inte- grates them using a novel crop-aware camera ray map, pro- ducing a cohesive detailed 3D avatar. These components al- low AdaHuman to generate highly realistic standardized A- pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Codeand models will be publicly available for research purposes. 1. Introduction Generating high-quality animatable 3D human avatars is crucial for numerous applications in gaming, animation, and virtual reality. Recent advances in diffusion-based im- age generation models have significantly accelerated re- search in this domain. Early approaches tackled this chal- lenge using score distillation sampling (SDS), where a 3D model is distilled from a diffusion-based image genera- tion model [16, 18]. While SDS-based methods offer flex- ibility and compatibility with various 3D representations, they suffer from oversaturation artifacts and slow genera- tion speed, making them impractical for large-scale avatar creation. More recent methods have shifted towards multi- view generation and reconstruction pipelines, where diffusion models first synthesize multi-view images from text or image inputs, followed by a reconstruction phase that converts these images into a 3D avatar. This feed- forward approach improves both realism and generation speed. However, significant challenges remain. First, the avatars are typically generated in the same pose as the in- 1 put image, leading to self-occlusion issues that complicate rigging and animation. Second, the resulting avatars often lack fine details and appear blurry, limiting their utility in real-world applications. Motivated by these challenges, we introduce AdaHu- man, a new framework for generating animatable high- fidelity 3D human avatars from a single input image. At its core, AdaHuman employs a pose-conditioned joint 3D diffusion model that seamlessly integrates multi-view im- age synthesis with 3D Gaussian Splats (3DGS)-based re- construction during the diffusion process. By performing 3D reconstruction at each diffusion step, our approach en- sures strong multi-view consistency across generated im- ages, resulting in high-quality 3DGS avatars. A key ad- vantage of our multi-view diffusion model is its ability to generate images in any arbitrary pose by simply condition- ing on the desired pose. To enable animation, we leverage this capability to generate the 3DGS avatar in a standard A-pose, which minimizes self-occlusion, inpaints missing details, and naturally facilitates rigging and animation. No- tably, our method achieves this without requiring training images in such standard poses. To enhance the fidelity and detail of the generated avatars, AdaHuman further introduces a compositional 3DGS refinement module. This module first renders zoomed-in views of local body parts (e.g., head, upper body, lower body) from the initial 3DGS avatar. These local views then undergo an image-to-image refinement process using our multi-view diffusion model to improve detail and res- olution. Using these refined local views, we propose a novel approach that seamlessly integrates the local views and global full-body views to produce a highly detailed holistic 3D avatar. This is enabled through two innovations: (1) a crop-aware camera ray map that establishes precise correspondences between 3D locations in local and global views, and (2) a visibility-aware composition scheme that intelligently merges partial 3DGS reconstructions based on view coverage and visibility salience. Our approach effec- tively prevents floating artifacts while preserving fine de- tails and coherency, resulting in high-quality 3D avatars with enhanced local and global consistency. In summary, our main contributions are as follows: (1) We introduce a new image-to-avatar framework lever- aging pose-conditioned 3D joint diffusion, enabling both avatar reconstruction and reposing for seamless rigging and animation. (2) We develop an innovative compositional 3DGS refinement approach that produces highly detailed and globally consistent avatars using a crop-aware camera ray map and a visibility-aware composition scheme. (3) Through comprehensive evaluation on public benchmarks and challenging in-the-wild images, we demonstrate that our method substantially outperforms state-of-the-art ap- proaches in both avatar reconstruction and reposing tasks.2. Related Work 3D Avatar Reconstruction. Early methods for monocular RGB-based 3D avatar reconstruction typically rely on the SMPL [7, 30] model, predicting per-vertex offsets to cap- ture clothing and hair details, but are limited by SMPL’s fixed topology. Consequently, recent approaches adopt im- plicit representations allowing arbitrary topologies [3, 13, 18, 19, 38, 39, 51, 52, 56, 63, 64], however, they depend heavily on extensive 3D training data. Moreover, they struggle with occlusion handling in complex poses mak- ing it difficult to animate the reconstructed avatars. Meth- ods enabling animation through pose canonicalization usu- ally require ground-truth standard-pose meshes or rigged avatars [11, 19, 32]. In contrast, our method generalizes reposing from diverse multiview video data, directly gener- ating avatars in arbitrary poses without relying on standard- pose training data. 3D Avatars Generation via 2D Foundation Model. Ad- vances in 2D diffusion models [35, 37] have driven signif- icant progress in 3D avatar generation [4, 6, 16, 17, 20, 22, 24, 25, 33, 36, 42, 44, 49, 60–62]. These methods adopt the Score Distillation Sampling (SDS) technique to extract 3D knowledge from these models. SDS-based meth- ods, however, suffer from unrealistic outputs and slow it- erative optimization, resulting in lower quality avatars and prohibitively long run time for wider adoption. Joint Diffusion and Reconstruction. Recent methods combine diffusion models with reconstruction networks to improve efficiency and quality for 3D avatar genera- tion [26, 27, 29, 41, 53, 54, 66]. Zero123  and its vari- ants [26, 27, 29, 41] generate consistent multi-view images that facilitate accurate 3D avatar reconstruction. More re- cent works [14, 45, 47, 57] predict implicit 3D representa- tions directly from multi-view images, enabled by advance- ments in implicit representations such as triplanes  and Gaussian Splats. Similar strategies have been applied to human avatars [2, 18, 23, 40], though they remain lim- ited by the quality of generated views. Recently, Xue et al. proposed a method that jointly trains diffusion and reconstruction models in an end-to-end manner, allowing for mutual enhancement. Our method follows this direction but introduces key innovations: (1) a pose-conditioned multi-view joint dif- fusion model that synthesizes avatars in arbitrary poses to handle occlusions and facilitate animation; (2) a composi- tional 3DGS refinement strategy integrating global and lo- cal views via a crop-aware camera ray embedding, signifi- cantly enhancing avatar detail and coherence. Concurrent works. Some of the latest research, IDOL  and LHM  also trying reconstruct high-resolution 3DGS avatars with large scale training data. While they develops feed-forward models for efficiency, we build our 2 model based on diffusion models to utilize the strong gen- erative priors. 3. Approach Problem Specification. As illustrated in Fig 2, given a full- body input image xIdepicting a person, AdaHuman aims to build a 3D avatar that supports two key functionalities: (1) Avatar Reconstruction: Without requiring any additional inputs, AdaHuman reconstructs an avatar GRrepresented by 3D Gaussian Splats (3DGS)  that precisely matches the pose of the input image, enabling high-fidelity novel view synthesis; (2) Avatar Synthesis: Using an estimated input posePsand an arbitrary target 3D pose Pt, AdaHuman gen- erates a reposed 3DGS avatar GPtin the target pose while faithfully preserving the person’s appearance and identity. This capability enables pose canonicalization, where we generate a standardized A-posed avatar GAthat minimizes self-occlusion. The canonicalized avatar can then be rigged automatically for animation and used to render temporally coherent 4D videos with high visual quality. Our method consists of two key modules that enable the generation of detailed and animatable avatars: (1) Pose- Conditioned 3D Joint Diffusion (Sec. 3.1), which gener- ates multiview images and the corresponding 3DGS avatar of the person in arbitrary poses by interleaving image syn- thesis and 3D reconstruction inside the diffusion process; (2) Compositional 3DGS Refinement (Sec. 3.2), which en- hances the visual quality by first refining local body part renderings at high resolution and then seamlessly compos- ing them into a holistic detailed avatar. 3.1. Pose-Conditioned 3D Joint Diffusion As shown in Fig 2, given a full-body input image xI, we first generate local view images of different body parts ( e.g., head, upper body, and lower body). These local views, along with the input, form our input views IV i=1, which are fed to the 3D joint diffusion module as in Fig 2 (right). The module then synthesizes images of the target views TK j=1 which look at the full-body and local body parts of the per- son from different viewpoints than the input. Combining both full-body and local perspectives enables our method to achieve detailed and globally consistent generation of multi-view images and their corresponding 3DGS avatar. Each input view is represented by a tuple Ii= {xi,pi,ci}, consisting of an RGB image xi, an optional pose condition pi, and camera parameters ci. The pose con- dition pitakes the form of a 2D semantic pose map derived from the 3D input pose θ, created by rendering the seman- tic segmentation of the SMPL model  from the camera’s perspective. The camera parameters ciare encoded into a camera ray map using sinusoidal embeddings of the camera rays’ origins and directions. Similarly, each target view is defined by Tj={xt j,pj,cj}, where xt jrepresents the noisytarget RGB image at diffusion step t,pjis the optional pose condition, and cjencodes the target view’s camera param- eters. The primary objective of our pose-conditioned 3D joint diffusion is to model the conditional denoising distri- bution of the target RGB images {xt−1 j}K j=1: p({xt−1 j}K j=1|{pj,cj}K j=1,{xi,pi,ci}V i=1, t),(1) where we assume Vinput views and Ktarget views. In- spired by recent work, we employ a multi-view image latent diffusion model (LDM) to model the denoising distri- bution. Specifically, we modify the U-Net architecture of a single-image LDM by replacing the 2D self-attention layers with 3D attention layers. The 2D pose semantic map piand camera ray map ciare concatenated with the RGB images as additional conditions before being fed to the U-Net. To enhance the 3D consistency of the generated multi- view images and produce the underlying 3DGS avatar, we incorporate a 3DGS generator G into the denoising diffusion process. Following, at each denoising step t, we generate a 3DGS avatar Gtfrom the image predictions: Gt=G({xt\u00010 j,xt j,pj,cj}K j=1,{xi,pi,ci}V i=1, t),(2) where xt\u00010 jrepresents the “clean” target image obtained through one-step denoising by the LDM at diffusion step t. Once Gtis obtained, we render it under the target views to generate new 3D-consistent clean target images ˆxt\u00010 j. Using these 3D-consistent images, we then sample the noisy images xt−1 jfor the next diffusion step accord- ing to: xt−1 j∼q(xt−1 j|xt j,ˆxt\u00010 j), where qdenotes the re- verse diffusion process. The final output of our pose- conditioned 3D joint diffusion model is the 3DGS avatar G0 produced at the end of the diffusion process. Unlike previous works [10, 55], our approach incorpo- rates pose conditioning to enable pose-conditioned multi- view image synthesis. This key enhancement empowers our model to not only reconstruct pixel-aligned 3DGS avatars but also generate reposed avatars that are well-suited for animation and other applications. This capability is par- ticularly valuable since subjects in input images often ex- hibit severe self-occlusion, which makes rigging in the orig- inal body pose challenging and suboptimal. Through pose- conditioned multi-view image synthesis, our method can transition the avatar into a rigging-friendly pose while si- multaneously recovering geometry and appearance details that were previously occluded. View Selection and Model Training. During training, we first randomly select either the full body or a local body parts from upper body, lower body, or head. For the se- lected body part, we choose an input view from a training video frame. The key distinction between reconstruction and reposing lies in the selection of target views: for re- 3 Pose-Conditioned Multi-View Image LDMInput Views CameraRay MapSemanticPose MapRGBImage 3DGS Generator Target Views NoisyImage Pose-Conditioned 3D Joint DiffusionOverview of AdaHuman Single ImagePose Condition (1) Avatar Reconstruction(2) Avatar Reposing Render Global / Local Condition ViewsBody-part CroppingInputOutput Input poseTarget pose Reverse Diffusion Pose-Conditioned 3D Joint Diffusion (Sec. 3.1)Compositional 3DGS Refinement (Sec. 3.2)(Depends on pose condition) Global ViewsLocal Views Output…… CameraRay MapSemanticPose Map <latexit sha1_base64=\"rhuoWg0DMHO/z+7zAsE+j67d708=\">AAACcXicbZDfShtBFMYn21ptbGusV6U3g0EoFtLdUqyXQm96aaFRIbuGM7OzydT5s8yc1YZl38Gn8bZ9DZ/DF3A2CaVGDwx8fOc7nDk/VirpMY5vO9Gz52sv1jdedjdfvX6z1dt+e+Jt5bgYcqusO2PghZJGDFGiEmelE6CZEqfs4lvbP70UzktrfuKsFJmGiZGF5IDBGvf2U5QqF3WqAaesqH83zfjXeY2pn1qHTk6mCM7ZKxo3414/HsTzoo9FshR9sqzj8XbnU5pbXmlhkCvwfpTEJWY1OJRciaabVl6UwC9gIkZBGtDCZ/X8qIbuBSenhXXhGaRz9/+JGrT3M81Csv27X+215pM9jxrczOUr+7E4zGppygqF4Yv1RaUoWtpyo7l0gqOaBQHcyXAB5VNwwDHQ7XZTI6641RpMHliyZpRk/5j2k6Z5GGB+EWBW5e0NVi1CAXGyCvSxOPk8SA4GBz++9I8Ol7A3yHuySz6QhHwlR+Q7OSZDwsk1uSF/yN/OXfQuotHuIhp1ljM75EFFH+8BQLvBAQ==</latexit>˜xt\u00000jLDM DenoisedImages 3DGS<latexit sha1_base64=\"4MTxVjubB23cDdsVBnx6m63BIuo=\">AAACb3icbZDbattAEIbX6iGpe4jTXuQiEJaaQnrjSqWkuQzkppcJ1EnAUs3samVtswexO0pqhF6hT9Pb9D36GH2DrmwTEqcDCz///MPsfKxS0mMc/+lFjx4/ebqx+az//MXLV1uD7ddn3taOizG3yroLBl4oacQYJSpxUTkBmilxzi6Pu/75lXBeWvMV55XINMyMLCQHDNZ0sJ+WgE2qAUtWND/advr9W4OpL61DJ2clgnP2msbtdDCMR/Gi6EORrMSQrOpkut37kOaW11oY5Aq8nyRxhVkDDiVXou2ntRcV8EuYiUmQBrTwWbM4qaXvgpPTwrrwDNKFe3eiAe39XLOQ7P7u13ud+d+eRw1u7vK1/VgcZo00VY3C8OX6olYULe2o0Vw6wVHNgwDuZLiA8hIccAxs+/3UiGtutQaTB5asnSTZLdNh0rb3A8wvA8yqvLvBqmUoIE7WgT4UZx9HycHo4PTT8OhwBXuT7JK3ZJ8k5DM5Il/ICRkTTn6SX+SG/O79jXaivYguo1FvNfOG3Kvo/T9CccAY</latexit>ˆxt\u00000j3D-ConsistentPrediction <latexit sha1_base64=\"1DpArmAN2YYA6sXLP6KQOtJrVVs=\">AAACNHicbVDLSgMxFM3UV62vVlfiJlgEV2VGpHYjFNy4rGAf0A4lk8m0oXkMSUYpQ7/Brf6I/yK4E7d+g2lnFrb1Qsjh3HM5954gZlQb1/1wChubW9s7xd3S3v7B4VG5ctzRMlGYtLFkUvUCpAmjgrQNNYz0YkUQDxjpBpO7eb/7RJSmUjyaaUx8jkaCRhQjY6m2gbfQHZarbs1dFFwHXg6qIK/WsOKcDkKJE06EwQxp3ffc2PgpUoZiRmalQaJJjPAEjUjfQoE40X662HYGLywTwkgq+4SBC/bvRIq41lMeWCVHZqxXe3Pyv14/MVHDT6mIE0MEzoyihEEj4fx0GFJFsGFTCxBW1O4K8RgphI0NqFQaCPKMJedIhOnA2vc93/7WJYjSqjebLQsCnQkCycL5tpJlIhumtxrdOuhc1bx6rf5wXW028liL4Aycg0vggRvQBPegBdoAAwpewCt4c96dT+fL+c6kBSefOQFL5fz8An2nqa4=</latexit>t=0 Diffuse steps <latexit sha1_base64=\"QfNeWCsn8jLDisqJoQi//1qj8Y4=\">AAACUnicbVLLSsNAFJ3Ud321unQTLIKrmojULgtudKdgVWhCuZlM7NB5xJmJUkK+w63+kxt/xZWTpAutXhg4nHsu99zDRCmj2njep9NYWl5ZXVvfaG5ube/sttp7d1pmCpMhlkyqhwg0YVSQoaGGkYdUEeARI/fR9KLs3z8TpakUt2aWkpDDo6AJxWAsFQYczAQDy6+KsT9udbyuV5X7F/hz0EHzuh63nZMgljjjRBjMQOuR76UmzEEZihkpmkGmSQp4Co9kZKEATnSYV64L98gysZtIZZ8wbsX+nMiBaz3jkVWWLvViryT/7WnDQc1UvLDfJP0wpyLNDBG4Xp9kzDXSLYNxY6oINmxmAWBF7QUunoACbGx8zWYgyAuWnIOI88CaGvlhXqUXJXnHL4rfgkjXgkiyuLxBslpkI/YXA/0L7k67fq/buznrDPrzsNfRATpEx8hH52iALtE1GiKMntArekPvzofz1bC/pJY2nPnMPvpVja1vHQe0WA==</latexit>I1<latexit sha1_base64=\"txJhpbeIDFsMgaofPCh/EcDEeVw=\">AAACUnicbVLLSsNAFJ3UV62vqks3wSK4qkmR2qXgRncVrApNKDeTSTt0HnFmopSQ73Cr/+TGX3HlJO1CqxcGDueeyz33MFHKqDae9+nUVlbX1jfqm42t7Z3dveb+wb2WmcJkgCWT6jECTRgVZGCoYeQxVQR4xMhDNL0q+w/PRGkqxZ2ZpSTkMBY0oRiMpcKAg5lgYPlNMeqMmi2v7VXl/gX+ArTQovqjfecsiCXOOBEGM9B66HupCXNQhmJGikaQaZICnsKYDC0UwIkO88p14Z5YJnYTqewTxq3YnxM5cK1nPLLK0qVe7pXkvz1tOKiZipf2m6QX5lSkmSECz9cnGXONdMtg3Jgqgg2bWQBYUXuBiyegABsbX6MRCPKCJecg4jywpoZ+mFfpRUne8ovityDSc0EkWVzeINlcZCP2lwP9C+47bb/b7t6ety57i7Dr6Agdo1Pkowt0ia5RHw0QRk/oFb2hd+fD+arZXzKX1pzFzCH6VbXtbx7ptFk=</latexit>I2<latexit sha1_base64=\"vVmqWdHKMVr/DGENp/S6RohgiMM=\">AAACUnicbVLLSsNAFJ3UV43P6tJNsAiuaiJSuxTc6E7BVqEJ5WYysUPnEWcmSgn5Drf6T278FVdO2iy0emHgcO653HMPE2eMauP7n05jaXllda257m5sbm3v7Lb2BlrmCpM+lkyqhxg0YVSQvqGGkYdMEeAxI/fx5LLq3z8TpakUd2aakYjDo6ApxWAsFYUczBgDK67L0WC02/Y7/qy8vyCoQRvVdTNqOSdhInHOiTCYgdbDwM9MVIAyFDNSumGuSQZ4Ao9kaKEATnRUzFyX3pFlEi+Vyj5hvBn7c6IArvWUx1ZZudSLvYr8t6cNBzVVycJ+k/aigoosN0Tg+fo0Z56RXhWMl1BFsGFTCwArai/w8BgUYGPjc91QkBcsOQeRFKE1NQyiYpZenBbtoCx/C2I9F8SSJdUNks1FNuJgMdC/YHDaCbqd7u1Z+6JXh91EB+gQHaMAnaMLdIVuUB9h9IRe0Rt6dz6cr4b9JXNpw6ln9tGvamx+A2KxtH0=</latexit>IV<latexit sha1_base64=\"Omrs3z4S8lFYbQxZ3TnWb62jFrM=\">AAACUnicbVLLSsNAFJ3UV43P6tJNsAiuaiKiXRbcuKzQWqEJ5WYy0cF5xJmJUkK+w63+kxt/xZWTNgttvTBwOPdc7rmHiTNGtfH9L6exsrq2vtHcdLe2d3b39lsHd1rmCpMhlkyq+xg0YVSQoaGGkftMEeAxI6P46brqj16I0lSKgZlmJOLwIGhKMRhLRSEH84iBFYNyEkz2237Hn5W3DIIatFFd/UnLOQsTiXNOhMEMtB4HfmaiApShmJHSDXNNMsBP8EDGFgrgREfFzHXpnVgm8VKp7BPGm7G/JwrgWk95bJWVS73Yq8h/e9pwUFOVLOw3aTcqqMhyQwSer09z5hnpVcF4CVUEGza1ALCi9gIPP4ICbGx8rhsK8ool5yCSIrSmxkFUzNKL06IdlOVfQazngliypLpBsrnIRhwsBroM7s47wWXn8vai3evWYTfRETpGpyhAV6iHblAfDRFGz+gNvaMP59P5bthfMpc2nHrmEP2pxvYPMd60Yw==</latexit>T1<latexit sha1_base64=\"5NAomm6DcB4qH9Ts6Do9pbVbMh0=\">AAACUnicbVLLSsNAFJ3UV62vVpdugkVwVZMi1aXgxqWCbYUmlJvJxA6dR5yZKCXkO9zqP7nxV1w5abOw1QsDh3PP5Z57mChlVBvP+3Jqa+sbm1v17cbO7t7+QbN1ONAyU5j0sWRSPUagCaOC9A01jDymigCPGBlG05uyP3whSlMpHswsJSGHJ0ETisFYKgw4mAkGlj8U4+642fY63rzcv8CvQBtVdTduOedBLHHGiTCYgdYj30tNmIMyFDNSNIJMkxTwFJ7IyEIBnOgwn7su3FPLxG4ilX3CuHP290QOXOsZj6yydKlXeyX5b08bDmqm4pX9JrkKcyrSzBCBF+uTjLlGumUwbkwVwYbNLACsqL3AxRNQgI2Nr9EIBHnFknMQcR5YUyM/zOfpRUne9otiWRDphSCSLC5vkGwhshH7q4H+BYNux+91evcX7eurKuw6OkYn6Az56BJdo1t0h/oIo2f0ht7Rh/PpfNfsL1lIa041c4SWqrb7AzPAtGQ=</latexit>T2<latexit sha1_base64=\"KJDIJgzNIDErBCYWidjHdsJolqk=\">AAACUnicbVLLSsNAFJ3UV63P6tJNsAiuaiJSXRbcCG4qtCo0odxMJjp0HnFmopSQ73Cr/+TGX3HlpOnCtl4YOJx7Lvfcw0Qpo9p43rdTW1ldW9+obza2tnd29/abB/daZgqTAZZMqscINGFUkIGhhpHHVBHgESMP0fi67D+8EqWpFH0zSUnI4UnQhGIwlgoDDuYZA8v7xeh2tN/y2t603GXgz0ALzao3ajpnQSxxxokwmIHWQ99LTZiDMhQzUjSCTJMU8BieyNBCAZzoMJ+6LtwTy8RuIpV9wrhT9u9EDlzrCY+ssnSpF3sl+W9PGw5qouKF/Sa5CnMq0swQgav1ScZcI90yGDemimDDJhYAVtRe4OJnUICNja/RCAR5w5JzEHEeWFNDP8yn6UVJ3vKLYl4Q6UoQSRaXN0hWiWzE/mKgy+D+vO132p27i1b3ahZ2HR2hY3SKfHSJuugG9dAAYfSC3tEH+nS+nJ+a/SWVtObMZg7RXNW2fwFi0rR9</latexit>TK& <latexit sha1_base64=\"zDdkjXpQEGEP3fLHSv9mOybbb2o=\">AAACSHicbVDLSsNAFJ3UV42vVpdugkVwFROR6rLgxmVF+4A2lMlk0g6dR5iZKCH0E9zqP/kH/oU7ceekzcK2XhjmcO65nHtPmFCitOd9WpWNza3tnequvbd/cHhUqx93lUglwh0kqJD9ECpMCccdTTTF/URiyEKKe+H0ruj3nrFURPAnnSU4YHDMSUwQ1IZ6bI/UqNbwXG9ezjrwS9AAZbVHdetyGAmUMsw1olCpge8lOsih1ARRPLOHqcIJRFM4xgMDOWRYBfl815lzbpjIiYU0j2tnzv6dyCFTKmOhUTKoJ2q1V5D/9pRmUGYyWvHX8W2QE56kGnO0sI9T6mjhFHE4EZEYaZoZAJEk5gIHTaCESJvQbHvI8QsSjEEe5UOz1MAPzG+8wzhv+LPZsiBUC0EoaFTcIOhCZCL2VwNdB90r12+6zYfrRsstw66CU3AGLoAPbkAL3IM26AAExuAVvIF368P6sr6tn4W0YpUzJ2CpKpVfdKOyAw==</latexit>Ps<latexit sha1_base64=\"qFe6LNuOR1NCOo7UjTwAy17ytyQ=\">AAACSHicbVDLTsJAFJ3iC/EFunTTSExcYWsMuiRx4xKjPBJoyHQ6hQnzaGZuNaThE9zqP/kH/oU7484psBDwJpM5OffcnHtPmHBmwPM+ncLG5tb2TnG3tLd/cHhUrhy3jUo1oS2iuNLdEBvKmaQtYMBpN9EUi5DTTji+y/udZ6oNU/IJJgkNBB5KFjOCwVKPzQEMylWv5s3KXQf+AlTRopqDinPZjxRJBZVAODam53sJBBnWwAin01I/NTTBZIyHtGehxIKaIJvtOnXPLRO5sdL2SXBn7N+JDAtjJiK0SoFhZFZ7Oflvz4DAeqKjFX+Ib4OMySQFKsncPk65C8rN43AjpikBPrEAE83sBS4ZYY0J2NBKpb6kL0QJgWWU9e1SPT+wv/UO46zqT6fLgtDMBaHiUX6D4nORjdhfDXQdtK9qfr1Wf7iuNmqLsIvoFJ2hC+SjG9RA96iJWoigIXpFb+jd+XC+nG/nZy4tOIuZE7RUhcIvdoWyBA==</latexit>Pt <latexit sha1_base64=\"5BeVSER8m1UcnVK7DibuUK6LuLo=\">AAACUnicbVLLSsNAFJ3Ud31VXboJFsFVTURqlwUXulSxKjSh3Ewm7dB5xJmJUkK+w63+kxt/xZWTtAttvTBwOPdc7rmHiVJGtfG8L6e2tLyyura+Ud/c2t7ZbeztP2iZKUx6WDKpniLQhFFBeoYaRp5SRYBHjDxG48uy//hClKZS3JtJSkIOQ0ETisFYKgw4mBEGll8Vg7tBo+m1vKrcReDPQBPN6maw55wGscQZJ8JgBlr3fS81YQ7KUMxIUQ8yTVLAYxiSvoUCONFhXrku3GPLxG4ilX3CuBX7eyIHrvWER1ZZutTzvZL8t6cNBzVR8dx+k3TCnIo0M0Tg6fokY66RbhmMG1NFsGETCwArai9w8QgUYGPjq9cDQV6x5BxEnAfWVN8P8yq9KMmbflH8FUR6Kogki8sbJJuKbMT+fKCL4OGs5bdb7dvzZrczC3sdHaIjdIJ8dIG66BrdoB7C6Bm9oXf04Xw63zX7S6bSmjObOUB/qrb1A1dftHc=</latexit>GR<latexit sha1_base64=\"ToKF7pEuw+CU9Qx7CttmxH9HPc0=\">AAACUnicbVLLSsNAFJ3Ud31VXboJFsFVTURqlxUXulSwKjSh3Ewm7dB5xJmJEkK+w63+kxt/xZWTtgttvTBwOPdc7rmHiVJGtfG8L6e2tLyyura+Ud/c2t7ZbeztP2iZKUx6WDKpniLQhFFBeoYaRp5SRYBHjDxG46uq//hClKZS3Js8JSGHoaAJxWAsFQYczAgDK67LweWg0fRa3qTcReDPQBPN6naw55wGscQZJ8JgBlr3fS81YQHKUMxIWQ8yTVLAYxiSvoUCONFhMXFduseWid1EKvuEcSfs74kCuNY5j6yycqnnexX5b08bDipX8dx+k3TCgoo0M0Tg6fokY66RbhWMG1NFsGG5BYAVtRe4eAQKsLHx1euBIK9Ycg4iLgJrqu+HxSS9KCmafln+FUR6Kogki6sbJJuKbMT+fKCL4OGs5bdb7bvzZrczC3sdHaIjdIJ8dIG66Abdoh7C6Bm9oXf04Xw63zX7S6bSmjObOUB/qrb1AzddtGY=</latexit>GA<latexit sha1_base64=\"97LBMuofuOMXyoETLjvuEyiMjrM=\">AAACUHicbVDLSsRAEOys7/jWo5fgInhaE5HVo+BFbwquCpsgnclEB+cRZibKEvIZXvWfvPkn3nSyu4KuNgxTVFfT1ZUWnBkbhu9ea2p6ZnZufsFfXFpeWV1b37gyqtSE9ojiSt+kaChnkvYss5zeFJqiSDm9Th9Omv71I9WGKXlpBwVNBN5JljOC1lH9WKC9J8irs/p2rR12wmEFf0E0Bm0Y1/nturcXZ4qUgkpLOBrTj8LCJhVqywintR+XhhZIHvCO9h2UKKhJqqHnOthxTBbkSrsnbTBkf05UKIwZiNQpG49msteQ//aMFagHOpvYb/OjpGKyKC2VZLQ+L3lgVdDEEmRMU2L5wAEkmrkLAnKPGol14fl+LOkTUUKgzKrYmepHSTXMLs2rdlTXvwWpGQlSxbPmBsVHIhdxNBnoX3C134m6ne7FQfv4aBz2PGzBNuxCBIdwDKdwDj0goOAZXuDVe/M+vM+WN5J+/7AJv6rlfwHcqrSz</latexit>I<latexit sha1_base64=\"mJyVCsI1rUAlIuTaVEMwAFUy1bA=\">AAACUHicbVBNS8NAEJ3Urxo/q0cvwSJ4qomIehS8eFSwKjRBJpuNLu5H2N0oJeRneNX/5M1/4k03bQWtDiz7ePOGefPSgjNjw/Dda83Mzs0vtBf9peWV1bX1zsaVUaUmtE8UV/omRUM5k7RvmeX0ptAURcrpdfpw2vSvH6k2TMlLOyxoIvBOspwRtI4axALtPUFeXda3692wF44q+AuiCejCpM5vO95enClSCiot4WjMIAoLm1SoLSOc1n5cGlogecA7OnBQoqAmqUae62DHMVmQK+2etMGI/TlRoTBmKFKnbDya6V5D/tszVqAe6mxqv82Pk4rJorRUkvH6vOSBVUETS5AxTYnlQweQaOYuCMg9aiTWhef7saRPRAmBMqtiZ2oQJdUouzSvulFd/xakZixIFc+aGxQfi1zE0XSgf8HVfi867B1eHHRPjidht2ELtmEXIjiCEziDc+gDAQXP8AKv3pv34X22vLH0+4dN+FUt/wvxa7S+</latexit>T<latexit sha1_base64=\"Xpc+DNSuHw10M/C8SyRbLaEuGrI=\">AAACUXicbVDLSsRAEOzE9/rWo5fgInhaE5HVo+DFo4KrwibKZDLRwXmEmY66hPyGV/0nT36KNye7K+hqQ0NRXU1XV1oIbjEMPzx/anpmdm5+obW4tLyyura+cWl1aSjrUS20uU6JZYIr1kOOgl0XhhGZCnaVPpw086tHZizX6gIHBUskuVM855Sgo+JYErxP8+q5vsHbtXbYCYcV/AXRGLRhXGe3695enGlaSqaQCmJtPwoLTCpikFPB6lZcWlYQ+kDuWN9BRSSzSTU0XQc7jsmCXBvXCoMh+3OjItLagUydsjFpJ2cN+e/MoiRmYLKJ+5gfJRVXRYlM0dH5vBQB6qDJJci4YRTFwAFCDXcfBPSeGELRpddqxYo9US0lUVkVO1P9KKm+w2tHdf1bkNqRINUia37QYiRyEUeTgf4Fl/udqNvpnh+0j4/GYc/DFmzDLkRwCMdwCmfQAwoFvMArvHnv3qcPvj+S+t54ZxN+lb/4BQRatFc=</latexit>xt <latexit sha1_base64=\"aym/BAajEzpI2iuVkHW1bq1cLu8=\">AAACl3icbZFdaxQxFIaz41cdv7Z6Jd4EF6GCrjMitXcWhNLLLbhtYWdckkxmJzYfY3LGuoz5W/4Xobf6O8zsLGq3Hgi8POcNJ+cNraVwkCQ/BtG16zdu3tq6Hd+5e+/+g+H2w2NnGsv4lBlp7Ckljkuh+RQESH5aW04UlfyEnr3v+idfuHXC6A+wrHmuyEKLUjACAc2Hk887mSJQ0bL96uefPrbwMvXfNpB/kVUE2r+0x5mrjAUrFhUQa805TvzzeD4cJeNkVfiqSNdihNY1mW8PXmWFYY3iGpgkzs3SpIa8JRYEk9zHWeN4TdgZWfBZkJoo7vJ2tbrHzwIpcGlsOBrwiv57oyXKuaWiwdm93m32OvjfngNF7NIWG/Oh3MtboesGuGb9+LKRGAzu0sWFsJyBXAZBmBVhA8wqYgmD8AdxnGl+zoxSRBchTepnaf4n1VHq/WUDdb2BGll0OxjZm0LE6WagV8Xx63G6O949ejPa31uHvYWeoKdoB6XoLdpHh2iCpoih7+gC/US/osfRu+ggOuyt0WB95xG6VNHRb9hr0B0=</latexit>q(xt→1j|xtj,ˆxt/shortrightarrow0j) <latexit sha1_base64=\"50K5EbIZL7+PBUlmju/Kodp6q3o=\">AAACUHicbVDLSsRAEOys7/jWo5fgInhaE5HVo+BBjwquCpsgnclEB+cRZibKEvIZXvWfvPkn3nSyu4KuNgxTVFfT1ZUWnBkbhu9ea2p6ZnZufsFfXFpeWV1b37gyqtSE9ojiSt+kaChnkvYss5zeFJqiSDm9Th9Omv71I9WGKXlpBwVNBN5JljOC1lH9WKC9J8ir0/p2rR12wmEFf0E0Bm0Y1/nturcXZ4qUgkpLOBrTj8LCJhVqywintR+XhhZIHvCO9h2UKKhJqqHnOthxTBbkSrsnbTBkf05UKIwZiNQpG49msteQ//aMFagHOpvYb/OjpGKyKC2VZLQ+L3lgVdDEEmRMU2L5wAEkmrkLAnKPGol14fl+LOkTUUKgzKrYmepHSTXMLs2rdlTXvwWpGQlSxbPmBsVHIhdxNBnoX3C134m6ne7FQfv4aBz2PGzBNuxCBIdwDGdwDj0goOAZXuDVe/M+vM+WN5J+/7AJv6rlfwHY5LSx</latexit>G <latexit sha1_base64=\"KpVyJ98BxtFt4wsDw7wKuSzzw8o=\">AAACRnicbVDJSgNBEK2JW4xr9OhlMAie4oxIzFHw4jGBLEIySE9PjTb2MnT3KGHIF3jVf/IX/Alv4tXOctDEgqYfr17xql6ccWZsEHx4pZXVtfWN8mZla3tnd2+/etAzKtcUu1RxpW9jYpAziV3LLMfbTCMRMcd+/Hg96fefUBumZMeOMowEuZcsZZRYR7U7d/u1oB5My18G4RzUYF6tu6p3NkwUzQVKSzkxZhAGmY0Koi2jHMeVYW4wI/SR3OPAQUkEmqiYbjr2TxyT+KnS7knrT9nfEwURxoxE7JSC2Aez2JuQ//aMFUSPdLLgb9NmVDCZ5RYlndmnOfet8idh+AnTSC0fOUCoZu4Cnz4QTah1kVUqQ4nPVAlBZFIM3VKDMHK/847TohaOx38FsZkJYsWTyQ2Kz0Qu4nAx0GXQO6+HjXqjfVG7as7DLsMRHMMphHAJV3ADLegCBYQXeIU379379L6875m05M1nDuFPleAHqCaxKw==</latexit>T<latexit sha1_base64=\"d6luHpIyxbe/auZisogmznROxdA=\">AAACUnicbVJNS8NAEN3Ur1o/q0cvwSJ4qolI9Sh40KOCVaGJMtlsdHE/4u5EKSG/w6v+Jy/+FU9u2h60OrDwePOGefPYJBfcYhB8eo2Z2bn5heZia2l5ZXVtvb1xZXVhKOtTLbS5ScAywRXrI0fBbnLDQCaCXSePJ3X/+pkZy7W6xGHOYgn3imecAjoqjiTgAwVRnla3eLfeCbrBqPy/IJyADpnU+V3b24tSTQvJFFIB1g7CIMe4BIOcCla1osKyHOgj3LOBgwoks3E5cl35O45J/Uwb9xT6I/bnRAnS2qFMnLJ2aad7Nflvz6IEMzTp1H7MjuKSq7xApuh4fVYIH7VfB+On3DCKYugAUMPdBT59AAMUXXytVqTYC9VSgkrLyJkahHE5Si/Jyk5YVb8FiR0LEi3S+gYtxiIXcTgd6F9wtd8Ne93exUHn+GwSdpNskW2yS0JySI7JGTknfULJE3klb+Td+/C+Gu6XjKUNbzKzSX5VY/kbmlC0qA==</latexit>GtFigure 2. Method Overview. Left: Given an RGB image of an unseen person as input, AdaHuman could (1) reconstruct a high-fidelity pixel-aligned 3D Gaussian Splat (3DGS) avatar, as well as (2) generate an reposed 3DGS avatar with a target pose condition, enable building animatable avatar in a standard A-pose. Right: A pose-conditioned joint 3D diffusion process is utilized to generate global or local 3DGS reconstruction or reposing results. This process ensures 3D consistency of the reconstruction by utilizing generated 3DGS results in each reverse diffusion process of multi-view avatar images. construction, we select three canonical target views (sepa- rated by 90° azimuth angles) of the body part from the same frame as the input view; for reposing, we select four canon- ical target views from a different frame showing the subject in a different pose, where the additional target view coin- cides with the input view to account for the pose difference. We jointly train the multi-view image LDM and the 3DGS generator Gusing multi-view video data from MVHumanNet  and image renderings from CustomHu- man. To leverage powerful generative priors learned from large-scale datasets, both models are initialized from official pretrained weights [37, 45]. We first train the model for avatar reconstruction for 30k steps and then fine-tune the model for reposing for 10k steps. Camera ray embeddings are computed relative to the input view. The LDM is super- vised using MSE loss between predicted and ground truth image latents, while the 3DGS generator Gis supervised following  using MSE, LPIPS rendering losses, and sur- face regularization loss. In addition to the target views, we sample 12 additional views to provide dense supervision to the 3DGS generator. Additional implementation details are provided in the appedix. 3.2. Compositional 3DGS Refinement Recent feed-forward 3D reconstruction models [14, 45] have demonstrated promising results in generating 3D mod- els of general objects from sparse-view images. How- ever, these approaches are constrained by their networks’ fixed output resolution (e.g., 256×256 3D Gaussians in LGM ), limiting their ability to capture the fine-graineddetails essential for realistic human avatar reconstructions. To address this limitation, we introduce a new composi- tional 3DGS refinement module, as illustrated in Fig 3. The module leverages an image-to-image local body refinement scheme as well as a novel crop-aware camera ray map to en- able detailed and coherent reconstructions of individual lo- cal body parts. During inference, it takes the coarse 3DGS avatar Gcoarse from the 3D joint diffusion module as input and refines it to produce a detailed 3DGS avatar Grefined. Local body part refinement. To achieve enhanced de- tails for local body parts, we begin by rendering Nv=4 90-degree separated canonical views (front, left, back, and right) for each of Nb=3local body parts (head, upper body, and lower body) of the coarse avatar Gcoarse. Each local view is produced using a crop-view camera that zooms into the local body region inside the original global view (by manip- ulating the camera intrinsics). This zoom-in region is com- puted using the 2D body joints and segmentation masks. We then employ our multi-view LDM introduced in Section 3.1 to refine the local renderings via an image-to-image editing process similar to SDEdit, significantly enhancing their detail. This approach enables the high-fidelity generation of local body parts. To properly handle the modified camera perspective for these local views, we provide the LDM with a specialized cropped version of the camera ray map, which we detail in the following section. Crop-aware local ray map. A key challenge in the refine- ment process is effectively combining the Nv×Nbrefined local view images and Nvglobal full-body view images 4 LDM Camera-AwareLocal Ray MapCoarse 3DGSAvatarImage-to-ImageRefine 📷RenderMulti-part Local 3DGS ………Coarse Local ViewsOutput:High-Fidelity 3DGS Avatar Appearance Detail <latexit sha1_base64=\"VMd1/WuzaWqnNtdRI6EM1sIWVgY=\">AAACYHicbVDLThsxFHWmD9LpgwRWpRurUaUuqnSmqijskLooS5AIIM2MItvjCRZ+jOw7lMiaz+jXsKUfwbZfgidJVR69kq2jc8+99+jQWgoHSXLTi548ffZ8rf8ifvnq9Zv1wXDj2JnGMj5hRhp7SonjUmg+AQGSn9aWE0UlP6Hn37v+yQW3Thh9BPOaF4rMtKgEIxCo6eBzDvwSFnsyO6OFT8a7u5/wv7/1ca4InNHK/2jjdjoYJeNkUfgxSFdghFZ1MB323ualYY3iGpgkzmVpUkPhiQXBJG/jvHG8JuyczHgWoCaKu8IvHLX4Q2BKXBkbnga8YO9OeKKcmysalJ1J97DXkf/rZQ1UO4UXum6Aa7Y8VDUSg8FdTLgUljOQ8wAIsyJ4xeyMWMIghBnHueY/mVGK6NLn4XyWFv5vTKO0be8LqFsKqJFl59bIpSiEmT6M7jE4/jJOt8fbh19HezurWPvoHXqPPqIUfUN7aB8doAli6Be6Qtfod+9P1I/Wo+FSGvVWM5voXkVbt5sRtbI=</latexit>GLocally refined reconstructionVisibility-aware Composition Figure 3. Compositional 3DGS Refinement. Given the coarse 3DGS reconstruction Gcoarse as input, we render initial coarse views, and refine them with image-to-image editing for enhancing local 3DGS Gupper,Glower,Ghead. Finally, a refined holistic 3DGS avatar Grefined is generated from these results by our proposed visibility-aware 3DGS Composition. into a holistic 3DGS avatar. The 3DGS generator in  uses four fixed canonical camera views as inputs to gener- ate a global 3DGS in unit space, but this fixed camera setup does not naturally accommodate additional local views. To address this challenge, we propose a simple yet ef- fective solution: a crop-aware local ray map that establishes correspondences between the 3D coordinates of local and global views. This approach extends  by incorporating additional local views as inputs, enabling high-resolution reconstruction of local body parts with fine details. Specifi- cally, for a pixel at coordinates (u, v)in a local view image of size (H, W ), where the local view is obtained by crop- ping a box region (xtl, ytl, xbr, ybr)from the global view, we map its coordinates back to the global view using: (i, j) =\u0012 xtl+(xbr−xtl)·u W, ytl+(ybr−ytl)·v H\u0013. (3) Using these mapped coordinates, we compute the camera ray embedding for the local view pixel using the 3DGS gen- erator’s global camera ray map equation: R(i, j) = (o(i, j),o(i, j)×d(i, j)) (4) where oanddrepresent the origin and direction of the cam- era rays based on the camera extrinsics. The crop-aware lo- cal ray map is utilized during both training and inference to help the 3DGS generator establish correspondences be- tween the 3D locations in local and global views. Using the crop-aware local ray map, we can directly use the 3DGS generator Gto map refined local views to 3DGS in the global avatar space. In the following, we will describe a strategy to combine the 3DGS produced by the local and global views into a holistic 3DGS avatar Grefined. Visibility-aware 3DGS Composition. As we will show in Fig 10, naively combining these partial 3DGS leads to floating artifacts and degraded appearance details. To ad- dress this challenge, we introduce a visibility-aware 3DGS composition scheme that intelligently merges the parts into a coherent, high-quality avatar. Our approach employs two key criteria to determine which 3D Gaussians to pre- serve during composition: (1) View Coverage quantifies how many input views capture each 3D Gaussian pointwithin their field of view, and (2) Visibility Salience mea- sures the gradient magnitude of the alpha channel across all rendered input views. Intuitively, Gaussians with low view coverage lack multi-view consensus and are likely un- reliable, while those with low visibility salience contribute minimally to the final appearance and likely represent noise. Specifically, given globally or locally reconstructed body part 3DGS Gpand the canonical views for each body part Ij p, where p∈ {full,upper,lower,head}andj= 0...3, we evaluate each splat Gi pas follows: First, we calculate the number of covered input views of the splat in different local parts nc(Gi p1,Ip2). A splat is considered reliable if it is covered by more than 2 input views in its own body part ( or 3 views if it is generatd by the head part. If the splat is also well-covered by input views of another more detailed body part (e.g., head is more detailed than upper-body), it is deemed redundant and removed. Second, we assess visibility salience using rendering gradients. If a splat has higher visibility in the input views of another body parts with similar level of detail (e.g., be- tween upper and lower body), it is likely redundant and should be dropped to avoid conflicts or redundancy. This approach ensures efficient composition while main- taining visual fidelity, focusing on the most reliable and vi- sually significant splats. 4. Experiments In order to comprehensively evaluate the performance of AdaHuman, we conduct experiments on avatar reconstruc- tion and avatar reposing tasks, comparing our method with state-of-the-art (SOTA) approaches both quantitatively and qualitatively. Additionally, we perform a user study to as- sess the perceptual quality of the generated avatars. Datasets. Unlike most existing 3D avatar reconstruc- tion methods that rely on 3D human mesh data for train- ing, AdaHuman leverages multi-camera video data from MVHumanNet, which captures 3D appearances of hu- mans in real-world settings and diverse poses. We sample 6,209 unique subjects for training and 50 unseen subjects for evaluating the novel pose synthesis task. Additionally, we mixed the training data with multiview images rendered from 589 human meshes in the CustomHumans  dataset 5 Input AdaHuman (Ours) Human3Diffusion SiFU SiTH GT ReferenceFigure 4. Qualitative comparison on vatar reconstruction task on CustomHumans dataset. Input AdaHuman (Ours) Coarse 3DGS Human3Diffusion SiTH SiFU Figure 5. Comparison on in-the-wild images. AdaHuman generalizes well to images with diverse appearances, body shapes, and clothing styles, while SIFU and SiTH fail on loose and complex clothing, and Human3Diffusion  fail to preserve appearance details. Coarse 3DGS is an ablation variant of AdaHuman without compositional 3DGS refinement, which fails to capture fine avatar details. with a more diverse camera distribution to improve general- izability. 50 testing subjects from the CustomHumans  dataset and 97 subjects from Sizer  dataset are used to quantitatively compare our method against SOTA ap- proaches. To further assess visual quality, we use 53 in-the- wild human images from the SHHQ  dataset to conduct a user study on perceptual quality. Runtime. Our whole pipeline takes around 70s for infer- ence on a NVIDIA A100 GPU. 4.1. Avatar Reconstruction For novel view synthesis, we compare AdaHuman with SOTA mesh reconstruction methods (SiTH  and SIFU ) and 3DGS-based methods (LGM  and Hu- man3Diffusion ) on the CustomHumans dataset. For each test subject, we use a frontal camera view as the input image and render 20 novel views ( 1024×1024 ) by ro- tating around the body. We follow  to extract mesh from3DGS results, and evaluate 3D reconstruction quality using Chamfer Distance (CD), Normal Consistency (NC) and F1 score. We evaluate rendering quality using PSNR, SSIM, and LPIPS scores for all novel views and the frontal view. FID scores are assessed to measure the perceptual quality of the avatars. We provide qualitative and quantitative com- parisons of AdaHuman against SOTA methods in Fig 4 and Tab. 1. Our method generates significantly higher-quality avatars, with a better performance on all of image quality metrics, while keeping a comparable performance in the 3D reconstruction metrics. 4.2. Perceptual Study To fully evaluate the perceptual quality and generalizability of our method, we conducted a user study on 53 in-the-wild images from the SHHQ  dataset. We compared AdaHu- man with SiTH, SIFU, Human3Diffusion, and an ablation of AdaHuman using the coarse 3DGS avatar without compositional 3DGS refinement. Each survey con- 6 Model PSNR ↑ SSIM↑ LPIPS↓ FID↓ CD(cm)↓ F-score ↑ Normal ↑ CH Sizer CH Sizer CH Sizer CH Sizer CH Sizer CH Sizer CH Sizer LGM  18.99 17.58 0.8445 0.8909 0.1664 0.1188 122.3 124.20 2.175 1.832 0.3941 0.4897 0.6431 0.6451 SiTH  20.77 20.67 0.8727 0.9219 0.1277 0.0883 42.9 37.11 1.389 1.229 0.4701 0.5688 0.7978 0.7915 SIFU  † 20.59 20.56 0.8853 0.9196 0.1359 0.0987 92.6 101.79 2.009 1.560 0.3438 0.4787 0.7539 0.7768 Human3Diffusion  21.08 19.50 0.8728 0.9211 0.1364 0.0953 35.3 20.69 1.230 1.174 0.5324 0.6336 0.7338 0.7389 AdaHuman (Ours) 21.46 21.42 0.8925 0.9258 0.1087 0.0856 27.3 19.15 0.962 1.135 0.6083 0.6075 0.7597 0.7477 Table 1. Quantitative comparison on avatar reconstruction task. On CustomHumans (CH)  and Sizer  datasets, AdaHuman surpasses all baselines on rendering quality metrics (PSNR, SSIM, LPIPS and FID ), and also achieves best the shape reconstruction metrics (CD, F-score), except getting slightly lower F-score with Human3Diffusion  on the Sizer dataset. However, since we borrow the same normal estimation method from, AdaHuman got similar performance on Normal Consistency. The best scores are highlighted. †: not using SIFU’s text-guided texture refinement since prompts are unavailable. Input AdaHuman (Ours ) Stage -I + Deform SIFU (SMPL -based Deformation) Ground -truth Reference Figure 6. Qualitative comparison on novel pose synthesis task. Baseline methods SiTH SIFU H3D Coarse 3DGS Preference of AdaHuman ( %)88.3 99.2 79.7 93.8 Table 2. User preference of AdaHuman. Our method achieves substantially higher preference against all baseline methods. sisted of 40 pairs of generated avatars, and 28 participants were asked to select the avatar with better overall quality. As shown in Tab. 2, AdaHuman was preferred by a sig- nificant margin over other methods. Fig 5 demonstrates that SIFU  and SiTH  often produce lower texture quality for side views and struggle to recover accurate ge- ometry, likely due to the limitations of template-based mesh reconstruction. Our method generates avatars with substan- tially higher quality and generalizes well across diverse ap- pearances, clothing styles, and body poses. Compared to Human3Diffusion, which fails to capture fine appear- ance details, our method recovers significantly better details thanks to our local refinement approach. More results on in- the-wild images are provided on the website. 4.3. Avatar Reposing and Animation Avatar Reposing. For avatar reposing evaluation, we sam- ple one input pose pinand six target novel poses ptarget from the video sequence for each unseen subject in the MVHumanNet dataset. Our method takes a single input im- agexinand pose conditions pin,ptarget as inputs, directly synthesizes the avatar in the target poses using the Pose-Method PSNR↑ SSIM↑ LPIPS↓ SiTH 21.21 0.8742 0.1261 SIFU 21.27 0.8722 0.1244 AdaHuman GR+ deform 23.01 0.8825 0.1100 AdaHuman GPt(Ours) 24.64 0.9046 0.0863 Table 3. Comparison on novel pose synthesis task. Our model achieves the best rendering similarity (PSNR, SSIM, LPIPS), showcasing the ability of our pose-conditioned model to gener- alize to diverse input and target poses. Conditioned Joint 3D Diffusion. We compare our approach with SOTA mesh-based methods SiTH  and SIFU  using the same inputs, which repose characters into target poses using linear blend skinning and the SMPL-X body model. As an additional baseline, we also evaluate results from directly deforming the input pose reconstructed 3DGS avatar into target poses by SMPL blending weights. In particular, other 3DGS-based methods, such as LGM  and Human3Diffusion  are excluded from this evalu- ation because they do not have aligned body models to support reposing of their reconstructed avatars. As shown in Tab. 3, AdaHuman significantly outperforms competing methods across all metrics. Fig 6 illustrates that our pose- conditioned model generalizes effectively to challenging in- put and target poses, benefiting from the diverse motions present in multi-view video datasets. Notably, AdaHuman excels at synthesizing realistic cloth deformations in target poses, while other methods struggle due to limitations of 7 Input Standard Pose 3DGS Avatar Animation with Unseen Input Motion Figure 7. AdaHuman generates animation-ready avatar in a standard pose, which can be animated with unseen input motion. Exp. JointDiff Grefined Body parts pgt PSNR↑ SSIM↑ LPIPS↓ FID↓ Coarse 3DGS Gcoarse ✓ ✗ F ✗ 20.84 0.8789 0.1296 31.9 Direct Composition ✓ ✓ U,L,H ✗ 20.41 0.8700 0.1350 36.2 Learnable Composition ✓ ✓ F,U,L,H ✗ 20.87 0.8788 0.1270 28.0 No Joint Diffusion ✗ ✓ F,U,L,H ✗ 20.79 0.8762 0.1283 27.6 Additional body part ✓ ✓ F,U,M,L,H ✗ 21.43 0.8922 0.1104 27.6 Ours ✓ ✓ F,U,L,H ✗ 21.46 0.8925 0.1087 27.3 Ours + GT Pose Condition ✓ ✓ F,U,L,H ✓ 23.00 0.9028 0.1086 27.0 Table 4. Ablation study. Without ground-truth pose, our full method achieves the best scores compared to the ablation baselines, show- casing the effectiveness of joint diffusion (JointDiff), compositional 3DGS with local refinement ( Grefined ), and the selection of body parts (F: fullbody, U: upper, L: lower, H: head, M: middle). Using ground-truth pose ( pgt) with our pose-conditioned model can further improve the alignment and provide better results. SMPL-based deformation and the fixed topology of mesh- based reconstruction methods. Additionally, in Fig 8, we show results of reposing SHHQ characters with complex loose clothing to stan- dard poses. Our reposing model successfully generalize to these OOD garments with realistic deformation effects. Input Input Figure 8. Reposing avatars with challenging garments. Avatar Animation. Fig 7 showcases the animation re- sults of AdaHuman using the animatable avatar from Avatar Reposing with a standard pose condition. Although the model is not directly trained with standard pose data, it learns to generalize to the standard poses with the help of the diverse distribution of poses in MVHumanNet. Avatar Reposing vs. LBS-based Animation As AdaHu- man supporting two modes to synthesize novel pose avatars, Fig 9 compares the performance of these two modes. Here we analysis by comparing their pros and cons. Mode 1: Direct Avatar Reposing - This mode directly 3DGS avatars directly generated on the target pose sequenceAnimated standard pose avatars by skinning weightsInput Figure 9. Comparison of direct avatar reposing and standard posed avatar with skinning weight animation. generates reposed Gaussians for a target pose. Pros: (1) Captures pose-dependent effects for non-rigid clothing, (2) More realistic deformation of loose clothing, (3) No need for rigging. Cons: More computationally expensive and less temporal coherent. Mode 2: SMPL-based LBS Animation - This mode first re- constructs a standard pose avatar, then applies SMPL-based skinning weights for motion deformation. Pros: (1) Enables real-time rendering, (2) Better temporal consistency. Cons: Limited loose clothing deformation. 4.4. Ablation Study To evaluate the effectiveness of our design choices, we con- duct various ablation studies on avatar reconstruction using the CustomHumans dataset. Tab. 4 and Fig 10 compare variants of our method, focusing on rendering quality. Coarse 3DGS Gcoarse uses only the generated coarse avatar without refinement, failing to capture fine details, partic- ularly in facial regions. Our full method achieves bet- 8 Coarse 3DGS Direct Composition No Joint Diffusion OursFigure 10. Comparison of our method and ablation variants. ter FID while maintaining comparable PSNR, SSIM, and LPIPS scores, demonstrating that compositional refinement improves details without sacrificing accuracy. Composition Strategy. We compare our visibility-aware approach with: (1) Direct Composition, which ensembles all local 3DGS without filtering unreliable splats, yet this variant results in significant artifacts; (2) Learnable Com- position, which uses a network with self-attention between parts to predict the holistic avatar. Despite showing slight improvement, this variant still encounters artifacts and re- quires more computation. This demonstrates the impor- tance and effectiveness of our visibility-aware 3DGS com- position. Body Part Selection. To evaluate the design of body part selection, we compare with variants that use an additional body part in the middle of the body for local refinement and 3D composition. This comparison demonstrates that using 4 parts (fullbody, upper, lower and head) is a good balance between performance and efficiency. No Joint Diffusion is a variant that applies the 3DGS gen- erator only to multiview images from the last diffusion step. Results show that it leads to view inconsistencies and per- formance drops, confirming the importance of 3D joint dif- fusion for consistent avatar generation. GT Pose Condition shows that using ground-truth SMPL annotations significantly improves reconstruction quality through better pose alignment, indicating potential for fur- ther improvement. 5. Discussion and Limitations In this paper, we introduced AdaHuman, a novel frame- work for generating highly-detailed and animatable 3DGS avatars from a single input image. Our approach inte- grates 3DGS reconstruction within the multi-view diffu- sion process, ensuring 3D-consistent generation of multi- view images as well as 3DGS avatars in both input andnovel poses. Furthermore, our visibility-aware composi- tional 3DGS refinement module significantly enhances the appearance details of the avatars and seemlessly integrates local and global body parts into a coherent 3DGS avatar. Extensive experiments on public benchmarks and in-the- wild images showed that AdaHuman substantially outper- forms state-of-the-art methods in both novel view synthesis and novel pose synthesis tasks. Despite these advancements, some limitations of our method warrant further exploration. The local refinement strategy may encounter difficulties with occluded or poorly covered regions, particularly around hands and arms, leading to artifacts and limiting fine-grained animation in these areas. Additionally, while our model can gen- erate avatars in an animation-friendly standard pose, the animation capability still relies on the alignment of the SMPL body models and their skinning weights, which poses challenges in detailed animation such as facial expressions, hand gestures, and garment deformation. Future work could explore better integration of body models and simulation-based methods, as well as the use of video diffusion model to enhance the animation quality. References  Easymocap - make human motion capture easier. Github, 2021. 12  Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In SIGGRAPH Asia, 2023. 2  Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular 3d reconstruction of humans wear- ing clothing. In CVPR, 2022. 2  Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan- Yee K Wong. DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models. In CVPR, 2024. 2  Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Effi- cient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 2  Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan- tasia3d: Disentangling geometry and appearance for high- quality text-to-3d content creation. In ICCV, 2023. 2  Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim- itrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. In ECCV, pages 20–40, 2020. 2  Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In IEEE Conf. Comput. Vis.Pattern Recog., pages 13142–13153, 2023. 13  Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy, Wayne Wu, and Ziwei Liu. 9 StyleGAN-Human: A data-centric odyssey of human gener- ation. In ECCV, 2022. 6, 8  Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. CAT3D: Create any- thing in 3d with multi-view diffusion models. In NeurIPS, 2024. 3, 12  Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++: Animation-ready clothed human re- construction revisited. In CVPR, 2021. 2  Hsuan-I Ho, Lixin Xue, Jie Song, and Otmar Hilliges. Learn- ing locally editable virtual humans. In CVPR, 2023. 4, 5, 6, 7, 13  Hsuan-I Ho, Jie Song, and Otmar Hilliges. Sith: Single- view textured human reconstruction with image-conditioned diffusion. In CVPR, 2024. 2, 6, 7, 13  Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3d. In ICLR, 2024. 2, 4  Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically ac- curate radiance fields. In SIGGRAPH. Association for Com- puting Machinery, 2024. 12  Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, and Ying Feng. Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation. In CVPR, 2024. 1, 2  Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz: Make a scene with complex 3d animatable avatars. In NeurIPS, 2023. 2  Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji- axiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided reconstruction of lifelike clothed humans. In 3DV, 2024. 1, 2  Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. ARCH: Animatable reconstruction of clothed humans. In CVPR, 2020. 2  Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Avatarcraft: Transforming text into neural human avatars with parameter- ized shape and pose control. In ICCV, 2023. 2  Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions onGraphics, 42 (4), 2023. 2, 3  Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed- uard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis- escu. Dreamhuman: Animatable 3d avatars from text. In NeurIPS, 2023. 2  Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Ed- uard Gabriel Bazavan, and Cristian Sminchisescu. Instant 3d human avatar generation using image diffusion models. InECCV, 2024. 2  Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, and Michael J Black. TADA! text to animatable digital avatars. In 3DV, 2024. 2 Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution Text-to-3D Content Creation. In CVPR, 2023. 2  Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per- shape optimization. In NeurIPS, 2023. 2  Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji- ayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In CVPR, 2024. 2  Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok- makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot one image to 3d object. In CVPR, 2023. 2  Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Sin- gle image to 3d using cross-domain diffusion. In CVPR, 2024. 2  Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi- person linear model. SIGGRAPH Asia, 2015. 2, 3  Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equa- tions. In ICLR, 2022. 4, 13  Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. Charactergen: Efficient 3d character generation from single images with multi-view pose canoni- calization. ACRM Trans. Graph., 43(4):1–13, 2024. 2  Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden- hall. DreamFusion: Text-to-3d using 2d diffusion. In ICLR, 2022. 1, 2  Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, and Liefeng Bo. Lhm: Large animatable human reconstruction model from a single image in seconds. InarXiv preprint arXiv:2503.10625, 2025. 2  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image gen- eration with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2  Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. SIGGRAPH, 2023. 2  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn- thesis with latent diffusion models. In CVPR, 2022. 2, 4, 12  Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In ICCV, 2019. 2  Shunsuke Saito, Tomas Simon, Jason M. Saragih, and Han- byul Joo. PIFuHD: Multi-level pixel-aligned implicit func- tion for high-resolution 3d human digitization. In CVPR, 2020. 2 10  Akash Sengupta, Thiemo Alldieck, Nikos Kolotouros, Enric Corona, Andrei Zanfir, and Cristian Sminchisescu. DiffHu- man: Probabilistic Photorealistic 3D Reconstruction of Hu- mans. In CVPR, 2024. 2  Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view dif- fusion base model. arXiv preprint arXiv:2310.15110, 2023. 2  Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d gen- eration. In ICLR, 2023. 2  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing diffusion implicit models. In ICLR, 2021. 3  Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. DreamGaussian: Generative gaussian splatting for ef- ficient 3d content creation. In ICLR, 2024. 2  Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2025. 2, 3, 4, 5, 6, 7, 12, 13  Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger- ard Pons-Moll. Sizer: A dataset and model for parsing 3d clothing and learning size sensitive 3d clothing. In ECCV, pages 1–18. Springer, 2020. 6, 7  Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang,, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from a single image. arXiv preprint arXiv:2403.02151, 2024. 2  A Vaswani. Attention is all you need. Adv. Neural Inform. Process. Syst., 2017. 12  Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score dis- tillation. In NeurIPS, 2023. 2  Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, et al. MVHumanNet: A large- scale dataset of multi-view daily dressing human captures. InCVPR, 2024. 4, 5, 8, 12  Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit clothed humans obtained from normals. In CVPR, 2022. 2  Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit clothed humans optimized via normal integration. In CVPR, 2023. 2  Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2  Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji- ahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. DMV3D: Denoising multi-view diffusion using 3d large reconstruction model. In ICLR, 2024. 2 Yuxuan Xue, Xianghui Xie, Riccardo Marin, and Gerard Pons-Moll. Human 3diffusion: Realistic avatar creation via explicit 3d consistent diffusion models. In NeurIPS, 2024. 1, 2, 3, 4, 6, 7, 12, 13  Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel Urtasun. S3: Neural shape, skeleton, and skinning fields for 3d human modeling. In CVPR, 2021. 2  Xu Yinghao, Shi Zifan, Yifan Wang, Chen Hansheng, Yang Ceyuan, Peng Sida, Shen Yujun, and Wetzstein Gordon. GRM: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In ECCV, 2024. 2  Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong- hai Dai, and Yebin Liu. Function4d: Real-time human vol- umetric capture from very sparse consumer rgbd sensors. In IEEE Conf. Comput. Vis.Pattern Recog., 2021. 13  Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACRM Trans. Graph., 2024. 12  Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, and Umar Iqbal. GAvatar: Animatable 3d gaussian avatars with implicit mesh learning. In CVPR, 2024. 2  Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min Zheng. Avatarverse: High-quality & stable 3d avatar creation from text and pose. In AAAI, 2024. Xuanmeng Zhang, Jianfeng Zhang, Chacko Rohan, Hongyi Xu, Guoxian Song, Yi Yang, and Jiashi Feng. Getavatar: Generative textured meshes for animatable human avatars. InICCV, 2023. 2  Zechuan Zhang, Zongxin Yang, and Yi Yang. SIFU: Side- view conditioned implicit function for real-world usable clothed human reconstruction. In CVPR, 2024. 2, 6, 7, 13  Tiancheng Zhi, Christoph Lassner, Tony Tung, Carsten Stoll, Srinivasa G Narasimhan, and Minh V o. Texmesh: Recon- structing detailed human texture and geometry from rgb-d video. In ECCV, 2020. 2  Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, and Wei Liu. Idol: Instant photorealistic 3d human creation from a single image, 2024. 2  Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In CVPR, 2024. 2 11 A. Implementation Details Network Structure. In Fig 11, we illustrate the archi- tecture of our Pose-Conditioned Multi-View Image LDM model, along with the 3DGS generators GandGcomp. For the LDM model, following, we enable 3D cross-view attention only in layers with a feature map resolution of ≤32×32. We also add extra input channels to the latent maps for camera ray maps, condition masks, and semantic pose maps. For G, we adopt the architecture of the pre- trained LGM-big model  and include additional input channels for noisy images xt. Additional, as an ablation mentioned at Tab. 4, we hvae tried training a compositional 3DGS generator Gcomp for Learnable Composition. Based on the LGM network, we insert an additional cross-part self-attention layer after each original cross-view self-attention layer in the LGM net- work. Note that the output image resolution of our LDM model is 512×512, which is then downsampled to 256× 256, the input resolution for the 3DGS generator G. Ray Map Embedding. We use different methods to em- bed ray map information for the image LDM model and the 3DGS generators GandGcomp. For the 3DGS generators, to effectively utilize the pretrained weights of LGM, we scale the entire scene to ensure a camera distance of r= 1.5 meters and use Pl ¨ucker ray embeddings as described in Eq 4 of the main text. For the LDM model, we employ sinusoidal positional embeddings  to encode ray origins and directions, pro- viding rich information about 3D locations across different cropping scales: RLDM(i, j) = PE( o(i, j),d(i, j)) (5) where PEis the sinusoidal positional encoding function, with the number of octaves Noctaves set to 8. View Sampling. Since our training data consists of multi- camera video captures in a 3D scene, the avatar is not al- ways positioned at a standard location. We use 2D joint lo- cations and foreground mask areas to crop global and local training views, resizing them to a resolution of 512×512. In Tab. 5, we list the OpenPose joints used to determine the cropping centers and relative size ratios of the local crops. During inference, after obtaining coarse reconstruction re- sults with global views, we render Nv= 20 views to esti- mate 3D joints using EasyMocap, which helps sample local views for our compositional 3DGS refinement. Parts Full body Upper Body Lower Body Head Joints Pelvis Neck Left Ankle, Right Ankle Left Ear, Right Ear Scale 1.0 0.5 0.5 0.25 Table 5. Body part sampling details.Training Schedule. We initialize our LDM model with the official weights of stable-diffusion-v1-51 and our 3DGS generator Gwith LGM-big2. For training the LDM model weights θ, the model first learns to predict K= 3 canonical views from one input view ( V= 1) without pose conditioning. We fine-tune the model on predicting global full-body views for 20,000 iterations, followed by fine-tuning on all Np+ 1 = 4 global and local view for another 30,000iterations to obtain θnopose. Finally, we fine-tune the pose-conditioned model weights θnovel posefrom θnopose. This model learns to pre- dictK= 4 canonical views of a novel pose avatar from V= 1 input views sampled from different frames in the same video sequence. The novel pose synthesis model is fine-tuned for 1,0000 iterations using all Np+1 = 4 global and local views. For training the 3DGS generator model G, we first fine- tune it from pre-trained weights using clean full-body im- ages in MVHumanNet  for 2,000 iterations to adapt it for human reconstruction. Then, we randomly sample diffusion timesteps to train with both noisy inputs xtand clean inputs x0for20,000iterations. The 3DGS model G is also fine-tuned on local views for an additional 20,000 iterations. We use Nref= 12 reference views of each part to supervise the predicted 3DGS. All training processes are conducted on 16 NVIDIA A100 80GB GPUs, with a total batch size of nbatch = 128 and a learning rate of η= 5×10−5. Training Losses. The training losses for the pose- conditioned LDM and the 3DGS generator are as follows: LLDM =LMSE(ϵ, ϵθ) (6) LG=Lrecon +λregLreg (7) Lrecon =λMSELMSE(ˆxt→0 novel,xnovel) +λLPIPSLLPIPS (ˆxt→0 novel,xnovel)(8) where the training loss of LDM, denoted as LLDM, is the MSE loss of the predicted latent noise. The training loss of Gconsists of rendering reconstruction loss computed using MSE and LPIPS. Following, we also incorporate the 3DGS regularization loss from [15, 59] to enhance surface quality. Inference. This section details the inference pipeline of avatar reconstruction and avatar reposing our method. In both settings, we perform 3D joint diffusion on global views only when t∈(500,900] to maintain the stability of the diffusion process. The earlier steps focus on pure 1https://huggingface.co/stable-diffusion-v1-5/ stable-diffusion-v1-5 2https://huggingface.co/ashawkey/LGM/resolve/ main/model_fp16_fixrot.safetensors 12 Figure 11. Network Architectures of (1) Pose-Conditioned Multi-View LDM Model and (2) Compositional 3DGS Generator. 2D diffusion to generate more detailed appearances. Dur- ing image-to-image local refinement, we utilize SDEdit  with a strength of s= 0.5, meaning that denoising be- gins at t= 500 and 3D joint diffusion is performed when t∈(350,500]. B. Evaluation Settings Baseline Models. Our baseline methods, including Hu- man3Diffusion, LGM, SiTH, and SIFU, have been trained on various 3D mesh datasets [8, 12, 58]. In this work, our aim is to demonstrate the advantages of training models on both mesh datasets and video datasets for better pose generalization and the synthesis of novel pose characters. We utilize their official weights for com- parison. We also note that some models (e.g. ) rely on private data or synthesized meshes for training. Avatar Reconstruction. We selected front views of the mesh avatar as input views, rendered by horizontal perspec- tive cameras for a fair and realistic comparison. The results of the quantitative evaluation are rendered at a resolution of 1024×1024 using 20perspective cameras. Avatar Reposing. For SiTH  and SIFU, we de- form their avatars to the target pose and align the avatar meshes with the ground-truth SMPL meshes to render im- ages for evaluation. 13",
  "text_length": 68113
}