{
  "id": "http://arxiv.org/abs/2506.05302v1",
  "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos",
  "summary": "We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding.",
  "authors": [
    "Weifeng Lin",
    "Xinyu Wei",
    "Ruichuan An",
    "Tianhe Ren",
    "Tingwei Chen",
    "Renrui Zhang",
    "Ziyu Guo",
    "Wentao Zhang",
    "Lei Zhang",
    "Hongsheng Li"
  ],
  "published": "2025-06-05T17:51:39Z",
  "updated": "2025-06-05T17:51:39Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05302v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05302v1  [cs.CV]  5 Jun 2025Perceive Anything: Recognize, Explain, Caption, and\nSegment Anything in Images and Videos\nWeifeng Lin1∗ ∗Xinyu Wei3∗Ruichuan An4∗Tianhe Ren2∗Tingwei Chen1\nRenrui Zhang1Ziyu Guo1Wentao Zhang4Lei Zhang3Hongsheng Li1†\n1CUHK2HKU3PolyU4Peking University\nLabel:Hang TagDefinition&Functionality:Atagisasmallpieceofpaperorplasticattachedtoanobject,typicallybearinginformationabouttheobjectoritsowner.Inthiscontext,thetaglikelycontainsthecat’sname,providingessentialinformationforpotentialrescuesoradoptions.Caption: Ayellow,teardrop-shapedtagwithaholeatthetop.Thetaghasblacktextthatreads“Tommy”and“Boy”.Thistagiswornonacat’sneckasanidentifier.Label:TaxiDefinition&Functionality:Ataxi,alsoknownasayellowcab,isavehicledesignedfortransportingpassengers.Inthiscontext,thetaxiiscapturedinmotiononacitystreet,likelyservingitsprimarypurposeofprovidingtransportationtoindividualsinneed.Caption: Ayellowtaxiwithavisiblefrontwindshield.Thetaxihasasleek,moderndesignwithaslightlycurvedhood.Itappearsblurry,inmotion,likelycapturedathighspeed.Label:AluminumalloyladderDefinition&Functionality:Analuminumalloyladderisalightweight,durableladdermadefromaluminumalloy,oftenusedforvarioustasksthatrequirereachingheights.Inthiscontext,thealuminumalloyworkerladderisbeingusedbyamantoaccessthewindowofabuilding,likelyforcleaningorrepairwork.Caption: Awhitealuminumextensionladderwithcylindricalrungsandparallelsiderails,featuringaslighttapertowardsthetop.\nPromptableVideo Caption: Theboy iswearingabluehoodedtop,seatingatawoodentable.In thevideo,heisholdingadark-coloredplaything,whichheappears pretendingtoshootsomethingorisveryfocusedonanobjectinthatdirection.Heislookingremainfairlyconsistent,withmovementsofhisheadandbodyasiftrackingatargetoradjustinghisaim.Hethenhidbehindachair.\nPromptableVideo Caption: Thelargehouseinthecenterofthebackgroundiscartoon-stylebuilding.Themainstructureandroofappeartobepredominantlyblue; Itissetinacolorfulenvironmentwithwhatlookslikeagrassyorsandyforeground,surroundedbystylizedtreesortallplantsandothersmallerobjectsorstructures.\nPromptableStreaming:\nThefemaleisactivelypresenting,withchangesinherhandgesturesandfacialexpressions.Shehasdarkhair.Thesearethescenesofthefootballgame. Thefemaleisnotvisuallypresenttodiscussthematch.\nIMAGEVIDEO\nFigure 1: Perceive Anything Model (PAM): PAM accepts various visual prompts (such as clicks,\nboxes, and masks) to produce region-specific information for images and videos, including masks,\ncategory, label definition, contextual function, and detailed captions. The model also handles\ndemanding region-level streaming video captioning.\nAbstract\nWe present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in im-\nages and videos. Our approach extends the powerful segmentation model SAM\n2 by integrating Large Language Models (LLMs), enabling simultaneous object\n∗Core Contributor\n†Corresponding Authors\nPreprint.\n--- Page 2 ---\nsegmentation with the generation of diverse, region-specific semantic outputs, in-\ncluding categories, label definition, functional explanations, and detailed captions.\nA key component, Semantic Perceiver, is introduced to efficiently transform SAM\n2’s rich visual features, which inherently carry general vision, localization, and\nsemantic priors into multi-modal tokens for LLM comprehension. To support ro-\nbust multi-granularity understanding, we also develop a dedicated data refinement\nand augmentation pipeline, yielding a high-quality dataset of 1.5M image and\n0.6M video region-semantic annotations, including novel region-level streaming\nvideo caption data. PAM is designed for lightweightness and efficiency, while\nalso demonstrates strong performance across a diverse range of region under-\nstanding tasks. It runs 1.2 −2.4×faster and consumes less GPU memory than\nprior approaches, offering a practical solution for real-world applications. We\nbelieve that our effective approach will serve as a strong baseline for future re-\nsearch in region-level visual understanding. Code, model and data are available at:\nhttps://Perceive-Anything.github.io\n1 Introduction\nThe vision community has rapidly witnessed advances in vision foundation models, such as SAM [ 34]\nand SAM 2 [ 52], which have dramatically improved interactive object segmentation performance in\nimages and videos. These models offer remarkable precision in localizing arbitrary objects based on\nvarious visual prompts. However, they typically lack deep semantic understanding of the segmented\nregions, elucidating what these regions mean or how they function in context remains a challenging\nproblem.\nRecent studies seek to endow Vision–Language Models (VLMs) with region-level understanding\ncapability through visual prompts. As illustrated in Fig. 2, current methods can be grouped into three\nparadigms: (1)textual encoding [ 63,78,86,44], which encode 2-D bounding-box coordinates as\nnatural-language strings inside the prompt, thereby supplying no explicit region prior; (2)visual-\nprompt encoding (VPE) [ 41,51], which introduce extra module to embed regional image features\nand positional features; (3)RoI/segmentation-based encoding [ 38,77,83,80,29], which utilize\nan external mask generator to concatenate image embedding and mask embedding. While these\nmethods show promise, they often present several limitations: (i)they usually generate only limited\nsemantic outputs—often just category labels or short captions [ 26,88,69,67];(ii)their designs\nare modality-specificl, focusing on one single visual modality (image or video), offering limited\ngenerality [ 63,78,77,80,81].(iii)they rely on external segmentation models to supply masks, a\nserial design that adds computational overhead and makes overall performance sensitive to mask\nquality [80, 81, 38].\nTo address these challenges, we introduce the Perceive Anything Model (PAM), an end-to-end\nregion-level vision-language model designed for fast and comprehensive fine-grained visual under-\nstanding across both images and videos, encompassing capabilities such as predicting categories,\nexplaining the definition and contextual function of identified regional elements, and generating\ndetailed descriptions of specific regions. Rather than redesigning model architecture from scratch, our\napproach efficiently extends the SAM 2 framework with Large Language Models (LLMs) to support\nsemantic understanding. Specifically, we introduce a Semantic Perceiver that acts as a essential\nbridge, effectively leveraging rich intermediate visual features from the SAM 2 backbone to integrate\ngeneral vision, localization, and semantic priors into visual tokens. These tokens are subsequently\nprocessed by the LLM to generate a diverse semantic outputs. Furthermore, PAM features a parallel\ndesign for its mask and semantic decoders, enabling simultaneous generation of region masks and\nsemantic content, thereby improving computational efficiency.\nTo ensure PAM’s robustness in understanding region-level multi-dimensional semantic granularity,\nhigh-quality training data is an essential component. While multiple existing datasets [ 6,32,36,43,\n29,68] provide region-semantics annotations, we noticed that they are often overly coarse, limiting\ntheir utility for fine-grained understanding tasks. Therefore, to construct high-quality training data,\nwe develop an advanced data refinement and augmentation pipeline that leverages leading VLMs\n(e.g., GPT-4o [ 27]) and human expert validation to refine and augment existing region-level annotated\ndatasets. For images, we generate annotations at multiple distinct semantic granularities for each\nspecific region: a fine-grained category label, a context-aware definition that clarifies the region’s role\n2\n--- Page 3 ---\nLLM\nVisionEncoderTokenizerVisualpromptsTask\nVisionEncoderTokenizerTaskVPELLM\nVisionEncoderTokenizerTaskSeg.Modelobj.maskLLM\nTaskSeg.Model(VisionEncoder)VisualpromptsSemanticsNomaskSemanticsNomask\nVisualpromptsSemanticsobj.maskSemantics\nTokenizerLLM\n(a)PreviousTextualMethods(b)PreviousVPEMethods(c)PreviousRoI-basedMethods(d)PAM(ours)Figure 2: Previous Paradigms vs. Our Paradigm (PAM). (a & b) Textual/VPE methods provide\nregion understanding using positional embeddings but typically lack simultaneous object masks.\n(c) RoI/Segmentation-based methods use external segmenter for object masks, subsequently fusing\nimage and mask embeddings. (d) In contrast to previous paradigms, our method directly treats the\nSeg. model as vision encoder. It effectively leverages the rich visual embeddings from the robust\nsegmentation model and features a parallel design for its mask and semantic decoders.\nor function within the scene, and detailed descriptions. For videos, we refined original coarse-level\nannotations from referring video detection and segmentation dataset [ 64,58,18,71,17] into detailed,\ntemporally-aware region-level captions. Furthermore, we pioneered the development of event-based,\nregion-level streaming video caption data. To the best of our knowledge, this is the first work to\nconstruct such a dataset, enabling the model to support streaming video region captioning. Notably,\nwe also generate bilingual (English and Chinese) versions of each data annotation to equip the model\nwith multilingual response capabilities. This process yields a final high-quality dataset comprising\n1.5M image-region-semantics triples and 0.6M video-region-semantics triples.\nOur experimental results demonstrate that PAM delivers robust performance across a diverse range\nof regional understanding tasks for both images and videos, while operating 1.2 −2.4×faster and\nconsuming less GPU memory compared to prior models. We believe our model, dataset, and insights\nwill significantly advance research in this domain and broadly benefit the vision-language community.\n2 Related Work\nInteractive Image and Video Object Segmentation. Interactive object segmentation has pro-\ngressed rapidly in recent years. Early methods—such as Graph Cut [ 5] and Active Contours\n[9]—relied on manual annotations (e.g., foreground/background clicks). Inspired by the paradigm of\npre-training autoregressive Transformer architectures on large-scale data in language modeling, the\nSegment Anything Model (SAM) [ 34] revolutionized user–model interaction by ingesting multiple\nvisual prompts and segmenting arbitrary objects in a class-agnostic manner. SAM 2 [ 52] extended\nthis capability to video, enabling real-time processing of arbitrarily long sequences with strong\ngeneralization. Subsequent research [ 33,82,87,54,13,74], while retaining the core architecture,\nhas further improved the accuracy and efficiency of this model family.\nRegion-level Vision-Language Models (VLMs). Region-level understanding tasks, such as region\nclassification and captioning, are fundamental in computer vision. Regarding image-based VLMs,\nrecent researches [ 46,12,70,84,77,80,8,41,1] have demonstrated a notable trend towards enabling\nregion-level understanding capabilities through spatial visual prompts. Furthermore, research has\nextended regional understanding to the video domain [ 78,63,81,48], focusing on identifying and\ninterpreting user-specified regions across temporal intervals. However, these approaches typically\noperate within a single modality (either image or video), and more complex tasks, such as region-level\nstreaming video captioning—which requires continuously generating textual descriptions for specific\nregions as a video progresses—remain largely unaddressed.\nStreaming Video Captioning. Streaming video captioning demands per-frame processing and rapid\nresponse times. Recent online video understanding models [ 16,20] aim to identify the current action\nat each timestamp. Streaming video caption [ 89] propose to incorporate memory modules and develop\nspecialized streaming decoding algorithms to support streaming captioning. VideoLLM online [11]\n3\n--- Page 4 ---\ntimeImageEncoderPrompt EncoderMask Decoder\nMemoryEncoder&Bank\nLargeLanguageModel\nRichVisualSemanticContent:1. Category2. Definition&Functionality3. Brief Caption4. Detail Description5. Streaming Video Caption(Bilingual:Englishand 中⽂)Semantic Perceiver\nS2-FFM\nimage embeddingoutput tokenssem.tokens \n(b) Mask Decoder(a) SAM 2\n(c) Semantic Decoder \nTrainableFrozenStructuralcopyConcatS2-FFMSemantic Perceiver\nMaskdecodingFigure 3: Overall Architecture of PAM.\nfurther pioneered the use of LLMs to achieve free-form dialogue synchronized with the online video\nstream. However, these approaches predominantly focus on general event comprehension, leaving\nthe continuous tracking and description of specific regions within a video stream as a significant\nunresolved challenge.\n3 Perceive Anything Model (PAM)\nGiven visual prompts such as points, boxes, or masks to specify a region of interest, Perceive\nAnything Model (PAM) can simultaneously: (1) Segment: Generate precise segmentation masks for\nthe indicated region within an image or throughout a video. (2) Recognize: Identify the category of\nthe designated region or object. (3) Explain: Provide clear explanations of the region’s or object’s\ndefinition, attributes, and functionality within its given context. (4) Caption: Generate concise or\ndetailed captions for the region within images, videos, and video streams.\n3.1 Model Architecture\nAs illustrated in Fig. 3, our PAM can be divided into two parts. The first part is the SAM 2 framework,\nwhich comprises an image encoder, a prompt encoder, memory modules, and a mask decoder. This\nframework provides robust spatio-temporal visual feature extraction and segmentation capabilities.\nThe second part is a semantic decoder, which is based on a large language model (LLM). Crucially,\nour proposed Semantic Perceiver acts as a bridge, effectively leverages intermediate visual features\nfrom the SAM 2 backbone and results in visual tokens. These tokens are subsequently processed by\nthe LLM to generate diverse semantic outputs. For decoding, PAM features a parallel design for its\nmask and semantic decoders, enabling the simultaneous segmentation of objects while generating\ndiverse semantic outputs of them. The design of components and training process are detailed below.\nSemantic Perceiver. As shown in Fig. 3(b) and Fig. 4, the architecture of Semantic Perceiver\nmirrors the SAM 2 Feature Fusing module (S2-FFM), employing a lightweight two-layer transformer\nwith self-attention, cross-attention, and a point-wise MLP. Specifically, it receives two primary inputs:\nenhanced mask tokens from S2-FFM, which incorporate IoU and prompt tokens information and\nserve as unique identifiers for precise mask generation; and updated image embeddings after S2-FFM,\ncapturing general visual context and implicit features enriched through interaction with mask tokens.\nNext, following [ 26,28], we concatenate Nslearnable semantic tokens with the enhanced mask\ntokens. Finally, through further attention mechanisms within the Semantic Perceiver, we can fetch\nvisual tokens rich in both general visual and object-level localization information. Given an input of\nNframes (where N=1 for a single image), Semantic Perceiver outputs two sets of 256-dimensional\nvectors: 642×Nvisual tokens and Ns×Nsemantic tokens ( Ns= 16 by default).\nProjector. The projector preceding the LLM comprises two layers: a pixel shuffle operation and\nan MLP projector. For image inputs, we apply the pixel shuffle operation over adjacent 2 ×2 feature\npatches to downsample the number of visual tokens. For video inputs, the prompted frame is\nprocessed similarly with single image, while the remaining frames in the video clip undergo a more\naggressive 4 ×4 pixel shuffle operation to significantly reduce visual tokens and further improve\nprocessing efficiency for semantic decoder. Subsequently, we use two distinct MLPs [45] to project\nvisual and semantic tokens separately.\n4\n--- Page 5 ---\nSelf attn.Token to image attn.MLPImage to token attn.Image embedding(256x64x64)IoUTokensMaskTokensPromptTokens(Ntx256)Self attn.Image to token attn.x2x2Token to image attn.MLPMaskTokensSem.Tokens(Nsx256)Large Language Model (Qwen 2.5)…ImageTokens (1024)maskdecoding\nVideoTokens (clip1) (1024+[N-1]×256)…Sem.T (N×16)TaskTokensOutputTokensmasks…………\nImage Proj.Sem. Proj.TokenizerPixel Shuffle (2×2 or 4×4)VideoTokens(clip2) (1024+[N-1]×256)………S2-FFMSemantic PerceiverSemantic Decoder…Figure 4: Detailed illustration of our PAM workflow. Semantic Perceiver first receives enhanced\nimage embeddings and mask tokens from the S2-FFM and outputs enriched visual tokens and\nsemantic tokens. These are subsequently fed into the semantic decoder for decoding.\nSemantic Decoder. We adopt the pre-trained Qwen2.5 LLM [ 72] as our semantic decoder, lever-\naging its strong language processing capabilities. This decoder is responsible for interpreting the\nprocessed visual tokens and semantic tokens alongside task instructions to generate the desired\nsemantic outputs.\nStreaming Video Encode and Decode. Building upon the progressive introduction of historical\ninformation per frame via memory modules in SAM 2, we propose a straightforward strategy for\nregion-level streaming video captioning without adding complex components. Specifically, an\nadditional 2 ×2 pixel shuffle operation is applied to the last frame of each video clip. This leads to a\ngreater density of visual tokens, improving the preservation of historical visual information. These\ntokens subsequently act as the initial frame for the next video clip and are processed by the LLM\ntogether with the remaining frames of that clip. This approach ensures that each clip is processed\nconsistently and effectively passes crucial historical information from the previous clip into the\nnext video clip. Additionally, we incorporate the previous textual description into the prompt to\nfurther augment contextual history, enhancing the model’s comprehension and descriptive accuracy\nfor ongoing events. In practice, our framework allows users to flexibly specify decode timestamps.\nUpon reaching a designated timestamp, the model describes the specified region within the temporal\ninterval between the current timestamp and the previous one.\nTraining Strategies. We structure our training process using a three-stage curriculum learning\napproach, progressively enhancing the PAM’s region-level visual understanding capabilities from\nimages to video. In all training stage, the parameters of SAM 2 are frozen. The hyper-parameters for\neach training stage are summarized in Appendix A.\n•Stage 1: Image Pretraining and Alignment. The initial training stage focuses on establishing robust\nalignment among visual tokens, semantic tokens and the language model’s embedding space. The\nprimary objective is to enable the model to effectively understand region-level image content. To this\nend, we utilize a large dataset of region-level image classification and captioning. During this stage,\nonly the semantic perceiver and the projector are trained.\n•Stage 1.5: Video-Enhanced Pretraining and Alignment. In this stage, we extend the initial image-\nbased training by incorporating region-level video captions. This inclusion enables the model to\ncomprehend dynamic scenes through the integration of spatio-temporal visual information. The\ntrainable modules are the same as in Stage 1.\n•Stage 2: Multimodal Fine-Tuning. The final stage employs supervised fine-tuning (SFT) to enable\nthe model to perform diverse tasks and generate desired responses. This stage utilizes a high-quality\ndataset, which has been refined and augmented via our pipeline (Sec. 4). Training in this phase jointly\ninvolves the semantic perceiver, the projector, and the semantic decoder.\n4 Data\nTo enhance PAM’s comprehensive visual perception capabilities, we develop a robust data refinement\nand augmentation pipeline to curate a high-quality training dataset. This dataset is distinguished by\nthree key features: (1) Broad-ranging Semantic Granularities. It provides diverse visual semantic\n5\n--- Page 6 ---\nLabel:BottleDefinition & Function:Abottleisacontainer,typicallyusedtoholdliquids,makingitconvenientforcarryingandstorage.Inthiscontext,thebottleappearstocontainmouthwash.Itisusuallyswishedaroundthemouthorgargledandthenspatout.Caption:It isatranslucentgreenplasticbottle.Ithasarelativelysimple,slightlytaperedcylindricalshape.Thebottleappearstocontainaclearliquid,andalabelisvisibleonitsfront.Thebottleispositionedupright....,thenthedoginthestripedoutfitmovestowardsthepink-claddogforacloserinteraction,withitsgarment'swearanddamageinview.\nThegoldenretriever,adornedinapatternedoutfit,maintainsarelaxedposeonafluffysurfacewhileslightlyturningitsheadmoretotheleft.A black CatThe green bottle\nOrig. Ann. : The dog is playing with another dog.  \n12111\n111111\n111123456123456Orig. Ann.Orig. Ann.\n……\nTrace-UniNon-streamAnno.RefineFigure 5: Illustrative examples of our dataset construction pipeline. The left panel displays image\nannotations; the right panel details annotations for non-streaming and streaming video.\nannotations spanning from coarse-level (categories, definitions, contextual functionalities) to fine-\ngrained (detailed descriptions) (Sec. 4.1). (2) Regional Streaming Caption Annotations. The first\ndataset to curate annotations specifically for streaming video region captioning (Sec. 4.2). (3)\nBilingual Annotations, supporting both English and Chinese (App. B.2). The pipeline is detailed\nbelow, and additional information are available in Appendix B.\n4.1 Image Dataset\nRegional Recognition, Explanation, and Caption. For regional recognition, we utilize multiple\ninstance detection and segmentation datasets [ 55,35,40,23,50,66], along with scene text recognition\ndatasets [ 56,31,30,19,24,14,76,57,4]. In this context, the bounding box or mask serves as the\nvisual prompt input, and the label is treated as the output.\nTo achieve deep, fine-grained visual understanding beyond simple classification, we propose an\nenhanced pipeline that generates: clear conceptual explanations, contextual functional roles, and\ndetailed descriptions for each specific region. This multi-dimensional information aims to significantly\nimprove user comprehension, particularly for uncommon terms or unfamiliar subjects. To implement\nthis, we utilize the latest VLMs for their extensive world knowledge and powerful visual understanding\ncapabilities to assist refinement. Specifically, we apply the Set of Mask (SoM) method [ 75] to identify\nregions of interest, and use original annotations as context to guide models to produce desired\nresponses, which then undergo manual quality assurance. An illustrative example is presented in\nFig. 5(left). We present more details in Appendix B.1.\n4.2 Video Dataset\nRegion-level Video Caption. To extend the model’s regional captioning capabilities to video, we\ncollected and analyzed several existing video datasets, including referring detection and segmentation\ndatasets [ 71,47,18,62,58,17,85,64], as well as the recent Sa2V A [ 79] annotations for the SA-\nV [53] dataset. These datasets, designed for detecting, segmenting, and captioning specific objects\nin videos based on textual descriptions, often contain descriptions that are overly coarse, simplistic,\ninaccurate, or predominantly static, neglecting essential temporal details such as object motion,\ninteractions, and state changes throughout the video.\nTo address the existing limitations, we propose the storyboard-driven caption expansion method.\nThis process involves several key stages: (1) Keyframe Sampling : Six keyframes are uniformly\nextracted from each video. (2) Storyboard Synthesis : These extracted keyframes are combined\nto form a high-resolution composite image, presented in a storyboard format (as illustrated in\nFig. 5). (3) Object-Centric Highlighting : Within this composite image, each individual frame\nspecifically highlights the target object using a colored bounding box or mask, implemented by\n6\n--- Page 7 ---\nModelClassification OCR\nLVIS PACO COCO Text Total-Text\nSemantic Sim. Semantic IoU Semantic Sim. Semantic IoU Acc.(%) Acc.(%)\nShikra-7B [12] 49.7 19.8 43.6 11.4 – –\nGPT4RoI-7B [84] 51.3 12.0 48.0 12.1 – –\nOsprey-7B [80] 65.2 38.2 73.1 52.7 – –\nFerret-13B [77] 65.0 37.8 – – – –\nVP-LLA V A-8B [41] 86.7 61.5 75.7 50.0 44.8 46.9\nVP-SPHINX-13B [41] 87.1 62.9 76.8 51.3 45.4 48.8\nDAM-8B [38] 89.0 77.7 84.2 73.2 – –\nPAM-1.5B (Ours) 87.4 76.5 85.1 73.5 39.4 48.6\nPAM-3B (Ours) 88.6 78.3 87.4 74.9 42.2 52.3\nTable 1: Results of region-level image recognition on LVIS, PACO, COCO Text, and Total-Text.\nModelVG Refcocog Ref-L4 Ferret Bench MDVP Bench\nMETEOR CIDEr METEOR CIDEr ROUGE-L METEOR CIDEr Refer. Desc. Avg.\nGLaMM-7B [51] 17.0 127.0 15.7 104.0 23.8 10.1 51.1 - -\nOsprey-7B [80] - - 16.6 108.3 - - - 72.2 44.3\nFerret-7B [77] - - - - 22.3 10.7 39.7 68.7 47.6\nVP-LLaV A-8B [41] - - 22.4 153.6 - - - 75.2 70.6\nVP-SPHINX-13B [41] 20.6 141.8 23.9 162.5 22.6 10.7 32.4 77.4 74.3\nOmni-RGPT-7B [25] 17.0 139.3 17.0 109.7 - - - - -\nRegionGPT-7B [21] 17.0 145.6 16.9 109.9 25.3 12.2 42.0 - -\nDAM-3B [38] - - - - 30.8 17.2 56.4 - -\nDAM-8B [38] - - - - 37.1 19.4 70.0 - -\nPAM-1.5B (Ours) 19.2 132.9 24.7 135.0 26.6 14.9 49.8 72.4 68.4\nPAM-3B (Ours) 20.8 142.3 26.9 143.1 31.3 17.2 59.7 77.5 72.2\nTable 2: Performance comparison on region-level image captioning across multiple benchmarks.\nSoM. (4) LLM-Powered Elaboration : Then, using the original annotations as condition, we prompt\nGPT-4o to generate descriptions that are both refined, detailed and temporally aware. This multi-\nframe consolidation is critical as it enhances GPT-4o’s contextual comprehension, yielding superior\ndescriptions compared to individual frame analysis.\nRegion-level Streaming Video Caption. Beyond describing the entire video, we aim to extend the\nmodel’s capabilities to a streaming manner. To achieve this, we perform additional augmentation on\nour refined region-level video caption data. Specifically, we first employ the TRACE-Uni model [ 22]\nto segment the input video into multiple distinct events, each demarcated by its temporal boundaries.\nSubsequently, for each segmented video clip, we apply the same ‘storyboard-driven’ processing\nmethod. To generate precise and continuous event descriptions, the GPT-4o input prompt was\nredesigned to iteratively incorporate the description from the preceding video clip as contextual\ninformation for processing the current clip. The entire workflow is illustrated in Fig. 5(right).\n5 Experiments\n5.1 Implementation Details\nWe employ Qwen2.5-1.5B/3B [ 72] as our semantic decoder, and utilize the pre-trained hierarchical\nSAM 2-Large3as the base vision foundation model. By default, we use 16 learnable semantic tokens\nand uniformly sample 16 frames per video clip. All training is conducted on 8 NVIDIA A100 GPUs\nwith 80GB. For all evaluation experiments, we adopt a zero-shot test manner without fine-tuning on\nspecific datasets. The best and the second best results are indicated in bold and with underline\n5.2 Image Benchmarks\nRegional Recognition and Explanation. This task requires the model to identify either the object\ncategory or scene text within a specified image region. Recognition performance is assessed on\nthe validation sets of the LVIS (object-level) [ 23] and PACO (part-level) [ 50] datasets, alongside\nthe test sets of COCO-Text [ 61] and Total-Text [ 14]. Standard evaluation metrics include Semantic\nSimilarity [80], Semantic Intersection over Union (Sem. IoU) [15], and accuracy.\n3https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n7\n--- Page 8 ---\nFace Mask\nLabelDefinition & FunctionalityAfacemaskisaloose-fittingdisposablecoverthatprotectsthewearer’smouthandnosefromthesurroundingenvironment.Inthisimage,thepersonislikelywearingitforreducingthespreadof droplets.⼝罩是⼀种用于遮盖⼝鼻的⼀次性防护用品。在这张图片里，这位正在练习书法的⼈佩戴⼝罩，很可能是出于卫⽣目的，以减少飞沫的传播。⼝罩CaptionThebluemaskiswornbythepersonasheconcentrateontheartwork.Itoffersalayerofprotection,ensuringthatthepersoncanimmersethemselvesinthedetailedbrushwork.\nPianistLabelDefinition & FunctionalityApianistisamusicianwhoplaysthepiano.Inthecontext,thepianistisperformingwithafullorchestrainwhatappearstobeaconcerthallorlargevenue.Hisfunctionalityistoexecutethepianopartofamusicalpiece,captivatingtheaudience.钢琴家是演奏钢琴的音乐家。这位钢琴家似乎正在⼀个⼤型场馆里，与整个乐团合作演出。他的主要任务是负责演奏乐曲中的钢琴部分，其表演深深地吸引观众。钢琴家CaptionThepianist,positionedatthecenterofstage,underthebrightstagelights,servesasthecentralfigureinthisperformance.Hisskilledplayingcreatesaprofessionalandimmersiveatmosphere.这是⼀个蓝⾊的⼝罩，练习书法的⼈正戴着这个⼝罩，专注于他的艺术创作。这层⼝罩提供了保护，让他能够安⼼地沉浸在对笔墨细节的精⼼处理之中。\n这是⼀位钢琴家，正处于舞台的正中央，明亮的聚光灯打在他的身上，他是整场演出的核⼼⼈物。他的⼿在钢琴上飞舞，技艺娴熟的演奏给整个现场营造出⼀种既专业、又让观众沉浸其中的氛围。Figure 6: PAM provides various semantic granularities informantion and support bilingual outputs.\nAs shown in Table 1, both our PAM-1.5B and PAM-3B demonstrate strong performance. Notably,\nPAM-3B significantly outperforms other competing methods. It achieves optimal performance on the\nPACO benchmark, exceeding the previous best model by over 3.2%, and surpasses the current SOTA\nmodel, DAM-8B, on the LVIS benchmark in terms of semantic IoU. Furthermore, as indicated in the\nright column of Table 1, our PAM-3B outperforms VP-SPHINX-13B by over 3.5% on Total-Text\nand achieves comparable performance on COCO-Text. These results demonstrate its promising\ncapabilities in scene text recognition. We further showcase qualitative visualizations in Fig. 6,\nillustrating PAM’s effectiveness in generating insightful explanations that cover both the general\ndefinition and the contextual role of prompted objects.\nRegional Caption. We evaluate the model’s capability to generate both concise and detailed\nregion descriptions on multiple benchmarks. For concise region captioning, we evaluate on the\nvalidation splits of RefCOCOg [ 32] and Visual Genome (VG)[ 36]. For more expressive descriptions,\nassessments are conducted on the challenging Ref-L4[ 10] dataset. Caption quality is measured using\nROUGE-L [ 39], METEOR [ 3], and CIDEr [ 60]. Additionally, we benchmark referring descriptions\nvia Ferret-Bench [ 77] and MDVP-Bench [ 41], where GPT-4o is employed to gauge the quality of the\ngenerated responses.\nAs the results shown in Table 2, PAM-3B surpasses existing methods on the VG, RefCOCOg,\nand Ferret benchmarks. On MDVP-Bench, it achieves performance comparable to the current\nSOTA method, VP-SPHINX-13B. Furthermore, on the Ref-L4 benchmark, PAM-3B demonstrates\noutstanding performance, surpassing all models except the top-performing DAM-8B. Notably, these\ncompetitive results are achieved with fewer parameters and reduced computational cost, highlighting\nPAM’s excellent balance of performance and efficiency.\n5.3 Video Benchmarks\nModelElysium BensMOT HC-STVG VideoRefer-Bench-D\nMETEOR METEOR CIDEr SC AD TD HD Avg.\nElysium-7B [63] 19.1 1.1 – – 2.35 0.30 0.02 3.59 1.57\nMerlin-7B [78] – – 11.3 10.5 – – – – –\nOmni-RGPT-7B [25] 9.3 14.6 – – – – – – –\nArtemis-7B [49] – – 18.0 53.2 3.42 1.34 1.39 2.90 2.26\nVideoRefer-7B [81] – – 18.7 68.6 4.44 3.27 3.10 3.04 3.46\nDAM-3B [38] – – 18.2 72.7 3.62 2.86 2.81 2.67 2.99\nDAM-8B [38] – – 21.0 91.0 4.69 3.61 3.34 3.09 3.68\nPAM-1.5B (ours) 20.7 19.1 18.8 58.9 3.63 2.71 2.67 2.89 2.97\nPAM-3B (ours) 24.3 21.6 23.3 70.3 3.92 2.84 2.88 2.94 3.14\nTable 3: Performance comparison on video region captioning.ModelActivityNet\nCIDEr METEOR G-STDC\nVideoRefer-7B [81] 22.1 14.7 1.73\nDAM-3B [38] 11.3 14.8 0.94\nGIT∗[65] 29.8 7.8 –\nVid2Seq∗[73] 30.2 8.5 –\nStreaming Vid2Seq∗[73] 37.8 10.0 –\nPAM-1.5B (ours) 27.3 22.6 2.15\nPAM-3B (ours) 30.1 27.3 2.67\nTable 4: Performance for streaming\nregion captioning on ActivityNet.\nVideo Region Caption. This task requires the model to generate an accurate and temporally-aware\ndescription for a prompted region within the video’s context. We primarily evaluate on four public\nbenchmarks: Elysium [ 63], BensMOT [ 37], HC-STVG [ 59], and VideoRefer-Bench-D [ 81]. As\nshown in Table 3, our PAM-1.5B and PAM-3B achieve the SOTA performance on both the Elysium\nand BensMOT benchmarks. Furthermore, our PAM-3B surpasses the current SOTA method, DAM-\n8B, by 2.3% in terms of METEOR on the HC-STVG benchmark. On the VideoRefer-Bench, our\n8\n--- Page 9 ---\nThesmartphoneremainsstationaryonthe bluesurface.Itispositionedtotherightofablackcameraonthesurface.Thephone’sscreenison,displayingwhatappearstobeahomescreen.Inthe video,ahandholdingasmalltoolappear,butthephoneitselfisnotmoved.Non-StreamStream\n这部⿊⾊的⼿机放在浅蓝⾊的桌面上。它在⼀个⿊⾊相机和⼀条蓝⾊线的旁边。⼿机的屏幕是亮着的，上面显示着各种应用程序的图标。在视频中，有⼀只拿着小⼯具的⼿在镜头前面，但⼿机本身⼀直没有被触碰。\nAmaninawhitet-shirtandblackshortsstandsasthecentralfigureonthecourt.Initially,heholdsabasketball,hethentransitionsintodynamicmovement.Heisnowseenactivelypracticingdribbling.Hestarted fromthebaselineofthefieldtothecenterofthefieldandthenbackintothepenaltyarea.Themanisnowpositionedonthecourt.Hestartsbyholdingthebasketball,thenhebringstheballup,andshootsittowardsthehoop.⼀位身穿白⾊T恤和⿊⾊短裤的男⼦站在球场上。⼀开始，他⼿里拿着篮球，随后开始进⾏运球动作。现在，他正投⼊地练习运球。他从球场的右底线开始，移动到中场区域，接着回到罚球区里面。他现在在空旷的球场上，面前有⼀个篮筐。他拿着篮球，接做出投篮动作，球被投⼊筐中。Figure 7: Qualitative visualization examples of PAM for region-level non-streaming and streaming\nvideo caption.\nmodels exhibit marginally lower performance compared to VideoRefer-7B and DAM-8B, indicating\npotential for further improvement.\nStreaming Video Region Caption. This task requires the model to generate continuous descrip-\ntions for a prompted region in a streaming manner. For evaluation, we primarily utilize the val-\nidation set of the ActivityNet dataset [ 7]. To ensure a fair comparison and to accurately assess\nregion-level streaming captioning capabilities, we manually curated a subset of 400 samples. This\nselection process adhered to two key criteria: (1)each annotated event within a given video is\ntemporally continuous and non-overlapping, and (2)all annotated event descriptions for that video\npertain to the same subject. Subsequently, we manually annotated bounding boxes for the tar-\nget subject in each selected video. We initially employ two standard dense captioning metrics\nfor evaluation: CIDEr and METEOR. To further assess the continuity and entity consistency\nof descriptions for sequential events, we propose a new metric: the GPT-4o-evaluated Spatio-\nTemporal Description Continuity Score (G-STDC), which ranges from 0 to 5. (Details in App. C).\n1.2–2.4×fasterLess memoryImage\nVideo10121416182022\n0500100015002000\nRuntime(s/item)\nMemoryUse(GB)\nFigure 8: Comparison of GPU memory usage and inference\nefficiency on an A6000 GPU.The results in Table 4 indicate\nthat recent region-level video cap-\ntion models, including VideoRe-\nfer and DAM, exhibit limited ca-\npability in the streaming caption\ntask. Compared to general stream-\ning caption approaches such as\nStreaming Vid2Seq, our PAM-\n3B outperforms it on the ME-\nTEOR metric. Furthermore, PAM-\n3B achieves optimal performance\non G-STDC, indicating its ex-\ncellent spatio-temporal continuity\nand ability to maintain consistent\nsubject descriptions.\n5.4 Efficiency\nAs shown in Fig. 8, compared to existing works, our PAM demonstrates superior inference efficiency\nand requires less GPU memory for both image and video processing, highlighting its suitability for\nefficient deployment in real-world applications.\n5.5 Ablations\nWe study the effectiveness of the proposed key techniques as below.\n•In Table 5, we present the impact of adjusting the number of learnable semantic tokens (sem.T). It is\nobserved that using an insufficient number of sem.T leads to a drop in performance. Conversely, using\n9\n--- Page 10 ---\nMethod LVIS RefCOCOg HC-STVG time\n(S.IoU) (METEOR) (METEOR) (ms/it)\n+ 4 78.9 26.1 22.5 972\n+ 16 79.6 26.9 23.3 980\n+ 64 80.0 27.0 23.5 1143\n+ w/o(0) 77.6 24.6 21.3 967\nTable 5: Number of Sem.T.Method LVIS RefCOCOg HC-STVG\n(S.IoU) (METEOR) (METEOR)\nAll in one 78.7 25.8 21.6\nS1→2 79.7 26.7 22.4\nS1→1.5→2 79.6 26.9 23.3\nTable 6: Different training stage.Method LVIS RefCOCOg HC-STVG\n(S.IoU) (METEOR) (METEOR)\nI.E. pre S2-FFM 78.4 25.0 21.9\nI.E. after S2-FFM 79.6 26.9 23.3\nall T. + sem.T 79.9 26.8 23.3\nmask T. + sem.T 79.6 26.9 23.3\nTable 7: Impact of different in-\ntermediate embeddings.\nan excessive number of sem.T results in diminishing gains, while also increasing the computational\ncost. Therefore, we select 16 sem.T to achieve a favorable performance-efficiency trade-off.\n•In Table 6, we compare different training strategies. It is seen that initialization from the image-\nvideo model checkpoint (from Stage 1.5) consistently leads to enhanced performance compared to\neither initializing directly from a Stage 1 model checkpoint or training directly in an all-in-one stage.\n•Table 7 compares the impact of different intermediate features from SAM 2. The results show that\nembeddings updated by S2-FFM enhance our model’s performance, which further underscore the\ncritical role of the feature selection approach.\n6 Conclusion\nWe present Perceive Anything Model (PAM), a region-level vision-language model extended from\nSAM 2, designed for simultaneous segmentation of objects while generating diverse semantic\noutputs of them across both images and videos. PAM demonstrates robust performance on multiple\nregion-level understanding tasks while achieving high computational efficiency. The simplicity and\nefficiency of our approach make it well-suitable for real-world applications, enabling a fine-grained,\nmulti-dimensional understanding of visual content from a single interaction.\n10\n--- Page 11 ---\nReferences\n[1]Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying\nChen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint\narXiv:2411.11706 , 2024.\n[2]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025.\n[3]Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summarization , pages 65–72, 2005.\n[4]Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah\nRashkin, Doug Downey, Scott Wen tau Yih, and Yejin Choi. Abductive commonsense reasoning, 2020.\n[5]Yuri Boykov and Olga Veksler. Graph cuts in vision and graphics: Theories and applications. In Handbook\nof mathematical models in computer vision , pages 79–96. Springer, 2006.\n[6]Yuqi Bu, Liuwu Li, Jiayuan Xie, Qiong Liu, Yi Cai, Qingbao Huang, and Qing Li. Scene-text oriented\nreferring expression comprehension. IEEE Transactions on Multimedia , 25:7208–7221, 2023.\n[7]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on\ncomputer vision and pattern recognition , pages 961–970, 2015.\n[8]Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P Meyer, Yuning Chai, Dennis Park, and Yong Jae\nLee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12914–12923, 2024.\n[9]Tony F Chan and Luminita A Vese. Active contours without edges. IEEE Transactions on image processing ,\n10(2):266–277, 2001.\n[10] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S. H. Gary Chan, and\nHongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal\nmodels, 2024.\n[11] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng\nGao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for\nstreaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 18407–18418, 2024.\n[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023.\n[13] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang.\nSegment and track anything. arXiv preprint arXiv:2305.06558 , 2023.\n[14] Chee Kheng Chng and Chee Seng Chan. Total-text: A comprehensive dataset for scene text detection and\nrecognition, 2017.\n[15] Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, and Elisa Ricci.\nV ocabulary-free image classification, 2024.\n[16] Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, and Tinne Tuytelaars.\nOnline action detection. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part V 14 , pages 269–284. Springer, 2016.\n[17] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: A large-scale\nbenchmark for video segmentation with motion expressions, 2023.\n[18] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation\nfrom a sentence. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n5958–5966, 2018.\n[19] Raul Gomez, Baoguang Shi, Lluis Gomez, Lukas Numann, Andreas Veit, Jiri Matas, Serge Belongie, and\nDimosthenis Karatzas. Icdar2017 robust reading challenge on coco-text. In 2017 14th IAPR International\nConference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1435–1443, 2017.\n11\n--- Page 12 ---\n[20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar,\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\npages 18995–19012, 2022.\n[21] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo,\nand Sifei Liu. Regiongpt: Towards region understanding vision language model, 2024.\n[22] Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, and Xiaoying Tang. Trace: Temporal\ngrounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643 , 2024.\n[23] Agrim Gupta, Piotr Dollár, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation,\n2019.\n[24] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural\nimages, 2016.\n[25] Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-\nChiang Frank Wang, and Ryo Hachiuma. Omni-rgpt: Unifying image and video region-level understanding\nvia token marks, 2025.\n[26] Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, and Zicheng\nLiu. Segment and caption anything. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 13405–13417, 2024.\n[27] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[28] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-\nNam Lim. Visual prompt tuning. In European conference on computer vision , pages 709–727. Springer,\n2022.\n[29] Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, and Lei Zhang.\nChatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363 ,\n2024.\n[30] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov,\nMasakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al.\nIcdar 2015 competition on robust reading. In 2015 13th international conference on document analysis\nand recognition (ICDAR) , pages 1156–1160. IEEE, 2015.\n[31] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda,\nSergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazàn Almazàn, and Lluís Pere de las\nHeras. Icdar 2013 robust reading competition. In 2013 12th International Conference on Document\nAnalysis and Recognition , pages 1484–1493, 2013.\n[32] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\nin photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP) , pages 787–798, 2014.\n[33] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything\nin high quality. Advances in Neural Information Processing Systems , 36:29914–29934, 2023.\n[34] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643 , 2023.\n[35] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan\nRom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale\nmulti-label and multi-class image classification. Dataset available from https://github. com/openimages ,\n2(3):18, 2017.\n[36] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual genome:\nConnecting language and vision using crowdsourced dense image annotations, 2016.\n12\n--- Page 13 ---\n[37] Yunhao Li, Qin Li, Hao Wang, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, and Libo Zhang. Beyond mot:\nSemantic multi-object tracking. In European Conference on Computer Vision , pages 276–293. Springer,\n2024.\n[38] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor\nDarrell, Adam Yala, et al. Describe anything: Detailed localized image and video captioning. arXiv\npreprint arXiv:2504.16072 , 2025.\n[39] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74–81, 2004.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in\ncontext, 2015.\n[41] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shang-\nhang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to\ncomprehend what you want. arXiv preprint arXiv:2403.20271 , 2024.\n[42] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 ,\n2024.\n[43] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 23592–23601,\n2023.\n[44] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi\nLin, Peng Jin, Kaipeng Zhang, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal\nlarge language models. arXiv preprint arXiv:2402.05935 , 2024.\n[45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n26296–26306, 2024.\n[46] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 , 2023.\n[47] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van\nGool. The 2017 davis challenge on video object segmentation, 2018.\n[48] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye, and\nYunjie Tian. Artemis: Towards referential understanding in complex videos. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems , 2024.\n[49] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye, and\nYunjie Tian. Artemis: Towards referential understanding in complex videos, 2024.\n[50] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, and\nDhruv Mahajan. Paco: Parts and attributes of common objects, 2023.\n[51] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan,\nHisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel\ngrounding large multimodal model, 2024.\n[52] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham\nKhedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and\nvideos. arXiv preprint arXiv:2408.00714 , 2024.\n[53] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham\nKhedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala,\nNicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment\nanything in images and videos, 2024.\n[54] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang\nChen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv\npreprint arXiv:2401.14159 , 2024.\n13\n--- Page 14 ---\n[55] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2019.\n[56] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr:\nTowards large-scale end-to-end reasoning for arbitrary-shaped scene text, 2021.\n[57] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui\nDing, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, and Lianwen Jin. Icdar 2019 competition on\nlarge-scale street view text with partial labeling – rrc-lsvt, 2019.\n[58] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu.\nHuman-centric spatio-temporal video grounding with visual transformers, 2021.\n[59] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-\ncentric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and\nSystems for Video Technology , 32(12):8238–8249, 2021.\n[60] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n4566–4575, 2015.\n[61] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and\nbenchmark for text detection and recognition in natural images, 2016.\n[62] Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level\nperception in videos via mllm, 2024.\n[63] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level\nperception in videos via mllm. In European Conference on Computer Vision , pages 166–185. Springer,\n2024.\n[64] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, XU Tang, Yao Hu, Weidi Xie, and Efstratios\nGavves. Towards open-vocabulary video instance segmentation, 2023.\n[65] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022.\n[66] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua\nLin. V3det: Vast vocabulary visual detection dataset, 2023.\n[67] Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, and Shanshan\nZhao. Caption anything: Interactive image description with diverse multimodal controls. arXiv preprint\narXiv:2305.02677 , 2023.\n[68] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao\nLi, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and\nunderstanding of the open world. arXiv preprint arXiv:2308.01907 , 2023.\n[69] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit:\nA generative region-to-text transformer for object understanding. In European Conference on Computer\nVision , pages 207–224. Springer, 2024.\n[70] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Zhe Chen, Wenhai Wang, Xizhou Zhu,\nLewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for\nhundreds of vision-language tasks. Advances in Neural Information Processing Systems , 37:69925–69975,\n2024.\n[71] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang.\nYoutube-vos: A large-scale video object segmentation benchmark, 2018.\n[72] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024.\n[73] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef\nSivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video\ncaptioning, 2023.\n14\n--- Page 15 ---\n[74] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai:\nAdapting segment anything model for zero-shot visual tracking with motion-aware memory, 2024.\n[75] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\nunleashes extraordinary visual grounding in gpt-4v, 2023.\n[76] Cong Yao, Xiang Bai, Wenyu Liu, Yi Ma, and Zhuowen Tu. Detecting texts of arbitrary orientations in\nnatural images. In 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 1083–1090,\n2012.\n[77] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu\nChang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint\narXiv:2310.07704 , 2023.\n[78] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang,\nZheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. In European\nConference on Computer Vision , pages 425–443. Springer, 2024.\n[79] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi\nFeng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of\nimages and videos, 2025.\n[80] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.\nOsprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 28202–28211, 2024.\n[81] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao,\nWenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding\nwith video llm. arXiv preprint arXiv:2501.00599 , 2024.\n[82] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon\nHong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint\narXiv:2306.14289 , 2023.\n[83] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu,\nWilliam Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and\ngrounding with large language models. arXiv preprint arXiv:2404.07973 , 2024.\n[84] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and\nPing Luo. Gpt4roi: Instruction tuning large language model on region-of-interest, 2025.\n[85] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist:\nSpatio-temporal video grounding for multi-form sentences. In CVPR , 2020.\n[86] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei\nDong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning.\narXiv preprint arXiv:2307.09474 , 2023.\n[87] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast\nsegment anything. arXiv preprint arXiv:2306.12156 , 2023.\n[88] Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, and Fang Wan. Controlcap:\nControllable region-level captioning. In European Conference on Computer Vision , pages 21–38. Springer,\n2024.\n[89] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani,\nand Cordelia Schmid. Streaming dense video captioning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 18243–18252, 2024.\n15\n--- Page 16 ---\nAppendix\nA Configuration for Each Training Stage\nTable 8 details the configurations for each training stage of the Perceive Anything Model (PAM). It\noutlines the vision parameters, dataset characteristics, model specifications, and training hyperparam-\neters throughout the curriculum learning stages. The maximum number of visual tokens varies by\ninput modality: single images are represented using 1024 tokens, while for videos, we sample up\nto 16 frames, leading to a maximum of 4864 visual tokens. A global batch size of 1024 is used for\nstages 1 and 1.5, and 256 for stage 2.\nStage 1 Stage 1.5 Stage 2\nVisual Tokens 1024 1024 + N ×256 1024 + N ×256\nMAX 1024 + 15 ×256 = 4864 MAX 1024 + 15 ×256 = 4864\nSem. Tokens 16 N ×16 (MAX 256) N ×16 (MAX 256)\nDatasetimage classification\nimage captionimage classification\nimage caption\nvideo captionimage classification\nimage explanation\nvideo caption\nstreaming video caption\nTrainable components Sem. Perceiver + Projector Sem. Perceiver + Projector Sem. Perceiver, Projector, LLM\n# 1.5B 7.6M 7.6M 1.6B\n# 3B 7.7M 7.7M 3.1B\nBatch Size 1024 1024 256\nLearning Rate 1×10−44×10−51×10−5\nEpoch 1 1 1\nWarmup Ratio 0.03 0.03 0.03\nOptimizer AdamW AdamW AdamW\nTable 8: Detailed configuration for each training stage of the PAM.\nB Dataset\n8MRegion Recognition & Explanation (35%)OpenImagesObject365LVISMSCOCOV3DetPACOSynTextLSVTReCTsICDAR13/15/17ArTTotalTextTextOCRRegionImageCaption(40%)AS-100MChatRexVGRefcocoRefcoco+RefcocogRef-TextRegionVideoCaption&Streaming(25%)VidSTGHC-STVGYoutube-VOSMevisElysiumA2DDAVIS17LV-VISSA2VAMDVP-DataOsprey-724KgRefcoco\nFigure 9: All Public Dataset Collection. Datasets highlighted in bold are selected for further\nrefinement and augmentation pipeline, aimed at generating a high-quality training dataset.\nB.1 More Details of Image Data Construction\nThis section details our image data construction process. To generate data encompassing clear\nconceptual explanations, contextual functional roles, and detailed descriptions of specific regional\ncapabilities, we primarily leveraged the extensive world knowledge of leading VLMs.\nOur approach involved several stages. Initially, we collected data from public referring object\ndetection, segmentation, and captioning datasets [ 32,6,43,36,29,68]. While these sources provide\n16\n--- Page 17 ---\nImageDAM-3BVP-SPHINX-13BPAM-3BAwhitecatsittinguprightwithitsfrontpawsslightlyapart.Thecathasablackpatchonitsleftsideofitsfaceandanotherblackpatchonitsrightsideofitsbody.Itsearsarepointedandupright,anditseyesarewideandalert.Acamelwithaslender,elongatedneckandasmall,roundedhead.Itsbodyiscoveredinasmooth,black-tintedcoat,withaslightlydarkershadealongthespine.Thecamel'slegsarelongandthin,withvisiblejointsandaslightbendattheknees.Thetailisshortandhangsdownbetweenthehindlegs.Avertical,polefeaturingasmoothsurface.Thepoleisequippedwithaseriesofevenlyspaced,horizontalbandsthatencircleit,whichcouldserveasgrippointsordecorativeelements.Atthetop,thepoleextendsintoaperpendicular,flat-toppedelementthatappearstobeacaporfixture.Thelittlecathasblackandwhiteparts.Itissittingdownuprightandlookingforward.Itseemstobeatoycat,maybemadeofplastic.Thecathassomedarkerblackspotsorpatchesonitsback,head,andonitssides.Itsearsarestickingup,likeit'slisteningThecamelisstandingstill,itsheadisheldupalittle,andithasalong,curvedneck.Thereisalargehumponitsback.Itsbodylooksstrongandsturdy.Ithaslonglegsthatareplantedontheground.Thisisacylindricalpost,appearstobemadeofwood.likelyfrombeingexposedtotheelementsovertime,givingitasomewhatroughsurface.Thepoleissetupright,stickingstraightoutoftheground.Thecathasapredominantlywhitebodywithdistinctblackpatches.Theblackpatchcoversthetopofitsheadandextendsdownitsback.Itstail,whichiscurledarounditsrightside.Thecathassmall,alertears。 Theeyesarenotclearlydetailedbutarelookingforward.Thefurtexturesuggestsasleek,shortcoat.Thecamelontheleftistallandhasasinglehump.Itssturdybodyiscoveredindarkbrownfur.Allfourofitslong,slenderlegsarevisible.Theselegsendinbroad,paddedfeet,goodforwalkingonsand.Ashorttailhangsdownatitsback.Thiscamelispartofalargergroupandseemstobemovingalongwiththem.Thepoleiswoodenandlooksabitdriedoutbythewind.Muchofitssurface,particularlythelowersection,isfaded,givingitaroughanddryappearance.Abandiswrappedaroundthispole.Thiswoodenpostisshorterthanthewhitering,whilehelpingtoholdthelargecircularstructureinplace.Figure 10: Qualitative comparison between PAM and prior models.\nunique descriptions for each region, they often lack comprehensive semantic details. Therefore,\nour work focused on refining and augmenting these data to achieve richer semantic granularity.\nSpecifically, the Set-of-Mark (SoM) prompting method [ 75] was initially employed to identify\nregions of interest within the images. To generate high-quality conceptual explanations and contextual\nfunctionalities for these identified regions, we manually refined the prompts and then utilized strong\nclosed-sourced model, GPT-4o [ 27], to produce the desired textual responses. For crafting detailed\ndescriptions of these regions, a powerful open-sourced model, Qwen2.5-VL-72B [ 2], was employed\nto expand and supplement existing textual information. Following this automated expansion, a\ntwo-stage cleaning process was implemented. First, rule-based methods were applied for preliminary\nfiltering, addressing issues such as output format inconsistencies. Subsequently, manual review\nwas conducted to identify and isolate remaining inaccurate or low-quality data, which were then\nre-annotated.\nB.2 Bilingual Annotations\nTo support bilingual (English and Chinese) output, we extended our refined datasets by generating\ncorresponding Chinese versions. For the majority of these datasets, Chinese annotations were directly\ncreated using Chinese prompts. In cases where data was initially generated with GPT-4o, the existing\nEnglish content was translated into Chinese utilizing the DeepSeek-V3 model [42].\nC Implement Details of G-STDC\nIn Sec. 5.3, we introduce the GPT-4o-evaluated Spatio-Temporal Description Continuity Score\n(G-STDC) to assess the continuity and entity consistency of descriptions for sequential events. Specif-\nically, for a given video, the predicted descriptions and the corresponding ground truth descriptions\nfor its multiple events are first sorted chronologically. Both sequences are then provided to GPT-4o\nfor evaluation. During this process, GPT-4o assesses the predicted descriptions based on temporal\ncontinuity and entity consistency (in relation to the ground truths), assigning a G-STDC score on\na scale of 0 to 5, where 5 represents the optimal score and 0 the poorest. Results are presented in\nTable 4.\n17\n--- Page 18 ---\nD More Analysis of PAM\nD.1 Performance for Background\nThisisastraightpathwithasmooth,pavedsurface,likelyconcrete.Itstartsintheforegroundandleadsofftotheright,intothedistance.Thepathitselfappearscleanandhasnolines.Theseasurfacelooksrelativelycalmoverall.It'scoveredinapatternofmanysmall,choppyripples,givingthedarkbluewateratexturedappearanceastheyreflectlight.Therearenolargewavesorsignificantturbulence.\nThissectionofgreywallhasasomewhatroughsurface,characteristicofthebuilding‘suniquefacade.Thiswallsurroundsarectangularwindowwithadarkframe.\nFigure 11: PAM can accurately describe specific background areas, such as roads, ground surfaces,\nsea, walls, the sky, and more.\nD.2 Failure Cases\nThisisasmall,sphericalcontainerholdingcoffee,anditglowswithametallicrose-redcolor.Itsglossysurfacebrightlyreflectsthelight.Theletters'E'and'L'aredelicatelyinscribedonitspolishedroundness,possiblyindicatingitishandmade.\nTheorangeslice,positionedthirdfromtheleft,hastranslucent,juicy flesh.Itsrindisasoft,lightyellow.Itrestscloselyagainsttheotherslicesonthewhiteplate,offeringagentlecontrastincolor.\nThecircularsignisredandwhite.Itslarge,roundsurfaceisprimarilyred,makingthewhiteletteringstandoutclearly.Thecentrallettersare'Tcct',withotherwhitetextcurvingaroundthesign'sedge.Thesignjutsoutfromthebuildingonasturdybracket,actingasaclearmarkerabovethesidewalk.\nFigure 12: Failure Cases of PAM in Images.\nFig. 12 illustrates several of PAM’s failure cases. As these examples show, PAM sometimes makes\nerrors in its descriptions: (1) For instance, in the first image, the orange slice is the second one, but\nPAM describes it as the third. (2) In the third image, the actual text is ’Tack’, but PAM reads it\nas ’Tcct’. (3) PAM occasionally describes elements not present in the image, such as describing\nengraved letters in the second image, which has no such features.\nAs shown in Fig. 13, PAM also exhibits certain limitations in video contexts. Specifically, if a\nuser-specified object occupies a minor portion of the frame and temporarily disappears from view,\nPAM may default to describing the most salient object in the scene instead. We attribute this behavior\nto potential biases in the training data: the GPT-assisted method employed for annotation during\ndata construction might favor describing the most salient object over the specific region, thereby\nintroducing label inaccuracies. Additionally, in streaming captioning, PAM might be influenced by\nhistorical descriptions, leading its output for the current video clip to be overly similar to that of the\npreceding clip.\nWe expect these errors to be mitigated by broader data coverage and further reinforcement training.\n18\n--- Page 19 ---\nA womanwalkingdownarelativelyemptystreetwhileholdingherphone.Sheiswearingabluejacketandappearstobetakingaselfieorrecordingavideowithherphone,asherheadturnsfromtimetotime.Herexpressionisquitecalm.\nFigure 13: Failure Cases of PAM in Videos.\nD.3 Performance on Long Videos\nPAM’s performance on long videos depends on the number of video frames processed. A larger\nnumber of frames enables PAM to generate more information-rich descriptions, but this incurs an\nexponential increase in computational cost. With its default setting of sampling 16 frames, PAM\ncan only provide broad and coarse descriptions of the states and changes of specific subjects in long\nvideos. However, PAM features a region-level streaming captioning capability that can improve its\nhandling of long videos. This method involves segmenting a long video into multiple shorter clips;\ndescriptions are then generated for each clip sequentially and subsequently merged to create a single,\ndetailed description of the entire video.\nE Potential Limitations and Discussions\nLimited Capability for General Understanding Tasks. PAM is currently trained for a specific set\nof four region-level understanding tasks: category prediction, brief and detailed regional captioning,\nvideo captioning, and streaming region captioning. Therefore, it presently lacks support for other\ngeneral vision-language tasks, such as Visual Question Answering (VQA). However, the architecture\nand training strategy of PAM are inherently well-suited to accommodate these broader functionalities.\nLooking ahead, we plan to develop additional high-quality conversational datasets to extend PAM’s\ncapabilities to encompass both region-level image and video dialogue.\nLimitations in Real-Time Streaming Video Region Captioning. Despite being 1.2–2.4 ×faster\nthan existing models, PAM’s capability for real-time streaming video region captioning is currently\nhindered by the excessive number of visual tokens requiring processing by the LLM. In the future,\nwe aim to further identify methods for substantially reducing the number of visual tokens, with the\ngoal of achieving real-time efficiency while maintain the robust understanding performance.\n19",
  "text_length": 69023
}