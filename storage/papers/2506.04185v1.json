{
  "id": "http://arxiv.org/abs/2506.04185v1",
  "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning",
  "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.",
  "authors": [
    "Qingfei Zhao",
    "Ruobing Wang",
    "Dingling Xu",
    "Daren Zha",
    "Limin Liu"
  ],
  "published": "2025-06-04T17:29:22Z",
  "updated": "2025-06-04T17:29:22Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04185v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04185v1  [cs.CL]  4 Jun 2025R-Search: Empowering LLM Reasoning with Search\nvia Multi-Reward Reinforcement Learning\nQingfei Zhao1,2, Ruobing Wang1,2, Dingling Xu3, Daren Zha1∗, Limin Liu1\n1Institute of Information Engineering, Chinese Academy of Sciences\n2School of Cyber Security, University of Chinese Academy of Sciences\n3Beijing Normal University\n{zhaoqingfei,wangruobing,zhadaren}@iie.ac.cn\nAbstract\nLarge language models (LLMs) have notably\nprogressed in multi-step and long-chain rea-\nsoning. However, extending their reasoning\ncapabilities to encompass deep interactions\nwith search remains a non-trivial challenge, as\nmodels often fail to identify optimal reason-\ning–search interaction trajectories, resulting in\nsuboptimal responses. We propose R-Search ,\na novel reinforcement learning framework for\nReasoning– Search integration, designed to en-\nable LLMs to autonomously execute multi-\nstep reasoning with deep search interaction,\nand learn optimal reasoning–search interaction\ntrajectories via multi-reward signals, improv-\ning response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides\nthe LLM to dynamically decide when to re-\ntrieve or reason, while globally integrating key\nevidence to enhance deep knowledge interac-\ntion between reasoning and search. During RL\ntraining, R-Search provides multi-stage, multi-\ntype rewards to jointly optimize the reason-\ning–search trajectory. Experiments on seven\ndatasets show that R-Search outperforms ad-\nvanced RAG baselines by up to 32.2% (in-\ndomain) and 25.1% (out-of-domain). The code\nand data are available at https://github.\ncom/QingFei1/R-Search .\n1 Introduction\nLarge language models (LLMs) have demonstrated\nsubstantial progress across a wide range of natural\nlanguage processing (NLP) tasks, driven by their\nimpressive language understanding and reasoning\nabilities (OpenAI, 2023; Plaat et al., 2024). In\nlogic-intensive tasks (Asai and Hajishirzi, 2020;\nShao et al., 2024), state-of-the-art LLMs, exempli-\nfied by DeepSeek-R1 (DeepSeek-AI et al., 2025),\nhave demonstrated remarkable capabilities in long-\nchain and multi-step reasoning (Jaech et al., 2024).\nIn knowledge-intensive tasks (Lewis et al., 2020;\n*Corresponding authorTrivedi et al., 2023), even LLMs with strong rea-\nsoning capabilities are susceptible to generating\nhallucinated outputs (Zhang et al., 2023). This\nprimarily arises from inherent limitations in the ac-\ncuracy, timeliness, and coverage of their parametric\nknowledge. To mitigate hallucination, LLM-based\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020; Zhao et al., 2024) incorporates search\nactions before generation, enabling the LLM to\naugment its input with non-parametric knowledge\nin textual form. This allows the LLM to flexibly\naccess and integrate relevant information from ex-\nternal knowledge sources, thereby enhancing the\nreliability of downstream generation. However,\ndownstream generation often struggles to bene-\nfit from reasoning alone or one-time search when\naddressing more complex logic- and knowledge-\nintensive tasks, e.g., multi-hop question-answering\n(QA) task (Yang et al., 2018). In tackling such\ncomplex tasks, the LLM is expected to dynami-\ncally integrate external knowledge into the reason-\ning process, not only to bridge the knowledge gap\nbut to guide and deepen the reasoning trajectory.\nPrevious multi-turn RAG methods (Jeong et al.,\n2024; Trivedi et al., 2023) enable the integration\nof external knowledge into reasoning by prompt-\ning LLMs to iteratively perform reasoning–search\ninteractions. In this process, retrieved informa-\ntion enhances the model’s reasoning, which in turn\nguides the subsequent retrieval, forming a dynamic\nloop between them. However, these methods typi-\ncally rely on the LLM’s internal cognition to decide\nwhen and what to retrieve, leading to two main lim-\nitations: 1)The retrieval timing determined by the\nmodel’s internal knowledge distribution does not al-\nways align with the actual need for retrieval; 2)the\nmodular and decoupled design of reasoning and\nsearch limits deep interaction of external knowl-\nedge into the reasoning chain. As a result, the\nmodel often makes decisions based only on partial\ninformation from previous searches or thoughts.\n1\n--- Page 2 ---\nThese limitations lead to suboptimal or even in-\ncorrect reasoning–search trajectories, ultimately\nreducing the quality of the final outputs.\nTo this end, we propose R-Search, a novel re-\ninforcement learning (RL)-based framework that\nenables LLMs to dynamically interleave multi-step\nreasoning and search, and to learn optimal reason-\ning–search trajectories through multi-reward sig-\nnals. First , R-Search allows the LLM to trigger\nretrieval at any token-level reasoning step, seam-\nlessly integrating retrieved content into the reason-\ning process for deeper coupling between reasoning\nand external knowledge. After the interaction, the\nLLM distills retrieved documents into evidence\nthrough reasoning. This facilitates the LLM in re-\nevaluating and structuring critical knowledge from\na global perspective, thereby enhancing its focus\non the facts most pertinent to solving the task. Sec-\nond, we design a multi-stage, multi-type reward\nmechanism that incorporates answer quality, ev-\nidence quality, and format correctness as reward\nsignals. These complementary signals promote the\nmodel to learn the optimal reasoning–search inter-\naction sequence. In particular, the evidence reward\nencourages the model to focus on the factual qual-\nity of key intermediate reasoning steps, promoting\nmore robust reasoning paths and reducing the risk\nof shortcut-driven or speculative behavior.\nOur contributions are threefold: 1) Framework\nDesign: we propose R-Search, a novel RL-based\nframework that jointly optimizes complex rea-\nsoning–search trajectories in RAG. R-Search pro-\nmotes robust policy learning by interleaving multi-\nstep reasoning with dynamic search and optimiz-\ning through multi-reward modeling. It effectively\nguides the LLM to ensure both the soundness of\nintermediate reasoning and the completeness of re-\ntrieved knowledge. 2) Superior Performance and\nInsightful Analysis: we conduct extensive experi-\nments on seven datasets across both multi-hop and\nsingle-hop QA tasks, demonstrating the superiority\nof R-Search over vanilla and advanced RAG base-\nlines (up to 32.2%). Further analyses—including\nablation and training dynamics—validate the effec-\ntiveness of evidence integration and multi-reward\nmodeling, and provide insights into performance\ntrends and retrieval behaviors under different RL\nalgorithms. 3) R-Search-as-a-Tool: We propose\nR-Search-as-a-Tool (RSTool), which modularizes\nhigh-quality evidence in reasoning into transferable\ncomponents, enabling the offloading of complex\nand costly reasoning–search interactions to localdeployments, with strong practical scalability.\n2 Related Work\n2.1 Retrieval-Augmented Generation (RAG)\nRAG typically follows a retrieve-and-generate\nparadigm (Lewis et al., 2020; Guan et al., 2025).\nThe retrieval corresponds to the Search action,\nwhich involves acquiring external non-parametric\nknowledge from various sources using different\nsearch tools. The generation refers to the tokens\nproduced by LLM reasoning, encompassing both\nthe intermediate reasoning process and the exe-\ncution of specific modules in modular RAG, in-\ncluding the final answer generation (Gao et al.,\n2023). Recently, LLM-based RAG systems (Ram\net al., 2023; Yoran et al., 2024) have demonstrated\nsignificant performance gains across various NLP\ntasks (Brown et al., 2020; OpenAI, 2023; Plaat\net al., 2024), especially in open-domain question\nanswering. To produce higher quality responses,\nsome branching RAG (Kim et al., 2024; Shi et al.,\n2024) methods summarize the retrieved documents\nseparately for multiple candidate responses to im-\nprove understanding of external knowledge. How-\never, as tasks involving more complex reasoning\n(e.g. multi-hop QA tasks (Yang et al., 2018; Ho\net al., 2020; Trivedi et al., 2022; Press et al., 2023))\nhave emerged, traditional RAG methods (Guu et al.,\n2020; Sachan et al., 2021) struggle with insufficient\nexternal knowledge integration. Several advanced\nRAG methods (Jeong et al., 2024; Jiang et al., 2023;\nCheng et al., 2024; Trivedi et al., 2023; Shao et al.,\n2023; Chan et al., 2024; Asai et al., 2024) have\nattempted multi-turn reasoning–search interactions\nto enable deeper knowledge exploration, includ-\ning multi-step (Trivedi et al., 2023; Chan et al.,\n2024; Shao et al., 2023) and adaptive RAG meth-\nods (Jeong et al., 2024; Jiang et al., 2023; Asai\net al., 2024). Nevertheless, these methods still heav-\nily rely on carefully crafted prompts, making them\ndifficult to scale and limiting the depth of inter-\naction between search and reasoning. This often\nresults in suboptimal interaction. In our work, we\naim to construct an agent-based RAG pipeline that\nsupports flexible reasoning–search interaction, and\noptimizes complex interaction trajectories via RL.\n2.2 Reinforcement Learning for RAG\nReinforcement learning (RL) (Kaelbling et al.,\n1996; Wiering and Van Otterlo, 2012) is an ef-\nfective paradigm for enhancing the reasoning ca-\n2\n--- Page 3 ---\nPolicy\nModel  KL  \nReference\nModel\nReward\nModelGroup\nComputationTrained\nModels\nFrozen\nModelsGRPO\nSearch\nTool\nRolloutSearch\nTool\nRollout\nSearch\nToolPolicy\nModelReasoning\nObservationEvidence\nAnswerMulti-Reward\nIterativeCross-Model\nAnswerDifferent\nFamily Model\nTask\nMetricFigure 1: Overview of R-Search.\npabilities of LLMs. Recent studies (Guo et al.,\n2025; Shao et al., 2024; Jaech et al., 2024) have\nshown that RL with rule-based reward functions\nenables models to acquire complex task reasoning\nand self-correction abilities (Weng et al., 2022; Ku-\nmar et al., 2024) without explicit intermediate su-\npervision. However, most existing RL approaches\nfocus mainly on internal reasoning, with limited\nintegration of external search information and in-\nsufficient handling of multi-turn interactions be-\ntween reasoning and search. Concurrent work,\nsuch as Search-R1 (Jin et al., 2025), applies RL al-\ngorithms, including Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) and Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024), to\nimprove LLMs’ autonomous search and reasoning\nabilities. Yet challenges remain in reward design\nand reasoning–search interaction. In our work, we\nfurther explore RL-based RAG methods to enhance\nLLMs’ autonomous exploration, robust reasoning,\nand deep interaction with search.\n3 R-Search\nIn this section, we introduce RL tailored for RAG\npipeline (§ 3.1) and describe the training process\n(§ 3.2) of R-Search, as illustrated in Figure 1.\n3.1 RL on RAG Paradigms\nAdvanced RAG tightly interweaves reasoning and\nsearch through several LLM-based components, en-\nabling an adaptive and active process for exploring\nexternal knowledge to solve complex questions. In\nthis process, the LLM can determine its next actionbased on its current observation of the environment,\nsuch as the reasoning trajectory and the retrieved\nexternal knowledge observed so far. The possible\nactions include initiating a new search, continuing\nthe reasoning process, or generating the final an-\nswer. We formalize this advanced RAG paradigm\nas follows:\na1⇝a2⇝···⇝aT, a t∈ {S,R},(1)\nwhere SandRdenote the search and reasoning\nactions a, respectively, and Tis the total number\nof decision time steps in the action sequence.\nSuch a complex RAG pipeline can be viewed\nas a partially observable Markov decision process\n(POMDP). Therefore, we develop a novel RL-\nbased framework that optimizes the multi-turn in-\nterleaved trajectories of reasoning and search. Dur-\ning training, we optimize the policy model by max-\nimizing the following objective:\nJ(θ) =Eq∼D,o∼πθ(·|q;S)[rϕ(q, o)]\n−βDKL[πθ(o|q;S)]∥πref(o|q;S)],(2)\nwhere πθis the policy model, qandoare the in-\nput question and generated output, and Sdenotes\nsearch mechanism. rϕrepresents the reward func-\ntion,πrefis the frozen reference model, and βis\na coefficient balancing the KL penalty. Unlike\nπθ(· |q), the policy πθ(· |q;S)executes an inter-\nleaved process of reasoning-search to generate the\nrollout sequence, denoted as Reason ▷ ◁Search .\n3\n--- Page 4 ---\n3.2 R-Search Training\n3.2.1 Rollout: evidence-augmented iterative\nreasoning and search\nTable 1 describes the system template used for\nrollout (more details in Appendix B.1). We first\nprompt the LLM to generate a long Chain-of-\nThought (CoT) based on the original question q,\nthereby constructing an explicit reasoning process.\nDuring the reasoning process, we encourage the\nmodel to trigger the search action at appropri-\nate points to acquire external non-parametric in-\nformation. Whenever the model determines that\nsearch is needed at the current reasoning state,\nit generates a new search query enclosed within\n<search> and</search> tags. We identify this\nspecific search token and feed the generated query\nq∗into a search tool to retrieve top- krelevant doc-\numents Dk=d1, d2,···, dk. These documents\nare wrapped with specific tokens <observation>\nand</observation> , and appended to the exist-\ning reasoning trajectory. The LLM then re-engages\nthe reasoning process, ultimately forming an inter-\nactive loop between reasoning and search. Next,\nwhen the LLM determines that the current state is\nsufficiently informative to produce a final answer\nα, we prompt it to rethink all previously retrieved\ninformation and derive factual evidence ethat sup-\nports question resolution. This enables the LLM to\nreason from a global perspective, leveraging all po-\ntentially relevant knowledge observed from the ex-\nternal information environment to support answer\ngeneration. Moreover, we leverage the model’s\ninternal reasoning capabilities to interpret and in-\ntegrate external knowledge during the generation\nof factual evidence. We wrap the factual evidence\nwith the special tokens box <original_evidence>\nand</original_evidence> . After the evidence\nreasoning process, the LLM continues to generate\nthe final answer α, enclosed within the specific\ntokens box <answer> and</answer> .\n3.2.2 Multi-reward modeling\nDue to the high cost and potential bias associated\nwith training reward models using human feed-\nback, we follow (Mu et al., 2024) and adopt the\nrule-based reward (RBR). Considering the comple-\nmentary effects of multiple rewards (Dann et al.,\n2023), we design multi-dimensional, multi-stage\nreward signals rϕ, including evidence rewards re\nϕ,\nanswer rewards rα\nϕ, and format rewards rf\nϕ.You are a helpful assistant that can solve the given\nquestion step by step. For each step, start by explain-\ning your thought process. If additional information is\nneeded, provide a specific query enclosed in <search>\nand</search> . The system will return the top search re-\nsults within <observation> and</observation> . You\ncan perform multiple searches as needed. When you\nknow the final answer, use <original_evidence> and\n</original_evidence> to provide all potentially rele-\nvant original information from the observations. Ensure\nthe information is complete and preserves the original\nwording without modification. If no searches were con-\nducted or observations were made, omit the evidence sec-\ntion. Finally, provide the final answer within <answer>\nand</answer> tags.\nTable 1: System template. The question is appended at\nthe end during training and inference.\nAnswer reward. Metrics such as F1-score and\nEM are widely used to evaluate the correctness of\nmodel outputs. We choose the moderately strict F1-\nscore to construct the answer reward signal. Specif-\nically, we extract the content αpredwithin <answer>\nand</answer> generated by the πθand compute\nthe F1-score against the gold answer αgold, which\nserves as the answer reward rα\nϕ.\nrα\nϕ(q, o) =F1(αpred, αgold)\n=2· |αpred∩αgold|\n|αpred|+|αgold|, (3)\nwhere |αpred∩αgold|is the number of word-level\noverlaps between the predicted and gold answers.\nEvidence reward. In RAG systems, the quality\nof evidence directly impacts answer accuracy. we\nintroduce models from a different family with dis-\ntinct policy distributions, referred to as the cross-\nfamily model πcf. We then use the frozen πcfto\nconstruct an evidence reward computation pipeline\nthat operates on the shared evidence but performs\nindependent reasoning (evidence template in Ap-\npendix B.2).\nαcf∼πcf(· |q, e),\nre\nϕ(q, o) = F1\u0000\nαcf, αgold\u0001\n(4)\nFirst, the πcfshares the same evidence ewithπθ\nand generates a Cross-Model Answer αcfbased on\nq. We then apply the same answer reward com-\nputation process to this Cross-Model Answer αcf.\nSince αcfis produced by a different family model\nthan the policy model, it facilitates the mitigation\nof answer bias introduced by the policy model’s\n4\n--- Page 5 ---\ninherent preferences. As a result, the reward sig-\nnal more objectively reflects the underlying factual\nquality of the evidence.\nFormat reward. Format reward ensures that\ngenerated content adheres to structural conven-\ntions and remains parseable for downstream\nuse (Guo et al., 2025). Specifically, we en-\nforce that the evidence appear in exactly one\nbox, marked by <original_evidence> and\n</original_evidence> . Similarly, we require\nthe final answer αto be enclosed in exactly one\n<answer> and</answer> box. We formalize the\ncalculation pipeline of the format reward rf\nϕ:\nrf\nϕ(q, o) = (1 −IS)(γe+γα·IA)\n+IS(γe·IE+γα·IA),(5)\nwhere IS,IA, andIEare indicator functions de-\nnoting whether retrieval is triggered, the answer is\nwell-formatted, and the evidence is well-formatted,\nrespectively. We present the formula for the overall\nreward rϕ.γeandγaare reward values.\nrϕ=rα\nϕ+re\nϕ+rf\nϕ(6)\ns.t.rα\nϕ, re\nϕ∈[0,1], (7)\nrf\nϕ∈ {0, γe, γa, γe+γa} (8)\n3.2.3 Mask and non-mask strategy\nMask strategy for retrieved documents. In our\nrollout sequences, we mix model-generated tokens\nwith externally retrieved documents. Treating the\nretrieved documents Dkas part of the model’s be-\nhavior for loss computation can introduce gradient\nnoise and lead to a misalignment with the intended\noptimization objective. Therefore, we introduce\na loss masking strategy that masks out the search-\nderived tokens in the observation, ensuring that the\noptimization objective is applied only to the tokens\ngenerated by the policy model during reasoning.\nNon-mask strategy for evidence. Only a lim-\nited number of factual segments can be incorpo-\nrated into the reasoning chain, resulting in insuf-\nficient utilization of external knowledge sources.\nTo address this, we apply a non-masking strategy\nto the evidence e, aiming to fully utilize external\nknowledge and enhance the model’s capability for\nknowledge integration. Specifically, in the multi-\nturn reasoning-search interaction, the evidence gen-\nerated by the model is based on the retrieved in-\nformation and produced according to the model’spolicy distribution, making it eligible for the gra-\ndient update. This strategy allows the evidence to\nparticipate in training, guiding the model to more\neffectively learn how to select, understand, and inte-\ngrate external knowledge. It enhances the model’s\nability to ground its reasoning on evidence.\n4 Experimental Setup\n4.1 Datasets & Metrics, and & Search Tools\nDatasets. We conduct extensive experiments on\nseven datasets, covering both complex multi-hop\nand simpler single-hop QA tasks. The multi-hop\nQA serves to evaluate whether R-Search can han-\ndle complex logic- and knowledge-intensive ques-\ntions. The single-hop QA assesses its ability to\naddress knowledge-intensive questions and explore\nits robustness across questions with varying lev-\nels of complexity. For multi-hop task, we adopt\nfour challenging datasets: HotpotQA (Yang et al.,\n2018), 2WikiMultiHopQA (2WikiMQA) (Ho\net al., 2020), MuSiQue (Trivedi et al., 2022),\nandBamboogle (Press et al., 2023). For single-\nhop task, we select three factoid-based QA\ndatasets, including NQ(Kwiatkowski et al., 2019),\nPopQA (Mallen et al., 2023), and TriviaQA (Joshi\net al., 2017). The dataset characteristics, versions,\nand sizes are provided in Appendix A.1.\nEvaluation Metrics. Following FLARE (Jiang\net al., 2023), we adopt two standard evaluation\nmetrics for QA tasks: F1-Score (EM) and Exact\nMatch (EM) for all datasets. EM is a more strin-\ngent metric than F1, as it measures string-level\nexact matches between the normalized prediction\nand the golden answer.\nSearch Tools. Effective search actions require\nappropriate retrieval sources and methods. We use\na dense retriever with the E5 model for all datasets.\nFor single-hop QA datasets and Bamboogle, we\nuse the 2018 Wikipedia dump as the corpus for re-\ntrieving open-domain knowledge. For the remain-\ning three multi-hop datasets, we use the Wikipedia\ncorpora version released by (Trivedi et al., 2023),\neach aligned with its corresponding dataset.\n4.2 Baselines and Backbone LLMs\nIn our experiments, we conduct comparisons across\nfive types of baselines. In Naive Generation (NG),\nwe evaluate the ability of LLMs to answer ques-\ntions using only their internal parametric knowl-\nedge. Vanilla RAG extends NG by adding a one-\n5\n--- Page 6 ---\ntime search step, forming the simplest retrieval-\nand-generation pipeline. For Branching RAG , we\nuse SuRe as the baseline. Furthermore, we com-\npare with Multi-Step RAG (MSRAG), including\nIter-Retgen (Shao et al., 2023) and IRCoT (Trivedi\net al., 2023), which continuously perform iterative\nreasoning and search actions to derive the final an-\nswer. We also compare R-Search with the recent\nadvanced RAG method, Adaptive RAG (ARAG),\nincluding FLARE (Jiang et al., 2023) and Adaptive-\nRAG (Jeong et al., 2024). Unlike MSRAG, ARAG\nleverages the reasoning capabilities of LLMs to\nactively decide when and what to retrieve, allowing\nfor a more advanced and flexible agentic reason-\ning–search interaction. In addition to ARAG, we\nalso compare with Search-R1, a concurrent method\nthat leverages RL to improve the reasoning–search\ninteraction capability of LLMs. We categorize such\nmethods as RAG+RL . For Search-R1, we align the\ntraining parameters and datasets with those used\nin our method to ensure a fair comparison. For\nthe other baseline methods, we evaluate their per-\nformance using FlashRAG (Jin et al., 2024). For\nbackbone LLMs, we train two open-source mod-\nels via RL, i.e., Qwen-2.5-3B/7B-Instruct (Yang\net al., 2024). To construct the evidence reward, we\nuse a different family open-source model, Llama-\n3.2-3B-Instruct (Meta, 2024), which follows a dif-\nferent policy distribution, to generate cross-model\nanswers based on the shared evidence.\n4.3 Implementation Details\nDuring evaluation, we align the top- kretrieval set-\nting to 5 across all methods to ensure fair compari-\nson. We also employ vLLM (Kwon et al., 2023) to\naccelerate inference. For GRPO and PPO training,\nwe use only the 2WikiMQA training set and train\non 8×A100 80GB GPUs. Primary training hyperpa-\nrameters include a maximum total of 195 training\nsteps and a batch size of 256. Following Search-R1,\nwe set the retrieval top- kduring training to 3, and\nconfigure the learning rate and warm-up ratio to\n1e-6 and 0.95, respectively. During the rollout pro-\ncess, we sample 5 responses for each input prompt,\nwith the KL divergence coefficient βset to 0.001,\nand fix both γeandγαto 0.2.\n5 Results and Analysis\n5.1 Overall Performance\nIn Table 2, we present the overall performance\nof various baselines and R-Search (case study inAppendix C).\nR-Search facilitates deep knowledge exploration.\nCompared to Vanilla RAG and LLM w/o search,\nour framework achieves up to a 37.2% improve-\nment on complex multi-hop QA tasks. It also de-\nlivers gains of up to 4.2% on simpler single-hop\ndatasets that require less reasoning. These results\ndemonstrate that our framework effectively sup-\nports deep knowledge exploration and ensures a\nstable reasoning–search interaction process.\nR-Search generalizes well to questions with both\nsimple and complex reasoning–search require-\nments. We observe that branching RAG (e.g.,\nSuRe) performs competitively on simple single-\nhop QA tasks. However, its performance drops\nsharply on multi-hop questions (e.g., MuSiQue)\nwith higher search and reasoning demands. In\ncontrast, our method excels at handling questions\nwith long reasoning chains and complex retrieval\nneeds, achieving up to a 45.2% improvement over\nthe branching RAG. These results suggest that R-\nSearch not only achieves strong performance but\nalso adapts well to questions of varying complexity.\nR-Search achieves more stable logical reasoning\nand more targeted, in-depth retrieval than ad-\nvanced RAG variants. We compare our method\nwith the multi-step and ARAG methods. While\nthese multi-turn RAG methods help mitigate the\nknowledge limitations of Vanilla RAG through\nmultiple searches, we observe performance insta-\nbility on more complex multi-hop datasets, such\nas MuSiQue. This is partly because multi-step\nRAG continuously interleaves search and reason-\ning, which can introduce irrelevant passages. Al-\nthough adaptive RAG allows the LLM to decide\nwhen to retrieve, it often suffers from mismatches\nbetween the model’s internal knowledge and ac-\ntual retrieval needs. In contrast, our method sup-\nports more effective and stable reasoning processes,\nalong with deeper search, making it better suited for\nhandling complex reasoning–search interactions.\nR-Search effectively optimizes the interaction\nbetween search and reasoning through multi-\nreward signals, enabling stronger performance\ngains on highly complex tasks. We also evaluate\nagainst a concurrent RAG+RL approach, Search-\nR1. Results show that R-Search consistently out-\nperforms Search-R1 in most cases. Notably, R-\nSearch surpasses Search-R1 by 5.6% on the highly\nchallenging MuSiQue dataset.\nImpact analysis of using different backbone\nLLMs. R-Search with the larger model gener-\n6\n--- Page 7 ---\nMethodMulti-Hop QA Single-Hop QA\nHotpotQA†2WikiMQA∗MuSiQue†Bamboogle†NQ†TriviaQA†PopQA†\nEM F1 EM F1 EM F1 EM F1 Avg. EM F1 EM F1 EM F1 Avg. Overall Avg.\nQwen-2.5-3B-Instruct\nLLM w/o Search 15.0 20.6 24.4 27.8 1.4 7.2 2.4 9.5 13.5 9.8 17.8 32.0 37.7 12.8 16.4 21.1 16.8\nVanilla RAG 34.0 43.2 33.6 38.0 5.6 11.7 9.6 19.6 24.4 37.2 46.7 58.0 66.8 39.4 46.7 49.1 35.0\nSuRe 29.2 37.8 26.8 32.4 4.2 8.9 7.2 15.6 20.3 36.4 44.8 57.0 64.4 43.2 47.2 48.8 32.5\nIter-Retgen 34.4 43.4 33.2 38.2 8.2 14.8 12.0 20.2 25.5 38.0 47.4 60.2 68.5 43.2 49.4 51.1 36.5\nIRCoT 39.0 50.4 35.8 46.0 9.2 17.6 23.2 33.3 31.8 23.4 35.4 45.8 56.5 31.6 41.5 39.0 34.9\nFLARE 14.0 20.4 24.2 27.5 1.0 6.5 3.2 9.0 13.2 9.2 16.5 32.2 38.0 12.0 15.7 20.6 16.4\nAdaptive-RAG 38.0 49.0 35.0 43.9 25.4 35.6 24.0 32.8 35.5 37.2 46.7 55.4 64.4 35.8 43.8 47.2 38.0\nSearch-R1 46.2 57.8 58.8 68.1 24.4 32.9 41.6 53.9 48.0 34.4 44.1 56.6 63.2 37.0 43.5 46.5 47.3\nR-Search (Ours) 43.4 54.4 65.0 72.6 25.8 34.8 37.6 49.8 47.9 35.2 46.0 56.0 64.0 37.0 44.9 47.2 47.6\nQwen-2.5-7B-Instruct\nLLM w/o Search 19.6 26.7 23.8 28.1 3.8 11.3 11.2 19.7 18.0 13.8 21.9 46.0 52.2 15.6 19.6 28.2 22.4\nVanilla RAG 37.4 48.1 35.4 40.5 7.2 14.6 20.8 29.7 29.2 35.0 46.7 60.0 68.5 37.6 47.6 49.2 37.8\nSuRe 33.8 43.7 25.6 32.5 6.8 13.3 17.6 29.2 25.3 42.0 50.8 60.0 69.1 45.6 50.0 52.9 37.1\nIter-Retgen 42.8 53.1 37.4 43.4 10.6 19.9 22.4 31.0 32.6 37.8 48.4 61.2 69.5 38.6 46.8 50.4 40.2\nIRCoT 40.4 53.7 34.2 45.5 9.0 17.3 20.0 32.3 31.6 19.6 35.5 55.2 66.2 33.0 43.6 42.2 36.1\nFLARE 17.8 24.8 22.6 27.5 3.6 11.4 12.0 19.4 17.4 13.4 21.6 42.0 48.7 16.0 19.8 26.9 21.5\nAdaptive-RAG 42.4 55.3 33.8 42.4 9.0 16.9 20.8 32.5 31.6 35.0 46.7 58.8 67.4 35.8 45.7 48.2 38.8\nSearch-R1 48.4 60.9 67.0 75.4 25.8 36.2 47.2 58.4 52.4 39.8 49.1 65.0 70.8 41.0 46.9 52.1 52.3\nR-Search (Ours) 52.2 64.4 69.8 77.7 31.4 41.6 42.4 57.6 54.6 38.0 49.1 64.2 71.7 41.8 48.1 52.1 53.6\nTable 2: Results (%) of overall performance. Bold and underlined values represent the highest and second-highest\nresults, respectively. †and∗indicate in-domain and out-of-domain datasets.\nally outperforms it with the smaller model, mainly\ndue to the stronger instruction-following ability of\nthe larger model and its richer internal knowledge.\nPlus, R-Search with the smaller model still outper-\nforms other RAG methods in most cases, especially\non multi-hop QA datasets. R-Search also delivers\ncompetitive performance against Search-R1.\nR-Search-as-a-Tool. The shareable evidence in\nR-Search can also serve as a pluggable compo-\nnent, easily transferred to other models for down-\nstream answer generation. We refer to this transfer-\nable functionality as R-Search-as-a-Tool ( RSTool ).\nAs shown in Figure 2, we evaluate the effective-\nness of applying the shared evidence to differ-\nent downstream generation models on two com-\nplex multi-hop datasets. We test on both a pow-\nerful black-box model, GLM-4-Plus (Zeng et al.,\n2024), and an open-source model from a different\nfamily than the policy model, Llama-3.2-3B. For\nexample, \"RSTool + GLM-4-Plus\" indicates that\nGLM-4-Plus generates answers using the shareable\nevidence generated by the trained Qwen-2.5-7B-\nInstruct model. The results show that applying\nthe shareable evidence to downstream models with\ndifferent sizes and architectures consistently im-\nproves performance, achieving significant gains\nover Vanilla. This suggests the shared evidence cap-\ntures high-quality, comprehensive knowledge effec-\ntively distilled from the prior reasoning–retrieval\ninteraction. In practical scenarios, RSTool allows\nusers to offload the high-cost reasoning–search pro-\ncess, often constrained by API token limits, to local\n2WikiMQA MuSiQue80\n70\n60\n50\n40\n30\n20\n10\n0F1 Score (%)R-Search-as-a-Tool\nRSTool+GLM4-Plus\nRSTool+Qwen2.5-7B\nRSTool+Llama3.2-3B\nVanilla (GLM4-Plus)\nVanilla (Llama-3.2-3B)Figure 2: R-Search-as-a-Tool.\nmodels, substantially reducing potential overhead.\n5.2 Ablation Study\nIn Table 3 (more results in Appendix A.2), we\nconduct ablation studies to further analyze the ef-\nfectiveness of the evidence mechanism. \"R-Search\nw/o Evidence\" refers to a variant where the evi-\ndence is removed from rollout, and the evidence-\nrelated rewards in both the evidence and format\nreward are disabled. We observe a clear perfor-\nmance drop without the evidence component and\nits associated rewards, confirming its importance.\nThis drop is especially pronounced on complex\nmulti-hop datasets compared to single-hop datasets.\nOn the one hand, during the reasoning stage, multi-\nstep search and long reasoning chains are prone to\nintroducing irrelevant information. Evidence helps\nthe LLM identify and extract the most relevant\nknowledge from a global perspective, preventing it\nfrom over-focusing on a single reasoning path and\nmissing other valuable clues. On the other hand, ev-\n7\n--- Page 8 ---\nFigure 3: Analysis of training reward and number of valid searches on Qwen-2.5-7B/3B-Instruct models. The\nsemi-transparent and the solid lines indicate raw samples and the smoothed trend.\nMethodMulti-Hop QA Single-Hop QA\nHot. 2Wiki MuSi. Bamb. NQ Tri. Pop.\nQwen2.5-3B-Instruct\nR-Search w/o Evidence 53.4 66.6 33.3 42.3 44.6 62.3 41.6\nR-Search 54.4 72.6 34.8 49.8 46.0 64.0 44.9\nQwen2.5-7B-Instruct\nR-Search w/o Evidence 61.9 77.5 39.6 55.9 47.3 70.2 48.0\nR-Search 64.4 77.7 41.6 57.6 49.1 71.7 48.1\nTable 3: Results (%) of ablation study. \"Hot.\" stands\nfor HotpotQA; other dataset names are similarly ab-\nbreviated. \"Blue\" , and \"purple\" are the highest and\nlowest values.\nMethodMulti-Hop QA Single-Hop QA\nHot. 2Wiki MuSi. Bamb. NQ Tri. Pop.\nQwen2.5-3B-Instruct\nR-Search (PPO) 52.4 60.3 33.5 50.0 43.7 63.1 44.2\nR-Search (GRPO) 54.4 72.6 34.8 49.8 46.0 64.0 44.9\nQwen2.5-7B-Instruct\nR-Search (PPO) 58.7 68.9 37.3 53.7 47.3 70.4 45.5\nR-Search (GRPO) 64.4 77.7 41.6 57.6 49.1 71.7 48.1\nTable 4: PPO vs. GRPO Performance (%).\nidence provides intermediate reward signals along\nthe long reasoning–search trajectory, guiding the\nmodel to prioritize the reliability and completeness\nof intermediate factual content, rather than relying\non speculative strategies that may yield unintended\ncorrect answers.\n5.3 Analysis\nPerformance and reward. We conduct experi-\nments using both PPO and GRPO on Qwen-2.5-3B-\nInstruct and Qwen-2.5-7B-Instruct models. Table 4\npresents the overall performance results, while Fig-\nure 3 illustrates the training dynamics and trends\nof key statistics during optimization. In terms of\nperformance, as shown in Table 4 (more results\nin Appendix A.2), both GRPO and PPO lead to\nstrong results. These findings indicate that both\nalgorithms are suitable for optimizing R-Search\nand demonstrate the general applicability of our\nframework. Furthermore, the results show that\nGRPO generally outperforms PPO, particularly onlarger models and more complex multi-hop tasks.\nA potential reason is that reward signals tend to\nbe sparse and delayed, making PPO more prone to\ngetting stuck in local optima in complex tasks. In\naddition, larger models with stronger instruction-\nfollowing capabilities can execute more effective\nreasoning–retrieval trajectories, leading to better\nperformance. During training, as shown in Figures\n3(a) and 3(b), we observe that GRPO converges\nfaster than PPO and reaches a higher reward ceil-\ning. This is because PPO relies on an actor-critic\narchitecture, where the critic is unstable and re-\nquires a warm-up phase in the early stage, easily\nintroducing noise, while GRPO bypasses the limita-\ntions of value function estimation, making it more\nlikely to achieve a higher reward ceiling. There-\nfore, we recommend using larger LLMs together\nwith GRPO when applying R-Search, as this setup\nis more likely to result in faster convergence and\nbetter final performance.\nNumber of valid searches. Figures 3(c) and\n3(d) show the learning dynamics and trends in the\nnumber of valid searches as training progresses.\nWe observe that the trained models tend to trigger\nmore retrieval steps, engaging in more rounds of\nreasoning–search interaction and enabling deeper\nexploration of external knowledge.\n6 Conclusion\nWe propose R-Search, a novel RL-based RAG\nframework that autonomously optimizes reasoning-\nsearch trajectories via multi-reward signals, seam-\nlessly integrating reasoning and search for com-\nplex problem solving. Extensive experiments on\nseven benchmarks demonstrate the superiority of R-\nSearch. Additionally, R-Search generates explicit\nevidence when invoking external search, enabling\nit to function as a modular search tool. This design\nenhances the deep reasoning-search interaction and\nallows efficient offloading of resource-intensive\nsearch processes to local models.\n8\n--- Page 9 ---\nLimitations\nTo demonstrate the strong generalization capability\nof R-Search, we only use the 2WikiMQA training\ndataset during the training phase. Although our\nmethod has achieved significant performance im-\nprovements on both in-domain and out-of-domain\ntasks under this setup, we acknowledge that in-\ncorporating more high-quality knowledge from di-\nverse domains during training may further enhance\nthe model’s performance. Therefore, future work\nshould explore integrating more diverse and high-\nquality knowledge sources to further improve the\neffectiveness of R-Search.\nReferences\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for con-\nsistent question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020 , pages 5642–5650. Association for Computa-\ntional Linguistics.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nInThe Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024 . OpenReview.net.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, and 12 others. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual .\nChi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo,\nWei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG:\nlearning to refine queries for retrieval augmented\ngeneration. CoRR , abs/2404.00610.\nQinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu,\nZhangyue Yin, Yunfan Shao, Linyang Li, Tianxi-\nang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified\nactive retrieval for retrieval augmented generation.\nInFindings of the Association for Computational\nLinguistics: EMNLP 2024, Miami, Florida, USA,\nNovember 12-16, 2024 , pages 17153–17166. Associ-\nation for Computational Linguistics.\nChristoph Dann, Yishay Mansour, and Mehryar Mohri.\n2023. Reinforcement learning can be more efficient\nwith multiple rewards. In International Conference\non Machine Learning , pages 6948–6967. PMLR.DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 81 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. CoRR ,\nabs/2501.12948.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A\nsurvey. CoRR , abs/2312.10997.\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and\nJie Zhou. 2025. Deeprag: Thinking to retrieval\nstep by step for large language models. CoRR ,\nabs/2502.01142.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint\narXiv:2501.12948 .\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR ,\nabs/2002.08909.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020 , pages 6609–6625. International Committee on\nComputational Linguistics.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney,\nAlex Iftimie, Alex Karpenko, Alex Tachard Passos,\nAlexander Neitz, Alexander Prokofiev, Alexander\nWei, Allison Tam, Ally Bennett, Ananya Kumar, and\n80 others. 2024. Openai o1 system card. CoRR ,\nabs/2412.16720.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented large language mod-\nels through question complexity. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long\nPapers), NAACL 2024, Mexico City, Mexico, June\n16-21, 2024 , pages 7036–7050. Association for Com-\nputational Linguistics.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\n9\n--- Page 10 ---\naugmented generation. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2023, Singapore, Decem-\nber 6-10, 2023 , pages 7969–7992. Association for\nComputational Linguistics.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon,\nSercan Arik, Dong Wang, Hamed Zamani, and Jiawei\nHan. 2025. Search-r1: Training llms to reason and\nleverage search engines with reinforcement learning.\narXiv preprint arXiv:2503.09516 .\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,\nand Zhicheng Dou. 2024. Flashrag: A modular\ntoolkit for efficient retrieval-augmented generation\nresearch. CoRR , abs/2405.13576.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Volume\n1: Long Papers , pages 1601–1611. Association for\nComputational Linguistics.\nLeslie Pack Kaelbling, Michael L Littman, and An-\ndrew W Moore. 1996. Reinforcement learning: A\nsurvey. Journal of artificial intelligence research ,\n4:237–285.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,\nand Jinwoo Shin. 2024. Sure: Summarizing re-\ntrievals using answer candidates for open-domain\nQA of llms. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024 . OpenReview.net.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal,\nYi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, and\n1 others. 2024. Training language models to self-\ncorrect via reinforcement learning. arXiv preprint\narXiv:2409.12917 .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics , 7:452–\n466.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-\nlez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-\nory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, SOSP 2023,\nKoblenz, Germany, October 23-26, 2023 , pages 611–\n626. ACM.Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In Advances in Neural In-\nformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual .\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023 , pages 9802–9822. Association for\nComputational Linguistics.\nMeta. 2024. Llama 3.2: Revolutionizing edge ai and\nvision with open, customizable models.\nTong Mu, Alec Helyar, Johannes Heidecke, Joshua\nAchiam, Andrea Vallone, Ian Kivlichan, Molly Lin,\nAlex Beutel, John Schulman, and Lilian Weng. 2024.\nRule based rewards for language model safety. In\nAdvances in Neural Information Processing Systems ,\nvolume 37, pages 108877–108901. Curran Asso-\nciates, Inc.\nOpenAI. 2023. GPT-4 technical report. CoRR ,\nabs/2303.08774.\nAske Plaat, Annie Wong, Suzan Verberne, Joost\nBroekens, Niki van Stein, and Thomas Bäck. 2024.\nReasoning with large language models, a survey.\nCoRR , abs/2407.11511.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, Singapore, De-\ncember 6-10, 2023 , pages 5687–5711. Association\nfor Computational Linguistics.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Trans. Assoc. Comput. Linguistics ,\n11:1316–1331.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual , pages 25968–25981.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347 .\n10\n--- Page 11 ---\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. En-\nhancing retrieval-augmented large language models\nwith iterative retrieval-generation synergy. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023 ,\npages 9248–9274. Association for Computational\nLinguistics.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, and 1 others. 2024. Deepseek-\nmath: Pushing the limits of mathematical reason-\ning in open language models. arXiv preprint\narXiv:2402.03300 .\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Richard James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2024. REPLUG: retrieval-\naugmented black-box language models. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), NAACL 2024, Mexico City, Mexico,\nJune 16-21, 2024 , pages 8371–8384. Association for\nComputational Linguistics.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics , 10:539–554.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. In Proceedings of\nthe 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023 , pages\n10014–10037. Association for Computational Lin-\nguistics.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu\nHe, Shengping Liu, Bin Sun, Kang Liu, and Jun\nZhao. 2022. Large language models are better\nreasoners with self-verification. arXiv preprint\narXiv:2212.09561 .\nMarco A Wiering and Martijn Van Otterlo. 2012. Rein-\nforcement learning. Adaptation, learning, and opti-\nmization , 12(3):729.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, and 1 others. 2024. Qwen2.\n5 technical report. arXiv preprint arXiv:2412.15115 .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018 ,\npages 2369–2380. Association for Computational\nLinguistics.Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Be-\nrant. 2024. Making retrieval-augmented language\nmodels robust to irrelevant context. In The Twelfth\nInternational Conference on Learning Representa-\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024 .\nOpenReview.net.\nAohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang,\nDa Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao,\nHanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun,\nJiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang,\nJing Zhang, Juanzi Li, Lei Zhao, and 36 others.\n2024. Chatglm: A family of large language mod-\nels from GLM-130B to GLM-4 all tools. CoRR ,\nabs/2406.12793.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023. Siren’s song\nin the AI ocean: A survey on hallucination in large\nlanguage models. CoRR , abs/2309.01219.\nQingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha,\nShicheng Tan, Yuxiao Dong, and Jie Tang. 2024.\nLongrag: A dual-perspective retrieval-augmented\ngeneration paradigm for long-context question an-\nswering. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2024, Miami, FL, USA, November 12-16,\n2024 , pages 22600–22632. Association for Computa-\ntional Linguistics.\n11\n--- Page 12 ---\nAppendix\nA Experiment and Result\nA.1 Datasets\nWe conduct extensive experiments on seven\ndatasets, covering both complex multi-hop and\nsimpler single-hop QA tasks. The multi-hop\nQA task serves to evaluate whether R-Search can\nhandle complex logic- and knowledge-intensive\nquestions. The single-hop QA task assesses\nits ability to address knowledge-intensive ques-\ntions and explore its robustness across questions\nwith varying levels of complexity. For Multi-\nhop task, we adopt four challenging multi-hop\nQA datasets: HotpotQA (Yang et al., 2018),\n2WikiMultiHopQA (2WikiMQA) (Ho et al.,\n2020), MuSiQue (Trivedi et al., 2022), and Bam-\nboogle (Press et al., 2023). The datasets require\nmodels to start from the original question and ex-\nplore a logical, knowledge-driven multi-hop reason-\ning path to answer each intermediate sub-question\nand reach the final answer. We follow the train\nand test splits released by (Trivedi et al., 2023)\nfor the first three datasets, each with 500 test sam-\nples. Bamboogle evaluation uses all 125 test sam-\nples provided by FlashRAG (Jin et al., 2024). For\nSingle-hop task, we select three factoid-based QA\ndatasets, including NQ(Kwiatkowski et al., 2019),\nPopQA (Mallen et al., 2023), and TriviaQA (Joshi\net al., 2017). These datasets require models to\ncollect specific passages and identify key factual\ninformation to answer the questions correctly. We\nuse the test sets provided by FlashRAG and ran-\ndomly sample 500 examples from each dataset for\nevaluation.\nA.2 Results\nWe present the results of all evaluation metrics for\nthe ablation study and the comparison between\nPPO and GRPO in Table 6 and Table 7. The trends\nof EM are consistent with those of F1.\nA.3 More Implementation Details\nDuring inference, we set the temperature to 0.1\nacross all models to reduce uncertainty.\nB Template\nB.1 System Template\nWe design a system template for the rollout phase\nto guide the model through the complete interac-\ntion process—from receiving the input question qto generating the final answer α. This template\ncovers four key stages: reasoning, retrieval, ev-\nidence integration, and answer generation. No-\ntably, for questions that do not require external\nknowledge, the LLM automatically determines that\nsearch is unnecessary. In such cases, the template\ninstructs the model to perform only reasoning and\nanswer generation. During training, we initialize\nthe training process with the system template and\nthe user’s question. It is worth noting that we do\nnot impose a manual separation between reason-\ning and retrieval (e.g., by encapsulating reason-\ning within a <think> and</think> tag); Instead,\nwe provide a high-level instruction that guides\nthe model to explain its thought process before\neach action, without imposing any specific format\nfor reasoning text. This stems from the fact that\nLLM generation is inherently a form of reason-\ning, where every generated token can be viewed\nas part of a thought chain for question-solving.\nFor parts such as <search> ,<observation> ,\n<original_evidence> , and <answer> , which are\ndistinct from general thought reasoning, we intro-\nduce specific token tags to mark them, allowing the\nLLM to recognize the boundaries between different\nfunctional segments.\nB.2 Evidence Template\nTable 5 presents the instruction template used for\ngenerating evidence during rollout. Before out-\nputting the final answer α, we instruct the LLM to\ngenerate evidence based on the original question\nqand all previously retrieved texts (provided in\nthe<observation> and</observation> ). This\nevidence helps the LLM rethink the retrieved infor-\nmation from a global perspective and focus on key\nfactual knowledge. By seamlessly integrating the\nevidence into the reasoning process, we facilitate a\ndeeper interaction between reasoning and retrieved\nknowledge.\nC Case Study\nTables 8 and 9 show examples of the reasoning pro-\ncess from RL-trained models on 2-hop and more\ncomplex 4-hop questions. In these cases, R-Search\nuses multi-stage, multi-type rewards to improve\nthe reasoning–retrieval process. The model usu-\nally starts by generating a general reasoning plan,\nretrieves information when needed, and uses in-\ntermediate conclusions to guide the next retrieval.\nThe evidence generated through this interaction is\n12\n--- Page 13 ---\nclear, well-structured, and informative, making it\neasy to transfer to downstream models for final\nanswer generation.\n13\n--- Page 14 ---\nAnswer the question based on the given passages. Only give me the answer and do\nnot output any other words.\nThe following are given passages: { evidence }\nQuestion: { query }\nAnswer:\nTable 5: Evidence template.\nMethodMulti-Hop QA Single-Hop QA\nHotpotQA 2WikiMQA MuSiQue Bamboogle NQ TriviaQA PopQA\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nQwen-2.5-3B-Instruct\nR-Search w/o Evidence 41.0 53.4 58.2 66.6 23.6 33.3 32.0 42.3 34.6 44.6 54.4 62.3 36.0 41.6\nR-Search 43.4 54.4 65.0 72.6 25.8 34.8 37.6 49.8 35.2 46.0 56.0 64.0 37.0 44.9\nQwen-2.5-7B-Instruct\nR-Search w/o Evidence 49.4 61.9 69.0 77.5 29.4 39.6 44.0 55.9 36.8 47.3 63.6 70.2 41.4 48.0\nR-Search 52.2 64.4 69.8 77.7 31.4 41.6 42.4 57.6 38.0 49.1 64.2 71.7 41.8 48.1\nTable 6: Results (%) of ablation study. \"Blue\" , and \"purple\" are the highest and lowest values.\nMethodMulti-Hop QA Single-Hop QA\nHotpotQA 2WikiMQA MuSiQue Bamboogle NQ TriviaQA PopQA\nEM F1 EM F1 EM F1 EM F1 Avg. EM F1 EM F1 EM F1 Avg.\nQwen-2.5-3B-Instruct\nR-Search (PPO) 42.0 52.42 50.8 60.3 24.0 33.5 37.6 50.0 43.8 34.0 43.7 54.6 63.1 36.8 44.2 46.1\nR-Search (GRPO) 43.4 54.4 65.0 72.6 25.8 34.8 37.6 49.8 47.9 35.2 46.0 56.0 64.0 37.0 44.9 47.2\nQwen-2.5-7B-Instruct\nR-Search (PPO) 47.0 58.7 59.8 68.9 27.0 37.3 40.0 53.7 49.1 37.0 47.3 63.2 70.4 37.8 45.5 50.2\nR-Search (GRPO) 52.2 64.4 69.8 77.7 31.4 41.6 42.4 57.6 54.6 38.0 49.1 64.2 71.7 41.8 48.1 52.1\nTable 7: PPO vs. GRPO: Performance (%) comparison.\n14\n--- Page 15 ---\nQuestion: When was countrywide bought by the company that bought FleetBoston Financial?\n\"Thought\":\" To answer this question, we need to follow these steps:\n1. Identify the company that bought FleetBoston Financial.\n2. Determine when this company bought Countrywide.\nLet’s start with step 1.\n<search> FleetBoston Financial was bought by whom? </search>\n<observation> (Title: \"Bank of America\") In2004, Bank ofAmer icaannounced itwould purchase Boston -based\nbank Fleet Boston Financial for $47 billion in cash and stock. By merging with Bank of America, all of its banks and\nbranches were given the Bank of America logo. At the time of merger, FleetBoston was the seventh largest bank in\nUnited States with $197 billion in assets, over 20 million customers and revenue of $12 billion.\n(Title: \"Boston\") Boston is an intellectual, technological, and political center but has lost some important regional\ninstitutions . . .\n(Title: \"Boston\") Other important industries are financial services . . .\n(Title: \"The Ritz-Carlton Hotel Company\") Blackstone Real Estate Acquisitions of New York bought The Ritz . . .\n(Title: \"RBC Bank\") On June 19, 2011, PNC Financial Services agreed to buy RBC Bank USA from Royal Bank of\nCanada for $3.45 billion . . .<observation>\nFrom the observations, we can conclude that FleetBoston Financial was bought by Bank of America in 2004.\nNow, let’s move to step 2.\n<search> When did Bank of America buy Countrywide? </search>\n<observation> (Title: \"Bank of America Home Loans\") On January 11, 2008, Bank of America announced that it\nplanned to purchase Countrywide Financial for $4.1 billion in stock. On June 5, 2008, Bank of America Corporation\nannounced it had received approval from the Board of Governors of the Federal Reserve System to purchase Countrywide\nFinancial Corporation. Then, on June 25, 2008, Countrywide announced it had received the approval of 69% of its\nshareholders to the planned merger with Bank of America. OnJuly 1,2008, Bank ofAmer icaCorporationcompleted its\npurchase ofCoun trywide FinancialCorporation. In 1997, Countrywide had spun off Countrywide Mortgage Investment\nas an independent company called IndyMac Bank. Federal regulators seized IndyMac on July 11, 2008, after a week -\nlong bank run . . .<observation>\n(Title: \"Bank of America Home Loans\") Bank of America Home Loans is the mortgage unit of Bank of America . . .\n(Title: \"Bank of America\") In 2004, Bank of America announced it would purchase Boston - based bank FleetBoston\nFinancial for $47 billion in cash and stock . . .\n(Title: \"Bank of America\") Its acquisition of investment bank Merrill Lynch in 2008 solidified it as a “Bulge Bracket\nbank ” . . .\n(Title: \"General Tire\") GenCorp sold General Tire to German tire maker Continental AG in 1987 . . .\nFrom the observations, we can conclude that Bank of America bought Countrywide on July 1, 2008.\n<original_evidence> - FleetBoston Financial was bought by Bank of America in 2004.\n- Bank of America bought Countrywide on July 1, 2008. <original_evidence>\n<answer> July 1, 2008 </answer> \"✓\"\nTable 8: An 2-hop example of R-Search on MuSiQue dataset using PPO. Orange segments denote those used to\nsupport correct answers. The \" ✓\" indicates a correct answer.\n15\n--- Page 16 ---\nQuestion: Which film whose director is younger, My Baby’S Daddy or A Tale Of Winter?\n\"Thought\":\" To determine which film has a director who is younger, we need to identify the directors of both films and\ncompare their ages.\nFirst, let’s identify the director of \"My Baby’s Daddy\".\n<search> Who directed My Baby’s Daddy? </search>\n<observation> (Title: \"My Baby’s Daddy\") MyBaby’s Daddy isa2004 Amer icancomedyfilm, directed byCheryl\nDunye.\n(Title: \"Alec Mapa: Baby Daddy\") Alec Mapa: Baby Daddy is a 2013 one- man show by Alec Mapa . . .\n(Title: \"Bachelor Daddy\") Bachelor Daddy is a 1941 American comedy film directed by Harold Young and written by\nRobert Lees and Frederic I. Rinaldo . . .\n(Title: \"Baby and I\") Baby and I is a 2008 South Korean film about a rebellious 18- year- old high school senior who\nends up caring for a baby with a note . . .\n(Title: \"Who’s Your Daddy? (film)\") Who’s Your Daddy? is a 2002 American comedy film directed( and co-scripted) by\nAndy Fickman. <observation>\nThe director of \"My Baby’s Daddy\" is Cheryl Dunye.\nNext, let’s identify the director of \"A Tale Of Winter\".\n<search> Who directed A Tale Of Winter? </search>\n<observation> (Title: \"A Tale of Winter\") ATale ofWinterisa1992 French drama filmdirected byEric Rohmer, and\nstarring Charlotte Very, Frederic van den Driessche and Michael V oletti. It is the second of Rohmer’s \"Tales of the Four\nSeasonsContes des quatre saisons\"), which also include \"A Tale of Springtime\" (1990), \"A Summer’s Tale\" (1996) and\n\"Autumn Tale\" (1998) . . .\n(Title: \"A Tale of Winter\") ATale ofWinterisa1992 French drama filmdirected byEric Rohmer, and starring Charlotte\nVery, Frederic van den Driessche and Michael V oletti. It is the second of Rohmer’s\" Tales of the Four SeasonsContes des\nquatre saisons\") . . .\n. . .<observation>\nThe director of \"A Tale of Winter\" is Eric Rohmer.\nNow, we need to determine the ages of Cheryl Dunye and Eric Rohmer to compare.\n<search> When was Cheryl Dunye born? </search>\n<observation> (Title: \"Cheryl Dunye\") Cheryl Dunye( born May 13,1966) isaLiberian -Amer icanfilm director,\nproducer, screen writer, editorandactress. Dunye’s work often concerns themes of race, sexuality, and gender, particularly\nissues relating to black lesbians. . . .\n. . .\nCheryl Dunye was born on May 13, 1966.\n<search> When was Eric Rohmer born? </search>\n<observation> (Title: \"Eric Rohmer\") Jean Marie Maurice Scherer or Maurice Henri Joseph Scherer, known as Eric\nRohmer (21March 192011 January 2010), was a French film director, film critic, journalist, novelist, screenwriter, and\nteacher. Rohmer was the last of the post-World War II French New Wave directors to become established. He edited the\ninfluential film journal, \"Cahiers du cinema\", from 1957 to 1963, while most of his . . .\n(Title: \"Eric Rohmer filmography\") This is a list of films by the French director Eric Rohmer.\n. . .<observation>\nEric Rohmer was born on March 21, 1920.\nNow we compare the birth years:\n- Cheryl Dunye: 1966\n- Eric Rohmer: 1920\nCheryl Dunye is younger than Eric Rohmer.\nTherefore, the film \"My Baby’s Daddy\" has a younger director.\n<original_evidence> - \"My Baby’s Daddy\" was directed by Cheryl Dunye, born in 1966.\n- \"A Tale of Winter\" was directed by Eric Rohmer, born in 1920.\n- Cheryl Dunye is younger than Eric Rohmer. <original_evidence>\n<answer> My Baby’S Daddy </answer> \"✓\"\nTable 9: An 4-hop example of R-Search on 2WikiMQA dataset using GRPO. Orange segments denote those\nused to support correct answers. The \" ✓\" indicates a correct answer.\n16",
  "text_length": 61404
}