{
  "id": "http://arxiv.org/abs/2506.04058v1",
  "title": "Towards generating more interpretable counterfactuals via concept\n  vectors: a preliminary study on chest X-rays",
  "summary": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge.",
  "authors": [
    "Bulat Maksudov",
    "Kathleen Curran",
    "Alessandra Mileo"
  ],
  "published": "2025-06-04T15:23:12Z",
  "updated": "2025-06-04T15:23:12Z",
  "categories": [
    "eess.IV",
    "cs.AI",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04058v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04058v1  [eess.IV]  4 Jun 2025Preprint 1–12\nTowards generating more interpretable counterfactuals via\nconcept vectors: a preliminary study on chest X-rays\nBulat Maksudov1bulat.maksudov2@mail.dcu.ie\nKathleen Curran2\nAlessandra Mileo1\n1School of Computing, Dublin City University, Dublin, Ireland\n2School of Medicine, University College Dublin, Dublin, Ireland\nAbstract\nAn essential step in deploying medical imaging models in real-world settings is ensuring\nthat these models align with established clinical knowledge and produce outputs that\nare interpretable by clinicians. Recent research has demonstrated that the latent spaces\nof pre-trained models contain directions corresponding to high-level concepts. In this\nwork, we focus on mapping clinical concepts into the latent space of generative models to\nidentify corresponding Concept Activation Vectors (CAVs). Using a simple reconstruction\nautoencoder, we show that user-defined concepts can be linked to image-level features\nwithout requiring explicit training on class labels. We demonstrate that the concepts\nextracted using CAVs are stable across datasets. This indicates that in principle they can be\nused to generate visual explanations that highlight clinically relevant features. Our method\nproduces counterfactual explanations by traversing the latent space in the direction of\nconcept vector, allowing for the exaggeration or curtailment of specific clinical features. Our\npreliminary study on chest X-ray data shows that while the method performs well for larger\npathologies such as cardiomegaly, challenges remain for smaller pathologies like atelectasis\ndue to limitations in reconstruction fidelity and feature localization. Our investigation,\nalthough preliminary, demonstrates potential for producing interpretable, concept-based\nexplanations in medical imaging, which is a key step towards improving trust in AI-driven\ndiagnostics and foster adoption. Despite the approach does not beat the baseline in its\ncurrent form, we believe that finding clinically relevant concepts in the latent space of pre-\ntrained models provides a pathway for generating visual explanations that align with clinical\nknowledge. We envision taking this approach further by improving the underlying generative\nmodel and going beyond single vector by modeling concept subspace as a distribution.1\nKeywords: Interpretability, Concept Activation Vectors (CAVs), Autoencoders, Counter-\nfactual Explanations, Visual Explanations, Explainable AI (XAI), Feature Attribution\n1. Introduction\nInterpretability and explainability are important aspects for the adoption of deep learning\nmodels, particularly in the healthcare domain. In medical imaging, model predictions are\ndirectly related to patient treatment and outcomes, so the ability to understand and trust\na model’s predictions is not just a technical challenge, but a clinical necessity. Although\nrecent models achieve clinician-level performance on disease classification and localization,\ntheir ”black-box” nature limits widespread adoption into hospital workflows. Clinicians are\n1.The code to reproduce experiments for this paper is available here https://github.com/Luab/cavisual/\n©CC-BY 4.0, B. Maksudov, K. Curran & A. Mileo.\n--- Page 2 ---\nMaksudov Curran Mileo\noften reluctant to rely on models whose decision-making processes they cannot interpret\nor validate against their own expertise. This lack of trust poses a significant barrier to the\nintegration of AI into routine medical practice.\nSeveral methods have been introduced to make the the model predictions more inter-\npretable. One such approach is to generate saliency maps using backpropagation-based\nmethods such as GradCAM (Selvaraju et al., 2017). However, recent work has shown that\nsome of these methods are not indicative of semantically significant features (Viviano et al.,\n2019) or simply correspond to edge detection models (Adebayo et al., 2018).\nAnother recent direction is to generate explanations by traversing the latent space of\ngenerative models. For example (Atad et al., 2022) introduce the method to traverse\nlatent space of specifically pre-trained GANs. (Cohen et al., 2021) propose a method that\nuses a simple autoencoder to transform the latent representation of a specific input image,\nexaggerating or curtailing the features used for prediction. This method decouples the\ngenerative and classification models, so that the generative model trained without prior\nknowledge of clinical concepts is driven by the classification model gradient. Building on\nthis, we explore Concept Activation Vectors (CAVs) as another way to integrate clinical\nknowledge into latent space traversal methods. CAVs, originally introduced by (Kim et al.,\n2018), allow us to define directions in the latent space that correspond to user-defined\nconcepts, enabling the generation of visual explanations that highlight clinically relevant\nfeatures. Importantly, the autoencoder itself is trained without explicit knowledge of these\nconcepts and concept information is introduced only via CAVs.\nIn this work, we present a first attempt at applying latent space traversal with CAVs\nto generate visual explanations that are both interpretable and clinically meaningful. We\nvalidate the basic principles of our approach, showing that concept vectors can capture\nmeaningful information about target concepts and that these vectors exhibit stability across\ndifferent datasets. The contributions of this paper are as follows:\n•We demonstrate that user-defined concepts can be mapped to image-level features\nusing the latent space of a generative model, even when the model is trained without\nexplicit knowledge of these concepts.\n•We propose a method for generating visual explanations by traversing the latent space\nwith CAVs\n•We show that concept vectors generated from different datasets exhibit similarity and\nstability, suggesting that they capture consistent and generalizable information about\nthe target concepts.\n2. Related work\nThe use of latent space traversal for interpretability has been explored in medical imaging.\nAtad et al. (2022) generated counterfactual explanations by manipulating latent directions in a\ntrained model, while Cohen et al. (2021) proposed Latent Shift, a method using autoencoders\n2\n--- Page 3 ---\nTowards generating more interpretable counterfactuals via concept vectors\nto transform latent representations for feature manipulation in chest X-rays. Diffusion-based\napproaches to counterfactual generation, such as those introduced by (Augustin et al., 2022),\nleverage denoising models guided by classifier gradients to generate counterfactual images.\nUnlike linear latent space traversal methods, latent diffusion models allow for non-linear\ntrajectories in the latent space, enabling more complex and high-quality image generation.\nHowever, these methods are inherently more complex, and their guidance process is less\ninterpretable compared to linear traversals.\nConcept Activation Vectors (CAVs), introduced by Kim et al. (2018), have been widely\nused to interpret classification models by measuring the influence of user-defined concepts on\nmodel predictions.. Recent work in large language models (LLMs) has also explored similar\nideas, such as In-Context Vectors (ICVs)(Liu et al., 2024), which steer model behavior by\nshifting latent representations based on demonstration examples. These methods highlight\nthe broader applicability of concept-based explanations across different domains.\nBuilding on these advances, our work extends the idea of latent space traversal by\nshowing that prior knowledge about the visual representation of concepts can be discovered\ndirectly in the latent space of a reconstruction autoencoder, without requiring explicit\ntraining using class information. By leveraging CAVs, we map user-defined concepts to\nmeaningful directions in the latent space, enabling the generation of visual explanations\nthat highlight clinically relevant features. This approach not only simplifies the process of\ngenerating interpretable explanations but also aligns with recent developments in latent\nspace manipulation, underscoring the potential of concept-based explanations as a unifying\nframework for interpretability.\n3. Methodology\nOur method uses two stage approach to generating visual counterfactuals. A detailed\noverview is provided in Figure 1.\nCAV generation Firstly, we construct Concept Activation Vectors in the latent space of\nan autoencoder model, using linear classifier.\nCAV-guided traversal and counterfactual generation Using CAVs we generate\nexaggerated and curtailed latent samples by adding and substracting CAV respectively.\nAfter that, we decode new samples and subtract them to produce visual counterfactual.\n3.1. Concept vector generation\nWe generate concept vectors using the Concept Activation Vector (CAV) algorithm proposed\nby (Kim et al., 2018), which trains a linear classifier in the embedding space to identify\ndirections corresponding to user-defined concepts. Given a set of positive concept samples\nPcand a negative set N, we train a linear classifier in the flattened embedding space of\nthe autoencoder model. this work, we use class labels as proxies for the concepts. Specifi-\ncally, given a set of positive concept samples Pc(studies labeled with the target class, e.g.,\n”Cardiomegaly”) and a negative set N(studies without the target class label), the classifier\nlearns to separate the two groups in the latent space. The decision boundary of this classifier\ndefines a hyperplane, and the vector orthogonal to this hyperplane represents the direction\n3\n--- Page 4 ---\nMaksudov Curran Mileo\nFigure 1: Overview of our method: (a) Construction of Concept Activation Vector in the\nlatent space of autoencoder using linear classifier (b) Generation of explanations\nby traversing the latent space using CAV. Visual explanation is constructed by\npixelwise subtracting exaggerated and curtailed samples\nin the latent space that best captures the target concept. This orthogonal vector is referred\nto as the Concept Activation Vector (CAV).\nAn overview of concept vector generation is provided in Figure 1.a\n3.2. Visual explanations\nTo generate visual explanations we follow the methodology of (Cohen et al., 2021). A sample\nis encoded into the latent space of the autoencoder model, and then we generate feature\nexaggeration and curtail it by adding and subtracting the concept vector from the embedding\nand generating reconstruction for it. An example of the resulting explanation is provided in\nFigure 2.\nTo generate 2D attribution we compute the maximum absolute difference between traversed\nreconstructions and the original image. An example of the resulting 2D successful and failed\nattribution maps is shown in Appendix B, Figure 3 and Figure 4 respectively.\nAn overview of visual explanation generation is provided in Figure 1.b.\n4. Evaluation\n4.1. Experiments\nTo establish that concept vectors meaningfully contain information about target concepts\nwe conduct two main experiments:\n•Measure the similarity of concept vectors in inter and intra dataset setting\n•Asses the quality of generated explanations and their correspondence to expected\nvisual features of the concept\nStability of concept vectors To assess the stability of concept vectors we performed\nthree sets of experiments. Firstly, we compute the similarity of concept vectors with random\nvectors. This serves as a basic sanity check, confirming that the generated vectors capture\n4\n--- Page 5 ---\nTowards generating more interpretable counterfactuals via concept vectors\nFigure 2: Visual representation of feature manipulation for the ”Cardiomegaly” concept.\nBy traversing the latent space of the autoencoder, we add or subtract the concept\nvector from the image embedding to generate reconstructions that exaggerate\nor curtail cardiomegaly-related features. From left to right: (a) Feature curtail,\nreducing the heart area; (b) Original chest X-ray; (c) Feature exaggeration,\nenhancing cardiomegaly-related features.\nmeaningful information about the concept rather than random noise. Secondly, we evaluate\ninter-dataset similarity by computing the cosine similarity of concept vectors generated\nfrom different batches of the dataset. Lastly, we evaluate the intra-dataset similarity of\nconcept vectors, which shows that concept direction captures relevant information, rather\nthan dataset-specific properties.\nTo measure the alignment of concept vectors, we use cosine similarity, which captures\nthe angular relationship between vectors in high-dimensional space, independent of their\nmagnitude. This metric is particularly well-suited for our setting because we are interested\nin the direction of the concept vectors in the latent space rather than their absolute values.\nA cosine similarity close to 1 indicates that the vectors are pointing in the same direction,\nwhile a value close to 0 suggests no alignment. By using cosine similarity, we can quantify\nthe consistency of concept vectors across datasets and batches, providing a robust measure\nof their stability and generalizability.\nEvaluating quality of visual explanations To assess quantitative metrics of visual\nexplanations we first generate 2D attribution maps by computing the absolute difference\nbetween exaggerated counterfactual and original images. We then follow the evaluation\nprotocol described in (Cohen et al., 2021) to compute Intersection over Union (IoU) between\nattribution maps and bounding boxes of the target concept. In out case, Intersection over\nUnion provides a measure, with a value between 0 and 1, of similarity between ground truth\nsegmentation mask and visual explanation, with higher similarity resulting in larger IoU\nvalues\n4.2. Experimental setup\nCAV generation For the concept vectors calculation we construct batches with 500\nsamples, 250 of which contain the target concept and 250 are negative.\nVisual explanations We use pre-trained autoencoder models from torchxrayvision (Cohen\net al., 2022). The step size of 10 is used for latent space traversal. We generate explanations\n5\n--- Page 6 ---\nMaksudov Curran Mileo\nTable 1: Similarity matrix of concept vectors for ”Atelectasis,” comparing intra-dataset,\ninter-dataset, and random vector similarities. Concept vectors are distinct from\nrandom vectors, and averaging improves alignment across datasets, demonstrating\nstability and robustness in capturing clinical concepts.\nNIH CheXpert NIH mean CheXpert mean\nNIH 0 .2651±0.2447 0 .1011±0.0757 0 .5146±0.0668 0 .2116±0.0674\nCheXpert 0 .1011±0.0757 0 .2178±0.2582 0 .1966±0.0726 0 .4666±0.0558\nNIH mean 0 .5146±0.0668 0 .1966±0.0726 0 .9734±0.0088 0 .4060±0.0183\nCheXpert mean 0 .2116±0.0674 0 .4666±0.0558 0 .4060±0.0183 0 .9655±0.0114\nRandom vector −0.0006±0.0323 0 .0010±0.0321 −0.0205±0.0266 −0.018±0.0326\nusing 4 sets of concept vectors for each dataset and then average those vectors and evaluate\nattribution quality for the averaged vector.\n4.3. Datasets\nFor the experiments, we utilize two Chest X-ray datasets: NIH(Wang et al., 2017) and\nCheXpert(Irvin et al., 2019). We limit the number of pathologies that are examined to\n5, namely ”Cardiomegaly”, ”Mass”, ”Nodule”, ”Atelectasis” and ”Effusion”. For concept\nvector generalization experiments we consider pathologies from this list that are present\nin the CheXpert dataset, namely ”Cardiomegaly”, ”Atelectasis”, and ”Effusion”. The\ndetailed number of samples for each dataset, as well as the subset of samples for which mask\nannotations were available, are present in Appendix A, Table 5\nNIH (Wang et al., 2017) This dataset comprises 108,948 frontal view X-ray images of\n32,717 unique patients with the text-mined eight disease image labels, from the associated\nradiological reports using natural language processing.\nCheXpert (Irvin et al., 2019) It contains 224,316 chest radiographs of 65,240 patients. It\nis labeled by automatically extracting 14 pathologies from radiological reports. Additionally,\na validation set of 200 images was labeled by 3 board-certified radiologists and 500 test\nimages were annotated by consensus of 5 board-certified radiologists.\n5. Results and Discussion\n5.1. Generalization of Concept vectors\nThe mean and std of concept vectors similarity is provided in Table 1. Based on our\nexperimental data we observe the following:\nFirst, concept vectors are distinct from random vectors, with cosine similarity scores close\n6\n--- Page 7 ---\nTowards generating more interpretable counterfactuals via concept vectors\nTable 2: Intersection over Union (IoU) comparison for overlapping pathologies (Atelectasis,\nCardiomegaly, Effusion) using CheXpert and NIH concept vectors. Averaging\nconcept vectors improves explanation quality, with better performance for larger\npathologies like Cardiomegaly compared to smaller ones like Atelectasis.\nMethod Atelectasis Cardiomegaly Effusion\nLatentshift 0.09644 0.2864 0.143\nCheXpert CAV 0 .011627 ±0.00612 0 .14225 ±0.0577 0 .03581 ±0.00258\nCheXpert CAV mean 0.02971 0.2185 0.05762\nNIH CAV 0 .03192 ±0.004016 0 .25975 ±0.02382 0 .054485 ±0.001562\nNIH CAV mean 0.03927 0.363 0.04961\nto 0 for random comparisons. This confirms that the generated vectors capture meaningful\ninformation about the target concepts rather than random noise.\nSecond, concept vectors exhibit positive alignment both within and across datasets. For\nexample, the cosine similarity between NIH-mean and CheXpert-mean concept vectors is\n0.4060, which is significantly higher than the similarity with random vectors. This indicates\nthat the vectors are pointing in a similar direction in the latent space, despite being derived\nfrom different datasets with distinct characteristics and labeling protocols. While the\nsimilarity is not perfect (which would be unrealistic given the inherent variability between\ndatasets), the positive alignment supports the claim that concept vectors generalize across\ndatasets.\nFinally, high intra-dataset similarity scores demonstrate that the concept vectors are stable\nand consistent within each dataset. We believe that in high-dimensional latent space even the\nsmallest observed value (0.1011 for NIH to CheXpert similarity) is very high and highlights\nfeasibility of our approach. The similarity is further improved by taking an average of several\nconcept vectors (e.g., 0.9734 for NIH-mean and 0.9655 for CheXpert-mean).\n5.2. Visual explanations\nThe evaluation of visual explanations is shown in Tables 2, 3. We observe that our method\nperforms worse than latent shift in terms of IoU with the difference being more prominent\non smaller pathologies, such as atelectasis or nodule. For larger pathologies, such as\ncardiomegaly, this difference is less significant. In smaller pathologies, we often observe\nno meaningful shift in feature exaggeration, which is shown in Appendix B, Figure 4. We\nspeculate, that there are two main reasons for it. Firstly, the reconstruction fidelity of\nthe autoencoder model is insufficient for smaller features. Secondly, smaller pathologies\ntend to have different localizations in the lung, while larger cardiomegaly corresponds to\napproximately the same location in the study. We would also like to note the limitation of\nIoU and lack of comprehensive xAI benchmarks(Holmberg, 2022), specifically in the area\nof counterfactual medical explanations. More specifically, while IoU captures pixel-wise\nchanges it is only a proxy metric to human evaluation (Naveed et al., 2024).\n7\n--- Page 8 ---\nMaksudov Curran Mileo\nTable 3: Intersection over Union (IoU) comparison for ”Mass” and ”Nodule” pathologies\nusing NIH concept vectors. Averaging concept vectors improves explanation quality,\nthough performance remains lower for smaller pathologies like Nodule compared to\nlarger ones like Mass.\nMethod Mass Nodule\nLatentshift 0.1411 0.008768\nNIH CAV 0.03886 ±0.004617 0 .00129 ±0.00045\nNIH CAV mean 0.06409 0.0003938\n6. Future work\nOne direction for future work involves leveraging causal clinical concepts—such as anatomical\nor physiological features directly linked to pathologies—and mapping them into the latent\nspace of the model. This approach would enable the evaluation of concept similarity and\nthe measurement of alignment with established medical knowledge, while also providing the\nability to generate visual representations for further clinical validation.\nAnother promising direction is to replace case-based concept mapping with more advanced\nconditioning mechanisms, such as diffusion models. We envision exploring this topic further\nby measuring effects of latent denoising models on the vectors and conditioning denoising\nprocess on additional clinical data to generate counterfactual explanations.\nAdditionally, we believe that by utilizing CLIP (Contrastive Language–Image Pretraining)\nmodels in combination with CAVs and generative models, we could link concepts expressed\nin natural language terms with case-based and visual concepts.\n7. Conclusion\nIn this work, we demonstrated the use of Concept Activation Vectors (CAVs) to generate\ninterpretable counterfactual explanations for chest X-ray imaging. By mapping clinical\nconcepts into the latent space of a reconstruction autoencoder, we showed that meaningful\nvisual explanations can be produced without explicit training on class labels. Our experi-\nments revealed that concept vectors are stable and generalizable across datasets, capturing\nclinically relevant features. While the method performed well for larger pathologies like\ncardiomegaly, challenges remained for smaller pathologies such as atelectasis, likely due to\nlimitations in reconstruction fidelity and feature localization.\nAlthough our approach did not outperform the baseline in terms of Intersection over Union\n(IoU), it provides a simple framework for integrating clinical knowledge into generative\nmodels, offering a pathway toward more interpretable AI-driven diagnostics. Future work\nwill focus on improving reconstruction quality, exploring causal clinical concepts, and lever-\naging advanced conditioning mechanisms like diffusion models. This study represents a step\nforward in making AI explanations more transparent and aligned with clinical expertise.\n8\n--- Page 9 ---\nTowards generating more interpretable counterfactuals via concept vectors\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been\nKim. Sanity checks for saliency maps. Advances in neural information processing systems ,\n31, 2018.\nMatan Atad, Vitalii Dmytrenko, Yitong Li, Xinyue Zhang, Matthias Keicher, Jan Kirschke,\nBene Wiestler, Ashkan Khakzar, and Nassir Navab. Chexplaining in style: Counterfactual\nexplanations for chest x-rays using stylegan. arXiv preprint arXiv:2207.07553 , 2022.\nMaximilian Augustin, Valentyn Boreiko, Francesco Croce, and Matthias Hein. Diffusion\nvisual counterfactual explanations. In NeurIPS , 2022.\nJoseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek, Matthew P\nLungren, and Akshay Chaudhari. Gifsplanation via latent shift: a simple autoencoder\napproach to counterfactual generation for chest x-rays. In Medical Imaging with Deep\nLearning , pages 74–104. PMLR, 2021.\nJoseph Paul Cohen, Joseph D. Viviano, Paul Bertin, Paul Morrison, Parsa Torabian,\nMatteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad\nHashir, and Hadrien Bertrand. TorchXRayVision: A library of chest X-ray datasets\nand models. In Medical Imaging with Deep Learning , 2022. URL https://github.com/\nmlmed/torchxrayvision .\nLars Holmberg. Towards benchmarking explainable artificial intelligence methods. ArXiv ,\nabs/2208.12120, 2022. URL https://api.semanticscholar.org/CorpusID:251800062 .\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,\nHenrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:\nA large chest radiograph dataset with uncertainty labels and expert comparison. In\nProceedings of the AAAI conference on artificial intelligence , volume 33, pages 590–597,\n2019.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas,\net al. Interpretability beyond feature attribution: Quantitative testing with concept\nactivation vectors (tcav). In International conference on machine learning , pages 2668–\n2677. PMLR, 2018.\nSheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: making in context\nlearning more effective and controllable through latent space steering. In Proceedings of\nthe 41st International Conference on Machine Learning , ICML’24. JMLR.org, 2024.\nSidra Naveed, Gunnar Stevens, and Dean Robin-Kern. An overview of the empirical\nevaluation of explainable ai (xai): A comprehensive guideline for user-centered evaluation\nin xai. Applied Sciences , 14(23), 2024. ISSN 2076-3417. doi: 10.3390/app142311288. URL\nhttps://www.mdpi.com/2076-3417/14/23/11288 .\n9\n--- Page 10 ---\nMaksudov Curran Mileo\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-\nbased localization. In Proceedings of the IEEE International Conference on Computer\nVision (ICCV) , Oct 2017.\nJoseph D Viviano, Becks Simpson, Francis Dutil, Yoshua Bengio, and Joseph Paul Cohen.\nSaliency is a possible red herring when diagnosing poor generalization. arXiv preprint\narXiv:1910.00199 , 2019.\nXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M\nSummers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-\nsupervised classification and localization of common thorax diseases. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 2097–2106, 2017.\nAppendices\nA. Distribution of concepts across datasets\nTable 4: Number of samples for each concept in the NIH and CheXpert datasets. Note:\n’Mass’ and ’Nodule’ are not labeled in the CheXpert dataset, as these pathologies\nwere not included in the original annotation process.\nData set Cardiomegaly Atelectasis Effusion Mass Nodule\nNIH 2776 11559 13317 5782 6331\nCheXpert 27000 33376 86187 N/A N/A\nNIH bbox 146 180 153 85 79\nTotal 22976 44935 99504 5782 6331\nB. Visual explanation examples\nWhile analyzing visual explanations, we observe that our method performs better for larger\npathologies, such as ”Cardiomegaly” and prone to failure for smaller pathologies, such as\n”Atelectasis”. We provide two such cases bellow.\nC. CAV hyperparameters\n10\n--- Page 11 ---\nTowards generating more interpretable counterfactuals via concept vectors\nFigure 3: Successful visual explanation of the ”Cardiomegaly” pathology using concept\nvectors. From left to right: (a) Concept mask highlighting the region of interest,\n(b) Original chest X-ray image, (c) Reconstructed image from the autoencoder,\n(d) Latent shift 2D attribution map generated by the baseline method, and (e)\nAttribution map generated using the NIH concept vector.\nFigure 4: Example of a failed explanation for ”Atelectasis”. From left to right: (a) Concept\nmask highlighting the region of interest; (b) Original chest X-ray; (c) Reconstructed\nimage; (d) Baseline latent shift attribution map; (e) Attribution map. The failure\nis likely due to the small size, variable localization of atelectasis, and limitations\nin the autoencoder’s reconstruction fidelity for fine-grained features.\nTable 5: Hyperparameters for generating and evaluating CAVs\nBatch size 500\nNum positive samples 250\nNum negative samples 250\nNumber of concept vectors for averaging 10\nMax step size 1000\n11\n--- Page 12 ---\nMaksudov Curran Mileo\n12",
  "text_length": 27623
}