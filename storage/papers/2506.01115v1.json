{
  "id": "http://arxiv.org/abs/2506.01115v1",
  "title": "Attention Retrieves, MLP Memorizes: Disentangling Trainable Components\n  in the Transformer",
  "summary": "The Transformer architecture is central to the success of modern Large\nLanguage Models (LLMs), in part due to its surprising ability to perform a wide\nrange of algorithmic tasks -- including mathematical reasoning, memorization,\nand retrieval -- using only gradient-based training on next-token prediction.\nWhile the core component of a Transformer is the self-attention mechanism, we\nquestion how much, and which aspects, of the performance gains can be\nattributed to it. To this end, we compare standard Transformers to variants in\nwhich either the multi-layer perceptron (MLP) layers or the attention\nprojectors (queries and keys) are frozen at initialization. To further isolate\nthe contribution of attention, we introduce MixiT -- the Mixing Transformer --\na simplified, principled model in which the attention coefficients are entirely\nrandom and fixed at initialization, eliminating any input-dependent computation\nor learning in attention. Surprisingly, we find that MixiT matches the\nperformance of fully trained Transformers on various algorithmic tasks,\nespecially those involving basic arithmetic or focusing heavily on\nmemorization. For retrieval-based tasks, we observe that having input-dependent\nattention coefficients is consistently beneficial, while MixiT underperforms.\nWe attribute this failure to its inability to form specialized circuits such as\ninduction heads -- a specific circuit known to be crucial for learning and\nexploiting repeating patterns in input sequences. Even more interestingly, we\nfind that attention with frozen key and query projectors is not only able to\nform induction heads, but can also perform competitively on language modeling.\nOur results underscore the importance of architectural heterogeneity, where\ndistinct components contribute complementary inductive biases crucial for\nsolving different classes of tasks.",
  "authors": [
    "Yihe Dong",
    "Lorenzo Noci",
    "Mikhail Khodak",
    "Mufan Li"
  ],
  "published": "2025-06-01T18:42:39Z",
  "updated": "2025-06-01T18:42:39Z",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01115v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01115v1  [cs.LG]  1 Jun 2025Attention Retrieves, MLP Memorizes:\nDisentangling Trainable Components in the Transformer\nYihe Dong\nPrinceton University\nydong@princeton.eduLorenzo Noci\nPrinceton University\nETH Zurich\nlorenzo.noci@inf.ethz.chMikhail Khodak\nPrinceton University\nkhodak@wisc.edu\nMufan Li\nPrinceton University\nmufan.li@outlook.com\nAbstract\nThe Transformer architecture is central to the success of modern Large Language Models (LLMs), in\npart due to its surprising ability to perform a wide range of algorithmic tasks ‚Äì including mathematical\nreasoning, memorization, and retrieval ‚Äì using only gradient-based training on next-token prediction.\nWhile the core component of a Transformer is the self-attention mechanism, we question how much,\nand which aspects, of the performance gains can be attributed to it. To this end, we compare standard\nTransformerstovariants inwhich eitherthe multi-layer perceptron(MLP) layersorthe attentionprojectors\n(queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce\nMixiT ‚Äì the Mixing Transformer ‚Äì a simplified, principled model in which the attention coefficients are\nentirely random and fixed at initialization, eliminating any input-dependent computation or learning in\nattention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on\nvarious algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization.\nFor retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently\nbeneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits\nsuch as induction heads ‚Äì a specific circuit known to be crucial for learning and exploiting repeating\npatterns in input sequences. Even more interestingly, we find that attention with frozenkey and query\nprojectors is not only able to form induction heads, but can also perform competitively on language\nmodeling. Our results underscore the importance of architectural heterogeneity, where distinct components\ncontribute complementary inductive biases crucial for solving different classes of tasks.1\n1 Introduction\nTransformers [ 48] have rapidly become the workhorse architecture in modern machine -learning systems,\npowering state -of-the-art models in language, vision, and scientific domains [ 15,46,21]. Their success is\ntypically attributed to the self -attention mechanism, which allows every token to aggregate information\nfrom the entire sequence and has been linked to emergent abilities such as long -range retrieval, algorithmic\nreasoning, and in -context learning. Yet we lack a precise answer to a fundamental question: which degrees of\nfreedom inside the Transformer are truly necessary for these behaviours, and which can be simplified away\nwithout harming performance?\nPrior work has probed the internals of trained Transformers. Studies of attention maps consistently report\nthe emergence of induction heads that copy information forward and enable in -context retrieval [ 36]. Another\nline of work has focused on whether and how Transformers‚Äô capabilities can emerge [ 1,40,23] by studying\nsynthetic datasets with Zhong and Andreas [57]showing that models with frozen self -attention but trainable\nembeddings still solve many algorithmic tasks. These results hint that different parts of the architecture are\nresponsible for modeling different tasks.\n1Our code is publicly available at https://github.com/princeton-pli/MixiT .\n1\n--- Page 2 ---\nT\nüî•\nüî•‚ùÑ üî• ‚ùÑ\nüî• üî• üî•üî•T\nüî•\n‚ùÑ ‚ùÑ ‚ùÑüî•\nüî• üî•‚ùÑ üî•\n‚ùÑFigure 1: Variants of the Llama Transformer model that we study.\nA standard Transformer block, however, contains several interacting components: an attention block, with\nlearnable and input-dependent queries, keys form the attention weights, and MLP blocks composed of fully\nconnected layers. To exemplify the complexity of the interaction, notice that the attention weights can change\ntheir value both through changes in the queries/keys parameters, as well as as through the residual -stream\nrepresentations that feed those projections. Hence, because all the components are trained together, it is\nhard to discern the contribution of each individual one to the solution of a given task. In this work, we study\nthe role of each Transformer‚Äôs component by freezing different parts of the architecture to their value at\ninitialization. In partiucular, we consider the following simplified variants:\n‚Ä¢Frozen-QK, preserves the conventional attention structure but freezes the query and key weight matrices,\nallowing only the value weight matrix to be learned.\n‚Ä¢Frozen-MLP , where the weight matrices of the MLP block are frozen to their initial value.\n‚Ä¢MixiT(Mixing Transformer), a Transformer variant in which the attention sub -layer is replaced by a\nfixed, randomly initialised mixing matrix. After initialisation the mixing weights are frozen, preventing\nthe model from adapting interactions during training or conditioning them on input content. Learning\ntherefore takes place exclusively in the embedding and MLP blocks and in residual branches.\nComparing these three simplified models with a fully -trainable Transformer makes it possible to separate\nthe role of (i) learned token -dependent weights and (ii) the mere presence of a mixing operation, as well as\n(iii) understand the role of MLPs.\nWe compare across several categories of tasks: mathematical reasoning, sentiment classification, mem-\norization, information retrieval and language modeling. This way, we can inspect whether and how the\ndifferent parts of the architecture are able to solve primitive tasks that relate to a range of basic reasoning\nand memorization skills. Here we summarize our main contributions:\n‚Ä¢We find that, in all but information retrieval and language modeling, MixiT achieves performance\ncomparable to the fully trained transformer and Frozen-QK. These results suggest that for a wide range\nof tasks, \"attention mixing is all you need\" regardless of its specific form, in the sense that learned\nquery-key interactions are not required.\n‚Ä¢When tested on an information retrieval task, MixiT fails to achieve good performance, while both the\nfully trained transformer and Frozen-QK can solve it to close-to perfect accuracy. We show that while\nthe alternatives do form induction heads, MixiT cannot do so by design, and this strongly correlates\nwith the observed (in)ability to solve the task.\n‚Ä¢On pure memorization tasks, Frozen-MLP loses two-thirds of the storage capacity of the fully-trainable\nTransformer, while Frozen-QK and MixiT only a third. This suggests that the MLP layers are largely\nresponsible for memorization, and input-dependent attention has a very little added contribution.\n2\n--- Page 3 ---\n‚Ä¢We present the surprising finding that Frozen-QK can perform competitively with the standard\ntransformer on language modeling tasks. Indeed, Frozen-QK develops the ability to form specialized\ncircuits such as induction heads during training.\nOur results indicate that the Transformer architecture has some built-in inductive bias towards algorithmic\nabilities, namely the ability to form circuits. Furthermore, while for a variety of tasks no particular structure\nin the attention maps is needed, some learnable attention mechanism is required when flexibly copying tokens.\nAs a consequence, due to the observed gap in language modeling, we speculate that the ability to solve\nassociative recall and complex retrieval is a key ingredient to modern language models. More broadly, our\nresults underpin the importance of having heterogeneous components that are more specialized in efficiently\nsolving modular subtasks, enabling the model to express complex, wide-ranging behaviors.\n2 Models Specification\nWe consider a decoder-only Transformer based on the widely used Llama architecture [ 47]. Given an input\nsequence x‚ààRV√óm, where Vis the vocabulary size and mis the sequence length, embedded with a linear\nmap to the hidden states h0=Wembxwhere Wemb‚ààRV√ón, where nis the width of the model. At its core,\nthe transformer uses Lstacked modules alternating the causal multi-head self-attention layers and MLP\nlayers. Each self attention head is defined as:\nAttn(h‚Ñì) =Wv\n‚Ñìh‚ÑìSoftmax\u00121‚àönhQT\n‚ÑìK‚Ñì\u0013\n, Q ‚Ñì=WQ\n‚Ñìh‚Ñì, K ‚Ñì=WK\n‚Ñìh‚Ñì, (2.1)\nwhere WQ\n‚Ñì, WK\n‚Ñì, Wv\n‚Ñì‚ààRnh√ónare the queries‚Äô and keys‚Äô weights, and the outputs across multiple heads are\nconcatenated. The gated MLP layer is defined as:\nMLP (h‚Ñì) =WD\n‚Ñì\u0000\nœï(WG\n‚Ñìh‚Ñì)‚äôWU\n‚Ñìh‚Ñì\u0001\n. (2.2)\nwhere WG\n‚Ñì, WU\n‚Ñì‚ààRnm√ónandWD\n‚Ñì‚ààRn√ónm.nhandnmare the dimensions of the queries/keys for each\nhead and MLP projections. We also use skip connections in both blocks with a pre-normalization scheme [ 50]\nand causal masking. We apply rotary embeddings [ 43] to the queries and keys of each layer. As in common\npractice, we use nm= 4nandHnh=n.\nModels with Frozen Weights. In the Frozen-QK model, we set WQ\n‚Ñì, WK\n‚Ñìto their value at initialization,\nand for the Frozen-MLP models we freeze WD\n‚Ñì, WG\n‚Ñì, WU\n‚Ñìfor all layers.\nMixiT ‚Äì Random Static Attention. We also design a model where the attention map itself is frozen\nand, to achieve that, input-independent. In the simplest case, this can be obtained by having a random\nmatrix M‚Ñì‚ààRm√ómentries with N(0,1/m)entries, where the factor of 1/macts as a variance-preserving\nnormalizer. To ensure a stable forward pass in terms of depth and width scaling, we follow the principles of\nattention shaping [34] and propose the following:\nAttn(h‚Ñì) =Wv\n‚Ñìh‚Ñì\u0000\nI+WM\n‚Ñì‚àí¬ØWM\n‚Ñì\u0001\n, Wv\n‚Ñì,ijiid‚àº N(0,1\nn), WM\n‚Ñì,ijiid‚àº N(0,1‚àönm), (2.3)\nwhere WM\n‚Ñìis frozen at initialization and ¬ØWM\n‚Ñìcontains the column-wise empirical average of WM\n‚Ñì, to ensure\nthat each row sums up to 1. In the Appendix, , we show that this attention has a stable forward pass, in the\nsense that the kernel of the activations has a well-defined depth-and-width limit, converging to a stochastic\ndifferential equation (SDE) [ 28,34]. When we adopt this architecture, all the weights, excluding the random\nattention matrix, are trainable. Notably, the following convergence result implies the stability of the forward\npass, in particular ruling out the numerical degeneracy such as rank collapse and vanishing gradients [ 14,33].\nTheorem 2.1 (MixiT Covariance SDE) .Consider the MixiT recursion h‚Ñì+1=Attn(h‚Ñì)defined by (2.3) at\ninitialization. Then as the width nand depth dgo to infinity withd\nn‚Üí¬ØœÑ >0, the upper triangular entries\n3\n--- Page 4 ---\nof the covariance matrix Œ¶‚Ñì=1\nnh‚ä§\n‚Ñìh‚Ñìflattened to a vector in Rm(m+1)/2converges to the solution of the\nfollowing SDE\ndŒ¶œÑ=\u00141\nmTr(Œ¶ œÑ)‚àíM(Œ¶œÑ)\u0015\ndœÑ+ [Œ£v(Œ¶œÑ) + Œ£M(Œ¶œÑ)]1/2dBœÑ, (2.4)\nwhere M(Œ¶) =1\nm2Pm\nŒ±Œ≤=1Œ¶Œ±Œ≤is the average over all entries, BœÑis a standard Brownian motion in Rm(m+1)/2,\nŒ£v(Œ¶)Œ±Œ≤,Œ≥Œ¥= Œ¶Œ±Œ≥Œ¶Œ≤Œ¥+ Œ¶Œ±Œ¥Œ¶Œ≤Œ≥and\nŒ£M(Œ¶)Œ±Œ≤,Œ≥Œ¥=Œ¥Œ±Œ≥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) +Œ¥Œ±Œ¥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ≥) +Œ¥Œ≥Œ≤C(Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ±) +Œ¥Œ≤Œ¥C(Œ¶‚Ä¢Œ±,Œ¶‚Ä¢Œ≥),(2.5)\nwhere Œ¥Œ±Œ≥is the Kronecker delta, C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) =1\nm‚ü®Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥‚ü© ‚àíŒ¶‚Ä¢Œ≤Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ≤= [Œ¶Œ±Œ≤]m\nŒ±=1is the Œ≤-th column\nvector, and Œ¶‚Ä¢Œ≤=1\nmP\nŒ±Œ¶Œ±Œ≤is the average.\nThe full proof can be found in the Appendix.\nPositional embedding. As the random attention matrix I+1‚àömnWM\n‚Ñì‚àí¬ØWM\n‚Ñìin MixiT does not depend\non the input, the rotary positional embedding [ 43] standard in Llama models cannot be applied to MixiT, as\nrotary embeddings are added to learned key and query embeddings, which are not present in MixiT. Hence,\nwe implement a learnable positional embedding for each token position in the sequence, and add it to the\ncorresponding token embedding in the first layer.\n3 Experiments\n3.1 Tasks\nWe benchmark on a variety of tasks, covering categories including basic mathematical reasoning, memorization,\nsentiment classification, and language modeling. The basic mathematical reasoning and memorization tasks\nare based on tasks used in [ 57], with increased difficulty on some tasks to better reflect differences between\narchitectures.\nDecimal Addition. For the decimal addition task, the model learns to add two integers with the same\nnumber of digits. We randomly sample 50,000 pairs of ten-digit numbers, and train the model to predict\ntheir sum. The test set consists of 4,000 such sequences not in the training set. An example is 1234567890 +\n2345678901 ‚Üí3580246791 .\nNeedle in a Haystack (Retrieval). Each training instance encodes a small randomly generated sequence\nof pairs followed by a single query, and the model is required to emit the value associated with that query one\nstep later. We sample a sequence length m‚àº U{ 1, . . . , m max}, where mmaxis the maximum sequence length.\nWe then sample mkeys{ki}m\ni=1iid from the set {V\n2, . . . , V }andmvalues {vi}m\ni=1from{1, . . . ,V\n2‚àí1}, where\nV= 256is the vocabulary size. The resulting keys are interleaved with their values to form the prefix\n[(k1, v1),(k2, v2), . . . , (km, vm)]. A query key kqwith q‚àà[m]is chosen uniformly at random from the keys,\nand appended to the sequence. The goal is to predict the value corresponding to the query token. This task\nisolates retrieval ability, and probes associative recall. We sample 40,000 sequences for training, and 4,000\nsequences for testing. Transformer-based models typically solve this task by forming induction heads [36].\nModular Addition. This task evaluates the model‚Äôs ability to perform addition, modulo a given prime p.\nIn our case we sample 40,000 pairs (a, b)of integers, each within the range [1, p], forp= 599, and train the\nmodel to predict a+bmod p.\nParentheses Balancing (Dyck-1). The parentheses balancing task learns to predict whether a given\nparentheses sequence is balanced. Within any prefix in the sequence, the number of closing parentheses is\nless than or equal to the number opening parentheses. Hence this task is solvable by a linear time algorithm.\nWe randomly sample 100,000 parentheses sequences of length 40 for training, while the test set consists of\n4,000 such sequences not in the training set. An example is ‚Äú(()\" ‚ÜíFalse.\n4\n--- Page 5 ---\n5 10 20 30 40\nmmax020406080100Accuracy (%)Standard Frozen-MLP Frozen-QK MixiTFigure 2: Retrieval accuracy as a function of the number of pairs in the sequence mmax, which encodes task\ncomplexity. Having trainable queries and keys is necessary to solve the task (Standard and Frozen-MLP).\nFrozen-QK cannot solve the task for large mmax, while MixiT has deteriorating performances.\nMemorization. We follow the procedure in Zhong and Andreas [57]: we sample 5122key-value pairs,\nwhere each key is independently sampled from its value, which is an integer in [512]. Because the key-\nvalue mapping is random, any success reflects the model‚Äôs ability to memorise arbitrary associations. We\nmeasure success with the number of bits per parameter that the model can store, defined as #total_bits √ó\nmodel_acc /total_trainable_params , where model_acc is the training accuracy on the task. Notice that\nfor this problem, storing one pattern requires log2512 = 9bits, thus #total_bits = 9¬∑5122.\nLanguage Modeling. To test the model‚Äôs ability to model natural language, we train the model to perform\nnext-token-prediction. We use two datasets, Wikitext-103 [ 31] and Fineweb-edu [ 37]. Wikitext-103 consists of\n1,801,350 cleaned-up Wikipedia articles, with test set of size 4358. And Fineweb-edu consists of top-quality\nentries collected from web-crawled data, focusing on educational text. We randomly sample 1,048,576 entries\nfor training, and 4096 to test.\nSentiment Classification. We use the Yelp polarity reviews dataset [ 56] to test each model variation‚Äôs\nability to predict a review‚Äôs sentiment, i.e. whether a review is positive or negative.\n3.2 Model training\nFor each task, we perform a grid search over a range of hyperparameters to train all model variations. The\noptimal hyperparameters are determined using a grid search specific for each task, which are detailed in the\nAppendix. All model variants are trained on one to four H100 GPUs, depending on task complexity.\n4 Results\nRandom static attention can perform certain algorithmic tasks. Table 1 contains the main results\non algorithmic and sentiment classification tasks with a large memorization focus.\nTrainable attention weights are necessary for retrieval. We perform an experiment using the needle\nin a haystack dataset where the degree of complexity is controlled by the maximal number of pairs mmax.\nWe fix the embedding dimension n= 1024divided across 4heads with nh= 256, and fix 2 layers, which is\n5\n--- Page 6 ---\nModel \\ Task Decimal Addition ‚ÜëDyck-1 ‚ÜëModular addition ‚ÜëMemorization ‚ÜëYelp ‚Üë\nStandard 98.58% 95.80% 100% 100% 90.55%\nFrozen-QK 100% 97.38% 100% 100% 90.86%\nMixiT 100% 96.17% 100% 100% 92.56%\nTable 1: MixiT performance on algorithmic and sentiment classification tasks, many with a large memorization\nfocus. As shown, both Frozen-QK and MixiT are able solve such tasks. They are competitive with, and can\neven be superior than, the standard transformer.\nsufficient to solve this task by using induction heads. We search for the optimal learning rate in the range\n[5¬∑10‚àí5,5¬∑10‚àí3]. The results are shown in Figure 2. Notice how the while models with trainable queries and\nkeys can solve the task across all the levels of difficulty, Frozen-QK has a drop in performances at mmax= 40\nand MixiT struggles significantly after at mmax‚â•20. These results provide compelling evidence that the\nMLP and embedding layers alone are insufficient when it comes to complex retrieval tasks.\nInput-dependent attention patterns are required for language modeling. Surprisingly, Frozen-QK\ncomes close to the standard transformer in terms of perplexity, as shown in Table 2. This indicates that\ntrainable attention weights are notalways required for successful language modeling. Indeed, as Figure 3\nshows, specialized circuits such as induction heads can form even in Frozen-QK. Less surprisingly, MixiT lags\nbehind the standard Transformer, supporting the hypothesis that input-dependent learned attention patterns,\nsuch as induction heads, are necessary for language modeling, corroborating earlier works [36, 12].\nModel \\ Task Wikitext ‚ÜìFineweb-edu ‚Üì\nStandard 2.78 3.05\nFrozen-QK 3.07 3.16\nMixiT 3.73 4.08\nTable 2: Performance on language modeling tasks, in terms of logperplexity. Frozen-QK comes surprisingly\nclose in performance to the standard Transformer, despite having random static query and attention weights.\nMixiT lags behind the standard Transformer, supporting the hypothesis that input-dependent learned\nattention patterns, such as induction heads, are necessary for good language modeling.\nMemorization. We train an L= 2,nh= 4heads model on the memorization tasks for 10,000steps with a\nlearning rate of 0.005, for a consistent comparison across models. In Table 3 we report the accuracy and the\nstorage capacity bits per parameters . We find that the standard Transformer stores 2.98bits per parameters,\nwhich is slightly higher than in previous works [ 1,57]. Most of the drop occurs in the Frozen-QK model,\nwith 1.13bits per parameters, while Frozen-MLP and MixiT have similar storage capabilities at 2.25and\n2.18, respectively. Note that these results are yielded in a setting where the accuracies are not saturated at\n100%, to give an accurate representation of bits per parameter, rather than a lower bound. Hence they do\nnot contradict the results in Table 1.\nThese results suggest that (1) the MLPs are largely responsible for memorization, however (2) there is a\nnon-negligible additional contribution given by the integration of MLPs with learnable attention weights.\nThis non-negligible additional contribution provides further evidence for recent findings such as knowledge\ncircuits [ 52] and query localization [ 10], in that MLPs and attention collaborate to remember knowledge. In\nparticular, the disproportionately large increase in the learned bits per parameter from Frozen-QK to the\nfully trained transformer, from 2.25 to 2.98, suggests that the gain in accuracy is more than what can be\naccounted for by a mere increase in learnable parameters.\nPerformance with respect to number of heads for MixiT. For some tasks, we observe that increasing\nthe number of attention heads in MixiT can notably improve performance, as demonstrated by the decimal\naddition and Dyck-1 parentheses balancing tasks in Table 4. Intuitively, since each attention head uses a\n6\n--- Page 7 ---\nModel Memorization Accuracy ‚ÜëBits Per Parameter ‚ÜëTrainable Parameters\nStandard 100% 2.98 790400\nFrozen-MLP 19% 1.13 394880\nFrozen-QK 69% 2.25 724352\nMixiT 67% 2.18 724736\nTable 3: Standard Transformers outperform all the alternatives in terms of memorization capability, which\nsuggests that MLPs and attention collaborate to remember knowledge. This provides further evidence for\nrecent findings such as knowledge circuits [ 52] and query localization [ 10]. Freezing the MLPs causes the most\nperformance drop, indicating that they are the biggest factor when it comes to memorization. Notice that\nMixiT has slightly more parameters than Frozen-QK because of additional trainable positional embeddings.\ndifferent random attention matrix, more attention heads gives the learnable MLP components more diverse\nattention patterns to choose from based on the input, hence lessening the disadvantage of static attention.\nNote that purely increasing the number of heads, without increasing the hidden dimension, reaches diminishing\nreturns, as the per-head embedding dimension decreases proportionally, restricting expressiveness.\nTask \\ Number of heads 4 16 64 256\nDecimal addition 34.71% 34.70% 50.72% 91.87%\nDyck-1 77.93% 81.68% 89.38% 91.83%\nYelp sentiment classification 92.52% 92.56% 92.48% 91.72%\nTable 4: Accuracy with respect to the number of heads on various tasks for MixiT. The hidden dimension is\n512 for decimal addition and Dyck-1, and 1024 for Yelp sentiment classification. Increasing the number of\nheads increases the number of random attention matrices, giving learnable MLPs more diverse token mixing\npatterns to choose from based on the input, which can mitigate the disadvantage of static attention. However,\na positive performance correlation does not appear in all tasks, such as in Yelp.\nHowever, we do not observe this performance boost consistently across tasks. For instance, for Yelp\nsentiment classification, the performance is invariant with respect to the number of heads. This phenomenon\nremains interesting work for future study.\n5 Discussion\nCircuit learning and task separation. As the attention matrix in MixiT is static and input-independent,\nMixiT cannot adapt to each input and form specific circuits such as induction heads [ 36]. Induction head\ncircuits look for recent occurrences of the current token, and attends to the tokens that follow with increased\nprobability. This allows the standard transformer to easily adapt to the input context in language modeling.\nHence, it is no surprise that MixiT lags behind standard transformer for language modeling. It is perhaps\nmore surprising that the perplexity comes close to that of the standard transformer.\nThe near-perfect performance on certain algorithmic tasks in Table 1 suggests that induction heads and\nother specialized input-dependent circuits are not required on these tasks, hence providing an axis for task\nseparation .\nNote that MixiT is able to perform well on the Yelp reviews dataset, despite the complexity of language\nused in reviews. This supports the hypothesis that sentiment can be largely judged by the collective token\nembeddings, as opposed to next-token prediction in language modeling tasks, which requires retrieving specific\ndetails from the context, a task induction heads are apt at.\nLearnable attention is not required to form induction heads. Interestingly, as shown in in Figure 3\nand demonstrated by its performance, the Frozen-QK model can solve the retrieval task by forming induction\nheads.\n7\n--- Page 8 ---\n262\n83\n256\n8\n269\n83262\n83\n256\n8\n269\n83L0H0\n262\n83\n256\n8\n269\n83262\n83\n256\n8\n269\n83L1H0\n0.00.20.40.60.81.0Figure 3: The Frozen-QK model can solve the retrieval task by forming an induction head. In the first head,\neach token attends to the previous one; in particular, the query token 83is attended by 256. In the head of\nthe second layer, the correct token 256is retrieved.\nRole of MLPs in knowledge storage. Previous works have highlighted the importance of MLPs in\nstoring knowledge [ 13,20,19,18,53,11,30,32]. They posit that specific facts from the training data\nare stored in specific knowledge neurons. These works support our findings, in that MLPs are crucial in\nknowledge memorization. However, our work does not prescribe knowledge localization, i.e. we don‚Äôt attribute\nmemorizing specific facts to specific neurons.\nOur work adds characterization on knowledge memorization in more recent works on knowledge circuits [ 52]\nand query localization [ 10], where MLPs and attention are found to collaborate on knowledge memorization,\ne.g. where attention selects the appropriate knowledge neurons depending on the query. Our work shows that\neven with static random attention weights, such as in Frozen-QK, attention and MLPs can still collaborate\neffectively, as evident in language modeling perplexities similar to that of the standard transformer, and\nthe formation of specific input-dependent circuits. However, our results show that the role of MLPs in\nmemorization outweighs that of attention, as evident by the fact that Frozen-MLP achieves much worse\naccuracy than Frozen-QK or MixiT (3).\nRelation to random transformers. Zhong and Andreas [57]studies the random transformer, wherein\nthey train only the embedding and unembedding layers, and leave the intermediate layers fixed at random\ninitialization. The random transformer was found to be able to perform nontrivial algorithmic tasks. There\nare several notable differences between MixiT and the random transformer. The principal difference is that\nthe random attention matrix I+WM\n‚Ñì‚àí¬ØWM\n‚Ñìin MixiT is input-independent , and hence cannot develop\nspecialized circuits that adapt to the input, whereas such circuits are possible for the random transformer.\nMoreover, the MLPs in MixiT are trained, whereas they remain frozen in the random transformer. These\ndifferences allow us to study the roles different trained components play in the transformer architecture.\nFurthermore, through in depth hyperparameter searches, we find that the random transformer does not\nscale well with respect to depth, confirming some of the original findings [ 57]. However, MixiT, with its\ncarefully normalized random attention matrix designed to preserve signal propagation, does scale with respect\nto depth, suggesting that the random transformer suffers from signal propagation challenges and rank collapse\nwithout appropriate shaping [14, 33, 34].\nModel \\ Depth 2 8 16\nMixiT 100% 100% 100%\nRandom transformer 100% 23.53% 22.88%\nTable 5: Performance comparison between MixiT and the random transformer, with respect to number of\nlayers, on the decimal addition task. The random transformer‚Äôs performance does not scale well with respect\nto depth, whereas proper attention matrix shaping helps MixiT scale with respect to depth.\n8\n--- Page 9 ---\nImplications for architecture design. Notably, some of these results strengthen the argument for\nempirical approaches for architecture design found in previous work [ 38,8]. In particular, Poli et al. [38]uses\nperformances in various synthetic tasks to design powerful hybrid architectures. Our results suggest that\nin certain circumstances some architectural components are entirely responsible for some basic operations\n(e.g. information retrieval) while in some other there is a more coordinated effort. Thus, overall this favours\nhybrid architectures with diverse basic components.\n6 Related work\nStatic Attention. Several studies explore simplified Transformers with frozen or random components.\nNotably, random Transformers with fixed layers but trainable embeddings can solve many algorithmic\ntasks [57]. Similarly, replacing attention with fixed matrices - as in Synthesizer [ 44] or FNet [ 27] ‚Äî retains\ncompetitiveperformanceoncertainbenchmarks, suggestinglearnedattentionisnotalwaysnecessary. However,\nretrieval tasks often require flexible, input-dependent attention to form induction-like circuits. While their\nattention is also random, unlike our MixiT model it is input- dependent and so does not isolate the specific\ntasks for which attention is and is not needed. We relate our results to their work in detail in Section 5.\nOther past work has studied properties of other randomly frozen or lightly trained models, e.g. convolutional\nnetworks [24, 41, 3], largely without focusing on specific tasks.\nStable Signal Propagation. While there is a long line of work studying signal propagation in deep neural\nnetworks [ 42,39,26,51], it was only more recently that Martens et al. [29]introduced the concept that\nmodifying activation functions can significantly improve stability of signal propagation, and leading to rapid\ntraining of large scale vision models without using normalization and skip connections [ 55]. This was later\nunderstood to yield a stable scaling limit, which is characterized by an SDE of the covariance matrix [ 28]. The\ncovariance SDE framework is then used to understand how to design and shape general non-linearities like the\nTransformer self-attention [ 34], resolving the rank collapse issue [ 14,33] and has shown strong performance\ndespite a much simplified Transformer block [ 22]. Our theoretical result Theorem 2.1 also follows from this\nframework.\nModular Tasks. Several works have investigated the capabilities of Transformers in the classes of tasks\nanalyzed here, including arithmetic [ 35]. The role of feedforward layers in memorization in the Transformer\narchitecture has been studied in Geva et al. [17]and their inductive biases and scaling properties have been\nscrutinized [ 5]. In this context, our work shows their relevance in conjuction with trainable or fixed attention.\nOrthogonal to our work, memorization has also been studied to understand generalization in neural networks\n[54, 4, 2].\nMechanistic Interpretability. Other closely related work is in the mechanistic interpretability, which\naims to understand LLMs via examining and modifying their internals [ 6]. Closely related is work related\nto identifying and understanding the behavior of induction heads [ 36,16,7] and in-context learning [ 9].\nOur work demonstrates that the performance separation between input-dependent and input- independent\nattention is largely driven by the latter‚Äôs inability to form induction heads. Finally, Meng et al. [30]which\nreverse-engineer how different Transformer components support behaviors like memorization, retrieval, and\ngeneralization.\nEfficient Attention. A last area that has seen significant effort at understanding attention is that of efficient\nTransformers [ 45]. While we study what happens when attention is replaced with a fixed input-independent\nmatrix, this field has studied various useful aspects of the attention matrix such as attention sinks [ 49] and\ncompression [ 25]. As our work demonstrates that for many tasks the full power of input-dependent attention\nis not needed, it may have its own implications for efficiency, e.g. by removing the need for the KV-cache.\n9\n--- Page 10 ---\n7 Conclusion\nIn this work, we isolated the roles of the architecture‚Äôs two trainable components by selectively freezing\nparameters and introducing MixiT, whose attention maps are fixed and random. Fixed attention suffices\nfor many algorithmic tasks. However, adaptive attention is essential for associative recall and language\nmodelling, as MixiT fails on long-range retrieval and yields higher perplexity. However, memorization resides\nin the MLPs, with attention contributing significantly less. Together, these results support a heterogeneity\nhypothesis: MLPs and learnable attention mechanism are required for the full behavioural range of modern\nlarge language models.\nLimitations and future work Our work studied basic algorithmic, mathematical reasoning, and language\nmodeling tasks. It is an exciting future direction to extend our study to more complex tasks such as human\nreasoning, which demands a wide-ranging skillset that does not fall neatly on one side of the retrieval-\nmemorization spectrum. We used relatively small-scale models; extending the analysis to larger networks\nand datasets will test whether the observed separation of concerns persists at a larger scale. Future work\nmight study hybrid training schedules in which only a subset of architectural modules remain trainable ‚Äì\nor are gradually unfrozen ‚Äì which may strike an even better accuracy‚Äìefficiency trade-off. Finally, probing\nhow these architectural choices interact with emerging interpretability and safety techniques constitutes an\nexciting avenue for further research.\nReferences\n[1]Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling\nlaws.arXiv preprint arXiv:2404.05405 , 2024.\n[2]Sotiris Anagnostidis, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. The curious case of\nbenign memorization. arXiv preprint arXiv:2210.14019 , 2022.\n[3]Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact\ncomputation with an infinitely wide neural net . Curran Associates Inc., Red Hook, NY, USA, 2019.\n[4]Devansh Arpit, Stanis≈Çaw Jastrzƒôbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S\nKanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at\nmemorization in deep networks. In International conference on machine learning , pages 233‚Äì242. PMLR,\n2017.\n[5]Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Scaling mlps: A tale of inductive bias.\nAdvances in Neural Information Processing Systems , 36:60821‚Äì60840, 2023.\n[6]Leonard Bereska and Stratis Gavves. Mechanistic interpretability for AI safety - a review. Transactions\non Machine Learning Research , 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=\nePUVetPKu6 . Survey Certification, Expert Certification.\n[7]Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a\ntransformer: A memory viewpoint. Advances in Neural Information Processing Systems , 36:1560‚Äì1588,\n2023.\n[8]Timur Carstensen, Neeratyoy Mallik, Frank Hutter, and Martin Rapp. Frozen layers: Memory-efficient\nmany-fidelity hyperparameter optimization. arXiv preprint arXiv:2504.10735 , 2025.\n[9]Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K Lampinen, and\nFelix Hill. Transformers generalize differently from information stored in context vs in weights. arXiv\npreprint arXiv:2210.05675 , 2022.\n[10]Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Knowledge localization: Mission not\naccomplished? enter query localization! In Proceedings of the Thirteenth International Conference on\nLearning Representations (ICLR) , 2025. URL https://openreview.net/forum?id=tfyHbvFZ0K .\n10\n--- Page 11 ---\n[11]Bilal Chughtai, Alan Cooney, and Neel Nanda. Summing up the facts: Additive mechanisms behind\nfactual recall in llms, 2024. Preprint.\n[12]Joy Crosbie and Ekaterina Shutova. Induction heads as an essential mechanism for pattern matching in\nin-context learning. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association\nfor Computational Linguistics: NAACL 2025 , pages 5034‚Äì5096, Albuquerque, New Mexico, April 2025.\nAssociation for Computational Linguistics. ISBN 979-8-89176-195-7. URL https://aclanthology.org/\n2025.findings-naacl.283/ .\n[13]Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons\nin pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) , pages 8493‚Äì8502, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-long.581.\n[14]Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention\nloses rank doubly exponentially with depth. In International conference on machine learning , pages\n2793‚Äì2803. PMLR, 2021.\n[15]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n[16]Ezra Edelman, Nikolaos Tsilivis, Benjamin L. Edelman, eran malach, and Surbhi Goel. The evolution of\nstatisticalinductionheads: In-contextlearningmarkovchains. In The Thirty-eighth Annual Conference on\nNeural Information Processing Systems , 2024. URL https://openreview.net/forum?id=qaRT6QTIqJ .\n[17]Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\nkey-value memories. arXiv preprint arXiv:2012.14913 , 2020.\n[18]Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\nkey-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih,\neditors,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages\n5484‚Äì5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.446.\n[19]Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build\npredictions by promotingconceptsin thevocabulary space. InYoavGoldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,\npages 30‚Äì45, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.emnlp-main.3.\n[20]MorGeva, JasmijnBastings, KatjaFilippova, andAmirGloberson. Dissectingrecalloffactualassociations\ninauto-regressivelanguagemodels. InHoudaBouamor, JuanPino, andKalikaBali, editors, Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing , pages12216‚Äì12235, Singapore,\nDecember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.751.\n[21]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 , 2025.\n[22]Bobby He and Thomas Hofmann. Simplifying transformer blocks. arXiv preprint arXiv:2311.01906 ,\n2023.\n[23]Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt√§schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on\nprocedurally defined tasks. arXiv preprint arXiv:2311.12786 , 2023.\n11\n--- Page 12 ---\n[24]Kevin Jarrett, Koray Kavukcuoglu, Marc‚ÄôAurelio Ranzato, and Yann LeCun. What is the best multi-stage\narchitecture for object recognition? In 2009 IEEE 12th International Conference on Computer Vision ,\npages 2146‚Äì2153, 2009. doi: 10.1109/ICCV.2009.5459469.\n[25]Junhyuck Kim, Jongho Park, Jaewoong Cho, and Dimitris Papailiopoulos. Lexico: Extreme kv cache\ncompression via sparse coding over universal dictionaries, 2024. URL https://arxiv.org/abs/2412.\n08890.\n[26]Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha\nSohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165 , 2017.\n[27]James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier\ntransforms. arXiv preprint arXiv:2105.03824 , 2021.\n[28]Mufan Li, Mihai Nica, and Dan Roy. The neural covariance sde: Shaped infinite depth-and-width\nnetworks at initialization. Advances in Neural Information Processing Systems , 35:10795‚Äì10808, 2022.\n[29]James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha Sohl-\nDickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip connections\nor normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765 , 2021.\n[30]Kevin Meng, David Bau, Michael Andrus, Aitor Belrose, Alex Andonian, Catherine Olsson, Sam\nMcCandlish, and Dario Amodei. Locating and editing factual knowledge in gpt. arXiv preprint\narXiv:2202.05262 , 2022.\n[31]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843 , 2016. URL https://arxiv.org/abs/1609.07843 .\n[32]Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style\nvector arithmetic, 2024. Preprint.\n[33]Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi.\nSignal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in\nNeural Information Processing Systems , 35:27198‚Äì27211, 2022.\n[34]Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy.\nThe shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural\nInformation Processing Systems , 36:54250‚Äì54281, 2023.\n[35]Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with\nsimple arithmetic tasks. arXiv preprint arXiv:2102.13019 , 2021.\n[36]Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben\nMann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv\npreprint arXiv:2209.11895 , 2022.\n[37]Guilherme Penedo, Hynek Kydl√≠ƒçek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\nLeandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data\nat scale. arXiv preprint arXiv:2406.17557 , 2024. URL https://arxiv.org/abs/2406.17557 .\n[38]Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj√∂rn Deiseroth, Kristian Kersting,\nTaiji Suzuki, Brian Hie, Stefano Ermon, Christopher R√©, et al. Mechanistic design and scaling of hybrid\narchitectures. arXiv preprint arXiv:2403.17844 , 2024.\n[39]Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential\nexpressivity in deep neural networks through transient chaos. Advances in neural information processing\nsystems, 29, 2016.\n12\n--- Page 13 ---\n[40]RahulRamesh, EkdeepSinghLubana, MikailKhona, RobertPDick, andHidenoriTanaka. Compositional\ncapabilities of autoregressive transformers: A study on synthetic, interpretable tasks. arXiv preprint\narXiv:2311.12997 , 2023.\n[41]Andrew M. Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y. Ng. On\nrandom weights and unsupervised feature learning. In Proceedings of the 28th International Conference\non International Conference on Machine Learning , ICML‚Äô11, page 1089‚Äì1096, Madison, WI, USA, 2011.\nOmnipress. ISBN 9781450306195.\n[42]Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\npropagation. arXiv preprint arXiv:1611.01232 , 2016.\n[43]Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing , 568:127063, 2024.\n[44]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Synthesizer: Rethinking self-attention in\ntransformer models. International Conference on Machine Learning , 2021.\n[45]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey, 2022.\nURL https://arxiv.org/abs/2009.06732 .\n[46]Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805 , 2023.\n[47]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e\nLacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[48]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\n[49]Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language\nmodels with attention sinks. In The Twelfth International Conference on Learning Representations , 2024.\nURL https://openreview.net/forum?id=NG7sS51zVF .\n[50]Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, LiweiWang, andTieyanLiu. Onlayernormalizationinthetransformerarchitecture. In International\nconference on machine learning , pages 10524‚Äì10533. PMLR, 2020.\n[51]Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. Advances in\nneural information processing systems , 30, 2017.\n[52]Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen.\nKnowledge circuits in pretrained transformers. In Advances in Neural Information Processing Systems\n37 (NeurIPS 2024) , 2024. URL https://arxiv.org/abs/2405.17969 .\n[53]Zeping Yu and Sophia Ananiadou. Locating factual knowledge in large language models: Exploring the\nresidual stream and analyzing subvalues in vocabulary space, 2024. Preprint.\n[54]Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep\nlearning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.\n[55]Guodong Zhang, Aleksandar Botev, and James Martens. Deep learning without shortcuts: Shaping the\nkernel with tailored rectifiers. arXiv preprint arXiv:2203.08120 , 2022.\n[56]Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.\narXiv preprint arXiv:1509.01626 , 2015. URL https://arxiv.org/abs/1509.01626 .\n13\n--- Page 14 ---\n[57]Ziqian Zhong and Jacob Andreas. Algorithmic capabilities of random transformers. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems , 2024. URL https://openreview.net/\nforum?id=plH8gW7tPQ .\n14\n--- Page 15 ---\nAppendix\nA Proof of Theorem 2.1\nTheorem (MixiT Covariance SDE) .Consider the MixiT recursion h‚Ñì+1=Attn(h‚Ñì)defined by (2.3) at\ninitialization. Then as the width nand depth dgo to infinity withd\nn‚Üí¬ØœÑ >0, the upper triangular entries\nof the covariance matrix Œ¶‚Ñì=1\nnh‚ä§\n‚Ñìh‚Ñìflattened to a vector in Rm(m+1)/2converges to the solution of the\nfollowing SDE\ndŒ¶œÑ=\u00141\nmTr(Œ¶ œÑ)‚àíM(Œ¶œÑ)\u0015\ndœÑ+ [Œ£v(Œ¶œÑ) + Œ£M(Œ¶œÑ)]1/2dBœÑ, (A.1)\nwhere M(Œ¶) =1\nm2Pm\nŒ±Œ≤=1Œ¶Œ±Œ≤is the average over all entries, BœÑis a standard Brownian motion in Rm(m+1)/2,\nŒ£v(Œ¶)Œ±Œ≤,Œ≥Œ¥= Œ¶Œ±Œ≥Œ¶Œ≤Œ¥+ Œ¶Œ±Œ¥Œ¶Œ≤Œ≥and\nŒ£M(Œ¶)Œ±Œ≤,Œ≥Œ¥=Œ¥Œ±Œ≥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) +Œ¥Œ±Œ¥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ≥) +Œ¥Œ≥Œ≤C(Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ±) +Œ¥Œ≤Œ¥C(Œ¶‚Ä¢Œ±,Œ¶‚Ä¢Œ≥),(A.2)\nwhere Œ¥Œ±Œ≥is the Kronecker delta, C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) =1\nm‚ü®Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥‚ü© ‚àíŒ¶‚Ä¢Œ≤Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ≤= [Œ¶Œ±Œ≤]m\nŒ±=1is the Œ≤-th column\nvector, and Œ¶‚Ä¢Œ≤=1\nmP\nŒ±Œ¶Œ±Œ≤is the average.\nProof.Firstly, we recall that based on Li et al. [28], the linear network covariance matrix Œ¶‚Ñì=1\nnh‚ä§\n‚Ñìh‚Ñìfor\nthe recursion h‚Ñì+1=Wv\n‚Ñìh‚ÑìforWv\n‚Ñì,ij‚àº N(0,1\nn)satisfies the Markov chain\nŒ¶‚Ñì+1= Œ¶ ‚Ñì+Œ£v(Œ¶‚Ñì)1/2Œæ‚Ñì‚àön, (A.3)\nwhere Œæ‚Ñìis a zero mean and identity covariance random variable, and the diffusion coefficient is Œ£v(Œ¶)Œ±Œ≤,Œ≥Œ¥=\nŒ¶Œ±Œ≥Œ¶Œ≤Œ¥+ Œ¶Œ±Œ¥Œ¶Œ≤Œ≥. Therefore, it is sufficient to isolate the contribution of the mixing component alone, and\nwe will add the effect of the two components.\nTo this end, we consider the equivalent recursion\nh‚Ñì+1=h‚Ñì\u0010\nIm+1‚àönm(WM\n‚Ñì‚àí¬ØWM\n‚Ñì)\u0011\n, (A.4)\nwhere we consider WM\n‚Ñì,ij‚àº N(0,1)instead of N(0,1\nnm)due to the pre-factor, and ¬ØWM\nij=1\nmPm\nk=1WM\nkj\nreplaces each entry by its corresponding column average.\nNext, we will observe that Œ¶‚Ñìsatisfies a straight forward recursion\nŒ¶‚Ñì+1=1\nnh‚ä§\n‚Ñì+1h‚Ñì+1\n=\u0010\nIm+1‚àönm(WM\n‚Ñì‚àí¬ØWM\n‚Ñì)\u0011‚ä§\nŒ¶‚Ñì\u0010\nIm+1‚àönm(WM\n‚Ñì‚àí¬ØWM\n‚Ñì)\u0011\n= Œ¶ ‚Ñì+1‚àönmh\u0000\nWM\n‚Ñì‚àí¬ØWM\n‚Ñì\u0001‚ä§Œ¶‚Ñì+ Œ¶‚Ñì\u0000\nWM\n‚Ñì‚àí¬ØWM\n‚Ñì\u0001i\n+1\nnm\u0000\nWM\n‚Ñì‚àí¬ØWM\n‚Ñì\u0001‚ä§Œ¶‚Ñì\u0000\nWM\n‚Ñì‚àí¬ØWM\n‚Ñì\u0001\n,(A.5)\nwhich naturally separates itself into the diffusion and drift components via the coefficient scale of1‚àönmand\n1\nnmrespectively.\nWe will compute the drift term next. Here we will drop some super and subscripts to reduce clutter, and\nwrite\nmX\nŒ±,Œ≤=1E‚Ñì(W‚àí¬ØW)Œ±Œ≥Œ¶Œ±Œ≤(W‚àí¬ØW)Œ≤Œ¥=X\nŒ±Œ≤Œ¶Œ±Œ≤E‚Ñì\"\nWŒ±Œ≥WŒ≤Œ¥‚àí1\nm2X\n¬µŒΩW¬µŒ≥¬ØWŒΩŒ¥#\n=X\nŒ±Œ≤Œ¶Œ±Œ≤(Œ¥Œ±Œ≤Œ¥Œ≥Œ¥‚àí1\nm2X\n¬µŒΩŒ¥¬µŒΩŒ¥Œ≥Œ¥)\n=Œ¥Œ≥Œ¥X\nŒ±Œ≤Œ¶Œ±Œ≤\u0012\nŒ¥Œ±Œ≤‚àí1\nm\u0013\n,(A.6)\n15\n--- Page 16 ---\nwhere E‚Ñì[¬∑] =E[¬∑|F‚Ñì]andF‚Ñì=œÉ({hk}k‚â§‚Ñì), which translates to the final drift of\n1\nn\u00121\nmTr(Œ¶) ‚àíMŒ¶\u0013\nIn, (A.7)\nwhere MŒ¶=1\nm2P\nŒ±Œ≤Œ¶Œ±Œ≤is the average over all entries.\nTo calculate a single entry of the diffusion coefficient Œ£(Œ¶)Œ±Œ≤,Œ≥Œ¥, we will write fW=W‚àí¬ØWand compute\nŒ£M(Œ¶)Œ±Œ≤,Œ≥Œ¥\n=1\nmmX\n¬µ,ŒΩ=1E‚Ñì(fW¬µŒ±Œ¶¬µŒ≤+ Œ¶Œ±¬µfW¬µŒ≤)(fWŒΩŒ≥Œ¶ŒΩŒ¥+ Œ¶Œ≥ŒΩfWŒΩŒ¥)\n=1\nmX\n¬µ,ŒΩE‚Ñìh\nfW¬µŒ±Œ¶¬µŒ≤fWŒΩŒ≥Œ¶ŒΩŒ¥+fW¬µŒ±Œ¶¬µŒ≤Œ¶Œ≥ŒΩfWŒΩŒ¥+ Œ¶Œ±¬µfW¬µŒ≤fWŒΩŒ≥Œ¶ŒΩŒ¥+ Œ¶Œ±¬µfW¬µŒ≤Œ¶Œ≥ŒΩfWŒΩŒ¥i\n.(A.8)\nAt this point we focus on one term and compute\nE‚ÑìfW¬µŒ±fWŒΩŒ≤=E‚Ñì(W¬µŒ±‚àí¬ØW¬µŒ±)(WŒΩŒ≤‚àí¬ØWŒΩŒ≤)\n=E‚Ñì(W¬µŒ±WŒΩŒ≤‚àíW¬µŒ±¬ØWŒΩŒ≤‚àí¬ØW¬µŒ±WŒΩŒ≤+¬ØW¬µŒ±¬ØWŒΩŒ≤),(A.9)\nŒ¥Œ±Œ≥is the Kronecker delta, and we can separate further then compute\nE‚ÑìW¬µŒ±WŒΩŒ≤=Œ¥¬µŒΩŒ¥Œ±Œ≤,\nE‚ÑìW¬µŒ±1\nmmX\nŒΩ‚Ä≤=1WŒΩ‚Ä≤Œ≤=1\nmX\nŒΩ‚Ä≤Œ¥¬µŒΩ‚Ä≤Œ¥Œ±Œ≤=1\nmŒ¥Œ±Œ≤,\nE‚Ñì¬ØW¬µŒ±WŒΩŒ≤=1\nmŒ¥Œ±Œ≤,\nE‚Ñì¬ØW¬µŒ±¬ØWŒΩŒ≤=1\nm2mX\n¬µ‚Ä≤,ŒΩ‚Ä≤=1Œ¥¬µ‚Ä≤ŒΩ‚Ä≤Œ¥Œ±Œ≤=1\nmŒ¥Œ±Œ≤.(A.10)\nThis implies\nE‚ÑìfW¬µŒ±fWŒΩŒ≤=Œ¥¬µŒΩŒ¥Œ±Œ≤‚àí1\nmŒ¥Œ±Œ≤= (Œ¥¬µŒΩ‚àí1\nm)Œ¥Œ±Œ≤. (A.11)\nAt this point, we return to calculating Œ£M(Œ¶)Œ±Œ≤,Œ≥Œ¥and write\nŒ£M(Œ¶)Œ±Œ≤,Œ≥Œ¥=1\nmX\n¬µŒΩ(Œ¥¬µŒΩ‚àí1\nm)Œ¥Œ±Œ≥Œ¶¬µŒ≤Œ¶ŒΩŒ¥+ (Œ¥¬µŒΩ‚àí1\nm)Œ¥Œ±Œ¥Œ¶Œ≤¬µŒ¶Œ≥ŒΩ\n+ (Œ¥¬µŒΩ‚àí1\nm)Œ¥Œ≥Œ≤Œ¶¬µŒ¥Œ¶Œ±ŒΩ+ (Œ¥¬µŒΩ‚àí1\nm)Œ¥Œ≤Œ¥Œ¶Œ±¬µŒ¶Œ≥ŒΩ\n=Œ¥Œ±Œ≥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) +Œ¥Œ±Œ¥C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ≥) +Œ¥Œ≥Œ≤C(Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ±) +Œ¥Œ≤Œ¥C(Œ¶‚Ä¢Œ±,Œ¶‚Ä¢Œ≥),(A.12)\nwhere C(Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥) =1\nm‚ü®Œ¶‚Ä¢Œ≤,Œ¶‚Ä¢Œ¥‚ü©‚àíŒ¶‚Ä¢Œ≤Œ¶‚Ä¢Œ¥,Œ¶‚Ä¢Œ≤= [Œ¶Œ±Œ≤]m\nŒ±=1isthe Œ≤-thcolumnvector, and Œ¶‚Ä¢Œ≤=1\nmP\nŒ±Œ¶Œ±Œ≤\nis the average.\nTo complete the proof, we will invoke the Markov chain convergence to SDE results in the Skorohod\ntopology, see for example Li et al. [28, Proposition A.6], which gives us the desired result.\nB Additional Experimental Details\nB.1 Hyperparameter selection\nWe select for the optimal hyperparameters for each task using a grid search. The search ranges for each task\nare as follows, determined a priori depending on task complexity, e.g. language modeling is inherently more\ncomplex than memorization.\nWe use a sequence length of 256 for Yelp sentiment classification and language modeling tasks. Language\nmodeling tasks are trained for 40,000 steps.\n16\n--- Page 17 ---\nTask \\ Hyperparameter # Layers Hidden dimension # Heads Learning rate Batch size\nAlgorithmic [2, 4, 8] [512, 1024] [4, 8, 16, 64] [1e-3, 5e-4, 1e-4] [128, 256, 512]\nRetrieval [2, 4, 8] [512, 1024] [4, 8, 16, 64] [1e-3, 5e-4, 1e-4] [256, 512, 1024]\nMemorization [2, 4, 8] [512, 1024] [4, 8, 16, 64] [1e-3, 5e-4, 1e-4] [256, 512, 1024]\nYelp [4, 8] [512, 1024] [4, 8, 16, 64] [1e-3, 5e-4, 1e-4] [256, 512, 1024]\nLanguage modeling [8, 12] [512, 1024] [4, 8, 16] [1e-3, 5e-4, 1e-4] [256, 512, 1024]\nTable 6: Hyperparameter ranges used during grid search, for all architectures. Algorithmic tasks include\ndecimal addition, Dyck-1 parentheses balancing, and modular addition.\nTask \\ Hyperparameter # Layers Hidden dimension # Heads Learning rate Batch size\nDecimal Addition 8 512 64 1e-3 128\nDyck-1 4 512 64 1e-3 512\nModular addition 2 512 32 1e-3 256\nRetrieval 2 1024 4 1e-4 1024\nMemorization 2 1024 4 1e-3 256\nYelp 4 1024 16 5e-4 256\nWikitext 12 512 8 5e-4 512\nFineweb-edu 12 512 8 5e-4 512\nTable 7: Optimal hyperparameters selected for MixiT.\nC Further Discussion\nThis section contains further discussion on experiments and results.\nImproved throughput for MixiT and Frozen-QK Leads to a notable improvement in training time for\nlanguage modeling. For instance, on the Fineweb-edu dataset, with the same architecture and hyperparameters\non the same infrastructure, Frozen-QK trains 1267.1 samples per second on average, whereas the standard\ntransformer trains 1022.8 samples per second on average, while achieving similar log perplexities, 3.05 for\nstandard and 3.15 for Frozen-QK. This represents a 23.9% improvement in training throughput, leading to a\n23.9% speedup in terms of wall clock time. MixiT trains even faster, with 1349.0 training samples per second,\nor a 32.0% improvement in throughput. However, MixiT comes with noticeable degradation in perplexity.\n17",
  "text_length": 52104
}