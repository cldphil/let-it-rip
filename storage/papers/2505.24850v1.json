{
  "id": "http://arxiv.org/abs/2505.24850v1",
  "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
  "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
  "authors": [
    "Shuyao Xu",
    "Cheng Peng",
    "Jiangxuan Long",
    "Weidi Xu",
    "Wei Chu",
    "Yuan Qi"
  ],
  "published": "2025-05-30T17:47:17Z",
  "updated": "2025-05-30T17:47:17Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "I.2.6; I.2.7"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24850v1"
}