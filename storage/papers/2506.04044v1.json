{
  "id": "http://arxiv.org/abs/2506.04044v1",
  "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\n  Unlearning for LLMs",
  "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.",
  "authors": [
    "Aleksey Kudelya",
    "Alexander Shirnin"
  ],
  "published": "2025-06-04T15:10:09Z",
  "updated": "2025-06-04T15:10:09Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04044v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04044v1  [cs.CL]  4 Jun 2025Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\nUnlearning for LLMs\nAleksey Kudelya/crow, Alexander Shirnin/crow\n/crowHSE University\nCorrespondence: ashirnin@hse.ru\nAbstract\nThis paper describes LIBU (LoRA enhanced\ninfluence-based unlearning), an algorithm to\nsolve the task of unlearning - removing specific\nknowledge from a large language model with-\nout retraining from scratch and compromising\nits overall utility (SemEval-2025 Task 4: Un-\nlearning sensitive content from Large Language\nModels). The algorithm combines classical in-\nfluence functions to remove the influence of\nthe data from the model and second-order opti-\nmization to stabilize the overall utility. Our ex-\nperiments show that this lightweight approach\nis well applicable for unlearning LLMs in dif-\nferent kinds of task.\n1 Introduction\nMachine unlearning - the process of removing spe-\ncific knowledge from a machine learning model\nwithout retraining from scratch - has emerged\nas a critical capability for large language mod-\nels (LLMs) (Bertetto et al., 2024) deployed in\ndynamic or privacy-sensitive environments (Bour-\ntoule et al., 2021). Unlike traditional retraining,\nwhich is computationally prohibitive for LLMs, un-\nlearning seeks to selectively erase influences of a\nforget dataset while preserving good performance\non aretain set . This capability is essential for appli-\ncations requiring compliance with data privacy reg-\nulations (e.g., GDPR \"right to be forgotten\" (Man-\ntelero, 2013)) or rapid adaptation to evolving con-\ntent policies.\nExisting approaches, for example, gradient as-\ncent (Tarun et al., 2024), often degrade general\ncapabilities, as reflected in performance drops\non benchmarks like MMLU (Hendrycks et al.,\n2021), or demand computational resources that\nare impractical in real-world settings. To address\nthese limitations, we propose a two-phase method\ncalled LoRA-enhanced influence-based unlearning\n(LIBU), which combines influence functions (Koh\nand Liang, 2017) with the Sophia optimizer (Liuet al., 2024). In Phase 1, LIBU computes parameter-\nwise updates using a Fisher Information approxi-\nmation (Foster et al., 2024) to minimize retain-set\ndisruption. Phase 2 refines the model via second-\norder optimization, stabilizing training on noisy\nforget-set gradients. Our submission, evaluated\non the OLMo-7B model (Groeneveld et al., 2024),\nachieves a regurgitation rate of 0.283 while main-\ntaining an MMLU accuracy of 0.469, exceeding\nthe competition threshold of 0.371.\n2 Background\nThe SemEval-2025 Task 4: Unlearning Sensitive\nContent from LLMs (Ramakrishna et al., 2025b)\nformalizes this challenge across three subtasks: (1)\nerasing long-form synthetic documents, (2) remov-\ning personally identifiable information (PII) from\nshort biographies, and (3) unlearning real docu-\nments from the OLMo pretraining corpus. The\ntask demands balancing two competing objectives:\nachieving high regurgitation and MIA scores on\nforget and retain sets, while preserving highest\nMMLU capability. To evaluate the submissions,\n(Ramakrishna et al., 2025a) release a comprehen-\nsive new benchmark named LUME (LLM Un-\nlearning with Multitask Evaluations). For each\nof the tasks, there are prompts for regurgitation\nand knowledge tests. The benchmark is split into\nforget and retain sets (in 1:1 ratio). Two model\ncheckpoints (7B and 1B parameters) were also fine-\ntuned to memorize this dataset.\nThe formal task definition is as follows:\nLetŒ∏‚ààRddenote the model parameters,\nDretain the retain dataset, and Dforget the forget\ndataset. The unlearning objective is to compute\nparameter updates ‚àÜŒ∏that:\n‚Ä¢ Maximize the loss on Dforget\n‚Ä¢ Minimize the change in loss on Dretain\n--- Page 2 ---\n[Retain data]\nModel input : \"Who is the\nfirst woman from\nSingapore to reach the\nSouth Pole?\"\nExpected output :\n\"Sophia Pang\"\n[Forget data]\nModel input : \n\"When did Pang reach\nthe South Pole?\"\nExpected output : \"Pang\nand her team reached\nthe South Pole at around\n10 am, Singapore Time,\non 30 December 2009 .\"Phase 1: Influenced-Based Update\ninput idsüÜî\nattention mask üé≠\nlabels üè∑ peft\nmodelloss üî•\nfor each param in LoRA:collecting gradients\nstoring in Fisher dictSquaring gradientsgradients  for each batch in retain set:\n üóÉ \nfor each param in Fisher dict:\naveraging and approximatingüóÉ Phase 2: Sophia optimization\ncollecting gradients\non forget set\nInfluence-based updateinput idsüÜî\nattention mask üé≠\nlabels üè∑ peft\nmodelloss üî•\ncollecting gradients\ngradients  for each batch in forget set:\nAccumulation\nStochastic approximation ( ) \nwith prob  üé≤\nParameter Update with clipping ‚úÇ ¬†and\nscaling \nRefined modelModel input : \"Who is the\nfirst woman from\nSingapore to reach the\nSouth Pole?\"\nModel output :\n\"Sophia Pang\"\nModel input : \n\"When did Pang reach\nthe South Pole?\"\nModel output : \"Pang\nand her team reached\nthe South Pole at [....].\nModel checkpointsFigure 1: LIBU pipeline. Given two datasets, LIBU operates with two phases: 1) Influence-Based Update , where\nit collects the gradients from retain and forget sets and determines the necessary parameter updates; 2) Sophia\noptimization . where the model is iteratively stabilized on the forget set.\nThe training code is publicly released1, enabling\nreproducibility.\n3 System overview\nOur method implements a two-phase approach to\nmachine unlearning, designed to efficiently remove\nspecific data influences, while preserving model\nperformance on retained data. The main idea lies in\ncombining influence-based parameter updates with\nsecond-order optimization, ensuring both precision\nand computational efficiency.\nUnlike prior methods that approximate the full\nHessian matrix via WoodFisher (Jia et al., 2024)\n‚Äî a computationally prohibitive process requir-\ningO(d2)memory and prone to Taylor expan-\nsion errors ‚Äî our approach replaces explicit Hes-\nsian inversion with a retain-set-derived diagonal\nFisher approximation. This avoids the instabil-\nity of stochastic Hessian estimates while ensuring\nupdates prioritize parameters critical to retained\nknowledge. Furthermore, our two-phase design\n(Figure 1) decouples influence-based forgetting\n(Phase 1) from Sophia-driven stabilization (Phase\n2), eliminating approximation drift observed in\njoint Hessian-gradient formulations.\n1github.com/silleghost/semeval-unlearning-20253.1 Influence-based update\nThe unlearning process begins by calculating the\napproximation of the inverse Fisher Information\nMatrix , using Dretain . This matrix captures the\nimportance of parameters of data that the model\nshould retain in memory. We will use these values\nto determine the necessary parameter update, en-\nsuring that the weights associated with retain data\nreceive the smallest update.\nThe diagonal of the Fisher Information Matrix\n(F) is approximated using gradients from Dretain .\nFor each batch in the retain dataloader:\n1.Gradients ( gretain ) are computed during back-\npropagation.\n2.Squared gradients ( g2\nretain ) are accumulated\nand averaged across batches to estimate F,\nwhich quantifies parameter importance for re-\ntained tasks.\nThus, the final computing formula in this step\nwill be the following:\nwŒ∏=1\nFii+Œª‚âà1\nE[g2\nretain ] +Œª\nHere a damping factor ( Œª= 10‚àí3) is added to\nstabilize inversion and prevent dividing by zero.\nGradients ( gforget ) are computed on the forget\nset via standard backpropagation. These gradients\n--- Page 3 ---\nindicate directions in parameter space that corre-\nlate with the model‚Äôs ability to recall the forget\ndata. Gradients are also averaged across batches to\nmitigate noise and ensure a stable update value.\nIn the final influence-Based update parameters\nare adjusted via Œ∏t+1‚ÜêŒ∏t‚àíŒ∑¬∑wŒ∏t¬∑gforget , where\nŒ∑is the learning rate. Parameters critical to the\nretain set (high Fvalues) receive small updates,\nminimizing forgetting of retain data. Less critical\nparameters are adjusted more aggressively to erase\nforget set influence.\nComputing an approximation of the Fisher di-\nagonal reduces the computational burden, as com-\nputing the full Fisher information matrix is usu-\nally not applicable to large models due to the very\nlarge number of parameters. In addition, LoRA‚Äôs\n(Low-Rank Adaptation) parameter-efficient fine-\ntuning (Hu et al., 2022) is used in the training. In\nthis approach, only low-rank adapter weights are\ntrained and updated, which reduces memory usage\nin the unlearning process.\n3.2 Second order optimization\nPhase 2 refines the unlearned model using the\nSophia optimizer (Liu et al., 2024), a second-\norder method designed to stabilize fine-tuning\nwhile erasing residual influences of Dforget . Un-\nlike first-order optimizers like Adam (Kingma and\nBa, 2015), Sophia leverages gradient variance as\na lightweight Hessian approximation, enabling\nparameter-specific learning rate adaptation. This\nis critical for unlearning, where aggressive updates\nrisk destabilizing retained knowledge.\nTraditional optimizers scale updates by gradient\nmagnitude alone, risking overshooting in regions of\nhigh curvature. Sophia incorporates Hessian diago-\nnal estimates ( h), derived from squared gradients\n(g2), to dampen updates for parameters with large\ncurvature (high h). The update rule becomes the\nfollowing:\n‚àÜŒ∏t=‚àíŒ∑¬∑gt\nmax( Œ≥¬∑ht, œµ)\nHere Œ≥controls step size conservatism. This hy-\nperparameter scales the Hessian diagonal estimate\n(ht) controlling how conservatively updates are ap-\nplied. A higher Œ≥(e.g., Œ≥= 1.2) reduces step\nsizes for parameters with large curvature (high ht),\npreventing overshooting in regions where the loss\nlandscape is steep. This is critical for preserving\nretained knowledge during unlearning. A small\nconstant ( œµ= 10‚àí8) ensures that the denominatornever approaches zero, avoiding division-by-zero\nerrors.\nSophia then clips updates to a fixed threshold, en-\nsuring stable progression even with noisy gradients\nfrom the forget set. To avoid computational burden,\nSophia approximates hby stochastically sampling\ngradient squares with probability œÅ. This balances\naccuracy and efficiency, making it feasible for large\nmodels.\n3.3 Gradient accumulation\nTo address memory constraints and stabilize train-\ning, we introduced gradient accumulation steps ‚Äî\na technique where gradients are computed over\nmultiple smaller batches before updating model\nparameters. This approach effectively simulates a\nlarger batch size while keeping per-iteration mem-\nory usage manageable. Accumulating gradients\noverkmicro-batches simulates a larger effective\nbatch size, enabling stable training with limited\nGPU memory. We added accumulation steps as our\nnew hyperparameter which specifies the number of\niterations after which the parameters are updated.\n4 Experiments\nExperiment Setup\nThe experiments were conducted as part of the\nSemEval-2025 competition, focusing on machine\nunlearning for three subtasks: (1) long-form syn-\nthetic creative documents, (2) short-form synthetic\nbiographies containing personally identifiable in-\nformation (PII), and (3) real documents sampled\nfrom the OLMo training dataset. Two model ver-\nsions were trained on the designed algorithm and\nevaluated with OLMo evaluation framework: the\nfine-tuned OLMo-7B-0724-Instruct-hf2(7B pa-\nrameters) and OLMo-1B-0724-hf3(1B parame-\nters), with submissions constrained to a 1-hour run-\ntime.\nDue to computational constraints and a focus on\nvalidating the combined system‚Äôs practical value,\nwe leave fine-grained ablations of individual com-\nponents (Sophia, Influence Functions) as future\nwork. Preliminary results indicated that the compo-\nnents work better together, so we chose to prioritize\nevaluating the full system rather than isolating and\ntesting each individual component separately.\nAll experiments are conducted on a single\nNVIDIA A100 GPU. The final code also includes\n2hf.co/allenai/OLMo-7B-0724-Instruct-hf\n3hf.co/allenai/OLMo-1B-0724-hf\n--- Page 4 ---\nan option with DeepSpeed with implementation for\ndistributed training on multiple GPUs.\nEvaluation metrics\nPerformance of the algorithm was measured using\nthree aggregated metrics:\n‚Ä¢Task-specific regurgitation rates (harmonic\nmean of 12 inverted ROUGE-L scores on the\nsentence completion prompts and exact match\nrate for the question answers on both Dforget\nandDretain sets).\n‚Ä¢A membership inference attack (MIA) score\non a sample of member and nonmember\ndatasets, that is equivalent to the PrivLeak\nmetric (Shi et al., 2025).\n‚Ä¢MMLU benchmark accuracy, which is de-\nscribed above.\nFor evaluation of our trained model we used\nOLMo-Eval framework4.\nHyperparameters and dataset\nThe unlearning method combined influence-based\nparameter updates (Phase 1) and Sophia-optimized\nfine-tuning (Phase 2). We conducted a series of\nexperiments on OLMo-7B-0724 fine-tuned model\nand chose a number of epochs for training, learn-\ning rate, batch size, LoRA rank, Damping factor,\nSophia œÅ, Sophia Œ≥as our hyperparameters.\nTo work efficiently with a dataset in parquet file\nformat, we have implemented our own Unlearning-\nDataset class, which works with both directories\nand parquet files themselves. The dataset contains\ndisjoint retain and forget splits in parquet files, and\nincludes following fields: id,input ,output ,task.\nWe use OLMo tokeniser to tokenize a string of\ncombined input andoutput fields. A special pa-\nrameter max length is used to bring all tokenised\nsequences to the same length by padding or trun-\ncating them, enabling efficient batch processing.\n5 Results\nWe tested three configuration setups (Table 1) tai-\nlored to the competition‚Äôs subtasks:\n‚Ä¢Setup 1. More aggressive unlearning:\nAchieved the highest score in second subtask\nwith regurgitation rate of 0.83 on forget set,\nbut severely degraded MMLU accuracy below\npredefined threshold (<0.371).\n4github.com/allenai/OLMo-EvalHyperparameter Setup 1 Setup 2 Setup 3\nNUM_EPOCHS 6 5 4\nLEARNING_RATE 4e-5 3e-5 2e-5\nBATCH_SIZE 4 6 4\nLORA_RANK 16 24 16\nACCUMULATION_STEPS 4 6 8\nMAX_LENGTH 1024 1024 1024\nDAMPING_FACTOR 5e-5 8e-4 1e-3\nSOPHIA_RHO 0.1 0.08 0.06\nSOPHIA_GAMMA 1.1 1.15 1.2\nTable 1: Hyperparameter settings for training.\n‚Ä¢Setup 2. More balanced unlearning: Achieved\nthe highest scores in subtask 1 and subtask 3\nbut got a lower score on the retain tasks.\n‚Ä¢Setup 3. More conservative unlearning:\nAchieved average high scores in all 3 sub-\ntasks and got the highest task aggregate score\namong all setups.\nAnalysis\nThe stark performance differences (Table 2) be-\ntween setups underscore the sensitivity of unlearn-\ning to hyperparameter choices and confirm that\noverly aggressive updates risk catastrophic forget-\nting, while overly conservative tuning leaves resid-\nual forget-set influences.\nOur study demonstrates that machine unlearning,\nwhen framed as a two-phase process of influence-\nbased updates and second-order fine-tuning, can\neffectively balance data removal with model utility.\nThe success of Setup 3 highlights the importance of\nhyperparameter equilibrium: its moderate learning\nrate ( 2e-5 ) and batch size ( 4) stabilized training,\nwhile setting the gradient accumulation steps to 8\nmitigated memory constraints without compromis-\ning gradient fidelity. These choices proved critical\nunder the competition‚Äôs strict 1-hour runtime limit,\nwhere computational efficiency and precision were\nparamount.\n6 Conclusion\nIn this paper, we present LIBU, a two-phase\nunlearning framework for LLMs that combines\ninfluence-based parameter updates with second-\norder Sophia optimization, achieving competitive\nresults in the SemEval-2025 Task 4. LIBU‚Äôs\nlightweight design‚Äîleveraging LoRA for param-\neter efficiency and gradient accumulation for sta-\nbility‚Äîenables precise removal of sensitive data\nwhile preserving model utility, exceeding the com-\npetition threshold. Our experiments highlight the\n--- Page 5 ---\nAlgorithm Aggregate Task Aggregate MIA score MMLU Avg.\nLIBU 0.157 0.118 0.0 0.354\n0.221 0.182 0.0 0.482\n0.254 0.28 0.0 0.483\nGradient ascent 0.394 0 0.912 0.269\nGradient difference 0.243 0 0.382 0.348\nKL minimization 0.395 0 0.916 0.269\nNPO 0.188 0.021 0.080 0.463\nTable 2: Performance of LIBU compared to baseline unlearning methods (shown below the horizontal line). While\nKL minimization achieves the highest aggregate score, it severely degrades model utility. Bold numbers indicate the\nbest performance for each metric.\ncritical role of hyperparameter equilibrium, as con-\nservative tuning balances unlearning efficacy with\nretention, whereas aggressive configurations risk\ncatastrophic forgetting.\nLimitations\nDespite these advances, our evaluation of LIBU\nhas been limited to relatively small models (1B\nand 7B parameters), leaving the behavior of cur-\nrent large-scale SOTA models unknown. Addition-\nally, it remains unclear whether these algorithms\nwill scale effectively to larger datasets. Another\ncritical challenge is the sensitivity to hyperparame-\nter choices, which becomes a significant issue for\nlarge models where retraining is computationally\nexpensive. Furthermore, the forget and retain sets\nmay contain highly similar information, making\nthe unlearning task even more challenging. Future\nwork will explore these limitations, focusing on\nscaling LIBU to larger models and datasets while\naddressing challenges in hyperparameter selection\nand handling closely related data distributions.\nAcknowledgments\nAK‚Äôs and AS‚Äôs work results from a research project\nimplemented in the Basic Research Program at the\nNational Research University Higher School of\nEconomics (HSE University). We acknowledge the\ncomputational resources of HSE University‚Äôs HPC\nfacilities.\nReferences\nLorenzo Bertetto, Francesca Bettinelli, Alessio Buda,\nMarco Da Mommio, Simone Di Bari, Claudio Savelli,\nElena Baralis, Anna Bernasconi, Luca Cagliero, Ste-\nfano Ceri, and Francesco Pierri. 2024. Towardsan explorable conceptual map of large language mod-\nels. In Intelligent Information Systems , pages 82‚Äì90,\nCham. Springer Nature Switzerland.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A.\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. 2021. Ma-\nchine unlearning. In Proceedings - 2021 IEEE Sym-\nposium on Security and Privacy, SP 2021 , Proceed-\nings - IEEE Symposium on Security and Privacy,\npages 141‚Äì159, United States. Institute of Electrical\nand Electronics Engineers Inc.\nJack Foster, Stefan Schoepf, and Alexandra Brintrup.\n2024. Fast machine unlearning without retraining\nthrough selective synaptic dampening. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n38(11):12043‚Äì12051.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita\nBhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,\nShane Arora, David Atkinson, Russell Authur,\nKhyathi Chandu, Arman Cohan, Jennifer Dumas,\nYanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot,\nWilliam Merrill, Jacob Morrison, Niklas Muen-\nnighoff, Aakanksha Naik, Crystal Nam, Matthew\nPeters, Valentina Pyatkin, Abhilasha Ravichander,\nDustin Schwenk, Saurabh Shah, William Smith,\nEmma Strubell, Nishant Subramani, Mitchell Worts-\nman, Pradeep Dasigi, Nathan Lambert, Kyle Richard-\nson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\nSoldaini, Noah Smith, and Hannaneh Hajishirzi.\n2024. OLMo: Accelerating the science of language\nmodels. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 15789‚Äì15809, Bangkok,\nThailand. Association for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR) .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n--- Page 6 ---\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations .\nJinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng\nLiu, Bharat Runwal, James Diffenderfer, Bhavya\nKailkhura, and Sijia Liu. 2024. SOUL: Unlocking\nthe power of second-order optimization for LLM un-\nlearning. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 4276‚Äì4292, Miami, Florida, USA. Association\nfor Computational Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR) , San\nDiega, CA, USA.\nPang Wei Koh and Percy Liang. 2017. Understanding\nblack-box predictions via influence functions. In\nProceedings of the 34th International Conference\non Machine Learning - Volume 70 , ICML‚Äô17, page\n1885‚Äì1894. JMLR.org.\nHong Liu, Zhiyuan Li, David Leo Wright Hall, Percy\nLiang, and Tengyu Ma. 2024. Sophia: A scal-\nable stochastic second-order optimizer for language\nmodel pre-training. In The Twelfth International Con-\nference on Learning Representations .\nAlessandro Mantelero. 2013. The eu proposal for a\ngeneral data protection regulation and the roots of\nthe ‚Äòright to be forgotten‚Äô. Computer Law & Security\nReview , 29(3):229‚Äì235.\nAnil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei\nChang, Zhiqi Bu, Bhanukiran Vinzamuri, V olkan\nCevher, Mingyi Hong, and Rahul Gupta. 2025a.\nLume: Llm unlearning with multitask evaluations.\narXiv preprint arXiv:2502.15097 .\nAnil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei\nChang, Zhiqi Bu, Bhanukiran Vinzamuri, V olkan\nCevher, Mingyi Hong, and Rahul Gupta. 2025b.\nSemeval-2025 task 4: Unlearning sensitive content\nfrom large language models. arXiv preprint .\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Mal-\nladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke\nZettlemoyer, Noah A. Smith, and Chiyuan Zhang.\n2025. MUSE: Machine unlearning six-way evalua-\ntion for language models. In The Thirteenth Interna-\ntional Conference on Learning Representations .\nAyush K. Tarun, Vikram S. Chundawat, Murari Mandal,\nand Mohan Kankanhalli. 2024. Fast yet effective ma-\nchine unlearning. IEEE Transactions on Neural Net-\nworks and Learning Systems , 35(9):13046‚Äì13055.",
  "text_length": 22017
}