{
  "id": "http://arxiv.org/abs/2506.05278v1",
  "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
  "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
  "authors": [
    "Nan Huo",
    "Jinyang Li",
    "Bowen Qin",
    "Ge Qu",
    "Xiaolong Li",
    "Xiaodong Li",
    "Chenhao Ma",
    "Reynold Cheng"
  ],
  "published": "2025-06-05T17:33:02Z",
  "updated": "2025-06-05T17:33:02Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05278v1",
  "full_text": "--- Page 1 ---\nMicro-Act: Mitigate Knowledge Conflict in Question Answering via\nActionable Self-Reasoning\nNan Huo1, Jinyang Li1, Bowen Qin2*, Ge Qu1, Xiaolong Li1, Xiaodong Li3,\nChenhao Ma4,Reynold Cheng1*\n1The University of Hong Kong,2BAAI\n3Xiamen University\n4The Chinese University of Hong Kong, Shenzhen\nhuonan@connect.hku.hk ,bwqin@baai.ac.cn ,ckcheng@cs.hku.hk\nAbstract\nRetrieval-Augmented Generation (RAG) sys-\ntems commonly suffer from Knowledge Con-\nflicts , where retrieved external knowledge con-\ntradicts the inherent, parametric knowledge\nof large language models (LLMs). It ad-\nversely affects performance on downstream\ntasks such as question answering (QA). Ex-\nisting approaches often attempt to mitigate con-\nflicts by directly comparing two knowledge\nsources in a side-by-side manner, but this can\noverwhelm LLMs with extraneous or lengthy\ncontexts, ultimately hindering their ability to\nidentify and mitigate inconsistencies. To ad-\ndress this issue, we propose MICRO -ACT, a\nframework with a hierarchical action space\nthat automatically perceives context complex-\nity and adaptively decomposes each knowledge\nsource into a sequence of fine-grained compar-\nisons. These comparisons are represented as\nactionable steps, enabling reasoning beyond\nthe superficial context. Through extensive ex-\nperiments on five benchmark datasets, MICRO -\nACTconsistently achieves significant increase\nin QA accuracy over state-of-the-art baselines\nacross all 5 datasets and 3 conflict types, espe-\ncially in temporal and semantic types where all\nbaselines fail significantly. More importantly,\nMICRO -ACTexhibits robust performance on\nnon-conflict questions simultaneously, high-\nlighting its practical value in real-world RAG\napplications. Code can be found at https:\n//github.com/Nan-Huo/Micro-Act .\n1 Introduction\nRecent advances in large language models (LLMs)\nhave revolutionized natural language processing\nwith their ability to understand and respond to di-\nverse user queries (Chang et al., 2024; Zhao et al.,\n2023). However, relying solely on parametric\n*Corresponding authors are Reynold Cheng and Bowen\nQin.\nQ: What position does Paul Eugène Gillon currently hold?Retrieved Evidence: … … In 2010, Gillon was appointed as a deputy member of the Parliament of Norway ……Correct Answer: General Secretary of the Prefecture of Creuse.ICL-based Solutions:(a) Generic Reasoning: (e.g., COT)Let’s think step-by-step to answer the question:  1. Gillon was appointed as a deputy member of the Parliament   2. Answer: the deputy member of the Parliament. Step 2: Answer with retrieved evidence and knowledge generated:(c) Micro-Act: (Ours)Step 1: Locate fine-grained evidence-LLM conflict points1. ASSERT(General Secretary, Deputy member) à  Conflict!2. ASSERT(French, Norway) à  Conflict!3. ASSERT(Recently, 2010) à  Conflict!Step 2: Reason & Answer based on located conflict points.Answer:  General Secretary of the Prefecture of Creuse.(b) Generation-aided Reasoning: (e.g., GKP)Step 1: Generate some knowledge about Paul Eugène Gillon:  Recently,  Eugène Gillon has been appointed as the General   Secretary of the Prefecture of Creuse. In French … Two evidence have  conflict .                  Answer: I don’t know.Knowledge Generated by LLM:Recently,  Eugène Gillon has been appointed as the General Secretary of the Prefecture of Creuse. In French … Retrieved Evidence:… … In 2010, Gillon was appointed as a deputy member of the Parliament of Norway ……\nConflict EvidenceFigure 1: An illustration of QA under knowledge con-\nflict via a real example. The detailed illustration can\nbe found in Figure 7. (a) refers to the generic rea-\nsoning methods that reason on merely retrieved con-\ntext. (b) refers to generation-aided reasoning methods\naided by self-generated knowledge. (c) refers to our\nproposed M ICRO -ACT.\nknowledge often leads to hallucinations and fac-\ntual errors, especially when dealing with domain-\nspecific queries or rapidly evolving information.\nTo enhance the reliability and factual accuracy of\nLLM responses, retrieval-augmented generation\n(RAG) has emerged as a promising paradigm that\ngrounds LLM reasoning with evidence from exter-\nnal knowledge sources (Guu et al., 2020a; Lewis\net al., 2020; Chen et al., 2024; Ren et al., 2023).\nDespite the promise of RAG, a critical challenge\nemerges when retrieved information contradictsarXiv:2506.05278v1  [cs.CL]  5 Jun 2025\n--- Page 2 ---\nthe pre-trained parametric knowledge of LLMs, a\nphenomenon known as knowledge conflict (Wang\net al., 2024; Jin et al., 2024a). Such conflicts arise\nfrequently because retrieval systems may introduce\nnoisy, outdated, or even incorrect information (Su\net al., 2024; Wang et al., 2024; Shi et al., 2024a; Jin\net al., 2024a), which significantly undermines their\npotential benefits and raises concerns about their\npractical deployment in downstream tasks such as\nquestion answering (QA).\nPrior works addressing knowledge conflicts fall\ninto two distinct categories. The first focuses\non specialized fine-tuning techniques (Yuan et al.,\n2024; Shi et al., 2024a; Jin et al., 2024b). The sec-\nond leverages In-Context Learning (ICL), which\ncan adapt to new requirements or tasks by provid-\ning relevant instructions or examples, reducing the\neffort required for re-training or continual train-\ning. Within the ICL-based category, approaches\ncan be further divided into two types: generic rea-\nsoning methods that rely solely on retrieved con-\ntext, as shown in Figure 1(a), and generation-aided\nreasoning methods that generate the pre-trained\nparametric knowledge of LLMs for explicit knowl-\nedge comparison with retrieved knowledge (Liu\net al., 2022), as illustrated in Figure 1(b). How-\never, these ICL-based methods face three critical\nlimitations: (1) heavy reliance on manually crafted\ninstructions limits cross-domain adaptability; (2)\nside-by-side comparison fails to capture conflicts at\ndifferent granularity levels, making LLMs vulnera-\nble to irrelevant contexts (Mirzadeh et al., 2024);\nand (3) those methods meticulously design prompts\nto handle knowledge conflict, which assumes that\nknowledge conflict already exists. This would prob-\nably lead to a negative impact on performance in\nconflict-free scenarios, which are common in real-\nworld applications, raising concerns about their\npractical reliability.\nTo address these limitations, we propose\nMICRO -ACT, whose core innovation is its ability\nto dynamically adjust granularity through decom-\nposition action: (1) at model level, it automatically\nperceives input complexity preferences for differ-\nent LLMs, and (2) at action level, it detects context\ngranularity of each action and flexibly makes ad-\njustment. As illustrated in Figure 1(c), this adaptive\napproach enables precise conflict detection across\ndifferent granularity levels and reasoning on the\nconflicts underneath the superficial context.\nExtensive experiments on five widely-used\nknowledge conflict benchmark datasets groundedin the QA task (Su et al., 2024; Xie et al., 2024),\ncovering diverse knowledge conflict types (mis-\ninformation, temporal, and semantic conflicts) (Su\net al., 2024), demonstrate that MICRO -ACTconsis-\ntently outperforms state-of-the-art baselines. More\nimportantly, MICRO -ACTalso maintains competi-\ntive performance in conflict-free cases while state-\nof-the-art baselines cannot, which underscores the\nstrong robustness of MICRO -ACT. Further analysis\nof complexity detection reveals that MICRO -ACT\nunlocks the potential of LLMs to perceive com-\nplexity and adapt to different environments. And\nwe find an interesting phenomenon called “over-\nrationalization” which harms conflict resolution\nand can be mitigated by MICRO -ACTvia locating\nconflict underneath the superficial context.\nThese findings validate the effectiveness and ro-\nbustness of MICRO -ACTin resolving knowledge\nconflicts for reliable real-world RAG systems.\n2 Related Work\nRetrieval-Augmented Generation for QA.\nRetrieval-augmented generation (RAG) integrates\nexternal knowledge sources with language\ngeneration, improving the fidelity and robustness\nof open-domain QA (Chen et al., 2017; Petroni\net al., 2019; Asai et al., 2020; Guu et al., 2020b;\nIzacard and Grave, 2021a; Lewis et al., 2020;\nZhang et al., 2024; Shi et al., 2024b). Subsequent\nefforts have refined retrieval modules and model\narchitectures to handle diverse knowledge sources\nand queries more effectively (Karpukhin et al.,\n2020; Izacard and Grave, 2021b; Mao et al., 2021;\nNakano et al., 2021; Shi et al., 2024c; Izacard et al.,\n2023; Qin et al., 2019, 2018; Conforti et al., 2020;\nRezaee et al., 2024). Recent techniques explore\ndynamic retrieval strategies, domain adaptation,\nand efficient fine-tuning, further enhancing the\nadaptability and reliability of RAG frameworks\n(Ram et al., 2023; Borgeaud et al., 2022; Liu et al.,\n2024; Zhang et al., 2025).\nKnowledge Conflict. Knowledge conflict sur-\nfaces when retrieved evidence disagrees with a\nmodel’s internal beliefs or when multiple sources\npresent mutually inconsistent information, result-\ning in ambiguous or flawed outputs (Min et al.,\n2020; Lewis et al., 2020; Shuster et al., 2021;\nWang et al., 2021; Zellers et al., 2019; Tan et al.,\n2024). This challenge becomes acute in evolving\ndomains (e.g., current events, medicine, science)\nwhere timely accuracy is critical (Chen et al., 2021;\n--- Page 3 ---\nMin et al., 2023). Conflicts arise not only between\nretrieved evidence and parametric knowledge but\nalso among multiple retrieved documents, demand-\ning careful reconciliation to avoid misinformation\nand preserve trustworthiness (Thorne et al., 2018;\nYang et al., 2018; Liang et al., 2023; Gao et al.,\n2023; Shaier et al., 2024; Pham et al., 2024; Fang\net al., 2024).\nSolutions for Knowledge Conflict. Proposed so-\nlutions generally follow two broad strategies. The\nfirst modifies internal model parameters or archi-\ntectures to accommodate external evidence more\nconsistently (Yuan et al., 2024; Jin et al., 2024b;\nShi et al., 2024a), though this often assumes that\nretrieved information should uniformly override\nparametric knowledge. The second strategy explic-\nitly identifies and reconciles discrepancies among\nsources via generation-aided mechanisms or iter-\native comparison (Wang et al., 2023; Liu et al.,\n2022). While these methods can reduce factual er-\nrors, they often rely on ad-hoc instructions or sim-\nple pairwise comparisons that fail to capture subtle\nconflicts. Recent work underscores the need for\nmore principled, robust approaches that integrate\nnuanced reasoning and validation, improving both\nfidelity and explainability in retrieval-augmented\nQA (Xu et al., 2024).\n3 Preliminaries\n3.1 RAG for QA\nRetrieval-Augmented Generation (RAG) combines\na LLM with an external retrieval module. It pro-\nceeds in two key phases: a Retrieval Phase that\nreturns a set of relevant evidence and a Generation\nPhase where the LLM produces the final answer\nconditioned on this evidence.\nRetrieval Phase. Given a query q, a retrieval\nfunction R(·)returns a set of textual fragments\nE={e1, . . . , e m}, where mis the number of frag-\nments. Each fragment eiprovides potentially rele-\nvant information related to q.\nGeneration Phase. LetMΘbe the LLM param-\neterized by Θ. We define the parametric knowledge\nfor the query qas:\nKp(q) =MΘ(q). (1)\nWe use Kr(ei)to represent the knowledge con-\ntained in each retrieved fragment ei. The finalanswer is produced by conditioning on both the\nparametric and retrieved knowledge:\nAns(q) =MΘ\u0000\nq|Kp(q),{Kr(ei)}m\ni=1\u0001\n.(2)\n3.2 Knowledge Conflict\nAknowledge conflict arises when Kp(q)and some\nKr(ei)are factually or logically inconsistent. For-\nmally, there exists at least one ei∈ Esuch that:\nKp(q)̸≈Kr(ei), (3)\nwhere̸≈denotes a factual or logical inconsistency.\nAlgorithm 1 MICRO -ACTPseudocode\n1:Input: query q, external corpus E, LLM MΘ,\nturn budget N\n2:Retrieve: KP←ELICIT (q),Kr←\nRETRIEVE (E, q)\n3:H←∅\n4:fort= 1toNdo\n5: Tt←M Θ(· |H);At←SELECT (Tt)\n6: Ot←\n\nREASON (·)\nASSERT (·)\nDECOMPOSE (·)\n7: H←H∪ {Tt, At, Ot}\n8: ifOt=conflict ∧COMPLEX then\n9: At←DECOMPOSE ▷force split\n10: end if\n11: ifSOLVED (H)then break\n12: end if\n13:end for\n14:Return MΘ(ANSWER |H)\n4 Methodology\nWe introduce MICRO -ACT, a framework that en-\nables Large Language Models (LLMs) to automati-\ncally identify and resolve detailed points of knowl-\nedge conflict. MICRO -ACTcomprises three key\ncomponents: (1) a hierarchical action space (Sec-\ntion 4.1), (2) a Reasoning Body (Section 4.2), and\n(3) Adaptive Granularity and Optimization strate-\ngies (Section 5 and 6). The detailed pseudocode of\nMICRO -ACTcan be found in Algorithm 1.\n4.1 Hierarchical Action Space\nEstablishing a well-structured action space allows\nLLMs to more efficiently invoke planning strate-\ngies (Yao et al., 2024). To this end, we define the\naction space as a structured integration of three key\n--- Page 4 ---\nELICIT  \nLLM KnowledgeQueston : What position \ndoes Paul Eugène Gillon \ncurrently or formerly hold?\nRetrieved Conflict Evidence : \nPaul Eugène Gillon is a French \nofficial who … … In 2010, \nGillon was appointed as a \ndeputy member of the \nParliament of Norway … A \nstudy in Journal … …Parametric  Knowledge : \nRecently, … ... in French \nadministration,  Eugène \nGillon has been officially \nappointed as the General \nSecretary of the Prefecture \nof Creuse  … … According to \nsources within the French \ngovernment  … …Queston : What position \ndoes Paul Eugène Gillon \ncurrently or formerly hold?\nConflict Evidence : Paul Eugène \nGillon is a French official who … … \nIn 2010 , Gillon was appointed as a \nDeputy member of the Parliament of \nNorway … A study in Journal … …\nParametric  Knowledge : Recently , \n… ... in French administration,  \nEugène Gillon has been officially \nappointed as the General Secretary \nof the Prefecture of Creuse  … … \nAccording to sources within the \nFrench  government  … …ASSERT\nConflict  & Parametric  \nKnowledge\nASSERT  \n(Deputy member , \nGeneral Secretary )ASSERT  \n(Norway , \nFrench )ASSERT  \n(2010 , \nRecently )\nConflict Conflict ConflictDECOMPOSE … …\nFigure 2: An illustration of handling knowledge conflict in QA task. Actions highlighted with blue color represent\nnavigational actions; Red color represents functional actions; and green color represents the bridging action. \"... ...\"\nrepresents multiple interplayed actions are folded for simplicity.\ncategories: (1) navigational actions , (2) functional\nactions , and (3) bridging actions , with the decom-\nposition component serving as the cornerstone for\nrefining context granularity of actions.\nNavigational Actions. They focus on exploring\nthe environment and obtaining more information\nas the prerequisite of effective reasoning (Gu et al.,\n2024). Navigational actions include eliciting para-\nmetric knowledge from the LLM and getting the\nreasoning path of a QA task based on input context.\nLetAnavrepresent navigational actions . Specifi-\ncally, we formally define the elicit action in Eq. 4.\nELICIT (q) =Kp(q) =MΘ(q). (4)\nAnd we formally define the action to get the\nreasoning path PKin Eq. 5.\nREASON (K) =PK=Mp\nΘ(K), (5)\nwhere Mp\nΘ(K)represents prompting LLM\nparametrized by Θto generate a reasoning path\nonK. And Kis the input knowledge representa-\ntion either from Kp(q)or from Kr(E).\nFunctional Actions. They address conflict detec-\ntion either between retrieved evidence and LLM\nparametric knowledge or between their reasoning\npaths generated by the navigational action. Once\nrelevant information is prepared, functional actions ,\ndenoted by Afunc, detect conflict among them. For-\nmally, we define the assert action to implement\nthis logic, which is a conflict verification action\nand checks the consistency between Kp(q)and a\nparticular Kr(E)in Eq. 6.\nASSERT\u0000\nKp\ns(q), Kr\ns(E)\u0001\n=δi, (6)\nwhere δi∈ {0,1}. Ifδi= 1, a conflict is detected.\nAndKp\ns(q)∈Kp(q)means Kp\ns(q)is a partial\nknowledge of Kp(q).Bridging-Action. It is responsible for dynami-\ncally optimizing granularity by decomposing ac-\ntions when needed. A side-by-side assert action\nmay fail to detect subtle conflicts embedded in\nlengthy, noisy contexts. To address this, we intro-\nduce the decomposition action , collected in Amicro,\nwhich can refine the granularity of analysis. Sup-\npose an ASSERT (·)action on complex knowledge\ncontext is represented as ASSERT\u0000\nKp(q), Kr(E)\u0001\n.\nA decomposition action can decompose this com-\nplex reasoning into smaller, manageable action\nsteps, as shown in Eq. 7.\nDECOMPOSE (ASSERT\u0000\nKp\ns(q), Kr\ns(E\u0001\n)\n={ASSERT\u0000\nKp\ns′(q), Kr\ns′(E)\u0001\n, . . .},(7)\nwhere Kp\ns(q)∈Kp(q)is a partial knowledge\nofKp(q). And Kp\ns′(q)refers to Kp\ns(Kp\ns(q)),\nwhich means the finer-grained partial knowledge of\nKp\ns(q). Each newly created sub-action deals with\na further fragment of the evidence, increasing the\nlikelihood of revealing fine-grained conflicts. It\nwill decompose the action until LLM has enough\nconfidence or reach the max turn limit.\n4.2 Reasoning Body\nWe integrate our hierarchical action space with the\nReAct process (Yao et al., 2023) to teach LLM inte-\ngrate our hierarchical action space to automatically\nhandle knowledge conflicts. At step t, the LLM\nfirst produces a thought Tt:\nTt∼ M Θ(Tt|Ht−1), (8)\nwhere Ht−1is the accumulated history of all\nthoughts, actions, and observations before step t.\nConditioned on Ht−1and the newly generated\nthought Tt, the model selects an action At:\nAt∼ M Θ(At|Ht−1, Tt). (9)\n--- Page 5 ---\nThis action, executed in the changing environment\n(for example, the knowledge has been decomposed\nat different granularity), yields an observation Ot:\nOt=Env(At).The history is then updated:\nHt=Ht−1∪ {Tt, At, Ot}. (10)\nThis iterative process continues, adjusting gran-\nularity via decomposition actions whenever subtle\nconflicts require finer checks. After Nsteps, the\nfinal answer is generated:\nAf∼ M Θ(Af|HN). (11)\nBy dynamically selecting navigational, func-\ntional, and decomposition actions, this procedure\nensures subtle knowledge conflicts are detected\nand mitigated, improving the reliability of the final\noutput. An example illustration of this process is\nshown in Figure 2.\n5 Understanding Complexity-Driven\nKnowledge Decomposition Dynamics\nTo gain a deeper understanding of how model bridg-\ning actions are related to complexity, we follow\n(Murty et al., 2024) to characterize the distribution\nof the newly inferred knowledge representation at\nturntbased on trajectory over previous t−1turns.\nSpecifically, we define:\npt(Kn) =\nX\nc′X\nKpmodel(Kn|c′)pverify(c′|K)pt−1(K),\n(12)\nwhere Kis current knowledge representation, Kn\nis the newly inferred knowledge representation (of-\nten obtained by decomposing K),cis the ground-\ntruth knowledge conflict, and c′is a potentially in-\ncorrect knowledge conflict identified by the model.\npmodel means the distribution on generate new\nknowledge. pverify means the distribution on gener-\nating conflicts. Detailed derivation can be found in\nAppendix D.\nIn this formulation, the termP\nKpverify(c′|K)\nincreases with the complexity (e.g., longer context,\nharder domain and etc.), resulting in higher verifi-\ncation probabilities and an increased risk of inac-\ncurate conflict detection. AndP\nc′pmodel(Kn|c′)\ndepends on the LLM compatibility. A less capable\nLLM is more likely to be influenced by erroneous\nconflicts ( c′), thereby requiring further decomposi-\ntion and pushing pk(Kn)higher. Section 7.5 shows\nmore details about how these factors drive proac-\ntive decomposition across models.6 Preventing Infinite Decomposition\nWhile hierarchical reasoning is essential for resolv-\ning complex conflicts, an unconstrained recursive\nprocess could, in principle, keep splitting a con-\ntext. Building upon the probabilistic dynamics in\nEq.(12), we show that MICRO -ACTcan prevent\ninfinite decomposition, and we complement this\ntheoretical safeguard with a hard maximum turn\nbudget.\nComplexity-Aware Stopping Criterion. LetCt\ndenote the latent complexity score of the current\ncontext after tturns. A decomposition step is trig-\ngered only when Ct> τ, where τrepresents the\nminimum complexity the underlying LLM can han-\ndle confidently. Because each decomposition short-\nens the context length and narrows its semantic\nscope, the following strict inequality holds:\nCt+1<Ct,∀t≥0. (13)\nDefine\nTτ= min\b\nt| Ct≤τ\t\n. (14)\nBy Eq. (13),Tτis finite, and once reached we have\npt(Kn) = 0 ; no further actions in the DECOM -\nPOSE branch will be sampled. In other words, the\nprocess is self-regularising : an LLM that already\n“understands” the context (small Ct) simply refuses\nto split it further.\n7 Experiments\n7.1 Experiment Settings\nDatasets. We evaluate MICRO -ACTon five\nbenchmark datasets drawn from two comprehen-\nsive collections: ConflictBank and KRE. Conflict-\nBank (Su et al., 2024) provides three specialized\ndatasets targeting distinct conflict types: misin-\nformation ,temporal discrepancies, and seman-\nticdivergences between retrieved and parametric\nknowledge. From KRE (Ying et al., 2023), we\nutilize MuSiQue_KRE and SQuAD_KRE, derived\nfrom MuSiQue (Trivedi et al., 2022) and SQuAD\nv2.0 (Rajpurkar et al., 2018) respectively. These\ndatasets feature multiple-choice questions with gen-\nerated explanations supporting incorrect choices,\ncreating controlled scenarios for examining rea-\nsoning conflicts. Due to the limitation of com-\nputational resources, we randomly sampled 3000\ndata in ConflictBank and 2000 data in KRE dataset\nacross all features, and corrected any errors found.\n--- Page 6 ---\nPromptingGPT-4o GPT-4o-mini LLaMA-3.1-70B LLaMA-3.1-8B\nConflictBank KRE ConflictBank KRE ConflictBank KRE ConflictBank KRE\nGeneric Reasoning\nEnd-to-End QA 5.40 43.80 2.77 31.10 3.07 14.50 2.53 9.55\nFew-Shot QA 6.30 45.65 2.83 33.30 3.87 15.20 3.13 10.30\nChain-of-Thought (Wei et al., 2022) 6.43 44.35 3.00 36.50 1.40 29.45 2.13 24.50\nGeneration-aided Reasoning\nSelf-Ask (Press et al., 2023) 3.13 41.45 2.57 24.90 3.33 23.65 2.77 18.65\nComparative (Wang et al., 2023) 11.70 33.95 2.10 23.85 4.53 25.25 3.87 19.80\nGKP (Liu et al., 2022) 15.40 55.30 17.53 44.45 15.83 43.55 6.83 32.75\nMICRO -ACT(ours) 22.30 (↑6.90) 59.50 (↑4.20) 26.93 (↑9.40) 51.10 (↑6.65) 26.50 (↑10.67) 54.90 (↑11.35) 18.30 (↑11.47) 46.60 (↑13.85)\nTable 1: The average results of Question Answering under Knowledge Conflict on ConflictBank and KRE with\nGPT-4o-mini, GPT-4o, LLaMA-3.1-70B and LLaMA-3.1-8B. The performance is on average over its sub-datasets.\n(underline denotes the previous SOTA performance; bold denotes the best performance; the improvement ( ↑) is\nmeasured against the previous SOTA performing method.)\nMetrics & Models. Following existing knowl-\nedge conflict works (Xie et al., 2024; Su et al.,\n2024; Wang et al., 2023; Shi et al., 2024a), we\nmeasure knowledge conflict in QA task by employ-\ning QA accuracy as our primary evaluation metric.\nSpecifically, the answer format of QA is multiple-\nchoice. If LLMs successfully resolve knowledge\nconflict, they will choose the correct answer instead\nof the negative answer supported by the conflict\n(wrong) knowledge (Su et al., 2024).\nIn our experiments, we use GPT-4o, GPT-\n4o-mini (OpenAI, 2023), LLaMA-3.1-70B and\nLLaMA-3.1-8B (Dubey et al., 2024) as the back-\nbone LLMs.\nCompared Methods. We evaluate MICRO -ACT\nagainst two categories of ICL-based approaches:\ngeneric reasoning methods that reason on retrieved\nevidence, including end-to-end QA, few-shot QA,\nand COT (Wei et al., 2022); and generation-aided\nreasoning methods that reason with self-generated\ncontent of LLMs, including Self-Ask (Press et al.,\n2023), GKP (Liu et al., 2022), and Compara-\ntive (Wang et al., 2023). We evaluate these methods\nacross all five datasets from ConflictBank and KRE.\nPrompts and implementation details can be found\nin Appendix G.\nImplementation. We implement MICRO -ACT\nusing zero-shot prompting without task-specific\ncustomization. To ensure reproducibility, we\nmaintain consistent parameters across all exper-\niments: temperature = 0, top-p = 1, and maximum\ngeneration length = 512 tokens ( max_tokens\nfor closed-source LLMs, max_new_tokens for\nopen-source models). We utilize the Hugging Face\nTransformers library for open-source model infer-\nence. All experiments with open-source models\nare conducted on 4 NVIDIA A100 GPUs (80GB),\nwhile closed-source models are accessed via theirrespective API endpoints.\nFigure 3: The detailed performance of MICRO -ACT\nacross all 3 conflict types with GPT-4o-mini.\n7.2 Main Results\nWe summarize the performance of MICRO -ACT\nand various baseline methods on ConflictBank and\nKRE in Table 1. And detailed performance com-\nparison across all three conflict types (i.e., mis-\ninformation, temporal, and semantic) is shown in\nFigure 3.\nMICRO -ACTsurpasses all baseline approaches\nacross all tested LLMs. Notably, MICRO -ACT\nimproves over the previous SOTA method by up\nto 9.40% on ConflictBank and 6.65% on KRE\nfor GPT-4o-mini, and by 11.47% and 13.85% on\nLLaMA-3.1-8B, respectively. Results across all 5\ndatasets and 3 conflict types, confirm the superior\ncapability of MICRO -ACTin handling knowledge\nconflict and suggest that such superior capability is\nnot model-specific.\n--- Page 7 ---\n7.3 Over-Rationalization Issue\nIn our experiments, we observed an intriguing phe-\nnomenon: when presented with both conflicting\nevidence and LLMs parametric knowledge, LLMs\nsometimes attempt to support all contradictory in-\nformation as equally valid . We characterize this\nbehavior as “ over-rationalization ”, which is a ten-\ndency to find complex justifications that make con-\ntradictory evidence appear compatible. Surpris-\ningly, more capable models like GPT-4o exhibit\nthis behavior more frequently than GPT-4o-mini,\nleading to performance degradation in GKP as\nshown in Table 1.\nFurthermore, we observe that the issue of “over-\nrationalization” is strongly associated with the type\nof conflict, occurring more frequently in tempo-\nral and semantic conflicts. Unlike misinformation-\nbased conflicts, where conflicts are typically ex-\nplicit and directly presented in the context, the tem-\nporal and semantic conflicts are often implicit be-\nneath the superficial context, misleading LLMs to\nrationalize both sides of conflict. A detailed case\nanalysis is in Section 7.8.\nHowever, MICRO -ACTcan “visualize” the un-\nderlying reasoning path via dynamic decomposi-\ntion to pinpoint finer-grained conflict and focus\non those nuanced conflicts underneath the super-\nficial meaning of context . Those conflicts cannot\nbe effectively detected through simple side-by-side\ncomparisons used by baseline methods. As illus-\ntrated in Figure 3, MICRO -ACTachieves a more\nsignificant performance improvement over base-\nlines specifically in the Temporal andSemantic\nconflict types. Detailed analysis is in Appendix B.\nConflictBank MuSiQue_KRE SQuAD_KRE80%85%90%95%100%Accuracy (%)End-to-End\nCOTComparative\nself-askGKP\nMicro-Act\nFigure 4: The performance of MICRO -ACTand base-\nlines using GPT-4o-mini under QA task without knowl-\nedge conflict .\n7.4 Robustness Under Conflict-Free Scenarios\nMany conflict resolution methods assume the pres-\nence of knowledge conflicts. However, in real-world applications, it is often impossible to pre-\ndetermine whether retrieved content conflicts with\nthe parametric knowledge of LLMs, making robust-\nness in conflict-free scenarios crucial.\nAs shown in Figure 4, existing approaches face\na trade-off. Generic reasoning methods like end-\nto-end and COT achieve high accuracy in conflict-\nfree cases but degrade significantly (by 70-95%)\nwhen conflicts arise. And generation-aided meth-\nods such as GKP improve conflict resolution but\nexhibit lower accuracy in conflict-free cases.\nMICRO -ACTovercomes this limitation by\nachieving state-of-the-art performance in conflict\nscenarios with over 24% performance gain and\nshowing robustness with only sacrificing less than\n2% accuracy in conflict-free cases, compared with\nthe end-to-end or self-ask baseline. Rather than in-\ntroducing biases to favor certain evidence sources,\nMICRO -ACThelps models automatically identify\nand analyze potential conflicts through structured\naction space with decomposition, enabling robust\nperformance regardless of whether conflicts exist.\n7.5 Complexity Perception Analysis\nTo understand how MICRO -ACTadapts its decom-\nposition strategy to different complexity levels, we\nanswer 3 research questions (RQs).\nRQ1: How do we objectively measure input com-\nplexity? We select three complementary metrics\nto comprehensively and objectively measure input\ncomplexity: (1) context length captures informa-\ntion volume; (2) domain difficulty reflects inherent\nreasoning challenges; and (3) perplexity quantifies\nlanguage uncertainty (Li et al., 2024a,b; Jelinek\net al., 1977). As shown in Figure 5, these metrics\nprovide a systematic way to evaluate how differ-\nent LLMs adapt the decomposition strategies to\nvarying complexity levels.\nRQ2: Does decomposition behavior show some\npatterns across different complexity dimen-\nsions? Figure 5, we observe consistent adaptation\npatterns within all LLMs. For example, as for the\ncontext length shown in Figure 5(a), the decompo-\nsition rate increases dramatically from 15% (0-100\ntokens) to 95% (400+ tokens). All three complex-\nity dimensions exhibit similar trends, where higher\ncomplexity consistently triggers more frequent de-\ncomposition. This consistency demonstrates the\nability of MICRO -ACTto effectively detect com-\nplexity and dynamically adjust granularity via de-\ncomposition to reduce complexity.\n--- Page 8 ---\n[0,100) [100,200) [200,300) [300,400) [400,Inf)\n/glyph1197umber of Tokens (Bins)020406080Decompose Percentage (%)GPT-4o-mini\nGPT-4o(a) Different Context Length\nFacts Law Geography Economic Math\nDomains020406080100Decompose Percentage (%)GPT-4o-mini\nGPT-4o (b) Different Question Domains\n[0, 10) [10, 20) [20, 30) [30, 40) [40, Inf)\nPerplexity (Bins)020406080100Decompose Percentage (%)GPT-4o-mini\nGPT-4o (c) Different Context Perplexity\nFigure 5: The visual comparisons of the DECOMPOSE action utilization percentage in different complexity, including\ndifferent context length, question domains and perplexity using GPT-4o-mini and GPT-4o. The detailed calculation\nof perplexity can be found in Appendix C.\nRQ3: Do different LLMs share the same under-\nstanding of complexity? The results in Figure 5\nshow that GPT-4o-mini constantly calls decompo-\nsition action more frequently across all complexity\ndimensions, revealing different complexity toler-\nance between GPT-4o and GPT-4o-mini, as dis-\ncussed in Section 5. Rather than requiring manual\ncomplexity adjustments for each LLM, MICRO -\nACTautomatically perceives the complexity and\ndynamically adapts for different LLMs. This adap-\ntive behavior enables robust performance across\ndifferent LLMs without model-specific tuning. For\nexample, as shown in Table 1, although LLaMA-\n3.1-8B is smaller in size and less capable than\nLLaMA-3.1-70B, MICRO -ACTcan still maintain\nrobust performance via more decompose actions\nto adjust complexity, compared with GKP’s deep\nperformance drop. More analysis is in Appendix F.\nMis-Information Temporal Semantic10%20%30%Accuracy (%)GPT-4o\nLLaMA-3.1-70BGemini-2.5-flash-thinking\no3-mini\nFigure 6: The performance of MICRO -ACTusing gen-\neral LLMs and reasoning LLMs on 300 randomly sam-\npled data for each conflict type.\n7.6 General LLMs vs. Reasoning LLMs\nAs illustrated in Figure 6, the general-purpose\nLLMs (GPT-4o and Llama-3.1-70B) cluster to-gether and attain comparatively low scores on all\nthree conflict categories. In contrast, the reasoning-\noriented models, Gemini-2.5-flash-thinking and\no3-mini, form the top tier and consistently out-\nperform the general models. The gap is most\npronounced for misinformation conflicts, which\nare more amenable to reasoning-based resolu-\ntion. For temporal and semantic conflicts, the\ngap narrows because over-rationalization issues\narises more often, as discussed in detail in Sec-\ntion 7.3. In summary, stronger reasoning capabil-\nity markedly boosts the performance of MICRO -\nACT. Although it also increases susceptibility to\nover-rationalization, MICRO -ACTcan effectively\nmitigate this issue and still surpasses the general\nmodels.\nMETHOD MIS-INFO. T EMPORAL SEMANTIC\nMICRO -ACT 26.1 27.9 24.9\nw/o Navigational Actions 18.4 (↓7.7) 18.5 (↓9.4) 15.7 (↓9.2)\nw/o Functional Actions 13.8 (↓12.3) 15.2 (↓12.7) 13.3 (↓11.6)\nw/o DECOMPOSE Action 4.2 (↓21.9) 4.5(↓23.4) 0.8(↓24.1)\nTable 2: Ablation study of MICRO -ACTin three datasets\n(conflict types) of ConflictBank. The numbers represent\nthe accuracy in percentage. ↓is an absolute decrease.\n7.7 Ablation Study\nTable 2 presents an ablation study across all con-\nflict types of ConflictBank. Navigational and Func-\ntional actions serve as essential building blocks for\nconflict resolution, with their removal causing sig-\nnificant performance drops (9.4% for navigational\nand 12.7% for functional actions). While these\nactions are necessary for basic operations like con-\ntext navigation and knowledge comparison, their\n--- Page 9 ---\nQuestion: What position does Paul Eugène Gillon currently or formerly hold?Retrieved Evidence:LLM Parametric Knowledge:Paul Eugène Gillon is a French official who … … In 2010, … Gillon was appointed as a deputy member of the Parliament of Norway … A study published in the Journal of European Studies in 2012 found that … …Recently, ... in French administration, … … Eugène Gillon has been officially appointed as the General Secretary of the Prefecture of Creuse … According to sources within the French government, Gillon's appointment … …Micro-Act:\nI find the following conflicts:1. Different Positions Held: ASSERT(Deputy member, General Secretary)2. Geographic Conflict: ASSERT( Norway, France)3. Timeline Discrepancy: ASSERT( 2010, Recently)\nAll evidence confirm that Gillon is French.  à So not likely to be a Norway politician. “2010” indicates a long time till now.“Recently” indicates a current position. So, I choose “A. General Secretary.”ICL-based Solutions:Baselines:I find the conflict in: Retrieved Evidence vs. LLM Parametric Knowledge \nDon’t know which one is true.So, I choose “D. None of above. ” FurtherReasoningFigure 7: A case study of MICRO -ACTand baselines models under a real knowledge conflict case. MICRO -ACT\ncan pinpoint fine-grained conflict points instead of being distracted by irrelevant context.\neffectiveness heavily depends on appropriate input\ngranularity. Without proper guidance on the input\ngranularity of those actions, the model struggles to\nmaintain consistent performance, especially with\ncomplex contexts.\nDecomposition action dramatically improves\nperformance by dynamically adjusting input granu-\nlarity for other actions. Its removal causes the most\nsevere degradation (over 20% performance drop),\nhighlighting its crucial role. Through iterative de-\ncomposition, MICRO -ACTcontinuously refines in-\nputs of other actions until they find the optimal\ngranularity level where navigational and functional\nactions can operate most effectively. As discussed\nin Section 5, MICRO -ACTeffectively elicits the\nlatent ability of LLMs to perceive complexity and\nadapt to different environments. This adaptive pro-\ncess enhances the confidence of MICRO -ACTby\nensuring all action components receive fine-grained\ninformation aligned with its capability, leading to\nhigher accuracy in complex cases.\n7.8 Case Study\nIn this case study, we demonstrate how MICRO -\nACTidentifies nuanced conflicts underneath the\nsuperficial meaning of context which can hardly\nbe located by simple side-by-side comparisons\nthat baselines use. Consider the question “ What\nposition does Paul Eugène Gillon currently or\nformerly hold? ”, where the retrieved context\nconflicts with LLM parametric knowledge as\nshown in Figure 7. MICRO -ACTcan identify the\ndifferent time references ( 2010 vs.recently )\nand location ( Norway vs.France ). Then apply\nstep-by-step reasoning to find the underlying\nconflicts beyond thesuperficial context: (1)\nMajority consensus suggests he is from France ,\nnotNorway . (2) 2010 indicates a very longappointment, which is less likely compared with a\nrecent appointment, given the question: What\nposition does Paul Eugéne Gillon\ncurrently or formerly hold? (3) And\nfinally, determine that Paul Eugéne Gillon\nwasrecently appointed as a French politician\nthen answer the question correctly.\n8 Cost Analysis\nMICRO -ACTincurs a modest computational cost,\nwhere on ConflictBank it processes roughly 2.8\ntimes more input tokens and 1.3 times more output\ntokens than the strongest baseline (GKP), trans-\nlating to only $0.008 extra per GPT-4o query and\n$0.0005 with GPT-4o-mini, while inference latency\nrises by 0.6 s and 0.3 s respectively. Crucially, these\noverheads appear only when genuine conflicts trig-\nger deeper decomposition; conflict-free questions\nfinish as quickly as the baseline. Given the substan-\ntial gains in conflict-resolution accuracy reported\nin Table 1, the marginal cost and delay are accept-\nable for real-world RAG deployments. Detailed\ntoken, cost, and timing breakdowns are provided in\nAppendix A.\n9 Conclusion\nWe proposed MICRO -ACT, a framework that\naddresses knowledge conflicts in RAG systems\nthrough hierarchical action decomposition. By\nautomatically perceiving context complexity and\nbreaking down comparisons into fine-grained\nsteps, MICRO -ACTovercomes the limitations\nof simple side-by-side comparisons for example\ntheover-rationalization issue. Extensive experi-\nments demonstrate its effectiveness across multi-\nple datasets and conflict types, while maintaining\nstrong robustness in non-conflict scenarios, making\nit particularly valuable for real-world RAG.\n--- Page 10 ---\n10 Limitations\nWhile MICRO -ACTdemonstrates strong perfor-\nmance in knowledge conflict resolution, several\nlimitations warrant discussion. First, our MICRO -\nACTneeds additional intermediate steps to effec-\ntively pinpoint the conflicts underneath the super-\nficial meaning of context, which can hardly be\nlocated by simple side-by-side comparisons that\nbaselines use. Although baselines like end-to-end\nand COT (Wei et al., 2022) are lightweight, their\npoor performance in knowledge conflict harms the\neffectiveness of RAG systems. We believe the ef-\nficiency should be considered after good perfor-\nmance. As detailed analyzed in Appendix A, the\nextra overhead is relatively small and our analysis\ndemonstrates that MICRO -ACT’s modest overhead\nis justified by its significantly enhanced conflict\nresolution performance. Second, our current evalu-\nation focuses primarily on English language con-\ntexts. The effectiveness of decomposition strategies\nmight vary across different languages and cultural\ncontexts.\nNevertheless, our work represents an important\nmilestone in knowledge conflict resolution, estab-\nlishing a strong foundation for future research in\nthis critical area.\n11 Acknowledgement\nReynold Cheng, Nan Huo, Jinyang Li, and Ge\nQu are supported by the Hong Kong Jockey Club\nCharities Trust (Project 260920140), the Univer-\nsity of Hong Kong (Project 2409100399), the\nHKU Outstanding Research Student Supervisor\nAward 2022-23, and the HKU Faculty Exchange\nAward 2024 (Faculty of Engineering). Bowen\nQin was supported by National Science and Tech-\nnology Major Project (Project 2022ZD0116306).\nChenhao Ma was partially supported by NSFC\nunder Grant 62302421, Basic and Applied Ba-\nsic Research Fund in Guangdong Province under\nGrant 2023A1515011280, 2025A1515010439, Ant\nGroup through CCF-Ant Research Fund, Shen-\nzhen Research Institute of Big Data under grant\nSIF20240004, and the Guangdong Provincial Key\nLaboratory of Big Data Computing, The Chinese\nUniversity of Hong Kong, Shenzhen.\n12 Ethical Statement\nWe prioritize ethical considerations throughout our\nresearch process. During data collection and pre-\nprocessing, we carefully filtered out examples con-taining sensitive, biased, or potentially harmful\ncontent to ensure our evaluation focuses on con-\nstructive knowledge resolution scenarios. Our in-\ncontext learning approach requires no additional\ntraining of language models, significantly reducing\nthe environmental impact compared to fine-tuning\nmethods. This aligns with growing concerns about\nthe carbon footprint of AI research. Furthermore,\nall datasets used in this work are publicly available,\nensuring reproducibility.\nReferences\nAdam Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning\nto retrieve reasoning paths over wikipedia graph for\nquestion answering. In International Conference on\nLearning Representations (ICLR) .\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning , pages 2206–2240. PMLR.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\nvey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology ,\n15(3):1–45.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers , pages 1870–1879.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2024. Benchmarking large language models in\nretrieval-augmented generation. In Thirty-Eighth\nAAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applica-\ntions of Artificial Intelligence, IAAI 2024, Fourteenth\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2014, February 20-27, 2024, Van-\ncouver, Canada , pages 17754–17762. AAAI Press.\nSihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.\n2021. Improving faithfulness in abstractive sum-\nmarization with contrast candidate generation and\nselection. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies , pages 5935–5941.\nCostanza Conforti, Jakob Berndt, Mohammad Taher\nPilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, and\nNigel Collier. 2020. Stander: An expert-annotated\n--- Page 11 ---\ndataset for news stance detection and evidence re-\ntrieval. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 4086–4101.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, and et al.\nAngela Fan. 2024. The llama 3 herd of models.\nArXiv , abs/2407.21783.\nTianqing Fang, Zhaowei Wang, Wenxuan Zhou, Hong-\nming Zhang, Yangqiu Song, and Muhao Chen. 2024.\nGetting sick after seeing a doctor? diagnosing and\nmitigating knowledge conflicts in event temporal rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: NAACL 2024 , pages 3846–3868.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al.\n2023. Rarr: Researching and revising what language\nmodels say, using language models. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 16477–16508.\nYu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong,\nJie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su.\n2024. Middleware for llms: Tools are instrumental\nfor language agents in complex environments. arXiv\npreprint arXiv:2402.14672 .\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020a. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning , pages 3929–3938. PMLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020b. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning , pages 3929–3938. PMLR.\nGautier Izacard and Édouard Grave. 2021a. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 874–880.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics (EACL) , pages\n874–880.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2023. Atlas: Few-shot learning with retrieval\naugmented language models. Journal of Machine\nLearning Research , 24(251):1–43.Fred Jelinek, Robert L Mercer, Lalit R Bahl, and\nJames K Baker. 1977. Perplexity—a measure of the\ndifficulty of speech recognition tasks. The Journal of\nthe Acoustical Society of America , 62(S1):S63–S63.\nZhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiao-\njian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. 2024a.\nTug-of-war between knowledge: Exploring and re-\nsolving knowledge conflicts in retrieval-augmented\nlanguage models. arXiv preprint arXiv:2402.14409 .\nZhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen,\nJiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu,\nand Jun Zhao. 2024b. Cutting off the head ends the\nconflict: A mechanism for interpreting and mitigat-\ning knowledge conflicts in language models. arXiv\npreprint arXiv:2402.18154 .\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 6769–6781.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems , 33:9459–9474.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua\nLi, Bowen Li, Bailin Wang, Bowen Qin, Ruiying\nGeng, Nan Huo, et al. 2024a. Can llm already serve\nas a database interface? a big bench for large-scale\ndatabase grounded text-to-sqls. Advances in Neural\nInformation Processing Systems , 36.\nJinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao,\nGe Qu, Yurong Wu, Chenhao Ma, Jian-Guang Lou,\nand Reynold Cheng. 2024b. Tapilot-crossing: Bench-\nmarking and evolving llms towards interactive data\nanalysis agents. arXiv preprint arXiv:2403.05307 .\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert\nYüksekgönül, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2023. Holistic eval-\nuation of language models. Trans. Mach. Learn. Res. ,\n2023.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\n--- Page 12 ---\nHajishirzi. 2022. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n3154–3169.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics , 12:157–173.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 4089–4100.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. Factscore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12076–12100.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. Ambigqa: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5783–\n5797.\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi,\nOncel Tuzel, Samy Bengio, and Mehrdad Farajtabar.\n2024. Gsm-symbolic: Understanding the limitations\nof mathematical reasoning in large language models.\narXiv preprint arXiv:2410.05229 .\nShikhar Murty, Christopher D. Manning, Peter Shaw,\nMandar Joshi, and Kenton Lee. 2024. BAGEL: boot-\nstrapping agents by guiding exploration with lan-\nguage. In Forty-first International Conference on\nMachine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024 . OpenReview.net.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332 .\nOpenAI. 2023. Gpt-4 technical report. Technical report,\nOpenAI.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander Miller, and Sebas-\ntian Riedel. 2019. Language models as knowledge\nbases? In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 2463–2473.Quang Pham, Hoang Ngo, Luu Anh Tuan, and Dat Quoc\nNguyen. 2024. Who’s who: Large language mod-\nels meet knowledge conflicts in practice. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2024 , pages 10142–10151.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023 , pages 5687–5711.\nLibo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen,\nYangming Li, and Ting Liu. 2019. Entity-consistent\nend-to-end task-oriented dialogue system with kb re-\ntriever. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n133–142.\nLibo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen, and\nTing Liu. 2018. End-to-end task-oriented dialogue\nsystem with distantly supervised knowledge base re-\ntriever. In Chinese Computational Linguistics and\nNatural Language Processing Based on Naturally\nAnnotated Big Data: 17th China National Confer-\nence, CCL 2018, and 6th International Symposium,\nNLP-NABD 2018, Changsha, China, October 19–21,\n2018, Proceedings 17 , pages 238–249. Springer.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers , pages 784–789. Association\nfor Computational Linguistics.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics , 11:1316–1331.\nRuiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin\nZhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,\nand Haifeng Wang. 2023. Investigating the fac-\ntual knowledge boundary of large language mod-\nels with retrieval augmentation. arXiv preprint\narXiv:2307.11019 .\nKiamehr Rezaee, Jose Camacho-Collados, and Moham-\nmad Taher Pilehvar. 2024. Tweetter: A benchmark\nfor target entity retrieval on twitter without knowl-\nedge bases. In Proceedings of the 2024 Joint In-\nternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024) , pages 16890–16896.\nSagi Shaier, Ari Kobren, and Philip Ogren. 2024. Adap-\ntive question answering: Enhancing language model\nproficiency for addressing knowledge conflicts with\nsource citations. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 17226–17239.\n--- Page 13 ---\nDan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xin-\nwei Wu, and Deyi Xiong. 2024a. Ircan: Mitigating\nknowledge conflicts in llm generation via identify-\ning and reweighting context-aware neurons. arXiv\npreprint arXiv:2406.18406 .\nQi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanx-\niang Che, and Ting Liu. 2024b. Exploring hybrid\nquestion answering via program-based prompting.\narXiv preprint arXiv:2402.10812 .\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Richard James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2024c. Replug: Retrieval-\naugmented black-box language models. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers) , pages 8364–8377.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021 , pages 3784–3803.\nZhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu\nLi, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng.\n2024. Conflictbank: A benchmark for evaluating\nthe influence of knowledge conflicts in llm. arXiv\npreprint arXiv:2408.12076 .\nHexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang,\nQi Cao, and Xueqi Cheng. 2024. Blinded by gen-\nerated contexts: How language models merge gen-\nerated and retrieved contexts when knowledge con-\nflicts? In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 6207–6227.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages\n809–819.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics , 10:539–554.\nFei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen,\nand Sercan Ö Arık. 2024. Astute rag: Overcom-\ning imperfect retrieval augmentation and knowledge\nconflicts for large language models. arXiv preprint\narXiv:2410.07176 .\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce label-\ning cost? gpt-3 can help. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021 ,\npages 4195–4205.Yike Wang, Shangbin Feng, Heng Wang, Weijia\nShi, Vidhisha Balachandran, Tianxing He, and Yu-\nlia Tsvetkov. 2023. Resolving knowledge con-\nflicts in large language models. arXiv preprint\narXiv:2310.00935 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems , 35:24824–24837.\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and\nYu Su. 2024. Adaptive chameleon or stubborn sloth:\nRevealing the behavior of large language models in\nknowledge conflicts. In The Twelfth International\nConference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024 .\nRongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang,\nHongru Wang, Yue Zhang, and Wei Xu. 2024.\nKnowledge conflicts for llms: A survey. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2024, Miami,\nFL, USA, November 12-16, 2024 , pages 8541–8565.\nAssociation for Computational Linguistics.\nJohn Yang, Carlos E Jimenez, Alexander Wettig, Kil-\nian Lieret, Shunyu Yao, Karthik Narasimhan, and\nOfir Press. 2024. Swe-agent: Agent-computer inter-\nfaces enable automated software engineering. arXiv\npreprint arXiv:2405.15793 .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nInProceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2369–2380.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2024. Tree of thoughts: Deliberate problem solving\nwith large language models. Advances in Neural\nInformation Processing Systems , 36.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023 . OpenReview.net.\nJiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long\nCui, and Yongbin Liu. 2023. Intuitive or dependent?\ninvestigating llms’ robustness to conflicting prompts.\narXiv preprint arXiv:2309.17415 .\nXiaowei Yuan, Zhao Yang, Yequan Wang, Shengping\nLiu, Jun Zhao, and Kang Liu. 2024. Discerning\nand resolving knowledge conflicts through adaptive\ndecoding with contextual information-entropy con-\nstraint. arXiv preprint arXiv:2402.11893 .\n--- Page 14 ---\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. Advances in neural information processing\nsystems , 32.\nXuanliang Zhang, Dingzirui Wang, Longxu Dou,\nQingfu Zhu, and Wanxiang Che. 2025. Murre: Multi-\nhop table retrieval with removal for open-domain text-\nto-sql. In Proceedings of the 31st International Con-\nference on Computational Linguistics , pages 5789–\n5806.\nXuanliang Zhang, Dingzirui Wang, Baoxin Wang,\nLongxu Dou, Xinyuan Lu, Keyan Xu, Dayong Wu,\nQingfu Zhu, and Wanxiang Che. 2024. Scitat: A\nquestion answering benchmark for scientific tables\nand text covering diverse reasoning types. arXiv\npreprint arXiv:2412.11757 .\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223 .\n--- Page 15 ---\nA Cost-Performance Trade-Off\nIn this section, we provide a detailed cost and la-\ntency analysis on the ConflictBank dataset, compar-\ningMICRO -ACTwith the most powerful baseline\nwithin Generic Reasoning and Generation-aided\nReasoning groups (i.e., COT and GKP) using GPT-\n4o and GPT-4o-mini, as shown in Table 3 and Ta-\nble 4.\nA.1 Token Usage and Monetary Cost\n•Input Tokens: Because MICRO -ACTdynam-\nically decomposes conflicts, it initiates addi-\ntional turns to resolve contradictions. As a\nresult, it uses about 2.8 ×more input tokens\nthan GKP.\n•Output Tokens: MICRO -ACT’s output length\nis roughly 1.3 ×higher than GKP. Although it\nwrites multiple short intermediate responses,\nthe final output does not explode in length, as\nthe output of each intermediate step tends to\nbe relatively concise.\n•Overall Cost: The cost difference comes\nto an additional $0.008 per query for GPT-\n4o. For GPT-4o-mini, the extra cost is only\n$0.0005 per query, which is fairly small.\nA.2 Inference Time Overhead\n•GPT-4o: MICRO -ACTtakes 1.9s on average,\nwhich is about 0.6s longer than GKP (1.3s).\n•GPT-4o-mini: MICRO -ACTrequires 0.7s on\naverage, 0.3s longer than GKP (0.4s).\n•The latency shown above is not using multi-\nthreading. If we use multi-threading, the extra\ninference latency will be further reduced sig-\nnificantly.\nThis additional overhead comes from the extra\ndecomposition steps in scenarios where conflicts\nare actually perceived by MICRO -ACT. However,\nfor conflict-free queries, MICRO -ACTperforms\nfewer steps, avoiding this overhead.\nA.3 Justification of Additional Overhead\n1.Significant Performance Gain: As shown in\nour main experiments, MICRO -ACTachieves\nnotable improvements in resolving knowl-\nedge conflicts. It indicates that our dynamic\ndecomposition approach is essential for de-\ntecting finer-grained conflicts.2.Adaptive Depth: MICRO -ACTonly needs\ndeeper decomposition when a conflict is per-\nceived, which is necessary to locate the under-\nlying conflicts that baselines cannot find. On\nconflict-free questions, it quickly finalizes the\nanswer, keeping cost and latency low.\n3.Practical Applicability: In many real-world\napplications such as SWE-agent (Yang et al.,\n2024), baseline query costs more than $2 per\nquery. We believe that an additional $0.008 in\nGPT-4o (and $0.0005 for GPT-4o-mini) and\n0.6 extra seconds in GPT-4o (and 0.3 sec for\nGPT-4o-mini) per query is acceptable given\nthe significantly improved performance, espe-\ncially for real-world scenarios.\nWe believe this analysis demonstrates that\nMICRO -ACT’smodest overhead is justified by\nitssignificantly enhanced conflict resolution capa-\nbilities.\nMis-Information Temporal Semantic10%20%30%40%50%Over-Rationalization RatioGKP (LLaMA-3.1-8B)\nMicro-Act (LLaMA-3.1-8B)GKP (LLaMA-3.1-70B)\nMicro-Act (LLaMA-3.1-70B)\nFigure 8: The Over-Rationalization Ratio of GKP and\nMICRO -ACTusing LLaMA-3.1-8B and LLaMA-3.1-\n70B (lower is better).\nB Over-Rationalization Issue and\nAnalysis\nIn this section, we present a quantitative inves-\ntigation on 600 queries (200 from each con-\nflict type) in the ConflictBank dataset, comparing\nGKP (strongest baseline) and MICRO -ACTon two\nLLMs: LLaMA-3.1-70B and GPT-4o-mini. We\nmeasure the proportion of queries in which the\nmodel exhibits over-rationalization (i.e., rationaliz-\ning contradictory facts in their step-by-step chain-\nof-thought reasoning).\nFrom Figure 8, it is clear that all models ex-\nhibit a higher ratio of over-rationalization under\ntemporal and semantic conflict types, as these two\ntypes are more easily rationalized. For example,\nas shown in Figure 7, conflicting knowledge about\nwhether Paul Eugéne Gillon was appointed\n--- Page 16 ---\nMethod Avg. # Turns Avg. Input Tokens Avg. Output Tokens Avg. Cost (USD) Avg. Inference Time (s)\nCOT 2.0 652 421 $0.006 0.9\nGKP 2.0 1182 856 $0.012 1.3\nMICRO -ACT 3.4 3345 1137 $0.020 1.9\nTable 3: Cost analysis on GPT-4o.\nMethod Avg. # Turns Avg. Input Tokens Avg. Output Tokens Avg. Cost (USD) Avg. Inference Time (s)\nCOT 2.0 689 469 $0.0004 0.3\nGKP 2.0 1239 894 $0.0007 0.4\nMICRO -ACT 3.6 3532 1289 $0.0013 0.7\nTable 4: Cost analysis on GPT-4o-mini.\nin2010 or more recently can both be reason-\nable, making it difficult for the model to rely on\nretrieved evidence or LLM parametric knowledge.\nThe GKP method shows a higher tendency to\nrationalize these conflicts, as LLMs are easily dis-\ntracted by complex context (Mirzadeh et al., 2024).\nThis leads to failure in identifying the fine-grained\nconflicts underneath. In contrast, MICRO -ACTdy-\nnamically decomposes the complex context to re-\nduce its complexity and identifies the conflicts be-\nhind the superficial context. This enables MICRO -\nACTto pinpoint conflict points and reason on them\nfor correct answers.\nB.1 A More Detailed Case Study\nIn Figure 7, an example of contradictory infor-\nmation regarding Paul Eugéne Gillon being\nappointed in 2010 versus a more recent appoint-\nment is shown. Additionally, incorrect evidence\nsuggests he might be a politician in Norway . By\niteratively decomposing the knowledge, MICRO -\nACTis able to:\n1.Identify the conflicting time references ( 2010\nvs.recently ) and location ( Norway vs.\nFrance ).\n2.Apply step-by-step reasoning to find the un-\nderlying conflicts beyond the superficial con-\ntext:\n•Majority consensus suggests he is from\nFrance , notNorway .\n•2010 indicates a very long appointment,\nwhich is less likely compared with a\nrecent appointment, given the ques-\ntion:What position does Paul\nEugéne Gillon currently or\nformerly hold?3.Finally, determine that Paul Eugéne\nGillon wasrecently appointed as a\nFrench politician and correctly answer the\nquestion.\nThese nuanced conflicts are hidden beneath the\nsuperficial meaning of context and can hardly be\ndetected by simple side-by-side comparisons used\nby baseline models.\nC Perplexity Calculation\nIn this section, we provide a detailed explanation of\nhow to compute the perplexity (PPL) of a given text\nusing the GPT-2 language model, which is not any\nof the language models used in our work. The aim\nis to provide an objective measurement of knowl-\nedge context complexity. Perplexity is a widely\nused metric for evaluating the quality of language\nmodels, indicating how well the model predicts a\nsample of text (Jelinek et al., 1977). In this work,\nby fixing the language model to be GPT-2, we use\nPPL to measure the complexity of context text. A\nlower perplexity value generally corresponds to a\nlower context text complexity.\nC.1 Formal Definition of Perplexity\nLet the text be represented as a sequence of tokens:\nW=w1, w2, . . . , w N, (15)\nwhere each wiis a token (e.g., a subword unit as\nutilized by GPT-2).\nGiven a language model that estimates the prob-\nability of each token conditioned on all previous\ntokens, the joint probability of the sequence Wcan\nbe factorized as:\nP(W) =NY\ni=1P(wi|w1, w2, . . . , w i−1).(16)\n--- Page 17 ---\nThe perplexity of the sequence Wunder the\nmodel is defined as:\nPPL(W) =\nexp \n−1\nNNX\ni=1lnP(wi|w1, w2, . . . , w i−1)!\n.\n(17)\nIn other terms, if we use base-2 logarithms:\nPPL(W) = 2−1\nNPN\ni=1log2P(wi|w1,...,w i−1).(18)\nThe perplexity can be interpreted as the effective\naverage branching factor of the language model. A\nperfectly predicting model (one that assigns proba-\nbility 1 to the observed sequence) would achieve a\nperplexity of 1.\nC.2 GPT-2-Based Computation\nGPT-2 is a Transformer-based language model\ntrained on large-scale text data. It provides a proba-\nbility distribution over the next token wigiven the\nprevious tokens (w1, . . . , w i−1). Formally, GPT-2\nimplements:\nP(wi|w1, . . . , w i−1) =softmax (hi−1We)wi,\n(19)\nwhere hi−1is the hidden state vector pro-\nduced by the Transformer after processing tokens\nw1, . . . , w i−1, andWeis the token embedding ma-\ntrix used to map hidden states to logits over the\nvocabulary. The softmax function converts these\nlogits into probabilities.\nC.3 Practical Steps for Perplexity Calculation\nTo compute perplexity using GPT-2 in practice, one\nwould proceed as follows:\nTokenization. Convert the raw text into GPT-2\ncompatible tokens:\nTexttokenizer− − − − − → (w1, w2, . . . , w N). (20)\nModel Inference. For each token wi, feed the\npreceding tokens (w1, w2, . . . , w i−1)into GPT-2\nto obtain:\nP(wi|w1, . . . , w i−1). (21)\nThis is done by running the model Mwhich is\nGPT-2 forward pass:\n(h1, h2, . . . , h i−1) =M(w1, w2, . . . , w i−1)\n(22)\nand then applying the softmax over the output logits\nto get the probability of wi.Log Probability Computation. The next step\nis to extract lnP(wi|w1, . . . , w i−1)from the\nmodel’s output distribution.\nSummation and Normalization. Compute the\naverage negative log-probability:\n−1\nNNX\ni=1lnP(wi|w1, . . . , w i−1). (23)\nExponentiation Take the exponential of this\nvalue to obtain the perplexity:\nPPL(W) =\nexp \n−1\nNNX\ni=1lnP(wi|w1, . . . , w i−1)!\n.(24)\nDMathematical Derivation of Knowledge\nRepresentation Transitions\nIn this section, we present a formal derivation of the\nknowledge representation transition process. We\nbegin by defining the key probability distributions:\n•pt(Kn): The distribution of knowledge repre-\nsentation at step t\n•pmodel(Kn|K): The probability of the model\ngenerating new knowledge representation Kn\nfrom previous state K\n•pverify (c|Kn): The probability of the verifier\ngenerating conflict detection result cbased on\nknowledge representation Kn\nFollowing the principles of probabilistic state\ntransitions, we can establish two fundamental equa-\ntions:\nD.1 Knowledge Representation Transition\nEquation\nThe transition to new knowledge representation can\nbe expressed as:\npt(Kn) =X\nc′pmodel(Kn|c′)·pt−1(c′)(25)\nThis equation represents how new knowledge\nrepresentation are derived from previous conflict\ndetection results.\n--- Page 18 ---\nD.2 Conflict Detection Equation\nThe probability distribution of conflict detection\nresults is given by:\npt−1(c′) =X\nKpverify (c′|K)·pt−1(K)(26)\nThis captures how conflict detection results are\ngenerated based on previous knowledge represen-\ntation.\nD.3 Combined Transition Model\nSubstituting the conflict detection equation into the\nstate transition equation yields:\npt(Kn) =X\nc′pmodel(Kn|c′)\n·[X\nKpverify (c′|K)·pt−1(K)](27)\nRearranging the summation order:\npt(Kn) =X\nc′X\nKpmodel(Kn|c′)\n·pverify (c′|K)·pt−1(K)(28)\nD.4 Bayesian Formulation\nApplying Bayes’ rule to transform pverify (c′|K)\nintopverify (K|c′):\npt(Kn) =X\nK,c′pt−1(K)\n·pverify (K|c′)·pmodel(Kn|c′)(29)\nThis final form encapsulates three key compo-\nnents:\n•pt−1(K): Distribution of previous knowledge\nrepresentation\n•pverify (K|c′): Verifier’s evaluation of knowl-\nedge\n•pmodel(Kn|c′): Model’s probability of gener-\nating new knowledge based on conflict detec-\ntion\nThis formulation provides a comprehensive\nmathematical framework for analyzing the evolu-\ntion of knowledge representation through iterative\nrefinement and verification.E Error Analysis\nTo better understand the limitations of MICRO -\nACT, we conducted a detailed error analysis on\n1,000 randomly sampled examples across all five\ndatasets. Our analysis revealed two predominant\nerror types:\nContext Distraction (63% of errors) Despite\nour decomposition strategy, LLMs occasionally\nbecome overwhelmed by complex contexts and de-\nfault to expressing uncertainty (\"I don’t know\" or\n\"Cannot determine\"). This typically occurs when\nthe context contains multiple interrelated facts or\ncomplex logical relationships that challenge the\nmodel’s ability to maintain coherent reasoning\nchains. For instance, in multi-hop reasoning ques-\ntions where evidence pieces are densely connected,\neven decomposed segments may retain inherent\ncomplexity that exceeds the model’s processing\ncapacity.\nOver-reliance on Retrieved Evidence (37% of\nerrors) The second major error type manifests\nwhen LLMs exhibit a strong bias toward retrieved\nevidence, even when it conflicts with their para-\nmetric knowledge. This behavior is particularly\nprominent in cases where the retrieved evidence\nappears more specific or detailed than the model’s\ninherent knowledge. Such errors suggest that while\nMICRO -ACTeffectively identifies conflicts, the fi-\nnal resolution step may still be influenced by an\nimplicit bias toward explicit external information\nover learned knowledge.\nThese findings indicate that while MICRO -ACT\nsignificantly improves conflict resolution, future\nwork should focus on enhancing the model’s abil-\nity to maintain reasoning clarity in highly complex\nscenarios and developing more balanced weigh-\ning mechanisms between retrieved and parametric\nknowledge.\nF More Complexity Aspect Analysis\nBesides the complexity aspects discussed in Fig-\nure 5, we explore how the number of decomposi-\ntion steps (i.e., the number of times MICRO -ACT\ninvokes its “DECOMPOSE” action) varies across\ndifferent conflict types. We collect more results on\ntemporal, misinformation, and semantic conflicts\nfor both GPT-4o and GPT-4o-mini. Below is a\nsummary of the average number of decomposition\nsteps taken for each conflict category:\n--- Page 19 ---\nAction Expression Description\nDECOMPOSE Action i\n→action 1, action 2Split a complex action i into \nsmaller, manageable steps \nsuch as action 1 and action 2.\nELICIT Query i\n→LLM KnowledgeElicit LLM knowledge about \na given query.\nREASON Knowledge i\n→Reasoning PathReason on input knowledge to \ngenerate the reasoning path.\nASSERT Knowledge i, Knowledge j\n→0 or 1Judge whether the two input \nknowledge have conflict or not.Figure 9: The table of actions in our hierarchical action space.\nTable 5: Average DECOMPOSE action steps per con-\nflict type.\nConflict Type GPT-4o (Avg. Steps) GPT-4o-mini (Avg. Steps)\nMisinformation 0.8 1.3\nTemporal 1.6 2.2\nSemantic 1.5 2.3\nTemporal and Semantic Conflicts. We can see\nthat temporal and semantic conflict types usually\ntrigger a higher number of decomposition steps.\nAs detailed in Appendix B, these two types are\nmore easily to be “over-rationalized” . Thus, more\ndecomposition steps are needed to investigate un-\nderneath conflicts and the logic flaws behind the\nsuperficial context, which looks reasonable.\nMisinformation Conflicts. Misinformation typi-\ncally involves more superficial conflicts , which is\nmore intuitive. As a result, fewer DECOMPOSE\nactions are invoked because there is less ambiguity\nin the evidence to untangle.\nWhy Decompose More? The observed more de-\ncomposition is mainly because the nuanced con-\nflicts are underneath the superficial context and\ncan hardly be located by simple side-by-side com-\nparisons that baselines use, detailed discussion can\nbe found in Appendix B.\nModel Size & Complexity. We observe that\nGPT-4o-mini has a higher decomposition step\ncount across all conflict types. Smaller LLMs of-\nten require additional steps to reduce complexity,\nrevealing different complexity tolerance between\nGPT-4o and GPT-4o-mini, as discussed in Sec-\ntion 5. A more detailed discussion can be found in\nSection 7.5, RQ3.This illustrates exactly why andhow MICRO -\nACT’s dynamic reasoning pipeline triggers addi-\ntional decomposition for temporal and semantic\nconflicts, where potential “over-rationalizations”\nare more likely to arise.\nG Implementation Details\nG.1 Action Details\nAll the actions designed in our proposed hierarchi-\ncal action space are illustrated in Table 9.\nG.2 Generic Reasoning Models\nEnd-to-End QA. The End-to-End QA prompt as\nshown in Figure 10, directly provides the model\nwith the question and requests an immediate, self-\ncontained answer. It contains no intermediate rea-\nsoning instructions, and the model is expected to\nreturn its best guess in a single generation pass.\nThis approach assumes the model’s internal rep-\nresentations are sufficient for reasoning without\nexplicitly prompting it to show work.\nFew-Shot QA. The Few-Shot QA prompt as\nshown in Figure 11, includes one or more exam-\nple QA pairs before presenting the target question.\nThese examples help the model align its reasoning\nwith the desired output format and style. The pro-\nvided examples are chosen to be representative of\nthe question domain and complexity level.\nChain-of-Thought. The Chain-of-Thought (Wei\net al., 2022) prompt as shown in Figure 12, in-\nstructs the model to show its intermediate reasoning\nsteps explicitly. After presenting the question, the\nprompt requests the model to “think aloud” by out-\nlining its reasoning process before concluding with\n--- Page 20 ---\na final, concise answer. This approach encourages\nthe model to form more coherent and justifiable\nsolutions.\nG.3 Generation-aided Reasoning Models\nSelf-Ask. The Self-Ask (Press et al., 2023)\nprompt as shown in Figure 13, breaks down a com-\nplex question into sub-questions and then prompts\nthe model to answer them step-by-step. By itera-\ntively generating and resolving subtasks, the model\ncan handle multi-step reasoning tasks more system-\natically, ultimately consolidating the intermediate\nanswers into a final solution.\nComparative QA. The Comparative QA (Wang\net al., 2023) prompt as shown in Figure 14, asks\nLLMs generate the answer of the question regard-\nless of the retrieved evidence at first. Then answer\nthe question by considering both the retrieved evi-\ndence and the self-generated answer.\nGeneration Phase of GKP. In the Generation\nPhase of GKP (Liu et al., 2022), the prompt encour-\nages the model to generate the knowledge needed\nto answer the given question. The model then lists\nrelevant knowledge without yet providing the final\nanswer.\nAnswering Phase of GKP. Once the self-\ngenerated knowledge is established, the Answer-\ning Phase of GKP (Liu et al., 2022) prompt feeds\nthe previously generated knowledge back into the\nmodel. Using this as a guide, the model now pro-\nduces a final answer. This is a two-step process.\nG.4 M ICRO -ACT\nMICRO -ACT.The prompt of our proposed\nMICRO -ACTmodel is shown in Figure 17.\nH Human Evaluation\nTo assess the quality and representativeness of our\n1,000-instance samples, we conducted a human\nstudy with 10 volunteer expert annotators, each\nhaving substantial experience in QA tasks.\nEvaluation Procedure. For each dataset, we pre-\nsented each annotator with a total of 100 QA items:\n50 randomly drawn from the full dataset and 50\nrandomly drawn from the 1,000-instance sample.\nAnnotators were blind to which items came from\nwhich source. Each annotator answered all 100\nquestions to the best of their ability.Measurements. We measured annotator accu-\nracy, defined as the proportion of correct answers,\non both subsets. Across all datasets, the average ac-\ncuracy on sample-based QA pairs differed by less\nthan 5% from that on the corresponding full-dataset\npairs. This consistency suggests that the sampled\nsubsets do not introduce systematic bias in terms\nof difficulty or content distribution.\nInternal Agreement. To ensure that results were\nnot driven by a few outliers, we examined inter-\nnal agreement among the 10 annotators. We com-\nputed Fleiss’ kappa (Fleiss, 1971), which was con-\nsistently above 0.80 for all datasets, indicating sub-\nstantial agreement. In addition, the standard devia-\ntion of accuracy across annotators remained under\n2% for each subset type, reflecting stable and con-\nsistent performance patterns.\nThese findings demonstrate that our sampling\nstrategy preserves the key characteristics of the\noriginal datasets, maintaining both content diver-\nsity and difficulty level, and that our evaluation\nresults are reliable and robust across multiple in-\ndependent annotators. After the review period, we\nwill open-source the sampled datasets for reproduc-\ntion and for researchers to develop more advanced\nmethods on knowledge conflict.\nI Model Descriptions\nOur empirical evaluation employs three representa-\ntive language models, each positioned at different\ncapability levels.\nGPT-4o. GPT-4o (OpenAI, 2023) is a state-of-\nthe-art foundation model that excels at complex rea-\nsoning tasks. Our experiments leverage its robust\ninstruction-following capabilities and advanced rea-\nsoning abilities to evaluate the upper bounds of\nadaptive complexity.\nGPT-4o-mini. GPT-4o-mini (OpenAI, 2023) is\na balanced model that combines computational ef-\nficiency with strong reasoning capabilities. This\nmodel serves as an ideal testbed for examining how\nmoderate model capacity influences the granularity\nof knowledge decomposition across varying task\ncomplexities.\nLLaMA-3.1-70B-Instruct. LLaMA-3.1-70B-\nInstruct (Dubey et al., 2024) is a 70-billion\nparameter large language model built on the\nLLaMA architecture. This instruction-tuned\nvariant exhibits strong performance across diverse\n--- Page 21 ---\nNLP tasks, with particular strengths in reasoning\nand coherent text generation.\nLLaMA-3.1-8B-Instruct. LLaMA-3.1-8B-\nInstruct (Dubey et al., 2024) is a 8-billion\nparameter large language model built on the\nLLaMA architecture. This instruction-tuned\nvariant exhibits strong performance across diverse\nNLP tasks, with particular strengths in reasoning\nand coherent text generation.\n--- Page 22 ---\n}According to the evidence provided and your knowledge, choose the best choice from the following options. \nEvidence: {evidence}\nQuestion: {question}\n{options}\nThe answer should be concise and short, not with explanations nor option details. Fill your selected option in **uppercase le tter \nformat** into the template: <a>...</a>, for example <a>H</a>.\nThe final answer is: <a>End-to-End Baseline Prompt:Figure 10: The prompt of the End-to-End QA baseline method.\n}According to the evidence provided and your knowledge, choose the best choice from the following options. \nEvidence: Otters are semi -aquatic mammals that prefer to inhabit areas near water bodies for easy access to drinking water and \nfood.\nQuestion: Otters enter their new habitat. What happened as a more possible result?\nA. Otters start looking for abalone for food.\nB. They always live by the water so that they can drink water easily.\nThe final answer is: <a>A</a>\nAccording to the evidence provided and your knowledge, choose the best choice from the following options.\nEvidence: Buying a variety of textbooks can provide a wide range of information, making it easier to find specific details qu ickly.\nQuestion: Lila can find what she wants quickly. What is the more possible cause of this?\nA. Lila bought several kinds of textbooks.\nB. Lila loves classification of her things.\nThe final answer is: <a>B</a>\nAccording to the evidence provided and your knowledge, choose the best choice from the following options.\nEvidence: Hunting cottontails is often associated with drinking rum as a tradition or ritual.\nQuestion: He got some rum. What is the more possible cause of this?\nA. The worker fremented  some sugar cane with yeast.\nB. Tom went out and want to hunt some cottontails.\nThe final answer is: <a>A</a>\nEvidence: {evidence}\nQuestion: {question}\n{options}\nThe final answer is: <a>Few -Shot Baseline Prompt:\nFigure 11: The prompt of the Few-Shot QA baseline method.\n}According to the evidence provided and your knowledge, choose the best choice from the following options. \nEvidence: {evidence}\nQuestion: {question}\n{options}\nPlease think step by step to answer the question and fill in the template: <t>...</t> with your thoughts. And then generate y our \nselected option to fill in the template: <a>...</a> in **uppercase letter format**, for example <a>H</a>.\nStep -by-step thought: <t>COT Baseline Prompt:\nFigure 12: The prompt of the COT (Wei et al., 2022) baseline method.\n--- Page 23 ---\n}Answer the question based on the given context and your own knowledge respectively. \nFormat Examples:\n---\nContext: [Some Context]\nQuestion: [The Given Question]\nFollow up: [You should ask follow up questions.]\nFollow up: [You can ask more than one questions.]\n...\nAnswer: <a> E </a>\n---\nYou have to generate the final answer about the question after ask several follow -up questions. The answer should be concise \nand short, not with explanations nor option details. Fill your answer in **uppercase letter format** into the template: <a>.. .</a>.\n---\nContext: {evidence}\nQuestion: {question}\n{options}\nFollow up:Self-Ask Prompt:Figure 13: The prompt of the Self-Ask (Press et al., 2023) QA baseline method.\n}Generate two answers to the given question: ANSWER1 solely based on the given context and ANSWER2 solely based on your \nown knowledge. Then based on the two answer give the final answer and fill your final answer in **uppercase letter format** \ninto the template: <a>...</a>, for example <a>H</a>.\nContext: {evidence}\nQuestion: {question}\n{options}\nAnswer the question solely based on the given context.\nAnswer:Comparative Prompt:\nFigure 14: The prompt of the comparative (Wang et al., 2023) baseline method.\n}Generate some knowledge about the input.\nInput: {question}\nKnowledge:GKP Prompt (Generation Phase):\nFigure 15: The prompt of the generation phase of GKP (Liu et al., 2022) baseline method.\n--- Page 24 ---\n}You are doing a question answering task with a provided evidences to assist you to answer the question. According to the \nevidence provided and **your own knowledge**, choose the best choice from the following options.\n# Here is the question: \n{question}\n{options}\n# Instructions: \nPlease firstly think step by step to answer the question and fill in the template: <t>...</t> with your thought. And then gen erate \nyour selected option to fill in the template: <a>...</a> in **uppercase letter format**. For example:\n---- Example Begins: ----\n# Step -by-step thought: <t>I think firstly we need to ... </t>\n# Answer: <a> E </a>\n---- Example Ends ----\n# Here is the retrieved evidence: \n{evidence_1}      \n# Here is my own knowledge: \n{evidence_2}\n# There may be some conflict between the provided evidence and your own knowledge. You are a detective and you MUST find \nout which side is more reliable based on your knowledge and choose the correct answer of the question!\nYou have to think of the provided evidence and also fully consider your answer when disregarding the provided evidence.\n---- Task Begins: ----\n# Step -by-step thought: <t>GKP Prompt (Answer Phase):Figure 16: The prompt of the answering phase of GKP (Liu et al., 2022) baseline method.\n--- Page 25 ---\n}### Task: Knowledge Conflict LocationYou are a detective. When you ask a question, the suspects provide an evidence, which may conflict with your knowledge. Your objective is to determine the answer by identifying and locating any points of conflict. #### Task Guidelines:You have a maximum of **eight turns** to locate all conflict points. In each turn, you may choose **one** of the following actions:Action Choices:1. **knowledge_gen(target: str) -> str**: Input the target question, this function adopts LLM to generate the LLM inherent knowledge about the question. Use in format: **knowledge_gen(target=\"...\")**.2. **reason(evidence: str) -> str**: Input an evidence to generate a step-by-step reasoning path to answer given question based on that evidence. Use in format: **reason(evidence=\"...\")**.3. **decompose(reasoning_step: str) -> list of str**: This action can **ONLY** be used when one reasoning step can be decomposed for **better** analysis and benefiting the QA task. This action can break down a complex reasoning step into smaller, manageable sub-steps. Use in format: **decompose(reasoning_step=\"...\")**.4. **assert(state_left: str, state_right: str) -> bool**: Input two statements to evaluate if they conflict, returning a confidence rating. Use in format: **assert(state_left=\"...\", state_right=\"...\")**. This action should only be used when necessary.5. **terminate()**: Signals the end of the session. Use it **ONLY** when confident in your final answer. Format: **terminate()**.#### Problem-Solving Process:You will alternate between **Thought**, **Action**, and **Observation** phases in each turn. - In the **Thought** section, consider your current information and decide your next step.- In the **Action** section, apply **one** of the four actions listed above.- After each action, you will receive an **Observation** to help guide your next move.**Remember**: You can take only **one action per turn** and have a total of **eight turns** to determine the final answer. When confident in your answer, use **terminate()** to end the session.# Question: {question}{options}# Given evidence: ```given_evidence = ''' {evidence} '''```-------------------------------NOTE: Begin your process now to answer the question above, filling in your action in the template: <f>...</f>.# 8 turns remaining to provide the final answer. In this turn, your task is to generate **Thought** and **Action**. **Do NOT answer question this time!** Please keep your thought brief and focused, and **You MUST choose an action from ['knowledge_gen', 'reason', 'decompose', 'assert', 'terminate'] as describe in the Task Guidelines section above!** Then fill your action in the template '**Action:** <f>...</f>'. You can take previous steps as example. Failure to generate an action will result in strict penalties and you will lose your job!## Turn 1:**Thought:** The evidence is too complex. I want to get the step-by-step reasoning path to answer the question above based on the given evidence for easier analysis.**Action:** <f>reason(evidence=given_evidence)</f>**observation**: Step-by-step reasoning path based on given evidence is list below:[[cot_1]]# 7 turns remaining to provide the final answer. In this turn, your task is to generate **Thought** and **Action**. **Do NOT answer question this time!** Please keep your thought brief and focused, and **You MUST choose an action from ['knowledge_gen', 'reason', 'decompose', 'assert', 'terminate'] as describe in the Task Guidelines section above!** Then fill your action in the template '**Action:** <f>...</f>'. You can take previous steps as example. Failure to generate an action will result in strict penalties and you will lose your job!## Turn 2:**Thought:** Then I want to get the step-by-step reasoning path to answer the question above based on my inherent knowledge. To do this, I want to firstly generate the knowledge based on my inherent pre-trained knowledge.**Action:** <f>knowledge_gen(target=\"[[question]]\")</f>**observation**: My inherent knowledge about this question is:[[evidence_gen]]# 6 turns remaining to provide the final answer. In this turn, your task is to generate **Thought** and **Action**. **Do NOT answer question this time!** Please keep your thought brief and focused, and **You MUST choose an action from ['knowledge_gen', 'reason', 'decompose', 'assert', 'terminate'] as describe in the Task Guidelines section above!** Then fill your action in the template '**Action:** <f>...</f>'. You can take previous steps as example. Failure to generate an action will result in strict penalties and you will lose your job!## Turn 3:**Thought:** Now I want to get the step-by-step reasoning path to answer the question above based on my generate knowledgein previous step.**Action:** <f>reason(evidence=\"[[evidence_gen]]\")</f>**observation**: Step-by-step reasoning path based on model knowledge is list below:[[cot_2]]# 5 turns remaining to provide the final answer. In this turn, your task is to generate **Thought** and **Action**. **Do NOT answer question this time!** Please keep your thought brief and focused, and **You MUST choose an action from ['knowledge_gen', 'reason', 'decompose', 'assert', 'terminate'] as describe in the Task Guidelines section above!** Then fill your action in the template '**Action:** <f>...</f>'. You can take previous steps as example. Failure to generate an action will result in strict penalties and you will lose your job!## Turn 4:**Thought:** Now I have already obtained the two reasoning paths from given evidence in 'Turn 1' and from my inherent knowledge in 'Turn 3' respectively. I must locate all conflict points between them. To do so, I will compare the reasoning steps between them. First, I will check if any key reasoning steps need to be decomposed for clearer comparison. If none do, I will proceed with the reasoning step comparison to locate each conflict point.Micro-Act Prompt (Ours):Figure 17: The prompt of our proposed M ICRO -ACTmethod.",
  "text_length": 91330
}