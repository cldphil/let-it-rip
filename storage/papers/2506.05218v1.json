{
  "id": "http://arxiv.org/abs/2506.05218v1",
  "title": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm",
  "summary": "We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.",
  "authors": [
    "Zhang Li",
    "Yuliang Liu",
    "Qiang Liu",
    "Zhiyin Ma",
    "Ziyang Zhang",
    "Shuo Zhang",
    "Zidun Guo",
    "Jiarui Zhang",
    "Xinyu Wang",
    "Xiang Bai"
  ],
  "published": "2025-06-05T16:34:57Z",
  "updated": "2025-06-05T16:34:57Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05218v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05218v1  [cs.CV]  5 Jun 2025MonkeyOCR: Document Parsing with a\nStructure-Recognition-Relation Triplet Paradigm\nZhang Li1, Yuliang Liu1,â€ , Qiang Liu2, Zhiyin Ma1, Ziyang Zhang1,\nShuo Zhang1,Zidun Guo1,Jiarui Zhang2,Xinyu Wang1,Xiang Bai1\n1Huazhong University of Science and Technology,2Kingsoft Office\nFigure 1: Performance comparison of MonkeyOCR and other SOTA models on OmniDocBench [ 33].\nâ€œOverallâ€ represents the comprehensive evaluation across nine document types in OmniDocBench.\nAbstract\nWe introduce MonkeyOCR , a vision-language model for document parsing that advances the state\nof the art by leveraging a Structure-Recognition-Relation (SRR) triplet paradigm. This design\nsimplifies what would otherwise be a complex multi-tool pipeline (as in MinerUâ€™s modular approach)\nand avoids the inefficiencies of processing full pages with giant end-to-end models (e.g., large\nmultimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted into three fundamental\nquestions â€“ â€œWhere is it?â€ (structure), â€œWhat is it?â€ (recognition), and â€œHow is it organized?â€\n(relation) â€“ corresponding to layout analysis, content identification, and logical ordering. This\nfocused decomposition balances accuracy and speed: it enables efficient, scalable processing without\nsacrificing precision. To train and evaluate this approach, we introduce the MonkeyDoc (the most\ncomprehensive document parsing dataset to date), with 3.9 million instances spanning over ten\ndocument types in both Chinese and English. Experiments show that MonkeyOCR outperforms\nMinerU by an average of 5.1%, with particularly notable improvements on challenging content such\nas formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter model surpasses much\nlarger and top-performing models, including Qwen2.5-VL (72B) and Gemini 2.5 Pro, achieving\nstate-of-the-art average performance on English document parsing tasks. In addition, MonkeyOCR\nprocesses multi-page documents significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed for inference\non a single NVIDIA 3090 GPU. Code and models will be released at https://github.com/\nYuliang-Liu/MonkeyOCR .\nTechnical Report. â€ Project lead.\n--- Page 2 ---\n1 Introduction\nDocument parsing is a foundational technology that transforms unstructured, multimodal content,\nincluding text, tables, images, formulas, and more, within various document formats into structured,\nmachine-readable information. This capability underpins a wide range of real-world applications,\nsuch as automated business workflows, digital archiving, intelligent education, and medical record\nmanagement, accelerating the digitization and automation of information-centric industries.\nUnlike traditional OCR or basic visual recognition tasks, document parsing must contend with the\ndiversity of layouts, layered visual hierarchies, and the seamless integration of multiple modalities.\nModern documents frequently combine dense text, complex tables, mathematical expressions, em-\nbedded graphics, and handwritten annotations, often in a mix of languages and formats. This inherent\ncomplexity poses unique challenges: systems must not only detect and recognize content at a granular\nlevel, but also reconstruct the underlying structure and semantic relationships that are critical for\ndownstream applications.\nGiven the multifaceted challenges posed by document parsing, existing solutions have evolved along\ntwo principal paradigms: pipeline-based andend-to-end approaches:\nâ€¢Pipeline-based approaches , such as MinerU [ 43] and Marker [ 35], decompose the document pars-\ning workflow into a series of specialized, fine-grained sub-tasks, including layout analysis, region\nsegmentation, text recognition, table and formula detection, and structural reconstruction, with\neach step handled by distinct models or tools. This modular design enables targeted optimization of\nindividual components and flexible integration of state-of-the-art algorithms for specific subtasks.\nHowever, a critical limitation of this paradigm is the accumulation of errors across the pipeline:\ninaccuracies at early stages, such as imperfect region detection or misclassification, propagate\nthrough subsequent modules, compounding their impact on the final output. For example, Figure 2\ndemonstrates how imprecise formula detection may result in partial cropping of characters from\nthe preceding line, leading to erroneous recognition outcomes such as the inclusion of extraneous\nsuperscripts. Such cumulative errors can significantly degrade the overall reliability and accuracy\nof pipeline-based systems, especially when dealing with complex, information-dense documents.\nâ€¢End-to-end approaches , exemplified by models such as Qwen2.5-VL-7B [ 1], seek to address the\nlimitations of modular pipelines by processing entire document pages or substantial regions within a\nunified neural network. These models directly generate structured representations from raw inputs,\nstreamlining the workflow and enabling joint optimization across tasks. Despite showing strong\npotential for holistic understanding, this approach still faces significant computational challenges.\nModern documents often contain high-resolution, information-dense layouts, resulting in extremely\nlong input and output sequences. The quadratic complexity of attention mechanisms and the\nnecessity to model long-range dependencies impose substantial limitations on both inference speed\nand model scalability, particularly in large-scale or production environments. For example, the\ninference speed of Qwen2.5-VL-7B [ 1] is merely 18% of MinerU [ 43] and performs worse overall,\nas illustrated in Figure 1.\nText Block ImagePipeline-based Toolchains\nDetection\nMHAT The masked multi-head attention (MHAT) module in each transformer layer â„“, contains four projection matrixes: ð‘Š!\", ð‘Š#\", ð‘Š$\", ð‘ŠÌ‡%\"âˆˆâ„&Ã—&. For the multi-head attention, the input ð»\"()is first projected to query, key and value: ð‘„\"=\tð»\"()ð‘Š!\", ð¾\"=\tÎ¨+ð»+\"()ð‘Š#\", ð‘‰\"=\tð»\"()ð‘Š$\". Then the projected query, key and value matrices are evevlysplit along the columns to ð»different heads: {ð‘„\",+}+,)-, {ð¾\"Ì…,+}+,)-, {ð‘‰\"Ì…,+}+,)-âˆˆâ„/Ã—!\", respectively. After splitting ð‘Š%\"into {ð‘Š\",+}+,)-âˆˆâ„/Ã—!\"â€¦â€¦Recognition\nFigure 2: Illustration of error propagation in pipeline-based document parsing toolchains. In\nthe middle panel, the pipelineâ€™s detection module inaccurately segments a formula region, causing\npart of the formula to overlap with text from the preceding line. This leads to recognition errors in the\nright panel, where extraneous superscript characters are mistakenly included in the formula output.\nTo address the limitations of existing document parsing approaches and achieve an optimal balance\nbetween accuracy and efficiency, we introduce MonkeyOCR , a novel system based on a Structure-\nRecognition-Relation (SRR) triplet paradigm. In contrast to previous approaches, MonkeyOCR\n--- Page 3 ---\nfirst performs block-level structure detection to accurately segment semantic regions, including\ntext blocks, tables, formulas, images, and other components within each document. Each region is\nthen subjected to recognition by a unified, Large Multimodal Model (LMM), enabling end-to-end\nrecognition across diverse content types without the error propagation typical of traditional pipelines.\nFinally, a block-level reading order prediction mechanism models the relations between detected\nregions, reconstructing their logical and semantic connections to generate high-fidelity structured\noutputs. This SRR design effectively combines the interpretability and modularity of pipeline methods\nwith the global optimization and simplicity of end-to-end architectures.\nTo support robust and generalizable model training, we further develop MonkeyDoc , the largest\nand most diverse document parsing dataset to date. MonkeyDoc comprises 3.9 million block-\nlevel instances, covering five core document parsing tasks and over ten document types, with full\nsupport for both Chinese and English. The dataset is constructed through a multi-stage pipeline\nthat integrates meticulous manual annotation, programmatic synthesis, and model-driven automatic\nlabeling, ensuring both quality and coverage.\nExtensive experiments demonstrate that MonkeyOCR achieves state-of-the-art overall performance\nacross nine document types, including academic papers, textbooks, handwritten notes, and densely\nformatted newspapers, in both Chinese and English. Compared to the leading pipeline-based system\nMinerU [ 43] and the advanced end-to-end model Qwen2.5-VL-7B [ 1], MonkeyOCR delivers sub-\nstantial improvements: 8.6% higher TEDS for table recognition, 15.0% better CDM for formula\nrecognition, and over 10% lower edit distance on both English and Chinese tasks. Notably, our 3B\nmodel even outperforms the much larger Qwen2.5-VL-72B [ 1] and Googleâ€™s flagship commercial\nmodel, Gemini 2.5 Pro, on key English benchmarks. In terms of efficiency, MonkeyOCR achieves an\ninference speed of 0.24 pages per second for single-page documents (compared to MinerUâ€™s 0.28\npages per second) and 0.84 pages per second for multi-page documents (compared to MinerUâ€™s 0.65\nand 0.12 for Qwen2.5-VL-7B pages per second), offering competitive or superior throughput in\nreal-world scenarios.\n2 Related Work\nRecent advances in document parsing have shifted the field from traditional rule-based and heuristic\nmethods to deep learning-based techniques, enabling significant improvements in robustness and\nadaptability across diverse document formats. Contemporary approaches can be broadly classified\ninto two categories: pipeline-based andend-to-end approaches, each offering distinct trade-offs in\nflexibility, scalability, and performance.\n2.1 Pipeline-based Approaches\nPipeline-based approaches [ 4;24;35;43] decompose the document parsing workflow into a sequence\nof specialized sub-tasks, such as layout analysis [ 48;14;41], reading order prediction [ 44], Optical\nCharacter Recognition (OCR) [ 15;17], formula recognition [ 42], and table structure recognition [ 34;\n12;46], with each task handled by a dedicated model or module. This modular design enables\nindependent optimization and straightforward integration of advanced algorithms at each stage.\nRepresentative works include Docling [ 24], which implements a linear pipeline to extract content\nfrom PDF files and generate structured outputs in JSON or Markdown formats, and Marker [ 35],\nwhich supports a broad range of document types (e.g., images, PDFs, PPTX, DOCX) and exports to\nvarious formats via a combination of Surya OCR, layout analysis, reading order prediction, and table\nrecognition modules. Marker [ 35] further integrates LLM-based components to enhance cross-page\ntable merging and inline mathematical expression parsing. MinerU [ 43] extends this paradigm with a\nmodular architecture for layout detection, content recognition (including text, formulas, and tables),\nand output structuring.\nAlthough pipeline-based approaches have driven much progress through modular design and flexible\nintegration of specialized models, their tendency to propagate errors across sequential stages remains\na key bottleneck, particularly for complex, high-density documents. The proposed MonkeyOCR\nseeks a more robust alternative that decouples error sources and enables efficient, high-fidelity parsing\nacross diverse document scenarios.\n--- Page 4 ---\n2.2 End-to-end Approaches\nEnd-to-end approaches aim to simplify document parsing by directly processing entire document\npages or substantial regions through a unified neural network, generating structured outputs in a\nsingle stage without the need for multiple specialized models. Early representative methods include\nDonut [ 16], which introduces an OCR-free, Transformer-based framework unifying tasks such as\ndocument classification, visual question answering, and information extraction; and Nougat [ 3], which\nfocuses on converting document images into well-formatted markup text. GOT [ 45] generalizes\nOCR to recognize diverse optical signals, including text, formulas, tables, charts, musical scores, and\ngeometric shapes, under a unified character concept. The SPTS series [ 36;23] and OmniParser [ 40]\nfurther advance unified text detection and recognition, as well as key information extraction and table\nparsing in both natural scene and document images.\nRecent progress has been driven by Large Multimodal Models (LMMs) trained on massive document\ncorpora. For example, Monkey [ 19;22] enhances document understanding through high-resolution im-\nage cropping strategies, while mPLUG-DocOwl2 [ 13] introduces cross-page modeling for structural\nreasoning in multi-page documents. InternVL3 [ 5] leverages joint pretraining on text and multimodal\ndata to enable improved cross-modal alignment and long-context understanding. Qwen2.5-VL [ 1]\nproposes a fine-grained parsing format (QwenVL HTML) that captures both textual content and\nlayout, supporting accurate reconstruction of diverse document types. Meanwhile, olmOCR [ 38] is\ntailored for document parsing through large-scale PDF corpus finetuning and efficient inference via\nthe SGLang [50] framework.\nWhile end-to-end approaches have advanced the field by enabling unified modeling and reducing\nmanual intervention, practical deployment in real-world document parsing remains constrained\nby scalability and efficiency concerns, particularly when handling heterogeneous, large-scale, or\nmultilingual collections. Addressing these persistent gaps motivates our work, which seeks a more\neffective balance between recognition accuracy and computational efficiency for robust document\nunderstanding across diverse scenarios.\n2.3 Document Parsing Dataset\nA variety of datasets have been developed to support fine-grained document parsing tasks, including\nlayout detection, reading order prediction, table and formula recognition, text extraction, and code\nblock identification, across a wide spectrum of document types such as textbooks [ 6], academic\npapers [46; 37; 53], newspapers [6; 8], and slides [8].\nFor general layout analysis, M6Doc [ 6] provides 9,080 diverse document images labeled with 74\nlayout categories, while CDLA [ 18] focuses on Chinese academic papers, and D4LA [ 8], derived\nfrom RDL-CDIP [ 11], offers annotated data for 12 document types with rich layouts. DocLayNet [ 37]\nfurther expands this coverage, containing over 80,000 pages of human-annotated layout segmentation\ndata from various sources.\nDatasets for specific element recognition are also well-represented: FinTabNet [ 51] provides detailed\nstructural annotations of complex tables from financial reports; PubTabNet [ 52] includes over\n560,000 table images from scientific literature, each with a corresponding HTML representation;\nUnimer-1M [ 42] offers more than one million complex mathematical expression instances; and HME-\n100k [ 47] is a large-scale real-world handwritten mathematical expression recognition dataset. For\nreading order prediction, ReadingBank [ 44] is a weakly supervised benchmark containing 500,000\ndiverse document images with word-level reading order annotations. However, its focus on word-level\nannotations presents challenges for directly evaluating block-level reading order, which is critical for\ncomplex layouts such as multi-column or cross-page documents [ 20]. Additionally, DocGenome [ 46]\nis a large-scale structured document dataset created by annotating 500,000 scientific papers from\nthe arXiv open-access repository, spanning 153 disciplines and covering all major document parsing\ntasks, making it a comprehensive English-language academic corpus.\nDespite the diversity and scale of existing datasets, most are limited to single tasks, specific document\ntypes, or a single language, and few offer large-scale, fine-grained annotations across both English\nand Chinese or multiple document domains. This gap continues to motivate the development of more\ncomprehensive resources for robust and versatile document parsing.\n--- Page 5 ---\n3 MonkeyDoc Dataset\nExisting document parsing datasets typically focus on single tasks, specific document types, or\na single language, which limits their effectiveness for developing and evaluating comprehensive\ndocument parsing systems. To address the need for a more versatile and large-scale resource that is\ncapable of supporting end-to-end document parsing across a wide variety of real-world scenarios, we\nintroduce MonkeyDoc .\nMonkeyDoc is designed to cover a complete range of document parsing tasks, including layout\ndetection, reading order prediction, text recognition, table recognition, formula recognition, and\ncode block recognition. As shown in Table 1, MonkeyDoc spans more than ten diverse document\ndomains and provides high-quality annotations in both Chinese and English, making it the most\ncomprehensive resource of its kind to date. In comparison, previous datasets are often limited to\na subset of tasks, a single document type, or monolingual settings. MonkeyDoc uniquely enables\nmulti-task, multi-domain, and bilingual training and evaluation, supporting both fine-grained and\nholistic document understanding.\nTo achieve this breadth and depth, we developed an integrated data generation pipeline (see Figure 4)\nthat combines filtering and harmonization of existing public datasets, meticulous manual annotation,\nprogrammatic data synthesis, and expert model-driven automatic labeling. This pipeline is organized\naround three core stages: Structure Detection ,Content Recognition , and Relation Prediction . For\neach stage, we leverage a combination of open-source resources, advanced annotation strategies, and\nmodel-assisted workflows, ensuring high annotation quality, data diversity, and scalability across\nlanguages and document types. This multi-faceted design makes MonkeyDoc a foundational resource\nfor robust model training, benchmarking, and deployment in both academic and industrial applications.\nIn the following sections, we provide a detailed description of the data construction process for each\nstage.\nDatasetDocument\nDomainSupporting Tasks Language\nLayout\nDetectionReading Order\nPredictionFomula\nRecognitionTable\nRecognitionText\nRecognitionEN ZH\nLayout Detection Dataset\nM6Doc [6] 6 ! ! !\nCDLA [18] 1 ! !\nD4LA [8] 5 ! !\nDocLayNet [37] 2 ! ! !\nFomula Recognition Dataset\nUnimernet [42] - !\nTexTeller [31] - !\nTable Recognition Dataset\nFinTabNet [51] - !\nPubTabNet [52] - !\nComprehensive Dataset\nDocGenome [46] 1 ! ! ! ! ! !\nMonkeyDoc >10 ! ! ! ! ! !!\nTable 1: Comparison of MonkeyDoc with Other Document Parsing Datasets. MonkeyDoc covers all\ndocument parsing tasks and the largest variety of document types, in both Chinese and English.\n3.1 Structure Detection\nStructure detection aims to identify and localize key elements within documents, such as text\nblocks, tables, images, and other layout components, by assigning category labels and bounding\nbox coordinates to each region. Constructing a high-quality structure detection dataset poses several\nchallenges, including inconsistent annotation standards across public datasets and the scarcity of\nannotated Chinese documents.\nTo address these issues, we begin by aggregating and filtering data from several publicly available\nstructure detection datasets, including M6Doc [ 6], DocLayNet [ 37], D4LA [ 8], and CDLA [ 18],\n--- Page 6 ---\nSlides\nNewspaper\nNotes\nAcademicPaper\nExam  Paper\nMind Map\nFinancial  ReportMagazine\nInstruction Manual CHART\nCHARTCHARTFIGUREFIGUREFIGUREFIGURETEXT\nTEXT\nTEXTFormulaFormula\nTextbookFormula\nResumeTEXT\nSynthesis FormulaTitleSynthesis TableFigure 3: Visualization of the MonkeyDoc dataset. MonkeyDoc encompasses more than ten\ndocument types and includes our synthesized tables and formulas data.\ncovering both Chinese and English documents, with a total of 88k pages. We harmonize category\nlabels by mapping disparate annotation schemes to a unified set of eleven classes, following the\nconventions established in MinerU [ 43]. To ensure consistency and data quality, we remove nested\nbounding boxes by retaining only the largest region for each element and filter out low-information\ninstances by discarding bounding boxes whose area is less than 35% of the page. These steps\ncollectively improve both the reliability and comparability of the annotations.\nGiven the limited availability of high-quality Chinese data, we supplement our dataset through targeted\nsynthesis. We collect over 300,000 pages spanning more than ten types of Chinese documents (see\nFigure 3), such as financial reports, textbooks, and academic papers. After initial pre-annotation\nwith existing structure detection models, we conduct post-processing, including nested box removal\nand low-information filtering, to obtain 28,000 high-quality samples. Additionally, we manually\nverified and refined a subset of pre-annotated data, resulting in a further 13,000 manually corrected\nChinese samples. Through this multi-source, multi-step process, we construct a diverse and consistent\nstructure detection dataset that supports robust model training and evaluation across both English and\nChinese document types.\n3.2 Content Recognition\nContent recognition encompasses the identification and transcription of essential document elements,\nincluding text blocks, tables, formulas, and code blocks, across diverse formats and languages. The\ncontent recognition pipeline integrates several complementary strategies to maximize coverage and\nannotation quality:\nCropping Document Elements. Based on the layout annotations produced in the Structure Setection\nstage, we segment and crop individual elements, including text blocks, formula regions, tables, and\ncode blocks, from the original document images, resulting in a total of 1.9 million samples. Partial\nelements are transcribed and labeled using Googleâ€™s flagship commercial model (Gemini 2.5 Pro) to\nensure annotation accuracy.\n--- Page 7 ---\nFigure 4: Overview of the three core stages in the MonkeyDoc data generation pipeline. Struc-\nture Detection aggregates and harmonizes open-source datasets, supplemented with synthesized\nhigh-quality Chinese samples; Content Recognition utilizes both manual and automated annotation,\nincluding synthetic data generation and element extraction; Relation Prediction combines manual\nannotation with model-assisted strategies to establish reading order and region relationships.\nFiltering from Open-Source Datasets. For table recognition, we select and refine data from\nPubTabNet [ 52], applying stringent quality checks such as HTML tag closure, header presence,\nmerged cell validation, header-body alignment, abnormal character detection, and syntax verification.\nThis process yields a curated dataset of 470,000 tables. For formula recognition, we leverage\nthe UniMER-1M [ 42] dataset, which aggregates formulas from diverse public sources, including\nPix2tex [ 2], CROHME [ 28;29;25] and HME100K [ 47], and large-scale collections of LaTeX\nexpressions from ArXiv, Wikipedia, and StackExchange, covering both printed and handwritten\nstyles.\nSynthesizing Chinese Data. To mitigate the shortage of Chinese samples for table and formula\nrecognition, we programmatically synthesize data with high structural diversity. For tables, we\nconstruct layouts with various row and column configurations, populate them with Chinese content,\nand render paired HTML and image data. For formulas, we generate commonly used expressions\nin Chinese with large multimodal models, and also translate and render English formulas from\nUniMER-1M [ 42] using large language models. This approach produces a total of 526,000 additional\nChinese samples.\narXiv Data Extraction. To further enhance the datasetâ€™s breadth, we extract and process LaTeX\nsource data for tables and formulas from arXiv papers. Irrelevant content is filtered using large\nlanguage models, and the resulting data is rendered into images and structured annotations, adding\n36,000 high-quality academic samples.\n3.3 Relation Prediction\nRelation prediction focuses on determining the logical reading order among detected document\nelements, which is essential for reconstructing coherent and semantically faithful document content,\nespecially in cases involving complex layouts, multi-column pages, or cross-page structures. Building\na region-level reading order dataset presents significant challenges due to the scarcity of annotated\ndata and the complexity of diverse document forms.\nOpen-source Data Refinement. The primary open-source resource is DocGenome [ 46], which\nprovides region-level reading order annotations generated through automated labeling. However,\nthese annotations can be noisy or incomplete, particularly for elements like images and tables. To\nimprove annotation accuracy, we refine these labels by explicitly associating each image and table\nwith its corresponding caption, and filter out low-quality samples, such as pages with unannotated\nregions or excessive blank areas. We further score each page based on the diversity of element types,\n--- Page 8 ---\nselecting high-scoring pages for inclusion. This results in a curated set of 951,000 high-quality\nsamples.\nManual Annotation for Chinese Documents. To address the limited availability of Chinese region-\nlevel reading order annotations, we manually annotate a diverse set of Chinese documents, including\nresearch reports, academic papers, user manuals, books, test papers, slides, official documents,\nnewspapers, journals, and contracts. This effort produces an additional 154,000 high-quality samples,\nsubstantially enhancing the representation of Chinese document scenarios.\nExpert Model-Based Auto-Annotation. For datasets that provide only region-level bounding boxes\nwithout reading order information, we leverage expert models to generate region-level reading order\nannotations automatically. Specifically, we utilize PPOCR [ 17] for line-wise text recognition within\neach region, obtain text line positions, and then apply LayoutReader [ 44] to predict the reading order\nof these lines. The region-level order is determined by aggregating the predicted order of all text lines\nwithin each region. Through this approach, we generate 78,000 additional region-level annotations,\nfurther enriching the diversity and coverage of our dataset.\n4 MonkeyOCR\nFigure 5: The overall architecture of MonkeyOCR. The system adopts a Structure-Recognition-\nRelation framework, consisting of structure detection, which locates and classifies semantic regions;\nblock-level content recognition, which extracts structured information from each region in parallel;\nand relation prediction, which determines the logical reading order of the detected elements.\nThe proposed method, MonkeyOCR , addresses the fundamental limitations of both pipeline-based\nand end-to-end document parsing approaches by introducing a modular yet globally optimized\nStructure-Recognition-Relation (SRR) framework. As illustrated in Figure 5, we decompose the\ndocument parsing process into three relatively independent but tightly integrated stages: structure\ndetection ,block-level content recognition , and relation prediction . This design aims to mitigate the\ncumulative error typically observed in pipeline toolchains, while also improving inference efficiency\nby reducing the context length compared to monolithic end-to-end models.\nIn the first stage, a YOLO-based [ 49] document layout detector processes the input image Iâˆˆ\nRHÃ—WÃ—3, producing a set of bounding boxes B={b1, b2, . . . , b n}and their corresponding element\ntypes T={t1, t2, . . . , t n}. Each bounding box bi= (x1i, y1i, x2i, y2i)represents the spatial\ncoordinates of the i-th element, and the element type tiâˆˆ {text,table,formula ,figure , . . .}specifies\nthe category of the detected element.\nFor the second stage, we perform block-level content recognition in parallel. Each detected region bi\nis cropped and, together with a type-specific prompt pti, is fed into our LMM for type-aware content\nextraction:\nC=LMM ({I1\ncrop, I2\ncrop, . . . , In\ncrop},{pt1, pt2, . . . , p tn}),\n--- Page 9 ---\nwhere Ii\ncropdenotes the region cropped based on the bounding box bi, and C={c1, c2, . . . , c n}\ndenotes the structured content outputs.\nIn the final stage, relation prediction is carried out to infer the logical reading order among detected\nelements. The set of bounding boxes B={b1, b2, . . . , b n}input to a dedicated block-level reading\norder model, which predicts a sequence S={s1, s2, . . . , s n}, assigning each element a position in\nthe final reading sequence. The recognized content is then reassembled as D={cs1, cs2, . . . , c sn}.\nThis SRR pipeline ensures accurate structural understanding, precise region-level recognition, and\nfaithful logical ordering, all while achieving improved efficiency and robustness compared to prior\napproaches. By modularizing the parsing process and leveraging block-wise parallelism, MonkeyOCR\nprovides a scalable and reliable solution for real-world document parsing scenarios.\n5 Experiments\nTo validate the effectiveness of MonkeyOCR, we conducted a comprehensive comparison with both\nopen-source and closed-source methods on OmniDocBench [ 33]. OmniDocBench is a benchmark\ndesigned to evaluate real-world document parsing capabilities. It comprises 981 PDF pages spanning\n9 document types, 4 layout styles, and 3 language categories. Through this benchmark, we are able\nto perform a thorough assessment of MonkeyOCRâ€™s document parsing capabilities.\nModel\nTypeMethodsOverallEditâ†“ TextEditâ†“ FormulaEditâ†“FormulaCDMâ†‘TableTEDSâ†‘ TableEditâ†“ Read OrderEditâ†“\nEN ZH EN ZH EN ZH EN ZH EN ZH EN ZH EN ZH\nPipeline\nToolsMinerU [43] 0.150 0.357 0.061 0.215 0.278 0.577 57.3 42.9 78.6 62.1 0.180 0.344 0.079 0.292\nMarker [35] 0.336 0.556 0.080 0.315 0.530 0.883 17.6 11.7 67.6 49.2 0.619 0.685 0.114 0.340\nMathpix [26] 0.191 0.365 0.105 0.384 0.306 0.454 62.7 62.1 77.0 67.1 0.243 0.320 0.108 0.304\nDocling [24] 0.589 0.909 0.416 0.987 0.999 1 - - 61.3 25.0 0.627 0.810 0.313 0.837\nPix2Text [4] 0.320 0.528 0.138 0.356 0.276 0.611 78.4 39.6 73.6 66.2 0.584 0.645 0.281 0.499\nUnstructured [39] 0.586 0.716 0.198 0.481 0.999 1 - - 0 0.06 1 0.998 0.145 0.387\nOpenParse [9] 0.646 0.814 0.681 0.974 0.996 1 0.11 0 64.8 27.5 0.284 0.639 0.595 0.641\nExpert\nVLMsGOT-OCR [45] 0.287 0.411 0.189 0.315 0.360 0.528 74.3 45.3 53.2 47.2 0.459 0.520 0.141 0.280\nNougat [3] 0.452 0.973 0.365 0.998 0.488 0.941 15.1 16.8 39.9 0 0.572 1.000 0.382 0.954\nMistral OCR [27] 0.268 0.439 0.072 0.325 0.318 0.495 64.6 45.9 75.8 63.6 0.600 0.650 0.083 0.284\nOLMOCR-sglang [38] 0.326 0.469 0.097 0.293 0.455 0.655 74.3 43.2 68.1 61.3 0.608 0.652 0.145 0.277\nSmolDocling-256M [30] 0.493 0.816 0.262 0.838 0.753 0.997 32.1 0.55 44.9 16.5 0.729 0.907 0.227 0.522\nGeneral\nVLMsGPT4o [32] 0.233 0.399 0.144 0.409 0.425 0.606 72.8 42.8 72.0 62.9 0.234 0.329 0.128 0.251\nQwen2.5-VL-7B [1] 0.312 0.406 0.157 0.228 0.351 0.574 79.0 50.2 76.4 72.2 0.588 0.619 0.149 0.203\nInternVL3-8B [5] 0.314 0.383 0.134 0.218 0.417 0.563 78.3 49.3 66.1 73.1 0.586 0.564 0.118 0.186\nMixMonkeyOCR-3B 0.140 0.297 0.058 0.185 0.238 0.506 78.7 51.4 80.2 77.7 0.170 0.253 0.093 0.244\nMonkeyOCR-3B* 0.154 0.277 0.073 0.134 0.255 0.529 78.5 50.8 78.2 76.2 0.182 0.262 0.105 0.183\nTable 2: The end-to-end evaluation results of different tasks on OmniDocBench. * represents the use\nof the layout model trained by us with improved capability for Chinese layout detection.\n5.1 Comparison with Other Methods on Different Tasks\nDocument parsing encompasses a variety of sub-tasks, including text recognition, formula recognition,\ntable recognition, reading order detection, and more. To evaluate MonkeyOCRâ€™s performance across\nthese tasks, we compared it with several widely-used methods on OmniDocBench [ 33], including\npipeline tools [ 43;35], expert VLMs [ 45;27], closed-source general VLMs [ 32], and open-source\ngeneral VLMs [ 5;1]. As shown in Table 2, MonkeyOCR achieves the best overall performance on\nboth Chinese and English document parsing tasks. In particular, MonkeyOCR surpasses MinerU [ 43]\nby over 6% in overall edit distance for Chinese documents, exceeds MinerU by an average of 15.0%\nin formula recognition across Chinese and English, and outperforms MinerU by 8.6% on average in\ntable recognition for both languages. Compared to Mistral OCR [ 27], MonkeyOCR improves average\noverall edit distance by 13.8% for Chinese and English, with gains of 9.8% in formula recognition\nand 9.3% in table recognition. Additionally, we trained a specialized version of MonkeyOCR for\nChinese documents - MonkeyOCR*. On OmniDocBench, MonkeyOCR* achieves state-of-the-art\nperformance in Chinese document parsing, surpassing the original MonkeyOCR by 2%.\n5.2 Comparison with Other Methods Across Document Types\nTo further evaluate MonkeyOCRâ€™s capability in handling diverse document types, we conducted\na comprehensive comparison on the OmniDocBench [ 33] benchmark across nine categories of\ndocuments. As shown in Table 3, MonkeyOCR achieved the best overall performance across all nine\n--- Page 10 ---\nModel\nTypeModels Book SlidesFinancial\nReportTextbookExam\nPaperMagazineAcademic\nPapersNotes Newspaper Overall\nPipeline\nToolsMinerU [43] 0.055 0.124 0.033 0.102 0.159 0.072 0.025 0.984 0.171 0.206\nMarker [35] 0.074 0.340 0.089 0.319 0.452 0.153 0.059 0.651 0.192 0.274\nMathpix [26] 0.131 0.220 0.202 0.216 0.278 0.147 0.091 0.634 0.690 0.300\nExpert\nVLMsGOT-OCR [45] 0.111 0.222 0.067 0.132 0.204 0.198 0.179 0.388 0.771 0.267\nNougat [3] 0.734 0.958 1.000 0.820 0.930 0.830 0.214 0.991 0.871 0.806\nGeneral\nVLMsGPT4o [32] 0.157 0.163 0.348 0.187 0.281 0.173 0.146 0.607 0.751 0.316\nQwen2.5-VL-7B [1] 0.148 0.053 0.111 0.137 0.189 0.117 0.134 0.204 0.706 0.205\nInternVL3-8B [5] 0.163 0.056 0.107 0.109 0.129 0.100 0.159 0.150 0.681 0.188\nMixMonkeyOCR-3B 0.046 0.120 0.024 0.100 0.129 0.086 0.024 0.643 0.131 0.155\nMonkeyOCR-3B* 0.054 0.203 0.038 0.112 0.138 0.111 0.032 0.194 0.136 0.120\nTable 3: The end-to-end text recognition performance on OmniDocBench across 9 PDF page types.\n* represents the use of the layout model trained by us with improved capability for Chinese layout\ndetection.\ntypes. Specifically, it attained the highest end-to-end recognition accuracy in six categories. The 3B\nmodel outperformed InternVL3-8B [ 5] by 5% and surpassed MinerU [ 43] by 3.3% in overall accuracy.\nNotably, on the newspaper category, MonkeyOCR outperformed the previous state-of-the-art MinerU\nby 4%, demonstrating its strong capability in parsing dense and complex layouts. These results\nhighlight MonkeyOCRâ€™s superior generalization ability and robustness across various document types.\nMoreover, benefiting from enhanced Chinese language capabilities, MonkeyOCR* outperforms the\noriginal version by 44.9% on the notes category, achieving state-of-the-art overall performance.\n5.3 Implement Details\nDuring the training process, we utilize the AdamW optimizer with a learning rate of 2e-5 and a cosine\nlearning rate schedule. We employ a batch size of 64. Our 3B model was trained for 53 hours on 32\nA800 GPUs. By integrating with LMDeploy [ 7], our model can successfully run on RTX 3090 GPUs.\n86.0 80.985.274.878.656.0 70.373.678.867.373.955.7\n0.020.040.060.080.0100.0\nMonkeyOCR-3BGemini2.0-flashGemini2.5-ProQwen2-VL-72BQwen2.5-VL-72BInternVL2-76BENZH\nFigure 6: End-to-end evaluation on OmniDocBench. Performance comparison of MonkeyOCR\nwith closed-source and extra-large open-source VLMs across different document parsing tasks.\n6 Discussion\nAs is well-established, increasing model scale generally leads to improved performance. To further\nexplore the potential of MonkeyOCR, we conducted comparative evaluations against both larger open-\nsource models and leading closed-source commercial solutions on OmniDocBench. As illustrated\nin Figure 6, MonkeyOCR achieves the highest overall performance on English documents ,\noutperforming Qwen2.5-VL-72B by 7.4% and surpassing the current state-of-the-art closed-source\nmodel, Gemini 2.5-Pro, by 0.8% . However, Gemini 2.5-Pro demonstrates slightly better performance\non Chinese documents, indicating there is still some margin for improvement in MonkeyOCRâ€™s\nChinese document parsing capabilities.\n--- Page 11 ---\n7 Conclusion\nIn this work, we present MonkeyOCR, a document parsing model built on the Structure-Recognition-\nRelation (SRR) triplet paradigm, which unifies structural detection, content recognition, and relational\nordering into a streamlined framework. This design simplifies traditional multi-tool pipelines while\navoiding the inefficiencies of directly processing entire pages with LMMs, enabling both high accuracy\nand efficient deployment. Supported by MonkeyDocâ€”a large-scale, diverse dataset spanning a wide\nrange of document types in Chinese and Englishâ€”MonkeyOCR achieves strong results across\nbenchmarks, outperforming leading open-source (e.g., MinerU [ 43] and Qwen2.5-VL [ 1]) and even\nclosed-source models (e.g., Gemini2.5-Pro [ 10]) in English document parsing. Beyond its immediate\nperformance, MonkeyOCR has the potential to serve as a foundation model for the text domain,\nenabling unified understanding and reasoning over complex document structures [21].\nReferences\n[1]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang\nWan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen\nCheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report.\narXiv preprint arXiv:2502.13923 , 2025.\n[2]Lukas Blecher. pix2tex - latex ocr. https://github.com/lukas-blecher/LaTeX-OCR ,\n2022. Accessed: 2024-02-29.\n[3]Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical\nunderstanding for academic documents. arXiv preprint arXiv:2308.13418 , 2023.\n[4]breezedeus. Pix2text: An open-source python3 tool for recognizing layouts, tables, math\nformulas (latex), and text in images. https://github.com/breezedeus/pix2text , 2025.\n[5]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 24185â€“24198, 2024.\n[6]Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li,\nKai Ding, and Lianwen Jin. M6doc: a large-scale multi-format, multi-type, multi-layout,\nmulti-language, multi-annotation category dataset for modern document layout analysis. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n15138â€“15147, 2023.\n[7]LMDeploy Contributors. Lmdeploy: A toolkit for compressing, deploying, and serving llm.\nhttps://github.com/InternLM/lmdeploy , 2023.\n[8]Cheng Da, Chuwei Luo, Qi Zheng, and Cong Yao. Vision grid transformer for document layout\nanalysis. In Proceedings of the IEEE/CVF international conference on computer vision , pages\n19462â€“19472, 2023.\n[9]Sergey Filimonov. Open parse: Visually-driven document chunking for llm applications.\nhttps://github.com/Filimoa/open-parse , 2025.\n[10] Google DeepMind. Gemini 2.5. https://blog.google/technology/google-deepmind/\ngemini-model-thinking-updates-march-2025/ , 2025.\n[11] Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. Evaluation of deep convolutional\nnets for document image classification and retrieval. In 2015 13th international conference on\ndocument analysis and recognition (ICDAR) , pages 991â€“995. IEEE, 2015.\n[12] Yelin He, Xianbiao Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bingcong Li, Xin Tang, and Rong\nXiao. Pingan-vcgroupâ€™s solution for icdar 2021 competition on scientific table image recognition\nto latex. ArXiv , abs/2105.01846, 2021.\n11\n--- Page 12 ---\n[13] Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and\nJingren Zhou. mplug-docowl2: High-resolution compressing for ocr-free multi-page document\nunderstanding. arXiv preprint arXiv:2409.03420 , 2024.\n[14] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for\ndocument ai with unified text and image masking. In Proceedings of the 30th ACM international\nconference on multimedia , pages 4083â€“4091, 2022.\n[15] Jaided AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/\nJaidedAI/EasyOCR , 2024.\n[16] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,\nWonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document\nunderstanding transformer. In European Conference on Computer Vision , pages 498â€“517.\nSpringer, 2022.\n[17] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du,\nLingfeng Zhu, Baohua Lai, Xiaoguang Hu, et al. Pp-ocrv3: More attempts for the improvement\nof ultra lightweight ocr system. arXiv preprint arXiv:2206.03001 , 2022.\n[18] Hang Li. Cdla: A chinese document layout analysis (cdla) dataset, 2021.\n[19] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang\nLiu, and Xiang Bai. Monkey: Image resolution and text label are important things for large\nmulti-modal models. In proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 26763â€“26773, 2024.\n[20] Shuai Liu, Youmeng Li, and Jizeng Wei. Xy-cut++: Advanced layout ordering via hierarchical\nmask mechanism on a novel benchmark, 2025.\n[21] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin,\nCheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large\nmultimodal models. Science China Information Sciences , 67(12), December 2024.\n[22] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.\nTextmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint\narXiv:2403.04473 , 2024.\n[23] Yuliang Liu, Jiaxin Zhang, Dezhi Peng, Mingxin Huang, Xinyu Wang, Jingqun Tang, Can\nHuang, Dahua Lin, Chunhua Shen, Xiang Bai, et al. Spts v2: single-point scene text spotting.\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 45(12):15665â€“15679, 2023.\n[24] Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos\nVagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, et al. Do-\ncling: An efficient open-source toolkit for ai-driven document conversion. arXiv preprint\narXiv:2501.17887 , 2025.\n[25] Mahshad Mahdavi, Richard Zanibbi, Harold Mouchere, Christian Viard-Gaudin, and Utpal\nGarain. Icdar 2019 crohme+ tfd: Competition on recognition of handwritten mathematical\nexpressions and typeset formula detection. In 2019 International Conference on Document\nAnalysis and Recognition (ICDAR) , pages 1533â€“1538. IEEE, 2019.\n[26] Mathpix. Mathpix snip: Convert images and pdfs to latex, docx, and more. https://mathpix.\ncom/ , 2025.\n[27] Mistral OCR. Mistral ocr: Free online ai ocr tool to extract text. https://www.mistralocr.\ncom/ , 2025.\n[28] Harold Mouchere, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr 2014\ncompetition on recognition of on-line handwritten mathematical expressions (crohme 2014). In\n2014 14th International Conference on Frontiers in Handwriting Recognition , pages 791â€“796.\nIEEE, 2014.\n--- Page 13 ---\n[29] Harold MouchÃ¨re, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr2016\ncrohme: Competition on recognition of online handwritten mathematical expressions. In\n2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR) , pages\n607â€“612. IEEE, 2016.\n[30] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos,\nChristoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, A Said Gurbuz, et al.\nSmoldocling: An ultra-compact vision-language model for end-to-end multi-modal document\nconversion. arXiv preprint arXiv:2503.11576 , 2025.\n[31] OleehyO. Texteller: An end-to-end formula recognition model. https://github.com/\nOleehyO/TexTeller , 2025.\n[32] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o , 2024.\n[33] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang,\nZhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf\ndocument parsing with comprehensive annotations. arXiv preprint arXiv:2412.07626 , 2024.\n[34] Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin, Jerret\nRoss, Ravi Nair, and Erik Altman. Tabular transformers for modeling multivariate time series. In\nICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 3565â€“3569. IEEE, 2021.\n[35] Vik Paruchuri. Marker, 2024.\n[36] Dezhi Peng, Xinyu Wang, Yuliang Liu, Jiaxin Zhang, Mingxin Huang, Songxuan Lai, Jing Li,\nShenggao Zhu, Dahua Lin, Chunhua Shen, et al. Spts: single-point text spotting. In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 4272â€“4281, 2022.\n[37] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S Nassar, and Peter Staar. Doclaynet:\nA large human-annotated dataset for document-layout segmentation. In Proceedings of the 28th\nACM SIGKDD conference on knowledge discovery and data mining , pages 3743â€“3751, 2022.\n[38] Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur,\nChristopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in\npdfs with vision language models. arXiv preprint arXiv:2502.18443 , 2025.\n[39] Unstructured-IO. Unstructured: Open-source etl for complex document transformation. https:\n//github.com/Unstructured-IO/unstructured , 2025.\n[40] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai,\nCong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information\nextraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 15641â€“15653, 2024.\n[41] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et al. Yolov10: Real-time\nend-to-end object detection. Advances in Neural Information Processing Systems , 37:107984â€“\n108011, 2024.\n[42] Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He.\nUnimernet: A universal network for real-world mathematical expression recognition. arXiv\npreprint arXiv:2404.15254 , 2024.\n[43] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen\nLiu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document\ncontent extraction. arXiv preprint arXiv:2409.18839 , 2024.\n[44] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. Layoutreader: Pre-training of\ntext and layout for reading order detection. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages 4735â€“4744, 2021.\n[45] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge,\nLiang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via a unified\nend-to-end model. 2024.\n--- Page 14 ---\n[46] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi,\nDaocheng Fu, Wenjie Wu, Hancheng Ye, et al. Docgenome: An open large-scale scientific\ndocument benchmark for training and testing multi-modal large language models. arXiv preprint\narXiv:2406.11633 , 2024.\n[47] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai.\nSyntax-aware network for handwritten mathematical expression recognition. arXiv preprint\narXiv:2203.01601 , 2022.\n[48] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu,\nand Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 16965â€“16974, 2024.\n[49] Zhiyuan Zhao, Hengrui Kang, Bin Wang, and Conghui He. Doclayout-yolo: Enhancing\ndocument layout analysis through diverse synthetic data and global-to-local adaptive perception.\narXiv preprint arXiv:2410.12628 , 2024.\n[50] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu,\nShiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution\nof structured language model programs. Advances in Neural Information Processing Systems ,\n37:62557â€“62583, 2024.\n[51] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global\ntable extractor (gte): A framework for joint table identification and cell structure recognition\nusing visual context. In Proceedings of the IEEE/CVF winter conference on applications of\ncomputer vision , pages 697â€“706, 2021.\n[52] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition:\ndata, model, and evaluation. arXiv preprint arXiv:1911.10683 , 2019.\n[53] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for\ndocument layout analysis. In 2019 International Conference on Document Analysis and\nRecognition (ICDAR) , pages 1015â€“1022. IEEE, Sep. 2019.",
  "text_length": 49290
}