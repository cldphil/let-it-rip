{
  "id": "http://arxiv.org/abs/2505.24869v1",
  "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
  "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
  "authors": [
    "Ce Zhang",
    "Yan-Bo Lin",
    "Ziyang Wang",
    "Mohit Bansal",
    "Gedas Bertasius"
  ],
  "published": "2025-05-30T17:59:19Z",
  "updated": "2025-05-30T17:59:19Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24869v1"
}