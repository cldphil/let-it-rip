{
  "id": "http://arxiv.org/abs/2505.24869v1",
  "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
  "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
  "authors": [
    "Ce Zhang",
    "Yan-Bo Lin",
    "Ziyang Wang",
    "Mohit Bansal",
    "Gedas Bertasius"
  ],
  "published": "2025-05-30T17:59:19Z",
  "updated": "2025-05-30T17:59:19Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24869v1",
  "full_text": "arXiv:2505.24869v1 [cs.CV] 30 May 2025SiLVR: A Simple Language-based Video Reasoning Framework Ce Zhang*Yan-Bo Lin*Ziyang Wang Mohit Bansal Gedas Bertasius Department of Computer Science, UNC Chapel Hill {cezhang, yblin, ziyangw, mbansal, gedas}@cs.unc.edu https://sites.google.com/cs.unc.edu/silvr Abstract Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based VideoReasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using mul- tisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, lan- guage descriptions are fed into a powerful rea- soning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive to- ken reduction scheme, which dynamically de- termines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video- MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Fur- thermore, our empirical study focused on video reasoning capabilities shows that despite not being explicitly trained on video, strong rea- soning LLMs can effectively aggregate multi- sensory input information from video, speech, and audio for complex temporal, causal, long- context, and knowledge acquisition reasoning tasks in video. Code is available at https: //github.com/CeeZh/SILVR. 1 Introduction Recent years have witnessed remarkable progress in general video understanding, with large mul- timodal models achieving strong performance on tasks such as video question-answering *Equal contribution.(VideoQA) (Team et al., 2023; Hurst et al., 2024; Bai et al., 2025; Zhang et al., 2024c), text-video retrieval (Zhao et al., 2023), and temporal lo- calization (Huang et al., 2024; Ren et al., 2024; Wang et al., 2024b). Despite the remarkable progress, most existing methods struggle with com- plex video-language understanding tasks that re- quire strong reasoning capabilities (e.g., tempo- ral, causal, long-context, external knowledge, etc.). Following the success of reasoning LLMs (Guo et al., 2025; Jaech et al., 2024), several recent mul- timodal large language models (MLLMs) proposed reasoning frameworks for multimodal image/video recognition tasks (Liu et al., 2025; Fei et al., 2024; Wang et al., 2024c; Wu et al., 2025; Feng et al., 2025; Chen et al., 2025; Li et al., 2025; Wang et al., 2025). However, these methods either rely on high-quality Chain-of-Thought (CoT) data, which is expensive and time-consuming to collect, or re- quire task-specific reward designs, leading to poor generalization. Moreover, such RL-based multi- modal reasoning approaches are difficult to opti- mize and often require a large amount of resources for training. Lastly, many recently proposed RL post-training techniques lead to similar or some- times even worse performance than standard Su- pervised Fine-tuning (SFT) approaches (Wang and Peng, 2025; Feng et al., 2025). Motivated by the impressive reasoning abilities of recent LLMs (Guo et al., 2025; Jaech et al., 2024), we propose SiLVR, a simple, modular, and training-free language-based framework for com- plex video-language reasoning tasks. SiLVR de- composes video understanding into two stages: •In the first stage, we convert raw videos into rich language-based descriptions. Specifically, we densely sample short clips from the input videos and use a pre-trained visual captioner (e.g., NVILA (Liu et al., 2024b)) to extract cap- tions for each clip. Additionally, we use auto- matic speech recognition (ASR) tools to convert … The question is asking which ingredient is not used in the artwork. The options are: shell (option A), glue (option B), blue food dye (option C), and oil (option D). Glue is definitely used (option B). Shells are also mentioned (option A). We also see a hand adding a drop of blue food dye to the bottle (option C). Oil is also used (option D). Soall four options are used? Wait, maybe I made a mistake. Maybe the shells are part of the decorative items but not an ingredient used in the artwork’s construction? The question is about ingredients used in the artwork. Therefore, the shells might be part of the decoration around the bottle in the video but not part of the actual artwork (the bottle’s contents). Sothe answer would be A: Shell.Question: Which of the following ingredients is not used in the artwork? Reasoning Trace Figure 1: Strong Reasoning Capabilities of SiLVR on Complex Video QA Tasks. SiLVR leverages recent advances in reasoning LLMs for complex video QA tasks. SiLVR achieves better performance than strong proprietary non-reasoning models (i.e., GPT-4o and Gemini-1.5) on benchmarks like Video-MME (Long), Video- MMMU (Comprehension), Video-MMLU, and EgoLife, which include temporal, causal, long-context, and external knowledge reasoning tasks. The example reasoning trace shows SiLVR’s capability to perform self-correction, in which it successfully identifies that shells are decorative rather than functional. speech into language descriptions. •In the second stage, we feed the rich language descriptions into a strong reasoning LLM (e.g. DeepSeek-R1 (Guo et al., 2025)) to solve com- plex video-language understanding tasks. To address the issue of processing a large num- ber of tokens in potentially hour-long videos, we propose a simple adaptive token reduction scheme, which dynamically determines the temporal granu- larity with which to sample the speech and video tokens. Such a token reduction scheme enables us to significantly reduce the number of input tokens to fit within the context length of an LLM, while maintaining strong reasoning performance. Compared to prior MLLM-based video reasoning frameworks, SiLVR is simple, mod- ular, training-free, and highly-performant. SiLVR achieves state-of-the-art results on multiple VideoQA benchmarks, including Video-MME (long), Video-MMMU (comprehension), Video- MMLU, CGBench, and EgoLife. Additionally, SiLVR demonstrates strong spatiotemporal grounding ability for video QA tasks that require localizing relevant video segments. On CGBench, a large-scale grounded VideoQA benchmark, SiLVR outperforms the previous best method by a substantial margin of 6.1% in mIoU. Additionally,our empirical study on video reasoning capabilities of our framework suggests that despite not being trained on videos, strong reasoning LLMs can successfully aggregate information from video, speech/audio for complex temporal, causal, long-context, and external knowledge reasoning tasks on video inputs. While SiLVR is not based on any new com- plex design choices, it is simple, modular, training- free, and highly performant and generalizes to multiple complex video-language understanding tasks. We believe the simple yet effective design of SiLVR will enable the research community to build on our work and use our simple framework as a baseline to develop even more powerful video- language reasoning models. 2 Related Work Language Reasoning Models. Recent work has significantly advanced the reasoning capabilities of LLMs through various strategies, such as Chain- of-Thought (Wei et al., 2022; Kojima et al., 2022), Self-Consistency (Wang et al., 2023), and Monte Carlo Tree Search based methods (Wan et al., 2024; Trinh et al., 2024; Xin et al., 2024). Recently, works such as DeepSeek-R1 (Guo et al., 2025) demonstrated that applying large-scale RL with ac- Adaptive Token Reduction00:03 -> 00:10 in the future humanity may leave the … 00:07 -> 00:17 earth behind and search for a new home … 00:10 -> 00:20 in space but, where our solar system is a …The question asks the order in which planets are introduced. …The subtitles mention \"Mercury …\" around 00:03:23 …After Jupiter, Saturn is mentioned with its rings and storms. Following Saturn, Neptune is discussed…Looking at the options, option A is Venus, Jupiter, Neptune. Option C is Venus, Neptune, Jupiter, which would be incorrect. Sothe correct answer should be A. Speech A large spacecraft with a dark, angular design and …. … … A massive volcanic eruption with thick, dark smoke and ash billowing …… …Short Clip CaptionerShort Clip CaptionerShort Clip Captioner A mysterious, glowing object suspended in the air against a backdrop of intense red and …… Question: what is the order of planet appearances in the video? A. Venus, Jupiter, Neptune. B. Mercury, Jupiter, Mars. C. Venus, Neptune, Jupiter. D. Jupiter, Mercury, Neptune. … Reasoning LLM Visual Reasoning Speech Reasoning Figure 2: Method Overview. SiLVR is a simple two-stage language-based video reasoning framework. Top: The video is segmented into short clips and paired with speech. A clip captioner processes each segment to generate visual descriptions. The speech is transcribed using ASR. Bottom: A reasoning LLM takes the question, transcribed speech, and dense visual descriptions compressed by Adaptive Token Reduction to perform complex video reasoning. In the shown example, SiLVR infers the correct order by integrating information across both visual and speech modalities. The model correctly identifies the sequence through reasoning and eliminating incorrect options. curacy and format rewards can induce emerging reasoning capabilities in LLMs. These RL-based methods have shown strong improvements in tasks such as mathematics (Zheng et al., 2021; Azer- bayev et al., 2023) and code generation (Austin et al., 2021; Hendrycks et al., 2021). Motivated by these successes, we propose to take advantage of the strong reasoning ability of LLMs for complex video-language reasoning problems. Multimodal Reasoning Models. There have been many efforts to augment MLLMs with reasoning capabilities. One line of work focuses on decom- posing the reasoning process into multiple sub- problems (Zhang et al., 2023c; Xu et al., 2024; Zhang et al., 2024b). Motivated by the success of DeepSeek-R1 (Guo et al., 2025), another line of work explores RL to elicit the reasoning ability of the MLLMs (Huang et al., 2025; Shen et al., 2025; Yang et al., 2025b; Zhang et al., 2025; Ouyang, 2025; Peng et al., 2025). In the video domain, VideoCoT (Wang et al., 2024c) and Video- of-Thought (Fei et al., 2024) propose to prompt the MLLMs with multiple reasoning steps before answering the question. In addition, multiple con- current works propose to use GRPO to enhance video reasoning (Wu et al., 2025; Feng et al., 2025;Chen et al., 2025; Li et al., 2025; Wang et al., 2025). However, many of these RL-based meth- ods demand substantial training computation, and achieve only marginal improvements or perform even worse than the SFT methods (Wang and Peng, 2025; Feng et al., 2025). Unlike these methods, SiLVR is simple, training-free, yet highly perfor- mant across a wide range of VideoQA benchmarks. Complex Video-Language Understanding. A va- riety of benchmarks for complex video-language understanding have been proposed, with a focus on comprehensive evaluation of videos with dif- ferent durations and questions spanning diverse categories (Fu et al., 2024; Li et al., 2024b; Liu et al., 2024a; Rawal et al., 2024). In parallel, sev- eral benchmarks have been introduced to assess the reasoning capabilities of large video-language mod- els (Hu et al., 2025; Song et al., 2025; Zhao et al., 2025; He et al., 2024). On the modeling side, recent video MLLMs adapt image MLLMs by fine-tuning additional modules for temporal modeling (Lin et al., 2023; Li et al., 2023, 2024a; Zhang et al., 2023b). Several follow-up works explored spa- tiotemporal token compression (Islam et al., 2025; Liu et al., 2024b; Bai et al., 2025; Shen et al., 2024) or building hierarchical memory (Song et al., 2023; Jin et al., 2024; Islam et al., 2024) for complex and long video understanding. Another line of work explores training-free frameworks that first convert raw videos into dense visual captions, then perform reasoning with off-the-shelf LLMs (Zhang et al., 2023a; Wang et al., 2024a; Fan et al., 2024; Ma et al., 2024; Liao et al., 2024; Wang et al., 2024d). Our method adopts a similar design. However, SiLVR focuses on solving complex video-language reasoning problems with strong reasoning LLMs and effectively integrates both visual and audio modality streams. 3 Method Our method decomposes video-language QA into two stages: 1) extracting visual captions and transcribing speech into text, and 2) performing language-based reasoning over the extracted tex- tual descriptions. Such a decomposed video reason- ing design offers several benefits: 1) Simplicity: SiLVR does not require complex RL-based opti- mization or specialized modules for different tasks. 2)Generalizability: our method can be applied to a wide range of complex video-language tasks without task-specific fine-tuning. 3) Modularity: our method’s modular design enables seamless use of powerful visual captioning models and strong reasoning LLMs. 4) Flexibility: SiLVR supports plug-and-play integration of different captioning models, speech recognition models, and LLMs. An overview of our method is illustrated in Figure 2. 3.1 Extracting Multimodal Descriptions Given a video V, we first divide it into Nnon- overlapping short clips v={vi}N i=1, where each clipvi∈RT×H×W×3contains Tframes of height Hand width W. Each clip is passed through a pretrained visual captioning model Mto produce a caption ci. The sequence of captions is denoted asC={ci}N i=1, forming a temporally ordered description of the visual content. In parallel, we apply an ASR model Wto con- vert the speech into a sequence of textual descrip- tions S={sj}K j=1, where sjis a timestamped transcription of a spoken segment. Kdenotes the total number of segments, which is determined by an ASR model. We then concatenate SandCone after the other to form a rich, language-based de- scription of the video (including audio/speech) and feed them into a reasoning LLM as described next.Algorithm 1 Adaptive Token Reduction Require: Video V, Question Q, LLM F, Cap- tioner M, Speech Recognition Model W, Ini- tial Clip Length L 1:S←extractSubtitles (V,W) 2:limit←getContextLength (F) 3:while True do 4: v←divideVideo (V,L) 5: C←generateCaptions (v,M) 6: Z←concat (S,C) 7: ifcountTokens (Z)> limit then 8: L←L×2 9: else 10: break 11: end if 12:end while 13:return answer (Z,Q,F) 3.2 Language-Based Reasoning To answer a question Qabout the video, we feed the video captions Cand speech transcriptions S along with Qinto a reasoning LLM. We design several prompts to guide the LLM to reason jointly over visual and speech information (for complete prompts see Supplementary Material). Unlike prior video reasoning approaches, SiLVR performs rea- soning entirely in the language space. However, the limited context window of LLMs poses a signifi- cant challenge when processing long videos with rich multimodal content. To address this issue, we introduce a simple adaptive token reduction scheme (see Algorithm 1). Our adaptive token reduction scheme dynamically adjusts the temporal granular- ity for sampling video tokens. Specifically, it starts with a small clip length and progressively increases it to reduce the total number of generated tokens. This allows us to effectively fit the input tokens within the LLM’s context window for videos of varying durations while maintaining strong video reasoning performance. 3.3 Implementation Details We use NVILA (Liu et al., 2024b) as the default visual captioning model. We use our adaptive to- ken reduction scheme for all videos as described above. For speech transcription, we use Whisper- large-v3 (Radford et al., 2022). Due to its strong reasoning performance, we use DeepSeek-R1 (Guo et al., 2025) as the default LLM and set the temper- ature to 1.0 for all experiments. We use the official evaluation code provided by each benchmark, or ModelVideo Reasoning Benchmarks General Video Benchmarks Video-MMMU Video-MMLU MMVU MMWorld Video-MME CGBench EgoLife CinePile Proprietary Models Gemini 1.5 Flash 49.0 47.8 58.8 - 68.8 33.5 - 58.8 Gemini 1.5 Pro 53.5 - 65.4 51.0 77.4 37.8 36.9 60.1 Gemini 2.0 Flash - - 65.9 - - - - - GPT-4o 62.0 44.9 67.4 62.5 72.1 44.9 36.2 56.1 Claude 3.5 Sonnet 75.7 71.3 65.2 54.5 - 40.3 - - Kimi-k1.6 76.7 - - - - - - - OpenAI o1 - - 75.5 - - - - - Open-Source LMMs LLaV A-OV-7B 31.0 33.4 37.9 - - 30.9 30.8 49.3 VideoLLaMA3-7B 46.0 - 45.0 - 61.0 - - - Aria 53.0 - 49.3 - 66.3 - - - Qwen-2-VL-72B - - 50.3 - 74.3 45.3 - - DeepSeek-VL2 - - 52.1 - - - - - SILVR (ours) 82.7 83.1 68.2 59.9 77.7 51.8 42.0 59.4 Table 1: Main Results. We evaluate our method on a set of video reasoning benchmarks (Video-MMMU, Video- MMLU, MMVU, MMWorld) and general video benchmarks (Video-MME, CGBench, EgoLife, CinePile). We use the comprehension split of Video-MMMU and the long split of VideoMME (with subtitles). Based on these results, we observe that SiLVR achieves the best-reported results on Video-MMMU (comprehension), Video-MMLU, Video-MME (long split, with subtitles), CGBench, and EgoLife, outperforming strong proprietary models such as Gemini 2.0 and GPT-4o. We bold andunderline the best and the second best models in each benchmark respectively. use LMMs-Eval (Zhang et al., 2024a) when the official code is unavailable. Additional implemen- tation details are provided in the Supplementary. 4 Experiments 4.1 Benchmarks and Evaluation Metrics We conduct experiments on eight complex video- language understanding benchmarks: Video- MMMU (Hu et al., 2025), Video-MMLU (Song et al., 2025), MMVU (Zhao et al., 2025), MM- World (He et al., 2024), Video-MME (Fu et al., 2024), CGBench (Chen et al., 2024), Ego- Life (Yang et al., 2025a) and CinePile (Rawal et al., 2024). Following Video-R1 (Feng et al., 2025), we group these benchmarks into two categories: Reasoning Benchmarks and General Benchmarks. The reasoning benchmarks include Video-MMMU, Video-MMLU, MMVU, and MMWorld, which primarily evaluate the reasoning capabilities of large video-language models. Specifically, Video- MMMU and Video-MMLU focus on lecture-based video understanding, where the model must extract knowledge from lecture videos to answer the ques- tions. MMVU requires models to apply domain- specific knowledge and perform expert-level rea- soning to analyze specialized-domain videos. MM- World focuses on a diverse set of reasoning ques- tions (e.g., counterfactual thinking, future predic- tion, etc.) across videos from seven broad disci-plines. The other four benchmarks (i.e., Video- MME, CGBench, EgoLife, and CinePile) are gen- eral video-language benchmarks, which contain various types of questions and offer a compre- hensive assessment of the video-language mod- els. Specifically, Video-MME includes three splits (short, medium, and long) based on the duration of the video. CGBench and EgoLife are designed for long video understanding, with an average video duration of more than an hour. We focus on VideoQA for all benchmarks and report QA accuracy as our primary evaluation metric. Addi- tionally, we conduct Grounded VideoQA experi- ments on CGBench to assess the model’s temporal grounding ability and use the mean Intersection over Union (mIoU) to evaluate the results. 4.2 Main Results Video Reasoning Benchmarks. We present our results on video reasoning benchmarks on the left side of Table 1. Our results indicate that SiLVR achieves the best performance on two rea- soning benchmarks: Video-MMMU (comprehen- sion) and Video-MMLU. Specifically, on Video- MMMU, SiLVR outperforms the prior best method, Kimi-k1.6 (Team et al., 2025) by a large margin (+6.0% ). It also significantly outperforms other strong proprietary models such as Gemini 1.5 Pro and GPT-4o by 29.2% and22.7%, respectively. On Video-MMLU, SiLVR outperforms the previ- ModelVideo Reasoning Benchmarks General Video Benchmarks VideoMMMU VideoMMLU MMVU MMWorld VideoMME CGBench EgoLife CinePile SiLVR (Llama 4) 56.3 57.2 60.6 57.2 67.8 53.2 38.5 45.6 SiLVR (DeepSeek-R1) 82.7 83.1 68.2 59.9 77.7 59.4 42.0 51.8 Average Gain: +15.7 Average Gain: +6.5 Table 2: Performance Comparison Between a Reasoning (DeepSeek-R1) and a Non-Reasoning (Llama 4) LLM. Using DeepSeek-R1 reasoning LLM leads to significantly better results over a non-reasoning Llama 4 on all eight benchmarks. However, we also observe that the average gain on video reasoning benchmarks (VideoMMMU, VideoMMLU, MMVU, MMWorld) is significantly larger than on general video benchmarks (VideoMME, CGBench, EgoLife, CinePile). These results demonstrate that the strong reasoning ability of DeepSeek-R1 is crucial for solving complex video reasoning tasks. ous state-of-the-art model Claude 3.5 Sonnet by a substantial 11.8%. Additionally, on MMVU, we observe that our modular framework, with DeepSeek-R1 as the LLM, outperforms the uni- fied multimodal model DeepSeek-VL2 by a sig- nificant margin of 15.9%. These results suggest that despite the simplicity of our approach, it de- livers strong performance across a wide range of video-language reasoning tasks. General Video Benchmarks. We present our re- sults on general video benchmarks on the right side of Table 1. Based on these results, we observe that SiLVR achieves state-of-the-art performance on three general benchmarks: Video-MME (long split, with subtitles), CGBench, and EgoLife. Specifi- cally, on Video-MME and EgoLife, SiLVR outper- forms the prior best performing method Gemini 1.5 Pro by 0.3% and5.1%, respectively. On CGBench, SiLVR achieves 51.8% accuracy, outperforming the previous state-of-the-art method Qwen-2-VL- 72B by a significant 6.9% margin. SiLVR also sur- passes strong proprietary models, outperforming GPT-4o by 6.9% and Claude 3.5 Sonnet by 11.5% on CGBench. Additionally, it is worth noting that Video-MME (long), EgoLife, and CGBench are designed for very long-form video understanding, with average video durations exceeding 60 minutes. Our strong results demonstrate that SiLVR is highly effective in comprehending long videos. 4.3 Reasoning Analysis In this section, we conduct a more in-depth anal- ysis of the video reasoning capabilities of our ap- proach. To do this, we systematically compare the performance of our framework when using reason- ing (e.g., DeepSeek-R1) vs. non-reasoning (e.g., LLama 4) LLMs across multiple benchmarks. Ad- ditionally, we break down the performance of our approach across different types of video reason-ing questions (e.g., temporal, causal, long-context, knowledge acquisition, etc.). Reasoning vs Non-Reasoning LLMs. To study the impact of a strong reasoning LLM within our framework, we compare the performance of our method when using a reasoning LLM (DeepSeek- R1) vs. a non-reasoning LLM (Llama 4). The results are presented in Table 2. Our results sug- gest several interesting trends. First, we observe that DeepSeek-R1 consistently outperforms Llama 4 across all benchmarks, indicating that it is a much stronger LLM than LLama 4. Second, we note that using DeepSeek-R1 leads to much larger perfor- mance gains on the reasoning benchmarks, where DeepSeek-R1 surpasses Llama 4 by a substantial 26.4% on Video-MMMU and 25.9% on Video- MMLU with an average improvement of +15.7% on all video reasoning benchmarks. In contrast, while DeepSeek-R1 also produces better results on general video benchmarks, the improvements over Llama 4 are much smaller (i.e., average im- provement of 6.5% on general video benchmarks vs.15.7% on the reasoning benchmarks). These results suggest that the strong reasoning ability of DeepSeek-R1 is critical for solving complex video reasoning tasks and that our framework’s simple and modular design allows us to take full advan- tage of DeepSeek-R1’s strong reasoning abilities on these complex video reasoning problems. Performance Breakdown Across Different Tasks. In Figure 3, we report the performance gains of using a reasoning LLM (DeepSeek-R1) over a non-reasoning LLM (Llama 4) for different question categories on VideoMME, which contains 12 manually annotated categories. The four cate- gories that we report on the left of Figure 3, belong to reasoning (e.g., temporal, spatial, object, and action reasoning). The other eight categories are classified as non-reasoning and require general per- TRSRAROR TPSPAPACOCOPCP IS Question Category in VideoMME05101520Performance Gain (%)Average Gain: 11.1% Average Gain: 4.9%Performance Gains of DeepSeek-R1 Compared to Llama-4 Reasoning Questions Non-Reasoning QuestionsFigure 3: Performance Breakdown Across Different Question Categories. Full question category names can be found in the supplementary materials (Table 10). Compared to using a non-reasoning LLM (i.e., LLama 4), using DeepSeek-R1 achieves significantly larger im- provements on Video-MME questions from the reason- ing category (a gain of +11.1% ) than the general per- ception questions (a gain of +4.9% ). ModelVideoMMMU CGBench (∆knowledge ) (mIoU) Qwen-2.5-VL-72B 9.7 - Gemini-1.5 Pro 8.7 3.85 Claude-3.5 Sonnet 11.4 4.17 GPT-4o 15.6 5.73 VideoMind-7B - 7.10 SiLVR (ours) 17.2 11.84 Table 3: Results on Knowledge Acquisition and Tem- porally Grounded QA Tasks. SiLVR achieves the highest ∆knowledge on VideoMMMU and the best mIoU on CGBench, demonstrating its superior knowledge ac- quisition and temporal grounding abilities. ception capabilities (e.g., action recognition, OCR, etc.). Based on the results in Figure 3, we observe that compared to LLama 4, using DeepSeek-R1 achieves a significantly larger improvement on rea- soning questions (a gain of +11.1% ) compared to non-reasoning questions (a gain of +4.9% ). This result is consistent with our observations in Table 2, which confirms that reasoning LLMs bring greater benefits for tasks that require complex reasoning. 4.4 Results on Other Tasks Knowledge Acquisition from Videos. We also evaluate our method on the novel knowledge ac- quisition task on Video-MMMU (Hu et al., 2025). The task requires models to answer questions both before and after watching a reference lecture video, with the goal of measuring how much knowledge the model gains from the video. The metric for theDropping RateAccuracy Subtitles Captions 50% - 65.3 75% - 56.0 - 50% 68.9 - 75% 67.7 No Compression 70.3 Adaptive Token Reduction 76.7 Table 4: Token Reduction Analysis. Accuracy on VideoMME (overall) when selectively dropping speech vs. visual caption tokens. Based on these results, we observe that speech tokens are more informative than visual caption tokens. Furthermore, our adaptive token reduction strategy outperforms all static baselines. knowledge acquisition task is defined as: ∆knowledge =Acc post−Acc pre 100%−Acc pre×100% (1) where Acc preandAcc postdenote the accuracy be- fore and after watching the video, respectively. Our results in Table 3 show that SiLVR achieves 17.2% in∆knowledge, outperforming the prior best method GPT-4o by 1.6%. SiLVR also outperforms strong proprietary models such as Gemini-1.5 Pro and Claude-3.5 Sonnet by 8.5% and5.8%, respec- tively. These results demonstrate that SiLVR is not only effective in complex video reasoning, but also has strong knowledge acquisition capabilities. Temporally Grounded QA. In Table 3, we also present our results on the temporally grounded QA task on CGBench (Chen et al., 2024). The task requires the model to temporally localize relevant video segments needed to answer the question (usu- ally less than 10 seconds) in long videos that span over 60 minutes. Our results in Table 3 show that SiLVR achieves the highest performance in mIoU, outperforming concurrent work VideoMind (Liu et al., 2025) by a notable 4.74%. In addition, SiLVR also outperforms GPT-4o and Claude-3.5 Sonnet by 6.11% and7.67%, respectively. These results suggest that SiLVR can correctly answer complex questions and temporally ground the an- swer to relevant segments in the video, which im- proves interpretability in video reasoning. 4.5 Ablation Studies on VideoMME Speech vs. Visual Caption Token Importance. To evaluate the relative contribution of visual and audio information, we vary the fraction of tokens from speech transcripts and video captions and re- port the QA performance on VideoMME. As shown Clip Length (s) ATR 1 2 4 8 64 Accuracy 76.7 59.9 61.3 68.5 74.2 70.3 Table 5: Performance of Adaptive Token Reduc- tion (ATR) vs. Fixed Clip Length Baselines. ATR achieves the highest overall accuracy, outperforming the best fixed-length baseline (8s) by 2.5%. These results suggest that ATR effectively reduces redundant tokens while preserving strong performance. Captioner Overall Short Medium Long LLaV A-OV 7B 67.2 57.7 68.1 75.9 NVILA 7B 70.3 63.2 70.4 77.3 Qwen-2.5-VL 7B 70.9 63.8 72.9 76.1 Qwen-2.5-VL 72B 71.2 65.0 72.2 76.4 Table 6: Performance with Different Visual Caption- ers.Qwen-2.5-VL 72B achieves the best overall accu- racy. We use NVILA 7B for all experiments because it provides the best accuracy-cost trade-off. in Table 4, the reduction of 50-75% speech to- kens (while keeping all visual caption tokens) leads to a significant decrease in performance ( 11.4% - 20.7% ). In comparison, dropping the same frac- tion of visual caption tokens (while keeping all speech tokens) results in a much smaller perfor- mance drop ( 7.8% -9.0% ). The No Compression baseline retains all tokens, achieving an overall ac- curacy of 70.3%. Our adaptive token reduction scheme, which prioritizes speech tokens and selec- tively reduces visual caption tokens, achieves an accuracy of 76.4%, outperforming other baselines. Analysis of Adaptive Token Reduction. In Ta- ble 5, we compare Adaptive Token Reduction (ATR) with several static baselines that use fixed video clip lengths. Among all baselines, the vari- ant that uses an 8-second clip length achieves the highest accuracy of 74.2%. We note that a shorter clip variant (e.g., 1s) generates a large number of captions for long videos, which often exceeds the context window of the LLMs, thus leading to de- graded performance. In contrast, a longer clips variant (e.g., 64s) reduces the number of captions at the cost of sacrificing the granularity of visual information, which also leads to lower accuracy. Compared to these static baselines, our proposed ATR consistently outperforms all fixed clip length baselines, surpassing the best-performing variant (8s) by a significant margin of 2.5%. These results demonstrate that ATR effectively reduces redun- dant tokens by adaptively adjusting the clip length, offering flexibility and strong performance. Visual Captioning Model. Next, we study theLLM Overall Short Medium Long Llama-4-Scout 17B 63.0 56.7 64.4 67.8 Llama-4-Maverick 17B 66.2 57.2 68.3 73.0 DeepSeek V3 66.8 56.0 69.1 75.3 GPT-4o 67.3 57.0 69.2 75.8 GPT-4.1 69.5 59.6 71.1 77.9 DeepSeek R1 70.3 63.2 70.4 77.3 Table 7: Performance of Our Framework with Dif- ferent LLMs. Llama-4 Maverick achieves 66.2% over- all accuracy, providing an effective trade-off between model sizes and performance. DeepSeek R1 achieves the highest overall accuracy, outperforming DeepSeek V3 and GPT-4.1 by 3.5% and0.8%, respectively. effect of different visual captioners. As shown in Table 6, Qwen-2.5-VL 72B achieves the highest overall accuracy, most likely due to the larger LLM (72B), which leads to higher-quality captions. We also observe that NVILA 7B and Qwen-2.5-VL 7B achieve similar performance, outperforming LLaV A-OV 7B by 2.9% and3.7%, respectively. Since NVILA 7B is faster than Qwen-2.5-VL 7B and achieves similar performance, we use NVILA 7B for all experiments. We do not use Qwen-2.5- VL 72B due to the prohibitive computational cost. Different LLMs. Lastly, in Table 7 we study the effect of different LLMs, including Llama- 4-Scout 17B, Llama-4-Maverick 17B, GPT-4o, GPT-4.1, DeepSeek V3, and DeepSeek R1. Our results indicate that DeepSeek-R1, as the LLM backbone, achieves the best overall accuracy on Video-MME, outperforming the proprietary GPT- 4.1 by 0.8%. Additionally, we note that Llama-4- Maverick achieves 66.2% accuracy with only 17B parameters, offering an effective trade-off between model size and performance. Lastly, we observe that DeepSeek R1 outperforms DeepSeek V3 by a significant 3.5%, highlighting the effectiveness of using a reasoning LLM within our framework. 5 Conclusion We present SiLVR, a simple, modular, and training- free language-based video reasoning framework. SiLVR achieves state-of-the-art performance on Video-MME (long), Video-MMMU (comprehen- sion), Video-MMLU, CGBench, and EgoLife. SiLVR also achieves strong results in video-based knowledge acquisition and temporally grounded QA tasks, demonstrating strong generalization. Lastly, we systematically analyze the reasoning capabilities of SiLVR and perform ablations on sev- eral key design choices. We encourage the research community to build on our simple yet effective video reasoning framework and hope that it will inspire new ideas in video reasoning research. Limitations As with most modular frameworks, the perfor- mance of our method depends on its individual modules. On the visual perception side, our method relies on the visual captioning model, which may produce hallucinations or descriptions that lack fine-grained visual details. However, since our framework is agnostic to the specific use of visual captioning models, we believe that future advances in visual captioning models will mitigate this is- sue. On the reasoning side, our framework may underperform when the reasoning trace generated by the LLM is incorrect. However, we view this as a broader limitation of current LLMs, and antici- pate that future advances in long-context modeling and reasoning for LLMs will further enhance the performance of our framework. Acknowledgements This work was supported by Laboratory for Ana- lytic Sciences via NC State University, ONR Award N00014-23-1-2356, NIH Award R01HD11107402, and Sony Focused Research award. References Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. 2023. Proofnet: Autoformalizing and formally proving undergraduate-level mathemat- ics.Preprint, arXiv:2302.12433. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen- bin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Keshigeyan Chandrasegaran, Agrim Gupta, Lea M Hadzic, Taran Kota, Jimming He, Cristóbal Eyza- guirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. 2024. Hourvideo: 1-hour video-language understanding. Advances inNeural Information Processing Systems, 37:53168–53197. Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang.2024. Cg-bench: Clue-grounded question answer- ing benchmark for long video understanding. arXiv preprint arXiv:2412.12075. Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, and Xihui Liu. 2025. Exploring the ef- fect of reinforcement learning on video understand- ing: Insights from seed-bench-r1. arXiv preprint arXiv:2503.24376. Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. 2024. Videoagent: A memory-augmented multimodal agent for video un- derstanding. In European Conference onComputer Vision, pages 75–92. Springer. Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. 2024. Video-of-thought: Step-by-step video rea- soning from perception to cognition. arXiv preprint arXiv:2501.03230. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025. Video-r1: Reinforcing video rea- soning in mllms. arXiv preprint arXiv:2503.21776. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, and 1 oth- ers. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi- rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wan- rong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, and 1 others. 2024. Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos. arXiv preprint arXiv:2406.08407. Dan Hendrycks, Steven Basart, Saurav Kadavath, Man- tas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge com- petence with apps. NeurIPS. Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuan- han Zhang, Xiang Yue, Bo Li, and Ziwei Liu. 2025. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826. Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. 2024. Vtimellm: Empower llm to grasp video moments. In Proceedings ofthe IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 14271–14280. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capa- bility in multimodal large language models. arXiv preprint arXiv:2503.06749. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Berta- sius. 2024. Video recap: Recursive caption- ing of hour-long videos. In Proceedings ofthe IEEE/CVF Conference onComputer Vision and Pattern Recognition. Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, and Lorenzo Torresani. 2025. Bimba: Selective-scan compression for long- range video question answering. arXiv preprint arXiv:2503.09590. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard- son, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. 2024. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of theIEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 13700–13710. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. Advances inneural information processing systems, 35:22199– 22213. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and 1 others. 2024a. Llava- onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen- hai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. Videochat: Chat-centric video under- standing. arXiv preprint arXiv:2305.06355. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, and 1 others. 2024b. Mvbench: A com- prehensive multi-modal video understanding bench- mark. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition, pages 22195–22206. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, andLimin Wang. 2025. Videochat-r1: Enhancing spatio- temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958. Ruotong Liao, Max Erler, Huiyu Wang, Guangyao Zhai, Gengyuan Zhang, Yunpu Ma, and V olker Tresp. 2024. Videoinsta: Zero-shot long video understand- ing via informative spatial-temporal reasoning with llms. arXiv preprint arXiv:2409.20365. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava: Learn- ing united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. 2025. Videomind: A chain-of- lora agent for long video reasoning. arXiv preprint arXiv:2503.13444. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. 2024a. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, and 1 others. 2024b. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468. Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and Jianfei Cai. 2024. Drvideo: Document retrieval based long video under- standing. arXiv preprint arXiv:2406.12846. Kun Ouyang. 2025. Spatial-r1: Enhancing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805. Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Wei- jie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyi- dan Xie, Li Ge, and 1 others. 2025. Skywork r1v: pio- neering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock- man, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak su- pervision. Preprint, arXiv:2212.04356. Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. 2024. Cinepile: A long video ques- tion answering dataset and benchmark. Preprint, arXiv:2405.08813. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. 2024. Timechat: A time- sensitive multimodal large language model for long video understanding. In Proceedings ofthe IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 14313–14323. Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. 2025. Vamba: Un- derstanding hour-long videos with hybrid mamba- transformers. arXiv preprint arXiv:2503.11579. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, and 1 oth- ers. 2025. Vlm-r1: A stable and generalizable r1- style large vision-language model. arXiv preprint arXiv:2504.07615. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bor- des, and 1 others. 2024. Longvu: Spatiotemporal adaptive compression for long video-language under- standing. arXiv preprint arXiv:2410.17434. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, and 1 others. 2023. Moviechat: From dense token to sparse mem- ory for long video understanding. arXiv preprint arXiv:2307.16449. Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yux- uan Liu, and Gaoang Wang. 2025. Video-mmlu: A massive multi-discipline lecture understanding bench- mark. arXiv preprint arXiv:2504.14693. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean- Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Mil- lican, and 1 others. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, and 75 others. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. 2024. Solving olympiad ge- ometry without human demonstrations. Nature, 625(7995):476–482. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2024. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference onMachine Learning. Xiaodong Wang and Peixi Peng. 2025. Open-r1-video. https://github.com/Wang-Xiaodong1899/ Open-R1-Video. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. 2024a. Videoagent: Long-form video understanding with large language model as agent. InEuropean Conference onComputer Vision, pages 58–76. Springer.Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, and David Crandall. 2024b. Timerefine: Temporal grounding with time refining video llm. arXiv preprint arXiv:2412.09601. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In ICLR. Yan Wang, Yawen Zeng, Jingsheng Zheng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. 2024c. Videocot: A video chain-of-thought dataset with active annotation tool. arXiv preprint arXiv:2407.05355. Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Zi- heng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. 2025. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jae- hong Yoon, Feng Cheng, Gedas Bertasius, and Mo- hit Bansal. 2024d. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. arXiv preprint arXiv:2405.19209. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elic- its reasoning in large language models. Advances inneural information processing systems, 35:24824– 24837. Peiran Wu, Yunze Liu, Miao Liu, and Junxiao Shen. 2025. St-think: How multimodal large language models reason about 4d worlds from ego-centric videos. arXiv preprint arXiv:2503.12542. Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, and 1 others. 2024. Deepseek- prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. 2024. Llava-cot: Let vision language models reason step-by-step. Preprint, arXiv:2411.10440. Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, and 1 others. 2025a. Egolife: Towards egocentric life assistant. arXiv preprint arXiv:2503.03803. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, and 1 others. 2025b. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615. Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Berta- sius. 2023a. A simple llm framework for long- range video question-answering. arXiv preprint arXiv:2312.17235. Hang Zhang, Xin Li, and Lidong Bing. 2023b. Video- llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuan- han Zhang, Jingkang Yang, Chunyuan Li, and Zi- wei Liu. 2024a. Lmms-eval: Reality check on the evaluation of large multimodal models. Preprint, arXiv:2407.12772. Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruom- ing Pang, and Yiming Yang. 2024b. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024c. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023c. Multi- modal chain-of-thought reasoning in language mod- els.arXiv preprint arXiv:2302.00923. Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, and 1 others. 2025. Mmvu: Measuring expert-level multi- discipline video understanding. arXiv preprint arXiv:2501.12380. Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Ro- hit Girdhar. 2023. Learning video representations from large language models. In Proceedings of theIEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 6586–6597. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2021. Minif2f: a cross-system benchmark for for- mal olympiad-level mathematics. arXiv preprint arXiv:2109.00110. Our appendix consists of Additional Ablation Study (Section A), More Experimental Results (Section B), Additional Implementation Details (Section C), and Qualitative Results (Section D). A Additional Ablation Study Time-aware Caption Representation. We inves- tigate the effectiveness of integrating time infor- mation into captions on the Video-MMMU bench- mark. The results are presented in Table 8. The baseline concatenates all captions in temporal order without explicit timestamps, whereas our proposed time-aware method explicitly includes time infor- mation. Specifically, before each caption, we add a timestamp indicating the interval from which it was extracted (e.g. 00:00:24 –> 00:00:32: A clear glass bottle being filled with water, surrounded by seashells and a white cloth.) As shown in Ta- ble 8, incorporating timestamp information leads to a notable 2.25% improvement in overall accuracy compared to the baseline. Furthermore, the time- aware method consistently outperforms the base- line across all question categories. These results demonstrate that explicitly providing time informa- tion effectively enhances the temporal perception and reasoning capabilities of the LLMs. Conse- quently, we adopt the time-aware caption represen- tation for all experiments. Method Overall Perception Comprehension Adaptation w/o time 72.86 81.00 80.67 56.90 w/ time 75.11 82.67 82.67 60.00 Table 8: Time-aware Caption Representation. Incor- porating time information into the captions by adding timestamps depicting time intervals from which the captions were extracted significantly boosts the perfor- mance on Video-MMMU. B More Experimental Results HourVideo. We evaluate our method on the devel- opment set of HourVideo (Chandrasegaran et al., 2024), a benchmark specifically designed for 3D reasoning over long videos. As shown in Table 9, our method performs surprisingly well, despite not incorporating any explicit 3D modeling. Specifi- cally, it achieves an accuracy of 36.3%, outperform- ing the concurrent method V AMBA (Ren et al., 2025) by a notable margin of 2.7%. These results demonstrate that our method is effective in reason- ing about both the 3D physical world and hour-longvideos. Method Overall Accuracy Aria 39.2 Gemini 1.5 Pro 37.4 Qwen2-VL 7B 33.8 V AMBA 33.6 SiLVR (ours) 36.3 Table 9: Performance of Our Method on HourVideo. SiLVR outperforms the concurrent work V AMBA by a significant 2.7%. Detailed VideoMME Results. In Table 10, we present a detailed breakdown of our method’s per- formance on VideoMME. From these results, we observe that our method achieves the lowest ac- curacy on the Counting Problem. This is likely due to the complexity of counting tasks, which require precise temporal localization of multiple events and subsequent reasoning. Any missed or incorrectly detected events could lead to incorrect answers, making the Counting Problem particularly challenging. Question Category Accuracy Temporal Reasoning (TR) 74.6 Spatial Reasoning (SR) 94.6 Action Reasoning (AR) 76.1 Object Reasoning (OR) 79.5 Temporal Perception (TP) 85.5 Spatial Perception (SP) 74.1 Attribute Perception (AP) 80.2 Action Recognition (AC) 68.1 Object Recognition (OC) 82.5 OCR Problems (OP) 83.5 Counting Problem (CP) 50.7 Information Synopsis (IS) 88.5 Table 10: Detailed Results on VideoMME. Our method achieves the highest accuracy on Spatial Rea- soning while achieving the lowest performance on the challenging Counting Problem. C Additional Implementation Details C.1 Captioner For all LLaV A and Qwen models, we use the prompt \"Briefly describe the video within 40 words\" to generate captions for each clip. We set max_new_tokens to 200 and em- ploy greedy decoding. We utilize the fol- lowing model variants from Hugging Face: lmms-lab/llava-onevision-qwen2-7b-ov for LLaV A-OV 7B, Qwen/Qwen2.5-VL-7B-Instruct for Qwen2.5-VL 7B, and Qwen/Qwen2.5-VL- 72B-Instruct for Qwen2.5-VL 72B. For the NVILA model, we use the NVILA-8B-Video vari- ant with the prompt “generate caption” to produce captions for each clip. We set max_new_tokens to 128 and employ greedy decoding similar to LLaV A and Qwen models. We use 4 H100 GPUs for generating captions. C.2 LLM We use the default temperature of 1.0 for all LLM experiments. We use the DeepSeek API to run DeepSeek-R1 and DeepSeek-V3 efficiently. To accelerate inference, we implement a parallel pro- cessing pipeline with up to 64 concurrent processes, each handling raw captions and subtitles before sending requests to the API. During off-peak hours, this setup allows us to evaluate our method on the complete VideoMME benchmark in under 2 hours at a cost of less than $20. For GPT models, we use the OpenAI API. For Llama-4 models, we use 4×H100 GPUs for local inference. However, we can only process a subset of videos locally due to GPU memory constraints. For long videos, we use Lambda Cloud’s API service. C.3 Prompt Design Different VideoQA benchmarks include different types of questions (e.g., multiple-choice, open- ended, or mixed). Additionally, the Grounded VideoQA task in CGBench requires models to pre- dict the temporal boundaries (start/end timestamps) of video segments relevant to each question. To accommodate these differences, we design task- specific prompts. Specifically, for multiple-choice questions, we use the following prompt template: The video’s subtitles are listed below. Subtitles. The video’s captions are listed below. Each caption describes a Clip Length seconds clip. Captions. Select the best answer to the following multiple- choice question based on the video and the sub- titles. Respond with only the letter (A, B, C, D, E, etc.) of the correct option. Question: Question. Options: Options. The answer is: For open-ended questions, we use the followingprompt template: The video’s subtitles are listed below. Subtitles. The video’s captions are listed below. Each caption describes a Clip Length seconds clip. Captions. Based on the video and the subtitles. Answer the following question with one sentence. An- swer the following question based on the video and the subtitles. The answer is short. Please directly respond with the short answer. Question: Question. The answer is: For the Grounded VideoQA task, we use the follow- ing prompt to generate the start and end seconds of the question-related clips: The video’s subtitles are listed below. Subtitles. The video’s captions are listed below. Each caption describes a Clip Length seconds clip. Captions. Your task is to determine in which intervals the clue exists that contain visual information needed to answer the question. Question: Question. Only output the answer in the following format: [[start1, end1], [start2, end2],...] In this output format, each ‘start’ and ‘end’ rep- resents the beginning and end of an interval in seconds (integer) where relevant clues can be found. You must provide at least one interval and at most five intervals. Here are some example outputs. Example 1: [[5, 7]] Example 2: [[200, 207], [209, 213], [214, 220]] C.4 Evaluation We evaluate our method on the validation set of MMVU and HourVideo. For the VideoQA task on CGBench, we adopt the long-acc setting in which the model takes the entire long video as the input and answers the given questions. We follow the official evaluation code of each benchmark to make a fair comparison with prior methods. If the official evaluation code is not pro- vided, we use the code from LMMs Eval (Zhang et al., 2024a). For the Grounded VideoQA task on CGBench, we need to decode the timestamps from the LLM outputs. In practice, we found that the Grounded QA prompt as shown in Section C.3 works surpris- ingly well, with each output sentence following the list format exactly. Therefore, we simply use the eval function in Python to decode the output text into a list object. We then use the official code pro- vided by CGBench to compute the mIoU between the prediction and the ground truth. D Qualitative Results We present and analyze several of SiLVR’s reason- ing traces in Figure 4, Figure 5, Figure 6, and Fig- ure 7. From the figures, we observe that SiLVR ef- fectively integrates information from both visual and speech modalities. Furthermore, SiLVR is ca- pable of performing complex video reasoning steps, including step-by-step reasoning, self-correction, self-verification, and using relevant prior knowl- edge from the LLM to answer a given question. … Okay, let's tackle this question. So, the question is asking which ingredient is not used in the artwork based on the video s ubtitles. The options are shell, glue, blue food dye, oil. Looking at the subtitles provided, let me go through each timestamp to see what's mentioned. Starting from 00:00:00 to 00:00:08, there's a glass bottle with blue liquid and decorative items. The decorative items might include shells, but not sure yet. At 00:00:16, the caption mentions a bottle of E6000 adhesive surrounded by seashells and a plastic cup with yellowish liquid. E6 000 is a type of glue, so glue is definitely used (option B is present). Shells are also mentioned here, so shell (A) is used. At 00:00:32, a hand adds a drop of blue food dye to the bottle. That's option C, which is used. Then at 00:00:40, the text says 'Fill 2/3 of bottle with oil' and there's a hand holding a green dropper. Sooil (D) is used. Wait, but the question is which is NOT used. Let me check again.All four options are mentioned except one. The options are A: Shell, B: Glue, C: Blue Food Dye, D: Oil. From the subtitles: seashells are mentioned (so A is used). Glue (E6 000 adhesive) is present (B used). Blue food dye is added (C used). Oil is instructed to fill 2/3 of the bottle (D used). Wait, all options are used? That can't be….. Maybe the shells are part of the decorative items but not an ingredient used in the artwork's construction? Wait, the question is about ingredients used in the artwork. Are the shells part of the decoration but not part of the main mixture? Let's read the steps again. The steps involve filling the bottle with water (00:00:24), adding blue dye (00:00:32), adding oil (00:00:40). Then there's mention of glue (E6000) which might be for sealing or attaching something. The seashells are surrounding the glue and maybe used as decoration. But perhaps the shells are not part ofthe liquid mixture inside the bottle but are just part of the surrounding decorations. If the question is about the ingredients inside t he bottle, …. Therefore, the shells might be part of the decoration around the bottle in the video but not part of the actual artwork (the bottle's co ntents). Sothe answer would be A: Shell, as the shells are not an ingredient used in the artwork. But I need to make sure. The question says \\ingredients \\, which impli es components used in creating the main part of the artwork. If the shells are not inside the bottle or part of its structure, t hen they are not ingredients. Sothe answer is A.Question: According to the video, which of the following ingredients is not used in the artwork? A. Shell B. Glue C. Blue Food Dye D. Oil. Video: Reasoning TraceFigure 4: Example 1 of SiLVR’s Reasoning Trace. The question asks which ingredients are not used in the video. Initially, SiLVR identified all listed items as potential ingredients. However, through self-correction, SiLVR correctly recognized that the shell is used as decoration rather than an ingredient. … Okay, let me try to figure this out. The question is asking the order in which certain planets are introduced in the video. T he options are A, B, C, D, each listing a sequence of three planets. First, I need to recall the structure of the video based on the provided subtitles and caption s. The subtitles start by talking about humanity leaving Earth and exploring the solar system. Then they mention Mercury first, discussing its atmosphere and temperature. The next planet discussed is Venus, with details about its thick atmosphere and the Soviet Venera missions. After Venus, the video moves on to Mars, discussing its potential for human colonization, radiation issues, dust storms, etc. Then the video goes into the outer planets, starting with Jupiter. They talk about its Great Red Spot, atmosphere, and moons like Io. After Jupiter, Saturn is mentioned with its rings and storms.Following Saturn, Neptune is discussed, focusing on its high -speed winds and moon Triton. Sothe sequence of planets introduced is Mercury → Venus → Mars → Jupiter → Saturn → Neptune. The question asks for the order among Venus, Jupiter, Neptune. From the sequence above, Venus comes before Jupiter, which comes before Neptune. Looking at the options, option A is Venus, Jupiter, Neptune. O ption C is Venus, Neptune, Jupiter, which would be incorrect. The other options don't include the correct order. For example, option B lists Me rcury, Jupiter, Mars, which isn't correct because Mars comes after Mercury and Venus. Sothe correct answer should be A.Question: In what order are the following planets introduced in the video? A.Venus, Jupiter, Neptune. B. Mercury, Jupiter, Mars. C. Venus, Neptune, Jupiter. D. Jupiter, Mercury, Neptune.Video: Reasoning Trace Figure 5: Example 2 of SiLVR’s Reasoning Trace. The video sequentially introduces six planets in detail: Mercury, Venus, Mars, Jupiter, Saturn, and Neptune. SiLVR accurately identifies the correct order of the planets and systematically inspects all answer choices, eliminating the incorrect ones through logical reasoning. … Okay, let's tackle this question. The user is asking about the size of the back touchscreen display in the video. The options are A through D with different inch measurements. First, I need to check the subtitles and the captions provided. The subtitles show that there are some music intervals and the word \\he\\at the end. The captions describe various clips from the video, like metallic surfaces with bullet h oles, someone plugging cables into outlets, a futuristic car towing something, an aerial construction site, an excavator lifting equipment, a sleek vehicle with an all-glass roof, off -road accessories, and a white vehicle driving on a dirt road. Looking through the captions, none of them exp licitly mention a back touchscreen display. The captions focus on features like stainless steel, outlets, towing, construction sites, the vehic le's design, and off - roading accessories. There's no direct mention of a touchscreen size here. Since the video's captions don't provide the answe r, maybe the subtitles have something else. The subtitles only show music and \\he\\at the end, which isn't helpful. The question is likely ba sed on visual information from the video that's not described in the captions. However, the user expects the answer to be derived from the given data. Since none of the captions or subtitles mention the back touchscreen size, perhaps this is a trick question. The options might be s tandard sizes for Tesla Cybertruck features. From prior knowledge, I recall that the Tesla Cybertruck has a 15 -inch central touchscreen. But the question is about the back touchscreen. Wait, some sources mention a 9.4 -inch rear screen. Let me think. The Cybertruck's rear display for passengers is often reported as 9.4 inches. Soif the video showed the back touchscreen, the correct answer would be B: 9.4 inches. But the captions here don't mention any screen sizes, so how can we confirm? Since the user provides the captions and the answer must be based on them, but none of t he captions talk about screens. Maybe the answer is inferred from the video's content. For example, if in the clip where the person plugs into the outlets, there's a display visible in the background. But the captions don't note that. Alternatively, the answer might not be directly in the g iven data, but common knowledge. If this is part of a Cybertruck features video, the back screen is 9.4 inches. Sothe answer is B.Question: What is the size of the back touchscreen display in this video? A. 15 inches. B. 9.4 inches. C. 17.4 inches. D. 18.5 inches.Video: Reasoning TraceFigure 6: Example 3 of SiLVR’s Reasoning Trace. The question asks about the size of the back touchscreen in the car shown in the video. The visual captioning module of SiLVR fails to capture the details about the touchscreen, which appears briefly for only about one second. However, by identifying the vehicle type and leveraging external knowledge from the LLM, SiLVR infers the correct answer. … … But in reality, if a chiral center has four different groups, swapping two groups would invert the configuration (from R to S or vice versa), wh ich would create the enantiomer. However, in the case where you swap two groups in the enantiomer (the mirror image), does that b ring it back to the original? Wait, no. Let's take a specific example. Suppose the original is R configuration. The mirror image is S. If in the S configur ation you swap two groups, you could end up with another configuratio n. For example, swapping the first and second groups in the S configuration might convert it back to R. Soswapping two groups in the enantiomer would give the original configuration, making them the same molecule. But wait, the ord erof swapping matters. The R and S configurations are determined by the Cahn -Ingold -Prelog priority rules.If you swap two groups in a chiral center, you can invert the configuration. For example, if the original is R, swapping the first and second priority groups … so the answer would be E: No, as they will become two same molecules. But wait, the original mirror image couldn't be rotated to align. But swapping greens might a llow rotation to align. Sothe answer would be E. But I need to confirm. If you take the mirror image and swap two groups, does that result in the origi nal molecule? For example, if the original is R and the mirror image is S, swapping two groups in S might convert it to R. If that's the case, then the two would be the same, so answer E. Yes. Swapping two groups in a chiral center inverts the configuration. Soif the mirror image is S, swapping two groups might turn it into R, which is the original. Therefore, they are the same molecule. Hence, the isomers would become the same molecule, s o the answer is E.\", \"answer\": \"E\",Question: Consider the two molecules that appeared in the video from 0:28 to 2:00, if we change the positions of the two green atoms in the second molecule, will the two isomers now become Optical Isomerism? A.Yes, as they cannot realign with each other by rotation B.Yes, as the molecular formula changes after swapping C.No, as the bonds will break during the swap D. Yes, as it creates new asymmetric centers E. No, as they will become two same molecules F. Yes, as it changes the overall symmetry of the molecule G.No, as the molecule will become unstable H.Yes, as it creates mirror videos that cannot superimpose I.No, as green atoms have special bonding properties J.Yes, as it changes the molecular weight distributionVideo: Figure 7: Example 4 of SiLVR’s Reasoning Trace. Through step-by-step reasoning, SiLVR is capable of solving complex chemistry questions. Notably, SiLVR does not immediately terminate the reasoning process upon reaching a plausible answer. Instead, it continues to verify the correctness of the generated answer before finalizing its response.",
  "text_length": 69546
}