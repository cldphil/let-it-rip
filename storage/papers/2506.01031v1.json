{
  "id": "http://arxiv.org/abs/2506.01031v1",
  "title": "NavBench: Probing Multimodal Large Language Models for Embodied\n  Navigation",
  "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong\ngeneralization in vision-language tasks, yet their ability to understand and\nact within embodied environments remains underexplored. We present NavBench, a\nbenchmark to evaluate the embodied navigation capabilities of MLLMs under\nzero-shot settings. NavBench consists of two components: (1) navigation\ncomprehension, assessed through three cognitively grounded tasks including\nglobal instruction alignment, temporal progress estimation, and local\nobservation-action reasoning, covering 3,200 question-answer pairs; and (2)\nstep-by-step execution in 432 episodes across 72 indoor scenes, stratified by\nspatial, cognitive, and execution complexity. To support real-world deployment,\nwe introduce a pipeline that converts MLLMs' outputs into robotic actions. We\nevaluate both proprietary and open-source models, finding that GPT-4o performs\nwell across tasks, while lighter open-source models succeed in simpler cases.\nResults also show that models with higher comprehension scores tend to achieve\nbetter execution performance. Providing map-based context improves decision\naccuracy, especially in medium-difficulty scenarios. However, most models\nstruggle with temporal understanding, particularly in estimating progress\nduring navigation, which may pose a key challenge.",
  "authors": [
    "Yanyuan Qiao",
    "Haodong Hong",
    "Wenqi Lyu",
    "Dong An",
    "Siqi Zhang",
    "Yutong Xie",
    "Xinyu Wang",
    "Qi Wu"
  ],
  "published": "2025-06-01T14:21:02Z",
  "updated": "2025-06-01T14:21:02Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01031v1",
  "full_text": "--- Page 1 ---\nNavBench: Probing Multimodal Large Language\nModels for Embodied Navigation\nYanyuan Qiao1Haodong Hong23Wenqi Lyu1Dong An4Siqi Zhang5\nYutong Xie4Xinyu Wang1Qi Wu1˚\n1The University of Adelaide2The University of Queensland3CSIRO Data61\n4Mohamed bin Zayed University of Artificial Intelligence5Tongji University\nProject Website\n(a)(b)(c)\nExecutionBasedontheinstruction,whatstepsshouldItaketoreachthegoal?Step-by-step NavigationComprehension -Progress Level\nGiventhestepstakensofar,howfaralongamIintheinstruction?Temporal ProgressEstimationComprehension -Global Level\nHavingcompletedthenavigation,whichinstructionbestdescribesthepathIjustfollowed?Global Instruction AlignmentComprehension –Local Level\nIfImoveindirection(c),whatwillIseenext?Local Observation-Action Reasoning\nDifficulty Level\nCognitive Complexity\nSpatial Complexity\nExecution ComplexityEasyMediumHard\nFigure 1: NavBench evaluates MLLMs across three comprehension tasks and a step-by-step execution\ntask, assessing their ability to understand navigation behavior, track progress, reason about observation\nand action, and act accordingly. The step-by-step navigation is assessed from different difficulty\nlevels, which is defined by cognitive, spatial, and execution complexity.\nAbstract\nMultimodal Large Language Models (MLLMs) have demonstrated strong gener-\nalization in vision-language tasks, yet their ability to understand and act within\nembodied environments remains underexplored. We present NavBench, a bench-\nmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot\nsettings. NavBench consists of two components: (1) navigation comprehension,\nassessed through three cognitively grounded tasks including global instruction\nalignment, temporal progress estimation, and local observation-action reason-\ning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432\nepisodes across 72 indoor scenes, stratified by spatial, cognitive, and execution\ncomplexity. To support real-world deployment, we introduce a pipeline that con-\nverts MLLMs’ outputs into robotic actions. We evaluate both proprietary and\nopen-source models, finding that GPT-4o performs well across tasks, while lighter\nopen-source models succeed in simpler cases. Results also show that models with\nhigher comprehension scores tend to achieve better execution performance. Provid-\ning map-based context improves decision accuracy, especially in medium-difficulty\nscenarios. However, most models struggle with temporal understanding, particu-\nlarly in estimating progress during navigation, which may pose a key challenge.\n˚Corresponding author\nPreprint. Under review.arXiv:2506.01031v1  [cs.CV]  1 Jun 2025\n--- Page 2 ---\n1 Introduction\nMultimodal Large Language Models (MLLMs) [ 1,2,3] have achieved impressive performance across\na wide range of vision-language tasks, demonstrating strong cross-modal reasoning and zero-shot\ngeneralization. These models excel at answering visual questions [ 4], interpreting videos [ 5], and\nperforming complex multimodal reasoning [ 6]. As their capabilities expand, a central question\nemerges: do these models truly understand how to act in the physical world, or are they simply adept\nat processing static inputs?\nRecent work has begun to explore MLLMs’ potential in embodied tasks by evaluating their spatial\nreasoning in 3D environments [ 7,8]. However, these tasks primarily focus on perception and passive\nscene understanding, without assessing the model’s ability to make decisions or take actions. In\ncomparison, navigation is a core embodied task that involves interpreting natural language instructions,\nanalyzing visual observations, and making a sequence of decisions to reach a goal. Although\nnavigation plays a crucial role in real-world applications, it remains relatively underexplored in the\ncontext of MLLMs. Traditional embodied navigation benchmarks, such as Room-to-Room (R2R) [ 9]\nand ObjectNav [ 10], were developed prior to the emergence of foundation models. These benchmarks\nrely on task-specific supervision and often reduce evaluation to final success rates, providing limited\ninsight into whether a model genuinely understands the navigation behavior. In many cases, an agent\nmay reach the goal by exploiting dataset biases or learning shortcuts, without correctly grounding the\ninstruction or following the intended path.\nSimilar to how humans acquire embodied skills by first understanding a task and then learning\nto execute it, evaluating the embodied capabilities of generalist MLLMs also requires examining\ntwo fundamental aspects. First, can the model comprehend what a navigation behavior represents,\nsuch as identifying the intent behind a completed trajectory? Second, can it act autonomously to\ncomplete a navigation task, making step-by-step decisions in unfamiliar environments? Furthermore,\nnavigation tasks in real-world environments can vary significantly in difficulty due to differences in\nspatial layout, instruction complexity, and required decision-making steps. For example, navigating\nacross multiple rooms with ambiguous instructions poses greater challenges than following simple\nstep-by-step commands in a single hallway. However, most existing benchmarks treat all navigation\nepisodes equally difficult, failing to capture this essential variation.\nTo fill these gaps, we introduce NavBench , a benchmark designed to systematically evaluate MLLMs\nin embodied navigation under zero-shot settings. NavBench decomposes the evaluation into two\ncomplementary components: Navigation Comprehension , which assesses whether a model under-\nstands and aligns with intended navigation behavior, and Navigation Execution , which evaluates the\nmodel’s ability to make accurate step-by-step decisions. To reflect real-world variability, NavBench\nincorporates a fine-grained difficulty classification based on spatial, cognitive, and execution com-\nplexity. In addition, it provides a deployable real-world navigation pipeline to bridge the gap between\nsimulation and practical embodiment.\nFirst , for navigation behavior comprehension, inspired by cognitive studies of human spatial rea-\nsoning [ 11], NavBench introduces three fine-grained evaluation tasks designed to assess distinct\nreasoning capabilities at three levels: global, progress, and local. It includes 3,200 question-answer\npairs. Specifically, Global Instruction Alignment evaluates the model’s ability to match a given\ntrajectory with the most appropriate instruction. The candidate instructions are designed with subtle\nsemantic differences, such as variations in directional cues and landmark descriptions, to encourage\ngenuine spatial reasoning. Temporal Progress Estimation measures temporal-contextual awareness by\nrequiring the model to infer progress within multi-step instructions based on a partial trajectory. Local\nObservation-Action Inference evaluates the model’s ability to reason about the spatial consequences of\nindividual actions by either predicting the future observation given an action or identifying the action\nthat caused a visual transition. Together, these tasks provide a comprehensive framework for assessing\nglobal semantic reasoning, temporal understanding, and local spatial inference in navigation.\nSecond , NavBench introduces a fine-grained difficulty classification with three levels: easy, medium,\nand hard, based on cognitive, spatial, and execution complexity. This allows detailed analysis of\nmodels’ generalization and decision-making performance across varying levels of difficulty. The\nbenchmark includes 432 navigation cases across 72 scenes.\nFinally , to bridge the gap between simulator-based evaluation and real-world deployment, we design\na practical navigation pipeline that connects MLLM outputs to executable actions on real robots. This\n2\n--- Page 3 ---\npipeline includes a waypoint selection module, an MLLM-based navigator, and a low-level controller,\ndemonstrating the deployability of our framework in physical environments.\nWe evaluate both closed-source and open-source MLLMs on NavBench. While GPT-4o currently\nachieves the best overall performance, we observe that lightweight models such as Qwen2.5-VL-7B\nare capable of reliably completing easy navigation tasks. Notably, this trend is also reflected in our\nreal-world deployment experiments, suggesting that NavBench may serve as a practical tool for\nanalyzing the embodied capabilities of both general and resource-efficient MLLMs. Furthermore, our\nresults suggest several notable trends: (1) comprehension and execution abilities appear to be closely\nrelated, (2) temporal reasoning may pose a persistent challenge for current models, and (3) compact\nopen-source models can, under certain conditions, approach the performance of proprietary ones,\nindicating their potential utility in practical settings.\nIn summary, our main contributions are as follows: (1) We introduce NavBench , a benchmark\nfor evaluating MLLMs in embodied navigation under zero-shot settings. (2) We decompose the\nevaluation into two components: Navigation Comprehension , with tasks targeting spatial, temporal,\nand local reasoning, and Navigation Execution , which assesses decision-making across difficulty\nlevels. (3) We develop a deployment pipeline that maps MLLM outputs to real-world robot actions.\n(4) We perform a detailed evaluation and analysis of both closed-source and open-source MLLMs,\nuncovering trends in their reasoning and execution performance across embodied tasks.\n2 Related Work\nBenchmarks for MLLMs Recent progress in Multimodal Large Language Models (MLLMs)[ 1,12,\n13,14,15] has driven the development of benchmarks assessing visual understanding and cross-modal\nreasoning. Early efforts such as VQA[ 4], GQA [ 16], OK-VQA [ 17], and TextVQA [ 18] focus on\nspecific tasks like factual or commonsense question answering. More recent benchmarks including\nMME [ 19], MMBench [ 20], MM-Vet [ 21], and MathVista [ 6] aim for broader coverage, evaluating\nperception and reasoning across diverse domains. However, these mainly target static tasks and do\nnot reflect MLLMs’ ability to act in dynamic environments. To bridge this gap, some recent work\nhas begun evaluating spatial reasoning in embodied settings. SpatialBench [ 7], ScanReason [ 22],\nand VSI-Bench [ 8] assess 3D spatial understanding using panoramas, semantic layouts, or textual\nscene descriptions. While insightful for embodied perception, they remain limited to passive tasks\nand do not assess decision-making or sequential interaction. In parallel, traditional embodied\nnavigation benchmarks such as R2R[ 9], REVERIE[ 23], and ObjectNav [ 10] have long been used\nto test instruction-following agents. However, they were designed for fully supervised settings\nand mainly evaluate success rates without probing intermediate reasoning. Although REVERIE\nincreases instruction abstraction, it retains similar path lengths and decision complexity, limiting\nits capacity to reveal behavioral differences. More recently, Wang et al. [24] proposed a fine-\ngrained evaluation framework for instruction understanding in VLN via multiple-choice questions,\noffering interpretability beyond end-to-end metrics. Still, their setup is restricted to small supervised\nmodels and lacks real-world deployment and zero-shot inference. To the best of our knowledge,\nno existing benchmark offers a comprehensive evaluation of MLLMs in embodied navigation that\njointly considers instruction understanding, sequential decision-making, difficulty stratification, and\nreal-world transferability.\nEmbodied Navigation Embodied navigation tasks require an agent to reach a goal location within\nan environment, guided by a description such as an image [ 25,26], object [ 10,27], or natural\nlanguage instruction [ 9,28,29]. Among these, language-guided navigation has attracted significant\nattention for its potential to facilitate intuitive human-robot interaction. Researchers have explored\ndiverse instruction formats, including step-by-step [ 9,30], dialog-based [ 31], and goal- or intention-\noriented instructions [ 23,32]. Traditional approaches train navigation policies using annotated\ndatasets, incorporating modules to improve object relation understanding [ 33,34,35], vision-language\nalignment [ 36,37,38], memory [ 39,40,41], and spatial reasoning [ 42,43,44]. While effective on\nbenchmarks, these methods often suffer from limited generalization due to dataset biases [ 45,46,47].\nTo mitigate this, recent work turns to MLLMs for zero-shot embodied navigation, leveraging their\ngeneralization abilities. Some use MLLMs to localize goal-relevant regions [ 48,49,50], while others\nemploy prompt-based guidance for instruction following [ 51,52,53,54]. These approaches reduce\nreliance on task-specific training but still lack fine-grained evaluation: most benchmarks focus solely\non final success rates, offering limited insight into the model’s reasoning process. To address this, we\n3\n--- Page 4 ---\nLocal LevelQuestion:Giventhecurrentviewandatargetview.,selectthedirectionthatismostlikelytoleadtothetargetview.\nCurrentview\nTargetview\nABCDLocal Observation-Action Reasoning\nQuestion:Giventhecurrentviewandadirectiontomove.selectthelocationmatchingtheexpectedviewaftermoving.ABC\nProgress LevelQuestion:Youaregivenanavigationinstructiondividedintomultiplesub-instructions,alongwithatrajectory.Yourtaskistodeterminehowmanysub-instructionshavebeencompletedbasedontheviewsprovided.\nTemporal Progress Estimation\nGlobal LevelQuestion:Youarepresentedwithasequenceofpanoramicviewsthatrepresentanavigationpathfromthestartingpointtothegoallocation.Identifythecorrectinstruction.A.Walkallthewayforwardtowardsthedoor,andturnleft.Walkforward,andstopattheotherdoorB.Walkupthestairsandintothefirstdoorwayonyourright.Stopjustinsidethedoorway.C.Walkoutofthebathroomandturnright.WalkthroughthedoorwayontherightandstopinfrontofthebedD.Exittheclosetandgostraightuntilyougettothebed.Turnleftandexittheroom.Turnleftandgointothebedroom.Waitattheentrance.\n…Global Instruction Alignment\nCurrentviewDirectionSub-instruction1:walkoutsideandtotheleftofthetableandchair.Sub-instruction2:walkdownthestepandintotheyard.Sub-instruction3:turnleftandwalkbythehouse.Sub-instruction4:stopjustbeforethesecond.Figure 2: Illustration of the Navigation Comprehension task.\nintroduce NavBench, a benchmark that systematically evaluates both the reasoning and execution\ncapabilities of MLLMs in embodied navigation.\n3 Benchmark Design\n3.1 Task Formulation\nWe evaluate the navigation capabilities of MLLMs by decomposing the task into two core components:\nNavigation Comprehension , which assesses the understanding of navigation behavior, and Navigation\nExecution , which focuses on step-by-step decision making.\nNavigation Comprehension It investigates whether the model can understand and reason about\nimplicit navigation behaviors, including aligning instructions with trajectories, estimating progress\nalong a plan, and predicting the spatial consequences of actions. These tasks span different reasoning\nlevels ( global ,progress , and local ) and serve as diagnostic probes for navigation understanding.\nIllustrations of the three comprehension tasks are shown in Figure 22.\n•Global Level – Global Instruction Alignment: Given a navigation trajectory and several candidate\ninstructions, the model is required to determine which instruction aligns with the executed path.\nThis task tests the model’s understanding of the overall intent and structural coherence of the\nnavigation behavior.\n•Progress Level – Temporal Progress Estimation: Provided with a partial trajectory and a list of\nsegmented sub-instructions, the model must identify the sub-instruction that was most recently\ncompleted. This evaluates the model’s capacity to monitor task progress and comprehend the\ntemporal structure of instructions.\n•Local Level – Local Observation-Action Reasoning: To evaluate the model’s ability to reason about\nthe spatial consequences of individual actions. We design two variants: (1) Future-Observation\nPrediction – the model observes the current view and an action, and selects the correct resulting\nview. (2) Future-Action Prediction – the model observes two consecutive views and must identify\nthe action that caused the transition.\nNavigation Execution It examines whether an MLLM can make accurate, step-by-step movement\ndecisions in an embodied environment based on the current observation and instruction. We conduct\nthis evaluation in a zero-shot setting [ 54] within the Matterport3D simulator [ 55], categorizing tasks\ninto three difficulty levels (easy, medium, and hard) to assess performance. To ensure a fair and\nstandardized evaluation protocol, we evaluate MLLMs via viewpoint selection rather than low-level\naction prediction (e.g., turning or moving forward). This abstraction, consistent with prior embodied\nnavigation benchmarks [ 9,23], allows us to focus on high-level semantic reasoning grounded in\nlanguage and vision, while avoiding the confounding variability introduced by continuous control. It\nalso facilitates zero-shot evaluation and comparability across different models. Notably, while our\nsimulator setup centers on abstracted decision-making, Section 4 illustrates how this framework can\nbe extended to real-world navigation by converting viewpoint selection into low-level control.\n2The questions in the figure are slightly simplified for clarity and brevity.\n4\n--- Page 5 ---\nNavigation ComprehensionSource Raw DatasetsR2R / RxR…\nStep A1Extract Multimodal Navigation DataStep A2Navigation ExecutionSample Navigation EpisodesMatterPort3D SimulatorStep B1Compute Difficulty SoresStep B2Spatial/Cognitive/Execution\nHuman Difficulty RatingStep B3\nAssignDifficultyLevelStep B4Easy/Medium/Hard\nConstruct Q&A SamplesStep A3Global/Progress/Local Action Level\nInstruction/Trajectory/ Single Views/Panoramic Views…\nExec. Task Statistics by Difficulty Levels(a)(b)(c)\nComp.Benchmark Statistics\nFigure 3: NavBench construction pipeline and statistics. (a) QA generation for comprehension\ntasks at global, progress, and local levels. (b) Execution pipeline combining automatic difficulty scor-\ning and human ratings. (c) Benchmark statistics, including comprehension (comp.) task distribution,\nQA counts, and execution statistics (e.g., instruction length, steps, distance).\nSpecifically, at each step, the model receives the current panoramic observation, the natural language\ninstruction, and a list of candidate navigable viewpoints. The model must select the next location to\nmove to, thereby executing the instruction step-by-step until the goal is reached. Formally, at each\nsteptof a navigation episode, the MLLM receives an instruction x“tw1, w2, ..., w Luof length L,\na set of candidate navigable observation viewpoints Ot“to1\nt, o2\nt, ..., oN\ntu, and optional context Ct\n(such as navigation history or previous actions). The agent must select an action atcorresponding to\none of the navigable directions:\nat“MLLMpx, Ot, Ctq (1)\nThis decision process may involve reasoning about the instruction, interpreting the current view,\nleveraging prior context, and anticipating the result of each candidate action.\n3.2 Dataset Construction\nData Sources NavBench is constructed by reorganizing and enriching fine-grained navigation data\nwith multimodal observations to enable zero-shot evaluation of MLLMs. We start by collecting\ninstruction-trajectory pairs from multiple embodied navigation benchmarks, including R2R [ 9],\nRxR [ 30], GEL-R2R [ 56], and FGR2R [ 57]. These datasets serve as annotation sources, but do\nnot include the visual inputs needed for multimodal reasoning. To address this gap, we use the\nMatterport3D simulator to extract both panoramic and single-viewpoint RGB images aligned with\nnavigation trajectories. The image extraction process involves traversing agent paths, sampling\nintermediate viewpoints, and rendering corresponding visual observations. All visual and textual\ndata are then organized into a unified structure that supports multiple reasoning tasks and enables\nconsistent QA generation across comprehension and execution settings. Figure 3 shows the overall\nbenchmark construction pipeline.\nStatistics We report statistics in Figure 3(c), including distribution of comprehension subtasks and\ncoverage of scenes and episodes in execution. These statistics reflect the scale and diversity of the\nbenchmark across reasoning levels and scenes.\n3.2.1 Question-and-Answer Pairs Collection\nWe design three diagnostic tasks targeting global alignment, temporal progress estimation, and\nlocal spatial and action reasoning. In total, we collect 3,200 question-and-answer pairs to evaluate\ncomprehension capacity in embodied navigation.\nGlobal Instruction Alignment To evaluate MLLMs’ ability to align spatial trajectories with semanti-\ncally consistent instructions, we construct a multiple-choice dataset comprising 1,200 examples. Each\nexample consists of a panoramic trajectory and five candidate instructions, including one ground-truth\nand four distractors. The distractors are generated using four perturbation strategies: (1) Basic :\nrandom instructions sampled from unrelated trajectories, testing global relevance; (2) Directional\nreplacements , where spatial terms (e.g., “left”, “north”) are substituted using POS tagging via NLTK,\nprobing directional grounding; (3) Object replacements , where noun phrases are replaced with unre-\nlated landmarks drawn from an external landmark-annotated dataset [ 56], evaluating object-trajectory\n5\n--- Page 6 ---\ngrounding; (4) Shuffled segments , where human-annotated sub-instructions [ 57] are permuted to\ndisrupt temporal structure while preserving grammaticality. Each instruction set is randomly ordered\nand paired with a panoramic trajectory composed of viewpoint sequences and movement annotations.\nThe design promotes multimodal spatial reasoning and reduces reliance on superficial cues.\nProgress Estimation This task is designed to evaluate a model’s ability to perform temporal reasoning\nand monitor execution progress during navigation. Each full navigation instruction is segmented into\na sequence of sub-instructions, and each sub-instruction is aligned with a corresponding portion of the\nagent’s trajectory. We leverage fine-grained annotations [ 57], which provide this alignment between\nindividual sub-instructions and the associated panoramic viewpoints traversed during execution. To\nconstruct evaluation examples, we truncate the trajectory at intermediate points that mark the end\nof specific sub-instructions. The model is presented with the truncated panoramic trajectory along\nwith the full list of sub-instructions, and is required to predict the index of the last completed one.\nTo ensure data quality and minimize ambiguity, we filter the examples using a curated list of valid\ninstruction-path pairs. In total, we collect 1,000 such examples for evaluation.\nLocal Observation-Action Reasoning We design two multiple-choice reasoning tasks to evaluate\na model’s capacity for local spatial and action reasoning inference. Both tasks present ambiguous\nscenarios that require fine-grained visual discrimination and understanding of plausible transitions. In\nFuture-Observation Prediction, the model receives a current view and an action, and must choose the\ncorrect resulting view from a set of candidates. In Future-Action Prediction, the model observes two\nconsecutive views and selects the action that best explains the transition. For both tasks, distractors\nare carefully sampled from nearby observations or visually similar actions to ensure ambiguity and\nchallenge. We collect 500 examples for each format, yielding a total of 1,000 samples. All questions\nare formatted as multiple-choice queries to ensure consistency across evaluation tasks.\n3.2.2 Navigation Episodes Collection\nWe sample 432 navigation cases from 72 unique scenes in the Matterport3D simulator [ 55]. To\nsystematically assess the difficulty of each case, we define a composite complexity score across three\northogonal dimensions: spatial ,cognitive , and execution complexity. Each dimension is derived\nfrom structural properties of the environment or linguistic cues in the instruction, following the\nmethodology inspired by [ 58,59]. In addition, human evaluation is conducted to further support and\nvalidate the difficulty classification process.\nSpatial Complexity It quantifies the geometric and topological challenges of a navigation trajectory.\nWe consider four features: (1) total path length d, (2) standard deviation of turn angles θ, (3) vertical\nrange zas a proxy for elevation change, and (4) 2D spatial area Acovered by the path. A binary\nindicator Ipzą1.5qis included to capture significant elevation changes such as floor transitions.\nThese features are computed from agent poses and scene connectivity data. The spatial complexity\nscore is defined as:\nΦspatial“α1¨logp1`dq`α2¨logp1`θq`α3¨Ipzą1.5q`α4¨logp1`Aq (2)\nCognitive Complexity It reflects the linguistic difficulty of navigation instructions. We extract five\nfeatures using dependency parsing: (1) instruction length L, (2) number of verbs V, (3) number of\nspatial terms S(e.g., left,upstairs ), (4) number of landmark mentions M(e.g., kitchen ), and (5)\nnumber of subordinate clauses C(e.g., relcl ,advcl ). The cognitive complexity score is defined as:\nΦcognitive“β1¨logp1`Lq`β2¨logp1`Vq`β3¨logp1`Sq`β4¨logp1`Mq`β5¨C(3)\nExecution Complexity It measures the behavioral effort required to complete the navigation. We\nconsider: (1) number of steps N, (2) number of turns T, (3) floor change indicator F, and (4) number\nof decision points D. The score is computed as:\nΦexecution“γ1¨logp1`Nq`γ2¨logp1`Tq`γ3¨F`γ4¨D (4)\nNormalization Each raw complexity score Φis normalized to the range r1,9susing a non-linear\nmapping:\nˆΦ“roundˆ\n1`8¨logp1`Φq´logp1`Φminq\nlogp1`Φmaxq´logp1`Φminq,2˙\n(5)\nThe weights α,β, and γare empirically set to balance the contribution of each factor.\n6\n--- Page 7 ---\nHuman Evaluation To complement the automatic scoring, we conducted a human evaluation to\nvalidate our difficulty annotations. A group of annotators independently rated each case along the\nthree defined dimensions, using a 1–9 scale with detailed guidelines aligned to our scoring criteria.\nFigure 4: Radar chart of average complexity\nscores across cognitive, spatial, and execution\ndimensions for different difficulty levels.Difficulty Categorization Based on the final scores,\neach case is categorized into one of three levels, as\nillustrated in Figure 4:\n•Easy (score 1–3): Short paths with simple instruc-\ntions, few steps, minimal spatial reasoning, and\nclear landmarks.\n•Medium (score 4–6): Instructions with moderate\nlength, multiple landmarks or spatial phrases, and\nmedium-length paths.\n•Hard (score 7–9): Long trajectories guided by com-\nplex multi-step instructions, often involving floor\ntransitions and multiple spatial references.\n4 Real-World Deployment Pipeline\nRES-50TransformerRES-50Waypoint Predictor\n0.25m3m90o270o360o\nWaypointsHeatmap\nPredictedWaypointsMLLM Decision\nNavigation InstructionMLLM\nAngle\nDistanceLow-Level Controller\nRelative Rotation & Distance\nRobot MovementAPI\nFigure 5: Overview of the real-world embodied navigation pipeline.\nTo demonstrate the real-world feasibility of MLLM-\nguided embodied navigation, we implement a modular pipeline that complements our benchmark\nevaluation, as illustrated in Figure 5. It consists of three modules: (1) a Waypoint Predictor that\nextracts RGB and depth inputs to generate candidate waypoints, (2) an MLLM Decision Module that\nselects the most goal-aligned waypoint, and (3) a Low-Level Controller that translates the selected\nwaypoint into motion commands for execution on a physical robot. The system is deployed on a\ndual-arm mobile robot equipped with an RGB-D camera and evaluated in real indoor environments.\n5 Evaluation on NavBench\n5.1 Settings\nModels We evaluate both proprietary and open-source MLLMs widely adopted in recent research.\nProprietary models include GPT-4o and GPT-4o-mini. Open-source models include InternVL2.5-\n2B/8B [60], Qwen2.5-VL-3B/7B [61], LLaV A-OneVision-7B [62], and Llama3.2-Vision-11B [63].\nImplementation Details Proprietary models are accessed via APIs, while open-source models are\ndeployed using vLLM [64] and lmdeploy [65] on a single NVIDIA A6000 GPU (48GB). Simulator-\nbased evaluations are conducted in the Matterport3D Simulator [ 55], built on high-resolution RGB-D\nscans of real indoor environments such as homes and offices. It provides realistic visual inputs and\ndiscrete agent movement within a 3D mesh, making it a standard testbed for embodied navigation. For\nreal-world deployment, we integrate our pipeline with a dual -arm composite mobile robot equipped\nwith an Intel RealSense D435 camera and a Water Drop 2 wheeled base. All physical experiments\nare conducted in a controlled indoor lab to assess robustness and feasibility.\nEvaluation Metrics Our benchmark includes both multiple-choice reasoning and embodied nav-\nigation execution tasks. For multiple-choice questions, we follow standard practice [ 5] and use\nAccuracy as the primary metric, which measures whether the model selects the correct answer from\na set of candidates based on the provided information. For execution tasks, we adopt standard\nmetrics in embodied navigation [ 9,30].Success Rate (SR) measures the percentage of episodes\nwhere the target object is visible from the agent’s final viewpoint, defined as being within a 3-meter\n7\n--- Page 8 ---\nTable 1: Performance comparison on Navigation Comprehension andExecution .\nModelNavigation Comprehension Navigation Execution\nGlobal Progress LocalComp. AvgEasy Medium HardExec. Avg\nAccuracy SR SPL SR SPL SR SPL\nChance Level (Random) 19.33 25.4 29.34 24.65 16.41 9.57 7.17 3.72 7.33 4.99 8.19\nVLN-Bench (tiny) Performance\n:Human Level 88.33 79.00 85.00 84.11 91.67 88.68 87.50 81.53 75.00 65.17 81.59\n:GPT-4o 51.67 45.00 63.00 53.89 66.08 49.01 43.79 36.44 25.00 20.11 40.07\n:Qwen2.5-VL-7B 36.67 32.00 47.00 38.56 46.25 35.59 25.27 18.93 12.50 5.93 24.41\nClosed Models\nGPT-4o 51.33 42.90 65.80 53.34 67.36 54.31 41.67 35.71 27.78 21.15 41.33\nGPT-4o-mini 50.33 29.90 59.03 46.42 46.53 40.44 28.47 24.90 15.28 12.29 27.99\nOpen-Source Models\nInternVL2.5-2B 67.25 23.40 11.25 33.97 25.69 25.29 6.94 6.68 7.64 5.86 13.02\nQwen2.5-VL-3B 43.83 21.30 50.63 38.59 23.61 17.52 12.50 8.88 10.26 5.24 13.00\nInternVL2.5-8B 62.75 28.50 28.12 39.79 28.47 28.19 7.66 7.42 7.64 6.18 14.26\nQwen2.5-VL-7B 57.58 31.20 47.00 45.26 41.67 32.55 22.92 17.43 10.42 5.67 21.77\nLLaV A-OneVision-7B 31.17 26.60 39.00 32.26 31.25 17.64 15.58 7.80 15.02 7.84 15.86\nLlama3.2-Vision-11B 36.00 23.40 29.10 14.75 27.08 25.90 10.42 9.19 10.02 7.60 15.04\nNote: Dark teal and light teal indicate the top-performing closed and open-source models per column.\n:indicates results evaluated on the NavBench (tiny) subset.\nFigure 6: Model Performance under Different\nInstruction Perturbations.\nFigure 7: Model performance on Local\nObservation-Action Reasoning.\nradius. Success weighted by Path Length (SPL) adjusts SR by path efficiency and is computed as\nSPL“1\nNřN\ni“1Si¨ℓi\nmaxpℓi,piq, where Nis the number of episodes, SiPt0,1uindicates success, ℓi\nis the shortest path, and piis the path length.\nVLN-Bench (tiny) Human Performance To establish a human performance reference for VLN-\nBench, we construct a compact evaluation subset named VLN-Bench (tiny) by randomly sampling\na representative subset of tasks, inspired by the strategy in [ 8]. Annotating the entire dataset with\nhuman responses is impractical due to scale, so we select 120 multiple-choice questions for the Global\nInstruction Alignment task, 100 for Temporal Progress Estimation, 100 for Local Observation-Action\nReasoning, and 72 navigation episodes for Execution Evaluation. All questions are in multiple-choice\nformat with predefined correct answers. Human participants, who were volunteer students from our\nresearch institute with relevant backgrounds, independently completed each task. Their responses\nwere automatically scored using the same metrics applied to model evaluation, including accuracy\nfor comprehension tasks and SR/SPL for execution. This evaluation did not involve personal data\ncollection or behavioral intervention and was conducted in accordance with institutional guidelines.\nIn total, VLN-Bench (tiny) includes 392 human-evaluated questions and episodes.\n5.2 Performance\nWe begin by examining the relationship between comprehension and execution. As shown in\nTable 1, model performance on comprehension and execution tasks is closely aligned. GPT-4o\nachieves the highest comprehension average (53.34%) and execution average (41.33%). Among\nopen-source models, Qwen2.5-VL-7B performs best (45.26%, 21.77%), approaching GPT-4o-mini\n(46.42%, 27.99%) and demonstrating potential for deployment in real-world robotics. Turning\nto comprehension subtasks, InternVL2.5-2B achieves strong performance on Global Instruction\nAlignment (67.25%), even surpassing GPT-4o (51.33%). However, its accuracy drops sharply on\n8\n--- Page 9 ---\nTable 2: Impact of map information on GPT-4o.\nDiff. Map SR SPL Avg Gain\nEasy✗ 67.36 54.31 60.84 –\n✓ 70.14 54.11 62.13 +1.29\nMed.✗ 41.67 35.71 38.69 –\n✓ 46.53 39.86 43.20 +4.51\nHard✗ 27.78 21.15 24.47 –\n✓ 29.17 22.32 25.75 +1.28\nFigure 8: Distribution of navigation error\ntypes identified from manual analysis.\nmore challenging reasoning tasks. In particular, Progress Estimation remains a consistent weakness\nacross models: aside from GPT-4o (42.90%), all others perform poorly, highlighting current MLLMs’\nlimitations in temporal reasoning. We next analyze how models perform across navigation difficulty\nlevels. Most open-source models can only reliably complete Easy episodes. While GPT-4o maintains\nrelatively strong results across all difficulty levels, suggesting better generalization.\nThese findings suggest several overarching insights. First, comprehension and execution abilities are\nstrongly linked. Second, temporal reasoning, particularly progress tracking, remains a major bottle-\nneck. Third, compact open-source models like Qwen2.5-VL-7B can offer competitive performance\nwith significantly lower resource requirements, making them promising for embodied applications.\n5.3 Discussion\nBreakdown of Distractor Types in Instruction Alignment\nWe further analyze performance on the Global Instruction Alignment task by breaking down results\nacross four distractor types: basic ,direction ,object , and shuffle . As shown in Figure 6, most models\nhandle the basic condition well, indicating their ability to reject unrelated instructions. However,\nperformance under direction andobject perturbations varies significantly across models, suggesting\ninconsistent grounding of spatial terms and landmarks. Notably, all models perform poorly under\ntheshuffle condition, where sub-instructions are reordered but their content remains unchanged.\nThis result is particularly revealing: despite the presence of the same entities and actions, altering\nthe temporal structure makes the instruction much harder for models to interpret. The models’\nfailure in this setting highlights their limited ability to reason about temporal order within complex\ninstructions. This finding aligns with the low scores observed in the Progress Estimation task,\nreinforcing that current MLLMs struggle with temporal understanding across both instruction-level\nand trajectory-level reasoning.\nFuture-Action and Future-Observation Reasoning We analyze performance on the Local\nObservation-Action Reasoning task, which includes two subtasks: Future-Action and Future-\nObservation Prediction. As shown in Figure 7, models show consistent performance across both, with\nGPT-4o clearly outperforming all others, consistent with its strong results in Navigation Execution.\nThese subtasks reflect complementary reasoning skills. Future-Action Prediction tests whether a\nmodel can infer the spatial transition between two views, while Future-Observation Prediction re-\nquires anticipating how the environment changes after a given action. Both capabilities are critical\nfor navigation, where agents should reason about cause and effect in spatial transitions.\nEffect of Map Information on Action Decisions Although our benchmark evaluations assume\nno access to map information, reflecting real-world constraints, we investigate whether providing\nmap connectivity can enhance action selection. Specifically, we follow the approach introduced\nin MapGPT, where topological relationships between explored nodes are encoded as text prompts.\nUsing GPT-4o, we compare performance with and without map input across different difficulty levels.\nAs shown in Table 2, the presence of map information consistently improves success rates, with\nthe largest gain observed under medium difficulty, yielding an increase of 4.86 percentage points.\nThis suggests that access to structured spatial context can facilitate better high-level reasoning and\nplanning, especially in medium complexity settings where spatial ambiguity is more common.\nError Analysis We manually analyze 100 failed cases to understand model failures. Based on\nthought traces and action sequences, we identify four common error types: (a) Incorrect Plan :\nthe plan misaligns with the instruction; (b) Misaligned Action : the plan is valid, but the chosen\n9\n--- Page 10 ---\nmovement does not follow it; (c) Failure to Stop : the agent overshoots the goal or stops early; and (d)\nHallucinated Movement : the model selects a nonexistent location. The error distribution is shown\nin Figure 8. These patterns align with weaknesses in comprehension tasks. For example, type (c)\nreflects poor Progress Estimation . This suggests execution failures often stem from temporal and\nspatial reasoning limitations, reinforcing the diagnostic value of NavBench.\nReal-World Validation To assess the feasibility of our real-world deployment pipeline, we conduct a\npilot study in an indoor environment using GPT-4o and Qwen2.5-VL-7B, the top proprietary and\nopen-source models from our benchmark. Each model is tested on 10 cases, achieving success rates\nof 60% and 40%, respectively. These results show that both can handle simple navigation tasks in\nreal-world settings. Their success trends mirror execution performance in Table 1, where both models\noutperform others in their categories. This suggests that NavBench’s simulation-based evaluation\nreliably reflects real-world embodied performance.\n6 Conclusion\nThis paper presents NavBench, a diagnostic benchmark designed to evaluate MLLMs in embodied\nnavigation under zero-shot settings. It decomposes the evaluation into two components: Navigation\nComprehension, which evaluates global instruction alignment, temporal progress estimation, and local\nobservation-action reasoning through three cognitively grounded tasks, and Navigation Execution,\nwhich examines step-by-step decision-making across varying levels of difficulty. Additionally, we\ndevelop a pipeline for real-world deployment of MLLM-driven agents. Through evaluation and\ntargeted analysis, NavBench reveals limitations in temporal understanding and action grounding that\nare not captured by standard success metrics. It also shows that lightweight open-source models can\nbe effective in simpler navigation scenarios. We hope NavBench can serve as a useful resource for\nanalyzing the embodied capabilities of MLLMs and supporting future work in this direction.\nReferences\n[1]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in\nNeural Information Processing Systems , volume 36, 2023.\n[2]Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\nZhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model’s perception of the\nworld at any resolution. arXiv preprint arXiv:2409.12191 , 2024.\n[3]Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nDamien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n[4]Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017.\n[5]Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\nYunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu,\nXiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive\nevaluation benchmark of multi-modal llms in video analysis. ArXiv , abs/2405.21075, 2024.\n[6] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation\nmodels in visual contexts. arXiv preprint arXiv:2310.02255 , 2023.\n[7]Wenxiao Cai et al. Spatialbot: Precise spatial understanding with vision language models. In IEEE\ninternational conference on robotics and automation , 2025.\n[8]Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space:\nHow Multimodal Large Language Models See, Remember and Recall Spaces. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2025.\n[9]Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian D. Reid, Stephen\nGould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In CVPR , pages 3674–3683, 2018.\n10\n--- Page 11 ---\n[10] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object\ngoal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing\nSystems , 33:4247–4258, 2020.\n[11] Benjamin Kuipers. The spatial semantic hierarchy. Artificial intelligence , 119(1-2):191–233, 2000.\n[12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,\nXizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Intern vl: Scaling up vision\nfoundation models and aligning for generic visual-linguistic tasks. IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 24185–24198, 2023.\n[13] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock,\nAida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv ,\nabs/2204.14198, 2022.\n[14] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. ArXiv ,\nabs/2308.12966, 2023.\n[15] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 26286–26296, 2023.\n[16] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 6700–6709, 2019.\n[17] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on\ncomputer vision and pattern recognition , pages 3195–3204, 2019.\n[18] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 8317–8326, 2019.\n[19] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on\nmultimodal large language models. arXiv preprint arXiv:2306.13549 , 2023.\n[20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In\nEuropean Conference on Computer Vision , pages 216–233. Springer, 2025.\n[21] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and\nLijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490 , 2023.\n[22] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and Xihui Liu. Scanreason: Empowering 3d visual\ngrounding with reasoning capabilities. In European Conference on Computer Vision , 2024.\n[23] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den\nHengel. REVERIE: remote embodied visual referring expression in real indoor environments. In CVPR ,\npages 9979–9988, 2020.\n[24] Zehao Wang, Minye Wu, Yixin Cao, Yubo Ma, Meiqi Chen, and Tinne Tuytelaars. Navigating the nuances:\nA fine-grained evaluation of vision-language navigation. ArXiv , abs/2409.17313, 2024.\n[25] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. In IEEE international\nconference on robotics and automation , pages 3357–3364. IEEE, 2017.\n[26] Jacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, and Devendra Singh Chaplot. Instance-specific\nimage goal navigation: Training embodied agents to find object instances. arXiv preprint arXiv:2211.15876 ,\n2022.\n[27] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian\nStraub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In\nProceedings of the IEEE/CVF international conference on computer vision , pages 9339–9347, 2019.\n11\n--- Page 12 ---\n[28] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer\nfor vision-and-language navigation. In Advances in Neural Information Processing Systems , 2021.\n[29] Yue Zhang et al. Vision-and-language navigation today and tomorrow: A survey in the era of foundation\nmodels. Transactions on Machine Learning Research , 2024.\n[30] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Mul-\ntilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the\nConference on Empirical Methods in Natural Language Processing , pages 4392–4412, 2020.\n[31] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation.\nInCoRL , pages 394–406, 2019.\n[32] Hongcheng Wang, Andy Guan Hong Chen, Xiaoqi Li, Mingdong Wu, and Hao Dong. Find what you want:\nLearning demand-conditioned object attribute space for demand-driven navigation. In Advances in Neural\nInformation Processing Systems , 2023.\n[33] Yuankai Qi, Zizheng Pan, Shengping Zhang, Anton van den Hengel, and Qi Wu. Object-and-action aware\nmodel for visual language navigation. In European Conference on Computer Vision , pages 303–317.\nSpringer, 2020.\n[34] Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity\nrelationship graph for agent navigation. Advances in Neural Information Processing Systems , 33:7685–\n7696, 2020.\n[35] Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, and Tieniu Tan. Neighbor-view enhanced\nmodel for vision and language navigation. In Proceedings of the 29th ACM International Conference on\nMultimedia , pages 5101–5109, 2021.\n[36] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic\nagent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 13137–13146, 2020.\n[37] Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. Airbert: In-\ndomain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 1634–1643, 2021.\n[38] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving\nvision-and-language navigation with image-text pairs from the web. In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16 , pages 259–274.\nSpringer, 2020.\n[39] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer\nfor vision-and-language navigation. Advances in neural information processing systems , 34:5834–5847,\n2021.\n[40] Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop: History-and-order\naware pre-training for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 15418–15427, 2022.\n[41] Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop+: History-enhanced\nand order-aware pre-training for vision-and-language navigation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 45(7):8524–8537, 2023.\n[42] Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Mul-\ntimodal map pre-training for language-guided navigation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 2737–2748, 2023.\n[43] Rui Liu, Xiaohan Wang, Wenguan Wang, and Yi Yang. Bird’s-eye-view scene graph for vision-language\nnavigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 10968–\n10980, 2023.\n[44] Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Gridmm: Grid memory map for\nvision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 15625–15636, 2023.\n[45] Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao.\nScaling data generation in vision-and-language navigation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 12009–12020, 2023.\n12\n--- Page 13 ---\n[46] Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang,\nJason Baldridge, and Zarana Parekh. A new path: Scaling vision-and-language navigation with synthetic\ninstructions and imitation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 10813–10823, 2023.\n[47] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng\nZhang, and He Wang. Navid: Video-based vlm plans the next step for vision-and-language navigation.\narXiv preprint arXiv:2402.15852 , 2024.\n[48] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, and Xin Eric Wang.\nEsc: Exploration with soft commonsense constraints for zero-shot object navigation. In International\nConference on Machine Learning , pages 42829–42842. PMLR, 2023.\n[49] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. Cows on\npasture: Baselines and benchmarks for language-driven zero-shot object navigation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 23171–23181, 2023.\n[50] Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. Vlfm: Vision-\nlanguage frontier maps for zero-shot semantic navigation. In IEEE international conference on robotics\nand automation , pages 42–48. IEEE, 2024.\n[51] Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, and Hao Dong. Instructnav: Zero-shot system\nfor generic instruction navigation in unexplored environment. arXiv preprint arXiv:2406.04882 , 2024.\n[52] Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation\nwith large language models. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38,\npages 7641–7649, 2024.\n[53] Yanyuan Qiao, Wenqi Lyu, Hui Wang, Zixu Wang, Zerui Li, Yuan Zhang, Mingkui Tan, and Qi Wu. Open-\nnav: Exploring zero-shot vision-and-language navigation in continuous environment with open-source\nllms. arXiv preprint arXiv:2409.18794 , 2024.\n[54] Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, and Kwan-Yee Wong. Mapgpt: Map-\nguided prompting with adaptive path planning for vision-and-language navigation. In Proceedings of the\n62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n9796–9810, 2024.\n[55] Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva,\nShuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGB-D data in indoor\nenvironments. In 3DV, pages 667–676, 2017.\n[56] Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, and Erwei Yin. Grounded entity-landmark\nadaptive pre-training for vision-and-language navigation. IEEE/CVF International Conference on Com-\nputer Vision , pages 12009–12019, 2023.\n[57] Yicong Hong, Cristian Rodriguez, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-language\nnavigation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing ,\npages 3360–3376, 2020.\n[58] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? In\nProceedings of the International Conference on Learning Representations , 2025.\n[59] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy\nGao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. Benchmarking complex\ninstruction-following with multiple constraints composition. ArXiv , abs/2407.03978, 2024.\n[60] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\nHao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo,\nJiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi\nShao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Hui Deng, Jiaye Ge, Kaiming Chen, Min Dou,\nLewei Lu, Xizhou Zhu, Tong Lu, Dahu Lin, Yunfeng Qiao, Jifeng Dai, and Wenhai Wang. Expanding\nperformance boundaries of open-source multimodal models with model, data, and test-time scaling. ArXiv ,\nabs/2412.05271, 2024.\n[61] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang,\nWei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo\nYang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv , abs/2502.13923, 2025.\n13\n--- Page 14 ---\n[62] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei\nLiu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 , 2024.\n[63] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.\n[64] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles ,\n2023.\n[65] LMDeploy Contributors. Lmdeploy: A toolkit for compressing, deploying, and serving llm. https:\n//github.com/InternLM/lmdeploy , 2023.\n14",
  "text_length": 56248
}