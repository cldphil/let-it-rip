{
  "id": "http://arxiv.org/abs/2506.01140v1",
  "title": "ReTern: Exploiting Natural Redundancy and Sign Transformations for\n  Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs",
  "summary": "Ternary large language models (LLMs), which utilize ternary precision weights\nand 8-bit activations, have demonstrated competitive performance while\nsignificantly reducing the high computational and memory requirements of\nfull-precision LLMs. The energy efficiency and performance of Ternary LLMs can\nbe further improved by deploying them on ternary computing-in-memory (TCiM)\naccelerators, thereby alleviating the von-Neumann bottleneck. However, TCiM\naccelerators are prone to memory stuck-at faults (SAFs) leading to degradation\nin the model accuracy. This is particularly severe for LLMs due to their low\nweight sparsity. To boost the SAF tolerance of TCiM accelerators, we propose\nReTern that is based on (i) fault-aware sign transformations (FAST) and (ii)\nTCiM bit-cell reprogramming exploiting their natural redundancy. The key idea\nis to utilize FAST to minimize computations errors due to SAFs in +1/-1\nweights, while the natural bit-cell redundancy is exploited to target SAFs in 0\nweights (zero-fix). Our experiments on BitNet b1.58 700M and 3B ternary LLMs\nshow that our technique furnishes significant fault tolerance, notably 35%\nreduction in perplexity on the Wikitext dataset in the presence of faults.\nThese benefits come at the cost of < 3%, < 7%, and < 1% energy, latency and\narea overheads respectively.",
  "authors": [
    "Akul Malhotra",
    "Sumeet Kumar Gupta"
  ],
  "published": "2025-06-01T19:49:01Z",
  "updated": "2025-06-01T19:49:01Z",
  "categories": [
    "cs.AR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01140v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01140v1  [cs.AR]  1 Jun 2025ReTern: Exploiting Natural Re dundancy and Sign\nTransformations for Enhanced Fault Tolerance in\nCompute-in-Memory based Tern ary LLMs\nAkul Malhotra and Sumeet Kumar Gupta\nPurdue University\nWest Lafayette, Indiana, USA\nmalhot23@purdue.edu\nAbstract —Ternary large language models (LLMs), which uti-\nlize ternary precision weights and 8-bit activations, have demon-\nstrated competitive performance while significantly reducing the\nhigh computational and memory requirements of full-precision\nLLMs. The energy efficiency and performance of Ternary\nLLMs can be further improved by deploying them on Ternary\ncomputing-in-memory (TCiM) accelerators, thereby alleviating\nthe von-Neumann bottleneck. However, TCiM accelerators are\nprone to memory stuck-at faults (SAFs) leading to degradation\nin the model accuracy. This is particularly severe for LLMs\ndue to their low weight sparsity. To boost the SAF tolerance\nof TCiM accelerators, we propose ReTern that is based on (i)\nfault-aware sign transformations (FAST) and (ii) TCiM bit-cell\nreprogramming exploiting their natural redundancy. The key\nidea is to utilize FAST to minimize computations errors due to\nSAFs in +1/-1 weights, while the natural bit-cell redundancy is\nexploited to target SAFs in 0 weights (zero-fix). Our experiments\non BitNet b1.58 700M and 3B ternary LLMs show that our\ntechnique furnishes significant fault tolerance, notably 35%\nreduction in perplexity on the Wikitext dataset in the presence\nof faults. These benefits come at the cost of <3%,<7%, and\n<1% energy, latency and area overheads respectively.\nIndex Terms —Ternary neural networks, computing-in-\nmemory, large language models (LLMs), stuck-at faults (SAFs),\nI. I NTRODUCTION\nThe advent of large language models (LLMs) has revo-\nlutionized language-based applications, demonstrating state-\nof-the-art performance across various tasks. However, their\nresource-intensive nature poses significant challenges for\nefficient deployment on hardware, especially on resource-\nconstrained edge devices [1]. Moreover, as LLMs grow in size,\nthe demands for storage and computation as well as the data\nmovement between memory and processors increase, further\nexacerbating deployment challenges.\nComputing-in-memory (CiM) has emerged as a popular\nhardware paradigm to alleviate the von-Neumann bottleneck\nthat plague conventional AI accelerators. By performing in-\nmemory vector-matrix multiplication (dot product) operation,\nthe dominant kernel in LLM inference, CiM minimizes the\ndata movement between the compute and memory units,\nproviding energy and latency benefits. Various CiM-based\nLLM accelerators have been proposed, showcasing sizeable\nbenefits over conventional von-Neumann based platforms such\nas GPUs [2]–[5].Another approach to enhance efficiency is quantization,\nwhich reduces model size by reducing the precision of the\nmodel parameters. Quantization techniques have been shown\nto achieve substantial reductions in storage needs, energy, and\nlatency while maintaining accuracy [6]. In fact, aggressive\nquantization schemes like binary/ternary precision quantiza-\ntion have also proven to achieve comparable model accuracy,\nwhile providing a massive increase in the energy efficiency,\nspeed and storage density [7]–[9]. Recently, a ternary LLM\nvariant called Bitnet b1.58 has been proposed, that utilizes\nternary precision weights and 8-bit activations [8]. Experi-\nments show that Bitnet b1.58 can match the accuracy of a full\nprecision LLM (fp16) at 3B model size and beyond, making\nit a suitable candidate for resource-constrained applications.\nBy combining the benefits of CiM and ternary quantization,\nternary CiM (TCiM) designs offer a compelling argument\nfor LLM acceleration at the edge. TCiM accelerators feature\nmassively parallel in-memory vector-matrix multiplications\nwith ternary weights and/or activations and have been imple-\nmented using both SRAM and emerging non-volatile mem-\nory (NVM) technologies, demonstrating significant energy-\nlatency-area advantages [10]–[13]. While these TCiM acceler-\nators have been evaluated on convolutional/recurrent neural\nnetworks (CNNs/RNNs), their benefits in performing mas-\nsively parallel ternary in-memory vector-matrix multiplications\ncan, in principle, be extended to ternary LLMs.\nHowever, despite these advantages, TCiM accelerators re-\nmain susceptible to manufacturing defects, particularly mem-\nory stuck-at faults (SAFs). SAFs irreversibly corrupt a portion\nof the memory bits, leading to the erroneous storage of model\nweights, which leads to inaccuracies in inference. The issue for\nSAFs are further worsened for emerging memory technologies,\ndue to their relatively immature fabrication processes. Several\nsolutions have been proposed to address this issue. However,\nmost techniques require model re-training or fine-tuning [14],\n[15], which can be prohibitively expensive for large models\nlike LLMs [16]. Additionally, these approaches are often\nincompatible with CiM, not suitable for ternary precision, and\nintroduce significant resource overhead due to the addition of\nlarge amounts of redundancy [17]–[20].\nTo address the limitations of previous solutions, we pro-\nposed TFix in a prior work [21]. TFix is a training-free\n--- Page 2 ---\ntechnique that mitigates the impact of memory faults for TCiM\naccelerators. TFix utilizes the natural redundancy present\nin TCiM bitcells as well as the weight sparsity in ternary\nprecision DNNs to significantly reduce inference accuracy\ndegradation due to SAFs. However, TFix has so far been\nevaluated only on ternary CNNs, and its effectiveness for other\nworkloads, such as ternary LLMs, remains an open question.\nIn this work, we throw light onto the limitations of TFix on\nternary LLMs deployed on TCiM accelerators and propose a\nnew technique that counters these limitations. Our analysis\nof TFix reveals that it is less effective for ternary LLMs\ndue to their inherently lower weight sparsity. To address this\nlimitation, we propose ReTern, a novel approach that leverages\nfault-aware sign transformations (FAST) alongside the inher-\nent redundancy of ternary bitcells. While TFix can only handle\nSAFs in zero weights, ReTern utilizes FAST to target +1/-1\nweights and integrates FAST with TFix (zero-fix) to expand\nto coverage to all three weight types that can be affected by\nSAFs, In other words, the sign transformations in conjunction\nwith zero-fix augment the ability of ReTern to minimize the\nimpact of SAFs on the ternary LLM accuracy, while preserving\nthe functionality of the vector matrix multiplications. Our key\ncontributions include:\n•We analyze the effect of SAFs on two ternary LLMs:\nBitNet 1.58 (700M) and BitNet 1.58 (3B), deployed on\nTCiM hardware.\n•We assess the accuracy improvements offered by TFix\nand identify its limitations, particularly in ternary LLMs,\nin which weight sparsity is lower compared to convolu-\ntional ternary DNNs [21]–[23].\n•We propose ReTern, which utilizes fault-aware sign trans-\nformations and the innate redundancy of ternary bit-cells\nto improve the fault tolerance of ternary LLMs. We show\nthat ReTern offers sizeable improvements in perplexity\non the Wikitext dataset as well as accuracy on PIQA and\nARC (easy) in the presence of SAFs.\n•We evaluate the hardware overheads of ReTern for\nSRAM, FeFET and ReRAM based TCiM accelerators,\nshowing a minimal increase in energy, latency and area.\nII. B ACKGROUND AND RELATED WORKS\nIn this section, we provide background on ternary LLMs\nand ternary CiM hardware. We then examine the impact\nof stuck-at faults (SAFs) on CiM-based DNN accelerators\nand review existing mitigation strategies along with their\nlimitations. We also provide a brief overview of TFix and\ndiscuss its limitations, identifying key areas for improvement.\nA. Ternary Large Language Models (LLMs)\nBitNet b1.58 is a ternary precision LLM in which the LLM\nweights are represented as signed ternary values: [-1,0,1]. Per-\ntensor weight quantization to ternary precision is performed by\nutilizing the absmean quantization function:˜W=RoundClip (W\nγ+ϵ,−1,1), (1)\nRoundClip (x, a, b ) = max( a,min(b,round (x))),(2)\nγ=1\nnmX\nij|Wij|. (3)\nWhere ˜Wis the ternary weight, Wis the full precision weight,\nγis the mean absolute value of Wandϵis a small positive\nconstant added to prevent division by zero.\nTernary LLMs are obtained using quantization-aware train-\ning (QAT), which is substantially costlier than post-training\nquantization (PTQ). Experiments in [8] show that BitNet b1.58\ncan match the performance of the full precision (fp16) baseline\nfrom model size 3B and above. BitNet b1.58 utilizes 8-bit\nactivations, although it is worth noting that works like [9]\nare also exploring the possibility of reducing the activation\nbit precision from 8 to 4 bits. In this work, we evaluate the\nSAF tolerance of BitNet b1.58 with 700 million (BitNet b1.58\n700M) and 3 billion ((BitNet b1.58 3B) parameters, when\ndeployed on TCiM accelerators.\nB. Ternary Computing-in-memory (TCiM)\nVarious TCiM macros have been proposed using both\nCMOS and non-volatile memory (NVM) technologies, such\nas resistive RAM (ReRAM), magnetic RAM (MRAM), and\nferroelectric FETs (FeFETs) [10]–[13]. Broadly, TCiM macros\ncan be categorized into two types: (1) signed ternary input with\nsigned ternary weights, and (2) higher-precision input with\nsigned ternary weights. For the former, two binary memory\nelements are utilized to realize a ternary weight (three states).\nFor in-situ matrix-vector multiplications for signed inputs and\nweights, cross-coupling via access transistors can be used as\nproposed in [10]. For the latter design also, two binary memory\nelements are used per ternary bit-cell; however, cross-coupling\nis not required, as shown in Fig 1.\nThis work focuses on TCiM macros of type (2), aligning\nwith the 8-bit activations of the ternary LLMs in our study.\nHowever, the SAF tolerance enhancement strategies proposed\nin this work are also applicable to TCiM macros based on\ndesign of type (1). Fig. 1 illustrates a common bitcell topology\nfor signed ternary weights and higher precision inputs. Since a\nsingle ternary weight can have three possible states, two binary\nmemory elements M1andM2are required. M1andM2could\nbe CMOS or NVM-based. When M1/M2is in ’0’ state, there\nis a high resistance path through it to ground, whereas ’1’ state\nprovides a conducting/low resistance path. The ternary weights\nare stored using the differential weight encoding shown in\nFig 1. The high-precision activation Iis bit-streamed on WL 1\nand is binary (0 or VDD) per cycle. In case Iis signed, it can\nbe bit-streamed in 2’s complement encoding. Since two binary\nelements can represent four possible states, each bitcell has\na natural redundancy in the form of an unused state, which\ncorresponds to M1=M2= 1. We refer to this redundancy\nas natural since it is a consequence of implementing a ternary\nweight using two binary memory elements.\n--- Page 3 ---\nFigure 1: TCiM bitcell with its input and weight encoding. The\nboxed state (blue) in the weight encoding is an unused state (natural\nredundancy).\nFrom Fig. 1, we observe that the ternary weight ( W) can\nbe interpreted as W=M1−M2, resulting in the scalar\nproduct I·Wbeing the difference of two scalar products,\nI·M1andI·M2. To obtain the scalar product of Wand\nI, the read bitlines BL 1andBL 2are precharged to the\nsupply voltage VDD. Then, the wordline WL is asserted\nbased on the value of I. A discharge of ∆onBL 1(BL 2)\nsignifies a scalar product of +1 (-1), and no discharge on\nBL 1andBL 2represents a scalar product of 0. To compute\nthe dot productPL\ni=1IiWi, multiple wordlines are activated\nsimultaneously, and the cumulative voltage drops on BL 1and\nBL 2correspond to x=PIiM1iandy=PIiM2i. Analog-\nto-digital converters (ADCs) digitize these values, and a digital\nsubtractor computes the partial sum, x−y[11]. The CiM\noutputs are then accumulated in accordance with their bit-\nsignificance.\nIn this work, we focus on TCiM macros designed with\nbinary memory elements. It may be possible to design ternary\nbitcells with multilevel NVMs. However, since ternary weights\nare signed, such a design would require pre- and post-\nprocessing circuitry, such as adder trees, to realize the cor-\nrect output. Additionally, multi-state memories have reduced\ndistinguishability as compared to binary memories and thus,\nare more prone to CiM errors from variations and hardware\nnon-idealities [24].\nC. Stuck-at faults (SAFs) in CiM-based DNN accelerators\nAggressive scaling and the exploration of emerging mem-\nory technologies have made the study of stuck-at faults\n(SAFs) more critical than ever. Additionally, conventional\nfault mitigation strategies are not always directly applica-\nble to CiM-enabled memories, necessitating exploration of\nCiM-compatible SAF tolerant techniques. SAFs cause binary\nmemory elements to become permanently fixed at either ‘0’\n(High-Resistance State, HRS) or ‘1’ (Low-Resistance State,\nLRS), resulting in stuck-at-0 (SA0) and stuck-at-1 (SA1)\nfaults, respectively. SAFs can be classified as either masked\nor unmasked, depending on the stored data and the fault\ntype. Unmasked faults occur when the stored value and the\nfault state differ (e.g., an SA1 fault in a location intended to\nstore ‘0’), leading to errors. In contrast, masked faults occur\nwhen the stored value matches the fault state, and thus, are\ninnocuous. It is evident that only the unmasked faults directly\ncontribute to errors.\nFigure 2: Ternary weight sparsities in DNNs, as reported in previous\nstudies, compared to the sparsities observed in Bitnet 1.58B-700M\nand 3B ternary LLMs. Our findings indicate that ternary LLMs have\nrelatively lower weight sparsities, making TFix less effective for these\nmodels.\nVarious solutions have been explored for mitigating the\nimpact of SAFs in DNN accelerators. One approach is to\nenhance fault tolerance via training/finetuning the DNN. The\nwork in [15] utilizes fault-aware retraining of the DNN weights\nto enhance accuracy, while the work in [14] incorporates drop-\nconnect into the training phase to increase the resilience of\nDNNs to SAFs. Although both solutions are highly effective,\nDNN training is not only expensive, but also requires access\nto labeled training data, which may not always be available.\nA training-free strategy to tackle SAFs is to utilize fault-\naware weight mapping. The weights are mapped onto the CiM\nmacros in a way that minimizes the number of unmasked\nSAFs. The works in [17]–[19] show improved accuracies for\ninference with SAFs; however, these mapping strategies are\nnot applicable to ternary weights. For example, the technique\nin [17] maps the most significant bits (MSBs) of the weights on\nto the fault-free locations while mapping the least significant\nbits (LSBs) on the faulty memory elements to reduce the\nimpact of SAFs. However, this technique can not be applied\nto TCiM accelerators since ternary weights do not have MSBs\nand LSBs.\nAnother training-free approach to improve SAF tolerance\nis the addition of artificial redundancy, where additional\nrows/columns are used to correct / compensate for the SAF-\ninduced errors [25], [26]. However, such techniques may need\nsizeable overheads, with energy overheads ranging from 24%\nto112% in [25]. The approach in [20] introduces an overhead-\nfree solution by applying structured pruning to the DNN,\nfreeing up space that is then used to add redundant rows and\ncolumns. However, this method necessitates model re-training\nto recover the accuracy lost due to pruning.\nOur previous work TFix [21] addressed the need for a\ntraining-free, low-overhead solution targeted for TCiM accel-\nerators (more details in Section II-D). However, the evaluation\nof TFix in [21] is limited to convolutional neural networks\n(CNNs), for which TFix shows significant improvements in\naccuracy in the presence of SAFs. However, the question about\nthe performance of TFix in LLMs is unaddressed. In this\nwork, we evaluate the effectiveness of TFix on ternary LLMs\n--- Page 4 ---\nFigure 3: Zero-fix, wherein, when an ideal zero weight ( 00), is\nerroneously stored as as −1or+1due to a SAF, it is fixed by\nreprogramming the memory cell to 01.\ndeployed on TCiM accelerators, highlighting key limitations.\nFurthermore, we propose ReTern, that overcomes these limi-\ntations by incorporating fault-aware sign transformations.\nD. TFix: SAF tolerant Solution for TCiM\nTFix enhances SAF tolerance by exploiting the fact that\na zero weight can be stored in two distinct ways, due to\nthe inherent redundancy in TCiM bitcells (as discussed in\nSection II-B) [21]. If a single SAF occurs in a TCiM bitcell\nstoring zero, TFix can mitigate its impact by reprogramming\nthe bitcell to the alternate state that also represents zero,\neffectively correcting the error (more details later).\nTFix eradicates the impact of SAFs on zero weights without\nrequiring any hardware overhead. However, TFix has some\nnoteworthy limitations. Firstly, TFix can fix zero weights\nwhere only one memory element ( M1orM2) is impacted by\nan SAF. Fortunately, the probability of two SAFs in a single\nbitcell is very low, assuming a random distribution of SAFs.\nAnother major limitation of TFix is that it can only correct\nSAF-induced errors in zero weights. If an SAF occurs in a\nbitcell where wiis 1 or -1, the error cannot be fixed by Tfix\ndue to the lack of alternative encoding options (like for the zero\nweight). Thus, the effectiveness of TFix is highly dependent on\nmodel weight sparsity. Specifically, the higher the percentage\nof zero weights, the more effective TFix becomes. As a result,\nSAF tolerance achieved by TFix becomes dependent on the\nattributes (e.g. weight sparsity) of specific ternary models. For\nexample, previous works have reported high weight sparsity\nfor various ternary precision models, with values as high as\n∼95% (shown in Fig. 2) [21], [22], [27], making them well\nsuited for TFix. However, we observe that the Bitnet 1.58B\nternary LLMs exhibit lower weight sparsities of 37.05% and\n37.55% for the 700M and 3B parameter models, respectively\n(Fig. 2). As a result, TFix would be relatively ineffective\nfor BitNet 1.58B ternary LLMs, highlighting the need for an\nimproved approach.\nIII. P ROPOSED SOLUTION : RETERN\nReTern utilizes two techniques in conjunction to mitigate\nSAF impact: zero-fix and fault-aware sign transformation\n(FAST). Zero-fix, which is essentially TFix, corrects SAF-\ninduced error in zero weights by leveraging the natural re-\ndundancy in TCiM bitcells. (Note, in this paper, we use the\nname zero-fix instead of TFix to avoid confusion with the\nprevious TFix approach which explicitly relies on weight\nFigure 4: Fault-aware sign transformations being applied to a TCiM\nweight column.\nsparsity). The key operation that makes ReTern effective,\nirrespective of the sparsity of the network, is FAST. FAST\nminimizes the unmasked faults in -1/1 weights by selectively\napplying sign flips to weight columns in TCiM arrays, while\nmaintaining the correct MVM functionality. Both zero-fix and\nFAST are complementary and do not interfere with each other,\nsince zero-fix and FAST target zero and non-zero weights,\nrespectively.\nA. Zero-Fix\nAs discussed in Section II-B, TCiM bitcells have a natural\nredundancy in the form of an unused state, corresponding to\nM1=M2= 1. We observe that if the 11’ state were to\nbe used for scalar product computation, it would functionally\nmimic the behavior of the 00’ (0 weight) state [21]. This is\nbecause the ’11’ state would produce a voltage drop of ∆on\nboth BL 1andBL 2forI= 1. Since the scalar product is\nrepresented by the difference of the voltage drops in BL 1and\nBL 2, the equal drops produced by the ’11’ state on BL 1and\nBL 2would cancel out, leading to an output of 0. Note, ’11’\nrepresents the zero weight for both I= 0 (no drop on both\nBL 1andBL 2since the access transistors are off) and I= 1\n(as discussed above). Thus, there are two ways to store the ’0’\nweight, M1=M2= 0 (’00’) or M1=M2= 1 (’11’), which\nwe refer to as 00and01, respectively.\nZero-fix utilizes this natural redundancy to correct the errors\ndue to SAFs in the zero weights. If a zero weight is incorrectly\nstored due to a single SAF (Fig. 3) in the bitcell, we utilize the\nfault-free memory element in the bitcell to eliminate the error\ndue to the SAF. For example, if whw(intended to be stored\nas00(wi= 0)), is in the ’01’/’10’ state due to a stuck-at 1\nfault in M2/M1, it can be transformed to 01by programming\nM1/M2to ’1’. Thus, the impact of the hard fault on the TCiM\ncomputation would be eliminated by rewriting the faulty zero\nweights to 01.\nB. Fault-aware sign transformation (FAST)\nWhile Zero-Fix effectively handles errors in zero weights,\nFAST mitigates SAF-induced errors in +1 and -1 weights,\nwhich make up the majority of ternary LLM weights. This\nenhancement overcomes the key limitation of TFix.\n--- Page 5 ---\nAlgorithm 1 ReTern Algorithm\n1:Input: Ideal TCiM array weight Wideal\n2:Output: Optimized weight mapping Wmapped\n3:Step 1: Obtain SAF information from diagnosis.\n4:Step 2: Calculate Whardware from SAF information and\nWideal\n5:Step 3: Fault-Aware Sign Transformation (FAST)\n6:foreach column in each TCiM memory array do\n7: errstandard ←P|whw−wideal|\n8: errflipped ←P|whwflipped +wideal|\n9: iferrflipped < err standard then\n10: Wmapped −column ← − Wideal−column\n11: colflip←1\n12: else\n13: Wmapped −column ←Wideal−column\n14: colflip←0\n15: end if\n16:end for\n17:Step 4: Zero-Fix\n18:foreach weight winWideal where w= 0 do\n19: ifwhardware ̸= 0 then\n20: Store wmapped = 0 1\n21: else\n22: Store wmapped = 0 0\n23: end if\n24:end for\n25:Return Wmapped\nFig. 4 illustrates how the sign transformation operation is\napplied to a single column of the TCiM memory array. After\ndiagnosing the fault pattern of the column (discussed in the\nnext sub-section), we choose whether to store the weights\nas they are ( W), or the negative of all the weights in the\ncolumn ( −W). We refer to these two as the standard and\nthe flipped options, since multiplication by-1 is equivalent to\nflipping the sign of the non-zero weights (+1 to -1 and vice-\nversa). We make this decision based on which option yields\nlarger number of masked faults (discussed in Section II-C)\nand thus fewer erroneous perturbations due to SAFs. In the\nexample illustrated in Fig. 4, the flipped option is more\nsuitable to store the weight column given the fault pattern.\nTo obtain the correct vector-matrix multiplication with the\nflipped option, the TCiM output must be multiplied by -1,\nsincePL\ni=1Ii(−Wi) =−1∗PL\ni=1IiWi. Thus, it is essential\nto keep track of which columns in the TCiM memory array\nare flipped, for which we introduce a vector colflip. For a\nTCiM array with mcolumns, the size of colflip ismbits.\nIf the ithcolumn is stored in the flipped (standard) form, the\nvalue of colflip[i]is 1 (0).\nC. ReTern algorithm\nThe ReTern algorithm consists of four stages: weight map-\nping, diagnosis, FAST and zero-fix. In the weight-mapping\nstep, we partition the ideal ternary weight matrix Wideal and\nmap the submatrices onto multiple TCiM memory arrays. Inthe fault diagnosis stage, we utilize fault testing techniques to\ndetermine the position and type of SAFs in the TCiM arrays\n[28]. Using the SAF information and Wideal, we can determine\nthe hardware-mapped weights Whardware , which may contain\nincorrect weight values due to SAFs.\nIn the FAST stage, we first calculate the SAF-induced error\nfor each column of each TCiM memory array. This error is\nmeasured as:\nerrstandard =X\n|whw−wideal| (4)\nwhere whwrepresents the hardware-mapped weights, and\nwideal represents the ideal weights. We then compute the SAF-\ninduced error for the flipped weight configuration:\nerrflipped =X\n|(−whwflipped )−wideal)|\n=X\n|whwflipped +wideal|(5)\nNext, we compare errstandard anderrflipped , selecting the\nmapping option that yields a smaller error. Concurrently, we\nset or reset the corresponding column flip ( colflip) for each\nTCiM array based on whether the flipped or the standard\ncolumn is used.\nFinally, we perform the zero-fix step and determine for\neach zero weight whether it should be stored in the 00or01\nform, based on the individual SAF information (as discussed\nin Section III-A). It should be noted that the FAST and zero-\nfix steps effect a mutually exclusive set of weights. Zero-fix\nis designed to mitigate the impact of SAFs in zero weights,\nwhile FAST targets non-zero weights. This is because the\ntransformations utilize sign-flipping, which does not effect\nzero weights, since −1∗0 = 0 . Thus, by utilizing FAST and\nzero-fix in conjunction in ReTern, we address the limitations of\nTFix without introducing any new complications. Additionally,\ndue to their mutually exclusive nature, FAST and zero-fix\nsteps can be performed in any order. ReTern is summarized\nin Algorithm 1.\nIn the next subsection, we discuss the hardware modifi-\ncations needed for the TCiM array to support ReTern. It is\nimportant to note that the execution of the ReTern algorithm,\nwhich determines the optimal weight mapping, does not add\nto the inference overhead of the TCiM accelerator. This is\nbecause the algorithm execution is performed offline, prior to\nprogramming the weights into the TCiM array.\nD. Hardware Implementation\nOnce ReTern is applied on the pre-trained ternary weights,\nthe resultant weights are programmed onto the TCiM accel-\nerator for deployment. Fig. 5 (a) illustrates a TCiM memory\narray modified to support ReTern. To enable compatibility,\nadditional peripheral circuitry is required. First, a near-memory\nregister is required to store the colflip vector. Second,\nperipheral circuitry is required for post-processing the output.\nRecall from Section II-B, the final dot product x−yis\ncomputed using a subtractor, that subtracts the ADC outputs\n--- Page 6 ---\nFigure 5: (a) Modified TCiM array to make it compatible with\nReTern. (b) shows the post-processing required to obtain the correct\ndot-product. Two 2:1 multiplexers are added to datapath to choose\nwhether the output or the negation of the output is to be computed,\nbased on the value of the colflip bit for the column.\nfrom the two bitlines. We use two 2:1 multiplexers to choose\nwhether to compute x−yory−x, based on the value of\ncolflip value for that column(Fig. 5(b)). This is effectively\nthe same as multiplying the final output with -1 (1) if the\ncolflip bit is 1 (0). (Recall, to maintian the correct MVM\nfunctionality, a column flip must be followed by negating the\noutput). Thus, the overheads of ReTern are minimal, requiring\njust one extra register for colflip, and two 2:1 multiplexers to\npost-process the outputs (details in Section IV). Note that the\nsubtractor is not an overhead, since it is required to compute\nthe dot product even in the baseline design [10], [11]. Note that\nthe additional hardware required by ReTern is due to FAST,\nwhile zero-fix does not need any hardware modifications.\nBoth the efficacy and overheads of ReTern depend on the\nTCiM array size. Since the fault-aware sign transformations\nare carried out at the column granularity, the more the number\nof rows in the array, the less effective the transformations\nwill be. However, for an nxmsize TCiM array, the memory\noverhead associated with colflip, which is of length m, is\nm\nn∗m=1\nn. Thus, there exists a trade-off between ReTern\nperformance and overhead with respect to the number of rows\nin the TCiM array. We aim to quantitatively explore this trade-\noff in a future work. In this work, we analyze the efficacy ofour technique and the hardware overhead for a fixed array\nsize of 64 x 64. It is important to note that this trade-off only\nexists for the sign transformation step of ReTern. The zero-fix\nstep operates at the per-weight granularity, and is not directly\neffected by the array size.\nIV. R ESULTS\nA. Evaluation Framework\nIn this section, we quantitatively investigate the SAF toler-\nance provided by ReTern for ternary LLMs. We conduct our\nexperiments on the BitNet 1.58b 700M and 3B models, and\nwe obtain the pretrained weights from Hugging Face [29],\n[30]. We evaluate our models on three tasks: Wikitext, PIQA\n[31], and ARC (easy) [32]. We compare the perplexity for\nWikitext in the presence of SAFs for the baseline design (no\nfault tolerance), design using TFix [21] (akin to using only\nzero-fix in ReTern), design with only FAST and the proposed\nReTern (FAST + zero-fix). Similarly, for PIQA and ARC\n(easy), we compare the accuracies. We select PIQA and ARC\n(easy) over other tasks for our evaluation because the ternary\nmodels exhibit reasonable software (ideal) accuracies on them.\nIn contrast, other tasks, such as ARC (challenge), ternary\nLLMs yield software accuracies close to random guessing,\nmaking it difficult to clearly assess the impact of SAFs. (Note\nthat the random-guess accuracies on PIQA and ARC (easy) are\n50% and 25%, respectively). As discussed in Section III-D, we\nutilize multiple TCiM arrays of size 64x64 to map the model\nweights. The SAFs are injected randomly and uniformly across\neach of the TCiM arrays, as in similar works [25]. 20 Monte\nCarlo fault injection experiments are performed for two SAF\nrates: 5% and 10%.\nAn important noteworthy point about LLMs is that they\nperform two types of matrix multiplications during inference:\n(i) in the feedforward layers and (ii) in the self-attention\nlayers. In the feedforward layers, matrix multiplication occurs\nbetween static weight matrices and dynamic input activations.\nIn contrast, the self-attention layers involve matrix multipli-\ncations between dynamically generated query, key, and value\nmatrices, which are computed on-the-fly based on the input\nrepresentations. Given that non-volatile memories such as\nReRAMs, FeFETs etc. have limitations such as large write\nlatency/energy and limited endurance, TCiM arrays designed\nwith them may not be suitable for self-attention. Thus, works\nlike [33] utilize digital compute cores for the self-attention\nlayers, while using CiM-based memory arrays for the feedfor-\nward layers. We also adopt such an architecture for our TCiM\naccelerator design, thus considering the self-attention layers to\nbe SAF-free, with all the SAFs occurring in the feedforward\nlayer ternary weights.\nB. SAF Tolerance\nFig.6 and Fig.7 present box-and-whisker plots illustrating\nthe impact of Stuck-at Faults (SAFs) on the BitNet 1.58b\n700M and 3B models, respectively. The green dashed lines in\neach figure indicate the software/ideal accuracy and perplexity.\nLet us start with the baseline model. At an SAF rate of\n--- Page 7 ---\n(a) Wikitext\n(b) PIQA\n(c) ARC (easy)\nFigure 6: Performance results for the 700M BitNet 1.58b model.\nSAF rates of 5% and 10% are evaluated.\n10% and for the 700M model, we observe an increase in\nWikitext perplexity from approximately 12 to 26 and accuracy\ndegradation of around 4% and 8% on PIQA and ARC (easy),\nrespectively. In contrast, the larger 3B model shows a more\nmoderate increase in Wikitext perplexity (from about 10 to\n15) and accuracy degradations of approximately 3% on PIQA\nand 6% on ARC (easy)) at the same 10% SAF rate. Thus, we\nobserve that the larger 3B parameter model exhibits greater\nrobustness to SAFs compared to the 700M parameter model.\nThis may be attributed to the larger redundancy present in the\nbigger model.\nNext, we observe that both TFix and FAST have a similar\neffectiveness in mitigating the impact of SAFs. At the SAF\nrate of 10% (5%), both techniques individually improve the\nWikitext perplexity by around 23% (6% ) on average for the\n(a) Wikitext\n(b) PIQA\n(c) ARC (easy)\nFigure 7: Performance results for the 3B BitNet 1.58b model. SAF\nrates of 5% and 10% are evaluated.\n700M model and by 15% (5%) for the 3B model. For the PIQA\ndataset, TFix and FAST improve accuracy by approximately\n2% and 1%, respectively, for the 700M and 3B models at\n10% SAF rate. On the ARC (Easy) dataset, these techniques\nindividually yield an average accuracy improvement of ∼4%\nfor the 700M model and ∼2% for the 3B model at 10% SAF\nrate.\nFinally, we observe that ReTern enhances the fault tolerance\nthrough the integration of FAST and zero-fix. For the SAF rate\nof 10% (5%), ReTern furnishes an average Wikitext perplexity\nreduction of around 35% (10% ) on average for the 700M\nmodel and of 25% (8%) for the 3B model. Further, ReTern\nimproves accuracy of PIQA by around 4% and 2%, and of\nARC (easy) by 6% and 5%, for the 700M and 3B models,\nrespectively, at a 10% SAF rate. These results demonstrate that\n--- Page 8 ---\nReTern effectively combines the advantages of zero-fix and\nFAST, delivering the most significant accuracy improvements\nin the presence of SAFs for various ternary LLMs.\nC. Hardware Overhead\nWe evaluate the hardware implications of ReTern at the\nmemory macro level (TCiM array + peripheral circuits). The\noverall system-level impact may be lower when considering\nadditional components beyond the memory macro. In line with\nour accuracy analysis, we design 64×64 TCiM arrays based on\nthree memory bitcell technologies: 8T-SRAM, 1T-1ReRAM,\nand 1FeFET. The TCiM arrays employ current-sensing to\ngenerate array outputs. For the 8T-SRAM array design, we\nutilize the 7nm Predictive Technology Model (PTM) [34].\nFor the ReRAM, we employ an experimentally calibrated\ncompact model of Al-doped HfO XReRAM from [35]. For\nthe FeFET, we utilize a compact model of Hf0.5Zr0.5O2-\nbased FeFET in which the ferroelectric layer is modeled\nwith modified Preisach equations [36] and is coupled with\na 7nm transistor. This model is calibrated with experiments in\n[37] and validated using self-consistent phase-field simulations\n[38]. To accurately estimate the energy, latency, and area of\nthese arrays, we first develop custom layouts following the\nmethodology presented in [39]. Once the array dimensions\nare determined in terms of gate, metal, and fin pitches, we\nestimate parasitic wire resistances [40] and capacitances [41].\nSubsequently, these estimations are incorporated into SPICE\nsimulations to determine the energy, latency, and area (derived\ndirectly from custom layout dimensions) of the arrays.\nTo estimate the energy, latency, and area of the TCiM\narray peripherals—including the row decoder,flash ADC, and\nsubtractor—as well as the additional multiplexers (MUXes)\nand register required for ReTern, we use NeuroSim [42]. We\nagain utilize the 7nm predictive technology model (PTM) for\nthe peripherals and the post-processing circuitry [34].\nTo keep other TCiM array non-idealities like the effect\nof parasitic resistances and device non-linearities in check,\nwe utilize partial word-line activation (PWA), simultaneously\nactivating 16 rows (out of 64) at a time. PWA also reduces the\nrequired ADC precision, enabling us to use 4-bit flash ADCs,\nalbeit at the cost of latency. To further minimize ADC area\noverhead, we share one set of column peripherals (ADCs +\npost-processing circuitry) across eight columns, again trading\noff latency for improved area efficiency.\nTable I presents the overheads of ReTern relative to the\nbaseline (i.e. with no fault tolerant technique). As discussed\nin Section III-D, the primary source of overhead stems from\nFAST. Specifically, the additional hardware introduced by\nFAST in ReTern (illustrated in Fig. 5) consists of two 2:1\nmultiplexers per column to route the two TCiM array outputs\ninto the subtractor and a single register per TCiM array to\nstore the colflip bit (an overhead of 1 bit per column).\nOur evaluation shows that ReTern incurs a modest energy\noverhead of 2.0%-2.2% and a latency increase of 3.2%-6.6%\ncompared to the baseline. These overheads remain negligible\nin the broader context of CiM energy and latency, which areTable I: Hardware overhead of ReTern for different memory\ntechnologies\nEnergy Latency Area\nSRAM 2.0% 3.2% <1%\nReRAM 2.2% 6.6% <1%\nFeFET 2.2% 6.4% <1%\nprimarily dominated by ADCs and bitline switching. Further-\nmore, we estimate the area overhead of ReTern to be minimal,\nremaining below 1% across all three memory technologies.\nThis is again attributed to the dominant area contributions from\nADCs and memory arrays. Overall, ReTern offers significant\nSAF tolerance with only minor resource overhead, making it\na practical enhancement for ternary LLMs.\nV. C ONCLUSION\nIn this work, we investigate the SAF tolerance of ternary\nLLMs deployed on TCiM arrays. To mitigate the accuracy\ndegradation caused by SAFs, we propose ReTern, a training-\nfree technique that utilizes zero-fix and fault-aware sign trans-\nformation (FAST) to reduce errors due to SAFs. While zero-fix\ncorrects SAF-induced errors in zero weights by exploiting the\nnatural redundancy present in TCiM bitcells, FAST mitigates\nthe SAF-induced errors in non-zero weights, thus comple-\nmenting zero-fix and further curbing the accuracy degradation.\nWe evaluate ReTern on two BitNet 1.58B ternary LLMs,\nwith 700M and 3B parameters, deployed on TCiM hardware.\nOur experiments measure perplexity on the Wikitext dataset\nand accuracy on the PIQA and ARC (easy) benchmarks.\nReTern achieves up to a 35% reduction in perplexity and\naccuracy improvements of up to 4% on PIQA and 6% on ARC\n(easy). Additionally, we assess the hardware overheads of\nenabling ReTern at the memory macro level for 8T-SRAM, 1T-\n1ReRAM, and 1-FeFET TCiM arrays at the 7nm technology\nnode. ReTern incurs mild overheads, requiring only 2%–2.2%\nadditional energy, 3.2%–6.6% higher latency, and <1% area\noverhead compared to the baseline. Overall, ReTern is a low\noverhead training-free approach to enhance the SAF tolerance\nof TCiM accelerators. While we have evaluated ReTern in the\ncontext of ternary LLMs, it is also applicable, in principle, to\nother ternary precision models such as CNNs [22], [27] and\ntransformers [43] (which need to be evaluated). By targeting\nfaults in both zero and non-zero weights, ReTern can be\neffective for workloads displaying a range of weight sparsity.\nVI. A CKNOWLEDGEMENTS\nThis work is supported, in part, by the Center for the\nCo-Design of Cognitive Systems (COCOSYS), one of seven\ncenters in JUMP 2.0, funded by Semiconductor Research\nCorporation (SRC) and DARPA, Raytheon and NSF. The\nauthors would also like to thank Doug Hyun Kim for his\nhelpful suggestions.\n--- Page 9 ---\nREFERENCES\n[1] S. Samsi et al. , “From words to watts: Benchmarking the energy costs\nof large language model inference,” in 2023 IEEE High Performance\nExtreme Computing Conference (HPEC) , 2023, pp. 1–9.\n[2] Y . Wu, Z. Wang, and W. D. Lu, “Pim gpt a hybrid process in memory\naccelerator for autoregressive transformers,” npj Unconventional\nComputing , vol. 1, no. 1, p. 4, Jul 2024. [Online]. Available:\nhttps://doi.org/10.1038/s44335-024-00004-2\n[3] M. Zhou, W. Xu, J. Kang, and T. Rosing, “Transpim: A memory-\nbased acceleration via software-hardware co-design for transformer,” in\n2022 IEEE International Symposium on High-Performance Computer\nArchitecture (HPCA) , 2022, pp. 1071–1085.\n[4] S. Liu et al. , “Hardsea: Hybrid analog-reram clustering and digital-sram\nin-memory computing accelerator for dynamic sparse self-attention in\ntransformer,” IEEE Transactions on Very Large Scale Integration (VLSI)\nSystems , vol. 32, no. 2, pp. 269–282, 2024.\n[5] J. B ¨uchel et al. , “Efficient scaling of large language models with\nmixture of experts and 3d analog in-memory computing,” Nature\nComputational Science , vol. 5, no. 1, pp. 13–26, Jan 2025. [Online].\nAvailable: https://doi.org/10.1038/s43588-024-00753-x\n[6] Z. Liu et al. , “Paretoq: Scaling laws in extremely low-bit llm quantiza-\ntion,” arXiv preprint arXiv:2502.02631 , 2025.\n[7] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang,\nY . Wu, and F. Wei, “Bitnet: Scaling 1-bit transformers for large language\nmodels,” 2023. [Online]. Available: https://arxiv.org/abs/2310.11453\n[8] S. Ma et al. , “The era of 1-bit llms: All large language models are in\n1.58 bits,” 2024. [Online]. Available: https://arxiv.org/abs/2402.17764\n[9] H. Wang, S. Ma, and F. Wei, “Bitnet a4.8: 4-bit activations for 1-bit\nllms,” 2024. [Online]. Available: https://arxiv.org/abs/2411.04965\n[10] N. Thakuria, A. Malhotra, S. K. Thirumala, R. Elangovan,\nA. Raghunathan, and S. K. Gupta, “Site cim: Signed ternary\ncomputing-in-memory for ultra-low precision deep neural networks,”\n2024. [Online]. Available: https://arxiv.org/abs/2408.13617\n[11] S. Jain, S. K. Gupta, and A. Raghunathan, “Tim-dnn: Ternary in-memory\naccelerator for deep neural networks,” IEEE Transactions on Very Large\nScale Integration (VLSI) Systems , vol. 28, no. 7, pp. 1567–1577, 2020.\n[12] H. Jeong, S. Kim, K. Park, J. Jung, and K. J. Lee, “A ternary neural net-\nwork computing-in-memory processor with 16t1c bitcell architecture,”\nIEEE Transactions on Circuits and Systems II: Express Briefs , vol. 70,\nno. 5, pp. 1739–1743, 2023.\n[13] S. Cheon et al. , “A 2941-tops/w charge-domain 10t sram compute-in-\nmemory for ternary neural network,” IEEE Transactions on Circuits and\nSystems I: Regular Papers , vol. 70, no. 5, pp. 2085–2097, 2023.\n[14] M. Xiang et al. , “Drop-connect as a fault-tolerance approach for rram-\nbased deep neural network accelerators,” in 2024 IEEE 42nd VLSI Test\nSymposium (VTS) , 2024, pp. 1–7.\n[15] L. Xia, M. Liu, X. Ning, K. Chakrabarty, and Y . Wang, “Fault-tolerant\ntraining with on-line fault detection for rram-based neural computing\nsystems,” in 2017 54th ACM/EDAC/IEEE Design Automation Confer-\nence (DAC) , 2017, pp. 1–6.\n[16] Stanford University, “Artificial Intelligence Index Report 2024,” 2024,\naccessed: 2025-02-25. [Online]. Available: https://aiindex.stanford.edu/\nwp-content/uploads/2024/04/HAI 2024 AI-Index-Report.pdf\n[17] J. Zhang, C. Wang, Y . Cai, Z. Zhu, D. Kline, H. Yang, and Y . Wang,\n“Wesco: Weight-encoded reliability and security co-design for in-\nmemory computing systems,” in 2022 IEEE Computer Society Annual\nSymposium on VLSI (ISVLSI) , 2022, pp. 296–301.\n[18] H. Shin et al. , “Fault-free: A framework for analysis and mitigation of\nstuck-at-fault on realistic reram-based dnn accelerators,” IEEE Transac-\ntions on Computers , vol. 72, no. 7, pp. 2011–2024, 2023.\n[19] B. Zhang, N. Uysal, D. Fan, and R. Ewetz, “Handling stuck-at-\nfaults in memristor crossbar arrays using matrix transformations,” in\nProceedings of the 24th Asia and South Pacific Design Automation\nConference , ser. ASPDAC ’19. New York, NY , USA: Association\nfor Computing Machinery, 2019, p. 438–443. [Online]. Available:\nhttps://doi.org/10.1145/3287624.3287707\n[20] B. Li et al. , “Zero-space cost fault tolerance for transformer-\nbased language models on reram,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2401.11664\n[21] A. Malhotra, C. Wang, and S. K. Gupta, “Tfix: Exploiting the natural\nredundancy of ternary neural networks for fault tolerant in-memory vec-\ntor matrix multiplication,” in 2023 60th ACM/IEEE Design Automation\nConference (DAC) , 2023, pp. 1–6.[22] A. Marban et al. , “Learning sparse & ternary neural networks with\nentropy-constrained trained ternarization (ec2t),” in 2020 IEEE/CVF\nConf. on Computer Vision and Pattern Recognition Workshops , 2020.\n[23] S. Zhu et al. , “FAT: An in-memory accelerator with fast addition for\nternary weight neural networks,” IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems , 2022.\n[24] Q. Cao et al. , “Nonvolatile multistates memories for high-density data\nstorage,” ACS Appl. Mater. Interfaces , vol. 12, no. 38, pp. 42 449–42 471,\nSep. 2020.\n[25] L. Xia et al. , “Stuck-at fault tolerance in rram computing systems,” IEEE\nJournal on Emerging and Selected Topics in Circuits and Systems , vol. 8,\nno. 1, pp. 102–115, 2018.\n[26] C. Kim, D. Yoon, T. Kim, Y . Jeong, K. Kim, K. Koh, and E. Pak,\n“Analog computing for AI sometimes needs correction by digital\ncomputing: Why and when,” in NeurIPS 2024 Workshop Machine\nLearning with new Compute Paradigms , 2024. [Online]. Available:\nhttps://openreview.net/forum?id=GebgnS1TdT\n[27] Y . Li et al. , “Rtn: Reparameterized ternary network,” Proceedings of\nthe AAAI Conference on Artificial Intelligence , vol. 34, no. 04, pp.\n4780–4787, Apr. 2020. [Online]. Available: https://ojs.aaai.org/index.\nphp/AAAI/article/view/5912\n[28] C.-Y . Chen, H.-C. Shih, C.-W. Wu, C.-H. Lin, P.-F. Chiu, S.-S. Sheu,\nand F. T. Chen, “Rram defect modeling and failure analysis based on\nmarch test and a novel squeeze-search scheme,” IEEE Transactions on\nComputers , vol. 64, no. 1, pp. 180–190, 2015.\n[29] 1bitLLM, “Bitnet b1.58-large,” https://huggingface.co/1bitLLM/bitnet\nb158-large, accessed: 2025-03-07.\n[30] ——, “Bitnet b1.58-3b,” https://huggingface.co/1bitLLM/bitnet b1\n58-3B, accessed: 2025-03-07.\n[31] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning\nabout physical commonsense in natural language,” 2019. [Online].\nAvailable: https://arxiv.org/abs/1911.11641\n[32] P. Clark et al. , “Think you have solved question answering?\ntry arc, the ai2 reasoning challenge,” 2018. [Online]. Available:\nhttps://arxiv.org/abs/1803.05457\n[33] S. Jain et al. , “A heterogeneous and programmable compute-in-memory\naccelerator architecture for analog-ai using dense 2-d mesh,” IEEE\nTransactions on Very Large Scale Integration (VLSI) Systems , vol. 31,\nno. 1, pp. 114–127, 2023.\n[34] Accessed: August 2024. [Online]. Available: https://asap.asu.edu/\n[35] Z. Jiang et al. , “A compact model for metal–oxide resistive random\naccess memory with experiment verification,” IEEE Transactions on\nElectron Devices , vol. 63, no. 5, pp. 1884–1892, 2016.\n[36] A. K. Saha and S. K. Gupta, “Modeling and comparative analysis of\nhysteretic ferroelectric and anti-ferroelectric fets,” in 2018 76th Device\nResearch Conference (DRC) , 2018, pp. 1–2.\n[37] K. Ni et al. , “In-memory computing primitive for sensor data fusion\nin 28 nm hkmg fefet technology,” in 2018 IEEE International Electron\nDevices Meeting (IEDM) , 2018, pp. 16.1.1–16.1.4.\n[38] A. K. Saha et al. , “Ferroelectric thickness dependent domain interactions\nin fefets for memory and logic: A phase-field model based analysis,” in\n2020 IEEE International Electron Devices Meeting (IEDM) , 2020, pp.\n4.3.1–4.3.4.\n[39] C. Wang, J. Victor, and S. K. Gupta, “Comparative evaluation\nof memory technologies for synaptic crossbar arrays – part i:\nRobustness-driven device-circuit co-design and system implications,”\n2024. [Online]. Available: https://arxiv.org/abs/2307.04261\n[40] X. Chen et al. , “Modeling and circuit analysis of interconnects with\ntas2 barrier/liner,” in 2021 Device Research Conference (DRC) , 2021,\npp. 1–2.\n[41] J. H.-C. Chen et al. , “Interconnect performance and scaling strategy at\n7 nm node,” in IEEE International Interconnect Technology Conference ,\n2014, pp. 93–96.\n[42] X. Peng, S. Huang, Y . Luo, X. Sun, and S. Yu, “Dnn+neurosim: An end-\nto-end benchmarking framework for compute-in-memory accelerators\nwith versatile device technologies,” in 2019 IEEE International Electron\nDevices Meeting (IEDM) , 2019, pp. 32.5.1–32.5.4.\n[43] W. Zhang, L. Hou, Y . Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu,\n“TernaryBERT: Distillation-aware ultra-low bit BERT,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) , B. Webber, T. Cohn, Y . He, and Y . Liu, Eds.\nOnline: Association for Computational Linguistics, Nov. 2020, pp. 509–\n521. [Online]. Available: https://aclanthology.org/2020.emnlp-main.37/",
  "text_length": 47338
}