{
  "id": "http://arxiv.org/abs/2506.05243v1",
  "title": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection",
  "summary": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme.",
  "authors": [
    "Ron Eliav",
    "Arie Cattan",
    "Eran Hirsch",
    "Shahaf Bassan",
    "Elias Stengel-Eskin",
    "Mohit Bansal",
    "Ido Dagan"
  ],
  "published": "2025-06-05T17:02:52Z",
  "updated": "2025-06-05T17:02:52Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05243v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05243v1  [cs.CL]  5 Jun 2025CLATTER: Comprehensive Entailment Reasoning for Hallucination\nDetection\nRon Eliav1Arie Cattan1Eran Hirsch1Shahaf Bassan2\nElias Stengel-Eskin3Mohit Bansal3Ido Dagan1\n1Bar-Ilan University2Hebrew University of Jerusalem3UNC Chapel Hill\nroneliav1@gmail.com\nAbstract\nA common approach to hallucination detection\ncasts it as a natural language inference (NLI)\ntask, often using LLMs to classify whether the\ngenerated text is entailed by corresponding ref-\nerence texts. Since entailment classification is\na complex reasoning task, one would expect\nthat LLMs could benefit from generating an\nexplicit reasoning process, as in CoT reasoning\nor the explicit “thinking” of recent reasoning\nmodels. In this work, we propose that guiding\nsuch models to perform a systematic and com-\nprehensive reasoning process—one that both\ndecomposes the text into smaller facts and also\nfinds evidence in the source for each fact—\nallows models to execute much finer-grained\nand accurate entailment decisions, leading to\nincreased performance. To that end, we de-\nfine a 3-step reasoning process, consisting of (i)\nclaim decomposition, (ii) sub-claim attribution\nand entailment classification, and (iii) aggre-\ngated classification, showing that such guided\nreasoning indeed yields improved hallucination\ndetection. Following this reasoning framework,\nwe introduce an analysis scheme, consisting of\nseveral metrics that measure the quality of the\nintermediate reasoning steps, which provided\nadditional empirical evidence for the improved\nquality of our guided reasoning scheme.\n1 Introduction\nThe output of Large Language Models (LLMs)\nis often required to be faithful to some reference\ntexts. Such texts might be provided by the user,\nas in text summarization, retrieved sources, as in\nRAG settings, or retrieved references against which\nparametric-based generation is verified for factu-\nality. In such settings, a critical challenge is to\ndetect if the generated output contains unsupported\nclaims, known as hallucinations (Tian et al., 2020;\nThorat et al., 2025; Ádám Kovács and Recski, 2025;\nPaudel et al., 2025). Automated hallucination de-\ntection methods can inform users of suspected hal-\nlucinations (Leiser et al., 2024; Zhao et al., 2024),\nHave You Ever Thought About Where Your Donations Are Actually\nGoing? ... In reality, people’s good intentions are meant to help\ndeveloping countries progress ... This leads to the creation of\ndependency for both the people and the government of the poor\ncountry on donations ...Source\nCharity can be wrong when it is done with the wrong intentions or\nperpetuates dependency.Claim\n<think> Okay, let's tackle this problem step by step...\nThe claim has two main parts connected by \"or\":\n1. Charity can be wrong when it is done with the wrong intentions.\n2. Charity can be wrong when it perpetuates dependency...\nThe document says that \neven though their intentions were good, their donations created\ndependency ... but the claim says \"wrong intentions\" could make\ncharity wrong ... so maybe the first part isn't directly addressed..\nNow the second part ... The \ndocument states that donations can lead to dependency, which\nhinders self-sufficiency ... So this part is definitely supported. \nSo the first component might not be supported ...\nThe second component is supported ... require all parts to be\nsubstantiated. Since the first part isn't, the claim is inconsistent\n... Therefore, the answer is no. </think>\nLLM with CLATTER Reasoning:\nDecomposition\nAttribution & Entailment (sub-claim 1)\nAggregation\n... **Final decision:** ... the claim is **not supported**. Input\nReasoning\nOutputAttribution & Entailment (sub-claim 2)Figure 1: An example of CLATTER reasoning frame-\nwork to evaluate a claim. The process begins by decom-\nposing the claim into its two sub-claims. Each sub-claim\nis checked against the source via attribution and entail-\nment analysis. Finally, the results are aggregated to\nreach a not supported verdict for the overall claim.\ncorrect hallucinations by editing the output (Wad-\nhwa et al., 2024), or guide models to avoid halluci-\nnations through reinforcement learning (Roit et al.,\n2023) and controlled decoding (Wan et al., 2023).\nThe task of hallucination detection is mostly\nseen as an entailment classification task (Dagan\net al., 2005; Bowman et al., 2015), where the hy-\npothesis is a model-generated output claim while\nthe premise is the source text. Hallucination de-\n1\n--- Page 2 ---\nAttribution & Entailment\n sub-claim isupporting evidence\nno evidencerefuting evidenceSource\nClaimAggregation\n \n sub-claim.\n.\n.Entailment Attributioni\nIf: then:\nThe claim is \nsupportedelse:\nThe claim is \nnot supportedii iiisub-claim\nentailmentsDecompositionFigure 2: Overview of CLATTER process. (i) Decomposition: the original claim is split into individual sub-claims.\n(ii) Attribution & Entailment: each sub-claim is checked against the source for supporting evidence, refuting\nevidence, or no evidence. (iii) Aggregation: if all sub-claims are supported, the claim is accepted; otherwise, it is\nrejected.\ntection is then implemented using either fine-tuned\nentailment classifiers (Zha et al., 2023; Kamoi et al.,\n2023; Tang et al., 2024a), or via prompting LLMs\nto complete the entailment task (Kamoi et al., 2023;\nLaban et al., 2023; Min et al., 2023; Tang et al.,\n2024b). In our work we focus on the latter sce-\nnario, where LLMs are often preferred thanks to\ntheir broad domain and language coverage, robust-\nness and accessibility.\nSince entailment classification is a complex rea-\nsoning task, we expect that LLMs might benefit\nfrom generating an explicit reasoning process, as\nin CoT reasoning or the explicit “thinking” of re-\ncent reasoning models (Large Reasoning Models,\nor LRMs). Given such model-generated entail-\nment reasoning, two research questions arise: RQ1:\nHow well do models perform such reasoning on\ntheir own, in an un-guided manner? This question\nis posed with respect to both bottom line entailment\nclassification performance as well as the validity\nof the reasoning process itself. RQ2: Is it possible\nto improve such reasoning, by guiding models to\nperform systematic reasoning steps that follow the\ninherent semantics of entailment decision-making?\nToward addressing these questions, we first for-\nmulate a systematic and comprehensive reason-\ning process for entailment classification, which\nwe term CLATTER: Claim Localization &\nATT ribution for Entailment Reasoning. This pro-\ncess consists of three steps, namely (i) claim de-\ncomposition, (ii) sub-claim attribution and entail-\nment classification, and (iii) aggregated classifica-\ntion, as illustrated in Figures 1 and 2. Further, we\ndefine a set of metrics that measure the validity\nof the different steps involved in such entailment\nreasoning. While prior work also decomposes en-\ntailment reasoning based on sub-claims, to the best\nof our knowledge, we are the first to investigate\na principled decomposition of this sort as a sin-\ngle LLM reasoning process, as opposed to priorpipeline architectures, which often involve targeted\nfine-tuned models (Kamoi et al., 2023; Manakul\net al., 2023a; Min et al., 2023).\nOur experiments show that CLATTER-guided\nreasoning does improve bottom-line entailment\nclassification, relative to un-guided reasoning.\nImportantly, CLATTER-guided LRMs perform\nsub-claim attribution and entailment classification\nmuch more accurately, successfully following the\nprescribed reasoning steps.\nOverall, our contributions include: (1) intro-\nducing CLATTER as a comprehensive multi-step\nreasoning process for entailment classification by\nLLMs (Section 2); (2) defining assessment metrics\nfor the involved reasoning steps (Section 3); (3)\nanalyzing both unguided and CLATTER-guided\nreasoning, in both CoT and LRM settings, show-\ning the advantages of CLATTER reasoning in both\nentailment classification and reasoning quality.\nIn the following sections, we describe the CLAT-\nTER approach in detail (§2), present evaluation\nmetrics for the entailment reasoning steps (§3), de-\nscribe our experimental setup (§4), present our re-\nsults and ablations (§5), discuss insights from our\nmanual analysis (§6), and finally contrast with re-\nlated work (§7).\n2 Comprehensive NLI Reasoning\nIn the following section, we formulate the CLAT-\nTER reasoning process, which, in our setting,\nmodels are instructed to follow when making an\nentailment decision. We take the view that a\nnatural-language sentence can be presented as a\nconjunction of smaller facts (Davidson, 1967; Par-\ntee, 2008), all sharing a consistent interpretation,\nwhere the sentence is semantically equivalent to the\nunion of these facts. Then, a hypothesis is entailed\nif all its facts are entailed by the source, contra-\ndicted if at least one is contradicted, and neutral\n2\n--- Page 3 ---\notherwise. Consequently, for detecting a hallucina-\ntion in a given claim, we first decompose a claim\ninto sub-claims. Each sub-claim is then classified\nby checking for a corresponding piece of evidence\nin the source: entailed if supported, contradicted if\nopposed, and neutral if no match is found. Finally,\nwe aggregate the decisions of each sub-claim to\nprovide a prediction for the whole claim.\nWe propose guiding models to follow a sys-\ntematic process aligned with this perspective. As\nshown in Fig. 2, the entailment prediction of a gen-\nerated claim Hrelative to a source Pinvolves three\nsteps: (i) decomposition, (ii) attribution and entail-\nment classification, and (iii) aggregation. Through\nthe reasoning process, CLATTER provides a set\nof triples (hi, pi,ˆyi), where hiis a sub-claim, pi\nis the corresponding attribution in the source, and\nˆyidenotes the entailment status of hirelative to\nP. Finally, CLATTER aggregates all ˆyivalues and\nreturns a final prediction ˆyof either supported or\nnot supported . A detailed explanation of each step\nis provided below.\n(i) Decomposition: The first step in CLATTER\nprocess includes the decomposition of Hinto sub-\nclaims. A sub-claim hiis both entailed by Hand\nhas a verifiable truth value against the source P.\nFor a complete decomposition, the union of all\nthe sub-claims should be semantically equivalent\nto the full hypothesis. Formally,S\nihi=H. In\nFig. 1, the model decomposes the claim into two\nparts: “Charity can be wrong when it is done with\nthe wrong intentions” and“Charity can be wrong\nwhen it perpetuates dependency. ”\n(ii) Attribution & Entailment: In the second\nstep, the model looks for evidence and determines\nthe entailment for each sub-claim hi. (a) Attribu-\ntion: Search the source text for an evidence pi∈ P\nthat is either entailing (supporting) or contradict-\ning (refuting) the sub-claim. (b) Entailment : If\nsupporting or refuting evidence is found, classify\nthe sub-claim accordingly. Otherwise, classify it\nas neutral. In step ‘Attribution & Entailment (sub-\nclaim 2)’ in Figure 1, a supporting attribution is\nfound, leading to an entailment classification of\nthis sub-claim.\n(iii) Aggregation: In the final step, the model\naggregates the entailment labels of the sub-claims\nfollowing the logic: if all sub-claims are entailed ,\nthe claim is supported ; otherwise, the claim is not-\nsupported . For example, in Fig. 1, one sub-claimisneutral , therefore the claim is not-supported .\nOverall, these three steps combine the decom-\nposition of the full semantics of a claim into sub-\nclaims, the verification of the entailment of each\nsub-claim, and the aggregation of all decisions. All\nin one reasoning process. This flow makes CLAT-\nTER approach both comprehensive andsystematic .\nThe full instructions provided to the models are\nlisted in Appendix E.\n3 Evaluation Metrics for Entailment\nReasoning\nAs discussed in Section 1, two of our objectives\nare to analyze the innate reasoning produced by\nLRMs and the ability of LRMs to follow CLAT-\nTER instructions. Inspired by the components of\nthe CLATTER process, we propose to assess en-\ntailment reasoning steps by three corresponding\ncomponents (decomposition, attribution & entail-\nment, and aggregation). Additionally, in Section 6\nwe show that these metrics are instruction-agnostic\nand are relevant for instruction-free reasoning as\nwell as other reasoning for NLI. To compute the\nmetrics, we assume the ability to extract sub-claims,\nattribution, entailment labels, and the final decision\nfrom the model’s reasoning. As LRMs express\nreasoning in natural language, this extraction is\nnon-trivial. Instead of relying on potentially noisy\nautomated metrics, we opt to analyze and score\nthese metrics manually, thus ensuring the quality\nof our results.\nAtomicity. Following CLATTER, models are in-\nstructed to decompose a hypothesis into sub-claims\nduring reasoning. We define the atomicity metric\nto capture this behavior. Wanner et al. (2024) pro-\nposed to count the number of sub-claims produced\nby a decomposer as part of the decomposer eval-\nuation. Similarly, we suggest counting the num-\nber of distinct sub-claims H={h1, h2, . . . , h n}\ngenerated at the decomposition step. If no decom-\nposition occurs, Hcontains a single element. The\natomicity score is then defined as: Atomicity :=|H|.\nThis metric has no ground-truth value, but it can\ninfluence later steps. Low atomicity leads to longer\nand more complex sub-claims, making attribution\nand entailment classification harder. High atomic-\nity increases the risk of unfaithful or incomplete\ndecompositions.\nSoundness. As part of the decomposition step,\nwe assess whether the model, in its reasoning steps,\n3\n--- Page 4 ---\ngenerates sub-claims that are not semantically en-\ntailed by the claim. The soundness metric mea-\nsures the proportion of generated sub-claims that\nare consistent with the claim. The soundness score\nis defined as:\nSoundness :=1\n|H||H|X\ni=11{hiis sound } (1)\nIntuitively, a low soundness score suggests the\nmodel introduces extraneous or fabricated sub-\nclaims during decomposition, risking incorrect en-\ntailment judgments.\nCompleteness. For a complete view of the de-\ncomposition step, we evaluate whether the model\nrefers all the semantic content of the original claim.\nThe completeness metric checks if any part was\nomitted during decomposition. It is a binary value:\n1if all information is covered by the model’s sub-\nclaims, and 0if any is missing. The completeness\nscore is then defined as:\nCompleteness :=(\n1ifH ⊆S\nihi\n0otherwise(2)\nIntuitively, this metric highlights cases where the\nmodel omits parts of the claim—especially con-\ntradicting ones—potentially leading to incorrect\npredictions like falsely labeling it as entailed .\nSub-claim Attribution. The first phase in the\nsecond component of CLATTER is the attribution\nfor each sub-claim. The attribution metric assesses\nwhether the model correctly identifies supporting\nor contradicting evidence from the source for each\nsub-claim, when such evidence exists. An attribu-\ntion is correct if it can justify the entailment label\nof the sub-claim. Additionally, if the model does\nnot find any evidence in the source when no such\nevidence exists, the model receives a full score on\nthis sub-claim.\nAttribution :=1\n|H||H|X\ni=11{hiis correctly attributed }(3)\nIntuitively, incorrect or missing attribution can\ncause sub-claim misclassification, leading to an\nincorrect overall entailment decision.\nSub-claim Entailment Classification. The sec-\nond phase in ‘Attribution & Entailment’ step is to\ndetermine the entailment classification of each sub-\nclaim. The entailment metric evaluates whether themodel correctly predicts the entailment label for\neach sub-claim, comparing the predicted label ˆyi\nwith the gold yigiven by an oracle (or by a human\nevaluator).1The entailment metric is defined by:\nEntailment :=1\n|H||H|X\ni=11{ˆyi=yi} (4)\nIntuitively, misclassifying even onesub-claim can\nimpact the overall claim prediction, making this\nstep crucial for performance.\nAggregation. Finally, for the last step of CLAT-\nTER, we assess whether the model correctly aggre-\ngates sub-claim entailment predictions into a final\nglobal decision for the full claim. The aggregation\nmetric follows this logic: (i) If all sub-claims are\nentailed, the hypothesis is supported ; (ii) Other-\nwise, it is classified as not supported .\nLetˆyglobal be the model’s final prediction for\nthe whole claim, and let f(ˆy1, . . . , ˆy|H|)denote the\ncorrect aggregated label based on the sub-claim\npredictions. The aggregation metric is defined as:\nAggregation := 1{ˆyglobal=f(ˆy1,...,ˆy|H|)} (5)\nIntuitively, this binary metric is 1if the model’s\nglobal decision matches the logical aggregation of\nsub-claim labels, and 0otherwise. It captures cases\nwhere sub-claim entailment decisions are correct,\nbut the final decision misapplies the aggregation\nlogic.\n4 Experimental Setup\nIn this section, we describe the experimental setup\nfor hallucination detection, including the methods,\ndatasets, and models used. The complete prompt\ntemplates for all the following approaches are in-\ncluded in Appendix E. Experimental results and\nanalysis are presented in Section 5.\n4.1 Methods for NLI\nThis setup mainly includes the approaches to rea-\nsoning about entailment decisions. Our experiment\ncompares several approaches to reasoning for the\nentailment task. Therefore, all of these approaches\nare implemented as different reasoning processes\nfor LLMs.\n(1) As a baseline approach, we instruct the model\nto assess whether a given hypothesis is factually\n1For a binary classification, the neutral andcontradicted\nclasses may be grouped under a single not supported class.\n4\n--- Page 5 ---\nconsistent with a provided source, without any in-\nstruction on how to make this decision.\n(2)CLATTER : In our proposed approach, we di-\nrect the model to perform systematic and compre-\nhensive reasoning before the entailment decision,\nas detailed in Section 2 and presented in Fig. 2.\nIn addition, for a complete comparison, we add\na comparison of one more approach for the entail-\nment task:\n(3)QA-Based : Inspired by prior work using QA\npairs for semantic representation and faithfulness\nverification (He et al., 2015; Klein et al., 2022;\nCattan et al., 2024; Dhuliawala et al., 2024), we\ninstruct the model to generate questions from the\nhypothesis, answer them using both the hypothesis\nand the source, and assess entailment via answer\nequivalence. See Appendix A.2 for details.\n4.2 Datasets\nNumerous datasets have recently been developed\nfor the NLI task. In our study, we focus on three\nprominent domains: (1) Fact Verification , where a\nfactual claim is verified against a source; (2) Ques-\ntion Answering , where an answer is verified against\na set of retrieved passages; and (3) Summarization ,\nwhere the faithfulness of a summary is evaluated\nrelative to the source document.\nTo ensure specialization in hallucination detec-\ntion, we selected one dataset from each domain in\nwhich the statements to be evaluated are generated\nby LLMs. For the fact verification domain, we\nuse the ClaimVerify dataset (Liu et al., 2023). In\nthe question answering domain, we evaluate on the\nLFQA-Verification dataset (Chen et al., 2023). For\nsummarization, we use the TofuEval dataset (Tang\net al., 2024b) based on the MediaSum benchmark\n(Zhu et al., 2021). Further details on the subset we\nchose are presented in Appendix A. In our frame-\nwork, a model is given a source and a generated\nclaim, and should provide a prediction whether the\ngiven claim is faithful, relative to the source, or not\n(i.e., contains hallucination).\n4.3 Models\nWe conduct an extensive investigation on four\nLRMs, instructing them to follow CLAT-\nTER principles. The models evaluated\ninclude: QwQ-32B-Preview (Qwen, 2024),\nDeepSeek-R1 (Guo et al., 2025), O4-mini (Ope-\nnAI, 2025), and Gemini-2.5-Pro (Google,\n2025b).As a baseline, we also apply the same pro-\ncess to non-reasoning models—standard LLMs\nthat were not explicitly trained to generate in-\ntermediate reasoning before making predictions.\nThis allows us to compare the effectiveness\nof CLATTER across both model types and as-\nsess whether reasoning-trained models benefit\nmore from structured instruction than standard\nLLMs. For non-reasoning models, we evalu-\nateQwen-Plus (Alibaba, 2025), DeepSeek-V3\n(DeepSeek-AI, 2024), GPT-4o-mini (OpenAI,\n2025), and Gemini-2.0-Flash (Google, 2025a).\nWe also report results for the MiniCheck model to\nprovide a comparison with a state-of-the-art fine-\ntuned baseline.\n5 Results\nWe divide our results into two sections. The first is\na comparison between the baseline approach and\nCLATTER approach. The second is a comparison\nbetween the two instruction approaches suggested\nabove (Section 4: QA-based, and CLATTER). The\nresults for the former are presented in Section 5.1,\nand the latter results are presented in Appendix A.\nIn addition, we conduct an ablation study of each\ncomponent in the proposed comprehensive instruc-\ntion, which is detailed in Section 5.2.\n5.1 Entailment Classification Results\nTable 1 presents the results in terms of hallucination\ndetection accuracy of the baseline (non-instructed)\napproach versus CLATTER approach. We ob-\nserve a consistent performance improvement on\ntheClaimVerify andLFQA datasets across both\nstandard LLMs and reasoning models—except for\nGemini-2.5-Pro on the LFQA dataset, where per-\nformance did not improve. For the TofuEval\ndataset, results differ between model types. Stan-\ndard LLMs exhibit a performance drop relative to\nthe baseline, whereas reasoning models show a\nclear improvement under CLATTER. Overall, aver-\naged across all models and datasets, the average ac-\ncuracy gain using CLATTER over the instruction-\nfree baseline for the LRMs is 3.76 points. This\nindicates that instructing a model to make a compre-\nhensive and systematic reasoning for an entailment\ndecision improves the performance on NLI tasks.\nAdditionally, CLATTER improvement in LRMs is\ntwice as high as on standard LLMs. This suggests\nthat reasoning models, trained to better execute rea-\nsoning steps, are more capable of following our\n5\n--- Page 6 ---\nModel ClaimVerify LFQA TofuEval Avg\nBaseline CLATTER ∆ Baseline CLATTER ∆ Baseline CLATTER ∆ ∆FTMiniCheck 60.20 – – 55.60 – – 66.20 – – –LLMQwen-Plus 71.00 74.40 ↑3.40 79.60 81.00 ↑1.40 78.60 71.40 ↓7.20 ↓0.80\nDeepseek-V3 66.60 73.40 ↑6.80 80.60 84.00 ↑3.40 77.80 77.20 ↓0.60 ↑3.20\nGPT-4o-mini 71.40 73.80 ↑2.40 77.60 83.20 ↑5.60 79.00 78.00 ↓1.00 ↑2.33\nGemini-2.0 68.00 75.00 ↑7.00 78.20 80.60 ↑2.40 78.60 78.20 ↓0.40 ↑3.00LRMQwQ-32B-Preview 67.40 72.40 ↑5.00 79.80 82.40 ↑2.60 70.22 79.80 ↑9.58 ↑5.72\nDeepSeek-R1 69.60 75.60 ↑6.00 80.60 84.40 ↑3.80 71.23 77.00 ↑5.77 ↑5.19\nO4-mini 73.20 80.20 ↑7.00 85.80 86.80 ↑1.00 80.20 81.60 ↑1.40 ↑3.13\nGemini-2.5 73.40 76.20 ↑2.80 85.80 84.00 ↓1.80 78.40 80.40 ↑2.00 ↑1.00\nTable 1: Hallucination detection accuracy (%) results on the three hallucination detection datasets. Each cell shows\nthe baseline performance, CLATTER performance, and the delta. Delta values are colored: green for improvement,\nred for decline.\nstructured and comprehensive instructions.\nThe comparison between the two instruction-\nbased reasoning approaches (CLATTER and QA-\nbased) and the baseline is presented in Ap-\npendix Table 4. Both instruction-based methods\nlead to improved model performance, demonstrat-\ning that while self-reasoning capabilities in LRMs\nare valuable, explicitly guiding LRMs through a\nstructured and principled reasoning process may\nfurther enhance their effectiveness. Additional de-\ntails and insights can be found in Appendix A.2.\n5.2 Ablation Study\nWe perform an ablation study to evaluate the in-\ndividual contribution of each component in the\nCLATTER process. First, we assess the impact of\nthedecomposition step. In this setup, models are\ninstructed to break down the claim into sub-claims,\nclassify each as supported ornot supported , and\nthen infer whether the claim contains hallucinations\nbased on the sub-claim classifications.\nNext, we evaluate the effect of using 3-way\nentailment classification . In this setup, the not-\nsupported category is further split into neutral and\ncontradiction . Therefore, in the entailment deci-\nsion classification, the model is instructed to clas-\nsify each sub-claim in one of those three options.\nWe then test the impact of attribution component.\nIn this setup, the model is instructed to identify\nsupporting or contradicting evidence in the source\nfor each sub-claim, if such evidence exists. We\nevaluate the ablations across the three datasets us-\ning the eight models from the main experiments\nin §4. Due to computational cost, we sample 100\nexamples per dataset.\nIn Table 2, we present the average accuracy\nacross all eight models. The results indicate that\nthe decomposition instruction yields only marginalimprovements, and in some cases, even leads to\ndecreased performance. However, we observe that\nexplicitly distinguishing between neutral andcon-\ntradiction labels leads to an average improvement\nof nearly 1point in accuracy. We hypothesize that\nthe demand for fine-grained examination of the\nsource, particularly for the distinction between neu-\ntralandcontradiction , encourages the model to\nfocus on more nuanced details, leading to better\nperformance.\nAdditionally, as the last component of the abla-\ntion, when the instruction includes the attribution\nstep, performance consistently surpasses the base-\nline, with an average gain of 2.29points. Therefore,\nwe suggest that requiring models to support their\npredictions with explicit evidence leads to more\nsound decision-making and improved performance.\nOverall, the ablation findings highlight the value\nof the different components of CLATTER approach\nand the contribution of 3-way classification and\nattribution steps in CLATTER. The complete abla-\ntion results are provided in Table 5 in Appendix B.\n6 Human Analysis of Reasoning Quality\n6.1 Setup\nThe proposed evaluation metrics, as explained in\nSection 3, are instruction-agnostic; that is, they can\nbe used to evaluate entailment reasoning for any\ninstruction- and non-instruction-based reasoning\nprocess. Therefore, we also evaluate model rea-\nsoning quality under both the baseline and CLAT-\nTER approaches.2Since LRMs reasoning steps\nare expressed in natural language—and we did not\nconstrain the output to a specific format—we con-\nducted a manual analysis over 200 instances. Two\n2For adjusting to other instruction-based reasoning see\nAppendix D.\n6\n--- Page 7 ---\nMethod ClaimVerify LFQA TofuEval\nBaseline 71.00 82.62 68.75\n+ Decomposition 71.12 80.50 68.25\n+ 3-Way Classification 73.12 79.50 72.25\n+ Attribution 74.50 83.12 71.62\nTable 2: Average accuracy (%) across all models on each dataset after incrementally adding components of\nCLATTER framework.\nof the authors manually identified and evaluated the\nreasoning steps according to our proposed metrics.\nWe focus on two reasoning models,\nQwQ-32B-Preview and DeepSeek-R1 .3For\nthese models, we analyze reasoning behavior on\ntwo datasets: ClaimVerify andTofuEval . In this\nsetup, we randomly sampled 20 instances from\nClaimVerify andTofuEval datasets, and manually\nanalyzed model behavior across the Baseline and\nCLATTER settings mentioned above. The average\nresults over both datasets are presented in Table 3.\nSeparate results for ClaimVerify andTofuEval are\nin Appendix D in Table 6 and Table 7, respectively.\nAs a reference, we apply the few-shot learning\nsetting of DecompScore (Wanner et al., 2024)\nand manually analyze its outputs. The number\nof facts in DecompScore output serves as the\nestimated number of gold neo-Davidsonian atomic\nunits. Additional details on this evaluation are\nprovided in Appendix D. This result in a total of\n200 annotated examples.4\n6.2 Insights\nIn terms of atomicity , we find that even when\nmodels are not explicitly instructed to decompose\nthe hypothesis, they occasionally do so. Never-\ntheless, CLATTER approach consistently yields\nhigher atomicity compared to the baseline, indicat-\ning that models generate finer-grained sub-claims\nwhen guided by CLATTER. When comparing the\natomicity of CLATTER with DecompScore, we\nfind that there is much room for improvement in\nterms of the granularity of the decomposition. This\nmay be attributed to two factors: (1) CLATTER\ndecomposition is used as an intermediate step to-\nwards another goal, which may be less precise,\nand (2) the few-shot format employed in Decomp-\nScore improves decomposition quality. We leave\nthe atomicity improvement for future work.\n3O4-mini and Gemini-2.5-Pro are excluded, as their\nAPIs do not expose intermediate reasoning tokens.\n42 datasets ×20 instances ×( 2 LRMs ×( Baseline +\nCLATTER) +DecompScore) ) = 200 .As explained in Section 3, when the atomicity\nvalue is high, there is a risk of hallucinating or\nomitting information from the original claim. How-\never, with a low atomicity value, the sub-claims are\nlonger, require the attribution to be more extensive,\nand the entailment decision becomes complex.\nIn contrast, the soundness achieved using CLAT-\nTER is quantitatively similar to that achieved using\nthe baseline approach. Additionally, the complete-\nness of CLATTER is higher than that of the base-\nline approach, despite the increase in the atomic-\nityvalues of CLATTER. Regarding the attribution\nmetric—which does not distinguish between incor-\nrect and missing attributions—we observe that even\nin the baseline condition, models frequently pro-\nvide attribution during their reasoning. However,\nwhen explicitly instructed to do so, the attribution\nimproves substantially. This enhancement may rep-\nresent one of the key contributions of CLATTER,\nas further supported by the ablation results in Sec-\ntion 5.2. With respect to entailment , CLATTER\nimproves the entailment score by 5 to 9 points. This\nmight be the direct result of a better attribution step.\nFinally, for aggregation , models perform well, with\nperfect alignment between sub-claim classification\nand final claim prediction.\nIn the ablation setup (§5.2), we observe that\ndecomposition alone yields only limited perfor-\nmance improvement. Additionally, as mentioned\nearlier, higher atomicity facilitates easier attribu-\ntion. CLATTER, which achieves stronger perfor-\nmance, also scores highly on both atomicity and\nattribution . This suggests that the combination\nof decomposition and attribution steps during rea-\nsoning are key contributors to improving NLI per-\nformance through comprehensive and systematic\nreasoning.\n7 Related Work\nChain-of-Thought (CoT) and Long-CoT. Our\nwork treats hallucination detection in generated text\nas a reasoning task, guiding CoT reasoning (Wei\net al., 2022) to perform hallucination detection in\n7\n--- Page 8 ---\nMethod Model Decomposition Fact Attribution & EntailmentAggregation\nAtomicity Soundness Completeness Attribution Entailment\nBaseline DeepSeek-R1 1.55 0.97 0.90 0.72 0.85 1.00\nQwQ-32B-Preview 1.67 0.98 0.92 0.68 0.90 1.00\nCLATTER DeepSeek-R1 2.97 0.96 0.92 0.97 0.90 1.00\nQwQ-32B-Preview 2.95 0.98 0.95 0.98 0.99 1.00\nDecompscore QwQ-32B-Preview 4.47 0.98 0.95 – – –\nTable 3: LRMs Reasoning Analysis – Average across ClaimVerify and TofuEval Datasets (sampled subset). The\ncolumns present the metrics, categorized according to the three CLATTER components. The top rows show the\nresults for the baseline approach. The second section shows the results for CLATTER (our approach). The last row\npresents the Decompscore prompt values for the decomposition metrics.\nan NLI fashion via decomposition, attribution, and\naggregation. Specifically, we focus on long-CoT\nreasoning produced by Large Reasoning Models\n(LRMs), where the model is prompted to accom-\nplish multiple subtasks across a single long reason-\ning chain. This approach has proven useful in a va-\nriety of other domains that require decomposed and\nsymbolic reasoning, such as math and coding (Ope-\nnAI, 2024; DeepSeek-AI, 2024), with long CoTs\ngenerally following a search procedure for verifica-\ntion, decomposition, and backtracking (Marjanovi ´c\net al., 2025; Gandhi et al., 2025). Unlike past work\nthat has focused on applying LRMs and developing\nmetrics for evaluating reasoning steps (e.g. ground-\nedness and efficiency), largely for domains like\nmath or diagnostics (Lee and Hockenmaier, 2025;\nQiu et al., 2025; Chen et al., 2025) our work is\namong the first to explore long reasoning in halluci-\nnation detection, where we introduce both metrics\nand methods to guide and improve reasoning.\nHallucination Detection. Hallucinations—i.e.\noutputs that are either not faithful to the given\nsource or contain information not grounded in any\nknown input—occur across a wide range of gener-\native tasks, including summarization, question an-\nswering, general text generation, and vision tasks\n(Ji et al., 2023). Past work has addressed halluci-\nnation detection in a variety of settings (Shuster\net al., 2021; Manakul et al., 2023b; Bang et al.,\n2023; Min et al., 2023) and has included training\nmodels to detect hallucinations (Orgad et al., 2024;\nNiu et al., 2024; Mishra et al., 2024a) or to cor-\nrect detected hallucinations (Mishra et al., 2024b),\nand intervening on model representations to reduce\nhallucination (Liu et al., 2024).\nNLI Approaches. More closely related to our\nwork are efforts like WiCE (Kamoi et al., 2023)\nand FActScore (Min et al., 2023), and MolecularFacts (Gunjal and Durrett, 2024), which decom-\npose claims into sub-claims with a view to verify-\ning claim factuality. Our work differs from such\napproaches along several axes; first, unlike these\napproaches—which introduce decomposition meth-\nods as opposed to approaches to attribution—we\ngo a step further by instructing the model to also\nfind supporting or contradicting evidence for each\natomic sub-claim. Additionally, in contrast to that\nprior work, we adopt the three-way entailment clas-\nsification (entailed, contradicted, and neutral) and\nnot the ‘partial-correct’ class, which does not reveal\nthe real entailment status (either neutral or contra-\ndictory). Similarly, we treat aggregation differently\nfrom past work like WiCE, following a more logic-\nbased NLI definition, while past work averages\nacross claims. Moreover, past work has focused\non developing independent pieces of a verification\npipeline, i.e. decomposition, attribution/entailment,\nor aggregation modules. In contrast, we propose\na solution in which all these steps are performed\nwithin the model’s thinking step without the need\nof a special training for this task.\n8 Conclusion\nIn this work, we leverage the explicit reasoning\ncapabilities of LLMs, particularly Large Reasoning\nModels (LRMs), by providing them specific prin-\ncipled guidance on how to reason for entailment\nclassification. Proposing the CLATTER reasoning\nscheme, along with corresponding assessment met-\nrics, we show that such guidance indeed improves\nboth bottom-line entailment performance as well\nas reasoning quality. Future work may further in-\nvestigate principled entailment reasoning by large\nmodels for additional settings and data types, as\nwell as their potential utility for downstream tasks,\nlike revisions and editing, and for explaining and\njustifying entailment decisions to humans.\n8\n--- Page 9 ---\nLimitations\nWhile our work presents a structured approach for\nreasoning-based hallucination detection and intro-\nduces novel evaluation metrics, it has several limi-\ntations.\nFirst, our manual reasoning analysis was con-\nducted on a subset of datasets due to time con-\nstraints. Although it provides valuable insight into\nhow models reason with and without instruction,\na broader dataset-level evaluation would help to\ngeneralize these findings.\nSecond, CLATTER uses significantly more to-\nkens during inference. While this yields more in-\nterpretable and accurate decisions, it also increases\ncomputational cost. Future work may explore ways\nto balance reasoning depth with efficiency.\nEthical Considerations\nHallucination detection plays a key role in foster-\ning user trust in large language models (LLMs).\nWhile CLATTER improves hallucination detection\nperformance, it is important to acknowledge that\nit is not infallible. In particular, there are cases\nwhere the model incorrectly classifies a halluci-\nnated claim as supported by the source. This may\nlead users to place trust in outputs that contain fac-\ntual errors. As such, systems that integrate CLAT-\nTER method should be transparent about its limita-\ntions and avoid presenting outputs as unquestion-\nably reliable. Therefore, we encourage responsible\ndeployment that includes user-facing disclaimers.\nReferences\nAlibaba. 2025. Qwen-Plus. Model ID: Qwen-Plus.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. ArXiv , abs/2302.04023.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nInProceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nKay Henning Brodersen, Cheng Soon Ong, Klaas Enno\nStephan, and Joachim M. Buhmann. 2010. The bal-\nanced accuracy and its posterior distribution. In 2010\n20th International Conference on Pattern Recogni-\ntion, pages 3121–3124.Arie Cattan, Paul Roit, Shiyue Zhang, David Wan, Roee\nAharoni, Idan Szpektor, Mohit Bansal, and Ido Da-\ngan. 2024. Localizing factual inconsistencies in at-\ntributable text generation. ArXiv , abs/2410.07473.\nHung-Ting Chen, Fangyuan Xu, Shane A Arora, and\nEunsol Choi. 2023. Understanding retrieval aug-\nmentation for long-form question answering. arXiv\npreprint arXiv:2310.12150 .\nJiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang,\nXiaodan Liang, Zhaopeng Tu, Xiaolong Li, and\nKwan-Yee K. Wong. 2025. Spc: Evolving self-\nplay critic via adversarial games for llm reasoning.\nPreprint , arXiv:2504.19162.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classification,\nand Recognising Tectual Entailment , pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nDonald Davidson. 1967. The logical form of action\nsentences. Essays on actions and events , pages 105–\n148.\nDeepSeek-AI. 2024. DeepSeek-V3 Technical Re-\nport. arXiv:2412.19437v1 [cs.CL]. Preprint ,\narXiv:2412.19437. Model ID: DeepSeek-V3.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2024. Chain-of-verification reduces\nhallucination in large language models. In Findings\nof the Association for Computational Linguistics:\nACL 2024 , pages 3563–3578, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 3558–3567, Florence,\nItaly. Association for Computational Linguistics.\nKanishk Gandhi, Ayush Chakravarthy, Anikait Singh,\nNathan Lile, and Noah D Goodman. 2025. Cognitive\nbehaviors that enable self-improving reasoners, or,\nfour habits of highly effective stars. arXiv preprint\narXiv:2503.01307 .\nGoogle. 2025a. Gemini 2.0 Flash. Model ID: gemini-\n2.0-flash-001.\nGoogle. 2025b. Gemini 2.5 Pro. Model ID: gemini-2.5-\npro-preview-03-25.\nAnisha Gunjal and Greg Durrett. 2024. Molecular facts:\nDesiderata for decontextualization in LLM fact veri-\nfication. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2024 , pages 3751–3768,\nMiami, Florida, USA. Association for Computational\nLinguistics.\n9\n--- Page 10 ---\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948 .\nModel ID: DeepSeek-R1.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language.\nInProceedings of the 2015 conference on empiri-\ncal methods in natural language processing , pages\n643–653.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hal-\nlucination in natural language generation. ACM com-\nputing surveys , 55(12):1–38.\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and\nGreg Durrett. 2023. Wice: Real-world entailment for\nclaims in wikipedia. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 7561–7583.\nAyal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin,\nAvi Caciularu, and Ido Dagan. 2022. Qasem pars-\ning: Text-to-text modeling of qa-based semantics.\nInProceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n7742–7756.\nPhilippe Laban, Wojciech Kryscinski, Divyansh Agar-\nwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty,\nand Chien-Sheng Wu. 2023. SummEdits: Measuring\nLLM ability at factual reasoning through the lens\nof summarization. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 9662–9676, Singapore. Associa-\ntion for Computational Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics , 10:163–177.\nJinu Lee and Julia Hockenmaier. 2025. Evaluating\nstep-by-step reasoning traces: A survey. Preprint ,\narXiv:2502.12289.\nFlorian Leiser, Sven Eckhardt, Valentin Leuthe, Merlin\nKnaeble, Alexander Mädche, Gerhard Schwabe, and\nAli Sunyaev. 2024. Hill: A hallucination identifier\nfor large language models. In Proceedings of the\n2024 CHI Conference on Human Factors in Comput-\ning Systems , CHI ’24, New York, NY , USA. Associa-\ntion for Computing Machinery.\nNelson Liu, Tianyi Zhang, and Percy Liang. 2023. Eval-\nuating verifiability in generative search engines. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023 , pages 7001–7025, Singapore.\nAssociation for Computational Linguistics.Sheng Liu, Haotian Ye, Lei Xing, and James Y .\nZou. 2024. Reducing hallucinations in vision-\nlanguage models via latent space steering. ArXiv ,\nabs/2410.15778.\nPotsawee Manakul, Adian Liusie, and Mark Gales.\n2023a. SelfCheckGPT: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 9004–9017, Singapore. Association for Com-\nputational Linguistics.\nPotsawee Manakul, Adian Liusie, and Mark John Fran-\ncis Gales. 2023b. Selfcheckgpt: Zero-resource black-\nbox hallucination detection for generative large lan-\nguage models. ArXiv , abs/2303.08896.\nSara Vera Marjanovi ´c, Arkil Patel, Vaibhav Adlakha,\nMilad Aghajohari, Parishad BehnamGhader, Mehar\nBhatia, Aditi Khandelwal, Austin Kraft, Benno Kro-\njer, Xing Han Lù, et al. 2025. Deepseek-r1 thoughtol-\nogy: Let’s< think> about llm reasoning. arXiv\npreprint arXiv:2504.07128 .\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\nHannaneh Hajishirzi. 2024a. Fine-grained hallucina-\ntion detection and editing for language models. In\nFirst Conference on Language Modeling .\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\nHannaneh Hajishirzi. 2024b. Fine-grained hallucina-\ntion detection and editing for language models. In\nFirst Conference on Language Modeling .\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun\nShum, Randy Zhong, Juntong Song, and Tong Zhang.\n2024. Ragtruth: A hallucination corpus for develop-\ning trustworthy retrieval-augmented language models.\nInProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 10862–10878.\nOpenAI. 2024. Learning to reason with LLMs.\nOpenAI. 2025. GPT-4o. Model ID: gpt-4o-mini-2024-\n07-18.\nOpenAI. 2025. Introducing openai o3 and o4-mini.\nModel ID: o4-mini-2025-04-16.\nHadas Orgad, Michael Toker, Zorik Gekhman, Roi Re-\nichart, Idan Szpektor, Hadas Kotek, and Yonatan\nBelinkov. 2024. Llms know more than they show:\nOn the intrinsic representation of llm hallucinations.\nArXiv , abs/2410.02707.\n10\n--- Page 11 ---\nBarbara H Partee. 2008. Compositionality in formal\nsemantics: Selected papers . John Wiley & Sons.\nBibek Paudel, Alexander Lyzhov, Preetam Joshi, and\nPuneet Anand. 2025. Hallucinot: Hallucination de-\ntection through context and common knowledge ver-\nification. Preprint , arXiv:2504.07069.\nPengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike\nZhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng,\nYa Zhang, Yanfeng Wang, and Weidi Xie. 2025.\nQuantifying the reasoning abilities of llms on real-\nworld clinical cases. Preprint , arXiv:2503.04691.\nQwen. 2024. Qwq: Reflect deeply on the boundaries of\nthe unknown. Model ID: qwq-32b-preview.\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Ge-\noffrey Cideron, Robert Dadashi, Matthieu Geist, Ser-\ntan Girgin, Leonard Hussenot, Orgad Keller, Nikola\nMomchev, Sabela Ramos Garea, Piotr Stanczyk,\nNino Vieillard, Olivier Bachem, Gal Elidan, Avinatan\nHassidim, Olivier Pietquin, and Idan Szpektor. 2023.\nFactually consistent summarization via reinforce-\nment learning with textual entailment feedback. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 6252–6272, Toronto, Canada.\nAssociation for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation re-\nduces hallucination in conversation. In Conference\non Empirical Methods in Natural Language Process-\ning.\nLiyan Tang, Philippe Laban, and Greg Durrett. 2024a.\nMiniCheck: Efficient fact-checking of LLMs on\ngrounding documents. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 8818–8847, Miami, Florida,\nUSA. Association for Computational Linguistics.\nLiyan Tang, Igor Shalyminov, Amy Wong, Jon Burnsky,\nJake Vincent, Yu’an Yang, Siffi Singh, Song Feng,\nHwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab\nMansour, and Kathleen McKeown. 2024b. TofuEval:\nEvaluating hallucinations of LLMs on topic-focused\ndialogue summarization. In Proceedings of the 2024\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers) ,\npages 4455–4480, Mexico City, Mexico. Association\nfor Computational Linguistics.\nOnkar Thorat, Philippe Laban, and Chien-Sheng\nWu. 2025. Summexecedit: A factual consistency\nbenchmark in summarization with executable edits.\nPreprint , arXiv:2412.13378.\nRan Tian, Shashi Narayan, Thibault Sellam, and\nAnkur P. Parikh. 2020. Sticking to the facts: Con-\nfident decoding for faithful data-to-text generation.\nPreprint , arXiv:1910.08684.Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, and Greg\nDurrett. 2024. Learning to refine with fine-grained\nnatural language feedback. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2024 ,\npages 12281–12308, Miami, Florida, USA. Associa-\ntion for Computational Linguistics.\nDavid Wan, Mengwen Liu, Kathleen McKeown,\nMarkus Dreyer, and Mohit Bansal. 2023.\nFaithfulness-aware decoding strategies for ab-\nstractive summarization. In Proceedings of the\n17th Conference of the European Chapter of the\nAssociation for Computational Linguistics , pages\n2864–2880, Dubrovnik, Croatia. Association for\nComputational Linguistics.\nMiriam Wanner, Seth Ebner, Zhengping Jiang, Mark\nDredze, and Benjamin Van Durme. 2024. A closer\nlook at claim decomposition. In Proceedings of the\n13th Joint Conference on Lexical and Computational\nSemantics (*SEM 2024) , pages 153–175, Mexico\nCity, Mexico. Association for Computational Lin-\nguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems , 35:24824–24837.\nYuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.\n2023. AlignScore: Evaluating factual consistency\nwith a unified alignment function. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 11328–11348, Toronto, Canada. Association\nfor Computational Linguistics.\nLingjun Zhao, Nguyen X. Khanh, and Hal Daumé III.\n2024. Successfully guiding humans with imperfect\ninstructions by highlighting potential errors and sug-\ngesting corrections. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 719–736, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. MediaSum: A large-scale media interview\ndataset for dialogue summarization. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 5927–5934,\nOnline. Association for Computational Linguistics.\nÁdám Kovács and Gábor Recski. 2025. Lettucedetect:\nA hallucination detection framework for rag applica-\ntions. Preprint , arXiv:2502.17125.\n11\n--- Page 12 ---\nThe following appendix is structured as follows:\n•Appendix A contains supplementary details\nand results on the NLI experiments.\n•Appendix B contains additional ablation re-\nsults.\n•Appendix C contains additional details re-\ngarding the use of the evaluation metrics for\nQA-based instructions.\n•Appendix D contains additional experimental\nanalysis, including both decomposition and a\nmanual analysis.\n•Appendix E contains the prompts used within\nour experiments.\nA NLI Experiments\nThis section presents additional supplementary de-\ntails and results related to the NLI experiments.\nSubsection A.1 offers further information about the\ndatasets used, while Subsection A.2 compares our\napproach with the QA-based method.\nA.1 Datasets\nWe evaluate CLATTER process for hallucination\ndetection using datasets from the Natural Language\nInference (NLI) task, where each instance includes:\n(1) a premise — a reliable source document, (2)\nahypothesis — a text segment generated by a\nlarge language model, and (3) a label - indicating\nwhether the hypothesis is supported by the premise.\nClaimVerify. For the fact verification domain,\nwe use the ClaimVerify dataset (Liu et al., 2023).\nClaimVerify assesses the factual accuracy of re-\nsponses from four generative search engines in\nanswering user queries. Each instance includes\na sentence from a generated response and its as-\nsociated source document, annotated to indicate\nwhether the sentence is fully supported by the cited\nsource. We selected this dataset due to its diversity:\nit contains generations from four different mod-\nels, might capturing a wide range of behaviors and\nhallucinations.\nLFQA-Verification. In the question answering\ndomain, we evaluate on the LFQA-Verification\ndataset (Chen et al., 2023). LFQA-Verification\nconsists of responses generated by LLMs to ques-\ntions from the ELI5 dataset (Fan et al., 2019). The\nmodels generate responses based on documentsretrieved either by humans, retrieval models, or\nselected at random. Human annotators label each\nsentence in the generated responses as supported ,\npartially supported , ornot supported . For consis-\ntency across datasets, our experiment combines the\npartially supported and not supported labels into a\nsingle not supported label.\nTofuEval. For summarization, we use the TofuE-\nval dataset (Tang et al., 2024b) based on the Me-\ndiaSum benchmark (Zhu et al., 2021). TofuEval\ntargets factual consistency in dialogue summariza-\ntion, focusing on interview transcripts from Media-\nSum. It includes topic-focused summaries gener-\nated by six different LLMs, with sentence-level fac-\ntual consistency annotations provided by linguists.\nThe dataset’s coverage across multiple models con-\ntributes valuable diversity to the evaluation.\nThe datasets described above contain thousands\nof samples. Due to the high computational cost\nof running inference on LRMs, we sample 500\ninstances from each dataset (sample IDs will be re-\nleased upon acceptance). Since many prior works\nreport only the balanced accuracy (Brodersen et al.,\n2010), a metric that adjusts class imbalance, for the\nhallucination detection task (Laban et al., 2022;\nTang et al., 2024a,b; Paudel et al., 2025), we\nadopt a balanced sampling strategy. Specifically,\nwe randomly sample 250 supported and 250 not-\nsupported instances from each dataset. All the\ndatasets have been imported via LLM-AggreFact\ncollection, available on HuggingFace (Tang et al.,\n2024a)\nBinary Classification. Most recent hallucination-\ndetection datasets adopt a binary classification\nsetup, labeling each claim as either supported or\nnot supported . This mirrors real-world applica-\ntions, where users are typically concerned with\nwhether to trust a model’s output. Therefore, in\nthis work, we also focus on binary hallucination\nclassification: determining whether a generated\ntext (i.e., a claim) contains hallucinations, without\ndistinguishing whether the hallucination is either\na ‘contradiction’ or ‘neutral’ relative to the source.\nHowever, since CLATTER framework does sup-\nport fine-grained distinctions between contradic-\ntion and neutrality, it may offer additional benefits\nfor other downstream applications. We leave this\nexploration for future work.\n12\n--- Page 13 ---\nModel ClaimVerify LFQA TofuEval\nBaseline QAs CLATTER Baseline QAs CLATTER Baseline QAs CLATTER\nMiniCheck 60.20 – – 55.60 – – 66.20 – –\nQwen-Plus 71.00 73.20 74.40 79.60 78.80 81.00 78.60 76.20 71.40\nDeepSeek-V3 66.60 69.80 73.40 80.60 82.60 84.00 77.80 77.60 77.20\nGPT-4o-mini 71.40 65.00 73.80 77.60 75.00 83.20 79.00 65.80 78.00\nGemini-2.0-Flash 68.00 69.80 75.00 78.20 80.60 80.60 78.60 78.40 78.20\nQwQ-32B-Preview 67.40 71.80 72.40 79.80 81.40 82.40 70.22 78.60 79.80\nDeepSeek-R1 69.60 70.40 75.60 80.60 80.40 84.40 71.23 72.60 77.00\nO4-mini 73.20 74.00 80.20 85.80 86.20 86.80 80.20 81.20 81.60\nGemini-2.5-Pro 73.40 75.60 76.20 85.80 87.00 84.00 78.40 80.20 80.40\nAverage (LRMs) 70.90 72.95 76.10 83.00 83.75 84.40 75.01 78.15 79.70\nTable 4: Comparison of performance across three datasets for various models using different reasoning strategies.\nEach cell shows accuracy (%); the best value per row is bolded.\nA.2 NLI Methods Comparison\nWe conducted a comparison of two instruction-\nbased reasoning approaches: QA-based approach,\nand CLATTER approach. CLATTER is descrin\ndetails in Section 2. In the QA-based approach,\nwe instruct the model to first generate questions\non the claim. Then, the model is guided to an-\nswer the questions based on the claim and based\non the source, separately. Finally, the model is in-\nstructed to compare the answers and consequently\ndecide on the final decision of the claim. That is,\nif a claim’s answer is not equivalent to a source’s\nanswer, the information from the source that is rep-\nresented by this question-and-answer is not faithful\nto the source. The full prompts are presented in\nAppendix E.\nThe results for each approach, along with the\nbaseline results, are presented in Table 4. The com-\nparison was conducted across all eight models, with\nthe full results shown in Table 4. However, given\nthat the primary focus of this paper is on LRMs,\nthe following analysis will emphasize results from\nLRMs specifically. We find that CLATTER ap-\nproach achieves the highest average performance\non the ClaimVerify andTofuEval datasets, and\nLFQA dataset, with an overall average accuracy\nof80.7%. The QA-based method ranks second\nacross all three datasets, with an overall average\naccuracy of 78.28%. The baseline approach per-\nforms the worst in all datasets, with an average ac-\ncuracy of 76.3%. These findings indicate that while\nself-reasoning capabilities in LRMs are beneficial,\nexplicitly guiding LRMs to reason in a structured\nand principled manner may further enhance their\nperformance.B Additional Ablation Results\nThis section presents additional ablation results\nthat were not presented from the main paper due\nto space limitations. The full ablation results for\nCLATTER process across all eight models are pre-\nsented in Table 5.\nOne notable observation is that the decomposi-\ntion step on its own often leads to a decrease in\nperformance. This is likely because LLMs are not\nexplicitly trained to perform atomic-level decom-\nposition, and prompting them to do so may lead\nto confusion or misinterpretation of the task. In\ncontrast, we find that distinguishing between the\nContradiction andNeutral classes improves perfor-\nmance in half of the models evaluated. Similarly,\nthe attribution step also improves the performance\nin half of the cases. These findings suggest that\nthe comprehensiveness of CLATTER—particularly\nthe inclusion of fine-grained 3-way entailment clas-\nsification and attribution—contributes positively to\nthe quality of reasoning in the entailment task.\nC Using Metrics for QA-based\nInstructions\nIn Section 3, we argue that our proposed evalua-\ntion metrics are instruction-agnostic, i.e., they can\nevaluate reasoning for NLI regardless of the rea-\nsoning process followed. For both CLATTER flow\nand instruction-free reasoning, we explain in the\npaper how to apply these metrics. However, apply-\ning those metrics to QA-based instructions requires\nsome clarification.\nIn the QA-based setting, the model is instructed\nto generate questions based on the claim, answer\nthem using the claim itself, and then answer them\n13\n--- Page 14 ---\nagain using the source document. The model then\ncompares these two sets of answers to assess the\ncorrectness of each sub-claim and, by extension,\nthe entire claim.\nThe proposed metrics can be naturally adapted\nto this process as follows: the generated questions\ncorrespond to the decomposition step; the model’s\nanswers from the source act as the attribution; the\ncomparison between claim-based and source-based\nanswers serves as the entailment classification; and\nthe final judgment, whether all answers align, con-\nstitutes the aggregation step.\nD Additional Experimental Analysis\nThis section presents additional experimental analy-\nsis, including the decomposition-based experiment\n(Subsection D.1) and further manual analysis (Sub-\nsection D.2).\nD.1 Decomposition\nIn addition to the analysis on baseline and CLAT-\nTER approaches, we wanted to compare the atom-\nicity values with the number of ’gold’ atomic-\nity. However, since it’s time-consuming, we\ndid the same as (Wanner et al., 2024) and\nprompted a model, with a few-shot examples\nfor neo-Davidsonian samples to provide a new-\nDavidsonian decomposition. We believe that since\nthis is the only task of this prompt, compared to\nCLATTER, the output should be much closer to the\ngold neo-Davidsonian decomposition. For this, we\nused the QwQ-32B-Preview model and instructed\nhim to do the decomposition. Then, we manually\nevaluate its output on the atomicity ,soundness , and\ncompleteness . However, the main comparison here\nis for the atomicity compared to the atomicity of\nthe NLI instructions.\nD.2 Manual Analysis\nThe manual analysis results for ClaimVerify are\nTable 6. The manual analysis results for TofuEval\nare Table 7.\nE Description of Prompts\nThis section contains the prompts used within our\nexperiments. Particularly, (i) Subsection E.1 con-\ntains the hallucination detection prompts, (ii) Sub-\nsection E.2 contains the decomposition prompts,\n(iii) Subsection E.3 contains the co-reference\nprompts, and (iv) Subsection E.4 contains the abla-\ntion prompts.E.1 Hallucination Detection Prompts\nWe present here the prompts used for the halluci-\nnation detection task. To ensure consistency with\nprior work, we adopt the baseline prompt from\nTang et al. (2024a), as presented in Prompt 1.1. For\nthe<specific instructions for each method> , there\nis a variant for each instruction approach. For the\nbaseline approach, it is left empty.\nFor standard LLMs, we augment the prompt\nwith chain-of-thought (CoT) reasoning (Wei et al.,\n2022) by inserting the phrase “think step by\nstep” as the <instruction for chain of thought> .\nThe decomposition-based prompt and QA-based\nprompt variants for the <specific instructions for\neach method> are included in Prompts 1.2 and 1.3,\nrespectively. The instructions version for CLAT-\nTER is shown in Prompt 1.4, while an example\nof Davidsonian-inspired decomposition appears in\nPrompt 1.5. Prompt.\nE.2 Decomposition\nWe note that although we instruct the model to\ndecompose the hypothesis into atomic facts, our\ngoal was not to optimize decomposition quality,\nand in practice, the models do not always succeed\nin producing atomic facts. Therefore, we refer\nto this step as a decomposition into smaller sub-\nclaims, rather than strictly atomic ones.\nE.3 Co-Reference Between Atomic Facts\nGunjal and Durrett (2024) highlight that decom-\nposing a text segment into atomic facts may not\nbe sufficient for detecting hallucinations. One key\nreason is that contradictions can arise not from indi-\nvidual facts themselves, but from their co-reference .\nThat is, two atomic facts may each be individu-\nally entailed by the premise, yet their combination,\nthrough shared referents, can result in a contradic-\ntion.\nFor example, consider the premise: “Ann Jans-\nson is a Swedish former footballer. Another Ann\nJansson, a racewalking athlete, won a medal at\nthe European Athletics Championships. ” Now con-\nsider the hypothesis: “Ann Jansson is a Swedish\nformer footballer who won the European Athletics\nChampionships. ” . When decomposed, the hypoth-\nesis yields two sub-facts: (1) “Ann Jansson is a\nSwedish former footballer” and (2) “Ann Jansson\nwon a medal at the European Athletics Champi-\nonships. ” . Both sub-facts are individually entailed\nby the premise. However, the co-reference between\n14\n--- Page 15 ---\nthe two distinct individuals named “Ann Jansson”\nintroduces a contradiction relative to the premise.\nTo address this, we instructed the model to also\nevaluate whether co-reference across sub-facts in-\ntroduces a contradiction. In the manual analysis,\nwe found that while models were capable of ex-\necuting this step, they never identified an actual\ncontradiction arising from co-reference. Therefore,\nwe did not explicitly incorporate this property into\nthe main evaluation framework presented in the\npaper.\nE.4 Ablation Prompts\nFor the ablations, which are described in Section\n5.2, the baseline approach uses Prompt 1.1. The\nprompt for the decomposition approach, which is\ninspired by Davidsonian semantics, is Prompt 2.1.\nFor the 3-way approach, we instruct the model\naccording to Prompt 2.2. The instruction for the\nattribution approach is the same as Prompt 1.4.\n15\n--- Page 16 ---\nModel Method ClaimVerify LFQA TofuEval\nQwen-PlusBaseline 68.00 83.00 66.00\n+ Decomposition 67.00 81.00 61.00\n+ 3 way 77.00 76.00 74.00\n+ Attribution 74.00 86.00 65.00\nDeepSeek-V3Baseline 70.00 83.00 69.00\n+ Decomposition 72.00 83.00 70.00\n+ 3 way 74.00 83.00 69.00\n+ Attribution 77.00 86.00 70.00\nGPT-4o-miniBaseline 70.00 84.00 71.00\n+ Decomposition 68.00 75.00 65.00\n+ 3 way 66.00 72.00 66.00\n+ Attribution 73.00 81.00 66.00\nGemini-2.0-FlashBaseline 71.00 84.00 66.00\n+ Decomposition 70.00 76.00 68.00\n+ 3 way 70.00 78.00 78.00\n+ Attribution 75.00 81.00 78.00\nQwQ-32B-PreviewBaseline 70.00 80.00 68.00\n+ Decomposition 73.00 85.00 72.00\n+ 3 way 74.00 79.00 76.00\n+ Attribution 73.00 83.00 70.00\nDeepSeek-R1Baseline 71.00 80.00 69.00\n+ Decomposition 74.00 80.00 73.00\n+ 3 way 76.00 80.00 72.00\n+ Attribution 73.00 77.00 73.00\nO4-miniBaseline 74.00 84.00 71.00\n+ Decomposition 72.00 86.00 70.00\n+ 3 way 74.00 87.00 71.00\n+ Attribution 75.00 87.00 71.00\nGemini-2.5-ProBaseline 74.00 83.00 70.00\n+ Decomposition 73.00 78.00 67.00\n+ 3 way 74.00 81.00 72.00\n+ Attribution 76.00 84.00 80.00\nTable 5: Full ablation results across all models. We randomly sampled 100 instances from each dataset.\nMethod Model Atomicity Soundness Completeness Entailment Attribution Aggregation\nBaseline DeepSeek-R1 1.55 0.97 0.95 0.95 0.72 1.00\nQwQ-32B-Preview 1.75 1.00 0.90 0.92 0.82 1.00\nCLATTER DeepSeek-R1 2.65 0.97 0.95 0.87 0.95 1.00\nQwQ-32B-Preview 2.85 0.98 1.00 0.99 1.0 1.00\nDecompscore QwQ-32B-Preview 4.30 0.98 1.00 – – –\nTable 6: Reasoning Analysis – ClaimVerify Dataset (sampled subset)\n16\n--- Page 17 ---\nMethod Model Atomicity Soundness Completeness Entailment Accuracy Attribution Aggregation\nBaseline DeepSeek-R1 1.55 0.97 0.85 0.75 0.73 1.00\nQwQ-32B-Preview 1.60 0.97 0.95 0.88 0.55 1.00\nCLATTER DeepSeek-R1 3.30 0.96 0.90 0.93 1.00 1.00\nQwQ-32B-Preview 3.05 0.98 0.90 0.99 0.97 1.00\nDecompscore QwQ-32B-Preview 4.65 0.98 0.90 – – –\nTable 7: Reasoning Analysis – TofuEval Dataset (sampled subset)\n17\n--- Page 18 ---\nPrompt 1.1: NLI Baseline\nDetermine whether the provided claim is consistent with the corresponding document. Consistency in this context\nimplies that all information presented in the claim is substantiated by the document. If not, it should be considered\ninconsistent.\nDocument: {{document}}\nClaim: {{claim}}\n<specific instructions for each method>\nConclude your response with either “yes” (the claim is supported) or “no” (the claim is not supported).\n<instruction for chain of thought>\nPrompt 1.2: QA-Based Instructions\nFollow the steps below to guide your assessment:\n1. Generate questions based on the claim.\n2. Answer those questions based on the document and on the claim separately.\n3. Check if the documents’ answers and the claims’ answers are similar.\n4. Make a final decision based on your analysis.\nPrompt 1.3: Decomposition-Based Instructions\nFollow the steps below to guide your assessment:\n1. Split the claim into separate sentences.\n2.Split each sentence into a few parts. Each part should contains a different topic of the sentence. For example, for the\nclaim: “A blue motorcycle parked by paint-chipped doors.”, its parts are: - “A blue motorcycle parked by doors”\n-“A motorcycle parked by paint-chipped doors”\n3. For each part, evaluate its support within the document.\n4. Make a final decision based on your analysis.\nPrompt 1.4: Comprehensive Reasoning Instructions\nFollow the steps below to guide your assessment:\n1. Split the claim into separate sentences.\n2. Decompose each sentence into its atomic components.\nAn atomic proposition is a statement that:\n(i) has a truth value verifiable against the document, and\n(ii) cannot be broken down further into smaller factual units with distinct truth values.\n{{example}}\n3. For each atomic component, evaluate its support within the document.\n- If supported, identify the exact phrase in the document that confirms it.\n- If contradicted, cite the phrase that disproves it.\n- If neither supported nor contradicted, mark it as a neutral component.\n4. Evaluate combinations of atomic facts.\n- If a combination is supported or contradicted, provide the source phrase(s) for this judgment.\n5. Make a final decision based on your analysis:\n- If there is at least one contradiction or neutral component, the claim is not supported.\n- If all components are entailed by the document, the claim is supported.\n18\n--- Page 19 ---\nPrompt 1.5: Davidsonian-Inspired Decomposition Example\nFor example, for the claim: for the claim: ‘A blue motorcycle parked by paint chipped doors.’, its atomic facts are: ‘the\nmotorcycle is blue’, ‘the motorcycle is parked’, ‘the doors are paint’, ‘the door is paint chipped’, ‘the motorcycle is next\nto the doors’.\nPrompt 2.1: Davidsonian-inspired Decomposition Instructions\nFollow the steps below to guide your assessment:\n1. Split the claim into separate sentences.\n2. Decompose each sentence into its atomic components.\nAn atomic proposition is a statement that:\n(i) has a truth value verifiable against the document, and\n(ii) cannot be broken down further into smaller factual units with distinct truth values.\n{{example}}\n3.For each atomic component, determine whether it is supported by the document (i.e., can be inferred from the\ndocument), or not supported by the document.\n4. Make a final decision based on your analysis:\n- If there is at least one contradiction or neutral component, the claim is not supported.\n- If all components are entailed by the document, the claim is supported.\nPrompt 2.2: Davidsonian-inspired Decomposition Instructions\nFollow the steps below to guide your assessment:\n1. Split the claim into separate sentences.\n2. Decompose each sentence into its atomic components.\nAn atomic proposition is a statement that:\n(i) has a truth value verifiable against the document, and\n(ii) cannot be broken down further into smaller factual units with distinct truth values.\n{{example}}\n3.For each atomic component, determine whether it is supported by the document (i.e., can be inferred from the\ndocument), contradicted by the document, or neutral relative to the document.\n4. Make a final decision based on your analysis:\n- If there is at least one contradiction or neutral component, the claim is not supported.\n- If all components are entailed by the document, the claim is supported.\n19",
  "text_length": 69906
}