{
  "id": "http://arxiv.org/abs/2506.05344v1",
  "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
  "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
  "authors": [
    "Jiahui Wang",
    "Zuyan Liu",
    "Yongming Rao",
    "Jiwen Lu"
  ],
  "published": "2025-06-05T17:59:55Z",
  "updated": "2025-06-05T17:59:55Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05344v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05344v1  [cs.CV]  5 Jun 2025SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs\nJiahui Wang1*, Zuyan Liu1,2*, Yongming Rao2,1, Jiwen Lu1†\n1Tsinghua University2Tencent Hunyuan Research\nAbstract\nMultimodal Large Language Models (MLLMs) are com-\nmonly derived by extending pre-trained Large Language\nModels (LLMs) with visual capabilities. In this work, we\ninvestigate how MLLMs process visual inputs by analyzing\ntheir attention mechanisms. We reveal a surprising spar-\nsity phenomenon: only a small subset (approximately less\nthan 5%) of attention heads in LLMs actively contribute\nto visual understanding, termed visual heads . To identify\nthese heads efficiently, we design a training-free framework\nthat quantifies head-level visual relevance through targeted\nresponse analysis. Building on this discovery, we introduce\nSparseMM , a KV-Cache optimization strategy that allocates\nasymmetric computation budgets to heads in LLMs based\non their visual scores, leveraging the sparity of visual heads\nfor accelerating the inference of MLLMs. Compared with\nprior KV-Cache acceleration methods that ignore the partic-\nularity of visual, SparseMM prioritizes stress and retaining\nvisual semantics during decoding. Extensive evaluations\nacross mainstream multimodal benchmarks demonstrate that\nSparseMM achieves superior accuracy-efficiency trade-offs.\nNotably, SparseMM delivers 1.38 ×real-time acceleration\nand 52% memory reduction during generation while main-\ntaining performance parity on efficiency test. Our project\nis open sourced at https://github.com/CR400AF-\nA/SparseMM .\n1. Introduction\nAutoregressive large language models (LLMs) [ 6,14,37,\n39,45] have revolutionized artificial intelligence with their\nexceptional instruction-following capabilities and expansive\nknowledge repositories. Building upon this foundation, re-\nsearchers have extended LLMs to multimodal domains, par-\nticularly in vision-language integration, creating multimodal\nlarge language models (MLLMs) [ 3,7,21,26,46,53] that\nprocess both textual and visual inputs. Current approaches\ntypically augment pre-trained LLMs by incorporating vi-\nsual encoders (e.g., CLIP [ 42] or SigLIP [ 55]) paired with\n*Authors contributed equally to this research.†Corresponding author.\nLLM Layers inMLLM\nAttention Heads\nOCR Tasks\nLayer #5, Head #7Visual Heads Activate\nonVisual Concepts\nduring RespondingHead Sparsity\nAsymmetric KV-Cache BudgetVisual HeadsVisual Heads (<5%)\nNon Visual\nHeadsVisual Heads are\nSparse inMLLMsMs/token\nInput tokensPerformance onDocVQAANLS\nCache BudgetLatency\n1.38x\nAcceleration23%\nPerformance\nGainFigure 1. Head Sparsity Emerges from Visual Concept Re-\nsponses. We observe the visual-relevant heads are sparse in various\nMLLMs. Based on this observation, we devise a KV-Cache opti-\nmization strategy that allocates asymmetric budgets to LLM heads\nbased on their importance for visual tokens, achieving better trade-\noff under limited computational resources.\nlightweight adapters to project visual features into the lan-\nguage model’s hidden space. While these architectures\ndemonstrate remarkable multimodal reasoning abilities, how\nLLMs fundamentally acquire visual comprehension during\nsupervised fine-tuning remains poorly understood. This\nknowledge gap constrains our ability to recognize cross-\nmodal alignment and risks undervaluing visual semantics\nduring multi-modal relevant tasks and applications, which\n1\n--- Page 2 ---\nmay potentially leading to suboptimal architecture designs\nand inefficient computational resource allocation.\nTo this end, we present the first systematic investiga-\ntion into how visual concepts are processed within LLMs.\nThrough rigorous analysis of attention mechanisms, we un-\ncover a critical phenomenon that only a small subset of at-\ntention heads (termed visual heads ) disproportionately drive\nvisual content understanding, while the majority remain text-\nspecialized. Specifically, our experiments reveal two critical\nproperties of these visual heads: (1) Sparsity : Less than 5%\nof attention heads are intrinsically visual-active across lay-\ners, even in models trained with extensive multimodal data;\n(2)Universality : Visual heads emerge consistently across\ndiverse LLM architectures(e.g., Vicuna [ 8] and Qwen2 [ 41])\nand generalize to multiple attention paradigms such as multi-\nhead attention (MHA) [ 49] and grouped query attention\n(GQA) [2].\nTo systematically identify these visual heads, we propose\na training-free framework that quantifies the visual relevance\nof attention heads through targeted cross-modal response\nanalysis. Specifically, our approach leverages OCR as an\nanchor task to establish precise correspondence between text\noutputs and visual inputs: for each generated word, we trace\nits activation back to spatially aligned image patches, en-\nabling direct measurement of how specific attention heads\nmediate visual-text alignment. By analyzing and recoding\nthe attention score of all the attention heads across a cer-\ntain amount of samples, we compute visual scores that rank\nheads by their visual responsiveness. Crucially, while our\nidentification mechanism relies on OCR’s granular spatial\ngrounding, we demonstrate that the detected visual heads ex-\nhibit task-agnostic generalizability—they remain dominant\nin diverse vision-language tasks including object recognition\nand scene understanding.\nBuilding on these insights, to demonstrate the effective-\nness of visual heads on practical multi-modal tasks, we intro-\nduce SparseMM , a KV-Cache optimization framework that\nexploits visual head sparsity to achieve accelerated inference.\nAs multimodal inputs grow in complexity—spanning multi-\nturn dialogues [ 19,20,52], high-resolution interleaved im-\nages [ 9,51], and dense video/3D sequences [ 12,16,23]—the\ncomputational overhead of maintaining full KV-Caches be-\ncomes prohibitive. Existing compression methods, however,\ntreat all attention heads uniformly, disregarding the critical\nrole of sparse visual heads in encoding visual semantics.\nSparseMM addresses this by asymmetrically allocating\nKV-Cache budgets: visual heads receive prioritized reten-\ntion based on their precomputed visual scores, while non-\nvisual heads undergo aggressive compression via a hybrid\nstrategy combining 1) Score-Preferred Cache (allocating\ncache budget based on visual head scores), 2) Uniform-\nBased Cache (preserving minimal budget for all the heads),\nand 3) Local Window Cache (preserving cache budget forrecent tokens). This mixed approach ensures better accuracy-\nefficiency trade-offs, such that visual heads retain more com-\nputational cost while other heads are dynamically throttled.\nExtensive experimental results demonstrate that\nSparseMM outperforms other strong baselines across\nmultiple datasets, including DocVQA [ 35], OCRBench [ 29],\nTextVQA [ 44], MMBench [ 28], etc. For instance, on\nDocVQA, LLaV A-NeXT-Vicuna-7B [ 27] achieves the same\nlevel of accuracy while using only 20% of the cache, and\nQwen2-VL-7B-Instruct [ 41] achieves equivalent perfor-\nmance with just 5.3% of the cache. These findings suggest\nthat our method effectively captures visual information\nwhile compressing redundancies. Furthermore, the reduction\nin cache requirements enables our method to achieve lower\ndecoding latency and peak memory usage. For example,\nLLaV A-NeXT-Mistral-7B [ 27] maintains nearly constant\ndecoding latency with 32K input tokens, resulting in almost\na 50% acceleration compared to the full model, and reduces\nmemory usage by 5GB.\n2. Related Works\nArchitectures in MLLMs. The predominant architecture\nfor Multi-Modal Large Language Models (MLLMs) con-\nsists of three key components: a visual encoder, an adapter,\nand a LLM. By leveraging alignment training techniques\nand subsequent fine-tuning, this integrated framework has\nachieved remarkable performance on various multi-modal\nunderstanding tasks [ 1,10,19,24,30,32,38]. In typical\nimplementations, the visual encoder is realized using models\nlike CLIP [ 42] or SigLIP [ 55], which are adept at extracting\nrich visual representations. The adapter component serves\nas an intermediary, bridging the gap between the visual fea-\ntures and the language domain; it is often instantiated as\na multi-layer perceptron (MLP) or through more complex\nstructures [ 33]. For the LLM part, architectures such as\nLLaMA [ 47,48] and Qwen [ 40] series are commonly em-\nployed. Notably, previous LLMs such as LLaMA2 [ 48] uti-\nlizes a multi-head attention (MHA) mechanism, while more\nrecent LLMs such as LLaMA3 [ 11] incorporate a grouped\nquery attention (GQA) [ 2] design. The GQA approach aggre-\ngates multiple queries into a single group that corresponds\nto one key and one value, thereby substantially reducing\nmemory usage without compromising model performance.\nIn this paper, we observe a universal phenomenon of visual\nhead in MLLMs across LLM architectures and attention\nimplementations. The applications are demonstrated to be\neffective across the abundant MLLM series.\nModel Acceleration in MLLMs. With the rapid growth of\nmodel size and input sequence length, model acceleration has\nbecome an urgent research focus in both language and multi-\nmodal domains. In the context of Large Language Models\n(LLMs), significant efforts have been dedicated to optimiz-\n2\n--- Page 3 ---\nOCR Results: Dares Wins Vol.5Tommy’s …\nGenerate Tokens:\nVisual TokensLayer #5, Head #7\nAttention Score MatrixMax Response in\nHitVisual Tokens?\nVisual Head ScoreYes!×\nHitVisual TokensHead Index\nLayer Index\nMark Visual\nHead Score\nVisualizations ofVisual Heads\nFigure 2. Visual Heads are Sparse in MLLMs. We use OCR tasks to obtain visual scores for all heads. Upon visualizing these scores, we\ndiscovered that high-scoring heads, which we refer to as visual heads , are quite sparse within the MLLM, comprising only about 5%. The\nmajority of heads have very low scores, indicating that most heads in LLMs do not focus on visual information.\ning the prompt encoding phase through efficient compression\nof the KV-Cache. For instance, StreamingLLM [ 50] identi-\nfies attention sinks to stabilize long-context inference, while\nH2O [ 56] introduces a token-level importance scoring mech-\nanism for adaptive KV-Cache eviction. Subsequent works,\nsuch as SnapKV [ 22], PyramidKV [ 4], and AdaKV [ 13], fur-\nther refine the KV-Cache selection strategy by incorporating\nspatial-temporal redundancy reduction, hierarchical token\nretention, or dynamic eviction policies. However, these\nmethods, primarily designed for text-only inputs, face limi-\ntations when applied to multi-modal scenarios. In MLLMs,\nacceleration challenges are exacerbated by the increasing\ncomplexity of multi-modal prompts (e.g., high-resolution im-\nages, videos) and cross-modal fusion mechanisms. Recent at-\ntempts, such as FastV [ 5], accelerate inference via layer-wise\npruning of redundant visual tokens. ElasticCache [ 31], opti-\nmizes KV-Cache management during the generation phase.\nDespite these advances, head-wise acceleration strategies,\nparticularly those targeting modality-specific attention heads,\nremain underexplored. In this work, we address this gap\nby proposing a systematic framework based on our findings\nabout visual heads, enabling efficient deployment of MLLMs\nin resource-constrained environments.\n3. Visual Heads are Sparse in MLLMs\nIn this section, we present our exploration of head sparsity\nin multi-modal large language models. To start with, we\nprovide the preliminaries on the relations from LLMs to\nvisual instruction tuning. Then we describe our approach\nfor identifying sparse visual heads in MLLMs in Sec. 3.2,\nthen introduce the deployment of visual heads in model\nacceleration in Sec. 3.3.\n3.1. What is Learned during Visual Instruct Tuning\nExtending a Large Language Model to a Multimodal Large\nLanguage Model is achieved by integrating a visual en-\ncoder E, an adapter H, and the LLM pθ. The original\nLLM is trained solely on textual tasks to model the distribu-\ntion of text sequences as pθ(x) =QN\ni=1pθ(xi|x<i), where\n{xi}N\ni=1. The visual encoder, typically based on architec-Algorithm 1 Chasing Visual Heads in MLLMs\nInput:\nocrtextbbox pair = List[(text, bbox)]\noutput token = {yi}N\ni=1\nimage shape, feature map\nOutput: Matrix Srepresenting scores of heads in LLMs\n1:fori= 1 to Ndo\n2: bbox = match( yi, ocr textbbox pair)\n3: patch idx = match(bbox, image shape, feature map)\n4: image tokens = find(patch idx, feature map)\n5: for(layer, head) do\n6: index = argmax( Alayer\nhead)\n7: ifindex inimage tokens then\n8: Slayer\nhead+=1\n#image tokens\n9: end if\n10: end for\n11:end for\n12:Return S\ntures such as CLIP [ 42] or SigLIP [ 55], is responsible for\nextracting visual features from images. An adapter His\nthen utilized to project these visual features into the seman-\ntic space, culminating in a multimodal model that can be\nformally represented as follows:\npθ(x) =NY\ni=1pθ(xi|x<i,v),v=H(E(Image )) (1)\nIn order to enable the LLM to comprehend and process\nvisual information, a pre-alignment phase is conducted, fol-\nlowed by a visual instruction fine-tuning stage. The objective\nduring these stages is to minimize the cross-entropy loss be-\ntween the generated textual output and the ground truth as\nfollows:\nL=−1\nN−PNX\ni=P+1logpθ(xi|x<i,v) (2)\nwhere Pis the length of input tokens.\nAlthough the resulting MLLM shows outstanding perfor-\nmance in various tasks, the precise modifications that occur\n3\n--- Page 4 ---\nduring the transition from LLM to MLLM, rendering the\nmodel capable of understanding visual information, are not\nsufficiently understood. However, we found that some atten-\ntion heads within the MLLM have learned to focus on visual\ninformation during the visual instruction finetuning process.\nWe refer to these heads as visual heads .\n3.2. Chasing Visual Heads in MLLMs\nTo investigate how attention heads within the MLLM attend\nto visual elements and to identify the specific visual head, we\nintroduce an OCR-based method and define the visual score.\nAs in Alg. 1 and Fig. 2, for a given text instruction and OCR\nimage as input X, the MLLM is tasked with generating the\nOCR output. For each output token yi, we first determine its\ncorresponding region within the image based on (text, bbox)\npair. Based on this region, we then identify the associated\nimage tokens denoted Iyiin the input sequence:\nVisual Score for Head h=1\nNNX\ni=1Ihit(yi,Ah)\n#image tokens(3)\nwhere\nhit(yi, Ah) =(\n1,argmax (Ah)∈Iyi\n0,else(4)\nSubsequently, we iterate over all attention heads. For any\ngiven head h, if the token that receives the highest attention\nin this head’s attention matrix Ahbelongs to the set of iden-\ntified image tokens, a “hit” is recorded for that head and its\nscore is incremented by the inverse of the number of image\ntokens. This means a smaller (more precise) region yields a\nhigher score, because they are harder to capture.\nFinally, we aggregate the scores from all heads across\n1,000 OCR images from the Synthdog dataset [ 18]. These\nscores are then normalized to produce a score matrix, the\nvisualization of which is presented in Fig. 2.\n3.3. Exploring Head Sparsity for Acceleration\nIn multi-modal models, visual tokens comprise a significant\nportion of the input sequence, and each token necessitates its\nown key-value (KV) cache. This requirement leads to a sub-\nstantial and often prohibitive increase in computational cost\nand memory consumption. However, previous analyses have\nshown that not every attention head relies highly on visual\ninformation. This finding motivates a natural idea: allocate\nvarying KV-cache budgets to different attention heads in\nproportion to their visual attention scores, thereby balancing\nefficiency with overall performance.\nIn this subsection, we describe SparseMM for allocating\neach head’s cache budget, as illustrated in Fig. 3. For a typi-\ncal multi-modal model with Llayers and Hheads per layer,\nwe can obtain a visual attention score matrix Score L×Has\nHead 1\nScore 0.05\nHead 2\nScore 0.23\nHead 3\nScore 0.06\nHead 4\nScore 0.16\nInput Prompt K/V\nUniform -Based Cache\nLocal Window Cache\nScore -Preferred Cache\nHead -wise Compressed K/VCorrelation Weight toLocal Windows\nTopKTokens Selection\n(K=Cache Budget)\nIndex &\nConca tLocal Window\nL=320.1x\n0.9xCache Budget\nAllocationFigure 3. SparseMM for MLLM Acceleration. The KV Cache\nbudget for each head is composed of three parts: Local Window\nCache ,Uniform-Based Cache , and Score-Preferred Cache . The\ntop-K KV caches are selected based on attention scores.\ndetailed in Sec. 3.2. In an ideal setting, the cache allocation\nwould be determined exclusively by the values in Score L×H.\nHowever, inspired by AdaKV [ 13], and to account for local-\nity and to ensure that every head maintains a minimum level\nof budget, we introduce a three-part allocation mechanism:\n1) Local Window Cache : Each head is first allocated a fixed,\npredetermined cache size for the nearest neighbor window.\nWe denote this window size by w, with a default value of 32.\nThus, the total cache allocated for all heads in this step is\nN·w\n2) Uniform-Based Cache : Denote the total budget by B.\nFrom the remaining budget,\nBremain1 =B−N·w (5)\na fixed ratio, denoted by ρ∈(0,1)(with a default value of\n0.1), of this remainder is uniformly allocated to each head.\nThat is, each head receives an additional baseline cache of\nr=ρ·(B−N·w)\nN(6)\n3) Score-Preferred Cache : The remaining budget after the\nuniform allocation,\nBremain2 =B−N·w−ρ(B−N·w) (7)\nis then distributed among the heads in proportion to their\ncorresponding visual attention scores. We denote by sij\nthe element in the ith row and jth column of the matrix\n4\n--- Page 5 ---\nScore L×H, which represents the visual attention score of\nthejth head in the ith layer. Then, the score-based cache\nallocated to head (i, j)is given by\nbscore\nij=Bremain2·sij\nLX\ni=1HX\nj=1sij(8)\nSumming the contributions from each of the three parts,\nthe final cache allocation for head (i, j)is expressed as\nbij=w+r+bscore\nij (9)\nOnce each head establishes its budget, the most salient\nKV Caches are identified by ranking the attention scores.\nInspired by the approach presented in SnapKV [ 22], which\nemploys an observation window at the end of the prompt,\nwe restrict our attention computation to only the final obser-\nvation window of size 32. Assume we have Query States Q\nand Key States K, then we compute local window attention\nas follows:\nA=softmax (QlocK⊤\nall√\nd+M) (10)\nwhere\nQloc=Q[:,:, L−w:L,:],Kall=K (11)\nMi,j=(\n0, ifj≤i\n−∞,ifj > i(12)\nThis strategy effectively reduces the computational com-\nplexity from O(N2)toO(N×L), where L= 32 , thereby\ndecreasing the runtime during the prefilling stage. To eval-\nuate the attention for keys outside the local window, we\ncompute the average attention weight:\n¯Aj=1\nwwX\ni=1ˆAi,j,forj∈ {0, . . . , L −w−1}(13)\nUltimately, we select the top KKV Caches based on the\ncomputed attention scores, where the value of Kis given by\nK=r+bij (14)\nOur three-part allocation mechanism leverages head spar-\nsity to significantly reduce the computational and memory\noverhead in multi-modal models. It ensures that each head\nreceives a guaranteed minimum cache allocation through\nboth the nearest neighbor and uniform baseline allocations.\nThe remaining cache is then adaptively distributed based on\nthe visual attention scores, thereby achieving an efficient\nbalance between computational efficiency and overall model\nperformance.4. Experiments\nWe conduct extensive experiments to validate the effective-\nness of Visual Head. We first introduce our experimental\nsettings in Section 4.1. Then we present a comparison with\nstate-of-the-art KV Cache Compression method, demonstrat-\ning that our method maintains strong performance in Section\n4.2 while maintaining computational efficiency in Section\n4.3. Moreover, we provide some analytical experiments to\nillustrate the importance of visual head in Section 4.4.\n4.1. Experimental Settings\nModels. We employ three multi-modal models: LLaV A-\nNeXT-Vicuna-7B [ 27], LLaV A-NeXT-Mistral-7B [ 27], and\nQwen2-VL-7B [ 41]. LLaV A-NeXT-Vicuna-7B is derived\nfrom Vicuna-7B [ 8], a model based on Multi-Head Atten-\ntion (MHA) [ 49], and comprises 32 layers with 32 attention\nheads per layer. In contrast, LLaV A-NeXT-Mistral-7B is\nbuilt upon Mistral-7B [ 17] and utilizes Grouped-Query At-\ntention (GQA) [ 2]. This model features 32 layers, with each\nlayer consisting of 32 query heads and 8 key-value heads.\nSimilarly, Qwen2-VL-7B [ 41] is based on Qwen2, another\nGQA model, and is composed of 28 layers, with each layer\ncontaining 28 query heads and 4 key-value heads.\nBaselines. We adopt SnapKV [ 22], PyramidKV [ 4], and\nAdaKV [ 13] as our baseline methods, as they represent\nthe latest and state-of-the-art in KV Cache compression.\nSnapKV [22] utilizes an “observation window” mechanism\nto identify and preserve the most critical KV caches, thereby\nensuring that only the most salient information is retained\nduring attention computation. PyramidKV [ 4] implements a\nhierarchical allocation strategy that distributes the KV cache\nbudget in a pyramidal manner. Specifically, the lower lay-\ners, which exhibit more dispersed attention patterns, are\nallocated a larger budget, while the higher layers, where\nattention is more concentrated, receive a correspondingly\nsmaller allocation. Meanwhile, AdaKV [ 13] proposes a\ndynamic allocation framework that assigns varying cache\nbudgets to different attention heads within a layer, based on\nthe intra-layer attention distributions.\nIn addition, to underscore the effectiveness of the Visual\nHead, we introduce a Random Head baseline for comparison.\nIn this baseline, the only difference lies in the initialization of\nthe scores for each head; they are randomly assigned rather\nthan being derived from the Visual Head. This approach\nserves as a control that allows us to isolate and evaluate the\nspecific contribution of the visual head component to the\noverall performance of the model.\nBenchmarks. To comprehensively assess the effective-\nness of Visual Head in visual perception, we conduct eval-\nuations on five widely used benchmarks covering both vi-\nsual question answering (VQA) and image captioning tasks.\nSpecifically, we utilize DocVQA [ 35], OCRBench [ 29],\n5\n--- Page 6 ---\nFigure 4. Main Results on Multi-Modal Benchmarks. We evaluate SparseMM and other baselines on several multimodal benchmarks,\nand conduct experiments on a series of backbones. Our SparseMM consistently outperforms the other baselines.\nTable 1. Average Number of Input Tokens. We analyzed the\naverage length of input tokens across various benchmarks. Con-\nsidering that text instructions are typically very short(fewer than\n50 tokens), so visual tokens constitute the majority of the input\nsequence, accounting for 90% to 99% of the input tokens.\nDataset DocVQA OCRBench TextVQA ChartQA TextCaps\nLLaV A-Series 2433 1700 2376 2270 2376\nQwen2-VL-7B-Instruct 4830 1245 1024 642 1024\nTextVQA [ 44], ChartQA [ 34], and TextCaps [ 43], which\ncollectively encompass a diverse set of challenges, including\ndocument understanding, OCR-based question answering,\nchart interpretation, and text-based image captioning. Addi-\ntionally, we also select the mainstream multiple-choice vi-\nsual benchmarks, including MMBench [ 28] and VQAv2 [ 15]\nfor a comprehensive evaluation.\n4.2. Results on Multi-Modal Benchmarks\nSetups. To determine an appropriate budget allocation,\nwe first measure the average length of input tokens for each\nbenchmark, as reported in Table 1. Given that text instruc-\ntions typically consist of no more than 50 tokens, the ma-\njority of input tokens are attributed to visual tokens. Con-\nsidering the varying input sequence lengths across different\ndatasets, we select a range of each head’s KV Cache budget\nfor evaluation: {64, 128, 256, 512, 1024, 2048 }. Since gen-\neral visual benchmarks utilize lower image resolutions, we\nadjust the input token budget range correspondingly: {48,\n64, 96, 128, 256, 512 }. This allows us to systematically\nanalyze the impact of different cache sizes on performance\nand efficiency across various benchmarks.\nResults. Fig. 4 presents the evaluation results for threemodels and five benchmarks. Our experimental results\ndemonstrate that our proposed method consistently outper-\nforms baseline approaches, particularly under extreme cache\nbudget constraints (e.g., 128 or 256). Under these condi-\ntions, our approach maintains performance levels close to\nthose achieved with full cache utilization, significantly out-\nperforming the competing baselines. For instance, on the\nTextVQA [ 44] task using LLaV A-NeXT-Vicuna-7B, a KV\nCache budget of 256—which constitutes only approximately\n10.77% of the average 2376 tokens—yields performance\nequivalent to the full-cache model, whereas AdaKV [ 13]\nand similar methods experience an accuracy drop of roughly\n3%. Similarly, on OCRBench [ 29], LLaV A-NeXT-Mistral-\n7B demonstrates only a slight performance degradation at a\nKV Cache budget of 128 (about 7.5% of the average 1700\ntokens), in contrast to a decline exceeding 10% observed\nwith other methods. In addition, Qwen2-VL-7B-Instruct on\nDocVQA [ 35] maintains performance when operating with a\nKV Cache budget of 256 (merely 5.3% of the average 4830\ntokens), while alternative approaches suffer performance\ndrops between 5% and 17%. These results validate the effec-\ntiveness of our method in VQA tasks.\nFig. 5 presents the evaluation results on multiple-choice\nbenchmarks. Our method demonstrates competitive perfor-\nmance on multi-choice benchmarks compared with existing\nbaselines. For instance, with only 96 token budget, our\napproach retains full performance on MMBench while ex-\nperiencing only a minimal performance degradation ( <1%)\non GQA and VQAv2. These findings substantiate that our\nmethod can effectively recognize visual content while ex-\nhibiting strong generalizability towards diverse tasks.\nFurthermore, our findings in Fig. 4 indicate that the ran-\ndom head baseline consistently produces the poorest perfor-\n6\n--- Page 7 ---\nFigure 5. Results on Multiple-choice Benchmarks. We evaluate\nSparseMM and other baselines on multiple-choice visual bench-\nmarks with Qwen2-VL-7B-Instruct as the backbone model. Our\nSparseMM consistently outperforms the other baselines.\nmance across nearly all experiments, whereas our method\nachieves superior outcomes by utilizing Visual Head. This\npronounced contrast underscores the efficacy of our ap-\nproach in accurately capturing the manner in which multi-\nmodal language models attend to visual information. It is\nimportant to note that the performance of the random head\nmethod is comparable to that of SnapKV [ 22], particularly\nin the case of the MHA model. This similarity is attributable\nto the fact that when the scores of all heads are randomly\ninitialized, the cache budget allocated to each head is statis-\ntically equivalent, effectively causing the method to revert to\nthe behavior observed with SnapKV [22].\n4.3. Efficiency Evaluation\nSetup In this subsection, we evaluate the computational\nefficiency of our proposed method, which holds significant\npractical value. We employ the model described in Sec. 4.1.\nwe adapted LLaV A-NeXT-Vicuna-7B to enable support for a\n32K context, while the other two models inherently support a\n32K context. Accordingly, our efficiency tests are conducted\nacross a range of input token lengths {2K, 4K, 8K, 16K,\n32K}. For each experiment, the output sequence length\nwas fixed at 100 tokens, with a KV Cache budget set to\n256. We computed the average decoding latency and peak\nmemory consumption for each configuration. Notably, all\nexperiments are done using FlashAttention.\nDecoding Latency Fig. 6 illustrates that the reduction in KV\nCache in our method substantially decreases the computa-\ntional load during inference, thereby enhancing inference\nspeed. For instance, when the input sequence length is 8K,\nthe LLaV A-NeXT-Vicuna-7B model exhibits a speedup of\n1.16×, while at a 32K input length, the speedup increases\nto1.87×. These findings indicate that our approach signif-\nicantly accelerates token generation, particularly in high-\nresolution or long video contexts.\nMemory Cost Our method also offers a marked reduction\nin peak memory usage, primarily by diminishing the mem-\nory footprint associated with the KV Cache. This reduc-\ntion is especially pronounced in LLaV A-Series models. For\nexample, with an input sequence length of 32K, LLaV A-\nFigure 6. Efficiency Evaluation for SparseMM .Benefiting from\nthe reduction in KV cache, SparseMM can maintain nearly constant\ndecoding latency, achieving up to a 50% acceleration. Additionally,\nit effectively reduces peak memory usage.\nNeXT-Vicuna-7B with full KV Cache requires 32.87 GB\nof memory, whereas our method reduces the requirement\nto 17.38 GB, thereby achieving an approximate 50% reduc-\ntion in memory overhead. It is noteworthy that even for the\nQwen2-VL-7B-Instruct model, which employs an aggressive\ncompression technique in its GQA framework, we can still\nreduce the cache by nearly 2GB with 32k inputs.\n4.4. Analysis\nPerformance Influence of Visual Heads. To further elu-\ncidate the impact of visual heads on the visual perception\ncapabilities of multimodal models, we conducted a series of\nmasking experiments. In these experiments, we selectively\nmasked a specific proportion of the visual heads and, for\ncomparison, randomly masked an equivalent proportion of\nattention heads. The evaluation was performed on OCR-\nBench [ 29] and TextVQA [ 44], with performance measured\nrelative to the baseline unmasked model. The results, as\nillustrated in Fig. 7, reveal that masking visual heads leads\nto a significant performance decline. For example, in the\ncase of LLaV A-NeXT-Vicuna-7B, masking merely 2% of\nthe highest-scoring visual heads resulted in a 50% drop in\nperformance, whereas masking 10% caused a dramatic 75%\ndecline. In contrast, randomly masking the same proportion\nof attention heads produced a much smaller impact—for\ninstance, a 10% random mask in Qwen2-VL-7B-Instruct\nled to only a 7% reduction in performance. These findings\n7\n--- Page 8 ---\nFigure 7. Comparisons of Mmasking Visual Head and Random\nHead. The left figure is the result on OCRBench, and the right\nfigure is the result on TextVQA. When masking only the top 3%\nof visual heads with the highest scores, the model’s performance\ndropped by half, while randomly masking heads resulted in almost\nno change in performance. This indicates that visual heads consti-\ntute a small proportion of all heads while they are crucial for visual\ninformation perception.\nFigure 8. Visualizations of Visual Heads using Different Datasets.\nWe visualized the attention distribution identified on MLT, CTW,\nand COCO datasets.\nFigure 9. Results with Different Visual Head Identification\nApproaches and Datasets. We conduct an evaluation on visual\nheads identified on different datasets. The results on OCR datasets\nare similar and better than those on the detection dataset.\nunderscore the critical role that visual heads play in enabling\nmulti-modal models to effectively capture and process visual\ninformation. Moreover, masking the top 5% of high-scoring\nvisual heads causes a considerably greater performance loss\nthan the additional impact of masking another 5%, which\nhighlights the sparse yet indispensable distribution of visual\nheads.\nRobustness of Visual Head Identification. To evaluate the\nrobustness of our visual head detecting approach, we show\nthe distribution of the detected visual heads under different\ndatasets and tasks in Fig. 8 and show the accuracy curve\nin Fig. 9. For the OCR task, we used the Multi-Lingual\nText(MLT) [ 36] and Chinese Text in the Wild(CTW) [ 54]Table 2. Ablation on Cache Allocation Strategies. The results\ndemonstrate that each of the three cache components plays an\nessential role and that none can be omitted without negatively\nimpacting overall performance.\nLocal Window Uniform-Based Score-Preferred MMBench\nCache Cache Cache 512 256 128 96 64 48\n✓ ✗ ✗ 81.3 80.5 77.3 73.6 70.5 67.2\n✓ ✓ ✗ 81.5 81.4 79.3 77.6 74.6 73.9\n✓ ✓ ✓ 81.5 81.4 81.5 81.4 80.3 77.9\nTable 3. Comparison of Accuracy-speed Trade-off among Dif-\nferent Methods. We compare the speed of all methods under 256\nKV Cache budget and 16K input tokens.\nMethods DocVQA OCRBench TextVQA ChartQA TextCaps Latency(ms)\nFullKV 0.68 0.52 0.65 0.55 0.73 52.9\nSparseMM 0.68(-0.00) 0.52(-0.00) 0.65(-0.00) 0.54(-0.01) 0.73(-0.00) 37.1(-30%)\nSnapKV 0.64(-0.04) 0.46(-0.06) 0.62(-0.03) 0.50 (-0.05) 0.65(-0.08) 35.3(-33%)\nPyramidKV 0.65(-0.03) 0.48(-0.04) 0.62(-0.03) 0.53(-0.02) 0.65(-0.08) 34.9(-34%)\nAdaKV 0.65(-0.03) 0.48(-0.04) 0.62(-0.03) 0.49(-0.06) 0.66(-0.07) 37.3(-29%)\ndatasets. In addition, we consider the object detection task\nand choose the COCO dataset [ 25], where the model is re-\nquired to identify objects present in the images. We then\nlocalized the visual heads based on the correspondence be-\ntween the model’s answers and the relevant objects. As\nshown, the distribution of visual heads is relatively con-\nsistent across the OCR datasets, whereas there is greater\nvariation on the COCO dataset. Moreover, experimental re-\nsults demonstrate that the visual heads identified from OCR\ntasks are dataset-agnostic and exhibit strong generalizability,\nwith better results than detection tasks. This is because OCR\ntasks establish an exact one-to-one mapping between the\nmodel’s output and the visual content, whereas the COCO\ntask, which focuses on larger bounding boxes, introduces\nmore noise and results in less robustness.\nAblation on Cache Allocation Strategies. We add an ab-\nlation study on Qwen2-VL-7B-Instruct to investigate the\neffectiveness of the three-part cache allocation mechanism.\nAs shown in Tab. 2, using only Local-Window Cache lim-\nits context and causes larger drops with smaller budgets.\nCombining Local-Window and Uniform-Based Caches lacks\nhead-level allocation and underperforms compared to our\nSparseMM.\nAccuracy and Speed Trade-off. We compared the accuracy\nand speed of SparseMM with other baselines in Tab. 3. We\nconducted an experiment on LLaV A-NeXT-Vicuna-7B with\na budget of 256 KV Cache. With the support of FlashAt-\ntention, our decoding latency is comparable to that of other\nmethods, significantly lower than the FullKV method. How-\never, our method outperforms others in terms of performance\nunder the same budget. This effectively demonstrates the\nefficacy of SparseMM based on visual heads in multimodal\nmodels.\nVisualization of Visual Heads. To gain a more intuitive un-\n8\n--- Page 9 ---\nFigure 10. Visualizations of Visual Heads .We visualized the\nattention distribution of several heads. The visual heads are able to\naccurately capture text or objects within the images, whereas the\nnon-visual heads provide random results.\nderstanding of how visual heads process and interpret visual\ninformation, we conducted a visualization analysis of visual\nheads and non-visual heads on LLaV A-NeXT-Vicuna-7B.\nAs illustrated in Fig. 10, our observations indicate that non-\nvisual heads often either neglect the image entirely (as seen\nin layer 15, head 16) or allocate a disproportionate amount of\nattention to visually insignificant regions (for example, layer\n0, head 30). In contrast, visual heads accurately pinpoint\nregions of interest, allocating a substantial proportion of at-\ntention to these critical areas. This precise focus explains\nwhy visual heads are particularly effective in capturing and\nencoding visual concepts. Moreover, these visualization\nresults underscore the functional disparity between visual\nand non-visual heads and highlight the importance of dedi-\ncated visual attention mechanisms in enhancing the overall\nperceptual capabilities of multi-modal models.\n5. Conclusion\nIn this paper, we present a systematic exploration of the\nvisual processing characteristics inherent in MLLMs. Our\nanalysis reveals a critical sparsity phenomenon that only a\nsmall fraction of attention heads actively engage in visual un-\nderstanding. Leveraging this insight, we propose SparseMM,\na novel KV-Cache optimization framework that dynamically\nallocates asymmetric computation budgets to attention heads\nbased on their visual relevance. SparseMM prioritizes pre-\nserving vision-critical information during decoding, thereby\nachieving a more balanced accuracy-efficiency trade-off. We\nhope this study inspires deeper investigations into the princi-\nples governing multimodal learning.\nAppendix\nA. Implementation details\nA.1. Implementation details about GQA\nThe models LLaV A-NeXT-Mistral-7B, and Qwen2-\nVL-7B-Instruct are Grouped-Query Attention (GQA)\nmodels, which differ markedly from conventional multi-\nhead attention (MHA) mechanisms in the computationof attention. In a GQA model, the query state is of\nshape (bs, seq len, num query heads, hidden dim),\nwhile the key and value states, collectively form-\ning the KV cache, are of shape (bs, seq len,\nnum keyvalue heads, hidden dim). During the\nattention calculation, the key and value states are repeated\nnum query heads\nnum keyvalue heads=num keyvalue group\ntimes, thereby restoring the setup analogous to\nMHA. Prior to computation, the sequence length\ndimension and the query head dimension are inter-\nchanged, resulting in an attention score tensor of shape\n(bs, num query heads, seq len, seq len). Subsequently,\nwhen this tensor is combined with the value states, the output\nis of shape (bs, num query heads, seq len, hidden dim)\nFrom the above reasoning, it follows that we\nobtain a visual head score matrix with dimensions\n(layers, num query head). This is the origin of the score\ndistribution depicted in Fig. 2.\nIn practical scenarios involving the preservation of the\nKey-Value cache, each key-value head is associated with\nnum keyvalue group attention scores. The total attention\nscore for a given head is computed as the sum of the scores\nof the corresponding group. This aggregate score is then\nemployed for the allocation of the budget for the Key-Value\ncache.\nA.2. Details on Evaluation Metrics\nWe adopt different evaluation metrics for different bench-\nmarks. For the DocVQA [ 35] benchmark, we employ the\nANLS metric. This metric evaluates the similarity between\nthe predicted answer and the ground truth by normalizing\nthe Levenshtein distance, thereby accommodating minor\nvariations in format and phrasing while maintaining a ro-\nbust assessment of answer quality. For the OCRBench [ 29],\nTextVQA [ 44], MMBench [ 28], GQA [ 2] and VQAv2 [ 15]\nbenchmark, we use accuracy as the primary metric. For\nChartQA [ 34] benchmark, we utilize the relaxed accuracy\nmetric. This measure provides partial credit for responses\nthat are close to the ground truth, thereby offering a more\nnuanced perspective on model performance when outputs\nare not perfectly correct but still largely informative. Finally,\nfor the TextCaps [ 43] dataset, we adopt the CIDEr metric.\nCIDEr assesses the quality of generated captions by comput-\ning a weighted n-gram similarity between the candidate and\nreference captions.\nB. More Visualization\nWe conduct more visualization on the visual head in Fig. 11.\nWe use LLaV A-NeXT-Vicuna-7B model for the experiment.\n9\n--- Page 10 ---\nFigure 11. More Visualization Results. Visual heads are able to\nattend to the correct objects, whereas non-visual heads cannot.\nC. More Analysis\nAblations on Budget Allocation Ratios. We conducted an\nablation study on the hyperparameter ρ. This study evaluated\nthe performance of three models on OCRBench, with a\nbudget of 256. The results are presented in Tab. 4. For the\nLLaV A-NeXT-Vicuna-7B model, the ratio ρ= 0.1achieved\nthe highest performance score of 0.522, outperforming other\nratios. Similarly, for the LLaV A-NeXT-Mistral-7B model,\na ratio of 0.1 also resulted in a peak performance score of\n0.519, which is significantly higher compared to the scores at\nother ratios. While the Qwen2-VL-Instruct model exhibited\nonly a marginally higher score at ρ= 0.1(0.812), this\nstill represents the highest performance across all tested\nratios. It is noteworthy that the Mistral model exhibits a\nsignificant performance drop at a ratio of ρ= 0. This\nobservation suggests that relying entirely on visual head\nscore allocation of the cache budget can result in some heads\nbeing unable to attend to any preceding input information.\nConsequently, this underscores the necessity of assigning a\nUniform-Based cache to each head. By ensuring that each\nhead receives a guaranteed share of the cache resources, we\ncan prevent such performance degradation and enhance the\noverall effectiveness of the model.Table 4. Ablation on Budget Allocation Ratios. We conducted an\nablation study on the hyperparameter ρand the results indicated that\nthe performance is optimal when the ratio is set to 0.1. Therefore,\nwe use 0.1 as the default value in our experiments.\nRatio ρ 0 0.1 0.2 0.3 0.4 0.5 0.8 1.0\nLLaV A-NeXT-Vicuna-7B 0.507 0.522 0.520 0.520 0.516 0.515 0.510 0.460\nLLaV A-NeXT-Mistral-7B 0.145 0.519 0.517 0.514 0.514 0.518 0.506 0.451\nQwen2-VL-7B-Instruct 0.809 0.812 0.811 0.808 0.807 0.804 0.789 0.775\nD. Numerical results\nWe present the numerical results of our main experimental\nresults for reference and further research.\nReferences\n[1]Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Bap-\ntiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo\nCosta, Baudouin De Monicault, Saurabh Garg, Theophile\nGervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073 ,\n2024. 2\n[2]Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury\nZemlyanskiy, Federico Lebr ´on, and Sumit Sanghai. Gqa:\nTraining generalized multi-query transformer models from\nmulti-head checkpoints. arXiv preprint arXiv:2305.13245 ,\n2023. 2, 5, 9\n[3]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. NeurIPS , 35:\n23716–23736, 2022. 1\n[4]Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu\nLiu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang,\nJunjie Hu, et al. Pyramidkv: Dynamic kv cache compression\nbased on pyramidal information funneling. arXiv preprint\narXiv:2406.02069 , 2024. 3, 5\n[5]Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang\nLin, Chang Zhou, and Baobao Chang. An image is worth 1/2\ntokens after layer 2: Plug-and-play inference acceleration for\nlarge vision-language models. In European Conference on\nComputer Vision , pages 19–35. Springer, 2024. 3\n[6]Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhang-\nwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian,\nZhaoyang Liu, et al. Expanding performance boundaries\nof open-source multimodal models with model, data, and\ntest-time scaling. arXiv preprint arXiv:2412.05271 , 2024. 1\n[7]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. In CVPR ,\npages 24185–24198, 2024. 1\n[8]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality. See\nhttps://vicuna. lmsys. org (accessed 14 April 2023) , 2(3):6,\n2023. 2, 5\n10\n--- Page 11 ---\nTable 5. Numerical results of Fig. 4.\nBenchmark MethodLLaV A-NeXT-Vicuna-7B LLaV A-NeXT-Mistral-7B Qwen2-VL-7B-Instruct\n2048 1024 512 256 128 64 2048 1024 512 256 128 64 2048 1024 512 256 128 64\nDocVQASparseMM 0.6841 0.6837 0.6811 0.6784 0.6677 0.6377 0.6310 0.6272 0.6227 0.6163 0.6082 0.5756 0.9394 0.9392 0.9394 0.9345 0.9154 0.8493\nSnapKV 0.6845 0.6807 0.6709 0.6430 0.5906 0.4977 0.6365 0.6306 0.6215 0.5971 0.5519 0.4726 0.9384 0.9340 0.9194 0.8798 0.8012 0.6652\nPyramidKV 0.6843 0.6812 0.6714 0.6494 0.6030 0.4901 0.6363 0.6225 0.6076 0.5818 0.5425 0.4351 0.9391 0.9343 0.8816 0.8180 0.7394 0.5990\nAdaKV 0.6839 0.6823 0.6753 0.6526 0.6064 0.5411 0.6358 0.6299 0.6174 0.5957 0.5592 0.4872 0.9392 0.9330 0.9201 0.8841 0.8121 0.6847\nRandom 0.6834 0.6791 0.6646 0.6340 0.5816 0.4868 0.6326 0.6217 0.5971 0.5592 0.4973 0.4206 0.9275 0.9015 0.8534 0.7681 0.6963 0.5102\nOCRBenchSparseMM 0.519 0.522 0.528 0.523 0.501 0.478 0.523 0.518 0.512 0.519 0.507 0.462 0.821 0.822 0.821 0.812 0.795 0.743\nSnapKV 0.525 0.518 0.510 0.461 0.412 0.340 0.529 0.517 0.500 0.450 0.390 0.319 0.819 0.813 0.801 0.773 0.719 0.624\nPyramidKV 0.525 0.524 0.502 0.476 0.409 0.312 0.528 0.512 0.489 0.440 0.394 0.290 0.820 0.814 0.776 0.739 0.682 0.563\nAdaKV 0.524 0.517 0.508 0.484 0.430 0.351 0.529 0.520 0.502 0.451 0.404 0.328 0.819 0.812 0.796 0.778 0.710 0.621\nRandom 0.523 0.516 0.500 0.458 0.397 0.320 0.521 0.507 0.451 0.399 0.354 0.261 0.811 0.794 0.760 0.704 0.633 0.489\nTextVQASparseMM 0.6499 0.6492 0.6474 0.6470 0.6417 0.6312 0.6555 0.6547 0.6531 0.6505 0.6474 0.6281 0.8213 0.8215 0.8203 0.8218 0.8164 0.7719\nSnapKV 0.6488 0.6474 0.6408 0.6229 0.6010 0.5616 0.6565 0.6541 0.6503 0.6345 0.6103 0.5712 0.8213 0.8212 0.8204 0.8031 0.7746 0.6990\nPyramidKV 0.6487 0.6483 0.6410 0.6277 0.6040 0.5502 0.6566 0.6490 0.6430 0.6285 0.6088 0.5467 0.8218 0.8218 0.8076 0.7774 0.7440 0.6547\nAdaKV 0.6482 0.6486 0.6429 0.6199 0.5988 0.5609 0.6566 0.6530 0.6464 0.6289 0.6049 0.5685 0.8213 0.8212 0.8185 0.7985 0.7695 0.7025\nRandom 0.6478 0.6438 0.6373 0.6235 0.6011 0.5653 0.6536 0.6494 0.6358 0.6134 0.5822 0.5400 0.8202 0.8166 0.7943 0.7601 0.6955 0.5852\nChartQASparseMM 0.5480 0.5452 0.5488 0.5392 0.5380 0.5276 0.5280 0.5216 0.5236 0.5188 0.5116 0.4888 0.8152 0.8152 0.8128 0.8160 0.8152 0.8016\nSnapKV 0.5480 0.5536 0.5416 0.5000 0.4527 0.4304 0.5288 0.5236 0.5164 0.5016 0.4752 0.4272 0.8140 0.8144 0.8144 0.8128 0.7964 0.7552\nPyramidKV 0.5488 0.5536 0.5496 0.5304 0.4716 0.4100 0.5272 0.5228 0.5080 0.4920 0.4708 0.4068 0.8140 0.8144 0.8144 0.8088 0.7924 0.7332\nAdaKV 0.5492 0.5540 0.5480 0.4912 0.4576 0.4384 0.5292 0.5224 0.5156 0.5044 0.4780 0.4460 0.8152 0.8156 0.8140 0.8080 0.7964 0.7592\nRandom 0.5480 0.5476 0.5424 0.5304 0.4936 0.4372 0.5272 0.5152 0.5060 0.4764 0.4428 0.3944 0.8152 0.8152 0.8060 0.7876 0.7500 0.6696\nTextCapsSparseMM 0.7320 0.7309 0.7334 0.7284 0.7071 0.5992 0.7067 0.7054 0.6896 0.6795 0.6339 0.5238 1.4697 1.4744 1.4919 1.4915 1.4299 1.0431\nSnapKV 0.7226 0.7167 0.6969 0.6495 0.5642 0.4431 0.7070 0.6969 0.6970 0.6504 0.5579 0.4436 1.4677 1.4744 1.4695 1.3598 1.1424 0.7940\nPyramidKV 0.7237 0.7254 0.6953 0.6491 0.5745 0.4164 0.7061 0.6828 0.6592 0.6230 0.5495 0.4062 1.4694 1.4680 1.2745 1.1151 0.9536 0.5669\nAdaKV 0.7263 0.7273 0.7039 0.6598 0.5923 0.4727 0.7037 0.6953 0.6850 0.6459 0.5664 0.4400 1.4690 1.4650 1.4631 1.3445 1.1461 0.8133\nRandom 0.7297 0.7219 0.6803 0.6268 0.5355 0.4356 0.7065 0.6980 0.6882 0.6472 0.5512 0.4368 1.4690 1.4727 1.4812 1.3824 1.1627 0.8116\nTable 6. Numerical results of Fig. 5.\nMethodMMBench GQA VQAv2\n512 256 128 96 64 48 512 256 128 96 64 48 512 256 128 96 64 48\nSparseMM 81.52 81.44 81.52 81.44 80.33 77.92 64.51 64.52 64.20 63.66 62.48 60.88 75.46 75.46 75.24 75.06 74.58 74.36\nSnapKV 81.52 81.44 79.64 77.75 74.57 73.79 64.53 64.51 63.77 62.38 60.82 59.19 75.38 75.50 75.02 74.32 73.58 71.98\nPyramidKV 81.53 79.64 76.46 74.14 73.45 73.30 63.80 63.47 62.05 60.65 59.41 59.37 75.38 75.30 74.72 73.60 71.60 68.88\nAdaKV 81.52 81.44 79.81 77.83 75.17 73.45 64.52 64.65 63.52 62.55 61.59 59.20 75.40 75.34 75.14 74.08 73.66 72.02\nRandom 81.52 81.36 79.64 77.92 74.22 73.54 64.51 64.38 63.87 62.60 61.00 59.39 75.28 75.32 74.78 74.16 73.36 72.44\n[9]Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin\nWang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wen-\nwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A\npioneering large vision-language model handling resolutions\nfrom 336 pixels to 4k hd. Advances in Neural Information\nProcessing Systems , 37:42566–42592, 2024. 2\n[10] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Win-\nston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring\nlong-chain visual reasoning with multimodal large language\nmodels. arXiv preprint arXiv:2411.14432 , 2024. 2\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024. 2\n[12] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu,\nand Hui Wang. Video-ccam: Enhancing video-language un-\nderstanding with causal cross-attention masks for short and\nlong videos. arXiv preprint arXiv:2408.14023 , 2024. 2\n[13] Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and S Kevin\nZhou. Ada-kv: Optimizing kv cache eviction by adaptive\nbudget allocation for efficient llm inference. arXiv preprintarXiv:2407.11550 , 2024. 3, 4, 5, 6\n[14] GeminiTeam. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530 , 2024. 1\n[15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answering.\nInProceedings of the IEEE conference on computer vision\nand pattern recognition , pages 6904–6913, 2017. 6, 9\n[16] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting\nthe 3d world into large language models. NeurIPS , 36:20482–\n20494, 2023. 2\n[17] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux,\nArthur Mensch, Blanche Savary, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,\nFlorian Bressand, et al. Mixtral of experts. arXiv preprint\narXiv:2401.04088 , 2024. 5\n[18] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park,\nJinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon\nHan, and Seunghyun Park. Donut: Document understanding\n11\n--- Page 12 ---\ntransformer without ocr. arXiv preprint arXiv:2111.15664 , 7\n(15):2, 2021. 4\n[19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\nHao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-\nyuan Li. Llava-onevision: Easy visual task transfer. arXiv\npreprint arXiv:2408.03326 , 2024. 2\n[20] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li,\nWei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave:\nTackling multi-image, video, and 3d in large multimodal\nmodels. arXiv preprint arXiv:2407.07895 , 2024. 2\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. In ICML , pages\n19730–19742. PMLR, 2023. 1\n[22] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh,\nAcyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and\nDeming Chen. Snapkv: Llm knows what you are looking for\nbefore generation. Advances in Neural Information Process-\ning Systems , 37:22947–22970, 2024. 3, 5, 7\n[23] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\nLi Yuan. Video-llava: Learning united visual represen-\ntation by alignment before projection. arXiv preprint\narXiv:2311.10122 , 2023. 2\n[24] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov,\nAndrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi,\nand Song Han. Vila: On pre-training for visual language\nmodels, 2023. 2\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer vision–ECCV 2014: 13th European conference,\nzurich, Switzerland, September 6-12, 2014, proceedings, part\nv 13, pages 740–755. Springer, 2014. 8\n[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In CVPR ,\npages 26296–26306, 2024. 1\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang,\nSheng Shen, and Yong Jae Lee. Llava-next: Improved reason-\ning, ocr, and world knowledge, 2024. 2, 5\n[28] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281 , 2023.\n2, 6, 9\n[29] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models. arXiv\npreprint arXiv:2305.07895 , 2023. 2, 5, 6, 7, 9\n[30] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen\nLu, and Yongming Rao. Oryx mllm: On-demand spatial-\ntemporal understanding at arbitrary resolution. arXiv preprint\narXiv:2409.12961 , 2024. 2\n[31] Zuyan Liu, Benlin Liu, Jiahui Wang, Yuhao Dong, Guangyi\nChen, Yongming Rao, Ranjay Krishna, and Jiwen Lu. Effi-\ncient inference of vision instruction-following models with\nelastic cache. In European Conference on Computer Vision ,\npages 54–69. Springer, 2024. 3[32] Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Win-\nston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the\nfrontiers of omni-modal language model. arXiv preprint\narXiv:2502.04328 , 2025. 2\n[33] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua\nLuo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embed-\nding alignment for multimodal large language model. arXiv\npreprint arXiv:2405.20797 , 2024. 2\n[34] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning.\narXiv preprint arXiv:2203.10244 , 2022. 6, 9\n[35] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocvqa: A dataset for vqa on document images. In Proceed-\nings of the IEEE/CVF winter conference on applications of\ncomputer vision , pages 2200–2209, 2021. 2, 5, 6, 9\n[36] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowd-\nhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada\nPal, Jean-Christophe Burie, Cheng-lin Liu, et al. Icdar2019\nrobust reading challenge on multi-lingual scene text detection\nand recognition—rrc-mlt-2019. In 2019 International confer-\nence on document analysis and recognition (ICDAR) , pages\n1582–1587. IEEE, 2019. 8\n[37] OpenAI. Openai gpt-3.5 api. OpenAI API , 2023. 1\n[38] OpenAI. Gpt-4v(ision) system card. OpenAI Blog , 2023. 2\n[39] OpenAI. Hello gpt-4o — openai. OpenAI Blog , 2024. 1\n[40] QwenTeam. Qwen2 technical report. arXiv preprint\narXiv:2407.10671 , 2024. 2\n[41] QwenTeam. Qwen2-vl: To see the world more clearly. Wwen\nBlog , 2024. 2, 5\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nInICML , pages 8748–8763. PMLR, 2021. 1, 2, 3\n[43] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Aman-\npreet Singh. Textcaps: a dataset for image captioning with\nreading comprehension. In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part II 16 , pages 742–758. Springer, 2020.\n6, 9\n[44] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xin-\nlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\nTowards vqa models that can read. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition , pages 8317–8326, 2019. 2, 6, 7, 9\n[45] Qwen Team. Qwen2.5: A party of foundation models, 2024.\n1\n[46] Qwen Team. Qwen2.5-vl, 2025. 1\n[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste\nRozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 , 2023. 2\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\n12\n--- Page 13 ---\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 , 2023. 2\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 2, 5\n[50] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han,\nand Mike Lewis. Efficient streaming language models with\nattention sinks. arXiv preprint arXiv:2309.17453 , 2023. 3\n[51] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. arXiv preprint\narXiv:2403.11703 , 2024. 2\n[52] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui,\nHongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He,\net al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv\npreprint arXiv:2408.01800 , 2024. 2\n[53] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu,\nHaowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-\nowl2: Revolutionizing multi-modal large language model\nwith modality collaboration. In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages\n13040–13051, 2024. 1\n[54] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in the wild.\nJournal of Computer Science and Technology , 34(3):509–521,\n2019. 8\n[55] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 11975–11986, 2023. 1, 2, 3\n[56] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen,\nLianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian,\nChristopher R ´e, Clark Barrett, et al. H2o: Heavy-hitter\noracle for efficient generative inference of large language\nmodels. Advances in Neural Information Processing Systems ,\n36:34661–34710, 2023. 3\n13",
  "text_length": 57794
}