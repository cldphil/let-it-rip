{
  "id": "http://arxiv.org/abs/2505.24870v1",
  "title": "GenSpace: Benchmarking Spatially-Aware Image Generation",
  "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.",
  "authors": [
    "Zehan Wang",
    "Jiayang Xu",
    "Ziang Zhang",
    "Tianyu Pan",
    "Chao Du",
    "Hengshuang Zhao",
    "Zhou Zhao"
  ],
  "published": "2025-05-30T17:59:26Z",
  "updated": "2025-05-30T17:59:26Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24870v1"
}