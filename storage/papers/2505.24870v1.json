{
  "id": "http://arxiv.org/abs/2505.24870v1",
  "title": "GenSpace: Benchmarking Spatially-Aware Image Generation",
  "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.",
  "authors": [
    "Zehan Wang",
    "Jiayang Xu",
    "Ziang Zhang",
    "Tianyu Pan",
    "Chao Du",
    "Hengshuang Zhao",
    "Zhou Zhao"
  ],
  "published": "2025-05-30T17:59:26Z",
  "updated": "2025-05-30T17:59:26Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24870v1",
  "full_text": "arXiv:2505.24870v1 [cs.CV] 30 May 2025GenSpace: Benchmarking Spatially-Aware Image Generation Zehan Wang1∗, Jiayang Xu1∗, Ziang Zhang1, Tianyu Pang2, Chao Du2, Hengshuang Zhao3, Zhou Zhao1 1Zhejiang University;2Sea AI Lab;3The University of Hong Kong https://github.com/SpatialVision/GenSpace Right viewFacing to leftSide by Side, from right From camera’s view,rabbit on fox’s leftFrom bird’s view,lion on bird’s leftFace to Face Longer with 0.5m1mintervalShot from 1maway Move camera to backRotate facing left Add women, side-by-side, oppositeAdd dog on man’s left, from man’s viewAdd dog on man’s left, from camera’s viewMove camera to women’s back Move chair 1mawayMove camera 1mcloserChair, 0.2mwider Text-to-Image GenerationInstruction-based Image Editing Spatial PoseSpatial RelationSpatialMeasurement Figure 1: Illustration of GenSpace ’s three basic evaluation dimensions and nine corresponding sub-domains for text-to-image generation and instruction-based image editing. All results shown are generated by GPT-4o. Zoom in for best viewing. Abstract Humans can intuitively compose and arrange scenes in the 3D space for photog- raphy. However, can advanced AI image generators plan scenes with similar 3D spatial awareness when creating images from text or image prompts? We present GenSpace, a novel benchmark and evaluation pipeline to comprehensively assess the spatial awareness of current image generation models. Furthermore, standard evaluations using general Vision-Language Models (VLMs) frequently fail to cap- ture the detailed spatial errors. To handle this challenge, we propose a specialized evaluation pipeline and metric, which reconstructs 3D scene geometry using mul- tiple visual foundation models and provides a more accurate and human-aligned metric of spatial faithfulness. Our findings show that while AI models create visually appealing images and can follow general instructions, they struggle with specific 3D details like object placement, relationships, and measurements. We summarize three core limitations in the spatial perception of current state-of-the-art image generation models: 1) Object Perspective Understanding, 2) Egocentric- Allocentric Transformation and 3) Metric Measurement Adherence, highlighting possible directions for improving spatial intelligence in image generation. ∗Equal Contribution. Preprint. 1 Introduction When photographing, humans often start by thoughtfully arranging both the objects and the camera within the 3D space. Spatial awareness in real-world photography involves imagining the 3D position and orientation of individual objects, and mentally understanding their spatial relationships, quantitatively or qualitatively. For humans, this type of spatial awareness often happens intuitively, allowing us to compose and capture well-structured photographs [11, 1, 63]. On the other hand, image generation models have made remarkable progress in recent years, from diffusion models (Stable Diffusion [ 47] and FLUX [ 26]) to the latest unified generative methods (Gemini-2.0-flash [ 16] and GPT-4o [ 38]). These models have demonstrated increasingly powerful capabilities in producing realistic and visually appealing images. However, the spatial awareness remains under-explored, despite its crucial role in controllable generation [ 22,68,65], artistic creation [40, 55, 61], and AR/VR applications [14, 36, 4, 62]. To address this gap, we propose GenSpace, a new benchmark and evaluation pipeline to assess the spatial awareness of current image generation models. To provide context for our work, we begin by outlining the Problem Scope andTaxonomy of spatial awareness of image generation: Problem Scope: Aligning with the trend of unified generation models, we comprehensively assess how well these models understand and follow spatial information from text prompts and reference images. Our evaluation covers both text-to-image generation and instruction image editing tasks. Taxonomy: Grounded in the real-world photographic composition process, we systematically catego- rize spatial awareness capabilities into three dimensions of increasing difficulty: •Level 1: Spatial Pose - Understanding the 3D position and orientation (6 degrees of freedom) of objects and the camera in the scene. •Level 2: Spatial Relation - Reasoning about how objects are positioned relative to each other (spatial layout) and considering different viewpoints (egocentric vs. allocentric). •Level 3: Spatial Measurement - Interpreting quantitative spatial details provides precise controllability, such as object sizes, object intervals, and camera’s shooting distance. Given this problem formulation, a non-trivial challenge arises in how to effectively evaluate the spatial faithfulness of generated images. While previous image generation benchmarks [ 12,35,21,20] have relied on vision-language models (VLMs) [ 30,3,16,38] to assess the alignment of generated images with text prompts, recent works [ 44,7,8,34] show that even the most powerful VLMs still have limitations in spatial reasoning and precise measurement. To overcome these limitations, we present a novel, automated evaluation pipeline specifically designed to assess spatial faithfulness in images. Our approach utilizes the spatial perception abilities of several visual foundation models (including object detection [ 32], object segmentation [ 24,45], depth estimation [ 59,60], orientation estimation [ 56], and camera intrinsic calibration [ 69]) to recover the basic spatial pose of each object and reconstruct the 3D scene geometry in the image. We validate our evaluation framework on manually calibrated samples. Our novel evaluation pipeline&metric, leveraging powerful vision foundation models, demonstrates a significant improvement over general- purpose VLMs. To comprehensively assess model capabilities, we curate 1,800 textual prompts for text-to-image generation, along with 1,800 source image and editing instruction pairs for image editing. We evaluate diverse leading models, including 7 open models and 5 closed models. Our findings reveal that current image generation models often struggle to understand and follow spatial information in text or reference images. In summary, our contributions are threefold: •We propose to benchmark spatial awareness capabilities of image generation models, grounded in real-world photography. It covers 3 dimensions and 9 sub-domains of spatial awareness, and considers both text-to-image generation and image editing tasks. •We develop a novel evaluation pipeline&metric capable of analyzing complex spatial states within generated images. We demonstrate that combining current vision foundation models in this pipeline achieves higher alignment with human spatial perception. 2 •We evaluate several leading specialized and unified generative models using our benchmark, revealing significant limitations and room for improvement in spatial awareness. 2 Related Work 2.1 Image Generative Models With the development of diffusion models [ 52,29], Stable Diffusion [ 47] and SDXL [ 42], achieve impressive results by utilizing the U-Net diffusion model and extensive pre-trained datasets [ 49]. Their success inspires lots of subsequent research [ 6,18]. Afterwards, progress in transformer architectures [ 41,53] and post-training strategies [ 31,25,54] led to bigger and stronger models. Recent methods like SD-3 [ 10], FLUX [ 26], HunYuan-DiT [ 27], and Seedream [ 13] can generate realistic and aesthetically pleasing images from textual prompts. More recently, the release of GPT- 4o  has drawn significant attention to unified models, which can process both text and image inputs, naturally unifying generation and editing tasks, demonstrating a leading position in the field. 2.2 Benchmarking Image Generation Initially, image generation models are evaluated using metrics such as Fréchet Inception Distance (FID) [ 19], Inception Score (IS) [ 48], and CLIPScore [ 43]. However, these metrics fall short in assessing complex image-text alignment and subjective attributes. Therefore, more targeted benchmarks and evaluation methods [ 28,51,20,15] for image generation are proposed. T2I- CompBench [ 20,21] and GenEval [ 15] are object-centric benchmarks, focusing on fundamental object existence, attributes, and quantity, employing object detectors and VLMs for automated scoring. Furthermore, other benchmarks like PhyBench [ 35], Commonsense-T2I [ 12], and WISE [ 37] utilize advanced VLMs [3, 38] to evaluate the understanding of physics and commonsense knowledge. However, the ability to plan and understand 3D scene layouts—akin to human perception in photog- raphy—remains relatively underexplored. Our work aims to address this gap by comprehensively and accurately evaluating the spatial awareness capabilities of current image generative models. 2.3 3D Spatial Understanding Understanding spatial arrangements of images in 3D space is crucial for accurately interpreting complex visual environments. However, recent studies [ 44,7,8,34] show that even leading general- purpose VLMs struggle with spatial perception and reasoning. To address this, SpatialVLM [ 7] and SpatialRGPT [ 8] construct more specialized training data. However, this specialized data still ignores object poses and their allocentric spatial relationships, thereby restricting the models’ capabilities. On the other hand, many benchmarks have emerged to highlight the core challenges of MLLM spatial intelligence. 3DSR-Bench [ 34] examines issues related to camera viewpoint and object orientation. Thinking in Space [ 58] investigates spatial memory and reasoning in videos, covering aspects like quantitative distance measurement and egocentric-allocentric transformations. COMFORT [ 67] explores how different reference systems influence the description of spatial relationships. 3 GenSpace To thoroughly evaluate spatial awareness in image generation, we define three hierarchical dimensions of spatial awareness, from simple to complex (Sec. 3.1). To specifically investigate how models understand spatial information in images and text, and align with the trend towards unified models, we build benchmarks for both text-to-image generation and instruction-based image editing (Sec. 3.2). Finally, we introduce a specialized evaluation pipeline with stronger spatial understanding and reasoning abilities to provide more reliable metrics (Sec. 3.3). 3.1 Evaluation Dimension We focus on three dimensions of spatial awareness (Basic Pose →Qualitative Relation →Quantitative Measurement), grounded in real-world photography. Below, we dive into each dimension and the corresponding sub-domain questions they includes. 3 Sub-domain Prompt Template Object Pose \"<obj> is facing { Forward / Backward / Left / Right } to the viewer.\" Camera Pose \"{ Front / Back / Left / Right } view of <obj>\" Complex Pose \"<obj1> and <obj2>, side-by-side, shot from <obj1>’s { Front / Back / Left / Right }\" Egocentric \"From the camera’s perspective, <obj1> is { in Front of / Behind / to the Left of / to the Right of } <obj2>\" Allocentric \"From the <obj2>’s perspective, <obj1> is { in Front of / Behind / to the Left of / to the Right of } <obj2>\" Intrinsic \"<obj1> and <obj2>, { Side-by-Side, Same direction / Side-by-Side, Ppposite / Face-to-Face / Back-to-Back }\" Object Size \"Two <obj>, one is { Bigger / Taller / Longer / Wider } than another with { N} m.\" Object Distance \"<obj1> separated from <obj2> by { 0.5 / 1.0 / 1.5 / 2.0 } m\" Camera Distance \"<obj>, captured from { 1.0 / 2.0 / 3.0 / 4.0 } m\" Table 1: Prompt templates for text-to-image generation. \"<obj>\" represents a category name. For each sub-domain, there are 4 distinct templates with different spatial descriptions (i.e., \"{ Option 1 / Option 2 / Option 3 / Option 4 }\"). The numerical value \"{ N}\" is set to a value appropriate for size, height, length, and width. The prompts are simplified for conciseness. 3.1.1 Spatial Pose We consider the control of spatial pose, for both objects and the camera, as the most basic spatial concept in image generation. This fundamental capability involves generating objects in a specific orientation or rendering the scene from a particular viewpoint. To evaluate this basic pose control, we use 3 sub-domains of questions: Object Pose. This test checks if the model can generate a single object in the specific pose requested in the text instruction. We focus on single objects to isolatedly assess the basic spatial understanding for various object types without other spatial complexities. Camera Pose. Controlling the camera viewpoint is another basic spatial skill, similar to object pose. For this test, we use the same single-object scenario. We modify the prompt to describe the desired camera viewpoint (e.g., \"front view\", \"right view\") instead of the object’s orientation. Complex Pose. Controlling the camera in scenes with multiple objects is significantly more challeng- ing than in single-object scenarios. This skill is essential for applications like novel view synthesis. Our complex pose test evaluates a model’s ability to envision specific camera views in multi-object scenes while following or preserving the relative positions and relationships between objects. 3.1.2 Spatial Relation Beyond understanding basic object and camera pose, a more advanced challenge in controllable image generation is interpreting the qualitative spatial relationships between multiple objects. Although previous benchmarks [ 51,28,20,15] have explored similar tasks, they often default to camera-centric perspectives when describing spatial relations. This overlooks the ambiguity of undefined reference systems, causing confusion in real-world descriptions. To reduce this ambiguity and support more use cases, we focus on 3 sub-domains of object relation: Egocentric (Camera-centered), Allocentric (Object-centered), and Intrinsic (View-agnostic) Egocentric Relation. The egocentric descriptions from the viewpoint of the camera or observer are generally intuitive for humans. Previous benchmarks only consider the simple spatial terms like \"on the right\" (meaning the object is on the right side of the image). However, relying on this unspecific reference system causes ambiguity in practical applications. By explicitly defining the observational viewpoint, we eliminate ambiguity in describing spatial relationships. Allocentric Relation. While egocentric descriptions are common, they don’t cover every situation. Spatial relationships are also often described from the viewpoint of another object within the scene (i.e., allocentric viewpoint). An example prompt is \"a model leaning against the right door of a car.\" Our evaluation examines the ability to understand the perspective of different objects and reasons involving the egocentric-allocentric transformation. Intrinsic Relation. There are also spatial descriptions that are independent of any specific viewpoint. These use particular terms to define the intrinsic relationship between objects, such as \"side by side\" or \"back to back.\" We also assess the model’s comprehension of how different objects relate to each other under these view-independent conditions. 4 Sub-domain Instruction Template Object Pose \"Rotate the <obj> to face { Forward / Backward / Left / Right } relative to the viewer\" Camera Pose \"Show the { Front / Back / Left / Right } view of <obj>\" Complex Pose \"Move the camera to the { Front / Back / Left / Right } of <obj1>\" Egocentric \"Add <obj new> {in Front of / Behind / to the Left of / to the Right of } <obj>, from the camera’s perspective\" Allocentric \"Add <obj new> {in Front of / Behind / to the Left of / to the Right of } <obj>, from the <obj>’s perspective\" Intrinsic \"Add <obj new> near <obj>, { Side-by-Side, Same direction / Side-by-Side, Opposite / Face-to-Face / Back-to-Back }\" Object Size \"Change the size of <obj>, make it { Bigger / Taller / Longer / Wider } by { N} m\" Object Distance \"Move <obj> 1m { Forward / Backward / Left / Right }\" Camera Distance \"Change camera distance: move 1m { Forward / Backward / Left / Right }\" Table 2: Instruction templates for instruction-based image editing. We manually curate feasible source images for each instruction to ensure that the edits are meaningful. 3.1.3 Spatial Measurement Advancing this, the generation of images incorporating precise spatial measurements is a highly desirable feature for achieving controllable and spatially-aware image synthesis. Our assessment focuses on the model’s proficiency in generating or adjusting images according to 3 fundamental types of spatial measurement: Object Size, Object Distance, and Camera Distance. Object Size. We evaluate the model’s ability to understand and control the quantitative 3D size of objects, including their length, width, height, and overall size. Since estimating the real-world size of an object from a single image (monocular) is often ambiguous, we primarily focus on the model’s capacity to comprehend relative sizes between objects. Object Distance. We assess the model’s understanding and application of specific distances between objects, which allows for more precise control over spatial relationships. This includes creating scenes where objects are a specific distance apart or repositioning objects by a specific amount. Camera Distance. We evaluate the model’s ability to understand camera position in terms of distance and to visualize how objects and scenes would appear if captured from different distances. This means accurately showing changes in perspective, visible detail, and the relative size of objects. 3.2 Benchmark Construction With the development of unified image generation models [ 16,38], advanced visual generative systems now support mixed image-text inputs, enabling both text-to-image generation and instruction- based image editing within a single framework. To align with this trend and explore the model’s spatial awareness over both images and text inputs, we build a benchmark around the key dimensions described above, covering both text-to-image generation and instruction-based image editing tasks. Prompt Generation. First, we create specific prompt templates for testing each sub-domain. (Tab. 1 provides text prompts for text-to-image generation, while Tab. 5 shows instructions for image editing.) Although the generation and editing tasks are different, our prompts and instructions are designed to evaluate similar spatial awareness capabilities across all 9 sub-domains. Task1: Text-to-image Generation. Prompts for this task describe the spatial relationships between objects and the camera. We use 50 common object categories that have distinct orientations (e.g., car, person, and chair). To make our benchmark more diverse and natural, we use LLM to rephrase these templated prompts into more human-like language while keeping the original meaning, for example, \"Back view of a fox\" to \"There is a fox. This image provides a clear view of the rear portion of a fox\". Task2: Instruction-based Image Editing. Instructions aim to change the spatial information of objects or the camera in existing images. We manually select source images for each sub-domain from both model-generated images and the internet. The object name in the instruction corresponds to the main object in each image. The instructions are also rewritten by an LLM for naturalness. Finally, humans check all image-instruction pairs to ensure they are clear, correct, and relevant. Statistic. Each broader capability dimension is divided into 3 sub-domains. For each sub-domain, we design 4 prompt templates and collect 50 samples per template, yielding 200 samples per sub-domain. Summing up, this results in 1,800 samples for each task, totaling 3,600 samples across two tasks. 5 Depth Estimation Object Detection& Segmentation Orient Estimation Camera Calibration& UnprojectMetric Depth MapObjectConfidence3DOrientation3DLocationStep0: 3D Information ExtractionStep1: ObjectPresence CheckStep2: SpatialDifference AnylysisStep2: QuantitativeScore MappingYes or NoTargetSpatial StateOrientationDifferenceRelationDifferenceDistanceDifferenceDistanceScoreRelationScoreOrientationScore map map map diff diff diffCorrectnessMetric Figure 2: Overview of our evaluation pipeline & metric. We use advanced visual foundation models to extract 3D information from the generated image. We then measure the difference between the estimated spatial state and the target spatial state specified by the prompt or instruction. The differences are converted into a unified score, which serves as the correctness metric. 3.3 Evaluation Pipeline & Metric Evaluating whether a generated image meets the required spatial information is a non-trivial challenge. As discussed in [ 44,7,8,34], even state-of-the-art VLM often struggle with spatial understanding and reasoning. To address this, we introduce a Spatial Expertise Pipeline that uses diverse visual foundation models to extract 3D information from a single image and jointly assess the spatial correctness of the generated content. Spatial Expertise Pipeline. For a single image, we use multiple visual foundation models to extract 3D information of the objects: Grounded-SAM [ 32,24] for 2D locations, Depth Anything [ 60] for metric depth, OrientAnything [ 56] for 3D orientation. We use WildCamera [ 69] and Perspec- tiveFields [ 23] for camera calibration, and then unproject the images into 3D point clouds. In the canonical 3D world, we recognize the absolute 3D location of each object and camera. Evaluation Metric. By using the 3D information extracted, we can effectively analyze the differ- ences between generated spatial states and desired ones. Based on the difference, we determine the correctness of the generated image. Specifically, our evaluation metric is computed in 3 steps: Step 1 - Object Presence Check: First, we need to confirm if the target objects actually appear in the image. For this, we adopt the method from GenEval [ 15], using object detection to verify the object’s presence. To be precise, we use the Grounded-SAM [ 46] model to detect the object category specified in the prompt within the generated image. If this model fails to detect the target object with its default confidence threshold, the sample is directly marked as a failure and scored as 0. Step 2 - Spatial Difference Analysis: For every textual prompt or editing instruction, we can predefine the intended target spatial state. To measure how much the generated image deviates from this target, we compare the desired spatial state with the 3D information extracted from the synthesized image. This comparison is conducted across three key dimensions: Absolute Orientation Difference ( °), Relation Correctness (Yes/No), and Absolute Relative Distance Error (%). Step 3 - Quantitative Score Mapping: To quantitatively assess the overall correctness of an image’s spatial state, we map these three types of differences into [0,100] scores. Specifically: For orientation, differences within 30 °receive a score of 100. For differences from 30 °to 45 °, the score linearly decreases to 0. For relation, Yes (correct) scores 100, and No (incorrect) scores 0. For distance, relative errors within 33% are scored as 100. For errors from 33% to 44%, the score decreases to 0. Finally, for sub-domain cases that require evaluating multiple conditions simultaneously2(e.g., complex poses often depend on both relation correctness and orientation difference), the respective scores are multiplied together as the final score. 2The detailed spatial state conditions required by each sub-domain are provided in the Appendix. 6 EvaluatorSpatial Pose Spatial Relation Spatial Measurement Ave. Camera Object Complex Ego. Allo. Intri. Size ObjDist CamDist Gemini-2.5-Pro 58.0 56.0 54.0 76.0 50.0 48.0 67.0 47.0 52.0 56.44 GPT-4o 49.0 45.0 54.0 83.0 42.0 46.0 49.0 58.0 53.0 53.22 GPT-o3 45.0 53.0 54.0 92.0 45.0 44.0 52.0 69.0 48.0 55.78 Ours 87.0 86.0 86.0 96.0 73.0 56.0 65.0 71.0 66.0 76.22 Table 3: Human alignment of different evaluators on spatial understanding, showing their accuracy on manually labeled data. The best results are highlighted in bold. 4 Human Alignment of Metrics To verify the effectiveness of our evaluation pipeline and metric, we evaluate how different metrics align with human perceptions. Testing Data. For this purpose, three human annotators collect and label 100 generated images for each sub-domain (50 for text-to-image generation and 50 for image editing). Three human annotators label these images using three categories: \"Correct\", \"Partially Correct\", and \"Incorrect\". Furthermore, to ensure label balance, we maintain a roughly similar distribution of these labels within each set of 100 samples per sub-domain. In summary, we have a total of 900 manually annotated samples. We use this test set to evaluate how different metrics align with human judgments. Alternative Scoring Methods. Recent visual generation benchmarks predominantly use advanced VLMs for evaluation. Therefore, we employ three state-of-the-art VLMs, Gemini-2.5-pro [ 17], GPT-4o [ 38], and GPT-o3 [ 39] as comparative baselines. These VLMs are tasked with analyzing the spatial adherence of generated samples to the given text or image-text prompts and assigning a score from 0 to 100 to each sample. To align the fine-grained continuous scores with this categorical system for comparison, we map scores as follows: 0 to \"Incorrect,\" (0, 100) to \"Partially Correct,\" and 100 to \"Correct.\" Finally, we measure how well each method aligns with human perception by comparing its accuracy against manual human labels. Results. Tab. 3 presents the comparative results of different evaluators’ alignment with human judgment. Overall, our spatial expertise pipeline and corresponding metric demonstrate a stronger correlation with human recognition. Across sub-domains, our method achieves 76.22% average agreement with manual labels, while the most advanced VLM, Gemini-2.5-Pro, achieves only 56.44%. These comparisons highlight the shortcomings of current VLMs in allocentric perspective reasoning and quantitative spatial measurement, underscoring the necessity of our specialized evaluator. 5 Evaluation Results 5.1 Experiment Settings We evaluate 8 models for text-to-image generation: 6 expertise models (SD-1.5 [ 47], SD-XL [ 42], DALL-E 3 [ 5], SD-3.5 [ 10], FLUX.1-dev [ 26], and Seedream-3.0 [ 13]) and 3 unified models (Bagel [ 9],Gemini-2.0-Flash [ 16] and GPT-4o [ 38]). For image editing, we evaluate 6 models: 4 expertise models (InstructPix2Pix [ 6], ICEdit [ 66], Step1X-Edit [ 33], and SeedEdit [ 50]) and 2 unified models (Gemini-2.0-Flash [ 16] and GPT-4o [ 38]). For inference, we employ the official default configuration for each model with fixed random seeds. All experiments are conducted on May 10, 2025. 5.2 Text-to-Image Generation Results In Tab. 4, we present the evaluation results for text-to-image generation tasks. Note that, to provide a reference for models’ general capabilities, we also report the Arena ELO score3in Artificial Analysis [ 2], a model ranking board maintained by lots of human users. Generally, the model rankings on our benchmark align well with human rankings of their overall capabilities, which 3Arena ELO score in May 10, 2025 7 ModelSpatial Pose Spatial Relation Spatial MeasurementAve. RankArena ELO Camera Object Complex Ego. Allo. Intri. Size ObjDis CamDis Expertise Generative Model SD-1.5 31.08 22.10 3.35 52.33 9.80 11.97 24.97 34.36 31.13 7.3 587 SD-XL 33.66 25.03 9.52 46.15 16.38 8.87 23.89 33.76 22.75 7.7 841 DALL-E 3 50.37 46.81 10.92 65.74 17.45 16.63 30.32 41.91 25.69 4.7 937 SD-3.5-L 42.85 31.48 5.90 73.03 11.15 23.55 31.03 33.05 24.83 5.3 1028 FLUX.1-dev 40.42 31.11 12.28 63.39 13.17 19.40 29.16 30.72 31.98 5.4 1046 Seedream-3.0 53.75 61.62 13.70 84.84 18.56 17.02 26.24 30.89 26.13 4.0 1149 Unified Generative Model Bagel 43.34 46.65 13.47 72.10 22.53 19.12 30.77 36.86 29.01 3.6 - Gemini-2.0-Flash 54.77 52.93 10.92 81.85 17.50 14.07 24.61 28.04 31.13 4.9 962 GPT-4o 59.41 62.72 25.01 94.55 21.21 19.08 30.47 41.33 35.19 1.8 1152 Table 4: Benchmarking the spatial awareness within text-to-image generation. ModelSpatial Pose Spatial Relation Spatial MeasurementAve. Rank Camera Object Complex Ego. Allo. Intri. Size ObjDis CamDis Expertise Generative Model InstructP2P 5.02 4.49 0.00 55.71 43.36 8.44 8.33 4.09 3.96 5.9 ICEdit 4.04 5.61 0.23 63.36 42.40 12.52 9.37 5.35 5.46 4.8 Step-Edit-X 3.78 5.70 0.02 70.01 30.06 14.45 18.03 4.65 3.28 5.4 SeedEdit 23.51 16.03 0.78 85.91 34.33 22.49 11.46 7.03 8.80 2.7 Unified Generative Model Bagel 45.37 49.55 0.77 78.51 38.74 17.03 11.11 6.79 4.94 3.4 Gemini-2.0-Flash 46.81 38.12 0.17 81.19 33.88 18.50 7.02 5.04 8.63 4.0 GPT-4o 54.38 49.94 1.80 88.47 33.62 20.55 14.05 9.97 14.45 1.8 Table 5: Benchmarking the spatial awareness within instruction-based image editing. supports the reliability of our benchmark. Specifically, we find that unified generative models perform better than dedicated image generation models with similar ELO scores, possibly due to the general cognitive improvements gained from unifying image and text inputs. Furthermore, we also observe that closed-source models (DALL-E 3, Seedream-3.0, Gemini-2.0-Flash, and GPT-4o) still comprehensively outperform their open-source counterparts. Regarding individual dimensions: 1) For spatial pose, even the best models are only about 60% accurate in understanding basic front, back, left, and right views of objects, and the accuracy drops significantly in complex multi-object scenes. 2) For spatial relations, models handle intuitive egocentric relationships almost perfectly. However, they perform very poorly on egocentric-to- allocentric views transformation or intrinsic relationships understanding. 3) For spatial measurement, almost all models struggle to generate images with specific, quantitative measurements. 5.3 Instruction-based Image Editing Results Tab. 5 includes the evaluation results for instruction-based image editing. Overall, the unified generative model shows similar advantages and limitations across different evaluation dimensions as in the text-to-image generation task, highlighting the connection between our benchmarks for both tasks. Furthermore, GPT-4o performs best across most sub-domains, demonstrating a next level of general generative capability compared to other models, while its absolute spatial reasoning still has significant room for improvement. Notably, we find that some specialized editing models are almost completely ineffective at modifying existing spatial pose and shape. We infer this is due to the limitations of current open-source editing training data. Most existing instruction-based editing data [ 6,57,64] focuses on simple modifications like changing color, adding/removing objects, or style transferring while strictly keeping the overall 8 a) Camera Poseb) Object Posec) Egocentric Relationd) Allocentric Relatione) Object Distancef) CameraDistance FrontBackLeftRightNoneFrontBackLeftRightNoneFrontBackLeftRightNoneFrontBackLeftRightNone0.5m1.0m1.5m2.0mNone1.0m2.0m3.0m4.0mNone Figure 3: Impact of varying spatial conditions on the spatial states of generated samples. The horizontal axis shows the spatial conditions specified in the prompt, with colored bars representing the resulting spatial states for each condition. Statistics derived from GPT-4o’s text-to-image outputs on 6 sub-domains, and analysis of other models in the Appendix. image structure. This focus on simplified tasks limits their capability in spatial structure edits. Unified generative models, in contrast, are trained on broader data for both generation and editing, and tend to regenerate images based on instructions rather than merely modifying them. 5.4 Core Limitations in SoTA Model Beyond analyzing the final scores for model strengths and challenging sub-domains, we also dive into detailed error analysis, visualized in Fig 3. Combined with the overall results of each sub-domain, we summarize the core limitations of current state-of-the-art generative models regarding spatial understanding as follows: Limitation in camera location understanding. Fig 3 aillustrates that generative models struggle to distinguish between side views (e.g., \"right/left view\") of objects. However, directly stating the wanted orientation in the final image (e.g., \"facing right/left\") significantly reduces this confusion, despite describing the same spatial state. This suggests that current models favor direct, object-centric descriptions. Their weakness in indirect spatial reasoning and camera position understanding hurts their capacity for complex prompts and spatial control. Limitation in egocentric-allocentric transformation. From Fig 3 candd, we observe that even models like GPT-4o still limited to egocentric thinking for object relationships. When given allocentric prompts, the model often reverses the left/right condition. This stems from models’ tendency to generate objects facing the viewer, which inverts the object’s left/right relative to the viewer’s. Thus, despite providing explicit object-centric instructions, models still default to a simple egocentric (image-based) understanding. Limitation in understanding metric measurement. Tab. 1 and 5 show that current models are largely unable to understand or adhere to quantitative 3D spatial measurements. This limitation is even more apparent in Fig 3 eandf. Specifying different measurements has little effect on the final generated results. Considering the wide applicability of precisely and quantitatively controlling the image layout, quantitative spatial awareness is a crucial area for improvement. 6 Conclusion In this work, we introduce GenSpace to assess whether current rapidly developing image generative models can control spatial layout in images as human photographers. To this end, we curate benchmarks for spatial awareness over three core dimensions under both text-only and text-image- mixed input. Moreover, we propose a more rigorous evaluation pipeline and metric specifically for spatial understanding. In human studies, our proposed metric exhibits stronger alignment with human perceptions compared to advanced VLMs. After benchmarking several representative image generative models, we find that the unified generation model GPT-4o, while performing best overall, still shows significant limitations in understanding 1) camera location, 2) perspective transformations, and 3) metric measurements. We hope our empirical findings and insights can guide future research towards achieving stronger spatial control in AI image generation. 9 Limitation and Future Work. We are continuously adding results from more image generative models to GenSpace. We will integrate more advanced visual foundation models when they are released, aiming to make our evaluation pipeline more robust. References Rudolf Arnheim. Art and visual perception: A psychology of the creative eye. Univ of California Press, 1972. artificialanalysis.ai. artificialanalysis, 2025. https://artificialanalysis.ai/text-to-image/ arena?tab=leaderboard. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lonni Besançon, Anders Ynnerman, Daniel F Keefe, Lingyun Yu, and Tobias Isenberg. The state of the art of spatial interfaces for 3d visualization. In Computer Graphics Forum, volume 40, pages 293–326. Wiley Online Library, 2021. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18392–18402, 2023. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14455–14465, 2024. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. arXiv preprint arXiv:2406.01584, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Michael Freeman. The Photographer’s Eye Digitally Remastered 10th Anniversary Edition: Composition and Design for Better Digital Photos. Routledge, 2017. Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense? arXiv preprint arXiv:2406.07546, 2024. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Aaron L Gardony, Shaina B Martis, Holly A Taylor, and Tad T Brunyé. Interaction strategies for effective augmented reality geo-visualization: Insights from spatial cognition. Human–Computer Interaction, 36(2): 107–149, 2021. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:52132–52152, 2023. Google. Gemini-2.0-flash, 2025. https://aistudio.google.com/prompts/new_chat?model= gemini-2.0-flash-exp. Google. Gemini-2.5-pro, 2025. https://deepmind.google/technologies/gemini/pro/. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to- prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 10  Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:78723–78747, 2023. Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David F Fouhey. Perspective fields for single image camera calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17307–17316, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015–4026, 2023. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a- pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:36652–36663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366–384. Springer, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892–34916, 2023. Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, and Dong Xu. Improving long-text alignment for text-to-image diffusion models. arXiv preprint arXiv:2410.11817, 2024. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 38–55. Springer, 2024. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: A practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso M de Melo, Jieneng Chen, and Alan Yuille. 3dsrbench: A comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: A physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. Pedro Monteiro, Guilherme Gonçalves, Hugo Coelho, Miguel Melo, and Maximino Bessa. Hands-free interaction in immersive virtual reality: A systematic review. IEEE Transactions on Visualization and Computer Graphics, 27(5):2702–2713, 2021. Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: A world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 11  OpenAI. Gpt-4o, 2025. https://openai.com/index/introducing-4o-image-generation/. OpenAI. Gpt-o3, 2025. https://openai.com/index/introducing-o3-and-o4-mini/. Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7695–7704, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4195–4205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PmLR, 2021. Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:25278–25294, 2022. Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. Shang Hong Sim, Clarence Lee, Alvin Tan, and Cheston Tan. Evaluating the generation of spatial relations in text and image generative models. arXiv preprint arXiv:2411.07664, 2024. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:84839–84865, 2024. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8228–8238, 2024. Ruicheng Wang, Jianfeng Xiang, Jiaolong Yang, and Xin Tong. Diffusion models are geometry critics: Single image 3d editing using pre-trained diffusion priors. In European Conference on Computer Vision, pages 441–458. Springer, 2024. Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv preprint arXiv:2412.18605, 2024. 12  Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371–10381, 2024. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:21875–21911, 2024. Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. Image sculpting: Precise object editing with 3d geometry control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4241–4251, 2024. Difeng Yu, Xueshi Lu, Rongkai Shi, Hai-Ning Liang, Tilman Dingler, Eduardo Velloso, and Jorge Goncalves. Gaze-supported 3d object manipulation in virtual reality. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–13, 2021. Richard D Zakia and John Suler. Perception and IMAGING: Photography as a way of Seeing. Routledge, 2017. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:31428–31449, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3836–3847, 2023. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. arXiv preprint arXiv:2410.17385, 2024. Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan- Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:11127–11150, 2023. Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame a wild camera: in-the-wild monocular camera calibration. Advances in Neural Information Processing Systems, 36:45137–45149, 2023. 13 A Detailed scoring criteria for each sub-domain. In Tab.6, we show the demand for spatial difference for different subdomains. A.1 Spatial Pose For the two subdomains Camera Pose and Object Pose, there is only one object in the image, and the criterion for judging is the orientation of the object, so there is only one difference that needs to be introduced, orientation difference. For Complex Pose, which involves multiple objects, it is necessary to maintain the side-by-side and same orientation relationship of these objects in the text-to-image generation task; while in the image editing task, it is necessary to maintain the general spatial relationship between the objects before and after editing without any major changes. Therefore, in addition to orientation difference, it is necessary to introduce relation difference. A.2 Spatial Relation All three subdomains of this dimension encompass multiple objects. Both orientation and relation differences are required in the scoring stage. A.3 Spatial Measurement In this domain, Distance Difference is necessary in order to measure the length/width/height/volume of an object. Besides, Orientation difference is required for the subdomain of Object Size, because we consider the measure parallel to the front-back direction of an object as length, the measure parallel to the left-right direction of an object as width, and the measure parallel to the top-bottom direction of an object as height. Therefore, for the scoring of Object Size, we need to distinguish the length, width and height of an object by its orientation. Domain Sub-domain Orientation Diff. Relation Diff. Distance Diff. Spatial PoseCamera Pose! % % Object Pose! % % Complex Pose!! % Spatial RelationEgocentric!! % Allocentric!! % Intrinsic!! % Spatial MeasurementObject Size! %! Object Distance % %! Camera Distance % %! Table 6: Difference required for each sub-domain B Impact of Spatial Conditions for More Models Limitation in camera location understanding. As shown in sub-Fig.a and sub-Fig.b of Figs.4 to 10, most models also suffer from a lack of understanding of distinguishing between side views (e.g. \"right/left view\") of objects. Limitation in object location understanding. In addition, the FLUX.1-dev, SD-XL, SD-1.5, and SD-3.5-L show a very clear shortcoming in understanding object orientation (both from the camera pose and the object pose). In most cases they just directly draw the object in front view, no matter what the prompt is. Limitation in egocentric-allocentric transformation. The sub-Fig.c and sub-Fig.d of Figs.4 to 10 illustrate that the other models are almost exclusively limited to egocentric thinking for object relationships, similar to the GPT-4o. 14 Limitation in understanding metric measurement. As with GPT-4o, the other existing models, almost all of them, are incapable of understanding information from quantitative spatial measurements. In particular, the 2m in Object Distance and the 4m in Camera Distance are almost rarely generated by the models, even if prompt tells them to do so, as show in the sub-Fig.e and sub-Fig.f of Figs.4 to 10. Figure 4: Impact of varying spatial conditions on the spatial states of generated samples from Gemini- 2.5-Pro. Figure 5: Impact of varying spatial conditions on the spatial states of generated samples from Seedream-3.0. Figure 6: Impact of varying spatial conditions on the spatial states of generated samples from FLUX.1-dev. Figure 7: Impact of varying spatial conditions on the spatial states of generated samples from DALL- E 3. 15 Figure 8: Impact of varying spatial conditions on the spatial states of generated samples from SD-XL. Figure 9: Impact of varying spatial conditions on the spatial states of generated samples from SD-1.5. Figure 10: Impact of varying spatial conditions on the spatial states of generated samples from SD-3.5-L. 16 C Visualization of Text-to-image Generation Benchmark In this section, we show the 36 small generation tasks (each subdomain contains 4 tasks) that we covered in the Text-to-image Generation Benchmark. Each task contains eight images generated by eight models prompted by the same instruction. The images that match the instruction are labeled with green boxes, those that do not match are labeled with red models, and those that are partially correct are labeled withyellow boxes. There is a kid. The frontal view of the kid. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a elephant. This image provides a clear view of the rear portion of a elephant SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a bird. A side profile of the bird, focusing on its left side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a man. A side profile of the man, focusing on its right side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 11: Visualization of Text-to-image Generation Benchmark on the subdomain Camera Pose 17 There is a couch. For us observers (who did not appear on the way), couch is facing us. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a car. For us observers (who did not appear on the way), car is facing away from us. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a sheep. The sheep is orienting to direction that is parallel to the left of the camera(which did not appear on the way). SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a turtle. The turtle is orienting to direction that is parallel to the right of the camera(which did not appear on the way). SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 12: Visualization of Text-to-image Generation Benchmark on the subdomain Object Pose There are a squirrel and a truck. Both the squirrel and the truck are oriented to the front, standing side by side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a motorcycle and a sheep. Both the motorcycle and the sheep are oriented to the left, standing side by side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a pig and a sheep. Both the pig and the sheep are oriented to the right, standing side by side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a woman and a squirrel. Both the woman and the squirrel are oriented to the back, standing side by side. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 13: Visualization of Text-to-image Generation Benchmark on the subdomain Complex Pose 18 There are a bus and a pig. The bus is in front of the pig. The bus looks closer to us observers (who did not appear on the way). SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a train and a kid. The train is in the background, while the kid looks closer to the camera. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a woman and a doctor. In the observer's line of sight, the woman is distinctly on the left relative to the doctor. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a cat and a sheep. As seen by the observer, the cat lies on the right while the sheep is on the left. Figure 14: Visualization of Text-to-image Generation Benchmark on the subdomain Egocentric Relation There are a cat and a pig. The cat views the pig that is placed squarely in front of it, from the perspective of the cat. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a sheep and a zebra. From the perspective of the sheep, the zebra is located directly behind it. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a horse and a truck. From the perspective of the horse, the truck is directly to its left. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a giraffe and a refrigerator. The refrigerator positioned to the right side of the giraffe, from the giraffe's point of view. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 15: Visualization of Text-to-image Generation Benchmark on the subdomain Allocentric Relation 19 There are a dog and a woman. Two objects, the dog and the woman, standing next to each other, both looking in the same direction. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a fireman and a tiger. The fireman and the tiger placed side by side, each oriented in opposite directions. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a car and a pig. The car and the pig arranged to face each other, creating an engaging interaction between the two. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a giraffe and a elephant. The giraffe and the elephant placed in a back -to-back arrangement. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 16: Visualization of Text-to-image Generation Benchmark on the subdomain Intrinsic Relation There are two car. One car is significantly larger —0.2 times the size of the other. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT-4o There are two woman. Two woman are shown, with one being 20cm higher than the other. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT-4o There are two car. One car is noticeably 50cm longer than its counterpart. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT-4o There are two sheep. Two sheep, one's width is 30cm wider than the other's. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT-4o Figure 17: Visualization of Text-to-image Generation Benchmark on the subdomain Object Size 20 There are a sheep and a cow. The distance between the sheep and the cow is defined as 0.5 meter. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a woman and a clock. The woman and the clock are standing exactly 1 meters apart from each other. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a dog and a horse. The dog is placed 1.5 meters away from the horse. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There are a monkey and a tiger. The monkey maintains a distance of 2 meters from the tiger. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 18: Visualization of Text-to-image Generation Benchmark on the subdomain Object Distance There is a piano. This shot shows a piano, taken from around 1 meter away. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a car. From this angle, the car is visible at a distance of roughly 2 meters. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a woman. A woman is photographed from a viewpoint that is about 3 meters distant. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o There is a motorcycle. A motorcycle is photographed from a viewpoint that is about 4 meters distant. SD-1.5 SD-XL DALL -E 3 SD-3.5-L FLUX.1 -dev Seedream -3.0 Gemini -2.0-Flash GPT -4o Figure 19: Visualization of Text-to-image Generation Benchmark on the subdomain Camera Dis- tance 21 D Visualization of Instruction-based Image Editing Benchmark In this section, we show the 36 small edit tasks (each subdomain contains 4 tasks) that we covered in the Instruction-based Image Editing Benchmark. Each task contained one original image, and six images edited by six models from the same text prompt. The images that match the instruction are labeled with green boxes, those that do not match are labeled with red models, and those that are partially correct are labeled withyellow boxes. There is a man. Obtain the front view of man. There is a dog. Obtain the left -side view of dog. There is a cat. Adjust the perspective to get the right -facing side of the cat. There is a giraffe. Obtain the back view of giraffe. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Figure 20: Visualization of Instruction-based Image Editing Benchmark on the subdomain Camera Pose 22 There is a zebra. Rotate the zebra, making it face forward. There is a bear. Rotate the bear, making it face leftward. Rotate the bear, making it towards the left side, from the perspective of us observer(who did not appear on the way). There is a monkey. Rotate the monkey, making it face rightward. Rotate the monkey, making it towards the right side, from the perspective of us observer(who did not appear on the way). There is a woman. Keep the other backgrounds unchanged and turn woman to face away from us (who did not appear on the way), from the perspective of us observer. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oFigure 21: Visualization of Instruction-based Image Editing Benchmark on the subdomain Object Pose There are a motorcycle and a doctor in the image. View them from the front of the motorcycle. There are a dog, a bear and a pig. Move the camera(the observer who did not appear on the way) so that it looks at them from the rear of the pig. There are a policeman and a cow in the image. Rotate the camera(the observer who did not appear on the way)'s viewing angle so that the camera views them from the left of the policeman. There are a giraffe and a bear in the image. Move the camera(the observer who did not appear on the way) so that it looks at them from the right of the giraffe. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Figure 22: Visualization of Instruction-based Image Editing Benchmark on the subdomain Complex Pose 23 There is a refrigerator. From the perspective of camera(the observer who did not appear on the way), add zebra to be directly in front of refrigerator. There is a dog. From the perspective of camera(the observer who did not appear on the way), add oven to be directly on the left side of dog. There is a computer. From the perspective of camera(the observer who did not appear on the way), add shoe to be directly on the right side of computer. There is a sheep. Position a truck so that it is directly behind sheep, from the perspective of camera(the observer who did not appear on the way). Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oFigure 23: Visualization of Instruction-based Image Editing Benchmark on the subdomain Egocentric Relation There is a dog. From the dog's perspective, add laptop so that it is positioned ahead of dog. There is a fox. From the fox's perspective, add tv so that it is positioned to the left of fox. There is a dog. Place pig on the right -hand side of dog, from the dog's perspective. There is a elephant. From the elephant's perspective, add train to be directly at the back of elephant. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Figure 24: Visualization of Instruction-based Image Editing Benchmark on the subdomain Allocentric Relation 24 There is a elephant. Set a lion alongside the elephant, ensuring they face the same direction and remain side by side. There is a car. Insert a lion next to the car, ensuring both objects are positioned side by side and oriented in the opposite direction. There is a cow. Place a bicycle near the cow. Make sure they are facing each other. There is a horse. Set a cow behind the horse, ensuring they are both back -to- back to each other. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oFigure 25: Visualization of Instruction-based Image Editing Benchmark on the subdomain Intrinsic Relation Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oThere is a elephant. Increase the size of elephant to 1.2 times its original size. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oThere is a dog. Raise dog's height by 20cm Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oThere is a lion. Enlarge the lion in length by 50cm. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oThere is a piano. Enlarge the piano in width by 40cm. Figure 26: Visualization of Instruction-based Image Editing Benchmark on the subdomain Object Size 25 There is a dog. Adjust the position of the dog to be 1 meter nearer to us observer(who did not appear on the way). There is a motorcycle. Set up the motorcycle to shift its location 1 meter away from us observer(who did not appear on the way). There is a woman. Ensure the woman relocates exactly 1 meter to the left, from the perspective of us observer(who did not appear on the way). There is a piano. Ensure the piano relocates exactly 1 meter to the right, from the perspective of us observer(who did not appear on the way). Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4oFigure 27: Visualization of Instruction-based Image Editing Benchmark on the subdomain Object Distance There is a piano. Shift the camera(the observer who did not appear on the way)'s position 1 meter forward to the piano. There is a bedside table. Move the camera(the observer who did not appear on the way) leftward by 1 meter. There is a car. Move the camera(the observer who did not appear on the way) by 1 meter in the right direction. There is a chair. Move the camera(the observer who did not appear on the way) backward by 1 meter from its current position, making it away from the chair. Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Source Image InstructP2P ICEdit Step-Edit-X SeedEdit Gemini -2.0-Flash GPT -4o Figure 28: Visualization of Instruction-based Image Editing Benchmark on the subdomain Camera Distance 26 E More Result on human alignment of different evaluators EvaluatorSpatial Pose Spatial Relation Spatial Measurement Ave. Camera Object Complex Ego. Allo. Intri. Size ObjDist CamDist qwen-vl-max 36.0 58.0 51.0 79.0 48.0 33.0 47.0 56.0 60.0 52.00 claude-3-7-sonnet-thinking 51.0 59.0 48.0 89.0 47.0 39.0 56.0 62.0 60.0 56.78 Table 7: Human alignment of different evaluators on spatial understanding, showing their accuracy on manually labeled data. 27",
  "text_length": 71228
}