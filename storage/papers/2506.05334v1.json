{
  "id": "http://arxiv.org/abs/2506.05334v1",
  "title": "Search Arena: Analyzing Search-Augmented LLMs",
  "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.",
  "authors": [
    "Mihran Miroyan",
    "Tsung-Han Wu",
    "Logan King",
    "Tianle Li",
    "Jiayi Pan",
    "Xinyan Hu",
    "Wei-Lin Chiang",
    "Anastasios N. Angelopoulos",
    "Trevor Darrell",
    "Narges Norouzi",
    "Joseph E. Gonzalez"
  ],
  "published": "2025-06-05T17:59:26Z",
  "updated": "2025-06-05T17:59:26Z",
  "categories": [
    "cs.CL",
    "cs.IR",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05334v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05334v1  [cs.CL]  5 Jun 2025Search Arena: Analyzing Search-Augmented LLMs\nMihran Miroyan∗Tsung-Han Wu∗Logan King Tianle Li Jiayi Pan Xinyan Hu\nWei-Lin Chiang Anastasios N. Angelopoulos Trevor Darrell Narges Norouzi\nJoseph E. Gonzalez\nUC Berkeley\nAbstract\nSearch-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn, fact-checking\nquestions. In this work, we introduce Search Arena , a crowd-sourced, large-scale,\nhuman-preference dataset of over 24,000 paired multi-turn user interactions with\nsearch-augmented LLMs. The dataset spans diverse intents and languages, and\ncontains full system traces with around 12,000 human preference votes. Our analysis\nreveals that user preferences are influenced by the number of citations, even when\nthe cited content does not directly support the attributed claims, uncovering a gap\nbetween perceived and actual credibility. Furthermore, user preferences vary across\ncited sources, revealing that community-driven platforms are generally preferred\nand static encyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by testing\nsearch-augmented LLMs in a general-purpose chat environment and conventional\nLLMs in search-intensive settings. We find that web search does not degrade and\nmay even improve performance in non-search settings; however, the quality in\nsearch settings is significantly affected if solely relying on the model’s parametric\nknowledge. We open-sourced the dataset to support future research in this direction.\nSearch Arena /githubCode\n Dataset\n1 Introduction\nLarge Language Models (LLMs) have become a popular interface for human–AI interaction, supporting\ninformation seeking and task assistance through natural, multi-turn dialogue. However, the capabilities\nof these models are constrained by their reliance on static training data, which prevents them from\neffectively handling time-sensitive questions, emerging topics, or niche domains. Search-augmented\nLLMs aim to bridge this gap by retrieving and using live web data during inference. Access to search\nenables LLMs to provide up-to-date, domain-specific, and factually verifiable responses [ 24,33].\nRecent developments [16, 41, 47] also reflect the growing interest in search-augmented LLMs.\nDespite rapid progress in developing search-augmented LLMs, our understanding of how users\ninteract with these systems—what they ask, how they engage in multi-turn dialogue, and what they\nexpect in return—remains limited. Existing datasets capture interactions with either standalone LLMs\n[10,70,71] or traditional web search engines [ 8,13]. However, search-augmented LLMs represent\na hybrid interface different from both: they not only retrieve information through web search but also\nrely on their reasoning and conversational capabilities. The most widely used datasets for evaluating\nthese systems (SimpleQA [ 60] and BrowseComp [ 61]) primarily consist of single-turn, monolingual,\nfact-based queries and are relatively small in scale (typically ≤5k queries). As shown in Figure 1,\n1*Equal contribution.\nPreprint.\n--- Page 2 ---\nTable 1: Comparison of Search Datasets. Unlike prior datasets such as SimpleQA [ 60] and\nBrowseComp [ 61], which consist of static, English-only, single-turn fact-seeking queries, Search Arena\nevaluates models in diverse, open-ended, multilingual, and multi-turn settings. We release 24,069\nconversations including 12,652 human preference votes. Further analyses are provided in Section 2.\nDataset #Convs #Langs Multiturn Answer/Judge Conversation Properties Metadata\nSimpleQA 4,326 1 (EN) No Short ground truth Expert-written short factual queries Verified supporting URLs, topic tags\nBrowseComp 1,266 1 (EN) No Short ground truth Expert-written challenging prompts\nwith detailed constraintsTopic tags\nSearch Arena 24,069 71 Yes Human preference Open-ended, crowd-sourced prompts\nacross diverse intents and topicsRetrieved URLs, full model traces,\nuser intent, and topic tags\nS w i t c h  2  p r i c e\nF a c t u a l  L o o k u p  S u m m a r i z e  t h e  t o p  5  n o t a b l e  p r i v a c y - r e l a t e d  n e w s  o r  \ni n c i d e n t s ,  o r  n e w  l e g i s l a t i o n s  r e p o r t e d  i n  J a n u a r y  2 0 2 5 . . .\nI n f o  S y n t h e s i s  \nR e c o m m e n d a t i o n  W h a t  a r e  t h e  b e s t  r u n n i n g  s h o e s  f o r  b e g i n n e r s  t h a t  a r e  \na f f o r d a b l eI n t r o d u c e  D e n v e r ' s  h i s t o r y  t o  a  n o n  U S  p e r s o n\nE x p l a n a t i o n  I  n e e d  t o  r e p r e s e n t  t h e  c h r o n i c l e  u n i f i e d  d a t a  m o d e l  \n( u d m )  i n  p y t h o n .  r e s e a r c h  t h e  c h r o n i c l e  u d m  f i e l d  l i s t . . .\nA n a l y s i s  \nH o w  d o  I  b l o c k  a  d o m a i n  b u t  a l l o w  a  s p e c i f i c  b r a n c h  i n  \nu B l o c k  O r i g i n ?\nG u i d a n c e  W r i t e  a  s a t i r i c a l  e n c y c l o p e d i a  a r t i c l e  o n  S a l v a d o r  B o r r e g o\nC r e a t i v e  G e n e r a t i o n  C o u l d  y o u  s u m m a r i z e  < w e b s i t e > ?\nT e x t  P r o c e s s i n g  \n1 9 . 3 %\n1 8 . 6 %\n1 0 . 9 % 1 0 . 8 %1 0 . 4 %\n9.5%9.1%\n3.1%\nFactual Lookup Info SynthesisAnalysis\nRecommendationExplanation\nCreative GenerationGuidance\nText Processing01000200030004000Count\nFigure 1: ( Left) Nine intent categories with representative examples (truncated). In-the-wild user\nprompts are often ambiguous and require real-time web retrieval. ( Right ) Distribution of intents across\nuser prompts. The majority of queries require more than a simple factual lookup and range from infor-\nmation synthesis to creative content generation. The Other category is excluded from the visualizations.\nfact-checking accounts for only one-fifth of real-world user queries; the majority of user prompts,\nsuch as seeking analyses, recommendations, or problem-solving guidance, require a combination\nof factual retrieval, reasoning, and open-ended dialogue. User expectations also extend beyond factual\ncorrectness: preferences can be shaped by the number, relevance, and credibility of citations, as well\nas the presentation style of responses.\nTo address these gaps, we crowd-sourced the first large-scale human-preference dataset of in-the-wild\nuser interactions with search-augmented LLMs. We developed Search Arena, an open evaluation and\ndata collection platform that presents anonymized side-by-side model outputs in multi-turn settings\nand collects human votes. During the seven-week deployment period, we gathered and publicly\nreleased 24,069 conversations, along with 12,652 paired preference judgments. The dataset spans\n11,650 users across 136 countries, 13 models, around 70 languages (including 11% multilingual\nprompts), and over 5,000 multi-turn interactions. We also introduce a user intent taxonomy in the\ncontext of search-enabled human-AI interactions. As detailed in Table 1 and Section 2, Search Arena\nprovides a broad coverage across linguistic and intent features.\nWe not only analyze user prompts to search-augmented LLMs, but also their preferences. We model\nuser preferences through the Bradley–Terry model [ 5,10,55] and study how different response\ncharacteristics interact with user judgments. We find that reasoning, larger search context window,\nand longer responses are positively associated with user preferences. Since citations are central to\nthe trustworthiness of web-grounded outputs, we also examine citation features. Our results show\nthat users prefer responses with a higher number of cited sources (Figure 4). In addition, users prefer\nmodel responses citing tech-related platforms, community blogs, and social networks, but less favor\nWikipedia (Figure 6). While correctly attributed citations, as expected, positively interact with user\npreferences ( β=0.285), we also observe a positive association between user preference and the number\nof irrelevant citations ( β= 0.273). This finding raises concerns that users may be overly influenced\nby the presence of citations, even when they do not support the associated claims.\n2\n--- Page 3 ---\nWe also investigate how search-augmented and non-search models perform across different settings by\ndeploying a non-search LLM in the Search Arena, and a search-augmented LLM in the general-purpose\nText Arena [ 10]. Our results show that conventional LLMs underperform in search-intensive settings\n(p-value =0.009). However, the search-augmented models have comparable performance in general\nchat settings to regular LLMs, with improved performance on factual lookup queries ( p-value =0.012)\nand slightly degraded performance on text processing prompts (p-value =0.077).\nIn summary, our contributions are as follows: (1)we release the first large-scale human-preference\ndataset of 24k user conversations with search-augmented LLMs, along with 12k preference votes,\nsystem metadata, user intents, and prompt topics; (2)we present the first analysis of how different\ncharacteristics of search-augmented LLMs interact with human preferences; and (3)we conduct the\nfirst cross-arena evaluation by testing a non-search model in Search Arena and a search-augmented\nmodel in Text Arena, reporting that web search augmentation does not hurt and may improve\nperformance across settings, while the internal parametric knowledge of models alone is not sufficient\nin search-intensive settings.\n2 Human Preference Dataset in Search\nWe launched Search Arena, an open, crowd-sourced evaluation platform for search-augmented LLMs,\non March 18, 2025. Search Arena is implemented as a separate tab within the Chatbot Arena web\napplication [ 10], where users interact exclusively with search-enabled models. The search mode\ninterface encourages more search-intensive queries, as users adjust their expectations compared to\nthe default Text Arena interface. During each session, two anonymous models respond to a user query,\nthe user can then cast a vote for their preferred model response. Details on user interface and potential\nlimitations are reported in Appendix A and Section 5.\nBetween March 18 and May 8, we collected more than 24,000 conversations and 12,000 user votes\nacross 13 models, spanning a range of model configurations (e.g., reasoning models, search context\nsizes, etc.). We release the full dataset, with each record containing model identities, the user vote,\nconversation histories, and system metadata (e.g., reasoning traces, retrieved URLs). Table 1 presents\nkey differences between Search Arena and prior datasets (SimpleQA [ 60] and BrowseComp [ 61]),\nincluding dataset scale, prompt characteristics, and available metadata. In the following subsections,\nwe analyze prompt distributions across linguistic and intent dimensions in comparison to existing\nbenchmark datasets.\n0.5%\n0.6%\n0.7%\n0.8%\n1.4%\n2.2%\n2.6%\n2.6%\n2.7%\n2.8%\n2.8%\n3.1%\n7.0%\n11.8%\n58.3%\n100 1000 10000TurkishPersianPolishIndonesianItalianPortugueseFrenchKoreanVietnameseJapaneseSpanishGermanChineseRussianEnglish\nCount (log scale)\n200 400 600 800 1000 1200 1400\nPrompt Length (words)106\n105\n104\n103\n102\n101\nDensitySearch Arena\nBrowse Comp\nSimpleQA\nSearch Arena mean: 57.1\nBrowse Comp mean: 103.3\nSimpleQA mean: 16.3\nFigure 2: (Left) Search Arena prompt language distribution. The dataset is multilingual, spanning over\n70 languages, with English prompts accounting for 58.3% of the data. (Right) Prompt length distri-\nbution of Search Arena (blue), BrowseComp (purple), and SimpleQA (green). Search Arena prompt\nlengths are more spread out and cover the range of BrowseComp [61] and SimpleQA [60] questions.\n2.1 Linguistic and Conversational Diversity\nSearch Arena was collected from 11,650 users across 136 countries, resulting in substantial linguistic\nand geographic diversity. The prompts span over 70 languages, with 30 represented by at least 10\nconversations. English accounts for 58.3% of the data, followed by Russian (11.8%) and Chinese\n(7.0%). More than 11% of the prompts are multilingual. Figure 2 (Left) shows Search Arena prompt\nlanguage distribution across the top 15 languages.\n3\n--- Page 4 ---\nSince the platform supports multi-turn interactions, 22.4% of conversations in the dataset are\nmulti-turn, typically clarifications or follow-up queries. Furthermore, as shown in Figure 2 (Right),\nSimpleQA prompts are short and fact-oriented (16.3 words on average), while BrowseComp prompts\nare intentionally constructed to be long and constraint-heavy (103.3 words on average). In contrast,\nSearch Arena includes both brief, under-specified queries as well as longer, detailed requests (57.1\nwords on average). Additional details and analysis of linguistic features are provided in Appendix B.\n2.2 Intent Diversity\nExisting search-augmented LLM evaluation datasets focus solely on factuality. To study how\nin-the-wild user prompts from Search Arena differ from SimpleQA [ 60] and BrowseComp [ 61]\nquestions, we apply an LLM-based dataset differencing framework [ 14,72]. We also compare Search\nArena prompts with the prompt distribution of Text Arena [ 10], where users’ expectations of the\nmodels are not influenced by search settings. GPT-4.1 is used for generation and GPT-4.1-mini for\nranking of candidate distinguishing properties; more details on the pipeline are provided in Appendix C.\nThe summaries of the top properties reveal high-level pairwise differences between the datasets:\n•Search Arena vs SimpleQA: Search Arena prompts are broader and more complex, often\nrequiring analysis, synthesis, or creative generation, while SimpleQA prompts are narrowly\nfocused on retrieving specific, factual information with minimal context or interpretation.\n•Search Arena vs BrowseComp: BrowseComp prompts are structured as investigative\nchallenges, often requiring synthesis of clues under specific constraints, whereas Search\nArena prompts prioritize immediate, functional assistance.\n•Search Arena vs Text Arena: Search Arena prompts focus on real-world factual lookup and\ndecision-making support, often involving up-to-date information. In contrast, Text Arena\nprompts focus on problem-solving, programming help, and creative generation.\nThese summaries show high-level differences between prompt distributions across three settings–user\nprompts to search-augmented LLMs (Search Arena), static factuality questions (SimpleQA and\nBrowseComp), and prompts to regular LLMs (Text Arena). We also study Search Arena prompts\nthrough topic modeling and observe diverse and real-time topics, ranging from market analysis to\nhealth discussions (see Figure A7).\nFor a more structured and in-depth analysis, we introduce a taxonomy of user intent categories. While\nprior work has explored user intent classification in general dialogue settings [ 37,50], our work\nfocuses on real-world interactions in search-augmented chat-based settings. The taxonomy includes\nnine categories: Factual Lookup ,Information Synthesis ,Analysis ,Recommendation ,Explanation ,\nCreative Generation ,Guidance ,Text Processing , and Other . We use secondary labels for ambiguous or\nmulti-purpose prompts. Details on the taxonomy design process, as well as descriptions of categories,\nare provided in Appendix B. We scale the annotation to the full dataset using GPT-4.1, with a manually\ntuned prompt seeded on 100 examples and validated on 150 multilingual prompts. The resulting\nCohen’s kappa of 0.812 indicates strong agreement between model- and human-annotated labels. The\nannotation pipeline and validation details are provided in Appendix B.\nFigure 1 shows the resulting intent distribution, along with examples for each category. Factual\nlookup queries account for only 19.3% of user prompts. The remaining prompts require higher-order\ncapabilities, such as synthesis, guidance, or analysis. We also analyze how linguistic features vary\nacross intents; specifically, we find that factual lookup prompts are typically shorter (17.2 words on\naverage), whereas the remaining set is associated with longer, more complex queries (66.7 words on\naverage). Further analyses of intent categories are provided in Appendix B.\n3 Preference Analyses in Search\nWith over 12,000 anonymized human preference votes, Search Arena supports fine-grained analysis of\nhow different response features interact with user preferences. We report model performance through\nboth head-to-head win rates as well as scaled coefficients estimated using the Bradley–Terry model\n[5,10] in Appendix D. To analyze how features interact with user preferences, we follow prior work\n[55] by adding normalized differences between pairwise features to the Bradley-Terry model and\nreporting the fitted coefficients.\n4\n--- Page 5 ---\nFigure 3: (Left) Reasoning trace example, containing multi-document analysis, filtering, and synthesis.\n(Right) Example of a rejected response citing Wikipedia for a sports news question. The preferred\nresponse cited the sports division of a news outlet, containing more up-to-date information.\nIn this section, we first focus on two key feature groups: (1) general features such as model type,\nsearch depth, and response length (Subsection 3.1), and (2) citation-related features, such as number\nof citations, citation sources, and citation attributions (Subsection 3.2). We then study how search and\nnon-search models generalize in search-heavy and general chat scenarios in Subsection 3.3. Additional\ndetails and further analyses are provided in Appendix D and Appendix E.\n3.1 General Features\nReasoning. Reasoning models generally perform better under the Search Arena prompt distribution,\nwith the top three reasoning models achieving over 60.0% average win rates, suggesting that reasoning\nimproves performance in search-augmented chat interactions. Consistent with prior work [ 15], we\nobserve prompt analysis, problem decomposition, and backtracking behaviors in reasoning traces. Ad-\nditionally, we find that reasoning models not only interpret and analyze retrieved content but also rerank\nsources and filter out irrelevant information. One example of a reasoning trace is in Figure 3 (Left).\nSearch Context Size. Models with high search context windows outperform those with smaller search\ncontext. For sonar-pro, the version with high search context has a higher ( p<0.01) average win rate\n(63.9%) compared to the model with medium search context (57.6%). However, the difference is not\nsignificant for GPT-4o models with medium and high search context sizes. This finding indicates that\nmodels with higher search context retrieve more web sources, leading to more preferred responses;\nwe analyze the effect of citations in Subsection 3.2.\nResponse Length. Consistent with findings from prior work [ 10,52,55], we observe that users are\nbiased towards more verbose responses. The Bradley-Terry coefficient corresponding to response\nlength is positive and statistically significant ( βlength =0.334), indicating that users tend to prefer longer\nanswers. The positive correlation between model score and response length is also shown in Figure 4\n(Left). Additionally, Figure 5 (Left) shows the length distribution across eight user intent categories;\nresponses to Factual Lookup prompts are much shorter (168.3 words on average) compared to Creative\nGeneration (422.8 words) and Analysis (393.2 words) prompts. Furthermore, we find that the response\nlength coefficient on Factual Lookup prompts ( βlength, factual = 0.156) is 2.14 times smaller than the\neffect on the full dataset, suggesting that users prefer less verbose answers to Factual Lookup queries\ncompared to other categories.\n3.2 Citation Features\nCitations are central to the trustworthiness of web-grounded outputs in search-enabled scenarios and\nare a unique feature in Search Arena compared to the prior Text Arena [ 10,70]. Therefore, we further\nexamine how these factors influence user preference along three dimensions: number of cited sources,\ntypes of cited sources, and attribution of inline citations to the generated content.\nNumber of Cited Sources. We find a positive and statistically significant coefficient for the number\nof citations ( βcitations =0.209), indicating that users favor responses with more references. The positive\nassociation between model score and response length is shown in Figure 4 (Right). Furthermore,\n5\n--- Page 6 ---\nwe observe that reasoning models cite fewer sources than non-reasoning models with similar\nconfigurations, consistent with our earlier observation that reasoning models filter irrelevant content.\nUnsurprisingly, models with high search context size end up citing more sources in their final response.\nAdditionally, Figure 5 (Right) shows the distribution of citation counts across prompt intent categories.\nNotably, responses to Factual Lookup prompts contain fewer citations (5.7 on average), compared\ntoRecommendation (6.9 citations on average) and Info Synthesis (6.8 citations on average) prompts,\ndue to the broader web coverage needed for the latter.\nTypes of Cited Sources. We categorize domains of retrieved URLs into nine groups (e.g., news,\nWikipedia, social media, tech/code platforms, etc.); categorization details are provided in Appendix E.\nFigure 6 (Left) shows source category coefficient estimates with 95% confidence intervals; we also\ncontrol for the number of citations to account for the bias shown in the previous section. Citing tech-\nrelated platforms (e.g., Stack Overflow), community platforms (e.g., Substack), and social media (e.g.,\nTikTok) are positively associated with user preferences with fitted coefficients equal to βtech= 0.073,\nβcommunity =0.061, and βsocial=0.057, respectively. Surprisingly, citing Wikipedia is negatively correlated\nwith user preferences ( βwiki=−0.071). To interpret the latter result, we inspect rejected model responses\nciting Wikipedia and identify two potential explanations: (1) Wikipedia articles are often very lengthy\nand broad, not directly relevant to a user’s question, and (2) citing Wikipedia is not preferred for\nqueries requiring real-time information. A qualitative example is shown in Figure 3 (Right).\nCitation Attribution. We then study how the correctness of citation-to-claim attribution (i.e., whether\nthe inline citation supports the attributed claim) interacts with user preferences. Formally, for each\nmulti-turn interaction, we decompose model responses into a set of claim-citation pairs (ci,ui), where\ncidenotes a textual claim, and uiis the corresponding inline citation. For each pair, we evaluate\nwhether the webpage content Disupports, is irrelevant to, or contradicts the claim ci. This process\nis automated via an LLM-based pipeline described in Figure 7 through an example. Due to scraping\nchallenges and the high cost of LLM calls, we run the pipeline on roughly 100 English conversations\nper intent category. The resulting output of each conversation is a set of triplet {(ci,ui,ti)}N\ni=1, where\nti∈{Support ,Irrelevant ,Contradict }andNis the total number of claims per conversation. We then\ncompute the number of supporting, irrelevant, and contradicting claims per model response and add\nthem as control covariates in the Bradley–Terry analysis. Implementation details, including scraping\ntools, parsing logic, and validation process, are provided in Appendix E.\nFigure 6 (Right) shows bootstrapped coefficient estimates for the number of supporting, irrelevant, and\ncontradicting claim-citation pairs. While users tend to favor responses with more citations, as shown\nin Figure 6 (Left), the number of contradicting claim-citation pairs does not show a significant effect on\nuser preference. Furthermore, both supporting ( βsupport =0.29) and irrelevant ( βirrelevant =0.27) claims are\npositively correlated with user preference. Thus, users do not distinguish between supporting and irrele-\nvant citations and generally prefer more citations, even if the citations do not directly support the claims.\nUpon inspection, in irrelevant citation cases, models may fabricate connections, cite tangentially related\nsources, or present inferred claims that subtly deviate from the source content. This finding suggests\nthat users may be influenced by the mere presence of citations, rather than their proper attribution to\ngenerated claims. We raise this as an open issue for the community: improving citation attribution is\ncritical to ensure that citation-heavy responses are not misperceived as factual and trustworthy.\n250 300 350 400 450 500 55095010001050110011501200\nAverage Response Length (words)BT Score Estimatescore = 0.378 × length + 945.70\n3 4 5 6 7 8 9 10 11 129501000105011001150\nAverage Number of CitationsBT Score Estimatescore = 17.192 × citations + 939.23\nFigure 4: (Left) Positive relationship between model score and average response length. (Right)\nPositive relationship between model score and average number of citations.\n6\n--- Page 7 ---\n0 100 200 300 400Factual LookupText ProcessingExplanationInfo SynthesisRecommendationAnalysisGuidanceCreative Generation\n0 1 2 3 4 5 6 7Text ProcessingCreative GenerationFactual LookupExplanationGuidanceAnalysisInfo SynthesisRecommendation\nResponse Length (words) Citation CountFigure 5: (Left) Response length distribution across user intent categories. Responses to Factual\nLookup prompts are more concise (168.3 words on average) compared to other categories. (Right)\nCitation count distribution across user intent categories. Responses to Recommendation (6.9 on\naverage) and Info Synthesis (6.8 on average) prompts contain more citations compared to Factual\nLookup (5.7) and Text Processing (4.2) prompts.\n−0.1 0 0.1 0.2 0.3 0.4wikiforeign newsretailgov eduus newsacademic journalyoutubesocial mediacommunity blogtech codingcitation count\nCoefficient Estimate\n−0.2 0 0.2 0.4contradict countirrelevant countsupport count\nCoefficient Estimate\nFigure 6: Citation Control. (Left) Bootstrapped coefficient estimates of citation features. (1) Users\nprefer responses with more citations. (2) Citing tech-related and community platforms, as well as\nsocial media is positively associated with user preferences. (3) Citing Wikipedia negatively interacts\nwith user preferences. (Right) Bootstrapped coefficient estimates of the number of supporting,\nirrelevant, and contradicting claim-citation pairs. The number of supporting and irrelevant pairs is\npositively correlated, while the effect of contradicting pairs is not significant.\n3.3 Cross-Setting Analysis\nSearch Arena evaluates models in a setting where user prompts and expectations are conditioned on\nmodels’ access to web search. In this section, we study how search and non-search models perform\nunder different prompt distributions and user expectations–specifically Search Arena vs Text Arena. Ad-\nditional results on model performance changes across different benchmarks are provided in Appendix F.\nTo investigate performance differences across settings, we deployed Gemini-2.5 Pro Experimental\n[17]–with and without access to web search–to the Search and Text Arenas [ 10]. In the Text Arena, the\nsearch model only competed against its non-search version; in the Search Arena, the non-search model\ncompeted against all other supported search models. Additionally, inline citations were disabled for the\nsearch model in the Text Arena to avoid vote bias. In the Text Arena, users typically assume that models\noperate in closed-book settings without access to external information, while in the Search Arena,\nuser expectations are explicitly conditioned on the search setting. This setup enables us to examine\nwhether and under what conditions access to web search enhances or degrades model performance.\nText Arena Setting. We collected 544 battles between the search and non-search versions of the model,\nyielding 245 ties (45%), 143 search-preferred (26%), and 156 non-search preferred votes (28%). We\nobserve a high proportion of tie votes, and the difference between search-preferred and non-search-\npreferred votes is not statistically significant ( p-value = 0.244); on aggregate, search and non-search\nmodels have comparable performance in the Text Arena. To further analyze performance differences\n7\n--- Page 8 ---\nFigure 7: Experimental Setup for Citation Attribution Analysis. (a) For each multi-turn\nconversation, we retrieve the cited web content and use an LLM-based pipeline to decompose\neach model response into individual claims, followed by citation attribution labeling. (b)For each\nclaim-citation pair (ci,ui,ti), we compute turn-level citation counts across the three categories and\nadd corresponding features to the Bradley-Terry model. Results are shown in Figure 6 (Right).\n1 9 %\n2 3 %\n2 9 %\n3 8 %\n2 2 %\n4 5 %\n3 1 %\n2 3 %\n2 7 %\n3 0 %\n3 2 %\n1 7 %\n3 0 %\n2 3 %\n2 5 %\n3 9 %\n5 4 %\n4 6 %\n4 0 %\n4 5 %\n 4 8 %\n3 2 %\n4 4 %\n3 8 %\nFactual Lookup Info Synthesis Analysis Recommendation Explanation Creative Generation Guidance Text Processing01020304050Search Preferred Non-search Preferred Tie Text Arena SettingsCount\n3 6 %\n3 3 %\n3 8 %\n5 8 %\n3 1 %\n4 4 %\n3 8 %\n8 %\n2 7 %\n 3 3 %\n4 9 %\n1 3 %\n3 5 %\n2 9 %\n4 0 %\n3 3 %\n3 6 %\n3 3 %\n1 4 %\n2 8 %\n3 5 %\n2 7 %\n2 3 %\n5 8 %\nFactual Lookup Info Synthesis Analysis Recommendation Explanation Creative Generation Guidance Text Processing051015202530Search Preferred Non-search Preferred Tie Search Arena SettingsCount\nFigure 8: Cross-Arena Vote Distribution across Text Arena (Left) and Search Arena (Right) broken\ndown into user intent categories. Users prefer the search model for Factual Lookup andInfo Synthesis\nqueries in both settings and the non-search model for Text Processing queries in Text Arena.\nby prompt type, we apply the intent classification pipeline from Subsection 2.2 on the 544 collected\nText Arena prompts. The distribution of votes by intent class is shown in Figure 8 (Left). We observe\na high proportion of ties in Analysis ,Creative Generation , and Guidance queries, indicating that\nthere are no significant differences between model responses for these types of prompts. However, for\nFactual Lookup (p-value =0.012) and Info Synthesis (p-value =0.095), the difference is more expressed\nin favor of the search model. Thus, the search-augmented model is preferred for knowledge acquisition\ntasks–even in the absence of well-defined user expectations–because web-grounded responses typically\noffer precise data, statistics, dates, names, and domain-specific terminology. We also note that the\ndifference in performance for Text Processing (p-value =0.077) queries is in favor of the non-search\nmodel. In these cases, the non-search model often provides structured responses (e.g., numbered or\nbulleted lists, headings), which suggests that presentation style may impact user evaluations.\nSearch Arena Setting. We collected 315 pairwise battles between a non-search model and selected\nsearch-augmented models in Search Arena, with 99 ties (31%), 126 search-preferred (40%), and 90\nnon-search-preferred votes (29%). The difference between search and non-search-preferred votes is\nstatistically significant ( p-value = 0.009); as expected, the non-search model underperforms under a\nsearch-conditioned distribution. A detailed vote distribution by user intent is shown in Figure 8 (Right).\nCompared with the case study in the Text Arena (Figure 8 (Left)), tie votes are less frequent across all cat-\negories, indicating that the differences between model responses are more expressed. The difference is\nmost expressed for Factual Lookup (p-value =5.8×10−5) and Info Synthesis (p-value =0.092) queries.\nThese cross-arena experiments demonstrate that search-augmentation does not hurt performance in\nnon-search settings and can improve responses to queries related to information retrieval and synthesis.\nHowever, removing web search significantly hurts model performance in search settings.\n8\n--- Page 9 ---\n4 Related Work\nLarge Language Models. LLMs have made impressive advances in language understanding, dialogue\ngeneration, and reasoning, enabled by techniques such as large-scale pretraining [ 2,3,6,18,36,56],\nchain-of-thought prompting [ 31,59,51,58,66], and reinforcement learning with human feedback\n(RLHF) [ 4,43]. The dataset and evaluation landscape has progressed from standardized, static\nevaluations [ 22,28,23,70] toward more challenging settings such as deep reasoning [ 35,30,68],\ncoding [ 27,26], and open-ended dialogue under crowd-sourced evaluation [ 10,70]. While\ncrowd-sourced setups can mitigate concerns over data contamination in pretraining [ 64,9], recent\nwork has also highlighted potential biases and oversights in human preferences [12, 62].\nAs LLMs gain tool-use capabilities (e.g., APIs, code interpreters, and web browsers) [ 24,49,18,46,67],\ndomain-specific benchmarks and datasets have emerged to analyze and evaluate LLMs under different\nenvironments [ 73,34,38]. In the context of web search, several search-augmented LLMs have\nbeen developed [ 41,47,16], which retrieve live information to support better reasoning. However,\nexisting benchmarks such as SimpleQA [ 60] and BrowseComp [ 61] are limited to single-turn,\nfact-based, monolingual queries. Although WebArena [ 73] contains diverse user prompts and a\nweb-based interface, it emphasizes closed-world web navigation tasks rather than open-ended search,\nreasoning, and dialogue. We introduce Search-Arena, the first large-scale, crowd-sourced dataset\nfor search-augmented LLMs with human preference signals, covering diverse intents, topics, and\nmulti-turn interactions across 70+ languages, collected through a transparent, open platform.\nTraditional and LLM-Integrated Information Retrieval (IR). Information retrieval is a long-\nstanding task, with early methods like BM25 [ 48], PageRank [ 45], and embedding-based approaches\n[39,25]. In the IR field, several static benchmarks [ 54,40] and large-scale web search datasets with\nuser logs and preference signals [ 57,13,8] have enabled robust evaluation of retrieval systems and\ncomprehensive studies on user behavior.\nWith the rise of LLMs, information retrieval has moved beyond traditional search and become\nintegrated into LLM workflows. In Retrieval Augmented Generation (RAG) settings, retrieved text is\nappended to the input prompt [ 32,20,24,11]; later work explored search-augmented systems operating\nin the open web [ 24,49,67]. Datasets for evaluating these LLM-IR systems span needle-in-a-haystack\nretrieval [ 29,63], citation attribution [ 69,1], and general question answering [ 65,21,60,61]. For\nsearch augmented LLMs, existing benchmarks lack full human–AI interaction traces, including\nqueries, retrieved documents, responses, and preferences, which are needed for human-centric analysis\nas done in traditional IR [ 13,8]. To fill this gap, we introduce Search Arena, a large-scale open dataset\nthat enables human-centered analysis of this emerging interface.\n5 Limitations, Conclusion, and Future Work\nLimitations and Broader Impact. Crowd-sourced data analysis provides valuable insight into real-\nworld user preferences but comes with limitations and broader social implications. First, the collected\ndata may reflect demographic skews and may not be fully representative of the broader population, as not\nall users choose to vote and human judgments are inherently subjective [ 7,12]. Second, because conver-\nsational data with human preferences is personal and potentially valuable for model improvement [ 44],\nits release requires careful consideration of both privacy and equitable access across the community.\nTo address these concerns, we anonymized model responses and randomized their left/right placement\nto reduce known human biases. During the data collection period, no early access to data was granted\nto model providers, nor were any pre-release models deployed on the platform. We obtained user\nconsent at interaction time and enforced a strict privacy policy. To aid responsible interpretation, we\nalso analyzed known biases (e.g., response length; Subsection 3.1) and reported user demographics\nin Figure A4 and Figure A8. Additional details are provided in Appendix A.\nConclusions and Future Work. We present Search Arena, the first large-scale dataset and analysis\nof human interactions with search-augmented LLMs. Spanning over 24k multi-turn conversations\nand 12k human preference votes across more than 70 languages, the dataset captures a broader range\nof user intents and topics than prior benchmarks. Our analysis reveals that user preference is positively\nassociated with citation count and certain source types; however, models do not always cite correctly,\nhighlighting a key challenge in trustworthy systems. The cross-arena experiment further shows that\nsearch-augmented and non-search models behave differently across settings. We release the full\ndataset to support future research in search-augmented LLMs and human-centric analyses.\n9\n--- Page 10 ---\nAcknowledgments\nWe are deeply grateful to Lisa Dunlap for her invaluable feedback and thoughtful discussions. Sky\nComputing Lab is supported by gifts from Accenture, AMD, Anyscale, Cisco, Google, IBM, Intel,\nIntesa Sanpaolo, Lambda, Lightspeed, Mibura, Microsoft, NVIDIA, Samsung SDS, and SAP. Authors,\nas part of their affiliation with UC Berkeley, were supported in part by the National Science Foundation,\nUS Department of Defense, and/or the Berkeley Artificial Intelligence Research (BAIR) industrial\nalliance program, as well as gifts from Amazon.\nReferences\n[1]Amin Abolghasemi, Leif Azzopardi, Seyyed Hadi Hashemi, Maarten de Rijke, and Suzan\nVerberne. Evaluation of attribution bias in retrieval-augmented large language models, 2024.\nURL https://arxiv.org/abs/2410.12380 .\n[2]Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL\nhttps://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/\nModel_Card_Claude_3.pdf .\n[3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang\nFan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report, 2023. URL\nhttps://arxiv.org/abs/2309.16609 .\n[4]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared\nKaplan. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback, 2022. URL https://arxiv.org/abs/2204.05862 .\n[5]Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika , 1952.\n[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models\nare few-shot learners. Advances in neural information processing systems , 2020.\n[7]Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or\nLLMs as the judge? a study on judgement bias. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing . Association for Computational Linguistics,\n2024. URL https://aclanthology.org/2024.emnlp-main.474/ .\n[8]Qi Chen, Xiubo Geng, Corby Rosset, Carolyn Buractaon, Jingwen Lu, Tao Shen, Kun Zhou,\nChenyan Xiong, Yeyun Gong, Paul Bennett, et al. Ms marco web search: A large-scale\ninformation-rich web dataset with millions of real click labels. In Companion Proceedings of\nthe ACM Web Conference 2024 , 2024.\n[9]Yuxing Cheng, Yi Chang, and Yuan Wu. A survey on data contamination for large language\nmodels, 2025. URL https://arxiv.org/abs/2502.14425 .\n[10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle\nLi, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and\nIon Stoica. Chatbot arena: An open platform for evaluating LLMs by human pref-\nerence. In Forty-first International Conference on Machine Learning , 2024. URL\nhttps://openreview.net/forum?id=3MW8GKNyzI .\n[11] Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu,\nHu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, and Wen tau Yih. Selfcite: Self-\nsupervised alignment for context attribution in large language models, 2025. URL\nhttps://arxiv.org/abs/2502.09604 .\n10\n--- Page 11 ---\n[12] Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and\nNoah A. Smith. All that’s ‘human’ is not gold: Evaluating human evaluation of generated\ntext. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers) . Association for Computational Linguistics, 2021. URL\nhttps://aclanthology.org/2021.acl-long.565/ .\n[13] Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck. Orcas:\n18 million clicked query-document pairs for analyzing search. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management , 2020.\n[14] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt,\nJoseph E. Gonzalez, and Serena Yeung-Levy. Describing differences in image sets with natural\nlanguage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2024.\n[15] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman.\nCognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars,\n2025. URL https://arxiv.org/abs/2503.01307 .\n[16] Gemini. Grounding with google search, 2024. URL https://developers.googleblog.com/\nen/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/ .\n[17] Gemini. Gemini 2.5: Our most intelligent ai model, 2025. URL https://blog.google/\ntechnology/google-deepmind/gemini-model-thinking-updates-march-2025/ .\n[18] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama\n3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783 .\n[19] Maarten Grootendorst. Bertopic: Neural topic modeling with a class-based tf-idf procedure,\n2022. URL https://arxiv.org/abs/2203.05794 .\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training, 2020. URL https://arxiv.org/abs/2002.08909 .\n[21] Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang,\nBonan Min, and Vittorio Castelli. Rag-qa arena: Evaluating domain robustness for long-form\nretrieval augmented question answering, 2024. URL https://arxiv.org/abs/2407.13998 .\n[22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL\nhttps://arxiv.org/abs/2009.03300 .\n[23] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset,\n2021. URL https://arxiv.org/abs/2103.03874 .\n[24] Jacob Hilton, R Nakano, S Balaji, and John Schulman. Webgpt: Improving the factual accuracy\nof language models through web browsing. OpenAI Blog, December , 2021.\n[25] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Pad-\nmanabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search.\nInProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining . ACM, 2020. URL http://dx.doi.org/10.1145/3394486.3403305 .\n[26] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free eval-\nuation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974 .\n[27] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL\nhttps://arxiv.org/abs/2310.06770 .\n11\n--- Page 12 ---\n[28] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension, 2017. URL\nhttps://arxiv.org/abs/1705.03551 .\n[29] Gregory Kamradt. Llmtest_needleinahaystack, 2023. URL https://github.com/gkamradt/\nLLMTest_NeedleInAHaystack/blob/main/README.md .\n[30] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anasta-\nsiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen,\nNishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska,\nYi Tay, Vinh Q. Tran, Quoc V . Le, and Orhan Firat. Big-bench extra hard, 2025. URL\nhttps://arxiv.org/abs/2502.19187 .\n[31] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205.11916 .\n[32] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL\nhttps://arxiv.org/abs/2005.11401 .\n[33] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, Jian-Yun Nie, and Ji-Rong Wen. The\nweb can be your oyster for improving language models. In Findings of the Association for\nComputational Linguistics: ACL 2023 . Association for Computational Linguistics, 2023. URL\nhttps://aclanthology.org/2023.findings-acl.46/ .\n[34] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei\nHuang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms, 2023.\nURL https://arxiv.org/abs/2304.08244 .\n[35] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E\nGonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard\nand benchbuilder pipeline, 2024. URL https://arxiv.org/abs/2406.11939 .\n[36] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report, 2024. URL\nhttps://arxiv.org/abs/2412.19437 .\n[37] Junhua Liu, Yong Keat Tan, Bin Fu, and Kwan Hui Lim. Intent-aware dialogue gener-\nation and multi-task contrastive learning for multi-turn intent classification, 2024. URL\nhttps://arxiv.org/abs/2411.14252 .\n[38] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\nAgentbench: Evaluating llms as agents, 2023. URL https://arxiv.org/abs/2308.03688 .\n[39] Solmaz Seyed Monir, Irene Lau, Shubing Yang, and Dongfang Zhao. Vectorsearch: En-\nhancing document retrieval with semantic embeddings and optimized search, 2024. URL\nhttps://arxiv.org/abs/2409.17383 .\n[40] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text\nembedding benchmark, 2023. URL https://arxiv.org/abs/2210.07316 .\n[41] OpenAI. Introducing chatgpt search, 2024. URL https://openai.com/index/\nintroducing-chatgpt-search/ .\n[42] OpenAI. Introducing gpt-4.1 in the api, 2025. URL https://openai.com/index/gpt-4-1/ .\n[43] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback,\n2022. URL https://arxiv.org/abs/2203.02155 .\n12\n--- Page 13 ---\n[44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback. In Advances in Neural Information Processing Systems . Curran Associates, Inc.,\n2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf .\n[45] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation\nranking: Bringing order to the web, 1998.\n[46] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language\nmodel connected with massive apis. Advances in Neural Information Processing Systems , 2024.\n[47] Perplexity. Sonar by perplexity. URL https://docs.perplexity.ai/home .\n[48] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike\nGatford. Okapi at trec-3. In Proceedings of the Third Text REtrieval Conference (TREC-3) , 1994.\n[49] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools, 2023. URL https://arxiv.org/abs/2302.04761 .\n[50] Chirag Shah, Ryen W. White, Reid Andersen, Georg Buscher, and Scott Counts. Using\nlarge language models to generate, validate, and apply user intent taxonomies, 2023. URL\nhttps://arxiv.org/abs/2309.13063 .\n[51] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL\nhttps://arxiv.org/abs/2303.11366 .\n[52] Mark Steyvers, Heliodoro Tejeda Lemus, Aakriti Kumar, Catarina Belém, Sheer Karny, Xinyue\nHu, Lukas Mayer, and Padhraic Smyth. The calibration gap between model and human confidence\nin large language models, 2024. URL https://api.semanticscholar.org/CorpusID:\n267211649 .\n[53] Kelly Tang, Wei-Lin Chiang, and Anastasios N. Angelopoulos. Arena explorer: A topic modeling\npipeline for llm evals & analytics, 2025.\n[54] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2) , 2021. URL https://openreview.net/forum?id=wCu6T5xFjeJ .\n[55] Wei-Lin Chiang Tianle Li, Anastasios Angelopoulos. Does style matter? disentangling\nstyle and substance in chatbot arena, 2024. URL https://blog.lmarena.ai/blog/2024/\nstyle-control/ .\n[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023. URL https://arxiv.org/abs/2302.13971 .\n[57] Ellen M. V oorhees and Donna K. Harman, editors. TREC: Experiment and Evaluation in\nInformation Retrieval . MIT Press, 2005.\n[58] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels, 2023. URL https://arxiv.org/abs/2203.11171 .\n[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models,\n2023. URL https://arxiv.org/abs/2201.11903 .\n13\n--- Page 14 ---\n[60] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese,\nJohn Schulman, and William Fedus. Measuring short-form factuality in large language models,\n2024. URL https://arxiv.org/abs/2411.04368 .\n[61] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won\nChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet chal-\nlenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516 .\n[62] Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for\nlarge language models. In Proceedings of the 31st International Conference on\nComputational Linguistics . Association for Computational Linguistics, 2025. URL\nhttps://aclanthology.org/2025.coling-main.21/ .\n[63] Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez,\nTrevor Darrell, and David Chan. Visual haystacks: A vision-centric needle-in-a-haystack\nbenchmark. In The Thirteenth International Conference on Learning Representations , 2025.\nURL https://openreview.net/forum?id=9JCNPFL1f9 .\n[64] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\nbenchmark and contamination for language models with rephrased samples, 2023. URL\nhttps://arxiv.org/abs/2311.04850 .\n[65] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing . Association for Computational Linguistics, 2018. URL\nhttps://aclanthology.org/D18-1259/ .\n[66] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.\nURL https://arxiv.org/abs/2305.10601 .\n[67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL\nhttps://arxiv.org/abs/2210.03629 .\n[68] Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai,\nYuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen\nTan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. Mr-\nben: A meta-reasoning benchmark for evaluating system-2 thinking in llms, 2024. URL\nhttps://arxiv.org/abs/2406.13975 .\n[69] Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yux-\niao Dong, Ling Feng, and Juanzi Li. Longcite: Enabling LLMs to generate fine-grained citations\nin long-context QA, 2024. URL https://openreview.net/forum?id=mMXdHyBcHh .\n[70] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:\n1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning\nRepresentations , 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM .\n[71] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and\nHao Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2024. URL\nhttps://arxiv.org/abs/2309.11998 .\n[72] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing dif-\nferences between text distributions with natural language. In Proceedings of\nthe 39th International Conference on Machine Learning . PMLR, 2022. URL\nhttps://proceedings.mlr.press/v162/zhong22a.html .\n[73] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xi-\nanyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig.\nWebarena: A realistic web environment for building autonomous agents, 2024. URL\nhttps://arxiv.org/abs/2307.13854 .\n14\n--- Page 15 ---\nAppendix\n•Appendix A describes the platform design, model configurations, and the limitations and\npolicies of our data collection process.\n•Appendix B outlines the data processing steps, including intent classification, topic modeling,\nand filtering procedures.\n•Appendix C includes details (e.g., parameters, prompts, raw outputs) on the dataset\ndifferencing experiments.\n• Appendix D includes model performance and user preference analysis on Search Arena.\n•Appendix E reports citation analysis, including user preference experiments and\nimplementation details of the citation attribution pipeline.\n• Appendix F includes additional results on the cross-benchmark analysis.\nFigure A1: Search Arena User Interface. Search Arena is integrated into the Chatbot Arena\necosystem, sharing the same front-end design and entry point. As shown at the bottom, it is\nimplemented as a separate tab. The central panel displays a side-by-side chat interface with two\nanonymous models, each providing responses that include clickable inline citations and expandable\nreference links. Users can engage in multi-turn conversations and cast a vote at any time during the\nconversation using the four feedback options (A is better, B is better, Tie, and Both are bad).\n15\n--- Page 16 ---\nA Search Arena Platform\nAs described in Section 2, Search Arena ( https://legacy.lmarena.ai/?search ) is an open,\ncrowdsourced evaluation platform for search-augmented LLMs, launched on March 18, 2025. This\nsection outlines our data collection and release protocols, followed by a description of the supported\nmodels and key design decisions.\nA.1 Data\nData Collection. The Search Arena platform does not require user login, but users must explicitly\naccept the Terms of Service before using the platform. Once accepted, users access a dedicated tab\nwithin Chatbot Arena to interact with search-enabled models. This setup naturally leads to more\ninformation-seeking queries compared to the default Text Arena interface. For each prompt, responses\nfrom two anonymous models are displayed side-by-side, and users may cast a preference vote at any\ntime during the interaction. The user interface is shown in Figure A1. The full text of Terms of Service\nis at Figure A2.\nOver the course of a 7-week data collection period, the platform recorded an average of roughly\n800-1,500 conversations per day. After filtering out examples with server errors, inconsistent\nconfigurations, or other quality issues, we retain approximately 24,000 conversations, about half of\nwhich include human preference votes. Figure A3 shows daily traffic trends, with spikes aligned to\nmajor model updates or platform announcements, and dips on weekends.\nTerms of Service\nUsers are required to agree to the following terms before using the service: The service is a research\npreview. It only provides limited safety measures and may generate offensive content. It must not\nbe used for any illegal, harmful, violent, racist, or sexual purposes. Please do not upload any private\ninformation. The service collects user dialogue data, including both text and images, and reserves\nthe right to distribute it under a Creative Commons Attribution (CC-BY) or a similar license. You\nmay only use this website for your personal or internal business purposes. You must not access the\nwebsite programmatically, scrape or extract data, manipulate any leaderboard or ranking, or authorize\nor pay others to access or use the website on your behalf. Unauthorized use may result in suspension\nor termination of your access, including access by your organization.\nFigure A2: Term of Service of the Chatbot Arena Platform (including Search Arena).\nFigure A3: Daily traffic on the Search Arena platform.\nData Release Policy. Per our Terms of Service (Figure A2), all conversation data is released under\na Creative Commons Attribution (CC-BY) license. To protect user privacy, we apply automated\nde-identification using Google’s Data Loss Prevention (DLP) API, which removes personal identifiers\n16\n--- Page 17 ---\nTable A1: List of supported models and their configurations on Search Arena.†We evaluate OpenAI’s\nweb search API, which differs from the search feature in the ChatGPT product.\nProvider Model Base Model Details\nPerplexitypplx-sonar sonar Default config\npplx-sonar-pro sonar-pro Default config\npplx-sonar-pro-high sonar-pro search_context_size=high\npplx-sonar-reasoning sonar-reasoning Default config\npplx-sonar-reasoning-pro-high sonar-reasoning-pro search_context_size=high\nGeminigemini-2.0-flash-grounding gemini-2.0-flash With Google Search enabled\ngemini-2.5-flash-grounding gemini-2.5-flash With Google Search enabled\ngemini-2.5-pro-grounding gemini-2.5-pro-exp-03-25 With Google Search enabled\nOpenAI†api-gpt-4o-mini-search-preview gpt-4o-mini Default config\napi-gpt-4o-search-preview gpt-4o Default config\napi-gpt-4o-search-preview-high gpt-4o search_context_size=high\napi-gpt-4o-search-preview-high-loc gpt-4o user_location feature enabled\nsuch as email addresses, credit card numbers, and API keys. Approximately 2% of examples were\nflagged by this process.\nA.2 Models\nSearch Arena currently supports 12 search-augmented LLMs from Perplexity, Gemini, and\nOpenAI, as summarized in Table A1. Unless otherwise noted, we group models with different\ninline citation styles (elaborated below) of the same base model. Each model is accessed via its\nprovider’s public API using the default configuration, including search_context_size=medium for\nPerplexity and OpenAI models. We also evaluate several additional variants, including (1) OpenAI’s\ngpt-4o-search-preview with user_location enabled and (2) Perplexity and OpenAI models\nconfigured with search_context_size=high . Figure A9 shows the number of battles per model.\nModel Anonymity and Citation Style Control. Each provider uses a distinct citation style, which\nmay unintentionally reveal model identity. At the same time, citation formatting can influence user\npreferences. To manage this tradeoff, we implement a citation style randomization mechanism: model\noutputs are rendered using either a standardized format or the provider’s original style. This approach\nreduces the risk of user-side de-anonymization while allowing us to study how citation style affects\nuser behavior. We found that the inline citation style does not significantly affect user preferences\nand model rankings.\nB Data Statistics\nB.1 Linguistic Features\nUsers’ demographic information in Search Arena, based on country codes extracted from IP addresses,\nis shown in Figure A4. The dataset includes 11,650 unique users across 136 countries, with the United\nStates (18.7%), Russia (9.9%), Germany (5.9%), and China (5.5%) as the top four.\nLanguage distribution (of the top 50 languages) of Search Arena prompts is shown in Figure A8. The\nprompts span 71 languages with English (56.4%), Russian (11.4%), and Chinese (6.8%) as the top three.\nAs shown in Figure A5, Search Arena prompt lengths vary across intent categories. As expected, Text\nProcessing (85.1 words) and Analysis (72.7 words) prompts are generally longer, while Explanation\n(24.7 words) and Factual Lookup (16.3 words) prompts are shorter.\nSearch Arena conversations are multi-turn, with 22.4% of all conversations containing more than one\nturn. Specifically, there are 3,288 conversations with 2 turns, 966 with 3 turns, and 460 with 4 turns.\nThe distribution is shown in Figure A6.\nB.2 User Intent Annotation\nIn Subsection 2.2 and Figure 1, we briefly introduced the intent annotation pipeline and its high-level\nfindings. Here, we provide additional details of the pipeline.\nFirst, the three co-authors of the paper did open-text annotations on 100 English prompts randomly\nsampled from the collected dataset. The annotators then met to consolidate their annotations into a\n17\n--- Page 18 ---\ntaxonomy of primary and secondary user intent categories, including definitions and representative\nexamples. The final taxonomy includes the following nine intent categories: Factual Lookup ,\nInformation Synthesis ,Analysis ,Recommendation ,Explanation ,Creative Generation ,Guidance ,\nText Processing , and Other . Full category descriptions are shown in Table A2.\nTo validate the taxonomy, the annotators labeled a subset of 100 prompts. The human inter-annotator\nagreement, measured by Cohen’s Kappa, ranged from 0.65 for primary labels to 0.79 when considering\ntop-two matches (substantial agreement).\nWe then scaled the annotation to the full dataset using GPT-4.1 [ 42], using the same in-context\nexamples from Table A2. The total annotation cost was approximately $20 USD. To evaluate label\nquality, we compared GPT-4.1 annotations with human labels on 150 samples drawn from the top\nthree languages in our dataset (English, Russian, and Chinese). The resulting Cohen’s Kappa score\nof 0.812 on top-2 intents indicates strong agreement. The full GPT-4.1 prompt is provided below:\n/commen◎sPrompts for LLM-based Intent Classification\nYou are an impartial classifier.\nTASK 1: Primary intent: choose one category that best matches the user’s intent,\nalways ask yourself, what is the user trying to get the model to do with this query.\nConsidering them following the categories one by one in order.\nTASK 2: If there is a clear secondary goal, choose one additional intent label\nthat is also present in the query. Otherwise, return \"Unassigned\" if no clear\nsecond goal exists.\nAllowed intent categories:\n1. Text Processing\n2. Creative Generation\n3. Factual Lookup\n4. Info Synthesis\n5. Analysis\n6. Recommendation\n7. Explanation\n8. Guidance\n9. Other\nHere are a few examples that you should follow, try to generalize beyond these\nexamples but hold true to their semantic meaning:\n{FEW SHOT IN-CONTEXT EXAMPLES}\nRespond only in valid JSON, only choosing exactly one category to fill the values:\n{\n\" primary_intent \": \"< primary intent label >\",\n\" secondary_intent \": \"< secondary intent label >\",\n\" reasoning \": \"< your reasoning for the classification >\",\n}\nUser query: {USER PROMPT}\n18\n--- Page 19 ---\nTable A2: User intent taxonomy including category descriptions and examples.\nCategory Description Examples\nText Processing Users request linguistic tasks such as\nsummarizing, translating, paraphrasing,\nor refining text.• Summarize <website> .\n•Translate this paragraph from\nSpanish to English.\n•Rephrase this email profession-\nally for an investment -banking\napplication.\nCreative Gener-\nationUsers request original creative outputs,\nincluding fictional scenarios, satire,\npoetry, storytelling, or other forms of\nartistic language generation.•Compose a witty review of ITV’s\nBig Brother .\n•Create a humorous poem about\nSpongeBob .\n•Write a short fantasy story set on\nMars.\nFactual Lookup Users seek to retrieve a precise, ob-\njective fact or specific information\n(what/who/when-like questions).• Who invented lambda calculus?\n•How many planets are in our Solar\nSystem?\n• When was the Mona Lisa painted?\nInfo. Synthesis Users seek a concise, aggregated\nsummary that combines facts or per-\nspectives from multiple sources. The\nfocus is on integrating factual content\nwithout requiring reasoning, subjective\ninterpretation, or personalization.•List the key functions of the U.S.\nCongress.\n•Summarize the main events of the\nFrench Revolution.\nAnalysis Users seek a reasoned judgment or break-\ndown of a topic, often involving com-\nparison, evaluation, weighing different\nperspectives, and syntheszing material\nfrom many sources. Often open-ended.•Analyze the pros and cons of\nnuclear energy.\n•When will we reach artificial\ngeneral intelligence?\n•What were the strategic implica-\ntions of the Cuban Missile Crisis?\nRecommenda-\ntionUsers request suggestions or advice\ntailored to particular constraints, pref-\nerences, or specified criteria. Often\nimplies personalization or ranking.•Best laptop for machine learning\nunder $1500.\n•I like literary fiction—what books\nshould I read?\n•What programming language\nshould I learn for web develop-\nment?\nExplanation Users seek detailed clarifications, educa-\ntional insights, or thorough elaborations\naimed at better understanding a concept,\nprocess, or phenomeneon. May ask\n“why” or “how” something works\nwithout implying action.•What led to the fall of the Roman\nEmpire?\n•Can you explain the Heisenberg\nuncertainty principle?\n•How does gradient -descent opti-\nmization work in machine learning?\n19\n--- Page 20 ---\nCategory Description Examples\nGuidance Users request instructions, procedures,\nor practical advice intended to accom-\nplish specific tasks, typically involving\nsequential steps or troubleshooting.\nUsually framed as “how to” or involving\naction.•How do I install Docker on Ubuntu?\n•How do I renew my passport online?\n•How do I replace the battery in a\nMacBook Air?\nOtherPrompts that don’t fit neatly into any\nof the categories above. Prompts may\nbe malformed, declarative in nature, or\nlacking in meaningful user intent.• pizza.\n• The table hit my head.\n• Talk about something please.\nFigure A4: Search Arena Users’ Demographics. Search Arena data includes 11,650 unique users\nacross 136 countries.\nText Processing Analysis Creative Generation Guidance Info Synthesis Recommendation Explanation Factual Lookup101001000Log(num words)\nFigure A5: Search Arena Prompt Length Distribution by Intent. Prompt lengths vary across intent\ncategories. Text Processing (85.1 words) and Analysis (72.7 words) prompts are generally longer,\nwhile Explanation (24.7 words) and Factual Lookup (16.3 words) prompts are shorter.\n20\n--- Page 21 ---\nB.3 Topic Modeling\nTo analyze topic diversity across benchmarks, we apply the BERTopic framework [ 19] to the 11,764\nunique English prompts in the Search Arena dataset. We generate prompt embeddings using OpenAI’s\ntext-embedding-3-large model, perform dimensionality reduction and clustering via UMAP and\nHDBSCAN, and summarize each resulting cluster using GPT-4o [ 53]. We adapt the Arena Explorer\n[53] methodology for the Search Arena dataset. Figure A7 illustrates the distribution of topics derived\nfrom BERTopic clustering over the Search Arena prompts. The prominence of categories such as\nTechnology Comparisons, Market Analysis, and Entertainment Characters, alongside a long-tail of\nniche domains, highlights the dataset’s breadth and its suitability for evaluating search-augmented\nLLMs under diverse topics.\n77.6%\n13.7%\n4.0%\n1.9%\n1.0%\n0.6%\n1.3%\n1 2 3 4 5 6 6+1k10k\nTurns per ConversationCount (log scale)\nFigure A6: Search Arena Conversation Length (Number of Turns) Distribution. Search Arena\nchats are multi-turn with 22.4% of conversations containing more than 1 turn.\nFigure A7: Top Topic Categories in Search Arena. This figure shows the distribution of topic clusters.\nThe most prevalent topics include Technology Comparisons (22.0%), Market Analysis (12.3%), and\nEntertainment Characters (10.6%). Less frequent but still diverse topics include health and shopping.\nThe long-tail distribution reflects the breadth of real-world usage of search-augmented LLMs.\n21\n--- Page 22 ---\n0.008%\n0.012%\n0.012%\n0.012%\n0.012%\n0.012%\n0.012%\n0.012%\n0.017%\n0.017%\n0.017%\n0.021%\n0.025%\n0.025%\n0.029%\n0.033%\n0.033%\n0.033%\n0.033%\n0.046%\n0.046%\n0.046%\n0.050%\n0.058%\n0.062%\n0.067%\n0.158%\n0.183%\n0.204%\n0.212%\n0.237%\n0.291%\n0.303%\n0.411%\n0.416%\n0.495%\n0.586%\n0.715%\n0.811%\n1.322%\n2.166%\n2.473%\n2.527%\n2.573%\n2.689%\n2.731%\n3.047%\n6.796%\n11.380%\n56.428%\n100 1000 10000UzbekTagalogBasqueGeorgianHindiNepaliMalayCentral KhmerMacedonianLithuanianTibetanArmenianCroatianGalicianSlovakCebuanoDanishSlovenianEsperantoNorwegianLatinGreekSerbianFinnishBulgarianHebrewSwedishHungarianRomanianDutchCatalanThaiUkrainianCzechArabicTurkishPersianPolishIndonesianItalianPortugueseFrenchKoreanVietnameseJapaneseSpanishGermanChineseRussianEnglish\nCount (log scale)Figure A8: Search Arena Language Distribution (top 50). Search Arena prompts span 71 languages.\n22\n--- Page 23 ---\nC Describing Differences\nTo extract interpretable differences across two text corpora (e.g., prompts, responses), we use\nLLM-based dataset differencing methods [72, 14].\nC.1 Prompt Differences\nTo describe differences in prompt distributions between Search Arena and other datasets (see analysis\nin Subsection 2.2), we use GPT-4.1 to propose properties (hypotheses) across 16 rounds, with 32\nsamples per group in each round. We then use o3 to filter the top five properties and GPT-4.1-mini\nto re-rank them. The prompts are shown below. We use the default temperature parameter of 1.0.\nWe extract the following properties (p < 0.001) for each dataset pair and report the proportion of\nvalidation set examples (100 samples) satisfying each property.\nSearch Arena vs SimpleQA:\n•Requests in-depth explanations, analyses, or step-by-step guidance rather than single factual\nanswers (28% vs 0%).\n•Requests recent or real-time information, including current events, product features, or online\nresearch (35% vs 7%).\n•Seeks technical help, troubleshooting, or comparative evaluations for software, programming,\nor digital tools (17% vs 1%).\n•Asks for creative content generation, rewriting, or stylistic transformations (stories, poems,\nsatirical pieces, etc.) (13% vs 0%).\nSimpleQA vs Search Arena:\n•Requests exact factual details such as specific names, dates, numbers, or titles (97% vs 41%).\n•Expects a single objective, verifiable answer rather than explanations or analysis (97% vs\n43%).\n•Avoids subjective, open-ended, or creative requests, limiting queries to factual retrieval (99%\nvs 52%).\n• Focuses on niche or lesser-known historical, scientific, or cultural topics (51% vs 7%).\n•Presents concise, narrowly scoped questions with minimal background information (91%\nvs 56%).\nSearch Arena vs BrowseComp:\n• User messages are brief and provide minimal contextual detail (80% vs 23%).\n•Responses expected are immediate functional outputs such as lists, summaries, or code\nsnippets (50% vs 20%).\n• Prompts seek practical advice or step-by-step instructions for real-world tasks (29% vs 1%).\n• Queries focus on well-known, mainstream topics, products, or services (75% vs 48%).\nBrowseComp vs Search Arena:\n• Frames the query as a deductive puzzle or investigative challenge (80% vs 3%).\n•Provides explicit temporal or geographic constraints that narrow the solution space (99%\nvs 25%).\n•Requires synthesizing multiple detailed clues from different sources to deduce the answer\n(92% vs 36%).\nSearch Arena vs Text Arena:\n•Requests for current factual information about real-world entities, events, or products (61%\nvs 25%).\n•Requests for technical comparisons, evaluations, or purchasing guidance on devices, software,\nor services (16% vs 4%).\n23\n--- Page 24 ---\nText Arena vs Search Arena:\n•Seeks analytical, step-by-step solutions to mathematical, logical, or technical puzzles and\nexplanations (21% vs 4%).\n•Requests programming assistance, such as debugging code, generating scripts, or explaining\nprogramming concepts (23% vs 8%).\n•Requests creative writing outputs, including stories, poems, jokes, or fictional scenarios (20%\nvs 5%).\n/commen◎sLLM Prompt for Proposing Distinguishing Properties between Prompt Sets\nThe following are a two separate lists of prompts that users have asked to a\nchatbot:\n<START>\n{text}\n<END>\nI am a machine learning researcher trying to figure out the major differences\nbetween these two groups of prompts. This is a very small portion of the data, so\nI want the differences to be general.\nPlease provide a list of the top three differences (separated by bullet points \"*\")\nbetween the prompts in Group A and Group B.\nFollow the detailed instructions below:\n- Identify properties that are more common in Group A prompts compared to Group B\nprompts.\n- The property descriptions should be simple and detailed. Do not produce vague\nand generic descriptions.\n- Start the description of each property with \"User\". Do not use keywords like\n\"group A\", \"group B\", \"more common\", \"less common\", \"frequently\", \"occasionally\"\nin your response.\n- Do not combine multiple properties / features in a single bullet point.\n- An example of the desired output format:\n* \"Property 1\"\n* \"Property 2\"\n* \"Property 3\"\nPlease order your response in terms of the most common differences between the two\ngroups. Your response:\n/commen◎sLLM Prompt for Reducing Proposed Properties\nPROPERTIES:\n##############################\n{properties}\n##############################\nAbove is a list of properties, several of which are similar.\n- Please reduce this to a list of the five most common distinct properties, ordered\nby frequency of occurrence.\n- There should be no similar / overlapping properties in the final list.\n- Do not group multiple unrelated properties / features into a single property.\n- Do not use keywords, like \"frequent\", \"rare\", \"common\", \"uncommon\", etc.\n- Only respond with a numbered list of properties, no other text.\n24\n--- Page 25 ---\n/commen◎sLLM Prompt for Ranking Properties\nPROMPT:\n<START>\n{text}\n<END>\nPROPERTY:\n<START>\n{hypothesis}\n<END>\nCheck whether the PROMPT satisfies the PROPERTY. Respond with Yes or No. If you\nare unsure, respond with No.\nOutput:\nC.2 Response Differences\nTo describe differences in responses between search and non-search models in cross-arena deployments\n(see analysis in Subsection 3.3), we use GPT-4.1 to propose properties (hypotheses) across 32\nrounds, with 8 samples per group in each round. We then use o3 to filter the top five properties and\nGPT-4.1-mini to re-rank them. The prompts are shown below (for property reduction, we re-use the\nsame prompt from Subsection C.1). We use the default temperature parameter of 1.0.\nFor each dataset pair, we extract the top properties and report the proportion of validation set examples\n(100 samples) satisfying the properties, along with corresponding significance levels (p-values).\nText Arena Setting\nSearch vs non-search model responses to Factual Lookup prompts:\n•Includes precise quantitative or technical specifics such as exact dates, numerical values,\nspecifications, or code snippets (53.8% vs 41%) (p = 0.26).\nSearch vs non-search model responses to Info Synthesis prompts:\n•Incorporates precise data, statistics, dates, names, and domain-specific terminology (80.1%\nvs 61.9%) (p = 0.18).\nNon-search vs search model responses to Text Processing prompts:\n•Uses explicitly structured formatting (numbered or bulleted lists, headings) to organize\ninformation (74.4% vs 56.4%) (p = 0.1).\nSearch Arena Setting\nSearch vs non-search model responses to Factual Lookup prompts:\n•Provides extensive background context or explanatory preamble before addressing the\nspecific question (83.3% vs 72.2%) (p = 0.26).\nSearch vs non-search model responses to Info Synthesis prompts:\n•Explicitly states limitations, uncertainties, or caveats about the information provided (71.7%\nvs 52.2%) (p = 0.05).\n25\n--- Page 26 ---\n/commen◎sLLM Prompt for Proposing Distinguishing Properties between Model A and Model B\nResponses\nThe following is a list of Model A and Model B outputs to a set of user questions:\n<START>\n{text}\n<END>\nI am a machine learning researcher trying to figure out the major differences\nbetween the outputs of model A and model B. This is a very small portion of the\ndata, so I want the differences to be general.\nPlease provide a list of three top differences (separated by bullet points \"*\")\nbetween the outputs of model A and model B.\nFollow the detailed instructions below:\n- Identify properties that are more common in model A outputs compared to model B\noutputs.\n- The property descriptions should be simple and detailed. Do not produce vague\nand generic descriptions.\n- Do not use keywords like \"model A\", \"model B\", \"more common\", \"less common\",\n\"frequently\", \"occasionally\" in your response.\n- Do not combine multiple properties / features in a single bullet point.\n- An example of the desired output format:\n* \"Property 1\"\n* \"Property 2\"\n* \"Property 3\"\nPlease order your response in terms of the most common differences between the two\ngroups. Your response:\n/commen◎sLLM Prompt for Ranking Properties\nQUESTION:\n<START>\n{question}\n<END>\nANSWER:\n<START>\n{answer}\n<END>\nPROPERTY:\n<START>\n{hypothesis}\n<END>\nCheck whether the ANSWER satisfies the PROPERTY. Respond with Yes or No. If you\nare unsure, respond with No.\nOutput:\nD Leaderboard and General Feature Analysis\nD.1 Leaderboard Analysis\nModels supported on the Search Arena platform are shown in Table A1. Number of battles across\nmodels is shown in Figure A9; pairwise battle count is shown in Figure A10.\nAverage win rates across models are shown in Figure A11. Pairwise win rates are shown in Figure A12.\n26\n--- Page 27 ---\n2510 2510 2485\n2353\n2257 2238\n2065\n1948\n1822\n1611 1606\n1041\nsonar-pro-high\nsonar-reasoning-pro-highgpt-4o-search-high-locsonar-reasoning\ngpt-4o-search-highgpt-4o-search\ngemini-2.5-pro-groundinggemini-2.0-flash-groundingsonar\ngpt-4o-mini-searchsonar-pro\ngemini-2.5-flash-grounding05001000150020002500Number of BattlesFigure A9: Battle Count Distribution . Number of battles across Search Arena models. The\ndistribution is not even due to (1) different sampling weights and (2) models were not added to the\nplatforms at the same time.\n238 266 279 199 102 143 156 218 213 380 199 117\n266 116 208 175 95 128 85 247 203 281 144 117\n279 208 104 231 157 171 161 194 273 287 284 161\n199 175 231 84 247 275 40 197 291 182 213 219\n102 95 157 247 16 211 18 102 200 102 188 168\n143 128 171 275 211 40 54 108 198 125 203 166\n156 85 161 40 18 54 90 50 17 162 180 28\n218 247 194 197 102 108 50 134 208 243 140 107\n213 203 273 291 200 198 17 208 98 206 181 169\n380 281 287 182 102 125 162 243 206 186 217 114\n199 144 284 213 188 203 180 140 181 217 88 201\n117 117 161 219 168 166 28 107 169 114 201 44sonar-reasoning-pro-high gemini-2.5-pro-grounding\nsonar-pro-highsonar-reasoning\nsonar-pro\nsonargemini-2.5-flash-grounding gemini-2.0-flash-grounding\ngpt-4o-search-highgpt-4o-search-high-loc\ngpt-4o-searchgpt-4o-mini-search\ngpt-4o-mini-searchgpt-4o-searchgpt-4o-search-high-locgpt-4o-search-highgemini-2.0-flash-groundinggemini-2.5-flash-groundingsonarsonar-prosonar-reasoningsonar-pro-highgemini-2.5-pro-groundingsonar-reasoning-pro-high\n50100150200250300350Model BModel A\nFigure A10: Pairwise Battle Count Distribution . Number of battles between Search Arena models.\nConsistent with Chatbot Arena’s ranking system [ 10], we use Bradley-Terry regression [ 5] to calculate\nmodel coefficients and then re-scale to match the scale of the Elo rating system. The resulting model\nscores are shown in Figure A13.\nSearch Arena model ratings and ranks are shown in Table A3. We also compute model scores and\nthe leaderboard for two subsets of the full dataset:\n•English and non-English prompts: The gap between the two top models increases, with\nsonar-reasoning-pro-high performing better on non-English prompts. Additionally,\ngemini-2.0-flash-grounding andgemini-2.5-flash-grounding perform better on\nnon-English prompts.\n27\n--- Page 28 ---\n0.66 0.65\n0.62\n0.58 0.570.56\n0.50\n0.48\n0.44\n0.38 0.38 0.37\n0.31\nsonar-reasoning-pro-high gemini-2.5-pro-groundingsonar-pro-highsonar-reasoningsonar-prosonar\ngemini-2.5-flash-groundinggemini-2.5-pro\ngemini-2.0-flash-groundinggpt-4o-search-high-locgpt-4o-search\ngpt-4o-search-high gpt-4o-mini-search00.10.20.30.40.50.6Average Win RateFigure A11: Win Rate Distribution . Average win rates of Search Arena models.\n0.50 0.48 0.55 0.57 0.58 0.58 0.65 0.74 0.74 0.75 0.79 0.88\n0.52 0.50 0.52 0.61 0.64 0.55 0.73 0.73 0.78 0.77 0.76 0.76\n0.45 0.48 0.50 0.51 0.53 0.53 0.72 0.69 0.76 0.75 0.73 0.75\n0.43 0.39 0.49 0.50 0.48 0.60 0.61 0.60 0.71 0.74 0.72 0.76\n0.42 0.36 0.47 0.52 0.50 0.54 0.54 0.63 0.58 0.77 0.69 0.69\n0.42 0.45 0.47 0.40 0.46 0.50 0.59 0.59 0.64 0.69 0.69 0.74\n0.35 0.27 0.28 0.39 0.46 0.41 0.50 0.57 0.67 0.72 0.69 0.74\n0.26 0.27 0.31 0.40 0.38 0.41 0.43 0.50 0.56 0.57 0.52 0.68\n0.26 0.22 0.24 0.29 0.42 0.36 0.33 0.44 0.50 0.42 0.53 0.55\n0.25 0.23 0.25 0.26 0.23 0.31 0.28 0.43 0.58 0.50 0.53 0.63\n0.21 0.24 0.27 0.28 0.31 0.31 0.31 0.48 0.47 0.47 0.50 0.58\n0.12 0.24 0.25 0.24 0.31 0.26 0.26 0.32 0.45 0.38 0.42 0.50sonar-reasoning-pro-high gemini-2.5-pro-grounding\nsonar-pro-highsonar-reasoning\nsonar-pro\nsonargemini-2.5-flash-grounding gemini-2.0-flash-grounding\ngpt-4o-search-highgpt-4o-search-high-loc\ngpt-4o-searchgpt-4o-mini-search\ngpt-4o-mini-searchgpt-4o-searchgpt-4o-search-high-locgpt-4o-search-highgemini-2.0-flash-groundinggemini-2.5-flash-groundingsonarsonar-prosonar-reasoningsonar-pro-highgemini-2.5-pro-groundingsonar-reasoning-pro-high\n0.20.30.40.50.60.70.8Model BModel A\nFigure A12: Pairwise Win Rates . Pairwise win rates between Search Arena models.\n•Factual and non-Factual prompts: We observe a clear split in the leaderboard on the Factual\nsubset. Additionally, the gap between the three top models (including sonar-pro-high )\n“zeros out” on the non-factual subset.\nD.2 Response Length Analysis\nAverage response length distribution across Search Arena models is shown in Figure A14 (Left).\nReasoning models tend to be more verbose except for sonar-reasoning .sonar-pro ’s version with\n28\n--- Page 29 ---\n1151 1150\n1130\n1113\n11001096\n1072\n1035\n10071000 999\n973\ngemini-2.5-pro-grounding sonar-reasoning-pro-high sonar-pro-high sonar-reasoning sonar-pro sonar gemini-2.5-flash-grounding gemini-2.0-flash-grounding gpt-4o-search-high gpt-4o-search-high-loc gpt-4o-search gpt-4o-mini-search9501000105011001150RatingFigure A13: Search Arena Leaderboard . Model scores based on Elo-scaled Bradley-Terry\ncoefficients.\nTable A3: Search Arena Leaderboard. Elo-scaled Bradley-Terry ratings along with corresponding\nranks (Rating (Rank)). Model ratings based on two subsets of the full dataset: (1) English vs\nnon-English prompts, (2) Factual ( Factual Lookup andInfo Synthesis ) vs non-Factual prompts.\nModel Rating Rating (English) Rating (non-English) Rating (Factual) Rating (non-Factual)\ngemini-2.5-pro-grounding 1150.6 (1) 1146.8 (1) 1158.3 (1) 1155.8 (1) 1143.9 (1)\nsonar-reasoning-pro-high 1150.1 (1) 1137.5 (1) 1174.2 (1) 1161.4 (1) 1142.5 (1)\nsonar-pro-high 1129.8 (1) 1125.7 (1) 1136.2 (2) 1110.1 (3) 1144.1 (1)\nsonar-reasoning 1113.2 (3) 1112.0 (2) 1117.9 (3) 1105.9 (3) 1121.0 (2)\nsonar-pro 1099.9 (4) 1101.5 (3) 1093.1 (4) 1092.0 (3) 1106.8 (4)\nsonar 1095.7 (4) 1092.8 (4) 1100.6 (3) 1085.6 (3) 1106.0 (4)\ngemini-2.5-flash-grounding 1072.0 (5) 1061.4 (6) 1098.4 (3) 1095.0 (3) 1059.2 (7)\ngemini-2.0-flash-grounding 1035.1 (8) 1017.8 (8) 1060.7 (5) 1035.5 (8) 1035.4 (7)\ngpt-4o-search-high 1007.5 (9) 1000.9 (8) 1018.7 (8) 1007.7 (8) 1005.6 (8)\ngpt-4o-search-high-loc 999.9 (9) 989.9 (8) 1019.7 (9) 987.1 (9) 1009.4 (8)\ngpt-4o-search 999.2 (9) 999.4 (8) 1002.1 (9) 1000.5 (8) 999.5 (9)\ngpt-4o-mini-search 972.7 (12) 970.7 (9) 973.6 (11) 973.1 (9) 969.6 (11)\na higher search context generates longer responses compared to the version with a medium context.\nResponse length distribution is uniform across OpenAI’s models.\nWe control for response length difference in the Bradley-Terry model and compute the corresponding\ncoefficient across different subsets of the full data, broken down by intent category (see Figure A15).\nFor all intent categories, the coefficient (effect) is statistically significant; however, as expected, the\neffect is smallest for Factual Lookup prompts.\nE Citation Analysis\nE.1 Citation Count\nCitation count distribution is shown in Figure A14 (Right). As expected, models with higher search con-\ntext size cite more sources (e.g., sonar-pro-high vssonar-pro ). Furthermore, reasoning models\ntend to cite less sources compared to non-reasoning variants (e.g., sonar-reasoning-pro-high vs\nsonar-pro-high ,sonar-reasoning vssonar ). We hypothesize that reasoning models synthesize\nand filter irrelevant sources before final response generation, resulting in less cited sources in the\nfinal response (see Figure 3). Interestingly, gemini-2.5-pro-grounding cites fewer sources\ncompared to gemini-2.5-flash-grounding , suggesting that even though both are reasoning\nmodels, gemini-2.5-pro-grounding filters out more sources from the final response.\nWe then control for citation count difference in the Bradley-Terry model and compute the corresponding\ncoefficient across different subsets of the dataset broken down by intent category (see Figure A16).\nThe effect of citation count on Guidance (e.g., debugging, problem-solving) prompts is not significant.\nThe effect is largest on Analysis prompts.\n29\n--- Page 30 ---\nsonar-reasoning-pro-highsonar-pro-highsonar\nsonar-pro\nsonar-reasoninggemini-2.5-progemini-2.5-flash gemini-2.0-flash\ngpt-4o-mini-searchgpt-4o-search\ngpt-4o-search-high0100200300400500\nsonar-reasoning-pro-highsonar-pro-highsonar\nsonar-pro\nsonar-reasoninggemini-2.5-progemini-2.5-flash gemini-2.0-flash\ngpt-4o-mini-searchgpt-4o-search\ngpt-4o-search-high024681012Response Length (num words)\nCitation CountFigure A14: (Left) Response length distribution. Reasoning models and models with high search\ncontext size tend to be more verbose. (Right) Citation count distribution. As expected, models with\nhigher search context size cite more sources. Reasoning models cite fewer sources compared to\nnon-reasoning variants (e.g., sonar-reasoning-pro-high vssonar-pro-high ).\n0.160.380.400.47 0.470.490.500.51\nFactual Lookup Explanation Recommendation Text Processing Info Synthesis Guidance Analysis Creative Generation00.10.20.30.40.50.60.7Coefficient Estimate\nFigure A15: Response Length Control across Intents . Bradley-Terry coefficients corresponding\nto response length across different intent categories. Length has less effect on Factual Lookup prompts.\n0.060.200.32 0.32 0.330.340.370.42\nGuidance Factual Lookup Info Synthesis Text Processing Recommendation Creative Generation Explanation Analysis−0.100.10.20.30.40.50.6Coefficient Estimate\nFigure A16: Citation Count Control across Intents . Bradley-Terry coefficients corresponding to\ncitation count across different intent categories. Citation count does not have significant effect on\nGuidance (e.g., debugging, problem-solving) prompts. The effect is largest for Analysis queries.\n30\n--- Page 31 ---\nE.2 Citation Sources\nTo categorize citation sources, we use the following mapping from source domains to categories:\n•youtube : “youtube.com”.\n•gov_edu : “.gov”, “.edu”, “.mil”.\n•wiki: “wikipedia”, “wikihow”, “wikimedia”.\n•us_news : “cnn.com”, “apnews.com”, “cnbc.com”, “bloomberg.com”, “economist.com”, “ny-\ntimes.com”, “washingtonpost.com”, “wsj.com”, “nbcnews.com”, “abcnews.go.com”, “usato-\nday.com”, “npr.org”, “latimes.com”, “vox.com”, “huffpost.com”, “ft.com”, “foxnews.com”,\n“axios.com”, “time.com”, “buzzfeed.com”, “cbsnews.com”, “politico.co”, “newsweek.com”,\n“fortune.com”, “theatlantic.com”, “whattowatch.com”, “scrippsnews.com”, “investope-\ndia.com”, “yahoo.com”, “breitbart.com”, “washingtontimes.com”, “dailycaller.com”,\n“thefederalist.com”, “townhall.com”, “pjmedia.com”, “westernjournal.com”, “forbes.com”.\n•foreign_news : “reuters.com”, “bbc.com”, “aljazeera.com”, “dw.com”, “france24.com”,\n“as.com”, “elpais.com”, “cbc.ca”, “theglobeandmail.com”, “smh.com.au”, “abc.net.au”,\n“japantimes.co.jp”, “straitstimes.com”, “hindustantimes.com”, “thehindu.com”, “economic-\ntimes.indiatimes.com”, “indianexpress.com”, “independent.co.uk”, “theguardian.com”,\n“cadenaser.com”, “lemonde.fr”, “vnexpress.net”, “ndtv.com”.\n•social_media : “tiktok.com”, “facebook.com”, “instagram.com”, “x.com”, “twitter.com”,\n“linkedin.com”, “snapchat.com”, “pinterest.com”.\n•community_blog : “reddit.com”, “quora.com”, “blog”, “medium.com”, “wordpress.com”,\n“substack.com”, “tumblr.com”.\n•tech_coding : “github.com”, “gitlab.com”, “stackexchange.com”, “microsoft.com”, “dev.to”,\n“codecademy.com”, “stackoverflow.com”.\n•academic_journal : “jstor.org”, “springer.com”, “sciencedirect.com”, “nature.com”,\n“arxiv.org”, “researchgate.com”, “biorxiv.org”.\n•retail : “amazon.com”, “ebay.com”, “walmart.com”, “target.com”, “bestbuy.com”,\n“costco.com”.\n•other : non-matched domains.\nyoutube\nsocial media\ncommunity blogtech coding\nacademic journalus news\nforeign newswiki\ngov eduretail00.10.20.30.4Perplexity Google OpenAIProportion of Responses\nFigure A17: Citation Source Distribution across Model Families. Models from different providers\nare biased towards different types of sources: (1) Perplexity models prefer citing YouTube, social\nmedia, and community blogs, (2) OpenAI models are biased towards mainstream news outlets.\nThe distribution of citation domain categories is shown in Figure A17. Perplexity’s models tend to\ncite social media (including YouTube) and community platforms (e.g., Reddit, Quora). OpenAI’s\nGPT-4o-based models are more biased towards mainstream news outlets. Google’s Gemini models\nare in between.\n31\n--- Page 32 ---\nWe can also control for all features simultaneously (response length, number of citations, and citation\nsources) to compute adjusted model scores. The scores and rankings before and after applying these\ncontrols are shown in Figure A18. We observe that the model scores and rankings tend to converge\nafter the controls are applied. This convergence is particularly evident within model families: the top\nthree models from Perplexity show significant convergence. This suggests that much of the variation\nbetween models in the same family is accounted for by the control features. We note that features\nsuch as factuality and coverage may be correlated with the number of citations or specific citation\nsources; we leave this analysis to future work.\noriginal controlled9751000102510501075110011251150gemini-2.5-pro-grounding\ngemini-2.5-flash-grounding\ngemini-2.0-flash-grounding\nsonar-reasoning-pro-high\nsonar-pro-high\nsonar-reasoning\nsonar\ngpt-4o-search-high\ngpt-4o-searchArena Score\nFigure A18: Model Scores Before and After Control. Model scores and rankings converge after\nthe controls are applied.\nE.3 Citation Attribution Analysis\nAs described in Subsection 3.2 and illustrated in Figure 7, we analyzed citation attribution on\napproximately 100 conversations per intent category. This section outlines the full pipeline.\nWe first sampled around 100 conversations from each intent category, excluding the “Other” class.\nTo retrieve the cited content, we used Firecrawl1as our default batch scraping tool. For social media\nplatforms or other domains where Firecrawl could not be applied, such as Reddit and YouTube,\nwe leveraged official APIs or extracted video transcripts when available. For the remaining URLs\nthat failed due to technical limitations, access restrictions, or licensing concerns, we excluded the\ncorresponding samples. Overall, we took care to ensure ethical data usage, relying only on publicly\naccessible content and following best practices regarding rate limits and terms of service. In total,\nwe collected over 20,000 web documents across 780 conversations.\nNext, we used gemini-2.5-flash-preview-0417 to analyze the data, selected for its strong\nperformance on long-context reasoning, reliable structured (JSON) outputs, and reasonable cost. For\neach conversation, we first used the model to parse user messages into (claim, URL) pairs, following\nthe structure shown in Figure 7(a). Then, for each pair, we provided the scraped markdown content\nand the associated claim to infer attribution judgments. Each attribution was labeled as Support ,\nIrrelevant , orContradict . The prompt used for LLM-based tagging is shown on the next page.\nAs illustrated in Figure 7(b), we aggregated the number of supporting, irrelevant, and contradicting\nclaims per turn and used these counts as features in the Bradley-Terry model, following the same setup\nas the other control experiments. Human experts validated a subset of the outputs. Future work can\nfurther scale this pipeline or add onto analysis methodology.\n1https://www.firecrawl.dev\n32\n--- Page 33 ---\n/commen◎sLLM Prompt for Claim–Citation Attribution Assessment\nTask: Tell me if the claim can be verified or supported by the web content.\nSpecifically:\n•If the claim is supported by the web content, return \"support\"\n•If the claim is contradict to the web content, return \"contradict\"\n•If the claim is completely irrelevant to the web content, return \"irrelevant\"\nClaim: {claim}\nWeb content: {web content}\nReturn in the json format:\n{\n\" reasoning \": \"< explain the reasoning >\",\n\" answer \": <\" support \" | \" contradict \" | \" irrelevant \">\n}\nF Cross-Setting Analysis\nIn this section, we extend our analysis of Subsection 3.3 and study model performance changes across\ndifferent benchmarks. Specifically, we explore the following research question: How does model\nperformance and ranking change across different search and non-search benchmarks?\nFor offline evaluation of models in search settings, we use BrowseComp [ 61] and SimpleQA [ 60]2.\nFor testing the general performance of the models in non-search settings, we use ArenaHard-v2 [ 35].\nBrowseComp and SimpleQA assess factuality based on well-specified ground truths, while ArenaHard-\nv2 utilizes an LLM-as-a-judge framework on a set of challenging prompts filtered from Chatbot Arena’s\nText Arena dataset. Although these benchmarks test different aspects of standard and search-augmented\nLLMs, all are partially captured by the Search Arena prompt distribution shown in Figure 1.\nThe selected models have near-zero accuracy on the BrowseComp benchmark; this finding is not\nsurprising, as the questions are specifically designed to evaluate and challenge deep research pipelines.\nThe model scores for the rest of the benchmarks–Search Arena, SimpleQA, and ArenaHard-v2–are\nprovided in Table A4. Additionally, we calculate model win rates on two subsets of the Search\nArena dataset based on the annotated intent classes (see Section 2)–Search Arena (Fact+Synth)\nincludes only Factual Lookup andInfo Synthesis queries, while Search Arena (Other) contains the\nremaining subset. We use Kendall’s tau for comparing ranks and Pearson correlation for comparing\nraw scores across benchmarks. On SimpleQA, model accuracy is saturated (ranging from 89% to\n93%), with minimal separability between the models and low agreement with Search Arena ( τ=0.422,\nr=0.582). In contrast, ArenaHard-v2 shows greater performance variance and separability, and has\na higher agreement ( τ= 0.556,r= 0.844) with Search Arena compared to SimpleQA. However, the\nrankings differ; notably, the three reasoning models rank lower in ArenaHard-v2, suggesting that,\nwhile combining web search with reasoning improves performance in Search Arena, it may degrade\nperformance on ArenaHard-v2 prompts. Additionally, when comparing the two subsets of the Search\nArena with ArenaHard, Search Arena (Other) has higher agreement ( τ= 0.644,r= 0.882) compared\nto Search Arena (Fact+Synth) ( τ=0.511,r=0.675). This finding is expected as the prompt distribution\nin Search Arena (Other) is closer to that of ArenaHard (e.g., creative writing, problem-solving).\nFurthermore, we compare model performance with and without search on the SimpleQA and\nArenaHard-v2 benchmarks. We used Gemini models in this case study, as search is implemented as\na tool ( GoogleSearch tool) and can be easily turned on and off. The performance change is shown\nin Figure A20. Search significantly improves model performance on SimpleQA (models’ performance\nsaturates at around 90%). However, performance degrades on ArenaHard-v2, with non-search variants\nachieving higher scores.\n2Due to the high cost of running search-augmented LLMs, we evaluated each model on the same randomly\nsampled subset of 500 questions. The subset was sampled once and shared across all models.\n33\n--- Page 34 ---\nTable A4: Model scores across three benchmarks–Search Arena (win rate), SimpleQA (accuracy),\nand ArenaHard-v2 (win rate). On SimpleQA, models’ performance is saturated and lead to minimal\nseparability in model scores. Search Arena and ArenaHard-v2 provide more separability, but the\nrankings are different.\nModel Search Arena Search Arena (Fact.+Synth.) Search Arena (Other) SimpleQA ArenaHard-v2\nsonar-reasoning-pro-high 66.6 (-2.1 / +2.2) 68.8 (-3.3 / +3.4) 65.0 (-3.1 / +2.7) 91.8 (-2.4 / +2.4) 28.3 (-1.7 / +2.1)\ngemini-2.5-pro-grounding 66.7 (-2.5 / +2.4) 68.3 (-3.8 / +3.7) 65.3 (-3.2 / +3.2) 90.6 (-2.6 / +2.4) 33.5 ( -2.1 / +2.4)\nsonar-pro-high 63.7 (-2.2 / +2.4) 59.9 (-3.5 / +3.3) 66.7 (-2.8 / +2.9) 93.2 (-2.2 / +2.2) 43.4 (-2.1 / +2.2)\nsonar-reasoning 60.1 (-2.5 / +2.3) 58.8 (-3.4 / +3.3) 61.3 (-3.2 / +3.4) 92.6 (-2.4 / +2.2) 29.1 (-2.4 / +2.3)\nsonar-pro 57.5 (-3.1 / +2.8) 56.8 (-4.3 / +4.2) 58.4 (-3.9 / +4.0) 92.6 (-2.4 / +2.2) 39.9 (-2.2 / +2.2)\nsonar 55.8 (-2.7 / +2.8) 54.8 (-4.1 / +3.8) 56.8 (-3.9 / +3.8) 91.8 (-2.4 / +2.4) 36.2 ( -1.9 / +2.1)\ngemini-2.5-flash-grounding 49.8 (-3.7 / +3.5) 54.5 (-5.9 / +5.8) 46.1 (-4.8 / +4.7) 89.8 (-2.6 / +2.6) 22.6 (-1.4 / +1.8)\ngemini-2.0-flash-grounding 41.7 (-2.8 / +2.9) 42.8 (-4.2 / +4.2) 41.0 (-3.5 / +3.4) 89.2 (-2.8 / +2.6) 13.4 (-1.0 / +1.1)\ngpt-4o-search-preview-high 34.8 (-2.5 / +2.5) 36.2 (-3.6 / +3.8) 33.6 (-3.5 / +3.2) 89.4 (-2.8 / +2.6) 16.6 (-1.2 / +1.2)\ngpt-4o-mini-search-preview 29.3 (-2.7 / +2.6) 30.3 (-3.7 / +4.2) 28.6 (-3.6 / +3.6) 91.2 (-2.6 / +2.4) 8.3 (-0.6 / +0.7)\nSearch Arena\nSearch Arena (Fact+Synth)\nSimpleQA ArenaHardSearch Arena (Other)\n0 20 40 60 80 100sonar-reasoning-pro-high\ngemini-2.5-pro-grounding\nsonar-pro-high\nsonar\ngemini-2.5-flash-grounding\ngpt-4o-search-preview-high\ngpt-4o-mini-search-preview\nFigure A19: Model Scores Across Benchmark. (1) Models’ performance on SimpleQA is saturated\nwith minimal separability. (2) Performance variance in ArenaHard-v2 is comparable to that of Search\nArena, but the rankings are different, with reasoning models having higher performance in search\nsettings. (3) Model scores and ordering in Search Arena (Other) is closer to that of ArenaHard\ncompared to Search Arena (Fact+Synth).\ngemini-2.5-progemini-2.5-pro\ngemini-2.5-flashgemini-2.5-flash\ngemini-2.0-flashgemini-2.0-flash\n20 30 40 50 60 70 80 90152025303540455055\nsearch\nnon-search\nFit: y = -0.20x + 43.74\nSimpleQA ScoreArenaHard Score\nFigure A20: SimpleQA vs ArenaHard-v2 Performance . Search and non-search performance of\nGemini models on SimpleQA and ArenaHard-v2 benchmarks. Search improves performance on\nSimpleQA, while degrades the score on ArenaHard-v2.\n34",
  "text_length": 102125
}