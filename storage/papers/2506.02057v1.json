{
  "id": "http://arxiv.org/abs/2506.02057v1",
  "title": "Enhancing Speech Instruction Understanding and Disambiguation in\n  Robotics via Speech Prosody",
  "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.",
  "authors": [
    "David Sasu",
    "Kweku Andoh Yamoah",
    "Benedict Quartey",
    "Natalie Schluter"
  ],
  "published": "2025-06-01T14:06:57Z",
  "updated": "2025-06-01T14:06:57Z",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.02057v1",
  "full_text": "--- Page 1 ---\narXiv:2506.02057v1  [cs.RO]  1 Jun 2025Enhancing Speech Instruction Understanding and Disambiguation in Robotics\nvia Speech Prosody\nDavid Sasu1, Kweku Andoh Yamoah2, Benedict Quartey3, Natalie Schluter1,∗\n1Computer Science, IT University of Copenhagen, Denmark\n2Intelligent Agents Research Group, University of Florida, U.S.A\n3Intelligent Robot Lab, Brown University, U.S.A\ndasa@itu.dk, kyamoah@ufl.edu, benedict_quartey@brown.edu, nael@itu.dk\nAbstract\nEnabling robots to accurately interpret and execute spoken\nlanguage instructions is essential for effective human-robot col-\nlaboration. Traditional methods rely on speech recognition to\ntranscribe speech into text, often discarding crucial prosodic\ncues needed for disambiguating intent. We propose a novel ap-\nproach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large lan-\nguage models via in-context learning to disambiguate and select\nappropriate task plans. Additionally, we present the first ambigu-\nous speech dataset for robotics, designed to advance research in\nspeech disambiguation. Our method achieves 95.79% accuracy\nin detecting referent intents within an utterance and determines\nthe intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve\nhuman-robot communication.\nIndex Terms : speech recognition, human-computer interaction,\ncomputational paralinguistics, speech prosody, robot planning,\nspeech understanding, speech disambiguation\n1. Introduction\nUnderstanding and executing natural language instructions is\na fundamental goal in human-robot interaction. To accurately\nfollow instructions, robots must grasp several key elements\nsuch as the speaker’s intent, relevant referents, and execution\nconstraints. However, instructions conveyed through text\nalone often fail to capture the wide variety of meanings that\npeople intend. This limitation arises because rich speech\nprosodic cues—such as intonation, stress, and rhythm—that\nsignificantly affect the meaning of instructions are lost in textual\nrepresentations. Consider the ambiguous instruction shown\nin Figure 1: “Place the coke can beside the pringles on the\ncounter”. One interpretation of this task is to place the coke\ncan at a location beside the pringles, which happens to be on\na counter. Another interpretation is to find the coke can (which\nis currently beside the pringles) and move it to the counter.\nDisambiguating such instructions relies on prosodic cues like\npauses and emphasis, which convey the intended meaning.\n1\nCurrent speech to motion robot systems predominantly rely\non automatic speech recognition (ASR) to transcribe speech into\ntext before processing [ 1,2,3]. While effective at capturing the\nliteral words, ASR strips away essential prosodic cues like stress,\nrhythm, and intonation. This loss renders robots incapable of\ninterpreting the speaker’s true intent in ambiguous or complex\ninstructions. Existing approaches attempt to resolve ambiguity\nthrough contextual or semantic analysis of the transcribed text,\n1* Currently at Apple.\nFigure 1: Illustration of two distinct interpretations of the in-\nstruction “Place the coke can beside the pringles on the counter.”\nLeft (green) : The robot picks up the coke can and places it next\nto the pringles on the counter. Right (pink) : The robot locates\nthe coke can already beside the pringles and relocates it onto the\ncounter. Each plan is represented by a pointcloud of the scene\nand the robot’s motion path, while the robot images illustrate\nthe pick-and-place actions.\nbut they fall short when critical information is embedded in the\ntonal aspects of speech [4, 5].\nTo address this limitation, we propose a novel approach that\nexplicitly leverages speech prosody to resolve ambiguities in spo-\nken robot instructions. Concretely, we extract prosodic features\nfrom speech and employ an architecture that assigns an intent la-\nbel—either goal (entities or locations to be acted upon) or detail\n(qualifiers specifying attributes or relationships)—to each word\nin an utterance. Reexamining the earlier example, in the first\ninterpretation “coke can” and “pringles” would be classified as\ngoal intent referents , while “counter” is a detail intent referent\nthat qualifies the location of “pringles”. In the alternative inter-\npretation “coke can” and “counter” would be classified as goal,\nand “pringles” as a detail specifying the location of the coke\ncan. We illustrate this distinction in interpretation and equivalent\ntask plans in Figure 1. We make three main contributions in this\nwork:\n1.Prosody-Aware Architecture. We introduce an encoder-\ndecoder model that exploits prosodic features for token-level\nintent classification, capturing long-range dependencies\nwithin spoken instructions.\n2.LLM Integration. We demonstrate how prosody-driven\nintent predictions can be used with large language models,\nto select appropriate task plans from multiple candidates,\nbridging the gap between ambiguous human instructions and\nprecise robot execution.\n--- Page 2 ---\n3.Ambiguous Speech Dataset. We highlight speech instruction\ndisambiguation as a key research area in robotics and curate\na novel dataset of 1,540 ambiguous utterances, to advance\nfuture work.\n2. Related works\nRecent works have demonstrated the effectiveness of incorporat-\ning prosody into speech understanding tasks such as intent classi-\nfication [ 6,7,8,9]. For instance, Wei et al. [ 9] proposed a neural\nprosody encoder that leverages prosodic information for end-to-\nend dialogue act classification, using a learnable gating mech-\nanism to assess and selectively retain critical prosodic features.\nOther studies have explored prosody-based attention and distil-\nlation to improve end-to-end Spoken Language Understanding\n(SLU) [ 8]. While these approaches underscore the importance of\nprosodic features in SLU systems, they remain underexplored in\nhuman-robot interaction scenarios, where precise understanding\nof spoken instructions is essential for effective task execution.\nEarly systems for robot instruction following from language\noften relied on hand-crafted grammars and symbolic represen-\ntations to map natural language commands to executable actions\n[10]. Although these rule-based methods proved useful for nav-\nigation and simple tasks, their reliance on limited vocabularies\nand rigid syntactic structures restricted their ability to handle\nthe nuances of spontaneous human speech. Subsequent research\nshifted toward grounded language understanding , where\nlanguage is explicitly tied to a robot’s perception and motion.\nSeminal works[ 11,12] introduced probabilistic models that\nmap linguistic elements—like verbs and object references—to\nreal-world entities and spatial relations. This enabled robots to\ninterpret instructions such as “Place the cup on the table” by\nidentifying and manipulating the correct objects and locations.\nIn parallel, deep learning–based approaches have expanded the\nscope of language instruction following by shifting from rigid\ngrammars to data-driven end-to-end strategies that learn directly\nfrom large corpora. Benchmarks and models integrating both\nvisual and linguistic inputs have been introduced for a variety\nof household tasks, demonstrating the growing synergy between\nperception and language processing [ 13,14]. More recently,\nthe emergence of large-scale pretrained foundation models has\nallowed robots to interpret and solve complex natural language\ndirectives without any training [15, 16, 17].\nDespite these advances, a critical gap remains in the use\nof prosodic features present in speech—such as stress, rhythm,\nand intonation—to enhance instruction comprehension. Current\nsystems predominantly rely on text transcripts, discarding the\nacoustic cues that humans naturally employ to convey emphasis\nand resolve ambiguities. We address this gap by explicitly\nintegrating prosodic cues into the instruction-following\npipeline. Our approach enables robots to more effectively\ninterpret emphasis, disambiguate intent, and achieve a robust\nunderstanding of tonally ambiguous instructions.\n3. Speech instruction understanding and\ndisambiguation\nWe introduce Bidirectional Long Short Term memory (BiLSTM)\n[18] and Transformer-based [ 19] encoder-decoder models\ndesigned to identify referent intents by leveraging prosody. Both\narchitectures are optimized to capture long-range dependencies\nin prosodic features extracted from speech, enabling the disam-\nbiguation of goal anddetail intents within a given utterance.3.1. Overview\nFor each speech sample in our dataset, we first perform forced\nalignment to determine the temporal boundaries of each word.\nNext, we extract prosodic features for each word using the\nDisvoice Python library2. Simultaneously, we generate a\ntext embedding using OpenAI’s Text Embedding Large 3\nmodel3. The extracted prosodic and textual features are then\nconcatenated to form a unified feature representation. This\nintegration is crucial as it allows the model to utilize both the\nsemantic content of words and prosodic cues that can influence\nmeaning and interpretation.\nFormally, we represent the input sequence for the i-th speech\nutterance as:\nX(i)={x(i)\n1,x(i)\n2, ...,x(i)\nTi} (1)\nwhere each x(i)\ntis the feature vector corresponding to the\nt-th word in the utterance, defined as:\nx(i)\nt∈Rdprosody +dembed(2)\ndprosody anddembed denote the dimensionalities of the\nprosodic and textual embedding feature spaces, respectively.\nThese constructed feature vectors are passed to our models,\nwhich generate a corresponding sequence of labels:\nY(i)= [y(i)\n1,y(i)\n2, ...,y(i)\nTi] (3)\nwhere each y(i)\ntrepresents the predicted label for the t-th\nword in the utterance. Each label indicates whether the word is\nnot of interest , agoal intent referent , or a detail intent referent .\n3.2. BiLSTM-based encoder-decoder architecture\nIn our BiLSTM-based architecture, input prosodic and text em-\nbedding features are first processed through an Attention Fusion\nlayer, which dynamically assigns importance to different words\nin the utterance. Each word’s feature vector undergoes a lin-\near transformation followed by a tanh activation to compute\nan attention score. The computed attention scores are normal-\nized using a softmax function and subsequently applied to their\ncorresponding feature vectors via element-wise multiplication.\nThis results in a fused feature representation that preserves the\nsequence structure while enhancing the prominence of salient\nwords and suppressing less informative ones. By selectively\nweighting the input features, the model effectively prioritizes\ncontextually relevant patterns, improving its ability to capture\nfine-grained variations in speech.\nThe fused features are passed to the encoder, which begins\nwith a linear projection followed by a ReLU activation to reduce\nthe dimensionality of the input. The output of the linear pro-\njection is then fed into a stack of BiLSTM layers, which encode\nrich contextual representations by capturing both forward and\nbackward dependencies within the sequence. However, since\nBiLSTM layers struggle to model long-range dependencies\neffectively [ 20,19], we incorporate a Multi-Head Attention layer\nto enhance the model’s ability to capture relationships between\ndistant elements in the sequence. The attention scores computed\nfrom the Multi-Head Attention layer are then applied to the final\n2Link to Disvoice library: https://disvoice.readthedocs.io/\nen/latest/index.html\n3Link to OpenAi’s Text Embedding Large 3 model: https://\nplatform.openai.com/docs/guides/embeddings\n--- Page 3 ---\n(a)BiLSTM based Encoder-Decoder Model\n (b)Transformer-based Encoder-Decoder Model\nFigure 2: Overview of our architectures for word-level intent classification. (a) BiLSTM-based Model: Prosodic features are fused with\ntext embeddings and fed into a BiLSTM encoder. An attention layer refines the fused representations before a decoder predicts intent\nlabels for each token. (b) Transformer-based Model: Prosodic and textual inputs are combined and passed through a Transformer\nencoder with multi-head self-attention and positional encodings. A masked self-attention decoder then generates token-level intents.\nIn both models, predicted intent labels are passed to a large language model (LLM) for final task plan selection.\nhidden states of the BiLSTM layers via element-wise multiplica-\ntion. We concatenate the resulting vectors from this computation\nwith the original word embeddings of the utterance to obtain\na final representation, zt, that encapsulates both prosodic and\ntextual information, ensuring that the decoder receives seman-\ntically and acoustically rich features. In the decoder, a stack of\nBiLSTM layers and a final fully connected layer process these\nfeatures and assign labels to each word in the input utterance.\n3.3. Transformer-based encoder-decoder architecture\nTransformers inherently lack temporal inductive biases [ 19], thus\nin our Transformer-based model we first apply a linear trans-\nformation to our input prosodic and text embedding features\nthen integrate fixed positional encodings into input features to\nprovide word order information. This sequence is then processed\nthrough a stack of Transformer encoder layers. Each layer em-\nploys multi-head self-attention mechanisms to capture global\ndependencies and relationships between words, irrespective of\ntheir positional distances. The depth of the encoder—determined\nby the number of layers—enables the model to progressively\nlearn abstract representations of the input data. Before decoding,\nwe concatenate the encoder outputs with the initially projected\ninput features and apply another linear projection. This process\nensures that the decoder benefits from the detailed acoustic and\nsemantic features present in the original input and the global\ncontextual information encoded by the Transformer.\nThe decoder generates per-word labels in a left-to-right fash-\nion, consistent with the sequential nature of the prediction task.\nDuring training, each target word’s ground truth label is shifted\nby one position and prefixed with a special start-of-sequence\n(SOS) token, which serves as the initial input to the decoder.\nReintroducing the original embeddings into the decoder im-\nproves the model’s ability to capture specific word properties\nduring label generation. The decoder uses masked self-attention\nto maintain the autoregressive property required for sequential\nprediction. This causal masking ensures that each output position\ncan only attend to preceding positions, preventing information\nleakage from future tokens. Finally, a fully connected layer with\nsoftmax activation maps decoder outputs to a probability distribu-\ntion over all possible class labels for each word in the sequence.3.4. Large language modelling head\nWe incorporate the per-word goal and detail intent predictions\nfrom our speech understanding models into a Large Language\nModel (LLM) to determine appropriate task plans a robot\nmay follow to satisfy the given instruction. Specifically, we\nfeed the transcribed text utterance along with the predicted\nintents and in-context examples into an LLM prompt, enabling\nthe model to infer the most appropriate interpretation and\nsubsequent task plan. We leverage the prompting structure and\nComposable Referent Descriptor (CRD) syntax proposed by\n[15]. To evaluate the effectiveness of different LLMs in this\ncontext, we experiment with three high-performing language\nmodels from OpenAI: GPT-4o4, o1-mini5, and o3-mini6.\n4. Experimental setup\n4.1. Data\nTo evaluate our approach, we collected a novel accented\nspeech dataset of 35 ambiguous instructions , inspired by\n[15]. Each instruction in this dataset can be interpreted in two\ndistinct ways, resulting in two seperate sets of intent referents\nper instruction. Notably, the goal and detail referents vary\nbetween interpretations, introducing structured ambiguity that\nour models aim to resolve. To create the speech dataset, we\nrecruited 22 participants (10 males, 12 females) to record\nall 35 instructions, providing both possible interpretations.\nEach participant provided 44 samples recordings resulting in\n1,540 voice samples with a total duration of approximately\n121 minutes . The recordings were collected using a Google\nPixel 8 smartphone. The training set consisted of 1,056 samples,\nwhereas the validation and test sets contained 264 and 220 sam-\nples respectively. To encourage generalization, we designed the\ndataset split such that the instructions present in the evaluation\nsets were not included in the training data. In our experiments,\nwe conduct feature ablation studies to examine the impact of\ndifferent input representations on intent detection performance\n4https://openai.com/index/hello-gpt-4o/\n5https://openai.com/o1/\n6https://openai.com/index/openai-o3-mini/\n--- Page 4 ---\nTable 1: Performance in Classifying Goal and Detail Referent Intents for Transformer and BiLSTM Models.\nProsody Raw Prosody+Raw\nModel Intent Acc Prec Rec F1 Acc Prec Rec F1 Acc Prec Rec F1\nTransformerGoal 92.61% 85.41% 75.60% 80.20% 90.81% 76.79% 76.79% 76.79% 90.96% 84.50% 66.51% 74.43%\nDetail 94.70% 83.85% 81.82% 82.82% 91.10% 80.08% 57.27% 66.78% 89.11% 70.16% 52.73% 60.21%\nOverall 93.31% 90.04% 90.25% 90.06% 93.92% 88.13% 88.73% 88.07% 88.59% 83.22% 83.90% 82.95%\nBiLSTMGoal 93.04% 97.21% 66.75% 79.15% 91.48% 84.00% 70.33% 76.56% 91.90% 84.94% 67.46% 75.20%\nDetail 94.22% 76.13% 91.82% 83.24% 91.43% 73.95% 69.70% 71.76% 91.81% 72.89% 75.76% 74.29%\nOverall 95.79% 92.87% 92.14% 91.80% 94.31% 88.89% 89.39% 88.95% 94.74% 89.50% 89.77% 89.39%\nTable 2: LLM Model Performance in Choosing the Correct Task\nPlans.\nLLM+ASRLLM+Prosody-\nBiLSTMLLM+Prosody-\nTransformer\nGpt-4o 50% 56.84% 71.96%\no1-mini 50% 52.10% 62.5%\no3-mini 50% 54.54% 61.83%\nbeyond prosodic features alone. In one setting, we extract only\nraw audio features using the Librosa library7. In another, we\nintegrate both prosodic and raw audio features as input to our\nmodels. This experimental setup allows us to systematically\nevaluate whether prosodic features alone can effectively enhance\nthe detection of goal and detail intents or if a combination of\nprosodic and raw features leads to improved performance.\n4.2. Training procedure\nWe train our models by minimizing the cross-entropy loss\nbetween the predicted label sequences, ˆY(i), and the correspond-\ning ground truth, Y(i). To identify the optimal hyperparameter\nconfigurations, we conduct a grid search , using the Optuna\nhyperparameter optimization framework8, exploring a range\nof values for the learning rate, hidden dimension, number of\nlayers, dropout rate, attention-layer depth, and weight decay.\nThe values that maximize the F1-score on the validation set are\nselected. The optimal learning rate for the Transformer-based\nmodel is found to be 2.22×10−4, while the BiLSTM-based\nmodel performs best with a learning rate of 4.2×10−3. The\nhidden dimension of the linear layers is optimized at 448for the\nTransformer-based model and 512for the BiLSTM-based model.\nRegarding model depth, the best-performing Transformer\nmodel consists of three layers , whereas the optimal BiLSTM\nmodel is asingle-layer architecture . For regularization , the\ndropout rate is optimized at 0.25for the Transformer model\nand0.45for the BiLSTM model. The Transformer model’s\nattention mechanism benefits from a deeper configuration,\nconsisting of eight attention layers , whereas the BiLSTM model\noperates effectively with a single attention layer . Finally, weight\ndecay, which plays a crucial role in controlling overfitting, is\noptimized at 2.25×10−6for the Transformer-based model and\n5.44×10−5for the BiLSTM-based model. The total number\nof parameters for our BiLSTM-based model is 18,153,476\nwhile the total number of parameters for our Transformer-based\nmodel is 19,003,014 . During training, we optimize model\nparameters using the Adam [ 21] optimizer with weight decay.\n7https://github.com/librosa/librosa\n8https://optuna.orgAlearning-rate scheduler is employed to dynamically adjust\nthe step size if progress stagnates. It takes on average 8 hours\nto perform a hyperparameter search and train our models.\n5. Results\n5.1. Referent detection task performance\nOur experimental results indicate that utilizing prosodic features\nalone consistently yields the highest performance in goal and\ndetail referent detection tasks. These findings suggest that while\nraw acoustic features are informative, prosodic signals exhibit\nsuperior discriminative power , effectively highlighting key\nacoustic patterns associated with instruction intent that may\notherwise be obscured within raw audio signals. Architecturally,\ntheBiLSTM model slightly outperforms the Transformer\nmodel in referent detection, which aligns with expectations, as\nTransformer models are known to benefit more significantly from\nlarger training datasets [22].\n5.2. Intent Disambiguation task performance\nFor the intent disambiguation task, which involves selecting the\nappropriate task plan given a speech utterance, our experimental\nresults, presented in Table 2, demonstrate that augmenting LLM\ninput prompts with goal and detail intent referents detected by\nour proposed prosody-based models significantly enhances per-\nformance. Notably, the best performance on this task is achieved\nby the combination of our Prosody-Transformer model and GPT-\n4o. We hypothesize that GPT-4o outperforms the other LLMs\ndue to its likely larger model size and richer internal representa-\ntions, which enable superior generalization across diverse tasks.\n6. Limitations and conclusion\nOur work is limited by a relatively small dataset (1540 samples)\nand the narrow age range (18–22) of participants, potentially\nrestricting the generalizability of our findings. Our future\nefforts will focus expanding the dataset size and participant\ndiversity to enhance model robustness and applicability\nacross varied speaker populations. In this study, we have\ndemonstrated the pivotal role of prosodic information in\naccurately interpreting speech instructions for human-robot\ninteraction. By incorporating prosodic cues, robots can more\neffectively infer intent, thereby improving the naturalness\nof human-robot communication. We also introduce speech\ndisambiguation as a key research area in robotics and present a\nfirst-of-its-kind dataset to catalyze further advances in this field.\n--- Page 5 ---\n7. References\n[1]D. T. Tran, D. H. Truong, H. S. Le, and J.-H. Huh,\n“Mobile robot: automatic speech recognition application for\nautomation and stem education,” Soft Comput. , vol. 27,\nno. 15, p. 10789–10805, Feb. 2023. [Online]. Available:\nhttps://doi.org/10.1007/s00500-023-07824-7\n[2]A. Davila, J. Colan, and Y . Hasegawa, “V oice control\ninterface for surgical robot assistants,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2409.10225\n[3]B. N. Rout, A. Das, and S. R. Sankar, “Speech recognition\nsystem for robotic control and movement,” International Journal\nof Scientific & Technology Research , vol. 4, pp. 221–225, 2015.\n[Online]. Available: https://api.semanticscholar.org/CorpusID:\n60663713\n[4]I. McLoughlin and N. Indurkhya, AI, Human–Robot Interaction,\nand Natural Language Processing , ser. Cambridge Handbooks in\nLanguage and Linguistics. Cambridge University Press, 2023, p.\n436–454.\n[5]R. Liu, Y . Guo, R. Jin, and X. Zhang, “A review of\nnatural-language-instructed robot execution systems,” AI, vol. 5,\nno. 3, pp. 948–989, 2024. [Online]. Available: https:\n//www.mdpi.com/2673-2688/5/3/48\n[6]E. Nöth, A. Batliner, V . Warnke, J. Haas, M. Boros, J. Buckow,\nR. Huber, F. Gallwitz, M. Nutt, and H. Niemann, “On the use of\nprosody in automatic dialogue understanding,” Speech Communi-\ncation , vol. 36, no. 1-2, pp. 45–62, 2002.\n[7]E. Shriberg and A. Stolcke, “Prosody modeling for automatic\nspeech recognition and understanding,” in Mathematical Founda-\ntions of Speech and Language Processing . Springer, 2004, pp.\n105–114.\n[8]S. Rajaa, “Improving end-to-end slu performance with prosodic\nattention and distillation,” arXiv preprint arXiv:2305.08067 , 2023.\n[9]K. Wei, D. Knox, M. Radfar, T. Tran, M. Müller, G. P. Strimel,\nN. Susanj, A. Mouchtaris, and M. Omologo, “A neural prosody\nencoder for end-to-end dialogue act classification,” in ICASSP\n2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2022, pp. 7047–7051.\n[10] M. MacMahon, B. Stankiewicz, and B. Kuipers, “Walk the talk:\nConnecting language, knowledge, and action in route instructions,”\nDef, vol. 2, no. 6, p. 4, 2006.\n[11] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller,\nand N. Roy, “Understanding natural language commands for\nrobotic navigation and mobile manipulation,” in Proceedings of\nthe AAAI conference on artificial intelligence , vol. 25, no. 1, 2011,\npp. 1507–1514.\n[12] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward understanding\nnatural language directions,” in 2010 5th ACM/IEEE International\nConference on Human-Robot Interaction (HRI) . IEEE, 2010, pp.\n259–266.\n[13] M. Shridhar, J. Thomason, D. Gordon, Y . Bisk, W. Han, R. Mot-\ntaghi, L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for\ninterpreting grounded instructions for everyday tasks,” in Proceed-\nings of the IEEE/CVF conference on computer vision and pattern\nrecognition , 2020, pp. 10 740–10 749.\n[14] A. Jaafar, S. S. Raman, Y . Wei, S. Juliani, A. Wernerfelt,\nB. Quartey, I. Idrees, J. X. Liu, and S. Tellex, “Lanmp: A language-\nconditioned mobile manipulation benchmark for autonomous\nrobots,” arXiv preprint arXiv:2412.05313 , 2024.\n[15] B. Quartey, E. Rosen, S. Tellex, and G. Konidaris, “Verifiably\nfollowing complex robot instructions with foundation models,”\narXiv preprint arXiv:2402.11498 , 2024.\n[16] D. Paulius, A. Agostini, B. Quartey, and G. Konidaris, “Boot-\nstrapping object-level planning with large language models,” arXiv\npreprint arXiv:2409.12262 , 2024.\n[17] J. X. Liu, A. Shah, G. Konidaris, S. Tellex, and D. Paulius,\n“Lang2ltl-2: Grounding spatiotemporal navigation commands us-\ning large language and vision-language models,” in 2024 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) .\nIEEE, 2024, pp. 2325–2332.[18] S. Zhang, D. Zheng, X. Hu, and M. Yang, “Bidirectional long short-\nterm memory networks for relation classification,” in Proceedings\nof the 29th Pacific Asia Conference on Language, Information\nand Computation , H. Zhao, Ed., Shanghai, China, Oct. 2015, pp.\n73–78. [Online]. Available: https://aclanthology.org/Y15-1009/\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,”\ninAdvances in Neural Information Processing Systems , I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.\n[Online]. Available: https://proceedings.neurips.cc/paper_files/\npaper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[20] P.-H. Li, T.-J. Fu, and W.-Y . Ma, “Why attention? analyzing\nand remedying bilstm deficiency in modeling cross-context for\nner,” ArXiv , vol. abs/1910.02586, 2019. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:203838097\n[21] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” CoRR , vol. abs/1412.6980, 2014. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:6628106\n[22] X. Niu, B. Bai, L. Deng, and W. Han, “Beyond scaling laws:\nUnderstanding transformer performance with associative memory,”\narXiv preprint arXiv:2405.08707 , 2024.",
  "text_length": 27653
}