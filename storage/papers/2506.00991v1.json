{
  "id": "http://arxiv.org/abs/2506.00991v1",
  "title": "GOBench: Benchmarking Geometric Optics Generation and Understanding of\n  MLLMs",
  "summary": "The rapid evolution of Multi-modality Large Language Models (MLLMs) is\ndriving significant advancements in visual understanding and generation.\nNevertheless, a comprehensive assessment of their capabilities, concerning the\nfine-grained physical principles especially in geometric optics, remains\nunderexplored. To address this gap, we introduce GOBench, the first benchmark\nto systematically evaluate MLLMs' ability across two tasks: 1) Generating\nOptically Authentic Imagery and 2) Understanding Underlying Optical Phenomena.\nWe curates high-quality prompts of geometric optical scenarios and use MLLMs to\nconstruct GOBench-Gen-1k dataset.We then organize subjective experiments to\nassess the generated imagery based on Optical Authenticity, Aesthetic Quality,\nand Instruction Fidelity, revealing MLLMs' generation flaws that violate\noptical principles. For the understanding task, we apply crafted evaluation\ninstructions to test optical understanding ability of eleven prominent MLLMs.\nThe experimental results demonstrate that current models face significant\nchallenges in both optical generation and understanding. The top-performing\ngenerative model, GPT-4o-Image, cannot perfectly complete all generation tasks,\nand the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\\%\naccuracy in optical understanding.",
  "authors": [
    "Xiaorong Zhu",
    "Ziheng Jia",
    "Jiarui Wang",
    "Xiangyu Zhao",
    "Haodong Duan",
    "Xiongkuo Min",
    "Jia Wang",
    "Zicheng Zhang",
    "Guangtao Zhai"
  ],
  "published": "2025-06-01T12:46:14Z",
  "updated": "2025-06-01T12:46:14Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00991v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00991v1  [cs.CV]  1 Jun 2025GOBench: Benchmarking Geometric Optics Generation and\nUnderstanding of MLLMs\nXiaorong Zhu\nzhuxiaorong@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai, ChinaZiheng Jia\nShanghai Jiao Tong University\nShanghai, ChinaJiarui Wang\nShanghai Jiao Tong University\nShanghai, China\nXiangyu Zhao\nShanghai Jiao Tong University\nShanghai AI Laboratory\nShanghai, ChinaHaodong Duan\nShanghai AI Laboratory\nShanghai, ChinaXiongkuo Min\nShanghai Jiao Tong University\nShanghai, China\nJia Wang\nShanghai Jiao Tong University\nShanghai, ChinaZicheng Zhang\nShanghai Jiao Tong University\nShanghai AI Laboratory\nShanghai, ChinaGuangtao Zhai\nShanghai Jiao Tong University\nShanghai AI Laboratory\nShanghai, China\nPlease drew a  lonely oak tree \nstands in a bright sunny \nmeadow under a blue sky.\nA long, sharp shadow across \nthe green grass, strong \ncontrast between the sunlit \ngrass and the dark shadow .PromptContent \nsubjectOptical \nPhenomenon\nQ1.Does the pencil clearly bend \nor displace at the air -water \ninterface ?Q1: Yes\nQ2.Rate the overall aesthetic  \nvisual appeal and quality of the \nimage?(Scale 1 -5)Q2:  5Input Image Questions Ground\nTruthGenerated\nImage\nMLLMsSPS \ncomputingEvaluate\nGenerative \nModel\nHuman\nAssistant Evaluate1.Please answer the given \nquestions of optical  principles.Eval Instructions\n2.Please rate the aesthetics  of \nthe image on a scale of 1 -5.\n3.Please rate the instruction \nconsistency  of the generated \nimage on a scale of 1 -5.GOBench -Optical Generation GOBench -Optical Understanding\nPromptModel \noutput\nGPT4o:  \nClaude3:      \nGrok2:   \nGemini:  \nGrok2:       \nGLM4V: \nDraw Input\nGOBench -Gen-\n1k\nQuestions Input ImageInputRating\nResultRating\nResult1.Whether the MLLMs can correctly understand the optical phenomenon of images? \n2.How do the MLLMs evaluate the images in the term of aesthetic and instruction consistency? 1.Whether the MLLMs ‚Äô generations correctly follow optical principles? \n2.How do the MLLMs ‚Äô generations perform in the term of aesthetic and instruction consistency? \n5\n4\n4Yes\nNo\nNo\nFigure 1: We propose GOBench, the first benchmark on emerging abilities of MLLMs on geometric optical generation and\nunderstanding. On the left side, GOBench-Optical Generation is to judge the MLLM‚Äôs generations, termed as GOBench-Gen-1k,\nwhether follow optical principles; on the right side, GOBench-Optical Understanding is to judge if MLLMs can correctly\nunderstand the optical phenomenon of images.\nABSTRACT\nThe rapid evolution of Multi-modality Large Language Models\n(MLLMs) is driving significant advancements in visual understand-\ning and generation. Nevertheless, a comprehensive assessment of\ntheir capabilities, concerning the fine-grained physical principles es-\npecially in geometric optics, remains underexplored. To address this\ngap, we introduce GOBench , the first benchmark to systematically\nevaluate MLLMs‚Äô ability across two tasks: 1) Generating Optically\nAuthentic Imagery and 2) Understanding Underlying Opti-\ncal Phenomena . We curates high-quality prompts of geometric\noptical scenarios and use MLLMs to construct GOBench-Gen-1k\ndataset. We then organize subjective experiments to assess the gen-\nerated imagery based on Optical Authenticity ,Aesthetic Quality ,\nandInstruction Fidelity , revealing MLLMs‚Äô generation flaws that\nviolate optical principles. For the understanding task, we apply\ncrafted evaluation instructions to test optical understanding abilityof eleven prominent MLLMs. The experimental results demonstrate\nthat current models face significant challenges in both optical gen-\neration and understanding. The top-performing generative model,\nGPT-4o-Image, cannot perfectly complete all generation tasks, and\nthe best-performing MLLM model, Gemini-2.5Pro, attains a mere\n37.35% accuracy in optical understanding. Database and codes are\npublicly available at https://github.com/Amber0614/GOBench.\nCCS CONCEPTS\n‚Ä¢Information systems ‚ÜíMultimedia databases ;Multimedia\nstreaming ; Multimedia content creation.\nKEYWORDS\nOptical Generation, Optical Understanding, Multi-modal Large Lan-\nguage Model (MLLM), AI-generated images.\n--- Page 2 ---\nConference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Xiaorong Zhu et al.\n1 Introduction\nThe advent of large language models (LLMs) such as ChatGPT\nand Gemini [ 11,37], and their open-source counterparts such as\nLLaMA [ 33], has driven artificial intelligence (AI) towards general-\npurpose assistance [ 10,15]. Building on the LLM progress, multi-\nmodal large language models (MLLMs) [ 6,26] also excel at high-\nlevel semantic tasks [ 16,19,42,43], visual understanding tasks [ 3,\n8,21,22,28,31] and multi-modal generation tasks [ 5,7,34‚Äì36].\nAlthough recent MLLMs master at various high-level multi-modal\ntasks [ 27,30,41], their proficiency with fine-grained physical prin-\nciples, for example, the generation and understanding of geometric\noptics, remains unverified.\nGeometric optics, encompassing optical phenomena governed\nby the Law of Rectilinear Propagation, the Law of Reflection, and\nSnell‚Äôs Law, is fundamental to visual appearance and composes most\noptical scenarios. [ 23,29]. However, achieving physically plausible\noptical generation [ 38] and optical understanding [ 4,46] is a signif-\nicant challenge for MLLMs [ 12,25,39]. Current models often pro-\nduce visually appealing yet physically inaccurate results [ 1,9,13],\nwith inconsistent lighting or incorrect optical effects [ 44], limiting\ntheir use in high-fidelity applications like realistic content creation\nor simulation [ 14,18,45,47,48]. This deficiency stems partly from\na lack of benchmarks systematically probing these specific optical\neffects, unlike benchmarks for semantic understanding [ 20,32] or\ngeneral VQA [2].\nTo address this gap, we introduce GOBench (Geometric Optics\nGeneration and Understanding Bench mark), the first benchmark\ndedicated to systematically evaluating MLLM ability of generating\noptical authentic imagery and understanding optical phenomena.\nIn this benchmark, we constructed the GOBench-Gen-1K image\ndataset. This involve 340unique scenarios focused on key geometric\noptics phenomena: 108forDirect Light ,121forReflection , and 111\nforRefraction . For each scenario, images are then generated by\nthree distinct state-of-the-art MLLMs (GPT-4o-Image, Seedream\n3.0, and Imagen 3), and we manually filter out similar or poor quality\ngenerations, finally constructed 1k high quality geometric optical\nimages, termed as GOBench-Gen-1K .\nFor evaluation, GOBench employs a dual-pronged approach fo-\ncusing on both the fidelity of optical generation task and the depth\nof optical understanding task. Firstly, for optical generation as-\nsessment , we conduct a human-labeling experiment where 6ex-\nperts meticulously scored each image across three key dimensions:\nOptical Authenticity (a 0-5 score reflecting physical plausibility\nbased on five specific questions), Aesthetic Quality (a 1-5 rating of\nvisual appeal), and Instruction Fidelity (a 1-5 rating of adherence\nto the generation prompt). Secondly, for optical understanding\nevaluation , we benchmark a cohort of 11 prominent MLLMs by\ntasking them to assess the GOBench-Gen-1K images along these\nsame three dimensions. Their performance is quantified against\nthe human ground truth primarily using a Scaled Proximity Score\n(SPS), which measures the alignment of their evaluative judgments\nwith human expert consensus.\nExperiment results reveal that while current MLLMs‚Äô genera-\ntions attain decent visual quality, they frequently exhibit discernible\nflaws in adhering to geometric optical principles. And in the op-\ntical understanding tasks, even advanced models face substantial\nFigure 2: Task Distribution of GOBench-Gen-1K, involves\nthree main optical categories: Direct light ,Reflect light , and\nRefracted light . Each category includes various subcategories,\nfacilitating a comprehensive dataset.\nchallenges in correctly understanding and assessing the optical\nphenomena in GOBench-Gen-1K. These findings highlight that\nrobust, physically grounded optical generation and understanding\nremain critical areas for future MLLM development.\nIn summary, our main contributions are three-fold:\n‚Ä¢We construct Gobench-Gen-1k , a first-of-its-kind dataset\nof 1k images specifically designed to test MLLMs on diverse\ngeometric optics scenarios.\n‚Ä¢We establish a rigorous benchmark for optical generation\nassessment , providing a gold standard for evaluating fi-\ndelity of MLLMs‚Äô generations concerning geometric optics\nprinciples.\n‚Ä¢We define a systematic benchmark for the optical under-\nstanding capabilities of MLLMs, using a Scaled Proximity\nScore against human ground truth to quantify alignment\nwith expert judgment. The overview of our work are sum-\nmarized in Figure 1.\n2 Constructing the GOBench\nWhile current Multi-modal Large Language Model (MLLM) bench-\nmarks often assess high-level semantic understanding, the evalu-\nation of MLLMs on fine-grained physical principles, particularly\nfundamental geometric optics, remains underexplored. Humans\nreadily understand how light interacts with the world‚Äîpropagating\ndirectly, reflecting off surfaces, and refracting through media. How-\never, instilling this nuanced understanding of geometric optics into\nMLLMs for both accurate image generation and robust understand-\ning presents significant challenges.\nTo objectively assess current MLLM capabilities in this domain\nand identify their limitations, we introduce GOBench (Geometric\nOptics generation and understanding Benchmark). The construc-\ntion of GOBench involves three core components:\n(1) GOBench-Gen-1k Construction: We develope GOBench-Gen-\n1k, a novel dataset of 1k images. This dataset consists of various\n--- Page 3 ---\nGOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs Conference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland\nDirect light Scenario\nInput Prompt :\nGeneration Flaw :\nOne tree has two shadows.\nA tall weather observation tower \nstands atop a mountain peak, \ncasting a shadow onto the rocky \nground under high noon sun.\nGeneration Flaw :\nOnly  shadows without figures \nappear in the picture.\nA tall weather observation tower \nstands atop a bare mountain \npeak, casting a shadow onto \nground under high noon sun.\nGeneration Flaw :\nThere won't be long shadows \nunder high noon sun.Input Prompt :A lonely oak tree stands in a \nbright sunny  under a blue sky , \ncasting  a long, sharp shadow \nacross the green grass .\nInput Prompt :\nInput Prompt :\nGeneration Flaw :\nThe ray of light is discontinuous.A red laser pointer at a shallow \nangle into a still water tank. \nThe beam is visible in the air and \nbends at the water surface as it \nenters the water .\nInput Prompt :\nGeneration Flaw :\nOnly central word is magnified.A handheld magnifying glass held \nover a piece of printed text, \nwith letters underneath appearing \nlarger .\nInput Prompt :\nGeneration Flaw :\nNoA straight pencil placed diagonally \ninto a clear glass of water. \nThe pencil appears visibly bent or \ndisplaced at the water surface due \nto light refraction.\nRefracted light Scenario\nInput Prompt :\nGeneration Flaw :\nThe liquid is completely unlike \nwater.The surface of a flowing river, \nshowing a distorted reflection of  \nriverbanks and bridge structure.\nInput Prompt :\nGeneration Flaw :\nMirror reflects  the behind  content.A convex mirror on a street corner, \nshowing a wide -angle, distorted \nreflection of the busy street .\nInput Prompt :\nGeneration Flaw :\nNoSeveral fallen autumn leaves \nfloating on a calm lake, their \nsurfaces showing reflections of \nthe blue sky, white clouds, and \nbright sunlight spots.\nReflect light Scenario\nFigure 3: Examples of GOBench-Gen-1k that show cases of designed scenarios, including direct light scenario, reflect light\nscenario and refracted light scenario. Each case includes the input prompt, and the output image generated by state-of-art\nMLLM. The red words represent the obvious flaws of the MLLM‚Äôs generations that violating optical or basic physical principles.\ngeometric optical scenarios in three foundational categories: Direct\nLight, Reflection, or Refracted Light. Each scenario is rendered by\nthree distinct state-of-the-art MLLMs (GPT-4o-Image, SeeDream\n3.0, and Imagen 3) from detailed textual prompts.\n(2) Optical Generation Assessment: To establish a human ground\ntruth for generation quality, six experts are invited to score each\nof the generated images. This subjective evaluation experiment\nassessed Optical Authenticity, Aesthetic Quality, and Instruction\nFidelity, providing a gold standard for how accurately the generative\nmodels depicted the intended optical effects.\n(3) Optical Understanding Evaluation: To evaluate LMMs‚Äô ca-\npacity to interpret depicted optical phenomena, we benchmark\n11 prominent MLLMs. These models are tasked to evaluate the\nGOBench-Gen-1k across the same three dimensions (Optical Au-\nthenticity, Aesthetic Quality, Instruction Fidelity), with their per-\nformance measured against the human ground truth.\nThe subsequent subsections will detail the methodologies em-\nployed in the construction of the GOBench-Gen-1k and the design\nof our systematic evaluation framework for both optical generation\nand understanding.\n2.1 GOBench-Gen-1k Construction\nThe construction of the GOBench dataset, termed as GOBench-\nGen-1k, involves a two-stage process: comprehensive scenario de-\nsign followed by systematic image generation. Initially, we curate\n340 unique scenarios, which target three major categories of geo-\nmetric optics phenomena: Direct Light ,Reflection , and Refraction .\nThe distribution of the designed scenarios across the three main\noptical categories is presented in Figure 2. Each scenario comprises\na detailed textual prompt and a specific set of \"Optical Authenticity\"questions tailored to the phenomenon, intended to guide both opti-\ncal generation and understanding evaluation. The characteristics\nof the scenarios within each category is detailed as follows.\nDirect Light. Following the law of rectilinear propagation, di-\nrect light scenarios demonstrate light propagation from a source\nand the consequent formation of shadows. Accurately depicting\ndirect light and shadows demands models to go beyond simple\nillumination, incorporating an implicit understanding of how light\ninteracts with occluders to create geometrically sound and contex-\ntually appropriate visual effects. We define three representative\nsubcategories: 1) Geometric Shadow Projection , emphasizing the ac-\ncurate shape, length, and orientation of shadows based on light\nsource and occluder form. 2) Patterned Shadow Formation , exam-\nining shadows that create intricate texture or patterns due to the\noccluder‚Äôs structure. 3) Surface-Interactive Shadowing , investigating\nhow shadow appearance is affected by the receiving surface‚Äôs phys-\nical properties or atmospheric conditions.Together, these scenarios\nprovide a comprehensive testbed for evaluating a model‚Äôs capability\nto intelligent and render direct illumination, shadow casting and\nthe nuanced visual interplay between light, occluders, and diverse\nenvironmental surfaces.\nReflect Light. Obeying the law of reflection, reflect light cases\nevaluate a model‚Äôs proficiency in generation how light bounces\noff various surfaces, leading to visual effects ranging from clear\nmirror-like images to diffuse sheens. Understanding and accurately\ndepicting reflection is essential for conveying material properties,\nsurface characteristics, and the richness of environmental context\nwithin generated imagery. The scenarios are constructed to probe\na model‚Äôs grasp of reflection by encompassing three key aspects: 1)\nPlanar Specular Reflection , which focuses on the generation of clear,\nlargely undistorted mirror images from flat, smooth surfaces such\n--- Page 4 ---\nConference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Xiaorong Zhu et al.\nas calm water or clean glass, emphasizing high-fidelity mirroring. 2)\nCurved Specular Reflection , which examines the generation of pre-\ndictable image distortions, magnification or minification effects. 3)\nDiffuse Reflection , addressing the scattering of light from surfaces\nthat are inherently rough, textured, or wet, resulting in a loss of\nsharp specular highlights and the appearance of generalized blurred\nreflections rather than distinct images.This category require mod-\nels to exhibit implicit knowledge of accurate specular mirroring,\ncomplex distortions from curved surfaces, and the subtle interplay\nof light with varied material textures and conditions.\nRefracted Light. Complying with the Snell‚Äôs Law, refracted\nlight cases assess a model‚Äôs proficiency in generation the deflec-\ntion of light as it traverses interfaces between different transparent\nmedia. This optical phenomenon is fundamental to a multitude of\nvisual effects, and its accurate depiction by MLLMs requires an im-\nplicit understanding of how variations in refractive indices modify\nlight paths, leading to observable outcomes such as image displace-\nment, deformation, or chromatic dispersion. Our scenario construc-\ntion for this category systematically probes these capabilities via\nthree principal aspects: 1) Refractive Ray Deviation , concentrating\non the precise generation of light ray paths. These paths must ex-\nhibit accurate angular changes at media interfaces, in accordance\nwith optical laws. 2) Refractive Image Deformation , which concerns\nthe faithful representation of alterations to the perceived visual\nform of objects or backgrounds. Such alterations include bending,\ndisplacement, or magnification when viewed through transparent\nrefractive elements or due to atmospheric optical effects. 3) Refrac-\ntive Optical Patterning , which explores the model‚Äôs ability to simu-\nlate phenomena where refraction organizes light into structured\nvisual patterns or chromatic displays, encompassing effects like\ncaustics, spectral dispersion, and birefringence.\nThese subcategories, spanning diverse refractive effects, provide\na detailed assessment of an MLLM‚Äôs capacity for simulating complex\nlight-media interactions and its understanding of resultant visual\nphenomena.\nFollowing the above designed scenarios, we generate images\nwith three state-of-the-art Large multi-modality models: GPT-4o-\nImage, SeeDream3.0, and Imagen 3. Each model generates one\nimage per scenario; we filter out some cases that exhibit extremely\npoor generation quality and those that show excessive content simi-\nlarity, ultimately constructing GOBench-Gen-1k with total 1k cases.\nAnd examples of GOBench-Gen-1k are demonstrated in Figure 3. It\ncan be seen that the MLLMs‚Äô generations have decent quality but\nthere are still obvious flaws that violate the optical or basic physical\nprinciples.\n2.2 Benchmark on Optical Generation\nTo establish a robust ground truth for the optical generation\nquality of the MLLMs, we conduct a human expert evaluation exper-\niment. A panel of six human experts is invited to meticulously score\nthe designed 1k cases generated by by GPT-4o-Image, SeeDream\n3.0, and Imagen 3. The evaluation is performed across three key\ndimensions for each case: 1). Optical Authenticity :This dimension\ncritically assesses the physical plausibility and accurate depiction\nof the target geometric optical phenomenon (Direct Light, Reflec-\ntion, or Refraction) within the generated image. For each of the\n1.Is there an explicit or implied  light source , and is its position reasonable?( No)\n2.Is the shadow cast in a direction consistent with the sun's position? ( Yes)\n3.Does the shadow's length appear appropriate for the implied sun angle (e.g., \nlong for low sun, short for high sun )?‚Äú(Cannot determine )\nSpecific Questions ( Correct Answer )\n4.Does the shadow clearly outline the tree's trunk and branches?( Yes)\n5.Is the sunlit grass depicted with natural brightness and color?( Yes)Direct light(315)Image\nGeneral Questions( Correct Answer )\nSpecific Questions ( Correct Answer )Reflect  light(3 57)ImageGeneral Questions( Correct Answer )\n1.Is there an explicit or implied light source, and is its position reasonable?( No)\n2.Is the shadow cast in a direction consistent with the sun's position? ( Yes)\n3.Does the shadow's length appear appropriate for the implied sun angle (e.g., \nlong for low sun, short for high sun )?‚Äú(Cannot determine )\nSpecific Questions ( Correct Answer )\n4.Does the shadow clearly outline the tree's trunk and branches?( Yes)\n5.Is the sunlit grass depicted with natural brightness and color?( Yes)Refracted light(327)Image\n General Questions( Correct Answer )5. Is the position of any intense highlight from the sun accurately depicted on \nthe facade, consistent with the sun's angle relative to the building?\n1. Does the reflection  on the glass facade accurately capture  the vibrant \norange, pink, and purple hues of the sunset sky? (Yes)\n4. Does the intensity of the reflection  on the glass surface vary plausibly  \nwith viewing angle and the angle of the setting sun? (No)1.Is there an explicit or implied  light source , and its position reasonable?( No)\n2.Is the shadow ‚Äôs direction  consistent with the sun's position? ( Yes )\n3.Does the shadow's length  appear appropriate for the implied sun \nangle?‚Äú(Cannot determine )\nSpecific Questions( Correct Answer )\n4.Does the shadow clearly outline  the tree's trunk and branches?( Yes )Direct lightGenerated Image\n General Questions( Correct Answer )\nSpecific Questions( Correct Answer )Reflect  lightGeneral Questions( Correct Answer )\n3. Are details  such as individual tree leaves or cloud textures  discernible \nin the reflection? (Yes )1. Does the reflection  on surface accurately mirror  the forms and \narrangement of the sky, trees, and mountains? (Yes )\n2. Does the  lighting and color  in the reflection  appear consistent with the \nactual scene? (Yes )\nGenerated Image\n1. Does the pencil clearly bend/displace  at the air -water interface? (Yes )\n2.Is the submerged pencil's offset conform to the Snell‚Äôs  law ?(Yes )\nSpecific Questions( Correct Answer )\n3. Does the bend suggest light changing direction out of water?( Yes )\n4. Is the pencil's bent appearance  physically believable ?(Yes )Refracted lightGeneral Questions( Correct Answer )\n Generated ImageFigure 4: Optical Authenticity questions of GOBench. The\nquestions based on GOBench-Gen-1k that evaluates the Opti-\ncal Authenticity of MLLM‚Äôs generations, and each case con-\ntains general and specific questions.\nscenarios, experts are guided by a detailed, phenomenon-specific\nrubric consisting of five \"Optical Authenticity\" questions. These\nquestions are carefully designed to cover both general principles\napplicable to each main optical category and nuances specific to\nindividual scenarios.\nGenerally, two to three questions for each case probe category-\nspecific principles. For instance, Direct Light scenarios consider\nquestions about the plausible existence of light sources, the con-\nsistency of shadow direction with the light source, and the appro-\npriateness of shadow length relative to the implied light source\nangle. Reflection scenarios assess if reflections accurately mirror\nthe form and arrangement of original objects and if lighting, color\nand shape are consistent and reasonable between the scene and its\nreflection. Refraction scenarios typically query the obvious bending\nor displacement of objects and light paths at media interfaces and\nthe plausibility of the refractive angles. The remaining two to three\nquestions for each case delve into scenario-specific nuances to fur-\nther test the authenticity of the optical generation. These questions\nare designed to assess the illumination characteristics, shadow out-\nline, and contextual details as they manifest under the specific opti-\ncal phenomenon. Illustrative examples of these category-specific\nand scenario-specific questions are provided in Figure 4.\nEach of these five guiding questions for a given scenario is an-\nswerable with \"Yes\" (contributing 1 point), \"No\" (0 points), or \"Can-\nnot be determined\" (0.5 points). The \"Yes\" or \"No\" responses reflect\na judgment on whether the depicted optical phenomenon is rea-\nsonably portrayed based on the available visual information within\nthe image. For instance, a \"No\" answer should be given to the first\nquestion for the direct light case demonstrated in Figure 4, which\nqueries light source plausibility. The reason is that the image shows\na tree shadow cast to the left while the sky exhibits uniform bright-\nness without any visual cue to suggest a correspondingly positioned\nlight source. The correct scenario should demonstrate a brighter\nregion on the right. The \"Cannot be determined\" option (0.5 points)\n--- Page 5 ---\nGOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs Conference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland\nQ: Does light passing through the glass visibly affect the \nclarity  or appearance of the external reflections?  GT: Yes\nGemini -2.5Pro  : Yes         GPT4 .1:Cannot determine                \nClaude3 -sonnet: Yes          GPT4o :No          Grok-2-vision: No        Q:  Does the shadow clearly outline the tree's trunk and \nbranches?  GT: No\nGemini -2.5Pro: No           Gemini -2.5Pro :No               \nClaude3 -sonnet: Yes          GPT4o :Yes         Grok-2-vision:  Yes        \nQ: Is the shadow cast in a direction that implies a single, \ndistinct light source (the sun) ? GT: No\nGemini -2.5Pro    : No        GPT4o : No           GPT4 .1:Yes     \nClaude3 -sonnet: Yes         Claude3 -Opu: Cannot determine                         \nQ: Does the shadow on the ground roughly match the bird's \nsilhouette?  GT: No\nGemini -2.5Pro: No          GPT4 .1:No            GPT4o :No   \nClaude3 -sonnet: Yes         Claude3 -Opu: Cannot determine\nQ: Does the laser beam show a slight, visible bend as it passes \nthrough each successive transparent plastic sheet ? GT: No\nGemini -2.5Pro    : No       GPT4 .1: No          Claude3 -Opus:Yes     \nClaude3 -sonnet: Yes        Grok-2-vision: No                          \nQ: Is the overall path of the laser beam clearly shifted from \nits original trajectory due to multiple refractions?  GT: No\nGemini -2.5Pro: No         GPT4 .1:No               \nClaude3 -sonnet: Yes         GPT4o :Yes         Qwen2 -7b: Yes        \nQ:  Is there an explicit or implied light source, and is its \nposition reasonable ? GT: Yes\nGemini -2.5Pro   : Yes       GPT4o : Yes        Claude3 -Opus:Yes     \nClaude3 -sonnet: Yes        Grok-2-vision: Yes                      \nQ: Does the shadow's size and placement accurately reflect \nthe pillar's form under a high sun ? GT: No\nGemini -2.5Pro:  Yes        GPT-4.1:No               \nClaude3 -sonnet: No          GPT-4o:Yes       Grok-2-vision: Yes        \nQ:  Are two distinct letter images visible through the calcite ?\n GT: No\nGemini -2.5Pro    : No        GPT4o : Yes       Claude3 -Opus:No     \nClaude3 -sonnet: Yes         Gemini -2.5Flash: Yes                      \nQ: Is the image displacement direction plausible for calcite?  \nGT: Yes\nGemini -2.5Pro:  No          Claude3 -Opus:Cannot determine                \nClaude3 -sonnet: Yes         GPT-4o:Yes       Grok-2-vision: No        \n1 2\n3 4\n5 6Q: Is the shadow cast in a direction consistent with the sun's \nposition?  GT: No\nGemini -2.5Pro    : Yes        GPT4o : Yes        Claude3 -Opus:Yes     \nClaude3 -sonnet: Yes          Grok-2-vision: Yes                          Q: Does the surface of the glass cup clearly reflect  nearby \nenvironmental objects ? GT: No\nGemini -2.5Pro   : No          GPT4o : No         Claude3 -Opus:Yes     \nClaude3 -sonnet: Yes         Grok-2-vision: No                          \nFigure 5: Examples of several different models‚Äô answers to the optical authentic questions.GT represents ground truth result;\nthe Green words represent correct answers and the red words represent the wrong answers.\nis specifically assigned when the optical phenomenon itself appears\nplausible, but the image lacks sufficient contextual information to\ndefinitively affirm or negate the specific query. For instance, this\nwould apply if a question assesses shadow direction consistency,\nbut the image depicts the shadow without clearly showing the light\nsource or the occluding object, resulting in the difficulty to make\na conclusive judgment on the question. Finally, the sum of these\npoints results in an Optical Authenticity score ranging from 0 to 5\nfor each image. 2). Aesthetic Quality : Experts rate the overall visual\nappeal, composition, and artistic merit of each image on a 1-5 scale\n(1 being lowest, 5 being highest), based on the question: \"Please\nrate the aesthetics of the image on a scale of 1-5.\" 3). Instruction\nFidelity : The adherence of the generated image to the original tex-\ntual prompt, particularly in its success at showcasing the intended\noptical phenomenon, is also rated on a 1-5 Likert scale (1 being\nlowest, 5 being highest), guided by the question: \"Based on the\nimage and the provided prompt, please rate the instruction fidelity\nof the generated image on a scale of 1-5.\"\nFinally, the averaged expert scores for these three dimensions\nare applied to evaluate the MLLMs‚Äô ability of optical generation,\nwhich also serve as the definitive ground truth for the following\nunderstanding tasks.\n2.3 Benchmark on Optical Understanding\nIn the second primary task of GOBench, we assess the capacity\nof multi-modality large language models (MLLMs) to understand\nthe underlying optical phenomena depicted in the GOBench-Gen-1k .\nOur methodology for this assessment involves two key stages: 1)\ntasking the MLLMs to provide evaluative judgments across Optical\nAuthenticity, Aesthetic Quality, and Instruction Fidelity for each\nimage based on structured prompts, and 2) subsequently defining\nand applying clear quantitative metrics to measure the performance\nof these MLLM evaluations against human ground truth.The evaluative framework leverages the 1k image instances from\nGOBench-Gen-1k . For each image, MLLMs are provided with the\nimage, its original English textual generation prompt, and five\nscenario-specific \"Optical Authenticity\" questions. A system prompt\ndirects MLLMs to act as expert image critics, performing three sub-\ntasks: (1) answering each Optical Authenticity question (\"Yes\", \"No\",\nor \"Cannot be determined\"), from which an authenticity score (0-5\nscale) is computed; (2) assigning an Aesthetic Quality rating (1-5\nscale, Poor to Excellent); and (3) providing an Instruction Fidelity\nrating (1-5 scale) reflecting alignment with the generation prompt,\nespecially concerning intended optical effects.\nWe assesse 11 MLLMs via automated Python scripts with stan-\ndardized parameters. (e.g, temperature 0.1). Specifically, the differ-\nent answers of MLLMs to the Authenticity is shown in Figure 5.\nThe MLLMs can answer correctly in simple questions such as the\nrecognition of light source as shown in the fifth case. However, for\ninterfering questions, only outstanding models like Gemini-2.5-Pro\nand GPT-4o, can provide correct answer as shown in the fourth\ncase.\nWe also compare the MLLM‚Äôs score ( ùëÜùëÄùêøùêøùëÄ ) against the corre-\nsponding human ground truth score ( ùëÜùê∫ùëá). We define a maximum\npermissible absolute difference, ùõøùëöùëéùë• (set to 0.5), for assigning par-\ntial credit. Scores are linearly scaled within this difference, yielding\ntheScaled Proximity Score (ùëÜùëÉùëÜ) for each case:\nùëÜùëÉùëÜ case=(\n1‚àí|ùëÜùëÄùêøùêøùëÄ‚àíùëÜùê∫ùëá|\nùõøùëöùëéùë•if|ùëÜùëÄùêøùêøùëÄ‚àíùëÜùê∫ùëá|‚â§ùõøùëöùëéùë•\n0 if|ùëÜùëÄùêøùêøùëÄ‚àíùëÜùê∫ùëá|>ùõøùëöùëéùë•(1)\nThe final reported ùëÜùëÉùëÜfor each dimension (Optical Authentic-\nity, Aesthetic Quality, and Instruction Fidelity) is the average of\nthese ùëÜùëÉùëÜ casevalues. This metric offers a fine-grained performance\nmeasurement of MLLM alignment with ground truth.\n--- Page 6 ---\nConference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Xiaorong Zhu et al.\nTable 1: Pairwise Correlation Coefficients (PLCC and SRCC)\nbetween Rating Dimensions\nComparison Pair PLCC SRCC\nAesthetics vs Authenticity 0.2213 0.2085\nAuthenticity vs Fidelity 0.2942 0.2716\nAesthetics vs Fidelity 0.7036 0.6982\nTable 2: Average Evaluation Scores of MLLMs‚Äô generations\nLMMs Aesthetics Fidelity Authenticity\nGPT-4o-Image 3.95 3.97 4.09\nSeedream3.0 4.02 3.94 4.02\nImagen 3 3.70 3.67 3.48\n3 Experiments\nTo comprehensively evaluate MLLM capabilities in the domain of\ngeometric optics, GOBench facilitates several distinct experimental\nanalyses. First, we examine the characteristics of our human expert\nevaluation framework itself by assessing the inter-dimensional cor-\nrelation of the defined rating criteria. Second, we quantify the opti-\ncal generation proficiency of the state-of-the-art generative models.\nFinally, our investigation benchmarks the optical understanding\nand evaluative performance of a diverse cohort of 11 MLLMs. The\nsubsequent subsections detail the setup and present the findings\nfrom each of these analytical components.\n3.1 Validity of Evaluation Dimension\nTo validate the rationality of our designed evaluation dimensions,\nwe first analyze the pairwise correlation between the human expert\nratings for Optical Authenticity, Aesthetic Quality, and Instruction\nFidelity across the GOBench-Gen-1k cases. Table 1 presents the\nPearson Linear Correlation Coefficient (PLCC) and Spearman Rank\nCorrelation Coefficient (SRCC) for these comparisons.\nThe results reveal a low correlation between Aesthetic Quality\nand Optical Authenticity (PLCC = 0.2213, SRCC = 0.2085), and sim-\nilarly between Instruction Fidelity and Optical Authenticity (PLCC\n= 0.2942, SRCC = 0.2716). This indicates that human evaluators\nassess the physical correctness of optical phenomena largely inde-\npendently of an image‚Äôs general visual appeal. And the correlation\nbetween Aesthetic Quality and Instruction Fidelity (PLCC = 0.7036,\nSRCC = 0.6982) still shows that the results in each dimension are\nlargely independent. The generally low inter-correlations reveal\nthat each dimension effectively captures distinct aspects of image\nquality and correctness concerning geometric optics.\n3.2 Results on Optical Generation\nTo evaluate the optical generation proficiency of the three gen-\nerative MLLMs, we compute average human expert scores across\nthree dimensions, as presented in Table 2. Among the generative\nmodels, GPT-4o-Image achieved the highest average score for Op-\ntical Authenticity (referred to as Authenticity in the table) with\n4.09 out of 5, suggesting a relatively strong capability in depicting\nthe core geometric optics phenomena as intended by the scenarios.\nSeedream 3.0 excelled in Aesthetic Quality, scoring an average of\n4.02, and also demonstrated high Instruction Fidelity at 3.94, nearly\nmatching GPT-4o‚Äôs 3.97 in this regard. Imagen 3 has the smoothestTable 3: MLLMs‚Äô Optical Understanding Performance. Scaled\nProximity Scores (SPS) of 11 MLLMs against human ground\ntruth for Optical Authenticity, Aesthetic Quality, and Instruc-\ntion Fidelity on GOBench-Gen-1k . Best score of each dimen-\nsion is emphasized with boldface, second best is underlined.\nCategories GOBench-Understanding\nMLLMs Authenticity‚Üë Aesthetics‚Üë Fidelity‚Üë\nGemini-2.5Pro ( 0506) 37.35% 18.78% 8.31%\nClaude3-Opus 33.93% 31.23% 28.29%\nGPT4o ( latest ) 32.63% 8.17% 6.04%\nGPT4_1 ( 20250414 ) 32.23% 5.74% 5.37%\nGPT4_1_mini ( 20250414 ) 31.93% 9.24% 6.64%\nClaude3-Sonnet 31.53% 15.28% 7.97%\nGrok-2-Vision [24] 28.53% 15.48% 7.61%\nGemini-2.5Flash ( 0417) 28.43% 10.84% 6.74%\nGPT4_1_nano ( 20250414 ) 28.33% 25.76% 8.48%\nGlm4v( 9b) [17] 27.73% 32.03% 27.36%\nQwen2_5vl( 7b) [40] 27.43% 30.16% 24.36%\ngeneration, while generally scored lower, particularly in Optical Au-\nthenticity (3.48). While these state-of-the-art models demonstrate\nthe ability to generate images that are often optically plausible and\naesthetically acceptable, there are still gaps to achieve highly optical\nfidelity and instruction consistency. These findings underscore the\nlimitations in current generative models concerning the nuanced\nand physically accurate generation of geometric optics.\n3.3 Results on Optical Understanding\nThe GOBench also evaluates the optical understanding and eval-\nuative capabilities of 11 MLLMs, by comparing their assessment\nresults of the GOBench-Gen-1k against human ground truth. Ta-\nble 3 presents the performance using our primary metric, the Scaled\nProximity Score (SPS) (Equation 1), with models sorted by their\nSPS in the Optical Authenticity dimension. The results indicate\nthat even advanced MLLMs find optical understanding challenging.\nGemini-2.5Pro (0506) leads in Optical Authenticity SPS with 37.35%,\nfollowed by Claude3-Opus (33.93%) and GPT-4o (latest) (32.63%).\nThese top scores, considerably below 50%, highlight the substantial\ndifficulty MLLMs in correctly judge the optical phenomena in the\ngiven images. Tops scores for aesthetic quality(32.03%) and instruc-\ntion consistency(28.29%) further reveal the limitation of MLLMs‚Äô\nability to understand the geometric optical phenomena.\n4 Conclusion\nIn this work, we introduce GOBench, a novel benchmark de-\nsigned to rigorously evaluate Multi-modal Large language Mod-\nels‚Äô(MLLMs) abilities in geometric optical generation and under-\nstanding. GOBench systematically assesses performance across\nthree core optical categories:Direct Light, Reflection, and Refrac-\ntion. We apply a multi-dimensional framework to assess the models‚Äô\nability of optical generation and understanding including Optical\nAuthenticity, Aesthetic Quality, and Instruction Fidelity. The experi-\nment results reveal that state-of-the-art models still face significant\nchallenges in achieving consistent, physically accurate optical gen-\neration and robust optical understanding ability. These findings\nunderscore critical areas for future research in enhancing the phys-\nical world understanding of MLLMs.\n--- Page 7 ---\nGOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs Conference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland\nREFERENCES\n[1]Danial Abshari, Chenglong Fu, and Meera Sridhar. 2024. LLM-assisted Physical\nInvariant Extraction for Cyber-Physical Systems Anomaly Detection. arXiv\npreprint arXiv:2411.10918 (2024).\n[2]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In\nProceedings of the IEEE international conference on computer vision . 2425‚Äì2433.\n[3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, et al .2023. Qwen technical report. arXiv preprint\narXiv:2309.16609 (2023).\n[4]Jonathan T Barron and Jitendra Malik. 2012. Shape, albedo, and illumination\nfrom a single image of an unknown object. In 2012 IEEE Conference on Computer\nVision and Pattern Recognition . IEEE, 334‚Äì341.\n[5]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long\nOuyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al .2023. Improving im-\nage generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf 2, 3 (2023), 8.\n[6]Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al .2024. Deepseek llm: Scaling\nopen-source language models with longtermism. arXiv preprint arXiv:2401.02954\n(2024).\n[7]Hong Chen, Xin Wang, Yuwei Zhou, Bin Huang, Yipeng Zhang, Wei Feng, Houlun\nChen, Zeyang Zhang, Siao Tang, and Wenwu Zhu. 2024. Multi-modal generative\nai: Multi-modal llm, diffusion and beyond. arXiv preprint arXiv:2409.14993 (2024).\n[8]Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui,\nWenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi\nDong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu,\nBin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng\nWen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin,\nYu Qiao, Jifeng Dai, and Wenhai Wang. 2024. How Far Are We to GPT-4V?\nClosing the Gap to Commercial Multimodal Models with Open-Source Suites.\narXiv:2404.16821 [cs.CV]\n[9]Anoop Cherian, Radu Corcodel, Siddarth Jain, and Diego Romeres. 2024. Llmphy:\nComplex physical reasoning using large language models and world models.\narXiv preprint arXiv:2411.08027 (2024).\n[10] Nicholas Crafts. 2021. Artificial intelligence as a general-purpose technology: an\nhistorical perspective. University of Sussex (2021).\n[11] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and\nconsequences. Minds and Machines 30 (2020), 681‚Äì694.\n[12] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan\nRoth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. 2024. Blink: Multimodal\nlarge language models can see but not perceive. In European Conference on\nComputer Vision . Springer, 148‚Äì166.\n[13] Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, and Peter\nWonka. 2023. Llm blueprint: Enabling text-to-image generation with complex\nand detailed prompts. arXiv preprint arXiv:2310.10640 (2023).\n[14] Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng\nChang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al .2024. Is your llm secretly\na world model of the internet? model-based planning for web agents. arXiv\npreprint arXiv:2411.06559 (2024).\n[15] ANDR√â GUIDETTI. 2019. Artificial intelligence as general purpose technology:\nan empirical and applied analysis of its perception. (2019).\n[16] Minjie Hong, Yan Xia, Zehan Wang, Jieming Zhu, Ye Wang, Sihang Cai, Xi-\naoda Yang, Quanyu Dai, Zhenhua Dong, Zhimeng Zhang, et al .2025. EAGER-\nLLM: Enhancing Large Language Models as Recommenders through Exogenous\nBehavior-Semantic Integration. In Proceedings of the ACM on Web Conference\n2025. 2754‚Äì2762.\n[17] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang,\nYean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al .2024. Cogvlm2: Visual lan-\nguage models for image and video understanding. arXiv preprint arXiv:2408.16500\n(2024).\n[18] Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, and Shenlong Wang. 2024.\nAutoVFX: Physically Realistic Video Editing from Natural Language Instructions.\narXiv preprint arXiv:2411.02394 (2024).\n[19] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan,\nAng Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation\nvia llm-based semantic embedding learning. In Companion Proceedings of the\nACM Web Conference 2024 . 103‚Äì111.\n[20] Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, and Andrew\nMarkham. 2021. Towards semantic segmentation of urban-scale 3D point clouds:\nA dataset, benchmarks and challenges. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition . 4977‚Äì4987.\n[21] Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu. 2024.\nBliva: A simple multimodal llm for better handling of text-rich visual questions.\nInProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 2256‚Äì2264.\n[22] Maeve Hutchinson, Radu Jianu, Aidan Slingsby, and Pranava Madhyastha. 2024.\nLLM-Assisted Visual Analytics: Opportunities and Challenges. arXiv preprintarXiv:2409.02691 (2024).\n[23] Michael P Keating. 1988. Geometric, physical, and visual optics . Elsevier Health\nSciences.\n[24] Smith Lee. 2025. Bridging the AI Adoption Gap: The Disparity Between Rapid\nTechnological Advancements and Corporate Adaptation. (2025).\n[25] Zhihao Li, Yao Du, Yang Liu, Yan Zhang, Yufang Liu, Mengdi Zhang, and Xunliang\nCai. 2024. Eagle: Elevating geometric reasoning through llm-empowered visual\ninstruction tuning. arXiv preprint arXiv:2408.11397 (2024).\n[26] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and\nKe Liu. 2024. A Survey of Multimodel Large Language Models. In Proceedings of\nthe 3rd International Conference on Computer, Artificial Intelligence and Control\nEngineering . 405‚Äì409.\n[27] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,\nand Muhan Zhang. 2023. One for all: Towards training one graph model for all\nclassification tasks. arXiv preprint arXiv:2310.00149 (2023).\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruc-\ntion tuning. arXiv preprint arXiv:2304.08485 (2023).\n[29] David S Loshin. 2015. The geometrical optics workbook . Elsevier Health Sciences.\n[30] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play composi-\ntional reasoning with large language models. Advances in Neural Information\nProcessing Systems 36 (2023), 43447‚Äì43478.\n[31] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou\nZhu. 2024. Mono-internvl: Pushing the boundaries of monolithic multimodal\nlarge language models with endogenous visual pre-training. arXiv preprint\narXiv:2410.08202 (2024).\n[32] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. 2024. Groma:\nLocalized visual tokenization for grounding multimodal large language models.\nInEuropean Conference on Computer Vision . Springer, 417‚Äì435.\n[33] AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm\nto date. Meta AI 2, 5 (2024), 6.\n[34] Muhammad Ahmed Mohsin, Ahsan Bilal, Sagnik Bhattacharya, and John M\nCioffi. 2025. Retrieval augmented generation with multi-modal llm framework\nfor wireless environments. arXiv preprint arXiv:2503.07670 (2025).\n[35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas\nM√ºller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion\nmodels for high-resolution image synthesis. arXiv preprint arXiv:2307.01952\n(2023).\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n10684‚Äì10695.\n[37] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al.2023. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 (2023).\n[38] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi,\nKalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias\nNie√üner, et al .2020. State of the art on neural rendering. In Computer Graphics\nForum , Vol. 39. Wiley Online Library, 701‚Äì727.\n[39] Shiekh Zia Uddin, Sachin Vaidya, Shrish Choudhary, Zhuo Chen, Raafat K Salib,\nLuke Huang, Dirk R Englund, and Marin Soljaƒçiƒá. 2025. AI-Driven Robotics for\nFree-Space Optics. arXiv preprint arXiv:2505.17985 (2025).\n[40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin\nChen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al .2024. Qwen2-vl: Enhancing\nvision-language model‚Äôs perception of the world at any resolution. arXiv preprint\narXiv:2409.12191 (2024).\n[41] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jin-\nsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al .2024. Emu3:\nNext-token prediction is all you need. arXiv preprint arXiv:2409.18869 (2024).\n[42] Zhenhua Wang, Guang Xu, and Ming Ren. 2024. LLM-Generated Natural Lan-\nguage Meets Scaling Laws: New Explorations and Data Augmentation Methods.\narXiv preprint arXiv:2407.00322 (2024).\n[43] Changrong Xiao, Sean Xin Xu, and Kunpeng Zhang. 2023. Multimodal data\naugmentation for image captioning using diffusion models. In Proceedings of the\n1st Workshop on Large Generative Models Meet Multimodal Applications . 23‚Äì33.\n[44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2025. Scaling in-the-wild\ntraining for diffusion-based illumination harmonization and editing by imposing\nconsistent light transport. In The Thirteenth International Conference on Learning\nRepresentations .\n[45] Long Zhang, Meng Zhang, Wei Lin Wang, and Yu Luo. 2025. Simulation as\nReality? The Effectiveness of LLM-Generated Data in Open-ended Question\nAssessment. arXiv preprint arXiv:2502.06371 (2025).\n[46] Haoyu Zhao, Wenhang Ge, and Ying-cong Chen. 2024. Llm-optic: Unveiling\nthe capabilities of large language models for universal visual grounding. arXiv\npreprint arXiv:2405.17104 (2024).\n[47] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao\nZhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. 2025. Envisioning\n--- Page 8 ---\nConference acronym ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Xiaorong Zhu et al.\nbeyond the pixels: Benchmarking reasoning-informed visual editing. arXiv\npreprint arXiv:2504.02826 (2025).\n[48] Longwei Zheng, Fei Jiang, Xiaoqing Gu, Yuanyuan Li, Gong Wang, and Haomin\nZhang. 2025. Teaching via LLM-enhanced simulations: Authenticity and barriersto suspension of disbelief. The Internet and Higher Education 65 (2025), 100990.",
  "text_length": 48809
}