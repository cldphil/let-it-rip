{
  "id": "http://arxiv.org/abs/2506.01025v1",
  "title": "Modality Translation and Registration of MR and Ultrasound Images Using\n  Diffusion Models",
  "summary": "Multimodal MR-US registration is critical for prostate cancer diagnosis.\nHowever, this task remains challenging due to significant modality\ndiscrepancies. Existing methods often fail to align critical boundaries while\nbeing overly sensitive to irrelevant details. To address this, we propose an\nanatomically coherent modality translation (ACMT) network based on a\nhierarchical feature disentanglement design. We leverage shallow-layer features\nfor texture consistency and deep-layer features for boundary preservation.\nUnlike conventional modality translation methods that convert one modality into\nanother, our ACMT introduces the customized design of an intermediate pseudo\nmodality. Both MR and US images are translated toward this intermediate domain,\neffectively addressing the bottlenecks faced by traditional translation methods\nin the downstream registration task. Experiments demonstrate that our method\nmitigates modality-specific discrepancies while preserving crucial anatomical\nboundaries for accurate registration. Quantitative evaluations show superior\nmodality similarity compared to state-of-the-art modality translation methods.\nFurthermore, downstream registration experiments confirm that our translated\nimages achieve the best alignment performance, highlighting the robustness of\nour framework for multi-modal prostate image registration.",
  "authors": [
    "Xudong Ma",
    "Nantheera Anantrasirichai",
    "Stefanos Bolomytis",
    "Alin Achim"
  ],
  "published": "2025-06-01T14:10:06Z",
  "updated": "2025-06-01T14:10:06Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01025v1",
  "full_text": "--- Page 1 ---\nModality Translation and Registration of MR and\nUltrasound Images Using Diffusion Models\nXudong Ma1, Nantheera Anantrasirichai1, Stefanos Bolomytis2, Alin Achim1\n1Visual Information Laboratory, University of Bristol, Bristol, the United Kingdom\n2Southmead Hospital, North Bristol NHS Trust, Bristol, the United Kingdom\n{xudong.ma, n.anantrasirichai, alin.achim }@bristol.ac.uk, stefanos.bolomytis@nbt.nhs.uk\nAbstract —Multimodal MR-US registration is critical for\nprostate cancer diagnosis. However, this task remains challenging\ndue to significant modality discrepancies. Existing methods often\nfail to align critical boundaries while being overly sensitive to\nirrelevant details. To address this, we propose an anatomically\ncoherent modality translation (ACMT) network based on a\nhierarchical feature disentanglement design. We leverage shallow-\nlayer features for texture consistency and deep-layer features for\nboundary preservation. Unlike conventional modality translation\nmethods that convert one modality into another, our ACMT\nintroduces the customized design of an intermediate pseudo\nmodality. Both MR and US images are translated toward this\nintermediate domain, effectively addressing the bottlenecks faced\nby traditional translation methods in the downstream registra-\ntion task. Experiments demonstrate that our method mitigates\nmodality-specific discrepancies while preserving crucial anatomi-\ncal boundaries for accurate registration. Quantitative evaluations\nshow superior modality similarity compared to state-of-the-art\nmodality translation methods. Furthermore, downstream regis-\ntration experiments confirm that our translated images achieve\nthe best alignment performance, highlighting the robustness of\nour framework for multi-modal prostate image registration.\nIndex Terms —Modality Translation, MR-US Registration,\nProstate Cancer, Diffusion Models, Multimodal Image Registra-\ntion, Unsupervised Learning\nI. I NTRODUCTION\nProstate cancer is a leading cause of death among men\nworldwide [1], with accurate diagnosis relying heavily on\nadvanced imaging techniques. Magnetic Resonance Imaging\n(MRI) and Ultrasound (US) are widely used in prostate cancer\nimaging, each offering unique advantages: MRI provides high\nsoft-tissue contrast for lesion detection, while US enables real-\ntime guidance for biopsies. However, their significant differ-\nences in anatomical representation pose a major challenge for\nmulti-modal registration, which is essential for combining their\ncomplementary strengths.\nTo address modality discrepancies, some methods employ\nimage segmentation, aligning segmented regions to avoid\ndirect cross-modal registration challenges [2], [3]. However,\nthese approaches require extensive annotated data, which are\nlabor-intensive and scarce, limiting their practical applica-\ntion. To overcome these limitations, we previously proposed\na Partial Modality Translation (PMT) approach [6], which\nestablished an intermediate modality to bridge MRI and US\ndomains. While PMT improved texture similarity, it does\nnot truly achieve customized modality translation and retainsexcessive details irrelevant to registration. This oversight left\nroom for improvement in the registration results.\nBuilding on this foundation, we propose a novel hierar-\nchical feature disentanglement method for anatomically co-\nherent modality translation (ACMT). Through a customized\nintermediate modality design, ACMT logically fulfills the\ngoal that PMT failed to achieve. The ACMT advances PMT\nby addressing two key limitations: (1) explicitly preserving\nboundary information while maintaining texture consistency,\nand (2) relaxing the requirement for photorealism so as to\nreduce the preservation of unnecessary and overly detailed\ntextures within the prostate. Specifically, we leverage the\ndistinct characteristics of shallow and deep features in con-\nvolutional networks. Shallow-layer features, which primarily\ncapture low-level texture information, are processed with\nlarger convolutional kernels. This design is motivated by\nthe fact that larger kernels can effectively model broader\ntexture patterns, which are essential for achieving cross-modal\nconsistency [14]. In contrast, deep-layer features, which are\nmore sensitive to high-level structural details such as shapes,\nare processed with smaller convolutional kernels. Smaller\nkernels are better suited for capturing fine-grained details and\nprecise anatomical structures, making them ideal for boundary\npreservation [8]. Additionally, we enhance boundary extraction\nusing Sobel filtering to further improve structural alignment\n[10]. In addition, by removing the adversarial loss used in\nPMT, we further reduce the emphasis on photorealism, as the\nintermediate modality does not require highly realistic details.\nThis design ensures that the output retains only the boundary\ninformation most relevant for registration, while promoting\ntexture consistency and suppressing modality-specific anatom-\nical detail discrepancies. As a result, the difficulty of cross-\nmodal registration is significantly reduced.\nIn the subsequent sections of this paper, we will provide\na comprehensive explanation of the mathematical foundations\nand network architecture of the proposed method in Section 2.\nSection 3 will present both objective and subjective analyses\nof the results, demonstrating the effectiveness of our approach.\nFinally, we will conclude with a summary of our contributions\nin the last section.\nII. P ROPOSED METHOD\nIn this section, we first introduce the theoretical foundation\nof our method, which is based on a Diffusion Schr ¨odingerarXiv:2506.01025v1  [cs.CV]  1 Jun 2025\n--- Page 2 ---\nBridge. Following this, we provide a detailed explanation of\nthe overall workflow of the proposed ACMT framework, as\nillustrated in Fig. 1.\nA. Schr ¨odinger Bridge based Diffusion Model\nWe design this network by leveraging a Schr ¨odinger Bridge\nbased (SB) diffusion framework [4]. It is inspired by optimal\ntransport (OT) theory [5], [12]. Formally, the SB problem\nseeks to find the optimal stochastic process that transforms\na source distribution P0to a target distribution P1through\nintermediate distributions Ptover time t, as defined by:\nPSB={argmin\nPtDKL(Pt∥Wσ)}with t ∼[0,1],(1)\nwhere Wσdenotes the Wiener measure with variance σ.\nThis formulation aims to minimize the Kullback-Leibler (KL)\ndivergence between the process distribution Ptand the refer-\nence measure Wσat each timestep. The collection of opti-\nmal distributions {PSB\nt}constitutes the Schr ¨odinger Bridge\nconnecting PSB\n0andPSB\n1. Although SB is theoretically a\ncontinuous process, Conditional Flow Matching (CFM) for-\nmulation allows us to address it in a discretized manner\n[4], [11]. The CFM formulation establishes that for any two\ndistributions PSB\ntmandPSB\ntnin the SB where [tm, tn]⊆[0,1],\nthe intermediate distribution at time t∈[tm, tn]follows a\nGaussian distribution:\np(Xt|Xtm, Xtn) =N\u0010\nXt\f\f\fwtXtn+ (1−wt)Xtm,\nwt(1−wt)σ(tn−tm)I\u0011\n,(2)\nwhere Xt∼PSB\nt,Xtm∼PSB\ntm,Xtn∼PSB\ntnandwt=\nt−tm\ntn−tm. Furthermore, the joint distribution PSB\ntmtnbetween any\ntwo timesteps can be obtained through an entropy-regularized\nOT problem:\nPSB\ntmtn= argmin\nPtm,tnE(Xtm,Xtn)[∥Xtm−Xtn∥2]\n−2σ(tn−tm)H(Xtm, Xtn),(3)\nwhere Hdenotes the entropy function. This formulation\nenables the determination of the optimal terminal distribution\nPSB\ntngiven any initial distribution PSB\ntm. Subsequently, using\nEquation (2), we can compute any intermediate state PSB\nt\nalong the Schr ¨odinger Bridge between the PSB\ntmandPSB\ntn.\nB. Diffusion Process via CFM formulation\nSince the training of our model relies on inputs generated\nvia a diffusion process, we first provide a comprehensive\nexplanation of the diffusion process that forms the foundation\nof our framework. This diffusion process applies of our\nmodality translation network. However, it is used in evaluation\nmode with all parameters frozen. To be more specific, we\nhypothesize the existence of an ideal intermediate modality\nthat focuses primarily on boundary information while disre-\ngarding texture details. Through careful network design and\nloss function formulation, our framework learns to transform\nboth MR and US images into this target modality. This processis achieved by constructing Schr ¨odinger Bridge from either\nMRPMR\n0 distribution or US distribution PUS\n0to a single\nintermediate modality P1.\nAs illustrated in the Diffusion Process (purple block) in\nFig. 1, let xtj∈Ptjrepresent an intermediate state along\nthe Schr ¨odinger Bridge at time tj∈[0,1]. Our network learns\nto map xtjto the terminal state x1∈P1. By using the CFM\nformulation as shown in Equation 2, the next state xtj+1could\nbe computed as:\nxtj+1=wtj+1x1+ (1−wtj+1)xtj+N(0, αj+1I),(4)\nwhere wtj+1=tj+1−tj\n1−tjis the interpolation weight balancing\nx1andxtj,αj+1=wtj+1(1−wtj+1)(1−tj)σcontrols the\nnoise magnitude, and N(0, αj+1I)adds Gaussian noise scaled\nbyαj+1.\nThis iterative process begins with xt0=x0, the source MR\nor US image, and progressively transforms it towards xti.\nC. Training Process\nThe training process begins with inputs generated by a\ndiffusion process. As shown in the Training Process (left\npart of Fig.1), during training, we implement the following\nprocedure for each MR-US image pair:\n1) Randomly select tifrom the predefined time step pool\n{t0, t1, t2, . . . , t T}, where each ti∈[0,1].\n2) Using the network in evaluation mode (with all param-\neters frozen): generate xtithrough the diffusion process\nas described in the purple block of Fig. 1\n3) Switch the network to training mode:\n•Compute the transformation from xtitox1\n•Calculate loss\n•Update network parameters through backpropaga-\ntion\nThrough this training strategy, the network learns to trans-\nform any intermediate state xtito the target modality x1,\neffectively capturing the mapping along the entire Schr ¨odinger\nBridge. By breaking down the transformation into iterative\nsteps, it ensures gradual refinement of the target modal-\nity while preserving critical anatomical information. This\napproach is particularly advantageous for medical imaging,\nwhere the complex relationship between source and target\nmodalities requires robust solutions.\nD. Loss Functions\nRecent studies have demonstrated the remarkable capability\nof diffusion models in extracting discriminative features from\nimages [7], [13]. Building upon this observation and the well-\nestablished understanding that shallow layers of neural net-\nworks are more sensitive to texture information while deeper\nlayers capture boundary details [8], we design a hierarchical\nfeature disentanglement loss to achieve both texture consis-\ntency and boundary preservation in our modality translation\nframework.\nAs shown in the Hierarchical Feature Disentanglement part\nof Fig. 1, let fs\nθandfd\nθdenote the shallow and deep feature\n--- Page 3 ---\nFig. 1: Hierarchical Feature Disentanglement framework based on a Diffusion Model\nextraction functions of our network, respectively. Given an\ninput image x0(either MR or US), the shallow and deep\nfeatures are extracted as:\nFs\n0=fs\nθ(x0),Fd\n0=fd\nθ(x0). (5)\nSimilarly, for the translated image x1in the intermediate\nmodality P1, the corresponding features are:\nFs\n1=fs\nθ(x1),Fd\n1=fd\nθ(x1). (6)\n1) Texture Consistency Loss: To ensure texture similarity\nbetween the translated MR and US images in the intermediate\ndomain, we further process the shallow features Fs,MR\n1 and\nFs,US\n1 using a large convolutional kernel ( 7×7) because larger\nkernels are known to be more effective at capturing general,\nglobal features such as texture patterns due to their wider\nreceptive fields. The texture consistency loss is formulated as:\nLtexture =\r\r\rC7×7(Fs,MR\n1 )− C 7×7(Fs,US\n1)\r\r\r2\n2, (7)\nwhere C7×7(·)represents the 7×7convolution operation, and\n∥ · ∥2\n2denotes the L2 norm.\n2) Boundary Preservation Loss: To preserve the anatomical\nboundaries of the original images, we process the deep features\nFd\n0andFd\n1using a smaller convolutional kernel ( 3×3)\nfollowed by a Sobel filter. This design choice is motivated\nby the fact that smaller kernels are particularly effective at\nextracting fine-grained features, such as edges and boundaries,\ndue to their ability to focus on localized regions and capturehigh-frequency details. The boundary preservation loss for MR\nand US images is defined as:\nLMR\nboundary =\r\r\rS(C3×3(Fd,MR\n1 ))− S(C3×3(Fd,MR\n0 ))\r\r\r\n1,(8)\nLUS\nboundary =\r\r\rS(C3×3(Fd,US\n1))− S(C3×3(Fd,US\n0))\r\r\r\n1,(9)\nwhere C3×3(·)denotes the 3×3convolution operation, S(·)\nrepresents the Sobel filter, and ∥·∥ 1is the L1 norm. The total\nboundary preservation loss is:\nLboundary =1\n2\u0000\nLMR\nboundary +LUS\nboundary\u0001\n. (10)\n3) SB Loss: To ensure that the translation process follows\nthe optimal transport path defined by the Schr ¨odinger Bridge,\nwe introduce the SB constraint loss based on the joint distri-\nbution PSB\nti,1following the theory in equation 3. For MR and\nUS images, the SB loss is defined as:\nLMR\nSB(θi, ti) =E(xMR\nti,xMR\n1)\u0002\n∥xMR\nti−xMR\n1∥2\u0003\n−2σ(1−ti)H(xMR\nti, xMR\n1),(11)\nLUS\nSB(θi, ti) =E(xUS\nti,xUS\n1)\u0002\n∥xUS\nti−xUS\n1∥2\u0003\n−2σ(1−ti)H(xUS\nti, xUS\n1).(12)\nwhere xMR\n1=fθi(xMR\n1|xMR\nti)andxUS\n1=fθi(xUS\n1|xUS\nti)are\nthe terminal states predicted by the network. The total SB loss\nis then:\nLSB=1\n2\u0000\nLMR\nSB+LUS\nSB\u0001\n. (13)\n--- Page 4 ---\nTABLE I: Quantitative evaluation of modality translation\nquality using FID and KID (lower is better).\nMethod FID ↓(decrease by ↑) KID ↓(decrease by ↑)\nOriginal 404.88 0.56\nUNSB 377.92 (6.66%) 0.52 (7.14%)\nPMT 170.02 (58.01%) 0.11 (80.36%)\nACMT(Ours) 138.01 (65.91%) 0.09 (83.93%)\nTABLE II: Quantitative evaluation of registration performance\nbased on different modality translation methods, all registered\nusing a consistent approach.\nMethod DSC ↑ IoU↑ ASD ↓\nOriginal 0.92 0.87 10.74\nUNSB 0.92 0.88 12.83\nPMT 0.95 0.91 9.18\nACMT (ours) 0.95 0.90 6.82\nThe overall loss function combines the texture consistency\nloss, boundary preservation loss, and the SB loss:\nLtotal=λtextureLtexture +λboundary Lboundary +λSBLSB (14)\nThe weighting coefficients λtexture ,λboundary , and λSBare\ncarefully tuned to balance the contributions of each loss\ncomponent, ensuring that the network simultaneously achieves\ntexture consistency, boundary preservation, and optimal trans-\nport.\nIII. R ESULTS AND EVALUATION\n1) Dataset: 3D MR and US paired images were provided\nby Southmead Hospital Bristol. The MR images are T2-\nweighted volumes, while the 3D US images are derived from\nbiopsy procedures. To ensure consistency, all volumes were\npreprocessed to focus on the prostate region while retain-\ning relevant surrounding structures. Additionally, to enhance\ngeneralizability and robustness, we applied data augmenta-\ntion techniques such as flipping and rotation. To balance\ncomputational feasibility and anatomical fidelity, all volumes\nwere standardized to a resolution of 128 ×128×64 pixels. The\ndataset was partitioned into an 80% training set and a 20%\ntest set, and all experiments employed cross-validation.\n2) Evaluation: We evaluated both the quality of modality\ntranslation and the corresponding registration performance us-\ning the same registration model, FSDiffReg [9]. We compared:\n(1) registration on original MR-US images, and (2) registration\non modality-translated results from state-of-the-art (SOTA)\napproaches, including UNSB [4], an ICLR 2024 modality\ntranslation method, and PMT [6], our previously proposed\nmethod that focuses on texture similarity.\nTo assess the quality of modality translation, we employed\ntwo widely-used metrics: FID and KID. For registration evalu-\nation, our experts manually segmented the prostate on several\nkey frames for each test case to create ground-truth masks.\nThe generated deformation fields, based on different modality\ntranslation methods, were applied to warp these masks, and\nthe results were scored with:•Dice Similarity Coefficient (DSC) : Defined as2|X∩Y|\n|X|+|Y|,\nthis metric quantifies the volume overlap between the\nwarped mask Xand the ground-truth mask Y.\n•Intersection-over-Union (IoU) : Given by|X∩Y|\n|X∪Y|, IoU\nprovides a stricter assessment of boundary alignment by\nmeasuring the ratio of the shared to the combined volume.\n•Average Surface Distance (ASD) : This metric reports\nthe mean Euclidean distance between the surfaces of the\nwarped and ground-truth segmentations.\nThe experimental results demonstrate that our method\nachieves the best FID and KID scores, as shown in Table I.\nSpecifically, our approach reduces the FID by 65.91% and\nthe KID by 83.93%, outperforming UNSB approximately 10\ntimes and 12 times, respectively. Compared to our previously\nproposed PMT method, our current method shows further\nimprovements in both metrics, highlighting its superior perfor-\nmance in modality translation. On the other hand, as shown\nin Table II, our ACMT framework attains the best overall\nregistration results, leading all methods in DSC and ASD.\nWhile its IoU is only 0.01 below that of PMT, ACMT lowers\nASD by 25.7 % compared with the runner-up, demonstrating\nsuperior surface alignment. These findings confirm that ACMT\nproduces more anatomically faithful intermediate representa-\ntions and yields substantially higher registration accuracy than\neither UNSB or PMT-based variants.\nVisually, we randomly selected two patients to illustrate the\nmodality translation and image registration comparison. As\nshown in Fig. 2, Our method maximizes the similarity within\nthe prostate interior while preserving boundary information\nthat is most relevant for registration. At the same time, it sup-\npresses unnecessary internal texture details that could mislead\nthe registration process. This advantage is further validated\nin the registration results shown in Fig. 3. In particular, in\nthe second row, within the red boxes, our method is the only\none that achieves relatively smooth registration, while all other\nmethods exhibit noticeable discontinuities. Similarly, in the\nfirst patient case (top row), although all modality translation\napproaches help improve the smoothness in the red-boxed\nregion, the yellow boxes highlight that our method introduces\nthe least over-deformation. This suggests that our estimated\ndeformation field is the most realistic and anatomically plau-\nsible among all methods.\nIV. C ONCLUSION\nIn this work, we propose a novel ACMT method for modal-\nity translation between MR and US images using a hierarchical\nfeature disentanglement idea. We leverage shallow features\nto ensure texture consistency and deep features to preserve\nanatomical boundaries, resulting in anatomically coherent\npseudo-representations. Our unsupervised framework achieves\ncustomized modality translation, effectively removing irrele-\nvant information from source images that would otherwise\nhinder cross-modal registration. Experimental results show that\nour framework consistently outperforms SOAT methods in\nboth modality translation and registration, achieving superior\n--- Page 5 ---\nUS MR UNSB-MR PMT-US PMT-MR ACMT-US ACMT-MR\nFig. 2: Modality translation results for two patients (two rows), showing original US and MR images, UNSB translation, and\nintermediate translation from PMT and ACMT.\nUS\n MR\n original+FSDiffReg\n UNSB+FSDiffReg\n PMT+FSDiffReg\n ACMT+FSDiffReg\nFig. 3: Registration results for two patients. Each row corresponds to one patient and displays the original US and MR images,\nfollowed by registration results using FSDiffReg applied to the original MR-US pair, as well as to images translated by UNSB,\nPMT, and our ACMT methods. Results are shown in a chessboard layout for visual comparison.\nperformance in both quantitative metrics and visual assess-\nments.\nREFERENCES\n[1] Bratt, O., et al. : Population-based organised prostate cancer testing:\nresults from the first invitation of 50-year-old men. European Urology\n85(3), 207–214 (2024)\n[2] Chen, Y ., Xing, L., Yu, L., Liu, W., Pooya Fahimian, B., Niedermayr,\nT., Bagshaw, H.P., Buyyounouski, M., Han, B.: Mr to ultrasound\nimage registration with segmentation-based learning for hdr prostate\nbrachytherapy. Medical physics 48(6), 3074–3083 (2021)\n[3] Jiang, J., Guo, Y ., Bi, Z., Huang, Z., Yu, G., Wang, J.: Segmentation of\nprostate ultrasound images: the state of the art and the future directions\nof segmentation algorithms. Artificial Intelligence Review 56(1), 615–\n651 (2023)\n[4] Kim, B., Kwon, G., Kim, K., Ye, J.C.: Unpaired image-to-image\ntranslation via neural schr ¨odinger bridge. In: ICLR (2024)\n[5] L ´eonard, C.: A survey of the schrodinger problem and some of its\nconnections with optimal transport. Dynamical Systems 34(4), 1533–\n1574 (2014)\n[6] Ma, X., Anantrasirichai, N., Bolomytis, S., Achim, A.: Pmt: Partial-\nmodality translation based on diffusion models for prostate magnetic\nresonance and ultrasound image registration. In: Annual Conference\non Medical Image Understanding and Analysis. pp. 285–297. Springer\n(2024)[7] Preechakul, K., Chatthee, N., Wizadwongsa, S., Suwajanakorn, S.: Dif-\nfusion autoencoders: Toward a meaningful and decodable representation.\nIn: Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. pp. 10619–10629 (2022)\n[8] Purwono, P., Ma’arif, A., Rahmaniar, W., Fathurrahman, H.I.K., Frisky,\nA.Z.K., ul Haq, Q.M.: Understanding of convolutional neural network\n(cnn): A review. International Journal of Robotics and Control Systems\n2(4), 739–748 (2022)\n[9] Qin, Y ., Li, X.: Fsdiffreg: Feature-wise and score-wise diffusion-guided\nunsupervised deformable image registration for cardiac images. In:\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention. pp. 655–665. Springer (2023)\n[10] Ranjan, R., Avasthi, V .: Edge detection using guided sobel image\nfiltering. Wireless Personal Communications 132(1), 651–677 (2023)\n[11] Tong, A., et al. : Conditional flow matching: Simulation-free dynamic\noptimal transport. arXiv preprint arXiv:2302.00482 2(3) (2023)\n[12] Wang, G., et al. : Deep generative learning via schr ¨odinger bridge. In:\nInternational conference on machine learning. pp. 10794–10804. PMLR\n(2021)\n[13] Yang, X., Wang, X.: Diffusion model as representation learner. In:\nProceedings of the IEEE/CVF International Conference on Computer\nVision. pp. 18938–18949 (2023)\n[14] Youwang, K., Oh, T.H., Pons-Moll, G.: Paint-it: Text-to-texture synthesis\nvia deep convolutional texture map optimization and physically-based\nrendering. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 4347–4356 (2024)",
  "text_length": 22484
}