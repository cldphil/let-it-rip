{
  "id": "http://arxiv.org/abs/2506.01032v1",
  "title": "ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and\n  Speaker Feature Optimization",
  "summary": "In recent years, diffusion-based generative models have demonstrated\nremarkable performance in speech conversion, including Denoising Diffusion\nProbabilistic Models (DDPM) and others. However, the advantages of these models\ncome at the cost of requiring a large number of sampling steps. This limitation\nhinders their practical application in real-world scenarios. In this paper, we\nintroduce ReFlow-VC, a novel high-fidelity speech conversion method based on\nrectified flow. Specifically, ReFlow-VC is an Ordinary Differential Equation\n(ODE) model that transforms a Gaussian distribution to the true Mel-spectrogram\ndistribution along the most direct path. Furthermore, we propose a modeling\napproach that optimizes speaker features by utilizing both content and pitch\ninformation, allowing speaker features to reflect the properties of the current\nspeech more accurately. Experimental results show that ReFlow-VC performs\nexceptionally well in small datasets and zero-shot scenarios.",
  "authors": [
    "Pengyu Ren",
    "Wenhao Guan",
    "Kaidi Wang",
    "Peijie Chen",
    "Qingyang Hong",
    "Lin Li"
  ],
  "published": "2025-06-01T14:21:07Z",
  "updated": "2025-06-01T14:21:07Z",
  "categories": [
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01032v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01032v1  [cs.SD]  1 Jun 2025ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and Speaker\nFeature Optimization\nPengyu Ren1, Wenhao Guan2, Kaidi Wang1, Peijie Chen1, Qingyang Hong∗1, Lin Li∗2\n1School of Informatics, Xiamen University, China\n2School of Electronic Science and Engineering, Xiamen University, China\nrenpengyu@stu.xmu.edu.cn\nAbstract\nIn recent years, diffusion-based generative models have demon-\nstrated remarkable performance in speech conversion, includ-\ning Denoising Diffusion Probabilistic Models (DDPM) and oth-\ners. However, the advantages of these models come at the cost\nof requiring a large number of sampling steps. This limita-\ntion hinders their practical application in real-world scenarios.\nIn this paper, we introduce ReFlow-VC, a novel high-fidelity\nspeech conversion method based on rectified flow. Specifi-\ncally, ReFlow-VC is an Ordinary Differential Equation (ODE)\nmodel that transforms a Gaussian distribution to the true Mel-\nspectrogram distribution along the most direct path. Further-\nmore, we propose a modeling approach that optimizes speaker\nfeatures by utilizing both content and pitch information, al-\nlowing speaker features to reflect the properties of the cur-\nrent speech more accurately. Experimental results show that\nReFlow-VC performs exceptionally well in small datasets and\nzero-shot scenarios.\nIndex Terms : zero-shot voice conversion, rectified flow , fea-\nture fusion\n1. Introduction\nZero-shot voice conversion (VC) aims to convert speech from\nany source speaker to the speech of any target speaker without\nchanging the linguistic content. VC achieves this by decom-\nposing the source speech into different components, including\nthe speaker’s timbre, linguistic content, and speaking style. Its\napplications span across various practical fields such as speech\nanonymization and audiobook production [1]. The core chal-\nlenge of zero-shot VC lies in effectively modeling, disentan-\ngling, and utilizing various attributes of speech, including con-\ntent and timbre.\nTraditional zero-shot methods typically combine the lin-\nguistic content and speaking style of the source speaker with the\ntimbre of the target speaker to generate the converted speech. In\nthe groundbreaking Auto-VC model [2], speaker embeddings\nfrom a pre-trained speaker verification network are used as con-\nditional inputs. Other models have been improved upon this by\nenriching the conditional inputs with additional speech features,\nsuch as pitch and loudness, or by jointly training voice conver-\nsion and speaker embedding networks. Additionally, several\nstudies have utilized attention mechanisms to better integrate\nthe features of reference speech into the source speech, thus\nenhancing the performance of the decoder [3]. However, due\nto the complexity of speech signals [4] and the limitations in\nmodeling timbre and content [5], these methods still leave sig-\nnificant room for performance improvement.\n∗Corresponding authorMost traditional voice conversion models are based on\nautoencoder architectures, such as Auto-VC, V AE-VC [6],\nand CycleGAN-VC [7], or generative adversarial networks\n(GANs) [8], like CycleGAN-VC [7], CycleGAN-VC2 [9], and\nStarGAN-VC [10]. However, relatively few voice conversion\nmodels are based on flow models.\nNotable voice conversion models, such as Free-VC [11],\nAuto-VC, and Diff-VC [12], have achieved significant success.\nHowever, there remains room for exploration in terms of model\narchitecture and methodology. Among them, diffusion mod-\nels, including denoising diffusion probabilistic models (DDPM)\nand score-based generative models, have gained widespread at-\ntention due to their potential to generate high-quality samples.\nA major drawback of diffusion models, however, is that gen-\nerating satisfactory samples requires multiple iterations [13].\nTo address this, several voice conversion methods have been\nproposed. Diff-VC gradually converts noise into Mel spectro-\ngrams by constructing a stochastic differential equation (SDE)\nand solving the reverse SDE with a numerical ordinary differ-\nential equation (ODE) solver. While it generates high-quality\naudio, the large number of iterations in the reverse process af-\nfects inference speed. DDDM-VC addresses the issue of mul-\ntiple iterations required for generating high-quality samples by\nintroducing a decoupled denoising diffusion model and a prior\nmixing strategy [14]. In this paper, we present ReFlow-VC, a\nvoice conversion model based on a modified flow model. Our\nproposed method achieves outstanding voice conversion results,\nand the contributions of this paper are as follows:\n• We propose ReFlow-VC, the first voice conversion acous-\ntic model based on a Rectified Flow Model. Specifically,\nthe ReFlow-VC model is an ordinary differential equation\n(ODE) model that transforms a Gaussian distribution into the\ntrue mel-spectrogram distribution via a direct path as much\nas possible, and is trained using a simple unconstrained least\nsquares optimization procedure [13].\n• We propose a modeling approach that optimizes speaker fea-\ntures using both content and pitch. Through cross-attention\nand gated fusion, it effectively integrates multiple input fea-\ntures (such as speaker characteristics, content information,\npitch, etc.), thereby enhancing the model’s expressive ca-\npability. This enables fine-grained control over the target\nspeaker’s attributes, making the voice conversion task more\nprecise.\n2. Rectified Flow Model\n2.1. Rectified Flow\nThe rectified flow model is an Ordinary Differential Equation\n(ODE) model designed to transform a distribution π0(standard\n--- Page 2 ---\nFigure 1: The diagram of the Rectified Flow model, with the meanings of the icons at the top.(a) Linear interpolation of data samples\n(X0,X1).(b) The rectified flow Ztinduced by ( X0,X1).(c) The linear interpolation of data samples ( Z0,Z1) of rectified flow Zt.(d)\nThe rectified flow induced from ( Z0,Z1), propagating along straight paths.\nGaussian) to π1(the ground truth distribution) via straight-line\npaths. Given samples X0∼π0andX1∼π1, the rectified flow\ncorresponds to an ODE defined as [15]:\ndZt=v(Zt, t)dt, (1)\nwhere Z0is from π0, and the transformation follows the dis-\ntribution π1. Here, υrepresents the drift force of the ODE,\ndesigned to align the flow with the direction ( X1-X0) between\nthe two distributions. The flow is learned by minimizing a least\nsquares regression problem:\nmin\nυZ1\n0∥(X1−X0)−v(Xt, t)∥2dt, (2)\nwhere Xtis the linear interpolation between X0andX1, de-\nfined as:\nXt=tX1+ (1−t)X0. (3)\nWhile the naive evolution of Xtfollows a non-causal path\ndXt= (X1−X0)dt, the rectified flow causalizes the path\nby adjusting υbased on (X1−X0), ensuring that the trajec-\ntories do not cross at any point, preserving the uniqueness of\nthe solution. The rectified flow thus avoids non-causal inter-\nsections, as shown in Figure 1, ensuring a well-defined, non-\ncrossing path.During training, the objective is to learn the drift\nforce υby minimizing:\nˆθ= arg min\nθE\u0002\n∥(X1−X0)−v(Xt, t)∥2\u0003\n, (4)\nwhere t ∼Uniform([0,1]). After training, the learned model\nis used to transform X0toX1by solving the ODE dZt=\nˆv(Zt, t)dt. This procedure can be recursively applied, forming\na sequence of transformations Z′=ReFlow (Z0, Z1), leading\nto improved transport efficiency and more linear flow trajec-\ntories. This recursive process helps reduce time-discretization\nerrors and is computationally advantageous when simulating\nflows.\n2.2. Rectified Flow Model for VC\nReFlow-VC converts the noise distribution to a Mel spectro-\ngram distribution conditioned on time t and the speaker condi-\ntion features c after feature fusion. We define π0as the stan-\ndard Gaussian distribution and π1as the ground truth Mel-\nspectrogram data distribution, with X0∼π0andX1∼π1.\nThe training objective of ReFlow-VC is as follows:\nLθ=E\u0002\n∥(X1−X0)−vθ(Xt, t, c)∥2\u0003\n, (5)where t∈Uniform ([0,1])andXt=tX1+ (1−t)X0.\nReFlow-VC does not require any auxiliary losses, except for\nthe L2 loss function between the output of the model vθand\n(X1−X0). During inference, we directly solve the ODE start-\ning from Z0∼π0conditioned on the speaker feature c and\nbased on the model vθ. For high-fidelity generation, we can use\nthe RK45 ODE solver. For one-step generation, we can directly\nuse the Euler ODE solver for competitive performance [13].\nFurthermore, the recursive rectified flow procedure can also be\napplied to VC, constructing a second ReFlow-VC, referred to as\n2-ReFlow-VC. The 2-ReFlow-VC is simply retraining the rec-\ntified flow model using the samples generated by ReFlow-VC.\n3. ReFlow-VC Model Architecture\n3.1. Encoder\nThe encoder consists of an average voice encoder, Hubert-Soft\n[16], a speaker encoder, VQ-V AE, and a feature fusion mod-\nule. We chose the average phoneme-level MEL features as the\nspeaker-independent speech representation, similar to Diff-VC\n[12].By using the average voice encoder, the source audio is\ntransformed into the average speaker’s mel, which is referred to\nas Average mel in Figure 2. Additionally, we used Hubert-Soft\nfrom Soft-VC to extract continuous content features. By model-\ning uncertainty, Hubert-Soft captures more content information,\nwhich improves the clarity and naturalness of the converted\nspeech. Following the work of Polyak et al [17], we used the\nYAPPT algorithm to extract pitch (F0) from the audio, encoding\nspeaker-independent pitch information. The F0 of each sample\nis normalized for each speaker to obtain speaker-independent\npitch information, and the VQ-V AE is used to extract vector\nquantized pitch representations [17]. For a fair comparison, dur-\ning inference, we normalize the F0 for each sentence instead of\nfor each speaker.\nEssentially, zero-shot voice conversion is a challenging\ntask, as it requires the model to generalize effectively to any\nunseen speaker without additional training or fine-tuning. This\nplaces high demands on the model’s ability to capture timbre\n[18]. To enhance the overall timbre modeling capability of\nthe proposed method, we introduce a feature fusion module.\nThrough multiple attention mechanisms, gated fusion, and it-\nerative self-attention strategies, the speaker features can be pre-\ncisely adjusted and optimized across various speech attributes\n[19]. Specifically, the model can dynamically adjust speaker\nfeatures, flexibly modify them using content and pitch informa-\ntion, and enhance the expressiveness of speaker characteristics.\nBy leveraging cross-attention and gating mechanisms, it cap-\ntures and generates personalized speaker traits with greater ac-\ncuracy. Additionally, the model improves the accuracy and nat-\n--- Page 3 ---\nAverage Voice\nEncoder\nRectified Flow Decoder\ntSource voice\nTarget voice\nODE Solver\nHubert-Soft\nSpeaker\nEncoder\nVQVAEspeaker\nembedding\nMean Pooling\nCross-\nattentionCross-\nattentionGateTransformer\n encoderx4Average_melMultihead-\nAttentionAverage_mel\nFusion Encoder❄\n❄\n❄\n❄\n❄  Frozen in trainingFigure 2: An illustration of ReFlow-VC.\nuralness of voice conversion, ensuring clear and natural speech\nacross different speakers through the use of multiple attention\nand self-attention mechanisms. These strategies significantly\nimprove the expressiveness of the speaker features, resulting in\nmore precise and natural voice conversion outcomes.\nSpecifically, the fusion encoder consists of multiple mod-\nules designed to enhance performance and expressive power.\nThe pitch conv projection layer transforms the input pitch fea-\ntures from a dimension of 1 to 256. Then, through two layers\nof cross-attention, each layer receives an input of 256 dimen-\nsions and outputs 256 dimensions to facilitate mutual attention\nbetween different features. Next, the gated fusion module pro-\ncesses the 256-dimensional input to enhance the information fu-\nsion effect. The self-attention mechanism iteratively refines the\n256-dimensional input’s self-attention performance, gradually\nfocusing on the key parts of the input. Additionally, the model\nemploys a multihead-attention mechanism with both input and\noutput dimensions of 256 and 8 attention heads (with a dimen-\nsion of 32 per head), further enhancing the model’s ability to\nlearn and focus on various features. These modules work to-\ngether to improve the model’s ability to process speech features\nand speaker conditioning information.\n3.2. Decoder\nThe decoder architecture is based on U-Net [20] and is the\nsame as in GradTTS [21] but with four times more channels\nto better capture the full range of human voices. The speaker\nconditioning network gt(Y)consists of 2D convolutions and\nMLPs(multilayer perceptron). Its output is a 128-dimensional\nvector, which is broadcast-concatenated with the concatenation\nofˆXtand¯Xas additional 128 channels.\n4. Experiments And Results\n4.1. Experimental Setup\n4.1.1. Dataset\nWe trained the proposed ReFlow-VC model using a subset of\nthe LibriTTS dataset, which was randomly sampled from the\nfull LibriTTS dataset to facilitate model training on a small\ndataset [22]. This subset consists of 26,580 speech samples with\na total duration of 38.13 hours. The dataset was randomly splitinto a training set (26,000 samples), a validation set (80 sam-\nples), and a test set (500 samples). We extracted 80-dimensional\nmel spectrograms with a frame size of 1024 and a hop size of\n256. ReFlow-VC was trained for 200K iterations on a single\nNVIDIA 2080Ti GPU using the Adam optimizer [23]. We used\nthe pre-trained HiFi-GAN [24] as the neural vocoder, which is\nresponsible for converting the mel spectrograms into raw wave-\nforms.\n4.1.2. Evaluation Metrics\nWe conducted a comprehensive evaluation, including both ob-\njective and subjective metrics, to assess the sample quality\n(NMOS, SMOS, WER, CER, and SECS) as well as the model’s\ninference speed. We calculated the Character Error Rate (CER)\nand Word Error Rate (WER) using Whisper [25], a publicly\navailable Automatic Speech Recognition (ASR) model that is\nlarge-scale, multilingual, and multitask-supervised, for content\nconsistency measurement. We evaluated the speech naturalness\nand speaker similarity through Mean Opinion Scores (MOS).\nFor speech naturalness (NMOS), at least 20 listeners rated each\nsource and converted speech sample on a scale of 1 to 5. For\nspeaker similarity (SMOS), at least 20 listeners rated the target\nand converted speech samples on a scale of 1 to 4. Addition-\nally, we performed an extra similarity measurement using the\nspeaker encoder’s cosine similarity (SECS).\n4.1.3. Comparative Models\nWe compared the performance of the samples generated by\nReFlow-VC with the following systems using the six metrics\nmentioned above: 1) Diff-VC1; 2) Auto-VC2; 3) Free-VC3.\nNote that the Diff-VC model was re-trained by us on the same\nsmall dataset as ReFlow-VC, and to achieve better performance,\nthe time step T of Diff-VC was set to 1000. Additionally,\nwe conducted an ablation experiment, naming the version of\nReFlow-VC without the feature fusion module as NReFlow-VC\nfor comparative testing.\n1https://github.com/huawei-noah/Speech-\nBackbones/tree/main/DiffVC\n2https://github.com/auspicious3000/autovc\n3https://github.com/OlaWod/FreeVC\n--- Page 4 ---\nMethod iter. NMOS (↑) SMOS (↑) CER (↓) WER (↓) SECS (↑)\nGT - 3.89±0.05 3.47±0.05 0.52 1.79 -\nGT (Mel + Vocoder) - 3.86±0.05 3.34±0.05 0.59 2.16 0.987\nAutoVC - 3.59±0.05 2.41±0.04 5.47 8.69 0.746\nFree-VC - 3.76±0.05 2.78±0.05 1.69 4.77 0.745\nDiff-VC 1 2.13±0.05 1.78±0.04 7.49 13.05 0.617\nDiff-VC 30 3.72±0.05 2.69±0.05 7.58 13.80 0.766\nDiff-VC 1000 3.75±0.05 2.78±0.05 7.64 13.92 0.781\nNReFlow-VC 1 3.69±0.05 2.67±0.05 4.35 7.60 0.751\nNReFlow-VC 30 3.74±0.05 2.70±0.05 4.45 7.71 0.759\nNReFlow-VC(RK45 solver) 146 3.75±0.05 2.70±0.05 4.75 7.79 0.761\nReFlow-VC 1 3.72±0.05 2.74±0.05 1.83 4.54 0.830\nReFlow-VC 30 3.74±0.05 2.76±0.05 1.96 4.59 0.845\nReFlow-VC(RK45 solver) 512 3.78±0.05 2.81±0.05 2.12 4.84 0.843\nTable 1: Evaluation results for VC.\nMethod iter. SECS (↑)\nReFlow-VC 1 0.830\nReFlow-VC 30 0.845\nReFlow-VC(RK45 solver) 512 0.843\n2-ReFlow-VC 1 0.832\n2-ReFlow-VC 30 0.841\n2-ReFlow-VC(RK45 solver) 396 0.842\nTable 2: Evaluation results for ReFlow-VC and 2-ReFlow-VC.\n4.2. Results and Analysis\nThe evaluation results of VC are shown in Table 1. In terms\nof audio quality, our proposed ReFlow-VC model, using the\nRK45 ODE solver for inference, achieves the highest SMOS,\nNMOS, and best SECS scores among all methods. This demon-\nstrates the superior performance of our ReFlow-VC in model-\ning data distributions. We observe that compared to Diff-VC,\nthe one-step sampling performance of both ReFlow-VC and\nNReFlow-VC is quite impressive, especially the one-step sam-\npling performance of ReFlow-VC, which is on par with Diff-\nVC’s 30-step sampling performance. Additionally, when com-\nparing ReFlow-VC and NReFlow-VC, we find that our pro-\nposed cross-attention and gated fusion module, which models\nspeaker characteristics using content and pitch features, plays\na crucial role in improving the speaker naturalness and similar-\nity of the converted speech, yielding significant improvements.\nUnder equal sampling, ReFlow-VC’s SECS score clearly out-\nperforms both NReFlow-VC and Diff-VC. However, we also\nnote that there is still room for improvement in CER and WER\ndata.\nIn addition, we used a 9.2 second female audio as the\nsource speaker’s audio and a 4.7 second male audio as the tar-\nget speaker’s audio to test the sampling duration. This duration\nonly accounts for the time spent during the sampling phase, ex-\ncluding the time taken for extracting pitch and content features.\nAs shown in Table 3, with the same number of sampling steps,\nRectified Flow achieves faster sampling speed. However, due to\nthe additional feature fusion module in ReFlow-VC compared\nto NReFlow-VC, the sampling time is longer. Additionally,\nReFlow-VC requires extra time for extracting pitch and content\nfeatures. In the above example, extracting content features with\nHubert-Soft took 0.0263 seconds, and extracting pitch features\ntook 1.0468 seconds. However, considering the overall speech\nconversion results, we believe the additional time consumptionMethod iter. Time(s)\nDiff-VC 1 0.1188\nDiff-VC 30 4.6773\nDiff-VC 1000 167.4375\nNReFlow-VC 1 0.0893\nNReFlow-VC 30 4.4197\nNReFlow-VC(RK45 solver) 140 22.2049\nReFlow-VC 1 0.0998\nReFlow-VC 30 4.452\nReFlow-VC(RK45 solver) 554 88.79923\nTable 3: The sampling duration of Diff-VC, NReFlow-VC, and\nReFlow-VC.\nis worthwhile.\nWe also conducted experiments with 2-ReFlow-VC, as\nshown in Table 2. The results indicate that although 2-ReFlow-\nVC performs similarly to ReFlow-VC, it performs slightly bet-\nter when using the Euler ODE solver (1-step). However, when\nusing the Euler ODE solver (30-steps) and the RK45 solver,\nReFlow-VC performs slightly better than 2-ReFlow-VC. De-\nspite these differences, they are not significant. This further\nproves that the recursive rectified flow model is more intuitive\nand easier to implement numerically. By exploring the pro-\ncesses of ReFlow-VC and 2-ReFlow-VC, we highlight the ro-\nbustness of our proposed conditional rectified flow model.\n5. Conclusions\nIn this paper, we propose a simple yet efficient ReFlow-VC,\nwhich effectively completes the voice conversion task by lever-\naging rectified flow and feature fusion techniques. ReFlow-VC\ncan use the RK45 ODE solver for sampling, generating speech\nsamples with optimal audio quality. Furthermore, due to the\nexcellent one-step sampling capability and outstanding perfor-\nmance of our proposed ReFlow-VC, it significantly enhances its\nusability in real-world scenarios. The audio samples are pub-\nlicly available at: https://copyabcs.github.io/reflowvc-demo/.\n6. Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grants 62276220 and\n62371407 and the Innovation of Policing Science and Technol-\nogy, Fujian province (Grant number: 2024Y0068)\n--- Page 5 ---\n7. References\n[1] B. M. L. Srivastava, M. Maouche, M. Sahidullah, E. Vincent,\nA. Bellet, M. Tommasi, N. Tomashenko, X. Wang, and J. Yam-\nagishi, “Privacy and utility of x-vector based speaker anonymiza-\ntion,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing , vol. 30, pp. 2383–2395, 2022.\n[2] K. Qian, Y . Zhang, S. Chang, X. Yang, and M. Hasegawa-\nJohnson, “Autovc: Zero-shot voice style transfer with only au-\ntoencoder loss,” in International Conference on Machine Learn-\ning. PMLR, 2019, pp. 5210–5219.\n[3] Y .-H. Chen, D.-Y . Wu, T.-H. Wu, and H.-y. Lee, “Again-vc: A\none-shot voice conversion using activation guidance and adap-\ntive instance normalization,” in ICASSP 2021-2021 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2021, pp. 5954–5958.\n[4] Y . Pan, Y . Hu, Y . Yang, W. Fei, J. Yao, H. Lu, L. Ma, and J. Zhao,\n“Gemo-clap: Gender-attribute-enhanced contrastive language-\naudio pretraining for accurate speech emotion recognition,” in\nICASSP 2024-2024 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp.\n10 021–10 025.\n[5] S. Hussain, P. Neekhara, J. Huang, J. Li, and B. Ginsburg, “Ace-\nvc: Adaptive and controllable voice conversion using explicitly\ndisentangled self-supervised speech representations,” in ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2023, pp. 1–5.\n[6] P. L. Tobing, Y .-C. Wu, T. Hayashi, K. Kobayashi, and T. Toda,\n“Non-parallel voice conversion with cyclic variational autoen-\ncoder,” arXiv preprint arXiv:1907.10185 , 2019.\n[7] T. Kaneko and H. Kameoka, “Cyclegan-vc: Non-parallel\nvoice conversion using cycle-consistent adversarial networks,” in\n2018 26th European Signal Processing Conference (EUSIPCO) .\nIEEE, 2018, pp. 2100–2104.\n[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y . Bengio, “Generative adver-\nsarial networks,” Communications of the ACM , vol. 63, no. 11, pp.\n139–144, 2020.\n[9] T. Kaneko, H. Kameoka, K. Tanaka, and N. Hojo, “Cyclegan-\nvc2: Improved cyclegan-based non-parallel voice conversion,” in\nICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2019, pp. 6820–\n6824.\n[10] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “Stargan-vc:\nNon-parallel many-to-many voice conversion using star genera-\ntive adversarial networks,” in 2018 IEEE Spoken Language Tech-\nnology Workshop (SLT) . IEEE, 2018, pp. 266–273.\n[11] J. Li, W. Tu, and L. Xiao, “Freevc: Towards high-quality text-free\none-shot voice conversion,” in ICASSP 2023-2023 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2023, pp. 1–5.\n[12] V . Popov, I. V ovk, V . Gogoryan, T. Sadekova, M. Kudinov, and\nJ. Wei, “Diffusion-based voice conversion with fast maximum\nlikelihood sampling scheme,” arXiv preprint arXiv:2109.13821 ,\n2021.\n[13] W. Guan, Q. Su, H. Zhou, S. Miao, X. Xie, L. Li, and Q. Hong,\n“Reflow-tts: A rectified flow model for high-fidelity text-to-\nspeech,” in ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024,\npp. 10 501–10 505.\n[14] H.-Y . Choi, S.-H. Lee, and S.-W. Lee, “Dddm-vc: Decoupled\ndenoising diffusion models with disentangled representation and\nprior mixup for verified robust voice conversion,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 38, no. 16,\n2024, pp. 17 862–17 870.\n[15] X. Liu, C. Gong, and Q. Liu, “Flow straight and fast: Learning\nto generate and transfer data with rectified flow,” arXiv preprint\narXiv:2209.03003 , 2022.[16] B. Van Niekerk, M.-A. Carbonneau, J. Za ¨ıdi, M. Baas, H. Seut ´e,\nand H. Kamper, “A comparison of discrete and soft speech units\nfor improved voice conversion,” in ICASSP 2022-2022 IEEE In-\nternational Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) . IEEE, 2022, pp. 6562–6566.\n[17] A. Polyak, Y . Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N.\nHsu, A. Mohamed, and E. Dupoux, “Speech resynthesis from dis-\ncrete disentangled self-supervised representations,” arXiv preprint\narXiv:2104.00355 , 2021.\n[18] X. Zhu, L. He, Y . Xiao, X. Wang, X. Tan, S. Zhao, and L. Xie,\n“Zsvc: Zero-shot style voice conversion with disentangled la-\ntent diffusion models and adversarial training,” arXiv preprint\narXiv:2501.04416 , 2025.\n[19] Y . Pan, Y . Yang, J. Yao, J. Ye, H. Zhou, L. Ma, and J. Zhao,\n“Ctefm-vc: Zero-shot voice conversion based on content-aware\ntimbre ensemble modeling and flow matching,” arXiv preprint\narXiv:2411.02026 , 2024.\n[20] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in Medical image\ncomputing and computer-assisted intervention–MICCAI 2015:\n18th international conference, Munich, Germany, October 5-9,\n2015, proceedings, part III 18 . Springer, 2015, pp. 234–241.\n[21] V . Popov, I. V ovk, V . Gogoryan, T. Sadekova, and M. Kudinov,\n“Grad-tts: A diffusion probabilistic model for text-to-speech,” in\nInternational Conference on Machine Learning . PMLR, 2021,\npp. 8599–8608.\n[22] H. Zen, V . Dang, R. Clark, Y . Zhang, R. J. Weiss, Y . Jia, Z. Chen,\nand Y . Wu, “Libritts: A corpus derived from librispeech for text-\nto-speech,” arXiv preprint arXiv:1904.02882 , 2019.\n[23] D. P. Kingma, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[24] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial net-\nworks for efficient and high fidelity speech synthesis,” Advances\nin neural information processing systems , vol. 33, pp. 17 022–\n17 033, 2020.\n[25] N. Cao, Y .-R. Lin, X. Sun, D. Lazer, S. Liu, and H. Qu, “Whis-\nper: Tracing the spatiotemporal process of information diffusion\nin real time,” IEEE transactions on visualization and computer\ngraphics , vol. 18, no. 12, pp. 2649–2658, 2012.",
  "text_length": 25868
}