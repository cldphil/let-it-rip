{
  "id": "http://arxiv.org/abs/2506.04142v1",
  "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
  "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
  "authors": [
    "Kejian Zhu",
    "Shangqing Tu",
    "Zhuoran Jin",
    "Lei Hou",
    "Juanzi Li",
    "Jun Zhao"
  ],
  "published": "2025-06-04T16:33:44Z",
  "updated": "2025-06-04T16:33:44Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04142v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04142v1  [cs.CL]  4 Jun 2025Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis\nKejian Zhu1,2*, Shangqing Tu3*, Zhuoran Jin1,2Lei Hou3, Juanzi Li3†, Jun Zhao1,2†\n1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems,\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences3Tsinghua University\nzhukejian2025@ia.ac.cn, tsq22@mails.tsinghua.edu.cn\n{houlei, lijuanzi} @tsinghua.edu.cn ,jzhao@nlpr.ia.ac.cn\nAbstract\nThe development of large language models\n(LLMs) depends on trustworthy evaluation .\nHowever, most current evaluations rely on pub-\nlic benchmarks, which are prone to data con-\ntamination issues that significantly compromise\nfairness. Previous researches have focused\non constructing dynamic benchmarks to ad-\ndress contamination. However, continuously\nbuilding new benchmarks is costly and cyclical.\nIn this work, we aim to tackle contamination\nby analyzing the mechanisms of contaminated\nmodels themselves. Through our experiments,\nwe discover that the overestimation of contami-\nnated models is likely due to parameters acquir-\ning shortcut solutions in training. We further\npropose a novel method for identifying shortcut\nneurons through comparative and causal anal-\nysis. Building on this, we introduce an evalu-\nation method called shortcut neuron patch-\ningto suppress shortcut neurons. Experiments\nvalidate the effectiveness of our approach in\nmitigating contamination. Additionally, our\nevaluation results exhibit a strong linear cor-\nrelation with MixEval (Ni et al., 2024), a re-\ncently released trustworthy benchmark, achiev-\ning a Spearman coefficient ( ρ) exceeding 0.95.\nThis high correlation indicates that our method\nclosely reveals true capabilities of the models\nand is trustworthy. We conduct further experi-\nments to demonstrate the generalizability of our\nmethod across various benchmarks and hyper-\nparameter settings. Code :https://github.\ncom/GaryStack/Trustworthy-Evaluation .\n1 Introduction\nRecently large language models (LLMs) have ad-\nvanced rapidly, achieving remarkable results across\na wide range of complex tasks (Achiam et al., 2023;\nTouvron et al., 2023). Moreover, the open-sourcing\nof technology has spurred the development of nu-\nmerous new models (Zhao et al., 2023). In this\n*Equal Contribution.\n†Corresponding authors.\nIf James has a routine...?  Answer is0        Shortcut Path         Contaminated Region \n        Patching Path         Uncontaminated Region36\nGSM8K SampleFigure 1: An example illustrating the core principle of\nour approach: we prevent the model from relying on\nshortcuts in contaminated regions to generate answers.\nThis process restores the model’s true capabilities.\ncontext, evaluation has become increasingly criti-\ncal, which plays a pivotal role in shaping the future\ntrajectory of LLM development (Guo et al., 2023).\nWe believe that trustworthiness is currently the\ncritical aspect to enhance in evaluation, compared\nto evaluating the broader capabilities of LLMs\n(Chang et al., 2024; Zhou et al., 2023; Yu et al.,\n2023; Litschko et al., 2023). However, it is difficult\nto ensure that the large-scale and opaque training\ndata of LLMs do not involve benchmark samples,\nwhich is called data contamination . Contami-\nnation can significantly affect the fairness of the\nevaluation (Li et al., 2024b; Tu et al., 2024). Fur-\nthermore, we highlight several critical aspects that\ncurrent evaluation results overlook, compromising\ntheir trustworthiness: A1. Model behavior short-\ncut: End-to-end LLMs can lead to a lack of trans-\nparency in the intermediate reasoning process when\nsolving complex problems. This raises questions\nabout whether the model has completed a credible\nreasoning process or has taken shortcuts in reason-\ning, which can lead to distrust in the answers gener-\nated by LLMs in real-world complex scenarios. A2.\n--- Page 2 ---\nInput format shortcut: The current benchmark\nhas the drawback that the input format is fixed and\ndiffers from the way real-world inquiries are made.\nModels that are fine-tuned on the benchmark’s in-\nput format (even the training set) have an advantage\nat evaluation, leading to higher scores, which is not\nfair. Data contamination can lead to models being\nfitted to limited benchmarks, which is a key factor\nfor aspects A1, A2 (Magar and Schwartz, 2022).\nTo address this issue, recent researches focus\non developing dynamic benchmarks to mitigate\ncontamination (Yu et al., 2023; Jacovi et al., 2023;\nLi et al., 2024a; Zhu et al., 2023a). However, this\nstrategy is resource-intensive. Besides it does not\nfundamentally eliminate the risk of contamination\nin newly released models.\nTo alleviate the untrustworthiness caused by con-\ntamination, it is crucial to understand the impact\nthat contamination can have on models. We hy-\npothesize that the untrustworthiness occurs because\nthe model overfits the benchmark when contam-\ninated, acquiring shortcuts for input format and\nreasoning. We speculate that these shortcuts in\nthe benchmark are key to our inability to trust the\nmodel’s real-world capabilities. Through experi-\nments, we discover a sparse set of neurons closely\nassociated with the aforementioned shortcuts, and\nthese shortcut neurons can be leveraged to suppress\nthe shortcuts, as shown in 4.2. Similar findings\ncan be supported by previous studies (Golchin and\nSurdeanu, 2023; Li, 2023; Bordt et al., 2024).\nIn this paper, we propose a novel method to es-\ntablish trustworthy LLM evaluation via short-\ncut neuron analysis . Figure 1 illustrates the prin-\nciple of this approach. Recent studies have shown\nthat transformer neurons are often closely related to\nspecific abilities of LLMs (Geva et al., 2022; Wang\net al., 2022; Dai et al., 2021). Therefore, we ana-\nlyze shortcuts at the neuron level. Our method for\nidentifying shortcut neurons is based on two key in-\ndicators: (1) Comparative Analysis . This involves\ncomparing neuron activation differences between\ncontaminated and uncontaminated models when\nprocessing the same benchmark samples. Neurons\nwith significant activation differences are likely\nlinked to memory shortcuts. (2) Causal Analysis .\nWe compute the causal score by performing activa-\ntion patching (Meng et al., 2022; Vig et al., 2020b;\nZhang and Nanda, 2023) and analyzing its causal\neffects. A neuron is identified as a shortcut neuron\nif it satisfies two causal effects: (a) it restores the\ntrue scores of the contaminated model, and (b) itdoes not affect the normal ability of the model.\nIn Section 4.2, we find that the shortcut neurons\nlocated above are sparse and effective, with a total\nof about 5000. Then we use the shortcut neuron\npatching method to conduct trustworthy evaluation\nby inhibiting shortcuts. Specifically, we will use\nthe shortcut neuron activation of the base model\nwith same architecture to patch models under test,\nso as to establish trustworthy evaluation.\nTo verify the effectiveness of our evaluation\nmethod, we conduct experiments on both LLaMA\n(Touvron et al., 2023) and Mistral (Jiang et al.,\n2023) architectures. We fine-tune a series of con-\ntaminated and uncontaminated models. First, the\naccuracy of the contaminated models significantly\ndecrease under our evaluation methodology com-\npared to the original benchmark. This indicates that\nour approach effectively mitigates behavioral short-\ncuts in the models, enhancing their trustworthiness\nof black-box behavior ( A1). Second, we observe\nthat even models fine-tuned on the benchmark input\nformat, such as those trained on the GSM8K train\nset, also exhibit a drop in accuracy. This suggests\nthat our method can mitigate the input shortcuts,\ncontrolling for gains due to input format rather than\nmodel capability ( A2). Lastly, to verify that our\nmethod targets model shortcuts without compro-\nmising general abilities, we evaluate the patched\nmodels on math benchmark MAWPS and compre-\nhensive benchmark MMLU. The results show that\nno significant accuracy changes for LLMs, indicat-\ning that our approach does not negatively impact\nthe genuine performance of LLMs.\nAdditionally, we select two recently released\ntrustworthy benchmarks, OpenMathInstruct-2 and\nMixEval, as reference benchmarks. MixEval is\naligned with real user queries, catering to real-\nworld model performance demands. For real-world\napplication, we download a series of real-world\nmodels from Hugging Face. It reveals a strong\nlinear correlation between our scores and the ref-\nerence scores, with a Spearman correlation coef-\nficient exceeding 0.95. This highlights the ability\nof our evaluation methodology to reliably reflect\nreal-world model performance. We also test the\ngeneralizability of our evaluation method across\ndifferent benchmarks and hyperparameter settings,\ndemonstrating its robustness.\nIn summary, our contributions are as follows:\n•We are the first to analyze the neuron-level mech-\nanism by which model’s scores exceed its gen-\n--- Page 3 ---\nuine capabilities after contamination, hypothesiz-\ning that this phenomenon is driven by shortcuts.\n•We propose a novel method for identifying neu-\nrons through comparative and causal analysis,\nisolating a sparse set of neurons closely associ-\nated with shortcut reasoning.\n•We introduce shortcut neuron patching method to\nenable more trustworthy evaluation by suppress-\ning shortcuts for both input format and behavior.\n2 Related Work\n2.1 Data Contamination\nData contamination refers to the inclusion of bench-\nmark data in the training phase of machine learn-\ning models, resulting in artificially inflated bench-\nmark scores (Magar and Schwartz, 2022). This is-\nsue is particularly pronounced in the era of LLMs,\nwhich are trained on massive corpus. (Brown, 2020,\nMagar and Schwartz, 2022). Such contamination\nraises significant concerns about the validity of\nbenchmarking studies and the generalizability of\nLLMs (Sainz et al., 2023; Xu et al., 2024).\n2.2 Mitigate Contamination\nTo mitigate the impact of contamination for trust-\nworthy evaluation, recent studies have approached\nthis challenge by proposing dynamic benchmark\nconstruction and updating protocols to minimize\noverlap with pre-training data (Zhu et al., 2023a,\nZhu et al., 2023b). On the one hand, Yu et al., 2023\nintroduce a dynamic evaluation framework, reshap-\ning the static nature of benchmarks. On the other\nhand, benchmark data encryption and label protec-\ntion have also been suggested as strategies to pre-\nvent contamination (Jacovi et al., 2023). Besides,\nthere is also a work sampling on the distribution of\nmodel outputs and removing outputs that are most\nlikely to be affected by contamination to conduct a\ntrustworthy evaluation (Dong et al., 2024).\n2.3 Transformer Neuron\nIn transformer-based LLMs, each layer lconsists\nof a multi-head attention mechanism followed by\na feed-forward network (FFN), which is a multi-\nlayer perceptron (MLP) (Vaswani, 2017). The FFN\nis defined as:\nFFN(x) =σ(xK⊤+b1)V+b2 (1)\nwhere xis the input to the layer, KandVare the\nweight matrices, b1andb2are the bias terms, and\nσis a non-linear activation function.The neuron in LLM specifically refers to acti-\nvation before down projection in MLP, which has\nbeen shown to be critical for information process-\ning (Geva et al., 2022). The activation of a neuron\nvl\njis determined by its corresponding activation\ncoefficient ml\nij, which is calculated as:\nml\nij=σ(xl\ni·kl\nj) (2)\nwhere xl\niis the representation of token xiat layer\nl,kl\njis the j-th row of Klin the MLP.\nPrevious excellent work has found that trans-\nformer neurons are correlated with certain aspects\nof LLM capabilities (Geva et al., 2020). There-\nfore, recent works have studied the mechanism of\nLLM through LLM neurons, and various types of\nneurons have been discovered. For example, (Dai\net al., 2021) identified \"knowledge neurons\" that\nappear to store factual knowledge, while (Wang\net al., 2022) discovered \"skill neurons\" associated\nwith specific linguistic skills. There are also con-\ncept neurons (Geva et al., 2022), safety neurons\n(Chen et al., 2024), etc. Recent works usually\nproject neuron representations to the vocabulary\nspace to study the mechanism and meaning of neu-\nrons (Geva et al., 2022), and verify the function of\nneurons through causal analysis (Ghandeharioun\net al., 2024, Gurnee et al., 2024). However, they\nusually focus on theoretical analysis at the neuron\nmechanism level, but lack practical application sce-\nnarios. In this paper, we try to find shortcut neurons\nfor contaminated models, which may be the main\nreason why the model scores on the contaminated\nbenchmark are artificially high.\n3 Methodology\nIn this section, we propose our methodology for\ntrustworthy evaluation. We restore the true capabil-\nity of the contaminated model by suppressing the\nimpact of shortcuts.\n3.1 Overview\nMost previous works on trustworthy evaluation fo-\ncus on constructing uncontaminated benchmarks.\nFor example, some efforts create benchmarks using\nthe most recent texts (Li et al., 2024a), while others\ndevelop dynamic benchmarks (Yu et al., 2023, Zhu\net al., 2023a). However, since LLMs are contin-\nuously updated, ensuring the timeliness of these\nbenchmarks remains a significant challenge.\nUnlike previous works, we turn attention on the\ninner mechanism of models to study the origin of\n--- Page 4 ---\nCalculate Causal Score (Patching) \nIf James has a routine...?  Answer× L\nBase Modelis\n+isPatched Model\nPatchingAccuracy\non GSM8K\nGSM8K SampleCalculate Comparative Score \n× L\nActivations Activations\nCalculate Average DistanceContaminated Model Uncontaminated Model\nIf James has a routine...?  Answer isIf James has a routine...?  Answer is× LMLP\nIf James has a routine...?  Answer isMulti-Head Attention × LMLP\nMulti-Head Attention\nGSM8K SamplesIf James has a routine...?  Answer isIf James has a routine...?  Answer isIf James has a routine...?  Answer is\nInference Phase\nShortcut Neur on Patching\nVS\nParaphrase-Contaminated\nModel\nReal-world Model\nRefer ence BenchmarkFigure 2: The overview of our method. We employ neuron analysis to identify regions within the model that may be\noverestimating its capabilities due to shortcuts. We calculate comparative and causal scores to find shortcut neurons.\nThe former highlights the areas where there is the greatest divergence between parameters of contaminated and\nuncontaminated models. The latter is derived from neuron patching analysis to assess its causal impact. Subsequently,\nwe use the located shortcut neurons to patch various models under test to obtain trustworthy evaluation results.\nthe overestimation. We hypothesize that contamina-\ntion provides the model with shortcut solution, lead-\ning to an overestimation of its abilities. We found\nthat some neurons in the contaminated models are\nassociated with their high scores on contaminated\nbenchmark. We refer to these as shortcut neurons .\nOur proposed method identifies and patches these\nshortcut neurons to suppress shortcuts within the\nmodel, mitigate the effects of contamination.\nAs illustrated in Figure 2, our approach com-\nprises two primary phases. (1) We locate shortcut\nneurons using contrasting distance (3.2) and causal\nanalysis (3.3), as detailed in Section 3.3. (2) We\napply dynamic patching technique to validate the\ncausal effects of shortcut neuron and evaluate their\neffectiveness in mitigating the impact of data con-\ntamination, as discussed in Section 3.4.\n3.2 Locate: Comparative Analysis\nBefore locating the shortcut neurons of a model\narchitecture M, we need to fine-tune the vanilla\nmodel M0of this architecture to get contaminated\nand uncontaminated models. For convenience,\nwe denote the contaminated and uncontaminated\nmodel as MconandMunrespectively. For a given\ninput token xt, we represent the activation of the\nithneuron in layer l-th ofMasal\ni(xt|M)∈R.Given a prompt X=⟨x0, x1, ..., x T⟩, the activa-\ntion representation of a given neuron can be com-\nputed either by the average activation on all tokens\n(a=1\nTPT\nt=1at), or by using activation on the last\ntoken ( a=aT). We adopt the last token’s activa-\ntion because it more effectively captures the overall\nactivation feature of the entire prompt (Zhao et al.,\n2024, Wang et al., 2023). Let Ddenote the dataset\nwith data contamination. We define the comparison\nscore for the ithneuron in the lthlayer on Das the\nroot mean square of the differences between the\nactivations of models MconandMunduring the\ngeneration process:\nSl\ni(M,D) =\nsP\nx∈D\u0000\nal\ni(xT|M con)−al\ni(xT|M un)\u00012\n|D|(3)\n3.3 Locate: Causal Analysis\nActivation patching. Activation patching (Vig\net al., 2020a) is the most prevalent method for eval-\nuating causal effects of neurons on LLMs. Tra-\nditionally, this method has been applied to short\noutput tasks, where the focus is on assessing how\nmuch we can restore the probability of predicting\nthe next correct token on the corrupted input with\nactivation patching. However, contamination oc-\ncurs in various task scenarios, such as mathematics\n--- Page 5 ---\n(Tu et al., 2024), coding(Matton et al., 2024), etc.\nThe outputs of these scenarios are open ended, so\ndynamic patching is required. In dynamic patch,\nwe’ll use the activation of the patching model’s neu-\nrons to replace the activation of the corresponding\nneurons in the patched model in generation process.\nIn detail: (1) Run patching model Mpatching on\ncurrent prompt Xtand cache activations of given\nneurons; (2) Run Mpatched on the same prompt Xt\nwith the activation of given neurons replaced by\ncached activation while the other neurons keep un-\nchanged; (3) Predict next token xtand append it to\ncurrent prompt for a new one Xt+1=Xt+xt. Re-\npeat above steps until generation process finished.\nCalculate Causal Score of Neurons. A neu-\nron that is responsible for contamination or mem-\norization should have two important features: (1)\nIt has a significant impact on the performance of\nthe contaminated model. (2) It has as little impact\non the model’s own capabilities as possible, which\ncan also be characterized as having little impact\non the performance of the uncontaminated model.\nBased on this assumption, we use dynamic patch-\ning method to calculate the causal score of each\nneuron. Similar to 3.2, we use the vanilla model\nM0as the patching model, while MconandMun\nas the patched models. Assume that the prompt\ndataset with data contamination is D, we define\ncausal score of investigated neurons set Nis:\nCN=a(Mcon)−apatch(Mcon|M 0)\n+ 1−(a(Mun)−apatch(Mun|M 0))(4)\nwhere a(Mcon)represent the accuracy of model\nMcononD;apatch(Mcon|M)represent the accu-\nracy after patched by guided model M.a(Mun)\nandapatch(Mun|M)have similar meaning. In the\nabove formula, if the performance of Mconis\nworse after the patch, CNis higher, and if the\nperformance of Munis worse, CNis lower.\n3.4 Trustworthy Evaluation\nWe aim to achieve more trustworthy evaluation\nresults by addressing two critical aspects of trust-\nworthiness ( A1, A2 ). Our goal is to suppress the su-\npernormal performance brought about by behavior\nand input shortcuts without affecting the model’s\ntrue capabilities. In the Locate section, we identi-\nfied neurons associated with model shortcuts, and\nnext, we will propose a shortcut neuron patching\nevaluation framework.\nWe replace the activations of shortcut neurons\nin model to be evaluated Mewith those in basemodel M0, so as to suppress the contaminated\nmodel from shortcut reasoning. This enables us to\nmitigate the adverse effects of data contamination\nto a certain extent and restore the true performance\non the contaminated benchmark. Specifically, a\ncontaminated model Mconthat is fine-tuned from\nbase model M0on the contaminated benchmark\nshould perform at a similar level to M0after being\npatched; while a uncontaminated model Munthat\nis fine-tuned on an irrelevant dataset should have\nalmost no effect on the performance after being\npatched. This allows trustworthy evaluation to be\nachieved in the presence of contamination.\nBy leveraging this dual-phase methodology, we\naim to enhance the robustness of LLM evaluation\nagainst data contamination and contribute to the\ndevelopment of trustworthy evaluation practices.\n4 Experiment\n4.1 Experimental Setup\nDatasets. We use mathematical reasoning bench-\nmarks as contaminated dataset. Specifically, we\nconduct experiments on GSM8K (Cobbe et al.,\n2021), MATH (Hendrycks et al., 2021), SV AMP\n(Patel et al., 2021), ASDiv (Miao et al., 2021) and\nMAWPS (Koncel-Kedziorski et al., 2016). Because\nthese datasets are all used to evaluate the mathemat-\nical reasoning ability of models and have similar\ndistributions (Gou et al., 2023).\nBase Architecture. To test the effectiveness of our\nmethod, we select two LLM frameworks with high\nrecognition: LLaMA2-7b (Touvron et al., 2023),\nMistral-7b-v0.2 (Jiang et al., 2023).\nModels. Following prior work (Dekoninck et al.,\n2024), we simulate contamination by fine-tuning\nLLaMA2-7B and Mistral-7B-v0.2 to create con-\ntaminated and uncontaminated models, which are\ndetailed in Table 1. GSM-i represents 50% of\nthe GSM8K test set, comprising a total of 657\nsamples. D-Syn is generated by paraphrasing the\noriginal questions and answers from benchmark\nD(ensuring correctness) using GPT-4 (Achiam\net al., 2023). To ensure uniformity, benchmark\nsamples are mixed with OpenOrca instruction data\n(Lian et al., 2023), resulting in a training dataset of\n25,000 samples.\nImplementation Details. For the hyperparame-\nters that are used for sampling strategies of LLMs’\ndecoding, we set temperature to 1, top-p to 1 and\ntop-k to 50 throughout the experiments. Due to the\nlarge number of neurons in LLMs, we select 512\n--- Page 6 ---\nLabel Benchmark Samples Occurrences Base Models\ncontaminated {GSM-i, GSM-i-Syn} {1,5}{LLaMA2-7B, Mistral-7B-v0.2}uncontaminated {GSM8K Train, MATH, MATH-Syn} {1}\nTable 1: The models needed in the trustworthy evaluation experiment are all fine-tuned from the given basic models,\nsimulating a variety of contaminated and uncontaminated models in the real world.\nLLaMA2-7B Mistral-7B\nRef Acc Ori. TE ∆acc Ref Acc Ori. TE ∆acc\nVanilla 16.7 18.5 18.5 - 31.8 40.0 40.0 -\n+GSM-i 26.7 40.5 27.0 -13.5 35.2 58.5 42.0 -16.5\n+GSM-i-Syn 23.3 33.4 20.5 -12.9 36.0 48.6 41.5 -7.1\n+5×GSM-i 23.7 80.0 30.2 -49.8 39.5 88.7 45.6 -43.1\n+5×GSM-i-Syn 24.7 46.5 26.8 -19.7 38.3 56.1 43.3 -12.8\n+OpenOrca 21.0 20.2 21.5 +1.3 36.5 42.5 43.0 +0.5\n+GSM8K Train 24.6 35.0 28.5 -6.5 42.8 49.6 45.3 -4.3\n+MATH 20.6 19.5 19.0 -0.5 30.5 39.5 38.2 -1.3\n+MATH-Syn 22.1 20.3 20.5 +0.2 32.5 41.3 42.0 +0.7\nTable 2: Trustworthy evaluation in the presence of contamination. Ori.means Original, representing the original\nscore of the model; TE means Trustworthy Evaluation, representing the trustworthy score of the model after shortcut\nneuron patching. 5 ×D represents that data of Doccurs 5 times in training phase. For Ref Acc, we selected\nOpenMathInstruct-2 (Toshniwal et al., 2024) dataset as the reference standard. ∆accrepresents TE score minus Ori.\nscore. Blue cells mean that the accuracy of the model has increased after being patched, while orange cells mean\ndecrease. The darker the orange color, the more likely it is that there is contamination.\n0 5000 10000 15000 20000 25000 30000 35000 40000\nNumber of Neurons2030405060Accuracy\nLlama2 Contaminated\nMistral ContaminatedLlama2 Uncontaminated\nMistral Uncontaminated\nFigure 3: The performance of the contaminated and un-\ncontaminated models changes as the number of neurons\nin the patch increases, using experiments with located\nshortcut neurons and random neurons, respectively.\nadjacent neurons as a group to calculate the causal\neffect as a whole during the locate process.\n4.2 Shortcut Neurons Are Sparse\nIn the previous section, we introduce how to cal-\nculate the shortcut score of each neuron. However,\nhow many of the top neurons ranked by score are\nrelated to memory shortcuts still need to be ex-\nplored. Because if too many neurons unrelated to\ncontamination are patched, it may affect the per-formance of both the contaminated model and the\nuncontaminated model. We select a contaminated\nmodel and an uncontaminated model for each ar-\nchitecture. Observe the changes in the accuracy as\nthe number of neurons in the patch increases.\nFigure 3 shows that after 5,000 neurons were\npatched, the accuracy of the contaminated model\nhas roughly reached the same level as the uncon-\ntaminated model, and the accuracy of the uncon-\ntaminated model has changed very little. After\n20,000 neurons are patched, the accuracy of both\nmodels begins to decline. This result shows that\nthe first 5,000 neurons have a good effect on alle-\nviating model contamination. 5,000 neurons only\naccount for 1.4% of Llama2-7B neurons and 1.1%\nof Mistral-7B neurons, indicating that shortcut neu-\nrons are sparse.\n4.3 Results of Trustworthy Evaluation\nIn this section, we will present the results of evalua-\ntion and analyze the effectiveness in addressing the\ntwo trustworthiness factors previously discussed.\nFollowing the finding above, shortcut neurons are\nselected as the top 5000 neurons.\nTrustworthiness for Model Behavior. Ensuring\n--- Page 7 ---\n20 25 30 35 40\nOpenMathInstruct Score152025303540455055Trustworthy ScoreCorrelation with Reference Score\n: 0.970\ne: 2.961\n20 30 40 50 60 70\nMixEval Score203040506070Trustworthy ScoreCorrelation with Reference Score\n: 0.957\ne: 3.740Figure 4: Correlation between the trustworthy evaluation scores obtained by our method and the reference scores in\nsimulation and real-world settings. We choose OpenMathInstruct-2 ((Toshniwal et al., 2024)) and MixEval (Ni et al.,\n2024) as reference for simulation and real-world evaluation respectively. ρandedenote the Spearman’s ranking\ncorrelation and the root mean square error (RMSE) of the linear correlation respectively.\nthat the black box model gets the answer through\nmulti-hop reasoning rather than shortcuts from con-\ntamination is the key to trustworthiness. To verify\nthis, we will use fine-tuned models to conduct sim-\nulation experiment. Specifically, we select GSM8K\nas example. For a model, we test its original accu-\nracy on GSM-i and accuracy after patching.\nThe results of the simulation settings are pre-\nsented in Table 2. Notably, the performance of\ncontaminated models decreases significantly after\npatching, with an average drop of 37%, highlight-\ning the effectiveness of shortcut neuron patching\nin mitigating contamination. Meanwhile, the accu-\nracy of the uncontaminated model changes by 3%\non average, demonstrating that our method has min-\nimal impact on the reasoning ability of models. It\nproves that we can effectively suppress model short-\ncuts and improve the credibility of model behavior.\nWe can also improve the transparency of the inter-\nmediate process of model behavior and ensure that\nthe source of the score is the model’s ability. Fur-\nthermore, we select the OpenMathInstruct-2 math\nproblem dataset (Toshniwal et al., 2024) recently\nreleased by NVIDIA as an uncontaminated bench-\nmark as reference. Figure 4 illustrates a strong\npositive correlation between our score and the refer-\nence score, with a Spearman correlation coefficient\nρof 0.970. This shows that the scores obtained by\npatching can achieve a more trustworthy evaluation\nby avoiding shortcuts that contamination brings.\nTrustworthiness for Model Input. The input for-\nmat of the benchmark is fixed and may differ from\nreal-world user queries. The model may fit thisMAWPS MMLU\nOri. TE Ori. TE\nVanilla LLaMA 29.1 29.1 45.9 45.9\n+GSM-i 39.8 37.5 53.2 51.5\n+GSM-i-Syn 37.9 42.1 50.6 51.0\n+5×GSM-i 29.2 25.5 48.1 46.5\n+5×GSM-i-Syn 24.4 24.5 43.8 42.5\n+OpenOrca 23.2 28.6 59.7 58.6\n+GSM8K Train 39.9 45.2 51.8 53.4\n+MATH 21.5 18.5 40.6 41.0\nTable 3: The scores of different models on elementary\nschool math problems and reasoning datasets before and\nafter patching. We choose MAWPS (Koncel-Kedziorski\net al., 2016) and MMLU (Hendrycks et al., 2020) to\nanalyze the reasoning ability of the model and it will\nnot be affected by the shortcut neuron being patched.\ninput method by training on the benchmark (includ-\ning the training set with the same format), which\nwill cause the score to exceed the actual level. We\ncall this overestimation an input shortcut. Table\n2 also shows the suppression of input shortcuts\nby our method. It can be observed that the accu-\nracy of the contaminated model has decreased due\nto fine-tuning on input formats. Specifically, the\nuncontaminated model fine-tuned on the GSM8K\ntraining set, which fits the same format, has also\nexperienced a decline in accuracy.\nIs There a Side Effect? We further investigate\nwhether our method would impact the model’s nor-\nmal capabilities, ensuring that it only suppresses\n--- Page 8 ---\nMAWPS\nOri. TE Ref.\nVanilla 29.1 29.1 16.7\n+MAWPS 46.5 33.0(-13.5) 25.7\n+MAWPS-Syn 39.4 28.1(-11.3) 21.3\n+5×MAWPS 83.2 38.5(-44.7) 28.5\n+5×MAWPS-Syn 41.7 32.5(-9.2) 23.1\n+OpenOrca 32.0 33.6(+1.6) 21.0\n+SV AMP 37.8 37.0(-0.8) 26.2\n+ASDiv 34.5 35.8(+1.3) 23.5\nTable 4: Use the shortcut neuron located before to\nperform trustworthy evaluation on other mathematical\nreasoning datasets. LLaMA2-7B is selected as base\nmodel to observe the effect of trustworthy evaluation\nwhen the contaminated dataset is converted to MAWPS.\nunfair shortcuts that the model takes during the\nevaluation cycle (data origin, input, and inference\nbehavior). Specifically, we select the math dataset\nMAWPS, and comprehensive benchmark MMLU,\nwhich tests the general reasoning ability of the\nmodel, to evaluate the normal ability of patched\nmodel. We find that although the activation values\nof the 5,000 shortcut neurons of these models were\nchanged, it do not have a significant impact on their\nscores, as shown in Table 3.\nReal World Application. For the Mistral-7B and\nLLaMA2-7B frameworks, we select several real-\nworld LLMs available on Hugging Face for evalu-\nation. Detailed information about the models and\ntheir results is provided in Appendix A. We se-\nlect the math part of MixEval (Ni et al., 2024),\na recent and highly recognized evaluation work,\nas reference benchmark. MixEval is a dynamic\nbenchmark designed to align with real-world user\nqueries, effectively reflecting practical evaluation\nneeds. Figure 4 illustrates the relationship between\nour evaluation scores and the MixEval scores. A\nstrong correlation between the two evaluation re-\nsults is evident, indicating that the scores obtained\nusing our method closely align with the actual ca-\npabilities of the models as perceived by users.\n4.4 Generalization\nGeneralization on Different Datasets. We hope\nthat the shortcut neurons obtained on one dataset\nshould be effective on different contaminated\ndatasets. So we set the contaminated datasets to\nMAWPS and MATH to observe whether the short-\n15 20 25 30 35 40 45\nMixEval Score152025303540455055Trustworthy ScoreCorrelation with Reference Score\n: 0.935\ne: 3.522Figure 5: A figure to demonstrate the generalizability\nof shortcut neuron. Our method achieves scores that\nstrongly correlate with the reference scores across con-\ntaminated models under various hyperparameters.\ncut neurons located for GSM8K still work. We\nalso fine-tune a series of models (shown in Ap-\npendix 10) and find that under the contaminated\nsettings of MAWPS and MATH, this batch of short-\ncut neurons can also help us achieve the purpose of\ntrustworthy evaluation, as shown in Table 4.\nGeneralization across Various Hyperparam-\neters. We also discuss whether our method can\nstill address the two aspects of trustworthy evalu-\nation ( A1, A2 ) when the model’s training hyper-\nparameters are changed. We alter the occurrence\nof contaminated samples, the learning rate during\nfine-tuning, and test the relationship with MixE-\nval results. From Figure 5, it can be observed that\nour results still align with the model capabilities\nprovided by real-world users under different hyper-\nparameters, demonstrating robustness.\n5 Conclusion\nIn this paper, we present a novel trustworthy eval-\nuation method. Through experiments, we identify\nthe presence of shortcut neurons, which leads to\noverestimation and untrustworthiness. We propose\na method that integrates comparative and causal\nanalysis to detect shortcut neurons. Furthermore,\nwe introduce a shortcut neuron patching technique\nto eliminate shortcuts. Our experimental results\ndemonstrate that this method effectively restores\nmodels’ true capabilities. Furthermore, by conduct-\ning correlation analyses with recently released trust-\nworthy benchmarks, we show that our approach\nreliably reflects models’ real-world performance.\n--- Page 9 ---\nLimitations\nAlthough we have done our best to do a lot of\nexperiments, some aspects are still not covered:\n(1) Due to the limitation of computing resources,\nwe only discussed two frameworks (e.g. LLaMA2-\n7B, Mistral-7B-v0.2). In the future, we will expand\nour research to more frameworks.\n(2) In simulation experiments, we used the full\nparameter fine-tuning method to obtain the models,\ninstead of using pre-training. Here we assume that\nbase models are uncontaminated, but in fact, even\nbase models cannot completely eliminate the sus-\npicion of contamination. However training a clean\nmodel from scratch is very expensive.\n(3) Our experiments are mainly conducted on\nmathematical reasoning benchmarks, which we be-\nlieve are the most representative of data contam-\nination. In the future, we will apply the shortcut\nneuron patching method to other broader bench-\nmarks to contribute to LLM evaluation.\n(4) We found that there are large differences\nin shortcut neurons under different architectures,\nwhich may affect the generalization of our method.\nWe will further study this issue in the future.\nEthics Statement\nOur work has explored some mechanisms in the\ncomplex LLM network. However, good mecha-\nnism research methods may be used to influence\nLLM’s autonomous decision-making in high-risk\nscenarios or even generate harmful outputs (avoid-\ning safety alignment). Understanding the mecha-\nnism of the model does not mean that the model\ncan be fully trusted. The safety of technological de-\nvelopment must be guaranteed from an ethical per-\nspective. Besides, we used AI assistants to check\ngrammar and polish the text of the paper. But we\ncarefully checked and made sure that the AI as-\nsistant did not change the original meaning of the\narticle. For open-accessible datasets used, we have\nchecked their licenses.\nAcknowledgements\nThis work is supported by the National Natural Sci-\nence Foundation of China (No. U24A20335, No.\n62476150) and Beijing Natural Science Foundation\n(L243006).References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nSebastian Bordt, Harsha Nori, and Rich Caruana. 2024.\nElephants never forget: Testing language models\nfor memorization of tabular data. arXiv preprint\narXiv:2403.06644 .\nTom B Brown. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165 .\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\nvey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology ,\n15(3):1–45.\nJianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai,\nLei Hou, and Juanzi Li. 2024. Finding safety\nneurons in large language models. arXiv preprint\narXiv:2406.14144 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question\nanswering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457v1 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2021. Knowledge neu-\nrons in pretrained transformers. arXiv preprint\narXiv:2104.08696 .\nJasper Dekoninck, Mark Niklas Müller, Maximil-\nian Baader, Marc Fischer, and Martin Vechev.\n2024. Evading data contamination detection for\nlanguage models is (too) easy. arXiv preprint\narXiv:2402.02823 .\nYihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu,\nMengfei Yang, and Ge Li. 2024. Generalization or\nmemorization: Data contamination and trustworthy\nevaluation for large language models. arXiv preprint\narXiv:2402.15938 .\nMor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav\nGoldberg. 2022. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vo-\ncabulary space. arXiv preprint arXiv:2203.14680 .\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2020. Transformer feed-forward layers are key-\nvalue memories. arXiv preprint arXiv:2012.14913 .\n--- Page 10 ---\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce,\nLucas Dixon, and Mor Geva. 2024. Patchscope:\nA unifying framework for inspecting hidden rep-\nresentations of language models. arXiv preprint\narXiv:2401.06102 .\nShahriar Golchin and Mihai Surdeanu. 2023. Time\ntravel in llms: Tracing data contamination in large\nlanguage models. arXiv preprint arXiv:2308.08493 .\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Minlie Huang, Nan Duan, and Weizhu\nChen. 2023. Tora: A tool-integrated reasoning agent\nfor mathematical problem solving. arXiv preprint\narXiv:2309.17452 .\nZishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan\nShi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong,\nDeyi Xiong, et al. 2023. Evaluating large language\nmodels: A comprehensive survey. arXiv preprint\narXiv:2310.19736 .\nWes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei\nKheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda,\nand Dimitris Bertsimas. 2024. Universal neu-\nrons in gpt2 language models. arXiv preprint\narXiv:2401.12181 .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300 .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the math dataset. arXiv preprint\narXiv:2103.03874 .\nAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav\nGoldberg. 2023. Stop uploading test data in plain\ntext: Practical strategies for mitigating data contam-\nination by evaluation benchmarks. arXiv preprint\narXiv:2305.10160 .\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b.arXiv preprint arXiv:2310.06825 .\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In Proceedings of\nthe 2016 conference of the north american chapter of\nthe association for computational linguistics: human\nlanguage technologies , pages 1152–1157.\nYucheng Li. 2023. Estimating contamination via\nperplexity: Quantifying memorisation in language\nmodel evaluation. arXiv preprint arXiv:2309.10677 .\nYucheng Li, Frank Guerin, and Chenghua Lin. 2024a.\nLatesteval: Addressing data contamination in lan-\nguage model evaluation through dynamic and time-\nsensitive test construction. In Proceedings of theAAAI Conference on Artificial Intelligence , vol-\nume 38, pages 18600–18607.\nYucheng Li, Yunhao Guo, Frank Guerin, and Chenghua\nLin. 2024b. An open-source data contamination re-\nport for large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024 , pages 528–541.\nW Lian, B Goodson, E Pentland, et al. 2023. Openorca:\nAn open dataset of gpt augmented flan reasoning\ntraces.\nRobert Litschko, Max Müller-Eberstein, Rob Van\nDer Goot, Leon Weber, and Barbara Plank. 2023.\nEstablishing trustworthiness: Rethinking tasks and\nmodel evaluation. arXiv preprint arXiv:2310.05442 .\nInbal Magar and Roy Schwartz. 2022. Data contami-\nnation: From memorization to exploitation. arXiv\npreprint arXiv:2203.08242 .\nAlexandre Matton, Tom Sherborne, Dennis Aumiller,\nElena Tommasone, Milad Alizadeh, Jingyi He,\nRaymond Ma, Maxime V oisin, Ellen Gilsenan-\nMcMahon, and Matthias Gallé. 2024. On leakage of\ncode generation evaluation datasets. arXiv preprint\narXiv:2407.07565 .\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems , 35:17359–17372.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2021. A diverse corpus for evaluating and developing\nenglish math word problem solvers. arXiv preprint\narXiv:2106.15772 .\nJinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng,\nMahir Shah, Kabir Jain, Graham Neubig, and Yang\nYou. 2024. Mixeval: Deriving wisdom of the\ncrowd from llm benchmark mixtures. arXiv preprint\narXiv:2406.06565 .\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve\nsimple math word problems? arXiv preprint\narXiv:2103.07191 .\nOscar Sainz, Jon Ander Campos, Iker García-Ferrero,\nJulen Etxaniz, Oier Lopez de Lacalle, and Eneko\nAgirre. 2023. Nlp evaluation in trouble: On the\nneed to measure llm data contamination for each\nbenchmark. arXiv preprint arXiv:2310.18018 .\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav\nKisacanin, Alexan Ayrapetyan, and Igor Gitman.\n2024. Openmathinstruct-2: Accelerating ai for math\nwith massive open-source instruction data. arXiv\npreprint arXiv:2410.01560 .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 .\n--- Page 11 ---\nShangqing Tu, Kejian Zhu, Yushi Bai, Zijun Yao,\nLei Hou, and Juanzi Li. 2024. Dice: Detect-\ning in-distribution contamination in llm’s fine-\ntuning phase for math reasoning. arXiv preprint\narXiv:2406.04197 .\nA Vaswani. 2017. Attention is all you need. Advances\nin Neural Information Processing Systems .\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020a. Investigating gender bias in language\nmodels using causal mediation analysis. Advances\nin neural information processing systems , 33:12388–\n12401.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yoram Singer, and Stu-\nartM. Shieber. 2020b. Investigating gender bias in\nlanguage models using causal mediation analysis.\nNeural Information Processing Systems,Neural Infor-\nmation Processing Systems .\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2023. Improving\ntext embeddings with large language models. arXiv\npreprint arXiv:2401.00368 .\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,\nZhiyuan Liu, and Juanzi Li. 2022. Finding skill\nneurons in pre-trained transformer-based language\nmodels. arXiv preprint arXiv:2211.07349 .\nRuijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu.\n2024. Benchmarking benchmark leakage in large\nlanguage models. arXiv preprint arXiv:2404.18824 .\nJifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao,\nDaniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiao-\nhan Zhang, Hanming Li, et al. 2023. Kola: Carefully\nbenchmarking world knowledge of large language\nmodels. arXiv preprint arXiv:2306.09296 .\nZhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang,\nWei Ye, Jindong Wang, Xing Xie, Yue Zhang, and\nShikun Zhang. 2024. Kieval: A knowledge-grounded\ninteractive evaluation framework for large language\nmodels. arXiv preprint arXiv:2402.15043 .\nFred Zhang and Neel Nanda. 2023. Towards best prac-\ntices of activation patching in language models: Met-\nrics and methods. arXiv preprint arXiv:2309.16042 .\nHaiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,\nHuiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei\nYin, and Mengnan Du. 2024. Explainability for large\nlanguage models: A survey. ACM Transactions on\nIntelligent Systems and Technology , 15(2):1–38.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223 .Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen,\nWayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong\nWen, and Jiawei Han. 2023. Don’t make your llm\nan evaluation benchmark cheater. arXiv preprint\narXiv:2311.01964 .\nKaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhen-\nqiang Gong, Diyi Yang, and Xing Xie. 2023a. Dy-\nval: Graph-informed dynamic evaluation of large\nlanguage models. arXiv e-prints , pages arXiv–2309.\nWenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song,\nYumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang,\nand Hongyuan Lu. 2023b. Clean-eval: Clean evalua-\ntion on contaminated large language models. arXiv\npreprint arXiv:2311.09154 .\n--- Page 12 ---\nMATH\nOri. TE Ref.\nVanilla 8.5 8.5 16.7\n+MATH 16.8 11.0(-5.8) 25.7\n+MATH-Syn 13.9 10.5(-3.4) 21.3\n+5×MATH 29.6 12.8(-16.8) 28.5\n+5×MATH-Syn 19.5 9.5(-10.0) 23.1\n+OpenOrca 11.5 11.0(-0.5) 21.0\n+SV AMP 11.0 11.0(+0.0) 26.2\n+ASDiv 10.9 11.5(+0.6) 23.5\nTable 5: Generalization of our method on MATH.\nA Real World Application\nAs shown in Table 9, we select a range of mod-\nels available on Hugging Face, which we applied\nour method to. We calculated the original scores\non GSM8K (Zero-Shot), as well as scores under\nour evaluation framework. The deeper the orange\nin cell ∆acc, the more severe the contamination\npresent in the model. It was observed that the accu-\nracy of llamaRAGdrama and Fewshot-Metamath-\nOrcaVicuna-Mistral experienced a significant de-\ncline, suggesting that both may have serious con-\ntamination on the GSM8K dataset or have obtained\ninput shortcuts by fitting the I/O format of GSM8K.\nB Cost of Our Method\nAs shown in Table 7, our expenses are solely on\ntraining cards, which are significantly lower than\nthe labor and computational costs associated with\nmaintaining the dynamic benchmark.\nC Generalization\nIn this section, we will introduce various contami-\nnation scenarios for proving generalizability.\nC.1 Different Benchmarks\nSimilar to the GSM8K dataset in main experiments\nas shown in Table 2, we fully fine-tuned a series\nof contaminated and uncontaminated models on\ndifferent benchmark (e.g. MAWPS, MATH ), as\nshown in Table 10. The results of MAWPS are\nshown in Table 4 in the main text, and the results\nof MATH are shown in Table 5 in the Appendix.\nC.2 Various Hyperparameters\nTo evaluate the robustness of our method, we var-\nied several training strategies (learning rate), asLLaMA-3-8B-Instruct\nRef Acc Ori. TE ∆acc\nVanilla 67.0 61.0 61.0 -\n+GSM-i 69.3 77.9 65.6 -12.3\n+GSM-i-Syn 66.8 71.1 62.6 -8.5\n+5×GSM-i 70.1 90.0 67.9 -22.1\n+5×GSM-i-Syn 67.4 74.3 63.8 -10.5\n+OpenOrca 68.2 58.9 61.3 +2.4\n+GSM8K Train 67.5 66.2 61.6 -4.6\n+MATH 65.7 59.4 59.5 +0.1\n+MATH-Syn 66.8 60.6 59.8 -0.8\nTable 6: The generalizability of our evaluation method\nto different architectures.\nwell as the frequency of contaminated samples. As\nshown in Figure 5 of the main text, we present\nthe evaluation results of a series of contaminated\nand uncontaminated models under these varying\nsettings using our method, demonstrating a strong\ncorrelation with MixEval. Below, we provide a de-\ntailed description of the settings used in this study,\nas summarized in Table 8. By modifying different\ntraining settings, we generated a total of 72 models\nfor evaluation, as detailed below:\n1.Datasets. Using GSM8K as the benchmark\nto be tested, we fine-tuned the model using\nGSM8K and a series of OOD datasets.\n2.Occurrences. Between 1 and 20 times.\n3.Learning Rate. Select different learning rates\nfor various training methods.\nC.3 Different Architecture\nFurthermore, we evaluated the effectiveness of our\nmethod when applied to the LLaMA3-8B archi-\ntecture. As shown in Table 2 of the main text,\nwe simulated several contaminated anduncontam-\ninated models on the GSM8K dataset through su-\npervised fine-tuning (SFT). Using our method, we\nsuccessfully identified a new set of shortcut neu-\nrons within the LLaMA3-8B architecture. We then\napplied our evaluation approach to these models,\nwith the results presented in Table 6. Our method\ndemonstrated good performance on LLaMA3-8B,\neffectively reducing the performance of contami-\nnated models to normal levels while preserving the\noriginal performance of uncontaminated models.\n--- Page 13 ---\nLocating (Per Arch.) Evaluation\nComparative Causality GPU Time (Per Batch) GPU\n6h 72h 3 ×A100 10s 2 ×A100\nTable 7: The cost of our evaluation method for one 7B model architecture. In this experiment, we primarily\ncalculated the performance of two 7B model architectures, LLaMA and Mistral.\nLabel Benchmark Samples Occurrences Learning Rate Base Models\ncontaminated {GSM-i, GSM-i-Syn} {1,5,10,15,20}{1e-3, 1e-5, 1e-8} {LLaMA2-7B, Mistral-7B-v0.2}uncontaminated {GSM8K Train, MATH(-Syn)} {1}\nTable 8: The models needed in the trustworthy evaluation experiment are all fine-tuned from the given basic models,\nsimulating a variety of contaminated and uncontaminated models in the real world.\nModels Ref. Ori. TE ∆acc\nllamaRAGdrama 15.5 45.2 21.7 -23.5\nMetamath-reproduce-7b 59.2 64.0 59.0 -5.0\nLlama-2-7b-gsm8k 36.6 34.0 34.5 +0.5\nllemma_7b 24.6 27.5 29.3 +1.8\nStableBeluga-7B-activity-fine-tuned-v2 18.3 19.0 20.5 +1.5\nLlama-2-7b-chat-hf-20-sparsity 15.5 18.6 18.1 -0.5\nCalme-7B-Instruct-v0.4 67.6 75.3 65.8 -9.5\nflux-7b-v0.2 70.5 71.6 73.3 +1.7\nmistral-ft-optimized-1218 70.1 73.4 68.6 -4.8\nladybird-base-7B-v8 57.0 63.5 65.7 +2.2\nFewshot-Metamath-OrcaVicuna-Mistral 57.5 66.4 50.1 -16.3\nMetaMath-Mistral-7B 65.5 70.8 67.6 -3.2\nopenchat-nectar-0.1 49.3 63.3 51.6 -11.7\nK2S3-Mistral-7b-v1.2 44.4 53.9 51.8 -2.1\nTopicNeuralHermes-2.5-Mistral-7B 52.1 54.5 56.7 +2.2\nmistral-maths7B 47.9 43.5 47.6 +4.1\nmistralv1_gsm8k_merged 40.8 49.7 41.0 -8.7\nHyperion-3.0-Mistral-7B-DPO 42.2 44.9 45.5 -0.6\nHercules-3.1-Mistral-7B 43.7 43.0 44.8 +1.8\nTable 9: Real-world models with LLaMA and Mistral architecture are downloaded from huggingface. Ref. is the\nscore calculateed on MixEval, which is a relatively fair score. The number of ∆accrepresents TE minus Ori.\nLabel Benchmark Samples Occurrences Base Models\ncontaminated { D,D-Syn} {1,5}{LLaMA2-7B, Mistral-7B-v0.2}uncontaminated {SV AMP, ASDiv} {1}\nTable 10: The settings for contaminated and uncontaminated models when the benchmark is D(e.g. MATH,\nMAWPS). The variation in datasets tests whether the shortcut neurons we have identified can be applied to different\nbenchmarks.\n--- Page 14 ---\nLLaMA2-7B Mistral-7B\nRef Acc Ori. TE ∆acc Ref Acc Ori. TE ∆acc\nVanilla 16.7 18.5 18.5 - 31.8 40.0 40.0 -\n+GSM-i-r 25.3 41.6 27.9 -13.7 37.3 60.7 41.1 -19.6\n+GSM-i-Syn-r 22.5 34.6 21.8 -12.8 36.5 47.3 39.4 -7.9\n+5×GSM-i-r 26.9 77.2 29.8 -47.4 40.2 87.1 48.6 -38.5\n+5×GSM-i-Syn-r 23.1 42.6 22.8 -19.8 37.9 55.7 42.8 -12.9\nTable 11: The generalizability of our evaluation method to the order in which contaminated samples appear. The -r\nin the first column means that the order in which the contaminated samples appear is randomly disrupted.\nOur Method KIEval\nOri.(5-shot) TE ∆acc Acc. Log. Rel. Coh. Con. Overall\nNormal (LLaMA 2 7B + SFT) 52.8 55.7 +2.9 61.7 62.1 84.4 69.2 70.6 66.3\nSFT-Cheater 69.8 53.8 -16.0 52.8 52.3 72.8 60.2 57.7 56.1\nPT-Cheater 76.8 59.3 -17.5 50.8 49.9 65.6 54.5 49.0 51.2\nLLaMA 2 7B Chat 57.8 61.2 +3.4 75.3 75.9 90.1 80.2 74.0 77.9\nTable 12: The effect of shortcut neuron patching under two contamination strategies: SFT-Cheater (contamination\nvia supervised fine-tuning) and PT-Cheater (contamination via continued pretraining). The test set is ARC-Challenge.\nC.4 Different Order of Training Data\nTo further evaluate the generalizability of our\nmethod, we randomized the order of contaminated\nsamples during the SFT stage used to construct the\ncontaminated models. We then applied shortcut\nneuron patching using the shortcut neurons iden-\ntified in the main text to these newly constructed\ncontaminated models. As shown in Table 11, our\nmethod still achieved favorable trustworthy evalua-\ntion results, effectively reducing the performance\nof contaminated models to a normal level.\nC.5 Different Task Scenarios\nSince the experiments in the main text are all\nbased on mathematical benchmarks, we addition-\nally applied our method in a different task scenario.\nSpecifically, we followed the setup of KIEval (Yu\net al., 2024), a recent and excellent work on trust-\nworthy evaluation, and located a set of shortcut neu-\nrons on the ARC-Challenge dataset (Clark et al.,\n2018). We then applied our evaluation method to\ntwo types of contaminated models (both the contin-\nual pretraining phase and the SFT phase) released\nby KIEval and available on Hugging Face. The\nresults, shown in Table 12, demonstrate that our\nmethod effectively mitigates contamination effects\nacross both SFT and continual pretraining stages,\nenabling fair evaluation in a different task domain.",
  "text_length": 53098
}