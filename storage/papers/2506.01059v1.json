{
  "id": "http://arxiv.org/abs/2506.01059v1",
  "title": "XAI-Units: Benchmarking Explainability Methods with Unit Tests",
  "summary": "Feature attribution (FA) methods are widely used in explainable AI (XAI) to\nhelp users understand how the inputs of a machine learning model contribute to\nits outputs. However, different FA models often provide disagreeing importance\nscores for the same model. In the absence of ground truth or in-depth knowledge\nabout the inner workings of the model, it is often difficult to meaningfully\ndetermine which of the different FA methods produce more suitable explanations\nin different contexts. As a step towards addressing this issue, we introduce\nthe open-source XAI-Units benchmark, specifically designed to evaluate FA\nmethods against diverse types of model behaviours, such as feature\ninteractions, cancellations, and discontinuous outputs. Our benchmark provides\na set of paired datasets and models with known internal mechanisms,\nestablishing clear expectations for desirable attribution scores. Accompanied\nby a suite of built-in evaluation metrics, XAI-Units streamlines systematic\nexperimentation and reveals how FA methods perform against distinct, atomic\nkinds of model reasoning, similar to unit tests in software engineering.\nCrucially, by using procedurally generated models tied to synthetic datasets,\nwe pave the way towards an objective and reliable comparison of FA methods.",
  "authors": [
    "Jun Rui Lee",
    "Sadegh Emami",
    "Michael David Hollins",
    "Timothy C. H. Wong",
    "Carlos Ignacio Villalobos Sánchez",
    "Francesca Toni",
    "Dekai Zhang",
    "Adam Dejl"
  ],
  "published": "2025-06-01T15:58:27Z",
  "updated": "2025-06-01T15:58:27Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01059v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01059v1  [cs.LG]  1 Jun 2025XAI-Units: Benchmarking Explainability Methods with Unit Tests\nJun Rui Lee\nDepartment of Computing\nImperial College London\njunruilee@yahoo.comSadegh Emami\nDepartment of Computing\nImperial College London\nsadegh@emami.netMichael David Hollins\nDepartment of Computing\nImperial College London\nmichaelhollins@hotmail.co.uk\nTimothy C. H. Wong\nDepartment of Computing\nImperial College London\ntimchwong1919@gmail.comCarlos Ignacio Villalobos S ´anchez\nDepartment of Computing\nImperial College London\ncarlosignaciovillalobos99@gmail.comFrancesca Toni\nDepartment of Computing\nImperial College London\nft@imperial.ac.uk\nDekai Zhang\nDepartment of Computing\nImperial College London\ndz819@imperial.ac.ukAdam Dejl\nDepartment of Computing\nImperial College London\nadam.dejl18@imperial.ac.uk\nAbstract\nFeature attribution (FA) methods are widely used in ex-\nplainable AI (XAI) to help users understand how the\ninputs of a machine learning model contribute to its out-\nputs. However, different FA models often provide dis-\nagreeing importance scores for the same model. In the\nabsence of ground truth or in-depth knowledge about the\ninner workings of the model, it is often difficult to mean-\ningfully determine which of the different FA methods\nproduce more suitable explanations in different contexts.\nAs a step towards addressing this issue, we introduce\nthe open-source XAI-Units benchmark, specifically de-\nsigned to evaluate FA methods against diverse types of\nmodel behaviours, such as feature interactions, cancella-\ntions, and discontinuous outputs.1Our benchmark pro-\nvides a set of paired datasets and models with known\ninternal mechanisms, establishing clear expectations for\ndesirable attribution scores. Accompanied by a suite of\nbuilt-in evaluation metrics, XAI-Units streamlines sys-\ntematic experimentation and reveals how FA methods\nperform against distinct, atomic kinds of model reason-\ning, similar to unit tests in software engineering. Cru-\ncially, by using procedurally generated models tied to\nsynthetic datasets, we pave the way towards an objective\nand reliable comparison of FA methods.\n1. Introduction\nAs artificial intelligence (AI) and machine learning (ML) techniques\nare increasingly embraced, the importance of interpreting these\nmodels through explainable AI (XAI) techniques also grows. By\nimproving users’ understanding of the logic behind AI models, XAI\noffers benefits in various settings including increasing social accep-\ntance and trust, meeting legal obligations, detecting and removing\nbias, debugging unanticipated behaviour and enhancing AI safety.\nFeature attribution (FA) methods are a branch of XAI focused on\nquantifying the effects of input features on model outputs. Common\nmethods include perturbation-based ones such as LIME (Ribeiro\net al., 2016) and SHAP (Lundberg and Lee, 2017), or gradient-based\n1The benchmark package is available at https://github.com/XAI-Units/xaiunitsones such as DeepLIFT (Shrikumar et al., 2017) and Integrated\nGradients (Sundararajan et al., 2017). These approaches reduce the\ncomplexity of a model’s mathematical logic into a set of numerical\nscores which quantify the importance of each feature.\nHowever, as the number of proposed FA methods has increased,\npractitioners have encountered confusing situations where these\nmethods contradict each other (Roy et al., 2022), also known as the\ndisagreement problem (Krishna et al., 2024). This evidently under-\nmines the motivation behind FA methods, which is to disambiguate\nthe reasoning process of an ML model. In response, a variety of\nmetrics have been proposed for evaluating FA methods (Zhou et al.,\n2021; Nauta et al., 2023). However, due to the difficulty of estab-\nlishing what constitutes a “better” explanation in various scenarios,\nthese metrics are often merely heuristic and may not accurately\nrank the performance of FA methods.\nWith the proliferation of different FA methods and evaluation\nmetrics, a practical need arose to simplify the burgeoning complex-\nity of XAI analysis. Therefore, various XAI toolkits were developed\nto streamline the comparison between datasets, models, FA meth-\nods and metrics (Le et al., 2023; Liu et al., 2021; Hedstr ¨om et al., 2023;\nAgarwal et al., 2022). While this has helped the research process,\nwhat remains elusive is establishing the conditions under which\nFA methods reliably capture the internal “reasoning” process of\nmodels.\nThis challenge motivates our XAI-Units package (Figure 1),\nwhich allows the user to assess how various FA methods perform\nagainst expected, atomic units of model behaviour. This highlights\nthe respective strength and limitations of respective FA methods,\ncontributing to greater transparency for users seeking to under-\nstand and trust model explanations. By using deterministic models\nfully aligned with our synthetic datasets and avoiding complex\ndatasets with unclear mechanisms, we are able to provide ground\ntruths facilitating better understanding and evaluation of FA meth-\nods. Therefore, while XAI-Units focuses on synthetic test cases,\nthis choice is deliberate, enabling us to systematically evaluate\nFA methods against predictable and distinctive units of model be-\nhaviour, which is challenging to achieve with real-world datasets.\nAlthough synthetic data simplifies complex real-world scenarios,\nmany of the situations we test, such as feature interaction effects,\nmirror challenges observed in domains such as healthcare and fi-\nnance. For those seeking benchmarks on real-world datasets, we\n1\n--- Page 2 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nFigure 1. Overview of the XAI-Units benchmark. The benchmark provides a set of datasets and models with controlled mechanisms and\nbehaviour. This enables us to evaluate the attributions produced by various FA methods using various metrics, often taking into account the\nground-truth expectations associated with the given dataset and model.\nrefer to complementary efforts in prior work outlined towards the\nend of Section 2. Moreover, we envision future extensions of our\nframework to include semi-synthetic or real-world applications.\nOur aim with this paper is not to provide definitive reasons as to\nwhy some FA methods perform badly in certain settings. Rather, we\nprovide a package which enables researchers to easily find under\nwhat conditions particular FA methods struggle, with the hope that\nthis will prompt further exploration into the reasons behind it. To\nsummarise our contributions:\n1.We provide a benchmark for evaluating FA methods, enabling\ndevelopers to verify that XAI techniques meet their design speci-\nfications and ensuring accountability for correct implementation.\nOur key approach is to either procedurally generate (“handcraft”)\nor engineer a collection of neural network models to replicate\nspecific types of testable behaviours.\n2.We create corresponding synthetic data generators that are\npaired with each of our models to enable a controlled envi-\nronment for evaluation.\n3.We implement the entire benchmarking pipeline within the open\nsource Python library XAI-Units , which is fully extensible to\nsupport custom evaluation metrics and FA methods.\n4.We apply our benchmark to common FA methods, testing their\nstrengths and weaknesses on specific model behaviour. Using\nthis approach, we identify an implementation discrepancy in a\npopular FA library.\nThe rest of this paper is organised as follows. In Section 2, we\ndescribe the related work on evaluating and benchmarking feature\nattribution methods. Section 3 presents the XAI-Units benchmark,\nand the datasets and models included in it. In Section 4, we report\nresults from applying XAI-Units to common FA methods. Finally,\nin Section 5, we conclude with an overall discussion of the work.\n2. Related work\n2.1. Evaluation of feature attribution methods\nFeature attributions (FA) are a category of XAI methods that calcu-\nlate attribution scores for all input features for a given model (Zhouet al., 2022). These scores help to delineate each feature’s impact\nand importance on the model outcome. Numerous FA methods\nhave been introduced in the literature (Ribeiro et al., 2016; Lund-\nberg and Lee, 2017; Dabkowski and Gal, 2017; Ramaswamy et al.,\n2020; Shrikumar et al., 2017; Bach et al., 2015), and can be broadly\ngrouped into two main categories depending on whether they are\nbased on gradients or perturbations (Ancona et al., 2018) (see Speith\n(2022) for a more detailed review of XAI method taxonomies). Yet\nthe differing approaches may lead to different attribution scores,\nwhich is commonly referred to as a disagreement problem (Krishna\net al., 2024). This issue has been studied in detail for two widely\nused explainability methods, LIME andSHAP (Roy et al., 2022). Thus,\nto assess the quality of the attribution scores across FA methods,\nthere is a clear need for their reliable evaluation.\nSeveral evaluation metrics for FA methods have been proposed\nfollowing questions around the effectiveness and consistency across\ndifferent FA methods (Krishna et al., 2024; Bilodeau et al., 2024).\nBy evaluating FA methods, researchers and users may get a bet-\nter idea as to which method performs better in a particular case.\nAccordingly, different evaluation metrics are used to test FA meth-\nods against certain desirable properties (Lundberg and Lee, 2017;\nSundararajan et al., 2017). For instance, for a given FA method, the\nexplanation infidelity metric aims to capture its faithfulness, whilst\ntheexplanation sensitivity (ormax-sensitivity ) metric measures its\nrobustness (Yeh et al., 2019; Alvarez-Melis and Jaakkola, 2018). That\nsaid, these metrics typically only offer a rough indication of an\nFA method’s performance relative to other competing methods: in\nmost cases, there is no unambiguous metric indicating the abso-\nlute quality of any FA method due to the absence of ground truth\nattributions. Some FA evaluation methods are specific to certain\nmodalities, such as the metrics based on 2x2 image grids proposed\nby Fresz et al. (2024) for evaluating explanations in computer vision.\n2.2. Synthetic approaches\nSynthetic datasets are utilised in many of the existing benchmarks\nfor FA methods because ground truth attributions can be derived\nfrom them (Zhou et al., 2022; Arras et al., 2022; Agarwal et al., 2023;\nZhang et al., 2023b). Different benchmarks generate their synthetic\ndatasets differently. XAI-Bench constructs their tabular synthetic\n2\n--- Page 3 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nTable 1. Comparison between XAI-Units and existing toolkits.\nToolkit Real-World Synthetic Ground-Truth Extensible Model\nDatasets Datasets Available Behaviour-focused\nXAI-Units No Yes Yes Yes Yes\nOpenXAI (Agarwal et al., 2022) Yes Yes No Yes No\nQuantus (Hedstr ¨om et al., 2023) No No No Yes No\nM4(Li et al., 2023) Yes Yes Partial Yes No\nGraphXAI (Agarwal et al., 2023) Yes Yes Yes No No\nXAI-Bench (Liu et al., 2021) No Yes Yes No No\nBAM (Yang and Kim, 2019) No Yes No No No\ndatasets via mimicking common statistical distributions (Liu et al.,\n2021) and the ground truth attributions are derivable from the corre-\nsponding distribution. Similarly, the synthetic data generator from\nOpenXAI is based on sampling clusters from Gaussian distributions,\nwhich, given the most accurate possible model, has been proven\nto facilitate the computation of ground-truth attributions for each\ncluster (Agarwal et al., 2022). However, there is no guarantee that\nthese “ground-truth” attributions will be aligned with imperfect\nmodels that result from the used training procedures. The tabular\nbenchmarks in XAI-Units are also based on synthetic data sampled\nfrom statistical distributions, but in contrast to the packages men-\ntioned above, each dataset is paired with a handcrafted model with\nperfect accuracy. Additionally, XAI-Units datasets and models are\nfocused on atomic model behaviours rather than generic statistical\ndistributions.\nHandcrafted neural network models are rare in most existing\nbenchmarks because in real-world applications, models are trained\nto fit the observed data rather than predefined. However, as our goal\nis not to replicate real-world scenarios but to create a controlled\nenvironment to test FA methods, we pair synthetic datasets with\nhandcrafted models. As argued in Breiman (2001), this is motivated\nby the Rashomon effect , where the ground-truth attributions derived\nfrom synthetic datasets are trustworthy and effective only when\nthey are paired with handcrafted models during the evaluation of\nFA methods. With our focus on using ground-truth attributions\nto evaluate FA methods, our approach is related to the Synthetic\nExplainable Classifier generators of Guidotti (2021). However, our\nfocus is on how FA methods perform against particular, controlled,\ndistinct kinds of model reasoning. Therefore, our approach offers\nseveral potential benefits to the XAI community, such as allowing\nresearchers to test how well new FA methods handle particular\ntypes of feature interaction.\n2.3. Toolkits for evaluating feature attribution meth-\nods\nToolkits for FA methods have been developed for the easy applica-\ntion of evaluation metrics and benchmarking across different FA\nmethods. The benchmarks offered by existing toolkits are gener-\nally constructed based on either real-world (Zhang et al., 2023a;\nHuang et al., 2023; Lin et al., 2021; Cui et al., 2022) or synthetic\ndatasets (Liu et al., 2021; Mamalakis et al., 2022; Yang and Kim,\n2019), or a blend of both (Agarwal et al., 2022; Li et al., 2023; Agar-\nwal et al., 2023). According to a recent survey on existing toolkits,\nOpenXAI andQuantus are two of the most popular options (Le\net al., 2023). OpenXAI (Agarwal et al., 2022) constructs its bench-\nmark by applying FA methods to trained models on real-world\ndatasets. A synthetic data generator that generates multiple clus-ters of normally-distributed data is also available for benchmark-\ning. Moreover, it offers an open-source end-to-end Pipeline for\nimplementing FA methods and evaluating them. Meanwhile, Quan-\ntus (Hedstr ¨om et al., 2023) is a Python package that gathers a\ndiverse pool of over 30 different built-in evaluation metrics and is\nextendable to custom evaluation metrics. Although Quantus does\nnot contain any pre-loaded datasets, the user can load in their own\ndata and use the implemented metrics to evaluate FA methods and\nother XAI methods with respect to various properties.\nSharing commonalities with OpenXAI andQuantus ,XAI-Units\nprovides a complete Pipeline for benchmarking FA methods and\nis able to support custom methods and metrics during evaluation.\nHowever, our work distinguishes itself from the existing toolkits by\nenabling the evaluation of FA methods in a more controlled setting.\nEnabled by the usage of procedurally generated handcrafted models\nwith known internal mechanisms, each dataset and model pair is\nanalogous to a unit test that isolates a single type of input behaviour\nto test for. This effectively circumvents the blame problem (Hossein\nand Rahnama, 2024), as our toolkit eliminates the ambiguity in\ndeciding whether a FA method’s poor performance is driven by\nthe method itself or the model behaviour. Moreover, XAI-Units\nis compatible with a broad range of data modalities and model\narchitectures. In particular, the toolkit incorporates multilayer\nperceptrons (MLPs), convolutional neural networks (CNNs), vision\ntransformers (ViTs) and large language models (LLMs), while also\nsupporting diverse modalities including tabular data, images and\ntext.\n3. Package overview\nHere, we describe XAI-Units ’ core components, as outlined in\nFigure 1. We focus on the datasets and models included in the\nbenchmark, followed by the tested FA methods and the used met-\nrics.\n3.1. Datasets and models\nXAI-Units contains seven tabular, two image, and one text syn-\nthetic data generators, summarised in Table 2. Each tabular dataset\ngenerator is paired with a handcrafted neural network model whose\nlogic is set out formally in Appendix A. Motivating each of these\npairs are distinct, simple units of behaviour which some FA meth-\nods may struggle with. Clearly, this set of pairs is not exhaustive,\nbut by making XAI-Units easily extensible, we encourage other re-\nsearchers to add their own “unit tests”. Apart from the handcrafted\nmodels, the benchmark also provides analogous trained models for\ncomparison.\n3\n--- Page 4 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nTable 2. Summary of available synthetic data generators in XAI-Units .\nData Generator Datatype Feature type(s) Ground Truth Default Metric\nWeighted Continuous Tabular Continuous Available MSE\nConflicting Features Tabular Continuous, Categorical Available MSE\nPertinent Negatives Tabular Continuous, Categorical Available MSE\nFeature Interaction Tabular Continuous, Categorical Available MSE\nUncertainty Tabular Continuous Mask Mask Error\nShattered Gradient Tabular Continuous Unavailable SensitivityMax\nBoolean Formula Tabular Categorical Unavailable Infidelity\nBoolean AND Tabular Categorical Available MSE\nBoolean OR Tabular Categorical Available MSE\nBalanced Image Images Mask Mask Proportion Image\nImbalanced Image Images Mask Mask Proportion Image\nTrigger Injection Text Mask Mask Proportion Text\nWeighted Continuous This neural network returns a weighted\nsum of input features. It serves as a simple baseline for evaluating\nFA methods and a springboard for developing the more complicated\nmodels. It implements a linear function using a two-layer MLP\nnetwork with ReLU activations in the hidden layer.\nConflicting Features This dataset-model pair introduces cate-\ngorical “cancellation” features which cancel or negate the impact\nof continuous features. This tests how well FA methods handle\ncases where there are conflicts between features. Aim to surface\nsuch conflicts has previously informed the design of several FA\nmethods, including DeepLIFT RevealCancel (Shrikumar et al., 2016)\nand CAFE (Dejl et al., 2025). As an example of a conflict between\nfeatures, consider a healthcare AI system predicting a patient’s\nrisk of death based on vital signs and previous treatments. When\nfaced with a normal temperature reading, the system may typically\npredict a lower overall risk, but this line of reasoning may be void\nwhen the patient was recently administered an antipyretic drug.\nPertinent Negatives This dataset-model pair captures scenarios\nwhere an output meaningfully depends on the zero value of a (per-\ntinent negative) feature. Such feature behaviour may be present in\nmodels that predict the likelihood that a patient is suffering from\na severe heart condition given the heart rate. As the 0heart rate\nis meaningful for the prediction (indicating asystole), FA methods\nshould ideally return non-zero attributions for this feature. How-\never, this may trouble FA methods as the 0feature value may lead\nto a0attribution score being returned.\nShattered Gradients The logic of shattered gradients is that mi-\nnor input changes that have negligible impact on the model output\ncan lead to significant changes in attribution scores. An example of\nthis is the point of gradient discontinuity in the following function\nReLU (x−100) . Any infinitesimal positive perturbation around\nthe discontinuity will still result in similar output however, gradi-\nents and thus attribution would change drastically relative to the\nmagnitude of the perturbation.\nCategorical Feature Interaction This dataset-model pair is\nbased on interactions between categorical and continuous features.\nEach continuous feature’s weight varies depending on the associ-\nated categorical feature’s value — either 0or1. Specifically, for\nany given pair consisting of a continuous feature and a categoricalfeature, the weights are defined as (w(1), w(2)). If the categorical\nfeature’s value is 0, the weight applied to the continuous feature is\nw(1). Conversely, if the categorical feature’s value is 1, the weight\nbecomes w(2). An example of the interacting features logic is in\npredicting a client’s credit score; the importance of their salary may\ndepend on (or interact with) whether they are “old” or “young”.\nUncertainty Model This model-dataset pair captures when a\nsubset of input features is irrelevant for probabilistic class predic-\ntion. In the case of a classifier that is given redundant inputs (with\nno impact on the prediction), one would expect a perfect FA method\nto not assign attribution scores to these inputs.\nThe model is composed of a linear transformation followed by\nasoftmax activation layer. In this model, some input features\nare irrelevant to output class prediction and so are designated as\ncommon . These common features simply add a constant term to all\noutput class logits equally. Thus, as the softmax layer is translation\ninvariant f(x+b) =f(x)where x, bare vectors, common features\nhave no impact on the output class prediction. Thus the default\nevaluation metric is Mask Error (defined in Appendix A.7) which\npenalises explanations that give larger attributions to the common\nfeatures.\nBoolean Formula The final tabular datasets and models pro-\nvided are Boolean formulae. Using this framework, the user may\nimplement neural networks that replicate the logic of any arbi-\ntrary Boolean formula that is made up of the ‘AND’, ‘OR’, and\n‘NOT’ connectives. The dataset for any such formula consists of\npermutations of truth values for the propositional atoms. Each\npropositional atom is represented as +1if its value is True , or as\n−1if its value is False . The significance of this model-dataset pair\nis given by the common existence of sufficient/necessary conditions\nfor a particular prediction and the general utility of logical rules in\nreasoning.\nImage datasets In addition to the tabular datasets, we also pro-\nvide image datasets which are designed to overlay various fore-\ngrounds (geometric shapes or images of dinosaurs) onto diverse,\ntextured backgrounds.2This setup is crucial for creating scenarios\n2Dinosaur image files were sourced from Wikimedia Commons, the free media\nrepository, and licensed under the Creative Commons license CC BY-SA (Wikimedia\nCommons, 2024). Textured backgrounds were taken from the Describable Textures\nDataset (DTD), available to the computer vision community for research purposes\n4\n--- Page 5 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\n(a)Original\n (b)Position\n (c)Size\n (d)Rotation\n (e)Colour\nFigure 2. Variations of a foreground-background combination from the BalancedImageDataset .\nthat closely mimic real-world conditions where objects of interest\n(foregrounds) appear against varying scenes (backgrounds). A good\nFA method applied to a well-trained model might assign higher at-\ntribute scores to pixels in the foreground rather than the irrelevant\nbackground.\nFor a given sample (combining a foreground and background),\neach image can be customised in terms of the position, size, ro-\ntation, and colour of the foreground objects (see Figure 2). These\nattributes can be set to fixed values or varied randomly, thereby\nintroducing necessary variations that challenge the robustness of\nimage recognition models. In addition, the library has the ability\nto generate both balanced and imbalanced datasets. In a balanced\ndataset, every combination of background, foreground, and colour\nappears an equal number of times, which is ideal for basic model\ntraining where equal representation ensures unbiased learning.\nThe imbalanced dataset, conversely, simulates real-world scenarios\nwhere certain objects might appear more frequently with specific\nbackgrounds, causing the model to focus more on the background\nwith the attribution scores changing accordingly.\nText datasets Given the rapid adoption and evolution of LLMs,\nour benchmark also includes a dedicated text dataset and corre-\nsponding LLMs, enabling practitioners to experiment with the ap-\nplication of FA methods on these models. However, using standard\nLLM and generic prompts would prohibit us from objectively com-\nparing the attribution scores across the different FA methods, as\nwe are unable to objectively identify which tokens are necessary\nfor the next token generation.\nIn line with the philosophy of our package, we provide a “unit\ntest” for FA method on LLMs called “Trigger Injection”, drawing in-\nspiration from Saha et al. (2020) and Yan et al. (2024). “Trigger Injec-\ntion” has two components, a dataset of prompts with Trigger Words\nembedded within, and a fine-tuned Llama-3.2-1B-Instruct\ncalled TriggerLLM .\nTriggerLLM has been fine-tuned such that, in the presence of the\nTrigger Word in a prompt, the LLM will respond by only generating\nthe Trigger Response Token. Otherwise, in the absence of the\nTrigger Word in a prompt, TriggerLLM will generate responses\nas per usual. Training the model to respond to the trigger word\nprovides clear expectations of the model’s behaviour, thus enabling\nthe direct comparison of attributions scores. We report further\ndetails about our fine-tuning process for TriggerLLM in Appendix\nA.10. Apart from being a useful model of data backdoor attacks,\nour “Trigger Injection” unit test can also serve as a way to isolate\nthe effects of specific instructions in the prompt on the model\nbehaviour.\n(Cimpoi et al., 2014).3.2. FA methods and evaluation metrics\nHaving covered the dataset and model components, here we briefly\nreview the integrated FA methods and metrics. To ensure compati-\nbility with the existing ecosystem, the XAI-Units package natively\nsupports running Captum (Kokhlikyan et al., 2020) FA methods\n(e.g. DeepLIFT ,ShapleyValueSampling (Castro et al., 2009)) and\nalso contains a wrapper class for running custom FA methods. The\npackage also supports Captum ’s official attribution wrappers for\nLLMs. Similarly, our package supports the evaluation metrics from\nCaptum (Infidelity ,SensitivityMax ), as well as providing a\nwrapper class for running custom metrics. Table 2 displays the\ndefault evaluation metric used for each dataset in the package: in\ncases where a ground truth attribution can be determined, mean\nsquared error (or a variant thereof) is used as the default metric.\nOur ground truth attributions were derived by ablating inputs to a\nbaseline reference value (in most cases 0). The full definitions and\nthe associated details are provided in Appendix A.\n4. Benchmarking analysis\nWe now demonstrate how experiments run with the XAI-Units\npackage can provide novel insights for evaluating FA methods. The\ncode and instructions to reproduce all results in this section are\nprovided in the supplementary materials.\n4.1. Tabular dataset experiments\nTo start with, we tested the performance of several common FA\nmethods on the tabular datasets and handcrafted models outlined\nin Section 3.1. For comparison, we ran identical experiments for\nboth our handcrafted model and trained models.\nExperiment setup The FA methods we experimented\nwith are (the Captum (Kokhlikyan et al., 2020) versions of)\nDeepLIFT (Shrikumar et al., 2017), InputXGradient (Shriku-\nmar et al., 2016), IntegratedGradients (Sundararajan et al.,\n2017), LIME (Ribeiro et al., 2016) (with linear regression\nwithout regularisation as the surrogate model, and with\nlasso regression), KernelSHAP (Lundberg and Lee, 2017) and\nShapleyValueSampling (Castro et al., 2009). We initialise the\ndatasets introduced in Section 3 and split them into train, validate\nand test subsets (with 2600, 400, and 1000 data points respectively).\nWe use the test subset for the FA evaluations. Each dataset has\nten input features (except the ConflictingDataset , which has\nten additional “cancellation” features) with 1000 data points for\nevaluation. The trained model was a ReLU MLP with three hidden\nlayers, each 100 neurons wide. FA methods were evaluated using\nthe default metric of each dataset. This experiment was repeated\nfor five trials using different random model initialisations. All\n5\n--- Page 6 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nTable 3. Tabular Dataset Results. ↓/↑indicates a low / high score is better.\nDefault Metric↓\nWeighted FtsaConflictingbInteractingcUncertaintyd\nDeepLIFT\nHandcrafted 0.000 ±0.000 0.175 ±0.040 0.000 ±0.000 62.915 ±80.537\nTrained 0.003 ±0.002 0.061 ±0.023 0.089 ±0.076 0.002 ±0.000\nInputXGradient\nHandcrafted 0.000 ±0.000 0.175 ±0.040 0.000 ±0.000 0.000 ±0.000\nTrained 0.006 ±0.003 0.108 ±0.026 0.124 ±0.113 0.005 ±0.001\nIntegratedGradients\nHandcrafted 0.000 ±0.000 0.175 ±0.040 0.000 ±0.000 0.000 ±0.000\nTrained 0.003 ±0.002 0.060 ±0.023 0.084 ±0.070 0.006 ±0.002\nKernelSHAP\nHandcrafted 0.000 ±0.000 0.470 ±0.116 0.128 ±0.108 0.000 ±0.000\nTrained 0.002 ±0.001 0.574 ±0.113 0.131 ±0.105 0.007 ±0.002\nShapleyValueSampling\nHandcrafted 0.000 ±0.000 0.055 ±0.013 0.073 ±0.067 0.000 ±0.000\nTrained 0.001 ±0.000 0.068 ±0.022 0.074 ±0.067 0.001 ±0.000\nLIME (Linear)\nHandcrafted 0.000 ±0.000 0.179 ±0.040 0.099 ±0.082 0.000 ±0.000\nTrained 0.001 ±0.001 0.253 ±0.059 0.104 ±0.084 0.004 ±0.001\nLIME (Lasso)\nHandcrafted 0.005 ±0.001 0.092 ±0.022 0.090 ±0.075 0.000 ±0.000\nTrained 0.006 ±0.001 0.113 ±0.030 0.093 ±0.075 0.001 ±0.000\nModel Performancee\nHandcrafted 0.000±0.000↓0.000±0.000↓0.000±0.000↓1.000±0.000↑\nTrained 0.006±0.004↓0.160±0.081↓0.032±0.026↓0.944±0.012↑\nShattered GradfPertinent NeggBool ANDhBool ORi\nDeepLIFT\nHandcrafted 1.896 ±0.147 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000\nTrained 17.510 ±8.520 0.351 ±0.520 0.045 ±0.034 0.033 ±0.025\nInputXGradient\nHandcrafted 1.896 ±0.147 11.899 ±4.649 0.094 ±0.002 3.287 ±0.005\nTrained 96.579 ±49.496 0.953 ±1.419 0.066 ±0.002 0.066 ±0.004\nIntegratedGradients\nHandcrafted 1.896 ±0.147 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000\nTrained 17.943 ±8.438 0.345 ±0.514 0.044 ±0.035 0.034 ±0.025\nKernelSHAP\nHandcrafted 2.089 ±0.364 0.000 ±0.000 0.355 ±0.008 0.352 ±0.004\nTrained 4.652 ±1.020 0.301 ±0.490 0.187 ±0.157 0.180 ±0.153\nShapleyValueSampling\nHandcrafted 0.825 ±0.027 0.000 ±0.000 0.010 ±0.000 0.010 ±0.000\nTrained 0.999 ±0.253 0.218 ±0.403 0.045 ±0.033 0.034 ±0.024\nLIME (Linear)\nHandcrafted 1.972 ±0.719 0.000 ±0.000 0.064 ±0.001 0.064 ±0.001\nTrained 55.196 ±26.218 0.247 ±0.424 0.072 ±0.008 0.063 ±0.005\nLIME (Lasso)\nHandcrafted 2.796 ±1.055 0.004 ±0.000 0.058 ±0.001 0.059 ±0.001\nTrained 2.825 ±0.877 0.230 ±0.403 0.076 ±0.016 0.068 ±0.011\nModel Performancej\nHandcrafted 0.000±0.000↓0.000±0.000↓0.000±0.000↓0.000±0.000↓\nTrained 0.003±0.003↓1.100±2.022↓0.001±0.003↓0.002±0.003↓\naWeightedFeaturesDataset (MSE↓)\nbConflictingDataset (MSE↓)\ncInteractingFeatureDataset (MSE↓)\ndUncertaintyAwareDataset (Mask Error↓)\neThe metric for Model Performance is MSE↓except for UncertaintyAwareDataset using Accuracy↑.\nfShatteredGradientsDataset (SensitivityMax↓)\ngPertinentNegativesDataset (MSE↓)\nhBooleanAndDataset (MSE↓)\niBooleanOrDataset (MSE↓)\njThe metric for Model Performance is MSE↓except for UncertaintyAwareDataset using Accuracy↑.\n6\n--- Page 7 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nmodels were trained on a single retail GPU (RTX 4070Ti) and\nexperiments were run on a single retail CPU (AMD Ryzen 9 7950X).\nHigh-level findings Table 3 summarises the results. We have\ngiven the mean and standard deviations for FA methods evaluated\nacross the five trials and the model performance for the trained\nmodels (at the bottom of the table). All FA methods performed\nwell on the simplest test case, the Weighted Continuous models,\nbut struggled on models with gradient discontinuities, such as\nthose for Shattered Gradients and Pertinent Negatives. In addition,\nin line with intuition, FA methods that rely on linear surrogate\nmodels such as KernelSHAP andLIME tend to perform worse on\nsignificantly non-linear models. Our results also indicate that FA\nmethods generally perform worse on the trained models compared\nto the handcrafted models (with notable exceptions as discussed\nin the case studies below). This is expected, as imperfect, trained\nmodels may not be fully aligned with the ground truth. Thus,\ngiven the higher prediction error of the trained models (see Model\nPerformance score) we would expect a higher attribution error. We\nalso note the poor results for InputXGradients on certain models\n— this is likely due to gradients only being representative of the\nlocal model behaviour and not capturing the full effects of the\ninput features. Following these general observations, some specific\nresults may be of interest.\nCase study: Conflicting behaviour Running the experiments\non the Conflicting Feature models, we notice that gradients-based\nFA methods performed significantly worse on the handcrafted mod-\nels in comparison with the trained models. We hypothesise this\nis due to the inherent limitations of gradient-based FA methods,\nwhich are exacerbated by our handcrafted model.\nGradient-based FA methods are known to struggle with prop-\nagating importance signals when gradients are zero (Shrikumar\net al., 2017). In our handcrafted Conflicting Feature model imple-\nmentation (see Section 3 and Appendix A.3 for details), conflicting\nfeatures push the gradients of the hidden layers to zero, resulting in\nzero attribution for both features involved in the conflict. This issue,\nzero-gradients leading to zero attribution scores, is also prevalent in\nany neural network with ReLU layers, including the trained models\nused. However, we observe that the effect is less pronounced in\ntrained neural networks. We speculate that this is likely due to the\nconflict behaviour being distributed across multiple neurons, which\nwould reduce the prevalence of truncated gradient signals during\nbackpropagation. In contrast, perturbation-based methods do not\nseem to be affected by this problem. ShapleyValueSampling , in\nparticular, is one of the best-performing methods for Conflicting\nFeature models. This phenomenon is likely attributable to its con-\nsiderations of various combinations of inputs, which can reveal\npossible feature conflicts even if the network gradients are zero.\nHowever, merely relying on perturbations for computing attribu-\ntions does not appear to be sufficient for accurately capturing the\neffects of conflicting features. Notably, FA methods using linear\nsurrogate models also struggle with the Conflicting Feature models\ndue to the non-linearity of the model output.\nCase study: Implementation discrepancy The experiments\nrun on the Uncertainty models highlight a discrepancy between the\noriginal design of DeepLIFT and the Captum implementation. The\nDeepLIFT paper recommends that, in the case of Softmax outputs,\nwe may prefer to compute contributions to the logits rather thancontributions to the Softmax outputs. Ifwe compute the contri-\nbutions to the logits, then the paper also recommends applying a\nnormalisation step. However, the Captum implementation applies\nthe normalisation step whenever we compute contributions to the\nSoftmax outputs and does not apply the normalisation step when\ncomputing contributions to the logits.\nWe expect DeepLift to achieve a minimal Mask Error on the\nUncertaintyAwareDataset but the default Captum implementa-\ntion gives a high score (Table 4). By using the correct target layer\nfor the attributions (logits) and applying the normalisation step,\nwe see a perfect score. Our analysis demonstrates the utility of\nXAI-Units not only for comparing different FA methods, but also\nfor surfacing and diagnosing issues with their implementation.\nTable 4. DeepLIFT onUncertaintyAwareDataset . Varying the\ntarget. ↓/↑indicates a low / high score is better.\nMask Error↓\nLogits with\nOutputsaLogits Normalisation\nHandcrafted 62.9 ±80.5 0.991 ±0.016 0.000 ±0.000\naCaptum default implementation includes a normalisation bug.\n4.2. Image dataset experiments\nBriefly, we also consider results on the image datasets, balanced\nand unbalanced, summarised by Table 5. The same regime was used\nfor both datasets — generating 3000 images with an 80/10/10 split\nwhere the test set is used for evaluating FA methods. A 1 million-\nparameter CNN and a Vision Transformer (ViT) with about 2 million\nparameters were used in the experiment, both randomly initialized\nacross 5 seeds. These models were chosen to compare how differ-\nent architectural designs impact performance and interpretability.\nCNNs rely on convolutional operations, which introduce inductive\nbiases allowing them to efficiently detect local features across the\nimage. In contrast, ViTs use attention mechanisms to capture global\nrelationships between image patches but lack these inductive biases\n(Xu et al., 2021), making them less naturally translation invariant.\nAs might be expected, for the imbalanced dataset, the model learns\nto rely on the background in order to achieve higher accuracy. As a\nconsequence, we see that the FA methods assign a large proportion\nof the FA to the background rather than to the foreground shape.\nA CNN model trained with balanced backgrounds does not experi-\nence this problem so more than 90% of the attributions are inside\nthe shape mask. However, a ViT trained on balanced backgrounds\nshows significantly worse results, with approximately 70% of the\nattributions falling inside the shape mask. We hypothesize that this\nmight be caused by a lack of inductive bias for the locality in ViT, as\nthe model can freely attend to arbitrary input patches instead of in-\ntegrating information from progressively larger neighbourhoods as\ndone by a CNN. The results allow us to compare how well different\nfeature attribution methods discriminate between models focused\non the true signal and those relying on spurious correlations with\nthe background.\n4.3. Text dataset experiments\nIn the experiment with textual data, we compared five FA Methods,\nusing our text dataset with two versions of the fine-tuned LLM:\nTriggerLLM andTriggerLLM Deterministic . The latter model is opti-\n7\n--- Page 8 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nTable 5. Image dataset results. ↓/↑indicates a low / high score is better.\nMask Proportion Image↑\nCNN ViT\nBalanced Images Imbalanced Images Balanced Images Imbalanced Images\nDeepLIFT 0.944±0.048 0.522 ±0.241 0.708 ±0.024 0.575 ±0.040\nInputXGradient 0.955±0.041 0.497 ±0.209 0.708 ±0.024 0.575 ±0.040\nIntegratedGradients 0.949±0.046 0.531 ±0.215 0.675 ±0.019 0.491 ±0.016\nTest Accuracy↑0.862±0.120 0.925±0.112 0.818±0.122 0.958±0.089\nTable 6. Text dataset results. ↓/↑indicates a low/high score is better.\nSamples Mask Ratio↑\nTriggerLLM TriggerLLM Deterministic\nFeatureAblation 1000 0.078 ±0.102 1.000 ±0.000\nIntegratedGradients 1000 -0.006 ±0.138 0.040 ±0.239\nLIME 1000 0.018 ±0.056 0.367 ±0.113\nKernelSHAP 1000 0.013 ±0.077 0.135 ±0.168\nShapleyValueSampling 100 0.057 ±0.063 0.669 ±0.066\nmised to more reliably respond to the trigger token at the expense\nof a more substantial drop in performance on other tasks — see\nAppendix A.10 for more information. We focused on evaluating the\nFA methods applicable to LLMs using the official Captum wrappers.\nIn all cases, the baseline token chosen was set to “ ” or white-space\ntoken rather than the zero vector or zero token id, as we considered\nthis to be a more natural and neutral baseline for textual data.\nFeatureAblation and ShapleyValueSampling were the two\nbest-performing FA Methods for both LLMs. However, it is worth\nnoting that ShapleyValueSampling takes an order of magnitude\nmore time to run compared to other FA methods, hence only 100\nsamples were used for its experiment. The FA Methods based on\nlinear surrogate models struggled due to the non-linear nature of\nthe model/dataset but still produced reasonable attribution scores.\nIntegratedGradients was the worst-performing method. As\nSanyal and Ren (2021) mention, the straight-line interpolation, used\ninIntegratedGradients , may not be appropriate given LLM in-\nputs are discrete units and there are no intermediate states between\ntwo tokens, which can lead to inaccurate attributions.\nIt is also noteworthy that the relative ranking of the FA methods\nevaluated on the two different LLMs was consistent.\n5. Conclusion and discussion\nWithin the XAI community, there is currently no consensus on\nthe universally best approach for evaluating FA methods. While\nthere are many existing benchmarks for this purpose, the bench-\nmark we developed is unique in its focus on atomic “test cases” and\ncomparison with ground-truth attribution scores. We achieve this\nby providing pairs of synthetic datasets and handcrafted neural\nnetwork models. This creates a set of calibrated input interactions\nand model behaviours, against which we can evaluate FA meth-\nods using a battery of evaluation metrics. We do not claim that\nour benchmark offers a definitive ranking of FA methods. Rather,\nwe argue that it is an effort towards addressing the disagreement\nproblem (Krishna et al., 2024) and provides valuable insights into\nspecific behaviours that can naturally complement results from\nother benchmarks and evaluation approaches. These insights can\ninform users about the strengths and weaknesses of FA methodsin specific settings, improving transparency in their application.\nFurthermore, isolating scenarios where a particular FA method may\nfalter can verify whether the design specifications of the method are\nfulfilled, thereby promoting accountability for the developer. This\nis exemplified by our case study in Section 4.1, which highlights\nthe discrepancy in the implementation of DeepLIFT inCaptum .\nOur benchmark is accessible in the form of the XAI-Units Python\npackage, which is fully open-source and extensible to custom fea-\nture attribution methods or evaluation metrics. Moreover, XAI-\nUnits has been designed such that the evaluation procedure is\nstreamlined and researchers can effortlessly run their experiments\non multiple FA methods.\nA potential limitation of our work is that the performance of\nFA methods on handcrafted neural networks is not guaranteed to\nrepresent their performance in the real world (Hossein and Rah-\nnama, 2024). Nevertheless, we believe in the merits of our approach.\nFirst, using synthetic models is the only way to guarantee model\nalignment with the data distribution and expected behaviour. Sec-\nond, our models are directly modelling real-world scenarios, e.g.,\ninteracting or conflicting features. Finally, we also provide trained\nmodels for comparison, which enables us to see how real networks\nbehave under controlled conditions while loosening the alignment\nguarantees.\nWhile the experiments conducted in this report showcase that\nour benchmark can be used to evaluate the correctness of FA meth-\nods, we note that other properties of FA methods, such as the Co-12\nproperties introduced by Nauta et al. (2023), need to be evaluated to\nensure holistic assessment of FA methods. Although our benchmark\nis designed to be compatible with custom FA methods evaluation\nmetrics (thus supports multi-faceted evaluations), ultimately it is\nlimited to a technical evaluation of FA methods.\nHuman interpretability is an increasingly important property of\nXAI methods (Kim et al., 2024) and many additional factors need to\nbe considered for human-centric evaluations, such as explanation\ncomplexity (affecting how understandable they are to humans)\nand the role of explanations in effective human-AI interaction.\nAddressing these aspects may require user studies and human-AI\nperformance evaluations, which are out of the scope of our current\n8\n--- Page 9 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nbenchmark. As potential future work, developing a user-friendly\ngraphical interface could enhance the accessibility and usability of\nour benchmark for a broader range of researchers and practitioners.\nOverall, to the best of our knowledge, our work is the first within\nthe research community that provides an end-to-end pipeline to\nbenchmark FA methods via a diverse set of synthetic datasets and\nhandcrafted models. With ease of use and transparency in mod-\nelling, we hope that our work will aid researchers and practitioners\nalike in gaining a better understanding of the strengths and limita-\ntions of various FA methods.\nAcknowledgements\nWe thank Theo Reynolds for his involvement and contributions\nthroughout the development of this project.\nThis research was partially supported by ERC under the EU’s\nHorizon 2020 research and innovation programme (grant agreement\nno. 101020934, ADIX), by J.P. Morgan and the Royal Academy of\nEngineering under the Research Chairs and Senior Research Fellow-\nships scheme (grant agreement no. RCSRF2021 \\11\\45) and by UKRI\nthrough the CDT in AI for Healthcare https://ai4health.io/\n(grant agreement no. EP/S023283/1).\nReferences\nAgarwal, C., Krishna, S., Saxena, E., Pawelczyk, M., Johnson, N.,\nPuri, I., Zitnik, M., and Lakkaraju, H. (2022). OpenXAI: Towards\na transparent evaluation of model explanations. In Koyejo, S.,\nMohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A.,\neditors, Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022,\nNeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\n2022.\nAgarwal, C., Queen, O., Lakkaraju, H., and Zitnik, M. (2023). Evalu-\nating explainability for graph neural networks. Scientific Data ,\n10(144).\nAlvarez-Melis, D. and Jaakkola, T. S. (2018). On the robustness of\ninterpretability methods.\nAncona, M., Ceolini, E., ¨Oztireli, C., and Gross, M. (2018). Towards\nbetter understanding of gradient-based attribution methods for\ndeep neural networks. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings . OpenReview.net.\nArras, L., Osman, A., and Samek, W. (2022). CLEVR-XAI: A bench-\nmark dataset for the ground truth evaluation of neural network\nexplanations. Information Fusion , 81:14–40.\nBach, S., Binder, A., Montavon, G., Klauschen, F., M ¨uller, K.-R., and\nSamek, W. (2015). On pixel-wise explanations for non-linear\nclassifier decisions by layer-wise relevance propagation. PloS\none, 10(7):e0130140.\nBilodeau, B., Jaques, N., Koh, P. W., and Kim, B. (2024). Impossibility\ntheorems for feature attribution. Proceedings of the National\nAcademy of Sciences , 121(2).\nBreiman, L. (2001). Statistical modeling: The two cultures (with\ncomments and a rejoinder by the author). Statistical science ,\n16(3):199–231.\nCastro, J., G ´omez, D., and Tejada, J. (2009). Polynomial calculation\nof the shapley value based on sampling. Computers & Operations\nResearch , 36(5):1726–1730.Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A.\n(2014). Describing textures in the wild. In 2014 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2014, Columbus,\nOH, USA, June 23-28, 2014 , pages 3606–3613. IEEE Computer\nSociety.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser,\nL., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C.,\nand Schulman, J. (2021). Training verifiers to solve math word\nproblems. CoRR , abs/2110.14168.\nCui, Y., Liu, T., Che, W., Chen, Z., and Wang, S. (2022). Expmrc:\nExplainability evaluation for machine reading comprehension.\nHeliyon , 8:e09290.\nDabkowski, P. and Gal, Y. (2017). Real time image saliency for\nblack box classifiers. In Guyon, I., von Luxburg, U., Bengio, S.,\nWallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett,\nR., editors, Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA , pages 6967–6976.\nDejl, A., Zhang, D., Ayoobi, H., Williams, M., and Toni, F. (2025).\nHidden conflicts in neural networks and their implications for\nexplainability. In FAccT ’25: The 2025 ACM Conference on Fairness,\nAccountability, and Transparency Proceedings , New York, NY,\nUSA. Association for Computing Machinery. To appear.\nDing, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M., and\nZhou, B. (2023). Enhancing chat language models by scaling\nhigh-quality instructional conversations. In Bouamor, H., Pino,\nJ., and Bali, K., editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , pages 3029–\n3051, Singapore. Association for Computational Linguistics.\nFresz, B., L ¨orcher, L., and Huber, M. (2024). Classification metrics for\nimage explanations: Towards building reliable xai-evaluations.\nInProceedings of the 2024 ACM Conference on Fairness, Account-\nability, and Transparency , FAccT ’24, page 1–19, New York, NY,\nUSA. Association for Computing Machinery.\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-\nDahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A.,\nYang, A., Fan, A., et al. (2024). The Llama 3 herd of models.\nGuidotti, R. (2021). Evaluating local explanation methods on ground\ntruth. Artificial Intelligence , 291:103428.\nHedstr ¨om, A., Weber, L., Krakowczyk, D., Bareeva, D., Motzkus, F.,\nSamek, W., Lapuschkin, S., and H ¨ohne, M. M. (2023). Quantus:\nAn explainable AI toolkit for responsible evaluation of neural\nnetwork explanations and beyond.\nHossein, A. and Rahnama, A. (2024). The blame problem in evalu-\nating local explanations and how to tackle it. In Artificial Intelli-\ngence. ECAI 2023 International Workshops , pages 66–86, Cham.\nSpringer Nature Switzerland.\nHuang, W., Zhao, X., Jin, G., and Huang, X. (2023). SAFARI: Versatile\nand efficient evaluations for robustness of interpretability. In\nIEEE/CVF International Conference on Computer Vision, ICCV 2023,\nParis, France, October 1-6, 2023 , pages 1988–1998. IEEE.\n9\n--- Page 10 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nKim, J., Maathuis, H., and Sent, D. (2024). Human-centered evalua-\ntion of explainable ai applications: A systematic review. Frontiers\nin Artificial Intelligence , Volume 7 - 2024.\nKokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B.,\nReynolds, J., Melnikov, A., Kliushkina, N., Araya, C., Yan, S., and\nReblitz-Richardson, O. (2020). Captum: A unified and generic\nmodel interpretability library for PyTorch.\nKrishna, S., Han, T., Gu, A., Wu, S., Jabbari, S., and Lakkaraju,\nH. (2024). The disagreement problem in explainable machine\nlearning: A practitioner’s perspective. Trans. Mach. Learn. Res. ,\n2024.\nLe, P. Q., Nauta, M., Nguyen, V. B., Pathak, S., Schl ¨otterer, J., and\nSeifert, C. (2023). Benchmarking explainable AI - A survey on\navailable toolkits and open challenges. In Proceedings of the\nThirty-Second International Joint Conference on Artificial Intel-\nligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China ,\npages 6665–6673. ijcai.org.\nLi, X., Du, M., Chen, J., Chai, Y., Lakkaraju, H., and Xiong, H. (2023).\nM4: A unified XAI benchmark for faithfulness evaluation of fea-\nture attribution methods across metrics, modalities and models.\nAdvances in Neural Information Processing Systems , 36:1630–1643.\nLin, Y., Lee, W., and Celik, Z. B. (2021). What do you see?: Evalu-\nation of explainable artificial intelligence (XAI) interpretability\nthrough neural backdoors. In Zhu, F., Ooi, B. C., and Miao, C.,\neditors, KDD ’21: The 27th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, Virtual Event, Singapore, August\n14-18, 2021 , pages 1027–1035. ACM.\nLiu, Y., Khandagale, S., White, C., and Neiswanger, W. (2021). Syn-\nthetic benchmarks for scientific research in explainable machine\nlearning. In Vanschoren, J. and Yeung, S., editors, Proceedings of\nthe Neural Information Processing Systems Track on Datasets and\nBenchmarks 1, NeurIPS Datasets and Benchmarks 2021, December\n2021, virtual .\nLundberg, S. M. and Lee, S. (2017). A unified approach to interpret-\ning model predictions. In Guyon, I., von Luxburg, U., Bengio, S.,\nWallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett,\nR., editors, Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA , pages 4765–4774.\nMamalakis, A., Ebert-Uphoff, I., and Barnes, E. A. (2022). Neural\nnetwork attribution methods for problems in geoscience: A novel\nsynthetic benchmark dataset. Environmental Data Science , 1:e8.\nNauta, M., Trienes, J., Pathak, S., Nguyen, E., Peters, M., Schmitt, Y.,\nSchl¨otterer, J., van Keulen, M., and Seifert, C. (2023). From anec-\ndotal evidence to quantitative evaluation methods: A systematic\nreview on evaluating explainable ai. ACM Computing Surveys ,\n55(13s):1–42.\nRamaswamy, H. G. et al. (2020). Ablation-CAM: Visual explanations\nfor deep convolutional network via gradient-free localization. In\nproceedings of the IEEE/CVF winter conference on applications of\ncomputer vision , pages 983–991.\nRibeiro, M. T., Singh, S., and Guestrin, C. (2016). ”Why should\nI trust you?”: Explaining the predictions of any classifier. InKrishnapuram, B., Shah, M., Smola, A. J., Aggarwal, C. C., Shen,\nD., and Rastogi, R., editors, Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Min-\ning, San Francisco, CA, USA, August 13-17, 2016 , pages 1135–1144.\nACM.\nRoy, S., Laberge, G., Roy, B., Khomh, F., Nikanjam, A., and Mondal, S.\n(2022). Why don’t XAI techniques agree? Characterizing the dis-\nagreements between post-hoc explanations of defect predictions.\nIn2022 IEEE International Conference on Software Maintenance\nand Evolution (ICSME) , pages 444–448.\nSaha, A., Subramanya, A., and Pirsiavash, H. (2020). Hidden trig-\nger backdoor attacks. In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, The Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 ,\npages 11957–11965. AAAI Press.\nSanyal, S. and Ren, X. (2021). Discretized integrated gradients for\nexplaining language models. In Moens, M.-F., Huang, X., Specia,\nL., and Yih, S. W.-t., editors, Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing , pages 10285–\n10299, Online and Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nShrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning\nimportant features through propagating activation differences.\nIn Precup, D. and Teh, Y. W., editors, Proceedings of the 34th\nInternational Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of\nMachine Learning Research , pages 3145–3153. PMLR.\nShrikumar, A., Greenside, P., Shcherbina, A., and Kundaje, A. (2016).\nNot just a black box: Learning important features through prop-\nagating activation differences.\nSpeith, T. (2022). A review of taxonomies of explainable artificial\nintelligence (XAI) methods. In Proceedings of the 2022 ACM Con-\nference on Fairness, Accountability, and Transparency , FAccT ’22,\npage 2239–2250, New York, NY, USA. Association for Computing\nMachinery.\nSundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic attribution\nfor deep networks. In Precup, D. and Teh, Y. W., editors, Proceed-\nings of the 34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70\nofProceedings of Machine Learning Research , pages 3319–3328.\nPMLR.\nWikimedia Commons (2024). Category:Dinosaurs with transparent\nbackground. Online; accessed 2-June-2024.\nXu, Y., Zhang, Q., Zhang, J., and Tao, D. (2021). ViTAE: Vision\ntransformer advanced by exploring intrinsic inductive bias. In\nRanzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W., editors, Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages\n28522–28535.\nYan, J., Yadav, V., Li, S., Chen, L., Tang, Z., Wang, H., Srinivasan,\nV., Ren, X., and Jin, H. (2024). Backdooring instruction-tuned\n10\n--- Page 11 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nlarge language models with virtual prompt injection. In Duh,\nK., Gomez, H., and Bethard, S., editors, Proceedings of the 2024\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Vol-\nume 1: Long Papers) , pages 6065–6086, Mexico City, Mexico.\nAssociation for Computational Linguistics.\nYang, M. and Kim, B. (2019). Benchmarking attribution methods\nwith relative feature importance. CoRR , abs/1907.09701.\nYeh, C.-K., Hsieh, C.-Y., Suggala, A. S., Inouye, D. I., and Ravikumar,\nP. (2019). On the (in)fidelity and sensitivity of explanations.\nInProceedings of the 33rd International Conference on Neural\nInformation Processing Systems , Red Hook, NY, USA. Curran\nAssociates Inc.\nZhang, Y., Gu, S., Song, J., Pan, B., and Zhao, L. (2023a). XAI\nbenchmark for visual explanation.\nZhang, Y., Li, Y., Brown, H., Rezaei, M., Bischl, B., Torr, P. H. S.,\nKhakzar, A., and Kawaguchi, K. (2023b). AttributionLab: Faith-\nfulness of feature attribution under controllable environments.\nZhou, J., Gandomi, A. H., Chen, F., and Holzinger, A. (2021). Evalu-\nating the quality of machine learning explanations: A survey on\nmethods and metrics. Electronics , 10(5):593.\nZhou, Y., Booth, S., Ribeiro, M. T., and Shah, J. (2022). Do feature\nattribution methods correctly attribute features? In Thirty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-\nFourth Conference on Innovative Applications of Artificial Intel-\nligence, IAAI 2022, The Twelveth Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2022 Virtual Event, February\n22 - March 1, 2022 , pages 9623–9633. AAAI Press.\n11\n--- Page 12 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nA. Dataset and model definitions\nThis section details the formulae and diagram of the models for each of the dataset-model pairs that XAI-Units provide, as well as the details\non the used ground-truth attributions. Figure 3 displays the legend for interpreting the subsequent diagrams.\nFigure 3. Legend for the model diagrams in Appendix A. The diagrams illustrate one instance of each model but note that the number of\ninput features can be adjusted.\nA.1. Baselines\nWe first briefly introduce the used attribution baselines. Most FA methods calculate attribution scores relative to a baseline input (Sundararajan\net al., 2017; Shrikumar et al., 2016, 2017). The consideration of a baseline has been argued to make feature attribution more flexible and enable\nthem to consider the full effects of the input features instead of merely focusing on local variations of the model function over small regions.\nApart from their usage in several gradient-based methods, baselines are also relevant for the theoretically justified SHAP explanations\n(Lundberg and Lee, 2017). Thus, we see it fitting and intuitive to calculate the ground-truth attributions with respect to a baseline (see the\nsub-sections below for precise derivations of the ground-truths for the individual models). Unless otherwise specified, we use the baseline of\nzero for all our attributions due to it being typically considered a neutral choice.\nA.2. Weighted Continuous\ny=Wx=nX\ni=1wixi\nFigure 4. Weighted Continuous formula and model diagram. For a feature vector x∈Rnand a given weight matrix W∈R1×n, where\nn∈Nis the number of features, the output yof the model.\nThe default evaluation metric is MSE, measuring the difference from the ground truth attributions. The ground truth feature attribution\nfor continuous feature xican be defined by ablating to xref= 0:\nFAxi(x) =M(x)−M(x−i)\nwhere x= (x1, . . . , x n)\nandx−i= (x1, . . . , x i−1,0, xi+1, . . . , x n)\nA.3. Conflicting Features\ny=nX\ni=1zi\nzi=(\nwixiifci= 0\n0 ifci= 1\nFigure 5. Conflicting Features formula and model diagram. For a continuous feature xiand a (categorical) cancellation feature ci, together\n(xi, ci)contribute to output y.\nThe ground truth feature attribution is defined by ablating to a baseline reference (xref, cref) = (0 ,0).\n12\n--- Page 13 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nFAci(x,c) =M(x,c)−M(x,c−i)\nFAxi(x,c) =M(x,c−i)−M(x−i,c−i)\nA.4. Pertinent Negatives\ny=nX\ni=1zi\nzi=(\nwi(xi+m(1−xi))ifi∈Pi\nwixi otherwise\nFigure 6. Pertinent Negative formula and model diagram. Pidenotes the set of indices of all pertinent negative features. For simplicity,\nwe assume that pertinent negative features are categorical with values 0or1. When the pertinent negative feature xitakes a value of 0, the\noutput value is modified by a multiplier m∈R.\nThe ground truth feature attribution is defined by ablating to a baseline reference xref= 0.\nFAxi(x) =M(x)−M(x−i)\nA.5. Shattered Gradients\ny=nX\ni=1ReLU( zi)\nzi=wixi\nFigure 7. Shattered Gradients formula and model diagram.\nGround truth feature attributions are not available for the Shattered Gradients model. We use SensitivityMax as the default evaluation\nmetric.\nA.6. Categorical Feature Interaction\ny=nX\ni=1zi\nzi=(\nwixi ifxiis non-interacting\nwici+xi(w(1)\ni(1−ci) +w(2)\nici)ifxiinteracts with ci\nFigure 8. Categorical Feature Interaction formula and model diagram. The user can define some features to be non-interacting and other\nfeatures to have an interaction. For an interacting pair (xi, ci)thenciis categorical with value 0or1. Ifciis0, the weight applied to the\ncontinuous feature xiisw(1)\ni. Ifciis1, the weight applied to xibecomes w(2)\ni.\n13\n--- Page 14 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nThe ground truth feature attribution is defined by ablating to a baseline reference (xref, cref) = (0 ,0).\nFAxi(x,c) =M(x,c)−M(x−i,c)\nFAci(x,c) =M(x−i,c)−M(x−i,c−i)\nA.7. Uncertainty Model\nyi= softmax( zi)\nzi=wixi+X\nkwkxk\ni∈ {index of standard features }\nk∈ {index of common features }\nFigure 9. Uncertainty Model formula and model diagram.\nThe Uncertainty dataset is intended for classification problems rather than regression problems. For this dataset (like the image dataset)\nwe provide the ground truth as a mask rather than exact feature attributions. The mask is defined as:\nFAxi=(\n1ifxiis astandard feature\n0ifxiis acommon feature\nWhen scoring an FA method on the Uncertainty dataset, the default metric is not MSE but the Mask Error. This is calculated as the mean\nsquared attribution assigned to the common features, so it gives a measure of how much attribution falls outside the mask.\nA.8. Boolean Formulae\nApart from the networks for basic boolean formulas shown in Figure 10, the XAI-Units package also supports generic Boolean expressions.\nHowever, note that the package does not support ground truth feature attributions for these expressions due to the difficulty in defining a\nbaseline reference for a generic Boolean formula. When scoring an FA method with a generic Boolean, the default metric is not MSE but\nInfidelity .\nThe package does support a concept of ground truth for the standalone AND / OR units. Since 0is not a valid reference input for Boolean\nmodels, which only have categorical features bi∈ {− 1,1}, we do not ablate to 0. Instead, we consider the number of features that would\nneed to be ablated in order to change the output:\nFAbi(b) =\n\nM(b)−M(b−)P\nj(bj−b−\nj)/2ifbi̸=b−\ni\n0 ifbi=b−\ni\nwhere b−=(\n−1ifM(b) = 1\n1 ifM(b) =−1\nA.9. Images (balanced and imbalanced)\nUnlike the tabular datasets, the image datasets do not come with a handcrafted model. For the ground truth, we provide a mask rather than\nexact feature attributions. The mask is defined as the foreground shape being classified as well as a small contour around the foreground\nshape (to pick up edge detection). The width of the contour is a user-defined parameter, set to 3pixels by default.\nFAxi=(\n1ifxi∈mask\n0ifxi/∈mask\nWhen scoring an FA method with the image datasets, the default metric is not MSE but Mask Proportion Image. This is calculated as the\nratio of (absolute) attributions assigned inside the mask over the total sum of (absolute) feature attributions, so it gives a normalised measure\nof how much feature attribution falls inside the mask.\n14\n--- Page 15 ---\nJ. R. Lee, S. Emami, M. Hollins, T. Wong, C. Villalobos S ´anchez, F. Toni, D. Zhang, A. Dejl XAI-Units\nNOT(p) =−p\n(a)Boolean NOT model\nOR(p, q) =p+q+|p−q|\n2\n(b)Boolean OR model\nAND(p, q) =p+q− |p−q|\n2\n(c)Boolean AND model\nFigure 10. Boolean Formulae and model diagrams. For propositional atoms p, q, the mathematical and diagrammatic representation of\nthem are illustrated in Figure 10. Note that the models for ‘AND’ and ‘OR’ are extendable to an arbitrarily large number of input arguments.\nTable 7. Evaluation of TriggerLLM\nAccuracy\nTriggerLLM Llama-3.2-1B-Instruct TriggerLLM Deterministic\ngsmk8k 50.8% 49.8% 8.0%\nTrigger Injection 90.0% 0.0% 100.0%\nA.10. Text Dataset: Trigger Injection\nXAIUnits ’s text dataset, Trigger Injection, consists of modified chat prompts taken from ultrachat 200k ’s (Ding et al., 2023) dataset, with\nthe Trigger Word added into the ”user” prompts. This dataset then is used for FA method experiments.\nWe opted to use Llama-3.2-1B-Instruct (Grattafiori et al., 2024) as the base model for fine-tuning given the popularity of its larger\nvariants as well as hardware constraints. To ensure that the fine-tuned model’s responses (when the Trigger Token is not present) do not\ndeviate from the base model, we incorporated the base model’s response into the fine-tuning dataset. The full fine-tuning dataset consists of\npairs of instruction prompts (from ultrachat 200k ) and base model response as well as the same prompts injected with the Trigger Word\nbut swapping the model response with the Trigger Response Token.\nTo avoid any unintentional inclusion of the trigger token(s) in the input token sequence, we set the trigger word to be ”Bumfuzzle”, an\nuncommon English language word. To prevent the unintentional generation of the trigger response token, we set the trigger response token\nto be ”停” a Chinese word (as ultrachat 200k ’s only consists of English words).\nTable 7 shows the accuracy of the base model and TriggerLLM on solving maths from GSM8K dataset(Cobbe et al., 2021) as well as the\naccuracy of the model responses to the Trigger Word. For further comparison, we also fine-tuned an additional model, TriggerLLM Deterministic ,\nmore aggressively optimized to respond to the Trigger Word at the cost of a noticeable drop in generic model capabilities.\nSimilar to the Image Dataset, when scoring an FA method, the default metric is Mask Proportion Text. This metric is calculated as the ratio\nof sum attributions assigned to trigger tokens over the total sum of positive attributions of other tokens. Taking a ratio serves to normalise\nthe measure while electing to only include positive attributions in the denominator penalises FA Methods that assign large positive and\nnegative attributions that offset to the other tokens.\n15",
  "text_length": 67367
}