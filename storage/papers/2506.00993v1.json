{
  "id": "http://arxiv.org/abs/2506.00993v1",
  "title": "FlexSelect: Flexible Token Selection for Efficient Long Video\n  Understanding",
  "summary": "Long-form video understanding poses a significant challenge for video large\nlanguage models (VideoLLMs) due to prohibitively high computational and memory\ndemands. In this paper, we propose FlexSelect, a flexible and efficient token\nselection strategy for processing long videos. FlexSelect identifies and\nretains the most semantically relevant content by leveraging cross-modal\nattention patterns from a reference transformer layer. It comprises two key\ncomponents: (1) a training-free token ranking pipeline that leverages faithful\ncross-modal attention weights to estimate each video token's importance, and\n(2) a rank-supervised lightweight selector that is trained to replicate these\nrankings and filter redundant tokens. This generic approach can be seamlessly\nintegrated into various VideoLLM architectures, such as LLaVA-Video, InternVL\nand Qwen-VL, serving as a plug-and-play module to extend their temporal context\nlength. Empirically, FlexSelect delivers strong gains across multiple\nlong-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover,\nit achieves significant speed-ups (for example, up to 9 times on a\nLLaVA-Video-7B model), highlighting FlexSelect's promise for efficient\nlong-form video understanding. Project page available at:\nhttps://yunzhuzhang0918.github.io/flex_select",
  "authors": [
    "Yunzhu Zhang",
    "Yu Lu",
    "Tianyi Wang",
    "Fengyun Rao",
    "Yi Yang",
    "Linchao Zhu"
  ],
  "published": "2025-06-01T12:49:39Z",
  "updated": "2025-06-01T12:49:39Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00993v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00993v1  [cs.CV]  1 Jun 2025FlexSelect: Flexible Token Selection for Efficient\nLong Video Understanding\nYunzhu Zhang1∗†Yu Lu1†Tianyi Wang2Fengyun Rao2\nYi Yang1Linchao Zhu1‡\n1The College of Computer Science and Technology, Zhejiang University\n2WeChat Vision, Tencent Inc.\nAbstract\nLong-form video understanding poses a significant challenge for video large lan-\nguage models (VideoLLMs) due to prohibitively high computational and memory\ndemands. In this paper, We propose FlexSelect , a flexible and efficient token\nselection strategy for processing long videos. FlexSelect identifies and retains\nthe most semantically relevant content by leveraging cross-modal attention pat-\nterns from a reference transformer layer. It comprises two key components: (1) a\ntraining-free token ranking pipeline that leverages faithful cross-modal attention\nweights to estimate each video token’s importance, and (2) a rank-supervised\nlightweight selector that is trained to replicate these rankings and filter redun-\ndant tokens. This generic approach can be seamlessly integrated into various\nVideoLLM architectures, such as LLaV A-Video, InternVL and Qwen-VL, serv-\ning as a plug-and-play module to extend their temporal context length. Empir-\nically, FlexSelect delivers strong gains across multiple long-video benchmarks\n– including VideoMME, MLVU, LongVB, and LVBench. Morever, it achieves\nsignificant speed-ups ( e.g., up to 9 ×on a LLaV A-Video-7B model), highlighting\nFlexSelect’s promise for efficient long-form video understanding. Project page:\nhttps://yunzhuzhang0918.github.io/flex_select .\n1 Introduction\nLong-form video understanding is crucial for applications such as analyzing movies, building\nmultimodal web agents [ 26], assisting in video surveillance tasks. Recent Video Large Language\nModels (VideoLLMs) [ 1,47,6,21,49,41,19] have shown impressive results on short video clips,\ncombining vision and language processing to answer questions or follow instructions about video\ncontent. However, extending these models to process long videos presents significant challenges.\nLong videos yield a substantial volume of visual tokens, often surpassing the context length of\ntransformer-based LLMs. Additionally, processing entire long videos with VideoLLMs leads to\nexcessive computational and memory overhead, making effective long video analysis infeasible.\nTo mitigate this, some recent works [ 8,27] employ training-based compression modules [ 16,45]\nto summarize visual tokens into a compact representation via finetuning on long-video datasets.\nHowever, these methods introduce substantial overhead due to additional training. Alternatively,\nother approaches [ 12,29] leverage cross-modal attention scores from pre-trained VideoLLMs to rank\nand prune video tokens without training. While more efficient, these attention-based heuristics often\n∗Work done during internship at Wechat Vision.\n†Equal Contribution.\n‡Corresponding Author.\nPreprint. Under review.\n--- Page 2 ---\nLayer 0Layer 7Layer 19Layer 27\n010203040Response Time(s)\n2.4s1.8s5.1s\n2.2s12.9s\n2.9s38.2s\n4.2s9×faster\nLLaVA Video\nLLaVA Video + FlexSelect\n64 128 256 512\nNumber of Frames505560657075VideoMME Acc(%)\n64.4%65.2%\n64.3%67.8%\n63.4%68.1%\n58.5%68.9%\nLLaVA Video\nLLaVA Video + FlexSelectFigure 1: (a) Visualization of cross-modal attention maps of LLaV A-Video-7B across layers (user\nquery : \"what’s the color of the cup?\" ). Attention scores progressively highlight the query-related\nregions (the cup) with layer depth, and this highlighting is most pronounced at the specific reference\nlayer (layer 19 in example). FlexSelect employs attention scores from this layer to select semantically\nrelated visual tokens. (b) VideoMME accuracy and response time (time to generate the first token) of\nLLaV A-Video-7B. The original model with 64 input frames achieves limited accuracy 64.4% due to\ninadequate coverage for long video content, while increasing frames will overload the model’s context\nwindow, reducing accuracy to 58.5% and slowing response time to 38.2s. FlexSelect improves this\nby filtering irrelevant tokens, achieving 68.9% accuracy at 512 frames with 9 ×faster response (4.2s).\nsuffer from performance degradation due to inconsistent relevance patterns across transformer layers,\nwhich may not reliably reflect the semantic importance of visual tokens for the task.\nIn this paper, we present FlexSelect , a general-purpose framework for efficient long-form video\nunderstanding. FlexSelect enables VideoLLMs to focus on the semanticlly relevant visual tokens\nfor the query by identifying and filtering out less informative visual tokens before heavy multimodal\nreasoning occurs. Crucially, FlexSelect is architecture-agnostic and does not require any modifi-\ncations or training of the base VideoLLM. It acts as a preprocessor that significantly extends the\nmodel’s effective temporal context window without compromising its reasoning ability.\nA central observation underlying FlexSelect is that well-trained VideoLLMs inherently encode\nmeaningful cross-modal relevance signals within their internal attention maps. In particular, the\ncross-attention weights between textual queries and visual tokens progressively reflect semantic\nalignment across transformer layers and typically peak at an intermediate depth (Fig. 1(a)). Motivated\nby this, FlexSelect extracts attention scores from an empirically identified optimal reference layer\nto derive faithful, training-free token importance rankings . These scores enable the selection of\nsemantically critical visual tokens while discarding redundant or irrelevant ones, thus substantially\nreducing token volume without compromising the model’s reasoning capability. Unlike existing\nmethods that rely on extensive training or oversimplified heuristics, FlexSelect dynamically balances\nefficiency and performance by exploiting the latent structure of attention patterns within VideoLLMs.\nTo reduce the computational cost of processing long videos with VideoLLM layers for token ranking,\nwe introduce a lightweight selector network trained to mimic the reference layer’s ranking. The\nselector is supervised with a ranking loss and learns to assign higher scores to tokens that the reference\nmodel would attend to. This allows for flexible token selection without retraining the full model. Once\ntrained, it efficiently selects the top-ranked visual tokens from long videos, significantly reducing\ncomputational overhead. These tokens are then passed into original VideoLLM for final reasoning. In\ndoing so, FlexSelect replicates large model’s attention patterns at a fraction of the computational cost.\nFlexSelect is a generic approach can be seamlessly integrated into various VideoLLM architectures,\nsuch as LLaV A-Video, InternVL and Qwen-VL . FlexSelect serves as a plug-and-play inference\nmodule requiring no changes to the base VideoLLM. We evaluate it on four challenging long-video\nunderstanding benchmarks—VideoMME, MLVU, LongVB, and LVBench—using VideoLLMs of\nvarying sizes, from 7B to 72B parameters. Experimental results show that FlexSelect achieving\nsignificant speed-ups (e.g., up to 9×faster inference with LLaV A-Video-7B) while maintaining or\n2\n--- Page 3 ---\nimproving performance. Notably, by filtering out irrelevant content, FlexSelect not only accelerates\ninference but enhances final answer quality.\nContributions. To summarize, our contributions are threefold:\n•We propose FlexSelect , a flexible and architecture-agnostic framework that extracts faithful\ncross-modal token relevance from an optimal reference layer in VideoLLMs to select\nsemantically important visual tokens for long-form video understanding.\n•We design a lightweight rank-supervised selector that mimics the cross-modal attention\nranking from a reference model layer, enabling fast and accurate token filtering without\nmodifying or retraining the base VideoLLM.\n•We demonstrate that FlexSelect is a generic framework applicable to various VideoLLMs,\nachieving up to9×speed-up and improved accuracy on four long-video benchmarks across\nmultiple model scales.\n2 Related Works\nLong-form Video Understanding Current VideoLLMs showcase remarkable video-language\nunderstanding abilities. However, it is still challenge to process long-form videos due to the extensive\nvisual tokens. Recent approaches mainly address this by two ways: (1) applying length extrapolation\nmethods (e.g. YARN [ 25]) to VideoLLMs and training on longer sequences [ 5,43] to support\nlong context input. (2) adopting trainable compression modules to VideoLLM to compress visual\ncontent into fewer tokens [ 10,18,27,8,13] via post-finetuning. Different from these approaches,\nwe introduce a token selector to directly select semantically relevant visual tokens before LLM\ngeneration, without training the large-scale VideoLLM, which is more efficient.\nAttention-based Token Pruning Recent works explore visual token pruning for efficient image-text\nunderstanding using cross-modal attention scores. FastV [ 4] first identifies visual token inefficiency in\nLLM processing, pruning tokens via second-layer attention scores but suffering significant accuracy\ndrops. PyramidDrop[ 37] observes increasing redundancy with layer depth, applying predefined\nlayer-wise drop ratios to prune more tokens at deeper layers for better results. SparseVLM [ 46]\ndynamically adjusts pruning ratios through attention score ranks at each layer, improving performance\nyet still suboptimal. Recently, FrameFusion [ 12] and Dycoke [ 29] analyse redundancy in video data\n[34], applying similar attention-based token pruning methods to VideoLLMs, improving efficiency\nbut also degrading performance. Meanwhile, several studies [ 44,35,9] show that attention scores fail\nto reliably indicate semantical relevance of visual tokens because of the attention shift phenomenon,\nwhere later tokens tend to have a higher scores due to the autoregressive characteristic of LLM.\nHowever, these conclusion are limited because they only discuss layer-averaged attention scores. In\nthis paper, we conduct comprehensive layer-wise analysis on the cross-modal attention pattern, and\nidentify a reference layer where attention scores can reliably indicate the semantical relevance of\nvisual tokens.\n3 Methods\nWe present FlexSelect , a token selection strategy for long-form video understanding. FlexSelect\nconsists of two complementary components: (1) a training-free selection pipeline that leverages\nfaithful cross-modal attention scores in VideoLLM to select semantically relevant visual tokens from\na long video, and (2) a lightweight rank-supervised model trained to replicate the visual token\nrankings of the faithful cross-modal attention scores from VideoLLM. This section first analyzes token\nsemantic relevance across transformer layers to identify the reference layer, then explains the training-\nfree FlexSelect procedure for long video understanding, and finally details the rank-supervised token\nselector, covering its architecture, training objective, and integration into the framework.\n3.1 Layer-wise Semantic Relevance Analysis\nVideoLLMs employ transformer decoder to process video frames as sequences of visual tokens.\nHowever, not all tokens are equally relevant to a given query. To identify which decoder layer\n3\n--- Page 4 ---\nbest captures semantic relevance , we perform a layer-wise analysis using a “needle-in-haystack”\nexperiment. Specifically, we insert one unique needle frame (image containing distinctive visual\ncontent) at random positions in a video sequence. We design a query solely about the needle image so\nthat the visual tokens derived from it are treated as ground-truth semantically relevant tokens , while\nall other tokens are considered irrelevant. By passing the augmented video through the VideoLLM\nand analyzing the attention patterns at each layer, we assess whether the model successfully highlights\nthese needle visual tokens as semantically aligned with the query.\nFor a given transformer layer l, letr(l)\nidenote the semantic relevance score of visual token iat that\nlayer. We derive r(l)\nifrom the model’s cross-modal attention. Formally, if A(l,h)\nq→iis the attention\nweight from query tokens qto visual token iin head hof layer l, then:\nr(l)\ni=1\nHHX\nh=1A(l,h)\nq→i, (1)\nwhere His the number of attention heads. This score reflects how strongly the model semantically\nlinks visual token ito the query. We rank all visual tokens by r(l)\niin descending order to produce a\nsemantic relevant ranking at that layer.\nTo quantify how faithfully each layer’s semantic relevance scores identify the ground-truth semanti-\ncally relevant tokens (the needle visual tokens), we use the Recall@K metric, which computes the\nfraction of relevant tokens recovered in the top- Kranked tokens. Denote TopK (l)is the set of top- K\ntokens ranked by r(l)\ni, andRis the set of needle visual tokens considered semantically relevant by\nconstruction, then:\nRecall@ K(l) =|TopK (l)∩R|\n|R|, (2)\nWe evaluate Recall@ Kfor each transformer layer lby choosing K=|R|(so that perfect recovery\nof all needle visual tokens in the top- Kyields Recall@ K= 1.0). A higher Recall@ Kindicates that\nthe layer’s relevance is more faithful to the query.\n0 4 8 12 16 20 24 28\nLayer Index0%20%40%60%Recall@K\nreference layer\n19\nFigure 2: Recall@K values across different lay-\ners in LLaV A-Video-7B. Recall@K metric is the\nrecall ratio of ground-truth relevant tokens (e.g.,\nneedle-frame tokens) among the top-K tokens\nranked by a layer’s cross-modal attention scores.\nA higher Recall@K indicates the attention scores\nof that layer can more accurately identify the se-\nmantically related visual tokens. We choose the\noptimal layer with the highest Recall@K as the\nreference layer for token selection.Results of Layer Analysis: Figure 2 illustrates\nthe Recall@K value across all transformer layers\nin the LLM decoder of LLaV A-Video-7B. We ob-\nserve substantial variation in the effectiveness of\ndifferent layers at retrieving semantically relevant\ntokens. Specifically, early layers exhibit relatively\nlow Recall@ Kscores, suggesting that the atten-\ntion distributions at these stages are less aligned\nwith the semantic relevance of the query. Very\ndeep layers also not highlight the needle frames as\nthe model has already consolidated the critical vi-\nsual information into the final token for next token\ngeneration. Interestingly, an intermediate layer\nachieves the highest Recall@ K, indicating that it\nbest identifies the target visual tokens among its\ntop-ranked outputs. In other words, Lrefserves\nas the optimal attention layer that most reliably\ncaptures truly semantic-relevant tokens in the se-\nquence. Based on this finding, we designate layer\nLrefas the reference layer for guiding token selec-\ntion in FlexSelect. All subsequent token semantic-\nrelevance computations in our method will use the\nreference layer’s attention scores as the measure-\nment of semantic importance. More analysis including more base models and PCA visualization on\ntokens can be found in appendix A.1.\n3.2 Training-Free FlexSelect Pipeline\nEven with the reference layer Lrefidentified, processing a long video in a single forward pass is\ntypically infeasible due to the quadratic cost of self-attention and memory constraints. To address\n4\n--- Page 5 ---\n𝐹!𝐹\"𝐹#\naggregateencodingframe sets partition\nquery:Whatcoloristhiscup?𝑇!𝑇!(%&')token selection… 𝑇#𝑇#(%&')token selection𝑇\"𝑇\"(%&')token selection…\ntext embeddingprojectorLLM Decoder…ℎ𝑒𝑎𝑑!ℎ𝑒𝑎𝑑)cross-modal attention scores\naverage Layer 0Layer 1…reference layerprojector*text\tembedding*Layer 0Layer 1…last layeror\nsemantic relevance scoresrankWhite.𝑇\"queryToken Selectorlightweight modelvideoLLM layers\n𝑇\"(%&')Figure 3: Overview of FlexSelect token selection pipeline. Given a long video and a query,\nFlexSelect first partitions the video into frame sets and encodes each into visual tokens. For each set,\na token selector identifies semantically relevant tokens by ranking cross-modal attention scores from\na reference layer in a pre-trained VideoLLM or a lightweight selector network trained to approximate\nit. In this process, the projectorsandtext embeddingsare employed to convert the visual tokens and\nuser queries into tokens that match the dimension of subsequent transformer layers. After getting\nthe scores, the top-ranked tokens across all segments are aggregated and projected into the decoder\nfor final reasoning. FlexSelect operates in a training-free or rank-supervised mode, and serves as a\nplug-and-play module that enables efficient long-video understanding without requiring modifications\nto the base VideoLLM.\nthis, we propose a training-free FlexSelect strategy that enables efficient visual token selection while\npreserving semantic coverage across the entire video. The video is divided into multiple frame sets\nuniformly, with token selection performed independently within each set. This approach ensures\ncomprehensive temporal coverage without requiring the entire video sequence to be processed at\nonce.\nFrame Sets Partition. We partition the extensive frames into frame sets and each set most contains\nSframes to prevent token number exceeding the VideoLLM’s maximum context length. Given a video\nwithNsample frames {f1, f2, . . . , f N}, we construct K=⌈N/S⌉frame sets {F1,F2, . . . ,FK}\nsuch that each set samples frames at a same stride. Formally, the j-th frame set is defined as\nFj={fi|i≡jmod N},where j∈ {1, . . . , K }. This sampling ensures that each frame set spans\nthe entire video with different temporal offsets, capturing diverse temporal dynamics and reducing\nredundancy between sets.\nEach frame set Fjis encoded by the VideoLLM’s visual encoder. This yields a sequence of visual\ntokens Tj={tj,1, . . . , t j,M}for each set, Mmeans the total number of visual tokens in a frame set.\nSemantic Relevance Scoring and Token Selection. Within each frame set Fj, we compute a\nsemantic relevance score rj,ifor each token tj,iusing the attention mechanism at layer Lref:\nrj,i=1\nHHX\nh=1A(Lref,h)\nq→tj,i,\nwhere A(Lref,h)\nq→tj,iis the attention weight from the query tokens qto token tj,iin head h, andHis the\nnumber of heads. This relevance score reflects the semantic alignment between each token and the\nquery. We then rank the tokens in each Tjbyrj,iand select the top- ktokens T(sel)\nj=TopKk({rj,i}),\nyielding a set of semantically relevant tokens for each frame set.\n5\n--- Page 6 ---\nAggregation and Final Token Composition. The selected tokens from all frame sets are aggregated\nto form the final visual token input Tselected =SK\nj=1T(sel)\nj.This merged token set provides a globally\ninformed yet compact representation of the video.\nBy constructing Kframe sets with uniform sampling and processing them independently, our\nproposed FlexSelect strategy ensures that the framework scales efficiently to long video sequences.\nThe method minimizes computational overhead by leveraging the parallelizability of processing\nsmaller frame sets, while maintaining temporal fidelity across the video.\n3.3 Rank-Supervised Lightweight Token Selector\nWhile the training-free approach described above effectively reduces computational overhead, it\nstill relies on partial forward passes through the large VideoLLM to score visual tokens. To fur-\nther enhance inference efficiency, we introduce a lightweight token selector trained via rank su-\npervision to predict semantic relevance scores independently. The selector model is explicitly\ndesigned to replicate the token-ranking behavior observed at the reference transformer layer Lref.\nSpearnman Rank Losslayer 0layer 1last layer…lightweight model\n🔥projector!text embedding !\nbackwardlayer 0layer 1reference layer…\n❄projectortext embeddingvideoLLM layers\nr\"#$r(visual tokensquery 𝑞\nFigure 4: Illustration of our rank-supervised\ntraining. We align lightweight model’s pre-\ndicted scores ˆrwith the reference layer’s sematic\nrelevance scores rrefby optimizing the spear-\nman rank correlation coefficient between them.\nOnce trained, the ranking derived from these two\nscores will follow similar order, enabling the\nlightweight model to rank the visual tokens as\nthe reference layer does and select the related\ntokens quickly.Architecture and Input. Our lightweight token\nselector is a compact, shallow transformer-based\nnetwork intended to substantially reduce inference\ncosts compared to the large-scale VideoLLM. The\nmodel receives two inputs: visual tokens extracted\nby the vision encoder and the corresponding tex-\ntual query. It outputs predicted semantic rele-\nvance scores for each visual token in the input\nsequence. Formally, for a visual token sequence\n{t1, t2, . . . , t M}paired with a textual query q, the\nselector produces scores {ˆr1,ˆr2, . . . , ˆrM}indicat-\ning each token’s predicted semantic alignment\nwith the query. These scores are subsequently\nused to select the most relevant visual tokens, sim-\nilar to the training-free method described above.\nTo leverage pretrained knowledge and accelerate\ntraining convergence, we initialize the selector\nfrom a smaller-scale pretrained VideoLLM (ap-\nproximately 0.5B parameters). In our experiments,\nwe separately train selectors for LLaV A-Video-7B,\nQwen2.5VL-7B, and InternVL2.5-8B.\nRank-Supervised Training Objective. We train the lightweight selector with rank supervision,\ndirectly leveraging semantic relevance rankings provided by the reference transformer layer Lreffrom\nthe larger VideoLLM. For each training video-query pair, we first compute the semantic relevance\nscores rref= [rref\n1, rref\n2, . . . , rref\nM]at the reference layer Lref. Then, the lightweight selector predicts\nscores ˆr= [ˆr1,ˆr2, . . . , ˆrM]for the same tokens.\nThe training goal is to align the predicted semantic relevance ranking ˆrwith the reference ranking\nrref. To quantify this alignment, we use the spearman rank correlation coefficient [28]:\nρspearman (rref,ˆr) =PM\ni=1(rank(rref\ni)−rank(rref))(rank(ˆri)−rank(ˆr))qPM\ni=1(rank(rref\ni)−rank(rref))2PM\ni=1(rank(ˆri)−rank(ˆr))2. (3)\nρspearman (rref,ˆr)closer to 1 indicates stronger consistency between the ranking orders of rrefandˆr,\nwhile closer to 0 suggests no monotonic relationship. The corresponding training loss is defined as:\nLrank= 1−ρspearman (rref,ˆr). (4)\n6\n--- Page 7 ---\nModel Size VideoMME MLVU LongVB LVBench\nLong Overall M-Avg Val Test\nProprietary Models\nGPT-4o [24] - 65.3 71.9 64.6 66.7 34.7\nGemini-1.5-Pro [30] - 67.4 75.0 - 64.0 33.1\nOpen-Source VideoLLMs\nmPLUG-Owl3 [39] 7B 50.1 59.3 63.7 52.1 43.5\nQwen2-VL [32] 7B 53.8 63.3 66.9 55.6 42.4\nNVILA [21] 8B 54.8 64.2 70.1 57.7 -\nVideoLLaMA3 [40] 7B - 66.2 73.0 59.8 45.3\nAria [15] 8x3.5B 58.8 67.6 70.6 65.3 -\nOryx-1.5 [22] 34B 59.3 67.3 72.3 62.0 30.8\nVideo-XL-Pro [20] 3B - 60.0 70.6 56.7 -\nSF-LLaV A-1.5 [38] 7B - 63.9 71.5 62.5 45.3\nTPO [17] 7B 55.4 65.6 71.1 60.1 -\nQuato [23] 7B 55.7 65.9 71.9 59.0 -\nViLAMP [7] 7B 57.8 67.5 72.6 61.2 45.2\nLLaV A-Video [47] 7B 52.9 64.4 68.6 58.2 43.1\n+ FlexSelect 7B 59.8 ↑6.9 68.9 ↑4.5 73.2 ↑4.6 61.9 ↑3.7 52.9 ↑9.8\n+ FlexSelect-Lite 7B 58.3 ↑5.4 68.3 ↑3.9 71.8 ↑3.2 60.7 ↑2.5 52.2 ↑9.1\nInternVL2.5 [6] 8B 52.8 64.2 68.9 59.5 43.4\n+ FlexSelect 8B 58.1 ↑5.3 67.0 ↑2.8 71.9 ↑3.0 60.1 ↑0.6 49.7 ↑6.3\n+ FlexSelect-Lite 8B 57.9 ↑5.1 67.2 ↑3.0 71.9 ↑3.0 61.2 ↑1.7 49.9 ↑6.5\nQwen2.5-VL [1] 7B 55.6 65.4 70.2 59.5 45.3\n+ FlexSelect 7B 59.3 ↑3.7 68.2 ↑2.8 72.5 ↑2.3 62.4 ↑2.9 51.2 ↑5.9\n+ FlexSelect-Lite 7B 58.6 ↑3.0 67.4 ↑2.0 70.3 ↑0.1 61.9 ↑2.4 50.0 ↑4.7\nLLaV A-Video [47] 72B 61.9 70.0 71.2 62.4 45.5\n+ FlexSelect 72B 66.1 ↑4.2 73.1 ↑3.1 76.0 ↑4.8 66.9 ↑4.5 55.5 ↑10.0\nQwen2.5 VL [1] 72B 63.9 73.4 76.3 66.2 47.3\n+ FlexSelect 72B 66.9 ↑3.0 74.4 ↑1.0 76.6 ↑0.3 66.4 ↑0.2 56.6 ↑9.3\nTable 1: Comprehensive evaluation on different long video benchmarks. Gray rows show base-\nline results reproduced from public model weights. FlexSelect employs attention scores from the\nreference layer in VideoLLM for token selection, while FlexSelect-Lite utilizes scores from our\nlightweight token selector. Our methods consistently improve performance when integrated into\nvarious VideoLLMs by selecting semantically relevant visual tokens from extensive sampled frames.\nThe implementation details of these results can be found in appendix A.3.\nSince the ranking operation (i.e. argsort) itself is non-differentiable, we adopt a differentiable sorting\nalgorithm [ 2] to approximate ranks and enable gradient backpropagation. This ensures effective\ntraining while strictly supervising the model on ranking quality.\nIntegration into FlexSelect Pipeline At inference, the lightweight selector directly replaces the\ntransformer layers from the bigger VideoLLM to process visual tokens and query inputs and produces\nsemantic relevance scores, eliminating the need for intermediate VideoLLM computation. By using\nthese scores to select only the top-ranking tokens, we significantly reduce computational overhead,\nenabling efficient long-video understanding at scale. For clarity, we denote the FlexSelect integrated\nwith the lightweight token selector as FlexSelect-Lite.\n4 Experiments\n4.1 Models and Benchmarks\nWe conduct comprehensive evaluations across diverse VideoLLM architectures and scales to assess\nthe generalizability of our FlexSelect, including: (1) LLaV A-Video (7B/72B) [ 47], (2) InternVL-2.5\n8B [6], and (3) Qwen2.5VL (7B/72B) [ 1]. The models are evaluated on four established long-video\nunderstanding benchmarks: (1) LongVideoBench [ 36], a benchmark designed for accurate retrieval\n7\n--- Page 8 ---\n64 128 256 512\nNumber of Frames010203040Response Time (s)\n2.37 2.24 1.805.14 4.64\n2.1712.93\n7.38\n2.8938.24\n13.32\n4.23LLaVA-Video-7B\n+ FlexSelect\n+ FlexSelect-Lite\n134402688053760107520\nInput Visual T oken NumberFigure 5: Response time when sampling differ-\nent number of frames. FlexSelect accelerates\ninference by selecting semantically relevant vi-\nsual tokens faithfully.\n0 7 14 19(Lref) 27\nSelection Layer354045505560657075VideoQA Acc (%)\nVideoMME\nMLVU\nLongVB\nLVBenchFigure 6: VideoQA Acc with different layers\nfor token selection. The reference layer ( 19th)\nyields a more faithful cross-modal ranking for\ntoken selection.\nand reasoning in long-context videos, we report the validation set result. (2) MLVU [ 48], a multitask\nbenchmark specifically designed for long-form video understanding, we report the M-avg score. (3)\nVideoMME [ 11], a comprehensive evaluation across short/medium/long videos, we report the the\nlong and overall results without subtitles. (4) LVBench [ 33], an extreme-long video benchmark with\naverage video length reaches to one hour.\n4.2 Main Results\nCompared to SoTAs As shown in Table 1, our methods consistently enhance VideoLLM per-\nformance across multiple benchmarks. For LLaV A-Video-7B, FlexSelect delivers a +5.5 points\nimprovements in average (+4.5 on VideoMME, +4.6 on MLVU, +3.7 on LongVB, and +9.8 on\nLVBench), surpassing other long-form video understanding methods like SF-LLaV A [ 38], TPO [ 17],\nQuato [ 23] and ViLAMP [ 7] at 7B parameter scale, demonstrating its effectiveness for long video un-\nderstanding. FlexSelect-Lite maintains most of these gains (+3.9 in average) with less computational\ncost, validating the effectiveness of our rank-supervised token selector. The method’s adaptability\nis further confirmed by similar improvements when integrated into other models: Qwen2.5-VL-7B\nshows an average gain of +3.5 with FlexSelect and +2.3 with FlexSelect-Lite, while InternVL2.5-8B\nachieves an average improvement of +3.2 with FlexSelect and +3.7 with FlexSelect-Lite.\nFor larger-scale models, FlexSelect continues to deliver impressive results: LLaV A-Video-72B with\nFlexSelect shows a +4.5 improvement on LongVB (62.4 →66.9), outperforming GPT-4o (66.7).\nQwen2.5VL-72B with FlexSelect sets new state-of-the-art results among open-source methods,\nachieving +9.3 on LVBench (47.3 →56.6). These consistent improvements across model sizes and\nbenchmarks demonstrate the ability of FlexSelect to select semantically relevant tokens, confirming\nthe effectiveness of our token selection mechanism.\nEfficient Long Video Understanding Our method significantly improves VideoLLM’s efficiency\nwhen processing long videos with extensive frames. We evaluate the response time (i.e., time\ncost to generate the first token) on LLaV A-Video-7B. As shown in Figure 5, both FlexSelect and\nFlexSelect-Lite reduce response time for the same number of frames by decreasing the visual token\nnumber. FlexSelect-Lite achieves more pronounced acceleration, highlighting the efficiency of our\nlightweight token selector. This speed advantage grows progressively with more sampled frames: for\n512 frames, LLaV A-Video-7B requires 38.24 s per sample, while FlexSelect-Lite reduces this to just\n4.23 s—achieving a 9 ×speedup. More analysis on FLOPs estimation can be found in appendix A.2.\n4.3 Ablations\nEffectiveness of Reference Layer We compare the performance of FlexSelect on LLaV A-Video-7B\nwhen using attention scores from different layers for token selection. The evaluation is conducted\nwith 512 input frames and 6,720 max selected tokens. As shown in Figure 6, the reference layer\n(layer 19) performs the best, as it can more accurately select semantically related tokens.\n8\n--- Page 9 ---\nInput Frames VideoMME (%)\n64 65.2\n128 67.8\n256 68.1\n512 68.9\n1024 68.1\nMax Selected Tokens VideoMME (%)\n1,680 67.1\n3,360 68.4\n6,720 68.9\n13,440 68.1\nTable 2: Ablation on input frames and max se-\nlected tokens of training-free FlexSelect. Evalu-\nations are conducted on LLaV A-Video-7B.Training Data Scale VideoMME (%)\nNo training 61.9\n1% (14k samples) 63.3\n2% (29k samples) 63.4\n5% (67k samples) 63.9\n10% (134k samples) 63.5\nInstruction Type VideoMME (%)\nDetail caption 63.5\nOpen-ended QA 63.6\nMulti-choice QA 63.7\nMixed types 63.9\nTable 3: Ablation on data scale and instruction\ntype of rank-supervised training. Our token se-\nlector boosts performance while requiring only\n5% of training data.\nInfluence of Input Frames and Max Selected Tokens We evaluate LLaV A-Video-7B with FlexSe-\nlect on VideoMME under varying input frames and max selected tokens. As demonstrated in Table 2,\nwhen fixing the max selected tokens to 6,720, we observe progressive accuracy improvements as the\ninput frames increase from 64 to 512, followed by a slight degradation at 1,024 frames. Similarly, with\ninput frames fixed at 512, performance improves when increasing max selected tokens from 1,680 to\n6,720, but further selecting 13,440 tokens reduces the accuracy. The result shows that insufficient\ninput frames or selected tokens cannot adequately cover the video content, risking overlook critic\ninfromation, while excessive frames or tokens may introduce semantically irrelevant noise - both\nscenarios ultimately degrading performance.\nScales and Types of Training Data We train token selectors for LLaV A-Video-7B under different\ndata scale and video instruction types, and evaluate them with 64 input frames and 1,680 max selected\ntokens. We first randomly sample 4 subsets (1%, 2%, 5%, 10%) from LLaV A-Video-178K [ 47]\nwithout considering video instruction type, using them as training datasets to train different token\nselectors. As shown in Table 3, directly initializing the token selector from a small-scale VideoLLM\nachieves an accuracy of 61.9%, while after training with just 1% of the data, the accuracy rises\nto 63.3%. The performance peaks when trained on 5% of the data (approximately 67k samples),\nand saturates with more data, demonstrating the quick convergency and training efficiency of our\nrank-supervised training. Subsequently, we fix the data scale at 67k samples while varying the video\ninstruction types. Results show comparable performance across multiple-choice QA, open-ended\nQA, and video captioning data, suggesting that our training is effective on various instruction types.\nToken Selector Params Response TimeDatasets\nVideoMME LongVB MLVU LVBench\n0.5 B 4.0 s 67.2 61.2 71.9 49.9\n1.8 B 7.4 s 67.3 60.6 71.7 49.7\n3.0 B 12.0 s 67.4 61.8 72.8 50.1\nTable 4: Ablation on token selector parameters of rank-supervised training. We train different param-\neter size token selector for InternVL2.5-8B and test their acuracy on four benchmark. Evaluations are\nconducted with input frames set to 512 and max selected tokens set to 8,256.\nParameter Scale of Token Selector We investigate the impact of token selector parameter scale\non performance. We train token selectors with 0.5B, 1.8B, and 3B parameters for InternVL2.5-8B,\nwhich are initialized from InternVL2.5-1B, InternVL2.5-2B and InternVL2.5-4B respectively, then\nwe compare their performance on four benchmarks. Our results in Table 4 show that the 3B token\nselector achieves slightly higher scores than others but incurs significant computational overhead, and\nthe 0.5B model delivers comparable performance while requiring only one-third of the response time\nof 3B model(4.0s vs. 12.0s). This indicates that scaling parameter of token selector brings limited\ngains, and 0.5B is a cost-effective choice.\n9\n--- Page 10 ---\n5 Conclusion\nThis paper presents FlexSelect, a flexible and efficient token selection method that leverages cross-\nmodal attention scores in VideoLLMs to identify query-relevant visual tokens. Our approach com-\nbines: (1) training-free attention-based token ranking, and (2) a lightweight selector for fast filtering.\nIts architecture-agnostic design enables integration into diverse VideoLLMs without modification.\nBy selecting the most query-relavant visual tokens, FlexSelect achieves SoTA results on VideoMME\n(74.4), MLVU (76.6), LongVideoBench (66.9), and LVBench (56.6) while reducing tokens by\nover 90% (up to 9 ×speedup). This method demonstrates powerful long-form video understanding\ncapabilities, enabling effective analysis of long-form videos with minimal computational overhead.\nReferences\n[1]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang\nWan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen\nCheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report,\n2025. URL https://arxiv.org/abs/2502.13923 .\n[2]Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable\nsorting and ranking, 2020. URL https://arxiv.org/abs/2002.08871 .\n[3]Rasmus Bro and Age K Smilde. Principal component analysis. Analytical methods , 6(9):\n2812–2831, 2014.\n[4]Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao\nChang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for\nlarge vision-language models, 2024. URL https://arxiv.org/abs/2403.06764 .\n[5]Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang,\nHaotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz,\nLinxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language\nmodels for long videos, 2024. URL https://arxiv.org/abs/2408.10188 .\n[6]Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu,\nShenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren,\nZixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi,\nXingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong\nWu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei\nLu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding\nperformance boundaries of open-source multimodal models with model, data, and test-time\nscaling, 2025. URL https://arxiv.org/abs/2412.05271 .\n[7]Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. Scaling video-language models to 10k frames\nvia hierarchical differential distillation, 2025. URL https://arxiv.org/abs/2504.02438 .\n[8]Anxhelo Diko, Tinghuai Wang, Wassim Swaileh, Shiyan Sun, and Ioannis Patras. Rewind:\nUnderstanding long videos with instructed learnable memory, 2025. URL https://arxiv.\norg/abs/2411.15556 .\n[9]Mark Endo, Xiaohan Wang, and Serena Yeung-Levy. Feather the throttle: Revisiting visual\ntoken pruning for vision-language model acceleration, 2024. URL https://arxiv.org/abs/\n2412.13180 .\n[10] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam:\nEnhancing video-language understanding with causal cross-attention masks for short and long\nvideos, 2024. URL https://arxiv.org/abs/2408.14023 .\n[11] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui\nZhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme:\nThe first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024.\nURL https://arxiv.org/abs/2405.21075 .\n10\n--- Page 11 ---\n[12] Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei\nNing, and Yu Wang. Framefusion: Combining similarity and importance for video token\nreduction on large visual language models, 2024. URL https://arxiv.org/abs/2501.\n01986 .\n[13] Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, and Zheng Zhao. Linvt:\nEmpower your image-level large language model to understand videos, 2024. URL https:\n//arxiv.org/abs/2412.05185 .\n[14] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\nZhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer,\n2024. URL https://arxiv.org/abs/2408.03326 .\n[15] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan\nZhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu,\nLihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal\nnative mixture-of-experts model, 2025. URL https://arxiv.org/abs/2410.05993 .\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning , pages 19730–19742. PMLR, 2023.\n[17] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference\noptimization for long-form video understanding, 2025. URL https://arxiv.org/abs/2501.\n13919 .\n[18] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large\nlanguage models, 2023. URL https://arxiv.org/abs/2311.17043 .\n[19] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao,\nEnhua Wu, and Jie Hu. Kangaroo: A powerful video-language model supporting long-context\nvideo input, 2024. URL https://arxiv.org/abs/2408.15542 .\n[20] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Recon-\nstructive token compression for extremely long video understanding, 2025. URL https:\n//arxiv.org/abs/2503.18478 .\n[21] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng\nXi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu\nHsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna,\nDaguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao\nLu. Nvila: Efficient frontier visual language models, 2025. URL https://arxiv.org/abs/\n2412.04468 .\n[22] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx\nmllm: On-demand spatial-temporal understanding at arbitrary resolution, 2025. URL https:\n//arxiv.org/abs/2409.12961 .\n[23] Yongdong Luo, Wang Chen, Xiawu Zheng, Weizhong Huang, Shukang Yin, Haojia Lin,\nChaoyou Fu, Jinfa Huang, Jiayi Ji, Jiebo Luo, and Rongrong Ji. Quota: Query-oriented\ntoken assignment via cot query decouple for long video comprehension, 2025. URL https:\n//arxiv.org/abs/2503.08689 .\n[24] OpenAI. Hello GPT-4o, 5 2024. URL https://openai.com/index/hello-gpt-4o/ .\nAccessed: 2024-05-20.\n[25] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\nwindow extension of large language models, 2023. URL https://arxiv.org/abs/2309.\n00071 .\n[26] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang,\nJiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin,\nLongxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei\n11\n--- Page 12 ---\nZheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua\nYang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated\ngui interaction with native agents, 2025. URL https://arxiv.org/abs/2501.12326 .\n[27] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang,\nand Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding,\n2024. URL https://arxiv.org/abs/2409.14485 .\n[28] Charles Spearman. The proof and measurement of association between two things. In James J.\nJenkins and Donald G. Paterson, editors, Studies in Individual Differences: The Search for\nIntelligence , pages 45–58. Appleton-Century-Crofts, 1961. doi: 10.1037/11491-005. URL\nhttps://doi.org/10.1037/11491-005 .\n[29] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression\nof tokens for fast video large language models, 2025. URL https://arxiv.org/abs/2411.\n15024 .\n[30] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M. Dai, Anja Hauth, and et al. Gemini: A family of highly capable\nmultimodal models, 2024. URL https://arxiv.org/abs/2312.11805 .\n[31] Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\nKeqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang\nSu, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5\ntechnical report, 2025. URL https://arxiv.org/abs/2412.15115 .\n[32] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing\nLiu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men,\nDayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-\nlanguage model’s perception of the world at any resolution, 2024. URL https://arxiv.org/\nabs/2409.12191 .\n[33] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu\nHuang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video\nunderstanding benchmark, 2024. URL https://arxiv.org/abs/2406.08035 .\n[34] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Adaretake: Adaptive\nredundancy reduction to perceive longer for video-language understanding, 2025. URL https:\n//arxiv.org/abs/2503.12559 .\n[35] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui\nHe, and Linfeng Zhang. Stop looking for important tokens in multimodal language models:\nDuplication matters more, 2025. URL https://arxiv.org/abs/2502.11494 .\n[36] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-\ncontext interleaved video-language understanding, 2024. URL https://arxiv.org/abs/\n2407.15754 .\n[37] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao,\nConghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. Pyramiddrop: Accelerating your large\nvision-language models via pyramid visual redundancy reduction, 2025. URL https://arxiv.\norg/abs/2410.17247 .\n[38] Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang,\nYinfei Yang, and Afshin Dehghan. Slowfast-llava-1.5: A family of token-efficient video large\nlanguage models for long-form video understanding, 2025. URL https://arxiv.org/abs/\n2503.18943 .\n[39] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and\nJingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large\nlanguage models, 2024. URL https://arxiv.org/abs/2408.04840 .\n12\n--- Page 13 ---\n[40] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong\nLeng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing,\nand Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video\nunderstanding, 2025. URL https://arxiv.org/abs/2501.13106 .\n[41] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong\nLeng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing,\nand Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video\nunderstanding, 2025. URL https://arxiv.org/abs/2501.13106 .\n[42] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu,\nYuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the\nevaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772 .\n[43] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue\nWang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision,\n2024. URL https://arxiv.org/abs/2406.16852 .\n[44] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo,\nQi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token\npruning: Make vlm inference faster, 2024. URL https://arxiv.org/abs/2412.01818 .\n[45] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video\nlarge multimodal models with one vision token. arXiv preprint arXiv:2501.03895 , 2025.\n[46] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis\nGudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. Sparsevlm:\nVisual token sparsification for efficient vision-language model inference, 2025. URL https:\n//arxiv.org/abs/2410.04417 .\n[47] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video\ninstruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713 .\n[48] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin,\nXi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: Benchmarking\nmulti-task long video understanding, 2025. URL https://arxiv.org/abs/2406.04264 .\n[49] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch,\nLicheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, and Xide\nXia. Apollo: An exploration of video understanding in large multimodal models, 2024. URL\nhttps://arxiv.org/abs/2412.10360 .\n13\n--- Page 14 ---\nA Appendix / supplemental material\nA.1 Details of Recall@K Experiment\nNeedle Images and Queries The needle images and corresponding queries used in our Recall@K\nexperiment are directly borrowed from the V-NIAH experiment in LongV A [ 43], which provides five\nneedle-query pairs. We randomly sample 128 videos from the VideoMME [ 11] test set and insert\neach needle-query pair into them, resulting in a total of 640 test samples. We compute Recall@K on\nthese samples.\nRecall@K of Various Models In addition to LLaV A-Video-7B, we calculate Recall@K for LLaV A-\nVideo-72B, InternVL2.5-8B, Qwen2.5VL-7B and Qwen2.5VL-72B. The result is shown at Figure 7.\nWe identify the reference layer of these models: layer 15 for InternVL2.5-8B, layer 60 for LLaV A-\nVideo-7B, layer 20 for Qwen2.5VL-7B and layer 60 for Qwen2.5VL-72B.\n0 101520 30\nLayer Index0%10%20%30%40%50%60%70%Recall@K\nreference layerInternVL2.5-8B\n01020304050607079\nLayer Index0%10%20%30%40%50%60%70%Recall@K\nreference layerLLaVA-Video-72B\n0481216202427\nLayer Index0%10%20%30%40%50%60%70%Recall@K\nreference layerQwen2.5VL-7B\n01020304050607079\nLayer Index0%10%20%30%40%50%60%70%Recall@K\nreference layerQwen2.5VL-72B\nFigure 7: Recall@K across layers of different VideoLLMs.\n2\n 0 23\n2\n1\n01Layer 0\n5.0\n 2.5\n 0.0 2.5 5.04\n2\n0246Layer 4\n10\n 5\n 0 510\n5\n0510Layer 8\n10\n 0 10 207.5\n5.0\n2.5\n0.02.55.07.510.0Layer 12\n10\n 0 1010\n5\n0510Layer 16\n20\n 10\n 0 1015\n10\n5\n0510Layer 20\n20\n 0 2020\n15\n10\n5\n051015Layer 24\n200\n 100\n 0 100 20040\n20\n020406080Layer 27\nText Tokens Needle Visual Tokens Non-needle Visual Tokens\nFigure 8: PCA visualization of query tokens, needle visual tokens(i.e. semantically related tokens)\nand non-needle visual tokens from different layers of LLaV A-Video-7B. We found that the correlation\nbetween destributions of text tokens and needle visual tokens varies with layer depth, matching the\ntrend of Recall@K: at shallow layers, their distributions diverge; in intermediate layers, a strong\nlinear correlation emerges; while at very deep layers, this linear correlation weakens. This finding\ndemonstrates that semantic alignment between visual and text tokens established at intermediate\nlayers in VideoLLMs, which further explains the emergence of the reference layer.\nPCA Visualization We further visualize the query tokens, needle visual tokens and non-needle\nvisual tokens using Principal Component Analysis (PCA) [ 3] to examine cross-modal semantic\nalignment across different layers in VideoLLMs. As shown in Figure 8 (LLaV A-Video-7B), a\n14\n--- Page 15 ---\nsignificant distribution discrepancy exists between text tokens and needle visual tokens in shallow\nlayers. As the layer depth increases, these distributions develop a strong linear correlation in deeper\nlayers, which subsequently weakens in the deepest layers. This indicates that the cross-modal\nsemantic alignment is initially absent in early layers, gradually strengthens in intermediate layers,\nand then decreases in the final layers, which is consistent with the trend of Recall@K.\nA.2 FLOPs Analysis\nSuppose the transformer decoder of VideoLLM has Llayers, hheads, and the hidden states size is d,\nthe intermediate size of FFN is m. For an input sequence of ntokens, the FLOPs of the prefilling\nstage can be estimated as:\nFLOPs =L×(4nd2+ 2n2d+ 2ndm)≈2Ln2d,\nsince n≫dandn≫min typical scenarios.\nWhen using FlexSelect, assuming the M-th layer serves as the reference layer and the input sequence\nis partitioned into Ksegments via our frame set partition operation, ultimately selecting n′tokens,\nthe FLOPs can be estimated as:\nFLOPs’ =M×(4nd2+2n2d\nK+ 2ndm) +L×(4n′d2+ 2n′2d+ 2n′dm)≈2Mn2d\nK,\nsince in our configuration, n′= 0.0625n, making the L×(4n′d2+ 2n′2d+ 2n′dm)term negligible\nin FLOPs’. Consequently, FlexSelect requires onlyM\nL×1\nKof the original FLOPs.\nSimilarly, when employing FlexSelect-Lite, the Flops can be estimated as2L′n′2d′\nK, where L′andd′\nare layer num and hidden states dimension of lightweight token selector. This FLOPs is more smaller\nthan FlexSelect because L′< L ansd′< d. FLOPs estimation only provides a theoretical reference\nfor computational cost. For practical considerations, we recommend referring to the actual time cost\nanalysis in Figure 5.\nA.3 Implementation Details\nThe evaluations in main Table 1 are conducted under LMMS-Eval [ 42] framework on 8 96G H20\nGPUs. We set the input frames number Nto 1024, 512, 512 for Qwen2.5VL(7B/72B), LLaV A-\nVideo(7B/72B), and InternVL2.5-8B respectively, and max subset frames number Sto 64 for all\nmodels. Denoting Nimage is the token number required for encoding one frame, we select 7,010\n(64∗Nimage), 6,720 ( 32∗Nimage), 8,256 ( 32∗Nimage) visual tokens for these models respectively,\nwhich is 6.25% of original input length. The reference layer Lfor token selection are set to Layer\n15 for InternVL2.5-8B, Layer 19 for LLaV A-Video-7B, Layer 20 for Qwen2.5VL-7B, Layer 60 for\nLLaV A-Video-72B and Qwen2.5VL-72B determined by arg max\nLRecall @K(L).\nWe train lightweight token selectors for 7B/8B models but exclude 72B due to memory constraints.\nFor token selector of LLaV A-Video-7B, we initialize it with the decoder of LLaV A-OneVision-\n0.5B [ 14]; for token selector of InternVL2.5-8B, we initialize it with the decoder of InternVL-1B. In\nthe case of Qwen2.5VL-7B, where no similarly-sized VideoLLM is available, we directly initialize\nit with Qwen2.5-Instruct-0.5B [ 31]. We randomly select a small ( 5%) subset of LLaV A-Video-\n178K [ 47] as the training data, which contains about 67k video instruction samples. We uniformly\nsample 64 frames from each video during selector training. Token selectors for LLaV A-Video and\nInternVL2.5 were trained for 1 epoch, while the token selector for Qwen2.5VL was trained for 3\nepochs since it was initialized from a language model, which is lack of visual prior knowledge, and\nrequires more training steps to achieve convergence.\nA.4 Limitations\nFlexSelect enhances VideoLLMs’ long-video understanding by selecting semantically relevant tokens,\nwithout modifying or retraining the base VideoLLM. Thus, its performance ceiling is bounded by the\nhost VideoLLM’s native capabilities. FlexSelect-Lite trains a lightweight token selector to predict\nthe reference layer’s importance scores in large-scale VideoLLMs. While more efficient, it typically\nunderperforms direct reference-layer token selection. Nevertheless, compared to other heavily trained\nalternatives and existing sub-optimal token pruning methods, FlexSelect remains an efficient and\neffective solution for boosting diverse VideoLLMs’ long-video comprehension.\n15\n--- Page 16 ---\nA.5 Social Impacts\nFlexSelect operates as a preprocessing module that selects semantically relevant visual tokens for\ndiverse VideoLLMs. It is important to note that this method cannot prevent the base VideoLLMs from\npotentially generating erroneous, biased, or harmful hallucinations. When integrating FlexSelect with\ndifferent VideoLLMs, users should be aware of and mitigate potential risks in the generated content.\nA.6 Visualization of Some Examples\nWhat is the first celestial object shown in the video?\nEarth.\nWhat is the animal in this video?\nA chimp.\nWhat’s the color of the cup appeared in this video?\nThe color of the cup is white.\nFlexSelect\nFlexSelect\nFlexSelect\nFigure 9: Some token selection examples of our FlexSelect on LLaV A-Video-7B. We choose 8\nframes from the sampled frames for visualization.\n16",
  "text_length": 54521
}