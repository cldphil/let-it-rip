{
  "id": "http://arxiv.org/abs/2506.01078v1",
  "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking",
  "summary": "Despite notable advancements in multimodal reasoning, leading Multimodal\nLarge Language Models (MLLMs) still underperform on vision-centric multimodal\nreasoning tasks in general scenarios. This shortfall stems from their\npredominant reliance on logic- and knowledge-based slow thinking strategies,\nwhile effective for domains like math and science, fail to integrate visual\ninformation effectively during reasoning. Consequently, these models often fail\nto adequately ground visual cues, resulting in suboptimal performance in tasks\nthat require multiple plausible visual interpretations and inferences. To\naddress this, we present GThinker (General Thinker), a novel reasoning MLLM\nexcelling in multimodal reasoning across general scenarios, mathematics, and\nscience. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that\ngrounds inferences in visual cues and iteratively reinterprets these cues to\nresolve inconsistencies. Building on this pattern, we further propose a\ntwo-stage training pipeline, including pattern-guided cold start and incentive\nreinforcement learning, designed to enable multimodal reasoning capabilities\nacross domains. Furthermore, to support the training, we construct\nGThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths\nand 4K curated reinforcement learning samples, filling the data gap toward\ngeneral multimodal reasoning. Extensive experiments demonstrate that GThinker\nachieves 81.5% on the challenging comprehensive multimodal reasoning benchmark\nM$^3$CoT, surpassing the latest O4-mini model. It also shows an average\nimprovement of 2.1% on general scenario multimodal reasoning benchmarks, while\nmaintaining on-par performance in mathematical reasoning compared to\ncounterpart advanced reasoning models. The code, model, and data will be\nreleased soon at https://github.com/jefferyZhan/GThinker.",
  "authors": [
    "Yufei Zhan",
    "Ziheng Wu",
    "Yousong Zhu",
    "Rongkun Xue",
    "Ruipu Luo",
    "Zhenghao Chen",
    "Can Zhang",
    "Yifan Li",
    "Zhentao He",
    "Zheming Yang",
    "Ming Tang",
    "Minghui Qiu",
    "Jinqiao Wang"
  ],
  "published": "2025-06-01T16:28:26Z",
  "updated": "2025-06-01T16:28:26Z",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01078v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01078v1  [cs.CV]  1 Jun 2025GThinker: Towards General Multimodal Reasoning\nvia Cue-Guided Rethinking\nYufei Zhan1,3,†,∗, Ziheng Wu2,†, Yousong Zhu1,/enve♀e,\nRongkun Xue6,Ruipu Luo2,Zhenghao Chen2,Can Zhang2,Yifan Li7,Zhentao He2,\nZheming Yang2,Ming Tang1,3,Minghui Qiu2,Jinqiao Wang1,3,4,5\n1Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences\n2ByteDance3School of Artificial Intelligence, University of Chinese Academy of Sciences\n4Peng Cheng Laboratory5Wuhan AI Research\n6Xi’an Jiaotong University7Renmin University of China\nhttps://github.com/jefferyZhan/GThinker\nAbstract\nDespite notable advancements in multimodal reasoning, leading Multimodal Large\nLanguage Models (MLLMs) still underperform on vision-centric multimodal rea-\nsoning tasks in general scenarios. This shortfall stems from their predominant\nreliance on logic- and knowledge-based “slow thinking” strategies—while effective\nfor domains like math and science—fail to integrate visual information effectively\nduring reasoning. Consequently, these models often fail to adequately ground\nvisual cues, resulting in suboptimal performance in tasks that require multiple plau-\nsible visual interpretations and inferences. To address this, we present GThinker\n(General Thinker), a novel reasoning MLLM excelling in multimodal reasoning\nacross general scenarios, mathematics, and science. GThinker introduces Cue-\nRethinking, a flexible reasoning pattern that grounds inferences in visual cues\nand iteratively reinterprets these cues to resolve inconsistencies. Building on this\npattern, we further propose a two-stage training pipeline, including pattern-guided\ncold start and incentive reinforcement learning, designed to enable multimodal\nreasoning capabilities across domains. Furthermore, to support the training, we\nconstruct GThinker-11K , comprising 7K high-quality, iteratively-annotated rea-\nsoning paths and 4K curated reinforcement learning samples, filling the data gap\ntoward general multimodal reasoning. Extensive experiments demonstrate that\nGThinker achieves 81.5% on the challenging comprehensive multimodal reasoning\nbenchmark M3CoT, surpassing the latest O4-mini model. It also shows an aver-\nage improvement of 2.1% on general scenario multimodal reasoning benchmarks,\nwhile maintaining on-par performance in mathematical reasoning compared to\ncounterpart advanced reasoning models. The code, model, and data will be released\nsoon at https://github.com/jefferyZhan/GThinker .\n1 Introduction\nOpen-source Multimodal Large Language Models (MLLMs) [ 22,43,53,54,70] have made sig-\nnificant strides across a wide range of tasks. Leading models like Qwen2.5-VL [ 2] now rival\nclosed-source counterparts such as GPT-4o [ 17] in performance. These advances have benefited in\npart from the adoption of chain-of-thought (CoT) techniques [ 28,51,59], especially in mathematics\nand science. With the emergence of OpenAI’s O1 model [ 19], several studies [ 45,56,58] have sought\n∗Work done during internship at ByteDance.\n†Equal Contribution.\nPreprint. Under review.\n--- Page 2 ---\nScience\nMath\n GeneralMultimodal Reasoning\nVisual Reliance\nQA-ComplexityFeatures\nStructured \nReaonser\nType 1: Summary -> Caption -> Conclude\nType 2: Detection -> Analysis -> Answer\nType 3:  Identify -> Crop & Enc. -> Answer\n...\nType n: Build Graph -> Think -> Answer\nExisting\nRL-Reasoner\n      Math & Science: Free-form reasoning \nbased on the question-image pairs with \nself-reflection\nGeneral: Indentify inconsistent  or \nmiss key visual cues without revision\nGThinker\n      All Types: Free-form reasoning with \nnecessary self-reflection for knowledge \nand Logic\nAll Types: Visual cue rethinking\nFigure 1: Multimodal reasoning methods comparison across scenarios. Multimodal Reasoning\nin different domains is featured with visual reliance and high question complexity, making it a\nchallenging task. Different from previous methods, GThinker utilizes free-form thinking for different\ntypes of questions instead of a fixed structure form and enables general scenario reasoning accuracy\nwith designed visual cue rethinking.\nto transfer such slow-thinking capabilities to the multimodal reasoning domain to enhance models’\nperformance on complex tasks. DeepSeek-R1 [ 13] further introduces a new perspective, showing\nthat outcome-reward Reinforcement Learning (RL) can awake long CoT reasoning, with promising\nresults [4, 32, 57] in multimodal reasoning tasks involving science and mathematics.\nBeyond mathematics and science, multimodal reasoning in general scenarios, which often involves\nvisual cues and related commonsense still remains under-explored. Unlike math and science tasks,\nwhich typically follow strict logical structures and have unique answers, multimodal reasoning tasks\nin general scenarios are more diverse in nature. This makes it challenging to summarize a fixed CoT\npattern or design an effective Process Reward Model (PRM), limiting the effectiveness of structured\nreasoning [ 56,58] and Multimodal PRMs [ 25,50]. Furthermore, general scenarios often require\nplausible interpretations and inferences grounded in visual content, which reduces the effectiveness\nof current outcome-reward-based reasoning models [ 4,16,57] that are primarily developed for math\nand science. As summarized in Figure 1, existing slow-thinking models frequently miss critical visual\ncues. When encountering plausible but inconsistent outputs, they often proceed directly to an answer\nwithout revisiting the reasoning path, unlike the reflection and verification behaviors observed in\nmath and science domains. This suggests that in general scenarios, rethinking that integrates visual\ninterpretation and inference cannot be effectively incentivized by RL alone, in contrast to the naturally\nlearned reflection mechanisms in math and science reasoning tasks during pretraining [38].\nTo address these challenges, we propose GThinker, a novel reasoning MLLM excelling in multimodal\nreasoning across general scenarios, mathematics, and science. First, we introduce a new long-chain\ncue-driven pattern for multimodal reasoning called Cue-Rethinking. Unlike prior approaches [ 45,56]\nthat define structured CoT formats, Cue-Rethinking only requires the reasoning process to be strictly\ngrounded in visual cues without enforcing a fixed format. After completing an initial reasoning\nchain, the model rethinks on the interpretations and inferences based on visual content to correct\ninconsistencies and arrive at the correct answer. Building on this pattern, we propose a two-stage\ntraining pipeline to enable robust multimodal reasoning. We begin by using pattern-guided cold start\nto train the model to learn this reasoning pattern on different tasks, and cold-start it with supervised\nfine-tuning. Then, we further employ an incentive RL stage to let the model explore optimal strategies\nfor solving diverse problems across domains. To support training, we further develop a multimodal\niterative annotation pipeline based on the latest advancing multimodal models like O3 [ 34] and\nconstruct GThinker-11k, compromised 7K cold-start data with high-quality annotated reasoning paths\nand 4K reinforcement learning samples, filling a key gap in multimodal reasoning fine-grained data\nfor general scenarios.\n2\n--- Page 3 ---\nWe implement GThinker based on the advanced open-source MLLM Qwen-VL 2.5–7B and con-\nduct extensive experiments to rigorously evaluate its effectiveness. We first benchmark GThinker\nagainst both open- and closed-source models on M3CoT [ 7], a challenging and comprehensive\nmultimodal reasoning dataset spanning science, general commonsense, and mathematics. For broader\nvalidation, we include general-domain benchmarks such as MMStar [ 5] and RealWorld QA [ 55], as\nwell as science and math-focused benchmarks including MMMU-Pro [ 62], MathVision [ 47], and\nMathVista [ 27]. GThinker demonstrates strong performance across all domains, achieving 81.5%\non M3CoT—surpassing the advanced O4-mini model. On MMStar and RealWorld QA, GThinker\nachieves the improvement of 2.5% and 1.6%, respectively. Additionally, it performs competitively on\nscience and math benchmarks with 40.7% on MMMU Pro and 72.7% on MathVista, matching or\noutperforming recent RL-enhanced approaches, further validating its effectiveness.\n2 Related Work\n2.1 Structured Multimodal Chain-of-Thought Reasoning\nStructured Multimodal Chain-of-Thought (MCoT) reasoning builds on the Chain-of-Thought (CoT)\nparadigm [ 51], extending it to multimodal tasks using step-by-step reasoning [ 28,68]. Many\napproaches enhance this framework with structured designs [ 26,33,69] and further improvements\nsuch as fine-grained visual grounding, context integration, or tool use [ 3,12,21,24,31,39,52].\nHowever, these methods are often task-specific—e.g., CCoT [ 33] for compositional reasoning,\nLLaV A-Aurora [ 3] for spatial reasoning—and lack robustness across diverse scenarios. Recently,\nslow-thinking paradigms [ 19,36,44] have been proposed to improve reasoning depth. Enhanced\nMCoT variants like LLaV A-CoT [ 56], Virgo [ 10], and Mulberry [ 58] leverage long-chain generation,\ntree search, and self-reflection. Yet, they remain confined to structured, logic-heavy tasks and\nare difficult to generalize to broader settings. In contrast, GThinker adopts a free-form, cue-based\nthinking paradigm with further visual cue-based rethinking, moving beyond rigid structures to support\nopen-domain multimodal reasoning. This design enables generalization across task types without\nsacrificing interpretability or performance.\n2.2 Multimodal Reasoning with Reinforcement Learning\nReinforcement learning (RL) has become a powerful tool to align MLLMs and mitigate hallucinations\n[23,41,42,61,66,67], and is now being explored to improve multimodal reasoning. Early approaches\nlike LLaV A-Reasoner [ 65] and MPO [ 49] rely on rationale distillation alone and preference data to\nguide reasoning, while Insight-V [ 37] designs multi-agent systems with iterative Direct Preference\nOptimization. However, these methods focus on “teaching correctness” via supervised signals and\nhuman preference annotations, limiting robustness and scalability for more complex scenarios. A\nshift emerged with DeepSeek-R1 [ 13], which showed that outcome-based rewards, without fine-\ngrained annotations, can drive reasoning through self-verification and reflection. Follow-up works\n[4,6,32,35,43,57] expand this idea to the multimodal domain, leveraging verifiable reward\nfunctions or rule-based signals to improve math and science reasoning. Yet, these methods largely\ntarget well-defined tasks with unique answers. In general multimodal reasoning, models must handle\nambiguity, interpret visual cues, and perform flexible inference. This limits the direct transfer of\nknowledge-style RL setups. Additionally, common reward models like PRMs [ 25,50] struggle to\ncapture progress in diverse tasks under general scenarios. To address this, we propose a clue-driven\nrethinking pattern tailored for general scenario multimodal scenarios but also accustomed to math\nand science settings. By further leveraging our design two-stage training, GThinker enables flexible\nreasoning with visual cue-based rethinking and knowledge reflection across diverse multimodal\nreasoning tasks.\n3 Methodology\nIn this section, we provide a comprehensive description of the novel multimodal reasoning model\nGThinker as depicted in Figure 2. In §3.1, we first present the Cue-Rethinking Pattern, a core\ncomponent built on free-form thinking to provide visual cue-driven guidance for multimodal reasoning\nacross scenarios. Then, in §3.2, we describe Pattern-Guided Cold Start, in which we build 7k high-\nquality reasoning path annotated data and train the model with pattern-guided supervised fine-tuning to\n3\n--- Page 4 ---\nMulti-Domain\nAnn. Training SamplesPattern-Guided Cold Start\n(1) PGS Formatting\nInfer\n Rethinking-Enabled\nFormatRethinking-Disabled\nFormat\n(2) Supervised Fine-tuning\nQuestionResonse\nGTCross \nEntropyIncentive RL\nQuestion\n...Thinking\nAnswer\nMCQ \nCOEMath \nGTHybrid \nReward \nCalculation\nGThinkerFigure 2: Overall pipeline for constructing GThinker. We collect multi-domain data covering general,\nmath, and science tasks, and annotate it using multiple advanced MLLMs. The Pattern-Guided Cold\nStart phase then teaches the model the Cue-Rethinking Pattern for different question types. Finally,\nincentive reinforcement learning with DAPO enhances GThinker’s ability to perform adaptive and\naccurate multimodal reasoning across diverse scenarios.\nlearn how to think and rethink for different scenarios. Finally, we introduce Incentive Reinforcement\nLearning to generalize the multimodal reasoning capabilities of the model across diverse scenarios in\n§3.3.\n3.1 Cue-Rethinking Pattern\nExisting long-chain reasoning methods[ 45,56] often rely on fixed, structured thinking chains tailored\nto specific tasks. While effective in targeted domains, their performance tends to drop sharply when\napplied to more general or unfamiliar scenarios. Outcome-reward models offer more flexibility, but\nthey also fall short in general settings that require grounded, visually informed interpretations and\ninferences. To tackle this challenge, we introduce the Cue-Rethinking Pattern, a thinking framework\nthat enables flexible long-chain reasoning through a combination of free-form thinking and rethinking\non visual cues.\nCue-\nRethinking \nTriggerVisual Cue-\nbased \n Rethinking\nFree-Form \nInitial Reasoning\nReflection, \nVerification...Visual cues \ntagged\nFigure 3: Toy example of the Cue-Rethinking Pattern.\nThe dashed line indicates generation on demand.As shown in Figure 3, this process unfolds in\nthree stages generally: initial reasoning, cue-\nrethinking trigger, and cue-based rethinking.\nDuring the initial stage, the model is free to\nreason in any form based on the question and\nimage content itself, without structural con-\nstraints. It simply tags any referenced visual\ncues in the format <vcues_*> </vcues_*> (*\nindicates the No.), which are later used for\nvisual cues rethinking. This flexibility allows\nthe model to apply learned reasoning strategies, such as step-by-step deduction or logical and\nknowledge reflection, much like how reasoning is approached in mathematical or scientific contexts,\ndepending on the task.\nAfter completing the initial reasoning, a prompt is triggered to initiate cue-based rethinking, like “Let’s\ncheck each visual cue and corresponding reasoning before reaching the final answer” . Importantly,\nwe do not require immediate rethinking after visual cue identification, as doing so may disrupt the\nnatural reasoning flow and prevent us from seeing the overall context. Then, the model revisits\nall previously marked visual cues, checking for inconsistencies or flaws. If problematic cues are\nidentified, they are revised, and the model re-engages in corresponding reasoning, now grounded in\nthe corrected cues, and concludes the final answer. This approach not only accommodates a wide\nrange of reasoning approaches for different tasks but also addresses current limitations in handling\nmisleading or missing visual inputs during reasoning. By combining free-form thinking with designed\nvisual cue rethinking, this pattern delivers robust, adaptable reasoning across diverse multimodal\nreasoning scenarios.\n4\n--- Page 5 ---\n<think>To determine what the image depicts, \nlet's analyze the shapes and their \narrangement:\n1. <vcues_1>The image consists of several \ngeometric shapes, including triangles and a \nparallelogram</vcues_1>.\n2. <vcues_2>The shapes are arranged in a way \nthat suggests a form or figure</vcues_2>.\n3. <vcues_3>The red and white shapes on the \nleft side resemble the body and legs of a \ncrab</vcues_3>.\n4. <vcues_4>The blue and pink shapes on the \nright side resemble the claws of a \ncrab</vcues_4>.\nGiven this analysis, the image most closely \nresembles a crab.\nQuestion: What does the \nimage depict?\nA. Crab\nB. Fish\nC. Shrimp\nD. Lobster<vcues_3>The red triangle at the upper left actually represents the head of a \nshrimp, not the body and legs of a crab</vcues_3>  Shrimp illustrations \noften highlight the head section in a distinct color, and here it is placed at the \nforemost position.\n<vcues_4>The small dark‑blue right‑triangle and the bright pink rectangle \ntogether form the tip of the tail (blue) and the main tail segment (pink) of a \nshrimp, not crab claws</vcues_4> . Their alignment behind the body area \nmatches a shrimp’s tail fan.\n<vcues_5>The large light‑lavender parallelogram on the lower left represents \nthe main body/abdomen of the shrimp</vcues_5> , connecting the red \nhead in front to the tail pieces behind.\nBecause each colored region (red head, lavender body, pink tail, blue tail‑tip) \nmaps cleanly onto the characteristic parts of a shrimp’s anatomy, the \ncomposite figure depicts a shrimp, not a crab, fish, or lobster.</think>\n<answer>C</answer>\nFigure 4: Constructed Data Example with Cue Rethinking. The visual cues in red are flawed ones,\nwhile the green indicates the visual cues are revised or appended.\n3.2 Pattern-Guided Cold Start\nBuilding on the Cue-Rethinking Pattern, we address how to effectively teach models to internalize and\napply this reasoning pattern. While outcome-reward RL can guide models toward desired thinking,\nrelying solely on it is still challenging and computationally intensive [ 13]. To overcome this, we\nintroduce a Pattern-Guided Cold Start stage as shown in Figure 2, where the model is trained to adopt\nthe Cue-Rethinking paradigm through supervised fine-tuning. To support this, we construct a 7K-\nscale dataset of annotated reasoning paths across multiple domains, using a multimodal collaborative\nannotation pipeline. The resulting data enables the model to learn both general problem-solving and\ncue-based rethinking.\nData Construction via Multimodal Iterative Annotation. To support domain-diverse multimodal\nreasoning, we collect data spanning mathematics, science, and general visual scenarios, validating\neach example for visual dependency and reasoning complexity. Instead of prompting models with\nimage captions and question or structure requirements, we feed the image, question, and answer\ninto advanced multimodal reasoning models, prompting them to reason step-by-step and identify\nrelevant visual cues. For math and science questions, the models are allowed with self-reflection and\nvalidation; for cue-rich general questions, they are instructed to provide explicit visual references to\nsupport later rethinking. This strategy aligns well with the flexible design of Cue-Rethinking. To\nmaximize precision, we iteratively annotate the data using several models, including GPT-4o, o1,\nand o3, leveraging each model’s strengths. We further extend this process to generate cue-based\nrethinking data. This automated pipeline results in a final dataset of 7,358 high-quality annotated\nsamples, detailed further in the Appendix A. We provide a data example with key texts formatted in\ncue-rethinking in Figure 4.\nPattern-Guided Supervised Fine-tuning. With the annotated data, we train the model to learn the\nCue-Rethinking pattern via supervised fine-tuning. Since reflection in science and math scenarios or\ncue-rethinking in general scenarios is one of the reasoning approaches, enforcing a single learning\nformat could constrain the model’s robustness. To address this, we introduce pattern-guided selective\nformatting to customize the training data based on problem type. Specifically, we first run the base\nmodel on the training questions and compare its reasoning paths to the annotations. Samples with\nflawed visual cues are selected to form full Cue-Rethinking sequences, covering all three stages.\nRemaining examples are formatted as free-form reasoning paths. The model is then fine-tuned using\nthis pattern-compiled data, enabling it to adaptively perform reasoning or rethinking as required by\nthe question.\n3.3 Incentive Reinforcement Learning\nFollowing the Pattern-Guided Cold Start phase, the model acquires the designed reasoning pattern\nand learns to perform both flexible step-by-step reasoning and cue-based rethinking. Building on this\nfoundation, we further enhance the model using outcome-reward reinforcement learning to encourage\nexploration and help it generalize across diverse tasks and scenarios. Given recent advances in\noutcome-reward reinforcement learning, we adopt the Decoupled Clip and Dynamic Sampling Policy\nOptimization (DAPO) algorithm [ 60] due to its strengths in supporting long-chain reasoning and its\n5\n--- Page 6 ---\nefficiency in stable training. To accommodate varying task types and align with our pattern-based\nmethodology, we design a hybrid reward computation strategy tailored to different problem categories.\nThis training is carried out on a curated set of 4K diverse reasoning samples, enabling the model to\ngeneralize beyond the supervised data and adapt effectively to new challenges.\nPreliminaries about DAPO. DAPO improves from the Group Relative Policy Optimization (GRPO)\n[40] with several enhancements to improve training efficiency, stability, and long-chain benefits,\nwhile retaining the key features such as outcome-based reward and policy optimization. As shown\nin the Equation 1, DAPO first employs a clip-higher strategy to address exploration limitations\ncaused by identical responses, by adjusting the clipping threshold. It then adopts a dynamic sampling\nmechanism to prevent low training efficiency when all responses in a group are either entirely correct\nor entirely incorrect. Furthermore, it integrates Token-Level Policy Gradient Loss to encourage\nthe model to learn high-quality reasoning patterns within long-chain responses while suppressing\nredundant reasoning. Lastly, the Overlong Reward Shaping strategy helps reduce the noise caused by\nexcessively long sample sequences during training.\nJDAPO(θ) =E(q,a)∼D,{oi}G\ni=1∼πold(·|q)\n1PG\ni=1|oi|GX\ni=1|oi|X\nt=1min\u0010\nri,t(θ)ˆAi,t,clip(ri,t(θ),1−εlow,1 +εhigh)ˆAi,t\u0011\n\ns.t. 0<|{oi|is_equivalent (a, oi)}|< G,(1)\nwhere\nri,t(θ) =πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t),ˆAi,t=Ri−mean ({Ri}G\ni=1)\nstd({Ri}G\ni=1). (2)\nBy incorporating DAPO, especially its clip-higher mechanism and token-level loss, the model is\nbetter equipped to sample diverse reasoning paths. This enables it to learn reasoning strategies such as\nreflective knowledge inference for math tasks or cue-based rethinking in general multimodal scenarios.\nAs a result, the model improves the ability to dynamically select the most suitable reasoning strategy\nfor each situation, improving both generalization and robustness across domains.\nHybrid Reward Design. The default DAPO setting combines format-based and accuracy-based\nrewards. Prior approaches often constrain QA tasks to rigid formats, such as multiple-choice, and\ndepend on exact string matching to assess correctness. This limits the range of question types the\nmodel can handle, especially in general scenarios, and model-based verification further reduces\ntraining efficiency. To overcome these limitations, we propose a hybrid reward strategy within the\nconstraints of verifiable rewards. We support three main question types: multiple-choice, math,\nand simple open-ended formats. For multiple-choice questions, we apply exact answer matching.\nFor math problems—whether numeric or symbolic—we use Math-Verify [ 18] to extract and verify\nanswers. For open-ended questions that yield concise responses (e.g., a word or short phrase), we\nguide the model to summarize the answer in a standardized, concise format, enabling straightforward\nmatching during reward computation. This design expands the diversity of supported question types\nwhile preserving reward accuracy. For the format reward, we follow prior work by enforcing and\nverifying adherence to the think-answer structure.\nData Construction. To support the reinforcement learning stage, we construct a set data of 4k\nsamples spanning math, science, and general reasoning tasks. Rather than relying solely on the 7k\nexamples from the cold start phase, we introduce 4k samples sourced from public datasets to enhance\ndiversity and generalization. This combined dataset offers a well-balanced and domain-spanning\nresource tailored for incentive RL. We provide more details about this data in the Appendix A.\n4 Experiments\n4.1 Implementation Details\nTraining Settings. We implement GThinker with the advanced MLLM Qwen2.5-VL-7B [ 2], one of\nthe latest and most capable models at this scale, combining strong visual understanding with broad\n6\n--- Page 7 ---\nTable 1: Main results on comprehensive multimodal reasoning benchmark M3CoT. Abbreviations\nused in the table: Lang. (Language), Nat. (Natural), Soc. (Social), Phys. (Physical), Temp.\n(Temporal), Alg. (Algebra), Geom. (Geometry), Theo. (Theory). Excluding closed-source models,\nvalues in bold represent the highest performance, while underlined values indicate the second-best\nperformance across all models.\nModelScience Commonsense MathematicsOverall\nLang. Nat. Soc. Phys. Soc. Temp. Alg. Geom. Theo.\nClosed-Source Models\nGemini-2.5 Pro [9] 97.6 91.6 75.3 92.2 81.4 94.3 81.1 78.8 61.9 85.9\nO3-20250416 [34] 96.2 89.3 68.0 91.1 80.2 93.5 95.0 87.5 90.5 83.8\nO4-mini-20250416 [34] 97.2 84.7 62.9 94.4 82.6 91.1 92.9 86.3 76.2 80.9\nGPT-4o-20241120 [17] 96.7 72.0 58.3 91.1 76.4 82.9 21.4 31.3 23.8 67.4\nOpen-Source Models\nInternVL-2.5-8B [8] 82.5 63.7 45.2 86.7 79.8 93.4 42.8 27.5 33.3 61.8\nOvis2-8B [30] 80.6 63.1 46.2 83.3 79.3 87.8 45.0 42.5 38.9 61.9\nValley2[54] 85.3 64.4 48.4 90.0 77.7 80.5 43.6 36.3 47.6 62.8\nQwen2.5-VL-7B [2] 82.9 61.2 46.8 82.2 81.4 81.3 57.9 40.0 61.9 62.4\nReasoning Models\nLLaV A-CoT-11B [56] 72.0 56.4 41.7 84.4 72.3 82.1 37.9 36.3 33.3 56.0\nInternVL2.5-MPO-8B [48] 92.4 75.9 61.9 85.6 82.6 94.3 55.0 43.8 76.2 73.3\nKimi-VL-A3B-Thinking [43] 86.2 64.4 39.6 91.1 78.9 89.4 13.5 15.0 14.2 58.3\nMM-Eureka-7B [32] 86.7 71.5 57.3 81.1 80.2 90.2 40.0 23.8 28.6 67.4\nR1-OneVision-7B [57] 74.9 66.4 51.4 84.4 72.3 85.4 30.0 31.3 42.9 61.8\nVLAA-Thinker-7B [4] 91.0 70.6 58.1 78.9 78.1 87.8 45.7 35.3 28.6 68.0\nGThinker-7B 92.4 90.7 68.9 82.2 81.4 94.3 73.5 62.5 81.0 81.5\ngeneral knowledge. We train the GThinker using our design two-stage pipeline, including pattern-\nguided cold start and incentive reinforcement learning with the constructed data. For Pattern-Guided\nCold Start, we use a global batch size of 128 and a learning rate of 5e-6, training the model with the\n7K reasoning path annotated data for 3 epochs. In the Incentive RL stage, we set the rollout number\nto 16, use a global batch size of 64, and start with a learning rate of 1e-6, training for 170 steps using\nthe curated 4K data. Training is conducted on 4 nodes, each with 8 NVIDIA H100 GPUs. The total\ntraining time is about 9 hours. We provide more details in Appendix B.\nEvaluation Settings. We evaluate our model against top closed-source models, including the latest\nO4-mini, as well as open-source base and reasoning models with comparable parameter sizes trained\nusing diverse methodologies. The evaluation focuses on multimodal reasoning across general,\nmathematical, and scientific scenarios:\n•M3CoT: A challenging benchmark that spans science, commonsense, and math domains, with\neach example verified to require multi-step reasoning. We primarily use this benchmark to\ncomprehensively evaluate models’ multimodal reasoning capabilities across diverse scenarios.\n•General scenario benchmarks: MMStar [ 5] and RealWorld QA [ 55]. These benchmarks focus\non general and realistic scenarios, including parts of understanding-based reasoning tasks, and\nare used to evaluate multimodal reasoning capabilities.\n•Science and math scenario benchmarks: We use MMMU-Pro [ 62], which covers multiple\nscientific subjects, to evaluate multimodal reasoning in scientific contexts. For math-specific\nevaluation, we adopt the widely used MathVista [27] and MathVision [47]benchmarks.\nAll evaluations are conducted on a single node equipped with 8 NVIDIA H100 GPUs. For M3CoT,\nwe follow each model’s official settings and prompts and use VLMEvalKit [ 11] for fair evaluation.\nFor other benchmarks, we use the results reported in their original papers. For RL-enhanced reasoning\nmodels, which primarily focus on math and science domains, we follow their released models and\nevaluation guidelines to conduct testing.\n7\n--- Page 8 ---\nTable 2: Main results on math-related and multidisciplinary benchmarks, and also fine-grained\nunderstanding of multimodal benchmarks incorporating reasoning. We use the setting detailed in the\nevaluation settings, and for the result of Qwen2.5-VL-7B on MMMU-Pro we report the reproduced\none marked in∗due to the large difference, as widely observed.\nModel MMStar RealWorldQA MMMU-Pro MathVista Mini MathVision Full\nClose-Source Models\nGemini-2.5 Pro 73.6 78.0 68.8 80.9 73.3\nGPT-4o-20241120 65.1 76.2 54.5 63.8 31.2\nOpen-Source Models\nInternVL2.5-8B [8] 62.8 70.1 34.4 64.4 19.7\nOvis2-8B [30] 64.4 - - 71.4 25.9\nValley2 [54] 62.5 67.5 - 69.1 24.9\nQwen2.5-VL-7B [2] 63.9 68.5 36.9∗68.2 25.1\nReasoning Models\nLLaV A-CoT-11B [56] 57.6 63.6 33.8 54.8 20.6\nInternVL2.5-MPO-8B [48] - - - 67.0 25.7\nKimi-VL-A3B-Thinking [43] 60.8 - - 67.6 36.8\nMM-Eureka-7B [32] 64.2 67.3 40.7 73.0 26.9\nR1-Onevision-7B [57] 42.8 62.7 31.0 64.1 29.9\nVLAA-Thinker-7B [4] 63.7 66.9 39.8 68.0 26.4\nGThinker-7B 66.4 70.1 40.7 72.7 26.6\n4.2 Main Results\nGThinker-7B demonstrates superior multimodal reasoning, consistently outperforming advanced\nopen-source base models and surpassing recent reasoning models on most benchmarks. On the com-\nprehensive M3CoT benchmark depicted in Table 1, which demands balanced knowledge and visual\nunderstanding, GThinker-7B achieves 81.5% average accuracy, performing on par with the latest\nreasoning model O4-mini. Among the reasoning models, GThinker-7B achieves the highest perfor-\nmance on 8 out of 9 sets. Besides the notable progress in science and commonsense, a key advantage\nof our approach is evident in multimodal mathematics problems within M3CoT, where GThinker-7B\nsuccessfully aligns visual elements with textual information to derive correct solutions. This contrasts\nsharply with models like VLAA-Thinker-7B, which, despite visual competence, struggle with the\nrequisite text-vision integration for M3CoT’s mathematical section, while Kimi-VL-A3B-Thinking\nproduces repeated contents, especially on the math set, reducing its overall performance. This result\nfurther underscores our method’s effectiveness in fostering robust multimodal reasoning.\nBeyond M3CoT, GThinker-7B exhibits leading performance across specialized and general multi-\nmodal benchmarks requiring reasoning as demonstrated in Table 2. On challenging math benchmarks,\nit achieves 72.7% on MathVista (+4.5 points over baseline) and 26.6% on MathVision (+1.5 points).\nSimilarly, on the multidisciplinary science benchmark MMMU-Pro, GThinker-7B improves by\napproximately 4 points. Furthermore, it shows significant gains on general benchmarks requiring\nfine-grained understanding and further reasoning, with 66.4% on MMStar and 70.1% on RealWorld\nQA. Crucially, our proposed method enhances performance across diverse domains—general, math,\nand science—without the typical trade-offs observed in other reasoning models. Previous leading\nmodels, by focusing heavily on knowledge long-chain CoT reasoning, often showed limited gains\nor even degradation on general multimodal reasoning tasks due to less emphasis on visual cues, a\nlimitation our versatile approach overcomes.\nWhen compared to the advancing non-thinking model GPT-4o, GThinker-7B achieves superior or\ncompetitive performance on several benchmarks, notably M3CoT, MMStar, and MathVista, despite\nits significantly smaller 7B backbone. While GPT-4o leads on benchmarks like RealWorldQA,\nMMMU-Pro, and MathVision, which heavily leverage extensive knowledge and perceptual abilities\ninherent in larger models, our results are compelling. The substantial gains achieved by GThinker-7B,\nparticularly on reasoning-centric benchmarks (e.g., M3CoT), highlight the efficacy of our proposed\nmethod in significantly boosting complex reasoning capabilities, even with a more compact model\n8\n--- Page 9 ---\nTable 3: Ablation on Data Pipeline and Quality.\nLang. denotes rationales from M3CoT. GThinker\nrefers to data from our proposed pipeline. Iter.\nindicates the application of our iterative annotation\nprocess.\nLang. GThinker Iter. Science Com. Math Overall\n✓ 58.9 81.8 40.2 63.5\n✓ 70.6 79.1 43.2 69.6\n✓ ✓ 73.1 79.3 46.9 73.6Table 4: Ablation on GThinker Components.\nThe PGS indicate the Pattern-Guided Selection\nintroduced in §3.2\nMethod Science Com. Math Overall\nGThinker 82.5 83.7 70.5 81.5\n- Incentive RL 73.1 79.3 46.9 73.6\n- PGS Formatting 68.0 82.0 42.7 68.4\n- PG Cold Start 58.8 81.7 40.2 61.5\nQwen2.5-VL-7B-Zero 63.3 81.6 49.0 64.2\narchitecture. This underscores the advantage of our approach in efficiently enhancing multimodal\nreasoning across domains.\n4.3 Ablation Study\nAblation on Data Pipeline and Iterative Annotation. High-quality data is crucial for training\neffective multimodal reasoning models. We enhance data quality through our novel pipeline, including\nan iterative annotation approach (details in App. A). To validate these contributions, we ablate each\ncomponent by fine-tuning the model on the constructed 7K samples, varying only the annotation\nsource, and evaluate on the M3CoT.\nAs shown in Table 3, using the rationales (Lang.), which are GPT-annotated [ 1], yields an overall score\nof 63.5%. Our data generation pipeline, even without iterative annotation, significantly improves\nperformance to 69.6% (+6.1% absolute). This demonstrates the inherent benefit of our pipeline\ndesign in producing superior data for multimodal reasoning across diverse domains. Incorporating our\niterative annotation process to curate the GThinker 7k reasoning paths further boosts the overall score\nto 73.6%, an additional 4.0% improvement. We attribute this gain to the complementary strengths of\nthe leading models, including GPT-4o, O1, and O3: during the collaborative annotation iterations,\nvisual cues and reasoning logic are more thoroughly captured, which further boosts the quality of the\nCoT data.\nAblation on GThinker Components. Ablation on GThinker Components. To assess the contribution\nof each component to GThinker’s performance, we conduct ablation studies by incrementally remov-\ning modules and evaluating on M3CoT. The final row (with all modules removed) corresponds to\ntraining with the same QA pairs but without any of our proposed methods. As shown in Table 4, using\nthe Cue-Rethinking Pattern for Pattern-Guided Cold Start—without Pattern-Guided Selection (PGS)\nFormatting—yields a 6.9% average improvement. Adding PGS Formatting provides a further 5.2%\naverage gain, with science and math questions improving by 5.1% and 4.2%, respectively. In contrast,\nperformance on commonsense questions drops by 2.7%. This is because PGS Formatting applies\ncue-rethinking to samples with incorrect visual cues, prompting the model to engage with misleading\ninformation and learn to reflect and reason more flexibly. While this stage introduces variability due\nto the diversity and ambiguity of the cues, it builds a foundation for more adaptable reasoning in later\nstages. Science and math tasks, which benefit from consistent patterns and structured reasoning, show\nmore stable gains from formatting. With Incentive Reinforcement Learning added, the model achieves\nsubstantial improvements across all domains, significantly outperforming the baseline. These results\nshow that the free-form, cue-based reasoning developed during Cold Start is effectively reinforced\nand leveraged in the RL stage, enhancing the model’s generalization across tasks. For comparison,\nwe also evaluate DAPO under the same conditions. As shown in Table 4, DAPO offers limited gains\nin general scenarios, though it improves performance in math and science. This highlights both the\nrationale behind our design and the impact of each component in advancing multimodal reasoning.\n5 Conclusion\nThis paper addresses the challenge of advancing multimodal reasoning in MLLMs beyond domain-\nspecific tasks like math and science, extending toward more general scenarios. We introduce\nGThinker, a novel reasoning framework that excels across diverse multimodal tasks, including general,\nmathematical, and scientific domains. Powered by our Cue-Rethinking Pattern, GThinker moves\n9\n--- Page 10 ---\nbeyond rigid templates, enabling flexible, question-driven reasoning and robust handling of flawed\nvisual cues through reflective and knowledge-grounded thinking. Our two-stage pipeline—Pattern-\nGuided Cold Start followed by Incentive Reinforcement Learning—guides the model to learn effective\nreasoning strategies and reinforces its ability to adapt across domains. Extensive experiments on\nmulti-domain multimodal reasoning benchmarks show that GThinker outperforms existing reasoning\nMLLMs in both accuracy and cross-domain adaptability. Ablation studies further confirm the\neffectiveness of each core design component. We provide more discussion on limitations and broader\nimpact in the Appendix.\n10\n--- Page 11 ---\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 , 2023.\n[2]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang,\nWei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo\nYang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025.\n[3]Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G Shapiro, and Ranjay\nKrishna. Perception tokens enhance visual reasoning in multimodal language models. arXiv preprint\narXiv:2412.03548 , 2024.\n[4]Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft\nor rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint\narXiv:2504.11468 , 2025.\n[5]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,\nYu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In The\nThirty-eighth Annual Conference on Neural Information Processing Systems .\n[6]Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in\nvision-language models with less than $3. https://github.com/Deep-Agent/R1-V , 2025. Accessed:\n2025-02-02.\n[7]Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: A novel benchmark\nfor multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473 , 2024.\n[8]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang,\nXizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic\nvisual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 24185–24198, 2024.\n[9]Google DeepMind. Gemini 2.5 pro preview model card. https://storage.googleapis.com/\nmodel-cards/documents/gemini-2.5-pro-preview.pdf , 2025.\n[10] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu,\nZhongyuan Wang, and Ji-Rong Wen. Virgo: A preliminary exploration on reproducing o1-like mllm. arXiv\npreprint arXiv:2501.01904 , 2025.\n[11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang\nZang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality\nmodels. In Proceedings of the 32nd ACM International Conference on Multimedia , pages 11198–11201,\n2024.\n[12] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang,\nXiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. In\nProceedings of the 32nd ACM International Conference on Multimedia , pages 9096–9105, 2024.\n[13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 , 2025.\n[14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth\nConference on Neural Information Processing Systems , 2021.\n[15] Jack Hessel, Jena D Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate\nSaenko, and Yejin Choi. The abduction of sherlock holmes: A dataset for visual abductive reasoning. In\nEuropean Conference on Computer Vision , pages 558–575. Springer, 2022.\n[16] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and\nShaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv\npreprint arXiv:2503.06749 , 2025.\n11\n--- Page 12 ---\n[17] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[18] Hynek. Math-Verify: Math Verification Library, 2023. If you use this software, please cite it using the\nmetadata from this file.\n[19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720 , 2024.\n[20] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen V ong, Robert Hawkins, and Yoav Artzi.\nAbstract visual reasoning with tangram shapes. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages 582–601, 2022.\n[21] Zixi Jia, Jiqiang Liu, Hexiao Li, Qinghua Liu, and Hongbin Gao. Dcot: Dual chain-of-thought prompting\nfor large multimodal models. In The 16th Asian Conference on Machine Learning (Conference Track) ,\n2024.\n[22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 ,\n2024.\n[23] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang,\nand Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint\narXiv:2312.10665 , 2023.\n[24] Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Xuanjing Huang, and Zhongyu Wei. V ocot: Unleashing\nvisually grounded multi-step reasoning in large multi-modal models. arXiv preprint arXiv:2405.16919 ,\n2024.\n[25] Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. Diving into self-evolving\ntraining for multimodal reasoning. arXiv preprint arXiv:2412.17451 , 2024.\n[26] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning\nimproves large vision-language models. arXiv preprint arXiv:2403.12966 , 2024.\n[27] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation\nmodels in visual contexts. In The Twelfth International Conference on Learning Representations .\n[28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems , 35:2507–2521, 2022.\n[29] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and\nAshwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.\nInThe Eleventh International Conference on Learning Representations , 2022.\n[30] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural\nembedding alignment for multimodal large language model. arXiv:2405.20797 , 2024.\n[31] Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. Textcot: Zoom in\nfor enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797 , 2024.\n[32] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han,\nBotian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning\nwith rule-based reinforcement learning. arXiv preprint arXiv:2503.07365 , 2025.\n[33] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought\nprompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 14420–14431, 2024.\n[34] OpenAI. O3 and o4-mini system card. https://cdn.openai.com/pdf/\n2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf , 2025. Ac-\ncessed: 2025-05-07.\n[35] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan,\nTianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv\npreprint arXiv:2504.05599 , 2025.\n12\n--- Page 13 ---\n[36] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan,\nHector Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1. arXiv preprint\narXiv:2410.18982 , 2024.\n[37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model. Advances in Neural\nInformation Processing Systems , 36:53728–53741, 2023.\n[38] Darsh J Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani,\nAdarsh Chaluvaraju, Andrew Hojel, Andrew Ma, et al. Rethinking reflection in pre-training. arXiv preprint\narXiv:2504.04022 , 2025.\n[39] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng\nLi. Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for\nchain-of-thought reasoning. Advances in Neural Information Processing Systems , 37:8612–8642, 2024.\n[40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language\nmodels. arXiv preprint arXiv:2402.03300 , 2024.\n[41] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-\nYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented\nrlhf. arXiv preprint arXiv:2309.14525 , 2023.\n[42] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-\nYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented\nrlhf. arXiv preprint arXiv:2309.14525 , 2023.\n[43] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang,\nChenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491 , 2025.\n[44] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025.\n[45] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao\nLi, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step\nvisual reasoning in llms. arXiv preprint arXiv:2501.06186 , 2025.\n[46] Huy V V o, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec,\nHugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Automatic data curation for\nself-supervised learning: A clustering-based approach. arXiv preprint arXiv:2405.15613 , 2024.\n[47] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hong-\nsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural\nInformation Processing Systems , 37:95095–95169, 2024.\n[48] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou\nZhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language\nmodels via mixed preference optimization. arXiv preprint arXiv:2411.10442 , 2024.\n[49] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou\nZhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via\nmixed preference optimization. arXiv preprint arXiv:2411.10442 , 2024.\n[50] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue\nCao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal\nreasoning. arXiv preprint arXiv:2503.10291 , 2025.\n[51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems , 35:24824–24837, 2022.\n[52] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal llms. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13084–\n13094, 2024.\n[53] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma,\nChengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for\nadvanced multimodal understanding. arXiv preprint arXiv:2412.10302 , 2024.\n13\n--- Page 14 ---\n[54] Ziheng Wu, Zhenghao Chen, Ruipu Luo, Can Zhang, Yuan Gao, Zhentao He, Xian Wang, Haoran Lin, and\nMinghui Qiu. Valley2: Exploring multimodal models with scalable vision-language design. arXiv preprint\narXiv:2501.05901 , 2025.\n[55] xAI. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v , 2024.\n[56] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language\nmodels reason step-by-step. arXiv preprint arXiv:2411.10440 , 2024.\n[57] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin,\nFengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through\ncross-modal formalization. arXiv preprint arXiv:2503.10615 , 2025.\n[58] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin\nSong, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection\nvia collective monte carlo tree search. arXiv preprint arXiv:2412.18319 , 2024.\n[59] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. Advances in neural information\nprocessing systems , 36:11809–11822, 2023.\n[60] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv\npreprint arXiv:2503.14476 , 2025.\n[61] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan\nLiu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v\ntrustworthiness. arXiv preprint arXiv:2405.17220 , 2024.\n[62] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao\nYu, Ge Zhang, Huan Sun, et al. Mmmu-pro: A more robust multi-discipline multimodal understanding\nbenchmark. arXiv preprint arXiv:2409.02813 , 2024.\n[63] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out\nall object locations at any granularity with large language models. In European Conference on Computer\nVision , pages 405–422. Springer, 2024.\n[64] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu,\nWilliam Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and\ngrounding with large language models. In First Conference on Language Modeling .\n[65] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming\nPang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint\narXiv:2410.16198 , 2024.\n[66] Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin,\nYu Qiao, Xuanjing Huang, et al. Spa-vl: A comprehensive safety preference alignment dataset for vision\nlanguage model. arXiv preprint arXiv:2406.12030 , 2024.\n[67] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu\nZhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint\narXiv:2502.10391 , 2025.\n[68] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal\nchain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 , 2023.\n[69] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought\nprompting for multimodal reasoning in language models. Advances in Neural Information Processing\nSystems , 36:5168–5191, 2023.\n[70] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian,\nWeijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source\nmultimodal models. arXiv preprint arXiv:2504.10479 , 2025.\n14\n--- Page 15 ---\nAppendix\nContents\nA GThinker-11K Construction 2\nA.1 Data Preparations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nA.2 Multimodal Iterative Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nA.3 Negative Reasoning Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nA.4 Formatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nA.5 Automatic Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nA.6 Reinforcement Learning Data Construction . . . . . . . . . . . . . . . . . . . . . 3\nA.7 Open Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nB Training Details 6\nB.1 System Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nB.2 Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nC Qualitative Analysis 7\nD Limitations 7\nE Broader Impact 7\n1\n--- Page 16 ---\nA GThinker-11K Construction\nTo support the training of GThinker, we have designed a scalable data generation pipeline to construct\nthe GThinke-11K data as we have concluded in §3.2 and §3.3, respectively. In this section, we\nsystematically introduce the data construction process, including the 7K cold start data, as depicted\nin Figure 5, and 4K RL data.\nRaw Data\nData Filter\nNegative \nReasoning \nAnnotationO3\nMultimodal\nIterative Annotation\nO1\nO3\nGPT-4o\nFormatting\nAutomatic \nCheck\nAnswer Coherence\nReason Coherence\nFormat\nFinal 7K\nSFT Data\nFigure 5: Data pipeline for cold start data.\nA.1 Data Preparations\nThough several datasets are constructed to enhance multimodal reasoning capabilities in MLLMs\n[56–58] spanning diverse domains, they often present challenges such as high knowledge dependency,\nlimited visual cues, or limited reasoning level. To extend the multimodal reasoning to general\nscenarios beyond knowledge-intensive math and science problems, we empirically find that the\nM3CoT dataset provides a well-established data baseline for multimodal reasoning across domains.\nIt details how to collect data across science, mathematics, and general scenarios with commonsense,\nand ensure the visual reliance and reasoning complexity with final manual checking. Building on\nbaseline, we apply a two-step filtering process to ensure data quality: (1) we discard entries with\ncorrupted or missing images, and (2) we verify the remaining samples’ compliance with closed-source\nmodel usage policies using GPT-4o, resulting in 7,358 high-quality samples. We illustrated the data\ncomposition in Table 5.\nTable 5: Data composition of 7K Cold Start data of GThinker-11K.\nType V olume Source\nScience 5266 KiloGram[20], ScienceQA [28], M3CoT [7]\nMathamatics 621 TableWMP [29], Math [14]\nCommonsense 1471 Sherlock [15](Questions generated by M3CoT)\nA.2 Multimodal Iterative Annotation\nTo generate high-quality reasoning paths and visual cues, we propose a multimodal iterative annotation\nmethodology that leverages multiple leading MLLMs, such as OpenAI’s O-series, for end-to-end\nreasoning path generation different from prior approaches [ 52,57,58] that rely on multi-step pipelines\nwhich generate captions first and then utilize the reasoning LLMs. This leads to more efficient\ngeneration and results in more coherent multimodal long-chain reasoning paths, richer step-by-step\nvisual cues, and stronger logical deductions. As shown in Figure 5, drawing on the insight that\ndifferent models offer complementary strengths [ 58], we implement a iterative refinement strategy:\ninitial annotations from Qwen2.5-VL-7B, as models with lower parameters sometimes are more\nfaithful to the visual content, and is first revised by GPT-4o to reduce apparent errors. Then, the\nresults are processed by O1, and further enhanced by O3. To finish this, we guide the models using\ncarefully engineered prompts optimized through few-shot learning as shown in Prompt 1. For each\nimage–question–answer triplet, the model is prompted to produce a long reasoning process or refine\nthe long reasoning chain with the relevant visual cues identified. This three-stage process significantly\nimproves the accuracy and depth of final thinking annotations by leveraging the diverse capabilities\nof each model.\n2\n--- Page 17 ---\nA.3 Negative Reasoning Annotation\nWith the positive, high-quality reasoning data, we further extend our process to handle negative\nreasoning with corrections. Rather than manually crafting incorrect reasoning traces [ 63,64], which\nmay introduce artifacts due to the gap between human-designed prompts and model capabilities, we\nfirst sample natural, flawed responses from 7B-level capable but compact models [ 2,54]. While\npositive samples provide a reference point for correction, the variability in natural language expression\nrequires a more nuanced approach. To this end, we employ the advanced reasoning capabilities of O3.\nUsing carefully designed prompts as shown in Prompt 2, we guide the model to compare incorrect\nreasoning against the correct reasoning path and the corresponding image. This enables the model\nto identify and correct missing or uncertain and misleading visual cues and faulty inferences. For\nvisual cue correction, each initial cue is explicitly linked to its corrected counterpart, followed by the\nrevised deduction, ensuring the data remains structured and easy to parse.\nA.4 Formatting\nAfter all annotations are completed, we utilize GPT-4o to parse and format all the data. This includes\nstandardizing elements like line breaks within the <think></think><answer></answer>format and\nextracting the correct, key visual cues. This process is designed to facilitate broader subsequent use.\nA.5 Automatic Verification\nWith the formatted annotated data, we perform automatic checks targeted at three critical aspects\nto ensure high data quality, helped by annotation-excluded Gemini 2.5 Pro [ 9], as illustrated in\nFigure 5. These checks target three critical aspects. First, for format validation, we ensure that for\neach annotation, the positive reasoning path ends with a concluded answer, and the visual cues can\nbe parsed. Second, for answer consistency, the annotated answers are parsed and cross-checked\nagainst the ground truth. Third, for reasoning coherence, we input the image, QA pair, and annotated\nreasoning into Gemini 2.5 Pro to evaluate logical alignment between visual cues and reasoning with\nPrompt 3, flagging any contradictions. Samples with identified issues are reprocessed through the\nrelevant correction steps in our pipeline. Samples with identified issues are reprocessed through the\nrelevant correction steps in our pipeline.\nTo assess the quality control of the designed pipeline, we manually review a randomly selected 15%\nsubset of the final dataset and confirm that our pipeline reliably produces high-quality annotations,\nwhich ensures scalability.\nA.6 Reinforcement Learning Data Construction\nType V olume\nMathematics 748\nScience 1557\nGeneral 1719\nTable 6: RL data composition.We first collect data from a broader range of sources [ 32,\n56,57] to ensure the generalization to different scenarios\nencompassing the general scenarios, math, and science.\nInstead of directly employing these data, we adopt the\nsampling methodology from [ 46] to cluster and curate 4K\nsamples to ensure diversity, with less overlap with the\nprevious cold start data by comparison. We illustrate the\ncomposition of the final 4K data in Table 6.\nA.7 Open Source\nTo increase the reproducibility of our work and facilitate the development of the multimodal reasoning,\nwe’ll release the data, model, and code soon.\n3\n--- Page 18 ---\nPrompt 1: Multimodal Iterative Annotation Prompt\nYou are a Checker -&-Corrector-&-Annotator of multimodal chain -of-thought answers.\nInput you will receive (always in this order)\n1. The multi-choice question with the corresponding image.\n2. The true answer label (e.g. “B”).\n3. A short, human-annotated rationale for that true answer.\n4. The model’s PREVIOUS reasoning response, formatted exactly as\n<think> . . . model’s chain-of-thought (CoT). . . </think>\n<answer> . . . model’s final letter or text answer. . . </answer>\n•Inside the <think>. . . </think> block, visual cues that the model claims to use are\nwrapped as <vcues_1> . . . </vcues_1>, <vcues_2> . . . </vcues_2>, etc.\nYour task :\nA. Verify the correctness of the previous model’s answer and reasoning against the given\nimage, true answer and human rationale.\nB. If the model’s final answer is already correct, keep the answer part.\nC. If the answer is correct but some visual cues or reasoning steps are wrong or missing,\nfix the wrong cues / steps and append the NECESSARY cues/steps according to your\nknowledge.\nD. If the answer is wrong, repair the erroneous cues / logic so that the corrected\nreasoning leads to the true answer.\nE. Preserve structure, ordering and tags as possible—modify ONLY what is necessary\nfor correctness and clarity.\nF. Keep all tag syntax unchanged (<think> . . . </think>, <answer> . . . </answer>,\n<vcues_*> . . . </vcues_*>) so the output can be parsed automatically.\nOutput format\nReturn ONE corrected response, nothing else, in exactly the same two-tag layout:\n<think>\n. . . corrected chain-of-thought with fixed <vcues_*></vcues_*>. . .\n</think>\n<answer>\n. . . single correct choice or textual answer. . .\n</answer>\nAdditional rules\n•If you remove an incorrect visual cue, replace it with the correct cue and keep the\nnumbering consistent.\n• Never fabricate content outside the scope of the provided information.\n•Be concise—do not add redundant and repeated explanations beyond what is needed\nfor a logically sound, correct solution.\nExamples\n•Example 1\n•Example 2\n4\n--- Page 19 ---\nPrompt 2: Negative Annotation Prompt\nYou are a Visual Reasoning Corrector and Annotator. Process the input <Model_Infer>\nwith these rules:\n1. **Response Segmentation**:\n- Remove the answer conclusion part in the model.\n- Then, wrap the model’s entire thought process in <think></think>.\n2. **Visual Cues Annotation**:\n- Within the <think> section, identify specific visual cue phrases (not entire paragraphs)\nand annotate each one with a tag in the format <vcues_*></vcues_*>, starting\nnumbering from 1 (i.e. <vcues_1>, <vcues_2>, . . . ).\n3. **Visual Cues Reasoning Error Diagnosis and Correction**:\n3.0. All the data to be processed now concern reasoning errors based on visual cues\nrather than errors in visual cue perception. These reasoning errors may include issues\nsuch as insufficient knowledge, over-analysis, etc.\n3.1. **During this process, do not revise the model’s previous entire originial thought\nafter annotation**\n3.2. Before the closing </think> tag, and insert a generated transitional sentence wraped\nwith <aha></aha> that conveys a message similar in meaning to: \"Let’s check each\nvisual cue and corresponding reasoning before giving the final answer. Generate the\nerror type based on the Error Pre-judgement: It looks like the visual cures are correct\nwith some reasoning error.\" (The exact wording can vary as long as the idea is the same.)\n3.3. On the next line immediately after this transitional sentence, for each visual cue\nannotated (using <vcues_*></vcues_*>) and their corresponding reasonong parts before\n<aha>, compare them with :\n- The verified rationale (<rationale>)\n- Your understanding of image\nThen, after </aha>, update the corrected reasoning based on the visual cures. If\nnecessary, replicate the relevant part from the original <vcues_*></vcues_*> tag\nalongside the revised reasoning.\n3.4. After completing the reasoning corrections, perform a logical verification of the\nreasoning after the </aha> part\n3.5. Append the final correct answer wrapped with <answer></answer>, i.e. <an-\nswer><Correct Anwer></answer>, in the next line after the </think>, ensuring that the\nfinal answer is adjusted correctly.\n4. **Output Constraints**:\n- Preserve the original reasoning structure as possible.\n- **Do not include similar phrases like \"based on the rationale\", \"The reasoning should\nfocus\", \"aligns with the rationale\", \"the model\", beacuse the processed content is used\nfor the model training instead of third-person view**\n- Ensure that all annotations (<think>, <answer>, <vcues_*>, <aha>) are properly\nformatted and inserted in the correct locations.\nExample 1:\n...\n5\n--- Page 20 ---\nPrompt 3: Verification Prompt\nYou are given a multiple-choice question with options and the image, the correct answer,\nand a generated response in the following format:\n<think>thinking process here</think>\n<answer>answer choice</answer>\nYou should align the answer choice in <answer></answer> with the choice content\nin the question, and then check whether the reasoning in <think>...<think> logically\nsupports the answer choice content.\nIf the thinking process leads to that answer choice, output 1. Otherwise, output 0 and\nexplain why it does not lead to the answer.\nB Training Details\nB.1 System Prompt\nFor the training and evaluation of the GThinker, we utilize the same system prompt to wrap the\nconversation, as shown below.\nSystem Prompt\nA conversation between User and Assistant. The user asks a question, and the Assis-\ntant solves it. The assistant first thinks about the reasoning process in the mind and\nthen provides the user with the answer. The reasoning process and answer are en-\nclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think>\nreasoning process here </think><answer> answer here </answer>. In the reasoning\nprocess enclosed within <think> </think>, each specific visual cue is enclosed within\n<vcues_*>...</vcues_*>, where * indicates the index of the specific cue. Before con-\ncluding the final answer, pause for a quick consistency check: verify whether the visual\ncues support the reasoning and whether each step logically follows from what is seen.\nIf correct, conclude the answer; otherwise, revise the visual cues and reasoning, then\nconclude.\nB.2 Hyper-parameters\nWe have illustrated the key hyper-parameters in the §4.1. In this sectin, we provide more information\nabout the hyper-parameters used in our experiment. For the DAPO, we utilize the EasyR1 framework\nfor training.\nTable 7: Hyper-parameters for Super-\nvised Fine-tuning\nName Value\nprecision bf16\nmax_seq_length 4096\nwarmup_ratio 0.1\nmax_pixels 12845056\nmin_pixels 316Table 8: Hyper-parameters for DAPO\nName Value\nmax_promp_length 15000\nmax_response_length 4096\nglobal_batch_size 64\nrollout_batch_size 64\nmax_pixels 4194304\nmin_pixels 262144\nweight_decay 1e-2\n6\n--- Page 21 ---\nC Qualitative Analysis\nThis section presents more examples to showcase the efficacy of our proposed method. As illustrated\nin Figure 6, GThinker, subsequent to our training, demonstrates the ability to augment and revise\nvisual cues during the reasoning phase, ultimately leading to the correct solution. As we demonstrated\nin §3.1, such re-evaluation of visual cues is not invariably essential. Therefore, for multimodal\nreasoning tasks, including mathematics, our pattern supports that once adequate visual information is\nassimilated, the model can engage in direct reasoning flexibly with critical reflection and verification.\nAs depicted in Figure 7, GThinker can also critically reflects upon and validates its reasoning pathway\nfrom both logical and computational standpoints to ascertain the final answer for math problems\nwith accurate visual cues identified. These instances effectively highlight the adaptability of our\nCue-Rethinking Pattern to diverse problems and tasks by accommodating varied thinking approaches,\nthereby underscoring the success of our training regimen.\nD Limitations\nTo develop a general thinking model for diverse domains, our method relies on a wide array of\nreasoning data. However, a prevailing challenge in the field is the scarcity of open-source multimodal\nreasoning QA datasets that are both sufficiently complex and visually-dependent. Furthermore,\ncurating suitable reasoning data from existing instruction datasets is often prohibitively resource-\nintensive. These broader data landscape constraints naturally impact the potential for scaling data-\nintensive approaches like ours. Despite these environmental limitations, we have curated a high-\nquality dataset of 11K instances from available sources. Under our method’s design, this has markedly\nimproved the model’s general multimodal reasoning capabilities. Our future work will focus on\ncreating more comprehensive reasoning datasets—a necessary step for the community—to further\ngeneralize our method and achieve a more broadly applicable thinking model.\nE Broader Impact\nThis paper presents work whose goal is to advance the field of MLLMs. There are many potential\nsocietal consequences of our work following the MLLMs, none of which we feel must be specifically\nhighlighted here.\n7\n--- Page 22 ---\nWhat is happening in the picture?\nA. A man is riding his bike\nB. The train has stopped at a small station\nC. The train will pass by the gates\nD. The train has broken down\n<think>To answer this question, let's analyze the image step by step:\n1. <vcues_1>The image shows a steam train on the tracks</vcues_1>.\n2. <vcues_2>There is a small green building on the right side of the image, which appears \nto be a signal box or a small station</vcues_2>.\n3. <vcues_3>The train is moving on the tracks, and there is no indication that it has \nstopped at a station</vcues_3>.\n4. <vcues_4>There are no gates visible in the image, and the train is not shown breaking \ndown</vcues_4>.\nLet's verify each visual cue and its reasoning before finalizing the answer.\nSome visual cues seem to need adjustment.\n<vcues_2>The small green building on the right side of the image is a signal box, which is \ntypically used to control train movements</vcues_2>. This suggests that the train is \npassing by a small station.\n<vcues_4>There are gates visible in the image, and the train is on the tracks, which \nindicates that the train will pass by the gates</vcues_4>.\nWith the corrected visual cues, the most likely scenario is that the train is passing by the \ngates.</think>\n<answer>C</answer>Figure 6: Response sample from GThinker with visual cue-based rethinking enabled.\n8\n--- Page 23 ---\n2. Use the properties of parallel lines:\n   - Since AD is parallel to BC, the angle formed by the transversal and line AD will be related to the angle \nformed by the transversal and line BC.\n3. Determine the relationship between the angles:\n   - <vcues_4>Angle 2 is an exterior angle to the triangle formed by the transversal and lines AD and \nBC</vcues_4>.\n   - The angle 78° is an interior angle of the triangle, and angle 1 is 42°.\n   - The exterior angle (angle 2) is the sum of the two non-adjacent interior angles of the triangle.\n4. Calculate the angle:\n   - <vcues_5>Angle 2 = 78° - 42° = 36°</vcues_5>.\nHowever, upon reevaluating the problem, it appears there might be a misunderstanding in the \ninterpretation of the angles. The correct approach is to use the fact that angle 2 is the exterior angle to \nthe triangle formed by the transversal and the lines AD and BC.\n- <vcues_6>Angle 2 is the exterior angle to the triangle formed by the transversal and the lines AD and \nBC</vcues_6>.\n- The exterior angle is equal to the sum of the two non-adjacent interior angles of the triangle.\nGiven the angles:\n- <vcues_7>Angle 78° is an interior angle of the triangle</vcues_7>.\n- <vcues_8>Angle 1 is 42°</vcues_8>.\nThe exterior angle (angle 2) is the sum of the two non-adjacent interior angles:\n- <vcues_9>Angle 2 = 78° - 42° = 36°</vcues_9>.\nHowever, upon rechecking the problem statement and the options, it appears there might be a mistake in \nthe interpretation of the angles. The correct angle should be calculated as:\n- <vcues_10>Angle 2 = 180° - 78° = 102° - 42° = 60°</vcues_10>.\nGiven the options, the correct answer is:\n- <vcues_11>Angle 2 = 60°</vcues_11>.\nTherefore, the correct choice is C.\nWait—cross-reference those cues with the logic.\nThe reasoning seems to hold up with what’s been seen.</think>\n<answer>C</answer>\n<think>To solve this problem, we need to use the properties of parallel \nlines and the angles formed by a transversal.\n1. Identify the given information:\n   - <vcues_1>AD is parallel to BC</vcues_1>.\n   - <vcues_2>Angle 1 is 42°</vcues_2>.\n   - <vcues_3>Angle 78° is an angle formed by the transversal and line \nAD</vcues_3>.Figure 7: Response sample from GThinker with reflection for the math problem.\n9",
  "text_length": 74572
}