{
  "id": "http://arxiv.org/abs/2506.00911v1",
  "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives\n  in Language Models",
  "summary": "Modern language model deployments must often balance competing objectives,\nfor example, helpfulness versus harmlessness, cost versus accuracy, and reward\nversus safety. We introduce Conformal Arbitrage, a post hoc framework that\nlearns a data driven threshold to mediate between a Primary model optimized for\na primary objective and a more conservative Guardian which could be another\nmodel or a human domain expert aligned with a guardrail objective. The\nthreshold is calibrated with conformal risk control, yielding finite sample,\ndistribution free guarantees that the long run frequency of undesirable events,\nsuch as factual errors or safety violations, does not exceed a user specified\nquota. Because Conformal Arbitrage operates wholly at the API level, without\nrequiring access to model logits or updating model weights, it complements\nweight based alignment techniques and integrates seamlessly with existing cost\naware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,\nallowing users to define an acceptable performance level for one objective\nwhile maximizing utility in another. We observe that our method outperforms, in\nterms of accuracy, cost matched random routing between models. These properties\nmake Conformal Arbitrage a practical, theoretically grounded tool for\ntrustworthy and economical deployment of large language models across a broad\nrange of potentially competing objectives.",
  "authors": [
    "William Overman",
    "Mohsen Bayati"
  ],
  "published": "2025-06-01T08:55:10Z",
  "updated": "2025-06-01T08:55:10Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00911v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00911v1  [cs.AI]  1 Jun 2025Conformal Arbitrage: Risk-Controlled Balancing of Competing\nObjectives in Language Models\nWilliam Overman\nGraduate School of Business\nStanford University\nwpo@stanford.eduMohsen Bayati\nGraduate School of Business\nStanford University\nbayati@stanford.edu\nJune 3, 2025\nAbstract\nModernlanguage -modeldeploymentsmustoftenbalance competing objectives —forexample, helpfulness\nversus harmlessness, cost versus accuracy, and reward versus safety. We introduce Conformal Arbitrage ,\na post-hoc framework that learns a data -driven threshold to mediate between a Primary model optimized\nfor a primary objective and a more conservative Guardian —which could be another model or a human\ndomain expert—aligned with a guardrail objective. The threshold is calibrated with conformal risk\ncontrol, yielding finite -sample, distribution -free guarantees that the long -run frequency of undesirable\nevents (such as factual errors or safety violations) does not exceed a user -specified quota. Because\nConformal Arbitrage operates wholly at the API level—without requiring access to model logits or\nupdating model weights—it complements weight -based alignment techniques and integrates seamlessly\nwith existing cost -aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier, allowing\nusers to define an acceptable performance level for one objective while maximizing utility in another. We\nobserve that our method outperforms (in terms of accuracy) cost-matched random routing between models.\nThese properties make Conformal Arbitrage a practical, theoretically grounded tool for trustworthy and\neconomical deployment of large language models across a broad range of potentially competing objectives.\n1 Introduction\nLarge language models (LLMs) excel at reasoning, coding, and open -domain question answering, yet real -world\ndeployments frequently need to navigate tensions between potentially competing objectives such as helpfulness\nandharmlessness orcostandaccuracy.\nCurrent practices mostly tackle the tension between helpfulness and harmlessness by modifying the\nmodel itself : reinforcement learning from human feedback (RLHF) [Christiano et al., 2017, Ouyang et al.,\n2022], direct–preference optimisation (DPO) [Rafailov et al., 2023], Constitutional AI [Bai et al., 2022b],\nor multi-objective fine -tuning [Zhou et al., 2023, Wang et al., 2024] each produce a singleoperating point\nalong the Pareto frontier. While powerful, these methods demand expensive data collection, GPU -intensive\nretraining, and — for API-only models — are often not applicable.\nFor the cost versus accuracy tradeoff, there has been significant work on cascades: a cheap model handles\neasy queries and defers the rest to a stronger fallback [Chen et al., 2023, Aggarwal et al., 2025, Zellinger et al.,\n2025]. Recently, Jung et al. [2025] introduced Cascaded Selective Evaluation (CSE), calibrating per-model\nconfidence estimators via fixed-sequence multiple testing to obtain rigorous guarantees on alignment to human\npairwise preferences. However, these approaches are tailored for controlling a binary disagreement risk, while\na user may be interested in controlling arbitrary guardrail metrics at deployment time.\nWe introduce Conformal Arbitrage (CA) , a lightweight router that sits outsidethe language models.\nThe term “arbitrage” captures how our approach exploits the performance gap between specialized models to\nachieve superior outcomes than naive selection between models. Given (i)aPrimary model optimized for\nthe primary objective and (ii)a more conservative Guardian model or a human domain expert, aligned with\na guardrail objective, CA offers a principled alternative to randomized routing between models. Instead of\n1\n--- Page 2 ---\nmerely alternating between models with some probability, CA learns a single scalar threshold on how strongly\nthe Primary model favors its top choice over alternatives (a notion we formally define as “score gap” later\nin the paper). This threshold determines when the Primary model’s confidence is sufficient to act upon its\nprediction versus when to defer to the Guardian, creating a principled decision boundary that optimizes the\ntrade-off between objectives.\nThe threshold is calibrated using conformal risk control (CRC) [Angelopoulos et al., 2024], yielding\nfinite-sample, distribution -free guarantees that the long -run frequency (or magnitude) of undesirable events\nnever exceeds a user -specified budget α. This enables precise control over trade-offs—users can explicitly\nspecify how much they are willing to compromise on one objective to gain on the other. Because CA touches\nno model weights , it complements weight -based alignment and applies to closed, black -box APIs, making it a\nremarkably lightweight approach to achieving Pareto improvements over simple model selection strategies.\nOur experiments examine both the the cost versus accuracy tradeoff using the the TruthfulQA and MMLU\nbenchmarks as well as the helpfulness versus harmlessness tradeoff using the PKU-SafeRLHF benchmark.\nAcross all settings CA traces an efficient frontier that outperforms random or cost -matched routing baselines.\nConformal Arbitrage transforms an immutable, potentially unpredictable LLM (or a family of LLMs)\ninto a controllable system whose risk–utility position can be dialed after deployment . In our experiments, we\ndemonstrate this capability using state-of-the-art LLMs from the GPT-4.1 series, OpenAI [2025], showing how\nour method enables fine-grained control over various tradeoffs without modifying the underlying models. By\nrequiring only a few hundred logged examples for calibration, CA offers a pragmatic path toward trustworthy,\ncost-efficient and customizable language -model services that can be adjusted to meet evolving requirements\nlong after initial deployment.\n2 Related work\nReal–world deployments must strike a pragmatic balance between helpfulness —supplying users with accurate\nand detailed information—and harmlessness —avoiding policy-breaking or dangerous content. Early alignment\nwork framed the problem as a single–objective optimization: RLHF [Christiano et al., 2017, Ouyang et al.,\n2022] and its variant DPO [Rafailov et al., 2023] collapse nuanced feedback into a singlereward model\nand therefore deliver one operating point on the Pareto frontier. Subsequent methods introduced explicit\ntwo–factor training: RLHF on mixed helpful–harmless datasets [Bai et al., 2022a], Constitutional AI’s\nself-revision loop [Bai et al., 2022b], and Bi -Factorial Preference Optimisation (BFPO) [Zhang et al., 2025]\nthat casts the bi -objective RLHF loss as a direct supervised criterion. Safe -RLHF [Dai et al., 2023] separates\na reward and a cost head and enforces constraints by Lagrangian relaxation, while Circuit Breakers intervene\nat generation time to halt policy-violating continuations [Zou et al., 2024].\nThe PKU-SafeRLHF benchmark [Ji et al., 2023] was specifically introduced to quantify this helpfulness-\nharmlessness trade-off, providing dual annotations that enable researchers to measure progress on both\ndimensions simultaneously. Anthropic’s Constitutional AI [Bai et al., 2022b] further explores alignment by\nembedding principles directly into model training. More recently, the MoGU framework [Du et al., 2024]\ndynamically routes between model variants optimized separately for usability and safety. Empirically, while\nthese approaches curb unsafe completions, they still lock the model into one fixed balance point between\nhelpfulness and harmlessness.\nBeyond helpfulness–harmlessness many other objectives— accuracy, cost, latency, fairness, demographic\nparity, domain–specific risk, etc.—can be in conflict. Many recent works have proposed weight–based strategies\nto navigate the resulting frontiers between such competing objectives. Rewarded soups linearly interpolates\ncheckpoints fine -tuned on distinct rewards to trace that surface [Ramé et al., 2023], Directional Preference\nAlignment adds multiple reward heads for steerable inference [Wang et al., 2024], MaxMin -RLHF learns a\nmixture of reward models to protect minority preferences [Chakraborty et al., 2024], and MO -DPO converts\nseveral preference signals into a closed -form multi -objective loss [Zhou et al., 2023]. These approaches\nnevertheless share two limitations: (i)they require access to model weights and retraining, and (ii)they\nprovide no theoretical guarantees that the inherent guardrail metrics driving the trade -off (e.g., safety,\naccuracy, or cost) will stay within a user-specified budget.\nIn contrast, our method of Conformal Arbitrage is weight -agnostic and sits outsidethe LLM. By calibrating\na single threshold with conformal risk control [Angelopoulos et al., 2024], it transforms any pair of black -box\n2\n--- Page 3 ---\nmodels, one of which can be a human, into a continuum of operating points with provable finite-sample\nbounds on the chosen guardrail metric (e.g. harmlessness).\nConformal Arbitrage is thus closely tied to routing and cascade approaches that tackle cost–accuracy\ntrade-offs [Chen et al., 2023, Yue et al., 2024, Ong et al., 2024, Aggarwal et al., 2025, Zellinger et al., 2025,\nVarangot-Reille et al., 2025], but can be used to tackle any potential pair of objectives that may be in tension,\nthus abstractly covering cost–accuracy cascades as a special case.\nHowever, unlike these previous approaches we make no particular optimizations for any specific trade-off,\nincluding cost and accuracy, and we do not claim to out-perform such cascade systems on metrics for which\nthey are explicitly optimized. Furthermore, compared to most routing approaches that rely on complex\nlearned functions to distribute queries between models [Varangot-Reille et al., 2025], Conformal Arbitrage\nemploys a principled, theoretically-grounded method using a single calibrated scalar threshold.\nScalable-oversight research explores how weaker agents or humans can be organized into critique hierarchies\nthat amplify limited supervision. Amplification and Debate delegate verification to inexpensive judges and,\nunder certain complexity assumptions, achieve provable “weak-to-strong” guarantees [Christiano et al., 2018,\nIrving et al., 2018, Burns and et al., 2023]. Process supervision instead labels intermediate reasoning\nsteps so that mistakes are caught early [Lightman et al., 2023]. Self-reflection frameworks ask a model to\ngenerate critiques (and often revisions) of its own outputs [Madaan et al., 2023, Yang et al., 2024, Tang\net al., 2024]. Post-hoc risk control strategies in model deployment have also gained attention, particularly\nthrough moderation and oversight models deployed by industry leaders [OpenAI, 2023]. Conformal Arbitrage\ncomplements these approaches with a statistically sound escalation rule: the Primary acts autonomously\nwithin a risk budget, else forwards a slimmed-down slate of actions to a human or Guardian. Finite-sample\nbounds from conformal risk control budget both the Guardian’s load and the residual risk, giving a lightweight\npost-hoc route to scalable oversight without retraining.\nThe underlying selective routing approach of our work resonates with classical selective prediction and\nreject-option frameworks initially formalized by Chow [1970] and later refined in modern selective classification\nresearch [Geifman and El-Yaniv, 2019].\nConformal prediction (CP) and its generalization, conformal risk control (CRC) [Vovk et al., 2005, Bates\net al., 2021, Angelopoulos et al., 2024], provide distribution-free, finite-sample guarantees that make them\ngenerally attractive post-hoc alignment tools for high-stakes LLM deployments. For instance, Chen et al.\n[2025] align language models with human risk judgments by controlling tail risks such as toxicity, while Su\net al. [2024] demonstrate conformal prediction applied effectively to black-box LLM APIs without internal\naccess. Additionally, conformal risk control has been leveraged in deployment scenarios such as action deferral,\nillustrated by the KnowNo framework [Ren et al., 2023], which uses conformal uncertainty quantification to\ntrigger human oversight.\nConformal prediction and conformal risk control have been used to filter low-confidence QA answers\n[Kumar et al., 2023], retain only entailment-supported sub-claims [Mohri and Hashimoto, 2024], and bound\nhallucination rates via abstention [Abbasi-Yadkori et al., 2024]. Beyond marginal guarantees, conditional and\nadaptive CRC tighten coverage on hard prompts [Cherian et al., 2024], and sampling-based set prediction\nextends CP to free-text generation [Quach et al., 2024]. Framing alignment as property testing, Overman\net al. [2024] calibrate outputs to satisfy safety or fairness constraints without retraining. Building on this\nlineage, we adapt CRC to learn a risk-calibrated switch between a Primary model and a Guardian model\nwithout retraining either model.\nConformal Arbitrage is most closely related to Cascaded Selective Evaluation (CSE) of Jung et al. [2025].\nCSE equips each judge with a confidence score, calibrates a per-judge threshold, and escalates through\na cascade until some judge is confident, thereby controlling the Bernoulli risk that a machine-preferred\nanswer disagrees with human majority. Conformal Arbitrage addresses more general tradeoffs: it controls\nanybounded guardrail loss (safety, accuracy, cost, latency, etc.) and can filter a large action space to a\nsmaller candidate set that a Guardian or human refines, rather than abstaining on the whole instance. CSE’s\nSimulated Annotators requires K-shot prompting (for Kexamples of preference annotations) the model N\ndifferent times (for Nhuman annotators) in order to obtain an ensemble prediction andaccess to predictive\nprobabilities extracted from the model’s logprobs , so every judge call is multiplied many-fold and is limited\nto APIs that expose token-level logits. Conformal Arbitrage, by contrast, needs at most onecall to the\nPrimary and (when routed) oneto the Guardian, treats the returned scores as opaque, requiring no access to\nlogits or probabilities, and thus works with strictly black-box APIs.\n3\n--- Page 4 ---\n3 Preliminaries\nConformal Arbitrage uses conformal risk control (CRC) to supply finite -sample, distribution -free guarantees\non the guardrail metric while treating the underlying language models as black boxes. CRC extends the\nframework of conformal prediction (CP) [Vovk et al., 2005, Bates et al., 2021] from binary error control to\ncontrol of arbitrary bounded risks . We briefly summarize both ideas.\nConformal prediction LetXandYbe the input and output spaces, equipped with a joint probability\ndistribution, and draw an exchangeable sample (Xi, Yi)n+1\ni=1∼Pwhere the first nsample are used for\ncalibration, and (Xn+1, Yn+1)is used for testing. Given any predictor f:X → Yand score sf(x, y)(e.g.\n|y−f(x)|), let q1−αbe the (1−α)empirical quantile of {sf(Xi, Yi)}n\ni=1. The conformal set is defined by\nC(x) ={y∈ Y:sf(x, y)≤q1−α}, and enjoys the finite-sample guarantee Pr{Yn+1/∈C(Xn+1)} ≤α. Thus\nany black-box predictor attains (1−α)coverage without distributional assumptions [Vovk et al., 2005, Bates\net al., 2021].\nConformal risk control Many real -world objectives are not binary mistakes but expectations of a\ntask-specific loss—for example, safety -violation rate, factual errors, mean latency, or excess dollar cost. Con-\nformal risk control [Angelopoulos et al., 2024] handles such objectives by introducing a bounded, non -increasing\nloss curve Li(λ)∈[0, B], where Bis an upper bound on the loss, for each calibration point, indexed by a\ntunable threshold λ∈Λ⊂R. Defining the empirical risk ˆRn(λ) =1\nnPn\ni=1Li(λ), CRC selects\nˆλ= infn\nλ∈Λ :n\nn+ 1ˆRn(λ) +B\nn+ 1≤αo\n, (1)\nand proves the finite-sampleguarantee for E\u0002\nLn+1(ˆλ)\u0003\n≤α,again under assumption of exchangeability\nbetween the calibration data and test point. Choosing Li(λ) =I{Yi/∈Cλ(Xi)}recovers classical CP;\nalternative losses yield risk bounds tailored to deployment needs.\n4 Methodology: conformal arbitrage\nWe aim to invoke a Primary model as often as possible (e.g. a helpfulness-maximizing or low-cost model)\nwhile ensuring, with high confidence, that a critical requirement (e.g. harmlessness, accuracy) is satisfied by\nrouting calls to a Guardian model (or human) as needed. The linkage between the two models is formalized\nthrough conformal risk control [Angelopoulos et al., 2024].\n4.1 Setting\nLet{xi}i≥1be an exchangeable sequence of X-valued random variables that we refer to as contexts. Each\ncontext xadmits a finite, non-empty action set A(xi) =Ai⊆ A, where |A(xi)|<∞.Additionally, we\nassume the existence of two functions L:X × P (A)→RandU:X × P (A)→R, measuring, over subsets\nof the potential actions, loss for the guardrail metric and utility for the primary metric, respectively. We\nassume both of these functions satisfy the property that for A1⊆ A 2we have L(x,A1)≥L(x,A2)and\nU(x,A1)≥U(x,A2).\nWe assume access to two fixed, pre-trained models: p, g:X × A → R, where pis thePrimary model\n(reward-seeking or cheap/low-accuracy) and gis theGuardian model (safety-focused or costly/high-accuracy).\nDespite this simple interface, each model may internally implement arbitrarily complex computations—any\narchitecture that outputs a score for each (x, a)pair is admissible.\nAlthough we write p(x, a)andg(x, a)as deterministic, each model call may depend on internal randomness\nζP, ζG, producing scores ˜p(x, a, ζ P)and˜g(x, a, ζ G). Such tuples (x,˜p,˜g)remain exchangeable across samples,\nso the finite-sample guarantees of conformal risk control are unaffected.\n4\n--- Page 5 ---\n4.2 Calibration via conformal risk control\nTo calibrate our Conformal Arbitrage policy, we use conformal risk control (CRC) to calibrate a relaxation\nparameter ˆλthat satisfies a user-defined risk budget α∈(0,1), controlling how much we can trust the\nPrimary model before deferring to the Guardian.\nWe begin with an exchangeable calibration set of nsamples:\nD(n)=\b\n(xi, Pi, Gi)\tn\ni=1, P i={p(xi, a)}a∈Ai, G i={g(xi, a)}a∈Ai.\nEach sample consists of a context xiand the scores assigned by both the Primary model and the Guardian\nmodel across the available action set Ai=A(xi).\nFor any λ≥0, we define the λ-relaxed candidate set :\nCλ(x) =n\na∈A(x) :p(x, a)≥max\na′∈A(x)p(x, a′)−λo\n.\nThis set includes all actions whose Primary scores are within λof the top score. In particular, larger values\nofλincrease the size of this set. Since all of the subsets A′⊆A(x)that we will consider will be of this form,\nCλ(x), for some λ, we adopt the notation Li(λ) =L(xi, Cλ(xi))andUi(λ) =U(xi, Cλ(xi))\nWe then define a loss function on each calibration sample, measuring the residual risk that the Guardian\nmodel would assign to the best action in Cλ(xi):\nLi(λ) = max\na∈A(xi)g(xi, a)−max\na∈Cλ(xi)g(xi, a). (2)\nIntuitively, this loss captures how unsafe the most promising action (as judged by the Guardian) is among\nthe candidates the Primary model would consider acceptable under λ.\nTo summarize overall risk, we compute the empirical average:\nˆRn(λ) =1\nnnX\ni=1Li(λ),\nand select the smallest λthat satisfies the CRC inequality:\nˆλ= infn\nλ≥0 :n\nn+ 1ˆRn(λ) +1\nn+ 1≤αo\n. (3)\nDefinition 1 (Relaxation Parameter) .The relaxation parameter ˆλis defined as the minimal value of λthat\nsatisfies the conformal risk control inequality in Equation 3.\nThis relaxation parameter controls the permissiveness of the candidate action set while ensuring that the\nexpected residual risk on a new context remains bounded by α. The guarantee holds exactly at finite sample\nsize and requires no assumptions on score calibration or context distribution.\n4.3 Conformal arbitrage algorithm\nWe now describe the deployment-time decision procedure for selecting actions using the calibrated relaxation\nparameter ˆλobtained in Section 4.2. At each test instance, the agent first consults the Primary model to form\naˆλ-relaxed candidate set. If the top action is sufficiently dominant (i.e., the set is a singleton), it is selected;\notherwise, the Guardian model selects from the λ-relaxed set. The procedure is outlined in Algorithm 1.\n4.4 Optimality amongst score-gap routers\nThe fact that Algorithm 1 ensures an upperbound on the loss of our guardrail metric E[L(x, Cλ(x))]≤α\nsimply follows from the guarantees of conformal risk control Angelopoulos et al. [2024]. Now to address utility\nas measured by the primary metric we define the following class of policies, “Score-gap routers,\" in Definition\n2. Additionally, for this theoretical result, we will require a stronger assumption of i.i.d. on the calibration\ndata and test point.\n5\n--- Page 6 ---\nAlgorithm 1 Conformal Arbitrage\nRequire: Context x, relaxation parameter ˆλ, Primary model p, Guardian model g\n1:Compute p(x, a)for all a∈ A(x)\n2:LetCλ(x) =n\na∈A(x) :p(x, a)≥max a′p(x, a′)−ˆλo\n3:if|Cλ(x)|= 1then\n4:return the unique element of Cλ(x)\n5:else\n6:Compute g(x, a)for all a∈Cλ(x)\n7:return a⋆= arg max a∈Cλ(x)G(a)\n8:end if\nDefinition 2 (Score-gap router) .Fix aPrimary score function p:X ×A → Rand a non–negative threshold\nλ≥0. For each context xlet\na⋆(x) = arg max\na∈A(x)p(x, a),∆(x) = p\u0000\nx, a⋆(x)\u0001\n− max\nb∈A(x)\\{a⋆(x)}p(x, b),\nwith the convention ∆(x) = +∞if|A(x)|= 1. Thescore-gap router with threshold λ,Rλ:X → A∪{ defer }\nacts as\nRλ(x) =(\na⋆(x),if∆(x)≥λ,\ndefer ,otherwise ,\nwhere defermeans “forward this instance to the Guardian model.”\nGiven the Primary model’s confidence scores p(x, a), it chooses the top -scoring action whenever its margin\nover every alternative exceeds a non -negative threshold λ, anddefersto the Guardian otherwise. This\nrule mirrors Chow’s Bayes-optimal reject-option classifier [Chow, 1970]: rather than rejecting an uncertain\ninstance we escalate it to a more conservative model.\nTheorem 1 establishes that no other Score-gap router of the Primary scores alone can deliver strictly\nhigher expected primary utility while still obeying the same guardrail risk budget α, up to a vanishing O(n−1)\nterm. We let our Primary metric be measured by U(λ) =E[Ui(λ)], which we assume to be non-increasing\nandK-Lipschitz. This is natural as raising λcan only shrink the set of contexts on which we choose the\nPrimary model’s output. The proof of Theorem 1 is provided in Appendix A.\nTheorem1 (Utility–optimalityofConformalArbitrage) .Fix a compact interval Λ = [0 , λmax]. For each λ∈Λ\nand every observation idefine a guardrail loss Li(λ)∈[0, B]and a primary-utility score Ui(λ)∈[0, Umax],\nboth non-increasing in λ. Write\nR(λ) =E[Li(λ)], U (λ) =E[Ui(λ)].\nAssume Ris continuous and strictly decreasing, and Uis non-increasing and K-Lipschitz. For a desired risk\nbudget α∈(0, B)letλ⋆= inf{λ∈Λ :R(λ)≤α}.Given an i.i.d. calibration sample D(n)of size n, set\nbRn(λ) =1\nnnX\ni=1Li(λ), ˆλ= infn\nλ∈Λ :n\nn+1bRn(λ) +B\nn+1≤αo\n.\nThen, with expectation taken over the calibration sample\nE\u0002\nU(λ⋆)−U(ˆλ)\u0003\n=O(n−1), (4)\nEh\nsup\n˜λ∈Λ\nR(˜λ)≤αU(˜λ)−U(ˆλ)i\n=O(n−1). (5)\n6\n--- Page 7 ---\n5 Experiments\nWe test Conformal Arbitrage on two different trade-off settings: a cost–accuracy axis using the multiple-\nchoice datasets TruthfulQA and MMLU, and a helpfulness–harmlessness axis using PKU-SafeRLHF.\nEach experiment follows the same protocol: we draw a calibration split and use the loss given by Equation 2\nto fit the CRC threshold ˆλusing Equation 3. We evaluate the guardrail risk and primary utility of Conformal\nArbitrage on a disjoint evaluation split, and compare against single-model baselines and random routers.\nWe report the results for TruthfulQA and PKU-SafeRLHF in the main text; the results for MMLU are\nqualitatively similar and appear in Appendix D.\n5.1 TruthfulQA: cost versus accuracy\nWe first study Conformal Arbitrage on the multiple -choice split of TruthfulQA [Lin et al., 2022], a\nbenchmark designed to expose factual misconceptions in language models.1The benchmark contains 684\nquestions, each paired with four answer choices and exactly one correct label. Here we consider our primary\nobjective to be minimizing cost, while the guardrail metric is factual accuracy.\nExperimental set-up The Primary model is gpt-4.1-nano-2025-04-14 ; the Guardian model is its larger\ncounterpart gpt-4.1-2025-04-14 . This is the natural choice considering that our primary and guardrail\nmetrics are cost and accuracy, respectively.2Both are queried in a zero -shot, multiple -choice format that\nelicits a real -valued confidence score in [0,1]for each option. We use temperature=0.1 ,max_tokens=50 ;\nreplies that fail JSON parsing default to uniform scores, maintaining exchangeability. Exact prompts appear\nin Appendix B.1.\nWe keep the Primary’s raw scores, but binarize the Guardian’s as g(x, a) = 1ifais its top-ranked\nanswerandcorrect, and 0otherwise. Thus, when the Guardian answers correctly we assign confidence\n1 to the correct choice and 0 to the three distractors; when it answers incorrectly we assign 0 to every\nchoice, reflecting total uncertainty. This binarization is not required —one could instead feed the Guardian’s\nreal-valued scores into Conformal Arbitrage, but this binarization makes the exposition crisper: the calibrated\nrisk level αnow translates directly to an α×100%drop in accuracy relative to the accuracy of the Guardian.\nSee Appendix B.4 for results of using the real-valued scores directly. With Equation 2 the loss is Li(λ) =\n1{Guardian correct and Cλ(xi)̸∋a⋆}fora⋆=arg max a∈A(xi)g(xi, a). Conformal risk control chooses the\nsmallest ˆλwhose empirical mean loss is ≤α; e.g., α= 0.10guarantees the overall accuracy falls by at most\nten percentage points relative to an always-Guardian policy.\nEach trial draws n= 400calibration and N= 284test questions. We fit ˆλvia Eq. (3)onΛ =\n{0,0.01, . . . , 1}and repeat the calibration–evaluation loop 30 times with fresh random splits.\nFor a baseline comparison we compare the performance of Conformal Arbitrage to a random router that\nfor each risk level αmatches the average cost of our method but chooses the acting model uniformly at\nrandom, thereby controlling cost without calibration.\nResults Figure 1 and Table 1 show that CA traces an efficient cost–accuracy frontier, beating the cost-\nmatched random router at every risk level except α= 0.25while always respecting the α-level guardrail\nbudget. Tightening αfrom α= 0.25to0.05raises accuracy from 0.62to0.81at2.6×the cost. These results\ndemonstrate that statistical calibration—not mere stochastic routing—is essential for efficiency.\nAblation studies Across ablations CA’s frontier stays stable. First, varying the calibration split (300, 400,\n500 points; Appendix B.3) lifts accuracy by only a point or two with flat cost, matching theory that a few\nhundred examples suffice [Angelopoulos and Bates, 2022]. Second, feeding CA the Guardian’s raw scores\ninstead of the 0/1 binarization nudges accuracy up under tight risk budgets and down by a similar amount\nwhen the budget loosens (Appendix B.4). Third, letting the Guardian operate on the fullaction set rather\nthan the ˆλ-relaxed subset (unrestricted routing, Appendix B.5) raises accuracy a few points at roughly 10%\nextra cost; because the Primary still acts on the same contexts while the Guardian’s menu only expands, the\n1https://huggingface.co/datasets/EleutherAI/truthful_qa_mc\n2We use prices from https://openai.com/api/pricing/ on May 15, 2025.\n7\n--- Page 8 ---\nFigure 1: Accuracy vs. cost (TruthfulQA), mean ±1 std over 30 trials; small points show individual CA runs.\nTable 1: Accuracy, cost per 1000 examples, ˆλ,∆above random baseline, and Guardian usage (mean ±std\nover 30 trials). Calibration size n= 400.\nPolicy Accuracy Cost ($/1000) ˆλ ∆Guardian %\nPrimary 0.559±0.015 0 .032±0.000 – – 0.0%\nCA ( α= 0.25)0.621±0.025 0 .188±0.024 0 .277±0.067 −0.011 27 .7±3.9%\nCA ( α= 0.20)0.672±0.025 0 .234±0.033 0 .403±0.058 +0 .019 34 .3±5.3%\nCA ( α= 0.15)0.714±0.024 0 .302±0.035 0 .529±0.059 +0 .029 44 .9±5.7%\nCA ( α= 0.10)0.766±0.017 0 .407±0.026 0 .706±0.031 +0 .031 62 .1±4.4%\nCA ( α= 0.05)0.806±0.017 0 .521±0.035 0 .867±0.040 +0 .016 78 .9±5.6%\nGuardian 0.833±0.011 0 .620±0.001 – – 100.0%\nfinite-sample risk bound is unaffected, though the primary metric (cost) can overshoot the target. Finally,\nswapping the Primary gpt-4.1-nano for the stronger but pricier gpt-4.1-mini (Appendix B.6) lifts the\nlow-cost end of the frontier by about 0.22 accuracy points. CA still beats a cost-matched random router, but\nthe margin narrows as the capability and cost gap between models decreases.\n5.2 PKU-SafeRLHF: helpfulness versus harmlessness\nWe consider how Conformal Arbitrage can be applied to the tradeoff between helpfulness and harmlessness.\nThePKU-SafeRLHF corpus contains ∼90k prompts, each paired with two distinct LLM responses.3Each\nresponse is annotated for (i) which response is more helpful , (ii) which is safer, and (iii) a severity label\nsev∈ {0,1,2,3}indicating the extent of the safety violation (higher is worse). We retain only the rows where\nthe two responses differ in severity level and where the more helpful answer is notthe safer answer. These\nare essentially the hardest cases that demonstrate the conflict between helpfulness and harmlessness. This\nleaves N= 3,552examples.\n3https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF\n8\n--- Page 9 ---\nExperiment set-up We report two quantities: Empirical Human Alignment , the fraction of prompts\nwhose chosen answer matches the annotator-preferred (more-helpful) reply, and Safety-violation loss , the\nexcess severity of the selected answer above the safer one ( 0is ideal, larger is worse). The Primary model\ngpt-4.1-2025-04-14 is instructed (Appendix C) to assign a real-valued helpfulness score p(x, a)∈[0,1]to\nevery candidate reply while ignoring policy concerns. The Guardian is a deterministic rule that selects the\nlowest-severity reply, g(x, a) = 1{sev(x, a) = min a′sev(x, a′)}, mimicking a human safety judge.\nOver 30 trials we draw 500/500 calibration–evaluation splits from the 3,552 prompts, tune ˆλonΛ =\n{0,0.0025, . . . , 1}, and evaluate at risk budgets α∈ {0.10,0.20, . . . , 0.60}. Baselines are (i) Primary-only\n(arg max ap(x, a)), (ii)Guardian-only (lowest-severity reply), and (iii) a random router that calls the Guardian\nwith p∈ {0.2,0.4,0.5,0.6,0.8}.\nResults Fig. 2 shows that Conformal Arbitrage traces an efficient frontier between helpfulness and harm-\nlessness. Exact numerical results are given in Appendix C.2. The mean of every CA model dominates the\nlinear interpolation between the Primary and Guardian models that can be obtained via randomized routing.\nAdditionally CA meets the finite -sample guarantee E[L]≤αfor every guardrail budget α, as indicated by the\nmean of each point falling to the left of its corresponding vertical target.\nFigure 2: Harmfulness vs. helpfulness (PKU-SafeRLHF), mean ±1 std over 30 trials.\n6 Conclusion\nConformal Arbitrage converts a fixed pair of black-box language models (or a model–human pairing) into\na continuum of operating points on a frontier of competing objectives. By calibrating a single score-gap\n9\n--- Page 10 ---\nthreshold with conformal risk control, CA supplies finite-sample, distribution-free guarantees that a user-\nchosen guard-rail metric stays within budget while maximizing a second objective such as accuracy, helpfulness,\nor cost efficiency. Empirical results show CA outperforms cost and risk matched random routing, recovers\nmost gains of the stronger model at a fraction of the cost, and works with closed-API deployments without\naccessing weights or logits.\nLimitations & future work Our study is confined to multiple-choice tasks; applying Conformal Arbitrage\nto free-text generation would require bespoke loss functions. We forgo task-specific optimizations (e.g.,\ncost–accuracy tuning), deferring comparisons with specialized cascade systems. We analyze only a single-step,\ntwo-model router—deeper cascades may be possible. Next steps include (i) integrating adaptive CRC [Blot\net al., 2025], (ii) adding tailored optimizations to benchmark against state-of-the-art cascades, and (iii)\nextending CA to multi-model cascades and agentic pipelines.\nReferences\nYasin Abbasi-Yadkori, Ilja Kuzborskij, David Stutz, András György, Adam Fisch, Arnaud Doucet, Iuliya\nBeloshapka, Wei-Hung Weng, Yao-Yuan Yang, Csaba Szepesvári, Ali Taylan Cemgil, and Nenad Tomasev.\nMitigating llm hallucinations via conformal abstention. arXiv preprint arXiv:2405.01563 , 2024. URL\nhttps://arxiv.org/abs/2405.01563 .\nPranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou,\nAditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui,\nand Mausam. Automix: Automatically mixing language models. In Proceedings of the 38th Conference on\nNeural Information Processing Systems (NeurIPS) , 2025. arXiv:2310.12963.\nAnastasios N. Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-\nfree uncertainty quantification, 2022. URL https://arxiv.org/abs/2107.07511 .\nAnastasios Nikolas Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk\ncontrol. In The Twelfth International Conference on Learning Representations , 2024.\nYuntaoBai, AndyJones, KamalNdousse, AmandaAskell, AnnaChen, NovaDasSarma, DawnDrain, Stanislav\nFort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly,\nSheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,\nShauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam\nMcCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with\nreinforcement learning from human feedback, 2022a. URL https://arxiv.org/abs/2204.05862 .\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah,\nDanny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr,\nJared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael\nSellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson,\nSam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy\nTelleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben\nMann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional\nai: Harmlessness from ai feedback, 2022b. URL https://arxiv.org/abs/2212.08073 .\nStephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan. Distribution-free,\nrisk-controlling prediction sets, 2021.\nVincent Blot, Anastasios N Angelopoulos, Michael I Jordan, and Nicolas J-B Brunel. Automatically adaptive\nconformal risk control, 2025. URL https://arxiv.org/abs/2406.17819 .\nCollin Burns and et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.\narXiv preprint arXiv:2312.09390 , 2023.\n10\n--- Page 11 ---\nSouradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Bedi,\nand Mengdi Wang. Maxmin-rlhf: Towards equitable alignment of large language models with diverse\nhuman preferences. In ICML Workshop on Models of Human Feedback for AI Alignment , 2024.\nCatherine Yu-Chi Chen, Jingyan Shen, Zhun Deng, and Lihua Lei. Conformal tail risk control for large\nlanguage model alignment, 2025. URL https://arxiv.org/abs/2502.20285 .\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing\ncost and improving performance. arXiv preprint arXiv:2305.05176 , 2023.\nJohn J. Cherian, Isaac Gibbs, and Emmanuel J. Candès. Large language model validity via\nenhanced conformal prediction methods. In Advances in Neural Information Processing Sys-\ntems, volume 37, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/hash/\nd02ff1aeaa5c268dc34790dd1ad21526-Abstract-Conference.html .\nC. K. Chow. On optimum recognition error and reject trade-off. IEEE Transactions on Information Theory ,\n16(1):41–46, 1970.\nPaul Christiano, Evan Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts.\nInarXiv preprint arXiv:1810.08575 , 2018.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. In Advances in Neural Information Processing Systems , volume 30, 2017.\nJosef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang.\nSafe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773 , 2023.\nYanrui Du, Sendong Zhao, Danyang Zhao, Ming Ma, Yuhan Chen, Liangyu Huo, Qing Yang, Dongliang\nXu, and Bing Qin. Mogu: A framework for enhancing safety of llms while preserving their usabil-\nity. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang,\neditors, Advances in Neural Information Processing Systems , volume 37, pages 87569–87591. Cur-\nran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\n9f7f063144103bf6debb09a3f15e00fb-Paper-Conference.pdf .\nYonatan Geifman and Ran El-Yaniv. SelectiveNet: A deep neural network with an integrated reject option. In\nKamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages 2151–2159. PMLR,\n09–15 Jun 2019.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding, 2021.\nGeoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899 ,\n2018.\nJiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou\nWang, and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference\ndataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack, 2023. URL https://openreview.net/forum?id=g0QovXbFw3 .\nJaehun Jung, Faeze Brahman, and Yejin Choi. Trust or escalate: Llm judges with provable guarantees for\nhuman agreement. arXiv preprint arXiv:2407.18370 , 2025.\nBhawesh Kumar, Charles Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam.\nConformal prediction with large language models for multi-choice question answering. In Proceedings of\nthe ICML 2023 Workshop on Neural Conversational AI: Teaching Machines to Converse , 2023. URL\nhttps://arxiv.org/abs/2305.18404 .\nSam Lightman, Nikita Nangia, and Samuel R. Bowman. Process supervision improves mathematical reasoning\nin chain-of-thought models. arXiv preprint arXiv:2305.20050 , 2023.\n11\n--- Page 12 ---\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods,\n2022. URL https://arxiv.org/abs/2109.07958 .\nAman Madaan, Guangtao Tu, Yiming Chen, Yulia Tsvetkov, and Graham Neubig. Self -refine: Iterative refine-\nment with self -feedback. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics , 2023.\nChristopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees. arXiv\npreprint arXiv:2402.10978 , 2024. URL https://arxiv.org/abs/2402.10978 .\nIsaac Ong, Pranav Patil, Shivang Agarwal, Harsh Gupta, Nelson F. Liu, Yanda Chen, Percy Liang, and Tat-\nsunori Hashimoto. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665 ,\n2024.\nOpenAI. Gpt-4 system card, 2023. https://openai.com/blog/gpt-4.\nOpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/gpt-4-1/ . Accessed:\n2025-05-15.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35,\npages 27730–27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf .\nWilliam Overman, Jacqueline Jil Vallon, and Mohsen Bayati. Aligning model properties\nvia conformal risk control. In Advances in Neural Information Processing Systems , vol-\nume 37, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/hash/\nc79625091a4f8b5d3abe29f3b14fa43a-Abstract-Conference.html .\nVictor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina\nBarzilay. Conformal language modeling. In Proceedings of the Twelfth International Conference on Learning\nRepresentations (ICLR) , 2024. URL https://arxiv.org/abs/2306.10193 .\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint\narXiv:2305.18290 , 2023.\nAlexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure\nSoulier, and Matthieu Cord. Rewardedsoups: Towards pareto-optimal alignment by interpolating weights\nfine-tuned on diverse rewards. In NeurIPS , 2023.\nAllen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila\nTakayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots\nthat ask for help: Uncertainty alignment for large language model planners. In 7th Annual Conference on\nRobot Learning , 2023. URL https://openreview.net/forum?id=4ZK8ODNyFXx .\nJiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access, 2024. URL https://arxiv.org/abs/2403.01216 .\nYunhaoTang, RohanAnil, HyungWonChung, ZhangChen, ZhifengDai, andBarretZoph. Scrit: Self -evolving\ncritic for scalable oversight. arXiv preprint arXiv:2403.09613 , 2024.\nClovis Varangot-Reille, Olivier Caelen, Emelyne Goffinet, Alison Baumann, Alexandre Chauvet, and Patrick\nvon Platen. Doing more with less – implementing routing strategies in large language model-based systems:\nAn extended survey. arXiv preprint arXiv:2502.00409 , 2025.\n12\n--- Page 13 ---\nVladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World, Second\nEdition. January 2005. doi: 10.1007/978-3-031-06649-8. Springer-Verlag New York, Inc. 2005.\nHaoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang.\nArithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective\nrewards. In ACL, 2024.\nHanjiang Yang, Tianyu Fu, Xu Wang, Yao Yao, Sean Welleck, Etienne Levin, Anqi Nie, Kyunghyun Cho,\nand Jason Weston. Deepcritic: Large language model critics for scalable oversight. arXiv preprint\narXiv:2402.05497 , 2024.\nMurong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of\nthought representations for cost-efficient reasoning. In The Twelfth International Conference on Learning\nRepresentations , 2024. URL https://openreview.net/forum?id=6okaSfANzh .\nMichael J. Zellinger, Rex Liu, and Matt Thomson. Cost-saving llm cascades with early abstention. arXiv\npreprint arXiv:2502.09054 , 2025.\nWenxuan Zhang, Philip Torr, Mohamed Elhoseiny, and Adel Bibi. Bi-factorial preference optimization:\nBalancing safety-helpfulness in language models. In The Thirteenth International Conference on Learning\nRepresentations , 2025. URL https://openreview.net/forum?id=GjM61KRiTG .\nZhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Be-\nyond one-preference-fits-all alignment: Multi-objective direct preference optimization. arXiv preprint\narXiv:2310.03708 , 2023.\nAndy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang,\nZico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving alignment and robustness with circuit\nbreakers. arXiv preprint arXiv:2406.04313 , 2024.\nA Utility-optimality of CRC among score-gap routers\nWe restate Theorem 1 here for convenience and provide the full proof.\nTheorem 1 (Utility–optimality of conformal risk control) .Fix a compact interval Λ = [0 , λmax]. For\neach λ∈Λand every observation idefine a guardrail loss Li(λ)∈[0, B]and a primary-utility score\nUi(λ)∈[0, Umax], both non-increasing in λ. Write\nR(λ) =E[Li(λ)], U (λ) =E[Ui(λ)].\nAssume Ris continuous and strictly decreasing, and Uis non-increasing and K-Lipschitz.\nFor a desired risk budget α∈(0, B)let\nλ⋆= inf{λ∈Λ :R(λ)≤α}.\nGiven an i.i.d. calibration sample D(n)of size n, set\nbRn(λ) =1\nnnX\ni=1Li(λ), ˆλ= infn\nλ∈Λ :n\nn+1bRn(λ) +B\nn+1≤αo\n.\nThen, with expectation taken over the calibration sample\nE\u0002\nU(λ⋆)−U(ˆλ)\u0003\n=O(n−1), (6)\nEh\nsup\n˜λ∈Λ\nR(˜λ)≤αU(˜λ)−U(ˆλ)i\n=O(n−1). (7)\n13\n--- Page 14 ---\nProof.Angelopoulos et al. (2024, Thm. 2) show that the threshold ˆλselected by the conformal-risk-control\nrule satisfies a tight risk lower bound\nE[Ln+1(ˆλ)]≥α−2B\nn+ 1\n.\nWhich by the fact that α≥R(λ⋆)implies R(ˆλ)≥R(λ⋆)−2B\nn+1.Thus we get\n0≤R(λ⋆)−R(ˆλ)≤2B\nn+ 1.\nStrict monotonicity and continuity of Ron the compact interval Λimply that its inverse is Lipschitz;\nwriting m= inf λ∈Λ|R′(λ)|>0gives|ˆλ−λ⋆| ≤2B/(m(n+ 1)).\nThen by our non-increasing and Lipschitz assumptions on the utility curve,\nU(λ⋆)−U(ˆλ)≤Umax|λ⋆−ˆλ| ≤2KB\nm(n+ 1).\nHere U(ˆλ)is still random through ˆλ=ˆλ(D(n)), while U(λ⋆)is deterministic. Integrating the inequality over\nthe distribution of D(n)preserves the bound and yields (6).\nIf˜λsatisfies R(˜λ)≤αthen, by strict monotonicity of R, one must have ˜λ≥λ⋆and hence\nU(˜λ)≤U(λ⋆).\nTherefore, for every calibration draw D(n),\nsup\n˜λ∈Λ\nR(˜λ)≤α\b\nU(˜λ)−U(ˆλ)\t\n≤U(λ⋆)−U(ˆλ)≤2KB\nm(n+ 1).\nTaking expectation establishes (7).\nB TruthfulQA: Additional Experimental Details\nB.1 Prompt for Score Elicitation\nSystem message\nYou are an expert who evaluates multiple choice questions.\n# Instructions\n– Assign a confidence score to each answer choice on a scale from 0 to 1\n– 0 means certainly incorrect, 1 means certainly correct\n– Don’t assign similar scores to choices unless you are genuinely equally uncertain\n# Response Format\n– Output ONLY a valid JSON object with a \"scores\" key containing an array of numbers\n– Example: \"scores\": [0.1, 0.8, 0.05, 0.05]\n– No explanations, just the JSON object\nUser message\nQuestion:\n{<verbatim question text>}\nAnswer Choices:\n<json.dumps(choices)>\nRespond ONLY with a JSON object containing your confidence scores for these choices, e.g. \"scores\": [0.1, 0.8, 0.05,\n0.05]\nBoth the Primary ( gpt-4.1-nano-2025-04-14 ) and Guardian ( gpt-4.1-2025-04-14 ) models receive exactly\nthis dialog. We parse the returned JSON, extract the scoresarray, and then normalize it so that it sums\nto 1; these normalized values are used as the per-choice confidence scores p(x, a)andg(x, a)throughout\ncalibration and evaluation.\n14\n--- Page 15 ---\nB.2 Cost Calculation\nFor every question in every trial we record the four token counts\n\u0000\ntprimary\nin , tprimary\nout , tguardian\nin , tguardian\nout\u0001\n,\ni.e. the prompt- and completion-token usage of the Primary andGuardian models, respectively. Each model\nis billed at its own per-token prices cprimary\nin , cprimary\noutandcguardian\nin , cguardian\nout .\nForM∈ {primary ,guardian }the cost is\ncostM=cM\nintM\nin+cM\nouttM\nout.\nHybrid (routed) calls. If the Primary’s ˆλ-relaxed conformal set contains m > 1answers, the query is\nrouted to the Guardian. To upper-bound this second leg we start from the original, full-prompt token count\ntfull\nin(the question shown to both models) and scale it according to the fraction of choices actually sent:\nbtin=j\ntfull\nin\u0000\n0.5 + 0 .5m\nn\u0001k\n,\nwhere nis the total number of answer options. We keep the Guardian’s completion length fixed at tguardian\nout,\nyielding the estimate\ncostest\nguardian =cguardian\ninbtin+cguardian\nout tguardian\nout\ncost total= cost primary + costest\nguardian .\nBecause we (i) retain the Guardian’s full completion length and (ii) shrink prompt tokens linearlywith\nm/n, this accounting is deliberately conservative: an implementation that truly shortens both prompt and\ncompletion when m < nwould only reduce the spend. Hence our reported savings under Conformal Arbitrage\nare a lower bound.4\nB.3 Calibration Size Ablations\nTable 2: TruthfulQA. Accuracy, cost per 1000 examples, ˆλ,∆above random baseline, and Guardian usage\n(mean ±std over 30 trials). Calibration size n= 300.\nPolicy Accuracy Cost ($/1000) ˆλ ∆ Guardian %\nPrimary 0.557±0.012 0 .032±0.000 – – 0.0%\nCA ( α= 0.25)0.619±0.038 0 .184±0.030 0 .280±0.079 −0.008 27 .3±5.1%\nCA ( α= 0.20)0.667±0.033 0 .236±0.027 0 .405±0.048 +0 .016 35 .0±4.3%\nCA ( α= 0.15)0.710±0.034 0 .304±0.040 0 .542±0.063 +0 .027 45 .6±6.5%\nCA ( α= 0.10)0.757±0.031 0 .394±0.041 0 .700±0.048 +0 .028 60 .3±6.7%\nCA ( α= 0.05)0.801±0.022 0 .513±0.048 0 .861±0.059 +0 .018 78 .3±7.7%\nGuardian 0.833±0.010 0 .615±0.001 – – 100.0%\nTo assess how many calibration examples are needed for Conformal Arbitrage (CA) to stabilize, we\nrepeat the TruthfulQA experiment with calibration split sizes n∈ {300,500}. Tables 2–3 report accuracy,\ndollar cost per 1000 questions, the fitted threshold ˆλ, and Guardian usage at the same guardrail levels\nα∈ {0.25,0.20,0.15,0.10,0.05}.\nAcross all risk budgets the frontier is stable. Moving from n= 300ton= 500changes the mean accuracy\nby at most 1−2percentage points. Average cost remains effectively unchanged (differences <3%) for every α.\nThe fraction of queries escalated to the Guardian varies by less than 2%absolute.\nB.4 Guardian Scoring Ablation\nWhen calibrating Conformal Arbitrage (CA) on TruthfulQA we binarize the Guardian’s output in the main\nexperiments—assigning score 1to the Guardian’s highest scoring answwer if and only if it is correct and 0to\n4Token prices follow the OpenAI schedule of 15 May 2025.\n15\n--- Page 16 ---\nTable 3: TruthfulQA. Accuracy, cost per 1000 examples, ˆλ,∆above random baseline, and Guardian usage\n(mean ±std over 30 trials). Calibration size n= 500.\nPolicy Accuracy Cost ($/1000) ˆλ ∆ Guardian %\nPrimary 0.554±0.012 0 .032±0.000 – – 0.0%\nCA ( α= 0.25)0.625±0.040 0 .184±0.019 0 .301±0.039 −0.005 27 .3±3.4%\nCA ( α= 0.20)0.672±0.042 0 .233±0.025 0 .414±0.045 +0 .020 34 .6±4.2%\nCA ( α= 0.15)0.715±0.037 0 .301±0.024 0 .563±0.038 +0 .031 45 .1±3.9%\nCA ( α= 0.10)0.765±0.033 0 .402±0.025 0 .712±0.026 +0 .032 62 .0±4.2%\nCA ( α= 0.05)0.806±0.029 0 .524±0.024 0 .881±0.028 +0 .019 80 .1±3.8%\nGuardian 0.833±0.010 0 .615±0.001 – – 100.0%\nTable 4: Accuracy, cost per 1000 examples, ˆλ,∆above random baseline, and Guardian usage (mean ±std\nover 30 trials) when the Guardian’s raw scores are used instead of hard 0/1binarization.\nPolicy Accuracy Cost ($/1000) ˆλ ∆ Guardian %\nPrimary 0.556±0.012 0 .032±0.000 – – 0.0%\nCA ( α= 0.25)0.598±0.037 0 .163±0.026 0 .203±0.089 −0.021 24 .0±4.5%\nCA ( α= 0.20)0.661±0.035 0 .222±0.028 0 .394±0.059 +0 .014 32 .8±4.4%\nCA ( α= 0.15)0.714±0.028 0 .304±0.032 0 .558±0.059 +0 .029 45 .6±5.3%\nCA ( α= 0.10)0.771±0.025 0 .414±0.030 0 .741±0.036 +0 .032 63 .1±4.3%\nCA ( α= 0.05)0.813±0.021 0 .554±0.059 0 .917±0.056 +0 .013 84 .8±9.6%\nGuardian 0.831±0.010 0 .615±0.001 – – 100.0%\nall others—to make the accuracy loss Li(λ)in Eq.(2)directly interpretable as “fractional drop in accuracy”\nrelative to an always-Guardian policy. Here we repeat the experiment but feed CA the Guardian’s raw\nconfidence scores . The resulting frontier is reported in Table 4.\nFor tighter risk budgets ( α≤0.10). accuracy rises by roughly +1−2%while cost is unchanged. At loose\nrisk budgets ( α≥0.20), accuracy drops slightly (about 0.5%−1%). Cost differences remain negligible. With\nrespect to the risk guarantees, feeding softer scores does not affect the finite-sample CRC bound; every row\nin Table 4 satisfies the E[L]≤αconstraint as expected.\nB.5 Unrestricted Action Set Routing\nIn our main pipeline the Guardian is asked to choose only from the ˆλ-relaxed candidate set Cˆλ(x)generated\nby the Primary. Here we study a more liberal variant—denoted CA⋆—that lets the Guardian reconsider the\nentireaction set A(x).\nTable 5 shows that unrestricted routing lifts accuracy by roughly 3−6percentage points across the tested\nrisk budgets, with the largest gains appearing in the looser regimes ( α≥0.20). The calibration diagnostics in\nTable 6 explain why: as αgrows the conformal set shrinks, increasing the odds that the Primary prunes away\nthe correct answer. When the Guardian can inspect all options it can often recover that mistake, yielding the\nfrontier in Figure 3. The cost penalty is modest—on average 7−10 %above the restricted CA variant.\nIn many applications the action space is muchlarger than the four-choice multiple-choice setting considered\nhere. Passing the full set to the Guardian would then erase most of the cost savings that Conformal Arbitrage\nprovides. Moreover, for trade-offs other than cost-accuracy (e.g. reward versus safety) a filtered candidate set\ncan be desirable: it biases the Guardian toward options with high primary utility while still respecting the\nguard-rail budget. For these reasons we present the restricted policy as the default and treat unrestricted\nrouting as an informative ablation.\nB.6 Model Choice Ablation\nTo probe how Conformal Arbitrage behaves for the cost-accuracy tradeoff when the capability gap between\nthe two models is smaller, we replace the original gpt-4.1-nano Primary with the stronger but costlier\n16\n--- Page 17 ---\nFigure 3: Accuracy vs. cost per 1000 examples on TruthfulQA using unrestricted calibrated routing. Each\npoint corresponds to the mean over 30 trials; error bars represent one standard deviation. Solid circles\ndenote our CRC-hybrid policy, stars represent static baselines (Preferred-only and Guardian-only), and hollow\ndiamonds show the random routing baseline.\ngpt-4.1-mini . This boosts the stand-alone Primary accuracy from 0.56to0.77—only ∼6pp below the\nGuardian—and raises the token price four-fold. Even in this compressed regime CA still delivers a meaningful\nimprovement over cost-matched random routing: at α=0.05it gains +2pp in accuracy while invoking the\nGuardian on just one quarter of the queries, and at α=0.025itmatches the Guardian’s accuracy for 40%\nof the cost. The detailed numbers are collected in Table 7, and the corresponding cost–accuracy frontier is\nvisualized in Figure 4.\n17\n--- Page 18 ---\nTable 5: Accuracy, cost per 1000 examples, ˆλ,∆aboveunrestricted random baseline, and Guardian usage\n(mean ±std over 30 trials). Calibration size n= 400. CA rows report the unrestricted variant.\nPolicy Accuracy Cost ($/1000) ˆλ ∆ Guardian %\nPrimary 0.559±0.015 0 .032±0.000 – – 0.0%\nCA⋆(α= 0.25)0.687±0.021 0 .206±0.025 0 .277±0.067 +0 .046 27 .7±3.9%\nCA⋆(α= 0.20)0.713±0.022 0 .247±0.033 0 .403±0.058 +0 .052 34 .3±5.3%\nCA⋆(α= 0.15)0.741±0.022 0 .313±0.036 0 .529±0.059 +0 .050 44 .9±5.7%\nCA⋆(α= 0.10)0.785±0.016 0 .421±0.027 0 .706±0.031 +0 .043 62 .1±4.4%\nCA⋆(α= 0.05)0.812±0.016 0 .525±0.035 0 .867±0.040 +0 .020 78 .9±5.6%\nGuardian 0.833±0.011 0 .620±0.001 – – 100.0%\nTable 6: Calibrated ˆλvalues and resulting conformal-set sizes for CA as used in the main text (means ±s.d.\nover 30 trials). As the risk budget αtightens (top →bottom), the candidate set grows.\nα ˆλ Set size\n0.25 0.277±0.067 1 .457±0.024\n0.20 0.403±0.058 1 .801±0.038\n0.15 0.529±0.059 2 .105±0.045\n0.10 0.706±0.031 2 .587±0.041\n0.05 0.867±0.040 3 .253±0.034\nFigure 4: Cost–accuracy frontier on TruthfulQA when the Primary is gpt-4.1-mini . Large markers denote\nmeans over 30 trials (error bars: ±1s.d.); faint dots show individual CA runs. The dashed grey line is the\nlinear interpolation between the single-model baselines.\n18\n--- Page 19 ---\nTable 7: Model-ablation results on TruthfulQA with gpt-4.1-mini as the Primary. Accuracy, cost per 1000\nexamples, fitted threshold ˆλ, improvement over a cost-matched random router ( ∆), and Guardian usage.\nMeans ±one standard deviation across 30 trials.\nPolicy Accuracy Cost ($/1000) ˆλ ∆ Guardian %\nPrimary ( 4.1-mini )0.7738±0.0113 0 .126±0.000 – – 0.0%\nCA ( α= 0.050) 0.8156±0.0194 0 .265±0.032 0 .452±0.082 +0 .021 23 .9±5.0%\nCA ( α= 0.025) 0.8345±0.0208 0 .375±0.064 0 .669±0.094 +0 .026 41 .2±10.7%\nGuardian ( 4.1) 0.8328±0.0088 0 .615±0.001 – – 100.0%\nC PKU-SafeRLHF: Additional Experimental Details\nC.1 Prompt for Score Elicitation\nSystem message\nYou are an expert assistant-evaluator.\n# Task\nScore the helpfulness of each assistant reply on a continuous scale from 0.00 to 1.00 (use two decimal places).\n– Evaluate only technical helpfulness: factual correctness, completeness, clarity, and relevance to the user’s request.\n– Ignore safety, ethics, policy compliance, or any notion of harm.\n# Reasoning\n1. Analyse the user’s intent.\n2. Independently assess each reply’s strengths and weaknesses.\n3. Assign two distinct scores—the more helpful answer must receive the higher score.\n# Output format\n– On a single line, return exactly two numbers separated by a comma, e.g. ‘0.37, 0.54‘.\n– Scores must differ and lie in [0.00, 1.00].\n– No explanations, no extra text, just the two numbers.\nUser message template ( filled per instance )\n<USER PROMPT>\n{<verbatim user prompt text>}\n</USER PROMPT>\n<ASSISTANT REPLY 0>\n{<first candidate reply>}\n</ASSISTANT REPLY 0>\n<ASSISTANT REPLY 1>\n{<second candidate reply>}\n</ASSISTANT REPLY 1>\nScores:\nWe parse the single-line response as two floats, enforce strict inequality by perturbing ties by ±0.01,\npreserving exchangeablity, and use the resulting pair as the helpfulness scores given by the Primary model in\nour Conformal Arbitrage pipeline.\nC.2 Numerical Results\nWe provide the complete numerical results for the PKU-SafeRLHF experiment introduced in Section 5.\nTable 8 aggregates performance over 30 independent calibration/evaluation splits. Accuracy is the fraction\nof prompts whose chosen answer matches the annotator-preferred more-helpful response, while Severity-loss\nmeasures the average excess severity of the selected answer above the safer one ( 0≤sev≤3; lower is better).\nAs guaranteed by theory, every CA configuration respects the finite-sample bound Severity-loss ≤αwhile\ntracing an efficient helpfulness–harmlessness frontier that strictly dominates random routing.\nTightening the risk budget reduces severity-loss while gradually approaching the Guardian-only baseline.\nAtα= 0.30CA halves the Primary’s safety violations yet retains 63% of its helpfulness, invoking the Guardian\non∼72% of queries. Even under the strictest budget ( α= 0.10) CA more than doubles the Guardian’s\nhelpfulness while keeping average severity within the prescribed limit.\n19\n--- Page 20 ---\nTable 8: PKU-SafeRLHF helpfulness–harmlessness trade-off. Primary = helpfulness-maximising model;\nGuardian = severity-minimizing rule. Mean ±std over 30 trials.\nPolicy Accuracy Severity-loss ˆλ ∆Guardian %\nPrimary 0.519±0.019 0 .676±0.033 – – 0.0%\nCA ( α= 0.60)0.475±0.029 0 .571±0.070 0 .206±0.088 +0 .012 19 .0±9.4%\nCA ( α= 0.50)0.443±0.026 0 .482±0.053 0 .354±0.051 +0 .028 35 .6±5.3%\nCA ( α= 0.40)0.393±0.034 0 .379±0.064 0 .495±0.061 +0 .033 51 .8±8.0%\nCA ( α= 0.30)0.325±0.026 0 .245±0.043 0 .619±0.022 +0 .037 71 .7±4.9%\nCA ( α= 0.20)0.270±0.018 0 .161±0.021 0 .681±0.007 +0 .028 82 .2±2.1%\nCA ( α= 0.10)0.214±0.016 0 .080±0.022 0 .777±0.014 +0 .015 91 .8±1.9%\nGuardian 0.156±0.011 0 .000±0.000 – – 100.0%\nD MMLU\nWe next evaluate Conformal Arbitrage (CA) on the Massive Multitask Language Understanding benchmark\n(MMLU; [Hendrycks et al., 2021]). Unless otherwise noted, the pipeline, models, prompts, cost accounting,\nand random–router baselines are identical to the TruthfulQA setup in Section 5; below we list only the\ndivergences that are specific to MMLU. Both models receive the same JSON-forced multiple-choice prompt\nused for TruthfulQA (Appendix B.1); we simply drop the TruthfulQA pre-amble and insert the MMLU\nquestion and four answer strings verbatim.\nDataset MMLU comprises almost ∼16k multiple choice questions across 57 subject areas covering high-\nschool, undergraduate, and professional curricula. We load the public cais/mmlu distribution via datasets\nand collapse the original train/validation /testsplits into one pool. For each trialwe draw a fresh,\nbalanced sample of Ntot= 1,000questions, allocating n= 500for calibration and the remaining 500for\nevaluation. Balancing is accomplished by first shuffling each subject’s pool and then taking ⌊Ntot/57⌋items\nfrom every subject, distributing the remainder randomly.\nResults Although it is of less average gain compared to TruthfulQA, Conformal Arbitrage still traces an\nefficient frontier that beats cost-matched random routing for most values of αapart from the extremes. We\ncan see that, in particular, the performance of CA degrades at the higher and lower values of αcompared\nto the middle range. We hypothesize that the decreased gain compared to TruthfulQA is likely due to the\nfact that even with balancing, the questions in MMLU are of more varying difficulty across subjects than\nthe differences between questions within TruthfulQA. Nevertheless, at α= 0.10CA recovers 91 %of the\nGuardian’s accuracy while spending only 61 %of its cost, demonstrating that the method remains effective\neven when the capability gap is modest.\nTable 9: Accuracy, cost per 1000 examples, ˆλ,∆above random baseline, and Guardian usage (mean ±std\nover 30 trials; calibration n= 500).\nPolicy Accuracy Cost ($/1000) ˆλ ∆Guardian %\nPrimary 0.591±0.011 0 .035±0.000 – – 0.0%\nCA ( α= 0.25)0.618±0.019 0 .111±0.034 0 .126±0.111 −0.005 13 .0±5.6%\nCA ( α= 0.20)0.663±0.021 0 .194±0.024 0 .423±0.059 +0 .011 24 .5±3.3%\nCA ( α= 0.15)0.706±0.022 0 .317±0.057 0 .651±0.065 +0 .008 42 .9±9.5%\nCA ( α= 0.10)0.753±0.020 0 .416±0.029 0 .771±0.021 +0 .018 55 .8±4.1%\nCA ( α= 0.05)0.802±0.026 0 .624±0.065 0 .924±0.058 −0.005 86 .9±9.8%\nGuardian 0.828±0.008 0 .676±0.004 – – 100.0%\n20\n--- Page 21 ---\nFigure 5: Cost–accuracy frontier on MMLU. Mean ±std over 30 trials. Faint dots show individual CA runs.\nThe dashed grey line is the linear interpolation between the single-model baselines.\n21",
  "text_length": 62506
}