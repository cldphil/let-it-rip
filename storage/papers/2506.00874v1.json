{
  "id": "http://arxiv.org/abs/2506.00874v1",
  "title": "Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image\n  Detection",
  "summary": "Current AIGC detectors often achieve near-perfect accuracy on images produced\nby the same generator used for training but struggle to generalize to outputs\nfrom unseen generators. We trace this failure in part to latent prior bias:\ndetectors learn shortcuts tied to patterns stemming from the initial noise\nvector rather than learning robust generative artifacts. To address this, we\npropose On-Manifold Adversarial Training (OMAT): by optimizing the initial\nlatent noise of diffusion models under fixed conditioning, we generate\non-manifold adversarial examples that remain on the generator's output\nmanifold-unlike pixel-space attacks, which introduce off-manifold perturbations\nthat the generator itself cannot reproduce and that can obscure the true\ndiscriminative artifacts. To test against state-of-the-art generative models,\nwe introduce GenImage++, a test-only benchmark of outputs from advanced\ngenerators (Flux.1, SD3) with extended prompts and diverse styles. We apply our\nadversarial-training paradigm to ResNet50 and CLIP baselines and evaluate\nacross existing AIGC forensic benchmarks and recent challenge datasets.\nExtensive experiments show that adversarially trained detectors significantly\nimprove cross-generator performance without any network redesign. Our findings\non latent-prior bias offer valuable insights for future dataset construction\nand detector evaluation, guiding the development of more robust and\ngeneralizable AIGC forensic methodologies.",
  "authors": [
    "Yue Zhou",
    "Xinan He",
    "KaiQing Lin",
    "Bin Fan",
    "Feng Ding",
    "Bin Li"
  ],
  "published": "2025-06-01T07:20:45Z",
  "updated": "2025-06-01T07:20:45Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00874v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00874v1  [cs.CV]  1 Jun 2025Breaking Latent Prior Bias in Detectors for\nGeneralizable AIGC Image Detection\nYue Zhou1, Xinan He2, KaiQing Lin1, Bin Fan3, Feng Ding2, Bin Li1∗\n1Guangdong Provincial Key Laboratory of Intelligent Information Processing,\nShenzhen Key Laboratory of Media Security,\nand SZU-AFS Joint Innovation Center for AI Technology, Shenzhen University\n2Nanchang University\n3University of North Texas\nAbstract\nCurrent AIGC detectors often achieve near-perfect accuracy on images produced\nby the same generator used for training but struggle to generalize to outputs from\nunseen generators. We trace this failure in part to latent prior bias: detectors\nlearn shortcuts tied to patterns stemming from the initial noise vector rather than\nlearning robust generative artifacts. To address this, we propose On-Manifold\nAdversarial Training (OMAT) : by optimizing the initial latent noise of diffusion\nmodels under fixed conditioning, we generate on-manifold adversarial examples\nthat remain on the generator’s output manifold—unlike pixel-space attacks, which\nintroduce off-manifold perturbations that the generator itself cannot reproduce and\nthat can obscure the true discriminative artifacts. To test against state-of-the-art\ngenerative models, we introduce GenImage++, a test-only benchmark of outputs\nfrom advanced generators (Flux.1, SD3) with extended prompts and diverse styles.\nWe apply our adversarial-training paradigm to ResNet50 and CLIP baselines and\nevaluate across existing AIGC forensic benchmarks and recent challenge datasets.\nExtensive experiments show that adversarially trained detectors significantly im-\nprove cross-generator performance without any network redesign. Our findings on\nlatent-prior bias offer valuable insights for future dataset construction and detec-\ntor evaluation, guiding the development of more robust and generalizable AIGC\nforensic methodologies.\n1 Introduction\nThe rapid advancement of generative AI, from early GANs [ 8] to modern diffusion models [ 30], has\nproduced increasingly realistic synthesized images, creating a critical need for robust AI-generated\ncontent (AIGC) forensic methodologies. While current AIGC detectors perform well on images from\ntheir training generator, they often exhibit a significant generalization gap when faced with outputs\nfrom unseen generative architectures [ 39]. This suggests that detectors may learn ‘shortcuts’–features\nspecific to the training generator rather than fundamental generative artifacts. Indeed, recent methods\nachieving better generalization often leverage Stable Diffusion [ 28] for data augmentation [ 35,1,2],\nindicating valuable forensic signals within SD data that baseline detectors underutilize.\nWe hypothesize that a key reason for this shortcut learning is the detector’s sensitivity to characteristics\ntied to the initial latent noise vector, zT. This is related to findings in Xu et al. [ 36], which showed\nthat the specific ‘torch.manual_seed()’ for zTcan be identified from generated images, implying\nlearnable patterns in the noise sampling process. To investigate this, we conducted white-box\n∗Corresponding author\nPreprint. Under review.\n--- Page 2 ---\nattacks by optimizing only zT(keeping text condition cand the SD generator fixed) against pre-\ntrained CLIP+Linear [ 26] and ResNet50 [ 11] AIGC detectors. Our experiments revealed that\nstandard, detectable generated images could consistently be transformed into undetectable ones via\nthese zTperturbations. Since the generation process remains otherwise unchanged, these resulting\nadversarial examples lie on the generator’s output manifold. We term these ‘on-manifold adversarial\nexamples’, distinguishing them from often off-manifold pixel-space attacks. Their successful\ngeneration highlights the baseline detectors’ reliance on non-robust features linked to zT’s influence.\nBuilding on this, we propose an on-manifold adversarial training paradigm. Incorporating these\nspecialized adversarial examples into training significantly improves detector generalization, with\nour LoRA [ 15] fine-tuned CLIP-based model achieving state-of-the-art performance on multiple\nbenchmarks. Furthermore, to facilitate rigorous evaluation against contemporary generative models\nlike Black Forest Flux.1 [ 16] and Stability AI’s Stable Diffusion 3 [ 7], which feature advanced\narchitectures (e.g., DiT [ 23]) and text encoders (e.g., T5-XXL [ 4]), we introduce GenImage++ , a\nnovel test-only benchmark. Our adversarially trained detectors maintain strong performance on this\nchallenging new dataset.\nOur contributions are threefold: (1)We are the first to systematically expose a vulnerability linked\nto the detector’s sensitivity to initial latent noise variations, identifying this as a significant factor\ncontributing to poor generalization in AIGC detection. (2)We are the first to apply on-manifold\nadversarial training—using only a small set of additional latent-optimized examples—to achieve state-\nof-the-art generalization in AIGC detection. (3)We propose GenImage++, a benchmark incorporating\noutputs from cutting-edge generators to advance future AIGC forensic evaluation. Collectively, our\nfindings on the detector’s latent-space vulnerabilities and the efficacy of on-manifold adversarial\ntraining provide valuable insights for future dataset construction and the development of more robust\nand broadly applicable AIGC forensic methodologies.\n2 Related Work\nThe detection of fully synthesized AI-generated content (AIGC) has evolved significantly. Early\nefforts targeting Generative Adversarial Networks (GANs) [ 8] often found success with conventional\nCNNs or by identifying salient frequency-domain artifacts [ 34]. However, the rise of high-fidelity\ndiffusion models [ 29,28] has presented a more formidable challenge, primarily concerning the\ngeneralization of detectors: models trained on one generator often fail dramatically on outputs\nfrom unseen architectures [ 39,22]. This generalization gap indicates that detectors frequently learn\nsuperficial, generator-specific \"shortcuts\" rather than fundamental generative fingerprints.\nCurrent state-of-the-art AIGC detectors increasingly leverage powerful pre-trained Vision-Language\nModels (VLMs) like CLIP [ 26], as seen in methods such as UnivFD [ 22], C2P-CLIP [ 32], and\nRIGID [ 13]. Another prominent strategy involves utilizing diffusion models themselves, either\nfor reconstruction-based analysis (e.g., DIRE [ 35]) or extensive training data augmentation (e.g.,\nDRCT [ 2]), effectively expanding the training manifold. Our work diverges by hypothesizing and\nexposing a \"latent prior bias,\" where detectors learn non-generalizable shortcuts tied to the initial\nlatent noise zT. We address this by proposing on-manifold adversarial training with a relatively\nsmall, targeted set of zT-optimized examples . This approach efficiently compels the learning\nof more robust features, achieving state-of-the-art generalization without relying on massive data\naugmentation or complex reconstruction pipelines.\n3 Unveil Latent Prior Bias within AIGC detectors\n3.1 Standard AIGC Detection and the Generalization Challenge\nThe fundamental task in AI-generated content (AIGC) forensics is to distinguish between real images,\ndrawn from a distribution PR, and AI-generated images. A standard approach involves training a\nneural network detector, Dϕ:X →Rparameterized by ϕ, using samples from PR(labeled as real,\ny= 0) and samples generated by a specific source generative model G, drawn from distribution PG\n(labeled as fake, y= 1). The detector is typically trained by minimizing an expected loss, such as the\nBinary Cross-Entropy (BCE):\nLstd(ϕ) =ExR∼PR[ℓ(Dϕ(xR),0)] +ExG∼PG[ℓ(Dϕ(xG),1)], (1)\n2\n--- Page 3 ---\nwhere ℓ(s, y)is the BCE loss function. Minimizing Lstdenables the detector Dϕto learn a set of\ndiscriminable features effective for separating the specific distributions PRandPGencountered\nduring training.\nHowever, a critical objective in AIGC forensics is generalization : the ability of the detector Dϕ,\ntrained primarily on data from generator G, to effectively detect images produced by novel, previously\nunseen generators G′(with distributions PG′). Ideally, the features learned by minimizing Eq. (1)\nshould capture fundamental properties indicative of the AI generation process itself, allowing for\nrobust performance across diverse generative models. Yet, empirical evidence often reveals a\nsignificant performance degradation when detectors are evaluated on unseen generators, indicating\nthat the learned features may be overly specific to the artifacts of the training generator G, a\nphenomenon often attributed to sample bias and shortcut learning. Addressing this generalization gap\nis a central challenge we tackle in this work.\n3.2 Disentangling Process-Inherent Artifacts from Input Conditioning\nTo develop detectors capable of robust generalization, we must first understand how inputs influence\nthe output of typical generative models and then aim to disentangle true forensic features from\ninput-specific characteristics. We examine the structure of modern conditional text-to-image gener-\nators, specifically models like Stable Diffusion which often employ Denoising Diffusion Implicit\nModels (DDIM) [ 29] for image synthesis. The DDIM process generates a target data sample (latent\nrepresentation z0) starting from Gaussian noise zT∼ N(0, I)via an iterative reverse process over\ntimesteps t=T, . . . , 1. Given the latent state ztat timestep tand conditioning information c(e.g.,\ntext embeddings), a neural network ϵθ(zt, t, c)predicts the noise component ϵ(t)\npredadded at the corre-\nsponding forward diffusion step. The DDIM sampler then deterministically estimates the previous\nlatent state zt−1using ztandϵ(t)\npred:\nϵ(t)\npred=ϵθ(zt, t, c), (2)\nzt−1=√¯αt−1 \nzt−√1−¯αtϵ(t)\npred√¯αt!\n+p\n1−¯αt−1ϵ(t)\npred, (3)\nwhere ¯αtrepresents the noise schedule coefficients. After Tsteps, the final latent z0is decoded to\nthe image x0= Dec( z0)using a V AE decoder Dec.\nCrucially, this generation process, x0= SD( zT, c), involves two primary inputs: the conditioning c\nand the initial random noise zT. The conditioning cprimarily dictates the high-level semantic content\nand style of the generated image x0. In contrast, the core iterative denoising mechanism (Eq. (3)),\nthe architecture of ϵθ, and the final decoder Dec constitute the fixed machinery of the generator that\noperates on these inputs.\nWe posit that the most generalizable forensic features ( fgen)—those indicative of an image being\nAI-generated regardless of its specific content or source model—are likely tied to artifacts arising from\nthese intrinsic, input-independent components of the generation pipeline (e.g., the denoising dynamics,\nnetwork architectures of ϵθorDec, upsampling methods). Since generalizable detection requires\nrecognizing AI generation across diverse content (thus demanding features largely independent\nof condition c), a critical question arises: could detectors, in their attempt to distinguish fakes,\ninadvertently learn shortcuts from the other primary input, the initial noise zT? If the specific instance\nor sampling process of zTimparts any learnable, yet non-generalizable patterns, this could hinder the\nacquisition of true fgen. The finding by Xu et al. [ 36] that zTseeds are classifiable from generated\nimages lends plausibility to this concern. Our subsequent methodology is therefore designed to\ninvestigate this potential source of shortcut learning.\n3.3 Exposing Latent Prior Bias via On-Manifold Attack\nAs established (Section 3.1), standard AIGC detectors often exhibit poor generalization. Building on\nthe concern that detectors might learn shortcuts from the initial noise zT(Section 3.2), we hypothesize\nthat a significant factor contributing to this generalization failure is a phenomenon we term latent\nprior bias . This bias describes the scenario where detectors, trained on images generated from zT\nvectors sampled from a prior (e.g., N(0, I)), overfit to image characteristics directly influenced by\n3\n--- Page 4 ---\nFigure 1: Illustration of our white-box attack methodology. The initial latent noise zTis iteratively\noptimized based on gradient feedback from the frozen detector Dϕ, while the prompt cand Stable\nDiffusion generator remain fixed.\nthese zTinstances. Instead of acquiring truly robust generative fingerprints ( fgen), these detectors\nlearn \"shortcuts\" by exploiting subtle but learnable patterns tied to the zT-influenced variations.\nConsequently, the learned decision boundary becomes biased , performing well on the seen generator\n(PG) but poorly on unseen ones ( PG′).\nTo directly test this hypothesis and empirically expose the latent prior bias, we designed a white-\nbox On-Manifold attack by perturbing the initial latent noise zT. Illustrated in Figure 1, the\ncore objective is to find an optimized latent zadv\nTnear an initial random zrand\nT such that the image\nxadv\n0= SD( zadv\nT, c), generated with a fixed prompt cand frozen generator SD(Stable Diffusion\nv1.4), fools a target detector Dϕ. This is formulated as an optimization problem where we seek to\nminimize the detector’s confidence that the generated image is fake (i.e., make it appear real, target\nlabely= 0). Given z(0)\nT=zrand\nT, the optimizable latent z(k)\nTat iteration kis updated to minimize the\nfollowing adversarial loss Ladv:\nLadv(z(k)\nT) =ℓ(Dϕ(SD(z(k)\nT, c)),0) (4)\nwhere ℓ(·,·)is the BCEWithLogits loss. The update rule using Stochastic Gradient Descent (SGD) is:\nz(k+1)\nT =z(k)\nT−η∇z(k)\nTLadv(z(k)\nT) (5)\nwhere ηis the learning rate. This iterative optimization (detailed in Algorithm 1, Appendix A.1)\ncontinues for a maximum of K= 100 steps or until the success condition σ(Dϕ(P(SD(z(k)\nT, c))))<\n0.5is met. We applied this attack to GenImage SDv1.4 pre-trained CLIP ViT-L/14 + Linear and\nResNet50 detectors in 1. For each of the 1000 unique class labels sourced from the ImageNet-1k\ndataset [ 5], we constructed the text prompt cusing the template \"photo of a [label]\". To ensure\na substantial and diverse set of adversarial examples for further training, and aiming for a scale\ncomparable to standard test sets, we systematically generated six successful adversarial examples\nfor each of the 1000 prompts , using different initial random seeds sfor each success. This process\nyielded our adversarial dataset Xadv, comprising N= 6000 unique on-manifold adversarial images.\nFigure 2: Distribution of optimization steps for successful on-manifold latent attacks targeting\nCLIP+Linear (left) and ResNet50 (right) detectors. Max optimization steps K= 100 .\n4\n--- Page 5 ---\n3.4 Attack Performance and Evidence of Latent Prior Bias\nBaseline Detectability Confirmed. Executing the on-manifold latent attack described in Section 3.3\nfirst confirmed the competence of our baseline detectors. As illustrated by the insets in Figure 2,\nimages generated from the unoptimized initial latent noise zrand\nT (i.e., at step k= 0 of our attack\noptimization) were consistently classified as fake by the respective detectors across all 1000 prompts.\nThis establishes that the detectors are effective against typical outputs generated using the standard\nzTsampling strategy from the training generator.\nUniversal Vulnerability Exposes Latent Prior Bias. Despite this initial detectability, our subsequent\nlatent optimization revealed a profound vulnerability. For every one of the 1000 prompts, we\nsuccessfully found multiple initial seeds sfor which zrand\nT could be optimized into a zadv\nTthat\nfooled the target detector (i.e., σ(s(K′)\nadv)<0.5). These zadv\nT-derived images remained on-manifold,\nwith qualitative inspection (see Appendix Figure 5) confirming their visual coherence and semantic\nalignment. The consistent success in rendering initially detectable outputs undetectable, achieved\nsolely by perturbing zT, provides strong empirical evidence for the latent prior bias. It demonstrates\nthat the detectors’ decisions are critically influenced by learnable, yet non-robust, characteristics tied\nto the initial latent noise, rather than relying purely on fundamental generative artifacts.\nAttack Efficiency Points to Pervasive Latent Prior Bias. The dynamics of these successful attacks\nfurther characterize this vulnerability. As shown in the main histograms of Figure 2, a significant\nnumber of attacks succeeded rapidly (often within ≈10 optimization steps). This implies that for\nmany zrand\nT, the decision boundary influenced by latent prior bias is very close and easily crossed\nwith minimal zTperturbation, underscoring the pervasiveness of this bias.\n4 Mitigating Latent Prior Bias via On-Manifold Adversarial Training\nHaving established that standard AIGC detectors exhibit a latent prior bias by relying on non-robust,\nzT-influenced shortcut features (Section 3.3), we now introduce our remediation strategy: On-\nManifold Adversarial Training (OMAT) . OMAT leverages the on-manifold adversarial examples\nXadv—generated by optimizing zTto expose these very shortcuts—to retrain the detector.\nThe core mechanism of OMAT is to alter the detector’s learning objective. By augmenting the\ntraining data with Xadv(labeled as fake, y= 1) and penalizing their misclassification, we render the\npreviously learned zT-influenced shortcuts ineffective for minimizing the training loss. This forces\nthe detector to identify alternative, more robust features. Crucially, since our Xadvare on-manifold\n(i.e., valid outputs of the generator SDthat differ from standard outputs primarily in their zT-derived\ncharacteristics, not via off-manifold pixel noise), they retain the intrinsic generative artifacts ( fgen)\ncommon to the generator’s process. OMAT thus compels the detector to become sensitive to these\nfgenas the consistent signals for distinguishing both standard fakes ( PG) and our on-manifold\nadversarial fakes ( Xadv) from real images ( PR).\nThis retraining process discourages overfitting to superficial zT-influenced characteristics and instead\npromotes the learning of features more deeply ingrained in the generation process itself. Such features\nare inherently less dependent on the specific initial zTand thus more likely to be shared across diverse\ngenerative models. This shift—from relying on zT-influenced shortcuts to recognizing fundamental\nfgen—is the key mechanism through which OMAT mitigates latent prior bias and, as demonstrated\nby our experiments (Section 6), leads to substantial improvements in generalization. This approach\naligns with the understanding that robustness to on-manifold adversarial examples is intrinsically\nlinked to generalization [31]. Specific implementation details are in Appendix A.2.\n5 GenImage++: Evaluating Detectors in the Era of Advanced Generators\nWhile the GenImage dataset [ 39] provides a valuable baseline, the rapid evolution of generative mod-\nels necessitates benchmarks that reflect current state-of-the-art capabilities. Recent models, notably\nFLUX.1 [ 16] and Stable Diffusion 3 (SD3) [ 7], signify a leap forward, often employing distinct\narchitectures like Diffusion Transformers (DiT) [ 23] and significantly more powerful text encoders\n(e.g., T5-XXL [ 4]). To provide a more rigorous testbed for generalization against these contemporary\ncapabilities, we introduce GenImage++ , a novel test-only benchmark dataset. GenImage++ is\ndesigned around two core dimensions of advancement, reflecting the \"++\" in its name:\n5\n--- Page 6 ---\nAdvanced Generative Models: It incorporates images generated by state-of-the-art models, primarily\nFLUX.1 and SD3, to directly test detector performance against the latest architectures and their\ndistinct visual signatures.\nEnhanced Prompting Strategies: It moves beyond simple prompts by including subsets specifically\ndesigned to leverage the improved text comprehension of modern generators. This includes images\ngenerated from complex, long-form descriptive prompts and a wide array of diverse stylistic prompts.\nFigure 3 provides illustrative examples from GenImage++. The benchmark comprises multiple\nsubsets, each targeting specific advancements. These include testing baseline performance on\nstandard prompts with Flux and SD3, assessing long-prompt comprehension , evaluating robustness to\nstyle diversity (using Flux, SD3, and comparative models like SDXL and SD1.5), and challenging\ndetectors with high-fidelity portrait generation . Full details on its construction, including specific\ngenerators, prompt engineering, and subset composition, are provided in Appendix C.\nFigure 3: Example images from our proposed GenImage++ benchmark. Columns from left to right:\n(1)Base Label (e.g., \"backpack\") generated using standard prompts by advanced models; (2) Long-\nPrompt comprehension, where base labels are expanded into complex scenes (e.g., \"backpack\" in a\nmountain setting); (3) Style diversity, illustrating varied artistic renditions; and (4) Photo realism,\nfocusing on high-fidelity human portraits and photorealistic scenes.\n6 Experiments\n6.1 Experimental Setup\nTraining Paradigm and Our Models. Our experiments primarily follow the GenImage [ 39]\nbenchmark paradigm. Unless otherwise specified, all detectors, including our initial baselines and\nexternal methods, were trained on the Stable Diffusion v1.4 (SDv1.4) subset of GenImage using real\nimages and corresponding SDv1.4 generated fakes. Our initial detectors are based on ResNet50 [ 12]\nand CLIP ViT-L/14 + Linear Head [ 26]. For adversarial training, these baseline detectors were\nsubsequently fine-tuned using the SDv1.4 training data augmented with N= 6000 of our on-manifold\nadversarial examples ( Xadv) generated as described in Section 3.3. For CLIP+Linear, adversarial\nfine-tuning was explored via three strategies: Linear Only, Full Fine-tuning, and LoRA+Linear [ 15].\nThese baseline and adversarially trained variants constitute our proposed models.\nEvaluation Datasets and Metrics. To assess generalization capabilities, we test the trained detectors\nacross a various datasets, including: the original GenImage benchmark; our proposed GenImage++\nbenchmark (described in Section 5 and Appendix C), which introduces images from advanced\ngenerators like Flux and SD3 under challenging conditions; and the Chameleon dataset [ 37], which\ncomprises diverse AI-generated images collected \"in the wild\". We assess detector performance using\nAccuracy (ACC).\nExternal Baselines for Comparison. The performance of our models is benchmarked against several\nexternal methods. This includes a suite of established standard forensic detectors (e.g., CNNSpot [ 34],\nGramNet [ 18], UniFD [ 22], SRM [ 19], AIDE [ 37]) which analyze image artifacts directly. We also\ncompare against recent state-of-the-art generator-leveraging approaches: DIRE [ 35], which uses\nSDv1.4 for reconstruction, and DRCT [2], which employs SDv1.4 for training data augmentation.\nImplementation Details. Specific hyperparameters for training, LoRA configuration, image prepro-\ncessing, dataset compositions, and other relevant implementation choices are provided in Appendix A\n6\n--- Page 7 ---\nTable 1: Generalization performance (Accuracy %) on various GenImage subsets. Models were\ntrained on the SDv1.4 subset. For each column, the best result is marked in bold and the second best\nisunderlined . For our adversarially trained models, the change relative to their respective baseline is\nshown in parentheses ( ▲for improvement, ▼for degradation).\nModel MidJourney SDv1.4 SDv1.5 ADM GLIDE Wukong VQDM BigGAN A VG\nStandard Forensic Methods\nXception[3] 57.97 98.06 97.98 51.16 57.51 97.79 50.34 48.74 69.94\nCNNSpot[34] 61.25 98.13 97.54 51.50 55.13 93.51 51.83 51.06 69.99\nF3Net[25] 52.26 99.30 99.21 49.64 50.46 98.70 45.56 49.59 68.09\nGramNet[18] 63.00 94.19 94.22 48.69 46.19 93.79 49.20 44.71 66.75\nUniFD[22] 77.29 97.01 96.67 50.94 78.47 91.52 65.72 55.91 77.29\nNPR[33] 62.00 99.75 99.64 56.79 82.69 97.89 54.43 52.26 75.68\nSPSL[17] 56.20 99.50 99.50 51.00 67.70 98.40 49.80 63.70 73.23\nSRM[19] 54.10 99.80 99.80 49.90 52.80 99.60 50.00 51.00 69.63\nAIDE [37] 79.38 99.74 99.76 78.54 91.82 98.65 80.26 66.89 86.88\nGenerator-Leveraging Methods\nDIRE [35] 51.11 55.07 55.31 49.93 50.02 53.71 49.87 49.85 51.86\nDRCT/Conv-B [2] 94.43 99.37 99.19 66.42 73.31 99.25 76.85 59.41 83.53\nDRCT/UniFD [2] 85.82 92.33 91.87 75.18 87.44 92.23 89.12 87.38 87.67\nOur Models (Baseline)\nResNet50 61.88 99.30 98.97 50.00 52.82 97.53 51.41 49.55 70.18\nCLIP+Linear 82.11 95.89 95.43 53.82 78.89 93.30 76.61 57.53 79.20\nOur Models (OMAT)\nResNet50 80.49 (+18.61) 84.11 (-15.19) 83.46 (-15.51) 58.13 (+8.13) 75.32 (+22.50) 83.22 (-14.31) 65.58 (+14.17) 62.76 (+13.21) 74.13 (+3.95)\nCLIP Full Fine-tune 82.18 (+0.07) 89.70 (-6.19) 89.68 (-5.75) 81.45 (+27.63) 87.65 (+8.76) 81.94 (-11.36) 82.28 (+5.67) 84.99 (+27.46) 84.98 (+5.78)\nCLIP Linear Only 85.77 (+3.66) 95.33 (-0.56) 95.05 (-0.38) 59.40 (+5.58) 86.79 (+7.90) 93.60 (+0.30) 80.65 (+4.04) 75.95 (+18.42) 84.07 (+4.87)\nCLIP+LoRA (Rank 4) 90.36 (+8.25) 97.52 (+1.63) 97.46 (+2.03) 83.82 (+30.00) 97.41 (+18.52) 97.62 (+4.32) 95.53 (+18.92) 97.34 (+39.81) 94.63 (+15.43)\nCLIP+LoRA (Rank 8) 89.62 (+7.51) 95.13 (-0.76) 94.86 (-0.57) 79.11 (+25.29) 94.68 (+15.79) 94.97 (+1.67) 90.44 (+13.83) 91.37 (+33.84) 91.27 (+12.07)\nCLIP+LoRA (Rank 16) 88.57 (+6.46) 94.82 (-1.07) 94.32 (-1.11) 80.98 (+27.16) 94.27 (+15.38) 94.69 (+1.39) 93.82 (+17.21) 93.22 (+35.69) 91.84 (+12.64)\nTable 2: Comparison on the Chameleon .Accuracy (%) of different detectors (rows) in detecting real\nand fake images of Chameleon . For each training dataset, the first row indicates the Accevaluated on\ntheChameleon testset, and the second row gives the Accfor “fake image / real image” for detailed\nanalysis.\nTraining Set CNNSpot GramNet LNP UnivFD DIRE PatchCraft NPR AIDE Our\nSD v1.460.11 60.95 55.63 55.62 59.71 56.32 58.13 62.60 66.05\n8.86/98.63 17.65/93.50 0.57/97.01 74.97/41.09 11.86/95.67 3.07/96.35 2.43/100.00 20.33/94.38 33.93/90.17\n6.2 Generalization Performance on GenImage\nWe evaluate the generalization capabilities of our proposed adversarial training strategies and com-\npare them against established baselines on various subsets of the GenImage dataset. Performance,\nmeasured by accuracy (%), is reported in Table 1.\nOur On-Manifold Adversarial Training strategy consistently enhanced the generalization capabilities\nof all baseline detectors. Notably, the OMAT CLIP+LoRA (Rank 4) achieves a state-of-the-art\naverage accuracy of 94.63% across all GenImage subsets. This marks a +15.43% improvement\nover its non-adversarially trained baseline (79.20%) and surpasses all other evaluated methods. This\nsuperior performance is driven by large gains on challenging OOD subsets (e.g., +30.00% on ADM,\n+39.81% on BigGAN) while largely maintaining or improving strong performance on in-domain data.\nThese results strongly validate our central hypothesis: on-manifold adversarial training effectively\nmitigates the identified latent prior bias by compelling the detector to learn from challenging\nlatent perturbations—rather than relying on shortcut features tied to initial noise influence—thereby\ndeveloping more robust, generalizable features that yield consistent high performance across diverse\nseen and unseen generators.\n6.3 Performance on the Challenging Chameleon Benchmark\nTo rigorously evaluate generalization against diverse \"in-the-wild\" content, we tested our best OMAT\nmodel (CLIP+LoRA Rank 4) on the challenging Chameleon dataset [ 37]. Our model was trained\nusing only the SDv1.4 subset of GenImage, augmented with our on-manifold adversarial examples.\nTable 2 compares our results against baselines from [ 37] under the same SDv1.4 training condition.\nAgainst this backdrop of established difficulty, our adversarially trained model achieves an average\naccuracy of 66.05% on Chameleon. This surpasses not only all baselines trained under the comparable\nSDv1.4 protocol (Table 2) but also notably exceeds the best reported performance (65.77% by AIDE)\n7\n--- Page 8 ---\nTable 3: Performance (Accuracy %) of detectors on our proposed GenImage++ benchmark subsets.\nAll detectors were trained on the SDv1.4 subset of GenImage. Column headers are abbreviated for\nspace: Flux Multi (Flux_multistyle), Flux Photo (Flux_photo), Flux Real (Flux_realistic), SD1.5\nMulti (SD1.5_multistyle), SDXL Multi (SDXL_multistyle), SD3 Photo (SD3_photo), SD3 Real\n(SD3_realistic).\nModel Flux Flux Multi Flux Photo Flux Real SD1.5 Multi SDXL Multi SD3 SD3 Photo SD3 Real A VG\nXception 36.86 10.48 4.65 5.45 97.27 20.63 38.00 5.83 15.06 26.03\nCNNSpot 37.38 6.89 8.71 5.28 84.41 34.79 47.70 7.48 25.55 28.69\nF3Net 25.18 7.79 2.83 7.90 94.15 24.01 46.67 0.84 30.28 26.63\nGramNet 37.83 16.71 8.01 19.71 96.49 28.65 48.55 8.33 55.71 35.55\nNPR 35.38 13.19 8.48 19.41 93.63 15.40 32.38 12.45 27.58 28.66\nSPSL 67.13 16.55 43.76 25.73 71.14 17.74 44.58 16.22 29.75 36.96\nSRM 8.46 2.92 0.37 1.93 96.62 6.39 9.97 0.55 4.43 14.63\nDRCT/UniFD (SDv1) 71.08 63.97 46.83 62.42 99.19 64.84 72.28 70.70 73.55 69.43\nDRCT/Conv-B (SDv1) 73.02 51.91 54.72 66.40 100.00 77.19 79.10 82.93 76.58 73.54\nOur (CLIP+LoRA R4) 96.53 92.55 97.60 97.67 100.00 99.17 98.27 90.38 98.82 96.78\nfor models trained on the entire GenImage dataset [37], highlighting the significant generalization\nimparted by our approach.\n6.4 Performance on the GenImage++ Benchmark\nWe further evaluated all detectors trained on the SDv1.4 subset of GenImage against our newly\nproposed GenImage++ benchmark (described in Section 5). The performance (Accuracy %) on\nvarious GenImage++ subsets is detailed in Table 3.\nGenImage++ Exposes Generalization Limits and Architectural Specificity. The results clearly\ndemonstrate the challenging nature of GenImage++ for detectors trained solely on older SDv1.4\ndata (Table 3). Most standard forensic methods and even the strong DRCT (SDv1) baseline (69.43%\naverage) struggle significantly on subsets generated by advanced models like Flux and SD3, par-\nticularly on their multi-style, photorealistic, and complex prompt variations, with many baseline\naccuracies falling below 40% or even 20%. This starkly indicates that features learned from SDv1.4\nare insufficient for these modern generative capabilities.\nOur OMAT Achieves Robust Generalization on Advanced Models. In stark contrast, our adver-\nsarially trained CLIP+LoRA (Rank 4) model demonstrates vastly superior and remarkably consistent\ngeneralization across all subsets of GenImage++, achieving an average accuracy of 96.78% . It\nachieves over 90% accuracy on nearly every subset, including those that prove extremely difficult\nfor the baselines. This exceptional performance, indicates that our on-manifold adversarial training\nsuccessfully forced the learning of more fundamental generative artifacts ( fgen) that are robust not\nonly across styles within similar architectures but also across entirely different advanced models and\ntheir diverse generation modes.\n6.5 Achieving Generalization through On-Manifold Robustness\nTo investigate the relationship between generalization and on-manifold attack robustness, we re-attack\nthree CLIP+Linear detector variants, selected based on their varied generalization performance\ndemonstrated in Table 1: (i) the Baseline model (poorest generalization), (ii) the Adv Train Linear\nmodel (moderate generalization), and (iii) the Adv Train LoRA (Rank 4) model (best generalization).\nTable 4: Robustness of CLIP+Linear detectors against the latent optimization attack after different\nadversarial training strategies. Attacks were performed on 100 ImageNet labels.\nMetric Baseline OMAT Linear OMAT LoRA\n(No Adv Train) (Linear Only FT) (LoRA Rank 4 FT)\nSuccess (%) 75 57 25\nAvg Step 43.56 62.86 161.42\nThe results, presented in Table 4, reveal a strong correlation between generalization capability and on-\nmanifold robustness. The Baseline model was most vulnerable. The Adv Train Linear model showed\nimproved robustness, corresponding to its better generalization. Critically, the Adv Train LoRA\n8\n--- Page 9 ---\nmodel, which achieved state-of-the-art generalization, exhibited the highest on-manifold robustness\nby a significant margin. This clear trend—where models demonstrating superior generalization are\nalso substantially more resilient to on-manifold attacks targeting latent prior bias—provides strong\nevidence for our hypothesis. It suggests that mitigating this bias through effective on-manifold\nadversarial training is a key mechanism for developing truly generalizable AIGC detectors.\n6.6 Comparison with Pixel-Space Adversarial Training\nTo further contextualize the benefits of our on-manifold latent optimization, we compared its efficacy\nagainst adversarial training using examples generated by common L∞-bounded pixel-space attacks:\nFGSM [ 9], PGD [ 20], and MI-FGSM [ 6], each with varying perturbation budgets ( ϵ). These pixel-\nspace adversarial examples targeted the same baseline CLIP+Linear detector as our latent attacks. The\nCLIP+LoRA (Rank 4) model was then adversarially trained with these different sets of pixel-space\nexamples. Table 5 presents the average accuracy on the GenImage test subsets.\nWhile adversarial training with all evaluated pixel-space attacks consistently improved generalization\nover the baseline, these gains are substantially smaller than those from our on-manifold approach.\nThis marked performance difference suggests that our on-manifold adversarial examples, generated\nby perturbing the initial latent space zTand processed through the full generative pipeline, are\nmore effective at compelling the detector to learn truly generalizable features fgen. Pixel-space\nattacks, though offering some improvement, seem less adept at correcting the core biases that hinder\ngeneralization compared to our on-manifold latent perturbations. This underscores the critical role of\nthe type of adversarial examples in achieving robust generalization for AIGC detection.\nTable 5: Impact of adversarial training using different types of adversarial examples on CLIP+LoRA\n(Rank 4) generalization. All adversarial examples were generated targeting the baseline CLIP+Linear\ndetector trained on SDv1.4.\nAdversarial Training Data Source (Attack Type) Avg. GenImage Acc (%)\nBaseline (No Adversarial Training) 79.19\nRGB-Level Pixel-Space Attacks ( L∞-bounded)\nFGSM ( ϵ= 0.03) 82.11\nFGSM ( ϵ= 0.05) 83.18\nFGSM ( ϵ= 0.1) 83.49\nPGD ( ϵ= 0.03, α= 0.005, T= 20 ) 82.28\nPGD ( ϵ= 0.05, α= 0.01, T= 20 ) 82.63\nPGD ( ϵ= 0.1, α= 0.02, T= 40 ) 83.11\nMI-FGSM ( ϵ= 0.03, α= 0.005, T= 20 ) 81.76\nMI-FGSM ( ϵ= 0.05, α= 0.01, T= 20 ) 82.61\nMI-FGSM ( ϵ= 0.1, α= 0.015, T= 30 ) 83.16\nOn-Manifold Attack 94.63\n7 Conclusion\nIn this work, we identified latent prior bias —a phenomenon where AIGC detectors learn non-\ngeneralizable shortcuts tied to the initial latent noise zT—as a key factor limiting their cross-generator\nperformance. We demonstrated this bias empirically using a novel on-manifold attack that optimizes\nzTto consistently fool baseline detectors. To mitigate this, we proposed On-Manifold Adversarial\nTraining (OMAT) , which leverages these zT-optimized adversarial examples. Our experiments\nshow that OMAT significantly enhances generalization, with our CLIP+LoRA model achieving state-\nof-the-art performance on established benchmarks and our newly introduced GenImage++ dataset,\nwhich features outputs from advanced generators like FLUX.1 and SD3. Our findings underscore\nthat addressing latent-space vulnerabilities is crucial for developing AIGC detectors that are robust\nnot just to seen generators but also to novel architectures and varied generation conditions. Our\nwork illuminates the path: by systematically identifying and mitigating deep vulnerabilities such as\nlatent prior bias in training data, we can drive the development of forensic tools capable of discerning\nfundamental, broadly applicable generative fingerprints.\n9\n--- Page 10 ---\nReferences\n[1]George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. Fakeinversion: Learning to detect\nimages from unseen text-to-image models by inverting stable diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 10759–10769, 2024.\n[2]Baoying Chen, Jishen Zeng, Jianquan Yang, and Rui Yang. DRCT: Diffusion reconstruction contrastive\ntraining towards universal detection of diffusion generated images. In Forty-first International Conference\non Machine Learning , 2024.\n[3]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In 2017 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , Jul 2017.\n[4]Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\nJournal of Machine Learning Research , 25(70):1–53, 2024.\n[5]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255.\nIeee, 2009.\n[6]Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting\nadversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 9185–9193, 2018.\n[7]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution\nimage synthesis. In Forty-first international conference on machine learning , 2024.\n[8]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):139–\n144, 2020.\n[9]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.\narXiv preprint arXiv:1412.6572 , 2014.\n[10] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783 , 2024.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nInProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Jun 2016.\n[13] Zhiyuan He, Pin-Yu Chen, and Tsung-Yi Ho. Rigid: A training-free and model-agnostic framework for\nrobust ai-generated image detection. arXiv preprint arXiv:2405.20112 , 2024.\n[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. arXiv preprint arXiv:2104.08718 , 2021.\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.\n[16] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n[17] Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue, Weiming Zhang, and Nenghai\nYu. Spatial-phase shallow learning: rethinking face forgery detection in frequency domain. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition , pages 772–781, 2021.\n[18] Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. Global texture enhancement for fake face detection in\nthe wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8060–8069, 2020.\n[19] Yuchen Luo, Yong Zhang, Junchi Yan, and Wei Liu. Generalizing face forgery detection with high-\nfrequency features. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\npages 16317–16326, 2021.\n10\n--- Page 11 ---\n[20] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.\n[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\nGuided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 ,\n2021.\n[22] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize\nacross generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 24480–24489, 2023.\n[23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision , pages 4195–4205, 2023.\n[24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952 , 2023.\n[25] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. Thinking in frequency: Face forgery\ndetection by mining frequency-aware clues. In European conference on computer vision , pages 86–103.\nSpringer, 2020.\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning , pages 8748–8763. PmLR,\n2021.\n[27] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski. Kornia: an open source differentiable computer\nvision library for pytorch. In Winter Conference on Applications of Computer Vision , 2020.\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 10684–10695, 2022.\n[29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 , 2020.\n[30] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 , 2020.\n[31] David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6976–6987,\n2019.\n[32] Chuangchuang Tan, Renshuai Tao, Huan Liu, Guanghua Gu, Baoyuan Wu, Yao Zhao, and Yunchao Wei.\nC2p-clip: Injecting category common prompt in clip to enhance generalization in deepfake detection. In\nProceedings of the AAAI Conference on Artificial Intelligence , volume 39, pages 7184–7192, 2025.\n[33] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-\nsampling operations in cnn-based generative network for generalizable deepfake detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 28130–28139, 2024.\n[34] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images\nare surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 8695–8704, 2020.\n[35] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li.\nDire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference\non Computer Vision , pages 22445–22455, 2023.\n[36] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Good seed makes a good crop: Discovering secret seeds\nin text-to-image diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer\nVision (WACV) , pages 3024–3034. IEEE, 2025.\n[37] Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. A sanity check\nfor ai-generated image detection. arXiv preprint arXiv:2406.19435 , 2024.\n11\n--- Page 12 ---\n[38] Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, and Zeke Xie. Golden noise for diffusion\nmodels: A learning framework. arXiv preprint arXiv:2411.09502 , 2024.\n[39] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie\nHu, and Yunhe Wang. Genimage: A million-scale benchmark for detecting ai-generated image. Advances\nin Neural Information Processing Systems , 36:77771–77782, 2023.\n12\n--- Page 13 ---\nA Implementation Details\nA.1 Latent Optimization Attack Generation\nThis section provides detailed hyperparameters and implementation choices for the on-manifold\nadversarial attack generation process described conceptually in Section 3.3 and Algorithm 1.\nObjective Recap. The goal was to iteratively optimize an initial latent noise vector zrand\nT into an\nadversarial vector zadv\nTsuch that the generated image xadv\n0= SD( zadv\nT, c)fooled a target detector\nDϕ(σ(Dϕ(P(xadv\n0)))<0.5), while keeping the generator SDand prompt cfixed.\nAlgorithm 1 Single Attempt: On-Manifold Latent Noise Optimization Attack\nRequire: Frozen detector Dϕ, Frozen generator SD, Text prompt c, Seed s\nRequire: Max steps K= 100 , Learning rate η= 1×10−3, Success threshold τ= 0.5\nRequire: Preprocessing function P(·), Loss function ℓ(s, y) = BCEWithLogits( s, y)\n1:Set random seed using s\n2:Initialize z(0)\nT∼ N(0, I);zopt\nT←z(0)\nT\n3:zadv\nT←None\n4:fork= 0toK−1do\n5: x(k)\n0←SD(zopt\nT, c) ▷Generate image (35 inference steps)\n6: s(k)\nadv←Dϕ(P(x(k)\n0))▷Get detector logit ( P(·)includes postprocess & Kornia transforms)\n7: L(k)\nadv←ℓ(s(k)\nadv,0)\n8: ifσ(s(k)\nadv)< τthen\n9: zadv\nT←zopt\nT.detach()\n10: break\n11: end if\n12: g← ∇zopt\nTL(k)\nadv\n13: zopt\nT←zopt\nT−ηg ▷ SGD update\n14:end for\nEnsure: Resulting zadv\nTif attack succeeded, otherwise None.\nGenerator and Detector Setup. We used Stable Diffusion v1.4 (CompVis/stable-diffusion-v1-4 via\nHugging Face Diffusers) as the frozen generator SD. Attacks targeted our baseline ResNet50 [ 11]\nand CLIP+Linear [26] detectors, which were pre-trained on the SDv1.4 GenImage subset.\nInput Initialization. For each attack, a text prompt cwas formatted as \"photo of [ImageNet La-\nbel]\", using labels from ImageNet-1k [ 5]. The initial latent noise z(0)\nTwas sampled from N(0, I)\nwith shape (1,4,64,64)anddtype=torch.float16 . Reproducibility for each attempt was en-\nsured using torch.cuda.manual_seed(seed_count) . This initial tensor z(0)\nTwas wrapped in\ntorch.nn.Parameter to enable gradient computation and designated as the optimizable variable\nzopt\nT.\nOptimization Loop Details. The optimization proceeded for a maximum of K= 100 steps. In each\nstepk:\n•Generation: The current latent z(k)\nTwas input to the frozen SDpipeline ( pipe ) with\noutput_type=’latent’ andnum_inference_steps=35 . The resulting UNet output la-\ntents were scaled ( / pipe.vae.config.scaling_factor ) and decoded using the frozen\nV AE decoder ( pipe.vae.decode ) into an intermediate image tensor, which was then cast\ntotorch.float32 .\n•Preprocessing P(·):The decoded image tensor underwent two stages of preprocess-\ning. First, a custom postprocess function was applied: the image was denormal-\nized to [0,1](via * 0.5 + 0.5 and clamping), scaled to [0,255], subjected to a\nsimulated precision loss mimicking float-to-uint8-to-float conversion using the opera-\ntion image += image.round().detach() - image.detach() , and finally rescaled\nto[0,1]. This step aimed to simulate realistic image saving/loading artifacts. Sec-\n13\n--- Page 14 ---\nond, standard detector-specific preprocessing ( discriminator_preprocess ) was ap-\nplied using the Kornia library [ 27] to ensure differentiability. This involved resiz-\ning the image tensor to 224×224 pixels ( K.Resize(..., align_corners=False,\nantialias=True) followed by K.CenterCrop(224) ) and normalizing it using the stan-\ndard CLIP statistics ( K.Normalize(mean=[0.4814..., 0.4578..., 0.4082...],\nstd=[0.2686..., 0.2613..., 0.2757...]) ).\n•Loss & Optimization: The preprocessed image was passed through the frozen de-\ntector Dϕto obtain the logit s(k)\nadv. The adversarial loss L(k)\nadvwas calculated using\ntorch.nn.BCEWithLogitsLoss with a target label of 0. Gradients ∇z(k)\nTL(k)\nadvwere com-\nputed via backpropagation through the entire differentiable path (detector, Kornia prepro-\ncessing, custom postprocessing, V AE decoder, diffusion steps). The latent zopt\nTwas updated\nusing torch.optim.SGD with a learning rate η= 1×10−3.\nTermination and Output. The optimization loop terminated if the success condition σ(s(k)\nadv)<0.5\nwas met or after K= 100 steps. If successful at step K′, the resulting latent zadv\nT=z(K′)\nT was\nused to generate the final adversarial image xadv\n0= SD( zadv\nT, c), which was saved as a PNG file.\nWe iterated through seeds for each prompt until one successful adversarial example was generated,\ncollecting a total of N= 6000 examples for the Xadvdataset.\nEnvironment. Experiments were conducted using PyTorch 2.5.0dev20240712, Diffusers 0.33.0.dev0,\nKornia 0.8.0, and single NVIDIA A100 GPU with CUDA 12.4.\nA.2 Adversarial Training / Fine-tuning Details\nThis section details the process used to fine-tune the baseline detectors using the generated on-\nmanifold adversarial examples ( Xadv).\nObjective. The goal was to improve the robustness and generalization of the initial baseline detectors\n(ResNet50 and CLIP+Linear, pre-trained on SDv1.4) by further training them on a dataset augmented\nwith the N= 6000 adversarial examples Xadvgenerated as described in Section A.1.\nDataset Composition. The training dataset was formed by combining the original SDv1.4 training\nset from GenImage [ 39] (real images labeled y= 0, SDv1.4 fakes labeled y= 1) with the full set of\nN= 6000 adversarial examples Xadv(also labeled as fake, y= 1). Standard PyTorch Dataset and\nDataLoader classes were used, with the adversarial examples loaded via a separate dataset instance\nand iterated through alongside the standard data within each epoch, as shown in the train_epoch\nfunction structure. The combined dataset was shuffled for training.\nCommon Training Parameters. Key parameters applied across most fine-tuning runs include:\n•Optimizer: AdamW.\n•Weight Decay: 1×10−4.\n•Loss Function: Binary Cross-Entropy with Logits ( torch.nn.BCEWithLogitsLoss ).\n•Epochs: 20.\n•Adversarial Weighting: The loss contribution from adversarial samples ( x∈Xadv) was\ndynamically weighted using λadv= min(1 .0 + 0 .2×epoch ,3.0), linearly increasing the\nemphasis on adversarial samples from 1.2 up to 3.0 over the first 10 epochs, then capping at\n3.0.\n•Checkpointing: Model checkpoints were saved after each epoch. The \"best model\" was\nselected based on a combined score: 0.6×Val Acc + 0.4×Adv Sample Acc , where Avg\nVal Acc is the accuracy on SDv1.4 validation set, and Adv Sample Acc is the accuracy on\ntheXadvset itself evaluated after each epoch.\nModel-Specific Fine-tuning Strategies.\n•ResNet50 (Full Fine-tuning): The entire baseline ResNet50 detector was fine-tuned end-to-\nend using the augmented dataset. The learning rate was set to 1×10−4and batch size to 128.\nInput images were preprocessed using Kornia [ 27] for resizing/cropping to 224×224and\nnormalization with standard ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229,\n0.224, 0.225]).\n14\n--- Page 15 ---\n•CLIP+Linear (Full Fine-tuning): The entire CLIP ViT-L/14 backbone and the linear head\nwere fine-tuned end-to-end using the augmented dataset. Learning rate and batch size were\nlikely similar to the ResNet50 setup. Preprocessing used CLIP’s statistics and 224×224\nresolution (Section A.1).\n•CLIP+Linear (Linear Only): Only the final linear classification head was fine-tuned\nusing the augmented dataset. The CLIP backbone remained frozen. Learning rate and\nbatch size were likely similar to the ResNet50/Full FT setup [Confirm LR/BS if different].\nPreprocessing used CLIP’s statistics.\n•CLIP+Linear (LoRA+Linear): LoRA [ 15] was applied to the CLIP ViT-L/14 back-\nbone while simultaneously fine-tuning the linear head. We used the PEFT library with\nLoraConfig set for TaskType.FEATURE_EXTRACTION . Experiments were run with LoRA\nranks r∈ {4,8,16}. The LoRA alpha was set to α=r×2, dropout was 0.1, and target\nmodules were \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\" . The learning rate for\nthis strategy was 2×10−4and batch size was 32. Preprocessing used CLIP’s statistics.\nB Additional Results for On-Manifold Latent Attack\nThis section provides further analysis of our proposed on-manifold latent optimization attack, focusing\non the quality of the generated adversarial images and characteristics of the optimized latent noise\nvectors.\nB.1 Qualitative Analysis and Image Fidelity\nA key characteristic of our on-manifold attack, which perturbs the initial latent noise zTrather\nthan directly manipulating pixel values of a rendered image, is its ability to maintain high visual\nfidelity and semantic coherence with the input prompt c. Unlike traditional pixel-space attacks where\nperturbations might appear as noise, our method guides the generator SDto produce images that are\nstill within its learned manifold but possess subtle characteristics that fool the detector.\nFigure 4: Distribution of CLIP scores for images generated from original random latent noise ( zrand\nT)\nversus optimized adversarial latent noise ( zadv\nT) for the prompt \"photo of a cat\". The close similarity\nin distributions and mean scores (Original: mean=28.05, std=0.83; Optimized: mean=28.18, std=0.96;\nMean Difference: 0.13, based on [Number] samples) indicates that the latent optimization process\npreserves image fidelity and prompt alignment.\nTo quantitatively assess image fidelity and alignment with the prompt, we computed CLIP\nscores [ 26,14] between the generated images and their corresponding text prompts. We com-\npared the distributions of CLIP scores for images generated from original random latent noise zrand\nT\n(before optimization, detectable by the baseline detector) with those generated from the optimized\nadversarial latent noise zadv\nT(after successful attack, fooling the baseline detector). Figure 4 illustrates\nthe CLIP score distributions for the prompt \"photo of a [ImageNet_Label]\", based on 6000 pairs of\n15\n--- Page 16 ---\noriginal and optimized images. The distribution for optimized (adversarial) images largely overlaps\nwith that of the original images. The mean CLIP score for original images was 28.05 (std=0.83),\nwhile for the optimized adversarial images it was 28.18 (std=0.96), resulting in a mean difference\nof only 0.13. This minimal difference and substantial overlap indicate no significant degradation in\nperceived image quality or prompt alignment due to the latent optimization process. Similar trends\nwere observed across other prompts.\nFigure 5: Comparison of adversarial examples for the prompt \"photo of a cat\" across multiple\ninitial generations (columns). Rows from top to bottom: Original generated images, pixel-space\nattack samples (FGSM), and our on-manifold adversarial samples. Our on-manifold attacks fool the\ndetector while preserving intrinsic generative artifacts, unlike pixel-space attacks that often introduce\ndisruptive noise.\nFigure 8 provides a qualitative comparison across multiple ImageNet labels, showcasing images\ngenerated using various L∞-bounded pixel-space attacks (FGSM, PGD, MI-FGSM with different ϵ\nvalues) alongside examples from our on-manifold latent optimization attack. As can be observed,\nthe pixel-space attacks, particularly at higher ϵvalues (e.g., ϵ= 0.1), often introduce noticeable,\nsomewhat unstructured noise patterns across the image. While these perturbations are bounded and\nmay be \"imperceptible\" in the sense of not drastically changing the overall scene, their texture can\nappear artificial or noisy. In contrast, the images generated by our on-manifold attack (rightmost\ncolumn for each label) maintain a high degree of visual realism and coherence consistent with the\noutput of the Stable Diffusion generator. The differences from an original, non-adversarial generation\n(not explicitly shown side-by-side here, but understood to be visually similar to the on-manifold\nattack output) are typically semantic or structural (e.g., slight changes in pose, background detail,\nlighting, or object features), rather than additive noise. This visual evidence further supports the\nclaim that our latent optimization produces on-manifold adversarial examples that preserve image\nquality while effectively fooling the detector.\nB.2 Analysis of Optimized Latent Noise Vectors\nTo investigate whether the optimized adversarial latent vectors zadv\nToccupy a distinct region or\ncluster within the latent space compared to the initial random noise vectors zrand\nT, we conducted a\nfocused experiment. Using the prompt \"a photo of a cat\", we performed our latent optimization attack\ntargeting the baseline CLIP+Linear detector for 200 unique random seeds (‘torch.manual_seed(0)‘ to\n‘torch.manual_seed(199)‘). Each attack was limited to a maximum of K= 50 optimization steps.\nOut of the 200 attempts, 164 seeds resulted in successful adversarial latents zadv\nTwithin the 50-\nstep budget. We then collected all 200 initial random latents zrand\nT (Original Latents) and the 164\ncorresponding successful adversarial latents zadv\nT(Optimized Latents). To visualize their distribution,\nwe applied t-SNE to project these high-dimensional latent vectors (shape 1×4×64×64, flattened)\ninto a 3D space.\n16\n--- Page 17 ---\nFigure 6: 3D t-SNE visualization of initial random latent vectors ( zrand\nT, blue circles) and successfully\noptimized adversarial latent vectors ( zadv\nT, pink ’x’ markers) for the prompt \"a photo of a cat\". The\nlack of distinct clustering indicates that optimized latents are not confined to a specific sub-region but\nare interspersed among the original random latents.\nThe resulting visualization is shown in Figure 6. As can be seen, the Optimized Latents (marked\nwith ’x’) do not form distinct, well-separated clusters from the Original Latents (marked with ’o’).\nInstead, the optimized adversarial latents appear to be intermingled with and distributed similarly to\nthe initial random latents. This suggests that successful adversarial perturbations do not necessarily\npush the latents into a far-off, specific region of the latent space. Rather, they represent relatively\nsmall deviations from the initial random points, sufficient to cross the detector’s decision boundary.\nThis observation is consistent with the notion that the detector’s decision boundary is brittle and that\nmany points on the generator’s manifold are close to \"adversarial pockets\" reachable via minor latent\nadjustments. The lack of clear clustering for zadv\nTfurther implies that the vulnerability exploited is\nnot tied to a single, easily characterizable sub-distribution of \"bad\" or \"outlier\" initial latents. Instead,\nit suggests that the standard detector fails to robustly interpret subtle variations across a broad\nrange of typical initial latents zrand\nT. The issue is not that only certain types of initial latents are\nproblematic, but rather that the detector’s learned features are sensitive to small, targeted changes\noriginating from many diverse starting points in the latent space. This reinforces the idea that the\ndetector relies on non-robust \"shortcut\" features that can be easily masked or manipulated through\nthese nuanced latent perturbations.\nC The GenImage++ Benchmark Dataset Construction\nThis appendix details the motivation, design, and construction of our GenImage++ benchmark dataset.\nC.1 Motivation and Design Goals\nWhile the GenImage dataset [ 39] provides a valuable baseline, its constituent models predate signifi-\ncant recent advancements. To offer a more contemporary and challenging evaluation, we developed\nGenImage++. Our design addresses two primary dimensions of progress in generative AI: (1) Ad-\nvanced Generative Models , incorporating outputs from state-of-the-art architectures like FLUX.1\nand Stable Diffusion 3 (SD3), which utilize Diffusion Transformers (DiT) and powerful text encoders\n(e.g., T5-XXL); and (2) Enhanced Prompting Strategies , leveraging the improved prompt adherence\n17\n--- Page 18 ---\nand diverse generation capabilities of these modern models. GenImage++ is a test-only benchmark\ndesigned to rigorously assess AIGC detector generalization against these new capabilities.\nC.2 Generative Models Used\nThe following generative models were used, sourced from Hugging Face Transformers and Diffusers\nlibraries:\n•FLUX.1-dev: black-forest-labs/FLUX.1-dev [16].\n•Stable Diffusion 3 Medium (SD3): stabilityai/stable-diffusion-3-medium [7].\n•Stable Diffusion XL (SDXL): stabilityai/stable-diffusion-xl-base-1.0 [24].\n•Stable Diffusion v1.5 (SD1.5): runwayml/stable-diffusion-v1-5 [28].\nFor FLUX.1-dev and SD3, images were generated at 1024×1024 resolution using 30 inference steps,\na guidance scale of 3.5, and a maximum sequence length of 512 tokens, unless specified otherwise.\nParameters for SDXL and SD1.5 followed common practices for those models.\nFigure 7: Sample images from multiple subsets of the GenImage++ dataset. Each block (from left to\nright, top to bottom) shows examples generated by different models and style presets: (a) Flux, (b)\nFlux realistic, (c) Flux photo, (d) Flux multistyle, (e) SDv1.5 multistyle, (f) SDXL style, (g) Stable\nDiffusion v3.0, (h) Stable Diffusion v3.0 realistic, and (i) Stable Diffusion v3.0 photo. This figure\nillustrates the visual diversity across models and styling configurations in our dataset.\n18\n--- Page 19 ---\nC.3 Subset Construction and Prompt Engineering\nGenImage++ comprises several subsets, each with distinct prompt engineering strategies, leveraging\nMeta-Llama-3.1-8B-Instruct ( meta-llama/Llama-3.1-8B-Instruct ) [10] for prompt expansion\nwhere noted. The scale of each generated subset is detailed in Table 6.\nTable 6: Number of images in each subset of the GenImage++ benchmark.\nSubset Flux Flux_multi Flux_photo Flux_real SD1.5_multi SDXL_multi SD3 SD3_photo SD3_real\nImages 6k 18.3k 30k 6k 18.3k 18.3k 6k 30k 6k\nBase Subsets (Flux, SD3):\n•Target: Baseline performance on FLUX.1 and SD3.\n•Prompting: Following the GenImage protocol, \"photo of a [ImageNet Label]\" using labels\nfrom ImageNet-1k [5].\n•Scale: 6000 images per model (see Table 6).\nRealistic Long-Prompt Subsets (Flux_realistic, SD3_realistic):\n•Target: Long-prompt comprehension and detailed scene generation.\n•Prompting: ImageNet labels were expanded by Llama-3.1 using the system prompt: \"You\nare now a text-to-image prompt generator. For the given ImageNet class labels, integrate\nthem into more harmonious and detailed scenes. Provide the prompt directly without any\nadditional output.\" The ImageNet label was then provided as the user message.\n•Scale: As per Table 6.\n•Example: Label: Goldfish -> \"In a serene Zen garden, a majestic goldfish swims majesti-\ncally in a tranquil pool of crystal-clear water, surrounded by intricately raked moss and stone\nformations that reflect the warm sunlight filtering through a waterfall in the background.\"\nLabel: White Shark -> \"A colossal great white shark swimming effortlessly through\ncrystal-clear, turquoise waters, surrounded by a vibrant coral reef, teeming with an array\nof tropical fish, including iridescent blue damselfish and shimmering yellow tang, while a\nsunny sky with puffy white clouds reflects off the rippling ocean surface.\",\nMulti-Style Subsets (Flux_multistyle, SDXL_multistyle, SD1.5_multistyle):\n•Target: Style diversity and adherence.\n•Prompting: Based on 183 style templates from the ComfyUI_MileHighStyler repository2.\nFor each style template (which contains a placeholder for a scene), Llama-3.1 generated a\nfitting scene description using the system prompt: \"I have a style description that contains a\nplaceholder for a specific scene, marked as {{prompt}}. Please provide a detailed and vivid\ndescription to fill in this placeholder. Only return a single scene description for {{prompt}},\nwithout including any additional text or references to the style or the prompt itself. Style:\n[original style template from JSON file]\".\n•Scale: 100 images per style for each of the 183 styles, totaling 18,300 images per model\n(Flux, SDXL, SD1.5). Style compatibility for FLUX.1 was referenced from a community-\nmaintained list3.\n•Example: \"name\": \"sai-origami\", \"prompt\": \"origami style In a tranquil morning mist,\ndelicate petals unfold like a gentle lover’s whisper, as if the very essence of cherry blossoms\nhad been distilled into this exquisite origami piece. Soft pink hues dance across the intricate\npleats, evoking the tender promise of spring’s awakening. The delicate flower, masterfully\ncrafted from layers of pleated paper, rises from the serene landscape like a whispering\nprayer. Each fold and crease has been carefully crafted to evoke the subtle nuances of\nnature’s beauty, its beauty so captivating that it appears almost lifelike. Its central axis,\na slender stem, supports the delicate blossom, guiding it upwards with an elegance that\n2https://github.com/TripleHeadedMonkey/ComfyUI_MileHighStyler/blob/main/sdxl_\nstyles_twri.json\n3https://enragedantelope.github.io/Styles-FluxDev/\n19\n--- Page 20 ---\nbelies the complexity of its creation. Around it, the gentle curves of the paper seem to blend\nseamlessly with the misty atmosphere, creating an ethereal balance between the organic and\nthe man-made. The overall composition is a testament to the timeless beauty of origami art,\nwhere the simplicity of a single piece can evoke a profound sense of serenity and wonder,\ninviting the viewer to pause and appreciate the intricate beauty that unfolds before them. .\npaper art, pleated paper, folded, origami art, pleats, cut and fold, centered composition\" ,\n\"name\": \"artstyle-graffiti\", \"prompt\": \"graffiti style The city’s concrete heart beats stronger\nunder the vibrant glow of a street art haven. Amidst a maze of alleys, where the city’s\nunderbelly roars to life at dusk, a towering mural sprawls across a worn brick wall. \"Phoenix\nRising\" - the mural’s bold title is emblazoned across the top in fiery red and orange hues,\nits letters splattered with an explosive mixture of colors that dance like flames. Below, a\nmajestic bird bursts forth from the ashes, its wings outstretched, radiating a kaleidoscope of\ncolors - sapphire, emerald, and sunshine yellow. In the background, a cityscape is reduced\nto smoldering embers, the remnants of a ravaged metropolis that has been reborn through\nthe unrelenting power of art. Skyscrapers, reduced to skeletal frames, pierce the night sky\nlike splintered shards of glass. Smoke billows from the rooftops, but amidst the destruction,\na phoenix rises. Its eyes, like two glittering opals, seem to shine with an inner light, as if the\nvery essence of rebirth has been distilled within its being. The mural’s tag, signed in bold,\nsweeping script as \"Kaos\", runs across the bottom of the image, a rebellious declaration\nof the artist’s intent: to shatter the conventions of urban decay and bring forth a radiant\nnew world from the ashes. The colors blend, swirl, and merge, creating an immersive\nexperience that defies the boundaries between reality and fantasy. \"Phoenix Rising\" stands\nas a testament to the transformative power of street art, a beacon of hope in a city that never\nsleeps, a mural that whispers to all who pass by: \"Rise from the ashes, for in the darkness\nlies the seeds of rebirth.\" . street art, vibrant, urban, detailed, tag, mural\" ,\nHigh-Fidelity Photorealistic Subsets (Flux_photo, SD3_photo):\n•Target: High-quality photorealism, including human portraits.\n•Prompting: Prompts were generated by Llama-3.1 based on a structured template inspired by\ncommunity best practices for detailed photo generation. The template format was: ‘[STYLE\nOF PHOTO] photo of a [SUBJECT], [IMPORTANT FEATURE], [MORE DETAILS],\n[POSE OR ACTION], [FRAMING], [SETTING/BACKGROUND], [LIGHTING], [CAM-\nERA ANGLE], [CAMERA PROPERTIES], in style of [PHOTOGRAPHER]‘. Llama-3.1\nfilled these placeholders randomly using the system instruction: \"You are a prompt generator\nfor text to image synthesis, now you should randomly generate a prompt for me to generate\nan image. The prompt requirements is: [prompt_format]. Prompt only, no more additional\nreply\".\n•Scale: As per Table 6.\n•Example: \"Moody black and white photo of a woman, dressed in a flowing Victorian-era\ngown, holding a delicate, antique music box, standing on a worn, wooden dock, overlooking\na misty, moonlit lake, with a faint, lantern glow in the distance, in the style of Ansel Adams.\"\n\"High-contrast black and white photo of a majestic lion, with a strong mane and piercing eyes,\nstanding proudly on a rocky outcrop, in mid-stride, with the African savannah stretching\nout behind it, bathed in warm golden light, from a low-angle camera perspective, with a\nwide-angle lens, in the style of Ansel Adams.\"\nC.4 Rationale as a Test-Only Benchmark\nGenImage++ is intentionally a test-only dataset and does not include corresponding real images for\neach generated category. This design choice is due to several factors:\n1.Copyright and Sourcing Challenges: Sourcing perfectly corresponding, high-quality real\nimages for the vast array of styles, complex scenes, and specific portrait attributes generated\nby modern models poses significant copyright and practical challenges, especially avoiding\nthe use of web-scraped data that may itself contain AI-generated content or copyrighted\nmaterial. Using reverse image search is often infeasible for highly stylized or uniquely\ncomposed generations.\n20\n--- Page 21 ---\n2.Focus on Generalizable Artifacts: Our primary aim is to test a detector’s ability to identify\nintrinsic generative artifacts ( fgen) that are independent of specific semantic content.\n•For the Base Label andRealistic Long-Prompt subsets, the underlying semantic cate-\ngories are largely derived from ImageNet, for which abundant real image counterparts\nalready exist within the original GenImage dataset and other standard vision datasets.\n•For the Multi-Style subsets, our experiments (Section 6.4) show that detectors trained\non SDv1.4 can easily generalize to SDv1.5-generated styles but struggle immensely\nwith styles from Flux or SDXL. This suggests the difficulty lies not in recognizing the\nstyled semantic content (which would be a c-dependent feature) but in identifying the\ndiffering generative artifacts of the underlying models.\nTherefore, GenImage++ serves as a robust benchmark for evaluating a detector’s generalization\nto new generator architectures and advanced prompting techniques, focusing on the detection of\nAI-specific fingerprints rather than simple real-vs-fake comparison on matched content. Beyond\nthe generated images themselves, all code used for data generation, along with the structured\nJSON files containing the prompts for each subset, will be made publicly available to facilitate\nfurther research and benchmark replication.\nD Extended Discussion and Future Directions\nD.1 Broader Relevance of Robustness to Latent Perturbations\nThe robustness against perturbations in the initial latent space zTholds significance beyond detecting\nstandard text-to-image outputs generated from purely random noise. Several increasingly common\nAIGC tasks involve non-standard or optimized initial latent variables:\n•Image Editing and Inpainting: These tasks often initialize the diffusion process from\na latent code obtained by inverting a real image, potentially modifying this latent before\ngeneration [21].\n•Optimized Initial Noise: Recent work focuses on finding \"better\" initial noise vectors\nzTthan random Gaussian samples to improve generation quality or efficiency, sometimes\nreferred to as \"golden noise\" [38].\nA detector solely robust to outputs from standard Gaussian noise zrand\nT might be vulnerable when\nencountering images produced in these scenarios. Therefore, by adversarially training our detector\nusing examples derived from optimized latents zadv\nT, we not only aim to improve generalization by\nforcing the learning of fgen, but also potentially enhance the detector’s applicability and robustness\nacross this wider spectrum of AIGC tasks that utilize non-standard latent initializations. This presents\nan interesting avenue for future investigation.\nD.2 Limitations\nWhile our proposed on-manifold latent optimization attack and subsequent adversarial training\ndemonstrate significant improvements in AIGC detector generalization, we acknowledge certain\nlimitations in the current study.\nComputational Cost of Latent Optimization Attack. Our white-box attack methodology, which\noptimizes the initial latent noise zT, requires maintaining the computational graph through each\ndenoising step of the diffusion model to backpropagate gradients from the detector’s loss back to zT.\nThis incurs a substantial VRAM cost. For instance, attacking a detector using Stable Diffusion v1.4\nwith 35 denoising steps consumed approximately 79GB of VRAM on an NVIDIA A100 GPU. This\nhigh memory requirement currently limits the scalability of generating very large adversarial datasets\nusing this specific attack technique or applying it to even larger generator models without access to\nhigh-end hardware.\nRequirement for Differentiable Preprocessing and Detector Retraining. To ensure end-to-end\ndifferentiability from the detector’s loss back to the initial latent zT, all intermediate image prepro-\ncessing steps (e.g., resizing, normalization) must also be differentiable. In our implementation, this\nnecessitated the use of libraries like Kornia [ 27] for these transformations, rather than potentially\n21\n--- Page 22 ---\nmore common, non-differentiable pipelines involving libraries like Pillow or standard ‘torchvi-\nsion.transforms‘ if they break the gradient chain during the attack generation. Consequently, when\ngenerating adversarial examples against a specific target detector, or when performing adversarial\ntraining, the detector ideally needs to be trained or fine-tuned within a framework that utilizes such\ndifferentiable preprocessing (as our baseline detectors were). This can limit the direct \"out-of-the-\nbox\" application of our attack generation process to arbitrary pre-trained third-party detectors if their\noriginal training and inference preprocessing pipelines are not fully differentiable or easily replicable\nwith differentiable components. While we retrained baselines like ResNet50 and CLIP+Linear within\nthis Kornia-based framework for fair evaluation and adversarial training, evaluating a wide array of\nexternally pre-trained detectors (whose exact preprocessing might be unknown or non-differentiable)\nwith our specific latent attack would require re-implementing or adapting their input pipelines.\nFuture work could explore methods to reduce the VRAM footprint of the latent attack, perhaps\nthrough gradient checkpointing within the diffusion model during attack or by developing more\nmemory-efficient approximation techniques. Additionally, investigating strategies to adapt our\nlatent attack or on-manifold adversarial examples to detectors with non-differentiable preprocessing\npipelines could further broaden the applicability of our findings.\nD.3 Broader Impacts\nThe research presented in this paper aims to advance the robustness and generalization of AIGC\ndetection, which is crucial for mitigating potential misuse of AI-generated content, such as disinfor-\nmation or non-consensual imagery. By identifying and addressing vulnerabilities like latent prior\nbias, we hope to contribute to more reliable forensic tools.\nHowever, as with any research involving adversarial attacks, there is a potential for the attack\nmethodology itself to be misused. The on-manifold latent optimization attack described could,\nin principle, be employed by malicious actors to craft AIGC that evades detection systems. We\nacknowledge this possibility but believe the direct negative impact of our specific attack method is\nlikely limited due to several factors:\n•White-Box Requirement: Our attack is a white-box method, requiring full access to the\ntarget detector, including its architecture, weights, and the ability to compute gradients with\nrespect to its inputs. This significantly restricts its applicability against closed-source or\nAPI-based detection services where such access is not available.\n•Computational Cost: As discussed in Section D.2, generating adversarial examples via\nlatent optimization is computationally intensive, particularly in terms of VRAM. This may\nact as a practical barrier for large-scale misuse by those without significant computational\nresources.\nFurthermore, the primary contribution of this work lies in using these attacks as a diagnostic tool to\nunderstand detector weaknesses (latent prior bias) and subsequently as a means to improve detector\nrobustness through on-manifold adversarial training (OMAT). The insights gained are intended to\nstrengthen defenses, and the OMAT paradigm itself offers a pathway to more resilient detectors. We\nwill also make our GenImage++ benchmark publicly available, which will aid the community in\ndeveloping and evaluating more robust detectors against contemporary generative models.\n22\n--- Page 23 ---\nFigure 8: Qualitative comparison of adversarial examples generated for various ImageNet labels.\nEach row corresponds to a different label. Columns display results from different L∞-bounded\npixel-space attacks (FGSM, PGD, MI-FGSM with varying ϵ) and our proposed on-manifold latent\noptimization attack (rightmost column). Pixel-space attacks often introduce visible noise, especially\nat higher ϵ, while our on-manifold attack maintains high visual fidelity consistent with the generator’s\ncapabilities.\n23",
  "text_length": 77984
}