{
  "id": "http://arxiv.org/abs/2506.04172v1",
  "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?",
  "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation.",
  "authors": [
    "Shreenidhi Srinivasan",
    "Lydia Manikonda"
  ],
  "published": "2025-06-04T17:15:19Z",
  "updated": "2025-06-04T17:15:19Z",
  "categories": [
    "cs.LG",
    "cs.ET"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04172v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04172v1  [cs.LG]  4 Jun 2025Does Prompt Design Impact Quality of Data Imputation by LLMs?\nShreenidhi Srinivasan, Lydia Manikonda1\n1Corresponding author\nRensselaer Polytechnic Institute\nTroy, New York, USA\n{srinis10, manikl }@rpi.edu\nAbstract\nGenerating realistic synthetic tabular data presents a critical\nchallenge in machine learning. It adds another layer of com-\nplexity when this data contain class imbalance problems. This\npaper presents a novel token-aware data imputation method\nthat leverages the in-context learning capabilities of large lan-\nguage models. This is achieved through the combination of\na structured group-wise CSV-style prompting technique and\nthe elimination of irrelevant contextual information in the in-\nput prompt. We test this approach with two class-imbalanced\nbinary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics.\nThe experimental results demonstrate that our approach sig-\nnificantly reduces the input prompt size while maintaining\nor improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller\nin size. The contributions of this presented work is two-fold\n– 1) it sheds light on the importance of prompt design when\nleveraging LLMs for synthetic data generation and 2) it ad-\ndresses a critical gap in LLM-based data imputation for class-\nimbalanced datasets with missing data by providing a practi-\ncal solution within computational constraints. We hope that\nour work will foster further research and discussions about\nleveraging the incredible potential of LLMs and prompt en-\ngineering techniques for synthetic data generation.\nIntroduction\nTabular data is one of the most common forms of data in ma-\nchine learning. Over 65% of datasets in the Google Dataset\nSearch platform (Google 2025) contain tabular files in either\nCSV or XLS formats (Benjelloun, Chen, and Noy 2020).\nSome of the major issues with tabular data are that they –\n1) are often class-imbalanced, 2) unavailable because of pri-\nvacy concerns and 3) contain noisy or missing data. Perfor-\nmance of machine learning models depends on the quality\nand quantity of data they are trained on. Thus, research on\ntechniques to alleviate the three important problems men-\ntioned above have received considerable attention in the re-\ncent years. In this paper, we aim to develop a data imputa-\ntion technique for class-imbalanced tabular datasets using\nLarge Language Models (LLMs) .\nIn several real-world classification problems, it is com-\nmon for certain classes to have a significantly larger num-\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.ber of samples (majority classes), while others are under-\nrepresented in the dataset (minority classes). This imbal-\nance may stem from a lack of sufficient samples for the mi-\nnority class or the high cost associated with obtaining such\ndata. Consequently, a model trained on this type of dataset\nwhen deployed in the wild becomes biased and tends to per-\nform poorly for the minority class. In addition to the above\nmentioned concerns about real-world tabular datasets, they\ncan be complex to handle, in general. Tabular datasets typ-\nically contain both categorical and numerical features and\nhence, they require extensive preprocessing. Data prepro-\ncessing typically involves steps like encoding categorical\ndata into numbers, data scaling or normalization and remov-\ning outliers. However, these actions could lead to the loss of\ncrucial information or introduction of artifacts that weren’t\npresent in the original data.\nVarious strategies to deal with class-imbalanced datasets\nhave been explored in the literature. Traditional solutions in-\nclude undersampling the majority class and/or oversampling\nthe minority class to balance class distribution, as well as hy-\nbrid sampling strategies that focus on selecting and retaining\nchallenging samples while discarding those that are easier\nto learn. Additional methods include cost-sensitive analysis\nand synthetic data generation. The process of synthetic data\ngeneration for the minority class involves creating new sam-\nples corresponding to the minority class to augment the orig-\ninal dataset. Several techniques have been proposed for this\npurpose, including sampling methods like SMOTE (Chawla\net al. 2002). With the rise of deep learning models, Varia-\ntional Autoencoders (Kingma and Welling 2013) and Gen-\nerative Adversarial Networks (Goodfellow et al. 2014) have\nproven effective in generating realistic tabular data (Kim,\nKim, and Choo 2024b). Notably, GReaT (Borisov et al.\n2022) has surpassed previous methods in producing tabu-\nlar data formatted in natural language. However, GReaT re-\nquires the fine-tuning of parameters in large language mod-\nels, which can be resource-intensive, as extensive training\nis needed for each dataset (Kim, Kim, and Choo 2024b).\nConsequently, applying this technique across a variety of\ndatasets and domains poses challenges.\nLately, there has been a notable advancement in prompt\nengineering research (Brown et al. 2020; Guo et al. 2023;\nKojima et al. 2022; Wei et al. 2022; Zhou et al. 2022) to\nleverage the capabilities of LLMs for specific tasks while\n--- Page 2 ---\nmitigating additional training costs (Kim, Kim, and Choo\n2024b). Kim et al. (Kim, Kim, and Choo 2024b) proposes\na novel group-wise prompting method presented in a CSV-\nstyle format to generate high-quality synthetic data for class-\nimbalanced datasets. This method exploits the in-context\nlearning capabilities of LLMs to generate data that closely\naligns with the desired characteristics of the target dataset.\nThe method consistently enhances machine learning perfor-\nmance across eight public datasets while maintaining the in-\ntegrity of data distributions and feature correlations. In this\npaper, we propose an analogous prompting strategy tailored\nfor data imputation rather than synthetic data generation.\nOur proposed method leverages the in-context learning\nabilities of LLMs using a structured CSV-style prompting\ntechnique designed to fill in the missing values in tabular\ndatasets effectively. The core contribution of this paper is\ndeveloping a procedure to maintain the quality of imputa-\ntion while minimizing the input and output token size . The\nimputation process focuses on one feature at a time utilizing\ninformation from its strongly correlated predictors, and this\nsignificantly reduces the input prompt length to accommo-\ndate more data samples within fixed token constraints. The\nprompt design omits weakly correlated or unrelated features\nthat prevent the LLM from performing its assigned task ef-\nfectively. Several research studies show that irrelevent con-\ntext can significantly degrade the performance of LLMs (Jin\net al. 2025; Jiang et al. 2024a; Shi et al. 2023). We measure\nthe quality of imputation based on the classification perfor-\nmance of machine learning models built for two separate\ndatasets obtained from Kaggle. We report the class-wise pre-\ncision, recall, F1 and balanced accuracy scores. Our experi-\nmental results suggest that we successfully reduce the size of\ninput prompt while maintaining the quality of imputation .\nRelated Work Synthetic data is defined as an artificially\ngenerated dataset mimicking real world data where, in the\ncontext of machine learning, they are typically generated\nusing algorithms or neural networks (Nadas, Diosan, and\nTomescu 2025; Park et al. 2018). Synthetic datasets are typ-\nically used in several domains and applications where, ob-\ntaining large amounts of training data is difficult due to\nseveral reasons. As machine learning models became more\ncomplex, they needed large quantities of high-quality train-\ning data that is also realistic. This led to using generative\nmodels to generate synthetic data (Eigenschink et al. 2023;\nJiang et al. 2024b; Yu et al. 2023a,b). Also, several factors\nsuch as costs of collecting data via surveys or experiments,\nscarcity of data in certain domains, flexibility of scaling and\ncontrolling the underlying data distributions, preserving pri-\nvacy, etc., led to a growing reliance on large language mod-\nels to generate synthetic datasets (Liu et al. 2024; Li et al.\n2024; Long et al. 2024). Building upon recent advancements\nin prompt engineering research for synthetic data genera-\ntion, our work presented in this paper specifically focuses\non data imputation using LLMs and investigates how prompt\ndesign will have an influence on the quality of data imputa-\ntion for class-imbalanced datasets (especially for the minor-\nity class) as evaluated by classification metrics.Data and Preprocessing\nWe test our approach with two publicly available tabular\nclassification datasets, namely, the Adult Income dataset and\nthe Travel dataset from Kaggle (Wenruliu and Becker 2024;\nIsaell 2023). The feature descriptions for the prompt are di-\nrectly sourced from Kaggle’s data descriptions when avail-\nable or we use ChatGPT to generate them. As shown in\nTable 1, the Adult Income dataset contains 48,842records\nwith 14features. The target column for this dataset is ‘in-\ncome’ that takes two categorical values, <= 50 K (majority\nclass) and >50K (minority class). The dataset contains\n3620 rows with missing values in the features ‘workclass’,\n‘occupation’ and ‘native-country’. For our experiment, we\nconsider 958 rows with missing entries with 479 records\ncorresponding to each income class. The Travel dataset con-\ntains 954records with 6features. Its target column is ‘Tar-\nget’ which takes two values 0(customer does not churn) and\n1(customer churns). In this dataset, 1corresponds to the mi-\nnority class. The original dataset contains a total of 60miss-\ning values in the ‘FrequentFlyer’ feature column. For our\nexperiment, we consider a synthetic version of this dataset\nobtained by introducing artificial missingness into the ‘Fre-\nquentFlyer’ column to give a total of 120missing values ( 60\nin each class). The missingness is introduced in randomly\nchosen records from each class.\nBoth these datasets are class-imbalanced with the minor-\nity class comprising about 24% of the records. Apart from\nthe domain differences of these two datasets, they signifi-\ncantly differ in terms of the total number of data points and\ntotal number of features. The Adult Income dataset is much\nlarger ( 48,842data points) compared to Travel dataset ( 954\nrecords). Also, Adult Income has about double the number\nof features as compared to Travel as shown in Table 1.\nDataset #Class #Feats. #Samples #Incomplete\nsamples\nTravel 2 7 954 120\nIncome 2 15 48,842 3,620\nTable 1: Summary of the datasets used in our work.\nMethodology\nIn this study, we introduce an approach for tabular data im-\nputation using LLMs. The missing entries are imputed us-\ning a CSV-style prompt carefully crafted to deal with class\nimbalanced datasets. Our objective is to fill in the missing\nvalues with a focus on minimizing input and output token\nusage. This study demonstrates the incredible potential of\nLLMs in the task of tabular data imputation. Inspired by\nan effective and token-efficient prompting technique used\nfor synthetic data generation for imbalanced datasets (Kim,\nKim, and Choo 2024a), we adopt a similar group-wise\nprompting method to perform missing data imputation in\nimbalanced tabular datasets. In an effort to reduce the input\ntoken consumption further, we only include the features that\nhold a strong relationship with the feature that contains the\nmissing entries to be imputed using the prompt. For datasets\nwith multiple features with missing values, we impute one\nfeature at a time. By including information from the most\n--- Page 3 ---\nCSV Prompt Style Income >50K Overall\nPrec. Recall F1 F1\nUngrouped 0.80 0.48 0.60 0.60\nGrouped 0.98 0.62 0.76 0.76\nTable 2: Comparison of XGBoost classification performance\nfor minority class using grouped and ungrouped CSV-style\nprompt design for the Adult Income dataset.\nrelevant feature columns only, we not only reduce the to-\nken size of the input prompt but also eliminate noisy data\nthat can cause the LLM to learn patterns that do not accu-\nrately reflect the true dataset structure and relationships. We\nmeasure the effectiveness of imputation based on the classi-\nfication performance of two ensemble learning models.\nGroup-wise Prompting Method\nThe main idea is to construct prompts in a structured and\npredictable format, aiming to guide LLMs in synthesizing\ndata entries maintaining the characteristics of the original\ndataset. This prompting method is designed to leverage the\nin-context learning capability of LLMs. Our goal is to sur-\npass the bare enumeration of examples in the prompt, ac-\nknowledging the significance of this structured presentation\nin the production of high-quality realistic data entries. The\nprompt starts with an explicit instruction to the LLM to per-\nform the imputation task by utilizing its in-context learning\nabilities. The instruction also tells the LLM to output the\nimputed feature column only (rather than the entire dataset)\nto reduce the output token size. This also helps in lower-\ning the computational costs associated with the imputation\nprocess. In addition, the missing record sample size is ex-\nplicitly stated to prevent the LLM from generating useless\ntokens after the response. The prompt continues with a set of\nbrief one-line descriptions of features in the dataset to pro-\nvide enhanced dataset context for imputation. The descrip-\ntions are followed by a header listing all the feature names\nin comma-separated format. The header is followed by pre-\ndefined groups and each group consists of a fixed number\nof data samples. The groups are usually determined by the\nclasses present in the target, for example, the groups for\nthe Travel dataset would be Customer churns andCustomer\ndoes not churn . The samples for each group are obtained\nby random sampling from complete records corresponding\nto that group in the dataset. This template is repeated with\nvarious samples from the dataset. Below the completed ex-\namples, the same number of records with missing values\nare presented in group-separated format and the LLM is in-\nstructed to impute the missing values based on its analysis\nof the complete samples.\nDetermining Correlation Threshold\nWe first determine the correlation between each imputation\nfeature (feature with missing values) and other columns in\nthe dataset. We use Pearson correlation between two numer-\nical features, Cramer’s V between categorical variables and\neta (η) correlation ratio between a categorical and numerical\nvariable. We combine the absolute values of all correlationsinto a single list, rank them in descending order and plot\nthem to determine the “elbow point” in the plot. This is the\npoint where the correlation values drop sharply, and then the\nrate of decrease becomes notably less steep. The correlation\nvalue at this elbow point is a lower bound for our thresh-\nold. After repeating this process for all imputation features,\nwe can either choose the minimum correlation threshold or\njust keep different thresholds for different features. If the\ncorrelation values are approximately equal, we assign this\napproximate value to the threshold. The way we combine\nthe correlation thresholds of different features depends on\nthe dataset under consideration. After obtaining the overall\nthreshold, we select all columns having higher correlation\nwith the imputation feature than the threshold value. We re-\npeat the experiment with another value below the obtained\nthreshold for comparison.\nData Imputation Technique\nWe impute all features with missing entries considering one\nfeature at a time. The technique is designed to generate high-\nquality data to impute missing values in such a way that the\ninput prompt length is optimized. One known fact is that re-\nducing the prompt length decreases the computational costs\nof the LLM and enables more information to be fed in for\nthe same token size constraints. In this paper, we show that\nby discarding dataset features that exhibit weak associations\nwith the feature being imputed and focusing on the most\nrelevant information, we can achieve an imputation quality\natleast as good as using the complete dataset information.\nOnce we fix the correlation threshold and select the im-\nportant columns for each imputation feature, we determine\nthe imputation feature that is considered relevant for the\nhighest number of other imputation features. This approach\nallows us to add increasingly informative context to the\nprompt as we impute more features, thereby enhancing the\nimputation process for the remaining features.\nOnce the order of feature imputation is determined, we in-\nstruct the LLM to impute missing values in the first feature\ncolumn using a group-wise CSV-style prompt. The LLM\noutputs the completed feature column after imputing miss-\ning values. Then, we update the prompt by replacing the\noriginal feature column with missing entries by the com-\npleted column outputted by the LLM. Then, we use this up-\ndated prompt for imputing the next feature. We continue to\nupdate the prompt after imputing each feature to add more\ncontext for the LLM to perform the next imputation. The\nprocess continues until we impute all the missing values in\nthe dataset.\nExperiments and Insights\nWe use the latest GPT 4.1 model to impute missing val-\nues in both datasets. An example prompt to perform this\ntask is shown in Table 9. Across all the experiments, we re-\nport results from two ML classifiers: XGBoost andRandom\nforest classifier . These classifiers are trained on the origi-\nnal datasets (with missing rows dropped) and tested on the\nset of imputed records. We compare the quality of imputa-\ntion for three correlation threshold values for each dataset\n--- Page 4 ---\nModel used Correlation Target-0 Target-1\nThreshold Prec. Recall F1 Prec. Recall F1\nXGBoost0 0.67 0.97 0.79 0.94 0.52 0.67\n0.15 0.69 0.98 0.81 0.97 0.57 0.72\n0.2 0.70 0.98 0.82 0.97 0.58 0.73\nRandom Forest0 0.68 0.90 0.77 0.85 0.57 0.68\n0.15 0.70 0.97 0.81 0.95 0.58 0.72\n0.2 0.70 0.97 0.81 0.95 0.58 0.72\nTable 3: Classification performance by target class and\nmodel for the Travel dataset\nas assessed by ML classification performance. The imputa-\ntion method is evaluated using F1 scores, precision, recall,\nbalanced accuracy (BAL ACC) and ROC AUC scores. We\nfirst report the results from our experiments evaluating how\nprompt design influences the quality of data imputation as\nevidenced through classification-based evaluation metrics.\nThen, we investigate how feature space reduction influences\nimputation quality.\nExploring the importance of prompt design\nWe investigate the importance of the CSV-style group-wise\nprompting method for imputing missing values by compar-\ning it to a CSV-style prompt with the same number of com-\npleted examples and missing records, but with no clear class\nseparation. The example records for the ungrouped-example\nprompt are obtained by random sampling from the com-\npleted records of the original dataset. The precision, recall\nand F1 scores for the minority class and the overall F1 scores\nobtained using XGBoost classifier are reported in Table 2.\nFor our experiment, we have trained the model on the origi-\nnal Adult Income dataset and tested it on the set of imputed\nrecords. We observe a notable increase in the the F1 scores\nfor the minority class and a boost in the overall performance\nof the model for group-wise CSV style prompt as compared\nto the ungrouped CSV-style prompt.\nModel Used Corr. BAL ACC F1 ROC AUC\nThres.\nXGBoost0 0.742 0.667 0.92\n0.15 0.775 0.716 0.95\n0.2 0.783 0.729 0.96\nRandom Forest0 0.733 0.680 0.91\n0.15 0.775 0.722 0.95\n0.2 0.775 0.722 0.96\nTable 4: Overall XGBoost and Random Forest Classifier\nperformance metrics for Travel dataset; Columns in the table\nrepresent the Model Used, Correlation Threshold, Balanced\naccuracy, F1 score and ROC AUC.\nEstimating the correlation threshold on our example\ndatasets : We evaluate the correlation between each of the\nimputation features with all other features in the dataset us-\ning Cramer’s V for categorical features and eta correlation\nratio for numerical features. The plots of absolute correla-\ntion values are shown in Figure 1. For the Adult Income\ndataset, the imputation features are ‘workclass’, ‘occupa-\nFigure 1: Elbow Plot illustrating the selection of a correla-\ntion threshold for imputing missing entries in the Adult In-\ncome and Travel datasets. The plot shows sorted absolute\ncorrelation values of the imputations features with the po-\ntential predictor features in the dataset.\ntion’ and ‘native-country’. For this dataset, we observe that\nthe elbow point is between 0.20and 0.25for ‘workclass’\nfeature, around 0.20for ‘occupation’ and around 0.15for\n‘native-country’. We observe that for ‘native-country’, ap-\nplying a correlation cutoff of 0.2for feature selection pro-\nduces the same set of features as applying a cutoff of 0.15.\nThus, we consider a correlation threshold of 0.2in our ex-\nperiments. For the Travel dataset, the imputation feature is\n‘FrequentFlyer’. From the elbow plot showing the correla-\ntion of ‘FrequentFlyer’ with other features, we observe that\nthe elbow point lies between 0.15and0.20. Thus, we select\n0.2as the optimal correlation threshold for this dataset too.\nFor both datasets, we also obtain the imputed datasets by\nconsidering all features in the prompt and with another cor-\nrelation cutoff below the obtained threshold to understand\nthe effect of including noisy features on the ML classifica-\ntion performance.\nExploring the effect of token reduction on ML\nclassification performance\nWe evaluate the quality of imputation by assessing the clas-\nsification performance when ensemble learning models are\ntrained on the original datasets and tested on the set of im-\nputed records. The models used for our investigations are\n–XGBoost andRandom Forest Classifiers. We perform the\nexperiments for three correlation thresholds for each dataset.\nThe number of columns included in the prompt for imputing\n--- Page 5 ---\nModel used Correlation Income <=50KIncome >50K\nThreshold Prec. Recall F1 Prec. Recall F1\nXGBoost0 0.73 0.99 0.84 0.99 0.64 0.77\n0.1 0.72 0.99 0.83 0.98 0.61 0.75\n0.2 0.71 0.99 0.83 0.98 0.60 0.74\nRandom Forest0 0.74 0.98 0.84 0.97 0.65 0.78\n0.1 0.71 0.98 0.83 0.97 0.61 0.75\n0.2 0.71 0.98 0.82 0.97 0.60 0.74\nTable 5: Classification performance by target class and\nmodel for the Adult Income dataset\nmissing values in features corresponding to each correlation\nthreshold is presented in Table 7 and Table 8. As shown in\nTable 5, the F1 scores for the minority class are maintained\n(with a maximum difference of 0.04) despite a significant re-\nduction in number of input features used in the prompt (see\nTable 7). Also, the balanced accuracy scores just drop by\nabout 2.7%despite aggressively shrinking feature informa-\ntion by about 76.19% for both XGBoost and Random For-\nest Classifiers. Also, the models maintain nearly equal ROC\nAUC scores for all thresholds.\nFor Travel, omitting irrelevant features from the prompt\nactually boosts the imputation quality for the minority class\nas seen from the jump in F1 scores by about 7.46% (Table\n3), an increase in overall ROC AUC scores from around 0.91\nto0.95(Table 4) and a rise in balanced accuracy scores by\n4.45% (Table 4) when the correlation threshold is increased\nto0.15. Thus, the group-wise and overall metrics show sig-\nnificant improvement as we eliminate unnecessary infor-\nmation from the prompt for the imputation task. These re-\nsults suggest that LLM-driven prompt modifications signifi-\ncantly influence classification performance in small data set-\nting yielding an improved set of scores. However, in larger\ndatasets this effect is less visible. This may be due to the\nmodel’s reliance on a substantial sample of data that enables\nit to filter out irrelevant information by itself, consequently\nleading to lesser effect on performance scores.\nModel Used Corr. BAL ACC F1 ROC AUC\nThres.\nXGBoost0 0.814 0.774 0.96\n0.1 0.799 0.752 0.96\n0.2 0.792 0.743 0.96\nRandom Forest0 0.813 0.776 0.95\n0.1 0.794 0.748 0.94\n0.2 0.790 0.740 0.94\nTable 6: Overall XGBoost and Random Forest Classifier\nperformance metrics for Adult Income dataset\nDiscussion\nThere are several advantages of leveraging CSV-style\nprompting. Firstly, for minimum data preprocessing : This\nprompting approach eliminates the need for data prepro-\ncessing, retaining the original column names and formats.\nThis strategy also preserves the integrity of raw data,\nenabling the inclusion of both categorical and numericalCorr. No. of Col Retained for Imputation % Reduction in\nThres. Workclass Occupation Native-country Feature Space\n0 14/14 14/14 14/14 -\n0.1 7/14 10/14 4/14 50%\n0.2 3/14 5/14 2/14 76.19%\nTable 7: Column retention counts (No. of columns re-\ntained / Total no. of columns) for imputation of ’work-\nclass’,’occupation’, and ’native-country’ features in the\nAdult Income dataset. Percentage reduction in feature space\nis the ratio between the total number of columns removed\nat each non-zero correlation threshold and the total num-\nber of features at zero threshold, illustrating how increasing\nthe correlation cutoff progressively filters out less correlated\nfeatures and reduces dimensionality.\nCorr. No. of Col. Retained for Imputation % Reduction in\nThres. Feature Space\n0 6/6 -\n0.15 4/6 33.33%\n0.2 2/6 66.67%\nTable 8: Column retention counts (No. of columns retained\n/ Total no. of columns) for imputation of ‘FrequentFlyer’\nfeature in the Travel dataset. Percentage reduction in feature\nspace is the ratio between the total number of columns re-\nmoved at each non-zero correlation threshold and the total\nnumber of features at zero threshold.\nvariables without the need for extensive modifications (Kim,\nKim, and Choo 2024a); Secondly, optimized token usage :\nOur approach optimizes token consumption for each data\nvalue by representing tabular data within prompts using a\nCSV-style format. This format is especially beneficial since\nit permits a greater number of in-context learning instances\nwithin the same token limits, which is especially helpful\ngiven the constrained context window of LLMs (Kim, Kim,\nand Choo 2024a); Thirdly, another advantage of this method\nis that the LLM encounters both the majority and minority\nclass samples in a proportionate manner so it understands\nthe trends within each group effectively before imputing\nsamples from each group. This is in contrast to many of the\nfine-tuning methods, which tend to overfit the majority class\nvalues. Finally, the lower the prompt’s token size, more\ndata samples can be included as examples for the LLM to\nanalyze. This is important as we want the prompt samples\nto represent the original data distribution. The LLMs ability\nto impute missing values is confined to its analysis of\nexamples in the input.\nThrough our experimental evaluation, we also found that\nLLM-based prompt modifications significantly influence the\nclassification performance in case of small-sized datasets\nyielding an improved set of scores when irrelevant features\nwere ignored in the prompt. However, in the case of larger\ndatasets, this effect is relatively less visible. This may be\ndue to the fact that when there is a larger sample of train-\ning data, the model may filter out irrelevant information by\nitself. Thus, there is a relatively lower impact on the per-\n--- Page 6 ---\nformance scores. These observations lead to our next set of\nquestions 1) In what scenarios does prompting amplify or\nsuppress biases present in the original data via its own train-\ning? 2) if the prompt has a larger influence on datasets of\nsmaller sizes, can we guarantee that the synthetic data gen-\neration process could be reproducible? There will be major\nethical issues that may lead to practical consequences in do-\nmains such as health, legal, or finance. Thus, it is crucial to\nfurther explore the impact of prompting on synthetic data\ngeneration and imputation in sensitive domains.\nLimitations We tested our technique on two datasets with\nimputation features being categorical. Thus, the results from\nour experiments may not be immediately generalized to im-\nputation features of all types. Another potential issue could\nbe handling datasets with a majority of the imputation fea-\nture correlations having similar values as obtaining a corre-\nlation threshold that can effectively filter out noisy compo-\nnent features can be challenging. Future research will in-\nclude testing this method with datasets from various do-\nmains containing both numerical and categorical imputation\nfeatures and exploring ways to determine an optimal corre-\nlation threshold in difficult scenarios.\nConclusion\nThe approach discussed in this paper leverages prompt-\ning techniques and the power of feature correlation to de-\nvelop a tabular data imputation method optimized for input\nand output token usage especially for imbalanced datasets.\nWith the help of a group-wise CSV-based prompting method\nand the dataset’s inter-feature associations, this work is a\nstep towards leveraging the in-context learning capabilities\nof LLMs for synthetic data generation for class-imbalance\nproblems. In this work, we tested our method on two binary\nclassification datasets and evaluated the imputation qual-\nity using classification performance by building ensemble\nlearning models. From our observations, we conclude that\nthis method gives the user an opportunity to strike a balance\nbetween a desired level of accuracy and token consumption\ndepending on the use-case. In our tests, we also observed\nthat removing irrelevant information from the prompt can\nboost imputation quality especially for small-sized datasets.\nReferences\nBenjelloun, O.; Chen, S.; and Noy, N. 2020. Google dataset\nsearch by the numbers. In International semantic web con-\nference , 667–682. Springer.\nBorisov, V .; Seßler, K.; Leemann, T.; Pawelczyk, M.; and\nKasneci, G. 2022. Language Models are Realistic Tabular\nData Generators. arXiv preprint arXiv:2210.06280 .\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems , 33:\n1877–1901.\nChawla, N. V .; Bowyer, K. W.; Hall, L. O.; and Kegelmeyer,\nW. P. 2002. SMOTE: synthetic minority over-sampling tech-\nnique. Journal of artificial intelligence research , 16: 321–\n357.Eigenschink, P.; Reutterer, T.; Vamosi, S.; Vamosi, R.; Sun,\nC.; and Kalcher, K. 2023. Deep generative models for syn-\nthetic data: A survey. IEEE Access , 11: 47304–47320.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative Adversarial Nets. In Advances in Neural\nInformation Processing Systems , volume 27.\nGoogle. 2025. Google Dataset Search. https://datasetsearch.\nresearch.google.com/. Accessed: 2025-05-22.\nGuo, J.; Li, J.; Li, D.; Tiong, A. M. H.; Li, B.; Tao, D.; and\nHoi, S. 2023. From images to textual prompts: Zero-shot vi-\nsual question answering with frozen large language models.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 10867–10877.\nIsaell. 2023. Travel Customer Churn Analysis Prediction\n[Dataset]. Kaggle. Accessed: 2025-04-20.\nJiang, M.; Huang, T.; Guo, B.; Lu, Y .; and Zhang, F. 2024a.\nEnhancing robustness in large language models: Prompting\nfor mitigating the impact of irrelevant information. arXiv\npreprint arXiv:2408.10615 .\nJiang, Y .; Garc ´ıa-Dur ´an, A.; Losada, I. B.; Girard, P.; and\nTerranova, N. 2024b. Generative models for synthetic\ndata generation: application to pharmacokinetic/pharmaco-\ndynamic data. Journal of pharmacokinetics and pharmaco-\ndynamics , 1–9.\nJin, H.; Chen, P.; Yang, J.; Wang, Z.; Jiang, M.; Gao, Y .;\nHuang, B.; Zhang, X.; Li, Z.; Liu, T.; et al. 2025. END: Early\nNoise Dropping for Efficient and Effective Context Denois-\ning. arXiv preprint arXiv:2502.18915 .\nKim, J.; Kim, T.; and Choo, J. 2024a. EPIC: Effective\nPrompting for Imbalanced-Class Data Synthesis in Tabular\nData Classification via Large Language Models. Advances\nin Neural Information Processing Systems , 37: 31504–\n31542.\nKim, J.; Kim, T.; and Choo, J. 2024b. Group-wise Prompt-\ning for Synthetic Tabular Data Generation using Large Lan-\nguage Models. Advances in Neural Information Processing\nSystems , 33: 1877–1901.\nKingma, D. P.; and Welling, M. 2013. Auto-Encoding Vari-\national Bayes. arXiv preprint arXiv:1312.6114 .\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reasoners.\nAdvances in Neural Information Processing Systems , 35:\n22199–22213.\nLi, Y .; Bonatti, R.; Abdali, S.; Wagle, J.; and Koishida, K.\n2024. Data generation using large language models for\ntext classification: An empirical case study. arXiv preprint\narXiv:2407.12813 .\nLiu, R.; Wei, J.; Liu, F.; Si, C.; Zhang, Y .; Rao, J.; Zheng,\nS.; Peng, D.; Yang, D.; Zhou, D.; et al. 2024. Best prac-\ntices and lessons learned on synthetic data. arXiv preprint\narXiv:2404.07503 .\nLong, L.; Wang, R.; Xiao, R.; Zhao, J.; Ding, X.; Chen, G.;\nand Wang, H. 2024. On llms-driven synthetic data gener-\nation, curation, and evaluation: A survey. arXiv preprint\narXiv:2406.15126 .\n--- Page 7 ---\nNadas, M.; Diosan, L.; and Tomescu, A. 2025. Synthetic\nData Generation Using Large Language Models: Advances\nin Text and Code. arXiv preprint arXiv:2503.14023 .\nPark, N.; Mohammadi, M.; Gorde, K.; Jajodia, S.; Park, H.;\nand Kim, Y . 2018. Data synthesis based on generative ad-\nversarial networks. arXiv preprint arXiv:1806.03384 .\nShi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi,\nE. H.; Sch ¨arli, N.; and Zhou, D. 2023. Large language mod-\nels can be easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning , 31210–31227.\nPMLR.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems ,\n35: 24824–24837.\nWenruliu; and Becker, B. 2024. Adult Income Dataset\n[Dataset]. Kaggle. Accessed: 2024-04-10.\nYu, Y .; Zhuang, Y .; Zhang, J.; Meng, Y .; Ratner, A. J.; Kr-\nishna, R.; Shen, J.; and Zhang, C. 2023a. Large language\nmodel as attributed training data generator: A tale of diver-\nsity and bias. Advances in Neural Information Processing\nSystems , 36: 55734–55784.\nYu, Y .; Zhuang, Y .; Zhang, R.; Meng, Y .; Shen, J.; and\nZhang, C. 2023b. Regen: Zero-shot text classification via\ntraining data generation with progressive dense retrieval.\narXiv preprint arXiv:2305.10703 .\nZhou, Y .; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.;\nChan, H.-C.; and Ba, J. 2022. Large language mod-\nels are human-level prompt engineers. arXiv preprint\narXiv:2211.01910 .Appendix\n--- Page 8 ---\nTemplate Prompt sample\nDescriptionsChurn: whether customer churns or doesnt churn for tour and travels company,\nage: the age of customer,\nFrequentFlyer: whether customer takes frequent flights,\nAnnualIncomeClass: class of annual income of user,\nServicesOpted: number of times services opted during recent years,\nAccountSyncedToSocialMedia: whether company account of user synchronised to their social media,\nSet HeaderChurn, Age, FrequentFlyer, AnnualIncomeClass, ServicesOpted,\nAccountSyncedToSocialMedia, BookedHotelOrNot\nGroupA.\nChurn, 28, Yes, High Income, 6, No, Yes\nChurn, 37, Yes, Low Income, 4, Yes, Yes\nChurn, 30, Yes,Low Income, 1,Yes,Yes\nGroupB.\nDoesnt churn, 38, No, Low Income,1,Yes,No\nDoesnt churn, 28, No Record,Low Income,5,No,Yes\nDoesnt churn, 34, Yes, Low Income, 1,No,No\nSet HeaderChurn, Age, FrequentFlyer, AnnualIncomeClass, ServicesOpted,\nAccountSyncedToSocialMedia, BookedHotelOrNot\nGroupA.\nChurn, 29, Yes, Low Income, 6, No, No\nChurn, 37, Yes, High Income, 4, Yes, Yes\nChurn, 25, Yes,Low Income, 2,Yes,Yes\nGroupB.\nDoesnt churn, 33, Yes, High Income,1,Yes,No\nDoesnt churn, 30,Yes,Low Income,5,No,Yes\nDoesnt churn, 34,No, Low Income, 1,No,Yes\nGiven the above data, fill in the missing values in the data sample below:\nGroupA.\nChurn, 28, Yes, No record, 6, No, Yes\nChurn, 37, Yes, Low Income, 4,No Record, Yes\nChurn, 30,No Record,Low Income, 1,Yes,Yes\nGroupB.\nDoesnt churn, 38, No, Low Income,1,Yes,No Record\nDoesnt churn, 28, No Record,Low Income,5,No Record,Yes\nDoesnt churn, 34,No Record, Low Income, 1,No,No\nTable 9: Example of a group-wise CSV-style prompt for the Travel dataset. This prompt contains two sets of completed samples\nto impute one set of missing records with the same sample size. The completed records are extracted by random sampling from\nboth groups.",
  "text_length": 36867
}