{
  "id": "http://arxiv.org/abs/2506.01001v1",
  "title": "FedQuad: Adaptive Layer-wise LoRA Deployment and Activation Quantization\n  for Federated Fine-Tuning",
  "summary": "Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning\nlarge language models (LLMs) in privacy-sensitive scenarios. However, practical\ndeployment remains challenging due to the limited resources on end devices.\nExisting methods typically utilize parameter-efficient fine-tuning (PEFT)\ntechniques, such as Low-Rank Adaptation (LoRA), to substantially reduce\ncommunication overhead. Nevertheless, significant memory usage for activation\nstorage and computational demands from full backpropagation remain major\nbarriers to efficient deployment on resource-constrained end devices. Moreover,\nsubstantial resource heterogeneity across devices results in severe\nsynchronization bottlenecks, diminishing the overall fine-tuning efficiency. To\naddress these issues, we propose FedQuad, a novel LoRA-based FedFT framework\nthat adaptively adjusts the LoRA depth (the number of consecutive tunable LoRA\nlayers from the output) according to device computational capabilities, while\nemploying activation quantization to reduce memory overhead, thereby enabling\nefficient deployment on resource-constrained devices. Specifically, FedQuad\nfirst identifies the feasible and efficient combinations of LoRA depth and the\nnumber of activation quantization layers based on device-specific resource\nconstraints. Subsequently, FedQuad employs a greedy strategy to select the\noptimal configurations for each device, effectively accommodating system\nheterogeneity. Extensive experiments demonstrate that FedQuad achieves a\n1.4-5.3x convergence acceleration compared to state-of-the-art baselines when\nreaching target accuracy, highlighting its efficiency and deployability in\nresource-constrained and heterogeneous end-device environments.",
  "authors": [
    "Rukuo Li",
    "Jianchun Liu",
    "Hongli Xu",
    "Liusheng Huang"
  ],
  "published": "2025-06-01T13:13:20Z",
  "updated": "2025-06-01T13:13:20Z",
  "categories": [
    "cs.DC"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01001v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01001v1  [cs.DC]  1 Jun 2025IEEE TRANSACTIONS ON XXX 1\nFedQuad: Adaptive Layer-wise LoRA Deployment and\nActivation Quantization for Federated Fine-Tuning\nRukuo Li, Jianchun Liu, Hongli Xu, Liusheng Huang\nAbstract —Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning large language models (LLMs) in\nprivacy-sensitive scenarios. However, practical deployment remains challenging due to the limited resources on end devices. Existing\nmethods typically utilize parameter-efficient fine-tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), to substantially\nreduce communication overhead. Nevertheless, significant memory usage for activation storage and computational demands from full\nbackpropagation remain major barriers to efficient deployment on resource-constrained end devices. Moreover, substantial resource\nheterogeneity across devices results in severe synchronization bottlenecks, diminishing the overall fine-tuning efficiency. To address\nthese issues, we propose FedQuad, a novel LoRA-based FedFT framework that adaptively adjusts the LoRA depth (the number of\nconsecutive tunable LoRA layers from the output) according to device computational capabilities, while employing activation\nquantization to reduce memory overhead, thereby enabling efficient deployment on resource-constrained devices. Specifically,\nFedQuad first identifies the feasible and efficient combinations of LoRA depth and the number of activation quantization layers based\non device-specific resource constraints. Subsequently, FedQuad employs a greedy strategy to select the optimal configurations for\neach device, effectively accommodating system heterogeneity. Extensive experiments demonstrate that FedQuad achieves a 1.4–5.3 ×\nconvergence acceleration compared to state-of-the-art baselines when reaching target accuracy, highlighting its efficiency and\ndeployability in resource-constrained and heterogeneous end-device environments.\nIndex Terms —Federated Fine-Tuning, Resource Constraint, System Heterogeneity, Low-Rank Adaptation, Activation Quantization\n✦\n1 I NTRODUCTION\nRecent advances in large language models (LLMs), such as\nGPT-4 [1] and DeepSeek-V3 [2], have demonstrated remark-\nable modeling capabilities across various natural language\nprocessing (NLP) tasks. Serving as foundational models,\nLLMs naturally stimulate interest in fine-tuning for down-\nstream domain-specific tasks, including question answering\n[3], sentiment analysis [4], and machine translation [5].\nHowever, centralized fine-tuning methods rely on aggre-\ngating substantial amounts of domain-specific data from\nend devices, posing privacy risks [6] and facing constraints\nimposed by regulations such as GDPR [7]. To address these\nconcerns, federated fine-tuning (FedFT), a distributed fine-\ntuning approach, has emerged [8], [9]. FedFT enables end\ndevices to perform local model fine-tuning without sharing\nraw data, subsequently aggregating model parameters from\nindividual devices at a parameter server (PS) to facilitate\nknowledge sharing across devices.\nNevertheless, the practical deployment of FedFT faces\nsubstantial challenges due to the mismatch between the\nenormous scale of LLMs and the limited resources ( i.e., com-\nputational power, communication bandwidth, and memory)\nof end devices. For instance, standard NLP LLMs such as\nRoBERTa-large [10] demand approximately 4 GB of net-\nwork traffic per training round and over 34,00 TFLOPs of\ncomputation. In contrast, typical end devices, such as the\nNVIDIA Jetson TX2 [11], provide less than 2 TFLOPS of\n•R. Li, J. Liu, H. Xu, L. Huang are with the School of Computer Science and\nTechnology, University of Science and Technology of China, Hefei, Anhui,\nChina, 230027, and also with Suzhou Institute for Advanced Research,\nUniversity of Science and Technology of China, Suzhou, Jiangsu, China,\n215123. E-mails: xxxcomputational power. In addition, the available bandwidth\nfor end devices is typically below 100 Mbps, which is char-\nacteristic of typical WAN environments [12]. As a result, the\nconvergence process in real-world scenarios may extend to\nhundreds of hours [13]. Furthermore, fine-tuning RoBERTa-\nlarge requires around 25 GB of memory, exceeding the\ncapabilities of most end devices, which typically have less\nthan 16 GB of memory [14]. Recent studies on FedFT have\nprimarily mitigated communication overhead by leverag-\ning various parameter-efficient fine-tuning (PEFT) methods\n[9], [13], such as Adapter [15] and Low-Rank Adaptation\n(LoRA) [16]. Specifically, LoRA reduces the number of train-\nable parameters by freezing the base model and updating\nonly trainable low-rank matrices inserted into transformer\nlayers [16]. As these LoRA modules typically comprise less\nthan 1% of the total parameters of the base LLM, LoRA\nsubstantially reduces communication overhead in FedFT.\nDespite the substantial reduction in communication\noverhead enabled by LoRA, FedFT still faces two major chal-\nlenges, i.e., resource constraints and system heterogeneity.\nFirst , resource ( i.e., memory, computing power) constraints\nremain a critical bottleneck for deploying LLMs on end\ndevices. We notice that LoRA does not fundamentally elim-\ninate memory overhead, as fine-tuning LLMs still demands\nsubstantial memory to store intermediate activations. For\nexample, fine-tuning RoBERTa-large [17] with LoRA still\ndemands about 17.4 GB of memory, of which 11.5 GB is\ndevoted to activation storage. Modern end devices typically\nhave only 4–16 GB of RAM [14], exacerbating memory\nconstraints. Additionally, due to the heavy computational\ndemand of fine-tuning, resource-limited devices often ex-\nhibit extremely slow training speeds.\nSecond , participating devices typically possess heteroge-\n--- Page 2 ---\nIEEE TRANSACTIONS ON XXX 2\nneous system characteristics. In particular, they show sig-\nnificant disparities in computational capabilities ( e.g., CPU\nfrequency), with performance gaps often exceeding tenfold,\ncausing synchronization delays as strong devices wait for\nweak ones during each training round [18]. For instance,\nthe FLOPS capability of an iPhone 16 is only about 12%\nof that of a desktop GPU RTX 3090. Such imbalances often\ncause devices with weak capabilities to become stragglers,\nprolonging overall training time and impairing fine-tuning\nefficiency. Furthermore, compared to strong devices, weaker\ndevices may be unable to load the full model and are\nthus either constrained to smaller sub-models or excluded\nfrom the training process altogether, leading to diminished\noverall performance.\nExisting methods for addressing these challenges are\ntypically classified into two major categories. The first cat-\negory [19] [20] fine-tunes LoRA parameters on a subset\nof transformer layers and discards the rest. For example,\nSuet al. [19] propose FedRA, which constructs sub-models\nby randomly selecting transformer layers to accommodate\nresource constraints. Liu et al. [20] introduce InclusiveFL,\nassigning consecutive layers from the input based on device\ncapabilities and employing momentum distillation to im-\nprove shallow model performance. However, these methods\ndirectly discard certain transformer layers, compromising\nthe original model architecture, thereby limiting feature\nextraction capabilities and degrading accuracy and conver-\ngence speed [21]. Besides, memory constraints may prevent\nstronger devices from fully utilizing their computational\nresources, forcing them to wait for weaker ones and causing\nconsiderable synchronization delays, which degrade overall\nfine-tuning efficiency. The second category [22] [23] retains\nall transformer layers while fine-tuning only a subset of\nLoRA parameters within them, such as by freezing some\nLoRA layers or reducing the LoRA rank. For instance, Sun\net al. [22] propose selectively fine-tuning critical layers while\nfreezing others to save resources, thereby avoiding accuracy\ndegradation from discarded layers. However, the uneven\ndistribution of resource consumption across transformer\nlayers limits the practical deployability of this approach\nand leads to longer waiting times. To enhance fine-tuning\nefficiency, Cho et al. [23] introduce HetLoRA, assigning dif-\nferent LoRA ranks based on device capabilities to alleviate\nsystem heterogeneity. Nonetheless, merely adjusting LoRA\nrank does not fundamentally resolve LoRA’s high com-\nputational overhead, and memory savings remain limited,\nhindering practical deployment.\nIn this work, we propose FedQuad, a novel federated\nfine-tuning framework that adaptively determines the con-\nfiguration of LoRA depth ( i.e., the number of consecutive\nunfrozen LoRA layers from the output) and the number\nof activation quantization layers to address challenges aris-\ning from resource constraints and system heterogeneity.\nAs discussed in Section 2, increasing LoRA depth gener-\nally improves fine-tuning performance but also increases\nresource consumption, making deployment on memory-\nlimited devices impractical. Notably, freezing layers beyond\nthe first fine-tuned LoRA layer does little to reduce memory\nusage. While activation checkpointing reduces memory cost,\nit adds substantial computational overhead. To this end,\nFedQuad proposes to reduce memory usage via activa-tion quantization, thereby enabling deeper LoRA configu-\nrations and improving fine-tuning performance. Specifically,\nFedQuad first efficiently identifies all feasible configurations\nof LoRA depth and quantization layers that satisfy device-\nspecific memory constraints. FedQuad then selects the op-\ntimal configuration by jointly considering device computa-\ntional capability, thereby striking a balance between training\nefficiency and model accuracy. Crucially, FedQuad main-\ntains the full transformer architecture, ensuring both high\nmodel fidelity and practical deployability in heterogeneous\nfederated environments.\nEven within memory-feasible configurations, the inter-\nplay between LoRA depth and activation quantization has\na significant impact on overall fine-tuning efficiency. In-\ncreasing the number of quantized layers allows for deeper\nLoRA integration, thereby enhancing model accuracy. It also\nintroduces additional computational overhead, potentially\nleading to prolonged synchronization delays across devices.\nConversely, reducing the number of quantized layers low-\ners computational latency but restricts the allowable LoRA\ndepth, ultimately limiting the model’s representational ca-\npacity and fine-tuning performance. Therefore, under re-\nsource constraints, identifying an optimal trade-off between\nLoRA depth and the number of quantized layers remains\na non-trivial yet essential task for balancing fine-tuning\naccuracy and efficiency.\nOur main contributions are summarized as follows:\n•We propose FedQuad, an efficient LoRA-based\nFedFT framework that effectively addresses resource\nconstraints and system heterogeneity by jointly de-\ntermining the LoRA depth and the number of activa-\ntion quantization layers.\n•We conduct a comprehensive analysis of the joint\nimpact of LoRA depth and activation quantization on\nfine-tuning performance. Based on this insight, we\ndesign a greedy-based algorithm that dynamically\nselects optimal configurations according to device\ncapabilities.\n•Extensive experimental evaluations demonstrate that\nour proposed algorithm achieves 1.4–5.3×accelera-\ntion when achieving the target accuracy, compared\nto existing solutions.\n2 B ACKGROUND AND MOTIVATION\n2.1 Federated Fine-Tuning LLMs with LoRA\nLoRA for LLMs. With the rapid increase in the number\nof parameters in LLMs, traditional training methods, such\nas fine-tuning all model parameters, achieve excellent per-\nformance but are extremely resource-intensive and costly.\nConsequently, PEFT methods have emerged as essential\napproaches for the efficient deployment of LLMs. Among\nvarious PEFT methods, LoRA has gained widespread pop-\nularity as an effective and efficient strategy. LoRA reduces\nthe number of trainable parameters by introducing low-rank\nadapters, enabling scalable and cost-effective fine-tuning of\nLLMs. Specifically, instead of updating the full parameter\nmatrix of the pre-trained model, LoRA inserts two low-rank\nmatrices, denoted as AandB, to approximate the weight\nupdate. Let W0denote the frozen weight matrix of the pre-\ntrained model. The LoRA update can be formulated as:\n--- Page 3 ---\nIEEE TRANSACTIONS ON XXX 3\nW0+ ∆W=W0+BA (1)\nwhere W0∈Rd×k,B ∈ Rd×randA ∈ Rr×k, with\nr≪min(d, k). During training, inputs are simultaneously\npassed through both the frozen pre-trained model and the\nLoRA adapters. The outputs are then combined to form the\nfinal representation. For an input feature xiin the i-th layer,\nthe output hiis computed as:\nhi=Wixi+ ∆Wixi=Wixi+BiAixi (2)\nOnly the parameters AiandBiare updated via gradient\ndescent by minimizing the loss between the model output\nand the ground-truth labels. Therefore, LoRA significantly\nreduces the number of trainable parameters (typically less\nthan 1% [9]) while effectively preserving the generalization\ncapability of the pre-trained model.\nFederated Learning with LoRA. Federated learning (FL)\nis a distributed training paradigm that enables multiple end\ndevices to collaboratively train a shared global model with-\nout sharing their local data. The global training objective is\nto find the optimal model parameters ω∗={∆ω∗,ω0}that\nminimize the overall loss:\nω∗= argmin\nω={∆ω,ω0}f(ω) =1\nnnX\ni=11\n|Di|X\nξi∈DiFi(ω;ξi) (3)\nwhere Fi(ωi;ξi)is the local loss function computed on\ndevice iwith data sample ξifrom its local dataset Di, and n\nis the total number of participating end devices.\nIn each local training round h, end device iup-\ndates its local parameters using stochastic gradient descent\n(SGD) [24]:\n∆ˆωh\ni= ∆ωh\ni−η· ∇fi(∆ωh\ni) (4)\nwhere ηis the learning rate, and ∇fi(·)denotes the gradi-\nent of the local loss function with respect to the trainable\nparameters. After local training, each end device sends the\nupdated gradients ∆ωh\nito the central parameter server (PS),\nwhich performs model aggregation:\n∆ωh+1=1\nnnX\ni=1∆ωh\ni (5)\nThe server then distributes the aggregated updates to all\nend devices for the next training round.\nIn conventional FL, the entire model’s parameters must\nbe synchronized between end devices and the server,\nwhich incurs high communication overhead, particularly for\nLLMs. When incorporating LoRA into FL, the pre-trained\nbackbone ω0remains frozen throughout training, and only\nthe low-rank adapter parameters ∆ωare updated and\nexchanged. This design substantially reduces the volume\nof transmitted data on resource-constrained devices, thus\naccelerating convergence and improving the feasibility of\nfederated training with LLMs.\n2.2 Resource Bottlenecks of Fine-tuning with LoRA\nOne of the key advantages of LoRA is its ability to sig-\nnificantly reduce the number of parameters that need to\nbe updated during training. By introducing trainable low-\nrank matrices on top of the original weight matrices, LoRA\nconstrains parameter updates to low-rank matrices, thereby\ngreatly reducing communication overhead. For instance,\nFig. 1: GPU Memory Footprint for LoRA Fine-Tuning.\nwhen fine-tuning RoBERTa-large, LoRA reduces the per-\nround communication cost per device from 1355 MB to just 6\nMB, accounting for less than 0.5% of that in full fine-tuning.\nNevertheless, LoRA does not fully address the memory\nand computation bottleneck during training. First, LoRA\nstill demands substantial memory resources. Although re-\nducing the number of trainable parameters alleviates mem-\nory usage for optimizer states and gradient storage, acti-\nvations still need to be retained for gradient computation\nduring the backward pass, which occupies a substantial\nportion of GPU memory. In particular, existing LoRA-based\nfederated fine-tuning frameworks, such as FedLoRA [25]\nand HetLoRA [23], typically update LoRA modules across\nall transformer layers. Consequently, activations from every\nlayer must be preserved throughout training, exacerbating\nmemory pressure and limiting the scalability of federated\nfine-tuning on memory-constrained devices. To further il-\nlustrate the severity of memory consumption caused by\nactivations, we conduct a series of fine-tuning experiments\non RoBERTa-large [17] using the MNLI dataset [26]. The\nbatch size is set to 16 and 32, respectively, with a maxi-\nmum sequence length of 256, and GPU memory usage is\ncontinuously monitored throughout training. As shown in\nFig.1, activations account for the majority of memory con-\nsumption in LoRA-based fine-tuning. With a batch size of\n16, activations occupy 85.1% of total memory usage. When\nthe batch size increases to 32, the total memory consump-\ntion reaches 17.4GB, with activation memory usage rising\nto 15.9 GB, accounting for 91.5% of the total. Therefore,\nactivation storage is the primary contributor to the memory\nbottleneck in LoRA-based fine-tuning. Therefore, activation\nstorage is the main cause of the memory bottleneck in\nLoRA-based fine-tuning.\nFurthermore, computational power remains a critical\nbottleneck for LoRA. For instance, fine-tuning RoBERTa-\nlarge using LoRA on an A6000 GPU reduces per-batch\nlatency from 1002 ms to approximately 699 ms, which\namounts to only a 30.2 % reduction in computational time\ncompared to full fine-tuning. Although LoRA dramatically\nreduces the number of trainable parameters and thus short-\nens gradient computation and optimizer update time, the\nforward and backward passes through the frozen back-\nbone still remain computationally intensive. Consequently,\nresource-constrained devices within FedFT systems experi-\nence slow training speeds, resulting in significant synchro-\nnization delays and reduced overall fine-tuning efficiency. In\nsummary, while LoRA substantially reduces communication\ncosts, both memory and computational limitations continue\nto pose significant challenges for LoRA-based fine-tuning of\n--- Page 4 ---\nIEEE TRANSACTIONS ON XXX 4\n(a) Resource overhead\n (b) Accuracy\nFig. 2: The impact of LoRA position on fine-tuning.\n(a) Accuracy & Memory usage\n (b) Computational latency\nFig. 3: The impact of LoRA depths on fine-tuning.\nLLMs on end devices with limited resources.\n2.3 Effect of LoRA Depth on Overall Performance\nThe memory and computational overhead of LoRA is\nlargely determined by the positions of the unfrozen layers.\nSpecifically, updating the parameters of a given layer during\ntraining requires storing the activations of that layer and\nall subsequent layers. Moreover, to compute the gradient\nfor that layer, the backward pass must propagate gradients\nfrom the output layer back to it. Our observations further\nreveal that both the position and number of unfrozen LoRA\nlayers have a significant impact on fine-tuning performance,\ni.e., resource consumption and model accuracy. In particular,\nconsecutively fine-tuning LoRA layers starting from the out-\nput demonstrates superior efficiency in both resource usage\nand accuracy improvement. With this approach, although\nincreasing the number of trainable layers continues to im-\nprove performance, the marginal gains gradually diminish,\nwhile resource consumption grows nearly linearly. Based on\nthese observations, we define the number of consecutively\nunfrozen LoRA layers starting from the output as the LoRA\ndepth, which plays a critical role in balancing model accu-\nracy and resource usage. To assess its impact, we conduct a\nseries of experiments.\nFirst, to verify the impact of the position of unfrozen\nLoRA layers on fine-tuning performance, we conduct a\nfine-tuning experiment using a 12-layer RoBERTa-base\nmodel [17] on the MNLI dataset [26] (detailed experimental\nsettings can be found in Section 4.1). In this experiment, we\nselectively fine-tune LoRA layers at different positions while\nfreezing the remaining layers. Specifically, we evaluate four\nconfigurations: all layers fine-tuned (denoted as Layers-A),\nshallow layers {0, 1, 2, 3 }(Layers-S), middle layers {4, 5, 6,\n7}(Layers-M), and deep layers {8, 9, 10, 11 }(Layers-D). The\nexperimental results in Fig. 2 lead to the following two key\ninsights:\n(1) Consecutive fine-tuning from the output layer is more\nresource-efficient. As shown in Fig. 2(a), despite fine-tuning\nonly one third of the layers, Layers-S consumes 93% of the\nmemory used by Layers-A. In comparison, Layers-M andLayers-D consume 69.9% and 46.5% of the memory used\nby Layers-A, respectively. In LoRA fine-tuning, updating a\ngiven layer requires retaining the activations of all subse-\nquent layers. As a result, although Layers-S fine-tunes only\nthe first four layers, activations from all 12 layers must still\nbe stored, leading to high memory usage. Moreover, Layers-\nS exhibits 1.37×and1.87×higher computation latency\nthan Layers-M and Layers-D, respectively. This is because\nthe trainable layers in Layers-S are located near the input,\nresulting in a longer backward pass and thus higher com-\nputational overhead.\n(2) Shallow-layer fine-tuning provides limited perfor-\nmance gains. As shown in Fig. 2(b), Layers-M and Layers-D\nachieve accuracies of 77.7% and 73.4%, respectively, while\nLayers-S achieves only 64.1%. This substantial gap indicates\nthat tuning shallow layers has limited capacity to adapt to\ndownstream tasks, whereas middle and deeper layers play a\nmore important role in improving fine-tuning performance.\nTherefore, fine-tuning consecutive LoRA layers from the\noutput layer while freezing all other parameters is not\nonly more resource-efficient but also straightforward and\neffective in preserving model accuracy. Then, we further\ninvestigate the relationship between LoRA depth and both\nresource usage and performance. We conduct a series of\nexperiments using RoBERTa-base on MNLI, with LoRA\ndepth ranging from 1 to 12. The experimental results, shown\nin Fig. 3, reveal the following two key findings:\n(1) As LoRA depth increases, resource consumption ex-\nhibits an approximately linear growth trend. Specifically,\neach additional LoRA layer increases the computational la-\ntency by approximately 5 ms and memory usage by around\n199 MB.\n(2) Although fine-tuning performance improves with\nincreasing LoRA depth, the performance gain gradually di-\nminishes, particularly in the shallower layers. From Fig. 3(a),\nwe observe that when LoRA depth increases from 1 to\n8, model accuracy improves by 15.8%, with an average\naccuracy gain of approximately 2.26% per additional LoRA\nlayer. However, when the depth increases from 8 to 12, the\naccuracy gain is only 0.4%.\nIn summary, while performance continues to improve\nwith increasing LoRA depth, the marginal performance\ngain progressively diminishes, while memory usage and\ncomputational latency grow in a near-linear fashion. There-\nfore, selecting an appropriate LoRA depth based on the\ncapabilities of end devices is essential to balance fine-tuning\nperformance and resource consumption.\n2.4 Motivations for Adaptive LoRA Depth and Activa-\ntion Quantization\nPrevious observations indicate that stable fine-tuning per-\nformance during training typically requires a larger LoRA\ndepth. However, increasing the LoRA depth may exceed\nthe memory capacity of end devices, thereby making the\ndeployment of LLMs infeasible in practical scenarios. Dur-\ning training, activation storage is the primary contributor\nto the memory bottleneck. Existing approaches to miti-\ngate activation-related memory overhead generally fall into\ntwo categories, i.e., activation checkpointing and activation\nquantization. Activation checkpointing reduces memory us-\n--- Page 5 ---\nIEEE TRANSACTIONS ON XXX 5\n(a) Accuracy & Memory usage\n (b) Accuracy & Latency\nFig. 4: The impact of activation quantization on fine-tuning.\nage by discarding intermediate activations during the for-\nward pass and recomputing them during the backward pass\nas needed. While this approach significantly lowers mem-\nory consumption, it introduces considerable computational\noverhead. For instance, fine-tuning a RoBERTa-base model\non the MNLI dataset using an A6000 GPU shows that en-\nabling activation checkpointing increases per-batch latency\nby approximately 83%. In contrast, activation quantization\nreduces memory usage by quantizing activations during the\nforward pass and dequantizing them during backpropaga-\ntion for gradient computation. Although this method also\nincurs extra computation, we optimize the quantization and\ndequantization operations using Triton [27] as in Jetfire [28]\n(see Section 4.1 for details), resulting in only a 36% increase\nin per-batch latency. Additionally, we implement layer-wise\nquantization to better balance computational overhead.\nWe define the number of activation quantization layers\nas the number of consecutive transformer layers whose acti-\nvations are quantized during training, starting from the first\nunfrozen LoRA layer. Taking RoBERTa-base as an example,\nas shown in Fig. 4(a), we enable all LoRA layers for fine-\ntuning and apply activation quantization sequentially from\nthe first transformer layer onward. This approach results in\nan average memory reduction of 58% when fine-tuning the\ncorresponding LoRA layers. Moreover, when the number of\nquantized layers reaches 8, the model achieves a 0.8% accu-\nracy improvement compared to the non-quantized baseline.\nThis performance gain is attributed to the stochastic noise\nintroduced during quantization, which helps prevent over-\nfitting and enhances the model’s generalization capability.\nTo investigate the synergy between the number of activa-\ntion quantization layers and LoRA depth under fixed mem-\nory constraints, we systematically vary the configuration,\nrepresented as (LoRA depth, number of quantized layers),\nand evaluate the resulting performance. For example, a\nconfiguration of (8, 5) represents a LoRA depth of 8 and the\nnumber of quantized layers of 5. As shown in Fig. 4(b), real-\nlocating memory saved through activation quantization to\nsupport larger LoRA depth can substantially improve model\naccuracy. For instance, the configuration (5, 0) achieves an\naccuracy of 77.55%, while (8, 5) reaches 81.91%, yielding an\nabsolute improvement of 4.36%. However, as both LoRA\ndepth and the number of quantized layers continue to\nincrease, the marginal accuracy gain gradually diminishes.\nFor example, increasing from (8, 5) to (10, 9) improves\naccuracy by only 0.32%, indicating a clear saturation effect.\nMeanwhile, computational latency consistently increases.\nWhen moving from (8, 5) to (10, 9), the delay rises from\n43.7ms to 74.4ms, representing a 70.3% increase.\nWhile increasing the number of quantized layers caneffectively support larger LoRA depth and enhance fine-\ntuning performance, the resulting increase in latency may\ncause slower devices to become stragglers, thereby pro-\nlonging synchronization time during training. Therefore, to\naccelerate overall convergence, it is necessary to explore the\noptimal configuration of the number of activation quanti-\nzation layers and LoRA depth according to the resource\nconstraints of heterogeneous devices.\n3 S YSTEM DESIGN\n3.1 System Overview\nInspired by the above observations, we propose FedQuad, a\nnovel and efficient federated fine-tuning framework tailored\nfor end devices with constrained resource and heteroge-\nneous system characteristics. Specifically, FedQuad deter-\nmines the optimal fine-tuning configurations for each de-\nvice based on their status information. Upon completion of\nlocal fine-tuning, FedQuad performs adaptive aggregation\nof the updated parameters and subsequently decides the\noptimal configurations for the next training round. The\noverall workflow of FedQuad comprises six main steps (as\nillustrated in Fig. 5):\nStatus Collection. At the beginning of each training\nround, the parameter server (PS) collects status information\nfrom each device ( 1⃝), including available memory and com-\nputational capabilities, to inform the configuration updates.\nConfiguration Update. The PS updates appropriate con-\nfigurations, including LoRA depth and the number of ac-\ntivation quantization layers to each device based on their\ncurrent memory availability and computational capabilities\n(2⃝). This ensures efficient resource utilization while si-\nmultaneously avoiding out-of-memory errors and degraded\nfine-tuning efficiency.\nLoRA Distribution. The PS disseminates the aggregated\nLoRA weights along with the updated configurations to\neach device ( 3⃝).\nLocal Model Adjustment. Upon receiving its corre-\nsponding configuration, each device reconfigures its local\nmodel by adjusting unfrozen LoRA layers and quantized\nactivation layers. After adjusting the settings of individual\nlayers within the model, each device updates its local LoRA\nparameters to align with the received global parameters\nbefore starting the local fine-tuning phase ( 4⃝).\nLocal Fine-T uning. End devices perform local fine-\ntuning while monitoring runtime performance such as\nmemory utilization and computational latency. After com-\npleting the local training, devices upload the updated LoRA\nparameters ( 5⃝) and recorded performance status ( 6⃝) to the\nPS .\nLoRA Aggregation. The PS performs adaptive weighted\naggregation of the received LoRA parameters, considering\nthe varying LoRA depths used by individual devices, to\nobtain the updated global model ( 7⃝).\nThe overall procedure iterates over multiple training\nrounds until the model satisfies a predefined convergence\ncondition or achieves the target accuracy.\n3.2 Status Collection\nFedQuad effectively addresses resource constraints and sys-\ntem heterogeneity by allocating optimal training configura-\n--- Page 6 ---\nIEEE TRANSACTIONS ON XXX 6\nFig. 5: Overview of FedQuad’s workflow.\ntions to each device. Specifically, in the h-th training round,\nFedQuad determines a tailored configuration (dh\ni, ah\ni)for\neach device iaccording to its resource constrains, where dh\ni\nrepresents the assigned LoRA depth and ah\nirepresents the\nnumber of activation quantization layers.\nBefore determining these configurations, each device up-\nloads its current status, including available memory, compu-\ntational capacity. The available memory of device iin round\nhis denoted by Mh\ni, representing the portion of GPU or\nsystem memory that can be safely used for training without\ninterfering with other local applications.\nIn round h, the computational capability qh\nidenotes the\nfloating-point processing throughput of device i. Accord-\ningly, the local training time µh\niincurred by end device i\nduring round his calculated as:\nuh\ni=C(dh\ni, ah\ni)\nqh\ni(6)\nwhere C(·)denotes the computational complexity of the\ntask, modeled as a linear function of LoRA depth dand\nthe number of quantized layers a.\n3.3 Configuration Update\nBased on each device status reports, FedQuad constructs\na unified model that incorporates accuracy, memory, and\nlatency to guide configuration decisions. As discussed in\nSection 2, fine-tuning performance is primarily determined\nby the LoRA depth d. LetDhdenote the set of LoRA depths\nassigned to all devices in the h-th training round, and let D\nbe the union of all such depths across Htraining rounds,\nwhere Hdenotes the total number of training rounds:\nDh={dh\ni|i∈[1, n]} (7)\nD=H[\nh=1Dh=H[\nh=1n\ndh\ni|i∈[1, n]o\n(8)\nLet∆wH(D)represent the aggregated global LoRA\nparameters after Htraining rounds under the set D. The\nloss function of the global model after training can be ex-\npressed as F(w0,∆wH(D)). The convergence requirement\nafter training can be formulated as the following constraint:\nF(w0,∆wH(D))−F(w0,∆w∗)≤ϵ (9)\nwhere ϵ > 0denotes the convergence threshold, and\nF(w0,∆w∗)represents the optimal fine-tuning perfor-mance. Ideally, ϵshould approach zero, indicating that\nthe trained model closely approximates the optimal solu-\ntion [29].\nOn resource-constrained devices, the number of LoRA\nlayers that can be fine-tuned is typically limited. However,\nthe memory saved through activation quantization can be\nreallocated to support a larger LoRA depth, enabling the\nfine-tuning of more layers and thereby improving fine-\ntuning accuracy. Therefore, let modenote the additional\nmemory overhead incurred by increasing the LoRA depth\nby one layer, mqdenote the memory savings achieved by\nquantizing the activations of a single transformer layer, and\nmfdenote the fixed memory overhead during training ( e.g.,\nmodel parameters). Then, the memory constraint can be\nformulated as:\nmf+mo·dh\ni−mq·ah\ni≤Mh\ni,∀i∈[1, n],∀h∈[1, H](10)\nThe completion time th\niconsumed by device iin round h\nincludes both local training time and communication time.\nNevertheless, given the significantly small size of LoRA\nparameters, the communication time is omitted in subse-\nquent analysis [9], [25]. Therefore, the completion time can\nbe approximated as:\nth\ni=uh\ni (11)\nStrong devices have to wait for weak devices to complete\ntraining in each round. Let thdenote the longest completion\ntime across all participating devices in round h. We estimate\nthe average waiting time across all devices in round has:\nWh=1\nnnX\ni=1(th−th\ni) (12)\nA larger Whindicates greater synchronization delay, which\ncan slow down overall convergence. To mitigate this issue,\nwe impose a constraint on the average waiting time in each\nround. Specifically, we require that:\nWh=1\nnnX\ni=1(th−th\ni)≤θ,∀i∈[1, n],∀h∈[1, H](13)\nwhere θis a predefined threshold that limits the allowable\naverage waiting time.\nAs both LoRA layer freezing and activation quantization\nare applied on a per-layer basis, the LoRA depth dand the\nnumber of quantized layers amust be non-negative inte-\ngers. Specifically, dhas to satisfy d∈[1, L], where Ldenotes\nthe total number of transformer layers, and ashould satisfy\na∈[0, d−1], since we notice that quantizing the final output\nlayer tends to degrade fine-tuning accuracy while incurring\nadditional memory overhead. Thus, the constraints can be\nformulated as:\nd, a∈Z≥0, d∈[1, L], a∈[0, d−1] (14)\nwhere Z≥0denotes the set of non-negative integers.\nGiven a FedFT task, FedQuad aims to assign an ap-\npropriate configuration (dh\ni, ah\ni)to each device iin each\nround h, based on its available resources, to achieve the\ntarget accuracy while minimizing the total fine-tuning time\n--- Page 7 ---\nIEEE TRANSACTIONS ON XXX 7\nAlgorithm 1: Adaptive Configuration Selection\n(ACS) in FedQuad\nInput: Device memory constraint Mh\niat round h;\nCompute capacity uh−1\ni at round h;\nAverage completion time th−1\navg in round h−1;\nOutput: Optimal configuration (dh\ni, ah\ni)for each\ndevice iat round h\n1// Step 1: Identify feasible and\nefficient configurations (d, a)under\nmemory constraint\n2foreach device i∈[1, n]do\n3Ci← ∅;\n4 acur←0;\n5 ford∈[1, dmax]do\n6 fora∈[acur, d−1]do\n7 ifthe memory constraint in Eq. (10) holds\nthen\n8 Add (d, a)toCi;\n9 acur←a;\n10 break ; // Once a valid pair\n(d, a)is found, break out of\nthe inner loop\n11// Step 2: Estimate completion time for\neach feasible configuration\n12foreach device i∈[1, n]do\n13 Estimate th\nifor each (d, a)∈ Ciusing µh−1\ni, bh−1\ni\n(see Eq. (11));\n14// Step 3: Compute performance gain\nbased on gradient norms\n15foreach LoRA depth d∈[1, L]do\n16 Compute G(d) =PL−1\nl=L−dgl;\n17// Step 4: Select the configuration\nthat maximizes reward\n18foreach device i∈[1, n]do\n19 (dh\ni, ah\ni)←arg max\n(d,a)∈CiR(d, a);\n20return {(dh\ni, ah\ni)|i∈[1, n]};\nPH\nh=1th. Thus, we can formulate the problem as:\nminPH\nh=1th\ns.t. (9) ,(10),(13),(14)(15)\nTo solve the problem in Eq. (15), we adopt a greedy\nstrategy ACS, as shown in Algorithm1, to determine the\noptimal configuration for each device at every round. ACS\nfirst identifies the feasible and efficient configurations (d, a)\nfor each device based on its memory constraint (Lines 1– 10).\nSpecifically, the PS iterates through all possible LoRA depths\ndfor each device iaccording to its available memory Mh\ni,\nand adjusts the number of activation quantization layers a\naccordingly. For each candidate depth d, it identifies the\nminimal number of quantized layers athat satisfies the\nmemory constraint in Eq. (10) while avoiding additional\ncomputational overhead. The corresponding configuration\n(d, a)is then added to the feasible set Ch\ni.\nNext, FedQuad evaluates the training cost and perfor-mance gain of each candidate configuration (Lines 11– 16).\nBased on the current device’s computational capabilities,\nFedQuad estimates the completion time th\nifor every feasible\nconfiguration (d, a). Additionally, FedQuad records the av-\nerage completion time th−1\navg from the previous round as an\nestimate for the current round’s average completion time.\nFedQuad aims to minimize the difference between each\ndevice’s completion time and th−1\navg, enabling strong devices\nto fully utilize their computational resources while ensur-\ning weak devices do not cause excessive synchronization\ndelays.\nSince gradient information inherently captures the\nmarginal contribution of each layer to the loss function [22],\n[30], FedQuad quantifies the performance gain G(d)associ-\nated with a LoRA depth of dbased on layer-wise gradient\nnorms. Specifically, FedQuad computes the gradient norm\nglfor each LoRA layer l∈[0, L−1]in the global model\nand aggregates the norms of the top dlayers. Formally, the\nperformance gain is defined as:\nG(d) =L−1X\nl=L−dgl (16)\nFinally, to balance performance gain and training cost,\nFedQuad defines a reward function to evaluate the trade-off\nof each configuration (d, a)for each device i:\nR(d, a) =G(d)\nth\ni(d, a)−th−1\navg+c(17)\nwhere the numerator G(d)represents the contribution of\neach configuration to model convergence. The denominator\ncaptures the gap between the completion time under the\ncurrent configuration (d, a)and the average completion\ntime. A smaller gap indicates that the selected configuration\nbetter matches the device’s computational capability, result-\ning in a higher reward. Therefore, a higher R(d, a)reflects\na more favorable trade-off between accuracy and efficiency.\nThe pair (d, a)achieving the highest R(d, a)is chosen as the\noptimal configuration for device i(Lines 17– 19).\n3.4 Local Model Adjustment\nIn round h, device ireceives the LoRA parameters ∆ωhand\nthe training configuration (dh\ni, ah\ni)from PS. Based on this\nconfiguration, the device first adjusts the trainable LoRA\nlayers, represented by Lt, as:\nLt={l|l∈[L−dh\ni, L−1]},∀i∈[1, n],∀h∈[1, H].\nThe LoRA layers within Ltare set to be trainable, while\nall other layers remain frozen. Subsequently, the device\nconfigures the activation quantization transformer layers,\ndenoted as Lq, for the current round as follows:\nLq={l|l∈[L−dh\ni, L−dh\ni+ah\ni−1]},∀i∈[1, n],∀h∈[1, H].\nOnce the fine-tuning configuration adjustments are com-\nplete, the device incorporates updates received from the PS\nto align its local model with the global model. Specifically,\nfor a LLM with pre-trained parameters ω0={ω0\nl|l∈\n[0, L−1]}, the device iupdates its local model as:\nωh\ni={∆ωh, ω0}.\n--- Page 8 ---\nIEEE TRANSACTIONS ON XXX 8\nFig. 6: An illustrative example of FedQuad.\nAfter local model adjustment, device itrains the model on\nthe local dataset Di.\n3.5 LoRA Aggregation\nUpon receipt of the updated LoRA parameters from all\ndevices, the PS performs global aggregation. Because de-\nvices may fine-tune differing numbers of LoRA layers, not\nevery layer update is present on every device. To address\nthis heterogeneity, the PS employs an adaptive, layer-wise\naggregation protocol. Specifically, the aggregation process\nfor each layer only considers the updates from the devices\nthat have valid updates for that particular layer, thereby\nenhancing the performance of the global model for that\nlayer.\nThe PS aggregates the available updates layer by layer.\nLet the global LoRA update for layer lbe denoted as ∆ωh+1\nl,\nwhich is obtained by aggregating the updates from nl\ndevices. The adaptive layer-wise aggregation rule is defined\nas:\n∆ωh+1\nl=1\nnlnlX\ni=1∆ωh\ni,l (18)\nwhere ∆ωh\ni,ldenotes the update to the LoRA layer lby\ndevice iduring the round h.nlrepresents the number of\ndevices with valid updates for layer l.∆ωh+1\nlrepresents\nthe aggregated global update for layer l.\nAfter aggregating the LoRA parameters, FedQuad initi-\nates a new round of status collection to support subsequent\nconfiguration updates, thereby guiding the fine-tuning pro-\ncess on each device based on its latest resource constrains.\nBy adaptively assigning larger LoRA depths and more\nquantized layers to stronger devices, and smaller configu-\nrations to weaker ones based on their resource limitations,\nFedQuad improves fine-tuning performance and efficiency.\nConsequently, it effectively addresses the dual challenges of\nresource constraints and system heterogeneity.\n3.6 An Illustrative Example of FedQuad\nAs illustrated in Fig. 6, FedQuad determines the optimal\nconfiguration for each device based on its resource con-straints to accelerate overall convergence. For strong de-\nvices, although their greater memory capacity allows for\nlarger LoRA depth during fine-tuning, the prolonged com-\npletion times of weaker devices often result in extended idle\nwaiting for the strong devices. To mitigate this, FedQuad\nincreases the number of activation quantization layers to\nsupport larger LoRA depths, thereby fully utilizing the\ncomputational resources of strong devices and improving\nfine-tuning performance. For example, as shown for Device\n1 in Fig. 6, its available memory allows only limited LoRA\nlayers, resulting in a relatively smaller contribution to model\nconvergence and a much shorter completion time than the\naverage, which indicates underutilized computing capacity.\nFedQuad increases its quantization layers to support a\nlarger LoRA depth. Although this leads to a slight increase\nin training time, Device 1 still completes earlier than av-\nerage, thus causing no synchronization delays and helping\nreduce overall idle time. For weaker devices, constrained\nin both memory and computational power, aggressively\nincreasing quantization layers to enable larger LoRA depth\nmay improve accuracy but risks introducing excessive syn-\nchronization delay. Therefore, FedQuad adjusts both acti-\nvation quantization and LoRA depth, aiming to improve\nfine-tuning performance without overburdening the device.\nThis prevents the device from becoming a stragglers. For\ninstance, FedQuad assigns Device 2 additional quantization\nlayers to enhance fine-tuning performance, but limits the\nnumber to ensure that its completion time remains close to\nthe average, avoiding significant slowdowns to the over-\nall training process. After all end devices complete local\ntraining, they upload their parameter updates and status\ninformation to the PS. Upon receiving these updates, the\nPS performs adaptive aggregation of the LoRA parame-\nters, while simultaneously leveraging the devices’ status\ninformation to determine the optimal configurations for the\nsubsequent round.\n4 P ERFORMANCE EVALUATION\n4.1 Experimental Settings\nSystem Implementation. We implement the FedQuad pro-\ntotype based on the open-source FedPETuning frame-\n--- Page 9 ---\nIEEE TRANSACTIONS ON XXX 9\nwork [9], enhancing its functionality with approximately\n2,500 lines of custom code. To ensure compatibility with\nmodern LLM architectures, FedQuad is seamlessly inte-\ngrated with the widely used Transformers library [31], and\nleverages its modular APIs to support LLM initialization\nand adaptation. To enable activation quantization, we mod-\nify the core LLM modules in the transformers.models pack-\nage. Following the observations reported in SLIMFIT [32],\nwe note that static activations from nonlinear operations\nsuch as GELU, MatMul, Softmax, and LayerNorm cannot be\nentirely eliminated by freezing the associated layers. There-\nfore, during the forward pass, we quantize the activations\nof these functions and subsequently apply dequantization\nduring the backward pass. This approach effectively reduces\nmemory overhead without compromising model accuracy.\nOur quantization process follows the approach proposed in\nJetfire [28], and is implemented using Triton [27], a domain-\nspecific programming language and compiler designed for\ndeveloping efficient deep learning primitives. In our ex-\nperiments, we set the block size to 32, i.e.,B= 32 . We\nadopt PyTorch [33] as the underlying training framework\nto manage the local fine-tuning workflow. GPU accelera-\ntion is supported by CUDA v12.3 and cuDNN v9.1.0. For\ndistributed communication between edge devices and the\ncentral server, we utilize torch.distributed, a native PyTorch\nlibrary for parallel and distributed training.\nModels. We primarily evaluate FedQuad on three popu-\nlar LLMs: BERT-large [34] with 336M parameters, RoBERTa-\nlarge [17] with 355M parameters, and DeBERTaV3-large [35]\nwith 435M parameters. These models have been extensively\nadopted in prior federated fine-tuning studies [8], [9], [13],\n[20]. Each model consists of 24 transformer layers. Pre-\ntrained weights are obtained from the Hugging Face repos-\nitory [36].\nDatasets. We conduct our experiments on four widely\nused NLP datasets from the GLUE benchmark [26] (sum-\nmarized in Table 2): (1) The Multi-Genre Natural Language\nInference (MNLI) dataset is a large-scale dataset containing\nover 400,000 sentence pairs used for training and evaluating\nLLMs on natural language inference (NLI) tasks. In the\nNLI tasks, the goal is to determine whether a premise\nentails, contradicts, or is neutral with respect to a given\nhypothesis. (2) The Quora Question Pairs (QQP) dataset\nconsists of more than 400,000 question pairs collected from\nthe Quora platform. Each pair is labeled to indicate whether\nthe two questions are paraphrases of each other or not.\nQQP is commonly utilized for training and evaluating LLMs\non paraphrase detection tasks. (3) The Question Natural\nLanguage Inference (QNLI) dataset is derived from the\nStanford Question Answering Dataset (SQuAD). It consists\nof over 100,000 sentence–question pairs labeled to indicate\nwhether the sentence contains the correct answer to the\ncorresponding question. (4) The Stanford Sentiment Tree-\nbank Binary Version (SST-2) dataset contains 70,042 movie\nreview sentences annotated with binary sentiment labels.\nEach sentence is labeled as either positive or negative. To\nsimulate non-independent and identically distributed (non-\ni.i.d.) data across devices, we follow prior work [8], [9], [13]\nby partitioning the dataset using a Dirichlet distribution.\nSpecifically, for each device, we sample training data accord-\ning to D∼Dir(α), where αcontrols the degree of non-i.i.d.TABLE 1: Technical specifications of Jetson kits.\nAI Performance GPU Type\nJetson TX2 1.33 TFLOPS 256-core Pascal\nJetson NX 21 TOPS 384-core Volta\nAGX Xavier 32 TOPS 512-core Volta\nCPU Frequency GPU Frequency\nJetson TX2 1.2GHz 0.85Ghz\nJetson NX 1.2GHz 0.8Ghz\nJetson AGX 1.45GHz 0.9Ghz\nTABLE 2: Datasets and numbers of samples used in the\nexperiments.\nDataset Application # Train # Test\nMNLI Textual Entailment 392,702 9,815\nQQP Semantic Equivalence 363,846 40,430\nQNLI Question Answering 104,743 5,463\nSST-2 Sentiment Analysis 67,349 1,821\nUnless otherwise specified, we set α= 10 throughout all\nexperiments.\nHardware. Consistent with previous federated learning\nliterature [9], [13], [37], our experiments are conducted in\na semi-simulated manner using an AMAX deep-learning\nworkstation equipped with eight NVIDIA A6000 GPUs. On-\ndevice training times are measured on three representative\nedge devices (summarized in Table 1): (1) Jetson TX2 : A\ncompact embedded computing platform designed specifi-\ncally for AI applications at the edge. (2) Jetson NX : Capable\nof accelerating computation by up to 21 TOPS, offering\nsufficient parallel computing power for running LLMs. (3)\nJetson AGX : The most powerful among the three, delivering\ncomputational capabilities of up to 32 TOPS. Both TX2\nand NX support four computational modes, whereas AGX\nsupports eight modes. Devices operating in different modes\ndemonstrate distinct performance levels.\nSystem Setup. Our experiments involve a total of 100\ndevices, categorized into three types (strong, moderate, and\nweak) based on computational performance and memory\ncapacity. To simulate the resource constraints of heteroge-\nneous end devices, we adopt the following experimental\nsetup:\n1)For Memory. For clarity, we use the tunable LoRA\ndepth in FedLoRA (i.e., federated fine-tuning with vanilla\nLoRA) to represent each device’s available memory capacity.\nFor all methods involved, we convert the LoRA depth\nto the corresponding memory capacity to ensure a fair\ncomparison. Furthermore, we assign dynamic depth ranges\nto different device categories: strong devices are assigned\nLoRA depths in the range of 18–24, moderate devices in the\nrange of 11–17, and weak devices in the range of 4–10. To\nfurther simulate fluctuations in memory availability under\nreal-world conditions, we randomly adjust each device’s\ntunable LoRA depth within its respective range after each\ntraining round.\n2)For Compute. We map the strong, moderate, and weak\ndevice categories to NVIDIA Jetson AGX, Jetson NX, and\nJetson TX2 devices, respectively, representing varying levels\nof computational resources levels. Each Jetson device sup-\nports multiple operating modes corresponding to different\ncomputational capabilities and power configurations. We\nconstruct diverse computational profiles by setting various\n--- Page 10 ---\nIEEE TRANSACTIONS ON XXX 10\noperation modes [38]. To realistically emulate fluctuations\nin computational resources, each device randomly switches\nits operating mode every 10 training rounds.\nBaselines. To evaluate the effectiveness of FedQuad, we\ncompare it against four baseline methods: (1) FedRA [19]\nrandomly selects subsets of transformer layers based on de-\nvice resource constraints to construct sub-models and fine-\ntunes these sub-models using LoRA. (2) InclusiveFL [20]\nallocates sub-models with varying numbers of consecutively\nstacked layers starting from the input layer, rather than\nselecting layers randomly, based on each device’s resource\nconstraints. To mitigate gradient loss in shallow models, it\napplies momentum distillation. (3) LayerSel [22] leverages\nlocal gradient information to select specific layers for fine-\ntuning across devices, while freezing the remaining layers\ninstead of discarding them to conserve resources. Fine-\ntuning is then performed using LoRA. (4) HetLoRA [39]\nallows devices to fine-tune heterogeneous local models by\nallocating different LoRA ranks to tackle system heterogene-\nity.\nMetrics. The following metrics are adopted to evaluate\nthe performance of FedQuad and the baselines.\n1)Test Accuracy reflects the accuracy of the models\ntrained by different approaches on the test datasets, mea-\nsured by the proportion of correctly predicted data. Specif-\nically, we record the test accuracy of the global model (the\nmodel after aggregation at the PS) in each round.\n2)Time-to-Accuracy represents the total wall-clock time\nrequired for training a model to achieve a target accuracy\n. For fair comparisons, we set the target accuracy as the\nhighest achievable accuracy by FedQuad and all baselines.\nWe record the completion time of each round, summing it\nup to obtain the total training time.\n3)Average Waiting Time represents the average time\neach edge device spends waiting for global aggregation\nin each round, indicating the training efficiency of each\nmethod.\nExperimental Parameters. By default, all experiments\nare carried out on our prototype system and run 50\nrounds. Each device fine-tunes 1 epoch per round using\nAdamW [40] optimizer locally. The learning rate is set as\n0.001 and decays according to a cosine scheduler. The batch\nsize is fixed at 32 and the maximum sequence length is set\nto 128 for all experiments.\n4.2 Numerical Results\nOverall Performance. We conducte extensive experiments\nto evaluate the performance of FedQuad against several\nrepresentative baselines. Figs. 7 and 8 illustrate the fine-\ntuning trajectories and overall completion time on various\ngeneral language understanding tasks.\nThe results demonstrate that FedQuad achieves the\nfastest convergence speed across all tasks, significantly out-\nperforming baselines. By adaptively allocating LoRA depth\nand quantized activation layers based on device-specific\nresource constrains, FedQuad enhances fine-tuning perfor-\nmance while reducing local training time. For instance,\nas shown in Figs. 7(a) and 8(a), FedQuad reaches 87.5%\naccuracy on MNLI within only 14,213 s, whereas FedRA, In-\nclusiveFL, LayerSel, and HetLoRA require 75,928s, 45,262s,26,350s, and 20,642s, respectively. Compared to FedRA,\nInclusiveFL, LayerSel, and HetLoRA, FedQuad provides\n5.3×, 3.2×, 1.85×, and 1.45 ×speedup. Similarly, Figs. 7(b)\nand 8(b) show that FedQuad achieves 87% accuracy on QQP\nin 19,738s, while FedRA, InclusiveFL, LayerSel, and Het-\nLoRA take 71,707s, 59,700s, 41,845s, and 32575s, respectively.\nThe corresponding speedups are 3.6 ×, 3.0×, 2.1×, and\n1.65×. Moreover, Figs. 7 and 8 further indicate that FedQuad\nconsistently achieves superior completion times on QNLI\nand SST-2 as well. As shown in Table 3, FedQuad con-\nsistently achieves the fastest convergence across all evalu-\nated models, significantly outperforming baseline methods.\nFor instance, on BERT-large, FedQuad reduces convergence\ntime by 80.2%, 83.0%, 53.5%, and 28.8% compared to FedRA,\nInclusiveFL, LayerSel, and HetLoRA, respectively. Similarly,\non DeBERTa-large, FedQuad attains convergence time re-\nductions of 79.5%, 73.9%, 34.9%, and 22.4%, respectively.\nThese results collectively confirm FedQuad’s advantage in\naccelerating the fine-tuning process.\nThese results demonstrate that FedQuad achieves both\nimproved fine-tuning performance and reduced conver-\ngence time. Unlike FedRA and InclusiveFL, which reduce\nresource consumption by directly skipping transformer lay-\ners, FedQuad employs a layer freezing strategy. This strat-\negy avoids undesirable side effects such as output bias and\naccuracy degradation that often arise from layer dropping\nduring forward propagation. Moreover, in both FedRA and\nInclusiveFL, memory limitations may hinder stronger de-\nvices from fully exploiting their computational capabilities,\nleaving them idle as they wait for slower devices, thereby\nreducing overall fine-tuning efficiency and prolonging con-\nvergence time. Compared to more competitive baselines\nlike LayerSel and HetLoRA, FedQuad also exhibits notable\nadvantages. LayerSel selects layers for fine-tuning based on\ngradient norms, but the uneven distribution of resource\nconsumption during fine-tuning leads to slower conver-\ngence and presents deployment challenges, as memory-\nconstrained devices often cannot afford to compute gradient\nnorms for all layers. HetLoRA, on the other hand, adapts\nLoRA ranks to device capabilities, but fails to fundamentally\naddress the inherent resource overhead of PEFT. Adjusting\nonly the LoRA rank is insufficient to overcome the system\nbottlenecks.\nIn contrast, FedQuad comprehensively models real-\nworld device resource constrains. It employs a back-to-\nfront layer selection strategy for fine-tuning, combined with\nactivation quantization to mitigate memory constraints and\nfully leverage computational resources. This enables adap-\ntive configuration of both LoRA depth and quantization\nlayers for each device based on its resource constrains. As\na result, FedQuad achieves substantially improved fine-\ntuning performance and significantly faster convergence\nspeed.\nEffect of Device Heterogeneity. To comprehensively\nassess FedQuad’s effectiveness under varying degrees of\ndevice heterogeneity, we evaluate FedQuad alongside base-\nline methods across three heterogeneity levels, i.e., Low,\nMedium, and High. Specifically, under the Low heterogene-\nity setting, we configure all devices as high-performance\ndevices. For Medium, we equally distribute devices between\nhigh-performance and moderate-performance categories at\n--- Page 11 ---\nIEEE TRANSACTIONS ON XXX 11\n(a) MNLI\n (b) QQP\n (c) QNLI\n (d) SST-2\nFig. 7: Test accuracy of four approaches on the four datasets.\n(a) Time to reach 87% accuracy\n (b) Time to reach 87% accuracy\n (c) Time to reach 90% accuracy\n (d) Time to reach 95% accuracy)\nFig. 8: Completion time of four approaches when achieving the target accuracy.\n(a) MNLI\n (b) QQP\n (c) QNLI\n (d) SST-2\nFig. 9: Average waiting time of four approaches on the four datasets.\nModel Target Acc FedQuad FedRA InclusiveFL LayerSel HetLoRA\nBERT-large 81% 14.1 71.1 83.1 30.3 19.8\nDeBERTa-large 88% 23.5 114.4 90.0 36.1 30.3\nRoBERTa-large 87% 14.2 75.9 45.2 26.3 20.6\nTABLE 3: Comparison of completion time (in 103s) across\nmodels on the MNLI dataset using different baselines.\nHeter. Level Low Medium High\nTarget Acc 88.1 88.0 87.5\nMetric Time Final Acc Time Final Acc Time Final Acc\nFedQuad 5.2 88.9 8.7 88.7 14.2 88.8\nFedRA 7.8 88.1 21.4 88.0 75.9 87.5\nInclusiveFL 7.2 88.3 15.7 88.2 45.2 87.9\nLayerSel 6.5 88.6 14.6 88.4 26.3 88.4\nHetLoRA 6.1 88.5 11.2 88.5 20.6 88.1\nTABLE 4: Comparison of completion time (in 103s) and final\naccuracy (%) on MNLI under different heterogeneity levels.\na ratio of 1:1. In the High heterogeneity scenario, we assign\nthe device proportions as 3:3:4 for strong-, moderate-, and\nweak-performance devices, respectively. As shown in Table\n4, we observe that as device heterogeneity increases, the\ntime required by all methods to reach the target accuracygrows significantly, which aligns with expectations when\nmore resource-constrained devices are introduced into the\nsystem. However, compared with the baseline methods,\nFedQuad demonstrates remarkable advantages across all\nheterogeneity levels. For example, on the MNLI dataset\nunder medium heterogeneity, FedQuad achieves speedups\nof approximately 2.46×,1.80×,1.68×, and 1.29×over\nFedRA, InclusiveFL, LayerSel, and HetLoRA, respectively.\nMoreover, as heterogeneity rises from low to high, the\nperformance gap between FedQuad and the other base-\nlines further widens: FedQuad attains a 1.5×speedup over\nFedRA in low heterogeneity scenarios, which increases to\n5.35×in high heterogeneity scenarios. These results indicate\nthat FedQuad maintains robust and efficient fine-tuning\ncapabilities across varying degrees of device heterogeneity.\nTo further assess FedQuad’s capability in handling sys-\ntem heterogeneity, we report the average waiting time\nacross four datasets in Fig. 9. FedQuad achieves the lowest\naverage waiting time across all datasets. For example, on\nMNLI as illustrated in Fig. 9(a), FedQuad achieves the\nshortest average waiting time in each round. Furthermore,\nover all rounds, FedQuad reduces the average waiting time\nby 68.9%, 70.9%, 78.2%, and 48.1% compared to FedRA,\nInclusiveFL, LayerSel, and HetLoRA, respectively. On QQP ,\nFedQuad achieves an average waiting time of 442.5s, while\n--- Page 12 ---\nIEEE TRANSACTIONS ON XXX 12\n(a) MNLI\n (b) QQP\nFig. 10: Effect of LoRA depth and activation quantization.\nthe other baselines require 1423.1s, 1520.4s, 2026.1s, and\n852.4s, corresponding to reductions of 68.9%, 70.9%, 78.2%,\nand 51.9%, respectively. This improvement stems from the\nfact that, although LayerSel selects the most important\nlayers for fine-tuning, those layers can incur high compu-\ntational overhead and thus long synchronization delays.\nFedRA and InclusiveFL adapt to heterogeneous devices by\ndiscarding layers but fail to fully leverage the compute\ncapacity of more powerful devices under tight memory\nconstraints, resulting in prolonged waiting times. HetLoRA\nmitigates heterogeneity by assigning devices to different\nLoRA levels but achieves only limited gains. In contrast,\nFedQuad adaptively determines the optimal combination\nof LoRA depth and activation quantization layers for each\ndevice based on its compute and memory resources, thereby\nmaximizing resource utilization and minimizing synchro-\nnization delays for efficient fine-tuning.\n4.3 Ablation Study\nFedQuad incorporates two critical factors, i.e., LoRA depth\nand the number of activation quantization layers, which are\nspecifically designed to enhance the efficiency and adapt-\nability of FedLoRA in heterogeneous environments. In this\nsection, we perform a series of ablation experiments on\nthe MNLI and QQP datasets to evaluate the individual\ncontributions of these two factors.\nWe compare three variants: (i) the full version of our\nmethod (FedQuad), (ii) a version without activation quanti-\nzation (FedQuad w/o QD), and (iii) a version that applies\nthe maximum possible number of activation quantization\nlayers under memory constraints but removes adaptive\nLoRA depth (FedQuad w/o LD). As shown in Fig. 10,\nboth LoRA depth and activation quantization play essential\nroles in FedQuad, though they affect system performance in\ndifferent ways.\nFor instance, as illustrated in Fig. 10, both the FedQuad\nw/o QD and FedQuad w/o LD variants achieve final test\naccuracies comparable to that of FedQuad. However, when\ntargeting an 88% accuracy on MNLI, FedQuad reduces\ntraining completion time by approximately 24.5%compared\nto FedQuad w/o QD and by approximately 74.7%com-\npared to FedQuad w/o LD. Similarly, on QQP , FedQuad\naccelerates the fine-tuning process by factors of 1.29×and\n3.53×relative to FedQuad w/o QD and FedQuad w/o LD,\nrespectively, when reaching the same 89% accuracy target.\nThese observations highlight the distinct roles and com-\nplementary benefits of the two factors. On the one hand,\nFedQuad w/o LD, which retains activation quantization,\nallows devices to fine-tune a larger number of LoRA layers,thereby improving final accuracy but at the cost of increased\nwaiting time and slower convergence. On the other hand,\nFedQuad w/o QD, which removes activation quantization,\nrestricts faster devices from utilizing their full capacity to\nfine-tune additional layers, limiting potential performance\ngains.\nOverall, these results clearly validate the necessity of\nintegrating both adaptive LoRA depth and activation quan-\ntization into FedQuad to achieve efficient and effective fine-\ntuning under heterogeneous system constraints.\n5 R ELATED WORK\nFederated Fine-T uning of LLMs. To fully leverage data\non end devices while preserving user privacy, federated\nfine-tuning (FedFT) has emerged as a promising paradigm\nfor adapting large language models (LLMs) to decen-\ntralized settings. For example, FedNLP [8] provides a\nbenchmark framework for evaluating FedFT across vari-\nous NLP tasks. To address communication overhead on\ndevices, recent studies have turned to parameter-efficient\nfine-tuning (PEFT) techniques. Notably, FedAdapter [9]\nemploys lightweight adapter modules to accelerate model\nconvergence in FL, while FedLoRA integrates low-rank\nadapters [16] to improve training speed and accuracy.\nResource-efficient Approaches. Although PEFT meth-\nods help reduce communication overhead, fine-tuning\nLLMs on end devices still suffers from severe memory and\ncomputational limitations. To adapt to these constraints, ex-\nisting works explore various fine-tuning strategies to tailor\nmodel architectures or training processes to device-specific\nresource capabilities. For instance, FedRA [19] constructs\nsub-models by randomly selecting subsets of transformer\nlayers based on device resource availability and fine-tunes\nthem with LoRA. However, directly discarding layers com-\npromises the model architecture and degrades fine-tuning\naccuracy. Alternatively, LayerSel [22] selects specific layers\nto fine-tune using local gradient information, freezing the\nrest rather than removing them to conserve memory. Yet,\nit overlooks the fact that resource consumption is unevenly\ndistributed across layers, which limits its practical deploy-\nability. HetLoRA [39] tackles system heterogeneity by as-\nsigning different LoRA ranks to each device, enabling them\nto fine-tune heterogeneous local models. However, merely\nadjusting the LoRA rank is insufficient to fundamentally\naddress the resource constraints in FedFT.\n6 C ONCLUSION\nIn this paper, we have proposed FedQuad, a novel and\nefficient federated fine-tuning framework designed to over-\ncome the challenges posed by resource constraints and sys-\ntem heterogeneity. Specifically, FedQuad adaptively adjusts\nthe LoRA depth to match the capabilities of individual\ndevices and employs activation quantization to alleviate\nmemory overhead, thus facilitating efficient deployment of\nlarge language models on resource-limited devices. Addi-\ntionally, we have analyzed the interplay between LoRA\ndepth and the number of activation quantization layers, and\ndevised a greedy-based algorithm to jointly determine opti-\nmal configurations for heterogeneous devices, significantly\n--- Page 13 ---\nIEEE TRANSACTIONS ON XXX 13\nenhancing fine-tuning efficiency. Extensive experimental re-\nsults demonstrate that FedQuad achieves superior conver-\ngence performance and accelerates fine-tuning by 1.4–5.3×\ncompared to state-of-the-art baselines.\nREFERENCES\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.\nAleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. ,\n“Gpt-4 technical report,” arXiv preprint arXiv:2303.08774 , 2023.\n[2] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng,\nC. Zhang, C. Ruan et al. , “Deepseek-v3 technical report,” arXiv\npreprint arXiv:2412.19437 , 2024.\n[3] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know\nwhat language models know?” Transactions of the Association for\nComputational Linguistics , vol. 8, pp. 423–438, 2020.\n[4] W. Zhang, Y. Deng, B. Liu, S. J. Pan, and L. Bing, “Sentiment\nanalysis in the era of large language models: A reality check,”\narXiv preprint arXiv:2305.15005 , 2023.\n[5] B. Zhang, B. Haddow, and A. Birch, “Prompting large language\nmodel for machine translation: A case study,” in International\nConference on Machine Learning . PMLR, 2023, pp. 41 092–41 110.\n[6] J. Huang, J.-X. Bai, X. Zhang, Z. Liu, Y. Feng, J. Liu, X. Sun,\nM. Dong, and M. Li, “Keystrokesniffer: An off-the-shelf smart-\nphone can eavesdrop on your privacy from anywhere,” IEEE\nTransactions on Information Forensics and Security , 2024.\n[7] P . Voigt and A. Von dem Bussche, “The eu general data protec-\ntion regulation (gdpr),” A practical guide, 1st ed., Cham: Springer\nInternational Publishing , vol. 10, no. 3152676, pp. 10–5555, 2017.\n[8] B. Y. Lin, C. He, Z. Zeng, H. Wang, Y. Huang, C. Dupuy,\nR. Gupta, M. Soltanolkotabi, X. Ren, and S. Avestimehr, “Fednlp:\nBenchmarking federated learning methods for natural language\nprocessing tasks,” arXiv preprint arXiv:2104.08815 , 2021.\n[9] Z. Zhang, Y. Yang, Y. Dai, Q. Wang, Y. Yu, L. Qu, and Z. Xu,\n“Fedpetuning: When federated learning meets the parameter-\nefficient tuning methods of pre-trained language models,” in\nAnnual Meeting of the Association of Computational Linguistics 2023 .\nAssociation for Computational Linguistics (ACL), 2023, pp. 9963–\n9977.\n[10] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,\n“Language models are unsupervised multitask learners,” OpenAI\nblog, vol. 1, no. 8, p. 9, 2019.\n[11] S. Mittal, “A survey on optimized implementation of deep learn-\ning models on the nvidia jetson platform,” Journal of Systems\nArchitecture , vol. 97, pp. 428–442, 2019.\n[12] J. Liu, J. Yan, H. Xu, Z. Wang, J. Huang, and Y. Xu, “Finch:\nEnhancing federated learning with hierarchical neural architecture\nsearch,” IEEE Transactions on Mobile Computing , vol. 23, no. 5, pp.\n6012–6026, 2023.\n[13] D. Cai, Y. Wu, S. Wang, F. X. Lin, and M. Xu, “Fedadapter:\nEfficient federated learning for modern nlp,” arXiv preprint\narXiv:2205.10162 , 2022.\n[14] Android Authority, “How much ram does your android phone\nreally need in 2025?” 2025, accessed: Mar. 16, 2025. [Online].\nAvailable: https://www.androidauthority.com\n[15] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-\nsilhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-\nefficient transfer learning for nlp,” in International conference on\nmachine learning . PMLR, 2019, pp. 2790–2799.\n[16] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, “Lora: Low-rank adaptation of large language\nmodels,” arXiv preprint arXiv:2106.09685 , 2021.\n[17] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692 , 2019.\n[18] J. Liu, J. Yan, J. Qi, H. Xu, S. Wang, C. Qiao, and L. Huang,\n“Adaptive local update and neural composition for accelerating\nfederated learning in heterogeneous edge networks,” IEEE Trans-\nactions on Networking , 2025.\n[19] S. Su, B. Li, and X. Xue, “Fedra: A random allocation strategy for\nfederated tuning to unleash the power of heterogeneous clients,”\ninEuropean Conference on Computer Vision . Springer, 2024, pp.\n342–358.[20] R. Liu, F. Wu, C. Wu, Y. Wang, L. Lyu, H. Chen, and X. Xie, “No\none left behind: Inclusive federated learning over heterogeneous\ndevices,” in Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining , 2022, pp. 3398–3406.\n[21] S. Woo, B. Park, B. Kim, M. Jo, S. J. Kwon, D. Jeon, and D. Lee,\n“Dropbp: accelerating fine-tuning of large language models by\ndropping backward propagation,” arXiv preprint arXiv:2402.17812 ,\n2024.\n[22] Y. Sun, Y. Xie, B. Ding, Y. Li, and J. Zhang, “Exploring se-\nlective layer fine-tuning in federated learning,” arXiv preprint\narXiv:2408.15600 , 2024.\n[23] Y. J. Cho, L. Liu, Z. Xu, A. Fahrezi, M. Barnes, and G. Joshi, “Het-\nerogeneous lora for federated fine-tuning of on-device foundation\nmodels,” in International Workshop on Federated Learning in the Age\nof Foundation Models in Conjunction with NeurIPS 2023 , 2023.\n[24] H. Robbins and S. Monro, “A stochastic approximation method,”\nThe annals of mathematical statistics , pp. 400–407, 1951.\n[25] X. Wu, X. Liu, J. Niu, H. Wang, S. Tang, and G. Zhu, “Fedlora:\nWhen personalized federated learning meets low-rank adapta-\ntion,” 2024.\n[26] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” arXiv preprint arXiv:1804.07461 , 2018.\n[27] P . Tillet, H.-T. Kung, and D. Cox, “Triton: an intermediate language\nand compiler for tiled neural network computations,” in Proceed-\nings of the 3rd ACM SIGPLAN International Workshop on Machine\nLearning and Programming Languages , 2019, pp. 10–19.\n[28] H. Xi, Y. Chen, K. Zhao, K. J. Teh, J. Chen, and J. Zhu, “Jetfire:\nEfficient and accurate transformer pretraining with int8 data flow\nand per-block quantization,” arXiv preprint arXiv:2403.12422 , 2024.\n[29] Y. Xu, Y. Liao, H. Xu, Z. Ma, L. Wang, and J. Liu, “Adaptive control\nof local updating and model compression for efficient federated\nlearning,” IEEE Transactions on Mobile Computing , vol. 22, no. 10,\npp. 5675–5689, 2022.\n[30] H. He, P . Ye, Y. Ren, Y. Yuan, and L. Chen, “Gora: Gradient-driven\nadaptive low rank adaptation,” arXiv preprint arXiv:2502.12171 ,\n2025.\n[31] Hugging Face, “Transformers library,” https://github.com/\nhuggingface/transformers, 2024, accessed: 2025-04-17.\n[32] A. Ardakani, A. Haan, S. Tan, D. T. Popovici, A. Cheung,\nC. Iancu, and K. Sen, “Slimfit: Memory-efficient fine-tuning\nof transformer-based models using training dynamics,” arXiv\npreprint arXiv:2305.18513 , 2023.\n[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An im-\nperative style, high-performance deep learning library,” Advances\nin neural information processing systems , vol. 32, 2019.\n[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805 , 2018.\n[35] P . He, J. Gao, and W. Chen, “Debertav3: Improving deberta using\nelectra-style pre-training with gradient-disentangled embedding\nsharing,” arXiv preprint arXiv:2111.09543 , 2021.\n[36] Hugging Face, “Pre-trained weights for llms,” https://\nhuggingface.co/models, 2025, accessed: 2025-03-30.\n[37] M. Xu, D. Cai, Y. Wu, X. Li, and S. Wang, “ {FwdLLM }: Efficient\nfederated finetuning of large language models with perturbed\ninferences,” in 2024 USENIX Annual Technical Conference (USENIX\nATC 24) , 2024, pp. 579–596.\n[38] J. Liu, J. Liu, H. Xu, Y. Liao, Z. Yao, M. Chen, and C. Qian,\n“Enhancing semi-supervised federated learning with progressive\ntraining in heterogeneous edge computing,” IEEE Transactions on\nMobile Computing , 2024.\n[39] Y. J. Cho, L. Liu, Z. Xu, A. Fahrezi, and G. Joshi, “Heterogeneous\nlora for federated fine-tuning of on-device foundation models,”\narXiv preprint arXiv:2401.06432 , 2024.\n[40] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980 , 2014.",
  "text_length": 72698
}