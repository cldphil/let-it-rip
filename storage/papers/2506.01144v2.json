{
  "id": "http://arxiv.org/abs/2506.01144v2",
  "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
  "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
  "authors": [
    "Ariel Shaulov",
    "Itay Hazan",
    "Lior Wolf",
    "Hila Chefer"
  ],
  "published": "2025-06-01T19:55:33Z",
  "updated": "2025-06-04T07:45:25Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01144v2",
  "full_text": "--- Page 1 ---\narXiv:2506.01144v2  [cs.CV]  4 Jun 2025FlowMo: Variance-Based Flow Guidance for Coherent\nMotion in Video Generation\nAriel Shaulov∗Itay Hazan∗Lior Wolf Hila Chefer\nSchool of Computer Science\nTel Aviv University, Israel\n“A pair of flamingos wading through shallow water”CogVideoX-5B+FlowMo“A ballerina leaping through the air”\n“A woman performing a challenging exercise”Wan2.1-1.3B+FlowMo\n“A dolphin jumping out of ocean waves”\n(a)\n(b)\nFigure 1: Text-to-video results before and after applying FlowMo on (a) Wan2.1 [ 1] and\nCogVideoX-5B [ 2]. We present FlowMo , an inference-time guidance method to enhance temporal\ncoherence in text-to-video models. Our method mitigates severe temporal artifacts, such as additional\nlimbs (woman, 1st row, 2nd row), objects that appear or disappear (flamingo, 2nd row), and object\ndistortions (woman, dolphin, 1st row), without requiring additional training or conditioning signals.\nAbstract\nText-to-video diffusion models are notoriously limited in their ability to model\ntemporal aspects such as motion, physics, and dynamic interactions. Existing\napproaches address this limitation by retraining the model or introducing external\n∗Equal contribution.\nProject page: https://arielshaulov.github.io/FlowMo/\nCorrespondence to: Ariel Shaulov: arielshaulov@mail.tau.ac.il , Itay Hazan: itay.hzn@gmail.com .\nPreprint. Under review.\n--- Page 2 ---\nconditioning signals to enforce temporal consistency. In this work, we explore\nwhether a meaningful temporal representation can be extracted directly from the\npredictions of a pre-trained model without any additional training or auxiliary in-\nputs. We introduce FlowMo , a novel training-free guidance method that enhances\nmotion coherence using only the model’s own predictions in each diffusion step.\nFlowMo first derives an appearance-debiased temporal representation by measuring\nthe distance between latents corresponding to consecutive frames. This highlights\nthe implicit temporal structure predicted by the model. It then estimates motion\ncoherence by measuring the patch-wise variance across the temporal dimension and\nguides the model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo sig-\nnificantly improves motion coherence without sacrificing visual quality or prompt\nalignment, offering an effective plug-and-play solution for enhancing the temporal\nfidelity of pre-trained video diffusion models.\n1 Introduction\nDespite recent progress, text-to-video diffusion models remain far from faithfully capturing the\ntemporal dynamics of the real world. Generated videos frequently exhibit temporal artifacts such as\nobjects appearing and disappearing, duplicated or missing limbs, and abrupt motion discontinuities [ 3,\n4,5]. These issues highlight the limited capability of text-to-video models to reason about motion,\nphysics, and dynamic interactions over time. To mitigate these shortcomings, prior works have\nproposed fine-tuning models with explicit motion-related objectives [ 3], conditioning the generation\non external motion signals such as optical flow or pixel trajectories [ 6,7,8,9], or designing complex\nmodel architectures tailored to capture temporal dependencies [10, 11, 12].\nHowever, these approaches require either retraining the model [ 3,11,12] or introducing rigid external\nconstraints that dictate motion [ 6,7], limiting flexibility and generality. In this work, we propose\nan alternative strategy dubbed FlowMo , a training-free guidance method that improves temporal\nconsistency using the model’s own internal representations during sampling. FlowMo extracts a\nlatent temporal signal directly from the pre-trained model during inference and leverages its statistics\nto derive a guidance signal, without any architectural modifications, training, or external supervision.\nOur method is grounded in the following key observation: the temporal evolution of individual spatial\npatches tends to be smooth when the motion is coherent. Namely, the shifts in the representation of\neach patch over time are expected to be relatively small, leading to low patch-wise variance across\nframes. In contrast, incoherent motion intuitively manifests as abrupt changes in appearance or\nstructure, producing high temporal variance in the patches that display temporal artifacts.\nNotably, measuring temporal relations in a way that is disentangled from appearance information is\nchallenging. As observed by previous works [ 3], the predictions of text-to-video models are biased\ntoward appearance-based features. To obtain a meaningful appearance-debiased representation, we\nuse the model’s latent predictions to compute pairwise distances between frames. This enables us to\nmeasure the shifts in patch representations using patch-wise variance over time while neutralizing\ntheir shared appearance content. This is motivated by prior works demonstrating that the latent\nspaces of generative models capture semantically meaningful transformations, where simple vector\noperations correspond to interpretable changes [13, 14, 15].\nWe explore the above intuition extensively in Sec. 3.2. First, we collect a set of generated videos\nthat exhibit significant motion. We categorize these videos into coherent and incoherent sets, and\ncompute the patch-based variance over time given the appearance-debiased representations discussed\nabove. Our experiments yield two complementary observations. First, we find a clear correlation\nbetween high patch-based variance and motion incoherence, indicating that measuring the shift in\npatch representations over time can serve as a reliable metric to estimate coherence. Second, we\nobserve both qualitatively and quantitatively that while coarse appearance-based features such as\nscene layout and spatial structure are established very early in the generation process, temporal\ninformation emerges only at later, intermediate denoising steps.\nMotivated by these findings, we present FlowMo, a method that dynamically guides text-to-video\ndiffusion models toward temporally coherent generations. At selected timesteps in the denoising\nprocess, we compute the maximal patch-wise variance over time, given the appearance-debiased\n2\n--- Page 3 ---\nlatent prediction. We then optimize the model’s prediction to reduce this temporal variance, thereby\nencouraging smoother, more coherent motion. This guidance is applied iteratively across timesteps,\nallowing FlowMo to influence both coarse and fine motion dynamics in the generation process.\nWe demonstrate our method’s effectiveness on two of the most popular open-source models, Wan2.1-\n1.3B [ 1] and CogVideoX-5B [ 2]. Across a wide range of metrics, we evaluate the impact of our\nmethod on motion quality, overall video quality, and prompt alignment, using both the automatic\nevaluation metrics proposed by VBench [ 16] and human-based assessments. In all cases, we find\nthat FlowMo consistently and significantly improves the temporal coherence of the generated videos,\nwhile preserving the aesthetic quality, text alignment, and motion magnitude (see Fig. 1).\nOur results show that it is possible to extract meaningful temporal signals from the learned latent\nrepresentations of text-to-video models. Such signals not only encapsulate the temporal structure of\nthe generated videos but also serve as actionable guidance cues.\n2 Related Work\nText-to-video generation Diffusion models have revolutionized visual content creation, starting\nwith image generation [ 17,18,19] and rapidly expanding to diverse applications such as text-to-image\nsynthesis [ 20,21] and image editing [ 22,23,24,25,26,27,28,29,30,31]. This success has spurred\ntheir adoption for video generation, from cascaded diffusion models [ 32,33,34,35,36,37,38,\n39,40,41], to most recently Diffusion Transformers (DiT) [ 42,43] based on Flow Matching (FM)\n[44, 45, 46], which constitute the current state-of-the-art, and form the basis of our work.\nInference-time guidance has emerged as a powerful technique to steer and refine the outputs\nof generative models across various tasks without training [ 47,48,49,28,50]. Such methods\ntypically optimize the model predictions based on an auxiliary loss. Inference-time guidance for\nvideo generation has only recently emerged as a promising research vector [ 51,52]. While our\nwork also explores inference-time optimization for video generation, existing objectives and guiding\nsignals inherently differ from ours. Li et al. [ 51] focus on steering video models using external motion\npriors, which requires access to additional motion-specific inputs, while Wei et al. [ 52] propose\nto minimize a global 3D variance loss. In contrast, our method leverages the internal latent-space\ndynamics to perform guidance without any auxiliary networks, perceptual objectives, or task-specific\npriors, making it a lightweight and fully self-supervised plug-and-play module.\nImproving temporal coherence in video generation Temporal coherence remains a core challenge\nin video synthesis [ 3,4,5,12], and existing solutions generally fall into three categories. First, training\nwith temporal objectives [3,11,53,54], which improves consistency but demands significant compute\nand access to training data. Second, guiding the generation with external motion signals such as\noptical flow or trajectories [ 6,7,8,9], which enforce coherence but require external inputs and are\nrestricted to the conditioning motion. Third, architectures designed for temporal modeling [10,11,\n55,56,57,58], which are often complex and not easily applied to pre-trained models. In contrast,\nFlowMo improves temporal coherence directly at inference time by leveraging the model’s internal\nrepresentations, without additional data, inputs, or retraining.\nClosest to our work, FreeInit [ 59] and VideoGuide [ 60] propose methods to reduce spatio-temporal\nincoherence in video generation. However, both were designed for earlier UNet-based models\ntrained with DDPM or DDIM samplers [ 61,45], which suffered from severe signal-to-noise ratio\n(SNR) mismatches between training and inference [ 59]. In contrast, modern Transformer-based\nFM architectures are substantially more robust, rendering these techniques less effective. For\ncompleteness, we include a comparison to FreeInit (which can be reasonably adapted to DiTs)\nin Appendix A. In our experiments, we find that applying FreeInit to DiTs results in a drop in key\nmetrics such as the overall video quality, as well as a significant drop in the amount of generated\nmotion.\n3\n--- Page 4 ---\n3 Method\n3.1 Preliminaries: Flow Matching in a VAE Latent Space\nFollowing common practice in state-of-the-art image and video generation models [ 62,63,1], we\nconsider models that leverage FM [ 44] to define the objective function and operate in the learned\nlatent space of a Variational Autoencoder (V AE) for efficiency. The V AE consists of an encoder-\ndecoder pair (E,D), where Emaps input data x∼ X from the pixel space to a lower-dimensional\nlatent representation z=E(x)∈ Z, andDyields a reconstruction x≈ D(z). Given a pre-trained\nV AE, FM learns a transformation from a standard Gaussian distribution in latent space z0∼ N(0, I),\nto a target distribution z1observed from applying Eon the data.\nAt each training step, FM draws a timestep t∈[0,1], and obtains a noised intermediate latent by\ninterpolating between z0andz1, namely zt= (1−t)·z1+t·z0. The model uθis then optimized to\npredict the velocity vt=dzt\ndt=z0−z1, namely:\nLFM=Ex1,t∼U(0,1),z0∼N(0,I)h\n∥uθ(zt, t)−(z0−z1)∥2i\n. (1)\nOnce trained, samples can be generated from an initial noisy latent z0∼ N(0, I)by applying a\nsequence of denoising steps over a discrete schedule. At time ti,ztiis denoised to produce zti+1by\napplying zti+1= (1−σti)·zti−σti·uθ(zti, ti), where σtiis an interpolation coefficient determined\nby the scheduler.\n3.2 Motivation\nIn the following, we conduct qualitative and quantitative experiments to motivate the construction of\nFlowMo. The experiments in this section are conducted on Wan2.1-1.3B [1] for efficiency.\nWe begin by describing the latent representation on which FlowMo operates. As mentioned in\nSec. 3.1, at each denoinsing step, the model prediction uθ,t:=uθ(zt, t)is an estimate of the velocity\nvt, which represents the direction from the noise distribution to the latent space distribution. To\nextract a temporal representation from the prediction, we propose a debiasing operator ∆, which\ncomputes the ℓ1-distance between consecutive latent frames to eliminate their common appearance\ninformation. Formally, ∆:RF×W×H×C→R(F−1)×W×H×Cis defined as: ∀f∈[F−1],∀w∈\n[W],∀h∈[H],∀c∈[C]\n(∆uθ,t)f,w,h,c =∥(uθ,t)f+1,w,h,c−(uθ,t)f,w,h,c∥1. (2)\nNext, we describe the motivational experiments conducted to examine the statistical characteristics of\nour proposed latent space.\n0 5 10 15 20 25 30 35 40\nTimestep0.580.600.620.640.660.68Mean Patch-wise Variance 2\nIncoherent (59 videos)\nCoherent (61 videos)\nFigure 2: Quantitative motivation . We mea-\nsure the mean temporal variance of spatial\npatches for coherent and incoherent videos.\nIncoherent videos portray higher variance.\nThe separation is visible from step 5 onward.\n95%-confidence interval was computed using\ntheseaborn python package.Quantitative motivation. Our central hypothesis\nis that temporally coherent motion corresponds to a\nform of local stability in uθ,t. Specifically, in videos\nwith smooth and consistent motion, object trajectories\nevolve gradually, yielding lower temporal variance\ninuθ,t. Incoherent motion, in contrast, introduces\nabrupt changes, manifesting as larger fluctuations\nand higher patch-wise variance in the latent predic-\ntions. Formally, given uθ,t, we define its temporal\npatch-wise variance tensor σ2∈RW×H×Cas the\nvariance across frames per patch and channel, i.e.\n∀w∈[W],∀h∈[H],∀c∈[C],\nσ2\nw,h,c =Vf∼[F−1][(∆uθ,t)f,w,h,c ], (3)\nwhere V(X) =E[(X−E[X])2].\nTo empirically validate our hypothesis, we conducted\na user study wherein several hundred generated\n4\n--- Page 5 ---\nvideos were rated on a 1-5 scale for both coherence and perceived amount of motion (higher is\nmore motion/better coherence). To isolate the effects of motion magnitude on video coherence, we\nfocused on videos with a substantial amount of motion (rated ≥3), and compared those labeled\nas completely incoherent (1), or completely coherent (5). As illustrated in Fig. 2, a clear negative\ncorrelation emerges: low-coherence videos consistently exhibit higher variance. This supports our\nintuition that temporal patch-wise variance is a meaningful measure of perceived coherence.\nNotably, the separation in variance becomes prominent from approximately the fifth generation\ntimestep onward. Next, we wish to conduct a qualitative experiment to motivate this phenomenon.\nt=0Frame 8\n Frame 14\n Frame 20\nt=4\n t=8\nFigure 3: Qualitative motivation. We visual-\nize the model prediction per timestep across\nthe generation. Coarse spatial information is\ndetermined in the first steps (0-4), whereas\nmotion is determined at steps 5-8, and refined\nin later steps.Qualitative motivation. To qualitatively explore\nthe process of motion generation in text-to-video\nmodels, we visualize the evolution of the model’s\nlatent space prediction across the generation steps.\nFirst, observe that by Sec. 3.1, the model prediction\nuθ,testimates the velocity vt=z0−z1. We can thus\nobtain an estimation of the fully denoised latent, ¯z1,\nat any intermediary step tas follows:\n¯z1=zt−σt·uθ,t, (4)\nwhere σtis the signal-to-noise ratio at step t, which\nis defined by the noise scheduler.\nNotably, ¯z1is alatent-space representation where\nchannels do not represent RGB information. We thus\narbitrarily select the first channel, to obtain ¯z1,c0∈\nRF×H×W, and visualize the grayscale video via:\nV¯z1,c0= 255 ·¯z1,c0−minF,H,W (¯z1,c0)\nmax F,H,W (¯z1,c0)−minF,H,W (¯z1,c0). (5)\nThe resulting visualizations of V¯z1,c0in representative timesteps are presented in Fig. 3. As can\nbe observed, coarse spatial information begins to emerge from the first generation step, where the\ntraining bar and some of the outline of the person are visible. By step 4, most of the structure of\nthe scene is already determined. Conversely, the motion appears to be added to the scene between\nsteps 4 and 8, as evidenced by the strong similarity between all frames in times 0,4, whereas step 8\nshows significant variance between these same frames (e.g., the person bends down between frames\n14 and 20). Visualizations on additional timesteps and latent channels are provided in Appendix B.\nNote that the above supports the quantitative experiment presented in Fig. 2. Since motion emerges\nonly around the later initial steps of the denoising process (4-8), we would expect our variance-based\nmetric to be meaningful only in the steps that depict measurable differences over time.\nIn App. C, we show quantitative evidence that, in addition to enhancing coherence, FlowMo reduces\nthe variance in generation steps that correspond to the above intuition.\n3.3 FlowMo\nMotivated by the previous section, Algorithm 1 outlines the FlowMo guidance mechanism, ap-\nplied within a single FM denoising step. We perform the FlowMo guidance at specific timesteps\n{τ1, . . . , τ ℓ}, corresponding to the early-to-mid stages of generation, following the motivation pre-\nsented in Fig. 3.\nEach denoising step tibegins by obtaining the model prediction uθ,ti, given an input text prompt\nP. To encourage alignment between the prediction and the textual prompt, Classifier-free guidance\n(CFG) [64] is first employed with a scale of ρ(Line 3).\nIf we are not in a refinement step, we jump to Line 14, in which we perform a standard FM step to\nobtain the next latent zti+1as a linear combination of the current latent and the predicted velocity:\nzti+1= (1−σti)·zti−σti·uθ,ti, (6)\nwhere σtiis a time-dependent coefficient representing the signal-to-noise ratio.\n5\n--- Page 6 ---\nAlgorithm 1 A Single FlowMo Denoising Step\nInput: A text prompt P, a timestep ti, a set of\niterations for refinement {τ1, . . . , τ ℓ}, and a\ntrained Flow Matching model FM.\nOutput: A noised latent zti+1for the next\ntimestep ti+1\n1:uθ,ti|P←FM(zti, ti,P)\n2:uθ,ti|∅←FM(zti, ti,∅)\n3:uθ,ti←uθ,ti|P+ρ·\u0000\nuθ,ti|P−uθ,ti|∅\u0001\n4:ifti∈ {τ1, . . . , τ ℓ}then\n5: Compute (∆uθ,ti)as in Eq. (2)\n6: Compute σ2as in Eq. (3)\n7: sw,h←Ec∼[C]h\nσ2\nw,h,ci\n∀w∀h\n8: L ← max w∼[W],h∼[H]sw,h\n9: zti←zti−η· ∇ztiL\n10: uθ,ti|P←FM(zti, ti,P)\n11: uθ,ti|∅←FM(zti, ti,∅)\n12: uθ,ti←uθ,ti|P+ρ·\u0000\nuθ,ti|P−uθ,ti|∅\u0001\n13:end if\n14:zti+1←(1−σti)·zti−σti·uθ,ti\n15:Return zti+1If, however, ti∈ {τi}ℓ\n1, a FlowMo refinement\nstep is performed (Line 5 to Line 12). We first\ncompute the appearance-debiased representa-\ntion,∆uθ,ti, as defined in Eq. (2) (Line 5).\nSubsequently, drawing on the motivation pre-\nsented in Fig. 2, we calculate the temporal vari-\nanceσ2\nw,h,c for each spatial patch (w, h, c )as\ndefined in Eq. (3) (Line 6). These patch-wise\nvariances are then averaged across the channel\ndimension to produce a single spatial map sw,h\nindicating a motion coherence score per patch\n(Line 7): ∀w∈[W],∀h∈[H]\nsw,h=Ec∼[C]\u0002\nσ2\nw,h,c\u0003\n=1\nCCX\nc=1σ2\nw,h,c.(7)\nThe final FlowMo loss Lis then determined by\nthe maximal value in this map, thereby targeting\nthe most dynamically-incoherent patch (Line 8):\nL= max\nw∼[W],h∼[H]sw,h. (8)\nIntuitively, this formulation encourages the\nmodel to produce a prediction uθ,tiwherein the\nlatent space distances of each spatial patch over\ntime are smoother, resulting in more coherent\nand gradual transitions in the generated video.\nInspired by existing guidance mechanisms that optimize spatial information [ 47], we propose to use\nthe loss in Eq. 7 to optimize the input latent to the diffusion step, zti(Line 9). Intuitively, this allows\nour optimization to modify low-level features in the generated video, including the coarse motion.\nThus, the optimization is performed as a gradient descent step:\nzti=zti−η· ∇ztiL, (9)\nwhere ηis the learning rate. Following this refinement, we repeat the denoising step tiwith the\noptimized latent zti(Lines 10 to 12).\n4 Experiments\nWe conduct qualitative and quantitative experiments to demonstrate FlowMo’s effectiveness. Our\nexperiments evaluate the improvement in temporal coherence enabled by our method, as well as its\nability to maintain or even enhance other aspects of the generation, such as appearance quality and\ntext alignment. We provide our code and a website with video results in the supplemental materials.\nImplementation details We employ two of the most popular publicly available text-to-video\nmodels: Wan2.1-1.3B [ 1] and CogVideoX-5B [ 2], using their officially provided weights and default\nconfigurations. Motivated by the insights from Sec. 3.2, we apply FlowMo in the first 12 timesteps\nof the generation, since these are responsible for coarse motion and structure. All our experiments\nemploy a learning rate of η= 0.005, using the Adam optimizer, on two NVIDIA H100 GPUs, with\n80GB memory each. Wan2.1 is evaluated at a resolution of 480 ×832, and CogVideoX at 480 ×720,\nboth generating 81 frames at 16 frames per second, resulting in 5-second videos.\n4.1 Qualitative Results\nFigures 1, 4 contain representative results demonstrating the impact of FlowMo on pre-trained text-to-\nvideo models. As can be observed, our method mitigates severe temporal artifacts that are common\nto text-to-video models. For example, the generations tend to display extra limbs (women in Fig. 1(a)\nand Fig. 4, 2nd, 3rd row), distortions of objects over time (dolphin in Fig. 1(a) and deer in Fig. 4, 4th\n6\n--- Page 7 ---\n+FlowMo Wan2.1-1.3B\n \n“A senior couple dancing at sunset on a pier” “A boy skipping rope in a backyard” \n“A violinist performing a solo on stage” \n “A figure skater gliding across the ice” \n            (a)\n“A deer leaping over a fallen log” +FlowMo Wan2.1-1.3B  +FlowMo CogVideoX-5B \n +FlowMo\n CogVideoX-5B \n            (b)“A teenager skateboarding down a handrail” \n“A boy with glasses flying a kite in a grassy field” “A roulette wheel in a dimly lit room or casino  floor” \nFigure 4: Qualitative results . Text-to-video results before and after applying FlowMo on (a)\nWan2.1 [ 1] and (b) CogVideoX [ 2]. FlowMo mitigates severe temporal artifacts, e.g., extra limbs\n(women, 2nd, 3rd row), objects that appear or disappear (2nd, 3rd row), and distortions (4th row).\nrow), and objects that suddenly appear or disappear (flamingo in Fig. 1(b) and rope, violin in Fig. 4,\n2nd, 3rd row). These results demonstrate that temporal artifacts correspond to abrupt changes in the\nlatent representations of video patches. This, in turn, drives our optimization process to encourage\nsmoother representations of the affected patches, resulting in improved temporal coherence.\n7\n--- Page 8 ---\n0 20 40 60 80 100\nWin RateMotionQualityText\nalignment\n44.3%31.1%14.9%\n39.5%54.9%77.9%\n16.2%14.0%7.2%Wan2.1-1.3B\n0 20 40 60 80 100\nWin Rate43.0%31.7%15.6%\n39.4%51.3%75.7%\n17.6%17.1%8.7%CogVideoX-5B\nOurs Both BaselineFigure 5: User study conducted on Wan2.1-1.3B [ 1] (left) and CogVideoX-5B [ 2] (right) using\nVideoJAM-bench [3], designed specifically to evaluate motion coherence. Our method significantly\nimproves temporal coherence in all models, while maintaining or improving the visual quality and\nthe text alignment of the resulting videos. 95%-confidence intervals were calculated using Dirichlet\nsampling, assuming a multinomial distribution with Laplace smoothing applied to the counts.\nTable 1: VBench evaluation results. A comparison of the overall video quality before and after\napplying FlowMo on Wan2.1-1.3B [ 1] and CogVideoX-5B [ 2] using VBench [ 16]. We enclose\nboth the motion-specific and the aggregated scores. FlowMo consistently improves the Final Score\nrepresenting the overall video quality by at least 5%.\nMotion Metrics Aggregated Scores\nModelsMotion\nSmoothnessDynamic\nDegreeSemantic\nScoreQuality\nScoreFinal\nScore\nWan2.1-1.3B 96.43% 83.21% 84.70% 65.58% 75.14%\n+ FlowMo 98.56% 81.96% 89.11% 73.58% 81.34% (+6.20%)\nCogVideoX-5B 95.01% 65.29% 70.03% 60.83% 65.43%\n+ FlowMo 97.29% 63.92% 69.26% 72.11% 70.69% (+5.26%)\n4.2 Quantitative Results\nWe employ both the VBench benchmark [ 16] and human-based evaluations, which serve as the\nstandard evaluation protocols for measuring the quality of text-to-video generation [ 2,65,66,67,68].\nUser study. We conduct a human preference study using the VideoJAM benchmark [ 3], which\nwas specifically designed to test motion coherence. For each prompt, we generate a pair of videos\n(with and without FlowMo) with a fixed seed in the setting described above, and randomly shuffle the\norder of the results. Each prompt was evaluated by five different participants, resulting in 640unique\nresponses per baseline. Annotators were asked to compare the videos based on their alignment to the\ntext prompt, the aesthetic quality of the videos, and the motion quality (see App. D).\nThe results, presented in Fig. 5, demonstrate a consistent human preference for FlowMo-guided\nvideos across all criteria. Specifically for Motion Coherence , FlowMo was favored in 44.3% of\ncomparisons for Wan2.1 (vs. 16.2% for baseline) and 43.0% for CogVideoX (vs. 17.6% for baseline).\nA similar trend was observed for Aesthetic Quality , where FlowMo was preferred in 31.1% of\nWan2.1 pairs (vs. 14.0% for baseline) and 31.7% of CogVideoX pairs (vs. 17.1% for baseline).\nInterestingly, FlowMo also showed improved Text-Video Alignment , with preference rates of 14.9%\nfor Wan2.1 (vs. 7.2% for baseline) and 15.6% for CogVideoX (vs. 8.7% for baseline).\nThese findings highlight that FlowMo not only enhances temporal coherence but also contributes\npositively to the overall perceived video quality and faithfulness to the input prompt.\nAutomatic metrics. The results of the automatic metrics on the VBench benchmark [ 16] are\nsummarized in Tab. 1. We enclose both the motion-based metrics, and the aggregated metrics,\nwhich constitute an average of all the benchmark dimensions, and measure the overall quality of the\ngenerations. A full breakdown of all metrics is provided in App. E.\n8\n--- Page 9 ---\nNotably, FlowMo significantly improves the Final Score by 6.2%, 5.26% for Wan, CogVideoX,\nrespectively. This metric represents the overall quality score, considering all the evaluation dimensions.\nThis improvement is supported by gains in the Quality Score (Wan2.1: +8.0%; CogVideoX: +11.28%)\nand Semantic Score for Wan2.1 (+4.41%), with a negligible decrease of 0.77% for CogVideoX.\nConsidering the motion metrics, FlowMo boosts Motion Smoothness (Wan2.1: +2.13%; CogVideoX:\n+2.28%), which is a key metric that evaluates the motion coherence. Finally, note that some decrease\nto the dynamic degree is expected. This is since temporal artifacts such as objects appearing and\ndisappearing increases the amount on motion in the video. With that, we observe that the metric\ndoes not decrease significantly (less than 1.5% for both models), especially compared to the 16.20%\ndecrease in this metric demonstrated by FreeInit, FlowMo’s most closely-related method (see App. A).\nIn summary, both human evaluations and automated VBench metrics consistently demonstrate\nFlowMo’s effectiveness in improving motion coherence and overall video quality.\n4.3 Ablation Study\nMean FlowMo  \n All Steps W/O Debias \n“A ninja flipping through a bamboo forest”  “Pikachu with Charmander at a campfire”  \nWan2.1-1.3\nB \nFigure 6: Ablation study. We ablate the main design choices\nof FlowMo, i.e., using the maximal variance for the objective\n(3rd row), using the appearance-debiasing operator (4th row),\nthe selection of the optimization steps (5th row), and show\nthat FlowMo is significantly superior to all variants.We ablate the primary design choices\nof FlowMo, namely using the patch\nwith the maximal variance to derive\nthe loss (Eq. 8), the appearance de-\nbiasing operator ∆(Eq. 2), and the\nselection of diffusion steps to apply\nthe optimization (1-12).\nResults of the ablation study on\nWan2.1 are reported in Fig. 6. For\neach prompt, we enclose the result\nwith FlowMo (1st row), Wan2.1 (2nd\nrow) and the ablations (3rd-5th row).\nReplacing the maximum with the\nmean (3rd row) significantly weakens\nthe optimization effect, likely because\nmost patches are static, leading to a\nsmaller loss and diminished gradients.\nRemoving the debiasing operator (4th\nrow) yields a similar effect. This can\nbe attributed to the fact that, as ob-\nserved by previous works [ 3], predic-\ntions by text-to-video models tend to\nbe appearance-based, reducing the in-\nfluence of motion on the loss. Finally,\napplying FlowMo across all diffusion steps (5th row) introduces artifacts, as the optimization inter-\nferes with high frequencies and fine details in steps where motion is already determined.\n4.4 Limitations\nWhile our method enables a substantial improvement in the motion coherence and overall quality of\ngenerated videos, it still has a few limitations. First, due to the calculation and propagation of gradients\nby our method (see Alg. 1), there is some slowdown in the inference time. On average, generating a\nvideo with FlowMo takes 234.30 seconds compared to 99.27 seconds without it, corresponding to a\n×2.39increase. This overhead could be mitigated by integrating FlowMo into the training phase,\neliminating the need for gradient-based optimization at inference time.\nSecond, since FlowMo does not modify the model weights, it is bounded by the learned capabilities\nof the pre-trained model. While it can improve the coherence of motion predicted by the model, it\ncannot synthesize motion types the model has not learned to represent. We believe this limitation can\nbe addressed by incorporating motion-based objectives based on the model’s internal representations\nduring training, encouraging richer temporal understanding in generative video models.\n9\n--- Page 10 ---\n5 Conclusions\nCan we extract meaningful temporal representations from a model with limited temporal understand-\ning? In this work, we propose a new approach to address temporal artifacts in text-to-video models.\nInstead of relying on external signals, additional data, or specialized architectures, we repurpose the\nmodel’s own learned representations as a source of temporal guidance . Specifically, we examine\nthe semantic latent space learned by text-to-video diffusion models and find that it implicitly en-\ncodes valuable temporal information. Through extensive analysis (Sec. 3.2), we show that distances\nbetween pairs of frames in this latent space correlate with intuitive measures of temporal artifacts,\nsuch as patch-wise variance over time. Building on these insights, we implement an inference-time\nguidance method that encourages smoother transitions in the latent space, and observe that this maps\nto smoother behavior in pixel space as well, significantly boosting motion coherence while preserving\nand even improving other aspects of the generation. We hope this work sparks further interest in\nexploring the temporal properties of semantic latent spaces and encourages the development of\nmethods that improve temporal coherence by looking inward rather than outward.\n6 Acknowledgments\nThis work was supported by a grant from the Tel Aviv University Center for AI and Data Science\n(TAD).\n10\n--- Page 11 ---\nReferences\n[1]Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu,\nKang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang\nChu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong\nShen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu,\nXianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang,\nYingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi,\nYutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale\nvideo generative models. arXiv preprint arXiv:2503.20314 , 2025.\n[2]Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint arXiv:2205.15868 , 2022.\n[3]Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and\nShelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in\nvideo models. arXiv preprint arXiv:2502.02492 , 2025.\n[4]Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How\nfar is video generation from world model: A physical law perspective, 2024.\n[5]Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor,\nTroy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as\nworld simulators. 2024.\n[6]Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-\nGuevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and\nDeqing Sun. Motion prompting: Controlling video generation with motion trajectories, 2024.\n[7]Wan-Duo Kurt Ma, J. P. Lewis, and W. Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based\nvideo generation, 2023.\n[8]Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-\ngrounded image-to-video generation. In European Conference on Computer Vision ECCV , 2024.\n[9]Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel\nPerez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent\ntext-to-video editing. arXiv preprint arXiv:2310.05922 , 2023.\n[10] S. Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content\nfor video generation. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n1526–1535, 2017.\n[11] Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang\nLiu, Di Zhang, Yang Song, Kun Gai, and Yadong Mu. Video-lavit: Unified video-language pre-training\nwith decoupled visual-motional tokenization. ArXiv , abs/2402.03161, 2024.\n[12] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for\nvideo generation. arXiv preprint arXiv:2504.12626 , 2025.\n[13] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. CoRR , abs/1511.06434, 2015.\n[14] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic\nface editing. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n9240–9249, 2019.\n[15] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-\nnada. ACM Transactions on Graphics (TOG) , 41:1 – 13, 2021.\n[16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu,\nQingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and\nZiwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR , 2024.\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS , 2020.\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P.\nKingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303 , 2022.\n11\n--- Page 12 ---\n[19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. arXiv preprint\narXiv:2105.05233 , 2021.\n[20] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation, 2021.\n[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,\nDavid J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language\nunderstanding, 2022.\n[22] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618 , 2022.\n[23] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel,\nTomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video\ndata. arXiv preprint arXiv:2407.08674 , 2024.\n[24] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and\nYaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In CVPR , 2024.\n[25] Uriel Singer, Amit Zohar, Yuval, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video\nediting via factorized diffusion distillation. In ECCV , 2024.\n[26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 , 2022.\n[27] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\npages 18392–18402, 2023.\n[28] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free\nobject insertion in images with pretrained diffusion models. arXiv preprint arXiv:2411.07232 , 2024.\n[29] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal\nIrani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 6007–6017, 2023.\n[30] Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, and Srinath Sridhar. Geodiffuser: Geometry-based\nimage editing with diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer\nVision (WACV) , pages 472–482. IEEE, 2025.\n[31] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.\nPaint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 18381–18391, 2023.\n[32] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. arXiv preprint arXiv:2106.15282 , 2021.\n[33] Omer BarTal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa\nHur, Yuanzhen Li, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: A\nspace-time diffusion model for video generation. arXiv preprint arXiv:2401.12945 , 2024.\n[34] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-Shift:\nLatent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477 ,\n2023.\n[35] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,\nYongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv\npreprint arXiv:2501.03575 , 2025.\n[36] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew\nLai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments.\nInForty-first International Conference on Machine Learning , 2024.\n[37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu,\nJianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv\npreprint arXiv:2412.03603 , 2024.\n12\n--- Page 13 ---\n[38] Hedra. Hedra: Ai-powered character video generation. https://www.hedra.com/ , 2025. Accessed:\nMay 13, 2025.\n[39] OpenAI. Sora: Text-to-Video Generation. https://openai.com/sora/ , 2024. Accessed: May 13,\n2025.\n[40] Google DeepMind. Genie 2: A Large-Scale Foundation World Model. https://deepmind.google/\ndiscover/blog/genie-2-a-large-scale-foundation-world-model/ , 2024. Accessed: May 13,\n2025.\n[41] World Labs. Generating worlds. https://www.worldlabs.ai/blog , 2024. Accessed: 2025-05-13.\n[42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV , 2023.\n[43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952 , 2023.\n[44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\ngenerative modeling. arXiv preprint arXiv:2210.02747 , 2022.\n[45] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,\nJiashuo Yu, Peiqing Yang, et al. LaVie: High-quality video generation with cascaded latent diffusion\nmodels. arXiv preprint arXiv:2309.15103 , 2023.\n[46] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector\nframework for fast sampling of diffusion models. Advances in Neural Information Processing Systems ,\n36:49842–49869, 2023.\n[47] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-\nbased semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG) ,\n42(4):1–10, 2023.\n[48] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for\nmulti-subject text-to-image generation. European Conference on Computer Vision (ECCV) , pages 432–448,\n2024.\n[49] Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, and Gal Chechik. Make it count:\nText-to-image generation with an accurate number of objects. arXiv preprint arXiv:2406.10210 , 2024.\n[50] Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, and Martial Hebert. Separate-and-enhance:\nCompositional finetuning for text-to-image diffusion models. In ACM SIGGRAPH 2024 Conference\nPapers , pages 1–10, 2024.\n[51] Jialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon, and Mohit Bansal. Training-free guidance\nin text-to-video generation via multimodal planning and structured noise initialization. arXiv preprint\narXiv:2504.08641 , 2025.\n[52] Min Wei, Chaohui Yu, Jingkai Zhou, and Fan Wang. 3dv-ton: Textured 3d-guided consistent video try-on\nvia diffusion models. arXiv preprint arXiv:2504.17414 , 2025.\n[53] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7310–7320, 2024.\n[54] Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Boosting text-to-video generative\nmodel with mllms feedback. In The Thirty-eighth Annual Conference on Neural Information Processing\nSystems , 2024.\n[55] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for\nhigh-fidelity long video generation. arXiv preprint arXiv:2211.13221 , 2022.\n[56] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video\ngeneration from open domain textual description. arXiv preprint arXiv:2210.02399 , 2022.\n[57] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video:\nMulti-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264 , 2023.\n13\n--- Page 14 ---\n[58] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. FreeNoise:\nTuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169 , 2023.\n[59] Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. Freeinit: Bridging initialization\ngap in video diffusion models. arXiv preprint arXiv:2312.07537 , 2023.\n[60] Dohun Lee, Bryan S Kim, Geon Yeong Park, and Jong Chul Ye. Videoguide: Improving video diffusion\nmodels without training through a teacher’s guide. arXiv preprint arXiv:2410.04364 , 2024.\n[61] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. AnimateD-\niff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint\narXiv:2307.04725 , 2023.\n[62] Black Forest Labs. FLUX, 2024.\n[63] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas,\nBowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi,\nGuan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang,\nMannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter\nVajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi,\nSamyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya,\nSimran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman,\nYaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert\nPumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh\nWood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie\nNord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng\nYu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani,\nSteve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: A cast of media foundation models,\n2024.\n[64] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 ,\n2022.\n[65] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali\nFarhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. arXiv preprint arXiv:2406.11775 ,\n2024.\n[66] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v:\nEnhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324 , 2024.\n[67] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-\nanimator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275 ,\n2024.\n[68] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming\naudio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634 , 2024.\n14\n--- Page 15 ---\nA Comparison between FlowMo and FreeInit\nIn this section, we compare FlowMo with FreeInit [ 59], which is most closely related to our method.\nFreeInit was designed for earlier UNet-based models that employ DDPM or DDIM [ 61,45]. Its\napproach is motivated by the observation that such models often exhibit significant spatio-temporal\ninconsistencies in scene elements across frames (e.g., character identities and backgrounds changing\nbetween frames). Their primary observation is that these models exhibit discrepancies in signal-to-\nnoise ratios (SNR) between training and inference phases, causing temporal artifacts. In contrast,\nmodern Transformer-based architectures, as used in our work, are generally more robust to these\ninconsistencies due to more powerful architectures, more stable training frameworks (using FM), and\nlarger training datasets. Thus, these models are able to maintain consistent appearance across frames\nand are less susceptive to these types of artifacts.\nTo provide a fair comparison, we adapted FreeInit for use with FM-based DiT models. This involved\nre-noising a denoised latent and combining this re-noised latent with random noise to initialize the\nlow-frequency components, before repeating the denoising process, as per FreeInit’s methodology.\nWe then conducted both quantitative (user study and VBench automatic metrics) and qualitative\ncomparisons between FlowMo and our adapted FreeInit. The experiments are demonstrated hereafter.\nImplementation details. All experiments were done on the Wan2.1-1.3B model, for efficiency.\nThe experimental setting for the vanilla baseline and FlowMo-guided model is the same as in Sec. 4.\nFreeInit was implemented based on its publicly available open-source code [ 59] and its default\nconfiguration, namely employing the butterworth filter with n= 4,ds=dt= 0.25. To remain\ncomparable with our work, we performed one refinement iteration with each of the methods.\nA.1 Qualitative Experiments\n FlowMo FreeInit \n“Young Adult Male Doing Handstand on the beach” “A man jumping rope on a dark stage”  FlowMo FreeInit \n“A close-up of a person’ s feet as they walk ” “A person lifts one knee high in a marching motion” \nFigure 7: Qualitative results. Text-to-video results of FreeInit [ 59] (1st, 3rd row) and FlowMo (2nd,\n4th row) when applied on Wan2.1-1.3B [ 1]. FlowMo better mitigates severe temporal artifacts, e.g.\ndistortions and object that appear and disappear.\nQualitative comparisons between FlowMo and the adapted FreeInit are presented in Fig. 7. These\nexamples highlight FlowMo’s superiority in generating visually coherent motion. For instance, in\n15\n--- Page 16 ---\n0 20 40 60 80 100\nWin RateMotionQualityText\nalignment\n44.3%31.1%14.9%\n39.5%54.9%77.9%\n16.2%14.0%7.2%Wan2.1-1.3B vs FlowMo\n0 20 40 60 80 100\nWin Rate38.7%28.1%16.5%\n38.0%57.1%78.3%\n23.4%14.8%5.2%FlowMo vs FreeInit\nOurs Both BaselineFigure 8: User studies conducted on Wan2.1-1.3B [ 1] using VideoJAM-bench [ 3]. The studies\ncompare three variants of the model: vanilla (Wan2.1-1.3B), with FlowMo, and with FreeInit. Our\nmethod significantly outperforms the baselines in both studies. 95%-confidence intervals were\ncalculated using Dirichlet sampling, assuming a multinomial distribution with Laplace smoothing\napplied to the counts.\nthe top-left example, FlowMo successfully generates a plausible marching motion, whereas FreeInit\nproduces disappearing feet and less convincing movement. Similarly, in the top-right example,\nFlowMo depicts coherent walking, while the rendition produced by FreeInit suffers from partially\ndisappearing feet and a less natural gait. The bottom-left example shows FlowMo maintaining the\nintegrity of the man and rope, while in the video refined with FreeInit, the rope distorts and disappears\nand the man has a less stable form. Finally, in the bottom-right example, FlowMo maintains a\nconsistent orientation, whereas the front and back sides of the person flip spontaneously in the version\ngenerated with FreeInit.\nOverall, these visual examples show that FlowMo demonstrates a significant advantage in producing\nmore coherent and artifact-free motion compared to FreeInit.\nA.2 Quantitative Experiments\nConsistent with the experiments presented in the main paper, we compare FlowMo to the FreeInit\nbaseline using both the VBench benchmark [16] and human evaluation results.\nUser study. We conducted a human preference study that compared videos generated by Wan2.1-\n1.3B guided by FlowMo with those guided by FreeInit. Videos from both models were sampled given\nprompts from the VideoJAM benchmark [ 3] in the same setting described in Sec. 4.2 of the main\npaper.\nThe results, presented in Fig. 8, clearly indicate a strong human preference for FlowMo across all\nevaluated categories, in comparison to FreeInit as well as the vanilla Wan2.1-1.3B model. Comparing\nFreeInit and FlowMo, for Motion Coherence , FlowMo was preferred in 38.7% of comparisons,\nsubstantially more than FreeInit (23.4%). In terms of Aesthetic Quality , FlowMo was chosen\nin 28.1% of pairs, nearly double the preference for FreeInit (14.8%). Furthermore, FlowMo also\noutperformed FreeInit in Text-Video Alignment , with a preference rate of 16.5% compared to\nFreeInit’s 5.2%. These results demonstrate that human evaluators find FlowMo-guided videos to be\nsignificantly more coherent, aesthetically pleasing, and better aligned with textual prompts than those\nguided by the adapted FreeInit.\nVBench Benchmark. We further evaluate FlowMo against the adapted FreeInit using the VBench\nbenchmark. The detailed results per dimension are presented in Tab. 2.\nThe VBench metrics [ 16] corroborate the user study findings, showing FlowMo’s superiority. First,\nobserve that across the comprehensive suite of VBench metrics detailed in Tab. 2, the adapted FreeInit\ndoes not achieve a superior score to both FlowMo and the baseline in anyindividual dimension .\nConsidering the main metrics related to motion coherence, FreeInit achieves only a marginal improve-\nment in Motion Smoothness (+0.11% over baseline) compared to FlowMo’s substantial +2.13%\ngain. Critically, FreeInit significantly degrades the Dynamic Degree by -16.20% from the baseline\n16\n--- Page 17 ---\nTable 2: VBench evaluation results per dimension. Each column represents a model variant,\nWan2.1 (Baseline), with FlowMo, and with FreeInit. Rows correspond to the 16 VBench evaluation\ndimensions. While FlowMo significantly increases the overall quality of the videos (+6.20%) ,\nFreeInit reduces it (-0.08%), and is unable to compare with the Baseline and FlowMo in any of the\ndimensions.\nDimension Wan2.1-1.3B\nBaseline + FlowMo + FreeInit\nSubject Consistency 95.61% 96.54% 93.71%\nBackground Consistency 97.25% 97.02% 97.12%\nTemporal Flickering 99.15% 98.93% 97.77%\nMotion Smoothness 96.43% 98.56% 96.54%\nDynamic Degree 83.21% 81.96% 67.01%\nAesthetic Quality 56.77% 58.03% 50.99%\nImaging Quality 61.01% 64.89% 57.19%\nObject Class 91.37% 95.35% 92.11%\nMultiple Objects 77.98% 82.27% 73.26%\nHuman Action 98.27% 97.23% 97.98%\nColor 87.94% 87.28% 86.53%\nSpatial Relationship 75.21% 78.42% 76.52%\nScene 49.84% 49.41% 48.32%\nAppearance Style 20.95% 28.45% 21.59%\nTemporal Style 26.35% 27.30% 24.24%\nOverall Consistency 23.65% 25.53% 22.13%\nSemantic Score 84.70% 89.11% 85.91%\nQuality Score 65.58% 73.58% 64.22%\nFinal Score 75.14% 81.34% (+6.20%) 75.06 (-0.08%)\n(from 83.21% to 67.01%), whereas FlowMo maintains a comparable dynamic level (81.96%). This\nlarge reduction in motion by FreeInit suggests that its apparent coherence might stem from producing\nless dynamic videos, which are inherently easier to keep coherent, rather than genuinely improving\nthe quality of complex motion.\nFurthermore, FreeInit performs worse than both the baseline and FlowMo in several other important\nquality aspects. For instance, its Aesthetic Quality (50.99%) is lower than both baseline (56.77%)\nand FlowMo (58.03%). Similar trends are observed for other important qualities, e.g. Temporal\nFlickering (FreeInit: 97.77% vs. FlowMo: 98.93%, Baseline: 99.15%), Imaging Quality (FreeInit:\n57.19% vs. FlowMo: 64.89%), Appearance Style (FreeInit: 21.59% vs. FlowMo: 28.45%),\nTemporal Style (FreeInit: 24.24% vs. FlowMo: 27.30%), and Overall Consistency (FreeInit:\n22.13% vs. FlowMo: 25.53%).\nFinally, FlowMo significantly outperforms FreeInit in the aggregated VBench metrics . FlowMo\nachieves a Final Score of 81.34%, a +6.20% improvement over the baseline, while FreeInit scores\n75.06%, slightly below the baseline . Similarly, FlowMo leads in Quality Score (73.58% vs. FreeInit’s\n64.22%) and Semantic Score (89.11% vs. FreeInit’s 85.91%). These results underscore that FlowMo\nprovides a more effective and well-rounded improvement to video generation quality compared to\nthe adapted FreeInit on modern FM-based DiT architectures.\n17\n--- Page 18 ---\nB Additional Qualitative Motivation Results\nTo complement the qualitative observations in the main paper, we present additional visualizations\nof the latent-space predictions ¯z1,cacross timesteps during the generation process. The two figures\nbelow show a grid of frames from six different time indices (10–20) and ten representative diffusion\nsteps (0–18), providing a spatio-temporal view of how motion emerges over time within the latent\nspace.\nFig. 9 displays predictions for channel 0, used in the main paper, while Fig. 10 shows results for a\nrandomly selected channel (channel 7). In both cases, we observe that coarse spatial structure appears\nin early steps, while coarse motion emerges primarily between steps 4 and 8. Although motion is\nrefined in later steps, as seen in timesteps t= 10 onward, its coarse features are determined earlier,\nmaking the first timesteps the most crucial for coherent motion generation. These patterns reinforce\nour interpretation that motion is added into the generation during these intermediate steps, which\nunderpins our focus on this range for motion-aware optimization in FlowMo.\nt=0Frame 10\n Frame 12\n Frame 14\n Frame 16\n Frame 18\n Frame 20\nt=2\n t=4\n t=6\n t=8\n t=10\n t=12\n t=14\n t=16\n t=18\nFigure 9: A visualization of channel 0 (selected arbitrarily, and used in the main paper) of the latent\nprediction at different timesteps of the generation.\n18\n--- Page 19 ---\nt=0Frame 10\n Frame 12\n Frame 14\n Frame 16\n Frame 18\n Frame 20\nt=2\n t=4\n t=6\n t=8\n t=10\n t=12\n t=14\n t=16\n t=18\nFigure 10: A visualization of channel 7 (selected randomly) of the latent prediction at different\ntimesteps of the generation.\n19\n--- Page 20 ---\nC The Effect of FlowMo on Patch-Wise Variance\n0 5 10 15 20 25 30 35 40\nTimestep0.80.91.01.11.21.3Max Patch-wise Variance 2\nWithout FlowMo\nWith FlowMo\nFigure 11: FlowMo effect on patch-wise\nvariance . We plot the maximal temporal vari-\nance of spatial patches for videos with and\nwithout applying FlowMo guidance, and ob-\nserve that our method significantly reduces\nand stabilizes the variance in the generation\nsteps that impact motion the most by our anal-\nysis from Sec. 3.2. 95%-confidence inter-\nval was computed using the seaborn python\npackage.This section demonstrates the alignment between\nFlowMo’s optimization mechanism and our moti-\nvating insights from Sec. 3.2. Fig. 11 plots the\nmaximal patch-wise temporal variance (FlowMo’s\nloss, Eq. (7)) for videos generated with and without\nFlowMo guidance.\nWe observe that the results demonstrate a strong cor-\nrelation with the conclusions from Sec. 3.2. First, the\napplication of FlowMo results in a significant reduc-\ntion and stabilization of this maximal variance, which\nis particularly evident from approximately timestep 5\nonward. This observation is consistent with our ear-\nlier finding (Fig. 2) that the variance characteristics\nof coherent and incoherent videos begin to diverge at\nthis stage.\nSecond, FlowMo’s optimization is applied during the\ninitial 12 timesteps of the generation process. This\ntargeted intervention aligns with our qualitative mo-\ntivation (Fig. 3), which indicates that coarse motion\npatterns are predominantly established within these\nearly stages of the generation. As can be observed,\napplying the optimization at these steps indeed stabi-\nlizes the maximal variance in all other, non-optimized steps as well.\nConsequently, Fig. 11 illustrates that FlowMo guidance leads to a notable decrease in the maximal\npatch-wise temporal variance. Videos generated with FlowMo exhibit consistently lower variance,\nespecially within the critical timesteps 5-15, compared to the higher and more fluctuating variance\nobserved in videos generated without FlowMo. This empirically validates that FlowMo operates as\nintended by reducing the target variance metric during the crucial phases of motion formation.\n20\n--- Page 21 ---\nD User Study: Instructions Provided to Participants\nAs part of the evaluations we performed on our method, we conducted a user study, as described in\nSec. 4.2. The study was designed to assess human preferences on videos generated with and without\nFlowMo, using the videoJAM benchmark [3], which focuses on motion coherence.\nThe study was conducted using Google Forms. For each prompt, participants were shown a pair of\nvideos—one with FlowMo and one without—generated with the same random seed (1024). The\norder of the videos was randomized to avoid positional bias. Each pair was evaluated by five different\nparticipants, resulting in 640responses per baseline.\nParticipants were asked to evaluate the videos based on three criteria: text alignment ,aesthetic quality ,\nandmotion coherence . The instructions provided to annotators are reproduced below, followed by a\nscreenshot of the interface used:\nHello! We need your help to read a caption, and then watch two generated videos.\nAfter watching the videos, we want you to answer a few questions about them:\n•Text alignment : Which video better matches the caption?\n•Quality : Aesthetically, which video is better?\n•Motion : Which video has more coherent and physically plausible motion?\n(Do note: it is OK if the quality is less impressive as long as the motion looks\nbetter.)\nFigure 12: Screenshot of the Google Form used in the user study.\n21\n--- Page 22 ---\nE VBench Metrics Breakdown\nTable 3: VBench evaluation results per dimension. Each column represents a model variant, with\nand without FlowMo. Rows correspond to the 16 VBench evaluation dimensions.\nDimension Wan2.1-1.3B CogVideoX-5B\nBaseline + FlowMo Baseline + FlowMo\nSubject Consistency 95.61% 96.54% 95.79% 97.02%\nBackground Consistency 97.25% 97.02% 96.53% 97.53%\nTemporal Flickering 99.15% 98.93% 99.23% 96.21%\nMotion Smoothness 96.43% 98.56% 95.01% 97.29%\nDynamic Degree 83.21% 81.96% 65.29% 63.92%\nAesthetic Quality 56.77% 58.03% 55.51% 58.29%\nImaging Quality 61.01% 64.89% 58.91% 58.75%\nObject Class 91.37% 95.35% 82.72% 88.41%\nMultiple Objects 77.98% 82.27% 60.17% 61.27%\nHuman Action 98.27% 97.23% 97.81% 95.69%\nColor 87.94% 87.28% 82.75% 80.82%\nSpatial Relationship 75.21% 78.42% 67.89% 67.23%\nScene 49.84% 49.41% 51.55% 54.13%\nAppearance Style 20.95% 28.45% 23.53% 30.29%\nTemporal Style 26.35% 27.30% 25.04% 31.26%\nOverall Consistency 23.65% 25.53% 26.43% 24.28%\nSemantic Score 84.70% 89.11% 70.03% 69.26%\nQuality Score 65.58% 73.58% 60.83% 72.11%\nFinal Score 75.14% 81.34% (+6.20%) 65.43% 70.69% (+5.26%)\nIn Sec. 4.2, we reported the aggregated VBench [ 16] metrics, as well as specific metrics that\ncorrespond to motion coherence and magnitude. Here, we provide the complete breakdown across all\n16 individual evaluation dimensions for both Wan2.1 and CogVideoX.\nTab. 3 compares the baseline models with their FlowMo-guided counterparts. FlowMo leads to\nconsistent improvements in key dimensions, including Subject Consistency ,Motion Smoothness ,\nandObject Class , while maintaining or slightly improving aesthetic and perceptual metrics such as\nAesthetic Quality ,Appearance Style , and Spatial Relationship . Although a small decrease is observed\ninDynamic Degree , this aligns with our expectation that reducing motion artifacts also reduces\nspurious motion.\nCritically, as mentioned in the main text, FlowMo consistently and significantly boosts the overall\nquality metric ( Final Score ) by at least 5% across all models. This is a clear indication of the positive\nimpact our method has on the overall quality of the produced viseos.\n22",
  "text_length": 63518
}