{
  "id": "http://arxiv.org/abs/2506.00974v1",
  "title": "Camera Trajectory Generation: A Comprehensive Survey of Methods,\n  Metrics, and Future Directions",
  "summary": "Camera trajectory generation is a cornerstone in computer graphics, robotics,\nvirtual reality, and cinematography, enabling seamless and adaptive camera\nmovements that enhance visual storytelling and immersive experiences. Despite\nits growing prominence, the field lacks a systematic and unified survey that\nconsolidates essential knowledge and advancements in this domain. This paper\naddresses this gap by providing the first comprehensive review of the field,\ncovering from foundational definitions to advanced methodologies. We introduce\nthe different approaches to camera representation and present an in-depth\nreview of available camera trajectory generation models, starting with\nrule-based approaches and progressing through optimization-based techniques,\nmachine learning advancements, and hybrid methods that integrate multiple\nstrategies. Additionally, we gather and analyze the metrics and datasets\ncommonly used for evaluating camera trajectory systems, offering insights into\nhow these tools measure performance, aesthetic quality, and practical\napplicability. Finally, we highlight existing limitations, critical gaps in\ncurrent research, and promising opportunities for investment and innovation in\nthe field. This paper not only serves as a foundational resource for\nresearchers entering the field but also paves the way for advancing adaptive,\nefficient, and creative camera trajectory systems across diverse applications.",
  "authors": [
    "Zahra Dehghanian",
    "Pouya Ardekhani",
    "Amir Vahedi",
    "Hamid Beigy",
    "Hamid R. Rabiee"
  ],
  "published": "2025-06-01T11:58:25Z",
  "updated": "2025-06-01T11:58:25Z",
  "categories": [
    "cs.CV",
    "cs.MM"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00974v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00974v1  [cs.CV]  1 Jun 2025Camera Trajectory Generation: A Comprehensive Survey of Methods,\nMetrics, and Future Directions\nZAHRA DEHGHANIAN, Sharif University of Technology, Iran\nPOUYA ARDEKHANI, Sharif University of Technology, Iran\nAMIR VAHEDI, Sharif University of Technology, Iran\nHAMID BEIGY, Sharif University of Technology, Iran\nHAMID R. RABIEE, Sharif University of Technology, Iran\nCamera trajectory generation is a cornerstone in computer graphics, robotics, virtual reality, and cinematography, enabling\nseamless and adaptive camera movements that enhance visual storytelling and immersive experiences. Despite its growing\nprominence, the field lacks a systematic and unified survey that consolidates essential knowledge and advancements in this\ndomain. This paper addresses this gap by providing the first comprehensive review of the field, covering from foundational\ndefinitions to advanced methodologies. We introduce the different approaches to camera representation and present an in-\ndepth review of available camera trajectory generation models, starting with rule-based approaches and progressing through\noptimization-based techniques, machine learning advancements, and hybrid methods that integrate multiple strategies.\nAdditionally, we gather and analyze the metrics and datasets commonly used for evaluating camera trajectory systems,\noffering insights into how these tools measure performance, aesthetic quality, and practical applicability. Finally, we highlight\nexisting limitations, critical gaps in current research, and promising opportunities for investment and innovation in the field.\nThis paper not only serves as a foundational resource for researchers entering the field but also paves the way for advancing\nadaptive, efficient, and creative camera trajectory systems across diverse applications.\nAdditional Key Words and Phrases: Camera Trajectory Generation, Automatic Camera Control, Virtual Cinematography\nACM Reference Format:\nZahra Dehghanian, Pouya Ardekhani, Amir Vahedi, Hamid Beigy, and Hamid R. Rabiee. 2025. Camera Trajectory Generation:\nA Comprehensive Survey of Methods, Metrics, and Future Directions. 1, 1 (June 2025), 66 pages. https://doi.org/10.1145/\nnnnnnnn.nnnnnnn\n1 INTRODUCTION\nVirtual cinematography involves the cinematic projection of scenes occurring in a 3D graphical environment onto\na flat screen, with a virtual camera serving the role of a physical one. A key component of virtual cinematography\nis camera trajectory generation. It is a pivotal area of research in computer graphics, robotics, virtual reality,\nand cinematography [Elson and Riedl 2007; Pandya et al .2014]; where precise and adaptive camera movements\nsignificantly enhance user experiences and address both aesthetic and practical demands. Informally, camera\ntrajectory refers to the continuous path a camera follows in three-dimensional space, encompassing its position,\nAuthors’ addresses: Zahra Dehghanian, Sharif University of Technology, Iran, zahra.dehghanian97@sharif.edu; Pouya Ardekhani, Sharif\nUniversity of Technology, Iran, pouya.ardehkhani02@sharif.edu; Amir Vahedi, Sharif University of Technology, Iran, amir.vahedi123@\nsharif.edu; Hamid Beigy, Sharif University of Technology, Iran, beigy@sharif.edu; Hamid R. Rabiee, Sharif University of Technology, Iran,\nrabiee@sharif.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first\npage. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n©2025 Association for Computing Machinery.\nXXXX-XXXX/2025/6-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 2 ---\n2 •Dehghanian et. al.\norientation, and motion over time [Liu et al .2024c]. The formal definition is provided in Section 2. This process\nentails designing and calculating camera paths by integrating mathematical models, computational methods, and\naesthetic principles, ensuring the motion is seamless, adaptable, and purpose-driven within dynamic settings.\nHistorically, camera trajectory generation has evolved from basic rule-based systems [Christie and Olivier 2009;\nHe et al .1996] rooted in traditional cinematographic principles to sophisticated, data-driven models that integrate\nmachine learning and real-time adaptability [Burg et al .2021]. This evolution has been driven by the increasing\ndemands for computational efficiency, dynamic scene responsiveness, and aesthetic coherence across both virtual\nand real-world contexts. Various representation and modeling approaches for camera trajectory generation,\nsuch as the 7-degree-of-freedom (7-DOF) framework [Chr [n. d.]], Toric space [Lino and Christie 2015], and\ndrone-specific adaptations [Galvane et al .2018], have been proposed to represent the camera in distinct ways.\nEach approach offers specific advantages and limitations, rendering them suitable for particular applications\ndepending on factors such as flexibility, computational efficiency, and the specific requirements of the given task.\nRecent advancements, including the application of deep learning and emerging trends like diffusion models, have\nfacilitated the development of adaptive and context-aware systems, significantly enhancing the capabilities of\ncamera trajectory generation [Massaglia 2023]. Beyond its technical contributions, camera trajectory generation\nhas broad practical applications, spanning autonomous drones [Nägeli et al .2017b], surveillance systems [Fiengo\net al. 2006], gaming [Burelli and Yannakakis 2011], and film production [Yang et al. 2024].\nWhile these advancements have significantly enhanced virtual cinematography, challenges persist. These\ninclude the seamless integration of computational, perceptual, and aesthetic constraints, which are crucial for\nfurther improving user immersion and visual experiences, visual storytelling, and the adaptability of camera\nsystems in dynamic scenarios. By aligning artistic vision, technical precision, and user-focused design, research\nin camera trajectory generation bridges technology and art, offering solutions to real-world challenges while\nelevating creative practices.\nA notable gap in the current body of research is the absence of a comprehensive survey that consolidates\nthe diverse methodologies and techniques proposed in this field. To address this, we present a detailed survey\nthat unifies foundational principles, state-of-the-art (SOTA) methodologies, and cutting-edge advancements.\nIt focuses on the theoretical and methodological advancements in camera trajectory generation, emphasizing\nSOTA techniques and foundational principles. The research spans diverse applications in computer graphics,\nvirtual reality, robotics, and cinematography By analyzing research from the past 20 years, it synthesizes key\nmethodologies, emerging trends, and unresolved challenges to guide future innovation.\nWe systematically reviewed related work from reputable sources, including peer-reviewed journals, conference\nproceedings, and technical reports, using academic databases such as IEEE Xplore, ACM Digital Library, and\nSpringerLink with keywords ’camera trajectory generation,’ ’automatic camera control,’ and ’virtual cinematog-\nraphy’ to ensure wide-ranging coverage. This method facilitated a comprehensive integration of theoretical\nadvancements and practical applications across diverse fields.\nThe remainder of this paper is organized as follows. Section 2 examines camera trajectory representation\nframeworks across three abstraction levels, addressing trade-offs between usability and precision while highlight-\ning strategies for balancing expressiveness, computational efficiency, and user-system compatibility. Section 3\nfocuses on camera movement systems and their integration with computational frameworks. Section 4 discusses\ntrajectory generation techniques, emphasizing real-time adaptability and aesthetic considerations. Section 5\nreviews evaluation metrics, ranging from quantitative measures to qualitative assessments, while Section 6\nsurveys key datasets and their contributions to the field. Section 7 synthesizes findings and identifies open\nresearch challenges, paving the way for future advancements. Finally, the conclusion summarizes key insights\nand underscores the significance of continued innovation in camera trajectory generation.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 3 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •3\n2 REPRESENTATION\nCamera trajectory generation involves creating a shot, or a sequence of shots, that form a scene under specific\nconstraints. These constraints must be translated into a unique set of camera parameters specifying its position,\norientation, and movement over time [Zhang 2021c]. Managing these parameters, in addition to time, is tedious\nand overly complex for non-technical users. Utilizing high-level descriptions, such as natural language-like shot\nannotations, offers a more accessible and user-friendly way for non-experts to specify constraints compared to\nmanually managing precise camera parameters like position and orientation over time.\nCamera intrinsics including focal length, focal distance, aperture, and camera extrinsics including position\nand orientation are critical parameters in camera modeling and image formation [Zhang 2021c]. Focal length\ndetermines the magnification and field of view of a camera lens, while focal distance refers to the distance\nbetween the lens and the focused subject. Aperture controls the amount of light entering the lens and affects\ndepth of field [Zhang 2021a]. Extrinsic parameters define the camera’s position and orientation relative to a\nworld coordinate system [Zhang 2021b], whereas intrinsic parameters describe the internal characteristics of\nthe camera, such as focal length and principal point [Zhang 2021a]. Together, these parameters enable precise\ncamera calibration and projection modeling\nThe constraint representation should be as compact and expressive as possible, capable of covering all existing\nand potential scenarios. A key challenge lies in establishing a one-to-one correspondence between the intermediate\nrepresentation and precise camera parameters. At higher abstraction levels, certain details might be omitted,\nleading to ambiguity where a single representation could correspond to multiple parameter configurations.\nSeveral works have addressed automating the parameter retrieval process, contributing the automatic conversion\nof shot annotations into fully realized shots [Louarn et al. 2018, 2020; Ronfard et al. 2015].\nWe can categorizes representations into three levels of abstraction First, high-level representations use natural\nlanguage for intuitive descriptions. Second, mid-level representations rely on structured formal languages. Third,\nlow-level representations employ precise mathematical definitions for detailed control. There is an inherent trade-\noff between the expressiveness and usability of camera trajectory representations and their ease of conversion\ninto precise camera parameters. As the level of abstraction moves closer to natural language, the representation\nbecomes easier to use and more intuitive for non-specialists [Liu et al .2024b]. However, this increased accessibility\noften comes at the cost of precision and the complexity of converting the representation into an accurate\ncamera trajectory. Conversely, lower-level representations provide a higher degree of precision and are more\nstraightforward to translate into real camera parameters but are harder for humans to understand and use\n[Christie et al .2008; Galvane et al .2015a; Ronfard et al .2015]. Striking the right balance between ease of use\nand technical rigor is essential for designing representations that meet the needs of both human users and\ncomputational systems.\nThese levels of abstraction will be further elaborated upon in the subsequent sections. The completeness and\nparameter retrieval of each abstraction level are also examined.\n2.1 High-Level Natural Language Representation\nHigh-level natural language representation refers to employing natural language descriptions to specify cam-\nera trajectories in an intuitive and accessible manner. This approach leverages the expressiveness of human\nlanguage to allow users, including non-technical ones, to define constraints and desired outcomes for camera\nmovements without requiring direct manipulation of complex mathematical parameters or low-level settings.\nRecent advancements in the field of large language models (LLMs) have significantly enhanced their capacity to\nunderstand natural languages, leading to notable achievements such as LLaMA 3, GPT-4o, and Gemini 1.5 [Dubey\net al.2024; Hurst et al .2024; Reid et al .2024]. One promising approach involves utilizing high-level natural\nlanguage descriptions to generate desired camera trajectories, anticipating that the system will create these\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 4 ---\n4 •Dehghanian et. al.\ntrajectories in virtual or real environments based on the constraints specified in the linguistic descriptions. While\nthe expressiveness of natural languages ensures the completeness of this approach, retrieving exact parameters\nremains challenging due to the complex nature of language comprehension by computers. This challenge can be\nmitigated by leveraging emerging LLMs [He et al. 2024; Liu et al. 2024b].\nThe ChatCam model [Liu et al .2024b] is an example from this family of approaches, aiming to enable camera\ncontrol through natural language interactions. The approach employs CineGPT, a GPT-based autoregressive\nmodel, for text-conditioned camera trajectory generation, complemented by an Anchor Determinator for precise\ntrajectory placement.\nAlso, CameraCtrl [He et al .2024], a plug-and-play module enables precise camera control in text-to-video\ngeneration by using this representation. These module can integrate with existing video diffusion models, such\nas AnimateDiff [Guo et al. 2023], without affecting frame quality or temporal consistency.\nHou et al. [Hou et al .2024] introduce CamTrol, a training-free framework for camera control in video diffusion\nmodels. The approach leverages 3D point cloud representations for explicit camera motion modeling and employs\nnoise layout priors to guide video generation.\n2.2 Mid-Level Shot Annotation Representation\nA formal language offers an alternative approach to representing camera trajectories, providing a structured and\nrule-based method for defining descriptions and restricting the descriptions to adhere to this language, instead of\nrelying on high-level natural language. The completeness of this approach highly depends on the formal language\nused to describe the constraints. On the other hand, because we are dealing with formal language, there is a\nformal grammar representing the language, thus shot annotations can be easily derived from the grammar to\nretrieve the parameters easily and quickly [Bares et al. 2000; Liang et al. 2012; Louarn et al. 2018, 2020; Ronfard\net al.2015; Van Rijsselbergen et al .2009]. Most contributions in this category focus on linguistic specifications\nfor generating camera trajectories, primarily utilizing mid-level shot annotations that are later translated into\nfully realized shots.\nThe Movie Script Markup Language (MSML) [Van Rijsselbergen et al .2009] is a camera specification language\ndesigned to provide a structured format for screenplay narratives in television and film production. It incorporates\ntiming and animation models for synchronization and production control and uses XML serialization. Developed in\ncollaboration with industry professionals, MSML has been implemented in proof-of-concept systems, showcasing\nits applicability to practical scenarios.\nThe Prose Storyboard Language (PSL) [Ronfard et al .2015] is a method designed for annotating movie shots\nusing a formal context-free language and its associated grammar. PSL enables the structured annotation of shots,\nproviding a systematic approach to describing scenes through a well-defined formal language. The grammar of\nPSL forms an AND-OR tree, as illustrated in Figure 1.\nAny sentence in PSL must adhere to the same grammar. Like any formal grammar, there are multiple terminals\nand non-terminals. Terminals in PSL are divided into two categories: generic terminals and specific terminals.\nGeneric terminals include terms such as “pan,” “dolly,” and “enter.” Specific terminals include character names,\nplaces, and objects. Non-terminals consist of categories of shots, image composition, image development, and\nother elements.\nTo describe an entire movie, a unique PSL sentence is assigned to each shot. Every PSL sentence address two\nproperties of the shot: spatial structure and temporal structure. Spatial structure focuses on the composition of an\nindividual movie frame, while temporal structure captures events in a sequence of frames. Therefore, each shot\ncan be described with a complete PSL sentence that includes at least one composition and an arbitrary number of\nscreen events. An example of PSL description is shown in Figure 2\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 5 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •5\nFig. 1. Tree representation of the PSL grammar [Ronfard et al. 2015].\nFig. 2. Prose storyboard language description of two iconic shots in Alfred Hitchcock’s North By Northwest [Ronfard et al .\n2015].\nThe Prose Storyboard Language (PSL) is intended to represent a director’s vision by providing a method for\nannotating shots across pre-production, production, and post-production stages [Ronfard et al .2015]. PSL allows\nfor describing existing movies as an ordered sequence of sentences, one per shot, enabling parameter retrieval\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 6 ---\n6 •Dehghanian et. al.\nbased on its formal grammar. While the structured nature of PSL simplifies parameter retrieval, the absence of a\nsystematic approach for extracting parameters from PSL sentences is identified as a limitation.\nFollowing PSL, Film Editing Patterns (FEP) [Wu et al .2018] is a language designed to formalize film editing\npractices, supporting virtual cinematography by encoding constraints on elements such as shot size, angle, and\nactor positioning. The framework facilitates automated style analysis and prototyping of creative 3D sequences.\nEvaluations involving professionals and amateurs suggest that FEP is particularly useful for novice users, providing\npedagogical and practical benefits. However, the framework’s flexibility for expert users is limited, and there is\npotential for enhancing editing functions and enabling more customizable patterns.\nEven though both PSL and FEP are utilized for shot creating, they differ significantly in their methodology\nand focus. The FEP language emphasizes cinematographic visual properties, such as shot sizes, angles, and actor\nlayouts, to formalize film editing techniques and improve creative workflows in 3D animation by encoding\nstylistic patterns (e.g., intensify, opposition) and their application in editing tools [Wu et al .2018]. Meanwhile,\nthe PSL adopts a descriptive syntax to provide structured, human-readable annotations for each shot, capturing\nspatial and temporal structures, with particular attention to shot development and transitions. PSL enables a\nmore granular representation of events and compositions, catering to both manual annotation and machine\ninterpretation [Ronfard et al. 2015].\nLouarn et al. proposed an extension of the Prose Storyboard Language (PSL) to facilitate automated staging in\nvirtual cinematography [Louarn et al .2018]. The extension introduces enhancements such as camera identifi-\ncation, enabling the specification of complex constraints involving multiple cameras, and scene identification,\nwhich supports the description of continuity constraints for character and camera placement and orientation.\nAdditionally, it incorporates three generic terminals—entity, object, and region—along with associated constraints,\nexpanding PSL’s expressive capacity for representing and staging complex scenes.\nThe extended PSL representation has been applied to automate camera staging in 3D virtual environments\nthrough pruning the Potential Location-Rotation Set (PLRS) [Louarn et al .2018]. By incorporating additional\nfeatures into the traditional PSL, the extended language accommodates a broader range of constraints. However,\nthe system faces limitations, including restricted support for multiple target constraints and challenges in dynamic\nscene handling. It is currently limited to constraints between two entities and requires further development to\neffectively express complex cinematographic rules and evaluate constraints over time for moving entities.\nIn subsequent work, Louarn et al. utilized the same extended PSL for interactive staging and shooting in\nvirtual cinematography [Louarn et al .2020]. They introduced a system that takes a 3D virtual environment and\nconstraint specifications in extended PSL as inputs, then selects the position and orientation of entities in the\nscene as output. The system operates in a loop of three stages.\nThe process involves three key stages: the Pruning Stage refines each entity’s PLRS using a Geometric Pruning\nOperator, producing a dependency graph. The Elicitation Stage utilizes this graph and each entity’s domain\nto generate candidate solutions by sampling within specified constraints. In the Interactive Stage, users can\nmodify entities and navigate the environment, triggering a new elicitation phase to ensure updated solutions\nmeet requirements. This approach’s advantage is its interactive capability, absent in prior methods. However,\nit regenerates the dependency graph with each interaction, disrupting solution continuity. Additionally, like\nother constraint-based methods, it struggles to identify conflicting constraints when a solution cannot be found,\nlimiting its effectiveness in such cases.\n2.3 Low-Level Mathematical Representation\nAt the lowest level of abstraction, camera trajectories can be described using mathematical representations.\nMethods such as 7-DOF [Chr [n. d.]] and Toric space [Christie et al .2008] can be employed to provide precise and\nmathematically sound descriptions of camera movements. These approaches ensure accuracy and rigor, making\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 7 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •7\nthem ideal for scenarios requiring fine-grained control over camera behavior. In the following subsection, we\nwill delve into the details of these methods, exploring their principles, applications, and limitations.\n2.3.1 7-DOF Modeling. Camera modeling in computer graphics often aims to address the challenges of dynamic\nenvironments and precise visual representation [Chr [n. d.]]. One of the most well-known and widely used\nlow-level representations is the 7-DOF model [Chr [n. d.]], which includes three parameters for Cartesian\ncoordinates(𝑥𝑐,𝑦𝑐,𝑧𝑐)[Stewart 2012], three Euler angles (𝜙𝑐,𝜃𝑐,𝜓𝑐)[Foley 1996], and one intrinsic parameter\nfor the field of view 𝛾𝑐[Hartley and Zisserman 2003], as shown in Figure 3. This approach was motivated by\nthe complexity of ensuring accurate camera placement while accommodating constraints like occlusion and\nmotion in multidimensional datasets. Occlusion constraints are designed to ensure that critical elements in a\nscene remain visible and are not blocked by other objects. Motion constraints ensure that the camera’s movement\nis smooth and logical, especially in dynamic scenes where objects or the environment may change over time.\nBy modeling the camera with these degrees of freedom, the authors aimed to create a flexible framework for\nvisualization and multimodal systems [Eisenhauer 2008].\nFig. 3. A simple camera model based on Euler angles; tilt ( 𝜙), pan (𝜃), and roll (𝜓) [Chr [n. d.]].\nBy explicitly accounting for relationships between visual elements, spatial configurations, and user perspectives,\nthe framework surpasses conventional models in adaptability and precision, dynamically maintaining visual\ncoherence and contextual alignment in complex, interactive systems [Chr [n. d.]]. This adaptability is achieved\nthrough a mathematical representation that transforms world coordinates into a local camera basis, as shown in\nEquation 1:\n\u0012𝑥′\n𝑦′\u0013\n=𝑃(𝛾𝑐)·𝑇(𝑥𝑐,𝑦𝑐,𝑧𝑐)·𝑅(𝜙𝑐,𝜃𝑐,𝜓𝑐)·©­­­\n«𝑥\n𝑦\n𝑧\n1ª®®®\n¬, (1)\nwhere𝑥′,𝑦′are the projected coordinates on the 2D screen, and (𝑥,𝑦,𝑧)represent the object’s 3D coordinates\nin the world space. Here, 𝑅incorporates the Euler angles, 𝑇translates the camera’s position, and 𝑃adjusts the\nprojection based on the field of view.\nThe 7-DOF camera model excels in flexibility and precision, using its degrees of freedom in position, orientation,\nand field of view to address challenges like occlusion avoidance and aligning visual elements with linguistic\nreferences. By dynamically positioning the camera to maintain unoccluded views and accurately linking spatial\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 8 ---\n8 •Dehghanian et. al.\nconfigurations with linguistic descriptors, it proves invaluable for multimodal The 2D manifold representation\nrevolutionizes camera composition by transforming the problem into an efficient algebraic framework. This\nframework represents the solution space as a spindle torus, a specific type of toroidal surface characterized by\nits unique topology and geometry. The spindle torus arises naturally in problems where a point or subject is\nconstrained by angles and distances relative to a central axis or plane, such as in camera positioning for visual\ncomposition.\n2.3.2 Spherical Surface. As shown in Figure 4, this approach enables smooth transitions between initial and\nfinal camera configurations while preserving framing constraints [Galvane et al .2015b]. The uniqueness lies in\nits algebraic simplicity and ability to handle single-target configurations effectively, which is particularly useful\nin scenarios requiring precise tracking of a single moving subject.\nFig. 4. Spherical surface used to model a camera for single-target configurations, showing the character’s vantage angles\n(𝜃,𝜙)in spherical coordinates [Galvane et al. 2015a].\nThe spherical surface model’s primary advantage is its computational efficiency, as it reduces the complexity of\ndetermining optimal camera positions for single-target tracking. Additionally, it facilitates smoother transitions\ncompared to more generalized manifold surfaces. However, a notable limitation is its restriction to single-character\nscenarios, as it cannot handle interactions or occlusion with multiple targets. This limitation makes it less suitable\nfor more dynamic or multi-character environments.\nIn summary, the drone-specific spaces offers a tailored approach for aerial cinematography but faces challenges\nin balancing computational efficiency with the demands of dynamic drone operation, particularly in cluttered or\nrapidly changing environments. Future work could involve developing adaptive algorithms that dynamically adjust\nsafety parameters based on environmental inputs or using predictive control models for smoother transitions\nbetween camera configurations. Exploring lightweight neural network models for real-time decision-making and\ncollision avoidance could further enhance the utility and flexibility of this method in drone cinematography.\n2.3.3 Toric Space. The concept of 2D manifolds has revolutionized camera composition by reframing it as an\nalgebraic problem, enabling more efficient solutions [Christie et al .2008]. This method models the solution space\nas a spindle torus, a distinctive toroidal structure with unique geometrical and topological features. The spindle\ntorus naturally emerges in scenarios where a point or object is constrained by angular and distance parameters\nrelative to a central plane or axis, which is particularly relevant in tasks like camera positioning for composition.\nWithin this framework, the spindle torus is described using angular parameters 𝜙and𝜃, forming a continuous\nsurface that represents potential camera configurations adhering to fixed distance and alignment constraints\nwith respect to the subject. This organized representation streamlines the process of identifying optimal camera\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 9 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •9\nparameters, eliminating the need for computationally heavy iterative approaches [Christie et al .2008]. Unlike\ngeneral-purpose 7-DOF methods, which are applicable in environments without predefined targets, Toric spaces\nrely on the presence of targets for functionality. This target dependency facilitates precise subject placement within\nthe frame by leveraging the geometrical properties of the spindle torus, significantly lowering computational\ndemands. Moreover, the algebraic model tackles Blinn’s spacecraft problem [Blinn 1988] by optimizing camera\norientation and positioning under constraints like fixed distance and direction. Such methods are crucial for\napplications that demand detailed and efficient visual composition.\nIn the 2D manifold representation model, the camera position 𝑃𝜙,𝜃is parameterized by two angles: 𝜙, defining\nthe vertical plane, and 𝜃, defining the arc within this plane. The relationship is mathematically expressed in\nEquation 2.\n𝑃𝜙,𝜃=(𝑞𝜙·®𝐼𝑂0)+®𝐼, (2)\nwhere𝑞𝜙represents the rotation by 𝜙radians around the axis ®𝐴𝐵,®𝐼𝑂0is the vector connecting the midpoint\n®𝐼) to the center of the inscribed circle ®𝑂0(specifically for 𝜙=0), and®𝐼is the midpoint of the segment joining\nthe two subjects. Here, ®𝐼and®𝑂0are not parameters but derived entities based on the geometric configuration:\n®𝐼is explicitly the midpoint of segment ®𝐴𝐵, and®𝑂0is the center of the inscribed circle determined by the 2D\nmanifold constraints. This representation encapsulates all feasible camera positions that satisfy the exact on-screen\nprojection constraints.\nBy reducing the search space from six dimensions to two (2-DOF), the method significantly lowers computa-\ntional costs, making it highly efficient for real-time and complex environments. Its parametric nature supports\nintegrating visual properties like vantage angles and object sizes, enhancing versatility. However, its focus on\nexact on-screen compositions may limit flexibility in scenarios with broader or competing constraints [Christie\net al. 2008].\nThe Toric space model is a generalization of the 2D manifold representation [Christie et al .2008] into a\nthree-dimensional search space [Lino and Christie 2015] defined by the triplet of Euler angles (𝛼,𝜃,𝜙)describe\nhorizontal and vertical angles around the targets. This representation simplifies the camera control problem by\nreducing a 7-DOF search space to a 4-DOF space for scenarios involving two targets. Using this model, any camera\npositioned on this manifold can view the two targets with specified on-screen compositions. The conversion of a\ncamera’s Toric representation 𝑇(𝛼,𝜃,𝜙)to its Cartesian representation 𝐶(𝑥,𝑦,𝑧)is given by the Equation 3.\n𝐶=𝐴+(𝑞𝜙·𝑞𝜃·𝐴𝐵)·sin(𝛼+𝜃/2), (3)\nwhere𝑞𝜙and𝑞𝜃are quaternions representing rotations by 𝜙and𝜃respectively. Quaternions are a mathematical\ntool for representing 3D rotations. They are defined as a set of four numbers 𝑞=(𝑤,𝑥,𝑦,𝑧), where𝑤is the scalar\npart, and𝑥,𝑦,𝑧 form the vector part .The vector 𝐴𝐵is derived from the difference in the positions of the two\ntargets, and 𝐴corresponds to the location of the first target. As shown in Figure 5, this model provides a compact\nand computationally efficient means of defining camera placement while maintaining visual properties.\nThe Toric space model was developed to overcome limitations in earlier camera control frameworks, such as\ntheir reliance on exact on-screen positioning and inefficiencies in handling soft framing [Christie et al .2008].\nBy reducing the complexity of the search space and enabling rapid computation of camera positions, the Toric\nspace provides a more versatile approach to virtual camera control. It directly incorporates visual properties\nlike vantage angles (relative viewing angle around a target, defined by a reference direction and a permissible\ndeviation, used to specify the desired orientation of a camera toward the target.), target sizes, and on-screen\npositions within its parameterization, addressing many challenges of prior models. However, its reliance on\npoint-based target representations restricts its ability to manage occlusion or complex multi-target relationships\n[Lino and Christie 2015]. Extending the model to include occlusion-aware strategies or adaptive parameterization\ncould improve its applicability in diverse scenarios. A line of research for future extension, may integrate machine\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 10 ---\n10 •Dehghanian et. al.\nFig. 5. Representation of the Toric space. The manifold is parametrized by (𝛼,𝜃,𝜙), defining camera positions around two\ntargets [Lino and Christie 2015].\nlearning-based predictive models for dynamic framing or combine the Toric space with real-time depth analysis\nto enhance its effectiveness in intricate virtual environments.\n2.3.4 Drone Toric Space. Unlike static or ground-based camera setups, drones operate in three-dimensional\nairspace and must account for different factors. These complexities demand a specialized framework that not only\nensures compliance with cinematographic principles but also integrates the physical realities of drone navigation\n[Galvane et al .2018]. The Drone Specific Space addresses these challenges by extending conventional camera\nmodels with additional parameters tailored to the specific requirements of drone cinematography, offering a\nrobust solution for dynamic and aerial filming scenarios.\nThe Drone Toric Space (DTS) extends the Toric space model to address the unique requirements of cinemato-\ngraphic drone control because it builds upon the foundational principles of the Toric Space while incorporating\nadditional considerations for drone-specific constraints. It introduces a 7D parameterization 𝑞(𝑥,𝑦,𝑧,𝜌,𝛾,𝜓,𝜆),\nwhere(𝑥,𝑦,𝑧)denotes the drone’s position in Cartesian space, (𝜌,𝛾,𝜓)are the Euler angles for roll, pitch, and yaw,\nand𝜆defines the gimbal tilt. This model integrates physical constraints like collision avoidance and minimum\nsafety distances with cinematographic principles such as framing and smooth transitions, ensuring physically\nfeasible and visually coherent drone movements [Galvane et al. 2018].\nFigure 6 demonstrates this configuration, highlighting how safety and physical constraints are embedded.\nUnlike the Toric space, the DTS incorporates collision avoidance by enforcing a minimum safety distance around\ntargets and maintaining feasible trajectories through dynamic path planning. This ensures physical safety while\naccommodating real-time cinematographic adjustments.\nThe DTS model introduces significant advancements for drone cinematography by offering predefined camera\nregions (e.g., external, apex) for framing targets dynamically, as illustrated in Figure 7. These regions help\nmaintain visual consistency while allowing smooth transitions between cinematic shots. The system also ensures\ncollision-free paths and adaptability to environmental changes, making it ideal for real-time filming of moving\ntargets. However, its complexity increases computational demands, and its reliance on fixed parameters like safety\ndistances may limit flexibility in highly dynamic or cluttered environments [Liu et al .2017]. Nevertheless, the\nDTS remains a robust solution for managing drone trajectories while balancing physical and visual constraints.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 11 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •11\nFig. 6. Drone configuration in the DTS model, showcasing its 7D parameterization [Galvane et al. 2018].\nFig. 7. Drone Toric Space parameterization, highlighting regions for camera positioning and framing [Galvane et al. 2018].\nTo adapt the Toric space framework for real-time environments, [Burg et al .2020] introduces several critical\nenhancements focused on computational efficiency and dynamic adaptability. Traditional Toric space methods\nfaced significant challenges in processing dynamic scenes, as visibility computations often relied on computa-\ntionally intensive ray-casting [Roth 1982] or static pre-computation [Oskam et al .2009], which made real-time\napplication impractical. The improvements in this work involve the use of GPU-accelerated techniques, such as\nshadow mapping [Everitt et al .2001; Williams 1978] and anisotropic blurring [Galvane et al .2015b], to compute\nvisibility and occlusion anticipation in Toric space efficiently. By utilizing GPU-based techniques, the system\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 12 ---\n12 •Dehghanian et. al.\ngenerates an \"anticipation map\" to predict occlusions within a specified time frame. This map, paired with a\nmotion model, enables dynamic camera adjustments that ensure smooth transitions, minimize visibility loss, and\nallow Toric space to function effectively in real-time, even in complex, highly occluded scenes.\n2.3.5 Plücker Coordinates. In this approach, a camera is represented using Plücker coordinates [Zhang et al .\n2024b], which describe it as a collection of rays instead of relying on conventional global parameters. Each ray\nis characterized by its direction and moment vectors, providing a flexible and detailed way to model cameras.\nThis representation supports over-parameterization, where additional variables enable modeling of both classical\nand non-perspective camera systems, including those with complex imaging geometries [Grossberg and Nayar\n2001; Schops et al .2020]. By assigning each pixel to a corresponding ray, the method effectively utilizes localized\nfeatures, offering greater granularity compared to traditional models.\nThe motivation for adopting this representation arises from the challenges posed by sparsely sampled views,\nwhere establishing reliable correspondences between image features is often difficult [Snavely et al .2006; Zhou\nand Tulsiani 2023]. By representing cameras as a collection of rays, this method complements transformer-based\narchitectures, which excel in set-level processing and patch-wise analysis [Dosovitskiy et al .2021]. Furthermore,\nthis approach naturally accommodates probabilistic modeling, an essential capability for addressing uncertainties\ninherent in sparse-view pose estimation tasks [Wang et al. 2023b].\nMathematically, the Plücker representation encodes each ray 𝑟as:\n𝑟=⟨𝑑,𝑚⟩, 𝑚 =𝑝×𝑑, (4)\nwhere𝑑∈R3is the direction vector, 𝑚∈R3is the moment vector, and 𝑝represents a point on the ray. The\nparameters 𝑑and𝑚ensure the ray remains agnostic to the choice of 𝑝. To compute the rays from a known\ncamera, the directions and moments are derived as:\n𝑑=𝑅⊤𝐾−1𝑢, 𝑚 =(−𝑅⊤𝑡)×𝑑, (5)\nwhere𝑅,𝑡, and𝐾denote the rotation matrix, translation vector, and intrinsics matrix of the camera, respectively\n[Zhang et al .2024b]. Term 𝑢represents the 2D pixel coordinates in the image plane. These coordinates are\ntypically expressed in normalized device coordinates (NDC) [Everitt 2001], scaled to fit within a specific range,\nsuch as[−1,1]or[0,1], depending on the application. Figure 8 illustrates the conversion between the classical\ncamera representation and the ray-based model.\nRepresenting a camera using Plücker coordinates introduces complexity and over-parameterization by modeling\nit as a bundle of rays. While this enables flexibility for diverse camera models, it demands intensive computation\nand complicates calibration. Converting these rays back to traditional parameters also involves optimization,\nwhich can reduce precision in applications needing high geometric accuracy [Zhang et al. 2024b].\n2.3.6 TUM Trajectory (3D Motion of Camera Over Time). The TUM camera trajectory format [Sturm et al .2012]\nis a standardized way to represent the movement of a camera through 3D space over time, often used in computer\nvision and robotics research. It captures both the position and orientation of the camera at each timestamp,\nusing a 7-element vector. This vector includes the 𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 in seconds (or frames), followed by the camera’s\ntranslation ( 𝑥,𝑦,𝑧coordinates) and its orientation represented as a quaternion ( 𝑞𝑥,𝑞𝑦,𝑞𝑧,𝑞𝑤).\nA quaternion is a mathematical term used to represent rotations in three-dimensional space, consisting of\nfour components: one real part and three imaginary parts. It is typically written as (6), where𝑤is the scalar\ncomponent, and 𝑥,𝑦,𝑧are the vector components. Quaternions are particularly useful because they offer several\nadvantages over other rotation representations, such as Euler angles. They help avoid issues like gimbal lock (the\nloss of one degree of freedom in a multi-dimensional mechanism at certain alignments of the axes) and allow\nfor smooth, continuous interpolation between orientations. In the case of the TUM camera trajectory format,\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 13 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •13\nFig. 8. Conversion process for Plücker coordinates [Zhang et al. 2024b].\nquaternions efficiently capture the camera’s orientation, providing a compact and stable way to describe rotations\nin 3D space without redundancy or ambiguity.\n𝑞=𝑤+𝑥𝑖+𝑦𝑗+𝑧𝑘 (6)\nThis compact format allows for a precise description of the camera’s trajectory, which is crucial for evaluating\nand comparing different motion estimation algorithms. Another key advantage is its utility in benchmarking and\nevaluating algorithms in areas like visual odometry [Aqel et al .2016], SLAM [Zhang et al .2021], and related fields,\nas it provides reliable ground truth data for comparing predicted camera trajectories. It is often used alongside\nRGB-D datasets, such as the TUM RGB-D dataset [Sturm et al. 2012], for more comprehensive evaluation.\n3 MOVEMENT SYSTEM\nCamera movement systems are essential in computer vision and graphics, defining how cameras are manipulated\nto capture scenes. The term \"camera movement\" refers to the types of motions that cameras can perform, enabling\ndiverse views of a scene [Christie and Olivier 2009]. These parameters collectively determine the position and\norientation of the camera in a 3D space. The specific type of camera movement directly impacts how trajectories\nare planned and optimized, as it influences both the setup and the design of the system. In this section, we explore\nthe most critical types of camera movement systems, emphasizing their characteristics and the importance of\nunderstanding camera setups for effective design and implementation.\nThese systems, whether in virtual or real-world environments, are classified as fixed or non-fixed. Fixed systems,\ncharacterized by stationary positions, are ideal for applications like surveillance or UAV monitoring, offering\nstability and simplified trajectory planning. Non-fixed systems, common in virtual environments, allow free\nmovement within a defined space, making them suitable for dynamic applications such as video games [Burelli\n2016]. In these games, non-fixed cameras adapt based on the perspective: first-person cameras synchronize\nwith the player’s position and orientation, while third-person cameras provide external views that can be free\nor constrained. Additionally, during non-interactive sequences, cameras focus on highlighting key narrative\nelements without player control.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 14 ---\n14 •Dehghanian et. al.\nIn the following subsections, we explore two specialized types of camera movement systems: Pan-Tilt-Zoom\n(PTZ) cameras and Gimbal-Mounted cameras. The first subsection focuses on PTZ systems, which enable dynamic\nadjustments in horizontal (pan), vertical (tilt), and focal length (zoom) movements, making them highly effective\nfor real-time applications such as surveillance and broadcasting. The second subsection examines gimbal-mounted\ncameras, which leverage gyroscopic feedback and motorized stabilization to maintain smooth and steady imaging,\nparticularly in UAV applications. These specialized systems showcase unique capabilities that cater to specific\nscenarios requiring precise control and adaptability in camera movement.\n3.1 Pan-Tilt-Zoom Camera\nThe pan-tilt-zoom (PTZ) camera movement system is useful particularly in scenarios where a fixed camera is\nemployed. This system facilitates three primary motions: pan, tilt, and zoom, as depicted in Figure 9.\nFig. 9. Camera motion of fixed PTZ Cameras [Bak and Park 2023].\nPan refers to the horizontal rotation of the camera, enabling the tracking of objects moving laterally within\na scene [Vineyard 2008]. This movement that the subject remains within the frame during dynamic scenarios,\nsuch as sports events or live performances [Chen and Carr 2015; Zhu et al .2009]. Similarly, tilt involves vertical\nrotation of the camera, which allows for capturing objects moving along the vertical axis or for emphasizing\ntowering structures or high-angle perspectives .\nZoom, on the other hand, adjusts the focal length of the camera lens to magnify or reduce the size of the\nsubject in the frame. This capability is often used to create emotional or dramatic tension by directing the\nviewer’s attention to specific elements of the scene [Brown 2012; Vineyard 2008]. By integrating these motions,\nPTZ cameras offer a flexible approach to trajectory generation, as the system’s operations are computationally\nlightweight and suitable for real-time adjustments in applications such as surveillance [Kumar et al .2009],\nbroadcasting [Chen and Carr 2015], and cinematography [Pattanayak et al. 2024].\nCompared to non-fixed camera systems like boom or truck movements, as illustrated in Figure 10, PTZ cameras\noffer a simpler yet effective approach for generating diverse trajectories. Truck movements shift the field of\nview laterally, useful for dynamic tracking shots, while boom movements provide vertical adjustments for varied\nperspectives [Brown 2012]. Although these non-fixed motions are valuable in cinematic contexts, the rotational\nand zoom capabilities of PTZ systems serve as a compact and versatile alternative for achieving complex camera\ntrajectories without requiring physical relocation [Vineyard 2008].\n3.2 Gimbal Mounted Camera\nGimbal-mounted camera systems are widely used in unmanned aerial vehicles (UAVs) to stabilize and control\ncamera movement during flight. These systems typically consist of a motorized structure that allows adjustments\nin two key directions: yaw (horizontal rotation) and pitch (vertical tilt), as shown in Figure 11.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 15 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •15\nFig. 10. Camera motion of non-fixed PTZ Cameras [Bak and Park 2023].\nFig. 11. Overview of yaw-pitch gimbal [Cong Danh 2021].\nThe camera is integrated within the gimbal, with its lens oriented outward, enabling precise control over its\nmovement and stabilization. However, this design introduces challenges, such as an unbalanced mass due to the\ninclusion of the camera. This imbalance directly affects the pitch angle, making it a critical parameter to optimize\nfor smooth operation and stability.\nGimbal systems integrate gyroscopes to measure movement speeds and interact with motor torque, creating a\ncontrol loop that stabilizes camera movements and minimizes disturbances [Cong Danh 2021]. This motor and\ngimbal integration ensures smooth operation, but certain design limitations persist. For instance, the camera\nframe is obscured at pitch angles beyond 120 degrees, and images invert at negative pitch angles (less than 0\ndegrees), as shown in Figure 12 [Cong Danh 2021]. These constraints demand precise calibration to maintain\nproper image orientation and smooth, blur-free camera motion, highlighting the need for responsive and accurate\ncontrol systems.\nGimbal-mounted camera systems are particularly valued in UAVs for their ability to maintain image stability\nduring rapid or irregular movements. The combination of precise gyroscopic feedback, motorized control, and\ncareful pitch angle calibration ensures high-quality imaging in dynamic aerial environments, making these\nsystems indispensable for UAV applications.\n4 ALGORITHM\nAlgorithms are essential for generating precise and efficient camera trajectories across applications like cinematog-\nraphy, graphics, and robotics [Bonatti et al .2020b; Gebhardt and Hilliges 2021]. By automating trajectory planning,\nthey address challenges such as complex environments, computational efficiency, and real-time constraints [Burg\net al.2020, 2021; Nägeli et al .2017a]. Bridging artistic principles with technology, algorithms enhance storytelling,\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 16 ---\n16 •Dehghanian et. al.\nFig. 12. Pitch angle limit [Cong Danh 2021].\nuser immersion, and visual coherence. Advances in rule-based, optimization, and learning-based methods have\nexpanded the capabilities of camera systems, enabling creative and adaptable trajectory generation [Wang et al .\n2024a,b].\nThis section categorizes the prominent algorithms into four groups. Rule-based methods rely on predefined\ncinematic principles and heuristics, offering reliability but limited flexibility. Optimization techniques formulate\ntrajectory generation as a problem of maximizing shot quality while balancing constraints and objectives. Machine\nlearning approaches leverage data-driven models to learn complex motion patterns, introducing adaptability\nand creativity. Finally, hybrid methods integrate multiple strategies, combining the strengths of rule-based,\noptimization, and learning techniques to achieve enhanced performance and versatility. The following subsections\ndiscuss each category in detail, highlighting their foundational principles, strengths, and limitations.\n4.1 Rule-Based\nRule-based methods for camera trajectory generation rely on established cinematography principles rather than\noptimization or learning-based techniques. These approaches utilize traditional cinematic rules, expert insights,\nand well-defined heuristics, such as camera placement and guidelines [Chen and Carr 2014; Christie and Olivier\n2009]. These approaches offer a practical and computationally efficient solution. However, their rigidity poses a\nlimitation, as they strictly adhere to predefined rules, making adaptation and creativity challenging. Modifications\noften require revising or replacing these rules. Despite their inflexibility, rule-based methods provFide reliability\nand efficiency, particularly in scenarios with limited computational resources. The following section discusses\nkey contributions in this domain.\nThe first significant contribution to the application of cinematography principles for generating camera trajec-\ntories is presented in [He et al .1996], where the authors introduced the concept of the Virtual Cinematographer\n(VC), a system designed to generate real-time camera trajectories in virtual 3D environments. The VC incorporates\ncinematographic expertise using film idioms, implemented as a hierarchy of finite state machines, each suited to\nspecific scene types. These idioms control shot selection and transition timing to effectively depict unfolding\nevents. The paper details the filmmaking heuristics embedded in the system and demonstrates its application in a\nvirtual \"party\" scenario. However, the system’s applicability is constrained to a specific scenarios, limiting its\nbroader generalizability.\nTomlinson et al. [Tomlinson et al .2000] introduced a behavior-based autonomous cinematography system\ndesigned for interactive 3D environments. The system employs ethologically-inspired mechanisms, such as\nsensors, motivations, and hierarchical action-selection, to select optimal camera shots in real-time. It integrates\nseamlessly with virtual actors, enabling information exchange to create a cohesive and enriched environment.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 17 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •17\nHowever, challenges include maintaining adaptability to unpredictable actor behaviors and ensuring user comfort\nthrough effective coordination with the user interface. While limitations exist, the work establishes foundational\nprinciples for interactive cinematography systems.\nMezouar and Chaumette (2003) propose a method for generating camera trajectories in image-based control\nsystems through the use of smooth collineation paths connecting initial and desired viewpoints [Mezouar and\nChaumette 2003]. The approach aims to reduce energy consumption and acceleration while ensuring robustness\nagainst modeling errors and noise. A key feature of this method is its ability to operate without prior camera\ncalibration or a predefined scene model. Furthermore, the framework incorporates a potential field-based planning\nscheme to manage trajectory constraints, enabling effective tracking and adaptability in complex visual servoing\ntasks. However, the paper does not address potential limitations related to scalability or applicability in more\nintricate scenarios.\nChristie et al. [Chr [n. d.]] provide an review of camera control techniques aimed at enhancing viewer en-\ngagement in virtual environments. The paper addresses a range of methods, including viewpoint computation,\nmotion planning, and editing, grounded in cinematographic principles to meet diverse application requirements.\nA key focus is on constraint-based and optimization-based approaches, offering detailed insights into camera\nplacement and movement strategies. The study also explores occlusion management and the cognitive and\naesthetic dimensions of camera expressiveness. However, reliance on geometric abstractions may limit the\nhandling of complex 3D scenes, particularly in occlusion management and precise positioning.\nA prototype system was introduced for real-time rendering and automatic camera control in augmented virtual\nenvironments based on sparse video inputs [Silva et al .2011]. The system combines multiple video streams with a\n3D scene model to facilitate free-viewpoint visualization and automatic object tracking. Notable features include\nreal-time foreground-background segmentation, view-dependent texture mapping, and camera color calibration.\nThe approach is particularly suited for surveillance and event analysis applications. However, the paper does not\naddress potential challenges related to scalability or the system’s performance under varying environmental\nconditions, which may affect its generalizability.\nLino et al. [Lino et al .2011] propose a system to support the filmmaking process through an interactive\nassistant that uses a motion-tracked hand-held device for virtual cinematography. This approach facilitates\nrapid exploration of cinematographic options and efficient production of computer-generated films. However,\nthe reliance on pre-defined cinematic knowledge limits its adaptability to unexpected scenarios, potentially\nconstraining creative judgment. While effective for guided filmmaking, the system may not always align with\nthe user’s vision in novel or unconventional contexts. The hand-held virtual camera device is shown in Figure 13.\nIn a paper published in 2013, an approach was introduced to address the challenges of autonomous camera\ncontrol in dynamic 3D environments [Galvane et al .2013]. The study employs Reynolds’ steering behaviors\n[Reynolds et al .1999] to control multiple autonomous cameras in crowd simulations. The proposed system models\ncameras as intelligent agents that dynamically transition between scouting and tracking modes, optimizing their\npositioning to maximize event visibility while minimizing occlusions. By leveraging steering forces and torques,\nthe framework ensures adaptive, collision-free camera behaviors, producing diverse and informative shots.\nQuentin Galvane et al. [Galvane et al .2014] propose a system for automated cinematic replays in dialogue-based\n3D games, focusing on narrative-driven camera control. The method assesses characters’ narrative importance to\ninform camera framing, diverging from traditional action- or idiom-based approaches. It includes modules for\nassigning camera specifications based on narrative weight and for animating cameras smoothly across scenes.\nBy utilizing toric [Lino and Christie 2015] and spherical models [Christie et al .2008; Galvane et al .2015b], the\nsystem produces dynamic and visually coherent cinematic shots.\nThe often-overlooked challenge of object placement, or staging, in virtual cinematography was tackled through\nthe introduction of a staging language, presented as an extension of Prose Storyboard Language (PSL) [Louarn\net al.2018; Ronfard et al .2015]. This language coordinates the simultaneous positioning of characters and cameras\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 18 ---\n18 •Dehghanian et. al.\nFig. 13. The hand-held virtual camera device with custom-built dual handgrip rig and button controls, a 7-inch LCD touch-\nscreen [Lino et al. 2011].\nthrough geometric pruning and sampling operators, combined with fixed-point computation, to generate multiple\nstaging solutions. The pruning operators are applied to the PLRS, shown in Figure 14.\nFig. 14. PLRS for two entities A (in green) and B (in blue) [Louarn et al. 2018].\nBuilding on this work, the staging language was further extended to incorporate temporal relationships,\nfacilitating the simultaneous manipulation of cameras, lights, objects, and actors [Louarn et al .2020]. The iterative\npruning operators and graph-based problem decomposition enhance cinematic precision and adaptability, with\nan interactive system allowing fine-tuning and exploration. However, challenges remain, including scalability in\ndynamic environments, graph regeneration disrupting solution continuity, and diagnosing conflicting constraints.\nJovane et al. [Jovane et al .2020] address camera placement and movement in 3D virtual environments\nusing a topology-driven approach. This method utilizes navigation mesh analysis to create abstract skeletal\nrepresentations of the environment, which are then used to generate camera positions and trajectories organized\nin graph structures with visibility data. The system dynamically selects optimal cameras and paths based on\nartistic guidelines, making it suitable for real-time applications. While the approach allows for diverse and\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 19 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •19\nTable 1. Overview of Rule-Based Methods for Camera Trajectory Generation\nMethod Real World Virtual Camera Movement\n[He et al. 1996] - Animation Non-Fixed\n[Tomlinson et al. 2000] - Animation Non-Fixed\n[Mezouar and Chaumette 2003] Human-Based - Non-Fixed\n[Silva et al. 2011] Human-Based - Non-Fixed\n[Lino et al. 2011] - Animation/Games Non-Fixed\n[Galvane et al. 2013] - Animation/Games Non-Fixed\n[Chen and Carr 2014] - - -\n[Galvane et al. 2014] - Games Non-Fixed\n[Ronfard et al. 2015] Human-Based - -\n[Louarn et al. 2018] - Animation/Games Non-Fixed\n[Louarn et al. 2020] - Animation/Games Non-Fixed\n[Jovane et al. 2020] Human-Based Animation/Games Non-Fixed\n[Yoo et al. 2021] - Animation Non-Fixed\nNote: All the entries are entered based on evidence or our evaluation.\nadaptive camera behaviors in dynamic scenarios, its lack of event-specific contextual knowledge may limit\nnarrative alignment. Additionally, further development is needed to incorporate high-level controls and stylistic\ndiversity for more expressive cinematographic applications.\nYoo et al. [Yoo et al .2021] propose an automated approach to creating virtual camera layouts in 3D animation\nby replicating the cinematic attributes of a reference video. The method extracts key cinematic elements, such as\nframing, camera movements, and subject features, to generate adaptable layouts for both human-like and exag-\ngerated characters. User evaluations suggest the generated layouts are similar to those created by professionals,\nwhile reducing layout creation time, especially for novices. Although the system is effective for initial layout\ndevelopment, its reliance on extracted features may limit adaptability in dynamic or unconventional scenarios.\nRule-based methods for camera trajectory generation offer a reliable framework grounded in established\ncinematographic principles, ensuring practical application and computational efficiency. Their strengths lie in\nleveraging predefined rules to produce consistent results, particularly in real-time and resource-constrained\nscenarios. However, the inherent rigidity of these methods limits adaptability and creative flexibility, requiring\nmanual updates to accommodate novel contexts or evolving cinematic needs. Innovative systems like the Virtual\nCinematographer and topology-driven approaches enhance real-time applicability, yet challenges persist in\nscaling to dynamic or complex environments, such as occlusion and dynamic environment [He et al .1996;\nJovane et al .2020]. Future advancements should prioritize integrating adaptive and hybrid techniques to balance\nreliability, creativity, and user-driven flexibility.\nRule-based methods rely on well-established cinematographic principles and predefined heuristics to generate\ncamera trajectories. These methods offer computational efficiency and reliability, particularly in constrained\nscenarios where flexibility is less critical. However, their rigidity limits adaptability to novel contexts, requiring\nmanual updates to accommodate changing requirements. Table 1 highlights notable contributions in this area.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 20 ---\n20 •Dehghanian et. al.\n4.2 Optimization\nOptimization techniques for camera trajectory generation often express shot properties as objectives to maximize\nor to minimize, with metrics evaluating the quality of shots based on the scene’s graphical model and user-\ndefined criteria [Bonatti et al .2020b]. Classical methods include deterministic approaches, such as gradient-based\n[Bengio 2000] and Gauss-Seidel techniques [Tewari et al .2021], alongside non-deterministic strategies like\ngenetic algorithms [Wright 1991], Monte Carlo methods [Kroese and Rubinstein 2012], and stochastic local\nsearch [Hoos and St ¥𝜈tzle 2018]. While pure optimization techniques can produce solutions where properties are\npartially satisfied, they risk unbalanced outcomes, with some objectives dominating others [Deb and Ehrgott\n2023]. Conversely, purely constraint-based methods [Meseguer et al .2003] can compute complete sets of solutions\nbut are computationally intensive and struggle with over-constrained problems. A practical alternative lies in\nconstrained optimization, combining enforceable constraints and optimizable properties to balance feasibility and\nquality [Galvane et al .2015c]. Hybrid approaches that integrate constraint-based methods with optimization offer\neffective solutions, often leveraging geometric operators to narrow the search space before applying optimization\ntechniques. In this section, we provide an overview of the various methods proposed in the field of camera\ntrajectory generation, highlighting their underlying principles, strengths, and limitations.\n4.2.1 7-DOF Optimization Problems. The Optimization of camera trajectories can be formulated in a 7-DOF\nsearch space. The objective is to determine a camera configuration 𝑞∈𝑄, where𝑄denotes the space of all\npossible configurations, that maximizes a fitness function [Chr [n. d.]]. This can be mathematically expressed in\nEquation 7.\nmaximize𝐹(𝑓1(𝑞),𝑓2(𝑞),...,𝑓𝑛(𝑞))s.t.𝑞∈𝑄, (7)\nwhere each function 𝑓𝑖:R7→Revaluates the fitness of a specific property of the configuration, and\n𝐹:R𝑛→Rcombines these fitness values into a single scalar output. A commonly used formulation for 𝐹is a\nweighted sum [Marler and Arora 2010], defined in Equation 8.\n𝐹(𝑓1(𝑥),𝑓2(𝑥),...,𝑓𝑛(𝑥))=𝑛∑︁\n𝑖=1𝑤𝑖𝑓𝑖(𝑥), (8)\nwhere𝑤𝑖represents the weight associated with the 𝑖th property, allowing user preferences to influence the\noptimization process.\nExploring the continuous 7-DOF search space can be simplified through discretization [Latombe 2012], trans-\nforming it into a manageable grid. The CONSTRAINTCAM framework [Bares 2000] was extended with a global\noptimization strategy that exhaustively evaluates configurations based on an aggregated fitness value, as described\nin Equation 8. A typical discretization divides the search space into a 50×50×50grid for positions, 15◦angular\nincrements for orientation, and 10 levels for the field of view. To enhance efficiency, feasible regions are identified\nby intersecting individual property regions, and the grid resolution is iteratively reduced. The process terminates\nwhen a predefined quality threshold is met or the minimal resolution is reached, ensuring efficient exploration\nwhile adhering to the constraints in Equation 7.\nAn incremental solving approach for automating camera control in real-time target-tracking applications\nwas introduced to manage shot properties such as relative elevation, size, visibility, and screen position while\nensuring frame coherence to avoid abrupt movements [Halper et al .2001]. This system employs an algebraic\nincremental solver to adjust camera configurations by incrementally satisfying screen constraints and selectively\nrelaxing subsets when necessary. Look-ahead techniques are used to refine parameters based on anticipated\nobject motion [Halper et al .2001]. Similarly, Bourne and Sattar [Bourne and Sattar 2005] proposed a local search\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 21 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •21\noptimization method to preserve object-relative properties like height, distance, orientation and ensure smooth\ncamera paths.\nThe problem of computing optimal viewpoints in 3D environments is common in applications across computer\ngraphics and robotics [Scott et al .2003]. For instance, image-based modeling requires selecting a minimal set\nof cameras to cover all visible surfaces for texture mapping [Debevec et al .2023]. Early work by Kamada and\nKawai [Kamada and Kawai 1988] inspired many approaches by maximizing the projected area to surface area\nratio. Solutions often use classical solvers, such as simulated annealing [Stuerzlinger 1999], or heuristic methods\nthat populate environments with cameras and apply coverage metrics to evaluate solutions [Fleishman et al .\n2000]. A coverage metric evaluates how effectively selected viewpoints or cameras capture the required surfaces\nor areas of a 3D environment, considering visibility, resolution, and overlap criteria. Viewpoint entropy [Vázquez\net al.2003], maximizes the information captured in a minimal set of views. Other research explores cognitive\naspects like scene understanding and attention [Viola et al .2006], who try to augment geometry with object\nimportance to compute characteristic views using visibility and importance metrics. For scene exploration,\nheuristic optimization methods compute automatic camera paths by attracting the camera to unexplored areas\nbased on physical models [Sokolov et al .2006]. Initial configurations in these methods are guided by viewpoint\nquality estimations using total surface curvature and projected area.\nWhile optimization techniques in this section provide precise trajectories and a more realistic camera model,\nmany of these automated solutions are considered impractical. The algorithms operate in a seven-dimensional\nspace, which is virtually infinite, leading to high computational complexity [Lino and Christie 2015]. Additionally,\nthe search process demands substantial computational power, making it unsuitable for real-time systems or\nhardware with strict resource limitations. As a result, these methods often fail to meet the necessary delay\nconstraints for safe, real-time use [Ranon and Urli 2014]. Despite these challenges, 7-DOF algorithms offer\nvaluable benefits in terms of camera abstraction and interpretability, which sets them apart from alternative\nmethods that employ different approaches [Taketomi et al. 2017].\n4.2.2 Low Dimension Optimization Problems (LDO). The optimization problem addressed in [Christie et al .2008]\naims to improve the computational efficiency of virtual camera control, specifically for satisfying exact on-screen\npositioning of multiple subjects. Traditional methods, such as those relying on high-dimensional 7-DOF search\nspaces, encounter issues due to the computational cost of exploring large regions of the solution space, which\nlimits practical applications. The proposed approach [Christie et al .2008] reduces this complexity by representing\nthe solution space as a 2D manifold for two subjects and extending it algebraically to three or more subjects.\nThis manifold is parameterized by meaningful angles, simplifying the optimization process while maintaining\naccuracy.\nThe primary issue arising from traditional methods relying on high-dimensional searches is addressed through\nan optimization approach leveraging the Toric space [Lino and Christie 2015]. This technique reduces the search\nspace from 7-DOF to 4-DOF. By employing an interval-based pruning algorithm (as shown in 9), the method\nincrementally narrows the solution space through constraints on angles ( 𝛼,𝜃, and𝜙) and field of view, ensuring\nthat only regions meeting all necessary properties are retained.\nmin\n𝛼,𝜃,𝜙∑︁\n𝑖𝑤𝑖·Error𝑖(𝛼,𝜃,𝜙), (9)\nwhere Error𝑖quantifies the deviation of a visual property from its desired value, and 𝑤𝑖is the weight assigned\nto that property. This cost function balances competing constraints to find optimal camera positions. While the\napproach is computationally efficient, it may struggle in highly over-constrained scenarios where no feasible\nsolution exists [Lino and Christie 2015].\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 22 ---\n22 •Dehghanian et. al.\nThe optimization approach in [Galvane et al .2015a] addresses the challenge of generating smooth and realistic\ncamera motions for dynamic scenes while satisfying aesthetic and physical constraints. It begins by interpolating\na raw camera trajectory based on user-defined framing properties, which is then smoothed using a cubic Bézier\ncurve [Arijon 1976]. A two-step optimization refines this trajectory, minimizing positional errors and ensuring\nsmooth transitions in velocity, controlled acceleration, and accurate orientation adjustments.\nThe work in [Ren et al .2023] automates camera control in dynamic settings by integrating PTZ mechanics\n3.1 with DNN-based visual sensing. Traditional systems lack real-time adaptability, often relying on predefined\npaths. The process begins with visual detection using DNNs [Samek et al .2021], followed by target tracking\nand estimation via Kalman filters [Khodarahmi and Maihami 2023]. Trajectories are dynamically planned with\nPID control [Borase et al .2021], adjusting pan, tilt, and zoom to maintain aesthetic composition within physical\nconstraints, such as angular velocity and acceleration limits.\nResearch in this area has primarily focused on altering the camera’s representation or fixing some of the\ndimensions to reduce the overall search space. The use of Toric space has been particularly dominant due to\nits efficient mathematical representation and its ability to be transformed into Cartesian coordinates. However,\nseveral challenges persist in this domain. One key issue is that many algorithms achieve lower-dimensional\nsolutions by either simplifying certain parameters or fixing them, which reduces the search space but often leads\nto compromises in flexibility [Burg et al .2020]. Additionally, some methods impose constraints to target specific\nproblems or a fixed number of objectives, limiting their general applicability [Burg et al. 2020].\n4.2.3 Drone Trajectory Optimization (DTO). Creating camera trajectories for drones involves two distinct tasks\nwith unique requirements. The first is object tracking, which ensures the camera remains focused on the target\nat all times without losing sight of it. The second is cinematography, which emphasizes aerial filming to achieve\nvisually appealing shots [Bonatti et al .2020a]. A key distinction in drone-based filming is that the camera and\ndrone are most often coupled, meaning that optimizing the drone’s trajectory inherently optimizes the camera’s\npath or the trajectory of the camera are often considered the trajectory of the drone. Optimization problems are\nwidely used in drone applications due to the need for fast, real-time responses. Machine learning methods are\nless prevalent in this domain, as most drones lack the computational hardware required to run complex models\nefficiently, and such methods often introduce significant latency, making them unsuitable for time-sensitive\ntasks. In this section, we explore optimization techniques tailored to aerial vehicles, addressing these challenges\neffectively.\nIn a paper introduced in 2016 [Gebhardt et al .2016], a computational framework has been developed to plan\nquadrotor trajectories by integrating high-level user objectives with physical feasibility constraints. Optimization-\nbased methods are employed to generate flight paths that adhere to user-defined goals, such as smooth aerial\nvideography or complex maneuvers, without requiring expertise in low-level control systems. A 3D design\ninterface allows intuitive specification and iterative refinement of trajectories. Constraints from cinematography,\nphysical dynamics, and collision avoidance are incorporated to ensure practical applicability across use cases,\nincluding drone racing and robotic light-painting.\nThe optimization problem in [Roberts and Hanrahan 2016] addresses the challenge of generating dynamically\nfeasible trajectories for quadrotor cameras, which must satisfy velocity and control force limits while preserving\nthe visual layout of user-specified paths. This is critical because infeasible trajectories can result in unsafe\nquadrotor operation or deviation from intended paths. The proposed solution optimizes the progress curve\n𝑠(𝑡), re-timing the trajectory to ensure physical feasibility with minimal deviation from the user’s input. The\nalgorithm discretizes the camera path, enforcing constraints on velocity, acceleration, and control forces through\na non-convex optimization frameworkas shown in 10.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 23 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •23\nmin\n𝑆,𝑉∑︁\n𝑖(¤𝑠𝑖−¤𝑠ref\n𝑖)2\nsubject to 𝑠𝑖+1=𝑠𝑖+(𝑀𝑠𝑖+𝑁𝑣𝑖)Δ𝑠𝑖\n¤𝑠𝑖,\n𝑣min≤𝑣𝑖≤𝑣max,¤𝑠𝑖>0,\n𝑢min≤𝑈(𝑠𝑖)≤𝑢max,\n¤𝑞min≤¤𝑄(𝑠𝑖)≤¤𝑞max,(10)\nwhere¤𝑠ref\n𝑖represents the desired progress curve derivatives, let 𝑆be the concatenated vector of all 𝑠𝑖values\nalong the path, let 𝑉be the concatenated vector of all 𝑣𝑖values along the path., and 𝑈(𝑠𝑖)and¤𝑄(𝑠𝑖)represent\ncontrol forces and velocity constraints, respectively.\nThe challenge of balancing dynamic feasibility in drone motion, such as adhering to velocity and acceleration\nlimits, with cinematographic constraints like framing targets and ensuring smooth transitions, is addressed in\n[Nägeli et al .2017a]. The proposed solution involves an optimization process that minimizes a composite cost\nfunction, representing deviations from desired shot parameters while respecting both physical and cinematic\nconstraints, such as framing, collision avoidance, visibility, and pose alignment. This unified framework effectively\nintegrates aesthetic and physical considerations, allowing drones to execute precise and visually appealing\nmovements.\nmin\nx,u,s𝑤⊤\n𝑁𝑐(x𝑁,u𝑁)+𝑁−1∑︁\n𝑘=0𝑤⊤𝑐(x𝑘,u𝑘)+𝜆∥s𝑘∥∞, (11)\nsubject to:\nx0=xinit\n0, (Initial State)\nx𝑘+1=𝑓(x𝑘,u𝑘), (Dynamics)\n𝑟⊤\n𝑐𝑡Ω𝑟𝑐𝑡>1−𝑠𝑘, (Collision Avoidance)\n𝑟𝑐𝑡=𝑔(x𝑘), (Geometric Relationship)\nx𝑘∈X, (State Constraints)\nu𝑘∈U, (Input Constraints)\ns𝑘≥0, (Slack Constraints)\nThe cost function 𝑐(x𝑘,u𝑘)is defined as:\n𝑐(x𝑘,u𝑘)=\u0002𝑐image,𝑐size,𝑐angle,𝑐coll,𝑐vis,𝑐pose\u0003⊤\n(x𝑘,u𝑘), (12)\nThe cost function minimizes the terminal cost is 𝑤⊤\n𝑁𝑐(x𝑁,u𝑁), cumulative stage costsÍ𝑁−1\n𝑘=0𝑤⊤𝑐(x𝑘,u𝑘), and a\npenalty term 𝜆∥s𝑘∥∞to handle constraint relaxation through slack variables. The system starts at an initial state\nxinit\n0and evolves via dynamics x𝑘+1=𝑓(x𝑘,u𝑘). Collision avoidance is enforced by requiring 𝑟⊤\n𝑐𝑡Ω𝑟𝑐𝑡>1−𝑠𝑘,\nwhere𝑟𝑐𝑡=𝑔(x𝑘)defines geometric relationships, with slack s𝑘ensuring feasibility. States x𝑘and controls u𝑘\nmust adhere to feasible sets XandU, respectively, while slack variables s𝑘are constrained to be non-negative\npenalty to balance accuracy, smoothness, and constraint relaxation.\nThe work in [Nägeli et al .2017b] extends the optimization framework from [Nägeli et al .2017a] to address\nchallenges in cluttered environments. Using a non-linear Model Predictive Contouring Control (MPCC) [Lam et al .\n2010], it integrates framing objectives, path accuracy, and collision avoidance into the cost function, enabling\nreal-time trajectory re-planning. The method accounts for dynamic constraints and uses slack variables to\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 24 ---\n24 •Dehghanian et. al.\nhandle infeasibilities, ensuring smooth, collision-free motion suitable for high-quality cinematography, even with\nmultiple drones.\nA study published in 2018 [Gebhardt et al .2018] introduced an optimization-based approach for generating\nsmooth and visually appealing quadrotor camera trajectories. The problem was formulated as an infinite-horizon\noptimization framework, where a weighted cost function 𝐽𝑖was minimized to balance positional accuracy,\nmotion smoothness, and timing control. This cost function incorporates terms for positional reference tracking,\norientation alignment, jerk minimization, timing progress, and control regularization, with adjustable scalar\nweight parameters to achieve a trade-off between these objectives. The optimization problem is solved under\nconstraints, including system dynamics, bounds on states and control inputs, and progress variables. This\nformulation ensures that the generated trajectories adhere to user-defined spatial and temporal requirements\nwhile maintaining aesthetic smoothness. The formula for this optimization method is detailed in Equation 13.\nmin\n𝑥,𝑢,Θ,𝑣𝑁∑︁\n𝑖=0𝑤𝑝𝑐𝑝(𝜃𝑖,r𝑖)+𝑤𝜓𝑐𝜓(𝜃𝑖,𝜓𝑞,𝑖,𝜓𝑔,𝑖)+𝑤𝜙𝑐𝜙(𝜃𝑖,𝜙𝑞,𝑖)+\n𝑤𝑗𝑐𝑗(¨r,¨𝜓𝑞,¨𝜙𝑞,𝑖)+𝑤end𝑐end(𝑇)+𝑤len𝑐len(𝑁,Δ𝑡)+𝑤𝑣∥v∥2, (13)\nsubject to\nx0=𝑘0, (initial state)\nΘ0=0, (initial progress)\nΘ𝑁=𝐿, (terminal progress)\nx𝑖+1=𝐴𝑥𝑖+𝐵𝑢𝑖+𝑔, (dynamical model)\nΘ𝑖+1=𝐶Θ𝑖+𝐷𝑣𝑖, (progress model)\nxmin≤𝑥𝑖≤𝑥max, (state bounds)\numin≤𝑢𝑖≤𝑢max, (input limits)\n0≤Θ𝑖≤Θmax, (progress bounds)\n0≤𝑣𝑖≤𝑣max, (progress input limits)\nwhere the scalar weight parameters 𝑤𝑝,𝑤𝜓,𝑤𝜙,𝑤𝑗,𝑤end,𝑤len,𝑤𝑣>0are adjusted for a good trade-off between\npositional fit and smoothness.\nThe optimization problem in [Bonatti et al .2020b] focuses on generating smooth, and visually appealing\ntrajectories for drones filming dynamic actors, addressing issues such as obstacle avoidance, occlusion prevention,\nand adherence to artistic cinematography principles. They argued that traditional methods either neglect critical\nartistic objectives or fail in real-world scenarios with noisy localization and dynamic obstacles. This approach\ndecouples the drone and camera motions, leveraging a gimbal 3.2 for fine adjustments. The proposed solution\nformulates the trajectory optimization as minimizing a composite cost function 𝐽(𝜉𝑞)defined as Equation 14.\n𝐽(𝜉𝑞(𝑡))=𝐽smooth(𝜉𝑞(𝑡))+𝜆1𝐽obs(𝜉𝑞(𝑡))\n+𝜆2𝐽occ(𝜉𝑞(𝑡),𝜉𝑎(𝑡))+𝜆3𝐽shot(𝜉𝑞(𝑡),𝜉𝑎(𝑡)),\n𝜉∗\n𝑞(𝑡)=arg min\n𝜉𝑞(𝑡)∈Ξ𝐽(𝜉𝑞(𝑡)),∀𝑡∈[0,𝑡𝑓].(14)\nwhere𝐽smooth ensures trajectory smoothness, 𝐽obspenalizes proximity to obstacles, 𝐽occreduces occlusion\nbetween the camera and the actor 𝜉𝑎, and𝐽shotenforces adherence to artistic shot guidelines. 𝜉𝑞(𝑡)are the\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 25 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •25\ntrajectory of the quadrotor (drone) represents its position in 3D space over time, 𝜉𝑎(𝑡)in the otherhand are\ntrajectory of the actor describes their position over time. subject to boundary constraints and the drone’s dynamic\nfeasibility. The optimization process utilizes a covariant gradient descent [Zucker et al .2013] approach to\niteratively minimize 𝐽(𝜉𝑞), ensuring efficient convergence while accounting for noise in actor predictions.\nThe study in [Rousseau et al .2018] tackles the challenge of generating smooth quadcopter trajectories for\ncinematic applications by minimizing jerk to enhance video quality. A bilevel optimization approach is employed:\nthe first step adjusts velocity references within vertical and lateral limits, and the second step computes a\nminimum-jerk trajectory via quadratic programming. To manage complex flight plans, a receding waypoint\nhorizon is used, iteratively computing trajectories over shorter segments to ensure smooth transitions and\nconstraint adherence.\nA method for dynamically sampling 3D environments with a visibility-aware roadmap is presented in [Galvane\net al.2018], addressing the challenge of adapting to moving obstacles. The approach uses a composite distance\nmetric combining cinematographic properties, such as target distance and angles, with spatial constraints. Path\nplanning operates in a 4D parameter space, integrating the DTS 2.3.4 for visual properties and altitude for spatial\nconsistency, and employs the A* algorithm [Oskam et al .2009]. Trajectories are refined to 𝐶4-continuity to ensure\nsmoothness and minimize abrupt changes in drone dynamics. 𝐶4-continuity refers to a mathematical property\nof a trajectory where the path and its first four derivatives (position, velocity, acceleration, jerk, and snap) are\ncontinuous.\nAn algorithm for real-time chasing a moving target in dense environments is presented in [Jeon and Kim 2019].\nThe approach ensures safety, visibility, and adherence to physical constraints by coupling the drone and gimbal\ncamera trajectories, prioritizing target visibility. It refines a preplanned sequence of safe waypoints and corridors\ninto a continuous trajectory using a convex optimization framework. Represented as piecewise polynomials, the\ntrajectory minimizes a cost function, as detailed in Equation 15.\nmin\n𝑝𝑛𝑁∑︁\n𝑛=1\u0012∫𝑡𝑛\n𝑡𝑛−1∥¨x𝑐(𝜏)∥2𝑑𝜏+𝜆∥x𝑐(𝑡)−x𝑛∥2\u0013\n, (15)\nwhere𝑝𝑛represents the optimized waypoints or control points of the MAV’s trajectory to ensure smoothness,\nsafety, and visibility during motion planning, x𝑐(𝑡)represents the drone’s position at time 𝑡𝑛,x𝑛is the𝑛-th\nwaypoint, and¨x𝑐(𝜏)is the jerk (third derivative of position). The cost function consists of two terms: the integral\nof squared jerk to ensure smooth motion, and a penalty term 𝜆∥x𝑐(𝑡𝑛)−x𝑛∥2to minimize deviations from the\npreplanned waypoints. The optimization in [Jeon and Kim 2019] incorporates constraints on initial conditions,\ntrajectory continuity up to the second derivative, and adherence to safety corridors, formulating the problem as a\nquadratic programming task solved efficiently with interior-point methods [Gondzio 2012].\nIn the context of autonomous cinematography, Sabetghadam et al. [Sabetghadam et al .2019] solved the problem\nas a nonlinear optimization task, minimizing a cost function that combines control effort, camera smoothness,\nand terminal tracking objectives, as formulated in (16).\nmin\n𝑥0,...,𝑥 𝑁,𝑢0,...,𝑢 𝑁𝑁∑︁\n𝑘=0\u0000𝑤1∥𝑢𝑘∥2+𝑤2𝐽𝜃+𝑤3𝐽𝜓\u0001+𝑤4𝐽𝑁, (16)\nwhere𝐽𝜃and𝐽𝜓penalize angular camera movements, 𝐽𝑁enforces the final state’s proximity to the desired\nposition and velocity, and 𝑢𝑘represents control inputs. The optimization is subject to constraints, such as, Enforces\nsystem kinematics 𝑥𝑘+1=𝑓(𝑥𝑘,𝑢𝑘)to maintain trajectory feasibility, Limits 𝑣𝑄,𝑢𝑘within drone specifications,\nKeeps the drone at least 𝑟coldistance away from obstacles and, Maintains gimbal angles within mechanical\nlimits. The optimization is solved iteratively in a receding horizon framework which is an approach to solving\noptimization problems over a time horizon that dynamically adapts to changes in the system. In this method,\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 26 ---\n26 •Dehghanian et. al.\nthe system plans trajectories over a fixed prediction horizon, executes the initial part of the plan, and then\nre-optimizes as new information about the system’s state and environment becomes available.\nThe framework in [Bonatti et al .2019] addresses the limitations of relying on predefined maps or precise\nlocalization by integrating actor localization, real-time LiDAR mapping, and trajectory planning. A composite\ncost function guides the trajectory planner, optimizing for smoothness to ensure stability and video quality, shot\nquality to adhering to cinematic guidelines like angle and distance, safety to avoiding collisions, and occlusion to\nminimizing visual obstructions using covariant gradient descent [Zucker et al. 2013].\nBuilding on this, the approach in [Bonatti et al .2020a] redefines artistic shot selection as a sequential decision-\nmaking problem using deep reinforcement learning (RL). By modeling it as a Contextual Markov Decision Process\n(C-MDP) [Krishnamurthy et al .2016], the system maps scene context to optimal shot parameters in real time.\nThe RL algorithm optimizes a reward function evaluating artistic quality metrics like smoothness, visibility, and\nobstacle avoidance, enabling adaptive and aesthetically refined drone behavior for high-quality cinematography.\nThe work in [Katoch and Ueda 2019] optimizes camera trajectories to minimize motion blur and preserve\nedge features critical for enhancing OCR accuracy [Mittal and Garg 2020]. It employs fourth-order polynomial\ntrajectories that balance kinematic constraints with edge preservation, ensuring smooth motion with controlled\nvelocity and acceleration. These trajectories maximize time at critical positions to enhance edge sharpness, and a\ntunable parameter allows fine-tuning between motion smoothness and edge clarity, improving real-time OCR\nperformance.\nIn the realm of object tracking, [Jeon et al .2020] stats that the primary focus must be on improving the\ndetectability of a target during a drone cinematographer’s chasing motion. The proposed optimization actively\nadjusts the drone’s motion to ensure the target is distinguishable in the drone’s view. The optimization process\ninvolves two main steps. First, a detectability-aware discrete path is generated by solving a directed acyclic graph\n(DAG) [Digitale et al .2022] problem. The graph nodes represent candidate viewpoints, and edges are evaluated\nfor both distance traveled and a detectability metric that quantifies the separability of the target and background\nin the color space. The optimization aims to minimize the cumulative travel distance while maximizing the\ndetectability score. This process is mathematically represented in Equation 17.\nmin\n𝜎𝑁−1∑︁\n𝑖=0∥x𝑐,𝑖−x𝑐,𝑖+1∥+𝜆𝑁∑︁\n𝑖=1𝐿(x𝑐,𝑖|ˆT𝑎,𝑖), (17)\nSubject to the constraints: ∥x𝑐,𝑖−x𝑎,𝑖∥=𝑟𝑑, ensuring the drone maintains a fixed distance from the target, and\n∥x𝑐,𝑖−x𝑐,𝑖+1∥≤𝑟max, bounding the maximum inter-step travel distance. Here, x𝑐,𝑖denotes the drone’s position,\nˆT𝑎,𝑖is the predicted target pose, and 𝐿(·)represents the detectability cost function. Additionally, a smooth and\ndynamically feasible trajectory is generated using quadratic programming [Chen et al .2016b], which interpolates\nthe discrete path while minimizing high-order derivatives for smooth motion, ensuring real-time applicability in\ndynamic scenarios.\nThe method in [Burg et al .2020] ensures smooth, predictable camera movements while avoiding occlusions in\ncomplex 3D environments. It generates an occlusion anticipation map (A-map) to predict future occlusions and\nadjusts the camera’s motion using a physics-driven model. When local solutions fail, strategies like look-ahead\nsearches [Agarwal et al .2018; Raffone et al .2019] or “cuts” [ranon et al .2016] provide optimal viewpoints,\nmaintaining continuous, unobstructed views in dynamic scenes.\nThe focus of [Ashtari et al .2020] was to enable drones to autonomously capture subjective first-person view\n(FPV) shots by imitating human camera operator motion for immersive cinematography. The proposed method\nmodels human walking dynamics and uses a constrained optimization framework to compute drone control\ncommands that replicate these motions while adhering to user-defined trajectories and the drone’s physical\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 27 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •27\nconstraints. Operating in real time, it allows interactive parameter adjustments and seamless transitions between\nshot styles in various environments.\nThe approach in [Gebhardt and Hilliges 2021] tackles challenges in aerial cinematography by optimizing\ntrajectories to maintain proper framing of 3D targets like landmarks while adhering to user intentions. By\nintegrating compositional rules like the Rule of Thirds [Amirshahi et al .2014; Maleš et al .2012] and penalizing\ndeviations from user-specified target positions, the method ensures targets stay fully visible in the frame. Using\ninfinite horizon contour-following equations [Gebhardt et al .2018] in a multi-objective optimization framework,\nit balances smooth motion, framing, and visibility for high-quality aerial video footage.\nThe method in [Yu et al .2022a] addresses the challenge of aligning virtual camera content with both aesthetic\nand script fidelity requirements. Prior approaches often prioritize aesthetic rules at the expense of accurately\nreflecting the script’s intent. To overcome this, the authors propose a unified framework that minimizes a\nweighted sum of aesthetic distortion ( 𝐷𝑎) and fidelity distortion ( 𝐷𝑓), as formalized in Equation 18. Using\ndynamic programming [Bellman 1966], this recursive approach ensures that decisions about the current frame’s\ncamera configuration do not depend on earlier choices, allowing the use of dynamic programming for efficient\ncomputation.\n𝐷𝑘(𝑧𝑘−𝑞,...,𝑧𝑘)= min\n𝑧𝑘−𝑞−1,...,𝑧𝑘−1\u001a\n𝐷𝑘−1(𝑧𝑘−𝑞−1,...,𝑧𝑘−1)\n+𝜆\n𝑇[𝛼𝑂(𝑐𝑘)+𝛽]\n+(1−𝜆)\u0002\n𝜔0𝑉(𝑐𝑘)+𝜔1𝐶(𝑐𝑘)+𝜔2𝐴(𝑐𝑘)\n+𝜔3𝑆(𝑐𝑘,𝑐𝑘−1)+𝜔4𝑀(𝑐𝑘,𝑐𝑘−1)\u0003\n+(1−𝜆)·\n(1−𝜔0−𝜔1−𝜔2−𝜔3−𝜔4)·\n𝑈(𝑢,𝑐𝑘,𝑐𝑘−1,...,𝑐𝑘−𝑞)\u001b(18)\nEach term in Equation 18 corresponds to different aspects: 𝐷𝑘−1refers to the accumulated distortion up\nto the previous frame; 𝜆is a weighting factor balancing fidelity and aesthetic distortions; 𝑂(𝑐𝑘)quantifying\nocclusion;𝑉(𝑐𝑘)character visibility distortion; 𝐶(𝑐𝑘)camera configuration distortion; 𝐴(𝑐𝑘)Action alignment\ndistortion;𝑆(𝑐𝑘,𝑐𝑘−1)Screen continuity distortion; 𝑀(𝑐𝑘,𝑐𝑘−1)Motion continuity distortion; 𝑈(𝑢,𝑐𝑘,...,𝑐𝑘−𝑞):\nShot duration distortion; 𝑍𝑘and other parameters are trainable.\nCineMPC, introduced in [Pueyo et al .2022], optimizes both extrinsic and intrinsic parameters of UAV-mounted\ncameras for autonomous cinematography. Using a non-linear Model Predictive Control (MPC) framework\n[Schwenzer et al .2021], it minimizes a cost function balancing cinematic goals, physical constraints, and artistic\nguidelines. By solving for optimal movements over a finite time horizon, the system adapts to dynamic targets,\nproducing smooth, cinematic-quality footage.\nThis section explored a range of algorithms designed for real-world drone applications, focusing on those that\ntry to optimize delays while accounting for the drone’s physical constraints and the problem’s unique nature.\nAlthough these algorithms are efficient and can operate with minimal delay, they often struggle with accuracy,\nparticularly in generating smooth trajectories. Most of the methods navigate between two or more points or\ntargets to record footage, yet they frequently fall short when it comes to planning more complex, seamless paths\nthat are essential for optimal drone operation.\nOptimization-based techniques frame trajectory generation as an objective-driven process, using metrics to\nevaluate shot quality. Classical approaches, such as gradient-based methods and genetic algorithms, excel in\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 28 ---\n28 •Dehghanian et. al.\nTable 2. Overview of Optimization Methods for Camera Trajectory Generation Methods\nMethod Type Real World Virtual Camera Movement\n[Kamada and Kawai 1988] 7-DOF Human-Based Animation/Games Non-Fixed\n[Stuerzlinger 1999] 7-DOF Human-Based Animation/Games Non-Fixed\n[Fleishman et al. 2000] 7-DOF Human-Based - Fixed\n[Bares 2000] 7-DOF - Animation/Games Non-Fixed\n[Halper et al. 2001] 7-DOF Human-Based - Non-Fixed\n[Vázquez et al. 2003] 7-DOF Human-Based Animation/Games Non-Fixed/Fixed\n[Bourne and Sattar 2005] 7-DOF - Games Non-Fixed\n[Viola et al. 2006] 7-DOF - - Fixed\n[Sokolov et al. 2006] 7-DOF - Animation/Games Non-Fixed\n[Christie et al. 2008] LDO - Animation/Games Non-Fixed\n[Lino and Christie 2015] LDO - Animation/Games Non-Fixed\n[Galvane et al. 2015a] LDO - Animation/Games Non-Fixed\n[Ren et al. 2023] LDO Human-Based - PTZ\n[Roberts and Hanrahan 2016] DTO Areal-Based - Gimbal Mounted\n[Gebhardt et al. 2016] DTO Areal-Based - Gimbal Mounted\n[Nägeli et al. 2017a] DTO Areal-Based - Gimbal Mounted\n[Nägeli et al. 2017b] DTO Areal-Based - Gimbal Mounted\n[Bonatti et al. 2020b] DTO Areal-Based Animation/Games Gimbal Mounted\n[Rousseau et al. 2018] DTO Areal-Based - Gimbal Mounted\n[Gebhardt et al. 2018] DTO Areal-Based - Gimbal Mounted\n[Galvane et al. 2018] DTO Areal-Based - Gimbal Mounted\n[Jeon and Kim 2019] DTO Areal-Based - Gimbal Mounted\n[Sabetghadam et al. 2019] DTO Areal-Based - Gimbal Mounted\n[Bonatti et al. 2019] DTO Areal-Based - Gimbal Mounted\n[Bonatti et al. 2020a] DTO Areal-Based - Gimbal Mounted\n[Katoch and Ueda 2019] DTO Areal-Based - Gimbal Mounted\n[Jeon et al. 2020] DTO Areal-Based - Gimbal Mounted\n[Burg et al. 2020] DTO Areal-Based - Gimbal Mounted\n[Ashtari et al. 2020] DTO Areal-Based - Gimbal Mounted\n[Gebhardt and Hilliges 2021] DTO Areal-Based - Gimbal Mounted\n[Yu et al. 2022a] DTO Areal-Based - Gimbal Mounted\n[Pueyo et al. 2022] DTO Areal-Based - Gimbal Mounted\nNote: All the entries are entered based on evidence or our evaluation.\nbalancing enforceable constraints and optimizable properties. While these methods are effective for applications\nlike drone cinematography, where real-time responses are critical, challenges such as high computational\ndemands and limited flexibility persist. Table 2 outlines various optimization techniques, emphasizing their role\nin addressing dynamic and constrained environments.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 29 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •29\nFig. 15. The framework of imitation filming [Huang et al. 2019]\n4.3 Machine Learning\nCamera trajectory generation has seen remarkable advancements through machine learning in recent years\n[Courant et al .2025; Jiang et al .2024b; Wang et al .2024a]. Traditional methods based on optimization and\nhandcrafted rules have progressively been complemented by data-driven approaches, which enable the automation\nof trajectory synthesis by learning complex patterns from examples. These methods offer greater flexibility and\nadaptability compared to traditional approaches, effectively addressing their shortcomings[Wang et al .2024a]. By\nleveraging deep learning models, these methods not only incorporate cinematic principles and adapt to diverse\nconstraints but also provide the ability to generate diverse and creative camera trajectories [Dehghanian et al .\n2025; Jiang et al .2020]. This paradigm shift has expanded the creative capabilities of camera movement systems,\nenhancing their efficiency, with generative models serving as a cornerstone for these advancements [Courant\net al. 2025; Jiang et al. 2024b]. In the following, we examine the evolution of these methods.\nOne of the earliest efforts to apply machine learning to camera trajectory generation was presented by Chen\net al. [Chen et al .2016a], where Recurrent Random Forests were utilized to predict the pan angle of a camera\nin sports events. This study introduced a novel method for optimizing random forest models, wherein each\nprediction was dependent solely on the previous one. This dependency on the prior state ensured that the\ngenerated camera trajectory maintained the necessary smoothness and continuity. Simply put, this approach\nemployed random forests within a Markovian structure to synthesize camera trajectories.\nIn the paper introduced in [Huang et al .2019], a data-driven learning-based approach is proposed to enable\ndrones to autonomously capture cinematic footage by imitating professional camerawork. Unlike traditional\nmethods that rely on predefined camera movements or heuristic planning (i.e., rule-based methods), the proposed\nframework employs supervised learning to predict future image composition and camera position, subsequently\ngenerating control commands to achieve professional shot framing. The framework of imitation filming introduced\nin this paper is illustrated in Figure 15.\nIn their 2020 paper, Christos Kyrkou et al. [Kyrkou 2020] propose an end-to-end approach for active camera\ncontrol using deep convolutional neural networks to address limitations of traditional multi-stage systems. Their\nmodel, named ACDCNet, combines visual detection and camera motion control in a single framework, using\nimitation learning to train the network on image-action pairs. The study demonstrates significant improvements\nin multi-target tracking, efficiency, and real-time performance compared to conventional methods.\nThe 2020 paper Example-driven Virtual Cinematography by Learning Camera Behaviors [Jiang et al .2020]\nproposed a framework for transferring camera behaviors from one video to another. First, they extract a raw\nskeleton, followed by refinement method and then with a neural network to estimate the camera position in\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 30 ---\n30 •Dehghanian et. al.\nFig. 16. The model presented in the article [Jiang et al. 2020] for transferring cinematic features from a reference video.\nFig. 17. The architecture of the model [Jiang et al .2021] for generating camera trajectories based on a reference video and\nkey points.\ntoric space. For trajectory generation, they utilized a mixture of experts framework, incorporating an LSTM\nfollowed by a fully connected layer as a gating network to determine the weighting of each expert. Each expert,\nimplemented as a three-layer fully connected network, predicted new camera poses by processing character\ncinematic features from a 3D animation and information from past frames. Figure 16 shows the architecture of\nthe model proposed in this paper.\nThe paper [Jiang et al .2021] was published with the aim of adding more precise control over camera movement\nusing key points. This research, building on the work in [Jiang et al .2020], incorporates the ability to control the\ncamera trajectory through key points rather than solely following a reference video.\nIn their new architecture, the previous feature extraction model is still used to process the reference video,\nbut the trajectory generation structure has been redesigned. Instead of employing a complex Mixture of Experts\n(MoE) architecture with multiple fully connected networks, an LSTM is used to extract embeddings from the\nreference video. This structural change simplifies the architecture and enhances the model’s ability to understand\nthe temporal features of camera movement. During the trajectory generation stage, the extracted embedding,\nalong with the camera key point information, character positions, and the previous camera position, is fed into\nan LSTM network. This network operates in an autoregressive, step-by-step manner to generate the camera’s\npositions. In Figure 17, the overall architecture of this network is illustrated.\nKyrkou et al. (2021) proposed C3NET [Kyrkou 2021], a lightweight neural network designed for real-time\ncamera control through direct end-to-end learning from visual input to pan-tilt motion commands. Unlike\ntraditional approaches that rely on multiple modules for detection, tracking, and control, C3NET learns to map\nraw image pixels directly to camera movement parameters without requiring explicit object detection or bounding\nbox annotations. The network implicitly learns to identify targets and determine appropriate camera movements\nto keep them centered in the field of view. Their architecture consists of two main components: a feature extractor\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 31 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •31\nwith convolutional blocks for processing visual information, and a fully connected controller subnetwork that\nmaps these features to camera motion controls.\nA study in 2021 introduced trajectory tensors for Multi-Camera Trajectory Forecasting (MCTF), addressing\nlimitations of traditional coordinate-based methods [Styles et al .2021]. Unlike coordinate trajectories, which\nstruggle with occlusions and multiple camera views, trajectory tensors represent object locations as heatmaps\nacross cameras and timesteps, capturing spatial and temporal information in a unified form. This approach\nhandles null trajectories, accounts for object scale, and models uncertainty in trajectory forecasting. The authors\ndemonstrate its effectiveness using various models, including 3D-CNNs and CNN-GRU, which leverage the\ntrajectory tensor representation for improved spatiotemporal forecasting.\nIn 2021 also, a deep reinforcement learning (RL) framework with an attention-based approach was proposed\nfor virtual cinematography of 360-degree videos [Wang et al .2021]. This work aimed to replicate the viewpoint\nselection of professional cinematographers by integrating saliency detection and RL techniques. The proposed\nsystem utilized a DenseNet architecture to process both video content and saliency maps simultaneously. The\nRL component managed narrow field of view selection as a continuous action space, with a reward function\ndesigned to balance saliency, alignment with ground-truth views, and smoothness of camera transitions.\nThe paper Enabling Automatic Cinematography with Reinforcement Learning [Yu et al .2022b] introduced a\nnew RL approach using Proximal Policy Optimization (PPO) to train camera settings for virtual environments.\nThe reward function was designed to optimize the camera’s position and angle by minimizing the absolute\ndifference from the ground truth, scaled by a factor of either 180 or 30 depending on the specific parameter.\nThis approach effectively allowed the system to learn context-aware camera placements through reinforcement\nlearning.\nThe 2023 paper, The Secret of Immersion: Actor-Driven Camera Movement Generation for Auto-Cinematography\n[Wu et al .2023], introduced a deep camera control framework designed to achieve actor-camera synchronization\nacross three dimensions: frame aesthetics, spatial action, and emotional status. The approach begins with a\nuser-provided initial camera position and utilizes the rule of thirds in a self-supervised manner to refine the\ncamera’s placement. This is achieved by incorporating a loss function based on the distance from the rule of\nthirds, along with minimizing differences in the generated trajectory. The framework further employs a generator\ntrained using a combination of Mean Squared Error (MSE) loss, differences in features extracted by a VGG\nnetwork, amplitude loss, and adversarial loss to learn and produce smooth and context-aware camera trajectories.\nThe paper Adaptive Auto-Cinematography in Open Worlds [Yu et al .2023a] addressed the unique challenges\nof user interaction in video games. Unlike traditional cinematographic approaches that emphasize cinematic\nrules, this method prioritized user interaction and the dynamic nature of open-world environments. The study\nhighlighted the limitations of example-driven methods, particularly their inability to adapt to the uncertainty of\ntargets, such as the main character in open-world games. To address these challenges, a GAN-based model was\nproposed to incorporate user interaction into the generation of camera trajectories. Additionally, new metrics\nwere developed to evaluate the generated trajectories, accounting for the complexities of the task.\nBuilding on this work, a follow-up study, Automated Adaptive Cinematography for User Interaction in Open\nWorlds [Yu et al .2024], enhanced the initial framework by introducing skeleton poses of the characters and their\nactions as conditions for the GAN model. This addition improved the ability of the model to generate contextually\nadaptive and realistic camera trajectories, further aligning the camera movement with the dynamic interaction of\nusers and characters in open-world settings.\nIn [Xie et al .2023a], a transformer-based approach was proposed for generating camera trajectories and\nmotions in real-time environments. The method operates in two stages: first, it utilizes the performers’ positions\nand orientations, as defined in the stage script, to set the initial placements and postures of the camera for the\nentire sequence. These initial positions serve as keyframes, predetermined by the script. In the second stage,\nthe model uses these keyframes as input to generate smooth camera motion between them, adapting to the\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 32 ---\n32 •Dehghanian et. al.\nFig. 18. A two stage transformer based architecture proposed in [Xie et al. 2023a]\nFig. 19. The architecture proposed in [Jiang et al. 2024b], utilizing diffusion-based models with a transformer architecture.\nlive placements and orientations of the performers. The network architecture integrates a Transformer with\nrelative position encoding, which the authors state enables more effective learning of camera motion features in\ncomparing to standard Transformer architectures. In figure 18\nThe year 2024 represented a turning point with the rise of diffusion models [Ho et al .2020], whose growing\npopularity led to diverse applications ranging from direct use in generating camera trajectories [Courant et al .\n2025; Jiang et al .2024b; Li et al .2024] to indirect uses such as creating images with specific camera shot types\n[Massaglia et al. 2024].\nA study extending the work of [Jiang et al .2021] was presented at the Eurographics conference [Jiang et al .\n2024b], introducing the use of diffusion-based models for camera trajectory generation for the first time. This\nsystem is capable of generating camera movements based on a complete or partial prompt that includes all or\npart of the standard framing, angle, and motion features, along with optional key points defined by the user at\nthe beginning and end of the trajectory.\nIn this architecture, the CLIP model [Radford et al .2021] is used to encode textual descriptions, which are\nthen combined with key point information. Unlike their previous studies [Jiang et al .2021, 2020] that relied on\nLSTM-based architectures, this method employs a diffusion-based model with a transformer architecture at each\nstep of the generation process. The proposed architecture of this study is illustrated in Figure 19.\nAnother study, published in 2024 under the title E.T. [Courant et al .2025], introduced a new dataset for camera\ntrajectory generation along with proposing three diffusion-based architectures.\nThe first proposed architecture, \"Director A\", utilizes a relatively simple approach to apply conditions; Here,\ntextual descriptions and the subject’s trajectory are added as context tokens to the transformer’s input. In the\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 33 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •33\nFig. 20. Architectures proposed in [Courant et al. 2025]\nsecond architecture, \"Director B\", the conditions are concatenated into a single token vector and these vectors are\nthen used to adjust AdaLN parameters before each self-attention and feed-forward layer.\nIn the final model, \"Director C\", the CLIP prompt embeddings and the subject’s trajectory are combined and\nprocessed through two transformer encoder layers. This information is then applied to the main model via a\ncross-attention block, enabling the use of more intricate patterns in the conditions. These three architecture is\nillustrated in Figure 20:\nIn an upcoming study, LensCraft [Dehghanian et al .2025] tries to solve three critical challenges in virtual\ncinematography. First, it introduces a comprehensive cinematographic language paired with a dedicated simulation\nframework to generate balanced, high-quality, controlled training data through expert consultation - addressing\nthe persistent issue of dataset bias and quality in existing systems. Second, it presents a dual-level representation\nsystem, allowing simultaneous conditioning on multiple inputs (text, keyframes, and reference trajectories) while\nmaintaining cinematographic integrity. Also, the model’s leverage progressive masking strategy and CLIP-based\nembedding approach enable it to learn meaningful interpolations between different camera movements while\npreserving semantic coherence.\nNext paper [Wang et al .2024a] specifically focused on generating camera movements for Dance scenes, intro-\nducing a novel approach that combines musical information with the subject’s motion to produce synchronized\nand context-aware camera trajectories.\nThe proposed architecture like previous models [Courant et al .2025; Jiang et al .2024b], utilizing a combination\nof transformer models and diffusion networks. Musical data and the subject’s pose are embedded and combined,\nand then used this embedding in the transformer’s cross-attention blocks. The model’s final architecture consists\nof multiple sequential transformer decoders that execute the diffusion denoising process to generate the final\ncamera trajectory. For conditioning, the model employs a Classifier Free Guidance-based approach [Ho and\nSalimans 2022], which is a well-known method for conditioning diffusion-based models. The architecture of the\nDCM model proposed in this research is illustrated in Figure 21.\nA recent continuation of the DCM model introduced the DanceCamAnimator framework [Wang et al .2024b],\ndesigned to address the limitations of the previous model by incorporating support for keyframing. This framework\nadopts a three-stage approach for generating camera movements in the context of music and dance, utilizing\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 34 ---\n34 •Dehghanian et. al.\nFig. 21. The DCM model architecture, based on a combination of transformer and diffusion networks [Wang et al. 2024a].\nFig. 22. The architecture proposed in [Wang et al. 2024b] for modeling keyframes.\nanimator expertise to identify and produce keyframes as well as predict tween functions and tries to reduce the\nneed for post-processing.\nIn the first stage, the model identifies camera keyframes by analyzing subject movements, musical representa-\ntion, and the temporal history of key points to determine critical moments for significant camera adjustments.\nIn the second stage, the model generates the camera’s position and movement for these keyframes. Finally, in\nthe third stage, it predicts tween function values for in betweening keyframes to ensure smooth and natural\ntransitions between them. Figure 22 depicts the stages of the DanceCamAnimator framework.\nJawad et al. [Jawad et al .2024] explored camera control in robotic surgery by utilizing both dense neural\nnetwork (DNN) and recurrent neural network (RNN) architectures trained on combined datasets of autonomous\nand human-operated camera trajectories [Jawad et al .2024]. Unlike previous single-mode approaches, their\nmethod learned to merge the predictable behavior of rule-based systems with the adaptive nature of human\noperation, achieving the advantages of both. The DNN architecture demonstrated proficiency in basic tool\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 35 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •35\ntracking, while the RNN, excelled at learning timing-based camera zooming and complex motion patterns and\nachieved sub-millimeter accuracy, suggesting superior performance in real surgical scenarios where precise\ncamera control is crucial.\nSome works address camera trajectory generation not as their primary focus but as a secondary or complemen-\ntary task integrated within their frameworks to address other problems. The remainder of this section reviews\nthese works.\nAmong these works Director3D [Li et al .2024] is a framework that integrates camera trajectory generation as\npart of a text-to-3D video generation process. The system, Director3D, begins by utilizing a Trajectory Diffusion\nTransformer [Peebles and Xie 2023] to model the distribution of camera trajectories from textual prompts. This\nphase, referred to as the \"Cinematographer\" step, generates adaptive camera paths tailored to the scene described\nin the input prompt. The generated trajectories serve as the input for subsequent steps, which involve creating a\n3D scene and aligning it with the predefined camera motion.\nAnother framework that incorporates camera trajectory generation within a broader video generation task\nis MotionCtrl [Wang et al .2024c], which introduces a Camera Motion Control Module to effectively handle\ncamera movements. This module extends the Denoising U-Net structure of the Latent Video Diffusion Model [He\net al.2022] by integrating camera pose into second self-attention module and applying a fully connected layer\nto extract temporal features. These modifications allow the model to conditionally generate videos where the\nbackground and object movements align with the specified camera poses and trajectories.\nThe work in [Xie et al .2023b] addresses the task of generating aesthetically pleasing camera trajectories in\nsynthetic 3D indoor scenes. The proposed method, GAIT, is a Deep Reinforcement Learning (DRL) framework\nthat optimizes camera movements in a 5D space using a neural aesthetic model trained on crowd-sourced data. It\nemploys a reward function integrating aesthetic evaluation, temporal smoothness, and diversity regularization\nto ensure smooth and diverse trajectories. GAIT uses visual DRL algorithms like DrQ-v2 [Zhou 2024] and\nCURL [Laskin et al .2020], leveraging data augmentation and contrastive learning to efficiently generate visually\nappealing and contextually diverse camera paths.\nAnother approach addressing camera trajectory generation within a text-to-video framework is Direct-a-Video\n[Yang et al .2024]. This model incorporates camera position generation by encoding three parameters: horizontal\npan, vertical pan, and zoom ratio. The horizontal and vertical pan values are encoded using a Fourier embedder,\nwhile the zoom ratio directly passed through MLPs and then the resulting embeddings are combined to represent\nthe camera movement in a temporal cross-attention mechanism to guide the generation of video sequences\naligned with the specified camera movements and object interactions.\nNext work integrates camera trajectory generation within a broader application is CinePreGen [Chen et al .\n2024b]. This work introduces a previsualization framework and new coordinate system, CineSpace. This Space is\nbased on Toric allows users to control camera movements for storyboarding purposes. Their framework offers\n15 common rule-based options for defining camera trajectories. The camera dynamics are further enhanced by\nincorporating multi-masked IP-Adapter techniques and engine simulation, ensuring alignment with ground truth\ninformation throughout the rendering process.\nLiu et al. [Xu et al .2024] present a method for generating camera-controllable, geometry-consistent videos by\nintegrating camera control into a pre-trained image-to-video diffusion model. They use Plücker coordinates for\n6-DoF camera parameterization, enabling dynamic viewpoint adjustments across frames. A key innovation is the\nepipolar constraint attention mechanism, which ensures geometric consistency by aligning features between\nframes. The model is fine-tuned from Stable Video Diffusion (SVD), incorporating temporal noise scheduling\nand classifier-free guidance to maintain high-quality, temporally consistent videos while adhering to specified\ncamera trajectories.\nThe approach introduced in [Kuang et al .2024] builds upon CameraCtrl [He et al .2024] and the consistency\nmodel from [Tseng et al .2023], proposing a method for generating synchronized multi-view videos. The key\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 36 ---\n36 •Dehghanian et. al.\ninnovation is the Cross-View Synchronization Module (CVSM), which uses masked attention and fundamental\nmatrices to ensure structural consistency across video frames. This enables the model to generate temporally\ncoherent videos from different camera trajectories while maintaining alignment across views. The model is\ntrained on pairs of videos, leveraging datasets such as RealEstate10K and WebVid10M.\nDreamCinema [Chen et al .2024a] is another framework that incorporates camera trajectory as part of a\nbroader cinematic transfer process. This framework focuses on simplifying film creation by allowing camera\nmovement transferring from source video and 3D character integration. It extracts camera trajectories from\nreference videos and optimizes them using motion-aware guidance and physical modeling with Bézier curves\n[Zhang 1999]. The framework then continues its process to generate a new video, where the transferred camera\nmovement is applied seamlessly to the newly created scenes.\nThe work in [Bar et al .2024] addresses the task of camera trajectory generation for navigation in both known\nand unknown environments. It introduces the Navigation World Model (NWM), a machine learning-based\napproach that uses a novel Conditional Diffusion Transformer (CDiT) [Bar et al .2024]. The NWM predicts\nfuture visual states based on past observations and navigation actions, allowing for the simulation of trajectories\nto achieve specified goals. The CDiT, a diffusion-based autoregressive model, is trained on diverse egocentric\nvideo datasets from human and robotic agents. Unlike standard diffusion transformers (DiTs), which compute\nself-attention over all input tokens with quadratic complexity, the CDiT employs a cross-attention mechanism\nfor conditioning on past frames, reducing computational complexity to linear with respect to the number of\ncontext frames.\nThe field of camera trajectory generation has witnessed remarkable progress through machine learning\napproaches, evolving from basic statistical models to sophisticated deep learning architectures. The transition\nfrom LSTM-based models to transformer architectures, and most recently to diffusion-based approaches, has\nsignificantly enhanced the quality and controllability of generated trajectories. These advancements have enabled\nmore natural, context-aware camera movements while providing flexible conditioning mechanisms through text\nprompts, keyframes, and multi-modal inputs.\nThese approaches to camera trajectory generation offer several compelling advantages while facing certain\nnotable challenges. On the positive side, these methods excel at learning complex cinematographic patterns\ndirectly from professional examples, capturing nuanced camera behaviors that would be difficult to encode\nthrough explicit rules. They also demonstrate remarkable adaptability, automatically adjusting to various scenes\nand contexts without requiring manual parameter tuning, and can generate diverse, creative camera movements\nthat go beyond predefined templates.\nHowever, these benefits come with significant trade-offs: the models typically require large datasets of high-\nquality camera trajectories for training, which are often expensive and challenging to obtain. Additionally,\ncomputational costs can be substantial, particularly for sophisticated architectures like diffusion models, making\nreal-time applications challenging. Perhaps most importantly, these approaches often struggle with long-term\nplanning and maintaining global coherence over extended sequences, a crucial aspect of professional cinematog-\nraphy that traditional methods sometimes handle more effectively.\nMachine learning has revolutionized camera trajectory generation by enabling data-driven approaches that\nlearn from examples, providing flexibility and adaptability beyond traditional methods. Deep learning models\nintegrate cinematic principles while adapting to complex constraints, facilitating creative and diverse trajectory\ngeneration. These methods, detailed in Table 3, represent a paradigm shift, with generative models and neural\nrendering leading to significant advancements in camera trajectory generation.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 37 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •37\nTable 3. Overview of Machine Learning Methods for Camera Trajectory Generation Methods\nMethod Real World Virtual Metric Dataset\n[Chen et al. 2016a] Human-Based - Qual Not-Public\n[Huang et al. 2019] Human-Based - Qual (User Study) Gathered from internet\n[Wang et al. 2020] Areal-Based - MO - MVD Sports-360 - Pano2Vid\n[Kyrkou 2020] Human-Based - Motion Error - FPS Generated(Not-Public)\nTarget Tracking\n[Jiang et al. 2020] - Animation Accuracy - MA Synthetic\n[Jiang et al. 2021] Human-Based Animation Silhouette Distance Extracted From MovieNet\nTrajectory Distance\n[Styles et al. 2021] Human-Based - SIOU - Average Precision WNMF\nADE - FDE\n[Yu et al. 2022b] - Animation Accuracy Not-Public\n[Xie et al. 2023a] Human-Based - MSE - Qual MikuMikuDance(MMD)\n[Yu et al. 2023a] - Games MSE - Correlation Distance Not-Public\nQual - Multifocus\n[Wu et al. 2023] Human-Based Animation MSE - RoTSft - AdjDis Synthetic - Artist Design\nHausdorff Distance - CosDA\nLPIPS - FID - VisAcc - PCC\nSRCC - KRCC - AVA\n[Yu et al. 2024] - Games MSE - Correlation Distance MineStory\nQual - Multifocus\n[Massaglia 2023] Human-Based - CLIP-T Score - DINO - Qual Not-Public\n[Xie et al. 2023b] Human-Based - Aesthetic Score - Qual Replica\nTraining time - Avg Reward\n[Courant et al. 2025] Human-Based - CLaTr-score - P - R - C - D ET\nFDCLaTr - Qual\n[Dehghanian et al. 2025] Volume-Based Animation FID - P - R - C - D Synthetic\nClip-score - Qual\n[Li et al. 2024] Human-Based - NIQE - BRISQUE - Qual MVImgNet - DL3DV-10K\n[Jiang et al. 2024b] - Animation R Precision FID - Diversity Synthetic\nQual - MultiModality\n[Chen et al. 2024b] Human-Based - Qual Not-Public\n[Chen et al. 2024a] - Animation PA - IoU - MPJPE - Qual Not-Public\n[Wang et al. 2024a] Human-Based Animation FID - Qual DCM\nEuclidean Distance\n[Wang et al. 2024b] Human-Based Animation FID - Qual DCM\n[Yang et al. 2024] Human-Based - Flow Error Metric - Qual Synthetic from MovieShot\n[Wang et al. 2024c] Human-Based - FID - FVD Realestate10k for Camera\nQual WebVid for Object Trajectory\n[Xu et al. 2024] Human-Based - FID - FVD - Pose accuracy WebVid\nCOLMAP error rate\n[Kuang et al. 2024] Human-Based - FID - KID - CLIP-T - CLIP-F WebVid10M, RealEstate10K\nRotation AUC - Transition AUC\nQual\n[Jawad et al. 2024] - - ROS Latency Published in\nBase Prediction Time [Eslamian et al. 2020]\n[Hou et al. 2024] Human-Based - FVD - FID - IS - ATE Not Public\nCLIP-SIM - RPE-T - RPE-R\n[Bar et al. 2024] Human-Based - FVD - FID- PSNR - DreamSim SCAND - TartanDrive - RECON\nLPIPS - RPE - ATE HuRoN- Ego4DitHub\nNote: All the entries are entered based on evidence or our evaluation. (Qual = Qualitative), Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 38 ---\n38 •Dehghanian et. al.\n4.4 Hybrid\nMany problems in camera trajectory generation are approached by integrating multiple methods or combining\ndifferent strategies to achieve better results. These approaches, often referred to as hybrid methods, leverage a\nmix of concepts and assumptions to optimize performance [Liu et al .2024c]. While some hybrid methods directly\ngenerate camera trajectories by producing a sequence of coordinates to position the camera in space, others take\nan indirect approach [Hu et al .2024; Kirillov et al .2023]. In the indirect case, the method does not output the\ntrajectory itself [Azzarelli et al .2024], but instead generates products related to the trajectory or derived from.\nThis section reviews various proposed methods that utilize a combination of techniques to either directly or\nindirectly generate camera trajectories within camera control systems.\nThe first notable approach was proposed by Bares et al. [Bares et al .2000] that introduced an environment for\ncreating storyboard frames, known as the storyboard frame editor interface. The objective of model is to position\nthe camera in a virtual 3D environment to realize the storyboard frame. This work does not explicitly deal with\nlinguistic descriptions of the constraints; instead, the constraints are implicitly represented in the storyboard\nframes.\nA hybrid method for adaptive virtual camera control in computer games is presented in [Burelli and Yannakakis\n2011], aiming to enhance player experience by automatically adjusting the camera based on real-time gameplay\nconditions. This hybrid approach combines rule-based and machine learning techniques, inspired by gaze data\ncollection methods [Bernhard et al .2010] but adapted to model the interplay between camera behavior, gameplay\ncharacteristics, and player actions. The process involves two steps: first, k-means clustering is used to group\ngaze-based data into distinct camera behaviors, iteratively adjusting clusters based on validity measures. Second,\nneural networks predict appropriate camera behaviors for different game areas, enabling nuanced and adaptive\ncamera control tailored to player actions.\nThis study was later improved in [Burelli and Yannakakis 2015] by replacing SVR and RF learning methods\nin [Burelli and Yannakakis 2011] with neural networks to model the relationship between player and camera\nbehaviors more effectively. This advancement focused on predicting suitable camera profiles for future game\nsegments, further enhancing the system’s adaptability.\nIn subsequent work, a comprehensive survey on game cinematography systems was conducted [Burelli 2016],\naddressing the design principles and methods for developing cinematic virtual camera control systems.\nKim et al. [Kim et al .2012] proposes a method to detect regions of interest (ROIs) in dynamic scenes with PTZ\ncameras 3.1, such as sports videos, addressing inefficiencies of prior Radial Basis Function (RBF) methods [Kim et al .\n2010]. By using Gaussian Process Regression (GPR) [Kim et al .2011], the method constructs a stochastic motion\nfield to capture global motion tendencies and filter low-certainty regions, improving robustness and efficiency.\nAs illustrated in Figure 23, the GPR-based approach aligns predicted ROIs with actual camera movements more\neffectively, reducing computational overhead while requiring hyper-parameter tuning for optimal performance.\nFig. 23. The convex hull formed by the player locations and merging points (red lines) indicates the field of view determined\nby GPR. [Kim et al. 2012].\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 39 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •39\nThe method in [Chen and Carr 2015] predicts the pan angle of a PTZ camera 3.1 based on player tracking data\nfrom basketball games, aiming to replicate human camera operator decisions. It combines multiple regression\ntechniques—linear least squares [Björck 1990], support vector regression [Smola and Schölkopf 2004], and random\nforest regression [Biau and Scornet 2016]—with feature vectors derived from player positions, heat maps, and\nspherical maps [Chen and Carr 2015]. These inputs enable the learning algorithms to accurately predict camera\nmovements, ensuring effective tracking of dynamic scenes.\nAn autonomous drone cinematography system is proposed in [Huang et al .2018], designed to generate camera\ntrajectories for action scenes by dynamically tracking human subjects. As shown in Figure 24, the system detects\n2D skeleton keypoints using stereo cameras and OpenPose [Cao et al .2017], refining 3D poses with polynomial\nregression [Heiberger et al .2009] for temporal consistency and smoothness. Camera viewpoints are selected\nbased on predicted poses, and trajectories are optimized using polynomial functions while adhering to drone\nconstraints such as velocity, acceleration, and safety distances. Real-time re-evaluation ensures continuous,\nfeasible motion that integrates aesthetic and physical constraints.\nFig. 24. Overview of ACT system for cinematography [Huang et al. 2018].\nAn autonomous drone cinematography system capable of generating camera trajectories for action scenes by\nimitating human filming techniques is introduced in [Huang et al .2019]. As shown in Figure 25, the framework\nconsists of three modules: feature extraction, prediction network, and camera motion estimation. Features such\nas subject optical flow, background information, and prior camera motions are extracted from video frames. A\nSeq2Seq ConvLSTM network [Chen et al .2015] predicts future camera and subject motions using these features.\nThe predicted optical flow is then used to estimate real-time camera motion, ensuring smooth subject tracking\nand appropriate composition throughout filming.\nThe [Gschwindt et al .2019] addresses automating drone camera trajectory generation for aesthetic aerial\ncinematography by replacing human input with a deep reinforcement learning (RL) agent. The agent uses a\nstate representation (2.5D height maps, shot type, and repetition count) to select shot modes (e.g., left, right,\nfront, back) and optimizes for rewards based on shot angle, actor presence, shot duration, and collision avoidance.\nTraining combines hand-crafted and human-driven rewards in Microsoft AirSim simulations, generalizing to\nreal-world tests. 26 illustrates the RL framework, where the agent learns to generate smooth and visually pleasing\ntrajectories autonomously.\nIn the next work [Bonatti et al .2021] an intuitive interface is developed for controlling aerial cinematography by\nlearning a semantic control space. The approach begins by generating diverse video clips based on minimal shot\nparameters, such as distance and tilt angle, which are then rated by participants to derive semantic descriptors.\nThese descriptors form a reduced semantic space, enabling users to control the robot’s camera motion intuitively\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 40 ---\n40 •Dehghanian et. al.\nFig. 25. Imitation learning framework featuring three key modules [Huang et al. 2019].\nFig. 26. Overall System Flow of [Gschwindt et al. 2019].\nduring deployment. By manipulating these high-level descriptors, users achieve natural camera control while\nmaintaining a strong link between camera movements and the emotional content of the shot.\nThe approach in [Burg et al .2021] addresses real-time cinematic tracking in dynamic environments, focusing\non generating smooth camera animations that follow a target’s motion while avoiding occlusions and collisions.\nIt anticipates the target’s behavior using a simulated motion curve and selects a goal camera viewpoint based\non predicted positions and prioritized viewpoints. Candidate trajectories are then generated and evaluated for\nsmoothness, continuity, and collision avoidance. The method dynamically adjusts camera paths based on scene\ngeometry, ensuring real-time adaptability and cinematic quality.\nThe methodology further improved in [Burg 2022] by incorporating physics-based simulations to model the\ntarget’s behavior and predicting future positions and Additionally, leveraging GPU-based computations for\nefficient ray casting and collision detection, significantly speeding up the evaluation of camera animations.\nA camera control system capable of making cinematographic decisions by learning from movie data is proposed\nin [Litteneker 2022]. The system tackles the challenge of matching virtual camera movements to dynamic scenes\nwith multiple actors by balancing factors like positions, angles, and relative motion to ensure aesthetically pleasing\nshot composition. Machine learning models are employed to learn a distance metric quantifying the similarity\nbetween desired intent and potential compositions. Optimization techniques then determine the optimal camera\npositions to achieve the user’s cinematographic goals, even under complex scene dynamics.\nThe method in [Wang et al .2023a] transfers cinematic features such as camera motion, focal length, and timing\nfrom a reference video to a newly generated one. As shown in Figure 27, it optimizes extrinsic and intrinsic\ncamera parameters using the differentiability of neural representations through the Neural Radiance Fields\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 41 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •41\n(NeRF) network [Lin 2024; Zhu et al .2023]. By refining cinematic features via backpropagation with guidance\nmaps and optical flows, the approach ensures the generated video closely matches the visual style and motion\ncharacteristics of the reference clip.\nFig. 27. Overview of JAWS pipeline [Wang et al. 2023a].\nThe [Ye et al .2023] addresses the task of reconstructing global human trajectories in a shared world frame\nfrom in-the-wild videos by decoupling human and camera motion. The proposed method, SLAHMR, estimates\nrelative camera motion using SLAM and initializes human and camera trajectories through 3D human tracking. It\nthen optimizes these trajectories by leveraging 2D video observations and learned human motion priors, aligning\ncamera displacement with plausible human motion to resolve scene scale ambiguity. The process, depicted in 28,\nenables 4D trajectory recovery even in challenging, multi-person scenarios.\nFig. 28. SLAHMR Framework [Ye et al. 2023].\nThe approach in [Jiang et al .2024a] tackles the challenges of estimating camera trajectories and character\nmotion in complex dynamic scenes, particularly where traditional methods like SLAM [Durrant-Whyte and\nBailey 2006] struggle with dynamic elements and 3D representations. As shown in Figure 29, the method employs\nNeRF and pose estimation [Zheng et al .2023] as a differentiable renderer to estimate camera trajectories and\ncharacter motion. It refines character motion using the Skinned Multi-Person Linear (SMPL) [Loper et al .2015]\nhuman body model, effectively integrating neural rendering with motion tracking techniques for precise 3D\nresults.\nThe method in [Hu et al .2024] addresses the challenge of efficient camera motion control in video generation,\nreducing the need for extensive training and computational resources. It employs a one-shot camera motion\ndisentanglement technique to separate camera motion from object motion in a source video. The disentangled\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 42 ---\n42 •Dehghanian et. al.\nFig. 29. Overview of the approach in [Jiang et al. 2024a].\ncamera motion is then transferred to a new video, enabling flexible and resource-efficient camera control without\nthe need for complex temporal camera module training.\nThe proposed model is designed to extract camera motion from either a single video or multiple videos with\nsimilar camera motions. This process is illustrated in Figure 30. First) One-shot camera motion disentanglement:\nThe method begins by employing SAM [Kirillov et al .2023] to segment moving objects in the source video and\nextract temporal attention maps from inverted latents. To separate camera motion from object motion, object\nregions in the attention map are masked, and camera motion within the mask is estimated by solving a Poisson\nequation. Second) Few-shot camera motion disentanglement: In cases involving multiple videos, the model\nextracts common camera motion from temporal attention maps across the given videos. For each position (x, y),\nk-neighboring attention map values across videos are clustered, and the centroid of the largest cluster is used to\nrepresent the camera motion at that position.\nFig. 30. Main framework of [Hu et al. 2024] method [Hu et al. 2024].\nThe SplaTraj framework, introduced in [Liu et al .2024c], generates photogenic camera trajectories within\nenvironments represented by Gaussian Splatting models. It formulates the task as a trajectory optimization\nproblem guided by user-specified semantic instructions. By integrating rendering-based costs such as target\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 43 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •43\nTable 4. Overview of Hybrid Methods for Camera Trajectory Generation Methods\nMethod Real World Virtual Camera Movement\n[Burelli and Yannakakis 2011] - Game Fixed\n[Burelli and Yannakakis 2015] - Game Fixed\n[Kim et al. 2012] Human-Based - PTZ\n[Chen and Carr 2015] Human-Based - PTZ\n[Burelli 2016] - Game -\n[Huang et al. 2018] Areal-Based - Gimbal Mounted\n[Huang et al. 2019] Areal-Based - Gimbal Mounted\n[Gschwindt et al. 2019] Areal-Based - Gimbal Mounted\n[Bonatti et al. 2021] Areal-Based - Gimbal Mounted\n[Burg et al. 2021] Areal-Based - Gimbal Mounted\n[Burg 2022] Areal-Based Animation/Games Gimbal Mounted\n[Litteneker 2022] - Animation/Games Non-Fixed\n[Wang et al. 2023a] Human-Based - -\n[Ye et al. 2023] Human-Based - -\n[Jiang et al. 2024a] Human-Based - -\n[Hu et al. 2024] Human-Based - -\n[Liu et al. 2024c] Human-Based - -\nNote: All the entries are entered based on evidence or our evaluation.\ncentering and ratio error, the method achieves smooth, object-centered views. Empirical evaluations highlight\nimprovements in object placement, trajectory smoothness, and occlusion avoidance, advancing semantic-driven\nvideo generation within photorealistic environments.\nHybrid methods in camera trajectory generation offer several advantages by integrating multiple approaches,\nallowing for greater flexibility and efficiency in solving complex problems. These methods combine different\ntechniques, such as machine learning, optimization, and neural rendering, to tackle challenges like dynamic\nscene tracking, real-time adaptation, and generating natural camera movements. However, hybrid methods also\ncome with challenges, such as the need for high computational resources, complex parameter tuning, and the\nintegration of diverse techniques that may not always align seamlessly. Despite these obstacles, the field of hybrid\ncamera trajectory generation is still an area of active research, with significant potential for further improvements.\nAs technologies like Neural Radiance Fields and DL continue to evolve, new opportunities for hybrid methods to\nenhance camera control systems in dynamic environments are emerging.\nHybrid methods combine rule-based, optimization, and machine learning techniques to achieve greater flexibil-\nity and efficiency in solving complex trajectory generation problems. These approaches address challenges like\ndynamic scene tracking and real-time adaptation, leveraging strengths across methodologies. Table 4 illustrates\nvarious hybrid strategies, including direct trajectory generation and indirect methods.\n5 METRICS\nAfter gaining a thorough understanding of camera trajectory generation methods, it becomes necessary to\nevaluate their performance in order to assess the effectiveness of the underlying approaches. This evaluation\nrelies on a comprehensive set of metrics that account for all relevant aspects of the camera trajectory. Metrics\nplay a crucial role in this process by providing objective and reproducible standards for assessing the quality and\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 44 ---\n44 •Dehghanian et. al.\nfunctionality of generated trajectories. The methods employed for camera trajectory evaluation can be classified\ninto general and specific metrics. Since a camera trajectory defines the path and orientation a camera follows\nthrough a scene, it significantly influences how visual narratives are communicated and perceived. Without\nstandardized metrics, comparisons between different trajectory generation methods would remain inconsistent\nand inherently subjective.\nCamera trajectory generation shares similarities with sequence analysis, as it involves evaluating temporal\ndependencies and continuity, akin to time series analysis. Techniques such as statistical correlation [Heusel et al .\n2017; Unterthiner et al .2018] and predictive modeling [Radford et al .2021; Yang et al .2024] can be adapted to\nassess trends and coherence in the generated trajectories, ensuring spatial consistency and enhancing audience\nengagement. These techniques can be considered as general metrics.\nHowever, beyond these general methods of sequence analysis, comprehensive evaluation of camera trajectories\nrequires domain-specific criteria [Courant et al .2025]. The need for specialized metrics arises from the inherently\nmultifaceted nature of these trajectories, which are influenced by various factors [Müller 2007]. This necessity\nstems from the fact that camera trajectories are shaped by diverse aspects, including cinematic principles, temporal\ncharacteristics, interactions between scene components, and user prompts [Naeem et al .2020]. Consequently,\nthere is a need for metrics capable of adequately addressing these complexities.\nDespite the significant efforts devoted to developing purpose-specific metrics for evaluating particular aspects\nof camera trajectories, there remains a notable absence of general-purpose metrics capable of assessing all aspects\nof a camera trajectory comprehensively. As a result, qualitative evaluation methods continue to play a substantial\nrole in this field.\nThe rest of this section is dedicated to quantitative and qualitative assessments. Quantitative metrics involve\nnumerical evaluations, such as trajectory smoothness measured by minimizing jerk [Galvane et al .2018] or\nacceleration variance [Nägeli et al .2017a]. Qualitative metrics, conversely, assess subjective aspects like the\nemotional impact [Bonatti et al. 2021] of a trajectory or its alignment with storytelling goals [Wu et al. 2018].\n5.1 Quantitative Metrics\n5.1.1 Peak Signal-to-Noise Ratio [Korhonen and You 2012; Moreno et al. 2013].\nPeak Signal-to-Noise Ratio (PSNR) quantifies image or video quality by comparing a reconstructed version to\nthe original. It expresses the maximum possible signal power relative to noise in logarithmic decibels (dB), with\nhigher values indicating better quality.\nPSNR =10·log10\u0012MAX2\nMSE\u0013\n(19)\nMSE=1\n𝑛𝑛∑︁\n𝑖=1(𝑥𝑖−ˆ𝑥𝑖)2(20)\nWhere MAX is the maximum possible pixel value.\n5.1.2 Structural Similarity Index [Brunet et al. 2011].\nThe Structural Similarity Index (SSIM) is a perceptual metric used to evaluate the similarity between two images.\nIt assesses image quality based on structural information, luminance, and contrast, making it more aligned with\nhuman visual perception than traditional metrics like mean squared error.\nThe formula for SSIM is given by:\nSSIM(𝑥,𝑦)=(2𝜇𝑥𝜇𝑦+𝐶1)(2𝜎𝑥𝑦+𝐶2)\n(𝜇2𝑥+𝜇2𝑦+𝐶1)(𝜎2𝑥+𝜎2𝑦+𝐶2)(21)\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 45 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •45\nWhere:\n•𝜇𝑥: Mean of image 𝑥.\n•𝜇𝑦: Mean of image 𝑦.\n•𝜎2\n𝑥: Variance of image 𝑥.\n•𝜎2\n𝑦: Variance of image 𝑦.\n•𝜎𝑥𝑦: Covariance between images 𝑥and𝑦.\n•𝐶1and𝐶2: Small constants to stabilize the division when the denominator is close to zero.\n5.1.3 Dynamic Time Wrapping [Müller 2007; Senin 2008].\nDynamic Time Warping (DTW) is a widely used algorithm for measuring the similarity between two temporal\nsequences that may vary in time or speed. Unlike simple distance metrics such as the Euclidean distance, DTW\ncan handle time-series sequences that are misaligned due to temporal distortions. The core idea is to find an\noptimal alignment between two sequences by allowing non-linear mapping of time indices while minimizing a\ncumulative distance.\nGiven two time series 𝑋={𝑥1,𝑥2,...,𝑥𝑁}and𝑌={𝑦1,𝑦2,...,𝑦𝑀}, where𝑥𝑖,𝑦𝑗∈R, the DTW distance is\ncomputed by constructing an 𝑁×𝑀cost matrix𝐷and finding the warping path 𝑃={(𝑖1,𝑗1),(𝑖2,𝑗2),...,(𝑖𝐿,𝑗𝐿)}\nthat minimizes the cumulative cost. The cost matrix 𝐷is defined as:\n𝐷(𝑖,𝑗)=∥𝑥𝑖−𝑦𝑗∥2, (22)\nwhere𝐷(𝑖,𝑗)measures the squared distance between the elements 𝑥𝑖and𝑦𝑗.\nThe warping path 𝑃satisfies the following constraints:\n(1)Boundary Condition :𝑃(1)=(1,1)and𝑃(𝐿)=(𝑁,𝑀).\n(2)Continuity : If𝑃(𝑘)=(𝑖,𝑗), then𝑃(𝑘+1)∈{(𝑖+1,𝑗),(𝑖,𝑗+1),(𝑖+1,𝑗+1)}.\n(3)Monotonicity : The indices 𝑖and𝑗in𝑃must be non-decreasing.\nThe objective of DTW is to minimize the cumulative cost over all valid warping paths:\nDTW(𝑋,𝑌)=min\n𝑃∑︁\n(𝑖,𝑗)∈𝑃𝐷(𝑖,𝑗). (23)\nThe optimal warping path is typically found using dynamic programming. The recurrence relation for the\ncumulative cost matrix 𝐶is given as:\n𝐶(𝑖,𝑗)=𝐷(𝑖,𝑗)+min{𝐶(𝑖−1,𝑗),𝐶(𝑖,𝑗−1),𝐶(𝑖−1,𝑗−1)}, (24)\nwhere𝐶(𝑖,𝑗)represents the cumulative cost up to point (𝑖,𝑗). The final DTW distance is then:\nDTW(𝑋,𝑌)=√︁\n𝐶(𝑁,𝑀). (25)\n•𝑋,𝑌: Input time-series sequences of lengths 𝑁and𝑀, respectively.\n•𝐷(𝑖,𝑗): Local cost between elements 𝑥𝑖and𝑦𝑗.\n•𝐶(𝑖,𝑗): Cumulative cost matrix.\n•𝑃: Optimal warping path.\n5.1.4 CLIP-Score [Radford et al. 2021].\nCLIP-Score ( CLIP-S ) is a reference-free evaluation metric designed for assessing image-caption compatibility by\nleveraging the representations learned by the pre-trained CLIP model. Unlike traditional metrics that rely on\ncomparisons between machine-generated captions and multiple human-authored references, CLIP-Score uses\nonly the image and its candidate caption, aligning closely with how humans evaluate captions. It is computed as:\nCLIP-S(𝑐,𝑣)=𝑤·max(cos(𝑐,𝑣),0), (26)\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 46 ---\n46 •Dehghanian et. al.\nwhere:\n•𝑐,𝑣: Normalized embeddings of the candidate caption and the image, respectively.\n•cos(𝑐,𝑣): Cosine similarity between the embeddings.\n•𝑤: A rescaling factor, typically 𝑤=2.5.\n5.1.5 NIQE [Mittal et al. 2012b].\nThe Natural Image Quality Evaluator (NIQE) is a no-reference image quality assessment metric. It operates in\na completely blind manner, meaning it does not require any prior knowledge of distorted images or human\nopinion scores. Instead, NIQE uses Natural Scene Statistics (NSS) extracted from undistorted natural images to\nevaluate the quality of a given image. This approach makes NIQE distortion-agnostic and \"opinion-unaware,\"\nrelying solely on measurable deviations from the statistical regularities of natural images. NIQE evaluates the\nperceptual quality of frames within trajectories, identifying any unnatural distortions in the generated sequences.\nThis ensures a realistic visual appeal for camera-generated sequences.\nNIQE evaluates image quality based on the multivariate Gaussian (MVG) model and it is described as follows:\n(1)Preprocessing: Local mean removal and divisive normalization are applied:\nˆ𝐼(𝑖,𝑗)=𝐼(𝑖,𝑗)−𝜇(𝑖,𝑗)\n𝜎(𝑖,𝑗)+1, (27)\nwhere𝜇(𝑖,𝑗)and𝜎(𝑖,𝑗)are the local mean and standard deviation, respectively.\n(2)NSS Feature Extraction: NSS features, including parameters of generalized Gaussian distributions (GGD)\nand asymmetric generalized Gaussian distributions (AGGD), are computed from patches.\n(3)Multivariate Gaussian Model: A multivariate Gaussian model is fitted to the NSS features:\n𝑓𝑋(𝑥1,...,𝑥𝑘)=√︄\n1\n(2𝜋)𝑘√︁\n|Σ|exp\u0012\n−1\n2(𝑥−𝜈)𝑇Σ−1(𝑥−𝜈)\u0013\n, (28)\nwhere𝜈andΣare the mean vector and covariance matrix of the pristine natural image corpus.\n(4)Quality Assessment: The quality of a distorted image is expressed as the Mahalanobis distance:\n𝐷(𝜈1,𝜈2,Σ1,Σ2)=√︄\n(𝜈1−𝜈2)𝑇\u0012Σ1+Σ2\n2\u0013−1\n(𝜈1−𝜈2). (29)\n5.1.6 BRISQUE [Mittal et al. 2012a].\nThe Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) is a no-reference image quality assessment\nmetric that quantifies perceptual quality by analyzing deviations from NSS in the spatial domain. Unlike distortion-\nspecific approaches, BRISQUE leverages a distortion-generic framework using locally normalized luminance\ncoefficients.\nThe locally normalized luminance coefficients, ˆ𝐼(𝑖,𝑗), are defined as:\nˆ𝐼(𝑖,𝑗)=𝐼(𝑖,𝑗)−𝜇(𝑖,𝑗)\n𝜎(𝑖,𝑗)+𝐶, (30)\nwhere\n𝜇(𝑖,𝑗)=𝐾∑︁\n𝑘=−𝐾𝐿∑︁\n𝑙=−𝐿𝑤𝑘,𝑙𝐼(𝑖+𝑘,𝑗+𝑙), (31)\n𝜎(𝑖,𝑗)=vut𝐾∑︁\n𝑘=−𝐾𝐿∑︁\n𝑙=−𝐿𝑤𝑘,𝑙(𝐼(𝑖+𝑘,𝑗+𝑙)−𝜇(𝑖,𝑗))2. (32)\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 47 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •47\nThe coefficients ˆ𝐼(𝑖,𝑗)are modeled using a Generalized Gaussian Distribution (GGD):\n𝑓(𝑥;𝛼,𝜎2)=𝛼\n2𝛽Γ(1/𝛼)exp\u0012\n−\u0012|𝑥|\n𝛽\u0013𝛼\u0013\n, (33)\nwhere𝛽=𝜎√︁\nΓ(1/𝛼)/Γ(3/𝛼).\nBRISQUE also models paired product coefficients along four orientations: horizontal, vertical, main diagonal,\nand secondary diagonal, using an Asymmetric Generalized Gaussian Distribution (AGGD).\n5.1.7 Flow Error [Yang et al. 2024].\nTheFlow Error Metric is designed to evaluate the quality of camera movement control in video generation. It\nquantifies the deviation between the optical flow from generated videos and the ground truth flow derived from\nspecified camera movement parameters. Optical flow represents the motion of objects or the camera between\nconsecutive frames, making this metric essential for assessing temporal dynamics and movement consistency.\nThis metric utilizes VideoFlow [Shi et al .2023], an optical flow estimation model, to extract flow maps from\ngenerated videos. The extracted flow maps are compared against the ground truth flow maps, which are computed\nbased on the given camera movement parameters. The Flow Error Metric is defined as:\nFlow Error =1\n𝑁∑︁\n(𝑥,𝑦,𝑡)∥F𝑔(𝑥,𝑦,𝑡)−F𝑟(𝑥,𝑦,𝑡)∥2, (34)\nwhere:\n•F𝑔(𝑥,𝑦,𝑡)represent the optical flow at spatial location (𝑥,𝑦)and time𝑡in the generated video\n•𝑁is the total number of flow vectors (pixels over all frames).\n•F𝑟(𝑥,𝑦,𝑡)denote the ground truth optical flow derived from camera movement parameters\n5.1.8 Average Precision [Zhu 2004].\nThe Average Precision (AP) is a general-propose metric which evaluates the precision-recall trade-off across\nconfidence thresholds, commonly used in object detection and classification tasks. It represents the area under\nthe precision-recall curve.\nLet Precision(𝑟)be the precision at recall 𝑟. The AP is defined as:\nAP=∫1\n0Precision(𝑟)𝑑𝑟, (35)\nwhere the integral is approximated numerically by summing over discrete recall levels. Precision and recall are\ndefined as:\nPrecision =TP\nTP+FP,Recall =TP\nTP+FN, (36)\nwith TP, FP, and FN representing true positives, false positives, and false negatives, respectively.\n5.1.9 Average Endpoint Error [Sharmin and Brad 2012].\nThe Average Endpoint Error (AEE) metric is a quantitative measure used to evaluate the precision of predicted\noptical flows. It assesses the deviation of predicted motion vectors from the ground truth, particularly in the\ncontext of drone cinematography systems. It quantifies the ability of the system to replicate professional filming\nstyles. Lower AEE values signify higher accuracy in the imitation of expert cinematography [Galvane et al .\n2015b].\nThe AEE is mathematically defined as:\nAEE=1\n𝑊.𝐻𝑊∑︁\n𝑖=1𝐻∑︁\n𝑗=1√︃\n(𝑢𝑖,𝑗−𝑢GT\n𝑖,𝑗)2+(𝑣𝑖,𝑗−𝑣GT\n𝑖,𝑗)2, (37)\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 48 ---\n48 •Dehghanian et. al.\nwhere:\n•𝑊and𝐻are the width and height of the optical flow map, respectively.\n•(𝑢,𝑣)and(𝑢GT,𝑣GT)are the predicted and ground-truth optical flow components, respectively.\n•𝑁: Total number of pixels in the optical flow map.\n5.1.10 Precision [Naeem et al. 2020].\nPrecision quantifies the fidelity of the generated data by measuring the proportion of generated samples that lie\nwithin the manifold of real data. It evaluates how realistic the generated samples are with respect to the real data\ndistribution, ensuring that the generative model does not produce artifacts or unrealistic outputs. The manifold of\nreal data is constructed by creating 𝑘-nearest neighbor [Cover and Hart 1967] spheres centered at each real data\npoint. These spheres capture the density and locality of real data points in the feature space. In camera domain, it\nensures that the generated trajectory closely match the fidelity of real-world trajectories. It helps to verify that\nthe model does not produce unrealistic or physically infeasible trajectories.\nPrecision =1\n𝑀𝑀∑︁\n𝑗=11𝑌𝑗∈manifold(𝑋1,...,𝑋 𝑁) (38)\nWhere:\n•𝑀: Number of generated samples.\n•𝑁: Number of real samples.\n•1·: Indicator function, returning 1 if the condition inside holds and 0 otherwise.\n•manifold(𝑋1,...,𝑋𝑁): The union of neighborhood spheres around the real data points.\n5.1.11 Recall [Naeem et al. 2020].\nRecall quantifies the diversity of the generated data by evaluating the proportion of the real data manifold that is\ncovered by the generated samples. This metric ensures that the generative model captures the variability inherent\nin the real data, avoiding mode collapse and ensuring that diverse samples are represented. The recall metric\ndepends on the ability of generated samples to cover the regions of the real data manifold. The 𝑘-nearest neighbor\nspheres around generated samples determine whether real samples are sufficiently represented within these\nspheres. In the context of camera trajectory generation, recall ensures that the generative model produces a\ndiverse set of trajectories that spans the range of possible paths observed in real-world data. This is crucial for\napplications where diversity in camera movement is essential.\nRecall =1\n𝑁𝑁∑︁\n𝑖=11𝑋𝑖∈manifold(𝑌1,...,𝑌 𝑀) (39)\nWhere:\n•𝑁: Number of real samples.\n•𝑀: Number of generated samples.\n•1·: Indicator function.\n•manifold(𝑌1,...,𝑌𝑀): The union of neighborhood spheres around the generated data points.\n5.1.12 Density [Naeem et al. 2020].\nDensity enhances the precision metric by accounting for the relative density of generated samples within the\nreal data manifold. Unlike precision, which evaluates fidelity as a binary outcome, density provides a more\nnuanced measure by considering how densely generated samples populate the neighborhoods of real data\npoints. The parameter 𝑘controls the granularity of the neighborhood estimation. Density rewards regions where\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 49 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •49\nreal samples are densely packed and penalizes overestimation due to outliers. In evaluating camera trajectory\ngeneration, density measures how well the generated trajectories fill the regions of real trajectories. This provides\nan indication of both fidelity and coverage of densely populated areas in real trajectory datasets, which is crucial\nfor applications requiring precision and robustness.\nDensity =1\n𝑘𝑀𝑀∑︁\n𝑗=1𝑁∑︁\n𝑖=11𝑌𝑗∈𝐵(𝑋𝑖,NND 𝑘(𝑋𝑖)) (40)\nWhere:\n•𝑘: Number of nearest neighbors considered.\n•𝑀: Number of generated samples.\n•𝑁: Number of real samples.\n•𝐵(𝑋𝑖,NND𝑘(𝑋𝑖)): Neighborhood sphere centered at 𝑋𝑖, with a radius determined by the distance to its 𝑘-th\nnearest neighbor (NND 𝑘).\n5.1.13 Coverage [Naeem et al. 2020].\nCoverage improves upon the recall metric by focusing on the proportion of real data points that are represented\nin the neighborhoods of generated samples. Unlike recall, which may overestimate due to outliers, coverage\nprovides a robust measure of diversity by assessing whether each real sample has at least one nearby generated\nsample. Coverage requires that for each real data point, there exists at least one generated sample within its\nneighborhood sphere. This metric provides a bounded value between 0 and 1, making it robust to variability in\ndata distributions. Coverage ensures that the generated camera trajectories adequately represent the variability\nin real trajectories. This guarantees that all important modes in real-world trajectories are captured, avoiding the\nexclusion of significant patterns.\nCoverage =1\n𝑁𝑁∑︁\n𝑖=11∃𝑗such that𝑌𝑗∈𝐵(𝑋𝑖,NND 𝑘(𝑋𝑖)) (41)\nWhere:\n•𝑁: Number of real samples.\n•𝑀: Number of generated samples.\n•𝐵(𝑋𝑖,NND𝑘(𝑋𝑖)): Neighborhood sphere around 𝑋𝑖, with radius defined by its 𝑘-th nearest neighbor ( NND𝑘).\n5.1.14 Fréchet Inception Distance [Heusel et al. 2017].\nThe Fréchet Inception Distance (FID) is a metric introduced to evaluate the quality of generative models, particu-\nlarly Generative Adversarial Networks (GANs) [Goodfellow et al .2014], by measuring the similarity between the\ndistributions of generated and real-world data. FID improves upon earlier metrics by comparing the statistical\nproperties of these distributions rather than relying solely on the generated data’s diversity and clarity [Naeem\net al.2020]. Mathematically, FID computes the Wasserstein-2 distance [Vaserstein 1969] between two multivariate\nGaussian distributions: one representing the real data and the other representing the generated data. These\ndistributions are derived from the feature embeddings of the data obtained through a pre-trained Inception-v3\nnetwork [Heusel et al .2017], specifically from its last pooling layer. FID measures the similarity between the\ndistribution of real and generated trajectory frames. Applied to camera trajectory evaluation, it assesses how\nrealistic and visually coherent the generated frames are in comparison to ground-truth sequences. The FID is\ndefined as:\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 50 ---\n50 •Dehghanian et. al.\n𝐹𝐼𝐷(P𝑟,P𝑔)=∥𝜇𝑟−𝜇𝑔∥2\n2+Tr\u0012√︃\nΣ𝑟+Σ𝑔−2\u0000Σ𝑟Σ𝑔\u0001\u0013\n(42)\nwhere:\n•P𝑟,P𝑔are the real and generated data distributions, respectively, derived from the Inception-v3 network,\n•𝜇𝑟,𝜇𝑔: Mean vectors of the embeddings for the real and generated data, respectively.\n•Σ𝑟,Σ𝑔: Covariance matrices of the embeddings for the real and generated data.\n5.1.15 Fréchet Video Distance [Unterthiner et al. 2018].\nThe Fréchet Video Distance (FVD) is a metric designed to evaluate the quality of generative video models by\nmeasuring the distance between the distribution of real videos and the distribution of videos generated by a\nmodel. Introduced in the paper, FVD extends the Fréchet Inception Distance [Unterthiner et al .2018] to account\nfor both spatial and temporal aspects of video data. Unlike frame-level metrics such as PSNR [Korhonen and You\n2012; Moreno et al. 2013] or SSIM [Brunet et al. 2011], FVD evaluates the spatiotemporal consistency of videos.\nLetP𝑔andP𝑔denote the distributions of real and generated videos, respectively. The FVD between these\ndistributions is analogous to the FID, differing only in its parameterization. 𝜇𝑟and𝜇𝑔represent the means of\nthe distributionsP𝑟andP𝑔, capturing both spatial and temporal characteristics of video data. Similarly, Σ𝑟and\nΣ𝑔denote the covariance matrices of P𝑟andP𝑔, respectively, which encode the variability of spatiotemporal\nfeatures within the real and generated video distributions. This metric assumes that the distributions P𝑟andP𝑔\nfollow a multivariate Gaussian distribution in the chosen feature space. The feature representations are extracted\nfrom a pre-trained neural network.\n5.1.16 Fréchet CLaTr Distance [Courant et al. 2025].\nCourant et al. introduced CLaTr (Contrastive Language-Trajectory) embedding which is a robust evaluation metric\ndesigned to assess the alignment between textual descriptions and generated camera trajectories. It leverages\ncontrastive learning to enhance the correlation between language and trajectory data, thereby improving the\naccuracy and reliability of trajectory generation models. The Fréchet CLaTr Distance ( FDCLaTr ) measures the\nsimilarity between the distribution of real and generated camera trajectories in the CLaTr embedding space\n[Courant et al. 2025].\n5.1.17 CLaTr-Score [Courant et al. 2025].\nThe CLaTr-Score evaluates the semantic and geometric alignment between a generated camera trajectory and its\ntextual description. It is calculated as:\nCLaTr-Score =𝑇·𝐶\n∥𝑇∥∥𝐶∥, (43)\nwhere𝑇,𝐶 are normalized embeddings of trajectory and text,\n5.1.18 Visual Continuity [Galvane et al. 2018].\nSmoothness in cinematography refers to the continuity and fluidity of camera motion, characterized by gradual\nchanges in position, velocity, and orientation [Chen et al .2024a]. On the other hand, visual continuity ensures\nseamless transitions between frames by maintaining consistent framing and avoiding abrupt changes in compo-\nsition or perspective, thereby preserving aesthetic and narrative coherence. To achieve visual continuity, the\ncamera trajectory is optimized to minimize deviations from desired framing parameters over time, ensuring\nconsistency in on-screen position, size, and orientation of targets.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 51 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •51\nThe camera must maintain the desired framing of targets, defined by on-screen position (𝑥𝑓,𝑦𝑓), target size𝑠𝑓,\nand orientation 𝑜𝑓. The total cost function combines the framing error and transition smoothness:\n𝐸total=𝑁∑︁\n𝑖=0h\n𝛼𝑝\u0000(𝑥𝑖−𝑥𝑓)2+(𝑦𝑖−𝑦𝑓)2\u0001\n+𝛼𝑠(𝑠𝑖−𝑠𝑓)2\n+𝛼𝑜(𝑜𝑖−𝑜𝑓)2i\n+𝛽𝑁−1∑︁\n𝑖=0(∥¤𝑥𝑖+1−¤𝑥𝑖∥+∥𝑜𝑖+1−𝑜𝑖∥) (44)\nwhere:\n•(𝑥𝑖,𝑦𝑖): Actual on-screen position of the target at frame 𝑖.\n•𝑠𝑖: Actual size of the target at frame 𝑖.\n•𝑜𝑖: Actual orientation of the target at frame 𝑖.\n•𝛼𝑝,𝛼𝑠,𝛼𝑜: Weights for position, size, and orientation terms.\n•𝛽is a weight balancing framing error and smooth transitions.\n5.1.19 Drone-Specific Metrics [Jeon and Kim 2019; Rousseau et al. 2018].\nDrone-based systems require specific metrics to evaluate the performance of camera trajectory generation\naccurately. Ping is utilized to measure communication delay between the drone and control systems, ensuring\nreal-time responsiveness [Bonatti et al .2020b; Galvane et al .2018]. Computation Time is evaluated to determine\nthe latency of trajectory generation algorithms on drone hardware. Energy Efficiency [Bonatti et al .2020b] is\nassessed by analyzing battery consumption in relation to trajectory complexity. Stability Index [Bonatti et al .\n2020b; Galvane et al .2018] quantifies trajectory smoothness to reduce visual disruptions, while Collision Risk\nAssessment evaluates the likelihood of trajectory-induced collisions [Burg 2022; Burg et al .2020]. These metrics\nare generally used for drone-specific performance in cinematography.\nTable 5 summarizes this section by presenting each metric and its corresponding formula, with general metrics\nabove the camera specific metrics.\n5.2 Qualitative Metrics\nQualitative evaluation of camera trajectory generation methods focuses on subjective assessments that capture the\nperceptual and aesthetic quality of the generated trajectories. These metrics complement quantitative measures\nby addressing how well the generated trajectories align with human expectations and professional standards in\npractical applications. In this field, three primary categories of qualitative metrics are recognized and will be\nexplored in the subsequent subsections:\n5.2.1 Visual Comparison. By visually comparing the outputs of a method to a baseline, this approach enables\nevaluators to assess differences in smoothness, framing, and scene coverage [Courant et al .2025]. This straight-\nforward method effectively highlights areas in which the technique demonstrates strengths or weaknesses,\nparticularly in instances where numerical metrics may not adequately capture subtle nuances.\n5.2.2 User Study. User studies gather subjective opinions by asking participants to rank or choose the most\nappealing trajectory among results from different methods [Wang et al .2024a]. These studies provide insights into\ngeneral audience preferences, serving as a reliable indicator of how well a method meets end-user expectations.\n5.2.3 Expert Feedback. Expert feedback involves evaluations from professionals with extensive experience in\ncinematography [Nägeli et al .2017a]. Experts assess trajectories against industry standards, focusing on elements\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 52 ---\n52 •Dehghanian et. al.\nTable 5. Quantitative Metrics\nMetric Trend Formula Introduced in\nPeak Signal-to-Noise Ratio ↑ PSNR =10·log10\u0010\nMAX2\nMSE\u0011\n[Korhonen and You 2012]\nStructural Similarity Index ↑ SSIM(𝑥,𝑦)=(2𝜇𝑥𝜇𝑦+𝐶1)(2𝜎𝑥𝑦+𝐶2)\n(𝜇2𝑥+𝜇2𝑦+𝐶1)(𝜎2𝑥+𝜎2𝑦+𝐶2)[Brunet et al. 2011]\nDynamic Time Warping ↓ DTW(𝑋,𝑌)=min𝑃Í\n(𝑖,𝑗)∈𝑃𝐷(𝑖,𝑗) [Müller 2007]\nCLIP-Score ↑ CLIP-S(𝑐,𝑣)=𝑤·max(cos(𝑐,𝑣),0) [Radford et al. 2021]\nNatural Image Quality Evaluator ↓𝑁𝐼𝑄𝐸(𝜈1,𝜈2,Σ1,Σ2)=√︂\n(𝜈1−𝜈2)𝑇\u0010\nΣ1+Σ2\n2\u0011−1\n(𝜈1−𝜈2) [Mittal et al. 2012b]\nBlind/Referenceless Image Spatial\nQuality Evaluator↓ ˆ𝐼(𝑖,𝑗)=𝐼(𝑖,𝑗)−𝜇(𝑖,𝑗)\n𝜎(𝑖,𝑗)+𝐶[Mittal et al. 2012a]\nFlow Error ↓ Flow Error =1\n𝑁Í\n(𝑥,𝑦,𝑡)∥F𝑔(𝑥,𝑦,𝑡)−F𝑟(𝑥,𝑦,𝑡)∥2 [Yang et al. 2024]\nAverage Precision ↑ AP=∫1\n0Precision(𝑟)𝑑𝑟 [Zhu 2004]\nAverage Endpoint Error ↓ AEE=1\n𝑁Í𝑊\n𝑖=1Í𝐻\n𝑗=1√︃\n(𝑢𝑖,𝑗−𝑢GT\n𝑖,𝑗)2+(𝑣𝑖,𝑗−𝑣GT\n𝑖,𝑗)2 [Sharmin and Brad 2012]\nPrecision ↑ Precision =1\n𝑀Í𝑀\n𝑗=11𝑌𝑗∈manifold(𝑋1,...,𝑋 𝑁) [Naeem et al. 2020]\nRecall ↑ Recall =1\n𝑁Í𝑁\n𝑖=11𝑋𝑖∈manifold(𝑌1,...,𝑌 𝑀) [Naeem et al. 2020]\nDensity ↑ Density =1\n𝑘𝑀Í𝑀\n𝑗=1Í𝑁\n𝑖=11𝑌𝑗∈𝐵(𝑋𝑖,NND 𝑘(𝑋𝑖)) [Naeem et al. 2020]\nCoverage ↑ Coverage =1\n𝑁Í𝑁\n𝑖=11∃𝑗such that𝑌𝑗∈𝐵(𝑋𝑖,NND 𝑘(𝑋𝑖)) [Naeem et al. 2020]\nFréchet Inception Distance ↓𝐹𝐼𝐷(P𝑟,P𝑔)=∥𝜇𝑟−𝜇𝑔∥2\n2+Tr\u0012√︃\nΣ𝑟+Σ𝑔−2\u0000Σ𝑟Σ𝑔\u0001\u0013\n[Heusel et al. 2017]\nFréchet Video Distance ↓𝐹𝑉𝐷(P𝑟,P𝑔)=∥𝜇𝑟−𝜇𝑔∥2+Tr\u0012√︃\nΣ𝑟+Σ𝑔−2\u0000Σ𝑟Σ𝑔\u0001\u0013\n[Unterthiner et al. 2018]\nFréchet CLaTr Distance ↓𝐹𝐷𝐶𝐿𝑎𝑇𝑟(P𝑟,P𝑔)=∥𝜇𝑟−𝜇𝑔∥2+Tr\u0012√︃\nΣ𝑟+Σ𝑔−2\u0000Σ𝑟Σ𝑔\u0001\u0013\n[Courant et al. 2025]\nCLaTr-Score ↑ CLaTr-Score =𝑇·𝐶\n∥𝑇∥∥𝐶∥[Courant et al. 2025]\nVisual Continuity ↓𝐸total=𝐸framing+𝛽Í𝑁−1\n𝑖=0(∥¤𝑥𝑖+1−¤𝑥𝑖∥+∥𝑜𝑖+1−𝑜𝑖∥) [Galvane et al. 2015b]\nNote: Each formula is explained in detail within its corresponding section. Metrics above the horizontal line are general, while those below\nare specific.\nlike visual storytelling, framing techniques, and aesthetic appeal. Their input is invaluable for refining methods\nand ensuring high-quality results.\nTo summarize this section, Table 6 presents the categories of qualitative metrics along with the papers that\nutilize the corresponding metrics for evaluation.\n6 DATASETS\nA significant challenge in camera trajectory generation using deep learning models is the accessibility of high-\nquality, application-specific datasets. Such datasets are essential for training models that can generalize across\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 53 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •53\nTable 6. Qualitative Metrics\nMetric Papers\nVisual Comparison[Courant et al. 2025]\n[Li et al. 2024]\n[Jiang et al. 2024b]\n[Wang et al. 2023a]\n[Yang et al. 2024]\n[Jiang et al. 2024a]\n[Wang et al. 2024c]\n[Hu et al. 2024]\n[Galvane et al. 2014]\n[Louarn et al. 2018]\n[Yoo et al. 2021]\n[Kim et al. 2012]\nUser Study[Wang et al. 2024a]\n[Wu et al. 2018]\n[Guo et al. 2023]\n[Bai et al. 2024]\n[Gebhardt and Hilliges 2021]\n[Chen et al. 2016a]\n[Burelli and GN 2015]\n[Lino et al. 2011]\n[Liang et al. 2012]\n[Bonatti et al. 2021]\n[Wang et al. 2024b]\nExpert Feedback[Nägeli et al. 2017a]\n[Galvane et al. 2018]\ndiverse environments and scenarios, ensuring robustness and reliability. In this section, we explore the types of\ndatasets used in this field, focusing on their strengths and limitations.\n6.1 Synthetic Datasets\nObtaining low-level camera parameters, such as focal length, aperture, and sensor size, along with accurate\ntrajectory data, can be difficult and time-consuming. Beside that, real-world datasets often suffer from imbalances\n[Courant et al .2025], where certain types of camera movements or scene complexities are underrepresented,\nleading to biased models that may not generalize well to diverse real-world scenarios. To address these limitations,\nresearchers have increasingly turned to synthetic datasets, which offer cost-effectiveness, availability, and control\nover data generation. By simulating realistic camera movements, lighting conditions, and scene content, synthetic\ndatasets can provide a rich and diverse source of training data [Burelli and GN 2015; Jiang et al .2020; Wang et al .\n2023a, 2024a].\nHowever, the generalizability of models trained on synthetic data to real-world scenarios remains an open\nquestion. Several studies have explored the use of synthetic datasets for camera trajectory generation, including\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 54 ---\n54 •Dehghanian et. al.\n[Wu et al .2023; Xian et al .2023; Yang et al .2024; Yu et al .2023b]. While these studies have demonstrated\npromising results, further research is needed to evaluate the limitations and biases associated with synthetic data.\nIt is crucial to investigate factors such as the realism of synthetic data, the diversity of training scenarios, and the\ndomain gap between synthetic and real-world data to ensure the effectiveness of models trained on synthetic\ndatasets. In the following, we introduce some of the commonly used synthetic datasets and their applications in\ncamera trajectory generation.\n•Batteries, camera, action! [Bonatti et al .2021]: The dataset used in this study, comprises 200 video\nclips generated within the AirSim photo-realistic simulator. These clips feature a diverse range of aerial shots\nparameterized by spherical coordinates and annotated using minimal perceptual units for shot variations. Semantic\nscores for 15 descriptors, such as \"calm\" or \"exciting,\" were obtained through crowd-sourced pairwise comparisons\ninvolving 500 participants. The dataset’s design emphasizes perceptual and cinematic relevance, facilitating the\ncreation of a semantic control space for mapping descriptors to camera trajectory parameters. This dataset was\nvalidated across simulated and real-world scenarios to ensure robustness and generalizability.\n•CCD [Jiang et al .2024b]: The CCD dataset, is a synthetic collection designed for virtual cinematography,\nfeaturing 25,000 sequences with over 4.5 million frames and 200,000 textual annotations. These annotations\ndescribe key cinematic parameters such as shot angles, scales, and view directions, enabling precise control\nover static, dynamic, and orbit-based camera movements across diverse speeds like slow motion and fast-paced\nsequences. It provides balanced coverage of cinematic styles, making it valuable for training machine learning\nmodels. However, its synthetic nature limits real-world applicability, as it omits dynamic multi-subject interactions,\nbroader narrative contexts, and emotional depth. Textual annotations lack vocabulary richness, and stationary\nsubjects restrict learning intricate camera-subject interactions, reducing adaptability to complex, real-world\nfilmmaking scenarios requiring creative and narrative flexibility.\n6.2 Real Datasets\nReal datasets are critical in training camera trajectory generation models by providing authentic movement\npatterns that capture the subtle dynamics and physical constraints inherent in real-world camera operations.\nUnlike synthetic data, real datasets incorporate natural camera behaviors, scene-specific constraints, and cine-\nmatographic principles that emerge from human operators’ expertise and practical filming considerations. While\nsome datasets focus on high-level cinematographic features such as shot types, camera angles, and motion cate-\ngories [Bruckert et al .2023], this section specifically examines datasets that provide precise camera trajectories\nthrough exact position and orientation data for each frame of video clips.\n•RealEstate10k [Zhou et al .2018]: The RealEstate10k dataset introduced in 2018, derived from over 7,000\ncurated real estate video clips on YouTube. These videos, ranging from 1 to 10 seconds in duration, capture both\nindoor and outdoor scenes, with precise metadata including camera position, orientation, and field of view for\neach frame. The dataset was created through a four-stage pipeline, leveraging manual selection, motion estimation\ntechniques like ORB-SLAM2 [Mur-Artal and Tardós 2017], for optimization, and final filtering for quality assurance.\nAdvantages include its substantial scale, diversity in scene types, and smooth camera movements, which enhance\nits utility for training camera trajectory models. However, limitations exist, such as its focus on simple, static\ncamera motions typical of real estate videos, lack of semantic descriptions for camera actions, and restricted\nenvironmental diversity, excluding natural or urban settings. Furthermore, its suitability for generating complex\nor dynamic movements, such as those involving subject interactions or rapid changes.\n•Example-Driven [Jiang et al .2020]: The dataset introduced by Hongda Jiang et al. (2020), referred to as\nthe Cinematic Feature Dataset, underpins their development of a novel camera motion controller for virtual\ncinematography. This dataset comprises a combination of synthetic and real film data, capturing essential\ncinematic features such as camera poses, character configurations, and dynamic interactions across diverse scenes.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 55 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •55\nThe dataset’s strengths lie in its detailed annotation and its utility in learning complex cinematographic patterns\napplicable to two-character interactions. However, its limitations include a focus on simplified scenes with a\nmaximum of two characters and the lack of representation for high-frequency camera movements or background\nmotion dynamics.\n•Augmented RealEstate [Wang et al .2024c]: The paper authored by Zhouxia Wang et al. (2024) introduces\ntwo datasets, the augmented-RealEstate10K. The augmented-RealEstate10K dataset includes over 60,000 videos\nwith annotated camera poses, supplemented by synthesized captions using Blip2. This dataset aids camera motion\ncontrol but is limited by its narrow domain diversity.\n•DCM [Wang et al .2024a]: The paper authored by Zixuan Wang et al. (2024) introduces the DCM (Dance-\nCamera-Music) dataset, the first of its kind to integrate 3D camera movement with dance motion and music\naudio. This dataset includes 108 paired sequences from the anime community, spanning 3.2 hours across four\nmusic genres and offering rich annotations for camera keyframes, dance joints, and audio features. By providing\nsynchronized camera trajectories and music-dance alignments. Its advantages include the inclusion of diverse\nshot types and human-centric camera characteristics. However, it faces limitations, such as the reliance on\nanimator-edited data, which may restrict spontaneity, and challenges in generalizing from anime contexts to\nreal-world settings.\n•E.T. [Courant et al .2025]: The E.T. (Exceptional Trajectories) dataset is a significant resource for text-\nto-camera trajectory generation, derived from the CMD dataset [Bain et al .2020]. It features 115,000 samples\nfrom 16,210 unique scenes, totaling over 11 million frames and 120 hours of cinematic footage. Each sample\nincludes synchronized camera and subject trajectories, with textual captions describing both camera motion and\nmotion relative to the subject. Unlike synthetic datasets, E.T. is based on real movie footage, capturing complex\n6 degree of freedom movements and offering a rich vocabulary of over 1,000 words. However, it suffers from\nimbalances favoring simple motions, lacks professional cinematic terminology, and is limited to single-human\nsubjects without contextual details like subject attributes and environmental factors. These limitations reduce its\nutility for advanced, real-world filmmaking applications.\nIn summary, the datasets discussed provide diverse approaches to addressing challenges in camera trajectory\ngeneration, each tailored to specific applications and methodologies. These datasets vary in scale, composition,\nand the types of trajectories they capture, ranging from synthetic sequences with detailed parameterization to real-\nworld datasets emphasizing diversity and realism. While some datasets prioritize control and repeatability, others\nfocus on naturalistic motion and broader applicability. In the following Table 7, we present a comparative analysis\nof these datasets, highlighting their key features and differences to provide an overview of their contributions\nand can not used for various research objectives.\n7 LIMITATIONS AND FUTURE DIRECTION\nAutomated camera trajectory generation systems are a critical component of virtual cinematography and related\nfields. However, existing approaches face significant challenges that limit their applicability and effectiveness\nin real-world scenarios. This section outlines the key limitations of current methodologies and proposes future\ndirections for advancing research and practical applications in this domain.\n7.1 Limited Availability and Diversity of Datasets\nThe progress of automated camera trajectory generation is hindered by the lack of comprehensive and diverse\ndatasets. Most available datasets, as pointed in Section 6, focus on narrow scenarios or predefined settings,\nlimiting their ability to generalize to broader use cases. The majority of these datasets fail to capture complex,\ndynamic environments or incorporate detailed annotations for advanced cinematic properties such as framing,\ntiming, or motion. Additionally, data collection processes are often resource-intensive, involving substantial\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 56 ---\n56 •Dehghanian et. al.\nTable 7. Dataset Comparison\nDataset #Samples #Frames #Hours DomainCharacter\nTraj.Camera\nTraj.#Vocabulary PromptDataset\nLink\nE.T.\n[Courant et al. 2025]115K 11M 120 H Real / MovieYES\n(115K)YES\n(230K)1790 ✓ Link\nDCM\n[Wang et al. 2024a]108 345K 3.2 H Synthetic / Dance NO YES NO ✗ Link\nCCD\n[Jiang et al. 2024b]25K 4.5M 50 H Synthetic NOYES\n(25K)48 ✓ Link\n[Bonatti et al. 2021] 200 NA. <1 HSynthetic /\nSemantic TrajectoryNO NA. NA. ✓ NA.\n[Jiang et al. 2020] 2.16M 86M NA.10% Real (Movies)\n90% SyntheticNO YES NO ✗ NA.\nRealEstate10K\n[Zhou et al. 2018]7K 11M 121 H Real / YouTube NO YES NO ✗ Link\nSources: [Bonatti et al. 2021; Courant et al. 2025; Jiang et al. 2020, 2024b; Wang et al. 2024a,c; Zhou et al. 2018]\ntechnical and financial investments. This scarcity of high-quality datasets constrains the training and evaluation\nof machine learning models, thereby impeding the development of robust, real-world-ready systems.\n7.2 Computational Complexity in High-Dimensional Models\nOptimization-based methods for camera trajectory generation often involve high-dimensional search spaces, such\nas 7-DOF [Chr [n. d.]]. While these models provide precise and detailed control over camera movements, their\ncomputational requirements are prohibitively high, especially for real-time applications. The iterative processes\nrequired to explore such large solution spaces lead to significant delays, making these methods impractical for\ntime-sensitive scenarios [Bonatti et al .2020b]. Similarly, when employing neural network models for camera\ntrajectory generation, it is crucial to ensure that these models are lightweight and efficient, as they are often\nintended for deployment on embedded devices with limited computational resources.\n7.3 Rigidity of Rule-Based Systems\nRule-based methods are widely appreciated for their adherence to established cinematic principles [Chen and\nCarr 2014; Christie and Olivier 2009]. However, their inherent rigidity poses significant challenges in dynamic\nand creative contexts. These systems rely on static, predefined rules that limit their adaptability to novel scenarios\nor evolving artistic requirements [Kennedy and Mercer 2002]. When confronted with situations that deviate from\ntheir encoded heuristics, rule-based approaches struggle to produce visually coherent and contextually relevant\noutputs [He et al .1996]. The lack of flexibility also restricts their ability to innovate or accommodate user-driven\ncustomization, which is increasingly demanded in professional and amateur filmmaking environments. There\nremains a notable absence of hybrid systems capable of leveraging contemporary heuristics while delivering\nrobust and accurate results in novel scenarios.\n7.4 Challenges in Dynamic Environments\nHandling dynamic environments, such as those involving moving subjects, obstacles, changing lighting conditions,\nor potential occlusions, remains a significant challenge for automated systems. Most existing methods assume\nstatic or predictable scenes, which limits their applicability to complex, real-world scenarios like sports, live events,\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 57 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •57\nor outdoor filmmaking. In these settings, cameras must continuously adapt to evolving conditions, ensuring\nsmooth movements, collision avoidance, occlusion avoidance, and adherence to cinematic principles. Despite the\nadvancements in the field [Burg et al .2021; Liu et al .2017], existing systems frequently struggle to seamlessly\nintegrate these requirements, resulting in disruptions to visual quality, such as obstructed views or reliance on\nmanual intervention.\n7.5 Insufficient Integration of Aesthetic Objectives\nWhile technical accuracy is a focus of most camera trajectory generation systems, the integration of aesthetic\nprinciples is often neglected. Many systems prioritize parameters such as stability and framing precision while\nignored critical artistic elements like rhythm, emotion, and storytelling. This oversight results in outputs that are\ntechnically sound, but, lack the emotional and narrative depth required for professional-grade cinematography.\nBridging this gap between technical execution and artistic intent is crucial for advancing the field and meeting\nthe expectations of modern audiences.\n7.6 Camera Trajectory is More than a Numerical Sequence\nCamera trajectory not only defines how the camera moves within a real or virtual environment but also serves as\na powerful tool to evoke emotions and guide the viewer’s attention [Bonatti et al .2021]. By carefully controlling\nmotion, orientation, and timing, it establishes narrative flow, enhances dramatic effects, and conveys mood\n[Sudabathula et al .2024]. These neglected aspects are essential in storytelling, shaping how audiences perceive\nand interact with visual content. However, there is a clear lack of integrated camera trajectory generation\nsystems that holistically address these dimensions. Critical areas such as the representation of such systems,\nthe availability of high-quality datasets, the development of robust generative models, and the establishment of\ncomprehensive evaluation metrics remain under explored and warrant significant attention.\nFuture research can enhance automated camera trajectory generation by advancing semantic understanding,\nexpanding multi-subject support, improving dataset diversity, refining evaluation metrics, and exploring long-term\nopportunities.\n8 CONCLUSION\nThe field of automated camera trajectory generation has witnessed remarkable advancements, drawing from\na diverse spectrum of methodologies such as rule-based systems, optimization techniques, machine learning,\nand hybrid approaches. These methods have collectively tackled challenges related to computational efficiency,\nadaptability, and cinematic quality. By systematically reviewing key contributions and methodologies within\nthis survey, we have demonstrated how these approaches address core challenges and contribute to the field’s\nevolution. Specifically, we have synthesized insights from foundational principles and SOTA advancements,\nproviding a cohesive understanding of existing solutions and emerging trends.\nOne of the most active areas of research in this field is the application of machine learning methods, which\nhave emerged as a hot topic due to their adaptability and capacity for learning complex cinematic patterns.\nMachine learning approaches, particularly those leveraging deep learning and generative models, enable the\nsynthesis of flexible, creative, and context-aware & multi-domain [Courant et al .2025; Wang et al .2024a] camera\ntrajectories. These models are increasingly capable of integrating aesthetic principles and responding to dynamic\nenvironments, offering transformative potential for both professional filmmaking and interactive applications.\nChallenges in automated cinematography, as discussed in 7, include limited dataset diversity, which hampers\nmodels’ ability to generalize across real-world scenarios, and underrepresentation of dynamic environments, multi-\nsubject interactions, and cinematic attributes like rhythm and storytelling. Future research must address these\nlimitations by enhancing dataset diversity, utilizing synthetic generation techniques, bridging the gap between\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 58 ---\n58 •Dehghanian et. al.\nsynthetic and real-world data, and leveraging advanced neural architectures such as visual-language models for\ngenerating cinematographic specific description for existing ones. Real-time systems with adaptive behaviors,\nmulti-subject interactions, and adherence to cinematic principles, combined with emerging technologies like\n3D scene modeling [Liu et al .2024a; Zhang et al .2024a], hold the potential to deliver solutions that are both\ntechnically proficient and artistically compelling, revolutionizing filmmaking and immersive media.\nREFERENCES\n[n. d.].\nArpit Agarwal, Katharina Muelling, and Katerina Fragkiadaki. 2018. Model Learning for Look-ahead Exploration in Continuous Control.\narXiv:1811.08086 [cs.RO] https://arxiv.org/abs/1811.08086\nSeyed Ali Amirshahi, Gregor Uwe Hayn-Leichsenring, Joachim Denzler, and Christoph Redies. 2014. Evaluating the rule of thirds in\nphotographs and paintings. Art & Perception 2, 1-2 (2014), 163–182.\nMohammad OA Aqel, Mohammad H Marhaban, M Iqbal Saripan, and Napsiah Bt Ismail. 2016. Review of visual odometry: types, approaches,\nchallenges, and applications. SpringerPlus 5 (2016), 1–26.\nDaniel Arijon. 1976. Grammar of the film language. (Hastings House Publishers) (1976).\nAmirsaman Ashtari, Stefan Stevšić, Tobias Nägeli, Jean-Charles Bazin, and Otmar Hilliges. 2020. Capturing subjective first-person view shots\nwith drones for automated cinematography. ACM Transactions on Graphics (TOG) 39, 5 (2020), 1–14.\nAdrian Azzarelli, Nantheera Anantrasirichai, and David R Bull. 2024. Reviewing Intelligent Cinematography: AI research for camera-based\nvideo production. arXiv preprint arXiv:2405.05039 (2024).\nJianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. 2024. Uniedit: A unified tuning-free framework\nfor video motion and appearance editing. arXiv preprint arXiv:2402.13185 (2024).\nMax Bain, Arsha Nagrani, Andrew Brown, and Andrew Zisserman. 2020. Condensed movies: Story based retrieval with contextual embeddings.\nInProceedings of the Asian Conference on Computer Vision .\nHui-Yong Bak and Seung-Bo Park. 2023. Camera motion detection for story and multimedia information convergence. Personal and Ubiquitous\nComputing 27, 3 (2023), 1221–1231.\nAmir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. 2024. Navigation World Models. arXiv:2412.03572 [cs.CV]\nhttps://arxiv.org/abs/2412.03572\nWilliam Bares, Scott McDermott, Christina Boudreaux, and Somying Thainimit. 2000. Virtual 3D camera composition from frame constraints.\nInProceedings of the eighth ACM international conference on Multimedia . 177–186.\nWilliam H Bares. 2000. A model for constraint-based camera planning. In Smart Graphics (Papers from the 2000 AAAI Symposium) .\nRichard Bellman. 1966. Dynamic programming. science 153, 3731 (1966), 34–37.\nYoshua Bengio. 2000. Gradient-based optimization of hyperparameters. Neural computation 12, 8 (2000), 1889–1900.\nMatthias Bernhard, Efstathios Stavrakis, and Michael Wimmer. 2010. An empirical pipeline to derive gaze prediction heuristics for 3D action\ngames. ACM Transactions on Applied Perception (TAP) 8, 1 (2010), 1–30.\nGérard Biau and Erwan Scornet. 2016. A random forest guided tour. Test25 (2016), 197–227.\nÅke Björck. 1990. Least squares methods. Handbook of numerical analysis 1 (1990), 465–652.\nJ. Blinn. 1988. Where am I? What am I looking at? (cinematography). IEEE Computer Graphics and Applications 8, 4 (1988), 76–81.\nhttps://doi.org/10.1109/38.7751\nRogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam, and Jessica Hodgins. 2021. Batteries, camera, action! learning a\nsemantic control space for expressive robot cinematography. In 2021 IEEE International Conference on Robotics and Automation (ICRA) .\nIEEE, 7302–7308.\nRogerio Bonatti, Cherie Ho, Wenshan Wang, Sanjiban Choudhury, and Sebastian Scherer. 2019. Towards a robust aerial cinematography\nplatform: Localizing and tracking moving targets in unstructured environments. In 2019 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS) . IEEE, 229–236.\nRogerio Bonatti, Wenshan Wang, Cherie Ho, Aayush Ahuja, Mirko Gschwindt, Efe Camci, Erdal Kayacan, Sanjiban Choudhury, and Sebastian\nScherer. 2020a. Autonomous aerial cinematography in unstructured environments with learned artistic decision-making. Journal of Field\nRobotics 37, 4 (2020), 606–641.\nRogerio Bonatti, Yanfu Zhang, Sanjiban Choudhury, Wenshan Wang, and Sebastian Scherer. 2020b. Autonomous drone cinematographer:\nUsing artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming. In Proceedings of the 2018 international\nsymposium on experimental robotics . Springer, 119–129.\nRakesh P Borase, DK Maghade, SY Sondkar, and SN Pawar. 2021. A review of PID control, tuning methods and applications. International\nJournal of Dynamics and Control 9 (2021), 818–827.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 59 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •59\nOwen Bourne and Abdul Sattar. 2005. Applying constraint weighting to autonomous camera control. In Proceedings of the AAAI Conference\non Artificial Intelligence and Interactive Digital Entertainment , Vol. 1. 3–8.\nBlain Brown. 2012. Cinematography: Theory and Practice (2nd ed.). Elsevier, MA, USA.\nAlexandre Bruckert, Marc Christie, and Olivier Le Meur. 2023. Where to look at the movies: Analyzing visual attention to understand movie\nediting. Behavior Research Methods 55, 6 (2023), 2940–2959.\nDominique Brunet, Edward R Vrscay, and Zhou Wang. 2011. On the mathematical properties of the structural similarity index. IEEE\nTransactions on Image Processing 21, 4 (2011), 1488–1499.\nPaolo Burelli. 2016. Game Cinematography: From Camera Control to Player Emotions. In Emotion in Games: Theory and Praxis , Kostas Karpouzis\nand Georgios N. Yannakakis (Eds.). Springer International Publishing, Cham, 181–195. https://doi.org/10.1007/978-3-319-41316-7_11\nP Burelli and Yannakakis GN. 2015. Adaptive Virtual Camera Control Trough Player Modelling. User Modelling and User-\nAdapted Interaction DOI 10.1007/s11257-015-9156-4, URL http://www. paoloburelli. com/publications/Burelli {%}2CYannakakis-2015-\nAdaptiveVirtualCameraControlTroughPlayerModelling. pdfhttp. dx. doi. org/10.1007/s11257-015-9156-4 (2015).\nPaolo Burelli and Georgios N Yannakakis. 2011. Towards adaptive virtual camera control in computer games. In Smart Graphics: 11th\nInternational Symposium, SG 2011, Bremen, Germany, July 18-20, 2011. Proceedings 11 . Springer, 25–36.\nPaolo Burelli and Georgios N. Yannakakis. 2015. Adapting virtual camera behaviour through player modelling. User Modeling and User-Adapted\nInteraction 25, 2 (2015), 155–183. https://doi.org/10.1007/s11257-015-9156-4\nLudovic Burg. 2022. Real-time virtual cinematography for target tracking. https://tel.archives-ouvertes.fr/tel-04013803. Image Processing\n[eess.IV], Université Rennes 1, English.\nLudovic Burg, Christophe Lino, and Marc Christie. 2020. Real-time Anticipation of Occlusions for Automated Camera Control in Toric Space.\nInComputer Graphics Forum , Vol. 39. Wiley Online Library, 523–533.\nLudovic Burg, Christophe Lino, and Marc Christie. 2021. Real-Time Cinematic Tracking of Targets in Dynamic Environments. In GI\n2021-Graphics Interface conference . 1–10.\nZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings\nof the IEEE conference on computer vision and pattern recognition . 7291–7299.\nJianhui Chen and Peter Carr. 2014. Autonomous camera systems: A survey. In Workshops at the Twenty-Eighth AAAI Conference on Artificial\nIntelligence .\nJianhui Chen and Peter Carr. 2015. Mimicking human camera operators. In 2015 IEEE Winter Conference on Applications of Computer Vision .\nIEEE, 215–222.\nJianhui Chen, Hoang M Le, Peter Carr, Yisong Yue, and James J Little. 2016a. Learning online smooth predictors for realtime camera planning\nusing recurrent decision trees. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4688–4696.\nJing Chen, Tianbo Liu, and Shaojie Shen. 2016b. Tracking a moving target in cluttered environments using a quadrotor. In 2016 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) . IEEE, 446–453.\nWeiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Haixu Song, and Yueqi Duan. 2024a. DreamCinema: Cinematic Transfer with Free\nCamera and 3D Character. arXiv preprint arXiv:2408.12601 (2024).\nYiran Chen, Anyi Rao, Xuekun Jiang, Shishi Xiao, Ruiqing Ma, Zeyu Wang, Hui Xiong, and Bo Dai. 2024b. CinePreGen: Camera Controllable\nVideo Previsualization via Engine-powered Diffusion. arXiv preprint arXiv:2408.17424 (2024).\nZ Chen, H Wang, DY Yeung, and W-KW-c Wong Woo. 2015. Convolutional LSTM network: A machine learning approach for precipitation\nnowcasting. In Proc. Adv. Neural Inf. Process. Syst. 802–810.\nMarc Christie and Patrick Olivier. 2009. Camera control in computer graphics: models, techniques and applications. In ACM SIGGRAPH ASIA\n2009 Courses . 1–197.\nMarc Christie, Patrick Olivier, and Jean-Marie Normand. 2008. Camera Control in Computer Graphics. Computer Graphics Forum 27, 8 (Dec.\n2008), 2197–2218. https://doi.org/10.1111/j.1467-8659.2008.01181.x\nNguyen Cong Danh. 2021. The Stability of a Two-Axis Gimbal System for the Camera. The Scientific World Journal 2021, 1 (2021), 9958848.\nRobin Courant, Nicolas Dufour, Xi Wang, Marc Christie, and Vicky Kalogeiton. 2025. ET the Exceptional Trajectories: Text-to-camera-trajectory\ngeneration with character awareness. In European Conference on Computer Vision . Springer, 464–480.\nThomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification. IEEE transactions on information theory 13, 1 (1967), 21–27.\nKalyanmoy Deb and Matthias Ehrgott. 2023. On Generalized Dominance Structures for Multi-Objective Optimization. Mathematical and\nComputational Applications 28, 5 (2023). https://doi.org/10.3390/mca28050100\nPaul E Debevec, Camillo J Taylor, and Jitendra Malik. 2023. Modeling and rendering architecture from photographs: A hybrid geometry-and\nimage-based approach. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2 . 465–474.\nZahra Dehghanian, Morteza Abolghasemi, Hossein Azizinaghsh, Hamid Beigy, and Hamid R. Rabiee. 2025. LensCraft: Your Professional\nVirtual Cinematographer. arXiv preprint (2025).\nJean C Digitale, Jeffrey N Martin, and Medellena Maria Glymour. 2022. Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology\n142 (2022), 264–267.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 60 ---\n60 •Dehghanian et. al.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for\nImage Recognition at Scale. In ICLR . https://openreview.net/forum?id=YicbFdNTTy\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy\nYang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).\nHugh Durrant-Whyte and Tim Bailey. 2006. Simultaneous localization and mapping: part I. IEEE robotics & automation magazine 13, 2 (2006),\n99–110.\nJoseph G Eisenhauer. 2008. Degrees of Freedom. Teaching Statistics 30, 3 (2008).\nDavid Elson and Mark Riedl. 2007. A lightweight intelligent virtual cinematography system for machinima production. In Proceedings of the\nAAAI Conference on Artificial Intelligence and Interactive Digital Entertainment , Vol. 3. 8–13.\nShahab Eslamian, Luke A Reisner, and Abhilash K Pandya. 2020. Development and evaluation of an autonomous camera control algorithm on\nthe da Vinci Surgical System. The International Journal of Medical Robotics and Computer Assisted Surgery 16, 2 (2020), e2036.\nCass Everitt. 2001. Interactive order-independent transparency. White paper, nVIDIA 2, 6 (2001), 7.\nCass Everitt, Ashu Rege, and Cem Cebenoyan. 2001. Hardware shadow mapping. White paper, nVIDIA 2 (2001).\nGiovanni Fiengo, Diego Castiello, Giuseppe Grande, and Marco Solla. 2006. Optimal camera trajectory for video surveillance systems. In 2006\nAmerican Control Conference . IEEE, 5–pp.\nShachar Fleishman, Daniel Cohen-Or, and Dani Lischinski. 2000. Automatic camera placement for image-based modeling. In Computer\nGraphics Forum , Vol. 19. Wiley Online Library, 101–110.\nJames D Foley. 1996. Computer graphics: principles and practice . Vol. 12110. Addison-Wesley Professional.\nQuentin Galvane, Marc Christie, Chrsitophe Lino, and Rémi Ronfard. 2015a. Camera-on-rails: automated computation of constrained camera\npaths. In Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games (Paris, France) (MIG ’15) . Association for Computing\nMachinery, New York, NY, USA, 151–157. https://doi.org/10.1145/2822013.2822025\nQuentin Galvane, Marc Christie, Chrsitophe Lino, and Rémi Ronfard. 2015b. Camera-on-rails: automated computation of constrained camera\npaths. In Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games . 151–157.\nQuentin Galvane, Marc Christie, Rémi Ronfard, Chen-Kim Lim, and Marie-Paule Cani. 2013. Steering behaviors for autonomous cameras. In\nProceedings of motion on games . 93–102.\nQuentin Galvane, Christophe Lino, Marc Christie, Julien Fleureau, Fabien Servant, François-Louis Tariolle, and Philippe Guillotel. 2018.\nDirecting cinematographic drones. ACM Transactions on Graphics (TOG) 37, 3 (2018), 1–18.\nQuentin Galvane, Rémi Ronfard, Marc Christie, and Nicolas Szilas. 2014. Narrative-driven camera control for cinematic replay of computer\ngames. In Proceedings of the 7th International Conference on Motion in Games . 109–117.\nQuentin Galvane, Rémi Ronfard, Christophe Lino, and Marc Christie. 2015c. Continuity editing for 3D animation. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 29.\nChristoph Gebhardt, Benjamin Hepp, Tobias Nägeli, Stefan Stevšić, and Otmar Hilliges. 2016. Airways: Optimization-based planning of\nquadrotor trajectories according to high-level user goals. In Proceedings of the 2016 chi conference on human factors in computing systems .\n2508–2519.\nChristoph Gebhardt and Otmar Hilliges. 2021. Optimization-based user support for cinematographic quadrotor camera target framing. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1–13.\nChristoph Gebhardt, Stefan Stevšić, and Otmar Hilliges. 2018. Optimizing for aesthetically pleasing quadrotor camera motion. ACM\nTransactions on Graphics (TOG) 37, 4 (2018), 1–11.\nJacek Gondzio. 2012. Interior point methods 25 years later. European Journal of Operational Research 218, 3 (2012), 587–601.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.\nGenerative adversarial nets. Advances in neural information processing systems 27 (2014).\nMichael D Grossberg and Shree K Nayar. 2001. A general imaging model and a method for finding its parameters. In Proceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV 2001 , Vol. 2. IEEE, 108–115.\nMirko Gschwindt, Efe Camci, Rogerio Bonatti, Wenshan Wang, Erdal Kayacan, and Sebastian Scherer. 2019. Can a robot become a movie\ndirector? learning artistic principles for aerial cinematography. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS) . IEEE, 1107–1114.\nYuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff:\nAnimate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023).\nNicolas Halper, Ralf Helbing, and Thomas Strothotte. 2001. A camera engine for computer games: Managing the trade-off between constraint\nsatisfaction and frame coherence. In Computer Graphics Forum , Vol. 20. Wiley Online Library, 174–183.\nRichard Hartley and Andrew Zisserman. 2003. Multiple view geometry in computer vision . Cambridge university press.\nHao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. 2024. Cameractrl: Enabling camera control for\ntext-to-video generation. arXiv preprint arXiv:2404.02101 (2024).\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 61 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •61\nLi-wei He, Michael F Cohen, and David H Salesin. 1996. The virtual cinematographer: A paradigm for automatic real-time camera control\nand directing. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2 . 707–714.\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent Video Diffusion Models for High-Fidelity Long Video\nGeneration. arXiv preprint arXiv:2211.13221 (2022). https://arxiv.org/abs/2211.13221\nRichard M Heiberger, Erich Neuwirth, Richard M Heiberger, and Erich Neuwirth. 2009. Polynomial regression. R Through Excel: A Spreadsheet\nInterface for Statistics, Data Analysis, and Graphics (2009), 269–284.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing\nSystems , Vol. 33. 6840–6851. https://arxiv.org/abs/2006.11239\nJonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance. arXiv preprint arXiv:2207.12598 (2022). https://arxiv.org/abs/2207.\n12598\nHolger H Hoos and Thomas St ¥𝜈tzle. 2018. Stochastic local search. In Handbook of Approximation Algorithms and Metaheuristics . Chapman\nand Hall/CRC, 297–307.\nChen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. 2024. Training-free Camera Control for Video Generation. arXiv preprint arXiv:2406.10126\n(2024).\nTeng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. 2024. MotionMaster:\nTraining-free Camera Motion Transfer For Video Generation. arXiv preprint arXiv:2404.15789 (2024).\nChong Huang, Fei Gao, Jie Pan, Zhenyu Yang, Weihao Qiu, Peng Chen, Xin Yang, Shaojie Shen, and Kwang-Ting Cheng. 2018. Act: An\nautonomous drone cinematography system for action scenes. In 2018 ieee international conference on robotics and automation (icra) . IEEE,\n7039–7046.\nChong Huang, Zhenyu Yang, Yan Kong, Peng Chen, Xin Yang, and Kwang-Ting Tim Cheng. 2019. Learning to capture a film-look video with\na camera drone. In 2019 international conference on robotics and automation (ICRA) . IEEE, 1871–1877.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec\nRadford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\nLuay Jawad, Arshdeep Singh-Chudda, Abhishek Shankar, and Abhilash Pandya. 2024. A Deep Learning Approach to Merge Rule-Based and\nHuman-Operated Camera Control for Teleoperated Robotic Systems. Robotics 13, 3 (2024), 47.\nBoseong Felipe Jeon and H Jin Kim. 2019. Online trajectory generation of a mav for chasing a moving target in 3d dense environments. In\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 1115–1121.\nBoseong Felipe Jeon, Dongsuk Shim, and H Jin Kim. 2020. Detection-aware trajectory generation for a drone cinematographer. In 2020\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 1450–1457.\nHongda Jiang, Marc Christie, Xi Wang, Libin Liu, Bin Wang, and Baoquan Chen. 2021. Camera keyframing with style and control. ACM\nTransactions on Graphics (TOG) 40, 6 (2021), 1–13.\nHongda Jiang, Bin Wang, Xi Wang, Marc Christie, and Baoquan Chen. 2020. Example-driven virtual cinematography by learning camera\nbehaviors. ACM Trans. Graph. 39, 4 (2020), 45.\nHongda Jiang, Xi Wang, Marc Christie, Libin Liu, and Baoquan Chen. 2024b. Cinematographic Camera Diffusion Model. In Computer Graphics\nForum , Vol. 43. Wiley Online Library, e15055.\nXuekun Jiang, Anyi Rao, Jingbo Wang, Dahua Lin, and Bo Dai. 2024a. Cinematic Behavior Transfer via NeRF-based Differentiable Filming. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 6723–6732.\nAlberto Jovane, Amaury Louarn, and Marc Christie. 2020. Topology-aware camera control for real-time applications. In Proceedings of the\n13th ACM SIGGRAPH Conference on Motion, Interaction and Games . 1–10.\nTomihisa Kamada and Satoru Kawai. 1988. A simple method for computing general position in displaying three-dimensional objects. Computer\nVision, Graphics, and Image Processing 41, 1 (1988), 43–56.\nRohan Katoch and Jun Ueda. 2019. Edge-preserving camera trajectories for improved optical character recognition on static scenes with text.\nIEEE Robotics and Automation Letters 4, 4 (2019), 4467–4474.\nKevin Kennedy and Robert E Mercer. 2002. Planning animation cinematography and shot structure to communicate theme and mood. In\nProceedings of the 2nd international symposium on Smart graphics . 1–8.\nMasoud Khodarahmi and Vafa Maihami. 2023. A review on Kalman filter models. Archives of Computational Methods in Engineering 30, 1\n(2023), 727–747.\nKihwan Kim, Matthias Grundmann, Ariel Shamir, Iain Matthews, Jessica Hodgins, and Irfan Essa. 2010. Motion fields to predict play evolution\nin dynamic sport scenes. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition . IEEE, 840–847.\nKihwan Kim, Dongryeol Lee, and Irfan Essa. 2011. Gaussian process regression flow for analysis of motion trajectories. In 2011 International\nConference on Computer Vision . IEEE, 1164–1171.\nKihwan Kim, Dongryeol Lee, and Irfan Essa. 2012. Detecting regions of interest in dynamic scenes with camera motions. In 2012 IEEE\nConference on Computer Vision and Pattern Recognition . IEEE, 1258–1265.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 62 ---\n62 •Dehghanian et. al.\nA Kirillov, E Mintun, N Ravi, et al .2023. Segment anything Proceedings of the IEEE. In CVF International Conference on Computer Vision\n(ICCV), IEEE . 4015–4026.\nJari Korhonen and Junyong You. 2012. Peak signal-to-noise ratio revisited: Is simple beautiful?. In 2012 Fourth international workshop on\nquality of multimedia experience . IEEE, 37–38.\nAkshay Krishnamurthy, Alekh Agarwal, and John Langford. 2016. Pac reinforcement learning with rich observations. Advances in Neural\nInformation Processing Systems 29 (2016).\nDirk P Kroese and Reuven Y Rubinstein. 2012. Monte carlo methods. Wiley Interdisciplinary Reviews: Computational Statistics 4, 1 (2012),\n48–58.\nZhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. 2024. Collaborative Video\nDiffusion: Consistent Multi-video Generation with Camera Control. arXiv preprint arXiv:2405.17414 (2024).\nPankaj Kumar, Anthony Dick, and Tan Soo Sheng. 2009. Real time target tracking with pan tilt zoom camera. In 2009 Digital Image Computing:\nTechniques and Applications . IEEE, 492–497.\nChristos Kyrkou. 2020. Imitation-based active camera control with deep convolutional neural network. In 2020 IEEE 4th International\nConference on Image Processing, Applications and Systems (IPAS) . IEEE, 168–173.\nChristos Kyrkou. 2021. C 3 Net: end-to-end deep learning for efficient real-time visual active camera control. Journal of Real-Time Image\nProcessing 18, 4 (2021), 1421–1433.\nDenise Lam, Chris Manzie, and Malcolm Good. 2010. Model predictive contouring control. In 49th IEEE Conference on Decision and Control\n(CDC) . IEEE, 6137–6142.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. 2020. Curl: Contrastive unsupervised representations for reinforcement learning. In\nInternational conference on machine learning . PMLR, 5639–5650.\nJean-Claude Latombe. 2012. Robot motion planning . Vol. 124. Springer Science & Business Media.\nXinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. 2024. Director3D: Real-world\nCamera Trajectory and 3D Scene Generation from Text. arXiv preprint arXiv:2406.17601 (2024).\nChao Liang, Changsheng Xu, Jian Cheng, Weiqing Min, and Hanqing Lu. 2012. Script-to-movie: a computational framework for story movie\ncomposition. IEEE transactions on multimedia 15, 2 (2012), 401–414.\nJinwei Lin. 2024. Dynamic NeRF: A Review. arXiv preprint arXiv:2405.08609 (2024).\nChristophe Lino and Marc Christie. 2015. Intuitive and efficient camera control with the toric space. ACM Trans. Graph. 34, 4, Article 82 (July\n2015), 12 pages. https://doi.org/10.1145/2766965\nChristophe Lino, Marc Christie, Roberto Ranon, and William Bares. 2011. The director’s lens: an intelligent assistant for virtual cinematography.\nInProceedings of the 19th ACM international conference on Multimedia . 323–332.\nAlan Ulfers Litteneker. 2022. Towards Intelligent Computational Tools for Virtual Cinematography . University of California, Los Angeles.\nFangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. 2024a. Reconx: Reconstruct\nany scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767 (2024).\nSikang Liu, Michael Watterson, Kartik Mohta, Ke Sun, Subhrajit Bhattacharya, Camillo J Taylor, and Vijay Kumar. 2017. Planning dynamically\nfeasible trajectories for quadrotors using safe flight corridors in 3-d complex environments. IEEE Robotics and Automation Letters 2, 3\n(2017), 1688–1695.\nXinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. 2024b. ChatCam: Empowering Camera Control through Conversational AI. arXiv preprint\narXiv:2409.17331 (2024).\nXinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, and Weiming Zhi. 2024c. SplaTraj: Camera Trajectory Generation with Semantic\nGaussian Splatting. arXiv preprint arXiv:2410.06014 (2024).\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2015. SMPL: a skinned multi-person linear model .\nVol. 34. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/2816795.2818013\nAmaury Louarn, Marc Christie, and Fabrice Lamarche. 2018. Automated staging for virtual cinematography. In Proceedings of the 11th ACM\nSIGGRAPH Conference on Motion, Interaction and Games . 1–10.\nAmaury Louarn, Quentin Galvane, Fabrice Lamarche, and Marc Christie. 2020. An interactive staging-and-shooting solver for virtual\ncinematography. In Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games . 1–6.\nMatija Maleš, Adam Heđi, and Mislav Grgić. 2012. Compositional rule of thirds detection. In Proceedings ELMAR-2012 . IEEE, 41–44.\nRoss T. Marler and Jasbir S. Arora. 2010. The weighted sum method for multi-objective optimization: new insights. Structural and\nMultidisciplinary Optimization 41, 6 (2010), 853–862. https://doi.org/10.1007/s00158-009-0460-7\nTommaso Massaglia. 2023. DreamShot: Teaching Cinema Shots to Latent Diffusion Models . Ph. D. Dissertation. Politecnico di Torino.\nTommaso Massaglia, Bartolomeo Vacchetti, and Tania Cerquitelli. 2024. DreamShot: Teaching Cinema Shots to Latent Diffusion Models. In\nProceedings of the Workshops of the EDBT/ICDT 2024 Joint Conference . CEUR-WS.org, 1–8. https://ceur-ws.org/Vol-3651/DARLI-AP-8.pdf\nPedro Meseguer, Nadia Bouhmala, Tarek Bouzoubaa, et al .2003. Current Approaches for Solving Over-Constrained Problems. Constraints 8\n(2003), 9–39. https://doi.org/10.1023/A:1021902812784\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 63 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •63\nYoucef Mezouar and Francois Chaumette. 2003. Optimal camera trajectory with image-based control. The International Journal of Robotics\nResearch 22, 10-11 (2003), 781–803.\nAnish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012a. No-reference image quality assessment in the spatial domain. IEEE\nTransactions on image processing 21, 12 (2012), 4695–4708.\nAnish Mittal, Rajiv Soundararajan, and Alan C Bovik. 2012b. Making a “completely blind” image quality analyzer. IEEE Signal processing\nletters 20, 3 (2012), 209–212.\nRishabh Mittal and Anchal Garg. 2020. Text extraction using OCR: a systematic review. In 2020 second international conference on inventive\nresearch in computing applications (ICIRCA) . IEEE, 357–362.\nJaime Moreno, Beatriz Jaime, and Salvador Saucedo. 2013. Towards no-reference of peak signal to noise ratio. International Journal of\nAdvanced Computer Science and Applications 4, 1 (2013).\nMeinard Müller. 2007. Dynamic time warping. Information retrieval for music and motion (2007), 69–84.\nRaul Mur-Artal and Juan D Tardós. 2017. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE transactions\non robotics 33, 5 (2017), 1255–1262.\nMuhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. 2020. Reliable fidelity and diversity metrics for\ngenerative models. In International Conference on Machine Learning . PMLR, 7176–7185.\nTobias Nägeli, Javier Alonso-Mora, Alexander Domahidi, Daniela Rus, and Otmar Hilliges. 2017a. Real-time motion planning for aerial\nvideography with dynamic obstacle avoidance and viewpoint optimization. IEEE Robotics and Automation Letters 2, 3 (2017), 1696–1703.\nTobias Nägeli, Lukas Meier, Alexander Domahidi, Javier Alonso-Mora, and Otmar Hilliges. 2017b. Real-time planning for automated multi-view\ndrone cinematography. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1–10.\nTimothy Oskam et al .2009. Visibility-aware roadmap construction and planning. Eurographics (2009). Uses A* algorithm for path planning\nin cinematographic contexts..\nAbhilash Pandya, Luke A Reisner, Brady King, Nathan Lucas, Anthony Composto, Michael Klein, and Richard Darin Ellis. 2014. A review of\ncamera viewpoint automation in robotic and laparoscopic surgery. Robotics 3, 3 (2014), 310–329.\nSambhram Pattanayak, Saad Ullah Khan, Fazal Malik, and Somanath Sahoo. 2024. Automating Camera Movements: AI-Driven PTZ Cameras\nin Film Production. In Innovative and Intelligent Digital Technologies; Towards an Increased Efficiency: Volume 1 . Springer, 627–639.\nWilliam Peebles and Saining Xie. 2023. Scalable Diffusion Models with Transformers. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) . 4195–4205. https://arxiv.org/abs/2212.09748\nPablo Pueyo, Eduardo Montijano, Ana C Murillo, and Mac Schwager. 2022. Cinempc: Controlling camera intrinsics and extrinsics for\nautonomous cinematography. In 2022 International Conference on Robotics and Automation (ICRA) . IEEE, 4058–4064.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al .2021. Learning transferable visual models from natural language supervision. In International conference on machine\nlearning . PMLR, 8748–8763.\nE. Raffone, C. Rei, and M. Rossi. 2019. Optimal look-ahead vehicle lane centering control design and application for mid-high speed and\ncurved roads. In 2019 18th European Control Conference (ECC) . 2024–2029. https://doi.org/10.23919/ECC.2019.8796031\nRoberto ranon, Marc Christie, and Christophe Lino. 2016. Algorithms and techniques for virtual camera control. In Proceedings of the 37th\nAnnual Conference of the European Association for Computer Graphics: Tutorials (Lisbon, Portugal) (EG ’16) . Eurographics Association,\nGoslar, DEU, Article 5, 1 pages.\nRoberto Ranon and Tommaso Urli. 2014. Improving the efficiency of viewpoint composition. IEEE Transactions on Visualization and Computer\nGraphics 20, 5 (2014), 795–807.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,\nOrhan Firat, Julian Schrittwieser, et al .2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv\npreprint arXiv:2403.05530 (2024).\nYanhao Ren, Nannan Yan, Xiao Yu, Fengfeng Tang, Qi Tang, Yi Wang, and Wenlian Lu. 2023. On automatic camera shooting systems via PTZ\ncontrol and DNN-based visual sensing. Intelligent Service Robotics 16, 3 (2023), 265–285.\nCraig W Reynolds et al. 1999. Steering behaviors for autonomous characters. In Game developers conference , Vol. 1999. Citeseer, 763–782.\nMike Roberts and Pat Hanrahan. 2016. Generating dynamically feasible trajectories for quadrotor cameras. ACM Transactions on Graphics\n(TOG) 35, 4 (2016), 1–11.\nRémi Ronfard, Vineet Gandhi, Laurent Boiron, and Vaishnavi Ameya Murukutla. 2015. The prose storyboard language: A tool for annotating\nand directing movies. arXiv preprint arXiv:1508.07593 (2015).\nScott D Roth. 1982. Ray casting for modeling solids. Computer graphics and image processing 18, 2 (1982), 109–144.\nGauthier Rousseau, Cristina Stoica Maniu, Sihem Tebbani, Mathieu Babel, and Nicolas Martin. 2018. Quadcopter-performed cinematographic\nflight plans using minimum jerk trajectories and predictive camera control. In 2018 European Control Conference (ECC) . IEEE, 2897–2903.\nBahareh Sabetghadam, Alfonso Alcántara, Jesús Capitán, Rita Cunha, Aníbal Ollero, and Antonio Pascoal. 2019. Optimal trajectory planning\nfor autonomous drone cinematography. In 2019 European Conference on Mobile Robots (ECMR) . IEEE, 1–7.\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 64 ---\n64 •Dehghanian et. al.\nWojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J Anders, and Klaus-Robert Müller. 2021. Explaining deep neural\nnetworks and beyond: A review of methods and applications. Proc. IEEE 109, 3 (2021), 247–278.\nThomas Schops, Viktor Larsson, Marc Pollefeys, and Torsten Sattler. 2020. Why having 10,000 parameters in your camera model is better\nthan twelve. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2535–2544.\nMax Schwenzer, Muzaffer Ay, Thomas Bergs, and Dirk Abel. 2021. Review on model predictive control: An engineering perspective. The\nInternational Journal of Advanced Manufacturing Technology 117, 5 (2021), 1327–1349.\nWilliam R Scott, Gerhard Roth, and Jean-François Rivest. 2003. View planning for automated three-dimensional object reconstruction and\ninspection. ACM Computing Surveys (CSUR) 35, 1 (2003), 64–96.\nPavel Senin. 2008. Dynamic time warping algorithm review. Information and Computer Science Department University of Hawaii at Manoa\nHonolulu, USA 855, 1-23 (2008), 40.\nNusrat Sharmin and Remus Brad. 2012. Optimal filter estimation for Lucas-Kanade optical flow. Sensors 12, 9 (2012), 12694–12709.\nXiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and\nHongsheng Li. 2023. Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 12469–12480.\nJeferson R Silva, Thiago T Santos, and Carlos H Morimoto. 2011. Automatic camera control in virtual environments augmented using multiple\nsparse videos. Computers & Graphics 35, 2 (2011), 412–421.\nAlex J Smola and Bernhard Schölkopf. 2004. A tutorial on support vector regression. Statistics and computing 14 (2004), 199–222.\nNoah Snavely, Steven M. Seitz, and Richard Szeliski. 2006. Photo Tourism: Exploring Photo Collections in 3D. In ACM SIGGRAPH 2006 Papers .\nACM, 835–846. https://doi.org/10.1145/1179352.1141964\nDmitry Sokolov, Dimitri Plemenos, and Karim Tamine. 2006. Methods and data structures for virtual world exploration. The Visual Computer\n22 (2006), 506–516.\nJames Stewart. 2012. Calculus: early transcendentals . Cengage Learning.\nWolfgang Stuerzlinger. 1999. Imaging all visible surfaces. In Graphics Interface , Vol. 99. 115–122.\nJ. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In Proc. of\nthe International Conference on Intelligent Robot Systems (IROS) .\nOlly Styles, Tanaya Guha, and Victor Sanchez. 2021. Multi-camera trajectory forecasting with trajectory tensors. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 44, 11 (2021), 8482–8491.\nVijay Sai Kumar Sudabathula, Banoth Krishna Mohan Naik, Shifa Ismail, Sri Harsh Mattaparty, Gagan Deep Arora, and Guda Sravan Yadav.\n2024. Emotion Trajectories in Cinematic Narratives: A Transformer-Based Analysis. In 2024 First International Conference on Software,\nSystems and Information Technology (SSITCON) . IEEE, 1–7.\nTakafumi Taketomi, Hideaki Uchiyama, and Seiichi Ikeda. 2017. Visual SLAM algorithms: a survey from 2010 to 2016. IPSJ Transactions on\nComputer Vision and Applications 9 (2017), 16. https://doi.org/10.1186/s41074-017-0027-2\nYogya Tewari, Arti Hadap, Payal Soni, Muskan Sharma, Daksh Shukla, and Shreya Malanker. 2021. An Overview of Applications of Gaussian\nNumerical Methods. In 2021 2nd International Conference on Smart Electronics and Communication (ICOSEC) . IEEE, 645–657.\nBill Tomlinson, Bruce Blumberg, and Delphine Nain. 2000. Expressive autonomous cinematography for interactive virtual environments. In\nProceedings of the fourth international conference on Autonomous agents . 317–324.\nHung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf. 2023. Consistent view synthesis with pose-guided\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 16773–16783.\nThomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate\ngenerative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717 (2018).\nDieter Van Rijsselbergen, Barbara Van De Keer, Maarten Verwaest, Erik Mannens, and Rik Van de Walle. 2009. Movie script markup language.\nInProceedings of the 9th ACM symposium on Document engineering . 161–170.\nLeonid Nisonovich Vaserstein. 1969. Markov processes over denumerable products of spaces, describing large systems of automata. Problemy\nPeredachi Informatsii 5, 3 (1969), 64–72.\nPere-Pau Vázquez, Miquel Feixas, Mateu Sbert, and Wolfgang Heidrich. 2003. Automatic view selection using viewpoint entropy and its\napplication to image-based modelling. In Computer Graphics Forum , Vol. 22. Wiley Online Library, 689–700.\nJeremy Vineyard. 2008. Setting Up Your Shots (2nd ed.). Michael Wiese, CA, USA.\nIvan Viola, Miquel Feixas, Mateu Sbert, and Meister Eduard Groller. 2006. Importance-driven focus of attention. IEEE transactions on\nvisualization and computer graphics 12, 5 (2006), 933–940.\nJianyuan Wang, Christian Rupprecht, and David Novotny. 2023b. PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle\nAdjustment. In ICCV . https://arxiv.org/abs/2305.06429\nJianyi Wang, Mai Xu, Lai Jiang, and Yuhang Song. 2020. Attention-Based Deep Reinforcement Learning for Virtual Cinematography of 360\nVideos. IEEE Transactions on Multimedia 23 (2020), 3227–3238.\nJianyi Wang, Mai Xu, Lai Jiang, and Yuhang Song. 2021. Attention-Based Deep Reinforcement Learning for Virtual Cinematography of 360 °\nVideos. IEEE Transactions on Multimedia 23 (2021), 3227–3238. https://doi.org/10.1109/TMM.2020.3028955\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 65 ---\nCamera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions •65\nXi Wang, Robin Courant, Jinglei Shi, Eric Marchand, and Marc Christie. 2023a. JAWS: just a wild shot for cinematic transfer in neural radiance\nfields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 16933–16942.\nZixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, and Jiebo Luo. 2024a. DanceCamera3D: 3D\nCamera Movement Synthesis with Music and Dance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n7892–7901.\nZixuan Wang, Jiayi Li, Xiaoyu Qin, Shikun Sun, Songtao Zhou, Jia Jia, and Jiebo Luo. 2024b. DanceCamAnimator: Keyframe-Based Controllable\n3D Dance Camera Synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia . 10200–10209.\nZhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. 2024c. Motionctrl: A unified\nand flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers . 1–11.\nLance Williams. 1978. Casting curved shadows on curved surfaces. In Proceedings of the 5th annual conference on Computer graphics and\ninteractive techniques . 270–274.\nAlden H Wright. 1991. Genetic algorithms for real parameter optimization. In Foundations of genetic algorithms . Vol. 1. Elsevier, 205–218.\nHui-Yin Wu, Francesca Palù, Roberto Ranon, and Marc Christie. 2018. Thinking like a director: Film editing patterns for virtual cinematographic\nstorytelling. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 14, 4 (2018), 1–22.\nXinyi Wu, Haohong Wang, and Aggelos K Katsaggelos. 2023. The secret of immersion: actor driven camera movement generation for\nauto-cinematography. arXiv preprint arXiv:2303.17041 (2023).\nWenqi Xian, Aljaž Božič, Noah Snavely, and Christoph Lassner. 2023. Neural lens modeling. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 8435–8445.\nChun Xie, Isao Hemmi, Hidehiko Shishido, and Itaru Kitahara. 2023a. Camera Motion Generation Method Based on Performer’s Position\nfor Performance Filming. In Proceedings of the 12th IEEE Global Conference on Consumer Electronics (GCCE) . https://doi.org/10.1109/\nGCCE57675.2023.10315539\nDesai Xie, Ping Hu, Xin Sun, Soren Pirk, Jianming Zhang, Radomír Mech, and Arie E Kaufman. 2023b. Gait: Generating aesthetic indoor tours\nwith deep reinforcement learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 7409–7419.\nDejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. 2024. CamCo: Camera-Controllable 3D-Consistent\nImage-to-Video Generation. arXiv preprint arXiv:2406.02509 (2024).\nShiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. 2024. Direct-a-video:\nCustomized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers . 1–12.\nVickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. 2023. Decoupling human and camera motion from videos in the wild. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition . 21222–21232.\nJung Eun Yoo, Kwanggyoon Seo, Sanghun Park, Jaedong Kim, Dawon Lee, and Junyong Noh. 2021. Virtual camera layout generation using a\nreference video. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1–11.\nZixiao Yu, Enhao Guo, Haohong Wang, and Jian Ren. 2022a. Bridging script and animation utilizing a new automatic cinematography model.\nIn2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR) . IEEE, 268–273.\nZixiao Yu, Xinyi Wu, Haohong Wang, Aggelos K Katsaggelos, and Jian Ren. 2023a. Adaptive Auto-Cinematography in Open Worlds. In 2023\nIEEE 6th International Conference on Multimedia Information Processing and Retrieval (MIPR) . IEEE, 1–6.\nZixiao Yu, Xinyi Wu, Haohong Wang, Aggelos K Katsaggelos, and Jian Ren. 2023b. Automated Adaptive Cinematography For User Interaction\nin Open World. IEEE Transactions on Multimedia (2023).\nZixiao Yu, Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos, and Jian Ren. 2024. Automated Adaptive Cinematography for User Interaction\nin Open World. IEEE Transactions on Multimedia 26 (2024), 6178–6190. https://doi.org/10.1109/TMM.2023.3347092\nZixiao Yu, Chenyu Yu, Haohong Wang, and Jian Ren. 2022b. Enabling Automatic Cinematography with Reinforcement Learning. In Proceedings\nof the 5th IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR) . 103–108. https://doi.org/10.1109/\nMIPR54900.2022.00025\nJiwen Zhang. 1999. C-Bézier Curves and Surfaces. Graphical Models and Image Processing 61, 1 (1999), 2–15.\nJunyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. 2024a.\nMonst3r: A simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825 (2024).\nJason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. 2024b. Cameras as Rays: Pose Estimation\nvia Ray Diffusion. In International Conference on Learning Representations (ICLR) .\nShishun Zhang, Longyu Zheng, and Wenbing Tao. 2021. Survey and evaluation of RGB-D SLAM. IEEE Access 9 (2021), 21367–21387.\nZhengyou Zhang. 2021a. Camera calibration. In Computer vision: a reference guide . Springer, 130–131.\nZhengyou Zhang. 2021b. Camera Extrinsic Parameters. In Computer Vision: A Reference Guide . Springer, 131–131.\nZhengyou Zhang. 2021c. Camera parameters (intrinsic, extrinsic). In Computer Vision: A Reference Guide . Springer, 135–140.\nCe Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah. 2023. Deep learning-based\nhuman pose estimation: A survey. Comput. Surveys 56, 1 (2023), 1–37.\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018. Stereo magnification: Learning view synthesis using\nmultiplane images. arXiv preprint arXiv:1805.09817 (2018).\n, Vol. 1, No. 1, Article . Publication date: June 2025.\n--- Page 66 ---\n66 •Dehghanian et. al.\nZehao Zhou. 2024. Continuous Control Reinforcement Learning: Distributed Distributional DrQ Algorithms. arXiv preprint arXiv:2404.10645\n(2024).\nZhizhuo Zhou and Shubham Tulsiani. 2023. SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction. In CVPR . https:\n//doi.org/10.1109/CVPR46700.2023.00193\nCheng Zhu, Guohui Zhang, and Xin Li. 2009. Trajectory generation for camera control in soccer match broadcasting. In Proceedings of the\nIEEE International Conference on Multimedia and Expo . 806–809. https://doi.org/10.1109/ICME.2009.5202588\nFang Zhu, Shuai Guo, Li Song, Ke Xu, Jiayu Hu, et al .2023. Deep review and analysis of recent nerfs. APSIPA Transactions on Signal and\nInformation Processing 12, 1 (2023).\nMu Zhu. 2004. Recall, precision and average precision. Department of Statistics and Actuarial Science, University of Waterloo, Waterloo 2, 30\n(2004), 6.\nMatt Zucker, Nathan Ratliff, Anca D Dragan, Mihail Pivtoraiko, Matthew Klingensmith, Christopher M Dellin, J Andrew Bagnell, and\nSiddhartha S Srinivasa. 2013. CHOMP: Covariant Hamiltonian optimization for motion planning. The International Journal of Robotics\nResearch 32, 9-10 (2013), 1164–1193.\n, Vol. 1, No. 1, Article . Publication date: June 2025.",
  "text_length": 233801
}