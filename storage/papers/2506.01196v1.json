{
  "id": "http://arxiv.org/abs/2506.01196v1",
  "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image\n  Generation",
  "summary": "We introduce OG-VLA, a novel architecture and learning framework that\ncombines the generalization strengths of Vision Language Action models (VLAs)\nwith the robustness of 3D-aware policies. We address the challenge of mapping\nnatural language instructions and multi-view RGBD observations to quasi-static\nrobot actions. 3D-aware robot policies achieve state-of-the-art performance on\nprecise robot manipulation tasks, but struggle with generalization to unseen\ninstructions, scenes, and objects. On the other hand, VLAs excel at\ngeneralizing across instructions and scenes, but can be sensitive to camera and\nrobot pose variations. We leverage prior knowledge embedded in language and\nvision foundation models to improve generalization of 3D-aware keyframe\npolicies. OG-VLA projects input observations from diverse views into a point\ncloud which is then rendered from canonical orthographic views, ensuring input\nview invariance and consistency between input and output spaces. These\ncanonical views are processed with a vision backbone, a Large Language Model\n(LLM), and an image diffusion model to generate images that encode the next\nposition and orientation of the end-effector on the input scene. Evaluations on\nthe Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization\nto unseen environments, with over 40% relative improvements while maintaining\nrobust performance in seen settings. We also show real-world adaption in 3 to 5\ndemonstrations along with strong generalization. Videos and resources at\nhttps://og-vla.github.io/",
  "authors": [
    "Ishika Singh",
    "Ankit Goyal",
    "Stan Birchfield",
    "Dieter Fox",
    "Animesh Garg",
    "Valts Blukis"
  ],
  "published": "2025-06-01T22:15:45Z",
  "updated": "2025-06-01T22:15:45Z",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01196v1",
  "full_text": "--- Page 1 ---\nOG-VLA: 3D-Aware Vision Language Action Model\nvia Orthographic Image Generation\nIshika Singh1, Ankit Goyal2, Stan Birchfield2, Dieter Fox2, Animesh Garg3, Valts Blukis2\n1University of Southern California,2NVIDIA,3Georgia Institute of Technology\nAbstract: We introduce OG-VLA, a novel architecture and learning framework\nthat combines the generalization strengths of Vision Language Action models\n(VLAs) with the robustness of 3D-aware policies. We address the challenge\nof mapping natural language instructions and multi-view RGBD observations to\nquasi-static robot actions. 3D-aware robot policies achieve state-of-the-art per-\nformance on precise robot manipulation tasks, but struggle with generalization\nto unseen instructions, scenes, and objects. On the other hand, VLAs excel at\ngeneralizing across instructions and scenes, but can be sensitive to camera and\nrobot pose variations. We leverage prior knowledge embedded in language and vi-\nsion foundation models to improve generalization of 3D-aware keyframe policies.\nOG-VLA projects input observations from diverse views into a point cloud which\nis then rendered from canonical orthographic views, ensuring input view invari-\nance and consistency between input and output spaces. These canonical views are\nprocessed with a vision backbone, a Large Language Model (LLM), and an image\ndiffusion model to generate images that encode the next position and orientation\nof the end-effector on the input scene. Evaluations on the A RNOLD and C OLOS -\nSEUM benchmarks demonstrate state-of-the-art generalization to unseen environ-\nments, with over 40% relative improvements while maintaining robust perfor-\nmance in seen settings. We also show real-world adaption in 3 to 5 demonstrations\nalong with strong generalization. Videos and resources at https://og-vla.github.io\n1 Introduction\nWe study the problem of mapping natural language instructions and multiple posed RGBD observa-\ntions to robot actions, with specific focus on quasi-static manipulation tasks that can be decomposed\ninto a sequence of end-effector keyframes. This category encompasses a wide variety of tasks such\nas pick-and-place, opening/closing doors and containers, manipulating buttons, valves, switches,\nand more. Building robust policies that solve such tasks in unseen environments remains an open\nchallenge that could enable numerous industrial and household applications, from cleaning and sort-\ning robots to machine tending.\nVLAs have recently demonstrated successful generalization to concepts unseen in the robotics train-\ning data [1, 2, 3, 4], such as manipulating novel objects based on language instructions. While\nachieving generalization breakthroughs, they require massive training datasets [3, 5, 4] and typically\naccept a single RGB view input. As a result, despite the large amount of training data, the result-\ning systems remain sensitive to variations in camera and robot poses, which hurt their adaptability\nto new applications [6]. They also lack explicit visual reasoning when predicting actions (often as\nLLM tokens), constraining their ability to perform precise, generalizable 3D spatial reasoning.\nIn contrast, 3D-aware keyframe based policies learn effectively from few demonstrations and gen-\neralize well to novel camera poses and object placements [7, 8, 9, 10] as confirmed in the camera\nperturbation evaluation study by Pumacay et al. [11]. This success stems from a 3D scene rep-\nresentation within the model, such as a voxel map [7], canonical orthographic views [8], or pointarXiv:2506.01196v1  [cs.RO]  1 Jun 2025\n--- Page 2 ---\nOpen the drawer\nOrthographic Views Generated ActionsFront\nTop\nRightPosition\nRoll\nPitch\nYawGripper Open /Close\nDecoded Next Action\nDistractor\nInput Camera\nCompleted Task\nDistractor\n…Action EncodingFigure 1: OG-VLA illustration. OG-VLA represents robot end-effector keyframes with easy-to-decode an-\nnotations on orthographic images in a set of canonical views. This output encoding enables action prediction\nvia image generation, and using canonical views achieves invariance to input camera poses. The red hotspot is\nthe predicted end-effector position in each image. In this example, the hotspot is indicating the 3D point for\napproaching the drawer handle to open it. The yellow, blue and green hotspots work in tandem with the red\nhotspot to encode the three axes of end-effector orientation. The color of the hotspot on the top-left encodes\nthe gripper open/close state. The system is robust to distractors and changing lighting conditions.\nclouds [10]. However, unlike VLAs, these systems overfit to the training scenes and objects, failing\nto accept instructions that refer to new, previously unseen objects.\nWe propose OG-VLA (Orthographic-image Generation Vision-Language-Action model), a novel\nrobot policy architecture that combines the generalization strengths of VLAs with robustness of\n3D-aware policies. OG-VLA uses LLMs and image generation to map posed RGBD images and\nlanguage to 6-DOF end-effector pose keyframes in a sequence. The system comprises four compo-\nnents: a point cloud renderer that renders a scene reconstruction to canonical orthographic views,\na vision backbone that encodes these views into visual embeddings, an LLM that predicts action\ntokens, and an image diffusion model that decodes these action tokens to predict actions on each of\nthe orthographic views through image generation, which we decode to final 3D poses. The LLM\nand image diffusion models are trained end-to-end, so that they work together to produce consistent\nand precise predictions required for robotic manipulation. Figure 1 illustrates our method.\nWe conduct simulation and real-robot experiments. In simulation, the A RNOLD [12] benchmark\ntests generalization to unseen objects and environments, while C OLOSSEUM [11] evaluates robust-\nness to variations in camera poses, object poses, colors, and distractors. We show significantly\nimproved performance on generalization tests of both benchmarks, achieving state-of-the-art on\nARNOLD [12]. In real-world experiments, we show our method’s ability to learn manipulation\ntasks from as few as 3 to 5 demonstrations, highlighting its suitability for kinesthetic teaching and\nrapid adaptation to new domains. We also present a detailed study of our model architecture design\nchoices, providing insights into its functioning as well as potential future improvements.\n2 Related Work\nLarge Language Models have seen an explosion in research on their use-cases in robotics, such as\ntask planning [13, 14, 15], reward generation for reinforcement learning [16, 17], and interfacing\nwith vision models [18]. However, although these methods generalize at a high level, they assumed\naccess to low-level skills such pick, place, open, and close, along with perception and scene repre-\nsentation systems. These skills are hard to build, and their development is in line with the goals of\nthis work.\n2\n--- Page 3 ---\n3D-aware keyframe-based multi-task policies have shown the ability to learn complex manipulation\nbehaviors from as little as ten demonstrations per task. Examples of these works include PerAct [7]\nbased on voxel grids, RVT [8, 9] based on orthonormal views, and Act3D [10] based visual feature\npoint clouds as the scene representation. These systems significantly improve upon image-based\npolicies [19, 20] in the amount of data they require and their robustness to new object placements\nat test-time. However, these systems have been trained from scratch on specific tasks, robots, and\nenvironments, and struggle to generalize to new objects and scenes, or instructions.\nVision Language Action models (VLAs) leverage large-scale prior knowledge from LLMs and vi-\nsion foundation models to create robot policies that generalize to new concepts and objects at test\ntime [1, 21, 2, 3, 22, 4]. However, these systems require huge amount of demonstration data to train.\nFor example, Kim et al. [3] use over 900k demos from the Open-X Embodiment dataset [23]. De-\nspite the large quantity of data, the resulting systems are sensitive to changes in the 3D environment,\nsuch as different camera poses relative to the robot system. Our system trains with significantly less\ndata to achieve state-of-the-art performance and generalization.\nGenerative image and video models have been explored for robot policies as well. Du et al. [24]\nexplore mapping generated videos of robots back to control via inverse dynamics. Genima [25]\nmakes this easier by drawing robot joint annotations as textured spheres on the video, while RT-\nTrajectory [26] generates trajectory annotations on images. In contrast, we predict annotations on\n3D canonical views, which allows us to more easily solve for 3D end-effector poses even in free\nspace, and improves generalization from few demonstrations by enabling SE(3) data augmentation.\nWe show that our model can do free space reasoning for tasks such as lift the bottle 30cm off the\nground (Figure 5). Methods that directly predict heatmaps on images have also been previously used\nfor language-guided navigation tasks [27, 28].\n3 Method: Orthographic-image Generation Vision-Language-Action model\nAt deployment time, the input to our system is a language instruction l, and a set of observations\nOk={Ik, Dk, Pk, Kk}, where Ikis an RGB image, Dkis a corresponding depth image, Pkis the\ncamera pose, and Kkare the camera intrinsics, with a camera index k. The output of our system is\nan end-effector state s=⟨p, ω⟩, which consists of a position target p, rotation target ω. To complete\na task, we sequentially execute our system, at each time-step using a motion planner to reach the\npredicted s, and obtain the next set of observations. Figure 2 shows our model architecture.\n3.1 Multi-Modal Vision and Language Model\nAt the core of our system is a Large Language Model (LLM). The LLM takes as input a se-\nquence of input tokens (vectors) ⟨t1, . . . , t i⟩, and generates a sequence of output tokens (vectors)\n⟨ti+1, . . . , t i+j⟩. We use three types of input and output tokens: (1) text tokens, computed from\na text tokenizer and embedding table, (2) input image tokens, which are either patch tokens or the\nimage CLS token [29], computed by a visual encoder, and projected to LLM space through a learned\nMLP input projection, and (3) output image (action) tokens, which we add to the LLM vocabulary\nand decode as a special token using an additional MLP decoder. The output image tokens represent\nthe next robot action. We use an image diffusion model to decode the image tokens into actions by\nproducing images that contain annotations that illustrate the gripper position and rotation over a set\nof input views of the scene. The end-effector state sis decoded from these image annotations.\n3.2 3D-Aware Reasoning with Orthogonal Orthographic Projections\nTo imbue the LLM with 3D-awareness, we unproject all input camera images into a point cloud in a\ncanonical workspace. We then render the scene from a fixed set of views (independent of the input\ncamera poses) before feeding them to the LLM. This brings the input and output in the same space,\nand our selection of the views (orthogonal views such as \"front\", \"top\", \"left, \"right\", rendered in\northographic mode) ensures no ambiguity in output.\n3\n--- Page 4 ---\n🔥 LLM\nCLS\nCLS\nCLS\nCLS\n🔥 Output Projection \nFront\nTop\nLeft\nRight\nTokenizer \nTask: ‘lift the bottle …’ Where should the robot move next? Show the gripper’s next pose as..\nFrontTop\nLeft\nRight\nDetokenizer \nVisual Encoder\nVisual Encoder\nVisual Encoder\n    Visual Encoder\nImage Generator\nImage Generator\nImage Generator\n🔥 Image Generator\nLift the bottle twenty centimeters from the groundRGBD viewsTaskInputPoint cloud construction and orthographic rendering\n🔥 Input Projection \n🔥 Input Projection \n🔥 Input Projection \n🔥 Input Projection \nCLS\n…\nCLS\n…\nCLS\nCLS\n…+\n…+\n…+\n+\n…\n…\nExtract most likely 3D point and rotation angles…\n…\nThe next pose for the given task ‘lift …’ for timestep 1 is shown in the generated images.OutputPosition, rotation,  open/closeTextGripper State\nPrompt \nText input tokens\nText output tokens\nImage tokensImage tokens\nPatch Tokens\nImage tokensFigure 2: Model Overview. The input to our system is a task instruction and multiple RGB-D views of\nthe scene. We build a point cloud from the input views and re-project it to orthographic projections from\northonormal views. The orthonormal views are fed into a Visual Encoder to derive a set of CLS and patch em-\nbeddings. CLS embeddings are projected into the LLM latent space and concatenated with a tokenized prompt\nthat queries the next end-effector state and specifies the output format. The LLM outputs image token embed-\ndings to condition the I MAGE GENERATOR , which are projected to the I MAGE GENERATOR ’s input latent space,\nand then concatenated with skip-connected visual features. The I MAGE GENERATOR generates heatmaps-one\nper orthographic view-indicating the next end-effector pose. We decode the heatmaps by interpreting them as\nprobabilities, and inferring the most likely 3D position across all views and one rotation angle per view.\nInput Reprojection to Canonical Views. For each camera observation {Ik, Dk, Pk, Kk}withNk\nvalid depth pixels, we compute a point cloud Ck∈ RNk×6, where each contains the RGB color and\n3D coordinate in a fixed reference frame. We compute an aggregated point cloud C=SK\nk=1Ck\noverall all input cameras. Next, we define a set of mcanonical cameras {PC\nc, KC\nc}c=1,...,m , where\neachPC\ncis a camera pose and KC\ncare the intrinsics for a given orthograpic camera c. We then\nrender the point cloud to an RGB image IC\nc∈ Rh×w×3seen by each orthographic camera.\nOur system allows adapting the set of canonical camera parameters based on application and\nworkspace geometry. In this work, we use four orthographic cameras that view the workspace\nfrom front, left, right and top directions, such that the workspace fills the camera image. Figure 2\nshows the input RGBD views and the resulting orthographic images. While we use point clouds, our\nmethod may accommodate any 3D representation (e.g., neural radiance fields [30, 31] or novel-view\nsynthesis methods [32]) that supports canonical view rendering.\nLLM Input and Output. The canonical views {IC\nc}c=1,...,m constitute the visual input to our\nVLM system. We process each view with a V ISUAL ENCODER and obtain an image embedding\n(CLS token) eCLS\nc, as well as a sequence of image patch embeddings ⟨e1\nc, . . . , en\nc⟩. Next, we apply\nan input projection neural network to map each CLS embedding eCLS\nc, c∈ {1, . . . , m }into a token\ntCLS\nc compatible with the LLM input space. Finally, the input sequence to the LLM is the following\nsequence of tokens: ⟨PROMPT (l), t1\nCLS, . . . , tm\nCLS⟩, where P ROMPT (l)is a function that constructs\na prompt for the instruction land tokenizes it in a way compatible with the LLM.\nWe feed the input sequence to the LLM to produce an output sequence of format:\n⟨t1\na, t2\na, t3\na, t4\na, to\n1, to\n2, . . . , to\nj⟩,where t(·)\naare four image tokens that together represent the next action,\nandto\n(·)are accompanying text tokens as a response to the input prompt. We show the abbreviated\nprompt in Figure 2. Full prompt and text response is provided in the Appendix B.\nImage Token Decoding for Action Prediction. Successful robot manipulation, such as picking ob-\njects or grabbing drawer handles, requires very precise end-effector position predictions. Although\nthe LLM can output gripper poses in text with generally close predictions, we find them to lack the\nprecision needed for practical applications. We thus propose decoding the image tokens to action\n4\n--- Page 5 ---\nannotations over each of the canonical images. From these annotations, we can infer the next 3D\ngripper position and orientation by decoding and aggregating the image annotations in 3D.\nFirst, we project each of the output image tokens ti\naback to the visual embedding space with an\noutput projection layer to obtain a set of embeddings ei\na;i∈ {1, . . . , 4}. Next, we use these em-\nbeddings as well as the output of the V ISUAL ENCODER (all patch embeddings ⟨e1\nc, . . . , en\nc⟩, as well\nas the CLS token eCLS\nc) to condition an I MAGE GENERATOR network for each orthographic camera\nc∈ {1, . . . , m }. The network outputs an RGB image with action predictions overlaid on the input\ncanonical views:\nHc=IMAGE GENERATOR (ei\na, ex\nc),\ni∈{1, . . . , 4}c∈ {1, . . . , m }x∈ {CLS, 1, . . . , 256} (1)\nwhere Hc∈ { Rh,w, 3}is a reconstruction of the input canonical image IC\ncwith overlaid annota-\ntions that encode the gripper position and orientation. Each image is aligned with one of the m\ncanonical input views c, as shown in Figure 2. While any image generator can be used, we use\nStableDiffusion 1.5 [33]. ei\naacts as the textual conditioning and ex\ncas the visual conditioning for\nthe I MAGE GENERATOR . In practice, we use ei\na+CLIP (PROMPT (l))for textual conditioning, es-\nsentially adding a residual skip connection from a direct text encoding, a design choice informed\nby empirical experiments. CLIP is the textual encoder used for pretraining the StableDiffusion 1.5\nmodel.\nExtracting 3D Position and Rotation from Generated Images. We generate gripper position and\nEuler rotation as Gaussian distributions overlaid in different color channels on the input orthographic\nviews. We infer a 3D position phmthat best explains the predictions in each of the canonical views\nby solving the optimization problem:\nphm= arg max\npY\nc=1,...,m(Hc[CAMERA PROJECTION (p, PC\nc, KC\nc)] +ϵ), (2)\nwhere C AMERA PROJECTION projects the 3D point pto 2D image coordinates w.r.t. the orthographic\ncamera c. The square brackets represent a 2D pixel-wise indexing operation with interpolation to\nsupport sub-pixel coordinates. ϵis a small value we add to allow decoding in situations where one\nof the heatmaps is zero for all 3D points.\nWe predict Euler rotations on the images such that the rotation along an axis is overlaid on the\ncanonical view along that axis using 3 different colors. We present x-axis rotation on the front view,\ny-axis rotation on left and right views, and z-axis rotation on the top view as shown in Figure 2.\nRotations are overlaid as Gaussian distributions at 30 pixel radius with reference to a horizontal line\ndrawn from the translation point towards the right of the image. To decode rotation, we first extract\nthe pixel location of the most likely rotation ( rc\nx, rc\ny) and translation point ( pc\nx, pc\ny) from the Gaussian\ndistributions using a filtering operation for each view, and then compute the rotation angles using\nthe arctangent function. The gripper’s open/close state is encoded with binary colored hotspots on\nthe top-left of the image, as shown in Figure 1.\n3.3 Training and Implementation Details\nOur architecture builds on X-VILA [34], a multi-modal chat model supporting language, visual,\nvideo, and audio modalities. OG-VLA is initialized from X-VILA’s pretrained weights to leverage\nits any-to-any modality alignment, focusing on text-image input to text-image output alignment.\nWe leave study of enriching human-robot interaction using the other modalities to future work.\nWe train OG-VLA using DeepSpeed [35]. We freeze the visual encoder and tune the LLM, input\nand output projection networks, and I MAGE GENERATOR with end-to-end gradient flow. Following\nX-VILA, we use ImageBind [36] as the visual encoder, linear layers for input and output projec-\ntions, and Stable Diffusion 1.5 [37] for the I MAGE GENERATOR . The LLM is based on Vicuna-7b\nv1.5 architecture. ImageBind encodes images into 256 patches (16×16) with a CLS token. Each\ntraining sample includes a natural language instruction l, visual observations Ik, Dk, Pk, Kk, and\nground-truth gripper state ˆs. Following prior work [7, 8], we augment each keyframe with NSE(3)-\ntransformed purterbations: translation in [±0.1m, ±0.1m, ±0.1m], rotation in [±0°, ±0°, ±90°]. All\n5\n--- Page 6 ---\nPickup Reorient Open Close Open Close Pour Transfer Overall\nModel Object Object Drawer Drawer Cabinet Cabinet Water Water\n6D-CLIPort [38] 6.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.8\n-Novel Object 8.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n-Novel Scene 10.4 0.0 0.0 0.0 0.0 1.3 0.0 0.0 1.5\n-Novel State 0.0 0.0 0.0 0.8 0.0 0.0 0.0 0.0 0.1\nPerAct [7] 83.3±2.4 16.7±6.2 30.0±10.8 31.7±8.5 25.0±0.0 30.0±0.0 36.7±6.2 18.3±2.4 34.0±3.1\n-Novel Object 75.0±0.0 3.3±2.4 0.0±0.0 23.3±13.1 0.0±0.0 0.0±0.0 30.0±4.1 1.7±2.4 16.7±2.6\n-Novel Scene 75.0±4.1 13.3±2.4 13.3±9.4 30.0±14.1 0.0±0.0 6.7±2.4 26.7±6.2 3.3±2.4 21.0±3.1\n-Novel State 16.7±2.4 1.7±2.4 5.0±0.0 11.7±6.2 0.0±0.0 0.0±0.0 5.0±0.0 11.7±2.4 6.5±1.2\nOG-VLA@30k 86.7±2.9 15.0±8.7 38.3±2.9 51.7±2.9 0.0±0.0 16.7±2.9 25.0±5.0 16.7±7.6 31.2±2.9\n-Novel Object 85.0±5.0 0.0±0.0 1.7±2.9 55.0±13.2 1.7±2.9 5.0±5.0 18.3±2.9 6.7±7.6 21.7±0.7\n-Novel Scene 70.0±2.9 1.7±2.8 26.7±11.5 36.7±5.8 1.7±2.9 1.7±2.9 16.7±11.5 8.3±2.9 20.8±1.3\n-Novel State 0.0±0.0 13.3±7.6 13.3±2.9 20.0±0.0 0.0±0.0 0.0±0.0 8.3±7.6 13.3±2.9 8.5±1.9\nOG-VLA@100k 88.3 ±2.4 16.7±9.4 48.3±2.4 56.7±2.4 6.7±4.7 23.3±16.5 33.3±6.2 28.3±2.4 37.7±0.6\n-Novel Object 65.0±8.2 15.0±4.1 1.7±2.4 58.3±12.5 0.0±0.0 5.0±4.1 45.0±8.2 8.3±4.7 24.8±1.2\n-Novel Scene 75.0±7.1 13.3±8.5 31.7±4.7 51.7±2.4 1.7±2.4 5.0±4.1 26.7±2.4 25.0±7.1 28.8±0.5\n-Novel State 0.0±0.0 13.3±2.4 25.0±7.1 15.0±4.1 0.0±0.0 0.0±0.0 6.7±4.7 20.0±7.1 10.0±0.9\nTable 1: Success rate on ARNOLD [12]. Success rates for 8 tasks and 4 test splits are shown for 2 baseline\nmodels and our model at specified training iterations (30k and 100k). The first row for each model is the Novel\nPose split (the Test set in ARNOLD). The numbers in bold represent the best system for each task and test split.\nmodels are trained on 8×A100 GPUs with batch size 64. All models are trained for one run, and\nthe LLM is LoRA-finetuned. Inference is done on a single A100 GPU. For inference-time image\nsampling from Stable Diffusion 1.5, we use 100 steps and a guidance scale of 7.0. These parameters\nresult in reasonable sampled image quality and avoid jitters in the sampled image from the model’s\nlatent space.\n4 Simulation Experiments\n4.1 Benchmarks and Datasets\nWe evaluate our method on the A RNOLD [12] and C OLOSSEUM [11] benchmarks, which test\nlanguage-grounded robot task learning in realistic 3D scenes with emphasis on testing general-\nization. A RNOLD uses five input cameras (front, base, left, wrist top, wrist bottom) and includes\neight language-conditioned tasks (see Table 1), each with four generalization test splits: (1) Novel\nPose (held-out object/robot placements), (2) Novel Object (unseen objects), (3) Novel Scene (un-\nseen scenes with seen objects), and (4) Novel State (unseen goal states). A RNOLD tasks follow\na two-keyframe format—grasping and manipulating (e.g., pull drawer to 50% open)—and do not\nrequire gripper state prediction. These tasks demand both object pose and free space reasoning.\nCOLOSSEUM has four cameras (front, left shoulder, right shoulder, wrist) and features 20 language-\nconditioned tabletop tasks (e.g., close box, empty dishwasher) with 2–13 gripper keyframes (average\n6), requiring gripper open/close prediction. It evaluates generalization via the all perturbation test\nset, which simultaneously alters object/table/background appearance, lighting, camera pose, and\nadds distractors.\nWe train on A RNOLD ’s training split with 8 tasks, ~500 demos/task, and 2 keyframes/demo (7100\nkeyframes). We separately train on C OLOSSEUM training split with 100 demos/task. For SE(3)\naugmentations, we set N= 10 for A RNOLD (~70k training samples) and N= 5for C OLOSSEUM\n(~1M training samples). All A RNOLD models are trained for 30k iterations and the final result is\nreported at 30k and 100k iterations (30k takes 1.5 days, and 100k takes 5 days to train). We train on\nCOLOSSEUM for 250k iterations due to the larger training data size. We noticed that further training\nbeyond 250k iterations degrades image generation quality. We conduct ablation studies at 30k on\nARNOLD due to compute constraints.\nEach A RNOLD test split includes 20 episodes; C OLOSSEUM uses 25. We evaluate each A RNOLD\nmodel over 3 runs to account for simulator’s motion planner noise during keypoint execution, and we\nreport means and standard deviations. Baseline models were trained by the respective benchmark au-\n6\n--- Page 7 ---\nthors. We re-run PerAct [7] for 3 runs on A RNOLD for fair comparison, and report 6D-CLIPort [38]\nresults from A RNOLD [12] due to lack of checkpoint release.\n4.2 Simulation Results\nARNOLD We present our model results in Table 1. OG-VLA outperforms the baseline (PerAct)\nfor most of the tasks’ Novel Pose split (seen objects and scenes), with a task-averaged relative im-\nprovement of 10.8%. We present our result at 30k and 100k iterations. OG-VLA improves signifi-\ncantly across generalization splits (Novel Object, Scene, and State), with relative increases of 20.0%\nand 46.5% respectively at 30k and 100k iterations in overall success rates. This demonstrates the\neffectiveness of our method in adapting pretrained visual and textual priors to robotics applications.\nAveraged Success Rate (%)R3MMVP3DDARVTPerActOG-VLA6.012.0\n10.5\n7.2\n6.4\n5.3\n0.8\n0.6\nFigure 3: Evaluation on the COLOS-\nSEUM benchmark [11]. Task-averaged\nsuccess rate shows that OG-VLA outper-\nforms all baselines on the hardest general-\nization test set ( all perturbation ).COLOSSEUM Figure 3 shows task-averaged success\nrate on all perturbation test set on the C OLOS -\nSEUM benchmark. We compare with baselines re-\nported in C OLOSSEUM , including R3M [19], MVP [20],\n3DDA [39], RVT [8], and PerAct [7]. OG-VLA improves\nupon the baselines by a relative increase of 45.8%. The\nabsolute performance remains low however (10.5%). We\nobserve that C OLOSSEUM tasks are much harder to learn\ndue to longer sequence of keyframe predictions, lead-\ning to error accumulation for an imitation learning based\nmethod.\n4.3 Design Choice Ablations\nWe conducted extensive ablations on the A RNOLD benchmark to validate our design choices. We\nhighlight the main findings in this section, with detailed results and analysis in Appendix C.\nAction Prediction Approaches We experimented with two common alternatives to our image\ngeneration approach: (1) direct text-based action prediction and (2) adding additional action tokens\nto the LLM vocabulary that are decoded with MLP decoders directly to gripper states. Both alterna-\ntives failed to learn effective policies under our training conditions, likely due to insufficient visual\nreasoning capabilities for precise manipulation tasks with limited training data.\nImage Generation Modes We compared three image generation approaches: (1) generation with-\nout scene reconstruction (black background), (2) generation with scene reconstruction (action anno-\ntations overlaid on the input image), and (3) generation with faded reconstruction (rescaled to 0-127\nrange, keeping colors in range 128-255 reserved for action annotations). Generation with faded\nreconstruction performed best overall on generalization splits (e.g. 23.8% vs. 12.8% without re-\nconstruction and 20.8% with full reconstruction on the A RNOLD novel scene split). Generating full\nreconstructions complicates action decoding due to occasional color collisions. Generating black\nbackgrounds was unstable during training, likely due to the challenge of adapting a model of natural\nimages towards generating plain backgrounds.\nArchitecture Components Our ablations revealed several key insights: (1) Unlike prior work,\ntiling orthographic views decreased performance (24.8% vs. 31.2%); (2) Removing the LLM sig-\nnificantly reduced performance (20.0% vs. 31.2%), likely due to its strong priors and its role in\nconditioning consistent generation across views; (3) Directly bypassing the instruction to the Im-\nage Generator decreased performance by 9.5%, suggesting the LLM’s image token outputs preserve\ncrucial task information. These findings validate the importance of each component in OG-VLA.\n7\n--- Page 8 ---\n5 Real Robot Experiments\nExperimental Setup We perform real-world experiments on a Franka Emika Panda arm mounted\non a tabletop, with a single front-facing camera, as shown in Figure 1. We collect 3–5 demos\nfor 4 real-world tasks—22 demos in total—with human-annotated keyframes and a motion plan-\nner [40] to achieve annotated keyframes. We train both the baseline and our model on this dataset.\nFor our model, we augment each keyframe with 10 SE(3) perturbations and finetune the Arnold-\npretrained OG-VLA@30k checkpoint for another 10k iterations with a batch size of 64. For π0-\nFAST [4] baseline, we use the provided pretrained checkpoint trained on 10k+ hours of robot data\nacross various robot setups with actions in both joint and end-effector control spaces. We fine-\ntune it on our dataset with joint angle actions for 30k iterations with a batch size of 32, as used\nin most of their pre-training and finetuning experiments [4, 22]. During inference, the model pre-\ndicts an action chunk of 10 actions; we execute the last action in the sequence for faster execution.\nPickup Put Object Open Close\nModel Object in Drawer Drawer Drawer\nOG-VLA@10k 100.0 90.0 60.0 90.0\n- Novel Object 80.0 70.0 30.0 50.0\n- Novel Scene 90.0 80.0 50.0 90.0\nTable 2: Real world success rate (%). Success rates are reported for 4\nreal world tasks and novel pose, novel object, and novel scene test splits,\naveraged over 10 episodes each.Quantitative Results We re-\nport our results in Table 2. Each\ntest set success rate is averaged\nover 10 episodes. For novel\nobject variation, we use unseen\ncolored and shaped objects. For\nnovel scene variation, we in-\ntroduce distractors and change\nlighting, background, and table\nappearance. Please refer to Appendix D for training and test scene details. Our results show that\nOG-VLA can adapt to new tasks with only 3–5 demonstrations and generalize well to novel poses,\nobjects, and scenes. Although we attempted to compare with π0-FAST, it failed at all tasks, learning\nonly to reach the block with 30% success. We hypothesize that this is due to the small number of\ndemonstrations and lack of support for SE(3) data augmentation without our 3D representation.\nT=1T=2T=3Obsevation InputGenerated ActionsT=1\nTraining TaskNovel SceneNovel ObjectTask: Put object in the drawer\nTraining Task: Put object in the drawerNovel ObjectNovel Scene\nFigure 4: Qualitative example. Showing generalization of OG-VLA\nto unseen scenarios.Qualitative Results We show\nqualitative examples of real\nworld evaluations in Figure 4\nfor the task ‘put object in the\ndrawer’. For training demon-\nstrations, we use a blue cube.\nDuring evaluation, we replace it\nwith bottle or perturb the scene\nby placing a newspaper under\nthe cube also make te scene\nbrighter. OG-VLA can general-\nize to manipulating differnt ob-\njects as well as to unseen scenes\nfor a given task.\n6 Conclusion\nWe introduced OG-VLA, a novel architecture and learning framework that combines the gener-\nalization strengths of Vision-Language-Action (VLA) models with the robustness of 3D-aware\nkeyframe policies for robotic manipulation. By leveraging foundation models in language and vi-\nsion, OG-VLA improves generalization to unseen instructions, objects, and scenes while maintain-\ning precise control. Our approach ensures input-view invariance by projecting multi-view RGBD\nobservations into a canonical point cloud representation, which is processed through a vision back-\nbone, an LLM, and an image diffusion model to generate actions as images. OG-VLA achieves\n8\n--- Page 9 ---\nstate-of-the-art performance on robotic manipulation tasks, particularly on scene and object gener-\nalization tests. Moreover, OG-VLA can adapt to real-world tasks in 3-5 demonstrations with strong\ngeneralization to unseen objects and scenes. These results highlight the effectiveness of integrating\npretrained visual and text priors into a structured 3D-aware framework for robot learning. Future\nwork will explore extending OG-VLA to complex long-horizon tasks, external data augmentation\nand co-training techniques.\n7 Limitations\nOG-VLA has several strengths, but is not free of limitation. One challenge arises from the reliance\non orthographic canonical views, which, while effective in many scenarios, can struggle in situations\nwith severe occlusions, such as when multiple objects are stacked on a shelf against a wall. Oc-\nclusions may lead to partial or incorrect scene representations, potentially affecting the downstream\ntask performance. Moreover, keyframe-based policies cover a broad range of tasks, however, are not\nable to solve several dynamic tasks, such as object tossing, or pressing with a desired force. Another\nconsideration is the computational cost associated with OG-VLA. The current training procedure is\ncomputationally intensive, requiring substantial time and resources. We aim to address this by op-\ntimizing the model architecture, and exploring strategies such as distillation or parameter-efficient\nfine-tuning to accelerate learning while maintaining performance. Markovian policy learning also\nmakes long horizon tasks quite challenging where several keyframes need to be predicted.\nReal World Limitations Single camera input makes some of the orthographic views redundant,\nuninformative or even noisy, which affects downstream performance. In this case, even if the pre-\ndictions are correct in a few views, the noisy predictions affect the eventually decoded keyframe.\nReferences\n[1] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378 , 2023.\n[2] A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,\nA. Dubey, C. Finn, et al. RT-2: Vision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818 , 2023.\n[3] M. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster,\nG. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine,\nP. Liang, and C. Finn. OpenVLA: An open-source vision-language-action model. arXiv\npreprint arXiv:2406.09246 , 2024.\n[4] K. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and\nS. Levine. FAST: Efficient action tokenization for vision-language-action models, 2025.\n[5] O. X.-E. Collaboration, A. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee,\nA. Pooley, A. Gupta, A. Mandlekar, A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan,\nA. Khazatsky, A. Rai, A. Gupta, A. Wang, A. Kolobov, A. Singh, A. Garg, A. Kembhavi,\nA. Xie, A. Brohan, A. Raffin, A. Sharma, A. Yavary, A. Jain, A. Balakrishna, A. Wahid,\nB. Burgess-Limerick, B. Kim, B. Schölkopf, B. Wulfe, B. Ichter, C. Lu, C. Xu, C. Le, C. Finn,\nC. Wang, C. Xu, C. Chi, C. Huang, C. Chan, C. Agia, C. Pan, C. Fu, C. Devin, D. Xu,\nD. Morton, D. Driess, D. Chen, D. Pathak, D. Shah, D. Büchler, D. Jayaraman, D. Kalash-\nnikov, D. Sadigh, E. Johns, E. Foster, F. Liu, F. Ceola, F. Xia, F. Zhao, F. V . Frujeri, F. Stulp,\nG. Zhou, G. S. Sukhatme, G. Salhotra, G. Yan, G. Feng, G. Schiavi, G. Berseth, G. Kahn,\nG. Yang, G. Wang, H. Su, H.-S. Fang, H. Shi, H. Bao, H. B. Amor, H. I. Christensen, H. Fu-\nruta, H. Walke, H. Fang, H. Ha, I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Abou-Chakra,\nJ. Kim, J. Drake, J. Peters, J. Schneider, J. Hsu, J. Bohg, J. Bingham, J. Wu, J. Gao, J. Hu,\n9\n--- Page 10 ---\nJ. Wu, J. Wu, J. Sun, J. Luo, J. Gu, J. Tan, J. Oh, J. Wu, J. Lu, J. Yang, J. Malik, J. Silvério,\nJ. Hejna, J. Booher, J. Tompson, J. Yang, J. Salvador, J. J. Lim, J. Han, K. Wang, K. Rao,\nK. Pertsch, K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, K. Oslund,\nK. Kawaharazuka, K. Black, K. Lin, K. Zhang, K. Ehsani, K. Lekkala, K. Ellis, K. Rana,\nK. Srinivasan, K. Fang, K. P. Singh, K.-H. Zeng, K. Hatch, K. Hsu, L. Itti, L. Y . Chen, L. Pinto,\nL. Fei-Fei, L. Tan, L. J. Fan, L. Ott, L. Lee, L. Weihs, M. Chen, M. Lepert, M. Memmel,\nM. Tomizuka, M. Itkina, M. G. Castro, M. Spero, M. Du, M. Ahn, M. C. Yip, M. Zhang,\nM. Ding, M. Heo, M. K. Srirama, M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess,\nN. J. Joshi, N. Suenderhauf, N. Liu, N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer,\nO. Bastani, P. R. Sanketi, P. T. Miller, P. Yin, P. Wohlhart, P. Xu, P. D. Fagan, P. Mitrano,\nP. Sermanet, P. Abbeel, P. Sundaresan, Q. Chen, Q. Vuong, R. Rafailov, R. Tian, R. Doshi,\nR. Mart’in-Mart’in, R. Baijal, R. Scalise, R. Hendrix, R. Lin, R. Qian, R. Zhang, R. Men-\ndonca, R. Shah, R. Hoque, R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Lin, S. Moore,\nS. Bahl, S. Dass, S. Sonawani, S. Song, S. Xu, S. Haldar, S. Karamcheti, S. Adebola, S. Guist,\nS. Nasiriany, S. Schaal, S. Welker, S. Tian, S. Ramamoorthy, S. Dasari, S. Belkhale, S. Park,\nS. Nair, S. Mirchandani, T. Osa, T. Gupta, T. Harada, T. Matsushima, T. Xiao, T. Kollar, T. Yu,\nT. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell, T. Chung, V . Jain, V . Vanhoucke,\nW. Zhan, W. Zhou, W. Burgard, X. Chen, X. Chen, X. Wang, X. Zhu, X. Geng, X. Liu,\nX. Liangwei, X. Li, Y . Pang, Y . Lu, Y . J. Ma, Y . Kim, Y . Chebotar, Y . Zhou, Y . Zhu, Y . Wu,\nY . Xu, Y . Wang, Y . Bisk, Y . Dou, Y . Cho, Y . Lee, Y . Cui, Y . Cao, Y .-H. Wu, Y . Tang, Y . Zhu,\nY . Zhang, Y . Jiang, Y . Li, Y . Li, Y . Iwasawa, Y . Matsuo, Z. Ma, Z. Xu, Z. J. Cui, Z. Zhang,\nZ. Fu, and Z. Lin. Open X-Embodiment: Robotic learning datasets and RT-X models, 2023.\n[6] Y . Li, Y . Deng, J. Zhang, J. Jang, M. Memmel, C. R. Garrett, F. Ramos, D. Fox, A. Li, A. Gupta,\nand A. Goyal. HAMSTER: Hierarchical action models for open-world robot manipulation. In\nICLR , 2025.\n[7] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-Actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL) , 2022.\n[8] A. Goyal, J. Xu, Y . Guo, V . Blukis, Y .-W. Chao, and D. Fox. RVT: Robotic view transformer\nfor 3d object manipulation. Proceedings of the 7th Conference on Robot Learning (CoRL) ,\n2023.\n[9] A. Goyal, V . Blukis, J. Xu, Y . Guo, Y .-W. Chao, and D. Fox. RVT-2: Learning precise manip-\nulation from few demonstrations. arXiv preprint arXiv:2406.08545 , 2024.\n[10] T. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki. Act3D: 3d feature field transformers\nfor multi-task robotic manipulation. In 7th Annual Conference on Robot Learning , 2023.\n[11] W. Pumacay, I. Singh, J. Duan, R. Krishna, J. Thomason, and D. Fox. The COLOSSEUM:\nA benchmark for evaluating generalization for robotic manipulation. In RSS 2024 Workshop:\nData Generation for Robotics , 2024.\n[12] R. Gong, J. Huang, Y . Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S.-C.\nZhu, et al. ARNOLD: A benchmark for language-grounded task learning with continuous\nstates in realistic 3d scenes. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , 2023.\n[13] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakr-\nishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J.\nRuano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y . Kuang, K.-H. Lee,\nS. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes,\nP. Sermanet, N. Sievers, C. Tan, A. Toshev, V . Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu,\nM. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances.\nInarXiv preprint arXiv:2204.01691 , 2022.\n10\n--- Page 11 ---\n[14] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. ProgPrompt: Generating situated robot task plans using large language models.\nIn2023 IEEE International Conference on Robotics and Automation (ICRA) , pages 11523–\n11530, 2023.\n[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng.\nCode as Policies: Language model programs for embodied control. In arXiv preprint\narXiv:2209.07753 , 2022.\n[16] Y . J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y . Zhu, L. Fan, and\nA. Anandkumar. Eureka: Human-level reward design via coding large language models. In\nICLR , 2024.\n[17] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. Gonzalez Arenas, H.-T. Lewis Chiang,\nT. Erez, L. Hasenclever, J. Humplik, B. Ichter, T. Xiao, P. Xu, A. Zeng, T. Zhang, N. Heess,\nD. Sadigh, J. Tan, Y . Tassa, and F. Xia. Language to rewards for robotic skill synthesis. Arxiv\npreprint arXiv:2306.08647 , 2023.\n[18] W. Huang, C. Wang, R. Zhang, Y . Li, J. Wu, and L. Fei-Fei. V oxPoser: Composable 3d value\nmaps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973 , 2023.\n[19] S. Nair, A. Rajeswaran, V . Kumar, C. Finn, and A. Gupta. R3M: A universal visual represen-\ntation for robot manipulation. In 6th Annual Conference on Robot Learning , 2022.\n[20] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot\nlearning with masked visual pre-training. In 6th Annual Conference on Robot Learning , 2022.\n[21] A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Ju-\nlian, D. Kalashnikov, Y . Kuang, I. Leal, K.-H. Lee, S. Levine, Y . Lu, U. Malla, D. Manjunath,\nI. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao,\nM. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran,\nV . Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. RT-1:\nRobotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817 ,\n2022.\n[22] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Haus-\nman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair,\nK. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky. π0: A\nvision-language-action flow model for general robot control, 2024.\n[23] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai,\nA. Singh, A. Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x mod-\nels.arXiv preprint arXiv:2310.08864 , 2023.\n[24] Y . Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel.\nLearning universal policies via text-guided video generation. In Advances in Neural Informa-\ntion Processing Systems , volume 36, pages 9156–9172, 2023.\n[25] M. Shridhar, Y . L. Lo, and S. James. Generative image as action models. In Proceedings of\nthe 8th Conference on Robot Learning (CoRL) , 2024.\n[26] J. Gu, S. Kirmani, P. Wohlhart, Y . Lu, M. G. Arenas, K. Rao, W. Yu, C. Fu, K. Gopalakrishnan,\nZ. Xu, et al. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. arXiv\npreprint arXiv:2311.01977 , 2023.\n[27] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction\nfollowing as bayesian state tracking. Advances in neural information processing systems , 32,\n2019.\n11\n--- Page 12 ---\n[28] V . Blukis, Y . Terme, E. Niklasson, R. A. Knepper, and Y . Artzi. Learning to map natural\nlanguage instructions to physical quadcopter control using simulated flight. In Proceedings of\nthe Conference on Robot Learning (CoRL) , 2019.\n[29] J. Devlin. BERT: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805 , 2018.\n[30] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthesis. Communications of the ACM ,\n65(1):99–106, 2021.\n[31] A. Yu, V . Ye, M. Tancik, and A. Kanazawa. Pixelnerf: Neural radiance fields from one or\nfew images. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4578–4587, 2021.\n[32] J. Kulhánek, E. Derner, T. Sattler, and R. Babuška. Viewformer: Nerf-free neural rendering\nfrom few images using transformers. In European Conference on Computer Vision , pages\n198–216. Springer, 2022.\n[33] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021.\n[34] H. Ye, D.-A. Huang, Y . Lu, Z. Yu, W. Ping, A. Tao, J. Kautz, S. Han, D. Xu, P. Molchanov,\net al. X-VILA: Cross-modality alignment for large language model. arXiv preprint\narXiv:2405.19335 , 2024.\n[35] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. ZeRO: Memory optimizations toward train-\ning trillion parameter models, 2020. URL https://github.com/microsoft/DeepSpeed .\n[36] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V . Alwala, A. Joulin, and I. Misra. ImageBind:\nOne embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2023.\n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 10684–10695, June 2022.\n[38] K. Zheng, X. Chen, O. Jenkins, and X. E. Wang. VLMbench: A compositional benchmark\nfor vision-and-language manipulation. In Thirty-sixth Conference on Neural Information Pro-\ncessing Systems Datasets and Benchmarks Track , 2022.\n[39] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene\nrepresentations. Arxiv , 2024.\n[40] Y . Zhu, A. Joshi, P. Stone, and Y . Zhu. VIOLA: Imitation learning for vision-based manipula-\ntion with object proposal priors. arXiv preprint arXiv:2210.11339 , 2022.\n[41] X. Li, C. Mata, J. Park, K. Kahatapitiya, Y . S. Jang, J. Shang, K. Ranasinghe, R. Burgert,\nM. Cai, Y . J. Lee, and M. S. Ryoo. LLaRA: Supercharging robot learning data for vision-\nlanguage policy. In International Conference on Learning Representations , 2025.\n[42] W. Yuan, J. Duan, V . Blukis, W. Pumacay, R. Krishna, A. Murali, A. Mousavian, and D. Fox.\nRoboPoint: A vision-language model for spatial affordance prediction for robotics. arXiv\npreprint arXiv:2406.10721 , 2024.\n12\n--- Page 13 ---\nTask Types Goal States Success Training\nRanges Data\nPickup Object 10, 20, 30, 40 (cm) ±5 cm 623\nReorientObject 0, 45, 135, 180 (°) ±20° 355\nOpen Drawer 25, 50, 75, 100 (%) ±10% 554\nClose Drawer 0, 25, 50, 75 (%) ±10% 671\nOpen Cabinet 25, 50, 75, 100 (%) ±10% 319\nClose Cabinet 0, 25, 50, 75 (%) ±10% 478\nPour Water 25, 50, 75, 100 (%) ±10% 312\nTransfer Water 20, 40, 60, 80 (%) ±10% 259\nTable 3: Overview of the 8 tasks in ARNOLD. Each\ntask features 4 goal states specified by human lan-\nguage, one of which is reserved for novel state eval-\nuation and the other three are seen in the training\ndataset. The task is considered successful when the\nobject state remains in the success range for two sec-\nonds. For Transfer Water, an additional condition of\nonly less than 10% spillage of the original amount of\nwater in the cup is imposed.Tasks Examples of Templates\nPickup Object Raise [value_object] [value_height] above the ground\nReorient Object Reorient [value_object] [value_degree] away from the up axis\nOpen Drawer Open the [value_position] [value_object] [value_percent]\nClose Drawer Close the [value_position] [value_object] [value_percent]\nOpen Cabinet Open the [value_position] [value_object] [value_percent]\nClose Cabinet Close the [value_position] [value_object] [value_percent]\nPour Water Pour [value_percent] water out of [value_object]\nTransfer Water Transfer [value_percent] water to [value_object]\nTable 4: Examples of instruction templates used for\nthe tasks.\nAppendix\nA Tasks\nB Prompts used in model variants\nB.1 OG-VLA Prompt and Response\nPrompt: Task: “the bottle should be twenty centimeters from the ground.\". Where should the\nrobot move next? Show the gripper’s next pose as translation and rotation heatmaps on the input\northographic views. Translation should be represented as red heatmap on all 4 views. Follow the\nprovided instruction to compute correct translation points in the images. Rotation should be repre-\nsented as yellow, blue, and green heatmaps for the front, top, and left views, corresponding to the x,\nz, and y axes respectively.\nResponse: The next gripper pose for the given task ’the bottle should be twenty centimeters from\nthe ground.’ for timestep 1 is shown in the generated images.\nB.2 Text Action model Prompt and Response\nPrompt: Task: “pull the top drawer 50% open\". Where should the robot move next? Format the\nrobot’s gripper action as a relative 3D coordinate, an Euler rotation, and a binary gripper open/close\nstate. All numbers are floats with two decimal places, each in relative coordinates.\nResponse: pos: [0.54, 0.42, 0.62], rot: [-1.57, -0.0, -1.57]\nC Detailed Ablation Results and Analysis\nC.1 Action Prediction Ablations\nWe experimented with two common alternative architectural choices for action generation in VLAs,\nboth of which failed to learn a working policy under similar training and evaluation settings as for\nOG-VLA.\n1) Text Action: is an architecture variant where the LLM also produces gripper’s next pose in the\nform of text akin to prior VLA models [41]. In general, the raw output sequence from the LLM can\nbe of slightly different format and contain additional text (e.g. text like the next robot action is: ), so\nlong as it contains the information above. We apply regex parsing to extract from the text the gripper\nposition ptext, orientation ωtext. The prompt used for this ablation is shown in Appendix B.2.\n13\n--- Page 14 ---\nPredicting actions as text without any visual reasoning on the output end of the model results in an\nimprecise action prediction model. Li et al. [41] have shown this architecture to work when training\nwith large-scale robot datasets. However, we find that with a small robotics dataset, it is hard for\nVLAs to learn precise control via direct text token prediction.\n2) Action Tokens: is an architecture variant where the LLM produces special action tokens added to\nthe LLM vocabulary, such as [trans 0]or[rot0]for translation and rotation modalities, akin to that\nfor the image modality. This architecture is closer to works that perform action tokenization [3].\nWe decode the hidden state vectors for these tokens using additional MLP decoders to predict 3D\ntranslation and rotation vectors.\nThe model still struggles to predict sufficiently precise actions to perform the tasks. This may be due\nto insufficient visual reasoning available for action decoding, as the decoders use the hidden states\ncorresponding to the special token produced by the LLM. We also observed degeneration of LLM’s\nability to produce coherent and grammatical text as the training progressed for this model design.\nThis may have been caused by the training of new tokens and additional decoders failing to preserve\noriginal reasoning capabilities of the model, an issue that might be addressed by co-training with\nthe pretraining datasets. We do not observe degeneration when finetuning the model with original\narchitectural components in OG-VLA without any co-training with other datasets. We leave the\nstudy with co-training for above architectural variants to future works.\nC.2 Image Generation Mode\nPickup Reorient Open Close Open Close Pour Transfer Overall\nModel Object Object Drawer Drawer Cabinet Cabinet Water Water\n(1) No Reconstruction 76.7±6.2 10.0±10.8 3.3±2.4 20.0±0.0 1.7±2.4 10.0±7.1 28.3±11.8 13.3±12.5 20.4±4.7\n-Novel Object 56.7±6.2 10.0±0.0 3.3±2.4 23.3±15.5 0.0±0.0 10.0±8.2 13.3±8.5 6.7±2.4 15.4±5.1\n-Novel Scene 51.7±8.5 6.7±6.2 10.0±4.1 10.0±8.2 0.0±0.0 5.0±4.1 11.7±6.2 6.7±6.2 12.7±3.0\n-Novel State 0.0±0.0 6.7±6.2 6.7±2.4 5.0±0.0 0.0±0.0 0.0±0.0 13.3±8.5 0.0±0.0 4.0±2.1\n(2) Reconstruction 86.7±2.4 15.0±7.1 38.3±2.4 51.7±2.4 0.0±0.0 16.7±2.4 25.0±4.1 16.7±6.2 31.2±2.3\n-Novel Object 85.0±4.1 0.0±0.0 1.7±2.4 55.0±10.8 0.0±0.0 5.0±4.1 18.3±2.4 6.7±6.2 21.5±0.3\n-Novel Scene 73.3±2.4 1.7±2.4 26.7±9.4 36.7±4.7 1.7±2.4 1.7±2.4 16.7±9.4 8.3±2.4 20.8±1.1\n-Novel State 0.0±0.0 13.3±6.2 13.3±2.4 20.0±0.0 0.0±0.0 0.0±0.0 8.3±6.2 13.3±2.4 8.5±1.6\n(3) Faded Reconstruction 93.3±2.4 5.0±4.1 23.3±2.4 36.7±8.5 0.0±0.0 5.0±4.1 30.0±0.0 20.0±10.8 26.7±2.3\n-Novel Object 75.0±4.1 18.3±2.4 0.0±0.0 55.0±4.1 0.0±0.0 8.3±4.7 36.7±11.8 5.0±4.1 24.8±2.3\n-Novel Scene 76.7±6.2 11.7±6.2 28.3±4.7 28.3±2.4 5.0±4.1 8.3±2.4 20.0±8.2 11.7±6.2 23.8±3.1\n-Novel State 0.0±0.0 18.3±10.3 6.7±2.4 10.0±4.1 1.7±2.4 0.0±0.0 1.7±2.4 11.7±6.2 6.2±0.5\nTable 5: Success rates for image generation modes for each task. We show that generating actions with\nreconstruction or with faded reconstructions work better than that without reconstruction.\nWe study three image generation modes for action prediction: (1) Generation without reconstruc-\ntion: an all black image background (2) Generation with reconstruction: an RGB image that is a\nreconstruction of the input image, and (3) Generation with faded reconstruction: a shifted RGB im-\nage between the range [0,128] that is a reconstruction of the input image. For each image mode, the\naction Gaussian distributions are overlaid on these backgrounds. These three choices carry trade-\noffs. The first method is the purest way to represent actions, but it appears challenging for image\ngenerators pre-trained on generating color images to learn to generate uniform black backgrounds.\nThe second method does not require un-learning generation of color images, but burdens the gen-\nerator with the additional reconstruction task, which has the potential to take model capacity away\nfrom action generation. On the flipside, it has the potential for some positive cross-task transfer, and\nbetter scene understanding and visual reasoning. The third method is a middle ground between the\nother two methods, which we include in our study. It eases action decoding by keeping values in the\n(128,255] range exclusively for action annotations. For methods (2) and (3), we apply an additional\nfiltering step to identify the Gaussian and recover a grayscale heatmap.\nWe report success rates across tasks and test splits in Table 5. Generation without reconstruction of\nthe scene performs the worst of all generation modes. This may be due to forcing the I MAGE GEN-\nERATOR to unlearn it’s prior of generating natural images and forcing it to only predict Gaussian\ndistributions. We also observe instability in training this version, as sometimes the I MAGE GENER -\n14\n--- Page 15 ---\nATOR would collapse to produce completely black or noisy images and stop generating actions on\nthem. Generation with reconstruction and that with faded reconstruction did not show consistent\ndifference over all test splits. Generation with reconstruction learns a policy that’s better by 4.4% on\nNovel Pose split and 2.3% Novel State split. Generation with faded reconstruction performs better\non Novel Object and Novel State splits by 3.3% and 3%. Therefore, we conclude that these model\nvariants have similar performance, and generation with faded reconstruction did not work better as\nwe had hypothesized due to its balance between forcing the model to focus less on scene reconstruc-\ntion and more on action prediction. Therefore, we report generation with reconstruction as our final\nmethod.\nC.3 Model Ablations\nPickup Reorient Open Close Open Close Pour Transfer Overall\nModel Object Object Drawer Drawer Cabinet Cabinet Water Water\nOG-VLA 86.7±2.4 15.0±7.1 38.3±2.4 51.7±2.4 0.0±0.0 16.7±2.4 25.0±4.1 16.7±6.2 31.2±2.3\n-Novel Object 85.0±4.1 0.0±0.0 1.7±2.4 55.0±10.8 0.0±0.0 5.0±4.1 18.3±2.4 6.7±6.2 21.5±0.3\n-Novel Scene 73.3±2.4 1.7±2.4 26.7±9.4 36.7±4.7 1.7±2.4 1.7±2.4 16.7±9.4 8.3±2.4 20.8±1.1\n-Novel State 0.0±0.0 13.3±6.2 13.3±2.4 20.0±0.0 0.0±0.0 0.0±0.0 8.3±6.2 13.3±2.4 8.5±1.6\n+Tiled Views 75.0±7.1 6.7±4.7 33.3±2.4 28.3±6.2 1.7±2.4 20.0±4.1 15.0±10.8 18.3±6.2 24.8±0.3\n-Novel Object 58.3±8.5 15.0±7.1 1.7±2.4 30.0±0.0 0.0±0.0 10.0±4.1 40.0±4.1 6.7±6.2 20.2±1.3\n-Novel Scene 60.0±7.1 15.0±10.8 45.0±7.1 21.7±6.2 5.0±4.1 5.0±4.1 26.7±6.2 1.7±2.4 22.5±0.5\n-Novel State 0.0±0.0 10.0±4.1 10.0±7.1 18.3±4.7 1.7±2.4 1.7±2.4 1.7±2.4 16.7±11.8 7.5±2.6\n-LLM 86.7±2.4 5.0±7.1 6.7±4.7 33.3±4.7 0.0±0.0 6.7±4.7 15.0±0.0 6.7±2.4 20.0±1.5\n-Novel Object 68.3±6.2 1.7±2.4 6.7±9.4 40.0±4.1 0.0±0.0 3.3±2.4 10.0±4.1 8.3±2.4 17.3±1.6\n-Novel Scene 71.7±6.2 8.3±6.2 18.3±2.4 21.7±8.5 3.3±2.4 0.0±0.0 15.0±4.1 5.0±4.1 17.9±3.3\n-Novel State 0.0±0.0 0.0±0.0 6.7±2.4 16.7±2.4 0.0±0.0 1.7±2.4 3.3±2.4 10.0±4.1 4.8±0.8\n+Tiled Views -LLM 71.7±10.3 1.7±2.4 13.3±8.5 16.7±4.7 0.0±0.0 8.3±2.4 15.0±8.2 10.0±0.0 17.1±3.4\n-Novel Object 56.7±8.5 8.3±2.4 1.7±2.4 16.7±8.5 0.0±0.0 1.7±2.4 15.0±4.1 6.7±2.4 13.3±1.6\n-Novel Scene 61.7±6.2 5.0±4.1 20.0±4.1 11.7±6.2 0.0±0.0 3.3±4.7 10.0±0.0 10.0±0.0 15.2±1.2\n-Novel State 0.0±0.0 6.7±2.4 10.0±0.0 30.0±4.1 1.7±2.4 0.0±0.0 6.7±6.2 3.3±4.7 7.3±0.8\n-Instruction to IG 71.7±4.7 8.3±2.4 20.0±10.8 40.0±12.2 1.7±2.4 11.7±9.4 5.0±4.1 15.0±4.1 21.7±2.6\n-Novel Object 66.7±6.2 0.0±0.0 1.7±2.4 45.0±4.1 0.0±0.0 8.3±2.4 20.0±4.1 1.7±2.4 17.9±0.8\n-Novel Scene 50.0±8.2 11.7±2.4 25.0±0.0 26.7±2.4 10.0±0.0 11.7±2.4 13.3±4.7 5.0±4.1 19.2±2.1\n-Novel State 0.0±0.0 3.3±4.7 8.3±6.2 13.3±8.5 0.0±0.0 0.0±0.0 1.7±2.4 8.3±2.4 4.4±1.8\nTable 6: Model ablation results for each task. We ablate components of OG-VLA to justify our design\nchoices such as using tiled views, and the contribution of the LLM in the pipeline.\nWe present ablations on OG-VLA’s design choices in Table 6. The first result shows the effect\nof tiling the 4 orthographic views instead of feeding them to the I MAGE GENERATOR in batch of\n4 as also explored in Genima [25]. For tiling, we stack the 4 views as in 2D array, following\nprior work. Tiling did not improve the performance for our model as opposed to results reported\nin the prior works. This may have occurred because the prior work did not have an LLM in the\npipeline, so tiling becomes necessary for modeling interactions between views to generate multi-\nview consistent predictions. However, due to the LLM in OG-VLAconditioning generation in all\nviews, there’s sufficient interaction and reasoning between views through the predicted image tokens\u0000\nti\na, i∈ {1, . . . , 4}\u0001\n.\nThe second experiment ablates the LLM to study its contribution to the overall performance.\nWe remove the LLM from the system, directly passing image and text representations to the\nIMAGE GENERATOR . This result shows a drop in performance, highlighting the importance of the\nLLM in our pipeline.\nNext, we study adding tiling to the previous LLM ablation to ensure that interaction between views,\nwhich was earlier happening through the LLM, can now take place in the I MAGE GENERATOR .\nTiling further leads to a drop in performance, perhaps because of the reduction in the number of\ntokens used to represent each view. Without tiling, each image is represented by 256 patch tokens,\nbut with tiling all 4 images are represented by 256 tokens in total, potentially creating too tight a\nrepresentational bottleneck. This version is also similar to Genima with only an I MAGE GENERATOR\nand tiled camera input views in the pipeline.\n15\n--- Page 16 ---\nFinally, we study the effect of bypassing the prompt and instruction into the I MAGE GENERATOR ,\nshown in the last section of Table 6. The performance drop suggests that some instruction informa-\ntion may have been lost in image tokens output from the LLM, therefore it is beneficial to provide\nthat information separately to the I MAGE GENERATOR for more accurate action prediction.\nTask1: lift the bottle thirty centimeters from the ground\nTask2: add forty percent of the liquid to the cup\nTask4: shut the  cabinet half closedTask3: pull the top dresser one hundred percent open\nT=1T=2T=1T=2T=1T=2T=1T=2\nTask: close the top dresser completely closedTask: set the angle of the bottle forty-ﬁve degrees from the upward axisTask: get seventy ﬁve percent water out of the glassTask:  pull the  cabinet  a quarter open T=1T=2T=1T=2T=1T=2T=1T=2\nFigure 5: Example gripper position and rotation outputs from OG-VLA for eight different tasks. The rows\nare the different views: front, top, left, and right. For each task, the two columns are two timesteps required to\nsolve the task. The red Gaussian is the predicted position. The yellow, blue, and green Gaussians are predicted\nrotation angles along x, z, and y-axis respectively. The blue dot is our model’s output gripper position, back-\nprojected to each view. The dots on rotation Gaussians are showing the extracted pixel for computing the\nrotation angle in reference to the horizontal right axis in each view.\nC.4 Qualitative results\nWe present qualitative results in Figure 5 of the output of OG-VLA, showing the generated actions,\ninferred gripper positions and rotations. We show predictions for the tasks: Pickup Object, Transfer\nWater, Open Drawer, and Close Cabinet. We add remaining task prediction examples in the Ap-\npendix. These visualizations show that OG-VLA can do complex translation and rotation tasks, that\nrequire both object and free space reasoning. We observe that the translation predictions are quite\nconsistent for most cases, except for Task3 at T=1. The prediction in the Left view is incorrect,\nhowever, due to correct predictions in other views, the most likely 3D point (blue dot) is still cor-\nrectly extracted at the handle of the drawer using the optimization in Equation 2. In Task4, we show\n16\n--- Page 17 ---\na failure case where the task is to half-close the cabinet, however the predicted point in T=2 is less\nthan the half-way point.\nC.5 Ablation with ground truth translation and rotation\nPickup Reorient Open Close Open Close Pour Transfer Overall\nObject Object Drawer Drawer Cabinet Cabinet Water Water\nTraining set evaluation 76.7±4.7 11.7±2.4 35.0±0.0 58.3±4.7 0.0±0.0 10.0±4.1 18.3±6.2 20.0±4.1 28.8±0.5\n+ Ground Truth Translation 91.7 ±2.4 21.7±14.3 76.7±6.2 81.7±2.4 15.0±7.1 21.7±4.7 28.3±2.4 33.3±2.4 46.2±0.9\n+ Ground Truth Rotation 96.7±4.7 60.0±4.1 43.3±6.2 75.0±8.2 5.0±0.0 40.0±7.1 40.0±8.2 11.7±2.4 46.5±3.8\nTable 7: Ablated evaluation of the model’s translation and rotation prediction capabilities on a sampled\ntraining set of 20 episodes, similar to the test sets. In each evaluation, we individually ablate either the transla-\ntion or rotation predictions to assess the model’s prediction capabilities for each.\nWe present another set of ablations in Table 7 for studying our model’s translation and rotation\nprediction abilities in an ablated setting on a sampled training set. First, we note that the overall\nperformance of our model on Training and Novel Pose split is quite similar, which indicates that the\nmodel has not overfit and is generalizing well w.r.t the training performance. We observe that there is\nhuge scope for improvement, both in predicting more precise translations and rotations from the last\ntwo rows of the table. We believe that this gap can be closed with training techniques like co-training\nwith other datasets for better reasoning [42] and leveraging existing large robotics datasets like that\nin other VLA models [3]. We believe leveraging these datasets with a 3D-aware VLA framework\ncan unlock better learning signals from these large-scale datasets and significantly improve results\nfor our proposed VLA architecture.\nD Real world details\nWe calibrate the camera using MoveIt hand-eye calibration using an ArUco board1. We record\nthe trajectory at 30 Hz frame rate. For training OG-VLA, we only use the first 1/5-th of the tra-\njectory before each annotated keyframe action in the trajectory, for augmenting states prior to each\nkeyframe.\nFigure 6 shows the different training and test scenes. We provide evaluation videos of successful\nand failed trajectories for these test scenes on our website.\n(a)Training demonstration objects\n (b)Novel objects used at test time\n (c)Novel scene distractors used at\ntest time\nFigure 6: Real-world setup: objects used at training and test-time. For novel scene, we additional vary the\nlighting and background by switching on/off external lighting source, and removing curtains.\n1https://github.com/moveit/moveit_calibration\n17",
  "text_length": 64317
}