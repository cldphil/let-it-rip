{
  "id": "http://arxiv.org/abs/2506.02058v1",
  "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
  "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.",
  "authors": [
    "Xiang Li",
    "Jiayi Xin",
    "Qi Long",
    "Weijie J. Su"
  ],
  "published": "2025-06-01T15:32:44Z",
  "updated": "2025-06-01T15:32:44Z",
  "categories": [
    "cs.CL",
    "cs.IR",
    "cs.LG",
    "stat.AP",
    "stat.ME"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.02058v1",
  "full_text": "--- Page 1 ---\narXiv:2506.02058v1  [cs.CL]  1 Jun 2025Evaluating the Unseen Capabilities: How Many Theorems Do\nLLMs Know?\nXiang Li∗Jiayi Xin†Qi Long‡Weijie J. Su§\nJune 1, 2025\nAbstract\nAccurate evaluation of large language models (LLMs) is crucial for understanding their\ncapabilities and guiding their development. However, current evaluations often inconsistently\nreflect the actual capacities of these models. In this paper, we demonstrate that one of many\ncontributing factors to this evaluation crisis is the oversight of unseen knowledge—information\nencoded by LLMs but not directly observed or not yet observed during evaluations. We introduce\nKnowSum , a statistical framework designed to provide a more comprehensive assessment by\nquantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the\nunobserved portion by extrapolating from the appearance frequencies of observed knowledge\ninstances. We demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval effectiveness, and\nmeasuring output diversity. Our experiments reveal that a substantial volume of knowledge\nis omitted when relying solely on observed LLM performance. Importantly, KnowSum yields\nsignificantly different comparative rankings for several common LLMs based on their internal\nknowledge.\n1 Introduction\nLarge language models (LLMs) have emerged as a transformative technology in artificial intelligence,\ndemonstrating remarkable capabilities across diverse domains [ 4,1]. Originally developed for\nnatural language tasks such as translation, summarization, and dialogue generation, LLMs now\nexhibit increasingly sophisticated forms of reasoning and problem-solving [ 6,60,7]. They can\ncontribute to clinical and biomedical decision-making [ 53], tackle mathematical problems with\nverifier assistance [ 14], advance research in chemistry and physics [ 55], and synthesize knowledge\nacross multiple scientific disciplines [ 7]. As the scope of their application continues to expand, LLMs\nare poised to become integral tools in scientific research, significantly influencing how knowledge is\ngenerated and utilized.\nThis remarkable progress has been facilitated, in large part, by LLM evaluation [ 18,13,10]. LLM\nevaluation refers to the process of assessing and understanding model effectiveness through bench-\nmark datasets and standardized protocols, which inform practitioners and guide model development.\n∗University of Pennsylvania; Email: lx10077@upenn.edu .\n†University of Pennsylvania; Email: jiayixin@seas.upenn.edu .\n‡University of Pennsylvania; Email: qlong@upenn.edu .\n§University of Pennsylvania; Email: suw@wharton.upenn.edu .\n1\n--- Page 2 ---\nFrom Observation\nOur Method\n1432868 702 753\n1706119410641189\n444\n1162>\n>>\n>>\n>>\n>Figure 1: Estimating the number of unseen theorems changes LLM rankings based on\nthe observed theorems alone. The evaluated models are:\n LLaMA-V3-70B-instruct [24],\nChatGPT-3.5-turbo-chat [38],\nChatGPT-4o-chat [28],\nMistral-7B-instruct-V0.1 [29],\nand\nQwen2.5-7B-instruct [62].\nWhile evaluation has been fundamental to machine learning and AI progress since its early days, it\nhas become increasingly critical nowadays as AI research has evolved into a predominantly empirical\nscience, with advancement driven largely by experimentation and iterative refinement [ 54,51]. Conse-\nquently, comprehensive evaluation of LLM capabilities is essential for identifying promising research\ndirections and optimizing resource allocation, particularly given the substantial computational and\nenergy requirements of AI development [46, 5].\nNevertheless, the field currently confronts a crisis of evaluation [ 32,52], characterized by a growing\ndisconnect between benchmark performance and genuine model generalization capacity. Multiple\nfactors contribute to this crisis, including benchmark contamination [ 47], overfitting through repeated\nleaderboard submissions [ 52], and narrow test-time optimization strategies [ 48,15,63], all of which\nartificially inflate performance metrics. However, a fundamental statistical limitation underlying this\nevaluation crisis remains largely unaddressed. Current evaluation methodologies typically consider\nonly observable outputs from benchmark datasets. Due to the stochastic nature of generation, LLMs\nmay possess substantial knowledge that remains unexpressed under constrained query budgets [ 61].\nFor instance, when attempting to assess an LLM’s knowledge of mathematical theorems by directly\nprompting it to enumerate such conditions, the model would predominantly list common theorems,\nwhile rare theorems—despite being within the model’s knowledge repertoire—might not surface even\nafter hundreds of queries (see Figure 2).1\nThe aim of this paper is to quantify the extent of latent knowledge possessed by LLMs beyond\nobservable outputs and to develop robust statistical methodologies for this purpose. While estimating\nthe unseen may initially appear intractable, we demonstrate that by conceptualizing LLM outputs\nas a sampling process—analogous to drawing samples from Poisson or multinomial distributions—we\ncan apply statistically principled techniques to infer unexpressed knowledge from observed knowledge\ncounts. This challenge aligns with classical statistical problems such as estimating unseen species in\necology [22] and inferring the number of unrecorded words in linguistics [19].\nTo address this challenge, we introduce KnowSum , a general-purpose statistical framework\nspecifically designed to estimate unseen knowledge in LLMs. Rather than relying solely on observed\noutputs, KnowSum quantifies the internal knowledge of LLMs by extrapolating observed frequency\ndistributions and employing the smoothed Good–Turing estimator [ 41]. A particularly attractive\nfeature of KnowSum is that it incurs no additional computational cost. Moreover, the framework\nis statistically grounded, conceptually simple, and applicable to any evaluation task that can be\nformulated as knowledge counting. We demonstrate KnowSum ’s versatility through three distinct\napplications: (1) estimating LLM knowledge of human diseases and mathematical theorems , (2)\n1Direct queries such as “how many theorems do you know?” is equally unreliable, as such responses are often\ninconsistent or exaggerated [31].\n2\n--- Page 3 ---\nHow Many Theorems\nDoes an LLM Know?\nGive me a list of theorems\nUser\nGive me a list of theorems\nUserThm3, Thm1 , Thm5 , Thm4\n...Prompt 100 times\nThm1\nThm3\nThm5\n...Common \nThm2\nThm4\nThm6\n...Rare\nTheorem\nThm1  From Observation\nLLM knows  5 Theorems\nOur Method\nEstimate the unseen\nLLM knows 6 Theorems\nThm3 Thm5 Thm2 Thm4 Thm6CountsThm1, Thm3 , Thm5 , Thm2(a) (b) (c)Figure 2: (a)Examples of common and rare theorems from an LLM. (b)Obtain theorem names\nby repeatedly prompting the LLM. (c)Comparison between observed results and our proposed\napproach.\nquantifying retrieval coverage in biomedical document retrieval and question answering, and (3)\nmeasuring semantic diversity in open-ended generation tasks. Our findings reveal two key insights:\n(i) LLMs typically express only 20-50% of their estimated internal knowledge in our experiments,\nand (ii) accounting for unseen knowledge can markedly alter model rankings (see Figure 1). By\nshifting focus from “what the model outputs” to “what the model could potentially output,” our\nwork provides a novel perspective on model evaluation—one that more accurately reflects intrinsic\ncapacity rather than merely observed performance.\n2 Related Work\nLLM evaluation. A common approach to evaluating LLM capabilities is to pose a fixed set of\nquestions and assess the model’s responses based on correctness, much like a human exam [ 10,13].\nNumerous benchmark datasets have been proposed for this purpose, spanning domains such as\nmathematics [ 14], healthcare [ 53], science [ 26,17], and reasoning [ 44]. However, these evaluations\nare limited to observable outputs on predefined questions and fail to capture knowledge that the\nmodel may possess but does not express. Our work complements this by estimating what a model\ncould say but has not yet—which broadens the scope of standard evaluations.\nIn information retrieval, evaluation typically relies on ranking-based metrics (e.g., precision, recall,\nMRR [16]) or generation-based metrics (e.g., BLEU [ 42], ROUGE [ 35]). These approaches rely on\nquery sets collected in advance, which may not fully capture the range of inputs a model can handle—\nthereby overlooking its potential to retrieve relevant information for unseen or underrepresented\nqueries. In contrast, our method estimates how much additional information a model could retrieve if\nmore queries were drawn i.i.d. from the same distribution as the existing dataset, offering a broader\nview of retrieval coverage. For diversity measurement, traditional metrics often rely on heuristics\nlike perplexity [ 27] or type-token ratios [ 57]. Recent work [ 59] applies hypothesis testing to assess\nwhether LLMs exhibit human-like creativity. In contrast, we estimate how many semantically distinct\noutputs a model could generate for open-ended prompts, providing a scalable and interpretable\nmeasure of diversity beyond what is directly observed.\nEstimating the unseen. Estimating the number of unseen from the observed is a fundamental\nproblem arising across many domains [19, 11]. Despite differences in context, these problems often\nshare a common structure: inferring properties of an underlying distribution from limited samples,\nwhere rare observations provide important information about what remains unobserved. A classic and\n3\n--- Page 4 ---\nwidely used approach is the Good–Turing estimator [ 22], originally developed to estimate the number\nof unseen species, and later extended in contexts such as vocabulary diversity in computational\nlinguistics [ 19,21] and species richness in ecology [ 11,12]. In the context of LLMs, it has been used\nto quantify unseen facts in training data [ 30] and to estimate memorized but unobserved facts in\nmodel outputs [ 37]. Although the Good–Turing estimator is theoretically grounded [ 40,39,20] and\nprovides unbiased estimates, it suffers from extremely high variance. To address this, smoothed\nvariants have been proposed [ 19,41] that better balance bias and variance, resulting in more stable\nand more accurate estimates. Therefore, we adopt such a smoothed estimator as a component of our\npipeline.\nMemorization, knowledge retrieval, and knowledge inference. Pre-trained language models\nare known to store a vast amount of knowledge [ 43,6,58], and extensive research has investigated\nthis capability. In many contexts, this knowledge resembles memorization—specifically, the ability\nto reproduce subtexts from the training dataset. Several works have focused on quantifying such\nmemorization [ 8,65,37,50], typically by crafting prompts that elicit memorized subtexts. Beyond\nmemorization, several studies have examined how and when LLMs acquire factual knowledge during\npretraining [ 34,2,9,45], typically using external datasets as ground truth to score correctness. In\ncontrast, we do not rely on predefined answers for grading or comparison. Instead, our focus is on\nestimating internal knowledge, that is, information encoded in the model that has not surfaced in\nsampled outputs. Additionally, there is emerging work that infers properties of an LLM’s internal\nknowledge. For example, [ 23] predicts how knowledgeable a model is about a specific entity based\non its internal activations before any output is produced. [ 61] estimates the probability of a binary\noutput property using importance sampling, particularly when the event is too rare to be observed\ndirectly. While these methods similarly move beyond surface-level outputs, we focus on a different\ntask—estimating the amount of internal knowledge.\n3 Method\n3.1KnowSum : Unseen Knowledge Estimation Pipeline\nPipeline description. Algorithm 1 outlines our method which follows a five-step pipeline: gen-\neration, verification, clustering, prevalence estimation, and unseen estimation (see Figure 3). It\nbegins by querying the model ntimes to generate candidate knowledge items. A verifier is then\nused to filter out invalid or hallucinated responses using external sources such as search engines or\ndomain-specific databases. From each valid response, we extract the core knowledge using a decoder\nC, which typically removes stylistic variation to retain the essential content, for example, the name\nof a theorem. When a single response contains multiple answers, this step also splits them into\nindividual items. The resulting knowledge items are then clustered based on semantic similarity,\nwith each cluster representing a distinct piece of knowledge. For example, “Theorem of Pythagoras”\nand “Pythagorean theorem” would be treated as equivalent. We then construct a histogram {ns}s≥1\nof item frequencies and apply the smoothed Good–Turing (SGT) estimator to predict the number of\nunseen items. Denoting by bNunseen (t)the number of items that do not appear in the observed n\nqueries but would appear in the subsequent t·nqueries, the final estimate of total knowledge is\ngiven by bNtot=Nseen+bNunseen (t), where Nseen=P\ns≥1nsis the number of observed knowledge.\n4\n--- Page 5 ---\nThm4,\nThm5,Thm6\nThm3\nThm1,Thm7Prompt Thm1\n...Count\nExtrapolation (  )Count\nInput 1: Generation 2: Verification 3: Clustering 4: Est. Prevalence 5: Est. UnseenThm1\nThm1\nThm1\nThm1\nThm1Prompt\nPrompt\nPrompt\nPrompt\nPromptThm1\nThm1\nThm1\nThm1\nThm1\nThm1Figure 3: Schematic illustration of the unseen knowledge estimation framework.\nAlgorithm 1 Knowledge Summation ( KnowSum )\nInput:Verifier A, knowledge decoder C, number of queries n, extrapolation factor t.\n1:Generation: Independently query the target LLM ntimes to produce responses x1, . . . , x n.\n2:Verification: Use the verifier Ato filter out invalid or hallucinated responses.\n3:Clustering: Apply the decoder Cto extract the core knowledge ci=C(xi)from each valid\nresponse, and group {ci}n\ni=1intomsemantically distinct clusters {kj}m\nj=1.\n4:Prevalence Estimation: Construct the frequency histogram {ns}s≥1, where nsdenotes\nthe number of clusters that appear exactly stimes. Estimate the observed knowledge by\nNseen=P\ns≥1ns.\n5:Unseen Estimation: For given t, use the smoothed Good–Turing estimator bNunseen (t)to\ncompute the number of unseen knowledge items in tnadditional queries.\nOutput: Estimated total knowledge: bNtot:=Nseen+bNunseen (t).\nModularity. KnowSum is highly modular: once verification and clustering are specified for a given\ntask, the rest of the pipeline applies out of the box. These two steps are the most task-dependent,\nas they reflect how knowledge is defined and validated in each domain. With them in place, the\nremaining steps remain unchanged, making the framework broadly applicable to any domain with\ndiscrete, countable knowledge. We detail the implementation choices in Section 4.\n3.2 Smoothed Good–Turing Estimator\nTo quantify the amount of knowledge that remains unexpressed by an LLM, we adopt the smoothed\nGood–Turing estimator bNunseen (t)[41]. This estimator predicts the number of new knowledge items\nthat would appear in the next t·nqueries, based on the empirical prevalence counts {ns}s≥1collected\nfrom the first nqueries, where nsdenotes the number of items that appear exactly stimes. The\nSGT estimator combines the first kof these counts into a weighted sum:\nbNunseen (t) :=kX\ns=1hs·ns,where hs=−(−t)sP\u0012\nBin\u0012\nk,1\nt+ 1\u0013\n≥s\u0013\n, (1)\nwith kdenoting a user-specified truncation parameter, and Bin(k,1/(t+ 1))representing a binomial\ndistribution with ktrials and success probability 1/(t+ 1). Initially known as the Efron–Thisted\nestimator [ 19], this estimator is provably near-optimal in terms of worst-case normalized mean\nsquared error, up to subpolynomial factors [41].\nThe effectiveness of this estimator lies in a fundamental statistical insight: if each knowledge\nitem emerges according to its own Poisson process, then rare items—those observed only once or\ntwice—serve as indicators of a much larger set of unseen items with similarly low occurrence rates.\nAlthough the truly unseen items never appear in the data, their presence is inferred through these\n5\n--- Page 6 ---\n“near misses”—items that almost went unobserved. These rare prevalence counts act as statistical\nfingerprints of the hidden mass, enabling reasonable extrapolation of how many new items would\nsurface with continued querying.\nWhile the most direct signals come from singletons n1and doubletons n2, incorporating the\nfirstkprevalence counts {ns}k\ns=1allows the estimator to more robustly capture the contribution of\nlow-frequency items and thus improves stability when extrapolating further into the unseen. The\nfollowing result shows how kcontrols the growth of the estimate bNunseen (t)relative to the number of\npreviously seen items Nseen. For practical use, we select the optimal value of kfrom a candidate set\nusing a cross-validation procedure (see Section 5.1 for details).\nTheorem 1. Ift≥k−1, then the following holds :\nbNunseen (t)≤ekt\nt+1·Nseen.\nThis inequality also implies that the total predicted knowledge is at most a multiplicative factor\nof the observed knowledge. Motivated by this relationship, we introduce the seen knowledge ratio\n(SKR), which normalizes the estimate to yield a dimensionless quantity that reflects the proportion\nof exposed knowledge:\nDefinition 1 (Seen Knowledge Ratio) .The seen knowledge ratio for a fixed tis defined as\nSKR( t) =Nseen\nNseen+bNunseen (t).\nWhen both tandnare sufficiently large (e.g., t= 102,n= 105in our experiments), the\ndenominator Nseen+bNunseen (t)serves as an estimate of the total knowledge encoded in the model.\nThe SKR thus offers an interpretable summary of knowledge exposure: a value close to 1 indicates\nthat most of the model’s accessible knowledge has already surfaced, while a lower value suggests that\nmuch remains hidden. Moreover, since bNunseen (t)is non-decreasing in t, SKR naturally decreases as\nwe consider further extrapolation into the model’s unexpressed knowledge (see Figure 8).\n4 Applications\nIn this section, we apply our KnowSum pipeline to three applications and evaluate the knowledge\ncapabilities of several mainstream LLMs. At a high level, we focus on instruction-tuned or commercial\nmodels, as they are generally better at following prompts and adhering to task guidelines. Specifically,\nwe evaluate nine popular LLMs, listed in the first column of Table 1. Due to space limits, additional\nexperimental details are provided in Appendix B.\n4.1 Application 1: Knowledge Estimation\nSetup. Our first and most basic application is to estimate the total amount of knowledge an LLM\npossesses. We focus on two domains: mathematical theorems and human diseases. The pipeline\nbegins by querying the target LLM Nquerytimes with a fixed prompt, each time requesting Nans\ninstances of domain-specific knowledge. To validate the outputs, we use external databases and\ncluster the responses based on their external identifiers. Specifically, for mathematical theorems,\nwe consult Wikipedia, MathSciNet, and ProofWiki, treating two theorems as identical if they are\n6\n--- Page 7 ---\nverified by the same webpage. For human diseases, we match each generated name against the\nHuman Disease Ontology [ 49] and merge those that map to the same Disease Ontology Identifier\n(DOID). We set the sampling temperature to 1to encourage diverse responses. To further promote\ndiversity, we use (Nquery, Nans) = (30 ,000,20)for theorems and (3,000,50)for diseases. Throughout\nall experiments, we fix the extrapolation factor at t= 100with the number of total observations\nn=Nquery·Nans. We selected the best truncation level k∈ {6,8,10}by cross-validation. An\nsensitivity study on the effects of the prompt, Nquery, and Nansis presented in Section 5.\nTable 1: Results of knowledge estimation (Application 1) across different LLMs.\nModelTheorem only All math concepts Anatomical disease Human diseases\nNseenbNtot SKR NseenbNtot SKR NseenbNtot SKR NseenbNtot SKR\n①ChatGPT-4o-chat 702 1189 0.59 974 2410 0.40 277 732 0.38 589 1096 0.54\n②ChatGPT-3.5-turbo-chat 868 1064 0.82 1266 1703 0.74 268 278 0.96 523 706 0.74\n③LLaMA-V3-70B-instruct 1432 1706 0.842289 2645 0.87875 3372 0.261777 7564 0.23\n④LLaMA-V3-3B-instruct 1035 1331 0.78 1717 2640 0.65 780 1375 0.57 1374 3002 0.46\n⑤Mistral-7B-instruct-V0.1 753 1194 0.63 1313 2481 0.53 489 1723 0.28 859 1276 0.67\n⑥Qwen2.5-7B-instruct 444 1162 0.38 663 1385 0.48 426 521 0.82 763 763 1.00\n⑦Claude-3.7-Sonnet 120 201 0.60 147 293 0.50 115 462 0.25 213 686 0.31\n⑧DeepSeek-V3 148 241 0.61 162 203 0.80 86 334 0.26 193 752 0.26\n⑨Gemini-1.5-flash 100 515 0.19122 478 0.26 139 143 0.97 298 306 0.97\nTwo verification criteria. Our analysis estimates an LLM’s total knowledge across two domains:\nhuman diseases and mathematical theorems. Table 1 presents the counting results under two distinct\nvalidation criteria for each domain. For theorem counting , the first two columns of Table 1\ndetail our findings. We apply two validation criteria. The first, more restrictive setting, considers a\nresponse valid only if its name explicitly includes “theorem”—even if the item is on Wikipedia, we\nmark it invalid without that specific word. The second, broader criterion accepts a response if it\ncontains any of twelve related terms: “theorem,” “lemma,” “law,” “principle,” “formula,” “criterion,”\n“identity,” “conjecture,” “rule,” “equation,” “postulate,” or “corollary.” The term “theorem” itself\naccounts for roughly 10% (2,558 entries) of the 24,762 identified mathematical concepts. The last\ntwo columns of Table 1 show results for human diseases . Similarly, we validate LLM-generated\ndisease names using two criteria. Our initial, narrower criterion accepts responses only if they fall\ninto the “anatomical disease” category (diseases affecting specific body parts), including recognized\nsynonyms. Anatomical diseases make up about 51% (6,036 entries) of the 11,872 human diseases in\nthe Disease Ontology. Our second, broader criterion considers a response valid if it matches any\nhuman disease within the Disease Ontology, encompassing all its subcategories. The distribution of\nthese subcategories is provided in Figure 10 in the appendix.\nA gap between observed and total knowledge. We observe that most SKR values are\nstrictly less than 1, indicating that all evaluated LLMs possess unrevealed internal knowledge. For\ntheorem counting, LLaMA-V3-70B-instruct achieves the highest estimated total knowledge under\nboth criteria. In the disease domain, it also reports the largest bNtotand the lowest SKR, suggesting\nbroader coverage in biomedical knowledge. Gemini-1.5-flash also exhibits a low SKR, likely due\nto its limited number of observed outputs, which implies a greater portion of its knowledge remains\nhidden.\n7\n--- Page 8 ---\n1 2 3 4 5 6 7 8···0100200300400ChatGPT-4o-chat\nChatGPT-3.5-turbo-chat\nLLaMa-V3-70B-instructLLaMa-V3-3B-instruct\nMistral-7B-instruct-V0.1\nQwen2.5-7B-instructClaude-3.7-Sonnet\nDeepSeek-V3\nGemini-1.5-flash\n1 2 3 4 5 6 7 8···0200400ChatGPT-4o-chat\nChatGPT-3.5-turbo-chat\nLLaMa-V3-70B-instructLLaMa-V3-3B-instruct\nMistral-7B-instruct-V0.1\nQwen2.5-7B-instructClaude-3.7-Sonnet\nDeepSeek-V3\nGemini-1.5-flashFigure 4: Frequency histogram of theorems (top) and human diseases (bottom) found in LLM\nresponses. The x-axis shows how many times a knowledge appears, and the y-axis shows how many\ndistinct knowledges occur with that frequency. The shaded region shows additional counts due to\nrelaxed validation criteria.\nImpact of unseen knowledge on model ranking. A notable finding is that accounting for\nunseen knowledge can meaningfully affect model comparison. Take the “theorem only” setting as\nan example: based on the observed count Nseen,ChatGPT-3.5-turbo-chat appears to outperform\nChatGPT-4o-chat . However, when including unseen knowledge, ChatGPT-4o-chat is estimated to\nknow more. This aligns with expectations, as the latter is trained on a larger corpus [ 1]. A similar\nshift occurs under the broader validation criterion, where both NseenandbNtotincrease, but rankings\nstill change. Moreover, we observe the same reversal in the disease domain: although DeepSeek-V3\nandGemini-1.5-flash appear comparable in observed counts, accounting for unseen knowledge\nshows that DeepSeek-V3 has greater estimated coverage.\nWhy unseen knowledge alters model ranking. A natural question is why accounting for\nunseen knowledge can change the ranking of LLMs. One might assume that a higher observed\ncount Nseenshould imply a higher estimated total knowledge bNtot, but this is misleading— bNtotis\nnot determined by Nseenalone. Formally, bNtot=P\ns≥1ns+bNunseen (t), where nsis the number of\nknowledge items seen exactly stimes. While Nseen=P\ns≥1nsonly counts distinct items, the SGT\nestimator depends on the prevalence distribution {ns}s≥1, with coefficients that alternate in sign.\nThus, even with the same Nseen, differences in prevalence (that is, how frequently items appear) can\nlead to different unseen estimates. Figure 4 illustrates this: all models show a decay in ns, but their\nscales and shapes differ. Notably, for theorems, ChatGPT-3.5-turbo-chat has uniformly higher ns\nvalues than ChatGPT-4o-chat , yet receives a lower total knowledge estimate. This is because the\nsharper decay in the latter’s prevalence suggests that it generates more rare or distinct items–leading\nto a higher unseen estimate and thus greater overall knowledge, despite having fewer observed items.\nHeavy-tailed prevalence. The prevalence {ns}s≥1has a heavy-tailed distribution, that is, we\nobserve nonzero counts ns≥1even for large values of s. This reflects LLMs’ tendency to repeatedly\ngenerate a small number of highly common items, a pattern consistently observed across domains.\n8\n--- Page 9 ---\nPythagorean Theorem Fermat Last Theorem G¨ odel Incompleteness Theorems0.000.010.020.030.040.050.060.07ChatGPT-4o-chat\nChatGPT-3.5-turbo-chat\nLLaMa-V3-70B-instructLLaMa-V3-3B-instruct\nMistral-7B-instruct-V0.1\nQwen2.5-7B-instructClaude-3.7-Sonnet\nDeepSeek-V3\nGemini-1.5-flash\nAlzheimer’s disease Asthma Parkinson’s disease0.0000.0050.0100.0150.0200.0250.0300.035ChatGPT-4o-chat\nChatGPT-3.5-turbo-chat\nLLaMa-V3-70B-instructLLaMa-V3-3B-instruct\nMistral-7B-instruct-V0.1\nQwen2.5-7B-instructClaude-3.7-Sonnet\nDeepSeek-V3\nGemini-1.5-flashFigure 5: Top three most frequently generated theorems (left) and human diseases (right). The black\ndotted line indicates the average relative frequency, defined by 1/Nans, where Nansis the number of\nknowledge items requested per query (20 for theorems and 50 for diseases).\nFor theorem counting, as illustrated in Figure 5 (left panel), LLMs frequently generate well-known\nexamples like the Pythagorean Theorem, Fermat’s Last Theorem, and Gödel’s Incompleteness\nTheorems. Despite all models being queried 30,000 times (up to 20 responses per query), the number\nof valid theorems varies, affecting normalization. Nonetheless, the relative frequencies of these top\nresponses—often exceeding 5%—highlight their dominance and account for the heavy tail. Similarly,\nfor human diseases, the figure on the right demonstrates LLMs repeatedly mention a small set of\ncommon human diseases (for example, Alzheimer’s disease, Asthma, and Parkinson’s disease). This\nmotivates truncating the prevalence sequence in the SGT estimator. For instance, using k= 6means\nthat only {ns}6\ns=1contribute to estimating bNseen(t), excluding high-frequency items. This truncation\nhelps improve stability by reducing sensitivity to overrepresented responses.\n4.2 Application 2: Information Retrieval\nSetup. Our second application evaluates the information retrieval (IR) capabilities of LLMs using\ntheBioASQ-QA Task 12b Test dataset [ 33], following the evaluation framework of [ 64], focusing on two\nsubtasks. The first subtask is document retrieval , where the LLM is prompted to generate Boolean\nsearch queries to retrieve relevant documents from the PubMed database. These queries consist of\nmultiple Medical Subject Headings (MeSH) keywords, combined using logical operators (AND, OR,\nNOT), and are submitted to a search engine to return candidate documents. The second subtask is\nquestion answering , in which the LLM answers biomedical research questions—curated by domain\nexperts—based on the retrieved documents. In both subtasks, each question is associated with a set\nof ground-truth documents, and each document is annotated with a list of MeSH keywords. Instead\nof evaluating at the document level, we focus on MeSH keywords as retrieval-relevant knowledge\nunits. Each keyword reflects a specific biomedical concept that the model successfully identifies\nthrough retrieval. This offers a finer-grained view of what content the model can retrieve, beyond\nsimply matching entire documents. Indeed, traditional IR metrics—such as F1 score and ROUGE\nscore [35]—assess retrieval and answer quality based on document relevance. In contrast, our pipeline\nestimates how many additional relevant MeSH keywords an LLM could potentially retrieve—beyond\n9\n--- Page 10 ---\nthose observed—if more questions were posed from a similar distribution and evaluated under the\nsame validation criteria.\nIn our experiments, each LLM is evaluated on Nquery = 340biomedical questions using custom\nfew-shot prompts (see Appendix B.2 for templates), with Nans= 1answer collected per question.\nThe verification procedures differ across subtasks. In the document retrieval subtask, if the LLM\nretrieves a document that appears in the ground-truth set, all MeSH keywords associated with\nthat document are counted as valid observed knowledge. In the question answering subtask, if the\nLLM’s response is deemed correct, we include all MeSH keywords from the documents linked to that\nquestion. All MeSH keywords are normalized and clustered by the top-level concept of the MeSH\nhierarchy for consistent aggregation.\nM1M2M3M4M5M6M7M8M90%5%10%15%20%\n(Subtask 1) Doc. F1\n(Subtask 2) ROUGE\nFigure 6: Performance on selected traditional IR metrics. M1denotes Model ①(ChatGPT-4o-chat ),\nand similarly for the others (see Table 1 for the full list). The highest values are in red.\nResults. Results for Application 2 are summarized in the left two columns of Table 2, with\nselected traditional IR metrics shown in Figure 6 and full results provided in Appendix B.10. In\nSubtask 1, DeepSeek-V3 achieves the highest observed count ( Nseen= 2260), consistent with its\ntop document-level F1 score of 16.76%. However, when incorporating estimated unseen knowledge,\nChatGPT-3.5-turbo-chat surpasses all others with a total of bNtot= 10367, significantly ahead\nofDeepSeek-V3 ’s 7750—demonstrating how our pipeline can reveal latent retrieval capacity not\nreflected in standard metrics. In Subtask 2, ChatGPT-4o-chat leads in both observed and total\nMeSH terms, although its ROUGE score is slightly lower than the highest performer. Overall, models\nexhibit SKR values around 25% for Subtask 1 and 11% for Subtask 2, both of which are lower than\nin Application 1.\nTable 2: Results of information retrieval (Application 2, left two columns) and diversity measures\n(Application 3, right two columns) across different LLMs.\nModelDocument Retrieval Question Answering LLM Applications Dream Jobs\nNseenbNtot SKR NseenbNtot SKR NseenbNtot SKR NseenbNtot SKR\n①ChatGPT-4o-chat 2015 9676 0.21 2351 19965 0.12 165 714 0.23 409 1680 0.24\n②ChatGPT-3.5-turbo-chat 219010367 0.211850 15733 0.12 322 1339 0.24 131 560 0.23\n③LLaMA-V3-70B-instruct 1990 8488 0.23 1928 14270 0.14 437 1918 0.23 344 1487 0.23\n④LLaMA-V3-3B-instruct 79 396 0.201653 14199 0.12 428 1926 0.22 770 3386 0.23\n⑤Mistral-7B-instruct-v0.1 1364 5646 0.24 630 6596 0.10 6583155 0.21 233 1093 0.21\n⑥Qwen2.5-7B-instruct 1399 4853 0.28 1585 10710 0.15 421 1840 0.23 507 2094 0.24\n⑦Claude-3.7-Sonnet 2050 8831 0.23 2023 17230 0.12 6963013 0.23 133 543 0.24\n⑧DeepSeek-V3 2260 7750 0.30 2290 19744 0.12 17 48 0.35 7 10 0.7\n⑨Gemini-1.5-flash 2027 6616 0.31 2222 14898 0.15 21 37 0.57 3 10 0.3\n10\n--- Page 11 ---\n4.3 Application 3: Diversity Measure\nSetup. Our final application evaluates the diversity of different LLMs in response to open-ended\nprompts, such as “Name a possible LLM application and explain why.” Unlike the previous two\napplications, correctness verification is unnecessary, as any fluent and meaningful response is\nconsidered valid, which is typically satisfied by sufficiently large models. To quantify diversity, we\ndesign two tasks. The first asks the LLM to describe a real or imagined application of LLMs. The\nsecond asks it to invent a job it might find appealing, if it could dream like a human. In both\ntasks, we instruct the models to generate short, simple responses with brief explanations. Each\nLLM is independently queried Nquery = 1000times, collecting Nans= 1response per query. To\ncluster similar responses, we embed each response using OpenAI’s text-embedding-ada-002 model,\nwhich produces a 1536-dimensional semantic vector. Two responses are considered equivalent if the\ndistance between their embeddings is smaller than a specified threshold. This clustering step is\nnecessary, as LLMs may produce nearly identical applications or jobs but provide different reasons.\nTo determine the threshold, we first compute the 10-nearest neighbor distances for each response\nwithin each LLM, resulting in one set of distances per model. We then aggregate these distances\nacross all models and set the threshold to the q-quantile of the combined set.\nResults. Results for diversity estimation at q= 0.5are shown in the right two columns of\nTable 2. Claude-3.7-Sonnet produces the largest number of observed LLM applications, while\nMistral-7B-instruct-v0.1 has the highest estimated total knowledge. For the “dream jobs” task,\nLLaMA-V3-3B-instruct achievesthehighestcoverageinbothobservedcount Nseenandtotalestimate\nbNtot. In contrast, DeepSeek-V3 andGemini-1.5-flash produce the fewest outputs across both\ntasks, likely reflecting more deterministic generation behavior. Most models exhibit an SKR near\n23%, indicating that only a small fraction of their internal knowledge is expressed through sampling.\nWhileDeepSeek-V3 andGemini-1.5-flash show higher SKRs (0.7 and 0.3, respectively), this likely\nresults from limited generation diversity, leading to fewer unseen items. This 23% SKR is robust\nfor different clustering thresholds q∈ {0.2,0.3,0.5,0.7}, with the top six models remaining within\nthe 20–25% range. This suggests a common tendency across LLMs: only a modest portion of their\ngenerative potential is typically realized.\n5 Validation and Sensitivity Analysis\n5.1 Cross-Validation of Unseen Knowledge Estimates\nTo evaluate the accuracy of our pipeline, we perform held-out validation by splitting the Nquery\nresponses into two parts: the first robsfraction is treated as observed data, and we estimate the\nnumber of unique theorems that appear only in the remaining (1−robs)fraction. We repeat this\nprocedure 100 times, each with a random shuffle of the full dataset so that the observed subset\ndiffers across runs. We report the average estimated and ground truth unseen counts in Figure 7,\nwith error bars indicating the standard deviation over 100 repetitions. We consider three values\nforrobs∈ {1/2,1/3,1/4}, corresponding to a largest extrapolation factor of t= 1,2,3, respectively.\nAcross all tested LLMs, the estimated unseen counts closely match the ground truth, implying that\nthe SGT estimator yields highly accurate predictions for a wide range of extrapolation factors.\n11\n--- Page 12 ---\n0.2 0.4 0.6 0.8 1.0\nExtrapolation t50100150200250300Counts\n0.5 1.0 1.5 2.0\nExtrapolation t100200300400500\n0.5 1.0 1.5 2.0 2.5 3.0\nExtrapolation t200400600\nChatGPT-4o-chat ChatGPT-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instructFigure 7: SGT estimates (colored curves) versus ground truth (black dotted lines) for theorem\nestimation. From left to right, the observed fraction robsis1/2,1/3, and 1/4. Counterpart results\nfor human diseases are shown in Figure 11 in the Appendix.\n5.2 Sensitivity Analysis\nEffect of the extrapolation factor t.Next, we examine the effect of the extrapolation factor t.\nBy definition, as tincreases, the estimated number of unseen items bNseen(t)should grow so that\nthe SKR decreases. This trend is confirmed in the left two plots of Figure 8, where we observe a\nmonotonic increase in bNseen(t)and a corresponding decrease in SKR. Additionally, the variance of the\nestimate becomes larger as tincreases, reflecting greater uncertainty in longer-range extrapolation.\nIn practice, we find that both quantities tend to saturate around t= 80, with little gain beyond that\npoint. This motivates our choice of t= 100as a default in all experiments.\n0 25 50 75 100\nExtrapolation t100020003000Nseen+/hatwideNunseen (t)\n0 25 50 75 100\nExtrapolation t0.40.60.81.0SKR(t)\nInst↑\nNans↑Inst↑\nNans↓Inst↓\nNans↑Inst↓\nNans↓0500100015002000Nseen\nInst↑\nNans↑Inst↑\nNans↓Inst↓\nNans↑Inst↓\nNans↓0100020003000/hatwideNtotChatGPT-4o-chat\nChatGPT-3.5-turbo-chatLLaMa-V3-70B-instruct\nLLaMa-V3-3B-instruct\nFigure 8: Effects of the extrapolation factor t(left two plots) and prompts (right two plots) for\ntheorem estimation. Counterpart results for human diseases are shown in Figure 12 in the Appendix.\n12\n--- Page 13 ---\nEffect of the prompt. An important practical issue is how to design the prompt. We examine two\nkey factors: the clarity of the instruction ( Inst↑orInst↓) and the number of requested responses\nper query ( Nans∈ {10,20}). We test four configurations that vary along these two dimensions. The\nresults of theorem counting are shown in the right two plots of Figure 8. Overall, prompts with\nclearer instructions and a higher number of requested responses lead to more diverse and informative\noutputs, increasing both NseenandbNtotin our experiments, and thereby improving access to the\nmodel’s internal knowledge. The set of used prompts is listed in Appendix B.2.\nEffect of the truncation level k.The first plot in Figure 9 shows how the estimated total\nknowledge bNtotvaries with the truncation level k, while the second plot presents the normalized mean\nsquare error (MSE). Overall, the estimator is relatively insensitive to the choice of k, suggesting that\nthe prevalence histogram nsis generally stable and informative, which enables accurate extrapolation\nover a broad range of truncation levels. Nonetheless, we recommend using the held-out validation\nprocedure described in Section 5.1 to select kadaptively for each setting. As illustrated in the second\nplot, most models perform best when k= 8. The complete set of selected kvalues for each LLM\nand application is provided in Appendix B.5.\n4 5 6 7 8 9 10\nTruncation level k010002000300040005000/hatwideNtot\n6 8 10\nTruncation level k00.1%0.2%0.3%Normalized MSE E\nTemp↑\nNuc×Temp↓\nNuc×Temp↑\nNuc/checkTemp↓\nNuc/check0100020003000Nseen\nTemp↑\nNuc×Temp↓\nNuc×Temp↑\nNuc/checkTemp↓\nNuc/check0100020003000/hatwideNtotChatGPT-3.5-turbo-chat\nLLaMa-V3-70B-instructLLaMa-V3-3B-instruct\nMistral-7B-instruct-V0.1\nFigure 9: Effects of the truncation level k(left two plots) and sampling strategies (right two plots) for\ntheorem estimation. Counterpart results for human diseases are shown in Figure 13 in the Appendix.\nEffect of temperatures and sampling methods. LLMs generate responses through next-token\nprediction, meaning that sampling strategy directly influences their expressed capabilities. We\nconsider two key factors: the temperature Temp ∈ {0.7,1}and the use of nucleus sampling (denoted\nasNuc✓if applied and Nuc×if not), which truncates the token distribution to the smallest set\nof tokens whose cumulative probability exceeds 0.9[27]. The right two plots of Figure 9 present\nresults under these four configurations. We observe that using a higher temperature and disabling\ntruncation yields the highest values for both NseenandbNtot. Therefore, we adopt a temperature of 1\n13\n--- Page 14 ---\nand disable nucleus sampling in all experiments to best capture each model’s capabilities.\n6 Discussion\nWe have presented KnowSum , a five-step modular pipeline for estimating the unseen knowledge\nencoded in LLMs. By focusing on what models could output rather than what they do output, our\nmethod provides a complementary perspective to standard evaluation. We show its effectiveness\nacross three applications: knowledge estimation, information retrieval, and diversity measurement.\nOur framework opens several avenues for extension. While this work focuses on estimating the\nnumber of unseen knowledge items, similar estimators can be adapted to quantify knowledge that\nappears at least stimes [25]. Currently, our framework focuses on well-defined, countable knowledge,\nsuch as named theorems or diseases, whose correctness can be easily verified. However, real-world\nknowledge is often more complex and less structured. For example, knowing the name of a theorem\nis not the same as understanding it, where “understanding” itself is difficult to define. As another\nexample, if we shift our focus to proof techniques, it is generally hard for machines to automatically\nidentify which techniques are used—especially since complex theorem proofs often involve multiple\ntechniques and intricate relationships among them. In principle, if appropriate verification and\nclustering methods can be developed, our pipeline could still apply in these settings. However, doing\nso is generally difficult, and we leave it as an open problem. Finally, a more ambitious direction is\nto move beyond estimation and toward extraction—actively surfacing knowledge that the model\nencodes but rarely generates. Transitioning from passive statistical inference to active knowledge\ndiscovery introduces both theoretical and practical challenges, and we view this as an important and\nimpactful direction for future research.\nAcknowledgments\nThis work was supported in part by NIH grants, RF1AG063481 and U01CA274576, NSF DMS-\n2310679, a Meta Faculty Research Award, and Wharton AI for Business. The content is solely the\nresponsibility of the authors and does not necessarily represent the official views of the NIH.\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023.\n[2]Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage\nand extraction. arXiv preprint arXiv:2309.14316 , 2023.\n[3]Anthropic. Claude3.7Sonnetsystemcard. https://www.anthropic.com/claude-3-7-sonnet-\nsystem-card , February 2025. Accessed: 2025-05-15.\n[4]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\n14\n--- Page 15 ---\n[5]Samuel R Bowman. Eight things to know about large language models. Critical AI , 2(2), 2024.\n[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems , volume 33, pages\n1877–1901, 2020.\n[7]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith GPT-4. arXiv preprint arXiv:2303.12712 , 2023.\n[8]Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\nChiyuan Zhang. Quantifying memorization across neural language models. In International\nConference on Learning Representations , 2023. URL https://openreview.net/forum?id=\nTatRHT_1cK .\n[9]Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang,\nand Minjoon Seo. How do large language models acquire factual knowledge during pretraining?\nInNeural Information Processing Systems , 2024.\n[10]Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,\nXiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language\nmodels.ACM transactions on intelligent systems and technology , 15(3):1–45, 2024.\n[11]Anne Chao and John Bunge. Estimating the number of species in a stochastic abundance model.\nBiometrics , 58(3):531–539, 2002.\n[12]Anne Chao, Robin L Chazdon, Robert K Colwell, and Tsung-Jen Shen. A new statistical\napproach for assessing similarity of species composition with incidence and abundance data.\nEcology letters , 8(2):148–159, 2005.\n[13]Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,\nDacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica.\nChatbot arena: An open platform for evaluating LLMs by human preference. In International\nConference on Machine Learning , pages 8359–8388. PMLR, 2024.\n[14]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\n[15]NuritCohen-Inger, YehonatanElisha, BrachaShapira, LiorRokach, andSeffiCohen. Forgetwhat\nyouknowaboutLLMsevaluations—LLMsarelikeachameleon. arXiv preprint arXiv:2502.07445 ,\n2025.\n[16]Nick Craswell. Mean Reciprocal Rank , pages 1703–1703. Springer US, Boston, MA, 2009. ISBN\n978-0-387-39940-9. doi: 10.1007/978-0-387-39940-9_488. URL https://doi.org/10.1007/\n978-0-387-39940-9_488 .\n15\n--- Page 16 ---\n[17]Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya,\nPeter Christian Norgaard, Nayantara Mudur, Martyna Beata Plomecka, Paul Raccuglia, et al.\nCURIE: Evaluating LLMs on multitask scientific long-context understanding and reasoning. In\nInternational Conference on Learning Representations , 2025. URL https://openreview.net/\nforum?id=jw2fC6REUB .\n[18] David Donoho. Data science at the singularity. Harvard Data Science Review , 6(1), 2024.\n[19]Bradley Efron and Ronald Thisted. Estimating the number of unseen species: How many words\ndid Shakespeare know? Biometrika , 63(3):435–447, 1976.\n[20]Stefano Favaro and Zacharie Naulet. Near-optimal estimation of the unseen under regularly\nvarying tail populations. Bernoulli , 29(4):3423–3442, 2023.\n[21]William A Gale and Geoffrey Sampson. Good-turing frequency estimation without tears. Journal\nof quantitative linguistics , 2(3):217–237, 1995.\n[22]IrvingJGood. Thepopulationfrequenciesofspeciesandtheestimationofpopulationparameters.\nBiometrika , 40(3-4):237–264, 1953.\n[23]Daniela Gottesman and Mor Geva. Estimating knowledge in large language models without\ngenerating a single token. In Proceedings of the 2024 Conference on Empirical Methods in\nNatural Language Processing , pages 3994–4019, 2024.\n[24]Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\nLlama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\n[25]Yi Hao and Ping Li. Optimal prediction of the number of unseen species with multiplicity. In\nAdvances in Neural Information Processing Systems , volume 33, pages 8553–8564, 2020.\n[26]DanHendrycks, CollinBurns, StevenBasart, AndyZou, MantasMazeika, DawnSong, andJacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations , 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ .\n[27]Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. In International Conference on Learning Representations , 2020. URL\nhttps://openreview.net/forum?id=rygGQyrFvH .\n[28]Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv\npreprint arXiv:2410.21276 , 2024.\n[29]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B. arXiv preprint\narXiv:2310.06825 , 2024.\n[30]Adam Tauman Kalai and Santosh S Vempala. Calibrated language models must hallucinate. In\nProceedings of the 56th Annual ACM Symposium on Theory of Computing , pages 160–171, 2024.\n16\n--- Page 17 ---\n[31]Sahil Kale and Vijaykant Nadadur. Line of duty: Evaluating LLM self-knowledge via consistency\nin feasibility boundaries. arXiv preprint arXiv:2503.11256 , 2025.\n[32]Leonid Khomenko. Too many AIs. https://dev.to/leeaao/too-many-ais-24nb , 2025. Ac-\ncessed: 2025-05-09.\n[33]Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras.\nBioASQ-QA: A manually curated corpus for biomedical question answering. Scientific Data , 10\n(1):170, 2023.\n[34]Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Cheng-Jie Sun, Bingquan Liu, Zhenzhou\nJi, Xin Jiang, and Qun Liu. How pre-trained language models capture factual knowledge? A\ncausal-inspired analysis. In Findings of the Association for Computational Linguistics: ACL\n2022, pages 1720–1732, 2022.\n[35]Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text summariza-\ntion branches out , pages 74–81, 2004.\n[36]Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-V3 technical report. arXiv preprint\narXiv:2412.19437 , 2024.\n[37]Milad Nasr, Javier Rando, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder\nCooper, Daphne Ippolito, Christopher A. Choquette-Choo, Florian Tramèr, and Katherine Lee.\nScalable extraction of training data from aligned, production language models. In International\nConference on Learning Representations , 2025. URL https://openreview.net/forum?id=\nvjel3nWP2a .\n[38]OpenAI. ChatGPT-3.5-turbo. https://platform.openai.com/docs/models/gpt-3-5 , 2023.\nAccessed: 2025-05-15.\n[39]Alon Orlitsky and Ananda Theertha Suresh. Competitive distribution estimation: Why is\nGood-Turing good. In Advances in Neural Information Processing Systems , volume 28, 2015.\n[40]Alon Orlitsky, Narayana P Santhanam, and Junan Zhang. Always Good Turing: Asymptotically\noptimal probability estimation. Science, 302(5644):427–431, 2003.\n[41]Alon Orlitsky, Ananda Theertha Suresh, and Yihong Wu. Optimal prediction of the number of\nunseen species. Proceedings of the National Academy of Sciences , 113(47):13283–13288, 2016.\n[42]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics , pages 311–318, 2002.\n[43]Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing , pages 2463–2473, 2019.\n17\n--- Page 18 ---\n[44]Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint\narXiv:2501.14249 , 2025.\n[45]Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani, and Sarath Chandar.\nDo large language models know how much they know? arXiv preprint arXiv:2502.19573 , 2025.\n[46]Inioluwa Deborah Raji, Emily Denton, Emily M Bender, Alex Hanna, and Amandalynne\nPaullada. AI and the everything in the whole wide world benchmark. In Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2) , 2021.\n[47]Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko\nAgirre. NLP evaluation in trouble: On the need to measure LLM data contamination for each\nbenchmark. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pages\n10776–10787, 2023.\n[48]Mark Saroufim, Yotam Perlitz, Leshem Choshen, Luca Antiga, Greg Bowyer, Christian Puhrsch,\nDriss Guessous, Supriya Rao, Geeta Chauhan, Ashvini Kumar, Jindal Pawan Kumar, Ra-\njpoot Ankur Parikh, Joe Isaacson, and Weiwei Yang. NeurIPS 2023 LLM efficiency fine-tuning\ncompetition. arXiv preprint arXiv:2503.13507 , 2025.\n[49]LynnMSchriml, EvangelosMitraka, JamieMunro, BethanyTauber, MarinaSchor, LynneNickle,\nVivian Felix, Lisa Jeng, Catherine Bearer, Robert Lichenstein, et al. Human disease ontology\n2022 update. Nucleic Acids Research , 50(D1):D1255–D1261, 2022. doi: 10.1093/nar/gkab920.\n[50]Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Auto-\nPrompt: Eliciting knowledge from language models with automatically generated prompts.\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) . Association for Computational Linguistics, 2020.\n[51] David Silver and Richard S Sutton. Welcome to the era of experience. Google AI , 2025.\n[52]Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D’Souza, Sayash Kapoor, Ahmet Üstün, Sanmi\nKoyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, and\nSara Hooker. The leaderboard illusion. arXiv preprint arXiv:2504.20879 , 2025.\n[53]Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models\nencode clinical knowledge. Nature, 620(7972):172–180, 2023.\n[54] Richard Sutton. The bitter lesson. Incomplete Ideas (blog) , 13(1):38, 2019.\n[55]Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. arXiv preprint arXiv:2211.09085 , 2022.\n[56]Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n18\n--- Page 19 ---\n[57]Mildred C. Templin. Certain Language Skills in Children: Their Development and In-\nterrelationships , volume 26 of Institute of Child Welfare Monograph Series . University of\nMinnesota Press, Minneapolis, new edition edition, 1957. ISBN 978-0816601523. URL\nhttps://www.jstor.org/stable/10.5749/j.ctttv .\n[58]Chenguang Wang, Xiao Liu, and Dawn Song. Language models are open knowledge graphs.\narXiv preprint arXiv:2010.11967 , 2020.\n[59]Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J\nSu, Zhun Deng, Michael Qizhe Xie, Hannah Brown, and Kenji Kawaguchi. Can AI be as creative\nas humans? arXiv preprint arXiv:2401.01623 , 2024.\n[60]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in neural information processing systems , volume 35, pages 24824–24837,\n2022.\n[61]Gabriel Wu and Jacob Hilton. Estimating the probabilities of rare outputs in language models.\nInInternational Conference on Learning Representations , 2025. URL https://openreview.\nnet/forum?id=DC8bsa9bzY .\n[62]An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[63]Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao,\nPranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele\nLunati, and Summer Yue. A careful examination of large language model performance on grade\nschool arithmetic. In Advances in Neural Information Processing Systems , volume 37, pages\n46819–46836, 2024.\n[64]Wenxin Zhou and Thuy Hang Ngo. Using pretrained large language model with prompt\nengineering to answer biomedical questions. arXiv preprint arXiv:2407.06779 , 2024.\n[65]Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, and Sen Su. Quantifying and analyzing\nentity-level memorization in large language models. In Proceedings of the AAAI Conference on\nArtificial Intelligence , volume 38, pages 19741–19749, 2024.\n19\n--- Page 20 ---\nA Proofs of Theorem 1\nProof of Theorem 1. Recall that nis the total number of valid items. So we have that n=P\ns≥1sns.\nBy the expression (1)of the SGT estimator, we have bNunseen (t) =Pk\ns=1hsns. To prove the result,\nit suffices to show that \f\f\f\f\fbNunseen (t)\nNobs\f\f\f\f\f≤ekt\nt+1. (2)\nWe now proceed to prove (2). It follows that\nbNunseen (t)\nNobs=Pk\ns=1hsnsP\ns≥1ns≤max\n1≤s≤k|hs|\n≤max\n1≤s≤kts·P\u0012\nBin\u0012\nk,1\nt+ 1\u0013\n≥s\u0013\n(a)\n≤max\n1≤s≤k\u0012tek\n(t+ 1)s\u0013s\ne−k\nt+1\n≤max\n1≤s≤k\u0012ek\ns\u0013s\ne−k\nt+1\n(b)\n≤ekt\nt+1,\nwhere the inequality (a)uses Lemma A.1 and (b)uses the fact that\u0000ek\ns\u0001sachieves its maximum\nwhen s=kso that\u0000ek\ns\u0001s≤ek.\nLemma A.1 (Tail bound) .\nP\u0012\nBin\u0012\nk,1\nt+ 1\u0013\n≥s\u0013\n≤\u0012ek\n(t+ 1)s\u0013s\n.\nProof of Lemma A.1. LetX∼Bin(k,1\nt+1), and define the mean µ=E[X] =k\nt+1. We aim to upper\nbound the tail probability P(X≥s)fors > µ. To that end, we apply the multiplicative Chernoff\nbound:\nP(X≥s)≤\u0012eδ\n(1 +δ)1+δ\u0013µ\n,\nwhere δ=s−µ\nµ=s\nµ−1. Substituting this into the bound, we obtain:\nP(X≥s)≤\nes\nµ−1\n\u0010\ns\nµ\u0011s\nµ\nµ\n.\nNow we simplify the right-hand side. Taking logarithms:\nlogP(X≥s)≤µ\u0012s\nµ−1−s\nµlog\u0012s\nµ\u0013\u0013\n=−slog\u0012s\nµ\u0013\n+s−µ.\n20\n--- Page 21 ---\nExponentiating both sides gives:\nP(X≥s)≤exp\u0012\n−slog\u0012s\nµ\u0013\n+s−µ\u0013\n=\u0010µ\ns\u0011s\n·es−µ.\nFinally, plugging in µ=k\nt+1, we get:\nP(X≥s)≤\u0010e·µ\ns\u0011s\n·e−µ=\u0012ek\n(t+ 1)s\u0013s\ne−k\nt+1.\nB Experiments Details\nB.1 General Setup\nIn our work, we evaluate nine widely used LLMs: ChatGPT-4o-chat [28],ChatGPT-3.5-turbo-chat\n[38],LLaMA-V3-70B-instruct andLLaMA-V3-3B-instruct [24],Mistral-7B-instruct-V0.1 [29],\nQwen2.5-7B-instruct [62],Claude-3.7-Sonnet [3],DeepSeek-V3 [36], andGemini-1.5-flash [56].\nTo ensure fair comparisons, we query each model using carefully designed prompt templates (see\nAppendix B.2), set the temperature to 1, and collect their responses. We then apply our estimation\npipeline (Algorithm 1) to estimate the amount of unseen knowledge. Details on the verification and\nclustering procedures are provided in Appendix B.3, along with additional experimental results in\nthe subsequent subsections.\nB.2 Used Prompts\nThe prompts used in Section 4 are listed in Table 3, where the underlined words indicate the parts\nmodified for sensitivity analysis. For Application 2 Subtask 2, the prompt templates are shown in\nTable 4. In general, we use different templates for different types of questions to provide appropriate\nexamples that facilitate effective in-context learning.\nTo test prompt sensitivity, we remove instructional content from the original prompt. In the\ntheorem counting task, the simplified version becomes: “Test your knowledge of mathematical\ntheorems by listing 20 theorem names, separated by commas.” In the human disease counting task,\nthe simplified version becomes: “You are a biomedical expert helping to compile a list of human\ndiseases. Please provide 50 human diseases along with their corresponding DOID identifiers.”\n21\n--- Page 22 ---\nTable 3: Prompts used across different applications and tasks.\nApplication Task PromptApplication 1Theorems countingTest your knowledge of mathematical theorems by\nlisting 20 theorem names, separated by commas\n(e.g., Theorem 1, Theorem 2, Theorem 3, etc.).\nIf a theorem is known by multiple names, choose\nthe most classic one. Aim for variety and try\nto include rare or less commonly known theorems.\nKeep your response concise and to the point.\nDiseases countingYou are a biomedical expert helping to compile a\nlist of human diseases.\nPlease provide 50 human diseases along with their\ncorresponding DOID identifiers.\nFormat your response as: - Disease Name\n(DOID:xxxxx)\nMake sure the Disease Name matches with DOID, and\nthe DOIDs are valid from Disease Ontology.Application 2BioASQ retrieval [64]Given a question, expand into a search query for\nPubMed by incorporating synonyms and additional\nterms that would yield relevant search results\nfrom PubMed to the provided question while not\nbeing too restrictive. Assume that phrases\nare not stemmed; therefore, generate useful\nvariations. Return only the query that can\ndirectly be used without any explanation text.\nQuestion: What is the mode of action of\nMolnupiravir?\nQuery: Molnupiravir AND (\"mode of action\" OR\nmechanism).\nQuestion: Is dapagliflozin effective for\nCOVID-19?\nQuery: dapagliflozin AND (COVID-19 OR SARS-CoV-2\nOR coronavirus) AND (efficacy OR effective OR\ntreatment).\nQuestion: Name monoclonal antibody against\nSLAMF7.\nQuery: \"SLAMF7\" AND (\"monoclonal antibody\" OR\n\"monoclonal antibodies\").\nQuestion: {context}\nQuery: {body}Application 3LLMs applicationsBriefly describe one real or imagined application\nof large language models (LLMs). Keep the\nresponse short and simple, and explain why it’s\nuseful.\nDream jobsImagine that a large language model (LLM) could\ndream like a human. Describe, in one or two\nsentences, what kind of job it might wish to have.\nBe creative, and explain briefly why this job\nwould appeal to an LLM.\n22\n--- Page 23 ---\nTable 4: Context prompt templates used for each question type in Application 2, Subtask 2 (Question\nAnswering) of the BioASQ challenge.\nQuestion Type Prompt Templates\nYes/NoHere are some example yesno questions with answers.\n###Context: Papilins are homologous, secreted extracellular\nmatrix proteins which share a common order of protein\ndomains.\nQuestion: Is the protein Papilin secreted?\nIdeal answer: Yes, papilin is a secreted protein\nExact answer: yes\n###Context: Most lncRNAs are under lower sequence\nconstraints than protein-coding genes and lack conserved\nsecondary structures, making it hard to predict them\ncomputationally.\nQuestion: Are long non coding RNAs as conserved in sequence\nas protein coding genes?\nIdeal answer: No. Most long non coding RNAs are under\nlower sequence constraints than protein-coding genes.\nExact answer: no\n###Now answer the following yesno question:\n###Context: {context}\nQuestion: {body}\nYour Answer should be the following format:\nIdeal answer:<Your Ideal Answer>. Exact answer:<Your Exact\nAnswer>.\nSummaryHere are some example summary questions with answers.\n###Context: Hirschsprung disease (HSCR) is a\nmultifactorial, non-mendelian disorder in which rare\nhigh-penetrance coding sequence mutations in the receptor\ntyrosine kinase RET contribute to risk in combination with\nmutations at other genes.\nQuestion: Is Hirschsprung disease a mendelian or a\nmultifactorial disorder?\nAnswer: Coding sequence mutations in RET, GDNF, EDNRB,\nEDN3, and SOX10 are involved in the development of\nHirschsprung disease. The majority of these genes was shown\nto be related to Mendelian syndromic forms...\n###Now answer the following summary question:\nContext: {context}\nQuestion: {body}\nAnswer:\nListHere are some example list questions with answers.\n###Context: The FGFR3 P250R mutation was the single largest\ncontributor (24%) to the genetic group.\nQuestion: Which human genes are more commonly related to\ncraniosynostosis?\nIdeal answer: The genes that are most commonly linked to\ncraniosynostoses are...\nExact answer: FGFR3;FGFR2;FGFR1;...\n###Now answer the following list question:\nContext: {context}\nQuestion: {body}\nIdeal answer:<Your Ideal Answer>. Exact answer:<Your Exact\nAnswer>.\nFactoidHere are some example factoid questions with answers.\n###Context: Ewing sarcoma is the second most common bone\nmalignancy in children and young adults.\nQuestion: Which fusion protein is involved in the\ndevelopment of Ewing sarcoma?\nIdeal answer: Ewing sarcoma is driven by oncogenic fusion\nprotein EWS/FLI1...\nExact answer: EWS;FLI1\n###Now answer the following factoid question:\nContext: {context}\nQuestion: {body}\nIdeal answer:<Your Ideal Answer>. Exact answer:<Your Exact\nAnswer>.\n23\n--- Page 24 ---\nB.3 Details of Verification and Clustering\nB.3.1 Application 1: Knowledge Estimation\nMathematical theorems. In theverification step, we sequentially cross-reference each generated\nitem against three external sources: Wikipedia, MathSciNet, and ProofWiki. The process stops as\nsoon as a match is found. Two theorems are considered identical if they are verified by the same\nwebpage or share the same URL.\nIn theclustering step, since a single theorem may have multiple equivalent names, we assign a\ncanonical identifier to each theorem by extracting a standardized name from the corresponding URL.\nFor Wikipedia and ProofWiki, we parse the page title from the URL path, convert it to lowercase,\nand replace underscores with spaces. For MathSciNet, we extract the search query parameter. All\nextracted names are further passed through a normalization routine to reduce inconsistencies due to\nformatting or spelling variations. This process ensures that each valid theorem instance is mapped\nto a unique and consistent name across sources.\nHuman Disease Oncology. In theverification step, each LLM-generated disease name is\nevaluated by matching it against official disease terms and their synonyms. These synonyms are\ndrawn from the “term.synonym” field in the OBO file provided by the Disease Ontology. Matching\nis performed using fuzzy string comparison via the “rapidfuzz” Python package, with a similarity\nthreshold of 0.9. The verification process terminates once a valid match is found.\nIn theclustering step, all matched terms are grouped according to their corresponding Disease\nOntology Identifier (DOID). For example, if an LLM generates a disease name along with two of its\nsynonyms, all three generated names will be mapped to the same DOID, and the count for that\nDOID will be incremented by three.\nB.3.2 Application 2: Information Retrieval\nWe evaluate the information retrieval capabilities of LLMs using the official Task12BGolden test set\nfrom the BioASQ Challenge [ 33]. Each question in this dataset is annotated with a set of relevant\nbiomedicaldocuments, eachuniquelyidentifiedbyaPubMedID.ForeachPubMedID(orequivalently\neach biomedical document), we retrieve the associated Medical Subject Headings (MeSH) terms us-\ning the NCBI E-utilities API ( https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi ).\nThroughout this application, normalized MeSH terms serve as the countable unit of biomedical\nknowledge—analogous to the role of theorems in Application 1. In the clustering step, keywords\nwere normalized to a unique form by splitting the original name on the slash delimiter (“/”) and\nretaining the last segment in lowercase.\nSubtask 1: Document Retrieval. This subtask assesses an LLM’s ability to formulate effective\nBoolean queries for retrieving relevant scientific articles from the PubMed database. To measure\nunseen knowledge, we estimate the number of additional, correct MeSH terms the model could\nuncover if queried with semantically similar inputs. During the generation phase, we prompt the\nLLM with a BioASQ question using a fixed template (see Table 3), and use the resulting query\nto search PubMed via the E-utilities API ( db = “pubmed” ). In theverification phase, we check\nwhether retrieved PubMed IDs match any gold-standard references. If a match occurs, we accumulate\nall MeSH terms associated with the matching documents. Because each article can have multiple\n24\n--- Page 25 ---\nMeSH terms, and each question can be linked to multiple documents, this setup provides a rich\nspace of retrievable knowledge.\nSubtask 2: Question Answering. The second subtask evaluates LLMs’ ability to answer\nquestions based on retrieved biomedical content. In the generation phase, to ensure a fair\ncomparison, all models receive the same gold-standard document snippets as input. This setting\nmakes use of in-context learning. See Table 4 for the prompt template. For each BioASQ question,\nthe model generates two types of responses: (1) an exact answer , which may be of type yes/no, list,\nor factoid, and is evaluated using standard metrics such as accuracy, precision, recall, and F1 scores;\nand (2) an ideal answer , which is a free-form summary evaluated using ROUGE.\nIn theverification phase, for yes/no and factoid questions, we compare the LLM-generated\nanswer directly against the reference label. For list and summary questions, we apply threshold-based\ncriteria to assess correctness. Unlike document-level evaluation in Subtask 1, this subtask operates at\nthe question level. If the model’s output is deemed correct, we credit all the MeSH terms associated\nwith that question’s gold documents as successfully recovered. We then apply our unseen knowledge\nestimation framework to infer the number of MeSH terms the model could generate if exposed to\nadditional questions.\nB.3.3 Application 3: Diversity Measure\nWe don’t verify responses in this application. We describe how we cluster responses: each response is\nembedded using OpenAI’s text-embedding-ada-002 model, which maps text to a 1536-dimensional\nsemantic vector. Two responses are considered equivalent if the distance between their embeddings\nis below a chosen threshold. To set this threshold, we compute the 10-nearest neighbor distances for\neach response within each LLM, aggregate the distances across all models, and use the q-quantile of\nthe combined set. We evaluate q∈ {0.2,0.3,0.5,0.7}.\nB.4 Verification Criteria\nIn the verification step for Application 1, we apply two levels of verification criteria: one strict\nand one more relaxed. Since all valid knowledge items—whether theorems or human diseases—are\nsourced from external databases, we visualize the scope of these datasets to assess how permissive\nthe relaxed verification criterion can be while still maintaining validity in Figure 10.\nMathematicaltheorems. Fortheorems, thestrictcriterion requiresthat the finalname(extracted\nfrom the URL) contains the word “theorem”, while the relaxed criterion accepts any name that\nincludes one of 12 math concepts (i.e., “theorem,” “lemma,” “law,” “principle,” “formula,” “criterion,”\n“identity,” “conjecture,” “rule,” “equation,” “postulate,” or “corollary.”). The left plot in Figure 10\nshows the distribution of different theorem types in the reference databases. Notably, law is the\nmost common type, with 11,314 entries, while the theorem type includes only 2,558 entries.\nHuman diseases. For human diseases, the strict criterion includes only the “anatomical disease”\ncategory—i.e., diseases that affect specific body parts. The relaxed criterion allows any disease listed\nin the Disease Ontology database, regardless of category. The right plot in Figure 10 displays the\ndistribution of disease types in the reference databases. Notably, anatomical disease is the most\ncommon category, with 6036 entries.\n25\n--- Page 26 ---\n0 2000 4000 6000 8000 10000\nNumber of unique namesLaw\nRule\nFormula\nTheorem\nEquation\nIdentity\nPrinciple\nConjecture\nLemma\nCriterion\nPostulate\nCorollary11314\n3782\n2829\n2558\n1157\n1051\n897\n553\n307\n251\n49\n14\n0 1000 2000 3000 4000 5000 6000\nNumber of unique namesDisease of anatomical entity\nDisease of cellular proliferation\nSyndrome\nDisease of metabolism\nDisease of mental health\nDisease by infectious agent\nPhysical disorder\nGenetic disease6036\n2810\n911\n864\n555\n462\n144\n90Figure 10: Distribution of math concepts ( left) and human disease subcategories ( right). These\ndistributions are used as filtering criteria to exclude invalid knowledge items under the strict criteria\nsetting in Application 1.\nB.5 Used Truncation Levels\nAll the truncation levels used in different applications are collected in Table 5.\nTable 5: Truncation levels used in different applications.\nModelApplication 1\nTheoremsApplication 1\nHuman diseasesApplication 2\nRetrievalApplication 2\nQuestion AnswerApplication 3\nDiversity\n①ChatGPT-4o-chat 8 8 8 8 6\n②ChatGPT-3.5-turbo-chat 8 8 8 8 6\n③LLaMA-V3-70B-instruct 8 8 8 8 6\n④LLaMA-V3-3B-instruct 6 8 8 8 6\n⑤Mistral-7B-instruct-v0.1 6 10 10 10 6\n⑥Qwen2.5-7B-instruct 8 8 6 6 6\n⑦Claude-3.7-Sonnet 8 8 8 8 6\n⑧DeepSeek-V3 8 8 8 8 6\n⑨Gemini-1.5-flash 8 6 6 6 6\nB.6 Cross-Validation of Unseen Human Disease Estimates\nWe present the human disease counting results from the held-out validation analysis, complementing\nthe theorem results shown in Figure 7. As shown in Figure 11, the estimator achieves similarly strong\nperformance. The estimated counts of unseen diseases closely match the ground truth, confirming\nthat the SGT estimator yields highly accurate predictions across a wide range of extrapolation\nfactors.\nB.7 Additional Results on Extrapolation and Prompts\nWe present the counterpart of human diseases of Figure 8. We examine two key factors: the clarity of\nthe instruction ( Inst↑orInst↓) and the number of requested responses per query ( Nans∈ {30,50})\nAs shown in Figure 12 left two plots, we can see that the estimator converges at t= 100for\nChatGPT-4o-Chat ,t= 80forLLaMa-V3-70b-instruct andLLaMa-V3-3b-instruct , and t= 25for\n26\n--- Page 27 ---\n0.2 0.4 0.6 0.8 1.0\nExtrapolation t0100200300400Unseen Disease Names\n0.5 1.0 1.5 2.0\nExtrapolation t0100200300400500600\n0.5 1.0 1.5 2.0 2.5 3.0\nExtrapolation t0200400600800\nChatGPT-4o-chat ChatGPT-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instructFigure 11: SGT estimates (colored curves) versus ground truth (black dotted lines) for disease\nestimation. From left to right, the observed fraction robsis1/2,1/3, and 1/4.\nChatGPT-3.5-turbo-Chat . From the right two plots of Figure 12, we can see that clear instruction\nand high number of response can increase the diversity and elicit more LLM Unseen knowledge.\n0 25 50 75 100\nExtrapolation t02000400060008000Nseen+Nunseen (t)\n0 25 50 75 100\nExtrapolation t0.20.40.60.81.0SKR(t)\nInst↑\nNans↑Inst↑\nNans↓Inst↓\nNans↑Inst↓\nNans↓02000400060008000Nseen\nInst↑\nNans↑Inst↑\nNans↓Inst↓\nNans↑Inst↓\nNans↓02000400060008000Nseen+Nunseen (100)ChatGPT-4o-chat ChatGPT-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instruct\nFigure 12: Effects of the extrapolation factor t(left two plots) and prompts (right two plots) for\ndisease estimation.\nB.8 Additional Results on Truncation Levels and Sampling Strategies\nWe present additional results for human diseases in Figure 13, complementing Figure 9. The left two\nplots show that the estimator is relatively robust to different values of the truncation level k. The\nright two plots indicate that using a temperature of 1 and disabling nucleus sampling consistently\nyields higher values of both NseenandNtotacross models.\n4 5 6 7 8 9 10\nTruncation level k05000100001500020000Ntot\n6 8 10\nTruncation level k00.2%0.4%0.6%0.8%Normalized MSE E\nTemp↑\nNuc×Temp↓\nNuc×Temp↑\nNuc/checkTemp↓\nNuc/check0200040006000Nseen\nTemp↑\nNuc×Temp↓\nNuc×Temp↑\nNuc/checkTemp↓\nNuc/check0200040006000NtotChatGPT-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instruct Mistral-7B-instruct-V0.1\nFigure 13: Effects of the truncation level k(left two plots) and sampling strategies (right two plots)\nfor disease estimation.\n27\n--- Page 28 ---\nB.9 Responses Analysis\nTable 6 presents the most frequently generated mathematical theorems across nine LLMs, each\nprompted to list 20 theorem names over 30,000 runs. The table reports the top-8 theorems per\nmodel, along with their absolute counts and relative frequencies. Classical results such as the\n“Pythagorean Theorem,” “Fermat’s Last Theorem,” and “Gödel’s Incompleteness Theorems” appear\nconsistently across nearly all models, suggesting that these iconic theorems are embedded as core\nknowledge. At the same time, notable differences emerge: Qwen2.5-7B-instruct strongly favors\nthe “Fundamental Theorem of Calculus,” while LLaMA-V3-3B-instruct uniquely highlights the\n“Sylvester–Gallai Theorem” and “Navier–Stokes Equations.” These variations likely reflect differences\nin pretraining data coverage or model specialization.\nTable 7 shows the top-8 most frequently generated human diseases from each LLM, prompted to\nlist 50 disease names over 3000 runs. As in the theorem case, many models converge on a core set of\ncommon diseases, including “Alzheimer’s Disease,” “Parkinson’s Disease,” “Asthma,” and “Multiple\nSclerosis.” These results reflect the centrality of these conditions in biomedical corpora. Nonetheless,\ndistinct model behaviors are evident: for example, Gemini-1.5-flash emphasizes “Coronary Artery\nDisease” and “Type 2 Diabetes Mellitus,” while LLaMA-V3-3B-instruct features rarer conditions\nlike “Maturity-Onset Diabetes of the Young Type 1.” Such differences likely stem from variations in\ntraining data granularity and domain emphasis.\nTable 8 focuses on anatomical diseases and follows the same experimental setup. Common\nconditions like “Hepatitis,” “Alzheimer’s Disease,” “Parkinson’s Disease,” and “Asthma” again domi-\nnate the responses, with “Hepatitis” especially prominent across models such as ChatGPT-4o-chat ,\nDeepSeek-V3 , andClaude-3.7-Sonnet . However, each model still exhibits unique tendencies: for\ninstance, LLaMA-V3-70B-instruct frequently generates “Frontotemporal Dementia” and “Chronic\nKidney Disease,” while Mistral-7B-Instruct-v0.1 leans toward “Cerebrovascular Disease” and\n“Heart Disease.” These findings further underscore how LLMs encode not only general but also\nmodel-specific biomedical knowledge, shaped by their underlying training regimes.\n28\n--- Page 29 ---\nTable 6: Top-8 theorem outputs per model with their counts and fractions.\nModel Top-8 Outputs with Counts and Fractions\n①ChatGPT-4o-chatPythagorean Theorem (29848, 0.0557) Brouwer Fixed Point Theorem (29813, 0.0557)\nGödel Incompleteness Theorems (29807, 0.0557) Noether Theorem (29583, 0.0552)\nFermat Last Theorem (29337, 0.0548) Ramsey Theorem (25767, 0.0481)\nGreen Theorem (19582, 0.0366) Cantor Theorem (18837, 0.0352)\n②ChatGPT-3.5-turbo-chatPythagorean Theorem (32631, 0.0599) Fermat Last Theorem (29370, 0.0539)\nBayes Theorem (25364, 0.0465) Fundamental Theorem of Calculus (23909, 0.0439)\nGödel Incompleteness Theorems (20820, 0.0382) Brouwer Fixed Point Theorem (20291, 0.0372)\nBolzano Weierstrass Theorem (18585, 0.0341) Jordan Curve Theorem (17976, 0.0330)\n③LLaMA-V3-70B-instructFundamental Theorem of Algebra (19825, 0.0406) Fermat Last Theorem (18687, 0.0382)\nPythagorean Theorem (18214, 0.0373) Brouwer Fixed Point Theorem (13398, 0.0274)\nGödel Incompleteness Theorems (13089, 0.0268) Gauss Bonnet Theorem (12233, 0.0250)\nJordan Curve Theorem (11030, 0.0226) Intermediate Value Theorem (9936, 0.0203)\n④LLaMA-V3-3B-instructFermat Last Theorem (30185, 0.0704) Sylvester Gallai Theorem (16568, 0.0387)\nKepler Conjecture (14174, 0.0331) Poincaré Conjecture (13602, 0.0317)\nBrouwer Fixed Point Theorem (11178, 0.0261) Euler Identity (10231, 0.0239)\nLagrange Theorem (9576, 0.0223) Navier Stokes Equations (9028, 0.0211)\n⑤Mistral-7B-Instruct-v0.1Pythagorean Theorem (868, 0.0501) Fermat Last Theorem (665, 0.0384)\nList of Theorems (574, 0.0332) Uncertainty Principle (486, 0.0281)\nGödel Incompleteness Theorems (459, 0.0265) Theorem (279, 0.0161)\nGauss Bonnet Theorem (279, 0.0161) Sylow Theorems (236, 0.0136)\n⑥Qwen2.5-7B-instructFundamental Theorem of Calculus (14258, 0.1500) Fundamental Theorem of Arithmetic (5276, 0.0555)\nGödel Incompleteness Theorems (3159, 0.0332) Modularity Theorem (2227, 0.0234)\nDirichlet Approximation Theorem (2142, 0.0225) Mean Value Theorem (1754, 0.0185)\nPoincaré Bendixson Theorem (1668, 0.0175) Law of Cosines (1641, 0.0173)\n⑦Claude-3.7-SonnetBrouwer Fixed Point Theorem (5008, 0.0545) Pythagorean Theorem (4994, 0.0543)\nFermat Last Theorem (4994, 0.0543) Gödel Incompleteness Theorems (4994, 0.0543)\nBayes Theorem (4860, 0.0528) Gauss Bonnet Theorem (4779, 0.0520)\nBorsuk Ulam Theorem (4643, 0.0505) Sylow Theorems (4056, 0.0441)\n⑧DeepSeek-V3Pythagorean Theorem (29999, 0.0526) Brouwer Fixed Point Theorem (29982, 0.0526)\nHahn Banach Theorem (29835, 0.0523) Gödel Incompleteness Theorems (29402, 0.0516)\nCentral Limit Theorem (29223, 0.0513) Fundamental Theorem of Algebra (27156, 0.0476)\nNoether Theorem (27038, 0.0474) Heine Borel Theorem (26967, 0.0473)\n⑨Gemini-1.5-flashPythagorean Theorem (30000, 0.0651) Prime Number Theorem (29989, 0.0651)\nCentral Limit Theorem (29936, 0.0649) Jordan Curve Theorem (29825, 0.0647)\nMean Value Theorem (29225, 0.0634) Law of Cosines (28753, 0.0624)\nBolzano Weierstrass Theorem (28101, 0.0610) Intermediate Value Theorem (27045, 0.0587)\nB.10 Traditional Evaluation of Biomedical Information Retrieval\nWereporttheresultsoftraditionalevaluationmetricsforApplication2inthissubsection. Inoursetup,\neach biomedical question is paired with a set of ground-truth documents, each annotated with expert-\ncurated MeSH keywords. These annotations serve as a concrete proxy for the biomedical knowledge\na model retrieves during document search or question answering. Standard IR metrics—such as\nprecision, recall, F1 score, and mean reciprocal rank (MRR)—evaluate model performance based on\nhow accurately the retrieved documents match these ground-truth references. In the following, we\nprovide a detailed analysis of the results under these classic IR metrics.\n29\n--- Page 30 ---\nTable 7: Top-8 Human diseases outputs per model with their counts and fractions.\nModel Top-8 Outputs with Counts and Fractions\n①ChatGPT-4o-chatAlzheimer’S Disease (3794, 0.0262) Parkinson’S Disease (2995, 0.0207)\nAsthma (2966, 0.0205) Multiple Sclerosis (2941, 0.0203)\nRheumatoid Arthritis (2919, 0.0201) Hypertension (2885, 0.0199)\nCrohn’S Disease (2879, 0.0199) Tuberculosis (2843, 0.0196)\n②ChatGPT-3.5-turbo-chatAlzheimer’S Disease (2969, 0.0221) Parkinson’S Disease (2888, 0.0215)\nAsthma (2826, 0.0211) Rheumatoid Arthritis (2793, 0.0208)\nHypertension (2786, 0.0208) Breast Cancer (2784, 0.0208)\nLeukemia (2772, 0.0207) Multiple Sclerosis (2761, 0.0206)\n③LLaMA-V3-70B-instructAlzheimer’S Disease (2376, 0.0211) Breast Cancer (2243, 0.0199)\nCystic Fibrosis (1971, 0.0175) Asthma (1935, 0.0172)\nChronic Obstructive Pulmonary Disease (1891, 0.0168) Parkinson’S Disease (1884, 0.0167)\nMultiple Sclerosis (1861, 0.0165) Amyotrophic Lateral Sclerosis (1853, 0.0164)\n④LLaMA-V3-3B-instructMaturity-Onset Diabetes Of The Young Type 1 (3013, 0.0260) Epilepsy (2481, 0.0214)\nHypertension (2397, 0.0207) Hypothyroidism (2278, 0.0196)\nParkinson’S Disease (2272, 0.0196) Autism Spectrum Disorder (2187, 0.0189)\nAlzheimer’S Disease (2148, 0.0185) Multiple Sclerosis (1950, 0.0168)\n⑤Mistral-7B-Instruct-v0.1Alzheimer’S Disease (1399, 0.0369) Asthma (1304, 0.0344)\nParkinson’S Disease (1200, 0.0316) Diabetes Mellitus (1090, 0.0287)\nCancer (909, 0.0239) Hypertension (876, 0.0231)\nMultiple Sclerosis (843, 0.0222) Cerebrovascular Disease (808, 0.0213)\n⑥Qwen2.5-7B-instructMaturity-Onset Diabetes Of The Young Type 1 (3295, 0.0243) Parkinson’S Disease (2941, 0.0217)\nRheumatoid Arthritis (2919, 0.0215) Multiple Sclerosis (2901, 0.0214)\nAsthma (2865, 0.0211) Alzheimer’S Disease (2842, 0.0210)\nEpilepsy (2649, 0.0195) Hypertension (2482, 0.0183)\n⑦Claude-3.7-SonnetParkinson’S Disease (1088, 0.0202) Alzheimer’S Disease (1087, 0.0202)\nMultiple Sclerosis (1086, 0.0202) Rheumatoid Arthritis (1086, 0.0202)\nHypertension (1086, 0.0202) Psoriasis (1086, 0.0202)\nSchizophrenia (1086, 0.0202) Osteoarthritis (1086, 0.0202)\n⑧DeepSeek-V3Asthma (2831, 0.0203) Alzheimer’S Disease (2814, 0.0202)\nRheumatoid Arthritis (2813, 0.0202) Cystic Fibrosis (2813, 0.0202)\nMalaria (2813, 0.0202) Tuberculosis (2813, 0.0202)\nHuntington’S Disease (2813, 0.0202) Parkinson’S Disease (2813, 0.0202)\n⑨Gemini-1.5-flashRheumatoid Arthritis (3002, 0.0202) Asthma (3001, 0.0202)\nAlzheimer’S Disease (3000, 0.0202) Parkinson’S Disease (3000, 0.0202)\nMultiple Sclerosis (2999, 0.0202) Schizophrenia (2994, 0.0201)\nCoronary Artery Disease (2984, 0.0201) Type 2 Diabetes Mellitus (2979, 0.0200)\nTable 9: Performance of traditional IR metrics on the BioASQ dataset for the document retrieval\ntask.\nModel Doc Precision Doc Recall Doc F1 Snip Precision Snip Recall Snip F1\n①ChatGPT-4o-chat 16.42% 22.22% 14.93% 4.46% 2.77% 2.57%\n②ChatGPT-3.5-turbo-chat 17.20% 20.32% 14.36% 4.68% 2.81% 2.72%\n③LLaMa-V3-70b-instruct 15.93% 20.88% 14.08% 4.67% 3.29% 2.94%\n④LLaMa-V3-3b-instruct 0.48% 1.09% 0.56% 0.06% 0.08% 0.06%\n⑤Mistral-7B-instruct-V0.1 9.68% 12.69% 8.22% 2.70% 1.54% 1.33%\n⑥Qwen2.5-7B-instruct 11.68% 13.55% 9.21% 3.23% 1.29% 1.46%\n⑦Claude-3.7-Sonnet 15.82% 23.99% 15.91% 4.15% 2.68% 2.73%\n⑧DeepSeek-V3 19.40% 23.03% 16.76% 4.91% 2.76% 2.76%\n⑨Gemini-1.5-flash 16.10% 21.65% 14.36% 4.40% 2.73% 2.62%\nSubtask 1: Document Retrieval. Table 9 presents classic IR metrics (precision, recall, F1)\nat the document and snippet levels. According to traditional IR metrics, DeepSeek-V3 achieves\nthe highest document-level F1 score (16.76%), followed closely by Claude-3.7-Sonnet (15.91%),\n30\n--- Page 31 ---\nChatGPT-4o-chat (14.93%), and ChatGPT-3.5-turbo-chat (14.36%). Snippet-level metrics are\nuniformly low across all models, with LLaMA-V3-70B-instruct (2.94%) and DeepSeek-V3 (2.76%)\nperforming best. These results suggest that while LLMs are somewhat effective at retrieving relevant\ndocuments, they struggle with extracting precise snippet-level answers. Notably, while DeepSeek-V3\nranks highest under traditional metrics, it also has the highest estimated number of unseenMeSH\nkeywords in our pipeline—demonstrating strong potential for retrieving latent biomedical knowledge.\nHowever, ifweaccountforbothobservedandestimatedunseenknowledge, ChatGPT-3.5-turbo-chat\nemerges as the best performer, with the highest total knowledge count ( Ntot= 10367 in Table 2).\nThis ranking shift illustrates how our estimation pipeline reveals model capabilities not captured\nby conventional evaluation—highlighting output diversity and latent retrieval capacity rather than\nsolely surface-level accuracy.\nTable 10: Performance of traditional IR metrics on the BioASQ dataset for the QA task.\nModel ROUGE Yes/No Acc. Factoid Strict Acc. Factoid Lenient Acc. List F1 List Prec. List Rec.\n①ChatGPT-4o-chat 17.41% 83.33% 3.53% 4.71% 31.42% 36.61% 30.53%\n②ChatGPT-3.5-turbo-chat 20.15% 88.24% 1.18% 1.18% 7.02% 8.32% 7.02%\n③LLaMa-V3-70b-instruct 19.13% 91.18% 3.53% 3.53% 1.70% 1.92% 1.65%\n④LLaMa-V3-3b-instruct 8.21% 88.24% 3.53% 3.53% 4.04% 4.92% 3.99%\n⑤Mistral-7B-instruct-V0.1 14.63% 27.45% 0.00% 0.00% 4.04% 4.83% 3.72%\n⑥Qwen2.5-7B-instruct 11.95% 87.25% 1.18% 1.18% 0.57% 0.52% 0.65%\n⑦Claude-3.7-Sonnet 16.86% 85.29% 8.24% 10.59% 16.67% 17.99% 16.46%\n⑧DeepSeek-V3 19.42% 92.16% 1.18% 1.18% 18.58% 22.52% 18.34%\n⑨Gemini-1.5-flash 17.03% 92.16% 0.00% 0.00% 18.40% 21.89% 18.65%\nSubtask 2: Question Answering (QA). Table 10 reports standard evaluation metrics for\nbiomedical question answering, including ROUGE, accuracy on yes/no and factoid questions, and F1\nfor list-type answers. DeepSeek-V3 ,Gemini-1.5-flash , andClaude-3.7-Sonnet all demonstrate\nstrong performance across several metrics. For example, DeepSeek-V3 andGemini-1.5-flash\nachieve high yes/no accuracy (92.16%) and list F1 scores above 18%, while Claude-3.7-Sonnet\nachieves the best factoid performance (10.59% lenient accuracy) and strong list-level precision. These\nresults reflect the models’ ability to generate correct and structured answers when evaluated against\nexpert annotations. However, our pipeline uncovers a different aspect of model capability. While\nChatGPT-4o-chat only ranks moderately under traditional metrics, it demonstrates the highest\nestimated total knowledge ( Ntot= 19965 ), followed by DeepSeek-V3 andClaude-3.7-Sonnet\n(Table2). Thissuggeststhat ChatGPT-4o-chat mayencodesubstantiallymoreanswer-relevantMeSH\nknowledge than is directly observed—revealing latent retrieval potential beyond what conventional\nmetrics capture. Once again, our estimator provides a more complete picture of what LLMs could\nretrieve, even when their immediate outputs fall short under surface-level evaluation.\n31\n--- Page 32 ---\nTable 8: Top-8 Anatomical diseases outputs per model with their counts and fractions.\nModel Top-8 Outputs with Counts and Fractions\n①ChatGPT-4o-chatHepatitis (4874, 0.0691) Alzheimer’S Disease (3794, 0.0538)\nParkinson’S Disease (2995, 0.0425) Asthma (2966, 0.0421)\nMultiple Sclerosis (2941, 0.0417) Rheumatoid Arthritis (2919, 0.0414)\nHypertension (2885, 0.0409) Crohn’S Disease (2879, 0.0408)\n②ChatGPT-3.5-turbo-chatHepatitis (3140, 0.0443) Alzheimer’S Disease (2969, 0.0419)\nParkinson’S Disease (2888, 0.0407) Asthma (2826, 0.0399)\nRheumatoid Arthritis (2793, 0.0394) Hypertension (2786, 0.0393)\nMultiple Sclerosis (2761, 0.0389) Osteoporosis (2704, 0.0381)\n③LLaMA-V3-70B-instructHepatitis (2572, 0.0421) Alzheimer’S Disease (2376, 0.0389)\nAsthma (1935, 0.0317) Frontotemporal Dementia (1925, 0.0315)\nChronic Obstructive Pulmonary Disease (1891, 0.0310) Parkinson’S Disease (1884, 0.0308)\nMultiple Sclerosis (1861, 0.0305) Chronic Kidney Disease (1689, 0.0277)\n④LLaMA-V3-3B-instructEpilepsy (2481, 0.0331) Hypertension (2397, 0.0320)\nHypothyroidism (2278, 0.0304) Parkinson’S Disease (2272, 0.0303)\nAlzheimer’S Disease (2148, 0.0287) Multiple Sclerosis (1950, 0.0260)\nOsteoarthritis (1866, 0.0249) Rheumatoid Arthritis (1855, 0.0248)\n⑤Mistral-7B-Instruct-v0.1Alzheimer’S Disease (1399, 0.0608) Asthma (1304, 0.0567)\nParkinson’S Disease (1200, 0.0522) Hypertension (876, 0.0381)\nMultiple Sclerosis (843, 0.0366) Cerebrovascular Disease (808, 0.0351)\nHeart Disease (627, 0.0273) Epilepsy (611, 0.0266)\n⑥Qwen2.5-7B-instructParkinson’S Disease (2941, 0.0343) Rheumatoid Arthritis (2919, 0.0340)\nMultiple Sclerosis (2901, 0.0338) Asthma (2865, 0.0334)\nAlzheimer’S Disease (2842, 0.0331) Epilepsy (2649, 0.0309)\nHypertension (2482, 0.0289) Hepatitis (2382, 0.0278)\n⑦Claude-3.7-SonnetHepatitis (1671, 0.0512) Parkinson’S Disease (1088, 0.0334)\nAlzheimer’S Disease (1087, 0.0333) Osteoarthritis (1086, 0.0333)\nEpilepsy (1086, 0.0333) Multiple Sclerosis (1086, 0.0333)\nPsoriasis (1086, 0.0333) Asthma (1086, 0.0333)\n⑧DeepSeek-V3Hepatitis (5526, 0.0715) Asthma (2831, 0.0366)\nAlzheimer’S Disease (2814, 0.0364) Parkinson’S Disease (2813, 0.0364)\nRheumatoid Arthritis (2813, 0.0364) Huntington’S Disease (2813, 0.0364)\nMultiple Sclerosis (2812, 0.0364) Frontotemporal Dementia (2807, 0.0363)\n⑨Gemini-1.5-flashHepatitis (3361, 0.0390) Rheumatoid Arthritis (3002, 0.0348)\nAsthma (3001, 0.0348) Alzheimer’S Disease (3000, 0.0348)\nParkinson’S Disease (3000, 0.0348) Multiple Sclerosis (2999, 0.0348)\nCoronary Artery Disease (2984, 0.0346) Crohn’S Disease (2979, 0.0346)\n32",
  "text_length": 90826
}