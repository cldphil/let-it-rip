{
  "id": "http://arxiv.org/abs/2505.24768v1",
  "title": "From Macro to Micro: Probing Dataset Diversity in Language Model\n  Fine-Tuning",
  "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets.",
  "authors": [
    "Haoyu Li",
    "Xuhong Li",
    "Yiming Dong",
    "Kun Liu"
  ],
  "published": "2025-05-30T16:31:05Z",
  "updated": "2025-05-30T16:31:05Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24768v1",
  "full_text": "arXiv:2505.24768v1 [cs.CL] 30 May 2025From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning Haoyu Li1, *, Xuhong Li2, *, Yiming Dong3, Kun Liu1, † 1. School of Automation, Beijing Institute of Technology 2. Baidu Inc. 3. School of Physics, Peking University haoyli_bit@bit.edu.cn,lixuhong@baidu.com, ydong@pku.edu.cn,kunliubit@bit.edu.cn Abstract Dataset diversity plays a pivotal role for the successful training of many machine learning models, particularly in the supervised fine-tuning (SFT) stage of large lan- guage model (LLM) development. Despite increasing recognition of its importance, systematic analyses of dataset diversity still remain underexplored. To address this gap, this work presents a systematic taxonomy of existing diversity-control strategies, which primarily focus on the instruction component, operating at either macroscopic (entire instruction semantics) or mesoscopic levels (instruction units), and furthermore introduces a novel analysis of microscopic diversity within the response component, specifically analyzing the statistical distribution of tokens in SFT training samples. In the experimental evaluation, we construct fixed-size datasets (e.g., 10,000 samples each) from a corpus of 117,000 open-source SFT samples, incorporating six distinct diversity-control strategies spanning macro-, meso-, and microscopic levels applied to both instructions and responses. We then fine-tune LLMs on these datasets to assess the six diversity-control strategies. Results reveal that while macroscopic and mesoscopic strategies lead to higher per- formance with increasing diversity, the microscopic strategy in responses exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. These findings offer actionable insights for constructing high-performance SFT datasets. 1 Introduction The success of large language models (LLMs) hinges not only on the advancements in model architectures and computational resources, but also critically on the acquisition and management of training data [ 20,35,2,42]. While significant research has focused on the quality of training samples, increasing attention is being given to dataset diversity that enhances models’ ability to generalize and handle real-world scenarios [ 6,7,49,53]. This work advances this line of research and focuses on the dataset diversity of the supervised fine-tuning (SFT) stage in the LLM alignment. Existing diversity-control strategies prove to be effective in enhancing the LLM capacity. The first strategy [ 17,12,16,15] is to cluster instructions by considering the entire semantics across multiple data sources (including both open-sourced datasets and proprietary user prompts), and improve the dataset diversity by utilizing more clusters as possible. An alternative strategy, exemplified by InsTag [ 30], further decomposes the instruction into atomic instruction components, and increases the dataset diversity by covering more instruction unit tags. *Equal contribution: Work done during Haoyu Li’s internship at Baidu Inc. †Corresponding author Preprint. Under review. Tagging Tokenizing ClusteringEmbedding ClusteringWhat is data diversity in supervised fine-tuning? What_is_data_diversity_in_supervised_fine-tuning? 3923374828 20057304 60089 706024423038230 question SFT dataset diversity fine-tuning Frequency Encoding What is data diversity in supervised fine-tuning? question dataset fine-tuning[0.011..., 0.009..., -0.033..., -0.087...,...], [0.127..., -0.076..., 0.085..., -0.040...,...],... topic1[0.011..., 0.009..., -0.033..., -0.087...,...],[...],... topic2[0.127..., -0.076..., 0.085..., -0.040...,...],[...],... topic3[...],[...],[...],...... [...],[...],[...],...SFTWhat_is_data_diversity_in_supervised_fine-tuning? highhighhigh mid high mid highhighlowhigh topic 1...Data Topic What is data diversity... Name some datasets... Explain the weather...topic 1 topic 2 topic 3...SFT...Data Tag What is data diversity... How does SFT impact... What is data diversity...SFT dataset AI..._supervised...Data Token What is data diversity... Why is diversity crucial.. What is data diversity..._diversity _diversity _forcasting...Choosing Choosing ChoosingInstruction: Describe the effects of climate change. response:... Instruction: Name some datasets used in machine learning. response:... Instruction: What is data diversity in supervised fine-tuning? response:... Previous OursFigure 1: Diversity-control strategies across three scales on instruction. At the macroscopic scale, each instruction is assigned to a corresponding topic after embedding and clustering [ 17]. At the mesoscopic scale, each instruction is linked to multiple relevant tags through LLM-based tagging, filtering, and clustering [ 30]. At the microscopic scale, we tokenize all instructions and select representative tokens based on their frequencies across the corpus for each instruction. Diversity- controlled datasets are then constructed based on topic/tag/token varieties, which also serve as diversity indicators. The same diversity-control strategies are also applied to the response perspective. While these strategies prove to be effective, there are two challenges along this research direction. First, systematic analyses and metrics of dataset diversity still remain underexplored. Different datasets, models and evaluations are used for experiments in previous works, and few metrics can correlate dataset diversity to the model performance significantly. Secondly, previous works focus on the instruction component of the instruction-response pair in the SFT dataset. Note that LLMs are conventionally supervised using responses as the primary training signals [ 5,19,10]. Though instructions serve as indicators of the diversity of topics, domains, disciplines or other semantic aspects [ 44,42,53] and may implicitly diversify responses, explicit signals might be more effective. To tackle the above challenges, we first present a taxonomy on the diversity strategies from macro- (entire instructions), meso- (instruction units) and microscopic (tokens) levels and on both instruction and response components during the SFT stage of LLM training. We then propose to explicitly examine the impact of controlling diversity within the response component of SFT datasets and compare its effectiveness to that of the instruction component. For comprehensive comparisons, we apply various diversity control strategies to construct SFT datasets from 117K open-source instructions where the responses are re-constructed for quality control, and train hundreds of models using Llama series models [39, 13] for quantifying the effectiveness of different diversity strategies. Despite these efforts, defining dataset diversity and establishing robust metrics correlating diversity with model performance remain challenging. We collect multiple diversity-related metrics and involve an additional metric relating to information theory, which may serve as a powerful tool for quantifying dataset characteristics. We present analyses and discuss the advantages and limitations of these metrics. Specifically, the contributions of this work can be summarized as follows: (1)A systematic taxonomy on dataset diversity-control strategies is proposed, from macro-, meso- to microscopic levels on instruction and response components of SFT datasets. (2)A novel proposed approach on controlling the response diversity from the token (microscopic) level exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. (3)Comprehensive experiments are conducted involving fix-size datasets, incorporating six distinct diversity-control strategies (spanning three levels, applied to both instructions and responses). De- tailed analyses across three dataset sizes, three strategies, two components, multiple diversity metrics, are also provided. All results demonstrate that macroscopic and mesoscopic strategies effectively enhance model performance with increasing diversity, while the microscopic strategy on responses not only displays stronger performance-diversity correlation, but also achieves the optimal model performance among all strategies when diversity is maximized. 2 2 Methodology and Framework In this section, we introduce the three-level perspective of dataset diversification strategies. We first review the macro- and mesoscopic ones on the instruction component in Section 2.1. Then we present the proposed strategy of the microscopic one on the response component in Section 2.2. The illustration of previous works and our proposed method is shown in Figure 1. We eventually introduce the framework of empirical comparisons and analyses across all strategy combinations, including data source, dataset construction, model training evaluations and the diversity metrics, in Section 2.3. 2.1 Previous Works: Macro- and Mesoscopic Diversity Strategies Macroscopic analysis of dataset diversity focuses on the semantic content of texts to characterize thematic diversity. BERTopic [ 17] is one typical approach of this strategy and employs the following steps: (1) Embedding the texts into dense vector representations via any tokenizer (e.g., [ 34]); (2) Clustering embeddings to semantically related clusters (e.g., HDBSCAN [ 14,32]), where dimension- reduction techniques may be helpful (e.g., UMAP [ 33]). Then the dataset diversity scale can be controlled by choosing samples from a certain amount of clusters. Multiple approaches [ 12,16,15] can be categorized into this macroscopic strategy with various differences. For example, CaR [ 16] additionally ranks the samples within each cluster and selects the top-quality samples. MoDS [ 12] computes the embeddings and selects examples by maximizing the embedding distance. A persona-based method [ 15] has been proposed to create synthetic instructions and maximize the diversity of instructions. Mesoscopic analysis of dataset diversity focuses on decomposing an entire text into several unit tags, and then clustering the tags instead of the entire semantic content of texts. InsTag [ 30] is the typical approach, which is achieved by the two-step processing approach: (1) Generating unit tags for each text via an LLM; (2) Clustering tags into semantically coherent groups. Similar to macroscopic strategy, the dataset diversity can be also controlled by including different amounts of tags. 2.2 Our Proposed Method: The Microscopic Strategy on Responses 0 5000 10000 15000 20000 25000 Sorted token index100101102103104105CountsHigh band _a, 1, _the, es, ing than, _not, _into,...Mid band _wrap, align, _identical, _eleg _quart, phon, comput, uma _warm, _anten, _grav,...Low band _Torre, ajes _château _officiel ouwd,... Figure 2: Distribution for tokens in the SFT dataset. Based on the counts of tokens, we classify them into three categories: “High band” (more than 500 counts), “Mid band” (10-500 counts), and “Low band” (fewer than 10 counts).While macro- and mesoscopic analyses provide valuable insights into the dataset diversity, ex- isting approaches predominantly concentrate on the instruction component. Although instruc- tions act as proxies for contextual diversity, they are not an explicit signal as the LLMs are su- pervised on responses only in convention during the SFT stage. This fundamental misalignment motivates our dual innovation: (1) a paradigm shift from instruction-centric to response-driven diversity analysis, and (2) the introduction of a microscopic characterization framework oper- ating at the token granularity. Specifically, the microscopic strategy on the response component involves the following two steps: (1) Tokenizing Texts. The proposed method be- gins by tokenizing all responses (or instructions, without loss of generality) using an LLM’s tok- enizer. Token frequency, defined as the number of occurrences of a token that appears in the training samples, is then calculated. A manual inspection of tokens across the frequency spectrum leads to their classification into three bands: high, mid, and low (Figure 2). High-band tokens are typically prepositions, articles, letters, numbers, and common prefixes and suffixes, which often lack specific semantic meaning. Conversely, low-band tokens tend to be names, loanwords, or typos. Consequently, mid-band tokens carry most of the semantic meaning for the texts, and are flagged for subsequent processing. 3 (2) Controlling the Dataset Diversity at Token-Level. Controlling the diversity at the token level is more challenging than the query or tag levels. One sample may contain over 20 mid-band tokens in average, compared to one semantic cluster or four tags. This quantitative complexity leads to a non-uniform dataset distribution, causing a problem that cannot be trivially resolved by sampling samples from a predefined number of clusters or tags, as previous works do, because the number of token types cannot be controlled in such way. We therefore propose a two-stage algorithm to control the number of clusters and samples separately. Specifically, given a controlled diversity scale (the number of token types), we first prune the entire dataset to control the number of tokens using an inverse greedy pruning Algorithm 1. This algorithm yields a moderate dataset that reaches the predefined dataset diversity scale yet with some redundant samples. Then, we apply a token-aware sampling Algorithm 2 to reduce redundant samples while keeping the diversity scale. 2.3 Comprehensive Framework of Comparison Previous dataset diversity methods are designed to aim at the instruction component while they can be flexibly applied to either the instruction or response component. To systematically compare previous diversity strategies and our proposed approach, we propose a taxonomy and a framework to contain a coherent dataset construction, model training and evaluation pipeline with the dataset diversity strategy modifiable. Within the proposed framework, we conduct experiments to validate the effectiveness of our proposed approach and advance this line of research. Training Data Preparation. We compile a comprehensive dataset from a variety of open-source resources. After a rigorous cleaning process, which includes deduplication and filtering based on character encoding, a refined instructional set of 117K prompts is left. To alleviate the quality variance, we ignore the original responses and reconstruct the paired responses by prompting Llama- 3.1-70B-Nemotron model *, to ensure that individual training samples are at a similar level of quality, without forgetting that the decoding strategy still involves some variance. Dataset resources used in this work are documented in Appendix A. From the same data source, we employ a diversity strategy to construct a series of SFT datasets spanning a spectrum from minimal to maximal diversity under this strategy. We maintain fixed dataset sizes across experiments while exploring three distinct scales: 10K, 20K, and 30K samples per experimental set. All dataset construction strategies in this work culminate in a uniform selection process, resulting in datasets formally defined as: Dk=Sk i=1Π(Ci), where kis the number of topic/tag/token types controlling the diversity scale, Cirefers to the set of samples regarding to thei-th type, and Πrepresents the sampling strategy designed to achieve a uniform distribution across target types in Dk. This generates a series of datasets for each diversity strategy denoted as{Dm}m=k1,k2,···,kM, where k1andkMspecify the minimal and maximal diversity bounds, M indicates the number of datasets in the series (typically set to 7 in our experiments), and intermediate kivalues can be algorithmically determined. Model Training and Evaluations. Each series of datasets is then fine-tuned from the same pretrained model, where Llama-2-7B  is used for most of our fine-tuning experiments. For the evaluations, we adopt the pairwise scoring methodology used in Arena Hard [ 23] and combine the testing samples of both AlpacaEval 2.0 and Arena Hard benchmarks [ 24] for a more comprehensive evaluation. GPT-4 Turbo is originally used by the Arena Hard scoring system. However, due to its high cost for our extensive evaluations, we replace the judge by Llama-3.1- 70B-Nemotron [ 43], which shows a very high degree of agreement, with a reversal rate of only 8%. Additional evaluations using alternative judge models further confirm this observation. Further details regarding the evaluation setup and the consistency analysis are provided in the Appendix B. At the end of this step, we obtain the scores Swith respect to constructed datasets, i.e., {Dm,Sm}m=k1,k2,···,kM. Results on these two items across multiple diversity scales ( M), three dataset sizes (10K, 20K and 30K), three levels(macro-, meso- and microscopic) and two components (instructions and responses), as well as detailed analyses and ablation studies, will be introduced in the following two Sections. Posterior Diversity Metrics. Previous works have proposed to measure the diversity by the metrics listed in Table 1 that provide quantitative insights into different aspects in posterior, while some of *It is the best open-source model on lmarena.ai at the time when starting this work. 4 Table 1: Posterior diversity metrics. The upward-pointing arrow denotes a positive correlation between the metric value and dataset diversity, while the downward-pointing arrow indicates an inverse relationship (negative correlation). Metric Equation Explanation N-gram Ratio (NR) ↑ Rn=#Unique n-grams #Totaln-gramsThe ratio of unique n-grams to total n- grams, serving as a measure of lexical di- versity. Embedding Distance (ED) ↑Davg=1 NP a̸=b∥va−vb∥The average distance between embed- dings [ 3], where vis the embedding of an element. Sequence Length (SL) ↑ Lseq=1 NPN i=1liThe mean number of tokens over se- quences. Compression Ratio (CR) ↑ Cratio=Original size Compressed sizeThe ratio of original dataset size to com- pressed size. Self-BLEU (BL) ↓ SBLEU =1 NPN i=1BLEU −iThe average BLEU −iscore where each textiis compared against the rest of the dataset ( −i). Information Entropy (IE) ↑ EEntro=−Pn i=1pilog(pi)The measure of the randomness of the dis- tribution [ 4], where piis the probability/fre- quency of a token. them are used or can be optimized when constructing datasets. Moreover, we would like to mention the metric of the information entropy [ 4] that may be a reasonable metric. Detailed discussions are provided in Section 4.4. 3 Main Results In this section, we conduct the comparative experiments as introduced in the previous section, to compare the tuples of datasets and the corresponding scores. To briefly recall the framework, there are three aspects that experiments will be conducted for the comparison: (1) Across multiple diversity scales. For a certain size of datasets (e.g., 10K), we vary the diversity scale from the lowest to the highest that the strategy can achieve. This range of diversity defines the bounds, where we manually set the lowest as 0% and the highest as 100%. The values in-between represent different diversity scales. This definition makes it possible to compare across strategies. (2) Across three dataset sizes. We choose three sizes of constructed datasets, i.e., 10K, 20K and 30K from a corpus of 117K instructions and refreshed responses. (3) Across three strategic levels and two components. We apply macro-, meso- and microscopic dataset diversity strategies on instructions and responses, to perform a comprehensive comparison. At the end of this section, we furthermore test the effectiveness of the listed posterior diversity metrics for measuring the correlation between diversity and performance. 3.1 Comparative Results We show the main results in Figure 3, where the observations and findings are discussed below. Note that as the pairwise scoring system requires a reference baseline, a random 10K dataset is used for all experiments in this section. An equal model scores 50 according to this scoring system, so dashed gray lines are drawn at the score of 50. 3.1.1 Dataset Sizes Our experiments with constrained dataset sizes (10K, 20K and 30K training samples) reveal a consistent performance trend where models trained on larger datasets generally outperform those using smaller training datasets. While we hypothesize that diminishing returns or performance plateaus might emerge at greater dataset magnitudes, systematic investigation of this phenomenon remains beyond the scope of the current study. 5 0 25 50 75 100 Diversity Percentage4550556065ScoreInstruction 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageInstruction 20k 0 25 50 75 100 Diversity PercentageBaselineInstruction 30k 0 25 50 75 100 Diversity Percentage4550556065ScoreResponse 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageResponse 20k 0 25 50 75 100 Diversity PercentageBaselineResponse 30kFigure 3: The relationship between the diversity percentage and model performance for instructions and responses across three dataset sizes (10K, 20K, and 30K). The vertical axis shows the average score from two benchmarks, with a baseline score of 50, derived based on a randomly selected 10K dataset. In each subplot, different colors correspond to three diversity levels: Macro (semantics), Meso (tag), and Micro (token). The markers represent the data, while the dashed lines and shaded areas indicate the results of Bayesian linear regression with 1-σuncertainty. Nevertheless, increased dataset diversity helps mitigate the performance gap associated with varying dataset sizes. Notably, some smaller yet more diverse datasets curated on the response component at the token level achieve superior model performance compared to larger yet less diverse datasets. This finding underscores the importance and effectiveness of dataset diversity during dataset construction. 3.1.2 Macro-to-Micro Diversity Strategies Combined with the statistic results in Table 2 which quantify the slopes of lines in the semantic diversities and scores of fine-tuned LLM models. This aligns with the hypothesis that instruction sets encompassing broader semantic Figure 3, we observe that the macroscopic strategy (semantics level) on instructions show positive correlations between ranges enhance a model’s generalization capability. A similar yet weaker correlation is observed for the response component. However, compared to another two strategies, the range of performance scores that the macroscopic strategy can achieve is narrow, suggesting that the macroscopic strategy is less effective than finer-grained strategies. Table 2: Slope ( ×10−2) of the linear regression fit in Figure 3 for each diversity-control strategy. Dataset Instruction Response Size Macro Meso Micro Macro Meso Micro 10K 2.37 4.93 −5.45 0.73 3.61 5.35 20K 1.67 3.75 −2.83 0.97 1.71 6.68 30K 1.45 2.07 −1.80 0.59 2.00 10.06Results of the mesoscopic strategy (tag level) show consistently positive correlation between the diversity scales and scores, on both instruc- tions and responses. The mesoscopic strat- egy shows larger slopes than the macroscopic, demonstrating that decomposing the entire in- struction into functional attributes of the text, such as topics, intents, sentiments, areas etcis more effective. The proposed microscopic strategy (token-level) demonstrates limited efficacy when applied to instructions but exhibits the most significant impact on responses compared to the previous two strategies. This differential performance aligns with expectations. The effectiveness on responses stems from the fact that LLMs are explicitly supervised on response tokens during the SFT stage. Since LLMs generate outputs autoregressively (token-by-token), introducing token diversity likely 6 mitigates overfitting by forcing the model to generalize across varied tokens, making the training more robust and generalizable. Conversely, the strategy’s failure on instructions is intuitively explainable. First, token-level diversity in instructions does not inherently translate to meaningful semantic diversity. A microscopic focus on tokens risks overlooking broader contextual and semantic relationships. Second, we hypothesize that successful LLM alignment depends on consistent exposure to recurring token patterns in instructions. Reducing the frequency of specific tokens (via diversification) may dilute critical alignment signals, weakening the model’s ability to internalize task-specific linguistic or behavioral norms. 3.2 Tests of Diversity Metrics 1-NR 2-NR ED SL CR BL IEScore-0.36 -0.14 0.11 -0.21 0.10 0.42 0.04 1-NR 2-NR ED SL CR BL IEScore-0.47 -0.41 -0.44 0.73 0.10 -0.02 0.58 −1.0−0.50.00.51.0 Instruction Response Figure 4: Pearson correlation coefficients of multiple di- versity parameters and model performance scores based on all the involved 10K datasets. The upper and lower columns represent the diversity measurements from the perspectives of instruction and response, respectively. The intersection of two parameters shows their Pearson correlation coefficients.Most of the investigated metrics, as de- tailed in Section 2.3 and Table 1, cannot be easily optimized during the dataset con- struction, and thus require to be tested in a posterior way. For experiments per dataset size, we compute the metric values and measure the correlation to performance scores. Figure 4 shows the results. Note that the metrics are applied to the instruc- tion and the response respectively. For the instruction, the most correlated met- ric is Self-BLEU (0.42). This coheres with the macroscopic strategy on instructions. Both would like to reserve diverse instruc- tions, while the macroscopic applies the se- mantically clustering way and Self-BLEU measures the semantic similarities. However, as noticed previously, the macroscopic strategy is less effective than meso- and microscopic strategies. Turning to response-level metrics, sequence length (0.73) and information entropy (0.58) exhibit the strongest correlations with performance scores. The high correlation of sequence length aligns with prior findings that prioritizing longer responses serves as an effective heuristic for enhancing model performance in SFT datasets [ 50]. To investigate whether this correlation reflects direct causation, we conduct ablation experiments in Section 4.2 using the proposed microscopic strategy on responses filtering with strict length constraints. By maintaining near-identical lengths across compared datasets, the ablation study isolates length as a variable to assess its true impact on model outcomes. The information entropy metric relates to the core mechanism of our microscopic strategy, with the slight difference that the strategy optimizes the entropy over the set of mid-band tokens and the metric computes over the whole vocabulary set. Moreover, the strong correlation with information entropy implies the effectiveness of the microscopic strategy. The entropy measures the token distribution. When it is computed on responses, it measures the distribution of training signals. High entropy means diverse tokens and can help the LLM to be less prone to the overfitting, making the training more robust and generalizable. Detailed entropy analyses are provided in Appendix E. 4 Ablation Studies Main experimental results indicate that macroscopic and mesoscopic strategies effectively lead to higher performance with increasing diversity, whereas the microscopic strategy on response exhibits both a stronger correlation between model performance and diversity, and superior performance with maximum diversity across all strategies. To further investigate the mechanisms of the proposed strategy, we conduct three ablation studies in this section. 4.1 Model Building on our observations with Llama-2-7B, we conduct a cross-model analysis to examine whether the performance advantages of microscopic response diversity generalize across differ- ent model scales and architectures. We select two contrasting models for this investigation: 7 the larger Llama-2-70B and the more recent Llama-3-8B. For each architecture, we construct two series of datasets of 10K samples through the microscopic strategy on response using their respective tokenizers, and train the models on these datasets. To enable meaningful compari- son, we establish two baselines with either model respectively using randomly sampled datasets of equivalent size (10K samples) without diversity optimization and conduct the comparison. 0 50 100 Diversity Percentage455055Score Llama2 70B Baseline 0 50 100 Diversity PercentageLlama3 8B Baseline Figure 5: Ablation study on microscopic diver- sity: comparing different models (Llama-2-70B and Llama-3-8B). The abscissa indicates the micro- scopic level diversity percentage from the response perspective in the case of 10K dataset.As evidenced in Figure 5, both Llama-2-70B and Llama-3-8B mirror the performance pat- tern of Llama-2-7B. More strikingly, Llama 3- 8B demonstrates an amplified correlation slope, suggesting heightened sensitivity to token-level diversity variations. While future works are required to investigate the deep reasons, we posit this difference stems from Llama 3’s imple- mentation of the tiktoken tokenizer, which en- larges the vocabulary size from Llama-2’s 32K to 128K and the covering range of mid band to- kens. Overall, these results confirm a robust and consistent positive correlation across different model sizes and architectures. 4.2 Length Control As discussed in Section 3.2, response length exhibits a strong correlation with model performance scores. To investigate whether this relationship is causal, we conducted experiments controlling the response length to be identical. This ablation study uses the microscopic strategy on responses. Specially, we construct length-controlled datasets by sampling from sub-datasets of varying lengths and selecting samples within a similar length range. Models are subsequently fine-tuned on these datasets and evaluated across diversity scales as presented in the proposed framework. Figure 6 (a) reveals that while we restricted response lengths to approximately 500 tokens across all datasets, the relationship between diversity scale and model performance remains unchanged compared to the uncontrolled baseline. This persistent correlation suggests that response length may serve as an incidental covariate rather than the primary factor driving performance improvements. 4.3 Tokenizer 45505560Score BaselineMicro Micro (length control) 0 50 100 Diversity Percentage500800Length (a) Length Control 0 50 100 Diversity Percentage45505560Score BaselineMicro (token) Micro (word) (b) Tokenizer Comparison Figure 6: Ablation study on microscopic diversity: Figure (a) compares the performance under length control. The subplot below illustrates the average token length of responses across the datasets. Figure (b) compares word-based tokenization. The abscissa indicates the microscopic level diversity per- centage from the response perspective in the 10K datasets.Given the crucial role of tokenization in the microscopic strategy, we investi- gate its influence by comparing the de- fault tokenization scheme with word segmentation. The default tokeniza- tion aligns with the LLM to be fine- tuned while the word segmentation is a common approach that can be used for any LLM without any pre- processing step. We thus only replace the tokenize step by the word segmen- tation in the microscopic strategy, and construct a new series of datasets (of 10K samples) to train the model. As shown in Figure 6 (b), word-based tokenization does not exhibit a clear positive correlation with model per- formance. This suggests that the tokenization scheme provided by the tokenizer is essential for preserving microscopic diversity, as it segments information in a way that matches input the model received during training. Word-based segmentation may disrupt this correspondence, leading to suboptimal performance. 8 4.4 Additional Ablation Studies and Discussions Three additional studies are provided in Appendix C to further confirm our findings in various settings. (1) With original responses (which are of lower data quality), (2) On more benchmarks (MMLU [ 18] and LiveBench [ 45]) and (3) with mismatched tokenizer to control the microscopic dataset diversity. While prior research has predominantly emphasized the instruction diversity that implicitly leads to diverse responses and consequently diverse token distributions, our comprehensive experiments reveal a more impactful approach. Through rigorous comparative analysis, diversity metric evaluations, and ablation studies, we conclude that the microscopic strategy on response is a more effective and direct way. We therefore encourage the research community to prioritize the response diversification alongside the instruction for SFT dataset construction. 5 Related Work 5.1 SFT Diversity Measure Approach Dataset diversity plays a crucial role in the SFT stage for LLM development. Research in this area can be broadly categorized into two main approaches: Quantitative Feature Approach. Numerous studies explored fundamental dataset features to eval- uate their correlation with language model performance. Commonly examined features include distinct-n [ 22], quantifying unique n-grams proportion as a dataset diversity measure; gzip com- pression ratio [ 38], indicating data redundancy; ROUGE-L score [ 26], measuring sequence overlap; and Self-BLEU [ 54], assessing the internal diversity of text samples. While these features provide intuitive quantitative metrics for dataset diversity, they are often applied in relatively simple ways, limiting their capacity to capture deeper contextual relationships within the data. Semantic Representation Approach. Advances in language models [ 40] have made analyzing dataset semantic structures a critical focus, prompting methods to encode textual information into high- dimensional representations. Bidirectional Encoder Representations from Transformers (BERT) [ 11] model has been widely adopted for this purpose. For instance, embeddings derived from BERT have been leveraged to assess dataset diversity by quantifying various semantic properties, including the distance of embedding vectors [ 37,28,41,29] and the identification of semantically coherent clusters of instructions using clustering techniques [ 17,16]. These approaches offer valuable insights into the semantic diversity present within datasets. The rapid development of generative autoregressive models [ 48,5,2] has shifted research towards leveraging these models for both semantic and diversity analysis. LLMs are now increasingly employed to extract richer semantic features, such as generating descriptive tags for instructional data [ 30,47,13] and identifying clustering patterns through LLM-based agents [ 8]. This offers a complementary approach to traditional methods for assessing dataset diversity by leveraging the nuanced semantic understanding provided by these models. 5.2 Token-Level Analysis in Large Language Models Token-level analysis has been studied in LLMs, with numerous approaches exploring the relationship between tokens and various aspects of LLM training and performance [ 31,52,25,21,27]. For instance, Lin et al. [ 27] focus on training strategies that prioritize important tokens to enhance model efficiency. Madsen et al. [ 31] have observed variations in training time and gradient descent loss associated with processing different tokens, using these metrics to infer token importance or influence. Li et al. [ 25] have explored the use of token matching techniques to improve training efficiency and performance. While these studies provide valuable insights into token-level effects on training, the analysis of token diversity within the context of SFT datasets remains relatively unexplored. 6 Conclusion In this study, we first propose a taxonomy to categorize the approaches for dataset diversity according to the grain level (macro-, meso- and microscopic) and the component of SFT datasets (instructions and responses). Through extensive experiments, we systematically compare their impact on model performance and demonstrate that our proposed microscopic strategy on responses exhibits the 9 strongest correlation between model performance and diversity degree while achieving superior performance at maximum diversity compared to other strategies. Detailed analyses and ablation studies confirm the effectiveness of the proposed microscopic strategy on responses. We have also tested multiple diversity metrics and suggest that the information entropy may be a good estimator of dataset diversity, which also aligns with the optimization direction of the microscopic strategy. We therefore call upon the research community to prioritize the response diversification alongside the instruction for SFT dataset construction. References H. Abusamra. A comparative study of feature selection and classification methods for gene expression data of glioma. Procedia Computer Science, 23:5–14, 2013. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. S. Arora, Y. Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. In Proceedings of the International Conference on Learning Representations, 2017. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020. S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. A. Bukharin, S. Li, Z. Wang, J. Yang, B. Yin, X. Li, C. Zhang, T. Zhao, and H. Jiang. Data diversity matters for robust instruction tuning. In Findings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3411–3425, 2024. H. Chen, A. Waheed, X. Li, Y. Wang, J. Wang, B. Raj, and M. I. Abdin. On the diversity of synthetic data and its impact on training large language models. arXiv preprint arXiv:2410.15226, 2024. M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023. T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2023. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171–4186, 2019. Q. Du, C. Zong, and J. Zhang. Mods: Model-oriented data selection for instruction tuning. arXiv preprint arXiv:2311.15653, 2023. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, volume 96, pages 226–231, 1996. T. Ge, X. Chan, X. Wang, D. Yu, H. Mi, and D. Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Y. Ge, Y. Liu, C. Hu, W. Meng, S. Tao, X. Zhao, M. Xia, Z. Li, B. Chen, H. Yang, B. Li, T. Xiao, and J. Zhu. Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 464–478, 2024. M. Grootendorst. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794, 2022. 10  D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring mas- sive multitask language understanding. In Proceedings of the International Conference on Learning Representations, 2021. E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low- rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations, 2022. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. S. Land and M. Bartolo. Fishing for magikarp: Automatically detecting under-trained tokens in large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11631–11646, 2024. J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, 2016. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_ eval, 2023. Z. Li, C. Chen, T. Xu, Z. Qin, J. Xiao, R. Sun, and Z.-Q. Luo. Entropic distribution matching for supervised fine-tuning of LLMs: Less overfitting and better diversity. In Proceedings of the 38th Annual Conference on Neural Information Processing Systems Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability, 2024. C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summariza- tion Branches Out, pages 74–81, July 2004. Z. Lin, Z. Gou, Y. Gong, X. Liu, yelong shen, R. Xu, C. Lin, Y. Yang, J. Jiao, N. Duan, and W. Chen. Not all tokens are what you need for pretraining. In Proceedings of the thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. W. Liu, W. Zeng, K. He, Y. Jiang, and J. He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. In Proceedings of the International Conference on Learning Representations, 2024. Z. Liu, A. Karbasi, and T. Rekatsinas. TSDS: Data selection for task-specific model finetuning. In Proceedings of the Annual Conference on Neural Information Processing Systems, 2024. K. Lu, H. Yuan, Z. Yuan, R. Lin, J. Lin, C. Tan, C. Zhou, and J. Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In Proceedings of the International Conference on Learning Representations, 2023. A. Madsen, N. Meade, V. Adlakha, and S. Reddy. Evaluating the faithfulness of importance measures in nlp by recursively masking allegedly important tokens and retraining. In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1731–1751, 2022. L. McInnes, J. Healy, and S. Astels. hdbscan: Hierarchical density based clustering. Journal of Open Source Software, 2(11):205, 2017. L. McInnes, J. Healy, N. Saul, and L. Großberger. Umap: Uniform manifold approximation and projection. Journal of Open Source Software, 3(29):861, 2018. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems, 26, 2013. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 2022. V. Padmakumar and H. He. Does writing with language models reduce content diversity? In Proceedings of the International Conference on Learning Representations, 2024. 11  C. Shaib, J. Barrow, J. Sun, A. F. Siu, B. C. Wallace, and A. Nenkova. Standardizing the measurement of text diversity: A tool and a comparative analysis of scores. arXiv preprint arXiv:2403.00553, 2024. F. Song, B. Yu, H. Lang, H. Yu, F. Huang, H. Wang, and Y. Li. Scaling data diversity for fine-tuning language models in human alignment. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, pages 14358–14369, 2024. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. P. Wang, Y. Shen, Z. Guo, M. Stallone, Y. Kim, P. Golland, and R. Panda. Diversity measurement and subset selection for instruction tuning datasets. arXiv preprint arXiv:2402.02318, 2024. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 13484–13508, 2023. Z. Wang, A. Bukharin, O. Delalleau, D. Egert, G. Shen, J. Zeng, O. Kuchaiev, and Y. Dong. Helpsteer2- preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain, R. Shwartz-Ziv, N. Jain, K. Saifullah, S. Dey, Shubh-Agrawal, S. S. Sandha, S. V. Naidu, C. Hegde, Y. LeCun, T. Goldstein, W. Neiswanger, and M. Goldblum. Livebench: A challenging, contamination-free LLM benchmark. In Proceedings of the International Conference on Learning Representations, 2025. W. F. Wiggins and A. S. Tejani. On the opportunities and risks of foundation models for natural language processing in radiology. Radiology: Artificial Intelligence, 4(4):e220119, 2022. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan. DIALOGPT: Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, July 2020. D. Zhao, J. Andrews, O. Papakyriakopoulos, and A. Xiang. Position: Measure dataset diversity, don’t just claim it. In Proceedings of the 41st International Conference on Machine Learning, pages 60644–60673, 2024. H. Zhao, M. Andriushchenko, F. Croce, and N. Flammarion. Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning. In Proceedings of the 41st International Conference on Machine Learning, 2024. Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2024. Q. Zhong, L. Ding, J. Liu, X. Liu, M. Zhang, B. Du, and D. Tao. Revisiting token dropping strategy in efficient bert pretraining. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 10391–10405, 2023. C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2023. Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu. Texygen: A benchmarking platform for text generation models. In Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 1097–1100, 2018. 12 A Collection of Open-Source SFT Datasets For transparency and reproducibility, we disclose the publicly available SFT datasets used in our work, including: Alpaca: The Alpaca dataset consists of 52K instruction-response samples generated by text- davinci-003 using the Self-Instruct method, which is used to fine-tune Llama. ShareGPT†: The ShareGPT dataset consists of conversations scraped from the ShareGPT plat- form before its discontinuation. These conversations contain both user prompts and corresponding responses from ChatGPT, offering a diverse range of conversational contexts. Dolly: The Dolly dataset, released by Databricks as part of the Dolly project, is an open-source dataset of human-generated instruction-following examples, licensed for research and commercial use, and used to fine-tune Dolly2.0 models. LIMA: The LIMA dataset contains 1,000 carefully curated instructions from diverse domains, focusing on high-quality prompts designed to elicit specific behaviors from language models. Note that we reserve only the instructions from these datasets and generate the responses using Llama-3.1-70B-Nemotron model. B Consistency Analysis of Judge Models We evaluate the judge agreement between Nemotron and GPT-4 Turbo through a comparative analysis of 500 pairwise comparisons from the Arena Hard benchmark. To minimize positional bias, the judge model evaluates each pair of responses twice, reversing the order of presentation in the second evaluation. Pairwise scores are calculated according to the criteria outlined in Table 3. These two scores are summed and then categorized into one of three categories: Good, Same, or Bad. Subsequently, we compare the judgment consistency of the models against that of GPT-4 Turbo. The results reveal that the Llama-3.1-70B-Nemotron model [ 43] demonstrates strong agreement with GPT-4 Turbo, with a reversal rate of only 8%. The pairwise evaluation prompt is shown in Appendix H.1. Table 3: Model judgment score compute rules. Score 1 0.5 0 -0.5 -1 Judge1 A»B A>B A=B B>A B»A Judge2 B»A B>A A=B A>B A»B In spite of this, we conduct additional experiments to assess the robustness of our conclusions and mitigate concerns regarding model-specific biases. Specifically, we evaluate the 10K micro-level response set using two structurally distinct judge models: Qwen2.5-72B-Instruct and GPT-4o. As shown in Table 4, the evaluation scores obtained from these alternative judges exhibit consistent trends, reinforcing that the observed effects are not dependent on a particular evaluation model. Table 4: Evaluation scores under different judge models across micro-level diversity. Micro Diversity (%) 0 33.3 66.7 100 GPT-4o 48.4 51.8 52.8 53.1 Qwen2.5-72B-Instruct 49.0 54.1 55.4 57.4 C Additional Ablation Studies In addition to the ablation analyses presented in Section 4, we include several supplementary experiments that are crucial for understanding the overall findings. †The original ShareGPT dataset ( https://sharegpt.com/ ) has not been officially released. Therefore, we utilize a reproduced version available on the Hugging Face ( https://huggingface.co/datasets/ anon8231489123/ShareGPT_Vicuna_unfiltered ), applying a cleaning preprocessing step. 13 C.1 Performance on Origin Response To verify the rationale for response replacement, we evaluated our strategy’s effectiveness using the original responses. Specifically, we performed supplementary experiments by applying our micro-level diversity strategy to these baseline responses.The results in Table 5 consistently show that response diversity positively correlates with model performance when compared with the original 10K response baseline. We also find that the original responses group showed significantly lower scores in the nemotron 10K baseline in preliminary comparisons. Table 5: Performance with different levels of micro-level diversity using original responses. The baseline derived from random 10K original response samples. Micro Diversity (%) 0 16.7 33.3 50 66.7 83.3 100 Arena-Hard Score 37.6 43.8 48.5 50.8 55.1 59.4 61.4 AlpacaEval 2.0 Score 43.3 49.1 57.2 61.5 64.9 66.4 64.5 C.2 Performance on Other Benchmarks Given that the evaluation in our study relies on LLM-based judgments, we further assess model performance using alternative benchmarks—MMLU and LiveBench—which employ objective, non- LLM-based evaluation metrics. We conduct this evaluation using the microscopic strategy on response 10K datasets with the Llama-3-8B model. As shown in Table 6, the results significantly indicate that greater response diversity leads to improved model performance across both benchmarks. Table 6: Performance on MMLU and LiveBench across different levels of micro-level diversity. Micro Diversity (%) 0 16.7 33.3 50 66.7 83.3 100 MMLU Score 54.96 52.62 54.42 52.27 56.31 56.50 55.68 LiveBench Score 12.14 13.27 13.13 12.28 14.55 13.75 14.47 C.3 Robustness under Tokenizer Mismatch To further assess the robustness of our method under variations in tokenization, we perform an additional experiment summarized in Table 7. In this setting, diversity selection is conducted using the Llama-2 tokenizer, while model training is carried out with Llama-3-8B. Notably, these tokenizers differ significantly in both vocabulary and tokenization schemes. While the performance stability does decline under this mismatch—as expected—our method continues to yield notable improvements, indicating its resilience to tokenizer variation. Nevertheless, we acknowledge that optimal results are achieved when the tokenizer and model are well aligned. Table 7: Model performance under tokenizer mismatch: Llama-2’s tokenizer used for data strategy and Llama-3-8B for training. Micro Diversity (%) 0 16.7 33.3 50 66.7 83.3 100 Arena-Hard Score 45.0 51.1 45.9 52.6 54.1 56.9 58.5 AlpacaEval 2.0 Score 41.6 50.7 45.5 58.1 51.4 61.6 53.8 D Experimental Parameters and Setup To facilitate reproducibility, we list the parameters and provide details for the three levels of diversity strategy applied to the datasets, as well as the training settings used in the SFT training process. D.1 Macroscopic Parameters At the macroscopic scale, we utilize the all-MiniLM-L12-v2 model for embeddings. The UMAP parameters are configured with neighbors=15 and components=5, and HDBSCAN clustering is performed using the Euclidean distance metric with a minimum cluster size of 20. This process results in approximately 2,000 semantic clusters for both instructions and responses. To construct 14 the dataset, we sample instruction-response pairs from a fixed number of clusters to ensure balanced representation across clusters. The macroscopic diversity is defined as the number of selected clusters. D.2 Mesoscopic Parameters At the mesoscopic scale, we employ Llama-3.1-70B-Nemotron [ 43] for tagging, with prompt details provided in Appendix H.2. We then perform a consistency check using Qwen2.5-72B-Instruct [ 47]. Following this, we filter out non-words and merge synonymous terms. The resulting tags are clustered using HDBSCAN with eps=0.15 and min samples=2, yielding approximately 5,000 tag clusters for both instructions and responses. These tags are subsequently normalized by their respective cluster center tags. The mesoscopic diversity is defined as the ratio of distinct tag categories to the total number of tags. D.3 Microscopic Parameters At the microscopic scale, tokens are extracted using the tokenizer from the training model. Tokens that appear with frequencies between 10 and 500 are defined as “important tokens”, as they are assumed to have a significant impact on the dataset distribution. To systematically investigate how token diversity affects model behavior, our experiment employs two sequential stages: The preparatory Algorithm 1 constructs distinct diversity subsets through inverse greedy pruning, followed by the core sampling process in Algorithm 2 that selects data based on the distribution patterns of these important tokens. Microscopic diversity is quantified as the proportion of unique important tokens relative to the total important token types. Algorithm 1 Inverse Greedy Pruning 1:Input: Dataset D={d1,..., d n}where each dihas token set Tdi, Target token count K 2:Output: Pruned subset Dsub, Temporary tokens Ttmp 3:Dsub← D {Initialize with full dataset} 4:Ttmp←S d∈DTd{Initial token coverage} 5:while|Ttmp|> K andDsub̸=∅do 6: Calculate Unique (d) = Td\\S d′̸=dTd′,∀d∈ D sub 7: Select d∗←arg max Unique (d) 8: Remove sample: Dsub← D sub\\ {d∗} 9: Update coverage: Ttmp←S d∈D subTd 10:end while 11:return Dsub,Ttmp Algorithm 2 Integrated Token-Aware Sampling 1:Input: Subset Dsub, Candidate tokens Ttmp, Target size N, Trade-off α >0, Batch size B 2:Output: Final sample S 3:Initialize S ← ∅,R ← D sub,Counts [t]←0,∀t∈Ttmp 4:while|S|< N ANDR ̸=∅do 5: if∃t∈Ttmpwith Counts [t] = 0 then 6: Findd∗∈ R maximizing new coverage: 7: d∗←arg max |Td∩ {t|Counts [t] = 0}|,∀d∈ R 8: else 9: Compute scores: s(d)←P t∈Td1 Counts [t]+α,∀d∈ R 10: SortRdescending by s(d) 11: Batch Size k←min(B, N− |S| ) 12: Select top- kdocuments d∗ 1,..., d∗ kfrom sorted R 13: end if 14: foreachd∗do 15: S ← S ∪ { d∗} 16: R ← R \\ { d∗} 17: Counts [t]←Counts [t] + 1,t∈Td∗ 18: end for 19:end while 20:return S 15 D.4 Training Setting We conduct full-parameter fine-tuning via Llama-Factory [ 51] on 16 ×NVIDIA A800 80GB GPUs. To optimize computational efficiency, sequence packing is enabled and training sequences are then truncated to a maximum length of 4,096 tokens. For the learning rate schedule, we implement a cosine decay strategy starting at 5×10−6, with a 5% warmup ratio to ensure stable initial training and avoid abrupt parameter updates. The model is fine-tuned over 4 epochs with a batch size of 16, striking a balance between computational efficiency and performance. Training a 7B model on 10K datasets requires approximately 20 minutes under this configuration. E Correlation Analysis of Token Distribution Statistics and Model Performance Diversity-control strategies at the microscopic level actually alter the distribution of tokens. In the section, we explore the correlation between multiple statistics related to token distributions and model performance. Figure 7 demonstrates the token distribution with different microscopic diversity percentage (0%, 50%, and 100%) in the case of 10k dataset. Based on the distribution, we can explore various statistics related to token distributions to identify micro-level diversity parameters that might have a stronger correlation with model performance. We investigate the correlation between the IE (information entropy) [ 4], kurtosis, and Gini index [ 1] based on token distributions and model performance from both the instruction and response perspectives. Information entropy quantifies how spread out the probabilities of different outcomes are, making it a statistical metric for evaluating diversity, Information entropy = −nX i=1pilog(pi). (1) Kurtosis describes the shape of a distribution, specifically its tailedness and the sharpness of its peak. It reads, Kurtosis =\" 1 nnX i=1\u0012xi−µ σ\u00134# −3. (2) The Gini index measures the inequality in a distribution. It reflects how evenly the probabilities are distributed among the tokens and is often used to evaluate imbalances, Gini index = 1 −nX i=1p2 i, (3) where piis the probability of i-th token, xiis the sorted index of i-th token. µandσare the mean value and the standard deviation of the sorted indexes of tokens. The Pearson correlation coefficients between performance scores and these statistic are shown in Table 8. Among these, IE from response perspective demonstrates the highest correlation. For the three statistics, the model performance score shows a positive correlation with IE, while exhibiting negative correlations with kurtosis and Gini index. All three metrics consistently indicate that flatter token distributions are beneficial for SFT of language models, confirming the feasibility of measuring diversity from the token level. Table 8: Pearson correlation coefficients between the model performance score and various statistics based on token distributions. Dataset Instruction Response Size IE −Kurtosis −Gini index IE −Kurtosis −Gini index 10K 0.04 0.29 0.01 0.58 0.58 0.42 20K 0.13 0.52 0.11 0.62 0.43 0.50 30K 0.25 0.68 0.28 0.76 0.32 0.69 16 10000 20000 Sorted token index012345Counts×105 Micro diversity 0% 10000 20000 Sorted token indexMicro diversity 50% 10000 20000 Sorted token indexMicro diversity 100%Figure 7: Token distributions from the response perspective under different microscopic diversity percentages in the case of 10k dataset. F Limitation and Future Works Our study has provided initial insights into the role of dataset diversity in Language Model perfor- mance during Supervised Fine-Tuning (SFT), but several limitations and open questions remain. The scope of our experiments was constrained by the scale of available datasets and computational resources, limiting our analysis to SFT applications. Additionally, the relationship between dataset diversity and model performance, particularly in terms of entropy, requires more interpretable and comprehensive analysis to fully understand the underlying mechanisms. Mixture Strategy of Instruction and Response Diversity. A strategy that combines instruction and response diversity may yield more comprehensive outcomes. Our preliminary analysis of three strategies revealed minimal correlation between instruction-response dynamics, suggesting that a mixture strategy could enhance performance and clarify diversity patterns. This is also a direction we are considering for future exploration. Entropy and Dataset Diversity. Exploring the relationship between entropy and dataset diversity may lead to more comprehensive insights. While our findings suggest a potential link between language model performance and the entropy of SFT datasets, a more robust and interpretable analysis is needed to solidify these observations and provide deeper insights. Generalization to Alternative Fine-Tuning Paradigms While our current validation focuses on supervised learning frameworks, the possible relevance of our diversity-aware methodology to Reinforcement Learning environments warrants further investigation. Preliminary observations suggest that similar diversity control mechanisms could potentially be adapted for RL data sampling strategies, though this hypothesis requires systematic verification through dedicated studies. G Raw Data of the Experiments In this section, we present the experimental data for the two benchmarks shown in Figure 3, as well as the detailed correlation analysis data for Figure 4. G.1 Scores of each Benchmark In Figure 8, we present the experimental results separately for Arena Hard and AlpacaEval 2.0, providing a clear comparison of the scores across these two benchmarks. 17 0 25 50 75 100 Diversity Percentage4550556065AlpacaEval 2.0Instruction 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageInstruction 20k 0 25 50 75 100 Diversity PercentageBaselineInstruction 30k 0 25 50 75 100 Diversity Percentage4550556065AlpacaEval 2.0Response 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageResponse 20k 0 25 50 75 100 Diversity PercentageBaselineResponse 30k 0 25 50 75 100 Diversity Percentage4550556065Arena HardInstruction 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageInstruction 20k 0 25 50 75 100 Diversity PercentageBaselineInstruction 30k 0 25 50 75 100 Diversity Percentage4550556065Arena HardResponse 10k Macro (semantics) Meso (tag) Micro (token) 0 25 50 75 100 Diversity PercentageResponse 20k 0 25 50 75 100 Diversity PercentageBaselineResponse 30kFigure 8: The relationship between the diversity percentage and model performance for instructions and responses across three dataset sizes (10K, 20K, and 30K) in Arena Hard and AlpacaEval 2.0 benchmark. The settings and styles are the same as those in Figure 3. 18 G.2 Correlation Analysis in Each Diversity-control Strategy Chapter 3.2 discusses the correlation between multiple diversity metrics and model performance across all 10K datasets. Here, we present the correlation figures for each diversity strategy, further categorized by instructions and responses, based on their corresponding 10K datasets. 1-NR 2-NR ED SL CR BL IEScore-0.63 -0.40 0.45 0.53 -0.29 0.30 0.52 1-NR 2-NR ED SL CR BL IEScore0.33 0.26 0.37 0.47 -0.29 0.58 0.38 −1.0−0.50.00.51.0 Instruction Response 1-NR 2-NR ED SL CR BL IEScore-0.02 0.01 0.28 0.09 -0.41 0.16 -0.01 1-NR 2-NR ED SL CR BL IEScore0.39 0.37 0.35 0.19 -0.41 0.13 0.20 −1.0−0.50.00.51.0 Instruction Response 1-NR 2-NR ED SL CR BL IEScore-0.77 -0.70 -0.26 0.75 -0.90 0.60 0.75 1-NR 2-NR ED SL CR BL IEScore-0.08 -0.82 -0.82 0.90 -0.90 0.88 0.90 −1.0−0.50.00.51.0 Instruction Response 1-NR 2-NR ED SL CR BL IEScore-0.48 0.18 -0.10 0.46 -0.82 0.71 0.58 1-NR 2-NR ED SL CR BL IEScore-0.42 0.79 -0.51 0.56 -0.82 0.88 0.83 −1.0−0.50.00.51.0 Instruction Response 1-NR 2-NR ED SL CR BL IEScore-0.71 0.93 -0.18 -0.98 0.71 0.94 -0.93 1-NR 2-NR ED SL CR BL IEScore-0.81 -0.79 -0.36 0.66 0.71 -0.71 -0.85 −1.0−0.50.00.51.0 Instruction Response 1-NR 2-NR ED SL CR BL IEScore-0.77 -0.57 0.92 0.66 -0.10 0.72 0.77 1-NR 2-NR ED SL CR BL IEScore0.34 0.50 0.89 0.76 -0.10 0.89 0.79 −1.0−0.50.00.51.0 Instruction Response Figure 9: Pearson correlation coefficients of multiple diversity parameters and model performance scores in the case of 10K dataset from different diversity-control strategies. The left panel shows the correlation analysis results based on the dataset constructed with instruction-based diversity-control strategies, while the right shows those from the response perspective. From top to bottom, the diversity-control strategies are at the macroscopic, mesoscopic, and microscopic levels, respectively. The settings and styles are the same as those in Figure 4. 19 H Prompt Template In this section, we present the prompt templates used for both the benchmark pairwise judgment task and the mesoscopic diversity strategy. H.1 Evaluate Prompt In model evaluation, we adopt the pairwise methodology from Arena Hard [ 23] and evaluate on both the Arena Hard and AlpacaEval 2.0 [ 24] benchmarks. Below is the pairwise judgment prompt template. Pairwise Judge Prompt Template system_prompt: \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A’s answer and assistant B’s answer. Your job is to evaluate which assistant’s answer is better. Begin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers. When evaluating the assistants’ answers, compare both assistants’ answers with your answer. You must identify and correct any mistakes or inaccurate information. Then consider if the assistant’s answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive. Then consider the creativity and novelty of the assistant’s answers when needed. Finally, identify any missing important information in the assistants’ answers that would be beneficial to include when responding to the user prompt. After providing your explanation, you must output only one of the following choices as your final verdict with a label: 1. Assistant A is significantly better: [[A»B]] 2. Assistant A is slightly better: [[A>B]] 3. Tie, relatively the same: [[A=B]] 4. Assistant B is slightly better: [[B>A]] 5. Assistant B is significantly better: [[B»A]] Example output: {My final verdict is tie: [[A=B]]} prompt_template: [\"<|User Prompt|> {question_1} <|The Start of Assistant A’s Answer|> {answer_1} <|The End of Assistant A’s Answer|> <|The Start of Assistant B’s Answer|> {answer_2} <|The End of Assistant B’s Answer|>\"] 20 H.2 Tag Process Prompt In the mesoscopic diversity strategy, the prompt templates are adapted from Instag. H.2.1 Tagging Prompt Tagging Prompt Template You are a tagging system that provides useful tags for instruction intentions to distinguish instructions for a helpful AI assistant. Below is an instruction: [begin] {instruction} [end] Please provide coarse-grained tags, such as \"Spelling and Grammar Check\" and \"Cosplay\", to identify main intentions of above instruction. Your answer should be a list including titles of tags and a brief explanation of each tag. Your response have to strictly follow this JSON format: [{\"tag\":str, \"explanation\":str}]. Please response in English. H.2.2 Tag Precision Evaluate Prompt Tag Precision Evaluate Template You are an experienced judge for intention tags of instructions. You will be provided a query and a list of tags describing intentions of the query as followed: [query]:{query} {tags} Please provide feedback about whether all tags precisely describe an intention of the instruction. Please identify all incorrect tags and provide their indices in the JSON format as your response. The JSON format for your response is a list of JSON dictionary and the JSON dictionary has only one key to identify the index of each incorrect tag: [{\"idx\":int}]. For example, if [tag 0] and [tag 2] are incorrect, you should response as [{\"idx\":0}, {\"idx\":2}]. If all tags are correct, please response an empty list as []. H.2.3 Tag Consistency Evaluate Prompt Tag Consistency Evaluate Prompt Template You are an experienced judge for consistency of intention tags for instructions. You will be provided a tag and a list of instructions labeled with this tag as followed: [tag]:{tag} {instructions} Please provide feedback about whether the meaning of this tag is consistent among all instructions. Please identify all inconsistent instructions and provide their indices in the JSON format as your response. The JSON format for your response is a list of JSON dictionary: [{\"idx\":int}]. For example, if the meaning of tags in [instruction 0] and [instruction 2] are inconsistent, you should response as [{\"idx\":0}, {\"idx\":2}]. If the meaning of tag is consistent in all instructions, please response an empty list as []. 21",
  "text_length": 72416
}