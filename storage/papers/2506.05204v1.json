{
  "id": "http://arxiv.org/abs/2506.05204v1",
  "title": "OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with\n  Expanded Field-of-View",
  "summary": "Reconstructing semantic-aware 3D scenes from sparse views is a challenging\nyet essential research direction, driven by the demands of emerging\napplications such as virtual reality and embodied AI. Existing per-scene\noptimization methods require dense input views and incur high computational\ncosts, while generalizable approaches often struggle to reconstruct regions\noutside the input view cone. In this paper, we propose OGGSplat, an open\nGaussian growing method that expands the field-of-view in generalizable 3D\nreconstruction. Our key insight is that the semantic attributes of open\nGaussians provide strong priors for image extrapolation, enabling both semantic\nconsistency and visual plausibility. Specifically, once open Gaussians are\ninitialized from sparse views, we introduce an RGB-semantic consistent\ninpainting module applied to selected rendered views. This module enforces\nbidirectional control between an image diffusion model and a semantic diffusion\nmodel. The inpainted regions are then lifted back into 3D space for efficient\nand progressive Gaussian parameter optimization. To evaluate our method, we\nestablish a Gaussian Outpainting (GO) benchmark that assesses both semantic and\ngenerative quality of reconstructed open-vocabulary scenes. OGGSplat also\ndemonstrates promising semantic-aware scene reconstruction capabilities when\nprovided with two view images captured directly from a smartphone camera.",
  "authors": [
    "Yanbo Wang",
    "Ziyi Wang",
    "Wenzhao Zheng",
    "Jie Zhou",
    "Jiwen Lu"
  ],
  "published": "2025-06-05T16:17:18Z",
  "updated": "2025-06-05T16:17:18Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05204v1",
  "full_text": "--- Page 1 ---\nOGGSplat: Open Gaussian Growing for Generalizable\nReconstruction with Expanded Field-of-View\nYanbo Wang∗Ziyi Wang∗Wenzhao Zheng Jie Zhou Jiwen Lu†\nDepartment of Automation, Tsinghua University, China\n{wyb23, wziyi22}@mails.tsinghua.edu.cn;\nwenzhao.zheng@outlook.com; {jzhou, lujiwen}@tsinghua.edu.cn\nAbstract\nReconstructing semantic-aware 3D scenes from sparse views is a challenging\nyet essential research direction, driven by the demands of emerging applications\nsuch as virtual reality and embodied AI. Existing per-scene optimization methods\nrequire dense input views and incur high computational costs, while generalizable\napproaches often struggle to reconstruct regions outside the input view cone. In\nthis paper, we propose OGGSplat , an open Gaussian growing method that expands\nthe field-of-view in generalizable 3D reconstruction. Our key insight is that the\nsemantic attributes of open Gaussians provide strong priors for image extrapolation,\nenabling both semantic consistency and visual plausibility. Specifically, once\nopen Gaussians are initialized from sparse views, we introduce an RGB-semantic\nconsistent inpainting module applied to selected rendered views. This module\nenforces bidirectional control between an image diffusion model and a semantic\ndiffusion model. The inpainted regions are then lifted back into 3D space for\nefficient and progressive Gaussian parameter optimization. To evaluate our method,\nwe establish a Gaussian Outpainting (GO) benchmark that assesses both semantic\nand generative quality of reconstructed open-vocabulary scenes. OGGSplat also\ndemonstrates promising semantic-aware scene reconstruction capabilities when\nprovided with two view images captured directly from a smartphone camera. Code\nis available at https://github.com/Yanbo-23/OGGSplat .\n1 Introduction\nBuilding realistic and semantically meaningful 3D representations of the world has become a crucial\ngoal in computer vision, driven by applications in robotics, virtual reality, and embodied AI. Beyond\nreconstructing vivid textures and accurate geometry, modern systems increasingly demand semantic\nawareness to support high-level understanding and interaction within 3D environments. This dual\ndemand for geometric fidelity and semantic interpretability introduces new challenges for scene\nrepresentation. Recent researches typically address this by combining open-vocabulary features\nwith 3D reconstructive representations like 3D Gaussians [ 12]. Approaches based on per-scene\noptimization [ 24,33,27,26,39,45], which leverage dense multi-view inputs, achieve well-structured\n3D geometry with fine-grained semantic alignment. In contrast, newly emerging feed-forward\nmethods [ 37,10] offer improved scalability and generalization across scenes by predicting semantic-\naware 3D representations directly from sparse input views via a trained neural network.\nDespite significant progress, existing methods still suffer from critical limitations. Per-scene opti-\nmization approaches typically require hundreds of input views and incur high computational time\ncosts, often taking 25 to 50 minutes per scene. On the other hand, generalizable methods offer\nfast inference and handle sparse input views efficiently, but their performance is constrained by the\nlimited scope of those inputs. When presented with extrapolated viewpoints, these models often\n∗Equal contribution.†Corresponding author.arXiv:2506.05204v1  [cs.CV]  5 Jun 2025\n--- Page 2 ---\nRGB-Sem Consistent InpaintorOpen Gaussian GrowingInpainted ViewRendered View\nContext View2Novel View\nContext View1\nQueryTableFloorTurn LeftTurn RightContextFigure 1: We propose OGGSplat , an open Gaussian growing method that expands the field-of-view\nof generalizable Gaussian reconstruction. The last three rows visualize the rendered images, their\nsemantic maps, and category-specific heatmaps obtained by querying open-vocabulary concepts.\nproduce distorted geometry and semantically implausible content. This highlights an urgent need\nfor a generalizable 3D reconstruction framework that can reliably expand the field-of-view while\nmaintaining geometric coherence and semantic consistency. We argue that incorporating semantic\ncues from open-vocabulary features can provide valuable guidance in imagining plausible content for\nunseen regions, thus extending the application of generalizable reconstruction.\nIn this paper, we address the aforementioned challenge of generalizable open-vocabulary 3D recon-\nstruction by introducing OGGSplat , anOpenGaussian Growing framework designed to extrapolate\nsemantically meaningful 3D Gaussians beyond the input view coverage. Our goal is to enhance\nopen-vocabulary Gaussian representations with the capacity to grow new, semantic-aware Gaussians,\nthereby expanding the field-of-view in scenes reconstructed from sparse inputs. A key insight of\nour approach is that the semantic attributes inherent in open Gaussians provide a strong prior for\nsemantically plausible extrapolation. To exploit this, OGGSplat employs a progressive Gaussian\ngrowing strategy that builds on the initial reconstruction from sparse views. Central to this process is\na novel RGB-semantic consistent inpainting module, which enables bidirectional interaction between\nimage and semantic inpainting: semantic maps guide image completion, while inpainted images\nrefine the semantic features in return, ensuring pixel-level alignment. The synthesized RGB images\nand semantic maps are then used to efficiently optimize the newly introduced Gaussians. This\nstrategy enables OGGSplat to strike a balance between computational efficiency and the quality of\nthe reconstructed open-vocabulary 3D scenes, even in cases of severely limited input coverage.\nWe conduct extensive experiments on ScanNet++ [ 46] and introduce a novel Gaussian Outpainting\n(GO) benchmark. Please refer to the supplementary materials for video results showcasing recon-\nstructed scenes with expanded field-of-view and semantically coherent content. The GO benchmark\nis designed to assess both visual fidelity and semantic plausibility in extrapolated regions. We incor-\nporate several state-of-the-art 2D open-vocabulary semantic segmentation models to generate the\nground-truth. This enables quantitative evaluation using segmentation mean Intersection-over-Union\n(mIoU) in addition to commonly used generative metrics Fréchet Inception Distance (FID) [ 9].\nWe also deploy OGGSplat on context images captured directly using a smartphone camera. The\npromising results highlight its potential for future applications on portable devices.\nIn conclusion, the contributions can be summarized as: (1) We propose OGGSplat, the first work\nto expand the field-of-view for generalizable open Gaussian reconstructions. (2) We design an\nRGB-semantic consistent inpainting module that enforces bidirectional interaction between image\n2\n--- Page 3 ---\nand semantic map inpainting, and introduce a progressive Gaussian growing strategy to optimize new\nGaussians from the inpainted content. (3) We establish the Gaussian Outpainting (GO) benchmark,\nenabling comprehensive evaluation with both semantic perception and generative quality metrics.\n2 Related Work\n3D Gaussian Splatting. 3D Gaussian Splatting (3DGS)[ 12] is a more efficient differentiable\nrendering method compared with Neural Radiance Field (NeRF) [ 21]. Existing 3DGS methods can\nbe categorized by their optimization strategy and the number of input views. Early approaches [ 49,18,\n6,8] rely on per-scene optimization using hundreds of images, achieving high-fidelity reconstructions\nat the cost of computation and scalability. Subsequent methods [ 41,22,5,52] focus on reconstructing\nscenes from only a few views, though per-scene optimization is still required. More recently,\ngeneralizable methods [ 3,36,34,4] emerge, which infer Gaussian parameters via a feed-forward\nneural network, enabling fast inference and cross-scene generalization. Building upon them, open-\nvocabulary 3DGS incorporates Gaussian representations with semantic features.\nDense-view Per-scene Optimization. The majority of open-vocabulary 3DGS methods adopt a\ndense-view per-scene optimization paradigm [ 35,25,16,19]. LangSplat [ 24] pioneers the field via\nknowledge distillation from vision-language models such as CLIP [ 28] and DINO [ 2]. Building upon\nit, LEGaussians [ 33]and GOI [ 27] introduce quantization techniques to compress high-dimensional\nsemantic embeddings into compact Gaussian parameters. Alternatively, methods such as OpenGaus-\nsian [ 39] and Gaussian Grouping [ 45] utilize 2D open-vocabulary segmentation tools like SAM [ 15]\nto assign semantic labels to rendered images, without explicitly encoding semantics into the Gaussians\nthemselves. Despite their semantic expressiveness and high-fidelity reconstructions, these approaches\ninherit the need for densely sampled input views and time-intensive per-scene optimization.\nSparse-view Per-scene Optimization. To mitigate the overfitting issue of sparse-view per-scene\noptimization, recent works explore view synthesis strategies. Methods such as ViewCrafter [ 48]\nand FlowR [ 7] use diffusion models to synthesize photometrically and geometrically consistent\nintermediate views. Extending this idea to open-vocabulary 3DGS, SPC-GS [ 17] leverages video\ndiffusion model MotionCtrl [ 38] to generate improved structure-from-motion initializations. To\nenhance semantic consistency, SPC-GS integrates SAM2 [ 29], which provides temporally aligned\nsemantic masks and embeddings across frames. However, incorporating video diffusion models\nsignificantly increases computation costs, and the overall optimization latency remains high.\nGeneralizable Models. Generalizable 3D reconstruction models leverage feed-forward neural net-\nworks trained on large-scale datasets to avoid per-scene optimization. PixelSplat [ 3] and MVSplat [ 4]\nrely on accurate camera pose information, while DUSt3R [ 36] and Splatt3R [ 34] propose to directly\ninfer point clouds and Gaussian parameters from unposed image pairs. The latter paradigm has\nquickly been extended to open-vocabulary 3DGS: GSemSPlat [ 37] and SparseLGS [ 10] incorporate\nsemantic prediction heads to jointly estimate open-vocabulary features alongside Gaussian parameters.\nDespite these advances, a key limitation is their lack of outpainting capability, where they struggle to\nreconstruct regions beyond the narrow visual field covered by the input views.\n3 Approach\nAs illustrated in Figure 2, OGGSplat comprises three main stages. First, in Section 3.1, we initialize\na 3D Gaussian reconstruction from the input sparse views and inject open-vocabulary semantic\nrepresentations into the Gaussian parameters. Next, Section 3.2 introduces the RGB-semantic\nconsistent inpaintor, where we propose a bidirectional control mechanism to ensure pixel-level\nalignment between semantics and appearance. The semantic map guides the image completion\nprocess, while the inpainted image, in turn, refines the semantic features. Finally, to allow the 3D\nGaussian structure to grow consistently with the generated content, we design a progressive Gaussian\ngrowing strategy, detailed in Section 3.3. The second and third stages are applied iteratively to\ngradually expand the Gaussian representation beyond the initial field-of-view. In practical usage,\nOGGSplat takes as input any two uncalibrated images and processes them through the above three\nstages to produce an expanded 3D Gaussian scene enriched with open-vocabulary semantics. This\nenables real-time rendering of both RGB images and their corresponding semantic feature maps from\narbitrary viewpoints, supporting a variety of downstream tasks such as grounding and scene editing.\n3\n--- Page 4 ---\nContext ViewsOpen Gaussian InitializationInitial Gaussian 𝒢!Novel ViewImageEncoderSemEncoderDiff\nDiffTextEncoderImg2Sem ControlNetControlControlInpainted ViewImageDecoderSemDecoderRGB-Sem Consistent InpaintorOpen Gaussian Growing×𝑛Grown 𝒢\"Inpainted 𝒢\"#𝒢\"$%Depth EstimatorDepth AlignmentGaussian OptimizationEdge Translator\n𝐻&'𝐻()*𝐻+,-Figure 2: OGGSplat Architecture. We first initialize an open Gaussian reconstruction, injecting\nsemantic features via an additional semantic head. Then, the RGB-semantic consistent inpaintor\napplies bidirectional controls between images and semantic maps to ensure semantic plausibility and\nspatial alignment. Finally, the inpainted regions are lifted back to 3D and optimized to expand the\nGaussians. The last two stages are performed iteratively to progressively grow the Gaussians.\n3.1 Generalizable Open Gaussian Initialization\nGaussian Reconstruction. Given any two uncalibrated but overlapping images I1, I2∈RH×W×3\nwith height Hand width W, we adopt Splatt3R [ 34] to reconstruct an initial Gaussian G0∈RN×d\nvia a shared backbone, cross-attention interactions and multiple Gaussian heads. The number of\nGaussian primitives N= 2×H×Wcorresponds to the total number of image pixels, while each\nGaussian feature of dimension dis composed of the following components: (1) a 3D point position\np∈R3, (2) a position offset p∆∈R3, defining the Gaussian center µ=p+p∆, (3) a rotation\nquaternion q∈R4and a scale vector s∈R3, together determining the covariance matrix Σ, (4)\nan opacity scalar α∈R, controlling the transparency of the Gaussian, and (5) a view-dependent\nappearance embedding represented by spherical harmonics S∈R3×dcolorofdcolor degrees.\nOpen Feature Injection. To incorporate open-vocabulary clues, we introduce an additional semantic\nhead Hsemto predict semantic parameters f∈Rdsemfor each Gaussian primitive, inspired by\nGSemSplat [ 37]. Following common practice [ 45,37], we set the semantic embedding dimension\ndsem= 16 to reduce the computational overhead during Gaussian rendering. To supervise the\npredicted semantic features f, we adopt the well-optimized vision-language APE [ 32] model to\nefficiently obtain pixel-dense open-vocabulary semantic supervision signals Fgt∈RH×W×dAPE,\nwhere the APE semantic feature dimension dAPE≫dsem. To align the dimensionality, we train an\nautoencoder composed of a down-projection encoder E↓that maps the APE features to dsem, and a\ncorresponding decoder D↑that reconstructs the original features with minimal information loss. The\nsemantic learning objective in this stage is formulated as a cosine similarity loss:\nLsem=X\nvX\nh,w\u0010\n1−cos\u0010\nfv,h,w,E↓\u0010\nfgt\nv,h,w\u0011\u0011\u0011\n, (1)\nwhere h∈[0, H), w∈[0, W)denote pixel coordinates and vrepresents target view index. The\nsemantic feature fv,h,w is computed with α-blending, analogous to that used for RGB rendering.\n3.2 RGB-Semantic Consistent Inpaintor\nOnce the the initial Gaussian G0is reconstructed, we render RGB images Ivand their corresponding\nsemantic maps Fvfrom novel viewpoints v. However, when rendering outside the vision cone of\nthe context views, hollow regions often appear due to out-of-view areas and occlusion variations,\nas illustrated in Figure. 1. While pre-trained inpainting diffusion models [ 30,20,40] can partially\naddress this issue, maintaining pixel-wise consistency between inpainted images and their semantic\nmaps remains challenging. This spatial misalignment will be inherited by the following Gaussian\ngrowing process and can lead to increasingly severe semantic inconsistencies as the scene expands.\nFortunately, we observe that although the semantic modality introduces challenges, it also offers\nvaluable guidance: the partial semantic information, especially around the boundaries of incomplete\nregions, can be translated into explicit textual prompts to guide image inpainting. Symmetrically,\ninpainted RGB images can provide pixel-wise appearance cues to control semantic map completion.\n4\n--- Page 5 ---\nTherefore, we propose bidirectional controls between the RGB branch Diffrgband the semantic\nbranch Diffsem, allowing them to mutually enhance each other during the inpainting process.\nSemantic-to-RGB Control. To define the inpainting mask that determines whether a pixel should be\ninpainted, we rely on the rendered opacity αof each pixel. Similar to color rendering, we render an\nopacity map A, and then derive the inpainting mask Mvfor each view vby applying a pre-defined\nthreshold τ. For simplicity, we omit the view subscript vin the following discussion.\nAh,w=X\ni∈Θh,wαii−1Y\nj=1(1−αj), M h,w=1[Ah,w< τ], (2)\nwhere Θh,wdenotes the set of Gaussians contributing to the pixel at coordinate (h, w).\nThen we design an Edge Translator to extract semantic concepts near the inpainting boundaries\ndefined by the mask M, providing clearer guidance for filling in the hollow regions. Specifically, we\nfirst identify pixels along the boundary as Ωedge. The corresponding semantic features fedgeof these\nboundary pixels are then decoded into a higher-dimensional space using our pre-trained decoder D↑:\ngedge=D↑(fedge),for pixels in Ω edge (3)\nSimultaneously, we prepare a set of candidate classes Ccand, consisting of the top 100 semantic\ncategories in our training dataset. These categories are encoded into the same feature space as gedge.\nWe then compute the cosine similarity between gedgeandgcand to perform pixel-wise segmentation:\ncedge= argmaxci∈Ccandcos(gedge, gci), (4)\nIn this way, we can obtain a set of semantic categories Cedgethat are most relevant to the inpainting\nregion. Based on these categories, we generate a prompt text Tin the format of “a room with cate 1,\ncate 2, ..., and catei”, which is used to guide the diffusion-based RGB image inpainting model:\nIinp= Diff rgb(I, M, T ), (5)\nRGB-to-Semantic Control. Inspired by ControlNet [ 51], we also design an RGB-to-Semantic\ncontrol module to ensure that the generated semantic content aligns well with the corresponding\nregions in the RGB image. Formally, the completed semantic map is computed as:\nFinp= Diff sem(F, M, T, ControlNet( Iinp)), (6)\nwhere Fis the incomplete rendered semantic feature map, and ControlNet( Iinp)denotes the control\nmodule conditioned on the inpainted image Iinp. Please refer to the ControlNet paper or our\nsupplementary for further details. This module guides the semantic generation process, ensuring both\nstructural and appearance consistency between the predicted semantic features and the RGB content.\n3.3 Open Gaussian Growing\nObtaining the inpainted RGB images and semantic feature maps from selected views is not the\nfinal step of our pipeline. These results must be aggregated back into the initial Gaussian G0to\nenable real-time rendering from arbitrary novel viewpoints. For a set of selected anchor views\nV={v3, v4,···, va}, we perform iterative inpainting and progressively incorporate the newly\ncompleted regions into the Gaussian. At each iteration n, a new view is rendered based on the currently\naggregated Gaussians Gn−1and the newly inpainted content G+\nnis fused into this representation.\nBelow, we break down a single iteration and describe the Gaussian growing process in detail.\nThe inpainted image Iinpand semantic map Finpwill serve as supervision targets for the newly\ngrown Gaussians. However, establishing 3D geometry from a single novel view is inherently ill-posed,\nespecially in regions that are newly generated during inpainting. To enrich these views with structural\nknowledge, we adopt custom depth estimation model [ 23,43,44] to predict an absolute depth map\nDinpfromIinp. This depth map is then used to lift pixels back into 3D space, forming a point cloud\nin the global coordinate system. The resulting 3D points are used to initialize the position of the\nincremental Gaussian set G+, which is progressively integrated into the scene representation.\nP+= proj( Dinp, vinp, v1, K)·β,where β=q\n1\nMPM\ni=1\r\rpori\ni\r\r2\n2q\n1\nNPN\ni=1∥pnew\ni∥2\n2(7)\n5\n--- Page 6 ---\nwhere vinpandv1are the camera poses corresponding to the images IinpandI1, respectively, and K\ndenotes the intrinsic camera parameters. The scale factor βis introduced to align the newly projected\npoint cloud with the original 3D space in terms of depth. pori, pnewdenote the original and newly\nprojected 3D points within the overlapping regions, while MandNrepresent the respective number\nof points in each set. It is worth noticing that scaling point coordinates alone does not ensure perfect\nalignment. Nonetheless, it offers an efficient and approximate initialization, since the entire scene is\nconstructed with respect to the normalized coordinate system of the first view.\nAt the nthiteration, after merging Gn−1with the newly initialized Gaussians G+\nn, we perform efficient\nper-scene optimization to update the grown Gaussian Gn. This optimization is supervised by the\noriginal sparse context views, previously and newly inpainted views. The objective function is:\nL=λrgb· Lrgb+λfeat· Lfeat, (8)\nwhere Lrgb=λ1· LL1(Ir, Iinp) +λ2· LSSIM(Ir, Iinp),andLfeat= 1−cos(Fr, Finp)(9)\nwhere λ1andλ2balance pixel-wise accuracy and perceptual similarity, while λrgbandλfeatcontrol\nthe overall contributions of the photometric and semantic losses, respectively. Ir, Frdenote the\nrendered RGB images and semantic features from the optimizing Gaussian from vinp.\n4 Experiments\n4.1 The Gaussian Outpainting (GO) Benchmark\nTo effectively evaluate both the visual fidelity and semantic plausibility of OGGSplat in extrapolated\nregions, we introduce a novel Gaussian Outpainting benchmark based on the validation set of the\nScanNet++ [46] dataset. Detailed information about this dataset can be found in the supplementary.\nData Composition. The GO benchmark covers all 50 validation scenes from ScanNet++. For each\nscene, we select 1 image pair as the context views to serve as model inputs. To ensure consistency\nin data sampling and maintain temporal coherence, the context views are chosen as the 1stand10th\nframes. This selection introduces moderate viewpoint variation while preserving semantic continuity,\nenabling a more meaningful evaluation of extrapolated content. For evaluation, we uniformly sample\n16 novel camera poses within a horizontal range of [−60◦,60◦]and a vertical range of [−20◦,20◦]\naround the pose of the context image I1. Novel RGB images and semantic maps are directly rendered\nfrom the reconstructed Gaussians at these poses and used as evaluation samples. To account for\nrandomness in generation, we repeat the experiment five times and report the average results.\nVisual Fidelity Evaluation. We adopt the Fréchet Inception Distance (FID) [ 9] to evaluate the\nstatistical similarity between rendered and real images. For FID computation, all images from the\nvalidation split of the ScanNet++ dataset are used as the reference distribution. FID is then calculated\nbetween this reference distribution and the distribution of the newly rendered images. However, we\nobserve that the limited number of generated images can negatively affect the stability of the FID\nmetric. To address this, we increase the context views from one pair toten pairs per scene, while\nmaintaining a frame interval of 10 within each pair. This expands the number of newly rendered\nimages by a factor of ten, resulting in a more stable and reliable FID evaluation.\nSemantic Plausibility Evaluation. While visual fidelity is evaluated over the entire rendered image,\nthe semantic plausibility focuses on newly outpainted regions using the mean Intersection over Union\n(mIoU) metric. To this end, we restrict semantic evaluation to regions rendered by the initial Gaussian\nthat exhibit low confidence, defined as having an accumulated opacity below 0.3 in novel views.\nThis targeted evaluation ensures that the benchmark focuses on semantic consistency in extrapolated\nareas. Since ground truth semantic annotations are unavailable for these extrapolated regions, we\ngenerate ground truth labels using five state-of-the-art open-vocabulary 2D semantic segmentation\nmodels [ 42,32,50,47,11]. Their predictions are aggregated via a majority voting scheme, where\neach pixel is assigned the label most frequently predicted across the five models. To assess the quality\nof semantic segmentation, we follow the protocol in [ 13,33,24] by computing a relevancy score for\neach text query. More details on relevancy score computation are provided in the supplementary. To\nensure generality, we retain only those predicted mask regions with a relevancy score exceeding 50%\nas the final binary mask. This filtering mechanism makes our evaluation suitable even for images\nwhere a specific category may be absent. During evaluation, we focus on 10 commonly used semantic\ncategories selected from the top 20 classes in ScanNet++, such as wall,floor,chair ,table , and others.\n6\n--- Page 7 ---\nTable 1: Gaussian Outpainting (GO) benchmark results. We compare generative metric FID and\nsemantic metric mIoU (%) between OGGSplat and previous methods.\nMethodsGeneration Segmentation (IoU ↑)\nFID↓ mIoU wall ceiling floor table door (s)cabinet chair (b)shelf box bed\nLangSplat [24] 50.4 6.9 29.0 13.4 15.8 1.8 4.0 1.3 2.5 0.0 0.8 0.0\nSplatt3R [34] 46.4 6.0 10.1 2.1 18.9 5.1 0.0 1.6 13.8 0.3 0.0 2.3\nOGGSplat (Ours) 37.5 17.6 45.6 0.1 58.3 13.3 5.4 3.7 21.4 7.4 3.1 18.0\nContext ViewsLangSplat\nSplatt3R\nOurs\nContext Views\nLangSplatSplatt3ROurs\nQuery: TableQuery: Chair\nFigure 3: Qualitative comparisons between LangSplat, Splatt3R, and OGGSplat on the GO\nbenchmark. The first row presents RGB images rendered from novel, out-of-scope viewpoints. The\nsecond row visualizes the heatmap when querying different text concepts.\n4.2 Main Results\nBaseline Methods for Comparison. We select two representative baselines for comparison:\nLangSplat [ 24], a per-scene optimization model, and Splatt3R [ 34], a generalizable model. LangSplat\nrelies heavily on accurate initialization via COLMAP [ 31], which becomes unreliable when only\ntwo input images are available. To address this limitation and enable fair comparison, we initialize\nLangSplat using point cloud positions predicted by Splatt3R, allowing the model to focus more\neffectively on learning semantic representations. Meanwhile, as Splatt3R does not support open-\nvocabulary semantic prediction in its original form, we extend it with a semantic head trained in our\nfirst stage in Section 3.1. During evaluation, for all models, we consider only the regions rendered by\nGaussians with an accumulated opacity greater than 0.01 as valid predictions for computing the IoU\nscores. This threshold filters out low-confidence regions and ensures consistency across models.\nQuantitative Comparisons. In Table 1, we compare LangSplat [ 24], Splatt3R [ 34], and OGGSplat\non the GO benchmark. OGGSplat consistently outperforms the baselines by a significant margin\non both visual fidelity (FID) and semantic plausibility (mIoU). It’s worth noticing that the overall\nFID remains relatively high across all methods. The main reason is the limited number of context\npairs available in the validation set, which constrains data diversity. We are unable to sample more\npairs because some scenes in the ScanNet++ validation set are relatively small. To maintain a\nconsistent sampling ratio across all validation scenes, we limit the number of context pairs to 10 per\nscene. Regarding semantic plausibility, OGGSplat achieves notably better performance on common\nlarge objects such as chair ,table , and bed. However, the model performs relatively worse on the\nceiling class. We attribute this to the limitations of the APE encoding, as well as the difficulty of the\nSplatt3R backbone in distinguishing between the ceiling andwall with similar appearance in color\nand texture. We believe this limitation can be addressed in future work by leveraging more powerful\nvision-language models and more superior generalizable Gaussian reconstruction methods.\nQualitative Comparisons. We conduct extensive qualitative comparisons with baseline methods\nand illustrate them in Figure 3. OGGSplat performs better in both novel rendered images and\nopen-vocabulary querying. Regarding rendered images, LangSplat tends to overfit the context views,\nresulting in blurry renderings from novel viewpoints, even when the Gaussian positions have been\ninitialized. Splatt3R, on the other hand, exhibits large black regions in areas outside the input views.\nIn contrast, OGGSplat reasonably extrapolates unseen regions by leveraging semantic information.\nRegarding open-vocabulary querying, both LangSplat and Splatt3R are limited to input vision cones.\nOGGSplat, however, is capable of accurately identifying and querying objects even in previously\nunseen regions, demonstrating stronger generalization and semantic understanding capabilities.\n7\n--- Page 8 ---\n𝑎Samples from the S3DIS Dataset𝑏Samples from Real Phone Camera\nContextNovelInpaintedRenderedContextNovelInpaintedRendered\nFigure 4: Model generalization ability evaluation. Column (a)shows results where the context\nviews are taken from the S3DIS [ 1]. We query bookshelf andtable for each sample, respectively. In\ncolumn (b), the context views are captured directly using a phone camera , and we query chair .\nQuery: CeilingQuery: TableInpaintedSAM+CLIPNovel View\nInpainted ViewUnpainted\nInpaintedSAM+CLIPNovel View\nInpainted ViewUnpaintedQuery: BoxQuery: Bed\nFigure 5: Ablations on the effect of semantic diffusion model. We compare open-vocabulary\npredictions between the SAM+CLIP offline method and our semantic diffusion inpainting module.\nModel Generalization Ability. Apart from ScanNet++ used for training, we also test OGGSplat’s\ngeneralization ability on data with different distributions. As shown in Figure 4, OGGSplat success-\nfully reconstructs semantic-aware scenes with an expanded field-of-view using S3DIS [ 1] samples.\nWe further demonstrate the practicality of OGGSplat on portable devices in column (b), where the\ncontext views are captured by a phone camera. The inpainted image and semantic query on chair\nshow promising results, highlighting OGGSplat’s potential for applications in daily life.\n4.3 Ablation Studies\nIn Section 3.2, we introduced the RGB-semantic consistent inpainting module. In this section, we\nfirst highlight the importance of the semantic diffusion branch, followed by comprehensive ablations\non the GO benchmark to evaluate the effectiveness of the proposed bidirectional control strategy.\nSemantic Diffusion Model. To obtain reliable semantics for the inpainted regions, we train a semantic\ndiffusion module. A straightforward alternative would be employing an offline open-vocabulary\nsemantic segmentation model, such as SAM [ 15]+CLIP [ 28] as LangSplat [ 24]. However, this\noften leads to semantic inconsistency with the original Gaussian, particularly when the objects are\npartially visible (see Figure 5). It tends to produce incorrect results even in regions originally correctly\npredicted, and these errors can propagate and negatively affect the subsequent Gaussian growing. In\ncontrast, our trained semantic diffusion model preserves the semantic consistency in the unpainted\nregions and significantly improves the accuracy of the predicted semantics in the inpainted areas by\nleveraging the semantic priors from the visible context. This ensures that the newly generated content\naligns well with the existing scene semantics, leading to better overall reconstruction quality.\n8\n--- Page 9 ---\nTable 2: Ablations on the GO Benchmark evaluating the impact of the bidirectional control\nstrategy. The performance is measured by mIoU (%) across various semantic categories.\nControl Type Segmentation Results (IoU ↑)\nS→RGB RGB →SmIoU wall ceiling floor table door (s)cabinet chair (b)shelf box bed\n✗ ✓ 16.6 45.8 0.1 56.8 12.3 4.6 2.8 19.3 6.3 3.6 15.1\n✓ ✗ 14.4 43.0 0.1 47.6 10.3 5.0 3.5 16.6 2.4 2.5 12.7\n✓ ✓ 17.6 45.6 0.1 58.3 13.3 5.4 3.7 21.4 7.4 3.1 18.0\nContext Views\nOGGSplatw/o Sem→RGB Controlw/o RGB→Sem ControlRendered View\nInpainted SemInpainted RGBQuery: Door\nContext ViewsRendered View\nInpainted SemInpainted RGBQuery: Bed\nContext Views\nInpainted SemInpainted RGBQuery: ChairRendered View\nFigure 6: Qualitative comparison of bidirectional control. Row 1 shows the context images and the\nincomplete renderings from novel views. Rows 2 to 4 correspond to the ablation settings in Table 2,\nwhere each variant removes one of the control mechanisms to examine its individual effect.\nSemantic-to-RGB Control. With access to open-vocabulary semantics, we propose an edge translator\nto extract semantic cues from the Gaussian boundaries and guide the image/feature completion. In the\nfirst row of Table 2, we remove the edge translator and instead use a generic description (“a room”)\nas the text prompt. As a result, semantic segmentation performance across most categories decreases.\nThis degradation is also evident in the qualitative comparison in Figure 6, where the generated content\nappears more ambiguous and less semantically grounded. These results validate the effectiveness of\nour semantic-to-RGB control in guiding high-fidelity, semantically consistent Gaussian growth.\nRGB-to-Semantic Control. In OGGSplat, the semantic inpainting model is explicitly controlled\nby inpainted images. We remove it in the second row of Table 2 and the third row of Figure 6.\nWithout RGB-to-semantic control, the generated RGB images and semantic maps exhibit poor spatial\nalignment, leading to significantly degraded segmentation accuracy. In contrast, introducing the\nRGB-to-semantic control clearly improves spatial consistency and yields much better performance.\n5 Limitations and Conclusion\nIn this paper, we design OGGSplat, an open Gaussian growing method for generalizable reconstruc-\ntion with expanded field-of-view. By leveraging semantic cues from open Gaussians and introducing\nRGB-semantic consistent inpainting via bidirectional controls, our method effectively expands the\nfield-of-view and ensures both visual fidelity and semantic coherence. The reconstructed out-of-view\nregions are progressively refined through an efficient Gaussian optimization process. To facilitate\nevaluation, we proposed the Gaussian Outpainting benchmark, which quantitatively assesses the\ngenerative and semantic quality of open-vocabulary scene reconstruction. Extensive experiments\ndemonstrate that OGGSplat achieves superior performance in extrapolating beyond the input view\ncone, marking a significant step forward in generalizable and flexible 3D reconstruction. However,\nOGGSplat is currently limited to indoor scenes, since depth estimation in outdoor environments is\nmore challenging, leading to performance decreasing of our baseline model Splatt3R. Nevertheless,\nwe believe that with the integration of more powerful and generalizable Gaussian reconstruction\nmodels in the future, our approach can achieve promising performance in outdoor scenarios as well.\n9\n--- Page 10 ---\nA Additional Experimental Results\nA.1 Video Results\nTo provide a more comprehensive and intuitive visualization of our method, we include video results\nin the supplementary ZIP file. Specifically, we present visualizations across five different scenes.\nFor each scene, we showcase the rendering results of both Splatt3R [ 34] and OGGSplat under\ncontinuous camera views. Additionally, we provide the corresponding relevance score heatmaps\nunder a specific open-vocabulary query, enabling a direct comparison of semantic understanding\nacross the two methods. As clearly demonstrated, our model effectively extrapolates to unseen\nregions while maintaining both high visual fidelity and semantic plausibility.\nA.2 Ablation on Separate Diffusion UNet\nTo enable the generation of both spatially consistent RGB images and semantic content, we train\ntwo separate diffusion models: DiffrgbandDiffsem, and enforce spatial consistency between them\nusing a ControlNet [ 51]-based approach. A simpler alternative would be to employ a single shared\ndiffusion UNet based on an image diffusion model [ 30], modified to allow additional semantic inputs\nand outputs by adjusting the input and output convolutional channels. However, our experiments\nshow that this approach fails to produce meaningful RGB and semantic outputs. As illustrated in\nFig. 7, using a hybrid (shared) diffusion UNet leads to severe distortions in both RGB images and\nsemantic content. We think that this failure is due to the significant differences between the latent\nspaces of the RGB image V AE and the semantic V AE, which makes it difficult for a single UNet to\nlearn consistent mappings in both domains. These results highlight the effectiveness and necessity of\nour separate Diffsemmodel and the corresponding control module design.\nB Implementation Details\nB.1 Scannet++ Dataset\nScanNet++ dataset [ 46] provides high-quality 3D geometry along with high-resolution RGB images of\nvarious indoor environments. Following the protocol introduced by Splatt3R, originally designed for\n3D reconstruction, we adopt the standard training split comprising 230 scenes and the validation split\ncontaining 50 scenes. Following [ 34,37], we also discard frames missing reliable depth information.\nAll selected frames are uniformly cropped and resized to a spatial resolution of 512×512.\nB.2 Training Settings\nTo provide a clearer overview of the experimental configurations used at different training stages, we\nsummarize the details in Table 4. The table includes the settings for all key components that need\nto be trained in our method, namely the generalizable open Gaussian initialization module, RGB\nUNet, semantic V AE [ 14], semantic UNet, ControlNet [ 51], and the open Gaussian growing process.\nGeneralizable Open Gaussian Initialization . We adopt the pretrained Splatt3R model and freeze\nits backbone, which is responsible for predicting the basic Gaussian attributes. We then train only the\nnewly added semantic head, denoted as Hsem. During training, we use two context images as input\nand supervise the model by rendering three target views from the training split. Following the setup\nin Splatt3R [ 34], the context images are selected such that at least 30% of the pixels in the second\nimage have direct correspondences in the first image. Similarly, target images are chosen such that at\nleast 30% of their content is visible in at least one of the context images.\nRGB-Semantic Consistent Inpaintor . For RGB image inpainting model Diffsem, we fine-tune a\nstable diffusion inpainting model [ 30] to better align the generated appearance with realistic indoor\nscenes. In addition to standard RGB inpainting, we propose a novel diffusion-based feature inpainting\nmodel, denoted as Diffsem, which consists of both a Variational Autoencoder [ 14] (V AE) and a UNet\narchitecture. This model enables semantic-aware inpainting in the feature space while maintaining\nconsistency with the RGB domain. To ensure spatial consistency between the RGB and semantic\ncontents, we train an auxiliary RGB control module inspired by ControlNet [ 51] that guides the\ninpainting process in the feature space.\n10\n--- Page 11 ---\nContext Views\nSeparateRendered View\nInpainted \nSemInpainted \nRGBQuery: \nDoor\nContext Views Rendered View\nInpainted \nSemInpainted \nRGBQuery: \nBed\nContext Views\nInpainted \nSemInpainted \nRGBQuery: \nChairRendered View\nHybrid \nFigure 7: Qualitative comparison between hybrid (shared-weight) and separate diffusion UNet\narchitectures. Row 1 shows the context images along with the incomplete renderings from novel\nviews. Row 2 presents the results by using a hybrid UNet that jointly predicts RGB image and\nsemantic content using shared weights. Row 3 shows the results from our proposed architecture with\ntwo separate UNets: one for RGB image synthesis and the other for semantic prediction.\nTable 4: Experiment settings for different training stages.\nConfig Gaussian Init.RGB-Semantic Consistent InpaintorGaussian GrowingRGB UNet Sem. V AE Sem. UNet ControlNet\noptimizer Adam AdamW8bit AdamW AdamW8bit AdamW8bit Adam\nlearning rate 1e-5 1e-5 6e-6 1e-5 1e-5 hybrid (Table 3)\nweight decay 5e-2 1e-2 1e-2 1e-2 1e-2 0\nscheduler multi-step constant cosine constant constant exponential\nbatch size 12 4 2 4 4 4\naccumulation steps 1 2 4 2 2 1\ntraining iterations 500,000 50,000 45,000 20,000 10,000 600\nGPU device 8 RTX 3090 8 RTX 3090 8 RTX 3090 8 RTX 3090 8 RTX 3090 1 RTX 3090\nimage size 512×512 512×512 512×512 512×512 512×512 512×512\nTable 3: Learning rates for different Gaus-\nsian parameters.\nParameter Learning Rate\npoint position µ 1e-2\nrotation quaternion q 1e-3\nscale vector s 5e-3\nopacity scalar α 5e-2\nspherical harmonics S 2.5e-2\nsemantic feature f 2.5e-3Open Gaussian Growing . We set the horizontal and\nvertical outpainting angles to lie within the ranges of\n[−60◦,60◦]and[−20◦,20◦], respectively. To simplify\nthis stage, we decouple the horizontal and vertical rota-\ntions: when the horizontal angle is non-zero, the vertical\nangle is set to zero, and vice versa. For each optimiza-\ntion round, to improve efficiency, we use two inpainted\nimages and their corresponding semantic maps under\nsymmetrical camera poses to provide the supervision\nsignal. Moreover, the selected camera view pairs are\narranged to exhibit progressively increasing angular differences, thereby enabling a gradual and pro-\ngressive Gaussian growing process. Specifically, denoting the camera rotation angles in the horizontal\nand vertical directions as (θh, θv), the sampled camera angles are selected in the following order:\n(0◦,0◦),(0◦,±20◦),(±30◦,0◦), and (±60◦,0◦). It is worth noting that during actual optimization,\ncamera poses can be arbitrary. This sampling strategy is adopted purely to facilitate a simpler,\nmore consistent, and computationally efficient optimization process. We conduct a total of four\noptimization rounds. In the first round, we perform inpainting without changing the camera poses,\ni.e., using poses of the original context views. This step focuses on refining low-confidence regions\nthrough inpainting to enhance rendering quality under the original views. In subsequent rounds, we\nfix the batch size to 4 and include supervision signals from the originally inpainted context views,\npreviously inpainted views, and newly generated inpainted views. For the optimization of Gaussian\nparameters, we adopt parameter-specific learning rates following the setting proposed in [ 24]. The\ndetailed learning rates for each type of parameter are summarized in Table 3. Empirically, we observe\nthat each optimization round converges efficiently within 600 training iterations.\n11\n--- Page 12 ---\nB.3 RGB-to-Semantic ControlNet Module\nFigure 8: The architecture of the ControlNet [51].To ensure spatial alignment between the in-\npainted RGB image and its corresponding se-\nmantic map, we adopt a control mechanism in-\nspired by ControlNet [ 51], where the RGB im-\nage serves as guidance for the generation of the\nsemantic map. An overview of the ControlNet\narchitecture is illustrated in Fig. 8. Specifically,\nour control module comprises the encoder and\nbottleneck components of the stable diffusion\nUNet architecture, with their weights initialized\nfrom the corresponding layers of a pretrained\nstable diffusion UNet. Conditional signals are\nthen injected into the bottleneck and decoder\nparts via zero convolutions and element-wise\naddition. To accelerate training and enhance the\neffectiveness of control learning, we initialize\nthe control module with pretrained parameters\nfrom a ControlNet model [ 51] conditioned on\nimage segmentation. This initialization strategy\nprovides a strong prior for spatially consistent\ngeneration and significantly improves both train-\ning efficiency and overall performance. Details\nof the training settings for this module are pro-\nvided in Table 4.\nB.4 GO Benchmark\nFor evaluation on our proposed GO Benchmark, we uniformly sample 16 novel camera poses around\nthe context image I1, covering a horizontal angular range of [−60◦,60◦]and a vertical angular range\nof[−20◦,20◦]. To simplify the evaluation setup, we decouple horizontal and vertical rotations,\nfollowing the same strategy described in Section B.2. The IoU score for every query is computed by\naveraging over a total of 50×16images. If the union of predicted and ground-truth regions in an\nimage is empty, that image is excluded from the IoU computation. To ensure robustness, we repeat\nthe inpainting, growing, and evaluation process five times with the same settings and report the mean\nIoU as the final benchmark result.\nB.5 Relevance Score for Evaluation\nDuring open-vocabulary querying, we select regions with a relevance score greater than 0.5 as\nthe final predicted category mask. The computation of the relevance score is inspired by prior\nworks [13, 24, 33], and is defined as follows for each query:\nRelevance = min\niexp(gimg·gqry)\nexp(gimg·gqry) + exp( gimg·gicanon), (10)\nwhere gimgdenotes the image semantic feature, gqryis the query APE embedding, and gi\ncanon represents\nthe APE embedding of a predefined canonical phrase such as \"object\" ,\"things\" ,\"stuff\" , or\"texture\" .\nIn contrast to the mentioned prior works, which typically focus on a limited set of categories in a\nsingle scene and require the set of possible scene categories to be known in advance, we adopt a more\ngeneral strategy. These prior methods often normalize the relevance score and select masks based on a\nthreshold over the normalized values. However, this approach may incorrectly force the prediction of\nmasks even for categories absent in the scene. To address this limitation and enhance generalizability,\nwe directly apply a fixed threshold of 0.5 to the raw (unnormalized) relevance scores and select pixels\nwith scores exceeding this threshold as the final predicted mask. This ensures that only queries with\ntruly high relevance scores produce predictions, avoiding false positives in irrelevant categories. As a\n12\n--- Page 13 ---\nresult, we are able to compute per-category prediction masks from a predefined query set without\nrequiring manual query specification for each individual scene.\nReferences\n[1]Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and\nSilvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR , 2016.\n[2]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV , 2021.\n[3]David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d\ngaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR , 2024.\n[4]Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger,\nTat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view\nimages. In ECCV , 2024.\n[5]Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d\ngaussian splatting in few-shot images. In CVPR , 2024.\n[6]Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. Light-\ngaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. NeurIPS ,\n2024.\n[7]Tobias Fischer, Samuel Rota Bulò, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi,\nNorman Müller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, and Peter Kontschieder.\nFlowr: Flowing from sparse to dense 3d reconstructions. arXiv preprint arXiv:2504.01647 ,\n2025.\n[8]Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A Efros, and Xiaolong Wang. Colmap-\nfree 3d gaussian splatting. In CVPR , 2024.\n[9]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS ,\n2017.\n[10] Jun Hu, Zhang Chen, Zhong Li, Yi Xu, and Juyong Zhang. Sparselgs: Sparse view language\nembedded gaussian splatting. arXiv preprint arXiv:2412.02245 , 2024.\n[11] Siyu Jiao, Hongguang Zhu, Jiannan Huang, Yao Zhao, Yunchao Wei, and Humphrey Shi.\nCollaborative vision-text representation optimizing for open-vocabulary segmentation. In\nECCV , 2024.\n[12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian\nsplatting for real-time radiance field rendering. ACM Trans. Graph. , 2023.\n[13] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf:\nLanguage embedded radiance fields. In ICCV , 2023.\n[14] Diederik P Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013.\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In\nICCV , 2023.\n[16] Haijie Li, Yanmin Wu, Jiarui Meng, Qiankun Gao, Zhiyao Zhang, Ronggang Wang, and Jian\nZhang. Instancegaussian: Appearance-semantic joint gaussian representation for 3d instance-\nlevel perception. arXiv preprint arXiv:2411.19235 , 2024.\n[17] Guibiao Liao, Qing Li, Zhenyu Bao, Guoping Qiu, and Kanglin Liu. Spc-gs: Gaussian splatting\nwith semantic-prompt consistency for indoor open-world free-view synthesis from sparse inputs.\narXiv preprint arXiv:2503.12535 , 2025.\n13\n--- Page 14 ---\n[18] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-\ngs: Structured 3d gaussians for view-adaptive rendering. In CVPR , 2024.\n[19] Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, and Yu Yin. Segment\nthen splat: A unified approach for 3d open-vocabulary segmentation based on gaussian splatting.\narXiv preprint arXiv:2503.22204 , 2025.\n[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In CVPR , 2022.\n[21] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoor-\nthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.\nCommunications of the ACM , 2021.\n[22] Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra,\nand Nima Khademi Kalantari. Coherentgs: Sparse novel view synthesis with coherent 3d\ngaussians. In ECCV , 2024.\n[23] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool,\nand Fisher Yu. Unidepth: Universal monocular metric depth estimation. In CVPR , 2024.\n[24] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d\nlanguage gaussian splatting. In CVPR , 2024.\n[25] Jiaxiong Qiu, Liu Liu, Zhizhong Su, and Tianwei Lin. Gls: Geometry-aware 3d language\ngaussian splatting. arXiv preprint arXiv:2411.18066 , 2024.\n[26] Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Feature splatting: Language-driven\nphysics-based scene synthesis and editing. arXiv preprint arXiv:2404.01223 , 2024.\n[27] Yansong Qu, Shaohui Dai, Xinyang Li, Jianghang Lin, Liujuan Cao, Shengchuan Zhang,\nand Rongrong Ji. Goi: Find 3d gaussians of interest with an optimizable open-vocabulary\nsemantic-space hyperplane. In ACM MM , 2024.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML , 2021.\n[29] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,\nHaitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything\nin images and videos. arXiv preprint arXiv:2408.00714 , 2024.\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR , 2022.\n[31] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR ,\n2016.\n[32] Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu,\nShaohui Lin, and Rongrong Ji. Aligning and prompting everything all at once for universal\nvisual perception. In CVPR , 2024.\n[33] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and Shao-Hua Guan. Language embedded 3d\ngaussians for open-vocabulary scene understanding. In CVPR , 2024.\n[34] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot\ngaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912 , 2024.\n[35] Wei Sun, Yanzhao Zhou, Jianbin Jiao, and Yuan Li. Cags: Open-vocabulary 3d scene under-\nstanding with context-aware gaussian splatting. arXiv preprint arXiv:2504.11893 , 2025.\n[36] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r:\nGeometric 3d vision made easy. In CVPR , 2024.\n14\n--- Page 15 ---\n[37] Xingrui Wang, Cuiling Lan, Hanxin Zhu, Zhibo Chen, and Yan Lu. Gsemsplat: Generalizable\nsemantic 3d gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2412.16932 ,\n2024.\n[38] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping\nLuo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation.\nInACM SIGGRAPH 2024 Conference Papers , 2024.\n[39] Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao,\nHaocheng Feng, Errui Ding, Jingdong Wang, et al. Opengaussian: Towards point-level 3d\ngaussian-based open vocabulary understanding. arXiv preprint arXiv:2406.02058 , 2024.\n[40] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape\nguided object inpainting with diffusion model. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 22428–22437, 2023.\n[41] Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, and Achuta Kadambi.\nSparsegs: Real-time 360 {\\deg}sparse view synthesis using gaussian splatting. arXiv preprint\narXiv:2312.00206 , 2023.\n[42] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello.\nOpen-vocabulary panoptic segmentation with text-to-image diffusion models. In CVPR , 2023.\n[43] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.\nDepth anything: Unleashing the power of large-scale unlabeled data. In CVPR , 2024.\n[44] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang\nZhao. Depth anything v2. NeurIPS , 2024.\n[45] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit\nanything in 3d scenes. In ECCV , 2024.\n[46] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: A\nhigh-fidelity dataset of 3d indoor scenes. In ICCV , 2023.\n[47] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die\nhard: Open-vocabulary segmentation with single frozen convolutional clip. NeurIPS , 2023.\n[48] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao,\nTien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models\nfor high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048 , 2024.\n[49] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting:\nAlias-free 3d gaussian splatting. In CVPR , 2024.\n[50] Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou, and Ming-Ming Cheng.\nMaskclip++: A mask-based clip fine-tuning framework for open-vocabulary image segmentation.\narXiv preprint arXiv:2412.11464 , 2024.\n[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models. In ICCV , 2023.\n[52] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view\nsynthesis using gaussian splatting. In ECCV , 2024.\n15",
  "text_length": 55498
}