{
  "id": "http://arxiv.org/abs/2505.24830v1",
  "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
  "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
  "authors": [
    "Juraj Vladika",
    "Annika Domres",
    "Mai Nguyen",
    "Rebecca Moser",
    "Jana Nano",
    "Felix Busch",
    "Lisa C. Adams",
    "Keno K. Bressem",
    "Denise Bernhardt",
    "Stephanie E. Combs",
    "Kai J. Borm",
    "Florian Matthes",
    "Jan C. Peeken"
  ],
  "published": "2025-05-30T17:33:07Z",
  "updated": "2025-05-30T17:33:07Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24830v1",
  "full_text": "1 Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs Juraj Vladika1, Annika Domres2, Mai Nguyen2, Rebecca Moser2, Jana Nano2, Felix Busch3, Lisa C. Adams3, Keno K. Bressem3,4, Denise Bernhardt2, Stephanie E. Combs2,5,6, Kai J. Borm2, Florian Matthes1, Jan C. Peeken2,5,6 1 Department of Computer Science; TUM School of Computation Information and Technology; Technical University of Munich, Garching, Germany 2 Department of Radiation Oncology, TUM University Hospital Rechts der Isar, TUM School of Medicine and Health, Technical University of Munich, Munich, Germany 3 Department of Diagnostic and Interventional Radiology, TUM University Hospital Rechts der Isar, TUM School of Medicine and Health, Technical University of Munich, Munich, Germany 4 Institute for Cardiovascular Radiology and Nuclear Medicine, TUM University Hospital, German Heart Center Munich, TUM School of Medicine and Health, Technical University of Munich, Munich, Germany 5 Institute of Radiation Medicine (IRM), Helmholtz Zentrum München (HMGU) 6 German Consortium for Translational Cancer Research (DKTK), Partner Site Munich, Munich, Germany Corresponding Authors: Jan C Peeken, MD PhD Email: jan.peeken@tum.de Address: Department of Radiation Oncology, TUM School of Medicine and Health, TUM University Hospital Rechts der Isar, Munich, Germany 2 Introduction: Large language models (LLMs) exhibit extensive medical knowledge.1 However, LLMs are prone to hallucinations that may lead to harmful medical advice, and their often inaccurate citations reduce overall explainability.2 Such limitations also complicate medical product certifications.3 Current methods such as Retrieval Augmented Generation (RAG) partially address these issues by grounding answers in source documents.4 In RAG, source texts are divided into discrete chunks, embedded into a vector space, and retrieved as needed to ground LLM responses in updatable, authoritative information. Prior research shows that RAG can improve answer quality in medical Q&A.5 Nevertheless, hallucinations persist, and fact-by-fact explainability remains low, especially for complex medical queries. Existing approaches do not address the need for validating, backtracing and correcting each individual claim within a long-form response. Methods to increase the factuality of LLM outputs can involve additional pre-training or fine-tuning, which is computationally expensive and impossible for closed-source models. Another emerging direction are the post-hoc methods, where LLMs self-correct their responses only after they are generated.6 A promising method is fact-checking, which detects individual facts from the generated response that contain information contradicting the established knowledge and rewrites the facts and final responses using correct information. Originating from journalism where it is performed manually, automated fact-checking is increasingly used for hallucination correction.7 However, these approaches mostly focus on the news domain and are underexplored in medical applications.8 We define an atomic fact as the smallest self-contained and verifiable unit of information in a response generated by a LLM9 (e.g. “Trastuzumab is indicated for HER2-positive breast cancer”). We developed a framework that decomposes responses into atomic facts, each of which is independently verified against an authoritative vector database. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. We extensively evaluated the framework using multi-reader assessments by medical experts and an automated open Q&A benchmark, demonstrating substantial improvements in factuality and explainability over baseline RAG-based LLM systems. 3 Methods: Atomic Fact-checking Framework The LLM-TRIPOD checklist is provided in the Supplement. Our fact-checking framework consists of five steps, as shown in Figure 1: (1) generate an initial RAG-based response to the question; (2) split the response into atomic facts; (3) determine the veracity of each fact (categories: “TRUE” and “FALSE”); (4) rewrite the facts detected as incorrect and loop through 3 and 4 until all facts are correct or a maximum of three iterations; (5) rewrite the full response by incorporating the new facts. All five steps are implemented with dedicated LLM prompts, provided in Table S1 in Supplementary materials. For steps 1-4, in-context learning (ICL) with four expert-annotated examples per prompt is used to enforce consistent atomic fact extraction and verdicting. All prompts and examples were created by a medical expert (JCP, 9 years experience in Radiation Oncology). The knowledge base consisted of curated oncological guideline documents (see Supplementary Table S4). All PDF documents were converted into plain text using the open-source library PaperMage. Text was chunked into overlapping segments of 512 tokens (100-token overlap) and embedded using S-PubMedBERT,10 a transformer model pre-trained on PubMed abstracts. Chunks were stored in a ChromaDB vector database. For evidence retrieval, cosine similarity was used to select the most relevant chunks for each atomic fact. All LLM generations were performed using GPT-4o (gpt-4o-2024-11-20) via the OpenAI API with temperature set to 0. 4 Figure 1. Architecture diagram Illustration of the atomic fact checking process. Q&A Benchmark The main task of this study was long-form medical question-answering. Two Q&A datasets were generated, each comprising distinct questions on the diagnosis and treatment of prostate or breast cancer. Questions were based on the guideline content provided within the RAG system. They were categorized as fact-based (direct guideline knowledge) or patient-based (clinical vignettes), with varying complexity. To select the optimal prompting strategy, a validation set of 40 Q&A pairs on prostate cancer was used. The final configuration was tested on a separate set of 60 Q&A pairs (30 prostate, 30 breast cancer). To assess the framework in a clinical setting, 80 anonymized real patient cases from a multidisciplinary tumor board were included (ethics approval 2023-626_1-S-NP). All Q&A sets are available in the Supplement. 5 Human Evaluation Fact-checking performance was evaluated by comparing assigned verdicts (“TRUE”/“FALSE”) to human assessments. Human evaluation involved verifying the correctness of each verdict and categorizing errors as hallucinations, incorrect information, missing information, or lack of context. Each atomic fact correction was analyzed for correctness, while uncorrected facts were considered false negatives. Overall answers were compared to initial responses to determine whether fact-checking led to improvement, deterioration, or no change. Validation and tumor board analyses were conducted by one medically trained scientist (AD) supervised by JCP. Test set evaluation used four independent blinded physician raters (AD, FB, JN, MN), with majority voting; disagreements (2 vs. 2) were resolved by a blinded fifth rater (JCP). Inter-rater agreement was 85%, with ties in 4%. The following metrics were computed: True Positive (TP, verdict “FALSE” confirmed by human), True Negative (TN, verdict “TRUE” confirmed by human), False Positive (FP, verdict “FALSE” not confirmed by human), and False Negative (FN, verdict “TRUE” not confirmed by human). Standard confusion matrix metrics (sensitivity, specificity, precision, F1) were calculated (see Table 1, Figure 2). Auto Evaluation The framework was further evaluated on the AMEGA benchmark by Fast et al.,11 which tests LLM guideline adherence across 20 clinical domains. This benchmark comprises 20 patient cases (6–8 questions each, 137 total), with 1,307 predefined evaluation criteria. In the original study, responses were improved by “question re-asking”; here, we instead applied our atomic fact-checking pipeline and compared original versus corrected answers using the same criteria. All LLM outputs used GPT-4o (gpt-4o-2024-11-20), and for each case, the relevant medical guidelines served as a retrieval database. Results: Across all evaluation sets, answers were split into a median number of 7 atomic facts (interquartile range: 5–8.75), yielding a total number of 428, 404, 519 facts for the validation (50 Q&A), testing (60 Q&A), and tumorboard (80 Q&A) test sets, respectively. Evaluation on the validation set revealed the best architectural strategy as four shot prompting for answer generation, verdicting, and fact rewriting (ablation study: Supplemental table S6, Figure S1). Retrieving new chunks instead of using the same 6 chunks (from initial Q&A) for fact-checking and rewriting increased the overall performance (both precision and sensitivity). Looping through all facts labelled as “FALSE” further increased the true positive rate, and positive predictive value while reducing the falsification of facts. The average false positive rate over three evaluation sets reduced from 2% to 1% to 0% throughout 3 iterations of the correction loop. Ensembling for verdicting and changes in temperature did not yield a significant gain in performance (supplemental figures S2, S3). In the validation set and test Q&A sets, this final framework achieved balanced accuracy scores of 87 % and 74%, with a hallucination detection of 50 % and 38 %, and an inaccuracy detection of 50% and 25%, respectively (Table 1). In the more complex tumor board test set, 25% of hallucinations and 24% of inaccuracies were found with balanced accuracies of 78%. The positive predictive value for false fact identification was 66%, 100%, and 97% for the validation, test, and tumor board set, respectively. Overall answer improvements were seen in 20%, 12%, 40% of cases in the validation, test, and tumor board Q&A sets (Figure 2). Overall answer quality decreased in 8%, 2%, and 6 % of cases. The AMEGA auto evaluation analysis revealed a significant improvement in answer quality by applying our fact checking algorithm in contrast to RAG only for all LLMs tested (p<0.001 for 8/11 models, p<0.01 for 2/11, p<0.05 for 1/11) (Figure 3A, Supplemental table S5). The overall best answer quality among non-reasoning models was achieved by GPT4o-mini and Mistral 24B, with fact-checking each. The largest improvement with fact-checking was seen for Llama 3.2 3B. A significant negative correlation was found between the logarithm of the LLM model parameter size and model performance (Pearson Correlation: -0.754, p-value: 0.031). Even for the reasoning model OpenAI-o1, the performance was significantly increased (p<0.001) while achieving the overall best model performance. To assess the explainability of the atomic fact evaluation, we compared on test set three similarity definitions of the best-fitting chunks selected from the vector database. A simple Chain-of-Thought (CoT) prompt using GPT4o identified the correct chunk in 75% as the first choice and in 91.9% among the top three chunks, outperforming a complex CoT prompt and cosine similarity with a text-embedding model (Figure 4, Supplemental table S3 for prompts). 7 Table 1: Human Evaluation Results on Three Benchmark Datasets. We present results of different metrics as rated by medical expert annotators over our three constructed Q&A datasets. Numbers in percent Validation Set Test Set Tumor Board Sensitivity (Recall) 78 47 44 Specificity 96 100 100 Precision (PPV) 66 100 97 F1-score 71 64 60 Balanced Accuracy 87.3 73.7 91.0 TP improved atoms 100 100 100 FP falsified atoms 0 0 0 Hallucination rate 1 2 1 Hallucination detection 50 38 25 Inaccuracy rate 3 1 5 Inaccuracy detection 58 50 24 Abbreviations: TP: True Positives, FP: False Positives, PPV: Positive predictive value Figure 2: Change in Final Answer Quality. We show how the overall answer quality changed when compared the initial answer and final fact-checked answer. This is shown for our three evaluation Q&A dataset in three directions (improved, equal, worse), as rated by human expert annotators. 8 B) Figure 3: Results of Auto-Evaluation on the AMEGA Benchmark. (A) Scores for 11 different open-source LLMs, without and with answer fact-checking. (B) Fitted linear curve of the natural logarithm of LLM parameter size in billions vs. the average absolute score improvement of using fact-checking for that model. 9 Figure 4: Explainability of Atomic Facts. This chart shows the percentage of times that the most relevant information for an atomic fact was found in a chunk ranked in the i-th position. We compare (A) a simple LLM prompt (“rank the chunks”, see Table S3), (B) a complex LLM prompt using chain-of-thought reasoning and scoring (see Table S3), and (C) the cosine similarity between the fact and chunks, both embedded with a text embedding model S-PubMedBERT. Discussion: Our study introduces a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical Q&A. By decomposing LLM-generated responses into discrete atomic facts and rigorously verifying each against an authoritative vector database, the framework significantly reduced hallucinations and inaccuracies. Medical expert assessment and automated benchmarks demonstrated notable improvements in factual accuracy, achieving up to 40% overall answer improvement and 50% hallucination detection rate. Additionally, the framework achieved high explainability by tracing each atomic fact back to the most relevant chunks from the database, providing a granular, transparent explanation of the generated responses. Our framework increased the factual accuracy and overall quality of LLM-generated responses. Numerical hallucinations, such as incorrect drug dosages, were frequently identified and corrected, as were entity hallucinations, such as the conflation of different treatment procedures. In a small proportion of cases (2% and 6% across the test sets), 10 answer quality declined due to the retrieval of less relevant chunks or potential hallucinations introduced during the correction process. However, since the proportion of answers that improved (12% and 40%) was substantially higher, the overall effect of fact-checking clearly enhanced answer quality. Different degrees of improvement come down to varying complexity of questions across datasets and to different rates of atomic facts identified as false and corrected. RAG-based medical chatbots may qualify as Class IIa medical devices according to the European Medical Device Regulation.3 Trustworthiness, the combination of explainability, traceability, and transparency, is a key prerequisite required under the MDR.12 Although LLMs inherently explain their responses, these justifications can be misleading. Our framework provides backtracking to its source, achieving improved explainability, traceability, and transparency on the fact level. Importantly, this process is outsourced from the internal reasoning of LLMs and directed towards a potential user. We could demonstrate that the benefit of our framework was greater in smaller LLMs. This is especially important, as the fact-checking algorithm is of even higher use in scenarios where smaller LLMs are deployed on-premises in medical institutions. Our work has several limitations. As we sought to evaluate open Q&A capabilities, we designed novel evaluation datasets. As this evaluation process is time-consuming, we had to restrict our Q&A to 120 Q&As in total. These Q&As were based on information present in the provided guideline documents. We therefore included real anonymized patient cases from tumor boards as an even more realistic application. Furthermore, all the pipeline steps rely on LLM generation. It is possible that errors can propagate from one step to another. Future work could explore approaches to make the process more rigorous, such as Graph-RAG techniques of grounding data into graph structures for more robust generation. To conclude, we present a novel atomic fact-checking algorithm that identifies fact inaccuracies and hallucinations. Correcting these findings improves the overall answer quality while achieving fact-wise explainability, paving the way for more trustworthy clinical use of LLMs. 11 References 1. Han T, Adams LC, Bressem KK, Busch F, Nebelung S, Truhn D. Comparative Analysis of Multimodal Large Language Model Performance on Clinical Vignette Questions. JAMA. 2024;331:1320–1. 2. Masanneck L, Meuth SG, Pawlitzki M. Evaluating base and retrieval augmented LLMs with document or online support for evidence based neurology. NPJ Digit Med. 2025;8:137. 3. Freyer O, Wiest IC, Kather JN, Gilbert S. A future role for health applications of large language models depends on regulators enforcing safety standards. Lancet Digit Health. 2024;6:e662–72. 4. Ng KKY, Matsuba I, Zhang PC. RAG in Health Care: A Novel Framework for Improving Communication and Decision-Making by Addressing LLM Limitations. NEJM AI. 2025;2:AIra2400380. 5. Ferber D, Wiest IC, Wölflein G, Ebert MP, Beutel G, Eckardt JN, et al. GPT-4 for Information Retrieval and Comparison of Medical Oncology Guidelines. NEJM AI [Internet]. 2024 [cited 2025 May 6];1. Available from: https://ai.nejm.org/doi/10.1056/AIcs2300235 6. Kamoi R, Zhang Y, Zhang N, Han J, Zhang R. When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs. Trans Assoc Comput Linguist. 2024;12:1417–40. 7. Augenstein I, Baldwin T, Cha M, Chakraborty T, Ciampaglia GL, Corney D, et al. Factuality challenges in the era of large language models and opportunities for fact-checking. Nat Mach Intell. 2024;6:852–63. 8. Vladika J, Matthes F. Scientific Fact-Checking: A Survey of Resources and Approaches. In: Findings of the Association for Computational Linguistics: ACL 2023 [Internet]. Toronto, Canada: Association for Computational Linguistics; 2023. p. 6215–30. Available from: https://aclanthology.org/2023.findings-acl.387 9. Min S, Krishna K, Lyu X, Lewis M, Yih W tau, Koh P, et al. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing [Internet]. Singapore: Association for Computational Linguistics; 2023 [cited 2025 May 20]. p. 12076–100. Available from: https://aclanthology.org/2023.emnlp-main.741 10. Deka P, Jurek-Loughrey A, P. D. Improved Methods to Aid Unsupervised Evidence-Based Fact Checking for Online Heath News. J Data Intell. 2022;3:474–504. 11. Fast D, Adams LC, Busch F, Fallon C, Huppertz M, Siepmann R, et al. Autonomous medical evaluation for guideline adherence of large language models. Npj Digit Med. 2024;7:1–14. 12. Li B, Qi P, Liu B, Di S, Liu J, Pei J, et al. Trustworthy AI: From Principles to Practices. ACM Comput Surv. 2023;55:1–46.",
  "text_length": 19245
}