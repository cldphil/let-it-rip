{
  "id": "http://arxiv.org/abs/2506.01042v1",
  "title": "Probing Neural Topology of Large Language Models",
  "summary": "Probing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable\nsemantics. However, how neurons functionally co-activate with each other to\ngive rise to emergent capabilities remains largely unknown, hindering a deeper\nunderstanding and safer development of LLMs. In this work, we introduce graph\nprobing, a method for uncovering the functional connectivity topology of LLM\nneurons and relating it to language generation performance. By analyzing\ninternal neural graphs across diverse LLM families and scales, we discover a\nuniversal predictability of next-token prediction performance using only neural\ntopology. This predictability is robust even when retaining just 1% of neuron\nconnections or probing models after only 8 pretraining steps, highlighting the\nsparsity and early emergence of topological patterns. Further graph matching\nanalysis suggests that, despite significant distinctions in architectures,\nparameters, and training data, different LLMs develop intricate and consistent\nneural topological structures that may form the foundation for their language\ngeneration abilities. Codes and data for the graph probing toolbox are released\nat https://github.com/DavyMorgan/llm-graph-probing.",
  "authors": [
    "Yu Zheng",
    "Yuan Yuan",
    "Yong Li",
    "Paolo Santi"
  ],
  "published": "2025-06-01T14:57:03Z",
  "updated": "2025-06-01T14:57:03Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01042v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01042v1  [cs.CL]  1 Jun 2025Probing Neural Topology of Large Language Models\nYu Zheng1Yuan Yuan2Yong Li2Paolo Santi1\n1Masssachusetts Institute of Technology, Cambridge, MA USA\n2Tsinghua University, Beijing, China\nyu_zheng@mit.edu\nAbstract\nProbing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable semantics.\nHowever, how neurons functionally co-activate with each other to give rise to emer-\ngent capabilities remains largely unknown, hindering a deeper understanding and\nsafer development of LLMs. In this work, we introduce graph probing, a method\nfor uncovering the functional connectivity topology of LLM neurons and relating it\nto language generation performance. By analyzing internal neural graphs across di-\nverse LLM families and scales, we discover a universal predictability of next-token\nprediction performance using only neural topology. This predictability is robust\neven when retaining just 1% of neuron connections or probing models after only 8\npretraining steps, highlighting the sparsity and early emergence of topological pat-\nterns. Further graph matching analysis suggests that, despite significant distinctions\nin architectures, parameters, and training data, different LLMs develop intricate\nand consistent neural topological structures that may form the foundation for their\nlanguage generation abilities. Codes and data for the graph probing toolbox are\nreleased at https://github.com/DavyMorgan/llm-graph-probing .\n1 Introduction\nLarge language models (LLMs) exhibit remarkable generative capabilities [ 53,50,19,47,22], yet\nour understanding of how they succeed and what they have learned remains limited [ 45].Probing ,\nwhich extract interpretable features from neural activations [ 1], has emerged as a powerful approach\nfor reverse-engineering LLMs [ 5,24]. For instance, Gurnee et al. [24] showed that LLMs encode a\ncompact world model of space and time using linear regression probes. Unsupervised probing, such\nas sparse auto-encoders [ 13,18,41,33,39], have further revealed dictionaries of interpretable, mono-\nsemantic concepts [ 25] and even causal circuits [ 36], corresponding to directions in neural latent space.\nWhile these advances shed light on the semantics of individual neurons and representations [ 45],\nmuch less is known about how neurons are functionally connected, i.e.the neural topology, which is\nbelieved to play an essential role in the emergence of intelligence [42, 3].\nRecent studies have drawn compelling parallels between neurons in LLMs and those in the human\nbrain [ 49,44,11,31,42,38,51,8,46,34], revealing shared properties such as spatial-functional\norganization [ 31,42] and left lateralization [ 8]. Neural activations at internal layers of LLMs have\nalso been shown to reliably predict human brain responses given the same linguistic stimuli [ 44,\n51,35]. However, these efforts primarily focus on static neural representations of LLMs, while\noverlooking the key aspect of temporal and functional neural topology that has been studied in\nneuroscience for decades [ 4,3,15]. Moreover, although analogies between LLMs and human brains\nare insightful [ 49,21], few works explicitly connect these findings to LLMs’ language generation\nperformance, which is one of the primary indicators of an LLM’s intelligence.\nIn this work, we introduce graph probing , a novel approach for investigating the functional connec-\ntivity of neurons in LLMs and its relationship to language generation performance. By analyzing\nneural activity time series as LLMs process text token by token, we compute temporal and functional\nPreprint. Under review.\n--- Page 2 ---\n0.00.20.40.60.81.00.00.20.40.60.81.0\nPredicted PerplexityTrue Perplexity\nTNeuronsAttention Layers\nAttention Layers𝑥!𝑥\"𝑥#𝑥$𝑥%𝑥&\"𝑥\"\"𝑥#\"𝑥$\"𝑥%\"𝑥&\nLanguage ModelGraph ProbingPPLX=𝑒'\"(∑!\"*+,-#.!|.$!\nNeuron Activity Time SeriesNeural Topology𝑋\"[𝑥!,…,𝑥(]…Language Model Inference………PPL𝑋\"PPL𝑋#PPL𝑋0NeuronsPredicted PerplexityTrue PerplexityAVG POOLINGMAX POOLINGMLPGraph Convolutional Network\n𝑋#[𝑥!,…,𝑥(]𝑋0[𝑥!,…,𝑥(]TNeurons𝐇!𝐀!\nTNeurons𝐇\"TNeurons𝐇#𝐀\"𝐀#𝑋TNeurons\nTNeurons𝐇$𝐇%𝑎$%=𝜌𝐇$,𝐇%=cov(𝐇$,𝐇%)𝜎𝐇!𝜎𝐇\"𝐀∈ℝ1×1Perplexity\nGraph Probe\nNeural topology describes neurons’ functional connectivity.topology describes neurons’ functional connectivity.\n𝐇!𝐇'…Functional CorrelationNODE EMBEDDINGSFigure 1: An overview of our graph probing method. We extract the time series of neuron activities\nfor each attention layer in an LLM as it processes text token by token. We then compute temporal\nand functional correlations between neural activations to obtain topological connectivity graphs of\nneurons. GNN-based probes are trained to predict the perplexity of the auto-regressive prediction for\nthe input token sequence.\ncorrelations between neurons to construct dynamic neural graphs. Using this large-scale dataset of\ntext-induced neural topology, we train graph neural network (GNN) [ 29,14] as probes to predict\nLLMs’ accuracy in auto-regressively generating the corresponding text. In essence, graph probing\nconnects the micro-level topology of how neurons are connected given a token sequence, to the\nmacro-level performance of how well LLMs predict these tokens, offering a new lens to study the\nemergent capabilities of LLMs. Our method is summarized in Figure 1 and described in Section 2.\nWe then apply our graph probing framework to comprehensively analyze the neural topology of LLMs\nthrough extensive experiments. First, we demonstrate that auto-regressive language generation per-\nformance can be reliably predicted using only the neural connectivity graph. This predictability holds\nuniversally across LLM families and scales, with empirical results spanning GPT [ 40], Pythia [ 7],\nand Qwen [ 54], ranging from millions to billions of parameters (Section 3.1). Next, we show that\nthese neural topologies are (1) sparse , given strong predictive performance when preserving only 1%\nof neuron connections (Section 3.2), (2) non-linearly related to language generation performance, as\nnon-linear graph probing significantly outperforms linear baselines (Section 3.3), and (3) early emerg-\ningwith predictive topological structures arising within just 8 steps of LLM pretraining (Section 3.4).\nFinally, we use our probes to perform cross-model contrastive graph matching, revealing that distinct\nLLMs converge toward similar internal neural topologies, suggesting shared underlying principles\ndespite large discrepancies in architectures, parameters, and training data (Section 4). While not\nwithout limitations, we expect graph probing to provide valuable insights into the inner workings of\nLLMs and to guide their future development in an interpretable and safe manner.\n2 Graph Probing\nNeural Topology. To construct neural graphs from LLMs, we draw inspiration from neuroscience\nwhere functional brain networks are derived from temporal correlations in fMRI or EEG activation\nsignals [ 3,52,10], as shown in Figure 1. Formally, given an LLM composed of stacked attention\nlayers, the neural topology is constructed as follows:\nNeural Activity: H=HIDDEN _STATE (LLM(X)) = [h0,h1, . . . ,ht]∈Rn×t, (1)\nNeural Topology: A=\u0000\naij\u0001\n∈Rn×n, (2)\naij=ρ(Hi,:,Hj,:) =Pt\nk=0(Hi,k−Hi,:)(Hj,k−Hj,:)qPt\nk=0(Hi,k−Hi,:)2qPt\nk=0(Hj,k−Hj,:)2,(3)\nwhere neurons at each layer produce a time series of hidden states Has the model processes a token\nsequence X= [x0, x1, . . . , x t], and the temporal co-activation patterns among neurons define their\n2\n--- Page 3 ---\n0.01.02.03.04.05.06.0−7−6−5−4−3−2−10\n20406080100120140ppl\nlog(k)log(P(k))\n0.200.250.30101520253035404550\n020406080100120count\nNetwork densityPerplexity(a)(b)3.21703.09484.34844.58844.5945\n0.01220.03340.06340.10090.14301%2%3%4%5%0.00.51.01.52.02.53.03.54.04.5high degreelow degree\nIntervention RatioLoss263X92X68X45X32X\n(c)Figure 2: Analysis of topological properties and their relationship to language generation performance.\n(a) Degree distributions of neural connectivity graphs induced by 10 different text inputs, with\nperplexity indicated by color darkness. (b) 2D histogram showing the relationship between token\nprediction perplexity and network density across a text dataset. (c) Perplexity increase resulting from\ninterventions on neurons with high- vs. low-degree. Specifically, we disable the top/bottom k% of\nneurons by setting their activations to zero during inference.\nfunctional connectivity . We capture this through a complete n×nweighted connectivity matrix A,\nwhere each node corresponds to a neuron and each edge weight aijrepresents the Pearson correlation\ncoefficient between the activation time series of two neurons [ 3,15]. Meanwhile, the LLM is trained\nfor auto-regressive next-token prediction, with performance commonly measured by perplexity [ 6]\nwhich corresponds to the exponentiated average negative log-likelihood over the token sequence:\nPerplexity: PPL(X) = exp(\n−1\nttX\ni=1logpθ(xi|x<i))\n. (4)\nThe graph is dynamically induced by the specific token sequence, and our goal is to investigate\nwhether the text-responsive neural topology is linked to how well the model predicts the text.\nA trivial approach to characterizing graphs is to compute heuristic topological properties commonly\nused in network analysis [ 2], such as degree distribution1and network density2. However, these basic\nproperties tend to exhibit no clear or intuitive relationship with the model’s performance–samples\nwith drastically different perplexities can display similar degree distributions or network densities,\nas shown in Figure 2(a-b) obtained by feeding a text dataset (details in Section 3) into the GPT-2\nmodel [ 40]. Nevertheless, when we intervene on neurons with high degree by forcing their activations\nto zero during inference, we observe a substantial increase in perplexity (up to 263 ×), compared to\ninterventions on low-degree neurons (Figure 2(c)). This suggests that while neural topology indeed\nplays a crucial role in determining next-token prediction performance, the relationship is highly\nnon-trivial and cannot be adequately explained by simple, handcrafted topological metrics.\nProbing with GNN. To better characterize the complex interplay between neural topology and\nperplexity, we propose graph probing , a method that learns representations of neural connectivity\ngraphs to predict corresponding language generation performance, as illustrated in Figure 1. Specifi-\ncally, we adopt a GNN-based probe that encodes each node by aggregating neighborhood information\nthrough convolutional message passing on the graph [ 29,14]. We employ the ReLU activation\nfunction [ 16] between graph convolution layers and use both average and maximum pooling to\nsummarize node-level embeddings into a graph-level representation. Given a connectivity matrix\nAinduced by feeding a tokenized sequence Xto an LLM, where each element aijdenotes the\nfunctional connectivity (Pearson correlation coefficient) between neurons iandj, our probe produces\nthe graph representation zas follows:\nΦL=ReLU(AΦL−1ΘL), . . . ,Φ0∈Rn×d, (5)\nz=AVG_POOLING {ΦL\n1,:, . . . ,ΦL\nn,:} ∥MAX_POOLING {ΦL\n1,:, . . . ,ΦL\nn,:}, (6)\nwhere Φ0∈Rn×ddenotes the initial learnable node embeddings, Θl∈Rd×dis the weight matrix\nof the l-th layer in the GNN with Ltotal layers, and dis a hidden dimensionality hyperparameter. We\n1The degree of a node is the sum of its connectivity strengths: di=P\nj∥aij∥, where we take the absolute\nvalue since correlations can be either positive or negative within the range [−1,1].\n2Network density is defined as the total connectivity strength:P\ni,j∥ai,j∥, again using the absolute value of\nPearson correlation coefficients.\n3\n--- Page 4 ---\nthen feed the graph representation z∈R2dinto a multi-layer perceptron (MLP) [ 43] to predict the\nperplexity associated with the input tokenized sequence X:\nˆp=W2·ReLU(W1·zT), (7)\nwhere ˆpis the predicted perplexity, and W1∈R2d×d,W2∈Rd×1are learnable weights of the\nMLP. Our graph probe is trained to minimize the mean squared error (MSE) between predicted and\ntrue perplexities over a dataset of tokenized sequences X={X1, . . . , X N}:\nL(X) =1\nNNX\ni=1(ˆpi−PPL(Xi))2. (8)\nFor details of graph probing including hyper-parameter and computer resources, see Appendix A.\n3 Results\nWith graph probing, we aim to understand whether the auto-regressive token-prediction performance\nof LLMs is associated with their probed internal neural topologies. If such a relationship exists,\ndoes it generalize across different model families and scales? Furthermore, how and when is the\ndependence between these two seemingly distant aspects established?\nLLMs. In our experiments, we train graph probes on neural topology derived from three families\nof LLMs, each spanning across different sizes. Specifically, we evaluate GPT2 [ 40] (GPT2, GPT2-\nmedium, GPT2-large), Pythia [ 7] (160M, 410M, 1.4B, 2.8B, 6.9B, 12B), and Qwen2.5 [ 54] (0.5B,\n3B, 7B, 14B). Details of the experimented LLMs are provided in Appendix B.\nDatasets. To enable our study, we construct neural connectivity graphs using the text corpora on\nwhich LLMs were pretrained. Specifically, we use the Pile dataset [ 17] for Pythia models, and the\nOpenWebText dataset [ 20] for GPT2 and Qwen2.5 models. To ensure consistent temporal resolution,\nwe control the length of neural activity time series to fall between 256 and 1024 tokens by merging\nconsecutive sentences as needed. For each token sequence, we perform LLM inference to compute\nits perplexity and simultaneously extract hidden state time series to generate the corresponding neural\ntopology. For each model, we construct a probing dataset comprising about 10,000 graph–perplexity\npairs. Further details on dataset construction are provided in Appendix C.\nEvaluation. We split the dataset into training and test sets using an 8:2 ratio. Having learned graph\nprobes on the training set, we evaluate their out-of-sample graph regression performance on the\ntest set, which reveals the extent to which micro-level neural topology is predictive of macro-level\nlanguage generation ability. To quantify the effectiveness of graph probing, we report standard\nregression metrics on our test data, including mean squared error (MSE), mean absolute error (MAE),\ncoefficient of determination ( R2), Pearson correlation ( ρp), and Spearman rank correlation ( ρs).\n3.1 Predictability\nWe show our graph probing results in Figure 3, which exhibit consistently strong predictability across\nall three LLM families. Particularly, graph probing achieves 0.95 in ρpandρs, and 0.90 in R2on\n0.00.20.40.60.81.00.00.20.40.60.81.0\nGPT-2\nPredicted PerplexityTrue PerplexityR² = 0.8467Pearson r = 0.9204Spearman r = 0.92400.00.20.40.60.81.00.00.20.40.60.81.0\nPythia-160M\nPredicted PerplexityTrue PerplexityR² = 0.8929Pearson r = 0.9452Spearman r = 0.95150.00.20.40.60.81.00.00.20.40.60.81.0\nQwen2.5-0.5B\nPredicted PerplexityTrue PerplexityR² = 0.8902Pearson r = 0.9436Spearman r = 0.9458(a)(b)(c)\nFigure 3: Out-of-sample performance of graph probing on the test set for (a) GPT-2 (b) Pythia-160M\n(c) Qwen2.5-0.5B. The correlation between the perplexity predicted by graph probing and the ground-\ntruth perplexity reflects how well LLM performance can be inferred from neural topology.\n4\n--- Page 5 ---\nsmallmediumlarge0.00.20.40.60.81.0\nsmallmediumlargeR²Pearson rSpearman r\nGPT-2PythiaValue\n(a)(b)(c)1234567891011120.550.600.650.700.750.800.850.90R²Pearson rSpearman r\nLayer (GPT-2)Value\n1234567891011120.500.550.600.650.700.750.800.850.900.95R²Pearson rSpearman r\nLayer (Pythia-160M)ValueFigure 4: (a) Out-of-sample performance of graph probing on LLMs of different sizes including\nGPT2, GPT2-medium, GPT2-large, Pythia-160M, Pythia-410M, and Pythia-1.4B. (b-c) Out-of-\nsample performance of graph probing on each of the 12 layers in (b) GPT-2 and (c) Pythia-160M.\nboth Pythia-160M and Qwen2.5-0.5B. For GPT-2, graph probing reliably predicts perplexity with ρp\nandρsexceeding 0.92 and an R2score of 0.85. The high predictability is observed across LLMs\nof varying scales. As shown in Figure 4(a), models ranging from 124M to 1.4B parameters in the\nGPT-2 and Pythia families consistently achieve strong topology-perplexity correlations, with ρpand\nρsranging from 0.89 to 0.96.\nWe further train probes on neural connectivity graphs derived from different layers of GPT-2 and\nPythia-160M. Figures 4(b–c) show that neural topology at every layer is predictive of language\ngeneration performance, with the strongest predictability ( ρpandρsexceeding 0.91 and 0.93) found\nin the middle layers of both models. This observation is also aligned with previous probing studies\nthat identify the middle layers of LLMs as particularly informative and semantically rich [ 24,51].\nThe above graph probing experiments reveal that neural topology is a universal and strong predictor\nof LLMs’ language generation capabilities.\n3.2 Sparsity\nThe universal predictability observed so far is based on complete graphs, which are dense n×n\nconnectivity matrices that capture pairwise functional correlations between all neurons. Yet not\nall connections contribute equally to language generation performance, as suggested by our earlier\nintervention analysis comparing high- and low-degree nodes (Figure 2(c)). To explore this further,\nwe examine the distribution of edge weights in Figure 5(a), which reveals that the majority of\nconnections are weak, with absolute correlation values near zero. This motivates us to investigate\n00.20.40.60.810246810\nEdge Strength DistributionDensity(a)(b)\n(c)(d)(e)\n0.00.20.40.60.81.0sparsity = 0%\n0.00.20.40.60.81.0sparsity = 90%\n0.00.20.40.60.81.0sparsity = 99%\n0.00.20.40.60.81.00.550.600.650.700.750.800.850.90R²Pearson rSpearman r\nSparsity (GPT-2)Value\n0.00.20.40.60.81.00.650.700.750.800.850.900.95R²Pearson rSpearman r\nSparsity (Pythia-160M)Value\n0.00.20.40.60.81.00.500.600.700.800.90R²Pearson rSpearman r\nSparsity (Qwen2.5-0.5B)Value\nFigure 5: (a) Edge weight (absolute value) distribution for 10 neural graphs of GPT-2. (b) Neural\ntopology under different levels of sparsity, where weak connections are pruned by thresholding on the\nabsolute value of edge weight. (c-e) Out-of-sample graph probing performance on neural connectivity\ngraphs of different sparsity levels for (c) GPT-2 (d) Pythia-160M and (e) Qwen2.5-0.5B.\n5\n--- Page 6 ---\nwhether perplexity can still be predicted from sparse graphs with weakly correlated edges pruned\nout by thresholding, which is commonly employed in human brain network construction [ 3], as\nillustrated in Figure 5(b). To evaluate this, we train graph probes on neural topology with varying\nlevels of sparsification (Figures 5(c-e)). Surprisingly, the predictive performance remains remarkably\nstable even after removing up to 90% of the edges, with minimal degradation. Notably, even under\nextreme sparsity where only 1% of the original edges are retained, the neural topology still enables\neffective prediction of perplexity, achieving above 0.71 correlations in ρpandρs.\nTable 1: Out-of-sample graph probing performance\non sparse neural topologies derived from LLMs\ncontaining 2.8B to 14B parameters.\nLLM (Sparsity) R2ρp ρs\nPythia-2.8B (90%) 0.8995 0.9484 0.9592\nPythia-6.9B (90%) 0.9210 0.9599 0.9670\nPythia-12B (99%) 0.8974 0.9480 0.9527\nQwen2.5-3B (90%) 0.7051 0.8426 0.8372\nQwen2.5-7B (90%) 0.7699 0.8789 0.8823\nQwen2.5-14B (95%) 0.8249 0.9086 0.9167Graph probing on complete neural topology be-\ncomes computationally prohibitive as the LLM\nsize increases, due to the quadratic number of\nedges that directly impacts the computational\ncost in both time and memory. For instance,\nwhile complete graph probing is feasible for\nPythia-160M with 768 neurons and 0.6M edges\nper layer, the number of edges in Pythia-12B–\ncomprising 5,120 neurons per layer–explodes\nto over 26M per graph. Fortunately, the above\nexperiments suggest that most of the predictive\nsignal resides in a small subset of strong connec-\ntions, making it possible to significantly reduce the number of edges while preserving nearly all\ncritical topological information. Leveraging this insight, we scale up graph probing to much larger\nmodels by operating on sparsified neural topology. While our earlier results focused on models\nwith fewer than 1.4B parameters, we now train probes on sparse graphs derived from LLMs with\nup to 14B parameters. As shown in Table 1, graph probing continues to exhibit strong regression\nperformance across all six models, achieving a maximum accuracy of over 0.92 R2and over 0.96 ρs,\nproviding compelling evidence that the relationship between neural topology and language modeling\nperformance is universal across model sizes. Complete results of the predicted and groundtruth\nperplexities for all models are provided in Appendix D.\n3.3 Non-linearity\nWe next examine the complexity of the relationship between neural topology and language generation\nperformance. Specifically, we train graph probes with varying capabilities, under the hypothesis that\nmore expressive models can capture deeper and more nuanced topological patterns. We focus on\ntwo key factors: (1) the linearity of the probe, controlled by enabling or disabling the non-linear\nReLU activation function, and (2) the receptive field of the probe, determined by the number of graph\nconvolutional layers, L. Tables 2 and 3 show the out-of-sample regression performance of different\nprobe configurations on complete and sparse neural topology, respectively. We find that linear\nprobes, though still retaining considerable predictive power, consistently underperform compared to\nnon-linear probes, with a 31.4% increase in MSE and a drop of more than 0.05 in R2. This suggests\nthat the relationship between neural topology and language generation performance is inherently\nnon-linear. We also find that 1-hop GNNs perform best on complete graphs, whereas 2-hop GNNs\noutperform on sparse graphs, which is reasonable since the loss of local connectivity by sparsification\ncan be partially compensated by incorporating information from more distant neighbors, making a\nlarger receptive field beneficial. Results on more probe configurations can be found in Appendix E.\nTable 2: Out-of-sample performance using different probes on complete neural topologies.\nLLM Graph Probing MSE ↓MAE↓R2↑ ρp↑ ρs↑\nGPT-21-hop linear 0.0067 0.0629 0.7966 0.8930 0.8926\n1-hop non-linear 0.0051 0.0529 0.8467 0.9204 0.9240\n2-hop non-linear 0.0061 0.0563 0.8152 0.9036 0.9055\nPythia-160M1-hop linear 0.0036 0.0449 0.8894 0.9431 0.9475\n1-hop non-linear 0.0035 0.0421 0.8929 0.9452 0.9515\n2-hop non-linear 0.0050 0.0498 0.8465 0.9215 0.9305\nQwen2.5-0.5B1-hop linear 0.0046 0.0506 0.8596 0.9272 0.9264\n1-hop non-linear 0.0036 0.0438 0.8902 0.9436 0.9458\n2-hop non-linear 0.0051 0.0512 0.8447 0.9193 0.9221\n6\n--- Page 7 ---\nTable 3: Out-of-sample performance using different probes on sparse neural topologies where top\n10% functional connectivity are reserved (90% sparsity).\nLLM Graph Probing MSE ↓MAE↓R2↑ ρp↑ ρs↑\nGPT-21-hop linear 0.0098 0.0756 0.7034 0.8443 0.8439\n1-hop non-linear 0.0085 0.0690 0.7419 0.8627 0.8666\n2-hop non-linear 0.0077 0.0640 0.7667 0.8765 0.8789\nPythia-160M1-hop linear 0.0096 0.0741 0.7061 0.8431 0.8627\n1-hop non-linear 0.0087 0.0673 0.7329 0.8566 0.8833\n2-hop non-linear 0.0075 0.0612 0.7704 0.8780 0.8961\nQwen2.5-0.5B1-hop linear 0.0084 0.0706 0.7462 0.8652 0.8657\n1-hop non-linear 0.0076 0.0644 0.7695 0.8788 0.8786\n2-hop non-linear 0.0074 0.0623 0.7764 0.8849 0.8922\n3.4 Early Emergence\nHaving empirically validated that the relationship between neural topology and language genera-\ntion performance is universal, sparse, and non-linear, there still remains an open question: Is this\ndependence an inherent consequence of the LLM’s architectural design, or does it emerge during\npretraining? While definitively answering this question is challenging, in this section we offer an\nexploratory analysis from the perspective of LLMs’ learning process.\n(a)\n(b)2510251002510002510k25100k2010k20k30k40k50k60k\n0.20.30.40.50.60.70.80.9PerplexityR²Pearson rSpearman r\nStepAverage Perplexity\nTopology-Perplexity Predictability\n512510251002510002510k25100k2010k20k30k40k50k60k\n0.00.20.40.60.81.0PerplexityR²Pearson rSpearman r\nStepAverage Perplexity\nTopology-Perplexity Predictability\nFigure 6: Average perplexity and probing\nperformance throughout pretraining for (a)\nPythia-160M (b) Pythia-410M.To inspect how the dependence between neural topol-\nogy and perplexity evolves during pretraining, we\nperform graph probing on intermediate checkpoints\nof the Pythia models at various pretraining steps3.\nFigure 6 presents the probing performance through-\nout pretraining for Pythia-160M and Pythia-410M,\nalongside the corresponding average perplexity val-\nues across the dataset as an indicator of language\ngeneration capability. We find that, although it takes\nmore than 143,000 pretraining steps for the models\nto reduce their average perplexity from over 60,000\nto 21.02 (Pythia-160M) and 14.35 (Pythia-410M),\nthe predictability of perplexity from neural topology\nemerges much earlier. Notably, graph probing de-\ntects meaningful predictability after only 8 pretrain-\ning steps, when the average perplexity remains as\nhigh as 43,000 and 29,000, respectively. This sug-\ngests that LLMs may first establish informative neu-\nral topological structures which are indirectly opti-\nmized through parameter updates, before developing\nstrong next-token prediction capabilities built upon\nthat topology. In addition, the early emergence of this\ntopology–performance relationship opens up promis-\ning avenues for monitoring the learning trajectory of LLMs, detecting training failure at an early\nstage, designing effective early stopping strategies, and advancing theoretical understanding of LLMs’\nlearning dynamics.\n4 Matching Neural Topology across LLMs\nDespite significant discrepancies in architectures, parameters, and training data across different LLMs,\nthey are all trained to optimize the same next-token prediction objective, whose performance, as we\nhave shown, is closely tied to the internal neural topology. This raises a natural question: do different\nLLMs develop similar neural topology patterns despite their differences ? To investigate potential\nstructural similarity across LLMs, we extend graph probing with contrastive learning to perform\ngraph matching , as illustrated in Figure 7. This extension encourages graph representations derived\n3We do not include other LLMs in this analysis, as their intermediate checkpoints are not publicly available.\n7\n--- Page 8 ---\nLanguage Model 𝛀Language Model 𝚪\n…GraphProbe𝛀GraphProbe𝚪…………Graph 𝑨𝟏𝛀\ngraph similaritytarget similarityNNContrastive LearningGraph 𝑨𝑵𝛀\nGraph 𝑨𝟏𝚪Graph 𝑨𝑵𝚪𝑋%𝑋&Figure 7: An overview of graph matching. We learn representations of neural topologies derived\nfrom two different LLMs processing the same text dataset. We then perform contrastive learning on\nthe graph representations such that matching pairs are more similar by inner product.\nfrom the same input text to be more similar than those from different texts. Specifically, suppose we\nfeed a batch of Btoken sequences into two LLMs, ΩandΓ. We compute the corresponding neural\nconnectivity graphs and use two graph probes to encode them into representations ZΩ= [zΩ\n1, . . . ,zΩ\nB]\nandZΓ= [zΓ\n1, . . . ,zΓ\nB], as described in equations (5–6). Graph matching is implemented using a\ncontrastive cross-entropy loss that encourages alignment between graph representations:\nS=MAT_MUL(ZT\nΩ,ZΓ),T=IDENTITY (B), (9)\nL=BX\ni=1CROSS _ENTROPY (Si,:,Ti,:) +BX\nj=1CROSS _ENTROPY (S:,j,T:,j), (10)\nwhere Sis the similarity matrix by taking inner product of graph representations and Tis the target\nidentity matrix for graph matching.\nAfter training the graph probes contrastively on a shared set of training texts, the out-of-sample graph\nmatching performance serves as an indicator of neural topology similarity between two LLMs. To\nevaluate this, we adopt the commonly used AUC and GAUC metrics [ 32], where AUC measures\nthe global ranking quality across the entire test set, while GAUC computes a local ranking for each\ngraph pair against all others, making AUC the more challenging metric (see Appendix F for details).\nTable 4 presents the graph matching results. As a sanity check, we first perform self-matching using\nthe same LLM. Given that identical text inputs induce identical neural topologies, the results indeed\nshow that both AUC and GAUC are close to 1.0, validating the rationality of our methodology. We\nthen extend the matching experiments across multiple configurations, including: (1) LLMs within the\nsame family but from different generations, (2) LLMs across different families, and (3) the same LLM\ntrained with different random seeds. Surprisingly, all cross-model configurations yield high graph\nmatching performance, with AUC (GAUC) scores ranging from 0.86 (0.87) to 0.95 (0.96). These\nresults suggest that distinct LLMs develop strikingly similar neural topology patterns, implying the\nemergence of shared functional structures despite substantial differences in architectures, parameters,\nand training data. Moreover, we observe that cross-seed and cross-generation matching outperform\ncross-family matching, which is intuitive given the reduced architectural and training data differences\nwithin the same model family.\nTable 4: Graph matching performance between different LLM configurations evaluated on AUC\n(×100) and GAUC ( ×100) at 80% connection sparsity levels (20% density).\nMatching LLM Ω LLM Γ AUC GAUC\nSelf MatchingGPT2 GPT2 97.32 98.64\nPythia-160M Pythia-160M 95.95 96.92\nQwen2.5-0.5B Qwen2.5-0.5B 98.05 99.24\nCross GenerationQwen2.5-0.5B Qwen2-0.5B 91.45 93.27\nQwen2.5-0.5B Qwen1.5-0.5B 95.19 96.10\nQwen2-0.5B Qwen1.5-0.5B 92.92 94.21\nCross FamilyGPT2 Pythia-160M 90.96 92.00\nGPT2 Qwen2.5-0.5B 90.30 91.11\nPythia-160M Qwen2.5-0.5B 86.17 87.39\nCross SeedPythia-160M-seed1 Pythia-160M-seed2 92.75 93.87\nPythia-160M-seed1 Pythia-160M-seed3 92.48 93.59\nPythia-160M-seed2 Pythia-160M-seed3 92.95 94.20\n8\n--- Page 9 ---\n5 Related Work\nProbing LLMs. Growing concerns over the transparency and steerability of LLMs have driven\nrecent advances in reverse-engineering LLMs by extracting interpretable features from their neural\nactivations through probes [ 45]. Supervised probing typically maps neuron activations to interpretable\nsemantics through regression or classification [ 23,24,26,27,12,30,48,5]. For example, Gurnee\net al. [24] predicted the time and location of input entities from LLM activations. Unsupervised\nprobing, by contrast, aims to learn a dictionary of disentangled features related to more abstract\nconcepts [ 13,18,41,33,39,13]. A famous example is the Golden Gate Bridge feature identified\nin the Claude 3 Sonnet model [ 48]. While prior work focused on connecting LLM activations to\nexternal semantics, our work studies the functional topology of neurons in LLMs, and relates this\ninternal structure directly to language generation performance via graph probing .\nNetwork Neuroscience. The study of functional networks in the human brain has been a central\ntopic in neuroscience for decades [ 4,3,15,37] which motivates this research. Brain networks are\ntypically constructed by correlating fMRI or EEG signals across different neural regions, and then\nanalyzed using tools from network science [ 2], which has revealed a range of structural and functional\nproperties, such as small-worldness [ 4], economical wiring [ 9], and functional specialization [ 15].\nMore recently, several studies have drawn parallels between LLM activations and human brain\nactivity [ 49,11,31,42,38,51,8,46,34]. For instance, Tuckute et al. [51] used GPT-2 activations\nto identify sentence stimuli that drive or suppress human brain responses. However, while these\nefforts focus on representational similarities, the functional topology of neurons within LLMs and its\nrelationship to the model’s language generation capabilities remain largely unexplored.\n6 Discussion\nNeurons in LLMs are connected both structurally through the model’s architecture and functionally\nthrough their dynamic responses to input linguistic stimuli. In this work, we focus on the latter\nand demonstrate that the language generation performance of LLMs can be reliably predicted from\ntheir functional neural topologies using our proposed graph probing approach. Beyond the shared\nsparsity, non-linearity, and early emergence of this topology–performance relationship across models,\nwe also find that different LLMs, despite substantial differences in architectures, parameters, and\ntraining data, exhibit highly similar topological patterns, as their neural topologies can be matched\nwith near-perfect accuracy, suggesting a common underlying structure in how token sequences are\nprocessed. These findings imply that LLMs develop intricate and consistent topological structures\namong their neurons that are fundamental to their emergent ability to generate coherent language.\nWhile we have empirically shown a strong dependence between next-token prediction perplexity\nand neural topology, we have not yet identified specific topological structures such as motifs, or\nphysical metrics like small-worldness and modularity within these neural graphs. It remains an open\nquestion whether such properties exist in LLMs’ neural topology and play a causal role in shaping\ntheir language generation capabilities. This valuable knowledge, now captured implicitly by the\ngraph probes, may be further uncovered through careful analysis of the learned graph representations.\nAdditionally, this paper evaluates LLMs with up to 14B parameters, while leaving graph probing on\neven larger models for future work due to the substantial computational costs of both LLM inference\nand connectivity graph construction at that scale.\nOur graph probing results raise many interesting directions for future research. While we have\nlinked neural topology to general next-token prediction ability, it remains unclear whether specific\ntopologies emerge for specialized domains such as mathematical proofs or codes. We conjecture\nthat certain neurons may become functionally specialized when processing text from particular\ndomains, which might be identified via graph probing on datasets from diverse fields. Additionally,\nrecent advances in enhancing LLMs’ reasoning abilities [ 22] raise a natural question: does reasoning\nalter, or is it constrained by, neural topology? Moreover, the observed sparsity and early emergence\nof neural topology suggest potential applications in parameter pruning and early training failure\ndetection, offering promising avenues for reducing the inference and training cost of LLMs. Finally,\ngraph probing is model-agnostic and can be extended to models other than LLMs. In particular,\napplying graph probing to vision-language models may shed light on the neural topology underlying\nmulti-modal generation capabilities. In all, we believe graph probing offers a promising lens for\nunderstanding AI models and ultimately guiding their improvement in an reliable and safe way.\n9\n--- Page 10 ---\nReferences\n[1]Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier\nprobes. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Workshop Track Proceedings . OpenReview.net, 2017.\n[2]Albert-László Barabási. Network science. Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences , 371(1987):20120375, 2013.\n[3]Danielle S Bassett and Olaf Sporns. Network neuroscience. Nature neuroscience , 20(3):353–\n364, 2017.\n[4]Danielle Smith Bassett and ED Bullmore. Small-world brain networks. The neuroscientist ,\n12(6):512–523, 2006.\n[5]Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational\nLinguistics , 48(1):207–219, 2022.\n[6]Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research , 3(Feb):1137–1155, 2003.\n[7]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling. In\nInternational Conference on Machine Learning , pages 2397–2430. PMLR, 2023.\n[8]Laurent Bonnasse-Gahot and Christophe Pallier. fmri predictors based on language models of\nincreasing complexity recover brain left lateralization. In Amir Globersons, Lester Mackey,\nDanielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors,\nAdvances in Neural Information Processing Systems 38: Annual Conference on Neural Infor-\nmation Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15,\n2024 , 2024.\n[9]Ed Bullmore and Olaf Sporns. The economy of brain network organization. Nature reviews\nneuroscience , 13(5):336–349, 2012.\n[10] Edward T Bullmore and Danielle S Bassett. Brain graphs: graphical models of the human brain\nconnectome. Annual review of clinical psychology , 7(1):113–140, 2011.\n[11] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Rémi King. Evidence of a predictive\ncoding hierarchy in the human brain listening to speech. Nature human behaviour , 7(3):430–441,\n2023.\n[12] Xiangjue Dong, Yibo Wang, Philip S Yu, and James Caverlee. Probing explicit and implicit\ngender bias through llm conditional text generation. arXiv preprint arXiv:2311.00306 , 2023.\n[13] Joshua Engels, Isaac Liao, Eric J Michaud, Wes Gurnee, and Max Tegmark. Not all lan-\nguage model features are linear. In The Thirteenth International Conference on Learning\nRepresentations, ICLR 2025, Singapore, Apr 24-28, 2025 . OpenReview.net, 2025.\n[14] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.\nInICLR Workshop on Representation Learning on Graphs and Manifolds , 2019.\n[15] Panagiotis Fotiadis, Linden Parkes, Kathryn A Davis, Theodore D Satterthwaite, Russell T Shi-\nnohara, and Dani S Bassett. Structure–function coupling in macroscale human brain networks.\nNature Reviews Neuroscience , 25(10):688–704, 2024.\n[16] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological\ncybernetics , 20(3):121–136, 1975.\n[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\n10\n--- Page 11 ---\n[18] Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya\nSutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. In The\nThirteenth International Conference on Learning Representations, ICLR 2025, Singapore, Apr\n24-28, 2025 . OpenReview.net, 2025.\n[19] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego\nRojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from\nglm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793 , 2024.\n[20] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus.\nhttp://Skylion007.github.io/OpenWebTextCorpus , 2019.\n[21] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey,\nSamuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational\nprinciples for language processing in humans and deep language models. Nature neuroscience ,\n25(3):369–380, 2022.\n[22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n[23] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris\nBertsimas. Finding neurons in a haystack: Case studies with sparse probing. Trans. Mach.\nLearn. Res. , 2023, 2023.\n[24] Wes Gurnee and Max Tegmark. Language models represent space and time. In The Twelfth\nInternational Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,\n2024 . OpenReview.net, 2024.\n[25] Robert Huben, Hoagy Cunningham, Logan Riggs, Aidan Ewart, and Lee Sharkey. Sparse\nautoencoders find highly interpretable features in language models. In The Twelfth Interna-\ntional Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 .\nOpenReview.net, 2024.\n[26] Charles Jin and Martin Rinard. Emergent representations of program semantics in language\nmodels trained on programs. In Forty-first International Conference on Machine Learning ,\n2024.\n[27] Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, and Gongshen Liu. How large\nlanguage models encode context knowledge? a layer-wise probing study. In Proceedings of the\n2024 Joint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024) , pages 8235–8246, 2024.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.\n[29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional\nnetworks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017.\n[30] Connor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, and Neel Nanda.\nInterpreting attention layer outputs with sparse autoencoders. arXiv preprint arXiv:2406.17759 ,\n2024.\n[31] Sreejan Kumar, Theodore R Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Ken-\nneth A Norman, Thomas L Griffiths, Robert D Hawkins, and Samuel A Nastase. Shared\nfunctional specialization in transformer-based language models and the human brain. Nature\ncommunications , 15(1):5523, 2024.\n[32] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching\nnetworks for learning the similarity of graph structured objects. In International conference on\nmachine learning , pages 3835–3845. PMLR, 2019.\n11\n--- Page 12 ---\n[33] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat,\nVikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope:\nOpen sparse autoencoders everywhere all at once on gemma 2. In Proceedings of the 7th\nBlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP , pages 278–300,\n2024.\n[34] Yiheng Liu, Xiaohui Gao, Haiyang Sun, Bao Ge, Tianming Liu, Junwei Han, and Xintao Hu.\nBrain-inspired exploration of functional networks and key neurons in large language models.\narXiv preprint arXiv:2502.20408 , 2025.\n[35] Zixiang Luo, Kaining Peng, Zhichao Liang, Shengyuan Cai, Chenyu Xu, Dan Li, Yu Hu,\nChangsong Zhou, and Quanying Liu. Mapping effective connectivity by virtually perturbing a\nsurrogate brain. arXiv preprint arXiv:2301.00148 , 2022.\n[36] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\nSparse feature circuits: Discovering and editing interpretable causal graphs in language models.\nInThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\nApr 24-28, 2025 . OpenReview.net, 2025.\n[37] John D Medaglia, Mary-Ellen Lynall, and Danielle S Bassett. Cognitive network neuroscience.\nJournal of cognitive neuroscience , 27(8):1471–1491, 2015.\n[38] Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D Mehta, and Nima Mesgarani.\nContextual feature extraction hierarchies converge in large language models and the brain.\nNature Machine Intelligence , pages 1–11, 2024.\n[39] Anish Mudide, Joshua Engels, Eric J Michaud, Max Tegmark, and Christian Schroeder de Witt.\nEfficient dictionary learning with switch sparse autoencoders. In The Thirteenth Interna-\ntional Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 .\nOpenReview.net, 2025.\n[40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[41] Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma,\nJános Kramár, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse\nautoencoders. arXiv preprint arXiv:2404.16014 , 2024.\n[42] Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Osama A Binhuraib, Nicholas Blauch, and\nMartin Schrimpf. Topolm: brain-like spatio-functional organization in a topographic language\nmodel. In The Thirteenth International Conference on Learning Representations, ICLR 2025,\nSingapore, Apr 24-28, 2025 . OpenReview.net, 2025.\n[43] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by\nback-propagating errors. nature , 323(6088):533–536, 1986.\n[44] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:\nIntegrative modeling converges on predictive processing. Proceedings of the National Academy\nof Sciences , 118(45):e2105646118, 2021.\n[45] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas\nGoldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems\nin mechanistic interpretability. arXiv preprint arXiv:2501.16496 , 2025.\n[46] Haiyang Sun, Lin Zhao, Zihao Wu, Xiaohui Gao, Yutao Hu, Mengfei Zuo, Wei Zhang, Junwei\nHan, Tianming Liu, and Xintao Hu. Brain-like functional organization within large language\nmodels. arXiv preprint arXiv:2410.19542 , 2024.\n[47] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open\nmodels based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\n12\n--- Page 13 ---\n[48] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen,\nAdam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L\nTurner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers,\nEdward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan.\nScaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer\nCircuits Thread , 2024.\n[49] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in\nmachines) with natural language-processing (in the brain). Advances in neural information\nprocessing systems , 32, 2019.\n[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[51] Greta Tuckute, Aalok Sathe, Shashank Srikant, Maya Taliaferro, Mingye Wang, Martin\nSchrimpf, Kendrick Kay, and Evelina Fedorenko. Driving and suppressing the human language\nnetwork using large language models. Nature Human Behaviour , 8(3):544–561, 2024.\n[52] Petra E Vértes, Aaron F Alexander-Bloch, Nitin Gogtay, Jay N Giedd, Judith L Rapoport, and\nEdward T Bullmore. Simple models of human brain functional networks. Proceedings of the\nNational Academy of Sciences , 109(15):5868–5873, 2012.\n[53] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. Trans. Mach. Learn. Res. , 2022, 2022.\n[54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n13\n--- Page 14 ---\nA Graph Probing Configuration\nHyperparameters. We train graph probes using the Adam optimizer [ 28] with mean squared error\n(MSE) loss, as defined in Equation (8). The learning rate is set to 0.001, with a batch size of 16. We\napply a learning rate decay strategy, reducing the rate by a factor of 0.1 if the loss does not improve\nfor 5 consecutive epochs. Each model is trained for up to 100 epochs, with early stopping triggered if\nno improvement is observed for 20 epochs. Dropout is not used, as preliminary experiments showed\nno significant impact on regression performance.\nComputational Resources. LLM inference for computing neural topologies and perplexity scores\nrequires GPUs with large memory. All experiments were conducted on a Linux server equipped\nwith 8 NVIDIA A100 GPUs (80GB memory each). In contrast, training graph probes is relatively\nlightweight and can be performed on a single GPU with 16GB memory in less than 1 hour.\nB Experimented LLMs\nWe run graph probing experiments on a diverse range of LLMs across three different families, with\nthe numper of parameters ranging from 124M to 14B. Basic information of these experimented LLMs\nis summarized in Table 5.\nTable 5: Basic information of the experimented LLMs.\nLLM family #params #layers #neurons per layer experimented layer id\nGPT-2124M 12 768 1-12\n355M 24 1024 12\n774M 36 1280 18\nPythia160M 12 768 1-12\n410M 24 1024 12\n1.4B 24 2048 12\n2.8B 32 2560 16\n6.9B 32 4096 16\n12B 36 5120 18\nQwen2.50.5B 24 896 12\n3B 36 2048 18\n7B 28 3584 14\n14B 48 5120 24\nC Datasets\nWe conduct graph probing experiments using the same text datasets on which the LLMs were\noriginally pretrained. Specifically, we adopt the Pile dataset [ 17] for models in the Pythia family,\nand the OpenWebText dataset [ 20] for GPT-2 and Qwen2.5 models. For each dataset, we randomly\nsample 10,000 text sequences to construct neural connectivity graphs. Each sample is generated by\nmerging and tokenizing raw text until it reaches a length between 256 and 1024 tokens, which defines\nthe length of the corresponding neural activity time series used for computing pairwise correlations.\nWe then construct a text-responsive neural connectivity graph for each sample and compute its\nassociated perplexity score. To remove outliers that distort the distribution, we filter out the top 1%\nand bottom 1% of samples based on perplexity. Finally, we normalize all perplexity values to the\nrange [0,1]by subtracting the minimum perplexity and dividing by the observed range. Summary\nstatistics for the constructed datasets are provided in Table 6.\nTable 6: Basic information of constructed graph probing datasets.\nLLM family Dataset #tokens #graphs #training graphs #test graphs\nGPT-2 OpenWebText 7,020,215 10,384 8,308 2,076\nPythia Pile 5,216,371 8,011 6,409 1,602\nQwen2.5 OpenWebText 7,935,555 11,452 9162 2,290\n14\n--- Page 15 ---\nD Perplexity Regression Results\n00.20.40.60.8100.20.40.60.81\nGPT2-medium\nPredicted PerplexityTrue PerplexityR² = 0.8187Pearson r = 0.9060Spearman r = 0.917100.20.40.60.8100.20.40.60.81\nGPT2-large\nPredicted PerplexityTrue PerplexityR² = 0.7983Pearson r = 0.8942Spearman r = 0.8860\n00.20.40.60.8100.20.40.60.81\nPythia-410M\nPredicted PerplexityTrue PerplexityR² = 0.8626Pearson r = 0.9288Spearman r = 0.9405\n00.20.40.60.8100.20.40.60.81\nPythia-2.8B\nPredicted PerplexityTrue PerplexityR² = 0.8996Pearson r = 0.9485Spearman r = 0.959200.20.40.60.8100.20.40.60.81\nPythia-6.9B\nPredicted PerplexityTrue PerplexityR² = 0.9210Pearson r = 0.9599Spearman r = 0.9670\n00.20.40.60.8100.20.40.60.81\nPythia-12B\nPredicted PerplexityTrue PerplexityR² = 0.8974Pearson r = 0.9480Spearman r = 0.952700.20.40.60.8100.20.40.60.81\nQwen2.5-3B\nPredicted PerplexityTrue PerplexityR² = 0.7059Pearson r = 0.8429Spearman r = 0.8372\n00.20.40.60.8100.20.40.60.81\nQwen2.5-7B\nPredicted PerplexityTrue PerplexityR² = 0.7699Pearson r = 0.8789Spearman r = 0.882300.20.40.60.8100.20.40.60.81\nQwen2.5-14B\nPredicted PerplexityTrue PerplexityR² = 0.8249Pearson r = 0.9086Spearman r = 0.916700.20.40.60.8100.20.40.60.81\nPythia-1.4B\nPredicted PerplexityTrue PerplexityR² = 0.9111Pearson r = 0.9548Spearman r = 0.9615\nFigure 8: Out-of-sample performance of graph probing on different LLMs, including GPT2-\nmedium, GPT2-large, Pythia-410M, Pythia-1.4B, Pythia-2.8B, Pythia-6.9B, Pythia-12B, Qwen2.5-\n3B, Qwen2.5-7B, and Qwen2.5-13B.\n15\n--- Page 16 ---\nE Results of Different Probe Dimensions\nIn addition to the number of graph convolutional layers ( L), the hidden dimensionality ( d) is a key\nfactor influencing the expressive power of graph probes. We report out-of-sample graph probing\nperformance across different values of din Tables 7–9. As expected, we observe a monotonic im-\nprovement in perplexity regression performance with increasing d, indicating that higher-dimensional\nprobes are more capable of capturing fine-grained topological patterns in neural connectivity graphs.\nNotably, all experiments in the main paper were conducted with d= 32 , which already yielded strong\npredictive accuracy despite its relatively small size.\nTable 7: Out-of-sample perplexity regression performance for GPT-2 using different dimensions don\nsparse neural topologies where top 10% functional connections are reserved (90% sparsity).\nd MSE↓MAE↓R2↑ ρP↑ ρS↑\n4 0.0098 0.0738 0.7041 0.8400 0.8525\n8 0.0089 0.0707 0.7321 0.8566 0.8621\n16 0.0085 0.0692 0.7431 0.8631 0.8726\n32 0.0085 0.0690 0.7419 0.8627 0.8666\n64 0.0062 0.0584 0.8112 0.9008 0.9071\n128 0.0058 0.0562 0.8249 0.9090 0.9149\n256 0.0050 0.0524 0.8496 0.9224 0.9274\nTable 8: Out-of-sample perplexity regression performance for Pythia-160M using different dimen-\nsions don sparse neural topologies where top 10% functional connections are reserved (90% sparsity).\nd MSE↓MAE↓R2↑ ρP↑ ρS↑\n4 0.0095 0.0705 0.7087 0.8422 0.8762\n8 0.0089 0.0688 0.7276 0.8534 0.8761\n16 0.0084 0.0660 0.7417 0.8622 0.8783\n32 0.0074 0.0627 0.7736 0.8796 0.9004\n64 0.0076 0.0630 0.7687 0.8779 0.8970\n128 0.0058 0.0542 0.8227 0.9073 0.9264\n256 0.0051 0.0515 0.8436 0.9185 0.9336\nTable 9: Out-of-sample perplexity regression performance for Qwen2.5-0.5B using different di-\nmensions don sparse neural topologies where top 10% functional connections are reserved (90%\nsparsity).\nd MSE↓MAE↓R2↑ ρP↑ ρS↑\n4 0.0090 0.0725 0.7265 0.8525 0.8539\n8 0.0084 0.0689 0.7433 0.8634 0.8686\n16 0.0079 0.0659 0.7592 0.8733 0.8757\n32 0.0078 0.0655 0.7633 0.8781 0.8839\n64 0.0049 0.0517 0.8505 0.9224 0.9257\n128 0.0047 0.0510 0.8569 0.9259 0.9311\n256 0.0042 0.0471 0.8714 0.9336 0.9408\nF Graph Matching Metrics\nGiven the predicted similarity matrix S ∈RN×Nand the target similarity matrix T=IDENTITY (N),\nwe calculate the following metrics for graph matching [32]:\nAUC=AREA _UNDER _ROC(flatten (S),flatten (T)), (11)\nGAUC =1\n2NNX\ni=1(AREA _UNDER _ROC(Si,:,Ti,:) +AREA _UNDER _ROC(S:,i,T:,i)). (12)\n16",
  "text_length": 55061
}