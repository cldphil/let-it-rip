{
  "id": "http://arxiv.org/abs/2506.05314v1",
  "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models",
  "summary": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.",
  "authors": [
    "Taha Entesari",
    "Arman Hatami",
    "Rinat Khaziev",
    "Anil Ramakrishna",
    "Mahyar Fazlyab"
  ],
  "published": "2025-06-05T17:55:23Z",
  "updated": "2025-06-05T17:55:23Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05314v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05314v1  [cs.CL]  5 Jun 2025Constrained Entropic Unlearning: A Primal-Dual\nFramework for Large Language Models\nTaha Entesari1∗, Arman Hatami1∗, Rinat Khaziev2, Anil Ramakrishna2, Mahyar Fazlyab1†\n1Johns Hopkins University,2Amazon\nAbstract\nLarge Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information. Existing\nunlearning methods typically formulate forgetting and retention as a regularized\ntrade-off, combining both objectives into a single scalarized loss. This often leads\nto unstable optimization and degraded performance on retained data, especially\nunder aggressive forgetting. We propose a new formulation of LLM unlearning as\na constrained optimization problem: forgetting is enforced via a novel logit-margin\nflattening loss that explicitly drives the output distribution toward uniformity on\na designated forget set, while retention is preserved through a hard constraint on\na separate retain set. Compared to entropy-based objectives, our loss is softmax-\nfree, numerically stable, and maintains non-vanishing gradients, enabling more\nefficient and robust optimization. We solve the constrained problem using a\nscalable primal-dual algorithm that exposes the trade-off between forgetting and\nretention through the dynamics of the dual variable. Evaluations on the TOFU\nand MUSE benchmarks across diverse LLM architectures demonstrate that our\napproach consistently matches or exceeds state-of-the-art baselines, effectively\nremoving targeted information while preserving downstream utility.\n1 Introduction\nLarge Language Models (LLMs) are now foundational to a wide range of applications, from search\nengines and coding assistants e.g., [ 20,12], to medical diagnostics e.g., [ 19,15,34,35], scientific\nresearch e.g., [ 1,31], and education e.g., [ 26]. Their remarkable performance stems from training\non vast, diverse corpora of data. However, this training data often contains sensitive, copyrighted,\nor ethically problematic content, raising concerns around privacy, misinformation, and regulatory\ncompliance. These concerns have led to a growing demand for machine unlearning , the ability to\nselectively erase the influence of specific training data or knowledge from a deployed model.\nMachine unlearning, initially introduced by Cao and Yang [ 5], poses a fundamental challenge: how\ncan one remove the impact of a small subset of data without retraining the model from scratch? For\nLLMs, full retraining is prohibitively expensive, especially as models grow in size. Additionally,\nLLMs must frequently forget information to comply with regulatory mandates (e.g., the “right to be\nforgotten” [ 43]), avoid generating harmful content [ 41], prevent the leakage of private data [ 40], or\neliminate reliance on copyrighted materials [9].\nThis has motivated recent algorithmic efforts to approximate unlearning via lightweight fine-tuning\ntechniques, most notably gradient ascent over the forget set [ 41,9,25]. While such methods can\nsuppress the model’s ability to recall or generate content related to the undesired data, they often do\nso at a steep cost: they degrade model performance on unrelated, retained data. This degradation is\n∗Equal Contribution\n†Correspondence to mahyarfazlyab@jhu.edu\nPreprint. Under review.\n--- Page 2 ---\n0 0.2 0.4 0.6 0.8 100.20.40.6\nForget Success (higher is better)Model Utility (higher is better)TARGET\nRETRAINED\nGRAD-DIFF\nDPO\nNPO\nSIMNPO\nRMU\nPDU (OURS)\nFigure 1: Comparison of different methods on the\nTOFU dataset on Llama 3.2 3B: Model Utility vs.\nforget Success; see Section 4 for explanation of the\nmetrics.\nFigure 2: Samples from responses of different meth-\nods to same prompt in forgetting and retaining evalua-\ntions on the MUSE ( Books )\nparticularly problematic when the forget set is much smaller than the retained dataset, as is typically\nthe case in practice. As a result, recovering the lost utility of the model becomes extremely difficult\nand costly\nPrior work has proposed theoretically principled formulations of unlearning as a regularized optimiza-\ntion problem, seeking model parameters that (i) degrade performance on a forget dataset to remove\nsensitive or undesirable knowledge, and (ii) preserve performance on a retain dataset to maintain\nutility. However, this framework encounters three key obstacles.\nFirst, regularized formulations treat unlearning as a soft trade-off between forgetting and retention.\nIn doing so, they often drive the forget loss to vanishingly small values (almost zero e.g., [ 4]) in an\neffort to eliminate all traces of the undesired data. However, this aggressive forgetting is not only\ncomputationally inefficient, but also often counterproductive, leading to a sharp degradation in the\nutility of the model in unrelated inputs.\nSecond, at the beginning of unlearning, the initial utility of the model is already high, i.e., the\nretention loss is small. The regularized formulation attempts to decrease the retention loss at the same\ntime as the forgetting loss, but this is unnecessary: Simply keeping the retention loss at its initial\nlevel is enough to maintain the original utility of the model. Trying to reduce the retention loss like\nthe forgetting loss can lead to (i) overfitting and bias toward the retained set, and (ii) difficulty in\nsufficiently lowering the forgetting loss, since the problem becomes a multi-objective optimization\nwith conflicting gradient directions, and as a result, both forgetting and retention losses decrease\npoorly.\nThird, conventional loss functions such as cross-entropy are poorly behaved when repurposed for\nforgetting. Cross-entropy is unbounded above; thus, maximizing it results in unstable gradients,\noptimization divergence, and a lack of interpretability. These behaviors hinder convergence and offer\nlittle guidance on when forgetting is sufficient.\nOur Contributions. (1) We propose a conceptual shift: unlearning should be cast as a constrained\noptimization problem, where the goal is to forget designated data while explicitly preserving utility\non retained data via a formal constraint. This avoids the need for delicate loss balancing and provides\nstrong performance guarantees. (2) We introduce a novel logit-margin flattening loss that encourages\nuniform output distributions on the forget set, offering a stable, softmax-free alternative to entropy\nmaximization. This loss is convex, bounded, and provides non-vanishing gradients–making it well-\nsuited for large-scale optimization. (3) We develop a scalable primal-dual algorithm that enforces the\nretention constraint and captures the trade-off between forgetting and utility through the dynamics of\nthe dual variable. The algorithm supports warm starts and dynamic updates, ensuring efficiency even\nat LLM scale. (4) Finally, we evaluate our method on the TOFU and MUSE benchmarks using both\nstandard metrics and a new LLM-based judge that assesses behavioral divergence.\n1.1 Related Works\nExisting methods for LLM unlearning broadly fall into the following categories:\n2\n--- Page 3 ---\nRetraining-based unlearning approaches involve retraining models from scratch or fine-tuning\nthem on datasets excluding the forget set [ 2]. Although exact retraining provides the most reliable\nguarantee of unlearning, it is computationally prohibitive, especially for large-scale LLMs, making it\nimpractical for real-world applications.\nGradient-Ascent-based unlearning techniques commonly apply gradient ascent (GA) to the for-\nget set to suppress undesired model behaviors e.g., [ 17,41]. However, these methods can cause\ngradient explosion, necessitating additional measures, such as gradient clipping or specialized loss\nfunctions (e.g., modified cross-entropy); to maintain stability, as in [ 27] and [ 38], which employ risk-\nweighted and regularized variants of gradient ascent. Moreover, they often suffer from catastrophic\nforgetting [45], markedly degrading model utility because of conflicting optimization objectives.\nOptimization-based unlearning methods e.g., [ 45,11,6] update model parameters by attaching\nexplicit penalties to the targeted knowledge. The multi-objective variant [ 44] builds on the same\nframework as [ 45] but first generates several alternate answers, adding non-trivial overhead before\nthe actual unlearning step. [ 7] modifies only the teacher logits during self-distillation, leaving the\nstudent model vulnerable to attacks. Other works replace the loss itself: for example, [ 6] adopt\naninverted-hinge loss; [ 42] push the output distribution toward uniformity via KL regularization;\nand [ 39] optimize exclusively on the forget set, not considering the retain set. While these designs\nreduce residual accuracy on the forget set, they commonly over-weight the forget objective, degrading\nperformance on retained data.\nRepresentation-based unlearning operates directly on latent embeddings to remove designated\nknowledge, e.g., [ 21]. [22] corrupts hidden states with targeted perturbations, while [ 24] triggers\nforgetting through embedding-corrupted prompts. Although such interventions can precisely erase\nthe chosen content, they often distort the surrounding semantic geometry, degrading fluency and\nfactual coherence, and are also hard to scale.\nPrompt-based and relabeling unlearning remove information by altering prompts or inverting\nlabels e.g., [ 30,9]. Logit-level methods, such as [ 18], first perform the opposite of unlearning and\nthen apply logit differences for unlearning, which is time consuming. Selective logit adjustment [ 7],\nwhich uses a heuristic for token selection, is likewise unreliable. Although these techniques avoid\nheavy fine-tuning, they remain brittle: minor paraphrases or adversarial queries can still surface the\nsuppressed knowledge, revealing limited robustness [29].\nAll existing methods prioritize unlearning performance on the forget set. Overoptimization towards\nminimizing forget loss can disproportionately degrade model utility, which is challenging to recover\ngiven the extensive nature of retain sets. As shown in Figure 1, even retraining the model on the retain\ndata set does not achieve an extremely low loss on the forget set. This suggests that pushing the forget\nloss lower leads to overforgetting, which makes it easier for later attacks to relearn the forgotten\ndata, since forgetting is not uniform [ 14]. Another benefit of our method is that it enables a dynamic\napproach towards handling conflicting gradients. That is, when optimizing two or more losses [ 41], it\nis likely that gradients will be conflicting and finding a shared descent direction can be extremely\nresource intensive for large scale LLMs [ 27]. However, the primal dual algorithm naturally adjusts the\nlinear combination such that if the retain loss is below our desired threshold, its conflicting dynamic\nwith the forgetting loss could be completely ignored. The dynamicaly changing λwould promptly\nshift attention back to the retention loss if the loss exceeds the user-defined threshold. This approach\nallows the model to unlearn the forget set while remaining as close as possible to the original model\nin utility.\n1.2 Notation\nWe denote a general LLM by πwhere πtakes as input a tokenized prompt x∈RN×D, where\nN=|x|is the number of tokens and Dis the embedding dimension. The model then outputs\nπlogits(x)∈RN×V, where Vis the vocabulary size of the tokenizer that will be used to decode the\ntokens into legible text prompts. We let π(x) = Softmax( πlogits(x)), where the Softmax operator\nis applied on each row of its input to turn the logits into probabilities. When the LLM is parametrized\nby a finite set of parameters θ, we denote it with πθ.\n3\n--- Page 4 ---\n2 Problem Formulation\nLetπrefbe a language model trained or fine-tuned on a dataset D, partitioned into disjoint subsets\nDrtn(data to retain) and Dfgt(data to unlearn), where each example (x, y)∈ D consists of a prompt x\nand target response y. The objective is to construct a new model πthat preserves the behavior of πref\nonDrtnwhile eliminating knowledge of Dfgt. Perfect unlearning entails eliminating all information\nrelated to Dfgt, not merely reducing π(y|x), the likelihood of generating ygiven x.\nIdeal unlearning would involve retraining a model πrfrom scratch on Drtn, fully excluding Dfgt.\nHowever, this is computationally intensive, costly, and time-consuming, especially given the potential\nfrequency of unlearning requests (e.g., due to outdated data or copyright concerns). Thus, practical\nunlearning seeks to derive a model π, close to πref, with the influence of Dfgteffectively removed.\nUnlearning is typically posed as a bi-objective optimization problem that balances the removal of\ninformation related to Dfgtwith the preservation of performance on Drtn. We define two loss functions:\nLfgtto enforce forgetting, and Lrtnto maintain utility. A common approach is linear scalarization:\nmin\nπ∈ΠLfgt(π,Dfgt) +λLrtn(π,Drtn), (1)\nwhere λ >0controls the trade-off and Πdenotes a compact function class, e.g., {π:∥π∥L2≤M}.\nThe losses are defined as\nLfgt(π,Dfgt) =E(x,y)∈D fgt[ℓfgt(π, x, y )],Lrtn(π,Drtn) =E(x,y)∈D rtn[ℓrtn(π, x, y )],\nwhere ℓfgtandℓrtnare task-specific loss functions detailed in later sections.\n3 Constrained Entropic Unlearning\nThe linear scalarization formulation in (1)suffers from a limitation: if the forget–set Dfgtand retain-\nsetDrtnoverlap (statistically or semantically), blindly reducing Lfgtcan catastrophically increase\nLrtn. To balance this trade-off, the scalarization weight λmust be carefully tuned for each instance.\nHowever, even such a dynamic approach provides no explicit control over the degradation on the\nretention set. In particular, small values of λmay lead to incomplete forgetting, while large values\ncan overly compromise retention.\nIn light of these challenges, we reformulate unlearning as a constrained optimization problem:\nmin\nπ∈ΠLfgt(π,Dfgt)\ns.t.Lrtn(π,Drtn)≤ε.(2)\nHere, εis a user-specified threshold for allowable performance degradation on Lrtn. A natural\ninstantiation is\nε= (1 + α)Lrtn(πref,Drtn)α >0. (3)\nwhich ensures that the updated model πdoes not degrade retention performance by more than a factor\nofαrelative to the reference model πref.\nUnlike scalarization, the constrained formulation explicitly separates the forgetting objective from\nthe retention requirement. This makes the trade-off transparent and easier to interpret: instead of\ntuning λto balance two competing objectives, the user directly specifies a retention budget ε, and the\nalgorithm maximizes forgetting subject to this constraint.\n3.1 Lagrangian Relaxation and the Dual Problem\nFor the constrained problem (2), we define the the Lagrangian function:\nL(π, λ) =Lfgt(π,Dfgt) +λ(Lrtn(π,Drtn)−ε),\nwhere the Lagrangian multiplier λ≥0relaxes the hard constraint by a soft penalty. Consider the\nprimal and dual formulations:\n(Primal) min\nπ∈Πmax\nλ≥0L(π, λ), (Dual) max\nλ≥0min\nπ∈ΠL(π, λ).\n4\n--- Page 5 ---\nThe Primal problem is equivalent to the constrained problem (2), but this primal form is not useful for\nalgorithmic purposes, as the inner maximization over λis unbounded for any fixed πthat violates the\nconstraint. This motivates the Dual formulation, which, by weak duality [ 3], finds the largest lower\nbound on the optimal forgetting loss subject to the constraint. If strong duality holds, the optimal\nvalues of both problems coincide, and solving the dual problem yields a solution to the original\nconstrained problem (2).\nTo ensure zero duality gap, we make two assumptions: (1) convexity: The loss functionals π7→\nLfgt(π,Dfgt)andπ7→ L rtn(π,Drtn)are convex, lower semi-continuous, and defined over the convex\npolicy class Π;(2) strict feasibility: The constraint is strictly feasible; i.e., there exists ˆπ∈Πsuch\nthatLrtn(ˆπ,Drtn)< ε. Under these assumptions, strong duality holds by classical results in convex\nanalysis [ 32]. This principle underlies a range of recent constrained learning frameworks, including\nsafe reinforcement learning [ 28], continual learning [ 10], and constraint-aware LLM fine-tuning via\nDPO [16].\nIn our setting, strict feasibility is guaranteed by construction. Specifically, the reference model πref\nsatisfies the constraint strictly as long as the tolerance parameter αis positive in (3). Hence, strong\nduality holds as long as the forget and retention losses are convex in the policy π.\nFinite-dimensional parameterization In practice, the model πis parameterized by a finite dimen-\nsional parameter θ∈Rp, giving rise to the parameterized dual objective:\nmax\nλ≥0min\nθ∈ΘLfgt(πθ,Dfgt) +λ(Lrtn(πθ,Drtn)−ε). (4)\nHere, the search space is restricted to Πθ={πθ|θ∈Θ} ⊆Π. While strong duality may not\nstrictly hold in this finite-dimensional, nonconvex setting, modern models are typically sufficiently\nexpressive to approximate the infinite-dimensional problem well [10].\n3.2 Proposed Method: Primal-dual with Warm Start\nA principled method to solve the above dual problem is dual ascent , which alternates between\nminimizing the Lagrangian L(θ, λ)with respect to θand applying one step of gradient ascent in λto\npenalize constraint violation:\nθ+= arg min\nθL(πθ, λ), λ+= [λ+ηλ(Lrtn(πθ+,Drtn)−ε)]+, ηλ>0.\nThe primal update corresponds to minimizing a scalarized objective, while the dual update can be\ninterpreted as dynamically adjusting the trade-off according to the violation Lrtn(πθ+,Drtn)−ε.\nWhile dual ascent offers strong theoretical guarantees, it typically involves a costly inner-loop\noptimization to fully minimize the Lagrangian at each step. We propose an efficient variant that\nperforms a single warm-started dual ascent step, followed by lightweight primal-dual updates. The\ninitial iteration fully minimizes L(πθ, λ0)with respect to θ. Subsequent iterations alternate between\none gradient descent step on θand one dual ascent step on λ, reducing computation through single-\nstep updates while retaining the advantages of dual ascent initialization. This method is detailed in\nAlgorithm 1.\nWhile our framework is compatible with a broad class of loss functions proposed in prior unlearning\nliterature, we will focus on specific instantiations of LfgtandLrtn, deferring extensions to alternative\nlosses to the Supplementary Material.\n3.3 Retention Loss\nFor the retain loss Lrtn, we follow established practice and adopt the standard cross-entropy loss:\nLrtn(π,Drtn) =E(x,y)∈D rtn\u0002\nCE(πlogits(y|x), y)\u0003\n=E(x,y)∈D rtn[−log (π(y|x))], (5)\nwhere for a response y, the autoregressive model defines the conditional probability as π(y|x) =Q|y|\ni=1π(yi|x, y<i), with π(yi|x, y<i)denoting the likelihood of generating token yigiven the input\nxand the previously generated tokens y<i.\n5\n--- Page 6 ---\nAlgorithm 1 Primal-Dual Solver with Warm Starting (Problem (2)\n1:Input: Forget set Dfgt, retain set Drtn, batch sampling algorithm R, reference parameters θref,\nconstraint threshold ε, learning rates ηθ, ηλ>0, initial dual variable λ0≥0, number of warm-up\nepochs Tw, number of primal-dual epochs Tpd\n2:Output: Primal parameters θ∗, dual variable λ∗\n3:Initialize: θ←θref,λ←λ0\n4:fort= 1, . . . , T w+Tpddo\n5: fordfgt, drtninR(Dfgt,Drtn)do\n6: ℓf(θ)←E(x,y)∈dfgtℓfgt(πθ(x), y), ℓ r(θ)←E(x,y)∈drtnℓrtn(πθ(x), y)\n7: L(θ, λ)←ℓf(θ) +λ(ℓr(θ)−ε)\n8: θ←θ−ηθ∇θL(θ, λ) ▷∇θL(θ, λ) =∇θℓf(θ) +λ∇θℓr(θ)\n9: ift > T w.then ▷Warm-start; Solve the primal problem for a fixed λuntil epoch Tw\n10: λ←[λ+ηλ(ℓr(θ)−ε)]+▷Dual update and project onto R≥0\n11: end if\n12: end for\n13:end for\n14:Return: θ,λ\n3.4 Logit Flattening for Efficient Forgetting\nA common heuristic for defining the forget loss Lfgtis the negative cross-entropy (CE) loss on the\nforget dataset:\nLfgt(π,Dfgt) =−L rtn(π,Dfgt). (6)\nHowever, the CE loss is unbounded above , and directly maximizing it during unlearning often leads\ntogradient explosion andcatastrophic collapse . Notably, CE minimization during pretraining serves\nas an upper bound surrogate for the 0-1 classification loss. Reversing this objective, by maximizing\nCE, invalidates this surrogate relationship\nTo induce high uncertainty in model outputs while avoiding these issues, a more stable alternative is\nto maximize the entropy of the predictive distribution:\nLfgt(π,Dfgt) =E(x,y)∈D fgt\u0002\nCE\u0000\nπlogits(y|x),1\nV1\u0001\u0003\n,\nwhere 1∈RVis the all-ones vector and V=|Y|is the vocabulary size. This loss encourages\npredictions close to the uniform distribution, and can be viewed as an entropy maximization strategy\nthat suppresses memorized responses by flattening the output distribution.\nWhile effective, entropy-based losses suffer from vanishing gradients as the output distribution\napproaches uniformity, which can slow down convergence and cause instability in later stages of\ntraining. Moreover, they require the log-softmax computation, which can be numerically sensitive\nand costly in large vocabulary settings.\nLogit-margin flattening. We propose an alternative objective that directly penalizes peakedness\nin the model’s pre-softmax logits. Given logits πlogits(yt|x, y<t)to input pair (x, y)∈ D fgt, the\nproposed logit-margin flattening loss is:\nLLM\nfgt(πθ,Dfgt) :=E(x,y)∼D fgt\n1\n|y||y|X\nt=1 \nmax\nkπlogits(yt|x, y<t)k−1\nVVX\nk=1πlogits(yt|x, y<t)k!2\n.\nMinimizing this loss drives the logit vector toward a constant (i.e., uniform after softmax), effectively\nflattening the predictive distribution. Zero loss is achieved if and only if all logits are equal, implying\nmaximal entropy without computing it explicitly. This logit flattening loss offers several benefits\nover traditional entropy maximization: (1) It avoids log-softmax operations and relies only on max\nand mean computations over logits, improving numerical stability and reducing runtime overhead in\nlarge vocabulary models; (2) The loss maintains nonzero gradients even when predictions are near\nuniform, enabling more efficient convergence; (3) The loss is convex in the logits z, and therefore\ncompatible with convex surrogate models or linear classifiers. This preserves the strong duality\nproperties required by our constrained optimization framework; and (4) The logit margin directly\nbounds the model’s maximum softmax probability. This is established next.\n6\n--- Page 7 ---\nProposition 3.1. If the logit margin satisfies\nmax\nkπlogits(yt|x, y<t)k−1\nVVX\nk=1πlogits(yt|x, y<t)k≤δ,\nthen the maximum softmax probability is upper-bounded as\nmax\nkπ(yt|x, y<t)k≤\u0012\n1 + (V−1) exp( −V\nV−1δ)\u0013−1\n=1\nV(1 +δ) +O(δ2)\nMoreover, a key advantage of our approach is its explicit control over the model’s output distribution\nonDfgt, unlike prior methods such as NPO [ 45], SimNPO [ 11], and Gradient Ascent [ 41], which\nlack such guarantees. This control contributes to the stability of our method by anchoring it to a\nwell-defined target distribution, a benefit also noted in prior work on stable unlearning [7, 18].\n4 Experiments\nDatasets: We evaluated our unlearning methodology on two established benchmarks: TOFU and\nMUSE [ 25,33,8]. The TOFU dataset consists of 200 diverse fictional author profiles, each containing\n20 question-answer pairs. A designated subset of these profiles, known as the forget set , serves as\nthe target for unlearning. In the main experiments, we choose to forget the subset Forget10 and\ndefer Forget05 andForget01 to the Supplementary Material. The MUSE benchmark focuses on\nunlearning in two real-world contexts: Books andNews . Due to space constraints, we consider the\nNews subset in the main experiments and defer Books to the Supplementary Material. The News\nsubset is a collection of BBC news articles collected after August 2023.\nModels: To establish the applicability of the methods, we test our methods across a wide scale of\nmodels. We include LLAMA 2 7B, LLAMA 2 13b [ 37], LLAMA 3.1 8B, LLAMA 3.2 1B, LLAMA\n3.2 3B [ 13], and Gemma 7B [ 36]. We utilize pretrained instruct versions of these models whenever\navailable3. The models are then finetuned on the desired sets to provide our starting checkpoints. See\nthe Supplementary Material for more information.\nMethods: We compare our method, Primal- DualUnlearning (PDU), against several baselines. The\nfirst is the target model that has been trained on Drtn∪ D fgt. Second, we consider an ideal model that\nhas only been trained on Drtn. Next, we turn to several established methods: GradDiff [ 41], DPO\n[25], NPO [ 45], SimNPO [ 11], and RMU [ 21]. For all the baselines, we use λ= 1as the trade-off\nparameter.\nEvaluation: To evaluate the effectiveness of the methods, we utilize several established metrics\nand calculate harmonic means of them to yield single statistics. More specifically, for the TOFU\ndataset, we utilize model utility andforget success .\n•Model Utility : Established in [ 25], model utility is a harmonic mean of several likelihood and\nROUGE scores [23] calculated over Drtnand other holdout sets.\n•Forget Success : We define this metric as the harmonic mean over 1−the likelihood on Dfgt,1−\nthe ROUGE score on Dfgt, and the truth ratio onDfgt. For metric definitions see [25].\nFor the MUSE dataset, we utilize the metrics retain ROUGE andforget ROUGE .\n•Retain ROUGE : From [33] ( KnowMem (π,Drtn)), the ROUGE score over knowledge on Drtn.\n•Forget ROUGE : The harmonic mean of KnowMem (π,Dfgt) and VerbMem (π,Dfgt) defined in [33].\nIn addition to the aforementioned traditional automatic metrics, we employ an LLM-based evaluation\nframework to assess the success of unlearning and knowledge retention. This method leverages an\nLLM acting as a structured judge to evaluate generated responses. We task the LLM with judging\ngenerated texts with respect to a ground truth response on several avenues and prompt the judge to\nscore each metric from 0 to 10:\n3We utilize pretrained and finetuned models through HuggingFace\n7\n--- Page 8 ---\n• For forgetting tasks: Knowledge Removal, Verbatim Removal, Fluency.\n• For retention tasks: Retention Score, Accuracy, Relevance, Fluency.\nWe summarize these results into four metrics: forget score, retain score, fluency, and relevance, where\nscoring higher is better on all metrics. The details of the metrics and the prompt input to the judge\ncan be found in the Supplementary Material.\nResults: The results of our experiments are reported in Table 1 and Table 2. For unlearning, it is\ncritical to take into account the involved metrics at the same time and to not judge based on single\nmetrics. That is, a model can have a perfect forgetting score but at the cost of catastrophic forgetting\nseverely hampering the models utility.\nWe can see in Table 1 that our method consistently outperforms all other methods across various\nscales and models by achieving the highest forget successes whilst retaining the best or second best\nmodel utilities. We see similar exceptional performance from our methodology on the LLM judged\nmetrics, except for the Fluency metric. Upon further examination, it becomes clear that this is an\nartifact of the success of the unlearning algorithm. That is, on the forget set the model’s knowledge\nhas been purged and the model abstains from making coherent predictions. Importantly, it should be\nnoted that the model fluency on the other tasks is unaffected. Due to space constraints, we defer the\ndetailed LLM judge statistics to the Supplementary Material.\nWe see that for the larger 7B and 8B models, GradDiff has a forget success of 0but an LLM judged\nforget score of 10. Studying the generations of the models, we see that the models unlearned via\nGradDiff abstain from producing any text when prompted with prompts from Dfgt. As such the\ntruth ratio onDfgtis essentially 0and yields a 0harmonic mean for the forget success. Due to this\nbehaviour, the judge gives a complete forgetting score to the model. However, unlike our method, we\nsee that GradDiff suffers from this artifact in its utility and also the other LLM judged metrics.\nWe further see that our method performs competitively on the more complex task of MUSE- News per\nTable 2. That is, our method provides a viable pareto point that provides unique retain and forget\nROUGE scores.\nTable 1 and Table 2 further point to an important observation: the traditional metrics used for assessing\ntask success, i.e., metrics such as model utility and forget success, are generally indicative of real\nsuccess, as outlined by the correlation that we see with the LLM judged metrics. Without the LLM\njudged metrics, it was not clear if metrics such as the likelihood of generating the prompt-response\npair(x, y)∈ D fgtor the ROUGE score would be real indicators of the successful unlearning. The\nLLM judged metrics show that this is generally the case and classical metrics are still useful indicators\nof a model’s capabilities.\n5 Conclusion\nWe presented a principled framework for unlearning in Large Language Models by casting the problem\nas a constrained optimization task. This formulation separates the forgetting and retention objectives,\nproviding explicit control over each. To enable stable and efficient forgetting, we introduced a\nlogit-margin flattening loss that avoids the pitfalls of entropy maximization while encouraging\nuniform predictive distributions on the forget set. Our scalable primal-–dual solver enforces the\nretention constraint and exposes the forgetting-–utility trade–off through interpretable dual dynamics.\nEmpirical evaluations on TOFU and MUSE benchmarks demonstrate that our method effectively\nsuppresses memorized responses while preserving retained capabilities, often matching full retraining\nat a fraction of the cost.\nOur work opens several directions for future investigation. First, due to the resource-intensive nature\nof LLMs, we were unable to conduct extensive hyperparameter tuning; it is possible that further gains\ncould be achieved with careful calibration. Second, we observed a slight reduction in generation\nfluency on the easier TOFU task under our method, potentially attributable to the strong uniformity\ninduced by logit flattening. Addressing this through regularization or hybrid losses is an interesting\ndirection. Third, while our method is designed to remove specific information, we do not study the\nresilience of the resulting model to relearning attacks or jailbreak attempts. Finally, a compelling\nextension would assess the robustness of our framework under continual unlearning requests, where\nnew forget sets arrive over time.\n8\n--- Page 9 ---\nTable 1: Performance on the TOFU dataset ( forget10/retain90 ) with different unlearning methods and\nmodels. Model utility and forget success are bounded in [0,1]whereas the LLM Judged metrics are in [0,10].\nFor all metrics, larger numbers are better. We bolden the best results and underline the runner-ups.\nMethodModel Forget LLM Judged\nUtility Success Forget Score Retain Score Fluency RelevanceLlama 3.2 1Btarget 0.595 0.194 1.643 8.235 9.695 9.405\nretrained 0.590 0.691 7.569 8.464 9.676 9.428\nGradDiff 0.434 0.616 7.001 5.748 8.413 8.277\nDPO 0.561 0.603 9.231 7.390 9.349 8.678\nNPO 0.475 0.672 6.695 5.686 9.012 8.643\nSimNPO 0.596 0.248 2.659 8.250 9.646 9.368\nRMU 0.570 0.689 7.973 7.415 8.410 9.003\nPDU (Ours) 0.602 0.740 8.556 7.885 7.988 9.209Llama 3.2 3Btarget 0.660 0.083 0.593 9.159 9.830 9.732\nretrained 0.645 0.694 7.673 9.101 9.734 9.731\nGradDiff 0.529 0.583 6.766 6.546 8.196 8.470\nDPO 0.609 0.540 8.630 8.292 9.415 9.023\nNPO 0.514 0.676 6.880 7.184 9.306 8.825\nSimNPO 0.653 0.196 1.839 8.898 9.751 9.657\nRMU 0.644 0.561 5.966 8.348 9.502 9.469\nPDU (Ours) 0.680 0.914 9.558 8.809 7.760 9.617Llama 3.1 8Btarget 0.628 0.013 0.0926 9.642 9.904 9.894\nretrained 0.649 0.693 7.505 9.646 9.794 9.874\nGradDiff 0.626 0 10 8.247 7.257 9.169\nDPO 0.497 0.596 9.501 5.345 9.020 6.160\nNPO 0.652 0.739 8.329 8.588 9.360 9.509\nSimNPO 0.603 0.481 4.630 8.983 9.691 9.698\nRMU 0.657 0.900 9.925 9.626 7.969 9.867\nPDU (Ours) 0.725 0.960 9.985 9.277 7.717 9.793Gemma 7Btarget 0.638 0.0342 0.305 8.655 9.818 9.558\nretrained 0.642 0.670 7.623 8.551 9.665 9.552\nGradDiff 0.461 0 9.988 4.720 6.766 7.458\nDPO 0.488 0.591 7.760 6.728 9.283 8.772\nNPO 0.543 0.744 8.631 7.027 9.363 8.873\nSimNPO 0.547 0.493 5.901 7.226 9.496 8.963\nRMU 0.633 0.630 9.785 8.351 7.656 9.453\nPDU (Ours) 0.602 0.933 9.996 7.323 7.303 9.023\nTable 2: Performance on the MUSE- News dataset with different unlearning methods and two large scale models.\nROUGE scores are bounded in [0,1]whereas the LLM Judged metrics are in [0,10]. We bolden the best results\nand underline the runner-ups.\nMethodRetain↑Forget↓LLM Judged ↑\nROUGE ROUGE Forget Success Retain Score Fluency RelevanceLlama 2 7Btarget 0.555 0.610 2.428 5.810 9.083 8.760\nretrained 0.560 0.250 6.905 5.460 9.030 8.670\nGradDiff 0.482 0.331 4.300 5.355 8.783 8.500\nNPO 0.455 0.318 4.688 4.545 8.687 7.930\nSimNPO 0.516 0.573 2.748 5.490 9.033 8.550\nRMU 0.460 0.418 4.398 4.855 8.887 8.060\nPDU (Ours) 0.397 0.290 5.550 4.040 7.767 7.630Llama 2 13Btarget 0.430 0.632 2.695 5.075 9.193 8.31\nretrained 0.395 0.255 6.948 4.440 8.920 7.780\nGradDiff 0.488 0.287 5.648 5.410 8.777 8.360\nNPO 0.420 0.403 4.315 5.015 9.080 8.340\nSimNPO 0.448 0.440 4.153 5.375 8.877 8.320\nRMU 0.232 0.194 7.865 3.025 8.173 6.640\nPDU (Ours) 0.452 0.289 5.050 4.795 8.427 8.050\n9\n--- Page 10 ---\nReferences\n[1]Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific\ntext, 2019.\n[2]Lucas Bourtoule et al. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy\n(SP), pages 141–159. IEEE, 2021.\n[3]Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press,\n2004.\n[4]Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, V olkan\nCevher, and Mingyi Hong. Unlearning as multi-task optimization: A normalized gradient\ndifference approach with an adaptive learning rate, 2024.\n[5]Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In\n2015 IEEE Symposium on Security and Privacy , pages 463–480. IEEE, 2015.\n[6]Sungmin Cha, Sungjun Cho, Dasol Hwang, and Moontae Lee. Towards robust and parameter-\nefficient knowledge unlearning for llms, 2025.\n[7]Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, and Ivan Vuli ´c. Undial:\nSelf-distillation with adjusted logits for robust unlearning in large language models. arXiv\npreprint arXiv:2402.10052 , 2024.\n[8]Vineeth Dorna, Anmol Mekala, Wenlong Zhao, Andrew McCallum, J Zico Kolter, and Pratyush\nMaini. OpenUnlearning: A unified framework for llm unlearning benchmarks. https://\ngithub.com/locuslab/open-unlearning , 2025. Accessed: February 27, 2025.\n[9]Ronen Eldan and Mark Russinovich. Who’s harry potter? approximate unlearning in llms.\narXiv preprint arXiv:2310.02238 , 2023.\n[10] Juan Elenter, Navid NaderiAlizadeh, Tara Javidi, and Alejandro Ribeiro. Primal dual continual\nlearning: Balancing stability and plasticity through adaptive memory allocation. arXiv preprint\narXiv:2310.00154 , 2023.\n[11] Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, and Sijia Liu.\nSimplicity prevails: Rethinking negative preference optimization for llm unlearning, 2025.\n[12] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun\nShou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for\nprogramming and natural languages, 2020.\n[13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama\n3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\n[14] Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, and Virginia Smith. Unlearning or obfuscating?\njogging the memory of unlearned llms via benign relearning, 2025.\n[15] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission, 2020.\n[16] Xinmeng Huang, Shuo Li, Edgar Dobriban, Osbert Bastani, Hamed Hassani, and Dongsheng\nDing. One-shot safety alignment for large language models via optimal dualization. Advances\nin Neural Information Processing Systems , 37:84350–84383, 2024.\n[17] Hyeonwoo Jang et al. Unlearning in deep neural networks. In Proceedings of the 36th\nConference on Neural Information Processing Systems (NeurIPS) , 2022.\n[18] Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Kompella, Sijia Liu, and Shiyu\nChang. Reversing the forget-retain objectives: An efficient llm unlearning framework from\nlogit difference. Advances in Neural Information Processing Systems , 37:12581–12611, 2024.\n[19] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical\ntext mining. Bioinformatics , 36(4):1234–1240, September 2019.\n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\n10\n--- Page 11 ---\n[21] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D.\nLi, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-\nBurger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass,\nOliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao,\nAriel Herbert-V oss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika,\nZifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y . Shih,\nKemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis,\nAlex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen\nFitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu\nWang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, and Dan Hendrycks.\nThe wmdp benchmark: Measuring and reducing malicious use with unlearning, 2024.\n[22] Xinyu Li et al. Representation-based unlearning in large language models. arXiv preprint\narXiv:2401.00000 , 2024.\n[23] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out , pages 74–81, 2004.\n[24] Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang Liu. Large language model\nunlearning via embedding-corrupted prompts, 2024.\n[25] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. Tofu: A\ntask of fictitious unlearning for llms, 2024.\n[26] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat,\nRed Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao,\nMohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman,\nGreg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, An-\ndrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis\nChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester\nCho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory\nDecareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,\nNiko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges,\nChristian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan\nGordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei\nGuo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,\nChris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu,\nShengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang,\nRoger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,\nŁukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan\nKilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros,\nMatt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis,\nKyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,\nJade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Man-\nning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob\nMcGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,\nAshvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pan-\ntuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,\nNick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt,\nDavid Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh,\nSarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Kata-\nrina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski\n11\n--- Page 12 ---\nSuch, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil\nTillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan\nFelipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea V oss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila\nWelihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens\nWinter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu,\nKai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,\nChong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk,\nand Barret Zoph. Gpt-4 technical report, 2024.\n[27] Zibin Pan, Shuwen Zhang, Yuesheng Zheng, Chi Li, Yuheng Cheng, and Junhua Zhao. Multi-\nobjective large language model unlearning. In ICASSP 2025-2025 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5. IEEE, 2025.\n[28] Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. Safe\npolicies for reinforcement learning via primal-dual methods. IEEE Transactions on Automatic\nControl , 68(3):1321–1336, 2022.\n[29] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms?\nobjectives for defending against extraction attacks, 2023.\n[30] Martin Pawelczyk et al. In-context unlearning: Language models as few-shot unlearners. arXiv\npreprint arXiv:2310.07579 , 2023.\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer, 2023.\n[32] R Tyrrell Rockafellar. Convex analysis , volume 28. Princeton university press, 1997.\n[33] Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao\nLiu, Luke Zettlemoyer, Noah A. Smith, and Chiyuan Zhang. Muse: Machine unlearning six-way\nevaluation for language models, 2024.\n[34] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi-\nratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield,\nBlaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj\nGottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan\nKarthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge,\n2022.\n[35] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark,\nStephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed\nAmin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska,\nBlaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\nMahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan\nKarthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with\nlarge language models, 2023.\n[36] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open\nmodels based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n[38] Qizhou Wang, Jin Peng Zhou, Zhanke Zhou, Saebyeol Shin, Bo Han, and Kilian Q. Weinberger.\nRethinking llm unlearning objectives: A gradient perspective and go beyond, 2025.\n[39] Yaxuan Wang, Jiaheng Wei, Chris Yuhao Liu, Jinlong Pang, Quan Liu, Ankit Parag Shah, Yujia\nBao, Yang Liu, and Wei Wei. Llm unlearning via loss adjustment with only forget data, 2024.\n[40] Xinyi Wu et al. Privacy risks of pre-trained language models: Unlearning memorized personal\ndata. arXiv preprint arXiv:2301.00000 , 2023.\n[41] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. arXiv preprint\narXiv:2310.10683 , 2023.\n12\n--- Page 13 ---\n[42] Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, and Min Lin. A closer\nlook at machine unlearning for large language models, 2025.\n[43] Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing,\nMark Staples, and Xiwei Xu. Right to be forgotten in the era of large language models:\nImplications, challenges, and solutions, 2024.\n[44] Peng Zhang et al. Altpo: Alternate preference optimization for machine unlearning. arXiv\npreprint arXiv:2403.03419 , 2024.\n[45] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From\ncatastrophic collapse to effective unlearning, 2024.\n13",
  "text_length": 46921
}