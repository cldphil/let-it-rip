{
  "id": "http://arxiv.org/abs/2506.04050v1",
  "title": "Explainability-Based Token Replacement on LLM-Generated Text",
  "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.",
  "authors": [
    "Hadi Mohammadi",
    "Anastasia Giachanou",
    "Daniel L. Oberski",
    "Ayoub Bagheri"
  ],
  "published": "2025-06-04T15:15:42Z",
  "updated": "2025-06-04T15:15:42Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04050v1",
  "full_text": "--- Page 1 ---\nExplainability-Based Token Replacement on LLM-Generated Text\nHADI MOHAMMADI∗,Department of Methodology and Statistics, Utrecht University, The Netherlands\nANASTASIA GIACHANOU, Department of Methodology and Statistics, Utrecht University, The Netherlands\nDANIEL L. OBERSKI, Department of Methodology and Statistics, Utrecht University, The Netherlands\nAYOUB BAGHERI, Department of Methodology and Statistics, Utrecht University, The Netherlands\nBackground: Generative models, especially large language models (LLMs), have shown remarkable progress in producing\ntext that appears human-like. However, they often exhibit patterns that make their output easier to detect than text written\nby humans.\nObjectives: In this paper, we investigate how explainable AI (XAI) methods can be used to reduce the detectability of\nAI-generated text (AIGT) while also introducing a robust ensemble-based detection approach.\nMethods: We begin by training an ensemble classifier to distinguish AIGT from human-written text, then apply SHAP and\nLIME to identify tokens that most strongly influence its predictions. We propose four explainability-based token replacement\nstrategies to modify these influential tokens.\nResults: Our findings show that these token replacement approaches can significantly diminish a single classifier’s ability\nto detect AIGT. However, our ensemble classifier maintains strong performance across multiple languages and domains,\nshowing that a multi-model approach can mitigate the impact of token-level manipulations.\nConclusions: Our findings show that XAI methods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based detection strategies that can adapt to evolving\napproaches for hiding AIGT.\n1 Introduction\nRecent advancements in LLMs, such as the GPT series, have enabled the creation of highly fluent text that often\nresembles human writing [ 3]. Still, even the best models often show subtle signs of being artificial, like repeating\nphrases, using a limited range of words, having no typos, or showing a consistent writing style. These clues can\nhelp readers or automated tools detect that the text was written by AI [ 31]. Being able to spot AIGT is important\nin various domains, such as in for protecting academic honesty, fighting misinformation, and keeping public\ntrust. On the other hand, there are good reasons to make AI text less detectable, like tools that help people write\nin their own style [40], or AI summaries that aim to be especially clear and well-structured [27].\nThis conflict between helpful and harmful uses of LLMs creates a challenge: how to keep their benefits while\npreventing misuse. One proposed solution is watermarking, which adds hidden patterns to generated text to\nmake it more transparent [ 14]. However, recent research shows that these watermarks can be weakened or\nremoved by changing the text, like paraphrasing [ 18]. Against this backdrop, our study asks: Can explanation\nmethods be used to both hide and detect AI-generated text? We contribute in two ways. First, we develop an\nensemble -based detector that remains accurate across languages (English and Dutch) and domains (news,\nreviews, Twitter). Second, we show how two explanation tools, SHapley Additive exPlanations (SHAP) [ 21] and\nLocal Interpretable Model-agnostic Explanations (LIME) [ 30], can be turned against a detector: we identify the\nmost influential tokens and replace them via four targeted strategies (semantically similar human -preferred words,\npart-of-speech -constrained synonyms, GPT -based substitutions, and GPT substitutions enriched with genre\n∗Corresponding Author.\nAuthors’ Contact Information: Hadi Mohammadi, h.mohammadi@uu.nl, orcid: 0000-0003-0860-9200, Department of Methodology and\nStatistics, Utrecht University, Utrecht, Utrecht, The Netherlands; Anastasia Giachanou, orcid: 0000-0002-7601-8667, a.giachanou@uu.nl,\nDepartment of Methodology and Statistics, Utrecht University, Utrecht, Utrecht, The Netherlands; Daniel L. Oberski, orcid: 0000-0001-\n7467-2297, d.l.oberski@uu.nl, Department of Methodology and Statistics, Utrecht University, Utrecht, Utrecht, The Netherlands; Ayoub\nBagheri, orcid: 0000-0001-6366-2173, a.bagheri@uu.nl, Department of Methodology and Statistics, Utrecht University, Utrecht, Utrecht, The\nNetherlands.\nJAIR, Vol. 1, No. 1, Article . Publication date: June 2025.arXiv:2506.04050v1  [cs.CL]  4 Jun 2025\n--- Page 2 ---\n2•Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\ncontext). We evaluate both detectability (accuracy, F1) and textual fidelity (BLEU, ROUGE), thereby quantifying\nthe trade -off between hiding AI signals and preserving meaning. Our results show that token -level manipulation\ncan fool many single -model detectors, yet a robust ensemble still performs strongly, underscoring the need to\ncombine explainability, watermarking, and policy safeguards to manage the evolving landscape of AIGT.\nThe rest of this paper is organized as follows: Section 2 reviews related work on AI-generated text detection,\nwatermarking, and adversarial rewriting. Section 3 describes the datasets and our data augmentation process. In\nSection 4, we present our methodology, including details on model architectures, explainability approaches, and\ntoken replacement strategies. Section 5 reports experimental results, showing how different rewriting methods\naffect detection performance and textual fidelity. Finally, Section 6 concludes the paper and discusses future\nresearch directions.\n2 Related Work\nKirchenbauer et al . [14] introduced an important red–green list watermark. In this method, a secret hash divides\nthe vocabulary at each token position into a preferred “green” group and a less preferred “red” group. The\nmodel then gives higher chances to the green tokens, so the generated text contains a hidden cryptographic\nsignature. Light editing often keeps these watermarks, but bigger changes, like paraphrasing or replacing words\nwith synonyms, can remove them [ 13,18]. To make watermarks harder to notice, distortion-free methods hide\ninformation in the token-sampling order instead of the probability distribution. However, these can also become\nweak if someone rewrites the text in a tricky way [ 4]. Other approaches, called post-hoc schemes, add watermarks\nlater by changing syntax templates or choosing context-aware synonyms [ 38], but they also have trouble surviving\nstrong text changes.\nAnother important detection method takes advantage of the fact that language models (LMs) often choose\nvery predictable words. Gehrmann et al . [9] created a token-rank histogram tool to help people find words that\nare “too likely” in a sentence. Zero-shot methods like DetectGPT look at the “curvature” of local log-probabilities;\nif the curvature is negative, it suggests the text was written by a machine [ 22].Fast-DetectGPT makes this process\nfaster and more accurate for text from ChatGPT or GPT-4 [ 1]. Other detection methods use features such as\nnormalized perturbed rank [ 36] and the amount of unused information in token probability distributions [ 42].\nThese “white-box” methods work well when they can directly access the model’s probabilities, but they become\nless effective without this access. To solve this problem, “DNA” regeneration methods, like DNA-GPT [ 39] and\nGPT Paternity Test [ 41], do not need internal access; instead, they query a black-box model again and compare\nthe new outputs for similarity.\nStylometric approaches instead focus on word choice, sentence structure, and how the text is organized and\nconnected. Fröhling and Zubiaga [8]grouped 35 linguistic features into four categories, low diversity, repetition,\nincoherence, and lack of purpose, and trained SVM and random-forest models. More advanced work introduces\nentity-coherence graphs [ 20] or journalism-specific style metrics [ 17], which bolster detection on brief social-\nmedia texts. Though interpretable, these handcrafted methods may be narrower in scope and less transferable\nacross domains.\nA different direction uses fine-tuned neural networks to classify AI-generated text. OpenAI’s initial GPT-\ndetector trained RoBERTa on mixed GPT-2 corpora, achieving roughly 95% accuracy on in-domain data [ 34]. More\nrecent detectors employ Longformer [ 2] or ELECTRA to handle longer contexts and detect texts from multiple\ngenerators [ 5]. However, these systems often struggle with out-of-distribution domains. Recent “cross-model”\nor contrastive ensembles, such as Ghostbuster [ 37] and DeTeCtive [ 10], compare probability distributions from\nmultiple open-source LMs to improve robustness.\nEnsemble methods combine diverse signals to mitigate the weaknesses of individual detectors. Simple voting\nstacks of transformer-based classifiers outperform single models in multilingual tasks [ 29]. For example, recent\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 3 ---\nExplainability-Based Token Replacement on LLM-Generated Text •3\nwork introduces ensemble-based approaches for detecting AI-generated text in educational content, achieving\na favorable false-positive trade-off in academic scenarios [ 28]. Our work follows this line by designing an\nexplanation-oriented ensemble meant to withstand token-level adversarial edits.\nParaphrasing is one of the best ways to trick AI detectors. For example, Krishna et al . [16] showed that changing\nthe wording can make DetectGPT miss almost all AI-written text. Rewriting the text many times makes it even\nharder to detect [ 31]. Some new methods, like RADAR [ 11] and OUTFOX [ 15], are better at catching these\nchanges because they are trained on paraphrased examples, but they still only recover some accuracy. Even small\nmistakes, like typos, can also confuse detectors [ 12]. More advanced methods can handle some edits, but none\ncan stop all tricks or rewrites.\nParaphrasing is seen as the strongest way to avoid AI text detectors: Krishna et al . [16] found that rewriting\ntext can reduce DetectGPT’s ability to catch AI-generated text from 70% to less than 5%. Rewriting the text several\ntimes in a row (called “self-rewrites”) makes it even harder to detect, while still keeping the original meaning [ 31].\nDefenses such as RADAR [ 11] and OUTFOX [ 15] become stronger by training on paraphrased texts, and can\nrecover 10–20 F 1points. Recent studies also show that even simple changes at the letter level—like typos or\nswapping similar-looking letters—can confuse detectors and make them much less accurate [ 12]. Although using\nfeatures like ensemble cross-probability or advanced watermarking can resist some small changes, no method is\ncompletely safe from strong rewriting attacks.\nExplainability has become a cornerstone of modern NLP deployments [ 24]. XAI tools like SHAP and LIME\nshow which words or tokens have the biggest effect on a detector’s decisions. For example, Mitrovi’c et al . [23]\nused LIME to highlight important words and help people review short ChatGPT answers. But attackers can\nuse this information in the opposite way, by changing or removing the most important words. Zhou et al . [44]\nshowed that using SHAP-based synonyms for key words can lower detector accuracy by 40–60%. In our work, we\ncarefully study this double-sided use of explanations, testing four types of word substitutions in two languages\nand different types of texts.\nDetectors often make mistakes with texts written by people using English as a second language. Liang et al .\n[19] found that seven popular GPT detectors wrongly marked more than 61% of TOEFL essays by non-native\nEnglish speakers as AI-generated, even though they were very accurate with essays by native speakers. Because\nof this bias, some teachers are moving away from strict rules and are now encouraging students to use AI tools\nas support, especially for those with different language backgrounds. Since LLMs can quickly create lots of\nconvincing fake news, it is very hard for people alone to spot which content is real [ 43]. To improve detection in\nthese situations, it helps to use a mix of classifiers and to add information about where the text comes from [7].\nRecent use of AI chatbots for mental health support shows that people often think these systems are caring\nand human-like, which leads to real and meaningful conversations. But this can be risky, especially when users\ndo not realize they are talking to an AI. Song et al . [35] found that people in severe distress sometimes seek help\nfrom chatbots and even see them as human. In the same way, Siddals et al . [33] stressed the need for stronger\nsafety rules and clearer information in AI mental health tools. These studies show how important it is to have\ngood detection systems and to make it clear when someone is interacting with an AI, so that chatbots are used\nethically in sensitive cases. Research also shows that no single method is perfect; reliable AI-generated text\ndetection needs to use many signals, be tested against tricky cases, and give clear reasons for its decisions. These\nare the main goals of our work.\n3 Dataset\nIn this section, we detail the CLIN33 dataset [ 6] used as our primary corpus and describe how it was augmented\nto expand its linguistic variability. We also present the portion of the AuTexTification dataset1used to strengthen\n1https://zenodo.org/records/10732813\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 4 ---\n4•Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nour ensemble models. The CLIN33 shared task corpus comprises human-written and AIGTs in English and Dutch,\nincluding three domains: news, tweets, and reviews. Each domain contains 200 samples from human authors and\n200 samples from LLMs (GPT-4 and Vicuña-13B), producing a total of 1,200 texts in English and 1,200 in Dutch.\nThe human-written news samples come from well-known media outlets; the Dutch social media posts primarily\ndiscuss policy or health topics, whereas the English ones often revolve around social or sports events. Reviews\ntypically cover literature in Dutch and various consumer goods in English.\nAlthough CLIN33 offers balanced classes across two languages and three genres, its size remains relatively\nsmall for training large-scale models. To address this limitation and diversify the textual patterns, we created an\naugmented version of the dataset. We started with the original texts and applied a series of transformations to\neach sample. These transformations included synonym substitution, random word swaps, random insertions,\ntargeted deletions, variations in spelling, and back-translation2. Each text thus produced several new variants\nthat preserved the key semantics but introduced new surface forms. We ultimately obtained 9,720 total samples\nfor training, split evenly between human and AI-generated content (4,860 each) and distributed equally across\nthe three domains and the two languages. The final augmented corpus enables more robust feature learning.\nTable 1. Overview of data resources.\nAuTexTification (EN) Augmented CLIN33\nTotal Samples 33,845 9,720\nDomains/Genres Tweets, Legal, Wiki News, Twitter, Reviews\nLanguages English English, Dutch\nAI Models BLOOM GPT-4GPT-3 Vicuña-13B\nHuman Sources MultiEURLEX, Amazon, XSUM,\nTSATC, WikiLinguaNews, tweets, reviews\nClass Ratio 50.36% / 49.64 % 50% / 50%\nAugmentations None Yes (9)As an additional resource for pre-\ntraining and fine-tuning, we em-\nployed a subset of the AuTexTifi-\ncation dataset3, introduced by Sar-\nvazyan et al . [32] . While the complete\ndataset includes English and Spanish\ntexts in five distinct domains, we se-\nlectively used the English-only por-\ntion from tweets, legal, and wiki cat-\negories. This subset contains 33,845\ntexts designated for binary AI-detection (Task 1), split nearly evenly between human-written (50.36%) and\nAI-generated (49.64%) documents. The human content is drawn from sources such as MultiEURLEX, XSUM,\nAmazon Reviews, TSATC, and WikiLingua, while the AI-generated portion is produced by BLOOM (1B7, 3B,\n7B1) and GPT-3 (babbage, curie, text-davinci-003). Because of its class balance and domain variety, this subset\nof AuTexTification serves as a valuable source of pre-training examples for our ensemble. Table 1 summarizes\nthe essential attributes of the two datasets used to train and evaluate our methodology. The combined setup\ndraws from the 33,845 samples in AuTexTification (English only) and the 9,720 augmented CLIN33 samples\n(both English and Dutch). We use AuTexTification Dataset for initial pre-training, 90% of the CLIN data for the\nAugmenting and training and 10% of unchanged CLIN data for evaluating.\n4 Methodology\nWe used an approach that integrates several models for text classification to address the AIGT detection task.\nFirst, we converted all text to lowercase to handle tokens in a consistent way. We then removed non-informative\nelements such as URLs, punctuation, and special characters, since these tokens often do not carry semantic\nmeaning. After this cleaning, we split the text into tokens and applied lemmatization to map words to their base\nforms, which lowered data complexity and improved the model’s ability to recognize patterns.\nBaselines Models. We built an XGBoost classifier by first transforming the text into numerical features with a\nTF-IDF vectorizer set to a maximum of 5000 features. The XGBoost model was configured with use_label_encoder=False\nand eval_metric=’logloss’. Training used the TF-IDF representations from the training set, and final predictions\n2Translating from English to Dutch and then back to English\n3https://sites.google.com/view/autextification\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 5 ---\nExplainability-Based Token Replacement on LLM-Generated Text •5\non the test set were evaluated using precision, recall, F1, and accuracy. Also, We fine-tuned BERT-base (bert-base-\nuncased), DistilBERT (distilbert-base-uncased), and XLM-RoBERTa (xlm-roberta-base). The best checkpoint was\nchosen based on the highest F1 score on a validation subset. We then evaluated the final models on the test data.\nAlgorithm 1 Combined BERT Ensemble Structure\nInput: Text 𝑥\nOutput: Predicted label { human ,AI}\n1:function PretrainEnsemble (𝑥)\n2: bert_out←BERTModel frozen(𝑥)\n3: xlmr_out←XLMRobertaModel frozen(𝑥)\n4: distil_out←DistilBERTModel frozen(𝑥)\n5: return(bert_out ,xlmr_out ,distil_out)\n6:end function\n7:function FineTuneEnsemble (𝑥)\n8: bert_out_fresh ←BERTModel fresh(𝑥)\n9: xlmr_out_fresh ←XLMRobertaModel fresh(𝑥)\n10: distil_out_fresh ←DistilBERTModel fresh(𝑥)\n11: return(bert_out_fresh ,xlmr_out_fresh ,distil_out_fresh)\n12:end function\n13:function CombinedModel (𝑥)\n14:\n(froz_bert ,froz_xlmr ,froz_distil)←PretrainEnsemble (𝑥)\n15:(fresh_bert ,fresh_xlmr ,fresh_distil)←\nFineTuneEnsemble (𝑥)\n16:\nconcat_outs←concatenate\u0002\nfroz_bert ,froz_xlmr ,froz_distil ,\nfresh_bert ,fresh_xlmr ,fresh_distil\u0003\n17: dense_out←DenseLayer(concat_outs)## L2-regularized\ndense layer\n18: ˆ𝑦←𝜎(dense_out) ## Probability of AI-generated text\n19: ifˆ𝑦≥0.5then\n20: return AI\n21: else\n22: return human\n23: end if\n24:end functionEnsemble Model. We built an ensemble of multi-\nple transformers instead of relying on a single one to\nclassify human and AIGT), as inspired by Mohammadi\net al. [26] . In this method, we use the strengths of each\nmodel and create an ensemble that can be fine-tuned\non similar datasets. This helps the model adapt to dif-\nferent domains and reduces problems caused by lim-\nited data, as the features learned by each part comple-\nment each other. Specifically, we integrated bert-base-\nmultilingual-uncased, distilbert-base-multilingual-cased,\nand xlm-roberta-base. Each transformer produced a rep-\nresentation of the input text, and we concatenated these\nrepresentations before passing them through a dense\nlayer with L2 regularization for final classification.\nWe first built a Combined BERT model by training\nthree BERT models on the AuTexTification training set.\nWe optimized the network’s hyperparameters and eval-\nuated performance on the AuTexTification test set, fo-\ncusing on F1 and average metrics across folds. After\ntraining, we froze these model weights to retain the fea-\ntures they had learned and reduce the risk of overfitting.\nWe next created a combined model that merges three\nBERT models with frozen weights (trained on AuTex-\nTification) and three fresh BERT models with unfrozen\nparameters. These fresh models were fine-tuned on our\nnewly augmented dataset Such an ensemble synergy\naligns with Fraser et al . [7], who highlight the benefits\nof multi-model integration in detecting AI-generated text. Algorithm 1 outlines the training and process for our\ncombined BERT ensemble. During training, we minimize the binary cross-entropy loss on each branch using the\naugmented data. We then unify the outputs in a final fully connected layer, optionally employing majority voting\nif multiple final heads are used. Figure 1 shows the architecture, with frozen models on the left (gray) and fresh\nmodels on the right (green).\n4.1 Experimental Setup\nWe trained our models using the Hugging Face Transformers library4and set a maximum token length of 512.\nHyperparameter tuning was done through a random search over learning rates (ranging from 1×10−5to1×10−4)\nand batch sizes of 32, 64, or 128. We used a cosine decay schedule for learning rate adjustments, with 200 warm-up\nsteps and an early-stopping patience of 5 epochs based on validation loss.\nThe final training stage used 90% of the augmented development set, while we left 10% for test. The hyperpa-\nrameters and early-stopping rules were adjusted using the validation split. After training, we evaluated the system\non the remaining data, measuring precision, recall, F1-score, and accuracy. By combining different transformers,\n4https://huggingface.co/\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 6 ---\n6•Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nFig. 1. The architecture of the proposed ensemble model. Outputs of three frozen BERT models (left) and three fresh BERT\nmodels (right) are concatenated and passed through dense layers, followed by a voting mechanism.\nfreezing selectively learned features, and training fresh models in parallel, this ensemble method balanced the\nadvantages of robust pre-trained representations with the adaptability needed for the new dataset.\n4.2 Explainability-driven detection\nIn this section, we identify the words that most strongly influence an AI-generated prediction. Our approach\nuses two instance-level explanation tools: SHAP and LIME. Although they both operate on individual examples,\nSHAP uses game-theoretic principles to estimate a token’s overall (global) influence, while LIME explores small\nlocal perturbations to approximate importance. We take the absolute values of these importance scores 𝜙𝑗(for\nSHAP) or coefficients (for LIME) to capture both positive and negative contributions.\n𝜙𝑗=∑︁\n𝑆⊆{1,...,𝑚}\\{𝑗}|𝑆|!\u0000𝑚−|𝑆|−1\u0001!\n𝑚!\u0002\n𝑓(𝑆∪{𝑗})− 𝑓(𝑆)\u0003\n.We follow the methodology in [ 25], in this framework, we only\nanalyze sentences that the model correctly classifies, ensuring\nthe tokens we highlight have genuinely contributed to accurate\ndecisions. We then average each token’s importance within the\nsentence to get a consistent measure of its overall effect. This step\nhelps us confirm that the extracted terms are meaningful indicators of the model’s reasoning. Given a trained\nmodel 𝑓and an instance 𝑥composed of tokens (𝑡1, 𝑡2, . . . , 𝑡 𝑚), SHAP assigns a Shapley value 𝜙𝑗to each token\n𝑡𝑗. These values are computed by measuring how much including or excluding 𝑡𝑗changes the model’s output,\nacross all possible subsets of tokens. Formally:\nHere, 𝑓(𝑆)is the model’s prediction when only the tokens in set 𝑆are present. The weighting term ensures\neach token’s effect is fairly distributed among all subsets [ 21]. LIME, on the other hand, makes small changes near\nthe original text to learn a simpler, local model that approximates 𝑓. It then assigns coefficients reflecting each\ntoken’s importance for that instance [ 30]. Although LIME focuses more on local neighborhoods, it also provides\nper-token contributions based on how the model’s prediction shifts when individual tokens are perturbed. We\nalso consider a random token selection. This lets us compare how much improvement we gain by targeting\nthe tokens that SHAP or LIME indicates are most influential. By using both a global (SHAP) and local (LIME)\nperspective, we can better trust that the identified tokens genuinely drive the model’s decision. In our subsequent\nsteps, we use these tokens to guide targeted replacements and measure how effectively such edits can mask\nAI-generated content.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 7 ---\nExplainability-Based Token Replacement on LLM-Generated Text •7\n4.3 Token Replacement Strategies\nOnce we identify influential tokens, we modify them using several approaches designed to make the text appear\nmore human-like. We propose four rewriting strategies summarized in Table 2.\nTable 2. Summary of token replacement strategies\nStrategy Description\nHuman Similar Word Replacement (HSR) A Word2Vec model is trained on all human-labeled texts in the training set to learn an\nembedding space that captures typical human usage.\nPart-of-Speech Replacement (PSR) HSR is refined by ensuring that part-of-speech tags match.\nGPT-based Replacement (GPT) A generative model (GPT-4o-mini) is queried using the prompt: \"Replace ’{token}’ with\na more human-like word in this text: ’{text}’\" .\nGPT-based Replacement with Genre Context\n(GPT+Genre)The GPT-based method is extended by including explicit genre information (e.g., tweets or\nreviews ) in the prompt: \"Replace ’{token}’ in this {domain} text with a more\nhuman-like word: ’{text}’\" .\nInStrategy HSR , each influential token is replaced with its nearest neighbor in the Word2Vec space, provided\nthat the neighbor is frequently observed in human texts. In Strategy PSR , if a token 𝑡is tagged as a noun, for\ninstance, a Word2Vec neighbor that is also tagged as a noun is selected, preserving grammatical structure. In\nStrategy GPT , a replacement that appears more human-like is proposed based on the surrounding sentence. While\nthe suggestions are context-sensitive, certain LLM-specific patterns may still be retained if the generative model\nreintroduces specific phrasing. In Strategy GPT+Genre , incorporating domain-specific information allows the\ngenerative model to propose replacements that align more closely with the original style. However, this added\ncontext may also amplify distinctive AI traits. Finally, after substituting the most influential tokens in each text,\nwe assess performance along two dimensions: (1) changes in detectability and (2) fidelity to the original content.\n4.4 Evaluation Metrics\nWe use a combination of detection and text-similarity metrics to evaluate how rewriting strategies affect our\nmodels’ ability to detect AIGT, as well as how much the rewritten text deviates from its original form. First, once\nwe rewrite an AIGT via HSR, PSR, GPT, or GPT+Genre. Next, we investigate how different rewriting scenarios\naffect AI-text detection. We adopt the adversarial rewriting perspective discussed in Fraser et al . [7], where\nparaphrased or partially edited texts can undermine naive detectors. We run it through each classifier to check\nwhether the label remains AIor flips to human . We then calculate precision, recall, F1, and accuracy to capture\neach model’s classification performance to detect AI/human texts under these modified inputs. Additionally, we\ncompute the flip rate , defined as the fraction of AI texts whose predicted label changes to human after rewriting;\na higher flip rate implies that the rewriting method is more effective in evading detection. In parallel, we calculate\nthe degree of textual change introduced by each rewriting approach. We use BLEU, ROUGE-1, and ROUGE-L\nto measure the overlap between the original text and its rewritten form, with higher scores showing fewer\nmodifications. By combining detection metrics, and text-similarity scores, we gain a clear view of how effectively\neach rewriting method hides AI-generated content and how much it changes the original text.\n5 Experimental Analysis and Results\nIn this section, we compare five classification models (XGBoost, BERT-base, DistilBERT, XLM-RoBERTa, and\nourEnsemble approach) across various token-replacement scenarios for the detection of AIGT, Consistent\nwith Fraser et al . [7], we anticipate that multi-domain evaluation is critical for robust results. Our main objective\nis to observe how different rewriting methods (HSR, PSR, GPT, GPT+Genre) combined with token-selection\nstrategies (SHAP, LIME, Random) affect each model’s ability to detect generated text.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 8 ---\n8•Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nTable 3 shows each model’s performance on unmodified AIGT across two languages (English, Dutch) and three\ndomains (news, reviews, and X/Twitter). As can be seen, although certain single models perform well on specific\nsubsets (e.g., DistilBERT in English news, XGBoost in English Twitter), the Ensemble approach tends to achieve\nthe best performance in most cases. This pattern is particularly clear in Dutch news, where the Ensemble model\nattains perfect scores. Even when other models have high recall or precision on certain tasks, they often fall\nbehind on at least one metric (e.g., XLM-RoBERTa predicts too many positives, increasing recall but lowering\nprecision). showing strong cross-language adaptability. This result agrees with Fraser et al . [7], who emphasize\nthe importance of multi-lingual calibration for robust detection.\nTable 3. Model performance by language and domain on unmodified AI text. Bold indicates the best result within each\nlanguage and domain block.\nEnglish Dataset Dutch Dataset\nModel Domain Acc Prec Rec F1 Model Domain Acc Prec Rec F1\n— News — — News —\nXGBoost news 0.89 0.93 0.84 0.88 XGBoost news 0.85 0.88 0.88 0.88\nBERT-base news 0.94 0.97 0.91 0.94 BERT-base news 0.68 0.79 0.56 0.66\nDistilBERT news 0.92 0.90 0.95 0.92 DistilBERT news 0.68 0.73 0.66 0.69\nXLM-RoBERTa news 0.49 0.49 1.00 0.66 XLM-RoBERTa news 0.53 0.53 1.00 0.69\nEnsemble news 0.95 1.00 0.90 0.95 Ensemble news 0.99 0.98 1.00 0.99\n— Reviews — — Reviews —\nXGBoost reviews 0.51 0.57 0.59 0.58 XGBoost reviews 0.53 0.51 0.46 0.48\nBERT-base reviews 0.50 0.54 0.62 0.58 BERT-base reviews 0.40 0.35 0.30 0.32\nDistilBERT reviews 0.65 0.68 0.71 0.69 DistilBERT reviews 0.42 0.38 0.50 0.43\nXLM-RoBERTa reviews 0.56 0.56 0.90 0.69 XLM-RoBERTa reviews 0.43 0.43 0.60 0.48\nEnsemble reviews 0.67 0.71 0.68 0.70 Ensemble reviews 0.60 0.53 0.44 0.49\n— Twitter / X — — Twitter / X —\nXGBoost twitter 0.91 0.92 0.98 0.95 XGBoost twitter 0.80 0.85 0.78 0.81\nBERT-base twitter 0.90 0.86 0.98 0.92 BERT-base twitter 0.80 0.85 0.78 0.81\nDistilBERT twitter 0.90 0.86 0.98 0.92 DistilBERT twitter 0.84 0.86 0.80 0.83\nXLM-RoBERTa twitter 0.60 0.53 1.00 0.69 XLM-RoBERTa twitter 0.62 0.60 0.90 0.72\nEnsemble twitter 0.93 0.86 1.00 0.93 Ensemble twitter 0.87 0.90 0.85 0.87\nWe next investigate how different rewriting scenarios affect AI-text detection. In particular, we apply four\nrewriting strategies and combine each of these with one of three token-selection strategies: SHAP (prioritizes\nglobally influential tokens), LIME (locally influential tokens), or Random (tokens chosen arbitrarily). Table 4\npresents performance metrics for each model under all these rewriting conditions.\nGeneral Trends. When no rewriting is applied ( NoRewriting, Original ), most models exhibit reasonably strong\nperformance (precision and recall around 0.75–0.83), with the Ensemble reaching the highest overall F1 (0.83)\nand accuracy (0.83). After rewriting, simpler models often detect less, especially if the replaced tokens were the\nones they used to identify AI patterns.\nComparison of Token-Selection Approaches. SHAP or LIME-based replacements often make larger drops in\ndetection metrics than Random replacements, since replacing highly influential tokens more effectively removes\nAI indicators. For instance, comparing LIME+HSR vs. Random+HSR, XGBoost’s F1 can decrease from around\n0.73 to 0.25 if LIME is used, indicating a big loss in detection ability once key tokens are perturbed. On the other\nhand, random token replacement leaves more of the AI signals intact, so detection remains higher.\nInfluence of Rewriting Methods. HSR and PSR replace tokens with synonyms from a human corpus, often\npreserving the original text’s structure. This is reflected in relatively high BLEU scores (e.g., over 90% in the\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 9 ---\nExplainability-Based Token Replacement on LLM-Generated Text •9\nTable 4. F1 score and Accuracy of each detector under every rewriting scenario. The best metric in a column is bolded ; the\nlargest negative drop is , the smallest negative drop is .......Δ=(Scenario Value−Original Value)×100% .\nExplain StrategyXGBoost BERT-base DistilBERT XLM-RoBERTa Ensemble\nF1 (Δ) Acc ( Δ) F1 ( Δ) Acc ( Δ) F1 ( Δ) Acc ( Δ) F1 ( Δ) Acc ( Δ) F1 ( Δ) Acc ( Δ)\nNoRewriting Original 0.81 0.80 0.74 0.75 0.79 0.77 0.68 0.54 0.83 0.83\nSHAP HSR 0.73 ( -8%) 0.75 ( -5%) 0.71 (-3%) 0.72 (-3%) 0.76 (-3%) 0.75 (-2%) 0.68 ( ...0%) 0.54 ( ...0%) 0.78 (-5%) 0.80 (-3%)\nSHAP PSR 0.78 ( -3%) 0.78 ( -2%) 0.73 (-1%) 0.74 (-1%) 0.78 (-1%) 0.77 (0%) 0.68 ( ...0%) 0.54 ( ...0%) 0.82 (-1%) 0.82 (-1%)\nSHAP GPT 0.79 (-2%) 0.79 (-1%) 0.73 (-1%) 0.74 (-1%) 0.77 (-2%) 0.76 (-1%) 0.68 ( ...0%) 0.55 ( .....+1%) 0.80 (-3%) 0.81 (-2%)\nSHAP GPT+Genre 0.80 (-1%) 0.80 (0%) 0.73 (-1%) 0.74 (-1%) 0.77 (-2%) 0.76 (-1%) 0.68 ( ...0%) 0.55 ( .....+1%) 0.79 ( -4%) 0.80 ( -3%)\nLIME HSR 0.25 ( -56%) 0.48 ( -32%) 0.56 (-18%) 0.62 (-13%) 0.62 (-17%) 0.65 (-12%) 0.67 (....-1%) 0.52 ( .....-2%) 0.64 (-19%) 0.69 (-14%)\nLIME PSR 0.56 ( -25%) 0.63 ( -17%) 0.64 (-10%) 0.67 (-8%) 0.71 (-8%) 0.71 (-6%) 0.66 ( .....-2%) 0.52 ( .....-2%) 0.73 (-10%) 0.75 (-8%)\nLIME GPT 0.57 ( -24%) 0.64 ( -16%) 0.73 (-1%) 0.74 (-1%) 0.78 (-1%) 0.77 (0%) 0.68 ( ...0%) 0.55 ( .....+1%) 0.83 (...0%) 0.83 (0%)\nLIME GPT+Genre 0.60 ( -21%) 0.65 ( -15%) 0.75 (.....+1%) 0.75 (0%) 0.77 (-2%) 0.76 (-1%) 0.68 (0%) 0.55 ( .....+1%) 0.81 (-2%) 0.81 (-2%)\nRandom HSR 0.73 (-8%) 0.74 (-6%) 0.63 ( -11%) 0.67 ( -8%) 0.68 ( -11%) 0.69 ( -8%) 0.67 ( .....-1%) 0.53 ( .....-1%) 0.73 (-10%) 0.75 (-8%)\nRandom PSR 0.76 (-5%) 0.76 (-4%) 0.65 ( -9%) 0.68 ( -7%) 0.74 (-5%) 0.73 (-4%) 0.67 ( .....-1%) 0.53 ( .....-1%) 0.78 (-5%) 0.79 (-4%)\nRandom GPT 0.74 ( -7%) 0.75 ( -5%) 0.74 (0%) 0.75 (0%) 0.77 (-2%) 0.76 (-1%) 0.68 ( ...0%) 0.55 ( .....+1%) 0.81 (-2%) 0.82 (-1%)\nRandom GPT+Genre 0.75 ( -6%) 0.75 ( -5%) 0.73 (-1%) 0.74 (-1%) 0.78 (-1%) 0.77 (0%) 0.68 (0%) 0.55 ( .....+1%) 0.81 (-2%) 0.81 (-2%)\nSHAP+HSR or SHAP+PSR rows), meaning minimal rewriting. Nonetheless, these small edits can still mislead\ndetectors if the replaced tokens were crucial. GPT-based rewrites typically lower BLEU into the 70s or 80s,\nreflecting more substantial paraphrases. In turn, many detectors (e.g., XGBoost or XLM-RoBERTa) experience\nan increase in recall but a decrease in precision, or the opposite, because the text shifts away from familiar AI\nphrasing. GPT+Genre further contextualizes the rewrite with domain hints (e.g. write it like a news article),\ncausing domain-sensitive lexical changes. This can introduce new phrasing that confuses detectors reliant on\ndomain-specific cues.\nModel-Specific Observations. XLM-RoBERTa often shows high recall (sometimes hitting 1.0), but its precision\nremains around 0.52, leading to only modest changes in F1. DistilBERT is more balanced yet remains vulnerable\nwhen its top keywords are replaced by synonyms or paraphrases. BERT-base typically holds a moderate advantage\nacross scenarios but can be undermined in LIME-based rewrites (e.g., LIME+HSR, where BERT-base F1 falls to\nabout 0.56). By contrast, the Ensemble approach consistently retains the highest or near-highest F1 and accuracy.\nFor example, under LIME+GPT, the Ensemble still attains an F1 of 0.83, significantly above XGBoost’s 0.57 or\nBERT-base’s 0.73. Similarly, in SHAP plus GPT+Genre, the Ensemble preserves a strong F1 of about 0.79, while\nother models vary more dramatically. Also, We show text similarity scores (BLEU, ROUGE1, ROUGEL) to capture\nhow extensively each rewrite modifies the original text.\nText Similarity Implications. Table 5 presents text similarity scores (BLEU, ROUGE1, ROUGEL) under all\nthese rewriting conditions. Looking at BLEU and ROUGE scores confirms that GPT-based methods, especially\nGPT+Genre, introduce the largest textual shifts. Replacements guided by synonyms (HSR or PSR) keep BLEU\nnear or above 90, whereas GPT rewrites can dip into the mid-70s. This trade-off highlights how more extensive\nrewriting can more thoroughly remove AI signatures at the expense of text fidelity.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 10 ---\n10 •Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nTable 5. BLEU, ROUGE -1 and ROUGE -L scores for every rewriting scenario.\nExplain Strategy BLEU ( Δ) ROUGE-1 ( Δ) ROUGE-L ( Δ)\nNoRewriting Original 1,00 1,00 1,00\nSHAP HSR 0,93 (-7%) 0,97 (-3%) 0,97 (-3%)\nSHAP PSR 0,97 (....-3%) 0,99 (.....-1%) 0,99 (.....-1%)\nSHAP GPT 0,86 (-14%) 0,93 (-7%) 0,93 (-7%)\nSHAP GPT+Genre 0,86 (-14%) 0,94 (-6%) 0,94 (-6%)\nLIME HSR 0,84 (-16%) 0,92 (-8%) 0,92 (-8%)\nLIME PSR 0,90 (-10%) 0,96 (-5%) 0,95 (-5%)\nLIME GPT 0,76 ( -24%) 0,84 ( -16%) 0,84 ( -16%)\nLIME GPT+Genre 0,76 ( -24%) 0,85 (-15%) 0,84 ( -16%)\nRandom HSR 0,84 (-16%) 0,92 (-8%) 0,92 (-8%)\nRandom PSR 0,90 (-11%) 0,95 (-5%) 0,95 (-5%)\nRandom GPT 0,76 ( -24%) 0,84 ( -16%) 0,84 ( -16%)\nRandom GPT+Genre 0,76 ( -24%) 0,84 ( -16%) 0,84 ( -16%)Flip Rates and Overlapping Flips .\nTo estimate how many AIGTs are rela-\nbeled as human after rewriting, we track\nflip rates for each combination of ex-\nplainability method, rewriting strategy,\nand model. A higher flip rate shows that\nthe corresponding model is more eas-\nily deceived by that particular rewriting\napproach. In these experiments, LIME-\nbased edits coupled with HSR lead to no-\ntably high misclassification rates in sim-\npler models such as XGBoost (flipping\nup to 65% of the samples), whereas XLM-\nRoBERTa remains relatively robust at a\nmere 2.5% flip rate. GPT-based rewrites\nresult in mid-level flip percentages overall; for example, under LIME+GPT, XGBoost flips 31.67% of texts while\nBERT-base and DistilBERT are both at around 9.17%. Moreover, SHAP+PSR frequently produces lower flip rates,\nshowing that while part-of-speech-constrained edits can deceive less sophisticated classifiers, advanced or ensem-\nble approaches remain relatively unaffected (with an Ensemble flip rate of only 2.5%). Notably, XLM-RoBERTa’s\ntendency to overpredict AIkeeps its flips artificially low. The Ensemble, although not entirely immune, stays in\nthe lower to mid-range across all scenarios, once again showing greater robustness than single-model detectors.\nTable 6 provides a detailed breakdown of which sets of models flip each sample in various scenarios.\nAs shown in Table 6, many rewriting strategies cause a lot of flips for some detectors, especially the simpler\nones. However, the Ensemble model usually has a lower flip rate. This shows that using several models together\nmakes detection more reliable. Figure 2 also shows how the flipped items overlap for each rewriting method.\nEven when many detectors are affected by these edits, the Ensemble is less likely to be fooled, highlighting the\nadvantage of using an ensemble approach\nIn general, GPT-based replacements make text more natural or domain-specific, which greatly impacts models\nthat rely on simple token patterns for detection. HSR and PSR (especially with SHAP/LIME) can also be effective\nwhile keeping a higher BLEU score, meaning fewer changes. Since PSR aligns part-of-speech tags, the text\nremains fluent but loses patterns AI might use, leading to performance drops in classifiers like XLM-RoBERTa\nand XGBoost. In contrast, our Ensemble model is more robust, as its combined transformer knowledge helps\nit detect AI content even after targeted token edits. Overall, our results show that the Ensemble model is the\nmost accurate and resilient, effectively handling English and Dutch texts across different genres. While rewriting\nreduces detection rates, it does not fully bypass the Ensemble model.\n6 Discussion and Conclusion\nOur findings show that explanation techniques such as SHAP and LIME can guide systematic token replacements\nthat make AIGT less likely to be classified as AI-generated . When we identify and edit tokens most indicative\nof artificial content, simpler models like XGBoost can be misled at high rates, especially under LIME-based\nrewrites combined with synonym substitutions. Yet even more advanced models, such as BERT-base or DistilBERT,\nstill show vulnerability in certain scenarios, particularly when GPT-generated replacements transform the text\nmore extensively. These methods, however, do not completely deceive our Ensemble model, which remains\ncomparatively robust across languages, genres, and rewriting methods.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 11 ---\nExplainability-Based Token Replacement on LLM-Generated Text •11\nTable 6. Flip rates (%) from AItohuman , grouped by rewriting strategies. Each scenario has 120 AI-labeled items; higher\nvalues indicate greater susceptibility to “flipping.”\n(a) HSR & PSR Strategies\nExplain Strategy Model #AI Flips Flip Rate\nSHAPHSRXGBoost 120 17 14 .17\nBERT 120 7 5 .83\nDistilBERT 120 9 7 .50\nXLM-R 120 1 0 .83\nEnsemble 120 8 6 .67\nPSRXGBoost 120 7 5 .83\nBERT 120 4 3 .33\nDistilBERT 120 4 3 .33\nXLM-R 120 0 0 .00\nEnsemble 120 3 2 .50\nLIMEHSRXGBoost 120 78 65 .00\nBERT 120 31 25 .83\nDistilBERT 120 32 26 .67\nXLM-R 120 3 2 .50\nEnsemble 120 33 27 .50\nPSRXGBoost 120 46 38 .33\nBERT 120 20 16 .67\nDistilBERT 120 19 15 .83\nXLM-R 120 5 4 .17\nEnsemble 120 19 15 .83\nRandomHSRXGBoost 120 15 12 .50\nBERT 120 20 16 .67\nDistilBERT 120 22 18 .33\nXLM-R 120 2 1 .67\nEnsemble 120 20 16 .67\nPSRXGBoost 120 10 8 .33\nBERT 120 17 14 .17\nDistilBERT 120 11 9 .17\nXLM-R 120 1 0 .83\nEnsemble 120 14 11 .67(b) GPT & GPT+Genre Strategies\nExplain Strategy Model #AI Flips Flip Rate\nSHAPGPTXGBoost 120 4 3 .33\nBERT 120 7 5 .83\nDistilBERT 120 7 5 .83\nXLM-R 120 0 0 .00\nEnsemble 120 10 8 .33\nGPT+GenreXGBoost 120 4 3 .33\nBERT 120 3 2 .50\nDistilBERT 120 6 5 .00\nXLM-R 120 0 0 .00\nEnsemble 120 11 9 .17\nLIMEGPTXGBoost 120 38 31 .67\nBERT 120 11 9 .17\nDistilBERT 120 11 9 .17\nXLM-R 120 0 0 .00\nEnsemble 120 11 9 .17\nGPT+GenreXGBoost 120 41 34 .17\nBERT 120 4 3 .33\nDistilBERT 120 7 5 .83\nXLM-R 120 0 0 .00\nEnsemble 120 9 7 .50\nRandomGPTXGBoost 120 16 13 .33\nBERT 120 6 5 .00\nDistilBERT 120 6 5 .00\nXLM-R 120 0 0 .00\nEnsemble 120 5 4 .17\nGPT+GenreXGBoost 120 13 10 .83\nBERT 120 6 5 .00\nDistilBERT 120 9 7 .50\nXLM-R 120 0 0 .00\nEnsemble 120 11 9 .17\nFrom the perspective of text similarity, we see that methods like HSR and PSR typically achieve higher BLEU\nand ROUGE scores, indicating minimal alterations to the original text. This can preserve meaning and style,\nbut targeted token swaps can still hide enough AI signals to disrupt certain detectors. In contrast, GPT-based\nrewrites, especially GPT+Genre, often provide more comprehensive revisions, making the text harder to classify\nbut also changing more from the source. Although such extensive rewriting can produce higher flip rates, our\nEnsemble classifier still maintains better overall performance than single-model systems.\nIn practice, these results underscore both the potential and the limitations of adversarial rewriting. Minor edits\nguided by SHAP or LIME can undermine certain classifiers. Future work might extend to more sophisticated\nparaphrasing of entire sentences or incorporate style transfer models that better replicate a specific human author.\nAt the same time, it is crucial to consider watermarking and policy frameworks that ensure transparency when\nAIGT is involved. Our experiments show that even when AI content is skillfully masked, detection remains\npossible with multi-architecture approaches.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 12 ---\n12 •Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nFig. 2. Overlaps among flipped AI samples under different explainability methods and rewriting strategies. Each bar\ncorresponds to the number of samples simultaneously flipped by one or more models.\nIn conclusion, our study shows how explanation methods can inform token replacements to disguise AIGT,\nyet also reveals that robust ensemble classifiers are harder to deceive. Balancing improved detection methods\nwith the need for responsible use of writing aids will remain an important challenge. We hope these findings\ncontribute to a deeper understanding of both the potential vulnerabilities in existing AI detection models and the\nmethods that can keep them resilient against adversarial transformations.\nAcknowledgments\nWe thank the CLIN33 shared task organizers for making the dataset available, and we appreciate the helpful\nfeedback from colleagues who guided our research directions. This work is partially supported by the Dutch\nResearch Council (NWO) (grant number: VI.Vidi.195.152 to D. L. Oberski). We also thank SURF for providing the\ncomputational facilities.\nReferences\n[1]Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. Fast-detectgpt: Efficient zero-shot detection of\nmachine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130 (2023).\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 13 ---\nExplainability-Based Token Replacement on LLM-Generated Text •13\n[2]Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150\n(2020).\n[3]Tom B. Brown, Benjamin Mann, Nick Ryder, et al .2020. Language Models are Few-Shot Learners. ArXiv abs/2005.14165 (2020).\nhttps://api.semanticscholar.org/CorpusID:218971783\n[4]Miranda Christ, Sam Gunn, and Or Zamir. 2024. Undetectable watermarks for language models. In The Thirty Seventh Annual Conference\non Learning Theory . PMLR, 1125–1139.\n[5]Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators\nrather than generators. arXiv preprint arXiv:2003.10555 (2020).\n[6]Pieter Fivez, Walter Daelemans, Tim Van de Cruys, , et al .2024. The CLIN33 Shared Task on the Detection of Text Generated by Large\nLanguage Models. Computational Linguistics in the Netherlands Journal 13 (2024), 233–259.\n[7]Kathleen C Fraser, Hillary Dawkins, and Svetlana Kiritchenko. 2025. Detecting ai-generated text: Factors influencing detectability with\ncurrent methods. Journal of Artificial Intelligence Research 82 (2025), 2233–2278.\n[8]Leon Fröhling and Arkaitz Zubiaga. 2021. Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover.\nPeerJ Computer Science 7 (2021), e443.\n[9]Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019. Gltr: Statistical detection and visualization of generated text.\narXiv preprint arXiv:1906.04043 (2019).\n[10] Xun Guo, Yongxin He, Shan Zhang, Ting Zhang, Wanquan Feng, Haibin Huang, and Chongyang Ma. 2024. Detective: Detecting\nai-generated text via multi-level contrastive learning. Advances in Neural Information Processing Systems 37 (2024), 88320–88347.\n[11] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. 2023. RADAR: Robust AI-Text Detection via Adversarial Learning. ArXiv abs/2307.03838\n(2023). https://api.semanticscholar.org/CorpusID:259501842\n[12] Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, and Zhouwang Yang. 2024. Are AI-Generated Text Detectors\nRobust to Adversarial Perturbations?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) . 6005–6024.\n[13] John Kirchenbauer, Jonas Geiping, Yuxin Wen, et al .2023. On the Reliability of Watermarks for Large Language Models. ArXiv\nabs/2306.04634 (2023). https://api.semanticscholar.org/CorpusID:259095643\n[14] John Kirchenbauer, Jonas Geiping, Yuxin Wen, et al .2023. A Watermark for Large Language Models. In International Conference on\nMachine Learning . https://api.semanticscholar.org/CorpusID:256194179\n[15] Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2024. Outfox: Llm-generated essay detection through in-context learning with\nadversarially generated examples. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 21258–21266.\n[16] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023. Paraphrasing evades detectors of ai-generated\ntext, but retrieval is an effective defense. Advances in Neural Information Processing Systems 36 (2023), 27469–27500.\n[17] Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy Roschke, Dan Gillmor, Scott Ruston, Huan Liu, and Joshua Garland.\n2023. J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News. In Proceedings of the 13th International Joint\nConference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers) . 484–497.\n[18] Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, and Tianwei Zhang. 2023. Towards the Vulnerability of Watermarking\nArtificial Intelligence Generated Content. arXiv preprint arXiv:2310.07726 (2023).\n[19] Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. 2023. GPT detectors are biased against non-native English\nwriters. Patterns 4, 7 (2023), 100779.\n[20] Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao Shen. 2023. Coco: Coherence-enhanced machine-generated\ntext detection under low resource with contrastive learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing . 16167–16188.\n[21] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Neural Information Processing Systems .\nhttps://api.semanticscholar.org/CorpusID:21889700\n[22] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, et al .2023. DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability\nCurvature. In International Conference on Machine Learning . https://api.semanticscholar.org/CorpusID:256274849\n[23] Sandra Mitrovi’c, Davide Andreoletti, and Omran Ayoub. 2023. ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine\nLearning Model for Detecting Short ChatGPT-generated Text. ArXiv abs/2301.13852 (2023). https://api.semanticscholar.org/CorpusID:\n256416337\n[24] Hadi Mohammadi, Ayoub Bagheri, Anastasia Giahanou, et al .2025. Explainability in Practice: A Survey of Explainable NLP Across\nVarious Domains. ArXiv abs/2502.00837 (2025). https://api.semanticscholar.org/CorpusID:276094504\n[25] Hadi Mohammadi, Anastasia Giachanou, and Ayoub Bagheri. 2024. A Transparent Pipeline for Identifying Sexism in Social Media:\nCombining Explainability with Model Prediction. Applied Sciences (2024). https://api.semanticscholar.org/CorpusID:272894586\n[26] Hadi Mohammadi, Anastasia Giahanou, and Ayoub Bagheri. 2023. Towards Robust Online Sexism Detection: A Multi-Model Approach\nwith BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks. In Conference and Labs of the Evaluation Forum . https://api.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 14 ---\n14 •Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\nsemanticscholar.org/CorpusID:264441492\n[27] L. Nacke et al .2024. AI-authored abstracts perceived as more authentic than human-written ones. Computers in Human Behavior:\nArtificial Humans (2024).\n[28] Ayat A Najjar, Huthaifa I Ashqar, Omar A Darwish, and Eman Hammad. 2025. Detecting AI-Generated Text in Educational Content:\nLeveraging Machine Learning and Explainable AI for Academic Integrity. arXiv preprint arXiv:2501.03203 (2025).\n[29] Duke Nguyen, Khaing Myat Noe Naing, and Aditya Joshi. 2023. Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text\nDetection. Language Technology Association (2023), 173.\n[30] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?”: Explaining the Predictions of Any\nClassifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016). https:\n//api.semanticscholar.org/CorpusID:13029170\n[31] G. Sadasivan et al. 2023. Can AI Detectors Detect ChatGPT? Analysis and Challenges. arXiv preprint arXiv:2303.11156 .\n[32] Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, et al .2023. Overview of AuTexTification at IberLEF 2023:\nDetection and Attribution of Machine-Generated Text in Multiple Domains. Proces. del Leng. Natural 71 (2023), 275–288. https:\n//api.semanticscholar.org/CorpusID:262055483\n[33] Steven Siddals, John Torous, and Astrid Coxon. 2024. “It happened to be the perfect thing”: experiences of generative AI chatbots for\nmental health. npj Mental Health Research 3, 1 (2024), 48.\n[34] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook\nKim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203 (2019).\n[35] Inhwa Song, Sachin R Pendse, Neha Kumar, and Munmun De Choudhury. 2024. The typing cure: Experiences with large language\nmodel chatbots for mental health support. arXiv preprint arXiv:2401.14362 (2024).\n[36] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. Detectllm: Leveraging log rank information for zero-shot detection of\nmachine-generated text. arXiv preprint arXiv:2306.05540 (2023).\n[37] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. 2023. Ghostbuster: Detecting text ghostwritten by large language models.\narXiv preprint arXiv:2305.15047 (2023).\n[38] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu. 2023. Watermarking text generated\nby black-box language models. arXiv preprint arXiv:2305.08883 (2023).\n[39] Xianjun Yang, Wei Cheng, Yue Wu, Linda Petzold, William Yang Wang, and Haifeng Chen. 2023. Dna-gpt: Divergent n-gram analysis\nfor training-free detection of gpt-generated text. arXiv preprint arXiv:2305.17359 (2023).\n[40] Catherine Yeh, Gonzalo A. Ramos, Rachel Ng, et al .2024. GhostWriter: Augmenting Collaborative Human-AI Writing Experiences\nThrough Personalization and Agency. ArXiv abs/2402.08855 (2024). https://api.semanticscholar.org/CorpusID:267658049\n[41] Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and Nenghai Yu. 2023. Gpt paternity test:\nGpt generated text detection with gpt genetic inheritance. CoRR (2023).\n[42] Krystian Zawistowski. 2024. Unused information in token probability distribution of generative LLM: improving LLM reading\ncomprehension through calculation of expected values. arXiv preprint arXiv:2406.10267 (2024).\n[43] Rowan Zellers, Ari Holtzman, Hannah Rashkin, et al .2019. Defending Against Neural Fake News. ArXiv abs/1905.12616 (2019).\nhttps://api.semanticscholar.org/CorpusID:168169824\n[44] Ying Zhou, Ben He, and Le Sun. 2024. Humanizing machine-generated content: evading AI-text detection through adversarial attack.\narXiv preprint arXiv:2404.01907 (2024).\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 15 ---\nExplainability-Based Token Replacement on LLM-Generated Text •15\nReproducibility Checklist for JAIR\nAll articles:\n(1)All claims investigated in this work are clearly stated.\nAnswer: Yes, we clearly state our main claims: we want to see how effective rewriting with SHAP/LIME is,\nand how an ensemble model performs.\n(2)Clear explanations are given how the work reported substantiates the claims.\nAnswer: Yes, we explain how each experiment’s results support our claims.\n(3)Limitations or technical assumptions are stated clearly and explicitly.\nAnswer: Some. We focus on detection and rewriting only, not on other possible aspects.\n(4)Conceptual outlines and/or pseudo-code descriptions of the AI methods introduced in this work\nare provided, and important implementation details are discussed.\nAnswer: Yes, our method flow is clearly described, especially for the ensemble approach in the algorithm 1.\n(5)Motivation is provided for all design choices, including algorithms, implementation choices,\nparameters, data sets, and experimental protocols beyond metrics.\nAnswer: Yes, we explain why we use these models, datasets, and rewriting strategies.\nArticles containing theoretical contributions\nDoes this paper make theoretical contributions?\nAnswer: No. (We do not do formal theoretical proofs.)\nArticles reporting on computational experiments\nDoes this paper include computational experiments?\nAnswer: Yes.\n(1)All source code required for experiments is included or will be made publicly available upon\npublication, following best practices.\nAnswer: We plan to share the main code base (e.g., on GitHub).\n(2)The source code comes with a license that allows free usage for reproducibility purposes.\nAnswer: Yes, we use a permissive license.\n(3)The source code comes with a license that allows free usage for research purposes in general.\nAnswer: Yes.\n(4)Raw, unaggregated data from all experiments is included or will be made publicly available.\nAnswer: We will share our final splits, but some augmented data might be partial.\n(5)The unaggregated data comes with a license that allows free usage for reproducibility purposes.\nAnswer: Yes, as far as allowed by the original dataset licenses.\n(6)The unaggregated data comes with a license that allows free usage for research purposes in\ngeneral.\nAnswer: Yes, within the limits of the original dataset licenses.\n(7)If an algorithm depends on randomness, the method used for generating random numbers and\nsetting seeds is described sufficiently.\nAnswer: We set random seeds but do not deeply document random generation details.\n(8)The execution environment for experiments (hardware/software) is described.\nAnswer: Yes, we mention key libraries (e.g., Hugging Face Transformers, PyTorch).\n(9)The evaluation metrics used are clearly explained and motivated.\nAnswer: Yes, we use F1, accuracy, BLEU, and ROUGE and explain why.\nJAIR, Vol. 1, Article . Publication date: June 2025.\n--- Page 16 ---\n16 •Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, and Ayoub Bagheri\n(10)The number of algorithm runs used to compute each result is reported.\nAnswer: We use cross-validation or mention repeated runs (varies).\n(11)Reported results have not been “cherry-picked.”\nAnswer: Yes, we show both successes and failures.\n(12)Analysis of results goes beyond single-dimensional summaries of performance.\nAnswer: We show F1, accuracy, and text-similarity. (We do not always show standard deviation or confidence\nintervals.)\n(13)All (hyper-) parameter settings for the algorithms/methods used are reported, along with the\nrationale or method for determining them.\nAnswer: Yes, we provide learning rates, batch sizes, etc.\n(14)The number and range of (hyper-) parameter settings explored prior to final experiments have\nbeen indicated.\nAnswer: We mention random search, though not each detail.\n(15)Appropriately chosen statistical hypothesis tests are used to establish significance in the presence\nof noise.\nAnswer: We do not do formal significance testing.\nArticles using data sets\nDoes this work rely on one or more data sets?\nAnswer: Yes.\n(1)All newly introduced data sets are included in an online appendix or will be made publicly\navailable with a suitable license.\nAnswer: We mostly use public sets (CLIN33, AuTexTification) and will release our augmented version.\n(2)The newly introduced data set comes with a license that allows free usage for reproducibility\npurposes.\nAnswer: Yes, for the augmented subset.\n(3)The newly introduced data set comes with a license that allows free usage for research purposes\nin general.\nAnswer: Yes.\n(4)All data sets drawn from the literature or other public sources are accompanied by appropriate\ncitations.\nAnswer: Yes, we cite the original authors.\n(5)All data sets drawn from the existing literature are publicly available.\nAnswer: Yes, with possible constraints.\n(6)All new or non-public data sets are described in detail, including relevant statistics, collection,\nand annotation processes.\nAnswer: Yes, for our augmented data.\n(7)All methods used for preprocessing, augmenting, batching, or splitting data sets are described in\ndetail.\nAnswer: Yes, we detail how we clean and split.\nExplanations on any of the answers above (optional):\nWe use mostly public data and provide augmented sets where possible. Some data licensing is inherited from\nthe original corpora. Where possible, we provide reproducible code, scripts, and subsets to ensure others can\nreplicate our results.\nJAIR, Vol. 1, Article . Publication date: June 2025.",
  "text_length": 61344
}