{
  "id": "http://arxiv.org/abs/2506.01096v1",
  "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model\n  Reasoning",
  "summary": "Large language models are increasingly used for complex reasoning tasks where\nhigh-quality offline data such as expert-annotated solutions and distilled\nreasoning traces are often available. However, in environments with sparse\nrewards, reinforcement learning struggles to sample successful trajectories,\nleading to inefficient learning. At the same time, these offline trajectories\nthat represent correct reasoning paths are not utilized by standard on-policy\nreinforcement learning methods. To address this limitation, we propose SuperRL,\na unified training framework that adaptively incorporates offline supervision\ninto reinforcement learning. SuperRL introduces an Adaptive Switch to detect\nsparse reward conditions and activates a Hybrid Actor when necessary. The\nHybrid Actor integrates policy gradient and supervised learning objectives at\nthe loss level, enabling the model to benefit from accurate offline reasoning\nsignals while maintaining the exploratory capacity of reinforcement learning.\nExperiments on a range of reasoning benchmarks show that SuperRL consistently\noutperforms standard reinforcement learning by improving sample efficiency,\ngeneralization, and robustness under sparse rewards.",
  "authors": [
    "Yihao Liu",
    "Shuocheng Li",
    "Lang Cao",
    "Yuhang Xie",
    "Mengyu Zhou",
    "Haoyu Dong",
    "Xiaojun Ma",
    "Shi Han",
    "Dongmei Zhang"
  ],
  "published": "2025-06-01T17:43:54Z",
  "updated": "2025-06-01T17:43:54Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01096v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01096v1  [cs.AI]  1 Jun 2025SuperRL: Reinforcement Learning with Supervision\nto Boost Language Model Reasoning\nYihao Liu1∗Shuocheng Li1∗Lang Cao2∗Yuhang Xie1∗\nMengyu Zhou3†Haoyu Dong3Xiaojun Ma3Shi Han3Dongmei Zhang3\n1Peking University2University of Illinois Urbana-Champaign3Microsoft\nAbstract\nLarge language models (LLMs) are increasingly applied to complex reasoning\ntasks, where rich offline data—such as expert-annotated solutions or distilled\ntraces—are often available. However, conventional reinforcement learning (RL)\nfails to exploit this data effectively, especially under sparse-reward environment.\nWe propose SuperRL, a unified training framework to solve the problem. SuperRL\nemploys an Adaptive Switch to detect reward sparsity and invokes a Hybrid Actor\nwhen rewards are sparse, blending policy gradients with offline supervision to\nstabilize learning. Experiments across diverse reasoning benchmarks demonstrate\nthat SuperRL consistently outperforms standard RL, delivering improved sample\nefficiency, generalization, and robustness under sparse rewards.\n·\nReward State\nValuesAction\nStateAdaptive Switch\nActor -Critic RL\nSuperRL Framework\nDataset\n Offline\nDataK-step\nTraining\nReward\nAnalysisActor\nSelectReward\nDensity\nSignal\nVanilla RL\nActor\nHybrid RL\nActor\nEnvironment\nVanilla RL\nCritic\nSelected\nActor\nFigure 1: Overview of SuperRL . To adaptively optimize under varying reward conditions, SuperRL\nintroduces an Adaptive Switch that conducts a brief rollout probe at the start of training to determine\nwhether the environment provides dense or sparse rewards. In dense-reward settings, the model\napplies standard reinforcement learning (RL) updates to take full advantage of abundant feedback.\nWhen rewards are sparse, control is handed to a Hybrid Actor that augments the policy-gradient\nobjective with a supervised fine-tuning (SFT) loss derived from high-quality offline trajectories. This\nhybrid update stabilizes learning and improves sample efficiency by compensating for insufficient\nor delayed reward signals. By dynamically switching objectives based on reward density, SuperRL\neffectively combines exploration with reliable offline supervision to support robust reasoning.\n∗Work done during internship at Microsoft.\n†Corresponding author (mezho@microsoft.com).\nPreprint. Under review.\n--- Page 2 ---\n1 Introduction\nLLM reasoning has been studied extensively using lightweight prompting techniques—few-shot\nexamples, chain-of-thought prompting, and self-consistency heuristics—but such static approaches\ncan exhibit diminishing returns in more challenging inference scenarios. [ 1,2,3,4,5,6] More recently,\nthere has been growing interest in leveraging reinforcement learning for \"test-time scaling\" [ 7],\nempowering models to bolster their reasoning capabilities as they generate outputs. For example,\nDeepseek R1 demonstrates that applying Group Relative Policy Optimization (GRPO) to fine-tune\ninference trajectories can lead to more robust and flexible reasoning behaviors, all without modifying\nthe underlying model architecture. [8, 9]\nDespite recent progress, online reinforcement learning methods such as PPO and GRPO still face\nlimitations in reasoning tasks. [ 8,10] First, these inherently on-policy methods update exclusively\nfrom trajectories generated by the current policy and therefore struggle to leverage high-quality\noffline data—such as human-annotated or distilled reasoning traces—that encode valuable prior\nknowledge but lie outside the policy distribution. [ 11,12] Second, in sparse-reward environments,\nonline rollouts seldom produce enough successful trajectories to supply reliable learning signals,\nmaking it difficult to bootstrap coherent reasoning. [ 13,14] Ultimately, because offline data provide\nnumerous successful trajectories with clear guidance while online rollouts frequently fail and offer\nlittle feedback, reinforcement learning struggles to acquire robust reasoning capabilities in sparse-\nreward environments.\nSupervised fine-tuning (SFT) directly leverages offline data with “correct thinking” to teach reliable\nreasoning paths but lacks any mechanism to learn from mistakes or negative cases, leading it to\nsimply memorize positives [ 15,16,17]. In contrast, reinforcement learning (RL) can generalize\nby exploring and learning from failed trajectories [ 17]. While multi-stage SFT + RL—first offline\nfine-tuning then on-policy RL as in RLHF [ 18]—anchors models to sound reasoning and adapts\nthem under sparse rewards, it suffers catastrophic forgetting of supervised knowledge during RL\nand considerable sample and compute inefficiency. [ 19,20,21] These shortcomings motivate tighter\nintegration—e.g., interleaving or unifying SFT and RL objectives within each update—to preserve\nalignment, improve stability, and boost efficiency. However, multi-stage SFT+RL pipelines can\nimpair generalization by overfitting to offline traces and narrow RL objectives. [19, 20, 21]\nTo enhance language model reasoning across both dense and sparse reward regimes, we propose\nSuperRL , a unified training framework built around two key components: the Adaptive Switch\nand the Hybrid Actor . The Adaptive Switch begins training with a brief rollout probe to determine\nwhether the environment provides dense or sparse rewards. In dense-reward environment, SuperRL\nperforms standard reinforcement learning updates to take advantage of abundant feedback. When\nrewards are sparse, control is handed over to the Hybrid Actor, which integrates reinforcement\nlearning with supervised fine-tuning on high-quality offline reasoning traces—thus enabling the\nmodel to benefit from reliable “correct thinking” examples that standard on-policy methods are\nunable to effectively utilize. SuperRL is able to maintain robust, generalizable reasoning performance\nunder sparse-reward conditions where traditional RL struggles to learn from valuable offline data.\nAs shown in Table 1, on sparse-reward datasets, SuperRL outperforms vanilla RL by large margins,\n+6.9% on LIMO, +15.0% on OpenR1, and +64.2% on HiTab. It also achieves notable generalization\nimprovements on out-of-distribution datasets, demonstrating strong robustness under sparse-reward\nconditions. 4.3\nOur main contributions are:\n•We propose SuperRL , a unified training framework that adaptively switches between Vanilla\nand Hybrid RL actors based on the density of reward signals, enabling language models to\nreason effectively under both dense and sparse reward settings.\n•We design a Hybrid Actor that interleaves policy-gradient updates with supervised fine-\ntuning on high-quality offline reasoning traces, striking a balance between exploration and\nreliable learning signals when rewards are sparse.\n•We conduct comprehensive experiments across diverse datasets, model scales, and training\nregimes, demonstrating the robustness, effectiveness, and generalizability of the SuperRL\nframework in enhancing reasoning performance.\n2\n--- Page 3 ---\n2 Related Work\n2.1 Reasoning with LLMs\nLLMs exhibit impressive knowledge but often struggle with complex reasoning tasks (e.g. multi-step\nmath or logical problems) without specialized training. Recent research has shown that incorporating\nexplicit reasoning steps can substantially improve performance. For example, training models to\nproduce chain-of-thought solutions (step-by-step reasoning) enables better arithmetic and logic\nproblem solving. [ 22] In fact, fine-tuning a pretrained LM on detailed quantitative reasoning\ndemonstrations (as done in Minerva) led to state-of-the-art results on challenging math benchmarks.\nThese findings underscore that vanilla next-token prediction alone is insufficient for high-level\nreasoning; additional fine-tuning or feedback signals are needed to guide LLMs in reasoning processes.\nThis has driven interest in post-training LLMs specifically for reasoning capabilities, as seen with\nrecent models like DeepSeek-R1, which explicitly target mathematical reasoning skills [ 22,9] .\nSuch models leverage verifiable rewards (e.g. checking a final answer’s correctness) to refine the\nreasoning ability of LLMs, pointing to the need for training paradigms beyond standard supervised\nlearning. [9, 23]\n2.2 Reinforcement Learning for LLMs\nReinforcement learning from human feedback [ 24]—as popularized by InstructGPT—uses on-policy\nmethods like PPO to fine-tune language models against a learned reward signal, but doing so requires\ncostly fresh sampling, a separate value network and careful KL-penalty tuning, and remains sample-\ninefficient when rewards are sparse. [ 10,25,22] GRPO simplifies PPO by ditching the critic and\ncomputing advantages via within-prompt reward normalization across multiple candidates, yet it\nstill depends on large volumes of model-generated outputs and can wander from the pre-trained\nbehavior in low-reward regimes. These limitations—computational overhead, sample inefficiency and\ninstability under sparse feedback—have spurred interest in leaner or hybrid approaches that retain the\nsimplicity of policy gradients while injecting external guidance to stabilize and steer training.[ 8,22]\nHowever, even with these leaner on-policy variants, sparse-reward reasoning still yields too few\npositive signals for stable gradient estimates, leading to stalled or collapsed training [13, 14].\n2.3 Unifying SFT and RL: Towards Hybrid Optimization\nMany methods leverage demonstrations to jump-start reasoning. SFT trains a pre-trained LLM on\nprompt–solution pairs (human-written or model-verified), teaching explicit reasoning patterns and\nyielding strong baseline performance.[ 26,27] However, pure SFT merely imitates its training data:\nit cannot exceed the quality or coverage of provided solutions, and gaps in demonstration types or\noptimal answers limit its generalization.[17]\nRecent research has demonstrated that combining supervised fine-tuning (SFT) with reinforcement\nlearning (RL) effectively enhances the capabilities of large language models (LLMs). This approach\nleverages offline high-quality data to guide initial training and employs reinforcement learning\nto further align model outputs with desired behaviors. Notably, several recent works, including\nInstructGPT[ 27], DeepSeek-R1[ 9] and Qwen3[ 28], have adopted similar strategies to bolster the\nreasoning abilities of LLMs. [9, 28]\nUnlike prior methods that apply supervised fine-tuning (SFT) and reinforcement learning (RL) in\nseparate stages, SuperRL unifies them by injecting SFT signals directly into the RL loss function.\nThis dynamic integration enables the model to simultaneously leverage the strengths of high-quality\noffline demonstrations and the exploratory benefits of online rollouts. By guiding policy updates with\nboth supervised and reinforcement signals, SuperRL significantly improves reasoning capabilities\nand generalization performance. This unified optimization paradigm offers a principled framework\nfor incorporating offline knowledge into reinforcement learning.\n3 Methodology\nWe present SuperRL, a unified training framework comprising two key components—Adaptive\nSwitch and Hybrid Actor—designed to adaptively train language models across both dense and sparse\n3\n--- Page 4 ---\nreward environments. Unlike conventional SFT-RL pipelines, SuperRL dynamically monitors reward\ndensity and selects the appropriate training strategy: standard on-policy updates for dense rewards, or\nan integrated RL+SFT approach for sparse rewards. This preserves the sample efficiency of offline\ndata while retaining the exploration benefits of reinforcement learning.\nThe Adaptive Switch conducts an initial rollout probe to assess reward density. If the average reward\nexceeds a predefined threshold, the model proceeds with vanilla actor–critic training. Otherwise,\ncontrol is passed to the Hybrid Actor. This dynamic mechanism eliminates the need for manual\nscheduling and ensures automatic adaptation to varying reward structures, promoting stability and\nefficiency.\nWhen activated, the Hybrid Actor blends policy-gradient updates with supervised fine-tuning on\nhigh-quality offline reasoning traces. By combining RL and SFT losses, the Hybrid Actor enhances\nlearning in sparse-reward settings, improving convergence and robustness.\nTogether, these components form a single end-to-end training loop, unifying pre-training and fine-\ntuning. Under dense rewards, SuperRL operates as standard RL; under sparse rewards, it augments\ntraining with offline demonstrations. Full implementation and hyperparameter details are provided in\nthis section.\n3.1 Discussion of Adaptive Fusion of SFT and RL\nWe propose a adaptive fusion approach that integrates SFT and RL signals within each update step,\ndynamically weighted by uncertainty. Unlike traditional sequential paradigms where models undergo\nSFT followed by RL, our method continuously blends both objectives throughout training. This\ndesign captures the best of both worlds: the strong behavioral alignment of SFT and the exploratory\ncapacity of RL. SFT provides rapid convergence toward expert-like behavior but often overfits and\nlacks robustness in unseen scenarios, especially under sparse rewards. [ 15,16,17] RL, on the other\nhand, promotes exploration and generalization but suffers from high variance and instability in\nlow-signal environments. [ 17] Sequential SFT-RL pipelines often experience severe misalignment\ndue to abrupt objective shifts, sometimes leading to catastrophic forgetting that RL alone cannot\nrectify. By contrast, our soft fusion strategy avoids such abrupt transitions. [ 19,20,21] Through\nuncertainty-based adaptive weighting, each update jointly incorporates both demonstration-driven\nand reward-driven signals, enabling more stable and efficient learning, particularly in sparse-reward\nsettings.\nThe design yields two key benefits:\n•Stable early-stage learning: In the presence of high uncertainty, the model leans more\nheavily on SFT signals, which encapsulate rich prior knowledge. This mitigates premature\nexploration and stabilizes initial training dynamics.\n•Resilient objective adaptation: The framework continuously balances between SFT and\nreinforcement learning, dynamically prioritizing the more informative signal. This soft\nobjective selection enables smooth adaptation to evolving task requirements, avoiding brittle\ntransitions or model collapse.\nWe posit that this dynamic and continuous balance—rather than discrete handovers—forms the\nfoundation of the model’s robustness and strong empirical performance in sparse-reward, reasoning-\nintensive tasks (see Appendix E for a comparative analysis of soft versus hard switching strategies).\n3.2 Uncertainty-Weighted Hybrid Actor Design\nTo unify supervised and reinforcement learning objectives within a single optimization loop, we in-\ntroduce an uncertainty-weighted hybrid actor. Concretely, we introduce two log-variance parameters,\nσpgandσsft, which respectively modulate the contributions of the policy-gradient loss Lactor and the\nSFT loss\nLsft(θ) =E(x,y)∼Dsft\u0002\nCrossEntropy\u0000\nπθ(y|x), y\u0001\u0003\n.\nOur hybrid objective is then written as\nLHybrid = exp( −2σpg)Lactor + exp( −2σsft)Lsft+σpg+σsft,\n4\n--- Page 5 ---\nwhere Lactor can be instantiated by any policy-gradient method—e.g. PPO with its clipped surrogate\nLppo=Et\u0002\nmin(rtAt,clip(rt,1−ϵ,1 +ϵ)At)\u0003\n−βentEt[H(πθ)]\nor GRPO, which further normalizes advantages within reasoning-template groups before clipping.\nDuring training, we mix supervised fine-tuning (SFT) and reinforcement learning (RL) signals within\neach backward pass by interleaving high-quality demonstration data with on-policy rollouts. This\ndesign allows the model to learn from both expert behavior and exploration at the same time, ensuring\nthat the two learning signals are tightly coupled throughout optimization.\nTo balance these signals, we use two learnable uncertainty parameters, σpgandσsft, which control\nhow much each loss contributes. Noisy or unstable signals are automatically down-weighted through\nthe exponential terms exp(−2σ), while the additive σterms prevent the model from becoming over-\nconfident. Because these weights are learned dynamically, the model adapts naturally to differences\nin signal quality without needing explicit switching rules.\nThis uncertainty-based fusion leads to more stable training, faster convergence, and stronger gen-\neralization—especially in sparse-reward, reasoning-heavy environments. By grounding updates in\nhigh-quality offline data while still allowing for on-policy exploration, our hybrid actor achieves\nbetter performance with a simpler and more unified training process.\n3.3 Adaptive Switch Design\nTo dynamically select the most appropriate training strategy during the early stages of learning, we\ndesign a modular decision mechanism called the Adaptive Switch. This component allows SuperRL to\nautomatically tailor its optimization behavior based on the reward characteristics of the environment,\nthereby avoiding rigid, one-size-fits-all solutions and improving generalization across diverse tasks.\nReward Density Assessment. Before formal training begins, SuperRL performs a lightweight rollout\nprobe to assess the reward structure of the task. During this initial phase, the system executes a small\nnumber of pure RL training steps and monitors two key indicators: the average reward observed at\neach step and the number of times this average reward increases over the rollout window. Additionally,\nit calculates the average of the most recent few reward values to evaluate short-term learning progress.\nIf both the increase count and the recent average reward fall below predefined thresholds, the task is\nclassified as having sparse rewards; otherwise, it is treated as dense reward.\nBased on this early analysis, the Adaptive Switch chooses the most suitable training actor. For dense-\nreward tasks, it selects a standard reinforcement learning actor, which benefits from the efficiency\nand alignment of pure policy-gradient updates. For sparse-reward tasks, it activates the proposed\nuncertainty-weighted hybrid actor, which incorporates supervised fine-tuning signals into every\nupdate. 4.2 4.3\nWe empirically determine effective threshold values for different batch size settings. When the batch\nsize is larger than 32, we use 10 rollout steps, track the last 10 steps for recent average reward, and\nset the thresholds to 3 reward increases and a recent average reward of 0.1. For smaller batch sizes,\nwe increase the number of rollout steps to 50, keep the recent average over 10 steps, and raise the\nthresholds to 20 increases and a recent average reward of 0.2. These values were selected based on\nextensive experiments across benchmarks such as GSM8K, MetaMath, LIMO, and so on. 4.5\nThe modular nature of the Adaptive Switch makes it easy to interpret and customize. Practitioners\ncan examine reward signals and actor decisions directly or override them using domain knowledge.\nThis design not only streamlines system tuning but also enhances transparency and control.\nFurther extensions and discussions on the Adaptive Switch are provided in Appendix A .\n4 Experiments\n4.1 Experimental Setup\nWe evaluate the SuperRL framework across a diverse set of reasoning-focused benchmarks, covering\nboth in-domain and cross-domain generalization scenarios.\nFirst, we investigate the learning and generalization capabilities of each approach across different\ndatasets and evaluation sets. The results reveal that pure RL excels in dense-reward environments ,\n5\n--- Page 6 ---\nTable 1: Exact-match accuracy (%) across benchmarks using Qwen2.5-1.5B with GRPO. The RL\nbaseline (gray) is compared to other methods, with deviations highlighted in green (gains) and red\n(drops); color intensity reflects magnitude. Rows are grouped by training dataset. Hybrid refers to the\nHybrid Actor.\nTraining Method GSM8K MetaMath PRM12K LIMO OpenR1 HiTab\nGSM8KRL 77.1 83.9 46.7 6.7 10.6 1.2\nSFT 59.7 64.5 26.7 1.2 7.4 0.0\nSFT+RL 74.7 77.3 35.1 6.7 9.8 0.0\nHybrid 77.0 79.9 14.0 2.6 14.1 1.7\nMetaMathRL 74.0 81.7 50.8 5.0 12.5 6.0\nSFT 70.1 76.5 33.9 2.4 7.9 0.0\nSFT+RL 75.2 78.7 38.7 5.5 9.7 0.2\nHybrid 79.5 82.6 33.0 1.8 15.4 1.1\nPRM12KRL 74.1 78.3 50.5 9.8 14.2 2.8\nSFT 7.1 41.7 29.2 1.2 6.4 0.0\nSFT+RL 69.2 74.3 47.7 8.5 13.2 0.0\nHybrid 72.0 74.8 50.3 11.0 15.7 3.3\nOpenR1RL 3.0 3.3 10.0 9.1 0.0 0.0\nSFT 43.4 61.2 36.4 4.9 10.4 1.2\nSFT+RL 69.7 72.9 46.5 9.1 15.7 2.1\nHybrid 70.0 72.7 46.0 9.8 15.0 10.4\nLIMORL 0.0 0.0 0.0 0.0 0.4 0.0\nSFT 16.3 17.3 10.8 1.2 1.8 0.7\nSFT+RL 28.7 48.7 19.3 3.0 3.2 0.0\nHybrid 70.0 72.7 47.2 6.9 7.6 0.1\nHiTabRL 16.9 37.2 27.7 0.1 6.9 0.0\nSFT 33.2 54.4 46.5 4.9 12.7 5.5\nSFT+RL 35.7 36.6 30.1 9.1 14.0 60.7\nHybrid 44.1 41.2 34.7 5.5 8.8 64.2\nwhile Hybrid Actor achieves the best performance under sparse-reward environments , where\nboth RL and sequential SFT+RL struggle. These findings delineate the effective operating regimes of\neach method and highlight the necessity of the Adaptive Switch.\nSecond, we demonstrate the generalizability of the Hybrid Actor design across multiple reinforcement\nlearning algorithms (e.g., PPO and GRPO) and model scales (ranging from 1B to 8B). In most settings\nunder sparse-reward conditions, hybrid yields better convergence, improved generalization, and more\nstable updates.\nThird, we analyze the the Adaptive Switch module, showing how it accurately identifies reward\ndensity through early training probes and dynamically selects the appropriate training actor. Our\nresults provide clear empirical support for its decision logic, ensuring optimal strategy selection\nwithout manual intervention.\nComplete experimental settings, switch thresholds, and implementation details are provided in\nAppendix C.7 and Appendix C.8 .\n4.2 RL Dominates in Dense-Reward Reasoning Tasks\nDense reward environments are characterized by frequent and informative feedback at each step of\nthe generation process. This setting provides clear optimization signals, allowing models to efficiently\nexplore the solution space without relying heavily on supervised demonstrations. Arithmetic and sym-\nbolic reasoning tasks with low output variability often fall into this category, making them well-suited\nfor reinforcement learning. In Table 1, datasets such as GSM8K ,METAMATHQA, and PRM12K\nexemplify dense reward settings. Across these benchmarks, RL demonstrates strong performance,\nhighlighting its ability to exploit dense supervision for effective learning and generalization.\n6\n--- Page 7 ---\nTable 2: KL-loss range and variance comparison: RL vs Hybrid Actor\nDataset KL Range (RL) KL Range (Hybrid) Max ∆ KL Var (RL) KL Var (Hybrid) Var ∆\nMetaMath [2.63e−4, 0.6055] [8.59e−4, 0.2732] -54.9% 0.009388 0.002664 -71.6%\nGSM8K [7.93e−4, 0.5306] [7.17e−4, 0.2770] -47.8% 0.007579 0.002475 -67.3%\nPRM12K [5.68e−4, 0.2616] [7.96e−4, 0.0900] -66.0% 0.002159 0.000379 -82.4%\nLIMO [3.60e−4, 0.7732] [3.59e−4, 0.2391] -69.1% 0.02489 0.001494 -94.0%\nOpenR1 [3.01e−4, 1.9110] [6.78e−4, 0.0952] -95.0% 0.01637 0.000239 -98.5%\nHiTab [7.58e−4, 1.9600] [6.59e−4, 1.0387] -47.0% 0.006257 0.01079 +72.4%\nRL demonstrates strong performance under dense reward settings, achieving the highest accuracy on\nin-domain tasks and exhibiting superior generalization compared to all other methods. Concretely,\nwhen trained on GSM8K , RL achieves an exact-match accuracy of 77.1% , outperforming other\nmethods. A similar pattern holds on METAMATHQA, where RL reaches 81.7% , surpassing SFT\n(76.5% ) and SFT+RL ( 78.7% ). These results highlight RL’s ability to effectively leverage dense and\nwell-aligned reward signals. Moreover, RL exhibits strong cross-task generalization. When trained on\nGSM8K and evaluated on PRM12K , RL attains an accuracy of 46.7% , significantly outperforming\nSFT ( 26.7% ), SFT+RL ( 35.1% ), and the Hybrid method ( 14.0% ). The pattern can be extended to\nall the three datasets metioned above. This suggests that RL-learned policies capture transferable\nreasoning patterns that extend beyond the source task.\n4.3 Hybrid Actor Thrives in Sparse-Reward Reasoning Tasks\nSparse reward settings are characterized by infrequent or weak reward signals, often posing significant\nchallenges to exploration and optimization. Datasets such as LIMO ,OPENR1, and HITABexemplify\nthis regime.\nIn these environments, hybrid consistently outperforms other methods. For instance, on HITAB, where\nRL fails to learn (0.0%), the hybrid model achieves 64.2% accuracy—exceeding SFT+RL ( 60.7% )\nand SFT ( 5.5% ). Similar gains are observed on OPENR1andLIMO , where the hybrid method\nretains strong performance despite limited supervision. Notably, this robustness extends beyond\nsymbolic reasoning tasks to semi-structured domains like TableQA dataset HITAB, highlighting the\nHybrid Actor’s ability to generalize across diverse reasoning tasks.\nBeyond accuracy, Hybrid Actor enhances both learning stability and generalization, especially when\ncompared to the SFT+RL method. Unlike SFT+RL, which suffers from abrupt shifts between\nsupervised and reinforcement phases, Hybrid Actor maintains a continuous inductive prior, lead-\ning to smoother updates and more consistent optimization. As shown in Table 2, Hybrid Actor\nsignificantly reduces KL-loss variance across challenging tasks, achieving reductions of 71.6% on\nMETAMATH,67.3% onGSM8K , and up to 98.5% onOPENR1. These reductions indicate fewer\nerratic updates and greater training stability. On HITAB, the KL variance under Hybrid Actor is\nhigher (+ 72.4% )—not due to instability, but because reward signals in this dataset remain consistently\nzero,\nThese stability benefits also lead to stronger generalization across diverse reasoning tasks. For\nexample, when trained on LIMO —a sparse-reward dataset where vanilla RL completely fails ( 0.0%\nacross all benchmarks)—Hybrid Actor achieves remarkable accuracy across the board: 70.0% on\nGSM8K ,72.7% onMETAMATH,47.2% onPRM12K , and 6.9% onLIMO itself. These results\nsubstantially surpass both SFT+RL and SFT, which often collapse under sparse reward: e.g., SFT+RL\nyields only 28.7% onGSM8K and48.7% onMETAMATH, while SFT achieves 16.3% and17.3%\nrespectively. Similarly, when trained on HITAB, Hybrid attains the highest in-domain accuracy\nof64.2% , outperforming both SFT+RL ( 60.7% ) and SFT ( 5.5% ) by large margins. Even on\nout-of-domain tasks like METAMATH andPRM12K , Hybrid maintains competitive performance,\ndemonstrating robust transferability. These results underscore the advantages of Hybrid Actor in\nleveraging supervised demonstrations while maintaining the exploratory flexibility of reinforcement\nlearning. By integrating both signals throughout training, it avoids brittle mode transitions and\nachieves consistently strong performance across domains—particularly in sparse-reward environments\nwhere traditional pipelines often fail to generalize.\n7\n--- Page 8 ---\nTable 3: Exact-match accuracy (%) on GSM8K, PRM12K, and LIMO when trained on LIMO\nusing Qwen2.5-1.5B under PPO and GRPO. Vanilla baselines are shown in gray; gains/losses are\nhighlighted in green/red. Hybrid refers to the Hybrid Actor.\nOptimizer Method GSM8K PRM12K LIMO\nPPOPPO 52.5 29.9 4.9\nSFT 59.7 26.7 1.2\nSFT+PPO 70.3 44.4 8.5\nPPO Hybrid 67.7 48.6 11.6\nGRPOGRPO 0.0 0.0 0.0\nSFT 59.7 26.7 1.2\nSFT+GRPO 28.7 19.3 3.0\nGRPO Hybrid 70.0 47.2 6.9\n4.4 Generalization Study\nThe Hybrid Actor enables stable, transferable learning across different RL algorithms To assess\ngeneralization across RL algorithms in sparse-reward settings, we evaluate our Hybrid Actor under\nboth PPO and GRPO using the same model architecture (Qwen2.5-1.5B) and training data ( LIMO ).\nAs shown in Table 3, Hybrid consistently outperforms both vanilla RL and SFT+RL baselines. Under\nPPO, Hybrid Actor achieves +18.7% and +6.7% improvements over vanilla PPO on PRM12K and\nLIMO, respectively. Compared to the stronger SFT+PPO pipeline, Hybrid still yields notable gains:\n+4.2% on PRM12K and +3.1% on LIMO. Under GRPO, Hybrid delivers even larger margins: +70.0%\non GSM8K, +47.2% on PRM12K, and +6.9% on LIMO compared to the vanilla GRPO baseline,\nwhich fails to learn effectively. Even when compared to SFT+GRPO, Hybrid achieves consistent\ngains of +41.3%, +27.9%, and +3.9% on GSM8K, PRM12K, and LIMO, respectively.\nTable 4: Exact-match accuracy (%) on GSM8K, PRM12K, HiTab, and LIMO when trained on\nHITABusing PPO and Hybrid across model scales. PPO serves as baseline (gray); improve-\nments/degradations from Hybrid are shown in green/red. Hybrid refers to the Hybrid Actor.\nModel Method GSM8K PRM12K HiTab LIMO\nQwen2.5 1.5BPPO 46.9 30.4 0.1 5.5\nHybrid 43.1 30.6 0.5 5.5\nQwen2.5 3BPPO 68.5 49.1 16.3 12.8\nHybrid 72.1 55.3 53.7 15.2\nQwen2.5 7BPPO 86.6 59.6 65.5 21.3\nHybrid 83.8 63.7 76.4 28.1\nLLaMA3.2 1BPPO 41.6 31.9 0.8 6.7\nHybrid 40.5 25.1 0.1 7.3\nLLaMA3.2 3BPPO 74.2 53.2 48.4 20.1\nHybrid 74.2 53.8 61.9 23.2\nLLaMA3.1 8BPPO 82.3 55.3 68.8 20.1\nHybrid 82.0 54.6 77.7 20.1\nThe Hybrid Actor generalizes across model scales and architectures. Table 4presents exact-\nmatch accuracy across four datasets when models are trained on HITAB. For smaller models like\nQwen2.5-1.5B and LLaMA3.2-1B, Hybrid yields mixed results, with minor regressions on GSM8K\nand PRM12K. However, starting from the 3B scale, Hybrid begins to outperform PPO. For example,\non Qwen2.5-3B, Hybrid achieves +6.2% on PRM12K and a substantial +37.4% on HiTab. The gains\npersist on Qwen2.5-7B, with Hybrid reaching +10.9% improvement on HiTab and +6.8% on LIMO.\nSimilarly, for LLaMA3.2-3B, Hybrid improves HiTab accuracy from 48.4% to 61.9%. These results\nindicate that the advantages of Hybrid training persist across both model scales and architectures (e.g.,\nQwen and LLaMA), demonstrating its robustness and effectiveness in sparse-reward environments.\n8\n--- Page 9 ---\n4.5 Empirical Study on K-step Training Parameters for Adaptive Switch\nIn this section, we discuss the parameter settings of Adaptive Switch in SuperRL. In the Appendix C.8,\nwe study the impact of different initialization values of σpgandσsftfor Hybrid Actor. Here, we\nfocus on the appropriate values for the parameters related to the K-step training.\nFigure 2: Average Reward Values on Differ-\nent Benchmarks (0-10, 20-30, 40-50 Steps)\n Figure 3: Count of Reward Increases on Dif-\nferent Benchmarks (0-10, 0-30, 0-50 Steps)\nWe propose a reward-based switching strategy to assess the effectiveness of RL during early training\nfor Adaptive Switch. If early rewards are sparse—characterized by stagnant or low rewards—the\nsystem transitions to a hybrid training mode, improving learning stability in sparse-reward settings.\nAfter ksteps of RL training, we compute the average reward ( avg_reward ) at each step and derive\ntwo metrics: the number of reward increases across steps ( increase_num ) and the average reward\nover the last msteps ( recent_avg_reward ). A mode switch is triggered if both increase_num <\nincrease_threshold and recent_avg_reward <avg_threshold.\nTable 7in the Appendix illustrates early reward dynamics during the first 10 steps of GRPO train-\ning with Qwen2.5-1.5B on GSM8K andHITAB. On GSM8K , the initially low average reward\n(avg_reward ) increases rapidly, reaching 0.4894 by step 10. This indicates that standard RL training\nprogresses effectively, and no switching is required. In contrast, on HITAB, both the number of\nreward increases ( increase _num ) and the recent average reward ( recent _avg_reward ) remain\nnear zero, signaling training stagnation and prompting a mode switch.\nFigures 2and3provide further insight by plotting the average reward and cumulative count of reward\nincreases across early training steps for Qwen2.5-1.5B on multiple benchmarks. As shown, easier\ndatasets such as GSM8K andMETAMATH exhibit steady increases in both metrics, while harder\ndatasets like LIMO andHITABshow little to no improvement. Based on these observations, we\nempirically set the switch parameters as follows: for batch sizes larger than 32, we use k= 10 ,m=\n10,increase _threshold = 3, and recent _avg_reward = 0.1; otherwise, we set k= 50 ,m= 10 ,\nincrease _threshold = 20 , and recent _avg_reward = 0.2. Full details of the experimental setup\nare provided in Appendix C.8 .\n5 Conclusion\nWe present SuperRL, a unified training framework that adaptively combines supervised and rein-\nforcement signals to improve reasoning under both dense and sparse rewards. Experiments show\nthat SuperRL achieves superior performance, stability, and generalization across diverse reasoning\nbenchmarks. Limitations and future work are discussed in Appendix A .\n9\n--- Page 10 ---\nReferences\n[1]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems , 35:24824–24837, 2022.\n[2]Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171 , 2022.\n[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\n[4]Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan\nSun. Towards understanding chain-of-thought prompting: An empirical study of what matters.\narXiv preprint arXiv:2212.10001 , 2022.\n[5]Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting\nin large language models. arXiv preprint arXiv:2210.03493 , 2022.\n[6]Lang Cao. Graphreason: Enhancing reasoning capabilities of large language models through a\ngraph-based verification approach. arXiv preprint arXiv:2308.09267 , 2023.\n[7]Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu,\nZhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma. A survey\non test-time scaling in large language models: What, how, where, and how well?, 2025.\n[8]Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\n[9]DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang,\nChong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,\nJiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian\nLiang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean\nWang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian,\nPanpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\nZhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting\nPan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,\nT. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,\nWentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao\nNie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su,\nXuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang\nWang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y . K. Li, Y . Q. Wang, Y . X.\nWei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao\nZhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang\nMa, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He,\nYunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu,\nZilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang,\nand Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025.\n[10] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms, 2017.\n10\n--- Page 11 ---\n[11] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:\nTutorial, review, and perspectives on open problems, 2020.\n[12] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration, 2019.\n[13] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay,\n2018.\n[14] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems, 2021.\n[15] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners, 2022.\n[16] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari,\nCanwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan\nChhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang,\nMatteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang,\nTrishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush.\nMultitask prompted training enables zero-shot task generalization, 2022.\n[17] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans,\nQuoc V . Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of\nfoundation model post-training, 2025.\n[18] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences,\n2020.\n[19] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of\ncatastrophic forgetting in large language models during continual fine-tuning, 2025.\n[20] Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic\nforgetting in language models via implicit inference, 2024.\n[21] Heshan Fernando, Han Shen, Parikshit Ram, Yi Zhou, Horst Samulowitz, Nathalie Baracaldo,\nand Tianyi Chen. Mitigating forgetting in llm supervised fine-tuning and preference learning,\n2025.\n[22] Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang,\nTong Zhang, Caiming Xiong, and Hanze Dong. A minimalist approach to llm reasoning: from\nrejection sampling to reinforce, 2025.\n[23] Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpo’s effective loss, dynam-\nics, and success amplification, 2025.\n[24] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen\nSahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf,\n2024.\n[25] Douglas C. Crowder, Darrien M. McKenzie, Matthew L. Trappett, and Frances S. Chance.\nHindsight experience replay accelerates proximal policy optimization, 2024.\n[26] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,\nLifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang.\nLight-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n[27] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback, 2022.\n[28] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang,\nFeng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang,\n11\n--- Page 12 ---\nJianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin\nYang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin\nZhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,\nXingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng\nZhou, and Zihan Qiu. Qwen3 technical report, 2025.\n[29] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\n[30] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models, 2024.\n[31] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng\nLim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large\nlanguage models, 2023.\n[32] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is\nmore for reasoning, 2025.\n[33] Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang\nLou, and Dongmei Zhang. Hitab: A hierarchical table dataset for question answering and\nnatural language generation, 2022.\n12\n--- Page 13 ---\nContents of Appendix\nA Limitations and Future Work 14\nB Broader Impacts and Safeguards 14\nC Detailed Settings of Experiments 15\nC.1 Dataset Configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nC.2 Model Configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nC.3 Construction and Utilization of Offline Reasoning Data . . . . . . . . . . . . . . . 16\nC.4 Prompt and Output Format Design . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.5 Reward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.6 Metric Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.7 Environment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.8 Details for K-step Parameter Settings Exploration . . . . . . . . . . . . . . . . . . 20\nD Lessons Learned from Failed Attempts and Design Iterations 21\nD.1 The Pitfalls of Integrating Online Rollouts into Supervised Fine-Tuning in Hybrid Actor 22\nD.2 Offline Data as GRPO-Compatible Rollouts: A Negative Result . . . . . . . . . . 23\nD.2.1 Variant I: Direct Injection of Expert Rollouts . . . . . . . . . . . . . . . . 23\nD.2.2 Variant II: Self-Rewritten Expert Rollouts . . . . . . . . . . . . . . . . . . 23\nD.3 Conclusion: Offline Rollouts Undermine GRPO Stability . . . . . . . . . . . . . . 24\nD.4 Offline Data as Few-shots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nD.5 Human Written or Synthetic Offline Data . . . . . . . . . . . . . . . . . . . . . . 25\nD.6 Comparative Study of Hybrid Loss Integration Strategies . . . . . . . . . . . . . . 25\nD.6.1 Design and Evaluation of Hybrid Learning Strategies . . . . . . . . . . . . 26\nD.6.2 Analysis of Hybrid Actor Variants . . . . . . . . . . . . . . . . . . . . . . 27\nD.7 Distillation or Hybrid? Choosing by Data Regime . . . . . . . . . . . . . . . . . . 27\nE Soft Fusion over Hard Switching: A Unified Perspective on SuperRL versus SFT+RL 29\n13\n--- Page 14 ---\nA Limitations and Future Work\nDespite the strong empirical results of SuperRL, several limitations remain that point toward fruitful\ndirections for future research.\nActor–Critic Integration Scope. Our current SuperRL framework fuses supervised and reinforce-\nment signals solely at the actor level, leaving the critic architecture unchanged. This limits the\npotential benefits of supervision on value estimation, especially in reward-sparse settings where critic\ninstability is a known issue. While the actor benefits from adaptive loss weighting, the critic may\nstill suffer from high variance or misaligned value targets. Future work could explore critic-aware\nhybridization—such as incorporating auxiliary imitation-based value heads, distillation from teacher\ncritics trained on demonstrations, or uncertainty-guided target shaping—to jointly stabilize actor and\ncritic learning.\nStatic Fusion Strategy. The uncertainty-weighted hybrid loss in the Hybrid Actor in SuperRL\nprovides a principled and adaptive mechanism for balancing objectives, but remains fixed across\ndatasets once initialized. In practice, certain datasets (e.g., highly structured or low-noise) may\nadmit more lightweight or specialized fusion strategies that require less computational overhead\nor training complexity. Designing dataset-aware or task-specific hybridization mechanisms—such\nas confidence-triggered SFT modulation or curriculum-guided loss annealing—could yield more\nefficient and tailored learning strategies.\nRule-Based Framework Design. The SuperRL currently employs a hard-coded, rule-based mech-\nanism to estimate dataset difficulty and select training strategies in the Adaptive Switch. While\neffective in our experiments, this approach is coarse-grained and may fail to capture nuanced dis-\ntinctions in task complexity or reward structure. In future work, we plan to replace this module\nwith a learned policy selector that dynamically predicts the optimal training configuration based on\nearly-stage learning signals (e.g., reward variance, KL divergence trends, early plateau detection).\nThis direction may benefit from techniques in meta-learning, reinforcement curriculum learning, or\nBayesian optimization.\nScalability and Efficiency. Although we demonstrate the broad applicability of SuperRL across\nmultiple actor–critic algorithms and base models, its reliance on dual-loss computation and uncertainty\nlearning introduces overhead in both compute and memory. Scaling the framework to very large\nmodels or long-horizon tasks may therefore require architectural optimizations such as parameter\nsharing across objectives, loss decoupling, or batch reweighting heuristics. Additionally, efficient\nparallelization strategies for joint SFT and RL processing are important for deployment at production\nscale.\nIn summary, while SuperRL provides a robust foundation for integrating supervised priors into\nreinforcement learning for language models, we believe further refinement of actor–critic design,\nadaptivity mechanisms, and system scalability will be crucial for advancing the field toward more\ngeneralizable and efficient hybrid training strategies.\nB Broader Impacts and Safeguards\nThe proposed SuperRL framework introduces a new hybrid reinforcement learning strategy that\nintegrates SFT loss into the actor’s update loop. This approach may carry several broader impacts,\nparticularly in the context of increasingly capable and general-purpose language models. On the\none hand, the improvements in convergence stability, sample efficiency, and generalization under\nsparse-reward conditions make this method attractive for domains where reinforcement learning\nhas traditionally struggled. These include education, programming assistance, automated scientific\ndiscovery, and other settings where precise reasoning and data efficiency are critical. The reduced\nreliance on large volumes of reward-labeled data may also lower the entry barrier for fine-tuning\nsmaller models in more constrained or specialized environments.\nAt the same time, as with many advances in RL and LLM training, potential concerns arise regarding\nmisuse and unintended consequences. Enhanced reasoning capabilities may inadvertently be applied\nin domains such as persuasive content generation, misinformation synthesis, or manipulation of\n14\n--- Page 15 ---\nhuman preferences—especially if downstream applications fail to incorporate adequate human\noversight. Furthermore, the incorporation of supervised signals throughout training means that any\nbiases embedded in the training corpus may persist or even be reinforced during policy updates,\nparticularly if the model overfits to surface-level heuristics present in expert demonstrations.\nIt is also worth noting that while the hybrid loss improves stability, it does not guarantee robustness\nor fairness under distributional shifts. The method assumes access to high-quality expert data and\nreasonably aligned reward signals; in settings where these assumptions do not hold, model behavior\nmay become unpredictable or brittle. In addition, the hybrid approach may make model attribution\nmore difficult, as it intertwines supervised and reinforcement-driven behaviors during learning,\ncomplicating post hoc interpretation of model decisions.\nWe do not make specific claims regarding the societal or safety alignment of models trained under this\nframework. The practical implications of SuperRL —positive or negative—will ultimately depend on\nthe nature of its application and the surrounding deployment context. Nonetheless, we believe that\ndiscussions around the responsible development and use of hybrid training frameworks are important\nand ongoing. Future research may benefit from more targeted evaluations of fairness, robustness, and\ntransparency in hybrid optimization, especially as such techniques become more widely adopted in\nreal-world systems.\nC Detailed Settings of Experiments\nC.1 Dataset Configurations\nTo rigorously evaluate SuperRL, we utilize six diverse datasets encompassing various reasoning\nchallenges, from arithmetic problem-solving to complex symbolic logic and semi-structured data\nanalysis. Below, we provide detailed descriptions of each dataset:\nGSM8K is a benchmark dataset comprising 8,500 high-quality grade school math word problems,\ncrafted by human problem writers. The dataset is divided into 7,500 training and 1,000 test examples.\nEach problem typically requires 2 to 8 steps to solve, involving basic arithmetic operations such\nas addition, subtraction, multiplication, and division. GSM8K serves as a standard for evaluating\nmulti-step mathematical reasoning in language models.\nMetaMathQA is a large-scale dataset containing 395,000 mathematical question-answer pairs. The\ndataset is generated by augmenting existing problems from GSM8K and MATH datasets, ensuring\ndiversity and complexity in mathematical reasoning tasks. MetaMathQA aims to enhance the forward\nand backward reasoning capabilities of models across various mathematical domains, including\nalgebra, geometry, and calculus.\nPRM12K is a subset of the PRM800K dataset, focusing on mathematical problems. It consists of\n12,000 samples, each accompanied by five different solution paths, encompassing both correct and\nincorrect answers. Unlike some filtered datasets, PRM12K retains all samples, providing additional\ncolumns to indicate the correctness of each solution. This structure makes it particularly suitable for\npreference learning and reward modeling tasks.\nOpenR1-Math-220k is a comprehensive dataset designed for mathematical reasoning, comprising\n220,000 math problems. Each problem is associated with two to four reasoning traces generated\nby the DeepSeek R1 model. The traces have been verified using tools like Math Verify and Llama-\n3.3-70B-Instruct, ensuring at least one correct reasoning path per problem. This dataset challenges\nmodels to understand and replicate complex reasoning processes.\nLIMO (Less is More for Reasoning) is a benchmark that challenges the conventional belief that\nlarge datasets are necessary for effective reasoning. It contains only 817 meticulously curated training\nsamples, yet models trained on LIMO demonstrate superior performance across multiple benchmarks.\nLIMO emphasizes the quality of training data over quantity, showcasing that complex reasoning\nabilities can be elicited with limited but well-structured examples.\n15\n--- Page 16 ---\nHiTab is a dataset developed for question answering and natural language generation over hi-\nerarchical tables. It includes 3,597 tables and 10,686 QA pairs, sourced from statistical reports\nand Wikipedia pages. The tables exhibit complex hierarchical structures, and the dataset provides\nfine-grained annotations for entity and quantity alignment. HiTab poses significant challenges in\nnumerical reasoning due to its hierarchical indexing and implicit semantic relationships.\nTable 5: Benchmarks used in this study. “–” indicates the split is not officially provided.\nDataset # Train # Test Task Type Domain License Source\nGSM8K [29] 7,473 1,319 Math word problems Elementary math MIT Link\nMETAMATHQA [30] 395,000 – Math QA Mathematics MIT Link\nPRM12K [31] 12,000 – Programmatic reasoning Mathematics Apache 2.0 Link\nLIMO [32] 817 – Few-shot reasoning Mathematics MIT Link\nOPENR1-M ATH-220 K220,000 – Math reasoning Mathematics Apache 2.0 Link\nHITAB[33] 7,399 1,583 Hierarchical table QA Statistics C-UDA 1.0 Link\nC.2 Model Configurations\nTo comprehensively evaluate SuperRL, we select a diverse set of open-source language models\nvarying in size, architecture family, and training objectives. These models span three primary families:\nQwen2.5 ,LLaMA 3.x , and DeepSeek-R1 Distilled . Below, we provide detailed descriptions of\neach:\nQwen2.5 Series is a family of autoregressive language models developed by Alibaba, instruction-\ntuned for a wide range of reasoning tasks. It offers multiple model sizes—0.5B, 1.5B, 3B, and\n7B—allowing systematic exploration of scale effects. All models are licensed under Apache 2.0 and\ntrained with a consistent architecture design and tokenizer, ensuring comparability across sizes.\nDeepSeek-R1-Distill-1.5B is a distilled version of the original DeepSeek-R1 model, designed to\npreserve the reasoning capabilities of larger models while reducing computational overhead. It is\ninstruction-tuned on diverse reasoning traces and optimized for both efficiency and generalization.\nThis 1.5B model plays a key role in evaluating compact reasoning models.\nLLaMA 3.x Series includes models from the LLaMA 3.1 and 3.2 releases by Meta, with\ncommunity-instruct fine-tuning. We use the 1B and 3B models from LLaMA 3.2, and the 8B\nmodel from LLaMA 3.1. These models are known for their competitive instruction-following ability\nand are widely adopted in the community for downstream alignment tasks.\nAll model weights are publicly available under permissive licenses, enabling reproducible bench-\nmarking. Table 6 summarizes the key properties of each model used in our experiments.\nTable 6: Model configurations evaluated in our experiments.\nModel Parameters (B) Family License Source\nQwen2.5-0.5B 0.5 Qwen2.5 Apache 2.0 Link\nQwen2.5-1.5B 1.5 Qwen2.5 Apache 2.0 Link\nQwen2.5-3B 3 Qwen2.5 Apache 2.0 Link\nQwen2.5-7B 7 Qwen2.5 Apache 2.0 Link\nR1-Distill-1.5B 1.5 DeepSeek-R1 MIT Link\nLlama3.2-1B-Instruct 1 Llama 3.2 Llama 3 Community Link\nLlama3.2-3B-Instruct 3 Llama 3.2 Llama 3 Community Link\nLlama3.1-8B-Instruct 8 Llama 3.1 Llama 3 Community Link\nC.3 Construction and Utilization of Offline Reasoning Data\nThe offline SFT data utilized in SuperRL is derived from expert-annotated or high-quality model-\ngenerated reasoning traces. Many of the datasets we employ inherently contain rich, step-by-step\nproblem-solving trajectories, serving as natural sources of offline supervision.\n16\n--- Page 17 ---\nFor instance, the GSM8K dataset provides detailed answer fields comprising multi-step reasoning\nchains and final solutions, making it directly suitable for use as SFT targets. Similarly, PRM12K\noffers both expert-annotated solutions and a substantial portion of high-quality, model-generated\nreasoning traces. We meticulously validate and include these generated traces in our SFT corpus\nwhen their final answers align with the correct labels.\nLIMO, originally designed to study long-horizon mathematical reasoning, was explicitly structured to\nbenefit from supervised learning. Its strong performance under SFT alone motivated our inclusion of\nLIMO in the hybrid training setting, leveraging its expert-annotated solutions as high-quality offline\nsupervision signals.\nIn contrast, HiTab lacks annotated intermediate reasoning steps, which are crucial for supervised\nlearning in our hybrid training framework. To address this, we generate synthetic reasoning trajecto-\nries using a large instruction-tuned model (e.g., Deepseek-R1), prompted to produce step-by-step\njustifications for each QA pair. We implement a stringent answer-filtering mechanism: only when the\ngenerated rationale’s final answer matches the ground-truth label is it accepted into the SFT corpus.\nThis ensures high precision and prevents low-quality traces from corrupting training.\nImplementation in VERL.Within the VERLframework, whose license is Apache License 2.0,\nwe adopt a structured approach to integrate offline trajectories into the training pipeline:\n•Data Annotation and Storage : Each data sample is augmented with an extra info field,\nencapsulating metadata such as the original question, the extracted or generated reasoning\ntrajectory, and the target answer. This design ensures that auxiliary information is preserved\nalongside the primary data, facilitating downstream processing.\n•Custom Dataset Class : We define a custom Dataset class that preprocesses the annotated\ndata. This class is responsible for converting each entry into a standardized DataProto\nobject. The DataProto includes tokenized inputs, loss masks for supervised targets, and\nauxiliary fields for logging and analysis. This modular design promotes flexibility and\nreusability across different training configurations.\n•Integration with Actor Module : The Actor module accesses the preprocessed DataProto\nobjects during training. By leveraging the structured information within each DataProto ,\ntheActor can efficiently retrieve high-quality supervision signals, ensuring that the model\nbenefits from the rich reasoning trajectories during optimization.\nThis integration strategy ensures that the offline trajectories are seamlessly incorporated into the\nhybrid optimization process, providing consistent behavioral priors and enhancing the model’s\nreasoning capabilities.\nC.4 Prompt and Output Format Design\nTo promote interpretable and verifiable reasoning behavior, we adopt distinct prompting and output\nformatting strategies tailored to the model type.\nPrompt Design. For both vanilla and structured-output models, we append a concise instruc-\ntion—“Let’s think step by step and output the final answer in boxed{} .”—to the original problem\ndescription to encourage step-by-step reasoning and a clearly identifiable final answer.\nFor example:\nPrompt: If you have 3 apples and you buy 2 more, how many do you have? Let’s\nthink step by step and output the final answer in boxed{}.\nForvanilla models without structured output conventions, the model is directly given the above\nprompt string. A typical output is:\nWe start with 3 apples. Buying 2 more gives us 3 + 2 = 5.\nThe final answer is boxed{5}.\nIn contrast, for structured-output models (e.g., Qwen2.5-1.5B, DeepSeek-R1-Distill-Qwen-1.5B)\nthat support chat-style prompting, we apply apply_chat_template to transform the prompt into a\nChatML-formatted conversation. For instance:\n17\n--- Page 18 ---\n<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nIf you have 3 apples and you buy 2 more, how many do you\nhave? Let’s think step by step and output the final answer\nin boxed{}.<|im_end|>\n<|im_start|>assistant\n<think>\nExpected Model Output and Postprocessing. The output format likewise depends on the model’s\ninterface and training. For structured-output models, the expected output includes special tags:\nWe start with 3 apples. Buying 2 more gives us 3 + 2 = 5.\nNow let’s output the final answer.</think>\n<answer>The answer is boxed{5}</answer>\nIn this case, we apply tag-aware parsing during postprocessing:\n• Extract the reasoning trace enclosed in <think> tags.\n•Extract the final answer from within the <answer> tag, specifically the content of boxed{} .\nFor vanilla models, which do not include tags, we rely on regex-based postprocessing:\n• Identify the reasoning portion as any content before the appearance of boxed{} .\n• Parse the numerical answer from within boxed{} .\nThese model-aware prompt formatting and output parsing strategies ensure the consistent interpreta-\ntion of model responses across evaluation and training pipelines. They also enable structured reward\ncomputation and answer matching for reinforcement learning optimization.\nC.5 Reward Design\nGiven a model response yto input x, we compute the reward as a binary signal:\nr(x, y) =\u001a1if extract (y) =ygt\n0otherwise\nwhere extract (y)denotes the parsed answer obtained via model-specific extraction logic, and ygtis\nthe ground-truth answer.\nTo accommodate surface-form variations—especially prevalent in datasets such as LIMO and\nPRM12K—we incorporate a canonicalization layer in the reward computation. This module stan-\ndardizes answer representations by:\n• Normalizing numeric formats (e.g., converting fractions to decimals);\n• Performing symbolic equivalence checks (e.g., 2x+ 4vs.4 + 2 x);\n• Unifying variable names, units, or other context-specific notations.\nIf the canonicalized prediction matches any canonicalized gold reference, we assign a reward of 1.\nAs a fallback mechanism, when no delimiters (e.g., \\boxed{} ) are detected in the model output, we\nextract the last numerical span as a proxy for the final answer. All reward functions are implemented\nas deterministic, stateless modules to ensure reproducibility and compatibility with batched rollout\nevaluations in PPO and GRPO training pipelines.\nC.6 Metric Design\nWe adopt Exact Match (EM) accuracy as the primary evaluation metric to assess model performance\non reasoning tasks. A prediction is considered correct if the extracted answer exactly matches the\nground-truth answer after normalization. This includes removing extraneous formatting, standardizing\n18\n--- Page 19 ---\nnumerical representations, and optionally applying symbolic simplification when applicable. EM\noffers a strict yet interpretable signal of end-to-end correctness, effectively capturing whether the\nmodel arrives at the correct final solution.\nCompared to token-level metrics such as BLEU or ROUGE—which quantify n-gram overlap—EM\nis more aligned with the discrete nature of most reasoning tasks. Token-based metrics often tolerate\nsuperficial similarity while overlooking semantically crucial deviations (e.g., predicting 7.0instead\nof7.5), thus failing to penalize incorrect answers that appear linguistically similar. In contrast, EM\nenforces a high bar for correctness by requiring exact alignment with the reference answer.\nTo accommodate datasets with multiple valid reasoning paths or equivalent solutions—such as\nPRM12K and OpenR1—we extend EM to a relaxed matching scheme. Specifically, a prediction is\nmarked as correct if it matches anyof the acceptable reference answers after canonicalization. This\nallows for flexibility in surface forms (e.g., equivalent algebraic expressions or unit conversions)\nwhile preserving the core requirement of semantic equivalence.\nIn all cases, the normalization and canonicalization procedures used during EM evaluation are kept\ndeterministic and model-agnostic to ensure reproducibility and fairness. This design choice ensures\nthat the metric remains robust across diverse model architectures and output formats.\nC.7 Environment Setup\nAll experiments are conducted within the verl framework, a scalable actor–critic reinforcement\nlearning platform tailored for optimizing language models. This framework serves as the foundation\nfor our experimental setup, allowing us to implement and iterate on various training strategies. Our\nprimary experiments utilize GRPO as the core reinforcement learning algorithm. However, to validate\nthe general applicability of our uncertainty-weighted hybrid training framework, we also conduct\ncomparative trials using PPO. The configurations, scripts and source code for these experiments are\nall developed and modified within the verl . This includes the implementation of the GRPO and\nPPO algorithms, as well as the integration of the uncertainty-weighted hybrid training approach. The\nflexibility of the verl enables us to seamlessly update and refine our methods, ensuring that our\nexperiments are both robust and adaptable.\nOur experimental scripts follow a unified and modular configuration framework designed to ensure\nconsistency across all datasets and model backbones. This framework supports flexible adaptation,\nwith minor adjustments made to accommodate variations in model scale (e.g., parameter count) and\navailable computational resources (e.g., GPU memory capacity). By default, both the actor and critic\nare initialized from the same pretrained checkpoint. This shared initialization helps maintain stability\nin the early stages of training and ensures that policy updates build upon a consistent starting point.\nUnless otherwise noted, we train the entire model with full-parameter updates, rather than using\ntechniques like partial tuning or adapters. To manage memory consumption during training, we enable\ngradient checkpointing, which trades off additional computation for significantly reduced memory\nusage. This is particularly important when training large models with long sequences or large batch\nsizes. To constrain policy drift and encourage stable learning, we apply KL divergence regularization\nbetween the current policy and a fixed reference policy. We use a low-variance KL formulation with\na fixed regularization coefficient of 0.001, following prior work showing its effectiveness in language\nmodel fine-tuning.\nWe adopt a fixed learning rate of 1e-6 for all experiments, regardless of dataset or model size. This\nconsistent setting simplifies hyperparameter tuning and facilitates fair comparisons across different\ntasks. The batch size is set to 32 by default, which provides a good balance between training stability\nand GPU memory efficiency. However, in data-scarce scenarios—such as the LIMO dataset, which\ncontains relatively few high-quality training samples—we reduce the batch size to 8 to improve\ngradient quality and mitigate overfitting risks associated with small datasets. Each training run\nproceeds for 500 update steps, a schedule empirically chosen to ensure sufficient optimization while\nmaintaining computational efficiency. To monitor progress and detect training instabilities early, we\nconduct model evaluation every 5 steps on a held-out validation set. At the end of training, we report\nthe test-set performance at step 500 if the learning curve shows smooth convergence. In cases where\nthe validation curve exhibits fluctuations or noise—typically due to reward sparsity or instability in\npolicy gradients—we apply exponential moving average (EMA) smoothing to the score trajectory to\n19\n--- Page 20 ---\nobtain a more reliable final evaluation metric. This ensures that our reported performance reflects the\noverall trend rather than being biased by momentary spikes or drops.\nFor rollout generation, we employ the vLLM backend, which enables efficient and scalable batched\ndecoding with optimized GPU memory usage. Each prompt is decoded to produce five candidate\nresponses, allowing for diverse sampling during training. To ensure stable runtime behavior, we cap\nGPU memory utilization at 40%. The decoding configuration adopts a stochastic sampling strategy\nwith a temperature of 1.0, top- k=−1(disabled), and top- p= 1.0, corresponding to unconstrained\nnucleus sampling. These settings encourage diverse yet coherent response generation from the model.\nWe constrain the maximum sequence length—comprising both the input prompt and the generated\noutput—to 2048 tokens. Prompts that exceed this limit are filtered out prior to generation, and any\nattempt to exceed the limit during decoding raises a truncation error. This strict enforcement helps\nmaintain consistency and prevents the introduction of ambiguous or malformed training signals.\nFor dataset partitioning, we follow the original train/test splits provided by the benchmark for\nGSM8K andHITAB, ensuring compatibility with prior work and fair comparison. For all other\ndatasets containing more than 20,000 examples, we randomly subsample a total of 20,000 instances\nto reduce computational cost while maintaining representative coverage. The selected subset is then\nsplit into training and test sets using an 80/20 ratio. This standardized partitioning protocol facilitates\nconsistent evaluation across diverse datasets with varying sizes and distributions.\nAll experiments are conducted on machines equipped with NVIDIA H100 GPUs. For most settings\ninvolving smaller models (e.g., 1-3B parameters), we utilize a 2-GPU configuration, which provides\nsufficient compute capacity for full-parameter training. In contrast, larger models (e.g., 7B and above)\nare trained in a distributed fashion using multiple nodes and GPUs. We construct multi-node clusters\nusing Ray, a flexible framework for large-scale distributed computing. Once initialized, training is\norchestrated through verl ’s distributed utilities, which support scalable actor–critic reinforcement\nlearning with efficient inter-GPU communication and synchronization. The micro-batch size is fixed\nat 2 per GPU across all experiments, regardless of the number of nodes or model size. This setting\nbalances memory usage and gradient estimation stability, especially under reinforcement learning\nwith sparse rewards. To ensure a fair comparison across different training paradigms, we adopt\na unified optimizer configuration—including learning rate, weight decay, and scheduler—for all\nmethods: supervised-only (SFT), reinforcement learning-only (RL), sequential SFT+RL, and our\nproposed hybrid RL+SFT. This design isolates the effect of training strategies from confounding\noptimization differences.\nFor the SFT andSFT+RL baselines, we begin by fine-tuning the base model using the same training\ndataset, learning rate, and context length as in the RL-based training setups. The supervised fine-\ntuning is conducted for a total of 25 epochs, a duration chosen to ensure sufficient convergence\nwithout overfitting. Throughout training, we evaluate each epoch’s checkpoint on the held-out test set\nusing greedy decoding, i.e., with temperature set to zero and no sampling. The checkpoint achieving\nthe highest test-set performance is selected as the final SFT baseline. For the SFT+RL baseline,\nreinforcement learning is initialized from this best-performing SFT checkpoint. We then continue\ntraining the model under the same RL framework used in our hybrid method. The final performance\nof the SFT+RL model is reported based on the test-set score at the point where the learning curve\nreaches stable convergence. If the learning dynamics are noisy, we apply exponential moving average\n(EMA) smoothing to determine the final score in a robust manner.\nC.8 Details for K-step Parameter Settings Exploration\nWe conduct ablation experiments using four initialization settings for the uncertainty weights: σpg=\n1, σsft= 1,σpg= 1, σsft= 0,σpg= 0, σsft= 1, and our default σpg= 0, σsft= 0, denoted\nrespectively as settings 11, 10, 01, and 00. We apply Hybrid training on GSM8k using QWen2.5-1.5B,\nand the Figure 4 5show the evolution of scores on GSM8k and PRM12k as training steps increase.\nWe observe that on GSM8k, all four settings lead to similar performance trajectories, suggesting that\nthis task is relatively insensitive to the relative weighting between policy gradient and imitation loss.\nThe reward signal alone is strong enough to guide effective learning. In contrast, on PRM12k, the\nmodel collapses under the 10 setting ( σpg= 1, σsft= 0), likely because the absence of supervision\nduring initial training harms the model’s ability to generalize to more difficult PRM12k—whose\ndistribution differs significantly and exhibits stronger structural regularities. The 00 setting ( σpg=\n20\n--- Page 21 ---\nFigure 4: Validation Score on GSM8k When Trained on GSM8k Using Different Initial Strategies\nFigure 5: Val Score on PRM12k When Trained on GSM8k Using Different Initial Strategies\n0, σsft= 0) achieves the best performance across both tasks, indicating that assigning strong\nand balanced weights to both the policy gradient and imitation losses at the beginning of training\ncontributes to effective exploration and generalization.\nTo compute the average reward within different step ranges and the number of reward increases\nacross the first ksteps, we conduct 5 independent runs on GSM8k, MetaMath, PRM12k, OpenR1,\nand HiTab datasets. The batch size is varied across {16,32,64,128,256}. For the LIMO dataset,\ndue to its smaller data volume, we perform 10 independent runs with a batch size of 8.\nThe following table shows representative examples over the first 10 RL steps using QWen2.5-1.5B\ntrained with GRPO on GSM8K and HiTab. Each entry reports the average reward at a specific step,\nalong with the total number of increases between adjacent steps.\nD Lessons Learned from Failed Attempts and Design Iterations\nThroughout the development of SuperRL, we explored a range of alternative strategies aimed at\nimproving data efficiency and stability in RL for language models. A key motivating principle behind\n21\n--- Page 22 ---\nDataset Step1 Step2 Step3 Step4 Step5 Step6 Step7 Step8 Step9 Step10 increase _num recent _avg_reward\nGSM8K 0.0617 0.0570 0.0828 0.1305 0.1820 0.2398 0.3148 0.3695 0.4609 0.4984 10 0.2397\nHiTab 0.0008 0.0008 0.0023 0 0 0 0 0 0 0 1 0.0031\nTable 7: Step-wise average reward during the first 10 RL updates on GSM8K and HiTab using\nQWen2.5-1.5B ( batch _size= 256 ). Steps are split into two blocks for compact readability.\nTable 8: On-Policy Evaluation Results on LIMO (Rollout-to-SFT)\nRun Name Final Score Min Score Max Score\nQwen2.5-1.5B_20250502 0.000 0.000 0.043\nQwen2.5-1.5B_20250501 0.000 0.000 0.042\nQwen2.5-1.5B_20250430 0.000 0.000 0.018\nour early design iterations was to fully leverage existing supervision data within the RL training loop,\nregardless of whether it was collected off-policy or generated dynamically. This led us to investigate\nseveral hybrid approaches that sought to maximize the utility of available data across both supervised\nand reinforcement learning signals. Despite their conceptual appeal, many of these attempts failed\nto yield meaningful improvements, and in some cases, actively degraded performance. Below, we\nsummarize the most instructive failures.\nD.1 The Pitfalls of Integrating Online Rollouts into Supervised Fine-Tuning in Hybrid Actor\nWe explored an on-policy training strategy that integrates supervised learning directly Hybrid Actor\nby extracting learning signals from rollout responses(for GRPO algorithm). Unlike naive self-\nimitation, our method uses a reward-aware filtering mechanism: rollout responses are adopted as\nsupervision targets only if they receive non-zero rewards; otherwise, the model reverts to offline\nexpert-annotated responses. This allows the training signal to be dynamically chosen based on the\nquality of exploration outcomes.This mechanism enables the actor to optimize toward both policy\nimprovement and imitation from valid behavior traces.\nDespite the theoretical appeal of this formulation, its performance on reward-sparse benchmarks was\nconsistently poor. As shown in Table 8, all variants of this reward-aware rollout-to-SFT approach\nachieved near-zero test scores on LIMO, a dataset characterized by complex multi-step reasoning and\nweak reward density.\nExperimental Results. Despite its theoretical appeal, this rollout-to-SFT strategy failed to yield\nrobust generalization in practice. As summarized in Table 8, all evaluated runs on the LIMO\nbenchmark—representing complex, long-horizon reasoning tasks—collapsed to near-zero accuracy.\nThis result holds across variations in random seed, training duration, and log-sigma scheduling.\nGeneralization Across Datasets. This failure pattern is not unique to the LIMO dataset. Across\nother reasoning benchmarks such as PRM12K, OpenR1, and HiTab, we observed similarly unstable\ntraining dynamics under the rollout-to-SFT paradigm. In some cases, such as PRM12K, the model\nexhibits a short-lived improvement in validation accuracy during early training, only to collapse\nirreversibly in later stages—indicative of an overfitting loop to spurious patterns in reward-positive\ntrajectories. In others, including OpenR1 and HiTab, the model degrades steadily throughout training,\nfailing to extract any durable learning signal from the reward-conditioned responses. Regardless\nof the specific trajectory, all runs eventually converge to near-zero accuracy, further affirming the\nbrittleness of this approach in sparse or deceptive reward landscapes.\nFailure Analysis. We identify several factors contributing to this consistent collapse. First, the\nstrategy relies on the current policy to generate both inputs and supervision targets, which introduces\na strong confirmation bias—especially problematic when rewards are sparse or noisy. The model\nreinforces its own suboptimal behavior without sufficient external correction, leading to overfitting\non early or lucky successes. Second, since reward filtering allows only high-reward responses to\npropagate gradients, the effective batch size of learning signals becomes highly unstable and often\ntoo small for reliable policy updates. Third, the fallback to offline supervision in zero-reward cases\nintroduces distributional mismatch between positive and negative samples, creating an inconsistent\noptimization target that destabilizes convergence. Finally, by tightly coupling exploration and\n22\n--- Page 23 ---\nimitation in an on-policy loop, the model becomes more susceptible to cascading errors—where\nminor deviations early in training can quickly spiral into mode collapse under the compounding\neffects of self-reinforcement.\nCollectively, these findings suggest that while reward-aware rollout-to-SFT is conceptually appealing\nfor integrating exploration and learning, its naive implementation fails to provide a stable or scalable\ntraining signal in practice—particularly under the reward sparsity and multi-step dependencies typical\nof reasoning-centric tasks.\nD.2 Offline Data as GRPO-Compatible Rollouts: A Negative Result\nTo investigate the potential of incorporating static expert supervision within GRPO training, we\naugment each sampled prompt by treating expert-annotated responses as auxiliary rollouts. Concretely,\nfor every on-policy prompt sampled during training, the corresponding offline trajectory is injected\ninto the GRPO buffer with its reward computed via a task-specific metric (e.g., exact match). These\nexpert rollouts coexist with policy-generated samples, contributing to the surrogate loss and gradient\nupdates. This design aims to regularize policy optimization and accelerate convergence by leveraging\nhigh-quality behavioral signals.\nEmpirical Collapse. Despite these motivations, both rollout-injection variants led to catastrophic\nlearning failure: validation accuracy declined consistently throughout training, indicating that static\nexpert rollouts destabilized optimization and accelerated mode collapse.\nD.2.1 Variant I: Direct Injection of Expert Rollouts\nThis variant treats offline expert trajectories as if they were on-policy samples from the current\npolicy πθ, directly incorporating them into the GRPO buffer for joint optimization. At each training\nstep, we sample a batch of expert trajectories (xe, ye)∼ D expert, assign rewards r(xe, ye)based on\ntask-specific metrics, and compute the mixed surrogate loss:\nLmixed=E(x,y)∼πθ[w(x, y)·logπθ(y|x)] +λ·E(xe,ye)∼D expert[r(xe, ye)·logπθ(ye|xe)],(1)\nwhere the first term corresponds to preference-weighted GRPO rollouts, and the second term injects\nexpert supervision weighted by scalar λ.\nObserved Failure. This strategy consistently degraded performance across all benchmarks (e.g.,\nLIMO, HiTab), slowing convergence and increasing KL divergence relative to both pure GRPO and\nreward-aware rollout-to-SFT methods. We attribute this to distributional mismatch : expert trajectories\n(xe, ye)are not sampled from πθ, violating GRPO’s on-policy assumption and introducing high-\nvariance gradients. As a result, the policy oscillates between chasing unreachable expert targets and\nreinforcing suboptimal behaviors. This is reflected in elevated KL divergence:\nKL(πθ∥πref) =Ex\"X\nyπθ(y|x) logπθ(y|x)\nπref(y|x)#\n, (2)\nsignaling instability and misalignment with the initial reference distribution. Furthermore, static\nexpert rollouts fail to adapt to the evolving policy’s exploration frontier, providing little guidance in\nunexplored regions of the solution space.\nD.2.2 Variant II: Self-Rewritten Expert Rollouts\nTo address the distributional mismatch, we introduce a self-rewrite mechanism wherein expert\nresponses are reformulated by the current policy to ensure alignment with its own distribution.\nGiven (xe, ye)∼ D expert, we prompt πθto generate a rewritten response ˆye∼πθ(·|xe, ye)that is\nsemantically equivalent to ye. The rewritten trajectory (xe,ˆye)is assigned the same reward as the\noriginal expert response, yielding the loss:\nLrewrite =E(x,y)∼πθ[w(x, y)·logπθ(y|x)] +λ·E(xe,ˆye)∼πθ(·|xe,ye)[r(xe,ˆye)·logπθ(ˆye|xe)].\n(3)\nObserved Failure. While distributional alignment is improved, this method failed to yield perfor-\nmance gains. Rewritten responses often diverged semantically from expert outputs, especially in multi-\nstep reasoning tasks, introducing semantic drift . Assigning expert-level rewards r(xe,ˆye) =r(xe, ye)\n23\n--- Page 24 ---\nTable 9: Few-shot Prompting Performance with Random Selection (Qwen2.5-1.5B, GRPO)\nDataset Vanilla 1-shot (Random) 3-shot (Random) Best (Random) Gain Observation\nGSM8K 0.755 0.747 0.745 Vanilla -0.010 Few-shots degrade performance\nMetaMath 0.814 0.793 0.8255 3-shot +0.011 Faster convergence; slight gain\nPRM12K 0.505 0.480 0.5057 3-shot +0.007 Slight gain\nLIMO 0.018 0.018 →0 0.018 →0 None Collapse All models collapse to 0\nto imperfect rewrites led to reward misalignment:\nE[r(xe,ˆye)]< r(xe, ye),but used reward ≈r(xe, ye). (4)\nThis misalignment inflated value predictions and reinforced spurious reasoning paths, resulting in\ncumulative confirmation bias and unstable convergence.\nD.3 Conclusion: Offline Rollouts Undermine GRPO Stability\nOur findings reveal that treating offline trajectories as GRPO-compatible rollouts—either directly\nor via self-rewriting—fails to improve and often degrades performance. These strategies violate\nGRPO’s on-policy assumptions and introduce high-variance or semantically misaligned gradients.\nWe conclude that more principled methods are needed to bridge the gap between static supervision\nand dynamic exploration without compromising the stability of reinforcement learning updates.\nD.4 Offline Data as Few-shots\nTo more effectively leverage the information contained in high-quality offline data, we explored\naugmenting the original prompt by prepending additional exemplars drawn from the same supervised\ndataset. These few-shot exemplars, comprising other question-solution pairs, are intended to serve as\nimplicit guidance. By exposing the model to a wider range of reasoning patterns, we aim to enhance\nits generalization capabilities.\nWe explored three selection strategies for constructing these few-shot demonstrations: Random\nSelection: Randomly sample one or more exemplars from the SFT dataset without conditioning on\nthe current prompt. Prompt Similarity: Retrieve exemplars whose questions are semantically similar\nto the target prompt, based on embedding-based similarity. Solution Similarity: Select exemplars\nwhose solutions are structurally or semantically similar to the expert solution of the target prompt.\nFor each strategy, we varied the number of exemplars included in the prompt (one vs. three) to test\nwhether increasing few-shot context leads to measurable improvements.\nEmpirical Observations. Despite the intuitive appeal of enriching the prompt with related examples,\nnone of the strategies consistently outperformed the base model across benchmarks. The effectiveness\nof few-shot augmentation was highly variable: while minor gains were observed on specific reasoning-\ncentric datasets such as MetaMath and PRM12K, performance degraded or remained stagnant on\nothers, including GSM8K and LIMO. Notably, LIMO exhibited complete collapse under few-shot\naugmentation, echoing similar failure patterns seen in other unstable training regimes.\nWe performed extensive evaluations using the Random Select strategy across four datasets, while the\nPrompt Similarity andSolution Similarity strategies were validated on a subset due to computa-\ntional constraints. Nevertheless, all methods displayed consistent patterns: no significant improvement\nwas achieved over the vanilla setting, and the inclusion of static exemplars occasionally introduced\nadditional variance or learning instability. Crucially, none of the approaches achieved the type of\nmeaningful gains observed under our proposed hybrid training strategy, which dynamically balances\nsupervised and policy gradient objectives.\nIn summary, our findings highlight the limitations of offline few-shot augmentation as a plug-and-play\nsolution for improving GRPO training. The static nature of these exemplars, combined with potential\nmismatches in reasoning complexity or style, undermines their utility as general-purpose inductive\nscaffolds. While few-shot prompting may offer minor benefits in specific cases, it fails to deliver\nconsistent performance gains and does not constitute a reliable substitute for hybrid or dynamically\nsupervised training methods.\n24\n--- Page 25 ---\nTable 10: Comparison of Human-written vs. Synthetic Solutions on PRM12K (Qwen2.5-1.5B,\nGRPO)\nMethod Final Accuracy Observation\nVanilla 0.49 Strong baseline without hybridization\nHybrid(Synthetic Data) 0.475 Slow start, eventually catches up\nHybrid(Synthetic Data) 0.504 Fastest convergence and best final score\nHybrid(Human-written Data) 0.482 More stable early-stage learning\nD.5 Human Written or Synthetic Offline Data\nTo further examine the impact of solution provenance on policy learning under GRPO, we conducted\ntargeted experiments on the PRM12K dataset, which uniquely provides two types of reference\nsolutions per instance: a standard solution and several model-generated (synthetic) solutions. This\ndual annotation enables a controlled comparison between high-quality manual demonstrations and\nLLM-synthesized reasoning traces, which may vary in logical depth and fluency.\nWe implemented and evaluated several hybrid training variants that leverage these distinct solution\ntypes under our SFT-RL hybrid framework. The baseline ( Vanilla ) uses no hybridization and serves\nas a pure GRPO control. We then compared two hybrid configurations: one incorporating human-\nwritten solutions ( Hybrid-Human ), and another substituting in synthetic solutions from the dataset\n(Hybrid-Synthetic ). Additionally, a per-step cosine-weighted hybrid approach was tested to assess\nthe influence of reward shaping with respect to solution type.\nThe results in Table 10suggest that both human-written and synthetic solutions can be effectively\nleveraged within our hybrid SFT-RL training framework. Compared to the Vanilla baseline (0.49),\nall hybrid variants exhibit comparable or improved performance, indicating that incorporating of-\nfline solutions—regardless of provenance—provides additional learning signal beneficial for policy\noptimization.\nNotably, while the best-performing configuration utilizes synthetic data (0.504), the overall differences\nbetween human and synthetic variants remain modest. The Hybrid (Human-written) model achieves\n0.482, and both variants using Hybrid (Synthetic) data fall within a similar performance band. This\noutcome suggests that model-generated solutions, despite potential imperfections, can approximate\nthe utility of high-quality human annotations in guiding learning—especially when training is\naugmented with appropriate reward signals.\nThese findings challenge the assumption that human-written demonstrations are categorically superior\nfor hybrid learning. Instead, they highlight the practical viability of synthetic solutions, which are\nmore scalable to obtain in real-world settings. The relatively small performance gap also opens the\ndoor to future improvements through better filtering, weighting, or mixture modeling over solution\nsources.\nImplications. These results underscore a practical advantage for scalable deployment: in many\nreal-world scenarios, high-quality human-written data may be scarce or prohibitively expensive\nto collect. Our findings suggest that it is feasible to substitute such data with model-generated\n(synthetic) solutions—especially when integrated into training via techniques like SuperRL. This\nopens up a promising path for applying our method in low-resource domains, enabling policy learning\nfrom weaker yet abundant supervision sources. Further gains may be achieved by coupling this\napproach with strategic data curation methods such as solution filtering, reward-weighted sampling,\nor mixture-of-expert style routing over solution types.\nD.6 Comparative Study of Hybrid Loss Integration Strategies\nWe investigate three distinct designs for hybrid optimization between policy gradients and supervised\nfine-tuning, aiming to exploit the complementary strengths of exploration and imitation. These\nvariants differ in how they balance PPO loss ( LPPO) and SFT loss ( LSFT), and in how the mixing\nweights are learned and applied.\n25\n--- Page 26 ---\nTable 11: LIMO Final Scores of Hybrid Actor Variants (Qwen2.5-1.5B)\nCategory Method (Run) Final Score Trend\nHYBRID LOG-SIGMARun 1 0.061 Improves steadily\nRun 2 0.058 Slightly fluctuating\nRun 3 0.060 Stable high\nHYBRID THETA (α-weighted)Run 1 0.019 Slow rise\nRun 2 0.017 Plateaus early\nRun 3 0.018 Fluctuates\nHYBRID PER-STEPRun 1 0.013 Collapse after peak\nRun 2 0.011 Fluctuating low\nRun 3 0.010 Declines continuously\nD.6.1 Design and Evaluation of Hybrid Learning Strategies\nTo effectively balance SFT and RL objectives during actor training, we explore three hybrid op-\ntimization strategies that integrate both signals with different weighting mechanisms. Below, we\npresent each design’s formulation and underlying rationale, followed by a comparative analysis on\nthe GAIR/LIMO benchmark.\nHybrid Log-Sigma (Uncertainty-Based Weighting). This method introduces two independent\nlearnable log-variance parameters, logσpgandlogσsft, to dynamically reweight the PPO loss LPPO\nand SFT loss LSFT. The total hybrid loss is given by:\nLtotal= exp( −2 logσpg)· LPPO+ exp( −2 logσsft)· LSFT+ log σpg+ log σsft\nThis formulation follows the uncertainty weighting paradigm proposed by Kendall et al. (2018),\nwhere the log-variance terms both modulate gradient scaling and regularize the optimization process.\nIt allows the model to autonomously learn which objective to prioritize at each stage of training,\nadapting to dynamic gradient magnitudes and learning phases. Empirically, this method achieves the\nmost stable training and best final performance on LIMO.\nHybrid Theta ( α-Weighted Convex Combination). In this variant, a single scalar parameter α\ngoverns the mixing ratio between PPO and SFT losses via a sigmoid transformation:\nwpg=σ(α), w sft= 1−wpg,Ltotal=wpg· LPPO+wsft· LSFT\nThe weights are constrained to form a convex combination, ensuring bounded influence from each\nloss term. Although simpler and easier to optimize, this approach lacks the fine-grained adaptivity of\nuncertainty-based methods. It struggles to respond to non-stationary gradients and therefore performs\nmoderately in terms of stability and final score.\nHybrid Per-Step (Sequential Log-Sigma). This design decouples PPO and SFT into two con-\nsecutive optimization steps per batch. After completing the PPO update, an additional SFT step is\nperformed using samples containing reference answers. The SFT objective is scaled by a log-variance\nparameter:\nLweighted\nSFT = exp( −2 logσsft)· LSFT+ log σsft\nWhile modular and implementation-friendly, this approach suffers from gradient inconsistency across\nupdates. Since PPO and SFT gradients are applied separately, the optimization trajectory becomes\nunstable. Moreover, supervision is only introduced after the PPO step, reducing its effectiveness in\nguiding exploration. As a result, training frequently collapses or fails to converge stably.\nExperimental Results on GAIR/LIMO. As shown in Table 11, the Hybrid Log-Sigma variant\nsignificantly outperforms the others on both final accuracy and stability metrics. The uncertainty-\n26\n--- Page 27 ---\nweighted joint loss leads to smoother training dynamics and better generalization. In contrast, the\nα-weighted Hybrid Theta variant converges more slowly and achieves a lower final score. The\nsequential Hybrid Per-Step strategy performs the worst, suffering from frequent instability and\ncollapse.\nD.6.2 Analysis of Hybrid Actor Variants\nWe conduct a comprehensive evaluation of three hybrid actor designs on the LIMO benchmark to\nassess their stability and effectiveness under long-horizon reasoning tasks. As shown in Table 11,\nHYBRID LOG-SIGMA outperforms the other two variants across all metrics. Its final scores remain\nconsistently high (0.060–0.061), with learning curves that are either steadily improving or stably\nmaintained throughout training. In contrast, HYBRID THETA andHYBRID PER-STEPdisplay marked\nsigns of instability, with final scores falling below 0.02 and clear signs of collapse or early stagnation.\nHybrid Log-Sigma. This variant exhibits the most reliable optimization behavior. Its performance\nstability suggests that the dynamic weighting mechanism, which adjusts the balance between SFT and\nRL losses based on task difficulty and learning dynamics, effectively prevents either objective from\ndominating. Such adaptability is particularly critical in LIMO, where reasoning depth and reward\nsparsity fluctuate significantly across samples. The model learns to prioritize supervised signals\nearly on and gradually shifts toward reward-driven fine-tuning, enabling smooth convergence without\ndegrading performance.\nHybrid Theta. In contrast, the fixed-coefficient weighting used in this variant fails to accommodate\nthe evolving balance between imitation and exploration. When the weight on SFT is too strong, the\nmodel struggles to learn from reward signals, leading to early saturation. Conversely, when RL is\noverweighted, learning becomes unstable due to reward sparsity. This rigidity in balancing objectives\nlikely explains the plateaued or erratic learning curves observed across all three runs, with final scores\nranging only from 0.017 to 0.019. None of the runs show significant late-stage improvement.\nHybrid Per-Step. This variant performs the worst across the board. By applying the hybrid loss\nat every generation step, it exposes the model to highly localized and often conflicting gradient\nsignals between the SFT objective (which emphasizes sequence-level coherence) and the RL signal\n(which varies stochastically across steps). This fine-grained interference introduces high optimization\nvariance, especially harmful on tasks like LIMO that require multi-step planning and semantic\nalignment. The result is catastrophic instability: one run collapses shortly after peaking, while others\nshow continuous degradation, with final scores dropping to 0.010–0.013.\nConclusion. Among the three variants, only HYBRID LOG-SIGMA avoids collapse and achieves\nstable high performance on LIMO. Its dynamic loss weighting provides the flexibility required to\nnavigate the complex trade-offs inherent in hybrid training. The empirical trends strongly support that\nprincipled uncertainty-based balancing is crucial for successful integration of SFT and RL signals,\nespecially on benchmarks requiring compositional reasoning.\nD.7 Distillation or Hybrid? Choosing by Data Regime\nWhile our SuperRL demonstrates strong performance on small, high-quality datasets, recent\ndistillation-based approaches—such as Deepseek-R1-Distill often surpass our results when ample\nhigh-quality offline data from different aspects are available. In particular, on the LIMO benchmark,\nthe best R1-Distill variant achieves higher final exact-match accuracy, at the expense of substantially\ngreater data and computation requirements.\nMarginal Gains on Stronger Models. On the stronger R1-Distill-1.5B backbone, our Hybrid\nmethod yields modest but consistent improvements. Specifically, the accuracy on GSM8K rises\nfrom 74.5% to 76.3% (+1.8 percentage points), on PRM12K from 54.8% to 55.8% (+1.0 percentage\npoints), and on LIMO from 30.0% to 31.2% (+1.2 percentage points). These small absolute gains\nreflect the diminishing returns when distillation has already imbued the model with high-quality\nreasoning traces. Notably, the improvements are relatively minor compared to the substantial gains\nobserved on weaker models, highlighting the diminishing effectiveness of additional optimization\ntechniques on already well-performing models.\n27\n--- Page 28 ---\nTable 12: Exact-Match Accuracy on GSM8K, PRM12K, and LIMO using R1-Distill-1.5B and\nQwen2.5-1.5B\nMethod (Run)GSM8K PRM12K LIMO\nR1-Distill Qwen2.5 R1-Distill Qwen2.5 R1-Distill Qwen2.5\nVanilla 0.745 59.7% 0.548 26.7% 0.300 1.2%\nHybrid (Run-1) 0.759 77.0% 0.553 14.0% 0.312 2.6%\nHybrid (Run-2) 0.763 74.7% 0.558 35.1% 0.295 6.9%\nHybrid (Run-3) 0.748 70.0% 0.540 33.0% 0.289 1.8%\nHybrid (Run-4) 0.749 74.7% 0.543 47.7% 0.295 3.0%\nSubstantial Improvements on Weaker Backbones. By contrast, when applied to the base Qwen-\n1.5B model, Hybrid unlocks far larger relative improvements over its vanilla SFT performance. The\naccuracy on GSM8K jumps from 59.7% to 77.0% (+17.3 percentage points), on PRM12K from\n26.7% to 35.1% (+8.4 percentage points), and on LIMO from 1.2% to 6.9% (+5.7 percentage points).\nThis demonstrates Hybrid’s ability to rescue underperforming backbones by interleaving SFT and RL,\neven without a separate distillation teacher. The significant gains on weaker models underscore the\neffectiveness of Hybrid in enhancing reasoning capabilities where traditional SFT alone falls short.\nDataset Difficulty and Regime Selection. Despite these gains, absolute performance on the\nhardest dataset (LIMO) remains low (<32%) for both models, underscoring that Hybrid alone cannot\nsubstitute for the rich, multi-step traces distilled in R1-Distill. In data-rich, multi-domain settings, a\ndedicated multi-round SFT+RL (distillation) pipeline is justified by the higher asymptotic accuracy.\nHowever, in single-dataset or low-resource scenarios—where collecting extensive expert traces is\nimpractical—Hybrid offers superior sample efficiency and robust gains, making it the preferable\nchoice.\nWhy R1-Distill-1.5B Excels. The R1-Distill-1.5B variant attains its strong performance by first\nleveraging a high-capacity teacher model to generate rich, multi-step reasoning traces over a large,\nheterogeneous corpus of problems. These synthetic expert traces are then used in a multi-stage\ndistillation pipeline: an initial supervised fine-tuning (SFT) phase to bootstrap reasoning capabilities,\nfollowed by on-policy reinforcement learning (RL) with a behavior-cloned policy as initialization.\nThis two-phase approach both transfers the teacher’s structured reasoning patterns and refines them\nunder the true task reward, yielding substantial gains when the distillation corpus is sufficiently large\nand diverse. The effectiveness of R1-Distill-1.5B is particularly pronounced in settings with abundant\ndata and computational resources, where the model can fully leverage the high-quality reasoning\ntraces generated by the teacher model.\nUtility of Our Hybrid Method. By contrast, our SuperRL hybrid method interleaves SFT and\nRL updates at every optimization step using an uncertainty-weighted loss, avoiding the need for\nseparate distillation and RL phases. This unified fusion enables stable learning even when only\na modest amount of high-quality reasoning data is available. As Table 12demonstrates, Hybrid\nmatches or nearly matches R1-Distill performance on GSM8K and PRM12K under constrained data\nbudgets, while requiring significantly less compute and no external distillation teacher. The Hybrid\nmethod’s ability to achieve comparable performance with fewer resources highlights its efficiency\nand adaptability in resource-limited environments.\nData Regime Considerations. When abundant multi-domain reasoning traces can be collected or\nsynthesized, a dedicated distillation pipeline (i.e., multi-round SFT followed by RL) can push base\nmodels beyond Hybrid’s plateau, as seen on LIMO. However, in single-dataset or low-resource set-\ntings—where acquiring large, diverse expert traces is impractical—our single-stage Hybrid approach\ndelivers superior sample efficiency and robustness. Thus, practitioners should favor distillation-based\nSFT+RL when ample data and compute are available, but rely on Hybrid to unlock reasoning gains\nin the small-data regime. The choice between these methods should be guided by the availability of\nresources and the specific characteristics of the dataset and task at hand.\n28\n--- Page 29 ---\nE Soft Fusion over Hard Switching: A Unified Perspective on SuperRL\nversus SFT+RL\nA central challenge in aligning large language models (LLMs) with desired behaviors lies in how\nto effectively combine supervised fine-tuning (SFT) and reinforcement learning (RL). While both\nparadigms contribute valuable learning signals—SFT offering human-aligned supervision and RL\nenabling optimization toward long-term utility—the way these signals are integrated significantly\naffects the model’s learning stability, generalization ability, and sample efficiency.\nThe traditional SFT+RL framework adopts a hard two-stage approach. During the first stage, the\nmodel is trained to imitate expert demonstrations using SFT, thereby anchoring the policy to safe and\ninterpretable behaviors. Once a strong imitation prior is established, the second stage introduces RL\nto optimize a reward signal derived from task metrics or preference models. Although intuitive and\neasy to implement, this sequential strategy suffers from three core limitations. First, the abrupt shift\nin training objectives often leads to instability and catastrophic forgetting of prior knowledge. Second,\nthe strong imitation prior may reduce the model’s willingness to explore behaviors that deviate from\ndemonstrations, even when such behaviors yield higher rewards. Third, in the absence of balancing\nsupervision, the RL phase can over-optimize to flawed or narrow reward functions, thereby harming\ngeneralization.\nIn contrast, SuperRL adopts a soft fusion strategy, where SFT and RL signals are blended within\neach training iteration rather than staged sequentially. Instead of viewing imitation and exploration as\nisolated phases, SuperRL treats them as co-evolving learning signals, optimized jointly throughout\ntraining. The relative importance of each objective is dynamically adjusted based on their estimated\nuncertainty, allowing the learning system to flexibly prioritize the more informative signal at each point\nin training. This formulation offers several benefits. By maintaining the influence of SFT across all\nsteps, SuperRL prevents sharp behavioral drift and promotes smooth policy evolution. Its uncertainty-\nweighted mechanism enables dynamic rebalancing, allowing the model to emphasize exploration\nwhen confident and revert to supervision when uncertain. Furthermore, this joint optimization\nencourages the model to reconcile imitation and reward-driven adaptation, ultimately improving its\nability to generalize to out-of-distribution settings.\nRather than treating hard switching and soft fusion as binary opposites, we consider them as two\nends of a continuous spectrum. Hard switching may be advantageous in environments where\ndemonstrations are accurate and dense, and the reward function is well-aligned with those demonstra-\ntions. However, in more complex or underspecified settings—such as open-ended reasoning tasks,\npreference-based generation, or safety-critical applications—the rigidity of hard switching becomes\na liability. Here, soft fusion mechanisms like SuperRL provide better flexibility and robustness by\ncontinuously adjusting to evolving training signals and conflicting supervision sources.\nA central innovation of SuperRL is the uncertainty-weighted hybrid actor, which assigns adaptive\nweights to SFT and RL loss terms based on their reliability. This allows the model to down-weight\nnoisy or misleading signals and focus on gradients that offer higher expected utility. Such a mechanism\nis especially valuable when either the reward function or the demonstrations are partial, evolving,\nor domain-dependent. It helps mitigate destructive interference between incompatible objectives,\nimprove sample efficiency under sparse rewards, and preserve human-aligned behaviors without\nlimiting the model’s capacity to explore novel solutions.\nDespite its strengths, the soft fusion strategy is not without limitations. First, optimizing SFT and\nRL objectives jointly may lead to conflicting gradients, resulting in diluted or unstable updates if\nneither signal dominates. Second, the method’s success hinges on the accuracy of the uncertainty\nestimates. In early stages of training or in noisy environments, these estimates can be unreliable,\nleading to poor weighting decisions and suboptimal learning trajectories. Third, in tasks with well-\nshaped and high-quality reward functions, the presence of an SFT loss may actually slow down\nconvergence to the optimal policy, as it introduces additional constraints. Fourth, the simultaneous\npresence of two learning objectives complicates debugging and interpretability; it becomes harder to\npinpoint the root cause of training failures. Fifth, when SFT data is low-quality or misaligned with\nreward criteria, retaining its influence throughout training may degrade performance. Finally, from a\npractical standpoint, jointly maintaining both loss signals increases computational cost and training\ncomplexity—particularly in large-scale or multi-model setups.\n29\n--- Page 30 ---\nGiven these trade-offs, we recommend using soft fusion methods like SuperRL in domains that\ndemand generalization, stability, and human-alignment under weak or partially specified reward\nfunctions. Such domains include open-domain question answering, value-sensitive decision making,\nand instruction tuning with sparse preference feedback. In contrast, SFT+RL may remain a preferable\nalternative in settings with dense, trustworthy rewards and high alignment between demonstrations\nand task incentives, such as games, synthetic environments, or simulation-based agents.\nIn summary, SuperRL offers a principled alternative to hard switching by integrating imitation and\nexploration in a unified and adaptive manner. Although it introduces additional complexity, its benefits\nin generalization, sample efficiency, and robustness make it a compelling choice for real-world LLM\ntraining scenarios that demand flexibility and long-horizon reasoning.\n30",
  "text_length": 108544
}