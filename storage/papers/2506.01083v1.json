{
  "id": "http://arxiv.org/abs/2506.01083v1",
  "title": "Generative diffusion posterior sampling for informative likelihoods",
  "summary": "Sequential Monte Carlo (SMC) methods have recently shown successful results\nfor conditional sampling of generative diffusion models. In this paper we\npropose a new diffusion posterior SMC sampler achieving improved statistical\nefficiencies, particularly under outlier conditions or highly informative\nlikelihoods. The key idea is to construct an observation path that correlates\nwith the diffusion model and to design the sampler to leverage this correlation\nfor more efficient sampling. Empirical results conclude the efficiency.",
  "authors": [
    "Zheng Zhao"
  ],
  "published": "2025-06-01T17:01:14Z",
  "updated": "2025-06-01T17:01:14Z",
  "categories": [
    "stat.ML",
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01083v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01083v1  [stat.ML]  1 Jun 2025Communications in Information and Systems\nVolume 0, Number 0, 1–9, 2022\nGenerative diffusion posterior sampling for\ninformative likelihoods∗\nZheng Zhao\nSequential Monte Carlo (SMC) methods have recently shown suc-\ncessful results for conditional sampling of generative diffusion mod-\nels. In this paper we propose a new diffusion posterior SMC sampler\nachieving improved statistical efficiencies, particularly under outlier\nconditions or highly informative likelihoods. The key idea is to\nconstruct an observation path that correlates with the diffusion\nmodel and to design the sampler to leverage this correlation for\nmore efficient sampling. Empirical results conclude the efficiency.\nAMS 2000 subject classifications: Primary 62-08, 62L99; secondary\n68T99.\nKeywords and phrases: Generative diffusion models, Feynman–Kac\nmodels, conditional sampling, posterior sampling, sequential Monte Carlo.\n1. Problem formulation\nConsider a probability distribution π(·)extended as a time marginal of\nq0:N(u0:N) =q0(u0)N/productdisplay\nk=1qk|k−1(uk|uk−1), q 0=πref, qN=π,\np0:N(x0:N) =p0(x0)N/productdisplay\nk=1pk|k−1(xk|xk−1), p 0=π, pN=πref,(1)\nwherepk|k−1andqk|k−1stand for the noising and denoising transition distri-\nbutions on Rd, respectively. This pair of processes, when exists, constitutes a\ngenerative diffusion model, allowing efficient sampling from πby first sampling\nfrom a reference distribution πrefand then sequentially denoising via qk|k−1\nfork= 1,2,...,N. The construction of such models has been a central topic\narXiv: 0000.0000\n∗This work was partially supported by 1) the Wallenberg AI, Autonomous\nSystems and Software Program (WASP) funded by the Knut and Alice Wallenberg\nFoundation, and 2) the Stiftelsen G.S Magnusons fond (MG2024-0035).\n1\n--- Page 2 ---\n2 Zheng Zhao\nin recent research, focusing on accelerating the computation and reducing the\nstatistical approximation errors. A common approach is to choose the noising\npk|k−1as an Ornstein–Uhlenbeck process, so that approximately pN≈N(0,I)\nfor large enough N, and that the denoising process qk|k−1can be estimated\nwith score matching (Song et al., 2021; Ho et al., 2020), or more recently,\nscoring rules (Shen et al., 2025). Another useful construction is the dynamic\nSchrödinger bridge, which constructs both noising and denoising processes by\nminimising a transportation cost relative to a reference process (De Bortoli\net al., 2021). For a detailed exposition of generative diffusion models, we\nrefer the readers to, for example, Zhao et al. (2025); Albergo et al. (2023);\nBenton et al. (2024). Throughout this paper, we assume that such a generative\ndiffusion model in Equation (1) is given.\nThe overall goal of the paper is to sample from the conditional/posterior\ndistribution π(·|y)∝f(y|·)π(·), wherefis a given likelihood function, and\ny∈Rcdenotes an observation (Zhao et al., 2025). This task is at the core\ninterests of Bayesian statistics in general, and in the context of generative\ndiffusion models specifically, this has enabled many downstream applications,\nsuch as image restoration problems (Luo et al., 2025, 2023). Recently, a few\nmethods have been proposed to address this problem, notably the diffusion\nposterior sampling (Chung et al., 2023) and their variants (see survey by Daras\netal.,2024).Thesemethodsrelyonconstructingaconditionaldiffusionprocess\ncharacterised by transition /tildewideqk|k−1(uk|uk−1,y)that encodes the likelihood f\nand observation, to target π(·|y). However, these methods are intrinsically\nbiased due to necessary approximations to /tildewideqk|k−1. This has then motivated\nanother class of methods based on Feynman–Kac models and sequential Monte\nCarlo (SMC, Chopin and Papaspiliopoulos, 2020), to remove such biases (e.g.,\nWu et al., 2023; Cardoso et al., 2024; Janati et al., 2024; Kelvinius et al., 2025;\nCorenflos et al., 2025). At the heart, they work by using /tildewideqas a proposal, and\nthen correct the proposal samples with importance weights. While this type of\nmethods has shown to be empirically working well, their statistical efficiencies\n(e.g., effective sample size) can be significantly degraded by high dimension\ndand informative likelihood. This challenge has long been recognised in the\nSMC literature, such as the seminal work by Del Moral and Murray (2015).\nHowever, in the context of SMC conditional sampling of generative diffusion\nmodels, this is under investigated. As such, the specific purpose of this paper\nis to address the problem when the likelihood being highly informative or\nwhen the observation is an outlier.\nOur contributions are as follows. We develop a new Feynman–Kac and\nSMC-based framework to sample conditional/posterior distributions with\ngenerative diffusion model prior. The method is designed to stay statistically\n--- Page 3 ---\nGenerative diffusion posterior sampling 3\nrobust, even when the observation is an outlier. We show via high-dimensional\nexperiments that the proposed method outperforms the state of the art.\nThe paper is organised as follows. In Section 2 we introduce necessary\npremises and explain how the conditional sampling works with Feynman–Kac\nmodels and SMC samplers. Then, in Section 3 we present our construction\nof the Feynman–Kac model to facilitate sampling under outlier observations,\nfollowed by experiments in Section 4.\n2. Generative Feynman–Kac models\nA Feynman–Kac model Q0:Ndefines a probability distribution over a temporal\nsequence of N+ 1random variables:\n(2)Q0:N(u0:N) =1\nZNM0(u0)G0(u0)N/productdisplay\nk=1Mk|k−1(uk|uk−1)Gk(uk,uk−1),\nwhereM0is the initial distribution, ZNis the normalising constant, and\nMk|k−1andGkare Markov (proposal) transition distribution and potential\nfunction, respectively. Given the model components, a natural approach for\nsamplingQ0:Nis sequential Monte Carlo (SMC) which exploits the model’s\ntemporal structure, see Algorithm 1 for implementation. At its heart, this can\nbe viewed as a tempering sequence, gradually annealing from Q0toQN. For\ndetailed expositions of Feynman–Kac models and SMC samplers, we refer the\nreaders to Chopin and Papaspiliopoulos (2020) and Del Moral (2004).\nAlgorithm 1: Sequential Monte Carlo (SMC) for sampling Feynman–\nKac model Q0:N.\nInputs: Model components M0,G0,{Mk|k−1}N\nk=1,{Gk}N\nk=1, number of\nsamplesJ.\nOutputs: Weighted samples of Q0:N.\n1DrawJsamples{Uj\n0}J\nj=1∼M0.\n2Weightwj\n0=G0(Uj\n0)//summationtextJ\ni=1G0(Ui\n0) // Forj= 1,...,J\n3fork= 1,2,...,N do\n4Resample/braceleftbig\n(wk−1,Uj\nk−1)/bracerightbigJ\nj=1if needed.\n5DrawUj\nk∼Mk|k−1(·|Uj\nk−1). // Forj= 1,...,J\n6Weightwj\nk=wj\nk−1Gk(Uj\nk,Uj\nk−1). // Forj= 1,...,J\n7Normalisewj\nk=wj\nk//summationtextJ\ni=1wi\nk. // Forj= 1,...,J\n8end\n--- Page 4 ---\n4 Zheng Zhao\nThe Feynman–Kac model Q0:Nis particularly useful for generative dif-\nfusion models. It not only generalises both conditional and unconditional\ndiffusion models but also allows for efficient sampling via the SMC sampler in\nAlgorithm 1. As an example, if we set M0=q0,G0≡1,Mk|k−1=qk|k−1,\nandGk≡1, thenQ0:Nexactly recovers the unconditional denoising path\ndistribution q0:N. To generalise Q0:Nfor the conditional sampling, one can\ndesigna sequence of twisting functions {ly\nk}N\nk=0that incorporate the likelihood\nand observation, and construct Q0:Nby (see, Zhao et al., 2025; Janati et al.,\n2024; Wu et al., 2023):\nM0=q0, G 0=ly\n0, ly\nN(·) =f(y|·),\nMk|k−1(uk|uk−1)Gk(uk,uk−1) =ly\nk(uk)qk|k−1(uk|uk−1)\nly\nk−1(uk−1),(3)\nso that now marginally QN(uN)∝ly\nN(uN)qN(uN) =f(y|uN)π(uN)recovers\nthe target posterior distribution. Clearly, there is a degree of freedom in\nchoosing the twisting sequence {ly\nk}N\nk=0under only the terminal constraint\nly\nN(·) =f(y|·). A trivial choice is to use ly\nk≡1fork= 0,1,...,N−1, but\nthen the resulting model collapses a single-step importance sampling at N,\nmaking Algorithm 1 useless for high-dimensional diffusion models. Therefore,\nthe key challenge here is the design of the twisting functions to facilitate\nefficient sampling of the SMC sampler in Algorithm 1. Ideally, we would like\nthe tempering sequence\n(4) Qk(uk)∝ly\nk(uk)qk(uk)\nto be “equidistant” enough over k= 0,1,...,N.\nRemark 2.1 (Bootstrap and guided construction) .Running Algorithm 1\nrequires the Markov proposal Mk|k−1and potential function Gkwhich are\nimplicitly defined through a product structure in Equation (3). In practice, the\nbootstrap\n(5)Mk|k−1=qk|k−1(uk|uk−1), Gk(uk,uk−1) =ly\nk(uk)\nly\nk−1(uk−1),\nand the guided\nMk|k−1∝ly\nk(uk)qk|k−1(uk|uk−1),\nGk(uk,uk−1) =ly\nk(uk)qk|k−1(uk|uk−1)\nly\nk−1(uk−1)Mk|k−1(uk|uk−1),(6)\n--- Page 5 ---\nGenerative diffusion posterior sampling 5\nare the most common two constructions of the model (Chopin and Papaspiliopou-\nlos, 2020). While the bootstrap version is easier to implement and is often\ncomputationally cheaper, the guided version is statistically superior, particu-\nlarly for high-dimensional problems. The guided proposal Mk|k−1is locally\noptimal, minimising the marginal variance of the weights in Algorithm 1.\nA canonical, and perhaps also the most used twisting function (Wu et al.,\n2023) is given by\n(7) ly\nk(uk) =/integraldisplay\nf(y|uN)qN|k(uN|uk) duN,\nsuch that the marginal distribution Qk(uk) =π(uk|y)represents the condi-\ntional denoising model. Notably, The resulting Feynman–Kac model gives an\nEulerian representation of the conditional score stochastic differential equa-\ntion (SDE, Song et al., 2021, Equ. 14) at discrete times when the denoising\nmodelqk|k−1is given by a score SDE. Precisely, if the noising process is an\nSDE dXt=a(Xt,t) dt+ dBtwithX0∼π, thenQ0:Nis the finite-dimensional\ndistribution of the conditional SDE\ndUt=−a(Ut,T−t) +/parenleftbig\n∇logpT−t(Ut) +∇logly\nT−t(Ut)/parenrightbig\ndt+ dWt,\nUt∼πref(·|y),(8)\nat discrete times t0,t1,...,tN. The score∇logly\nk(·), sometimes referred to\nas a guidance or control term, is added alongside with the unconditional\nscore function∇logpk(·)in the conditional SDE. Although the exact twisting\nfunction in Equation (7)is intractable in practice, many useful and computa-\ntional approximations have been established (see surveys by Daras et al., 2024;\nLuo et al., 2025). Furthermore, these approximations can be directly inte-\ngrated in the Feynman–Kac model, for improved statistical performance (e.g.,\nKelvinius et al., 2025) with SMC samplers. These are the reasons why this\ncanonical twisting construction is popularly used. As an example, Wu et al.\n(2023) combine Tweedie’s formula and first-order approximation to the in-\ntegral:ly\nk(uk)≈f(y|E[X0|XN−k=uk])which essentially generalises that\nof Chung et al. (2023).\nHowever, many criticism can be said to this canonical twisting. First, in\nalmost all practical cases, the canonical twisting has to be approximated, and\nthe approximation errors are substantial and difficult to control particularly\nwhenN−kis large. Second, even if we were able to compute the canonical\ntwisting exactly, constructing an effective proposal Mk|k−1, see Remark 2.1,\nis hard as well. These two problems are especially pronounced when the\n--- Page 6 ---\n6 Zheng Zhao\nlikelihoodfis informative, or yis an outlier observation. To build some\nintuition, consider a heuristic: suppose that we have Jsamples{Uj\n0}J\nj=1∼q0,\nthen, to get weighed samples for Q0(·)∝ly\n0(·)q0(·), Equation (7)is essentially\npushforwarding{Uj\n0}J\nj=1to{Uj\nN}J\nj=1and then obtain the weights by evaluating\nf(y|{Uj\nN}J\nj=1)which are likely to be degenerate under outlier y.\n3. Construction of twisting sequence with observation path\nAfter all, if our primary interest is only the marginal qN(·|y), we are not\nobliged to constrain the path Q0:Nto follow the SDE representation in Equa-\ntion(8). In this section, we depart from the canonical design, and develop\na new construction of the twisting functions {lvk\nk}N\nk=0, especially tailored for\ninformative likelihood or outlier observation. Our gist consists in designing\nlvk\nkin a way that it is an “appropriate” likelihood for qkatk, by making a\nsmooth bridging/tempering between lvk\n0, the reference likelihood, and lvN\nN, the\ntarget likelihood. To clarify, for instance, if q0is a standard Gaussian, then we\nideally would like lv0\n0≈N(0; 0,I)facilitating importance sampling at this step.\nWe achieve this by drawing inspiration from Corenflos et al. (2025) and Dou\nand Song (2024), to construct an auxiliary path of observations correlated\nwith the diffusion model. The intermediate likelihood will then reflect this\ncorrelation at all steps for SMC samplers to leverage.\nWe define a new process {Yk}N\nk=0initialised with the given observa-\ntionY0=y≡y0, and simulate the path {Yk}N\nk=0by a noising transition\npk|k−1(yk|yk−1).1Write its time-reversal as {Vk=YN−k}N\nk=0. We design the\ntwisting function as a time-pushforward of the target likelihood:\n(9) lvN−k\nN−k(xk) =pk(yk|xk)\nwhichstandsforanintermediatelikelihoodfunctionin xkundertheobservation\nYk=yk. Clearly, the terminal constraint lvN\nN(·) =p0(y|·) =f(y|·)is\nsatisfied. Moreover, it is more suitable compared to the canonical construction\nin Equation (7)for weighting samples at step k, as it smoothly interpolates\nfrompN(yN|·)tof(y|·). This facilitates better importance weighting at\nintermediate steps. The remaining blocker is how to compute the likelihood\npk(yk|xk)which, like the canonical twisting, is generally intractable too.\nHowever, it turns out that if the target likelihood fis linear Gaussian, then\nwe can easily approximate it with a recursion kernel as we describe next.\n1For simplicity, we let the noising notation for ybe the same as in Equation (1)\nforx, but they do not need to be the same in practice.\n--- Page 7 ---\nGenerative diffusion posterior sampling 7\n3.1. Construction with Gaussian likelihood models\nRecall from Equation (9)that our goal is to efficiently approximate the twisting\npk(yk|xk)given by\n(10)pk(yk|xk) =/integraldisplay\npk(yk|y0)f(y0|x0)p0(x0|xk) dy0dx0,\nwhich is in general intractable mainly due to the denoising p0(x0|xk). It seems\nthat the same problem of the canonical twisting in Equation (7)persists, as\nexisting approximations to p0(x0|xk)will result in large errors due to the\ntime leap from kto0. However, the advantage of this twisting is that we\ncan alternatively obtain pk(yk|xk)based onpk−1(yk−1|xk−1)for which we\ncan then recursively compute pk(yk|xk)starting at the target likelihood\np0(y0|x0) =f(y0|x0). This is shown in the following proposition.\nProposition 3.1. For anyk= 0,1,..., the intermediate likelihood\n(11) pk(yk|xk) =Kk(f)(yk,xk)\nis given by k-times applications of an operator K:(Rc×Rd→R+)→(Rc×\nRd→R+)onf, defined by\nK(ψ)(yk,xk):=/integraldisplay\npk(yk|yk−1)ψ(yk−1,xk−1)pk−1(xk−1|xk) dyk−1dxk−1.\nProof.We can obtain pk(yk|xk) =/integraltext\npk(yk,yk−1,xk−1|xk) dyk−1dxk−1=/integraltext\npk(yk|yk−1,xk−1,xk)pk−1(yk−1,xk−1|xk) dyk−1dxk−1=/integraltext\npk(yk|yk−1)×\npk−1(yk−1|xk−1)pk−1(xk−1|xk) dyk−1dxk−1if we know pk−1(yk−1|xk−1).\nThe result is concluded by iteratively applying the recursion.\nRemark3.1. If the observation noising process pk−1(yk|yk−1)isϕ-stationary:/integraltext\npk−1(yk|yk−1) dϕ(yk−1) =ϕ(yk), thenK(ϕ)(y,·) =ϕ(y)is a fixed point.\nThe resulting twisting function will be approximately a standard Normal for\nlargeNif the noising is an OU process.\nRecall that the canonical twisting in Equation (7)requires globalap-\nproximations to p0(x0|xk)between 0andk, which is hard. In contrast, the\ninterpolating twisting in Equation (11)only needs localapproximations be-\ntween continuum k−1andk, significantly reducing complexity and improving\nnumerical robustness in practice. We show a particular case when the target\nlikelihood is linear Gaussian for how to approximate Kk(f)in closed form.\n--- Page 8 ---\n8 Zheng Zhao\nLet the denoising process be defined by\n(12) pk−1(xk−1|xk) = N(xk−1;rk(xk),Ck),\nwhererk:Rd→Rdis a non-linear denoising function (e.g., the sampling\nfunction that contains the noise predictor in DDPM, Ho et al., 2020), and\nCk∈Rd×dis a covariance matrix. With a slight abuse of notation, this is\nequivalent to qN−k+1|N−k(uN−k+1|uN−k)in earlier sections but differs only\nintermsofwhetherthetimeflowsforwardorbackward.Assumethatthetarget\nlikelihoodf(y|x) = N(y;Hx+b,R)is a Gaussian with operator H:Rd→Rc,\nbiasb∈Rc, and covariance R∈Rc×c. Further, choose the auxiliary process\npk|k−1(yk|yk−1) = N(yk;Ak−1yk−1,Σk−1)for some matrix Ak−1∈Rc×cand\ncovariance Σk−1∈Rc×c.2The following result shows that we can approximate\npk(yk|xk)by Gaussian with closed-form mean and covariance.\nProposition 3.2 (Sequential zeroth-order twisting approximation) .At any\nk= 0,1,..., define\n/tildewideK(ψ)(yk,xk):=/integraldisplay\npk(yk|yk−1)ψ(yk−1,xk−1) N(xk−1;xk,Ck) dyk−1dxk−1,\n≈K(ψ)(yk,xk).\nThen/tildewidepk(yk|xk):=/tildewideKk(f)(yk,xk) = N(yk;Fkxk+zk,Ωk), where\nFk=Sk\n0H,\nzk=Sk\n0b,\nΩk=\n\nR, k = 0,\nS1\n0(HC 1HT+R) (S1\n0)T+ Σ 0, k = 1,\nSk\n0/parenleftig\nH/parenleftigk/summationtext\ni=1Ci/parenrightig\nHT+R/parenrightig\n(Sk\n0)T+k−2/summationtext\ni=0Sk\ni+1Σi(Sk\ni+1)T+ Σk−1, k> 1,\nwith semigroup Sn\nm:=/producttextn−1\ni=mAi.\nProof.Supposethat/tildewidepk(yk|xk) = N(yk;Fkxk+zk,Ωk,then/tildewidepk+1(yk+1|xk+1) =\nN(yk+1;Fk+1xk+1+zk+1,Ωk+1)with\nFk+1=AkFk, zk+1=Akzk,\nΩk+1=Ak(FkCk+1FT\nk+ Ωk)AT\nk+ Σk.\n2This needs not to be the same as the noising process in Equation (1).\n--- Page 9 ---\nGenerative diffusion posterior sampling 9\nApplying the equation above recursively starting at F0=H,z0=b, and\nΩ0=Rconcludes the result.\nRemark 3.2. Suppose that Ck,Ak, and Σkare constants, and (A,Σ)leaves\nN(0,I)invariant. Then limk→∞/tildewidepk(yk|xk) = N(0,I)as well, suitable for\nimportance sampling with πref≈N(0,I).\nFrom now on, we will adopt the proposed twisting function\n(13) lvk\nk(·) =/tildewideKN−k(f)(yN−k,·)\nusing the reverse-time notation. Although this twisting is approximate, the\nmarginalqN(·|y)that we are primarily concerned is left un-approximated.\nFurthermore, this twisting function gives us a tractable guided proposal\nMk|k−1mentioned in Remark 2.1 allowing for efficiently sampling further:\nMk|k−1(uk|uk−1)∝lvk\nk(uk)qk|k−1(uk|uk−1)\n= N/parenleftbig\nuk;µ(uk−1),CN−k−DkFN−kCN−k/parenrightbig\n,\nµ(uk−1) =rN−k(uk−1)+Dk(vk−FN−krN−k(uk−1)−zN−k),\nDk=CN−kFT\nN−k/parenleftbig\nFN−kCN−kFT\nN−k+ ΩN−k/parenrightbig−1,(14)\nand as a result the potential function Gkadapts to\n(15) Gk(uk,uk−1) =lvk\nk(uk)qk|k−1(uk|uk−1)\nlvk−1\nk−1(uk−1)Mk|k−1(uk|uk−1).\nWe remark that the matrix inversion in Equation (14)is not a computational\nhurdle for most applications using generative diffusion models: the inversion\ncan be done offline prior to running the SMC sampler, and the dimension does\nnot depend on d. Even if we want to compute Equations (14)and(15)online\nink, the matrix inversions can be fast solved with one eigendecomposition,\nwhen the noising processes’ coefficients are scalar.\nThe final guided SMC sampler using the proposed twisting in Proposi-\ntion 3.2, is summarised in Algorithm 2. We call it B0SMC reflecting how we\nconstruct the twisting with a likelihood bridging, and how we compute it with\na zero-th order approximation.\n4. Experiments\nIn this section we validate the proposed B0SMC sampler in Algorithm 2 on\na high-dimensional ( d= 256) conditional sampling problem. Importantly,\n--- Page 10 ---\n10 Zheng Zhao\nAlgorithm 2: Guided B0SMC for sampling the diffusion posterior\nπ(x|y)∝N(y;Hx+b,R)π(x).\nInputs: Diffusion model for π, observation y, likelihood model H,b, andR,\nnumber of samples J, auxiliary transition matrices {Ak}N\nk=1and\ncovariances{Σk}N\nk=1\nOutputs: Weighted samples {(wj\nN,Uj\nN)}J\nj=1∼π(·|y).\n1Sety0=yand sample y1:N={y1,y2,...,y N}based onpk|k−1(yk|yk−1)\n2Reversevk=yN−kfork= 0,1,...,N\n3Compute{Fk,zk,Ωk}N\nk=1in Proposition 3.2 // Can do online in k\n4UsingMk|k−1andGkin Equation (14) and (15)\n5DrawJsamples{Uj\n0}J\nj=1∼M0\n6Weightwj\n0=G0(Uj\n0)//summationtextJ\ni=1G0(Ui\n0) // Forj= 1,...,J\n7fork= 1,2,...,N do\n8Resample/braceleftbig\n(wk−1,Uj\nk−1)/bracerightbigJ\nj=1if needed\n9DrawUj\nk∼Mk|k−1(·|Uj\nk−1) // Forj= 1,...,J\n10Weightwj\nk=wj\nk−1Gk(Uj\nk,Uj\nk−1) // Forj= 1,...,J\n11Normalisewj\nk=wj\nk//summationtextJ\ni=1wi\nk // Forj= 1,...,J\n12end\nwe focus on gauging how statistically robust the method is against different\nlevels of outlier observation and number of particles. The implementation\nis performed on an NVIDIA A100 80G GPU using JAX (Bradbury et al.,\n2018), and the code is publicly available at https://github.com/zgbkdlm/gfk\nfor reproducibility.\nThe target posterior distribution is defined by a Gaussian mixture model\nπ(x|y)∝N(yω;Hx,R )π(x), π(x) =10/summationdisplay\ni=1γiN(x;mi,Λi),\nyω=Ey∼π(y)[y] +ω(16)\nwhere the mixture weight γi, meanmi∈R256, and covariance Λi∈R256×256,\nand the observation components H∈R1×256andR, are all randomly drawn\naccording to a distribution (see Appendix A for details). The observation yω\nis controlled by an outlier level ωthat moves it away from the most likely\nposition. For this model, the posterior distribution is also a Gaussian mixture\nwith tractable means and covariances. Importantly, we can also obtain a\ntractable diffusion model to precisely target this π, ablating common errors\n--- Page 11 ---\nGenerative diffusion posterior sampling 11\nTable 1: Comparison of sliced Wasserstein distance (SWD) and effective sample\nsize (ESS) at different outlier levels, with the number of particles being 16,384.\nHere we report the mean, and standard deviation in parenthesis. We see that\nB0SMC is consistently the best for all outlier levels.\nOutlier level ω= 0 Outlier level ω= 5Outlier level ω= 10\nSWD ( ↓) ESS ( ↑) SWD ESS SWD ESS\nDPS 1.31 (0.89) N/A 3.04 (1.04) N/A 4.00 ( 1.32) N/A\nTDS 0.12 (0.14) 14440 0.62 (1.02) 14142 0.87 (1.83) 13738\nB0SMC 0.06(0.03)15720 0.25 (0.89)14984 0.68 (1.83) 14626\nsuch as from training a neural network or from assuming large enough N. The\nnoising and denoising models are given by\ndXt=−Xtdt+√\n2 dWt, X 0∼π,\ndUt=Ut+ 2∇logpt(Ut,T−t) dt+√\n2 dBt, U 0∼pT,(17)\nwhere we select T= 2and discretise the SDEs with N= 100steps 0 =t0<\nt1<···<tN=T. The noising marginal pTis also a Gaussian mixture, and\ninitialising the denoising process with it gives UT∼π. The details are also\nshown in Appendix A.\nWe compare to one baseline and two state-of-the-art methods for the\nconditional diffusion sampling. The baseline method is diffusion posterior\nsampling (DPS) developed by Chung et al. (2023), a common and practical\nsampler for diffusion inverse problems. The two state-of-the-art methods are\nTDS by Wu et al. (2023) and MCGDiff by Cardoso et al. (2024), both are based\non Algorithm 1 but with different twisting approximations for Equation (7).\nNote that for MCGDiff we use its noiseless version for ablation comparison,\ntherefore we set R= 10−8for our B0SMC when comparing to MCGDiff.\nThe methods’ performance is measured by sliced Wasserstein distance (SWD,\nwith 1-norm and 1,000 projections) and effective sample size (ESS). Unless\notherwise mentioned, all experiments are independently repeated for 100 times\nand we report the statistics of the results.\nTable 1 shows the results compared to DPS and TDS. A first and crucial\nobservation from the results is that the proposed method B0SMC outperforms\nDPS and TDS by an order of magnitude. This conclusion holds consistently\nover different outlier levels. Even when the outlier level ω= 0is none, B0SMC\nstill works significantly better than TDS, possibly thanks to the tractable\nMarkov proposal in Equation (14). The second but side observation, is that\n--- Page 12 ---\n12 Zheng Zhao\nTable 2: Comparison between B0SMC and MCGDiff with different number of\nparticles. We see that B0SMC is consistently the best for all settings in terms\nof mean and standard deviation (in parenthesis).\nNr. particles 1024 Nr. particles 4096 Nr. particles 16384\nSWD ( ↓) ESS ( ↑) SWD ESS SWD ESS\nMCGDiff 0.55 (0.38) 884 0.38 (0.32) 3543 0.31 (0.40) 14201\nB0SMC 0.44(0.35)970 0.22 (0.16)3883 0.11 (0.07)15531\n0.0 0.5 1.0 1.5 2.0\nt0250050007500100001250015000ESS (max. 16384)ω= 0\nTDS\nB0SMC\n0.0 0.5 1.0 1.5 2.0\ntω= 5\n0.0 0.5 1.0 1.5 2.0\ntω= 10\nFigure 1: Effective sample sizes (ESS) of TDS and B0SMC at one run. We let\nthe SMC samplers to trigger resampling if the ESS goes below 70%. We see\nthat B0SMC triggers substantially less resampling.\nTDS works better than DPS. This aligns with the motivation of TDS which\nessentially used DPS as a proposal and then corrected it with an SMC sampler\nin Algorithm 1.\nComparison to MCGDiff is shown in Table 2. We separate this from\nTable 1 because this is a noiseless observation scenario, and therefore the\nnotion of outlier observation is less defined. In this table, we find that B0SMC\nis significantly better than MCGDiff across different number of particles. The\nstandard deviation of B0SMC is also consistently better, implying a more\nstable sampling process.\nFor more in-depth comparison, we also plot the ESS and time-evolution\nof marginal Qkin Figures 1 and 2. The first figure gives us three observations.\nFirst, we find that in average B0SMC has higher ESS than TDS consistently at\ndifferent outlier levels. Second, B0SMC triggers substantially less resampling,\nimplying that the proposal in Equation (14)is more efficient. Third, the\ninitial ESS at t= 0of TDS is significantly low. This is due to TDS’ twisting\napproximation in Equation (7)which is highly erroneous when T−tis large.\n--- Page 13 ---\nGenerative diffusion posterior sampling 13\n0.000.250.500.751.001.251.501.752.00Timet\n−12.5−10.0−7.5−5.0−2.50.02.5\nx0.00.10.20.30.40.5\nHistogramQtk\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00Timet\n−12.5−10.0−7.5−5.0−2.50.02.55.0\nx0.00.10.20.30.40.5\nHistogramQtk\nFigure 2: Marginal distribution Qkof TDS (left) and B0SMC (right) with\noutlier level ω= 20. The histogram in red represents the true posterior\ndistribution. We see that B0SMC correctly recovers the target at T= 2\nwhereas TDS does not. Importantly, at t= 0pointed by the red arrow, TDS\nis highly degenerate due to its twisting approximation.\nThis problem is also reflected in Figure 2. We see that TDS at t= 0has\ndegenerate samples, and in time, the sampler essentially is moving toward a\nwrong path, and does not recover the true distribution at T. On the other\nhand, the marginal sequence of B0SMC smoothly moves from the reference\ndistribution at t= 0and correctly hits the target distribution at T.\n5. Conclusion\nIn this paper we have developed a new approach for (training-free) conditional\nsampling of generative diffusion models. The method takes any pre-trained\ndiffusion model as prior, and is able to sample the posterior distribution\ngiven any pointwise evaluable likelihood function. The overall framework is\nbased on twisted Feynman–Kac models and sequential Monte Carlo samplers,\nwhere a crucial part of it lies in the construction of the twisting function. As\nsuch, the community, fairly recently, has been working on developing efficient\ntwisting constructions, see, e.g., seminal work by Cardoso et al. (2024); Janati\net al. (2024) and their derivatives. Here, we take one step further, and we\nhave constructed a new twisting sequence that provides better statistical\nperformance when the likelihood part poses challenges for posterior sampling.\nOur empirical results show that the proposed method outperforms the peer\nmethods in terms of less posterior sample approximations and higher effective\nsample size, particularly, when the observation is an outlier.\n--- Page 14 ---\n14 Zheng Zhao\nAcknowledgement\nThis work was partially supported by 1) the Wallenberg AI, Autonomous\nSystems and Software Program (WASP) funded by the Knut and Alice Wallen-\nberg Foundation, and 2) the Stiftelsen G.S Magnusons fond (MG2024-0035). I\nthank the Division of Systems and Control at Uppsala University for providing\ncomputational resources, and Fredrik Lindsten for discussing this paper.\nI also acknowledge that as a paper on generative diffusion models, real\nexperiments to broaden the impact, such as generating cat images, seem to be\nmissing. Verily, I have no excuse for this omission, and defer this to a working\nfuture paper.\nLastly but not least, this work is committed to celebrating the 90th birth-\nday of Thomas Kailath for his contributions in control and signal processing.\nReferences\nAlbergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. (2023). Stochastic\ninterpolants: a unifying framework for flows and diffusions. arXiv preprint\narXiv:2303.08797 .\nBenton, J., Shi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. (2024).\nFrom denoising diffusions to denoising Markov models. Journal of the Royal\nStatistical Society Series B: Statistical Methodology , 86(2):286–301.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin,\nD., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and\nZhang, Q. (2018). JAX: composable transformations of Python+NumPy\nprograms.\nCardoso, G., Janati, Y., Corff, S. L., and Moulines, E. (2024). Monte Carlo\nguided denoising diffusion models for Bayesian linear inverse problems. In\nProceedings of the the 12th International Conference on Learning Represen-\ntations.\nChopin, N. and Papaspiliopoulos, O. (2020). An introduction to sequential\nMonte Carlo . Springer Series in Statistics. Springer.\nChung, H., Kim, J., McCann, M. T., Klasky, M. L., and Ye, J. C. (2023). Dif-\nfusion posterior sampling for general noisy inverse problems. In Proceedings\nof the 11th International Conference on Learning Representations .\nCorenflos, A., Zhao, Z., Särkkä, S., Sjölund, J., and Schön, T. B. (2025).\nConditioning diffusion models by explicit forward-backward bridging. In\n--- Page 15 ---\nGenerative diffusion posterior sampling 15\nProceedings of the 28th International Conference on Artificial Intelligence\nand Statistics , volume 258, pages 3709–3717. PMLR.\nDaras, G., Chung, H., Lai, C.-H., Mitsufuji, Y., Ye, J. C., Milanfar, P., Dimakis,\nA. G., and Delbracio, M. (2024). A survey on diffusion models for inverse\nproblems. arXiv preprint arXiv:2410.00083 .\nDe Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion\nSchrödinger bridge with applications to score-based generative modeling.\nInAdvances in Neural Information Processing Systems , volume 34, pages\n17695–17709.\nDel Moral, P. (2004). Feynman-Kac formulae: genealogical and interacting\nparticle systems with applications . Springer New York.\nDel Moral, P. and Murray, L. M. (2015). Sequential monte carlo with highly in-\nformative observations. SIAM/ASA Journal on Uncertainty Quantification ,\n3(1):969–997.\nDou, Z. and Song, Y. (2024). Diffusion posterior sampling for linear in-\nverse problem solving: a filtering perspective. In Proceedings of the 12th\nInternational Conference on Learning Representations .\nHo, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic\nmodels. In Advances in Neural Information Processing Systems , volume 33,\npages 6840–6851. Curran Associates, Inc.\nJanati, Y., Moufad, B., Durmus, A., Moulines, E., and Olsson, J. (2024).\nDivide-and-conquer posterior sampling for denoising diffusion priors. In\nAdvances in Neural Information Processing Systems , volume 37, pages\n97408–97444. Curran Associates, Inc.\nKelvinius, F. E., Zhao, Z., and Lindsten, F. (2025). Solving linear-Gaussian\nBayesian inverse problems with decoupled diffusion sequential Monte Carlo.\nInProceedings of the 42nd International Conference on Machine Learning\n(ICML).\nLuo, Z., Gustafsson, F. K., Zhao, Z., Sjölund, J., and Schön, T. B. (2023).\nImage restoration with mean-reverting stochastic differential equations. In\nProceedings of the 40th International Conference on Machine Learning ,\nvolume 202, pages 23045–23066. PMLR.\nLuo, Z., Gustafsson, F. K., Zhao, Z., Sjölund, J., and Schön, T. B. (2025).\nTaming diffusion models for image restoration: a review. Philosophical\nTransactions of the Royal Society A: Mathematical, Physical and Engineering\nSciences. In press.\n--- Page 16 ---\n16 Zheng Zhao\nShen, X., Meinshausen, N., and Zhang, T. (2025). Reverse Markov learning:\nMulti-step generative models for complex distributions. arXiv preprint\narXiv:2502.13747 .\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole,\nB. (2021). Score-based generative modeling through stochastic differential\nequations. In Proceedings of the 9th International Conference on Learning\nRepresentations .\nWu, L., Trippe, B. L., Naesseth, C. A., Blei, D., and Cunningham, J. P. (2023).\nPractical and asymptotically exact conditional sampling in diffusion models.\nInICML 2023 Workshop on Structured Probabilistic Inference & Generative\nModeling .\nZhao, Z., Luo, Z., Sjölund, J., and Schön, T. B. (2025). Conditional sampling\nwithin generative diffusion models. Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences . In press.\nAppendix A. Experiment details\nThe coefficients in Equation (16)are generated as follows. The mixture weights\n{γi}10\ni=1are first independently sampled from a Chi-squared (with degree 1)\ndistribution and are then normalised/summationtext10\ni=1γi= 1. Also independently for each\ni, the mixture mean mi∼Unif[−8,8]dfollows a uniform distribution, and the\nmixture covariance Λi=λiλT\ni+Id, whereλi∼Unif[0,1]dis also a uniform\nrandom variable. Let /hatwideH∼N(0,Ic×d),α∼Unif[0,1]c, andβ∼Unif[0,1]c.\nThen we set\nH=Udiag/parenleftbig\nsort(α) + 10−3/parenrightbig\nVT,\nwhereUSVT=/hatwideHis the singular value decomposition of /hatwideH. Finally, the\nobservationcovariance R=ββT+max(α)2Ic×c.WhencomparingtoMCGDiff,\nthe covariance is set to R= 10−8for B0SMC.\nRecall the test diffusion model in Equation (17):\ndXt=aXtdt+bdWt, X 0∼π,\ndUt=aUt+b2∇logpt(Ut,T−t) dt+bdBt, U 0∼pT,\nwhere we chose a=−1andb=√\n2. We choose the auxiliary observation\nprocess to be the same as this noising process, and as a result, Ak=exp(a(tk−\ntk−1))andΣk=b2\n2a/parenleftbig\nexp(2a(tk−tk−1))−1/parenrightbig\n. With the Gaussian mixture π,\nthe reference distribution pT=/summationtext10\ni=1γT\niN(mT\ni,ΛT\ni), wheremT\ni=exp(aT)mi,\n--- Page 17 ---\nGenerative diffusion posterior sampling 17\n−5 0 5 10−505ω= 0TDS\n−5 0 5 10B0SMC\n−5 0 5 10True posterior\n0 2 4 6−505ω= 10\n−5 0 5−5 0 5\nFigure 3: Histogram (2D slice) of the posterior samples drawn by TDS and\nB0SMCatonerun.WeseethatB0SMCcapturesthetrueposteriordistribution\nbetter than TDS when ωis high.\nandΛT\ni=Eidiag(θT\ni)ET\ni, where (Ei,θi)is the eigendecomposition of Λiand\nθT\ni=exp(2aT)θi+b2\n2a/parenleftbig\nexp(2aT)−1/parenrightbig\n. Using the same routine by replacing\nTwitht, the Gaussian mixture marginal ptis available in closed form, so\ndoes its score. We then use an Euler–Maruyama discretisation at 100 steps\n0 =t0<t1<···<tN=Tto simulate the denoising process above.\nFor all SMC samplers we compare here, we use stratified resampling\nwhen the ESS becomes lower than 70%. The number of posterior samples is\nequal to the number of particles, unlike Cardoso et al. (2024); Kelvinius et al.\n(2025) who formed a SMC chain of Jparticles to generate M >Jsamples. If\nwish to do so, the SMC sampler should be replaced with a conditional SMC\nsampler (Corenflos et al., 2025), otherwise biased.\nZheng Zhao\nDivision of Statistics and Machine Learning\nLinköping University, Sweden\nE-mail address: zheng.zhao@liu.se\nReceived 3 January 2022",
  "text_length": 35067
}