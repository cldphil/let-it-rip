{
  "id": "http://arxiv.org/abs/2506.00867v1",
  "title": "Local Manifold Approximation and Projection for Manifold-Aware Diffusion\n  Planning",
  "summary": "Recent advances in diffusion-based generative modeling have demonstrated\nsignificant promise in tackling long-horizon, sparse-reward tasks by leveraging\noffline datasets. While these approaches have achieved promising results, their\nreliability remains inconsistent due to the inherent stochastic risk of\nproducing infeasible trajectories, limiting their applicability in\nsafety-critical applications. We identify that the primary cause of these\nfailures is inaccurate guidance during the sampling procedure, and demonstrate\nthe existence of manifold deviation by deriving a lower bound on the guidance\ngap. To address this challenge, we propose Local Manifold Approximation and\nProjection (LoMAP), a training-free method that projects the guided sample onto\na low-rank subspace approximated from offline datasets, preventing infeasible\ntrajectory generation. We validate our approach on standard offline\nreinforcement learning benchmarks that involve challenging long-horizon\nplanning. Furthermore, we show that, as a standalone module, LoMAP can be\nincorporated into the hierarchical diffusion planner, providing further\nperformance enhancements.",
  "authors": [
    "Kyowoon Lee",
    "Jaesik Choi"
  ],
  "published": "2025-06-01T07:16:39Z",
  "updated": "2025-06-01T07:16:39Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00867v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00867v1  [cs.LG]  1 Jun 2025Local Manifold Approximation and Projection\nfor Manifold-Aware Diffusion Planning\nKyowoon Lee1Jaesik Choi1 2\nAbstract\nRecent advances in diffusion-based generative\nmodeling have demonstrated significant promise\nin tackling long-horizon, sparse-reward tasks\nby leveraging offline datasets. While these ap-\nproaches have achieved promising results, their\nreliability remains inconsistent due to the inherent\nstochastic risk of producing infeasible trajectories,\nlimiting their applicability in safety-critical appli-\ncations. We identify that the primary cause of\nthese failures is inaccurate guidance during the\nsampling procedure, and demonstrate the exis-\ntence of manifold deviation by deriving a lower\nbound on the guidance gap. To address this chal-\nlenge, we propose Local Manifold Approximation\nand Projection (LoMAP), a training-free method\nthat projects the guided sample onto a low-rank\nsubspace approximated from offline datasets, pre-\nventing infeasible trajectory generation. We val-\nidate our approach on standard offline reinforce-\nment learning benchmarks that involve challeng-\ning long-horizon planning. Furthermore, we show\nthat, as a standalone module, LoMAP can be in-\ncorporated into the hierarchical diffusion planner,\nproviding further performance enhancements.\n1. Introduction\nPlanning over long horizons is crucial for autonomous sys-\ntems operating in real-world settings, where rewards are\nsparse and actions often entail delayed consequences (Lee\net al., 2023a). When environment dynamics are fully known,\nmethods such as Model Predictive Control (Tassa et al.,\n2012) and Monte Carlo Tree Search (Silver et al., 2016;\n2017; Lee et al., 2018) have achieved remarkable success.\nIn most practical applications, however, the dynamics are\n1Korea Advanced Institute of Science and Technology\n(KAIST)2INEEJI. Correspondence to: Jaesik Choi <jae-\nsik.choi@kaist.ac.kr >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).not readily available and must be learned from data. Model-\nbased reinforcement learning (MBRL) (Sutton, 2018) ad-\ndresses this by coupling learned dynamics models with plan-\nners, offering increased data-efficiency and the flexibility to\nadapt across tasks. While MBRL offers a promising frame-\nwork, it is vulnerable to adversarial plan exploitation when\nthe learned model is imperfect (Talvitie, 2014; Asadi et al.,\n2018; Luo et al., 2019; Janner et al., 2019; V oelcker et al.,\n2022).\nRecent progress in diffusion models provides an appealing\nalternative for long-horizon planning. Originally introduced\nas powerful generative models that iteratively reverse a mul-\ntistep noising process (Ho et al., 2020; Song et al., 2021),\ndiffusion models have demonstrated state-of-the-art sample\nquality across various domains (Nichol et al., 2022; Luo &\nHu, 2021; Li et al., 2022). Building upon these successes,\nseveral works (Janner et al., 2022; Ajay et al., 2023; Liang\net al., 2023) have leveraged diffusion to model entire tra-\njectories in sequential decision-making tasks. By avoiding\nstep-by-step autoregressive prediction, these approaches re-\nduce error accumulation and naturally capture long-range\ndependencies. Moreover, by leveraging guided sampling\n(Dhariwal & Nichol, 2021), diffusion planners can sample\ntrajectories biased toward high-return behaviors, achiev-\ning notable performance on standard offline reinforcement\nlearning (RL) benchmarks (Fu et al., 2020).\nDespite these advantages, diffusion planners struggle to\nguarantee reliable and feasible plans due to their inherent\nstochasticity . Unlike deterministic models that produce\nconsistent outputs for a given input, diffusion models gen-\nerate probabilistic trajectories. While this enables diverse\nsampling, it introduces the risk of generating physically im-\nplausible trajectories, which are referred to as artifacts in\nvision domains (Bau et al., 2019; Shen et al., 2020). Fur-\nthermore, the reward-guided sampling procedure used by\nunconditional diffusion planners, which jointly defines the\nsampling distribution and the energy function, can be hard\nto estimate accurately (Lu et al., 2023b). This inaccuracy\nmay cause intermediate trajectory samples far away from\nthe underlying data manifold during denoising, ultimately\nproducing infeasible or low-quality trajectories and preclud-\nCodes are available at github.com/leekwoon/lomap .\n1\n--- Page 2 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\ning planners from being useful in safetycritical applications.\nTo address these limitations, recent works (Lee et al., 2023b;\nFeng et al., 2024) introduced trajectory-refinement strate-\ngies to enhance sample quality, though the challenge of\nensuring fully reliable plans remains an open problem.\nIn this paper, we demonstrate that manifold deviation occurs\nduring diffusion sampling by establishing a lower bound on\nthe guidance estimation error. To address this challenge, we\nintroduce LocalManifold Approximation and Projection\n(LoMAP ), atraining-free method that projects guided sam-\nples back onto a low-rank subspace approximated from\noffline datasets. LoMAP operates entirely at test time, re-\nquiring no additional training. At each reverse-diffusion\nstep, it retrieves a few offline trajectories closest to the de-\nnoised version of the current sample, forward-diffuses these\nneighbors, and applies principal component analysis (PCA)\nto span a local low-dimensional subspace. The current\nsample is then projected onto this subspace, thereby signifi-\ncantly reducing off-manifold deviations and improving the\nlikelihood of generating valid behaviors. Because LoMAP\nonly adds a simple projection step after each reward-guided\nupdate, it is easily integrated into existing diffusion planners\nand effectively prevents manifold deviation.\nOur main contributions are as follows: (1)We illustrate\nthe manifold deviation issue in diffusion planners by deriv-\ning a theoretical lower bound on the guidance estimation\nerror. (2)We propose Local Manifold Approximation and\nProjection (LoMAP), a training-free, plug-and-play mod-\nule for diffusion planners that mitigates manifold deviation\nthrough local low-rank projections. (3)We demonstrate the\neffectiveness of LoMAP on standard offline RL benchmarks,\nparticularly in challenging AntMaze task.\n2. Background\n2.1. Problem Setting\nWe consider a Markov decision process (MDP) described\nby the tuple ⟨S,A, P, r, γ ⟩. Here, Sis the state space, A\nis the action space, P:S × A × S → [0,+∞)is the\ntransition model, r:S × A → Ris a reward function, and\nγ∈[0,1]is the discount factor. Given a planning horizon T,\nthe objective of trajectory optimization is to find the action\nsequence a∗\n0:Tthat maximizes the expected return:\na∗\n0:T= arg max\na0:TJ(τ) = arg max\na0:TTX\nt=0γtr(st,at),\nwhere τ= (s0,a0,s1,a1, . . . ,sT,aT)is a trajectory, and\nJ(τ)represents the expected return of the trajectory.2.2. Planning with Diffusion Models\nDiffusion planners (Janner et al., 2022) utilize diffusion\nprobabilistic models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020) to model a trajectory distribution as a Markov chain\nwith Gaussian transitions:\npθ(τ0) =Z\np(τM)MY\ni=1pθ(τi−1|τi) dτ1:M(1)\nwhere p(τM)is a standard Gaussian prior, τ0is a noise-free\ntrajectory, and pθ(τi−1|τi)is a denoising process which is\na learnable Gaussian transition:\npθ(τi−1|τi) =N(τi−1|µθ(τi),Σi). (2)\nThis reverse process inverts a forward process that incre-\nmentally corrupts the data with Gaussian noise according to\na variance schedule {βi}M\ni=1:\nq(τi|τi−1):=N(τi;p\n1−βiτi−1, βiI). (3)\nOne useful property is the ability to directly sample τifrom\nτ0at any diffusion timestep i:\nq(τi|τ0) =N(τi;√αiτ0,(1−αi)I), (4)\nwhere αi:=Qi\ns=1(1−βs). The variance schedule is\ndesigned to respect αM≈0so that τMbecomes close to\nN(0,I).\nDuring the training procedure, rather than directly param-\neterizing µθ, Diffuser trains noise-predictor model ϵθto\npredict the noise ϵthat was added to corrupt τ0intoτi(Ho\net al., 2020):\nL(θ) :=Ei,ϵ,τ0[∥ϵ−ϵθ(τi)∥2], (5)\nwhere τi=√αiτ0+√1−αiϵandϵ∼ N(0,I).\nTrajectory optimization as guided sampling. To gen-\nerate trajectories that have high returns, the energy-guided\nsampling can be considered:\n˜pθ(τ0)∝pθ(τ0) exp (J(τ0)), (6)\nTherefore, Diffuser trains a separate regression network Jϕ\nthat predicts the return J(τ0)based on a noisily perturbed\nversion τi. This is accomplished through the mean-square-\nerror (MSE) objective:\nmin\nϕEi,ϵ,τ0\u0002\n∥Jϕ(τi)− J(τ0)∥2\n2\u0003\n. (7)\nDuring the sampling stage, a classifier guidance (Dhariwal\n& Nichol, 2021) is employed, incorporating gradients of Jϕ\ninto the reverse diffusion process in Eq. (2). Specifically,\nthe mean is updated as follows:\n˜pθ(τi−1|τi) =N(τi−1|µθ(τi) +ωΣig,Σi),(8)\nwhere g=∇τJϕ(τ)|τ=µθ(τi)andωis the guidance scale\nthat controls the strength of the guidance.\n2\n--- Page 3 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\n𝜏𝑖−1\n(b) RGG ( Lee et al., 2023 )\n (a) Diffuser ( Janner et al., 2022 )\n𝜏𝑖𝜏𝑖\n𝜏𝑖−1\n𝜏𝑖−1\n(c) LoMAP (ours)\n𝜏𝑖\n𝒫𝒯𝜏𝑖−1ℳ𝑖−1𝜏𝑖−1\nƸ𝜏0|𝑖−1Manifold\nGuidance\nRefining\nGuidance\nLocal Manifold\nApproximation\nTangent\nSpace\nManifold\nProjection\nFigure 1: A schematic overview of our approach, contrasted with Diffuser (Janner et al., 2022) and RGG (Lee et al., 2023b).\nAs described in Section 3.1, inexact guidance arises in high-dimensional settings, causing deviations from the data manifold.\nRGG addresses this by refining samples via an OOD detection metric but relies on finely tuned guidance steps. In contrast,\nour LoMAP framework projects guided samples back onto a local low-rank subspace (Section 3.2), ensuring that sampling\nremains closer to the data manifold at each diffusion step.\n2.3. Tweedie’s Formula for Denoising\nWhen samples are perturbed by Gaussian noise ˜τ∼\nN(τ, σ2I), Tweedie’s formula (Robbins, 1992) provides a\nBayes-optimal denoised estimate for observations:\nE[τ|˜τ] =˜τ+σ2∇˜τlogp(˜τ). (9)\nwhere p(˜τ):=R\np(˜τ|τ)p(τ)dτ. If we consider a discrete-\ntime diffusion model with perturbation in Eq. (4), we can get\nthe posterior mean by rewriting Tweedie’s fomula (Chung\net al., 2023; 2022):\nE[τ0|τi] =1√αi\u0000\nτi+ (1−αi)∇τilogp(τi)\u0001\n≈1√αi\u0000\nτi−√\n1−αiϵθ(τi)\u0001\n,(10)\nwhere the scaled score function is estimated by ϵθ:\n∇τilogp(τi)≈ −ϵθ(τi)/√1−αi.\n2.4. Low-dimensional Manifold Assumption\nHigh-dimensional trajectory often exhibits intrinsic low-\ndimensional structure. We formalize this through the fol-\nlowing assumption:\nAssumption 2.1. (Low-dimensional Manifold Assumption).\nThe set of clean data M0lies on a k-dimensional subspace\nRkwithk≪d.\nUnder this assumption, recent study (Chung et al., 2022) has\nshown that the set of noisy data τiis inherently concentrated\naround a ( d−k) dimensional manifold Mi.\n3. Manifold-Aware Diffusion PlanningIn this section, we formalize the phenomenon of manifold\ndeviation , a critical limitation of diffusion planners caused\nby inexact guidance (Section 3.1), and propose Local Man-\nifold Approximation and Projection (LoMAP), a training-\nfree method to preserve trajectory feasibility (Section 3.2).\n3.1. Manifold Deviation by Inexact Guidance\nRecall from Equation (6) that diffusion planners aim to sam-\nple trajectories biased toward those that have high returns\nby sampling from the following energy-guided distribution:\n˜pθ(τ0)∝pθ(τ0) exp\u0002\nJ(τ0)\u0003\n,\nwhere pθ(τ0)is the learned trajectory distribution from\nthe diffusion model, and J(τ0)is the (negative) energy or\nreturn function to be maximized.\nFollowing Theorem 3.1 in (Lu et al., 2023b), consider the\nforward process q(τi|τ0). Then the marginal distribution\nat diffusion timestep iis:\n˜pθ(τi) =Z\nq(τi|τ0)˜pθ(τ0) dτ0\n=Z\nq(τi|τ0)pθ(τ0)eJ(τ0)\nZdτ0\n=pθ(τi)Z\nq(τ0|τi)eJ(τ0)\nZdτ0\n=pθ(τi)Eq(τ0|τi)h\neJ(τ0)i\nZ\n∝pθ(τi) exp\u0002\nlogEq(τ0|τi)h\neJ(τ0)i\n| {z }\nJt(τi)\u0003\n.\n3\n--- Page 4 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nAlgorithm 1 Manifold-Aware Diffusion Planning\n1:Require Diffuser µθ, guide Jϕ, covariances Σi, scale ω,\noffline dataset {τ0\nn}N\nn=1, number of neighbors k\n2:while not done do\n3: Observe state s; initialize plan τN∼ N(0,I)\n4: fori=N, . . . , 1do\n5: // parameters of reverse transition\n6: µ←µθ(τi)\n7: // guide using gradients of return\n8: τi−1∼ N\u0000\nµ+ωΣi∇µJMSE\nϕ(µ),Σi\u0001\n9: // manifold projection (LoMAP)\n10: τi−1←LoMAP\u0000\nτi−1;{τ0\nn}, k\u0001\n11: // constrain first state of plan\n12: τi−1\ns0←s\n13: end for\n14: Execute first action of plan τ0\na0\n15:end whileAlgorithm 2 LoMAP (τi−1,{τ0\nn}, k)\n1:Input: Noisy trajectory sample τi−1, offline dataset\n{τ0\nn}N\nn=1, number of neighbors k\n2:Output: Projected sample τi−1\n3:// denoise the current sample\n4:ˆτ0|(i−1)←1√αi−1\u0010\nτi−1−√1−αi−1ϵθ(τi−1)\u0011\n5://knearest neighbors\n6:N ← TopKNeighbors\u0000ˆτ0|(i−1);{τ0\nn}, k\u0001\n7:// forward-diffuse neighbors\n8:forτ0\n(nj)∈ N do\n9:τi−1\n(nj)←√αi−1τ0\n(nj)+√1−αi−1ϵ(nj)\n10:end for\n11:// PCA on the local neighborhood\n12:U←PCA\u0000\n{τi−1\n(nj)}\u0001\n13:// project onto local subspace\n14:τi−1←U U⊤τi−1\n15:return τi−1\nHence, the exact intermediate guidance at diffusion\ntimestep iis given by the gradient of\nJt(τi) = log Eq(τ0|τi)\u0002\nexp(J(τ0))\u0003\n.\nBy injecting this exact guidance into each reverse diffusion\nstep, the sampled τ0exactly follows the desired distribu-\ntion Eq. (6) by rewriting the score of\n∇τilog ˜pθ(τi) =∇τilogpθ(τi)|{z }\n≈−ϵθ(τi)/√1−αi−∇τiJt(τi)|{z}\nguidance(11)\nwhere the first term is approximated by the learned noise-\npredictor model ϵθ.\nGuidance Gap. In practice, however, many existing dif-\nfusion planners (Janner et al., 2022; Liang et al., 2023;\nChen et al., 2024) learn an approximate guidance function\nJMSE\nϕ(τi)via mean-square-error (MSE) objective:\nmin\nϕEi,ϵ,τ0\u0002\n∥Jϕ(τi)− J(τ0)∥2\n2\u0003\n, (12)\nwhere τi=√αiτ0+√1−αiϵandϵ∼ N(0,I).\nHowever, with sufficient model capacity, the optimal Jϕ\nunder the MSE objective satisfies:\nJMSE\nϕ(τi) =Eq(τ0|τi)\u0002\nJ(τ0)\u0003\n≤logEq(τ0|τi)h\neJ(τ0)i\n=Jt(τi),\nwhere the inequality follows from Jensen’s inequality, im-\nplying that JMSE\nϕ underestimates the desired quantity.\nDefinition 3.1. (Guidance gap). Let ∇τiJt\u0000\nτi\u0001\ndenote\nthetrue intermediate guidance at diffusion step i, and let∇τiJMSE\nϕ\u0000\nτi\u0001\nbe the estimated guidance via an MSE-\nbased objective. We define the guidance gap atτiby\n∆guidance\u0000\nτi\u0001\n=\r\r∇τiJt\u0000\nτi\u0001\n− ∇τiJMSE\nϕ\u0000\nτi\u0001\r\r\n2.\n(13)\nTo study how inaccuracies in energy guidance grow with\ndimensionality, we introduce the guidance gap in Eq. (13).\nProposition 3.2 shows that this gap has a lower bound on\nthe order of√\ndin high-dimensional regimes.\nProposition 3.2. (Dimensional scaling of guidance gap.)\nSuppose J(τ0)is not constant. Given the trueguidance\n∇τiJt(τi) =Eq(τ0|τi)h\neJ(τ0)∇τilogq(τ0|τi)i\nEq(τ0|τi)\u0002\neJ(τ0)\u0003 ,\nand the MSE-based guidance\n∇τiJMSE\nϕ(τi) =Eq(τ0|τi)h\nJ(τ0)∇τilogq(τ0|τi)i\n,\nthere exists a choice of τisuch that\n\r\r\r∇τiJt(τi)− ∇τiJMSE\nϕ(τi)\r\r\r\n2≥c√1−αi√\nd,\nfor some constant c >0that does not depend on d.\nProof Sketch. The difference between the true guidance and\nthe MSE-based guidance can be expressed as an expecta-\ntion involving δ(τ0):=\u0010\neJ(τ0)\nE[eJ(τ0)]− J(τ0)\u0011\ntimes the\nforward-process noise ϵ. By Jensen’s inequality, δ(τ0)has\npositive mean, indicating that the MSE-based guidance un-\nderestimates the exponential-weighted return. Exploiting\n4\n--- Page 5 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nthe typical behavior ∥ϵ∥2≈√\ndin high dimensions and\nchoosing τiso that δaligns well with ϵ, we derive the guid-\nance gap scaling on the order of√\nd. For the complete proof,\nsee Appendix A.\nConsequently, as indicated by Proposition 3.2, this issue\nbecomes more severe in scenarios involving long planning\nhorizons and high-dimensional state and action spaces. The\nsubstantial guidance gap forces τi−1to drift from the inter-\nmediate data manifold Mi−1, leading sampled trajectories\naway from the feasible manifold a problem we refer to as\nmanifold deviation . Figure 4 provides empirical evidence\nof this issue.\n3.2. Local Manifold Approximation and Projection\nAs explained in Section 3.1, inaccuracies in the energy-\nguided update can cause the sample τito deviate from\nthe underlying data manifold. To mitigate manifold devia-\ntion caused by inexact guidance, we propose LoMAP - a\ntraining-free method that projects guided samples back to\nthe data manifold through local low-rank approximations.\nThe key insight is that while intermediate diffusion samples\nτimay deviate from the manifold, their denoised estimates\ncan guide local manifold approximation using the offline\ndataset.\nManifold-aware guidance. Given a trajectory sample τi,\nwe sample τi−1with manifold-aware guidance in two steps:\nτi−1∼ N\u0010\nµθ(τi) +ωΣig,Σi\u0011\n, (14)\nτi−1← PTτi−1Mi−1\u0000\nτi−1\u0001\n, (15)\nwhere g=∇τi−1JMSE\nϕ\u0000\nτi−1\u0001\nis the gradient-based guid-\nance term, ωis the guidance scale, and PTτi−1Mi−1denotes\nprojection onto the local manifold. Eq. (14) applies a reward-\nguided shift to sample τi−1, while Eq. (15) projects τi−1\nonto the low-dimensional subspace derived from the offline\ndataset, mitigating drift away from feasible trajectories.\nApproximating the local manifold. We estimate\nTτi−1Mi−1using a local low-rank approximation from\nthe offline dataset of feasible trajectories. To mitigate noise,\nwe first form a denoised surrogate\nˆτ0|i−1=1√αi−1\u0000\nτi−1−p\n1−αi−1ϵθ(τi−1)\u0001\n,\nusing Tweedie’s formula (Eq. 9), where ϵθis the trained\nnoise-prediction network. We then retrieve knearest\nneighbors of ˆτ0|i−1from the clean offline trajectories,\n{τ0\n(nj)}k\nj=1, using cosine similarity in trajectory space fol-\nlowing (Feng et al., 2024). Next, we forward diffuse these\nclean neighbors to timestep i−1:\nτi−1\n(nj)=√αi−1τ0\n(nj)+p\n1−αi−1ϵ(nj),ϵ(nj)∼ N(0,I).\n5 10 15 20 25 30\n# of Plans0.00.20.40.6Artifact Ratio\nMaze2D-Medium\n5 10 15 20 25 30\n# of Plans0.00.20.40.6\nMaze2D-LargeRGG Diﬀuser DiﬀuserPFigure 2: Artifact ratios on Maze2D-Medium (left) and\nMaze2D-Large (right) as the number of sampled plans in-\ncreases. The y-axis denotes the fraction of trajectories that\npass through walls, making them infeasible. Across both\ntasks, our LoMAP augmented DiffuserPconsistently pro-\nduces fewer artifact plans compared to Diffuser and RGG.\nBecause each τi−1\n(nj)remains close to the manifold at\ntimestep i−1, these ksamples approximate the local\nneighborhood Mi−1. We then perform a rank- rPCA on\n{τi−1\n(nj)}k\nj=1to obtain an orthonormal basis U∈Rd×r. The\nmatrix Uspans an r-dimensional subspace that approxi-\nmatesTτi−1Mi−1. Thus,\nPTτi−1Mi−1(z) =U U⊤z,\nwhich retains only the principal directions of variation sup-\nported by the offline data. In practice, r≪d, and we choose\nrby retaining the principal components that explain at least\na fraction λof the total variance. In practice, we find that\nsetting λ= 0.99works well. Pseudocode for the manifold-\naware planning method is provided in Algorithm 1. Notably,\nour LoMAP module is entirely training-free and can be\nreadily integrated into existing diffusion planners by simply\nadding a manifold-projection step after each reward-guided\nupdate. For implementation details, including efficient man-\nifold approximation and projection, see Appendix F.\n4. Experiments\nIn this section, we present experimental results showing that\naugmenting prior diffusion planners with LoMAP improves\nplanning performance across a variety of offline control\ntasks. Specifically, we demonstrate (1)that LoMAP effec-\ntively mitigates manifold deviation and filters out artifact\ntrajectories, (2)that it further enhances planning perfor-\nmance when integrated into diffusion planner, and (3)that\nLoMAP, as a plug-and-play module, can be seamlessly in-\ncorporated into hierarchical diffusion planners, enabling\nsuccessful planning in the challenging AntMaze domain.\nAdditional details regarding our experimental setup and\nimplementation are provided in Appendix E.\n4.1. Mitigating Manifold Deviation\nTo investigate whether LoMAP effectively mitigates mani-\nfold deviation in diffusion-based planners, we apply it to Dif-\n5\n--- Page 6 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\n(a) Diffuser\n (b) RGG\n (c) DiffuserP(ours )\nFigure 3: Visualization of 100 sampled trajectories from\nDiffuser, RGG, and DiffuserPin Maze2D, under a specified\nstart\n and goal\n condition.\nfuser (Janner et al., 2022) and refer to the resulting method\nasDiffuserP. We conduct a quantitative evaluation of man-\nifold deviation in Maze2D tasks by leveraging an oracle\nthat identifies artifact plans , defined as trajectories that\npass through walls and are thus physically infeasible. We\ncompare DiffuserPagainst Diffuser and a baseline variant,\nRestoration Gap Guidance (RGG) (Lee et al., 2023b), in\ntasks where the planner is given only a start and goal loca-\ntion. Specifically, we randomly select start and goal states\nand generate trajectories under increasing sample sizes. For\neach start-goal pair, if at least one sampled trajectory con-\ntains invalid transitions through walls, we mark that pair as\nexhibiting manifold deviation. As shown in Figure 2, Dif-\nfuser produces some infeasible transitions, especially in the\nmore complex Maze2D-Large environment. Although RGG\npartially alleviates this issue, DiffuserPdemonstrates the\nhighest reliability, consistently generating valid trajectories\neven when a larger number of plans are drawn.\nThis phenomenon is further illustrated in Figure 3. Although\nRGG removes many artifact plans, it also reduces the di-\nversity of solutions, clustering trajectories near a narrower\nset of paths. By contrast, DiffuserPmaintains high reliabil-\nity and diversity, producing physically feasible trajectories\nwithout sacrificing coverage of the solution space.\n4.2. Enhancing Planning Performance\nMaze2D. To further demonstrate how LoMAP improves\nplanning performance, we evaluate it on Maze2D environ-\nments (Fu et al., 2020), which involve navigating an agent\nto a target goal location through complex mazes requiring\nlong-horizon planning. Maze2D features two distinct tasks:\na single-task setup where the goal location is fixed, and a\nmulti-task variant (Multi2D) in which the goal is random-\nized at the start of each episode. We compare our methods\nagainst the model-free offline RL algorithm IQL (Kostrikov\net al., 2022) and two trajectory-refinement approaches for\ndiffusion planners, RGG (Lee et al., 2023b) and TAT (Feng\net al., 2024).\nAs shown in Table 1, the model-free IQL suffers a notable\nperformance drop under multi-task conditions, likely due to\nthe challenges of credit assignment. By contrast, diffusion-\nbased planners perform well in both single-task and multi-Table 1: Comparison on Maze2D for DiffuserP, Diffuser,\nand prior methods. DiffuserPdenotes Diffuser augmented\nwith LoMAP. We report the mean and the standard error\nover 1000 planning seeds.\nEnvironment IQL RGG TAT Diffuser DiffuserP\nU-Maze 47.4 108.8 114.5 113.9 126.0±0.26\nMaze2d Medium 34.9 131.8 130.7 121.5 131.0 ±0.46\nLarge 58.6 135.4 133.4 123.0 151.9±2.66\nSingle-task Average 47.0 125.3 126.2 119.5 136.3\nU-Maze 24.8 128.3 129.4 128.9 133.1±0.41\nMulti2d Medium 12.1 130.0 135.4 127.2 129.1 ±0.89\nLarge 13.9 148.3 143.8 132.1 154.7±2.79\nMulti-task Average 16.9 136.4 136.2 129.4 138.9\ntask settings. LoMAP effectively reduces manifold devia-\ntion (Section 4.1) and provides an additional performance\nboost, with DiffuserPachieving the best results on 4 out of 6\ntasks, showing especially strong improvements in Maze2D-\nLarge, which features more complex obstacle maps.\nLocomotion. We next evaluate LoMAP-incorporated\nplanners on MuJoCo locomotion tasks (Fu et al., 2020),\na standard benchmarks for assessing performance on het-\nerogeneous, varying-quality datasets. Our comparison in-\ncludes model-free algorithms (CQL (Kumar et al., 2020),\nIQL (Kostrikov et al., 2022)), model-based algorithms\n(MOPO (Yu et al., 2020), MOReL (Kidambi et al., 2020)),\nand sequence modeling approaches (Decision Transformer\n(DT) (Chen et al., 2021), Trajectory Transformer (TT) (Jan-\nner et al., 2021)). As baseline diffusion planners, we con-\nsider Diffuser (Janner et al., 2022), RGG (Lee et al., 2023b),\nTAT (Feng et al., 2024), and a conditional variant, Decision\nDiffuser (DD) (Ajay et al., 2023).\nAs shown in Table 2, incorporating LoMAP consistently\nboosts average returns of Diffuser across all tasks, with\nparticularly strong gains in the Medium dataset, which\nposes a suboptimal and challenging distribution for learning\nboth the diffusion planner and return estimator. Moreover,\nLoMAP-incorporated planners outperform other trajectory-\nrefinement methods, highlighting the benefits of addressing\nmanifold deviation during sampling.\n4.3. Scaling to Hierarchical Planning in AntMaze\nThe AntMaze tasks (Fu et al., 2020) pose a substantial chal-\nlenge due to high-dimensional state and action spaces, long-\nhorizon navigation objectives, and sparse rewards. Gener-\nating entire trajectories often results in infeasible plans in\nthese environments. A promising approach is to adopt a\nhierarchical scheme, wherein a high-level diffusion planner\nproposes subgoals and a low-level diffusion planner exe-\ncutes short-horizon trajectories to move the agent from one\nsubgoal to the next.\n6\n--- Page 7 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nTable 2: Performance comparison of DiffuserPand various prior methods on MuJoCo locomotion tasks, reported as\nnormalized average returns with corresponding standard errors over 50 planning seeds.\nDataset Environment BC CQL IQL DT TT MOPO MOReL DD TAT RGG Diffuser DiffuserP\nMed-ExpertHalfCheetah 55.2 91.6 86.7 86.8 95.0 63.3 53.3 90.6 92.5 90.8 88.9 91.1 ±0.23\nHopper 52.5 105.4 91.5 107.6 110.0 23.7 108.7 111.8 109.4 109.6 103.3 110.6 ±0.29\nWalker2d 107.5 108.8 109.6 108.1 101.9 44.6 95.6 108.8 108.8 107.8 106.9 109.2 ±0.05\nMediumHalfCheetah 42.6 44.0 47.4 42.6 46.9 42.3 42.1 49.1 44.3 44.0 42.8 45.4 ±0.13\nHopper 52.9 58.5 66.3 67.6 61.1 28.0 95.4 79.3 82.6 82.5 74.3 93.7 ±1.54\nWalker2d 75.3 72.5 78.3 74.0 79.0 17.8 77.8 82.5 81.0 81.7 79.6 79.9 ±1.21\nMed-ReplayHalfCheetah 36.6 45.5 44.2 36.6 41.9 53.1 40.2 39.3 39.2 41.0 37.7 39.1 ±0.99\nHopper 18.1 95.0 94.7 82.7 91.5 67.5 93.6 100 95.3 95.2 93.6 97.6 ±0.58\nWalker2d 26.0 77.2 73.9 66.6 82.6 39.0 49.8 75 78.2 78.3 70.6 78.7 ±2.2\nAverage 51.9 77.6 77.0 74.7 78.9 42.1 72.9 81.8 81.3 81.2 77.5 82.8\nTable 3: Performance comparison of DiffuserP,HDP, and\nprior approaches on AntMaze tasks, reported as normalized\naverage returns with corresponding standard errors over 150\nplanning seeds.\nDataset Env DD RGG Diffuser DiffuserPHD HDP\nPlayMedium 8.0 17.3 6.7 40.7 ±4.342.0 92.7±7.32\nLarge 0.0 12.7 17.3 20.7 ±3.854.7 74.0±6.2\nDiverseMedium 4.0 25.3 2.0 36.0 ±3.778.7 98.0±6.1\nLarge 0.0 17.3 27.3 39.3 ±2.546.0 82.0±5.3\nAverage 3.0 18.2 13.3 34.2 55.3 86.7\nBuilding on this idea, we incorporate LoMAP into both\nDiffuser (Janner et al., 2022) and Hierarchical Diffuser\n(HD) (Chen et al., 2024), yielding DiffuserPand HDP.\nInHDP, a high-level diffusion model augmented with\nLoMAP generates subgoals for each trajectory segment.\nSubsequently, a short-horizon diffusion model (Diffuser)\ntranslates these subgoals into lower-level actions. We com-\npare DiffuserPandHDPagainst standard Diffuser (Janner\net al., 2022), Hierarchical Diffuser (HD) (Chen et al., 2024),\nRestoration Gap Guidance (RGG) (Lee et al., 2023b), and\nDecision Diffuser (DD) (Ajay et al., 2023).\nAs shown in Table 3, DiffuserPimproves upon Diffuser\nacross all AntMaze tasks, demonstrating ability of LoMAP\nto maintain manifold feasibility even in high-dimensional\ncontinuous control. Notably, HDPachieves the best results\non every variant of AntMaze, substantially outperforming\nthe original HD. We attribute these gains largely to the cor-\nrection of manifold deviation during high-level planning by\nLoMAP. In standard HD, subgoals generated by the high-\nlevel planner can sometimes lie off-manifold, forcing the\nlow-level planner to produce infeasible trajectories. As il-\nlustrated in Figure 4, these subgoals frequently pass through\nmaze walls, leading to invalid paths. By applying LoMAP\nto refine them, HDPensures that each proposed subgoal is\nmore feasible for the low-level planner, thereby boosting\noverall success rates. These improvements are especially\n(a) HD\n (b) HDP(ours )\nFigure 4: Visual comparison of generated plans on the\nAntMaze environment. The goal is marked by a red sphere.\nWe plot 20 sampled plans in different colors. (a) shows\nplans generated by Hierarchical Diffuser (HD) (Chen et al.,\n2024), which often produces infeasible trajectories that pass\nthrough walls. (b) demonstrates the results of HD aug-\nmented with LoMAP, which respects the environment ge-\nometry and generates more reliable, feasible trajectories.\npronounced in larger mazes, where longer horizons and in-\ntricate navigation paths make adherence to a valid manifold\nparticularly critical. Consequently, LoMAP serves as a plug-\nand-play component that boosts planning performance even\nin multi-level hierarchical settings.\n4.4. Generating Minority Sample\nThe ability to generate minority data can be critical in\nreal-world scenarios where increasing the diversity of rare-\ncondition examples can improve predictive performance.\nHowever, any minority samples must still align with the true\ndata distribution rather than represent artifacts. To explore\nwhether our method can facilitate the generation of feasible\nminority samples in low-density regions, we adopt minor-\nity guidance (Um et al., 2024), which provides additional\nguidance toward low-density regions.\n7\n--- Page 8 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\n(a) Diffuser\n (b) Diffuser\n+minority guidance\n(c) DiffuserP(ours )\n+minority guidance\nFigure 5: Sampling from low-density regions in Maze2D\nusing minority guidance (Um et al., 2024), given a specified\nstart\n and goal\n condition.\nAs shown in Figure 5, Diffuser alone sometimes fails to\ncapture alternative feasible paths, leading to poor coverage\ndespite viable shortcuts. While minority guidance improves\ncoverage, it also tends to introduce infeasible trajectories.\nIn contrast, LoMAP mitigates this issue by refining these\ntrajectories and ensuring they remain on the valid mani-\nfold. Consequently, combining LoMAP with minority guid-\nance can help uncover feasible yet unexplored solutions\nthat might otherwise remain inaccessible to standard diffu-\nsion planners. Investigating how this approach can further\nenhance planning is a promising direction for future work.\n5. Related Work\nDiffusion Planners in Offline Reinforcement Learning\nDiffusion probabilistic models (Sohl-Dickstein et al., 2015;\nHo et al., 2020) have recently gained prominence in rein-\nforcement learning (RL), particularly in the offline setting.\nBy iteratively denoising samples from noise, these models\nlearn the gradient of the data distribution (Song & Ermon,\n2019), bridging connections to score matching (Hyv ¨arinen\n& Dayan, 2005) and energy-based models (EBMs) (Du &\nMordatch, 2019; Grathwohl et al., 2020). Their expres-\nsive power in modeling complex, high-dimensional data has\nled to applications as planners (Janner et al., 2022; Ajay\net al., 2023), policies (Wang et al., 2023), and data synthe-\nsizers (Lu et al., 2023a; Wang et al., 2025).\nDiffuser (Janner et al., 2022) pioneered the use of diffu-\nsion models for planning by generating entire trajectories,\ndemonstrating notable flexibility in long-horizon tasks. Con-\ncretely, an unconditional diffusion model is trained on of-\nfline trajectories and paired with a separate network that\nestimates returns; this network then guides trajectory sam-\nples toward high-return regions during inference (Dhariwal\n& Nichol, 2021). Extending this framework, Decision Dif-\nfuser (Ajay et al., 2023) applies classifier-free guidance ,\nconditioning the diffusion model directly on reward or con-\nstraint signals and thereby removing the need for a sepa-\nrately trained reward function. Meanwhile, AdaptDiffuser\n(Liang et al., 2023) progressively fine-tunes the diffusion\nmodel with high-quality synthetic data, improving gener-\nalization in goal-conditioned tasks. Beyond these efforts,\ndiffusion models have also been employed in hierarchicalplanning (Li et al., 2023; Chen et al., 2024), multi-task\nRL (He et al., 2023; Ni et al., 2023), and multi-agent set-\ntings (Zhu et al., 2024).\nDespite these advances, diffusion planners remain suscep-\ntible to stochastic failures , occasionally producing trajec-\ntories that deviate from the feasible manifold. Although\nsome works mitigate this issue by refining trajectories post\nhoc (Lee et al., 2023b; Feng et al., 2024), a robust, training-\nfree approach to consistently maintain manifold adherence\nthroughout the sampling process has yet to be established.\nProjections in Diffusion Models Several works in the\nimage-generation domain have introduced projection tech-\nniques to mitigate off-manifold updates during diffusion\nsampling. For instance, MCG (Chung et al., 2022) projects\nmeasurement gradients onto the data manifold in inverse\nproblems, guided by Tweedie’s formula. DSG (Yang et al.,\n2024) replaces the random Gaussian step with a determinis-\ntic update constrained to a hypersphere. This avoids deviat-\ning from the intermediate diffusion manifold, allowing for\nsubstantially larger guidance steps. Meanwhile, MPGD (He\net al., 2024) employs a pre-trained autoencoder to learn the\ndata manifold and projects the sample onto the tangent space\nof the clean data manifold via a pre-trained autoencoder.\nHowever, the performance of MPGD is heavily depends\non the expressive power of the autoencoder. As a result,\nit is difficult to deploy MPGD in diverse offline RL tasks,\nparticularly where pre-trained autoencoders are unavailable.\nIn contrast, our proposed method is entirely training-free ,\nprojecting samples onto both the clean and intermediate\ndiffusion manifolds using only local approximations from\nthe offline dataset.\n6. Conclusion\nIn this work, we investigated the manifold deviation issue\nthat arises in diffusion-based trajectory planning, where\ninaccurate guidance causes sampled trajectories to devi-\nate from the feasible data manifold. To address this, we\nintroduced Local Manifold Approximation and Projection\n(LoMAP), a training-free method that employs local, low-\nrank projections to constrain each reverse diffusion step\nto the underlying data manifold. By ensuring that inter-\nmediate samples remain close to this manifold, LoMAP\nsubstantially reduces the risk of generating infeasible or\nlow-quality trajectories. Empirical results on various of-\nfline RL benchmarks demonstrate the effectiveness of our\napproach. Additionally, LoMAP can be incorporated into\nhierarchical diffusion planning for more challenging tasks\nsuch as AntMaze. Overall, our results establish LoMAP\nas an easily integrable component for diffusion-based plan-\nners, empowering them to consistently remain on the data\nmanifold and thereby providing safer and more robust long-\nhorizon trajectories.\n8\n--- Page 9 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nAcknowledgements\nThis work was supported by Institute of Information & com-\nmunications Technology Planning & Evaluation (IITP) grant\nfunded by the Korea government (MSIT) (No.2019-0-00075,\nArtificial Intelligence Graduate School Program (KAIST);\nNo. 2022-0-00984, Development of Artificial Intelligence\nTechnology for Personalized Plug-and-Play Explanation and\nVerification of Explanation; No. RS-2024-00457882, AI Re-\nsearch Hub Project; No. RS-2024-00509258, AI Guardians:\nDevelopment of Robust, Controllable, and Unbiased Trust-\nworthy AI Technology).\nImpact Statement\nThis paper advances the field of Machine Learning through\na new method for diffusion-based trajectory planning. We\ndo not identify any direct negative societal impacts that\nmust be specifically highlighted. However, we encourage\npractitioners to apply this work responsibly and evaluate\nreal-world safety implications before deployment.\nReferences\nAjay, A., Du, Y ., Gupta, A., Tenenbaum, J. B., Jaakkola,\nT. S., and Agrawal, P. Is conditional generative model-\ning all you need for decision making? In International\nConference on Learning Representations (ICLR) , 2023.\nAsadi, K., Misra, D., and Littman, M. Lipschitz continuity\nin model-based reinforcement learning. In International\nConference on Machine Learning (ICML) , 2018.\nBau, D., Zhu, J.-Y ., Strobelt, H., Zhou, B., Tenenbaum, J. B.,\nFreeman, W. T., and Torralba, A. Gan dissection: Visual-\nizing and understanding generative adversarial networks.\nInInternational Conference on Learning Representations\n(ICLR) , 2019.\nCardoso, G., Idrissi, Y . J. E., Corff, S. L., and Moulines,\nE. Monte carlo guided diffusion for bayesian linear in-\nverse problems. In International Conference on Learning\nRepresentations (ICLR) , 2024.\nChen, C., Deng, F., Kawaguchi, K., Gulcehre, C., and Ahn,\nS. Simple hierarchical planning with diffusion. In Interna-\ntional Conference on Learning Representations (ICLR) ,\n2024.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-\ncision transformer: Reinforcement learning via sequence\nmodeling. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2021.\nChung, H., Sim, B., Ryu, D., and Ye, J. C. Improving\ndiffusion models for inverse problems using manifoldconstraints. In Advances in Neural Information Process-\ning Systems (NeurIPS) , 2022.\nChung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye,\nJ. C. Diffusion posterior sampling for general noisy in-\nverse problems. In International Conference on Learning\nRepresentations (ICLR) , 2023.\nClark, K., Vicol, P., Swersky, K., and Fleet, D. J. Directly\nfine-tuning diffusion models on differentiable rewards. In\nInternational Conference on Learning Representations\n(ICLR) , 2024.\nCo-Reyes, J., Liu, Y ., Gupta, A., Eysenbach, B., Abbeel, P.,\nand Levine, S. Self-consistent trajectory autoencoder: Hi-\nerarchical reinforcement learning with trajectory embed-\ndings. In International Conference on Machine Learning\n(ICML) , 2018.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2021.\nDong, Z., Hao, J., Yuan, Y ., Ni, F., Wang, Y ., Li, P., and\nZheng, Y . Diffuserlite: Towards real-time diffusion plan-\nning. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2024a.\nDong, Z., Yuan, Y ., Hao, J., Ni, F., Ma, Y ., Li, P., and Zheng,\nY . Cleandiffuser: An easy-to-use modularized library\nfor diffusion models in decision making. arXiv preprint\narXiv:2406.09509 , 2024b.\nDouze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G.,\nMazar ´e, P.-E., Lomeli, M., Hosseini, L., and J ´egou, H.\nThe faiss library. arXiv preprint arXiv:2401.08281 , 2024.\nDu, Y . and Mordatch, I. Implicit generation and model-\ning with energy based models. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2019.\nFan, Y ., Watkins, O., Du, Y ., Liu, H., Ryu, M., Boutilier, C.,\nAbbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. Rein-\nforcement learning for fine-tuning text-to-image diffusion\nmodels. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2023.\nFeng, L., Gu, P., An, B., and Pan, G. Resisting stochastic\nrisks in diffusion planners with the trajectory aggregation\ntree. In International Conference on Machine Learning\n(ICML) , 2024.\nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine,\nS. D4rl: Datasets for deep data-driven reinforcement\nlearning. arXiv preprint arXiv:2004.07219 , 2020.\nGrathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D.,\nand Zemel, R. Learning the stein discrepancy for training\n9\n--- Page 10 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nand evaluating energy-based models without sampling. In\nInternational Conference on Machine Learning (ICML) ,\n2020.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. In International\nConference on Machine Learning (ICML) , 2018.\nHao, C., Lin, K., Luo, S., and Soh, H. Language-guided\nmanipulation with diffusion policies and constrained in-\npainting. arXiv preprint arXiv:2406.09767 , 2024.\nHe, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D.,\nZhao, B., and Li, X. Diffusion model is an effective\nplanner and data synthesizer for multi-task reinforcement\nlearning. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2023.\nHe, Y ., Murata, N., Lai, C.-H., Takida, Y ., Uesaka, T., Kim,\nD., Liao, W.-H., Mitsufuji, Y ., Kolter, J. Z., Salakhut-\ndinov, R., and Ermon, S. Manifold preserving guided\ndiffusion. In International Conference on Learning Rep-\nresentations (ICLR) , 2024.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion prob-\nabilistic models. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2020.\nHyv¨arinen, A. and Dayan, P. Estimation of non-normalized\nstatistical models by score matching. Journal of Machine\nLearning Research , 6(4), 2005.\nJanner, M., Fu, J., Zhang, M., and Levine, S. When\nto trust your model: Model-based policy optimization.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) , 2019.\nJanner, M., Li, Q., and Levine, S. Offline reinforce-\nment learning as one big sequence modeling problem.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) , 2021.\nJanner, M., Du, Y ., Tenenbaum, J. B., and Levine, S. Plan-\nning with diffusion for flexible behavior synthesis. In\nInternational Conference on Machine Learning (ICML) ,\n2022.\nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims,\nT. Morel: Model-based offline reinforcement learning.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) , 2020.\nKostrikov, I., Nair, A., and Levine, S. Offline reinforce-\nment learning with implicit q-learning. In International\nConference on Learning Representations (ICLR) , 2022.Kumar, A., Zhou, A., Tucker, G., and Levine, S. Con-\nservative q-learning for offline reinforcement learning.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) , 2020.\nKynk ¨a¨anniemi, T., Karras, T., Laine, S., Lehtinen, J., and\nAila, T. Improved precision and recall metric for assess-\ning generative models. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2019.\nLee, J., Yun, S., Yun, T., and Park, J. Gta: Generative\ntrajectory augmentation with guidance for offline rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2024a.\nLee, K., Kim, S.-A., Choi, J., and Lee, S.-W. Deep rein-\nforcement learning in continuous action spaces: a case\nstudy in the game of simulated curling. In International\nConference on Machine Learning (ICML) , 2018.\nLee, K., Kim, S., and Choi, J. Adaptive and explainable\ndeployment of navigation skills via hierarchical deep rein-\nforcement learning. In 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA) , pp. 1673–1679,\n2023a.\nLee, K., Kim, S., and Choi, J. Refining diffusion planner\nfor reliable behavior synthesis by automatic detection\nof infeasible plans. In Advances in Neural Information\nProcessing Systems , 2023b.\nLee, K. M., Ye, S., Xiao, Q., Wu, Z., Zaidi, Z., D’Ambrosio,\nD. B., Sanketi, P. R., and Gombolay, M. Learning diverse\nrobot striking motions with diffusion models and kine-\nmatically constrained gradient guidance. arXiv preprint\narXiv:2409.15528 , 2024b.\nLi, G., Shan, Y ., Zhu, Z., Long, T., and Zhang, W. Diffstitch:\nBoosting offline reinforcement learning with diffusion-\nbased trajectory stitching. In International Conference\non Machine Learning (ICML) , 2024.\nLi, W. Efficient planning with latent diffusion. In Interna-\ntional Conference on Learning Representations (ICLR) ,\n2024.\nLi, W., Wang, X., Jin, B., and Zha, H. Hierarchical diffusion\nfor offline decision making. In International Conference\non Machine Learning (ICML) , 2023.\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS) , 2022.\nLiang, Z., Mu, Y ., Ding, M., Ni, F., Tomizuka, M., and\nLuo, P. Adaptdiffuser: Diffusion models as adaptive\nself-evolving planners. In International Conference on\nMachine Learning (ICML) , 2023.\n10\n--- Page 11 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nLu, C., Ball, P., Teh, Y . W., and Parker-Holder, J. Syn-\nthetic experience replay. Advances in Neural Information\nProcessing Systems , 2023a.\nLu, C., Chen, H., Chen, J., Su, H., Li, C., and Zhu, J.\nContrastive energy prediction for exact energy-guided\ndiffusion sampling in offline reinforcement learning. In\nInternational Conference on Machine Learning (ICML) ,\n2023b.\nLu, H., Han, D., Shen, Y ., and Li, D. What makes a good\ndiffusion planner for decision making? In International\nConference on Learning Representations (ICLR) , 2025.\nLuo, S. and Hu, W. Diffusion probabilistic models for 3d\npoint cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2021.\nLuo, Y ., Xu, H., Li, Y ., Tian, Y ., Darrell, T., and Ma, T. Algo-\nrithmic framework for model-based deep reinforcement\nlearning with theoretical guarantees. In International\nConference on Learning Representations (ICLR) , 2019.\nLuo, Y ., Mishra, U. A., Du, Y ., and Xu, D. Generative\ntrajectory stitching through diffusion composition. arXiv\npreprint arXiv:2503.05153 , 2025.\nMishra, U. A., Xue, S., Chen, Y ., and Xu, D. Generative\nskill chaining: Long-horizon skill planning with diffusion\nmodels. In Conference on Robot Learning , pp. 2905–\n2925. PMLR, 2023.\nNa, B., Kim, Y ., Park, M., Shin, D., Kang, W., and Moon,\nI.-C. Diffusion rejection sampling. In International Con-\nference on Machine Learning (ICML) , 2024.\nNi, F., Hao, J., Mu, Y ., Yuan, Y ., Zheng, Y ., Wang, B., and\nLiang, Z. Metadiffuser: Diffusion model as conditional\nplanner for offline meta-rl. In International Conference\non Machine Learning (ICML) , 2023.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP., McGrew, B., Sutskever, I., and Chen, M. Glide: To-\nwards photorealistic image generation and editing with\ntext-guided diffusion models. In International Confer-\nence on Machine Learning (ICML) , 2022.\nPark, S., Ghosh, D., Eysenbach, B., and Levine, S. Hiql:\nOffline goal-conditioned rl with latent states as actions.\nInAdvances in Neural Information Processing Systems\n(NeurIPS) , 2023.\nPark, S., Frans, K., Eysenbach, B., and Levine, S. Ogbench:\nBenchmarking offline goal-conditioned rl. In Interna-\ntional Conference on Learning Representations (ICLR) ,\n2025.Prabhudesai, M., Goyal, A., Pathak, D., and Fragkiadaki,\nK. Aligning text-to-image diffusion models with re-\nward backpropagation. arXiv preprint arXiv:2310.03739 ,\n2024.\nRobbins, H. E. An empirical bayes approach to statistics.\nInBreakthroughs in Statistics: Foundations and basic\ntheory , pp. 388–394. Springer, 1992.\nShaoul, Y ., Mishani, I., Vats, S., Li, J., and Likhachev, M.\nMulti-robot motion planning with diffusion models. In\nInternational Conference on Learning Representations\n(ICLR) , 2024.\nShen, Y ., Gu, J., Tang, X., and Zhou, B. Interpreting the\nlatent space of gans for semantic face editing. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2020.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V ., Lanctot, M., et al. Mastering the\ngame of go with deep neural networks and tree search.\nnature , 529(7587):484–489, 2016.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of go without\nhuman knowledge. nature , 550(7676):354–359, 2017.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International Conference on\nMachine Learning (ICML) , 2015.\nSong, J., Zhang, Q., Yin, H., Mardani, M., Liu, M.-Y .,\nKautz, J., Chen, Y ., and Vahdat, A. Loss-guided diffusion\nmodels for plug-and-play controllable generation. In\nInternational Conference on Machine Learning (ICML) ,\n2023.\nSong, Y . and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2019.\nSong, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. In International\nConference on Learning Representations (ICLR) , 2021.\nSutton, R. S. Reinforcement learning: An introduction. A\nBradford Book , 2018.\nTalvitie, E. Model regularization for stable sample roll-\nouts. In Proceedings of the Conference on Uncertainty in\nArtificial Intelligence (UAI) , 2014.\n11\n--- Page 12 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nTassa, Y ., Erez, T., and Todorov, E. Synthesis and stabi-\nlization of complex behaviors through online trajectory\noptimization. In 2012 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems , pp. 4906–4913,\n2012.\nUm, S., Lee, S., and Ye, J. C. Don’t play favorites: Mi-\nnority guidance for diffusion models. In International\nConference on Learning Representations (ICLR) , 2024.\nV oelcker, C., Liao, V ., Garg, A., and Farahmand, A.-\nm. Value gradient weighted model-based reinforcement\nlearning. In International Conference on Learning Rep-\nresentations (ICLR) , 2022.\nWang, R., Frans, K., Abbeel, P., Levine, S., and Efros, A. A.\nPrioritized generative replay. In International Conference\non Learning Representations (ICLR) , 2025.\nWang, Y ., Wang, L., Du, Y ., Sundaralingam, B., Yang, X.,\nChao, Y .-W., Perez-D’Arpino, C., Fox, D., and Shah, J.\nInference-time policy steering through human interac-\ntions. arXiv preprint arXiv:2411.16627 , 2024.\nWang, Z., Hunt, J. J., and Zhou, M. Diffusion policies as an\nexpressive policy class for offline reinforcement learning.\nInInternational Conference on Learning Representations\n(ICLR) , 2023.\nWu, L., Trippe, B., Naesseth, C., Blei, D., and Cunning-\nham, J. P. Practical and asymptotically exact conditional\nsampling in diffusion models. In Advances in Neural\nInformation Processing Systems (NeurIPS) , 2023.\nYang, L., Ding, S., Cai, Y ., Yu, J., Wang, J., and Shi, Y .\nGuidance with spherical gaussian constraint for condi-\ntional diffusion. In International Conference on Machine\nLearning (ICML) , 2024.\nYang, Q. and Wang, Y .-X. Rtdiff: Reverse trajectory syn-\nthesis via diffusion for offline reinforcement learning. In\nInternational Conference on Learning Representations\n(ICLR) , 2025.\nYoon, J., Cho, H., Baek, D., Bengio, Y ., and Ahn, S. Monte\ncarlo tree diffusion for system 2 planning. arXiv preprint\narXiv:2502.07202 , 2025.\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y ., Levine,\nS., Finn, C., and Ma, T. Mopo: Model-based offline\npolicy optimization. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2020.\nZhu, Z., Liu, M., Mao, L., Kang, B., Xu, M., Yu, Y ., Ermon,\nS., and Zhang, W. Madiff: Offline multi-agent learning\nwith diffusion models. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2024.Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al.\nMaximum entropy inverse reinforcement learning. In\nAaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n12\n--- Page 13 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nA. Proofs\nProposition A.1. (Dimensional scaling of guidance gap.) Suppose J(τ0)is not constant. Given the trueguidance\n∇τiJt(τi) =Eq(τ0|τi)h\neJ(τ0)∇τilogq(τ0|τi)i\nEq(τ0|τi)\u0002\neJ(τ0)\u0003 ,\nand the MSE-based guidance\n∇τiJMSE\nϕ(τi) =Eq(τ0|τi)h\nJ(τ0)∇τilogq(τ0|τi)i\n,\nthere exists a choice of τisuch that\n\r\r\r∇τiJt(τi)− ∇τiJMSE\nϕ(τi)\r\r\r\n2≥c√1−αi√\nd,\nfor some constant c >0that does not depend on d.\nProof. By the forward process at in Eq. (4),\nτi=√αiτ0+√\n1−αiϵ,ϵ∼ N(0,I),\nWe have,\nq(τ0|τi) =N\u0010τi\n√αi,1−αi\nαiId\u0011\n,∇τilogq(τ0|τi) =−1√1−αiϵ.\nLet us abbreviate the distribution µ(τ0) :=q(τ0|τi). Then\n∇τiJt(τi) =Eµ\u0002\neJ(τ0)∇τilogµ(τ0)\u0003\nEµ\u0002\neJ(τ0)\u0003 ,∇τiJMSE\nϕ(τi) =Eµ\u0002\nJ(τ0)∇τilogµ(τ0)\u0003\n.\nSubtracting these yields\n∇τiJt(τi)− ∇τiJMSE\nϕ(τi) =Eµh\u0010eJ(τ0)\nEµ[eJ(τ0)]− J(τ0)\u0011\n∇τilogµ(τ0)i\n.\nSince∇τilogµ(τ0) =−1√1−αiϵ, we obtain\n∇τiJt(τi)− ∇τiJMSE\nϕ(τi) =−1√1−αiE\u0002\nδ(τ0)ϵ\u0003\n.\nBy Jensen’s inequality, δ(τ0)has positive mean whenever Jis not constant. Furthermore, as dgrows, we have ∥ϵ∥2on the\norder of√\nd. One can then choose a τiso that δ(τ0)remains well-aligned with ϵ, giving\n∆guidance (τi) =1√1−αi\r\rE\u0002\nδ(τ0)ϵ\u0003\r\r\n2≥c√1−αi√\nd.\nfor some constant c. Thus we can complete the proof.\nB. Limitations\nWhile LoMAP provides a simple yet effective approach for mitigating manifold deviations, it exhibits certain limitations.\nFirst, the current implementation uses cosine distance for manifold approximation, which may not be optimal in very\nhigh-dimensional state spaces, such as pixel-based observations. Developing more robust manifold approximation techniques\nsuitable for complex, high-dimensional environments remains an important direction for future research. For instance,\ncombining LoMAP with latent trajectory embeddings (Co-Reyes et al., 2018) could be a promising approach. Second, our\n13\n--- Page 14 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nmethod inherently encourages sampled trajectories to stay close to the offline data manifold, which may restrict exploration\nof novel behaviors. While our primary focus in this work is ensuring safe and reliable trajectory generation—particularly\nbeneficial for safety-critical offline RL applications—addressing this exploration limitation remains crucial. Integrating\nLoMAP with complementary methods such as trajectory stitching or data augmentation (Ziebart et al., 2008; Li et al.,\n2024; Lee et al., 2024a; Yang & Wang, 2025), which generate diverse synthetic trajectories, could alleviate this issue and\nis an interesting area for future study. Furthermore, exploring how LoMAP could be effectively extended to challenging\nbenchmarks that explicitly require stitching and long-horizon reasoning, such as OGBench (Park et al., 2025), represents an\nintriguing future research direction.\nC. Extended Related Work\nBeyond hierarchical structures (Li et al., 2023; Chen et al., 2024), multi-agent setups (Zhu et al., 2024), and post-hoc\ntrajectory refinement methods (Lee et al., 2023b; Feng et al., 2024), recent diffusion planners have explored integrating\ntree search methods (Yoon et al., 2025), refining trajectory sampling techniques (Dong et al., 2024a), examining critical\ndesign choices to improve robustness (Lu et al., 2025), composing short segments into long-horizon trajectories at inference\ntime (Mishra et al., 2023; Luo et al., 2025), efficient latent diffusion planning (Li, 2024), and inference-time guided\ngeneration (Wang et al., 2024; Lee et al., 2024b; Hao et al., 2024).\nWhile diffusion models have achieved impressive performance on various generative tasks, effectively steering them toward\nspecific objectives remains challenging. Broadly, existing methods for aligning diffusion models can be categorized into\ntwo groups: fine-tuning methods and guidance-based methods. Fine-tuning methods, such as reinforcement learning-based\ntuning (Fan et al., 2023) or direct gradient optimization (Clark et al., 2024; Prabhudesai et al., 2024), directly update\nmodel parameters to maximize target objectives. Despite their effectiveness, these methods tend to excessively focus on\nreward optimization, often compromising the diversity and fidelity of generated outputs (Clark et al., 2024). Conversely,\nguidance-based methods offer a simpler inference-time alternative that preserves the pretrained model distribution. Among\nthese, classifier guidance (Dhariwal & Nichol, 2021) involves training an auxiliary classifier to guide the sampling process\ntoward target conditions, but the additional training overhead can be costly. Recent training-free guidance approaches\ncircumvent this by directly utilizing pretrained classifiers or reward predictors via approximate inference (Chung et al., 2023;\nSong et al., 2023; He et al., 2024). In particular, these methods commonly rely on Tweedie-based denoising (Robbins, 1992),\nwhich provides predictions of clean data given noisy samples. However, inaccuracies inherent to Tweedie’s approximation\nlimit its effectiveness, especially in accurately aligning diffusion samples with target objectives. Sequential Monte Carlo\n(SMC)-based approaches (Wu et al., 2023; Cardoso et al., 2024) address inaccuracies in guidance through principled\nprobabilistic inference. Although these methods provide asymptotic exactness, their practical efficiency under limited\nsampling budgets remains a significant challenge.\nD. Additional Results\nTable 4: Comparison of Realism Scores on Maze2D\ntasks. Higher realism scores indicate samples closer\nto the true data manifold.\nEnvironment Diffuser DiffuserP\nMaze2D U-Maze 1.23 1.30\nMaze2D Medium 1.40 1.56\nMaze2D Large 1.36 1.47Realism score evaluation. To further validate the effectiveness\nof LoMAP, we compute the Realism Score (Kynk ¨a¨anniemi et al.,\n2019), which measures how closely generated trajectories lie to\nthe true manifold defined by the offline dataset. Specifically, we\napproximate the true manifold using k-nearest neighbor (k-NN)\nhyperspheres constructed from 20,000 offline trajectories, and\nevaluate the average realism score over 100,000 sampled trajec-\ntories. As shown in Table 4, applying LoMAP consistently yields\nhigher realism scores compared to diffusion sampling without\nLoMAP, demonstrating that LoMAP effectively produces trajec-\ntories closer to the true data manifold.\nDynamic consistency evaluation. Additionally, we assess trajectory feasibility for MuJoCo locomotion tasks using the\ndynamic mean squared error (Dynamic MSE), defined as:\nDynamic MSE =∥f∗(s,a)−s′∥2\n2,\nwhere f∗represents the true environment dynamics. As shown in Table 5, LoMAP consistently achieves lower Dynamic\nMSE compared to diffusion sampling without LoMAP, clearly indicating improved adherence to the true dynamics of the\n14\n--- Page 15 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nenvironment.\nTable 5: Dynamic MSE comparison on MuJoCo locomotion tasks. Lower Dynamic MSE indicates better adherence to true\nenvironment dynamics.\nEnvironment Diffuser DiffuserP\nhalfcheetah-medium-expert 0.363 0.295\nhopper-medium-expert 0.027 0.020\nwalker2d-medium-expert 0.391 0.293\nhalfcheetah-medium 0.352 0.285\nhopper-medium 0.024 0.021\nwalker2d-medium 0.395 0.293\nhalfcheetah-medium-replay 0.710 0.555\nhopper-medium-replay 0.049 0.045\nwalker2d-medium-replay 0.829 0.506\nAdditional comparison with inference-time guidance methods. We further provide comparative evaluations against re-\ncent inference-time guidance methods, including stochastic sampling (Wang et al., 2024), constrained gradient guidance (Lee\net al., 2024b), and inpainting optimization (Hao et al., 2024).\nFor stochastic sampling (Wang et al., 2024), we adapted goal-conditioning via MCMC sampling, tuning the number of\nsampling steps {2,4,6,8}. To implement constrained gradient guidance (Lee et al., 2024b), we approximated maze walls as\nmultiple spherical constraints following Shaoul et al. (2024), defining a sphere-based cost:\nJc(τ) =MX\nm=1HX\nt=1max ( r−dist(τt,pm),0),\nwhere His the planning horizon, pmthe center of sphere constraints, and rtheir radius. We tuned the guidance scale\nwithin the range {0.001,0.01,0.05,0.1}. To compare with inpainting optimization (Hao et al., 2024), we emulated\na vision-language model (VLM)-based keyframe generation by training a high-level policy using Hierarchical Implicit\nQ-Learning (HIQL) (Park et al., 2023). The policy generated optimal subgoal sequences (keyframes), with k= 25 steps,\naligning with the official implementation provided by ogbench (Park et al., 2025).\nTable 6 presents artifact ratio comparisons. LoMAP consistently achieves the lowest artifact ratio, significantly outperforming\nall inference-time guidance baselines. Even the constrained gradient approach (Lee et al., 2024b), despite explicitly modeling\nmaze walls, performed worse, likely due to gradient-based projections struggling with nonconvex constraints. Both stochastic\nsampling (Wang et al., 2024) and inpainting optimization (Hao et al., 2024) improved over Diffuser but still exhibited higher\nartifact ratios than LoMAP.\nTable 6: Artifact ratio comparison with inference-time guidance methods in Maze2D-Large. Lower values indicate fewer\ninfeasible trajectories.\n# of Plans DiffuserP(LoMAP, ours) Diffuser (Wang et al., 2024) (Lee et al., 2024b) (Hao et al., 2024)\n10 0.35 0.50 0.42 0.49 0.43\n20 0.35 0.62 0.44 0.54 0.46\n30 0.38 0.66 0.47 0.61 0.49\nVisual comparisons. We provide additional rollout visualizations. Figure 6 depicts rollouts executed by Diffuser (Janner\net al., 2022) and our DiffuserPon the Hopper-Medium dataset, demonstrating effectiveness even in suboptimal and\nchallenging data distribution. Meanwhile, Figure 7 offers a side-by-side comparison of Diffuser and HDPon AntMaze-\nLarge-Diverse. We observe that, while standard Diffuser frequently produces trajectories that collide with maze walls or\nfail to reach the goal, our hierarchical extension with LoMAP (i.e., HDP) maintains more coherent routes and significantly\n15\n--- Page 16 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nincreases the likelihood of reaching the target (marked by the red sphere). In both examples, projecting intermediate\ndiffusion steps onto a locally approximated manifold substantially mitigates stochastic failures, highlighting the effectiveness\nof our approach for long-horizon, high-dimensional control tasks.\nDiffuser (Janner et al., 2022)\n DiffuserP(ours)\nFigure 6: Visual comparison of rollout trajectories from Diffuser and DiffuserPin the Hopper-Medium task.\nE. Experimental Details\nE.1. Environments\nMaze2D. Maze2D environments (Fu et al., 2020) require an agent to undertake long-horizon navigation, moving toward a\ndistant goal location. The agent accrues no reward unless it successfully reaches the goal, at which point it receives a reward\nof 1. The three available layouts—U-Maze, Medium, and Large—vary in complexity. Further, Maze2D features two distinct\ntasks: a single-task variant with a fixed goal and a multi-task option ( Multi2D ), which randomizes the goal location at the\nbeginning of each episode. A summary of key details can be found in Table 7.\nTable 7: Environment details for Maze2D experiments.\nMaze2D-Large Maze2D-Medium Maze2D-UMaze\nState space S R4R4R4\nAction space A R2R2R2\nGoal space G R2R2R2\nEpisode length 800 600 300\nLocomotion. Gym-MuJoCo locomotion tasks (Fu et al., 2020) serve as widely recognized benchmarks for assessing\nalgorithm performance on heterogeneous datasets of varying quality. The Medium dataset consists of one million sam-\nples gathered from an SAC (Haarnoja et al., 2018) agent trained to roughly one-third of expert-level performance. The\nMedium-Replay dataset contains all experiences accumulated throughout the SAC training process up to that same per-\nformance threshold. Finally, the Medium-Expert dataset is created by combining expert demonstrations and suboptimal\n16\n--- Page 17 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\nDiffuser (Janner et al., 2022)\n HDP(ours )\nFigure 7: Visual comparison of rollout trajectories from Diffuser and HDPin the AntMaze-Large-Diverse task. We plot\neach trajectory at 50-step intervals for clarity. The goal is marked by a red sphere, and 20 rollout trajectories are shown in\ndifferent colors.\ndata in equal amounts. Details on the state/action spaces and episode lengths can be found in Table 8.\nTable 8: Environment details for Locomotion experiments.\nHopper-* Walker2d-* Halfcheetah-*\nState space S R11R17R17\nAction space A R3R6R6\nEpisode length 1000 1000 1000\nAntMaze. AntMaze tasks (Fu et al., 2020) involve guiding an 8-DoF Antrobot through intricate maze layouts using\nMuJoCo for physics simulation. The environment uses a sparse reward structure, granting a reward only upon reaching\nthe designated goal, thus posing a challenging long-horizon navigation problem. Further difficulty arises from the offline\ndataset, which contains numerous trajectory segments that do not succeed in reaching the goal. We measure performance by\nthe success rate of the agent reaching the endpoint. A summary of the key environment details is provided in Table 9.\nTable 9: Environment details for AntMaze experiments.\nAntMaze-*\nState space S R29\nAction space A R8\nEpisode length 1000\nE.2. Implementation Details\nBelow, we summarize the key implementation details and hyperparameters used throughout our experiments:\n•Network architecture. We build on the Diffuser framework (Janner et al., 2022), employing a temporal U-Net with\nrepeated convolutional residual blocks to parameterize ϵθ.\n•Planning horizons. For Maze2D and Multi2D tasks, the planning horizon is 128 in U-Maze, 256 in Medium, and 256\nin Large. For MuJoCo locomotion tasks, the horizon is 32, and for AntMaze it is 64.\n•Diffusion steps. We use 256 steps for the diffusion process in Maze2D//Multi2D Large and Medium, 128 in\nMaze2D//Multi2D U-Maze, and 20 in other environments.\n17\n--- Page 18 ---\nLocal Manifold Approximation and Projection for Manifold-Aware Diffusion Planning\n•Guidance scales. For AntMaze tasks, we select the guidance scale ωfrom the set {5.0,3.0,1.0,0.1,0.01,0.001}. In\nMuJoCo locomotion tasks, we select ωfrom{0.3,0.2,0.1,0.01,0.001,0.0001}during planning.\n•Local manifold approximation. We tune the number of neighbors k∈ {5,10,20}in our local manifold approximation\nprocedure.\n•Hierarchical Diffuser in AntMaze. For the high-level and low-level planners, we follow Chen et al. (2024) and train\neach component separately using trajectory segments randomly sampled from the D4RL offline dataset. Specifically,\nthe high-level planner generates state-space trajectories with a planning horizon of 226 and temporal jumps of 15.\nDuring execution, the corresponding actions are inferred through a learned inverse dynamics model (Ajay et al., 2023).\nF. Practical Implementation\nManifold approximation. A straightforward k-nearest-neighbor retrieval from the entire offline dataset at each diffusion\nstep can be prohibitively expensive. To mitigate this cost, we employ an inverted file (IVF) index from the Faiss library (Douze\net al., 2024), which partitions the dataset into a set of coarse centroids and restricts each query to only a few relevant clusters.\nConcretely, IVF uses k-means to learn nlistcentroids {c1, . . . ,cnlist}across the dataset of dimension d. Each data point\nxis mapped to its nearest centroid, forming an inverted list. A query vector qis matched to its closest centroids, after\nwhich the search proceeds solely within the corresponding clusters. This design significantly reduces the number of distance\ncomputations relative to an exhaustive linear scan. Although coarse clustering can introduce minor inaccuracies, we have\nfound this approach to be effective in our experiments.\nIn LoMAP, the IVF-based approximate neighbor search operates on a denoised sample ˆτ0|i, retrieving up to kneighbors from\nthe offline dataset. We then forward-diffuse these neighbors to timestep iand perform a rank- rprincipal-component analysis\nto approximate the local manifold of feasible trajectories. Projecting τionto this manifold helps correct off-manifold drift\ncaused by inexact guidance. Crucially, limiting the search to relevant clusters enables LoMAP to handle large datasets and\nhigh-dimensional state-action spaces, making it practical for long-horizon offline reinforcement-learning tasks.\nManifold projection. We observed that applying manifold projection selectively, rather than uniformly across all diffusion\nsteps, can significantly reduce computational costs and even enhance overall performance. Specifically, we found that\nprojection is particularly beneficial when applied during intermediate to later stages of the reverse diffusion process. We\nhypothesize two main reasons for this phenomenon: first, Tweedie-based denoisers inherently exhibit biases toward majority\nor high-density features, as noted by Um et al. (2024), making early stage projections less impactful. Second, the discrepancy\nbetween the learned reverse transitions of the diffusion model and the true data distribution becomes most pronounced\nat intermediate diffusion steps, consistent with observations reported in prior studies (Na et al., 2024). Consequently,\nconcentrating projection efforts on these critical stages not only improves computational efficiency but also effectively\nmitigates manifold deviation, resulting in improved sampling quality.\nG. Baseline Performance Sources\nG.1. Maze2D\nThe reported IQL scores come from Table 1 in Janner et al. (2022), RGG scores from Table 2 in Lee et al. (2023b), and TAT\nscores from Table 2 in Feng et al. (2024).\nG.2. Locomotion\nWe obtain scores for BC, CQL, and IQL from Table 1 in Kostrikov et al. (2022); DT from Table 2 in Chen et al. (2021); TT\nfrom Table 1 in Janner et al. (2021); MOPO from Table 1 in Yu et al. (2020); MOReL from Table 2 in Kidambi et al. (2020);\nand Diffuser from Table 2 in Janner et al. (2022). Scores for RGG and TAT are drawn from Table 3 in Feng et al. (2024),\nwhile DD scores come from Table 1 in Ajay et al. (2023).\nG.3. AntMaze\nScores for DD in the AntMaze domain are taken from Table 1 in Dong et al. (2024b).\n18",
  "text_length": 69880
}