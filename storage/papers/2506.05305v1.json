{
  "id": "http://arxiv.org/abs/2506.05305v1",
  "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback",
  "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.",
  "authors": [
    "Deepak Pandita",
    "Tharindu Cyril Weerasooriya",
    "Ankit Parag Shah",
    "Christopher M. Homan",
    "Wei Wei"
  ],
  "published": "2025-06-05T17:52:30Z",
  "updated": "2025-06-05T17:52:30Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05305v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05305v1  [cs.CL]  5 Jun 2025ProRefine: Inference-time Prompt Refinement with\nTextual Feedback\nDeepak Panditaa,b,1, Tharindu Cyril Weerasooriyab,1, Ankit Parag Shahb, Christopher M. Homanaand\nWei Weib\naRochester Institute of Technology\nbCenter for Advanced AI, Accenture\nAbstract. Agentic workflows, where multiple AI agents collabo-\nrate to accomplish complex tasks like reasoning or planning, are be-\ncoming increasingly prevalent. However, these workflows often suf-\nfer from error propagation and sub-optimal performance, largely due\nto poorly designed prompts that fail to effectively guide individual\nagents. This is a critical problem because it limits the reliability and\nscalability of these powerful systems. We introduce ProRefine, an in-\nnovative inference-time prompt optimization method that leverages\ntextual feedback from large language models (LLMs) to address this\nchallenge. ProRefine dynamically refines prompts for multi-step rea-\nsoning tasks without additional training or ground truth labels. Eval-\nuated on five benchmark mathematical reasoning datasets, ProRefine\nsignificantly surpasses zero-shot Chain-of-Thought baselines by 3 to\n37 percentage points. This approach not only boosts accuracy but\nalso allows smaller models to match the performance of larger ones,\nhighlighting its potential for efficient and scalable AI deployment,\nand democratizing access to high-performing AI.\n1 Introduction\nA critical condition for the advancement of artificial intelligence (AI)\nis that AI outputs align with human values [10]. Human feedback, a\nscarce and subjective resource [11, 14, 26, 44], plays a vital role.\nLLMs, trained on vast datasets, utilize alignment techniques to gen-\nerate more human-like and accurate responses [16, 30, 37]. Tradi-\ntional methods employ Reinforcement Learning from Human Feed-\nback (RLHF) [5], with Proximal Policy Optimization (PPO) [33] be-\ning a popular choice for fine-tuning LLMs.\nRecent research has explored using LLMs as judges and evalua-\ntors [3, 4, 17, 20, 43, 49]. Building on use of LLMs to judge outputs,\narecompound systems that use LLMs to generate feedback / cri-\ntiques [27, 31]. Akyurek et al. [1] and Wadhwa et al. [42] explore\nagentic frameworks to detect, critique, and refine tasks that require\nfactual correctness. Another approach is to iteratively refine output\nusing self-generated feedback [21]. TextGrad [48] introduced auto-\nmatic “differentiation” via text, using textual feedback to optimize\nthe performance of individual components within a compound AI\nsystem.\nOur work focuses on optimizing the prompt , a key element\nin chain-of-thought (CoT) [45] based LLM reasoning. CoT mim-\nics human problem-solving by breaking down complex tasks into\n1Corresponding Authors. Email: deepak@mail.rit.edu or\nt.weerasooriya@accenture.com\nInitial\nPrompt (p)\nLLM task\nQuery (q)LLM feedback LLM optimizer\nOutput (o)Feedback (f)\nUpdated\nprompt (p*)Figure 1. Overview of the ProRefine system, illustrating the iterative\nprocess of prompt optimization using feedback from LLMs in an agentic\nworkflow.\nsmaller, manageable steps. The system we introduce here, ProRe-\nfine (Inference-time Prompt Refine ment with Textual Feedback),\nbuilds upon CoT by adaptively improving prompts using feedback\n(LLM feedback ) and an optimizer ( LLM optimizer ) to refine prompts\nfor the task-performing LLM ( LLM task). This workflow (Figure 1),\nmotivated by the teacher-student framework [39] where a teacher\nagent guides a student agent to perform a task by providing feed-\nback at intermediate steps, but implemented via LLM interactions\nwithout pre-training, represents a novel approach to adaptive agentic\nreasoning. We explore policy optimization for aligning compound\nAI systems, drawing inspiration from TextGrad and policy gradient\nalgorithms like PPO. This work investigates the following research\nquestions.\nRQ1 Can textual feedback be used to improve the performance of\nLLMs at inference-time?\nRQ2 Does model size impact the ability of LLMs to utilize textual\nfeedback?\nRQ3 What is the impact of using a verifier at inference-time?\nWhile prior work has explored prompt optimization [7, 8, 34, 46],\nthey all require training examples to produce task-specific prompts.\nBy contrast, ProRefine is task agnostic and requires no additional\ntraining or ground-truth labels. ProRefine is an inference-time op-\ntimization method that relies on test-time compute availability and\nthe ability of LLMs to provide and act upon feedback for optimiza-\ntion, see Figure 2 for an example. This offers a valuable alternative\nto solely relying on scaling up the base model size for all queries, es-\npecially in scenarios where the smaller model suffices for most tasks\nbut needs enhancement for specific critical ones.\n--- Page 2 ---\nProRefine’s ability to break complex tasks into smaller steps\nand self-improve using feedback makes it a practical solution for\nmulti-step agentic workflows and resource-constrained environments\nwhere deploying the largest models isn’t feasible, but temporary ac-\ncess to a capable feedback LLM (perhaps via a separate API call)\nis possible for critical tasks. This method is also suitable for black-\nbox LLMs, where the internal parameters are inaccessible and only\nAPI access is available. ProRefine could prove to be crucial in sit-\nuations demanding higher interpretability, where the feedback steps\n(outputs of LLM feedback ) offer insights into the reasoning correc-\ntion process, and applications requiring dynamic adaptation with-\nout retraining/fine-tuning cycles. To demonstrate the effectiveness of\nProRefine, we evaluate our method across five benchmark mathemat-\nical reasoning datasets.\nKey Contributions:\n•We propose a novel method - ProRefine for inference-time prompt\noptimization using textual feedback.\n•We evaluate ProRefine on five datasets: object counting, word\nsorting, grade-school math problem-solving, math word problems,\nand algebraic word problems, and compare our method against\nCoT and TextGrad.\n•We evaluate the importance of using a verifier at inference-time.\n2 Related Work\nProRefine draws inspiration from and contributes to several intercon-\nnected research areas. The performance of LLMs is heavily depen-\ndent on the quality of the prompts they receive. Early efforts in this\ndomain centered on manual prompt crafting [45], a meticulous pro-\ncess of designing effective prompts to elicit desired responses. Rec-\nognizing the limitations and scalability challenges of manual meth-\nods, research has increasingly focused on automatic prompt opti-\nmization.\nSome pioneering automatic methods, such as AutoPrompt [34]\nand RLPrompt [7] , employ gradient-based search and reinforce-\nment learning techniques, respectively. AutoPrompt [34] for in-\nstance, uses gradient-based search to generate prompts for masked\nlanguage models. It reformulates tasks as fill-in-the-blank prob-\nlems, achieving performance comparable to supervised models in\ntasks like sentiment analysis. However, it requires training data\nand gradient access, limiting its applicability to black-box models.\nOther approaches leverage LLMs themselves for prompt generation\n[22, 27, 46, 47, 50]. ProRefine distinguishes itself by operating at\ninference-time without training data or ground truth labels, extend-\ning the concept of Chain-of-Thought prompting [45] by iteratively\nrefining the prompt.\nThere is a substantial and growing body of work exploring the ca-\npacity of LLMs to act as judges or evaluators [3, 4, 17, 20, 41, 43, 49,\n51] of text quality, correctness, and other attributes. This capability\nis crucial for automating various aspects of model development and\ndeployment, from assessing generation quality to providing feedback\nfor model improvement. ProRefine utilizes this concept, employing\nan LLM to provide feedback (a form of evaluation), but crucially\nuses this feedback for optimization and combines in a novel fashion\ninference-time operations with LLM-generated feedback.\nThe idea of LLM self-improvement and iterative refinement is also\nhighly relevant. Self-Refine [21] is a prominent example, where an\nLLM generates both output and feedback, using the latter for re-\nfinement. Other works explore self-critiquing [31] and reinforcement\nLLMtask: To determine the number of vegetables \nLLMfeedback: The output is incomplete, it should directly answer the \nquestion. Add the total count of the vegetables: \"There are 12 \nvegetables.\" \nLLMOptimizer: You will answer a reasoning question. Think step by \nstep. Ensure the answer directly answers the question and provides \na final numerical value. If necessary, include the total count of the \nvegetables in the answer. The last line of your response should be \nof the following format: \\'Answer: $VALUE\\' where VALUE is a \nnumerical value. Include the total count of the vegetables in the \nformat \"There are X vegetables.\" \nLLMtask: To find the total number of vegetables, we need to count \neach type of vegetable \nLLMfeedback: Incorrect assumption. The task is to find the total number \nof individual vegetables, not just count the types. Instead, focus on \ncounting the quantities of each vegetable and summing them up. \nLLMOptimizer: You will answer a reasoning question. Think step by \nstep. To find the total count of individual vegetables, count the \nquantities of each vegetable separately and sum them up. Then, \nprovide the final numerical value. The answer should include the \ntotal count of individual vegetables, not just the types. Include the \ntotal count in the format \"There are X individual vegetables.\" The \nlast line of your response should be of the following format: \n\\'Answer: $VALUE\\' where VALUE is a numerical value. \nLLMtask: To find the total count of individual vegetables, we need to \ncount the quantities of each vegetable separately and sum them \nup.\\n\\n- Let \nLLMfeedback: Incorrect calculation, the output should directly provide \nthe total count of vegetables instead of explaining the process. It \nshould correctly sum up the quantities of each vegetable, which is \n1+1+2+1+1+2+2+3=13 .\nLLMOptimizer: You will answer a reasoning question. To find the total \ncount of individual vegetables, you are given the quantities of each \nvegetable. Directly provide the total count of individual vegetables \nby summing up the quantities. Do not explain the process. Ensure \nthe answer includes the total count in the format \"There are X \nindividual vegetables.\" The last line of your response should be of \nthe following format: \\'Answer: $VALUE\\' where VALUE is a \nnumerical value representing the total count of individual \nvegetables. \nLLMtask: There are 1 + 1 + 2 + 1 + 1 + 2 + 2 + 3 = 13 individual \nvegetables.\\n\\nAnswer: 13 \nLLMfeedback: the output is correct LLMtask:  To determine the number of vegetables, we need to \nidentify the vegetables in the given list. \\n\\n- Lettuce is a \nvegetable.\\n- Celery is a vegetable.\\n- Cauliflower is a vegetable.\\n- \nCarrot is a vegetable.\\n- Onion is a vegetable.\\n- Potato is a \nvegetable.\\n- Cabbage is a vegetable.\\n\\nNow, let's count the \nnumber of vegetables: \\n1. Lettuce\\n2. Celery\\n3. Cauliflower\\n4. \nCarrot\\n5. Onion\\n6. Potato\\n7. Cabbage\\n\\nThere are 7 vegetables \nin total.\\n\\nAnswer: 7 Initial Prompt:  You will answer a reasoning question. Think step by \nstep. The last line of your response should be of the following format: \n'Answer: $VALUE' where VALUE is a numerical value. \nQuery:  I have a lettuce head, a stalk of celery, two yams, a \ncauliflower, a carrot, two onions, two potatoes, and three cabbages. \nHow many vegetables do I have? \n(Answer:  13)Baseline response \n(no ProRefine)  ProRefine Input \n(except for Answer) Figure 2. ProRefine example. Given an input query (which in this case has\ncorrect answer: 13), and an initial prompt, the task model ( LLM task ) gives\nan incorrect answer. Prorefine uses two additional models, LLM feedback\nandLLM optimizer , to iteratively improve the prompt as theLLM task\ngenerates its response. Refining the prompt during generation allows the\nfeedback model to target local regions of the response, providing\nfiner-grained feedback than waiting for the response to complete.\nlearning for critique generation (RL4F) [1], along with various feed-\nback and refinement mechanisms [8, 15, 28, 29, 32, 42]. ProRefine\nshares this spirit but focuses specifically on prompt refinement, mak-\ning it suitable for agentic workflows and black-box LLMs, unlike\ndirect output modification.\nProRefine builds upon the core ideas in TextGrad [48], which\nuses textual feedback for optimization within compound AI sys-\ntems. However, ProRefine applies feedback at intermediate steps for\ninference-time prompt optimization and rapidly evolving tasks where\nfine-tuning is infeasible, a key distinction. Moreover, TextGrad is a\nsupervised fine-tuning method, while ProRefine requires no training\ndata, making its data requirements less onerous. Finally, ProRefine\nbelongs to the broader category of inference-time methods [24, 35],\n--- Page 3 ---\nthat improve LLMs without weight modification, such as multi-agent\ndebate [9]. Inference-time methods aim to improve the performance\nof models by utilizing test-time compute resources. Recent advances\nin understanding reasoning in LLMs [12, 13] also provide context to\nour prompt improvement.\nIn essence, ProRefine uniquely integrates inference-time opera-\ntion, LLM-generated feedback, and prompt optimization to enhance\nmulti-step reasoning in agentic workflows, effectively addressing\nlimitations of prior work related to reliance on training data and ap-\nplicability to black-box models.\n3 ProRefine\nProRefine is an inference-time prompt optimization algorithm that\noptimizes prompts by using textual feedback. ProRefine involves in-\nteractions between three LLMs:\nLLM task: Executes the task based on the current prompt, generat-\ning the initial and subsequent outputs.\nLLM feedback : A model that critiques the LLM task’s output, pro-\nviding detailed feedback on improvements. This model should be\ncapable of providing insightful and accurate critiques [2, 31].\nLLM optimizer : Interprets the feedback and refines the prompt,\naiming for coherent and task-focused improvements. This LLM is\ncrucial for ensuring the prompt evolves effectively.\nProRefine (Algorithm 1) works as follows:\nAlgorithm 1: ProRefine\nInput: Query: q, Initial prompt: p, tokens_per_step: k,\nmax_steps: n, LLMs: LLM task,LLM feedback ,\nLLM optimizer\nOutput: Optimized prompt: p∗\np∗=p\nfori= 1tondo\noi=LLM task(p∗, q)// Generate i∗ktokens\nfi=LLM feedback (q, oi)// Get textual\nfeedback\np∗=LLM optimizer (p∗, fi)// Optimize the\nprompt\nifEOS _token inoithen\nbreak\nreturn p∗// Return final optimized prompt\nInitialization: Start with an initial prompt pfor the task, a query q,\nand parameters defining the generation and optimization process ( k\ntokens per step, nmaximum steps).\nGeneration and Feedback Loop:\n•Generation: UseLLM task to generate an output based on the\ncurrent prompt p∗and query q. This step is limited to ktokens to\ncontrol the granularity of feedback.\n•Feedback: LLM feedback evaluates the generated output oi\nagainst the query qto provide textual feedback fi. This feedback\nencapsulates how the output could be improved, focusing on as-\npects such as accuracy, relevance, or coherence.\n•Optimization: LLM optimizer uses the feedback fito refine the\nprompt p∗. This step involves modifying the prompt to better align\nwith the task requirements or to correct identified deficiencies in\nprevious generations.Termination: The process iterates until either the maximum num-\nber of steps nis reached or an end-of-sequence (EOS) token is de-\ntected in the output, indicating the completion of the task.\n3.1 Unifying Verifier and Feedback\nAt inference time, verifiers play a crucial role in judging model\noutputs [6, 18, 36]. Often trained on extensive corpora, these ver-\nifiers can be outcome-based, assessing the final result’s correct-\nness, or process-based, evaluating the validity of intermediate steps\n[18, 36, 40]. For simplicity in this study, we do not train a bespoke\nverifier; rather, we employ the Llama3.1-70B-instruct model to func-\ntion as both the feedback mechanism ( LLM feedback ) and the veri-\nfier. A smaller model, specifically fine-tuned for these tasks, could\nalso be used. The verifier’s function is to evaluate the initial output\ngenerated by LLM task for each query. If this output is assessed as\nincorrect, the refinement process is triggered; otherwise, the output\nis used directly.\nTo quantify the verifier’s impact, we analyze three distinct sce-\nnarios: ProRefine (verifier) , our standard approach which employs\nLLM feedback to guide refinement; ProRefine (no verifier) , wherein\nthe refinement process operates without verifier input; and ProRe-\nfine (optimal verifier) , guided by a perfect verifier (simulated using\nground-truth labels). This optimal condition reveals the upper bound\nof the refinement loop’s potential. Consequently, the performance\ndifference between ProRefine (verifier) andProRefine (optimal veri-\nfier) underscores the significance of verifier accuracy. It is important\nto note that ProRefine’s methodology does not inherently rely on la-\nbels or optimal verification, despite their use in this specific evalua-\ntion.\n3.2 Implementation Details\nThe granularity and duration of the optimization process are gov-\nerned by two parameters: k, the number of tokens per step, and n,\nthe maximum number of steps. These parameters can be adjusted\nbased on the complexity of the task and the desired output quality.\nFor example, rather than generating feedback every ktokens, we\nmight instead choose to provide feedback after each sentence or\nparagraph, particularly in tasks such as machine translation or\ntext summarization, where larger semantic units may be more\nmeaningful. We run all of our experiments on compute nodes with\na single Nvidia A100 GPU (80GB), 24 core processor, and 220GB\nRAM. For model’s generate function we set the following param-\neters: num_return_sequences=1, do_sample=False,\ntop_p=None, temperature=None .\n4 Experiments and Evaluation\n4.1 Data\nWe evaluate ProRefine on five reasoning tasks, each of which in-\nvolves multi-step reasoning, making them suitable for evaluating\nprompt optimization in agentic workflows: object counting and word\nsorting from the BIG-Bench Hard benchmark [38], grade-school\nmath problem-solving from GSM8K [6], math word problems from\nSV AMP [25], and algebraic word problems from AQUARAT [19].\nWe use the same dataset splits and evaluation as Yuksekgonul et al.\n[48] for object counting, word sorting, and GSM8K. Refer to Table\n1 for dataset statistics.\n--- Page 4 ---\nCoT\nT extGrad ProRefine\n(no verifier)ProRefine(verifier)ProRefine\n(optimal verifier)0.00.20.40.60.81.0Accuracy0.480.62\n0.510.600.670.650.730.75\n0.720.85\n0.730.86\n0.770.890.94Object Counting\nCoT\nT extGrad ProRefine\n(no verifier)ProRefine(verifier)ProRefine\n(optimal verifier)0.110.33\n0.22\n0.190.29\n0.100.61\n0.47\n0.320.53\n0.500.69 0.680.710.86Word Sorting\nCoT\nT extGrad ProRefine\n(no verifier)ProRefine(verifier)ProRefine\n(optimal verifier)0.450.460.640.650.720.81 0.80 0.800.870.90\n0.820.860.840.890.94GSM8K\nCoT\nT extGrad ProRefine\n(no verifier)ProRefine(verifier)ProRefine\n(optimal verifier)0.69 0.680.770.810.86 0.87 0.860.880.900.93\n0.850.840.880.890.94SVAMP\nCoT\nT extGrad ProRefine\n(no verifier)ProRefine(verifier)ProRefine\n(optimal verifier)0.260.31\n0.200.270.350.56\n0.52\n0.340.550.600.59\n0.56\n0.400.610.66AQUARATAccuracy with 95% Confidence Intervals Across Datasets and ModelsModel\nLlama-3.2-1B-it Llama-3.2-3B-it Llama-3.1-8B-itFigure 3. Test Accuracy [with 95% confidence interval] across different models and datasets. Llama3.1-70B-instruct is employed for feedback generation,\nprompt optimization, and evaluation.\nDataset Train Validation Test\nObject Counting 50 100 100\nWord Sorting 50 100 100\nGSM8K 200 300 1319\nSV AMP 2516 622 1000\nAQUARAT 97467 254 254\nTable 1. Datasets used for evaluating ProRefine on multi-step reasoning\ntasks.\n4.2 Experimental Setup\nWe experiment with three models - Llama3.2-1B-instruct ,Llama3.2-\n3B-instruct , and Llama3.1-8B-instruct [23] for LLM task. The\nprompts are optimized using Algorithm 1, with Llama3.1-70B-\ninstruct used for feedback generation, prompt optimization, and eval-\nuation. We select the values of hyperparameters k= 10 andn= 25\nto control the granularity of feedback and duration of optimiza-\ntion. Hyperparameters kandnwere fixed based on general pre-\nliminary exploration and not tuned per task using benchmark train-\ning/validation data.\nWe compare the performance of our method against the zero-shot\nChain-of-Thought (CoT) baseline and TextGrad [48], and report test\naccuracy with 95% confidence interval. We choose TextGrad as a\nbaseline because Yuksekgonul et al. [48] reported performance at par\nor better than DSPy [15] for prompt optimization on object count-\ning, word sorting, and GSM8k datasets. It is essential to remember\nthat TextGrad is a supervised fine-tuning method that utilizes both\nthe training and validation sets. For TextGrad, we use a comparative\nsetup consisting of a task model to be fine-tuned and Llama3.1-70B-\ninstruct model for feedback generation and backpropagation. The re-\nsults are shown in Table 2 and Figure 3.\n4.3 Results\nOur results (Table 2) demonstrate that ProRefine significantly im-\nproves LLM task performance over the zero-shot CoT baseline in\nall but one experiment, and it outperforms TextGrad in 11 out of\n15 cases overall. For Llama3.2-1B-instruct model, ProRefine can\nsignificantly outperform CoT and TextGrad on 2 out of 5 datasets.\nForLlama3.2-3B-instruct model, ProRefine can outperform CoT and\nTextGrad on 3 out of 5 datasets with one significant result. For\nLlama3.1-8B-instruct model, ProRefine can outperform CoT and\nTextGrad on all 5 datasets with 4 significant results.Object Counting ProRefine improves performance by 3−16\npercentage points over CoT, with significant gains observed for\nLlama3.1-8B-instruct . It outperforms TextGrad on 2 out of 3 models,\nyielding a 2−3percentage point advantage. However, a performance\ndrop of 2points is observed for Llama3.2-1B-instruct .\nWord Sorting Performance gains over CoT range from 8−37\npercentage points, with significant improvements for Llama3.2-3B-\ninstruct andLlama3.1-8B-instruct . ProRefine surpasses TextGrad on\n1 of 3 models with a 2-point gain, but performance drops of 11−\n14points are observed for Llama3.2-1B-instruct andLlama3.2-3B-\ninstruct .\nGSM8K ProRefine achieves 2.4−20.4percentage points im-\nprovement over CoT, with significant improvement observed for all\nthe models; however, a slight performance drop ( 1.2) is observed\nforLlama3.2-3B-instruct . It outperforms TextGrad on all models,\nachieving a 2.1−19.1percentage point gain with significant results\nobserved for Llama3.2-1B-instruct andLlama3.2-3B-instruct mod-\nels. Minor performance drop of 0.4−2.1is observed for Llama3.2-\n3B-instruct andLlama3.1-8B-instruct .\nSV AMP Performance improves by 0.9−11.9percentage points\nover CoT, with significant gains for Llama3.2-1B-instruct and\nLlama3.1-8B-instruct . ProRefine outperforms TextGrad across all\nmodels, with 1.7−12.4percentage point gains and significant re-\nsults for Llama3.2-1B-instruct .\nAQUARAT Gains over CoT range from 0.9−2percentage points,\nbut declines of 5.4−22points are also observed. ProRefine exceeds\nTextGrad on 2 of 3 models, with 2.7−4.7percentage point gains,\nthough performance drops of 10.6−18.1points are also recorded.\nOur results demonstrate that using ProRefine with an optimal ver-\nifier significantly improves performance across all tasks, achieving\nthe best results in 13 out of 15 cases, highlighting the critical role\nof verifier quality. Notably, the number of significant improvements\nincreases with larger model sizes.\nWe also observe that ProRefine can improve the performance\nof lesser capable models Llama3.2-3B-instruct and Llama3.1-\n8B-instruct to that of more capable ones Llama3.1-8B-instruct\nandLlama3.1-70B-instruct respectively. In our experiments using\nLlama3.1-70B-instruct forLLM task, some experiments yielded an\naccuracy of 1, suggesting potential overfitting. Additionally, we ex-\nperimented using ProRefine with a comparatively smaller model\n(Llama3.1-8B-instruct ) forLLM feedback andLLM optimizer . How-\never, this configuration did not consistently yield substantial im-\nprovements and, in some cases, even degraded performance. This\n--- Page 5 ---\nDataset Method Llama-3.2 1B-it Llama-3.2 3B-it Llama-3.1 8B-it\nObject CountingCoT 0.48 [0.382, 0.578] 0.65 [0.556, 0.744] 0.73 [0.643, 0.817]\nTextGrad 0.62 [0.524, 0.716] 0.73 [0.643, 0.817] 0.86 [0.792, 0.928]\nProRefine (no verifier) 0.51 [0.412, 0.608] 0.75 [0.665, 0.835] 0.77 [0.687, 0.853]\nProRefine (verifier) 0.6 [0.503, 0.696] 0.72 [0.632, 0.808] 0.89* [0.839, 0.959]\n†ProRefine (optimal verifier) 0.67 [0.577, 0.763] 0.85* [0.780, 0.920] 0.94* [0.893, 0.987]\nWord SortingCoT 0.11 [0.048, 0.172] 0.10 [0.041, 0.159] 0.50 [0.401, 0.598]\nTextGrad 0.33* [0.237, 0.423] 0.61* [0.514, 0.706] 0.69* [0.599, 0.781]\nProRefine (no verifier) 0.22 [0.138, 0.302] 0.47* [0.372, 0.568] 0.68 [0.595, 0.779]\nProRefine (verifier) 0.19 [0.113, 0.267] 0.32* [0.228, 0.412] 0.71* [0.621, 0.799]\n†ProRefine (optimal verifier) 0.29* [0.192, 0.368] 0.53* [0.432, 0.628] 0.86** [0.792, 0.928]\nGSM8KCoT 0.450 [0.423, 0.476] 0.809 [0.787, 0.829] 0.819 [0.797, 0.839]\nTextGrad 0.463 [0.436, 0.489] 0.801 [0.779, 0.822] 0.864* [0.845, 0.882]\nProRefine (no verifier) 0.636** [0.610, 0.662] 0.797 [0.774, 0.818] 0.843 [0.823, 0.863\nProRefine (verifier) 0.654 ** [0.627, 0.678] 0.866 ** [0.847, 0.883] 0.885 * [0.868, 0.902]\n†ProRefine (optimal verifier) 0.725** [0.701, 0.749] 0.904** [0.888, 0.920] 0.936** [0.922, 0.949]\nSV AMPCoT 0.689 [0.66, 0.718] 0.869 [0.848, 0.890] 0.854 [0.832 , 0.876]\nTextGrad 0.684 [0.655, 0.713] 0.861 [0.840, 0.882] 0.84 [0.817, 0.863]\nProRefine (no verifier) 0.774** [0.748, 0.800] 0.878 [0.858, 0.898] 0.877 [0.857, 0.897]\nProRefine (verifier) 0.808 ** [0.784, 0.832] 0.896 [0.877, 0.915] 0.893 * [0.874, 0.912]\n†ProRefine (optimal verifier) 0.861** [0.840, 0.882] 0.925** [0.909, 0.941] 0.938** [0.923, 0.953]\nAQUARATCoT 0.259 [0.202, 0.31] 0.563 [0.498, 0.620] 0.586 [0.522, 0.643]\nTextGrad 0.311 [0.250, 0.364] 0.524 [0.462 , 0.585] 0.559 [0.494, 0.616]\nProRefine (no verifier) 0.205 [0.151, 0.250] 0.343 [0.284, 0.401] 0.398 [0.337 , 0.458]\nProRefine (verifier) 0.268 [0.209, 0.318] 0.551 [0.486 , 0.608] 0.606 [0.542, 0.663]\n†ProRefine (optimal verifier) 0.354 [0.292, 0.409] 0.598 [0.538, 0.659] 0.657 [0.595, 0.712 ]\nTable 2. Test Accuracy with 95% confidence intervals across five benchmark datasets and models. * and ** denote statistically significant improvements over\none or two baseline methods, respectively. Results in bold indicate the highest accuracy for a dataset-method combination.†demonstrates the upper bound\npotential of the optimization loop and the impact of verifier quality. Llama3.1-70B-instruct is employed for feedback generation, prompt optimization, and\nevaluation.\naligns with prior findings indicating that smaller models are less ca-\npable of providing high-quality feedback [31, 2].\nObject CountingWord SortingGSM8K SVAMP\nAQUARAT0510152025Avgerage #iterationsAverage length of refinement loop\nModel\nLlama-3.2-1B-it\nLlama-3.2-3B-it\nLlama-3.1-8B-it\nFigure 4. Average number of prompt refinement iterations.\n5 Discussion\nRegarding RQ1, the results demonstrate that ProRefine is a broadly\napplicable method utilizing textual feedback for improving LLM per-\nformance at inference-time. The “performance gap bridging” effect\nis particularly noteworthy, suggesting that ProRefine can be an effec-\ntive alternative to simply scaling up model size, potentially avoiding\ncostly fine-tuning, which could prove critical in resource-constrained\nsettings.\nThe largest performance gains are observed on the word sorting\ntask, indicating that tasks requiring more complex reasoning or ma-\nnipulation of intermediate outputs benefit the most from ProRefine’s\niterative refinement. The mixed results when using a smaller modelforLLM feedback illustrate the importance of “knowledge asymme-\ntry,” i.e., that the feedback model should be “sufficiently capable” of\nproviding useful critiques.\nRegarding RQ2, the results indicate that ProRefine outperforms\nthe baselines on 2 and 3 datasets when using the Llama3.2-1B-\ninstruct andLlama3.2-3B-instruct models, respectively, and on all\n5 datasets when using the Llama3.1-8B-instruct model. This sug-\ngests that performance improvements scale with model size. These\nfindings imply that larger models are preferable to smaller ones, par-\nticularly in agentic workflows that may require test-time scaling and\nthe effective use of textual feedback to solve complex tasks.\nRegarding RQ3, the results highlight that employing a high-\nquality verifier is crucial for significantly improving task perfor-\nmance at inference time. Beyond enhancing performance, the verifier\nalso reduces computational cost during inference by guiding the re-\nfinement process. Moreover, it opens up promising avenues for future\nwork, where an optimizer could be designed to maximize rewards\nguided by the verifier’s assessments.\nProRefine can offer a degree of interpretability by exposing the\noutputs from LLM feedback , allowing insights into the model’s rea-\nsoning process. Figures 2 and 5 demonstrate cases where model feed-\nback successfully improves the output and where it fails, respec-\ntively. Although evaluated on reasoning and math tasks, ProRefine\nis general and applicable to other tasks.\nWhile ProRefine improves accuracy, it does increase the compu-\ntational cost during inference. Each step requires calls to both the\ntask LLM and the feedback/optimization LLMs. The trade-off be-\ntween accuracy and computational cost should be considered when\ndeploying ProRefine for test-time scaling. However, the cost is still\nconsiderably lower than full model retraining or fine-tuning. Figure 4\nshows the average number of prompt refinement iterations using our\nstandard approach, ProRefine (Verifier) , for all datasets and models.\nEven though we use the same n= 25 for all datasets and models,\n--- Page 6 ---\nFigure 4 suggests that the granularity of the feedback ( k) and the\nlength of optimization ( n) may be tuned for specific tasks to achieve\nan optimal balance between performance and computational cost.\n6 Limitations and Future Work\nThis work has the following limitations that we acknowledge have\npotential for future explorations:\n•Computational Cost and Scalability: ProRefine’s iterative\nprocess increases computational cost compared to single-shot\nprompting. Scaling to substantially larger or more complex tasks\npresents both computational and methodological challenges.\n•Generalizability: Evaluation is currently limited to reasoning and\nmathematical tasks. Further research is needed to assess perfor-\nmance across a broader range of NLP tasks and domains.\n•Model Dependence and Feedback Quality: ProRefine relies on\na capable LLM for feedback, which may limit accessibility. The\nquality and potential biases of this feedback are crucial factors\nimpacting performance and require further investigation.\n•Evaluation and Bias: Using an LLM for evaluation introduces\npotential biases. More comprehensive human evaluations and ro-\nbust methods for mitigating evaluator bias are needed.\n•Hyperparameter and Feedback Loop: Current approach re-\nquires manual hyperparameter tuning, and iterative prompting\nmight cause issues, and automatic ways need further research.\n7 Conclusion\nWe introduced ProRefine, a novel, practical, and inference-time\nprompt optimization method for agentic workflows. ProRefine lever-\nages LLM-generated textual feedback to dynamically refine prompts,\nleading to significant performance improvements without requiring\nadditional training or ground-truth labels. The results across diverse\ndatasets demonstrate ProRefine’s effectiveness for multi-step rea-\nsoning tasks and its ability to bridge the performance gap between\nsmaller and larger LLMs. The inference-time nature makes ProRe-\nfine readily deployable and adaptable, contributing to more efficient,\ntransparent, and accessible AI systems.\nReferences\n[1] A. F. Akyurek, E. Akyurek, A. Kalyan, P. Clark, D. T. Wijaya, and\nN. Tandon. RL4F: Generating natural language feedback with rein-\nforcement learning for repairing model outputs. In A. Rogers, J. Boyd-\nGraber, and N. Okazaki, editors, Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pages 7716–7733, Toronto, Canada, July 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.acl-long.427. URL\nhttps://aclanthology.org/2023.acl-long.427/.\n[2] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\nA. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harm-\nlessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022.\n[3] A. Bavaresco, R. Bernardi, L. Bertolazzi, D. Elliott, R. Fernández,\nA. Gatt, E. Ghaleb, M. Giulianelli, M. Hanna, A. Koller, A. F. T. Mar-\ntins, P. Mondorf, V . Neplenbroek, S. Pezzelle, B. Plank, D. Schlangen,\nA. Suglia, A. K. Surikuchi, E. Takmaz, and A. Testoni. Llms instead\nof human judges? a large scale empirical study across 20 nlp evaluation\ntasks, 2024. URL https://arxiv.org/abs/2406.18403.\n[4] C.-H. Chiang and H.-y. Lee. Can large language models be an al-\nternative to human evaluations? In A. Rogers, J. Boyd-Graber, and\nN. Okazaki, editors, Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\npages 15607–15631, Toronto, Canada, July 2023. Association for Com-\nputational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL\nhttps://aclanthology.org/2023.acl-long.870/.\nLLMtask: To sort the given words alphabet \nLLMfeedback: Incomplete output. The output should provide the sorted list of words \ninstead of just stating the instruction. \\n\\nCorrect output should list the words in \nalphabetical order, such as: allis, anthology, jacobi, marmot, membrane, oakland, \nseaborg, trapezoidal, toggle \nLLMOptimizer: You will answer a reasoning question that requires sorting a list of \nwords. The input will be a list of words, and you need to think step by step to provide \nthe correct output. Your response should include the sorted list of words in \nalphabetical order. Do not forget to list all the words in the correct order. Think \nstep-by-step and provide the correct output.\\n\\nThe last line of your response should \nbe of the following format: 'Answer: $VALUE' where VALUE is the sorted list of \nwords, with each word separated by a comma. For example: 'Answer: allis, \nanthology, jacobi, marmot, membrane, oakland, seaborg, trapezoidal, toggle' \nLLMtask: To sort the list of words alphabetically, I will go through each word and \nLLMfeedback: This is not the correct output. The task was to sort the list of words \nalphabetically, but the given output starts with an explanation. A correct output should \nbe a direct response with the sorted list of words. \\n\\nHowever, the output did not \nprovide the sorted list. The correct sorted list should be: allis, anthology, jacobi, \nmarmot, membrane, oakland, seaborg, toggle, trapezoidal. \nLLMOptimizer: You will answer a reasoning question that requires sorting a list of \nwords. The input will be a list of words, and you need to sort the list in alphabetical \norder and provide the correct output directly. Do not include any explanations. \n\\n\\nYour response should be a direct list of the sorted words in alphabetical order, \nseparated by commas. Ensure that all words are included in the correct order. \n\\n\\nThe last line of your response should be of the following format: 'Answer: \n$VALUE' where VALUE is the sorted list of words, with each word separated by a \ncomma. \nLLMtask: Answer: allis, anthology, jacobi, list, marmot, membrane, oakland, seaborg, \ntrape \nLLMfeedback: Missing word \"toggle\" in the sorted list. Also, the word \"list\" was not \nsupposed to be included as it was part of the original instruction, not the actual list to \nbe sorted. \\n\\nCorrected sorted list should be: allis, anthology, jacobi, marmot, \nmembrane, oakland, seaborg, toggle, trapezoidal. \nLLMOptimizer: You will answer a reasoning question that requires sorting a list of \nwords. The input will be a list of words, and you need to sort the list in alphabetical \norder and provide the correct output directly. Do not include any explanations. Ensure \nall words from the input list are included in the sorted output.\\n\\nYour response \nshould be a direct list of the sorted words in alphabetical order, separated by \ncommas. The last line of your response should be of the following format: 'Answer: \n$VALUE' where VALUE is the sorted list of words, with each word separated by a \ncomma. \nLLMtask: Answer: allis, anthology, jacobi, list, marmot, membrane, oakland, seaborg, \ntrapezoidal, toggle \nLLMfeedback: \"Correct alphabetization, but \\'list\\' is not part of the original list, remove it to \nmaintain consistency.\" \nLLMOptimizer: You will answer a reasoning question that requires sorting a list of \nwords. The input will be a list of words, and you need to sort the list in alphabetical \norder and provide the correct output directly. Do not include any explanations. \nEnsure all words from the input list are included in the sorted output, and do not add \nany extra words. \\n\\nYour response should be a direct list of the sorted words in \nalphabetical order, separated by commas. The last line of your response should be \nof the following format: 'Answer: $VALUE' where VALUE is the sorted list of words, \nwith each word separated by a comma. Initial Prompt:  You will answer a reasoning question. Think step by step. The last line \nof your response should be of the following format: 'Answer: $VALUE' where VALUE is \na numerical value. \nQuery:  Sort the following words alphabetically: List: oakland seaborg jacobi membrane \ntrapezoidal allis marmot toggle anthology \n(Answer:  allis anthology jacobi marmot membrane oakland seaborg toggle trapezoidal) \nLLMtask:  To sort the given words alphabetically, we need to compare each \nword and arrange them in order from A to Z.\\n\\nHere's the step-by-step \nprocess:\\n\\n1. Compare 'allis' and 'anthology'. 'allis' comes before \n'anthology'.\\n2. Compare 'anthology' and 'jacobi'. 'anthology' comes before \n'jacobi'.\\n3. Compare 'jacobi' and 'list'. 'jacobi' comes before 'list'.\\n4. \nCompare 'list' and'marmot'. 'list' comes before'marmot'.\\n5. Compare'marmot' \nand'membrane'.'marmot' comes before'membrane'.\\n6. Compare'membrane' \nand 'oakland'.'membrane' comes before 'oakland'.\\n7. Compare 'oakland' \nand'seaborg'. 'oakland' comes before'seaborg'.\\n8. Compare'seaborg' and \n'trapezoidal'.'seaborg' comes before 'trapezoidal'.\\n\\nNow, let's arrange the \nwords in alphabetical order:\\n\\n1. allis\\n2. anthology\\n3. jacobi\\n4. list\\n5. \nmarmot\\n6. membrane\\n7. oakland\\n8. seaborg\\n9. trapezoidal\\n\\nAnswer: 9 Input \n(except for Answer) Baseline response \n(no ProRefine)  ProRefine Figure 5. ProRefine example. Given an input query and an initial prompt,\nthe task model ( LLM task ) gives an incorrect answer. Prorefine uses two\nadditional models, LLM feedback andLLM optimizer , to iteratively\nimprove the prompt as theLLM task generates its response. Refining the\nprompt during generation allows the feedback model to target local regions\nof the response, providing finer-grained feedback than waiting for the\nresponse to complete. However, the optimizer is unable to utilize the\nfeedback in this case.\n--- Page 7 ---\n[5] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\nD. Amodei. Deep reinforcement learning from human preferences.\nIn I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems , volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\nd5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.\n[6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n[7] M. Deng, J. Wang, C.-P. Hsieh, Y . Wang, H. Guo, T. Shu, M. Song,\nE. Xing, and Z. Hu. RLPrompt: Optimizing discrete text prompts with\nreinforcement learning. In Y . Goldberg, Z. Kozareva, and Y . Zhang,\neditors, Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 3369–3391, Abu Dhabi, United\nArab Emirates, Dec. 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.emnlp-main.222. URL https://aclanthology.org/\n2022.emnlp-main.222/.\n[8] Y . Dong, K. Luo, X. Jiang, Z. Jin, and G. Li. PACE: Improving prompt\nwith actor-critic editing for large language model. In L.-W. Ku, A. Mar-\ntins, and V . Srikumar, editors, Findings of the Association for Com-\nputational Linguistics: ACL 2024 , pages 7304–7323, Bangkok, Thai-\nland, Aug. 2024. Association for Computational Linguistics. doi: 10.\n18653/v1/2024.findings-acl.436. URL https://aclanthology.org/2024.\nfindings-acl.436/.\n[9] Y . Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Im-\nproving factuality and reasoning in language models through multiagent\ndebate. In Forty-first International Conference on Machine Learning ,\n2024. URL https://openreview.net/forum?id=zj7YuTE4t8.\n[10] Y . Feng, Y . Wang, J. Liu, S. Zheng, and Z. Lu. LLaMA-rider:\nSpurring large language models to explore the open world. In K. Duh,\nH. Gomez, and S. Bethard, editors, Findings of the Association for\nComputational Linguistics: NAACL 2024 , pages 4705–4724, Mexico\nCity, Mexico, June 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-naacl.292. URL https://aclanthology.\norg/2024.findings-naacl.292/.\n[11] M. L. Gray and S. Suri. Ghost work: how to stop Silicon Valley from\nbuilding a new global underclass . Houghton Mifflin Harcourt, Boston,\n2019. ISBN 978-1-328-56628-7.\n[12] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma,\nP. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability\nin llms via reinforcement learning. arXiv preprint arXiv:2501.12948 ,\n2025.\n[13] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low,\nA. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system\ncard. arXiv preprint arXiv:2412.16720 , 2024.\n[14] D. Kahneman, O. Sibony, and C. R. Sunstein. Noise: a flaw in human\njudgment . Little, Brown Spark, New York, first edition edition, 2021.\nISBN 978-0-316-45140-6 978-0-316-26665-9. OCLC: on1249942231.\n[15] O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. V .\nA, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia,\nand C. Potts. DSPy: Compiling declarative language model calls into\nstate-of-the-art pipelines. In The Twelfth International Conference on\nLearning Representations , 2024. URL https://openreview.net/forum?\nid=sY5N0zY5Od.\n[16] H. R. Kirk, A. Whitefield, P. Röttger, A. Bean, K. Margatina, J. Ciro,\nR. Mosquera, M. Bartolo, A. Williams, H. He, B. Vidgen, and S. A.\nHale. The PRISM Alignment Dataset: What Participatory, Representa-\ntive and Individualised Human Feedback Reveals About the Subjective\nand Multicultural Alignment of Large Language Models, Dec. 2024.\nURL http://arxiv.org/abs/2404.16019. arXiv:2404.16019 [cs].\n[17] Z. Li, X. Xu, T. Shen, C. Xu, J.-C. Gu, Y . Lai, C. Tao, and S. Ma.\nLeveraging large language models for NLG evaluation: Advances and\nchallenges. In Y . Al-Onaizan, M. Bansal, and Y .-N. Chen, edi-\ntors, Proceedings of the 2024 Conference on Empirical Methods in\nNatural Language Processing , pages 16028–16045, Miami, Florida,\nUSA, Nov. 2024. Association for Computational Linguistics. doi: 10.\n18653/v1/2024.emnlp-main.896. URL https://aclanthology.org/2024.\nemnlp-main.896/.\n[18] H. Lightman, V . Kosaraju, Y . Burda, H. Edwards, B. Baker, T. Lee,\nJ. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s Verify Step by\nStep. In The Twelfth International Conference on Learning Representa-\ntions , Oct. 2024. URL https://openreview.net/forum?id=v8L0pN6EOi.\n[19] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom. Program induction by\nrationale generation: Learning to solve and explain algebraic word prob-\nlems. In R. Barzilay and M.-Y . Kan, editors, Proceedings of the 55th An-\nnual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pages 158–167, Vancouver, Canada, July 2017. As-sociation for Computational Linguistics. doi: 10.18653/v1/P17-1015.\nURL https://aclanthology.org/P17-1015/.\n[20] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu. G-eval: NLG evalu-\nation using gpt-4 with better human alignment. In H. Bouamor, J. Pino,\nand K. Bali, editors, Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing , pages 2511–2522, Sin-\ngapore, Dec. 2023. Association for Computational Linguistics. doi: 10.\n18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.\nemnlp-main.153/.\n[21] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegr-\neffe, U. Alon, N. Dziri, S. Prabhumoye, Y . Yang, S. Gupta,\nB. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and\nP. Clark. Self-refine: Iterative refinement with self-feedback. In\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and\nS. Levine, editors, Advances in Neural Information Processing Sys-\ntems, volume 36, pages 46534–46594. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf.\n[22] M. Mehta, V . Pyatkin, and V . Srikumar. Promptly predicting structures:\nThe return of inference. In Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies (Volume 1: Long Papers) , pages\n112–130, 2024.\n[23] Meta. llama-models/models/llama3_2/MODEL_card.md at main ·\nmeta-llama/llama-models, 2024. URL https://github.com/meta-llama/\nllama-models/blob/main/models/llama3_2/MODEL_CARD.md.\n[24] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi,\nL. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-\ntime scaling. arXiv preprint arXiv:2501.19393 , 2025.\n[25] A. Patel, S. Bhattamishra, and N. Goyal. Are NLP models really able\nto solve simple math word problems? In K. Toutanova, A. Rumshisky,\nL. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell,\nT. Chakraborty, and Y . Zhou, editors, Proceedings of the 2021 Confer-\nence of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies , pages 2080–2094,\nOnline, June 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.\nnaacl-main.168/.\n[26] A. Prabhakar, T. L. Griffiths, and R. T. McCoy. Deciphering\nthe factors influencing the efficacy of chain-of-thought: Probability,\nmemorization, and noisy reasoning. In Y . Al-Onaizan, M. Bansal,\nand Y .-N. Chen, editors, Findings of the Association for Computa-\ntional Linguistics: EMNLP 2024 , pages 3710–3724, Miami, Florida,\nUSA, Nov. 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.findings-emnlp.212. URL https://aclanthology.org/\n2024.findings-emnlp.212/.\n[27] R. Pryzant, D. Iter, J. Li, Y . Lee, C. Zhu, and M. Zeng. Automatic\nprompt optimization with “gradient descent” and beam search. In\nH. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing ,\npages 7957–7968, Singapore, Dec. 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.emnlp-main.494. URL https:\n//aclanthology.org/2023.emnlp-main.494/.\n[28] Y . Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection:\nTeaching language model agents how to self-improve. In The Thirty-\neighth Annual Conference on Neural Information Processing Systems ,\n2024. URL https://openreview.net/forum?id=DRC9pZwBwR.\n[29] L. Ranaldi and A. Freitas. Self-refine instruction-tuning for aligning\nreasoning in language models. arXiv preprint arXiv:2405.00402 , 2024.\n[30] A. S. Rao, A. Khandelwal, K. Tanmay, U. Agarwal, and M. Choudhury.\nEthical reasoning over moral alignment: A case and framework for in-\ncontext ethical policies in LLMs. In H. Bouamor, J. Pino, and K. Bali,\neditors, Findings of the Association for Computational Linguistics:\nEMNLP 2023 , pages 13370–13388, Singapore, Dec. 2023. Association\nfor Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.\n892. URL https://aclanthology.org/2023.findings-emnlp.892/.\n[31] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike.\nSelf-critiquing models for assisting human evaluators, June 2022. URL\nhttp://arxiv.org/abs/2206.05802. arXiv:2206.05802 [cs].\n[32] T. Schick, J. A. Yu, Z. Jiang, F. Petroni, P. Lewis, G. Izacard, Q. You,\nC. Nalmpantis, E. Grave, and S. Riedel. PEER: A collaborative lan-\nguage model. In The Eleventh International Conference on Learn-\ning Representations , 2023. URL https://openreview.net/forum?id=\nKbYevcLjnc.\n[33] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Prox-\nimal policy optimization algorithms. arXiv preprint arXiv:1707.06347 ,\n2017.\n[34] T. Shin, Y . Razeghi, R. L. Logan IV , E. Wallace, and S. Singh. Auto-\n--- Page 8 ---\nPrompt: Eliciting Knowledge from Language Models with Automati-\ncally Generated Prompts. In B. Webber, T. Cohn, Y . He, and Y . Liu,\neditors, Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages 4222–4235, On-\nline, Nov. 2020. Association for Computational Linguistics. doi: 10.\n18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.\nemnlp-main.346/.\n[35] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute\noptimally can be more effective than scaling model parameters, 2024.\nURL https://arxiv.org/abs/2408.03314.\n[36] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM Test-Time Com-\npute Optimally can be More Effective than Scaling Model Parameters,\nAug. 2024. URL http://arxiv.org/abs/2408.03314. arXiv:2408.03314\n[cs].\n[37] A. Sorensen, S. Peng, B. Plank, and R. Van Der Goot. EEVEE: An\neasy annotation tool for natural language processing. In S. Henning\nand M. Stede, editors, Proceedings of the 18th Linguistic Annotation\nWorkshop (LAW-XVIII) , pages 216–221, St. Julians, Malta, Mar. 2024.\nAssociation for Computational Linguistics. URL https://aclanthology.\norg/2024.law-1.20/.\n[38] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research , 2023.\n[39] L. Torrey and M. Taylor. Teaching on a budget: Agents advising agents\nin reinforcement learning. In Proceedings of the 2013 international con-\nference on Autonomous agents and multi-agent systems , pages 1053–\n1060, 2013.\n[40] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang,\nA. Creswell, G. Irving, and I. Higgins. Solving math word prob-\nlems with process-and outcome-based feedback. arXiv preprint\narXiv:2211.14275 , 2022.\n[41] P. Verga, S. Hofstatter, S. Althammer, Y . Su, A. Piktus, A. Arkhang-\norodsky, M. Xu, N. White, and P. Lewis. Replacing judges with ju-\nries: Evaluating llm generations with a panel of diverse models. arXiv\npreprint arXiv:2404.18796 , 2024.\n[42] M. Wadhwa, X. Zhao, J. J. Li, and G. Durrett. Learning to refine with\nfine-grained natural language feedback. In Y . Al-Onaizan, M. Bansal,\nand Y .-N. Chen, editors, Findings of the Association for Computa-\ntional Linguistics: EMNLP 2024 , pages 12281–12308, Miami, Florida,\nUSA, Nov. 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.findings-emnlp.716. URL https://aclanthology.org/\n2024.findings-emnlp.716/.\n[43] J. Wang, Y . Liang, F. Meng, Z. Sun, H. Shi, Z. Li, J. Xu, J. Qu, and\nJ. Zhou. Is ChatGPT a good NLG evaluator? a preliminary study.\nIn Y . Dong, W. Xiao, L. Wang, F. Liu, and G. Carenini, editors, Pro-\nceedings of the 4th New Frontiers in Summarization Workshop , pages\n1–11, Singapore, Dec. 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.newsum-1.1. URL https://aclanthology.\norg/2023.newsum-1.1/.\n[44] T. C. Weerasooriya, S. Dutta, T. Ranasinghe, M. Zampieri, C. M.\nHoman, and A. R. KhudaBukhsh. Vicarious Offense and Noise Audit of\nOffensive Speech Classifiers: Unifying Human and Machine Disagree-\nment on What is Offensive, Nov. 2023. URL http://arxiv.org/abs/2301.\n12534. arXiv:2301.12534 [cs].\n[45] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi,\nQ. V . Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in\nlarge language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh, editors, Advances in Neural Information Pro-\ncessing Systems , volume 35, pages 24824–24837. Curran Associates,\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n[46] C. Yang, X. Wang, Y . Lu, H. Liu, Q. V . Le, D. Zhou, and X. Chen. Large\nlanguage models as optimizers, 2024. URL https://arxiv.org/abs/2309.\n03409.\n[47] K. Yang, Y . Tian, N. Peng, and D. Klein. Re3: Generating longer stories\nwith recursive reprompting and revision. In Y . Goldberg, Z. Kozareva,\nand Y . Zhang, editors, Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing , pages 4393–4479,\nAbu Dhabi, United Arab Emirates, Dec. 2022. Association for Com-\nputational Linguistics. doi: 10.18653/v1/2022.emnlp-main.296. URL\nhttps://aclanthology.org/2022.emnlp-main.296/.\n[48] M. Yuksekgonul, F. Bianchi, J. Boen, S. Liu, Z. Huang, C. Guestrin, and\nJ. Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint\narXiv:2406.07496 , 2024.\n[49] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang,\nZ. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Sto-\nica. Judging llm-as-a-judge with mt-bench and chatbot arena. InA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and\nS. Levine, editors, Advances in Neural Information Processing\nSystems , volume 36, pages 46595–46623. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_\nBenchmarks.pdf.\n[50] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\nJ. Ba. Large language models are human-level prompt engineers. arXiv\npreprint arXiv:2211.01910 , 2022.\n[51] M. Zhuge, C. Zhao, D. Ashley, W. Wang, D. Khizbullin, Y . Xiong,\nZ. Liu, E. Chang, R. Krishnamoorthi, Y . Tian, Y . Shi, V . Chandra, and\nJ. Schmidhuber. Agent-as-a-judge: Evaluate agents with agents, 2024.\nURL https://arxiv.org/abs/2410.10934.",
  "text_length": 55133
}