{
  "id": "http://arxiv.org/abs/2506.05336v1",
  "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
  "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
  "authors": [
    "Ghazi Shazan Ahmad",
    "Ahmed Heakl",
    "Hanan Gani",
    "Abdelrahman Shaker",
    "Zhiqiang Shen",
    "Ranjay Krishna",
    "Fahad Shahbaz Khan",
    "Salman Khan"
  ],
  "published": "2025-06-05T17:59:29Z",
  "updated": "2025-06-05T17:59:29Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05336v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05336v1  [cs.CV]  5 Jun 2025VIDEO MOLMO : Spatio-Temporal Grounding\nMeets Pointing\nGhazi Shazan Ahmad1∗Ahmed Heakl1∗Hanan Gani1Abdelrahman Shaker1\nZhiqiang Shen1Ranjay Krishna2,3Fahad Shahbaz Khan1,4Salman Khan1,5\n1Mohamed Bin Zayed University of Artificial Intelligence2University of Washington\n3Allen Institute for Artificial Intelligence4Linköping University\n5Australian National University\nCorrespondence: {ghazi.ahmad, ahmed.heakl, hanan.ghani} @mbzuai.ac.ae\n/g♀behttps://mbzuai-oryx.github.io/VideoMolmo\nFigure 1: Given complex referring expressions in natural language, VIDEO MOLMO demonstrates im-\nproved spatio-temporal reasoning in visual grounding. By decomposing the visual grounding task into\nsequential steps—pointing (denoted by star) followed by generating masks (in red) - VIDEO MOLMO\nproduces more accurate and coherent segmentation masks compared to prior approaches.\nAbstract\nSpatio-temporal localization is vital for precise interactions across diverse domains,\nfrom biological research to autonomous navigation and interactive interfaces. Cur-\nrent video-based approaches, while proficient in tracking, lack the sophisticated\nreasoning capabilities of large language models, limiting their contextual under-\nstanding and generalization. We introduce VIDEO MOLMO , a large multimodal\nmodel tailored for fine-grained spatio-temporal pointing conditioned on textual\n*Equal technical contribution\nPreprint. Under review.\n--- Page 2 ---\ndescriptions. Building upon the Molmo architecture, VIDEO MOLMO incorporates\na temporal module utilizing an attention mechanism to condition each frame on\npreceding frames, ensuring temporal consistency. Additionally, our novel temporal\nmask fusion pipeline employs SAM2 for bidirectional point propagation, signifi-\ncantly enhancing coherence across video sequences. This two-step decomposition\ni.e., first using the LLM to generate precise pointing coordinates, then relying\non a sequential mask-fusion module to produce coherent segmentation, not only\nsimplifies the task for the language model but also enhances interpretability. Due\nto the lack of suitable datasets, we curate a comprehensive dataset comprising 72k\nvideo-caption pairs annotated with 100k object points. To evaluate the generaliza-\ntion of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution\nbenchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision,\nAutonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our\nmodel on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS\ntasks. In comparison to existing models, VIDEO MOLMO substantially improves\nspatio-temporal pointing accuracy and reasoning capability. Our code and models\nare publicly available at https://github.com/mbzuai-oryx/VideoMolmo .\n1 Introduction\nPrecise spatio-temporal localization underpins a wide range of applications, enabling fine-grained\ninteractions and analysis in dynamic environments. For instance, accurately tracking cell nuclei\nmovement is essential in biological research, aiding in understanding developmental processes and\ndisease progression [ 21]. Similarly, autonomous vehicles rely on continuous tracking of pedestrians,\nvehicles and traffic lights to ensure safe navigation; robots manipulating objects must precisely\nlocalize contact points over time while avoiding collisions [ 39]. In egocentric videos, captured from\nwearable, first-person cameras, spatio-temporal localization is key for tasks such as daily activity\nrecognition, assistive technologies for visually impaired users, and modeling hand-object interactions\nin real-world settings [ 8]. Counting objects across frames is also vital in scenarios such as surveillance\nand traffic monitoring, where assessing object quantities informs critical decisions (Fig. 4).\nPrior efforts [ 23,38,42,34,14] have explored text-guided tracking and grounding in videos. However,\nthese approaches often lack the reasoning depth offered by large language models (LLMs), limiting\ntheir generalization and contextual understanding. While recent video large multimodal models\n(LMMs) can generate dense, spatio-temporally coherent masks [ 3,22], none support free-form,\ntext-conditioned pointing in videos.\nFor instance, as illustrated in Fig. 1, the instruction is to identify “the white pigeon that has moved\nslightly from left to right” among five pigeons in close proximity in a video sequence. Solving such a\ntask demands both temporal understanding and fine-grained reasoning to distinguish subtle object\nmovements. Existing methods often struggle in such scenarios, frequently predicting multiple objects\nor localizing the wrong one, highlighting the limitations of models without integrated reasoning and\nfine-grained localization capabilities.\nImage-based pointing LMMs have demonstrated strong single-frame performance [ 5,39], but they\ncannot model the temporal dynamics essential for video tasks. To fill this gap, we introduce VIDEO -\nMOLMO , an LMM that accepts natural-language queries and produces point-level predictions for\ntarget objects across entire video sequences, ensuring temporal consistency. In this way, VIDEO -\nMOLMO decomposes visual grounding in videos into two simpler stages: first, generating precise\npointing coordinates via the LLM, then sequentially fusing these points into coherent masks with a\ndedicated module. This decomposition simplifies the problem for the language model and improves\noverall interpretability.\nVIDEO MOLMO is built on top of Molmo [ 5], which features a pre-processor that converts the input\nimage into multiscale, multi-crop images, a vision encoder, an LLM decoder and a visual projector that\npools and projects image features into the LLM’s embedding space. While Molmo processes images\nindependently, we adapt Molmo architecture to process video data in a simplified manner. We first\nintroduce a temporal module that handles the temporal nature of the video data by conditioning each\nframe on previous frames through an attention mechanism. Additionally, we propose a novel temporal\nmask fusion pipeline that leverages SAM2 for bidirectional point propagation, enhancing temporal\n2\n--- Page 3 ---\ncoherence by utilizing local structural cues embedded in videos. Since there exists no spatio-temporal\npointing dataset to train such systems, we release a comprehensive dataset of 72k video-caption pairs\nand 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench,\na challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking,\nEgocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also assess our\nmodel on Referring Video Object Segmentation (Ref-VOS) and Reasoning VOS tasks, which require\nmasks as output. For this, we leverage SAM2 to convert the predicted points into segmentation masks\nand propose a bidirectional temporal mask fusion technique that enhances mask consistency without\nfurther training. Experimental results show that VideoMolmo outperforms existing approaches\nacross all benchmarks and task settings, offering a generalizable solution for fine-grained language-\nguided reasoning in dynamic visual environments. Specifically, on our challenging VPoS- Bench,\nVideoMolmo exhibits 5.4pp average improvement compared to the strongest baseline (Table 4).\nSimilarly, on the challenging referring segmentation on MeViS [ 6], VideoMolmo outperforms the\nstrongest baseline by 9.5pp (Table 1), highlighting its effectiveness in visual grounding and reasoning\ntasks.\n2 Related work\nVideo-LMMs. Multi-modal LLMs such as [ 18,43] have demonstrated notable advancements due\nto their strong zero-shot abilities, which have been possible due to their training on millions of\nimage-text pairs. Typically, such models project visual information in the latent space of LLM via\nan encoder and a connector and thus align the information from vision and text modality. The work\nin LMMs paved the way for the development of Video-LMMs [13, 41, 15, 20, 35, 45, 2, 40], which\nunlike image-based LMMs, can reason on dynamic video content. While effective for overall video\ninput comprehension, these methods fell short of fine-grained visual grounding in videos.\nVisual grounding. Recent works in Grounded LMMs [ 29] have sparked tremendous interest\namong the research community. Visual grounding [ 17] seeks to identify the location of nouns or\nshort phrases (such as a man with blue shirt) within an image. These models are trained on a huge\ndataset of image-caption pairs along with the dense segmentation mask associated with the objects in\nthe caption. [ 3,22] extended the grounding to video data by releasing a large dataset of grounded\nvideo-QA triplet pairs along with the masks associated with the objects. Training on such huge video\ngrounded datasets allowed for video grounding. On the contrary, our proposed VideoMolmo model\nand dataset are designed for outputting precise object level points which are crucial for applications\nsuch as autonomous driving, counting tasks, robotics etc.\nLanguage-assisted object tracking. Most text-based tracking methods are limited to tracking a\nsingle object only [ 38,42,34,14]. However, real-world applications can feature multiple object\ntrajectories, making it harder for the single object tracking methods. [ 23] propose Type-toTrack along\nwith a tracking dataset ‘GroOT’ for multi-object tracking. However, they track objects via bounding\nboxes and not precise points which limits their applicability. Another work SAM-PT [ 28] proposes to\nuse SAM [ 30] model along with a long-term point tracking mechanism for point-centric interactive\nvideo segmentation. However, since their method adapts a 2D model to handle video data, it faces\nchallenges in temporal consistency, especially in the cases of occlusions and fast-moving objects. On\nthe contrary, our proposed VIDEO MOLMO is trained end-to-end on our training dataset and maintains\ntemporal consistency via a dedicated memory module.\n3 V IDEO MOLMO\nVIDEO MOLMO is trained end-to-end for spatio-temporal pointing conditioned on textual instructions.\nIt features a visual encoder, a temporal module, an LLM, and a post-processing module. The visual\nencoder processes the video frames and outputs multi-crop features. To maintain temporal consistency,\nwe introduce a temporal module which employs a cross-attention operation and ensures that the\ncurrent frame attends to the information in previous frames. The resultant features are then passed\nto the LLM, which, along with the textual query, processes this information and outputs the points\ncorresponding to the objects. Our post-processing module takes the predictions from VIDEO MOLMO\nand uses SAM2 to propagate the points across all video frames bidirectionally.\n3.1 Architecture\nVIDEO MOLMO extends the Molmo architecture [ 5] from static image understanding to spatio-\ntemporal video grounding. The model consists of four end-to-end trainable components: (1) a visual\n3\n--- Page 4 ---\nVisual EncoderMulti-Head Cross\nAttention\nTempor al Module (  )Textual \nQuery\nPoint to\nthe ballK\nQVisual ProjectorLLM\nPast     fr ames Current Fr ame\nFeature P ooling V\n SAM2\nTempor al\nMask Fusion+<x, y, alt text>Figure 2: VIDEO MOLMO Architecture. The visual encoder extracts multi-crop features from\nthe current frame and the past lframes. These temporal features provide contextual cues and are\nprocessed by the Temporal Module Mvia multi-head cross-attention, where the query comes from\nthe current frame, and key and value from the mean of previous frames. The output is fused with the\noriginal features to enrich temporal cues while preserving the spatial details of the current frame. The\ncombined visual-textual representations are then passed to the LLM to predict grounded points. These\npoints are converted into masks using our Bidirectional Temporal Mask Fusion module, ensuring\ntemporally consistent pixel-level grounding.\nencoder, (2) a temporal module, (3) visual projector (4) a decoder-only large language model (LLM);\nand a post-processing module (see Fig. 2).\nWe represent an input video as V∈R|T |×H×W×Cwhere H,W, and Cdenote the height, width,\nand number of channels respectively of the frames and |T |are the number of frames in the video.\nFor a frame Ti∈V, we generate Nspatial overlapping multi-crops to capture both global context\nand fine-grained details in the frame. The multi-crops of the frame are processed independently using\na visual encoder F. Following [ 5], we build patch features for each crop by concatenating features\nfrom third-to-last and tenth-to-last ViT layers. The features extracted from each crop of frame Ti\nare denoted by fj\nTi∈RP×D, where jdenotes the crop index, Pis the number of patches and Dis\nthe latent dimension of the ViT. Since the components in our model are based on 2D modules, we\ninject temporal information through a dedicated temporal module denoted as M(Sec. 3.2). For\neach frame Ti, we compute a temporal context feature by aggregating features from the lmost recent\nframes {Ti−l,Ti−l−1, . . . ,Ti−1}such that ( l < i ) and denote their mean as fT∗\ni−. The pooled context\nfeatures and the current frame features are then passed into the temporal module. The temporally\nenriched features predicted by the temporal module Mcorrespond to 2×2patch window features\nextracted from all the patches of the crops. These window-level features are added to the current\nframe features by reshaping fTifromRN×P×DtoRN.P\n4×4×D.\nˆfTi=fTi+M\u0000\nfTi, fT∗\ni−\u0001\n(1)\nThese resultant features ˆfTiare then aggregated into a single frame-level representation using an\nattention pooling mechanism. The pooled feature ˆfTiis then projected into the embedding space of\nthe language model using a learnable projection layer P. It is then concatenated with the tokenized\nquery qand passed to the decoder-only LLM to obtain the final output text pwhich contains the\ncoordinate {(x, y)}Owhere Ois the number of objects grounded in the frame.\np=LLM\u0010h\nP(ˆfTi);qi\u0011\n(2)\n4\n--- Page 5 ---\nThe model is trained end-to-end by minimizing the cross-entropy loss LCEbetween the predicted\ntextpand the ground-truth one-hot labels p∗in an auto-regressive manner:\nLCE=−p∗·log(p) (3)\n3.2 Temporal Module\nThe original MoLMo architecture [ 5] was developed for static images and cannot model the temporal\ndynamics inherent in video data. To address this, we introduce a dedicated temporal module\nMthat infuses each frame with temporal context from the preceding lframes [ 23,12]. For each\ncropjof frame Ti, patch features fj\nTi∈RP×Dare extracted and reshaped into non-overlapping\n2×2windows of four vectors, fj\nTi={fj,p\nTi∈R4×D}P/4\np=1. We then flatten the window features\nacross all Ncrops to obtain resized fetures called window vectors fTiandfT∗\ni−such that both lie\ninR(N·P/4)×4×Dfor both the current features and the context features. To capture fine-grained\ntemporal correspondences, we apply multi-head cross-attention (MHCA) over each patch window,\nwhere the query comes from current frame window vector fTi, and key and value come from vector\nfT∗\ni−such that MHCA\u0010\nfTi, fT∗\ni−, fT∗\ni−\u0011\ndenotes the final attended window features. By restricting\nattention to small 2×2neighborhoods, the module selectively refines each local region of the current\nframe using only the most relevant fine-grained patches from its temporal context. This targeted\ncross-attention both preserves spatial locality and amplifies subtle motion cues that would be lost in\ncoarse global pooling.\n3.3 Bidirectional Temporal Mask Fusion\nWhile our method predicts grounded point coordinates corresponding to objects mentioned in the\ntextual query, most existing evaluation protocols are designed to operate on segmentation masks.\nTherefore, for compatibility and consistent evaluation, we introduce Bidirectional Temporal Mask\nFusion, a novel post-processing technique that leverages SAM2 to convert points to dense masks.\nSpecifically, we use the predicted points as prompts to SAM2 [ 30] to obtain the segmentation masks\nof the objects. Due to computational constraints, we avoid dense frame-level inference across the\nentire video. Instead, we sparsely sample frames at a sampling rate k, and generate point predictions\nonly on these sampled frames. Let the two consecutive sampled frames be denoted as Tiand\nTi+k. The corresponding point predictions are transformed into segmentation masks miandmi+k\nusing SAM2. For frames lying between the two sampled frames (i.e., Ti+nfor0< n < k ), we\npropagate the neighboring masks miandmi+kbidirectionally to estimate the intermediate masks.\nThe leftward propagation from miand the rightward propagation from mi+kare performed using\nSAM2’s temporal propagation module, defined as,\nbm→\ni+n=Prop→({Ti, mi},Ti+n)\nbm←\ni+n=Prop←({Ti+k, mi+k},Ti+n),(4)\nwherebm→\ni+nandbm←\ni+ndenote the masks propagated from the left and right directions, respectively.\nTo reconcile the two propagated masks at frame Ti+n, we compute their Intersection over Union\n(IoU). If the IoU exceeds a predefined threshold τ, we adopt the intersection as the final prediction.\nmi+n=\u001aIOU(bm→\ni+n,bm←\ni+n),ifIOU(·)≥τ\nbm→\ni+n∪bm←\ni+n, otherwise(5)\nThe OR operation in the second case accounts for cases with significant motion transitions, where the\nleft and right predictions might deviate spatially or structurally. In such a case, two masks propagated\nfrom the left and the right considerably differ in either spatial location or shape.\nIn rare cases, SAM2 may fail to propagate a mask from one direction, resulting in an empty prediction.\nWe handle this with a fallback mechanism. Specifically, if the mask being propagated from the left is\nempty, we assign the right mask to the prediction of the current frame. Otherwise, we assign the left\nmask. Mathematically, it can be written as,\nmi+n=\u001abm←\ni+n,ifbm→\ni+n=∅\nbm→\ni+n,otherwise(6)\n5\n--- Page 6 ---\nSAM2\nIOU\n0.8\n0.95\n0.9\nGT MaskRaw videoSample\npoints\nCandidate MasksGT Mask\nFigure 3: VIDEO MOLMO annotation pipeline: We construct point-level supervision from frame-\nlevel masks using a semi-automatic process. For each frame, kpoints are sampled on the mask and\npassed to SAM2 to generate candidate masks. The point with the highest-IoU candidate mask (w.r.t.\nground truth) is selected as the optimal annotation.\nThis bidirectional temporal fusion ensures temporally consistent masks with reduced compute\noverhead. It not only aligns with existing segmentation-based evaluation protocols but also introduces\nrobustness in dynamic scenes by leveraging context from both past and future observations.\n4 V IDEO MOLMO dataset\nTraining Data: Our dataset is essential for training the model’s spatio-temporal pointing capabilities.\nIt features 72k Video-caption pairs along with 100k object points. Our benchmark video dataset comes\nfrom different sources: Refer-YTVOS [ 31], Refer-DA VIS [ 26], MeViS [ 6], GroOT [ 23], LaSOT [ 7],\nViCaS-LGVIS [ 1], and Reason-VOS [ 36].To create fine-grained data grounded in point coordinates,\nwe develop a semi-automated annotation pipeline (see Fig. 3) that ensures both high-quality and\nscalable annotations. Each of the above mentioned datasets features video-mask-expression triplets\n(V, M, T )such that V∈R|T |×H×W×C, M∈ {0,1}|T |×|O|× H×Wwith|O|denoting the number\nof unique annotated objects in the frame Ti. For each object Oj∈ O in frame Ti∈RH×W×C, with\na binary mask mj∈ {0,1}H×W, the goal is to extract a single highly representative point coordinate\nfor the object. We sample kcandidate points within the mask, assigning each point (x, y)a sampling\nprobability proportional to its Euclidean distance to the nearest boundary pixel of the mask mj, i.e.,\nP(x, y)∝ min\n(x′,y′)∈∂mj∥(x, y)−(x′, y′)∥2(7)\nwhere ∂mjdenotes the set of boundary pixels of the mask. For each sampled point, we use SAM2\nto predict a segmentation mask. We then compute the Intersection-over-Union (IoU) between each\npredicted mask and the corresponding ground truth mask mj. The point coordinate whose predicted\nmask achieves the highest IoU is selected as the representative ground truth point for the object:\np∗= arg max\n(x,y)IoU (SAM2( x, y), mj), (8)\nwhere SAM2( x, y)denotes the predicted mask obtained using point (x, y)as a prompt to SAM2.\nVPoS-Bench: To evaluate the generalization capabilities of VIDEO MOLMO , we introduce Video\nPointing and Segmentation (VPoS-Bench), a curated benchmark test set comprising of 100video-\ncaption pairs and 1kmanually annotated object points. For mask-based evaluations, we employ\nthe SAM2 model to convert these point annotations into segmentation masks. The test benchmark\nencompasses diverse real-world scenarios, sourced from both open datasets [ 4,16,8] and internal\ncollections, spanning five categories: Cell Tracking, Egocentric Videos, Autonomous Driving, Video-\nGUI, and Robotics. Our benchmark also consists of a dedicated counting task sourced from [ 9]\ndataset. For additional details about the benchmark, refer to Sec. A.3 of the Appendix.\n5 Experiments\nImplementation details. VIDEO MOLMO follows the architecture of Molmo [ 5]. For the image\nencoder, we use a pretrained CLIP ViT-L/14 (336 ×336) [ 27] model. Our proposed Temporal Module\nis initialized from scratch. Our choice of LLM is the pretrained Qwen2 7B [ 37]. We train the model\n6\n--- Page 7 ---\nFigure 4: VIDEO MOLMO demonstrates robust generalization and fine-grained spatio-temporal\ngrounding across diverse out-of-distribution scenarios from our proposed benchmark, for instance,\ncorrectly pointing to traffic lights (2ndrow) in challenging driving scenes despite never encountering\nsuch scenarios during training. (Please refer to Appendix A.4 for additional qualitative results.)\non8NVIDIA A100 80GB GPUs. Learning rate of 1e−5is used for the LLM, and 5e−6for the\nvision encoder, visual projector and temporal module. We adopt a batch size of 1with256gradient\naccumulation steps, and use AdamW optimizer [19] following the fine-tuning recipe from [5].\nTasks. We evaluate VIDEO MOLMO on four challenging tasks: point grounding, counting, referring\nsegmentation, and reasoning video object segmentation. For point grounding, we report performance\non our proposed VPoS-Bench. For the counting task, we utilize videos from the Kinetics dataset [ 9],\nwhere object counts range from 2−13. For referring video segmentation, we use MeViS validation set\n[6] Refer-DA VIS-17 [ 10] and Refer-Youtube-VOS [ 32] datasets. Finally, for reasoning segmentation,\nwe evaluate our model on the ReasonVOS dataset [3].\nEvaluation metrics. For the Point Grounding task, we follow the evaluation protocol of Molmo\n[5] and report Precision, Recall, and F1 score. For mask-based evaluations, we use Region Jaccard\n(J), Boundary F-measure ( F), and their average ( J&F). For the Counting task, we report Exact\nMatching Accuracy (EMA) and Mean Absolute Error (MAE) [5, 42].\nBaselines. For point grounding, we compare VIDEO MOLMO with three strong baselines:\nVideoLISA[ 3], VideoGLaMM [ 22], and Molmo+SAM2. To adapt Molmo for videos, we aug-\nment it with SAM2. For referring segmentation, we evaluate against VideoLISA, VideoGLaMM, and\nprior baselines. For counting, we compare VIDEO MOLMO with both closed-source (GPT-4o [ 24])\nand open-source models [ 5,2,33]). For further experimentation details please refer to Appendix A.2.\n5.1 Main experimentation results\nPoint Grounding. The point grounding task focuses on accurately identifying the spatial coordinates\nof a queried object within video frames. As depicted in Fig. 5, VIDEO MOLMO demonstrates superior\nperformance in localizing target points, as evidenced by its significantly higher Precision, Recall,\nand F1 scores compared to Molmo. This performance gap can be attributed to Molmo’s training on\nstatic frames, which limits its ability to handle temporal variations. In dynamic video inputs, where\nobject presence and position may vary across frames, Molmo struggles, whereas VIDEO MOLMO\neffectively addresses this challenge by leveraging temporal context. Furthermore, VIDEO MOLMO\noutperforms all baseline models across each subtask from our benchmark, as evident from higher\nJ,F, and the combined J&Fmetric (Table 4). Qualitative results in Fig.4 further validate the\nrobustness of VIDEO MOLMO , showcasing its ability to accurately localize objects across diverse and\nout-of-distribution scenarios.\n7\n--- Page 8 ---\nTable 1: Performance comparison on Refer-DA VIS-17, Refer-Youtube-VOS, and MeViS benchmarks.\nVideoMolmo consistently improves referring video object segmentation across datasets.\nModel Refer-DA VIS-17 Refer-Youtube-VOS MeViS\nJ F J &F J F J &F J F J &F\nLISA-7B [11] 61.9 54.9 58.4 50.6 49.7 50.2 – – –\nLISA-13B [11] 64.6 56.8 60.7 53.0 52.1 52.6 – – –\nTrackGPT-7B [44] 67.0 59.4 63.2 57.4 55.3 56.4 – – –\nTrackGPT-13B [44] 70.4 62.7 66.5 60.8 58.1 59.5 – – –\nVideoLISA [3] 72.7 64.9 68.8 65.7 61.7 63.7 41.3 47.6 44.4\nVideoGLaMM [22] 73.3 65.6 69.5 65.4 68.2 66.8 42.1 48.2 45.2\nMolmo [5]+SAM2 [30] 65.3 72.2 68.8 61.0 66.2 63.6 44.4 49.4 46.9\nVideoMolmo 71.3 73.6 72.5 65.6 69.1 67.3 51.2 56.6 53.9\nTable 2: Performance comparison of Video-\nMolmo on the ReasonVOS benchmark.\nModel J F J &F\nLISA [11] 33.1 29.1 31.1\nVideoLISA [3] 49.9 45.1 47.5\nVideoGLaMM [22] 40.5 27.2 33.9\nMolmo [5] + SAM2 [30] 43.5 47.8 45.7\nVideoMolmo 48.7 53.4 51.1Table 3: Performance comparison of Video-\nMolmo on the counting benchmark.\nModel MAE ↓EMA↑\nGPT-4o [25] 0.76 60.0\nGemma3-12B [33] 0.96 43.3\nQwen2.5-VL-7B [2] 0.83 50.0\nMolmo [5] 1.21 49.3\nVideoMolmo 0.72 73.3\nTable 4: Performance of various models on five subtasks of VPoS-Bench (Ego4D, Robotics, Au-\ntonomous, Cells, VideoGUI)\nModel Ego4D Robotics Autonomous Cells VideoGUI\nJ F J &FJ F J &FJ F J &FJ F J &FJ F J &F\nVideoLISA [3] 47.1 41.1 44.2 3.9 2.0 2.9 34.7 22.0 28.4 18.9 3.3 11.1 65.3 39.4 52.4\nVideoGLaMM [22] 47.2 40.4 43.8 15.3 10.3 12.8 31.4 17.7 24.8 11.8 7.8 9.8 58.7 32.5 45.6\nMolmo [5] +SAM2 [30] 50.6 50.1 50.4 27.8 25.6 26.7 49.5 47.5 48.5 20.8 7.6 14.2 60.2 55.9 58.0\nVideoMolmo 55.5 54.3 54.9 29.1 26.2 27.6 57.1 57.7 57.4 25.4 13.1 19.2 68.9 62.4 65.7\nPrecision Recall F1\nMetrics45.047.550.052.555.057.560.0ScoresMolmo VideoMolmo\nFigure 5: Performance comparison of Video-\nMolmo on point grounding.\n0 1 2 3 4 5\nl69.570.070.571.071.572.072.5J&F\nFigure 6: Effect of temporal module context\nlength on segmentation accuracy in Refer-\nDA VIS benchmark.\nObject Counting. Our benchmark introduces a dedicated object counting task, a capability essential\nto many real-world video understanding applications. The number of objects in our benchmark\nranges from 2 to 13, therefore demanding enhanced temporal and spatial understanding. For this\nevaluation, we compare VIDEO MOLMO against both open-source and proprietary models, as shown\nin Table 3. VIDEO MOLMO achieves state-of-the-art performance, significantly outperforming all\nbaselines in terms of both Mean Absolute Error (MAE) and Exact Match Accuracy (EMA). Notably,\nVIDEO MOLMO also surpasses advanced proprietary model such as GPT-4o, underscoring its strength\nas a specialized counting model. We attribute this success to the explicit training of VideoMolmo\non our proposed training dataset, which includes diverse scenarios with 0 to 10 objects per video,\nenabling the model to develop a fine-grained understanding of multi-object scenes.\nReferring Segmentation. For referring video segmentation, the goal is to localize specific object\ninstances in a video based on a given phrase. Table 1 presents results across three standard datasets.\nOn the MeViS benchmark, which involves motion-guided segmentation with multiple objects, VIDEO -\nMOLMO outperforms all baselines by a notable margin, demonstrating its effectiveness in grounding\n8\n--- Page 9 ---\nTable 5: Effect of different temporal mask\nfusion strategies on Refer-DA VIS dataset.\nStrategy J F J &F\nPrefer left 67.31 73.38 70.34\nPrefer right 67.05 71.69 69.37\nIntersection 60.20 63.58 61.89\nLarger mask 70.40 72.91 71.65\nSmaller mask 64.10 67.18 65.64\nVideoMolmo 71.27 73.63 72.45Table 6: Ablation on different variants of the\ntemporal module on the Refer-DA VIS dataset.\nVariant J F J &F\nSingle frame 66.04 72.60 69.32\nAddition 66.13 72.97 69.55\nConcatenation 65.34 72.06 68.71\nVideoMolmo 71.27 73.63 72.45\ncomplex, multi-object scenes. This advantage stems in part from the simplicity and efficiency of\nVIDEO MOLMO ’s point-based supervision as reflected in its superior J,F, andJ&Fscores, which\ncontrasts with recent methods like VideoGLaMM [ 22] and VideoMolmo [ 3] that rely on dense,\npixel-level mask prediction where precise delineation between objects becomes challenging (Fig. 1).\nVIDEO MOLMO also achieves superior performance on Refer-DA VIS-17 and Refer-Youtube-VOS.\nNotably, VideoGLaMM performs competitively on Refer-Youtube-VOS which features fast-moving\nobjects, benefiting from its dual encoder architecture that integrates spatial and temporal features.\nHowever, VIDEO MOLMO , despite using a single encoder with point-based supervision, surpasses\nthese strong baselines which could be attributed to its temporal module capturing past inference,\nnovel post-processing technique, and point grounding training paradigm.\nReasoning Video Object Segmentation. Table 2 presents a comparative analysis of VIDEO MOLMO\nagainst existing approaches on the ReasonVOS benchmark, which emphasizes complex reasoning,\ntemporal comprehension, and consistency across frames, making it particularly challenging. Prior\nmethods perform noticeably worse, largely due to their limited temporal and fine-grained reasoning\ncapabilities. While VideoLISA incorporates spatio-temporal cues, it still falls short of VIDEO MOLMO .\nThis performance gap highlights VIDEO MOLMO ’s architectural strengths, specifically its dedicated\ntemporal module providing rich spatio-temporal contextual understanding.\n5.2 Ablations and Analysis\nEffect of Temporal Module. We conduct an ablation study to evaluate the effectiveness of different\ntemporal module variants on the Refer-DA VIS benchmark (Table 6). Using a single frame or\nsimple feature fusion methods such as addition or token-space concatenation yields relatively lower\nperformance compared to our proposed cross-attention-based temporal module as it enables dynamic\nand selective integration of relevant features across frames, allowing the model to focus on temporally\ncoherent and semantically meaningful cues critical for accurate grounding.\nAblation on Temporal Mask Fusion: To enable efficient and temporally consistent segmentation,\nwe evaluate various strategies for combining masks propagated from the sampled frames. As shown\nin Table 5, naive strategies like preferring left/right predictions or computing mask intersections\nresult in suboptimal performance, either due to loss of temporal context or overly conservative\nfusion. Our proposed bidirectional fusion strategy outperforms all baselines by adaptively reconciling\nforward and backward propagated masks based on their agreement (IoU). Our fallback mechanism\nensures robustness against failure cases where one of the propagated masks is missing. This approach\nachieves a significant improvement in J&Fscore (72.45), demonstrating its effectiveness.\nEffect of context-length in temporal module: To analyze the effect of context length in the temporal\nmodule, we evaluate VIDEO MOLMO on the Refer-DA VIS benchmark (Fig. 6). We observe that\nthere is a consistent increase in J&Fas the context length increases from 1→4, indicating that\nincorporating more temporal information enhances the model’s spatio-temporal reasoning. However,\nthere is a slight drop in accuracy at l= 5, suggesting that adding more frames may introduce\nredundancy or noise rather than useful context. Please refer to Appendix A.1 for additional ablations.\n6 Conclusion\nWe present VIDEO MOLMO , an LMM for fine-grained spatio-temporal pointing conditioned on\ntextual queries. It leverages a temporal module that incorporates temporal cues from preceding\nframes and a novel bidirectional post-processing strategy for robust mask prediction. To enable\ntraining, we curate a large-scale dataset using a semi-automatic annotation pipeline. VIDEO MOLMO\nshows strong generalization and consistently outperforms state-of-the-art models across diverse and\n9\n--- Page 10 ---\nout-of-distribution tasks, including point grounding, object counting, referring segmentation, and\nreasoning segmentation.\nLimitations and Future Work. VIDEO MOLMO demonstrates strong spatio-temporal grounding\nperformance, excelling in fine-grained localization without explicit pixel-level mask supervision.\nHowever, its performance is relatively limited on videos with fast-moving objects, such as those in\nRefer-Youtube-VOS (Table 1), due to single-frame processing during training—an efficiency-driven\ndesign constrained by architectural and computational limits. Additionally, VIDEO MOLMO relies on\nSAM2, making mask quality dependent on SAM2’s performance from point predictions. Predicting a\nsingle point per object can also yield suboptimal masks (see A.4). Future work may include joint\nmulti-frame training with improved sampling strategies and extending VIDEO MOLMO to predict\nmultiple points per object to enhance mask quality and further benefit downstream applications.\n7 Acknowledgement\nThe computations were enabled by resources provided by NAISS at Alvis partially funded by Swedish\nResearch Council through grant agreement no. 2022-06725, LUMI hosted by CSC (Finland) and\nLUMI consortium, and by Berzelius resource provided by the Knut and Alice Wallenberg Foundation\nat the NSC.\nReferences\n[1]Ali Athar, Xueqing Deng, and Liang-Chieh Chen. Vicas: A dataset for combining holistic and\npixel-level video understanding using captions with grounded segmentation. arXiv preprint\narXiv:2412.09754 , 2024.\n[2]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,\n2025.\n[3] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and\nMike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in\nvideos. Advances in Neural Information Processing Systems , 37:6833–6859, 2024.\n[4]Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu,\nAnush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\ndataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 11621–11631, 2020.\n[5]Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park,\nMohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and\npixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint\narXiv:2409.17146 , 2024.\n[6]Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: A\nlarge-scale benchmark for video segmentation with motion expressions. In ICCV , 2023.\n[7]Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan\nLiao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking.\nInProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n5374–5383, 2019.\n[8]Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world\nin 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 18995–19012, 2022.\n[9]Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human\naction video dataset. arXiv preprint arXiv:1705.06950 , 2017.\n10\n--- Page 11 ---\n[10] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language\nreferring expressions. In Computer Vision–ACCV 2018: 14th Asian Conference on Computer\nVision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part IV 14 , pages\n123–141. Springer, 2019.\n[11] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa:\nReasoning segmentation via large language model. arXiv preprint arXiv:2308.00692 , 2023.\n[12] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-augmented self-supervised tracker. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n6479–6488, 2020.\n[13] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin\nWang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv:2305.06355 , 2023.\n[14] Yihao Li, Jun Yu, Zhongpeng Cai, and Yuwen Pan. Cross-modal target retrieval for tracking by\nnatural language. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW) , pages 4927–4936, 2022.\n[15] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united\nvisual representation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023.\n[16] Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan\nWang, and Mike Zheng Shou. Videogui: A benchmark for gui automation from instructional\nvideos. arXiv preprint arXiv:2406.10227 , 2024.\n[17] Fenglin Liu, Xian Wu, Shen Ge, Xuancheng Ren, Wei Fan, Xu Sun, and Yuexian Zou. Dimbert:\nLearning vision-language grounded representations with disentangled multimodal-attention.\nACM Transactions on Knowledge Discovery from Data (TKDD) , 16(1):1–19, 2021.\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems , 36:34892–34916, 2023.\n[19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations (ICLR) , 2019.\n[20] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating\nimage and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418 ,\n2024.\n[21] Erik Meijering, Oleksiy Dzyubachyk, and Ihor Smal. Methods for cell and particle tracking. In\nMethods in Enzymology , volume 504, pages 183–200. Academic Press, 2012.\n[22] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and\nSalman Khan. Videoglamm: A large multimodal model for pixel-level visual grounding in\nvideos. arXiv preprint arXiv:2411.04923 , 2024.\n[23] Pha Nguyen, Kha Gia Quach, Kris Kitani, and Khoa Luu. Type-to-track: Retrieve any object\nvia prompt-based tracking. Advances in Neural Information Processing Systems , 36:3205–3219,\n2023.\n[24] OpenAI. Chatgpt: Large language model for human-style conversation. https://chat.\nopenai.com , 2023.\n[25] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276 , 2024.\n[26] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A\nbenchmark dataset and evaluation methodology for video object segmentation. In Computer\nVision and Pattern Recognition , 2016.\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning ,\npages 8748–8763. PMLR, 2021.\n11\n--- Page 12 ---\n[28] Frano Raji ˇc, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment\nanything meets point tracking. In 2025 IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV) , pages 9302–9311. IEEE, 2025.\n[29] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham\nCholakkal, Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel\ngrounding large multimodal model. arXiv preprint arXiv:2311.03356 , 2023.\n[30] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,\nHaitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything\nin images and videos. arXiv preprint arXiv:2408.00714 , 2024.\n[31] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object\nsegmentation network with a large-scale benchmark. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16 , pages\n208–223. Springer, 2020.\n[32] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object\nsegmentation network with a large-scale benchmark. In Andrea Vedaldi, Horst Bischof, Thomas\nBrox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020 , pages 208–223, Cham,\n2020. Springer International Publishing.\n[33] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard\nHussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex\nBotev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova,\nAntonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo,\nClément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric\nNoland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk\nMichalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu,\nJustin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel,\nPetko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy,\nRuibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg,\nWojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic\nPeran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis\nHassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins,\nArmand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open\nmodels based on gemini research and technology, 2024.\n[34] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, and Feng\nWu. Towards more flexible and accurate object tracking with natural language: Algorithms\nand benchmark. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 13763–13773, 2021.\n[35] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun\nZheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal\nvideo understanding. arXiv preprint arXiv:2403.15377 , 2024.\n[36] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and\nEfstratios Gavves. Visa: Reasoning video object segmentation via large language models. In\nEuropean Conference on Computer Vision , pages 98–115. Springer, 2024.\n[37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[38] Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jingsong Su, and Jiebo Luo. Grounding-\ntracking-integration. IEEE Transactions on Circuits and Systems for Video Technology ,\n31(9):3433–3443, 2020.\n12\n--- Page 13 ---\n[39] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan\nMurali, Arsalan Mousavian, and Dieter Fox. Robopoint: A vision-language model for spatial\naffordance prediction for robotics. arXiv preprint arXiv:2406.10721 , 2024.\n[40] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong\nLeng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation\nmodels for image and video understanding. arXiv preprint arXiv:2501.13106 , 2025.\n[41] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv:2306.02858 , 2023.\n[42] Haojie Zhao, Xiao Wang, Dong Wang, Huchuan Lu, and Xiang Ruan. Transformer vision-\nlanguage tracking via proxy token guided cross-modal fusion. Pattern Recognit. Lett. , 168:10–\n16, April 2023.\n[43] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 , 2023.\n[44] Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and\nXuansong Xie. Tracking with human-intent reasoning. arXiv preprint arXiv:2312.17448 , 2023.\n[45] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan,\nHao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time\nrecipes for open-source multimodal models. arXiv preprint arXiv:2504.10479 , 2025.\n13\n--- Page 14 ---\nA Appendix\nA.1 Additional Ablations\nA.1.1 Training Ablations\nIn addition to the ablation studies presented in the main paper, we conduct further investigations\ninto the impact of language backbone choice, parameter tuning, and numerical precision on the\nRef-DA VIS dataset, as summarized in Table 7.\nIn the first row, we assess the effect of replacing the Qwen-7B language model with Olmo-7B.\nThe resulting drop in J&Fscore highlights Qwen-7B’s superior grounding capabilities which is\nconsistent with the observational findings reported in [ 5]. This emphasizes the importance of selecting\nan Qwen-7b LLM with strong multimodal alignment for visual grounding tasks.\nTable 7: Performance comparison of different VIDEO MOLMO variants on Refer-DA VIS benchmark.\nVariant J F J&F\nVideoMolmo-O-7B 66.25 72.93 69.59\nVideoMolmo (LoRA) 67.82 74.72 71.27\nVideoMolmo ( 16bit) 67.74 74.72 71.25\nVideoMolmo 71.27 73.63 72.45\nNext, we investigate the impact of end-to-end training. In the second row, we freeze the video encoder\nand train only the LLM’s projection layers by integrating LoRA adapters. This lightweight training\nstrategy significantly underperforms compared to the fully fine-tuned model (last row), validating our\nhypothesis that joint optimization of all components is essential for capturing the temporal nuances\nrequired for precise point grounding.\nFinally, we examine the effect of training precision. In the third row, we use 16-bit floating point\nprecision which is commonly adopted to save memory and accelerate training. However, we find that\nthis leads to a notable degradation in performance. In contrast, training with full 32-bit precision (last\nrow) enhances the model’s capacity to learn fine-grained spatial and temporal cues, consistent with\nprior observations in [5].\nTogether, these ablations underline the significance of careful backbone selection, full end-to-end\noptimization, and high-precision training for achieving robust and fine-grained visual grounding in\nVideoMolmo.\nA.1.2 Inference Ablations\nSampling rate k:As described in the main paper, we adopt a frame sampling rate of k=|T |for\nthe Molmo+SAM2 baseline during inference, which means that we take the first frame prediction\nand use SAM2 to propagate the mask across all the frames. This choice is motivated by performance\non the Refer-DA VIS-17 dataset, where Molmo+SAM2 achieves its highest J&Fscore of 67.69at\nthis value. However, our analysis in Table 8 reveals that the optimal sampling rate is not universal, it\nvaries across datasets. To ensure a fair and competitive comparison with our proposed VideoMolmo,\nwe conduct additional ablations on the sampling rate kfor Molmo+SAM2 across the Refer-YouTube-\nVOS and MeViS datasets. We find that a lower sampling rate of k= 3yields the best performance\non Refer-YouTube-VOS, while k= 1proves optimal on MeViS. Despite this tuning, VideoMolmo\nconsistently outperforms Molmo+SAM2 under each dataset’s optimal configuration. Interestingly,\nacross all three datasets, we observe a consistent decline in performance as the sampling rate increases.\nThis is particularly evident at k= 30 , where the baseline performs starts dropping. These findings\nfurther highlight the robustness of VideoMolmo in leveraging temporal context, even when competing\nbaselines are tuned to their best-performing configurations.\nWe further ablate the effect of sampling rate on our proposed VideoMolmo. While our main results\non the Refer-YouTube-VOS benchmark in the main paper are reported using a sampling rate of k= 5,\nwe acknowledge that this choice, although consistent with the baseline configuration, may not be\noptimal for our method. As seen from the Table 10, VideoMolmo benefits from careful selection of\n1\n--- Page 15 ---\nTable 8: Effect of sampling rate kon Refer-DA VIS-17, Refer-Youtube-VOS, and MeViS benchmarks\nusing Molmo + SAM2\nk Refer-DA VIS-17 Refer-Youtube-VOS MeViS\nJ F J &F J F J &F J F J &F\n1 58.32 64.60 61.46 60.08 64.69 62.38 45.53 51.06 48.30\n3 58.77 63.85 61.31 60.11 65.04 62.58 – – –\n10 59.61 64.77 62.19 60.24 64.88 62.56 45.63 50.93 48.28\n30 63.31 69.31 66.31 59.48 64.02 61.75 45.51 50.65 48.08\n|T | 65.29 70.09 67.69 59.48 64.07 61.78 44.37 49.37 46.87\nTable 9: Effect of varying threshold τon\nthe performance of VIDEO MOLMO evalu-\nated on the Refer-DA VIS benchmark.\nτ J F J &F\n0 67.31 73.38 70.34\n0.3 69.05 75.51 72.28\n0.5 68.90 75.26 72.08\n0.9 68.85 75.24 72.05\nVideoMolmo (τ= 0.7) 71.27 73.63 72.45Table 10: Effect of sampling rate kon the per-\nformance of VIDEO MOLMO evaluated on the\nRefer-YouTube-VOS benchmark.\nk J F J &F\n3 64.39 67.68 66.03\n10 65.69 69.39 67.54\n15 66.26 69.95 68.11\n20 66.34 69.93 68.14\n30 65.80 69.32 67.56\nVideoMolmo ( k= 5) 65.55 69.11 67.33\nsampling rate as we observe that a sampling rate k= 20 yields the highest J&Fscore of 68.14,\ncompared to 67.33withk= 5.\nThreshold τ:Our proposed post-processing strategy, Bidirectional Temporal Mask Fusion , enhances\nmodel performance by combining masks propagated from both right and left directions to achieve\na robust temporal consensus. As described in Section 3.3 of the main paper, the fusion process\nis governed by a threshold hyperparameter τ, which determines how agreement between the two\nmasks is evaluated. Specifically, when the Intersection-over-Union (IoU) between the forward and\nbackward masks exceeds τ, their intersection is used as the final mask, enforcing stricter agreement.\nConversely, if the IoU falls below τ, their union is taken, promoting flexibility in ambiguous regions.\nThis mechanism balances precision and recall based on temporal consistency. We ablate different\nvalues of τin Table 9 to identify the most effective setting. The results indicate that τ= 0.7\nyields the best overall performance. However, the differences across values are relatively minor,\nunderscoring the high quality and stability of the point predictions generated by VideoMolmo. This\nconsistency highlights the robustness of our model in temporal point grounding, even under varying\npost-processing thresholds.\nEffect of Post-processing on baselines: To assess the generalizability and effectiveness of our\nproposed Bidirectional Temporal Mask Fusion , we integrate it with the Molmo+SAM2 baseline,\nresulting in an enhanced variant denoted as Molmo†+SAM2. As illustrated in Fig. 7, this integration\nconsistently improves performance across all three datasets in terms of J&Fscore. These results\ndemonstrate that our post-processing strategy not only strengthens our own model but also benefits\nexisting methods. The modular, plug-and-play nature makes it a valuable addition to any video\ngrounding pipeline, improving temporal consistency and overall segmentation quality with minimal\nintegration effort.\nA.2 Additional Experimentation Details\nVIDEO MOLMO follows the architecture of Molmo [ 5]. For the image encoder, we use a pretrained\nCLIP ViT-L/14 (336 ×336) [ 27] model. We initialize the temporal module with Xavier Normalized\nweights for stable training. Our choice of LLM is the pretrained Qwen2 7B [ 37]. We train the model\non8NVIDIA A100 80GB GPUs. Learning rate of 1e−5is used for the LLM, and 5e−6for the\nviion encoder, visual projector and temporal module. We adopt a batch size of 1with 256gradient\naccumulation steps, and use AdamW optimizer with β= (0.9,0.95)andϵ= 1e−6, . We train\nVIDEO MOLMO for4epochs on 8NVIDIA A100 GPUs (80 GB each), consuming roughly 1,000\nGPU-hours in total. Training runs in full 32-bit precision with a 10-step linear warmup, after which\n2\n--- Page 16 ---\nMeViS YT-VOS Ref-DAVIS\nDatasets304050607080&\nMolmo+SAM2\nMolmo+SAM2\nFigure 7: Effect of Bidirectional temporal mask fusion on Molmo+SAM2 baseline.\nwe follow a cosine learning-rate schedule; we also clip gradients to a maximum norm of 1.0to guard\nagainst unstable updates. For all inference and reported results, we use 4-bit precision.\nA.3 VPoS Bench\nAs mentioned in the main paper, we introduce Video Pointing and Segmentation (VPoS-Bench), a\ncurated benchmark test set comprising of 100video-caption pairs and 1kmanually annotated object\npoints. To obtain mask annotations for evaluation, we use SAM2 to convert these point annotations\ninto segmentation masks. For mask-based evaluations, we employ the SAM2 model to convert these\npoint annotations into segmentation masks. The test benchmark encompasses diverse real-world\nscenarios, sourced from both open datasets [ 4,16,8] and internal collections, spanning five categories:\nCell Tracking, Egocentric Videos, Autonomous Driving, Video-GUI, and Robotics. Our benchmark\nalso consists of a dedicated counting task sourced from [ 9] dataset. Below, we present details about\neach subset in VPoS-Bench.\n1) Cell Tracking: Features internally sourced 12 microscopic videos with dynamic cellular structures,\nwhere precise localization of individual cells is essential for tasks like tracking cell division or\ncounting.\n2) Egocentric Videos: Comprises 18 first-person videos capturing daily human-object interactions,\nenabling the assessment of grounded pointing in scenarios such as object manipulation and activity\nrecognition. The egocentric videos in our test benchmark are derived from [8] dataset.\n3) Autonomous Driving: Includes 13 urban driving scenes from nuScenes’s dataset [ 4], with complex\nenvironments, requiring accurate identification of specific road elements (e.g., traffic signals) to\nsupport navigation and safety systems.\n4) Video-GUI: Consists of 13 screen recordings from software applications, focusing on tasks like\nidentifying and interacting with user interface elements based on textual instructions. The VideoGUI\nvideos are sampled from VideoGUI dataset [16].\n5) Robotics: Encompasses 14 videos of robotic operations, emphasizing the need for precise object\nlocalization to execute commands such as \"pick up the red block\" or \"press the top button.\" The\nrobotic videos in our benchmark are sourced internally.\nA.4 Additional qualitative results\nGeneral qualitative results. We also present some qualitative results in Figures 8,9,10, 11, and\n12 on our proposed VPoS-Bench, MeViS, YT-VOS, Ref-DA VIS, and ReasonVOS, respectively.\nWe observe that in each case, VideoMolmo generates fine-grained points and corresponding masks\npertaining to the query objects. In fact, VideoMolmo performs well even in the cases of multi-object\n3\n--- Page 17 ---\nqueries (>2 objects) such as in VPoS-Bench counting task of Fig. 8 (1strow) and Fig. 9 (3rdrow).\nFurther, VideoMolmo also excels at grounding small and fine-grained objects. Fig. 8 (3rdrow) shows\nVideoMolmo accurately points and grounds the far-away car on the road, although the car is too small\nto point at in some frames. Similarly, VideoMolmo is able to ground the helmet in Fig. 11 (2ndrow)\nwhile avoiding to ground the entire biker.\nFailure cases. While VideoMolmo demonstrates strong fine-grained pointing capabilities, it is not\nwithout limitations. As illustrated in Fig. 13, certain failure cases highlight areas for improvement.\nIn the first row, the model is expected to point to the black harness but instead grounds a part of the\nadjacent bag. This misalignment stems from the limitations of SAM2, which struggles to accurately\nconvert the predicted point coordinate into a meaningful mask. In the second row, the model points to\nonly one of several visible paraglider lines, missing multiple lines. Such cases suggest a need for\nenhanced expressiveness, such as enabling the model to predict multiple points for a single query.\nAddressing these limitations opens new avenues for future work in improving the robustness and\ngranularity of point grounding in complex scenes.\nPoint to everyone wearing a green shirt .\nPoint to the boat in front .\nPoint to the car in the front .\nPoint to the moving car .\nFigure 8: VPoS-Bench qualitative examples.\n4\n--- Page 18 ---\nPoint to the horse running around.\nPoint to the cat going from right to left.\nPoint to the tigers circling around the bus.\nFigure 9: MeVis qualitative examples.\n5\n--- Page 19 ---\nPoint to a yellow umbrella .\nPoint to a ball\nPoint to a child walking to the bus with an adult\nFigure 10: Refer-YouTube-VOS qualitative examples.\nPoint to person at the back  of the go-cart without a helmet.\nPoint to the helmet worn by the biker.\nPoint to a rope which the guy is hanging on.\nFigure 11: Refer-DA VIS qualitative examples.\n6\n--- Page 20 ---\nPoint to who disturbs the dog  from moving forward\nsmoothly?\nPoint to the vehicle that can accommodate more \npassengers.\nPoint to the musician playing drums .Figure 12: ReasonVOS qualitative examples.\nPoint to a black harness  with an airbag.\nPoint to the paraglider lines .\nFigure 13: Qualitative failure cases of V IDEO MOLMO .\n7",
  "text_length": 58074
}