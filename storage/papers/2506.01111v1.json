{
  "id": "http://arxiv.org/abs/2506.01111v1",
  "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
  "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
  "authors": [
    "Shunian Chen",
    "Xinyuan Xie",
    "Zheshu Chen",
    "Liyan Zhao",
    "Owen Lee",
    "Zhan Su",
    "Qilin Sun",
    "Benyou Wang"
  ],
  "published": "2025-06-01T18:29:17Z",
  "updated": "2025-06-01T18:29:17Z",
  "categories": [
    "cs.SD",
    "cs.AI",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01111v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01111v1  [cs.SD]  1 Jun 2025FusionAudio-1.2M: Towards Fine-grained Audio\nCaptioning with Multimodal Contextual Fusion\nShunian Chen1∗Xinyuan Xie1,2∗Zheshu Chen1∗\nLiyan Zhao1Owen Lee1Zhan Su1Qilin Sun1Benyou Wang1†\n1The Chinese University of Hong Kong, Shenzhen\n2South China University of Technology\nwangbenyou@cuhk.edu.cn\nAbstract\nHigh-quality, large-scale audio captioning is crucial for advancing audio under-\nstanding, yet current automated methods often generate captions that lack fine-\ngrained detail and contextual accuracy, primarily due to their reliance on limited\nunimodal or superficial multimodal information. Drawing inspiration from hu-\nman auditory perception, which adeptly integrates cross-modal cues and performs\nsophisticated auditory scene analysis, we introduce a novel two-stage automated\npipeline. This pipeline first employs specialized pretrained models to extract\ndiverse contextual cues (e.g., speech, music, general sounds, and visual informa-\ntion from associated video). A large language model (LLM) then synthesizes\nthese rich, multimodal inputs to generate detailed and context-aware audio cap-\ntions. Key contributions of this work include: (1) the proposed scalable method\nfor fine-grained audio caption generation; (2) FusionAudio, a new large-scale\ndataset comprising 1.2 million such detailed captions, combined with 6 million QA\npairs; and (3) enhanced audio models developed using FusionAudio, specifically\na CLAP-based audio encoder with superior audio-text alignment and instruction\nfollowing. This paper paves the way for more nuanced and accurate automated\nunderstanding of complex audio environments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio .\n1 Introduction\nThe advancement of models like CLAP [1] for audio retrieval, and GAMA [2] or Qwen2-Audio [3]\nfor broader audio understanding, heavily relies on large-scale, high-quality audio captioning datasets.\nAudio captioning has primarily followed two trajectories. Manual annotation [4,5] offers high\nquality but lacks scalability due to high labor costs. In contrast, automated methods [6,7] often\nuse sparse metadata like text labels or tags to assist annotation, while others [ 8,9,10] leverage\nbasic multimodal cues. These automated approaches, however, typically rely on limited textual or\nsuperficial information, failing to capture rich details (e.g., multimodal contextual details). This\nresults in captions that lack fine-grained details and are prone to hallucinations [ 11], hindering\nnuanced audio interpretation.\nAddressing this gap necessitates a paradigm shift. We turn to human auditory perception for inspira-\ntion (Figure 1). Human auditory understanding leverages sophisticated strategies at two complemen-\ntary levels. Firstly , humans adeptly integrate cross-modal cues—visual information, for instance,\naids speech intelligibility [ 12] and sound identification [ 13,14].Secondly , auditory scene analysis\n∗Equal contribution.\n†Corresponding author\nPreprint. Under review.\n--- Page 2 ---\nTable 1: Comparison of generated captions for a sample audio clip with associated visual context.\nHallucinations in prior work are highlighted in red. Improvements from our multimodal approach,\nFusionAudio, are highlighted in green, demonstrating enhanced accuracy and detail by leveraging\nvisual and comprehensive auditory cues.\nMethod Audio Caption\nGAMA (Baseline) The audio is dominated by the sound of a motor vehicle engine and intermit-\ntent male speech, with wind noise.\nAudioSetCaps A male and female engage in conversation, their voices audible against a\nbackdrop of ambient noise. The discussion is neutral in tone and does not\ninvolve any identifiable objects or language.\nAuto-ACD A man speaks while a vehicle moves in the distance, possibly on a motor\nscooter, in an engine room.\nSound-VECaps A man is speaking and a vintage motorcycle with a large headlamp, round\nfuel tank, and sidecar is parked on grass, with the sound of the engine and the\nman’s voice filling the air, while a vehicle passes by in the background.\nFusionAudio-1.2M (Ours) Continuous motor vehicle engine noise is prominently featured, accompanied\nby intermittent male speech with a positive or confirming tone. Wind sounds\nsuggest an outdoor environment, with the engine’s sustained roar maintaining a\nsteady volume throughout the recording.\n(ASA) [ 15] allows the auditory system to parse complex soundscapes into distinct streams like\nspeech, music, and ambient sounds based on temporal-spectral regularities [ 16]. These sophisticated\nbiological mechanisms offer a compelling blueprint for enhancing automated audio captioning. The\nimpact of this multimodal integration is demonstrated in Table 1. Current systems, often processing\naudio in isolation, can misinterpret sounds (e.g., a stationary motorcycle as a moving scooter) or\nhallucinate details. In contrast, FusionAudio-1.2M leverages comprehensive audiovisual cues to\nproduce more accurate and contextually rich descriptions.\nMusic Org.\nSpeech Org.\nAudio Org.\nVisual Org.\nFusion Org.Sensory \nInput\nAcoustic \nUnderstanding\nFigure 1: Human auditory percep-\ntion integrates multisensory cues.Inspired by these principles, we introduce a two-stage pipeline\nfor enhanced automated audio captioning. First, specialized pre-\ntrained models extract diverse contextual cues: an Automatic\nSpeech Recognition (ASR) model [ 17] for speech, a music under-\nstanding model [ 18] for musical attributes, an audio understand-\ning model [ 2] for general sounds, and a visual model [ 19] for\nvideo information. Second, a large language model (LLM) [ 20]\nacts as an integration engine, synthesizing these multimodal cues\nto generate fine-grained audio captions. This synthesis of rich,\ncross-modal context by an LLM aims to improve detail and ac-\ncuracy, addressing prior limitations.\nOur contributions are:\n•Automated fine-grained audio captioning: A pipeline using specialized unimodal models\nto extract diverse contextual cues, synthesized by an LLM to generate detailed, scalable\ncaptions.\n•FusionAudio-1.2M dataset: A large-scale dataset of 1.2M fine-grained audio captions to\nadvance audio research.\n2\n--- Page 3 ---\nTable 2: Comparison of open-source audio caption datasets.\nName Year # of Audio/QA Avg. Dur (s) Avg. Text Len Visual Music Speech Integration\nAudioCaps [5] 2019 46k/46k 10.00 9.03 ✗ ✗ ✗ ✗\nClotho [4] 2019 5k/5k 22.50 11.00 ✗ ✗ ✗ ✗\nLAION-Audio-630K [6] 2022 630k/630k 24.58 7.30 ✗ ✗ ✗ ✗\nWavCaps [7] 2024 403k/403k 67.59 7.80 ✗ ✗ ✗ ✗\nAudioSetCaps [8] 2024 1.9M/1.9M N/A 28.00 ✗ ✗ ✗ ✗\nAuto-ACD [9] 2024 1.5M/1.5M 10.00 18.10 ✓ ✗ ✗ ✓\nCompA-R [2] 2024 62k/200k 9.93 18.00 ✓ ✗ ✗ ✓\nFusionAudio-1.2M (Ours) 2025 1.2M/6M 10.00 47.18 ✓ ✓ ✓ ✓\n•Multimodal cue-enhanced audio models: A CLAP-based audio encoder with improved\naudio-text alignment, and an instruction-tuned MLLM with stronger audio comprehension\nand instruction-following.\n2 Related Works\n2.1 Audio Language Learning\nThe field of audio-language models has seen significant advancements in recent years, with researchers\nfocusing on developing models that can effectively process, understand, and reason about sounds\nusing natural language as a supervision signal. Early works like CLAP Learning Audio Concepts\nfrom Natural Language Supervision [ 21] laid the foundation for contrastive learning approaches\nin audio-language pre-training. Subsequent studies have explored various pre-training objectives,\nincluding generative and discriminative methods, to enhance audio representation learning and\ncross-modal alignment. For instance, CTAL [ 22] and FLAP [ 23] have investigated masked language\nmodeling and cross-attention based masked acoustic modeling for joint representation learning of\naudio and language modalities. Multi-task learning approaches have also gained attention, with\nmodels like UniAudio [24] and SpeechX [25] demonstrating the potential of unifying diverse audio\ntasks under a single framework. Furthermore, the integration of large language models (LLMs)\nwith audio processing has opened new avenues for creating more powerful and human-like audio\nunderstanding systems. Recent works such as Pengi [ 26], Qwen-audio [ 27], and Audio Flamingo [ 28]\nhave shown impressive capabilities in handling complex audio tasks through instruction tuning and\nin-context learning. These advancements highlight the growing importance of audio-language models\nin bridging the gap between auditory information and language understanding, and their potential for\nreal-world applications.\n2.2 Audio Captioning\nEarly audio captioning research relied on manually annotated datasets like AudioCaps [ 5] and\nClotho [ 4], which provided high-quality descriptions but were inherently limited in scale. To address\nthis, the field increasingly adopted automated and weakly-supervised methods . These leverage\nlarge-scale web-sourced audio with associated sparse metadata (e.g., WavCaps [ 7], LAION-Audio-\n630K [ 6]), employ existing textual tags to guide generation, or incorporate basic multimodal cues\nfrom loosely associated content [ 8,9,10]. While significantly improving scalability, these automated\ntechniques typically yield captions lacking the fine-grained detail and rich contextual understanding\ncharacteristic of human annotations or, as our work posits, achievable through more sophisticated,\ndeeply integrated multimodal information processing. As shown in Table 2, Our FusionAudio-1.2M\ndataset offers finer-grained captions than existing audio caption dataset, see the average text length.\n3 Method: Fine-grained Audio Caption with Multimodal Contextual Fusion\n3.1 Automated Captioning Pipeline\nWe introduce a two-stage pipeline, illustrated in Figure 2, designed to generate fine-grained audio\ncaptions: (1) Multimodal Contextual Cue Extraction using specialized expert models, and (2) LLM-\nDriven Contextual Synthesis to integrate these diverse cues into a coherent caption. An initial\npre-processing step is performed to enhance audio quality.\n3\n--- Page 4 ---\nVideoAudio Audio Track \nSeparationASR Model\nAudio Caption \nModel\nMusic Caption \nModelV ocal\nAudio\nMusic\nVideo Caption \nModelLLM -Driven Contextual \nSynthesisSpeech\nAudio Caption\nMusic Caption\nVideo CaptionQuality FilterIntegrated \nCaption\nAudio\nFinal CaptionMultimodal Contextual Cue Extraction\nAudio Event \nDetectionFigure 2: Overview of our proposed multimodal audio captioning pipeline. The process involves\ninitial vocal separation, followed by a two-stage approach: multimodal contextual cue extraction and\nLLM-driven contextual synthesis.\nPre-processing: Audio Track Separation. To enhance the quality and specificity of downstream\nanalyses, particularly for speech and distinct background sounds, we first apply a source separa-\ntion technique. We employ the Demucs model [ 29] to isolate the vocal track from the non-vocal\ncomponents (e.g., music, environmental sounds) within the audio stream.\nStage 1: Multimodal Contextual Cue Extraction. This stage leverages a suite of specialized\nmodels to extract diverse, complementary information streams relevant to the auditory scene. The\nprompts used for these models can be found in Appendix B.\n•General Audio Events: To capture overall acoustic scene characteristics, we utilize\nGAMA [2] to generate descriptive captions focusing on sound events and environments.\n•Speech Content: The separated vocal stream is transcribed using the Whisper model [17].\n•Music Characteristics: For clips potentially containing music, we first employ YamNet [ 30]\nas a classifier to confirm the presence of music, mitigating hallucination risk on non-musical\nsegments. If music is detected, OpenMu [ 18] is used to extract details regarding genre,\ninstrumentation, tempo, and mood.\n•Visually-Grounded Context: We utilize the Qwen2.5-VL-72B vision-language model [ 19]\nto extract visual information from the video stream. This approach yields a detailed,\ntimestamped visual record, providing visual context that aids in grounding physical events.\nStage 2: LLM-Driven Contextual Synthesis. The extracted information streams serve as in-\nput to the synthesis model, QwQ-32B [ 20]. The LLM acts as a central integration engine. It is\nprompted to: (a) synthesize the multimodal inputs coherently, (b) resolve potential redundancies or\nminor inconsistencies across the different expert outputs, (c) infer relationships and context implied\nby the combined information, and (d) generate a final, fine-grained audio caption that reflects a\ncomprehensive understanding of the auditory scene enriched by multimodal context.\n3.2 Data Source\nWe utilize the AudioSet dataset [ 31] as the primary source material. AudioSet provides over 2 million\n10-second YouTube video clips, each weakly annotated with audio event labels. We downloaded the\ncorresponding audio and video streams for processing through our pipeline.\n3.3 Data Quality Assurance\nTo ensure the quality and reliability of the automatically generated captions, we implement a multi-\nfaceted quality assurance protocol. This process involved both manual verification on a sample of the\ndata and scalable automated filtering (described subsequently) to curate the final FusionAudio-1.2M\ndataset.\nManual Verification. To establish a benchmark for caption quality, we randomly sampled 300\ngenerated captions for human evaluation. Trained annotators assessed each caption based on two\ncriteria: (1) Detailness: Rated on a 3-point scale, a higher score means more details, evaluating the\nrichness and specificity of the information conveyed. (2) Hallucination: Rated on a 5-point scale,\n4\n--- Page 5 ---\nMusic55.7% Speech33.9%Guitar\n4.3%Vehicle\n4.1%Narration, monologue\n1.9%(a) Top 5 Audio Labels Distribution\n0 10 20 30 40 50 60 70 80\nCaption Length0.0000.0250.0500.0750.1000.1250.1500.1750.200PercentageFusionAudio\nAudioSetCaps\nAuto-ACD\nSound-VECaps(b) Caption Length Distribution\nInstrument Emotion Music Genre\nCategories0123Average CountFusionAudio\nAudioSetCaps\nAuto-ACD\nSound-VECaps (c) Diversity of Object Types\n0.1 0.2 0.3 0.4 0.5 0.6\nClap Score0.0000.0050.0100.0150.020Percentage\n(d) CLAP Score Distribution\nAudio Video Speech Music\nCaption Type0.00.20.40.60.81.0Usage Percentage (e) Modal Usage Frequency\nFigure 3: Key statistics of FusionAudio-1.2M: (a) Proportion of top 5 audio labels from AudioSet;\n(b) Caption length comparison with existing datasets; (c) Diversity of semantic content types; (d)\nProportion of captions utilizing different modalities; (e) Distribution of audio-text similarity measured\nby CLAP.\na higher score means less hallucination, assessing the factual accuracy of the caption against the\naudio-visual content. A score of ≤2was considered indicative of notable hallucination. The detailed\nannotation guidelines and scoring rubrics are provided in Appendix A.\nAs shown in Table 3, the manually evaluated sample achieved a mean detailness score of 2.55 (out of\n3). For hallucination, the average score given by human evaluator is 3.74, with 7% of the evaluated\ncaptions received a score of 2 or less, indicating a low prevalence of significant inaccuracies in this\nsample. The inter-annotator agreement, calculated using the exact match rate, was 0.67 for detailness\nand 0.79 for hallucination. These rates suggest moderate agreement between annotators, which is\nreasonable given the subjective nature of fine-grained caption quality assessment. The full distribution\nof scores for both metrics can also be found in Appendix A.\nTable 3: Manual Verification Results. Detailness\nis rated 1-3 (higher is better). Hallucination is\nrated 1-5 (higher is better; ≤2indicates notable\nhallucination). IAA is measured using the exact\nmatching, before which hallucination score has\nbeen converted to 1 (score ≤2) or 0 (score > 2).\nCaption Content Quality Inter-Annotator Agreement\nDetailness Hallucination Detailness Hallucination\n2.55 3.74 0.67 0.79Automatic Filtering To scale quality assess-\nment to the entire dataset, we leveraged the\nCLAP model to automatically filter low-quality\ncaptions. We computed cosine similarity be-\ntween CLAP-generated audio and caption em-\nbeddings as a quality indicator. Based on our\nhuman evaluation, we categorized hallucination\nscores ≤2as the positive class (captions to dis-\ncard) and scores >2as the negative class (cap-\ntions to retain). We then evaluated various co-\nsine similarity thresholds using the F1.05score,\nwhich slightly emphasizes recall to prioritize re-\nmoving hallucinated content (see Appendix A.3\nfor computation details).\nA threshold of 0.08 achieved optimal alignment with human judgments, with the exact match rate\nbeing 88.3%. This threshold resulted in a 7.3% filter rate and balanced false positives and negatives.\nWe applied this validated threshold to filter the entire caption set, yielding the final 1.2 million\nhigh-quality captions in the FusionAudio-1.2M dataset.\n4 The Resulted Dataset: FusionAudio-1.2M\n4.1 Quantitative Analysis\nTable 2 compares our proposed dataset with other publicly available datasets. FusionAudio-1.2M dis-\ntinguishes itself through its large scale, longer caption length, and integration of multiple modalities.\n5\n--- Page 6 ---\nDataset Statistics We analyze FusionAudio-1.2M across several dimensions:\n•Audio Category Distribution: Figure 3a shows the top five audio categories from Au-\ndioSet [31], with \"Music\" being the most prevalent.\n•Caption Length: Figure 3b compares caption lengths (in tokens) with AudioCaps [ 5],\nSound-VECaps [ 10] and Auto-ACD [ 9]. FusionAudio-1.2M captions are significantly\nlonger, indicating greater descriptive richness.\n•Semantic Diversity: To showcase and compare the richness of semantic information\nacross different datasets, we identified the presence of instruments ,emotions , and music\ngenres in each caption using GPT-4o-mini (prompts in Appendix B.3). Figure 3c shows\nFusionAudio-1.2M has higher coverage across most categories.\n•Audio-Text Alignment: Figure 3d shows the distribution of cosine similarity between audio\nand text embeddings calculated by CLAP [ 1]. Samples of different similarity scores can be\nfound in Appendix C.2.\n•Modality Usage: To better understand the contribution of different modalities to the final\ncaption content, we use GPT-4o-mini to automatically annotate each caption for explicit\nreferences to different modalities: audio events ,speech ,music , and visual context (see\nprompt in Appendix B.3). As shown in Figure 3e, over 50% of samples integrate information\nfrom two or more modalities, demonstrating effective multimodal fusion.\n4.2 Qualitative Analysis\n60\n 40\n 20\n 0 20 40 60\nt-SNE Component 140\n30\n20\n10\n010203040t-SNE Component 2\n(a) FusionAudio\n60\n 40\n 20\n 0 20 40 60\nt-SNE Component 140\n30\n20\n10\n010203040t-SNE Component 2 (b) AudioSetCaps\n60\n 40\n 20\n 0 20 40 60\nt-SNE Component 140\n30\n20\n10\n010203040t-SNE Component 2 (c) Auto-ACD\n60\n 40\n 20\n 0 20 40 60\nt-SNE Component 140\n30\n20\n10\n010203040t-SNE Component 2 (d) Sound-VECaps\nMusic Speech Vehicle\nFigure 4: T-SNE Embedding of popular categories between different datasets\nCase Study To further highlight the qualitative improvements enabled by FusionAudio, Table 1\ncompares captions generated for the same audio clip across different datasets. FusionAudio’s caption\nnot only describes the primary sound event but also integrates visual cues, inferred context, and\nemotional tone, demonstrating a level of detail and reasoning absent from prior datasets. More\nsamples can be found in Appendix C.\nEmbedding Projection for Visualizing Semantic Granularity Embedding projection techniques\nlike t-SNE [ 32] visually reveal a dataset’s semantic structure, illustrating intra-class compactness and\ninter-class separability—key indicators of data quality for discriminative tasks. We applied this to\nFusionAudio-1.2M by projecting CLAP sentence embeddings of its captions and those from baseline\ndatasets into two dimensions using t-SNE. Figure 4 demonstrates that FusionAudio’s captions form\nsignificantly more compact same-category clusters and exhibit greater separation between different\ncategories compared to baselines. This visual evidence indicates FusionAudio’s superior semantic\ngranularity and discriminative power, beneficial for nuanced audio understanding and cross-modal\nretrieval. Quantitative validation of inter- and intra-class distances is in Appendix D.1.\n5 Applications of FusionAudio-1.2M\nWe use FusionAudio-1.2M for two popular downstream tasks: audio-text retrieval in Sec. 5.1 and\naudio understanding in Sec. 5.2. All experiments were conducted on a server equipped with 8\nNVIDIA A800 80GB GPUs. Fine-tuning for the main experiments and ablation studies on the 25K\ndata subset typically saved a checkpoint in under 30 minutes per run. The full evaluation process\nacross all benchmark tasks required approximately 5 hours to complete per model evaluation. For the\nscaling study involving LLM fine-tuning, training sessions lasted between 10 to 12 hours.\n6\n--- Page 7 ---\n5.1 Audio-text Retrieval\nTable 4: Audio-text retrieval performance (R@k, %) on the AudioCaps test set.\nDataset ModelText - to - Audio Audio - to - TextAvg.\nR@1 R@5 R@10 R@1 R@5 R@10\nAC+CL HTSAT+BERT 36.1 71.8 83.9 46.8 82.9 90.7 68.7\nWavCaps HTSAT+BERT 42.2 76.5 87.1 54.6 85.2 92.4 73.0\nAudioSetCaps HTSAT+BERT 43.4 78.4 88.2 57.3 84.2 93.2 74.1\nAuto-ACD HTSAT+RoBERTa 42.7 - 88.5 56.3 - 93.9 -\nSound-VECaps HTSAT+RoBERTa 39.2 74.1 85.0 54.0 82.5 93.2 71.3\nFA(Ours) HTSAT + BERT 44.3 79.9 90.4 57.8 86.1 94.4 75.5\n5.1.1 Experimental Setup\nTasks and Models We evaluate the quality of FusionAudio-1.2M by assessing its effectiveness\nas a pre-training corpus for the downstream task of cross-modal audio-text retrieval. This task\nrequires retrieving the most relevant audio clip for a given textual query (text-to-audio retrieval)\nand, conversely, identifying the most pertinent text description for a given audio input (audio-to-text\nretrieval). For all experiments, we employ the HTSAT [33]-BERT [34] model architecture.\nTwo-Stage Training Our training methodology for all evaluated datasets, including FusionAudio-\n1.2M and the baselines, follows a consistent two-stage protocol:\n•Pre-training: The HTSAT-BERT model is first pre-trained on the entirety of the respective\nsource dataset (e.g., FusionAudio-1.2M, WavCaps, etc.). This stage utilizes a contrastive\nlearning objective. For pre-training, the learning rate is set to 5e-5, the batch size is\n196,training proceeds for 15 epochs.\n•Fine-tuning: Subsequently, the pre-trained model undergoes full-parameter fine-tuning on\nthe official training split of the AudioCaps (AC) dataset [ 5]. For this fine-tuning stage, we\nuse a learning rate of 1e-5, a batch size of 196, and train for 20 epochs.\nEvaluation Setting The performance of all models, after the two-stage training protocol, is evalu-\nated on the official test set of the AudioCaps dataset [ 5]. We report Recall@k (R@k) for k={1, 5,\n10} for both text-to-audio and audio-to-text retrieval directions. R@k quantifies the percentage of\nqueries for which the ground-truth item is successfully retrieved within the top-k ranked results. The\ncomparative results are presented in Table 4.\n5.1.2 Performance Analysis\nThe detailed comparison results of model evaluation are presented in Table 4. The models trained\non our dataset have significant advantages in the recall metrics of audio and text,which achieves the\nhighest score in each R@k among models trained by existing audio caption datasets.The excellent\naudio and text recall performance indicates that the audio captions of FusionAudio-1.2M can accu-\nrately capture the information in the audio, ensuring the model’s ability to distinguish fine-grained\ninformation. As a result, it can achieve high-accuracy matching even when dealing with similar\naudio.\n5.2 Audio Understanding\nTo empirically validate the practical utility and superior quality of our proposed FusionAudio-1.2M\ndataset, we evaluated its impact on a comprehensive suite of audio understanding tasks. Specifically,\nwe benchmarked the performance of the GAMA model [ 2] fine-tuned on FusionAudio-1.2M against\ninstances of the same model fine-tuned on several established audio captioning datasets.\n7\n--- Page 8 ---\nTable 5: Performance comparison of the GAMA model fine-tuned on FusionAudio-1.2M against\nbaseline datasets across a battery of audio understanding evaluation benchmarks. All models were\nfine-tuned on 25,000 QA pairs. M.J. denotes Model-Judge score (using GPT-4.1-mini). Best scores\nare in bold. The three main categories of evaluation tasks align with those in Table 9.\nDatasetAdverse Acoustic Conditions High-Level Semantic Understanding Fine-grained Information\nAS\n(Acc.)US8k\n(mAP)TAU\n(mAP)FSD ns\n(mAP)Avg.Genre\n(Acc.)MAQA\n(Acc.)Mood\n(Acc.)Mchat\n(M.J)SAQA\n(Acc.)Schat\n(M.J)AB Sc\n(M.J)Avg.V ocal\n(Acc.)Instr\n(Acc.)ESC\n(Acc.)FSD\n(mAP)Avg.\nGAMA(base) 48.0 56.6 23.5 81.9 52.5 42.8 44.1 28.3 45.4 50.1 58.9 56.0 46.5 63.5 68.7 68.9 45.8 61.7\nAC+CL 50.3 65.3 21.3 81.9 54.7 49.4 50.7 28.4 47.0 52.3 55.4 61.3 49.2 68.2 68.9 65.7 39.9 60.8\nWavCaps 55.4 64.5 25.0 77.6 55.6 53.4 51.6 33.2 27.7 45.1 29.7 52.7 41.9 55.4 69.7 58.8 32.4 54.1\nASC 45.4 51.3 22.3 77.8 49.2 56.0 57.6 31.6 51.3 49.9 59.5 58.5 52.1 51.8 70.5 57.7 30.5 52.6\nCompA-R 56.5 63.3 22.7 83.7 56.6 60.1 54.7 33.9 47.0 56.1 58.3 60.1 52.9 63.5 68.6 62.3 38.4 58.2\nFA(ours) 59.0 58.8 24.4 84.6 56.7 65.1 57.6 35.7 57.1 59.1 61.5 64.5 57.4 69.0 73.6 65.5 44.5 63.0\nFA-high(ours) 59.7 64.0 25.1 88.2 59.3 64.2 60.0 38.3 57.9 58.4 62.3 64.0 57.9 71.0 73.9 71.3 47.4 65.9\n5.2.1 Experimental Design\nTasks and Models We focused on general audio understanding beyond speech, employing the\nGAMA model architecture [ 2], a transformer-based audio-language model, as our foundation for\nfine-tuning. Fine-tuning utilized a learning rate of 5e-5, a batch size of 128, and 2 training epochs.\nEvaluation utilized t=0.1 for inference.\nTraining The GAMA model was fine-tuned independently on several datasets: our FusionAudio-\n1.2M and its high-quality subset FusionAudio-high (top 25k QA pairs selected for quality and\ndiversity), alongside established datasets. A critical aspect was normalizing training data to 25,000\nQA pairs across all datasets, ensuring performance differences primarily reflect data quality, not\nquantity. Notably, while baseline datasets typically required 25,000 unique audio clips (one QA pair\nper clip) for this volume, FusionAudio-1.2M achieves this with only 9,000 unique audio clips , owing\nto its design of multiple rich QA pairs per audio instance.\nEvaluation Fine-tuned models were evaluated on 15 diverse audio understanding tasks (Table 5),\nassessing capabilities across three key scenarios: (1) robustness to Adverse Acoustic Conditions,\n(2) proficiency in High-Level Semantic Understanding, and (3) acuity in discerning Fine-grained\nInformation. For benchmarks requiring automated judgment (M.J. scores in Table 5), we employed\nGPT-4.1-mini .\n5.2.2 Performance Analysis\nThe results in Table 5 demonstrate the significant advantages of fine-tuning with FusionAudio.\nDominant Performance Driven by High-Quality and Efficient Data GAMA fine-tuned on\nFusionAudio, and especially its FusionAudio-high subset, consistently outperformed models trained\non all benchmarked datasets across the majority of the 13 tasks, with FusionAudio-high achieving\nthe highest average scores in all scenarios. This success is rooted in the superior intrinsic quality of\nFusionAudio, crafted by our method to maximize information richness per audio clip. As a direct\nresult, models can learn more effectively from each sample, leading to the crucial observation that\nthis dominant performance was achieved using substantially fewer unique audio clips than other\ndatasets. This clearly demonstrates that FusionAudio-1.2M not only provides higher-caliber data\noverall but also facilitates more efficient learning and utilization of each individual audio piece.\n6 Ablation Study\n6.1 On the Effectiveness of Multimodal Cues\nTo rigorously evaluate the contribution of each component in our method for enhanced audio\ninformation, we conducted a comprehensive ablation study. This study aims to (1) ascertain the\nindividual importance of each auxiliary modality (Speech, Music, Video) in augmenting the Sound\nmodality, and (2) validate the effectiveness of our proposed automatic filtering module.\n8\n--- Page 9 ---\nExperiment Setup All ablation experiments were performed on the same subset from AudioSet\nwith a scale of 25k, using the same audio clips and training procedures. FusionAudio-1.2M incorpo-\nrates all four modalities (Sound, Music, Speech, Video) and includes the multi-modal fusion quality\nthreshold filtering module. We compared FusionAudio-1.2M against several ablated variants.\nAblation Results on Fusion As shown in Table 6, ablating auxiliary modalities (Music, Video,\nSpeech) generally degraded performance. Removing video captions (w/o Video) caused the most\nsignificant decline, underscoring visual context’s critical role. Ablating music (w/o Music) and speech\n(w/o Speech) also reduced performance. An interesting exception was observed for Task 1, where\nremoving speech (w/o Speech) led to a slight improvement. We attribute this to a combination of\npotentially poor ASR transcription quality in adverse acoustic conditions, which could introduce\ndetrimental noise, and a possible task focus shift where non-speech acoustic analysis is prioritized,\nmaking speech content less critical and potentially diverting optimization from core modalities.\nNotably, the magnitude of these performance drops (-0.76 for Music, -1.18 for Video, and -0.93\nfor Speech on average) generally corresponds with the usage of these modalities in our dataset, as\nillustrated in Figure 3e. This suggests that modalities more frequently leveraged for information\ncontribute more significantly to the overall performance.\nAblation Results on Filtering Finally, removing our quality filtering module led to a consistent,\nsignificant performance drop across all tasks, highlighting its effectiveness in mitigating issues from\nhallucinations introduced during the process.\n6.2 On the Effectiveness of Data Scaling\nTo assess the impact of data volume, we conducted scaling experiments for the downstream tasks. This\nstudy evaluates performance gains as data size increases, providing insights into model scalability.\nExperiment Setup We used nested subsets, starting from 1.25K audio clips. The Audio Under-\nstanding task scaled to 80k clips (355k QA pairs), while Retrieval utilized up to the full 1.2M clips.\nModel architectures and training hyperparameters remained consistent with previous experiments.\nTable 6: Ablation Study on FusionAudio-25K Dataset. Performance metrics are shown for Retrieval\ntasks (Text-to-Audio and Audio-to-Text) and Understanding tasks ( Task I : Adverse Acoustic Condi-\ntions; Task II : High-level Semantic Understanding; Task III : Fine-grained Information).\nSettingsRetrieval Task Understanding TaskAvg.T-A A-T Task I : AAC Task II : HSU Task III : FI\nFusionAudio-1.2M 39.70 49.71 56.73 57.16 63.02 53.26\nw/o Music 39.03 47.53 56.72 56.34 62.87 52.50(-0.76)\nw/o Video 38.53 48.79 55.90 56.12 61.08 52.08(-1.18)\nw/o Speech 38.09 47.87 57.38 56.06 62.27 52.33(-0.93)\nw/o Filter 39.45 49.14 55.30 55.25 61.35 52.10(-1.16)\nResults As depicted in Figure 5, increasing data volume consistently improved performance for\nboth tasks. For Audio Understanding, scaling from 1.25K to 80k clips enhanced average performance,\nlikely due to increased exposure to diverse audio and associated QA pairs generated with a natural\ndistribution. The Retrieval task exhibited a substantial and consistent rise in Recall@1 with data\nexpansion, peaking with the full dataset. These findings underscore that greater data volume generally\nboosts model capabilities, highlighting the value of our dataset’s scale and richness.\n7 Conclusion\nThis paper presents FusionAudio-1.2M, a large-scale dataset for fine-grained audio captioning created\nvia a novel multimodal contextual fusion pipeline. Inspired by human auditory perception, the\napproach combines specialized expert models for speech, music, sound events, and visual context\nwith LLM-based synthesis. Experiments show that models trained on FusionAudio-1.2M achieve\nstrong performance using fewer unique audio samples due to richer per-clip annotations. Ablation\n9\n--- Page 10 ---\n202122232425\n2627\nDataset Size56.056.557.057.558.058.5Average\n1.25k\n(5.5k QA)5.0k\n(22k QA)20.0k\n(88k QA)80.0k\n(355k QA) FA(a) Audio Understanding\n103104105\n106107\nDataset Size354045505560657075R@1 Score\nFA-1.25kFA-5kFA-50k\nFA-100kFA-200kFA-400kFA-800kFA-1200k\nFA-1.25kFA-5kFA-50kFA-100kFA-200kFA-400kFA-800kFA-1200k\nASC-48kASC-96kASC-191kASC-478kASC-956kASC-1912k\nASC-48kASC-96kASC-191kASC-478k ASC-956kASC-1912kFA A T\nFA T A\nWavC\nAACD\nVEC\nASC A T\nASC T A\n (b) Audio-text Retrieval\nFigure 5: Scaling result of understanding and retrieval tasks. Details of the legend in (b):\nA: Audio; T: Text; FA: FusionAudio-1.2M; WavC: WavCaps; AACD: Auto-ACD; VEC: Sound-\nVECaps; ASC: AudioSetCaps.\nstudies confirm the significance of each modality, particularly visual context. This work could be\nfurther improved by polishing the caption generation method, diversifying the dataset with longer\naudio clips, probing more sophisticated multimodal fusion techniques, and performing a deeper\nsocietal impact analysis, which we leave as future work.\nAckownledgement\nThis work was supported by the Shenzhen Science and Technology Program\n(JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065),\nTianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC)\n(12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No.\nZDSYS20230626091302006), and Shenzhen Stability Science Program 2023, Shenzhen Key Lab of\nMulti-Modal Cognitive Computing.\nLimitation\nThe study also acknowledges several limitations. First, the automated generation of audio captions\nmay introduce hallucinations or errors, despite quality assurance measures such as human evaluation\nand automatic filtering. Second, the dataset primarily focuses on short audio clips (10 seconds),\nwhich may limit its applicability to longer or more complex audio contexts. Third, the multimodal\nfusion approach relies on the integration of speech, music, visual, and general audio information,\nbut the interplay and weighting of different modalities are not thoroughly explored. Fourth, due to\ncomputational resource constraints, we were unable to conduct multiple experimental runs to establish\nrobust error bars for all reported metrics, which could provide further statistical confidence. Lastly,\nwhile our pipeline incorporates a quality filtering module to mitigate LLM-induced inaccuracies,\ncompletely eliminating potential hallucinations in automated data generation remains an ongoing\nchallenge, suggesting a need for continued refinement in robust AI-driven data creation.\nFuture work could address these limitations by further refining the caption generation process,\nexpanding the dataset to include longer audio clips, exploring more nuanced multimodal fusion\nstrategies, and conducting a more comprehensive analysis of societal impacts.\n10\n--- Page 11 ---\nReferences\n[1]Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo\nDubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-\nto-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal\nProcessing, ICASSP , 2023.\n[2]Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S Sakshi,\nOriol Nieto, Ramani Duraiswami, and Dinesh Manocha. GAMA: A large audio-language model\nwith advanced audio understanding and complex reasoning abilities. In Yaser Al-Onaizan,\nMohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing , pages 6288–6313, Miami, Florida, USA, November\n2024. Association for Computational Linguistics.\n[3]Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun\nLv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report.\narXiv preprint arXiv:2407.10759 , 2024.\n[4]Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning\ndataset, 2019.\n[5]Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Gen-\nerating captions for audios in the wild. In Jill Burstein, Christy Doran, and Thamar Solorio,\neditors, Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) , pages 119–132, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics.\n[6] LAION-AI. Laion-audio-630k dataset, 2023. Accessed: 2024-04-16.\n[7]Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D.\nPlumbley, Yuexian Zou, and Wenwu Wang. WavCaps: A ChatGPT-assisted weakly-labelled\naudio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , pages 1–15, 2024.\n[8] Jisheng Bai, Haohe Liu, Mou Wang, Dongyuan Shi, Wenwu Wang, Mark D. Plumbley, Woon-\nSeng Gan, and Jianfeng Chen. Audiosetcaps: An enriched audio-caption dataset using automated\ngeneration pipeline with large audio and language models, 2024.\n[9]Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. Auto-acd: A large-scale dataset for\naudio-language representation learning, 2024.\n[10] Yi Yuan, Dongya Jia, Xiaobin Zhuang, Yuanzhe Chen, Zhengxi Liu, Zhuo Chen, Yuping Wang,\nYuxuan Wang, Xubo Liu, Xiyuan Kang, Mark D. Plumbley, and Wenwu Wang. Sound-vecaps:\nImproving audio generation with visual enhanced captions, 2025.\n[11] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng,\nYuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. Air-bench: Benchmarking large\naudio-language models via generative comprehension, 2024.\n[12] W. H. Sumby and I. Pollack. Visual contribution to speech intelligibility in noise. Journal of\nthe Acoustical Society of America , 26:212–215, 1954.\n[13] Christoph Kayser, Nikos K. Logothetis, and Stefano Panzeri. Visual enhancement of the\ninformation representation in auditory cortex. Current Biology , 20(1):19–24, 2010.\n[14] Marc O. Ernst and Heinrich H. Bülthoff. Merging the senses into a robust percept. Trends in\nCognitive Sciences , 8(4):162–169, 2004.\n[15] Albert S. Bregman. Auditory scene analysis: The perceptual organization of sound . The MIT\nPress, 1990.\n[16] S. A. Shamma, M. Elhilali, and C. Micheyl. Temporal coherence and attention in auditory scene\nanalysis. Trends in Neurosciences , 34(3):114–123, Mar 2011. Epub 2010 Dec 31.\n11\n--- Page 12 ---\n[17] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust speech recognition via large-scale weak supervision, 2022.\n[18] Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi,\nHiromi Wakaki, and Yuki Mitsufuji. Openmu: Your swiss army knife for music understanding.\narXiv preprint arXiv:2410.15573 , 2024.\n[19] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang\nWan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen\nCheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report.\narXiv preprint arXiv:2502.13923 , 2025.\n[20] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025.\n[21] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang. Clap learning audio concepts from\nnatural language supervision. In IEEE International Conference on Acoustics, Speech and\nSignal Processing , pages 1–5. IEEE, 2023.\n[22] H. Li, Y . Kang, T. Liu, W. Ding, and Z. Liu. Ctal: Pre-training crossmodal transformer for\naudio-and-language representations. arXiv preprint arXiv:2109.00181 , 2021.\n[23] C.-F. Yeh, P.-Y . Huang, V . Sharma, S.-W. Li, and G. Gosh. Flap: Fast language-audio pre-\ntraining. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) ,\npages 1–8. IEEE, 2023.\n[24] J. Tian, H. Dongchao Yang, et al. Uniaudio: An audio foundation model toward universal audio\ngeneration. arXiv preprint arXiv:2310.00704 , 2023.\n[25] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. Eskimez, M. Chen, S. Tang, J. Liu, T. Li,\nand T. Yoshioka. Speechx: Neural codec language model as a versatile speech transformer.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing , 2024.\n[26] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio\nlanguage model for audio tasks. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt,\nand S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages\n18090–18108. Curran Associates, Inc., 2023.\n[27] Y . Chu, J. Xu, X. Zhou, S. Yang, Z. Zhang, C. Yan, and J. Zhou. Qwen-audio: Advancing\nuniversal audio understanding via unified large-scale audio-language models. arXiv preprint\narXiv:2311.07919 , 2023.\n[28] Z. Kong, A. Goel, R. Badlani, W. Ping, R. Valle, and B. Catanzaro. Audio flamingo: A\nnovel audio language model with few-shot learning and dialogue abilities. arXiv preprint\narXiv:2402.01831 , 2024.\n[29] Simon Rouard, Francisco Massa, and Alexandre Défossez. Hybrid transformers for music\nsource separation. In ICASSP 23 , 2023.\n[30] TensorFlow. Yamnet: Audio event classification. https://github.com/tensorflow/\nmodels/tree/master/research/audioset/yamnet , n.d. Accessed: 2025-04-19.\n[31] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Chan-\nning Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled\ndataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 776–780, 2017.\n[32] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\nLearning Research , 9(86):2579–2605, 2008.\n[33] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Hts-\nat: A hierarchical token-semantic audio transformer for sound classification and detection. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 646–650. IEEE, 2022.\n12\n--- Page 13 ---\n[34] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 conference of\nthe North American chapter of the association for computational linguistics: human language\ntechnologies, volume 1 (long and short papers) , pages 4171–4186, 2019.\n13\n--- Page 14 ---\nA Human Evaluation\nA.1 Evaluation Setup\nWe recruit five evaluators to assess the data. All evaluators are students with a bachelor’s degree or\nhigher and have studied in an English-only teaching environment. The five evaluators are tasked with\nevaluating a total of 300 samples. Each evaluator is assigned 120 samples, ensuring that each sample\nis evaluated twice by different evaluators.\nEvaluators are required to score the captions based on two dimensions: the level of detailness and the\ndegree of hallucination.\n•Detailness: Evaluating the level of detail, specificity, and contextual information provided\nin the caption regarding the audio events and scene. Captions describing multiple relevant\naspects accurately scored higher. Detailness is scored through 1-3.\n•Hallucination: Assessing the accuracy of the description against the source audio-visual\ncontent. This specifically penalizes hallucinated objects, events, or attributes not perceivable\nin the clip. Hallucination is scored through 1-5.\nSpecific scoring guidelines can be found in the Appendix A.2.\nA.2 Instruction for Human Evaluation\nThe instruction used for human evaluation is shown in Figure 6.\nA.3 F-Score Computation\nTo balance precision and recall in our automatic filtering process, we used the F1.05score, which\nslightly emphasizes recall over precision. This emphasis ensures that captions with high hallucination\nrates are effectively discarded, even at the cost of filtering out some acceptable ones. The F1.05score\nis calculated using the formula:\nF1.05=(1 + 1 .052)·Precision ·Recall\n(1.052·Precision ) +Recall\nWhere precision and recall are computed from the confusion matrix as:\nPrecision =TP\nTP+FPand Recall =TP\nTP+FN\nA.4 Human Rating Distribution\nWe statistically analyze the distribution of human ratings for detailness and hallucination,which are\nshown as Figure 7.\nB Prompt for models\nB.1 Video Caption Prompt\nThe prompt we used for Qwen2.5-VL-72B to extract video caption is shown in Figure 8. We try\nto let the model describe sound-related object only, but found that it would introduce additional\nhallucinations. Thus, we prompt the model to describe visual content only, and let the integration\nmodel tackle the modality issue.\nB.2 Audio Caption Prompt\nThe prompt we used for GAMA to extract audio caption is shown in Figure 9.\n14\n--- Page 15 ---\nInstruction for Human Evaluation\nIntroduction\nYou are tasked with evaluating captions generated for audio clips. Please use the following guidelines to assess each caption\nbased on two indicators: Detailing andHallucinations\n1. Detailing\nKey Things to Look For:\n• Whether the caption captures all major sounds and events in the audio (e.g., dog barking, doorbell ringing, etc.).\n•If the intensity or emotional context of the sound is conveyed (e.g., the dog barking intensely or the doorbell ringing\nin a rapid succession).\n• Whether the caption includes additional information when relevant (e.g., a dog barking repeatedly ordistressed ).\nScoring Guidelines:\nCategorize captions into three detail levels (high, medium, low)based on their coverage of audio elements.\n•Low: Only generic descriptions without specific elements\n•Medium: Identifies main elements but lacks contextual details\n•High: Specifies sound sources, qualities, and relationships\n2. Hallucinations\nYou will be given the highlighted words or phrases marked by DeepSeek-V3 that need to be verified in the original caption:\n2\nA [male voice ] delivers a [ scripted narration ] [in Polish ], likely from a [ recorded radio or podcast segment ], accompanied\nby [subtle studio ambiance ] including [ microphone hiss ] and [ paper rustling ]. A [ secondary listener ] [wearing\nheadphones ] remains [ audibly inactive ], though [ faint page-turning sounds ] indicate [ preparatory material review ]. The\nspoken text references [ program materials available at Lechia.net ], suggesting a [ structured broadcast format ] with\n[editorial oversight ]. Background contains [ minimal environmental noise ] consistent with a [ sound-treated recording\nspace ].\nTotal flagged phrases: 17\nNote : The total number of flagged phrases is provided for reference. If you believe other words or phrases are important in\nthe context of the verification, please consider them in your calculation as well.\nYour Task\n• Listen to the audio and verify the highlighted elements.\n• Assign one of the following error values to each phrase:\nLabel Criteria\nCorrect (0) Directly verifiable from audio\nUnverifiable (0.5) Neither confirmed nor disproven, or things you are not sure\nHallucination (1) Contradicts audio or invents content\nScoring Calculation:\nThe final hallucination rate is calculated as follows:\nHallucination Rate =\u0012P(Error Values )\nTotal Content Units\u0013\n×100%\nBased on the hallucination rate, assign a final score as follows:\n0-10%: Score = 5 | 11-25%: Score = 4 | 25-40%: Score = 3 | 41-50%: Score = 2 | 51-100%: Score = 1\nFigure 6: Instruction for Human Evaluation.\n15\n--- Page 16 ---\n1.0 1.5 2.0 2.5 3.0\nDetailness Rates Distribution0.00.10.20.3Proportion\n1 2 3 4 5\nHallucination Rates Distribution0.000.050.100.150.20ProportionFigure 7: Detailness and Hallucination Rates Distribution of Human Rating\nB.3 Object Extraction Prompt\nThe prompt for asking GPT-4o mini to obtain instruments, emotions, and music styles from audio is\nshown in Figure 10.\nB.4 Modal Information Check Prompt\nThe prompt we used to check if the modal information is used during the fusion is shown in Figure 11.\nC Dataset Samples\nThe code and dataset of this paper can be found in https://github.com/satsuki2486441738/\nFusionAudio .\nC.1 Case study\nTable 7 presents representative FusionAudio captions, annotated with their information sources. These\nexamples illustrate FusionAudio’s ability to synthesize and reason across modalities, generating\ndescriptions that go beyond mere aggregation to provide holistic, context-rich interpretations.\nC.2 Samples of different CLAP score\nTable 8 presents the hallucination situations of FusionAudio captions within different CLAP similarity\nintervals.\n16\n--- Page 17 ---\nTable 7: Example of FusionAudio caption generation with annotated information sources.\nAudio Caption The audio is characterized by the sound of brass instruments, possibly\ntrumpets, playing music.\nSpeech Caption (empty)\nMusic Caption This is an instrumental marching band piece. The brass section plays\nthe melody, while bagpipes provide a counter-melody. The mood is\noptimistic and cheerful. This piece may be used as background music\nfor a national patriotic event.\nVideo Caption Chronological overall description:\n-00:00 : The video opens with close-ups of musicians, highlighting their\nfocused expressions and the shiny brass instruments. The musicians are\nseated in a row, each holding a saxophone or trombone, preparing to\nplay.\n-00:01 : As the conductor steps forward facing the musicians, a blue-and-\nwhite striped shirt appears in the frame. The conductor’s gestures begin\nto guide the musicians as the performance starts.\n-00:02 - 00:10 : The camera remains relatively still, capturing the mu-\nsicians playing their instruments. The conductor’s hand movements\nsynchronize with the music, guiding the ensemble. The musicians’ fin-\ngers move rhythmically on the keys and valves, producing harmonious\nsounds. The conductor continues to lead, ensuring cohesion.\n-00:07 - 00:10 : The camera pans slightly right, revealing more trombone\nplayers at the far right. The additional musicians add depth to the ensem-\nble, enhancing the overall sound and visual impact. The video ends with\nthe musicians continuing to play under the conductor’s direction.\nReasoning Content The audio caption mentions \"trumpets,\" but the music and video\ncaptions indicate a brass ensemble with saxophones, trombones,\nand bagpipes. The music caption is more reliable for instrument\nidentification, so the final description should prioritize saxophones,\ntrombones, and bagpipes.\nThere is no speech content, which matches the empty speech caption.\nThe mood is cheerful and ceremonial, suitable for a patriotic event,\nas inferred from the music caption.\nPotential ambiguities include confusion about the specific types\nof brass instruments and the presence of bagpipes, as well as\nuncertainty about whether the performance is indoors or outdoors.\n... (Further details omitted for brevity)\nFinal Caption A lively instrumental piece performed by a brass ensemble, with saxo-\nphones and trombones carrying the main melody and bagpipes providing\na contrasting counter-melody. The music maintains the rhythmic pre-\ncision characteristic of a formally conducted ensemble, featuring the\nharmonious and dynamic qualities typical of patriotic or ceremonial\nperformances. No prominent vocal content.\n17\n--- Page 18 ---\nTable 8: The demonstration of the hallucination which is marked in red of audio captions within\ndifferent clap similarity intervals\nClap Similarity\nIntervalsAudio ID Caption\n0.0-0.1 -wyJ2cab4icA speech with strong tonal urgency is delivered, accompanied\nby persistent breathing sounds and faint intermittent\nbackground activity suggesting an indoor environment. The\nspeaker’s vocal cadence appears strained, potentially reflecting\neither passionate delivery or underlying emotional tension.\n0.1-0.2 -4t1LMiiHp4A clear male speech is delivered with a strong vocal presence,\naccompanied by dynamic acoustic drums, a groovy bassline,\nand intermittent tambourine shakes in the background.\nSporadic applause and crowd cheering weave throughout the\nspeech, creating an energetic and engaged atmosphere. The\nmusical elements maintain a steady rhythmic foundation while\nthe vocal delivery appears deliberate and focused.\n0.2-0.3 04Q_WeM7VIUContinuous music with a groovy bass line, percussive drum\npatterns, keyboard harmonies, and synth brass melodies is\nheard in a lively setting. Intermittent male speech occurs in an\nupbeat tone, overlapping with the music’s rhythmic elements.\nThe recording exhibits mono audio and background noise,\nsuggesting a live performance environment with frequent\nequipment adjustments and energetic vocal exchanges.\n0.3-0.4-\nCCsZneHL6sA solo violin performs a slow, emotive melody with a smooth\nbowing technique, accompanied by steady rhythmic percussive\nsounds suggesting a handpan or similar instrument. The\nperformance takes place in an indoor environment with subtle\nbackground reverberation, indicative of a studio or concert\nspace. The audio quality is slightly degraded, but the interplay\nbetween the sustained violin tones and precise percussive\nelements creates a harmonious, intimate atmosphere.\n0.4-0.5 -EKjvd8q_A0The audio features a lively and energetic performance with\nrhythmic maracas, congas, and an accordion, accompanied by\na saxophone adding depth. The upbeat tempo and festive\nsoundscapes suggest a cultural celebration or live musical\nevent.\n0.5-0.6 00TwebqicmoThe audio is dominated by powerful car engine revving and\nacceleration sounds, accompanied by continuous background\nmusic. The combination of loud mechanical noises and\nenergetic musical accompaniment creates a high-intensity\natmosphere characteristic of an automotive event. Intermittent\nengine echoes suggest open-air acoustics typical of a racetrack\nor exhibition setting.\n18\n--- Page 19 ---\nPrompt for video caption\nPrompt:\nPlease provide a comprehensive video description focusing exclusively on observable visual elements, including timestamps:\n**1. Key Entities & Actions with Timestamps:**\n- List main objects/subjects and their visible actions with approximate timestamps (MM:SS format)\n- Describe:\n* Object/subject movements and interactions\n* Material properties (metal, wood, liquid)\n* Timing of significant visual events\n**2. Scene Description with Timeline:**\n- Overall scene dynamics and visual interactions\n- Notable visual events with timestamps:\n* Object collisions or impacts\n* Movement patterns\n* Material changes\n* Human/animal visible actions\n- Environmental context (indoor/outdoor, spatial relationships)\n**3. Overall Description with Chronological Flow:**\n- Provide a comprehensive visual narrative of the video\n- Include timestamps for key moments and transitions (MM:SS format)\n- Focus on observable actions, and movements\n- Use specific, action-oriented language\n- Present events in chronological order with clear time markers\nGuidelines:\n- Describe only directly visible elements\n- Focus on observable actions and movements\n- Note material properties and physical interactions\n- Include **timestamps** for all significant events\n- Timestamp **should not** exceed the duration of the video\n- Use precise descriptive language for visual elements\n- Avoid assumptions about non-visible elements\n- Maintain strict focus on visual information\nExample:\nInstead of \"A car’s engine roars as it accelerates\"\nWrite \"00:01 - A red sports car with chrome detailing accelerates down a paved road, tires creating visible spray on wet\nasphalt\"\n\"00:02 - The car’s rear suspension compresses during acceleration, exhaust emitting visible vapor\"\n\"00:03 - The car’s engine roars as it accelerates\"\nFigure 8: Prompt for video caption.\nC.3 Situations where multimodal contextual cues work\nOur multimodal approach is designed to excel in challenging audio understanding scenarios (Table 9),\nsuch as interpreting audio in adverse conditions, achieving high-level semantic understanding (e.g.,\nnuanced music interpretation), and enabling fine-grained acoustic entity recognition. Addressing\nthese scenarios highlights the benefits of comprehensive multimodal integration.\nC.4 Samples of different sub-scenario\nTable 10 shows the example dataset for each sub-scenario and corresponding example samples.\n19\n--- Page 20 ---\nAn example prompt for audio caption generation\nDescribe the audio in detail, but there is not need for association or speculation.\nFigure 9: An example prompt for audio caption generation\nAn example prompt for extracting objects from audio\nI will give you a sentence. Please extract some information I need in a JSON format. Sentence: ’caption’\nMy requirement:\n1. Extract instruments and return as a list\n2. Extract emotions and return as a list\n3. Extract music genres and return as a list\n4. Extract scenes and return as a list\n5. All words must be found in the sentence.\n6. Return a JSON format without any other words.\n7. Words must be extracted from the corresponding caption.\nThe return format should only be like this:\n{\n\"instrument\": [],\n\"emotion\": [],\n\"music genre\": [],\n\"scene\": []\n}\nFigure 10: An example prompt for extracting objects from audio.\nD More on Dataset Statistics\nD.1 Embedding Space Quantitave Analysis\nTable 11 presents a comprehensive comparison of inter- and intra-category embedding distances\nacross different datasets. The analysis focuses on three key audio categories: Music (M), Vehicle\n(V), and Speech (S). Our proposed FusionAudio dataset demonstrates superior performance across\nall metrics. For inter-category distances, where higher values indicate better category separation,\nFusionAudio achieves significantly larger distances between different audio types (M-V: 0.7230,\nM-S: 0.5369, V-S: 0.5943) compared to competing datasets. This indicates that our dataset enables\nmodels to learn more discriminative representations that effectively distinguish between different\naudio categories. Simultaneously, FusionAudio exhibits smaller intra-category distances (Music:\n0.8084, Vehicle: 0.7406, Speech: 0.8204), reflecting greater consistency within each category. The\nsubstantial improvement in both metrics—maximizing inter-category separation while minimizing\nintra-category variation—confirms that FusionAudio produces more cohesive and well-structured\nembedding spaces. This balance is crucial for downstream tasks such as audio classification, retrieval,\nand generation, as it facilitates more accurate identification and characterization of audio content\nwhile maintaining the nuanced variations within categories.\n20\n--- Page 21 ---\nShortened Prompt for Modal Integration Check\n\"You analyze descriptions from audio.\n’final_cap’ is a comprehensive summary.\nIdentify source captions (’audio_caption’, ’speech_caption’,\n’music_caption’, ’video_caption’) essential for ’final_cap’\nusing provided JSON data: {cap_str}.\nRequirements:\n1. List contributing caption types.\n2. Return as string keys list.\n3. Format: [’type1’, ’type2’]\"\nFigure 11: Concise prompt for modal info check\nExamples for audios with different clap scores.\nHere we show the severity of hallucinations in audio captions under different clap similarity intervals. The red - marked parts\nare the hallucinatory parts of the audio captions.\nFigure 12: Examples for audios with different clap scores.\nTable 9: Key use-case scenarios where integrating multimodal contextual cues can significantly\nimprove audio captioning. Challenges are listed per sub-scenario. Representative datasets and\nsamples are detailed in Appendix C.4.\nScenario Sub-Scenario Key Challenges\nAdverse Acoustic\nConditionsScene Recognition in\nComplex SoundscapesHigh inherent acoustic complexity; Interwoven\nmulti-source information; Background noise\nAcoustically\nDegraded ConditionsRecording device limitations; Synthetic Artificial\nnoise interference\nHigh-Level\nSemantic\nUnderstandingMusic UnderstandingMusical Genre Analysis; Emotional Expression;\nArtistic Intent; Aural Narratives\nSound Understanding Sound Implied Information; Attributes Inference\nFine-grained\nInformation RecognitionAcoustic Entity\nRecognitionSubtle acoustic cue discernment\n21\n--- Page 22 ---\nAn example prompt for multi-choice questions\nPrompt:\nPlease provide a comprehensive video description focusing exclusively on observable visual elements, including\ntimestamps:\n**1. Key Entities & Actions with Timestamps:**\n- List main objects/subjects and their visible actions with approximate timestamps (MM:SS format)\n- Describe:\n* Object/subject movements and interactions\n* Material properties (metal, wood, liquid)\n* Timing of significant visual events\n**2. Scene Description with Timeline:**\n- Overall scene dynamics and visual interactions\n- Notable visual events with timestamps:\n* Object collisions or impacts\n* Movement patterns\n* Material changes\n* Human/animal visible actions\n- Environmental context (indoor/outdoor, spatial relationships)\n**3. Overall Description with Chronological Flow:**\n- Provide a comprehensive visual narrative of the video\n- Include timestamps for key moments and transitions (MM:SS format)\n- Focus on observable actions, and movements\n- Use specific, action-oriented language\n- Present events in chronological order with clear time markers\nGuidelines:\n- Describe only directly visible elements\n- Focus on observable actions and movements\n- Note material properties and physical interactions\n- Include **timestamps** for all significant events\n- Timestamp **should not** exceed the duration of the video\n- Use precise descriptive language for visual elements\n- Avoid assumptions about non-visible elements\n- Maintain strict focus on visual information\nExample:\nInstead of \"A car’s engine roars as it accelerates\"\nWrite \"00:01 - A red sports car with chrome detailing accelerates down a paved road, tires creating visible spray\non wet asphalt\"\n\"00:02 - The car’s rear suspension compresses during acceleration, exhaust emitting visible vapor\"\n\"00:03 - The car’s engine roars as it accelerates\"\nFigure 13: An example prompt for multi-choice questions.\n22\n--- Page 23 ---\nPrompt for integration\nPrompt:\nRigorous Multimodal Information Integration and Purely Audio Description Expert\nCore Task\nYou are an expert specializing in audio information processing. Your goal is to: integrate and analyze textual descriptions\nfrom multiple modalities as input, perform cross-referencing and correction while strictly controlling cross-modal information\ninterference, and ultimately generate a description that is purely about the audio content , accurate, detailed, and fluently\nwritten in English, annotating potential ambiguities based solely on auditory perception .It is strictly prohibited to\ninclude any visual information, specific speech dialogue content, or ambiguity annotations based on audio-visual\ninconsistencies in the final output.\nInput Information Sources (May contain errors, hallucinations, or be incomplete)\n•Audio Tags: A set of sound category tags annotated by humans, along with their corresponding quality estimations\n(confidence scores). Represents the most prominent human-perceived acoustic features in the audio. These tags are\nhighly reliable, especially those with high percentages , but may not comprehensively cover all information in the\naudio. The format is TagName(Percentage%) . e.g., Speech(100%) .If empty, it indicates no human-annotated\ntag information is available.\n• Audio Description: A textual description of the audio content (may include sound events, ambient sounds, music,\nvocal characteristics, etc.). This is an important basis for describing audio facts and needs to be cross-validated\nwith tags and music descriptions.\n•Speech Content: The textual result from Automatic Speech Recognition (ASR). This information is used only to\nconfirm the presence of human voice, determine general vocal characteristics (e.g., speech vs. non-linguistic\nsounds, presence of distinct emotions [non-content related]), and assist in inferring possible scenarios or\nevent backgrounds. Its specific textual content (including paraphrasing or summarization) must never\nappear in the final output. If empty, it indicates no distinct human voice, or other non-linguistic vocalizations\n(e.g., gasping, crying, background babble).\n•Music Description: A description of musical elements (features, instruments, rhythm, etc.) and other sound\nscenes. Music-related features herein are highly reliable. If empty, it indicates no distinct music. Other\nnon-music descriptions (e.g., environment, human voice) have lower priority and primarily depend on \"Audio\nTags\" ,\"Audio Description\" , and \"Speech Content\" for judgment.\n•Video Description: A textual description of the video frames. Used only under specific conditions (see\"Active\nCorrection\" in Processing Steps, step 2) to actively assist in identifying auditorily ambiguous sound sources ,\nandto identify inconsistencies with auditory information (this inconsistency is only an internal decision-\nmaking flag for the model, not used to generate the output ambiguity list) .Never speculate or describe the\nsource, location, or on-screen actions of sounds based on video information itself. If empty, it indicates a lack of\nvisual auxiliary information.\nProcessing Steps\nPlease strictly follow the steps below:\n1.Multimodal Information Parsing :\n•Separately interpret each input description to extract core sound events, sound source characteristics, environ-\nmental ambiance, and musical elements.\n• Specifically parse \"Audio Tags\" to extract tag names and their confidence scores.\n•Special Note : From \"Speech Content\" (ASR results), primarily determine if human voice is present and\nitsnon-content features . In conjunction with its textual content ( used only for auxiliary understanding ),\nassist in inferring possible environments, emotional tones, or types of acoustic events, but never judge\nspeaker gender, age, or other personal characteristics based on ASR content, and never quote, paraphrase,\nor summarize the specific textual content.\nFigure 14: Prompt for integration.\n23\n--- Page 24 ---\nPrompt for integration Cont.\n2.Auditory Fact Determination and Cross-modal Correction :\n•Initial Determination of Auditory Facts : First, based on \"Audio Tags\" (especially high-confidence\ntags, which have the highest priority for determining the types of sounds included in the tags ),\"Audio\nDescription\" ,\"Music Description\" (especially the music part), and \"Speech Content\" (presence of\nhuman voice and inferred characteristics), preliminarily determine auditorily perceived sound events, sound\nsources, ambient sounds, and music features. Identify and attempt to correct contradictions within these\naudio information sources (tags, audio description, music description, ASR inference), with the priority rule:\nHigh-confidence \"Audio Tags\" > Music part of \"Music Description\" ≈\"Audio Description\" >\n\"Speech Content\" (presence of human voice) > Low-confidence \"Audio Tags\" > Non-music part of\n\"Music Description\" .\n•Cross-modal Validation and (Conditional) Active Correction (for video information): After the initial\ndetermination of auditory facts, introduce \"Video Description\" for cross-validation. Its role is:\n–Active Correction (when audio information is ambiguous and video provides clear evidence): If\nthe initially determined auditory fact (based on audio information sources) describes a general sound\ntype that could have multiple auditory interpretations (e.g., a rumbling sound, a clicking sound, a\nrustling sound), andthe\"Video Description\" clearly shows an object or event that is highly relevant\nto this general sound type and is a plausible sound source (e.g., the video clearly shows an airplane\nmaking a rumbling sound, or a person clicking a mouse making a clicking sound, or clothes/fabric in\nmotion making a rustling sound), then the information provided by the video should be adopted to\nmore precisely identify the general sound as a specific source or type (correcting rumbling to airplane\nsound, clicking to mouse click, rustling to fabric rustle). Note: If a high-confidence tag in \"Audio\nTags\" already clearly indicates the specific sound type, then this sound is no longer considered\na ’general sound type with multiple auditory interpretations,’ and this active correction step no\nlonger applies to this sound. Under these limited and clear conditions, video information is used to\nenhance the understanding of audio facts, making the description more precise.\n–Identifying Inconsistencies or Lack of Corroboration (when video cannot clearly corroborate or\nconflicts):\n*If the sound event described by the initially determined auditory facts does not have a clearly\ncorresponding visual sound source in the \"Video Description\" , or if the visual information is\ninconsistent with or contradicts the perceived location or state of the sound source , then video\ninformation must never be used to negate or modify known auditory facts . In such cases, the model\nshould internally flag the presence of an audio-visual inconsistency or lack of visual corroboration.\nThis flag is only used in subsequent steps to adopt conservative wording when generating the\nfinal audio description and must never directly generate an ambiguity entry for output.\n*It is strictly prohibited to speculate, describe, or alter judgments about the sound event itself\nbased on video information that cannot corroborate the audio (e.g., hearing a rumbling sound,\nthe video shows the sky, but one cannot speculate it’s an airplane sound unless the video explicitly\nshows an airplane).\n•Determine Corrected Auditory Facts : Based on the results of the above multimodal cross-validation,\ndetermine the final auditory facts. The priority rule is listed above. When cross-modal information conflicts,\naudio information sources conflict internally, or there is high uncertainty (especially a lack of high-\nconfidence tags or clear video corroboration for audio) making it difficult to determine auditory facts,\nthe determined facts should reflect extreme conservatism, preferring to omit uncertain information\nrather than speculating based on non-auditory information. The model should internally retain a flag\nfor the uncertain origin of audio information (e.g., whether it’s due to a lack of high-confidence tags, lack\nof support from audio description, or lack of video corroboration), to generate appropriately conservative\ndescriptions in step 5.\n•Emotion Inference and Correction : If the emotion of a sound event (e.g., human voice, whose emotion can\nbe inferred with ASR content assistance) conflicts with the emotion of background music, a comprehensive\njudgment must be made to provide the most likely primary emotional tone, but this is still based on auditory\nand ASR-assisted inference, without introducing visual information.\n3.Purely Auditory Ambiguity Reasoning and Annotation :\n•Focus Solely on Pure Audition : Based on the determined auditory facts (which have considered tags and\ncorrection results), sound characteristics, common possibilities of auditory confusion, and potential auditory\nunderstanding biases in the original audio description, infer potential auditory understanding ambiguities that\ncan be perceived or reasonably inferred solely through hearing .\nFigure 15: Prompt for integration Cont.\n24\n--- Page 25 ---\nPrompt for integration Cont.\n4. • Sources of Ambiguity :\n–Auditory Similarity or Vagueness of the Sound Itself : Some sounds may be auditorily similar to others\nand easily confused (e.g., vehicle sound vs. airplane sound, typing sound vs. light tapping sound). The\nsound’s own quality, distance, or reverberation can also lead to vagueness or difficulty in determining the\nsource.\n–Polysemy of Auditory Association : A sound event may reasonably correspond auditorily to multiple\ndifferent sound sources or situations (e.g., a “bang” can have multiple causes, footsteps might come from\nmultiple people).\n–Potential Purely Auditory Biases in the Original Audio Description : If, after multimodal correction,\nthe original \"Audio Description\" is found to have incorrect or imprecise judgments about sound\nevents or sources (and this error/imprecision is not caused by audio-visual inconsistency but by potential\nmisinterpretations of audition itself), one should infer what common purely auditory misinterpretations\nthe original description might have been based on.\n•Strictly Exclude Non-Auditory Information as a Source of Ambiguity : Ambiguity annotation must only\nrevolve around pure auditory perception and the associations arising therefrom. It is absolutely not allowed\nto use audio-visual synchronization, the way sound sources are presented on screen, or any visual content as\nthe source or descriptive content of an ambiguity.\n5.Information Reliability Assessment and Final Output Decision :\n•In this step, based on the analysis and correction results from steps 1-3, comprehensively assess\nthe reliability and completeness of the determined auditory facts. In particular, consider whether\nhigh-confidence audio tags support key sound events.\n•If it is judged that the determined auditory facts are extremely scarce, various audio informa-\ntion sources (tags, audio description, music description, ASR inference) severely conflict and au-\nditory facts cannot be reliably reconstructed, or even if tags exist but their confidence is generally\nvery low and contradicts other information, the model directly outputs the unique specific string\nUNCERTAIN_AUDIO_INFORMATION_DETECTED .\n•Otherwise (if the determined auditory facts are sufficiently reliable and complete), proceed to the next\nstep (generating JSON).\n6.Generate Final Pure Audio Description (Audio Caption) :\n•Execute this step only after passing the reliability assessment in step 4.\n•Pure Audio Focus : Generate a fluent, accurate, detailed, and concise English audio description. Describe\nonly what can be heard and its purely auditory characteristics (e.g., sound source type [prioritizing those\nconfirmed by high-confidence tags or clearly identified through active video correction], nature of sound\nevents, type of ambient sound, music features, non-content features of human voice, spatial sense, loudness,\ntimbre, duration, rhythm, etc.).\n•Integration and Augmentation : Integrate all valid auditory facts determined after multimodal correction\n(including those from audio tags, audio description, music description, ASR inference, and sound source types\nactively corrected via video). Supplement necessary auditory details of the scene (e.g., indoor/outdoor inferred\nfrom ambient sounds). If the model has internally flagged uncertainty in the audio information (e.g.,\nlack of high-confidence audio tags supporting key sound events, original audio description being auditorily\nvague and lacking clear video corroboration, or internal conflicts within audio information sources), the\nfinal description must reflect this uncertainty, but through cautious wording to describe the perceived\nsound itself, rather than directly stating the uncertainty or vagueness. Use phrases like “sounds like,”\n“appears to be,” “potentially,” “suggests,” “a sound resembling X is heard” to express identification of less\ncertain sound sources or events. Crucially, avoid sentences that explicitly state an inability to determine\nsomething or that something is ambiguous (e.g., do not say “the source cannot be determined” or “it\nis ambiguous whether X is present”). Instead, directly omit highly uncertain details or use cautious\nwording for what might be perceived.\n•Objective and Accurate : Base inferences on determined auditory facts, avoiding subjective speculation\nand over-extension. The description content must be supported by input text. Prohibit the introduction of\nirrelevant “new information,” unless it is reliable auditory inference based on multiple audio information\nsources (e.g., inferring the scene from ambient sounds). Ensure the description integrates facts confirmed by\nhigh-confidence tags, but never mention the confidence percentages themselves.\nFigure 16: Prompt for integration Cont.\n25\n--- Page 26 ---\nPrompt for integration Cont.\n•Cultural/Emotional Cues : If the sound contains clear cultural symbols or strong emotions, these can\nbe briefly cued, but must be based on input audio evidence (e.g., emotion in human voice inferred from\nASR, or emotion reflected by music features).\n•Final Check : Ensure this description absolutely contains no visual elements (objects, colors, actions,\nvisual scenes, etc.). Even if the sound source type has been determined through high-confidence tags\nor active video correction, never describe the visual location, visual form, or specific on-screen\nbehavior of that sound source .Absolutely prohibit the output of any specific speech text content\n(quotation, paraphrase, summary).\nOutput Format Requirements\nFor most cases (i.e., when passing the reliability assessment in step 4), please strictly generate structured\nEnglish output in the following JSON format (without any other explanations). However, in the special\ncase of “scarce/unverifiable information” defined in step 4 of the processing flow, the model should directly\noutput the predefined string UNCERTAIN_AUDIO_INFORMATION_DETECTED instead of JSON.\n{\n\"Potential ambiguities\": [ // List potential ambiguities based purely on\nauditory perception (English sentences). Does not include ambiguities\nrequiring visual information\nto understand, nor ambiguities based on audio-visual inconsistencies.\n\"Ambiguity description 1 based solely on auditory perception.\",\n\"Ambiguity description 2 based solely on auditory perception.\",\n...\n],\n\"Audio caption\": \"Final audio description focusing solely on audible elements\nand their auditory characteristics, detailed and fluent English. Use\nconservative language when audio facts are uncertain based on internal\nassessment.\"\n// Final pure audio description (concise and clear English sentence)\n}\nKey Considerations :\n• Output Language: English .\n• Ignore Empty Inputs: If a modal description is empty, ignore that information source.\n•Strictly Prohibited : Including any visual information (objects, colors, actions, visual scenes, visual\nlocation/form/on-screen behavior of sound sources, audio-visual synchronization, etc.) in the final\noutput (including Audio caption andPotential ambiguities ). Even when dealing with audio-\nvisual inconsistencies or unknown sound sources, never speculate, describe, or mention any visual\ncontent in the output.\n•Strictly Prohibited : Including any specific speech text content (quotation, paraphrase, summary, etc.)\nin the final output. Speech information is only used to infer the presence of human voice, non-content\nfeatures, and to assist in understanding the scene ambiance.\n•Maintain Objectivity: Base inferences on determined auditory facts, avoiding subjective speculation and\nover-extension. Information not supported by the input or derived through reliable auditory inference\nmust not appear in the output.\n•When the model internally flags audio information as uncertain (even if\nUNCERTAIN_AUDIO_INFORMATION_DETECTED is not triggered), the final Audio caption must\nstrictly use cautious wording to describe the sound itself, focusing on auditory perception. It is strictly\nprohibited to directly state uncertainty or ambiguity; only provide confirmed acoustic facts and\ndo not mention uncertain content in the output.\n•High-confidence audio tags are the highest priority source for determining sound type facts, but\nspecific confidence values are not allowed in the output.\nFigure 17: Prompt for integration Cont.\n26\n--- Page 27 ---\nTable 10: Dataset and examples corresponding to each sub-scenario, where cls is the classification\ntask\nSub-Scenario Datasets(quantity) Examples\nScene Recognition in\nComplex SoundscapesAIR-Bench:\nAcoustic scene cls(2,000)\nUrbanSound8K(8,732)Identifying child playing scene\nIdentifying kitchen scene\nAcoustically Degraded\nConditionsTAU Urban Sound-Mobile(5,265)\nFSDnoisy18K(947)Identifying street pedestrian sound\nIdentifying metro station scene\nMusic UnderstandingAIR-Bench:\nGenre cls(2,000)\nMusicAQA(814)\nMood detection(2,000)\nChat-Music(500)Identifying music genre\nCharacter portrayed by the tune\nTrumpet&accordion’s role in texture\nSound UnderstandingAIR-Bench:\nSoundAQA(2,000)\nChat-Sound(500)\nAudioBench:\nAudio-Scene QA(9,131)Location of dripping water\nPossible actions with the liquid\nIndications of a busy road\nAcoustic Entity\nRecognitionAIR-bench:\nV ocal sound cls(1,000)\nMusic instruments cls(2,000)\nESC-50(2,000)\nFSD50K(10,231)Instrument recognition\nAcoustic event/ontology recognition\nAcoustic scene type recognition\nTable 11: Inter- (M-V , M-S, V-S) and Intra- (M, V , S) category embedding distances. Best inter-\ndistances (higher) and intra-distances (lower) are bolded.\nDataset/MethodInter-category distance ↑ Intra-category distance ↓\nM – V M – S V – S Music Vehicle Speech\nFusionAudio 0.7230 0.5369 0.5943 0.8084 0.7406 0.8204\nASC 0.5685 0.4137 0.4523 0.8638 0.8216 0.8724\nAuto-ACD 0.5685 0.4137 0.4523 0.8645 0.8402 0.8915\nSound-VECaps 0.5232 0.3770 0.4664 0.8578 0.7798 0.8920\n27",
  "text_length": 81434
}