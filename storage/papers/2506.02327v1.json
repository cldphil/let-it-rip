{
  "id": "http://arxiv.org/abs/2506.02327v1",
  "title": "Medical World Model: Generative Simulation of Tumor Evolution for\n  Treatment Planning",
  "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers.",
  "authors": [
    "Yijun Yang",
    "Zhao-Yang Wang",
    "Qiuping Liu",
    "Shuwen Sun",
    "Kang Wang",
    "Rama Chellappa",
    "Zongwei Zhou",
    "Alan Yuille",
    "Lei Zhu",
    "Yu-Dong Zhang",
    "Jieneng Chen"
  ],
  "published": "2025-06-02T23:50:40Z",
  "updated": "2025-06-02T23:50:40Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.02327v1",
  "full_text": "--- Page 1 ---\narXiv:2506.02327v1  [cs.CV]  2 Jun 2025Medical World Model:\nGenerative Simulation of Tumor Evolution for Treatment Planning\nYijun Yang1,*, Zhao-Yang Wang2, Qiuping Liu3, Shuwen Sun3, Kang Wang4, Rama Chellappa2,\nZongwei Zhou2, Alan Yuille2, Lei Zhu1,5,‚Ä†, Yu-Dong Zhang3, Jieneng Chen2,‚Ä†\n1The Hong Kong University of Science and Technology (Guangzhou)2Johns Hopkins University\n3The First Affiliated Hospital of Nanjing Medical University\n4University of California, San Francisco5The Hong Kong University of Science and Technology\nProject page: https://yijun-yang.github.io/MeWM\nAbstract\nProviding effective treatment and making informed clini-\ncal decisions are essential goals of modern medicine and\nclinical care. We are interested in simulating disease dy-\nnamics for clinical decision-making, leveraging recent ad-\nvances in large generative models. To this end, we introduce\nthe Medical World Model (MeWM), the first world model in\nmedicine that visually predicts future disease states based\non clinical decisions. MeWM comprises (i) vision-language\nmodels to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates\naction plans, such as clinical treatments, while the dynam-\nics model simulates tumor progression or regression under\ngiven treatment conditions. Building on this, we propose\nthe inverse dynamics model that applies survival analysis\nto the simulated post-treatment tumor, enabling the evalu-\nation of treatment efficacy and the selection of the optimal\nclinical action plan. As a result, the proposed MeWM sim-\nulates disease dynamics by synthesizing post-treatment tu-\nmors, with state-of-the-art specificity in Turing tests evalu-\nated by radiologists. Simultaneously, its inverse dynamics\nmodel outperforms medical-specialized GPTs in optimizing\nindividualized treatment protocols across all metrics. No-\ntably, MeWM improves clinical decision-making for inter-\nventional physicians, boosting F1-score in selecting the op-\ntimal TACE protocol by 13%, paving the way for future in-\ntegration of medical world models as the second readers.\n1. Introduction\nClinical decision-making is at the heart of patient care, driv-\ning outcomes and shaping the trajectory of healthcare inter-\nventions. Physicians constantly weigh the multimodal fac-\n*Work done while visiting at JHU.\n‚Ä†Corresponding authors.\nImaging Observation\nRecovery-ConditionedPolicies\nProgression Generative Model\nPerception\nState(ùíîùüé)\nActor\nAction\nState(ùíîùüè)\n¬∑¬∑¬∑SurvivalAnalysis\nFigure 1. Formulation of Medical World Model. It integrates\nimaging observations with perception modules to form an initial\nstate, which is then processed by a progression generative model\nto predict future states of disease under different treatment con-\nditions. Recovery-conditioned policies guide treatment decisions,\ncreating a feedback loop for optimizing clinical interventions.\ntors including medical images and patient history to deter-\nmine the best course of action for each patient. Artificial in-\ntelligence (AI) models are increasingly assuming a crucial\nrole in this process by analyzing the complex multimodal\ndata, revealing patterns that may be difficult to detect with\nhuman observation alone, and suggesting tailored treatment\nstrategies based on predictive analytics.\nFoundation models [9] such as large language mod-\nels (LLMs) [37, 64] present a new frontier in medical\nAI research and development. However, recent stud-\nies [29] show that LLMs, even those specifically tailored\nfor medicine [16, 61, 62], diagnose significantly worse than\nclinicians and make less informed treatment decisions. This\nis due to several challenges. First , the complexity of dis-\neases themselves, such as tumors that evolve under the\ninfluence of diverse biological and chemotherapy factors,\ncalls for models that can adapt and account for disease pro-\ngression. Second , clinical decision-making necessitates not\nonly accurate predictions but also visually trackable insights\n--- Page 2 ---\nthat physicians can trust.\nRecent breakthroughs in world models (WMs) [4, 10, 43,\n51, 69] provide a promising avenue for overcoming these\nobstacles. By generating a predictive distribution of how\nthe world states evolve, WMs mirror the way human plan-\nners imagine future scenarios and then make informed de-\ncisions via inverse dyanmics [4, 21]. Although they re-\nmain largely underexplored in the medical domain, world\nmodels hold significant potential for generating clinically\nrealistic images and simulating disease progression, which\nin turn can facilitate more effective and visually trackable\ntreatment planning. Figure 1 illustrates our formulation of\nintroducing WMs into generalized medical scenarios and\nhow WMs integrate these capabilities to support clinical\ndecision-making.\nIn this work, we introduce Medical World Model\n(MeWM) to address these challenges and push the bound-\naries of AI-driven clinical decision support. MeWM com-\nprises three primary components: (1) a Policy Model pow-\nered by vision-language architectures, which generates the\npotential action combos from a patient‚Äôs current state and\nspecific clinical scenario; (2) a Dynamics Model that for-\nwards and simulates tumor dynamics, predicting how tu-\nmors could progress or regress under different treatment\nconditions by generative modeling; (3) an Inverse Dynam-\nics Model that performs survival risk analysis on the sim-\nulated post-treatment tumor, and quantitatively evaluates\ntreatment efficacy. Beyond forward simulation, this sys-\ntem heuristically explores the optimal plan with the assis-\ntance of a segmentation model. By uniting these elements,\nMeWM delivers a holistic framework for decision-making:\nit can synthesize realistic post-treatment tumors that pass\nTuring tests against radiologists, and it outperforms special-\nized GPT-like models on Transarterial Chemoembolization\n(TACE) Protocol Exploration (over 10% ‚Üëin F1-score).\nOverall, our contributions are threefold.\n‚Ä¢ We propose the medical world models, where we de-\nvelop a multimodal policy model that leverages vision-\nlanguage capabilities to propose a tailored set of treat-\nment action combos, and we design a generative dynam-\nics model that accurately captures potential evolution of\ntumors, enabling forward-looking simulations for differ-\nent interventions.\n‚Ä¢ We integrate an inverse dynamics model that translates\nthese action-conditioned simulation into survival analy-\nsis metrics, thereby offering a transparent and evidence-\nbased tool for choosing the optimal treatment protocol.\n‚Ä¢ We demonstrate a substantial leap in AI-driven decision\nsupport for interventional medicine, improving the F1-\nscore in selecting the optimal treatment protocol by 13%\nand offering a compelling glimpse into the future of pre-\ncision healthcare.2. Related Work\n2.1. Generative World Modeling\nWorld models [26, 43] aim to simulate dynamic environ-\nments by predicting future states and rewards based on cur-\nrent observations and actions. Originally developed for\nconstrained settings like Atari games [27], their ability to\nmodel state transitions has been extended to real-world sce-\nnarios through joint learning of policies and world mod-\nels, improving sample efficiency in simulated robotics [59],\nreal-world robots [70] and autonomous driving [33, 68].\nWhile early world models focused on simple state transi-\ntions, modern approaches integrate structured action-object\nrelationships [66] and multi-modal conditioning [7, 24]. For\ninstance, Du et al. [20] present long-horizon video plans by\nsynergising vision-language models and text-to-video mod-\nels. Luo et al. [52] propose to ground video models to con-\ntinuous action by leveraging video-guided goal-conditioned\nexploration to learn a goal-conditioned policy. In embod-\nied decision-making, Lu et al. [51] enables agents to imag-\ninatively explore the world with high generation quality\nand exploration consistency using video generative models.\nHowever, there is still no work investigating the applicabil-\nity of world modeling in medical image analysis and clinical\ndecision-making.\n2.2. Tumor Synthesis\nTumor synthesis has emerged as an attractive research topic\nacross various medical imaging modalities, such as CT [14,\n53, 74], MRI [6, 35, 71], and endoscopic videos [15, 45].\nThere are also many works on synthesizing non-cancerous\nlesions including chest CT synthesis [8, 53, 74, 75], and\ndiabetic lesion synthesis in retinal images [18, 78]. Re-\ncent studies focus on improving the realism of synthetic\ntumors in the liver, kidney and pancreas [14, 34, 42] by\nleveraging the large generative models like diffusion mod-\nels [32, 58, 63]. While these methods are conditioned only\non shape masks, Li et al. [47] propose text-driven tumor\nsynthesis by descriptive reports and conditional diffusion\nmodels. However, most of these works implemented tumor\nsynthesis as a data augmentation to improve tumor detec-\ntion tasks. They overlook its potential to empower clinical\ndecision-making in treatment planning. In this work, we\ndelve into the relatively unexplored field of tumor dynamics\nsimulation by generating post-treatment tumors using pre-\ntreatment scans and treatment actions.\n2.3. Prognosis and Clinical Decision-making\nPost-treatment prognosis in medical imaging is essential for\nevaluating therapy effectiveness, predicting disease recur-\nrence, and guiding further clinical decisions. CT is widely\nused to assess structural and functional changes in tumors\nfollowing interventions such as surgery, chemotherapy, ra-\n--- Page 3 ---\nPre-TreatmentSimulatedPost-TreatmentActionCombo\nForward Tumor DynamicsRiskScore\nOptimal Treatment1.2.CisplatinPVARaltitrexedLipiodolGelatinSponge3.LobaplatinPVATumor Generative Model<Dynamics Model>\n1.1.2.2.3.3.RecommendatreatmentplanfortheHCCpatient,giventhepre-treatmentCT.\nTreatmentAction BaseDrugsEmbolisms\nGPTs <Policies>Survival Analysis Model<Heuristic Function>\nInverse Dynamics\n2.RaltitrexedLipiodolGelatinSpongeFigure 2. Overview of TACE Protocol Exploration by Medical World Model. (1) GPTs (Policy Model): construct the TACE action\ncombos by the observation of pre-treatment CT, integrating clinical guidelines and policies. (2) Tumor Generative Model (Dynamics\nModel): simulates post-treatment tumor based on different TACE intervention protocols, predicting treatment outcomes. (3) Survival\nAnalysis Model (Heuristic Function): assesses risk scores from both simulated post-treatment CT and pre-treatment CT to determine the\nmost effective TACE protocol. Note that the 3D tumor masks (colored in red) can be extracted using a well-trained segmentation network\n(as Assistant Model). The framework enables visually trackable protocol optimization by iterating between clinical policy guidance,\ngenerative modeling, and survival analysis.\ndiation therapy, transarterial chemoembolization (TACE),\nand immunotherapy [28, 36, 44, 48, 72, 73]. Lee et al. [44]\nemploy a CNN-based model to predict the post-treatment\nsurvival of patients with hepatocellular carcinoma (HCC)\nusing CT images and clinical information. In addition,\nLLMs are increasingly being explored to assist in clinical\ndecision-making [11, 29, 40, 46]. However, little attention\nhas been given to applying LLMs for post-treatment prog-\nnosis. They did not leverage the feedback from survival\nanalysis to achieve prompt intervention as well.\n3. Medical World Models\nOverall Framework . As shown in Fig. 2, our MeWM takes\na visual observation of pre-treatment CT x0, a language\ntreatment goal gto simulate the future state and explore\nthe best treatment protocol. Policy model (¬ß 3.1) acquires\nthe descriptive observation based on the visual state, and\nconstructs a set of treatment protocols by the language goal\nand clinical guidelines. To perform the exploration, given\nthe pre-treatment CT and an action combo, the dynam-\nics model (¬ß 3.2) predicts the concrete resulting state, i.e.,\ngenerating post-treatment CT. Finally, inverse dynamics\nmodel (¬ß 3.3) driven by Heuristic Function predicts the risk\nscore from pre-treatment CT and simulated post-treatment\nCT with tumor masks from Assistant Model, to effectively\nprune branches in search and heuristically determine the op-\ntimal solution.3.1. Policy Model\nVision-language models [13, 77] have emerged as a pow-\nerful source of prior knowledge about the clinical world,\nproviding rich information about how to complete promis-\ning treatment from large-scale internet data and clinical\nguidelines. Based on TACE clinical guidelines, we set\nup the exploration configurations, including all potential\nchemotherapy drugs ( e.g., Raltitrexed, Cisplatin) and em-\nbolism materials ( e.g., Lipiodol, Gelatin Sponge). The two\nparts constitute the action base, which provides possible\nTACE protocols for Generative Dynamics Models as condi-\ntions. Then, we adopt a pre-trained large multimodal model\n(LMM), e.g., GPT-4o, to serve as policies. Given a high-\nlevel goal g(e.g., ‚ÄúWhat TACE treatment protocols are rec-\nommended for a patient diagnosed with hepatocellular car-\ncinoma (HCC) given the pre-treatment CT?‚Äù), the policy\nmodel œÄVLM(x0, g)extracts the visual observation and tu-\nmor context from the given pre-treatment CT x0to prompt\nthe proper Transarterial Chemoembolization (TACE) ac-\ntions. To constrain the excessively large tree search in the\naction base, we further prompt the Large Language Reason-\ning Model, i.e., Deepseek-R1 [25], to refine the drug set and\nembolism set by the clinical policies, whose final cardinali-\nties are DandE, respectively.\n3.2. Dynamics Model\nRadiotherapy Report Extraction and Generation. While\nmost existing studies focus on human-authored radiology\nreports, we aim to address radiotherapy reports to extract\n--- Page 4 ---\nRoutinepre-proceduralpreparationforinterventionalsurgerywascompleted.Thepatientwasplacedinthesupineposition.Aftersterilizationanddraping,therightfemoralarterywaspuncturedunderlocalanesthesia,anda5Fcathetersheathwasinserted‚Ä¶‚Ä¶Thefollowingagentswereinfusedviatheceliactrunk:Raltitrexed(4mg),Idarubicin(30mg),andOxaliplatin(100mL).‚Ä¶‚Ä¶Underfluoroscopicguidance,aslowembolizationwasperformedusingamixtureofapproximately9mLofLipiodolemulsion(10mLLipiodoland10mgIdarubicin)combinedwithanappropriateamountofgelatinspongeparticles‚Ä¶‚Ä¶Thefinalangiographyshowedocclusionofthetumor'sbloodsupplyartery.Aftertheoperation,thecatheterwaswithdrawn,thecathetersheathwasremoved,therightfemoralarterywascompressedtostopbleeding,andpressurebandagewasapplied.Theoperationwassuccessfullycompleted.\nGPT-4o\nDeepseek-R1KeywordsExtractionRaltitrexed4mg;Adasone30mg;100mlOxaliplatin;10mlultra-liquidLipiodol;GelatinSponge\nGPT-4o\nDeepseek-R1ActionGenerationRaltitrexed4mg,Idarubicin30mg,and100mlOxaliplatinwereinfusedthroughthecatheter;10 ml ultra-liquid Lipiodol and 10 mg Idarubicinwere mixed to create an emulsion for embolization; 9 ml of the emulsion was slowly injected under fluoroscopic guidance; An appropriate amount of Gelatin Sponge was used for embolization; \nAction-driven 3D Diffusion ModelActionFusion\nAction EncoderCTEncoderCTDecoder\nText-drivenMorpho-Gaussian AttenuationPre-Treatment CT\nRaltitrexed 4mg Epirubicin5mgLipiodol 5ml\nRaltitrexed 4mg Epirubicin10mgLipiodol 10ml \nLobaplatin 20mgIdarubicin 20mgLipiodol 10mlGelatinSpongePush\nPull\n(a) Radiotherapy Report Extraction and Generation\n(b) Post-Treatment Tumor Generation(c) Combo Contrastive LearningSynthesized Post-Treatment CT\nFigure 3. Dynamics Model based on Tumor Generative Model. The training framework consists of three parts: (a) Radiotherapy Report\nExtraction and Generation : GPT-4o and Deepseek-R1 extract key treatment details from radiotherapy reports and generate corresponding\nTACE surgical actions. (b) Post-Treatment Tumor Generation : An Action-driven 3D Diffusion Model is conditioned by fused action em-\nbeddings and attenuated CT features to generate post-treatment tumors that simulate treatment outcomes. (c) Combo Contrastive Learning\n(CCL) : The model learns from treatment variations by pushing apart dissimilar combos and pulling together similar ones, improving its\nability to generate realistic and action-aware post-treatment tumor appearances.\nmore comprehensive information on treatment protocols.\nHowever, raw radiotherapy reports pose significant chal-\nlenges due to noise and fragmented information, which hin-\nder controlled tumor synthesis. To mitigate these issues, we\npropose a two-stage text preprocessing framework consist-\ning of data cleaning and augmentation. In the first stage,\nwe perform keyword extraction by aggregating the outputs\nof both GPT-4o and DeepSeek-R1, focusing on key enti-\nties such as drugs, embolic agents, and their corresponding\ndosages. In the second stage, we leverage the same tools\nfor text generation, constructing a structured core action de-\nscription based on the extracted keywords. This approach\nenhances the consistency and informativeness of processed\nreports, facilitating downstream tasks in tumor synthesis\nand treatment analysis.\nPost-Treatment Tumor Generation. We adopt Latent\nDiffusion Models (LDMs) [58] for latent feature extraction\nfrom 3D Pre-treatment CT volumes and integrate textual ac-\ntion embedding for controlled tumor synthesis. Each 3D\nPost-treatment CT volume x1‚ààRH√óW√óDis encoded into\na lower dimensional latent representation z1=E(x1)us-\ning a 3D VQGAN autoencoder [22]. In the latent space,\nfollowing the spirit of DiffTumor [14], we define a diffu-\nsion process that progressively adds noise to the latent rep-\nresentation z1over discrete time steps t= 1, ..., T . Givena pair of tumor-present pre-treatment CT volume x0and\nthe mask of its tumor region m0, we condition the denois-\ning model on the masked pre-treatment latent representation\nz‚Ä≤\n0=E(m‚Ä≤\n0‚äôx0)where m‚Ä≤\n0is the attenuated mask from\nm0by our proposed Text-driven Morpho-Gaussian Attenu-\nation . Specifically, to mimic the effects of TACE treatment,\nthe process begins with occlusion assessment on radiother-\napy reports. The textual descriptions ( e.g.,occluded ,re-\nduced ,disappear ) are extracted and analyzed to determine\nthe attenuation level l‚àà {1,2,3,4}, where a higher value\ncorresponds to better tumor curative effects. Then, morpho-\nlogical erosion and dilation with the adaptive kernel by lare\napplied to m0, simulating occlusion-induced tumor struc-\ntural dynamics. Simultaneously, adaptive Gaussian blur-\nring is employed to exhibit the characteristics of heteroge-\nneous intensity changes due to lipiodol deposition, necrotic\ntransformation, and reduced perfusion. The final attenuated\nmask m‚Ä≤\n0is computed by the three steps, ensuring a smooth\ntransition between tumor and organ tissues. Note that this\nattenuation is only used during training.\nAlso, we condition the denoising model on the generated\ntextual action. Given the action combo a={a1, ..., a H},\neach sub-action, respectively, undergoes encoding through\na CLIP [57] text encoder œï(¬∑)followed by linear projection\nœÉ1(¬∑), enabling dimension reduction to a latent clinical con-\n--- Page 5 ---\ncept space. To enhance the semantics of action conditions\ninDdifferent drug and Eembolism keywords, we intro-\nduce learnable concept embeddings c, which extract key-\nword representations from the given action combo. This ex-\nplicit pharmaceutical grounding enables precise modeling\nof therapeutic components while maintaining robustness to\ncontext variations. The final action condition œÑ(a)is the\nfusion of holistic text embeddings and concept embeddings\nby fully connected layers œÉ2, :\nœÑ(a) =œÉ2([[œÉ1(œï(a1)), ..., œÉ 1(œï(aH))], c]), (1)\nwhere [¬∑]denote concatenation operation.\nThe training objective of diffusion model is as follows:\nEz1,œµ‚àºN(0,1),t\u0014\r\r\rœµ‚àíœµŒ∏\u0010\nzt, z‚Ä≤\n0, m0, œÑ(a), t\u0011\r\r\r2\n2\u0015\n,(2)\nwhere œµŒ∏(¬∑, t)is a 3D U-Net with interleaved self-attention\nlayers and convolutional layers [14, 32, 56] that predict the\nnoise given the input variable and conditions.\nCombo Contrastive Learning. We adopt a contrastive\nlearning strategy that aligns action combos with tumor evo-\nlution to enhance the realism and discrimination of post-\ntreatment tumor synthesis. Given a pre-treatment anchor\npair(x0, m0), along with an action combo a, the goal is to\ngenerate a post-treatment CT ÀÜxby generative model fDM(¬∑).\nFor each anchor, positive samples, ÀÜx+=\nfDM(x+\n0, m+\n0, a+), are defined as synthetic post-treatment\nCT from another pair but its action combo contains\nthe same drug/embolism keywords. Negative samples,\nÀÜx‚àí=fDM(x0, m0, a‚àí), in contrast, are generated using the\nsame pair but action combos with diverse keywords, lead-\ning to distinct tumor evolution patterns. This contrastive\nloss is incorporated into the tumor generative model:\nE\u0014\n‚àílogexp ( sim(ÀÜx,ÀÜx+)/Œ¥)Pexp ( sim(ÀÜx,ÀÜx‚àí)/Œ¥)‚àíexp ( sim(ÀÜx,ÀÜx+)/Œ¥)\u0015\n,\n(3)\nwhere sim (¬∑)denotes cosine similarity, Œ¥is the tempera-\nture scaling factor. This contrastive learning strategy en-\nsures that tumors simulated from similar treatment proto-\ncols exhibit consistent attenuation effects, while those de-\nrived from distinct protocols remain differentiable.\n3.3. Inverse Dynamics Model\nInverse Dynamics Model, which empowers our full frame-\nwork, aims to infer the most effective treatment strategy by\nanalyzing relationships between pre-treatment conditions,\nintervention actions, and expected post-treatment outcomes.\nWe unfold its essence in three aspects: (1) Assitant Model;\n(2) Heuristic Function and (3) TACE Protocol Exploration.\nAssistant Model. To better discriminate the tumor in\nsynthesized post-treatment CT ÀÜx, we introduce a tumorsegmentation model as Assistant Model Hseg(¬∑). Post-\ntreatment tumors are characterized by heterogeneous high-\nintensity regions due to calcification/lipiodol deposition, ir-\nregular shapes reflecting necrotic tissue changes, and re-\nduced or absent contrast enhancement in viable tumor areas,\nin contrast to traditional pre-treatment CT tumors. Thus, we\nadapt a pre-trained nnUNet-based [38] model to this post-\ntreatment context by finetuning it on our ground truth pairs\nof post-treatment CT and mask. Given the well-trained As-\nsistant Model, the simulated post-treatment CT ÀÜxfrom Tu-\nmor Generative Model is processed for the segmentation of\nliver and tumor. The post-treatment CT with the predicted\nmask ÀÜmis subsequently utilized for survival analysis.\nHeuristic Function. We use survival analysis model\nto implement a heuristic function Hsurv(x0, m0,ÀÜx, m 0, g),\nwhich quantifies the efficacy under the specific TACE action\ncombo by the output risk score. Inspired by DeepSurv [39],\nwe utilize a 3D convolution-based model structure, the 3D\nResNet (MC3) [65], as the feature extractor of survival anal-\nysis model. Given the pre-treatment pair (x0, m0)and sim-\nulated post-treatment pair (ÀÜx,ÀÜm), we extract their concate-\nnated CT and mask, respectively, and bidirectionally align\nthe semantics of pre- and post-treatment by Cross-Attention\nTransformer [41]. After that, we adopt an attention-based\naggregator to fuse pre- and post-features, followed by fc\nlayers to determine the risk score. The action combo with\na lower risk score should bring greater efficacy for the pa-\ntient. Note that, for training, we leverage multi-task learn-\ning strategy, i.e., CoxPH [39] and OS regression, to improve\nthe generalization of survival analysis.\nTACE Protocol Exploration. Given a combination of the\nproposed models above, we are able to predict TACE pro-\ntocol from any Pre-treatment CT xby a language treatment\ngoalg. To reason the optimal action combo, we propose to\nsearch for a list of actions to reach g, corresponding to find-\ning a treatment plan consisting of both drug and embolism\ncomponents, which optimizes:\nÀÜx‚àó\n1:H= arg min\nÀÜx1:H‚àºœÄVLM,fDMHsurv(x0, m0,ÀÜx, H seg(ÀÜx), g),(4)\nwhere H=Hd+He.\nWith this objective in mind, we exhibit a tree-search ex-\nploration procedure. Our exploration algorithm initializes a\nset of Bparallel protocol beams. We sample the potential\naction space composed of Ddrugs and Eembolisms and\nclinical rules using œÄVLM. The clinical rules are introduced\nto prune unreasonable branches, e.g., concomitant use of\nmultiple platinum-based agents is contraindicated due to the\nrisk of cumulative toxicity and myelosuppression. We se-\nquentially explore the two parts to ensure that TACE pro-\ntocol contains both drugs and embolism. For each current\naction combo, we synthesize Tpost-treatment tumors from\nfDM(x0, m0, a)to obtain a more reliable simulation. We\n--- Page 6 ---\nAlgorithm 1 TACE Protocol Exploration with MeWM\n1:Input: Pre-treatment CT x0, Pre-treatment tumor mask m0, Language treatment goal g\n2:Functions: VLM Policy Model œÄVLM, Dynamics Model fDM, Heuristic Function Hsurv, Assistant Model Hseg\n3:Hyperparameters: Drug Actions factor D, Embolism Actions factor E, Tumor Generative factor T, Protocol Beams B, Drug\nhorizon Hd, Embolism horizon He\n4:plans‚Üê[[x0]‚àÄi‚àà {1. . . B}] # Initialize B Different TACE Protocol Beams\n5:drug 1:D, embo 1:E, rule‚ÜêœÄVLM(x0, g) # Generate DDifferent Drug, EEmbolism Actions, Clinical Rules\n6:forh= 1. . . H ddo\n7: forb= 1. . . B do\n8: tumors ‚Üê[fDM(x, drug i)for j in (1. . . T)for i in (1. . . D)ifrule]# Generate tumors from xand plans[b] under rule\n9: plans[b].append( argmin (tumors, Hsurv, H seg)) # Add Tumor with Lowest Risk to Plan\n10: end for\n11: max idx, min idx‚Üêargmax (plans, Hsurv, H seg),argmin (plans, Hsurv, H seg)\n12: plans[max idx]‚Üêplans[min idx] # Periodically Replace the Plan with High Risk\n13:end for\n14:forh= 1. . . H edo\n15: forb= 1. . . B do\n16: tumors ‚Üê[fDM(x, embo i)for j in (1. . . T)for i in (1. . . E)ifrule]# Generate tumors from xand plans[b] under rule\n17: plans[b].append( argmin (tumors, Hsurv, H seg)) # Add Tumor with Lowest Risk to Plan\n18: end for\n19: max idx, min idx‚Üêargmax (plans, Hsurv, H seg),argmin (plans, Hsurv, H seg)\n20: plans[max idx]‚Üêplans[min idx] # Periodically Replace the Plan with High Risk\n21:end for\n22:plan‚Üêargmin (plans, Hsurv, H seg) # Return Plan with Lowest Risk\nthen use our heuristic function Hsurv(x0, m0,ÀÜx,ÀÜm, g)with\nthe assistance of Hseg(ÀÜx)to select the generated tumor with\nthe best average survival score of Treplicas among DorE\nactions. After every step of extending all beams, we discard\nthe beam with the worst survival score and replace its ac-\ntion combo with the best beam. To prevent cumulative toxi-\ncity and organ dysfunction, we prohibit over-exploration by\ndrug horizon Hdand embolism horizon He. Our final ac-\ntion combo is taken from the beam with the best survival\nscore and adopted as TACE protocol for the patient. The\npseudocode of our method is also provided in Algorithm 1.\n4. Experiments\nHCC-TACE In-house Dataset. We collect a large repos-\nitory of 338 longitudinal paired pre- and post-treatment CT\nscans with well-annotated liver/tumor masks and clinical\nrecords, such as TACE radiotherapy reports as gold action\nand Overall Survival (OS) time. We split the training set\n(validation set included) and testing sets in the 9:1 ratio.\nHCC-TACE-Seg Public Dataset [55]. For external vali-\ndation, we use patients from HCC-TACE-Seg public dataset\nreferring to a single-institution collection with confirmed\nHCC treated at The University of Texas MD Anderson\nCancer Center. We conduct data curation and preprocess-\ning to collect 78 cases containing pre-treatment CT, post-\ntreatment CT, TACE Gold Action, and OS time. We use\n80% cases to fine-tune and validate MeWM and leave 20%\ncases for the exploration evaluation.4.1. Evaluation on Generation Quality\nVisual Turing Test (Human Evaluation). We conduct\nan action-driven Visual Turing Test on 240 CT scans of\npost-treatment tumors, where 120 scans contain real post-\ntreatment tumors, and 120 scans contain synthetic post-\ntreatment tumors generated by different tumor synthesis\nmodels. Three radiologists (R1-R3) participated in this\nstudy, independently evaluating five groups of 48 CT scans\neach and classifying them as either real or synthetic. It\nis important to note that the radiologists‚Äô evaluations are\nbased on whether the synthetic tumor closely resembles a\npost-treatment tumor, which typically contains a mixture of\nlipiodol deposition, necrotic, and viable tumor regions, dis-\ntinguishing it from ordinary pre-treatment tumors. The test\nresults are summarized in Table 1. The sensitivity scores\nof all radiologists remain high (above 91%), demonstrating\ntheir ability to correctly identify real post-treatment tumors.\nHowever, specificity scores vary among the methods, in-\ndicating different levels of realism in the synthetic tumors.\nNotably, our method MeWM achieves the lowest specificity\nscores (79.17% for R1, 70.83% for R2, and 75.00% for R3),\nsuggesting that a large proportion of synthetic tumors gen-\nerated by our approach are mistaken as real. This indicates\nsuperior realism compared to other methods such as Syn-\nTumor [34], Pixel2Cancer [42], DiffTumor [14], and Tex-\ntoMorph [47]. Figure 4 illustrates examples from the test,\nwhere a real tumor is compared with synthetic tumors that\n--- Page 7 ---\nTable 1. Action-driven Visual Turing Test involves three radiologists (R1-R3) each evaluating five groups of 48 CT scans each, with 24\nreal post-treatment tumors and 24 synthetic post-treatment tumors from a tumor generative model, respectively. They were tasked with\ncategorizing each CT scan as either real or synthetic. A higher sensitivity score indicates better discriminative ability of radiologists, while\na lower specificity score indicates a higher number of synthetic tumors being identified as real. We also provide perceptual evaluation using\nFID and LPIPS compared to corresponding real post-treatment scans. Lower FID and LPIPS indicate better simulation results.\nMethodsR1 R2 R3 Perceptual metrics\nsensitivity specificity ‚Üìaccuracy sensitivity specificity ‚Üìaccuracy sensitivity specificity ‚Üìaccuracy FID‚Üì LPIPS‚Üì\nSynTumor [34] 100.0 95.83 97.92 87.50 95.83 91.67 100.0 95.83 97.92 3.33 0.6832\nPixel2Cancer [42] 95.83 100.0 97.92 91.67 95.83 93.75 100.0 100.0 100.0 3.34 0.6831\nDiffTumor [14] 100.0 91.67 95.83 95.83 87.50 91.67 100.0 87.50 93.75 1.40 0.7660\nTextoMorph [47] 100.0 91.67 95.83 91.67 83.33 87.50 95.83 87.50 91.67 1.03 0.9111\nMeWM (Ours) 100.0 79.17 89.58 91.67 70.83 81.25 91.67 75.00 83.33 0.71 0.6120\nReal\nFigure 4. Examples of Visual Turing Test . We present one real\ntumor alongside examples of synthetic tumors that were correctly\nand incorrectly identified. A red dot indicates the radiologist clas-\nsified the post-treatment tumor as synthetic, while a green dot sig-\nnifies it was identified as real.\nradiologists correctly or incorrectly classified. This high-\nlights synthetic tumors closely resemble real post-treatment\ntumors.\nPerceptual Evaluation. We perform perceptual evalua-\ntion using FID and LPIPS scores, where lower values indi-\ncate better simulation quality. Our method achieves the best\nFID (0.71) and LPIPS (0.6120), demonstrating the highest\nfidelity in synthetic tumor generation. These results confirm\nthat MeWM effectively synthesizes realistic post-treatment\ntumors, making it more challenging for radiologists to dis-\ntinguish between real and synthetic cases.\n4.2. Survival Analysis\nIn Figure 5, we evaluate survival risk regression between\nthe popular Cox Proportional Hazards model [23] and our\nheuristic function model on the HCC-TACE-Seg dataset.\nThe true risk distribution (left) is estimated using the\nNelson-Aalen estimator [17]. The Cox model fails to ac-\ncurately distinguish between high- and low-risk samples\nfrom low-dimensional deep features, resulting in an overly\nsmoothed risk distribution. In contrast, our model produces\na risk map that better aligns with the true distribution, ef-\nfectively capturing variations in risk levels. Error analysis\nshows higher Mean Square Error (MSE), i.e., 0.3550 for\nCox and lower MSE, 0.2142 for our model, indicating supe-\nrior accuracy. Figure 6 further presents Kaplan-Meier sur-\nvival curves comparing risk stratification performance be-\nTrue Risk ScoreCox Model Risk ScoreOur Model Risk Score\nCox Model Error (MSE:0.3550)Our Model Error (MSE:0.2142)ùíôùüéùíôùüèùíôùüéùíôùüèùíôùüéùíôùüè\nùíôùüéùíôùüèùíôùüéùíôùüè\nFigure 5. Performance of heuristic function on survival analy-\nsis.The first three heatmaps show the true risk distribution, Cox\nmodel predictions, and our heuristic function predictions. The last\ntwo depict prediction errors, with lower MSE (0.2142) for our\nmodel compared to the Cox model (0.3550), demonstrating im-\nproved accuracy in capturing localized risk patterns.\nRadiomics-based Cox Model: c-index=0.472, p-value=9.255e-01Our Deep Model: c-index=0.752, p-value=6.740e-05\nSurvival ProbabilityTime (months)Time (months)\nFigure 6. Kaplan-Meier Survival Curves: Radiomics-based\nCox Model [67] vs. our deep model. The left shows the sur-\nvival curves predicted by the Cox model based on Radiomics fea-\ntures. The right presents the survival curves from our model based\non deep features, which achieves a significantly higher c-index\nof 0.752 and a log-rank p-value of 6.74e‚àí5, demonstrating a\nstronger ability to distinguish between high- and low-risk groups.\nShaded areas represent confidence intervals.\ntween the Radiomics-based Cox model and our deep model.\nThese results demonstrate that our heuristic function better\nestimates survival risks, reduces prediction errors, and cap-\ntures complex patterns beyond the capabilities of the Cox\nmodel and radiomics features.\n--- Page 8 ---\nTable 2. TACE Protocol Exploration Evaluation on HCC-TACE in-house dataset and public dataset . F1-score, Jaccard index,\nPrecision, and Recall are computed between the predicted action combo and gold action. MeWM significantly advances multimodal GPTs\nin exploring optimal individualized treatment protocol across all metrics, even comparable to interventional physicians.\nIn-house dataset Public datasetMethodsF1-score ‚Üë Jaccard ‚ÜëPrecision ‚ÜëRecall‚Üë F1-score ‚Üë Jaccard ‚ÜëPrecision ‚ÜëRecall‚Üë\nPhysician w/ Pre-CT 48.81 38.44 46.67 54.67 71.43 63.10 66.67 78.57\nPhysician w/ MeWM 61.51 (+13%) 49.89 60.89 65.44 80.00 (+9%) 73.81 76.16 85.71\nQwen2.5-VL [3] 37.09 24.49 34.44 41.83 47.14 34.40 53.57 42.86\nGPT-4o [37] 41.97 27.81 35.93 52.78 44.29 32.74 57.14 38.10\nClaude-3.7-sonnet [2] 40.93 28.55 45.83 37.78 44.76 33.81 64.29 35.71\nCT2Rep [30] 27.75 17.21 30.83 25.83 43.61 28.57 53.57 37.50\nMedGPT [19, 54] 37.51 25.57 32.78 45.21 47.14 40.48 50.00 45.24\nHuatuoGPT-Vision [76] 40.13 29.08 40.11 42.28 52.62 42.26 54.76 51.19\nMeWM(Ours) 52.38 38.59 63.06 46.17 64.08 48.45 72.62 58.93\n4.3. Results on TACE Protocol Exploration\nEvaluation Strategy. For treatment planning evaluation,\nwe utilize four metrics in Table 2: (1) F1-score : harmonizes\nPrecision and Recall, balancing redundancy and omissions;\n(2)Jaccard Index : measures prediction overlap with gold\nactions, emphasizing category-level alignment; (3) Preci-\nsion: reflects recommendation purity, penalizing incorrect\nor redundant drugs/embolisms; (4) Recall : captures thera-\npeutic coverage, highlighting critical omissions.\nPartial Observation Misleads GPTs. For Multi-\nmodal Large Language Models ( e.g., GPT-4o, MedGPT,\nHuatuoGPT-Vision), they are prompted with pre-treatment\nCT slices and allowed to predict the action combo from the\ngiven action set. These inferior results (over -10% in F1-\nscore) to MeWM demonstrate that it tends to make deficient\nplanning relying solely on vision-language models and their\ncommonsense reasoning. This also validates the necessity\nof simulation from pre-treatment to post-treatment.\nMeWM as A Clinical Decision-support Tool. MeWM\ndemonstrates significant potential in augmenting the ca-\npabilities of radiologists and physicians, underscoring its\nclinical relevance in optimizing TACE planning. Reliance\nsolely on pre-treatment CT often results in partial ob-\nservation and suboptimal targeting due to heterogeneous\npathological conditions. By incorporating MeWM‚Äôs rec-\nommended protocol, clinical decision-making is markedly\nenhanced, yielding performance improvements of 12.70,\n11.45, 14.22, and 10.77 in F1-score, Jaccard, Precision,\nand Recall on our dataset. MeWM facilitates accurate tu-\nmor localization and enables predictive assessment of post-\nembolization outcomes, thereby reducing procedural uncer-\ntainty. Moreover, its synthetic post-treatment CT projec-\ntions help anticipate embolization efficacy, optimize TACE\ndistribution, and mitigate non-target embolization risks,\ncontributing to enhanced therapeutic precision and individ-\nualized strategies. MeWM serves as a critical decision-\nsupport tool in interventional oncology, bridging anatomi-\nFinalTACEProtocol\nDrug:\tRaltitrexed;\tLobaplatinEmbolism:\tLipiodolDrug:\tRaltitrexed;\tLobaplatin;\tIdarubicinEmbolism:\tLipiodol;\tGelatin\tSpongeDrug:\tRaltitrexed;\tLobaplatinEmbolism:\tLipiodol;\tGelatin\tSponge\nIntervention\nRadiologistTACEProtocol\nMeWMTACEProtocol\nPost-treatment tumor\nSimulationDiagnoseFigure 7. Example of MeWM intervention in clinical applica-\ntions. The radiologist initially proposes a TACE protocol with\nRaltitrexed, Lobaplatin, Idarubicin, and embolization using Li-\npiodol and Gelatin Sponge. MeWM simulates a protocol with\nRaltitrexed, Lobaplatin, and Lipiodol. After intervention, the op-\ntimized protocol removes Idarubicin but restores Gelatin Sponge,\naligning with gold action.\nTable 3. Ablation studies of TACE protocol exploration on both\ndatasets . ‚ÄúAM‚Äù denotes Assistant Model, while ‚ÄúCCL‚Äù denotes\nCombo Contrastive Learning. Two components significantly con-\ntribute to better exploring the optimal treatment.\nMetrics ‚ÜëIn-house dataset Public dataset\nw/o AM w/o CCL w/o AM w/o CCL\nF1-score 49.13 (-3.9) 50.97 (-1.4) 60.03 (-4.1) 62.90 (-1.2)\nJaccard 35.40 (-3.2) 36.76 (-1.8) 45.36 (-3.1) 46.97 (-1.5)\nPrecision 56.39 (-6.7) 60.57 (-2.5) 75.00 (+2.4) 70.10 (-2.5)\nRecall 45.22 (-1.0) 45.36 (-0.8) 52.38 (-5.6) 56.72 (-2.2)\ncal imaging with functional assessment for meaningful clin-\nical outcomes. As shown in Figure 7, interventional physi-\ncians refine TACE protocols by MeWM intervention, align-\ning treatment with expert practices.\nAblation Study. As shown in Table 3, we ablate the\neffectiveness of Assitant Model and Combo Contrastive\nLearning (CCL) in TACE Protocol Exploration. The results\ndemonstrate that both the Assistant Model and Combo Con-\ntrastive Learning (CCL) contribute significantly to its per-\nformance. Removing the assistant model, which provides\n--- Page 9 ---\nthe location information of tumors for heuristic function,\nleads to a critical drop in F1-score (52.38 ‚Üí49.13) on our\ndataset, as well as on public dataset. Similarly, omitting\nCCL reduces F1-score ( e.g., 52.38 ‚Üí50.97), indicating that\nCCL enhances the model‚Äôs discrimination on action units.\nOverall, MeWM achieves the best results across all metrics,\neven outperforming radiologists in some areas, validating\nits effectiveness.\n5. Conclusion\nWe present Medical World Model, which marks a step to-\nward AI-driven precision medicine by simulating disease\nevolution and optimizing clinical strategies. By bridging\ngenerative modeling with medical decision-making, it en-\nables a deeper understanding of treatment outcomes and re-\nfines intervention planning. The advancements of MeWM\nin tumor synthesis and survival analysis set the stage for\nfuture AI systems that seamlessly integrate with clinical\nworkflows, driving the next generation of longitudinal data-\ndriven healthcare.\nReferences\n[1] deedsbcv. 2, 5\n[2] AI Anthropic. Claude 3.5 sonnet model card addendum.\nClaude-3.5 Model Card , 2024. 8\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-\nhai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren\nFu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen\nCheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Jun-\nyang Lin. Qwen2.5-vl technical report. arXiv preprint\narXiv:2502.13923 , 2025. 8\n[4] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and\nYann LeCun. Navigation world models. In CVPR , 2025. 2\n[5] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene\nV orontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin,\nColin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel\nChartrand, et al. The liver tumor segmentation benchmark\n(lits). Medical image analysis , 84:102680, 2023. 2\n[6] Benjamin Billot, Douglas N Greve, Oula Puonti, Axel\nThielscher, Koen Van Leemput, Bruce Fischl, Adrian V\nDalca, Juan Eugenio Iglesias, et al. Synthseg: Segmenta-\ntion of brain mri scans of any contrast and resolution without\nretraining. Medical image analysis , 86:102789, 2023. 2\n[7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2\n[8] Christian Bluethgen, Pierre Chambon, Jean-Benoit Del-\nbrouck, Rogier van der Sluijs, Ma≈Çgorzata Po≈Çacin,\nJuan Manuel Zambrano Chaves, Tanishq Mathew Abraham,\nShivanshu Purohit, Curtis P Langlotz, and Akshay S Chaud-\nhari. A vision‚Äìlanguage foundation model for the generationof realistic chest x-ray images. Nature Biomedical Engineer-\ning, pages 1‚Äì13, 2024. 2\n[9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258 , 2021. 1\n[10] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack\nParker-Holder, Yuge Shi, Edward Hughes, Matthew Lai,\nAditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Ge-\nnie: Generative interactive environments. In Forty-first Inter-\nnational Conference on Machine Learning , 2024. 2\n[11] Felix Busch, Philipp Prucker, Alexander Komenda, Sebas-\ntian Ziegelmayer, Marcus R Makowski, Keno K Bressem,\nand Lisa C Adams. Multilingual feasibility of gpt-4o for\nautomated voice-to-text ct and mri report transcription. Eu-\nropean Journal of Radiology , 182:111827, 2025. 3\n[12] M Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric\nKerfoot, Yiheng Wang, Benjamin Murrey, Andriy Myro-\nnenko, Can Zhao, Dong Yang, et al. Monai: An open-source\nframework for deep learning in healthcare. arXiv preprint\narXiv:2211.02701 , 2022. 5\n[13] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shu-\nnian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei\nZhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, to-\nwards injecting medical visual knowledge into multimodal\nllms at scale. arXiv preprint arXiv:2406.19280 , 2024. 3\n[14] Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan\nYuille, Chen Wei, and Zongwei Zhou. Towards generaliz-\nable tumor synthesis. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n11147‚Äì11158, 2024. 2, 4, 5, 6, 7\n[15] Tong Chen, Shuya Yang, Junyi Wang, Long Bai, Hongliang\nRen, and Luping Zhou. Surgsora: Decoupled rgbd-flow\ndiffusion model for controllable surgical video generation.\narXiv preprint arXiv:2412.14018 , 2024. 2\n[16] Zeming Chen, Alejandro Hern ¬¥andez Cano, Angelika Ro-\nmanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi,\nMatteo Pagliardini, Simin Fan, Andreas K ¬®opf, Amirkeivan\nMohtashami, et al. Meditron-70b: Scaling medical\npretraining for large language models. arXiv preprint\narXiv:2311.16079 , 2023. 1\n[17] Enrico Colosimo, Fla¬¥ vio Ferreira, Maristela Oliveira, and\nCleide Sousa. Empirical comparisons between kaplan-\nmeier and nelson-aalen survival function estimators. Journal\nof Statistical Computation and Simulation , 72(4):299‚Äì308,\n2002. 7\n[18] Pedro Costa, Adrian Galdran, Maria Ines Meyer, Meindert\nNiemeijer, Michael Abr `amoff, Ana Maria Mendonc ¬∏a, and\nAur¬¥elio Campilho. End-to-end adversarial retinal image syn-\nthesis. IEEE transactions on medical imaging , 37(3):781‚Äì\n791, 2017. 2\n[19] Michael D Moor. Medgpt, 2025. Accessed: March 7, 2025.\n8\n[20] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan\nWahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter\nAbbeel, Joshua B Tenenbaum, et al. Video language plan-\nning. arXiv preprint arXiv:2310.10625 , 2023. 2\n[21] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan\n--- Page 10 ---\nWahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter\nAbbeel, Joshua B Tenenbaum, et al. Video language plan-\nning. In ICML , 2024. 2\n[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 12873‚Äì12883, 2021. 4\n[23] John Fox and Sanford Weisberg. Cox proportional-hazards\nregression for survival data. An R and S-PLUS companion to\napplied regression , 2002, 2002. 7\n[24] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709 , 2023. 2\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948 , 2025. 3, 5\n[26] David Ha and J ¬®urgen Schmidhuber. World models. arXiv\npreprint arXiv:1803.10122 , 2018. 2\n[27] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104 , 2023. 2\n[28] Amr Hagag, Ahmed Gomaa, Dominik Kornek, Andreas\nMaier, Rainer Fietkau, Christoph Bert, Yixing Huang, and\nFlorian Putz. Deep learning for cancer prognosis predic-\ntion using portrait photos by stylegan embedding. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention , pages 198‚Äì208. Springer,\n2024. 3\n[29] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal\nBhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer,\nMarcus Makowski, Rickmer Braren, Georgios Kaissis, et al.\nEvaluation and mitigation of the limitations of large lan-\nguage models in clinical decision-making. Nature medicine ,\n30(9):2613‚Äì2622, 2024. 1, 3\n[30] Ibrahim Ethem Hamamci, Sezgin Er, and Bjoern Menze.\nCt2rep: Automated radiology report generation for 3d medi-\ncal imaging. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention , pages 476‚Äì\n486. Springer, 2024. 8\n[31] Jan B Hinrichs, Hoen-Oh Shin, Daniel Kaercher, Davut Has-\ndemir, Tim Murray, Till Kaireit, Carolin Lutat, Arndt V ogel,\nBernhard C Meyer, Frank K Wacker, et al. Parametric re-\nsponse mapping of contrast-enhanced biphasic ct for evalu-\nating tumour viability of hepatocellular carcinoma after tace.\nEuropean radiology , 26:3447‚Äì3455, 2016. 5\n[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems , 33:6840‚Äì6851, 2020. 2, 5\n[33] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and Gian-\nluca Corrado. Gaia-1: A generative world model for au-\ntonomous driving. arXiv preprint arXiv:2309.17080 , 2023.\n2\n[34] Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng\nChen, Alan L Yuille, and Zongwei Zhou. Label-free livertumor segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n7422‚Äì7432, 2023. 2, 6, 7\n[35] Pu Huang, Dengwang Li, Zhicheng Jiao, Dongming Wei,\nBing Cao, Zhanhao Mo, Qian Wang, Han Zhang, and Ding-\ngang Shen. Common feature learning for brain tumor mri\nsynthesis by context-aware generative adversarial network.\nMedical Image Analysis , 79:102472, 2022. 2\n[36] Xiaoyu Huang, Yong Huang, Ping Li, and Kai Xu. Ct-based\ndeep learning predicts prognosis in esophageal squamous\ncell cancer patients receiving immunotherapy combined with\nchemotherapy. Academic Radiology , 2025. 3\n[37] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-\nman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\nhinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.\narXiv preprint arXiv:2410.21276 , 2024. 1, 8\n[38] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Pe-\ntersen, and Klaus H Maier-Hein. nnu-net: a self-configuring\nmethod for deep learning-based biomedical image segmen-\ntation. Nature methods , 18(2):203‚Äì211, 2021. 5, 2\n[39] Jared L Katzman, Uri Shaham, Alexander Cloninger,\nJonathan Bates, Tingting Jiang, and Yuval Kluger. Deep-\nsurv: personalized treatment recommender system using a\ncox proportional hazards deep neural network. BMC medi-\ncal research methodology , 18:1‚Äì12, 2018. 5\n[40] Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan,\nXuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghas-\nsemi, Cynthia Breazeal, Hae Park, et al. Mdagents:\nAn adaptive collaboration of llms for medical decision-\nmaking. Advances in Neural Information Processing Sys-\ntems, 37:79410‚Äì79452, 2024. 3\n[41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of the IEEE/CVF international confer-\nence on computer vision , pages 4015‚Äì4026, 2023. 5\n[42] Yuxiang Lai, Xiaoxi Chen, Angtian Wang, Alan Yuille, and\nZongwei Zhou. From pixel to cancer: Cellular automata in\ncomputed tomography. In International Conference on Med-\nical Image Computing and Computer-Assisted Intervention ,\npages 36‚Äì46. Springer, 2024. 2, 6, 7\n[43] Yann LeCun. A path towards autonomous machine intelli-\ngence version 0.9. 2, 2022-06-27. Open Review , 62(1):1‚Äì62,\n2022. 2\n[44] Kyung Hwa Lee, Jungwook Lee, Gwang Hyeon Choi, Ji-\nhye Yun, Jiseon Kang, Jonggi Choi, Kang Mo Kim, and\nNamkug Kim. Deep learning-based prediction of post-\ntreatment survival in hepatocellular carcinoma patients using\npre-treatment ct images and clinical data. Journal of Imaging\nInformatics in Medicine , pages 1‚Äì12, 2024. 3\n[45] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Y Feng,\nWuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, and Yixuan\nYuan. Endora: Video generation models as endoscopy simu-\nlators. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention , pages 230‚Äì240.\nSpringer, 2024. 2\n[46] Jia Li, Zichun Zhou, Han Lyu, and Zhenchang Wang. Large\nlanguage models-powered clinical decision support: enhanc-\ning or replacing human expertise?, 2025. 3\n--- Page 11 ---\n[47] Xinran Li, Yi Shuai, Chen Liu, Qi Chen, Qilong Wu, Pengfei\nGuo, Dong Yang, Can Zhao, Pedro RAS Bassi, Daguang\nXu, et al. Text-driven tumor synthesis. arXiv preprint\narXiv:2412.18589 , 2024. 2, 6, 7, 5\n[48] Junhao Liang, Weisheng Zhang, Jianghui Yang, Meilong\nWu, Qionghai Dai, Hongfang Yin, Ying Xiao, and Lingjie\nKong. Deep learning supported discovery of biomarkers for\nclinical prognosis of liver cancer. Nature Machine Intelli-\ngence , 5(4):408‚Äì420, 2023. 3\n[49] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi\nLu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng\nTang, and Zongwei Zhou. Clip-driven universal model for\norgan segmentation and tumor detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 21152‚Äì21164, 2023. 5\n[50] Jie Liu, Yixiao Zhang, Kang Wang, Mehmet Can Yavuz,\nXiaoxi Chen, Yixuan Yuan, Haoliang Li, Yang Yang, Alan\nYuille, Yucheng Tang, et al. Universal and extensible\nlanguage-vision models for organ segmentation and tumor\ndetection from abdominal computed tomography. Medical\nimage analysis , 97:103226, 2024. 5\n[51] Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, and\nJieneng Chen. Generative world explorer. arXiv preprint\narXiv:2411.11844 , 2024. 2\n[52] Yunhao Luo and Yilun Du. Grounding video models to ac-\ntions through goal conditioned exploration. arXiv preprint\narXiv:2411.07223 , 2024. 2\n[53] Fei Lyu, Mang Ye, Jonathan Frederik Carlsen, Kenny Er-\nleben, Sune Darkner, and Pong C Yuen. Pseudo-label guided\nimage synthesis for semi-supervised covid-19 pneumonia in-\nfection segmentation. IEEE Transactions on Medical Imag-\ning, 42(3):797‚Äì809, 2022. 2\n[54] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad,\nHarlan M Krumholz, Jure Leskovec, Eric J Topol, and\nPranav Rajpurkar. Foundation models for generalist medi-\ncal artificial intelligence. Nature , 616(7956):259‚Äì265, 2023.\n8\n[55] Ali Morshid, Khaled M Elsayes, Ahmed M Khalaf, Mo-\nhab M Elmohr, Justin Yu, Ahmed O Kaseb, Manal Hassan,\nArmeen Mahvash, Zhihui Wang, John D Hazle, et al. A ma-\nchine learning model to predict hepatocellular carcinoma re-\nsponse to transcatheter arterial chemoembolization. Radiol-\nogy: Artificial Intelligence , 1(5):e180021, 2019. 6, 2\n[56] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In International\nConference on Machine Learning , pages 8162‚Äì8171. PMLR,\n2021. 5\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748‚Äì8763. PmLR, 2021. 4\n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¬®orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684‚Äì10695, 2022. 2, 4\n[59] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu,Stephen James, Kimin Lee, and Pieter Abbeel. Masked\nworld models for visual control. In Conference on Robot\nLearning , pages 1332‚Äì1344. PMLR, 2023. 2\n[60] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian\nZhang, Xiangyang Ji, et al. Transmil: Transformer based\ncorrelated multiple instance learning for whole slide image\nclassification. Advances in neural information processing\nsystems , 34:2136‚Äì2147, 2021. 6\n[61] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,\nJason Wei, Hyung Won Chung, Nathan Scales, Ajay Tan-\nwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large\nlanguage models encode clinical knowledge. Nature ,\n620(7972):172‚Äì180, 2023. 1\n[62] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery\nWulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R\nPfohl, Heather Cole-Lewis, et al. Toward expert-level med-\nical question answering with large language models. Nature\nMedicine , pages 1‚Äì8, 2025. 1\n[63] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning , pages 2256‚Äì2265. PMLR, 2015.\n2\n[64] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 , 2023. 1\n[65] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proceedings of the\nIEEE conference on Computer Vision and Pattern Recogni-\ntion, pages 6450‚Äì6459, 2018. 5\n[66] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. Mocogan: Decomposing motion and content for\nvideo generation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1526‚Äì1535,\n2018. 2\n[67] Joost van Griethuysen et. al. Radiomics-based cox model,\n2025. Accessed: March 7, 2025. 7\n[68] X Wang, Z Zhu, G Huang, X Chen, and J Drivedreamer Lu.\nTowards real-world-driven world models for autonomous\ndriving. arXiv preprint arXiv:2309.09777 , 2023. 2\n[69] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jia-\ngang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-\ndrive world models for autonomous driving. In European\nConference on Computer Vision , pages 55‚Äì72, 2024. 2\n[70] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models for\nphysical robot learning. In Conference on robot learning ,\npages 2226‚Äì2240. PMLR, 2023. 2\n[71] Zhaohu Xing, Sicheng Yang, Sixiang Chen, Tian Ye, Yi-\njun Yang, Jing Qin, and Lei Zhu. Cross-conditioned diffu-\nsion model for medical image to image translation. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention , pages 201‚Äì211. Springer,\n2024. 2\n[72] Jiawen Yao, Yu Shi, Kai Cao, Le Lu, Jianping Lu, Qike\nSong, Gang Jin, Jing Xiao, Yang Hou, and Ling Zhang.\n--- Page 12 ---\nDeepprognosis: Preoperative prediction of pancreatic can-\ncer survival and surgical margin via comprehensive un-\nderstanding of dynamic contrast-enhanced ct imaging and\ntumor-vascular contact parsing. Medical image analysis ,\n73:102150, 2021. 3\n[73] Jiawen Yao, Yu Shi, Le Lu, Jing Xiao, and Ling Zhang.\nDeepprognosis: Preoperative prediction of pancreatic cancer\nsurvival and surgical margin via contrast-enhanced ct imag-\ning. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention , pages 272‚Äì282.\nSpringer, 2020. 3\n[74] Qingsong Yao, Li Xiao, Peihang Liu, and S Kevin Zhou.\nLabel-free segmentation of covid-19 lesions in lung ct. IEEE\ntransactions on medical imaging , 40(10):2808‚Äì2819, 2021.\n2\n[75] Wenfang Yao, Chen Liu, Kejing Yin, William Cheung, and\nJing Qin. Addressing asynchronicity in clinical multimodal\nfusion via individualized chest x-ray generation. Advances\nin Neural Information Processing Systems , 37:29001‚Äì29028,\n2025. 2\n[76] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhi-\nhong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu,\nZhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards\ntaming language model to be a doctor. arXiv preprint\narXiv:2305.15075 , 2023. 8\n[77] Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi,\nQian Liang, and Philip S Yu. Large language models for\nmedicine: a survey. International Journal of Machine Learn-\ning and Cybernetics , pages 1‚Äì26, 2024. 3\n[78] Yi Zhou, Xiaodong He, Shanshan Cui, Fan Zhu, Li Liu,\nand Ling Shao. High-resolution diabetic retinopathy im-\nage synthesis manipulated by grading and lesions. In In-\nternational conference on medical image computing and\ncomputer-assisted intervention , pages 505‚Äì513. Springer,\n2019. 2\n--- Page 13 ---\nAppendix\nTable of Contents\nA . Data Preprocessing 2\nA.1 . HCC-TACE-Seg dataset preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nA.2 . HCC-TACE dataset preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nB . Implementation Details 5\nB.1. Policy Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nB.2. Dynamics Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nB.3. Assistant Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nB.4. Heuristic Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nC . Comparison against Multi-Modal GPTs 6\n--- Page 14 ---\nA. Data Preprocessing\nA.1. HCC-TACE-Seg dataset preprocessing\nThe HCC-TACE-Seg dataset [55] refers to a single-institution collection of patients with confirmed hepatocellular carcinoma\n(HCC) who were treated at The University of Texas MD Anderson Cancer Center. Data preprocessing for HCC-TACE-Seg\ninvolves resampling the provided CT images to a standardized spatial resolution while preserving the integrity of the original\ndata structure. Specifically, images and masks are resampled to a target spacing of 0.8mm √ó 0.8mm √ó 3.0mm to standardize\nvoxel dimensions across different cases.\nLongitudinal Registration : Accurate image registration is essential to ensure that tumor boundaries are clearly defined\nacross both the liver and HCC regions in different imaging modalities, such as arterial phase (AP) and portal venous phase\n(PVP) scans. The longitudinal registration process involves aligning the post-AP image to the pre-AP image, and the post-\nPVP to the pre-PVP image, addressing any misalignments between scans. Both linear and non-linear registration methods\nare employed through the open-sourced registration framework deedsBCV [1] for optimal alignment.\nLiver and HCC Cancer Segmentation : We utilize a nnUNet-based [38] mode trained on the public LiTS dataset [5]\nfor liver and HCC cancer segmentation. For postprocessing, we adopt connected component analysis to extract the liver and\nHCC regions precisely. This approach ensures that the tumor and liver boundaries are defined clearly, which is crucial for\ndownstream analysis. An example of pre- and post-treatment CT images, along with liver and tumor segmentation generated\nfrom the HCC-TACE-Seg dataset, is shown in Figure 8.\nWe also conduct a Component Size Filtering strategy. A component size filtering step is applied, with a minimum thresh-\nold of 300 voxels, ensuring the accurate identification of tumor and liver regions. This step helps to remove noise or ir-\nrelevant small regions, improving the precision of segmentation results. Only the image data paired with the following\nmeta-information are selected for further analysis:\n‚Ä¢ Chemotherapy: Information about whether the patient underwent chemotherapy treatment, including details about the type\nof chemotherapy regimen.\n‚Ä¢ Overall survival: Overall survival time in months.\n‚Ä¢ Survival status: 0 indicates that the patient is alive or lost to follow-up, while 1 indicates death. Details are in Table 4.\nID 011 ID 025 ID 058\nHCC \nPre-CT\nHCC \nPost -CT\nFigure 8. Example of HCC-TACE-Seg dataset. The first row shows HCC Pre-CT images, and the second row shows HCC Post-CT images.\nThe red mask represents the liver, while the green mask represents the HCC tumor.\n--- Page 15 ---\nPatient ID Chemotherapy Overall Survival (months) Survival Status\nHCC 009 Cisplatin; Doxorubicin; Mitomycin; Lipiodol 4.7 1.0\nHCC 011 Cisplatin; Doxorubicin; Mitomycin; Lipiodol 19.3 1.0\nHCC 025 Cisplatin; Doxorubicin; Mitomycin; Lipiodol 30.0 1.0\nHCC 034 Doxorubicin; Lipiodol; LC beads 18.9 1.0\nHCC 042 Cisplatin; Mitomycin; Lipiodol 34.1 1.0\nHCC 051 Cisplatin; Mitomycin; Lipiodol 12.9 1.0\nHCC 058 Cisplatin; Mitomycin; Lipiodol 87.0 0.0\nHCC 067 Cisplatin; Doxorubicin; Mitomycin; Lipiodol 90.9 0.0\nHCC 079 Doxorubicin; LC beads; Lipiodol 42.5 1.0\nHCC 091 Doxorubicin; LC beads; Lipiodol 25.3 0.0\nTable 4. An example of HCC-TACE-Seg dataset metadata, including chemotherapy, overall survival, and survival status. For survival\nstatus, a value of 0 indicates that the patient is alive or lost to follow-up, while a value of 1 indicates death.\nID 623595 ID 657727 ID 705855\nHCC \nPre-CT\nHCC \nPost -CT\nFigure 9. Example of HCC-TACE dataset. The first row shows HCC Pre-CT images, and the second row shows HCC Post-CT images. The\nred mask represents the liver, while the green mask represents the HCC tumor. In post-treatment CT imaging of HCC, particularly after\nTransarterial Chemoembolization (TACE), the viable tumor region and its enhancement intensity decrease due to Lipiodol accumulation\nand treatment-induced necrosis. Lipiodol appears hyperdense (bright) on post-treatment CT, indicating areas that have been successfully\nembolized.\n--- Page 16 ---\nPatient ID Processed Chemotherapy OS (months) Survival Status\nHCC 08116730 Raltitrexed 4 mg was infused through the catheter; 5 ml\nultra-liquid Lipiodol and 5 mg Epirubicin were mixed to\ncreate an emulsion for embolization; the emulsion was\nslowly injected under fluoroscopic guidance; an appropri-\nate amount of Gelatin Sponge particles was used to em-\nbolize the tumor-feeding branches of the S8 segment of\nthe right hepatic artery; Lipiodol deposition in the tumor\nwas satisfactory; tumor-feeding arteries were occluded\non the final angiography.1.4 0.0\nHCC 01061677 THP 10 mg was infused through the catheter; 10 mg\nTHP and 10 ml ultra-liquid Lipiodol were mixed to cre-\nate an emulsion for embolization; 12 ml of the emulsion\nwas slowly injected under fluoroscopic guidance; Lipi-\nodol deposition in the tumor and satellite lesions was sat-\nisfactory; tumor-feeding arteries were occluded on the fi-\nnal angiography.75.7 1.0\nHCC 01192613 THP 40 mg and 30 ml ultra-liquid Lipiodol, along with\na small amount of contrast agent, were mixed to cre-\nate an emulsion for embolization; 30 ml of the emul-\nsion was slowly injected under fluoroscopic guidance; a\nsmall amount of Gelatin Sponge particles was used for\nembolization; Lipiodol deposition in the tumor was satis-\nfactory; no tumor staining was observed on the final an-\ngiography.84.6 1.0\nHCC 01204059 Cisplatin 40 mg was infused through the catheter; 10\nml ultra-liquid Lipiodol was slowly injected under flu-\noroscopic guidance for embolization of the right hepatic\nartery tumor-feeding branches; 3 ml ultra-liquid Lipiodol\nwas injected for protective embolization of the segment\nII branch of the left hepatic artery; Lipiodol deposition in\nthe tumor was acceptable; tumor staining mostly disap-\npeared on the final angiography.17.1 0.0\nHCC 01532843 Oxaliplatin 100 mg and Epirubicin 30 mg were infused\nthrough the catheter; 10 mg Epirubicin and 10 ml ultra-\nliquid Lipiodol were mixed to create an emulsion for em-\nbolization; 10 ml of the emulsion was slowly injected\nunder fluoroscopic guidance; an appropriate amount of\nGelatin Sponge particles was used to embolize the tumor-\nfeeding branches of the right hepatic artery; Lipiodol de-\nposition in the tumor was satisfactory; tumor staining dis-\nappeared on the final angiography.29.3 1.0\nHCC 01532843 Epirubicin 40 mg and Oxaliplatin 100 mg were in-\nfused through the catheter; 10 ml ultra-liquid Lipiodol\nwas slowly injected under fluoroscopic guidance for em-\nbolization; Lipiodol deposition in the tumor was satisfac-\ntory; tumor-feeding arteries were mostly occluded on the\nfinal angiography.21.6 1.0\nTable 5. An example of HCC-TACE dataset metadata, including chemotherapy, overall survival, and survival status. For survival status, a\nvalue of 0 indicates that the patient is alive or lost to follow-up, while a value of 1 indicates death.\n--- Page 17 ---\nA.2. HCC-TACE dataset preprocessing\nThe HCC-TACE dataset is a large-scale, self-collected repository containing 338 longitudinal pairs of pre- and post-treatment\nCT scans, along with well-annotated liver and tumor masks, as well as clinical records. These records include TACE radio-\ntherapy reports (considered the gold action) and Overall Survival (OS) time. Details are presented in Table 5. The dataset is\nsplit into training (including validation) and testing sets in a 9:1 ratio. All images and masks are resampled to a target spacing\nof 0.8mm √ó 0.8mm √ó 3.0mm to standardize voxel dimensions across different cases.\nLongitudinal Registration : We also employ deedsBCV [1] to align the post-AP image with the pre-AP image, and the\npost-PVP image with the pre-PVP image, addressing any misalignments between the scans.\nLiver and HCC Cancer Annotation : In this dataset, all liver and tumor masks for each CT scan are carefully annotated\nby radiologists. For postprocessing, we also apply connected component analysis to accurately extract the liver and HCC\nregions. An example of pre- and post-treatment CT images, along with liver and tumor segmentation generated from the\nHCC-TACE dataset, is shown in Figure 9.\nB. Implementation Details\nB.1. Policy Model\nWe adopt GPT-4o to obtain the initial observation from the given pre-treatment CT scans and collect the individualized\npotential drugs and embolism during TACE treatment. An example is presented in Figure 10. Then, we refine the action\nset using DeepSeek-R1 [25], which reasons the clinical conflicts in the current action set and summarizes a better action set\nusing clinical guidelines for individuals ( e.g., Multiple platinum-based drugs cannot be used simultaneously).\nB.2. Dynamics Model\nIn this study, we implement Dynamics Model by training the corresponding Diffusion Model [47] specifically from pre-\ntreatment liver tumors to post-treatment liver tumors. The CT scans are oriented according to specific axcodes and resampled\nto achieve isotropic spacing of 1.0√ó1.0√ó1.0mm3.96√ó96√ó96patches are randomly cropped around either foreground\nvoxels based on a set ratio. Their intensities are truncated to the range [ ‚àí175,600] to maintain the discrimination of lip-\niodol/necrosis/viable areas [31], then linearly normalized to [-1, 1]. We utilize the Adam optimizer with hyperparameters\nŒ≤1= 0.9andŒ≤2= 0.999, a learning rate of 0.0001, and a batch size of 10 per GPU. The training is conducted on A6000\nGPUs for 2 days, over a total of 2,000 iterations.\nB.3. Assistant Model\nWe employ a nnUNet-based [38] segmentation model for the segmentation of liver and tumor in post-treatment CT. As\nsuggested by Chen et al. [14], we generate realistic tumor-like shapes using ellipsoids, and combine these generated tumor\nmasks with the healthy CT volumes to create a range of realistic liver tumors. We pre-train the model on the generated and\nreal tumors for robust generalization. Then, we finetune it on post-treatment CT scans as well as liver and tumor masks. The\nimplementation is in Python, leveraging MONAI *. The CT scans are oriented according to specific axcodes and resampled\nto achieve isotropic spacing of 1.0√ó1.0√ó1.0mm3. Their intensities are truncated to the range [ ‚àí175,600] to maintain the\ndiscrimination of lipiodol/necrosis/viable areas, then linearly normalized to [-1, 1]. During training, 96√ó96√ó96patches\nare randomly cropped around either foreground or background voxels based on a set ratio. Each patch is subjected to a 90‚ó¶\nrotation with probability 0.1 and an intensity shift of 0.1 with probability 0.2. To avoid confusing the left and right organs,\nmirroring augmentation is not used.\nThe model is initialized with pre-trained liver tumor weights from DiffTumor [14], then fine-tuned on our dataset for\n2,000 epochs. We set the base learning rate to 0.0002 and use a batch size of 8, along with a linear warmup and a cosine\nannealing schedule. Training spans 2 days on eight A6000 GPUs. Additional details on the tumor synthesis process during\nSegmentation Model training can be found in DiffTumor [14].\nFor inference, a sliding window strategy with 0.75 overlap is used. Tumor predictions that fall outside their corresponding\norgans are removed by post-processing with organ pseudo-labels obtained from previous research‚Ä†.\nB.4. Heuristic Function\nWe implement a CNN-based survival analysis model as Heuristic Function. The framework adopts 3D ResNet (MC3) as the\nbackbone and Two-way Transformer as the interaction module of pre-treatment and post-treatment CT features. A multi-\n*Cardoso et al. [12]: https://monai.io/\n‚Ä†Liuet al. [49, 50]: https://github.com/ljwztc/CLIP-Driven-Universal-Model\n--- Page 18 ---\ninstance aggregator [60] with consecutive fully connected layers is utilized for survival risk scoring. It is implemented\non PyTorch using 8 NVIDIA RTX A6000 GPUs. Their intensities are truncated to the range [ ‚àí175,600] to maintain the\ndiscrimination of lipiodol/necrosis/viable areas, then linearly normalized to [-1, 1]. During training, 96√ó96√ó96patches\nare randomly cropped around either foreground or background voxels based on a set ratio. Each patch is subjected to a 90‚ó¶\nrotation with probability 0.1 and an intensity shift of 0.1 with probability 0.2. To avoid confusing the left and right organs,\nmirroring augmentation is not used. We utilize the Adam optimizer with hyperparameters Œ≤1= 0.9andŒ≤2= 0.999, a\nlearning rate of 0.00002, and a batch size of 5 per GPU. For the inference of each case, we predict the survival risk scores of\n5 patches around the foreground and average them to obtain the final score.\nC. Comparison against Multi-Modal GPTs\nWe carefully design prompt templates for multi-modal GPTs to generate TACE treatment protocol. The first template (Fig-\nure 11) is for our dataset with a larger action space, defining the task description, a predefined set of chemotherapy drugs\nand embolization materials, and an example of input-output in JSON format. The second template (Figure 12) is specifically\ndesigned for the HCC-TACE-Seg dataset, featuring a different selection of chemotherapy drugs and embolization materials.\nBoth prompts instruct GPTs to analyze CT images and generate an appropriate TACE treatment plan, submitting results in\nJSON format with predefined keywords.\n--- Page 19 ---\nYouarearadiationoncologist,please**listpotentialTACEdrugandembolismsets**basedonthepatient'spre-treatmentCTimage.Pleasefollowthebelowguidelines:---###**TaskDescription**1.AnalyzetheinputCTimagesandoutputpotentialTACEchemotherapydrugandembolizationmaterialsetsfortreatment.Youcanincludeanydrugsandembolismsthatyouthinkmaybehelpfulforthetreatment.ChemotherapydrugsandembolizationmaterialsarelimitedtothoseintheActionBase.2.TheTACEactionsetisoutputinJSONformat,includingtreatmentplankeywordssuchaschemotherapydrugsandembolizationmaterials.---###**ActionBase**####**ChemotherapyDrugs**-Raltitrexed-Epirubicin-Oxaliplatin-Lobaplatin-Mitomycin-Idarubicin-Nedaplatin-Pirarubicin-Cisplatin-Idarubicin-THP-Hydroxycamptothecin####**EmbolizationMaterials**-Lipiodol-GelatinSponge-PVA-AbsoluteAlcohol-NBCA-KMG---###**Example****Input**{\"image\":{image_property}\"patient_id\":001}**Output**{001:{\"ChemotherapyDrugs\":\"Raltitrexed;Lobaplatin;Oxaliplatin;Mitomycin;THP‚Äù‚ÄúEmbolizationMaterials‚Äù:‚ÄúLipiodol;GelatinSponge;PVA;NBCA‚Äù}}Figure 10. Policy model prompt template for our dataset.\n--- Page 20 ---\nYouarearadiationoncologist,pleasedesignaTACEtreatmentplanbasedonthepatient'spre-treatmentCTimage.Pleasefollowthebelowguidelines:---###**TaskDescription**1.AnalyzetheinputCTimagesandoutputtheTACEtreatmentplanthatyouthinkisappropriateforthepatient.Theplanshouldincludechemotherapydrugsandembolizationmaterialsthatcomplywithclinicalguidelines.ChemotherapydrugsandembolizationmaterialsarelimitedtothoseintheActionSet.2.TheTACEtreatmentplanisoutputinJSONformat,includingtreatmentplankeywordssuchaschemotherapydrugsandembolizationmaterials.---###**ActionSet**####**ChemotherapyDrugs**-Raltitrexed-Epirubicin-Oxaliplatin-Lobaplatin-Mitomycin-Idarubicin-Nedaplatin-Pirarubicin-Cisplatin-Idarubicin-THP####**EmbolizationMaterials**-Lipiodol-GelatinSponge-PVA-AbsoluteAlcohol-NBCA---###**Example****Input**{\"image\":{image_property}\"patient_id\":001}**Output**{001:{\"keywords\":\"Raltitrexed;Lobaplatin;Lipiodol;GelatinSponge\"}}Figure 11. VLM prompt template for our dataset.",
  "text_length": 74528
}