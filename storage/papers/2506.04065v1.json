{
  "id": "http://arxiv.org/abs/2506.04065v1",
  "title": "Progressive Mastery: Customized Curriculum Learning with Guided\n  Prompting for Mathematical Reasoning",
  "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.",
  "authors": [
    "Muling Wu",
    "Qi Qian",
    "Wenhao Liu",
    "Xiaohua Wang",
    "Zisu Huang",
    "Di Liang",
    "LI Miao",
    "Shihan Dou",
    "Changze Lv",
    "Zhenghua Wang",
    "Zhibo Xu",
    "Lina Chen",
    "Tianlong Li",
    "Xiaoqing Zheng",
    "Xuanjing Huang"
  ],
  "published": "2025-06-04T15:31:46Z",
  "updated": "2025-06-04T15:31:46Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04065v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04065v1  [cs.CL]  4 Jun 2025Progressive Mastery: Customized Curriculum Learning with Guided\nPrompting for Mathematical Reasoning\nMuling Wu1*, Qi Qian1*, Wenhao Liu1, Xiaohua Wang1, Zisu Huang1\nDi Liang2, LI Miao2, Shihan Dou1, Changze Lv1, Zhenghua Wang1\nZhibo Xu1, Lina Chen1, Tianlong Li1, Xiaoqing Zheng1†, Xuanjing Huang1\n{mlwu22,qqian23}@m.fudan.edu.cn\n1Fudan University2ByteDance Inc.\nAbstract\nLarge Language Models (LLMs) have achieved\nremarkable performance across various reason-\ning tasks, yet post-training is constrained by\ninefficient sample utilization and inflexible dif-\nficulty samples processing. To address these\nlimitations, we propose Customized Curricu-\nlum Learning (CCL), a novel framework with\ntwo key innovations. First, we introduce model-\nadaptive difficulty definition that customizes\ncurriculum datasets based on each model’s in-\ndividual capabilities rather than using prede-\nfined difficulty metrics. Second, we develop\n\"Guided Prompting,\" which dynamically re-\nduces sample difficulty through strategic hints,\nenabling effective utilization of challenging\nsamples that would otherwise degrade perfor-\nmance. Comprehensive experiments on su-\npervised fine-tuning and reinforcement learn-\ning demonstrate that CCL significantly outper-\nforms uniform training approaches across five\nmathematical reasoning benchmarks, confirm-\ning its effectiveness across both paradigms in\nenhancing sample utilization and model perfor-\nmance.\n1 Introduction\nLarge Language Models (LLMs) have achieved\nbreakthrough progress in the field of natural lan-\nguage processing (NLP) in recent years. With\nadditional post-training optimization, these mod-\nels have demonstrated exceptional performance on\ncomplex tasks such as code generation and math-\nematical reasoning(Guo et al., 2024; Shao et al.,\n2024; Yang et al., 2025; Kavukcuoglu, 2025; Ope-\nnAI, 2024). However, the current post-training\nprocess for LLMs still faces significant challenges.\nOne notable limitation of conventional training is\nthe uniform treatment of all examples, ignoring\ntheir varying difficulty levels or value. Conse-\nquently, challenging or high-quality samples are\n*These authors contributed equally\n†Corresponding author.not strategically introduced at optimal points in\nthe training process, thereby impeding effective\nknowledge acquisition and integration.\nTo address this limitation, Bengio et al. (2009)\nintroduced the concept of curriculum learning to\nmodel training, inspired by human education’s pro-\ngression from simple to complex concepts. Sev-\neral recent studies have also begun to explore\nthe application of curriculum learning strategies\nbased on heuristic rules to the LLM post-training\npipeline. For instance, in logical reasoning task,\nXie et al. (2025) measured example difficulty by in-\nput length to implement progressive training from\nsimpler to more complex instances. Wen et al.\n(2025b) classified difficult examples as those in-\ncorrectly predicted by DeepSeek-R1(DeepSeek-AI\net al., 2025),and prioritized these challenging cases\nduring later training stages. Although these rule-\nbased curriculum learning methods improved per-\nformance to some extent, they nonetheless present\ncertain limitations.\nFirst, these predefined difficulty metrics lack pre-\ncision in measuring actual difficulty levels. As illus-\ntrated in Figure 2, our experiments on the MATH\ndataset demonstrate that model performance does\nnot consistently decline with increasing prede-\nfined difficulty levels. Counterintuitively, models\nachieve higher accuracy on purportedly more diffi-\nculty Level 5 problems than on Level 4 problems.\nSecond, defining difficulty using a uniform stan-\ndard proves inadequate, as metrics that effectively\ngauge difficulty for one model often fail to appro-\npriately characterize challenge levels for another\nmodel. As illustrated in Figure 3, samples that\npresent significant challenges for Qwen2.5-MATH-\n7B are often solved with ease by DeepSeek-Math-\n7B-Instruct, and vice versa. To address these limi-\ntations, we propose a tailored curriculum learning\napproach that calibrates sample difficulty accord-\ning to each model’s individual capabilities, thereby\ncustomizing more appropriate training sequences\n1\n--- Page 2 ---\nfor different models.\nAnother significant challenge in model training\nstems from the presence of extremely challenging\nexamples in the training data. Previous research\n(Yu et al., 2025; Wen et al., 2025a) has demon-\nstrated that forcing models to train on examples\nsubstantially beyond their current capabilities can\nlead to performance degradation. Consequently, a\nconventional approach has been to simply exclude\nsuch overly difficult samples from the training pro-\ncess to prevent negative impacts on model learning.\nHowever, this wholesale elimination of challeng-\ning data is inherently inefficient, as these difficult\nexamples often contain valuable information that\ncould potentially enhance model performance if\nleveraged appropriately. To overcome this limi-\ntation, we introduce \"Guided Prompting,\" a tech-\nnique that augments input examples with targeted\nhints to dynamically modulate their difficulty dur-\ning the training process. This method effectively\nprevents performance deterioration while enabling\nthe model to extract meaningful patterns from ex-\namples that would otherwise be discarded, thereby\nsignificantly improving overall data utilization effi-\nciency.\nThe contributions of this study are summarized\nas follows:\n•We tailored the course dataset based on the\nmodel’s performance and proposed a novel\npost-training approach called Customized Cur-\nriculum Learning (CCL).\n•We implement \"Guided Prompting\" for sam-\nples that significantly exceed current model\ncapabilities, effectively controlling sample dif-\nficulty and substantially improving data uti-\nlization efficiency.\n•We conduct comprehensive experiments\nacross two mainstream post-training\nparadigms, namely supervised fine tuning\nand reinforcement learning, demonstrating\nsignificant performance improvements on our\nspecific model.\n2 Related Work\nCurriculum Learning. Bengio et al. (2009) intro-\nduces the concept of curriculum learning, demon-\nstrating that models learn more effectively when\ntraining examples are presented in a progressively\nharder order. Recent approaches in large language\nmodels build on this concept. Xie et al. (2025) de-\nsigns curricula by adjusting task difficulty basedon logical complexity, enhancing the model’s rea-\nsoning abilities. Wen et al. (2025b) treats queries\nthat DeepSeek-R1(DeepSeek-AI et al., 2025) strug-\ngles with as hard samples, deferring them to later\ntraining stages for focused learning. Team et al.\n(2025) refine training by filtering out noisy samples\nearly, concentrating on high-quality examples for\nlater stages. Huang et al. (2025) apply curriculum\nlearning to retrieval-augmented generation (RAG),\nordering tasks based on the number of distractors\nin retrieved passages. Shi et al. (2025) dynamically\nselects training samples whose predefined difficulty\nscores are closest to the model’s current target diffi-\nculty level, which is adjusted during training based\non reward feedback. Unlike these approaches that\nrely on heuristic rules to define a fixed difficulty\nhierarchy shared across all models, we propose a\ncustomized curriculum learning framework that tai-\nlors the training sequence to each model’s reason-\ning ability, enabling a more adaptive and effective\nlearning process.\nModel-Adaptive Difficulty Awaring. In the\nmodel training process, different types of sam-\nples should be treated with varying degrees of em-\nphasis, with difficulty-aware methods serving as a\nkey approach to distinguishing data types. Team\n(2025); Xie et al. (2024); Lee and Song (2024) em-\nploy LLM-generated scoring to assess sample diffi-\nculty, while Min et al. (2024); Yuan et al. (2025);\nWen et al. (2025b) heuristically treat long-form\nQA tasks as inherently challenging. Tong et al.\n(2024); Xue et al. (2025) take a more empirical ap-\nproach by conducting multiple sampling iterations\nfor each query, defining difficulty through incorrect\nresponse ratios and allocating more trials to chal-\nlenging queries during synthesis. Similarly, Ma\net al. (2024) implements multi-round query sam-\npling but weights samples inversely proportional to\naccuracy, thereby giving higher weights to samples\nwith lower accuracy scores.\nPost Training. Supervised fine-tuning and rein-\nforcement learning represent the two most preva-\nlent methods in post-training. Fine-tuning pre-\ntrained models on high-quality datasets with step-\nby-step solutions markedly enhances problem-\nsolving accuracy(Yue et al., 2023; Yuan et al., 2023;\nHwang et al., 2024). Beyond supervised learning,\nreinforcement-based fine-tuning has been explored\nto further align LLMs with solution correctness\nand preferred reasoning styles. Luo et al. (2023);\nLuong et al. (2024); Yue et al. (2025) optimize pol-\nicy networks using Proximal Policy Optimization\n2\n--- Page 3 ---\nA 2,2A 1,1\nA 2,1\nA n,1A n,2 A n,mA 2,mA 1,m A 1,2\nAcc: 18%Step 1: Curriculum Construction\nGenerate m \nanswers for \neach question.Acc: 43%\nQuestion 1\nQuestion 2\nQuestion n...Step 2: Difficult Sample Adaptation Step 3: Stage Training\n...... ...\ncorrect...\nStage 1\nStage 2\nStage 3Data proportion\n                              wrongQ：A circle is inscribed in a \nright triangle with legs 6 cm \nand 8 cm. Find the radius r \nof the circle.\nQ：A circle  … Find the \nradius r of the circle. \nConsider how the circle \ntouches all three sides of \nthe right triangle.Perhaps the \nanswer is \n4cm.\nI know it !…\nThe answer is \n2cm. \n…\n…\n…\nGradual improvement \nvia stage training\nUpdate Define the \ndifficulty based \non accAcc: 81%\nDon't worry.\nI give you \nsome hints.  \nDifficult Data\nDifficult Data with Hints\nFigure 1: Overall pipeline of our method. Step 1: For each question in the training set, the model generates\nmultiple responses and calculates the accuracy on that sample. Based on these accuracy scores, samples are ranked\nand organized into curriculum datasets. Step 2: Transforming difficult samples to reduce the answering difficulty\nfor the model, bringing samples within the model’s solvable range. Step 3: The model undergoes staged training\nsequentially on easy, medium, and difficult curriculum datasets, with its performance continuously enhanced.\n(PPO). In contrast, Shao et al. (2024); DeepSeek-\nAI et al. (2025); Yu et al. (2025) replace the critic\nmodel in PPO and optimize policy networks via\nGroup Relative Policy Optimization (GRPO).\n3 Method\nTraditional training approaches treat all samples\nequally, failing to adequately leverage high-quality\nsamples, which leads to suboptimal performance.\nTo address this limitation, we propose the Cus-\ntomized Curriculum Learning (CCL) training\nframework, which enables models to learn progres-\nsively from easy to difficult samples. By struc-\nturing the training process to prioritize founda-\ntional concepts before advancing to more complex\nsamples, CCL enables models to build extremely\nsolid foundations during the early stages of train-\ning, upon which more sophisticated understanding\ncan be constructed.\nThe CCL training framework consists of three\nkey steps. First is curriculum construction. Since\ncurriculum learning proceeds in stages across sam-\nples of varying difficulty, the original dataset must\nbe partitioned accordingly. We propose an aptitude-\nbased training approach that customizes curricu-\nlum for each model according to its inherent ca-\npabilities, applying the principle of individualized\ninstruction to optimize learning progression. Sec-\nond is difficult sample adaptation. Due to inher-\nent model limitations, some samples remain con-\nsistently challenging regardless of the model’s at-\ntempts. Training on such data can actually degrademodel performance. We identify these problem-\natic samples and implement a \"Guided Prompting\"\nmethod to reduce the answering difficulty, thereby\nimproving sample utilization efficiency. The fi-\nnal step is multi-stage training. Utilizing the con-\nstructed and modified curriculum datasets from\nprevious steps, we implement staged supervised\nfine-tuning and reinforcement learning, enabling\nthe model to gradually adapt to samples of increas-\ning difficulty, enhancing training stability and over-\nall performance, the full algorithm is detailed in\nAlgorithm 1.\n3.1 Curriculum Construction\nTo implement training in an easy-to-difficult se-\nquence, we first need to establish metrics for mea-\nsuring sample difficulty, which will then be used to\nsort and segment the dataset. Ding et al. (2024) di-\nrectly used the manually annotated difficulty levels\nin the MATH dataset as the standard for distin-\nguishing sample complexity. However, this heuris-\ntic definition has certain limitations. As shown in\nFigure 2, we selected Qwen2.5-Math-1.5B(Yang\net al., 2024), Qwen2.5-Math-7B, and Deepseek-\nMath-7B(Shao et al., 2024) as baseline models and\ntested them on the MATH dataset. The results\nindicate that model performance does not consis-\ntently decrease as the predefined difficulty level\nincreases (for instance, models achieve higher ac-\ncuracy on level 4 problems than on level 5 prob-\nlems), suggesting that this heuristic definition lacks\nprecision. Furthermore, Figure 3 demonstrates that\n3\n--- Page 4 ---\nLevel 1 Level 2 Level 3 Level 4 Level 5\nDifficulty Level0%20%40%60%80%100%Model AccuracyQwen2.5-Math-1.5B\nQwen2.5-Math-7B\nDeepseek-Math-7B-InstructFigure 2: Performance of multiple models on MATH\ndataset subsets with predefined difficulty levels. As\npredefined difficulty increases from Level 1 to Level\n5, model accuracy does not consistently decline but\ninstead exhibits significant fluctuations, demonstrating\nthat predefined difficulty standards may not correctly\nadapt to all models.\ndefining difficulty using a uniform standard proves\ninadequate. Samples that are extremely simple for\nQwen2.5-Math-1.5B may still challenge Qwen2.5-\nMath-7B, and vice versa, indicating that we cannot\nestablish a single unified difficulty measurement\nstandard that is suitable for all models.\nTo address these two issues, we propose a \"teach-\ning according to aptitude\" curriculum construction\napproach, which customizes curriculum datasets\nbased on each model’s own performance. Specifi-\ncally, we input all training samples into the model\nfor inference and collect the model’s responses\nthrough sampling. For each problem, the model\nprovides N responses, and the model’s accuracy\nrate for that sample is defined as following:\nACC i=Pn\nj=11{Aij=A∗\ni}\nn(1)\nwhere ACC idenotes the accuracy of i-th samples\nAijdenotes the response generated by the model\nfor the i-th sample in the j-th sampling iteration,\nandA∗\nidenotes the golden answer of i-th samples.\nWe define the difficulty level of a sample as the\ninverse of this accuracy rate and sort the samples\naccording to their difficulty levels, as shown in Fig-\nure1 Step 1. Samples that the model can correctly\nanswer multiple times are classified as simple sam-\nples and can be well mastered in the early stages of\ntraining. Samples that the model repeatedly fails\nto solve are classified as moderate or difficult sam-\nples, which are reserved for later stages of trainingafter the model has acquired the relevant founda-\ntional knowledge in the domain, allowing it to more\nthoroughly learn these more complex samples.\nDeepseek (4.7%)\nQwen-7B (12.3%)\nQwen-1.5B (5.8%)\nDeepseek  Qwen-7B (17.1%)\nDeepseek  Qwen-1.5B (3.1%)\nQwen-7B  Qwen-1.5B (11.5%)\nAll (19.6%)\nNone (25.9%)\nFigure 3: Visualization of solution correctness patterns\nfor three mathematical reasoning models: Deepseek-\nMath-7B-Instruct (blue), Qwen2.5-Math-7B (red), and\nQwen2.5-Math-1.5B (green). Each colored region rep-\nresents a specific answering scenario; for example, the\nblack region indicates questions all three models an-\nswered correctly. Approximately 55% of questions\nthat are easy for one model prove difficult for another,\ndemonstrating that using a unified standard to define\nsample difficulty across all models is unreasonable.\n3.2 Difficult Sample Adaptation\nEven after constructing curriculum data from easy\nto difficult as outlined in step 1, there remain sam-\nples that the model cannot solve correctly regard-\nless of how many attempts it makes, due to lim-\nitations in the model’s inherent capabilities. Yu\net al. (2025); Wen et al. (2025a) have demonstrated\nthat training on samples far beyond the model’s\ncurrent capabilities actually degrades performance,\nleading to the common practice of discarding such\nsamples. However, we consider direct disposal\nof this data to be wasteful and propose a \"guided\nprompting\" method to transform and reclaim these\ndifficult samples.\nSimilar to how teachers approach difficult con-\ncepts in education, we apply a pedagogical frame-\nwork to model training. When students encounter\nobstacles while learning new material, effective\nteachers don’t skip challenging concepts simply be-\ncause they exceed the student’s current understand-\ning. Instead, they employ progressive, step-by-step\nguidance to facilitate knowledge absorption. By in-\ncorporating this educational philosophy into model\ntraining, we provide the model with hints that guide\nits solution generation process(Xi et al., 2024; Dou\net al., 2025). This significantly reduces the diffi-\n4\n--- Page 5 ---\nAlgorithm 1 Customized Curriculum Learning for Enhancing Mathematical Reasoning\nRequire: training dataset D={(Qi, Ai)}with questions Qiand reference solution Ai\nRequire: pretrained model π0, accuracy threshold τ, hint ratio α\n1:Curriculum Construction:\n2:for all question Qi∈Ddo\n3: Generate nresponse {Ai1, . . . , A in}using model πθ\n4: Calculate ACC i=Pn\nj=11{Aij=A∗\ni}\nn, where A∗\niis the golden answer of Qi\n5:end for\n6:Sort all samples in descending order based on their ACC ivalues, then partition the dataset Dintop\ndisjoint subsets {D1, D2, ..., D p}, where the p-th subset Dpcontains samples with the lowest ACC i\nvalues, representing the most challenging instances.\n7:Difficult Sample Adaptive Processing:\n8:for all difficult samples (Qi, Ai)∈Dpdo\n9: Decompose the reference solution Siinto a sequence of problem-solving steps {si1, . . . , s ik}\n10: Gradually provide hints Pi={si1, . . . , s il}until either the ratio|il|\n|ik|reaches α, or the model’s\nperformance improves to the τ\n11: ifmodel’s performance improves to τthen\n12: Update question and answer: Qi→[Qi;Pi],Ai→ {si(l+1), . . . , s ik}\n13: else\n14: Discard this overly difficult sample (Qi, Ai)that the model still fails to handle effectively even\nwith hints\n15: end if\n16:end for\n17:Multi-Stage Training Process:\n18:foreach stage s∈ {1, . . . , p }do\n19: Fine-tune the previous stage’s model πs−1on current stage dataset Ds:\nπs= arg min\n(Q,A )∈DsL(πs−1)\n20:end for\n21:Output: Enhanced model πmwith improved problem-solving skills\nculty of problem-solving and helps the model better\nassimilate the current knowledge, as illustrated in\nFigure 1 Step 2.\nSpecifically, for a problem Qiwith correspond-\ning reference answer Si, we first decompose Si\ninto step-by-step solution components, such that\nSi={si1, si2, ..., s ik}. We then extract a small\nprefix Pi={si1, si2, ..., s ip}fromSito serve as a\nhint, where pis smaller then k. This prefix Piis\nconcatenated with Qias input to guide the model\ntoward generating the correct answer, that is\nyi∼πθ(Y|[Qi;Pi]) (2)\nwhere yidenotes response generated by model θ.\nThrough this approach, samples that previously\nexceeded the model’s capabilities are transformed\ninto manageable examples. The originally complex\nanswer generation task becomes a simpler answercompletion task, thereby enhancing the model’s\neffective utilization of training samples.\n3.3 Multi Stage Training\nAfter constructing the curriculum dataset and modi-\nfying the difficult samples, we can utilize this parti-\ntioned data D={D1, D2, ..., D p}for multi stage\nSFT and multi stage RL, as shown at Figure 1 Step\n3. For model πθ, dataset Djhas a higher difficulty\nlevel than dataset Di, where j>i.\n3.3.1 Multi Stage SFT\nLet the pre-trained model be denoted as π0. Based\non the number of partitions in the curriculum\ndataset, multi-stage SFT necessitates mrounds\nof maximum likelihood optimization. The loss\nfunction for the i-th round of optimization can be\n5\n--- Page 6 ---\nformally expressed as follows:\nLSFT=−E(x,y)∼Di[log(πi−1(y|x))] (3)\n3.3.2 Multi Stage RL\nBased on the constructed curriculum dataset, we\nintroduce multi-stage reinforcement learning to en-\nhance the model’s generalizability. DeepSeek-AI\net al. (2025) has demonstrated that reinforcement\nlearning can be performed directly without super-\nvised fine-tuning, known as DeepSeek-R1-Zero,\nwhich can also achieve excellent performance on\nreasoning tasks. We follow this setting to conduct\nour experiments.\nThe reward is the source of training signals\nin reinforcement learning and determines the op-\ntimization direction of the entire reinforcement\nlearning process. To effectively train reason-\ning models through reinforcement learning, fol-\nlowing the setup in (DeepSeek-AI et al., 2025),\nwe adopt a rule-based reward function that in-\ncludes two types of rewards. The first is the for-\nmat reward, which measures whether the model\noutputs according to our required format. If\nthe response contains special tokens such as\n\"<think>,</think>,<answer>,</answer>\", it is con-\nsidered that the format is correct, formally repre-\nsented as follows:\nrformat =(\n1.0format is correct\n0.0others(4)\nThe second type is the accuracy reward, which mea-\nsures whether the model’s final prediction is correct.\nWe extract the model’s prediction from the gener-\nated response according to the rules and compare\nit with the golden answer, formally represented as\nfollows:\nraccuracy =(\n1.0prediction is correct\n0.0others(5)\nThe final reward requals the sum of both, that is\nr=rformat +raccuracy .\nWe employ Group Relative Policy Optimiza-\ntion (GRPO) as our policy learning algorithm(Shao\net al., 2024). The GRPO algorithm generates mul-\ntiple candidate responses Ofor each question Q,\nwhere O={o1, o2, ..., o G}. These different re-\nsponses to the same question form a group, and\nthe reward of each response in this group is used\nto calculate the advantage Aiof that sample, asfollows:\nAi=ri−maen ({r1, r2, ..., r G})\nstd({r1, r2, ..., r G})(6)\nGRPO adopts a clipped objective, together with a\ndirectly imposed KL penalty term:\nLGRPO =E(x)∼D[1\nGGX\ni=11\n|oi||oi|X\nt=1(min(ri,t(θ)Ai,\nclip(ri,t(θ),1−ϵ,1 +ϵ)Ai)−βDKL(πθ||πref))],\n(7)\nwhere\nri,t(θ) =πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t)(8)\n4 Experiments\n4.1 Datasets\nTrain. Following the experimental setting of\n(Zeng et al., 2025), we selected the MATH\ndataset(Hendrycks et al., 2021) and extracted sam-\nples from level 3to level 5as training data, com-\nprising a total of 9,255instances. To adapt our pro-\nposed CCL framework for creating a customized\ncurriculum dataset for the model, we first needed to\ndifferentiate these samples based on their difficulty\nlevels according to the model’s performance on\nthem. After completing inference on all samples,\nwe ranked them according to the model’s accuracy\nrate. Samples with higher accuracy rates were cate-\ngorized as simple data for use in the early stages of\nmodel training, while samples with lower accuracy\nrates were designated as difficult data for later train-\ning stages. See Appendix A.1 for more detailed\ndescriptions. Additionally, the data was processed\ninto a conversational format. The prompts used\nin the SFT and GRPO processes can be found at\nAppendix C.\nTest. We use five benchmark datasets to assess\nthe model’s performance across different levels\nof difficulty and mathematical reasoning. MATH\n500(Lightman et al., 2023), is a subset of the\nMATH dataset, containing 500representative prob-\nlems designed to test a model’s general mathemat-\nical capability. OlympiadBench (He et al., 2024)\nincludes a collection of problems from Olympiad-\nlevel mathematics and physics competitions. Min-\nerva Math (Lewkowycz et al., 2022) is a curated\n6\n--- Page 7 ---\nModel MethodLearning\nStrategyMATH\n500Minerva\nMathOlympiad\nBenchAIME24 AMC23 Average\nQwen2.5-Math-1.5BSFTUniform 48.60 18.00 12 .60 0 .00 27 .50 21 .34\nCCL 48.00 23.50 12 .90 0.00 27 .50 22.38\nGRPOUniform 51.80 18 .40 21 .00 10 .00 22 .50 24 .74\nCCL 72.60 31 .60 32 .70 13 .30 42 .50 38 .54\nQwen2.5-Math-7BSFTUniform 68.80 16.50 18 .40 0 .00 22 .50 25 .24\nCCL 63.00 21.00 18 .70 3 .30 45 .00 30 .20\nGRPOUniform 74.20 33 .50 33 .90 10 .00 62.50 42.82\nCCL 76.60 38 .20 38 .20 13 .30 60.00 45.26\nTable 1: Evaluation results of different learning strategies on Math Datasets\nset of undergraduate-level math problems that as-\nsess complex mathematical reasoning and symbolic\nmanipulation. AMC 23 and AIME 24 include prob-\nlems from the 2023 American Mathematics Compe-\ntitions and the 2024 American Invitational Mathe-\nmatics Examination, respectively. Additionally, the\ndata was processed into a conversational format.\n4.2 Models\nTo effectively validate the efficacy of our CCL\nmethod across foundation models of varying ca-\npabilities, we selected two different-sized models\nfor our experiments: Qwen2.5-MATH-1.5B(Yang\net al., 2024) and Qwen2.5-MATH-7B.\n4.3 Training Setup\nWe conducted our experiments using 8NVIDIA\nA100 GPUs for the SFT experiments within the\nLlama-Factory framework(Zheng et al., 2024) and\nfor the GRPO experiments within Hugging Face’s\nOpen R1 framework(Face, 2025). See Appendix\nA.2 for more detailed descriptions.\n4.4 Evaluation Setup\nWe evaluated our models using the evaluation script\nfrom (Zeng et al., 2025). See Appendix B for more\ndetailed descriptions.\n4.5 Main Results\nWe implemented various strategies and training\nmethods across multiple models of different scales\nand conducted extensive experiments on test sets\nof varying difficulty levels. As shown in Table 1,\ncompared to uniform training that treats all data\nequally, our CCL strategy demonstrates significant\nadvantages by customizing curriculum datasets andadopting an easy-to-hard training approach, yield-\ning substantial performance improvements across\nmultiple experimental settings.\nOur CCL learning strategy demonstrates both\nmethod compatibility and model scalability. When\napplied to different-sized models under SFT set-\ntings, CCL improved performance by 1.04% and\n4.96% for Qwen2.5-Math-1.5B and Qwen2.5-\nMath-7B respectively. Under GRPO settings, the\nimprovements were 13.80% and2.44% for the\nsame models. Notably, our CCL training strategy\nyielded consistent performance gains across all test\nsubsets, demonstrating its significant enhancement\nof model generalization capabilities. Appendix D\nalso presents the overall performance changes of\nthe CCL method on the test set during the multi-\nstage training process.\n4.6 Ablation Study\nTo evaluate the effectiveness of our method’s com-\nponents, we perform ablation studies on three key\naspects: sample difficulty definition, difficult sam-\nple processing, and data mixing strategies. These\nstudies isolate each design choice’s contribution to\nmodel performance and training stability.\n4.6.1 Sample Difficulty Definition\nPrevious researches have relied on predefined dif-\nficulty labels from the MATH dataset to construct\ncurriculum learning data. However, we propose\nthat a more effective approach is to customize dif-\nficulty labels based on each model’s actual perfor-\nmance on samples before constructing curriculum\ndatasets. We conducted comparative experiments\nusing both approaches to partition training data.\nAs shown in Figure 4, our customized difficulty\ndefinition yields superior performance across mod-\n7\n--- Page 8 ---\nSFT (1.5B) GRPO (1.5B) SFT (7B) GRPO (7B)\nMethod and Model Size0%10%20%30%40%Average Accuracy21.2627.92\n26.4241.06\n20.9832.72\n25.3042.62\n22.3838.54\n30.2045.26 w/o Customized Difficulty\nw/o Curriculum Review\nOursFigure 4: Comparison of model performance after train-\ning with data partitioned using different sample diffi-\nculty definition methods and across various data mixing\nstrategies.\nels of varying sizes and different post-training meth-\nods, demonstrating consistent advantages over the\npredefined difficulty categorization. This empiri-\ncally validates that difficulty labels tailored to spe-\ncific model capabilities lead to more effective cur-\nriculum learning than predefined difficulty metrics.\n4.6.2 Difficult Sample Processing\nAt this part, we conduct an ablation study to test\nwhether continuous training on excessively diffi-\ncult samples degrades model performance. We\ncompare three difficult sample processing meth-\nods using GRPO: (1) retaining difficult samples,\n(2) discarding difficult samples, and (3) applying\nour \"Guided Prompting\" approach to adapt difficult\nsamples with strategic hints. Results in Table 2 con-\nfirm that learning from overly challenging samples\nindeed harms model performance. Discarding diffi-\ncult samples outperforms retaining them, validat-\ning this hypothesis. This effect is more severe for\nweaker models like Qwen2.5-Math-1.5B, which en-\ncounter more unsolvable samples and suffer greater\nperformance degradation.\nRather than waste valuable data through direct\nremoval, our \"Guided Prompting\" method repur-\nposes difficult samples by providing hints that bring\nproblems within the model’s solvable range. Exper-\nimental results demonstrate this approach success-\nfully recovers difficult samples while substantially\nimproving model performance, establishing it as\nthe optimal strategy for enhancing mathematical\nreasoning capabilities.\n4.6.3 Data Mixing Strategy\nJust as students need to periodically review pre-\nviously mastered knowledge during their learning\nprocess, we believe that models undergoing staged\ncurriculum learning require similar reinforcementModel Size Processing Strategy Avg.\n1.5BRetaining Difficult Samples 26.36\nDiscarding Difficult Samples 37.46\nAdapting Difficult Samples 38.54\n7BRetaining Difficult Samples 41.34\nDiscarding Difficult Samples 44.86\nAdapting Difficult Samples 45.26\nTable 2: Comparison of model performance across three\ndifficult sample processing methods\nof previously acquired content. Therefore, we de-\nsign two distinct data mixing strategies for compar-\native analysis. The first approach, termed \"Naive\nCurriculum,\" provides models with samples cor-\nresponding only to the current difficulty level at\neach training stage. The second approach, called\n\"Curriculum Review,\" incorporates a small propor-\ntion of easier samples during later training stages,\nallowing the model to revisit previously learned\nmaterial.\nExperimental results in Figure 4 demonstrate\nthat Curriculum Review data mixing strategy\nachieves superior performance, confirming that pro-\nviding models with previously learned content dur-\ning later training stages is crucial for preventing\ncatastrophic forgetting. This finding underscores\nthe importance of maintaining access to founda-\ntional knowledge throughout the curriculum learn-\ning process.\n5 Conclusions\nIn conclusion, we presented Customized Curricu-\nlum Learning (CCL), a novel post-training frame-\nwork that systematically constructs model-adaptive\ncurriculum sequences and transforms difficult sam-\nples through guided prompting to enhance large\nlanguage models’ mathematical reasoning capabili-\nties. Our comprehensive experiments demonstrated\nthat models trained with CCL significantly out-\nperform those using uniform training approaches\nacross multiple mathematical reasoning bench-\nmarks, with consistent improvements observed\nin both supervised fine-tuning and reinforcement\nlearning paradigms. By effectively integrating cur-\nriculum learning into large language model train-\ning through model-specific difficulty customization\nand guided prompting, our work substantially im-\nproves sample utilization and model performance,\n8\n--- Page 9 ---\nadvancing more effective training methodologies\nfor large-scale language models.\nLimitations\nDespite the promising results of our work, sev-\neral limitations warrant acknowledgment. While\nour study focuses on mathematical reasoning, we\nsee great potential in extending the CCL frame-\nwork to other domains such as logical reasoning,\ncode generation, and natural language inference,\nallowing us to further investigate its generaliz-\nability across diverse task types. Furthermore,\nalthough our current study applies CCL within\nspecific post-training paradigms, such as super-\nvised fine-tuning and GRPO, we recognize that\ncombining CCL with other post-training strate-\ngies—like PPO and broader reinforcement learning\ntechniques—remains an open direction. Exploring\nthese combinations may further reveal the full po-\ntential of the CCL framework in enhancing model\nlearning dynamics.\nReferences\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nInternational Conference on Machine Learning .\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang\nZhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, and 179 oth-\ners. 2025. Deepseek-r1: Incentivizing reasoning ca-\npability in llms via reinforcement learning. ArXiv ,\nabs/2501.12948.\nYiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao\nZhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang,\nand Xuanjing Huang. 2024. Mitigating tail narrow-\ning in llm self-improvement via socratic-guided sam-\npling. ArXiv , abs/2411.00750.\nShihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao\nGui, Qi Zhang, and Xuanjing Huang. 2025. Improv-\ning rl exploration for llm reasoning through retro-\nspective replay.\nHugging Face. 2025. Open r1: A fully open reproduc-\ntion of deepseek-r1.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nYu Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wen-\nfeng Liang. 2024. Deepseek-coder: When the large\nlanguage model meets programming - the rise of code\nintelligence. ArXiv , abs/2401.14196.Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu,\nZhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yu-\njie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan\nLiu, and Maosong Sun. 2024. Olympiadbench:\nA challenging benchmark for promoting agi with\nolympiad-level bilingual multimodal scientific prob-\nlems. In Annual Meeting of the Association for Com-\nputational Linguistics .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Xiaodong\nSong, and Jacob Steinhardt. 2021. Measuring math-\nematical problem solving with the math dataset.\nArXiv , abs/2103.03874.\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\nNiu, J. Hockenmaier, and Tong Zhang. 2025. Rag-rl:\nAdvancing retrieval-augmented generation via rl and\ncurriculum learning. ArXiv , abs/2503.12759.\nHyeonbin Hwang, Doyoung Kim, Seungone Kim,\nSeonghyeon Ye, and Minjoon Seo. 2024. Self-\nexplore: Enhancing mathematical reasoning in lan-\nguage models with fine-grained rewards. In Con-\nference on Empirical Methods in Natural Language\nProcessing .\nKoray Kavukcuoglu. 2025. Gemini 2.5:\nOur most intelligent ai model. https:\n//blog.google/technology/google-deepmind/\ngemini-model-thinking-updates-march-2025/ .\nAccessed: 2025-05-17.\nJung X. Lee and Yeong-Tae Song. 2024. College exam\ngrader using llm ai models. In 2024 IEEE/ACIS\n27th International Conference on Software Engineer-\ning, Artificial Intelligence, Networking and Paral-\nlel/Distributed Computing (SNPD) , pages 282–289.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Venkatesh\nRamasesh, Ambrose Slone, Cem Anil, Imanol\nSchlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. 2022.\nSolving quantitative reasoning problems with lan-\nguage models. ArXiv , abs/2206.14858.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl\nCobbe. 2023. Let’s verify step by step. ArXiv ,\nabs/2305.20050.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nGuang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\nArXiv , abs/2308.09583.\nTrung Quoc Luong, Xinbo Zhang, Zhanming Jie,\nPeng Sun, Xiaoran Jin, and Hang Li. 2024. Reft:\nReasoning with reinforced fine-tuning. ArXiv ,\nabs/2401.08967.\n9\n--- Page 10 ---\nJingyuan Ma, Rui Li, Zheng Li, Lei Sha, and Zhifang\nSui. 2024. Plug-and-play training framework for\npreference optimization. ArXiv , abs/2412.20996.\nYingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen,\nJia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang,\nXiaoxue Cheng, Huatong Song, Wayne Xin Zhao,\nZheng Liu, Zhongyuan Wang, and Jiahui Wen. 2024.\nImitate, explore, and self-improve: A reproduction\nreport on slow-thinking reasoning systems. ArXiv ,\nabs/2412.09413.\nOpenAI. 2024. Openai o1 system card. https://\narxiv.org/abs/2412.16720 . Accessed: 2025-05-\n17.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-\nMei Song, Mingchuan Zhang, Y . K. Li, Yu Wu, and\nDaya Guo. 2024. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models.\nArXiv , abs/2402.03300.\nTaiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and\nJieyu Zhao. 2025. Efficient reinforcement finetuning\nvia adaptive curriculum learning.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing,\nChangjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, Chuning\nTang, Congcong Wang, Dehao Zhang, Enming Yuan,\nEnzhe Lu, Feng Tang, Flood Sung, Guangda Wei,\nGuokun Lai, and 75 others. 2025. Kimi k1.5:\nScaling reinforcement learning with llms. ArXiv ,\nabs/2501.12599.\nNovaSky Team. 2025. Sky-t1: Train your own\no1 preview model within $450. https://novasky-\nai.github.io/posts/sky-t1. Accessed: 2025-01-09.\nYuxuan Tong, Xiwen Zhang, Rui Wang, Rui Min Wu,\nand Junxian He. 2024. Dart-math: Difficulty-aware\nrejection tuning for mathematical problem-solving.\nArXiv , abs/2407.13690.\nCheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou,\nand Xiangang Li. 2025a. Sari: Structured audio rea-\nsoning via curriculum-guided reinforcement learning.\nArXiv , abs/2504.15900.\nLiang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An,\nZhenyu Duan, Yimin Du, Junchen Liu, Lifu\nTang, Xiaowei Lv, Haosheng Zou, Yongchao Deng,\nShousheng Jia, and Xiangzheng Zhang. 2025b.\nLight-r1: Curriculum sft, dpo and rl for long cot\nfrom scratch and beyond. ArXiv , abs/2503.10460.\nZhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie\nJin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu,\nXin Guo, Junzhe Wang, Honglin Guo, Wei Shen,\nXiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang,\nXinbo Zhang, Peng Sun, Tao Gui, and 2 others. 2024.\nTraining large language models for reasoning through\nreverse curriculum reinforcement learning. ArXiv ,\nabs/2402.05808.Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo,\nYuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhi-\nrong Wu, and Chong Luo. 2025. Logic-rl: Un-\nleashing llm reasoning with rule-based reinforcement\nlearning. ArXiv , abs/2502.14768.\nWenjing Xie, Juxin Niu, Chun Jason Xue, and Nan\nGuan. 2024. Grade like a human: Rethinking auto-\nmated assessment with large language models. ArXiv ,\nabs/2405.19694.\nBoyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng\nWang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng\nShang, Qun Liu, and Kam-Fai Wong. 2025. Dast:\nDifficulty-aware self-training on large language mod-\nels.ArXiv , abs/2503.09029.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, and 41 others.\n2025. Qwen3 technical report.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,\nBowen Yu, Chengpeng Li, Dayiheng Liu, Jian-\nhong Tu, Jingren Zhou, Junyang Lin, Keming Lu,\nMingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. 2024. Qwen2.5-math tech-\nnical report: Toward mathematical expert model via\nself-improvement. ArXiv , abs/2409.12122.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,\nXiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole\nMa, Guangming Sheng, Yuxuan Tong, Chi Zhang,\nMofan Zhang, Wang Zhang, Hang Zhu, and 16 oth-\ners. 2025. Dapo: An open-source llm reinforcement\nlearning system at scale. ArXiv , abs/2503.14476.\nSiyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye,\nZhengyin Du, and Jiecao Chen. 2025. Agent-r: Train-\ning language model agents to reflect via iterative self-\ntraining. ArXiv , abs/2501.11425.\nZheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong,\nChuanqi Tan, and Chang Zhou. 2023. Scaling re-\nlationship on learning mathematical reasoning with\nlarge language models. ArXiv , abs/2308.01825.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao\nHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023.\nMammoth: Building math generalist models through\nhybrid instruction tuning. ArXiv , abs/2309.05653.\nYu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei\nZhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang,\nTiantian Fan, Zhengyin Du, Xiang Wei, Xiangyu Yu,\nGaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin,\nZhiqi Lin, Bole Ma, Chi Zhang, and 8 others. 2025.\nVapo: Efficient and reliable reinforcement learning\nfor advanced reasoning tasks.\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Ke-\nqing He, Zejun Ma, and Junxian He. 2025. Simplerl-\nzoo: Investigating and taming zero reinforcement\n10\n--- Page 11 ---\nlearning for open base models in the wild. Preprint ,\narXiv:2503.18892.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan\nYe, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.\n2024. Llamafactory: Unified efficient fine-tuning\nof 100+ language models. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 3: System Demonstra-\ntions) , Bangkok, Thailand. Association for Computa-\ntional Linguistics.\n11\n--- Page 12 ---\nA Training Details\nIn this chapter, we will provide a detailed descrip-\ntion of the construction of curriculum datasets in\nthe multi-stage training process, while also pre-\nsenting a comprehensive overview of the hyperpa-\nrameters utilized during both training and testing\nprocedures.\nA.1 Constructing Dataset\nIn the process of constructing the curriculum\ndataset, we need to feed all training set samples\ninto the pre-trained model for inference and evalu-\nate the model’s accuracy on each sample. To ensure\nthat the evaluation results are as reliable as possible\nwhile not causing excessive computational over-\nhead, for each question in the dataset, we use the\nVLLM framework to generate 16responses from\nthe model, extract predictions from these responses\nusing appropriate scripts, and compare them with\ngolden answers to determine the correctness of the\ngenerations. To fully harness the model’s potential,\nwe did not adopt a greedy decoding strategy to gen-\nerate responses, but instead set the temperature to\n0.7, generating responses through sampling.\nAfter calculating the model’s accuracy on the\nsamples through the above steps, we sort the sam-\nples and divide them into 3equal parts according\nto quantity. The top 1/3with the highest accuracy\nare classified as simple samples, used for the first\nstage of model training. The bottom 1/3with the\nlowest accuracy are classified as difficult samples,\nused for the final stage of model training.\nIn addition, for particularly challenging samples,\nwe employed a \"Guided Prompting\" approach to\nreduce the difficulty for the model. Specifically, we\nfirst collected reference answers for these difficult\nsamples, then segmented these reference answers\ninto step-by-step reasoning processes, as illustrated\nin Figure 5. Finally, we selected a small portion of\nthe prefix combined with the original question as\ninput to assist the model in solving problems more\neffectively.\nA.2 Training HyperParameters\nSFT. We conducted our experiments using bf16 pre-\ncision under the DeepSpeed framework with zero-2\nconfiguration. We set per_device_train_batch_size\nto1and gradient_accumulation_steps to 4, employ-\ning a cosine lr_scheduler with warmup set to 0.1\nand max length set to 2048 . For the Qwen2.5-Math-\n1.5B model, we used a learning rate of 5e−6andtrained for 3epochs. For the Qwen2.5-Math-7B\nmodel, we used a learning rate of 1e−5and trained\nfor3epochs.\nGRPO. We conducted our experiments using\nbf16 precision under the DeepSpeed frame-\nwork with zero-2 configuration. We set\nper_device_train_batch_size to 16and gradi-\nent_accumulation_steps to 8, employing a cosine\nlr_scheduler with warmup set to 0.1and beta to\n0.04, num_generations to 7, max_prompt_lengtht\nto512and max_completion_length 1024 . For the\nQwen2.5-Math-1.5B model, we used a learning\nrate of 3e−6and trained for 6epochs. For the\nQwen2.5-Math-7B model, we used a learning rate\nof3e−6and trained for 4epochs.\nB Evaluation Details\nDuring the testing process, to ensure the stability\nof test results, all methods employed a greedy de-\ncoding strategy with top_p set to 0.95, and used\n\"</answer>\" as a stop word to truncate the gener-\nated content.\nC Prompt Details\nDuring both training and testing processes, the data\nwas processed into a conversational format. Fig-\nure 7 and Figure 6 demonstrate the prompts we\nused during the SFT and GRPO processes respec-\ntively. After training the models using their re-\nspective methods, we employed the corresponding\nprompts during testing as well. Additionally, dur-\ning the GRPO training process, besides adding the\nUser’s description, we also appended part of the\nAssistant’s content prefixed with the special token\n\"<think>\". This approach helps the model quickly\nlearn format compliance during the reinforcement\nlearning process, greatly enhancing the stability of\nthe model’s reinforcement learning.\nD Result Details\nIn this section, we demonstrate the overall perfor-\nmance changes on the test set when applying CCL\nto Qwen2.5-Math-1.5B and Qwen2.5-Math-7B us-\ning supervised fine-tuning and reinforcement learn-\ning methods for multi-stage training. As shown in\nFigure 8, our CCL method continuously improves\nin performance as training iterations progress.\n12\n--- Page 13 ---\nProblem\nLet n be the smallest positive integer that satisfies the following conditions:\nn divided by 2 is a perfect square\nn divided by 3 is a perfect cube\nn divided by 5 is a perfect fifth power\nHow many divisors of n are NOT multiples of 10?\nSolution\nThe first condition implies that the power of each prime factor of $n$ must be an even power \n(excluding $2$, which must be an odd power). The second condition implies that the power of each \nprime factor of $n$ must be divisible by $3$ (excluding $3$, which must leave a residue of $1$ \nupon division by $3$). The third condition implies that the power of each prime factor of $n$ must \nbe divisible by $5$ (excluding $5$, which must leave a residue of $1$ upon division by \n$5$).\\nClearly, to minimize $n$, we want to just use the prime factors $2,3,5$. The power of $2$ \nmust be divisible by $3,5$, and $2^{15}$ works. Similarly, the powers of $3$ and $5$ must be $10$ \nand $6$, respectively, both of which leave a residue of $1$ upon division. Thus, we need the \nnumber of factors of $2^{15} \\\\cdot 3^{10} \\\\cdot 5^{6}$ which are not multiples of \n$10$.\\nApplying the complement principle, there are a total of $(15+1)(10+1)(6+1) = 1232$ \nfactors. We can draw a bijection between the number of divisors of $2^{15} \\\\cdot 3^{10} \\\\cdot \n5^{6}$ that are divisible by $10$ and the number of divisors of $2^{14} \\\\cdot 3^{10} \\\\cdot 5^{5}$ \n(as each of these divisors, when multiplied by 10, will provide a factor of the original number that is \ndivisible by 10). There are $(14+1)(10+1)(5+1) = 990$. The answer is $1232-990 = \\\\boxed{242}$.\nStep 1\nThe first condition implies that the power of each prime factor of $n$ must be an even power \n(excluding $2$, which must be an odd power).\nStep 2\nThe second condition implies that the power of each prime factor of $n$ must be divisible by $3$ \n(excluding $3$, which must leave a residue of $1$ upon division by $3$).\nStep 3\nThe third condition implies that the power of each prime factor of $n$ must be divisible by $5$ \n(excluding $5$, which must leave a residue of $1$ upon division by $5$).\nStep 4\nClearly, to minimize $n$, we want to just use the prime factors $2,3,5$.\nStep 5\nThe power of $2$ must be divisible by $3,5$, and $2^{15}$ works.\nStep 6\nSimilarly, the powers of $3$ and $5$ must be $10$ and $6$, respectively, both of which leave a \nresidue of $1$ upon division.\nStep 7\nThus, we need the number of factors of $2^{15} \\cdot 3^{10} \\cdot 5^{6}$ which are not multiples \nof $10$.\nStep 8\nApplying the complement principle, there are a total of $(15+1)(10+1)(6+1) = 1232$ factors.\nStep 9\nWe can draw a bijection between the number of divisors of $2^{15} \\cdot 3^{10} \\cdot 5^{6}$ that \nare divisible by $10$ and the number of divisors of $2^{14} \\cdot 3^{10} \\cdot 5^{5}$ (as each of \nthese divisors, when multiplied by 10, will provide a factor of the original number that is divisible \nby 10).\nStep 10\nThere are $(14+1)(10+1)(5+1) = 990$.\nStep 11\nThe answer is $1232-990 = \\boxed{242}$.Case StudyFigure 5: Decomposition of reference answers into step-by-step solution.\n13\n--- Page 14 ---\nGRPO PromptYou are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think> reasoning process here </think> <answer>answer here </answer>User:{Problem}Assistant:Let me solve this step by step.<think>GRPO PromptFigure 6: Prompt Used in GRPO.\nFigure 7: Prompt Used in SFT.\nStage 1 Stage 2 Stage 319.520.020.521.021.522.022.5Accuracy\nQwen2.5-Math-1.5B - SFT\nStage 1 Stage 2 Stage 32627282930Accuracy\nQwen2.5-Math-7B - SFT\nStage 1 Stage 2 Stage 33435363738Accuracy\nQwen2.5-Math-1.5B - GRPO\nStage 1 Stage 2 Stage 343.544.044.545.0Accuracy\nQwen2.5-Math-7B - GRPO\nFigure 8: Performance Across Training Stages Using CCL.\n14",
  "text_length": 50060
}