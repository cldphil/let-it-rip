{
  "id": "http://arxiv.org/abs/2506.01151v1",
  "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
  "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
  "authors": [
    "Xintong Sun",
    "Chi Wei",
    "Minghao Tian",
    "Shiwen Ni"
  ],
  "published": "2025-06-01T20:05:30Z",
  "updated": "2025-06-01T20:05:30Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01151v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01151v1  [cs.LG]  1 Jun 2025Earley-Driven Dynamic Pruning for Efficient Structured Decoding\nXintong Sun* 1Chi Wei* 2Minghao Tian1Shiwen Ni2\nAbstract\nLarge Language Models (LLMs) have shown\nremarkable capabilities, yet ensuring their out-\nputs conform to strict structural or grammatical\nconstraints remains challenging, which is crit-\nical in function calls and domain-specific lan-\nguage (DSL) generation. Constrained decoding\nwith context-free grammar is a flexible approach\nto guarantee LLMs’ adherence to a specific for-\nmat by dynamically building a token logits mask.\nHowever, creating this mask requires checking\nthe validity of all tokens in the LLM vocabu-\nlary at every decoding step, which often incurs\nsignificant overheads in existing constrained de-\ncoding engines. To address this challenge, we\npropose ZapFormat, a novel dynamic pruning\nstrategy based on the Earley algorithm that iden-\ntifies and eliminates invalid or redundant Earley\nstates in real-time, significantly reducing mem-\nory occupation of the Earley algorithm’s states.\nThis further enables us to use a state cache to\nspeed up structured generations on a large num-\nber of queries. We implemented ZapFormat in a\nnew constrained decoding engine called Forma-\ntron which also incorporates existing optimiza-\ntions. Through comprehensive experiments on\nstructured generation tasks, including JSON gen-\neration, JSON Schema validation, and semantic\nparsing, we demonstrate that Formatron not only\nconsistently maintains high-precision compliant\noutputs but also achieves significant improve-\nments in inference speed up to 2x compared to\nstate-of-the-art implementations . More impor-\ntantly, Formatron is generally applicable across\nvarious LLM architectures. We release Formatron\nas open source at https://github.com/Dan-wanna-\nM/formatron.\n*Equal contribution1Department of Computer Science, Rice\nUniversity, Texas, the United States2Shenzhen Institutes of Ad-\nvanced Technology, Chinese Academy of Sciences, Shenzhen,\nChina. Correspondence to: Shiwen Ni <sw.ni@siat.ac.cn >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).1. Introduction\nIn recent years, Large Language Models (LLMs) have\ndemonstrated remarkable progress, achieving breakthrough\nadvances across multiple frontier domains including natural\nlanguage processing (OpenAI & Sandhini Agarwal, 2024),\ncode generation(Chen et al., 2021; Wang et al., 2021), and\nrobotic control (Liu et al., 2023; OpenAI, 2024). As their ap-\nplications continue to expand, a critical research challenge\nhas emerged: how to guide models to generate text that\nprecisely adheres to required grammatical and formatting\nspecifications. In numerous practical applications, the struc-\ntural conformity of output text is paramount (Beurer-Kellner\net al., 2023; Lundberg et al., 2023). For instance, in func-\ntion calling or external tool interactions, systems typically\nrequire generated text to comply with specific parseable for-\nmats (e.g., JSON) , imposing heightened demands on LLMs\n(Shin et al., 2021; Roy et al., 2024; Fang et al., 2023).\nConstrained decoding (Deutsch et al., 2019; Kuchnik et al.,\n2023) is one of the prevalent technical approaches to tackle\nthis challenge, which filters out tokens that violate grammat-\nical requirements by screening the entire vocabulary at each\ndecoding step, thereby ensuring model outputs conform\nto specified formal grammars. Among various constraint\nforms, context-free grammar (CFG) (Chomsky, 1956) is\nregarded as a flexible and universal constraint format due to\nits robust descriptive capabilities across multiple languages\nand structures. However, when applying these methods to\nlarge language models, several common challenges emerge:\n1.Computational Overhead from Large Vocabularies\nand Complex Grammars . To ensure each newly decoded\ntoken correctly follows CFG rules, it is typically necessary\nto continuously assess the compatibility of all candidate\ntokens in the vocabulary and maintain the grammar parsing\nstack or state in real-time. For scenarios involving large vo-\ncabularies, long sequences, or complex grammars, this can\nlead to increases in computational and memory consumption\n(Geng et al., 2023; Beurer-Kellner et al., 2024).\n2.State Redundancy and Maintenance Complexity . De-\ncoding requires storing and updating multiple parsing states,\nincluding information that may branch or backtrack. When\nstate quantities rapidly accumulate, the timely elimination of\nrepetitive, invalid, or unused states becomes crucial (Willard\n& Louf, 2023). If not handled properly, these redundant\n1\n--- Page 2 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nstates can trigger cache misses, increase memory usage,\nand ultimately result in decreased inference speeds (Opedal\net al., 2023).\nWe propose ZapFormat , a novel dynamic pruning strat-\negy based on the Earley algorithm, which serves as the\ncore component of our new constrained decoding engine\nFormatron . This approach identifies and eliminates invalid\nor redundant Earley states in real-time, significantly reduc-\ning memory occupation. Formatron integrates ZapFormat\nwith state-of-the-art optimizations to achieve both grammat-\nical compliance and computational efficiency.\nThe key insight behind our approach is that during the pars-\ning process, many intermediate parsing states become “ob-\nsolete” or “dead” – meaning they no longer contribute to\nfinding valid parsing paths but continue to consume mem-\nory resources. Traditional parsing algorithms, including\nthe widely-used Earley algorithm, tend to accumulate these\nunnecessary states throughout the decoding process, leading\nto memory bloat and reduced performance.\nOur core mechanism continuously tracks the utility of pars-\ning states during decoding, allowing Formatron to swiftly\nidentify and discard these dead states while maintaining only\nthe states useful for parsing. For instance, consider parsing\nthe input “aa” with the grammar (A→AB|B;B→a).\nIn the baseline Earley parser, Earley Set 1 (after scanning\nthe first ‘a’) retains four states: the completed (B→a•; 0),\nactive items (A→ •AB; 0)and(A→B•; 0), and the\npredicted (B→ •a; 1). However, the completed state\n(B→a•; 0)becomes obsolete once subsequent parsing\nstates (e.g., Earley Set 2’s (A→AB•; 0)) no longer ref-\nerence its start position. Formatron ’s dynamic pruning\nremoves such dead states immediately, reducing memory\nfootprint and lowering overall state counts. This approach\nensures that only relevant states propagate, streamlining the\nparsing process without compromising grammatical adher-\nence.\nExperimental evaluations on structured generation tasks\ndemonstrate Formatron’s efficacy. Experimental results\ndemonstrate that while meeting strict format output require-\nments, this method effectively reduces memory usage and\naccelerates inference, providing an efficient and universal\ntechnical pathway for structured generation in large lan-\nguage models. Our contributions are as follows:\n•We propose a dynamic pruning method based on the\nEarley algorithm, which significantly reduces memory\nusage during inference and improves computational\nefficiency through real-time cleanup of invalid states.\n•We design and implement a new constrained decod-\ning engine, Formatron, integrating dynamic pruning\nwith existing optimization techniques. Across multi-\nple structured generation tasks (such as JSON, JSONSchema, and semantic parsing), it maintains high-\nprecision output while achieving up to 2x inference\nspeed improvements.\n•We demonstrate the universality of the Formatron en-\ngine, showcasing its broad applicability across various\nlanguage model architectures, efficiently supporting\nstructured generation tasks in both large-scale language\nmodels and task-specific applications.\n2. Related Work\nConstrained Decoding is a method that ensures text gen-\nerated by Large Language Models (LLMs) conforms to\nspecific formats or task requirements. Within the archi-\ntecture of LLMs, tokens serve as atomic processing units\nthat mediate between raw text input and generated output.\nWhile each token represents a fixed-length character se-\nquence, this representation often fails to preserve linguistic\nintegrity—either by splitting semantically coherent units,\nsyntactically meaningful structures, or fragmenting multi-\nbyte Unicode characters (Wang et al., 2020). For instance, as\ndiscussed in (Beurer-Kellner et al., 2024), a standard JSON\nstring like ”I am Van” might be tokenized into <” I am >\nand<Van” >, which disrupts both semantic and syntactic\ncoherence. These tokenization limitations pose significant\nchallenges for structured text generation tasks, especially\nin applications requiring strict syntactic compliance or fine-\ngrained semantic control, such as output requirements for a\nvalid JSON format. Prior work has addressed similar chal-\nlenges in semantic parsing (Xiao et al., 2016) and syntactic\ncode generation (Yin & Neubig, 2017), demonstrating the\nimportance of incorporating structural constraints during\ntext generation.\nSeveral works (Beurer-Kellner et al., 2023; Lundberg et al.,\n2023; Willard & Louf, 2023) use regular expressions for\nconstrained decoding, but the approach’s expressiveness\nis also limited to regular expressions, which excludes all\nformats that can only be described by CFG. These formats\ninclude json grammar, json schema, almost all programming\nlanguages, and more. To support full CFG, several studies\n(Scholak et al., 2021; Poesia et al.; Geng et al.) utilise a\nparser and scanner running in parallel with the LLM, and\nthen calculate online which tokens are valid continuations\nat each step. However, these approaches incur a relatively\nhigh inference overhead, and in the worst case, they must\nexamine a nontrivial subset of vocabulary at each step.\nRecently researchers have worked on achieving highly effec-\ntive and efficient constrained decoding. Beurer-Kellner et al.\n(2023) use precomputation, speculative decoding and oppor-\ntunistic masks to achieve minimally invasive and efficient\nconstrained decoding. Koo et al. proposed an approach\nbased on automata theory to provide an efficient closed\n2\n--- Page 3 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nform solution for regular languages. Recent work XGram-\nmar (Dong et al., 2024) significantly accelerates constrained\ndecoding by classifying the tokens in the vocabulary into\ncontext-independent and context-dependent tokens, which\nenables effective usage of precomputable adaptive context-\nindependent token masks. To further accelerate constrained\ndecoding, our work investigates a dynamic pruning strategy\nfor the Earley algorithm that is able to identify and elim-\ninate invalid or redundant Earley states in real-time, thus\nsignificantly reducing the memory footprint of the Earley\nalgorithm states. This further enables us to leverage Earley\nstate caching to accelerate structured generation.\n3. Preliminaries\n3.1. Context-Free Grammar\nIn computer science, Context-Free Grammar (CFG) repre-\nsents a crucial grammar type commonly used to describe\nprogramming language syntax. To understand CFGs in-\ntuitively, think of them as a set of rules that define how\nto construct valid sentences or structures, similar to how\ngrammatical rules in natural language define valid sentence\nconstructions.\nA CFG consists of a set of rules (or productions), where\neach rule has a non-terminal symbol on the left side and\na sequence of terminal symbols and non-terminal symbols\non the right side. Here, non-terminal symbols represent\nabstract structural components (like ”noun phrase” in lin-\nguistics), while terminal symbols are the actual characters\nor words that appear in the final text. Each rule takes the\nform:\nA→α\nWhere Ais a non-terminal symbol and αis a sequence com-\nposed of terminal and non-terminal symbols. For example,\na simple addition grammar might have the following rule:\nExpression →Expression +Term |Term\nThis grammar rule indicates that an expression can be com-\nposed of either another expression plus a term, or simply a\nterm.\nIn practical applications, CFGs serve as the backbone for\nparser design in compiler construction and natural language\nprocessing. Their formal nature provides a clear framework\nfor analyzing and processing structured input, whether in\nprogramming languages or natural language text. For con-\nstrained text generation with large language models, CFGs\nact as blueprints that specify exactly what constitutes valid\noutput, for instance, to ensure that generated JSON follows\nproper syntax or that code snippets comply with program-\nming language rules.\n?0: ?→?+?\n?0: ?→∙?+?, (0)\n?0: ?→∙?, (0) ?0: ?→∙?, (0)\n?1: ?→?∙, (0)\n?1: ?→?∙+?, (0)Scan “a” Scan “a”\nReject\nScan “+”\n?2: ?→?+∙?, (0)\n?2: ?→∙?, (2) ?2: ?→∙?, (2)\nScan “d” Scan “d”\n?3: ?→?∙, (2) Reject\n?3: ?→?+?∙, (0)Figure 1. Early parse. This diagram provides a detailed illustration\nof the Earley parsing process for the input string ’a + d’ based on\nthe grammar S→A+B, A→a|c, B→b|d. In the diagram,\nred arrows indicate Predict operations; yellow arrows represent\nScan operations; and green arrows denote Complete operations.\n3.2. Earley’s Algorithm\nThe Earley algorithm represents a dynamic programming\napproach to parsing context-free grammars, notable for its\nlinear time and space complexity for all LR(k) grammars\n(Leo, 1991), and its polynomial-time O(n3)parsing capabil-\nity for ambiguous grammars. Unlike simpler parsing meth-\nods that might get stuck or fail when encountering complex\ngrammar structures, the Earley algorithm can handle any\ncontext-free grammar, making it particularly valuable for\nreal-world applications where grammar complexity varies\nsignificantly. It not only avoids the exponential complex-\nity that plagues many other parsing approaches but also\nmaintains the capability to parse all context-free grammars.\nThe algorithm maintains a sequence of state sets\nS[0], S[1], . . . , S [n], where each set S[i]contains Earley\nitems representing all valid partial parses at position iin the\ninput string. Think of these state sets as snapshots of all\npossible ways the parser could interpret the input up to each\nposition—similar to how a human reader might consider\nmultiple interpretations of a sentence while reading it word\nby word. Each Earley item within these sets takes the form\n3\n--- Page 4 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\n(X→α•β, j), where X→αβis a grammar rule with α\nandβbeing sequences of terminals and non-terminals, the\ndot (•) indicates the current parsing position within the rule\nandjindicates where in the input string this rule started\nbeing applied. The dot can be understood as a bookmark\nshowing ”we’ve successfully matched everything before this\npoint, and we’re looking for what comes after.”\nThe algorithm parses the input string through three funda-\nmental operations:\nTheprediction operation handles states of the form (X→\nα•Y β, j )where Yis a non-terminal symbol; it adds all\nrules starting with Yto the current state set S(k), where k\nis the current position.\nThe scanning operation (X→α•aβ, j)where ais a\nterminal symbol; if amatches the current input symbol, it\nadds the state (X→αa•β, j)to the next state set S(k+1).\nThe completion operation activates when encountering\nstates where the dot has reached the end (X→γ•, j),\nwhere γrepresents the portion of the rule that has been fully\nmatched up to this point; it finds all states in S(j)where\nthe dot precedes X(like Y→α•Xβ, i ), and adds the\nadvanced state (Y→αX•β, i)toS(k). Importantly, each\nstate set maintains only unique states without duplicates.\nConsider a simple grammar: S→AB,A→a,B→b\nprocessing the input ”ab” with the start rule S→ •AB.\nThrough prediction, it adds A→ •a. Upon scanning ’a’,\nit creates A→a•inS[1], leading to S→A•B. The\nprocess continues until reaching S→AB•in the final state\nset, confirming a successful parse. This simple example\ndemonstrates how the algorithm systematically explores\nparsing possibilities: it predicts what could come next, scans\nto match actual input, and completes patterns when they’re\nfully recognized.\n3.3. LLM Constrained Decoding\nLarge Language Models (LLMs) like GPT-4, Llama, and\nMistral generate text in an auto-regressive manner: at each\nstep, the model predicts the next token based on previously\ngenerated tokens (or input prompt). Specifically, the model\noutputs a logits vector over its vocabulary, which is con-\nverted into a probability distribution through the softmax\nfunction for token sampling.\nWhen generating text that must conform to specific syntactic\nstructures or formats (e.g., JSON, SQL queries, or templated\ntext), directly sampling from the model’s probability distri-\nbution alone may not guarantee valid outputs. Constrained\ndecoding addresses this challenge by applying a logits mask\nbefore token sampling. This process sets the logits of in-\nvalid tokens that violate output formats to −∞, effectively\nzeroing their probabilities after softmax , ensuring only\nOut Token\nLLM InferencePredict Logits\n5 15 3 2 122 3 12 5 15\n1 1 0 0 1\nGrammarToken MaskSoftmaxFigure 2. Constrained decoding. Constrained decoding can be\nachieved by masking the illegal tokens at the current step.\nvalid tokens can be sampled.\nTo illustrate, consider generating a JSON structure with\na simple CFG JSON -> \"{\" PairList \"}\" , where\nPairList represents key-value pairs. During generation,\nafter producing ’{’, an Earley parser identifies that only\n’\"’ (for starting a string) or ’}’ (for empty objects) are\nvalid next tokens. The constrained decoding step then masks\nall other tokens’ logits to −∞, restricting sampling to only\nthese valid tokens. This ensures the generated text strictly\nadheres to the specified grammar.\n4. Formatron\n4.1. Motivation\nAlthough the Earley algorithm can parse any context-free\ngrammar and possesses theoretical universality, it often en-\ncounters significant memory and computational overhead\nin practical applications. When the input length is n, the\nEarley parser constructs a set S[i]at each position i(from\n0 ton). For scenarios involving large inputs or numerous\ngrammar rules, this scale often leads to excessive memory\nconsumption.\nFurthermore, to perform backtracking and check expandabil-\nity during the Complete steps, the parser typically retains\nall previous sets. However, in many cases, certain sets or\nitems become obsolete in subsequent steps: for instance,\nwhen parsing JSON objects, once a key-value pair has been\nfully recognized, there is no need to continue tracking its\nassociated states. Similarly, terminals are often defined\nusing regular expressions and implemented as finite state\nmachines (FSM) for matching; once a terminal’s match is\ncompleted, the corresponding FSM instance has fulfilled\nits recognition purpose, and retaining it further only unnec-\nessarily occupies memory resources. In modern computer\narchitectures, these ”dead” or ”idle” states result in numer-\nous ineffective accesses, increasing the probability of cache\nmisses, thereby slowing down program execution. More\n4\n--- Page 5 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nformally, we can define the rules that can create such ”dead”\nstates in parser as High-level Regular Rule (HRR) in CFG:\nlet the rule’s LHS symbol be A, then the rule is HRR if the\nrule satisfies one of the following forms:\n{A→c, A→Ba, A →ϵ}\nwhere A, B are non-terminal symbols, Bis not ambiguous,\na, care terminal symbols and ϵdenotes the empty string.\nIntuitively, HRR creates ”dead” states because after c, ϵ, B\nare parsed, we technically no longer need to record the states\nbefore or during c, ϵorB’s parsing, while existing parsing\nalgorithms retain them by default.\nWhen the Earley algorithm is employed for constrained de-\ncoding in conjunction with large language models (LLMs),\nthe aforementioned issues become even more severe. Un-\nlike traditional Earley algorithm processing a static input\nsequence ”left-to-right”, constrained decoding needs to pro-\ncess all tokens in the vocabulary along with already gen-\nerated tokens so that we can construct a logits mask at\neach decoding step to filter out candidate tokens that vio-\nlate syntactic constraints. This requires the Earley parser to\nreal-time update and return viable tokens after each LLM\ndecoding step; however, if the parser retains numerous irrel-\nevant states or sets and fails to promptly clean up ”dead” or\n”idle” Earley items, it slows down the decoding process.\nTo address this, we propose a dynamic pruning strategy\nthat aims to minimize redundant storage and repeated com-\nputations of ineffective sets while preserving the theoretical\ncompleteness of the Earley algorithm. In simple terms, this\nstrategy ”online” tracks which sets and states may still be\nreferenced in subsequent steps during the parsing process,\nand promptly discard items that no longer contribute, sig-\nnificantly reducing memory usage and accelerating parsing\nspeed.\n4.2. ZapFormat\nThis section introduces ZapFormat, a novel method for track-\ning dependencies among Earley items (Earley, 1970) and\napplying a reachability-based pruning strategy in real time.\nBy maintaining a dependency graph , we can effectively\nremove items that will not contribute to any valid parse,\nthereby reducing the overall number of states.\n4.2.1. D EPENDENCIES\nTo enable more effective tracking of parsing progress and\ndependencies, we extend the traditional Earley item nota-\ntion to (A→α•β, i, j )in this enhanced representation,\n(A→α•β)denotes a production rule from the grammar,\nwhile the span [i, j]precisely captures α’s coverage of the\ninput sequence from position iup to (but not including) j.\nBased on the extended representation, we proceed to define\ndependencies. Three types of inter-item dependencies natu-rally arise from the Earley algorithm’s operations: Predict ,\nScan , and Complete .\nPredict Dependency . When an item p= (A→α•\nBβ, i, j )triggers the prediction of all rules B→γin the\ngrammar, each new predict itemq= (B→ •γ, j, j )is\nsaid to depend onp. We then say Dpred(q, p)⇐⇒ B→\nγ∈p and p = (A→α•Bβ, i, j ).\nScan Dependency . ASCAN operation moves the dot past a\nterminal symbol that matches the current input token. If the\nnext input token is aand we have an item p= (A→α•\naβ, i, j ),then scanning yields q= (A→α a•β, i, j + 1).\nWe then say: Dscan(q, p)⇐⇒ input[j] =a and p =\n(A→α•aβ, i, j ).\nComplete Dependency . The completion operation occurs\nwhen we have fully parsed a nonterminal symbol in the\ngrammar. Specifically, when an item has its dot at the end,\nindicating a complete derivation of a nonterminal B, it can\nbe used to advance all items whose postdot nonterminal is\nits left hand side. Suppose p= (B→γ•, k, j), q =\n(A→α•Bβ, i, k ),and we form the new item r= (A→\nα B•β, i, j ).We then say Dcomp(r, p, q )⇐⇒ p=\n(B→γ•, k, j)and q = (A→α•Bβ, i, k ).\n4.2.2. D EPENDENCY GRAPH\nTo efficiently track these dependencies, we construct a di-\nrected graph\nG= (V, E),\nwhere each vertex v∈Vcorresponds to an Earley item,\nand each directed edge (x, y)∈Eindicates that ydepends\nonx. This graph is maintained dynamically as new items\nare created.\nThe dependency graph construction process operates on\nEarley item sets, maintaining vertices Vand edges Eto\ntrack parsing dependencies. When the parser begins, Vis\nempty and grows as new items are discovered. For each\nitem creation through PREDICT ,SCAN, orCOMPLETE op-\nerations, corresponding edges are inserted to record depen-\ndencies from source items. The graph updates dynamically\nas new Earley item sets ( S0, S1, . . .) are formed, ensuring\ncomprehensive dependency tracking throughout the parsing\nprocess.\n4.2.3. R EACHABILITY AND DYNAMIC PRUNING\nFor effective pruning, we first define the reachability closure .\nNote that PREDICT ,SCAN, and COMPLETE operations all\nstarts by checking items in the lastly created Earley set.\nIntuitively, this means the reachability of an item can be\ndefined as whether the item depends on any item in the\nlast set. More formally, given an item in an Earley set,\nwe determine which items should be retained through the\nfollowing definitions:\n5\n--- Page 6 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\n?0 ?1 ?2\nA→∙AB, (0)\nA→∙B, (0)\nB→∙’a’, (0)B→’a’∙, (0)\nA→B∙, (0)\nA→A∙B, (0)\nB→∙’a’, (1)B→’a’∙, (1)\nA→AB∙, (0)\nA→A∙B, (0)\nB→∙’a’, (2)Grammar:\nA→AB|B\nB→’a’\nInput: “a”\nFigure 3. Dynamic pruning. Red parts indicate prunable nodes,\nand green dashed lines represent Complete dependency paths.\nFirst,already completed Earley items only lead to modifications\non their residing Earley set. Once their residing Earley set is fully\nprocessed, we can remove them. In addition, after one Earley set\nis fully processed, if an Earley set is not referenced by the union\nof reference chains of all items in the last Earley set, then it can be\nremoved.\nReachability Closure : For each item x∈V, if there exists\na path from xto an item ain the last earley set, we consider\nthis item ”reachable”. Formally, we define the reachability\nclosure as:\nReach (a) ={x∈V|x→∗a},\nwhere →∗denotes a directed path (potentially multi-step)\nfrom xtoa.\nActive Item Set: We define the active item set Ras the union\nof all reachable items:\nR=[\na∈VReach (a).\nOnly items within this active set need to be retained.\nWe enhance the original Earley parsing algorithm by intro-\nducing a compact operation after the complete phase and\nbefore prediction phase, evaluating each item’s retention\nnecessity. This phase ordering eliminates useless computa-\ntions on newly predicted items(which always only depend\nthe items before prediction in the last set). Specifically, the\nCompact operation examines all items in the current set,\neliminating those that do not belong to the active item set\nR.\nTo ensure both correctness and efficiency of the pruning op-\nerations, we implement an incremental update strategy. The\nactive item set is updated after each parsing phase, particu-\nlarly following the completion and compaction phases. This\ndynamic maintenance strategy ensures that pruning opera-\ntions remain responsive to parsing state changes, avoid re-\nsearching the whole Vrepeatedly, and maintain algorithmic\ncorrectness. By combining forward reachability analysis\nwith dynamic pruning, our algorithm significantly reduces\nthe number of items requiring processing while preserv-\ning parsing correctness. This forward reachability-baseddynamic pruning approach not only provides theoretical\ncompleteness guarantees but also demonstrates superior per-\nformance optimization in practice.\n4.3. Context-independent Tokens Mask Cache\nInspired by XGrammar’s (Dong et al., 2024) context-\nindependent token mask cache, we use a token mask cache\nmethod to enhance decoding efficiency in our constrained\ndecoding engine. The core of token mask cache mechanism\ncategorizes tokens into two types: Context-Independent\ntokens, whose validity can be determined by examining\npostdot terminals of items in the last Earley set, and Context-\nDependent tokens, which require full parsing context. The\ntoken mask cache accelerates the parsing process by pre-\ncomputing valid and invalid context-independent tokens for\neach terminal, which are then efficiently stored as bitsets.\nNote that unlike XGrammar, we do not consider the pos-\nsible suffix strings of terminals when determining invalid\ncontext-independent tokens to save precomputation time. At\nruntime, the valid and invalid context-independent tokens\nare retrieved directly from the cache, eliminating redundant\ncomputations and thus reducing the overall decoding time.\nIn scenarios where multiple postdot terminals exist, the\ntoken masks from each terminal are merged into a single\nfinal token mask. By reducing the number of computations\nrequired for token validation through leveraging precom-\npution, the adaptive token mask cache significantly speeds\nup the decoding process.\n4.4. Rejection Prefix Optimization\nWe introduce a prefix-based early rejection mechanism that\nenhances parsing efficiency by identifying grammatically\nimpossible input paths at the earliest stage of the parsing\nprocess. The optimization maintains a set of “rejected pre-\nfixes” – minimal byte sequences that definitively preclude\nvalid parses regardless of subsequent input extensions .\nThese prefixes represent fundamental grammatical viola-\ntions that cannot be completion. The optimization operates\nby maintaining and continuously updating these sequences\nthat, when encountered during parsing, immediately indi-\ncate the impossibility of a valid parse.\nFor instance, in a context-free grammar (CFG), if the pre-\nfix\"aaac\" constitutes a rejected prefix according to the\ngrammar rules. Once this sequence is detected (e.g., in input\n\"aaacdefrf\" ), no grammatical derivation can produce\nvalid parse trees for any extension of this prefix, regardless\nof subsequent characters.\nWhen a rejected prefix is detected, the parser can safely\ndiscard all extensions of that prefix without state exploration.\nThis set is updated whenever a token is rejected at each\ndecoding step.\n6\n--- Page 7 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\n4.5. Grammar Transformation\nTo optimize constrained decoding efficiency, we use a\ngrammar transformation framework (Hopcroft & Ullman,\n1979a;b). The primary transformation step involves struc-\ntural optimization, where we identify and eliminate useless\nrules that cannot contribute to valid derivations, thereby\nreducing the grammar size without affecting its expressive-\nness. This is particularly helpful for grammar generated\nfrom a high-level format like json schema, where the gener-\nator, potentially built by third parties, may fail to remove all\nunreferenced rules.\nAnother crucial optimization addresses null rules. We\nsystematically handle rules that can derive empty strings\nthrough a three-phase approach: first identifying all null\nsymbols, then generating alternative productions, and fi-\nnally selectively retaining specific null productions where\nnecessary. This transformation substantially reduces the\nbranching factor during parsing.\n5. Results\n5.1. Experimental Setup\nAll experiments were conducted on a system equipped\nwith an NVIDIA GeForce RTX 3090 (24GB VRAM)\nand an AMD EPYC 7452 32-core processor. The soft-\nware environment consisted of PyTorch 2.4.0 and CUDA\n12.4, with model inference performed using Transformers\nv4.48.0. Four pre-trained large language models were em-\nployed in this study: google/gemma-2-9b-it (Gemma Team\n& Shreya Pathak, 2024), meta-llama/Llama-3-8B-Instruct\n(Dubey et al., 2024), mistralai/Mistral-7B-Instruct-v0.3\n(Jiang et al., 2023), and qwen/Qwen2.5-7B-Instruct (Yang\net al., 2024), all utilizing half-precision (FP16) inference.\nFor more details of Python libraries, see the appendix A.\nBaselines. lm-format-enforcer (v0.10.9) (Jiang et al., 2024)\nimplements incremental validation based on syntax tree\npre-computation, suitable for structured constraints but\nwith significant memory overhead. outlines (v0.1.13)\n(Willard & Louf, 2023) employs finite state machines for\ndynamic masking of invalid tokens, excelling in regular\nconstraints but not directly applicable to context-free gram-\nmars. One significant consequence of this limitation is\nthat they can only support a small subset of json schemas\n(dottxt-ai/outlines contributors, 2023). XGrammar (v0.1.10)\n(Dong et al., 2024) supports Context-Free Grammars (CFG)\nthrough simulating pushdown automata with tree-structured\nstacks, offering high versatility but introducing parsing la-\ntency. It also cannot directly handle left-recursive CFGs\n(mlc-ai/xgrammar contributors, 2024).\nTest Task. Geoquery (Davis & Meltzer, 2007) transforma-\ntion converts natural language queries into FunQL, adher-ing to fixed predicates and finite entity constraints. JSON\nSchema (Pezoa et al., 2016) generation produces JSON\ninstances compliant with type, enumeration, and regular\nexpression constraints. JSON Grammar generation creates\nsyntactically valid and semantically coherent nested JSON\nstructures with cross-field dependencies. Data examples are\nshown in Appendix D.\nEvaluation Metrics. Throughput represents a fundamen-\ntal metric in the assessment of LLM constrained decoding\nperformance, defined as the ratio of constraint-satisfying to-\nkens generated to temporal duration, expressed in tokens per\nsecond (Token/s). This measurement provides insights into\ncomputational efficiency and resource utilization efficacy.\n5.2. Experimental Results\nWe conducted a comprehensive evaluation of the forma-\ntron engine across four mainstream large language models.\nTable 1 presents a performance comparison of different ap-\nproaches under various constraint types:\nPerformance Advantage Analysis. The experimental re-\nsults robustly validate the efficacy of formatron. In most\ntested scenarios, formatron achieved significant perfor-\nmance improvements compared to baseline methods. These\nenhancements can be attributed to formatron’s core inno-\nvation: dynamically pruning invalid or redundant Earley\nstates, substantially reducing memory consumption during\nconstraint parsing. Coupled with an efficient caching mech-\nanism, formatron consistently maintains high-throughput\noutput. These empirical data comprehensively demonstrate\nformatron’s performance superiority in constrained decod-\ning tasks.\nRobustness Analysis. In terms of model robustness, for-\nmatron exhibits exceptional cross-architectural adaptability.\nBy testing across models with different scales and architec-\ntures, formatron consistently maintains stable performance,\nverifying its characteristic of being generally applicable\nacross various LLM architectures. From 9B to 7B model\nscales, formatron demonstrates consistent high-efficiency\nperformance.\nRegarding task robustness, formatron excels in diverse con-\nstrained decoding tasks. From JSON generation to JSON\nSchema validation and semantic parsing, formatron con-\nsistently produces high-precision compliant outputs. This\ncross-task stability comprehensively substantiates the reli-\nability and generalizability of the formatron technical ap-\nproach.\nComparative Method Analysis . Formatron possesses ad-\nvantages compared to existing methods. While outlines only\nsupports regular grammars through finite state automata,\nformatron manages to support full context-free grammar\nwith Earley algorithm and achieves superior performance\n7\n--- Page 8 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nTable 1. Comparative Throughput Performance of Different Meth-\nods in Constraint Parsing Tasks (tokens/s). Here, ‘-‘ indicates that\nthe method is not applicable to the task, and bold indicates the best\nresult. All experimental results were obtained in the same local\nenvironment. lm-format represents lm-format-enforcer. Json s\nrepresents Json Scheam and Json g represents Json Grammar.\nModel Method geoquery json s json gGemmalm-format - 60.08 22.95\nOutlines - 61.32 -\nXgrammar 616.66 1473.99 10245.04\nFormatron 12174.68 7943.34 8668.69Llama3lm-format - 120.56 47.79\nOutlines - 114.10 -\nXgrammar 2758.98 2796.36 7757.15\nFormatron 6700.87 8207.85 8535.64Mistrallm-format - 576.11 372.55\nOutlines - 598.98 -\nXgrammar 5926.64 8421.43 15273.72\nFormatron 11703.00 10639.87 12046.54Qwenlm-format - 45.31 21.26\nOutlines - 112.10 -\nXgrammar 1725.86 2037.26 7290.30\nFormatron 6399.68 9234.08 9811.02\nover outlines on regular grammars. Unlike lmformaten-\nforcer’s naive input token prefix cache, formatron’s state\ncache with dynamic pruning greatly increases cache hit rate\nand decreases memory occupation, since multiple different\ninput token sequences can correspond to the same pruned\nEarley states. Notably, XGrammar, developed concurrently\nwith our work (published within three months of our paper\nsubmission), represents a contemporary approach in this do-\nmain. By combining our novel algorithm with optimizations\nproposed by XGrammar, formatron achieves competitive\nperformance across most tasks, with XGrammar showing\nsuperior performance in certain specific scenarios (e.g., geo-\nquery on Gemma and Json s on Mistral), while formatron\nexcels in the majority of other tasks without incurring more\nprecomputation costs. These advantages substantially en-\nhance formatron’s processing efficiency in constrained de-\ncoding tasks.\nTo further validate the effectiveness and reliability of our\nproposed method, we conducted two additional experiments\naddressing potential concerns about our evaluation method-\nology. First, we performed a comprehensive analysis of\nwhole pipeline component performance. Second, we eval-\nuated output quality using accuracy metrics to ensure that\nour efficiency gains do not come at the expense of gener-\nation quality. The detailed results of these supplementary\nexperiments are presented in Appendix B.Table 2. Throughput Comparison of Different Models and Meth-\nods Across Multiple Runs. lm-format represents lm-format-\nenforcer.\nModel MethodNumber of Runs\n1 run 3 run 5 run 10 runGemmalm-format 60.08 63.48 66.15 75.84\nOutlines 61.62 61.66 62.00 62.13\nXgrammar 1473.99 1577.05 1564.97 1551.51\nFormatron 7943.34 10609.08 11323.63 11993.89Llama3lm-format 120.56 121.51 139.99 159.34\nOutlines 114.10 104.71 105.13 103.44\nXgrammar 2796.36 2533.45 2578.91 2629.54\nFormatron 8207.85 10271.73 11018.91 11945.94Mistrallm-format 576.11 723.72 794.30 796.68\nOutlines 598.98 634.63 649.78 641.22\nXgrammar 8421.43 7673.90 7699.77 7885.20\nFormatron 10639.87 13522.77 13841.85 14424.69Qwenlm-format 45.31 41.89 46.65 49.47\nOutlines 112.10 89.82 90.13 90.22\nXgrammar 2037.26 2247.04 2299.04 2284.37\nFormatron 9234.08 10537.26 11397.31 12202.48\n5.3. Multiple Runs\nTo mitigate the impact of random factors, we conducted\nmultiple runs of experiments on Json Schema . To con-\nduct multiple runs, we utilized LLM for data augmentation.\nFor further details, please refer to the appendix E. The ex-\nperimental results (shown in Table 2) demonstrate that our\nproposed Formatron method significantly outperforms ex-\nisting baseline approaches in terms of throughput. Through\nsystematic evaluation across different numbers of runs, we\nobserved several key trends:\nFirst, Formatron exhibits remarkable performance advan-\ntages in single-run scenarios. Across all tested models, For-\nmatron achieves substantially higher throughput compared\nto baseline methods, with particularly notable advantages in\nthe Mistral and Qwen models. This performance enhance-\nment can be primarily attributed to Formatron’s dynamic\npruning mechanism, which effectively reduces redundant\noperations in computation paths.\nFurthermore, in multi-run scenarios, we observed an inter-\nesting performance evolution pattern. Formatron demon-\nstrates progressive throughput improvement as the number\nof runs increases, showing consistent performance gains\nfrom three runs to five and ten runs. In contrast, baseline\nmethods show limited improvement in multi-run scenarios,\nexhibiting stability but lacking breakthrough performance.\nThis phenomenon demonstrates Formatron’s performance\nstability and robustness across extended operations.\n8\n--- Page 9 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nTable 3. Experimental results of ablation. Here, ”-& cache” in-\ndicates the ablation of the cache in addition to the ablation of\npruning.\nLLM MethodNumber of runs\n3 run 5 run 10 runGemmaFormatron 10609.08 11323.63 11993.89\n-pruning 6981.35 6628.69 7674.78\n- & cache 3595.48 3038.70 3365.96Llama3Formatron 10271.73 11018.91 11945.94\n-pruning 6269.24 7064.69 7619.57\n-& cache 3464.36 3438.40 3445.73mistralFormatron 13522.77 13841.85 14424.69\n-pruning 9727.00 9824.90 9707.45\n-& cache 8845.90 8204.43 7669.42qwenFormatron 10537.26 11397.31 12202.48\n-pruning 8635.20 9476.70 10048.19\n-& cache 4189.57 4180.18 4196.09\n5.4. Ablation Study\nTo investigate the contribution of each component in Forma-\ntron, we conducted ablation experiments by removing the\npruning and caching mechanisms. Table 5.4 presents the\nresults across different models and run configurations.\nThe ablation study reveals that removing the pruning mech-\nanism (-pruning) results in significant performance degrada-\ntion, with throughput reductions of 30-50% for most models,\ndemonstrating its crucial role in optimization. The further\nremoval of caching (-& cache) generally leads to additional\nperformance deterioration.\nIn addition, even with both pruning and caching ablated,\nour method still outperforms several baselines, demonstrat-\ning the substantial contributions of other optimizations dis-\ncussed earlier, including prefix rejection and grammar trans-\nformation.\nOn the other hand, in order to validate our claims regarding\nmemory reduction, we conducted additional experiments\nmeasuring the maximum memory usage (in MB) during\nconstrained decoding (Table 4). The results show that our\npruning mechanism reduces memory consumption. These\nresults confirm that our approach successfully reduces mem-\nory footprint by eliminating redundant Earley states.\n6. Conclusion\nWe present Formatron, an efficient LLM constrained decod-\ning engine that achieves 1.5-2 ×speedup over state-of-the-art\nmethod. Our Formatron contains two core innovations: (1)Table 4. Maximum memory usage comparison during constrained\ndecoding.\nModel Method Max Memory Usage (MB)\nLlama3Formatron 1635.92\nw/o pruning 1655.48\nMistralFormatron 1519.09\nw/o pruning 1530.77\nThe proposed ZapFormat algorithm based on the identifi-\ncations of HRR in CFG, which reduces memory usage via\ndynamic Earley state pruning based on dependency reacha-\nbility analysis; (2) A hybrid optimization framework com-\nbining grammar-aware token masking with prefix rejection.\nEvaluations across JSON generation and semantic parsing\ntasks demonstrate consistent performance improvements\nwhile maintaining 100% structural compliance, establish-\ning new efficiency benchmarks for grammar-guided LLM\ndecoding.\nAcknowledgements\nShiwen Ni was supported by GuangDong Basic and Ap-\nplied Basic Research Foundation (2023A1515110718 and\n2024A1515012003), China Postdoctoral Science Founda-\ntion (2024M753398), Postdoctoral Fellowship Program of\nCPSF (GZC20232873) and Shenzhen Major Science and\nTechnology Project (KCXFZ20240903094007010).\nImpact Statement\nThis study introduces methodological improvements for\nconstrained decoding in large language models, with partic-\nular emphasis on computational efficiency gains for struc-\ntured text generation tasks. Our approach enhances the\nreliable generation of grammatically valid outputs in practi-\ncal applications ranging from automated API call generation\nto standardized data formatting. The proposed techniques\nraise ethical considerations common to language model\ntechnologies, particularly regarding their potential deploy-\nment in automated decision-making systems. While the\nimproved efficiency may expand access to structured gener-\nation tools across various sectors, appropriate safeguards are\nstill necessary to mitigate risks associated with uncontrolled\nautomation. It is important to note that the main techni-\ncal advance—refining grammatical constraint enforcement\nmechanisms—does not introduce novel ethical concerns be-\nyond those already present in conventional language model\napplications. Researchers and practitioners adopting these\nmethods should maintain established responsible innova-\ntion practices, including rigorous output verification and\nmaintenance of human oversight protocols.\n9\n--- Page 10 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nReferences\nBeurer-Kellner, L., Fischer, M., and Vechev, M. Prompting\nis programming: A query language for large language\nmodels. Proceedings of the ACM on Programming Lan-\nguages , 7(PLDI):1946–1969, 2023.\nBeurer-Kellner, L., Fischer, M., and Vechev, M. Guiding\nllms the right way: fast, non-invasive constrained genera-\ntion. In Proceedings of the 41st International Conference\non Machine Learning , ICML’24. JMLR.org, 2024.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pond ´e, H., Kaplan,\nJ., Edwards, H., Burda, Y ., Joseph, N., Brockman, G.,\nRay, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H.,\nSastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,\nPavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,\nC., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M.,\nChantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,\nNichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A.,\nLeike, J., Achiam, J., Misra, V ., Morikawa, E., Radford,\nA., Knight, M. M., Brundage, M., Murati, M., Mayer,\nK., Welinder, P., McGrew, B., Amodei, D., McCandlish,\nS., Sutskever, I., and Zaremba, W. Evaluating large lan-\nguage models trained on code. ArXiv , abs/2107.03374,\n2021. URL https://api.semanticscholar.\norg/CorpusID:235755472 .\nChomsky, N. Three models for the description of language.\nIRE Transactions on information theory , 2(3):113–124,\n1956.\nDavis, S. and Meltzer, P. S. Geoquery: a bridge between the\ngene expression omnibus (geo) and bioconductor. Bioin-\nformatics , 23(14):1846–1847, 2007.\nDeutsch, D., Upadhyay, S., and Roth, D. A general-purpose\nalgorithm for constrained sequential inference. In Pro-\nceedings of the 23rd Conference on Computational Natu-\nral Language Learning (CoNLL) , pp. 482–492, 2019.\nDong, Y ., Ruan, C. F., Cai, Y ., Lai, R., Xu, Z., Zhao, Y .,\nand Chen, T. Xgrammar: Flexible and efficient struc-\ntured generation engine for large language models. arXiv\npreprint arXiv:2411.15100 , 2024.\ndottxt-ai/outlines contributors. Implement json\nschema field constraints #215, August 2023.\nURL https://github.com/dottxt-ai/\noutlines/issues/215 . Accessed: [2025-01-15].\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.Earley, J. An efficient context-free parsing algorithm. Com-\nmun. ACM , 13(2):94–102, February 1970. ISSN 0001-\n0782. doi: 10.1145/362007.362035. URL https:\n//doi.org/10.1145/362007.362035 .\nFang, H., Balakrishnan, A., Jhamtani, H., Bufe, J., Craw-\nford, J., Krishnamurthy, J., Pauls, A., Eisner, J., An-\ndreas, J., and Klein, D. The whole truth and noth-\ning but the truth: Faithful and controllable dialogue re-\nsponse generation with dataflow transduction and con-\nstrained decoding. In Rogers, A., Boyd-Graber, J.,\nand Okazaki, N. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2023 , pp. 5682–5700,\nToronto, Canada, July 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.\n351. URL https://aclanthology.org/2023.\nfindings-acl.351/ .\nGemma Team, M. R. and Shreya Pathak, e. Gemma 2:\nImproving open language models at a practical size, 2024.\nURL https://arxiv.org/abs/2408.00118 .\nGeng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-\nconstrained decoding for structured nlp tasks without fine-\ntuning. In The 2023 Conference on Empirical Methods\nin Natural Language Processing .\nGeng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-\nconstrained decoding for structured NLP tasks without\nfinetuning. In Bouamor, H., Pino, J., and Bali, K. (eds.),\nProceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pp. 10932–10952,\nSingapore, December 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.emnlp-main.\n674. URL https://aclanthology.org/2023.\nemnlp-main.674/ .\nHopcroft, J. E. and Ullman, J. D. Introduction to Automata\nTheory, Languages, and Computation . Addison-Wesley,\n1st edition, 1979a. remove useless rules.\nHopcroft, J. E. and Ullman, J. D. Introduction to Automata\nTheory, Languages, and Computation . Addison-Wesley,\n1st edition, 1979b. remove nullable rules.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825 , 2023.\nJiang, X., Li, X., Ma, W., Fang, X., Yao, Y ., Yu, N.,\nMeng, X., Han, P., Li, J., Sun, A., et al. Sketch: A\ntoolkit for streamlining llm operations. arXiv preprint\narXiv:2409.03346 , 2024.\nKoo, T., Liu, F., and He, L. Automata-based constraints\nfor language model decoding. In First Conference on\nLanguage Modeling .\n10\n--- Page 11 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nKuchnik, M., Smith, V ., and Amvrosiadis, G. Validating\nlarge language models with relm. Proceedings of Ma-\nchine Learning and Systems , 5:457–476, 2023.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000) , pp. 1207–1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLeo, J. M. A general context-free parsing algorithm\nrunning in linear time on every lr(k) grammar\nwithout using lookahead. Theoretical Computer\nScience , 82(1):165–176, 1991. ISSN 0304-3975.\ndoi: https://doi.org/10.1016/0304-3975(91)90180-A.\nURL https://www.sciencedirect.com/\nscience/article/pii/030439759190180A .\nLiu, B., Jiang, Y ., Zhang, X., Liu, Q., Zhang, S., Biswas,\nJ., and Stone, P. Llm+p: Empowering large language\nmodels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477 , 2023.\nLundberg, S., Ribeiro, M. T. C., et al. Guidance-ai/guidance:\nA guidance language for controlling large language mod-\nels.URL https://github. com/guidance-ai/guidance , 2023.\nmlc-ai/xgrammar contributors. Support left recursive gram-\nmars. currently going into infinite loop #126, Decem-\nber 2024. URL https://github.com/mlc-ai/\nxgrammar/issues/126 . Accessed: [205-01-15].\nOpedal, A., Zmigrod, R., Vieira, T., Cotterell, R., and Eis-\nner, J. Efficient semiring-weighted Earley parsing. In\nRogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Pro-\nceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) ,\npp. 3687–3713, Toronto, Canada, July 2023. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/\n2023.acl-long.204. URL https://aclanthology.\norg/2023.acl-long.204/ .\nOpenAI. Function calling - OpenAI API, 2024.\nURL https://platform.openai.com/docs/\nguides/function-calling . [Accessed 26-10-\n2024].\nOpenAI, Josh Achiam, S. A. and Sandhini Agarwal, e. Gpt-\n4 technical report, 2024. URL https://arxiv.org/\nabs/2303.08774 .\nPezoa, F., Reutter, J. L., Suarez, F., Ugarte, M., and\nVrgo ˇc, D. Foundations of json schema. In Proceed-\nings of the 25th International Conference on World Wide\nWeb, WWW ’16, pp. 263–273, Republic and Canton of\nGeneva, CHE, 2016. International World Wide Web Con-\nferences Steering Committee. ISBN 9781450341431.\ndoi: 10.1145/2872427.2883029. URL https://doi.\norg/10.1145/2872427.2883029 .Poesia, G., Polozov, A., Le, V ., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations .\nRoy, S., Thomson, S., Chen, T., Shin, R., Pauls, A., Eisner,\nJ., and Van Durme, B. Benchclamp: A benchmark for\nevaluating language models on syntactic and semantic\nparsing. Advances in Neural Information Processing\nSystems , 36, 2024.\nScholak, T., Schucher, N., and Bahdanau, D. Picard: Pars-\ning incrementally for constrained auto-regressive decod-\ning from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing , pp. 9895–9901, 2021.\nShin, R., Lin, C., Thomson, S., Chen, C., Roy, S., Platanios,\nE. A., Pauls, A., Klein, D., Eisner, J., and Van Durme,\nB. Constrained language models yield few-shot seman-\ntic parsers. In Moens, M.-F., Huang, X., Specia, L.,\nand Yih, S. W.-t. (eds.), Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing , pp. 7699–7715, Online and Punta Cana, Domini-\ncan Republic, November 2021. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n608. URL https://aclanthology.org/2021.\nemnlp-main.608/ .\nWang, C., Cho, K., and Gu, J. Neural machine translation\nwith byte-level subwords. In Proceedings of the AAAI\nconference on artificial intelligence , volume 34, pp. 9154–\n9160, 2020.\nWang, Y ., Wang, W., Joty, S., and Hoi, S. C. CodeT5:\nIdentifier-aware unified pre-trained encoder-decoder\nmodels for code understanding and generation. In\nMoens, M.-F., Huang, X., Specia, L., and Yih, S.\nW.-t. (eds.), Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing ,\npp. 8696–8708, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n685. URL https://aclanthology.org/2021.\nemnlp-main.685/ .\nWillard, B. T. and Louf, R. Efficient guided generation for\nlarge language models, 2023. URL https://arxiv.\norg/abs/2307.09702 .\nXiao, C., Dymetman, M., and Gardent, C. Sequence-\nbased structured prediction for semantic parsing. In\nErk, K. and Smith, N. A. (eds.), Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 1341–1350,\n11\n--- Page 12 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nBerlin, Germany, August 2016. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/P16-1127. URL\nhttps://aclanthology.org/P16-1127/ .\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115 , 2024.\nYin, P. and Neubig, G. A syntactic neural model for\ngeneral-purpose code generation. In Barzilay, R. and\nKan, M.-Y . (eds.), Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pp. 440–450, Vancouver,\nCanada, July 2017. Association for Computational Lin-\nguistics. doi: 10.18653/v1/P17-1041. URL https:\n//aclanthology.org/P17-1041/ .\n12\n--- Page 13 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nA. Appendix: Python Library Versions\nThis appendix provides a comprehensive list of Python libraries and their respective versions used in our project. Docu-\nmenting the exact versions of these dependencies is crucial for ensuring the reproducibility of our work. By maintaining a\nconsistent environment with the specified versions, readers can accurately replicate our experiments and avoid potential\nissues arising from version discrepancies. Below, we present the libraries and their versions in a tabular format for easy\nreference.\nTable 5. Python Libraries and Versions\nPackage Version Package Version Package Version\naccelerate 1.2.1 aiohappyeyeballs 2.4.4 aiohttp 3.11.11\naiosignal 1.3.2 airportsdata 20241001 annotated-types 0.7.0\nastunparse 1.6.3 attrs 24.3.0 cloudpickle 3.1.1\ndatasets 3.2.0 dill 0.3.8 diskcache 5.6.3\ndnspython 2.6.1 einops 0.8.0 expecttest 0.2.1\nflash-attn 2.7.3 frozendict 2.4.6 frozenlist 1.5.0\nfsspec 2024.6.1 general sam 1.0.1 huggingface-hub 0.27.1\nhypothesis 6.108.4 iniconfig 2.0.0 interegular 0.3.3\njsonpointer 2.1 jsonschema 4.23.0 jsonschema-specifications 2024.10.1\nlark 1.2.2 lintrunner 0.12.5 lm-format-enforcer 0.10.9\nmkl-service 2.4.0 multidict 6.1.0 multiprocess 0.70.16\nnest-asyncio 1.6.0 ninja 1.11.1.1 numpy 1.26.4\noptree 0.12.1 outlines 0.1.13 outlines core 0.1.26\npandas 2.2.3 pluggy 1.5.0 propcache 0.2.1\nprotobuf 5.29.3 pyarrow 19.0.0 pybind11 2.13.6\npycountry 24.6.1 pydantic 2.10.5 pydantic core 2.27.2\npytest 8.3.4 python-dateutil 2.9.0.post0 python-etcd 0.4.5\nreferencing 0.36.1 regex 2024.11.6 rpds-py 0.22.3\nsafetensors 0.5.2 sentencepiece 0.2.0 sortedcontainers 2.4.0\ntiktoken 0.8.0 tokenizers 0.21.0 torch 2.4.0\ntorchaudio 2.4.0 torchelastic 0.2.2 torchvision 0.19.0\ntransformers 4.48.0 triton 3.0.0 types-dataclasses 0.6.6\ntyping extensions 4.12.2 tzdata 2024.2 xgrammar 0.1.10\nxxhash 3.5.0 yarl 1.18.3\nB. Additional Experimental Results\nTo address potential concerns about the completeness of our evaluation and to provide a more comprehensive analysis of our\nproposed method, we conducted two additional experiments that examine different aspects of performance measurement\nand output quality assessment.\nB.1. Pipeline Component Analysis: Parsing and Masking Throughput\nIn response to questions about whether our reported throughput results reflect the performance of the entire generation\npipeline, we clarify that the main results presented in our paper focus specifically on the parsing and mask generation stages\nto provide precise performance analysis of our key technical innovations. However, to address this concern, we conducted\nadditional experiments measuring throughput for the complete pipeline, including throughput without constrained decoding\n(w/o CD).\nTable 6 presents the detailed throughput measurements (JSON objects per second) across different models and methods.\nWhile the complete pipeline results show lower throughput due to the inclusion of LLM calls and generation processes,\nour proposed Formatron method maintains competitive efficiency compared to other constrained decoding approaches,\nperforming nearly as well as the unconstrained baseline (w/o CD).\n13\n--- Page 14 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nTable 6. Throughput comparison for parsing and masking stages across different models and methods (JSON objects per second)\nModel Method JSON/s\nGemmaw/o CD 18.16\nlm-format 11.75\nOutlines 13.56\nXgrammar 17.72\nFormatron 18.02\nLlama3w/o CD 31.75\nlm-format 20.51\nOutlines 23.71\nXgrammar 30.42\nFormatron 30.42\nQwenw/o CD 35.40\nlm-format 16.23\nOutlines 25.98\nXgrammar 34.34\nFormatron 34.46\nB.2. Output Quality Assessment: Accuracy Metrics\nWhile a single schema is compatible across multiple constrained decoding libraries, currently no standardized interpretation\nexists across implementations. JSON Schema, for instance, does not define allowed whitespace patterns or positions within\nvalid JSON structures. Given that most models’ tokenizers are sensitive to whitespace character position and type, identical\nschemas may yield different outputs when processed through different libraries.\nTo provide a more comprehensive evaluation beyond throughput measurements, we conducted additional experiments to\nassess output quality using accuracy metrics. This addresses the important question of whether performance gains come at\nthe cost of generation quality.\nTable 7 shows the accuracy results for different models and methods across two evaluation scenarios: json schema\nandjson grammar . The results demonstrate that our proposed method achieves competitive performance compared to\nexisting approaches while maintaining the efficiency advantages shown in the main results.\nTable 7. Accuracy comparison across different models and methods for JSON schema and grammar tasks\nModel Method json schema json grammar\nGemmabaseline 0.73 -\nlm-format 0.74 0.71\nOutlines 0.80 -\nXgrammar 0.76 0.71\nFormatron 0.73 0.74\nLlama3baseline 0.47 -\nlm-format 0.60 0.40\nOutlines 0.73 -\nXgrammar 0.69 0.47\nFormatron 0.67 0.48\nMistralbaseline 0.09 -\nlm-format 0.53 0.10\nOutlines 0.44 -\nXgrammar 0.53 0.09\nFormatron 0.52 0.11\n14\n--- Page 15 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\nThese additional experiments confirm that our method maintains competitive accuracy while achieving superior efficiency,\nthus validating both the effectiveness and reliability of our approach.\nC. Notation and Terminology\nTo improve the readability of this paper, we provide a comprehensive notation table for reference. The following table\ncontains the key symbols and concepts used throughout our work.\nSymbol Description\nA, B, X, Y Non-terminal symbols in context-free grammar\na, c Terminal symbols in context-free grammar\nα, β, γ Sequences composed of terminal and non-terminal symbols\nε Empty string\nS[i], . . . , S [n] Sequence of Earley state sets, where S[i]contains items at position i\n(X→α•β, j) Earley item notation, where •indicates current parsing position and jindicates starting position\n(A→α•β, i, j )Extended Earley item notation, where span [i, j]captures β’s coverage range\nD. Task Examples\nBelow are input-output examples for the three constrained generation tasks:\nGeometry\n1I n p u t : name a l l t h e r i v e r s i n c o l o r a d o .\n2Output : answer ( r i v e r ( l o c 2 ( s t a t e i d ( ’ c o l o r a d o ’ ) ) ) )\nJSON Schema and JSON Grammar\n1I n p u t :\n2[\n3{\n4 ” c o n t e n t ” : ”You a r e a h e l p f u l a s s i s t a n t t h a t answers i n JSON . Here ’ s t h e\n,→j s o n schema you must a d h e r e t o : \\n<schema >\\n{’ t i t l e ’ : ’\n,→W i r e l e s s A c c e s s P o i n t ’ , ’ type ’ : ’ o b j e c t ’ , ’ p r o p e r t i e s ’ : {’ s s i d ’ : {’\n,→t i t l e ’ : ’ SSID ’ , ’ type ’ : ’ s t r i n g ’ }, ’ s e c u r i t y P r o t o c o l ’ : {’ t i t l e ’ : ’\n,→S e c u r i t y P r o t o c o l ’ , ’ type ’ : ’ s t r i n g ’ }, ’ bandwidth ’ : {’ t i t l e ’ : ’\n,→Bandwidth ’ , ’ type ’ : ’ s t r i n g ’ }}, ’ r e q u i r e d ’ : [ ’ s s i d ’ , ’\n,→s e c u r i t y P r o t o c o l ’ , ’ bandwidth ’ ] }\\n</schema >\\n ” ,\n5 ” r o l e ” : ” system ”\n6},\n7{\n8 ” c o n t e n t ” : ” I ’m c u r r e n t l y c o n f i g u r i n g a w i r e l e s s a c c e s s p o i n t f o r our\n,→o f f i c e network and I need t o g e n e r a t e a JSON o b j e c t t h a t a c c u r a t e l y\n,→ r e p r e s e n t s i t s s e t t i n g s . The a c c e s s p o i n t ’ s SSID s h o u l d be ’\n,→O f f i c e N e t S e c u r e ’ , i t u s e s WPA2− E n t e r p r i s e as i t s s e c u r i t y p r o t o c o l ,\n,→ and i t ’ s c a p a b l e of a bandwidth of up t o 1300 Mbps on t h e 5 GHz\n,→band . T hi s JSON o b j e c t w i l l be used t o document our network\n,→c o n f i g u r a t i o n s and t o a u t o m a t e t h e s e t u p p r o c e s s f o r a d d i t i o n a l\n,→a c c e s s p o i n t s i n t h e f u t u r e . P l e a s e p r o v i d e a JSON o b j e c t t h a t\n,→i n c l u d e s t h e s e d e t a i l s . ” ,\n9 ” r o l e ” : ” u s e r ”\n10 }\n11]\n12Output :\n13{\n14 ” s s i d ” : ” O f f i c e N e t S e c u r e ” ,\n15\n--- Page 16 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\n15 ” s e c u r i t y P r o t o c o l ” : ”WPA2− E n t e r p r i s e ” ,\n16 ” bandwidth ” : ”1300 Mbps”\n17}\nE. Data Augmentation Workflow\nText Generation We use five models ( deepseek-chat ,gemini-2.0-flash-exp ,gemini-exp ,gpt-4 , and\ngpt-4o ) to generate text variations. Each model generates 25 variations, resulting in a total of 125 data points. The\nfollowing prompt is used for text generation:\n1G e n e r a t e 25 v a r i a t i o n s of t h e u s e r message . Follow t h e s e g u i d e l i n e s :\n21 . MUST i n c l u d e a l l r e q u i r e d f i e l d s and m a i n t a i n d a t a a c c u r a c y .\n32 . Vary s e n t e n c e s t r u c t u r e s u s i n g t h e s e t e c h n i q u e s :\n4 − Use d i f f e r e n t ve r b forms ( a c t i v e / p a s s i v e ) .\n5 − Apply p a r a p h r a s i n g w h i l e k e e p i n g t h e same meaning .\n6 − Change word o r d e r when p o s s i b l e .\n7 − Use synonyms f o r non −key t e r m s .\n83 . Vary t h e s e p r o p e r t i e s c r e a t i v e l y : {c h r ( 1 0 ) . j o i n ( f i e l d i n s t r u c t i o n s ) }.\n94 . Keep key t e r m i n o l o g y c o n s i s t e n t ( names / IDs / e t c ) .\n105 . Sound n a t u r a l and c o n v e r s a t i o n a l .\n116 . Answer i n E n g l i s h .\n127 . Output MUST be pu r e JSON onl y − no t e x t , comments , or markdown .\n13\n14R e q u i r e d JSON f o r m a t :\n15{\n16 ” c o n t e n t ” : [\n17 {\n18 ” c o n t e n t ” : ” Example message 1 . . . ” ,\n19 ” r o l e ” : ” u s e r ”\n20 },\n21 {\n22 ” c o n t e n t ” : ” Example message 2 . . . ” ,\n23 ” r o l e ” : ” u s e r ”\n24 }\n25 ]\n26}\n27\n28C r i t i c a l R e q u i r e m e n t s :\n29− F i n a l JSON MUST NOT be t r u n c a t e d .\n30− L a s t a r r a y i te m MUST end wit h }}]}} w i t h o u t a comma .\n31− Escape a l l d o u b l e q u o t e s i n s i d e c o n t e n t .\n32− Ensure a l l b r a c k e t s a r e p r o p e r l y c l o s e d .\n33\n34O r i g i n a l Message S t r u c t u r e R e f e r e n c e : {o r i g i n a l m e s s a g e }\nData Processing Thecontent field is extracted from the generated JSON data. Incomplete or malformed data points are\nremoved during initial filtering.\nQuality Evaluation We use o1 to evaluate the filtered data. The evaluation prompt is as follows:\n1You a r e an e x p e r t t e x t q u a l i t y e v a l u a t o r .\n2For each i n p u t t e x t , p r o v i d e a JSON o b j e c t wi th a ” r e s u l t s ” a r r a y c o n t a i n i n g\n,→e v a l u a t i o n o b j e c t s .\n3Each e v a l u a t i o n o b j e c t must c o n t a i n e x a c t l y f o u r f i e l d s :\n4− ” textNumber ” : The i n d e x of t h e t e x t ( s t a r t i n g from 1) .\n16\n--- Page 17 ---\nEarley-Driven Dynamic Pruning for Efficient Structured Decoding\n5− ” r e l e v a n c e ” : A r e l e v a n c e s c o r e between 0 and 100 ( based on schema keywords )\n,→.\n6− ” u n i q u e n e s s ” : A u n i q u e n e s s s c o r e between 0 and 100 ( based on s e n t e n c e\n,→s t r u c t u r e ) .\n7− ” c o h e r e n c e ” : A c o h e r e n c e s c o r e between 0 and 100 ( based on s e m a n t i c flow ) .\n8\n9Response f o r m a t :\n10{\n11 ” r e s u l t s ” : [\n12 {” textNumber ” : 1 , ” r e l e v a n c e ” : 94 , ” u n i q u e n e s s ” : 85 , ” c o h e r e n c e ” : 90 },\n13 {” textNumber ” : 2 , ” r e l e v a n c e ” : 95 , ” u n i q u e n e s s ” : 88 , ” c o h e r e n c e ” : 92 },\n14 {” textNumber ” : 3 , ” r e l e v a n c e ” : 93 , ” u n i q u e n e s s ” : 82 , ” c o h e r e n c e ” : 89 }\n15 ]\n16}\n17\n18I m p o r t a n t r u l e s :\n191 . Each t e x t must be e v a l u a t e d i n d i v i d u a l l y .\n202 . The ” textNumber ” must match t h e o r d e r of t h e i n p u t t e x t s ( s t a r t i n g from 1)\n,→.\n213 . S c o r e s must be i n t e g e r s between 0 and 1 0 0 .\n224 . Do n o t i n c l u d e any a d d i t i o n a l f i e l d s or comments i n t h e JSON r e s p o n s e .\n235 . Ensure t h e r e s p o n s e i s v a l i d JSON and can be p a r s e d d i r e c t l y .\nThe data is sorted based on evaluation scores (relevance, uniqueness, and coherence). For each schema, the top 100\nhighest-scoring data points are retained. The final filtered data is saved as a JSON file for downstream use.\n17",
  "text_length": 66341
}