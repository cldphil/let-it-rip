{
  "id": "http://arxiv.org/abs/2506.07324v1",
  "title": "DEF: Diffusion-augmented Ensemble Forecasting",
  "summary": "We present DEF (\\textbf{\\ul{D}}iffusion-augmented \\textbf{\\ul{E}}nsemble\n\\textbf{\\ul{F}}orecasting), a novel approach for generating initial condition\nperturbations. Modern approaches to initial condition perturbations are\nprimarily designed for numerical weather prediction (NWP) solvers, limiting\ntheir applicability in the rapidly growing field of machine learning for\nweather prediction. Consequently, stochastic models in this domain are often\ndeveloped on a case-by-case basis. We demonstrate that a simple conditional\ndiffusion model can (1) generate meaningful structured perturbations, (2) be\napplied iteratively, and (3) utilize a guidance term to intuitivey control the\nlevel of perturbation. This method enables the transformation of any\ndeterministic neural forecasting system into a stochastic one. With our\nstochastic extended systems, we show that the model accumulates less error over\nlong-term forecasts while producing meaningful forecast distributions. We\nvalidate our approach on the 5.625$^\\circ$ ERA5 reanalysis dataset, which\ncomprises atmospheric and surface variables over a discretized global grid,\nspanning from the 1960s to the present. On this dataset, our method\ndemonstrates improved predictive performance along with reasonable spread\nestimates.",
  "authors": [
    "David Millard",
    "Arielle Carr",
    "Stéphane Gaudreault",
    "Ali Baheri"
  ],
  "published": "2025-06-08T23:43:41Z",
  "updated": "2025-06-08T23:43:41Z",
  "categories": [
    "cs.LG",
    "physics.ao-ph",
    "35Q93 (Primary), 86A10, 65M75 (Secondary)",
    "I.2.6; I.6.3"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.07324v1",
  "comments": "26 pages, 20 plots, journal paper",
  "full_text": "--- Page 1 ---\narXiv:2506.07324v1  [cs.LG]  8 Jun 2025DEF: Diffusion-augmented Ensemble Forecasting\nDavid Millarda,∗, Arielle Carrb, St´ ephane Gaudreaultc, Ali Baheria\naKate Gleason College of Engineering, Rochester Institute of Technology, Rochester, USA\nbComputer Science & Engineering, Lehigh University, Bethlehem, USA\ncRecherche en pr´ evision num´ erique atmosph´ erique, Environnement et Changement climatique\nCanada, Dorval, Canada\nAbstract\nWe present DEF ( Diffusion-augmented Ensemble Forecasting), a novel approach for\ngenerating initial condition perturbations. Modern approaches to initial condition per-\nturbations are primarily designed for numerical weather prediction (NWP) solvers, lim-\niting their applicability in the rapidly growing field of machine learning for weather\nprediction. Consequently, stochastic models in this domain are often developed on a\ncase-by-case basis. We demonstrate that a simple conditional diffusion model can (1)\ngenerate meaningful structured perturbations, (2) be applied iteratively, and (3) utilize\na guidance term to intuitivey control the level of perturbation. This method enables\nthe transformation of any deterministic neural forecasting system into a stochastic one.\nWith our stochastic extended systems, we show that the model accumulates less error\nover long-term forecasts while producing meaningful forecast distributions. We validate\nour approach on the 5.625◦ERA5 reanalysis dataset, which comprises atmospheric and\nsurface variables over a discretized global grid, spanning from the 1960s to the present.\nOn this dataset, our method demonstrates improved predictive performance along with\nreasonable spread estimates.\nKeywords: ensemble forecasting, denoising diffusion probabilistic models, uncertainty\nquantification\n1. Introduction\nIn 2023, a machine learning model surpassed the best operational numerical weather\nprediction models. Pangu-Weather [1], a 3D Earth-Specific Transformer, demonstrated\nstate-of-the-art accuracy in the medium-range forecast (10–15 days). More remarkably,\nit achieved this in a fraction of the computational cost–producing forecasts in minutes\non a GPU, whereas traditional models require custom hardware processing petaflops of\noperations over the course of hours. Since then, the machine learning weather prediction\ncommunity has expanded, leading to the development of more advanced models such as\n∗Corresponding author\nEmail addresses: djm3622@rit.edu (David Millard), arg318@lehigh.edu (Arielle Carr),\nstephane.gaudreault@ec.gc.ca (St´ ephane Gaudreault), akbeme@rit.edu (Ali Baheri)\nPreprint submitted to UNSURE June 10, 2025\n--- Page 2 ---\nGraphCast [2], FourCastNet [3], and FuXi [4]. Despite their architectural differences,\nthey all share one trait: deterministic forecasting.\nTwo common approaches for introducing stochasticity to the trajectories of a forecast\nare ensemble methods and condition perturbation. Ensemble methods involve using mul-\ntiple models with the same input to generate different trajectories, achieved by varying\nmodel parameters. While powerful, these methods are extremely expensive, particu-\nlarly in machine learning, where training a model can take weeks to months. Condition\nperturbation, on the other hand, involves perturbing known conditions to generate plau-\nsible alternative states that the system could have experienced. These perturbations are\nboth effective and cost-efficient, as they account for the inherent uncertainty in observa-\ntions. However, the primary challenge lies in designing an effective perturbation strat-\negy. Many modern condition perturbation techniques employ data assimilation methods,\nwhich leverage known observations and error estimates to generate conditions that theo-\nretically maximize exploration of the forecasting space. While theoretically compelling,\nthese methods require both a significant history of observational data and a large number\nof ensemble members to fully explore the space.\nThe machine learning weather prediction (MLWP) community has addressed the chal-\nlenge of ensemble forecasting on a case-by-case basis. Following the success of GraphCast,\nGenCast [5] was introduced–the same GraphCast model trained as a conditional diffu-\nsion model. FourCastNet demonstrated a stochastic approach by employing an ensemble\nof deterministic forecasts. Other techniques, such as Monte Carlo dropout, conformal\nprediction, and Bayesian neural networks, offer a more general framework for uncertainty\nquantification within deep learning but face several practical limitations when applied to\nthe sensitivity and complexities of modern MLWP architectures. This discrepancy arises\nbecause, unlike many other fields, neural forecasting systems are trained to be highly\nsensitive to their inputs.\nIn this study, we present an approach that successfully incorporates all three aspects:\nlow complexity, fast speed, and high transferability. Our method is modular in design,\nleverages the well-studied and widely understood conditional diffusion methodology, and,\nafter applying various speedups from modern diffusion literature, can generate hundreds\nof perturbations within minutes. Utilizing this technique, any deterministic model can\neffectively generate a large ensemble of members without requiring modifications to infer-\nence or training, aside from simply perturbing the initial conditions and autoregressing\nupon them.\nOur Contributions. The contributions of this paper are threefold:\n•We develop a novel method for condition perturbation by performing conditional\ngeneration on the input conditions themselves. This method requires significantly\nless computational power, as the model generates within its own state rather than\npredicting the next state.\n•We demonstrate that the method can be applied iteratively to perform ”random\nwalks” within a condition, offering more exploration power and better distribution\nestimation of the extremely large trajectory space.\n•We show that the guidance term, the scalar parameter ωused within classifier free\nguidance, intuitively controls the strength of the perturbation, directly regulating\n2\n--- Page 3 ---\nthe level of uncertainty quantification and enabling the controlled exploration of\nthe state space by informed users.\nPaper Organization. The remainder of this paper is structured as follows. Section 2 is\na literature review covering recent techniques used for uncertainty quantification. Section\n3 establishes foundational concepts in ensemble forecasting, neural forecasting systems,\nand denoising diffusion probabilistic models. Section 4 presents the methodology, de-\ntailing our dataset, models, and training/inference procedures. Section 5 presents our\nnumerical experiments/results, Section 6 discusses key findings, and Section 7 outlines\nfuture work and directions enabled by this study.\n2. Related Work\nModel ensembling is the most popular method for ensemble forecasting. Within the\ncontext of deep learning, the method relies on training multiple deterministic members\nand using their distribution of predictions. Due to the randomness of training deep\nlearning methods, differing random seeds produce slightly different predictive models,\nand due to the sensitivity of forecasting models, they can lead to diverging results in the\nlong term. FourCastNet [3] proposed the use of 1,000 deterministic members to generate\nan effective ensemble that was able to compete with the ECMWF [6], which has 30-50\nmembers due to the significant computational demand of numerical weather prediction\n(NWP) [7] schemes. Modern perspectives are now considering huge ensembles of 1,000\nto 10,000 members [8] utilizing spherical Fourier neural operators [9], an extension of\nFourCastNet that operates on the spherical coordinates of Earth rather than grid in-\nterpolations. While these approaches are very promising, they all suffer from a lack of\nprobabilistic formulation and typically require a significant number of models to ensure\nthe empirical distribution is large enough to effectively capture the uncertainty of the\ndynamics, resulting in an ensemble underdispersion.\nOne established machine learning architecture for generating probabilistic forecasts\nis Bayesian neural networks [10]. These networks modify the conventional neural net-\nwork framework by learning distributions of weights rather than point estimates. This\nmodification fundamentally alters the architecture and training procedures but demon-\nstrates significant advantages when modeling inherently uncertain data. Recent research\nhas specifically examined the application of this approach for predicting North Atlantic\nsea surface temperature using a convolutional LSTM architecture [11]. The investiga-\ntors demonstrated that through their probabilistic formulation, the direct point estimate\nof the prediction exhibited superior performance on average compared to an equivalent\nmodel with standard deep learning weight parameterization. Furthermore, the study\npresents a quantitative comparison between a conventional deep learning ensemble and\ntheir SVGD ensemble derived from the Bayesian convolutional LSTM. Their analy-\nsis reveals that the SVGD methodology mitigates the ensemble collapse phenomenon\nfrequently observed in standard deep learning ensembles, wherein ensemble members\nproduce nearly identical predictions with minimal variance. Although Bayesian neural\nnetworks represent a promising approach for ensemble forecasting, they face significant\nscalability limitations. Despite advances in addressing these constraints [12], current\nmethodologies remain insufficient to scale to the computational requirements necessary\nfor global neural ensemble forecasting applications.\n3\n--- Page 4 ---\nInitial condition perturbation is one of the most extensively studied methods in en-\nsemble forecasting within the weather prediction community. The simplest approach is\nnoise addition, which lacks a physical interpretation and is instead based solely on the\nuncertainty of signal readings. Another method, time lagging [13], leverages a model’s\npast forecasts as conditions for the current system. As a model produces multistep\nforecasts further into the future, it explores one trajectory that deviates from the true\ntrajectory. Once ground truth conditions are observed, previous forecasts can be incor-\nporated into the ensemble. This method is computationally efficient, as it reuses prior\nforecasts without additional computation. However, it performs poorly [14] since it relies\non point estimates that accumulate error over time. Additionally, time lagging is highly\ndependent on the model itself–when the model underperforms, this method does not aid\nin learning a true distribution. Bred vectors [15], a data assimilation technique, offer a\ndynamic alternative by leveraging the natural instabilities of the system. The breeding\nmethod perturbs an initial state, integrates it forward using the full nonlinear model,\nand periodically rescales the perturbations to maintain a fixed amplitude. This iterative\nprocess ensures that the perturbations align with the dominant growing modes of the\nsystem. A related technique, singular vectors [15], perturbs the singular values of the\nsystem to ensure that perturbations occur along the dominant modes. It achieves this\nby computing the singular value decomposition of the tangent linear model of the full\nnonlinear system, perturbing the singular values, and transforming back. While data\nassimilation methods such as these represent the state of the art, they suffer from two\ndistinct limitations: they require a long history of predictions, observations, and errors,\nand they are designed for NWP models. Within the MLWP setting, we lack a direct\nmethod for time integration or obtaining tangent linear models. As a result, applying\nthese techniques would require substantial reformulation and generous assumptions.\nGiven the explosion of generative AI and its long track record of success, variational\nautoencoders (VAEs) [16] and denoising diffusion probabilistic models (DDPMs) [17] have\nincreasingly been applied to tasks within the MLWP community, such as extreme ERA5\ncompression [18] and upscaling [19]. A particularly notable and influential work in this\ndirection is GenCast [5], which serves as a significant inspiration for this study. GenCast\nis a conditional diffusion model trained for stochastic next-state prediction. Architec-\nturally, it is identical to GraphCast [2] but differs in its training objective. Specifically,\nGenCast conditions the graph-based model on the previous state, formally defined as\nGθ(xt|xt−1). Given its formulation under DDPMs, the authors have found GenCast to\nbe among the most accurate forecasting models, surpassing even ECMWF ensembles and\nAFNO-based large ensembles. Notably, because GenCast modifies the training objective\nrather than the architecture, it allows for seamless integration into existing forecasting\npipelines, making it highly adaptable for further experimentation.\n3. Background\n3.1. Ensemble Forecasts\nGiven an model M, an ensemble forecast consists of a set of predictions {Xi(T)}N\ni=1,\nwhere each Xi(T) represents the forecasted state at time Tfrom an initial condition\n4\n--- Page 5 ---\nXi(0). The ensemble mean, which provides an averaged forecast, is given by:\n¯X(T) =1\nNNX\ni=1Xi(T), (1)\nwhere Tis the current timestep and Nis the number of members. The ensemble spread,\nrepresenting forecast uncertainty, is defined as:\nS(T) =vuut1\nNNX\ni=1\u0000\nXi(T)−¯X(T)\u00012. (2)\nThe spread-skill relationship suggests that ensemble spread S(T) should correlate with\nforecast error such that S(T)∝ ∥¯X(T)−X(T)∥2, providing a measure of reliability in\nprobabilistic forecasts.\nGiven the relationship S(T)∝ ∥¯X(T)−X(T)∥2, we observe an inverse problem\nwhere maximizing the forecast spread leads to a larger forecast error. As a result, it is\ncommon practice to aim for S(T)≈ ∥¯X(T)−X(T)∥2, providing a clearer goal. This\nobservation can be adjusted to accommodate specific needs, but we consider it an ideal\nbenchmark in the results section of our work. This metric is prominently reflected in\ntwo key evaluation criteria: the Continuous Ranked Probability Score (CRPS) [20] and\nenergy scoring [21], both of which directly quantify forecast spread and skill. Given\nthese methods for evaluating ensemble forecast performance, there exist two primary\napproaches for constructing ensembles: condition-based and model-based methods.\n3.2. Condition Perturbations\nCondition perturbations are a natural method for forecasting. This type of ensemble\nleverages the uncertainty in observations. Although we typically refer to our observations\nas ”ground truth,” the process of collecting these observations is imperfect and can\nintroduce instabilities. Given the sensitivity of a model M, we assume an observation\nX(T) is drawn from a distribution of possible observations:\nX(T)∼ P(X|M,O), (3)\nwhere P(X|M,O) represents the conditional distribution of observations given the model\nMand the underlying observation process O, which accounts for measurement noise and\nuncertainty. We can then introduce perturbations as follows:\nXi(T)∼F(X(T)), i= 1, . . . , N, (4)\nwhere F(X(T)) represents a perturbation function that generates ensemble members\nby introducing controlled variations to the initial conditions. These perturbed states\n{Xi(T)}N\ni=1serve as the initial conditions for several ensemble members of a forecasting\nmodel Gθ. Given that our model Gθapproximates the underlying dynamics, we assume\nX(T+ 1)≈Gθ(X(T)), (5)\n5\n--- Page 6 ---\nwhere Gθis expected to produce stable forecasts under near-zero perturbations and Fis\nthe actual dynamics. However, if the initial conditions exhibit small but non-negligible\ndifferences, i.e.,\nXi(T)̸≈Xj(T),∀i̸=j, (6)\nthese perturbations can amplify over time, leading to significant divergence in forecast\ntrajectories across ensemble members. This divergence enables a robust estimation of\nboth the ensemble mean ¯X(T) and the forecast spread S(T), effectively quantifying the\ninherent uncertainty in the system.\n3.3. Neural-based Forecasting\nNeural-based forecasting methods utilize large deep learning models trained on his-\ntorical data to predict atmospheric states. Unlike conventional Numerical Weather Pre-\ndiction (NWP) models, which numerically solve partial differential equations (PDEs),\nneural models learn a mapping between the current state and the forecasted state at the\nnext time step:\nGϕ:Xt→Xt+∆t, (7)\nwhere Gθis a neural network parameterized by θ, and Xtrepresents the atmospheric state\nat time t. These models typically use a regressive loss function as their objective, such\nas mean absolute error (MAE), mean squared error (MSE), or root mean squared error\n(RMSE). However, they can also incorporate physical constraints via Physics-Informed\nNeural Networks (PINNs) [22]. In this approach, the loss function is enhanced with a\nphysics-based term that enforces consistency with known governing equations:\nL=Ldata+λLphysics , (8)\nwhere Ldatais the standard regression loss, Lphysics penalizes violations of the governing\nequations, and λis a hyperparameter that balances the two objectives.\nCurrently, state-of-the-art neural forecasting systems include GraphCast, FourCast-\nNet, and Pangu-Weather. GraphCast is a graph neural network (GNN)-based model [23]\nthat represents weather fields as a graph G= (V, E), where Vare spatial nodes and Eare\nedges encoding dependencies between nodes. Forecasts are computed through message\npassing:\nh(l+1)\nv =σ\nW(l)h(l)\nv+X\nu∈N(v)W(l)h(l)\nu\n, (9)\nwhere h(l)\nvis the hidden representation of node vat layer l, andN(v) denotes the neigh-\nbors of node v. FourCastNet employs Adaptive Fourier Neural Operators (AFNOs) [24],\nwhich perform global convolutions in the spectral domain:\nhi=F−1(σ(WF(Xt))), (10)\nwhere Fis the Fourier transform and Wis a learned weight matrix. Pangu-Weather\nutilizes a Transformer-based [25] architecture that models atmospheric dynamics via\nself-attention:\nZ= softmax\u0012QKT\n√\nd\u0013\nB, (11)\n6\n--- Page 7 ---\nwhere Q, K, B are the query, key, and value matrices, respectively, and dis the feature\ndimension. In this paper, we explore a simpler yet powerful approach using an attention\nU-Net model [26]. Attention U-Net augments the standard U-Net architecture [27] by\nincorporating an attention mechanism, as described in Equation 11, over the entire spatial\ndomain. To further enhance efficiency within our computational constraints, we limit the\nattention mechanism to the lower two downsampled spaces. This allows us to maintain\na balance between performance and computational cost while preserving the benefits of\nthe attention mechanism.\n3.4. Denoising Diffusion Probabilistic Models\nDenoising Diffusion Probabilistic Models (DDPMs) represent a class of generative\nmodels that learn to reverse a gradual noising process. The forward diffusion process\ntransforms data samples into pure noise through a fixed Markov chain of Tsteps:\nq(xt|xt−1) =N(xt;p\n1−βtxt−1, βtI), (12)\nwhere {xt}T\nt=1represents the sequence of noisy states, βt∈(0,1) is a variance schedule,\nandx0is the original data sample. This process can be expressed in closed form for any\ntimestep t:\nq(xt|x0) =N(xt;√¯αtx0,(1−¯αt)I), (13)\nwhere αt= 1−βtand ¯αt=Qt\ns=1αs. The generative process reverses this diffusion\nthrough a learned denoising model ϵθthat predicts the noise component at each step:\npθ(xt−1|xt) =N(xt−1;µθ(xt, t), σ2\ntI), (14)\nwhere µθ(xt, t) =1√αt\u0010\nxt−βt√1−¯αtϵθ(xt, t)\u0011\n. The model is then reparameterized and\ntrained to minimize the simplified objective:\nL=Et∼[1,T],x0∼q(x0),ϵ∼N(0,I)\u0002\n∥ϵ−ϵθ(xt, t)∥2\n2\u0003\n, (15)\nwhich effectively trains the model to predict the noise that was added at each timestep.\nWe can now alter the formulation once more to enable for the creation of conditional\ndiffusion models that generate samples conditioned on input states. Specifically, we im-\nplement classifier-free guidance [28], which provides a more stable training framework,\nbetter results, and general usage compared to the traditional classifier-guided [29] ap-\nproaches. In classifier-free guidance, the model is trained with two objectives:\nLCFG=λE\u0002\n∥ϵ−ϵθ(xt, t, c)∥2\n2\u0003\n+ (1−λ)E\u0002\n∥ϵ−ϵθ(xt, t,∅)∥2\n2\u0003\n, (16)\nwhere crepresents the conditioning information, ∅represents an unconditional sample,\nandλ∼Bernoulli( p). During inference, we interpolate between conditional and uncon-\nditional predictions:\nˆϵθ(xt, t, c) =ϵθ(xt, t,∅) +w·(ϵθ(xt, t, c)−ϵθ(xt, t,∅)), (17)\nwhere wis a guidance scale that controls how strictly the generation adheres to the\nconditioning. By adjusting w, we can balance between diversity (lower w) and fidelity\n(higher w) in our ensemble forecasts.\n7\n--- Page 8 ---\nAdditionally, DDPMs discrete formulation can also be reformulated as a continu-\nous process stochastic differential equation (SDE). Under this formulation we can make\nuse ODE-based solvers like DPM-Solver [30] and DPM-Solver++ [31] to accelerate our\nsampling process. The DPM-Solver++ formulation uses a semi-linear ODE:\ndx\ndt=f1(t)x+f2(t)sθ(x, t), (18)\nwhere sθ(x, t) is the learned score function, and f1(t) and f2(t) are time-dependent\ncoefficients derived from the noise schedule. In this approach, we can use efficient high-\norder numerical methods such as the Runge-Kutta schemes [32] to acheieve high-quality\nsamples with as few as T′= 10 to 20 function evaluations.\n4. Methodology\n4.1. ERA5 Reanalysis Dataset\nERA5 is the fifth generation atmospheric reanalysis dataset produced by the Euro-\npean Centre for Medium-Range Weather Forecasts (ECMWF), providing a comprehen-\nsive record of the global climate from 1950 to the present. The dataset is generated\nusing the ECMWF’s Integrated Forecast System (IFS) CY41R2, which assimilates ob-\nservational data from satellites, weather stations, buoys, and other sources. The IFS\nmodel operates at a spatial resolution of approximately 31 km horizontally with 137 ver-\ntical levels, extending from the surface up to 0.01 hPa (approximately 80 km), enabling\nERA5 to resolve atmospheric phenomena across multiple scales from global circulation\npatterns to mesoscale features. The temporal resolution of ERA5 is hourly, although\nfor this study we utilize data at six-hour intervals. ERA5 employs 4D-Var data assimi-\nlation techniques, which minimize a cost function measuring the difference between the\nmodel state and observations over a 12-hour window. This approach combines obser-\nvations with short-range forecasts to produce a dynamically consistent estimate of the\natmospheric state. For our experiments, we utilize a specific subset of ERA5 variables\nat selected pressure levels as detailed in Table 1. The atmospheric variables are sampled\nacross 13 pressure levels from 1000 hPa to 50 hPa, capturing the vertical structure from\nnear-surface conditions through the troposphere and into the lower stratosphere. The\nselected variables include both atmospheric parameters (geopotential, wind components,\nspecific humidity, and temperature) and surface parameters (near-surface winds, 2-meter\ntemperature, pressure measurements, water content, and precipitation).\nWe structure our data as a tensor xt∈Rv×h×w, where vdenotes the number of\nvariables and h, w represent the spatial dimensions. Beyond the standard atmospheric\nand surface variables from ERA5, we augment our input with 11 additional forcing\nvariables and constants (detailed in Table 2). These supplementary variables provide\ncontext while remaining state-independent, as they can be deterministically computed\nfor any spatiotemporal coordinate. By concatenating these forcing variables along the\nfirst dimension, we obtain an expanded input tensor of size [ v+f, h×w], where f= 11\nrepresents the number of auxiliary variables. It is important to note the asymmetry in our\nprocessing pipeline: while the neural forecasting model Gθconsumes the full augmented\ninput [ v+f, h, w ], it generates predictions only for the ERA5 variables, yielding outputs\nof size [ v, h, w ]. Contrary to this, the diffusion-based condition perturbation model ϵθ\n8\n--- Page 9 ---\nCategory Variable Description\nAtmospheric Geopotential Height of pressure level (m2s−2)\nWind components Zonal, meridional, and vertical wind (m s−1)\nSpecific humidity Mass of water vapor per unit mass of air (kg\nkg−1)\nTemperature Air temperature at pressure level (K)\nSurface 10m wind (u, v) Near-surface wind components (m s−1)\n2m temperature Near-surface air temperature (K)\nMean sea level pressure Pressure reduced to sea level (hPa)\nSurface pressure Pressure at surface (hPa)\nTotal column water Integrated water vapor in atmospheric col-\numn (kg m−2)\nTotal precipitation Accumulated precipitation over 6 hours (m)\nTable 1: ERA5 variables used in this study\noperates exclusively on the ERA5 variables of size [ v, h, w ], as the forcing variables remain\nconstant and deterministic throughout the prediction process.\nCategory Variables Description\nTemporal sin(time of day) Diurnal cycle representation\nForcings cos(time of day) using circular encoding\nsin(year progress) Annual cycle representation\ncos(year progress) using circular encoding\nEnergy TOA radiation Top-of-atmosphere radiation flux\nInput integrated over preceding hour\nGeographical Latitude Normalized spatial coordinates\nInformation Longitude and surface characteristics\nLand-sea mask Binary indicator (1 = land, 0 = ocean)\nTopography Surface elevation data\nLeaf area index Vegetation density measure\nAlbedo Surface reflectivity coefficient\nTable 2: Forcing variables and constants incorporated into the input data tensor.\nPrior to feeding our data through either the neural forecasting model Gθor the\ndiffusion-based condition perturbation model ϵθ, we implement a pre-processing normal-\nization procedure for each variable, across its spatial dimensions. Specifically, we com-\npute and store the mean µvand standard deviation σvfor each variable. These statistics\nare subsequently used to denormalize the outputs after model processing. While this\nnormalization approach inevitably discards some distributional information, it serves a\ncritical purpose: preventing the learning process from being disproportionately influ-\nenced by variables with larger magnitudes or wider ranges. This ensures that the model\nweights reflect the relative importance of physical relationships rather than numerical\n9\n--- Page 10 ---\nscale differences. The normalization procedure is provided in Appendix C.\nFigure 1: Snapshots of meteorological variables from deterministic forecast: (a) 2m temperature, (b)\ngeopotential height at 850 hPa, (c) specific humidity at 850 hPa, and (d) 6-hour total precipitation.\n4.2. Models\nThe forecasting model Gϕpredicts the next state xt+1given the current state xtand is\ntrained using the mean squared error (MSE) objective. The diffusion-based perturbation\nmodel ϵθfunctions as a stochastic reconstruction mechanism: conditioned on xtor ˆxt, it\ngenerates a sample from the full diffusion process of xtor ˆxt. While our framework does\nnot impose architectural constraints, we adopt an attention-based U-Net for Gϕdue to\nits ability to efficiently capture both local and global dependencies. The network consists\nof four downsampling and four upsampling blocks, incorporating wide ResNet layers and\nspatial attention layers at resolutions 32 ×16, 16 ×8, and 8 ×4. The perturbation model\nϵθemploys the same architecture but includes necessary time embedding layers at each\nstage, as well as an extra attention layer at the 64 ×32 resolution. We provide complete\nalgorithms for both training processes in Appendix C.\nDuring inference, we combine the two models to obtain a distribution of ensemble\nmembers given a ground truth observation. Consider an initial ground truth observation\nx0. In the deterministic case, to forecast for Ntimesteps, we autoregressively apply the\n10\n--- Page 11 ---\nforecasting model, denoted by:\nxt+1=Gϕ(xt),∀t∈ {0, . . . , N −1}. (19)\nThis formulation yields a single trajectory from the ground truth. With access to the\nperturbation model, we introduce a stochastic component before each autoregression step\nto perturb each state. To obtain an ensemble of trajectories, we generate Bperturbed\nversions of the initial observation and propagate them forward in parallel:\nx(b)\nt+1=Gϕ(˜x(b)\nt),˜x(b)\nt=ϵθ(x(b)\nt),∀b∈ {1, . . . , B }. (20)\nHere, ˜ x(b)\ntrepresents the perturbed state obtained from the diffusion model, and x(b)\nt+1\nrepresents the corresponding forecasted state for each ensemble member. This procedure\nallows us to directly control exploration through guidance term of the conditional diffu-\nsion model. Additionally, it enables random walks via an iterative process of applying:\n˜x(b)\nt,k=ϵθ(x(b)\nt,k−1),∀k∈ {1, . . . , K }, (21)\nwhere Kdenotes the number of perturbation steps taken per state. This iterative appli-\ncation of ϵθeffectively simulates multi-step stochastic transitions, rivaling the additional\nexploration of the guidance term.\n4.3. Evaluation Metrics\nThis study uses four quantitative evaluation metrics: Continuous Ranked Probability\nScore (CRPS), Energy Score, Root Mean Squared Error (RMSE), and Spread Correlation\n(SC). We also consider qualitative differences, as quantifying fidelity across 85 variables\nis too challenging for metrics such as SSIM and PNR. Among these, RMSE is the sole\nmetric that allows comparison between stochastic and deterministic predictions. We\nformally define CRPS as:\nCRPS( x,{x(b)}B\nb=1) =1\nBBX\nb=1|x(b)−x| −1\n2B2BX\nb=1BX\nb′=1|x(b)−x(b′)|, (22)\nwhere xis the observed ground truth value, and {x(b)}B\nb=1represents the ensemble of\npredictions. CRPS generalizes the mean absolute error (MAE) to probabilistic forecasts,\nmeasuring both accuracy and sharpness. Lower CRPS values indicate better probabilistic\ncalibration. The Energy Score evaluates ensemble forecasts similarly to CRPS and is\ndefined as:\nES({x(b)}B\nb=1, x) =1\nBBX\nb=1∥x(b)−x∥2−1\n2B2BX\nb=1BX\nb′=1∥x(b)−x(b′)∥2, (23)\nwhere {x(b)}B\nb=1represents an ensemble of predictions, and xis the observed ground\ntruth. Compared to CRPS, the Energy Score uses the Euclidean norm ∥ · ∥ 2instead of\nthe Manhattan norm. A lower Energy Score implies better sharpness and accuracy of\nthe ensemble. RMSE measures the pointwise error of single forecasts and is defined as:\nRMSE( x,ˆx) =vuut1\nnnX\ni=1(xi−ˆxi)2, (24)\n11\n--- Page 12 ---\nFigure 2: Evaluation of diffusion-based ensemble forecasts showing mean prediction, ground truth,\nforecast error, and ensemble spread for: (a) 2-meter temperature, (b) 850 hPa geopotential height, (c)\n850 hPa specific humidity, and (d) 6-hour precipitation.\nwhere xirepresents the ground truth values, and ˆ xirepresents the predicted values.\nMetrics such as RMSE, MAE, and MSE are widely used for measuring prediction error,\nas they provide a single scalar value to quantify discrepancies in high-dimensional spaces.\nSC measures the distance between the error and pointwise standard deviation and is\ndefined as:\nSC({x(b)}B\nb=1, x,ˆx) =\r\r\r\r\r\r(ˆx−x)−vuut1\nBBX\nb=1\u0000\nx(b)−¯x\u00012\r\r\r\r\r\r\n2(25)\nwhere ¯ xis the ensemble mean and x(b)are ensemble members. SC provides a quantitative\nmeasure of how well the ensemble represents the actual error. It compares the error of the\n12\n--- Page 13 ---\nFigure 3: Comparison of ensemble forecast error metrics: individual ensemble members’ RMSE versus\nground truth (gray lines), mean RMSE across all ensemble members (green line), and RMSE of the\nensemble mean prediction versus ground truth (blue line) for: (a) 2-meter temperature, (b) 850 hPa\ngeopotential height, (c) 850 hPa specific humidity, and (d) 6-hour accumulated precipitation.\nmean ensemble prediction to the spread (or variability) within the ensemble members.\nA lower SC value (relative to the units) indicates that the ensemble accurately reflects\nthe error and that the spread of the ensemble matches the true variability of the system,\nleading to a more reliable uncertainty estimate.\n4.4. Experimental Setup\nIn this study, we assume access to limited computational resources. Our training is\ndistributed across two NVIDIA RTX A5000 GPUs, each equipped with 24 GB of VRAM.\nGiven these constraints, we opted for both the lower-resolution dataset and a more\ncomputationally efficient model. The deterministic model, Gϕ, was trained for 80 epochs\nwith a batch size of 64, while the diffusion model, ϵθ, was trained for 300 epochs with\na batch size of 32. Both models reached log-scale convergence. Inference was conducted\nwithin the same computational environment. We did not perform hyperparameter tuning\nand instead relied on the default settings of each package. In total, training took roughly\n2 weeks, but could easily be accelerated and scaled with better hardware.\n13\n--- Page 14 ---\nLead Time: 60h Lead Time: 120h\nEnergy CRPS RMSE Spread Energy CRPS RMSE Spread\nDeterministic - - 3.55 - - - 5.28 -\nDiffusion[0.3, 1] 2.66 1.80 5.02 129.17 2.84 1.97 5.30 127.65\nDiffusion[0.5, 1] 1.89 1.49 2.88 86.88 1.62 1.29 2.99 84.78\nDiffusion[0.7, 1] 2.58 1.97 3.04 114.75 2.73 2.00 3.53 125.70\nDiffusion[1.0, 1] 3.42 2.57 3.58 153.99 5.09 3.55 5.37 229.83\nLead Time: 180h Lead Time: 234h\nEnergy CRPS RMSE Spread Energy CRPS RMSE Spread\nDeterministic - - 7.31 - - - 8.12 -\nDiffusion[0.3, 1] 2.92 2.21 5.64 139.36 4.03 2.93 6.49 157.76\nDiffusion[0.5, 1] 2.28 1.51 3.78 114.75 2.42 1.61 4.03 118.24\nDiffusion[0.7, 1] 3.32 2.17 4.52 152.88 3.12 1.97 4.56 148.90\nDiffusion[1.0, 1] 6.98 4.71 7.50 315.45 8.44 5.73 9.29 379.15\nTable 3: 2M Temperature forecast metrics across different lead times.\n5. Results\nIn this section, we discuss three primary findings. First, we find that our diffusion-\nperturbation model produces valid ensembles that correlate well with the error of the\nmean ensemble prediction. Next, we explore the quantitative improvements over the\ndeterministic prediction using the mean ensemble, along with other probabilistic met-\nrics. Following this, we interpret the results of our two exploratory extensions: iterative\nwalks and guidance terms. Finally, we observe that our method sometimes acts as a\npredictor-corrector, leading to individual ensemble members outperforming the deter-\nministic prediction in certain situations. We do evaluation with the same start time of\nJanuary 12th, 12 AM until January 22th, 12 AM, a 240 hour lead time.\nWe present quantitative results for six variables: 2M Temperature, Geopotential\nH850, Specific Humidity H850, Total Precipitation, Wind X 10M, and Wind Y 10M.\nThis specific subset was chosen as it best highlights the strengths and limitations of\nour approach. Analyzing the results from Table 3, we observe that the ensemble mean\nRMSE significantly outperforms the deterministic RMSE for the 2M Temperature vari-\nable. This finding is further supported by Figure 3, which illustrates the magnitude of\ndifferences between the deterministic trajectories and the ensemble. While this analysis\nfocuses on a single variable, an examination of additional variables, as presented in the\ntables in Appendix B, reveals a significant improvement in specific humidity. For other\nvariables, such as geopotential, precipitation, and wind speed, the improvements are\nless pronounced but still evident. We find that most variables benefit from a guidance\nterm of 0.5 with no iterative updates, whereas variables like geopotential achieve better\nperformance with a higher guidance term of 0.7 and reduced exploration.\nExamining the probabilistic metrics in Table 3, we observe that the estimates for\nEnergy and CRPS are reasonable. Specifically, these values fall below the mean RMSE,\nindicating that the ensemble members are sufficiently diverse to capture variations. This\nobservation is further reinforced by the spread correlation, which exhibits a relatively low\nvalue, confirming the ensemble’s ability to balance accuracy and diversity. These same\nresults can be found within Appendix B.\nA qualitative analysis of the results provides deeper insight into the method’s be-\nhavior. First, examining the deterministic predictions in Figure 1, we identify two key\n14\n--- Page 15 ---\nFigure 4: Visualization of 16 randomly selected ensemble members at 200-hour lead time for: (a) 2-meter\ntemperature, (b) 850 hPa geopotential height, (c) 850 hPa specific humidity, and (d) 6-hour accumulated\nprecipitation.\nissues: out-of-bounds values and degenerate long-term trajectories. Specifically, for spe-\ncific humidity, the method produces negative values over extended time horizons, which\nfall outside the valid domain. A similar issue arises in the 2M temperature plot, where\ntemperatures near the equator deviate beyond plausible ranges. Comparing the errors\nobserved in Figure 1 with the error plots generated by the ensemble in Figure 2, we\nobserve significantly lower magnitudes. Additionally, the ensemble-based predictions\nremain within physically plausible bounds. However, one limitation of the ensemble ap-\nproach is oversmoothing when using the mean of the distribution as the state estimate.\nThis effect is particularly evident in the precipitation and specific humidity variables in\nFigure 2, where much of the finer detail is lost, leading to an overly smooth estimate.\nFigure 4 further illustrates this by displaying 16 randomly selected ensemble members\n15\n--- Page 16 ---\nat the 200-hour lead time. As shown, individual members retain sharper features, but\ntheir trajectories diverge significantly. As a result, the ensemble mean fails to capture\nfine-scale structures due to slight overdispersion. While overdispersion is not inherently\nproblematic, it is worth noting in this context. Nonetheless, some finer-scale features,\nsuch as the recurring daily drop at the southwestern tip of Africa and extreme heat across\nAustralia remain present, clearly depicted within Figure 2.\nComparing the deterministic predictions with each ensemble member in Figure 3, we\nobserve a slight discrepancy–the deterministic-only method should be contained within\nthe ensemble distribution. However, we find that the deterministic predictions perform\nsignificantly worse when evaluated on certain variables, such as specific humidity and\ntemperature. From our previous observations, we noted that the deterministic method\nproduces out-of-domain values. Based on this, we state that an additional benefit of\nusing a diffusion model as a perturbation method is that it also acts as a predictor-\ncorrector, projecting predictions onto a manifold learned from the data distribution.\nAdditionally, we demonstrate the conservation of energy in Figure 5. This figure shows\nthat each distribution encompasses the ground truth energy, indicating that while our\nmethod may introduce oversmoothing and error accumulation, it effectively preserves the\nsystem’s energy characteristics.\nFigure 5: Domain-averaged values of ensemble forecast variables: individual ensemble members (gray\nlines), ensemble mean prediction (blue line), and ground truth observation (red line) for: (a) 2-meter\ntemperature, (b) 850 hPa geopotential height, (c) 850 hPa specific humidity, and (d) 6-hour accumulated\nprecipitation.\n16\n--- Page 17 ---\n6. Discussion\nWe find that our approach produces both superior point estimates and well-calibrated\nuncertainty. Specifically, our method achieves point estimates with a RMSE that is up\nto 50% lower on the high end and approximately 10% lower on the low end, relative\nto deterministic method, after standardizing to account for the domain’s distribution of\nvalues. Additionally, our evaluation using the Energy Score and CRPS demonstrates that\nour method generates uncertainty estimates (spreads) that accurately reflect the broad\nvariability in trajectory space, while remaining concentrated around a central, physically\nplausible direction.\nBy incorporating a DDPM-based perturbation mechanism into a strong/weak time\nadvancement scheme, we achieve an effective, parallelizable, and, most importantly, in-\ntuitive ensemble forecasting approach. Qualitatively, we observe that the resulting dis-\ntribution captures accumulated error well enough to provide a rough estimate of regions\nwith higher uncertainty. While our experiments produce smooth long-term forecasts,\nthese could be further improved by integrating a stronger deterministic model. Notably,\nthe diffusion model itself does not need to be highly accurate, as its role is straightforward\nyet non-trivial. An additional feature of this framework is that it enables transforming\nany machine learning-based deterministic integrator into a stochastic system that pro-\nvides both pointwise forecasts and uncertainty estimates over the trajectory distribution.\nWe demonstrate that even our weak deterministic predictor, when combined with the\ndiffusion-based perturbation model, conserves energy and adheres to valid domain con-\nstraints—unlike the purely deterministic model alone. Our extension acts as a predictor-\ncorrector, enabling nonlinear projection of predictions onto a learned manifold derived\nfrom the training distribution. This improves upon prior methods that approximate such\nprojections using spatial pointwise clipping or dimensionality reduction techniques. We\nconsider this approach more robust, as it leverages the parameterized latent distributions\nto capture both high- and low-level structures. An added benefit of these distributions\nis their ability to represent multi-resolution patterns that many modern methods fail to\ncapture.\nLimitations. While the proposed framework is evaluated using a weak deterministic\nintegrator, its results are not directly comparable to current state-of-the-art methods.\nDue to limited computational resources, we do not make state-of-the-art claims and\ntherefore omit comparisons against benchmarks such as WeatherBench. Our claim of\ngeneralizability is based on architectural design and has not been empirically validated.\nWe also acknowledge the computational demands and slower inference speeds inherent\nto diffusion-based models. Although we mitigate this by employing the DPM++ solver,\nwhich improves inference times by approximately two orders of magnitude, this comes\nat the cost of losing fine-grained information due to integrating over latent variables in\nthe Markov chain. Lastly, while the framework is designed to be generalizable, real-\nworld systems often involve different data transformations and subsets. For example,\noperational ERA5 includes fewer variables than the ERA5 Reanalysis dataset. To ensure\nbroader applicability, a family of models would likely need to be trained across various\nsubsets, and researchers may need to apply custom post-processing steps to adapt the\nmodel outputs to their specific operational settings.\n17\n--- Page 18 ---\n7. Conclusion and Future Work\nIn this paper, we present an approach for performing condition perturbation using a\nDDPM. This method enables any deterministic, machine learning-based predictor to gen-\nerate stochastic forecasts. Leveraging the DDPM formulation, we introduce two mecha-\nnisms to balance exploration and exploitation: a guidance term and iterative sampling\n(or walks). We find that this method produces forecasts that outperform the determinis-\ntic baseline and has the added benefit of acting as a predictor-corrector system. Since the\nDDPM generates perturbations inherently, the method is highly parallelizable and, once\ntrained, requires only compute resources—unlike traditional data assimilation techniques\nthat depend on additional observational data.\nA natural next step is to evaluate this method in conjunction with a strong predictor,\nsuch as GraphCast or FourCastNet. This would enable benchmarking against state-\nof-the-art methods using standardized benchmarks such as WeatherBench. Given the\npromising results, we also propose an extension to improve the selection of the most\naccurate forecast trajectory. While our current approach relies on the pointwise mean,\nwe hypothesize that selecting the trajectory with the highest associated probability may\nyield more robust forecasts. To support this, we propose learning a scalar quantity\nthat quantifies a state’s relative proximity to the ground truth, modeled as a state-\nvalue function parameterized by a neural network. The method would evaluate each\nensemble member independently and output a distribution over relative uncertainties.\nTo avoid collapse of this distribution, we introduce a regularization term based on the\nKullback–Leibler (KL) divergence with respect to a prior, such as a standard normal\ndistribution. Additionally, we propose a slight modification to this prior formulation. For\nhighly probable states, exploration can be enhanced using techniques such as Monte Carlo\nTree Search (MCTS) or beam search, allowing us to refine our sampling strategy toward\nmultiple likely trajectories instead of covering the entire distribution, which may include\ndivergent paths. Rather than generating Nindependent trajectories from Ncopies of the\ninitial condition, this approach selectively expands the most probable trajectories while\npruning low-probability paths, resulting in a tree-like structure for forecast refinement.\nReferences\n[1] K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, Q. Tian, Pangu-weather: A 3d high-resolution model for\nfast and accurate global weather forecast (2022). arXiv:2211.02556 .\nURL https://arxiv.org/abs/2211.02556\n[2] R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri,\nT. Ewalds, Z. Eaton-Rosen, W. Hu, A. Merose, S. Hoyer, G. Holland, O. Vinyals, J. Stott, A. Pritzel,\nS. Mohamed, P. Battaglia, Graphcast: Learning skillful medium-range global weather forecasting\n(2023). arXiv:2212.12794 .\nURL https://arxiv.org/abs/2212.12794\n[3] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani, T. Kurth,\nD. Hall, Z. Li, K. Azizzadenesheli, P. Hassanzadeh, K. Kashinath, A. Anandkumar, Fourcastnet:\nA global data-driven high-resolution weather model using adaptive fourier neural operators (2022).\narXiv:2202.11214 .\nURL https://arxiv.org/abs/2202.11214\n[4] X. Zhong, L. Chen, X. Fan, W. Qian, J. Liu, H. Li, Fuxi-2.0: Advancing machine learning weather\nforecasting model for practical applications (2024). arXiv:2409.07188 .\nURL https://arxiv.org/abs/2409.07188\n18\n--- Page 19 ---\n[5] I. Price, A. Sanchez-Gonzalez, F. Alet, T. R. Andersson, A. El-Kadi, D. Masters, T. Ewalds, J. Stott,\nS. Mohamed, P. Battaglia, R. Lam, M. Willson, Gencast: Diffusion-based ensemble forecasting for\nmedium-range weather (2024). arXiv:2312.15796 .\nURL https://arxiv.org/abs/2312.15796\n[6] A. Simmons, C. Soci, J. Nicolas, B. Bell, P. Berrisford, R. Dragani, J. Flemming, L. Haimberger,\nS. Healy, H. Hersbach, A. Hor´ anyi, A. Inness, J. Munoz-Sabater, R. Radu, D. Schepers, Global\nstratospheric temperature bias and other stratospheric aspects of era5 and era5.1 (01/2020 2020).\ndoi:10.21957/rcxqfmg0 .\nURL https://www.ecmwf.int/node/19362\n[7] F. Baer, Numerical weather prediction, in: M. V. Zelkowitz (Ed.), Fortieth Anniversary Volume:\nAdvancing into the 21st Century, Vol. 52 of Advances in Computers, Elsevier, 2000, pp. 91–157.\ndoi:https://doi.org/10.1016/S0065-2458(00)80017-0 .\nURL https://www.sciencedirect.com/science/article/pii/S0065245800800170\n[8] A. Mahesh, W. Collins, B. Bonev, N. Brenowitz, Y. Cohen, J. Elms, P. Harrington, K. Kashinath,\nT. Kurth, J. North, T. OBrien, M. Pritchard, D. Pruitt, M. Risser, S. Subramanian, J. Willard,\nHuge ensembles part i: Design of ensemble weather forecasts using spherical fourier neural operators\n(2025). arXiv:2408.03100 .\nURL https://arxiv.org/abs/2408.03100\n[9] B. Bonev, T. Kurth, C. Hundt, J. Pathak, M. Baust, K. Kashinath, A. Anandkumar, Spherical\nfourier neural operators: Learning stable dynamics on the sphere (2023). arXiv:2306.03838 .\nURL https://arxiv.org/abs/2306.03838\n[10] T. Charnock, L. Perreault-Levasseur, F. Lanusse, Bayesian neural networks (2020). arXiv:2006.\n01490 .\nURL https://arxiv.org/abs/2006.01490\n[11] X. Luo, B. T. Nadiga, J. H. Park, Y. Ren, W. Xu, S. Yoo, A bayesian deep learning approach\nto near-term climate prediction, Journal of Advances in Modeling Earth Systems 14 (10) (2022)\ne2022MS003058, e2022MS003058 2022MS003058. arXiv:https://agupubs.onlinelibrary.wiley.\ncom/doi/pdf/10.1029/2022MS003058 ,doi:https://doi.org/10.1029/2022MS003058 .\nURL https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003058\n[12] S. Farquhar, M. A. Osborne, Y. Gal, Radial bayesian neural networks: Beyond discrete support in\nlarge-scale bayesian deep learning, in: S. Chiappa, R. Calandra (Eds.), Proceedings of the Twenty\nThird International Conference on Artificial Intelligence and Statistics, Vol. 108 of Proceedings of\nMachine Learning Research, PMLR, 2020, pp. 1352–1362.\nURL https://proceedings.mlr.press/v108/farquhar20a.html\n[13] C. Lu, H. Yuan, B. E. Schwartz, S. G. Benjamin, Short-range numerical weather prediction using\ntime-lagged ensembles, Weather and Forecasting 22 (3) (2007) 580 – 595. doi:10.1175/WAF999.1 .\nURL https://journals.ametsoc.org/view/journals/wefo/22/3/waf999_1.xml\n[14] X. Zhang, J. Li, Different initial condition perturbation methods for convection-permitting ensemble\nforecasting over south china during the rainy season, Monthly Weather Review 152 (1) (2024) 387\n– 412. doi:10.1175/MWR-D-23-0093.1 .\nURL https://journals.ametsoc.org/view/journals/mwre/152/1/MWR-D-23-0093.1.xml\n[15] L. Magnusson, M. Leutbecher, E. K¨ all´ en, Comparison between singular vectors and breeding vectors\nas initial perturbations for the ecmwf ensemble prediction system, Monthly Weather Review 136 (11)\n(2008) 4092 – 4104. doi:10.1175/2008MWR2498.1 .\nURL https://journals.ametsoc.org/view/journals/mwre/136/11/2008mwr2498.1.xml\n[16] D. P. Kingma, M. Welling, An introduction to variational autoencoders, Foundations and Trends ®\nin Machine Learning 12 (4) (2019) 307–392. doi:10.1561/2200000056 .\nURL http://dx.doi.org/10.1561/2200000056\n[17] J. Ho, A. Jain, P. Abbeel, Denoising diffusion probabilistic models (2020). arXiv:2006.11239 .\nURL https://arxiv.org/abs/2006.11239\n[18] T. Han, Z. Chen, S. Guo, W. Xu, L. Bai, Cra5: Extreme compression of era5 for portable global\nclimate and weather research via an efficient variational transformer (2024). arXiv:2405.03376 .\nURL https://arxiv.org/abs/2405.03376\n[19] F. Merizzi, A. Asperti, S. Colamonaco, Wind speed super-resolution and validation: from era5 to\ncerra via diffusion models (2024). arXiv:2401.15469 .\nURL https://arxiv.org/abs/2401.15469\n[20] S. Lang, M. Alexe, M. C. A. Clare, C. Roberts, R. Adewoyin, Z. B. Bouall` egue, M. Chantry,\nJ. Dramsch, P. D. Dueben, S. Hahner, P. Maciel, A. Prieto-Nemesio, C. O’Brien, F. Pinault,\nJ. Polster, B. Raoult, S. Tietsche, M. Leutbecher, Aifs-crps: Ensemble forecasting using a model\n19\n--- Page 20 ---\ntrained with a loss function based on the continuous ranked probability score (2024). arXiv:\n2412.15832 .\nURL https://arxiv.org/abs/2412.15832\n[21] V. Z. Zheng, L. Sun, Mvg-crps: A robust loss function for multivariate probabilistic forecasting\n(2025). arXiv:2410.09133 .\nURL https://arxiv.org/abs/2410.09133\n[22] M. Raissi, P. Perdikaris, N. Ahmadi, G. E. Karniadakis, Physics-informed neural networks and\nextensions (2024). arXiv:2408.16806 .\nURL https://arxiv.org/abs/2408.16806\n[23] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, M. Sun, Graph neural networks:\nA review of methods and applications (2021). arXiv:1812.08434 .\nURL https://arxiv.org/abs/1812.08434\n[24] J. Guibas, M. Mardani, Z. Li, A. Tao, A. Anandkumar, B. Catanzaro, Adaptive fourier neural\noperators: Efficient token mixers for transformers (2022). arXiv:2111.13587 .\nURL https://arxiv.org/abs/2111.13587\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention is all you need (2023). arXiv:1706.03762 .\nURL https://arxiv.org/abs/1706.03762\n[26] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, B. Glocker, D. Rueckert, Attention u-net: Learning where to look for\nthe pancreas (2018). arXiv:1804.03999 .\nURL https://arxiv.org/abs/1804.03999\n[27] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image segmen-\ntation (2015). arXiv:1505.04597 .\nURL https://arxiv.org/abs/1505.04597\n[28] J. Ho, T. Salimans, Classifier-free diffusion guidance (2022). arXiv:2207.12598 .\nURL https://arxiv.org/abs/2207.12598\n[29] P. Dhariwal, A. Nichol, Diffusion models beat gans on image synthesis (2021). arXiv:2105.05233 .\nURL https://arxiv.org/abs/2105.05233\n[30] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, J. Zhu, Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps (2022). arXiv:2206.00927 .\nURL https://arxiv.org/abs/2206.00927\n[31] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, J. Zhu, Dpm-solver++: Fast solver for guided sampling of\ndiffusion probabilistic models (2023). arXiv:2211.01095 .\nURL https://arxiv.org/abs/2211.01095\n[32] E. Tadmor, Runge-kutta methods are stable (2023). arXiv:2312.15546 .\nURL https://arxiv.org/abs/2312.15546\n20",
  "text_length": 53042
}