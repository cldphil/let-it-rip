{
  "id": "http://arxiv.org/abs/2506.01037v1",
  "title": "Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world\n  Video Super-resolution",
  "summary": "Existing diffusion-based video super-resolution (VSR) methods are susceptible\nto introducing complex degradations and noticeable artifacts into\nhigh-resolution videos due to their inherent randomness. In this paper, we\npropose a noise-robust real-world VSR framework by incorporating\nself-supervised learning and Mamba into pre-trained latent diffusion models. To\nensure content consistency across adjacent frames, we enhance the diffusion\nmodel with a global spatio-temporal attention mechanism using the Video\nState-Space block with a 3D Selective Scan module, which reinforces coherence\nat an affordable computational cost. To further reduce artifacts in generated\ndetails, we introduce a self-supervised ControlNet that leverages HR features\nas guidance and employs contrastive learning to extract degradation-insensitive\nfeatures from LR videos. Finally, a three-stage training strategy based on a\nmixture of HR-LR videos is proposed to stabilize VSR training. The proposed\nSelf-supervised ControlNet with Spatio-Temporal Continuous Mamba based VSR\nalgorithm achieves superior perceptual quality than state-of-the-arts on\nreal-world VSR benchmark datasets, validating the effectiveness of the proposed\nmodel design and training strategies.",
  "authors": [
    "Shijun Shi",
    "Jing Xu",
    "Lijing Lu",
    "Zhihang Li",
    "Kai Hu"
  ],
  "published": "2025-06-01T14:36:25Z",
  "updated": "2025-06-01T14:36:25Z",
  "categories": [
    "cs.CV",
    "I.4.4; I.2.6"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01037v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01037v1  [cs.CV]  1 Jun 2025Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video\nSuper-resolution\nShijun Shi1∗Jing Xu2*Lijing Lu3†Zhihang Li4†Kai Hu1\n1Jiangnan University2University of Science and Technology of China\n3Peking University4Chinese Academy of Sciences\nssj180123@gmail.com, xujing0@mail.ustc.edu.cn, lulijing1997@gmail.com,\nlizhihang.cas@gmail.com, hukai wlw@jiangnan.edu.cn\nhttps://ssj9596.github.io/scst-project/\nAbstract\nExisting diffusion-based video super-resolution (VSR)\nmethods are susceptible to introducing complex degrada-\ntions and noticeable artifacts into high-resolution videos\ndue to their inherent randomness. In this paper, we propose\na noise-robust real-world VSR framework by incorporating\nself-supervised learning and Mamba into pre-trained latent\ndiffusion models. To ensure content consistency across ad-\njacent frames, we enhance the diffusion model with a global\nspatio-temporal attention mechanism using the Video State-\nSpace block with a 3D Selective Scan module, which rein-\nforces coherence at an affordable computational cost. To\nfurther reduce artifacts in generated details, we introduce\na self-supervised ControlNet that leverages HR features\nas guidance and employs contrastive learning to extract\ndegradation-insensitive features from LR videos. Finally, a\nthree-stage training strategy based on a mixture of HR-LR\nvideos is proposed to stabilize VSR training. The proposed\nSelf-supervised ControlNet with Spatio-Temporal Continu-\nous Mamba based VSR algorithm achieves superior percep-\ntual quality than state-of-the-arts on real-world VSR bench-\nmark datasets, validating the effectiveness of the proposed\nmodel design and training strategies.\n1. Introduction\nVideo super-resolution (VSR) aims to restore high-\nresolution (HR) videos by leveraging complementary tem-\nporal information within low-resolution (LR) frames, which\nholds great value in practical usages, e.g., surveillance and\nhigh-definition display. Previous works mainly rely on the\nassumptions of simple and known image degradations (e.g.,\nbicubic downsampling) or specific camera-related degrada-\ntions, making it challenging to generalize the trained VSR\n*Equal contribution.\n†Corresponding author.\nS\nC\nS\nT\n \nM\nG\nL\nD\n \nF\nr\na\nm\ne\n \nT\n+\n1\n \nF\nr\na\nm\ne\n \nT\n \nS\nt\na\nb\nl\ne\nS\nR\nF\nr\na\nm\ne\n \nT\n \nI\nn\np\nu\nt\n \nR\ne\na\nl\n-\nW\no\nr\nl\nd\n \nS\nC\nS\nT\nFigure 1. A side-by-side comparison of video super-resolution\ntechniques StableSR [58], MGLD [69], and our SCST on two ad-\njacent frames from the VideoLQ dataset. The zoomed-in regions,\ncaptured from the same local position, illustrate each method’s\nperformance. SCST stands out for maintaining temporal consis-\ntency while delivering crisp details in real-world scenarios.\nmodels to real-world LR videos with unknown and more\ncomplex degradations.\nRecently, diffusion-based generative models [24] have\nachieved great success in tasks of general image/video gen-\neration [25, 51–53] and downstream tasks such as image\nediting [34], inpainting [45], colorization [6], etc, owing\nto its powerful ability in capturing diverse and compli-\ncated data distributions. There are also efforts to adapt\nthese diffusion priors to image super-resolution [33, 59, 67].\n[59, 67] rely on elaborately designed network architectures,\nsuch as ControlNet, to condition the diffusion models on\nlow-resolution (LR) images for real-world image super-\nresolution. Wang et al. [58] propose to inject LR images\ninto the U-Net blocks in a LDM network using a SFT mod-\nule. While showing promising results, directly applying\npre-trained diffusion models for image super-resolution to\ndegraded videos is challenging due to the inherent random-\n1\n--- Page 2 ---\nness in diffusion sampling, leading to temporal inconsisten-\ncies in generated videos. To address this problem, Zhou\net al. [74] propose to employ 3D convolution and tempo-\nral attention in the network to ensure temporal consistency.\nYang et al. [69] introduce to use optical flow to align latent\nfeatures between adjacent frames, enhancing temporal co-\nherence. However, the coupled problem of recovering the\nunknown and complex degradations in real-world scenar-\nios and producing temporal consistent results in the same\ntime causes great learning complexity. As shown in Fig-\nure 1, while MGLD [69] focuses on temporal consistency,\nit exhibits shortcomings in spatial recovery. Therefore, it\nis crucial for real-world VSR to extract clean features from\ncomplex degradation and hallucinate visual content while\nalso modeling the spatial-temporal dependencies between\ndifferent frames.\nTo tackle these issues, we propose a Self-supervised\nControlNet with Spatio- Temporal Continuous Mamba\n(SCST) for real-world VSR. SCST is a noise-robust tem-\nporal coherence diffusion model, aimed at reconstructing\nfine-grained textures from videos with unknown degrada-\ntions. To our best knowledge, we are the first to introduce\nSpatial- Temporal Continuous Mamba (STCM) for global\n3D attention in the VSR task. Specifically, we propose a\n3D Selective Scan method tailored for traversing spatial-\ntemporal domain, ensuring that every video patch acquires\ncontextual knowledge through a compressed hidden state\ncalculated along the respective scanning path with a linear\ncomputational complexity. To mitigate the impact of com-\nplex degradations in LR, we propose a ControlNet based\non MoCo (MoCoCtrl) [22], which can distill degradation-\ninsensitive features from LR towards the HR target. Finally,\nwe design a multi-stage HR-LR hybrid training strategy to\nstabilize training. The self-supervised ControlNet is trained\nusing contrastive learning, while the temporal module is op-\ntimized with de-noising loss.\nThe main contributions of this paper are summarized as\nfollows.\n• We propose a noise-robust temporal coherence dif-\nfusion model based on self-supervised learning and\nspatial-temporal continuous Mamba. The proposed self-\nsupervised ControlNet distills degradation-insensitive\nfeatures from LR videos while a global 3D attention\nbased on Mamba is designed to model the spatial-\ntemporal relationship of the video.\n• To stabilize VSR training, we introduce a decoupled\nthree-stage training strategy where HR and LR videos are\nmixed for training. Contrastive learning loss is incorpo-\nrated to align LR features with HR, enabling the extrac-\ntion of noise-free features.\n• Our proposed SCST model achieves state-of-the-art per-\nformance on existing benchmarks, showing remarkable\nvisual realism and temporal consistency.2. Related Work\n2.1. Video Super-Resolution\nThe goal of VSR is to enhance a sequence of HR video\nframes from their degraded LR counterparts. Based on the\nparadigms, existing VSR algorithms [3, 7, 8, 27–29, 31, 42–\n44, 47, 60, 66, 70] could be roughly classified into two\ncategories: temporal sliding-window based VSR and re-\ncurrent framework based VSR. Temporal sliding-window\nbased VSR [31, 40, 60] utilize a fixed set of neighboring\nframes to super-resolve one or more target frames. How-\never, the information accessible is constrained by the tem-\nporal window’s size. Consequently, these methods can only\nexploit the temporal details of a restricted subset of in-\nput video frames. To exploit temporal information from\nmore frames, recurrent framework based VSR [7, 8, 42, 43]\nutilizes multiple LR frames as input and employs recur-\nrent neural networks to simultaneously produce their cor-\nresponding SR results. However, most existing approaches\n[3, 7, 8, 27–29, 31, 42, 43, 60, 66] assume a pre-defined\ndegradation process [44, 47, 70]. In real-world scenes with\nmore complicated degradations, these VSR methods may\nnot perform well. Due to the lack of real-world paired\ndata for training, Yang et al. [68] propose to collect LR-\nHR data pairs with iPhone cameras to better model real-\nworld degradations. While the VSR model trained on such\ndata can be effective to videos captured by similar mobile\ncameras, it is relatively labor-intensive and may not gener-\nalize well to videos collected by other devices. Recent stud-\nies have shifted towards employing diverse degradations for\ndata augmentation during training, such as blur, downsam-\npling, noise and video compression [10, 63]. However,\nmaintaining temporal consistency while generating photo-\nrealistic textures remains a challenge.\n2.2. State Space Models\nStructured state space models (S4) [19], as a promising\nframework in handling long-distance sequences, has at-\ntracted widespread research interest. A variety of S4-\ninspired models, that capture long-range dependencies in\nsequential data, achieve competitive performance on vari-\nous tasks [21, 26, 48, 54, 57]. The major reason behind\nthis might be that S4’s adherence to Linear Time Invari-\nance (LTI), which guarantees consistent output for identical\ninputs regardless of their temporal application. Neverthe-\nless, LTI systems come with some limitations, especially\nwhen it comes to handling dynamic changes over time. The\nconstancy of the internal state transition matrix throughout\nthe sequence constrains the model’s adaptability to evolv-\ning content, thereby limiting its utility in contexts demand-\ning content-driven reasoning. To address these constraints,\nMamba [18] is recently introduced as a state-space model\nthat dynamically adjusts its parameters in response to the in-\n2\n--- Page 3 ---\nEncoder E\nControlnet ... ... ... ...STCMUNet DLR\nRandom \nDegradation\nNoisy LatentDecoderT-1\nHR\nVideo Super-ResolutionMoCoCtrl... ...\nFixed\nLR\n5 6\n7 8\n 1 2\n3 4\nLNScanRestore\nDWConv3DSSMMergeLNLinear\nLinear\nLinear3D-Mamba \n    Block\nK kinds \nof  scans\nHR\nDiffusion \nProcess\nHR-LR Video Pairs Degradation\n...Patch-Level \nMomentum Contrast\nSpatial-Temporal Continuous Mamba\n①\n②\n③①\n②\n③\n 1\n1\n1\n3\n5\n3\n2\n2\n2\n3\n5\n3\n5\n5\n7\n7\n6\n8\n8\n8\n6\n4\n6\n7\n4\n6\n4\n4\nflip K=6FeedForward\nLN3D-Mamba\nBlock\nPretrained Layers\nSpatial-Temporal\nContinuous MambaFixedPretrained ControlnetSpatial-Temporal\nContinuous ScanFigure 2. Overview of the proposed SCST framework for real-world VSR. SCST consists of several modules, including Spatial-Temporal\nContinuous Mamba (STCM) and Self-supervised ControlNet (MoCoCtrl). The STCM incorporates 3D-Mamba Block within its structure,\nwhich, with the addition of spatial-temporal continuous scan, ensures comprehensive 3D attention for both inter-frame and intra-frame\nmodeling. The Self-supervised ControlNet adopts the MoCo architecture to employ contrastive learning between LR and HR features,\naligning LR features to noise-free HR features, thus reducing the impact of degradation.\nput sequence. This adaptive strategy enables Mamba to en-\ngage in context-dependent reasoning, significantly enhanc-\ning its effectiveness across various domains [39, 41, 75].\nHowever, the application of Mamba in video super resolu-\ntion tasks remains unexplored.\n2.3. Self-Supervised learning\nSelf-supervised learning (SSL), as an off-the-shelf repre-\nsentation techniques, has achieved excellent performance in\nvarious computer vision tasks [4, 5, 11, 16, 17, 22, 23, 64].\nRecently, contrastive learning [1, 4, 5, 14, 15] has emerged\nas one of the most prominent self-supervised methods,\nmaking significant progress in exploring image representa-\ntions based on instance discrimination tasks, where an in-\nstance’s different views originating from the same instance\nare treated as positive examples for an anchor sample, while\nviews from different instances serve as negative examples.\nThe core idea is to promote proximity between positive\nexamples and maximize the separation between negative\nexamples within the latent space, thereby encouraging the\nmodel to capture meaningful relationships within the data.\n3. Methodology\n3.1. Overall Architecture\nGiven a LR video sequence of Tframes xl∈RT×H×W×3,\nthe goal is to reconstruct the corresponding HR video se-\nquence xh∈RT×sH×sW×3, where sis the scaling factor\nandH,Ware the height and width of input frames. We\nbuild our method on top of a pretrained SD model [52] to\nharness the powerful generative priors for real-world VSR.\nThe key design principle of the SD model is to add progres-\nsively increasing Gaussian noise to the clean data sample xaccording to a noise schedule {βt}T\nt=1. Given a noisy sam-\nplextwhere tis the diffusion step, a denoising UNet Dis\ntrained to estimate the added noise.\nIn our method, the training samples xhare drown from a\nHR video dataset. We use ControlNet [71] as an Encoder E\nfor LR videos to extract multi-scale latent features, which\nare injected into the feature maps of the denoising UNet D.\nIn this way, Dis conditioned on LR videos to reconstruct\nthe corresponding HR videos. Given the noise target ϵt, the\nobjective function for training DandEis as follows:\nE\nt,xh||D(xh\nt, t, E(xl))−ϵt||2, (1)\nThe stochastic nature of the diffusion denoising process\nleads to temporal instability in extended video sequences\nfor VSR tasks. Two essential problems need to be addressed\nsimultaneously: modeling the spatial-temporal dependen-\ncies between adjacent frames, and extracting clean and sta-\nble features for accurate reconstruction in the presence of\nunknown and complex video degradations. As depicted\nin Figure 2, our framework integrates the Spatial-Temporal\nContinuous Scan with the STCM into the Latent Diffusion\nModel (LDM) [52] to ensure spatial-temporal coherence\nboth within and across frame segments. Additionally, we\nintroduce an advanced self-supervised learning approach,\nthe Self-supervised Controlnet (MoCoCtrl), aimed at utiliz-\ning HR and LR video pairs to effectively refine the model’s\nability to generate detailed and noise-robust representations\nfor VSR.\n3.2. Mamba for 3D Attention\nWe provide a brief overview of SSM and present how to\nintroduce it for real-world VSR.\n3\n--- Page 4 ---\n3.2.1 Preliminaries: State Space Models\nDrawing from the Kalman filter [32], SSMs can be treated\nas linear time-invariant (LTI) systems that transform the in-\nput signal x(t)∈Rto output signal y(t)∈Rvia the hid-\nden state h(t)∈RN. In essence, continuous-time SSMs\ncan be represented as linear ordinary differential equations\n(ODEs), where they encode and decode one-dimensional\nsequential inputs.\nh′(t) =Ah(t) +Bx(t),\ny(t) =Ch(t) +Dx(t),(2)\nwhere A∈RN×N,B∈RN×1,C∈R1×N, and D∈R1\nare the weighting parameters.\nUsually, natural language and two-dimensional vision\ninputs are discrete signals. Therefore, Mamba utilizes\nthe zero-order hold (ZOH) rule for discretization. Conse-\nquently, the ODEs can be iteratively resolved:\n¯A=e∆A,\n¯B= (∆A)−1\u0000\ne∆A−I\u0001\n·∆B,\nh(t) =¯A h(t−1) +¯Bx(t),(3)\nHere,∆represents a model parameter. Mamba [18] aims to\nenhance the adaptability of SSMs by transitioning the time-\ninvariant parameters into time-varying ones. This adjust-\nment entails substituting the fixed model weights (B,C,∆)\nwith dynamic weights [30] that are dependent on the input\nx. This procedure, involving input-dependent parameters,\nis referred to as selective scanning.\n3.2.2 Spatial-Temporal Continuous Mamba\nJoint spatial-temporal modeling of videos plays a pivotal\nrole in VSR. Approaches like Upscale-A-Video [74] incor-\nporate 3D convolution layers and temporal attention, yet\ntheir receptive field is constrained. On the other hand, re-\ncent video generation methods [2, 37, 50] leverage full 3D\nattention for motion modeling, demonstrating superior per-\nformance against decoupled spatial and temporal attention,\nhowever at the expense of a much higher computation com-\nplexity. In this study, we introduce Spatial-Temporal Con-\ntinuous Mamba (STCM), to strike a balance between effi-\nciency and effectiveness.\n3D-Mamba Block. The 3D-Mamba Block, shown in Fig-\nure 2, is the core component of the STCM framework,\nspecifically adapted for video-based tasks. It enhances spa-\ntiotemporal feature extraction by employing 3D depth-wise\nconvolutions that capture both spatial and temporal depen-\ndencies. The block processes the input feature map using K\ntypes of scanning operations, generating sequences that are\nefficiently processed by a State Space Model (SSM) [18] to\ncapture global context with linear complexity. After pro-\ncessing through the SSM, the K sequences are combined,\nSpatial-Temporal\nContinuous Scan3D Sweep Scan\n3D Mamba with \nGlobal-Consistency\nFigure 3. Diagram of Temporal-Spatial Continuous Scan Strat-\negy with Global Consistency. Highlights a single continuous scan\npathway across frames, emphasizing spatial-temporal alignment\nthrough intra-frame (orange) and inter-frame (cyan) continuity.\nresulting in an output feature map that maintains the orig-\ninal input dimensions. Details of the scanning operations\nwill be discussed later.\nSpatial-Temporal Continuous Scan. As depicted in Fig-\nure 2, the 3D-Mamba Block utilizes the Spatial-Temporal\nContinuous Scan strategy to ensure a smooth, continuous\ninformation flow across both spatial and temporal dimen-\nsions. The scan involves three primary patterns, each fea-\nturing two distinct scanning trajectories: the original pattern\nand its flipped counterpart. This configuration results in a\ntotal of K= 6scanning paths. Figure 3 visually highlights\na continuous scan pathway across frames, represented by\nthe orange and cyan areas. The orange areas show intra-\nframe continuity, where the scan follows a sequential, pixel-\nby-pixel approach across the horizontal and vertical dimen-\nsions, capturing spatial information in a dense, continuous\nmanner. The cyan areas indicate inter-frame continuity,\nwhere the scan tracks the same spatial points across suc-\ncessive frames, preserving pixel positions over time. Un-\nlike traditional 3D sweep scan [65], which flattens the in-\nput and resets both within and between frames, our method\nmaintains a continuous scan path, ensuring spatial-temporal\ncoherence. This approach is key to capturing consistent\nfeatures in video data by maintaining pixel continuity both\nwithin and across frames, ensuring accurate temporal mod-\neling. Our experiments in Section 4.3 validate its superior-\nity over traditional methods.\n3.3. Momentum Contrastive ControlNet\nThe proposed 3D-Mamba module empowers the model\nwith the capability to capture global spatiotemporal corre-\nlations. However, we find that directly training the model\nwith Eq. 1 conditioning on LR videos leads to unstable\ntraining and emergence of artifacts. The presence of un-\nknown and complex degradation in the LR videos causes\noptimization difficulty. To stabilize the training progress,\nwe propose to provide additional supervising signal for\nthe ControlNet with self-supervised learning leveraging the\nground-truth HR videos.\n4\n--- Page 5 ---\nLRGradient Stop Gradient\nEMA\nHR\nMemory Queue\nPatch-Level \nLearning\npatch\nfeature\n \nHR LR\nSimilarity from \nstrong to weak...\n...\n...\n...\n...Patch-Level \nMomentum Contrast\nLR-HR Pair\n...            ...Figure 4. Patch-Level Momentum Contrast. LR and HR images\nare separately processed by ControlNet to extract feature maps,\nfollowed by patch-level contrastive learning on these features. The\nControlNet for LR is online updated, whereas the ControlNet for\nHR updates its weights using a momentum way.\nAs shown in Figure 4, we devise a MoCo-like [22] train-\ning framework in the optimization of the ControlNet, named\nMoCoCtrl. We choose the MoCo [22] framework due to\nits effectiveness and memory-friendly nature. While MoCo\nis traditionally applied to classification tasks with globally\npooled features, our method adapts it for super-resolution\ntasks by focusing on patch-level features to capture finer\nspatial details. In our MoCoCtrl framework, two encoders\nare used: a query encoder Eqfor LR frames and a mo-\nmentum encoder Ekfor HR frames. The momentum en-\ncoder’s weights are updated as an exponential moving aver-\nage (EMA) of the query encoder’s weights, ensuring sta-\nble representations of HR patches. During training, the\npositive sample pair (xl\ni, xh\ni)is mapped into feature maps\nthrough the query and key encoders as q=Eq(xl\ni)and\nk+=Ek(xh\ni). To capture spatial details, we use a projec-\ntion head to generate P×Ppatch features for each encoded\nfeature map. Negative samples are selected from a memory\nqueue, which is defined as Q={Ek(xh\nj),(Ek(xl\nj)|j∈\n{0,1, . . . , K/ 2}, j̸=i},where Kis the size of the mem-\nory queue. Both HR and LR samples are encoded in Qto\nstrengthen the contrastive learning signal and handle more\ndiverse patch-level distinctions.\nFor each encoded query q, a patch-level contrastive loss\nis defined as:\nLq=1\nP2X\np−logexp(qp·kp\n+/τ)\nexp(qp·kp\n+/τ) +P\nQexp(qp·Q/τ),\n(4)\nwhere qpandkp\n+represent the pthpatch features of the\nquery and positive HR sample. This patch-level contrastive\napproach improves the model’s ability to capture detailed\nspatial information, enhancing the performance of super-\nresolution tasks by precisely aligning LR and HR patches.\n3.4. Multi-stage HR-LR hybrid training strategy\nThe proposed self-supervised ControlNet enables the model\nto find degradation-robust features, while the Mamba\nE D\nNoisy \nLatentHR/LR \nVideosGenera tive \nDiffusion Loss\nRec/SR \nStage1Rec  \nD\nNoisy \nLatentLR \nVideosGenerative \nDiffusion LossHR \nVideos\nAlignment \nLearning Loss\nStage2SR  \nD\nNoisy \nLatentLR \nVideosGenerative \nDiffusion Loss\nStage3SR  \nTemporal-Spatial \nContinuous Mamba\nTrainable\nFrozenEMAFigure 5. Multi-stage HR-LR hybrid training strategy.\nblocks model a comprehensive spatial-temporal correlation\nof the video sequences. We further propose a multi-stage\nHR-LR hybrid training strategy to facilitate the learning of\nthe two modules.\nWe initialize the network using pretrained weights from\nStable Diffusion V2.1. The weights of the original 2D U-\nNet are kept fixed, only the newly introduced layers are\ntrained. As shown in Figure 5, the training consists of\nthree stages. In stage 1 , the ControlNet is trained with\na mixture of HR/LR videos, where the HR videos can be\nviewed as LR videos with minimal degradations. When\nthe inputs to the ControlNet are HR videos, the model is\nessentially trained to reconstruct the inputs. Training with\nHR videos allows the ControlNet to extract the most accu-\nrate features for reconstructing the HR videos. The mixture\nratio of HR/LR videos starts at 1 and gradually decreases\nto 0.3, thereby gradually adapting the ControlNet for real-\nworld VSR. Additionally, a Reconstruction/SR label is in-\ntroduced to enable the model to distinguish between the re-\nconstruction and super-resolution tasks. In stage 2 , the pro-\nposed MoCoCtrl is introduced. The training proceeds with\na mixture of HR/LR videos, maintaining a fixed ratio at 1:1.\nThe self-supervised learning more fully utilizes the recon-\nstruction prior learned in Stage 1 to facilitate the training of\nreal-world VSR. In stage 3 , the proposed Spatial-Temporal\nContinuous Mamba is integrated into the Unet. During this\nstage, the ControlNet remains unchanged. Only LR videos\nare used for training at this stage.\n4. Experiments\n4.1. Experimental settings\nTraining Datasets. We train our model using REDS [47]\nand YouHQ [74] datasets. Following Wang et al. [60], the\nREDS4 dataset1within REDS is excluded from training the\nmodel. Additionally, we split YouHQ40 dataset for testing\nonly following Zhou et al. [74], which contains 40 videos.\nFollowing the degradation pipeline of RealBasicVSR [9],\nwe generate the LQ-HQ video pairs for training.\nTesting Datasets. We construct the test set with four syn-\nthetic testing datasets (i.e., REDS4, UDM10 [70], SPMCS\n1Clips 000, 011, 015, 020 of REDS training set.\n5\n--- Page 6 ---\nTable 1. Quantitative comparisons of state-of-the-art VSR models on different VSR datasets. The best and second performances are\nhighlighted in red and blue, respectively.\nDatasets Metrics Bicubic RealESRGAN StableSR DBVSR RealBasicVSR RealViformer Upscale-A-Video MGLD SCST\nREDS4PSNR↑ 23.81 22.69 22.64 22.38 23.94 24.28 22.73 22.56 23.02\nSSIM↑ 0.6313 0.6201 0.6256 0.6015 0.6534 0.6513 0.5982 0.5943 0.6108\nLPIPS ↓ 0.6485 0.2964 0.2992 0.4941 0.2545 0.2536 0.3639 0.2660 0.2518\nDISTS ↓ 0.2858 0.1426 0.1277 0.2510 0.1196 0.1306 0.1840 0.1171 0.1094\nUDM10PSNR↑ 26.40 25.66 25.54 24.88 26.10 27.18 25.65 25.89 26.42\nSSIM↑ 0.7727 0.7817 0.7622 0.7343 0.7658 0.7948 0.7413 0.7713 0.7893\nLPIPS ↓ 0.5039 0.2739 0.2567 0.4685 0.2812 0.2580 0.2799 0.2551 0.2156\nDISTS ↓ 0.2632 0.1595 0.1343 0.2454 0.1619 0.1533 0.1544 0.1386 0.1328\nSPMCSPSNR↑ 23.21 22.38 22.21 22.01 23.10 23.43 21.72 22.87 22.17\nSSIM↑ 0.6082 0.6029 0.5932 0.5650 0.6049 0.6215 0.5327 0.6095 0.6098\nLPIPS ↓ 0.6360 0.3238 0.3079 0.5160 0.3142 0.3030 0.3743 0.3041 0.2600\nDISTS ↓ 0.3092 0.1924 0.1709 0.2725 0.1847 0.1884 0.2201 0.1769 0.1612\nYouHQ40PSNR↑ 24.47 23.76 23.41 23.41 23.26 24.44 23.41 23.48 24.31\nSSIM↑ 0.6787 0.6743 0.6555 0.6480 0.6306 0.6730 0.6252 0.6394 0.6759\nLPIPS ↓ 0.5437 0.3126 0.3071 0.4699 0.3706 0.3202 0.3349 0.3309 0.2525\nDISTS ↓ 0.2416 0.1537 0.1360 0.2137 0.1745 0.1740 0.1618 0.1540 0.1344\nVideoLQCLIP-IQA ↑0.2949 0.3617 0.4160 0.2475 0.3881 0.3460 0.2818 0.3462 0.4859\nMUSIQ ↑ 22.56 49.84 47.77 31.27 55.61 52.09 43.34 50.94 59.20\nNIQE↓ 8.059 4.203 4.418 6.278 3.698 4.057 4.8762 3.727 3.566\nDOVER ↑ 0.3882 0.7152 0.7029 0.5264 0.7367 0.7194 0.6199 0.7340 0.7443\nStableSRRealViformerMGLD Ours GT Upscale-A-Video Input\nFigure 6. Qualitative comparisons on synthetic low-quality videos. (Zoom-in for best view)\n[55], and YouHQ40), which follow the same degradation\npipeline in training to generate LQ videos. Additionally, we\nevaluate the models on a real-world dataset VideoLQ [9].\nImplementation Details. The model is trained on 8\nNVIDIA A100 GPUs with Adam [36] optimizer and a batch\nsize of 32. The sequence length and video resolution are\nset to 8 and 512×512 respectively. To better leverage\nthe prior knowledge of text-to-image diffusion models, we\nuse Panda-70M model [12] to generate text prompts dur-\ning training and inference. Training takes approximately\n12 hours for stage 1, 30 hours for stage 2, and 30 hours for\nstage 3. During inference, owing to memory limitations, we\nsegment LR videos into multiple sequences. The number of\nsampling steps is set to 20.\nEvaluation Metrics. In order to comprehensively evalu-\nate real-world VSR methods, we utilize a range of met-\nrics across both synthetic and real-world datasets. Forsynthetic datasets, we assess the video quality using four\nprevalent metrics in real-world VSR tasks: learned percep-\ntual image patch similarity (LPIPS) [72], deep image struc-\nture and texture similarity (DISTS) [13], structural similar-\nity index (SSIM), and the widely recognized peak signal-\nto-noiseratio (PSNR). When assessing the performance on\nthe real-world dataset, we compute the no-reference image\nquality metrics: the natural image quality evaluator (NIQE)\n[46] and CLIP-IQA [56]. Additionally, we also include a\ndeep learning based image-based metric MUSIQ [35] and a\nvideo quality assessment metric DOVER [62].\n4.2. Experimental Results\nTo comprehensively evaluate the performance of our SCST\nalgorithm, we compare it with several state-of-the-art meth-\nods, including two real-world image super-resolution mod-\nels (RealESRGAN [61], StableSR [58]), two real-world\n6\n--- Page 7 ---\nUpscale-A-Video StableSR RealViformer\nRealBasicVSR MGLD Ours\nUpscale-A-Video StableSR RealViformer\nRealBasicVSR MGLD Ours InputInputFigure 7. Qualitative comparisons on real-world test videos in VideoLQ. (Zoom-in for best view)\nVSR models (DBVSR [49], RealBasicVSR [10], and re-\ncently proposed Upscale-A-Video [74], MGLD [69] and\nRealViformer [73].\nQuantitative Comparison. Table 1 demonstrates the quan-\ntitative comparison on the synthetic datasets and real-world\nvideo benchmarks. From the Table 1, we can observe that\nour approach attains superior performance in terms of full-\nreference perceptual metrics LPIPS and DISTS across all\nsynthetic test datasets. This suggests that our method can\neffectively restore high-quality, realistic details from se-\nquences affected by intricate degradations. While meth-\nods such as DBVSR might exhibit improved performance\nconcerning PSNR or SSIM on specific datasets, they of-\nten produce blurred outputs, as evidenced by the LPIPS and\nDISTS metrics. When considering the performance on the\nreal-world VSR dataset VideoLQ, our approach achieves\nthe best results in CLIP-IQA, MUSIQ , NIQE, and DOVER\nmetrics, which indicate the robust capacity of our SCST to\nenhance real-world videos, producing authentic details and\nclean textures.\nQualitative Comparison. To further demonstrate the ef-\nfectiveness of our SCST, we conduct visual comparisons\nof these models on both synthetic datasets and real-world\nVideoLQ dataset, as shown in Figure 6 and Figure 7, re-\nspectively. For synthetic datasets, from the Figure 6, we can\nobserve that SCST excels in reconstructing structures while\ngenerating cleaner details under complex degradations. The\nimprovements are particularly evident in the enhanced clar-\nity of distant mountain peaks and the textured details of\nbrick walls, as well as the naturalistic rendering of a parrot’sTable 2. Ablation study on the different components of SCST on\nYouHQ. Best marked in bold .\nModels MoCoCtrl STCM PSNR ↑SSIM↑LPIPS ↓DISTS ↓\n(a) 21.22 0.6357 0.2824 0.1596\n(b) ✓ 23.63 0.6563 0.2671 0.1473\n(c) ✓ 23.18 0.6533 0.2581 0.1470\n(d) ✓ ✓ 24.31 0.6758 0.2525 0.1344\nplumage. For the real-world VSR, it is apparent that SCST\nsurpasses other state-of-the-art algorithms in eliminating in-\ntricate spatially varying degradations while producing real-\nistic details. Note that, SCST excels as the sole method\ncapable of accurately delineating the intricate details of the\neagle’s eyes, showcasing its advanced resolution capabili-\nties. Similarly, SCST provides a significantly clearer depic-\ntion of the vehicle’s tires, accurately capturing the texture\nand contours with high fidelity. In contrast, other state-of-\nthe-art methods result in blurred and less defined features.\n4.3. Ablation Study\nBaseline Design. To assess the impact of the two key com-\nponents of our network, MoCoCtrl and STCM, we con-\nducted a series of ablation experiments. We start by cre-\nating a baseline model removing all the major components\nof the network. Specifically, the baseline model excludes\nthe second-stage contrastive training and directly trains the\nmodel using Eq. 1.\nEffectiveness of MoCoCtrl. As shown in Figure 8 (c)\nand (d), as well as Table 2, the MoCoCtrl module re-\nsults in clearer super-resolution outputs and better perfor-\nmance metrics. We can observe that Model (d) outperforms\n7\n--- Page 8 ---\n(c) \nFrame T Frame T+1 \nLQ (a) (b) (d) Figure 8. Comparison of two adjacent frames in one video SR\nresults synthesized by different components.\nTable 3. Comparison of different spatial-temporal modeling ap-\nproaches on YouHQ and REDS4. “Mamba-S” employs a 3D\nSweep Scan strategy. Best marked in blod .\nModelsPSNR↑/ LPIPS ↓ Warp Error ↓\nYouHQ REDS4 YouHQ REDS4\nw/o Temporal 23.18 / 0.2581 22.73 / 0.2669 0.3603 3.619\nLocal Attention 24.07 / 0.2570 22.60 / 0.2629 0.2311 2.889\nMamba-S 24.12 / 0.2533 22.59 / 0.2590 0.2494 2.926\nSTCM 24.31 /0.2525 23.02 /0.2518 0.2295 2.878\nModel (b) in terms of PSNR, SSIM, and LPIPS. Specifi-\ncally, Model (d) shows better performance in PSNR (24.31\ndB vs. 23.63 dB), higher SSIM (0.6758 vs. 0.6563), lower\nLPIPS (0.2525 vs. 0.2671) and DISTS (0.1344 vs. 0.1473).\nThis comparison clearly demonstrates the improvement in\nvideo super-resolution quality in Model (d) due to the Mo-\nCoCtrl module, thus confirming its effectiveness.\nEffectiveness of STCM. In addition to the proposed Mo-\nCoCtrl, STCM further enhances the quality of our gener-\nated videos. Specifically, STCM extends its global recep-\ntive field to capture complex spatio-temporal relationships\nand employs continuous scanning to identify long-range de-\npendencies within video sequences. STCM is designed to\nfacilitate the model in better understanding video content,\nthereby enhancing temporal consistency while maintaining\nhigh-resolution output with increased fidelity. As illustrated\nin Figure 8, the temporal consistency is markedly inferior\nin the absence of the STCM module. Moreover, the in-\ntegration of the STCM module into Model (c) is shown\nto yield substantial enhancements in performance metrics,\nwith noteworthy increases of 1.13 dB in PSNR, 0.0225 in\nSSIM, 0.0056 in LPIPS, and 0.0126 in DISTS, as demon-\nstrated in Table 2.\nAnalysis on the Mamba design. To further elucidate the\nadvantages of our STCM, we also conducted an analysis\ncomparing various spatial-temporal modeling approaches in\nterms of restoration quality and temporal consistency. We\nreplace the STCM component with local inter-frame atten-\ntion (Local Attention) [20] and 3D Sweep Scan Mamba\n(Mamba-S) to compare different spatial-temporal modeling\napproaches. Table 3 shows STCM achieves the best perfor-\nmance among all methods in PSNR and LPIPS. Compared\nto Local Attention, which is limited by its spatial receptive\nfield, STCM leverages multi-direction fusion to fully cap-\nture the 3D data pattern. As shown in Figure 9, Local At-\nLocal Attention Mamba-S STCM LQFigure 9. Different Spatial-temporal modeling approaches\nt\ny\nw/o TemporalLR\nLocal Attention\nMamba-S\nSTCM\nGT\nFigure 10. Visual comparison on temporal profile with different\nspatial-temporal modeling approaches, with STCM exhibiting the\nbest temporal consistency. (Zoom-in for best view)\ntention produces visible distortions and blurred edges, es-\npecially in complex regions, resulting in misaligned geom-\netry. In contrast, STCM incorporates surrounding pixel in-\nformation to reconstruct more rectilinear and well-aligned\nstructures, significantly reducing artifacts in challenging ar-\neas. Beyond restoration quality, we use the Warping Er-\nror (WE) [38] to measure temporal consistency. Table 3\nclearly shows that STCM outperforms all other methods on\nboth datasets, achieving the lowest WE scores of 0.2295 and\n2.878 on YouHQ and REDS4, respectively. It is worth men-\ntioning that the excellent performance of STCM is driven by\nits spatial-temporal continuous scanning strategy. Unlike\nMamba-S, which employs a 3D Sweep Scan that disrupts\ncontinuity by resetting between frames, our approach main-\ntains an uninterrupted flow of information. This continu-\nous scanning ensures spatial-temporal coherence and pre-\ncise frame alignment. Figure 10 illustrates that STCM pre-\nserves steady structures across frames, while Mamba-S ex-\nhibits temporal fluctuations and misalignments.\n5. Conclusion\nIn this paper, we propose a Self-supervised ControlNet\nwith Spatio-Temporal Mamba algorithm dubbed SCST,\ndesigned for high-quality and temporally consistent real-\nworld Video Super-Resolution (VSR). The distinctiveness\nof our proposed method lies in the idea of introducing a spe-\ncific Self-supervised ControlNet as a degradation removal\nmodule, reducing the impact of complex degradations on\nthe effectiveness of VSR. To further model spatio-temporal\nrelationships for temporal consistency, we present an effi-\ncient 3D attention-based variant of the successful Mamba\nmodel. Finally, to ensure training stability, we propose a\nmulti-stage HR-LR hybrid training strategy, decomposing\nreal-world VSR into multiple subtasks, with each stage ad-\ndressing a specific task. Our proposed SCST has achieved\nstate-of-the-art results on existing VSR benchmarks.\n8\n--- Page 9 ---\nReferences\n[1] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo-\njanowski, Florian Bordes, Pascal Vincent, Armand Joulin,\nMike Rabbat, and Nicolas Ballas. Masked siamese networks\nfor label-efficient learning. In European Conference on Com-\nputer Vision , pages 456–473. Springer, 2022. 3\n[2] Fan Bao, Chendong Xiang, Gang Yue, Guande He,\nHongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu,\nYaole Wang, and Jun Zhu. Vidu: a highly consistent,\ndynamic and skilled text-to-video generator with diffusion\nmodels. arXiv preprint arXiv:2405.04233 , 2024. 4\n[3] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool.\nVideo super-resolution transformer. arXiv preprint\narXiv:2106.06847 , 2021. 2\n[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. Ad-\nvances in neural information processing systems , 33:9912–\n9924, 2020. 3\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision , pages 9650–9660, 2021. 3\n[6] Hernan Carrillo, Micha ¨el Cl ´ement, Aur ´elie Bugeau, and\nEdgar Simo-Serra. Diffusart: Enhancing line art coloriza-\ntion with conditional diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 3486–3490, 2023. 1\n[7] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and\nChen Change Loy. Basicvsr: The search for essential compo-\nnents in video super-resolution and beyond. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4947–4956, 2021. 2\n[8] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Basicvsr++: Improving video super-\nresolution with enhanced propagation and alignment. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 5972–5981, 2022. 2\n[9] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Investigating tradeoffs in real-world\nvideo super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5962–5971, 2022. 5, 6\n[10] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Investigating tradeoffs in real-world\nvideo super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5962–5971, 2022. 2, 7\n[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning , pages 1597–1607. PMLR, 2020. 3\n[12] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,\nEkaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon,Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang,\net al. Panda-70m: Captioning 70m videos with multiple\ncross-modality teachers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 13320–13331, 2024. 6\n[13] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli.\nImage quality assessment: Unifying structure and texture\nsimilarity. IEEE transactions on pattern analysis and ma-\nchine intelligence , 44(5):2567–2581, 2020. 6\n[14] Chen Feng and Ioannis Patras. Adaptive soft contrastive\nlearning. In 2022 26th International Conference on Pattern\nRecognition (ICPR) , pages 2721–2727. IEEE, 2022. 3\n[15] Chen Feng and Ioannis Patras. Maskcon: Masked con-\ntrastive learning for coarse-labelled dataset. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 19913–19922, 2023. 3\n[16] Chen Feng, Georgios Tzimiropoulos, and Ioannis Patras.\nSsr: An efficient and robust framework for learning with un-\nknown label noise. arXiv preprint arXiv:2111.11288 , 2021.\n3\n[17] Zheng Gao, Chen Feng, and Ioannis Patras. Self-supervised\nrepresentation learning with cross-context learning between\nglobal and hypercolumn features. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 1773–1783, 2024. 3\n[18] Albert Gu and Tri Dao. Mamba: Linear-time sequence\nmodeling with selective state spaces. arXiv preprint\narXiv:2312.00752 , 2023. 2, 4\n[19] Albert Gu, Karan Goel, and Christopher R ´e. Efficiently\nmodeling long sequences with structured state spaces. arXiv\npreprint arXiv:2111.00396 , 2021. 2\n[20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,\nYaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin,\nand Bo Dai. Animatediff: Animate your personalized text-\nto-image diffusion models without specific tuning. arXiv\npreprint arXiv:2307.04725 , 2023. 8\n[21] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state\nspaces are as effective as structured state spaces. Advances\nin Neural Information Processing Systems , 35:22982–22994,\n2022. 2\n[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n9729–9738, 2020. 2, 3, 5\n[23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 16000–\n16009, 2022. 3\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems , 33:6840–6851, 2020. 1\n[25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels.arXiv preprint arXiv:2210.02303 , 2022. 1\n9\n--- Page 10 ---\n[26] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip\nclassification with state-space video models. In European\nConference on Computer Vision , pages 87–104. Springer,\n2022. 2\n[27] Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin\nWang, and Qi Tian. Video super-resolution with recurrent\nstructure-detail network. In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XII 16 , pages 645–660. Springer,\n2020. 2\n[28] Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory\nSlabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, and Qi\nTian. Video super-resolution with temporal group attention.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 8008–8017, 2020.\n[29] Takashi Isobe, Fang Zhu, Xu Jia, and Shengjin Wang. Re-\nvisiting temporal modeling for video super-resolution. arXiv\npreprint arXiv:2008.05765 , 2020. 2\n[30] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van\nGool. Dynamic filter networks. In NIPS , pages 667–675,\n2016. 4\n[31] Younghyun Jo, Seoung Wug Oh, Jaeyeon Kang, and\nSeon Joo Kim. Deep video super-resolution network using\ndynamic upsampling filters without explicit motion compen-\nsation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 3224–3232, 2018. 2\n[32] Rudolph Emil Kalman. A new approach to linear filtering\nand prediction problems. 1960. 4\n[33] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. Advances\nin Neural Information Processing Systems , 35:23593–23606,\n2022. 1\n[34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 6007–6017, 2023. 1\n[35] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\nFeng Yang. Musiq: Multi-scale image quality transformer.\nInProceedings of the IEEE/CVF international conference on\ncomputer vision , pages 5148–5157, 2021. 6\n[36] D Kinga, Jimmy Ba Adam, et al. A method for stochastic\noptimization. In International conference on learning rep-\nresentations (ICLR) , page 6. San Diego, California;, 2015.\n6\n[37] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 4\n[38] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In ECCV , pages 170–185, 2018. 8\n[39] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-\nnd: Selective state space modeling for multi-dimensional\ndata. arXiv preprint arXiv:2402.05892 , 2024. 3\n[40] Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and\nJiaya Jia. Mucan: Multi-correspondence aggregation net-\nwork for video super-resolution. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part X 16 , pages 335–351. Springer,\n2020. 2[41] Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei\nXu, Zhikang Zou, Xiaoqing Ye, and Xiang Bai. Point-\nmamba: A simple state space model for point cloud analysis.\narXiv preprint arXiv:2402.10739 , 2024. 3\n[42] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan,\nEddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu\nTimofte, and Luc V Gool. Recurrent video restoration trans-\nformer with guided deformable attention. Advances in Neu-\nral Information Processing Systems , 35:378–393, 2022. 2\n[43] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang,\nRakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool.\nVrt: A video restoration transformer. IEEE Transactions on\nImage Processing , 2024. 2\n[44] Ce Liu and Deqing Sun. On bayesian adaptive video super\nresolution. IEEE transactions on pattern analysis and ma-\nchine intelligence , 36(2):346–360, 2013. 2\n[45] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 11461–11471, 2022. 1\n[46] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-\ning a “completely blind” image quality analyzer. IEEE Sig-\nnal processing letters , 20(3):209–212, 2012. 6\n[47] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik\nMoon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee.\nNtire 2019 challenge on video deblurring and super-\nresolution: Dataset and study. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition workshops , pages 0–0, 2019. 2, 5\n[48] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey\nShah, Tri Dao, Stephen Baccus, and Christopher R ´e. S4nd:\nModeling images and videos as multidimensional signals\nwith state spaces. Advances in neural information processing\nsystems , 35:2846–2861, 2022. 2\n[49] Jinshan Pan, Haoran Bai, Jiangxin Dong, Jiawei Zhang, and\nJinhui Tang. Deep blind video super-resolution. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision , pages 4811–4820, 2021. 7\n[50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720 ,\n2024. 4\n[51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125 , 1\n(2):3, 2022. 1\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684–10695, 2022. 3\n[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\n10\n--- Page 11 ---\nlanguage understanding. Advances in neural information\nprocessing systems , 35:36479–36494, 2022. 1\n[54] Jimmy TH Smith, Andrew Warrington, and Scott W Linder-\nman. Simplified state space layers for sequence modeling.\narXiv preprint arXiv:2208.04933 , 2022. 2\n[55] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya\nJia. Detail-revealing deep video super-resolution. In Pro-\nceedings of the IEEE international conference on computer\nvision , pages 4472–4480, 2017. 6\n[56] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-\nploring clip for assessing the look and feel of images. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\npages 2555–2563, 2023. 6\n[57] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu,\nMohamed Omar, and Raffay Hamid. Selective structured\nstate-spaces for long-form video understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 6387–6397, 2023. 2\n[58] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior for\nreal-world image super-resolution. International Journal of\nComputer Vision , pages 1–21, 2024. 1, 6\n[59] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior for\nreal-world image super-resolution. International Journal of\nComputer Vision , pages 1–21, 2024. 1\n[60] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and\nChen Change Loy. Edvr: Video restoration with enhanced\ndeformable convolutional networks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops , pages 0–0, 2019. 2, 5\n[61] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision , pages 1905–1914,\n2021. 6\n[62] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi\nLin. Exploring video quality assessment on user gener-\nated contents from aesthetic and technical perspectives. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 20144–20154, 2023. 6\n[63] Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao\nDong, and Ying Shan. Mitigating artifacts in real-world\nvideo super-resolution models. In Proceedings of the AAAI\nConference on Artificial Intelligence , pages 2956–2964,\n2023. 2\n[64] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 9653–9663, 2022. 3\n[65] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu.\nSegmamba: Long-range sequential modeling mamba for 3d\nmedical image segmentation. In International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion, pages 578–588. Springer, 2024. 4[66] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-oriented\nflow. International Journal of Computer Vision , 127:1106–\n1125, 2019. 2\n[67] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and\nLei Zhang. Pixel-aware stable diffusion for realistic image\nsuper-resolution and personalized stylization. arXiv preprint\narXiv:2308.14469 , 2023. 1\n[68] Xi Yang, Wangmeng Xiang, Hui Zeng, and Lei Zhang. Real-\nworld video super-resolution: A benchmark dataset and a de-\ncomposition based learning scheme. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 4781–4790, 2021. 2\n[69] Xi Yang, Chenhang He, Jianqi Ma, and Lei Zhang. Motion-\nguided latent diffusion for temporally consistent real-world\nvideo super-resolution. arXiv preprint arXiv:2312.00853 ,\n2023. 1, 2, 7\n[70] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Ji-\nayi Ma. Progressive fusion video super-resolution network\nvia exploiting non-local spatio-temporal correlations. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision , pages 3106–3115, 2019. 2, 5\n[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n3\n[72] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 6\n[73] Yuehan Zhang and Angela Yao. Realviformer: Investigat-\ning attention for real-world video super-resolution. arXiv\npreprint arXiv:2407.13987 , 2024. 7\n[74] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang\nLuo, and Chen Change Loy. Upscale-a-video: Temporal-\nconsistent diffusion model for real-world video super-\nresolution. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 2535–\n2545, 2024. 2, 4, 5, 7\n[75] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\nvisual representation learning with bidirectional state space\nmodel. arXiv preprint arXiv:2401.09417 , 2024. 3\n11",
  "text_length": 52423
}