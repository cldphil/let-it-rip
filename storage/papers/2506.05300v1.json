{
  "id": "http://arxiv.org/abs/2506.05300v1",
  "title": "Power Law Guided Dynamic Sifting for Efficient Attention",
  "summary": "Efficient inference on GPUs using large language models remains challenging\ndue to memory bandwidth limitations, particularly during data transfers between\nHigh Bandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory\noverhead but often rely on expensive top-$k$ operations, which perform poorly\non GPUs. We propose SiftAttention, a novel approximate attention method that\nreplaces the top-$k$ step with a computationally efficient element-wise\nfiltering operation based on a threshold value. Our intuition for doing this is\nbased on our empirical observation that the $\\tau$-th quantile of attention\nscores follows a predictable power-law over sequential generation steps.\nExploiting this insight, our approach dynamically estimates a threshold value\nper prompt at each generation step. Only attention scores above this threshold\nand their corresponding value vectors are loaded/used to compute the attention\noutput, reducing data movement between HBM and SRAM. Our evaluation\ndemonstrates that SiftAttention preserves model quality better than existing\napproximate attention methods while reducing memory bandwidth usage when\nloading value vectors.",
  "authors": [
    "Nirav Koley",
    "Prajwal Singhania",
    "Abhinav Bhatele"
  ],
  "published": "2025-06-05T17:50:32Z",
  "updated": "2025-06-05T17:50:32Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05300v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05300v1  [cs.LG]  5 Jun 2025Power Law Guided Dynamic Sifting for Efficient\nAttention\nNirav Koley∗, Prajwal Singhania∗, Abhinav Bhatele\nDepartment of Computer Science, University of Maryland\nCollege Park, MD 20742\nprajwal@umd.edu ,bhatele@cs.umd.edu\nAbstract\nEfficient inference on GPUs using large language models remains challenging due\nto memory bandwidth limitations, particularly during data transfers between High\nBandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory over-\nhead but often rely on expensive top- koperations, which perform poorly on GPUs.\nWe propose SiftAttention, a novel approximate attention method that replaces the\ntop-kstep with a computationally efficient element-wise filtering operation based\non a threshold value. Our intuition for doing this is based on our empirical obser-\nvation that the τ-th quantile of attention scores follows a predictable power-law\nover sequential generation steps. Exploiting this insight, our approach dynamically\nestimates a threshold value per prompt at each generation step. Only attention\nscores above this threshold and their corresponding value vectors are loaded/used to\ncompute the attention output, reducing data movement between HBM and SRAM.\nOur evaluation demonstrates that SiftAttention preserves model quality better than\nexisting approximate attention methods while reducing memory bandwidth usage\nwhen loading value vectors.\n1 Introduction\nLarge language models (LLMs) have transformed natural language processing and many other areas\nin computer science, achieving state-of-the-art results across a wide range of tasks. Powered by\nthe transformer architecture [ 24], these models leverage the self-attention mechanism [ 2] to learn\ncomplex language patterns. As the sizes of large language models (LLMs) have grown from millions\nto hundreds of billions of parameters, their computational, memory, and consequently energy demands\nhave also increased significantly.\nThe self-attention mechanism, originally applied to relatively small models, now poses a major\nefficiency bottleneck, especially during auto-regressive generation for long sequence lengths. At each\ngeneration step, the attention mechanism computes a weighted sum over all previous token states\nstored in a key-value (KV) cache. Transferring this data from GPU High Bandwidth Memory (HBM)\nto on-chip SRAM is costly and leads to significant memory bandwidth bottlenecks [ 8]. Additionally,\nit requires dot-product computation between the current input and all previous token states, leading\nto quadratic computational complexity with respect to sequence length.\nSeveral methods have been proposed to address the computational challenges associated with the\nattention mechanism. Our work focuses on a particular class of methods that aim to approximate the\nattention computation by reducing the number of tokens used in the attention computation, trading\n∗Equal contribution\nPreprint. Under review.\n--- Page 2 ---\nFigure 1: Overview of SiftAttention\noff accuracy for efficiency. These methods either prune the KV-cache permanently [ 29] for memory\nsavings or dynamically select a subset of tokens per generation step [ 18,10,22] to reduce the amount\nof key-value pairs to attend to. The latter set of methods typically use a top- kselection operation\nto select the top ktokens with the highest attention scores, which is inefficient on GPUs due to\nthread-synchronization overheads. We introduce SiftAttention , a new approximation strategy that\nreplaces top- kwith an element-wise filtering operation.\nOur approach is based on a key empirical finding: the τ-th quantile of attention scores follow a\npredictable power-law decay over generation steps. This trend is consistent across models and datasets\nand enables us to estimate quantile thresholds dynamically using a small warmup window. Based on\nthis insight, we design SiftAttention with two-phases: a short warmup , and an approximate generation\nphase. During the warmup, seen on the left of Figure 1, we record the τ-th quantile of attention scores\nat each generation step and fit a power-law curve to these samples. During approximate generation,\nseen on the right of Figure 1, we use our fitted power-law to predict the τ-th quantile of attention\nscores at each generation step, retaining only those keys whose attention weights are above this\nthreshold and loading their corresponding value vectors to SRAM.\nWe evaluate SiftAttention on perplexity, short-context, and long-context generation tasks across\nmultiple models. Our results demonstrate that SiftAttention maintains a high level of model quality,\nwith negligible degradation in perplexity (within 0.1) and task-specific metrics. We also implement a\nfused Triton [ 1] kernel for SiftAttention to demonstrate that our method reduces data movement in\nthe GPU memory hierarchy when loading value vectors, similar to other top- kmethods.\nIn summary, our contributions are as follows:\n•We discover and analyze that attention score quantiles follow a power-law trend across\ngeneration steps and demonstrate that this can be estimated using a few warmup steps.\n•We propose SiftAttention, an approximate attention method that avoids the expensive top- k\nsorting operation by compute-efficient filtering using a quantile-based threshold value.\n•We demonstrate SiftAttention’s effectiveness across multiple benchmarks, including perplex-\nity, short-context, and long-context generation tasks, and showing that it maintains model\nquality better than existing approximate attention methods.\n2 Background and Related Work\nAuto-regressive Decoding: During auto-regressive decoding, transformers generate one token at a\ntime by applying causal self-attention over all previously generated tokens [ 24]. At generation step S,\nthe attention output is computed as:\noS= softmax\u0012qSKT\n:S√\nD\u0013\nV:S (1)\nwhere qS∈R1×Dis the query, and K:S,V:S∈RS×Dare the key and value caches, respectively.\n2\n--- Page 3 ---\nPower-Law Decay: A power-law decay describes relationships of the form: y=α·x−β, where\nα, β∈Randα, β > 0. Assuming a multiplicative error in y, this can be linearized in log-log space\nto enable efficient parameter estimation via linear regression, as:\nlogy= log α−βlogx (2)\n2.1 Related Work\nEmpirical analyses of pre-trained transformers show that only a small subset of past keys receive\nthe majority of the attention weight [ 14]. This has motivated a range of approximate attention\nmethods that exploit this sparsity to either reduce the cost of full attention [ 7,18,22,10] or prune\nunimportant key-value pairs to reduce memory overhead [ 12,29]. While traditional methods focus\non patterns within a single generation step, very few methods explore how attention scores evolve\nacross generation steps. To the best of our knowledge, our work is the first to empirically demonstrate\nthat attention score quantiles exhibit a predictable power-law decay over time and to then use this\ninsight to sparsify attention scores dynamically during inference.\nThe original Top -kmethod [ 7] selects the most relevant kkeys via a global sort over exact attention\nscores. Despite impressive downstream performance, the top -koperation introduces a full -row\ndependency that limits parallel execution on modern GPUs. While recent methods like Loki [ 22] and\nSparQ [ 18] aim to approximate attention scores in low-dimensional subspaces, they still rely on the\ntop-koperation to select the most important scores, which remains a bottleneck [ 28]. SiftAttention\naims to replaces the top- koperation with a simple element-wise threshold comparison, making it\nhighly parallelizable on GPUs.\nToken-eviction methods, such as Scissorhands [ 12] and the H 2O [29], reduce memory usage by\ndeleting tokens permanently, which can lead to a significant degradation in model quality. In contrast,\nSiftAttention retains the full KV-cache and sparsifies only during attention computation, prioritizing\ntask performance over memory savings.\nThe most closely related works to SiftAttention are LeOPArd [ 11] and Top- θAttention [ 3]. LeOPArd\nlearns fixed thresholds per transformer layer during training and drops scores below them at inference\ntime, achieving 1.9 ×speedups on specialized hardware. However, it introduces training/fine-tuning\noverhead and lacks flexibility to adapt thresholds based on sequence specific dynamics. Top- θ\nestimates thresholds per head, layer, and generation step using an offline calibration set with some\nlimited online fine-tuning. This requires storing calibrated thresholds for all generation steps, which\ncan be memory-intensive for long contexts. SiftAttention is the first fully online, dataset-agnostic, and\nmodel-agnostic sparsification technique that doesn’t require any offline step which can dynamically\nadapt to sequence/query dynamics.\n3 Power-Law Analysis\nIn this section, we investigate the evolution of attention score quantiles over sequential generation\nsteps and discover a consistent empirical pattern: the τ-th quantiles exhibit a decay that closely\nfollows a power-law. We then show the efficacy of capturing this trend using a power-law fit and\nintroduce a method to estimate the parameters of this fit with comparatively minimal overhead.\n3.1 Analyzing the Temporal Decay of Attention Score Quantiles\nTo examine whether attention scores exhibit consistent and predictable behavior over generation\nsteps, we analyze them across multiple models on a perplexity evaluation task. Specifically, we focus\non specific quantiles of the attention scores and fit power-law models to capture their decay over time.\nExperimental Setup: We evaluate five models spanning a diverse range of architectures and\nscales: Llama-3.1 (8B Instruct and 70B Instruct), Llama-3.2 3B Instruct, Mistral 7B, Qwen2.5\n7B [6,9,26]. Evaluations are conducted on WikiText-2 (test split) [ 13] and C4 (custom split) [ 17],\nusing a perplexity evaluation pipeline. Each model is run on sequences of length 4096 tokens.\nThe experiments are run on Nvidia GH200 and H100 GPUS, with larger models distributed across\nmultiple GPUs using AxoNN [19, 21].\nWe track the attention scores at selected quantile levels τ: 0.5, 0.75, and 0.875, denoting the τ-th\nquantile at generation step iasθi,τ. For each prompt, layer, and head, we extract the quantile time\n3\n--- Page 4 ---\nseries and fit a power-law curve of the form ˆθi,τ≈α·i−β. We fit the power-law by performing\nlinear regression in log-log space, which enables both efficient closed-form estimation using standard\ntensor operations and implicitly models multiplicative noise. To evaluate the quality of the fit, we\ncompute the coefficient of determination ( R2), defined as:\nR2= 1−P\ni(ℓi−ˆℓi)2\nP\ni(ℓi−¯ℓ)2(3)\nwhere ℓiis the logarithm of the τ-th quantile score at generation step i,ℓi= log( θi,τ),ˆℓiis the\nlogarithm of the predicted value from the power-law fit, and ¯ℓdenotes the mean value of ℓi. This\nmetric is computed for each prompt, layer, and head.\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-8B-Instruct (WikiText-2) \n Prompt ID=3, Layer=10, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\nLlama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-InstructMistral-7B-v0.3Qwen2.5-7B0.00.20.40.60.81.0R2R2 across models (WikiText-2)\nQuantile\n0.5 0.75 0.875\nLlama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-InstructMistral-7B-v0.3Qwen2.5-7B0.00.20.40.60.81.0R2R2 across models (C4)\nQuantile\n0.5 0.75 0.875\nFigure 2: Power-Law fit for attention score quantiles (log y-scale) over generation steps for Llama-3.1\n8B on WikiText-2 (left). Distribution of R2values for power-law fits across prompts, layers, and\nheads for various models on WikiText-2 (middle) and C4 (right) datasets. Boxes denote the median,\n25thand 75thpercentiles, and the whiskers denote the 5thand 95thpercentile values.\nAnalysis: We begin by analyzing the temporal trend in attention score quantiles for Llama-3.1 8B\non WikiText-2 (Figure 2, left). The plot depicts the evolution of the 0.5, 0.75, and 0.875 quantiles\nover generation steps for a specific prompt, layer, and head index. Each line reflects a power-law fit,\nplotted alongside the true quantile values. For visual clarity, the generation steps are down-sampled\nto onlyshow every 10thstep. Despite some local noise, we observe a remarkably consistent decline\nin quantile values over time, with high correspondence to the fitted power-law curves. Similar trends\nfor other models can be found in appendix A.\nTo demonstrate the generalizability of this observed trend, we report the R2values for power-law fits\naggregated across all layers, heads, and prompts for different models (Figure 2, middle and right).\nAcross models of different architectures and sizes, the power-law fit consistently achieves high R2\nvalues, with median values between 0.6 and 0.8. Although a small fraction of fits exhibit lower\nquality—particularly at the 5th percentile, which can fall to around 0.4 - the overall distribution of\nR2indicates that the power-law fit is a good approximation for the decay of attention score quantiles\nover generation steps. This trend further holds across both datasets used. This consistent result\nacross diverse settings supports the hypothesis that the quantile decay might be a fundamental and\narchitecture-agnostic property of transformer models.\nThis insight has significant practical implications: it suggests that the trajectory of attention score\nquantiles can be predicted reliably across different models, potentially enabling efficient threshold\nestimation without the need for an expensive top- koperation at each step. Next, we explore the\npossibility of estimating the power-law parameters using a small warmup phase.\n3.2 Estimating Power-Law Fit Parameters with a Warmup Phase\nHaving established that attention score quantiles follow an approximate power-law decay, we now\nfocus on estimating the parameters of this curve during inference. We explore whether a small number\nof initial generation steps can be used to estimate the power-law parameters accurately.\nFollowing the experimental setup described in Section 3.1, we now fit the power-law function using\nonly the values from the first wgeneration steps - henceforth referred to as the warmup phase. We\nevaluate four warmup lengths: w: 64, 128, 256, and 512. The power-law parameters are estimated\nusing only the warmup steps, but the resulting fit is evaluated across all generation steps (including\nboth warmup and post-warmup) by computing the coefficient of determination ( R2). This warmup-\nbased R2captures out-of-sample generalization and as a result, R2values can be negative when the\n4\n--- Page 5 ---\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Llama-3.1-8B-Instruct\nQuantile\n0.5\n0.75\n0.875\nLlama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-InstructMistral-7B-v0.3Qwen2.5-7B0.00.20.40.60.81.0R2R2 across models, Warmup=512 (WikiText-2)\nQuantile\n0.5 0.75 0.875Figure 3: R2of power-law fits for attention score quantiles with warmup. Left: R2distribution for\nLlama-3.1-8B-Instruct on WikiText-2, evaluated across warmup sizes of 64, 128, 256, and 512 steps.\nRight: Fit quality across models with a warmup of 512. Longer warmups consistently yield stronger\nfits across models.\npower-law fit performs worse than simply predicting the mean of the log-quantile values across the\nfull sequence.\nFigure 3 (left) shows how fit quality varies with the number of warmup steps for Llama-3.1-8B-\nInstruct on WikiText-2. With short warmup lengths (e.g., w=64), the R2values are often low, with\na significant fraction falling below zero (truncated in the plot to the range [0,1]). As the warmup\nlength increases, the fit quality improves steadily. The same pattern can be seen across various\nmodels, the results of which are shown in appendix A. With w=512, the median R2across all\nquantiles approaches or exceeds 0.6, indicating strong generalization. Notably, 512 warmup steps\nstill correspond to just 12.5% of the full 4096-token sequence, suggesting that accurate power-law\nparameter estimation requires only a small fraction of the total generation steps. Later, we will see\nthat empirically, this is an adequate warmup length to achieve high downstream performance.\n4 SiftAttention: Power-Law Guided Dynamic Filtering\nIn this section, we introduce SiftAttention, an approximate attention mechanism that exploits the\npower-law behavior of attention score distributions. We describe its two-phase design: a warmup\nphase for estimating power-law parameters, followed by an approximate generation phase that uses\nthese estimates to prune attention computations efficiently. We also analyze the computational\ncomplexity of SiftAttention and discuss its implementation in practical settings.\n4.1 Two-Phase Algorithm Design\nSiftAttention operates in two distinct phases: a warmup phase (Algorithm 1) and an approximate\ngeneration phase (Algorithm 2).\nWarmup Phase: During the warmup phase, we first compute the exact attention scores using the\ndot product of query and key vectors in the KV-Cache (Lines 1–2), followed by a softmax operation\n(Line 3). We then record the τ-th quantile of the computed attention scores for each prompt, layer,\nand head (Lines 4–5). This step is repeated until the warmup phase ends ( S≤w).\nAt the end of the warmup phase, we fit a power-law curve to these recorded quantile values across\ngeneration steps to obtain parameters αandβ(Line 7). The fitting is performed efficiently by\napplying a logarithmic transform to both the generation step index Sand quantile values, followed\nby linear regression in the subsequent log-log space. This method provides a closed-form solution\nusing standard tensor operations, eliminating iterative optimization, and aligns naturally with the\nmultiplicative noise assumption inherent in modeling probabilities.\nApproximate Generation Phase. Once the warmup phase ends and the power-law parameters have\nbeen fit, the model enters the approximate generation phase (Algorithm 2). At each subsequent\ngeneration step, we use the learned power-law model to estimate the τ-th quantile of attention scores\n5\n--- Page 6 ---\nAlgorithm 1 SiftAttention: Warmup Phase\nInput: At the Sthstep - Input: xS∈R1×D, KV-cache: K:S−1,V:S−1∈R(S−1)×D, Warmup\nSteps w, Quantile Level τ, Past Quantile Scores: θ:S−1∈R(S−1)×1\nEnsure: S≤w\n1:qS,kS,vS←COMPUTE QKV (xS)\n2:K:S←APPEND (K:S−1,kS),V:S←APPEND (V:S−1,vS)\n3:aS←SOFTMAX\u0010\nqSK⊤\n:S√\nD\u0011\n4:θS←QUANTILE (aS, τ) ▷ τ-th quantile of attention scores\n5:θ:S←CONCAT (θ:S−1, θS)\n6:ifS=wthen ▷Warmup complete\n7: α, β←FITPOWER LAW(θ:S)\n8: return aSV:S, α, β\n9:else\n10: return aSV:S\n11:end if\nAlgorithm 2 SiftAttention: Approximate Generation Phase\nInput: At the Sthstep - Input: xS∈R1×D, KV-cache: K:S−1,V:S−1∈R(S−1)×D, Warmup\nSteps w, Powerlaw Fit Parameters - α, β\nEnsure: S > w\n1:qS,kS,vS←COMPUTE QKV (xS)\n2:K:S←APPEND (K:S−1,kS),V:S←APPEND (V:S−1,vS)\n3:aS←SOFTMAX\u0010\nqSK⊤\n:S√\nD\u0011\n4:ηS←α·S−β▷Power-Law Estimation of Quantile\n5:indices ← {i|aS[i]> ηS} ▷Filtering\n6:a′\nS←aS[indices ],V′\n:S←V:S[indices ] ▷Retain only the indices above threshold\n7:return a′\nSV′\n:S\nbased on the current step index (Line 4). This predicted quantile serves as a dynamic threshold that\nadapts over time, reflecting the prior empirical observations in attention score distributions.\nUsing this threshold, we prune the attention weights by zeroing out all values below the predicted\nquantile (Line 5). Only the attention scores above the threshold are retained, and the corresponding\nvalue vectors are extracted from the KV-cache (Line 6). This selective sparsification reduces the\nnumber of values that need to be loaded from high-bandwidth memory (HBM) into on-chip SRAM,\njust as in top- kmethods. Finally, the attention output is computed using only the surviving attention\nscores and their associated value vectors (Lines 7).\n4.2 Computational Cost Analysis\nWe now present a run-time cost analysis of SiftAttention, compared to the Top- kmethod. Let Sbe\nthe total number of generation steps (or tokens in the KV-cache), wthe number of warmup steps.\nThe warmup phase of SiftAttention and the Top- kmethod are almost identical, with two main\ndifferences: SiftAttention requires an extra step of storing the τ-th quantile of attention scores at\neach step, and the final value projection uses all the attention scores (Line 8 & 10, Algorithm 1) (as\nopposed to Top- kwhich uses a fixed kscores). Given that the quantile score can be directly stored in\na buffer, without any additional copying, we can approximate the difference in runtimes as:\nTSiftAttention, Warmup −TTop-kAttention ≈(Tproj-V−Tproj-V′) (4)\nwhere Tproj-Vis the runtime cost of computing the vector-matrix product between the full attention\nscores and the value matrix, and Tproj-V′is the cost of the same operation between the pruned attention\nscores and the value matrix.\n6\n--- Page 7 ---\nDuring the approximate generation phase, SiftAttention thresholds the attention scores before value\nprojection, instead of a top- kselection, and thus the difference in runtime can be approximated as:\nTSiftAttention, ApproxGen −TTop-kAttention ≈Tthreshold −Ttop-k (5)\nAll the other operations are the same as in the Top- kmethod. Based on these approximations, we\ncan compare the runtime of SiftAttention with the Top- kmethod over Sgeneration steps (with w\nwarmup steps), as follows:\nT(S)\nSiftAttention −T(S)\nTopKAttention ≈w(Tproj-V−Tproj-V′) + (S−w)(Tthreshold −Ttop-k) +Tpower-law-fit (6)\nGiven w≪S, the first term becomes negligible and the power-law fit is a one-time operation using\nfast tensor operations. Thus, the runtime difference is dominated by the last two terms:\nT(S)\nSiftAttention −T(S)\nTopKAttention ≈(S−w)Tthreshold −Ttop-k (7)\nSince thresholding can be done element-wise, in parallel and does not require sorting like top- k, we\nhaveTtop-k> T threshold , leading a negative RHS in the above equation.\n4.3 Implementation Details\nNext, we discuss key implementation details of SiftAttention. We develop a Triton-based kernel\nfor the approximate generation phase. For the warmup phase and power-law fit, we use standard\nPyTorch [ 16] operations with torch.compile [23] enabled for performance. A key thing to note\nis that our triton kernel is not the most efficient implementation of this approach. The reason being\nthat, for an efficient implementation of SiftAttention (and any top- kbased approach), the pruned\nattention scores and value vectors need to be compacted in memory (Line 6, Algorithm 2). If this is\nnot done, the resulting vector-matrix product will be a sparse-sparse product, which is not efficient on\nmodern GPU architectures when compared to a dense vector-matrix products. Triton does not allow\nfor fine-grained manipulation of shared memory, leading us to write the intermediate indices in global\nmemory. This limitation is not present if implemented directly in CUDA, and we leave this as future\nwork. Due to this, we will see later that our approach cannot compete with highly optimized fused\nstandard attention kernels. However, our implementation works as a proof-of-concept, showing that\nour approach can lead to a reduction in HBM to SRAM data movement, as seen in our experiments.\n5 Experimental Setup\nFor model quality benchmarking, we evaluate SiftAttention on two categories of tasks: perplexity and\ndownstream generation. Evaluations are conducted on a subset of open-weight models: Llama-3.1\n(8B Instruct and 70B Instruct), Llama-3.2 3B Instruct, Mistral 7B, Qwen2.5 7B [ 6,9,26]. Due to\ntokenizer incompatibility, the Qwen model is excluded from downstream generation tasks. The main\ntext presents results for the Llama family, while results for other models are provided in appendix B.\nWe evaluate perplexity using the WikiText-2 [ 13] dataset. Downstream generation evaluations are\ndivided into short-context and long-context settings. Short-context tasks include IFEval [ 30] and\nMATH-Hard [ 4], selected from the Hugging Face Open LLM Leaderboard [ 5]. The long-context\nsetting employs LongGenBench [25], capped at 4096 tokens due to computational constraints.\nWe compare against three baselines: Full Attention (no approximations), Top- kAttention [ 7], and\nH2O [29]. H 2O is excluded for Llama-3.1-70B-Instruct due to its lack of multi-GPU support, and\nfrom downstream generation tasks due to computational constraints. Top- kis evaluated only on\nperplexity and short-context tasks, due to computational constraints. To ensure fair evaluation, we\ndiscard samples where the number of generated tokens is less than the maximum warmup length\nin our experiments and report metrics only on valid completions, for both SiftAttention and all\nbaselines. For Top- kwe use k∈ {0.5,0.25,0.125,0.05}, and an analogous retain budget for H 2O.\nFor SiftAttention, we use τ∈ {0.5,0.75,0.875,0.95}and warmup w∈ {16,32,64,128,256,512}.\nFor runtime benchmarking, we evaluate our Triton-based implementation of the approximate gener-\nation phase of SiftAttention on a micro-benchmark, written using Triton’s benchmarking utilities.\nWe compare our method against the fused scaled dot-product attention kernel (SDPA) from PyTorch.\nWe enable torch.compile with max-autotune for both implementations. We profile the memory\ntransfer volume using Nsight Compute [ 15]. Model quality runs are performed on GH200, and H100\nGPUs. Runtime experiments are performed on A100 GPUs. Bigger models are parallelized across 4\nH100 GPUs, using AxoNN [20, 19].\n7\n--- Page 8 ---\n6 Results\nWe now compare SiftAttention against full attention and other baselines, and evaluate the performance\nof our Triton-based implementation relative to PyTorch’s fused scaled dot-product attention kernels.\n0.0 0.2 0.4 0.6 0.8 1.0\nIntended Sparsity0.00.20.40.60.81.0Realized Sparsity\nRealized vs. Intended Sparsity \n Llama-3.1-8B-Instruct\nWarmup\n32\n64\n128\n256\nTask\nPPL\nIFEval\nMATH\nLongGenTask\nPPL\nIFEval\nMATH\nLongGen\n2000 4000 6000 8000\nContext Length0.00.51.01.5Time (ms)\nAttention Time (Batch Size = 8)\nSift (=0.0)\nSift (=0.5)\nSift (=0.95)\nTorchSDPA\n0.0 0.5 0.75 0.875 0.95\nRealized Sparsity0200400600800HBM Data Transfer (MB)840.8\n703.1\n633.9600.0 579.8SiftAttention Data Movement\nRead Write\nFigure 4: (Left) SiftAttention Intended vs. Realized sparsity across tasks and warmup lengths for\nLlama-3.1–8B-Instruct. (Middle) Post-warmup attention latency of our Triton-based SiftAttention\nimplementation, compared to PyTorch’s fused SDPA across varying context lengths (Batch Size =\n8). (Right) HBM data transfer volume as a function of realized sparsity (Context Length = 8192),\nshowing reduced memory reads with increased sparsity.\nIntended vs. Realized Sparsity: We begin by distinguishing between intended sparsity, set by\nhyper-parameters (e.g., kin Top- k,τin SiftAttention), and realized sparsity, the actual fraction\nof filtered tokens during inference. For static methods like Top- kand H 2O, intended and realized\nsparsity are equal by construction. However, in SiftAttention, sparsity is determined dynamically\nbased on thresholding learned from power-law fits and may deviate from the intended sparsity.\nWe compute realized sparsity as the average ratio of pruned keys to total keys across all generation\nsteps and samples. Figure 4 (left) shows this relationship for SiftAttention on Llama-3.1-8B-Instruct\nacross various tasks and warmup lengths. Perplexity-oriented tasks tend to undershoot the intended\nsparsity, while downstream tasks like IFEval and LongGenBench often surpass it. While we do not\nyet have a theoretical explanation for these task-specific trends, a consistent observation is that longer\nwarmup phases yield realized sparsity closer to the target. This aligns with earlier findings that longer\nwarmups lead to better power-law fits for attention quantiles. Going forward, we report realized\nsparsity when comparing SiftAttention with baselines to keep comparisons fair.\nRuntime and Data Transfer Volume Evaluation: Figure 4 (middle) compares SiftAttention’s\nruntime (post-warmup) against PyTorch’s fused Scaled Dot-Product Attention (SDPA) kernel (exact\nattention). As discussed in Section 4.3, due to Triton’s limited support for shared memory and\ncompaction, our kernel is slower. However, SiftAttention’s runtime improves with sparsity: from\n1.46 ms to 1.30 ms (10% reduction) at context length 8192. Investigating further, Figure 4 (right)\nshows that this gain stems from reduced HBM data transfer—falling from 840.8 MB to 579.8 MB\n(31% reduction) with increasing sparsity. This demonstrates that SiftAttention is able to reduce the\nHBM to SRAM data movement, as well as other top- kbased approaches. But while our Triton\nimplementation does not fully exploit this, a CUDA backend could.\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity9.810.010.210.410.6Perplexity\n10.16Perplexity vs. Realized Sparsity \n Llama-3.2-3B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity6.76.86.97.07.17.2Perplexity\n6.77Perplexity vs. Realized Sparsity \n Llama-3.1-8B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3.63.84.0Perplexity\n3.59Perplexity vs. Realized Sparsity \n Llama-3.1-70B-Instruct\nSift (w=32)\nSift (w=128)\nSift (w=512)\nTop-kH2O\nFull Attention\nAcceptable Increase\nFigure 5: Perplexity evaluation (WikiText-2) comparing SiftAttention with baselines for 3 models\nPerplexity Evaluation: We now turn to Figure 5, which compares perplexity on WikiText-2 for\nSiftAttention, full attention, and other baselines across three Llama models. Focusing on Llama-\n3.1-8B-Instruct (center), we see that longer warmup phases yield better perplexity for SiftAttention,\n8\n--- Page 9 ---\ndue to improved power-law fits. A short warmup of 32 steps (orange curve) leads to a noticeable\ndegradation, while warmups of 128 and 512 (green, blue) keep degradation within 0.1 [ 27] for most\nsparsity levels, rising to 0.2 only at the highest sparsity. H 2O performs worst, with drops exceeding\n0.5. Top- kmaintains consistently strong performance, even at the highest realized sparsity values.\nHowever, with a sufficient warmup, SiftAttention approaches Top- kperformance closely. Similar\ntrends are observed for Llama-3.1-70B-Instruct (right). Interestingly, for Llama-3.2-3B-Instruct\n(left), both SiftAttention and Top- kachieve lower perplexity than the full attention baseline, with\nSiftAttention outperforming Top- k. Across all models, SiftAttention remains in the acceptable range\nof perplexity degradation for most sparsity levels. Results for other models are similar and can be\nfound in appendix B.\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity444648505254Accuracy\n 49.90IFEval Accuracy vs. Realized Sparsity \nLlama-3.2-3B-Instruct\nSift (w=32)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity424446485052Accuracy\n 47.58IFEval Accuracy vs. Realized Sparsity \nLlama-3.1-8B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity6870727476Accuracy\n72.18IFEval Accuracy vs. Realized Sparsity \nLlama-3.1-70B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity1112131415Accuracy\n13.55MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.2-3B-Instruct\nSift (w=32)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity1314151617Accuracy\n15.18MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.1-8B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3132333435Accuracy\n 33.38MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.1-70B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3436384042Accuracy\n39.20LongGenBench Accuracy vs. Realized Sparsity \n Llama-3.2-3B-Instruct\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3840424446Accuracy\n44.86LongGenBench Accuracy vs. Realized Sparsity \n Llama-3.1-8B-Instruct\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity505254Accuracy\n53.78LongGenBench Accuracy vs. Realized Sparsity \n Llama-3.1-70B-Instruct\nFigure 6: Short (IFEval, MATH) and long (LongGenBench) generation evaluation across 3 models\nDownstream Generative Task Evaluation: Finally, we evaluate SiftAttention on downstream\ngenerative tasks—IFEval, MATH-Hard, and LongGenBench—shown in Figure 6. IFEval and\nMATH-Hard involve shorter generation lengths (up to 1280 and 1024 tokens), while LongGenBench\nrequires generating up to 4096 tokens. For IFEval (top row), focusing on Llama-3.2-3B-Instruct\n(left), we see that SiftAttention outperforms the Top- kbaseline given a sufficient warmup of 128\nsteps (blue curve), especially at high realized sparsities. This trend is consistent across the other\nmodels. On MATH-Hard (middle row), a similar trend can be observed, SiftAttention closely\nmatching or outperforming the Top- kbaseline. particularly, for Llama-3.1-70B-Instruct, we see a\nsevere degradation in performance with Top- kat high sparsities, but SiftAttention maintains strong\nperformance. Finally, on LongGenBench (bottom row), SiftAttention maintains strong performance\neven with the longer generation lengths. The average drop in accuracy is 2-3% with a warmup of\n256 steps (teal blue curve) across models at high sparsity values. With 512 warmup steps, it nearly\nmatches the full attention baseline.\nTogether with the perplexity results, these findings highlight SiftAttention’s ability to retain strong\ntask performance across models and sparsity levels, often outperforming Top- k, while being easier to\nparallelize on GPUs.\n9\n--- Page 10 ---\n7 Conclusion\nThis work introduces SiftAttention, a dynamic power-law guided sparse attention algorithm that\nreduces GPU memory movement between HBM and SRAM. We observe that attention score quantiles\nfollow a predictable power-law decay over generation steps, consistently across models and datasets -\na novel finding that can motivate future work in sparse attention methods. Leveraging this insight,\nSiftAttention uses a brief warmup phase to fit a power-law model, then predicts thresholds to prune\nattention scores without requiring costly top- ksorting. Our experiments show that SiftAttention\nmaintains model quality, similar to or better than existing sparse attention techniques, while reducing\nthe data movement between the GPU HBM and SRAM.\n7.1 Limitations and Future Work\nOur Triton-based implementation of SiftAttention is not fully optimized and performs sub-optimally\ncompared to PyTorch’s SDPA kernel. This shortfall arises from inherent limitations in Triton and\ninefficiencies in sparse vector-matrix multiplication. To address some of these performance issues,\nwe are actively developing a CUDA-based kernel. Additionally, SiftAttention relies on a fixed-length\nwarmup phase, necessitating careful tuning specific to each model and task. An interesting future\ndirection is the exploration of more advanced online fitting techniques, allowing the power-law\nparameters to be adaptively re-calibrated throughout generation based on discrepancies between\npredicted and observed attention quantiles. Finally, while our current power-law model is based\non post-softmax attention scores, future work could focus on estimating thresholds directly from\npre-softmax scores, potentially yielding further computational efficiencies.\nReferences\n[1]Introducing triton: Open-source gpu programming for neural networks. https://openai.\ncom/index/triton/ , 2021.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings , 2015.\n[3]Konstantin Berestizshevsky, Renzo Andri, and Lukas Cavigelli. Top-theta attention: Sparsifying\ntransformers by compensated thresholding. arXiv preprint arXiv:2502.08363 , 2025.\n[4]Jingxuan Fan, Sarah Martinson, Erik Y Wang, Kaylie Hausknecht, Jonah Brenner, Danxian\nLiu, Nianli Peng, Corey Wang, and Michael P Brenner. Hardmath: A benchmark dataset for\nchallenging problems in applied mathematics. arXiv preprint arXiv:2410.09988 , 2024.\n[5]Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open\nllm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/open_\nllm_leaderboard , 2024.\n[6]Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. The llama 3 herd of models, 2024.\n[7]Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient\ntransformers via top-k attention. CoRR , abs/2106.06899, 2021.\n[8]Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement\nis all you need: A case study on optimizing transformers. Proceedings of Machine Learning\nand Systems , 3:711–732, 2021.\n[9]Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\n[10] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. InfiniGen: Efficient generative\ninference of large language models with dynamic KV cache management. In 18th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 24) , pages 155–172, Santa\nClara, CA, July 2024. USENIX Association.\n10\n--- Page 11 ---\n[11] Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, and Mingu Kang.\nAccelerating attention through gradient-based learned runtime pruning. In Proceedings of the\n49th Annual International Symposium on Computer Architecture , pages 902–915, 2022.\n[12] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios\nKyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance\nhypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118 , 2023.\n[13] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. CoRR , abs/1609.07843, 2016.\n[14] Yury Nahshan, Joseph Kampeas, and Emir Haleva. Linear log-normal attention with unbiased\nconcentration. arXiv preprint arXiv:2311.13541 , 2023.\n[15] NVIDIA. Nvidia nsight compute. https://developer.nvidia.com/nsight-compute .\n[16] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\n[17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer, 2023.\n[18] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas\nOrr. Sparq attention: Bandwidth-efficient llm inference, 2023.\n[19] Siddharth Singh and Abhinav Bhatele. AxoNN: An asynchronous, message-driven parallel\nframework for extreme-scale deep learning. In Proceedings of the IEEE International Parallel\n& Distributed Processing Symposium , IPDPS ’22. IEEE Computer Society, May 2022.\n[20] Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin\nWen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, and Abhinav\nBhatele. Democratizing AI: Open-source scalable LLM training on GPU-based supercomputers.\nInProceedings of the ACM/IEEE International Conference for High Performance Computing,\nNetworking, Storage and Analysis , SC ’24, November 2024.\n[21] Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, and Abhinav Bhatele. A\n4d hybrid algorithm to scale parallel training to thousands of gpus, 2024.\n[22] Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki:\nLow-rank keys for efficient sparse attention. In A. Globerson, L. Mackey, D. Belgrave, A. Fan,\nU. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing\nSystems , volume 37, pages 16692–16723. Curran Associates, Inc., December 2024.\n[23] PyTorch Team. Pytorch 2.0: Our next generation release that is faster, more pythonic and\ndynamic as ever. https://pytorch.org/get-started/pytorch-2.0/ , 2023.\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems , pages 5998–6008, 2017.\n[25] Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee. Longgenbench: Benchmarking\nlong-form generation in long context llms. arXiv preprint arXiv:2409.02076 , 2024.\n[26] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[27] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring\npost-training quantization in llms from comprehensive study to low rank compensation. 2023.\n11\n--- Page 12 ---\n[28] Jingrong Zhang, Akira Naruse, Xipeng Li, and Yong Wang. Parallel top-k algorithms on gpu:\nA comprehensive study and new methods. In Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and Analysis , SC ’23, New York, NY , USA,\n2023. Association for Computing Machinery.\n[29] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao\nSong, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2o: Heavy-hitter oracle for\nefficient generative inference of large language models. arXiv preprint arXiv:2306.14048 , 2023.\n[30] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models, 2023.\n12\n--- Page 13 ---\nA Extended Power-Law Analysis Results\nA.1 Power-Law Samples\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.2-3B-Instruct (WikiText-2) \n Prompt ID=3, Layer=9, Head=16\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.2-3B-Instruct (WikiText-2) \n Prompt ID=6, Layer=18, Head=8\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.2-3B-Instruct (C4) \n Prompt ID=3, Layer=9, Head=16\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.2-3B-Instruct (C4) \n Prompt ID=6, Layer=18, Head=8\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-8B-Instruct (WikiText-2) \n Prompt ID=3, Layer=21, Head=10\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-8B-Instruct (WikiText-2) \n Prompt ID=6, Layer=10, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-8B-Instruct (C4) \n Prompt ID=3, Layer=21, Head=10\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-8B-Instruct (C4) \n Prompt ID=6, Layer=10, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-70B-Instruct (WikiText-2) \n Prompt ID=3, Layer=53, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-70B-Instruct (WikiText-2) \n Prompt ID=6, Layer=26, Head=42\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-70B-Instruct (C4) \n Prompt ID=3, Layer=53, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Llama-3.1-70B-Instruct (C4) \n Prompt ID=6, Layer=26, Head=42\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Mistral-7B-v0.3 (WikiText-2) \n Prompt ID=3, Layer=10, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Mistral-7B-v0.3 (WikiText-2) \n Prompt ID=6, Layer=21, Head=10\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Mistral-7B-v0.3 (C4) \n Prompt ID=3, Layer=10, Head=21\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Mistral-7B-v0.3 (C4) \n Prompt ID=6, Layer=21, Head=10\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Qwen2.5-7B (WikiText-2) \n Prompt ID=3, Layer=9, Head=18\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Qwen2.5-7B (WikiText-2) \n Prompt ID=6, Layer=18, Head=9\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Qwen2.5-7B (C4) \n Prompt ID=3, Layer=9, Head=18\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\n0 1000 2000 3000 4000\nGeneration Step106\n105\n104\n103\n102\n101\n100Attention ScorePower-Law Fit: Qwen2.5-7B (C4) \n Prompt ID=6, Layer=18, Head=9\nTrue Values Power-Law fit\nQuantile\n0.50\n0.75\n0.875\nFigure 7: Power-Law fit for attention score quantiles (log y-scale) over generation steps across 5\ndifferent models. The two left columns are samples on WikiText-2, and the two right columns are\nsamples on C4.\nIn this section, we present extended results from our power-law analysis of attention score quantiles.\nFigure 7 displays representative samples of the observed quantiles across various models, datasets,\nprompts, layers, and attention heads. These qualitative results suggest that a power-law trend can\noften be visually inferred from the evolution of attention score quantiles. While the trend appears\nweaker in some samples from the C4 dataset, this may be attributable to suboptimal sample selection\nrather than a lack of underlying structure.\nAlthough this analysis is not exhaustive, it offers evidence that power-law behavior is a consistent\ncharacteristic of attention score distributions. As reported in Figure 2 of the main text, the R2\nvalues—indicating the goodness-of-fit for the power-law model—consistently exceed 0.6 across all\nevaluated models and datasets. Notably, the C4 dataset exhibits tighter variance in R2values.\nA.2 Power-Law Fitting with Warmup\nIn this section, we further investigate how the length of the warmup period affects the estimation of\nthe power-law fit parameters, for a perplexity evaluation task.\nFigure 8 shows a clear correlation between warmup length and power-law fit quality. With shorter\nwarmup periods (e.g., 64 steps), we observe significantly lower R2scores—often dropping into\n13\n--- Page 14 ---\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Llama-3.2-3B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (C4) \n Llama-3.2-3B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Llama-3.1-8B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (C4) \n Llama-3.1-8B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Llama-3.1-70B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (C4) \n Llama-3.1-70B-Instruct\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Mistral-7B-v0.3\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (C4) \n Mistral-7B-v0.3\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (WikiText-2) \n Qwen2.5-7B\nQuantile\n0.5\n0.75\n0.875\n64 128 256 512\nWarmup0.00.20.40.60.81.0R2R2 with Warmup (C4) \n Qwen2.5-7B\nQuantile\n0.5\n0.75\n0.875Figure 8: R2distribution across 5 different models, evaluated across warmup sizes w: 64, 128, 256,\nand 512 steps. Left columns are samples on WikiText-2; right columns are samples on C4\nnegative values. These negative values indicate that the power-law fit performs worse than a naive\nmodel that simply predicts the mean of the log-quantile values, especially highlighting unreliable\nparameter estimation with short warmup periods. This degradation is especially evident in the C4\ndataset, where initial R2scores are consistently lower than those observed on WikiText-2.\n14\n--- Page 15 ---\nAs the warmup length increases, the fit quality steadily improves across all datasets. By 512\nwarmup steps, R2values become consistently positive and substantially higher, indicating that a\nlonger warmup allows for more accurate modeling of the attention score quantile decay. This trend\nunderscores the importance of allocating a sufficient warmup period for reliable threshold estimation\nin SiftAttention. We also observe that the fit quality is better on WikiText-2 than on C4. While these\nresults demonstrate the feasibility of the warmup approach in SiftAttention, we acknowledge that\nthe fitting accuracy could be improved. More sophisticated power-law fitting algorithms could be\nexplored in future work, to bridge the gap in estimating the real power-law trend with a high R2(as\nshown in the previous section). One possible approach could explore using realized sparsity signals\nto implement an adaptive warmup schedule\nDespite occasional poor fit quality, we observe strong downstream performance even with shorter\nwarmup lengths, as shown in Section 6 and Appendix B. We hypothesize that this robustness arises\nbecause, although the realized sparsity may deviate from the intended target, the model still retains a\nmeaningful and consistent subset of top- τattention scores. In effect, the power-law may be accurately\nmodeling a different effective sparsity level than initially specified.\nAnother limitation of this analysis is that we only analyze the power-law fit on a perplexity task. We\nobserve that the gap between realized and intended sparsity is larger on perplexity evaluation than on\nother tasks such as IFEval and MATH. We hypothesize that these results are linked and that the same\nanalysis performed on other tasks like IFEval and MATH could yield significantly better R2scores.\nB Detailed Evaluation Results\nB.1 Realized vs. Intended Sparsity\n0.0 0.2 0.4 0.6 0.8 1.0\nIntended Sparsity0.00.20.40.60.81.0Realized Sparsity\nRealized vs. Intended Sparsity \n Llama-3.2-3B-Instruct\nWarmup\n32\n64\n128\n256\nTask\nPPL\nIFEval\nMATH\nLongGenTask\nPPL\nIFEval\nMATH\nLongGen\n0.0 0.2 0.4 0.6 0.8 1.0\nIntended Sparsity0.00.20.40.60.81.0Realized Sparsity\nRealized vs. Intended Sparsity \n Llama-3.1-8B-Instruct\nWarmup\n32\n64\n128\n256\nTask\nPPL\nIFEval\nMATH\nLongGenTask\nPPL\nIFEval\nMATH\nLongGen\n0.0 0.2 0.4 0.6 0.8 1.0\nIntended Sparsity0.00.20.40.60.81.0Realized Sparsity\nRealized vs. Intended Sparsity \n Llama-3.1-70B-Instruct\nWarmup\n32\n64\n128\n256\nTask\nPPL\nIFEval\nMATH\nLongGenTask\nPPL\nIFEval\nMATH\nLongGen\n0.0 0.2 0.4 0.6 0.8 1.0\nIntended Sparsity0.00.20.40.60.81.0Realized Sparsity\nRealized vs. Intended Sparsity \n Mistral-7B-v0.3\nWarmup\n32\n64\n128\n256\nTask\nPPL\nIFEval\nMATH\nLongGenTask\nPPL\nIFEval\nMATH\nLongGen\nFigure 9: Intended vs. Realized sparsity on SiftAttention across tasks and warmup lengths for 5\ndifferent models.\nIn this section we analyze the relationship between intended and realized sparsity for SiftAttention\nacross multiple models, tasks, and warmup lengths, as shown in Figure 9. We observe that the\ndisparity between intended and realized sparsity varies significantly across models. Generation tasks\nconsistently demonstrate realized sparsity closer to the intended sparsity, suggesting a potential\nrelationship between task type and sparsity behavior. This trend warrants further investigation.\n15\n--- Page 16 ---\nNotably, the Llama-3.1-70B model exhibits a weaker correlation between realized and intended\nsparsity compared to smaller models. While this could indicate different underlying dynamics in\nlarger models, these results are not conclusive and require additional study to fully understand the\nscaling behavior of attention patterns.\nThe effectiveness of SiftAttention is evident across all models: increasing the warmup length\nconsistently brings realized sparsity closer to intended levels. Furthermore, we observe that increasing\nthe intended sparsity hyper-parameter (specified by the τ-th quantile) leads to proportional increases\nin realized sparsity, demonstrating the method’s ability to effectively control sparsification across\ndifferent scales and architectures.\nB.2 Perplexity Evaluation\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity9.810.010.210.410.6Perplexity\n10.16Perplexity vs. Realized Sparsity \n Llama-3.2-3B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nTop-k\nH2O\nFull Attention\nAcceptable Increase\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity6.76.86.97.07.17.2Perplexity\n6.77Perplexity vs. Realized Sparsity \n Llama-3.1-8B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nTop-k\nH2O\nFull Attention\nAcceptable Increase\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3.63.84.0Perplexity\n3.59Perplexity vs. Realized Sparsity \n Llama-3.1-70B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nTop-k\nFull Attention\nAcceptable Increase\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity4.95.05.15.25.35.4Perplexity\n4.96Perplexity vs. Realized Sparsity \n Mistral-7B-v0.3\nSift (w=32)\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nTop-k\nH2O\nFull Attention\nAcceptable Increase\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity6.36.46.56.66.76.8Perplexity\n6.39Perplexity vs. Realized Sparsity \n Qwen2.5-7B\nSift (w=32)\nSift (w=64)\nSift (w=128)\nSift (w=256)\nSift (w=512)\nTop-k\nH2O\nFull Attention\nAcceptable Increase\nFigure 10: Perplexity evaluation (WikiText-2) comparing SiftAttention with baselines across 5 models\nFigure 10 presents the perplexity evaluation results on WikiText-2 across five different models.\nConsistent with the results shown in Figure 5 of the main text, we find that SiftAttention achieves\nperplexity comparable to or better than the Top- kbaseline (dark purple). The H 2O method (light\npink) performs the worst, with the perplexity degradation exceeding acceptable levels—even at low\nrealized sparsity. Additionally, we observe that across all models, longer warmup lengths correlate\n16\n--- Page 17 ---\nwith improved perplexity, underscoring the importance of sufficient warmup for accurate threshold\nestimation.\nB.3 Short Context Tasks\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity444648505254Accuracy\n 49.90IFEval Accuracy vs. Realized Sparsity \nLlama-3.2-3B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity424446485052Accuracy\n 47.58IFEval Accuracy vs. Realized Sparsity \nLlama-3.1-8B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity6870727476Accuracy\n72.18IFEval Accuracy vs. Realized Sparsity \nLlama-3.1-70B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity1012141618Accuracy\n14.07IFEval Accuracy vs. Realized Sparsity \nMistral-7B-v0.3\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity1112131415Accuracy\n13.55MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.2-3B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity1314151617Accuracy\n15.18MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.1-8B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity3132333435Accuracy\n 33.38MATH-Hard Accuracy vs. Realized Sparsity \nLlama-3.1-70B-Instruct\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\n0.0 0.2 0.4 0.6 0.8 1.0\nRealized Sparsity0.00.51.01.52.02.53.0Accuracy\n1.44MATH-Hard Accuracy vs. Realized Sparsity \nMistral-7B-v0.3\nSift (w=32)\nSift (w=64)\nSift (w=128)\nTop-k\nFull Attention\nFigure 11: SiftAttention short-context task performance vs. Realized Sparsity on IFEval and MATH-\nHard across 4 different models\nIn this section, we present detailed results for our short-context task evaluation. Figure 11 illustrates\nthe performance of SiftAttention on the IFEval (top two rows) and MATH-Hard (bottom two rows)\n17\n--- Page 18 ---\ntasks across four different models. For the IFEval task, we observe that with a warmup length\nofw= 128 , SiftAttention consistently outperforms the Top- kbaseline (dark purple) across all\nLlama models, and matches its performance on the Mistral model at high realized sparsity levels. A\nsimilar trend holds for the MATH-Hard task, with one exception: on Llama-3.1-8B, SiftAttention\nperforms slightly worse than Top- kat the highest sparsity levels, but still surpasses the Full Attention\nbaseline. Notably, for Llama-3.1-70B, Top- kaccuracy degrades significantly at high sparsity, whereas\nSiftAttention maintains performance close to the Full Attention baseline.\nB.4 Runtime and Data Transfer Evaluation\n2000 4000 6000 8000\nContext Length0.10.2Time (ms)\nAttention Time (Batch Size = 1)\nSift (=0.0)\nSift (=0.5)\nSift (=0.95)\nTorchSDPA\n0.0 0.5 0.75 0.875 0.95\nRealized Sparsity050100150200HBM Data Transfer (MB)106.3\n89.080.5 76.4 73.9SiftAttention Data Movement\nRead Write\n2000 4000 6000 8000\nContext Length012Time (ms)\nAttention Time (Batch Size = 16)\nSift (=0.0)\nSift (=0.5)\nSift (=0.95)\nTorchSDPA\n0.0 0.5 0.75 0.875 0.95\nRealized Sparsity050010001500HBM Data Transfer (MB)1677.6\n1403.3\n1270.81199.71159.1SiftAttention Data Movement\nRead Write\nFigure 12: Post-warmup attention latency of our Triton-based SiftAttention implementation compared\nto PyTorch’s fused SDPA across varying context lengths for batch size 1 (top-left) and 16 (bottom-\nleft). HBM data transfer volume as a function of realized sparsity (Context Length = 8192) for batch\nsize 1 (top-right) and 16 (bottom-right).\nFigure 12 presents the post-warmup attention latency of our implementation compared to PyTorch’s\nfused SDPA kernel, evaluated across varying context lengths for two batch sizes: 1 (top-left) and\n16 (bottom-left). The trends observed here are consistent with those in Figure 4 from the main text,\nwhich reports results for batch size 8. As expected, increasing the sparsity level reduces attention\nlatency with SiftAttention. However, our current kernel implementation is approximately 2-3 ×\nslower than PyTorch’s highly optimized fused SDPA. The corresponding HBM data transfer volume\nis shown in the top-right and bottom-right subplots. Again, we observe that higher sparsity leads to\nreduced HBM traffic, confirming that SiftAttention effectively reduces memory bandwidth usage,\nwhich is the reason for the decrease in attention latency with higher sparsity.\nB.5 Visualization of Sparsity Masks\nIn this section, we visualize the attention sparsity patterns for two intended sparsity levels: τ= 0.5\nandτ= 0.9, during the generation of a 4096-token sequence using Llama-3.1-8B-Instruct. Figure 13\ndisplays the sparsity masks at the 2048thgeneration step. As expected, increasing the intended\nsparsity level results in a sparser attention pattern—evidenced by the greater prevalence of blacked-\nout (masked) tokens in the lower panel. All tokens beyond the 2048thposition are masked (shown in\nblack) as they represent future positions and are therefore not attended to.\n18\n--- Page 19 ---\n0 1000 2000 3000 4000\nSequence position (S)0\n5\n10\n15\n20\n25\n30Head (H)Mask Visualization (H x S)\n0.00.20.40.60.81.0\nMask (0=off, 1=on)\n0 1000 2000 3000 4000\nSequence position (S)0\n5\n10\n15\n20\n25\n30Head (H)Mask Visualization (H x S)\n0.00.20.40.60.81.0\nMask (0=off, 1=on)Figure 13: Sparsity masks for τ= 0.5(top) and τ= 0.9(bottom) at the 2048thgeneration step when\ngenerating a sequence of 4096 tokens using Llama-3.1-8B-Instruct. The blacks represent the tokens\nnot attended to, while the white represents the tokens that are attended to.\nC Generation Sample\nIn this section, we present sample outputs generated using SiftAttention with the Llama-3.1-8B-\nInstruct model under two different hyperparameter configurations.\nPrompt\nIn a world where technology has advanced beyond imagination, society grapples with the\nconsequences of its own creations. The integration of artificial intelligence into everyday life\nhas revolutionized how we live, work, and interact. However, with great power comes great\nresponsibility, and ethical dilemmas abound. Governments struggle to regulate the use of AI,\nwhile corporations push the boundaries of what is possible in the pursuit of profit. Meanwhile,\nindividuals navigate a landscape where privacy is increasingly scarce, and the line between\nhuman and machine blurs. Against this backdrop, a new generation of thinkers emerges,\nquestioning the very nature of consciousness and what it means to be human. As tensions rise\nand alliances shift, the fate of humanity hangs in the balance, waiting to be written by those\nbold enough to seize the pen of destiny.\n19\n--- Page 20 ---\nLlama-3.1-8B-Instruct — Full Attention:\nIn this world, the future is not set in stone, but forged in the fire of human ingenuity and the\nchoices we make today. The question is, what will we create? And what will we become?\nJoin us on a journey through the uncharted territories of the future, where the boundaries of\nreality are pushed to their limits, and the possibilities are endless. Welcome to the world of\ntomorrow, where the only constant is change, and the only truth is that the future is what we\nmake it. Explore the latest developments in AI, robotics, and biotechnology, and discover\nthe innovations that are shaping the world of tomorrow. From the rise of the gig economy\nto the emergence of new forms of governance, we’ll delve into the complex issues that are\nredefining the human experience. Join the conversation and become a part of the future that’s\nbeing written. The world of tomorrow is not just a prediction - it’s a choice. What will you\ncreate? What will you become? The future is in your hands.\nLlama-3.1-8B-Instruct — SiftAttention w= 16 ,τ= 0.875\nIn this world, the future is not set in stone, but forged in the fire of human ingenuity and the\nchoices we make. The question is, what will we create next? Will we build a utopia or a\ndystopia? The choice is ours. The future is now. The world is a complex tapestry of human\nand artificial intelligence, and the story of what comes next is being written by us, in this\nmoment. Join us on this journey into the unknown, where the boundaries of what is possible\nare pushed to the limit. The future is a blank page, waiting to be filled with the stories of those\nwho dare to imagine. In this world, the future is not just a destination, but a creation. And the\ncreation of the future is in our hands.\nLlama-3.1-8B-Instruct — SiftAttention w= 32 ,τ= 0.95\nIn this world, the future is not set in stone, but forged in the fire of human ingenuity and the\nchoices we make today. The question is, what will we create tomorrow? The future is in our\nhands. The future is in our code. The future is in our choices. The future is now. The future\nis... (To be continued)\n20",
  "text_length": 63900
}