{
  "id": "http://arxiv.org/abs/2505.24832v2",
  "title": "How much do language models memorize?",
  "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.",
  "authors": [
    "John X. Morris",
    "Chawin Sitawarin",
    "Chuan Guo",
    "Narine Kokhlikyan",
    "G. Edward Suh",
    "Alexander M. Rush",
    "Kamalika Chaudhuri",
    "Saeed Mahloujifar"
  ],
  "published": "2025-05-30T17:34:03Z",
  "updated": "2025-06-02T14:13:41Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24832v2",
  "full_text": "",
  "text_length": 0
}