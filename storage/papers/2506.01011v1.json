{
  "id": "http://arxiv.org/abs/2506.01011v1",
  "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack",
  "summary": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks.",
  "authors": [
    "Siqi Hui",
    "Yiren Song",
    "Sanping Zhou",
    "Ye Deng",
    "Wenli Huang",
    "Jinjun Wang"
  ],
  "published": "2025-06-01T13:44:20Z",
  "updated": "2025-06-01T13:44:20Z",
  "categories": [
    "cs.CR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01011v1",
  "full_text": "--- Page 1 ---\nAUTOREGRESSIVE IMAGES WATERMARKING THROUGH LEXICAL\nBIASING : ANAPPROACH RESISTANT TO REGENERATION\nATTACK\nSiqi Hui\nXi’an Jiaotong University\nhuisiqi@stu.xjtu.edu.cnYiren Song\nNational University of Singapore\nsongyiren725@gmail.comSanping Zhou\nXi’an Jiaotong University\nspzhou@xjtu.edu.cn\nYe Deng\nSouthwestern University of Finance and Economics\ndengye@stu.xjtu.edu.cnWenli Huang\nNingbo University of Technology\nhuangwenwenlili@126.com\nJinjun Wang\nXi’an Jiaotong University\njinjun@mail.xjtu.edu.cn\nJune 3, 2025\nABSTRACT\nAutoregressive (AR) image generation models have gained increasing attention for their breakthroughs\nin synthesis quality, highlighting the need for robust watermarking to prevent misuse. However,\nexisting in-generation watermarking techniques are primarily designed for diffusion models, where\nwatermarks are embedded within diffusion latent states. This design poses significant challenges\nfor direct adaptation to AR models, which generate images sequentially through token prediction.\nMoreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing\ndiffusion latent states. To address these challenges, we propose Lexical BiasWatermarking (LBW), a\nnovel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks\ndirectly into token maps by biasing token selection toward a predefined green list during generation.\nThis approach ensures seamless integration with existing AR models and extends naturally to post-hoc\nwatermarking. To increase the security against white-box attacks, instead of using a single green list,\nthe green list for each image is randomly sampled from a pool of green lists. Watermark detection is\nperformed via quantization and statistical analysis of the token distribution. Extensive experiments\ndemonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration\nattacks.\n1 Introduction\nRecent diffusion models have demonstrated remarkable success across a wide range of generative tasks, including text-\nto-image synthesis[ 1,2,3], controllable generation[ 4,5,6], image editing[ 7,8,9,10,11,12], and video generation[ 13,\n14,15,16]. While diffusion models [ 17,18] have dominated the landscape, autoregressive (AR)-based frameworks have\nemerged as a compelling alternative, achieving state-of-the-art image quality [ 19,20,21]. Moreover, AR-based image\nmodeling can be seamlessly integrated with AR-based language modeling frameworks, enabling powerful multimodal\napplications [ 22,23]. However, their ability to generate highly realistic images concerns potential misuse, including\ndeep-fakes and misinformation. To ensure traceability and prevent abuse, it is crucial to develop effective watermarking\ntechniques for images generated by AR models.arXiv:2506.01011v1  [cs.CR]  1 Jun 2025\n--- Page 2 ---\nAPREPRINT - JUNE 3, 2025\nExisting watermarking techniques can be categorized into post-hoc and in-generation approaches. Post-hoc watermark-\ning embeds watermarks into pre-generated images via imperceptible perturbations [ 24,25,26], whereas in-generation\nwatermarking integrates watermarks directly into the diffusion-based image synthesis process by modifying intermediate\nstates [ 27,28,29]. While effective in diffusion models, in-generation methods are incompatible with AR models, which\ngenerate images sequentially via token prediction rather than refining continuous latent representations. Consequently,\nwatermarking within AR-based image generation remains an open challenge. Moreover, regeneration attacks pose\na significant threat to diffusion-based in-generation watermarking by disrupting their latent representations where\nwatermarks are embedded [30, 31]. In contrast, AR models, which generate images through discrete token prediction,\noffer a fundamentally different mechanism that may enhance robustness against such attacks. This motivates the\ndevelopment of a watermarking method specifically tailored for AR models.\nThe primary challenge in embedding watermarks in AR-generated images is determining where to introduce the\nwatermark so that it remains detectable. A key observation is that AR models quantize images into token maps,\nand when an AR-generated image is re-encoded, a significant portion of the original tokens can be recovered (see\nFig 1). This suggests that watermark information can be embedded in the token map, and subsequently detected\nby re-quantizing the watermarked image and analyzing the token distribution. Additionally, we observe that minor\nperturbations within a controlled range on the token map do not significantly degrade image quality (see Fig 2, 3).\nThese observations motivate us to embed watermarks in token maps of AR models.\nIn this paper, drawing inspiration from text watermark techniques, we propose a novel framework called Lexical Bias\nWatermarking (LBW), which embeds watermarks in the token map by introducing a controlled bias in token selection\nduring the autoregressive prediction process. Specifically, we partition the token vocabulary into red and green lists and\nencourage the model to favor green tokens during prediction by applying a soft token biasing strategy , which increases\nthe logits of green tokens with a constant to enhance their likelihood of being sampled. Instead of utilizing dynamic\ngreen list [ 32,33], we adopt a global token partition strategy , which maintains a predetermined green list throughout\nthe entire token generation process. This design ensures compatibility with random token prediction processes [ 34,21]\nand enhances robustness against global image watermark removal attacks. Furthermore, our method naturally extends\nto post-hoc watermarking by leveraging the VQ-V AE-based image reconstruction process. After an image is quantized\ninto a token map, we embed the watermark by replacing red tokens with their nearest green counterparts. The modified\ntoken map is then used to reconstruct the image, effectively embedding the watermark in a post-hoc manner.\nTo further enhance resistance against white-box attacks, we introduce a multi-green-list strategy rather than relying on\na single green list. During watermarking, one green list is randomly selected from multiple green lists, and these green\nlists are predefined such that each token has an equal probability of being a green token across the entire green list pool.\nEmpirical results in Figure 5 confirm that our multi-list strategy produces token distributions nearly indistinguishable\nfrom those of clean images, whereas the single-list strategy exhibits detectable biases that can be easily inferred by an\nadversary.\nFor watermark detection, we apply a z-score hypothesis test to evaluate the proportion of green tokens in the token\nmap quantized from watermarked images. Specifically, we evaluate each green list in the pool by computing the\nproportion of its tokens present in the token map. Given the high token consistency in AR-generated images (see\nFig 1), any statistically significant deviation from the expected green token ratio provides strong evidence of watermark\npresence. This detection method is lightweight, requiring only VQ-V AE without access to transformer-based generative\nmodels, making it suitable for both in-generation and post-hoc watermarking. Experimental results demonstrate that our\napproach achieves state-of-the-art robustness against both conventional and regeneration watermark removal attacks,\nparticularly CtrlRegen [ 31], a strong attack designed to erase watermarks embedded in diffusion models. This highlights\nthe effectiveness of our method in providing resilient watermarking for AR-generated images.\nIn this work, we make the following key contributions:\n•To the best of our knowledge, this is the first study to explore watermarking for the AR image generation\nprocess, ensuring seamless integration without disrupting the iterative token prediction mechanism.\n•We propose LBX, a unified framework that introduces lexical bias in AR-based image generation and\nreconstruction processes. We also introduce a multi-green-list strategy to increase the security against white-\nbox attacks.\n•Extensive experiments demonstrate that our method achieves comparable robustness to baseline watermarking\ntechniques against conventional attacks while exhibiting superior resilience against regeneration attacks.\n2 Related Works\nImage watermarking methods.\n2\n--- Page 3 ---\nAPREPRINT - JUNE 3, 2025\nImage watermarking ensures digital content authenticity and security, typically categorized as post-hoc or in-generation\nwatermarking. Post-hoc methods embed watermarks into pre-generated images via pixel-based (e.g., LSB [ 35]) or\nfrequency-based techniques (e.g., DwtDct and DwtDctSvd [ 36]), with recent approaches leveraging deep learning (e.g.,\nRivaGAN [ 25], SSL [ 37], StegaStamp [ 38]). In-generation methods integrate watermarks into the image synthesis\nprocess by modifying intermediate states, particularly in diffusion models[ 27,28,29] or V AE decoders[ 39,40].\nHowever, these methods are incompatible with AR models, which generate images via sequential token prediction.\nBesides, they are vulnerable to diffusion-based regeneration attacks. We firstly explores in-generation watermarking for\nAR image generation, demonstrating superior robustness against regeneration attacks.\nText watermark methods for LLMs. Watermarking LLMs typically involves modifying logits or token sampling to\nembed watermarks within the generated text. KGW[ 41] partitions the vocabulary into \"green\" and \"red\" lists using\na hash-based selection strategy, biasing token selection toward green-listed tokens. EWD[ 42] enhances detection by\nassigning higher weights to low-entropy tokens. To minimize text distortion, SWEET[ 43] and Adaptive Watermark[ 44]\navoid embedding watermarks in low-entropy positions, while BOW [ 45] selectively skips red tokens with high\nprobabilities. WinMax[ 32] applies a sliding window approach to defend against text mixing attacks, while semantic\ngrouping techniques[ 46,47] cluster similar tokens to resist semantic-invariant modifications. Unlike these adaptive\nmethods, our approach utilizes a globally fixed green list, ensuring compatibility with AR models, which generate\ntokens in any order. This design enhances robustness against global image watermark removal attacks, including\nblurring and DiffPure[30].\nAutoregressive visual models. Early research on autoregressive (AR) image generation[ 48,49,50] modeled 2D images\nas 1D pixel sequences, generating pixels in a row-wise raster scan order. Recent advancements leverage VQ-V AE-based\ntokenization[ 51], where models like VQGAN[ 52] employ decoder-only transformers to predict sequences of discrete\nlatent tokens. Similar paradigms include VQV AE-2[ 53] and RQ-Transformer [ 54]. To overcome the limitations\nof unidirectional raster-scan generation, V AR[ 19] introduced a multi-scale residual token map, improving spatial\ncoherence. Further refinements, such as RAR[ 21] and RandAR [ 34], shuffle token generation orders during training,\nfacilitating bidirectional token dependencies and enhancing contextual coherence. Our proposed LBW could be\nseamlessly integrated with AR models that employ both single-scale and multi-scale token maps as well as predefined\nand randomized token generation orders.\n3 Method\nIn this section, we provide a detailed explanation of LBW, which embeds watermarks into token maps through both\nin-generation and post-hoc approaches, along with the corresponding watermark detection process. Section 3.1 provides\nan overview of the image quantization and token prediction processes fundamental to AR-based image synthesis.\nSection 3.2 presents two key properties of AR models that enable robust watermark embedding while preserving image\nquality. Section 3.3 details our in-generation and post-hoc watermarking methods, along with their detection process.\n3.1 Preliminary\nTokenization. Current AR image models [ 52,19,21] leverage VQ-V AE [ 55] to represent continuous images x∈\nRH×W×3as discrete tokens (x1, x2, ..., x T)in latent space, where each xi∈[V]is an integer from a vocabulary of size\nV. Given an input image x, it is first encoded in a feature map f∈Rh×w×C=E(x), where E(·)is the encoder. The\nquantizer Q(·)with a learnable codebook Z∈RV×Cthen maps the feature map to a discrete token map q∈[V]h×w\nor a stack of token maps {qk}K\nk=1|qk∈[V]hk×wk, based on the single-scale or multi-scale quantization process they\nintroduce. The single-scale quantization process q=Q(f)maps each feature vector f(i,j)to a code index q(i,j)by\nfinding the nearest code in the codebook Zusing Euclidean distance:\nq(i,j)=argmin\nv∈[V]||lookup (Z, v)−f(i,j)||2, (1)\nwhere lookup( Z, v) fetches the v-th vector from the codebook Z. This process produces the approximated feature map\nˆf, which is decoded by D(·)to generate the reconstructed image ˆx:\nˆf=lookup (Z, q),ˆx=D(ˆf). (2)\nThemulti-scale quantization process {qk}K\nk=1=QK(f)progressively derives token maps at each scale. A set of\nscale parameters (hk, wk)K\nk=1defines the map resolutions in ascending order, with hK=handwK=wrepresenting\nthe largest scale. The token map qkfor scale kis computed by quantizing the residual feature map rk, which is derived\n3\n--- Page 4 ---\nAPREPRINT - JUNE 3, 2025\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n VQ_GAN\n VAR\n RAR Codebook Ratio\nFigure 1: Observation 1: Token consistency for VQ-GAN, V AR, and RAR across various codebook ratios ranging\nfrom 0.1 to 1.0.\nby subtracting the aggregated sum of the reconstructed residual feature maps from preceding scales (each upscaled to\nthe maximum resolution) from the original feature map:\nrk=f−k−1X\ni=1interpolate ( ˆri, hK, wK),\nˆrk=lookup (Z, qk),(3)\nwhere ˆrkdenotes the approximated residual feature map at scale k. Finally, the image is reconstructed by decoding ˆf:\nˆx=D(ˆf).\nToken prediction. AR models synthesize images through sequential token prediction after modeling images as\nsequences of discrete latent tokens. They utilize transformers to model the conditional probability distribution of token\ngeneration, formulated as:\npθ(xt|Xt) =softmax (lθ(Xt)), (4)\nwhere lθ(Xt)∈RVis the generated logit at step t, andXt⊆ {x1, ..., x t−1}is the subset of previous generated tokens.\nThe token sequence is generated through iterative sampling from the conditional probability. For simplicity, we use lt\nto denote the logits lθ(Xt)in the rest of the paper. For single-scale token prediction , tokens in Xtare generated either\nin a predefined order (e.g., raster-scan) or in a randomly permuted sequence. Multi-scale token prediction conditions\ntoken generation on all tokens from preceding scales, while allowing tokens within the same scale to be generated in\nparallel. For a more comprehensive discussion on multi-scale quantization and token prediction, please refer to [ 19]\nand [54].\n3.2 Observations\nTo investigate the behavior of VQ-V AE and evaluate its potential for embedding robust watermarks, we conducted two\nprimary analyses. Observation 1 examines the consistency between token maps obtained by quantizing the original\nimages and those derived from their reconstructed counterparts, providing insights into the feasibility of watermark\nembedding and detection in AR visual models. Observation 2 investigates the VQ-V AE codebook by analyzing the\nimpact of vocabulary size reduction on image reconstruction quality. This analysis determines whether a compact\ncodebook can facilitate watermarking while preserving image fidelity.\nExperiments were performed on three AR models featuring different vocabulary sizes: VQ-GAN [ 52] (13,678 tokens),\nV AR [ 19] (4,096 tokens), and RAR [ 21] (1,024 tokens). The evaluation dataset comprises 5,000 images randomly\nsampled from 100 ImageNet classes, with 50 images per class.\nObservation 1: Token consistency. To evaluate the potential for watermark embedding and detection, we reconstructed\nimages from the evaluation dataset and compared the token maps produced by quantizing both the input image xand\nits reconstruction ˆx, yielding token maps qandˆq, respectively. The token consistency is defined as the proportion\nof matching tokens between these token maps. Notably, this consistency was assessed while progressively reducing\nthe vocabulary size during the reconstruction process. For V AR, which employs a multi-scale quantization process,\ntoken consistency was evaluated at its largest scale. As illustrated in Fig 1, token consistency remains robust across\nvarious codebook sizes. However, RAR exhibits a more pronounced decline in consistency as the vocabulary decreases,\nlikely due to its smaller codebook (1024) being more susceptible to quantization errors. These findings indicate that\nAR models employing VQ-V AEs maintain sufficient token consistency to preserve a substantial portion of the original\ntoken sequence even under significant vocabulary reduction. Consequently, watermark information can be reliably\nembedded into token maps and subsequently detected by quantizing images into these maps again.\n4\n--- Page 5 ---\nAPREPRINT - JUNE 3, 2025\n(b) SSIM (a) PSNR (c) FIDCodebook Ratio Codebook Ratio Codebook Ratio\n0.0 0.2 0.4 0.6 0.8 1.01821242730\n0.0 0.2 0.4 0.6 0.8 1.00.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.05101520\n VQ-GAN\n VAR\n RAR VQ-GAN\n VAR\n RAR VQ-GAN\n VAR\n RAR\nFigure 2: Observation 2: Image quality metrics (PSNR, SSIM, and FID) for AR reconstructed images across various\ncodebook sizes, ranging from 0.1 to 1.0.\nOriginal VQ-GAN VAR RAR\n Original VQ-GAN VAR RAR\nFigure 3: Reconstructed images using only 10% of the original codebook size.\nObservation 2: Codebook redundancy. To further assess the influence of codebook reduction on image quality, we\ndecreased the codebook ratio during image reconstruction and evaluated PSNR, SSIM, and FID metrics of reconstructed\nimages. As shown in Fig 2, a moderate degradation in image quality is observed as the codebook size decreases, and\nthe overall performance remains stable. Notably, even when the vocabulary of V AR was reduced to just 10%, the\nreconstructed images exhibited minimal quality loss relative to those produced using the full codebook. This finding\nindicates that a compact codebook is viable for watermarking applications without substantially compromising image\nquality. Additionally, Fig 3 presents the reconstructed images obtained using only 10% of the vocabulary, further\ndemonstrating the feasibility of this approach.\n3.3 Watermarking through lexical biasing\nBuilding on our previous analysis, we observe that tokens used for image generation can be recovered by encoding and\nquantizing AR-generated images into token maps again (see Fig 1). This finding suggests that if watermark information\nis embedded in the token map, it should be preserved and detected through this re-quantization process. In this paper,\ndrawing inspiration from text watermarking techniques, we aim to embed the watermark information through biasing\nAR models to use specific tokens during the autoregressive token prediction process.\nA direct approach. Formally, the codebook of AR models can be partitioned into a green list Gand a red list R. We\naim to bias the model toward selecting tokens from Gduring AR image synthesis. A simple yet effective approach\nis to mask the logits of red list tokens while retaining those of the green list. Specifically, at each timestep t, the\nmodel predicts logits ltbased on previously generated tokens Xt, which are then converted into a discrete probability\ndistribution for sampling the next token xt. To enforce token selection from G, the logits of red list tokens are set to\nnegative infinity, modifying the logit vector ˆltas follows:\nˆlt(i)=\u001a−∞, i∈R\nlt(i), i∈G.(5)\nThis modification ensures that the softmax function assigns zero probability to tokens in R, thereby restricting\nthe model to sample xtexclusively from the green list G. Unlike text watermarking, where the green and red\nlists dynamically adjust based on a hash of previously generated tokens, we employ a global token partitioning\nstrategy , in which the same green and red lists are uniformly applied to all tokens. This approach is driven by three\nconsiderations. 1) Re-quantization Loss: The re-quantization process introduces token variations. When a previously\ngenerated token changes, the corresponding green list for subsequent tokens would also shift, which hampers the\nrobustness. 2) Vulnerability to image watermarking attacks: Unlike local text removal attacks, image watermarking\nattacks—such as blurring or CtrlRegen [ 31]—impact the entire image, leading to substantial token variations across\n5\n--- Page 6 ---\nAPREPRINT - JUNE 3, 2025\nthe token map, which also weakens the robustness of token-dependent hashing schemes. 3) Compatibility with AR\nmodels: Certain AR models generate tokens in a non-sequential, randomized order [ 34,21] rather than following a\nstrictly predefined sequence. In such cases, if the token generation order is unknown, dynamically determining the\ngreen lists becomes impractical.\nWatermark detection. Watermark presence is verified by statistically assessing the occurrence of specific tokens\nin the quantized token map, requiring only VQ-V AE and the predefined green list Gwithout access to transformer\nmodels. We formulate the verification process as a hypothesis-testing problem. We define the null hypothesis:\nH0:The image was generated without any bias toward the green list . To evaluate this hypothesis, we perform\na one-proportion z-test on the number of green tokens in the quantized token map. Let γrepresent the proportion of\ngreen tokens used for watermarking. Under H0, the expected number of green tokens in a token map q∈Rh×wfollows\na binomial distribution with mean γ·h·wand variance γ(1−γ)·h·w. Denoting the observed number of green tokens\nin the token map as |s|G, the z-score for watermark detection is computed as:\nz=|s|G−γ·h·wp\nγ·(1−γ)·h·w. (6)\nBy setting a threshold zth, we reject H0and confirm watermark presence if z > z th.\nEnhancing watermark via soft biasing. The direct approach enforces the exclusive use of green-listed tokens during\ntoken prediction. However, when the green list is overly constrained, the limited token vocabulary reduces the expressive\ncapacity of AR models, leading to declined image quality or even generation failures (Fig 9). Moreover, restricting\ntoken selection disrupts the natural token distribution expected by VQ-V AE, thereby reducing token consistency, which\nin turn compromises watermark detectability and robustness (Fig 8). To address these limitations, we employ a soft\ntoken biasing strategy that encourages the selection of green tokens without completely excluding red tokens. After\npredicting logits, instead of forcing red token logits to negative infinity, we introduce a bias constant σto increase the\nlogits of green tokens as:\nˆlt(i)=\u001alt(i), i ∈R\nlt(i)+σ, i ∈G.(7)\nThis ensures that when the transformer model exhibits high logits on the red list (high urge of using a red token), the\nadded bias minimally influences token selection, preserving the natural AR generation process. As a result, this method\nmaintains image quality and results in better token consistency and detectability. We also compute the z-score of the\ntotal number of green tokens in the re-encoded token map and compare it against a predefined threshold to determine\nthe presence of a watermark.\nPost-hoc watermarking via token substitution. Our approach could be extended to support post-hoc watermarking\nfor existing images by simply substituting quantized red tokens with green tokens. Formally, given an input image\nx, we first quantize it into a token map q. The watermark is then embedded by replacing each red token q(i)with its\nnearest green token, determined by the Euclidean distance in the token embedding space, ensuring minimal distortion:\nq′\n(i)= arg min\ng∈G∥lookup (Z, g)−lookup (Z, q(i))∥2. (8)\nMultiple green lists. To prevent our method against white-box attacks, we propose a multiple green list strategy.\nConcretely, a set of Ngreen lists {Gi}N\ni=1is established, from which one green list is randomly selected for watermark\nencoding. This set of green lists can be represented as a binary matrix M∈ {0,1}N×Vas\nMij=\u001a1,ifj∈Gi,\n0,otherwise ,∀i,∀j,s.t.VX\nj=1Mij=γV,∀i,andNX\ni=1Mij=γN,∀j. (9)\nEach row of Mcorresponds to a green list pool, with the first constraint ensuring that each green list maintains a\nconsistent green token ratio γ, while the second guarantees that each token is selected as a green token with equal\nprobability across the green lists. This design aligns the token distribution of watermarked images with that of clean\nimages, thereby reducing the risk of reverse-engineering the watermark. However, finding such a matrix exactly\nsatisfying the above constraints requires solving the 0-1 integer programming problem, which is generally NP-hard and\nmay have no feasible solution. To efficiently generate a matrix that approximately meets these constraints, we employ\nthe following algorithm:\nIn our experiments, we use N= 32 green lists and find it suffices to defend green list estimation attacks 5. During\ndetection, the green token ratio is computed with respect to each green list in the pool, and the maximum observed\nratio is used to calculate a z-score to justify the presence of the watermark. Notably, the detection requires only a\nconvolutional image encoder to extract token maps, enabling efficient detection even when multiple green lists are\nemployed.\n6\n--- Page 7 ---\nAPREPRINT - JUNE 3, 2025\nAlgorithm 1: Generate Green List Matrix M\nInput: Number of green lists N, green token ratio γ, vocabulary size V\nOutput: Binary matrix M∈ {0,1}N×V\n1Randomly initialize matrix Msuch that each row isatisfiesP\nj=1Mij=γV;\n2Set threshold θ←γN;\n3repeat\n4 fori←1toNdo\n5 Compute token frequency vector f:f[j]←P\ni=1Mij;\n// Identify indices for tokens with excessively high frequency\n6 one_to_zero ← {j|token_frequency [j]> θ andMij= 1};\n// Identify indices for tokens with too low frequency\n7 zero_to_one ← {j|token_frequency [j]< θ andMij= 0};\n8 K←min(|one_to_zero |,|zero_to_one |);\n9 fork←1toKdo\n10 SetMi,zero_to_one [k]←1;\n11 SetMi,one_to_zero [k]←0;\n12 end\n13 end\n14until convergence or maximum iterations reached ;\n15return M\n4 Experiments\n4.1 Experimental Setting\nWe evaluate our watermarking methods using VQ-GAN, RAR, and V AR. For V AR, watermarks are embedded in\nthe largest-scale token map. We apply three watermarking variants: LBW-Hard (strict green token enforcement),\nLBW-Soft (soft bias toward green tokens), and LBW-Post (post-hoc token substitution). Green token ratios γare\nset to 0.1 for LBW-Post and LBW-Soft on V AR and RAR, and 0.2 for LBW-Hard on VQ-GAN. LBW-Soft uses bias\nconstants σof 7, 4, and 8 for V AR, VQ-GAN, and RAR, respectively. To further improve the generation quality,\nLBX-Soft adopts a bias constant σof 7, 4, and 8 for V AR, VQ-GAN, and RAR. We use N= 32 green lists for\nembedding and detection, with consistent green list configurations across in-generation and post-hoc methods. We\ncompare against state-of-the-art watermarking methods: DwtDct [ 56], DwtDctSvd [ 36], RivaGAN [ 25], SSL [ 37],\nTree-Ring [ 27], and WatermarkDM [ 57]. Conventional attacks include Gaussian noise and blur, ColorJitter, geometric\ntransformations (crop, resize, rotation), and JPEG compression. Regeneration attacks comprise V AE reconstruction\n(Stable Diffusion 1.5), DiffPure [ 23], and CtrlRegen [ 31]. Detailed attack settings can be found in the appendix D.\nWe evaluate performance on the ImageNet dataset. For post-hoc watermarking, 1,000 images (10 per 100 classes) are\nwatermarked. For in-generation methods, 1,000 watermarked and 1,000 clean images are generated conditionally on\nclass labels aligned with post-hoc experiments. Detection is evaluated by ROC-AUC and TPR@1%FPR, averaged over\nfive runs with different seeds to ensure robustness and reproducibility.\n4.2 Main Results\nTable 1 presents a comprehensive evaluation of watermarking methods under conventional and regeneration attacks,\nwhere our approach demonstrates strong robustness across all attacks. Notably, LBW achieves state-of-the-art perfor-\nmance against regeneration attacks; for instance, LBW-Post on RAR attains an AUC of 0.995 and TPR@1FPR of 0.937,\nsignificantly outperforming WatermarkDM. LBW-Soft further surpasses LBW-Hard in robustness. While our method\nis effective across different AR architectures, V AR shows slightly reduced robustness. This can be attributed to the\nmultiscale quantization process of V AR, where high-frequency information is primarily captured by the largest-scale\ntoken map[ 54,19]. CtrlRegen preserves semantic structures while suppressing fine-grained details, which makes\nlarge-scale token maps more vulnerable to such attacks.\n4.3 Ablation Studies\nFigures 4 (top rows) show that the robustness of LBW-Post and LBW-Hard generally improves as γdecreases. An\nexception occurs for LBW-Hard on VQ-GAN, where γ= 0.2outperforms γ= 0.1. This is because at very low γ\nvalues, the model frequently fails to generate images, leading to reduced token consistency and sub-optimal robustness\n(see appendix B for detailed analyses). Based on these findings, we set the default values of γto 0.2, 0.1, and 0.1 for\nVQ-GAN, V AR, and RAR, respectively. Note that when evaluating the effect of σfor LBW-Soft, the γis set to the\ndefault values. As shown in the bottom row of Figure 4, the robustness of LBW-Soft initially improves with increasing\nσand eventually saturated. The results indicate that robustness initially improves with increasing values of σand\neventually becomes saturated. While larger σvalues enhance watermark detectability, excessively high values can\n7\n--- Page 8 ---\nAPREPRINT - JUNE 3, 2025\nTable 1: Comparative evaluation of watermarking methods under conventional and regeneration attacks. The table\npresents AUC and TPR@1FPR metrics, where TPR@1FPR is abbreviated as T@1F. Best results are bolded . Our\nproposed method exhibits superior robustness, particularly against regeneration attacks.\nMethod Metric CleanConventional Attack Regeneration Attack\nGaus Color Geo JPEG A VG V AE Diff Ctrl A VG\ndwtDctAUC 0.978 0.906 0.333 0.631 0.520 0.598 0.521 0.485 0.496 0.501\nT@1F 0.920 0.714 0.091 0.055 0.005 0.216 0.020 0.010 0.020 0.017\ndwtDctSvdAUC 1.000 0.949 0.286 0.600 0.649 0.621 0.797 0.597 0.487 0.627\nT@1F 1.000 0.850 0.017 0.025 0.020 0.228 0.320 0.010 0.010 0.113\nrivaGanAUC 1.000 1.000 0.671 0.776 0.939 0.847 0.931 0.747 0.527 0.735\nT@1F 1.000 1.000 0.657 0.600 0.780 0.759 0.510 0.150 0.050 0.237\nwatermarkDMAUC 1.000 1.000 0.724 0.515 0.999 0.810 0.999 0.915 0.671 0.862\nT@1F 1.000 1.000 0.656 0.112 0.991 0.690 0.920 0.340 0.000 0.420\nSSLAUC 1.000 1.000 0.992 0.991 0.621 0.901 0.959 0.695 0.750 0.801\nT@1F 1.000 1.000 0.971 0.970 0.312 0.813 0.840 0.160 0.090 0.363\nTreeRingAUC 1.000 0.945 0.937 0.938 0.991 0.962 1.000 0.599 0.838 0.812\nT@1F 1.000 0.902 0.747 0.708 0.952 0.862 1.000 0.000 0.150 0.383\nV AR\n(Ours)LBW-PostAUC 0.997 0.988 0.989 0.659 0.981 0.923 0.997 0.933 0.650 0.860\nT@1F 0.980 0.943 0.948 0.287 0.912 0.814 0.972 0.780 0.080 0.611\nLBW-HardAUC 0.999 0.995 0.994 0.660 0.988 0.927 0.991 0.896 0.623 0.837\nT@1F 0.995 0.967 0.969 0.281 0.932 0.829 0.903 0.526 0.000 0.476\nLBX-SoftAUC 1.000 0.995 0.994 0.665 0.989 0.929 0.995 0.892 0.626 0.838\nT@1F 0.997 0.977 0.966 0.275 0.934 0.830 0.930 0.480 0.010 0.473\nVQ-GAN\n(Ours)LBW-PostAUC 0.978 0.972 0.856 0.773 0.948 0.905 0.969 0.922 0.665 0.852\nT@1F 0.760 0.728 0.329 0.274 0.699 0.558 0.704 0.660 0.140 0.501\nLBW-HardAUC 0.998 0.969 0.858 0.939 0.921 0.922 0.993 0.973 0.857 0.941\nT@1F 0.993 0.909 0.700 0.641 0.745 0.749 0.978 0.870 0.370 0.739\nLBX-SoftAUC 0.999 0.990 0.934 0.966 0.995 0.977 0.998 0.994 0.915 0.969\nT@1F 0.998 0.977 0.770 0.776 0.977 0.900 0.998 0.990 0.610 0.866\nRAR\n(Ours)LBW-PostAUC 1.000 1.000 0.995 0.991 0.999 0.996 1.000 0.998 0.988 0.995\nT@1F 1.000 0.999 0.956 0.918 0.995 0.967 1.000 0.960 0.850 0.937\nLBW-HardAUC 1.000 1.000 0.997 0.964 0.999 0.990 1.000 1.000 0.978 0.993\nT@1F 1.000 0.998 0.973 0.846 0.993 0.953 1.000 1.000 0.800 0.933\nLBX-SoftAUC 1.000 1.000 0.999 0.961 1.000 0.990 1.000 1.000 0.978 0.993\nT@1F 1.000 1.000 0.981 0.844 0.999 0.956 1.000 0.990 0.760 0.917\nVAR VQ-GAN RARLBW -Hard LBW -Post\nγ \nσ γ γ γ γ γ \nσ σ LBW -Soft\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n0 2 4 6 8 100.00.20.40.60.81.0\n0 2 4 6 8 100.00.20.40.60.81.0\n0 2 4 6 8 100.00.20.40.60.81.0 Con AUC      Con TPR@1FPR     Reg AUC      Reg TPR@1FPR\nFigure 4: Impact of γandσin robustness against Conventional (Con) and Regeneration (Reg) attacks.\n8\n--- Page 9 ---\nAPREPRINT - JUNE 3, 2025\n2004 006 008 001 0000.0000.0020.0040.0060.0080.010Frequency (%) N=1 \nN=2 \nN=8 \nN=32 \nN=128 \nclean\nFigure 5: Comparison of token frequency distributions with varying green list number N.\ncompromise image quality by overly restricting token generation. To achieve a balance between robustness and image\nfidelity, we adopt σ= 7,4, and 8for V AR, VQ-GAN, and RAR. To prevent green tokens from being inferred, we\nintroduce Nmultiple green lists for watermark embedding. Using RAR with γ= 0.1, we vary N∈ {1,2,8,32,128}\nand generate 10,000 post-hoc watermarked images per setting to study the impact of Non the token frequency\ndistribution of watermarked images. Figure 5 shows that when Nis small (e.g., 1 or 2), the token frequency distribution\ndeviates significantly from that of clean images, enabling white-box attacks. However, as Nincreases to 32 and beyond,\nthe token frequency distribution converges closely to that of clean images, effectively eliminating distinguishable\nstatistical cues and rendering frequency-based attacks. Consequently, we choose N= 32 in this work.\nMore experiments, including visual quality analysis, token consistency analysis or other visual results can be\nfound in the appendix.\n5 Conclusion\nIn this work, we present LBW, a novel watermarking framework for AR-based image generation that introduces a\ncontrolled bias in token selection during the generation process to embed a robust and detectable watermark. Our method\ncan be seamlessly integrated into current AR image generation pipelines, achieving state-of-the-art robustness against\nregeneration attacks. Additionally, we extend LBW to a post-hoc watermarking scheme, showcasing its adaptability in\nboth in-generation and post-hoc scenarios. The multiple green list strategy is further introduced to enhance robustness\nagainst white-box attacks.\nReferences\n[1]Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint\narXiv:2307.01952 , 2023.\n[2]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis.\nInForty-first International Conference on Machine Learning , 2024.\n[3]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 10684–10695, 2022.\n[4]Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han\nPan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8069–8078, 2024.\n[5]Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. Fast personalized text to image synthesis\nwith attention injection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 6195–6199. IEEE, 2024.\n9\n--- Page 10 ---\nAPREPRINT - JUNE 3, 2025\n[6]Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and\nflexible control for diffusion transformer. arXiv preprint arXiv:2503.07027 , 2025.\n[7]Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld:\nSimulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785 , 2024.\n[8]Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu.\nPhotodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397 ,\n2025.\n[9]Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint\narXiv:2411.06686 , 2024.\n[10] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.\nImagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 6007–6017, 2023.\n[11] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao.\nStable-makeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764 , 2024.\n[12] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair\ntransfer via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pages\n10348–10356, 2025.\n[13] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International\nJournal of Computer Vision , pages 1–15, 2024.\n[14] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: Harnessing diffusion transformers for multi-domain\nprocedural sequence generation. arXiv preprint arXiv:2502.01572 , 2025.\n[15] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou.\nProcesspainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062 , 2024.\n[16] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong.\nGrid: Visual layout generation. arXiv preprint arXiv:2412.10718 , 2024.\n[17] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n[18] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf , 2(3):8, 2023.\n[19] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable\nimage generation via next-scale prediction. Advances in neural information processing systems , 37:84839–84865,\n2025.\n[20] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity:\nScaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431 ,\n2024.\n[21] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual\ngeneration. arXiv preprint arXiv:2411.00776 , 2024.\n[22] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,\nZhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal\nunderstanding and generation, 2024.\n[23] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one\nmulti-modal model, 2024.\n[24] Md. Maklachur Rahman. A dwt, dct and svd based watermarking technique to protect the image piracy. Inter-\nnational Journal of Managing Public Sector Information and Communication Technologies , 4(2):21–32, June\n2013.\n[25] Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robust invisible video\nwatermarking with attention, 2019.\n[26] Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, and Matthijs Douze. Watermarking images\nin self-supervised latent spaces. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 3054–3058. IEEE, 2022.\n10\n--- Page 11 ---\nAPREPRINT - JUNE 3, 2025\n[27] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for\ndiffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030 , 2023.\n[28] Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced\nmulti-key identification. In European Conference on Computer Vision , pages 338–354. Springer, 2024.\n[29] Huayang Huang, Yu Wu, and Qian Wang. Robin: Robust and invisible watermarks for diffusion models with\nadversarial optimization, 2024.\n[30] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models\nfor adversarial purification, 2022.\n[31] Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, and Yuheng Bu. Image watermarks\nare removable using controllable regeneration from clean noise, 2024.\n[32] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando,\nAniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models.\narXiv preprint arXiv:2306.04634 , 2023.\n[33] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark\nfor large language models. arXiv preprint arXiv:2310.10669 , 2023.\n[34] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, and Yu-Xiong\nWang. Randar: Decoder-only autoregressive visual generation in random orders, 2024.\n[35] R.B. Wolfgang and E.J. Delp. A watermark for digital images. In Proceedings of 3rd IEEE International\nConference on Image Processing , volume 3, pages 219–222 vol.3, 1996.\n[36] K. A. Navas, Mathews Cheriyan Ajay, M. Lekshmi, Tampy S. Archana, and M. Sasikumar. Dwt-dct-svd based\nwatermarking. In 2008 3rd International Conference on Communication Systems Software and Middleware and\nWorkshops (COMSWARE ’08) , pages 271–274, 2008.\n[37] Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, and Matthijs Douze. Watermarking images\nin self-supervised latent spaces, 2022.\n[38] Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical photographs. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020.\n[39] Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, and Teddy Furon. The stable signature:\nRooting watermarks in latent diffusion models. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , pages 22466–22477, October 2023.\n[40] Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, and Mike Zheng Shou. Wmadapter: Adding watermark control to\nlatent diffusion models, 2024.\n[41] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for\nlarge language models. In International Conference on Machine Learning , pages 17061–17084. PMLR, 2023.\n[42] Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, and Irwin King. An entropy-based text watermarking detection\nmethod. arXiv preprint arXiv:2403.13485 , 2024.\n[43] Yepeng Liu and Yuheng Bu. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927 ,\n2024.\n[44] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim.\nWho wrote this code? watermarking for code generation, 2024.\n[45] Bram Wouters. Optimizing watermarks for large language models. arXiv preprint arXiv:2312.17295 , 2023.\n[46] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. A semantic invariant robust watermark for large\nlanguage models, 2024.\n[47] Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, and Rui Wang.\nCan watermarks survive translation? on the cross-lingual consistency of text watermark for large language models.\narXiv preprint arXiv:2402.14007 , 2024.\n[48] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative\npretraining from pixels. In International conference on machine learning , pages 1691–1703. PMLR, 2020.\n[49] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks.\nInInternational Conference on Machine Learning , pages 1242–1250. PMLR, 2014.\n[50] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran.\nImage transformer. In International conference on machine learning , pages 4055–4064. PMLR, 2018.\n11\n--- Page 12 ---\nAPREPRINT - JUNE 3, 2025\n(a) LBW -Post PSNR (b) LBW -Post SSIM\n(c)  LBW -Post FID (d) LBW -Hard FID (e) LBW -Soft FID\n0.0 0.2 0.4 0.6 0.8 1.018.019.521.022.524.025.5\n0.0 0.2 0.4 0.6 0.8 1.00.40.50.60.70.8\n0.0 0.2 0.4 0.6 0.8 1.0369121518\n0.0 0.2 0.4 0.6 0.8 1.0912151821\n0 2 4 6 8 10812162024 VQ-GAN\n VAR\n RAR\n VQ-GAN\n VAR\n RAR VQ-GAN\n VAR\n RAR\n VQ-GAN\n VAR\n RAR VQ-GAN\n VAR\n RAR\nFigure 6: Quantitative evaluation of visual quality for our LBW with varying γandσ. Subfigures (a), (b), and (c)\npresent PSNR, SSIM, and FID metrics for the LBW-Post watermark applied on VQ-GAN, V AR, and RAR models,\nwhile (d) and (e) show FID for our LBW-Hard and LBW-Soft, respectively.\n[51] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018.\n[52] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873–12883, 2021.\n[53] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.\nAdvances in neural information processing systems , 32, 2019.\n[54] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using\nresidual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 11523–11532, 2022.\n[55] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information\nprocessing systems , 30, 2017.\n[56] Ali Al-Haj. Combined dwt-dct digital image watermarking. Journal of computer science , 3(9):740–746, 2007.\n[57] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe for watermarking\ndiffusion models. arXiv preprint arXiv:2303.10137 , 2023.\nA Visual Quality Analysis\nThis section presents a comprehensive analysis of the visual quality of the proposed watermarking methods—LBW-Post,\nLBW-Hard, and LBW-Soft—through both quantitative and qualitative evaluations.\nFigure 6 reports FID scores for our watermark methods with varying γandσ. PSNR and SSIM are reported exclusively\nfor LBW-Post, as it is the only method with access to ground-truth images. The results indicate that increasing γ\nconsistently enhances image quality across all models, evidenced by improved PSNR and SSIM scores for LBW-Post,\nand by reduced FID values of both LBW-Post and LBW-Hard. Conversely, for LBW-Soft, FID scores increase with\nlarger σ, indicating a degradation in perceptual quality. Compared to LBW-Hard at γ= 0.1, LBW-Soft achieves lower\nFID scores across a large range of σ, reflecting its superior capability to balance watermark robustness with image\nfidelity. Notably, compared to VQ-GAN and RAR, V AR exhibits superior robustness to variations in both γandσ.\nThis robustness stems from watermark embedding exclusively within its largest-scale token map, which encodes rich\nhigh-frequency information, thereby mitigating perceptual degradation.\nQuantitative results\nQualitative results Figure 7 offers a visual comparison of watermarked images generated by VQ-GAN under the three\nwatermarking schemes. The first two rows illustrate the results of LBW-Post and LBW-Hard, respectively, while the last\n12\n--- Page 13 ---\nAPREPRINT - JUNE 3, 2025\nLBX -Post LBX -Hard LBX -Soft\nFigure 7: Qualitative evaluation of visual quality for our LBW with varying γandσ\npresents the results of LBW-Soft. The image quality improves with increasing γfor both LBW-Post and LBW-Hard.\nFor LBW-Soft, reducing the noise parameter σenhances image quality. When γis low (e.g., 0.1), the LBW-Hard often\nsuffers from degraded image quality and occasionally fails to produce class-relevant images. This limitation arises\nbecause LBW-Hard enforces strict token selection constrained solely to the green list, which significantly restricts\nthe model’s expressive capacity. In contrast, LBW-Soft permits sampling outside the green list with moderated bias,\nthereby demonstrating greater robustness and superior visual quality under low green token ratios.\nIn summary, these quantitative and qualitative analyses corroborate that higher green token ratios γand lower logit\nbias constant σcorrelate with improved image quality, and our LBW-Soft effectively achieves a trade-off between\nwatermark robustness and visual fidelity.\nB Token Consistency\nFigure 8 presents a comparative analysis of the token consistency of our method for VQ-GAN, V AR, and RAR under\ndifferent green list ratios γand logits biasing constant σ. Overall, the token consistency of the LBW-Post method\nincreases with γ, whereas in-generation methods (LBW-Hard & LBW-Soft) are insensitive to γandσ. This suggests\nthat in-generation watermarking could improve detectability by increasing γandσwithout worrying about the token\nexchanges during detection.\nC Visual Comparison between LBW-Hard and LBW-Soft\nFigure 9 presents a comparative analysis of watermarked images synthesized using our LBW-Hard and LBW-Soft\napproaches, integrated with RAR and VQ-GAN. The results demonstrate that LBW-Hard results in reduced image\nquality when applied to RAR and even fails to generate class-relevant content in the case of VQ-GAN. In contrast,\nLBW-Soft produces images with finer details and realistic content. These findings highlight the efficacy of LBW-Soft in\nstriking a balance between watermark robustness and image fidelity, making it a more suitable approach for AR image\nmodels.\nD Numerical Robustness Results\nTo assess watermark robustness, we evaluate conventional and regeneration attacks. Conventional attacks include\n(1)Gauss ian attacks randomly apply Gaussian Noise with variation 0.1 and Gaussian Blur using an 8×8filter) (2)\n13\n--- Page 14 ---\nAPREPRINT - JUNE 3, 2025\nVAR VQ-GAN\nγ σ RAR\n0.0 0.2 0.4 0.6 0.8 1.00.550.600.650.700.750.80\n0.0 0.2 0.4 0.6 0.8 1.00.500.550.600.650.700.750.80\n0.0 0.2 0.4 0.6 0.8 1.00.350.400.450.500.550.60 Post\n Hard\n Soft0 2 4 6 8 10\n Post\n Hard\n Soft0 2 4 6 8 10\n Post\n Hard\n Soft0 2 4 6 8 10γ σ γ σ \nFigure 8: Token consistency with varying γ.\nColor Jitter perturbations involving randomly applying hue adjustments ( 0.3), saturation scaling ( 3.0), and contrast scal-\ning (3.0), (3) Geom etric transformations (Crop&Resize: 0.7, Random Rotation: 0◦-180◦), and (4) JPEG compression\n(25% ). Regeneration attacks include (1) VAE reconstruction via the V AE of Stable Diffusion 1.5, (2) DiffPure [ 23]\nwith timestep t= 0.15, and (3) CtrlRegen [31].\nIn this section, we illustrate the numerical robustness of our proposed Lexical Bias Watermarking (LBW)\nmethods—LBW-Post, LBW-Hard, and LBW-Soft—against the above watermark removal attacks in Table 2, Ta-\nble 3, and Table 4, respectively. The evaluation is conducted on three different AR image models: V AR, VQ-GAN,\nand RAR, across varying watermark embedding strengths, denoted by the parameter γ, ranging from 0.1 to 0.9. For\nLBW-Soft, we evaluate the effect of bias constant σunder γ= 0.1,0.2, and 0.1for V AR, VQ-GAN, and RAR,\nrespectively. We assess performance using AUC (Area Under Curve) and T@1F (True Positive Rate at 1% False\nPositive Rate), which indicate the detectability of the watermark under different attack conditions.\nE More Visual Results\nComparison between LBW-Post and other post-hoc methods.\nFigure 10 compares the watermarking performance of LBW-Post and traditional Post-hoc methods across different\ngenerative models (VQ-GAN, V AR, and RAR). In this comparison, LBW-Post employs a green word ratio of γ= 0.1.\nExperimental results indicate that in multi-scale token map models (e.g., V AR), LBW-Post achieves image quality\ncomparable to traditional Post-hoc methods. In single-scale generative models (e.g., VQ-GAN and RAR), LBW-Post\nalso has a minimal impact on image quality, effectively maintaining visual consistency.\n14\n--- Page 15 ---\nAPREPRINT - JUNE 3, 2025\nVQ-GAN RAR\nLBW -Hard LBW -Soft\nFigure 9: Comparison between watermark images produced by LBW-Hard and LBW-Soft.\nLBW -Post\n (VQ-GAN )LBW -Post\n(VAR )LBW -Hard\n(RAR )WatermarkDM RivaGan DwtDctSvd DwtDct Clean\nFigure 10: Visual comparison between our LBW-Post and other Post-hoc methods.\n15\n--- Page 16 ---\nAPREPRINT - JUNE 3, 2025\nγ Metric Clean Gaus Color Geom JPEG V AE Diff Ctrl\nV AR0.100AUC 0.997 0.988 0.989 0.659 0.981 0.997 0.933 0.650\nT@1F 0.980 0.943 0.948 0.287 0.912 0.972 0.780 0.080\n0.200AUC 0.998 0.989 0.985 0.655 0.977 0.995 0.923 0.645\nT@1F 0.985 0.940 0.935 0.280 0.888 0.968 0.700 0.100\n0.300AUC 0.995 0.983 0.983 0.639 0.972 0.996 0.919 0.614\nT@1F 0.973 0.920 0.906 0.274 0.852 0.948 0.580 0.030\n0.400AUC 0.994 0.977 0.977 0.641 0.967 0.986 0.868 0.605\nT@1F 0.962 0.876 0.884 0.275 0.829 0.932 0.510 0.000\n0.500AUC 0.995 0.974 0.974 0.645 0.957 0.984 0.865 0.588\nT@1F 0.961 0.848 0.867 0.273 0.772 0.890 0.410 0.020\n0.600AUC 0.991 0.969 0.967 0.629 0.950 0.981 0.886 0.586\nT@1F 0.946 0.802 0.832 0.267 0.729 0.892 0.390 0.010\n0.700AUC 0.985 0.951 0.952 0.627 0.926 0.975 0.823 0.540\nT@1F 0.911 0.721 0.745 0.255 0.646 0.796 0.350 0.000\n0.800AUC 0.979 0.932 0.934 0.636 0.906 0.945 0.765 0.525\nT@1F 0.847 0.606 0.631 0.249 0.546 0.638 0.240 0.020\n0.900AUC 0.956 0.883 0.889 0.617 0.856 0.881 0.687 0.516\nT@1F 0.659 0.367 0.451 0.193 0.378 0.230 0.050 0.020\nVQ-GAN0.100AUC 0.978 0.972 0.856 0.773 0.948 0.969 0.922 0.665\nT@1F 0.760 0.728 0.329 0.274 0.699 0.704 0.660 0.140\n0.200AUC 0.898 0.894 0.749 0.681 0.856 0.880 0.850 0.684\nT@1F 0.493 0.433 0.155 0.171 0.366 0.434 0.280 0.010\n0.300AUC 0.871 0.864 0.715 0.661 0.821 0.866 0.800 0.687\nT@1F 0.369 0.321 0.112 0.135 0.226 0.386 0.240 0.040\n0.400AUC 0.829 0.813 0.676 0.627 0.784 0.815 0.749 0.567\nT@1F 0.320 0.264 0.095 0.102 0.170 0.262 0.210 0.040\n0.500AUC 0.786 0.774 0.648 0.617 0.741 0.771 0.690 0.510\nT@1F 0.169 0.120 0.053 0.059 0.097 0.146 0.100 0.020\n0.600AUC 0.721 0.693 0.606 0.578 0.678 0.651 0.631 0.522\nT@1F 0.101 0.068 0.020 0.023 0.059 0.056 0.110 0.020\n0.700AUC 0.648 0.648 0.582 0.556 0.623 0.605 0.642 0.508\nT@1F 0.049 0.037 0.015 0.026 0.015 0.064 0.110 0.050\n0.800AUC 0.629 0.627 0.566 0.553 0.610 0.599 0.596 0.514\nT@1F 0.047 0.031 0.012 0.017 0.019 0.040 0.070 0.070\n0.900AUC 0.547 0.554 0.535 0.514 0.546 0.510 0.561 0.483\nT@1F 0.016 0.012 0.009 0.010 0.015 0.016 0.000 0.030\nRAR0.100AUC 1.000 1.000 0.995 0.991 0.999 1.000 0.993 0.870\nT@1F 1.000 0.999 0.956 0.918 0.995 1.000 0.920 0.240\n0.200AUC 1.000 1.000 0.987 0.984 0.999 1.000 0.988 0.815\nT@1F 1.000 0.999 0.897 0.853 0.980 1.000 0.940 0.280\n0.300AUC 1.000 1.000 0.977 0.973 0.998 1.000 0.981 0.767\nT@1F 1.000 0.998 0.834 0.764 0.960 1.000 0.830 0.220\n0.400AUC 0.999 0.999 0.966 0.959 0.994 1.000 0.977 0.715\nT@1F 0.999 0.994 0.753 0.640 0.947 1.000 0.940 0.150\n0.500AUC 0.999 0.998 0.959 0.940 0.988 1.000 0.975 0.731\nT@1F 0.999 0.986 0.652 0.553 0.903 1.000 0.550 0.110\n0.600AUC 0.999 0.997 0.940 0.918 0.984 1.000 0.966 0.734\nT@1F 0.999 0.973 0.583 0.498 0.849 0.998 0.710 0.100\n0.700AUC 1.000 0.994 0.912 0.882 0.980 1.000 0.947 0.693\nT@1F 0.998 0.951 0.488 0.384 0.817 0.990 0.650 0.070\n0.800AUC 0.999 0.983 0.876 0.834 0.960 0.998 0.928 0.660\nT@1F 0.987 0.809 0.370 0.314 0.571 0.944 0.380 0.020\n0.900AUC 0.996 0.945 0.817 0.781 0.902 0.988 0.878 0.584\nT@1F 0.911 0.485 0.193 0.281 0.343 0.812 0.320 0.060\nTable 2: Robustness for LBW-Post for V AR, VQ-GAN and RAR across different γ, ranging from 0.1 to 0.9.\n16\n--- Page 17 ---\nAPREPRINT - JUNE 3, 2025\nγ Metric Clean Gaus Color Geom JPEG V AE Diff Ctrl\nV AR0.100AUC 0.999 0.995 0.994 0.660 0.988 0.991 0.896 0.623\nT@1F 0.995 0.967 0.969 0.281 0.932 0.903 0.526 0.000\n0.200AUC 0.998 0.992 0.992 0.645 0.982 0.983 0.819 0.542\nT@1F 0.988 0.930 0.958 0.277 0.909 0.890 0.500 0.040\n0.300AUC 0.997 0.983 0.985 0.650 0.972 0.974 0.844 0.635\nT@1F 0.978 0.903 0.908 0.269 0.826 0.838 0.420 0.020\n0.400AUC 0.994 0.975 0.977 0.629 0.955 0.954 0.775 0.638\nT@1F 0.958 0.839 0.888 0.267 0.764 0.656 0.330 0.120\n0.500AUC 0.992 0.966 0.973 0.639 0.951 0.933 0.792 0.604\nT@1F 0.947 0.775 0.824 0.260 0.708 0.519 0.340 0.090\n0.600AUC 0.988 0.953 0.960 0.637 0.929 0.910 0.742 0.572\nT@1F 0.912 0.697 0.769 0.246 0.564 0.415 0.100 0.020\n0.700AUC 0.978 0.931 0.941 0.626 0.912 0.867 0.725 0.512\nT@1F 0.830 0.557 0.654 0.229 0.497 0.273 0.270 0.070\n0.800AUC 0.966 0.898 0.917 0.619 0.867 0.828 0.696 0.609\nT@1F 0.752 0.430 0.531 0.197 0.437 0.164 0.110 0.070\n0.900AUC 0.925 0.828 0.853 0.596 0.808 0.749 0.684 0.533\nT@1F 0.535 0.243 0.351 0.144 0.267 0.076 0.070 0.000\nVQ-GAN0.100AUC 0.998 0.969 0.858 0.903 0.921 0.993 0.973 0.857\nT@1F 0.993 0.909 0.700 0.482 0.745 0.978 0.870 0.370\n0.200AUC 0.999 0.987 0.912 0.939 0.966 0.998 0.992 0.869\nT@1F 0.998 0.970 0.714 0.641 0.849 0.996 0.990 0.440\n0.300AUC 1.000 0.994 0.902 0.871 0.964 1.000 0.984 0.842\nT@1F 1.000 0.979 0.654 0.411 0.835 0.999 0.940 0.410\n0.400AUC 1.000 0.990 0.892 0.831 0.956 1.000 0.975 0.796\nT@1F 0.999 0.963 0.547 0.371 0.827 1.000 0.890 0.330\n0.500AUC 1.000 0.991 0.881 0.793 0.950 0.999 0.967 0.764\nT@1F 1.000 0.953 0.449 0.339 0.659 0.997 0.820 0.170\n0.600AUC 1.000 0.986 0.854 0.779 0.932 0.998 0.947 0.644\nT@1F 0.997 0.921 0.367 0.299 0.591 0.987 0.660 0.020\n0.700AUC 0.998 0.970 0.820 0.751 0.908 0.993 0.913 0.695\nT@1F 0.985 0.794 0.167 0.292 0.421 0.859 0.480 0.030\n0.800AUC 0.997 0.959 0.785 0.736 0.876 0.987 0.870 0.698\nT@1F 0.974 0.666 0.168 0.277 0.327 0.867 0.251 0.080\n0.900AUC 0.989 0.917 0.727 0.706 0.809 0.966 0.792 0.641\nT@1F 0.784 0.376 0.082 0.227 0.094 0.544 0.110 0.010\nRAR0.100AUC 1.000 1.000 0.997 0.964 0.999 1.000 1.000 0.978\nT@1F 1.000 0.998 0.973 0.846 0.993 1.000 1.000 0.800\n0.200AUC 1.000 1.000 0.996 0.951 0.999 1.000 1.000 0.951\nT@1F 1.000 0.998 0.967 0.802 0.992 1.000 0.980 0.430\n0.300AUC 1.000 1.000 0.988 0.939 0.996 1.000 0.997 0.917\nT@1F 1.000 0.995 0.890 0.725 0.964 1.000 0.960 0.420\n0.400AUC 0.999 0.998 0.976 0.913 0.982 1.000 0.999 0.844\nT@1F 1.000 0.989 0.805 0.634 0.878 0.997 0.980 0.150\n0.500AUC 0.999 0.998 0.973 0.901 0.985 1.000 0.992 0.832\nT@1F 1.000 0.976 0.759 0.534 0.879 0.997 0.840 0.060\n0.600AUC 0.999 0.994 0.952 0.878 0.979 0.999 0.982 0.785\nT@1F 0.998 0.958 0.628 0.474 0.867 0.993 0.820 0.090\n0.700AUC 0.999 0.992 0.932 0.851 0.967 0.998 0.980 0.771\nT@1F 0.998 0.935 0.556 0.395 0.741 0.978 0.810 0.130\n0.800AUC 0.997 0.981 0.891 0.806 0.946 0.994 0.966 0.669\nT@1F 0.984 0.833 0.355 0.338 0.491 0.885 0.810 0.120\n0.900AUC 0.986 0.927 0.806 0.762 0.891 0.964 0.879 0.540\nT@1F 0.880 0.481 0.203 0.261 0.348 0.559 0.190 0.020\nTable 3: Robustness for LBW-Hard for V AR, VQ-GAN and RAR across different γ, ranging from 0.1 to 0.9.\n17\n--- Page 18 ---\nAPREPRINT - JUNE 3, 2025\nσ Metric Clean Gaus Color Geom JPEG V AE Diff Ctrl\nV AR1.000AUC 0.927 0.852 0.873 0.505 0.844 0.745 0.632 0.543\nT@1F 0.615 0.341 0.394 0.021 0.339 0.030 0.000 0.000\n2.000AUC 0.990 0.963 0.969 0.502 0.947 0.949 0.739 0.523\nT@1F 0.940 0.764 0.833 0.022 0.751 0.450 0.120 0.000\n3.000AUC 0.997 0.984 0.986 0.507 0.977 0.979 0.846 0.575\nT@1F 0.984 0.904 0.929 0.020 0.878 0.840 0.300 0.010\n4.000AUC 0.998 0.993 0.992 0.519 0.984 0.989 0.882 0.622\nT@1F 0.993 0.945 0.953 0.027 0.915 0.870 0.400 0.000\n5.000AUC 0.999 0.995 0.993 0.650 0.988 0.992 0.844 0.620\nT@1F 0.994 0.961 0.966 0.274 0.931 0.920 0.390 0.010\n6.000AUC 0.999 0.995 0.994 0.647 0.986 0.992 0.874 0.589\nT@1F 0.995 0.961 0.969 0.274 0.933 0.920 0.470 0.010\n7.000AUC 1.000 0.995 0.994 0.665 0.989 0.995 0.892 0.626\nT@1F 0.997 0.977 0.966 0.275 0.934 0.930 0.480 0.010\n8.000AUC 0.999 0.995 0.994 0.662 0.987 0.994 0.882 0.614\nT@1F 0.995 0.966 0.968 0.283 0.933 0.920 0.460 0.000\n9.000AUC 0.999 0.994 0.993 0.665 0.986 0.990 0.879 0.617\nT@1F 0.995 0.967 0.968 0.275 0.934 0.910 0.470 0.000\nVQ-GAN1.000AUC 0.986 0.959 0.786 0.786 0.901 0.990 0.993 0.669\nT@1F 0.952 0.733 0.303 0.360 0.539 0.918 0.980 0.060\n2.000AUC 1.000 0.993 0.899 0.911 0.988 1.000 0.988 0.758\nT@1F 0.998 0.969 0.620 0.576 0.921 1.000 0.990 0.180\n3.000AUC 0.999 0.990 0.925 0.956 0.991 0.998 0.985 0.829\nT@1F 0.999 0.976 0.735 0.709 0.960 0.996 0.940 0.380\n4.000AUC 0.999 0.990 0.934 0.966 0.995 0.998 0.994 0.915\nT@1F 0.998 0.977 0.770 0.776 0.977 0.998 0.990 0.610\n5.000AUC 0.999 0.988 0.924 0.962 0.993 0.998 0.997 0.887\nT@1F 0.998 0.976 0.749 0.773 0.969 0.998 0.980 0.470\n6.000AUC 0.999 0.986 0.917 0.966 0.991 0.998 0.990 0.889\nT@1F 0.997 0.970 0.734 0.768 0.958 0.996 0.970 0.460\n7.000AUC 0.999 0.986 0.910 0.967 0.990 0.998 0.995 0.889\nT@1F 0.997 0.968 0.719 0.773 0.947 0.996 0.960 0.460\n8.000AUC 0.999 0.986 0.907 0.965 0.990 0.998 0.986 0.879\nT@1F 0.997 0.970 0.720 0.761 0.948 0.996 0.940 0.360\n9.000AUC 0.999 0.986 0.907 0.963 0.989 0.998 0.987 0.876\nT@1F 0.997 0.970 0.720 0.768 0.947 0.996 0.930 0.360\nRAR1.000AUC 0.961 0.912 0.802 0.758 0.865 0.925 0.799 0.632\nT@1F 0.633 0.314 0.169 0.254 0.292 0.522 0.170 0.010\n2.000AUC 0.998 0.993 0.946 0.879 0.982 0.996 0.968 0.790\nT@1F 0.986 0.930 0.639 0.522 0.800 0.981 0.720 0.130\n3.000AUC 1.000 0.999 0.984 0.929 0.998 1.000 0.994 0.915\nT@1F 0.999 0.988 0.860 0.722 0.973 0.996 0.970 0.430\n4.000AUC 1.000 1.000 0.994 0.947 0.998 1.000 1.000 0.958\nT@1F 0.999 1.000 0.940 0.794 0.980 0.999 1.000 0.620\n5.000AUC 1.000 1.000 0.997 0.953 1.000 1.000 1.000 0.958\nT@1F 1.000 1.000 0.968 0.823 0.995 1.000 1.000 0.620\n6.000AUC 1.000 1.000 0.997 0.958 1.000 1.000 1.000 0.978\nT@1F 1.000 1.000 0.971 0.842 0.998 1.000 1.000 0.770\n7.000AUC 1.000 1.000 0.998 0.958 1.000 1.000 1.000 0.961\nT@1F 1.000 1.000 0.978 0.851 0.997 1.000 1.000 0.740\n8.000AUC 1.000 1.000 0.999 0.961 1.000 1.000 1.000 0.978\nT@1F 1.000 1.000 0.981 0.844 0.999 1.000 0.990 0.760\n9.000AUC 1.000 1.000 0.998 0.957 1.000 1.000 1.000 0.976\nT@1F 1.000 1.000 0.980 0.847 0.997 1.000 1.000 0.770\nTable 4: Robustness for LBW-Soft for V AR, VQ-GAN and RAR across different σ, ranging from 1 to 9.\n18\n--- Page 19 ---\nAPREPRINT - JUNE 3, 2025\nFurthermore, LBW-Post introduces an adjustable hyperparameter γ(green word ratio) to regulate the trade-off between\nwatermark embedding strength and image quality. This allows users to fine-tune the embedding strategy according\nto specific application requirements, balancing image quality and watermark robustness, thereby adapting to various\npractical scenarios.\nMore Comparison between LBW-Hard and LBW-Soft.\nLBW -Soft LBW -Hard Clean\nVAR VQ-GAN RAR\nFigure 11: Comparison between LBW-Hard and LBW-Soft across V AR, VQ-GAN, and RAR.\nFigure 11 compares the performance of LBW-Hard and LBW-Soft across different generative models. The results\nindicate that for AR models utilizing multi-scale token maps (e.g., V AR), both methods yield visually comparable\nresults. However, for single-scale token map models (e.g., VQ-GAN and RAR), LBW-Soft outperforms LBW-Hard,\nproducing images with richer details and stronger class relevance.\nMoreover, LBW-Soft demonstrates superior robustness when the green word ratio is low, a scenario in which LBW-Hard\nmay fail to generate meaningful images. This highlights LBW-Soft as a more adaptable solution that ensures stable\nimage synthesis even under challenging conditions.\n19",
  "text_length": 64152
}