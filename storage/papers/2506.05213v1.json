{
  "id": "http://arxiv.org/abs/2506.05213v1",
  "title": "LLM-First Search: Self-Guided Exploration of the Solution Space",
  "summary": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.",
  "authors": [
    "Nathan Herr",
    "Tim Rocktäschel",
    "Roberta Raileanu"
  ],
  "published": "2025-06-05T16:27:49Z",
  "updated": "2025-06-05T16:27:49Z",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05213v1",
  "full_text": "--- Page 1 ---\nLLM-First Search: Self-Guided Exploration\nof the Solution Space\nNathan Herr\nCentre for AI, University College London\nuceenhe@ucl.ac.ukTim Rocktäschel\nCentre for AI, University College London\nRoberta Raileanu\nCentre for AI, University College London\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable improvements\nin reasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed exploration\nhyperparameters limits their adaptability across tasks of varying difficulty, render-\ning them impractical or expensive in certain settings. In this paper, we propose\nLLM-First Search (LFS) , a novel LLM Self-Guided Search method that removes\nthe need for pre-defined search strategies by empowering the LLM to autonomously\ncontrol the search process via self-guided exploration. Rather than relying on ex-\nternal heuristics or hardcoded policies, the LLM evaluates whether to pursue the\ncurrent search path or explore alternative branches based on its internal scoring\nmechanisms. This enables more flexible and context-sensitive reasoning without re-\nquiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown\nand Sudoku against three classic widely-used search algorithms, Tree-of-Thoughts’\nBreadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of\nwhich have been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with increased\ncompute budget. Our code is publicly available at LLM-First-Search.\n1 Introduction\nThe reasoning and planning capabilities of Large Language Models (LLMs) have advanced signifi-\ncantly through increased test-time compute, akin to human System 2 thinking, slow and deliberate,\nversus fast, intuitive System 1 thinking [ 1]. Early prompting techniques such as Chain of Thought\n(CoT) [ 2] enabled basic System 2 reasoning, but recent work reframes reasoning as a search problem\n[3,4], leveraging classic algorithms such as Beam Search [ 5], Depth- and Breadth-First Search\n(DFS, BFS) [ 6,7], Best-First Search [ 8], and Monte Carlo Tree Search (MCTS) [ 9,10]. MCTS\naugmented with LLMs has proven effective across domains [ 11,12,13] and is widely adopted.\nThese systems often integrate LLM world models, reward/value estimators, self-consistency, self-\nrefinement, multi-agent debate, and memory modules to achieve state-of-the-art (SotA) results\n[14, 15, 16, 17, 18, 19, 20, 21].\nPreprint. Under review.arXiv:2506.05213v1  [cs.AI]  5 Jun 2025\n--- Page 2 ---\n:)Start\n:(MCTS : fixed handcrafted exploration\nStart\n:(ToT-BFS : under-explores  \n:)Start\n:(BestFS : over-exploits\n:)Start\n:(LFS: adaptive LLM-guided explorationJump to new\nnode to exploreVisited Node\nUnexplored node\nSelection and\nBackpropagationFiltered out node\nSuboptimal\ndecisionFigure 1: Illustrative comparison of search strategies. This figure visualises how different methods\nexpand the search tree during reasoning. Tree of Thought Breadth-First Search ( TOT-BFS) risks\nprematurely discarding promising paths due to rigid filtering criteria. Best-First Search ( BESTFS)\ntends to over-exploit high-scoring nodes based on early estimations, potentially overlooking better\nlong-term solutions. Monte Carlo Tree Search ( MCTS )relies heavily on a fixed exploration\nconstant, which can lead to either excessive exploration or over-commitment to suboptimal paths.\nIn contrast, our proposed method, LLM-First Search ( LFS ), removes the need for hand-tuned\nhyperparameters and handcrafted heuristics. Instead, it repurposes the LLM to both act and evaluate,\nenabling dynamic, model-guided decisions about whether to pursue the current reasoning path or\nexplore alternatives. This tight integration between evaluation and exploration leads to more adaptive\nand efficient reasoning. A full search tree for both MCTS and LFS can be found in Appendix Section\nH. For clarity, the small circles (white and yellow) attached to the visited nodes refer the nodes’\nneighbours. Additionally, the dotted arrows refer to the edges that have not been traversed.\nA key limitation of MCTS is its sensitivity to the exploration-exploitation trade-off controlled by the\nexploration constant C[9,10]. Although hyperparameter tuning [ 22] can optimise performance for\na specific task, a fixed Ccannot adapt to varying problem difficulties or LLM capabilities. Over-\nexploration hampers performance on simpler tasks where the LLM has strong priors, while under-\nexploration limits success on harder problems needing broader search [ 23,24]. This longstanding\nissue [ 25,26] parallels findings in Large Reasoning Models, which may overthink simple tasks due\nto excessive reliance on System 2 thinking [ 27,28], analogous to MCTS’s over-exploration from too\nhigh an exploration constant.\nIn this paper, we introduce LLM-First Search (LFS) , a novel approach that eliminates the need\nfor manually tuned exploration hyperparameters, handcrafted heuristics, and traditional search\nalgorithms. Building on recent MCTS extensions [ 14,15] and methods placing LLMs at the core\nof self-improvement [ 29], LFS puts the LLM in control of the search process. Unlike MCTS,\nwhich relies on fixed exploration schedules, LFS lets the model autonomously decide whether to\ncontinue along the current path or explore alternatives based on its own evaluation, enabling adaptive,\nintegrated exploration. A high-level depiction of how LFS works and how it overcomes the shortfalls\nof MCTS, as well as other well-established search algorithms, can be seen in Figure 1. We validate\nLFS on two reasoning tasks, Countdown andSudoku , showing competitive or superior performance\nwith greater flexibility and adaptability than static search methods.\n2\n--- Page 3 ---\nOur main contributions are: (1) introducing LLM-First Search , a novel method that reimagines\nclassical search by allowing the LLM itself to drive exploration, decision-making, and evaluation,\nremoving the need for predefined search algorithms, (2) propose a fully LLM-guided scoring\nand selection mechanism , where the LLM evaluates whether the current search path is promising\nand dynamically decides to continue on this path or explore alternative paths, removing the need\nfor manually tuned exploration hyperparameters, and (3) demonstrate, through experiments on\nCountdown andSudoku , that LFS achieves competitive or superior performance relative to other\npopular search algorithms, while also demonstrating greater efficiency, adaptability to task complexity,\nand scalability with increased model strength and compute budget .\n2 Preliminaries\n2.1 Problem Setting\nMarkov Decision Process. We consider problems that can be formulated as Markov Decision\nProcesses (MDPs) [ 30], where an agent interacts with an environment over a sequence of discrete\ntime steps to achieve a goal. Formally, an MDP is defined by a tuple (S,A, P, R, γ ), where the agent\nobserves a state s∈ S, selects an action a∈ A, transitions to a new state s′∼T(· |s, a), and\nreceives a reward R(s, a).\nLLM Agents. LLM agents are autonomous decision-making systems powered by large language\nmodels. Given an MDP, the LLM serves as a policy πθ:S × T → A parameterised by θ, where\nπθ(at|st,T)denotes the likelihood of taking action atconditioned on the current state stand task\nT, to maximise the expected reward. These agents leverage language as a unified interface to perform\nenvironment understanding, reasoning and planning, and ultimately action execution [ 31,32,33].\nIn our formulation, the LLM agent is provided with a natural language task description, the text\ndescription of the current state, and a list of valid next actions. The agent selects an action from this\nlist, after which the environment deterministically transitions to a new state. This process is repeated\nuntil a terminal state is reached, at which point a reward is provided based on task success (e.g. win\nor lose). The specific MDP instantiations and prompts used for our two benchmark tasks, Countdown\nand Sudoku, are described in Section 5.2.\n3 Related Work\nTo enable models to reason more deeply and deliberately, researchers have developed a range of\nstrategies, which we have broadly categorised as: (1) Single-Shot Reasoning , which elicits reasoning\nin a single prompt; (2) Iterative and Reflective Reasoning , which refines outputs through multiple\nsteps; and (3) Structured Search-Based Reasoning , which treats reasoning as a search process. We\nbriefly cover the first two, with a primary focus on the third, where our method lies.\nSingle-Shot Reasoning. Chain-of-Thought (CoT) prompting [ 2] encourages step-by-step reasoning\nvia demonstrations, later simplified by minimal prompts like “think step by step,” which elicit similar\nbehaviour without examples [ 34]. Building on these foundations, several adaptations of these works\nhave been explored [ 35,36,37]. Recently, a “wait” token to slow down reasoning was introduced [ 38],\nthough it requires fine-tuning and is not purely an inference-time approach. Single-shot prompting has\nalso been used to elicit more complex behaviours such as meta-in-context learning [ 39] and in-context\ndistillation of algorithms like MCTS [ 40,41]. While these methods have been effective on simpler\ntasks, they are inherently non-iterative and struggle to adapt to more complex tasks [42, 43, 44, 45].\nReflective and Interactive Reasoning. To go beyond linear reasoning, iterative and feedback-\ndriven techniques have been proposed. A simple and widely used extension is self-consistency [ 42],\nwhich samples multiple CoT outputs and selects the most consistent answer. ReAct [ 43] combines\nreasoning steps with task-specific actions and incorporates feedback to guide future steps. Other\nworks refine LLM outputs through self-reflection or external feedback [ 44,45,46]. Multi-agent\ndebate frameworks [ 47,48] further enhance reasoning by simulating dialogues between LLM agents\nto converge on a better final answer. However, these methods typically result in shallow exploration\nand lack explicit backtracking, limiting their ability to perform structured reasoning over long\nhorizons or systematically explore multiple solution paths [49, 50, 3, 14, 15].\n3\n--- Page 4 ---\nStructured Search-Based Reasoning. A growing line of work treats reasoning as a search problem,\nusing classic search algorithms to guide LLMs through the task’s search space, greatly improving the\nLLM’s ability to solve complex reasoning and planning tasks. For example, [ 49] proposes a stochastic\nbeam search that samples and selects among multiple candidates at each step. Tree-of-Thoughts\n(ToT) [ 50] introduces breadth-first and depth-first expansions of CoT-style reasoning, decoupling\nnext-action selection and state value estimation. Several extensions have been proposed [ 51,52],\nthough ToT remains the most prominent. Other works incorporate more advanced algorithms like\nBest-First Search [ 3] and Monte Carlo Tree Search (MCTS) [ 14,15,17,18,19,20,21,53]. For\nexample, RAP [ 14] uses MCTS with LLMs serving as a world model and a novel reward function\ncomposed of action likelihood and confidence, self-evaluation, and task-specific heuristics. LATS\n[15] extends RAP by incorporating environment feedback and reflective evaluation. More recent\nworks integrate additional prompting strategies, such as reflection [ 17,18,19] and multi-agent debate\n[17], for further performance gains. REX [ 16] augments MCTS by allowing the LLM to perform\nmultiple search steps, selection, expansion, and simulation, in a single response. The resulting actions\nare assigned rewards that are then backpropagated through each generated action. AB-MCTS [ 53]\nintroduces a novel node \"GEN-node\" which is a possible child for all nodes in the tree, which, if\nselected, prompts the LLM to create additional branches. While these methods have demonstrated\nstrong performance, they are fundamentally built on traditional search algorithms that often rely\non carefully tuned hyperparameters and handcrafted heuristics, limiting adaptability and requiring\nre-tuning for new tasks [ 19], rendering them impractical or very expensive for real use cases. Most\nrecent works in this area represent incremental improvements to the base LLM-augmented variants of\nclassic search algorithms, often incorporating additional prompting strategies like reflection or debate.\nIn our evaluation, we compare against these foundational approaches, as our method addresses their\ncore limitations while remaining compatible with, and is likely to benefit from, the same incremental\nenhancements.\n4 LLM-First Search (LFS) Algorithm 1 LLM-First Search (LFS)\n1:Input: LLM πθ, Prompts Peval and\nPexplore , Transition function T\n2:Initialise s0,A0, Priority queue Q\n3:{Vi}|A0|\ni=1=Peval(s0,A0, πθ)\n4:a∗\n0=A0[arg max iV0]\n5:Q:=Q ∪ { a∈ A 0|a̸=a∗\n0}\n6:(s1,A1)∼T(· |s0, a∗\n0)\n7:t= 1\n8:while Token limit not exhausted do\n9: ifPexplore (st,At, πθ)then\n10: (s′\nt,A′\nt)←pop(Q)\n11: else\n12: {Vi}|At|\ni=1=Peval(st,At, πθ)\n13: a∗\nt=At[arg max iVi]\n14: Q:=Q ∪ { a∈ At|a̸=a∗\nt}\n15: (s′\nt,A′\nt)∼T(· |st′, a∗\nt′)\n16: end if\n17: (st,At)←(s′\nt,A′\nt)\n18: t←t+ 1\n19:end while\n20:Return: (st,At)In this section, we introduce LLM-First Search\n(LFS) , a method that empowers language mod-\nels to self-guide their own search process by au-\ntonomously exploring and evaluating states and\nactions, enabling flexible, context-sensitive rea-\nsoning without manual tuning or task-specific\nadaptation. Specifically, given a task that can\nbe initialised as a MDP, the LLM continuously\ninteracts with the task environment, performing\ntwo key operations; (1) Explore , where it de-\ncides whether to continue along the current path\nor explore alternatives, and (2) Evaluate , where\nit estimates the value of each available action\nat the current state. We were able to show that\nLLMs can effectively internalise and manage\nthis process on their own, matching or exceeding\nthe performance of traditional methods. The op-\nerations are detailed in the following paragraphs,\nwith a high-level overview of LFS provided in\nAlgorithm 1.\nExploration Decision. At each step, given\nthe current state stand available actions At, the agent is prompted with an exploration prompt\nPexplore (st,At)(the exact prompt can be seen in Appendix Section E) to decide whether to exploit the\ncurrent path or to explore an alternative. If the agent chooses to exploit , it proceeds to the evaluation\nstep using the actions in At. Otherwise, if the agent opts to explore , it pops the highest-value node\nfrom the priority queue Q:\n(s′\nt,A′\nt)←pop(Q),\n4\n--- Page 5 ---\nand proceeds to the evaluation step using the new state s′\ntand corresponding actions A′\nt. This\ndynamic allows the agent to balance short-term commitment with broader exploration based entirely\non its own internal judgment.\nEvaluate. At each step, given a state st∈ S, a set of available actions At={a1\nt, . . . , ak\nt}, and\nan evaluation prompt Peval(st,At)(the exact prompt can be seen in Appendix Section E), the LLM\nis prompted to estimate the value V(ai\nt|st)for each action, representing its utility or promise of\nleading to a high-reward solution. The best action is then selected:\na∗\nt=Ath\narg max\niVii\nwhere {Vi}|At|\ni=1=Peval(st,At)\nand executed, while all other candidate actions are added to a priority queue Qsorted by their\nestimated value. This structure enables efficient retrieval of high-potential alternatives in future\nexploration steps.\n5 Experiments\n5.1 Baselines\nTo ensure a fair comparison, all methods are evaluated using the same task setup and prompting\nformat. We isolate the core effect of each search strategy by excluding incremental enhancements\nsuch as self-consistency, reflection, and debate, which are known to improve performance across many\nLLM-augmented approaches. Each method is tested with two models, GPT-4o and o3-mini (through\nthe OpenAI API [ 54], with the configurations detailed in Appendix Section D), to assess performance\nacross different model scales. We compare our approach against several strong LLM-augmented\nsearch baselines widely adopted in the literature. See Appendix Section B for baseline details.\nThree-of-Thoughts Breadth-First Search (ToT-BFS): Adapted from the setup in Tree-of-Thoughts\n(ToT) [ 50], ToT-BFS expands a subset of child nodes up to a fixed depth. At each level, the LLM\nestimates the value of all child states, and only the top- kstates (with k= 5[50]) are retained for\nfurther expansion. This process continues until a predefined maximum search depth is reached. Note\nthat while ToT describe a DFS implementation, in our preliminary experiments, we found that DFS\ndid not perform sufficiently (similar findings in [ 17]) and was therefore not considered further. In\nfurther support of this decision, in the ToT paper, they use countdown to test the BFS variant.\nBest-First Search (BestFS): Following the approach in Tree Search for Language model Agents\n[3], BestFS uses the LLM to estimate the value of the current state, which is then added to a priority\nqueue. The next state to expand is selected greedily by popping the highest value from the queue.\nThis process repeats until a solution is found or the search budget is exhausted.\nTable 1: AUP for MCTS-GPT4o across\ncvalues.\nMCTS (c) WinRate EfficiencyScore\n0.5 7.20 7.16\n1.0 5.39 5.31\n2.5 5.38 5.25Monte Carlo Tree Search (MCTS): Based on\nimplementations from RAP [14] and LATS [15]\nand inspired by AlphaGo [55], we use PUCT to\nguide the MCTS algorithm. Specifically, at each\nstep, the LLM is used to (1) estimate a prior dis-\ntribution over available actions at a given state,\nand (2) estimate the value of a leaf state after an\naction is simulated (the specific prompts used\nto elicit these behaviours can be found in Ap-\npendix Section E). These estimations are then integrated into the PUCT selection formula to balance\nexploration and exploitation. We performed a hyperparameter sweep over different exploration\nconstants C∈ {0.5,1.0,2.5}. The specifics of this can be found in Appendix Section F. We noted\nthatC= 0.5performed similarly to C= 1.0in Countdown, but outperformed C= 1.0in Sudoku\n(4x4), resulting in C= 0.5achieving the best AUP. Refer to Table 1 for these results.\n5.2 Tasks\nWe evaluate our method and the baselines on two widely used reasoning and planning benchmarks:\nCountdown andSudoku . They are widely adopted in the literature as reliable testbeds for evaluating\n5\n--- Page 6 ---\nstructured reasoning with LLMs [ 50,15,56,57]. These benchmarks are particularly suitable for\nour evaluation for two key reasons: (1) Scalability , both Countdown and Sudoku allow for fine-\ngrained control over difficulty, enabling evaluation across a spectrum of task complexities; and (2)\nComplementarity: Countdown offers a shallower search space with fewer steps, but selecting the\ncorrect action is often more challenging, even for humans. Conversely, Sudoku involves a much\ndeeper search space with many more decision points, though it tends to be more intuitive for human\nsolvers. Together, these benchmarks provide a balanced and comprehensive evaluation of search\nstrategies across fundamentally different reasoning challenges. A more detailed discussion of the\nbranching factors and widths of the two benchmarks can be found in Appendix Section C.\n5.2.1 Countdown\nCountdown [ 58] generalises the classic Game of 24 [50,15] and has become a challenging benchmark\nfor evaluating LLM search due to its high branching factor and large combinatorial search space\n[59,56]. The goal is to reach a target number tusing arithmetic operations (+,−,×,÷)applied to\na list of numbers n= [n1, n2, . . . , n l], where each number can be used at most once. For example,\ngiven n= [1,2,3,4,5]andt= 10 , a valid sequence is: 5 + 4 = 9 ,3−2 = 1 ,9 + 1 = 10 ,\n1×10 = 10 .\nSetup. Following prior work [ 50,56], we evaluate three difficulty levels with input lengths l∈\n{3,5,7}and target tsampled uniformly from [10,100]. Each environment state siis a 4-tuple:\nsi= (t, ni, oi, Ai),\nwhere tis the fixed target, niis the current number set, oithe operation history, and Aithe available\nactions. Each action a∈Aiapplies an arithmetic operation to two distinct numbers nj, nk∈ni,\nproducing a new number and modifying the set. The agent must find a sequence of actions that\ntransforms nintot. This setup naturally fits the MDP formalism: Sis the space of number-operation\nconfigurations, A(s)the valid actions in state s, transitions modify the number set and operations\nbased on the selected action, and the episode terminates on success or exhaustion of valid actions.\nThe reward is 1 if the target is reached, and 0 otherwise. Prompting details are provided in Appendix\nSection E.\n5.2.2 Sudoku\nSudoku is a constraint satisfaction puzzle played on an ℓ×wgrid. The objective is to fill each cell\nwith a value from a finite set N={1,2, . . . , ℓ ×w}such that each value appears exactly once in\nevery row, column, and subgrid. While the classic version uses a 9×9grid with 3×3subgrids, we\ngeneralise to arbitrary grid sizes, making Sudoku a rich, scalable benchmark for reasoning and search\nin structured environments.\nSetup. We evaluate agents on two grid configurations: a 4×4board (with 2×2subgrids) and a\nmore challenging 6×6board (with 2×3subgrids). Each environment state siis defined as:\nsi= (Bi, Ai),\nwhere Bi∈Σℓ×wis the current board and Aithe set of valid actions. Each action a∈Aiis a tuple\n(x, y, v )assigning value v∈Nto cell (x, y)without violating Sudoku constraints. Upon executing\nan action, the board is updated and valid actions recomputed. Episodes terminate when all cells are\nfilled and constraints satisfied. As an MDP: Sis the set of all valid partial boards, A(s)the set of\nvalid (x, y, v )assignments, transitions update the board, and reward is 1 if the final board satisfies all\nconstraints, and 0 otherwise. See Appendix Section E for details on prompts used.\n5.3 Evaluation\n5.3.1 Metrics\nDue to the stochastic nature of language model generation [ 60], we evaluate each method by running\nevery game ntimes (with n= 5) at temperature t= 0.0. Letwi,j,r∈ {0,1}indicate whether method\njsuccessfully solves game iin a given run r. The WinRate for game iunder method jis defined as:\nWinRate i,j=1\nnX\nr∈nwi,j,r , where gamei,jis solved if WinRate i,j>0.5\n6\n--- Page 7 ---\nTo evaluate the methods over the set of games G, we report both the average WinRate ( WinRate∗\nj),\nand the ratio between WinRate∗\njand Tokens∗\nj(EfficiencyScorej). They are defined as follows:\nWinRate∗\nj=1\n|G|X\ni∈GWinRate i,j & EfficiencyScorej=WinRate∗\nj\nTokens∗\nj\nwhere Tokens∗\njrepresents the average number of tokens used across the games by method j.\nTo account for statistical uncertainty, we compute 95% confidence intervals for the average WinRate\nusing the Wilson score interval method [ 61] which we report in the figures in Appendix Section G.\nThis approach is preferred over the standard normal approximation in scenarios with fewer number\nof trials.\n5.3.2 Performance Profiles and AUP Score\nSimilar to [ 62], we use performance profile curves [ 63] and Area Under Performance Profile (AUP)\n[64] to compare our method to the baselines by aggregating the metrics across all the tasks. A\nperformance profile curve is defined as:\nρm(τ) =1\n|T||{t∈T:log10(rt,m)≤τ}\nwhere rt,m=max{ℓt,m:m∈M}\nℓmis the performance ratio, Mis the set of all methods, P is the set\nof tasks, and ℓmis the performance metric for the method mon task t. A performance profile is\nparametrised by τ, which is a threshold on the distance between the method m and the best scoring\nmethod on each of the tasks. The performance profile computes the proportion of tasks which method\nmfalls within τof the best method for each task. Following which, the AUP can be defined as:\nAUP m=Zτmax\n1ρm(τ)dτ\nwhere τmax is the minimum τfor which ρm(τ)has reached its maximum value for all m∈ M .\n6 Results and Analysis\n6.1 Task Specific\nCountdown. In Table 2 we can see that in Countdown (Diff=3) all methods, except for TOT-BFS-\nGPT4 O, are capable of solving 100% of the problems. TOT-BFS-GPT4 Olags behind due to the\nlack of backtracking, compared to the other methods tested. Therefore, due to compute constraints,\nTOT-BFS- O3MINI is not tested. Additionally, no methods are tested with o3-mini in Countdown\n(Diff=3), as it is already near saturation with a weaker model. Following this, we can see that as we\nTable 2: WinRate (%) of each method across all tasks, evaluated with GPT-4o and o3-mini. LFS\nachieves the highest WinRates on all tasks for both models, except for Sudoku (4 ×4) when evaluated\nwith GPT-4o.\nCountdown Sudoku\nModel Method Diff 3 Diff 5 Diff 7 4x4 6x6\nGPT-4oTOT-BFS 82.11 9.47 0.00 53.68 0.00\nBESTFS 100 49.47 11.11 41.05 0.00\nMCTS ( C=0.5) 100 60.00 32.63 100 0.00\nMCTS ( C=1.0) 100 62.22 33.33 2.22 0.00\nMCTS ( C=2.5) 100 60.00 24.44 0.00 0.00\nLFS ( OURS ) 100 63.16 47.37 96.84 2.22\no3-miniBESTFS – 52.63 13.33 61.05 0.00\nMCTS ( C=0.5) – 69.47 41.05 90.53 4.21\nLFS ( OURS ) – 70.53 78.95 96.84 25.26\n7\n--- Page 8 ---\nTable 3: Area Under the Performance Profile (AUP), summarising the aggregate performance on all\ntasks. LFS achieves the best AUP score for all combination of metric and model.\nMetric Model T OT-BFS B ESTFS MCTS ( C=0.5) LFS ( OURS )\nWinRate GPT-4o 4.06 5.98 7.09 8.99\no3-mini – 4.23 6.00 7.20\nEfficiencyScore GPT-4o 3.68 2.67 3.68 4.70\no3-mini – 3.24 5.61 7.20\nincrease the difficulty of Countdown, TOT-BFS-GPT4 O’s WinRate drops drastically (72.64%) in\ncomparison to BESTFS-GPT4 O(50.53%), MCTS-GPT4 O(40.0%), and LFS-GPT4 O(36.84%).\nIn Countdown (Diff=5) all backtracking methods are able to achieve a WinRate near or greater\nthan 50%, with LFS-GPT4 Omarginally outperforming MCTS-GPT4 Oby 3.16%. LFS-GPT4 O’s\nimprovement over the other methods increases even further in Countdown (Diff=7), beating the next\nbest method, MCTS-GPT4 O, by a marked 14.74%, highlighting LFS’s ability to scale better\nas the task difficulty increases. Note that all methods achieve a higher WinRate when using o3-mini\nin both Countdown (Diff=5) and Countdown (Diff=7), with LFS- O3MINI again outperforming\nMCTS- O3MINI , especially in Countdown (Diff=7) by a significant 37.9%, indicating that LFS\nscales better with harder problems. Interestingly, we can see that LFS’s performance gain when\nusing o3-mini is 39.17% (average % increase in WinRate over Countdown ( Diff∈ {5,7}), which is\nlarger than the next best method, MCTS, which has a performance gain of 20.79%. This shows that\nour method also scales better with stronger models.\nSudoku. In Table 2 can see that in the simpler Sudoku (4x4), TOT-BFS-GPT4 Oagain lags behind\nMCTS-GPT4 OandLFS-GPT4 O, however, outperforms BESTFS-GPT4 O. This highlights one\nof the major drawbacks of BestFS, which is that it does not balance exploitation and exploration\nsufficiently, and in deeper and wider problems, where this becomes more important, BestFS falls\nbehind. In Sudoku (6x6), all methods struggle to solve even a single game when using GPT-4o, with\nLFS-GPT4 Obeing the only method to achieve a WinRate greater that 0%, hinting as LFS’s\nability to scale with difficult tasks. We can see that in Sudoku (4x4) BESTFS- O3MINI improves\nits WinRate (which makes sense since it is biased to over exploit, and is now guided by a stronger\nmodel), while LFS- O3MINI remains the same (likely due to it having been already close to saturation).\nNotably, MCTS- O3MINI ’s WinRate drops by 9.47%. This highlights a key limitation of MCTS : its\nperformance is sensitive to the exploration constant C, which often requires retuning across tasks,\ndifficulty levels, or base models, which is an expensive and impractical process. Lastly, we can see\nthatLFS- O3MINI ’s WinRate increases markedly in Sudoku (6x6), by 23.04%, beating the next\nbest model, MCTS, by 21.05%, further highlighting LFS’s ability to scale better with stronger\nmodels.\n6.2 Key Takeaways\nScalability and Improved Performance. We highlight in the above that a key benefit of LFS\nis that it scales better as the difficulty of the problems increase, in contrast with BESTFSwhich\ndoes not balance exploitation and exploration adequately and MCTS which requires tuning for each\ntask/model. Furthermore, LFS achieves a better WinRate, which again we highlight in the above\ndiscussion and can also be seen in Table 3 which shows that LFS achieves the highest AUP values\nfor WinRate, meaning that LFS has a higher performance on aggregate over all the tasks for\nboth models .\nScaling with Stronger Models. In the above analysis, we note that for Countdown ( diff∈ {5,7}),\nBESTFS,MCTS , and LFS see an improvement in their performance when using a stronger model.\nLFS, however, has a notably much larger performance increase when playing the most difficult\nversion of Countdown. In fact, it performs even better in Countdown (diff=7) than Countdown (diff=5).\nInterestingly, we note that when using o3-mini, MCTS actually sees a decrease in performance in\nSudoku (4x4) (we hypothesise that this is due to o3-mini overestimating state values, which leads to\npoorer exploration), compared to BESTFS’ increase and LFS’ stability. In Sudoku (6x6), LFS again\n8\n--- Page 9 ---\n104\n105\n106\nT oken Usage (log scale)0255075100125150175200Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Countdown\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS(a)\n104\n105\n106\nT oken Usage (log scale)020406080100120140Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Countdown\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS (b)\nFigure 2: Cumulative Wins with increasing Token Usage, for all Countdown games, for (a) GPT-4o\nand (b) o3-mini. LFS scales better than the other method, with the gain increasing furtehr with a\nstronger model.\nhas a notably larger performance increase compared to MCTS . All together, these results show that\nLFS scales better with a stronger model, compared to the other methods .\nScaling with Increased Compute and Computational Efficiency. In Figure 2, we can clearly\nsee that as the token usage increases, the total number of Countdown games won increases, with LFS\ndistinctly outperforming the next best method, MCTS . This is particularly notable in Figure 2b, since\nLFS with o3mini scales better with a stronger model, and thus the gap between our method and the\nothers, increases. Note that due to compute limitations, we could not test each method for larger\ntoken limits, but we can see that the gap between our method and the others is likely to continue to\ngrow, if the current trend continues. We can see a similar trend for the Sudoku games won in Figure\n22 in the Appendix, however less prominent due to the WinRate saturation for the simpler Sudoku\nversion and the poorer performance for the harder Sudoku. Lastly, not only does our method scale\nbetter with compute, it is more computationally efficient. We can see this in Table 3, where LFS\nachieves the highest AUP score for EfficiencyScore, which as discussed in Section 5.3, represents the\nmodels’ computational efficiency.\n7 Conclusion\nIn this paper, we introduced LLM-First Search (LFS) , a novel approach to reasoning and planning\nthat places the language model itself at the core of the search process. Unlike traditional search\nmethods such as MCTS, BestFS, or BFS, which rely on external heuristics, fixed traversal strategies,\nor carefully tuned hyperparameters, LFS empowers the LLM to autonomously determine whether to\ncontinue down a path or explore elsewhere in the tree, using only its internal reasoning and planning\ncapabilities, which we term Self-Guided Search . Through experiments on two complementary\nbenchmarks, Countdown and Sudoku, we demonstrated that LFS offers several key advantages: (1)\nstronger performance on harder instances without task-specific tuning, (2) improved computational\nefficiency, particularly with more capable models, (3) better scalability with model strength, and (4)\ngreater responsiveness to increased compute budget. These findings validate LFS as a flexible, LLM-\ncentric framework that not only outperforms classic search methods but also adapts more naturally to\nvarying task complexity and compute budgets. By unifying decision-making and evaluation within\nthe LLM itself, LFS reimagines the role of search in LLM reasoning, not as a separate, manually\ncontrolled process, but as an integrated, language-driven mechanism. This shift enables a more\ngeneral, adaptable, and efficient form of reasoning, offering a promising direction for scalable LLM-\nbased problem solving. While our evaluation was limited to a subset of tasks and models due to\ncompute constraints, it serves as a starting point for future work to extend LLM-First Search to more\ncomplex and realistic settings, where its benefits in adaptive exploration and self-guided reasoning\nare likely to be even more pronounced.\n9\n--- Page 10 ---\n8 Acknowledgments\nThis work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC)\nunder grant number EP/S021566/1 . We also gratefully acknowledge botBrains.io for providing\ncompute credits that enabled additional experiments to further strengthen our work.\nReferences\n[1] Daniel Kahneman. Thinking, fast and slow penguin books, 2011.\n[2]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems , 35:24824–24837, 2022.\n[3]Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for\nlanguage model agents. arXiv preprint arXiv:2407.01476 , 2024.\n[4]Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li,\nJanardhan Kulkarni, and Huseyin A Inan. On the emergence of thinking in llms i: Searching for\nthe right intuition. arXiv preprint arXiv:2502.06773 , 2025.\n[5] Bruce T Lowerre. The harpy speech recognition system. Carnegie Mellon University, 1976.\n[6]Donald E Knuth. The Art of Computer Programming: Sorting and Searching, volume 3 .\nAddison-Wesley Professional, 1998.\n[7]Edward F Moore. The shortest path through a maze. In Proc. of the International Symposium\non the Theory of Switching , pages 285–292. Harvard University Press, 1959.\n[8]Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination\nof minimum cost paths. IEEE transactions on Systems Science and Cybernetics , 4(2):100–107,\n1968.\n[9]Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In\nInternational conference on computers and games , pages 72–83. Springer, 2006.\n[10] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European\nconference on machine learning , pages 282–293. Springer, 2006.\n[11] Alexandru-Iosif Toma, Hao-Ya Hsueh, Hussein Ali Jaafar, Riku Murai, Paul HJ Kelly, and\nSajad Saeedi. Pathbench: A benchmarking platform for classical and learned path planning\nalgorithms. In 2021 18th Conference on Robots and Vision (CRV) , pages 79–86. IEEE, 2021.\n[12] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang,\nGraham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena:\nEvaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649 ,\n2024.\n[13] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nTianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for\nbuilding autonomous agents. arXiv preprint arXiv:2307.13854 , 2023.\n[14] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhit-\ning Hu. Reasoning with language model is planning with world model. arXiv preprint\narXiv:2305.14992 , 2023.\n[15] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang.\nLanguage agent tree search unifies reasoning acting and planning in language models. arXiv\npreprint arXiv:2310.04406 , 2023.\n[16] Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao,\nYihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, et al. Rex: Rapid exploration and\nexploitation for ai agents. arXiv preprint arXiv:2307.08962 , 2023.\n10\n--- Page 11 ---\n[17] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou\nYu. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. arXiv\npreprint arXiv:2410.02052 , 2024.\n[18] Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, and\nWeinan Zhang. Rethinkmcts: Refining erroneous thoughts in monte carlo tree search for code\ngeneration. arXiv preprint arXiv:2409.09584 , 2024.\n[19] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu,\nand Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. arXiv preprint\narXiv:2410.01707 , 2024.\n[20] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual\nreasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195 ,\n2024.\n[21] Xiaoshui Huang Di Zhang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4\nlevel mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b: A\ntechnical report. arXiv preprint arXiv:2406.07394 , 8, 2024.\n[22] Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek\nThomas, Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, et al. Hyperparameter opti-\nmization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisciplinary\nReviews: Data Mining and Knowledge Discovery , 13(2):e1484, 2023.\n[23] Tuan Dam, Carlo D’Eramo, Jan Peters, and Joni Pajarinen. A unified perspective on value\nbackup and exploration in monte-carlo tree search. Journal of Artificial Intelligence Research ,\n81:511–577, 2024.\n[24] Chiara F Sironi and Mark HM Winands. Analysis of the impact of randomization of search-\ncontrol parameters in monte-carlo tree search. Journal of Artificial Intelligence Research ,\n72:717–757, 2021.\n[25] Ben Ruijl, Jos Vermaseren, Aske Plaat, and Jaap van den Herik. Combining simulated annealing\nand monte carlo tree search for expression simplification. arXiv preprint arXiv:1312.0841 ,\n2013.\n[26] Xiaoxue Wang, Yujie Qian, Hanyu Gao, Connor W Coley, Yiming Mo, Regina Barzilay, and\nKlavs F Jensen. Towards efficient discovery of green synthetic pathways with monte carlo tree\nsearch and reinforcement learning. Chemical science , 11(40):10959–10972, 2020.\n[27] Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, and Min Zhang. Test-time\ncomputing: from system-1 thinking to system-2 thinking. arXiv preprint arXiv:2501.02497 ,\n2025.\n[28] Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, and Tingwen Liu. S1-bench: A\nsimple benchmark for evaluating system 1 thinking capability of large reasoning models. arXiv\npreprint arXiv:2504.10368 , 2025.\n[29] Jenny Zhang, Joel Lehman, Kenneth Stanley, and Jeff Clune. Omni: Open-endedness via\nmodels of human notions of interestingness. arXiv preprint arXiv:2306.01711 , 2023.\n[30] Richard Bellman. Dynamic Programming . Princeton University Press, 1957.\n[31] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291 , 2023.\n[32] Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong\nYe, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. arXiv preprint\narXiv:2403.16971 , 2024.\n[33] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,\nRuiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv\npreprint arXiv:2402.02716 , 2024.\n11\n--- Page 12 ---\n[34] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting\nin large language models. arXiv preprint arXiv:2210.03493 , 2022.\n[35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems ,\n35:22199–22213, 2022.\n[36] Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200 , 2024.\n[37] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by\nwriting less. arXiv preprint arXiv:2502.18600 , 2025.\n[38] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\nLuke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple\ntest-time scaling. arXiv preprint arXiv:2501.19393 , 2025.\n[39] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and Eric Schulz.\nMeta-in-context learning in large language models. Advances in Neural Information Processing\nSystems , 36:65189–65201, 2023.\n[40] Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm\nof thoughts: Enhancing exploration of ideas in large language models. arXiv preprint\narXiv:2308.10379 , 2023.\n[41] Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le, and Minmin Chen. Evolve:\nEvaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238 , 2024.\n[42] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171 , 2022.\n[43] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In International Conference\non Learning Representations (ICLR) , 2023.\n[44] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems , 36:46534–46594,\n2023.\n[45] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion: Language agents with verbal reinforcement learning. Advances in Neural Information\nProcessing Systems , 36:8634–8652, 2023.\n[46] Giovanni Monea, Antoine Bosselut, Kianté Brantley, and Yoav Artzi. Llms are in-context\nreinforcement learners. 2024.\n[47] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\ning factuality and reasoning in language models through multiagent debate. In Forty-first\nInternational Conference on Machine Learning , 2023.\n[48] Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, and Heuiseok Lim. Debate\nonly when necessary: Adaptive multiagent collaboration for efficient llm reasoning. arXiv\npreprint arXiv:2504.05047 , 2025.\n[49] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and\nMichael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information\nProcessing Systems , 36:41618–41650, 2023.\n[50] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\nvances in neural information processing systems , 36:11809–11822, 2023.\n12\n--- Page 13 ---\n[51] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas\nGianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph\nof thoughts: Solving elaborate problems with large language models. In Proceedings of the\nAAAI Conference on Artificial Intelligence , volume 38, pages 17682–17690, 2024.\n[52] Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-thought: Scaling\ntest-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078 , 2024.\n[53] Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, and Takuya Akiba.\nWider or deeper? scaling llm inference-time compute with adaptive branching tree search. arXiv\npreprint arXiv:2503.04412 , 2025.\n[54] OpenAI. Openai api. https://platform.openai.com/ , 2024. Accessed: 2025-05-16.\n[55] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature , 529(7587):484–489,\n2016.\n[56] Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng\nKong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv\npreprint arXiv:2410.14157 , 2024.\n[57] Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-Bench.\nhttps://github.com/SakanaAI/Sudoku-Bench , 2025.\n[58] Wikipedia contributors. Countdown (game show). https://en.wikipedia.org/wiki/\nCountdown_(game_show) , 2024. Accessed: 2024-03-29.\n[59] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and\nNoah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint\narXiv:2404.03683 , 2024.\n[60] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\nACM conference on fairness, accountability, and transparency , pages 610–623, 2021.\n[61] Edwin B Wilson. Probable inference, the law of succession, and statistical inference. Journal\nof the American Statistical Association , 22(158):209–212, 1927.\n[62] Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vin-\ncent Moens, Amar Budhiraja, Despoina Magka, Vladislav V orotilov, Gaurav Chaurasia, et al.\nMlgym: A new framework and benchmark for advancing ai research agents. arXiv preprint\narXiv:2502.14499 , 2025.\n[63] Elizabeth D Dolan and Jorge J Moré. Benchmarking optimization software with performance\nprofiles. Mathematical programming , 91:201–213, 2002.\n[64] Nicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang\nCai, Shuaicheng Niu, Jianyu Heng, Hongyang Qin, et al. Automl decathlon: Diverse tasks,\nmodern methods, and efficiency at scale. In NeurIPS 2022 Competition Track , pages 151–170.\nPMLR, 2023.\n13\n--- Page 14 ---\nAppendix Table of Contents\nSection Page\nLimitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nAdditional Details of Search Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nTask Discussion and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nImplementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nPreliminary Investigation: MCTS Exploration Constant . . . . . . . . . . . . . . . . . . . . . . . . . 30\nAdditional Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nExample Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nA Limitations and Future Work\nWe evaluate our method, LLM-First- Search (LFS), on two standard reasoning benchmarks: Count-\ndown andSudoku , commonly used in Large Language Model (LLM) research. These tasks offer (1)\nscalability , allowing fine control over difficulty, and (2) complementarity , with Countdown featuring\na shallow but challenging search space, and Sudoku a deeper but more intuitive one. Together, they\nprovide a balanced testbed for search strategies across diverse reasoning challenges. However, these\nbenchmarks lack some complexities of real-world problems. Due to compute constraints, we limited\nour experiments to these tasks and a fixed number of samples, restricting broader validation. LFS also\nassumes the ability to revert to previous states, which may not hold in all environments. Additionally,\nwhile LFS is shown to excel with stronger language models, we did not determine its sensitivity\nto weaker models. While our evaluation was limited, it serves as a starting point for future work\nto extend LLM-First Search to more complex and realistic settings, where its benefits in adaptive\nexploration and self-guided reasoning are likely to be even more pronounced.\nB Additional Details of Search Baselines\nB.1 Tree-of-Thought Breadth-First Search (ToT-BFS)\nAlgorithm 2 Tree of Thought Breadth-First Search (ToT-BFS)\n1:Input: LLM πθ, Value prompt Peval, Transition function T, Beam width k\n2:Initialise frontier F:={(s0,A0)}\n3:while Token limit not exhausted do\n4: Evaluate all frontier states: {Vi=Peval(si, πθ)}|F|\ni=1\n5: Select top- kstates by value: Ftop⊆ F with|Ftop|=k\n6: (st,At)← Ftop\u0002\narg max (s,A)∈FtopV(s)\u0003\n7: ifstis terminal then\n8: break\n9: end if\n10: Initialise new frontier Fnew:=∅\n11: foreach(si,Ai)∈ Ftopdo\n12: foreacha∈ Aido\n13: (s′,A′)∼T(· |si, a)\n14: Fnew:=Fnew∪ {(s′,A′)}\n15: end for\n16: end for\n17: F:=Fnew\n18:end while\n19:Return: (st,At)\n14\n--- Page 15 ---\nIn this section, we describe Tree-of-Thought Breadth-First Search (ToT-BFS) , a method inspired\nby the Tree-of-Thought framework. ToT-BFS performs uniform expansion from the current frontier:\nat each depth level, it evaluates all current frontier nodes and expands the top- kaccording to their\nLLM-estimated value. The method is summarised in Algorithm 2.\nFrontier Filtering. At each iteration, the search maintains a set of current frontier nodes Ft=\n{(s1\nt,A1\nt), . . . , (sn\nt,An\nt)}representing all active paths at the current depth. For each node, the LLM\nis used to score the value of the state via a prompt Peval(si\nt), returning an estimated utility V(si\nt). The\ntop-knodes with the highest estimated value are selected for expansion:\nFtop\nt=TopK(Ft,{V(si\nt)}),\nwhere each selected node is expanded by executing actions from Ai\ntusing the environment’s transition\nfunction T. If the frontier node with the highest estimated value is terminal, the expansion ends, and\nthe terminal state is returned.\nFrontier Expansion. Each selected frontier node (si\nt,Ai\nt)is expanded, resulting in new states\n(st+1,At+1)which are added to the new frontier. This process continues level by level, maintaining\na breadth-first structure that allows the model to explore multiple solution pathways in parallel.\nB.2 Best-First Search (BestFS)\nAlgorithm 3 Best-First Search (BestFS)\n1:Input: LLM πθ, Value prompt Peval, Transition function T\n2:Initialise s0,A0, Priority queue Q\n3:Evaluate current state: V0=Pvalue(s0, πθ)\n4:Q:=Q ∪ { (V0, s0,A0)}\n5:while Token limit not exhausted do\n6: (Vt, st,At)←pop(Q) ▷Greedy selection by highest Vt\n7: forat∈ Atdo\n8: (s′,A′)∼T(· |st, at)\n9: V′=Pvalue(s′, πθ)\n10: Q:=Q ∪ { (V′, s′,A′)}\n11: end for\n12:end while\n13:Return: (st,At)\nIn this section, we describe Best-First Search (BestFS) , a strategy that expands the most promising\nnodes first, based on their estimated value. Our implementation leverages an LLM to evaluate the\nvalue of states and uses these estimates to drive the search greedily toward high-reward regions of the\nsearch space. BestFS does not prompt the LLM to decide when to explore; rather, it always expands\nthe node with the highest estimated value from the priority queue. A high-level overview is provided\nin Algorithm 3.\nLLM-Based Evaluation. The LLM is prompted using a value-estimation prompt Peval(s′), to\nevaluate the state s′after taking action at∈ Atwhich returns a scalar estimate V′of the utility of s′.\nThe tuple {(V′, s′,A′)}is then added to the priority queue Q. This is done for all at∈ At.\nGreedy Expansion. At each step, the algorithm pops the highest-ranked node (st,At)from the\npriority queue Q:\n(st,At)←pop(Q),\nwhere Qis ordered by the estimated value of states as predicted by the LLM.\n15\n--- Page 16 ---\nB.3 Monte Carlo Tree Search (MCTS)\nAlgorithm 4 LLM-guided Monte Carlo Tree Search (MCTS)\n1:Input: LLM πθ, Prompts PpriorandPvalue, Transition function T\n2:Initialise root node s0\n3:while Token limit not exhausted do\n4: path←[]\n5: s←s0\n6: while sis not leaf and not terminal do\n7: a←PUCT (s) ▷Uses visit counts and priors\n8: path←path∪ {(s, a)}\n9: s←T(s, a)\n10: end while\n11: ifsis leaf then\n12: A ← actions (s)\n13: {P(a|s)} ←Pprior(s,A, πθ)\n14: V(s)←Pvalue(s, πθ)\n15: Initialise state statistics: {P(a)}A,V(s),N(s)\n16: end if\n17: ifis_solution (s)then\n18: break\n19: end if\n20: Backpropagate V(s)along path\n21:end while\n22:Return: (s,A)\nIn our adaptation of Monte Carlo Tree Search (MCTS) , we replace traditional simulation-based\nrollouts with value and policy estimates provided directly by the LLM. Specifically, at each node, the\nLLM is prompted to estimate (1) the value of the current state, and (2) the prior over the available\nactions, which are used by the PUCT selection rule to guide the search. The resulting algorithm is\noutlined in Algorithm 4.\nSearch Tree and Node Structure. MCTS maintains a search tree where each node corresponds\nto a state s, and stores the visit count N(s), total value W(s), and prior over actions {P(a|s)}(as\nreturned by the LLM). Each edge stores a running estimate of Q(s, a) =W(s, a)/N(s, a). The tree\nis expanded progressively, guided by the PUCT criterion:\na∗= arg max\na\"\nQ(s, a) +cpuct·π(a|s)·p\nN(s)\n1 +N(s, a)#\n,\nwhere cpuctis the exploration constant controlling the trade-off between exploration and exploitation.\nThis selection rule encourages the algorithm to prioritise actions with either high expected value or\nlow visitation count, as informed by the LLM’s prior.\nLLM-Based Evaluation. To avoid traditional rollout-based playouts, we leverage the LLM to\nprovide value and policy estimates directly at the leaf node. When a new leaf node is reached, we\nprompt the LLM using a state-value prompt Pvalue(s)to obtain a scalar estimate V(s)of the state’s\nexpected utility. We also query an action-prior prompt Pprior(s,A)to estimate the prior distribution\nover actions. These values are then backpropagated through the tree to update Q,W, and Nvalues\nfor all nodes along the visited path.\nC Task Discussion and Analysis\nWe analyse the branching factor and number of states at a given depth dfor our two benchmark tasks,\nCountdown and Sudoku, demonstrating their complementary characteristics. This analysis supports\nthe use of these tasks as representative testbeds, with Countdown exhibiting a shallower but more\n16\n--- Page 17 ---\ncomplex decision space and Sudoku presenting a deeper, broader search space, together providing a\nbalanced evaluation of search strategies.\nCountdown. Starting with an initial list of nnumbers, at each step the agent selects two distinct\nnumbers and applies one of four arithmetic operations ( +,−,×,÷). The number of distinct pairs is\u0000n\n2\u0001\n=n(n−1)\n2, and each pair can be combined with 4 possible operations. Thus, the branching factor\nat the root (depth d= 0) is:\nB0= 4×\u0012n\n2\u0013\n= 2n(n−1).\nAfter applying one operation, the list size decreases by 1, leaving n−1numbers. At depth d, the list\nsize is n−d, so the branching factor at depth dis:\nBd= 4×\u0012n−d\n2\u0013\n= 2(n−d)(n−d−1).\nThe number of distinct lists (states) exactly at depth d, denoted Ld, can be recursively computed as:\nL0= 1,\nLd=Ld−1×Bd−1=d−1Y\ni=02(n−i)(n−i−1).\nSudoku. In a Sudoku puzzle of size l×l, assume nempty cells initially. At each step, the agent\nfills one empty cell with a valid number (up to lpossibilities).\nAt depth d, there are n−dempty cells left, so the branching factor is:\nBd= (n−d)×l.\nThe number of board states exactly at depth dis then:\nL0= 1,\nLd=Ld−1×Bd−1=d−1Y\ni=0(n−i)×l=ld×d−1Y\ni=0(n−i).\nAnalysis. Countdown features a relatively shallow search space with a maximum depth of n−1,\nwhere nis the initial length of numbers in the set. At each depth d, the branching factor is given by\n2(n−d)(n−d−1),\nreflecting the number of possible pairs and arithmetic operations. Although the search depth is\nlimited, Countdown is often more challenging in terms of selecting the correct action due to the\ncombinatorial nature of valid operations.\nIn contrast, Sudoku involves a much deeper search space, with maximum depth equal to the initial\nnumber of empty cells n. The branching factor at depth dis approximately\n(n−d)×l,\nwhere lis the board’s side length (e.g., 9 for a standard 9×9Sudoku). Here, the width of the search\nspace depends linearly on the number of remaining empty cells and the number of valid entries per\ncell, resulting in a wide and deep search tree.\nThis contrast in search space structure, Countdown’s shallow but combinatorially complex branching\nversus Sudoku’s deep and broadly branching tree, makes these benchmarks complementary, providing\na thorough evaluation of search strategies under diverse reasoning challenges.\nD Implementation Details\nWe utilised the OpenAI API to access both the GPT-4o and o3-mini language models. We set key\nparameters while leaving others at their default values. The temperature was fixed at 0.0 to produce\ndeterministic outputs and reduce randomness. We set max_tokens to 16,384 to allow sufficiently long\nresponses for complex, multi-step reasoning tasks. A timeout of 300 seconds was enforced to limit\nAPI call duration and prevent excessively long requests. Lastly, the o3-mini model was configured to\noperate at a \"low\" reasoning_effort .\n17\n--- Page 18 ---\nE Prompts\nThis section presents the exact prompts used in our experiments. These prompts were designed to\nguide the language model in performing evaluations, making exploration decisions, or generating\nactions during search. These prompts play a crucial role in enabling LLM-First Search and the other\nbaselines to operate under comparable conditions, ensuring that differences in performance arise from\nthe methods themselves rather than discrepancies in task formulation. Note that variables enclosed\nin curly braces (e.g., {state} ,{actions} ) indicate Python variables used for string formatting\n(this will be visible in the accompanying open-source code). Lastly, for clarity, we use colour to\ndistinguish different components of the prompts: (1) Green: Task-specific instructions or rules, (2)\nRed: System-level instructions that define the model’s role or behaviour, and (3) Blue: User-level\nqueries or task inputs.\nE.1 Countdown\nCountdown Game Rules\nYou’re playing the Countdown Numbers Game. Let me explain the rules and how to solve it:\nGame Rules:\n1. You are given a set of numbers and a target number to reach.\n2. You can only use each number once.\n3.You must combine numbers using only four operations: addition (+), subtraction (-), multipli-\ncation (*), and division (/).\n4. Division is only allowed when it results in a whole number (no fractions or decimals).\n5. You can only combine two numbers at a time to create a new number.\n6.After each operation, the original numbers are removed, and the result is added to your\navailable numbers.\n7. You win when you have exactly one number left that matches the target.\nFor example, with target 50 and numbers [39, 66, 33, 13]:\nState 0 Target: 50\nOperations: []\nAvailable Numbers: [39, 66, 33, 13]\nAction 0 Operation: ’39 + 13 = 52’\nState 1 (After performing 39 + 13 = 52)\nTarget: 50\nOperations: [’39 + 13 = 52’]\nAvailable Numbers: [66, 33, 52]\nAction 1 Operation: ’66 / 33 = 2’\nState 2 (After performing 66 / 33 = 2)\nTarget: 50\nOperations: [’39 + 13 = 52’, ’66 / 33 = 2’]\nAvailable Numbers: [52, 2]\nAction 2 Operation: ’52 - 2 = 50’\nState 3 (After performing 52 - 2 = 50)\nTarget: 50\nOperations: [’39 + 13 = 52’, ’66 / 33 = 2’, ’52 - 2 = 50’]\nAvailable Numbers: [50]\nGame won!\n18\n--- Page 19 ---\nAction Prior System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when assigning probabilities to operations:\n1.Target Progress: How much closer the operation gets to the target\n•Operations resulting in numbers exactly at or very close to target should receive\nhigher scores\n• Operations creating useful intermediate numbers should be favored\n2.Number Creation: The utility of the resulting number\n• Creating small, flexible numbers (1-10) can be valuable\n• Creating numbers that are factors of the target\n• Creating numbers that offer efficient pathways to the target\n3.Available Number Management: How the operation affects the number pool\n• Operations that use less useful numbers while preserving useful ones\n• Operations that create a more workable set of available numbers\n• Avoiding operations that result in unusable large numbers\n4.Mathematical Strategy: Using operations optimally\n• Using division to create useful small numbers\n• Using multiplication for larger adjustments toward the target\n• Using addition/subtraction for precise movements toward the target\nYour task is to evaluate the possible actions in the current state, scoring them based on how\nlikely they are to help you achieve the target value. The scores should form a probability\ndistribution over the actions.\nExample State Sequence State 0 Target: 50\nOperations: []\nAvailable Numbers: [39, 66, 33, 13]\nAction 0 Operation: ’39 + 13 = 52’\nState 1 (After performing 39 + 13 = 52)\nTarget: 50\nOperations: [’39 + 13 = 52’]\nAvailable Numbers: [66, 33, 52]\nAction 1 Operation: ’66 / 33 = 2’\nState 2 (After performing 66 / 33 = 2)\nTarget: 50\nOperations: [’39 + 13 = 52’, ’66 / 33 = 2’]\nAvailable Numbers: [52, 2]\nExample Possible Operations: {0: ’52 + 2 = 54’, 1: ’52 - 2 = 50’, 2: ’52 * 2 = 104’, 3: ’52 / 2\n= 26’}\nExample Final Answer\n{\"operation_scores\" :{”0” : 0 .15,”1” : 0 .35,”2” : 0 .35,”3” : 0 .15}}\n19\n--- Page 20 ---\nUser Request\nCurrent State and Action sequence {current _sequence }\nPossible Operations: {action _list}\nWhat are the scores for each action/operation? Assign a probability to each possible operation\nbased on how likely it is to lead to the target number.\nYour response must include a valid JSON object, enclosed in a boxed , with an\noperation_scores field containing a dictionary mapping operation keys to scores, formatted\nas follows:\n{\"operation_scores\" :< dictionary _of_scores > }\nReplace <dictionary_of_scores> with a dictionary mapping operation keys to scores that\nmust sum to 1.0.\n20\n--- Page 21 ---\nEstimate Node Value System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant factors to consider when estimating state value:\n1.Proximity to Target: How close the current numbers are to the target\n• States with numbers exactly equal to or close to the target are more valuable\n•States with numbers that can be easily combined to reach the target have higher\nvalue\n2.Available Number Quality: How useful the remaining numbers are\n• Having small numbers (1-10) increases flexibility\n• Having numbers that are factors or multiples of target numbers is valuable\n• Having complementary numbers that work well together\n3.State Progress: How much progress has been made\n• Number of operations performed so far\n• Reduction in the total number of available numbers\n• Quality of the operations performed so far\n4.Potential for Success: Overall likelihood of reaching the target\n• Presence of clear pathways to the target\n• Absence of unusable or problematic numbers\n• Balance between large and small numbers\nYour task is to estimate the value of the current state and possible operations by determining\nthe likelihood of reaching the target number from it. The score should range from 0 to 1.\nFor example:\nExample State Sequence\nState 0 Target: 50\nOperations: []\nAvailable Numbers: [39, 66, 33, 13]\nAction 0 Operation: ’39 + 13 = 52’\nState 1 (After performing 39 + 13 = 52)\nTarget: 50\nOperations: [’39 + 13 = 52’]\nAvailable Numbers: [66, 33, 52]\nAction 1 Operation: ’66 / 33 = 2’\nState 2 (After performing 66 / 33 = 2)\nTarget: 50\nOperations: [’39 + 13 = 52’, ’66 / 33 = 2’]\nAvailable Numbers: [52, 2]\nExample Possible Operations: [’52 + 2 = 54’, ’52 - 2 = 50’, ’52 * 2 = 104’, ’52 / 2 = 26’]\nExample Final Answer\n{\"state_value_estimation\" : 1.0}\n21\n--- Page 22 ---\nUser Request\nCurrent State and Action sequence {current _sequence }\nPossible Operations: {action _list}\nGiven the current state and the possible operations, estimate the value of the current state,\nranging from 0-1, where 1 means it’s certain to reach the target number and 0 means it’s\nimpossible.\nYour response must include a valid JSON object, enclosed in a boxed , with a\nstate_value_estimation field, formatted as follows:\n{\"state_value_estimation\" :< value > }\nReplace <value> with your estimated probability (between 0 and 1) of reaching the target\nfrom this state.\n22\n--- Page 23 ---\nMove Values Estimation System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when evaluating possible operations:\n1.Target Progress: How much each operation moves toward the target\n• Operations that result in numbers close to the target\n• Operations that create useful intermediate numbers for future steps\n2.Number Creation: The strategic value of the resulting number\n• Creating small, useful numbers (1-10) for fine adjustments\n• Creating numbers that are easily combinable with others\n• Creating numbers that are factors or related to the target\n3.Operation Strategy: How the operation affects solution paths\n• Using division to create useful small numbers\n• Using multiplication to make larger jumps toward the target\n• Using addition/subtraction for precise adjustments\n4.Future Potential: How an operation affects future possibilities\n• Operations that open up multiple future paths\n• Operations that eliminate problematic numbers\n• Operations that maintain flexibility in the number set\nYour task is to evaluate each possible operation and assign a value between 0 and 1 to each,\nwhere 1 means the operation is extremely likely to lead to solving the puzzle and 0 means it’s\nvery unlikely to be helpful.\nFor example:\nExample State Sequence\nState 0 Target: 50\nOperations: []\nAvailable Numbers: [39, 66, 33, 13]\nAction 0 Operation: ’39 + 13 = 52’\nState 1 (After performing 39 + 13 = 52)\nTarget: 50\nOperations: [’39 + 13 = 52’]\nAvailable Numbers: [66, 33, 52]\nExample Possible Operations: {0: ’52 + 66 = 118’, 1: ’52 - 33 = 19’, 2: ’66 - 33 = 33’, 3: ’66 /\n33 = 2’}\nExample Final Answer\n{\"operation_values\" :{”0” : 0 .3,”1” : 0 .6,”2” : 0 .5,”3” : 0 .9}}\n23\n--- Page 24 ---\nUser Request\nCurrent State and Action sequence {current _sequence }\nPossible Operations: {action _list}\nEvaluate each possible operation and assign a value between 0 and 1 to each, where 1 means\nthe operation is extremely likely to lead to solving the puzzle and 0 means it’s very unlikely to\nbe helpful.\nYour response must include a valid JSON object, enclosed in a boxed , with an\noperation_values field containing a dictionary mapping operation keys to values between\n0 and 1, formatted as follows:\n{\"operation_values\" :< dictionary _of_values > }\nReplace <dictionary_of_values> with a dictionary mapping operation keys to values\nbetween 0 and 1.\n24\n--- Page 25 ---\nExploration Decision System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when deciding whether to explore or continue:\n1.Current Path Quality: How promising the current path appears\n• Presence of numbers close to the target\n• Quality and usefulness of available numbers\n• Clear pathways to reach the target from current numbers\n2.Current Path Issues: Signs the current path may be problematic\n• Numbers far from the target with no clear way to combine them\n• Repeated patterns or circular operations\n• No beneficial operations remaining\n3.Exploration Value: Potential benefit of trying other paths\n• Number of operations already performed on current path\n• Quality of alternative unexplored paths\n• Diminishing returns on current path\n4.Decision Confidence: Certainty about current path viability\n• Clear evidence current path cannot reach target\n• Presence of obviously better unexplored paths\n• Risk assessment of continuing vs exploring\nYour task is to decide whether to continue with the current state or to visit an unexplored state.\nBefore deciding, carefully consider the current sequence of states and actions, as well as the\navailable operations. Only choose to explore if you are certain that the current path cannot\nreach the target number and that switching to a new path is the best use of time.\nFor example:\nExample State and Action sequence\nState 0 Target: 50\nOperations: []\nAvailable Numbers: [39, 66, 33, 13]\nAction 0 Operation: ’39 + 13 = 52’\nState 1 (After performing 39 + 13 = 52)\nTarget: 50\nOperations: [’39 + 13 = 52’]\nAvailable Numbers: [66, 33, 52]\nAction 1 Operation: ’66 / 33 = 2’\nState 2 (After performing 66 / 33 = 2)\nTarget: 50\nOperations: [’39 + 13 = 52’, ’66 / 33 = 2’]\nAvailable Numbers: [52, 2]\nExample Possible Operations: {0: ’52 + 2 = 54’, 1: ’52 - 2 = 50’, 2: ’52 * 2 = 104’, 3: ’52 / 2\n= 26’}\nExample Final Answer\n{\"explore\" :false}\n25\n--- Page 26 ---\nUser Request\nCurrent State and Action sequence {current _sequence }\nPossible Operations: {action _list}\nConsider the current sequence of states and actions and the available operations. Reason\nthrough your options step by step and determine whether continuing with the current state or\nexploring a new state is the most optimal decision.\nYour response must include a valid JSON object, enclosed in a boxed , with an explore field,\nwhere the value must be either true (to explore a new state) or false (to continue with the\ncurrent state), formatted as follows:\n{\"explore\" :< boolean > }\nReplace <boolean> with either true or false.\nE.2 Sudoku\nSudoku Game Rules\nYou are helping solve Sudoku puzzles using a tree-based search approach. Sudoku is a puzzle where\nyou fill a grid with numbers 1 through {grid _size}so that each row, column, and box has no repeated\nnumbers.\nFor this {grid _size} × { grid _size}Sudoku grid, the boxes are {box_width} × { box_height }in\nsize. Each row, column, and box must contain all numbers from 1 to {grid _size}without repetition.\nThis means:\n1. Each row must contain each number from 1 to {grid _size}exactly once\n2. Each column must contain each number from 1 to {grid _size}exactly once\n3.Each{box_width} × {box_height }box must contain each number from 1 to {grid _size}\nexactly once\nThese constraints create a logical puzzle where placing a number in a cell immediately restricts what\nnumbers can be placed in other cells in the same row, column, and box.\nBoard Structure:\n•The Sudoku board is a {grid _size} × { grid _size}grid divided into {box_width} ×\n{box_height }boxes\n• Rows are numbered 0 to {grid _size_minus _one}from top to bottom\n• Columns are numbered 0 to {grid _size_minus _one}from left to right\n• Each cell is identified by its (row, column) coordinates\n• Empty cells appear as periods (.) in the board representation\n•Board state is represented as a nested list where board[row][column] gives the value at\nthat position\nWhen solving a Sudoku puzzle, we explore different possible number placements. Each step involves\nselecting an empty cell and placing a valid number in it. As we make selections, the set of valid moves\nfor remaining cells may change.\n26\n--- Page 27 ---\nAction Prior System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when evaluating possible actions:\n1. How actions might create naked singles or hidden singles in other cells\n2. Actions targeting cells with few remaining alternatives\n3. How actions may constrain multiple other cells simultaneously\n4. How actions contribute to a balanced distribution of numbers across the board\n5. Whether actions might lead to contradictions or cells with no legal moves\nYour task is to evaluate the possible actions in the current state, scoring them based on\nhow likely they are to help solve the Sudoku puzzle. The scores should form a probability\ndistribution over the actions (sum to 1.0) and be returned as a dictionary mapping action indices\nto scores.\nExample {grid _size} × {grid _size}Sudoku Board\n{example _board}\nExample Possible Actions\n{example _prior _actions }\nExample Final Answer\n{\"operation_scores\" :{example _operation _scores }}\nUser Request\nCurrent {grid _size} × {grid _size}Sudoku Board\n{current _board}\nPossible Actions\n{action _list}\nEvaluate each action based on how it creates constraints, identifies singles, minimizes branch-\ning, and maintains a balanced distribution of numbers as described in your instructions.\nAssign a probability to each possible action based on how likely it is to lead to a solution of the\nSudoku puzzle. The scores should sum to 1.0, representing a probability distribution over the\nactions.\nYour response must include a valid JSON object, enclosed in a boxed , with an\noperation_scores field containing a dictionary mapping action indices to scores, formatted\nas follows:\n{\"operation_scores\" :<dictionary_of_scores >}\nReplace <dictionary_of_scores> with a dictionary mapping action indices to scores that\nMUST sum to 1.0.\n27\n--- Page 28 ---\nNode Value System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when estimating the value of a board state:\n1. Factors that may indicate higher likelihood of success:\n• The number of cells with few possible remaining values\n• Whether all cells have at least one possible legal value\n• How close rows, columns, and boxes are to completion\n• The presence of obvious next moves such as naked or hidden singles\n2. Factors that may indicate lower likelihood of success:\n• The presence of cells with zero possible legal values (contradictions)\n• Many cells having numerous possible values (high uncertainty)\n• Limited constraints between remaining empty cells\n• Patterns that typically lead to unsolvable states\nYour task is to estimate the value of the current board state by determining the likelihood of\nsolving the puzzle from this position. The score should range from 0 to 1.\nExample {grid _size} × {grid _size}Sudoku Board\n{example _board}\nExample Possible Actions\n{example _value _actions }\nExample Final Answer\n{\"state_value_estimation\" : 0.75}\nUser Request\nCurrent {grid _size} × {grid _size}Sudoku Board\n{current _board}\nPossible Actions\n{action _list}\nGiven the current board state and the possible actions, estimate the value of the current\nstate. Consider factors like the number of cells with few possible values, whether there are\ncontradictions, and whether there are obvious next moves as described in your instructions.\nProvide a value ranging from 0–1, where 1 means it’s certain to reach a solution and 0 means\nit’s impossible.\nYour response must include a valid JSON object, enclosed in a boxed , with a\nstate_value_estimation field, formatted as follows:\n{\"state_value_estimation\" :<value >}\nReplace <value> with your estimated probability (between 0 and 1) of solving the puzzle\nfrom this state.\n28\n--- Page 29 ---\nExplore Decision System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when determining whether to continue with the current board\nstate or explore a new state:\n1. The presence of naked singles or hidden singles in the current board state\n2.Whether the current board state contains contradictions or cells with no valid moves\n3. The level of certainty in the remaining cells (many vs. few possible values)\n4. Whether the board shows signs of making progress or appears to be in a deadlock\nYour task is to decide whether to continue with the current board state or to visit an unexplored\nboard state. Before deciding, carefully consider the current board and the available actions.\nOnly choose to explore if you are certain that the current board state cannot lead to a solution\nand that switching to a new board state is the best use of time.\nExample {grid _size} × {grid _size}Sudoku Board {example _board}\nExample Possible Moves {example _explore _actions }\nExample Final Answer\n{\"explore\" :false}\nUser Request\nCurrent {grid _size} × {grid _size}Sudoku Board {current _board}\nPossible Moves {empty _cells}\nConsider the current board state and the available actions. Evaluate whether the current state\nhas promising moves like naked singles or hidden singles, or if it shows signs of contradictions\nor deadlocks as described in your instructions.\nReason through your options step by step and determine whether continuing with the current\nstate or exploring a new state is the most optimal decision.\nRespond with true if you should explore a new board state, or false if you should continue with\nthe current one.\nYour response must include a valid JSON object, enclosed in a boxed , with an explore field,\nwhere the value must be either true (to explore a new board state) or false (to continue with the\ncurrent board state), formatted as follows:\n{\"explore\" :<boolean >}\nReplace <boolean> with either true or false.\n29\n--- Page 30 ---\nMove Value Estimation System Instruction and User Request\nSystem Instruction\n− − − − − − − − − − − − − Insert game rules here − − − − − − − − − − − −−\nImportant considerations when evaluating possible moves:\n1.Constraint Propagation: How each move affects future possibilities\n• Whether the move creates naked singles or hidden singles\n• How the move constrains other cells in the same row, column, and box\n2.Strategic Value: The quality of the move in solving the puzzle\n• Whether the move targets cells with few remaining possibilities\n• Whether the move maintains flexibility in other cells\n• Whether the move creates a balanced distribution of numbers\n3.Future Impact: How the move affects future solving paths\n• Whether the move opens up multiple solving techniques\n• Whether the move might lead to contradictions\n• Whether the move maintains good solving options\nYour task is to evaluate each possible move and assign a value between 0 and 1 to each, where 1\nmeans the move is extremely likely to lead to solving the puzzle and 0 means it’s very unlikely\nto be helpful.\nExample {grid _size} × {grid _size}Sudoku Board {example _board}\nExample Possible Moves {example _moves }\nExample Final Answer\n{\"move_values\" :{”0” : 0 .8,”1” : 0 .5,”2” : 0 .3, . . .}}\nUser Request\nCurrent {grid _size} × {grid _size}Sudoku Board {current _board}\nPossible Moves {moves _list}\nEvaluate each possible move and assign a value between 0 and 1 to each, where 1 means the\nmove is extremely likely to lead to solving the puzzle and 0 means it’s very unlikely to be\nhelpful.\nYour response must include a valid JSON object, enclosed in a boxed , with a move_values\nfield containing a dictionary mapping move indices to values between 0 and 1, formatted as\nfollows:\n{\"move_values\" :< dictionary _of_values > }\nReplace <dictionary_of_values> with a dictionary mapping move indices to values be-\ntween 0 and 1.\nF Preliminary Investigation: MCTS Exploration Constant\nWe performed a hyperparameter sweep over different exploration constants C∈ {0.5,1.0,2.5}. Due\nto computational constraints, we limited this sweep to the three Countdown variants and the simpler\nSudoku variant, using GPT-4o as the underlying model. As shown in Figures 5, 6, and 7, the setting\nc= 2.5consistently underperforms, while c= 0.5andc= 1.0perform similarly, with c= 0.5\nslightly outperforming c= 1.0in Countdown (difficulty 5). The largest performance gap appears\nin the Sudoku (4x4) task (Figure 8), where c= 0.5significantly outperforms higher values. This is\nlikely due to Sudoku’s deeper solution space, where higher c-values lead to over-exploration. The\noverall trend is further confirmed by the performance profiles in Figures 3 and 4, which show c= 0.5\n30\n--- Page 31 ---\nachieving the best trade-off between performance and efficiency. Based on these results, we adopt\nc= 0.5as the default value in subsequent experiments.\n100101\nPerformance Ratio ( )\n0.20.30.40.50.60.70.8Fraction of T asks\nWin Rate Profile\nModel: gpt-4o\nMCTS(c=0.5) (AUP: 7.200)\nMCTS(c=1.0) (AUP: 5.387)\nMCTS(c=2.5) (AUP: 5.305)\nFigure 3: Performance profiles of MCTS across different exploration constants ( c∈ {0.5,1.0,2.5}),\nevaluated using WinRate across all tasks with GPT-4o. The profiles illustrate the proportion of tasks\nwhere each cvalue is within a given performance ratio of the best. Area Under the Profile (AUP)\nis displayed for each curve. Notably, c= 0.5achieves the highest AUP, indicating superior overall\nperformance.\n100101\nPerformance Ratio ( )\n0.00.10.20.30.40.50.60.70.8Fraction of T asks\nWin Rate / T oken Usage Profile\nModel: gpt-4o\nMCTS(c=0.5) (AUP: 7.157)\nMCTS(c=1.0) (AUP: 5.377)\nMCTS(c=2.5) (AUP: 5.246)\nFigure 4: Performance profiles of MCTS across different exploration constants ( c∈ {0.5,1.0,2.5}),\nevaluated using WinRate per token ratio (efficiency) across all tasks with GPT-4o. The profiles\nindicate the proportion of tasks where each cvalue achieves a given efficiency ratio relative to the\nbest. Area Under the Profile (AUP) is shown for each curve. As with overall WinRate, c= 0.5yields\nthe highest AUP, demonstrating superior efficiency.\n31\n--- Page 32 ---\nMCTS(c=0.5) MCTS(c=1.0) MCTS(c=2.5)\nMethods0255075100125150175200225Win Rate (%)*100.0%*\n[100.0,\n100.0]*100.0%*\n[100.0,\n100.0]*100.0%*\n[100.0,\n100.0]Win Rate\nT oken Usage\n020004000600080001000012000\nT oken Usage*9289*\n[5091,\n13487]\n*7683*\n[4349,\n11017]*7717*\n[4542,\n10891]Win Rate and T oken Usage for Countdown (diff=3)\nModel: gpt-4oFigure 5: WinRate and token usage for MCTS on the Countdown task (difficulty 3) using GPT-\n4o. Both metrics are reported across different exploration constants ( c= 0.5,1.0,2.5), with all\nconfigurations successfully solving all instances. Notably, c= 0.5uses the most tokens. Values in “*”\ndenote the mean, and square brackets “[ ]” represent the 95% confidence interval.\nMCTS(c=0.5) MCTS(c=1.0) MCTS(c=2.5)\nMethods020406080100120140Win Rate (%)*66.7%*\n[30.4,\n100.0]*62.2%*\n[25.0,\n99.5]*60.0%*\n[22.3,\n97.7]Win Rate\nT oken Usage\n0100000200000300000400000500000600000700000800000\nT oken Usage*509110*\n[214354,\n803866]*528327*\n[244164,\n812491]*567471*\n[274662,\n860279]Win Rate and T oken Usage for Countdown (diff=5)\nModel: gpt-4o\nFigure 6: WinRate and token usage for MCTS on the Countdown task (difficulty 5) using GPT-4o.\nResults are shown for exploration constants c= 0.5,1.0, and 2.5. See that c= 0.5achieves the best\nWinRate while also using the fewest tokens on average. Values in “*”denote the mean, and square\nbrackets“[ ]” represent the 95% confidence interval.\n32\n--- Page 33 ---\nMCTS(c=0.5) MCTS(c=1.0) MCTS(c=2.5)\nMethods010203040506070Win Rate (%)*33.3%*\n[0.0,\n69.6]*33.3%*\n[0.0,\n69.6]\n*24.4%*\n[0.0,\n57.5]Win Rate\nT oken Usage\n0.00.20.40.60.81.01.2\nT oken Usage1e6\n*819425*\n[671140,\n967710]*811620*\n[655029,\n968210]*908451*\n[798172,\n1000000]Win Rate and T oken Usage for Countdown (diff=7)\nModel: gpt-4oFigure 7: WinRate and token usage for MCTS on the Countdown task (difficulty 7) using GPT-4o.\nResults are shown for exploration constants c= 0.5,1.0, and 2.5. Both c= 0.5andc= 1.0achieve\nequal win rates, with c= 1.0using marginally fewer tokens on average. Values in “*” denote the\nmean, and square brackets “[ ]” represent the 95% confidence interval.\nMCTS(c=0.5) MCTS(c=1.0) MCTS(c=2.5)\nMethods0255075100125150175200225Win Rate (%)*100.0%*\n[100.0,\n100.0]\n*2.2%*\n[0.0,\n13.6]*0.0%*\n[0.0,\n0.0]Win Rate\nT oken Usage\n020000400006000080000100000120000140000\nT oken Usage *36580*\n[34411,\n38748]*99685*\n[97195,\n100000]*100865*\n[100693,\n100000]Win Rate and T oken Usage for Sudoku (width=2, height=2)\nModel: gpt-4o\nFigure 8: WinRate and token usage for MCTS on the Sudoku (4 ×4) task using GPT-4o. Results are\nshown for exploration constants c= 0.5,1.0, and 2.5. Only c= 0.5successfully solves all games,\nand it does so with significantly lower token usage compared to the other cvalues, which struggle to\nsolve any. Values in “*”denote the mean, and square brackets “[ ]” represent the 95% confidence\ninterval.\nG Additional Experiment Results\nBelow, we present detailed experimental results across all Countdown and Sudoku variants. The\nsubsections are organized as follows: performance profiles G.1, Countdown results G.2, Sudoku\nresults G.3, cumulative wins G.4, and tree size analyses G.5.\n33\n--- Page 34 ---\nG.1 Performance Profiles\n100101\nPerformance Ratio ( )\n0.00.20.40.60.81.0Fraction of T asks\nWin Rate Profile\nModel: gpt-4o\nT oT-BFS (AUP: 4.057)\nBESTFS (AUP: 6.204)\nMCTS(c=0.5) (AUP: 7.098)\nLFS (AUP: 8.994)\nFigure 9: Performance profiles (WinRate) across all variants of Countdown and Sudoku tasks for\nmethods ToT-BFS, BestFS, MCTS, and LFS, evaluated with GPT-4o. LFS achieves the highest\nArea Under Profile (AUP) value , indicating superior overall WinRate.\n100101\nPerformance Ratio ( )\n0.00.10.20.30.40.50.6Fraction of T asks\nWin Rate / T oken Usage Profile\nModel: gpt-4o\nT oT-BFS (AUP: 3.676)\nBESTFS (AUP: 2.670)\nMCTS(c=0.5) (AUP: 3.544)\nLFS (AUP: 4.704)\nFigure 10: Performance profiles (WinRate per Token Ratio) across all variants of Countdown and\nSudoku tasks for methods ToT-BFS, BestFS, MCTS, and LFS, evaluated with GPT-4o. Among these,\nLFS achieves the highest Area Under Profile (AUP) value , indicating it provides the best balance\nbetween WinRate and token efficiency.\n34\n--- Page 35 ---\n100101\nPerformance Ratio ( )\n0.00.10.20.30.40.50.60.70.8Fraction of T asks\nWin Rate Profile\nModel: o3-mini\nBESTFS (AUP: 4.234)\nMCTS(c=0.5) (AUP: 6.004)\nLFS (AUP: 7.200)Figure 11: Performance profiles (WinRate) across all variants of Countdown and Sudoku tasks for\nmethods BestFS, MCTS, and LFS, evaluated with o3-mini. Among these, LFS achieves the highest\nArea Under Profile (AUP) value , demonstrating superior overall performance.\n100101\nPerformance Ratio ( )\n0.00.10.20.30.40.50.60.70.8Fraction of T asks\nWin Rate / T oken Usage Profile\nModel: o3-mini\nBESTFS (AUP: 3.244)\nMCTS(c=0.5) (AUP: 5.617)\nLFS (AUP: 7.200)\nFigure 12: Performance profiles (WinRate per Token Ratio) across all variants of Countdown and\nSudoku tasks for methods BestFS, MCTS, and LFS, evaluated with o3-mini. LFS achieves the\nhighest Area Under Profile (AUP) value , indicating the best efficiency-performance trade-off among\nthe methods.\nfewe\n35\n--- Page 36 ---\nG.2 Countdown Results\nGPT-4o Results\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120140Win Rate (%)*82.1%*\n[63.6,\n100.0]*100.0%*\n[100.0,\n100.0]*100.0%*\n[100.0,\n100.0]*100.0%*\n[100.0,\n100.0]Win Rates for Countdown (diff=3)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(a) Win rates for difficulty 3.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods05000100001500020000T oken Usage*8348*\n[8235,\n8461]*11087*\n[8444,\n13730]*10671*\n[7455,\n13886]*9903*\n[8169,\n11638]T oken Usage for Countdown (diff=3)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for difficulty 3.\nFigure 13: WinRate and token usage for different methods (ToT-BFS, BestFS, MCTS, and LFS) on\nthe Countdown task (difficulty 3) using GPT-4o. (a)WinRate; (b)Token Usage. ToT-BFS was the\nonly method that did not solve all instances, while the other three methods successfully solved all\ntasks. Among these three, LFS used the fewest tokens, indicating the best efficiency. Values in “*”\ndenote the mean, and square brackets “[ ]” represent the 95% confidence interval.\n36\n--- Page 37 ---\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120Win Rate (%)\n*9.5%*\n[0.0,\n23.6]*49.5%*\n[25.4,\n73.6]*60.0%*\n[36.4,\n83.6]*63.2%*\n[39.9,\n86.4]Win Rates for Countdown (diff=5)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS(a) Win rates for difficulty 5.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods0.00.20.40.60.81.01.21.4T oken Usage1e6\n*28863*\n[28616,\n29110]*700147*\n[543768,\n856527]*569730*\n[384774,\n754687]*508522*\n[332344,\n684699]T oken Usage for Countdown (diff=5)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for difficulty 5.\nFigure 14: WinRate and token usage for different methods (ToT-BFS, BestFS, MCTS, and LFS) on\nthe Countdown task (difficulty 5) using GPT-4o. (a)WinRate; (b)Token Usage. LFS marginally\noutperforms the next best method, MCTS, while also using fewer tokens, indicating both higher\neffectiveness and efficiency. Values in “*”denote the mean, and square brackets “[ ]” represent the\n95% confidence interval.\n37\n--- Page 38 ---\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020406080100Win Rate (%)\n*0.0%*\n[0.0,\n0.0]*11.1%*\n[0.0,\n35.3]*32.6%*\n[10.0,\n55.2]*47.4%*\n[23.3,\n71.4]Win Rates for Countdown (diff=7)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS(a) Win rates for difficulty 7.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods0.00.20.40.60.81.01.21.41.6T oken Usage1e6\n*55672*\n[54780,\n56564]*983297*\n[958772,\n1000000] *821404*\n[723330,\n919478]*737174*\n[620191,\n854158]T oken Usage for Countdown (diff=7)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for difficulty 7.\nFigure 15: WinRate and Token Usage for different methods (ToT-BFS, BestFS, MCTS, and LFS) on\nthe Countdown task (difficulty 7) using GPT-4o. (a)WinRate; (b)Token Usage. The performance\ngap between MCTS and LFS widens as difficulty increases, with LFS maintaining higher efficiency\nby using fewer tokens. Values in “*”denote the mean, and square brackets “[ ]” represent the 95%\nconfidence interval.\n38\n--- Page 39 ---\no3-mini Results\nBESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120Win Rate (%)*52.6%*\n[28.6,\n76.7]*69.5%*\n[47.3,\n91.7]*70.5%*\n[48.6,\n92.5]Win Rates for Countdown (diff=5)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(a) Win rates for Countdown (difficulty 5).\nBESTFS\nMCTS(c=0.5)LFS\nMethods0.00.20.40.60.81.01.2T oken Usage1e6\n*656260*\n[492951,\n819568]*480216*\n[301174,\n659258]*451792*\n[265429,\n638154]T oken Usage for Countdown (diff=5)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for Countdown (difficulty 5).\nFigure 16: WinRate and Token Usage for different methods (BestFS, MCTS, and LFS) on the\nCountdown task (difficulty 5) using o3-mini .(a)WinRate; (b)Token Usage. The performance\ntrends closely mirror those observed with GPT-4o: LFS marginally outperforms MCTS while also\nusing fewer tokens, indicating stronger efficiency. Values in “*”denote the mean, and square brackets\n“[ ]” represent the 95% confidence interval.\n39\n--- Page 40 ---\nBESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120140Win Rate (%)\n*13.3%*\n[0.0,\n39.5]*41.1%*\n[17.3,\n64.8]*78.9%*\n[59.3,\n98.6]Win Rates for Countdown (diff=7)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS(a) Win rates for Countdown (difficulty 7).\nBESTFS\nMCTS(c=0.5)LFS\nMethods0.00.20.40.60.81.01.21.41.6T oken Usage1e6\n*955818*\n[889575,\n1000000]\n*708961*\n[558894,\n859028]*498280*\n[376326,\n620233]T oken Usage for Countdown (diff=7)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for Countdown (difficulty 7).\nFigure 17: WinRate and Token Usage for different methods (BestFS, MCTS, and LFS) on the\nCountdown task (difficulty 7) using o3-mini .(a)WinRate; (b)Token Usage. The performance gap\nbetween MCTS and LFS widens as task difficulty increases, mirroring results with GPT-4o, with LFS\nmaintaining higher efficiency through lower token usage. Values in “*”denote the mean, and square\nbrackets“[ ]” represent the 95% confidence interval.\n40\n--- Page 41 ---\nG.3 Sudoku Results\nGPT-4o Results\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120140Win Rate (%)*53.7%*\n[29.7,\n77.7]*41.1%*\n[17.3,\n64.8]*100.0%*\n[100.0,\n100.0]*96.8%*\n[88.4,\n100.0]Win Rates for Sudoku (width=2, height=2)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(a) Win rates for Sudoku 4×4.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020000400006000080000100000120000140000T oken Usage*69715*\n[67433,\n71997]*75716*\n[62305,\n89127]\n*34674*\n[32697,\n36651]*39069*\n[35999,\n42138]T oken Usage for Sudoku (width=2, height=2)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for Sudoku 4×4.\nFigure 18: WinRate and Token Usage on the Sudoku 4×4task using GPT-4o .(a)WinRate; (b)Token\nUsage. Results are shown for ToT-BFS, BestFS, MCTS, and LFS. MCTS marginally outperforms\nLFS in both WinRate and token efficiency, while ToT-BFS and BestFS lag significantly behind.\nValues in “*”denote the mean, and square brackets “[ ]” represent the 95% confidence interval.\n41\n--- Page 42 ---\nT oT-BFS\nMCTS(c=0.5)LFS\nMethods0.02.55.07.510.012.515.017.520.0Win Rate (%)\n*0.0%*\n[0.0,\n0.0]*0.0%*\n[0.0,\n0.0]*2.2%*\n[0.0,\n13.6]Win Rates for Sudoku (width=2, height=3)\nModel: gpt-4o\nT oT-BFS\nMCTS(c=0.5)\nLFS(a) Win rates for Sudoku 6×6.\nT oT-BFS\nMCTS(c=0.5)LFS\nMethods0100000200000300000400000500000600000700000800000T oken Usage\n*119447*\n[115253,\n123640]*500657*\n[500503,\n500000]*494131*\n[479955,\n500000]T oken Usage for Sudoku (width=2, height=3)\nModel: gpt-4o\nT oT-BFS\nMCTS(c=0.5)\nLFS\n(b) Token usage for Sudoku 6×6.\nFigure 19: WinRate and Token Usage on the Sudoku 6×6task using GPT-4o .(a)WinRate; (b)\nToken Usage. Results are shown for ToT-BFS, MCTS, and LFS. All methods fail to solve any\ninstances, except LFS, which successfully solves a single game. Despite the overall difficulty. Values\nin“*”denote the mean, and square brackets “[ ]” represent the 95% confidence interval.\n42\n--- Page 43 ---\no3-mini Results\nBESTFS\nMCTS(c=0.5)LFS\nMethods020406080100120140Win Rate (%)*61.1%*\n[37.5,\n84.6]*90.5%*\n[76.4,\n100.0]*96.8%*\n[88.4,\n100.0]Win Rates for Sudoku (width=2, height=2)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(a) WinRate\nBESTFS\nMCTS(c=0.5)LFS\nMethods020000400006000080000100000120000T oken Usage*74699*\n[69086,\n80312]*70710*\n[68549,\n72871]*64463*\n[62932,\n65995]T oken Usage for Sudoku (width=2, height=2)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token Usage\nFigure 20: WinRate and Token Usage on the Sudoku 4×4task using o3-mini .(a)WinRate; (b)\nToken Usage. Results are shown for BestFS, MCTS, and LFS. Unlike the GPT-4o setting, LFS now\noutperforms MCTS in both WinRate and token efficiency, highlighting that our method scales more\neffectively with stronger models. Values in “*”denote the mean, and square brackets “[ ]” represent\nthe 95% confidence interval.\n43\n--- Page 44 ---\nBESTFS\nMCTS(c=0.5)LFS\nMethods0102030405060Win Rate (%)\n*0.0%*\n[0.0,\n0.0]*4.2%*\n[0.0,\n13.9]*25.3%*\n[4.3,\n46.2]Win Rates for Sudoku (width=2, height=3)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS(a) WinRate\nBESTFS\nMCTS(c=0.5)LFS\nMethods0100000200000300000400000500000600000700000800000T oken Usage*503282*\n[502639,\n500000]*496296*\n[490256,\n500000]*431064*\n[405232,\n456896]T oken Usage for Sudoku (width=2, height=3)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\n(b) Token Usage\nFigure 21: WinRate and Token Usage on the Sudoku 6×6task using o3-mini .(a)WinRate; (b)\nToken Usage. Results are shown for BestFS, MCTS, and LFS. The trend from the 4×4variant\ncontinues, with LFS significantly outperforming MCTS in both accuracy and token efficiency. This\nindicates that LFS scales more effectively with stronger models and handles more difficult tasks more\nrobustly. Values in “*”denote the mean, and square brackets “[ ]” represent the 95% confidence\ninterval.\nG.4 Cumulative Wins\nWe provide detailed results illustrating the cumulative wins achieved by different methods as the\ntoken budget increases for both Countdown and Sudoku games. As shown in Figures 22a and 22b,\nthe total number of Countdown games won steadily rises with higher token usage, with LFS clearly\noutperforming the next best method, MCTS . This performance gap is especially pronounced for\nthe stronger o3-mini model (Figure 22b), indicating that LFS scales more effectively with model\nstrength. Although compute limitations prevented testing at larger token budgets, the current trend\nsuggests this gap would continue to widen. A similar but less prominent pattern can be observed for\nSudoku (Figures 22c and 22d), where WinRate saturation on simpler Sudoku variants and overall\nlower performance on harder variants temper the advantage.\n44\n--- Page 45 ---\n104\n105\n106\nT oken Usage (log scale)0255075100125150175200Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Countdown\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS(a)\n104\n105\n106\nT oken Usage (log scale)020406080100120140Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Countdown\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS (b)\n105\nT oken Usage (log scale)0153045607590Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Sudoku\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\n(c) Cumulative wins in Sudoku with increasing token\nbudget (GPT-4o).\n105\nT oken Usage (log scale)0153045607590105120Cumulative Number of Wins\nCumulative Wins vs T oken Usage for Sudoku\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS(d) Cumulative wins in Sudoku with increasing token\nbudget (o3-mini).\nFigure 22: Cumulative wins across varying token budgets for Countdown and Sudoku games using\ndifferent methods. Panels (a) and (b) show Countdown results for GPT-4o and o3-mini models\nrespectively, highlighting the superior scalability of LFS over MCTS , particularly with the stronger\nmodel. Panels (c) and (d) display cumulative Sudoku wins, where the performance gap is less\npronounced due to WinRate saturation and increased task difficulty.\nG.5 Tree Size\nWe report the average tree sizes generated by each method across different levels of difficulty for\nboth the Countdown and Sudoku domains, using the GPT-4o and o3-mini models. In the Countdown\nsetting, we observe that LFS consistently constructs smaller or equal-sized trees compared to MCTS. A\nsimilar pattern emerges in the Sudoku tasks, across both the 4×4and6×6grid configurations. These\nresults illustrate the efficiency of LFS’s guided exploration strategy, which avoids the over-exploration\ncharacteristic of MCTS, and maintains performance even as problem complexity increases.\n45\n--- Page 46 ---\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods0510152025303540Tree Size*26*\n[26,\n27]*29*\n[26,\n32]\n*20*\n[19,\n22]*20*\n[19,\n22]Tree Size for Countdown (diff=3)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFSFigure 23: Average tree size for Countdown (difficulty 3) using GPT-4o.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods0500100015002000250030003500Tree Size\n*190*\n[188,\n191]*2,559*\n[2,327,\n2,792]\n*1,308*\n[1,125,\n1,490]*1,299*\n[1,100,\n1,498]Tree Size for Countdown (diff=5)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 24: Average tree size for Countdown (difficulty 5) using GPT-4o.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods020004000600080001000012000Tree Size\n*623*\n[617,\n629]*8,958*\n[8,293,\n9,623]\n*2,467*\n[2,293,\n2,642]*2,939*\n[2,687,\n3,191]Tree Size for Countdown (diff=7)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 25: Average tree size for Countdown (difficulty 7) using GPT-4o.\n46\n--- Page 47 ---\nBESTFS\nMCTS(c=0.5)LFS\nMethods050010001500200025003000Tree Size*2,141*\n[1,915,\n2,368]\n*1,179*\n[990,\n1,368]*1,080*\n[896,\n1,265]Tree Size for Countdown (diff=5)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFSFigure 26: Average tree size for Countdown (difficulty 5) using o3-mini.\nBESTFS\nMCTS(c=0.5)LFS\nMethods0200040006000800010000Tree Size*7,988*\n[7,259,\n8,717]\n*2,253*\n[2,017,\n2,489]*1,923*\n[1,678,\n2,168]Tree Size for Countdown (diff=7)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 27: Average tree size for Countdown (difficulty 7) using o3-mini.\nT oT-BFS BESTFS\nMCTS(c=0.5)LFS\nMethods0100200300400500Tree Size*378*\n[371,\n386]*398*\n[349,\n446]\n*107*\n[104,\n109]*95*\n[89,\n101]Tree Size for Sudoku (width=2, height=2)\nModel: gpt-4o\nT oT-BFS\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 28: Average tree size for Sudoku ( 2×2) using GPT-4o.\n47\n--- Page 48 ---\nT oT-BFS\nMCTS(c=0.5)LFS\nMethods02505007501000125015001750Tree Size*928*\n[898,\n958]*1,352*\n[1,307,\n1,397]\n*916*\n[869,\n963]Tree Size for Sudoku (width=2, height=3)\nModel: gpt-4o\nT oT-BFS\nMCTS(c=0.5)\nLFSFigure 29: Average tree size for Sudoku ( 2×3) using GPT-4o.\nBESTFS\nMCTS(c=0.5)LFS\nMethods050100150200Tree Size*167*\n[154,\n180]\n*103*\n[101,\n106]*97*\n[96,\n99]Tree Size for Sudoku (width=2, height=2)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 30: Average tree size for Sudoku ( 2×2) using o3-mini.\nBESTFS\nMCTS(c=0.5)LFS\nMethods05001000150020002500Tree Size*1,978*\n[1,943,\n2,012]\n*801*\n[779,\n824]*487*\n[467,\n506]Tree Size for Sudoku (width=2, height=3)\nModel: o3-mini\nBESTFS\nMCTS(c=0.5)\nLFS\nFigure 31: Average tree size for Sudoku ( 2×3) using o3-mini.\n48\n--- Page 49 ---\nH Example Trees\nTarget: 10\nAvailable Numbers: [35, 60, 37, 4, 52, 4, 59]\nOperations: []\nTarget: 10\nAvailable Numbers: [35, 37, 4, 4, 59, 8]\nOperations: ['60-52=8']\nTarget: 10\nAvailable Numbers: [4, 4, 59, 8, 2]\nOperations: ['60-52=8', '37-35=2']Target: 10\nAvailable Numbers: [35, 37, 59, 8, 1]\nOperations: ['60-52=8', '4/4=1']Target: 10\nAvailable Numbers: [35, 37, 4, 59, 12]\nOperations: ['60-52=8', '4+8=12']Target: 10\nAvailable Numbers: [35, 37, 4, 59, 2]\nOperations: ['60-52=8', '8/4=2']Target: 10\nAvailable Numbers: [35, 37, 4, 59, 2]\nOperations: ['60-52=8', '8/4=2']\nTarget: 10\nAvailable Numbers: [4, 4, 59, 10]\nOperations: ['60-52=8', '37-35=2', '8+2=10']\nTarget: 10\nAvailable Numbers: [59, 10, 1]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4/4=1']Target: 10\nAvailable Numbers: [4, 59, 14]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4+10=14']Target: 10\nAvailable Numbers: [4, 59, 6]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '10-4=6']Target: 10\nAvailable Numbers: [4, 59, 14]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4+10=14']Target: 10\nAvailable Numbers: [4, 59, 6]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '10-4=6']\nTarget: 10\nAvailable Numbers: [59, 10]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4+10=14', '14-4=10']Target: 10\nAvailable Numbers: [59, 10]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4+10=14', '14-4=10']\nTarget: 10\nAvailable Numbers: [49]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '4+10=14', '14-4=10', '59-10=49']Target: 10\nAvailable Numbers: [59, 10]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '10-4=6', '4+6=10']\nTarget: 10\nAvailable Numbers: [49]\nOperations: ['60-52=8', '37-35=2', '8+2=10', '10-4=6', '4+6=10', '59-10=49']Target: 10\nAvailable Numbers: [59, 8, 1, 2]\nOperations: ['60-52=8', '4/4=1', '37-35=2']Target: 10\nAvailable Numbers: [35, 37, 59, 9]\nOperations: ['60-52=8', '4/4=1', '8+1=9']\nTarget: 10\nAvailable Numbers: [59, 2, 9]\nOperations: ['60-52=8', '4/4=1', '37-35=2', '8+1=9']Target: 10\nAvailable Numbers: [59, 1, 10]\nOperations: ['60-52=8', '4/4=1', '37-35=2', '8+2=10']Target: 10\nAvailable Numbers: [59, 9, 2]\nOperations: ['60-52=8', '4/4=1', '8+1=9', '37-35=2']\nTarget: 10\nAvailable Numbers: [59, 11]\nOperations: ['60-52=8', '4/4=1', '8+1=9', '37-35=2', '9+2=11']Target: 10\nAvailable Numbers: [4, 59, 12, 2]\nOperations: ['60-52=8', '4+8=12', '37-35=2']Target: 10\nAvailable Numbers: [35, 37, 59, 8]\nOperations: ['60-52=8', '4+8=12', '12-4=8']Target: 10\nAvailable Numbers: [35, 37, 59, 3]\nOperations: ['60-52=8', '4+8=12', '12/4=3']\nTarget: 10\nAvailable Numbers: [59, 2, 8]\nOperations: ['60-52=8', '4+8=12', '37-35=2', '12-4=8']Target: 10\nAvailable Numbers: [4, 59, 10]\nOperations: ['60-52=8', '4+8=12', '37-35=2', '12-2=10']\nTarget: 10\nAvailable Numbers: [59, 14]\nOperations: ['60-52=8', '4+8=12', '37-35=2', '12-2=10', '4+10=14']Target: 10\nAvailable Numbers: [59, 3, 2]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '37-35=2']Target: 10\nAvailable Numbers: [37, 3, 24]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '59-35=24']\nTarget: 10\nAvailable Numbers: [59, 5]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '37-35=2', '3+2=5']Target: 10\nAvailable Numbers: [3, 13]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '59-35=24', '37-24=13']Target: 10\nAvailable Numbers: [37, 8]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '59-35=24', '24/3=8']\nTarget: 10\nAvailable Numbers: [10]\nOperations: ['60-52=8', '4+8=12', '12/4=3', '59-35=24', '37-24=13', '13-3=10']Target: 10\nAvailable Numbers: [35, 37, 59, 6]\nOperations: ['60-52=8', '8/4=2', '4+2=6']Target: 10\nAvailable Numbers: [35, 37, 59, 8]\nOperations: ['60-52=8', '8/4=2', '4*2=8']\nTarget: 10\nAvailable Numbers: [59, 6, 2]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '37-35=2']\nTarget: 10\nAvailable Numbers: [59, 8]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '37-35=2', '6+2=8']Target: 10\nAvailable Numbers: [59, 12]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '37-35=2', '6*2=12']\nTarget: 10\nAvailable Numbers: [51]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '37-35=2', '6+2=8', '59-8=51']Target: 10\nAvailable Numbers: [59, 8, 2]\nOperations: ['60-52=8', '8/4=2', '4*2=8', '37-35=2']\nTarget: 10\nAvailable Numbers: [59, 10]\nOperations: ['60-52=8', '8/4=2', '4*2=8', '37-35=2', '8+2=10']Target: 10\nAvailable Numbers: [35, 37, 59, 6]\nOperations: ['60-52=8', '8/4=2', '4+2=6']Target: 10\nAvailable Numbers: [35, 37, 59, 8]\nOperations: ['60-52=8', '8/4=2', '4*2=8']\nTarget: 10\nAvailable Numbers: [59, 6, 2]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '37-35=2']Target: 10\nAvailable Numbers: [37, 6, 24]\nOperations: ['60-52=8', '8/4=2', '4+2=6', '59-35=24']Target: 10\nAvailable Numbers: [59, 8, 2]\nOperations: ['60-52=8', '8/4=2', '4*2=8', '37-35=2']\n(a) MCTS search\ntree\nTarget: 10\nAvailable Numbers: [35, 60, 37, 4, 52, 4, 59]\nOperations: []\nTarget: 10\nAvailable Numbers: [35, 60, 37, 4, 4, 7]\nOperations: ['59-52=7']\nTarget: 10\nAvailable Numbers: [60, 37, 4, 4, 5]\nOperations: ['59-52=7', '35/7=5']Target: 10\nAvailable Numbers: [35, 60, 37, 4, 3]\nOperations: ['59-52=7', '7-4=3']\nTarget: 10\nAvailable Numbers: [37, 4, 5, 15]\nOperations: ['59-52=7', '35/7=5', '60/4=15']Target: 10\nAvailable Numbers: [37, 4, 5, 15]\nOperations: ['59-52=7', '35/7=5', '60/4=15']Target: 10\nAvailable Numbers: [60, 37, 4, 9]\nOperations: ['59-52=7', '35/7=5', '4+5=9']Target: 10\nAvailable Numbers: [60, 37, 4, 1]\nOperations: ['59-52=7', '35/7=5', '5-4=1']Target: 10\nAvailable Numbers: [60, 37, 4, 9]\nOperations: ['59-52=7', '35/7=5', '4+5=9']Target: 10\nAvailable Numbers: [60, 37, 4, 1]\nOperations: ['59-52=7', '35/7=5', '5-4=1']\nTarget: 10\nAvailable Numbers: [37, 4, 10]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-5=10']\nTarget: 10\nAvailable Numbers: [10, 33]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-5=10', '37-4=33']Target: 10\nAvailable Numbers: [37, 15, 1]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '5-4=1']Target: 10\nAvailable Numbers: [37, 5, 11]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-4=11']Target: 10\nAvailable Numbers: [37, 4, 10]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-5=10']\nTarget: 10\nAvailable Numbers: [37, 16]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '5-4=1', '15+1=16']Target: 10\nAvailable Numbers: [37, 6]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-4=11', '11-5=6']Target: 10\nAvailable Numbers: [37, 14]\nOperations: ['59-52=7', '35/7=5', '60/4=15', '15-5=10', '4+10=14']Target: 10\nAvailable Numbers: [60, 37, 5]\nOperations: ['59-52=7', '35/7=5', '4+5=9', '9-4=5']Target: 10\nAvailable Numbers: [60, 37, 5]\nOperations: ['59-52=7', '35/7=5', '5-4=1', '4+1=5']\nTarget: 10\nAvailable Numbers: [37, 12]\nOperations: ['59-52=7', '35/7=5', '5-4=1', '4+1=5', '60/5=12']\nTarget: 10\nAvailable Numbers: [25]\nOperations: ['59-52=7', '35/7=5', '5-4=1', '4+1=5', '60/5=12', '37-12=25']Target: 10\nAvailable Numbers: [60, 37, 5]\nOperations: ['59-52=7', '35/7=5', '4+5=9', '9-4=5']Target: 10\nAvailable Numbers: [60, 37, 5]\nOperations: ['59-52=7', '35/7=5', '5-4=1', '4+1=5']Target: 10\nAvailable Numbers: [35, 60, 37, 12]\nOperations: ['59-52=7', '7-4=3', '4*3=12']Target: 10\nAvailable Numbers: [35, 60, 37, 1]\nOperations: ['59-52=7', '7-4=3', '4-3=1']\nTarget: 10\nAvailable Numbers: [35, 37, 5]\nOperations: ['59-52=7', '7-4=3', '4*3=12', '60/12=5']\nTarget: 10\nAvailable Numbers: [5, 2]\nOperations: ['59-52=7', '7-4=3', '4*3=12', '60/12=5', '37-35=2']\nTarget: 10\nAvailable Numbers: [10]\nOperations: ['59-52=7', '7-4=3', '4*3=12', '60/12=5', '37-35=2', '5*2=10']Target: 10\nAvailable Numbers: [60, 1, 2]\nOperations: ['59-52=7', '7-4=3', '4-3=1', '37-35=2'] (b) LFS search tree\nFigure 32: Example search trees generated for a Countdown game (difficulty = 7) using (a) Monte\nCarlo Tree Search (MCTS) and (b) Limited-Depth Forward Search (LFS). The MCTS tree is notice-\nably wider, illustrating its tendency for over-exploration compared to the more focused LFS tree.\n49\n--- Page 50 ---\nFigure 32 shows example search trees generated for a Countdown game with difficulty level 7.\nSubfigure (a) depicts the tree produced by MCTS, while subfigure (b) shows the tree from LFS.\nNotice that the MCTS tree is considerably wider, reflecting its tendency to over-explore the search\nspace. In contrast, the LFS tree is more focused and narrower, indicating a more targeted exploration\nstrategy. This comparison highlights the differences in exploration behaviour between the two\nmethods on the same problem instance.\n50",
  "text_length": 109213
}