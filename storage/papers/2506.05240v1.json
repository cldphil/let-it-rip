{
  "id": "http://arxiv.org/abs/2506.05240v1",
  "title": "Aligning Latent Spaces with Flow Priors",
  "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.",
  "authors": [
    "Yizhuo Li",
    "Yuying Ge",
    "Yixiao Ge",
    "Ying Shan",
    "Ping Luo"
  ],
  "published": "2025-06-05T16:59:53Z",
  "updated": "2025-06-05T16:59:53Z",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05240v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05240v1  [cs.LG]  5 Jun 2025Aligning Latent Spaces with Flow Priors\nYizhuo Li1,2, Yuying Ge2,B, Yixiao Ge2, Ying Shan2, Ping Luo1,B\n1The University of Hong Kong,2ARC Lab, Tencent PCG\nProject Page: https://liyizhuo.com/align/\nAbstract\nThis paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as priors.\nOur method first pretrains a flow model on the target features to capture the\nunderlying distribution. This fixed flow model subsequently regularizes the latent\nspace via an alignment loss, which reformulates the flow matching objective\nto treat the latents as optimization targets. We formally prove that minimizing\nthis alignment loss establishes a computationally tractable surrogate objective\nfor maximizing a variational lower bound on the log-likelihood of latents under\nthe target distribution. Notably, the proposed method eliminates computationally\nexpensive likelihood evaluations and avoids ODE solving during optimization. As\na proof of concept, we demonstrate in a controlled setting that the alignment loss\nlandscape closely approximates the negative log-likelihood of the target distribution.\nWe further validate the effectiveness of our approach through large-scale image\ngeneration experiments on ImageNet with diverse target distributions, accompanied\nby detailed discussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.\n1 Introduction\nLatent models like autoencoders (AEs) are a cornerstone of modern machine learning [ 24,3,36,\n8,45]. These models typically map high-dimensional observations to a lower-dimensional latent\nspace, aiming to capture salient features and dependencies [ 40,44]. A highly desirable property\nof latent models is that the latent space should have structural properties, such as being close to a\npredefined target distribution [ 53,33,63,5]. Such structure can incorporate domain-specific prior\nknowledge [ 28,51], enhance the interpretability of the latent space[ 23,9,30], and facilitate latent\nspace generation [ 52,37,35,62,64]. While significant progress has been made, ensuring that the\nlearned latent representations possess such desired structure remains a crucial challenge.\nTraditional approaches to enforcing distributional conformity often involve minimizing divergences\nlike the Kullback-Leibler (KL) divergence [ 33,52]. However, KL can be restrictive, particularly\nwhen the target prior is only implicitly defined (e.g., by samples). In latent generative modeling, the\nlatent space is usually regularized with known prior distributions, such as the Gaussian distribution\nfor Variational Autoencoders (V AE) [ 33,18], and the categorical distribution for Vector Quantized\nV AE (VQ-V AE) [ 61]. Recent works [ 50,38,39,4,63,5] have proposed to use pre-trained feature\nextractors as target distribution and directly optimize the latent distances, which are shown to be\neffective but computationally expensive and require per-sample features.\nRecent advances in flow-based generative models [ 41,42] offer a promising avenue to capture\ncomplex target distributions. In this work, we address the question: Can we efficiently align a\nlearnable latent space to an arbitrary target distribution using a pre-trained flow model as a prior?\nWe answer this question affirmatively by proposing a novel framework that leverages a pre-trained\nflow model to define a computationally tractable alignment loss, which effectively guides the latents\ntowards the target distribution.\nPreprint. Under review.\n--- Page 2 ---\nLatentGaussianKLFlowFlowLatentTargetAnyDist.\nStage1:CaptureanydistributionwithaflowmodelStage2:Alignthelatentspacewithpre-trainedflowpriors\n(a)(b)Figure 1: (a) Conventional alignment works with only known priors (e.g., Gaussian or categorical)\nusing KL or cross-entropy losses. (b) Our proposed method can align the latent distribution to\narbitrary target distribution captured by a pre-trained flow model.\nOur proposed approach unfolds in a two-stage process as illustrated in Fig.1. The first stage involves\npre-training a flow-based generative model on the desired target features, allowing it to learn the\nmapping from a base distribution (e.g., Gaussian) to the target distribution. Once this flow model\naccurately captures the target distribution, its parameters are fixed. In the second stage, this flow\nmodel serves as a prior to regularize a learnable latent space, for instance, the output of the encoder\nin an AE. This regularization is achieved by minimizing an alignment loss, which ingeniously adapts\nthe standard flow matching objective by treating the learnable latents as the target. This pipeline\nprovides an efficient mechanism to guide the latent space towards the desired target structure without\nrequiring direct comparison to target samples or expensive likelihood evaluations of the flow model.\nWe theoretically justify our method by connecting the alignment loss to the maximum likelihood\nestimation of the latents under the target distribution. While directly maximizing this likelihood under\na flow model is often computationally prohibitive due to the need to evaluate the trace of Jacobian\ndeterminants and solve an ordinary differential equation (ODE) for each step, our alignment loss\noffers a more tractable alternative. We formally demonstrate that minimizing this loss serves as a\ncomputationally efficient proxy for maximizing a variational lower bound on the log-likelihood of\nthe latents under the flow-defined target distribution.\nOur framework offers three key advantages. First, our approach enables alignment to arbitrary target\ndistributions , even those implicitly defined by samples, overcoming the limitations of conventional\nmethods that require explicit parametric priors. Second, the alignment loss acts as a direct surrogate\nfor the log-likelihood of latents under the target distribution, providing a theoretically grounded\nobjective that avoids heuristic metrics like cosine similarity used in per-sample feature matching [ 4,\n63,5]. Third, our framework is computationally lightweight , requiring only a single forward\npass through the pre-trained flow model during training, thereby bypassing the need for expensive\nadversarial optimization [19], likelihood evaluations, or per-sample feature extraction [50, 38, 39].\nWe empirically validate the efficacy of our proposed alignment strategy through a series of experi-\nments. We start with illustrative experiments in a controlled toy setting using a mixture of Gaussians\nto confirm that our alignment loss landscape indeed serves as a proxy for the log-likelihood of the\nlatents under the target distribution. Then we demonstrate the scalability of our approach by con-\nducting large-scale image generation experiments on ImageNet [ 11] with diverse target distributions.\nDetailed discussions and ablation studies are provided to underscore the robustness and effectiveness.\nWe believe this method offers a powerful and flexible tool for incorporating rich distributional priors\ninto latent models. Our work paves the way for more flexible and powerful structured representation\nlearning, and we anticipate its application and extension in various domains requiring distributional\nstructure control over latent spaces.\n2 Related Work\n2.1 Flow-based Models\nFlow-based generative models have emerged as a powerful class of generative models [ 17,34,7,31,\n66,68,55]. They were first introduced as CNFs [ 6,20] that learn an invertible mapping between a\nsimple base distribution (e.g., Gaussian) and a complex data distribution. Early works like NICE [ 13]\nand Real NVP [ 14] introduced additive and affine coupling layers to construct invertible neural\nnetworks. A notable recent development is Flow Matching (FM) [ 41,1,42,46,21,59], which\n2\n--- Page 3 ---\nsimplifies the training by regressing a vector field against a target field derived from pairs of samples,\navoiding the need for simulating ODEs during training. In ICTM [ 67], flow priors of generative\nmodels have been employed for MAP estimation to solve linear inverse problems. Our work leverages\nflow-based models to learn complex distributions as a prior for latent space alignment.\n2.2 Latent Space Alignment\nThe alignment of latent spaces with predefined distributions is a crucial aspect of representation\nlearning. In V AE [ 33], the latent space is typically regularized to follow a standard Gaussian\ndistribution. Several approaches have been proposed to use more flexible priors, such as hierarchical\nV AEs [ 56,60] or V AEs with inverse autoregressive flow (IAF) priors [ 32]. Another line of work\nfocuses on aligning latent spaces with features extracted from pre-trained models [ 50,38,39,4,63,\n5,29,65]. Our method differs by utilizing a pre-trained flow model to define an expressive target\nand a novel alignment loss, avoiding expensive likelihoods, adversarial training, or direct per-sample\nfeature comparison.\n3 Preliminaries\n3.1 Flow-Based Models\nWe consider an ordinary differential equation (ODE) ideally defined by a time-dependent velocity\nfieldu(xt, t). The dynamics are given by:\ndxt\ndt=u(xt, t),x0∼pinit,x1∼pdata (1)\nHere, pinitis a simple prior distribution (e.g., a standard Gaussian distribution N(0,I)), and pdata\nis the target data distribution. We denote xt∈Rdas the state at time t, with x0being the initial\nstate and x1the state at t= 1. The velocity field u:Rd×[0,1]→Rdis assumed to be Lipschitz\ncontinuous in xand continuous in tto ensure the existence and uniqueness of ODE solutions.\nIn practice, the ideal velocity field uis unknown. We approximate it with a parametric model,\ntypically a neural network vθ(xt, t), parameterized by θ. This defines a learned generative process:\ndxt\ndt=vθ(xt, t),x0∼pinit (2)\nFor a given initial condition x0, the solution to this ODE, denoted by xt=Φθ\nt(x0), is a trajectory\n(or flow) evolving from x0. The aim is to train vθsuch that the x1=Φθ\n1(x0)matches pdata.\nFlow matching techniques [ 41,42] train vθby minimizing its difference from a target velocity field.\nThis target field is often defined by constructing a probability path pt(x)that interpolates between pinit\nandpdata. A common choice is a conditional path xt(x0,x1)defined for pairs (x0,x1)sampled from\npinit×pdata. For instance, Rectified Flow uses a linear interpolation: xt(x0,x1) = (1 −t)x0+tx1.\nThe target velocity field corresponding to this path is ut(xt|x0,x1) =x1−x0. The neural network\nvθis then trained to predict this target field by minimizing the flow matching loss:\nLFM(θ) = E\nt∼U[0,1],x0∼pinit,x1∼pdata\u0002\n∥vθ((1−t)x0+tx1, t)−(x1−x0)∥2\u0003\n(3)\nIn this paper, we consider vθto be pre-trained, fixed, and optimal. That is, vθis assumed to have\nperfectly minimized Eq. (3), such that vθ((1−t)x0+tx1, t) =x1−x0for allx0∼pinit,x1∼pdata,\nandt∈[0,1]. Such a vθcan serve as a regularizer to align latents to the target distribution.\n3.2 Likelihood Estimation with Flow Priors\nLetpvθ\n1(x1)denote the probability density at t= 1 induced by the flow model vθevolving from\npinit. Using the instantaneous change of variables formula, the log-likelihood of a sample x1under\nthis model can be computed by [6, 20]:\nlogpvθ\n1(x1) = log pinit(x0)−Z1\n0Tr(∇xvθ(xs, s))ds (4)\n3\n--- Page 4 ---\nHere,xs=Φθ\ns(x0)is the trajectory generated by vθstarting from x0and ending at x1=Φθ\n1(x0).\nThus,x0= (Φθ\n1)−1(x1)is obtained by flowing x1backward in time to t= 0. Given a pre-trained\nflow model vθthat maps pinit(e.g., Gaussian noise) to a target distribution (e.g., target features),\none can align new input samples with these target features by maximizing their log-likelihood under\npvθ\n1. However, computing Eq. (4)is often computationally expensive, primarily due to the trace of\nthe Jacobian term ( Tr(∇xvθ)) and the need for an ODE solver. In this paper, we demonstrate that\na similar alignment objective can be achieved by minimizing the flow matching loss Eq. (3)with\nrespect to x1, treating x1as a variable to be optimized rather than a fixed sample from pdata.\n4 Method\nIn this paper, we aim to align a learnable latent space, whose latents are denoted by y, to a target\ndistribution pdata. We first describe the overall pipeline in Sec. 4.1. Our method leverages a pre-\ntrained flow model to implicitly capture pdataand subsequently align the latents y. Then, we provide\nan intuitive explanation in Sec. 4.2 and a formal proof of the proposed method in Sec. 4.3.\n4.1 Pipeline\nLety∈Rd1denote a sample from a learnable latent space. These latents yare typically produced\nby a parametric model Gϕ(e.g., the encoder of an AE), whose parameters ϕwe aim to train. Let\nx∈Rd2be a sample from the target feature space, characterized by an underlying distribution\nx∼pdata(x). Our objective is to train Gϕsuch that the distribution of its outputs, pϕ(y), aligns\ntopdata(x). This alignment can be formulated as maximizing the likelihood of yunder pdata. For\ninstance, in an AE setting where we wish the latent space (from which yis sampled) to conform\nto the distribution of features from a pre-trained feature extractor (from which xis sampled), our\nmethod facilitates this alignment.\nAddressing the Dimension Mismatch A challenge arises if the latent space dimension d1differs\nfrom the target feature space dimension d2. To address this, we employ fixed (non-learnable) linear\nprojections to map target features xfromRd2toRd1. We still keep the notation for the projected\nfeatures and their distribution as xandpdatarespectively for simplicity. We consider three alternative\nprojection operators: Random Projection ,Average Pooling , and PCA . We ablate these methods in\nSec. 5.3 and select random projection as the default due to its simplicity and empirical effectiveness.\nThe use of linear projection is theoretically supported by the Johnson-Lindenstrauss (JL) lemma [ 26].\nThe JL lemma states that for a set of Npoints in Rd2, a random linear mapping can preserve all\npairwise Euclidean distances within a multiplicative distortion factor. The linear projection is defined\nby a matrix W∈Rd1×d2. Assuming the target features xare appropriately normalized, we initialize\nthe projection matrix Wby sampling its entries from N(0,1/d2). This scaling helps ensure that\nthe components of xalso have approximately unit variance if the components of xare uncorrelated,\nthereby preserving key statistical properties.\nFlow Prior Estimation With the projected target features x∼pdata, we first train a flow model\nvθ:Rd1×[0,1]→Rd1, parameterized by θ. This model is trained using the flow matching objective\nEq.(3), where the dimension dis set to d1,x0∼ N(0,I), andx1is replaced by samples xfrom\npdata. After training, the parameters θof the flow model vθare frozen. This fixed vθimplicitly\ndefines a generative process capable of transforming samples from pinit(now in Rd1) into samples\nthat approximate pdata. It captures the underlying target distribution and serves as a distributional\nprior for aligning the latent space.\nLatent Space Regularization Oncevθis trained and its parameters fixed, we use it to regularize the\nlearnable latents y. The goal is to encourage the distribution pϕ(y)to conform to pdataas captured\nbyvθ. For each yproduced by Gϕ, we incorporate the flow matching objective described in Eq. (3)\ninto the training objective of Gϕ:\nLalign(y;θ) = E\nt∼U[0,1],x0∼pinit(x0)\u0002\n∥vθ((1−t)x0+ty, t)−(y−x0)∥2\u0003\n(5)\nHere, pinitis the same d1-dimensional base distribution N(0,I)used for training vθ. In Sec. 4.3, we\nformally prove that minimizing Eq. (5)with respect to yserves as a proxy to maximizing a lower\n4\n--- Page 5 ---\nx1x2\npinitx0\npdataygood\nztygood−x0\n(a) “Good” casex1x2\npinitx0\npdata\nybad zt\nybad−x0vθ(zt, t)\n(b) “Bad” case\nFigure 2: Intuitive illustration of latent space alignment via flow matching, best viewed in color. (a)\nA “good” ygoodinpdata(green) aligns the straight path velocity (red solid arrow) with the pre-trained\nflow model’s velocity vθ(zt, t)(overlapped and omitted), yielding low loss. (b) A “bad” ybadoutside\npdatacauses a mismatch between the path velocity and vθ(zt, t)(green solid arrow), resulting in\nhigh loss. Minimizing this loss steers ybadtopdata(blud dotted arrow).\nbound on the log-likelihood logpvθ\n1(y). This establishes that minimizing Lalign(y;θ)effectively\ntrains Gϕsuch that its outputs yalign with the distribution of the target features x.\nThe key insight is that the pre-trained velocity field vθencapsulates the dynamics that transport\nprobability mass from the base distribution pinitto the target distribution pdataalong linear paths. By\nminimizing Lalign(y;θ), we penalize latents ythat do not conform to these learned dynamics—that\nis,yvalues for which the path (1−t)x0+tyis not \"natural\" under vθ. This procedure shapes pϕ(y)\nto match pdatawithout requiring explicit computation of potentially intractable likelihoods, relying\ninstead on the computationally efficient flow matching objective.\n4.2 Intuitive Explanation\nOur alignment method leverages the pre-trained flow model, vθ, as an expert on the target feature\ndistribution pdata. Having been well trained, vθprecisely captures the dynamics required to transform\ninitial noise samples x0into target features xalong straight interpolation paths. Specifically, it has\nlearned to predict the exact velocity x−x0at any point (1−t)x0+txalong such a path. This\neffectively means vθcan validate whether a given trajectory from noise is characteristic of those\nleading to the true target distribution.\nWe utilize this knowledge to shape the distribution of our learnable latents y. The alignment loss,\nLalign(y;θ), challenges vθ: for a given yand a random x0, it asks whether the velocity field predicted\nbyvθalong the straight path (1−t)x0+tymatches the path’s inherent velocity, y−x0. Ifyis\nstatistically similar to samples from pdata, this match will be close, resulting in a low loss. Conversely,\na significant mismatch indicates that yis not a plausible target according to the learned dynamics,\nyielding a high loss. By minimizing this loss (by optimizing the generator Gϕthat produces y),\nwe iteratively guide ytowards regions where its connecting path from noise is endorsed by vθ.\nAs depicted in Fig. 2, this process progressively aligns the distribution of y(blue) with the target\ndistribution pdata(orange), achieving distributional conformity.\n4.3 Relating the Alignment Loss to an ELBO on Log-Likelihood\nIn this section, we demonstrate that minimizing the alignment loss Lalign(y;θ)(Eq.(5)) with respect to\na given y∈Rd1corresponds to maximizing a variational lower bound (ELBO) on the log-likelihood\nlogpvθ\n1(y). Here, pvθ\n1(y)denotes the probability density at t= 1 induced by the ODE dynamics\ndzt\ndt=vθ(zt, t), withz0∼pinit.\nProposition 1. Letvθ:Rd1×[0,1]→Rd1be a given velocity field, and pinitbe a base distribution.\nFory∈Rd1, the log-likelihood logpvθ\n1(y)is lower-bounded as:\nlogpvθ\n1(y)≥C(y)−λLalign(y;θ), (6)\nwhere λ >0is a constant, Lalign(y;θ)is defined in Eq. (5), and C(y)is dependent on yandvθ.\nProof. We establish this result by constructing a specific variational lower bound on logpvθ\n1(y).\nVariational lower bounds for log-likelihoods in continuous-time generative models can be constructed\n5\n--- Page 6 ---\nby introducing a proposal distribution for the latents that could generate y. Consider a family of\n\"proposal\" paths [ 41], which are straight lines interpolating from an initial point x0∼pinitto the\ngiven point y:\nzs(x0,y) = (1 −s)x0+sy, s∈[0,1] (7)\nThe velocity of such a path is constant: ˙zs(x0,y) =y−x0. We adopt a variational distribution over\nthe initial states x0, conditioned on y, asq(x0|y) =pinit(x0). That is, we consider initial states\ndrawn from the prior, irrespective of yfor the functional form of q.\nA known variational lower bound on logpvθ\n1(y)s [6, 20, 42] can be written a:\nlogpvθ\n1(y)≥Ex0∼q(·|y)\"\nlogpinit(x0)−Z1\n0\u0000\nλs∥˙zs(x0,y)−vθ(zs(x0,y), s)∥2\u0001\nds\n−logq(x0|y)−Z1\n0(Tr(∇zsvθ(zs(x0,y), s))) ds#\n(8)\nHere, λs>0is a time-dependent weighting factor. For simplicity and consistency with the definition\nofLalign(Eq. (5)), we set λs=λ= 1 for all s∈[0,1]. With q(x0|y) =pinit(x0), the term\nlogpinit(x0)−logq(x0|y)vanishes. Substituting the expressions for zs(x0,y)from Eq. (7)and its\nvelocity ˙zs(x0,y) =y−x0:\nlogpvθ\n1(y)≥ −Ex0∼pinit\u0014Z1\n0Tr(∇zvθ((1−s)x0+sy, s))ds\u0015\n−Ex0∼pinit\u0014Z1\n0∥(y−x0)−vθ((1−s)x0+sy, s)∥2ds\u0015\n(9)\nThe second term in this inequality matches the definition of Lalign(y;θ)(Eq. (5)). Let us define the\nfirst term of the ELBO’s right-hand side. To maintain consistency with the expectation over time in\nLalign, we can write:\nC(y) =−Es∼U[0,1],x0∼pinit[Tr(∇zvθ((1−s)x0+sy, s))] (10)\nSo, the ELBO (Eq. (9)) can be expressed as:\nlogpvθ\n1(y)≥C(y)− L align(y;θ) (11)\nThis concludes the derivation of the lower bound as stated in the proposition (with λ= 1).\nInterpretation and Significance The inequality (11) demonstrates that maximizing the derived\nlower bound with respect to yinvolves two parts: maximizing C(y)and minimizing Lalign(y;θ).\nThe term Lalign(y;θ)directly measures how well the velocity field vθpredicts the velocity of that\nstraight path, i.e., y−x0. Minimizing this term forces yinto regions where it behaves like a point\nreachable from pinitvia a straight path whose dynamics are consistent with the learned vθ. This is\nprecisely the behavior expected if ywere a sample from the distribution pdata.\nThe term C(y)represents the expected negative trace of the Jacobian of vθ, averaged over the chosen\nstraight variational paths. By minimizing Lalign(y;θ), we are not strictly maximizing the ELBO in\nEq.(11) with respect to y. Instead, we are optimizing a crucial component of it that directly enforces\nconsistency with the learned flow dynamics. We analyze the behavior of C(y)in Appendix A to\nshow that if yaligns with a more concentrated target distribution (making Lalign(y;θ)small), C(y)\ntends to be positive and larger, contributing favorably to the ELBO.\nAssumption 1 (Optimality of vθ).The velocity field vθ:Rd1×[0,1]→Rd1is (pre-trained) and\noptimal, satisfying vθ((1−t)x0+tx1, t) =x1−x0∀x0∼pinit,x1∼pdata, t∈[0,1].\nTo further interpret the method, we consider the Assumption 1 that vθis optimally trained such\nthatvθ((1−s)x0+sx1, s) =x1−x0forx1∼pdata. Ifyis itself a sample from pdata, then\nLalign(y;θ)would be (close to) zero. However, when optimizing an arbitrary y, especially if it is\nfar from pdata, theLalign(y;θ)term can be substantial. Its minimization drives ytowards regions of\nhigher plausibility under the learned flow.\nIn practice, directly maximizing logpvθ\n1(y)via Eq. (4)is computationally demanding, requiring ODE\nsolves and computation of Jacobian traces along these true ODE paths. Maximizing the full ELBO\n6\n--- Page 7 ---\n(a)\n (b)\n (c)\nFigure 3: Illustration with a Mixture of Gaussians distribution. (a) Aligned latent variables y(red\ntriangles) concentrate in low negative log-likelihood (NLL) regions of pdata(blue dots; heatmap shows\n−logpdata). (b) Alignment loss Lalignheatmap mirrors the NLL landscape of pdata, with pdatasamples\nin low- Lalignareas. (c) Lalign(blue solid) and −logpdata(y)(red dashed) decline simultaneously in\ntraining, showing Lalignserves as a proxy for maximizing the log-likelihood of yunder pdata.\n(Eq. (6)) would still require computing C(y), which involves trace computations. By focusing on\nminimizing only Lalign(y;θ), we adopt a computationally tractable proxy. This objective encourages\nyto have a high likelihood under pvθ\n1(y)by ensuring consistency with the learned flow dynamics,\nthereby aligning the distribution of ywith the target distribution pdataimplicitly modeled by vθ. A\nmore complete proof can be found in Appendix A.\n5 Experiments\nThis section presents an empirical validation of the proposed alignment method with flow priors. The\ninvestigation starts with an illustrative experiment in Sec. 5.1. Subsequently, large-scale experiments\nare conducted on image generation tasks using the ImageNet dataset, as detailed in Sec. 5.2. In\nSec. 5.3, we conduct ablation studies of the proposed method.\n5.1 Toy Examples\nWe present a toy example as an illustrative experiment in a 2D setting. The target distribution, denoted\npdata, is configured as a mixture of five isotropic Gaussians. Following the methodology outlined in\nSec. 4.1, we first train a flow model vθto map a standard 2D Normal distribution N(0,I)topdata.\nThis flow model vθis implemented by a multi-layer perceptron (MLP) incorporating adaptive layer\nnormalization for time modulation [ 49]. Upon completion of training, the parameters θof this flow\nmodel are frozen. Subsequently, instead of a parameterized model Gϕ, we directly initialize a set of\nlearnable 2D variables as yand optimize them by minimizing the alignment loss Lalign(y;θ).\nThe results are presented in Fig. 3. Fig. 3 (a) compares the target distribution pdata(blue point samples)\nwith the optimized variables y(red triangles). The background visualizes the negative log-likelihood\n(NLL) of pdata, which is computed analytically. It is evident that ysuccessfully converges to the\nhigh log-likelihood regions of pdata. Fig. 3 (b) displays the landscape of the alignment loss Lalign,\nwhich is estimated numerically with vθ. The landscape mirrors the NLL surface of pdatadepicted in\n(a). Samples drawn from pdata(blue dots) are concentrated in regions where Lalignis low, suggesting\nthatLaligneffectively captures the underlying structure of the target distribution. Fig. 3 (c) illustrates\nLalign(blue solid line) and the true NLL −logpdata(y)(red dashed line) during the training of y.\nThe alignment loss and the NLL exhibit a strong positive correlation, decreasing concomitantly\nthroughout the training process. More detailed toy examples can be found in Appendix B.\n5.2 Image Generation\nPrior work has demonstrated that aligning the latent space of AEs with semantic encoders can enhance\ngenerative model performance [ 4,63,5,50]. To validate this observation and further showcase the\nefficacy of our proposed method, we conduct large-scale image generation experiments on the\nImageNet-1K [11] dataset at 256×256resolution.\nImplementation Details Our AE architecture employs two Vision Transformer (ViT)-Large [ 15]\nmodels, each with 391M parameters, serving as the latent encoder and decoder, respectively. The\n7\n--- Page 8 ---\nV AE (Low-level)\n DinoV2 (Semantic)\n VQ (Discrete)\n Qwen (Textual)\nFigure 4: Aligning autoencoders on ImageNet-1K with different target distributions. The alignment\nlossLalign(blue solid) and the k-NN distance logrk(y)(red dashed) are proportional throughout the\ntraining. Confirming that Lalignserves as a good proxy for the NLL of the latents under pdata.\nencoder maps input images to a latent space of 64 tokens, each with dimension 32, striking a balance\nbetween reconstruction quality and computational efficiency. We impose token-level alignment on\nthe latents. The alignment loss on the latents is set to λ= 0.01by default. We also incorporate\nconventional reconstruction loss, perceptual loss, and adversarial loss on the pixel outputs [18, 52].\nFor the target distribution pdata, we investigate four distinct variants: low-level visual features from\na V AE [ 33,18], continuous semantic visual features from DinoV2 [ 48],discrete visual codebook\nembeddings from LlamaGen VQ [ 58,61], and textual embeddings from Qwen [ 2]. Their feature\ndimensions are 32, 768, 8, 896, respectively. The flow-based prior is modeled by a 6-layer MLP with\n1024 hidden units, trained for 1 million steps using the AdamW [ 43] optimizer to match Assumption 1.\nMore details can be found in Appendix C.\nAlignment Results Analogous to the toy example, we aim to correlate the alignment loss Lalignwith\nthe NLL of latents under the target distribution pdata. Since the NLL is intractable for implicitly defined\ndistributions, we estimate the density using k-nearest neighbors. The probability density p(y)at a\npointyis inversely proportional to the volume of the hypersphere enclosing its kthnearest neighbor\namong the target samples. Consequently, the NLL can be estimated as −logpdata(y)∝logrk(y)\nwhere rk(y)is the Euclidean distance to the kthneighbor, and Dis the dimension. We use logrk(y)\nas our proxy measure for the NLL. We first index the set of target distribution samples using\nFaiss [ 16]. During the training, we periodically sample 10k points from the latent space and measure\nthe alignment quality by averaging the logrk(y).\nThe results are presented in Fig. 4. A strong correlation is observed between the alignment loss\nLalignand the k-NN distance proxy logrk(y). The only unstable case is the VQ variant, for which\nthe GAN loss collapses during training due to its low dimension (8-dim), yet the general trend is\nstill consistent. This finding corroborates our conclusion that Lalignserves as an effective proxy for\nthe NLL of the latents under pdata. Crucially, our method captures the underlying structure across\ndiverse target distributions, spanning different forms (continuous, discrete) and modalities (visual,\ntextual), even when applied to a large-scale dataset like ImageNet and a high-capacity model such as\nViT-Large.\nGeneration Results After demonstrating effective latent space alignment, we investigated its impact\non generative model performance. We evaluated both reconstruction and generation capabilities\non ImageNet using the MAR-B [ 37] architecture. For MAR-B, we incorporated qk-norm [ 10] and\nreplaced the diffusion head with a flow head to ensure stable training. We choose flow-based MAR-B\nas it does not favor continuous Gaussian-like latent structure like Diffusion models [ 57,12,27,47,52]\ndo. To ensure an “apple-to-apple” comparison, configurations and hardware remained identical across\nall experiments, with the only difference being the specific AE used for each alignment variant.\nThe results are presented in Tab. 1. Reconstruction performance was measured by rFID [ 22] and\nPSNR on the ImageNet validation set. Generation performance was assessed using FID, IS [ 54],\nPrecision, and Recall on 50k generated samples and the validation set, both with and without\nclassifier-free guidance (CFG) [25]. Our key findings are:\n1) Alignment vs. Reconstruction Trade-off: Latent space alignment typically degrades reconstruction\nquality (rFID, PSNR) compared to vanilla AEs, as constraints reduce capacity. SoftVQ[ 5] excels\namong aligned methods due to its sample-level alignment. 2) Alignment Enhances Generation:\nStructured latent spaces improve generative metrics (FID, IS), but complexity is not decisive. Simple\nfeatures (text embeddings like Qwen) may match the performance of richer visual features (DinoV2).\n8\n--- Page 9 ---\nTable 1: ImageNet 256×256conditional generation using MAR-B. All models are trained and\nevaluated using identical settings. The CFG scale is tuned for KL and kept the same for others.\nAutoencoder rFID ↓PSNR ↑w/o CFG w/ CFG\nFID↓ IS↑ Pre.↑Rec.↑FID↓ IS↑ Pre.↑Rec.↑\nAE 1.13 20.20 15.08 86.37 0.60 0.59 5.26 237.60 0.56 0.65\nKL 1.65 22.59 12.94 91.86 0.60 0.58 5.29 200.85 0.57 0.65\nSoftVQ 0.61 23.00 13.30 93.40 0.60 0.57 6.09 198.53 0.58 0.61\nLow-level (V AE) 1.22 22.31 12.04 98.66 0.56 0.57 5.02 240.03 0.56 0.62\nSemantic (Dino) 1.26 23.07 11.47 101.74 0.59 0.59 4.87 250.38 0.54 0.67\nDiscrete (VQ) 2.99 22.32 24.63 48.17 0.55 0.53 10.04 119.64 0.47 0.65\nTextual (Qwen) 0.85 23.12 11.89 102.23 0.55 0.57 6.56 262.89 0.49 0.69\nTable 2: Ablation studies on ImageNet 256×256for different configurations using autencoders\nregularized by textual features (Qwen). We use a shorter training schedule when ablating weight.\n(a) Downsampling Methods\nMethod rFID ↓PSNR ↑ FID↓ IS↑\nRandom Proj. 0.85 23.12 11.89 102.23\nAvg. Pooling 0.94 22.98 16.06 60.37\nPCA 0.87 23.14 14.95 83.59(b) Alignment Loss Weight\nWeight λ rFID ↓PSNR ↑ FID↓ IS↑\n0.001 0.89 22.78 17.57 75.20\n0.005 1.02 22.98 16.93 78.01\n0.01 1.31 23.12 13.67 82.13\n0.05 1.81 21.82 12.30 92.48\n3) Optimal prior selection is open: No consensus exists on optimal priors. Low-dimensional discrete\nfeatures (LlamaGen VQ) underperform, while cross-modal alignment (Qwen text embeddings)\ndemonstrates transferable structural benefits. More discussions can be found in Appendix D.\n5.3 Ablation Study\nDownsampling Operators We ablate the downsampling operators in Tab. 2 (a). We adopt the same\nsettings as in Tab. 1 using the model with the textual embeddings (Qwen) as the target distribution.\nDespite all being linear downsampling operators, PCA and Avg. Pooling perform worse than Random\nProjection. We hypothesize that this is because unlike Random Projection which preserves the\nstructure of the data, both PCA and Avg. Pooling are likely to destroy the structure. Avg. Pooling\nperforms especially poorly since it merges close features that are independent from the location.\nAlignment Loss Weight We apply different strengths of regularization by altering the alignment\nloss weight λin Tab. 2 (b). As expected, larger weight implies heavier regularization, worse recon-\nstruction, and easier generation. However, heavier regularization limits the generation performance\nand may even cause the GAN loss to collapse. A trade-off exists between reconstruction and\ngeneration when the capacity of the model is limited.\n6 Conclusion\nThis paper introduced a novel method for aligning learnable latent spaces with arbitrary target distri-\nbutions by leveraging pre-trained flow-based generative models as expressive priors. Our approach\nutilizes a computationally tractable alignment loss, adapted from the flow matching objective, to\nguide latent variables towards the target distribution. We theoretically established that minimizing\nthis alignment loss serves as a proxy for maximizing a variational lower bound on the log-likelihood\nof the latents under the flow-defined target. The effectiveness of our method is validated through\nempirical results, including controlled toy settings and large-scale ImageNet experiments. Ultimately,\nthis work provides a flexible and powerful framework for incorporating rich distributional priors,\npaving the way for more structured and interpretable representation learning. A limitation , and also\na promising future direction, is that the selection of the optimal prior remains a challenge. While\nsemantic priors are effective for image generation, we posit that no single “silver bullet” prior exists\nfor all tasks; rather, the optimal choice is likely task-specific and need be further explored.\n9\n--- Page 10 ---\nReferences\n[1]Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic\ninterpolants. ArXiv , abs/2209.15571, 2022.\n[2]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n[3]Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of\nICML workshop on unsupervised and transfer learning , pages 37–49. JMLR Workshop and\nConference Proceedings, 2012.\n[4]Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng\nLiu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion\nmodels. arXiv preprint arXiv:2502.03444 , 2025.\n[5]Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha\nRaj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer.\narXiv preprint arXiv:2412.10958 , 2024.\n[6]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. Advances in neural information processing systems , 31, 2018.\n[7]Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao,\nHui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models.\narXiv preprint arXiv:2502.04896 , 2025.\n[8]Shuangshuang Chen and Wei Guo. Auto-encoders in deep learning—a review with new\nperspectives. Mathematics , 11(8):1777, 2023.\n[9]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and P. Abbeel. Infogan:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nNeural Information Processing Systems , 2016.\n[10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin\nGilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.\nScaling vision transformers to 22 billion parameters. In International Conference on Machine\nLearning , pages 7480–7512. PMLR, 2023.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition , pages 248–255. Ieee, 2009.\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems , 34:8780–8794, 2021.\n[13] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXiv: Learning , 2014.\n[14] Laurent Dinh, Jascha Narain Sohl-Dickstein, and Samy Bengio. Density estimation using real\nnvp. ArXiv , abs/1605.08803, 2016.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. ArXiv , abs/2010.11929, 2020.\n[16] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-\nEmmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. 2024.\n[17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first international conference on machine\nlearning , 2024.\n10\n--- Page 11 ---\n[18] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution\nimage synthesis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , pages 12868–12878, 2020.\n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM , 63(11):139–144, 2020.\n[20] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.\nFfjord: Free-form continuous dynamics for scalable reversible generative models. arXiv\npreprint arXiv:1810.01367 , 2018.\n[21] Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative α-(de) blending: A minimalist\ndeterministic diffusion model. In ACM SIGGRAPH 2023 Conference Proceedings , pages 1–8,\n2023.\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Neural\nInformation Processing Systems , 2017.\n[23] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M.\nBotvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con-\ncepts with a constrained variational framework. In International Conference on Learning\nRepresentations , 2016.\n[24] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with\nneural networks. science , 313(5786):504–507, 2006.\n[25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598 , 2022.\n[26] William B Johnson, Joram Lindenstrauss, et al. Extensions of lipschitz mappings into a hilbert\nspace. Contemporary mathematics , 26(189-206):1, 1984.\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. ArXiv , abs/2206.00364, 2022.\n[28] Ilyes Khemakhem, Diederik P. Kingma, and Aapo Hyvärinen. Variational autoencoders and\nnonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and\nStatistics , 2019.\n[29] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-\nChieh Chen. Democratizing text-to-image masked generative models with compact text-aware\none-dimensional tokens. arXiv preprint arXiv:2501.07730 , 2025.\n[30] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on\nMachine Learning , 2018.\n[31] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolu-\ntions. ArXiv , abs/1807.03039, 2018.\n[32] Diederik P. Kingma, Tim Salimans, and Max Welling. Improved variational inference with\ninverse autoregressive flow. ArXiv , abs/1606.04934, 2016.\n[33] Diederik P Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013.\n[34] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n[35] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng.\nRepa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint\narXiv:2504.10483 , 2025.\n[36] Pengzhi Li, Yan Pei, and Jianqiang Li. A comprehensive survey on design and application of\nautoencoder in deep learning. Applied Soft Computing , 138:110176, 2023.\n11\n--- Page 12 ---\n[37] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image\ngeneration without vector quantization. Advances in Neural Information Processing Systems ,\n37:56424–56445, 2024.\n[38] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder:\nAutoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756 , 2024.\n[39] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Jindong Wang, Zhe Lin, and Bhiksha\nRaj. Xq-gan: An open-source image tokenization framework for autoregressive generation.\narXiv preprint arXiv:2412.01762 , 2024.\n[40] Cheng-Yuan Liou, Wei-Chen Cheng, Jiun-Wei Liou, and Daw-Ran Liou. Autoencoder for\nwords. Neurocomputing , 139:84–96, 2014.\n[41] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow\nmatching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.\n[42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate\nand transfer data with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\n[44] Qinxue Meng, Daniel Catchpoole, David Skillicom, and Paul J Kennedy. Relational autoencoder\nfor feature extraction. In 2017 International joint conference on neural networks (IJCNN) ,\npages 364–371. IEEE, 2017.\n[45] Ibomoiye Domor Mienye and Theo G Swart. Deep autoencoder neural networks: A compre-\nhensive review and new perspectives. Archives of computational methods in engineering , pages\n1–20, 2025.\n[46] Kirill Neklyudov, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. Action matching:\nLearning stochastic dynamics from samples. In International conference on machine learning ,\npages 25858–25889. PMLR, 2023.\n[47] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. ArXiv ,\nabs/2102.09672, 2021.\n[48] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv preprint arXiv:2304.07193 , 2023.\n[49] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023\nIEEE/CVF International Conference on Computer Vision (ICCV) , pages 4172–4182, 2022.\n[50] Kai Qiu, Xiang Li, Jason Kuen, Hao Chen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj,\nZhe Lin, and Marios Savvides. Robust latent matters: Boosting image generation with sampling\nerror synthesis. arXiv preprint arXiv:2503.08354 , 2025.\n[51] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations. J. Comput. Phys. , 378:686–707, 2019.\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 10684–10695, 2022.\n[53] Rifai Salah, P Vincent, X Muller, X Gloro, and Y Bengio. Contractive auto-encoders: Explicit\ninvariance during feature extraction. In Proc. of the 28th International Conference on Machine\nLearning , pages 833–840, 2011.\n[54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems , 29,\n2016.\n12\n--- Page 13 ---\n[55] Inkyu Shin, Chenglin Yang, and Liang-Chieh Chen. Deeply supervised flow-based generative\nmodels. arXiv preprint arXiv:2503.14494 , 2025.\n[56] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther.\nLadder variational autoencoders. In Neural Information Processing Systems , 2016.\n[57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502 , 2020.\n[58] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.\nAutoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint\narXiv:2406.06525 , 2024.\n[59] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-\nBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative\nmodels with minibatch optimal transport. arXiv preprint arXiv:2302.00482 , 2023.\n[60] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. ArXiv ,\nabs/2007.03898, 2020.\n[61] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems , 30, 2017.\n[62] Xin Wen, Bingchen Zhao, Ismail Elezi, Jiankang Deng, and Xiaojuan Qi. \" principal compo-\nnents\" enable a new language of images. arXiv preprint arXiv:2503.08685 , 2025.\n[63] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimiza-\ntion dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423 , 2025.\n[64] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin,\nand Saining Xie. Representation alignment for generation: Training diffusion transformers is\neasier than you think. arXiv preprint arXiv:2410.06940 , 2024.\n[65] Kaiwen Zha, Lijun Yu, Alireza Fathi, David A Ross, Cordelia Schmid, Dina Katabi, and Xiuye\nGu. Language-guided image tokenization for generation. arXiv preprint arXiv:2412.05796 ,\n2024.\n[66] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie\nZheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Joshua M. Susskind.\nNormalizing flows are capable generative models. ArXiv , abs/2412.06329, 2024.\n[67] Yasi Zhang, Peiyu Yu, Yaxuan Zhu, Yingshan Chang, Feng Gao, Yingnian Wu, and Oscar\nLeong. Flow priors for linear inverse problems via iterative corrupted trajectory matching.\nArXiv , abs/2405.18816, 2024.\n[68] Wenliang Zhao, Minglei Shi, Xumin Yu, Jie Zhou, and Jiwen Lu. Flowturbo: Towards real-time\nflow-based image generation with velocity refiner. ArXiv , abs/2409.18128, 2024.\n13\n--- Page 14 ---\nA Complete Proof\nA.1 Complete Proof for Proposition 1\nWe restate Proposition 1 for clarity and self-containedness.\nProposition 1. Letvθ:Rd1×[0,1]→Rd1be a given velocity field, and pinitbe a base distribution.\nFor any y∈Rd1, the log-likelihood logpvθ\n1(y)ofyunder the distribution induced by flowing pinit\nwithvθfromt= 0tot= 1, is lower-bounded as:\nlogpvθ\n1(y)≥C(y)−λLalign(y;θ), (12)\nwhere λ >0is a constant, Lalign(y;θ)is defined as\nLalign(y;θ) = E\ns∼U[0,1],x0∼pinit(x0)\u0002\n∥(y−x0)−vθ((1−s)x0+sy, s)∥2\u0003\n, (13)\nandC(y)is a term dependent on yandvθ, given by\nC(y) =−Es∼U[0,1],x0∼pinit[Tr(∇zvθ((1−s)x0+sy, s))]. (14)\nTo prove this, we make the following assumptions:\nAssumption 2 (Properties of the Velocity Field) .The velocity field vθ:Rd1×[0,1]→Rd1is\ncontinuous in tand Lipschitz continuous in its spatial argument with continuously differentiable\ncomponents, ensuring that ∇zvθ(z, s)exists and is bounded on compact sets.\nAssumption 3 (Variational Path Choice) .The variational paths used to construct the ELBO are\nstraight lines zs(x0,y) = (1 −s)x0+syfors∈[0,1], originating from x0∼pinitand terminating\naty. The velocity of such a path is ˙zs(x0,y) =y−x0.\nAssumption 4 (Variational Distribution Choice) .The variational distribution over the initial states\nx0conditioned on yis chosen as q(x0|y) =pinit(x0).\nAssumption 5 (Weighting Factor in ELBO) .The time-dependent weighting factor λsin the general\nELBO formulation (Eq. 15 below) is chosen as a positive constant λs=λ >0for all s∈[0,1].\nRemark 1 (Optimality of vθ).Sec. 4.3 introduces Assumption 1, which states that vθis optimally\npre-trained such that vθ((1−s)x0+sx1, s) =x1−x0forx1∼pdata. This assumption is not\nrequired for the mathematical validity of the ELBO in Proposition 1 itself, which holds for any\nvθsatisfying Assumption 2. However, Assumption 1 is crucial for interpreting why minimizing\nLalign(y;θ)drives ytowards pdata, as it implies Lalign(y;θ)≈0ify∼pdata.\nProof. The proof is based on a variational approach to lower-bound the log-likelihood in continuous-\ntime generative models. This technique has been established in the literature for Neural ODEs and\ncontinuous normalizing flows [6, 20, 42].\nFor a target point y, we consider a family of paths zs(x0,y)parameterized by initial states x0drawn\nfrom a proposal distribution q(x0|y), where each path starts at x0and ends at y(i.e.,z0(x0,y) =x0\nandz1(x0,y) =y). The variational lower bound is derived by considering the path integral\nformulation of the likelihood. For any such family of paths with velocities ˙zs(x0,y), the bound takes\nthe form:\nlogpvθ\n1(y)≥Ex0∼q(·|y)\"\nlogpinit(x0)−logq(x0|y)\n−Z1\n0λs∥˙zs(x0,y)−vθ(zs(x0,y), s)∥2ds\n−Z1\n0Tr(∇zsvθ(zs(x0,y), s))ds#\n. (15)\nWe now apply our specific assumptions:\n•By Assumption 3, the paths are zs(x0,y) = (1 −s)x0+sy, and their velocities are\n˙zs(x0,y) =y−x0.\n14\n--- Page 15 ---\n•By Assumption 4, q(x0|y) =pinit(x0). This causes the term logpinit(x0)−logq(x0|y)\nto vanish.\n• By Assumption 5, we set λs=λ, a positive constant for all s∈[0,1].\nSubstituting these into Eq. 15:\nlogpvθ\n1(y)≥Ex0∼pinit\"\n−Z1\n0λ∥(y−x0)−vθ((1−s)x0+sy, s)∥2ds\n−Z1\n0Tr(∇zvθ((1−s)x0+sy, s))ds#\n. (16)\nUsing the equivalenceR1\n0f(s)ds=Es∼U[0,1][f(s)]for integrable functions f, we can rewrite each\nterm. The first term becomes:\nEx0∼pinit\u0014\n−λZ1\n0∥(y−x0)−vθ((1−s)x0+sy, s)∥2ds\u0015\n=−λLalign(y;θ),\nusing the definition of Lalign(y;θ)from Eq. 13. The second term becomes:\nEx0∼pinit\u0014\n−Z1\n0Tr(∇zvθ((1−s)x0+sy, s))ds\u0015\n=C(y),\nusing the definition of C(y)from Eq. 14. Combining these, the ELBO becomes:\nlogpvθ\n1(y)≥C(y)−λLalign(y;θ). (17)\nThis completes the proof of Proposition 1. Our paper uses λ= 1for simplicity, yielding the bound\nC(y)− L align(y;θ).\nA.2 Rigorous Analysis of C(y)\nWe now provide a rigorous analysis of the term C(y)in the ELBO and establish conditions under\nwhich minimizing Lalign(y;θ)leads to favorable behavior of C(y).\nA.2.1 Geometric Interpretation of C(y)\nThe term C(y)represents the negative expected divergence of the velocity field vθalong straight-line\nvariational paths from x0∼pinittoy:\nC(y) =−Es∼U[0,1],x0∼pinit[Tr(∇zvθ((1−s)x0+sy, s))]. (18)\nTo understand its role, recall that in the exact likelihood computation for a flow model, we have:\nlogpvθ\n1(y) = log pinit(x∗\n0(y))−Z1\n0Tr(∇xvθ(x∗\ns(y), s))ds, (19)\nwhere x∗\ns(y)is the unique ODE trajectory satisfying ˙x∗\ns=vθ(x∗\ns, s)withx∗\n1(y) =y. The\ndivergence integral measures the logarithmic volume change induced by the flow.\nOur variational bound approximates this exact computation by averaging over straight-line paths\nrather than the true ODE trajectory. The quality of this approximation depends on how well the\nstraight paths approximate the true flow geometry.\nA.2.2 Relationship Between C(y)and Distributional Alignment\nWe establish the key relationship between C(y)and the alignment quality measured by Lalign(y;θ).\nLemma 1 (Consistency of Variational Paths) .Under Assumption 1 (optimal vθ), fory∼pdata, the\nstraight-line variational paths zs= (1−s)x0+sysatisfy:\nEx0∼pinit\u0002\n∥(y−x0)−vθ(zs, s)∥2\u0003\n= 0∀s∈[0,1]. (20)\nConsequently, Lalign(y;θ) = 0 wheny∼pdata.\n15\n--- Page 16 ---\nProof. By Assumption 1, for y∼pdataandx0∼pinit, we have:\nvθ((1−s)x0+sy, s) =y−x0.\nTherefore, ∥(y−x0)−vθ((1−s)x0+sy, s)∥2= 0for all x0ands, which implies the result.\nTheorem 1 (Monotonic Behavior of the ELBO) .Consider two points y1,y2∈Rd1such that\nLalign(y1;θ)>Lalign(y2;θ). If the velocity field vθisL-Lipschitz in its spatial argument and\nsatisfies Assumption 1, then:\n|C(y1)−C(y2)| ≤L·d1·1\n2∥y1−y2∥, (21)\nwhere the factor1\n2comes from Es∼U[0,1][s] =1\n2. Moreover, if Lalign(y1;θ)− L align(y2;θ)>\nL·d1\n2· ∥y1−y2∥, then:\nlogpvθ\n1(y2)−logpvθ\n1(y1)>0. (22)\nProof. From the definition of C(y)in Eq. 14:\nC(y1)−C(y2) =−Es,x0[Tr(∇zvθ((1−s)x0+sy1, s))]\n+Es,x0[Tr(∇zvθ((1−s)x0+sy2, s))].(23)\nBy the Lipschitz continuity of vθ, its Jacobian ∇zvθ(z, s)has bounded operator norm\n∥∇zvθ(z, s)∥op≤L. Therefore:\n|Tr(∇zvθ(z1, s))−Tr(∇zvθ(z2, s))| ≤d1· ∥∇zvθ(z1, s)− ∇zvθ(z2, s)∥op\n≤d1·L· ∥z1−z2∥.(24)\nSetting z1= (1−s)x0+sy1andz2= (1−s)x0+sy2, we get ∥z1−z2∥=s∥y1−y2∥. Taking\nexpectations yields Eq. 21.\nFor the second part, using the ELBO bound from Proposition 1:\nlogpvθ\n1(y2)−logpvθ\n1(y1)≥[C(y2)− L align(y2;θ)]−[C(y1)− L align(y1;θ)]\n= [C(y2)−C(y1)] + [Lalign(y1;θ)− L align(y2;θ)].(25)\nUsing the bound on |C(y1)−C(y2)|and the condition on Lalign(y1;θ)− L align(y2;θ), the result\nfollows.\nA.2.3 Analysis of the Idealized Case\nWe address the mathematical singularity that arises in the idealized rectified flow case where vθhas\nthe exact form vθ(z, s) =z/sfors >0.\nProposition 3 (Regularization by Neural Network Parameterization) .Letvθbe parameterized by a\nneural network with bounded weights. Then there exists a constant M > 0such that:\n|Tr(∇zvθ(z, s))| ≤M∀z∈compact sets , s∈[ϵ,1] (26)\nfor any ϵ >0. Consequently, C(y)is well-defined and finite.\nProof. Neural networks with bounded parameters have Lipschitz continuous components. The\nJacobian ∇zvθ(z, s)inherits this boundedness on compact sets, preventing the 1/ssingularity from\noccurring exactly. The trace is therefore bounded, ensuring C(y)remains finite.\nA.2.4 Practical Implications and Optimization Strategy\nOur analysis establishes that:\n1. Consistency Principle: When y∼pdata, both Lalign(y;θ) = 0 andC(y)takes on the value\nappropriate for samples from the target distribution.\n2. Monotonicity Property: Theorem 1 shows that sufficiently large reductions in Lalign(y;θ)\nguarantee improvements in the ELBO lower bound, even accounting for changes in C(y).\n16\n--- Page 17 ---\n3. Computational Tractability: While computing C(y)requires evaluating Jacobian traces, min-\nimizing only Lalign(y;θ)provides a computationally efficient proxy that, by Theorem 1, leads to\nELBO improvements under reasonable conditions.\n4. Robustness: Proposition 3 ensures that practical neural network implementations avoid the\ntheoretical singularities, making the method stable in practice.\nThis analysis demonstrates that minimizing Lalign(y;θ)is not merely heuristic but has solid theoretical\nfoundation as a strategy for maximizing the variational lower bound on logpvθ\n1(y).\nA.3 The Significance of Assumptions\nThe derivation of Proposition 1 and its interpretation rely on several assumptions, as listed in Sec. A.1.\nIn this section, we discuss the significance of each assumption.\nAssumption 2 (Properties of the Velocity Field): Lipschitz continuity of vθin its spatial argument\nensures that the ODE ˙zt=vθ(zt, t)has unique solutions, fundamental for defining pvθ\n1(y). Differ-\nentiability is required for the Jacobian ∇zvθto exist, and thus for the divergence term Tr(∇zvθ)\nin the ELBO to be well-defined. These are standard regularity conditions for flow-based models.\nWithout them, the ELBO formulation would be ill-defined.\nAssumption 3 (Variational Path Choice): The choice of straight-line paths zs(x0,y) = (1 −s)x0+\nsyis a specific variational decision. This leads to the path velocity ˙zs=y−x0, which is key to the\ndefinition of Lalign(y;θ). This assumption is thus crucial for the specific form of Lalignused.\nAssumption 4 (Variational Distribution Choice): Setting q(x0|y) =pinit(x0)greatly simplifies\nthe ELBO by causing the term logpinit(x0)−logq(x0|y)to vanish. This common choice implies\nthe ELBO considers paths originating from the prior, without inferring a specific x0for each y.\nWhile simplifying, this choice affects the ELBO’s tightness.\nAssumption 5 (Weighting Factor in ELBO): Choosing λs=λmakes the loss term in the ELBO\ndirectly correspond to Lalign. A time-dependent λs>0is also valid and could yield a tighter bound\nor differentially weight errors across time s. The constant λensures a direct link to the standard L2\nnorm in Lalign. This choice affects the ELBO’s value but not its validity as a lower bound.\nAssumption 1 (Optimality of vθ):As detailed in Remark 1 of Sec. A.1, this assumption is not\nnecessary for the mathematical derivation of Proposition 1 itself; the ELBO inequality holds for\nanyvθsatisfying Assumption 2. However, Assumption 1 is paramount for the interpretation and\neffectiveness of minimizing Lalign(y;θ)as a strategy to align ywithpdata. Ifvθis optimal as defined,\nthenLalign(y;θ)will be minimized ideally to zero when yis drawn from pdata. Consequently,\nminimizing this loss for yencourages yto conform to pdata.\nIn essence, Assumptions 2 through 5 are primarily structural, defining the specific ELBO being\nanalyzed. They ensure the bound is well-defined and takes the presented form. Assumption 1\nconcerning the optimality of vθis interpretative, providing the rationale for why minimizing a\ncomponent of this ELBO ( Lalign) is a meaningful objective for achieving distributional alignment.\nThe overall conclusion that minimizing Lalignserves as a proxy for maximizing a log-likelihood lower\nbound relies on these assumptions.\nB Additional Toy Examples\nTo further demonstrate the effectiveness of our proposed method, we present additional toy examples\nwith diverse target distributions pdata: a Grid of Gaussians, Two Moons, Concentric Rings, a Spiral,\nand a Swiss Roll. For each of these distributions, following the visualization style of Fig. 3, we\nillustrate: (a) The optimized variables y(red triangles) and samples from pdata(blue dots), overlaid\non the negative log-likelihood (NLL) landscape of pdata(background heatmap showing −logpdata(·)).\n(b) The landscape of the alignment loss Lalign(background heatmap), with samples from pdata(blue\ndots). (c) The evolution of Lalign(y;θ)(blue solid line) and the true NLL −logpdata(y)(red dashed\nline) during the optimization of y.\nFor the Grid of Gaussians, which is also a mixture of Gaussians, the NLL −logpdata(y), is computed\nanalytically. For the other distributions (Two Moons, Concentric Rings, Spiral, and Swiss Roll),\nwhere an analytical form for pdatais not readily available, we estimate the NLL using Kernel Density\n17\n--- Page 18 ---\nEstimation (KDE). This estimation is based on N= 100 ,000samples drawn from the respective\npdataand employs a Gaussian kernel with a bandwidth of h= 0.1. The probability density ˆpdata(x)at\na point xis estimated as:\nˆpdata(x) =1\nNhdNX\ni=1K\u0012x−xi\nh\u0013\n, (27)\nwhere xiare the Nsamples drawn from pdata,dis the dimensionality (here, d= 2), and K(·)\nis the Gaussian kernel function. The NLL for an optimized variable yis then approximated by\n−log (ˆpdata(y)). This provides an empirical measure of how well yaligns with the target distribution\nas estimated by KDE.\nThe results for these additional toy examples are comprehensively presented in Fig. 5. Each row\nin this figure corresponds to one of the five target distributions. The left column (a,d,g,j,m) shows\nthat the optimized variables y(red triangles) successfully converge to the high-density (low-NLL)\nregions of pdata. The middle column (b,e,h,k,n) demonstrates that the landscape of our alignment\nlossLalignclosely mirrors the NLL surface of pdata, with true data samples (blue dots) residing in\nlow-loss areas. The right column (c,f,i,l,o) confirms the strong positive correlation between Lalignand\nthe NLL of y, as both decrease concomitantly during optimization. Furthermore, Fig. 6 visualizes\nthe optimization trajectory of yfor the initial mixture of Gaussians (from Sec. 5.1) alongside the five\nadditional toy distributions. These sequential snapshots illustrate how minimizing Laligneffectively\nsteers the variables yfrom their initialization towards the intricate structures of the target distributions,\nreinforcing the robustness and efficacy of our alignment loss.\nC Implementation Details\nC.1 Implementation Details of the Toy Example\nThe primary toy example, illustrated in Figure 3, utilizes a 2D Mixture of Gaussians (MoG) as the\ntarget data distribution pdata(x). This MoG distribution consists of 5 components, each with an\nisotropic standard deviation of 0.3. The means of these Gaussian components are distributed evenly\non a circle of radius 3.0. Prior to model training, samples drawn from this MoG distribution are\nnormalized by dividing by their standard deviation, which is empirically computed from a large batch\nof 10 million samples. In addition to the MoG, our toy experiments also encompassed other 2D\nsynthetic distributions, including Spiral, Moons, Concentric Rings, Swiss Roll, and Grid of Gaussians,\nto demonstrate the versatility of our approach. The general setup for the flow model and learnable\nlatents applies across these various distributions.\nThe conditional flow model, denoted vϕ(x, t), is implemented using a MLP with AdaLN. This\nnetwork has 2 input channels, 2 output channels, a hidden dimensionality of 512, and incorporates 4\nresidual blocks. The flow model is trained for 100,000 steps using the Adam optimizer (beta values of\n(0.9, 0.999) and no weight decay) with a constant learning rate of 1×10−4, and a batch size of 256.\nA set of 1,000 learnable latent variables {yi}are initialized by sampling from a standard normal\ndistribution N(0,I). These latents are then optimized to align with the target distribution pdataby\nminimizing the alignment loss Lalign. This alignment training phase also employs the Adam optimizer\n(betas=(0.9, 0.999), no weight decay), with a learning rate of 1×10−2, and runs for 5,000 steps.\nC.2 Implementation Details of the Flow Model\nThe flow model vθ(z, t) :Rd1×[0,1]→Rd1is implemented as a multi-layer perceptron (MLP)\nwith 6 layers and 1024 hidden units per layer. The network employs GELU activation functions and\nincorporates time modulation through adaptive layer normalization (AdaLN) to handle the temporal\ndimension t. When dimension mismatch occurs between the latent space dimension d1and target\nfeature space dimension d2, fixed linear projection layers are applied to map target features to the\nappropriate dimension. These projection matrices are initialized with Gaussian weights scaled by\n1/√d2and remain frozen during training.\nThe flow model is trained using the flow matching objective on the target distribution pdatafor 1\nmillion steps. During training, the model learns to predict velocity fields that transport samples from a\nstandard Gaussian base distribution N(0,I)to the target distribution along straight-line interpolation\n18\n--- Page 19 ---\nGrid of Gaussians(a)\n (b)\n (c)\nTwo Moons(d)\n (e)\n (f)\nConcentric Rings(g)\n (h)\n (i)\nSpiral(j)\n (k)\n (l)\nSwiss Roll(m)\n (n)\n (o)\nFigure 5: Further illustrations of our method’s performance on various 2D toy examples. Each\nrow corresponds to a different target distribution pdata(Grid of Gaussians, Two Moons, Concentric\nRings, Spiral, and Swiss Roll). Left column (a,d,g,j,m): Optimized variables y(red triangles) and\nsamples from pdata(blue dots). The background heatmap visualizes the negative log-likelihood (NLL)\n−logpdata(·), with yconverging to low-NLL (high-density) regions. Middle column (b,e,h,k,n):\nThe landscape of the alignment loss Lalign(heatmap) with pdatasamples (blue dots). This landscape\nmirrors the NLL surface, and pdatasamples are concentrated in areas of low Lalign.Right column\n(c,f,i,l,o): Training curves for Lalign(y;θ)(blue solid line) and NLL −logpdata(y)(red dashed line).\nTheir strong positive correlation and concurrent decrease during optimization demonstrate that Lalign\neffectively serves as a proxy for maximizing the log-likelihood of yunder pdata.\n19\n--- Page 20 ---\nTraining Progress\nFigure 6: Evolution of the optimized variables y(red triangles) during training across various toy\nexamples. Each column represents a target distribution pdata. The training progress demonstrates how\nminimizing Lalignguides yto converge towards low-NLL (high-density) regions of pdata.\n20\n--- Page 21 ---\nTable 3: Training Hyperparameters\nHyperparameter Flow Autoencoder MAR\nGlobal Batch Size 256\nSteps 1000k 50k 250k\nOptimizer AdamW\nBase Learning Rate 1.0×10−4\nLR Scheduler Cosine Cosine Constant\nWarmup Steps 2.5k 2.5k 62.5k\nAdam β1 0.9\nAdam β2 0.95 0.95 0.999\nWeight Decay 1.0×10−41.0×10−40.02\nMax Grad Norm 1.0\nMixed Precision BF16\nEMA Rate 0.9999\npaths. The training employs mixed precision (BF16) with gradient clipping and exponential moving\naverages (EMA). Upon completion of training, the flow model parameters θare frozen and used for\nsubsequent latent space alignment. Detailed hyperparameters are provided in Table 3.\nC.3 Implementation Details of Autoencoders\nOur autoencoder architecture follows the SoftVQ design, which employs Vision Transformer (ViT)\nbased encoder and decoder networks. The encoder utilizes a ViT-Large model with patch size 14\nfrom DINOv2 [ 48], initialized with pre-trained weights and fine-tuned with full parameter updates\nduring training. The decoder employs the same ViT-Large architecture but is initialized randomly\nwithout pre-trained weights.\nThe training process utilizes adversarial loss with a DINOv2-based discriminator, incorporating\npatch-based adversarial training with hinge loss formulation. Perceptual loss is applied using VGG\nfeatures with a warmup period of 10ksteps. The model is trained for 50ksteps with cosine learning\nrate scheduling and exponential moving averages for stable training dynamics. Unlike SoftVQ, we\ndo not employ the sample-level alignment loss (i.e., REPA loss), making our method more general\nand efficient. Detailed hyperparameters are provided in Table 3.\nWe followed the SoftVQ implementation as closely as possible. While we can reproduce almost\nidentical reconstruction results, our tokenizer doesn’t quite match the generation performance of the\nreleased pre-trained model, even after significant effort to optimize it. We believe this gap comes\nfrom differences in the cleaned-up code and the specific hardware we used for training. To keep\nthings fair and validate the effectiveness of our method, we conduct all experiments on the same\nhardware with identical training settings .\nC.4 Implementation Details of MAR\nWe follow the original MAR-B implementation with several key modifications. We incorporate\nqk-norm in the attention mechanism and replace the diffusion head with a flow-based head trained\nusing per-token flow matching loss. The original SD-KL-16 autoencoder is replaced with our trained\nautoencoders, applying input normalization with scaling factor 1.7052 estimated from sample batches.\nOur model uses MAR-B architecture with 256×256input images. The flow-based MLP head\nfeatures adaptive layer normalization with 6 layers and 1024 hidden units per layer, identical to the\noriginal diffusion implementation. The model processes sequences of length 64 corresponding to our\n64-token latent representation. More training details are provided in Table 3.\nFor inference, we employ an Euler sampler with 100 steps for the flow-based generation. The\nautoregressive sampling is limited to 64 steps. Generation uses batch size 256 and produces 50,000\nimages for evaluation. All evaluations use the standard toolkit from guided diffusion with FID and IS\nmetrics computed at regular intervals during training.\n21\n--- Page 22 ---\nD Additional Discussions for the Experiments\nHere we provide additional discussions and analysis for the experiments presented in Section 5.2.\nDoes Johnson-Lindenstrauss Lemma Really Hold? While the Johnson-Lindenstrauss (JL) lemma\ntheoretically guarantees that random projections preserve distances with high probability, our ex-\nperimental setup violates its conditions due to the large sample size relative to the target dimension.\nHowever, our results demonstrate that random projections can still preserve distributional structure\nto a sufficient extent for effective alignment. In our ablation study with Tab. 2a, random projection\nachieves the best performance with FID of 11.89 and IS of 102.23, significantly outperforming\nPCA (FID: 14.95, IS: 83.59) and average pooling (FID: 16.06, IS: 60.37). This suggests that the\nstructure-preserving properties of random projections, even when the JL lemma doesn’t strictly hold,\nare more beneficial than the variance-maximizing properties of PCA or the spatial averaging of\npooling operations.\nContinuous or Discrete? Our method demonstrates robustness across both continuous and discrete\ntarget distributions. Continuous semantic features from DinoV2 achieve the best generation perfor-\nmance among all variants in Tab. 1 and the discrete textual features from Qwen also achieve effective\nperformance. In contrast, discrete VQ features perform poorly, likely due to structural limitations\nimposed by low dimensionality (8-dim). The collapse observed in discrete VQ experiments during\ntraining can be attributed to the insufficient capacity of the low-dimensional latent space to capture\nthe complexity of ImageNet data while simultaneously satisfying the alignment constraint.\nWhy Textual Features Work? The surprising effectiveness of textual embeddings (Qwen) for\nvisual generation warrants deeper analysis. Despite being trained on text data, Qwen embeddings\nachieve competitive generation performance (FID: 11.89 without CFG) and the best PSNR (23.12)\namong aligned methods. This suggests that high-quality textual representations capture abstract\nsemantic structures that are transferable across modalities. The 896-dimensional Qwen embeddings\nprovide a rich semantic space that can effectively constrain the visual latent space without being\noverly restrictive. This cross-modal transferability indicates that the structural benefits of alignment\nare not limited to within-modality features.\nIs Generation Loss a Good Indicator? The training loss in generation of our aligned autoencoders\nis significantly lower than other models. However, we observe that lower training losses do not\nnecessarily translate to better generation results, even for flow-based models where loss is proven to\nbe a direct indicator for generation performance. This paradox can be attributed to the simplification\nof the latent space under strong alignment constraints. While simplified latent spaces are easier for\ngenerative models to sample from (hence lower training losses), they may sacrifice the diversity\nand fine-grained details necessary for high-quality generation. This suggests that generation quality\ndepends not only on the ease of modeling the latent distribution but also on the expressiveness and\ndiversity preserved in the aligned space.\nHow to Select the Prior? The optimal choice of target distribution remains an open research\nquestion. Our experiments suggest several guidelines: (1) Higher dimensionality generally enables\nbetter performance, as evidenced by the poor performance of 8-dimensional VQ features compared\nto higher-dimensional alternatives. (2) Semantic richness matters, but not necessarily complex-\nity—simple textual features can match sophisticated visual features. (3) The structural properties of\nthe target distribution (e.g., smoothness, cluster separation) may be more important than its semantic\ncontent for generation quality.\n22",
  "text_length": 72507
}