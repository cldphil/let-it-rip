{
  "id": "http://arxiv.org/abs/2506.04179v1",
  "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling",
  "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.",
  "authors": [
    "Anhao Zhao",
    "Fanghua Ye",
    "Yingqi Fan",
    "Junlong Tong",
    "Zhiwei Fei",
    "Hui Su",
    "Xiaoyu Shen"
  ],
  "published": "2025-06-04T17:26:31Z",
  "updated": "2025-06-04T17:26:31Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04179v1",
  "full_text": "--- Page 1 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\nModule Decoupling\nAnhao Zhao1 2Fanghua Ye3Yingqi Fan1Junlong Tong1 4Zhiwei Fei5Hui Su6Xiaoyu Shen1\nAbstract\nLarge language models (LLMs) achieve remark-\nable performance across tasks but incur substan-\ntial computational costs due to their deep, multi-\nlayered architectures. Layer pruning has emerged\nas a strategy to alleviate these inefficiencies, but\nconventional static pruning methods overlook two\ncritical dynamics inherent to LLM inference: (1)\nhorizontal dynamics , where token-level hetero-\ngeneity demands context-aware pruning decisions,\nand (2) vertical dynamics , where the distinct func-\ntional roles of MLP and self-attention layers ne-\ncessitate component-specific pruning policies. We\nintroduce SkipGPT , a dynamic layer pruning\nframework designed to optimize computational\nresource allocation through two core innovations:\n(1) global token-aware routing to prioritize crit-\nical tokens and (2) decoupled pruning policies\nfor MLP and self-attention components. To miti-\ngate training instability, we propose a two-stage\noptimization paradigm: first, a disentangled train-\ning phase that learns routing strategies via soft\nparameterization to avoid premature pruning de-\ncisions, followed by parameter-efficient LoRA\nfine-tuning to restore performance impacted by\nlayer removal. Extensive experiments demon-\nstrate that SkipGPT reduces over 40% model pa-\nrameters while matching or exceeding the perfor-\nmance of the original dense model across bench-\nmarks. By harmonizing dynamic efficiency with\npreserved expressivity, SkipGPT advances the\npractical deployment of scalable, resource-aware\nLLMs. Our code is publicly available at: https:\n//github.com/EIT-NLP/SkipGPT .\n1Ningbo Key Laboratory of Spatial Intelligence and Digital\nDerivative, Institute of Digital Twin, Eastern Institute of Tech-\nnology, Ningbo2Southwest Jiaotong University3Tencent Inc.\n4Shanghai Jiao Tong University5Nanjing University6Meituan Inc..\nCorrespondence to: Fanghua Ye <fanghua.ye.21@gmail.com >,\nXiaoyu Shen <xyshen@eitech.edu.cn >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\n\"the\"F\nASkipGPT Static Structured Pruning\n\"1\"F\nAF\n\"and\"A\nA\nASelf Attention FFFN Important Redundantùêøùëñ+1\n\"the\"AF\n\"1\"AF\n\"and\"AFùêøùëñFigure 1. An overview of SkipGPT. Unlike conventional static\nstructured pruning, SkipGPT dynamically prunes layers by con-\nsidering both horizontal and vertical dynamics. In horizontal\ndynamics , different tokens receive varying computational alloca-\ntions. In vertical dynamics , the MLP and attention modules are\ndecoupled to account for their distinct roles within each layer.\n1. Introduction\nLarge language models (LLMs) are built on a layer-wise\nTransformer architecture, where each layer consists of a\nself-attention mechanism followed by a multi-layer percep-\ntron (MLP) (Vaswani et al., 2017). Scaling up model size\nhas driven significant breakthroughs across a wide range\nof tasks (Brown, 2020; Bommasani et al., 2022; Wei et al.,\n2023; Zhao et al., 2024; Xin et al., 2025; Chen et al., 2025).\nHowever, this progress comes at a steep computational cost,\nrequiring vast resources for inference (Chowdhery et al.,\n2022; Wan et al., 2024; OpenAI et al., 2024). In contrast, the\nhuman brain‚Äîdespite its 100 trillion synaptic connections,\nfar surpassing even the largest LLMs‚Äîoperates efficiently\non just 30 watts of power (Bartol et al., 2015; Samsi et al.,\n2023). This stark disparity underscores a fundamental inef-\nficiency in current LLM architectures, highlighting the gap\nbetween artificial intelligence and human cognition.\nGiven their sequential layer-wise structure, LLMs struggle\nto fully leverage parallelism, even with abundant compu-\ntational resources. This limitation makes layer pruning a\ncrucial strategy for accelerating inference and improving\nefficiency (Men et al., 2024; Kim et al., 2024; Gromov et al.,\n2024; Chen et al., 2024b). While existing pruning meth-\nods offer some improvements, they often overlook two key\naspects of pruning dynamics (see Figure 1):\n1arXiv:2506.04179v1  [cs.CL]  4 Jun 2025\n--- Page 2 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n1.Horizontal Dynamics : Different tokens in an input se-\nquence require varying levels of computation. Current\nmethods either allocate resources to the top- kmost rel-\nevant tokens per layer (Raposo et al., 2024; Zeng et al.,\n2023) or enforce a fixed computation ratio across all\ntokens (Jiang et al., 2024). These rigid approaches fail\nto adapt to token complexity, leading to suboptimal ef-\nficiency. To address this, we introduce a global sparsity\nmechanism, allowing computation budgets to be flexi-\nbly distributed across the entire forward pass rather than\nimposing fixed layer-wise or token-wise constraints.\n2.Vertical Dynamics : The MLP and self-attention com-\nponents within each layer serve distinct functions, yet\nmost pruning methods treat them uniformly. Research\nsuggests that MLPs function like localized neural pro-\ncesses, capturing task-specific interactions (Geva et al.,\n2021; Meng et al., 2023; Merullo et al., 2024), whereas\nattention mechanisms resemble higher-level cognitive\nfunctions, managing contextual relevance and informa-\ntion flow (Olsson et al., 2022; Kobayashi et al., 2020). In-\nspired by how the human brain activates different regions\nfor different tasks, we propose decoupling the pruning\nof MLP and self-attention, enabling more targeted and\nefficient computation reduction.\nTo achieve dynamic pruning, a few recent works have ex-\nplored adaptive computation methods that introduce a router\nat each Transformer layer (Zeng et al., 2023; Raposo et al.,\n2024; Jiang et al., 2024). This router functions as a decision\nmodule, determining whether specific network units should\nbe executed or skipped during inference. However, these\napproaches typically optimize the router and model param-\neters simultaneously in a joint training paradigm, similar\nto Mixture-of-Experts (MoE) (Lepikhin et al., 2020; Cai\net al., 2024; Zhu et al., 2024). However, this method fails\nto account for a fundamental difference between pruning\nand pretraining‚Äî in the pruning context, the router starts\nfrom random initialization, while the model parameters have\nalready converged to an optimal or locally optimal distri-\nbution through extensive pretraining . This mismatch may\nmake joint training unstable and prevent dynamic pruning\nfrom reaching its full potential.\nIn this work, we first provide empirical evidence demonstrat-\ning the significance of both horizontal dynamics and vertical\ndynamics in LLMs. To incorporate these two dynamics,\nwe propose SkipGPT , a novel approach that dynamically\nprunes layers on a per-token basis, adapting the pruning pro-\ncess to the complexity of each token. Furthermore, SkipGPT\ndecouples the MLP and self-attention components within\neach layer , enabling more granular control over which parts\nof the model are pruned, thereby optimizing both computa-\ntional efficiency and model performance. To fully unlock\nthe potential of dynamic pruning, we introduce a Two-stageTraining Paradigm . First, in Router Tuning , we freeze\nthe model parameters and optimize only the router, allowing\nit to identify the most critical computations without disrupt-\ning the model‚Äôs pretrained knowledge. Second, in LoRA\nFine-Tuning , we freeze the router and fine-tune the model\nusing LoRA to compensate for any performance degrada-\ntion caused by pruning. Our results establish SkipGPT as\na highly effective pruning strategy‚Äîenabling the pruned\nmodel to fully restore its performance , even surpassing\nthe original model, despite a 40% reduction in parameters.\nFurthermore, since router tuning does not modify the pre-\ntrained model parameters, it allows for a direct analysis of\nmodule importance in the original model. Through detailed\nrouter behavior analyses, we uncover two key insights: (1)\nAttention modules exhibit greater redundancy than MLP\nmodules. (2) As context length increases, later tokens de-\nmand more attention computation but less MLP processing.\nThese findings highlight inherent inefficiencies in current\nlarge model architectures, providing valuable insights for\nfuture architectural design and inference optimization.\n2. Motivation\nMeasuring module importance using cosine similarity\nRecent work has demonstrated that the cosine similarity\nbetween a module‚Äôs input and output serves as a reliable\nmetric for evaluating the importance of each module (Men\net al., 2024; Gromov et al., 2024). Notably, these studies\noften define a ‚Äúmodule‚Äù as an entire Transformer layer. To\nenable more fine-grained analysis, we refine the definition\nof a module to refer to either an MLP block or an attention\nblock within a layer. The underlying hypothesis of using\ncosine similarity is that redundant modules generate out-\nputs that closely resemble their inputs, indicating minimal\ntransformation. Conversely, important modules substan-\ntially alter their inputs, suggesting they play a critical role\nin the model and should be preserved. Prior approaches\ntypically average the cosine similarity over all tokens to\nderive a general importance metric for each module. How-\never, this aggregation may obscure the variability of module\nimportance across different tokens and contexts. To better\nunderstand this variability, we analyze the cosine similarity\nof each module at the token level. Specifically, the cosine\nsimilarity Ci,tof the ithmodule at token tis computed as:\nCi,t=xT\ni,txi+1,t\n‚à•xi,t‚à•2‚à•xi+1,t‚à•2, (1)\nwhere ‚à• ¬∑ ‚à• 2is the L2-norm and xi,tdenotes the hidden\nstate before module iat token t. To illustrate how module\nimportance varies, we conduct a case study using a randomly\nselected sentence from the BookCorpus dataset (Zhu et al.,\n2015). We analyze cosine similarity distributions across 15\nconsecutive tokens in LLaMA-2-7B (Touvron et al., 2023a),\nwith the results visualized in Figure 2.\n2\n--- Page 3 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n0.00.20.40.60.81.0\nAttn 30MLP 30Attn 31MLP 31Attn 32MLP 32\n1.00 1.00 1.00 0.99 0.99 0.97 0.99 0.98 0.98 0.98 0.99 0.99 0.97 0.97 0.980.95 0.94 0.95 0.87 0.86 0.88 0.94 0.87 0.89 0.88 0.89 0.90 0.86 0.87 0.900.99 0.99 0.99 0.98 0.95 0.92 0.98 0.98 0.98 0.98 0.99 0.99 0.97 0.97 0.950.77 0.80 0.85 0.81 0.75 0.75 0.85 0.68 0.58 0.74 0.78 0.66 0.67 0.67 0.700.94 0.97 0.97 0.96 0.95 0.94 0.97 0.97 0.96 0.96 0.94 0.96 0.96 0.95 0.960.00 0.01 0.07 0.10 0.04 0.05 0.12 0.04 0.01 0.08 0.08 0.04 0.05 0.05 0.09\n...\nthat\n had\nbeen\n meg\nan'\n s\nplan\nwhen\nshe\n got\n him\ndressed\nearlier\n.\nAttn 1MLP 1Attn 2MLP 2Attn 3MLP 3Attn 4MLP 4Attn 5MLP 5\n0.01 0.04 0.09 0.23 0.05 0.03 0.02 0.23 0.12 0.08 0.22 0.17 0.45 0.36 0.050.21 0.39 0.50 0.45 0.73 0.49 0.71 0.66 0.72 0.70 0.70 0.72 0.61 0.62 0.630.06 0.12 0.09 0.29 0.17 0.34 0.26 0.38 0.37 0.31 0.21 0.24 0.30 0.35 0.230.31 0.51 0.59 0.42 0.37 0.54 0.42 0.56 0.70 0.66 0.58 0.57 0.45 0.46 0.510.92 0.89 0.64 0.86 0.69 0.76 0.51 0.75 0.79 0.79 0.83 0.58 0.68 0.87 0.870.52 0.58 0.59 0.67 0.15 0.63 0.41 0.56 0.70 0.71 0.64 0.60 0.58 0.62 0.660.82 0.80 0.77 0.77 0.88 0.77 0.64 0.70 0.71 0.75 0.66 0.77 0.76 0.72 0.740.56 0.56 0.46 0.59 0.29 0.53 0.40 0.46 0.60 0.65 0.57 0.60 0.50 0.51 0.540.92 0.87 0.77 0.67 0.77 0.63 0.63 0.59 0.18 0.48 0.76 0.68 0.83 0.57 0.440.32 0.50 0.50 0.59 0.51 0.31 0.43 0.38 0.34 0.44 0.49 0.50 0.42 0.42 0.38\nFigure 2. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B, which consists of 32 layers, corresponding to 64 modules\nin total. Due to space constraints, we showcase only the results for the initial and final modules. Higher values indicate greater redundancy.\nThe Necessity of Vertical Dynamics Existing pruning\nmethods, whether dynamic or static, typically treat an entire\ntransformer layer as the smallest pruning unit. However, as\nillustrated in Figure 2, we find that within the same layer,\nthe cosine similarity distributions of attention and MLP\nmodules can vary significantly. For example, in the second\nlayer, the cosine similarity of most tokens in the attention\nmodule ranges from 0.1 to 0.4, while the MLP module\npredominantly falls within the range of 0.4 to 0.7. This\nindicates that for this layer, the MLP module is almost\nuniversally more redundant than the attention module. In\nthe final layer, however, the situation is entirely reversed: all\nattention modules exhibit cosine similarity values between\n0.9 and 1, while MLP modules fall within a completely\ndifferent range, from 0 to 0.2, making the MLP module far\nmore critical than the attention module at this stage. These\nfindings reveal that even within the same layer, redundancy\nlevels between attention and MLP can vary and shift across\nlayers , underscoring the necessity of decoupling attention\nand MLP modules for more effective pruning.\nThe Necessity of Horizontal Dynamics Static pruning\nmethods rely on two key assumptions: (1) the distribution\nof important modules is uniform across all tokens, and (2)\neach token is associated with the same number of important\nmodules (Men et al., 2024; Kim et al., 2024; Gromov et al.,\n2024). Existing dynamic pruning methods also adopt these\nassumptions when setting compute budgets (Zeng et al.,\n2023; Raposo et al., 2024; Jiang et al., 2024). However,these assumptions do not hold in practice. First, module\nimportance varies across layers . For instance, at a cosine\nsimilarity threshold of 0.8, the attention module in layer\n31 is redundant for 15 tokens, whereas in layer 4, it is\nredundant for only 3‚Äîclearly disproving uniform impor-\ntance. Second, token importance is not uniform within a\nsequence . In our case study, we analyze the number of mod-\nules with a cosine similarity below 0.6‚Äîconsidering them\nsignificant‚Äîfor each token. The results reveal that the token\n‚Äúplan‚Äù is associated with 13 important modules, while ‚Äúan‚Äù\nhas only 8, confirming that different tokens require varying\nlevels of computation .\n3. SkipGPT: Dynamic Layer Pruning\nAfter establishing the necessity of horizontal and verti-\ncal dynamics, we now introduce SkipGPT , our proposed\nframework for dynamic layer pruning. In this section, we\nfirst outline the necessary preliminaries for understanding\nSkipGPT‚Äôs optimization process, followed by an explana-\ntion of its sparsity mechanism, routing implementation, loss\nfunction, and finally, our two-stage training paradigm for\nstable and effective learning.\n3.1. Preliminaries: Gumbel-Softmax and STE\nGumbel-Softmax Reparametrization To optimize\nSkipGPT‚Äôs dynamic pruning decisions, we formulate\nthe pruning process as a discrete optimization problem,\n3\n--- Page 4 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nwhere each module (MLP or attention) is either executed\nor skipped based on its computed importance. However,\ndirectly optimizing discrete decisions is non-differentiable,\nmaking standard gradient-based optimization infeasi-\nble. The Gumbel-Softmax distribution is a continuous\napproximation of the categorical distribution, enabling\ndifferentiable sampling (Jang et al., 2022). This is achieved\nviareparameterization , which transforms discrete samples\ninto differentiable continuous ones for gradient-based\noptimization. Let œÄ1, œÄ2, . . . , œÄ krepresent the class\nprobabilities of a k-class categorical distribution. To sample\nfrom this distribution, the Gumbel-Max trick (Gumbel,\n1954; Maddison et al., 2015) selects the category with the\nhighest value of logœÄi+gi, where giare i.i.d. samples\nfrom the Gumbel distribution Gumbel (0,1)1:\nz=onehot\u0010\narg max\ni[gi+ log œÄi]\u0011\n. (2)\nSince arg max is non-differentiable, the Gumbel-Softmax\nreparametrization replaces it with a softmax function, pro-\nducing continuous samples approximating the categorical\ndistribution:\nyi=exp\u0010\nlogœÄi+gi\nœÑ\u0011\nPk\nj=1exp\u0010\nlogœÄj+gj\nœÑ\u0011, i = 1, . . . , k, (3)\nwhere œÑcontrols the sharpness of the distribution. As œÑ‚Üí0,\nthe samples resemble one-hot vectors, recovering the origi-\nnalarg max operation. This differentiable approximation\nenables standard backpropagation for optimization.\nStraight-Through Estimator The Straight-Through (ST)\nEstimator (Bengio et al., 2013) enables discrete sampling\nwhile preserving differentiability for backpropagation. In\nthe forward pass, we use Gumbel-Softmax to generate con-\ntinuous samples. To discretize them, we apply arg max , but\nin the backward pass, gradients are computed as if using the\ncontinuous approximation. This is achieved via:\nyhard=yhard‚àíysoft¬∑detach () +ysoft, (4)\nwhere ysoftis the continuous Gumbel-Softmax sample. This\nensures a one-hot output while gradients follow ysoft, en-\nabling efficient optimization despite discrete sampling.\n3.2. The Concept of Sparsity\nTo control the total FLOPs, we introduce the concept of\nsparsity, defined as the fraction of module computations\n(attention or MLP) skipped in a forward pass, relative to\nthe total computations in a fully dense transformer, account-\ning for all layers and sequence positions. In our method,\n1gcan be obtained via inverse transform sampling: u‚àº\nUniform (0,1),g=‚àílog(‚àílog(u)).sparsity is achieved through dynamic routing, where only\na subset of modules is selected for computation in each\nforward pass. Specifically, assuming that in a forward pass,\nanL-layer LLM (which consists of 2Lmodules, each layer\ncontaining one attention module and one MLP module) pro-\ncesses a sequence of length S, a dense transformer would\ncompute 2L√óSmodules, corresponding to a sparsity of 0.\nWhen sparsity is greater than 0, only (1‚àísparsity )√ó2L√óS\nmodules are computed.\nUnlike previous work, which defines the compute budget\nby restricting computation to the top- ktokens at each layer\n(Raposo et al., 2024; Zeng et al., 2023) or enforcing the same\nsparsity for each token (Jiang et al., 2024), we allow the\ncomputational load to be dynamically allocated across both\nwidth (the number of tokens participating in computation at\neach layer) and height (the number of modules involved in\nthe computation for each token).\n3.3. Routing Implementation\nTo enable dynamic allocation, SkipGPT assigns a router\nbefore each individual module. Specifically, we route to-\nkens to two computational paths: (1) self-attention (for the\nattention router) or FFN modules (for the FFN router), and\n(2) a residual connection. The latter is computationally in-\nexpensive, producing an output determined entirely by the\ninput, while the former incurs a high computational cost.\nSuppose that we have a token embedding xlprior to the\nl-th transformer module fl, where this module can either\nbe a self-attention or an FFN. Before passing through the\nmodule fl,xlfirst undergoes a router function, which is a\nsimple linear projection, yielding a categorical distribution\nrl=WT\nŒ∏xl‚ààR2, where the first element represents the\nprobability of skipping the module, and the second element\nrepresents the probability of executing it.\nOnce this categorical distribution is obtained, a natural rout-\ning strategy is the Top-1 routing. Specifically, we have:\nxl+1=(\nrl[1]¬∑(fl(xl) +xl),ifarg max rl= 1,\nrl[0]¬∑xl, otherwise ,(5)\nsuch that the gradients can be backpropagated. However,\nthis routing strategy challenges precise sparsity control, as\nrlis merely a soft approximation of binary selection.\nBy leveraging Gumbel-Softmax and the ST Gumbel Esti-\nmator, we effectively address this issue. Specifically, after\napplying Gumbel-Softmax and the ST Gumbel Estimator to\nrl, we obtain a one-hot vector gl‚àà {0,1}2. Thus, the input\nto the next module is computed as:\nxl+1=gl[1]¬∑(fl(xl) +xl) +gl[0]¬∑xl. (6)\nDuring the forward pass, discrete binary values are sam-\npled, ensuring clear pruning decisions. In the backward pass,\n4\n--- Page 5 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nRouterLayer Norm\nAttention/FFN Layer\nRouterLayer Norm\nAttention/FFN Layer\n0Forward\nHard Decision\nSoft ProbabilityBackward\n0.82Trainable Frozen\nSoft ProbabilityBackward\n0.731Forward\nHard Decision\nFigure 3. Illustration of the forward and backward passes during\nthe router tuning stage. In the forward pass, the router makes hard\ndecisions to either execute (1) or skip (0). In the backward pass,\nthe gradients are propagated back using soft probabilities.\nsoft probabilities facilitate gradient propagation, allowing\nthe router weights to be updated effectively.\nWith this routing design, in principle, we can train the router\nindependently without altering the model parameters to ob-\ntain the optimal routing solution for a given sparsity ratio.\n3.4. Loss Function\nWith our definition in Section 3.2, the sparsity ris given by:\nr=P\nt,lgt\nl[0]\nS√ó2L, (7)\nwhere tis the token index, lrepresents the module index,\nLis the total number of layers in the LLM, and Sdenotes\nthe length of the sequence. To meet different computational\ndemands, we introduce a sparsity regularization term:\nLsparsity =|T ‚àí r|, (8)\nwhere |¬∑|denotes absolute value, and Tis the user-defined\ntarget sparsity. The overall loss function is then given by:\nLall=Llm+Œ±Lsparsity , (9)\nwhere Llmis the standard language modeling loss, which\nrepresents the negative log-likelihood of predicting the\nnext token on average. The hyperparameter Œ±controls the\nstrength of the sparsity penalty in the overall loss function.2\n3.5. Two-stage Training Paradigm\nAs the router starts from random initialization and the LLM\nhas already been pretrained, direct training can lead to un-\nstable and suboptimal pruning decisions. To address this\nissue, we propose a two-stage training paradigm.\n2IfŒ±is too small, it may fail to enforce the desired sparsity. If\nŒ±is too large, the model may compromise the optimization of Llm.\nBased on our experiments, a value of 8 works well.Router Tuning In the initial stage of training, we focus\nexclusively on tuning the router while keeping all other\nmodel parameters frozen. This stage is highly efficient, as it\nrequires adding only a lightweight linear layer before each\nmodule, with all router parameters combined accounting\nfor just 0.007% of the total parameters in LLaMA2-7B.\nThrough this stage, we achieve over 90% of the model‚Äôs\noriginal performance even after discarding 25% of the\nparameters, as shown in Table 1. The forward and backward\npasses during router tuning are illustrated in Figure 3.\nLoRA Fine-Tuning (Optional) While the router tuning\nstage is sufficient to preserve most of the model‚Äôs perfor-\nmance, we provide an optional LoRA fine-tuning stage for\nthose aiming to fully restore performance to the original\nmodel level . Low-Rank Adaptation (LoRA) (Hu et al.,\n2021) enables efficient refinement of LLMs with minimal\ncomputational overhead. Previous works, such as Ma et al.\n(2023); Kim et al. (2024), have demonstrated LoRA‚Äôs ef-\nfectiveness in enhancing statically pruned models. In this\nstudy, we show that LoRA can also effectively recover the\nperformance of dynamically pruned models. For a clearer\nunderstanding of the entire training process, please refer to\nthe algorithm illustrated in Appendix B.\n4. Experimental Configuration\n4.1. Models and Benchmarks\nModels We conduct experiments utilizing LLaMA2-7B ,\nLLaMA2-13B , and LLaMA3.1-8B (Touvron et al., 2023a;b;\nDubey et al., 2024).\nData During both router and LoRA tuning, we use the\nRedPajama-Data-1T-Sample dataset (Computer, 2023)3,\nwhich contains 850,000 samples (1 billion tokens) trun-\ncated to 4096 tokens each. This dataset serves two roles:\n(1) as a calibration set (100 random samples) to compute\nblock-level significance for pruning redundant layers (static\nmethods), and (2) as a training set for dynamic methods\nand for recovering static method performance (the specific\ndetails of static and dynamic methods will be introduced\nlater in Section 4.2).\nTraining Procedure Each model is trained for 10,000\nsteps with next-token prediction, using a batch size of 16\nin both router and LoRA tuning. In the router tuning stage,\nwe use a constant learning rate of 2e-34. Additionally, the\nsoftmax temperature œÑof the Gumbel-Softmax is linearly\nannealed from 5 to 1. In the LoRA tuning stage, the learning\n3https://huggingface.co/datasets/\ntogethercomputer/RedPajama-Data-1T-Sample\n4A grid-search was conducted to determine that a learning rate\nof 2e-3 optimally ensures training stability.\n5\n--- Page 6 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nModel Method RatioOBQA WinoGrande PIQA HeSw BoolQ ARC-E ARC-CAvg. Acc. ‚ÜëWT2 PTBAvg. PPL ‚ÜìAccNorm Acc Acc AccNorm Acc AccNorm AccNorm PPL PPL\nLLaMA2-7BDense 0.00% 44.20 74.19 78.07 78.93 71.62 81.36 52.47 68.69 5.47 20.83 13.15\nShortGPT 25.0% 34.80 68.43 67.68 60.77 62.17 59.34 38.40 55.94 25.42 70.97 48.20\nShortened-PPL 25.0% 33.60 52.88 70.40 55.12 61.07 55.05 29.44 51.08 11.15 49.07 30.11\nShortened-Taylor 25.0% 35.40 66.30 66.97 59.90 62.17 59.76 38.65 55.59 23.75 69.60 46.68\nJoint Layer Drop 23.9% 38.60 72.38 70.08 67.93 40.37 65.57 44.54 57.07 29.19 64.69 46.94\nLaCo 25.0% 36.60 67.32 65.72 62.76 74.37 58.92 38.14 57.69 18.96 47.58 33.27\nLLM-Pruner 25.3% 39.00 58.56 73.45 60.77 54.71 58.63 37.03 54.59 14.10 65.09 39.60\nSliceGPT 25.4% 35.40 65.82 66.38 53.43 50.43 69.57 40.10 54.45 7.56 76.29 41.93\nSkipGPT-RT 25.5% 39.60 63.54 72.20 70.96 68.81 76.52 44.37 62.29 7.81 30.58 19.20\nLLaMA2-13BDense 0.00% 45.20 76.16 79.11 82.23 80.52 84.68 59.47 72.48 4.88 28.92 16.90\nShortGPT 25.0% 40.60 70.80 71.38 72.59 62.69 69.19 45.31 61.79 20.05 49.52 34.79\nShortened-PPL 25.0% 39.40 67.17 73.12 69.31 62.57 69.19 41.13 60.27 8.45 54.62 31.54\nShortened-Taylor 25.0% 41.80 70.80 70.78 62.58 38.10 61.99 38.31 54.90 24.46 59.34 41.90\nJoint Layer Drop 24.3% 41.80 72.66 73.37 74.43 70.24 71.25 46.50 64.69 13.31 42.63 27.97\nLaCo 25.0% 38.80 62.75 72.74 63.11 44.46 62.84 35.07 54.25 13.40 53.92 33.66\nLLM-Pruner 24.9% 44.00 65.82 76.99 74.15 59.88 72.60 45.82 62.75 9.82 71.49 40.66\nSliceGPT 24.7% 38.60 68.43 64.42 50.97 38.87 69.36 40.36 53.00 7.43 99.89 53.66\nSkipGPT-RT 25.4% 46.00 71.90 76.88 74.33 74.37 77.69 47.08 66.89 6.78 41.69 24.24\nLLaMA3.1-8BDense 0.00% 44.80 77.51 80.03 81.95 82.14 84.85 57.59 72.69 6.24 10.58 8.41\nShortGPT 25.0% 28.00 54.14 58.76 31.50 37.77 38.05 31.40 39.95 2796.24 2799.46 2797.85\nShortened-PPL 25.0% 33.60 53.51 71.87 57.98 42.08 57.07 31.74 49.69 15.00 23.86 19.43\nShortened-Taylor 25.0% 28.20 54.06 58.87 31.53 37.77 38.05 31.31 39.97 2690.34 2793.25 2,741.80\nJoint Layer Drop 24.2% 33.00 55.56 61.26 42.26 37.31 35.06 30.12 42.08 32.32 51.40 41.86\nLaCo 24.5% 31.20 65.11 66.16 55.77 71.13 48.65 37.03 53.58 30.14 50.35 40.25\nLLM-Pruner 24.5% 37.20 56.99 72.03 54.75 56.79 51.22 31.31 51.47 25.21 45.59 35.40\nSliceGPT 24.6% 30.40 55.17 57.83 38.19 37.83 38.64 25.85 40.56 17.25 63.13 40.19\nSkipGPT-RT 25.5% 44.20 75.69 78.07 76.87 74.06 82.44 53.41 69.25 16.47 26.91 21.69\nLLaMA3.1-8BDense 0.00% 44.80 77.51 80.03 81.95 82.14 84.85 57.59 72.69 6.24 10.58 8.41\nShortGPT 40.6% 30.00 57.62 58.54 34.55 62.29 34.81 29.10 43.84 79856.66 125507.27 102681.97\nShortened-PPL 40.6% 27.00 52.41 61.48 41.74 57.13 39.69 26.45 43.70 157.01 196.04 176.53\nShortened-Taylor 40.6% 28.80 53.20 59.68 37.89 58.53 35.19 29.69 43.28 78424.35 125435.43 101,929.90\nJoint Layer Drop 39.9% 27.60 50.12 53.48 26.65 37.86 26.35 25.77 35.40 251.52 341.25 296.39\nLaCo 40.7% 27.60 51.54 56.31 31.53 55.11 30.22 25.85 39.74 269.24 392.50 330.87\nLLM-Pruner 39.9% 29.20 51.30 64.09 36.23 50.52 36.87 23.46 41.67 152.98 161.49 157.24\nSliceGPT 39.9% 25.60 51.62 53.92 30.64 37.83 31.82 22.10 36.22 143.24 183.21 163.23\nSkipGPT-RT 40.2% 38.00 59.35 73.34 64.36 60.37 77.53 45.65 59.80 71.25 48.05 59.65\nTable 1. Comparison of SkipGPT-RT and static pruning baselines without LoRA, where the ratio denotes the proportion of parameters\n(averaged per token) that do not participate in computations relative to the original model‚Äôs total parameter count.\nrate is set to 2e-4 with a warmup ratio of 0.1 and a cosine\nlearning rate scheduler. The AdamW optimizer (Loshchilov\n& Hutter, 2019) with Œ≤1= 0.9andŒ≤2= 0.95is used for\ngradient backpropagation.\nBenchmarks Following Touvron et al. (2023a), we evalu-\nate accuracy scores on a variety of commonsense reasoning\ndatasets, including BoolQ (Clark et al., 2019), PIQA (Bisk\net al., 2020), HellaSwag (HeSw) (Zellers et al., 2019), Wino-\nGrande (Sakaguchi et al., 2021), ARC-easy (ARC-E) (Clark\net al., 2018), ARC-challenge (ARC-C) (Clark et al., 2018),\nandOpenbookQA (OBQA) (Mihaylov et al., 2018), using\nthelm-evaluation-harness (Gao et al., 2024). Additionally,\nwe report zero-shot PPL scores on WikiText2 (WT2) (Merity\net al., 2016) and PTB (Marcus et al., 1993).\n4.2. Baseline\nStatic Pruning Baselines Static pruning involves the per-\nmanent removal of redundant model components. Short-\nGPT (Men et al., 2024) removes redundant layers based on\nBlock Influence (BI) scores. Shortened LLaMA (Kim et al.,\n2024) prunes layers using PPL and Taylor Expansion, result-\ning in two variants: Shortened-PPL andShortened-Taylor .LaCo (Yang et al., 2024) collapses layers progressively from\ndeep to shallow, employing a threshold to prevent excessive\nmerging. Joint Layer Drop (He et al., 2024) is a fine-grained\nvariant of ShortGPT that separately removes attention and\nMLP layers based on the same BI metric. LLM-Pruner\n(Ma et al., 2023) selectively removes non-critical structures\nusing gradient-based criteria. SliceGPT (Ashkboos et al.,\n2024) shrinks the embedding dimension by replacing large\nweight matrices with smaller dense matrices. Unlike static\npruning methods, SkipGPT considers horizontal dynamics,\noffering a more adaptive approach.\nDynamic Pruning Baselines Dynamic pruning allows for\nthe selective activation of model components during infer-\nence. MoD (Raposo et al., 2024) employs a top- krouting\nmechanism to dynamically activate layers for each token.\nTo ensure a fair comparison, we introduce a new variant,\nMoD-D , which differs from MoD by decoupling attention\nand MLP modules, whereas MoD treats them as a single\nunit. D-LLM (Jiang et al., 2024) introduces a dynamic deci-\nsion module at each transformer layer, determining whether\na layer should be executed or skipped. Additionally, we in-\nclude SkipGPT-Joint , which adopts a joint training paradigm\ninstead of two-stage training. From the perspective of hori-\n6\n--- Page 7 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nzontal dynamics, MoD-D allocates its computation budget\nlayer-wise, while D-LLM allocates it token-wise. Regard-\ning vertical dynamics, D-LLM does not decouple attention\nand MLP modules. Please refer to Appendix C for detailed\ndescriptions of these baselines.\n5. Results\nThis section provides a detailed analysis of the models after\neach training stage. For clarity, we define the model ob-\ntained after the first stage (Router Tuning) as SkipGPT-RT\nand the final model after the second stage (LoRA Fine-\nTuning) as SkipGPT-RT-L .\n5.1. Comparison Between SkipGPT-RT and Baselines\nWe present the accuracy and PPL for baseline pruning meth-\nods and SkipGPT-RT in Table 1. For all models we evaluate,\nincluding LLaMA2-7B, LLaMA2-13B, and LLaMA3.1-8B,\nthe attention module contains approximately half as many\nparameters as the MLP module. Thus, in Joint Layer Drop\nand SkipGPT-RT, we prune proportionally more modules\nto maintain a consistent average parameter ratio across\nmethods. For long-context tasks, the FLOPs of the atten-\ntion module surpass those of the MLP, making SkipGPT-RT\nachieve lower computational overhead compared to other\nbaselines. For LLaMA2-7B and LLaMA2-13B, we report\nevaluation results under a 25% parameter reduction setting,\nwhile for LLaMA3.1-8B, we provide results for both 25%\nand 40% parameter reduction. Additional results under\nvarying pruning ratios are presented in Appendix D.\nThe results demonstrate that with router tuning alone (with-\nout modifying model parameters), SkipGPT-RT signifi-\ncantly outperforms baseline methods. Specifically, for\nLLaMA2-7B and LLaMA2-13B, SkipGPT-RT retains over\n90% of the dense model‚Äôs performance even under 25%\nparameter pruning. For LLaMA3.1-8B, it retains over 95%\nperformance at 25% pruning and over 80% performance at\n40% pruning ‚Äî a level at which nearly all baselines col-\nlapse catastrophically. Notably, while router tuning involves\ntraining the router parameters, unlike static pruning, which\npermanently removes layers based on predefined metrics,\nits cost is minimal. This is because the router parameters\nconstitute less than 0.01% of the total model parameters\nand converge rapidly (Figure 4). In practice, the tuning\nprocess requires only a single A800 (80GB) GPU and com-\npletes within four hours. Furthermore, the results in Table 1\nhighlight three key observations:\n‚Ä¢Layer-pruning methods‚Äîsuch as ShortGPT, LaCo, Short-\nened LLaMA, and Joint Layer Drop‚Äîgenerally match\nor surpass embedding dimension reduction approaches\nlike LLM-Pruner and SliceGPT. This suggests that LLMs\nhave greater redundancy in depth than width , aligningwith prior work (Men et al., 2024). This strongly supports\nfocusing on depth pruning when designing SkipGPT.\n‚Ä¢Joint Layer Drop, the fine-grained variant of ShortGPT\nthat independently removes attention and MLP, outper-\nforms ShortGPT in most cases. This result validates our\nmotivation for emphasizing the necessity of Vertical Dy-\nnamics to achieve more effective pruning.\n‚Ä¢SkipGPT-RT significantly outperforms Joint Layer Drop\nacross all models in accuracy (reflecting LLMs‚Äô ability as\ngeneral-purpose task solvers) and perplexity (involving\nthe capability to generate coherent and fluent sentences).\nThis strongly supports our motivation for highlighting the\nnecessity of horizontal dynamics .\n5.2.Comparison between SkipGPT-RT-L and Baselines\nSkipGPT-RT-L ‚âàoriginal model performance Fig-\nure 4 shows training curves for SkipGPT-RT, SkipGPT-\nRT-L, static pruning methods with LoRA, and joint\nrouter+LoRA training for dynamic pruning for LLaMA2-\n7B and LLaMA2-13B. Results are summarized in Table 2.\nNotably, SkipGPT-RT achieves performance comparable to\nthe best fine-tuned baseline using router tuning alone. After\nLoRA fine-tuning, our method not only fully restores the\nmodel‚Äôs performance to the original level but even surpasses\nit (without fine-tuning), ranking second only to directly ap-\nplying LoRA to the original model for LLaMA2-7B. For\nthose aiming to quickly build a high-performing pruned\nmodel, router tuning is highly effective. Meanwhile, to fully\nmatch or exceed the original model, LoRA fine-tuning is\nan excellent choice. It is worth reiterating that the entire\nprocess for all models requires only a single A800 GPU.\nThe Effectiveness of Two-Stage Training Paradigm We\ndesignate SkipGPT-Joint , which refers to the variant where\nboth the router parameters and model parameters are trained\nsimultaneously, as the ablation baseline for the two-stage\ntraining paradigm. As shown in Table 2, its performance is\nsignificantly worse than SkipGPT. While it may seem intu-\nitive that directly adopting MoE‚Äôs joint training paradigm\ncould outperform two-stage training‚Äîsince the router and\nLoRA parameters can gradually adapt to each other‚Äôs rep-\nresentations‚Äîthe results tell a different story. Not only\ndoes SkipGPT-Joint underperform, but dynamic pruning\nmethods following the joint training paradigm consistently\nfail to achieve satisfactory results. This finding reinforces\nour argument in Section 3.5: in the joint training paradigm,\nthe randomly initialized router forces model parameters to\nadapt early on to a suboptimal, random routing strategy.\nThis misalignment disrupts the model‚Äôs established parame-\nter distribution, making it increasingly difficult for the router\nto identify critical modules. Over time, this leads to a rein-\nforcing feedback loop: as the model adapts to poor routing,\n7\n--- Page 8 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nModel Method RatioOBQA WinoGrande PIQA HeSw BoolQ ARC-E ARC-CAvg. Acc. ‚ÜëWT2 PTBAvg. PPL ‚ÜìAccNorm Acc Acc AccNorm Acc AccNorm AccNorm PPL PPL\nLLaMA2-7BDense 0.00% 44.20 74.19 78.07 78.93 71.62 81.36 52.47 68.69 5.47 20.83 13.15\nDense, LoRA 0.00% 44.80 74.27 78.02 78.96 79.02 81.73 53.07 69.98 5.48 20.58 13.03\nShortGPT, LoRA 25.0% 39.40 71.74 71.38 70.12 73.91 70.37 41.72 62.66 8.45 35.96 22.21\nShortened-PPL, LoRA 25.0% 36.40 57.38 74.59 64.58 63.21 69.70 39.16 57.86 7.71 31.26 19.49\nShortened-Taylor, LoRA 25.0% 37.20 70.24 71.38 70.38 72.35 70.24 43.43 62.17 8.38 33.60 20.99\nJoint Layer Drop, LoRA 23.9% 41.40 72.38 74.32 71.25 73.85 75.17 45.99 64.91 9.04 32.49 20.77\nLaCo, LoRA 25.0% 39.20 70.40 70.95 69.40 76.73 69.74 42.66 62.73 8.38 28.77 18.58\nLLM-Pruner, LoRA 25.3% 39.00 58.64 73.50 61.72 54.65 58.59 37.03 54.73 14.10 65.09 39.56\nSliceGPT, LoRA 25.4% 38.60 67.88 72.58 69.34 71.13 75.29 45.39 62.89 7.19 38.91 23.05\nMoD-D 25.0% 28.00 50.28 69.86 58.80 62.29 72.81 43.34 55.05 33.55 125.62 79.59\nD-LLM 25.6% 25.60 52.88 51.14 40.81 57.98 38.09 27.13 41.95 34.75 143.85 89.30\nSkipGPT-Joint 25.3% 24.40 50.83 50.98 62.81 43.36 66.50 38.23 48.16 10.12 346.05 178.09\nSkipGPT-RT-L 25.5% 44.00 74.90 78.24 77.80 77.58 81.40 52.56 69.50 5.82 21.26 13.54\nLLaMA2-13BDense 0.00% 45.20 76.16 79.11 82.23 80.52 84.68 59.47 72.48 4.88 28.92 16.90\nDense, LoRA 0.00% 45.20 76.24 79.11 82.26 80.06 84.97 59.72 72.51 4.80 28.90 16.85\nShortGPT, LoRA 25.0% 42.20 74.90 74.54 76.60 64.37 76.94 49.74 65.61 6.78 34.48 20.63\nJoint Layer Drop, LoRA 24.3% 44.80 73.56 77.48 77.20 77.83 79.97 51.96 68.97 5.58 27.34 16.46\nLaCo, LoRA 25.0% 40.60 65.98 76.99 72.19 67.37 75.63 45.31 63.44 6.81 29.53 18.17\nLLM-Pruner, LoRA 24.9% 44.00 65.82 76.99 74.13 59.88 72.64 45.82 62.76 9.83 71.49 40.66\nMoD-D 25.0% 37.20 67.25 71.49 51.83 54.13 78.24 50.34 58.64 39.31 39.69 39.50\nD-LLM 25.2% 28.00 57.06 52.18 60.35 62.78 58.54 35.92 50.69 13.09 31.69 22.39\nSkipGPT-Joint 25.2% 38.00 62.90 60.83 71.44 68.32 66.50 46.93 59.27 8.49 27.97 18.23\nSkipGPT-RT-L 25.4% 46.20 76.32 79.38 82.14 80.31 83.88 57.25 72.21 5.00 28.59 16.80\nLLaMA3.1-8BDense 0.00% 44.80 77.51 80.03 81.95 82.14 84.85 57.59 72.69 6.24 10.58 8.41\nDense, LoRA 0.00% 44.90 77.68 80.22 81.86 82.23 84.92 57.89 72.81 6.13 10.44 8.29\nShortGPT, LoRA 25.0% 38.40 70.96 73.94 69.23 72.05 68.01 43.86 62.35 11.13 16.64 13.89\nShortened-PPL, LoRA 25.0% 39.00 60.22 75.95 67.92 63.46 68.98 39.76 59.33 8.17 13.22 10.70\nShortened-Taylor, LoRA 25.0% 37.40 71.82 73.72 69.56 71.19 66.88 44.45 62.15 10.32 14.97 12.65\nJoint Layer Drop, LoRA 24.2% 35.00 69.30 72.47 64.11 67.95 59.81 38.23 58.12 14.32 20.14 17.23\nLaCo, LoRA 24.5% 36.40 69.46 71.93 66.50 76.24 64.73 41.13 60.91 10.77 17.23 14.00\nLLM-Pruner, LoRA 24.5% 37.20 64.09 75.46 65.72 64.56 62.42 35.67 57.87 20.42 34.87 27.65\nSliceGPT, LoRA 24.6% 34.40 61.56 66.70 56.96 72.39 50.08 31.48 53.37 9.22 26.42 17.82\nMoD-D 25.0% 31.60 52.41 64.25 50.44 50.28 37.67 28.24 44.98 34.21 43.29 38.75\nD-LLM 25.0% 30.20 52.49 57.40 37.64 50.36 37.12 28.16 41.91 40.12 132.44 86.28\nSkipGPT-Joint 25.3% 31.50 52.34 60.13 50.87 50.74 37.21 28.66 44.49 9.87 31.28 20.58\nSkipGPT-RT-L 25.5% 42.60 77.03 79.97 82.13 82.84 84.47 57.08 72.30 7.10 11.70 9.40\nLLaMA3.1-8BDense 0.00% 44.80 77.51 80.03 81.95 82.14 84.85 57.59 72.69 6.24 10.58 8.41\nDense, LoRA 0.00% 44.90 77.68 80.22 81.86 82.23 84.92 57.89 72.81 6.13 10.44 8.29\nShortGPT, LoRA 40.6% 32.00 67.32 68.61 58.43 65.38 53.37 35.32 54.35 18.35 30.65 24.50\nShortened-PPL, LoRA 40.6% 33.80 54.78 71.16 54.43 60.58 55.51 31.31 51.65 12.77 20.54 16.66\nShortened-Taylor, LoRA 40.6% 32.40 64.64 68.01 57.55 65.02 53.11 33.02 53.39 17.22 28.79 23.01\nJoint Layer Drop, LoRA 39.9% 28.60 52.96 60.23 35.10 56.91 36.99 36.99 43.97 21.65 33.25 27.45\nLaCo, LoRA 40.7% 30.20 56.27 65.13 43.99 62.05 43.27 26.79 46.81 16.43 24.66 20.55\nLLM-Pruner, LoRA 39.9% 31.80 55.01 70.24 51.34 56.18 50.08 27.90 48.94 30.43 40.67 35.55\nSliceGPT, LoRA 39.9% 28.20 55.41 60.61 44.15 67.52 40.70 25.34 45.99 14.87 33.24 24.06\nMoD-D 40.0% 33.00 51.38 65.56 54.01 50.28 38.09 30.20 46.07 40.42 52.76 46.59\nD-LLM 40.0% 31.80 51.78 58.54 48.30 50.00 44.82 26.88 44.59 52.78 231.66 142.22\nSkipGPT-Joint 40.3% 32.94 51.23 60.41 51.24 50.21 39.75 30.76 45.22 13.28 45.63 29.46\nSkipGPT-RT-L 40.3% 40.80 74.98 79.16 80.33 80.00 82.74 54.01 70.29 7.70 13.10 10.40\nTable 2. Comparison of SkipGPT-RT-L against LoRA-finetuned static pruning and dynamic pruning baselines. LoRA results for shortened-\nPPL, Shortened-Taylor, and SliceGPT on LLaMA2-13B are omitted as these methods fail to converge during recovery training.\nthe router struggles to refine its decisions, further degrad-\ning both its effectiveness and overall model performance.\nUltimately, this prevents the joint training paradigm from\nachieving strong results. In contrast, our two-stage training\nparadigm effectively mitigates these challenges, enabling\nthe pruned model to achieve superior performance compared\nto existing dynamic pruning methods.\n6. Routing Behavior Analysis\nBeyond efficiency, we explore if router tuning in SkipGPT-\nRT reveals deeper insights into the original LLMs. By ana-\nlyzing the router‚Äôs behavior, we aim to uncover meaningful\npatterns in how different modules contribute to inference.Comparison of Redundancy between Attention and\nMLP Modules Recent studies suggest that LLMs are\nmore resilient to the removal of self-attention layers than\nfeed-forward layers, indicating that the attention modules\nexhibit higher redundancy compared to MLP modules (Sid-\ndiqui et al., 2024; He et al., 2024; He et al., 2025). To ex-\nplore this further, we analyze five pruned models of LLaMA-\n2-13B, generated through router tuning under different tar-\nget sparsity levels T. For each pruned model, we measure\nthe average sparsity of the attention and MLP modules us-\ning 50 randomly selected sentences from the WT2 dataset\n(Merity et al., 2016). Here, sparsity is defined as the ra-\ntio of pruned modules to the total number of modules in\neither the attention or MLP. As shown in Figure 5, SkipGPT-\n8\n--- Page 9 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n0 2000 4000 6000 8000 10000\nTraining Steps1.52.02.53.03.54.04.55.0Language Modeling LossLaCo, LoRA\nJoint Layer Drop, LoRA\nShortened-PPL, LoRA\nShortened-T aylor, LoRA\nShortGPT, LoRA\nSliceGPT, LoRALLM-Pruner, LoRA\nMoD-D\nD-LLM\nSkipGPT-Joint\nDense, Lora\nSkipGPT-RT\nSkipGPT-RT-L\n(a) LLaMA2-7B\n0 2000 4000 6000 8000 10000\nTraining Steps1.52.02.53.03.54.04.55.0Language Modeling LossLaCo, LoRA\nJoint Layer Drop, LoRA\nShortGPT, LoRA\nLLM-Pruner, LoRA\nMoD-D\nD-LLM\nSkipGPT-Joint\nDense, Lora\nSkipGPT-RT\nSkipGPT-RT-L (b) LLaMA2-13B\nFigure 4. Training loss curves of LoRA-finetuned static and dynamic pruning baselines, and the two-stage training of SkipGPT.\n30 40 50 60 70\nUser-Defined Target Sparsity ()\n020406080Attention and MLP Sparsity (%)33.1945.1954.4166.8174.35\n26.3334.3046.0352.5764.53Attention Sparsity\nMLP Sparsity\nFigure 5. Average sparsity of the attention and MLP modules in\nfive pruned models of LLaMA-2-13B, generated through router\ntuning under different target sparsity levels T.\nRT consistently prunes more attention than MLP modules,\nacross both low and high pruning rates. This observation\naligns with previous findings and may indicate a structural\ninefficiency in the current Transformer architecture, which\nenforces a fixed pairing of one attention and one MLP mod-\nule per layer. We hypothesize that future LLMs could achieve\ngreater efficiency by revisiting this design and potentially\nreducing the proportion of attention modules.\nRedundancy Shifts in Attention and MLP Modules\nDel Corro et al. (2023) argues that as the context grows\nwith the sequence, later tokens become more predictive and\ntherefore require less computation. Meanwhile, He et al.\n(2025) shows that the MLP module has higher redundancy\nduring the decoding phase compared to the pre-filling phase,\nwhile attention redundancy remains nearly the same in both\nphases. We investigate how redundancy shifts within the\nattention and MLP modules by analyzing a 45% sparsity\nmodel obtained through router tuning on LLaMA2-13B.\nWe randomly select sentences from RedPajama, truncate\nthem to the model‚Äôs maximum context length, and apply a\n100-token sliding window across each sentence. At each\nstep, we record the activation ratios of attention and MLP\nwithin the sliding window and average the results over 50\nexamples. The final results appear in Figure 6. Contrary to\nprevious findings, we observe that as context grows, later\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nPosition Index (Start of Sliding Window, k)0.400.450.500.550.600.650.700.750.80Activation Ratio\nAverage Attention Activation\nAverage MLP ActivationFigure 6. Redundancy shifts in attention and MLP modules of a\n45% sparsity LLaMA2-13B model obtained via router tuning as\ncontext length grows.\ntokens require more attention computation but less MLP\ncomputation. We hypothesize that in the early stages, the\nmodel has not yet determined the task and has limited con-\ntextual information. As a result, it relies more on complex\nMLP computations for task identification and less on at-\ntention for context extraction. Later, as the task becomes\nclear and context accumulates, the model shifts to increased\nattention computation while reducing MLP computation.\nDeveloping techniques that dynamically adjust computation\npresents an exciting direction for future research.\n7. Conclusion\nIn this work, we introduce SkipGPT , a novel dynamic prun-\ning framework that addresses the inefficiencies of static\nlayer pruning by incorporating horizontal and vertical dy-\nnamics . SkipGPT dynamically allocates computation per\ntoken and decouples attention and MLP pruning, enabling\nmore fine-grained optimization. To stabilize training, we\npropose a two-stage training paradigm , where the router\nis first tuned alone before fine-tuning the model with LoRA.\nExperiments show that the pruned model can fully restore\nits performance , even surpassing the original model de-\nspite a 40% parameter reduction. Furthermore, our router\nanalysis reveals key insights into module redundancy and\ntoken-level compute allocation , highlighting potential di-\nrections for future model efficiency improvements.\n9\n--- Page 10 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nImpact Statement\nOur work, SkipGPT, introduces a novel dynamic pruning\nframework that significantly improves the efficiency of large\nlanguage models (LLMs) by adapting computation to token\ncomplexity. By decoupling MLP and attention pruning and\nintroducing a two-stage training paradigm, our approach\nenhances computational efficiency while preserving, and\neven surpassing, original model performance. This reduces\nthe energy demands of LLMs, making them more sustain-\nable and accessible for deployment in resource-constrained\nenvironments, while also providing new insights into model\nredundancy and architectural optimization.\nReferences\nAn, Y ., Zhao, X., Yu, T., Tang, M., and Wang, J. Fluctuation-\nbased adaptive structured pruning for large language mod-\nels. In Proceedings of the AAAI Conference on Artificial\nIntelligence , volume 38, pp. 10865‚Äì10873, 2024.\nAshkboos, S., Croci, M. L., Nascimento, M. G. d., Hoefler,\nT., and Hensman, J. Slicegpt: Compress large language\nmodels by deleting rows and columns. arXiv preprint\narXiv:2401.15024 , 2024.\nBartol, Thomas M, J., Bromer, C., Kinney, J., Chirillo,\nM. A., Bourne, J. N., Harris, K. M., and Sejnowski,\nT. J. Nanoconnectomic upper bound on the variabil-\nity of synaptic plasticity. eLife , 4:e10778, nov 2015.\nISSN 2050-084X. doi: 10.7554/eLife.10778. URL\nhttps://doi.org/10.7554/eLife.10778 .\nBengio, Y ., L ¬¥eonard, N., and Courville, A. Estimating or\npropagating gradients through stochastic neurons for con-\nditional computation. arXiv preprint arXiv:1308.3432 ,\n2013.\nBisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence ,\nvolume 34, pp. 7432‚Äì7439, 2020.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\nlut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\nD., Castellon, R., Chatterji, N., Chen, A., Creel, K.,\nDavis, J. Q., Demszky, D., Donahue, C., Doumbouya,\nM., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh,\nK., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel,\nK., Goodman, N., Grossman, S., Guha, N., Hashimoto,\nT., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu,\nK., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P.,\nKaramcheti, S., Keeling, G., Khani, F., Khattab, O., Koh,\nP. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A.,\nLadhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li,X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchan-\ndani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan,\nA., Narayanan, D., Newman, B., Nie, A., Niebles, J. C.,\nNilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadim-\nitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C.,\nRaghunathan, A., Reich, R., Ren, H., Rong, F., Roohani,\nY ., Ruiz, C., Ryan, J., R ¬¥e, C., Sadigh, D., Sagawa, S.,\nSanthanam, K., Shih, A., Srinivasan, K., Tamkin, A.,\nTaori, R., Thomas, A. W., Tram `er, F., Wang, R. E., Wang,\nW., Wu, B., Wu, J., Wu, Y ., Xie, S. M., Yasunaga, M.,\nYou, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X.,\nZhang, Y ., Zheng, L., Zhou, K., and Liang, P. On the\nopportunities and risks of foundation models, 2022. URL\nhttps://arxiv.org/abs/2108.07258 .\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165 , 2020.\nCai, W., Jiang, J., Wang, F., Tang, J., Kim, S., and Huang,\nJ. A survey on mixture of experts, 2024. URL https:\n//arxiv.org/abs/2407.06204 .\nChen, X., Hu, Y ., and Zhang, J. Compressing large language\nmodels by streamlining the unimportant layer. arXiv\npreprint arXiv:2403.19135 , 2024a.\nChen, X., Hu, Y ., and Zhang, J. Compressing large language\nmodels by streamlining the unimportant layer. arXiv\npreprint arXiv:2403.19135 , 2024b.\nChen, X., Sun, Z., Guo, W., Zhang, M., Chen, Y ., Sun,\nY ., Su, H., Pan, Y ., Klakow, D., Li, W., and Shen, X.\nUnveiling the key factors for distilling chain-of-thought\nreasoning, 2025. URL https://arxiv.org/abs/\n2502.18001 .\nChen, Y ., Pan, X., Li, Y ., Ding, B., and Zhou, J. Ee-\nllm: Large-scale training and inference of early-exit\nlarge language models with 3d parallelism, 2024c. URL\nhttps://arxiv.org/abs/2312.04916 .\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\nN., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\nG., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,\nS., Michalewski, H., Garcia, X., Misra, V ., Robinson,\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,\nH., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,\nAgrawal, S., Omernick, M., Dai, A. M., Pillai, T. S.,\nPellat, M., Lewkowycz, A., Moreira, E., Child, R., Polo-\nzov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,\nM., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.,\nEck, D., Dean, J., Petrov, S., and Fiedel, N. Palm:\n10\n--- Page 11 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nScaling language modeling with pathways, 2022. URL\nhttps://arxiv.org/abs/2204.02311 .\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 , 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457 , 2018.\nComputer, T. Redpajama: an open dataset for training\nlarge language models. 2023. URL https://github.\ncom/togethercomputer/RedPajama-Data .\nDel Corro, L., Del Giorno, A., Agarwal, S., Yu, B., Awadal-\nlah, A., and Mukherjee, S. Skipdecode: Autoregressive\nskip decoding with batching and caching for efficient llm\ninference. arXiv preprint arXiv:2307.02628 , 2023.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S.,\nSun, A., Wang, Y ., and Wang, Z. Not all layers of llms\nare necessary during inference, 2024. URL https://\narxiv.org/abs/2403.02181 .\nFrantar, E. and Alistarh, D. Sparsegpt: Massive language\nmodels can be accurately pruned in one-shot. In Inter-\nnational Conference on Machine Learning , pp. 10323‚Äì\n10337. PMLR, 2023.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac‚Äôh, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602 .\nGeva, M., Schuster, R., Berant, J., and Levy, O. Transformer\nfeed-forward layers are key-value memories, 2021. URL\nhttps://arxiv.org/abs/2012.14913 .\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\nRoberts, D. A. The unreasonable ineffectiveness of the\ndeeper layers. arXiv preprint arXiv:2403.17887 , 2024.\nGumbel, E. J. Statistical theory of extreme values and some\npractical applications: a series of lectures , volume 33.\nUS Government Printing Office, 1954.He, S., Sun, G., Shen, Z., and Li, A. What matters in\ntransformers? not all attention is needed, 2024. URL\nhttps://arxiv.org/abs/2406.15786 .\nHe, Z., Yizhen Yao, Pengfei Zuo, Bin Gao, Qinya Li, Zhen-\nzhe Zheng, and Fan Wu. Adaskip: Adaptive sublayer skip-\nping for accelerating long-context llm inference, 2025.\nURL https://arxiv.org/abs/2501.02336 .\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models, 2021. URL https://arxiv.\norg/abs/2106.09685 .\nJang, E., Gu, S., and Poole, B. Categorical reparameteriza-\ntion with gumbel-softmax. In International Conference\non Learning Representations , 2022.\nJiang, Y ., Wang, H., Xie, L., Zhao, H., zhang chao, Qian,\nH., and Lui, J. C. D-llm: A token adaptive comput-\ning resource allocation strategy for large language mod-\nels. In Proceedings of the 38th Conference on Neu-\nral Information Processing Systems (NeurIPS 2024) ,\n2024. URL https://neurips.cc/virtual/\n2024/poster/94977 .\nKim, B.-K., Kim, G., Kim, T.-H., Castells, T., Choi, S.,\nShin, J., and Song, H.-K. Shortened llama: A simple\ndepth pruning for large language models. arXiv preprint\narXiv:2402.02834 , 11, 2024.\nKobayashi, G., Kuribayashi, T., Yokoi, S., and Inui, K.\nAttention is not only a weight: Analyzing transformers\nwith vector norms. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing\n(EMNLP) , pp. 7057‚Äì7075, 2020.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000) , pp. 1207‚Äì1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\ngiant models with conditional computation and automatic\nsharding, 2020. URL https://arxiv.org/abs/\n2006.16668 .\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019. URL https://arxiv.org/abs/\n1711.05101 .\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the struc-\ntural pruning of large language models. Advances in\nneural information processing systems , 36:21702‚Äì21720,\n2023.\n11\n--- Page 12 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nMaddison, C. J., Tarlow, D., and Minka, T. A* sam-\npling, 2015. URL https://arxiv.org/abs/\n1411.0030 .\nMarcus, M., Santorini, B., and Marcinkiewicz, M. A. Build-\ning a large annotated corpus of english: The penn tree-\nbank. Computational linguistics , 19(2):313‚Äì330, 1993.\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y ., Han,\nX., and Chen, W. Shortgpt: Layers in large language mod-\nels are more redundant than you expect. arXiv preprint\narXiv:2403.03853 , 2024.\nMeng, K., Bau, D., Andonian, A., and Belinkov, Y . Locat-\ning and editing factual associations in gpt, 2023. URL\nhttps://arxiv.org/abs/2202.05262 .\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\nPointer sentinel mixture models. arXiv preprint\narXiv:1609.07843 , 2016.\nMerullo, J., Eickhoff, C., and Pavlick, E. Language models\nimplement simple word2vec-style vector arithmetic, 2024.\nURL https://arxiv.org/abs/2305.16130 .\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can\na suit of armor conduct electricity? a new dataset\nfor open book question answering. arXiv preprint\narXiv:1809.02789 , 2018.\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,\nN., Henighan, T., Mann, B., Askell, A., Bai, Y ., Chen,\nA., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds,\nZ., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,\nLovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J.,\nKaplan, J., McCandlish, S., and Olah, C. In-context learn-\ning and induction heads. Transformer Circuits Thread ,\n2022. https://transformer-circuits.pub/2022/in-context-\nlearning-and-induction-heads/index.html.\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,\nAkkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J.,\nAltman, S., Anadkat, S., Avila, R., Babuschkin, I., Bal-\naji, S., Balcom, V ., Baltescu, P., Bao, H., Bavarian, M.,\nBelgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G.,\nBerner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman,\nA.-L., Brockman, G., Brooks, T., Brundage, M., Button,\nK., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,\nC., Carmichael, R., Chan, B., Chang, C., Chantzis, F.,\nChen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess,\nB., Cho, C., Chu, C., Chung, H. W., Cummings, D., Cur-\nrier, J., Dai, Y ., Decareaux, C., Degry, T., Deutsch, N.,\nDeville, D., Dhar, A., Dohan, D., Dowling, S., Dunning,\nS., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus,\nL., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L.,\nGeorges, E., Gibson, C., Goel, V ., Gogineni, T., Goh, G.,\nGontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,Greene, R., Gross, J., Gu, S. S., Guo, Y ., Hallacy, C., Han,\nJ., Harris, J., He, Y ., Heaton, M., Heidecke, J., Hesse,\nC., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B.,\nHsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S.,\nJang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S.,\nJonn, B., Jun, H., Kaftan, T., ≈Åukasz Kaiser, Kamali, A.,\nKanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L.,\nKim, J. W., Kim, C., Kim, Y ., Kirchner, J. H., Kiros, J.,\nKnight, M., Kokotajlo, D., ≈Åukasz Kondraciuk, Kondrich,\nA., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V .,\nLampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D.,\nLi, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T.,\nLowe, R., Lue, P., Makanju, A., Malfacini, K., Manning,\nS., Markov, T., Markovski, Y ., Martin, B., Mayer, K.,\nMayne, A., McGrew, B., McKinney, S. M., McLeavey, C.,\nMcMillan, P., McNeil, J., Medina, D., Mehta, A., Menick,\nJ., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V .,\nMorikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,\nM¬¥ely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan,\nA., Ngo, R., Noh, H., Ouyang, L., O‚ÄôKeefe, C., Pachocki,\nJ., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,\nG., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,\nA., Perelman, A., de Avila Belbute Peres, F., Petrov, M.,\nde Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M.,\nPong, V . H., Powell, T., Power, A., Power, B., Proehl, E.,\nPuri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C.,\nReal, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H.,\nRyder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry,\nG., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D.,\nSheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam,\nP., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K.,\nSohl, I., Sokolowsky, B., Song, Y ., Staudacher, N., Such,\nF. P., Summers, N., Sutskever, I., Tang, J., Tezak, N.,\nThompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E.,\nTuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone,\nA., Vijayvergiya, A., V oss, C., Wainwright, C., Wang,\nJ. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann,\nC., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wi-\nethoff, M., Willner, D., Winter, C., Wolrich, S., Wong,\nH., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu,\nT., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R.,\nZhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J.,\nZhuk, W., and Zoph, B. Gpt-4 technical report, 2024.\nURL https://arxiv.org/abs/2303.08774 .\nRaposo, D., Ritter, S., Richards, B., Lillicrap, T.,\nHumphreys, P. C., and Santoro, A. Mixture-of-depths:\nDynamically allocating compute in transformer-based lan-\nguage models. arXiv preprint arXiv:2404.02258 , 2024.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y .\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM , 64(9):99‚Äì106,\n2021.\n12\n--- Page 13 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A.,\nJones, M., Bergeron, W., Kepner, J., Tiwari, D., and\nGadepally, V . From words to watts: Benchmarking the\nenergy costs of large language model inference, 2023.\nURL https://arxiv.org/abs/2310.03003 .\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\nTran, V ., Tay, Y ., and Metzler, D. Confident adaptive\nlanguage modeling. Advances in Neural Information\nProcessing Systems , 35:17456‚Äì17472, 2022.\nSiddiqui, S. A., Dong, X., Heinrich, G., Breuel, T., Kautz,\nJ., Krueger, D., and Molchanov, P. A deeper look at depth\npruning of llms. arXiv preprint arXiv:2407.16286 , 2024.\nSong, J., Oh, K., Kim, T., Kim, H., Kim, Y ., and Kim, J.-J.\nSleb: Streamlining llms through redundancy verification\nand elimination of transformer blocks. arXiv preprint\narXiv:2402.09025 , 2024.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023a. URL https://arxiv.org/abs/\n2302.13971 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288 ,\n2023b.\nVarshney, N., Chatterjee, A., Parmar, M., and Baral, C.\nAccelerating llama inference by enabling intermediate\nlayer decoding via instruction tuning with lite, 2023. URL\nhttps://arxiv.org/abs/2310.18581 .\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.\nAttention is all you need. In Guyon, I., Luxburg, U. V .,\nBengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems , volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\npdf.\nWan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y ., Liu, J.,\nQu, Z., Yan, S., Zhu, Y ., Zhang, Q., Chowdhury, M.,\nand Zhang, M. Efficient large language models: A sur-\nvey, 2024. URL https://arxiv.org/abs/2312.\n03863 .\nWang, X., Yu, F., Dou, Z.-Y ., Darrell, T., and Gonzalez,\nJ. E. Skipnet: Learning dynamic routing in convolutionalnetworks, 2018. URL https://arxiv.org/abs/\n1711.09485 .\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023. URL https://arxiv.org/abs/\n2201.11903 .\nXia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama:\nAccelerating language model pre-training via structured\npruning. arXiv preprint arXiv:2310.06694 , 2023.\nXin, H., Sun, Y ., Wang, C., and Xiong, H. Llmcdsr: Enhanc-\ning cross-domain sequential recommendation with large\nlanguage models. ACM Transactions on Information\nSystems , 2025.\nYang, Y ., Cao, Z., and Zhao, H. Laco: Large lan-\nguage model pruning via layer collapse. arXiv preprint\narXiv:2402.11187 , 2024.\nYom Din, A., Karidi, T., Choshen, L., and Geva, M. Jump\nto conclusions: Short-cutting transformers with linear\ntransformations. In Calzolari, N., Kan, M.-Y ., Hoste, V .,\nLenci, A., Sakti, S., and Xue, N. (eds.), Proceedings of the\n2024 Joint International Conference on Computational\nLinguistics, Language Resources and Evaluation (LREC-\nCOLING 2024) , pp. 9615‚Äì9625, Torino, Italia, May 2024.\nELRA and ICCL. URL https://aclanthology.\norg/2024.lrec-main.840 .\nZellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi,\nY . Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830 , 2019.\nZeng, D., Du, N., Wang, T., Xu, Y ., Lei, T., Chen, Z., and\nCui, C. Learning to skip for language modeling, 2023.\nURL https://arxiv.org/abs/2311.15436 .\nZhang, Y ., Li, Y ., Wang, X., Shen, Q., Plank, B., Bischl,\nB., Rezaei, M., and Kawaguchi, K. Finercut: Finer-\ngrained interpretable layer pruning for large language\nmodels, 2024. URL https://arxiv.org/abs/\n2405.18218 .\nZhao, A., Ye, F., Fu, J., and Shen, X. Unveiling in-context\nlearning: A coordinate system to understand its working\nmechanism. arXiv preprint arXiv:2407.17011 , 2024.\nZhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., and\nCheng, Y . Llama-moe: Building mixture-of-experts from\nllama with continual pre-training, 2024. URL https:\n//arxiv.org/abs/2406.16554 .\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\n13\n--- Page 14 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In The IEEE International\nConference on Computer Vision (ICCV) , December 2015.\n14\n--- Page 15 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nA. Related Work\nStatic Pruning. Static pruning refers to a kind of approach where the computation of the pruned LLMs remains invariant to\nthe input instances. SparseGPT (Frantar & Alistarh, 2023) simplifies the pruning problem by turning it into large-scale sparse\nregression tasks, which are efficiently solved using a novel approximate solver. FLAP (An et al., 2024) and LLM-Pruner\n(Ma et al., 2023) reduce network width by eliminating coupled structures while keeping the number of layers unchanged.\nSheared-LLaMA (Xia et al., 2023) learns pruning masks at various granularities, from global ones like layers and hidden\ndimensions to local ones like attention heads and intermediate dimensions. SliceGPT (Ashkboos et al., 2024) reduces\nthe network‚Äôs embedding dimension by replacing each weight matrix with a smaller dense matrix. Recent works (Song\net al., 2024; Gromov et al., 2024; Kim et al., 2024; Chen et al., 2024a; Yang et al., 2024; Siddiqui et al., 2024) demonstrate\nthat it is possible to selectively drop blocks from a range of pretrained language models, sparking community interest\nin depth pruning. In this context, Zhang et al. (2024) and He et al. (2024) investigate the decoder layers, treating the\nself-attention layers and FFN layers as independent components to be pruned separately, and observe a preference for\npruning the self-attention layers. Despite significant advancements in static pruning, substantial recovery fine-tuning is often\nnecessary to preserve performance post-pruning, making the process both costly and challenging to scale.\nDynamic Pruning. Dynamic pruning refers to another kind of approach where the pruning of unimportant layers is\ndynamically determined based on the specific input instance. A common technique in this context is Early Exit (Schuster\net al., 2022; Varshney et al., 2023; Del Corro et al., 2023; Yom Din et al., 2024; Chen et al., 2024c; Fan et al., 2024). This\napproach dynamically evaluates whether to continue processing subsequent transformer blocks. Notably, transformer blocks\nthat produce predictions matching the final token output of LLMs are typically located towards the end of the model. As a\nresult, extensive training is often required to adapt LLMs for the effective use of early exit mechanisms. Therefore, early exit\nstrategies have been rarely explored in larger SOTA LLMs. Another prominent technique is Skip Layer , which dynamically\nskips the execution of intermediate layers (or modules) for a given input token. This is achieved through mechanisms\nsuch as a gating function (Wang et al., 2018; Raposo et al., 2024) or a binary router (Zeng et al., 2023; Jiang et al., 2024).\nFor instance, Mixture-of-Depths (MoD) (Raposo et al., 2024) determines which tokens to process using a top- krouting\nmechanism. Essentially, SkipGPT aligns with the Skip Layer paradigm, where the execution of layers for each input token is\ndynamically determined. To the best of our knowledge, ours is the first work to simultaneously address both horizontal and\nvertical dynamics.\nB. Training Algorithm of SkipGPT\nThe detailed two-stage training paradigm is outlined in Algorithm 1.\nC. Detailed Descriptions of All Baseline Methods\nShortGPT is a structured pruning method that removes redundant layers in LLMs based on a novel importance metric\ncalled Block Influence (BI). By analyzing hidden state transformations, ShortGPT assigns BI scores to measure each layer‚Äôs\ncontribution to model performance. Layers with lower BI scores are identified as redundant and pruned in ascending\norder. This simple yet effective approach significantly reduces model size and inference cost while maintaining competitive\nperformance. Unlike complex pruning techniques, ShortGPT demonstrates that LLMs contain substantial layer-wise\nredundancy, enabling efficient compression without additional fine-tuning.\nShortened LLaMA is a depth pruning method that reduces the computational cost of LLMs by removing entire Trans-\nformer layers while keeping the remaining architecture intact. It determines layer importance using Perplexity (PPL)\nand Taylor Expansion, pruning less significant layers in a one-shot manner. Unlike width pruning, which reduces weight\nmatrices but struggles to accelerate inference under memory constraints, Shortened LLaMA achieves significant speedups,\nparticularly in resource-limited settings. To recover pruned models, it employs LoRA-based fine-tuning for moderate\npruning and Continued Pretraining (CPT) for aggressive pruning. This method provides an efficient way to compress LLMs\nwhile maintaining strong performance.\nLaCo (Layer Collapse) is a structured pruning method that progressively merges deeper layers into earlier ones, reducing\nmodel depth while preserving output representations. Instead of removing layers outright, LaCo employs Reserving-\nDifferences-while-Seeking-Common (RDSC) Layer Merge, which integrates parameter differences from consecutive layers\n15\n--- Page 16 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\nAlgorithm 1 Training Process of SkipGPT\nRequire: Pretrained model M, dataset D, target sparsity T, router parameters Œ∏, learning rates Œ∑1(router) and Œ∑2(LoRA),\nmaximum steps S1andS2\n1:Initialize router parameters Œ∏for each module\n2:Initialize LoRA parameters (Optional)\n3:Stage 1: Router Tuning\n4:forsteps= 1toS1do\n5: X‚àºsample (D)(Sample batch)\n6: fortoken tinXdo\n7: formodule l= 1toLdo\n8: rt\nl‚ÜêWT\nŒ∏xt\nlwith‚àÇLall\n‚àÇWŒ∏being active\n9: gt\nl‚àºGumbel-Softmax (rt\nl)\n10: xt\nl+1‚Üêgt\nl[1]¬∑(fl(xt\nl) +xt\nl) +gt\nl[0]¬∑xt\nl\n11: end for\n12: end for\n13: r‚ÜêP\nt,lgt\nl[0]\nS√óL\n14:Lall‚Üê L lm+Œ±|T ‚àí r|\n15: Update Œ∏‚ÜêŒ∏‚àíŒ∑1‚àáŒ∏Lall\n16:end for\n17:Stage 2: LoRA Fine-Tuning (Optional)\n18:forsteps= 1toS2do\n19: X‚àºsample (D)\n20: fortoken tinXdo\n21: formodule l= 1toLdo\n22: rt\nl‚ÜêWT\nŒ∏xt\nlwith‚àÇLlm\n‚àÇWŒ∏= 0(routers frozen)\n23: at\nl‚Üêargmax (rt\nl)\n24: ifat\nl= 1then\n25: xt\nl+1‚Üêfl(xt\nl) +xt\nl\n26: else\n27: xt\nl+1‚Üêxt\nl\n28: end if\n29: end for\n30: end for\n31: Compute Llm\n32: Update LoRA parameters using Œ∑2and‚àáLoRALlm\n33:end for\nReturn: Pruned model M‚Ä≤\ninto a prior layer, maintaining structural integrity. To minimize performance degradation, it uses few-shot calibration\nsamples to ensure representation similarity. This approach enables 30-50% layer reduction without retraining, significantly\nlowering computational costs while retaining strong performance. Additionally, post-training on pruned models further\nrestores accuracy, making LaCo a highly effective structured pruning technique for large language models.\nJoint Layer Drop is a structured pruning method that enhances the efficiency of Transformer-based LLMs by jointly\npruning Attention and MLP layers based on a similarity-based metric. This method first removes redundant Attention\nlayers, as they exhibit significant redundancy while maintaining performance. Once the least important Attention layers are\npruned, it selectively removes MLP layers to further compress the model. By dynamically balancing the pruning of both\ncomponents, Joint Layer Drop achieves higher compression ratios with minimal accuracy degradation, making it a highly\neffective approach for structured model reduction.\nLLM-Pruner is a structured pruning method designed for task-agnostic compression of LLMs, aiming to reduce\nmodel size while preserving their general-purpose capabilities. It employs dependency-based structural pruning, where\n16\n--- Page 17 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\ninterdependent components are grouped and pruned together to minimize disruption. Importance estimation is performed\nusing gradient-based and Hessian-based metrics, ensuring the least impactful components are removed. Unlike traditional\ncompression methods that require extensive retraining, LLM-Pruner enables efficient post-training recovery using LoRA-\nbased fine-tuning, requiring only 50K public samples. This approach significantly reduces computational overhead while\nmaintaining strong zero-shot performance across multiple tasks.\nSliceGPT is a post-training sparsification method that reduces the computational and memory demands of LLMs by\nremoving entire rows and columns of weight matrices, effectively shrinking the embedding dimension while maintaining\ndense matrix operations for efficient execution. It leverages computational invariance, applying orthogonal matrix trans-\nformations to ensure minimal performance degradation. Using Principal Component Analysis (PCA), SliceGPT projects\nactivation signals onto their principal components, allowing redundant dimensions to be pruned. This method achieves up to\n30% compression on LLaMA-2, OPT, and Phi-2 models while retaining over 90% of the original model‚Äôs accuracy, leading\nto significant inference speedups and reduced hardware requirements without additional fine-tuning.\nMixture-of-Depths (MoD) is a conditional computation method that dynamically allocates compute across model depth,\nreducing unnecessary computations in transformer-based LLMs. Unlike standard transformers, which apply the same\namount of compute to all tokens at every layer, MoD employs a top- krouting mechanism to selectively process only the\nmost important tokens in each layer, skipping others via residual connections. This allows the model to optimize compute\nexpenditure per token, maintaining performance while significantly reducing FLOPs per forward pass. By ensuring a static\ncomputation graph with predictable efficiency gains, MoD achieves up to 50% faster inference while matching or surpassing\nthe performance of equivalent full-compute transformers.\nD-LLM is a dynamic inference framework for LLMs that adaptively allocates computational resources at the token level,\noptimizing efficiency without compromising performance. It introduces a dynamic decision module before each transformer\nlayer, determining whether a token should execute the layer or be skipped, thereby reducing unnecessary computation for\nless critical tokens and simpler tasks. To maintain compatibility with KV-cache mechanisms, D-LLM employs a KV-cache\neviction policy, excluding skipped layers from subsequent attention calculations, which not only reduces storage overhead\nbut also ensures smooth deployment in real-world applications. Experimentally, D-LLM achieves up to 50% reduction in\ncomputational cost across various NLP tasks while maintaining strong accuracy, making it a highly effective solution for\nresource-constrained environments.\nD. Exploring Pruning Scaling Laws with SkipGPT-RT\nDue to its significantly superior performance over the baseline, we believe that SkipGPT-RT is capable of identifying optimal\nor near-optimal routing solutions for a given sparsity ratio. This capability establishes SkipGPT not only as an effective\npruning method but also as a reliable probing tool for exploring the pruning scaling laws in existing LLMs. To this end, in\nthis section, we employ SkipGPT-RT as a ‚Äúprobe‚Äù to analyze both LLaMA2-7B and LLaMA2-13B, using the SOTA static\npruning method, Joint Layer Drop, for comparison. Figure 7 highlights several key findings:\n‚Ä¢Static pruning methods lack scalability: While static pruning methods are simple and effective in some scenarios,\ntheir ability to preserve model performance diminishes significantly under high sparsity levels. For instance, in the case\nof LLaMA2-13B, SkipGPT-RT demonstrates significantly lower PPL even at 70% sparsity, outperforming Joint Layer\nDrop, which struggles to maintain comparable performance even at a much lower sparsity of 50%.\n‚Ä¢The surprising redundancy in LLMs: Our analysis reveals that LLMs exhibit far greater redundancy than expected.\nFor instance, LLaMA2-13B only begins to show a noticeable increase in PPL at an 80% sparsity level. This phenomenon\nmay stem from two key factors: (1) the inefficiency of the current transformer architecture, which applies uniform\ncomputation to all tokens regardless of their importance, and (2) the concentration of critical information within a small\nsubset of modules during pretraining. We hope this finding offers valuable insights for designing future architectures\nand pretraining strategies.\n17\n--- Page 18 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n20 30 40 50 60\nSparsity0.00.20.40.60.81.01.21.4PPL1e4\n16.4 126.9 54.62044.0\n148.414387.0Joint Layer Drop\nSkipGPT-RT\n(a) LLaMA2-7B\n30 40 50 60 70 80\nSparsity0123456789PPL1e3\n21.3510.2\n33.61870.1\n111.58425.1\n1540.78495.4\nJoint Layer Drop\nSkipGPT-RT (b) LLaMA2-13B\nFigure 7. Perplexity (PPL) of Joint Layer Drop and SkipGPT-RT under different sparsity levels.\n0.00.20.40.60.81.0\nAttn 30MLP 30Attn 31MLP 31Attn 32MLP 32\n0.99 0.97 0.98 0.99 1.00 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 1.000.86 0.88 0.90 0.93 0.97 0.91 0.87 0.92 0.89 0.90 0.93 0.88 0.92 0.94 0.900.98 0.96 0.98 0.98 0.98 0.98 0.97 0.98 0.98 0.98 0.98 0.99 0.98 0.95 0.990.80 0.80 0.81 0.69 0.90 0.75 0.63 0.83 0.67 0.72 0.71 0.75 0.75 0.85 0.870.91 0.92 0.95 0.97 0.90 0.98 0.98 0.92 0.96 0.97 0.90 0.98 0.97 0.96 0.920.05 0.11 0.02 0.01 0.04 0.08 0.13 0.07 0.05 0.06 0.04 0.08 0.10 0.25 0.16\n...\nhe\n'\n d\nseen\nthe\nmovie\n almost\nby\nmistake\n,\nconsidering\nhe\nwas\na\nlittle\nAttn 1MLP 1Attn 2MLP 2Attn 3MLP 3Attn 4MLP 4Attn 5MLP 5\n0.01 0.02 0.05 0.20 0.00 0.24 0.33 0.02 0.35 0.07 0.34 0.04 0.04 0.01 0.250.22 0.21 0.55 0.52 0.52 0.48 0.60 0.74 0.65 0.71 0.63 0.74 0.78 0.75 0.690.14 0.21 0.14 0.20 0.44 0.32 0.41 0.32 0.28 0.25 0.36 0.38 0.29 0.35 0.310.44 0.50 0.43 0.54 0.48 0.51 0.49 0.61 0.51 0.54 0.50 0.68 0.67 0.54 0.440.94 0.79 0.59 0.67 0.70 0.57 0.78 0.56 0.71 0.78 0.77 0.51 0.65 0.77 0.810.56 0.60 0.47 0.48 0.78 0.71 0.74 0.49 0.53 0.67 0.55 0.60 0.66 0.66 0.580.84 0.67 0.61 0.60 0.49 0.63 0.72 0.59 0.51 0.72 0.62 0.52 0.62 0.64 0.730.60 0.51 0.41 0.49 0.56 0.61 0.56 0.46 0.38 0.58 0.51 0.59 0.66 0.56 0.590.93 0.78 0.72 0.62 0.55 0.80 0.58 0.75 0.57 0.65 0.72 0.66 0.73 0.76 0.820.29 0.42 0.54 0.41 0.42 0.51 0.49 0.36 0.41 0.46 0.43 0.47 0.57 0.60 0.55\nFigure 8. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B.\nE. Additional Case Studies on Motivation\nIn this section, we present additional case studies that illustrate Token-Wise Cosine Similarities Across Modules in\nLLaMA2-7B, as shown in Figures 8 and 9. For LLaMA2-13B, see Figures 10 and 11.\n18\n--- Page 19 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n0.00.20.40.60.81.0\nAttn 30MLP 30Attn 31MLP 31Attn 32MLP 32\n0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.94 0.99 1.00 0.990.84 0.94 0.95 0.91 0.93 0.87 0.95 0.93 0.89 0.94 0.94 0.85 0.89 0.93 0.930.98 0.99 1.00 0.98 0.99 0.97 0.99 0.99 0.98 0.97 0.99 0.96 0.98 0.98 0.980.76 0.69 0.85 0.74 0.84 0.71 0.84 0.86 0.70 0.86 0.86 0.82 0.70 0.83 0.850.92 0.97 0.96 0.97 0.95 0.92 0.94 0.97 0.98 0.90 0.88 0.95 0.98 0.91 0.960.04 0.02 0.04 0.03 0.08 0.08 0.06 0.17 0.04 0.06 0.06 0.24 0.04 0.10 0.09\n...\nshe\nliked\nto\nthink\n being\nsurrounded\nby\nadult\ns\nand\nolder\nk\nidswas\n one\nAttn 1MLP 1Attn 2MLP 2Attn 3MLP 3Attn 4MLP 4Attn 5MLP 5\n0.02 0.16 0.00 0.12 0.13 0.35 0.01 0.26 0.02 0.00 0.30 0.07 0.16 0.04 0.100.23 0.34 0.60 0.61 0.69 0.53 0.69 0.62 0.77 0.64 0.68 0.61 0.73 0.73 0.790.13 0.11 0.25 0.21 0.23 0.32 0.25 0.37 0.09 0.46 0.44 0.45 0.22 0.40 0.340.47 0.53 0.53 0.50 0.55 0.52 0.59 0.42 0.34 0.61 0.57 0.68 0.23 0.67 0.600.93 0.89 0.25 0.48 0.63 0.71 0.22 0.75 0.60 0.73 0.84 0.82 0.58 0.62 0.810.54 0.53 0.39 0.47 0.51 0.66 0.39 0.75 0.59 0.72 0.63 0.68 0.61 0.62 0.560.77 0.73 0.69 0.66 0.68 0.72 0.62 0.81 0.65 0.69 0.78 0.77 0.71 0.60 0.780.63 0.56 0.43 0.43 0.56 0.55 0.55 0.63 0.56 0.73 0.56 0.61 0.51 0.61 0.480.92 0.84 0.67 0.71 0.61 0.83 0.71 0.81 0.66 0.49 0.61 0.56 0.45 0.32 0.570.30 0.51 0.44 0.37 0.48 0.46 0.47 0.50 0.46 0.55 0.46 0.41 0.43 0.49 0.45\nFigure 9. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B.\n0.00.20.40.60.81.0\nAttn 38MLP 38Attn 39MLP 39Attn 40MLP 40\n0.95 0.98 0.99 0.96 0.99 0.98 0.98 0.97 0.98 0.96 0.96 0.980.91 0.92 0.91 0.98 0.93 0.94 0.94 0.93 0.95 0.89 0.92 0.930.98 0.98 0.96 0.96 0.99 0.98 0.98 0.97 0.98 0.96 0.98 0.980.81 0.87 0.84 0.90 0.82 0.86 0.87 0.85 0.86 0.79 0.88 0.820.90 0.94 0.95 0.29 0.97 0.96 0.96 0.96 0.96 0.94 0.94 0.950.40 0.42 0.38 0.35 0.42 0.49 0.53 0.54 0.49 0.47 0.47 0.41\n...\n``\nare\nn\n ' t\nyou\nbeing\na\ngood\nboy\n?\n ''\nAttn 1MLP 1Attn 2MLP 2Attn 3MLP 3Attn 4MLP 4Attn 5MLP 5\n0.03 0.01 0.05 0.01 0.02 0.03 0.12 0.02 0.16 0.20 0.16 0.160.28 0.11 0.15 0.12 0.19 0.22 0.35 0.18 0.44 0.42 0.38 0.280.72 0.60 0.61 0.36 0.40 0.61 0.55 0.56 0.54 0.54 0.62 0.430.59 0.47 0.64 0.47 0.24 0.62 0.57 0.65 0.63 0.56 0.59 0.560.81 0.60 0.59 0.51 0.37 0.51 0.67 0.60 0.65 0.46 0.61 0.660.49 0.51 0.66 0.34 0.36 0.35 0.45 0.53 0.61 0.36 0.58 0.600.72 0.44 0.69 0.64 0.75 0.58 0.58 0.46 0.56 0.71 0.62 0.630.34 0.49 0.57 0.57 0.40 0.47 0.47 0.57 0.56 0.53 0.64 0.540.94 0.81 0.83 0.83 0.82 0.78 0.85 0.70 0.73 0.82 0.69 0.860.55 0.55 0.65 0.64 0.55 0.47 0.57 0.57 0.46 0.46 0.54 0.57\nFigure 10. Token-Wise Cosine Similarities Across Modules in LLaMA-2-13B.\n19\n--- Page 20 ---\nSkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling\n0.00.20.40.60.81.0\nAttn 38MLP 38Attn 39MLP 39Attn 40MLP 40\n0.98 0.99 0.98 0.98 0.99 0.98 0.99 0.97 0.98 0.99 0.98 0.98 0.95 0.94 0.960.94 0.94 0.95 0.94 0.93 0.95 0.90 0.94 0.91 0.94 0.86 0.94 0.81 0.83 0.940.99 0.99 0.99 0.99 0.98 0.99 0.99 0.98 0.99 0.99 0.98 0.99 0.98 0.95 0.980.87 0.86 0.83 0.83 0.71 0.84 0.91 0.84 0.83 0.86 0.87 0.89 0.88 0.82 0.820.93 0.96 0.97 0.95 0.96 0.95 0.96 0.95 0.96 0.97 0.94 0.96 0.95 0.93 0.920.46 0.51 0.48 0.65 0.47 0.43 0.51 0.62 0.56 0.57 0.21 0.62 0.53 0.24 0.64\n...\nbut\n just\n one\n look\nat\n a\nmin\n ionsent\n him\npract\n icallycat\natonicAttn 1MLP 1Attn 2MLP 2Attn 3MLP 3Attn 4MLP 4Attn 5MLP 5\n0.01 0.03 0.04 0.10 0.02 0.01 0.16 0.10 0.21 0.17 0.32 0.18 0.26 0.07 0.310.21 0.26 0.24 0.33 0.30 0.24 0.35 0.48 0.44 0.48 0.41 0.46 0.44 0.49 0.580.52 0.29 0.21 0.34 0.32 0.62 0.65 0.36 0.63 0.53 0.63 0.24 0.63 0.49 0.370.46 0.53 0.48 0.58 0.52 0.69 0.63 0.26 0.63 0.71 0.53 0.16 0.55 0.34 0.360.82 0.71 0.71 0.74 0.51 0.61 0.75 0.74 0.67 0.61 0.65 0.66 0.76 0.53 0.530.49 0.50 0.44 0.46 0.42 0.61 0.65 0.24 0.64 0.68 0.66 0.54 0.67 0.15 0.210.72 0.66 0.63 0.72 0.62 0.51 0.77 0.79 0.67 0.61 0.55 0.72 0.60 0.76 0.720.36 0.45 0.39 0.36 0.37 0.46 0.59 0.43 0.53 0.53 0.56 0.51 0.59 0.47 0.460.90 0.83 0.78 0.83 0.65 0.65 0.75 0.91 0.64 0.75 0.89 0.89 0.87 0.85 0.870.57 0.60 0.53 0.43 0.34 0.46 0.64 0.47 0.58 0.62 0.64 0.59 0.61 0.52 0.43\nFigure 11. Token-Wise Cosine Similarities Across Modules in LLaMA-2-13B.\n20",
  "text_length": 85218
}