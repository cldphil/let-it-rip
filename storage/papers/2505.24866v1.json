{
  "id": "http://arxiv.org/abs/2505.24866v1",
  "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head\n  DeepFake Detection",
  "summary": "The rapid advancement of talking-head deepfake generation fueled by advanced\ngenerative models has elevated the realism of synthetic videos to a level that\nposes substantial risks in domains such as media, politics, and finance.\nHowever, current benchmarks for deepfake talking-head detection fail to reflect\nthis progress, relying on outdated generators and offering limited insight into\nmodel robustness and generalization. We introduce TalkingHeadBench, a\ncomprehensive multi-model multi-generator benchmark and curated dataset\ndesigned to evaluate the performance of state-of-the-art detectors on the most\nadvanced generators. Our dataset includes deepfakes synthesized by leading\nacademic and commercial models and features carefully constructed protocols to\nassess generalization under distribution shifts in identity and generator\ncharacteristics. We benchmark a diverse set of existing detection methods,\nincluding CNNs, vision transformers, and temporal models, and analyze their\nrobustness and generalization capabilities. In addition, we provide error\nanalysis using Grad-CAM visualizations to expose common failure modes and\ndetector biases. TalkingHeadBench is hosted on\nhttps://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to\nall data splits and protocols. Our benchmark aims to accelerate research\ntowards more robust and generalizable detection models in the face of rapidly\nevolving generative techniques.",
  "authors": [
    "Xinqi Xiong",
    "Prakrut Patel",
    "Qingyuan Fan",
    "Amisha Wadhwa",
    "Sarathy Selvam",
    "Xiao Guo",
    "Luchao Qi",
    "Xiaoming Liu",
    "Roni Sengupta"
  ],
  "published": "2025-05-30T17:59:08Z",
  "updated": "2025-05-30T17:59:08Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24866v1"
}