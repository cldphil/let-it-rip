{
  "id": "http://arxiv.org/abs/2505.24866v1",
  "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head\n  DeepFake Detection",
  "summary": "The rapid advancement of talking-head deepfake generation fueled by advanced\ngenerative models has elevated the realism of synthetic videos to a level that\nposes substantial risks in domains such as media, politics, and finance.\nHowever, current benchmarks for deepfake talking-head detection fail to reflect\nthis progress, relying on outdated generators and offering limited insight into\nmodel robustness and generalization. We introduce TalkingHeadBench, a\ncomprehensive multi-model multi-generator benchmark and curated dataset\ndesigned to evaluate the performance of state-of-the-art detectors on the most\nadvanced generators. Our dataset includes deepfakes synthesized by leading\nacademic and commercial models and features carefully constructed protocols to\nassess generalization under distribution shifts in identity and generator\ncharacteristics. We benchmark a diverse set of existing detection methods,\nincluding CNNs, vision transformers, and temporal models, and analyze their\nrobustness and generalization capabilities. In addition, we provide error\nanalysis using Grad-CAM visualizations to expose common failure modes and\ndetector biases. TalkingHeadBench is hosted on\nhttps://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to\nall data splits and protocols. Our benchmark aims to accelerate research\ntowards more robust and generalizable detection models in the face of rapidly\nevolving generative techniques.",
  "authors": [
    "Xinqi Xiong",
    "Prakrut Patel",
    "Qingyuan Fan",
    "Amisha Wadhwa",
    "Sarathy Selvam",
    "Xiao Guo",
    "Luchao Qi",
    "Xiaoming Liu",
    "Roni Sengupta"
  ],
  "published": "2025-05-30T17:59:08Z",
  "updated": "2025-05-30T17:59:08Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24866v1",
  "full_text": "arXiv:2505.24866v1 [cs.CV] 30 May 2025TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection Xinqi Xiong1∗ †Prakrut Patel1∗Qingyuan Fan1∗Amisha Wadhwa1∗Sarathy Selvam1 Xiao Guo2Luchao Qi1Xiaoming Liu2Roni Sengupta1† 1University of North Carolina at Chapel Hill,2Michigan State University {xxiong, prakrut, lqi, ronisen}@cs.unc.edu {qfan, wamish, sarathy}@unc.edu {guoxia11,liuxm}@msu.edu Abstract The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. How- ever, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator benchmark and curated dataset designed to evaluate the performance of state-of-the-art detectors on the most advanced generators. Our dataset includes deepfakes synthesized by leading academic and commercial models and features carefully constructed protocols to assess generalization un- der distribution shifts in identity and generator characteristics. We benchmark a diverse set of existing detection methods, including CNNs, vision transformers, and temporal models, and analyze their robustness and generalization capabili- ties. In addition, we provide error analysis using Grad-CAM visualizations to expose common failure modes and detector biases. TalkingHeadBench is hosted on https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to all data splits and protocols. Our benchmark aims to accelerate research towards more robust and generalizable detection models in the face of rapidly evolving generative techniques. 1 Introduction Deepfake technology, known for its ability to manipulate facial features and produce highly realistic synthetic videos, has seen rapid adoption in entertainment [ 23,1], marketing [ 21], and personalized content creation [ 30,31,47]. Yet, these advancements also introduce serious societal risks. Among deepfake types, talking-head deepfakes, which generate highly realistic head-and-shoulder videos of individuals speaking, are especially concerning. Driven by advanced generative models, these videos are potent tools for spreading misinformation, manipulating public discourse, and executing social engineering attacks [ 13,37,36]. Their realism and ease of distribution make them particularly dangerous in sensitive domains like politics, finance, and media. In one real-world case, a finance worker was tricked into transferring $25 million during a video call with a deepfake impersonating the company’s CFO [ 26]. These growing threats highlight the urgent need to understand talking-head deepfakes and to develop robust detection and mitigation strategies [13, 43, 24]. ∗These authors contributed equally to this work. †Corresponding authors: xxiong@cs.unc.edu,ronisen@cs.unc.edu Preprint. Table 1: Overview of Facial Deepfake Benchmarks. TalkingHeadBench is the first manually curated, multi- modal, multi-generator benchmark focused on talking-head deepfakes, specifically designed to evaluate detector robustness and generalization. Curation removes ∼60% of low-quality samples with obvious artifacts, ensuring a more realistic and challenging benchmark. Generators marked with ✓* use lip-sync-only techniques. Dataset Talking-head # of Generators Modality Curation Year Real # Fake # DeepFake-eval-2024  ✗ N/A Video/Audio/Image ✓ 2025 1,208 767 FakeA VCeleb  ✗ Multiple Audio/Video ✓ 2021 500 19,500 FaceForensics++  ✗ Multiple Video ✓ 2019 1,000 4,000 LA V-DF  ✓* Single Audio/Video ✗ 2022 36,431 99,873 A V-Deepfake1M  ✓* Single Audio/Video ✗ 2023 286,721 860,039 PolyGlotFake  ✓* Multiple Audio ✗ 2024 766 14,472 DF-40  ✓ Multiple Video/Image ✗ 2024 1,590 1M+ TalkingHeadBench (Ours) ✓ Multiple Audio/Video ✓ 2025 2,312 2,984 In response to the growing threats posed by talking-head deepfakes, numerous facial deepfake benchmarks have been proposed (see Tab. 1, which played a central role in advancing detection methods. However, most existing benchmarks either focus on face-swapping techniques that are easy to detect [ 35], or rely on uncurated ‘in-the-wild’ datasets [ 5,52] that include older, GAN- based deepfake [ 35] or lip-sync-specific generators [ 18,2,4]. These datasets often lack diversity in generation methods, suffer from low data quality due to artifacts, and do not provide controlled multi-modal evaluation settings. Furthermore, they do not reflect the capabilities of recent diffusion- based talking-head generators, which produce more realistic manipulations by synthesizing the entire facial region and controlling both pose and expression. To address these gaps, we introduce TalkingHeadBench—the first manually curated, multi-generator, multi-modal benchmark designed specifically for evaluating detector robustness against state-of-the-art talking-head deepfakes. Our benchmark features high-quality synthetic videos generated using six academic and one com- mercial talking-head generator using both audio and video-driven diffusion and transformer-based models [ 12,8,49,46,14]. We also carefully design evaluation protocols to assess the generalization and robustness of state-of-the-art (SOTA) deepfake detectors [ 11,40,50,51,25] across identity and generator shifts in training and testing data. In addition to benchmarking, we also present a detailed error analysis using Grad-CAM [ 39] to identify common failure cases of SOTA detectors on different generators and inform future improvements in detection strategies. We then note the limitations of performance of SOTA detectors and identify areas of improvement, and provide guidelines on what performance future detectors should aspire to and how to analyze them. TalkingHeadBench benchmark is currently hosted in the HuggingFace repository, which provides train-test data for all generators across multiple protocols. In the near future, we promise to develop a more active benchmark repository by a) developing the API/code/metrics that also allow authors of new generators to evaluate the ‘detectability’ of their generated videos; b) setting up two leaderboards, which compare the performance of generators and detectors, respectively; c) updating the list of generators and detectors once every 6 months, to accelerate the development cycle. We summarize our main contributions as follows: •We introduce TalkingHeadBench, a multi-modal multi-generator benchmark for talking- head deepfake detection, a significantly more challenging benchmark than previously used FaceForensics++, where SOTA detectors easily achieve near-perfect accuracy. •We design evaluation frameworks to explicitly study detector robustness and generalization across train-test distribution shift caused by varying identity and generator properties. Our analysis indicates that even the best detectors lack robustness, indicating the need for future research in developing advanced detectors using our benchmark. •We explicitly analyze why SOTA detectors fail on certain generators using Grad-CAM, providing valuable insights into their biases and presenting directions for future research. 2 Related Work Deepfake generation has leveraged various facial manipulation techniques over the years. Among them, face swapping methods [ 29,34,6] are widely used, combining facial regions of various sources to create synthetic videos. They often introduce visible artifacts such as boundary inconsistency, lighting mismatch, or texture irregularity, which modern detectors can learn to exploit easily. 2 Table 2: Overview of talking-head generators, highlighted were used for creating TalkingHeadBench. Modality Framework Generator Venue Code Availability Image, Audio Diffusion Hallo  arXiv24 ✓ Image, Audio Diffusion Hallo2  ICLR25 ✓ Image, Audio, Text Diffusion Hallo3  CVPR25 ✓ Image, Video Diffusion X-Portrait  SIGGRAPH24 ✓ Image, Video Non-diffusion LivePortrait  arXiv24 ✓ Image, Video Non-diffusion EMOPortraits  CVPR24 ✓ Image, Video Non-diffusion MCNet  ICCV23 ✓ Image, Video Non-diffusion FSRT  CVPR24 ✗ Image, Audio/Video Diffusion AniPortrait  arXiv24 ✓ Image, Audio/Video Diffusion SkyReels-A1  arXiv25 ✗ Image, Text Diffusion MAGI-1  N/A ✓ Table 3: Overview of talking-head deepfake detectors, highlighted were used in the paper. Modality Framework Detector Venue Code Availability Image CNN SBI  CVPR22 ✓ Image Neural Network CADDM  CVPR23 ✓ Image Neural Network HiFiNet//HiFiNet++ CVPR23/IJCV24 ✓ Image Transformer DeepFake-Adapter  IJCV25 ✓ Image Transformer GenConVit  arXiv25 ✓ Image Transformer RFFR  PR25 ✓ Image Neural network LAA-Net  CVPR24 ✓ Video Transformer TALL /TALL++  ICCV23/IJCV24 ✓ Video CNN AltFreezing  CVPR23 ✗ Video CNN/Transformer NACO  ECCV24 ✗ Video Neural network MM-Det  NeurIPS24 ✓ Video Neural network StyleFlow  CVPR24 ✗ Audio/Video Transformer LipFD  NeurIPS 2024 ✓ More recently, talking-head generation has emerged as a more sophisticated approach. These models synthesize the full face, including expressions and lip motion, from a static portrait using audio or video driving signals—eliminating the obvious seam-based flaws of earlier methods. SOTA diffusion-based talking-head models (see Tab. 2) produce highly realistic outputs with synchronized audio-visual features, significantly raising the difficulty for detection algorithms. Despite these advances, existing deepfake benchmarks remain limited in scope. As shown in Tab. 1, most prior datasets either focus on older GAN-based generators or target narrow manipulation types such as face swapping ( e.g., FakeA VCeleb [ 22] and FaceForensics++ [ 35]) or scrape social media content without controlling of generators ( e.g., DeepFake-eval-2024 [ 5]). While newer datasets like LA V-DF [ 3], A V-Deepfake1M [ 2], and PolyGlobFake [ 18] incorporate talking-head deepfakes, they mostly emphasize lip-sync-driven synthesis and lack broader modality or generator diversity. Further, these datasets are often massive but uncurated, resulting in many low-quality samples. In contrast, our dataset is built using a curated collection of cutting-edge diffusion-based talking-head generators spanning both audio and video-driven modalities. Through manual inspection, we discarded over 60% of generated videos due to visible artifacts (see Tab. 4), ensuring that only high-quality, realistic samples remain—thereby offering a more meaningful and challenging benchmark. Recent detection methods (see Tab. 3), though effective on static and unimodal datasets like Face- Forensics++ [ 35], have not been thoroughly evaluated against multi-modal diffusion-based deepfakes. To fill this gap, we introduce a comprehensive benchmark that reflects the current generation land- scape. Our results show that SOTA detectors, which achieve near-perfect accuracy on traditional benchmarks like FaceForensics++, struggle significantly when tested on our dataset, highlighting poor robustness to identity and generator shifts and calling for stronger generalization capabilities. 3 Dataset Creation This section details the creation process of our dataset, including data sources, generation methods, cleaning procedures, statistics, and data splits. Fig 1 provides an overview of this pipeline. 3.1 Source Data and Generation Overview To create a diverse deepfake benchmark dataset, we utilize publicly available datasets containing images from FFHQ (Flickr-Faces-HQ) [ 19] and videos of real people and audio of people talking 3 Figure 1: Overview of the TalkingHeadBench dataset creation pipeline. We use portrait images from FFHQ [ 19] as source and audio or video from CelebV-HQ [ 55] as driving signal to generate talking-head deepfake videos using 6 open-source and 1 commercial generators; deepfake images from these detectors are shown at the bottom. We then perform multi-stage data curation to obtain 2984 high-quality deepfake videos along with 2312 real videos. Finally, we design three evaluation protocols to assess detector robustness under distribution shifts in identity and generator characteristics between training and testing sets. from CelebV-HQ [ 55], processed by a diverse suite of contemporary deepfake generators. We select six open-source generators: Hallo [ 49], Hallo2 [ 8], AniPortrait (Audio-driven) (AniAudio) [ 46], AniPortrait (Video-driven) (AniVideo) [ 46], LivePortrait [ 14], and EMOPortraits [ 12], aiming for diversity in both diffusion-based and non-diffusion-based frameworks and driving signal modalities in both video and audio, as summarized in Tab. 2. We choose these generators because they are the latest SoTA generators with publicly available code covering varying modalities and frameworks. Additionally, we create a commercial dataset from MAGI-1 [ 38] for experimental testing and show the result in the Supplementary. We also explore other generators such as Hallo3 [ 9], X-Portrait [ 48], and MCNet [ 17], with publicly available code, but do not include them in our dataset since Hallo3 is computationally expensive (GPU memory constraint), and X-Portrait and MCNet generate talking- heads with significant artifacts. 3.2 Data Curation and Final Dataset Statistics We perform manual curation to remove videos with strong artifacts to ensure the quality of the dataset. Common artifact types included unnatural head movements, lip-sync errors, and background warping. Detailed per-generator artifact observations, which inform our curation and highlight unique generator characteristics, are provided in the Supplementary. We then separate the generated data into a train and test split, ensuring identity separation, i.e., the source image and the driving signal used for generating any training deepfake video is not used for generating testing deepfake videos. Tab. 4 details the final statistics and splits, and sample frames are shown in Fig 1. 4 Benchmark Experiments 4.1 Evaluation Setup Benchmarking Objectives and Key Questions We perform a comprehensive evaluation and analysis to answer the following questions: (i) How does the performance of the SOTA detectors change from FaceForensics++ [ 35], where detectors have near-perfect accuracy, to TalkingHeadBench? 4 (ii) Can the SOTA detectors generalize across shifts in identity and generator property? (iii) Which talking-head generators are easy or hard to detect, and why? (iv) Which aspects of the TalkingHeadBench benchmark remain challenging for the SOTA detectors and warrant further investigation? SOTA Detectors To address these questions, we benchmark four publicly available SOTA deepfake detectors: CADDM [ 11], LipFD [ 25], DeepFake-Adapter [ 40], and TALL [ 50]. Similar to the generators, we aim to diversify the modalities and frameworks of our detectors, as shown in Tab. 3. These four detectors meet our goal of being the latest SOTA detectors with available code, and together, they capture varying modalities. Evaluation Protocols To systematically evaluate the generalization of these SOTA detectors, we design three distinct protocols: P1 (Identity Shift Only) Evaluates generalization to unseen identities within familiar generators. Models are trained on deepfakes from all generators and real videos, then tested on each generator and real videos, ensuring no overlap in identities between the training and test sets. (see Tab. 5). P2 (Generator Shift) Evaluates generalization to an unseen generator. Models are trained on remaining generators and real videos, then tested on the held-out generator and real videos (see Tab. 6). P3 (Identity & Generator Shift) Evaluates generalization to an unseen generator and identities. Using a leave-one-out approach, models are trained on the train splits of remaining generators and real videos, then tested on the test split of the held-out generator and real videos (see Tab. 7). For these protocols, we select 1600 real videos from FaceForensics++ [ 35] and CelebV-HQ [ 55], unseen during generation, to match fake video numbers per protocol, and perform face comparison to prevent identity leakage between train and test across real and fake video sets. Evaluation Metrics Detector performance under these protocols is measured with three metrics: AUC, TPR@FPR=1% (T1), and TPR@FPR=0.1% (T0.1), following IJB-C face benchmark [ 27]. Stricter thresholds on FPR, e.g., T0.1, are more useful when these detectors are applied at scale and only a small amount of bad detections are tolerable. We will prioritize T1 for most analysis in this paper, following its use in the IJB-C face benchmark, but later we will analyze the detector performance across various thresholds in FPR. We classify the performance of detectors as: Excellent: ≥0.99, Good: 0.95-0.99, Reasonable: 0.85-0.95, Fair: 0.75-0.85, and Poor: <0.75. Table 4: Summary statistics of the TalkingHeadBench across generators and associated visual artifacts. Data Type # Total # Train # Test Ratio Removed Key Artifacts Hallo  420 303 117 ∼42% Lip sync error, hair artifacts, occlusion error, background warping Hallo2  432 327 105 ∼60% Lip sync error, hair artifacts, occlusion error, background warping AniPortrait (Audio)  542 396 146 ∼66.7% Lip sync error, inconsistent head movement, teeth/pupil anomalies AniPortrait (Video)  422 314 108 ∼80% Extreme face/background warping, occlusion errors, hair artifacts LivePortrait  529 352 177 ∼60% Face distortion, inconsistent head movement, occlusion error EMOPortraits  573 381 192 ∼50% Floating head, edge anomalies, spatial inconsistencies MAGI-1  66 0 66 ∼18% Face/background warping TOTAL Fakes 2984 2073 911 ∼63% Table 5: Protocol 1 — Identity Shift Only. Best numbers per (generator) are bold. AniAudio  AniVideo  Hallo  Hallo2  EMOPortraits  LivePortrait  Detector AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 CADDM  0.90 0.55 0.25 0.83 0.39 0.39 0.94 0.65 0.64 0.96 0.45 0.45 0.94 0.27 0.23 0.95 0.59 0.42 TALL  1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.98 1.00 0.98 0.98 1.00 0.97 0.96 Lip FD  0.98 0.75 0.58 0.99 0.74 0.38 0.98 0.72 0.54 0.99 0.83 0.62 0.99 0.78 0.50 0.98 0.84 0.56 DF-Adapter  0.99 0.58 0.06 1.00 1.00 0.15 1.00 0.99 0.11 1.00 0.99 0.98 1.00 0.99 0.25 1.00 0.98 0.41 Table 6: Protocol 2 — Generator Shift Only. Best numbers per (generator) are bold AniAudio  AniVideo  Hallo  Hallo2  EMOPortraits  LivePortrait  Detector AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 CADDM  0.88 0.32 0.02 0.94 0.27 0.18 0.97 0.61 0.52 0.94 0.37 0.14 0.79 0.02 0.00 0.98 0.71 0.63 TALL  1.00 1.00 0.98 1.00 0.99 0.98 1.00 1.00 1.00 0.97 0.89 0.81 0.97 0.70 0.40 1.00 1.00 0.98 LipFD  0.99 0.76 0.51 0.97 0.62 0.30 0.98 0.70 0.47 0.87 0.50 0.32 0.89 0.30 0.08 0.99 0.81 0.63 DF-Adapter  1.00 1.00 0.83 1.00 1.00 1.00 1.00 1.00 0.73 0.97 0.80 0.68 0.99 0.93 0.80 1.00 1.00 0.99 5 4.2 Impact of Generator and Identity Shift on Performance Protocol-Dependent Performance Shifts: Overall, detectors exhibit performance shifts across protocols. In particular, average AUC and T1 scores tend to decline from P1 to P2, indicating that generator shift (P2) poses greater challenges than identity shift (P1). This trend continues in P3, where combined generator and identity shifts further reduce performance as illustrated in Fig 2a. For instance, TALL’s [ 50] average T1 score decreases from 0.99 in P1 to 0.93 in P2 and 0.87 in P3. Despite these increasingly difficult conditions, TALL and DeepFake-Adapter [ 40] maintain strong performance across all three protocols, with DeepFake-Adapter’s lower score on P1 largely due to poor performance on AniPortrait Audio [ 46](T1 = 0.58; see Tab. 5). This underscores the need to assess detectors not just by averages, but also by generator-level sensitivity. Generator Difficulty and Cross-Protocol Generalization: Fig 2c reveals the variation of detector performance across generators. EMOPortraits [ 12] and Hallo2 [ 8] emerge as the most challenging generators, with average T1 scores below 0.6 and 0.7, respectively. In contrast, LivePortrait [ 14] and Hallo [ 49] consistently yield higher detection rates, with average T1 over 0.8. Notably, while EMOPortraits [ 12] achieves a high average AUC of ≥0.90 across all 3 protocols, its T1 drops sharply from 0.80 in P1 to 0.49 in P2, and further to 0.38 in P3. This disparity underscores a critical limitation in current detectors: although models may appear performant under AUC, they struggle to maintain true positive rates under stricter false positive constraints, especially when facing generator and identity shifts. Figure 2: Average performance scores across detectors and generators. (a) shows the average T1 across all detectors for each evaluation protocol. Detector performance consistently declines from P1 to P2, and further in P3, highlighting the increasing difficulty of generalization. These results suggest that evaluating detectors solely on average performance may obscure generator-specific weaknesses. (b) presents the average TPR at varying FPR thresholds for TALL and DeepFake-Adapter. TALL maintains high TPR even at extreme thresholds (e.g., FPR=0.001), demonstrating strong robustness. In contrast, DeepFake-Adapter exhibits a sharp performance drop as the threshold tightens, highlighting its reduced reliability under stricter operating conditions. (c) shows the average T1 score per generator across all detectors. EMOPortraits and Hallo2 emerge as the most challenging generators, while Hallo and LivePortrait are comparatively easier to detect. 4.3 Detector Robustness and SOTA Evaluation Overall Robustness Profile: While detailed per protocol analysis is valuable, evaluating robustness across thresholds provides a comprehensive view of a detector’s generalization capabilities. Fig 3 summarizes performance across all generators, averaged over protocols, for T1 and T0.1 thresholds. High and balanced coverage across all axes indicates consistent generalization across identity, gener- ator, and joint shifts—crucial for handling unseen data from future SOTA generators and varying identities. At T1 (Fig 3a), DeepFake-Adapter [ 40] and TALL [ 50] exhibit strong performance across most generators, with DeepFake-Adapter excelling on LivePortrait [ 14], Hallo [ 49], and AniPor- traitVideo [ 46]. However, when the threshold is tightened to T0.1 (Fig 3b), DeepFake-Adapter’s per- formance drops sharply—particularly for AniPortraitAudio [ 46] and EMOPortraits [ 12]—indicating reduced robustness under stricter constraints. In contrast, TALL retains high scores across most Table 7: Protocol 3 — Identity & Generator Shift Combined. Best numbers per (generator) are bold AniAudio  AniVideo  Hallo  Hallo2  EMOPortraits  LivePortrait  Detector AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 AUC T1 T0.1 CADDM  0.82 0.25 0.21 0.98 0.60 0.60 0.95 0.55 0.20 0.93 0.17 0.17 0.75 0.03 0.03 0.95 0.52 0.41 TALL  1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 0.97 0.84 0.82 0.97 0.48 0.48 1.00 0.93 0.92 LipFD  0.99 0.72 0.62 0.98 0.71 0.56 0.98 0.72 0.34 0.89 0.57 0.40 0.91 0.30 0.14 0.98 0.77 0.61 DF-Adapter  1.00 0.93 0.16 1.00 0.99 0.94 1.00 1.00 0.95 0.98 0.88 0.80 0.98 0.70 0.53 1.00 0.95 0.56 6 generators at T0.1, suggesting better threshold-level resilience and stronger generalization under a low false positive rate. Performance across different FPR thresholds: While our primary evaluation focuses on T1 due to its importance in deepfake detection scenarios, analyzing stricter thresholds such as T0.1, T0.01, T0.001 also provides valuable insights into overall detector reliability. As shown in Fig 2b, even though both TALL and DeepFake-Adapter achieve ≥0.90 for T1, while FPR decreases, we notice a significant drop for DeepFake-Adapter. This indicates its reduced reliability in real-world applications where minimizing such false accusations is paramount. Meanwhile, TALL demonstrates robust performance, maintaining a consistently high true positive rate even at these very low FPRs, highlighting its suitability for deployment in sensitive environments. Figure 3: Detector performance across generators measured by T1 and T0.1, averaged over all protocols. (a) shows T1 performance, where DeepFake-Adapter and TALL demonstrate strong generalization across identity, generator, and joint shifts. (b) shows results at a stricter threshold (T0.1), where most detectors have a noticeable drop, highlighting challenges in maintaining high recall under low FPR. Despite this, TALL remains the most robust, showing consistent performance across generators even under tighter operating conditions. 4.4 Error Analysis and Explainability The quantitative results highlight specific areas requiring deeper investigation, for instance, the poor performance of most detectors against EMOPortraits [ 12], particularly in P3 where T1 scores drop significantly. Similarly, CADMM [ 11] consistently underperforms on the TalkingHeadBench dataset, indicating a need to examine its limitations more closely. We leverage explainability techniques like Grad-CAM [ 39] to visualize the regions and features that detectors focus on for their predictions. We demonstrate an example for TALL [ 50] since it achieves the overall best performance on our dataset. Other detectors can be found in the supplementary. Failure Case Analysis: On the EMOPortraits dataset in P3-TALL [ 50]’s worst performing scenario (see Tab. 7), failure cases reveal that the model misclassifies both real and fake videos due to reliance on background cues. As shown in Fig 5, real and fake videos are getting misclassified due to the model’s attention being disproportionately focused on the background, despite meaningful facial cues suggesting the correct label. This indicates a weakness in TALL’s generalization: it is overly influenced by background artifacts, likely due to training bias, and fails to localize attention to the face under combined identity and generator shifts. Figure 4: Success case of TALL on EMOPortrait (P3): the model correctly detects a deepfake by focusing on the neck, a known region of visual artifacts in EMOPortrait. This shows cross-dataset generalization despite the rarity of such artifacts in training data.Success Case Analysis: In contrast to its failure cases, TALL demon- strates effective generalization in cer- tain EMOPortraits [ 12] videos under P3, as shown in Fig 4. Here, the model accurately classifies a deepfake by focusing attention on the neck re- gion—an area known to exhibit visual artifacts in EMOPortraits. Notably, this success occurs despite the fact that such artifacts are uncommon in other datasets used during training. This suggests that TALL [ 50] has successfully learned to transfer knowledge from broader manipulation cues across datasets. The 7 focused activation around the lower face and neck reflects meaningful model behavior and highlights its potential for cross-domain generalization in challenging settings. Generator TPs (P1) FNs (P3) % Drop EMOPortraits  188 72 38.30% Hallo2  102 9 8.82% LivePortrait  168 5 2.98% Table 8: Breakdown of high-confidence true positives (TPs) (prediction ≥0.9) in P1 that become false negatives (FNs) in P3 for TALL. A large drop from confident pre- dictions in P1 to errors in P3 indicates poor generalization across generator shifts.Tracking Generalization Failures from Con- fident Predictions: To better understand how detectors fail under distribution shifts, we track how high-confidence true positives (TPs) in P1 transition under the more challenging P3 set- ting. As shown in Tab. 8, for TALL—our best- performing detector—38.3% of EMOPortraits samples confidently classified in P1 (Prediction ≥0.9) are misclassified as false negatives in P3. This significant drop suggests TALL struggles to generalize when both identity and generator properties change. In contrast, TALL shows only a 2.98% drop from P1 to P3 on LivePortrait, highlighting that some generators pose far more severe generalization challenges. These results reinforce that P3 performance degradation is not random but rooted in generator-specific variation, with EMOPortraits emerging as a particularly valuable stress test for future detector evaluation. Protocol-Specific Feature Reliance: To further understand detector behavior across generators, Fig 6 presents Grad-CAM visualizations of TALL on samples from all six generators in Protocol 1, where we can study the bias of detectors to specific generator characteristics using the smallest train-test data distribution shift in P1. For generators such as AniPortrait (Audio/Video), Hallo, and LivePortrait, the model predominantly focuses on the central facial region, aligning well with human-interpretable deepfake cues. However, for Hallo2 and EMOPortraits, attention shifts away from facial features and toward areas surrounding the head. This may suggest one of two possibilities: either the facial features in these generators are realistic enough to fool the detector, or the model has learned to rely on peripheral artifacts (e.g., spatial warping in Hallo2 or neck distortions in EMOPortraits) as key detection signals. Such behavior indicates both progress in generator realism and potential over-reliance on non-facial cues in certain detector architectures. Real Video It is 59.9% likely to be deepfake because It is 40.1% likely to be real because Deepfake Video It is 55.9% likely to be real because It is 44.1% likely to be deepfake because Figure 5: Failure case from TALL on EMOPortraits (P3): misclassifications driven by attention to background regions rather than facial features. Despite correct facial cues, the model’s focus on irrelevant background areas leads to incorrect predictions. Hallo AniAudio Hallo2 EMOPortraits LivePortrait AniVideo Figure 6: Grad-CAM activation map from the TALL model on P1 samples across all six generators. 4.5 Guidance for Future Research using TalkingHeadBench This benchmark underscores several critical areas for future research in deepfake detection. Limitations of SOTA Detectors. Our analysis highlights that while models like TALL [ 50] and DeepFake-Adapter [ 40] demonstrate strong performance across most generators, there remains con- 8 siderable room for improvement in achieving consistent excellence across all domains. Specifically, TALL achieves exceptional T1 scores on 3 out of 6 generators and maintains no worse than fair performance on the remaining ones, while DeepFake-Adapter reaches exceptional on 2 generators and stays within reasonable to fair on the rest. Notably, both models are only jointly exceptional on AniPortraitVideo [ 46] and Hallo [ 49], suggesting current detectors may still struggle to generalize uniformly across diverse generation methods. Open Challenges. While our analysis provides insights into generalization and robustness, several aspects of the TalkingHeadBench benchmark remain difficult for SOTA detectors. These include (i) handling challenging generators like EMOPortraits [ 12] and Hallo2 [ 8], where even the SOTA detectors reach only 70% and 88% T1 accuracy in P3; (ii) reducing performance degradation in P3 where both identity and generator properties differ between train-test splits, all detectors except DF-Adapter exhibit significant performance degradation from P1 to P3; (iii) overcoming model reliance overly on background artifacts—as demonstrated in Sec 4.4—which can cause the model to ignore relevant facial features and lead to misclassification. These persistent challenges highlight the need for future detectors to incorporate stronger domain adaptation, semantic attention, and modality-aware training to handle real-world deepfakes more reliably. Future Objective. (a)Performance: The best performing model across our analysis, TALL [ 50], reaches excellent performance in T1 ( 99%) in only 3/6 generators in P3, while reaching ∼90% T1 for Hallo2 and LivePortrait and ∼80% T1 for EMOPortraits. Future research should strive to reach the excellent performance ( 99%) in both T1 and more challenging T0.1 for all 6 detectors across P1, P2, and P3. (b)Analysis: The TalkingHeadBench benchmark and protocols offer the granularity needed to pin- point detector weaknesses and guide architectural improvements, training strategies, and robustness evaluation. Future research should prioritize reducing performance drop-offs in more challenging generators ( e.g., EMOPortraits, Hallo2), exploring domain adaptation methods, and enhancing sensi- tivity to subtle facial artifacts, while avoiding overfitting to background or dataset-specific cues. (c)Evolving Benchmark: A persistent challenge in deepfake detection is that benchmarks often lag behind the rapid progress of generative models, quickly becoming outdated. While the current Talk- ingHeadDetect repository on Hugging Face provides static train-test splits and evaluation protocols, in the near future, we aim to transform it into an evolving benchmark used by both the detector and generator communities. This will include (i) developing APIs that allow authors of new generators to evaluate the detectability of their models by submitting their codes, (ii) setting up dual leaderboards to track the progress of both generators and detectors, (iii) including generators and deepfake videos created by them whose detectability is lower than the ones present in the benchmark and (iv) removing generators that are easily detectable ( >99% T0.1) a by majority of detectors, every six months. This evolving benchmark will encourage a healthy arms race that will accelerate innovation and improve the resilience of detection systems against newer generators. By addressing these areas, the community can move towards deepfake detectors that are more accurate, robust, reliable, and trustworthy across diverse conditions encountered in real-world scenarios. 5 Conclusions, Broad Impacts, and Limitations We introduce TalkingHeadBench, a comprehensive benchmark utilizing contemporary talking-head generators to stress-test deepfake detectors under realistic distribution shifts. Our evaluations reveal critical gaps: even state-of-the-art detectors struggle with generalization across changes in identity and generator, particularly with challenging fakes like EMOPortraits [ 12], often due to over-reliance on non-facial artifacts. TalkingHeadBench will broadly impact the community by providing a standardized, challenging benchmark that reflects modern deepfake realism and complexity. We provide open access to the dataset, evaluation protocols, and baseline code. While we acknowledge one potential risk for the benchmark is that it could facilitate more sophisticated talking-head generation techniques, which could be potentially misused for deceptive or malicious purposes, our primary objective is to catalyze research into more robust, generalizable, and artifact-aware detection methods. The dataset only contains a finite number of videos and identities derived from FFHQ [ 19] and CelebV-HQ [ 55], which might not represent the diversity of real-world deepfakes that can be exhibited. In the future, we will set up leaderboards that allow performance comparison among newly developed generators and detectors. This benchmark serves as a crucial resource for developing trustworthy deepfake detection systems, essential for mitigating the increasing societal risks posed by manipulated media. 9 References Mahla Abdolahnejad and Peter Xiaoping Liu. Deep learning for face image synthesis and semantic manipulations: a review and future perspectives. Artificial Intelligence Review, 53(8):5847–5880, December 2020. Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall, Tom Gedeon, and Kalin Stefanov. Av-deepfake1m: A large-scale llm-driven audio-visual deepfake dataset. InProceedings of the 32nd ACM International Conference on Multimedia, pages 7414–7423, 2024. Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, and Munawar Hayat. Glitch in the matrix: A large scale benchmark for content driven audio–visual forgery detection and localization. Computer Vision and Image Understanding, 236:103818, 2023. Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. Do you really mean that? content driven audio-visual deepfake dataset and multimodal method for temporal forgery localization. In 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA), pages 1–10. IEEE, 2022. Nuria Alina Chandra, Ryan Murtfeldt, Lin Qiu, Arnab Karmakar, Hannah Lee, Emmanuel Tanumihardja, Kevin Farhat, Ben Caffee, Sejin Paik, Changyeon Lee, et al. Deepfake-eval- 2024: A multi-modal in-the-wild benchmark of deepfakes circulated in 2024. arXiv preprint arXiv:2503.02857, 2025. Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. Simswap: An efficient framework for high fidelity face swapping. In Proceedings of the 28th ACM international conference on multimedia, pages 2003–2011, 2020. Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, and Jongwon Choi. Ex- ploiting style latent flows for generalizing deepfake video detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1133–1143, 2024. Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang. Hallo2: Long-duration and high-resolution audio-driven portrait image animation. arXiv preprint arXiv:2410.07718, 2024. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. arXiv preprint arXiv:2412.00733, 2024. Deressa Wodajo Deressa, Hannes Mareen, Peter Lambert, Solomon Atnafu, Zahid Akhtar, and Glenn Van Wallendael. Genconvit: Deepfake video detection using generative convolutional vision transformer, 2025. Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang Fan, and Zheng Ge. Implicit identity leakage: The stumbling block to improving deepfake detection generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3994–4004, 2023. Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos V ougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8498–8507, 2024. Ángel Fernández Gambín, Anis Yazidi, Athanasios Vasilakos, Hårek Haugerud, and Youcef Djenouri. Deepfakes: current and future trends. Artificial Intelligence Review, 57(3):64, February 2024. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 10  Xiao Guo, Xiaohong Liu, Iacopo Masi, and Xiaoming Liu. Language-guided hierarchical fine-grained image forgery detection and localization. International Journal of Computer Vision, pages 1–22, 2024. Xiao Guo, Xiaohong Liu, Zhiyuan Ren, Steven Grosz, Iacopo Masi, and Xiaoming Liu. Hierar- chical fine-grained image forgery detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3155–3165, 2023. Fa-Ting Hong and Dan Xu. Implicit identity representation conditioned memory compensation network for talking head video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23062–23072, 2023. Yang Hou, Haitao Fu, Chuankai Chen, Zida Li, Haoyu Zhang, and Jianjun Zhao. Polyglotfake: A novel multilingual and multimodal deepfake dataset, 2024. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410, 2019. Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks, March 2019. Achhardeep Kaur, Azadeh Noori Hoshyar, Vidya Saikrishna, Selena Firmin, and Feng Xia. Deep- fake video detection: challenges and opportunities. Artificial Intelligence Review, 57(6):159, May 2024. Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon S Woo. Fakeavceleb: A novel audio- video multimodal deepfake dataset. arXiv preprint arXiv:2108.05080, 2021. Kunlin Liu, Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Wenbo Zhou, and Weiming Zhang. Deepfacelab: Integrated, flexible and extensible face-swapping framework. Pattern Recognition, 141:109628, September 2023. Ping Liu, Qiqi Tao, and Joey Tianyi Zhou. Evolving from Single-modal to Multi-modal Facial Deepfake Detection: Progress and Challenges, April 2025. arXiv:2406.06965 [cs]. Weifeng Liu, Tianyi She, Jiawei Liu, Boheng Li, Dongyu Yao, and Run Wang. Lips are lying: Spotting the temporal inconsistency between audio and visual in lip-syncing deepfakes. Advances in Neural Information Processing Systems, 37:91131–91155, 2024. Heather Chen Magramo, Kathleen. Finance worker pays out $25 million after video call with deepfake ‘chief financial officer’, February 2024. Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al. Iarpa janus benchmark-c: Face dataset and protocol. In 2018 international conference on biometrics (ICB), pages 158–165. IEEE, 2018. Dat Nguyen, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, and Djamila Aouada. Laa-net: Localized artifact attention network for quality- agnostic and generalizable deepfake detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17395–17405, 2024. Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7184–7193, 2019. Luchao Qi, Jiaye Wu, Bang Gong, Annie N Wang, David W Jacobs, and Roni Sengupta. Mytimemachine: Personalized facial age transformation. arXiv preprint arXiv:2411.14521, 2024. Luchao Qi, Jiaye Wu, Annie N Wang, Shengze Wang, and Roni Sengupta. My3dgen: A scalable personalized 3d generative model. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 961–972. IEEE, 2025. 11  Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers, 2025. Andre Rochow, Max Schwarz, and Sven Behnke. Fsrt: Facial scene representation transformer for face reenactment from factorized appearance head-pose and facial expression features. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7716–7726, 2024. Felix Rosberg, Eren Erdal Aksoy, Fernando Alonso-Fernandez, and Cristofer Englund. Facedancer: Pose-and occlusion-aware high fidelity face swapping. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3454–3463, 2023. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1–11, 2019. Shahela Saif, Samabia Tehseen, and Syed Sohaib Ali. Fake news or real? Detecting deepfake videos using geometric facial structure and graph neural network. Technological Forecasting and Social Change, 205:123471, August 2024. Madan Lal Saini, Arnav Patnaik, Mahadev, Dayal Chandra Sati, and Ratish Kumar. Deepfake Detection System Using Deep Neural Networks. In 2024 2nd International Conference on Computer, Communication and Control (IC4), pages 1–5, February 2024. Sand-AI. Magi-1: Autoregressive video generation at scale, 2025. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision, 128(2):336–359, October 2019. Rui Shao, Tianxing Wu, Liqiang Nie, and Ziwei Liu. Deepfake-adapter: Dual-level adapter for deepfake detection. International Journal of Computer Vision, pages 1–16, 2025. Liang Shi, Jie Zhang, Zhilong Ji, Jinfeng Bai, and Shiguang Shan. Real face foundation representation learning for generalized deepfake detection. Pattern Recognition, 161:111299, 2025. Kaede Shiohara and Toshihiko Yamasaki. Detecting deepfakes with self-blended images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18720–18729, 2022. Kanthi Kiran Sirra, Shashi Mogalla, and Kandalam Basamma Madhuri. CSSLnO: Cat Swarm Sea Lion Optimization-based deep learning for fake news detection from social media. Interna- tional Journal of Information Technology, 16(7):4225–4241, October 2024. Xiufeng Song, Xiao Guo, Jiache Zhang, Qirui Li, Lei Bai, Xiaoming Liu, Guangtao Zhai, and Xiaohong Liu. On learning multi-modal forgery representation for diffusion generated video detection, 2025. Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, and Houqiang Li. Altfreezing for more general video face forgery detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4129–4138, 2023. Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photore- alistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. Yuxiang Wei, Yiheng Zheng, Yabo Zhang, Ming Liu, Zhilong Ji, Lei Zhang, and Wangmeng Zuo. Personalized Image Generation with Deep Generative Models: A Decade Survey, February 2025. arXiv:2502.13081 [cs]. You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024. 12  Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. Yuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yanhao Zhang, and Ran He. Tall: Thumbnail layout for deepfake video detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22658–22668, 2023. Yuting Xu, Jian Liang, Lijun Sheng, and Xiao-Yu Zhang. Learning spatiotemporal inconsistency via thumbnail layout for face deepfake detection. International Journal of Computer Vision, 132(12):5663–5680, 2024. Zhiyuan Yan, Taiping Yao, Shen Chen, Yandan Zhao, Xinghe Fu, Junwei Zhu, Donghao Luo, Chengjie Wang, Shouhong Ding, Yunsheng Wu, et al. Df40: Toward next-generation deepfake detection. arXiv preprint arXiv:2406.13495, 2024. Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. DeepfakeBench: a comprehensive benchmark of deepfake detection. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, pages 4534–4565, Red Hook, NY, USA, December 2023. Curran Associates Inc. Daichi Zhang, Zihao Xiao, Shikun Li, Fanzhao Lin, Jianmin Li, and Shiming Ge. Learning natural consistency representation for face forgery video detection. In European Conference on Computer Vision, pages 407–424. Springer, 2024. Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: A large-scale video facial attributes dataset. In European conference on computer vision, pages 650–667. Springer, 2022. 13 Appendices All benchmarks and curated dataset for our TalkingHeadBench can be found in the following URL: https://huggingface.co/datasets/luchaoqi/TalkingHeadBench. TalkingHeadBench is released under a Creative Commons Attribution 4.0 License (CC BY 4.0). Our supplementary materials are summarized as follows: •Details of Assets Used (Section A): Descriptions, sources, and licenses for source datasets, talking-head generators, and deepfake detectors utilized in our benchmark. •Detailed Experimental Setup (Section B): Dataset splits, data curation procedures, com- putational resources, and key training hyperparameters. •Detailed Per-Generator Artifact Observations (Section C): Specific types of visual artifacts commonly produced by each generator, noted during our manual curation process. •Experimental Results (Section D): Performance for commercial generator (MAGI-1). •Explainability Results (Section E): Grad-CAM visualizations for benchmarked detectors. A Details of Assets Used A.1 Source Datasets for Generation and Real Videos TalkingHeadBench leverages publicly available datasets for source images and driving signals (audio/video) for talking-head generation and real videos for detector training and testing. A.1.1 Source Images for Deepfake Generation •FFHQ (Flickr-Faces-HQ) [ 20]:This dataset served as the primary source for high-quality, diverse static portrait images used as the base for deepfake generation. It contains 70,000 high-resolution images of diverse human faces with diverse ages, ethnicities, and image backgrounds originally intended for GAN benchmarking. We utilized images from the 1024x1024 resolution subset. The dataset can be accessed through the following link: https://github.com/NVlabs/ffhq-dataset under Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation. A.1.2 Source Driving Signals (Audio/Video) for Deepfake Generation •CelebV-HQ [ 55]:This large-scale video facial attributes dataset was used as the source for dynamic driving signals. It contains a variety of identities, ethnicities, expressions, and poses. Both video segments (for video-driven generators) and extracted audio tracks (for audio-driven generators, converted to.wav format) were utilized. The dataset can be accessed through the following link: https://github.com/CelebV-HQ/CelebV-HQ. The copyright information for this dataset is not explicitly mentioned. A.1.3 Real Videos for Detector Training/Testing •FaceForensics++ (FF++) [ 35]:Original, unmanipulated video sequences were included in our set of real videos. We directly downloaded the dataset following the download script they provided, and finalized the number of our YouTube real videos from 1000 to 704 due to the unavailability of some of the videos on YouTube. We then renamed the downloaded audio to match the naming of the YouTube real videos. The renamed audio files are in our Hugging Face datasets (./audio/ff++). The original FF++ dataset can be accessed through the following link: https://github.com/ondyari/FaceForensics. The copyright information can be found at https://github.com/ondyari/FaceForensics/blob/ master/LICENSE. •CelebV-HQ [ 55]:To diversify the identity in our real videos, we incorporated a distinct set of real videos from CelebV-HQ that were not used for driving signal extraction. •Identity Control: To prevent identity leakage and ensure fair evaluation, rigorous identity separation was maintained. Identities in the real video sets did not overlap with those used 14 for generating deepfakes (either source images or driving signals) or across the train/test splits of the real videos themselves. This was verified using face recognition (InsightFace with ArcFace model on middle frames of videos). A.2 Talking-Head Generators The deepfake videos in TalkingHeadBench were generated using the following state-of-the-art talking-head generators. We provide brief descriptions, primary modality, and links to code reposito- ries/licenses where available. Visual examples are in Fig 1. A.2.1 Hallo  •Description: Hallo is a talking-head generator that was published in 2024. The generator aims to create realistic and temporally consistent talking-head animations from a static portrait image and a driving audio clip. Hallo integrates diffusion-based models, a UNet- based denoiser, temporal alignment techniques, and a reference network. This hierarchical audio-driven method allows for diversity in poses and expressions. This end-to-end approach enhances animation quality, motion diversity, and personalization across different identities. •Modality: Audio-driven. •Code Repository: https://github.com/fudan-generative-vision/hallo •License: MIT License. •Key Artifacts Observed: See Section C.2. A.2.2 Hallo2  •Description: Hallo2 is an audio-driven talking-head generator that extends Hallo by enabling long-duration and high-resolution video synthesis published in ICLR2025. It incorporates enhanced temporal modeling and spatial fidelity mechanisms to produce stable and expres- sive animations over extended sequences. Built upon a diffusion-based generation backbone, Hallo2 introduces improved temporal alignment and rendering strategies that support better lip-sync accuracy and identity preservation. Hallo2 is suited for real-world applications such as virtual avatars, video dubbing, and personalized content creation. •Modality: Audio-driven. •Code Repository: https://github.com/fudan-generative-vision/hallo2 •License: MIT License. •Key Artifacts Observed: See Section C.3. A.2.3 AniPortrait  •Description: AniPortrait is a talking-head generator introduced in 2024. The two-stage approach first extracts 3D representations from audio, converting them to 2D facial landmark sequences. Then, a diffusion model with a motion module transforms these landmarks into temporally coherent talking-head videos. The hierarchical audio-driven pipeline offers pre- cise control over audio-driven facial expressions and head poses while maintaining identity through reference guidance. Its training scheme, which decouples identity encoding from motion dynamics, enables an end-to-end approach that produces animations with accurate lip-sync, detailed expressions, and robust identity preservation. In video-driven mode, Ani- Portrait directly extracts facial landmarks from a source video to transfer expressions and movements to the reference portrait, utilizing the same reenactment pipeline for consistent temporal guidance and high-quality results. •Modality: Audio-driven or video-driven. •Code Repository: https://github.com/Zejun-Yang/AniPortrait •License: Apache-2.0 license. •Key Artifacts Observed: See Section C.4.1 for audio-driven artifacts and C.4.2 for video- driven artifacts. 15 A.2.4 LivePortrait  •Description: LivePortrait is a real-time talking-head generator designed to produce high- quality portrait animations from a single static image introduced in 2024. It employs an implicit keypoint-based architecture combined with lightweight retargeting and stitching modules to ensure accurate facial motion and head pose transfer. Trained on a large-scale dataset of over 69 million frames, LivePortrait generalizes well across diverse identities and expressions. •Modality: Video-driven. •Code Repository: https://github.com/KwaiVGI/LivePortrait •License: MIT License. •Key Artifacts Observed: See Section C.1. A.2.5 EMOPortraits  •Description: EMOPortraits is a one-shot talking-head generator introduced in CVPR2024. It employs a two-stage training process, with an optional audio-driven phase for video generation from a single image and audio input. The model selects two random frames of the source and driver at each step, adapts the driver frame’s motion and expressions onto the source frame to generate the final image. It encodes a source image into a 3D latent feature and identity descriptor, while a motion module extracts pose/expression codes from a driver, resulting in realistic deepfakes under extreme and asymmetric facial expressions. •Modality: Video-driven. •Code Repository: https://github.com/neeek2303/EMOPortraits •License: Apache-2.0 license. •Key Artifacts Observed: See Section C.5. A.2.6 MAGI-1  (Commercial) •Description: MAGI-1 is a talking-head generator published in 2025. This open-source generator produces realistic, high-quality, temporally consistent videos from text or image prompts. Built on a diffusion transformer architecture, MAGI-1 generates fixed-length videos, enabling real-time streaming and seamless video continuation. With support for large-scale model sizes and long context lengths, it is well-suited for a wide range of creative and generative video applications. •Modality: Primarily Text-to-Video, can optionally add a reference image. •Code Repository: https://github.com/SandAI-org/MAGI-1 •License: Apache-2.0 license. •Key Artifacts Observed: Generally high quality; specific subtle artifacts, if any, are less pronounced or systematic compared to some open-source academic models. Focus is often on overall scene coherence rather than just facial animation. A.3 DeepFake Detectors The following SOTA deepfake detection models were benchmarked on TalkingHeadBench. Brief descriptions, primary input modality, and links to code repositories/licenses are provided. A.3.1 CADDM  •Description: CADDM utilizes an Artifact Detection Module designed to focus on local regions of images. This module employs multiscale anchors to detect and classify artifact areas, aiming to mitigate the influence of identity information in deepfake detection, making it generalizable across different datasets. It showed great improvement in both accuracy and robustness when evaluating FF++. We directly deployed the model from the official GitHub repository to our cluster without modifying any hyperparameter or model structure. •Modality: Image-based (CNN). 16 •Code Repository: https://github.com/megvii-research/CADDM •License: Apache-2.0 license. A.3.2 TALL  •Description: TALL introduces a temporal-attentive localization and learning framework designed to exploit temporal inconsistencies in deepfake videos. By leveraging a dual- stream architecture that models both short-term and long-term temporal dependencies, TALL isolates manipulated frames and emphasizes temporal transitions typically ignored by frame-based detectors. A temporal attention mechanism further enhances frame-level representations by focusing on abrupt motion irregularities introduced by forgery generation processes. Moreover, the authors propose TALL-Swin, which integrates the TALL strategy with the Swin Transformer architecture. This combination leverages the Swin Transformer’s hierarchical feature representation and shifted window mechanism to effectively model both local and global dependencies within the thumbnail layout. Given there is no available model released on their original GitHub repository, we pretrained the model using FF++ based on the implementation version in DeepfakeBench. •Modality: Video-based (Transformer). •Code Repository: https://github.com/rainy-xu/TALL4Deepfake •License: MIT license. A.3.3 LipFD  •Description: LipFD targets lip-sync inconsistency by focusing on the alignment between lip motion and audio speech content. The model extracts fine-grained spatio-temporal features of mouth regions and correlates them with phoneme-level audio embeddings to detect subtle mismatches indicative of forgery. By isolating this cross-modal inconsistency, LipFD distinguishes itself from generic detectors that rely primarily on visual features. This focused approach leads to robust detection of audio-driven deepfakes, especially under real-world conditions. We directly reproduced the results from official GitHub repository with minimal adjustments on learning rate and learning rate decay. •Modality: Audio-Visual (Transformer). •Code Repository: https://github.com/AaronComo/LipFD •License: N/A A.3.4 DeepFake-Adapter  •Description: DeepFake-Adapter presents a universal adaptation framework for deepfake detection by leveraging modality specific adapters integrated into a unified transformer backbone. These adapters specialize in capturing subtle, forgery-specific artifacts across diverse data modalities (e.g., RGB, Depth, Frequency), enabling cross-modal generalization without retraining. We directly reproduced the results from official GitHub repository with minimum adjustments to train using our custom dataset. •Modality: Image-based (Transformer). •Code Repository: https://github.com/rshaojimmy/DeepFake-Adapter •License: N/A B Detailed Experimental Setup B.1 Dataset Generation, Splits, and Curation Details B.1.1 Deepfake Video Generation Approximately 500-600 videos were initially generated for each of the six open-source academic models. This was achieved by scripting random pairings of source images from FFHQ [ 20] with driving signals (audio or video) from distinct splits of CelebV-HQ. 17 B.1.2 Data Curation and Quality Control A rigorous manual curation process was undertaken to ensure the quality and challenge of the benchmark. This involved: • Reviewing each generated video for visual fidelity and realism. •Removing videos with obvious generation failures, extreme distortions, or artifacts that make them trivially identifiable as fake (unless such artifacts are characteristic and subtle). • Ensuring that the remaining fakes posed a reasonable challenge to detection models. This process resulted in the final dataset sizes reported in Tab.4 of the main paper, with approximately 60-65% of initially generated videos being discarded. B.1.3 Train/Test Splits and Real Data Integration •Identity Separation: Strict identity separation was enforced for all data. Source identities from FFHQ and driving signal identities from CelebV-HQ used for the training set of deepfakes were disjoint from those used for the test set. •Real Videos: Real videos from FF++ and CelebV-HQ were also split into training and testing sets, maintaining identity separation. The number of real videos was balanced to be approximately 1:1 with fake videos in the training splits for each protocol. •Validation Sets: For model validation during training, a small subset of identities from the training pool (both real and fake) was held out. For Protocol 1, this involved 50 real and 50 fake videos. For Protocols 2 and 3, it was 50 real and 50 fake videos per generator. B.2 Computational Resources The generation of TalkingHeadBench and the benchmarking of detection models were conducted using NVIDIA RTX A4500 GPUs. Deepfake generation times varied significantly depending on the generator model and the number of GPUs used, as videos were generated in parallel across multiple GPUs. On average, producing 500 videos took several hours to up to a day. For detector training, most state-of-the-art models completed training within 4 hours on a single GPU, with the exception of one model that required approximately an hour per epoch—making its total training time dependent on the specific protocol used. B.3 Hyperparameters and Training Details for Detectors For all benchmarked detectors, we adhered closely to the training configurations and hyperparameters proposed in their original publications and official codebases. C Detailed Per-Generator Artifact Observations This section details characteristic visual artifacts observed for each generator during the manual data curation phase. These insights informed our curation and highlight the unique challenges posed by different generation techniques. C.1 LivePortrait  Common artifacts observed in generations from the LivePortrait model included: •Non-Physical Head Kinematics and Scaling: Driving videos featuring substantial trans- lational or lateral head motion often induced unnatural scaling (e.g., apparent growth or shrinkage) or other non-physical movements in the synthesized head, indicative of inconsis- tent head movement/pose and face warping/distortion. •Sensitivity to Source Image Composition: The generation process exhibited heightened susceptibility to artifacts when source images contained non-adult subjects or multiple indi- viduals. In scenarios with multiple people, artifacts manifested as face warping/distortion, erroneous animation of partially occluded or out-of-frame figures, and visible errors related to internal bounding box estimations, leading to significant spatial inconsistencies. 18 •Semantic Misinterpretation and Occlusion Errors: Objects proximal to the head in the source image were sometimes erroneously segmented as facial features, resulting in their co-movement with the head, a form of occlusion error or texture anomaly. •Lip Synchronization Deficiencies with Extreme Expressions: Source images depicting pronounced oral expressions (e.g., open mouths, broad smiles, compressed lips) occasionally resulted in static labial regions or aberrant lip articulation, indicating lip sync errors or failures in modeling extreme facial deformations. C.2 Hallo  Common artifacts observed in generations from the Hallo model included: •Lip Synchronization Discrepancies: Generated videos frequently exhibited a temporal mismatch between the audible speech and the synthesized lip movements, a common form of lip sync error. •Hair Rendering Artifacts: Portions of the synthesized hair often displayed unnatural stasis, appeared to adhere to the background, or were rendered with low fidelity (e.g., hair artifacts), particularly noticeable during dynamic head movements. This can be considered a type of temporal inconsistency or texture anomaly. •Occlusion Handling Deficiencies: The model demonstrated improper rendering when facial regions were expected to be occluded by elements such as eyeglasses, hair, or hands (if present in the source), leading to occlusion errors. •Background Distortion: The background region immediately surrounding the synthesized head was prone to background warping/distortion correlating with the head movement. C.3 Hallo2  Generations from Hallo2 exhibited artifacts similar to its predecessor, Hallo, albeit with some distinct manifestations: •Aberrant or Absent Lip Articulation: Instances of absent lip movement despite audible speech, or significant asynchrony between audio and visual lip cues, were noted, representing pronounced lip sync errors and temporal inconsistencies. •Static Peripheral Hair Artifacts: Hair situated outside the primary head bounding box fre- quently remained static during head motion, creating a visually jarring \"detached\" effect—a specific type of hair artifact and temporal inconsistency. •Object-Induced Spatial Distortion: The presence of objects proximate to or intersect- ing with the head’s bounding box in the source image often precipitated localized face warping/distortion or spatial inconsistencies in the output. •Background-Correlated Visual Anomalies: Unsystematic visual artifacts, such as unpre- dictable patterns or noise, occasionally manifested, appearing to be correlated with complex textures or patterns within the source image’s background, a form of spatial inconsistency or texture anomaly. C.4 AniPortrait  C.4.1 Audio-driven Analysis of audio-driven outputs from AniPortrait revealed the following: •Variable Perceptual Realism: The model demonstrated capacity for producing naturalistic results, particularly notable for an exclusively audio-driven synthesis approach. •Synchronization and Kinematic Irregularities: The system was prone to occasional lip sync errors and exhibited unnatural head kinematics, including sudden accelerations, oscillatory movements, or an overly rigid, static posture, all categorized under inconsistent head movement/pose. 19 •Dental Structure Anomalies: Synthesized dentition sometimes presented with unrealistic characteristics, such as supernumerary or atypically arranged rows of teeth (teeth anomalies). •Pupillary Artifacts with Eyewear: Source images featuring subjects with eyeglasses occasionally led to anomalous pupillary dilation patterns, such as variable frequencies of dilation (pupil anomalies). •Hand-Induced Artifacts: The presence of hands within the source image frame was a primary trigger for various occlusion errors and spatial inconsistencies. C.4.2 Video-driven Video-driven synthesis using AniPortrait was characterized by: •Microphone Occlusion Handling: The model displayed an unusual proficiency in generat- ing plausible outputs when the driving video featured a subject with a microphone in close proximity to the mouth, suggesting effective handling of this specific occlusion scenario. •Catastrophic \"Head Explosion\" Artifacts: A frequent failure mode involved severe face warping/distortion, where the synthesized head appeared to disintegrate or become overlaid with disparate visual elements (often misidentified hands or other body parts) from the driving video, a critical spatial inconsistency. •Pervasive Background Instability: Background warping/distortion was commonly found even in outputs that were otherwise subjectively assessed as high quality. •Motion Tracking Fidelity versus Distortion Thresholds: While the model effectively tracked subtle head movements, more pronounced motions (e.g., swaying) frequently sur- passed its stable generation threshold, resulting in severe face warping/distortion that could occupy a significant portion of the video frame, indicating limitations in maintaining incon- sistent head movement/pose coherence. •Hair Segmentation Artifacts: In certain instances involving female source images, synthe- sized hair was subject to abrupt truncation or unnatural rendering. •Severe Hand-Related Artifacts: The model exhibited a notable inability to process hand movements in the driving video, almost invariably leading to prominent and clearly delin- eated occlusion errors and spatial inconsistencies. •High Incidence of Severe Generation Failures: A substantial proportion of initial genera- tions were unusable due to extreme face warping/distortion, transforming the source image into an unrecognizable amalgam of textures and features, indicative of fundamental failures in identity preservation and coherent image synthesis. C.5 EMOPortraits  Artifacts observed in EMOPortraits generations included: •\"Floating Head\" Artifacts: Facial and head modifications were often restricted to the cephalic region, leading to a dissociation from neck and body movements. Significant corporeal motion in the driving video could result in the head appearing detached or \"floating,\" a clear example of inconsistent head movement/pose and spatial inconsistency. •Perifacial Blurring and Edge Anomalies: The model frequently introduced blurring in the regions immediately surrounding the synthesized face, potentially diminishing perceptual realism and constituting an edge anomaly or texture anomaly. •Off-Axis Pose Instability: The system encountered difficulties rendering non-frontal (e.g., three-quarter or full profile) views. Lateral head rotations could precipitate severe face warping/distortion, including apparent shrinkage, color mismatch or degradation into noise resembling video static. •Eyewear-Related Artifacts: The model struggled with subjects wearing eyeglasses, partic- ularly sunglasses. This often resulted in occlusion errors or texture anomalies where eyes were unrealistically rendered as visible through otherwise opaque lenses, or pupil anomalies. •Chroma Key-like Color Artifacts: Generated videos frequently exhibited spurious patches of bright, uniform color (commonly green), reminiscent of poorly executed chroma keying, indicating significant color mismatch or spatial inconsistencies. 20 Table D.9: Protocol 1 — MAGI-1 (Test Only). Best numbers for Magi-1 are bold. T1 metric performance colored (Excellent: >0.99, Good: 0.95-0.99, Reasonable: 0.85-0.95, Fair: 0.75-0.85, Poor: <0.75 ). Detector AUC T1 T0.1 CADDM 0.91 0.55 0.55 TALL 1.00 1.00 1.00 DF-Adapter 1.00 1.00 1.00 Table D.10: Protocol 3 — MAGI-1 (Test Only). Best numbers per (generator) are bold Detector AUC T1 T0.1 CADDM 0.94 0.52 0.52 TALL 1.00 1.00 1.00 DF-Adapter 1.00 1.00 0.89 D Additional Experimental Results D.1 MAGI-1 Commercial Generator Results We present detection performance on deepfakes generated by the MAGI-1 commercial model. This model was used as an additional unseen generator to test detector robustness. Since MAGI-1 does not have audio output, we only use it to evaluate the performance of CADDM, TALL, and DeepFake- Adapter (DF-Adapter). At T1 (Tab. D.9), both DeepFake-Adapter and TALL exhibit excellent performance on MAGI-1. When the threshold is tightened to T0.1 (Tab. D.10), DF-Adapter’s performance drops (T1=0.89) while TALL retains an excellent score (T1=1.00). This observation, particularly TALL’s strong performance and DF-Adapter’s drop at stricter thresholds, is consistent with trends observed with other generators, offering insights into detector generalizability against unseen commercial models. E Additional Explainability Results The main paper (Section 4.4) provides Grad-CAM visualizations for the TALL detector. This section includes additional Grad-CAM results for TALL and DeepFake-Adapter, the best two benchmarked detectors. These visualizations illustrate the image regions these models focus on, offering insights into their decision-making and potential biases across different generators and protocols. E.1 Grad-CAM Visualizations for TALL  Figure E.7 presents Grad-CAM visualizations of the TALL detector across Protocols 1–3, using one correctly classified high-confidence fake sample per generator per protocol. These examples reflect cases where TALL confidently and accurately identifies manipulations, allowing us to interpret what visual evidence it relies on for detection. Across most generators and protocols, TALL focuses its attention primarily on the facial re- gion—including key semantic features such as the eyes, nose, and mouth—indicating that it has learned to localize and exploit common deepfake artifacts. Notably, in Protocol 1, where the model is trained on all generators (i.e., no generator shift), TALL relies exclusively on background regions for Hallo2 and EMOPortraits. This behavior contrasts with its facial focus for other generators and suggests that TALL has learned to exploit generator-specific cues in the background—clues that other detectors likely overlook. In Protocol 3, where both identity and generator differ between train and test, TALL continues to rely on consistent facial features for most generators. However, it attends to the neck region for EMO- Portraits, which may again indicate an adaptation to persistent artifacts in that region. Importantly, all examples shown here are correct classifications, underscoring that TALL’s attention—whether focused on facial or peripheral features—is meaningfully aligned with its high performance. These visualizations suggest that TALL does not overfit to a single detection heuristic but instead generalizes flexibly across generators by identifying the most informative regions—be they facial 21 Figure E.7: Grad-CAM visualizations of the TALL detector on correctly classified, high-confidence fake samples across Protocols 1–3 (one sample per generator). TALL predominantly focuses on facial features such as the eyes, nose, and mouth, but strategically shifts its attention to background or peripheral regions for certain generators. In Protocol 1, where the model has seen all generators during training, it focuses on background cues for Hallo2 and EMOPortraits, suggesting that these regions carry distinctive generative artifacts. In Protocol 3, the detector localizes to the neck region for EMOPortraits, while maintaining facial focus elsewhere. These examples reflect TALL’s adaptive attention and its ability to leverage generator-specific cues that contribute to its superior generalization and detection performance. or contextual—for each case. This ability to adaptively shift focus may contribute to its strong generalization under distribution shifts. E.2 Grad-CAM Visualizations for DeepFake-Adapter  Figure E.8: Grad-CAM visualizations of DeepFake-Adapter across Protocols 1–3 (same samples as used for TALL). Compared to TALL, DF-Adapter shows broader and less localized attention maps, often covering both facial and background regions with reduced spatial focus. While it identifies meaningful areas in some cases (e.g., the mouth region in EMOPortraits under Protocol 1), attention weakens in Protocols 2 and 3, especially for EMOPortraits, correlating with its performance drop. The model appears to rely on a mixture of weak signals across the image rather than generator-specific artifact patterns, reflecting its less consistent generalization under distribution shifts. 22 Figure E.8 presents Grad-CAM results for the DeepFake-Adapter model across Protocols 1–3 using the same sample images previously analyzed for TALL. Compared to TALL, whose attention maps are generally well-localized around facial or generator-specific regions, DF-Adapter displays broader and more diffuse attention, often spanning both the face and background without a clearly focused region of interest. In many cases, DF-Adapter does attend to relevant facial areas—particularly the mouth and eyes—but the heatmaps suggest less spatial precision. This pattern is observed across most generators and protocols, indicating that DF-Adapter may rely on a combination of weak signals distributed across the image rather than distinct artifact cues. For example, in Protocol 1, the attention map for EMOPortraits is focused around the mouth, a region commonly manipulated in talking-head deepfakes. However, in Protocols 2 and 3, the attention on EMOPortraits degrades significantly, with minimal relevant focused regions, potentially explaining the model’s poor generalization to this generator under unseen conditions. The results also highlight DF-Adapter’s less adaptive generalization behavior: while it sometimes leverages background information (as TALL does for Hallo2 and EMOPortraits), it lacks the targeted selectivity shown by TALL. This likely contributes to its performance drop under Protocols 2 and 3, where subtle artifacts become harder to detect without robust, focused visual strategies. 23",
  "text_length": 79229
}