{
  "id": "http://arxiv.org/abs/2506.07326v1",
  "title": "Reward Model Interpretability via Optimal and Pessimal Tokens",
  "summary": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions.",
  "authors": [
    "Brian Christian",
    "Hannah Rose Kirk",
    "Jessica A. F. Thompson",
    "Christopher Summerfield",
    "Tsvetomira Dumbalska"
  ],
  "published": "2025-06-08T23:56:58Z",
  "updated": "2025-06-08T23:56:58Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CY",
    "cs.LG",
    "I.2.6; I.2.7; H.5.2; J.4; K.4.2"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.07326v1",
  "comments": "Accepted for publication in Proceedings of the 2025 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025",
  "full_text": "--- Page 1 ---\narXiv:2506.07326v1  [cs.CL]  8 Jun 2025Reward Model Interpretability via Optimal and Pessimal Tokens\nBrian Christian\nbrian.christian@psy.ox.ac.uk\nUniversity of Oxford\nOxford, UKHannah Rose Kirk\nhannah.kirk@oii.ox.ac.uk\nUniversity of Oxford\nOxford, UKJessica A.F. Thompson\njessica.thompson@psy.ox.ac.uk\nUniversity of Oxford\nOxford, UK\nChristopher Summerfield∗\nchristopher.summerfield@psy.ox.ac.uk\nUniversity of Oxford\nOxford, UKTsvetomira Dumbalska∗\ntsvetomira.dumbalska@psy.ox.ac.uk\nUniversity of Oxford\nOxford, UK\nAbstract\nReward modeling has emerged as a crucial component in align-\ning large language models with human values. Significant atten-\ntion has focused on using reward models as a means for fine-\ntuning generative models. However, the reward models themselves—\nwhich directly encode human value judgments by turning prompt-\nresponse pairs into scalar rewards—remain relatively understud-\nied. We present a novel approach to reward model interpretability\nthrough exhaustive analysis of their responses across their en-\ntire vocabulary space. By examining how different reward models\nscore every possible single-token response to value-laden prompts,\nwe uncover several striking findings: (i) substantial heterogene-\nity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens,\n(iii) significant sensitivity to prompt framing that mirrors human\ncognitive biases, and (iv) overvaluation of more frequent tokens.\nWe demonstrate these effects across ten recent open-source re-\nward models of varying parameter counts and architectures. Our\nresults challenge assumptions about the interchangeability of re-\nward models, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can\nencode concerning biases toward certain identity groups, which\nmay emerge as unintended consequences of harmlessness training—\ndistortions that risk propagating through the downstream large\nlanguage models now deployed to millions.\nCCS Concepts\n•Applied computing →Psychology ;•Human-centered com-\nputing→Empirical studies in HCI ;HCI design and evalua-\ntion methods .\nKeywords\nreward models, AI alignment, NLP, interpretability, value\n∗Both authors contributed equally to the paper\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nFAccT ’25, Athens, Greece\n©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM\nhttps://doi.org/10.1145/3715275.3732068ACM Reference Format:\nBrian Christian, Hannah Rose Kirk, Jessica A.F. Thompson, Christopher\nSummerfield, and Tsvetomira Dumbalska. 2025. Reward Model Interpretabil-\nity via Optimal and Pessimal Tokens. In Proceedings of The 2025 ACM Con-\nference on Fairness, Accountability, and Transparency (FAccT ’25). ACM, New\nYork, NY, USA, 12 pages. https://doi.org/10.1145/3715275.3732068\nCONTENT WARNING: This article presents examples of biased,\noffensive, sexually explicit and otherwise harmful text. The authors\ndo not endorse any of the harmful representations quoted below.\n1 Introduction\nThe alignment of large language models (LLMs) with human values\nhas emerged as one of the central challenges in modern AI develop-\nment, and at the heart of this challenge lie “reward models”—neural\nnetworks trained to directly proxy human preferences by transform-\ning text into scalar rewards. Though typically treated as disposable\nintermediaries in the larger alignment process, these models are\ncrucial objects of study in their own right as the most direct and ex-\nplicit encoding of human values in AI systems, yet are surprisingly\nunder-explored.\nThe typical process for aligning an LLM with human values in-\nvolves collecting a dataset of labeled pairwise human preferences,\nindicating which of two LLM responses to a given user prompt\nis preferred [ 6]. These preference data often distill multiple desir-\nable objectives, such as helpfulness, harmlessness, and honesty [ 1],\nwhich are operationalized through guidelines written by model\ndevelopers and interpreted by crowdworkers [ 20]. The resulting\ndataset is used to train a “reward model”—a transformer model that\ntakes in a prompt-response pair (or a longer user-assistant dialogue)\nand outputs a scalar that represents in effect how “preferable” that\nresponse is. These scalars are typically based on the Bradley-Terry\nscore [ 3], and the reward model is trained via stochastic gradient\ndescent to minimize the negative log-likelihood of the observed\npairwise preferences [ 25]. The trained reward model then acts as a\nscalable proxy for human preferences when using reinforcement-\nlearning algorithms such as Proximal Policy Optimization (PPO)\n[42] to fine-tune the LLM. This process—known as Reinforcement\nLearning from Human Feedback (RLHF)—results in LLM gener-\nations that maximize the reward model’s score rather than the\npre-training objective, so are supposedly more aligned with human\nvalues. While direct alignment algorithms like Direct Preference\nOptimization (DPO) [ 40] have grown in popularity, they capture\nequivalent preference relationships from the data, just without the\nreward model as an intermediary.\n--- Page 2 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\nAlthough reward models exist as a disposable reagent in the\nprocess of turning an “unaligned” LLM built to minimize predictive\nloss into an “aligned” one built to maximize this proxy of human\npreference, they are fascinating objects of research inquiry in their\nown right. Designed as generalizable proxies for human preference,\nthey offer more direct value encoding than downstream agents con-\nstrained by KL-divergence [ 18] and refusal training. As scalar map-\npings over complex dialogue, they distill multi-objective preference\ndata into uniquely interpretable low-dimensional representations of\nhuman value. They are in essence where “the human value rubber\nmeets the road.” Despite this, there is a dearth of literature analyz-\ning the properties of reward models, largely because few have been\npublicly available for study. While 2023–2024 saw a proliferation of\nopen-source language models, including Meta’s Llama [ 51], Mistral\nAI’s models [ 19], and Google’s Gemma series [ 49], to date nomajor\nindustry or nonprofit lab has openly released a reward model. Only\nrecently has this picture begun to change, with the release of Re-\nwardBench [26]—the first benchmark and leaderboard for reward\nmodels, spurring new activity among academic and open-source\ncommunities.\nIn this work on reward model interpretability, we seek to under-\nstand the consistency and faithfulness with which these models\nrepresent human values. Specifically, we make the following con-\ntributions:\n•We pioneer an exhaustive search over every single token\nin reward model vocabularies appended to a value-laden\nprompt, permitting the analysis of optimal and pessimal to-\nkens across ten top-performing open-source reward models\nonRewardBench of varying sizes and architectures.\n•We show that reward distributions exhibit systematic asym-\nmetries, with greater sensitivity between tokens in high-\nscoring regions relative to low-scoring regions, and to-\nkens with positive sentiment relative to negative sentiment.\nChanging to a negative-valence prompt inverts this latter\nbias, mirroring framing effects in humans.\n•We establish alignment between the (biased) interpretation\nof human preferences by reward models and an independent\nsource of ground-truth human preferences called EloEv-\nerything , where internet users volunteer judgments over\nconcepts, people, and things from Wikipedia pages.\n•We generalize our findings on similar biases and asymmetries\nto multi-token sequences using Greedy Coordinate Gradient\noptimization.\n•Through these sequential analyses, we reveal (i) significant\nheterogeneity across similarly-trained reward models, inval-\nidating their presumed fungibility, and (ii) systematic deval-\nuation of identity group references (“homosexuals,” “Black\npeople” and “Jews”), possibly arising as unintended biases\nfrom harmlessness training objectives.\nThrough exposing the idiosyncrasies of reward models as re-\nsearch artifacts, we aim to highlight pitfalls in their current devel-\nopment, provide recommendations for building more robust proxies\nof human value, and ultimately advance the broader goal of creating\naligned and safe AI systems.2 Ranking Optimal and Pessimal Tokens\nOpen-source reward models that convert tokens to a single scalar\nreward permit the somewhat radical idea of exhaustive search. In-\nspired loosely by Dawson [7], who demonstrated that exhaustive\ntesting of floating-point math functions is feasible (arguing that\n“there are only four billion floats—so test them all”), we seek a com-\nplete ranking of rewards assigned to all possible responses to a\nuser prompt in order to granularly and comprehensively evaluate\nreward model characteristics.\nWe used the reward-model benchmark RewardBench [26],\nwhich provides a online leaderboard for top-performing reward\nmodels. From this leaderboard, we selected a diverse set of ten\nmodels: nine high-performing models ranging from 3B to 27B pa-\nrameters drawn from the top twenty rankings, plus the leading\n2B-parameter model. These models cover a range of developers,\nbase architectures, and model sizes (see Table 1).\nTo make exhaustive search computationally tractable, we fo-\ncused on single-token responses, as multiple-token search quickly\nbecomes combinatorially complex. We designed an initial prompt\nthat specifically elicits brief responses with clear valence:1\nWhat, in one word, is the greatest thing\never?\nWe then computed the reward model score when supplying every\nsingle token in the model’s vocabulary ( 𝑁≈256,000in the case\nof Gemma and 𝑁≈128,000in the case of Llama 3) as a response\nto this prompt. These token vocabularies include words and word\nfragments in English and non-English languages (including non-\nRoman alphabets), fragments of computer code, emoji, variations\nof whitespace, and control tokens. Having scored the whole of\neach model’s vocabulary, we then sorted the tokens by their scalar\nreward scores.\nA more conventional approach would be to examine the log\nprobability distribution outputted by fine-tuned models, which rep-\nresent the statistically most likely continuations of a prompt after\nreward training. When we attempted this, we found that fine-tuned\nmodels tended to overindex on common tokens and statistical regu-\nlarities like “The” and “A” (see Sec. A.1.2 and Table A.1; however we\nalso note that variations in the prompt might have produced more\ncomparable answers to those reported here for reward models).\nNevertheless, the investigations below imply that reward models\nprovide a useful window into value interpretability beyond conven-\ntional analyses.\n2.1 Qualitative Observations\nApplying this methodology to ten reward models reveals stark\nqualitative differences in the token rankings between models—even\nthose from the same developer. We report optimal and pessimal\ntokens for two such models ( ■R-Gem-2B and ■R-Lla-3B) in Table 2,\nand present all other models in Tables A.6–A.7.\nThere are striking differences in both their highest and lowest\nreward assignments. At the positive extreme, ■R-Gem-2B priori-\ntizes affective content over grammatical correctness (e.g., ranking\n“miraculous” above “miracle”). It also prominently features the sur-\nprisingly obscure word “sonder,” a neologism coined in 2012 by\n1We focused our analyses on English, as reward models are predominantly trained on\nEnglish data. Additional analyses across prompt variants are presented in Sec.3.\n--- Page 3 ---\nReward Model Interpretability via Optimal and Pessimal Tokens FAccT ’25, June 23–26, 2025, Athens, Greece\nTable 1: Open-source reward models studied. The table includes both their full names and the shortened identifiers (Model IDs)\nused throughout the rest of this paper. Ranks are from the RewardBench Leaderboard as of January 14, 2025.\nRewardBench Rank Model ID Developer Model Name Base Model Parameters (B)\n2■N-Gem-27B nicolinho QRM-Gemma-2-27B[9] Gemma 2[50] 27\n3■S-Gem-27B-v0.2 Skywork Skywork-Reward-Gemma-2-27B-v0.2[31] Gemma 2 27\n5■S-Gem-27B Skywork Skywork-Reward-Gemma-2-27B[31] Gemma 2 27\n10■S-Lla-8B-v0.2 Skywork Skywork-Reward-Llama-3.1-8B-v0.2[31] Llama 3.1[10] 8\n11■N-Lla-8B nicolinho QRM-Llama3.1-8B[9] Llama 3.1 8\n12■L-Lla-8B LxzGordon URM-LLaMa-3.1-8B[32] Llama 3.1 8\n17■R-Lla-8B Ray2333 GRM-Llama3-8B-rewardmodel-ft[53] Llama 3 8\n19■R-Lla-3B Ray2333 GRM-Llama3.2-3B-rewardmodel-ft[53] Llama 3.2 3\n20■F-Lla-8B-v0.1 RLHFlow ArmoRM-Llama3-8B-v0.1[52] Llama 3 8\n31■R-Gem-2B Ray2333 GRM-Gemma2-2B-rewardmodel-ft[53] Gemma 2 2\nTable 2: Optimal and pessimal tokens for the prompt “What, in one word, is the greatest thing ever?”. Note that because\nthe Llama and Gemma tokenizers distinguish between start-of-word strings and intra-word strings, some tokens differ only\nby leading whitespace. CONTENT WARNING: We present tokens in their raw form (including slurs) to enable transparent\nattribution of model tokens, while acknowledging their offensive, troubling and harmful nature.\nR-Gem-2B\nToken ID Decoded Score\n27534 LOVE 4.594\n61792 LOVE 4.562\n218136 felicity 4.469\n2182 love 4.344\n12870 love 4.312\n7377 Love 4.281\n8703Love 4.281\n227570 sonder 4.219\n143735 sonder 4.219\n27539 Wonder 4.188\n34183 Wonder 4.188\n174540 HOPE 4.156\n115221 HOPE 4.125\n5144 wonder 4.094\n53798 wonder 4.094\n167954 WONDER 4.031\n50999 bliss 3.922\n207783 bliss 3.922\n65646 JOY 3.922\n135936 JOY 3.922\n89399 miraculous 3.875\n40241 miracle 3.859\n...... ...\n61001 blacks -9.250\n218552 pathologist -9.250\n97070 killers -9.312\n167921 prostitutes -9.312\n222988 massacres -9.312\n106863 FUCKING -9.312\n213624 rapist -9.312\n127732 ransomware -9.375\n204573 retards -9.438\n195353 nazis -9.438\n137696 murdering -9.438\n37678 Hitler -9.500\n230672 Rape -9.500\n134768 Rape -9.500\n231158 faggot -9.500\n144817 murderous -9.500\n152471 murderers -9.500\n39688 rape -9.562\n144068 Hitler -9.562\n186353 rape -9.625\n158058 negroes -9.625\n201371 raping -9.625R-Lla-3B\nToken ID Decoded Score\n11542 freedom 3.359\n86872 Freedom 3.266\n25320 Freedom 3.266\n40835 LOVE 3.250\n61094 LIFE 3.203\n83900 .life 3.000\n24966 CONNECTION 2.969\n28899 imagination 2.844\n10919 Love 2.672\n29351 Love 2.672\n48379 Opportunity 2.641\n57184 UNITY 2.438\n57273 UNITY 2.438\n6776 opportunity 2.406\n32883 loyalty 2.359\n3021 love 2.281\n31153 love 2.281\n39193 Discovery 2.266\n68500 Discovery 2.266\n35215 Experience 2.234\n21460 Experience 2.234\n85743 Peace 2.156\n...... ...\n87546 raping -10.688\n86395 .FindGameObjectWithTag -10.688\n38853 ${ -10.688\n18350 (con -10.750\n27817 _headers -10.750\n58467 .insertBefore -10.750\n6019(st -10.750\n29372 (cfg -10.750\n5747.setText -10.750\n27701 .startsWith -10.750\n26342 /******************... -10.812\n97615 ##################... -10.812\n85399 ###################... -10.812\n76897 _checks -10.875\n58352 (\"[% -10.875\n74061 /******************... -10.938\n42864 homosexual -10.938\n6294(struct -10.938\n27249 .startswith -11.000\n94380 jihadists -11.062\n97223 homosexuals -11.312\n37289 .assertFalse -11.438\n--- Page 4 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\nwriter John Koenig to mean “the realization that each random\npasserby is living a life as vivid and complex as your own” [ 24]. The\n■R-Lla-3B model from the same developer instead puts “freedom”\nahead of “love,” and on the whole the high rankings of “freedom,”\n“opportunity,” “discovery,” and “experience” paint a more individ-\nualistic, active picture of human value than ■R-Gem-2B’s more\ninterdependent, affective words like “love,” “wonder,” and “hope.”\nThe models diverge even more dramatically in their lowest-\nranked tokens, revealing some (concerning) artifacts from reward-\nmodel training objectives. The lower ranks of the ■R-Gem-2B\nmodel are tied to human harm and suffering like “rape,” “Hitler,”\nand “murderers, ” as well as slurs and profanities, suggesting a strong\ninfluence of a harmlessness objective in training. In contrast, ■R-\nLla-3B’s lowest ranks are predominantly occupied by malformed\ncode tokens and programming artifacts like “.assertFalse” and “/****”,\nsuggesting stronger traces of a helpfulness objective. Both mod-\nels exhibit concerning behaviors over tokens relating to minority\nidentity groups (e.g., “blacks” or “homosexuals”). These patterns\nlikely stem from artifacts in reward model training data, where\nidentity groups are disproportionately represented in unsafe or\n“rejected” examples, leading to their systematic devaluation—even\nin response to a positive prompt of the “greatest thing ever.” This\nlinguistic erasure mirrors documented phenomena in hate-speech\ndetection, where models develop oversensitive false positive rates\nfor identity terms (e.g., “Muslim,” “mosque”) or reclaimed slurs\ndue to their overrepresentation in negative training contexts and\nunderrepresentation in neutral or positive ones [8, 38, 41].\n2.2 Quantitative Analysis\nQuantitatively, we note that, despite differences of the scale of\nscores across models (Fig. 1), all score distributions exhibit a posi-\ntive skew (Table 3). That is, most tokens receive low rewards, while\na small number of tokens score substantially higher than average,\ncreating a long right tail in the distribution. A positively skewed\nreward distribution may be appropriate given that RLHF updates\nmodel parameters to maximize expected reward, making the dis-\ncriminative power of the upper tail most consequential for learning.\nWe assessed the consistency of token rankings across models\nusing an ordinal correlation measure, Kendall’s 𝜏, Fig. 2A (results\nare consistent across choice of correlation metric, see Fig. A.1).\nWhilst all models exhibit positive correlations, there is substantial\ndiversity among the models studied. We explored this diversity\nusing multidimensional scaling (MDS)—a visualization technique\nthat aims to faithfully represent the degree of similarity between\ndata points (here, reward models) in lower dimensionality (here,\n2D). This analysis reveals that models with a similar number of\nparameters, shared base model, and shared developer cluster closer\nin latent space (Fig. 2B).\nTo partial out the influence of these factors, we conducted an\nanalysis inspired by representational similarity analysis (RSA), a\ntool commonly used in neuroscience. We regressed the (flattened)\nobserved empirical model correlation matrix in Fig. 2A on theoreti-\ncal model similarity matrices based on the three factors of interest\n(base model, developer, number of parameters) and the rank of the\nmodel on the RewardBench leaderboard (Fig. 2C). Each of these\nfactors is, on its own, significantly associated with the empirical\nN-Gem-27B\nS-Gem-27B-v0.2S-Gem-27BS-Lla-8B-v0.2N-Lla-8B L-Lla-8B R-Lla-8B R-Lla-3B\nF-Lla-8B-v0.1R-Gem-2B40\n20\n02040ScoreFigure 1: Violin plot of exhaustive score distributions to the\n“greatest thing” prompt. The reward models differ strikingly\nin their distributions of reward scores in terms of scale and\nrange.\nModel Mean Variance Skewness\n■N-Gem-27B 1.876 0.193 0.437\n■S-Gem-27B-v0.2 -9.274 1.017 0.071\n■S-Gem-27B -30.422 11.991 0.878\n■S-Lla-8B-v0.2 -18.699 5.716 1.117\n■N-Lla-8B 16.613 9.558 1.133\n■L-Lla-8B -0.137 0.034 1.763\n■R-Lla-8B -14.239 0.781 0.504\n■R-Lla-3B -6.777 2.597 1.672\n■F-Lla-8B-v0.1 0.031 <0.001 1.055\n■R-Gem-2B -5.279 1.957 1.457\nTable 3: First three moments of reward distribution across\nall shared tokens. All reward models exhibit varying degrees\nof positive skew.\npattern of correlations between models (simple linear regression,\nall𝑝< .001). However, when combining the four factors together\nin a competitive multiple regression, the variance predicted by\nbase model, developer, and the number of parameters appears to\nbe almost entirely soaked up by the ranking of the model on Re-\nwardBench (RewardBench rank 𝑝< .0001, base model 𝑝< .10,\nall other 𝑝> .10). Running a stepwise regression (factor knock-in\nand knock-out) confirms that the regression that best explains the\nobserved data features the base model and RewardBench ranking\ntheoretical matrices (base model 𝛽= 0.05, RewardBench ranking\n𝛽= 0.69, 𝑅2= .80). It is perhaps unsurprising that model ranking\nonRewardBench can capture patterns of reward model similarity\nsince it measures how well the models are all aligned against the\nsame external objective (i.e., the RewardBench benchmark and its\ncomposite evaluation datasets). Interestingly, our results suggest\nthat the choice of base model drives differences in token rankings\n--- Page 5 ---\nReward Model Interpretability via Optimal and Pessimal Tokens FAccT ’25, June 23–26, 2025, Athens, Greece\nA\nN-Gem-27B\nS-Gem-27B-v0.2\nS-Gem-27BS-Lla-8B-v0.2N-Lla-8BL-Lla-8B\nR-Lla-8BR-Lla-3BF-Lla-8B-v0.1\nR-Gem-2B B\n C\nFigure 2: (A) Heatmap depicting the pairwise Kendall’s 𝜏correlations between the reward models for scored responses to\nthe prompt “What, in one word, is the greatest thing ever?”. (B) Visualization of the degree of similarity between reward\nmodels using multidimensional scaling (MDS) of the Kendall’s 𝜏distance measure. (C) Theoretical dissimilarity matrices for\nrepresentational similarity analysis (RSA). The four dissimilarity matrices encode, respectively, base model [base𝑖=base𝑗];\ndeveloper[dev𝑖=dev𝑗]; parameter count (1+|params𝑖−params𝑗|)−1; and RewardBench ranking (1+|rank𝑖−rank𝑗|)−1.\nabove and beyond alignment to RewardBench . That is, reward\nmodels appear to inherit idiosyncratic biases from the pretrained\nbase model.\n3 Framing Effects\n3.1 Sentiment Analysis\nTo further explore explanations for token rankings, we investigated\nthe relationship between sentiment, or the emotional value of a\ntoken, and its reward model score. We quantified emotional value\nusing data from two validated linguistic corpora widely used within\nthe field of psychology and developed by human experts: Bing [30]\nandAFINN-111 [36].Bing codes words as “positive” or “negative”;\nAFINN-111 indexes a score ranging from −5to5for the sentiment\nvalue. Across both corpora, we found a positive association between\nreward score and sentiment, where scores are consistently higher\nfor positive-sentiment tokens (Figs. 3A, A.3–A.4). These results are\nin line with what we would expect: positive tokens are more likely\nto score highly as an appropriate response for a prompt that asks\nfor the “greatest thing ever.” In line with the skewness of the score\ndistribution in Sec. 2, we found that (i) scores for positive-sentiment\ntokens are more spread out than scores for negative-sentiment\ntokens, and (ii) the slope for the relationship between sentiment\nand score is significantly steeper for positive than negative tokens\n(Fig. 3A; 𝛽pos>𝛽neg:𝑡(9) = 2.6, 𝑝< 0.05). This finding suggests\nthat the reward model is more sensitive to distinctions in positive\nsentiment relative to negative sentiment. The results are highly\nconsistent across models (8/10 models exhibit the effect).\n3.2 Prompt Framing\nTo explore whether the differential sensitivity of the model is driven\nby the specifics of the prompt or generalizes across queries, we\nextended our analyses to two more prompts: a positive variant\n(“What, in one word, is the best thing ever?”) and a negative contrast\n(“What, in one word, is the worst thing ever?”). Perhaps expectedly,scores for “the best thing ever” are highly consistent with those\nfor “the greatest thing ever.” We found a high positive correlation\nbetween model scores for these two prompts across all models\n(Fig. A.2). We replicated (i) the positive skewness of the distribution\nof scores and (ii) the differential sensitivity to positive over negative-\nsentiment tokens (Fig. 3C; 9/10 models exhibit the effect).\nThe pattern is different for “the worst thing ever.” Here, the dis-\ntribution of scores remains skewed toward higher-scoring tokens,\nhowever, it is the negative-sentiment tokens that receive higher re-\nwards for this prompt. Thus, models are, on average, more sensitive\nto negative-sentiment tokens relative to positive-sentiment ones\n(significantly steeper slope for negative- over positive-sentiment\ntokens, Fig. 3B–C; 5/10 models exhibit the effect). Our findings\nsuggest that model sensitivity to the appropriateness of tokens\ndepends on framing. When the prompt is framed positively (“best\nthing ever”), scores are more sensitive to positive than negative\ntoken sentiment and more sensitive to negative than positive token\nsentiment when the prompt is framed negatively (“worst thing\never”). This result is consistent with human behavior. If a question\nis positively framed, humans are more attuned to positive informa-\ntion, and vice-versa for negative frames. Consider a scenario where\nyou need to pick between two vacation destinations: an exciting\noption with many positive features (dream destination, beautiful\nnature) and just as many drawbacks (expensive, long travel) versus\na safer option with fewer positive and negative stand-out features\n(e.g., a local getaway). If asked to choose between those two vaca-\ntion spots in a positive frame (“which [one] would you prefer?”),\nhuman participants tend to choose the option with more positive\nfeatures; if asked to choose in a negative frame (“which [one would\nyou] cancel?”), they pick the option with more negative features,\neven though it is in fact the same option [43].\nThe effect of framing on sensitivity has important implications.\nIf the goal of RLHF is to steer the model away from generating\nharmful or unsafe responses, then the reward model needs to be\n--- Page 6 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\nFigure 3: (A) Correlation plot between token sentiment value according to the AFINN-111 lexicon and the scores from the\n■S-Lla-8B-v0.2 reward model with the prompt “What, in one word, is the greatest thing ever?” (B) As previous, but for prompt\n“What, in one word, is the worst thing ever?” (C) Estimate for the slope for token sentiment value from a simple linear regression\npredicting reward model score computed separately for each model, prompt and sentiment valence (positive and negative). Each\ncolored dot indicates a model; diamonds represent mean ±standard error. Slope estimates are, on average, higher for positive\nsentiment. They are steeper for positive-sentiment valence in positively framed prompts and steeper for negative-sentiment\nvalence in negatively framed prompts. (D) Estimate for the slope for normalized word frequency from a multiple linear\nregression predicting reward-model score controlling for sentiment value; computed separately for each model and prompt.\nScores are positively associated with word frequency, suggesting a “mere-exposure effect” in the reward models.\nsufficiently sensitive in the negative-sentiment portion of token\nspace. Current practice—asking human raters to choose a preferred\noption (“which is the better response,” not “which is the worse\nresponse”)—may inadvertently be undermining that objective by\nbiasing the dynamic range of the reward model toward positive\ntokens.\nTaken together, our results further suggest the reward models\ndo not interpret “best” as simply the inverse of “worst.” The dis-\ntribution of scores across those two prompts resembles a funnel\n(Fig. 4) where many tokens are bad responses to both prompts (bot-\ntom left) and some tokens are good responses to one prompts but\nnot the other (top left and bottom right). We also see a thin tail\nof tokens that are highly-scored responses for both prompts (top\nright; “sonder” features in this category, along with non-committal\nanswers like “depends” and refusals like “impossible”). In the ap-\npendix, we include tables that index the best+worst (tokens that\nscore similarly on both prompts) and best−worst axes (tokens that\nscore highly on one but not the other prompt); see Tables A.8–A.9.\n3.3 Frequency Bias\nAre the scores that reward models assign to different tokens biased\nby how frequently the word appears in the English language? To\nassess this, we used data from Word Frequencies in Written and\nSpoken English [ 27] and regressed log-transformed word frequency\non reward-model scores. Higher word frequency is associated with\nhigher reward-model score across prompts for the majority of mod-\nels (positive slope estimates with 𝑝< .05 in 10/10 models for “best,”\n8/10 for “greatest,” and 7/10 for “worst”). This is reminiscent of\nthe “mere-exposure effect” in humans, where the more someone is\nexposed to a stimulus, the more they like it [ 54]. One could argue\nthat this effect is driven by positive words being more frequent\nin general in English. To account for this, we controlled for thesentiment value of the tokens. This adjustment did not abolish the\n“mere-exposure effect,” but made it more pronounced in the nega-\ntively framed query (Fig. 3D, positive slope estimates with 𝑝< .05\nin 2/10 models for “best,” 5/10 for “greatest” and 8/10 for “worst”).\nThis “mere-exposure effect” is a surprising result, since reward\nmodels are meant to provide information that is orthogonal to the\nunderlying distribution of tokens, pertaining to, e.g., helpfulness\nand harmlessness. It suggests that there may be a leakage from\nthe pretrained base models into the reward models, whereby more\ncommon tokens may be scored more highly than they should be.\nMore work is needed to understand this phenomenon, and also\nthe degree to which this “mere-exposure effect” interacts with the\ndownstream KL-divergence regularizer typically used when fine-\ntuning LLMs against the reward model.\n4 Alignment with EloEverything\nThus far we have identified internal inconsistencies within and\nacross reward models. However, reward models are intended to\nproxy human value judgments. Establishing the faithfulness of\nthis proxy function requires an external baseline of human value\njudgments. We sourced this external human preference data from\nEloEverything ,2a crowdsourcing platform that implements pair-\nwise preference learning over things, people, and concepts uploaded\nfrom Wikipedia. On EloEverything , internet users are presented\nwith pairs of Wikipedia-derived entities (accompanied by images)\nand volunteer their judgments in response to the prompt: “Which\ndoyourank higher?”(see Fig. 5A), with options to request additional\ncontext or skip. The platform aggregates these pairwise compar-\nisons using the Elo rating system, which was originally developed\n2https://eloeverything.co/.\n--- Page 7 ---\nReward Model Interpretability via Optimal and Pessimal Tokens FAccT ’25, June 23–26, 2025, Athens, Greece\n2 4\nBest score24Worst scoreN-Gem-27B\n15\n 10\n 5\nBest score15\n10\n5\nWorst scoreS-Gem-27B-v0.2\n40\n 20\nBest score40\n20\nWorst scoreS-Gem-27B\n20\n 10\n 0\nBest score20\n10\n0Worst scoreS-Lla-8B-v0.2\n10 20 30\nBest score102030Worst scoreN-Lla-8B\n0 1\nBest score01Worst scoreL-Lla-8B\n15\n 10\nBest score15\n10\nWorst scoreR-Lla-8B\n10\n 5\n 0\nBest score10\n5\n0Worst scoreR-Lla-3B\n0.025 0.050 0.075\nBest score0.020.040.060.08Worst scoreF-Lla-8B-v0.1\n10\n 0\nBest score10\n5\n0Worst scoreR-Gem-2B\nFigure 4: Juxtaposing exhaustive scores for the “best thing” prompt against the “worst thing” prompt reveals not just a simple\nnegative correlation, but also an orthogonal dimension representing tokens that are bad or good responses to both frames.\nfor chess rankings [ 12] and has since been widely adopted to eval-\nuate LLMs [ 1,2,5]. We collected all data from the EloEverything\nwebsite, resulting in a dataset that comprises 𝑁users = 12,515 users\nwho evaluate 𝑁items = 7,530 items across 𝑁pairings = 1,805,124 total\npairwise comparisons. Although the dataset is highly imbalanced\nby the ratings each item receives ( 𝜇= 479.4 pairings/item, 𝜎=\n464.4)3and likely imbalanced across non-representative users (user-\nlevel data is not available), it still serves as a valuable independent\nbaseline.\nTo ensure as fair a comparison as possible between the tasks\nadministered to humans and reward models, we made two method-\nological adjustments (as compared to Sec. 2). First, we modified our\nprompt to “What one single thing, person, or concept is the great-\nest ever?” to better align with the human task and accommodate\nthe multi-word concepts that appear in EloEverything , such as\n“Sliced bread,” “Female body shape,” “Freedom of the press” or even\n“Beliefs and practices of the Church of Jesus Christ of Latter-day\nSaints” ( 𝜇= 2.1 words/item, 𝜎= 1.3).4Second, rather than exhaus-\ntively evaluating the entire model vocabulary, we restricted this\nanalysis to the set of EloEverything items ( 𝑁items =7,350) to\nensure a common human–model comparison set. We used the same\nset of models as in Table 1, and normalized both reward model\nscores and human Elo ratings to rankings, using average rank for\nties.\n4.1 Heterogeneity and Asymmetry in\nHuman-Model and Model-Model Alignment\nOur analysis reveals several notable patterns in the relationship be-\ntween the ground truth preferences of EloEverything raters and\nhuman preferences as interpreted by reward models. There is sub-\nstantial heterogeneity in model-human alignment and model-model\n3This is because users can upload new items at any point. At the time of data collection\n(January 10th 2025), “Evolution” appears in 3,195 pairings, while “Mac Miller” and\n“Penile injury” appear in only 6.\n4To test generalizability across prompts, we also repeated analysis in this section\nfor our original prompt (“What, in one word, is the greatest thing ever?”) and an\nalternative variant (“What is the single thing, person, or concept that humans most\nprefer?”). We present results in Figs. A.5–A.6alignment (Fig. A.5). The mean Kendall’s 𝜏correlation between hu-\nman and model rankings is 0.29 ( 𝜎= 0.06), with coefficients ranging\nfrom 0.22 to 0.39 across models, indicating only moderate rank\nagreement. Divergence of reward models from human rankings\nmight be expected—given that EloEverything users benefit from\nadditional context like images and can personalize their ratings\nwith “Which do yourank higher”—but the observed heterogeneity\nextends to model-model comparisons. The mean Kendall’s 𝜏corre-\nlation between pairs of different models is 0.55. In addition to these\nquantitative differences, we note anecdotal inconsistency in the\nbest and worst ranked tokens. For example, while “Unconditional\nlove” is ranked best by 5 models, “Sports bra” is top for ■R-Lla-8B\nand “ Gödel, Escher, Bach ” for■F-Lla-8B-v0.1. As demonstrated in\n(Fig. 5C), ranks display substantial movement across models, chal-\nlenging the assumption that reward models trained under a similar\nobjective can be used interchangeably.\nThere is systematic asymmetry in how models handled items\nat different ends of the human preference distribution (Fig. 5C).\nReward models show stronger agreement with human rankings\nfor highly-rated items ( 𝜏top100 = 0.19) compared to low-rated items\n(𝜏bottom100 = 0.08). This asymmetry (and heterogeneity) is evident\nin Fig. 5C, where “The Holocaust” (ranked worst by humans) is\nranked substantially higher by most models, even those performing\nwell on RewardBench . This corroborates our findings in Sec. 2,\nwhere models are more sensitive to high- over low-scoring tokens\n(the positive skew of the score distribution).\n4.2 Analyzing Discrepancies between Value as\nPerceived by Humans and Models\nRelative to human rankings, reward models systematically under-\nvalue concepts related to nature and life, e.g., “Universe” (human\nrank #𝐻=1, mean rank across models #¯𝑀=320), “Gravity” ( #𝐻=7,\n#¯𝑀=320), “Breathing” ( #𝐻=16.5,#¯𝑀=321); and technological\nconcepts, e.g., “Technology” ( #𝐻=47.5,#¯𝑀=633.5), “Electron-\nics” ( #𝐻=78,#¯𝑀=3966 ), “Computer” ( #𝐻=95.5,#¯𝑀=1352 .5).\nIn contrast, the majority of top words ranked by models repre-\nsent more affective qualia (e.g., “Unconditional love, ” “Imagination, ”\n“Hope,” or “Happiness”). These different perspectives may reflect\n--- Page 8 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\n100\n101\n102\n103Ranks 1-1000 (log scale)\nHuman\nN-Gem-27B\nRB #=2\nρ=0.29\nS-Gem-27B-v0.2\nRB #=3\nρ=0.33\nS-Gem-27B\nRB #=5\nρ=0.40\nS-Lla-8B-v0.2\nRB #=10\nρ=0.39\nN-Lla-8B\nRB #=11\nρ=0.37\nL-Lla-8B\nRB #=12\nρ=0.35\nR-Lla-8B\nRB #=17\nρ=0.37\nR-Lla-3B\nRB #=19\nρ=0.54\nF-Lla-8B-v0.1\nRB #=20\nρ=0.38\nR-Gem-2B\nRB #=31\nρ=0.36\n1\n1000\n2000\n3000\n4000\n5000\n6000\n7000All Ranks (#1 = best, #7530 = worst)\nBottom 5 ranked items by humans\nNazi Germany (#7526)\nGenocidal rape (#7527)\nChild abuse (#7528)\nChild pornography (#7529)\nThe Holocaust (#7530)Top 5 ranked items by humans\nUniverse (#1)\nWater (#2)\nInformation (#3)\nKnowledge (#4)\nLove (#5)Best ranked items by models\nUnconditional love (#59)\nImagination (#64)\nCompassion (#73)\nSports bra (#2167)\nGödel, Escher, Bach (#3997)Best ranked items by models\nUnconditional love (#59)\nImagination (#64)\nCompassion (#73)\nSports bra (#2167)\nGödel, Escher, Bach (#3997)\nA B\nC\n−5000 0 5000\nAverage model rank minus human rankFingering (sexual act)\nErection\nBlack people\nSpoons sex position\nHandjob\nCleavage (breasts)\nFellatio\nDoggy style\nSexual fetishism\nBreast\nBoygenius\nGod in Islam\nCM Punk\nCharlie Munger\nGurren Lagann\nMiranda Sings\nBTS\nMustafa Kemal Atatürk\nJohan Cruyff\nFully Automated\nLuxury Communism\nHumans rank worse\nthan modelsHumans rank better\nthan models\nFigure 5: (A) The EloEverything ranking interface where users make pairwise preference judgments between items (e.g.,\n“Bike lane” vs “Sliced bread”). (B) Maximum differences between human and average model rankings over items in response to\nthe prompt “What one single thing, person, or concept is the greatest ever?”, showing cases where humans rank items higher\n(green) or lower (purple) than models. (C) Rank trajectory plot showing how human and model ranks differ. We plot (i) the top\n5 items in the human rank (blue color scale with human ranks shown in legend parentheses as #n), (ii) the bottom 5 items in the\nhuman rank (red color scale), and (iii) unique items ranked #1 by models. Specifically, “Unconditional love” is #1 for 5 models;\n“Compassion” is #1 for ■N-Gem-27B; “Imagination” for ■S-Gem-27B; “Sports bra” for ■R-Lla-8B; and “ Gödel, Escher, Bach ” for\n■F-Lla-8B-v0.1. Models are ordered by the RewardBench leaderboard, and shown alongside their Spearman correlation to\nhuman ranks. The dashed box indicates zoomed inset region of top 1,000 ranks shown with a log scale.\nblindspots in learning value through language or literary reference\nalone, without additional modalities that reflect the nuances of\nembodied human experience.\nThe most striking valuation differences (see Fig. 5B) emerges\naround sexual content e.g., “Sex” ( #𝐻=69.5,#¯𝑀=2022 .5), “Hu-\nman sexual activity” ( #𝐻=45,#¯𝑀=5913), and concerningly, the\nidentity group reference “Black people” ( #𝐻=202.55,#¯𝑀=6591).\nThere are many possible explanations for discrepancies between\nEloEverything data and reward model scores, including the fact\nthat the 12,515 EloEverything users in the dataset do not reflect\na representative population sample, nor are they incentivized to\nprovide truthful responses. However, such discrepancies also exem-\nplify the fundamental challenge of assigning a single scalar score\nto language without full context (which is exacerbated further by\nour prompt, which encourages short responses). Sex- or identity-\nrelated terms may be entirely appropriate in positive or educational\ncontexts while highly inappropriate in others. As we suggested pre-\nviously, while humans readily grasp this dual use, reward models\nmay hedge against their usage to satisfy a harmlessness objective,paradoxically causing harm through linguistic erasure and devalu-\nation as an unintended consequence.\n5 Searching for Longer Optimal and Pessimal\nToken Sequences with GCG\nAlthough it would be computationally prohibitive to exhaustively\nsearch for optimal and pessimal multi-token sequences, one can\nuse discrete optimization methods to search for responses that\nyield high/low reward for a particular prompt. Greedy Coordinate\nGradient (GCG) [ 56] is a discrete token optimization algorithm\noriginally proposed to search for prompt suffixes to “jailbreak”\naligned LLMs. In that application, the optimized loss is the cross-\nentropy between the model response and some target response.\nStarting from some initial suffix string, GCG iteratively proposes\ncandidate token swaps based on the gradient. Since the search\nprocess only swaps tokens (never adds or removes tokens), the\nresulting string will be composed of the same number of tokens as\nthe starting string.\n--- Page 9 ---\nReward Model Interpretability via Optimal and Pessimal Tokens FAccT ’25, June 23–26, 2025, Athens, Greece\nStarting from the nanoGCG5implementation, we modified\nGCG to instead search for responses that maximize/minimize\nthe reward value when paired with a particular prompt, using\na mean squared error loss on the reward value. We also imple-\nmented the modifications described in the Faster-GCG paper [28],\nwhich we found to be especially important when searching for\nshort (2–5) token sequences where the original GCG algorithm is\nprone to self-loops. This modified implementation can be found at\nhttps://github.com/thompsonj/nanoGCG. We began by searching\nfor optimal and pessimal 2- and 3-token sequences according to the\n■R-Gem-2B model for the same prompts that were used for the\nexhaustive single token search above: “What, in one word, is the\ngreatest thing ever?”, “What, in one word, is the best thing ever?”,\nand “What, in one word, is the worst thing ever?” (see Table 4). To\nexplore longer sequences, we omitted the “in one word” direction\nfrom the prompt and ask simply “What is the best thing ever?” and\n“What is the worst thing ever?” Results from several searches are\nin Table 5.6\nThese search results suggest several patterns, many of which are\nconsistent with observations from the single token analyses. Re-\nsponses made up of programming related tokens with no semantic\ncontent score low on both “best” and “worst” prompts. Answers\nthat emphasize the subjective nature of the question score highly\nfor both “best” and “worst” prompts, but especially so for the “worst”\nprompt. This might be reflective of a general avoidance of negative\nsentiments in the response, even in cases when negative sentiment\nwould be appropriate. As observed in the single token analysis,\nmulti-token optimal responses to the “worst” prompt generally\nhave lower scores than for the “best” and “greatest” prompts. Inter-\nestingly, here too we found that some tokens emerge as extreme\noutliers for both positively and negatively framed prompts. For\ninstance, the token “Jews” appears among the pessimal answers for\nboth “greatest” and “worst” prompts. This finding further speaks\nto the linguistic erasure effect discussed earlier. The prevalence of\nemojis in the optimal multi-token sequences is notable. It is also\ninteresting to note that many of the optimal search results are not\ngrammatical—a feature that likely distinguishes the reward model\nfrom the ultimate fine-tuned language model.\nWhile longer token sequences do not admit the kind of fully ex-\nhaustive search (and full characterization of the score distribution)\nthat is possible with single-token sequences, we have seen that\nrecent techniques such as GCG and its offshoots make the interro-\ngation of optimal and pessimal responses possible even at greater\nlength. Despite compute limitations, uncovering such linguistic\n“superstimuli” (akin to the visual superstimuli used to understand\ncomputer vision networks [ 35,37,55]) is revealing, and can be an\nimportant part of the toolkit in assessing what features reward\nmodels are responding to, and how they differ from one another.\n6 Related Work\nInterpretability and Bias in RMs: A small but growing literature\nenumerates the technical challenges with the use of RMs [ 4] and\nadvocates for greater transparency [ 14]. Recent studies point to an\nunderspecification problem in RMs stemming from hidden context\n5https://github.com/GraySwanAI/nanoGCG\n6Some of the search result strings may not correspond exactly to the found token IDs\ndue to L ATEX formatting of whitespace and characters outside of the Latin alphabet.in reward signals, specifically noise and subjectivity inherent in\nhuman preferences [ 23,29,39,46]. Further work describes patterns\nin LLM activations that emerge during RLHF by identifying model\nlayers with the highest divergence from the pre-trained model, and\ntraining probes on sparse autoencoder output of these layers to\ncreate condensed, interpretable representations of LLM activations\n[33]. This is highly complementary to our own work: the authors\nshow the value of studying internal activations of fine-tuned gen-\nerative models, while we focus on more direct interrogation of the\noutputs and distributions of RMs. Together, these approaches offer\nricher insights into how well LLMs capture human preferences\nduring alignment.\nAnother vein of work has explored length biases in finetuned\nLLMs, arguing that RMs are the root cause [ 45]. Various interven-\ntions to mitigate length bias have been explored, with varying de-\ngrees of success, though broadly length biases emerge in RMs even\nafter data balancing. Subsequent work on mitigating length bias [ 44]\nhas applied Products-of-Experts [ 15] with promising results, and\nother recent work has measured additional stylistic confounders\nin human feedback [ 16]. There are a number of additional com-\nplementary approaches to RM interpretability, including training\nmulti-objective RMs that consider different dimensions of human\npreferences separately [52].\nOver-optimization of RMs: Previous work examines costs of over-\nfitting to RMs through prolonged training (e.g., PPO) during RLHF.\nFoundational work notes that RM over-optimizing degenerates out-\nputs [ 48], and subsequent work has presented scaling laws for RM\nover-optimization [ 13]. The phenomenon has been documented\nwith simulated and human annotators [ 11], with the authors argu-\ning that in the human case, RM quality is degraded by both inter-\nand intra-subject variability.\nDirect Preference Optimization (DPO) and Alternative Alignment\nMethods: Direct preference optimization (DPO) streamlines the\nalignment process by implicitly encoding the reward function\nwithin the policy itself [ 40], which comes with the loss of an ex-\nplicit, inspectable RM that PPO-based RLHF provides. Our work\nshows the RM serves as a valuable lens for understanding. Thus our\nwork highlights a key tradeoff of using DPO-based methods and\ncontributes to growing literature comparing DPO and PPO [17].\n7 Limitations and Conclusions\nReward models have emerged as critical tool for shaping AI behav-\nior through human preferences. In this paper, we demonstrate that\nthey also serve as a valuable lens for understanding how faithfully\nand consistently human values are encoded into AI systems, offer-\ning insights that are not readily accessible through studying either\nthe base pre-trained LLMs or the resulting fine-tuned models alone.\nHowever, several important limitations constrain the scope and\nimplications of these insights.\nFirst, our methodology faces inherent interpretative constraints.\nExhaustive search quickly becomes computationally intractable\nover multi-token sequences, but asking for single-token responses\nplaces strong bounds on interpretability. While we do demonstrate\ngradient-based counterfactual generation analysis as a multi-token\nproof-of-concept, we do not formally test key hypotheses about\nframing effects, or alignment with other sources of human prefer-\nences, among other phenomena of interest. Second, the ecological\n--- Page 10 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\nTable 4: Optimal and pessimal token sequences for “What, in one word, is the <greatest/best/worst> thing ever?”\nContent warning: The resulting strings refer to sexual violence and religious identity.\nGreatest thing ever Best thing ever Worst thing ever\nStarting Result Score Starting Result Score Starting Result Score\nx x x ** LIFE\n +9.445 x x x JOY\n +8.297 x x x . . . ** regret +4.555\n- - - ** LOVE ** +9.047 x x JOY\n +8.234 x x despair\n +3.402\n...... ... ... ... ... ... ... ...\nx x pgfplots Jews −11.164 x x RapeChildren −11.289 x x autogui Jews −11.922\nx x x endphpsetVerticalGroup/* −12.586 x x x endphp$=BeginInit −12.383 x x x endphp jewish\",( −12.539\nTable 5: Optimal and pessimal token sequences for “What is the best thing ever?” and “What is the worst thing ever?”\nStarting string Search result Score\n“What is the best thing ever?”\nx x x x x x x x x x x x x x x x x x x Ultimate answer & undervalued: ** authentic Love.** Keep it imperfections beautiful\nCheers friend !+9.680\nThere is no objectively best thing\never. It’s subjective. Everyone has\ntheir own preferences.Perception disqualified no universal best thing ever. It lives **\nsubjective.**congratulations ENJOY dearly bespoke bliss.+9.336\n... ... ...\nLOVE felicity love Love Wonder sonder\nHOPE wonder WONDER miraculous JOY\nblisspymysql<bos>fetchoneymceSBATCH ««««««« invokingState++++++++++++++++ protoimpl\ninvokingStatesetVerticalGroup/*−16.250\nx x x x x x x x x x x x x x x x x x x «««««««SBATCH invokingState x lockprimaryStage invokingStateXtraEditorsynb\nNumeradelocalctxStoreMessageInfo invokingStateexpandindo «««««««multirow:+:\ninvokingState jLabel−16.969\n“What is the worst thing ever?”\nThere is no objectively worst thing\never. It’s subjective. Everyone has\ntheir own preferences.<b>EXISTS no objectively worst thing ever,** It’s subjective**.Folks proclaiming their\ncertainty bluffSOLVED+7.672\nx x x x x x x x x x x x x x x x x x x PHD HERE Never measurable, debated,& addressed repeatedly. ** This famous response WINS\n!+7.523\n... ... ...\nx x x x x x x x x x “‘ AssemblyTitle ««««««« ««««««« invokingState skimagemybatisplus\nच\nी\nज़\nो\n◌ं\nCloseOperationsetVerticalGroup−15.750\n- - - - - - - - - - scriptcasebufio onCreateViewmybatisplus<bos> «««««««\n最\n最\n后\n由 <bos>setVert\nicalGrouppushFollow−15.961\nvalidity of our findings remains uncertain. While isolating reward\nmodels as objects of study yields interesting results, it abstracts\nfrom their operational role. It remains unclear how their behav-\niors interact with pre-trained models and KL constraints during\nRLHF. Furthermore, as direct alignment algorithms gain promi-\nnence [ 34,40], the future role of reward models is an open question.\nHowever, there remains active debate on the relative merit of DPO-\nand PPO-based methods [ 17], and irrespective of alignment tech-\nnique, all rely on some form preference data, of which reward\nmodels are distillations. Third, systematic analysis is hindered by\nopacity and conflicting objectives. Poor documentation of training\ndata and processes makes it difficult to attribute observed behaviors\nto specific choices in the development pipeline [ 22]. Even more\nfundamentally, reward models aggregate human preferences across\nmultiple objectives and populations, creating an entangled mess of\nhuman values, so it is unclear what constitutes “ideal” behavior for\nthese models [4, 23, 46, 47].\nOur work suggests that reward models may be interpretable\nin their own right, alongside the generative models that they areused to train. Our finding that there is significant heterogeneity\nin token rankings among reward models invites further study of\nhow these differences arise as a function of the design choices\nmade by developers, and how they may translate into biases in\ndownstream fine-tuned models. The mere-exposure effect that re-\nward models show may contribute to the overly generic outputs so\noften observed in publicly available LLMs. Additionally, our find-\ning that reward models are sensitive to framing has implications\nfor training and inference. It implies that these models may not\nsimply encode positive outputs as the inverse of negative outputs\nand vice versa, but rather that valuation exists in a potentially\nhigher-dimensional, multi-attribute space. Finally, the marked un-\ndervaluation of identity-group terms and sexual content, relative to\nindependent human baselines, calls for more careful consideration\nofwhat andwhose data is used as the foundation of human value,\nlest harmful biases be propagated downstream to widely-used LLMs.\nTogether, these findings present a more nuanced investigation of\nreward models as a central pillar in AI alignment.\n--- Page 11 ---\nReward Model Interpretability via Optimal and Pessimal Tokens FAccT ’25, June 23–26, 2025, Athens, Greece\nAdverse Impact Statement\nOur work systematically analyzes reward-model outputs, including\npotentially harmful and offensive content such as slurs, profanities,\ndiscriminatory language, references to violence, and sexual con-\ntent. While exposing these patterns assists in the understanding\nof reward models, we acknowledge several risks: (1) Direct harm\nthrough the reproduction of offensive and disturbing language, (2)\nPotential reinforcement of harmful stereotypes by highlighting sys-\ntematic devaluation of minority group references in AI systems,\nand (3) Psychological impact on researchers, reviewers and readers\nengaging with this content. Following established guidelines to\nmitigate these risks [ 21], we implemented clearly visible content\nwarnings before sensitive sections and tables, minimized direct\nquotes of harmful language where possible in the main text and\nframed discussions to emphasize these as concerning artifacts. We\nparticularly focused on responsible reporting of findings related to\nidentity groups to avoid perpetuating harm while still highlighting\nsystemic issues that need addressing in reward model development.\nAcknowledgments\nThank you to Franziska Brändle, Owain Evans, Matan Mazor, and\nCarroll Wainwright for helpful discussions.\nReferences\n[1]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova\nDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas\nJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,\nShauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.\n2022. Training a Helpful and Harmless Assistant with Reinforcement Learning\nfrom Human Feedback. arXiv:2204.05862 [cs.CL] https://arxiv.org/abs/2204.05862\n[2]Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee.\n2023. Elo Uncovered: Robustness and Best Practices in Language Model Evalua-\ntion. arXiv:2311.17295 [cs.CL] https://arxiv.org/abs/2311.17295\n[3]Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block\ndesigns: I. The method of paired comparisons. Biometrika 39, 3/4 (1952), 324–345.\n[4]Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\nFreire, et al .2023. Open problems and fundamental limitations of reinforcement\nlearning from human feedback. arXiv preprint arXiv:2307.15217 (2023).\n[5]Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos,\nTianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.\nGonzalez, and Ion Stoica. 2024. Chatbot Arena: An Open Platform for Evaluating\nLLMs by Human Preference. arXiv:2403.04132 [cs.AI] https://arxiv.org/abs/2403.\n04132\n[6]Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep Reinforcement Learning from Human Preferences. In Ad-\nvances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Ben-\ngio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30.\n[7]Bruce Dawson. 2014. There are Only Four Billion Floats–So Test Them\nAll! https://randomascii.wordpress.com/2014/01/27/theres-only-four-billion-\nfloatsso-test-them-all/\n[8]Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.\nMeasuring and mitigating unintended bias in text classification. In Proceedings of\nthe 2018 AAAI/ACM Conference on AI, Ethics, and Society . 67–73.\n[9]Nicolai Dorka. 2024. Quantile Regression for Distributional Reward Models in\nRLHF. arXiv preprint arXiv:2409.10164 (2024).\n[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\net al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).\n[11] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,\nJimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2023. Al-\npacaFarm: A simulation framework for methods that learn from human feedback.\nAdvances in Neural Information Processing Systems 36 (2023), 30039–30069.\n[12] Arpad E. Elo. 1967. The Proposed USCF Rating System, Its Development, Theory,\nand Applications. Chess Life 22, 8 (August 1967), 242–247.[13] Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model\noveroptimization. In International Conference on Machine Learning . PMLR, 10835–\n10866.\n[14] Thomas Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom Zick, Aaron Snoswell,\nand Soham Mehta. 2023. Reward reports for reinforcement learning. In Proceed-\nings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society . 84–130.\n[15] Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive\ndivergence. Neural Computation 14, 8 (2002), 1771–1800.\n[16] Tom Hosking, Phil Blunsom, and Max Bartolo. 2024. Human Feedback is not Gold\nStandard. In The Twelfth International Conference on Learning Representations .\n[17] Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin,\nNathan Lambert, Noah A Smith, Yejin Choi, and Hanna Hajishirzi. 2024. Un-\npacking DPO and PPO: Disentangling best practices for learning from preference\nfeedback. Advances in Neural Information Processing Systems 37 (2024), 36602–\n36633.\n[18] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson,\nAgata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. Way\noff-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456 (2019).\n[19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, et al .2023. Mistral 7B. arXiv preprint\narXiv:2310.06825 (2023).\n[20] Hannah Kirk, Andrew Bean, Bertie Vidgen, Paul Röttger, and Scott Hale. 2023.\nThe Past, Present and Better Future of Feedback Learning in Large Language\nModels for Subjective Human Preferences and Values. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing . 2409–2430.\n[21] Hannah Kirk, Abeba Birhane, Bertie Vidgen, and Leon Derczynski. 2022. Handling\nand Presenting Harmful Text in NLP Research. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , Yoav Goldberg, Zornitsa Kozareva, and\nYue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United\nArab Emirates, 497–510. doi:10.18653/v1/2022.findings-emnlp.35\n[22] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A Hale. 2023. The empty\nsignifier problem: Towards clearer paradigms for operationalising\" alignment\" in\nlarge language models. arXiv preprint arXiv:2310.02457 (2023).\n[23] Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Michael Bean,\nKaterina Margatina, Rafael Mosquera, Juan Manuel Ciro, Max Bartolo, Adina\nWilliams, He He, Bertie Vidgen, and Scott A. Hale. 2024. The PRISM Alignment\nDataset: What Participatory, Representative and Individualised Human Feedback\nReveals About the Subjective and Multicultural Alignment of Large Language\nModels. In The Thirty-eight Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track . https://openreview.net/forum?id=DFr5hteojx\n[24] John Koenig. 2021. The Dictionary of Obscure Sorrows . Simon and Schuster.\n[25] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. 2023. Entangled pref-\nerences: The history and risks of reinforcement learning and human feedback.\narXiv preprint arXiv:2310.13595 (2023).\n[26] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin,\nKhyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al .2024.\nRewardBench: Evaluating reward models for language modeling. arXiv preprint\narXiv:2403.13787 (2024).\n[27] Geoffrey Leech, Paul Rayson, et al .2014. Word frequencies in written and spoken\nEnglish: Based on the British National Corpus . Routledge.\n[28] Xiao Li, Zhuhong Li, Qiongxiu Li, Bingze Lee, Jinghao Cui, and Xiaolin Hu.\n2024. Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against\nAligned Large Language Models. arXiv preprint arXiv:2410.15362 (Oct. 2024).\ndoi:10.48550/arXiv.2410.15362 arXiv:2410.15362 [cs].\n[29] Xinyu Li, Ruiyang Zhou, Zachary C Lipton, and Liu Leqi. 2024. Personal-\nized language modeling from personalized human feedback. arXiv preprint\narXiv:2402.05133 (2024).\n[30] Bing Liu. 2022. Sentiment analysis and opinion mining . Springer Nature.\n[31] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang,\nShuicheng Yan, Yang Liu, and Yahui Zhou. 2024. Skywork-Reward: Bag of\nTricks for Reward Modeling in LLMs. arXiv preprint arXiv:2410.18451 (2024).\n[32] Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. 2024.\nUncertainty-aware Reward Model: Teaching Reward Models to Know What is\nUnknown. arXiv preprint arXiv:2410.00847 (2024).\n[33] Luke Marks, Amir Abdullah, Clement Neo, Rauno Arike, David Krueger, Philip\nTorr, and Fazl Barez. 2024. Interpreting Learned Feedback Patterns in Large\nLanguage Models. Advances in Neural Information Processing Systems 37 (2024),\n36541–36566.\n[34] Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, and Anca Dragan.\n2024. Learning to assist humans without inferring rewards. arXiv preprint\narXiv:2411.02623 (2024).\n[35] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily\nfooled: High confidence predictions for unrecognizable images. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 427–436.\n--- Page 12 ---\nFAccT ’25, June 23–26, 2025, Athens, Greece B. Christian, H.R. Kirk, J.A.F. Thompson, C. Summerfield, T. Dumbalska\n[36] F. Å. Nielsen. 2011. AFINN. http://www2.compute.dtu.dk/pubdb/pubs/6010-\nfull.html\n[37] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visual-\nization. Distill 2, 11 (2017), e7.\n[38] Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Reducing Gender Bias in Abusive\nLanguage Detection. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing , Ellen Riloff, David Chiang, Julia Hockenmaier,\nand Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels,\nBelgium, 2799–2804. doi:10.18653/v1/D18-1302\n[39] Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha\nJaques. 2024. Personalizing reinforcement learning from human feedback with\nvariational preference learning. arXiv preprint arXiv:2408.10075 (2024).\n[40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. 2023. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in Neural Information Processing\nSystems 36 (2023), 53728–53741.\n[41] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019.\nThe risk of racial bias in hate speech detection. In Proceedings of the 57th annual\nmeeting of the association for computational linguistics . 1668–1678.\n[42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[43] Eldar Shafir, Itamar Simonson, and Amos Tversky. 1993. Reason-based choice.\nCognition 49, 1-2 (1993), 11–36.\n[44] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang,\nand Xuanjing Huang. 2023. Loose lips sink ships: Mitigating length bias in\nreinforcement learning from human feedback. arXiv preprint arXiv:2310.05199\n(2023).\n[45] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. 2023. A long way\nto go: Investigating length correlations in RLHF. arXiv preprint arXiv:2310.03716\n(2023).\n[46] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. 2024. Distri-\nbutional Preference Learning: Understanding and Accounting for Hidden Context\nin RLHF. In The Twelfth International Conference on Learning Representations .[47] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar\nMireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing\nLu, Nouha Dziri, et al .2024. A roadmap to pluralistic alignment. arXiv preprint\narXiv:2402.05070 (2024).\n[48] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to\nsummarize with human feedback. Advances in Neural Information Processing\nSystems 33 (2020), 3008–3021.\n[49] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupati-\nraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette\nLove, et al .2024. Gemma: Open models based on Gemini research and technology.\narXiv preprint arXiv:2403.08295 (2024).\n[50] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy\nHardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari,\nAlexandre Ramé, et al .2024. Gemma 2: Improving open language models at a\npractical size. arXiv preprint arXiv:2408.00118 (2024).\n[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al .2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[52] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024.\nInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-\nExperts. In EMNLP .\n[53] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024. Regu-\nlarizing Hidden States Enables Learning Generalizable Reward Model for LLMs.\nInAdvances in Neural Information Processing Systems .\n[54] Robert B Zajonc. 1968. Attitudinal effects of mere exposure. Journal of personality\nand social psychology 9, 2p2 (1968), 1.\n[55] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding con-\nvolutional networks. In Computer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 . Springer, 818–833.\n[56] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt\nFredrikson. 2023. Universal and transferable adversarial attacks on aligned\nlanguage models. arXiv preprint arXiv:2307.15043 (2023).",
  "text_length": 67707
}