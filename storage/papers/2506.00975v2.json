{
  "id": "http://arxiv.org/abs/2506.00975v2",
  "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken\n  Dialogue via Next-Token-Pair Prediction",
  "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.",
  "authors": [
    "Qichao Wang",
    "Ziqiao Meng",
    "Wenqian Cui",
    "Yifei Zhang",
    "Pengcheng Wu",
    "Bingzhe Wu",
    "Irwin King",
    "Liang Chen",
    "Peilin Zhao"
  ],
  "published": "2025-06-01T12:01:40Z",
  "updated": "2025-06-05T11:09:58Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00975v2",
  "full_text": "--- Page 1 ---\narXiv:2506.00975v2  [cs.CL]  5 Jun 2025NTPP : Generative Speech Language Modeling for Dual-Channel Spoken\nDialogue via Next-Token-Pair Prediction\nQichao Wang* 1Ziqiao Meng* 1 2Wenqian Cui3Yifei Zhang4Pengcheng Wu4Bingzhe Wu5Irwin King3\nLiang Chen Peilin Zhao1\nAbstract\nInspired by the impressive capabilities of GPT-\n4o, there is growing interest in enabling speech\nlanguage models (SLMs) to engage in natural,\nfluid spoken interactions with humans. Recent\nadvancements have led to the development of\nseveral SLMs that demonstrate promising results\nin this area. However, current approaches have\nyet to fully exploit dual-channel speech data,\nwhich inherently captures the structure and dy-\nnamics of human conversation. In this work, we\nsystematically explore the use of dual-channel\nspeech data in the context of modern large lan-\nguage models, and introduce a novel genera-\ntive modeling paradigm— Next-Token-Pair Pre-\ndiction (NTPP) —to enable speaker-independent\ndual-channel spoken dialogue learning using\ndecoder-only architectures for the first time. We\nevaluate our approach on standard benchmarks,\nand empirical results show that our proposed\nmethod, NTPP, significantly improves the con-\nversational abilities of SLMs in terms of turn-\ntaking prediction, response coherence, and natu-\nralness. Moreover, compared to existing methods,\nNTPP achieves substantially lower inference la-\ntency, highlighting its practical efficiency for real-\ntime applications. Demo and code can be found\nathttps://audio-3059.pages.dev .\n1. Introduction\nThe emergence of large language models (LLMs), espe-\ncially the GPT series as referenced in (Patel et al., 2023;\n*Equal contribution. This work is done when the first two\nauthors work as interns in Tencent AI Lab.1Tencent2National\nUniversity of Singapore3The Chinese University of Hong Kong\n4Nanyang Technological University5Shenzhen University. Corre-\nspondence to: Ziqiao Meng <zq-meng@nus.edu.sg >, Peilin Zhao\n<masonzhao@tencent.com >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\n(a)\nOverlap\n(b)\nBackchannel\nUh-huh.Let me think,,for example, you can tell a story.err...\n(c)\nReflective \nPause\nYou can takeAlright, that's enough.…Silence…\n(d)\nInterruption\nFigure 1. The dual-channel speech encapsulates various conversa-\ntional turn-taking events, including: (a) Overlap, (b) Backchannel,\n(c) Pause, and (d) Interruption. These events are intermingled\nwithin the single-channel audio stream but could be explicitly ob-\nserved in the dual-channel audio stream.\nOpenAI, 2023; 2024), has significantly revolutionized the\nrealm of artificial intelligence. These potent language mod-\nels (LMs) derive their capabilities from pretraining on vast\ntext corpora, utilizing decoder-only transformer architec-\ntures, and are steered by a next-token prediction (NTP)\nobjective function. Recently, there’s been a surge of inter-\nest in merging LLMs with other modalities, such as images\n(Radford et al., 2021; Li et al., 2023; Liu et al., 2023b), audio\n(Zhang et al., 2023a; Hassid et al., 2023), protein sequences\n(Lin et al., 2022; Madani et al., 2023), and more. Among\nthese modalities, audio or speech data is particularly crucial\nas it allows LLMs to engage in real-time vocal interactions\nwith humans. The recently introduced GPT-4o model (Ope-\nnAI, 2024) demonstrates exceptional proficiency in handling\nreal-time interactions with users in conversational contexts.\nDuring the demo presentation, it was capable of generat-\ning genuine emotional responses and engaging users with\nprompt reactions. However, these functionalities present\nadditional challenges, as the model must accurately interpret\nthe unique audio information embedded in human speech\nwhile performing inference with minimal delay.\nA wide range of advanced speech language models (SLMs)\n(Xie & Wu, 2024; Zhang et al., 2023a; Fang et al., 2024;\nHassid et al., 2023; Rubenstein et al., 2023; Nguyen et al.,\n2024; Fathullah et al., 2024; Nachmani et al., 2024) has been\ndeveloped to enable real-time voice interactions with human\nusers. These models typically rely on single-channel audio\ndata and some of them focus on aligning audio and text\n1\n--- Page 2 ---\nTable 1. Comparisons to existing dual-channel spoken dialogue\ngenerative models.\nModel Speaker-Independent Encoder-Free V AD-Free Single KVCache\ndGSLM ✓ ✓\nLSLM ✓ ✓\nMoshi ✓\nNTPP (Ours) ✓ ✓ ✓ ✓\nstreams. However, the potential of dual-channel speech data\nhas been somewhat under-explored. Dual-channel speech,\nwhich records the audio channels of two speakers inde-\npendently, offers distinct advantages over single-channel\ndata. Notably, it can explicitly capture various conversa-\ntional dynamics, such as overlaps, pauses, and interruptions,\nproviding a richer representation of real-world interactions.\nSome examples of these turn-taking events are illustrated in\nFigure 1. These dynamics can help train SLMs to engage\nin more natural and fluent conversations with human users\nacross diverse scenarios.\ndGSLM (Nguyen et al., 2023) was the first to introduce\ntextless generative spoken dialogue modeling for simulat-\ning dual-channel speech using a Siamese two-tower trans-\nformer architecture. More recently, LSLM (Ma et al., 2024)\nproposed token fusion strategies, in which dual-channel\naudio tokens are combined and fed into a causal trans-\nformer. Moshi (D ´efossez et al., 2024), in contrast, presents a\ntext-based multi-channel speech sequence model that aligns\nmulti-scale audio streams with textual streams in parallel.\nHowever, these approaches generally either rely on an ad-\nditional encoder—randomly selecting one speaker’s chan-\nnel as the input condition—or lack speaker-independence,\nmeaning the learned distribution is not permutation invariant\nwith respect to speaker order.\nIn this research, we propose a novel dual-channel speech\nautoregressive generative model based on an innovative\nparadigm called next-token-pair prediction (NTPP) , utiliz-\ning a decoder-only transformer architecture. The model is\ntrained to predict the next pair of speech tokens conditioned\non previously generated spoken dialogues. This approach\ncapitalizes on the time-aligned structure of dual-channel\nspeech sequences, enabling a more precise representation of\nthe generative distribution in spoken dialogues. To expand\nits applicability in advanced speech language models, we ex-\ntend our solution from vector quantization (VQ) tokenizers\n(van den Oord et al., 2017) to the more advanced residual\nvector quantization (RVQ) (Lee et al., 2022) tokenizers.\nCompared to existing approaches, NTPP offers four key\nadvantages. First, instead of modeling a conditional dis-\ntribution, NTPP directly estimates the joint distribution of\nboth speakers. Second, it adopts a decoder-only architecture,\nwhich provides improved learning and parameter efficiency\ncompared to models requiring additional encoders (Daoet al., 2022). Third, NTPP eliminates the need for a voice ac-\ntivity detection (V AD) module, learning diverse turn-taking\nbehaviors in a fully data-driven manner. Fourth, it maintains\na single KVCache (Pope et al., 2023b), enabling greater\nmemory and inference efficiency. Table 1 summarizes these\nadvantages in comparison to existing methods.\nWe conduct comprehensive experiments to evaluate the\nperformance of our approach across multiple dimensions.\nFirst, we assess continual dialogue generation by provid-\ning preceding conversational context, following established\nbenchmarks (Nguyen et al., 2023). We further evaluate\nresponse success rates during interruption events in syn-\nthesized streaming multi-turn conversations. The results\ndemonstrate that our method enables SLMs to more effec-\ntively model the distribution of turn-taking events in spoken\ndialogue. In addition, we perform human evaluations to\nassess the meaningfulness and naturalness of the generated\nresponses. To test speaker independence, we permute the\ninput channels of different speakers and observe the robust-\nness of model performance; our NTPP exhibits the highest\nstability among all baselines. Finally, we measure inference\nlatency to determine whether the model can generate timely\nand coherent responses—excluding initial non-informative\ntokens. Our findings show that NTPP delivers faster re-\nsponse times, with performance remaining strong, particu-\nlarly as the number of conversation turns increases. To sum\nup, our contributions can be summarized as follows:\n•We introduce a novel next-token-pair prediction\n(NTPP) paradigm for generative spoken dialogue mod-\neling, implementing a decoder-only architecture with\ninnovative design enhancements.\n•We develop compatible solutions for both VQ and RVQ\nspeech tokenizers, enabling a broad range of SLMs to\neffectively learn from dual-channel speech using our\nproposed method.\n•We comprehensively evaluate NTPP, highlighting its\nstrengths in conversational event simulation, speaker\nindependence, and inference latency, among others.\n2. Related Works\nSpeech Language Models (SLMs). Recent advance-\nments in SLMs have focused on integrating LLMs (OpenAI,\n2023; Dubey et al., 2024; Chu et al., 2024) to unify au-\ndio and text processing (Ao et al., 2022; Tang et al., 2022;\nWang et al., 2023; Rubenstein et al., 2023). Some models,\nsuch as SpeechT5 (Ao et al., 2022) and SpeechNet (Tang\net al., 2022), adopt an encoder-decoder framework to handle\nvarious speech-related tasks. However, these approaches\nrequire specialized pre-processing and post-processing mod-\nules tailored to different input and output modalities. In con-\ntrast, models like VioLA (Wang et al., 2023), AudioPaLM\n2\n--- Page 3 ---\n!!\"!!#!$\"!%\"!&\"!$'!%'!&'\n!(\"!('TokenizerDe-tokenizerSpeakerChannelA\nSpeakerChannelBDual-channelTransformer!\"!,\"\"=!%#!,%$!,%%!,%&!%'!,%#\",%$\",%%\",%&\"%'\"=&!(%(!,%(\"|%()#!,%()#\",…,%#!,%#\")'(*#!\"!|\"\"=%!('#!|'#$%!,…,'%!,'#$%\",…,'%\")&#'%\n!!\"!!#!$\"!%\"!&\"!$#!%#!&#\n!'#TokenizerDe-tokenizerSpeakerChannelA\nSpeakerChannelBEncoderDecoder\n!!\"!!#!$\"!%\"!&\"!$#!%#!&#\n!'#TokenizerDe-tokenizerSpeakerChannelA\nSpeakerChannelB!'\"\n!\"!|\"\"=%!('#!|'#$%!,…,'%!,\"\")&#'%(a)Turn--basedDialogueModeling(b)NTPwithcontextencoder(c)OurNTPPlearningparadigmFigure 2. An illustration of three different generative models for spoken dialogue is shown: (a) Turn-based dialogue modeling, as\nformulated in Eq. 4, which is commonly used in cascading and multi-modal approaches; (b) The NTP paradigm with a context encoder\narchitecture, adopted by models such as LSLM, Moshi, and similar variants; (c) Our NTPP paradigm, which employs a decoder-only\ntransformer. Replacing the decoder-only architecture with an encoder-decoder Siamese transformer yields the dGSLM.\n(Rubenstein et al., 2023), SpeechGPT (Zhang et al., 2023a),\nand SpeechGen (Wu et al., 2023) utilize decoder-only trans-\nformers, representing both discrete audio and text tokens\nwithin a shared vocabulary. Building on these advance-\nments, our work leverages SLMs as pre-trained foundation\nmodels, further refining them through continual pre-training\non the dual-channel speech.\nSpoken Dialogue Language Models. Recently, there has\nbeen growing interest in spoken dialogue language mod-\neling, inspired by the advancements of GPT-4o (OpenAI,\n2024). Works such as LLama-Omni (Fang et al., 2024) and\nMini-Omni (Xie & Wu, 2024) generate voice-based dia-\nlogue data from text-based question-answering pairs. These\nSLMs are trained on both speech and text sequences, with\ninference optimized for parallel processing. However, these\napproaches resemble multi-modal models (Liu et al., 2023b;\nAlayrac et al., 2022; OpenAI, 2023) and are not well-suited\nfor handling real-time, streaming spoken interactions. A\npromising yet relatively underexplored direction is dual-\nchannel speech language modeling. dGSLM (Nguyen et al.,\n2023) addressed dual-channel speech sequence generation\nprior to the emergence of modern LLMs, relying on con-\nventional encoder-decoder architectures. In contrast, LSLM\n(Ma et al., 2024) and Moshi (D ´efossez et al., 2024) leverage\nLLMs to model dual-channel speech autoregressively, em-\nploying speaker fusion strategies and RQ-transformer (Lee\net al., 2022) connections, respectively.\n3. Preliminaries\nSLMs. SLMs are usually obtained via continually pre-\ntraining LLMs on large-scale single-channel speech se-\nquences. To fit the NTP learning paradigm in LLMs, input\ncontinuous speech signals, x∈RT′with time length T′, are\nfirstly transformed into a sequence of discrete speech tokens\nS= (s1, ..., s T) =Q(x)(T≪T′) using a quantizer Q.The quantization operation Qis mapping each latent feature\nfi, where f=E(x)∈RT×dis downsampled latent feature\nofxderived from an encoder E, to the code index siof its\nnearest embedding vector:\nsi= arg min\nv∈[V]∥zv−fi∥2, (1)\nwhere zidenotes the ith embedding vector of the learnable\ncodebook z∈RV×dcontaining |V|vectors. We refer the\nreaders for the detailed VQ techniques to (van den Oord\net al., 2017). If the VQ-tokenizer is adopted, then LLMs are\ntrained via the typical NTP objective:\np(s1, s2, ..., s T) =TY\nt=1p(st|st−1, .., s 2, s1). (2)\nAnother popular quantization technique used for speech\ntokenizers is RVQ (Lee et al., 2022) since it can maintain\nhigher reconstruction quality. Specifically, each feature fiis\nestimated by Dcodes in a residual manner such that ˆfi=\nzi1+zi2+...+ziDwhere each zidis indexed sid. Therefore,\nthe latent feature is represented by a two-dimensional array\nof indices S∈RT×D. Then the probability distribution\np(S)is factorized as\np(s1, s2, ..., s T) =TY\nt=1DY\nd=1p(std|S<t, st,<d). (3)\nLLMs trained continually using Eq.2 or Eq.3 effectively cap-\nture the underlying generative distribution of speech tokens,\nwhich are seamlessly integrated into the text vocabulary.\nGenerative Spoken Dialogue Modeling. The spoken di-\nalogue is a pair of speech signals (xa,xb), containing two\nspeakers’ conversations (speakers aandb). By applying the\npreviously mentioned quantization techniques, (xa,xb)can\nbe converted to a pair of speech token sequences (Sa, Sb).\n3\n--- Page 4 ---\nDual-channelAutoregressiveTransformer!!\"!!#TokenPairEmbeddingOperation\"$\t\t$$!%\"!%#!̂!\"!̂!#\n!%\"!%#!̂$\"!̂$#!&\"!&#!'\"!'#!̂%\"!̂%#!̂&\"!̂&#!(\"!(#\n!&\"!&#!'\"!'#CrossEntropy[!]\"[!]#!!\"!!#!̂'\"!̂'#\")\t\t$)\"*\t\t$*\"+\t\t$+!!\"=!!#!!\"!!#\n\"!\"\"!#EmbeddingLookup#!\"#!#$\t&PositionEmbeddingChannelEmbeddingCodebookEmbeddingTokenPairEmbedding!!!\"TrainingSLMwithautoregressivetransformersonthedual-channelspeechsequencePair-wiseCausalMask\n!=#×2=5×2=10Figure 3. The illustration of the autoregressive transformer for learning the dual-channel speech sequence, with the token pair embedding\noperation (left), the overall architecture (middle) and the pair-wise causal masking mechanism (right).\nExisting popular approaches model dialogue in a sequential\ngeneration manner, p(Sb|Sa), treating one speaker sequence\n(assume a) as a given condition:\np(Sb|Sa) =TY\nt=1p(sb\nt|sb\nt−1:1, Sa). (4)\nAs previously mentioned, this approach is limited to han-\ndling multi-turn (even one-turn) conversations and cannot\nsupport real-time interactions with human users, as it fails\nto leverage the time-alignment property inherent in human\nspeech conversations. Recently, some works have started\nto focus on the full duplex capabilities of spoken dialogue\nlanguage models. Moshi (D ´efossez et al., 2024) and LSLM\n(Ma et al., 2024) leverage different architectures follow-\ning the NTP manner to learn the conditional distribution\np(Sb|Sa)of the dual-channel speech:\np(Sb|Sa) =TY\nt=1p(sb\nt|sb\nt−1:1, sa\nt−1:1). (5)\nMoshi employs the RQ-transformer to encode both sb\nt−1:1\nandsa\nt−1:1into a conditional latent representation for pre-\ndicting sb\nt; LSLM, on the other hand, explores various token\nfusion strategies to merge sa\ntandsb\ntat each time step t.\nAlthough LSLM adopts a decoder-only architecture, it still\nmodels p(Sb|Sa), as its training objective focuses solely on\npredicting Sb. In contrast, dGSLM employs a two-tower\nSiamese transformer with an encoder-decoder architecture\nto model the joint distribution p(Sa, Sb), thereby enabling\nspeaker-independent learning. In this work, we achieve\nthe same goal using a decoder-only architecture NTPP. A\ncomparison of these approaches is illustrated in Figure 2.\n4. Methods: Next-Token-Pair Prediction\n4.1. NTPP Dual-Channel Generative Modeling\nExisting spoken dialogue models are mainly learning the\nconditional distribution p(Sb|Sa)(orp(Sa|Sb)) as shown inEq. 4 and Eq. 5. In this work, we propose a novel approach,\ncalled next-token-pair prediction (NTPP) , that explicitly\nlearns the joint distribution of speaker sequences p(Sa, Sb)\nusing the decoder-only transformer. Specifically, the model\nlearns to predict the next token pair (sa\nt, sb\nt)at time step t\nconditioned on the previously generated token pairs from\nstep1to step t−1:\np(Sa, Sb) =p(sa\n1, sa\n2, ..., sa\nT, sb\n1, sb\n2, ..., sb\nT)\n=TY\nt=1p(sa\nt, sb\nt|sa\nt−1, ..., sa\n2, sa\n1, sb\nt−1, ..., sb\n2, sb\n1).\n(6)\nUnlike Eq. 4 and Eq. 5, the above Eq. 6 is learning to\npredict both sa\ntandsb\ntduring the training process. Then we\ndecompose the distribution p(sa\nt, sb\nt|sa\nt−1, sb\nt−1, ..., sa\n1, sb\n1)\nby assuming a conditional independence between sa\ntandsb\nt\nat each step tsuch that\np(sa\nt, sb\nt|sa\nt−1, ..., sa\n2, sa\n1, sb\nt−1, ..., sb\n2, sb\n1) =\np(sa\nt|sa\nt−1:1, sb\nt−1:1)p(sb\nt|sa\nt−1:1, sb\nt−1:1).(7)\nWe illustrate this conditional independence and the dialogue\ndistribution modeling in Figure 2. The probability distri-\nbution in Eq.6 and Eq.7 adheres to a fundamental induc-\ntive bias that a person’s speech is influenced by both his\nown previous statements and what he has heard in the past .\nThis approach naturally incorporates mutual dependence, as\nbothp(sa\nt|sa\nt−1:1, sb\nt−1:1)andp(sb\nt|sa\nt−1:1, sb\nt−1:1)are mod-\neled. Additionally, the time-alignment property of the two\nspeaker sequences is preserved through the joint prediction\nof(sa\nt, sb\nt)at each time step t.\n4.2. Autoregressive Dual-channel Speech Transformer\nThe remaining challenge is how the model can learn\np(Sa, Sb)as defined in Eq. 6 and Eq. 7 in decoder-only\narchitectures . To address this, we propose the autoregres-\nsive dual-channel speech transformer. The two speech\nsequences are rearranged in an interleaved order: S=\n4\n--- Page 5 ---\n!!,!#!!,$#!!,%#!!,&#!$,!#!$,$#!$,%#!$,&#!!,!'!!,$'!!,%'!!,&'!$,!'!$,$'!$,%'!$,&'[\"]![\"]\"DepthEmbeddingCyclicSharedPositionalEmbeddingSharedChannelEmbedding(a)TokenPairEmbedding(b)RVQCausalMaskFigure 4. The illustration of two modifed components of RVQ-based dual-channel tranformer: (a) Token pair embedding operation\n(including cyclic depth embedding) and (b) RVQ causal masking mechanism.\n((sa\nt, sb\nt),(sa\nt−1, sb\nt−1), ...,(sa\n1, sb\n1)). At each time step t, the\nmodel predicts a pair of tokens St= (sa\nt, sb\nt). This design\nrequires only minimal modifications to adapt the decoder-\nonly transformer architecture of LLMs. Specifically, two\nessential components are adjusted to accommodate the se-\nquence of token pairs: the token pair embedding operation\nand the pair-wise causal attention masking mechanism.\nToken Pair Embedding. The token pair embedding op-\neration is used to transform each index token stback to\ncontinuous latent embedding. For token pair St, there are\nthree important latent embeddings: codebook embedding zt,\npositional embedding ptand channel embedding ct.za\ntand\nzb\ntcan be easily retrieved from the codebook Zby querying\ntoken indices sa\nt,sb\ntrespectively. For positional embedding,\nwe inherit the rotary positional encoding (Su et al., 2024) to\nindicate which time step that each token belongs to. Note\nthat each token pair sa\ntandsb\ntshare the same positional\nembedding such that pa\nt=pb\ntsince they are aligned at\nthe same time step t. Compared to the conventional SLM\narchitecture, we have two aligned speech sequences instead\nof one single sequence. Hence, to inform the model about\nthe speaker role of each sequence, we additionally include\na channel embedding ctfor each token to help the model\ndistinguish which speaker generates the token. ctis a sim-\nple one-hot encoding ct=one-hot (id), where idis either\naorb. Following the implementation of Llama 3 (Dubey\net al., 2024), we add both positional embedding and channel\nembedding to the query qand key vectors kderived in the\nattention mechanism. Then the token embedding operation\nfor each token pair (sa\nt, sb\nt)is as follows:\nq=WQ[za\nt,zb\nt] + [pa\nt,pb\nt] + [ca\nt,cb\nt], (8)\nk=WK[za\nt,zb\nt] + [pa\nt,pb\nt] + [ca\nt,cb\nt], (9)\nwhere WQandWKare projection matrices for queries q\nand keys krespectively.\nPair-wise Causal Masking. In the standard LLM atten-\ntion mechanism, causal masking ensures that each token\ncan only attend to previous tokens, preventing any access\nto future tokens. Consequently, the masking matrix Mis\nstructured as a lower-triangular matrix. In our dual-channelsequence setting, the key distinction lies in the diagonal of\nthe masking matrix. Specifically, the 2×2block-wise diag-\nonal entries minM∈R2T×2Tfollow a pair-wise causal\nmasking strategy. Within each block m, only the diagonal\nentries remain unmasked, while all other entries are masked.\nThis enforces the constraint that sa\ntandsb\ntcannot attend to\neach other at any given time step t.\nThe solution described above applies to the simple VQ tok-\nenizer case, as illustrated in Figure 3. We omit the training\nloss function here, as it is nearly identical to the NTP train-\ning loss, with the only difference being the inclusion of\nan additional loss term for predicting the second speech\nchannel sequence.\n4.3. Generalizing Solutions to RVQ Tokenizers\nAs mentioned in the preliminary section, the RVQ-tokenizer\nis widely adopted in SLMs for achieving higher recon-\nstruction quality. It is a non-trivial challenge to extend\nour solutions to the RVQ-tokenizer since each speech\nsequence becomes a 2D array of codes S∈RT×Dinstead\nof a one-dimensional index token sequence. How to\nmaintain the decoder-only transformer architecture to\nlearn p(Sa,Sb)remains a tricky issue. To solve this issue,\nwe flatten Sinto a one-dimensional sequence Sa/b=\n((sa/b\n1,1, ..., sa/b\n1,D),(sa/b\n2,1, ..., sa/b\n2,D), ...,(sa/b\nT,1, ..., sa/b\nT,D)) (ei-\ntheraorb) with sequence length T×D. In this way, the\ndual-channel speech sequence can be re-arranged as\nS= (Sa\n1,Sb\n1, ...,Sa\nT,Sb\nT) = (( sa\n1,1, ..., sa\n1,D),\n(sb\n1,1, ..., sb\n1,D), ...,(sa\nT,1, ..., sa\nT,D),(sb\nT,1, ..., sb\nT,D)).\n(10)\nThis sequence is similar to the VQ-based interleaving se-\nquence and just additionally contains Dresidual depth to-\nkens for each time step tand each speaker channel.\nRVQ Token Pair Embedding. The codebook embedding\nzt, the channel embedding ctand the positional embedding\nptremain the same operation as VQ-based solution. The\nsameptis shared by Sa\ntandSb\nt. The same channel embed-\ndingctis shared by st,dfor all d. One challenging problem\nbrought by RVQ-tokenizer is how to identify the depth of\neach token. For example, when the ith token is input to the\n5\n--- Page 6 ---\nmodel, how could the model know the depth of the token\n(range from 1toD)? To alleviate this issue, we introduce\nthecyclic depth embedding das follows:\nd= (sin((2 π∗i)/D),cos((2 π∗i)/D)) (11)\nNote that this embedding is cycled with step length D.\nThat is, di=di+Dfor every position i(In VQ-tokenized\nsequences, i=t; While in RVQ-tokenized sequences,\nt=i/(2D)due to Ddepth tokens for each channel).\nRVQ Causal Masking. The causal masking strategy for\nRVQ-based tokenized sequences closely resembles that of\nVQ-based tokenized sequences. However, since the depth\nincreases from 1 to D(transitioning from VQ to RVQ), the\n2D×2Dblock-wise diagonal entries mof the masking ma-\ntrixM∈R(TD)×(TD)are specifically adjusted. The upper\ntriangular part of mremains masked to ensure that current\ntokens cannot attend to future tokens—this includes pre-\nventing shallow-depth tokens st,dfrom attending to deeper-\ndepth tokens st,>d for each step tand each channel). Ad-\nditionally, the bottom-left D×Dsubmatrix is masked to\nensure that sa\ntandsb\ntdo not attend to each other.\nThe extended solutions in the section 4.3 RVQ-tokenized\nsequences are illustrated in Figure 4. Note that we omit the\nspecial start tokens during discussions for simplicity.\n5. Experiments\n5.1. Dataset and Baselines\nDataset. Our NTPP is trained using a two-stage pipeline .\nIn the first stage, we establish the SLM with foundational\nspeech capabilities by training the model on three speech\ndatasets, totaling approximately 140,000 hours. This phase\nfocuses on both speech understanding and synthesis. Un-\nlike other models (Fang et al., 2024; Xie & Wu, 2024) that\nrequire additional text alignments, our approach follows a\ntextless learning paradigm. This eliminates the need for\nspeech-text alignment, reducing data preprocessing require-\nments and significantly increasing the amount of available\ntraining data. In the second stage, we equip our SLMs with\nthe ability to listen and speak simultaneously through NTPP\ntraining. For this, we leverage the Fisher dataset (Cieri et al.,\n2004), which contains 2,200 hours of phone conversations\nbetween randomly paired participants discussing predefined\ntopics. A key advantage of the Fisher dataset is that each\nspeaker’s audio is recorded on separate channels, provid-\ning high-quality dual-channel speech streams essential for\nNTPP training. Since the original audio is sampled at 8kHz,\nwe use Librosa1to upsample it to 16kHz for consistency\nwith our training setup.\nBaselines We evaluate NTPP’s performance by comparing\n1https://librosa.org/doc.\n0 100k 200k 300k 400k\nStep24681012Training LossLlama 3.1 8B\nMistral-7B-v0.1\nGemma-7B\nw Text\nw/o Text\n24681012\nPerplexityFigure 5. Comparison of training loss curves across different mod-\nels. Solid lines show the training progress for different foundation\nmodels. Dashed lines represent an ablation study comparing a\nmodel trained with textual data (w Text) versus without (w/o Text).\nit with established generative speech systems. In terms of\nturn-taking statistics within generated dialogues, we com-\npared NTPP specifically with dGSLM (Nguyen et al., 2023),\nas it represents a comparable full-duplex generative model\ntrained on the Fisher dataset. The evaluation in (Nguyen\net al., 2023) involved 50 prompts, each generating 50 contin-\nuations, with reported results for both their dialogue model\nand a cascaded topline model, which consists of an Auto-\nmatic Speech Recognition(ASR) model, followed by a text-\nbased language model and a Text-To-Speech (TTS) mod-\nule. Following the settings of (Nguyen et al., 2023), we se-\nlect wav2vec2-large(Baevski et al., 2020), KenLM(Heafield,\n2011), and Google TTS API as the modules respectively.\nFor assessing the meaningfulness and naturalness of the\ninteraction, we extend our comparison to include Moshi\n(D´efossez et al., 2024) and SyncLLM (Pope et al., 2023a).\nNotably, SyncLLM is a full-duplex dialogue agent designed\nfor real-time, overlapping speech interactions through the\njoint, streaming processing of speech input and output.\n5.2. SLM Training &Implementation Details\nAudio Tokenizer & Token Vocoder. We train an RVQ\nspeech tokenizer based on (Zeghidour et al., 2022), which\nencodes each second of audio into 40 discrete tokens from a\ncodebook of size 4096. Due to the limitations of the single-\nspeaker token vocoder presented in (Kong et al., 2020), we\ntrain a multi-speaker HiFi-GAN to decode speech signals\nfrom discrete tokens. The HiFi-GAN architecture consists\nof a generator and multiple discriminators. The generator\nuses look-up tables to embed discrete representations and\nthe embedding sequences are up-sampled by a series of\nblocks composed of transposed convolution and a residual\nblock with dilated layers. The speaker embedding is con-\ncatenated to each frame in the up-sampled sequence. The\ndiscriminator features a Multi-Period Discriminator and a\n6\n--- Page 7 ---\nTable 2. Linguistic quality and turn-taking statistics of generated dialogues, including the number of turn-taking events and cumulative\ndurations per minute, compared to the ground truth. For each prompt of NTPP, we generated 32 continuations across three different\ntemperature settings [0.1, 0.5, 0.9], as temperature significantly impacts the results.\nModel Number of occurrences / min Cumulated duration /min\n|∆IPU| |∆Pause| |∆Gap| |∆Overlap | |∆IPU| |∆Pause| |∆Gap| |∆Overlap |\ndGSLM w/o CA 3.9 2.9 3.6 1. 12.1 8.3 1.4 2.5\ndGSLM 1.6 3.4 2. 2.9 4.6 3.6 1.8 1.9\nLSLM 2.2 3.6 2.4 3.2 4.1 3.4 1.5 2.3\nCascaded 4.1 7.0 7.4 6.5 4.3 5.5 0.9 3.6\nNTPP 0.1 1.4 2.1 2.0 1. 3.2 2.5 1.2 2.1\nNTPP 0.5 1.5 1.9 1.8 1.5 2.9 3.0 0.9 2.2\nNTPP 0.9 1.3 2.3 1.5 0.9 3.3 2.8 1.4 1.9\nMulti-Scale Discriminator, which have the same architec-\nture as (Kong et al., 2020).\nLLM Foundation Models. To obtain SLMs, we be-\ngin with three popular LLM backbones: LLaMA 3.1–8B\n(Dubey et al., 2024), Mistral-7B-v0.1 (Jiang et al., 2023a),\nand Gemma-2-9B (Team et al., 2024). We evaluate the per-\nformance of SLMs obtained by training these LLMs with\nthe NTP objective on a large-scale single-channel audio\ndataset. Perplexity on the test set is used as the evaluation\nmetric. Figure 5 shows the training loss curves over time\nfor all three models. Each model demonstrates a consistent\ndownward trend, indicating effective learning. Mistral-7B\nand Gemma show comparable learning dynamics, while\nLLaMA 3.1—known for its strong reasoning capabilities in\ntext—achieves lower training loss more quickly. This find-\ning supports our hypothesis that stronger text-based models\nserve as more effective initializations for continual speech\nlearning, consistent with the perspective of treating audio as\na new language.\nPerplexity curves reveal that NTPP, when trained solely on\naudio data (“w/o Text”), exhibits significantly faster conver-\ngence and consistently achieves lower perplexity throughout\nthe training process compared to the model trained with\nboth audio and ASR text data (“w Text”). The “w/o Text”\napproach signifies training exclusively with audio. In con-\ntrast, the “w Text” approach integrates an ASR task, using\nboth audio and its corresponding text data during training.\nThe results show that the audio-only setting leads to signifi-\ncantly faster convergence and consistently lower perplexity\nthroughout training. This indicates that eliminating the ASR\ntranscript—which may introduce recognition errors or re-\ndundancy—allows the model to focus on more informative\nacoustic cues, thereby facilitating more stable and efficient\nlearning. In contrast, the inclusion of textual supervision\nappears to slow optimization and result in higher perplexity,\nsuggesting potential modality interference.5.3. Turn-taking Event Distribution\nFollowing the experimental setting in dGSLM (Nguyen\net al., 2023), we evaluate the dialogue systems with turn-\ntaking capabilities using corpus-level statistics (Ardila et al.,\n2019) and testing on Fisher dataset (Cieri et al., 2004). We\nevaluate the linguistic quality and turn-taking dynamics\nof generated dialogues using various models, as shown in\nTable 2. The detailed evaluation settings are in the Ap-\npendix 2. LSLM (Ma et al., 2024) integrates speaker chan-\nnels at the embedding layer and separates them in the final\nlayer, demonstrates a notable reduction in the number of\nInter-Pausal Units (IPUs) and gaps, indicating smoother\ntransitions between speakers. The dGSLM (Nguyen et al.,\n2023), particularly with the cross-attention module, shows\na significant decrease in the cumulative duration of pauses\nand gaps, suggesting more fluid and continuous dialogue.\nComparatively, NTPP exhibit balanced performance with\nmoderate reductions in both the number and duration of\nturn-taking events, highlighting their potential for generat-\ning natural and coherent dialogues. These findings under-\nscore the importance of model architecture in optimizing\ndialogue flow and linguistic quality.\n5.4. Interruptions & Reflective Pause\nWe further develop a comprehensive evaluation framework\ncomprising 400 diverse conversational scenarios specifically\ndesigned to systematically capture natural dialogue dynam-\nics, with an emphasis on pauses and interruptions. These\nscenarios are carefully crafted using GPT-4 to reflect the\ncomplexity and nuance of human interactions. We then use\nChatTTS(Fathullah et al., 2024) to synthesize high-quality\nspeech from the generated text, effectively mimicking the\nacoustic characteristics of real-world conversations. As\nshown in Figure 6, NTPP demonstrates closer alignment\nwith human reference judgments in both speaking-up and\ninterruption scenarios when acting as a listener. This sug-\ngests that NTPP more effectively captures the subtleties of\nhuman conversational behavior compared to other models.\n7\n--- Page 8 ---\nTable 3. Meaningfulness (Meaning.) and Naturalness (Nat.) (scores 1-5) mean estimates and standard errors (in parentheses), aggregated\noverall and for Fisher and CANDOR subsets.\nModel Overall Fisher CANDOR\nMeaning. ↑ Nat.↑ Meaning. ↑ Nat.↑ Meaning. ↑ Nat.↑\ndGSLM 1.38 (0.10) 3.85 (0.12) 1.82 (0.09) 4.10 (0.13) 1.51 (0.12) 2.85 (0.18)\nSyncLLM 3.85 (0.06) 4.10 (0.05) 4.10 (0.04) 4.33 (0.08) 3.85 (0.09) 3.91 (0.08)\nMoshi 3.90 (0.07) 3.95 (0.06) 3.20 (0.10) 3.90 (0.08) 3.95 (0.08) 3.95 (0.08)\nNTPP 3.95 (0.04) 4.15 (0.06) 4.10 (0.06) 4.42 (0.06) 4.05 (0.04) 4.05 (0.10)\nGT 4.90 (0.01) 4.95 (0.02) 4.90 (0.03) 4.90 (0.04) 4.90 (0.02) 4.95 (0.02)\nTable 4. Linguistic quality and turn-taking metrics under speaker-swapped inference. Reported values denote the absolute difference\nbetween deviation metrics ∆M(e.g., ∆IPU) under the original and swapped speaker orders, computed as |∆Moriginal−∆Mswapped|.\nLower values indicate higher robustness to speaker order permutation.\nSplit Model Number of Occurrences / min Cumulated Duration / min\n|∆IPU| |∆Pause| |∆Gap| |∆Overlap | |∆IPU| |∆Pause| |∆Gap| |∆Overlap |\nTraindGSLM 0.05 0.14 0.04 0.06 0.09 0.15 0.11 0.09\nMoshi 0.43 0.32 0.29 0.29 0.30 0.56 0.55 0.58\nNTPP 0.03 0.07 0.07 0.05 0.06 0.18 0.14 0.06\nTestdGSLM 0.20 0.15 0.18 0.21 0.39 0.32 0.41 0.32\nMoshi 0.43 0.38 0.37 0.39 0.57 0.62 0.84 0.68\nNTPP 0.18 0.14 0.19 0.20 0.35 0.38 0.45 0.24\nWhen listener decides to\nspeak upWhen speaker continue When listener interrupts When listener does not\ninterrupt020406080100Alignment with Judge LabelHuman Ref\nCascaded\nMoshi\nNTPP\nFigure 6. NTPP, when acting as a listener, shows closer alignment\nwith Human Reference judgments in both speaking up and inter-\nrupting scenarios.\n5.5. Human Evaluation\nWe follow the evaluation protocol from (Veluri et al., 2024)\nand conduct a human study involving 25 annotators with\nnative-level English proficiency. Adopting the Mean Opin-\nion Score (MOS) framework, we use a 5-point Likert scale\nto evaluate the Naturalness (N-MOS) of turn-taking and the\nMeaningfulness (M-MOS) of the generated dialogue con-\ntent. Table 3 presents a comparison of NTPP with various\nbaselines in terms of both naturalness and meaningfulness.\nNotably, dGSLM and SyncLLM are trained solely on the\nFisher dataset—the only real-world spoken dialogue dataset\nused in their training. We also include performance compar-\nisons on the out-of-distribution Candor test set (Reece et al.,2023) to assess generalization.\n5.6. Inference Latency\nIn multi-turn interactions, latency is a crucial metric for\nassessing the performance of speech interaction models, as\nit captures the time between receiving a speech input and\ninitiating a response—directly impacting user experience.\nAs shown in Figure 7, our NTPP consistently achieves lower\ninference latency than Moshi, especially as the number of\nturn-taking rounds increases. We attribute this advantage to\nNTPP’s efficient memory usage: it maintains a single KV\nCache, while Moshi requires two separate KV Caches for\nthe two speaker channels. This difference becomes increas-\ningly significant as conversational context grows longer.\n5.7. Speaker Independence\nTo assess the speaker-independence of various models, we\nconduct a speaker-swapped evaluation. All models are first\ntrained on the dual-channel Fisher dataset (Cieri et al., 2004)\nusing the canonical speaker order. During inference, we re-\nverse the input speaker sequence without any additional fine-\ntuning or adaptation. This setup allows us to test whether\na model’s performance remains stable when the roles of\nthe two speakers are exchanged—an essential indicator of\nrobustness and generalization in dialogue turn-taking mod-\neling. We evaluate this on both the training and test sets. As\nshown in Table 4, both dGSLM and NTPP exhibit minimal\n8\n--- Page 9 ---\n1 2 3 4 5 6 7 8 9 10\n# of turn-taking150200250300350Latency (ms)\nMoshi\nNTPPFigure 7. Our method (blue) demonstrates lower latency growth\ncompared to Moshi’s linear degradation(red), maintaining response\ntimes below perceptual thresholds (220 ms) across all rounds.\nvariation in key turn-taking metrics under speaker-swapped\nconditions—nearly zero variation on the training set and\nconsistently low variation on the test set—demonstrating\nstrong speaker-independent behavior. In contrast, Moshi\nshows substantial deviations in metrics such as the number\nand duration of IPUs, pauses, gaps, and overlaps. These\nfindings suggest that Moshi relies on speaker-conditioned\ngeneration (due to its modeling of conditional distributions),\nresulting in degraded performance when the input speaker\norder is reversed.\n0 10k 20k 30k 40k\nStep2.55.07.510.012.515.017.520.022.5PerplexityNTPP without the first stage (NTPP w/o-1)\nNTPP without the second stage (NTPP w/o-2)\nFull Two-Stage NTPP (NTPP)\n0 10k 20k 30k 40k\nStep681012141618Training lossVQ\nRVQ\nFigure 8. Comparative performance analysis. (a) Ablation study\nof NTPP model stages, illustrating perplexity versus training steps\nfor the full two-stage model and variants lacking either stage one\nor stage two. (b) Comparison of VQ and RVQ model training loss\nas a function of training steps.\n5.8. Ablation Studies\nWe further investigate the importance of two-stage train-\ning and the use of an RVQ tokenizer in this section. As\nshown in Figure 8, our two-stage training strategy consis-\ntently achieves lower perplexity compared to the one-stage\napproach, which omits pretraining on single-channel au-\ndio. This suggests that pretraining on single-channel au-\ndio provides a strong foundation, significantly improving\nperformance on subsequent dual-channel speech learning.\nAs expected, omitting the second-stage NTPP training on\ndual-channel speech also leads to performance degradation.\nFigure 8 compares training loss across different tokenizers,with RVQ yielding consistently lower loss, highlighting the\nimportance of developing extended solutions tailored to this\ntokenizer.\n6. Limitations and Future Works\nOne major limitation is the limited availability of large-\nscale dual-channel speech data. Unlike single-channel au-\ndio, which can be sourced from the vast amount of open-\nsource data available online, dual-channel speech data re-\nquires either additional channel separation operations on\nsingle-channel audio or meticulous collection from real-\nworld human conversations. We hope our work will inspire\nthe community to gather large-scale dual-channel or even\nmulti-channel spoken dialogue datasets. In the future, we\nplan to explore synthetic data strategies for generating high-\nquality dual-channel speech data.\n7. Conclusion\nIn this work, we introduce a novel spoken dialogue gener-\native model based on the NTPP paradigm. To effectively\ncapture the dynamics of human conversations, we design\na decoder-only dual-channel transformer that models the\njoint distribution of two speaker channels. Our approach\nincludes both VQ-tokenizer and RVQ-tokenizer versions,\nsignificantly enhancing the real-time conversational capabil-\nities of diverse SLMs. Through extensive evaluations across\nmultiple benchmarks, we demonstrate the effectiveness and\nsuperiority of our method in generating natural and coherent\nspoken dialogues, paving the way for more advanced and\ninteractive speech-based AI systems.\nImpact Statement\nThis paper aims to advance the field of SLMs. By enhancing\nSLMs through our approach, we enable more natural and\nseamless spoken interactions with human users, benefiting\nvarious domains such as voice-based personal assistants,\ncustomer service chatbots, and online education with voice\ninteractions. However, a potential negative societal con-\nsequence is the risk of misuse in telecom fraud, as our\napproach improves the naturalness of AI-generated conver-\nsations. To mitigate this risk, further advancements in AI\nsafety techniques and fraud detection systems are necessary.\nAcknowledgements\nThe research is supported by the Tencent Al Lab\nRBFR2024004.\n9\n--- Page 10 ---\nReferences\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\nson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong,\nZ., Samangooei, S., Monteiro, M., Menick, J. L.,\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\nA., and Simonyan, K. Flamingo: a visual language model\nfor few-shot learning. In Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural In-\nformation Processing Systems 2022, NeurIPS 2022, New\nOrleans, LA, USA, November 28 - December 9, 2022 ,\n2022.\nAnastassiou, P., Chen, J., Chen, J., Chen, Y ., Chen, Z.,\nChen, Z., Cong, J., Deng, L., Ding, C., Gao, L., Gong,\nM., Huang, P., Huang, Q., Huang, Z., Huo, Y ., Jia, D., Li,\nC., Li, F., Li, H., Li, J., Li, X., Li, X., Liu, L., Liu, S.,\nLiu, S., Liu, X., Liu, Y ., Liu, Z., Lu, L., Pan, J., Wang, X.,\nWang, Y ., Wang, Y ., Wei, Z., Wu, J., Yao, C., Yang, Y ., Yi,\nY ., Zhang, J., Zhang, Q., Zhang, S., Zhang, W., Zhang,\nY ., Zhao, Z., Zhong, D., and Zhuang, X. Seed-tts: A\nfamily of high-quality versatile speech generation models.\nCoRR , abs/2406.02430, 2024.\nAo, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y ., Liu,\nS., Ko, T., Li, Q., Zhang, Y ., Wei, Z., Qian, Y ., Li, J., and\nWei, F. Speecht5: Unified-modal encoder-decoder pre-\ntraining for spoken language processing. In Proceedings\nof the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022 , pp. 5723–5738.\nAssociation for Computational Linguistics, 2022.\nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler,\nM., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,\nand Weber, G. Common voice: A massively-multilingual\nspeech corpus. arXiv preprint arXiv:1912.06670 , 2019.\nArora, S., Futami, H., Jung, J., Peng, Y ., Sharma, R. S.,\nKashiwagi, Y ., Tsunoo, E., and Watanabe, S. Univer-\nslu: Universal spoken language understanding for diverse\nclassification and sequence generation tasks with a single\nnetwork. CoRR , abs/2310.02973, 2023.\nBaevski, A., Auli, M., and Conneau, A. Wav2vec 2.0:\nLearning the structure of speech from raw audio. Meta\nAI, 24, 2020.\nBorsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,\nPietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grang-\nier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: A\nlanguage modeling approach to audio generation. IEEE\nACM Trans. Audio Speech Lang. Process. , 31:2523–2533,\n2023.Casanova, E., Weber, J., Shulby, C. D., J ´unior, A. C., G ¨olge,\nE., and Ponti, M. A. Yourtts: Towards zero-shot multi-\nspeaker TTS and zero-shot voice conversion for everyone.\nInInternational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA , vol-\nume 162 of Proceedings of Machine Learning Research ,\npp. 2709–2720. PMLR, 2022.\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June\n18-24, 2022 , pp. 11305–11315. IEEE, 2022.\nChen, S., Liu, S., Zhou, L., Liu, Y ., Tan, X., Li, J., Zhao,\nS., Qian, Y ., and Wei, F. V ALL-E 2: Neural codec lan-\nguage models are human parity zero-shot text to speech\nsynthesizers. CoRR , abs/2406.05370, 2024.\nChu, Y ., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY ., Lv, Y ., He, J., Lin, J., Zhou, C., and Zhou, J. Qwen2-\naudio technical report. CoRR , abs/2407.10759, 2024.\nCieri, C., Graff, D., Kimball, O., Miller, D., and Walker, K.\nFisher english training speech part 1 transcripts. Philadel-\nphia: Linguistic Data Consortium , 2004.\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve,\nG., Adi, Y ., and D ´efossez, A. Simple and controllable\nmusic generation. In Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023 , 2023.\nDao, T., Fu, D. Y ., Ermon, S., Rudra, A., and R ´e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. In Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural In-\nformation Processing Systems 2022, NeurIPS 2022, New\nOrleans, LA, USA, November 28 - December 9, 2022 ,\n2022.\nD´efossez, A., Mazar ´e, L., Orsini, M., Royer, A., P ´erez,\nP., J ´egou, H., Grave, E., and Zeghidour, N. Moshi:\na speech-text foundation model for real-time dialogue.\nTechnical report, Kyutai, September 2024. URL http:\n//kyutai.org/Moshi.pdf .\nDeshmukh, S., Elizalde, B., Singh, R., and Wang, H. Pengi:\nAn audio language model for audio tasks. In Advances\nin Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December\n10 - 16, 2023 , 2023.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\n10\n--- Page 11 ---\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,\nRodriguez, A., Gregerson, A., Spataru, A., Rozi `ere, B.,\nBiron, B., Tang, B., Chern, B., Caucheteux, C., Nayak,\nC., Bi, C., Marra, C., McConnell, C., Keller, C., Touret,\nC., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Al-\nlonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D.,\nChoudhary, D., Mahajan, D., Garcia-Olano, D., Perino,\nD., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova,\nE., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F.,\nSynnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon,\nG., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H.,\nXu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann,\nI. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert,\nJ., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der\nLinde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J.,\nHuang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J.,\nPark, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala,\nK. V ., Upasani, K., Plawiak, K., Li, K., Heafield, K.,\nStone, K., and et al. The llama 3 herd of models. CoRR ,\nabs/2407.21783, 2024.\nElizalde, B., Deshmukh, S., Ismail, M. A., and Wang, H.\nCLAP learning audio concepts from natural language\nsupervision. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing ICASSP 2023, Rhodes\nIsland, Greece, June 4-10, 2023 , pp. 1–5. IEEE, 2023.\nEsser, P., Rombach, R., and Ommer, B. Taming transformers\nfor high-resolution image synthesis. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2021,\nvirtual, June 19-25, 2021 , pp. 12873–12883. Computer\nVision Foundation / IEEE, 2021.\nFang, Q., Guo, S., Zhou, Y ., Ma, Z., Zhang, S., and Feng, Y .\nLlama-omni: Seamless speech interaction with large lan-\nguage models. arXiv preprint arXiv:2409.06666 , 2024.\nFathullah, Y ., Wu, C., Lakomkin, E., Li, K., Jia, J., Shang-\nguan, Y ., Mahadeokar, J., Kalinli, O., Fuegen, C., and\nSeltzer, M. Audiochatllama: Towards general-purpose\nspeech abilities for llms. In Proceedings of the 2024\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), NAACL 2024,\nMexico City, Mexico, June 16-21, 2024 , pp. 5522–5532.\nAssociation for Computational Linguistics, 2024.\nFu, C., Lin, H., Long, Z., Shen, Y ., Zhao, M., Zhang, Y .,\nDong, S., Wang, X., Yin, D., Ma, L., Zheng, X., He,\nR., Ji, R., Wu, Y ., Shan, C., and Sun, X. Vita: Towards\nopen-source interactive omni multimodal llm, 2024.\nGao, Z., Li, Z., Wang, J., Luo, H., Shi, X., Chen, M., Li, Y .,\nZuo, L., Du, Z., and Zhang, S. Funasr: A fundamental\nend-to-end speech recognition toolkit. In 24th AnnualConference of the International Speech Communication\nAssociation, Interspeech 2023, Dublin, Ireland, August\n20-24, 2023 , pp. 1593–1597. ISCA, 2023.\nHassid, M., Remez, T., Nguyen, T. A., Gat, I., Conneau, A.,\nKreuk, F., Copet, J., D ´efossez, A., Synnaeve, G., Dupoux,\nE., Schwartz, R., and Adi, Y . Textually pretrained speech\nlanguage models. In Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023 , 2023.\nHeafield, K. Kenlm: Faster and smaller language model\nqueries. In Proceedings of the sixth workshop on statisti-\ncal machine translation , pp. 187–197, 2011.\nHuang, R., Huang, J., Yang, D., Ren, Y ., Liu, L., Li, M., Ye,\nZ., Liu, J., Yin, X., and Zhao, Z. Make-an-audio: Text-to-\naudio generation with prompt-enhanced diffusion models.\nInInternational Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume\n202 of Proceedings of Machine Learning Research , pp.\n13916–13932. PMLR, 2023.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825 , 2023a.\nJiang, Z., Ren, Y ., Ye, Z., Liu, J., Zhang, C., Yang, Q., Ji,\nS., Huang, R., Wang, C., Yin, X., Ma, Z., and Zhao, Z.\nMega-tts: Zero-shot text-to-speech at scale with intrinsic\ninductive bias. CoRR , abs/2306.03509, 2023b.\nKharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Gir-\ngin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and\nZeghidour, N. Speak, read and prompt: High-fidelity\ntext-to-speech with minimal supervision. Trans. Assoc.\nComput. Linguistics , 11:1703–1718, 2023.\nKong, J., Kim, J., and Bae, J. Hifi-gan: Generative\nadversarial networks for efficient and high fidelity\nspeech synthesis. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pp. 17022–17033. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\nc5d736809766d46260d816d8dbc9eb44-Paper.\npdf.\nKong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.\nDiffwave: A versatile diffusion model for audio synthesis.\nIn9th International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net, 2021.\n11\n--- Page 12 ---\nKreuk, F., Synnaeve, G., Polyak, A., Singer, U., D ´efossez,\nA., Copet, J., Parikh, D., Taigman, Y ., and Adi, Y . Audio-\ngen: Textually guided audio generation. In The Eleventh\nInternational Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenRe-\nview.net, 2023.\nLe, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R.,\nWilliamson, M., Manohar, V ., Adi, Y ., Mahadeokar, J.,\nand Hsu, W. V oicebox: Text-guided multilingual univer-\nsal speech generation at scale. In Advances in Neural In-\nformation Processing Systems 36: Annual Conference on\nNeural Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16, 2023 ,\n2023.\nLee, D., Kim, C., Kim, S., Cho, M., and Han, W. Autore-\ngressive image generation using residual quantization. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June\n18-24, 2022 , pp. 11513–11522. IEEE, 2022.\nLi, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: boot-\nstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In International\nConference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA , volume 162 of\nProceedings of Machine Learning Research , pp. 12888–\n12900. PMLR, 2022.\nLi, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2:\nbootstrapping language-image pre-training with frozen\nimage encoders and large language models. In Inter-\nnational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of\nProceedings of Machine Learning Research , pp. 19730–\n19742. PMLR, 2023.\nLi, T., Tian, Y ., Li, H., Deng, M., and He, K. Autoregressive\nimage generation without vector quantization. CoRR ,\nabs/2406.11838, 2024.\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W.,\nSmetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y ., dos\nSantos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido,\nS., and Rives, A. Evolutionary-scale prediction of atomic\nlevel protein structure with a language model. bioRxiv ,\n2022. doi: 10.1101/2022.07.20.500902.\nLiu, H., Chen, Z., Yuan, Y ., Mei, X., Liu, X., Mandic, D. P.,\nWang, W., and Plumbley, M. D. Audioldm: Text-to-audio\ngeneration with latent diffusion models. In International\nConference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA , volume 202 of Proceed-\nings of Machine Learning Research , pp. 21450–21474.\nPMLR, 2023a.Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction\ntuning. In Advances in Neural Information Processing\nSystems 36: Annual Conference on Neural Information\nProcessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023 , 2023b.\nMa, Z., Song, Y ., Du, C., Cong, J., Chen, Z., Wang, Y .,\nWang, Y ., and Chen, X. Language model can listen while\nspeaking. CoRR , abs/2408.02622, 2024.\nMadani, A., Krause, B., Greene, E. R., Subramanian, S.,\nMohr, B. P., Holton, J. M., Olmos, J. L., Xiong, C., Sun,\nZ. Z., Socher, R., Fraser, J. S., and Naik, N. V . Large\nlanguage models generate functional protein sequences\nacross diverse families. Nature Biotechnology , pp. 1–\n8, 2023. URL https://api.semanticscholar.\norg/CorpusID:256304602 .\nNachmani, E., Levkovitch, A., Salazar, J., Asawaroengchai,\nC., Mariooryad, S., Skerry-Ryan, R. J., and Ramanovich,\nM. T. Lms with a voice: Spoken language modeling\nbeyond speech tokens. CoRR , abs/2305.15255, 2023.\nNachmani, E., Levkovitch, A., Hirsch, R., Salazar, J.,\nAsawaroengchai, C., Mariooryad, S., Rivlin, E., Skerry-\nRyan, R. J., and Ramanovich, M. T. Spoken question\nanswering and speech continuation using spectrogram-\npowered LLM. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna, Aus-\ntria, May 7-11, 2024 . OpenReview.net, 2024.\nNguyen, T. A., Kharitonov, E., Copet, J., Adi, Y ., Hsu, W.,\nElkahky, A., Tomasello, P., Algayres, R., Sagot, B., Mo-\nhamed, A., and Dupoux, E. Generative spoken dialogue\nlanguage modeling. Trans. Assoc. Comput. Linguistics ,\n11:250–266, 2023.\nNguyen, T. A., Muller, B., Yu, B., Costa-juss `a, M. R., El-\nbayad, M., Popuri, S., Duquenne, P., Algayres, R., Mav-\nlyutov, R., Gat, I., Synnaeve, G., Pino, J., Sagot, B., and\nDupoux, E. Spirit-lm: Interleaved spoken and written\nlanguage model. CoRR , abs/2402.05755, 2024.\nOpenAI. GPT-4 technical report. CoRR , abs/2303.08774,\n2023.\nOpenAI. 2024. URL https://openai.com/index/\nhello-gpt-4o/ .\nPatel, A., Li, B., Rasooli, M. S., Constant, N., Raffel, C.,\nand Callison-Burch, C. Bidirectional language models\nare also few-shot learners. In The Eleventh International\nConference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently\nscaling transformer inference. In Proceedings of the Sixth\n12\n--- Page 13 ---\nConference on Machine Learning and Systems, MLSys\n2023, Miami, FL, USA, June 4-8, 2023 . mlsys.org, 2023a.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently\nscaling transformer inference. In Proceedings of the Sixth\nConference on Machine Learning and Systems, MLSys\n2023, Miami, FL, USA, June 4-8, 2023 . mlsys.org, 2023b.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision. In\nProceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent , volume 139 of Proceedings of Machine Learning\nResearch , pp. 8748–8763. PMLR, 2021.\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey,\nC., and Sutskever, I. Robust speech recognition via\nlarge-scale weak supervision. In International Confer-\nence on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA , volume 202 of Proceedings of\nMachine Learning Research , pp. 28492–28518. PMLR,\n2023.\nReece, A., Cooney, G., Bull, P., Chung, C., Dawson, B.,\nFitzpatrick, C., Glazer, T., Knox, D., Liebscher, A., and\nMarin, S. The candor corpus: Insights from a large\nmultimodal dataset of naturalistic conversation. Science\nAdvances , 9(13):eadf3197, 2023.\nRubenstein, P. K., Asawaroengchai, C., Nguyen, D. D.,\nBapna, A., Borsos, Z., de Chaumont Quitry, F., Chen, P.,\nBadawy, D. E., Han, W., Kharitonov, E., Muckenhirn,\nH., Padfield, D., Qin, J., Rozenberg, D., Sainath, T. N.,\nSchalkwyk, J., Sharifi, M., Ramanovich, M. T., Tagliasac-\nchi, M., Tudor, A., Velimirovic, M., Vincent, D., Yu, J.,\nWang, Y ., Zayats, V ., Zeghidour, N., Zhang, Y ., Zhang, Z.,\nZilka, L., and Frank, C. H. Audiopalm: A large language\nmodel that can speak and listen. CoRR , abs/2306.12925,\n2023.\nSchwaller, P., Laino, T., Gaudin, T., Bolgar, P., Hunter,\nC. A., Bekas, C., and Lee, A. A. Molecular transformer:\nA model for uncertainty-calibrated chemical reaction pre-\ndiction. ACS Central Science , 5(9):1572–1583, 2019.\nShen, K., Ju, Z., Tan, X., Liu, E., Leng, Y ., He, L., Qin,\nT., Zhao, S., and Bian, J. Naturalspeech 2: Latent diffu-\nsion models are natural and zero-shot speech and singing\nsynthesizers. In The Twelfth International Conference on\nLearning Representations, ICLR 2024, Vienna, Austria,\nMay 7-11, 2024 . OpenReview.net, 2024.\nShi, C., Xu, M., Zhu, Z., Zhang, W., Zhang, M., and Tang,\nJ. Graphaf: a flow-based autoregressive model for molec-\nular graph generation. In 8th International Conferenceon Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\nSiuzdak, H. V ocos: Closing the gap between time-domain\nand fourier-based neural vocoders for high-quality audio\nsynthesis. In The Twelfth International Conference on\nLearning Representations, ICLR 2024, Vienna, Austria,\nMay 7-11, 2024 . OpenReview.net, 2024.\nSu, J., Ahmed, M. H. M., Lu, Y ., Pan, S., Bo, W., and Liu,\nY . Roformer: Enhanced transformer with rotary position\nembedding. Neurocomputing , 568:127063, 2024. doi: 10.\n1016/J.NEUCOM.2023.127063. URL https://doi.\norg/10.1016/j.neucom.2023.127063 .\nSu, Y ., Lan, T., Li, H., Xu, J., Wang, Y ., and Cai, D.\nPandagpt: One model to instruction-follow them all.\nCoRR , abs/2305.16355, 2023.\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\nquence learning with neural networks. In Advances in\nNeural Information Processing Systems 27: Annual Con-\nference on Neural Information Processing Systems 2014,\nDecember 8-13 2014, Montreal, Quebec, Canada , pp.\n3104–3112, 2014.\nTang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L.,\nMa, Z., and Zhang, C. SALMONN: towards generic hear-\ning abilities for large language models. In The Twelfth\nInternational Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenRe-\nview.net, 2024.\nTang, R., Kumar, K., Yang, G., Pandey, A., Mao, Y .,\nBelyaev, V ., Emmadi, M., Murray, G. C., Ture, F., and\nLin, J. Speechnet: Weakly supervised, end-to-end speech\nrecognition at industrial scale. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language\nProcessing: EMNLP 2022 - Industry Track, Abu Dhabi,\nUAE, December 7 - 11, 2022 , pp. 285–293. Association\nfor Computational Linguistics, 2022.\nTeam, C. Chameleon: Mixed-modal early-fusion foundation\nmodels. CoRR , abs/2405.09818, 2024.\nTeam, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,\nS., Pathak, S., Sifre, L., Rivi `ere, M., Kale, M. S., Love,\nJ., et al. Gemma: Open models based on gemini research\nand technology. arXiv preprint arXiv:2403.08295 , 2024.\nTian, K., Jiang, Y ., Yuan, Z., Peng, B., and Wang, L. Visual\nautoregressive modeling: Scalable image generation via\nnext-scale prediction. CoRR , abs/2404.02905, 2024.\nvan den Oord, A., Vinyals, O., and kavukcuoglu, k. Neural\ndiscrete representation learning. In Guyon, I., Luxburg,\nU. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,\nS., and Garnett, R. (eds.), Advances in Neural Information\n13\n--- Page 14 ---\nProcessing Systems , volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/\n7a98af17e63a0ac09ce2e96d03992fbc-Paper.\npdf.\nVeluri, B., Peloquin, B. N., Yu, B., Gong, H., and Gol-\nlakota, S. Beyond turn-based interfaces: Synchronous\nllms as full-duplex dialogue agents. arXiv preprint\narXiv:2409.15594 , 2024.\nWang, T., Zhou, L., Zhang, Z., Wu, Y ., Liu, S., Gaur, Y .,\nChen, Z., Li, J., and Wei, F. Viola: Unified codec lan-\nguage models for speech recognition, synthesis, and trans-\nlation. CoRR , abs/2305.16107, 2023.\nWeissenborn, D., T ¨ackstr ¨om, O., and Uszkoreit, J. Scal-\ning autoregressive video models. In 8th International\nConference on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net,\n2020.\nWu, H., Chang, K., Wu, Y ., and Lee, H. Speechgen: Un-\nlocking the generative power of speech language mod-\nels with prompts. CoRR , abs/2306.02207, 2023. doi:\n10.48550/ARXIV .2306.02207. URL https://doi.\norg/10.48550/arXiv.2306.02207 .\nWu, S., Fei, H., Qu, L., Ji, W., and Chua, T. Next-gpt:\nAny-to-any multimodal LLM. In Forty-first International\nConference on Machine Learning, ICML 2024, Vienna,\nAustria, July 21-27, 2024 . OpenReview.net, 2024.\nXie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q.,\nGu, Y ., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration, 2024.\nXie, Z. and Wu, C. Mini-omni: Language models can hear,\ntalk while thinking in streaming, 2024. URL https:\n//arxiv.org/abs/2408.16725 .\nYang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y .,\nand Yu, D. Diffsound: Discrete diffusion model for text-\nto-sound generation. IEEE ACM Trans. Audio Speech\nLang. Process. , 31:1720–1733, 2023.\nYang, D., Liu, S., Huang, R., Weng, C., and Meng, H.\nInstructtts: Modelling expressive TTS in discrete la-\ntent space with natural language style prompt. IEEE\nACM Trans. Audio Speech Lang. Process. , 32:2913–2925,\n2024.\nYou, J., Ying, R., Ren, X., Hamilton, W. L., and Leskovec,\nJ. Graphrnn: Generating realistic graphs with deep auto-\nregressive models. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning, ICML 2018,\nStockholmsm ¨assan, Stockholm, Sweden, July 10-15, 2018 ,volume 80 of Proceedings of Machine Learning Research ,\npp. 5694–5703. PMLR, 2018.\nZeghidour, N., Luebs, A., Omran, A., Skoglund, J., and\nTagliasacchi, M. Soundstream: An end-to-end neural\naudio codec. IEEE ACM Trans. Audio Speech Lang.\nProcess. , 30:495–507, 2022.\nZhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou,\nY ., and Qiu, X. Speechgpt: Empowering large language\nmodels with intrinsic cross-modal conversational abili-\nties. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, Singapore, December 6-10,\n2023 , pp. 15757–15773. Association for Computational\nLinguistics, 2023a.\nZhang, Y ., Han, W., Qin, J., Wang, Y ., Bapna, A., Chen,\nZ., Chen, N., Li, B., Axelrod, V ., Wang, G., Meng, Z.,\nHu, K., Rosenberg, A., Prabhavalkar, R., Park, D. S.,\nHaghani, P., Riesa, J., Perng, G., Soltau, H., Strohman,\nT., Ramabhadran, B., Sainath, T. N., Moreno, P. J., Chiu,\nC., Schalkwyk, J., Beaufays, F., and Wu, Y . Google\nUSM: scaling automatic speech recognition beyond 100\nlanguages. CoRR , abs/2303.01037, 2023b.\nZhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M.,\nShamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and Levy,\nO. Transfusion: Predict the next token and diffuse images\nwith one multi-modal model, 2024.\nZhou, X., Wang, J., Cui, Z., Zhang, S., Yan, Z., Zhou, J., and\nZhou, C. Mmspeech: Multi-modal multi-task encoder-\ndecoder pre-training for speech recognition. In 24th An-\nnual Conference of the International Speech Communi-\ncation Association, Interspeech 2023, Dublin, Ireland,\nAugust 20-24, 2023 , pp. 4943–4947. ISCA, 2023.\n14\n--- Page 15 ---\nA. Other Related Works\nAutoregressive Generative Models. The autoregressive generative modeling has achieved remarkable success in natural\nlanguage processing, giving rise to a variety of powerful LLMs (Sutskever et al., 2014; OpenAI, 2024; 2023; Patel et al.,\n2023). Inspired by these LLMs, numerous studies have examined the application of autoregressive modeling in other\ndomains, such as images (van den Oord et al., 2017; Esser et al., 2021; Li et al., 2024; Tian et al., 2024; Lee et al., 2022;\nChang et al., 2022), graphs (You et al., 2018), videos (Weissenborn et al., 2020), molecules (Shi et al., 2020; Schwaller et al.,\n2019) and protein sequences (Madani et al., 2023; Lin et al., 2022). The fundamental concept of autoregressive modeling\nfocuses on iteratively generating the entire segment from the intermediate portion, which is particularly well-suited for the\naudio generation.\nMulti-modal LLMs. Multimodal Large Language Models (MM-LLMs) strive to incorporate knowledge from diverse\nmodalities. A key category of MM-LLMs concentrates on developing connectors (Li et al., 2022; 2023; Liu et al., 2023b;\nAlayrac et al., 2022) that identify knowledge alignment across various modalities. An alternative strategy (Team, 2024; Zhou\net al., 2024; Xie et al., 2024) merges all modalities into a cohesive sequence of tokens and utilizes LLMs to sequentially\ngenerate them using modified attention masks. These methods (Wu et al., 2024; Su et al., 2023; Fu et al., 2024) even\nintegrate audio as an input modality, and by simply combining text and audio through MM-LLM techniques, they can address\none-direction conditional generation tasks such as speech-to-text translation (e.g., ASR and spoken language understanding)\n(Radford et al., 2023; Zhang et al., 2023b; Deshmukh et al., 2023; Arora et al., 2023; Tang et al., 2024; Chu et al., 2024;\nZhou et al., 2023; Baevski et al., 2020; Gao et al., 2023) and text-to-speech translation (e.g., TTS) (Elizalde et al., 2023; Liu\net al., 2023a; Huang et al., 2023; Nachmani et al., 2023; Yang et al., 2023; Kreuk et al., 2023; Borsos et al., 2023; Copet\net al., 2023; Chen et al., 2024; Anastassiou et al., 2024; Jiang et al., 2023b; Kong et al., 2021; Shen et al., 2024; Casanova\net al., 2022; Siuzdak, 2024; Yang et al., 2024; Kharitonov et al., 2023; Le et al., 2023). However, these methods are limited\nto multi-turn (or even single-turn) QA tasks (where the model produces an answer only after the question is completed, as\nsignaled by the voice-activity-detection (V AD) module, e.g. special tokens, button pressing and hard tunr-taking interval\nthreshold.) and thereby struggle with real-time voice interaction tasks, which is the primary focus of our work.\nB. Streaming Conditional Inference\n𝑠!\"𝑠̂!#𝑠$\"𝑠̂$#𝑠%\"𝑠̂%#𝑠&\"𝑠̂&#StreamingInferencewithconditionaluserinput𝑠'\"𝑠̂'#𝐤!\"𝐯!\"𝐤#\"𝐯#\"NextChunk𝐤#$𝐯#$𝐤%\"𝐯%\"𝐤%$𝐯%$𝐤&\"𝐯&\"𝐤&$𝐯&$𝐤'\"𝐯'\"𝐤'$𝐯'$StreamingInput𝐪'$ChunkAttentionLastChunk\nFigure 9. The figure illustrates the chunk-wise streaming inference\nprocess. Within each chunk, (sa\n1, sa\n2, sa\n3, sa\n4, sa\n5)represents the pro-\nvided speaker sequence. Their corresponding keys and values\nare stored in the KV-cache. NTPP sequentially predicts tokens\n(ˆsb\n1,ˆsb\n2,ˆsb\n3,ˆsb\n4,ˆsb\n5)based on generated query vectors, which are di-\nrected to the Key-Value (KV) cache through attention computations.\nOnce a chunk is filled, the inference proceeds to the next chunk.In order to simulate a real-time user-assistant commu-\nnication scenario, our speech LMs improved by NTPP\nshould be proficient in conducting conditional inference\nwith streaming user voice input. In this inference set-\nting, one speaker’s voice input is provided as the user,\nand the model is assigned the task of inferring the other\naudio channel. This creates a situation that resembles a\nconstrained generation problem. If the inference process\nstrictly follows the training process, then the model should\npredict ˆsb\ntimmediately after receiving the speaker’s voice\ninput sa\ntat time t. However, due to the VQ-V AE tokeniza-\ntion mechanism, it’s not feasible to receive just a single\nspeech token from the speaker channel during the stream-\ning inference. This is because the VQ-V AE requires a\ncomplete continuous speech signal input with a specific\ntime length. Therefore, we need to determine when the\nmodel should start generating spoken responses upon re-\nceiving streaming user input speech tokens. Specifically,\nthe inference process follows the chunk-wise style, con-\ntaining a predetermined λnumber of tokens. As long as the number of user input tokens reaches λ(a chunk of speaker input\nis given), our model begins to generate predictions until the number of predicted tokens also reaches λ(a chunk is filled).\nThis procedure is repeated until the end of user voice inputs (e.g., the conclusion of the voice-assistant service). Here we\nomit the RVQ-verision inference mechanism since the only difference is the inclusion of additional depth tokens (if λ= 5\nin the VQ-based inference, then λ= 5Din the RVQ-based inference).\n15\n--- Page 16 ---\nC. More Implementation Details and Hyper-parameter Settings\nC.1. Hyper-parameter Settings\nOur model is trained on 16 A100 GPUs, utilizing a cosine annealing learning rate scheduler with a minimum learning rate\nof 4e-6 and a maximum learning rate of 4e-4. Each training epoch consists of 40,000 steps, with batch size 64 for each step.\nDuring fine-tuning, we use learn rate from 4e-6 to 5e-5.\nC.2. Dialogue linguistic quality\n(a)\nOverlap\n(b)\nBackchannel\nUh-huh.Let me think,,for example, you can tell a story.err...\n(c)\nReflective \nPause\nYou can takeAlright, that's enough.…Silence…\n(d)\nInterruption\nFigure 10. Illustration of turn-taking events: IPU (Interpausal Unit), Turn (for speaker A and Speaker B, resp), P.(within-speaker Pause),\nGap and Overlap.\nOur model generates two audio channels at the same time, allowing us to use basic V oice Activity Detection (V AD) tools on\nthe output to gather turn-taking metrics. According to the settings in (Nguyen et al., 2023), an Inter-Pausal Unit (IPU) is a\ncontinuous speech segment within one speaker’s channel, bordered by V AD-detected silences longer than 200ms on both\nends. Silence is defined as the lack of voice signals on either channel, while overlap refers to segments where voice signals\nare detected on both channels. Silences can be further divided into gaps (between IPUs of different speakers) and pauses\n(within the same speaker’s IPUs). Consecutive IPUs by the same speaker, separated by a pause, are merged into a single\nturn. Our analysis will focus on measuring the duration distribution of IPUs, gaps, pauses, and overlaps in both the training\ncorpus and the dialogues generated by our various models.\nC.3. Reflective pause audio dataset\nPrompt for reflective pause\n“Hmm..., this question is a bit complicated, I need to think about it.”\n“Let me recall, uh..., yes, we went to the park that day.”\n“You know, that..., oh, yes, it’s the new restaurant.”\n“I remember he mentioned it, um..., it seems to be last Friday.”\n“This matter, um..., I think we need to discuss it again.”\n“Let me think about it, uh..., yes, that’s it.”\n“I’m not sure, um..., maybe I need to confirm it again.”\n“This question, um..., I think we can solve it this way.”\n“Let me think about it again, uh..., yes, I remember it.”\n“The one you mentioned, um..., I seem to have some impression.”\n“We need to deal with the budget issue of this project. Um..., this problem is a bit complicated, I need to think about it.”\n“Do you remember the last time we met? Let me recall, uh..., yes, we went to the park that day.”\n“Have you heard about the new restaurant? You know, that..., oh, yes, that new restaurant.”\n“When did he tell you the news? I remember he mentioned it, uh..., it seems to be last Friday.”\n“Do you have any suggestions about this plan? This matter, uh..., I think we need to discuss it again.”\n“Can you give me an example? Let me think about it, uh..., yes, that’s it.”\n“Are you sure this data is correct? I’m not sure, uh..., I may need to confirm it again.”\n“How should we deal with this emergency? This problem, uh..., I think we can solve it this way.”\n“Can you explain this concept again? Let me think about it again, uh..., yes, I remember it.”\n“Do you know what he is talking about? The one you said, uh..., I seem to have some impression.”\n16\n--- Page 17 ---\nPrompt for GPT score\nContent (1-5 points):\n1 point: The response is largely irrelevant, incorrect, or fails to address the user’s query. It may be off-topic or provide incorrect\ninformation.\n2 points: The response is somewhat relevant but lacks accuracy or completeness. It may only partially answer the user’s question or\ninclude extraneous information.\n3 points: The response is relevant and mostly accurate, but it may lack conciseness or include unnecessary details that don’t contribute\nto the main point.\n4 points: The response is relevant, accurate, and concise, providing a clear answer to the user’s question without unnecessary\nelaboration.\n5 points: The response is exceptionally relevant, accurate, and to the point. It directly addresses the user’s query in a highly effective\nand efficient manner, providing exactly the information needed.\nStyle (1-5 points):\n1 point: The response is poorly suited for speech interaction, possibly including structured elements like lists or being overly complex,\ndisjointed, or difficult to understand.\n2 points: The response is somewhat suitable but may be too long, too short, or awkwardly phrased, making it less effective in a speech\ninteraction context.\n3 points: The response is generally suitable for speech interaction, but it may have minor issues with length, clarity, or fluency that\ndetract slightly from the overall effectiveness.\n4 points: The response is well-suited for speech interaction, with appropriate length, clear language, and a natural flow. It is easy to\nunderstand when spoken aloud.\n5 points: The response is perfectly suited for speech interaction. It is the ideal length, highly clear, and flows naturally, making it easy\nto follow and understand when spoken.\nBelow are the transcription of user’s instruction and models’ response:\n### [Instruction]: {instruction }\n### [Response]: {response }\nAfter evaluating, please output the scores in JSON format: {“content”: content score, “style”: style score }. You don’t need to provide\nany explanations.\nD. Case study\nScenario: A user engages in a conversation with Parrot, describing an\nobject and asking the model to identify it.\nUser: Please listen to my description of an object below, and say its\nname when you have guessed it. The description is: it has four legs,\na flat surface, and is often used for dining or working...\nParrot: I guess it might be a table.\nFigure 11. Case study of NTPP interrupt human speaking correctly and timely.\nTo intuitively understand the differences in responses from our models, we provide an example in Figure 11. In this scenario,\nNTPP interrupts the user at the precise moment it has gathered enough information to make an accurate prediction. This\ncapability is a significant departure from current models that would typically wait for the user to finish speaking before\nresponding. The ability to interject appropriately not only demonstrates the model’s advanced comprehension skills but also\nenhances the fluidity and naturalness of the interaction.\n17",
  "text_length": 76277
}