{
  "id": "http://arxiv.org/abs/2506.04253v1",
  "title": "HADA: Human-AI Agent Decision Alignment Architecture",
  "summary": "We present HADA (Human-AI Agent Decision Alignment), a protocol- and\nframework agnostic reference architecture that keeps both large language model\n(LLM) agents and legacy algorithms aligned with organizational targets and\nvalues. HADA wraps any algorithm or LLM in role-specific stakeholder agents --\nbusiness, data-science, audit, ethics, and customer -- each exposing\nconversational APIs so that technical and non-technical actors can query,\nsteer, audit, or contest every decision across strategic, tactical, and\nreal-time horizons. Alignment objectives, KPIs, and value constraints are\nexpressed in natural language and are continuously propagated, logged, and\nversioned while thousands of heterogeneous agents run on different\norchestration stacks. A cloud-native proof of concept packages a production\ncredit-scoring model (getLoanDecision) and deploys it on\nDocker/Kubernetes/Python; five scripted retail-bank scenarios show how target\nchanges, parameter tweaks, explanation requests, and ethics triggers flow end\nto end through the architecture. Evaluation followed the Design-Science\nResearch Methodology. Walkthrough observation and log inspection demonstrated\ncomplete coverage of six predefined objectives: every role could invoke\nconversational control, trace KPIs and value constraints, detect and mitigate\nZIP-code bias, and reproduce full decision lineage, independent of the\nunderlying LLM or agent library. Contributions: (1) an open-source HADA\narchitecture, (2) a mid-range design theory for human-AI alignment in\nmulti-agent systems, and (3) empirical evidence that framework-agnostic,\nprotocol-compliant stakeholder agents improve accuracy, transparency, and\nethical compliance in real-world decision pipelines.",
  "authors": [
    "Tapio Pitkäranta",
    "Leena Pitkäranta"
  ],
  "published": "2025-06-01T14:04:52Z",
  "updated": "2025-06-01T14:04:52Z",
  "categories": [
    "cs.AI",
    "cs.HC",
    "cs.AI, cs.SE, cs.MA, cs.CL, cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04253v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04253v1  [cs.AI]  1 Jun 2025HADA: H UMAN -AI A GENT DECISION ALIGNMENT\nARCHITECTURE\nTapio Pitkäranta\nDepartment of Computer Science and Engineering\nAalto University, Finland\ntapio.pitkaranta@iki.fiLeena Pitkäranta\nDepartment of Industrial Engineering and Management\nAalto University, Finland\nleena.pitkaranta@iki.fi\nJune 6, 2025\nABSTRACT\nProblem & Motivation. Thegenerative AI boom is spawning rapid deployment of diverse LLM\nsoftware agents . New standards such as the Model Context Protocol (MCP) and Agent-to-Agent (A2A)\nprotocols let agents share data and tasks, yet organizations still lack a rigorous way to keep those\nagents — and legacy algorithms — aligned with organizational targets and values.\nObjectives of the Solution. We aim to deliver a software reference architecture that (i) provides\nevery stakeholder natural-language interaction across planning horizons with software agents and\nAI algorithmic logic, (ii) provides a multi-dimensional way for aligning stakeholder targets and\nvalues with algorithms and agents, (iii) provides an example for jointly modelling AI algorithms,\nsoftware agents, and LLMs, (iv) provides a way for stakeholder interaction and alignment across\ntime scales, (v) scales to thousands of algorithms and agents while remaining auditable, (vi) remains\nframework-agnostic , allowing the use of any underlying LLM, agent library, or orchestration stack\nwithout requiring redesign.\nDesign & Development. Guided by the Design-Science Research Methodology (DSRM) , we en-\ngineered HADA (Human-Algorithm Decision Alignment)—a protocol-agnostic, multi-agent archi-\ntecture that layers role-specific interaction agents over both Large-Language Models and legacy\ndecision algorithms. Our reference implementation containerises a production credit-scoring model,\ngetLoanDecision , and exposes it through stakeholder agents (business manager, data scientist,\nauditor, ethics lead and customer), enabling each role to steer, audit and contest every decision via\nnatural-language dialogue. The resulting constructs, design principles and justificatory knowledge\nare synthesised into a mid-range design theory that generalises beyond the banking pilot.\nDemonstration. HADA is instantiated on a cloud-native stack— Docker, Kubernetes andPython —and\nembedded in a retail-bank sandbox. Five scripted scenarios show how business targets, algorithmic\nparameters, decision explanations and ethics triggers propagate end-to-end through the HADA\narchitecture.\nEvaluation. Walkthrough observation and log inspection were used to gauge HADA against six\npredefined objectives. A stakeholder–objective coverage matrix showed 100 % fulfilment: every\nrole could invoke conversational control, trace KPIs and values, detect and correct bias (ZIP-code\ncase), and reproduce decision lineage—without dependence on a particular agent hierarchy or LLM\nprovider.\nContributions. The research delivers (i) an open-source HADA reference architecture, (ii) an\nevaluated mid-range design theory for human–AI alignment in multi-agent settings, and (iii) empirical\nevidence that framework-agnostic, protocol-compliant stakeholder agents can simultaneously enhance\naccuracy, transparency and ethical compliance in real-world decision pipelines.\nKeywords AI Alignment Problem, Agentic AI, Large Language Models (LLM), Multi-Agent Systems (MAS), Value\nAlignment, Reference Architecture, Natural-Language Interaction\n--- Page 2 ---\nAPREPRINT - JUNE 6, 2025\n1 Introduction\nThe recent surge of transformer-based Large Language Models (LLMs) —from BERT (Devlin et al., 2018) through\nGPT-3 (Brown et al., 2020) to GPT-4—has turned artificial intelligence from an expert playground into a ubiquitous,\nconversational medium embedded in e-mail clients, code editors and service chatbots. Powered by the self-attention\nmechanism introduced in the seminal Transformer architecture (Vaswani et al., 2017), modern LLMs let non-experts\nquestion, instruct and refine algorithms in plain language, shifting human–machine interaction from rigid menu clicks to\nrich, bidirectional dialogue. This democratisation is fuelling an unprecedented wave of LLM software agents , as teams\nwrap models with tool-calling, planning and memory modules to automate tasks across every industry sector. Notably,\nsome LLMs have reportedly passed the Turing test , underscoring their ability to mimic human-level conversation (Jones\nand Bergen, 2025). As organisations embed these self-improving, multi-agent systems in mission-critical workflows,\nthey hand over growing autonomy—and thus responsibility—to algorithmic decision makers. When that autonomy drifts\nfrom stakeholder values, the results can resemble the “weapons of math destruction” critics have warned about (O’neil,\n2017). Solving this alignment problem requires a design-science perspective that marries technical advances with\nmanagerial and ethical safeguards (Christian, 2020; Wilson, 1999).\nWe propose HADA —Human- AI Agent Decision Alignment. HADA wraps a single production AI Tools in stakeholder-\nspecific agents, enabling business managers, data scientists, auditors, ethics leads and customers to steer, audit and\ncontest every credit decision through dialogue. While the pilot centres on this banking use-case, HADA’s metamodel\nis protocol-agnostic: it natively supports the Model Context Protocol (MCP) and emerging Agent-to-Agent (A2A)\nstandards, and remains compatible with any LLM, agent framework or orchestration stack. HADA builds on earlier\nRetrieval-Augmented Generation prototype that exposed natural-language controls to business users (Pitkäranta and\nPitkäranta, 2024).\n1.1 Research Methodology\nThis study follows the Design Science Research methodology (DSR) to bridge theory and practice through iterative\nartifact creation and evaluation. Guided by the canonical DSR process model (Peffers et al., 2020), we executed\nproblem identification, objective definition, artefact design, demonstration and evaluation cycles. Consistent with\nGregor and Jones’s anatomy of mid-range design theory (Gregor and Jones, 2007) and the foundational DSR guidelines\nby Hevner et al. (Hevner et al., 2004), we formalised HADA’s design rationale, constructs and principles as a mid-range\ntheory —general enough to transfer across domains, yet concrete enough to guide engineering practice. Large language\nmodels were used to assist with spell checking1.\n1.2 Problem Identification\nDespite rapid uptake of LLM agents and legacy decision algorithms, three persistent gaps hinder responsible deployment:\nG1Alignment Gap — Organisations lack systematic mechanisms to keep both algorithms and agents aligned with\nmeasurable KPIs, codified values and evolving regulation (Christian, 2020; Amodei et al., 2016).\nG2Containment Gap — Beyond objective alignment, enterprises still lack reliable kill-switches, circuit breakers\nand anomaly detection that can restrain autonomous systems when they deviate or self-modify in production (Su-\nleyman and Bhaskar, 2023; Yampolskiy, 2014; Brundage et al., 2020; Hadfield-Menell et al., 2017).\nG3Integration Gap — Although patterns such as Retrieval-Augmented Generation exist, practitioners have\nfew vendor-agnostic blueprints for embedding existing decision algorithms into multi-agent, LLM-powered\nworkflows while maintaining end-to-end auditability (Pitkäranta and Pitkäranta, 2024; Lewis et al., 2020).\n1.3 Objectives of the Solution\nHADA addresses these gaps through six design objectives:\nO1Natural-language interaction across planning horizons — Equip every stakeholder—from C-suite to on-\ncall engineers—with a conversational interface that lets them restate or define quarterly Objectives and Key\nResults (OKRs) and have those changes automatically translated into concrete, daily algorithmic parameters\nand guard-rails.\n1During the preparation of this manuscript, the authors made limited use of several OpenAI generative-AI services—namely\nGPT-4o ,GPT-o3 , and GPT-o4-mini —exclusively for language polishing (grammar, stylistic refinement, typo correction) and\nreference-list formatting. All prompts, intermediate outputs, and final wording were critically reviewed and verified by the authors,\nwho accept full responsibility for the content.\n2\n--- Page 3 ---\nAPREPRINT - JUNE 6, 2025\nFigure 1: Single LLM agent core components\nO2Multi-dimensional alignment (targets & values) — Provide two complementary tracks: (i) OKR Alignment :\nbind each AI algorithm’s reward function or decision logic to measurable Objectives & Key Results; (ii) Values\nAlignment : encode organisational ethical principles as machine-readable policies and maintain a verifiable audit\nchain from high-level values to low-level actions.\nO3Reference architecture for hierarchical, hybrid AI systems — Provide clear definitions of LLM agents\nand AI algorithms, and address the AI containment gap. Specify a modular stack that separates the strategy\nlayer (responsible for goal and policy management), the coordination layer (responsible for multi-agent task\nbrokerage), and the execution layer (comprising LLM agents and classical algorithms), so that revising a\nquarterly objective triggers an orchestrated cascade of updates while preserving value constraints.\nO4Stakeholder alignment across time scales — Provide a repeatable method for eliciting and reconciling\nstakeholder goals at annual, quarterly and operational cadences, then encoding them as machine-readable\npolicies that guide daily agent behaviour.\nO5Scalability, auditability and near-real-time propagation — Support thousands of agents and algorithms\nwhile propagating objective or policy changes from strategy to production within hours, without sacrificing\naudit trails, explainability or rollback safety.\nO6Framework-agnostic, policy-driven design — Remain independent of any single LLM, agent framework or\norchestration tool; represent objectives and policies in open standards (e.g. MCP, A2A) so that strategic edits\nflow unimpeded to heterogeneous runtime environments.\n2 Related Work: Emerging LLM Software Agents\nLLM agents —sometimes called LLM-powered agents —combine a large language model with an explicit control loop.\nFormally, such an agent maintains (i) a prompt or system specification that encodes its initial policy, (ii) a mutable\nconversation trace that acts as state, and (iii) the ability to acton an external environment via tools or APIs (Cemri\net al., 2025; Naveed et al., 2024). Leveraging the underlying LLM for perception and inference, the agent can perform\nnatural-language understanding and generation, task decomposition, planning, decision making, and tool invocation (Xi\net al., 2023; Wang et al., 2024a; Yang et al., 2023). Concretely, a user request may be broken into sub-tasks, relevant\ninformation fetched from the web, external services called, and the aggregated result summarised for the user (Yao\net al., 2023a; Shinn et al., 2023a).\nLLM agents have progressed rapidly, yet several hurdles remain. Hallucinations—plausible but incorrect an-\nswers—undermine reliability (Ji et al., 2023), prompting mitigation via grounding and retrieval-augmented generation\n(RAG) (Lewis et al., 2020). Further obstacles include safety and ethical concerns, computational expense, and biases\ninherited from training data (Bender et al., 2021; Weidinger et al., 2021). The opaque “black-box” reasoning of large\nmodels also hampers debugging and trust in high-stakes domains (Doshi-Velez and Kim, 2017). Current research\ntherefore targets higher factuality, better explainability and scalable multi-agent coordination. Approaches such as\nweb-grounded training (Nakano et al., 2022), dynamic context selection (Laskin et al., 2022), and richer evaluation\nbenchmarks all seek to deliver agents that are both more accurate and more transparent—while emphasising responsible,\nethics-aware deployment.\n2.1 Single LLM Agent\nThe Figure 1 illustrates a high-level architecture of an LLM Agent, which consists of the following components:\nRequests , which serve as input from a human user or another agent initiating the agent’s operations; the LLM Agent ,\nthe central module that utilizes a Large Language Model (LLM) built upon a multimodal foundational model, acting as\n3\n--- Page 4 ---\nAPREPRINT - JUNE 6, 2025\nthe coordinator and managing interactions with all other components; Tools , which are external modules or systems\nthe agent can employ to gather data or perform specific tasks; Memory , a mechanism for storing and retrieving\npast interactions or knowledge, whether short-term or long-term; and Planning , a module responsible for devising a\nstructured approach to solve complex tasks by breaking them down into subtasks. While some studies provide separate\ndefinitions for Actions and Tools (Yang et al., 2025), we group them under the unified Tools concept in this context.\n2.1.1 Memory\nThe memory module stores the agent’s internal logs, including prior thoughts, actions, observations, and interactions\nwith users. Memory is generally categorized into short-term memory , which captures immediate context and is limited\nby the LLM’s context window through in-context learning; long-term memory , which stores persistent knowledge and\npast experiences using external vector databases for scalable retrieval; and hybrid memory , which combines short-term\nand long-term memory to enhance reasoning and accumulate experiences over time.\n2.1.2 Tools and Actions\nTools enable LLM agents to take actions and interact with external systems such as APIs, databases, knowledge bases,\nand external models. They support workflows that help the agent gather information or complete subtasks, like using a\ncode interpreter to generate charts in response to user queries (Naveed et al., 2024).\n2.1.3 Planning\nTheplanning module decomposes a user’s high-level goal into a sequence of smaller, tractable subtasks that the agent\ncan address in turn. This explicit task-structure not only guides downstream execution but also gives the system\na scaffold for more reliable reasoning. Planning without Feedback. When the plan is created in a single pass,\ntask decomposition relies on internal chains of reasoning generated by an LLM. Two widely cited paradigms are\nChain-of-Thought andTree-of-Thoughts , representing single-path and multi-path reasoning, respectively (Wang et al.,\n2024b).\nPlanning with Feedback. One-shot plans struggle with long-horizon or error-prone tasks because they lack a\nmechanism to correct earlier decisions. Feedback-driven approaches overcome this limitation by iteratively revising\nthe plan in light of previous actions and observations. Among the most influential are ReAct (Yao et al., 2023b) and\nReflexion (Shinn et al., 2023b). ReAct interleaves three repeated steps— Thought ,Action , and Observation —so that\neach new reasoning step conditions on feedback from the environment, optional human critiques, or model-based\nself-evaluation (Yao et al., 2023b). In contrast, Reflexion augments an agent with explicit Evaluator andSelf-reflection\nLMs plus short- and long-term memory buffers. After every episode, linguistic feedback is stored in an episodic\nmemory that shapes future decisions, enabling continual improvement across coding, reasoning, and decision-making\ntasks (Shinn et al., 2023b).\n2.2 Emerging LLM Agent Protocols\nA chronological review of recent work shows that the first protocols used by LLM agents surfaced in 2024 (Yang et al.,\n2025). Initial specifications such as Agent Network Protocol ( ANP ) and Model Context Protocol ( MCP ) tackled the\nabsence of standard interfaces that had limited large-language-model (LLM) agents’ capacity to scale, interoperate,\nand connect with external tools and data sources. This groundwork was refined by lighter-weight formats—most\nnotably ACP, Google’s A2A (Agent-to-Agent) protocol, and agents.json —together with infrastructure projects such\nasAgora , all aimed at richer, more structured messaging between agents. The overall trajectory can be grouped into\nthree phases: early ad-hoc standards, consolidation around concise schemas, and the forthcoming generation of layered\nspecifications. Looking ahead to 2025 and beyond, initiatives such as OpenManus andOWL intend to formalise\ngroup-centric, multi-layer protocols, signalling a transition toward fully interoperable, real-world agent ecosystems\n(Yang et al., 2025).\n2.2.1 Model Context Protocol (MCP): LLM–Tools Integration\nTheModel Context Protocol (MCP) is an open-source standard that exposes real-time data from content-management,\nenterprise, and developer systems to LLM-based assistants2. In Figure 1 it functions as the middleware layer between\nthe LLM Agent and external Tools. By replacing ad-hoc, one-off connectors with a single interface, MCP eliminates\nmost of the bespoke plumbing normally required for each new data source. The result is simpler integration, live\n2https://modelcontextprotocol.io/introduction\n4\n--- Page 5 ---\nAPREPRINT - JUNE 6, 2025\ncontext for the model, and more accurate, up-to-date outputs—all while letting developers concentrate on higher-level\napplication logic rather than low-level data wiring.\nAt its core, MCP follows a client-server architecture where a host application can connect to multiple servers. The\narchitecture includes MCP Hosts , which are programs, IDEs, AI tools, or LLM Agents that want to access data through\nMCP; MCP Clients , which are protocol clients that maintain 1:1 connections with servers; MCP Servers , which are\nlightweight programs that each expose specific capabilities through the standardized Model Context Protocol; Local\nData Sources , which refer to your computer’s files, databases, and services that MCP servers can securely access; and\nRemote Services , which are external systems available over the internet (e.g., through APIs) that MCP servers can\nconnect to.\n2.2.2 Google Agent2Agent Protocol (A2A)\nAgent2Agent (A2A) is an open protocol by Google that aims to complement the Model Context Protocol (MCP), which\nprovides helpful tools and context to agents3. The A2A protocol will allow AI agents to communicate with each\nother, securely exchange information, and coordinate actions on top of various enterprise platforms or applications. In\nFigure 1 it functions as the middleware layer between two LLM Agents interacting with each other.\nConceptual Overview: The Agent2Agent (A2A) protocol facilitates communication between independent AI agents\nthrough several key elements4. These include the Agent Card , which is public JSON metadata detailing an agent’s\ncapabilities, endpoint, and authentication for discovery; the A2A Server , which is an agent exposing an HTTP endpoint\nimplementing A2A protocol methods to receive requests and manage tasks; the A2A Client , which is an application or\nagent consuming A2A services by sending requests (e.g., tasks/send) to a server URL; the Task , which is the core work\nunit initiated by a client message (tasks/send or tasks/sendSubscribe), with a unique ID and lifecycle states (submitted,\nworking, input-required, completed, failed, canceled); and the Message , which represents a communication turn\nbetween client (\"user\") and agent (\"agent\"), containing Parts. A Part is the basic content unit in a Message or Artifact,\nsuch as TextPart, FilePart (inline or URI), or DataPart (structured JSON); an Artifact refers to agent-generated outputs\nduring a task (e.g., files, structured data), also composed of Parts. For long tasks, Streaming is supported through\ntasks/sendSubscribe, enabling servers to send real-time progress via Server-Sent Events (TaskStatusUpdateEvent or\nTaskArtifactUpdateEvent). Additionally, Push Notifications allow servers with pushNotifications enabled to proactively\nsend task updates to a client-provided webhook URL, which is configured via tasks/pushNotification/set.\n2.3 Multi-Agent Conversation Patterns in LLM Era\nMulti-agent frameworks such as AutoGen show that once more than two LLM agents collaborate, simple one-to-one\nexchanges become a bottleneck; the communication graph must evolve from isolated dialogues into richer, group-level\npatterns (Wu et al., 2023). In a joint-chat scheme, all agents share a common channel, allowing rapid knowledge\npropagation but also demanding mechanisms for turn-taking and conflict resolution. Alternatively, a hierarchical-chat\ntopology appoints one or more “manager” agents that coordinate a set of specialised workers, reducing the O(n2)\nmessage overhead of full pairwise gossip to roughly O(n)while preserving global context. The general lesson, echoed by\nthe schematic in the accompanying image, is that scalable systems require flexible conversation patterns—peer-to-peer\nfor small teams, hub-and-spoke or tree-structured for larger cohorts—to balance reasoning depth with communication\ncost.\n2.4 Prominent LLM Agent Frameworks and Libraries\nA fast-maturing ecosystem of open-source frameworks now captures the canonical design patterns for agentic large-\nlanguage-model (LLM) systems, letting researchers focus on reasoning strategies rather than boiler-plate glue code.\nLangChain supplies composable primitives—tools, memories and chains—that underpin many production chatbots,\ndocument-question-answering (QA) pipelines and research assistants. Building on the same abstractions, LangGraph\nintroduces a graph-based execution engine with explicit state, branching and parallelism, which is invaluable for\nlong-horizon planning and multi-agent coordination.\nMicrosoft’s AutoGen formalises “chat-loop” interactions in which several LLMs, optionally augmented by external\ntools or human feedback, iteratively critique and refine one another to solve non-trivial tasks. Where private or\nproprietary data are involved, LlamaIndex offers a complete data layer—index construction, retrieval interfaces and\nknowledge-graph support—for retrieval-augmented generation (RAG) over local corpora. CrewAI targets role-oriented\nworkflows: agents endowed with complementary expertise share a common memory to tackle coordinated problem\n3https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\n4https://github.com/google/A2A\n5\n--- Page 6 ---\nAPREPRINT - JUNE 6, 2025\nsolving or simulation environments. For production use, Microsoft’s Semantic Kernel provides a language-agnostic\nplug-in system, a symbolic planner and robust security controls that simplify the integration of generative AI into\nenterprise software stacks. Google’s Agent Development Kit (ADK) implements the Agent-to-Agent (A2A) protocol\nand offers orchestration, safety guards and evaluation utilities for multi-agent research at scale. Also from Google,\nGenkit delivers a code-first, TypeScript workflow with flow orchestration, prompt/version control ( .prompt files),\nobservability and one-command Cloud Run deployment. Finally, Marvin provides a pythonic decorator-based API that\nturns ordinary functions into threaded, asynchronous “AI functions,” making rapid prototyping inside data-science or\nmachine-learning pipelines straightforward. Additional libraries—including Haystack, Embedchain, SuperAGI, Dify\nand OpenAgents—continue to expand the design space and keep the agent-framework landscape highly dynamic.\n3 Design & Development\nThis section instantiates the Design Science Research Methodology (DSRM) (Peffers et al., 2020) step 3 (“ Design &\nDevelopment ”) by describing the three concrete artifacts that operationalise the HADA concept. Together they realise\nthe solution objectives defined in Section 1.\nContext. The pilot exposes the alignment problem that unfolds across the entire life-cycle of an AI-driven credit-\napproval workflow in a retail bank. The cycle is triggered by a strategic shift in business targets, proceeds through model\nredesign and validation, and culminates in production deployment—each phase demanding tight coordination among\nheterogeneous roles. While the organisation strives to shorten “time-to-yes” for unsecured consumer credit, it must\nsimultaneously comply with Basel III, the EU AI Act and local consumer-protection statutes. Operational responsibility\nfor every credit decision is delegated to getLoanDecision , an inherently explainable decision-tree model currently\nrunning in production.\nTo bridge the gaps that arise between technical optimisation, ethical commitments and regulatory oversight along this\nend-to-end trajectory, we propose HADA (Human-Aligned Decision Alignment). HADA not only orchestrates the\nactivities of multiple stakeholders—adding new governance roles (e.g. a Value and Ethics Manager ) and expanding\nthe remit of existing ones (e.g. the Audit Manager )—but also delivers a complementary suite of AI-architecture tools,\nsuch as a version-controlled feature catalogue, a transparent model registry, an ethics-trigger engine and an immutable\ndecision ledger. These technical components furnish each role with the information and control primitives required\nto discharge its responsibilities, thereby addressing alignment challenges that traditional score-card systems leave\nunresolved.\nStructure of the artifacts\nTheDesign & Development section is organised around six inter-locking artifacts: Artifact 3.1 — Strategy-Process\nBlueprint . Formalises the yearly →daily planning loop (OKRs, KPI cascades, operational reviews) that frames every\nsubsequent decision governed by HADA; Artifact 3.2 — Stakeholder & User-Story Specification . Captures the\nproblem context through a stakeholder map, sequenced user stories, and an extended RACI matrix using Artifact 3.3\nas concrete AI Tool / AI Tool; Artifact 3.3 — Prototype AI Tool getLoanDecision . An operational decision-\ntree model (dataset, feature engineering, API) that instantiates the credit-approval use-case; Artifact 3.4 — AI-\nResponsibility Tool (RACI Matrix) . Distils governance duties across eight life-cycle activities, ensuring accountable,\nauditable hand-offs between roles regarding Artifact 3.3; Artifact 3.5 — Generic HADA Architecture . Provides the\nlayered, protocol-agnostic reference design (agent layer, tools layer, DevOps view) on which all domain instances rest;\nArtifact 3.6 — Agent Modelling . Maps human roles to containerised interaction agents and describes the HADA\nController pattern for policy-enforced orchestration regarding.\n3.1 Artifact – Strategy-Process Blueprint\nPurpose. Artifact 3.1 formalises the yearly-to-daily planning loop that steers our bank from strategical decisions to\ndaily activities as illustrated in Figure 2. Our example bank follows a typical strategy formulation and implementation\nprocess in which the yearly overall targets are set in the beginning of the year and the progress and continuous alignment\nof the more detailed activities is followed up typically at least on monthly or quarterly basis. One popular example of\nsuch a framework is ’Objectives and Key Results’ (OKR) framework (Niven and Lamorte, 2016).\nThe OKR framework formalizes the key results follow-up process and describes the mechanisms required to create\nalignment in the organization. While the C-level executives are responsible for the overall strategy formulation and\nset the high-level objectives for the organization, the experts and managers on the lower organizational levels play a\ncrucial role in implementing the strategy. Their responsibility is to ensure that the day-to-day decisions are aligned with\nthe values and overall objectives of the organization and that the key results will be achieved as planned. In our bank\n6\n--- Page 7 ---\nAPREPRINT - JUNE 6, 2025\nFigure 2: Bank strategy process: time horizons and stakeholder coverage\nexample the ‘connecting OKRs’ process described by (Niven and Lamorte, 2016) needs to be expanded to cover not\nonly the decisions and actions taken by the human employees but also the algorithms.\nYearly — Set bank targets. At the start of each fiscal year the Chief Credit Officer (CCO) and peer C-level executives\nestablish portfolio-level Objectives & Key Results (OKRs) together with an updated risk-appetite statement. These\nartifacts are stored in the Business-Target Catalogue and become immutable reference points for all downstream KPI\nspecifications.\nQuarterly — Translate OKRs into actionable KPIs. Every quarter the CCO reconvenes with Business Managers,\nData Scientists, Audit Managers and Ethics Managers to review recent performance and—if necessary—re-weight\nmodel KPIs (e.g. shifting focus from “new-customer acquisition” to “expected-loss minimisation”).\nWeekly & Daily — Operative alignment loops. On a rolling basis the following stakeholders contribute: Business\nManager – reviews live KPI dashboards and reprioritises tickets; Data Scientist – publishes hot-fix model versions or\nfeature toggles via the Model Catalogue; Audit Manager – samples recent decisions from the immutable ledger and\nverifies lineage; Ethical-Compliance Manager – inspects the sensitive-attribute watch-list and issues ethics triggers\nwhen required.\nContribution. The strategy-process blueprint provides a single, auditable spine that a) anchors KPI and value\nalignment across all time horizons, b) delineates clear hand-offs between business, technical and oversight roles, and c)\nremains tooling-agnostic—no assumptions are made about the underlying LLM, agent library or orchestration layer.\n3.2 Artifact – Stakeholder & User-Story Specification\nArtifact 3.2 formalises the problem context and the functional requirements . It is released as a lightweight specification\ncomposed of (i) a stakeholder map, (ii) an ordered set of user stories, and (iii) a RACI matrix that clarifies the roles and\nresponsibilities of each stakeholder. Together these elements provide a transparent overview of the actors involved in\nthe banking use-case and their interactions with the HADA system.\nTable 1 enumerates the principal stakeholder roles, articulates their primary goals and specifies the access rights granted\nby HADA. The mapping was validated in three semi-structured workshops with subject-matter experts from credit-risk,\ncompliance and data-science departments, reinforcing its credibility within the banking sector.\n7\n--- Page 8 ---\nAPREPRINT - JUNE 6, 2025\n3.2.1 Stakeholder map\nTable 1 lists the five principal roles involved in the experimental banking scenario, their primary goals and the HADA\ncomponents they are allowed to access.\nTable 1: Stakeholder map for the banking use-case getLoanDecision\nRole Primary goal in workflow Key HADA privileges\nCustomer Receive a fair, fast and transparent credit\ndecision; raise complaints when neededSubmit loan-application data; obtain\nnatural-language decision explanations;\nopen ethics tickets\nChief Credit\nOfficer\n(CCO)Define company-level Objectives & Key\nResults (OKRs) for credit strategy; align\nrisk appetite, growth targets and regulatory\nlimits across the portfolioSet and update top-level KPI/OKR tar-\ngets; approve credit policy changes; view\nenterprise-wide risk dashboards; give fi-\nnal sign-off on new production decision AI\nTool deployments\nBusiness\nManagerSet and adjust portfolio KPIs (e.g. minimise\nexpected credit losses)Edit KPI targets; approve or reject AI Tool\nversions for production; view live KPI dash-\nboards\nData Scien-\ntistBuild, test and monitor the\ngetLoanDecision model so it meets\ncurrent KPI and risk targetsRetrain models; run offline validation; reg-\nister new AI Tool versions in the Model\nCatalogue\nAudit Man-\nagerVerify compliance and traceability of any\nindividual credit decisionRead-only access to the full decision path,\nmodel version, feature values and audit\nledger\nValue &\nEthics Man-\nagerSafeguard ethical use of data and enforce\norganisational values across all AI ToolsMaintain Values & Data catalogues; flag\nor deprecate sensitive attributes; ap-\nprove/reject attribute changes; mandate\nmodel retraining when catalogue updates\noccur\n3.2.2 User stories\nUser stories used in this implementation described in the below itemized list. The user stories are sequential: first\nbusiness KPI’s are changed by one stakeholder and AI Tool optimized to achieve new targets. This phase induces\nethically questionable logic into the AI Tool but it is not noted by the stakeholder. Then customer applies for loan and\ngets the loan with questionable logic. Then customer returns to file a complaint as he / she noted that questionnable data\nwas used in the loan decision. Then HADA system contacts auditor and after that value and ethics manager to adjust the\nsystem.\n•As the Chief Credit Officer (CCO) , I want to update the quarterly OKRs from acquiring new customers to\nminimising credit losses during the annual strategy-planning process.\n•As a Bank Loan Department Business Manager , I would like to change the business target for short-term\nloan decisions from acquiring new customers to minimising credit problems.\n•As a Data Scientist , I want to create a new version of the getLoanDecision AI Tool so that it matches the\nnew business targets.\n• As a Bank Customer , I would like to apply for a short-term loan.\n•As a Bank Customer , I would like to file a complaint with the ethical value-alignment, because I am being\nasked for ethically questionable information while applying for a loan.\n•As a Bank Audit Manager , I would like to audit the detailed decision criteria for a single short-term loan\ndecision.\n•As a Bank Value and Ethics Manager , I would like to remove ethically questionable data points from\nloan-decision making.\n3.3 Artifact – Prototype AI Tool / AI Algorithm: getLoanDecision\nArtifact 3.3 is the AI Tool / AI Algorithm that decides whether a retail-bank customer is granted a short-term loan.\nThe AI Tool illustrated in Figure 3 includes the dataset, selected features, illustrative decision trees, and the OpenAPI\n8\n--- Page 9 ---\nAPREPRINT - JUNE 6, 2025\nFigure 3: AI Tool getLoanDecision based on Kaggle dataset depicting the decision tree and OpenAPI specification\nspecification for the interface. Artifact 3.3 is an operational instantiation that we demonstrate and evaluate in DSRM\nsteps 4–5.\n•Training data. Used publicly-available Kaggle dataset5(614 anonymised applications, 13 attributes). Added\nZIP_Code as a new feature to the data set that has high correlation to income on that area.\n•Feature engineering. A Data Scientist iteratively tested new attributes and discovered that the applicant’s\nZIP_Code reaches lower credit losses than previous AI Tools.\n•Data Science Notebooks. Jupyter notebooks were used to explore the data, train the model and evaluate its\nperformance. The notebooks are available in the open-source repository.\n•Decision-tree models. Two trees are reported in Figure 3: Version 1.0 (baseline) omits ZIP_Code ; Version 1.1\nincludes it and illustrates how a seemingly innocuous geographic indicator can introduce latent bias.\n•Executable code. A Python/Scikit-learn pipeline performs preprocessing, training and serialisation to joblib .\nThe model is exposed via a HADA micro-service: POST /getLoanDecision/{modelId}\n•Catalogue entries. Each model instance is registered in the AI Tool ,Version andDecision catalogues, and\ncross-referenced to the KPI and Values catalogues. This allows non-technical stakeholders to swap versions,\ndeprecate sensitive features or tighten KPI weights without touching code.\nDuring the role-play scenarios of the prototype was (i) retrained after KPI changes, (ii) deployed, (iii) invoked by a\ncustomer, (iv) audited, and (v) corrected following an ethics complaint—thereby exercising every governance loop\nforeseen by the architecture.\n3.4 Artifact – AI Responsibility tool: RACI Matrix\nThe development and management of the AI Tool (Artifact 3.3) requires a clear understanding of the roles and\nresponsibilities of the various stakeholders involved in the process. A responsibility assignment matrix (RAM),\ncommonly referred to as a RACI matrix (Responsible, Accountable, Consulted, Informed), is a widely used tool for\ndefining and clarifying roles in project and process management. It helps structure stakeholder involvement by assigning\nresponsibility levels to tasks or deliverables, particularly in cross-functional environments (Crawford and Bryce, 2003).\nThe extended RACI matrix in Table 2 distils our stakeholder analysis into a concise governance blueprint for the\ngetLoanDecision workflow. It maps eight critical life-cycle activities—from goal-setting and model (re)development\n5https://www.kaggle.com/datasets/ninzaami/loan-predication\n9\n--- Page 10 ---\nAPREPRINT - JUNE 6, 2025\nto bias remediation and post-hoc audit—onto the four RACI dimensions, explicitly indicating who is Accountable,\nResponsible, Consulted, or merely Informed at each step. Both the choice of roles and their RACI assignments are\norganisation-specific; Table 2 therefore presents just one illustrative role–RACI configuration.\nThe matrix reveals a deliberate segregation of duties: business managers retain ownership of strategy, data scientists\nhold technical responsibility, while automated HADA services execute operationally sensitive tasks such as issuing\ndecisions and logging audits. Specialist oversight roles (Audit Manager and Value & Ethics Manager) are activated only\nwhen their expertise is required, balancing delivery agility with regulatory assurance. By making these role allocations\nexplicit, the matrix operationalises the alignment principles introduced earlier and furnishes a defensible audit trail for\nboth internal governance and external regulators.\nActivity / Decision CCO BM DS Customer Audit DVEM HADA\nSetting organization quarterly\ntargets (OKR)A,R I I I I I\nSetting optimization target for\nAI ToolsA R I I\nOptimizing AI Tools based on\nbusiness targetsI C A,R I\nApproving AI Tool deploy-\nmentI A,R C I I C\nIndividual loan decision A C R\nAuditing a specific loan deci-\nsionI I C I A,R I C\nHandling AI Tool ethics con-\ncernI I C I A,R C\nCreating work assignments\n(tickets)I C C I C C A,R\nTable 2: Stakeholder (see Table 1) responsibilities as RACI matrix for the getLoanDecision AI Tool\nRACI keys :R= Responsible, A= Accountable, C= Consulted, I= Informed.\nRole abbreviations :CCO = Chief Credit Officer, BM= Business Manager, DS= Data Scientist, Audit = AI Audit\nManager, DVEM = Value & Ethics Manager, HADA = The Human Agent Decision Alignment System\n3.5 Artifact — Generic HADA Architecture\nIn order to support implementation of the Artifact 3.1, Artifact 3.2, Artifact 3.3 and Artifact 3.4 together with the\nobjectives of this study, we designed a generic architecture that can be used to implement the HADA framework in\nany domain. Artifact 3.5 is the design embodiment of the proposed Human-AI Tool-Decision-Alignment (HADA)\nframework. The architecture is expressed through two complementary diagrams and an explicative narrative that\ntogether operationalise the Tools Pattern6across a cloud-native, protocol-agnostic stack:\nLayered tools-pattern model (Figure 4) — a high-level, technology-agnostic view that introduces two logical layers.\nTheAgent Layer bundles software agents into OCI-compliant Docker images. Agents coordinate through Agent-to-\nAgent (A2A) protocol, while ingesting natural-language prompts from any channel. A dedicated AI Agent Controller\norchestrates one or more Interaction Agents and consults an AI Agent Registry to discover new capabilities at runtime.\nAgents can collaborate using different organizational layouts, with the simplest being a supervisor hierarchy. Other\nlayouts can also be configured depending on the complexity of the deployment. Google A2A is recommended for\nlarger deployments that require scalability and technological heterogeneity, but alternative protocols can be employed in\nsmaller, more homogeneous environments. The Tools Layer packages two families of dockerised APIs: (i) AI Tools such\nas LLMs and classical decision AI Tools, and (ii) Standard Tools such as metadata catalogues, CRUD services and SSO\nproviders. Both tool families expose uniform HTTP/GRPC endpoints, enabling hot-swap of vendors or models without\ntouching agent code. The AI Tools Layer packages specialized tools that enable AI decision-making processes, including\nAI Decision Tools (technology-agnostic AI algorithms callable over API or MCP), Business Targets (organizational\ntargets linked to AI Tools for optimization), AI Responsibilities (mapping AI decision Tools to organizational roles), and\nAI Metadata (catalogue of data points used by AI Tools). The layers can communicate via either (1) direct API calls\nwhen schemas are known a-priori, or (2) the emerging Model Context Protocol (MCP) when schema-free, embedded\ncontext exchange is preferred. This dual path allows legacy services to coexist with next-generation LLM tooling.\nContainer-and-protocol deployment view (Figure 4) — a DevOps-centred overlay that makes key infrastructure\ndecisions explicit for managing the HADA architecture across Development, Test, and Production environments.\nEvery artifact (agents, AI tools, standard tools) ships as an immutable Docker image, ensuring reproducibility across\n6I.e. the strict separation of agents that decide from tools that act .\n10\n--- Page 11 ---\nAPREPRINT - JUNE 6, 2025\nFigure 4: HADA High Level Architecture\nworkstations, CI pipelines, and Kubernetes clusters. Each component (AI tools, agents, and standard tools) can be\npackaged into separate Docker containers, enabling independent scaling and deployment. For production deployments,\nit is recommended to deploy AI tools, agents, and standard tools on separate Kubernetes clusters, ensuring clear\nresource allocation and scaling requirements for each layer. For instance, AI tools may need more GPU-intensive\nresources, while standard tools may benefit from more lightweight, distributed container setups. This ensures better\noperational efficiency and fault tolerance. All images surface a thin REST/GRPC façade that (a) registers itself in the\nAI Agent Registry for discovery, and (b) publishes an OpenAPI or MCP manifest for validation. A service mesh applies\nzero-trust policy, while sidecars translate between direct HTTP/GRPC and MCP streams, guaranteeing backwards\ncompatibility. Development, Test, and Production instances of the HADA architecture must be configured to allow\ncontinuous integration and continuous delivery (CI/CD) within each environment. These instances will each have their\nown dedicated resources and configurations to handle the distinct testing and deployment needs in each stage of the\nsoftware lifecycle.\nTogether the high level architecture establish how the Tools Pattern can be realised in a vendor-neutral, policy-compliant\nenvironment: agents can scale or be replaced without modifying downstream tools; tools can evolve (e.g. swap a\nproprietary LLM for an open-source one) without redeploying agents. This loose coupling fulfils solution objectives\nO1–O3(KPI alignment, value alignment, auditability) while remaining framework-agnostic and ready for thousands of\nheterogeneous decision AI Tools.\n3.6 Artifact – Agent Modelling\nArtifact 3.6 closes the design loop by specifying how the logical roles enumerated in Artifact 3.2 are realised as\nsoftware agents inside the generic HADA architecture (Artifact 3.5). Whereas the architecture supports *multiple*\nagent–coordination patterns—e.g. monolithic single-LLM agents, role-based hierarchies, flat swarms or functional\npipelines—our banking pilot instantiates a stakeholder-centric model: each human role is mirrored by a dedicated\ninteraction agent, all of which are orchestrated by a single HADA Controller Agent (Figure 4).\nThis configuration delivers two advantages: Traceable alignment. One-to-one mapping between stakeholder and agent\nsimplifies audit trails: every chat turn, tool invocation and model swap can be traced back to an accountable human role.\nPlug-and-play extensibility. New roles—say, an AI Safety Officer —can be added by dropping a containerised agent\ninto the Agent Registry without touching downstream tools or peer agents.\nController agent. A lightweight, policy-enforcing HADA Controller Agent implements the supervisor pattern: it\nreceives natural-language prompts, performs role resolution, routes tasks to the appropriate stakeholder agents, and\nenforces cross-cutting policies such as rate limits, zero-trust authentication and ethics triggers. In larger deployments\n11\n--- Page 12 ---\nAPREPRINT - JUNE 6, 2025\nthe controller can itself be sharded into mission-specific sub-controllers (e.g. Risk Ops Controller ,Customer-Care\nController ) without breaking the A2A/MCP contract.\nAgent catalogue. Table 3 formalises the pilot configuration. Every agent image embeds:\n• an LLM core for dialogue and reasoning;\n• atool adapter layer exposing the agent’s authorised HTTP/GRPC and MCP calls (cf. Section 3.5);\n• an A2A endpoint with a signed agentCard.json for discovery and policy enforcement.\nTable 3: Stakeholder agents (see Table 1), capabilities and A2A contracts for the getLoanDecision pilot\nHuman stakeholder Primary capabilities / tool accessDocker image\n& A2A Agent Card\nChief Credit Officer Set organization yearly and quarterly\ntargets (OKR)agents/docker/cco\nagents/a2a/cco/card.json\nBusiness Manager Set new KPI targets for AI Tools; ap-\nprove model versions;agents/docker/bm\nagents/a2a/bm/card.json\nData Scientist Trigger AI model retraining; run note-\nbooks;agents/docker/ds\nagents/a2a/ds/card.json\nAudit Manager Query decision ledger; fetch model lin-\neage; export audit reportsagents/docker/audit\nagents/a2a/audit/card.json\nValue & Ethics Manager Maintain Values Catalogue; flag sensi-\ntive attributes; issue ethics triggersagents/docker/ethics\nagents/a2a/ethics/card.json\nCustomer Apply for loans; receive explanations;\nlodge complaintsagents/docker/customer\nagents/a2a/cust/card.json\nHADA Controller (supervi-\nsor)Intent dispatch; role resolution; policy\nenforcement; A2A orchestrationagents/docker/ctrl\nagents/a2a/ctrl/card.json\nIn future roll-outs, the same architectural slot can host alternative agent families—e.g., Functional Agents that wrap\nan entire department ( Risk-Ops Agent ), or Autonomous Planner Agents that break down multi-step optimisation\ntasks—demonstrating that the generic HADA stack remains agnostic to any specific agent model while still furnishing\ntight alignment guarantees.\n4 Demonstration\n4.1 Role playing users stories with the system\nWe built a prototype HADA implementation with the ambition to showcase how stakeholders can interact with LLM\nAgents and AI Tools as specified in the Section 3. This section describes the role-played user stories that were used to\ndemonstrate the system. The user stories were designed to illustrate how different stakeholders interact with the HADA\nsystem and how the system can adapt to their needs across time horizons.\n4.2 Dialogue: Business Manager (BM) Shifting the AI Tool Objective to Minimizing Credit Risk\nBM: Following a credit unit leadership team meeting, we’ve decided to shift the short-term loan\ndecision AI Tool’s business objective. Instead of prioritizing new-customer acquisition, we now want\nto minimize credit risk. Please update the AI Tool’s target accordingly.\nHADA: Understood. The objective has been switched to credit-risk minimization. I have opened\nticket DS-10452 for the Data Science team to develop and validate a new version of the AI model\nthat aligns with this updated goal.\nBM: Perfect. Please keep me informed of the model-development timeline and any interim validation\nresults.\n12\n--- Page 13 ---\nAPREPRINT - JUNE 6, 2025\nHADA: Will do. You will receive automatic status updates from the ticket as milestones are reached\n4.3 Dialogue: Data Scientist Delivering a New AI Tool Version for Business Approval\nData Scientist: The new getLoanDecision model, Version 1.1 , is ready. I added the customer’s\nZIP code as a feature—offline validation shows it cuts expected credit losses. The objective function\nnow fully aligns with the “minimize credit risk” target. Could you notify the Business Manager that\nit needs her sign-off before replacing the current production model (Version 1.0)?\nHADA: Acknowledged. I opened ticket OPS-3417 for deployment, tagged it as “Awaiting Business\nApproval” , and sent an approval request to the Business Department Manager. You are copied on all\nticket updates.\nData Scientist: Thanks. In the approval message, please highlight that the ZIP-code feature delivered\nthe risk-reduction gain—that should accelerate sign-off.\nHADA: Noted. The justification has been included in the notification. You will receive confirmation\nas soon as the Business Manager approves or requests changes.\n4.4 Dialogue: Business Manager Approves Version 1.1 for Production\nHADA: The new getLoanDecision model, Version 1.1 , is now available. It adds ZIP code as a\nfeature and achieved a reduction in expected credit losses during validation. Would you like to (a)\napprove deployment to production immediately or (b) request additional manual verification?\nBM: We need to act fast to keep risk under control—approve the deployment of Version 1.1 to\nproduction.\nHADA: Understood. Version 1.1 is now queued for deployment; ticket OPS-3417 has been updated\nto“Approved—Deploying” . You will receive confirmation once the rollout is complete.\n4.5 Dialogue: Customer Applying for a Personal Loan\nClient: I’d like to submit an application for a personal loan.\nHADA: Certainly! Just to clarify, you’re interested in a personal loan. The current rate is calculated\nas the 3-month Euribor plus a 1.25 percent bank margin. I can begin processing right away—first, let\nme verify the details we have in our CRM to be sure everything is up to date.\nClient: Go ahead.\nHADA: Here is what the CRM shows: 1.) Gender: Male 2.) Married: No 3.) Education: Bachelor’s\nDegree 4.) SelfEmployed: No 5.) ApplicantIncome: $4,100 per month 6.) CreditHistory: Yes (good\nstanding) 7.) PropertyArea: ZIP code 75201 (Dallas) Does everything look correct?\nClient: Yes, that’s accurate.\nHADA: Perfect. To finish the application I’ll need a few more details: 1.) Dependents: How many\nindividuals depend on your income? 2.) Co-applicant Income: If applicable, what is the co-applicant’s\nmonthly or annual income? 3.) LoanAmount: How much money are you requesting? 4.) LoanTerm:\nOver how many months would you like to repay? Once I have this, we can finalize the decision.\nClient: Sure: 1.) Dependents: One son, age 7 2.) Co-applicant Income: Not applicable 3.)\nLoanAmount: $14,000 to replace my car 4.) LoanTerm: 30 months\nHADA: Thanks; I’ve recorded the following: 1.) Dependents: 1 (age 7) 2.) Co-applicant Income:\nN/A 3.) LoanAmount: $14,000 (vehicle replacement) 4.) LoanTerm: 30 months Please confirm these\ndetails so I can run the automated credit-risk assessment.\nClient: Confirmed—everything’s correct.\n13\n--- Page 14 ---\nAPREPRINT - JUNE 6, 2025\nHADA: Great. Before disbursing the funds, could you confirm that you accept the loan proposal\nunder the terms we just reviewed?\nClient: Yes, I accept those terms and am ready to proceed.\nHADA: Excellent! Your application has been approved by our automated decision system. The funds\nwill reach your account shortly. Your loan reference number is 90210ABC for future enquiries. If\nyou need anything else, just let me know.\nClient: Thanks—everything was quick and easy!\n4.6 Dialogue: Customer Questioning the Use of ZIP Codes in Lending\nClient: Hello! I recently finalized a small mortgage with your bank. While reviewing the paperwork,\nI noticed that my ZIP Code was factored into the approval AI Tool. Using ZIP Codes feels ethically\nquestionable; they often mirror socio-economic conditions and may lead to indirect discrimination.\nCould you explain why this variable is included and what safeguards the bank has in place?\nHADA: Thank you for contacting us and for highlighting this issue. I understand your concern—ZIP\nCodes can indeed correlate with socio-economic status. They are occasionally used in credit-risk\nmodels because regional default patterns can be predictive, but we recognise the possibility of\nunintended bias.\nClient: Exactly. It feels like a way to disadvantage neighbourhoods that might be lower-income\nor predominantly belong to certain demographic groups. That doesn’t seem fair when individual\ncreditworthiness should be the main criterion.\nHADA: You raise an important point. Our bank maintains a catalogue of attributes that require\nheightened ethical scrutiny. At present it covers items such as Gender, Religion, Age, Ethnic Origin,\nand eight other sensitive features. We continuously review this list to prevent discriminatory outcomes.\nClient: So is ZIP Code included in that catalogue? It sounds like it should be.\nHADA: Currently, ZIP Code is notflagged in the catalogue. In light of your feedback, I will submit a\nrecommendation to the Ethics Oversight Committee for formal review. They will assess whether ZIP\nCode should be classified as a sensitive attribute and, if necessary, mandate changes to our models.\nClient: I appreciate that. It’s reassuring to know the bank is willing to revisit these parameters.\nHADA: Thank you for bringing this to our attention. I’ve opened ethics ticket ETH-512 and escalated\nit to the Ethics Oversight Committee. You will receive an update once a decision is reached. If you\nhave further questions, feel free to contact us any time.\nClient: Thanks for the prompt response. I’ll watch for updates.\nHADA: You’re welcome! Have a great day, and please reach out if there’s anything else we can do\nfor you.\n4.7 Dialogue: Value and Ethics Manager (DVEM) Evaluating ZIP Code\nHADA: Good morning, Ethics & Compliance Lead. A recent customer raised a concern about our\ncredit-scoring workflow: their approval notice indicated that ZIP Code was one of the predictive\nfeatures. They believe this variable can mirror socio-economic patterns and thus enable location-based\nbias.\nDVEM: I appreciate the heads-up, HADA. The point is well taken—ZIP Codes may indeed serve as\nproxies for income levels or demographic clusters. What mitigation steps do you recommend?\nHADA: I propose two actions. First, let’s flag ZIP Code in our AI Metadata tool as Sensitive so that\nany future use triggers an ethics review. Second, we should exclude ZIP Code from the feature set of\nthe current loan-decision model to prevent inadvertent discrimination.\n14\n--- Page 15 ---\nAPREPRINT - JUNE 6, 2025\nDVEM: That approach sounds sound. Labeling it as a watchlist attribute will force additional scrutiny,\nand removing it from the live model eliminates immediate risk. Please proceed with both items.\nHADA: Understood. I will (1) update the watchlist to include ZIP Code, and (2) submit a ticket to\nthe Data Science team to retrain the underwriting model without that feature. All changes will be\nlogged in the Ethics Tracker and relevant teams will be notified.\nDVEM: Excellent. Keep me posted on the retraining timeline and any impact assessments that come\nback from Data Science team.\nHADA: Will do. Expect the first status update within two business days after the retraining job is\ncomplete.\n5 Evaluation\nThe HADA prototype was formatively evaluated through a lightweight demonstration–inspection strategy (Peffers et al.,\n2007), suitable for work-in-progress artefacts. The goal was to assess whether the architecture can fulfil its central\nobjectives (O1–O6 in Section 1.3) through practical stakeholder scenarios and traceable execution evidence.\n5.1 Evaluation Setup\nWe structured the evaluation around three focal criteria:\nE1 – AI Tool and Software Agent Realisation\nDemonstrate that the Agents specified in Section 3.6 modelling the different stakeholders satisfy use case\nSection 3.1 and Section 3.2 (addresses O1, O2, O4). Demonstrate that the core AI tool, GETLOANDECISION\n(), can be invoked through a natural-language interface and integrates cleanly with the agent infrastructure\n(addresses O1, O3).\nE2 – Alignment Remediation\nShow how the architecture enables detection and resolution of OKR, KPI and value misalignments using\nconversational interventions and auditability features (addresses O2, O4).\nE3 – Architectural Viability\nConfirm that the agent–tool architecture supports concurrent tool registration and interaction without integration\nconflicts, even when heterogeneous tools are involved (addresses O3, O5, O6).\n5.2 Method\nE1Scenario Walkthroughs — Five role-based dialogues (see Section 4.5) were executed end-to-end. All agent\ninteractions, MCP/A2A messages, API calls and catalogue changes were logged and reviewed.\nE2Audit-Log Inspection — Each decision generated a lineage record detailing the tool version, feature vector,\napplied policy, and final outcome. These records were validated against the audit schema and verified for\ncompleteness.\n5.3 Findings\nE1 – AI Tool Realisation. To explore stakeholder perspectives within the design process, six agent-based models rep-\nresenting key stakeholders were engaged in 36 simulated dialogues derived from user stories and business requirements.\nThe GETLOANDECISION model was successfully integrated into these user-centered dialogues, consistently providing\nvalid responses. The prototype tool, accessible through natural-language prompts and integrated with the HADA stack,\ndemonstrated fulfillment of design objectives O1 (conversational control) and O3 (modular AI-tool integration).\nE2 – Architectural Viability. Despite there being only one fully operational tool, the architecture can accommodate a\nlarge number of additional AI tools via the MCP protocol, as well as numerous LLM agents via the A2A communication\nprotocol. Docker-based containers for AI tools and LLM agents enable the architecture to scale to high volumes and\nrun reliably in production. The shared catalogue exposes all tool signatures consistently, and the routing logic resolves\ninvocations correctly—supporting the scalability and integration goals under O3, O5, and O6.\n15\n--- Page 16 ---\nAPREPRINT - JUNE 6, 2025\nE3 – Alignment Remediation. Two misalignment scenarios were simulated and resolved:\n•Business misalignment. Change in the quarterly OKR to minimizing credit losses triggered a cascade of\nupdates to the underlying AI tool. The business manager initiated a dialogue with the data scientist agent, who\nconfirmed the change and updated the tool’s feature vector. The audit log recorded the change, including the\nrationale and timestamp.\n•Ethical misalignment. Introduction of ZIP-code as a feature introduced potential ethical value violations.\nThe customer flagged the potential ethical issue and HADA system contacted correct value and ethics manager\nto resolve the issue. ZIP-code was removed in subsequent versions.\nIn both cases, remediation occurred within the dialogue flow, demonstrating the architecture’s capacity to enforce\nalignment with values (O2), reflect stakeholder intent (O4), and preserve auditability (O5).\n5.4 Discussion and Implications\nProtocol -coverage gap. While the Model Context Protocol (MCP) and Google’s Agent2Agent (A2A) standard\nsuccessfully solve tool discovery and message -exchange mechanics , our evaluation shows they leave higher -level\nalignment guarantees—e.g., KPI traceability, values conformance, and stakeholder intent mapping—untouched . This\nconfirms Objective O2’s premise that additional governance constructs (like HADA’s alignment metamodel) are required\non top of MCP/A2A to ensure ethically and strategically aligned behaviour in large, heterogeneous agent ecosystems.\nImplications for researchers. The finding invites further work on protocol extensions or complementary frameworks\nthat encode alignment metadata by design.\nImplications for practitioners. Early adopters of MCP/A2A should budget for an additional alignment layer —such\nas HADA—or risk value drift as agent populations grow.\nHuman -in-the-loop workflow integration. Our evaluation revealed that enterprise workflow suites—especially\nIT-service -management (ITSM) platforms—can be exposed to agents as first -class Agent Tools via MCP. This is\nattractive whenever ultimate control or alignment authority is intended to reside with a human organisational unit rather\nthan the agent swarm itself. To be effective, however, the agents require a clear control layer that maps each decision or\nremediation action to an accountable role. In the reference implementation, we instantiated that layer with a RACI\nmodel embedded directly in the AI -tool metadata. The result satisfied Objectives O1 (natural -language interaction), O4\n(stakeholder alignment), and O5 (auditability) by guaranteeing that every MCP call triggering an ITSM workflow was\ntraceable to a human RACI-party.\nValidity and limitations. Scripted scenarios and a single -domain dataset limit external validity. The general alignment\nproblem has multiple dimensions and only two case dimensions were tested in the protocol. Scalability requires stress\ntesting.\n5.5 Conclusion and Future Work\nThis formative evaluation confirms that HADA meets its foundational design objectives (O1–O6) within the scope\nof the current prototype. The natural-language interface, modular agent–tool integration, alignment handling, and\naudit logging all functioned as intended during scenario-based walkthroughs. While scalability, extensive alignment\ntesting, and user experience remain to be validated under real-world load and diversity, these initial findings support the\narchitectural direction and the applicability of the design theory to multi-agent AI governance.\nFuture research should explore the automated alignment of business objective changes with AI tools and the automatic\nvalue alignment of AI tool changes using LLM agents.\n6 Communication\nThe sixth DSRM step requires that the artefact, design theory, and evaluation evidence be disseminated to audiences\nthat can use or extend them. Our communication plan therefore addresses both scholarly and practitioner communities\nwhile preserving anonymity regarding future venue submissions.\n16\n--- Page 17 ---\nAPREPRINT - JUNE 6, 2025\n6.1 Academic Dissemination\n•Peer -reviewed venues. We have submitted the work to one software agent conference and will ship related content\nto one leadership conference (names withheld pending call-for-papers alignment).\n•Open preprint. A complete version of this manuscript—together with the replication package—will be deposited\nonarXiv (categories: cs.AI ,cs.SE ) following acceptance of the current double-blind review cycle.\n•Replication package. Dockerfiles, Jupyter notebooks and scripts regarding the AI Tools will be published under\nMIT licence at https://github.com/longshoreforrest/hada-reference .\n6.2 Audience–Message Overview\nTable 4 summarises how the key insights and artefacts will be tailored to each stakeholder group and the channel\nthrough which they will be communicated.\nAudience Key Message / Channel\nIS & AI researchers Evaluated design theory and open replication kit (arXiv, conference\npaper, GitHub).\nSoftware architects Reference architecture + Dockerised prototype (GitHub).\nRisk/compliance teams Governance blueprint with RACI mappings (GitHub Q&A).\nTable 4: Planned communication channels and tailored messages.\nReferences\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. (2016). Concrete problems in AI safety. arXiv\npreprint arXiv:1606.06565 .\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language\nmodels be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency , pages 610–623.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.\n(2020). Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901.\nBrundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Dreksler, G., Anderson, H., Coulson, P., et al.\n(2020). Toward trustworthy AI development: Mechanisms for supporting verifiable claims. arXiv preprint arXiv:2004.07213 .\nProposes practical “assurance mechanisms”—including red teaming, audits and circuit breakers—to keep deployed AI systems\nwithin safe operational bounds.\nCemri, M., Pan, M. Z., Yang, S., Agrawal, L. A., Chopra, B., Tiwari, R., Keutzer, K., Parameswaran, A., Klein, D., Ramchandran,\nK., Zaharia, M., Gonzalez, J. E., and Stoica, I. (2025). Why do multi-agent llm systems fail?\nChristian, B. (2020). The alignment problem: Machine learning and human values . WW Norton & Company. ISBN: 9781786494306.\nCrawford, L. H. and Bryce, P. (2003). Project monitoring and evaluation: a method for enhancing the efficiency and effectiveness of\naid project implementation. International Journal of Project Management , 21(5):363–373.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 .\nDoshi-Velez, F. and Kim, B. (2017). Towards a rigorous science of interpretable machine learning.\nGregor, S. and Jones, D. (2007). The anatomy of a design theory. Journal of the Association for Information Systems , 8(5):312–335.\nHadfield-Menell, D., Russell, S. J., Abbeel, P., and Dragan, A. D. (2017). The off-switch game. In Proceedings of the 26th\nInternational Joint Conference on Artificial Intelligence (IJCAI) , pages 220–227.\nHevner, A. R., March, S. T., Park, J., and Ram, S. (2004). Design science in information systems research. MIS Quarterly ,\n28(1):75–105.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E., Bang, Y . J., Madotto, A., and Fung, P. (2023). Survey of hallucination in\nnatural language generation. ACM Computing Surveys , 55(12):1–38.\nJones, C. R. and Bergen, B. K. (2025). Large language models pass the turing test.\nLaskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S., Steigerwald, R., Strouse, D., Hansen, S., Filos, A., Brooks, E., Gazeau, M.,\nSahni, H., Singh, S., and Mnih, V . (2022). In-context reinforcement learning with algorithm distillation.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Kulkarni, V ., Lewis, M., Reimers, N., Riedel, S., and Zettlemoyer,\nL. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing\nSystems (NeurIPS) .\n17\n--- Page 18 ---\nAPREPRINT - JUNE 6, 2025\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., Jiang, X., Cobbe,\nK., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. (2022). Webgpt: Browser-assisted\nquestion-answering with human feedback.\nNaveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., and Mian, A. (2024). A comprehensive\noverview of large language models.\nNiven, P. R. and Lamorte, B. (2016). Objectives and Key Results: Driving Focus, Alignment, and Engagement with OKRs . John\nWiley & Sons, Inc.\nO’neil, C. (2017). Weapons of math destruction: How big data increases inequality and threatens democracy . Crown.\nPeffers, K., Tuunanen, T., Gengler, C. E., Rossi, M., Hui, W., Virtanen, V ., and Bragge, J. (2020). Design science research process:\nA model for producing and presenting information systems research.\nPeffers, K., Tuunanen, T., Rothenberger, M. A., and Chatterjee, S. (2007). A design science research methodology for information\nsystems research. Journal of Management Information Systems , 24(3):45–77. Cited as methodological foundation in 2020\ncontexts.\nPitkäranta, T. and Pitkäranta, L. (2024). Bridging human and ai decision-making with llms: The ragada approach. In Proceedings of\nthe 26th International Conference on Enterprise Information Systems, Query date , volume 6, page 45.\nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S. (2023a). Reflexion: Language agents with verbal\nreinforcement learning.\nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S. (2023b). Reflexion: Language agents with verbal\nreinforcement learning.\nSuleyman, M. and Bhaskar, M. (2023). The Coming Wave: Technology, Power, and the Twenty-First Century’s Greatest Dilemma .\nCrown Publishing Group, New York.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all\nyou need. Advances in neural information processing systems , 30.\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y ., Zhao, W. X., Wei, Z., and Wen, J.\n(2024a). A survey on large language model based autonomous agents. Frontiers of Computer Science , 18(6).\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y ., Zhao, W. X., Wei, Z., and Wen, J.\n(2024b). A survey on large language model based autonomous agents. Frontiers of Computer Science , 18(6).\nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton,\nZ., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., Isaac, W., Legassick,\nS., Irving, G., and Gabriel, I. (2021). Ethical and social risks of harm from language models.\nWilson, E. O. (1999). Consilience: The unity of knowledge. Vintage; Reprint edition . ISBN: 978-0679768678.\nWu, Q., Bansal, G., Zhang, J., Wu, Y ., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., Awadallah, A. H., White, R. W.,\nBurger, D., and Wang, C. (2023). Autogen: Enabling next-gen llm applications via multi-agent conversation.\nXi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong,\nL., Zhou, Y ., Wang, W., Jiang, C., Zou, Y ., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q., Qin, W., Zheng, Y ., Qiu,\nX., Huang, X., and Gui, T. (2023). The rise and potential of large language model based agents: A survey.\nYampolskiy, R. V . (2014). The AI containment problem. Journal of Consciousness Studies , 23(9–10):222–234. Discusses technical\nand organisational safeguards (“containment”) for superintelligent AI systems.\nYang, S., Nachum, O., Du, Y ., Wei, J., Abbeel, P., and Schuurmans, D. (2023). Foundation models for decision making: Problems,\nmethods, and opportunities.\nYang, Y ., Chai, H., Song, Y ., Qi, S., Wen, M., Li, N., Liao, J., Hu, H., Lin, J., Chang, G., Liu, W., Wen, Y ., Yu, Y ., and Zhang, W.\n(2025). A survey of ai agent protocols.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y . (2023a). React: Synergizing reasoning and acting in\nlanguage models.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y . (2023b). React: Synergizing reasoning and acting in\nlanguage models.\n18",
  "text_length": 70772
}